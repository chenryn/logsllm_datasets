# Coarse-Grained Scheduling with Software-Defined Networking Switches

**Authors:**
- Myriana Rifai, University Nice Sophia Antipolis, France (Email: [EMAIL])
- Dino Lopez-Pacheco, University Nice Sophia Antipolis, France (Email: [EMAIL])
- Guillaume Urvoy-Keller, University Nice Sophia Antipolis, France (Email: [EMAIL])

## Abstract
Software-Defined Networking (SDN) consolidates the control plane of network equipment, providing fine-grained control over traffic flows. This work demonstrates that coarse-grained scheduling mechanisms can be implemented using SDN switches without requiring unsupported operations in OpenFlow. We leverage the flow statistics feedback loop provided by SDN switches to the controller, combined with priority queuing mechanisms typically available on switch output ports. Our approach is illustrated through experiments using an OpenvSwitch SDN switch controlled by a Beacon controller.

**Categories and Subject Descriptors:**
C.2.3 [Computer-Communication Networks]: Network Operations

**Keywords:**
Software-Defined Networking, Size-Based Scheduling

## 1. Introduction
Software-Defined Networking (SDN) offers centralized control over network traffic. The flexibility in defining flows enables advanced functions such as virtual networks and firewalls. However, the data plane remains opaque and cannot be directly influenced by the controller. Few studies have addressed extending SDN to the data plane with scheduling in mind. A notable exception is [6], which advocates for different scheduling or buffer management solutions based on specific scenarios.

Our objective is to demonstrate that, despite the limited capabilities of SDN in directly manipulating the data plane, it is possible to implement coarse-grained scheduling with legacy SDN equipment. We focus on size-based scheduling, where priority is given to flows in their early stages. This approach is valuable in current Internet and data center networks, where most traffic is carried by TCP and consists of many short flows. We utilize the feedback loop in SDN, where switches expose per-rule statistics to the controller, to identify long flows and separate them from short flows using multiple queues per port. Our goal is to minimize the completion time of the majority of flows.

Related work includes [7], where the authors divert some flows to longer, less-used paths to decrease completion time. In [2], the authors use commodity switches to offer several queues, complemented with ECN in the network. [4] proposes improving TCP performance in SDN networks, but at the cost of modifying end-hosts' protocol stacks. In contrast, we investigate a purely network-centric approach without modifying end-hosts.

We demonstrate the feasibility of building an SDN-based size-based scheduler using OpenvSwitch with a Beacon controller. We also propose a scalable version of our scheduler to avoid continuous monitoring of each active flow by the switch and the controller.

## 2. Design
We have designed two size-based schedulers: a state-full and a scalable scheduler, both implemented as applications on top of a Beacon controller. Both solutions require two queues per port (802.1p mandates eight queues per port) and assume these queues are managed with a strict priority scheduler. The high-priority queue is served as long as it has packets; the other queue is served when the first one is empty.

### State-Full Scheduler
The switch monitors all ongoing flows, and the controller queries flow statistics every 10 ms. Upon arrival of a new flow, the controller installs a new rule for this flow and assigns it to the high-priority queue of the corresponding port. If a flow exceeds 100 packets, the controller modifies the queue used by this flow and assigns it to the lower-priority queue.

### Scalable Scheduler
Continuous monitoring of each active flow is resource-intensive, and installing per-flow rules can quickly overload the forwarding table of SDN devices. When the load on the port is low (e.g., <50%), all schedulers typically offer the same performance as queues do not build up. To address these concerns, we propose a second scheduler where the controller initially sets up one default rule for a set of flows. If the throughput for that rule exceeds 10% of the link capacity, the scheduler zooms in on the traffic to separate large flows from short flows. The scheduler uninstalls the triggering rule and installs per-flow rules on demand. Every 10 ms, the scheduler checks the flow size; if it exceeds 100 packets, it modifies the queue to the lower-priority one. After a few cycles, large flows are isolated, and the grouped forwarding rule is reinstalled with high priority.

## 3. Preliminary Results
To illustrate our approach, we consider an experimental setup with ten clients and one server acting as a sink for traffic, connected via an OpenvSwitch (OVS) switch to a Beacon controller. The traffic workload is generated using impt 1 and consists of bulk TCP transfers. The flow size distribution follows a bounded Zipf distribution, with a flow size between 15 KB (10 packets) and 10 MB. The average flow size is around 100 packets, typical of the Internet. The load is controlled by tuning the flow inter-arrival time, which follows a Poisson process.

### Experimental Setup
- **Figure 1:** Experimental setup
- **Figure 2:** Flow completion time CDF for long and short flows

### Results
Results from 10 experiments at a load of about 90% show that small flows (defined as flows smaller than the 90th quantile of the flow size distribution, approximately 95 packets) represent about 15% of the load. The state-full scheduler offers the best response time for small flows and the majority of large flows, as their first 100 packets benefit from the scheduling mechanism. Only a minority of the largest flows suffer in the state-full scheduler. The scalable scheduler offers intermediate results, needing 10 additional ms to detect large flows. The longer tails of response times for the two schedulers compared to the no-scheduler case indicate that simultaneous better response times for all flows are not achievable.

Though preliminary, these results are encouraging and highlight the validity of our approach.

## 4. Future Work
In future work, we intend to develop an autonomic version of the scheduler that dynamically adjusts its parameters. For example, Tthreshold_pkts could be set as a quantile of the flow size distribution. Estimating the tail of a highly varying distribution is complex, but separating large flows from short flows does not require high precision. Variations in the input distribution or load are easily exposed through SDN statistics, and we expect to take advantage of this feature to build a fully adaptable scheduler.

We also plan to explore the use of size-based scheduling in data centers, where it could be deployed at any switch. Using size-based scheduling at several bottleneck links has not been explored, to our knowledge. Additionally, SDN enables us to combine routing and scheduling, which could be useful for specific applications like VM migration in modern data centers.

## 5. References
[1] K. Avrachenkov, U. Ayesta, P. Brown, and E. Nyberg. Differentiation between short and long TCP flows: Predictability of the response time. In Proceedings of IEEE INFOCOM 2004, 2004.

[2] W. Bai, K. Chen, H. Wang, L. Chen, D. Han, and C. Tian. Information-agnostic flow scheduling for commodity data centers. In 12th USENIX Symposium on Networked Systems Design and Implementation (NSDI 15), pages 455–468, Oakland, CA, May 2015. USENIX Association.

[3] T. Benson, A. Anand, A. Akella, and M. Zhang. Understanding data center traffic characteristics. Computer Communication Review, 40(1), 2010.

[4] M. Ghobadi, S. H. Yeganeh, and Y. Ganjali. Rethinking end-to-end congestion control in software-defined networks. In Proceedings of HotNets-XI, 2012.

[5] I. A. Rai, E. W. Biersack, and G. Urvoy-Keller. Size-based scheduling to improve the performance of short TCP flows. IEEE Network, 19(1), 2005.

[6] A. Sivaraman, K. Winstein, S. Subramanian, and H. Balakrishnan. No silver bullet: Extending SDN to the data plane. In Proceedings of HotNets-XII, 2013.

[7] F. P. Tso, G. Hamilton, R. Weber, C. S. Perkins, and D. P. Pezaros. Longer is better: Exploiting path diversity in data center networks. In Proceedings of IEEE ICDCS ’13, 2013.