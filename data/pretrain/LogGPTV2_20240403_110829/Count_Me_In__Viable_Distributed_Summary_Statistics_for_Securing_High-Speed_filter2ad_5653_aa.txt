title:Count Me In: Viable Distributed Summary Statistics for Securing High-Speed
Networks
author:Johanna Amann and
Seth Hall and
Robin Sommer
Count Me In: Viable Distributed Summary
Statistics for Securing High-Speed Networks
Johanna Amann1, Seth Hall1,2, and Robin Sommer1,2
1 International Computer Science Institute
2 Lawrence Berkeley National Laboratory
Abstract. Summary statistics represent a key primitive for proﬁling
and protecting operational networks. Many network operators routinely
measure properties such as throughput, traﬃc mix, and heavy hitters.
Likewise, security monitoring often deploys statistical anomaly detectors
that trigger, e.g., when a source scans the local IP address range, or
exceeds a threshold of failed login attempts. Traditionally, a diverse set
of tools is used for such computations, each typically hard-coding either
the features it operates on or the speciﬁc calculations it performs, or both.
In this work we present a novel framework for calculating a wide array of
summary statistics in real-time, independent of the underlying data, and
potentially aggregated from independent monitoring points. We focus on
providing a transparent, extensible, easy-to-use interface and implement
our design on top of an open-source network monitoring system. We
demonstrate a set of example applications for proﬁling and statistical
anomaly detection that would traditionally require signiﬁcant eﬀort and
diﬀerent tools to compute. We have released our implementation under
BSD license and report experiences from real-world deployments in large-
scale network environments.
1 Introduction
Researchers and operators alike routinely measure statistical properties of net-
work traﬃc, such as throughput, traﬃc mix, and “heavy hitters”; both for traﬃc
proﬁling and control, as well as for speciﬁc security purposes when aiming to
spot activity that “doesn’t look right”. For the latter, statistical anomaly detec-
tion proves particularly valuable by reporting activity that exceeds levels one
would normally expect so see, such as during port and address scans, login
brute-forcing, and application-layer vulnerability probing. Traditionally, we ﬁnd
a diverse set of approaches in use for implementing such monitoring, typically
limited to traﬃc features readily available in existing data sets such as NetFlow
records, SNMP counters, IDS output, or system logs; and often implemented in
the form of ad-hoc shell scripts processing ﬁles oﬄine in batches. While concep-
tually most proﬁling and anomaly detection tasks leverage just a rather small
set of statistical primitives, existing approaches tend to hard-code either the fea-
ture set they operate on or the speciﬁc computation they perform; and regularly
both. Consequently, sites ﬁnd it challenging to later adapt a setup to changes in
A. Stavrou et al. (Eds.): RAID 2014, LNCS 8688, pp. 320–340, 2014.
c(cid:2) Springer International Publishing Switzerland 2014
Count Me In: Viable Distributed Summary Statistics
321
requirements, miss out on opportunities for reuse in diﬀerent settings, and see
little incentive to optimize an implementation for performance.
In this work we present a novel summary statistics framework that facilitates
a wide array of typical proﬁling tasks and security applications. Our system
processes high-volume packet streams in real-time, operates transparently on
arbitrary features extracted from all levels of the protocol stack, and aggregates
results across independent monitoring points distributed across a network. We
focus on providing a transparent, easy-to-use user interface that, in particular,
hides the communication in distributed setups behind a simple, intent-based
API. We target operational deployment in large-scale network environments,
with link capacities of 10 GE and beyond; and we implement our design on top of
an existing open-source network monitoring system that is regularly deployed in
such settings. Our implementation includes a set of probabilistic data structures
to support memory-eﬃcient operation, as well as a plugin interface that allows
users to extend the supplied range of statistical primitives. We demonstrate a
number of real-world example security applications, including computation of
traﬃc matrices, detection of IP scans and SQL injection, and real-time “top-k”
measurements to determine, e.g., the most frequent hosts, HTTP destinations,
or DNS requests. We furthermore interface the latter to a browser-based visual-
ization library that renders the current “heavy hitters” in real-time for immediate
inspection. We evaluate our system in terms of the overhead it imposes on the
underlying network monitor with regards to CPU, memory, and inter-node com-
munication; and we ﬁnd it to scale well in realistic settings. We have released
our implementation as open-source software under a BSD-license as part of the
recent release of the underlying network monitor. It is in deployment now at a
broad range of sites, where it helps operations to protect their networks.
We structure the remainder of this paper as follows. §2 presents the moti-
vation and design of the summary statistics framework, and §3 describes our
implementation. §4 demonstrates a number of real-world example applications,
along with experiences from operational deployments. In §5 we asses performance
characteristics. §6 discusses related work, and we conclude in §7.
2 Design
Our work introduces a novel summary statistics framework that oﬀers a ﬂexible
platform to compute a wide variety of summary statistics in large-scale opera-
tional network environments. In the following we ﬁrst review the underlying mo-
tivation and then walk through a number of design aspects for the framework.
2.1 Motivation
While summary statistics constitute a crucial ingredient for many operational
network monitoring tasks, existing implementations generally cater to a speciﬁc
application or setting (see §6). Our framework instead aims to enable users to
deﬁne their own statistics, with no limitation on what input or computation to
322
J. Amann, S. Hall, and R. Sommer
use. The challenge with this approach lies in designing a system that provides
such ﬂexibility while also oﬀering the eﬃciency required to accommodate large-
scale deployment in high-performance settings.
To illustrate our motivation, consider the task of counting. Researchers and
operators alike tend to ask questions about their networks such as “How many
local IP addresses do we see?”, “What system produces the most traﬃc?”, “What
are the prevalent application protocols?”, and “Is there any host unsuccessfully
querying a large number of DNS names?”. Traditionally, answering such ques-
tions requires using a variety of diﬀerent tools. While conceptually these ques-
tions all come down to counting features, they process conceptually quite dif-
ferent information, from packet-level information like IP addresses to complex
application-layer attributes such as rejected DNS requests. Our goal is to unify
the computing of such results within a single system that decouples feature ex-
traction from the statistical infrastructure, providing users with a platform for
answering a wide range of their questions.
From experience with research and operations, we identify two overall types
of applications that network-based statistics tend to support: (i) network proﬁl-
ing aims to answer questions as sketched above for characterizing ongoing activ-
ity; and (ii) statistical anomaly detection identiﬁes situations where observed fea-
tures exceed an expected range, potentially leading to a security incident. Regard-
ing the former, while the range of possible proﬁling tasks is large, most consist of
a rather small set of computational primitives, such as summation and aggrega-
tion of values, standard set operations, computing simple measures such as maxi-
mum and average, and also sorting. Turning to statistical anomaly detection, one
typically ﬁnds conceptually simple measures deployed operationally; often just
straight-forward threshold schemes that trigger when activity exceeds a predeter-
mined value or ratio. The most common application is scan detection, which ﬁnds
hosts probing the local network by spotting an excessive number of failed attempts.
While traditionally scan detection refers to IP address or TCP/UDP port probing,
the concept extends to application-layer features as well, including probing web
servers with requests, email servers with destination addresses, DNS servers with
lookups, and also probing for vulnerable systems by trying application-layer ex-
ploits. While many monitoring systems support proﬁling and/or statistical
anomaly detection, their implementations typically hardcode either the feature
set they operate on or the speciﬁc calculation they perform.
2.2 Objectives
We identify the following objectives for our summary statistics framework.
Simple, Yet Flexible User Interface. The interface that the framework
exposes to the user should be easy to understand and use, yet suﬃciently ﬂexible
to support computation of a wide range of target statistics.
Data Agnostic. The framework should be data agnostic and avoid imposing
any constraints on the features it operates on.
Extensibility. The available statistical functionality should be adaptable and
extensible to computations not supported out-of-the-box.
Count Me In: Viable Distributed Summary Statistics
323
Real-Time Operation. The framework should process input in real-time
and provide results, including alarms, as quickly as possible.
Scalability. The framework needs to scale to large networks, including sup-
port for multiple traﬃc sources for either distributed monitoring or load-balancing
purposes.
Observation
Observation
Observation
Reducer
Statistical
Framework
Fig. 1. Basic Architecture
Summary 
Statistics
Trigger
2.3 Architecture
Figure 1 summarizes the summary statistics framework’s high-level architecture.
It observes a stream of tuples (key, value) in which in general both key and value
represent features derived in real-time from the incoming network traﬃc. As it
processes the stream, the summary statistics framework continuously reduces
each key’s values to an aggregate result. The framework also continuously eval-
uates a predicate on these aggregates to ﬂag speciﬁc situations by executing
corresponding triggers. Finally, at the end of a measurement interval, the frame-
work reports the ﬁnal summary statistics to the user in the form of (key, agg)
pairs where agg is the ﬁnal aggregate value for that key.
As one application example consider a simple TCP scan detector. Observa-
tions might take the form of tuples (s, d) representing failed connection attempts
from a source address s to a destination address d. A reducer Unique would
compute the number of unique destinations d for each source s, and a predicate
Threshold would ﬂag if that exceeds a speciﬁed limit by executing a ScanAlarm
trigger that reports an alarm. As another example, if one wanted to compute the
most popular DNS names overall, the observation values would be query names
extracted from DNS traﬃc. One would then aggregate all values into a single
global result by ﬁxing the observation key to a static value, and deploy a “top-k”
reducer that computes the k most frequently seen values among its inputs.
The summary statistics framework supports deployment in settings where the
traﬃc is not just monitored by a single process, yet with sets of physically sepa-
rated monitors, as long as the instances see disjunct packet streams. This could
be at diﬀerent ingress points of a large network, or in a cluster setting where
a load-balancer splits up the overall traﬃc to sent individual slices to separate
monitoring backends (as, e.g., in [25]). In such a setting the summary statistics
framework computes results transparently for the overall traﬃc aggregate, sim-
ilar to what a single instance would produce if it were seeing all the traﬃc at
one location. To accommodate such settings, we extend the basic architecture
into a distributed setup in which independent sensors reduce values locally ﬁrst,
and then at the end of a measurement interval forward their results to a master
server that merges them into global aggregates. That server then also evaluates
324
J. Amann, S. Hall, and R. Sommer
Observation
Reducer
Statistical Framework
Results & Outputs
Sensor 1
Tap
Sensor n
Tap
Observation
Reducer
Result
Observation
Reducer
Result
V
alu
e
s
N
& P
otify
oll
Observation
Reducer
Result
Observation
Reducer
Result
N otify
& P oll
V alu e s
Master
Merger
Result
Summary 
Statistics
Predicate
P di
t
Merger
Result
Trigger
Fig. 2. Distributed Architecture
the predicates and executes the trigger. Figure 2 illustrates the distributed set-
ting. As the reduced intermediary results will typically be small in volume, the
architecture scales well with increasing numbers of sensors.
As one additional ingredient to the distributed operation, we add result polling
that allows the server to request intermediary results from the sensors on de-
mand. Normally, the server would evaluate predicates only at the end of a mea-
surement interval once it has received all the local results. As that however might
introduce a potentially signiﬁcant delay until triggers execute, we introduce two
additional optimizations. First, we allow the server to poll sensors for their cur-
rent values on demand, even before the end of the measurement interval. It can
then already evaluate the predicate on the received intermediate values. Polling
alone however would not reduce trigger latencies suﬃciently without also causing
signiﬁcant communication overhead. Hence, we furthermore provide the sensors
with a notiﬁcation mechanism to signal that their intermediate local values have
changed suﬃciently to warrant requesting an update. For example, for a thresh-
old computation a sensor could notify the server once it has observed 20% of the
speciﬁed limit locally, with the assumption that other sensors are likely seeing
similar activity and, hence, globally the threshold might have been crossed. Upon
receiving the notiﬁcation, the server polls all the sensors, executes the predicate,
and runs the trigger if applicable.
2.4 Reducers
We conclude this section by examining the properties of reducers in more detail,
as they have to satisfy a number of requirements to ﬁt with the framework’s
operation. Recall that a reducer processes (key, value) pairs, aggregating them
into outputs (key, agg) where agg is an aggregate of all of key’s value as deter-
mined by the reducer’s computation. In the following we ﬁrst look at constraints
we impose on reducers, and then present a set of examples that satisfy these
requirements and all come with our implementation.
Count Me In: Viable Distributed Summary Statistics
325
Composable Results. As a crucial property for reducers in the distributed
setting, we require composability, i.e., support for aggregating the sensor’s local
results at the server-side. As a simple example, a reducer adding up numeri-
cal observations is trivially composable: the global sum is the total of the local
results. This constraint can however be challenging to satisfy for other opera-
tions, even if conceptually simple. For instance, when sampling input randomly,
deciding which samples to choose during merging without biasing the result is
non-trivial.
Constant Memory Size. When processing observations reducers typically
have to keep internal state during the measurement interval (e.g., the sum of
all values so far). However, to reliably support computing statistics on arbitrary
input volumes we require a constant bound on the amount of memory a reducer
maintains. Due to this restriction, our framework can, e.g., not compute the
median across observations.
Meaningful Intermediary Results. To support arbitrary measurement
intervals as well as continuous predicate evaluation, a reducer’s intermediary
values must be meaningful on their own at any time. This is again obviously the
case for a sum, which always reﬂects the current total; but less so for some of
the more complex data structures.
Summation, Average, Deviation, Variance, Maximum, Minimum.
These standard statistical measures are frequently used for traﬃc measurement
tasks. They all support a stream-based calculation model where the reducer
holds just the current result reﬂecting all observations seen so far, updating it
when a new observation comes in.
Unique. Determining the number of unique observations proves highly use-
ful for many network-oriented measurement tasks. However, a naive set-based
implementation would have a memory requirement of O(n) with n represent-