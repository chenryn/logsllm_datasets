title:Dataplane Specialization for High-performance OpenFlow Software Switching
author:L&apos;aszl&apos;o Moln&apos;ar and
Gergely Pongr&apos;acz and
G&apos;abor Enyedi and
Zolt&apos;an Lajos Kis and
Levente Csikor and
Ferenc Juh&apos;asz and
Attila Kor&quot;osi and
G&apos;abor R&apos;etv&apos;ari
Dataplane Specialization for High-performance
OpenFlow Software Switching
László Molnár∗, Gergely Pongrácz∗, Gábor Enyedi∗, Zoltán Lajos Kis∗,
Levente Csikor†, Ferenc Juhász∗,†, Attila K˝orösi‡, Gábor Rétvári†,‡
∗TrafﬁcLab, Ericsson Research
†Department of Telecommunications and Media Informatics, BME
‡MTA-BME Information Systems Research Group
ABSTRACT
OpenFlow is an amazingly expressive dataplane program-
ming language, but this expressiveness comes at a severe
performance price as switches must do excessive packet clas-
siﬁcation in the fast path. The prevalent OpenFlow software
switch architecture is therefore built on ﬂow caching, but
this imposes intricate limitations on the workloads that can
be supported efﬁciently and may even open the door to mali-
cious cache overﬂow attacks. In this paper we argue that in-
stead of enforcing the same universal ﬂow cache semantics
to all OpenFlow applications and optimize for the common
case, a switch should rather automatically specialize its dat-
aplane piecemeal with respect to the conﬁgured workload.
We introduce ESWITCH, a novel switch architecture that
uses on-the-ﬂy template-based code generation to compile
any OpenFlow pipeline into efﬁcient machine code, which
can then be readily used as fast path. We present a proof-
of-concept prototype and we demonstrate on illustrative use
cases that ESWITCH yields a simpler architecture, superior
packet processing speed, improved latency and CPU scala-
bility, and predictable performance. Our prototype can eas-
ily scale beyond 100 Gbps on a single Intel blade even with
complex OpenFlow pipelines.
CCS Concepts
•Networks → Bridges and switches; Network performance
modeling;
Keywords
OpenFlow software switching, packet classiﬁcation, template-
based code generation
Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for proﬁt or commercial advantage and that copies bear this notice
and the full citation on the ﬁrst page. Copyrights for components of this work
owned by others than ACM must be honored. Abstracting with credit is per-
mitted. To copy otherwise, or republish, to post on servers or to redistribute to
lists, requires prior speciﬁc permission and/or a fee. Request permissions from
permissions@acm.org.
SIGCOMM ’16, August 22-26, 2016, Florianopolis, Brazil
c(cid:13) 2016 ACM. ISBN 978-1-4503-4193-6/16/08. . . $15.00
DOI: http://dx.doi.org/10.1145/2934872.2934887
1.
INTRODUCTION
The OpenFlow switch is perhaps the most generic packet
processing device ever conceived [1, 2]. Depending on the
conﬁguration from the control plane, an OpenFlow switch
can identify ﬂows based on a broad combination of layer-
2, layer-3, and layer-4 (L2–L4) protocol attributes and ap-
ply essentially any meaningful packet processing action on
the packets of these ﬂows. This facilitates the rapid provi-
sioning and central administration of highly reconﬁgurable
network services on top of a potentially heterogeneous in-
ventory of switches and thereby providing Infrastructure-as-
a-Service capabilities [3, 4]; it unlocks networks for radical
innovation, allowing to mix circuit and packet switched in-
frastructures [5], heterogeneous control paradigms [6], and
legacy with clean-slate protocols [1, 7]; and it still retains
the familiar network operations mental model in expressive
high-level declarative policies [8–10]. Accordingly, Open-
Flow has found its use in a wide range of application areas,
from enterprise networks [11], data centers [12] and multi-
tenant clouds [13], optical transport networks [14], software-
deﬁned Internet exchanges [15], WANs [6] and WAN gate-
ways [16], all the way to telco load-balancers, stateless ﬁre-
walls, and mobile access gateways [17, 18]. Crucially, the
OpenFlow dataplane must support all these diverse applica-
tions with reasonable efﬁciency.
Unfortunately, this genericity conﬂicts with performance;
to quote [19], “OpenFlow is expressive but troublesome to
make fast on x86”. Embedded intrinsically into the Open-
Flow fast path is a series of costly packet classiﬁcation steps,
needed to associate packets with ﬂows for applying corre-
sponding actions, and, in spite of decades of continuous re-
search and development [20–24], OpenFlow software packet
classiﬁcation still remains too expensive for today’s line rates
[19]. Reasons for this all trace back to the rich packet pro-
cessing capabilities [25], the need to perform basically arbi-
trary wildcard matches on a broad selection of packet header
ﬁelds (40+ as of OpenFlow 1.4, [2]), ﬂow priorities impos-
ing a ﬁrm ordering on classiﬁcation that may break elemen-
tary networking conventions like IP longest preﬁx match-
ing (LPM), or the fact that a pipeline may include dozens
of stages and the corresponding classiﬁcations must be per-
formed successively, each stage using the results from the
539
previous stages. This rich semantics is already hard to sup-
port in hardware ﬁrewalls and intrusion-detection middle-
boxes, let alone in OpenFlow software switches [25]. With
the rise of server virtualization in data center networking and
network function virtualization, however, OpenFlow appli-
ances increasingly run on a stock x86 platform that lacks
the necessary hardware-based packet classiﬁcation compo-
nents [5, 6, 11–13, 15–18].
Consequently, most OpenFlow softswitch implementations
recur to excessive ﬂow caching at the fast path, in order
to amortize the computational costs of packet classiﬁcation
over the packets of ﬂows [26–28]. It is well-known, how-
ever, that ﬂow caching performs poorly for many impor-
tant applications that require forwarding decisions depend-
ing on diverse “high-entropy” packet ﬁelds, like transport-
layer ﬁrewalls, or produce short-lived ﬂows, e.g., peer-to-
peer protocols, MapReduce, or network monitoring [19,29].
Consequently, OpenFlow switches often exhibit abrupt per-
formance regressions in various hard-to-predict combinations
of ﬂow tables and trafﬁc patterns [29–34], opening the door
to malicious denial-of-service-like attacks even on as inno-
cently looking trafﬁc patterns as port scans [19, 29, 35].
In this paper, we argue that these adverse phenomena stem
from the fact that ﬂow caching over-generalizes: by enforc-
ing the same universal ﬂow cache semantics to fundamen-
tally diverse use cases it optimizes for the lowest common
denominator. For instance, the prevalent OpenFlow software
switch implementation, Open vSwitch (OVS), uses a hash-
based wildcard match store as (one of the hierarchy levels
of) its ﬂow cache, which works fairly well for simple Open-
Flow pipelines but inherently breaks down for large-scale IP
routing (that would rather require LPM, [30]) or ﬂow tables
that heavily match across layer boundaries [19, 29].
Instead of relying on an overly general-purpose fast path,
we argue, an OpenFlow switch should rather automagically
specialize itself for the actual workload, into an optimal ex-
act matching switch when the ﬂow tables specify pure L2
MAC forwarding [36], an LPM engine for L3 routing [37,
38], or a fast, optimized packet classiﬁer for L4 ACLs [20–
24], and a reasonable combination of these building blocks
whenever the OpenFlow pipeline matches heterogeneous pro-
tocol header ﬁelds.
We present ESWITCH, a new OpenFlow switch frame-
work that radically breaks with general-purpose datapaths
and embraces a fully customized dataplane. We view Open-
Flow as a declarative language to program the dataplane [8,
9] and we cast ESWITCH as a compiler that transforms a
declarative pipeline speciﬁcation into efﬁcient machine code.
Underlying ESWITCH is the observation that, similarly to
many natural programming languages [39], most real-world
OpenFlow applications compose the same small set of sim-
ple forwarding behaviors, or patterns, and therefore Open-
Flow pipelines lend themselves readily to be rewritten in
terms of a small set of predeﬁned static templates. ESWITCH,
accordingly, uses dynamic template-based code generation
to emit optimized OpenFlow fast paths that sidestep ﬂow
caching altogether. Thanks to the template abstraction we
can even construct meaningful performance models for the
generated code and reason about the fast path in simple quan-
titative terms.
We implemented ESWITCH on top of the Intel DataPlane
Development Kit (DPDK, [40]). The dataplane is written
entirely in assembly, hand-optimized to the x86 and ARM
platforms. We present extensive measurements to show that,
compared to OVS, ESWITCH features predictable and supe-
rior performance, latency and multi-core scalability with up
to two orders of magnitude improvement on complex Open-
Flow pipelines, while supporting similar update intensities.
Our rudimentary prototype easily scales beyond 100 Gbps
transmission speeds, downright beating many contemporary
hardware OpenFlow switches by a large margin [41].
The rest of the paper is structured as follows.
In Sec-
tion 2 we discuss OpenFlow switch architectures and present
our critiques for ﬂow caching.
In Section 3 we introduce
ESWITCH and in Section 4 we demonstrate pipeline compi-
lation on some common use cases, we analyze performance
related aspects, and we sketch simple analytic switch mod-
els. Finally, we review related work in Section 5 and we
conclude the paper in Section 6.
2. THE OPENFLOW PIPELINE
The crux of the OpenFlow dataplane is the pipeline, an ab-
stract description of the forwarding functionality programmed
into a switch. The pipeline is a linked hierarchy of ﬂow ta-
bles, each ﬂow table specifying a logically distinct stage of
packet forwarding in the form of a list of ﬂow entries. A
ﬂow entry in turn consists of a rule to be matched on packet
header ﬁelds, counters for maintaining statistics, and actions
to be applied to a packet whenever a match is found. Rules
designate ﬂows and actions establish pipeline processing for
these ﬂows, by triggering forwarding on a particular port,
updating packet contents, sending to a next stage ﬂow table
for further processing, etc. Flow entries are managed by the
controller via a dedicated OpenFlow channel, either reac-
tively (online, in response to received packets) or proactively
(ofﬂine, e.g., after a topology change).
Packet processing starts at the ﬁrst ﬂow table (“Table 0”),
trying to match the header ﬁeld tuple against the ﬁrst ﬂow
entry and then, should this fail, against successive ﬂow en-
tries in decreasing order of priority1. Processing terminates
when the matching ﬂow entry does not specify a next table
to be visited (using a goto_table instruction), at which
point the actions associated with the packet are executed.
Unmatched packets cause a table miss and, depending on
switch conﬁguration, can be dropped or sent to the controller
for further consideration.
Fig. 1a gives the pipeline for a simple ﬁrewall, arbitrating
packets between an Internet-facing external port and an
internal port connected to a web server hosted at the IP
address 192.0.2.1. The pipeline contains a single ﬂow
table; the ﬁrst ﬂow entry requires that packets received at
the internal port are forwarded to the external port
unconditionally, while in the reverse direction only HTTP
1When not stated otherwise, we use the convention that ﬂow
entries are listed in decreasing order of priority.
540
(a) sample ﬂow table
(b) equivalent pipeline
Figure 1: A simple ﬁrewall: (a) single-stage pipeline and (b)
an equivalent multi-stage pipeline. Flow entries are listed
in decreasing order of priority and priorities are not marked
explicitly. Note that in (b) we omitted irrelevant match ﬁelds
(ip_src and tcp_src).
packets (tcp_dst=80) are admitted and the rest of the traf-
ﬁc is dropped.
Industry best practices recommend to split the pipeline
into multiple consecutive stages, to modularize pipeline de-
sign by decoupling packet processing in different protocol
layers and to obtain simpler representations for complex for-
warding semantics and sidestep cross-product ﬂow-state ex-
plosion effects [2, 17, 18]. For example, the VMware NVP
network virtualization controller sets up more than a dozen
stages in its packet processing pipeline [13]. For our sam-
ple ﬁrewall, an equivalent multi-stage OpenFlow pipeline is
speciﬁed in Fig. 1b. Here the ﬁrst stage ﬂow table forwards
between switch ports, directing external packets to a second
stage ﬂow table that ﬁlters web trafﬁc.
2.1 OpenFlow Software Datapaths
The art and science of OpenFlow switch architectures re-
volve around the organization of functionality inside the soft-
ware pipeline implementation (the datapath) in order to per-
mit processing ﬂow entries and applying actions as fast as
possible, without sacriﬁcing OpenFlow’s inherent expres-
siveness. Existing implementations fall into the below two
coarsely deﬁned categories.
Direct datapath. A direct datapath performs packet classi-
ﬁcation right on the ﬂow tables. A simple implementation
strategy would be to organize ﬂow entries into a linked list
according to the order imposed by priorities and iterating
over this list priority-wise, possibly jumping to another table
whenever a match is found and starting linear iteration anew.
Thusly, a direct datapath in the worst case loops through all
ﬂow entries in all ﬂow tables until it ﬁnally ﬁnds a matching
ﬂow or can signal a table miss. Correspondingly, direct dat-
apaths are generally considered an inferior implementation
strategy but, thanks to their simplicity, they still ﬁnd impor-
tant use in reference designs and experimental implementa-
tions (OpenFlow Reference Switch [42], CPqD [43], xDPd
[44], LINC [45]), or as a last resort for complex pipelines to