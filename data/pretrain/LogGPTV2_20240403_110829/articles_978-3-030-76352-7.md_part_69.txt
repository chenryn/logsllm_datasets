come mismatch problem (i.e., how to retrieve concise documents, which might
be conceptually relevant, but do not explicitly contain some or all of the query
terms) query expansion techniques [21] provide alternatives.
Queryexpansion(QE)techniquesrefertotheprocessofreformulatingqueries
with additional terms that better define the information needs of the user [1].
Query expansion approaches rewrite the original query by adding other rel-
evant keywords or suggesting additional appropriate keywords. Classical QE
techniques have achieved good results in traditional text retrieval tasks. How-
ever, directly applying these methods to microblog information retrieval cannot
achievethedesiredperformance,giventhecharacteristicsoftheposts[12,21].To
overcome the limitations of existing methods, we propose a classification based
method to extract relevant keywords for query expansion. In this method, the
aim is to find expanded query terms from top documents. To do this, we firstly
preprocess an initial query and classify it; then we extract frequent terms from
top results. The idea is that we only consider top results that are in the same
class as a query to extract the expanded terms. For classification purpose, we
study and compare different models namely, Support Vector Machines (SVM),
Naive Bayes (NB) and Random Forest (RF); the convolutional neural model
(CNN) and the Multi-Task Deep Neural Network (MT-DNN) to find the best
model for classifying crisis micro-texts. We conducted experiments to assess the
models on a crisis tweet dataset [7,14].
446 M. Farokhnejad et al.
The main contributions of this paper include: (1) the leverage of a novel
approach to extract relevant terms to expand the original query, which can
better reflect users’ search intent, and (2) the implementation and comparison
of several classification techniques. The objective was to study and compare
differentmodelsforaddressingtheclassificationofcrisis-relatedtweets.Weshow
that we obtain competitive results with other works addressing crisis tweets
analysis [7,14], and we achieve to obtain better results applying MT-DNN for
tweets classification in a novel and original manner. Besides, the experimental
results demonstrate that our query expansion method is effective and reduces
noise in the expanded query terms, which improves the accuracy of microblog
retrieval.
Theremainderofthepaperisorganizedasfollows.Section2discussesrelated
work and compares our work with approaches addressing crisis micro-text clas-
sification and query expansion. Section3 describes the general micro-texts clas-
sificationstudyexpressedasaclassification problemaddressedusingsupervised
anddeeplearningmodels.Section4describestheexperimentalsettings,datasets
and discusses the obtained results. Section5 concludes the paper and discusses
future work.
2 Related Work
Millions of people use social media platforms, and the amount of data they
produce is enormous. Researchers are continuously working on developing sys-
tems which can efficiently process the human-generated data during events like
disasters to use them for building solutions that can save millions of lives.
Studieshaveshownhowcrisisdatacanbebeneficialandcrucialinanalysing
and collecting insight during and after a disaster. In the paper [19], the authors
proposedamatchdiscoveringsystemformappingthedisasteraidmessagesand
victims problemreports.Authorsinthe[2],analysedthesocialmediadatagen-
erated during the occurrence of a disaster. Paper [17] presented a classification
system for identifying the type of disaster tweet. Research in the field of query
processing can be classified, based on the source of expansion terms, into three
groups:queryexpansion basedonrelevancefeedback, queryexpansion basedon
local analysis, and query expansion based on global analysis [22]. Query expan-
sion based on relevance feedback utilises feedback from the initial retrieval to
enrichthe original query.Queryexpansion basedon local analysis is also known
as pseudo-relevance feedback method. Specifically, the retrieval system assumes
thatthefirstkdocumentsreturnedarerelevantdocumentsandqueryexpansion
words extract from the top k retrieved documents. Query expansion based on
global analysis aims to mine the relevance difference among words, and treats
the most relevant words as complements to the query.
Thetraditionaltextretrievalfieldappliesthequerymentionedaboveexpan-
sion methods. However, it is not easy to achieve the desired performance by
directly using these methods in microblog retrieval [10,22]. The reason is that
there is a large number of network vocabularies in microblogs and the junk
Classifying Micro-text Document Datasets 447
text, without any useful information. Because of these factors, if top-ranked
microblogs, returned by the initial search, are not relevant, microblog query
expansion through pseudo-relevance feedback will be of little use.
Ourworkintegratesatweetclassificationprocesstoretrievedocumentsthat
are in the query’s class to extract relevant keywords for query expansion. For
classifyingcrisis-relateddata,variousmachinelearningalgorithmsandtheirper-
formance have been proposed [5,7,8]. In [6], authors have shown DNN outper-
forms the traditional models in most of the tasks. The results of applying CNN
for analysing crisis data [13,14] have surpassed the traditional machine learning
modelsbyasignificantmargin.Theauthorsproposedthesemantically-enhanced
duel-CNN with two layers in [4].
Our work applied and compared the techniques previously used to classify
micro-texts, and particularly crisis tweets related to disasters. We reproduced
existingexperimentslike[7,14].Seekingforbetterperformancewiththedatasets
we used, we applied MT-DNN. The application of MT-DNN in this context is
novel and original and has led to promising results.Moreover, basedon our pre-
liminary experiments, weobservedthat queryexpansionbasedontheclassifica-
tion method obtains better candidate expansion words, which are semantically
close to the user query.
3 Query Expansion Based on Classification Results
Our approach is calibrated to explore disaster management datasets (e.g. earth-
quake, flooding, fire) produced by social media. We use prepared tweet disaster
datasets ready to be explored. We focused on expanding queries looking for
micro-texts(i.e.,tweets)relatedoftwoclasses:eventswhichrepresentsituations
producedduringthedisasterlifecycle(e.g.,someonelooksforshelter,abuilding
has been damaged); and actions performed in responseto events (a hotel is pro-
viding shelter for victims, people is approaching a damaged building to search
victims).
Figure1 illustrates the proposed framework to find the relevant terms to
expandaquery.Itconsistsoftwophases.Thefirstphasepre-processesaninitial
querytorewriteitbyextendingitwithrelevantterms.Thesecondisdevotedto
classifyingthequerytodetermineitstype(i.e.,event,action)inspiredin[15,20].
Fig.1. Classification based query expansion
448 M. Farokhnejad et al.
3.1 Phase A
Phase (A) consists of the following steps:
(i) The original query is preprocessed and cleaned removing stop words and
symbols. (ii) Then the query is classified into two classes (event, action). The
classified query is used to obtain a set of relevant tweets from a large unlabelled
tweet corpus using an inverted indexed matrix consisting of terms extracted
from the tweet corpus. (iii) Once we have a set of relevant tweets, we classify
them using our classification language models and select the tweets that belong
to the user query class. (iv) We obtain the m top frequent keywords out of the
classifiedrelevanttweets(instep(iii)).Wehaveelaboratedourdetailedapproach
for phase A in Algorithm1. The following paragraphs give details of the most
relevant steps of the algorithm.
Algorithm 1: Finding Relevant Terms
Data: (Crisis Related Tweets, User Query)
Result: Expanded Query Terms
begin
Cleaned Data←−Data Cleaning(CrisisRelatedTweets)
Indexed Data←−Inverted Index(Cleaned Data)
Query Keywords←−PreProcessing(UserQuery)
for Keyword∈Query Keywords do
Initial Result←−Finding tweets contain keyword(Keyword)
Query Label←−MTDNN Classifier(UserQuery)
for Tweet∈Initial Result do
if MTDNN Classifier(Tweet)==Query Label then
Final Result←−Tweet
ExpandedQueryTerms←−Term Frequency(Final Result)
Classifying Micro-texts. The data science workflow implementing the classi-
ficationphaseappliesdifferentmachinelearninganddeeplearningmodels[7,14].
Theworkflowsplitsintothreegroupsofactivities:(1)datapreparation;(2)clas-
sificationand(3)assessment.Theactivitiesofgroup2and3arespecialisedinto
thefollowingactivities:(2.1)Creationofabaselineapplyingsupervisedlearning
models (i.e., Support Vector Machines (SVM), Random Forest(RF) and Naive
Bayes (NB) as classic classifiers) and (3.1) their assessment. (2.2) Classification
withnopriorknowledgeand(3.2)assessment.(2.3)ClassificationbasedonMT-
DNNthatlooksforabetterclassificationscoreand(3.3)assessment.Assessment
activities enable the comparison of the performance of the models according to
their accuracy, to choose the one that provides the best results for rewriting the
queries.
Classification Baseline. As said before, for the classification step, our objective
was to identify 2 classes: events (situations coming up in disaster) and actions
(reactions performed in response to events).
Classifying Micro-text Document Datasets 449
Wefirstimplementedasupervisedlearningclassificationthatmapsaninput
(tweet)toanoutput(label)basedonlabelledcrisisdataavailableoncrisisNLP
website [7]. For example, the tweet “#BREAKING New Injury Numbers 172
injured, 7 fractures, 1 critical #napaquake” is related to the concepts of death
and accident, so it is mapped to the class event. In contrast, the tweet “Full
statementbyNapaValleyVintnersonnew#earthquakerelieffind,withalinkfor
making donations.” concerns Non-Governmental Organisations (NGO) efforts
and donations, so it is mapped to the class action. We used three supervised
learning algorithms, namely, Support Vector Machines (SVM), Random Forest
(RF) and Naive Bayes (NB) as classic classifiers.
Convolutional Neural Networks. The activities of the data science workflow spe-
cialisedonDeepneuralnetworks(DNNs)[9]weredesignedasfollows.Weuseda
Convolutional Neural Network (CNN) which a deep learning network consisting
of an input layer, multiple convolution layers and an output layer. For applying
CNNinNLPtasks,liketweetsclassification,weusedpreviouslycomputedtoken
sequencesasinputtotheCNN.Then,CNNfilterspreformasn-gramsovercon-
tinuous representations. These n-grams filters are combined by subsequent net-
worklayers,namelythedenselayers.CNNcanlearnthefeaturesanddistinguish
themautomatically, andtherefore,itdoesnotrequirehand-engineeredfeatures.
Thissaveshumaneffortandtimeandeliminatestheneedforpriorknowledge.A
distributed word representation and generalisation feature effectively utilise the
already used labelled data from the other event. This increases the efficiency of
theclassificationprocessonnewdata.Itremovestheneedtousemanuallycraft
features as it learns automatically latent features as distributed dense vectors,
which generalise well and have shown to benefit various NLP tasks [14].
Looking for Better Classification Results.WeusedMulti-TaskDeepNeuralNet-
works (MT-DNN) [11] to classify the tweets looking for better classification
results. MT-DNN is based on knowledge distillation which is a process of trans-
ferring the knowledge from a set of the larger, complicated model(s) to a lighter
compact, easier to deploy single model, without significant loss in performance.
In MT-DNN Lexicon Encoder l and Transformer Encoder l are the shared
1 2
layer. The input sentence: X = {x 1,x 2,··· ,x m} is a sequence of tokens of
length m. Then the lexicon encoder maps X into a sequence of input embed-
ding vectors, one for each token, constructed by summing the corresponding
word, segment, and positional embeddings. In layer l a multilayer bidirectional
2
Transformer encoder is used to map the input representation vectors l into a
1
sequence of contextual embedding vectors. MT-DNN learns the representation
using multi-task objectives, in addition to pre-training. The MT DNN model is
shown in Fig.2.
We fine-tuned the MT-DNN codebase to perform the specific task of single
sentenceclassification.TheinputX(asentence)isfirstrepresentedasasequence
of embedding vectors, one for each word, in l . Then the Transformer encoder
1
capturesthecontextualinformationforeachwordandproducesthesharedcon-
textual embedding vectors in l (l is a layer above l ). Finally, the additional
2 2 1
450 M. Farokhnejad et al.
Fig.2. MT-DNN model for representation learning.
task-specific layers generate task-specific representations (single sentence classi-
fication task in our case), followed by the process of knowledge distillation. The
logisticregressionwithsoftmaxpredictstheprobabilitythattweet(X)islabelled
as class c.
Finding Relevant Terms Set. In the following paragraph, we describe the
design of our proposed system framework, as shown in Fig.1. Given a set of
microblogcorpus.Wefirstperformdatacleaningandindexingintothedatabase.
Aquerytermwillthenbematchedwiththetweetindextoretrieveinitialresult
set.Thequeryandinitialresultsarethenclassifiedandonlythoseresults,which
are in the query class, used to expand the initial query with more relevant and
frequent terms. The logical flow of the process is detailed in Algorithm 1.
3.2 Phase B
Phase (B) consists of the following steps: (v) Obtain the a sentence-level vec-
tor for the user query by utilising the crisisNLP pretrained word embedding
via word2vec method. (vi) Compute the similarity between the query vector
obtained in step (v) compute m keyword vectors obtained in step (iv). Select
the expansion words with the highest similarity as the query expansion words.
(vi)Usethetopsimilarkeywordstoexpandtheuserqueryusingqueryexpansion
technique.
Classifying Micro-text Document Datasets 451
4 Experiments
We conducted experiments for finding the best classification model to classify
crisis related tweets as micro-text documents and then we used the best model
to set up experiments for our query expansion method1.
We used crisis NLP labelled data sets contain approximately 50k labelled
tweets and consist of various event types such as earthquakes, floods, typhoons,
etc.
The tweets are initially labelled into various informative classes (e.g., urgent
needs, donation offers, infrastructure damage, dead or injured people) and one
not-related or irrelevant class. The objective of the experiment was to find out
the model that can further and best classify the tweets into event and action
classes to have a vocabulary depicting respectively emerging situations (events
like water and shelter shortage) and performed actions during a disaster (relief
like water delivered to a given area, several rooms available for families).
4.1 Data Pre-processing
Pre-processing was required before using Tweets to address issues that charac-
terize them and thereby produce a clean dataset.
Data Cleaning. In our experiment, we considered that tweet texts are brief,
irregularexpressions,noisy,unstructured,andoftencontainingmisspellingsand
grammaticalmistakeswithwordsoutofthedictionary.Weremovedblankrows,
changed all the text to lowercase, removed URLs, re-tweets and user-mentions.
Then we moved towards tokenization that broke each tweet in the corpus into
a bag of words. Followed by removal of English stopwords, non-numeric and
special characters and perform word-stemming/lemmatization. WordNetLem-
matizerrequiredpostagstounderstandifthewordisanounorverboradjective
(by default, it was set to “noun”).
Indexing Data Collections. Asaresultofindexingthecleanedtweetscollection,
wecreatedaninvertedindexmatrixthatrepresentsthecontentofthecollection.
An inverted index is a dictionary where each word is associated with a list of
document identifiers in which that word appears. It enables agile access to the
position within a document in which a term appears. Indeed, this structure
allows avoiding making quadratic the running time of token comparisons. So,
instead of comparing, record by record, each token to every other token to see
if they match, the inverted indices are used to look up records that match on a
particular token.
1 https://github.com/MehrdadFarokhnejad/Classifying Tweeter Crisis Related
Data.
452 M. Farokhnejad et al.
Word Embedding Initialisation. In our CNN experiment setting, we have used
crisisembeddingtoinitialisetheembeddingatthebeginningoftheexperiment.
Crisisembeddingisa300-dimensionsdomain-specificembeddingcreatedby[13]