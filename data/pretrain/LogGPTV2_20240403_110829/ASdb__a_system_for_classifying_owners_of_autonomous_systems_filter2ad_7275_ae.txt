[61] M. E. Whiting, G. Hugh, and M. S. Bernstein. Fair work: Crowd work
minimum wage with one line of code. In AAAI Conference on Human
Computation and Crowdsourcing, volume 7, 2019.
[62] T. Ye, Y. Sangseof, and L. Robert, Jr. When does more money work?
Examining the role of perceived fairness in pay on the performance
quality of crowdworkers. In International AAAI Conference on Web
and Social Media, volume 11, 2017.
APPENDIX
A RIR DATA EXTRACTION
All RIRs release their own subset of information in a unique format.
We detail our specific data extraction method for each relevant AS
field below.
Baumann and Fabian [27] found that provided RIR pro-
Name.
vided AS names are often uninformative. Thus, across all RIRs, we
713
IMC ’21, November 2–4, 2021, Virtual Event, USA
Maya Ziv, Liz Izhikevich, Kimberly Ruth, Katherine Izhikevich, and Zakir Durumeric
extract names using the following fields and order of preference: or-
ganization name (provided for 80.19% ASes), description (provided
for 24.81% ASes) and AS name (provided for 100% of ASes).
Street Address. Our street address extraction method varies by
RIR, as detailed below.
(a) Coverage—The number of ASes classified increases as the reward of-
fered increases.
(b) Accuracy—ASes are not classified appreciably more accurately when
MTurks are offered increased rewards.
Figure 5: Evaluating Amazon Mechanical Turk
a premium price, but requires that at least 10 “qualified” workers
be assigned to a single unbatched classification task, drastically in-
creasing the cost of labeling a single AS (≥ $7). AMT also provides
a separate “master” qualification for select workers who “consis-
tently submit a lot of high quality work” [2]. Master MTurks cost 5%
more than regular MTurks (based on the offered reward) and can be
individually assigned to a single un-batched task. Prior work demon-
strates that Master MTurks provide higher-quality results [50], and
we therefore hire only Master MTurks for the duration of our ex-
periments.
Crowdworker Wages At Scale
We support the research community’s push for fair crowdworker
compensation. MTurks often make well below the US federal mini-
mum wage ($3 per hour on average [39]), and we strive to do better.
However, setting a fair crowdworker wage for micro-tasks at scale
is not a straightforward task. Amazon Mechanical Turk does not
monitor or enforce hourly wages, nor does it provide a way to set
MTurk pay as a function of minimum wage in the MTurk’s jurisdic-
tion. Setting MTurk payments per task can only be done in advance
of releasing the task, therefore requiring advance knowledge of how
much time a task should take, which can be difficult to estimate.
714
an address field).
• RIPE: We use the description field; RIPE has no address field.
• APNIC: We use the address field (99.98% of entries contain
• AFRINIC: We use the address field (90.01% of entries con-
tain an address field). Note that 92% of entries obfuscate
their address with “*” characters and only reveal the city,
state/province, and country; we remove all obfuscated parts
of the address.
• LACNIC: We use the provided city and country fields, as no
• ARIN: We use the address field (100% of entries contain the
other address data is available.
entire street address).
Phone. APNIC and ARIN provide contact phone numbers for
100% of their ASes. No other RIRs provide phone numbers.
Country. We use CAIDA’s AS2org dataset [12] to get country
information for 32% of ASes.
For all RIRs except LACNIC, we extract candidate do-
Domain.
mains by using the provided emails in the aut-num objects and
connected org and contact objects, in addition to a regex match
to find all URLs in the “remarks” field. LACNIC does not provide
domains or contact emails.
B CROWDWORK EXPLORATION
While building ASdb, we observed that our automated system strug-
gles with some classification tasks that appear “easy” from a hu-
man’s perspective, as humans have additional context and can more
skillfully infer what information is relevant to a given question. We
therefore tested whether human crowdworkers are effective at clas-
sifying ASes that automated solutions fail to correctly categorize.
In our experiments we find that crowdwork is impractical for
our use case. Here we describe our experiments and surface several
lessons for large-scale labeling of networking data.
Ethics. We submitted an IRB protocol for institutional review.
The Stanford IRB ruled that our study does not constitute human
subjects research and does not require IRB approval, as we are
studying only the quality of crowdworker-generated labeled data
and not identifiable individuals or their behavior. Nevertheless,
compensation and fair treatment of crowdworkers require careful
consideration, and we detail the steps we take to interact with
crowdworkers ethically.
Platform choice. We explore seven candidate crowdwork plat-
forms. Six are poor fits: Workfusion [22] does not guarantee that
“labelers” will be human, Appen [4] markets to companies with
bigger projects, Clickworker [8] is notably more expensive, Lab
In The Wild [14] requires tasks to be “fun,” and Upwork [20] and
Prolific [18] require a unique survey per task, therefore not scaling
to creating hundreds of labeling tasks. We thus choose Amazon
Mechanical Turk (AMT) [3], which offers an easily scalable frame-
work to deploy labeling tasks and custom pricing. AMT also allows
specifying worker qualifications (e.g., IT employment industry) at
102030405060Reward (Cents)0.10.20.30.40.50.60.70.80.91.0CoverageFinanceTech0.10.20.30.40.50.60.70.80.91.0Finance102030405060Reward (Cents)0.10.20.30.40.50.60.70.80.91.0                        AccuracyTechnologyLoose MatchStrict MatchASdb: A System for Classifying Owners of Autonomous Systems
IMC ’21, November 2–4, 2021, Virtual Event, USA
Figure 6: Amazon Mechanical Turk Wages—Reward-per-task
and median hourly wage is not directly correlated.
Research sub-communities have different approaches to these
hurdles: the ML community routinely labels large datasets using a
small or unstated flat fee per task (e.g., [37, 40, 43, 54, 56]). The HCI
community often sets a living-wage objective for task compensa-
tion [26, 36, 61] and regularly reports crowdworker wages for re-
search tasks (e.g., [25, 29, 44]). In addition, beyond fair-wage ethics,
MTurk compensation can also affect the accuracy of tasks [62].
To understand what compensation per task must be offered to
an MTurk in order to ensure fair wages, accurate results, and com-
patibility with our research budget, we conduct an experiment to
quantify MTurk performance depending upon the offered reward.
Concretely, we select a group of 20 technology and 20 finance
ASes and ask 3 MTurks to choose one or more NAICSlite layer 2
Technology category for each AS. We set the consensus requirement
to be at least two out of three MTurks assigning an AS the same cate-
gory label. We replicate this setup 6 times, varying only the amount
paid to each MTurk (between 10–60 cents in 10 cent increments),
and ensure that no MTurks overlap between assignments.
Based on the average amount of time spent by each MTurk for
each task, and the reward given for each task the average hourly
wage is $19.41/hour across all tasks. However, we discover that
reward-per-task and overall hourly wage is not directly cor-
related. We calculate the median hourly wage per task in Figure 6
and find that the median wage ranges extensively, between $55/hour
– $6.60/hour, due to MTurks spending a variable amount of time
across tasks. Only in one experiment did the median hourly wage
fall below the US federal minimum wage [16], due to MTurks un-
foreseeabley taking longer than expected compared to experiments
above and below the offered reward. Thus, increasing the reward-
per-task does not necessarily increase the overall median wage.
The number of ASes classified increases as the reward of-
fered increases (Figure 5a) due to increase in consensus (i.e., agree-
ment amongst MTurk labels). For example, offering 50 cents per
task leads to 95% of both tech and financial ASes being classified,
which is a 10–20% increase compared to offering 30 cents.
To understand how wages affect the accuracy of a task, we use the
same experiment and define accuracy using a strict-match criterion
Figure 7: Amazon Mechanical Turk Consensus—Increasing
the number of MTurks required for consensus (.e.g., requiring 4/5
MTurks to agree instead of 2/3) increases accuracy by up to 12%
and decreases coverage by up to 35%.
(i.e., all consensus-backed crowdworker categories match all Gold
Standard categories) and a loose-match criterion (i.e., at least one
consensus-backed crowdworker category is contained in the set of
Gold Standard categories). If no consensus among the MTurks is
reached for a particular AS, we exclude it from our accuracy count
because there is no reliable label.
We find that increased rewards and classification accuracy
are not directly correlated (Figure 5b). When evaluating loose-
match accuracy, all MTurks, no matter the reward, achieve a 100%
and 90%–100% accuracy when classifying finance and technology
ASes, respectively. As the reward decreases, there is a slight in-
crease in loose-match accuracy for technology ASes, which we
attribute to a decrease in coverage (as crowdworkers may not be
willing to spend as much time generating accurate answers for the
“hard” cases); as consensus decreases amongst MTurks, only the
“easy” cases achieve consensus, leading to higher accuracy. Strict
match accuracy is not correlated with reward; the difference in
average accuracy between rewarding 10 and 60 cents is less than
5% when classifying both technology and finance ASes. Across all
rewards, MTurks perform consistently worse at accurately labeling
technology categories compared to financial categories, even when
technology category labels are accompanied with definitions within
the task interface.
Crowdworker Consensus
Another factor affecting total cost of crowdwork in addition to
accuracy and coverage is the crowdworker consensus requirement.
To test if increasing the number of MTurks per assignment increases
accuracy and coverage, we run an experiment in which we select
the same technology and financial ASes, fix the reward at 30 cents,
715
102030405060Reward (Cents)010203040506070Median Wage ($/Hour)FinanceTechnology0.60.70.80.91.0AccuracyTech - Loose MatchTech - Strict MatchFinance - Loose MatchFinance - Strict Match233545Consensus Requirement0.60.70.80.91.0                       Fraction of ASesCoverageTechFinanceIMC ’21, November 2–4, 2021, Virtual Event, USA
Maya Ziv, Liz Izhikevich, Kimberly Ruth, Katherine Izhikevich, and Zakir Durumeric
System
ASdb (AMT = True)
Gold Standard
Test Set
Stage
0 Sources Matched
1 Source Matched
≥2 Sources Matched - None Agree
Overall Layer 1
Overall Layer 2
Coverage Accuracy
3%
18%
5%
97%
91%
33%
92%
100%
98% (+1%)
87% (+0%)
Coverage Accuracy
4%
14%
7%
94%
93%
0%
81%
74%
95% (+2%)
78% (+3%)
Uniform Gold Standard
Coverage Accuracy
5%
13%
12%
95%
97%
20%
80%
93%
93% (+4%)
83% (+1%)
Table 9: Evaluation of ASdb supplemented with crowdwork— Adding crowdwork to help categorize ASes affects coverage and
accuracy negligibly. We note that NAICSlite layer 2 coverage can be greater than NAICSlite layer 1 coverage, as only the ASes with a
labeler-assigned NAICSlite layer 2 category (142, 141, and 189 for the three data sets, respectively) are evaluated in NAICSlite layer 2 metrics.
Recall that our sample size is 150 ASes for the Gold Standard and test set, and 320 for the Uniform Gold Standard set.
and assign either 3 or 5 MTurks to each task. We vary the consensus
requirement to require 2/3, 3/5, or 4/5 MTurks to agree on a category.
Strengthening the consensus requirement from 2/3 to 4/5 leads
to a 100% loose matching accuracy (Figure 7). However, this comes
at the expense of coverage, which drops by 35% when classifying
tech ASes due to the lack of consensus. Crowdworker fair pay and
cost-effectiveness of research are not at odds for this crowdwork pa-
rameter: equivalent overall accuracy and better coverage is achieved
when paying 40 cents for 3 MTurks (in the previous experiment)
compared to 30 cents for 5 MTurks.
Applying Crowdwork To ASdb
Using our analysis of how offered reward and consensus require-
ment affect coverage and accuracy (Appendix B), we evaluate the
potential for crowdwork to address two concrete failure modes of
ASdb’s automated analysis: catching ML false negatives and choos-
ing between disagreeing data sources. We also evaluate the cost
efficiency of applying crowdwork to each of these use cases. In
all experiments, we define coverage to be the percentage of ASes
for which the Amazon Mechanical Turk crowdworkers (“MTurks”)
reached consensus for at least one category. We evaluate accuracy
using the Gold Standard and Uniform Gold Standard datasets.
Catching ML failure cases. We ask crowdworkers to classify
the ASes that our ML classifiers (Section 4.1) incorrectly classify,
at 30 cents per task. We find that a 2/3 MTurk consensus ratio
correctly classifies 60% of the tech ASes that were misclassified in
our experiment set, and a 3/5 consensus correctly classifies 100% of
the misclassified ASes. Thus, with a sufficiently forgiving consensus
requirement, MTurks are successful at catching ML failures.
Our classification pipeline achieves a low rate of false positives
(1% and 3% when classifying ISPs and hosting providers, respec-
tively). Given that crowdworkers are capable of catching a classi-
fier’s false negatives with high accuracy, we consider the possibility
that they could serve as an additional review stage for potential
false negatives. We identify the class of potential false negatives to
be any AS which is classified as Technology by a data source, but
not flagged by either of our classifiers. 23% of Gold Standard ASes
fall into this category (i.e., roughly 20.7K of all registered ASes),
thereby implying that it would cost at least $31,000 to complete this
task with the necessary pay and consensus requirements to achieve
high accuracy. We find no practical way to more granularly filter
out which ASes need human review beyond this heuristic, so we
rule out this application of crowdwork as too expensive.
716
Resolving data source disagreements. We test whether MTurks
can effectively determine the correct category in the presence of
conflicting labels from multiple data sources. We select 35 random
ASes from the Gold Standard with conflicting category labels, along
with their manually identified working websites, and ask MTurks
to select all applicable layer 2 NAICSlite categories (or “none of the
above”) from the union of all NAICSlite categories provided by the
matched data sources. Requiring a 2/3 MTurk consensus ratio at
10 cents per task, we see that MTurks achieve consensus in 92%
of cases and achieve a 94% and 50% loose-match and strict-match
layer 2 accuracy, respectively. Thus, MTurks can be leveraged to
resolve disagreement between data sources.
By contrast to catching ML failures, using crowdworkers to
choose the best NAICSlite category is more cost-efficient. MTurks
can accurately select an NAICSlite layer 2 category for 86% of all
provided ASes when each AS is labeled by 3 MTurks at a rate of
10 cents per task. Roughly 22% of all registered ASes could be sent
to MTurks to select the best NAICSlite category: 4% of all Gold Stan-
dard ASes are only matched to one data source, 17% are matched to
multiple disagreeing data sources, and an estimated 1% of ASes have
a working domain, but match to zero sources. In total, applying
crowdwork to these cases would cost an estimated $6,000.
We evaluate the potential impact of crowdwork-based data source
disagreement resolution on the overall ASdb system, compared to
an automated “auto-choose source” heuristic that we develop in Sec-
tion 5.1. Surprisingly, we find that crowdwork adds little to the
system overall: while crowdworkers inexplicably misclassify 9%
of ASes they are given, they achieve roughly the same accuracy (80–
92%) as our “auto-choose source” heuristic. Table 9 shows the final
accuracy of ASdb with crowdwork integrated; using crowdworkers
instead of “auto-choose source” leads to an accuracy improvement
of up to 3% and coverage decrease of up to 3%. We attribute this
result to the fact that unlike ML failure cases, data source disagree-
ments are typically difficult corner cases: of the ASes that we sent
to crowdworkers when evaluating our final ASdb system, 31% do
not have a working website, 11% have an uninformative website
(e.g., an Apache test page [19]), and 49% of organizations achieve
no consensus amongst data sources or crowdworkers. This analysis
calls into question whether the cost of resolving data source dis-
agreements is worth the small accuracy gain. For our application,
we conclude that it is not.
ASdb: A System for Classifying Owners of Autonomous Systems
IMC ’21, November 2–4, 2021, Virtual Event, USA
Source
D&B
Zvelo
Crunchbase
ASdb
Overall
229 / 341 (67%)
199 / 262 (75%)
108 / 125 (86%)
283 / 326 (86%)
Agriculture
13 / 17 (76%)
4 / 7 (57%)
2 / 3 (66%)
14 / 15 (93%)
Nonprofits
6 / 7 (85%)
2 / 5 (40%)
3 / 3 (100%)
6 / 6 (100%)
Tech
58 / 117 (49%)
73 / 91 (80%)
27 / 30 (90%)
96 / 112 (85%)
Construction
8 / 10 (80%)
6 / 9 (66%)
3 / 4 (75%)
8 / 9 (88%)
Education
18 / 22 (81%)
20 / 21 (95%)
4 / 4 (100%)
21 / 22 (95%)
Finance
15 / 17 (88%)
9 / 11 (81%)