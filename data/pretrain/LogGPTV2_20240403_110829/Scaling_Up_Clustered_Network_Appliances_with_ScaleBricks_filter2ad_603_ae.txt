8. RELATED WORK
Conventional Solutions for Global Partition Table There
is a wealth of related work on building efﬁcient dictionaries
mapping keys to values.
Standard hash tables cannot provide the space and perfor-
mance we require. Typically, they store keys or ﬁngerprints
of keys to resolve collisions, where multiple keys land in the
same hash table bucket. Storing keys is space-prohibitive
for our application. To reduce the effect of collisions, hash
tables typically allocate more entries than the number of el-
ements they plan to store. Simple hashing schemes such
as linear probing start to develop performance issues once
highly loaded (70–90%, depending on the implementation).
Multiple-choice based hashing schemes such as cuckoo hash-
ing [30] or d-left hashing [28] can achieve occupancies greater
than 90%, but must manage collisions and deal with perfor-
mance issues from using multiple choices.
Perfect Hashing schemes try to ﬁnd an injective mapping
onto a table of m entries for n (n ≤ m) distinct items from a
larger universe. Seminal early work in the area includes [12,
17, 16], Fredman and Komlós [16], and such work refers to
set separating families of hash functions. More recent work
on attempting to design perfect hash functions for on-chip
memory [26] is most similar to ours. Our approach uses both
less space and fewer memory accesses.
Perfect hashing data structures can be compressed; for ex-
ample, both ECT [25], as used in SILT [24], and CHD [3] use
fewer than 2.5 bits per key to store the index.2 However, these
schemes must also store the value associated with each key.
Nor do these compressed implementations of perfect hashing
provide sufﬁcient lookup throughput. ECT is optimized for
indexing data in external storage such as SSDs, and its lookup
latency is one to seven microseconds per lookup; CHD is
faster, but remains several times slower than our solution.
Bloom Filters [4] are a compact probabilistic data struc-
ture used to represent a set of elements for set-membership
tests, with many applications [7]. They achieve high space
efﬁciency by allowing false positives.
Bloom ﬁlters and variants have been proposed for set sep-
aration. For example, BUFFALO [33] attempts to scale the
forwarding table of a network switch. It does so by looking
up the destination address in a sequence of Bloom ﬁlters, one
per outgoing port. However, this approach to set separation
is inefﬁcient. A query may see positive results from multi-
ple Bloom ﬁlters, and the system must resolve these false
positives. SetSep is also more space efﬁcient. Finally, up-
dating the Bloom ﬁlter to change the mapping of an address
from port x to port y is expensive, because it must rebuild
the ﬁlter to delete a single item, or use additional structure
2The information theoretical lower bound for minimal perfect hashing is
approximately 1.44 bits per key.
Figure 11: # of FIB entries with different # of servers
As both intuition suggests and the ﬁgure shows, traditional
FIB duplication does not scale: The ensemble supports only
as many FIB entries as a single node does. In contrast, hash
partitioning of the FIB scales linearly, but at an extra forward-
ing cost. In ScaleBricks, the number of FIB entries that can
be handled scales nearly linearly for a small number of nodes.
7. DISCUSSION
Skewed Forwarding Table Distribution ScaleBricks as-
sumes the assignment of keys to handling nodes is not under
its control; in the case of EPC, a separate controller assigns
ﬂows to handling nodes. If the assignment is skewed, some
nodes must handle a disproportionately larger number of
ﬂows. In such a case, hash partitioning provides linear scal-
ability (at the cost of one more hop) because it evenly dis-
tributes the FIB across intermediate lookup/indirect nodes.
ScaleBricks, however, uses a combined lookup/handling node,
so the FIB partitioning is skewed according to the handling
node of each entry. In the ideal case, when each node has the
same number of FIB entries, ScaleBricks scales well to up
to 8 servers, as we have shown in the evaluation. However,
when the distribution of the FIB entries is skewed, Scale-
Bricks can no longer achieve this scalability. The tradeoff
between scalability and latency is fundamental, and Scale-
Bricks achieves near-optimal scalability with minimal latency
(switching hops).
Isolation of Failure In general, ScaleBricks exhibits better
failure tolerance properties than hash-partitioned clusters by
providing failure isolation—when a server fails, the network
communication from/to other servers can continue. This
isolation comes from the fate sharing between each server
and partial FIB hosted by itself. In a hash-partitioned cluster,
however, one failing node could cause forwarding errors for
the keys for which the node serves as a lookup node, even if
these keys are not handled by the failing node itself.
More Applications ScaleBricks helps improve the perfor-
mance of stateful, clustered network appliances which assign
ﬂows to their speciﬁc handling nodes without being able to
control the assignment. We demonstrated the usefulness of
ScaleBricks by using the LTE-to-Internet gateway as a driving
048121620242832# of Servers010203040506070Millions of FIB entriesFull DuplicationHash Partition (2 hops)ScaleBricks252(such as counting Bloom ﬁlters). Bloomier ﬁlters [9] provide
an alternative approach; the value associated with a key is
the exclusive-or of values set in multiple hash table locations
determined by the key. Approximate concurrent state ma-
chines [5] similarly provide an alternative structure based on
multiple choice hashing for efﬁciently representing partial
functions. Our approach is again more scalable than these
approaches.
Fast Software Routers and Switches RouteBricks [13]
demonstrated that a commodity server can forward 64-byte
IPv4 packets at 19 Mpps by batching packet I/O and paralleliz-
ing packet processing on modern multi-core CPUs and multi-
queue NICs. PacketShader [20] exploits the parallelism and
memory bandwidth of GPUs to provide fast IPv4 and IPv6
forwarding. ScaleBricks similarly exploits modern CPUs by
performing batched table lookups that make more efﬁcient
use of memory bandwidth and reduce CPU cycles.
Scaling Forwarding Tables Bloom ﬁlters [4] have been
used to improve memory-efﬁciency in packet forwarding
engines. Dharmapurikar et al. [11] deployed Bloom ﬁlters
to determine which hash table to use for a packet’s next hop.
Their scheme often requires additional hash table probes per
address lookup on false positives returned by the Bloom ﬁlters.
BUFFALO has a similar goal, as noted above.
CuckooSwitch [34] scales the forwarding table of a single
node. ScaleBricks adopts CuckooSwitch’s per-node FIB de-
sign, but it could use any efﬁcient FIB. The contribution of
ScaleBricks is, instead, the use of SetSep to build a global
partition table that is divided across switch cluster nodes and
helps forward incoming packets to the handling node in a
single hop.
Flat Address Routing Flat addresses enable simple network
topology and easy manageability of enterprise and datacen-
ter networks (e.g., SEATTLE [22]) as well as straightfor-
ward support for wide-area mobility (e.g., ROFL [8], AIP [2],
XIA [19]). ScaleBricks provides a new, scalable implementa-
tion option for such ﬂat designs.
9. CONCLUSION
ScaleBricks is a new mechanism for helping to “scale up”
clustered network applications. Its core contribution is the de-
sign and implementation of a new data structure, SetSep, for
compactly storing the mapping from keys, such as ﬂow IDs
or ﬂat addresses, to values, such as the node ID that should
handle that ﬂow. To make this structure practical, ScaleBricks
provides efﬁcient mechanisms for partitioning the full for-
warding state around a cluster and constructing and updating
its SetSep-based global partitioning table. SetSep requires
only 3.5 bits/key to store a mapping from arbitrary keys to
2-bit values, and provides extremely fast lookups. Scale-
Bricks is an effective technique for practical applications, and
moving to it improved the packet forwarding throughput of a
4-node LTE-to-packet network gateway by 23% and cut its
latency up to 10%.
Acknowledgments
This work was supported in part by NSF grants CNS-
1345305, CNS-1314721, IIS-0964473, CNS-1228598, and
CCF-1320231, and by the Intel Science and Technology Cen-
ter for Cloud Computing.
10. REFERENCES
[1] 3GPP. The Evolved Packet Core.
http://www.3gpp.org/technologies/keywords-
acronyms/100-the-evolved-packet-core.
[2] D. G. Andersen, H. Balakrishnan, N. Feamster, T. Koponen,
D. Moon, and S. Shenker. Accountable Internet Protocol
(AIP). In Proc. ACM SIGCOMM, Seattle, WA, Aug. 2008.
[3] D. Belazzougui, F. Botelho, and M. Dietzfelbinger. Hash,
displace, and compress. In Proceedings of the 17th European
Symposium on Algorithms, ESA ’09, pages 682–693, 2009.
[4] B. H. Bloom. Space/time trade-offs in hash coding with
allowable errors. Communications of the ACM, 13(7):
422–426, 1970.
[5] F. Bonomi, M. Mitzenmacher, and R. Panigrahy. Beyond
Bloom ﬁlters: From approximate membership checks to
approximate state machines. In Proc. ACM SIGCOMM, Pisa,
Italy, Aug. 2006.
[6] S. Bradner and J. McQuaid. Benchmarking Methodology for
Network Interconnect Devices. Internet Engineering Task
Force, Mar. 1999. RFC 2544.
[7] A. Broder, M. Mitzenmacher, and A. Broder. Network
Applications of Bloom Filters: A Survey. In Internet
Mathematics, volume 1, pages 636–646, 2002.
[8] M. Caesar, T. Condie, J. Kannan, K. Lakshimarayanan,
I. Stoica, and S. Shenker. ROFL: Routing on Flat Labels. In
Proc. ACM SIGCOMM, Pisa, Italy, Aug. 2006.
[9] B. Chazelle, J. Kilian, R. Rubinfeld, A. Tal, and O. Boy. The
Bloomier ﬁlter: An efﬁcient data structure for static support
lookup tables. In Proceedings of SODA, pages 30–39, 2004.
[10] Connectem. Connectem Inc. http://www.connectem.net/.
[11] S. Dharmapurikar, P. Krishnamurthy, and D. E. Taylor.
Longest preﬁx matching using Bloom ﬁlters. In Proc. ACM
SIGCOMM, Karlsruhe, Germany, Aug. 2003.
[12] M. Dietzfelbinger, A. Karlin, K. Mehlhorn, F. M. Auf
Der Heide, H. Rohnert, and R. E. Tarjan. Dynamic perfect
hashing: Upper and lower bounds. SIAM Journal on
Computing, 23(4):738–761, 1994.
[13] M. Dobrescu, N. Egi, K. Argyraki, B.-G. Chun, K. Fall,
G. Iannaccone, A. Knies, M. Manesh, and S. Ratnasamy.
RouteBricks: Exploiting parallelism to scale software routers.
In Proc. 22nd ACM Symposium on Operating Systems
Principles (SOSP), Big Sky, MT, Oct. 2009.
[14] B. Fan, D. G. Andersen, and M. Kaminsky. MemC3:
Compact and concurrent memcache with dumber caching and
smarter hashing. In Proc. 10th USENIX NSDI, Lombard, IL,
Apr. 2013.
253[15] B. Fan, D. Zhou, H. Lim, M. Kaminsky, and D. G. Andersen.
[25] H. Lim, D. G. Andersen, and M. Kaminsky. Practical
When cycles are cheap, some tables can be huge. In Proc.
HotOS XIV, Santa Ana Pueblo, NM, May 2013.
[16] M. L. Fredman and J. Komlós. On the size of separating
systems and families of perfect hash functions. SIAM Journal
on Algebraic Discrete Methods, 5(1):61–68, 1984.
[17] M. L. Fredman, J. Komlós, and E. Szemerédi. Storing a sparse
table with O(1) worst case access time. Journal of the ACM
(JACM), 31(3):538–544, 1984.
batch-updatable external hashing with sorting. In Proc.
Meeting on Algorithm Engineering and Experiments
(ALENEX), Jan. 2013.
[26] Y. Lu, B. Prabhakar, and F. Bonomi. Perfect hashing for
network applications. In Information Theory, 2006 IEEE
International Symposium on, pages 2774–2778. IEEE, 2006.
[27] Y. Mao, C. Cutler, and R. Morris. Optimizing ram-latency
dominated applications. APSys ’13, 2013.
[18] W. Haeffner, J. Napper, M. Stiemerling, D. Lopez, and
[28] M. Mitzenmacher and B. Vocking. The asymptotics of
J. Uttaro. Service Function Chaining Use Cases in Mobile
Networks. Internet Engineering Task Force, Jan. 2015.
[19] D. Han, A. Anand, F. Dogar, B. Li, H. Lim, M. Machado,
A. Mukundan, W. Wu, A. Akella, D. G. Andersen, J. W.
Byers, S. Seshan, and P. Steenkiste. XIA: Efﬁcient support for
evolvable internetworking. In Proc. 9th USENIX NSDI, San
Jose, CA, Apr. 2012.
selecting the shortest of two, improved. In Proc. the Annual
Allerton Conference on Communication Control and
Computing, volume 37, pages 326–327, 1999.
[29] M. Mitzenmacher, A. W. Richa, and R. Sitaraman. The power
of two random choices: A survey of techniques and results. In
Handbook of Randomized Computing, pages 255–312.
Kluwer, 2000.
[20] S. Han, K. Jang, K. Park, and S. Moon. PacketShader: a
[30] R. Pagh and F. Rodler. Cuckoo hashing. Journal of
GPU-accelerated software router. In Proc. ACM SIGCOMM,
New Delhi, India, Aug. 2010.
Algorithms, 51(2):122–144, May 2004.
[31] Spirent. Spirent SPT-N11U.
[21] Intel. Intel Data Plane Development Kit (Intel DPDK).
http://www.intel.com/go/dpdk, 2013.
http://www.spirent.com/sitecore/content/Home/
Ethernet_Testing/Platforms/11U_Chassis.
[22] C. Kim, M. Caesar, and J. Rexford. Floodless in SEATTLE: A
[32] L. G. Valiant and G. J. Brebner. Universal schemes for
scalable Ethernet architecture for large enterprises. In Proc.
ACM SIGCOMM, Seattle, WA, Aug. 2008.
[23] A. Kirsch and M. Mitzenmacher. Less hashing, same
performance: Building a better Bloom ﬁlter. Random
Structures & Algorithms, 33(2):187–218, 2008.
[24] H. Lim, B. Fan, D. G. Andersen, and M. Kaminsky. SILT: A
memory-efﬁcient, high-performance key-value store. In Proc.
23rd ACM Symposium on Operating Systems Principles
(SOSP), Cascais, Portugal, Oct. 2011.
parallel communication. In Proceedings of the Thirteenth
Annual ACM Symposium on Theory of Computing, STOC ’81.
[33] M. Yu, A. Fabrikant, and J. Rexford. BUFFALO: Bloom ﬁlter
forwarding architecture for large organizations. In Proc.
CoNEXT, Dec. 2009.
[34] D. Zhou, B. Fan, H. Lim, D. G. Andersen, and M. Kaminsky.
Scalable, High Performance Ethernet Forwarding with
CuckooSwitch. In Proc. 9th International Conference on
emerging Networking EXperiments and Technologies
(CoNEXT), Dec. 2013.
254