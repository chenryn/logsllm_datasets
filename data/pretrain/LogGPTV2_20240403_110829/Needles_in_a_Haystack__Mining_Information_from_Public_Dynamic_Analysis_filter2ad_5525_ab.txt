corporate in our study is the ﬁrst submission time of a
certain ﬁle to the service. We believe that by combining
the timestamps obtained from the VirusTotal and Syman-
tec datasets, we achieved an acceptable approximation of
the ﬁrst time a certain malicious ﬁle was observed in the
wild.
4 Sample Analysis
If a sample survived the data reduction phase, it means
that (with a certain approximation due to the coverage
of Symantec and Virustotal datasets) it had never been
observed in the wild before it was submitted to the on-
line malware analysis sandbox. Although this might be
a good indicator, it is still not sufﬁcient to ﬂag the sub-
mission as part of a potential malware development. In
fact, there could be other possible explanations for this
phenomenon, such as the fact that the binary was just
a new metamorphic variation of an already known mal-
ware family.
Therefore, to reduce the risk of mis-classiﬁcation, in
this paper we consider a candidate for possible develop-
ment only when we can observe at least two samples that
clearly show the changes introduced by the author in the
software. In the rest of this section we describe how we
ﬁnd these groups of samples by clustering similar sub-
missions together based on the sample similarity.
4.1 Sample Clustering
In the last decade, the problem of malware clustering
has been widely studied and various solutions have been
proposed [31, 33, 51, 32]. Existing approaches typi-
cally use behavioral features to group together samples
that likely belong to the same family, even when the
binaries are quite different. Our work does not aim at
proposing a new clustering method for malware. In fact,
our goal is quite different and requires to group ﬁles to-
gether only when they are very similar (we are looking
for small changes between two versions of the same sam-
ple) and not when they just belong to the same family.
Therefore, we leverage a clustering algorithm that simply
groups samples together based on their binary similarity
(as computed by ssdeep [38]) and on a set of features we
extract from the submission metadata.
Moreover, we decided to put together similar binaries
into the same cluster only if they were submitted to our
sandbox in a well deﬁned time window. Again, the as-
sumption is that when a malware author is working on a
new program, the different samples would be submitted
to the online sandbox in a short timeframe. Therefore, to
cluster similar binaries we compute the binary similari-
ties among all the samples submitted in a sliding window
of seven days. We then shift the sliding window ahead
of one day and repeat this step. We employ this sliding
window approach in order (1) to limit the complexity of
the computation and the total number of binary compar-
isons, and (2) to ensure that only the binaries that are
similar and have been submitted within one week from
each other are clustered together. We also experimented
with other window sizes (between 2 and 15 days) but
while we noticed a signiﬁcant reduction of clusters for
shorter thresholds, we did not observed any advantage in
increasing it over one week.
Similarities among binaries are computed using the ss-
deep [38] tool which is designed to detect similarities on
binary data. ssdeep provides a light-weight solution for
comparing a large-number of ﬁles by relying solely on
similarity digests that can be easily stored in a database.
As we already discarded packed binaries in the data re-
duction phase, we are conﬁdent that the similarity score
computed by ssdeep is a very reliable way to group to-
gether binaries that share similar code snippets. After
computing the similarity metrics, we executed a simple
agglomerative clustering algorithm to group the binaries
for which the similarity score is greater than 70%. Note
that this step is executed separately for each time win-
dow, but it preserves transitivity between binaries in dif-
ferent sliding windows. For example, if ﬁle A is similar
to B inside window1, and B is similar to ﬁle C inside the
next sliding window, at the end of the process A, B and C
will be grouped into the same cluster. As a result, a sin-
gle cluster can model a malware development spanning
also several months.
Starting from the initial number of binaries, we identi-
ﬁed 5972 clusters containing an average of 4.5 elements
each.
Inter-Cluster Relationships
The ssdeep algorithm summarizes the similarity using an
index between 0 (completely different) and 100 (perfect
match). Our clustering algorithm groups together sam-
ples for which the difference between the fuzzy hashes
is greater than the 70% threshold. This threshold was
chosen according to previous experiments [38], which
concluded that 70% similarity is enough to guarantee a
probability of misclassiﬁcation close to zero.
However,
if the malware author makes very large
changes on a new version of his program, our approach
may not be able to ﬁnd the association between the two
versions. Moreover, the ﬁnal version of a malware devel-
opment could be compiled with different options, mak-
ing a byte-level similarity too imprecise. To mitigate
these side effects, after the initial clustering step, we per-
form a reﬁnement on its output by adding inter-clusters
edges whenever two samples in the same time window
1060  24th USENIX Security Symposium 
USENIX Association
4
share the same submission origin (i.e., either from the
same IP address or using the same email address for the
registration). These are “weak” connections that do not
model a real similarity between samples, and therefore
they are more prone to false positives. As a consequence,
our system does not use them when performing its auto-
mated analysis to report suspicious clusters. However,
as explained in Section 6, these extra connections can be
very useful during the analysis of a suspicious cluster to
gain a more complete picture of a malware development.
After executing this reﬁnement step, we were able to
link to our clusters an additional 10,811 previously iso-
lated binaries. This procedure also connected several
clusters together, to form 225 macro groups of clusters.
Intra-cluster Analysis
4.2
Once our system had clustered the binaries that likely
belong to the same malware development, we investigate
each cluster to extract more information about its char-
acteristics. In particular, we perform a number of code-
based analysis routines to understand if the samples in
the same cluster share similar code-based features.
Code Normalization
Code normalization is a technique that is widely used to
transform binary code to a canonical form [26]. In our
study, we normalize the assembly code such that the dif-
ferences between two binaries can be determined more
accurately. Under the assumption that two consecutive
variations of the same program are likely compiled with
the same tool chain and the same options, code normal-
ization can be very useful to remove the noise introduced
by small variations between two binaries.
There are several approaches that have been proposed
to normalize assembly code [36, 49, 34]. Some of them
normalize just the operands, some the mnemonics, and
some normalize both.
In this paper, we chose to nor-
malize only the operands so that we can preserve the
semantics of the instructions.
In particular, we imple-
mented a set of IDA Pro plugins to identify all the func-
tions in the code and then replace, for each instruction,
each operand with a corresponding placeholder tag: reg
for registers, mem for memory locations , val for con-
stant values, near for near call offsets, and ref for ref-
erences to memory locations. These IDA scripts were
run in batch mode to pre-process all the samples in our
clusters.
Programming Languages
The second step in our intra-cluster analysis phase con-
sists in trying to identify the programming language used
to develop the samples. The programming language can
provide some hints about the type of development. For
example, scripting languages are often used to develop
tools or probes designed to exﬁltrate information from
the sandbox. Moreover, it is likely that a malware author
would use the same programming language for all the in-
termediate versions of the same malware. Therefore, if a
cluster includes samples of a malware development, all
samples should typically share the same programming
language. Exceptions, as the one explained in Section 6,
may point to interesting cases.
To detect the programming language of a binary we
implemented a simple set of heuristics that incorpo-
rate the information extracted by three tools: PEiD, the
pefile python library, and the Linux strings com-
mand. First, we use pefile to parse the Import Ad-
dress Table (IAT) and obtain the list of libraries that
are linked to the binary. Then, we search for program-
ming language speciﬁc keywords on the extracted list.
For example, the “VB” keyword in the library name is
a good indicator of using Visual Basic, and including
mscoree.dll in the code can be linked to the usage of
Microsoft .NET. In the second step of our analysis, we
analyze the strings and the output of PEiD to detect com-
piler speciﬁc keywords (e.g., type info and RTTI pro-
duced by C++ compilers, or “Delphi” strings generated
by the homonymous language).
With these simple heuristics, we identiﬁed the pro-
gramming language of 14,022 samples. The most rep-
resented languages are Visual Basic (49%), C (21%),
Delphi (18%), Visual Basic .Net (7%), and C++ (3%).
The large number of Visual Basic binaries could be a
consequence of the fact that a large number of available
tools that automatically create generic malware programs
adopt this language.
Fine-grained Sample Similarity
In this last phase, we look in more detail at the similar-
ity among the samples in the same cluster. In particular,
we are interested to know why two binaries show a cer-
tain similarity: Did the author add a new function to the
code? Did she modify a branch condition, or remove a
basic block? Or maybe the code is exactly the same, and
the difference is limited to some data items (such as a
domain name, or a ﬁle path).
To answer these questions, we ﬁrst extract the time-
line of each cluster, i.e., the sequence in which each sam-
ple was submitted to the sandbox in chronological order.
Moving along the timeline, we compare each couple of
samples using a number of static analysis plugins we de-
veloped for IDA Pro.
The analysis starts by computing and comparing the
call graph of the two samples. In this phase we compare
USENIX Association  
24th USENIX Security Symposium  1061
5
the normalized code of each function, to check which
functions of the second binary were present unchanged
in the ﬁrst binary. The output is a list of additional func-
tion that were not present in the original ﬁle, plus a list of
functions that were likely modiﬁed by the author – i.e.,
those function that share the same position in the call
graph but whose code does not perfectly match. How-
ever, at this level of granularity it is hard to say if some-
thing was modiﬁed in the function or if the author just
removed the function and added another with the same
callee.
Therefore, in these cases, we “zoom” into the function
and repeat our analysis, this time comparing their con-
trol ﬂow graphs (CFGs). Using a similar graph-based
approach, this time we look for differences at the basic
block level. If the two CFGs are too different, we con-
clude that the two functions are not one the evolution of
the other. Otherwise, we automatically locate the differ-
ent basic blocks and we generate a similarity measure
that summarize the percentage of basic blocks that are
shared by the two functions.
4.3 Feature Extraction
Based on the analysis described in the previous sections,
our system automatically extracts a set of 48 attributes
that we believe are relevant to study the dynamics of mal-
ware development.
This was done in two phases. First, we enriched each
sample with 25 individual features, divided in six cate-
gories (see the Appendix for a complete list of individual
features). The ﬁrst class includes self-explanatory ﬁle
features (such as its name and size). The Timestamps
features identify when the sample was likely created,
when it was submitted to Anubis Sandbox, and when it
was later observed in the wild. While the creation time of
the binary (extracted from the PE headers) could be man-
ually faked by the author, we observed that this is seldom
the case in practice, in particular when the author submits
a probe or an intermediate version of a program. In fact,
in these cases we often observed samples in which the
compilation time precedes the submission time by only
few minutes.
The third category of features contain the output of the
VirusTotal analysis on the sample, including the set of la-
bels associated by all AntiVirus software and the number
of AVs that ﬂag the sample as malicious. We then collect
a number of features related to the user who submitted
the sample. Since the samples are submitted using a web
browser, we were able to extract information regarding
the browser name and version, the language accepted by
the system (sometime useful to identify the nationality of
the user) and the IP from which the client was connect-
ing from. Two features in this set require more explana-
tion. The email address is an optional ﬁeld that can be
speciﬁed when submitting a sample to the sandbox web
interface. The proxy ﬂag is instead an attempt to identify
if the submitter is using an anonymization service. We
created a list of IP addresses related to these services and
we ﬂagged the submissions in which the IP address of the
submitter appears in the blacklist. In the Binary features
set we record the output of the ﬁne-grained binary anal-
ysis scripts, including the number of sections and func-
tions, the function coverage, and the metadata extracted
by the PE ﬁles. Finally, in the last feature category we
summarize the results of the sandbox behavioral report,
such as the execution time, potential runtime errors, use
of evasion techniques, and a number of boolean ﬂags that
represent which behavior was observed at runtime (e.g.,
HTTP trafﬁc, TCP scans, etc.)
In the second phase of our analysis we extended the
previous features from a single sample to the cluster that
contains it. Table 2 shows the ﬁnal list of aggregated at-
tributes, most of which are obvious extensions of the val-
ues of each sample in the cluster. Some deserve instead a
better explanation. For instance, the cluster shape (A3)
describes how the samples are connected in the cluster:
in a tightly connected group, in a chain in which each
node is only similar to the next one, or in a mixed shape
including a core group and a small tail. The Functions
diff (B13) summarized how many functions have been
modiﬁed in average between one sample and the next
one. Dev time (B25) tells us how far apart in time each
samples were submitted to the sandbox, and Connect
Back (B24) counts how many samples in the cluster open
a TCP connection toward the same /24 subnetwork from
which the sample was submitted. This is a very com-
mon behavior for probes, as well as for testing the data
exﬁltration component of a malicious program.
Finally, some features such as the number of crashes
(C8) and the average VT detection (D4) are not very in-
teresting per se, but they become more relevant when
compared with the number of samples in the cluster. For
example, imagine a cluster containing three very simi-
lar ﬁles. Two of them run without errors, while the third
one crashes. Or two of them are not detected by AV sig-
natures, but one is ﬂagged as malware by most of the
existing antivirus software.
While we are aware of the fact that each feature could
be easily evaded by a motivated attacker, as described in
Section 6 the combinations of all them is usually sufﬁ-
cient to identify a large number of development clusters.
Again, our goal is to show the feasibility of this approach
and draw attention to a new problem, and not to propose
its deﬁnitive solution.
1062  24th USENIX Security Symposium 
USENIX Association
6
A: Cluster Features
A.1 Cluster id
A.2 Num Elements
A.3 Shape
B: Samples Features
B.1-4 Filesize stats
B.5-8 Sections stats
B.9-12 Functions stats
B.13 Functions diff
B.14 Sections diff
B.15 Changes location