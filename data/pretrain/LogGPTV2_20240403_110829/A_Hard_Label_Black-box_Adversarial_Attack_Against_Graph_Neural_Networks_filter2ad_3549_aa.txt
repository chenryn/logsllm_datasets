title:A Hard Label Black-box Adversarial Attack Against Graph Neural Networks
author:Jiaming Mu and
Binghui Wang and
Qi Li and
Kun Sun and
Mingwei Xu and
Zhuotao Liu
A Hard Label Black-box Adversarial Attack Against Graph
Neural Networks
Jiaming Mu1, Binghui Wang2, Qi Li1, Kun Sun3, Mingwei Xu1, Zhuotao Liu1
1Institute for Network Sciences and Cyberspace, Department of Computer Science, and BNRist, Tsinghua University
{mujm19@mails, qli01@, xumw@, zhuotaoliu@}tsinghua.edu.cn, PI:EMAIL, PI:EMAIL
2 Illinois Institute of Technology 3George Mason University
ABSTRACT
Graph Neural Networks (GNNs) have achieved state-of-the-art per-
formance in various graph structure related tasks such as node
classification and graph classification. However, GNNs are vulnera-
ble to adversarial attacks. Existing works mainly focus on attack-
ing GNNs for node classification; nevertheless, the attacks against
GNNs for graph classification have not been well explored.
In this work, we conduct a systematic study on adversarial at-
tacks against GNNs for graph classification via perturbing the graph
structure. In particular, we focus on the most challenging attack,
i.e., hard label black-box attack, where an attacker has no knowl-
edge about the target GNN model and can only obtain predicted
labels through querying the target model. To achieve this goal, we
formulate our attack as an optimization problem, whose objective
is to minimize the number of edges to be perturbed in a graph while
maintaining the high attack success rate. The original optimiza-
tion problem is intractable to solve, and we relax the optimization
problem to be a tractable one, which is solved with theoretical
convergence guarantee. We also design a coarse-grained searching
algorithm and a query-efficient gradient computation algorithm
to decrease the number of queries to the target GNN model. Our
experimental results on three real-world datasets demonstrate that
our attack can effectively attack representative GNNs for graph clas-
sification with less queries and perturbations. We also evaluate the
effectiveness of our attack under two defenses: one is well-designed
adversarial graph detector and the other is that the target GNN
model itself is equipped with a defense to prevent adversarial graph
generation. Our experimental results show that such defenses are
not effective enough, which highlights more advanced defenses.
CCS CONCEPTS
‚Ä¢ Security and privacy; ‚Ä¢ Computing methodologies ‚Üí Ma-
chine learning;
KEYWORDS
Black-box adversarial attack; structural perturbation; graph neural
networks; graph classification
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea
¬© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-8454-4/21/11...$15.00
https://doi.org/10.1145/3460120.3484796
ACM Reference Format:
Jiaming Mu, Binghui Wang, Qi Li, Kun Sun, Mingwei Xu, Zhuotao Liu.
2021. A Hard Label Black-box Adversarial Attack Against Graph Neural
Networks. In Proceedings of the 2021 ACM SIGSAC Conference on Computer
and Communications Security (CCS ‚Äô21), November 15‚Äì19, 2021, Virtual Event,
Republic of Korea. ACM, NewYork, NY, USA, 18 pages. https://doi.org/10.
1145/3460120.3484796
1 INTRODUCTION
Graph neural networks (GNNs) have been widely applied to various
graph structure related tasks, e.g., node classification [25], link
prediction [59], and graph classification [55, 58, 62], and achieved
state-of-the-art performance. For instance, in graph classification,
given a set of graphs and each graph is associated with a label, a
GNN learns the patterns of the graphs by minimizing the cross
entropy between the predicted labels and the true labels of these
graphs [55] and predicts a label for each graph. GNN has been
used to perform graph classification in various applications such as
malware detection [49], brain data analysis [31], superpixel graph
classification [1], and protein pattern classification [43].
While GNNs significantly boost the performance of graph data
processing, existing studies show that GNNs are vulnerable to adver-
sarial attacks [13, 27, 41, 43, 43, 52, 63]. However, almost all the exist-
ing attacks focus on attacking GNNs for node classification, leaving
attacks against GNNs for graph classification largely unexplored,
though graph classification has been widely applied [1, 31, 43, 49].
Specifically, given a well-trained GNN model for graph classifica-
tion and a target graph, an attacker aims to perturb the structure
(e.g., delete existing edges, add new edges, or rewire edges [33])
of the target graph such that the GNN model will make a wrong
prediction for the target graph. Such adversarial attacks could cause
serious security issues. For instance, in malware detection [49], by
intentionally perturbing a malware graph constructed by a certain
malicious program, the malware detector could misclassify the mal-
ware to be benign. Therefore, we highlight that it is vital to explore
the security of GNNs for graph classification under attack.
In this work, we investigate the most challenging and practical
attack, termed hard label and black-box adversarial attack, against
GNNs for graph classification. In this attack, an attacker cannot
obtain any information about the target GNN model and can only
obtain hard labels (i.e., no knowledge of the probabilities associ-
ated with the predicted labels) through querying the GNN model.
In addition, we consider that the attacker performs the attack by
perturbing the graph structure. The attacker‚Äôs goal is then to fool
the target GNN model by utilizing the hard label after querying the
target model and with the minimal graph structural perturbations.
Session 1B: Attacks and Robustness CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea108We formulate the attack as a discrete optimization problem,
which aims to minimize the graph structure perturbations while
maintaining high attack success rates. Note that our attack is harder
than the existing black-box attacks (e.g., [11, 12]) that are continu-
ous optimization problems. It is intractable to solve the formulated
optimization problem due to the following reasons: (i) The objec-
tive function involves the ùêø0 norm, i.e., the number of perturbed
edges in the target graph, and it is hard to be computed. (ii) The
searching space for finding the edge perturbations increases expo-
nentially as the number of nodes in a graph increases. That is, it is
time-consuming and query-expensive to find appropriate initial per-
turbations. To address these challenges, we propose a three-phase
method to construct our attack. First, we convert the intractable
optimization problem to a tractable one via relaxing the ùêø0 norm
to be the ùêø1 norm, where gradient descent can be applied. Second,
we propose a coarse-grained searching algorithm to significantly
reduce the search space and efficiently identify initial perturbations,
i.e., a much smaller number of edges in the target graph to be per-
turbed. Note that this algorithm can effectively exploit the graph
structural information. Third, we propose a query-efficient gradient
computation (QEGC) algorithm to deal with hard labels and adopt
the sign stochastic gradient descent (signSGD) algorithm to solve
the reformulated attack problem. Note that our QEGC algorithm
only needs one query each time to compute the sign of gradients.
We also derive theoretical convergence guarantees of our attack.
We systematically evaluate our attack and compare it with two
baseline attacks on three real-world datasets, i.e., COIL [37, 39],
IMDB [57], and NCI1 [40, 45] from three different fields [35] and
three representative GNN methods. Our experimental results demon-
strate that our attack can effectively generate adversarial graphs
with smaller perturbations and significantly outperforms the base-
line attacks. For example, when assuming the same number of
edges (e.g., 10% of the total edges in a graph) can be perturbed, our
attack can successfully attack around 92% of the testing graphs in
the NCI1 dataset, while the state-of-the-art RL-S2V attack [13] can
only attack around 75% of the testing graphs. Moreover, only 4.33
edges on average are perturbed by our attack, while the random
attack perturbs 10 times more of the edges. Furthermore, to show
the effectiveness of our coarse-grained searching algorithm, we
compare the performance of three different searching strategies.
The results show that coarse-grained searching can significantly
speed up the initial searching procedure, e.g., it can reduce 84.85%
of the searching time on the NCI1 dataset. It can also help find
initial perturbations that can achieve higher attack success rates,
e.g., the success rate is improved by around 50%. We also evaluate
the effectiveness of the proposed query-efficient gradient compu-
tation algorithm. Experimental results show that it decreases the
number of queries dramatically. For instance, on the IMDB dataset,
our attack with query-efficient gradient computation only needs
13.90% of the queries, compared with our attack without it.
We also explore the countermeasures against the adversarial
graphs generated by our attack. Specifically, we propose two differ-
ent defenses against our adversarial attack: one to detect adversarial
graphs and the other to prevent adversarial graph generation. For
the former defense, we train a binary GNN classifier, whose train-
ing dataset consists of both normal graphs and the corresponding
adversarial graphs generated by our attack. Such a classifier aims
to distinguish the structural difference between adversarial graphs
and normal graphs. Then the trained classifier is used to detect
adversarial graphs generated by our attack on the testing graphs.
Our experimental results indicate that such a detector is not effec-
tive enough to detect the adversarial graphs. For example, when
applying the detector on the COIL dataset with 20% of the total
edges are allowed to be perturbed, 47.50% of adversarial graphs can
successfully evade the detector. For the latter one, we equip GNN
methods with a defense strategy, in order to prevent the generation
of adversarial graphs. Specifically, we generalize the low-rank based
defense [14] for node classification to graph classification. The main
idea is that only low-valued singular components of the adjacency
matrix of a graph are affected by the adversarial attacks. Therefore,
we propose to discard low-valued singular components to reduce
the effects caused by attacks. Our experimental results show that
such a defense achieves a clean accuracy-robustness tradeoff. Our
contributions are summarized as follows:
‚Ä¢ To our best knowledge, we develop the first optimization-based
attack against GNNs for graph classification in the hard label and
black-box setting.
‚Ä¢ We formulate our attack as an optimization problem and solve the
problem with convergence guarantee to implement the attack.
‚Ä¢ We design a coarse-grained searching algorithm and query-efficient
‚Ä¢ We propose two different types of defenses against our attack.
‚Ä¢ We systematically evaluate our attack and defenses on real-world
algorithm to significantly reduce the costs of our attack.
datasets to demonstrate the effectiveness of our attack.
2 THREAT MODEL
Attack Goal. We consider adversarial attacks against GNNs for
graph classification. Specifically, given a well-trained GNN model
ùëì for graph classification and a target graph ùê∫ with a label ùë¶0,
an attacker aims to perturb the target graph (e.g., delete existing
edges, add new edges, or rewire edges in the graph) such that the
perturbed target graph (denoted as ùê∫‚Ä≤) is misclassified by the GNN
model ùëì . The attacks can be classified into targeted attacks and
non-targeted attacks. In targeted attacks, an attacker will set a target
label, e.g., ùë¶ùëê, for the target graph ùê∫. Then the attack succeeds, if
the predicted label of the perturbed graph is ùë¶ùëê. In non-targeted
attacks, the attack succeeds as long as the predicted label of the
perturbed graph is different from ùë¶0. In this paper, we focus on
non-targeted attacks and we will also show that our attack can be
applied to targeted attacks in Section 4.2.
Attackers‚Äô Prior Knowledge. We consider the strictest hard label
black-box setting. Specifically, we assume that the attacker can
only query the target GNN model ùëì with an input graph and obtain
only the predicted hard label (instead of a confidence vector that
indicates the probabilities that the graph belongs to each class) for
the graph. All the other knowledge, e.g., training graphs, structures
and parameters of target GNN model, is unavailable to the attacker.
Attacker‚Äôs Capabilities. An attacker can perform an adversarial
attack by perturbing one of three components in a graph: (i) per-
turbing nodes, i.e., adding new nodes or deleting existing nodes;
(ii) perturbing node feature matrix, i.e., modifying nodes‚Äô feature
vectors; and (iii) perturbing edges, i.e., adding new edges, deleting
existing edges or rewiring edges, which ensures the total number
Session 1B: Attacks and Robustness CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea109of edges is unchanged. In this paper, we focus on perturbing edges,
which is practical in real-world scenarios. For example, in a social
network, an attacker can influence the interactions between user
accounts (i.e., modifying the edge status). However, it is hard for the
attacker to close legitimate accounts (i.e., deleting nodes) or to mod-
ify the personal information of legitimate accounts (i.e., modifying
the features). The attacker can also conduct adversarial attacks via
adding new nodes to the target graph, which is called fake node
injection attack. However, its attack performance is significantly
impacted by locations of injected nodes, e.g., an attacker needs
to add more edges if the injected nodes is on the boundary of a
graph, which is however easily detected. What‚Äôs worse, fake node
injection only involves adding edges but it cannot delete edges.
Thus, we focus on more generic cases, i.e., perturbing edges, in
this paper. More specifically, we assume that the attacker can add
new edges and delete existing edges to generate perturbations. To
guarantee unnoticeable perturbations, we set a budget ùëè ‚àà [0, 1]
for perturbing each target graph. That is, perturbed graphs with
a perturbation rate ùëü, i.e., fraction of edges in the target graph is
perturbed, exceeds the budget ùëè are invalid.
As an attacker is often charged according to the number of
queries, e.g., querying the model deployed by machine-learning-as-
a-service platforms, we also assume that an attacker attempts to
reduce the number of queries to save economic costs. In summary,
the attacker aims to guarantee the attack success rate with as few
queries as possible. Note that it is often a trade-off between the
budget and the number of queries. For instance, with a smaller
budget, the attacker needs to query the target GNN model more
times. Our designed three-phase attack (see Section 4) will obtain a