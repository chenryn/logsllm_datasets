done  
. ./test1.sh  
```  
开始生成测试数据    
```  
vi test2.sh  
for ((i=1;i/dev/null 2>&1 &  
done  
. ./test2.sh  
```  
输出插完后将pengding list 合并  
执行vacuum analyze或gin_clean_pending_list即可，参考    
https://www.postgresql.org/docs/9.6/static/functions-admin.html#FUNCTIONS-ADMIN-INDEX  
https://www.postgresql.org/docs/9.6/static/sql-vacuum.html  
https://www.postgresql.org/docs/9.6/static/gin-implementation.html#GIN-FAST-UPDATE  
### 圈人需求 - 性能测试  
对用例3进行压测  
1\. 圈人，10毫秒以内完成。     
比如查找s1包含3, s2包含4的人群  
```
postgres=# begin;
BEGIN
Time: 0.030 ms
postgres=# declare a cursor for select uid from test where s1 @> array[1] and s2 @> array[4];
DECLARE CURSOR
Time: 6.679 ms
postgres=# fetch 100 in a;
    uid    
-----------
  19246842
 118611240
 148504032
 185844649
(4 rows)
Time: 101.041 ms
```
这个人群太少，没有代表性，我们找一个人群多一点的   
```
postgres=# begin;
BEGIN
postgres=# declare a cursor for select uid from test where s1 @> array[1] or s2 @> array[4];
DECLARE CURSOR
Time: 3.484 ms
postgres=# fetch 100 in a;
   uid   
---------
 2911941
 2373506
 .....
   29713
 3353782
 2836804
 1602067
(100 rows)
Time: 3.892 ms
postgres=# fetch 100 in a;
   uid   
---------
  384170
 1332271
 4282941
 ......
 1190946
 4524861
 1110635
(100 rows)
Time: 4.005 ms
```
2\. 分页，前面已经提到了，使用游标。      
3\. 流式返回，前面的例子已经提到了。      
4\. 并行批量返回    
并行批量返回，可以使用plproxy插件，为每个分区指定一个并行，从而实现并行的批量返回。效果能好到什么程度呢？   
比如串行查询，所有的分片表是依次查询的，所以累加的时间比较长，例如，圈出15221个人，耗时113毫秒。       
```
postgres=# explain (analyze,verbose,timing,costs,buffers) select uid from test where s1 @> array[1];
                                                              QUERY PLAN                                                               
---------------------------------------------------------------------------------------------------------------------------------------
 Append  (cost=0.00..233541.24 rows=206876 width=4) (actual time=0.081..108.037 rows=15221 loops=1)
   Buffers: shared hit=60641
   ->  Seq Scan on public.test  (cost=0.00..0.00 rows=1 width=4) (actual time=0.001..0.001 rows=0 loops=1)
         Output: test.uid
         Filter: (test.s1 @> '{1}'::integer[])
   ->  Bitmap Heap Scan on public.test1  (cost=33.71..2901.56 rows=3188 width=4) (actual time=0.078..0.381 rows=242 loops=1)
         Output: test1.uid
         Recheck Cond: (test1.s1 @> '{1}'::integer[])
         Heap Blocks: exact=238
         Buffers: shared hit=243
         ->  Bitmap Index Scan on idx_test1_s1  (cost=0.00..32.91 rows=3188 width=0) (actual time=0.049..0.049 rows=242 loops=1)
               Index Cond: (test1.s1 @> '{1}'::integer[])
               Buffers: shared hit=5
...中间省略62个表
   ->  Bitmap Heap Scan on public.test64  (cost=34.00..2935.31 rows=3225 width=4) (actual time=0.068..0.327 rows=214 loops=1)
         Output: test64.uid
         Recheck Cond: (test64.s1 @> '{1}'::integer[])
         Heap Blocks: exact=211
         Buffers: shared hit=216
         ->  Bitmap Index Scan on idx_test64_s1  (cost=0.00..33.19 rows=3225 width=0) (actual time=0.041..0.041 rows=214 loops=1)
               Index Cond: (test64.s1 @> '{1}'::integer[])
               Buffers: shared hit=5
 Planning time: 2.016 ms
 Execution time: 109.400 ms
(519 rows)
Time: 113.216 ms
```
而并行查询的性能则相当于单个分区的耗时, 约0.几毫秒。   
```
postgres=# explain (analyze,verbose,timing,costs,buffers) select uid from test1 where s1 @> array[1];
                                                        QUERY PLAN                                                         
---------------------------------------------------------------------------------------------------------------------------
 Bitmap Heap Scan on public.test1  (cost=33.71..2901.56 rows=3188 width=4) (actual time=0.085..0.383 rows=242 loops=1)
   Output: uid
   Recheck Cond: (test1.s1 @> '{1}'::integer[])
   Heap Blocks: exact=238
   Buffers: shared hit=243
   ->  Bitmap Index Scan on idx_test1_s1  (cost=0.00..32.91 rows=3188 width=0) (actual time=0.051..0.051 rows=242 loops=1)
         Index Cond: (test1.s1 @> '{1}'::integer[])
         Buffers: shared hit=5
 Planning time: 0.097 ms
 Execution time: 0.423 ms
(10 rows)
Time: 1.011 ms
```
使用并行，可以大幅提升整体的性能。   
参考文档  
[《使用Plproxy设计PostgreSQL分布式数据库》](../201005/20100511_01.md)   
[《A Smart PostgreSQL extension plproxy 2.2 practices》](../201110/20111025_01.md)    
[《PostgreSQL 最佳实践 - 水平分库(基于plproxy)》](../201608/20160824_02.md)   
而如果你需要的是流式返回，则没有必要使用并行。   
### sharding  
当用户数达到几十亿时，我们可以按用户ID进行分片，使用多台主机。     
当然了，如果你的主机空间足够大，CPU核心足够多，可以满足业务的需求的话，完全没有必要使用多台主机。  
如果要使用多台主机，有哪些方法呢? 可以参考如下文章，也很简单，几步完成   
你就把postgres_fdw节点当成MySQL的TDDL或者DRDS就好了，支持跨节点JOIN，条件，排序，聚合 的下推等，用起来和TDDL DRDS一样的爽。   
postgres_fdw是无状态的，仅仅存储结构（分发规则），所以postgres_fdw节点本身也可以非常方便的横向扩展。   
![pic](20161225_01_pic_002.png)  
[《PostgreSQL 9.6 单元化,sharding (based on postgres_fdw) - 内核层支持前传》](../201610/20161004_01.md)    
[《PostgreSQL 9.6 sharding + 单元化 (based on postgres_fdw) 最佳实践 - 通用水平分库场景设计与实践》](../201610/20161005_01.md)     
[《PostgreSQL 9.6 sharding based on FDW & pg_pathman》](../201610/20161027_01.md)    
[《PostgreSQL 9.5+ 高效分区表实现 - pg_pathman》](../201610/20161024_01.md)      
## 基于位置圈人的需求与性能
这个需求直接落入PostgreSQL的怀抱，其实就是基于位置的KNN查询，PostgreSQL可以通过GiST索引来支撑这个需求。     
在数据分片后，PostgreSQL通过归并排序，依旧可以快速的得到结果。     
例如，  
```
postgres=# explain (analyze,verbose,timing,costs,buffers) select * from t order by id limit 10;
                                                              QUERY PLAN                                                               
---------------------------------------------------------------------------------------------------------------------------------------
 Limit  (cost=0.72..1.13 rows=10 width=4) (actual time=0.158..0.165 rows=10 loops=1)
   Output: t.id
   Buffers: shared hit=3 read=4
   ->  Merge Append  (cost=0.72..819.74 rows=20001 width=4) (actual time=0.157..0.162 rows=10 loops=1)
         Sort Key: t.id
         Buffers: shared hit=3 read=4
         ->  Index Only Scan using idx on public.t  (cost=0.12..2.14 rows=1 width=4) (actual time=0.003..0.003 rows=0 loops=1)
               Output: t.id
               Heap Fetches: 0
               Buffers: shared hit=1