title:Internet Outages, the Eyewitness Accounts: Analysis of the Outages
Mailing List
author:Ritwik Banerjee and
Abbas Razaghpanah and
Luis Chiang and
Akassh Mishra and
Vyas Sekar and
Yejin Choi and
Phillipa Gill
Internet Outages, the Eyewitness Accounts:
Analysis of the Outages Mailing List
Ritwik Banerjee1, Abbas Razaghpanah1(B), Luis Chiang1, Akassh Mishra1,
Vyas Sekar2, Yejin Choi3, and Phillipa Gill1
1 Stony Brook University, New York, USA
PI:EMAIL
2 Carnegie Mellon University, Pittsburgh, USA
3 University of Washington, Seattle, USA
Abstract. Understanding network reliability and outages is critical to
the “health” of the Internet infrastructure. Unfortunately, our ability
to analyze Internet outages has been hampered by the lack of access
to public information from key players. In this paper, we leverage a
somewhat unconventional dataset to analyze Internet reliability—the
outages mailing list. The mailing list is an avenue for network opera-
tors to share information and insights about widespread outages. Using
this unique dataset, we perform a ﬁrst-of-its-kind longitudinal analysis
of Internet outages from 2006 to 2013 using text mining and natural lan-
guage processing techniques. We observe several interesting aspects of
Internet outages: a large number of application and mobility issues that
impact users, a rise in content, mobile issues, and discussion of large-scale
DDoS attacks in recent years.
1 Introduction
As an increasing number of critical services rely on the Internet, network outages
can cause signiﬁcant societal and economic impact [10,18]. Indeed, this impor-
tance can be seen when network failures such as cloud computing outages [9],
BGP interceptions [14], and large scale DDoS attacks (e.g., [1,3]) make headlines
in the popular press. By some estimates, data center network outages can lead
to losses of more than $500,000 per incident on average [34], while costs of WAN
failures are more challenging to quantify [8]. Thus, there are a large number of
past and ongoing eﬀorts to detect and mitigate network outages, including work
on novel root cause analysis techniques [24,27], and better network debugging
tools [5,11,20,30,41].
While there are several eﬀorts, as mentioned above, to minimize the impact
of network outages, there is unfortunately a critical dearth of studies that sys-
tematically understand network outages. In part, our understanding of outages
and network reliability is hampered by the reluctance on the part of network
operators to release data due to policy requirements; e.g., even though the FCC
maintains a network outage reports system and mandates that network opera-
tors provide true estimates, the data is conﬁdential given its sensitive nature [2].
c(cid:2) Springer International Publishing Switzerland 2015
J. Mirkovic and Y. Liu (Eds.): PAM 2015, LNCS 8995, pp. 206–219, 2015.
DOI: 10.1007/978-3-319-15509-8 16
Internet Outages, the Eyewitness Accounts
207
Furthermore, providers have natural economic concerns that such studies may
reﬂect poorly on them and thus impact revenues. As such, the few studies that
obtain data from networks are only able to oﬀer insights from a single vantage
point such as an academic WAN [43], data center [22] or backbone ISP [32].
Our work is an attempt to bridge this critical gap in our understanding of
network reliability. For instance, we would like to understand if speciﬁc Internet
service providers (e.g., access vs. tier-1), protocols (e.g., DNS vs. BGP), network
locations (e.g., speciﬁc PoPs or co-location points), or content providers (e.g.,
web hosting services) are more likely to be involved in network outages. Such an
understanding can help network operators and architects focus their resources
on making Internet services more robust. For example, providers who know that
speciﬁc hosting services or protocols are prone to outages can proactively work
around these known hotspots.
Toward this goal, we leverage an underutilized dataset: the outages mailing
list [38] to answer the above types questions. The mailing list serves as a venue
for operators to announce and debug network failures. The outages list tends to
have some bias towards North American network operators self-reporting outages
perceived as ‘high impact’. Despite this bias, the dataset also has attributes that
are lacking, or only met in isolation in other data sets which can help illuminate
diﬀerent facets of network failures:
Semantic context. Posts contain rich semantic information about what
happened during the outage, in contrast to technical data which often requires
starting from low-level measurements and inferring whether an event incurred
real-world impact.
Interdomain coverage. The mailing list provides an overview of network fail-
ures that transcend network boundaries rather than focusing on the point-of-view
and failures experienced by a single network.
Longitudinal view. The outages list has been maintained since 2006 oﬀering
an unprecedented view of Internet reliability issues discussed by operators over
time.
The rich semantic and natural language information contained in the list also
presents a challenge in terms of analyzing the outages mailing list. To address
this challenge, we turn to natural language processing (NLP), text mining, and
machine learning (ML) techniques in order to automatically categorize the posts
and threads in the mailing list. However, naively applying these techniques “out
of the box” does a poor job of identifying useful semantic information (e.g.,
Level 3 would naively be considered two words). Thus, we use a careful synthesis
of domain knowledge and NLP/ML techniques to extract meaningful keywords
to build a classiﬁcation algorithm to categorize content along two dimensions:
(1) type of outage (e.g., attack vs. congestion vs. ﬁber cut) and (2) the type of
entity involved (e.g., cloud provider vs. ISP).
Our analysis reveals the following insights:
User issues dominate. The list is dominated by issues with user-facing com-
ponents such as misconﬁgurations and issues with application servers and mobile
208
R. Banerjee et al.
networks. In terms of entities, networks providing service to users such as access
and mobile networks are also prevalent.
Content and mobile issues are on the rise. Starting in 2009, we see a large
fraction of threads related to application server problems and content provider
networks. These issues tend to relate to common service providers such as Google,
Facebook, Netﬂix. Mobile-centric issues have also increased by 15 % over the past
7 years.
Attacks and censorship are relatively rare. There is less discussion of secu-
rity issues and censorship in the dataset. However, notable incidents like censor-
ship in Syria and large DNS-ampliﬁcation-based DDoS attacks (e.g.,
[35]) did
get the attention of the community with a signiﬁcant increase in posts containing
the keyword DNS spiking in 2012–2013.1
Contributions and Roadmap: This paper makes the following contributions:
(1) Performing an initial analysis of the outages mailing list to understand Inter-
net outages (Sect. 2); (2) A careful application of text mining, NLP, and machine
learning techniques to extract useful semantic information from this dataset
(Sects. 3, 4); (3) Shedding light on the types of outages and the key entities
involved in these outages over time (Sect. 5). Finally, we discuss related work in
Sect. 6 and conclude in Sect. 7.
2 Dataset
In this section, we provide background about the mailing list and our dataset
(Sect. 2.1), and limitations of using the mailing list to analyze network failures
(Sect. 2.2).
2.1 About the Outages Mailing List
The outages mailing list reports outages related to failures of major commu-
nications infrastructure components. It intends to share information so that
network operators and end users can assess and respond to major outages. The
list contains outage reports as well as post-mortem analysis and discussions on
troubleshooting.
We analyze a snapshot of the outages mailing list taken on December 31,
2013 containing threads since its inception in 2006. Our dataset is summarized in
Table 1. It contains over seven years of discussion on the mailing list. This discus-
sion is organized into 2,054 threads, with a total of 6,566 individual posts. Note
that the number of posts is higher than the number of threads and replies com-
bined since it also includes emails that are not part of a thread (e.g.“unsubscribe”
emails). A total of 1,194 individuals (identiﬁed by e-mail addresses) contributed
to the discussions.
1 DNS was used to amplify botnet attacks over this period.
Internet Outages, the Eyewitness Accounts
209
Table 1. Summary of the Outages Mail-
ing List Dataset
First email
Sep 29, 2006
Last email (in dataset) Dec 31, 2013
Number of posts
Number of threads
Number of replies
6,566
2,054
4,163
Number of contributors 1,194
Posts
Threads
y
t
i
v
i
t
c
A
600
500
400
300
200
100
0
2007
2009
2011
2013
Time
Fig. 1. Outages Mailing List Activity per
Quarter.
Activity on the mailing list shows an upwards trend since it was started in
2006. Figure 1 shows quarterly activity on the list in terms of the number of
threads and posts. The amount of activity on the list shows a periodic trends
with less activity in Q4 which includes the holiday season. We also observe a
spike in posts towards the end of 2012 which can be attributed to discussions
arising from Hurricane Sandy.
2.2 Limitations
While the mailing list provides a unique view of failures which had observ-
able impact over the past seven years, it also has some limitations. The data
is biased towards North American operators and Internet providers since many
of the users are US-based system administrators and the forum itself is hosted
in North America. Moreover, we are biased towards incidents which transcend
network boundaries as incidents which remain internal to a network are unlikely
to be posted. Further, the list does not contain technical information about the
underlying root cause, and indeed some posts lack a clear root cause. Finally,
while the list contains failures that impacted users, there is some selection bias
in terms of failures that users report to the list (e.g., the aforementioned North
American bias, and bias towards networks upstream of networks whose opera-
tors are more active in the list). Despite these limitations, the data contained in
the mailing list is valuable because it presents a longitudinal and cross-provider
view of failures that had real world impact on the Internet.
3 Keyword Analysis
In this section, we discuss how we extract keywords from the e-mail postings
(Sect. 3.1) and present preliminary analysis of topics over time (Sect. 3.2).
3.1 Data Preprocessing
The fact that e-mail postings are comprised of natural language text means
that they are rich with semantic information underlying the failure, but also
210
R. Banerjee et al.
presents a challenge in terms of automatically parsing and processing the data.
To address this challenge we employ techniques from text mining and natural
language processing (NLP).
In general, we consider the dataset at the level of
Step 1: Collate threads.
threads. Each thread consists of the set of e-mail messages (posts) in the thread.
For each thread we extract relevant terms and phrases after removing quoted
text (text from previous emails in the thread included in each email) from its
posts.
Step 2: Remove spurious data and stop-words. We ﬁrst discard spurious
data contained in the posts. This included identifying e-mail signatures used by
posters which contributed to terms and phrases unrelated to the content of the
thread. We also extract traceroute measurements which are often contained in
posts at this point. While traceroutes are useful for debugging, it is diﬃcult to
identify the root cause of an incident via automated analysis of the traceroutes,
since the list contains posts on a variety of topics. Thus, we focus on the natural
language content of the messages in this paper. We leverage a list of 572 stop
words (e.g., articles, prepositions and pronouns) obtained from the SMART
information retrieval system [37]. Punctuations are also removed.
The remaining words are lemmatized (the process of grouping together the
diﬀerent inﬂected forms of a word) using the Stanford CoreNLP toolkit [4] so
they can be analyzed as a single item. For example, determining that “walk”,
“walked” and “walking” are all forms of the same verb: “to walk”. Note that the
simple stemming (i.e., walking → walk) does not suﬃce as it cannot diﬀerentiate
the parts of speech based on context: e.g., when the term “meeting” acts as a
verb: “we are meeting tomorrow” vs. a noun “let’s go to the meeting”. Lemmati-
zation, on the other hand, can identify these contextual diﬀerences. Additionally,
we ﬁlter out words with term-frequency inverse document frequency (tf-idf) val-
ues less than 0.122. Low tf-idf indicates that the word is very common throughout
the dataset [36]. The threshold was chosen such that it ﬁltered out the bottom
25 % of terms in terms of tf-idf value.
Step 3: Extract nouns and named entities. To obtain additional information
about terms contained in the e-mail messages, we use the Stanford part-of-speech
tagger [42] and named-entity recognizer [21]. These tools allow us to identify nouns
as well as named entities (e.g., identifying “Los Angeles” as a single entity). This
process, however, is incomplete for domain-speciﬁc entities found in networking-
related e-mails. This problem is particularly acute for organization names (e.g.,
“Level 3”). Instead of retraining the named entity recognition system – a process
that would have required extensive human annotation – we leverage Wikipedia
to improve named entity recognition for networking entities. We use the simple
heuristic that if a term is a capitalized noun, we search for this term or phrase in
Wikipedia. If we identify a page which contains this term as the title, we check that
the page is a subcategory of the “Telecommunications companies” category. If the
page is in this category, we determine that the term is likely the name of a relevant
organization. For multi-word entities such as “Time Warner Cable”, we consider
noun sequences instead of a single term to search for Wikipedia titles.
Internet Outages, the Eyewitness Accounts
211
Fig. 2. Keyword trends over the years in the outages mailing list.
3.2 Keyword Trends
As a ﬁrst step, we consider keyword trends to understand failures discussed in
the list (Fig. 2). We focus on keywords in four categories: content providers, ISPs,
protocols, and security. For each category we select 5–6 potentially interesting
keywords. Among content providers, Google being the most popular, is more
heavily discussed than others. In terms of ISPs, AT&T, Verizon and Level-3 are
the most frequently discussed, with an upward trend in ISP-related discussion
over time. In terms of protocols, BGP and DNS dominate, with DNS experi-
encing a sharp uptick in discussions in 2012–2013. Our analysis based on binary
classiﬁers (explained in Sect. 4) shows that this is due to a more than twofold
increase of DNS-related issues among access (from 3.3 % in 2011 to 7.0 % in
2012) and content providers (0.9 % in 2011 to 2.2 % in 2012). Finally, we observe
DDoS as the most prevalent term related to security. It comprises nearly 8 % of
posts in 2006 (note that we only have two months of data in 2006) and surges
again to 5.5 % in 2012 as a result of large DDoS attacks which occurred that
year (e.g.,
[35]).
4 Classiﬁcation Methodology
The terms and phrases extracted in our initial processing give a high-level view
of the discussions on the mailing list. In this section, we discuss a classiﬁcation
methodology to help us systematically categorize the outages over time.
Conceptually, we can categorize a network outage along two orthogonal
dimensions: (1) type of the outage (e.g., ﬁber cut), and (2) entities involved in the
outage (e.g., access ISPs). Table 2 summarizes the speciﬁc categories of types and
entities of interest.2 Thus, our goal is to automatically characterize each outage
e-mail thread into categories along these dimensions. Next, we describe how we
designed such a classiﬁer.
Labeling: As a ﬁrst step toward automatic classiﬁcation, we created a simple
website to enable us and our collaborators to manually label a small random
2 We do not claim that this list is exhaustive; it represents a pragmatic set we chose
based on a combination of domain knowledge and manually inspecting a sample of
the dataset.
212
R. Banerjee et al.
Table 2. Summary of categories
sample of the posts along the above two dimensions. We had 5 volunteers, each
labeling around 30 threads. To validate that our manual annotations were con-
sistent, we use the Fleiss’ κ metric [29]; the κ value was 0.75 for entities and 0.5
for the outage types. To put this in perspective, 0.748 is considered very good
and 0.48 is considered a “moderate agreement” [29]. Given this conﬁdence, we
use these manual labels to bootstrap our learning process described below.
Choice of algorithm: Our initial intuition was to formulate this as a semi-
supervised clustering problem [6,17,46]. That is, we use the labeled data to
bootstrap the clustering process, learn features of the identiﬁed clusters, and
then iteratively reﬁne the clusters. However, we found that the training error
was quite high (i.e., low F-score on the labeled set). The primary reason for
this is the well-known class imbalance problem — most real-world datasets are
skewed with a small number of classes contributing the most “probability mass”.
The small number of training samples meant this problem was especially serious
in our context.
Given this insight, we reformulated the semi-supervised clustering as a classi-
ﬁcation problem. While classiﬁcation by itself is not immune to class imbalances,
it can be made robust using two well-known ideas: (1) learning multiple binary
classiﬁers and (2) suitable resampling [23,28,44]. For (1), instead of partitioning
the dataset into N categories, we learn a “concept” for each category indepen-
dently; i.e., a binary classiﬁer trying to determine whether a thread belongs in
a particular category or not. For (2), we setup the training with undersampling
the majority class and/or oversampling the minority class to make the training
data more balanced.
We chose a linear-kernel SVM for classiﬁcation using the LibLINEAR toolkit
[19] which performed well in terms of both accuracy and speed. We evaluate the
goodness of the learning step using a standard leave-one-out cross-validation