# Title: Internet Outages, the Eyewitness Accounts: Analysis of the Outages Mailing List

## Authors
Ritwik Banerjee<sup>1</sup>, Abbas Razaghpanah<sup>1, B</sup>, Luis Chiang<sup>1</sup>, Akassh Mishra<sup>1</sup>, Vyas Sekar<sup>2</sup>, Yejin Choi<sup>3</sup>, and Phillipa Gill<sup>1</sup>

- <sup>1</sup> Stony Brook University, New York, USA
- <sup>2</sup> Carnegie Mellon University, Pittsburgh, USA
- <sup>3</sup> University of Washington, Seattle, USA

**Abstract**
Understanding network reliability and outages is critical to the health of the Internet infrastructure. Unfortunately, our ability to analyze Internet outages has been hampered by the lack of access to public information from key players. In this paper, we leverage an unconventional dataset—the outages mailing list—to analyze Internet reliability. This mailing list serves as a platform for network operators to share information and insights about widespread outages. Using this unique dataset, we perform a first-of-its-kind longitudinal analysis of Internet outages from 2006 to 2013, employing text mining and natural language processing techniques. Our analysis reveals several interesting aspects of Internet outages, including a large number of application and mobility issues, a rise in content and mobile issues, and discussions of large-scale DDoS attacks in recent years.

## 1. Introduction
As an increasing number of critical services rely on the Internet, network outages can cause significant societal and economic impacts [10,18]. For instance, cloud computing outages [9], BGP interceptions [14], and large-scale DDoS attacks (e.g., [1,3]) often make headlines. Data center network outages can lead to losses exceeding $500,000 per incident on average [34], while the costs of WAN failures are more challenging to quantify [8]. Consequently, there have been numerous efforts to detect and mitigate network outages, including work on novel root cause analysis techniques [24,27] and better network debugging tools [5,11,20,30,41].

Despite these efforts, there is a critical lack of studies that systematically understand network outages. This is partly due to the reluctance of network operators to release data, often due to policy requirements. For example, although the FCC maintains a network outage reports system and mandates true estimates from network operators, the data remains confidential [2]. Additionally, providers have economic concerns that such studies may reflect poorly on them and impact revenues. As a result, studies that obtain data from networks typically offer insights from a single vantage point, such as an academic WAN [43], data center [22], or backbone ISP [32].

Our work aims to bridge this gap in understanding network reliability. We seek to determine if specific Internet service providers (e.g., access vs. tier-1), protocols (e.g., DNS vs. BGP), network locations (e.g., specific PoPs or co-location points), or content providers (e.g., web hosting services) are more likely to be involved in network outages. Such an understanding can help network operators and architects focus their resources on making Internet services more robust. For example, providers who know that specific hosting services or protocols are prone to outages can proactively address these known hotspots.

To achieve this, we leverage the outages mailing list [38], which is underutilized but provides a valuable venue for operators to announce and debug network failures. Despite some bias towards North American network operators, the dataset offers unique attributes:
- **Semantic context**: Posts contain rich semantic information about outages, in contrast to technical data.
- **Interdomain coverage**: The mailing list provides an overview of network failures that transcend network boundaries.
- **Longitudinal view**: The list has been maintained since 2006, offering an unprecedented view of Internet reliability issues over time.

The rich semantic and natural language information in the list presents a challenge in terms of analysis. To address this, we use natural language processing (NLP), text mining, and machine learning (ML) techniques. However, naive application of these techniques is insufficient. Therefore, we use a careful synthesis of domain knowledge and NLP/ML techniques to extract meaningful keywords and build a classification algorithm to categorize content along two dimensions: (1) type of outage (e.g., attack vs. congestion vs. fiber cut) and (2) the type of entity involved (e.g., cloud provider vs. ISP).

Our analysis reveals the following insights:
- **User issues dominate**: The list is dominated by issues with user-facing components such as misconfigurations and problems with application servers and mobile networks.
- **Content and mobile issues are on the rise**: Starting in 2009, we see a large fraction of threads related to application server problems and content provider networks, with a 15% increase in mobile-centric issues over the past seven years.
- **Attacks and censorship are relatively rare**: There is less discussion of security issues and censorship, though notable incidents like censorship in Syria and large DNS-amplification-based DDoS attacks did receive attention.

**Contributions and Roadmap**
This paper makes the following contributions:
1. An initial analysis of the outages mailing list to understand Internet outages (Section 2).
2. A careful application of text mining, NLP, and ML techniques to extract useful semantic information from the dataset (Sections 3 and 4).
3. Insights into the types of outages and the key entities involved over time (Section 5).
Finally, we discuss related work in Section 6 and conclude in Section 7.

## 2. Dataset
In this section, we provide background about the mailing list and our dataset (Section 2.1), and limitations of using the mailing list to analyze network failures (Section 2.2).

### 2.1 About the Outages Mailing List
The outages mailing list reports outages related to failures of major communications infrastructure components. It aims to share information so that network operators and end users can assess and respond to major outages. The list contains outage reports, post-mortem analyses, and discussions on troubleshooting.

We analyze a snapshot of the outages mailing list taken on December 31, 2013, containing threads since its inception in 2006. Our dataset is summarized in Table 1. It includes over seven years of discussion, organized into 2,054 threads with a total of 6,566 individual posts. A total of 1,194 individuals (identified by email addresses) contributed to the discussions.

| **Summary of the Outages Mailing List Dataset** |
| --- |
| **First email** | Sep 29, 2006 |
| **Last email (in dataset)** | Dec 31, 2013 |
| **Number of posts** | 6,566 |
| **Number of threads** | 2,054 |
| **Number of replies** | 4,163 |
| **Number of contributors** | 1,194 |

**Figure 1. Outages Mailing List Activity per Quarter.**

Activity on the mailing list shows an upward trend since it was started in 2006. Figure 1 illustrates quarterly activity on the list in terms of the number of threads and posts. The amount of activity shows a periodic trend with less activity in Q4, which includes the holiday season. We also observe a spike in posts towards the end of 2012, attributed to discussions arising from Hurricane Sandy.

### 2.2 Limitations
While the mailing list provides a unique view of failures with observable impact over the past seven years, it has some limitations. The data is biased towards North American operators and Internet providers, as many users are US-based system administrators, and the forum itself is hosted in North America. Moreover, the list is biased towards incidents that transcend network boundaries, as internal network issues are unlikely to be posted. The list does not contain technical information about the underlying root cause, and some posts lack a clear root cause. Finally, while the list contains failures that impacted users, there is some selection bias in terms of failures reported (e.g., North American bias and bias towards networks upstream of active list participants). Despite these limitations, the data in the mailing list is valuable because it presents a longitudinal and cross-provider view of failures with real-world impact on the Internet.

## 3. Keyword Analysis
In this section, we discuss how we extract keywords from the email postings (Section 3.1) and present preliminary analysis of topics over time (Section 3.2).

### 3.1 Data Preprocessing
Email postings are rich with semantic information, but they also present a challenge in terms of automatic parsing and processing. To address this, we employ techniques from text mining and natural language processing (NLP).

**Step 1: Collate threads.**
Each thread consists of a set of email messages (posts). For each thread, we extract relevant terms and phrases after removing quoted text from its posts.

**Step 2: Remove spurious data and stop-words.**
We discard spurious data, such as email signatures, and extract traceroute measurements. While traceroutes are useful for debugging, automated analysis of traceroutes is difficult. We focus on the natural language content of the messages. We use a list of 572 stop words from the SMART information retrieval system [37] and remove punctuation. The remaining words are lemmatized using the Stanford CoreNLP toolkit [4] to group different inflected forms of a word. We filter out words with term-frequency inverse document frequency (tf-idf) values less than 0.122, indicating common words throughout the dataset.

**Step 3: Extract nouns and named entities.**
To obtain additional information about terms in the emails, we use the Stanford part-of-speech tagger [42] and named-entity recognizer [21]. These tools identify nouns and named entities. For domain-specific entities, we use a simple heuristic: if a term is a capitalized noun, we search for it in Wikipedia. If the page is a subcategory of "Telecommunications companies," we determine the term is likely the name of a relevant organization.

**Figure 2. Keyword trends over the years in the outages mailing list.**

### 3.2 Keyword Trends
We consider keyword trends to understand failures discussed in the list (Figure 2). We focus on keywords in four categories: content providers, ISPs, protocols, and security. Among content providers, Google is the most frequently discussed. In terms of ISPs, AT&T, Verizon, and Level-3 are the most frequently mentioned, with an upward trend in ISP-related discussions over time. In terms of protocols, BGP and DNS dominate, with DNS experiencing a sharp uptick in discussions in 2012–2013. Our analysis based on binary classifiers (explained in Section 4) shows that this is due to a more than twofold increase in DNS-related issues among access (from 3.3% in 2011 to 7.0% in 2012) and content providers (0.9% in 2011 to 2.2% in 2012). Finally, DDoS is the most prevalent term related to security, comprising nearly 8% of posts in 2006 and surging to 5.5% in 2012 due to large DDoS attacks.

## 4. Classification Methodology
The terms and phrases extracted in our initial processing give a high-level view of the discussions on the mailing list. In this section, we discuss a classification methodology to systematically categorize the outages over time.

Conceptually, we can categorize a network outage along two orthogonal dimensions: (1) type of the outage (e.g., fiber cut) and (2) entities involved in the outage (e.g., access ISPs). Table 2 summarizes the specific categories of interest. Our goal is to automatically characterize each outage email thread into categories along these dimensions.

### 4.1 Labeling
As a first step toward automatic classification, we created a simple website to enable manual labeling of a small random sample of the posts along the two dimensions. Five volunteers, each labeling around 30 threads, validated the consistency of our manual annotations using the Fleiss' κ metric [29]. The κ value was 0.75 for entities and 0.5 for outage types, indicating very good and moderate agreement, respectively. Given this confidence, we use these manual labels to bootstrap our learning process.

### 4.2 Choice of Algorithm
Our initial intuition was to formulate this as a semi-supervised clustering problem [6,17,46]. However, we found that the training error was quite high due to the class imbalance problem. Real-world datasets are often skewed, with a small number of classes contributing the most "probability mass." The small number of training samples made this problem especially serious in our context.

Given this insight, we reformulated the semi-supervised clustering as a classification problem. While classification is not immune to class imbalances, it can be made robust using two well-known ideas: (1) learning multiple binary classifiers and (2) suitable resampling [23,28,44]. For (1), instead of partitioning the dataset into N categories, we learn a "concept" for each category independently; i.e., a binary classifier determines whether a thread belongs in a particular category or not. For (2), we set up the training with undersampling the majority class and/or oversampling the minority class to balance the training data.

We chose a linear-kernel SVM for classification using the LibLINEAR toolkit [19], which performed well in terms of both accuracy and speed. We evaluate the goodness of the learning step using standard leave-one-out cross-validation.