title:ROAR: increasing the flexibility and performance of distributed
search
author:Costin Raiciu and
Felipe Huici and
Mark Handley and
David S. Rosenblum
ROAR: Increasing the Flexibility and Performance of
Distributed Search
Costin Raiciu
University College London
PI:EMAIL
Felipe Huici
NEC Europe, Heidelberg
PI:EMAIL
David S. Rosenblum
University College London
PI:EMAIL
Mark Handley
University College London
PI:EMAIL
ABSTRACT
To search the web quickly, search engines partition the web index
over many machines, and consult every partition when answering a
query. To increase throughput, replicas are added for each of these
machines. The key parameter of these algorithms is the trade-off
between replication and partitioning:
increasing the partitioning
level improves query completion time since more servers handle
the query, but may incur non-negligible startup costs for each sub-
query. Finding the right operating point and adapting to it can sig-
niﬁcantly improve performance and reduce costs.
We introduce Rendezvous On a Ring (ROAR), a novel distributed
algorithm that enables on-the-ﬂy re-conﬁguration of the partition-
ing level. ROAR can add and remove servers without stopping the
system, cope with server failures, and provide good load-balancing
even with a heterogeneous server pool. We demonstrate these claims
using a privacy-preserving search application built upon ROAR.
Categories and Subject Descriptors
C.2.4 [Computer-Communication Nets]: Distributed Systems
General Terms
Algorithms, Design
1.
INTRODUCTION
Search, possibly the web’s most important application, is im-
plemented as a distributed computation over a large inverted Web
index. In order to improve the performance of queries, this index
is partitioned into many parts, and each part is replicated on a clus-
ter of commodity PCs. When a query is executed, it is sent to one
machine in each cluster so that the whole index is covered, and the
results aggregated [5].
From a distributed algorithms point of view, which cluster each
data item is stored on and which machines each query is sent to are
independent of the actual content of the data and queries. Indeed,
the algorithm is blind to this content: it is sufﬁcient to ensure that
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’09, August 17–21, 2009, Barcelona, Spain.
Copyright 2009 ACM 978-1-60558-594-9/09/08 ...$10.00.
Figure 1: Basic Distributed Rendezvous
each query reaches machines that between them hold all the data.
We call this class of algorithms distributed rendezvous.
Such algorithms contrast with other more constrained look-up
algorithms such as Distributed Hash Tables (DHTs), where a query
is sent to precisely the node that can answer the request. To some
extent, distributed rendezvous can be thought of as brute-force dis-
tributed matching. However inelegant this may seem, many real-
world problems fall into this category, including web search.
Successful web search engines such as Google use parallel index-
search algorithms [5], which are a form of distributed rendezvous.
The datasets involved can be many terabytes in size [5], can change
rapidly (consider Google News, updated continuously as news hap-
pens), and can have very high query rates. Only by spreading the
search across large numbers of servers can query latency be kept
low while achieving high overall throughput.
Figure 1 illustrates the basic concept. The servers are divided
into clusters and each data item to be searched is replicated on all
the machines in a single cluster. With this in place, a query is then
sent to one machine from each cluster, thus ensuring that the query
is matched against the full index. Each data entry is only matched
against the query on a single machine, allowing arbitrarily com-
plex matching rules to be performed locally. Having performed the
search, each machine ranks the matches and returns the best ones.
Finally, the results from all the query machines are merged, ranked
once again, and returned to the user.
Given this basic strategy, the obvious question is how many nodes
should be in each cluster? Each query must be sent to one node
from each cluster, so increasing the number of clusters means split-
ting the search index into more pieces. This involves more nodes
in each search, reducing the search completion time.
Although low query times are desirable, running a query on a
node also has a ﬁxed cost, as the query must be communicated to
the node and a search thread instantiated there. These costs are
291not negligible: if we took the extreme position of having only one
node per cluster, then every node would have to try to process every
query. Even though the search performed on each node would be
cheap, the overall throughput would be very low.
In essence, the problem is one of balancing search latency, which
beneﬁts from a larger number of clusters, with total throughput for
all nodes, which has a preference for a smaller number of clusters.
A sensible strategy would be to choose the smallest number of clus-
ters that satisﬁes a latency target, such as answering all queries in
under a second. Once this target is satisﬁed, splitting into more
clusters would only decrease peak throughput.
Of course, for a static data set and a constant query rate there
is no great problem ﬁguring out the number of clusters needed to
satisfy a target latency, and from there to calculate the number of
machines in each cluster needed to satisfy the overall throughput.
However, neither the data set nor the query rate remain constant for
most real applications, and the total number of machines cannot
normally be changed on short timescales.
Consider again Google’s search engine: over time the size of the
web increases, so the size of Google’s index grows. While ma-
chines can easily be added to existing clusters in order to maintain
throughput, keeping search latency constant requires repartitioning
the servers into more clusters.
Google does this by removing machines from an existing cluster
and adding them to a new cluster conﬁguration during a low trafﬁc
period [9]. Once this completes, the front-end load balancers start
using the updated machines to answer a fraction of queries. The
next batch of machines are then removed, repartitioned into the
new clusters and updated, and so on. This works but is inﬂexible:
repartitioning needs to be a rare event, and it cannot be performed
in response to a load spike because it must be done at a quiet time.
In this we paper examine the question of how to change the par-
titioning of a running distributed rendezvous system. We propose
a novel algorithm for distributing data and queries between servers
that balances load well, and is much more amenable to on-the-ﬂy
changes to partitioning, even under conditions of heavy load. This
additional ﬂexibility can be used to cope with ﬂash crowds, to man-
age data sets that change even more rapidly than Google’s, and may
even be used to adaptively control the total work done in such a
data center so as to reduce overall demand for electrical power, an
important concern for data centers these days.
2. THE NATURE OF THE PROBLEM
Let us parameterize the problem:
• Each data item is replicated and stored on r servers.
• Each query is run in parallel on p servers (we say the query
has been partitioned and that p is the partitioning level).
The aim is to perform data replication and query partitioning
such that every query meets every data item. If all data items have
the same number of replicas and all queries are sent to the same
number of servers, it is trivial to see from Figure 1 that with n
servers it must be the case that:
This characterizes the basic tradeoff in distributed rendezvous: as
p increases to improve latency, r generally decreases, so a node
stores less data but must handle more queries.
In reality the situation is not quite so simple, and so this provides
a lower bound. If load balancing is not perfect, or if nodes fail, or
just to add resilience, larger values of r may be used. Hence:
p · r = n
p · r ≥ n
(1)
(2)
Note that on each server, a local index (such as an inverted index)
may be created based on the items (documents) assigned to that
server. This index will be used to perform fast local matching.
However, the latency of a match on each node will still grow with
the number of documents indexed, only more slowly. Further, this
does not affect the nature of the replication process across servers.
2.1 Constraints
If we double p, the total cost across all servers of matching a
single query remains unchanged, but twice the number of servers
do half the amount of work each. Normally this will reduce query
delay. However, there are additional constraints that inﬂuence the
tradeoff between p and r; these are the focus of this paper:
• The processing resources of each node are bounded. Dou-
bling p also means each node must handle double the num-
ber of queries. Each additional query requires setting up a
search thread, network bandwidth to communicate the query,
and imposes extra context switching overhead. For an overall
system running at high utilization, p cannot increase indeﬁ-
nitely; beyond some point nodes will saturate.
• The long-term storage (memory and/or disk space depending
on the application) on each node is bounded. Thus, there is
also an upper bound on r, above which the nodes cannot store
their fraction of the data items.
• For a dataset that changes rapidly, increasing r means more
changes must be sent to each node. This extra work reduces
the capacity of each node to handle queries.
Thus, p cannot be too large lest nodes’ CPUs saturate, and it
cannot be too small or nodes’ storage will saturate. Generally we
want to choose p to be large enough to satisfy latency bounds that
are determined by usability factors, but choosing a larger p than this
will increase processing costs, requiring more machines to handle
peak workloads. For non-peak workloads one might assume that
using a larger p than necessary would not be a problem, but modern
servers require signiﬁcantly more energy when they are working
hard (which is the case when p is increased, due to the additional
per-request ﬁxed costs incurred); for companies such as Google
and Microsoft that run huge numbers of servers, minimizing power
consumption is an important goal.
In addition to these constraints, query rates vary over time due to
daily and weekly cycles as well as ﬂash crowds. This leads to the
question of whether it is feasible to change p relatively frequently.
Indeed, it may also be possible to shut down or sleep nodes to save
power at quiet times, and thus change r without changing p. In the
next section we will examine these questions in some detail; the
result will be a design for a new distributed rendezvous system that
makes such changes possible at acceptable cost.
2.2 Dynamic Repartitioning
The repartitioning strategy described in Section 1, whereby dur-
ing a quiet time servers are taken ofﬂine to be moved into new
clusters, has several problems.
First, reducing electricity use requires running fewer servers at
relatively high utilization levels rather than more servers at lower
utilization.1 Thus, reducing the capacity of the network to repar-
tition is difﬁcult while sustaining the required query throughput.
Either the workload must have predictable quiet periods lasting for
signiﬁcant periods of time, or spare machines must be maintained
1A server requires roughly half the power when idling as when fully loaded, with the
change in power between idle and loaded being fairly linear with CPU utilization
292(a) Storing data items
(b) Executing a query
Figure 2: A simple sliding window algorithm with p=4, r=3, and n=12.
Figure 3: ROAR with n=12, p=4 and r=3. Objects
are stored in arcs of length 1/p and queries sent to
p servers at 1/p intervals.
and powered up during repartitioning, increasing additional infras-
tructure and energy costs.
Second, this strategy incurs signiﬁcant data transfer costs while
repartitioning. Each server will dump its current documents and
reload its new part of the index. In effect, the index as a whole
is copied r times, which unnecessarily wastes bandwidth and cre-
ates signiﬁcant stress on the backing ﬁlesystem as servers down-
load their new index. How big is this waste? Google reports p to be
around 1,000 [3]; search is done in more than 40 data centers[18]
distributed globally. In each data center the replication level is low
(1-3) [10]. Let’s approximate r to be 80.
The index is reportedly a few terabytes in size (for the sake of
argument, let us assume it is 10TB), and thus the whole network
needs to transfer around 800TB in order to repartition.
Finally, the time to repartition may be signiﬁcant. An unpre-
dicted trafﬁc spike during repartitioning may cause an overload.
Google can avoid this by repartitioning only one data center at a
time and moving trafﬁc away from that data center using DNS load
balancing, but not all organizations can do so.
In both solutions above, distributed coordination is needed to de-
cide which servers should be migrated, when, and to which cluster.
Coordination is required to decide when to switch to the new con-
ﬁguration and when to stop the old conﬁguration. This makes the
whole process difﬁcult to automate, cumbersome and lengthy.
All of these problems stem from the simplicity of the algorithm.
Simple partitioning seems good enough in cases when r and p
rarely change. On the other hand, the ability to cheaply and fre-
quently repartition on the ﬂy can allow a great deal of ﬂexibility,
allowing adaptation to changing operating conditions, be they due
to spikes in load, changes in the data set, or equipment failure. To
achieve this level of ﬂexibility at reasonable cost we need to move
away from simple partitioning strategies and examine algorithms
that do not require the overlay structure to change. In the next sec-
tion we introduce ROAR, a novel distributed rendezvous algorithm
that meets these goals.
3. TOWARDS A SOLUTION
Our key observation is that there is no need to divide the nodes
into disjoint clusters: what is important is that each data item is
replicated on r nodes, and that we can arrange for every query to
visit at least one of these nodes. There are random-walk algorithms
that can do this [25], but they require p · r (cid:3) n, which is usually
unacceptable. Can a deterministic algorithm do better?
The simplest solution is probably a sliding window algorithm,
where the n nodes are arranged in a circle. The ﬁrst data item is
then stored on nodes 1...r, the second is stored on nodes 2...(r+1),
and the kth on nodes k...(r + k), with all arithmetic performed
modulo n. Now if a query visits every rth node it is guaranteed to
reach every data item, as shown in Figure 2. Such an algorithm has
some very nice properties:
• Each node stores the same number of items, and if a round-
robin algorithm is used to start queries, each node handles the
same number of queries (assuming r divides n precisely). In
this sense it is identical to the basic partitioning scheme.
• Increasing r by one merely requires replicating each data
item onto the successor node on the ring.
• Decreasing r by one merely requires deleting each data item
from the node that stores it that has the greatest ID.
Thus each node plays an equal role when changing r (and con-
sequently p). When decreasing r, no additional data needs to be
copied. When increasing r by one, each node needs to copy 1/nth
of the data. During the transition, search continues to function. If
r is decreasing, searches must use the new value of p during the
transition to ensure correctness. If r is increasing, searches must
use the old value of p until the transition is complete.
Despite these nice properties, such an algorithm has some short-
comings. First, while it works very well with a ﬁxed number of
reliable nodes, it does less well if a node fails. In this case, all the
objects stored on the failed node need to be replicated once more,
as they’ve just lost one replica. These replicas need to be added
by the r successors of the failed node; this implies that each node
needs to monitor the health of its r predecessors; for large values
of r, the costs can be signiﬁcant. Finally, until the new replicas are
added, query execution could miss some objects.
The basic problem with this simple sliding window algorithm
stems from the fact that the nodes have a discrete position on the
ring. Data is then replicated across consecutive nodes holding a
range of these discrete positions. If the list of nodes changes (nodes
are added, shutdown to save power, or fail), this impacts the relative
positions of nodes, and so has non-local consequences.
Beyond this, another problem is that all nodes are treated equally—
also a result of the discrete nature of the node positions on the ring.
In practice, it is rare that all nodes in a data center are of identical
performance, as equipment tends to be purchased over time. An ex-
plicit goal is to be able to effectively utilize heterogeneous servers
according to their capabilities.
2934. ROAR: RENDEZVOUS ON A RING
The problems above led us to develop a new continuous version
of the sliding window algorithm that we call Rendezvous On A
Ring (ROAR). Rather than simply arranging servers in a circular
list, ROAR uses a continuous circular ID space [0, 1]. Each server
is given a continuous range of this ID space that it is responsible
for, such that all points on the ring are owned by some server. Thus
ROAR uses the ring in a similar way to Chord [20], although that
is where the similarity ends.
The basic idea is that given a partitioning level p, ROAR stores
each object on the servers whose range intersects an arc of size 1/p
on the ring (Figure 3); for searching, ROAR randomly chooses a
starting point on the ring and forwards each query to p equally-
spaced points around the ring. Whereas the basic sliding window
algorithm stores a data item on exactly r consecutive nodes, ROAR
stores on an arc of the ring in which, on average, there are r servers.
This allows us to decouple query routing from the server identiﬁers.
We now look at these mechanisms in greater detail.
4.1 Storing objects
Each data item is assigned a uniformly random identiﬁer in [0, 1].
The data item now needs to be replicated on all the servers that are
responsible for the ring segment of length 1/p that starts with the
data item’s ID. How this replication is actually done is independent
of the basic functioning of ROAR. Possible strategies include:
• Push the data item to the ﬁrst server, and then forward it from
server to server around the ring.
• Have all the servers mount a shared ﬁlesystem such as GFS [14].
Servers periodically check the ﬁlesystem for ﬁles with IDs
that should be stored in their range.
• Push the data item to all the relevant ring servers from a back-
end update server that knows the ring topology.
A peer-to-peer solution using ROAR might use the ﬁrst, whereas
organizations with existing distributed ﬁlesystems might choose the
second. Our implementation does the last of these, using a central
coordinator to keep track of the ID ranges occupied by the servers.
4.2 Forwarding Queries
To perform a search, a query from a client is ﬁrst sent to a front-
end server. These front-end servers are responsible for partitioning
the query and sending the sub-queries to p nodes on the ring. In
our implementation, every front-end server is kept updated with
the ranges of IDs on the ring for which each node is responsible.
The front-end server then picks a random ID q on the ring for this
query, and sends sub-queries in parallel to the node responsible for
ID q and the nodes responsible for IDs q + 1/p, q + 2/p, . . . , q +
(p − 1)/p, modulo 1. As these IDs are 1/p apart on the ring and