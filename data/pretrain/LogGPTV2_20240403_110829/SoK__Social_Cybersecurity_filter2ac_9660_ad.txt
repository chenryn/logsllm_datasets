over these audiences is diminished. Wiest et al. [69] found
that users’ self-reported sense of closeness to their friends
(i.e., strength of ties) is the best predictor of how likely they
were to share various personal data. And, scenarios that trigger
willingness to share often include an exchange of information
that a user had in common with their friends—e.g., being
within 1 mile of their friend, or socializing with a person they
both knew—since users would not have to reveal completely
new information to their friends. In the same vein,
teens
routinely swap their phones with each other to add or update
contact information to each others’ address books [70].
As for what they don’t share, users often self-censor to
control their presentation of self. For example, Sleeper et
al. found that users would potentially share 50% more on
Facebook if they could more selectively choose and block
the audiences that could see the posts [71]. In a similar
vein, Nextdoor users censor themselves when concerned with
whether other users in their community would know they were
at home: for example, instead of specifying which nights they
would need a babysitter, which would reveal when they would
not be at home, users solicited general recommendations for
babysitters in the area [64]. And, if they don’t self-censor their
thoughts in time, users frequently go back and delete posts that
might be embarrassing or damaging as well [72].
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:11:32 UTC from IEEE Xplore.  Restrictions apply. 
71869
users manage their privacy and combat context collapse in
these environments. For example, Cobb and Kohno [78] ex-
plored user conceptions of privacy on online dating platforms,
ﬁnding that when users saw proﬁles of people they knew
ofﬂine, their impressions from the proﬁle lingered in future in-
person interactions. Even if men and women tend to present
themselves differently on these apps (men to ﬁnd hookups
and relationships, women to self-validate and make friends
[79]), online dating users as a whole selectively disclose dating
information, both on their proﬁles and to people they know
in person, purposefully trying to hide themselves from people
they know [78]. And, similar to ridesharers [50], while users
appreciate information on proﬁles that helps them look up
and research other users online, they are also wary of being
looked up themselves. Users also regularly take screenshots of
proﬁles and conversation to share with friends, either to shame
bad behavior or get opinions about potential matches [78].
These behaviors hint at some set of social expectations for
sharing personal information in online dating, but the extent
of this sharing is not yet clearly deﬁned. For example, are
personal health statuses fair game? Warner et al. [80] explored
user reactions to linking HIV-positive status to proﬁles on
Grindr, a geosocial hookup app, ﬁnding that while some HIV-
positive users hide their status to reduce their potential of being
stigmatized, others purposefully disclose their status for the
same reasons. Warner et al. also found that when users keep
their status private, other users make social assumptions about
why they were keeping their status private, revealing social
expectations that force them to disclose their statuses.
This series of events is an example of privacy unraveling,
termed by Peppet [81], who argues that users will eventually
face a limited set of options for disclosing personal infor-
mation, since others might assume that those who withhold
or remove information are doing so because they are hiding
something unsavory. (Indeed, Minaei et al. [82] have even also
begun exploring how to protect users against mass collection
of their deleted posts). As such, users might be forced into
disclosing their information to avoid being socially stigma-
tized: more broadly, advertising about location features might
pressure to overshare their location data with others [83].
Users also often carefully curate their digital availability via
online status indicators on social media services. For example,
some users avoid opening apps or sign out of apps quickly
when they see speciﬁc other people online with whom they
wanted to avoid communicating. Conversely, users often also
open apps and services just to check if speciﬁc people were
online, and make inferences about these people’s real-world
behavior, availability, and emotions. More adversarially, some
users describe feeling tracked by others via their indicators,
and end up turning off the indicator entirely [73].
Even though social media sites follow individualized models
of privacy control, the nature of privacy online is contextual
and networked. In particular, teens are forced to consider
alternate models of privacy and challenge conceptions that
young people do not care about their privacy online. Teens
deftly use audience controls and coded language to limit adult
access to intimate digital materials; since they know that they
cannot single-handedly control all of their privacy online,
they rely on managing their social relationships as a proxy
for negotiating the sharing of their information [17]. More
broadly, when a user shares information about themselves in
this networked model, they can inevitably reveal information
about others (e.g., photos of multiple friends); ConsenShare
[74], a system that notiﬁes users about information that others
share about them and encrypts it, attempts to combat this
bystander information leakage.
This goes against adult preferences for “locking everything
down”. For example, older adults are largely concerned with
unauthorized access to personal information, as opposed to
context collapse, unintended sharing, or misuse by large in-
stitutions [75]. These concerns, which deal with keeping the
details of their daily lives private, differ from those of younger
users, who are more concerned with self-presentation. Older
adults thus primarily mitigated these concerns by limiting what
they posted or staying off social media entirely. However,
even some adults have adapted indirect S&P behaviors from
teens to form their self-presentation. For example, adults in
the workplace share of iTunes playlists not to be able to listen
to others’ music, but to explicitly manage the way others form
an impression of the musical tastes of the sharer [76].
Lindqvist et al. [77] also found that users of Foursquare
use their check-in behavior to signal to others about their
personality and self. For example, some users avoid checking
in at
their own home over and over again because they
do not want to present themselves as boring people; others
avoid checking into fast food restaurants because they are
embarrassed about being seen there. And, like online status
indicators, check-ins could also signal availability. However,
unlike social-group-level signals, Foursquare users seemed
largely unconcerned with public visibility of their proﬁles.
D. The public
As users conduct more of their lives online, the bound-
aries between intimate and public social relationships grow
increasingly fuzzy. One domain where this is especially pro-
nounced is online dating; a few works have explored how
This phenomenon lends itself to thought exercises on the
future of self-presentation at the public level. TheWebConf,
for example, has held workshops devoted to exploring the
consequences of technologies in the dystopian science ﬁction
television series Black Mirror. One such workshop conjured
up future scenarios of users developing anxiety because they
cannot match the socially-shared ﬁtness levels of those around
them, and ﬁtness-tracker-wearing citizens being ranked by
public health agencies for priority access to resources, with
those who exercise less often deemed “lazy” and de-prioritized
[84]. There is even already evidence for such scenarios. Users
of wearable ﬁtness tracker devices tend to lack knowledge of
the threats associated with sharing data from their devices [85].
Meanwhile, the wearable ﬁtness tracker company Fitbit has
already signed agreements with health insurance companies
to provide discounted plans to their users [86].
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:11:32 UTC from IEEE Xplore.  Restrictions apply. 
81870
There have been a few end-user tools that could help
circumvent tracking and enable users to more effectively ob-
fuscate their identities. Tor [87], of course, is the well-known
onion network that relays users’ encrypted trafﬁc through
random network nodes to conceal their Internet activity. Since
it requires a network of volunteers to run these relay notes,
it is thus cooperative by design: the more volunteers, the
more robust the network is against attacks. TrackMeNot [88],
a browser extension, aims to help users obfuscate online
searches by submitting “ghost queries” to search engine com-
panies to conceal users’ actual searches. Similarly, AdNau-
seum [89] aims to protect users from tracking advertisers by
silently clicking on ads that have been blocked and sending
noisy data back to advertisers. However, since these tools tend
to resist mainstream adoption and thus collective, coordinated
use, there have not been demonstrable public effects of hidden
browsing information.
E. Takeaways
At each social scale, users continuously weigh the conse-
quences of divulging personal information against building
trust and camaraderie with those around them, carefully curat-
ing their digital self-presentation while often simultaneously
conceding that it is impossible for them to control all digital
content about themselves. However, existing audience control
technologies do not fully help users navigate the broad range
of their social sharing preferences. The upshot is that users
are prone to interpersonal privacy violations where unintended
audiences encounter sensitive content. Moreover, as divulging
personal information online becomes more normative, indi-
viduals may feel increasing peer pressure to follow suit or
otherwise risk being labeled as suspicious and/or paranoid.
We foresee an opportunity for future systems that empower
users to challenge these norms by, e.g., affording users greater
agency over who can use personal data that they share online.
VII. INFLUENCING OTHERS’ S&P BEHAVIORS
A ﬁnal broad category of study in the literature has focused
on how social groups can inﬂuence others’ S&P behaviors.
Users engage in positive S&P behaviors more frequently when
spurred on by social triggers than when forced to do so, and
are more likely to share their own S&P experiences with
others when the experiences were socially-triggered [90]. This
behavior is reﬂected in work across different social scales:
reliance on partners for S&P knowledge, children consulting
parents for help with S&P online, older adults preferring S&P
aid from in-person interactions over online searches, along
with a slew of S&P behavioral nudges via social proof.
A. Intimate relationships
Within romantic relationships, users tend to follow advice
and inﬂuence behaviors similar to those within households:
the user with the more technical knowledge tends to set
up device and network settings within the home [91]. In
IPV situations, abusers tend to ﬁll the more “tech-savvy”
role, from purchasing the devices to setting them up with
restrictive surveillance controls [36]. As such, some survivors
avoid technology altogether after leaving these relationships,
even though they need it to register for social services or
look for employment. With no one else to turn to, survivors
rely on professionals in the IPV ecosystem, like social case
managers and attorneys, to help them with their S&P [36].
These professionals, in turn, feel limited in the S&P advice
they can offer since they themselves are not S&P experts;
instead, they are forced to learn on the job on behalf of their
survivor clients.
B. Families and households
Much research at
this scale has explored how parents
inﬂuence (and control) children’s S&P behavior in the home.
For example, young children often recognize S&P behaviors
(e.g., identifying information as sensitive, understanding what
was appropriate to share) and develop strategies to manage
their concerns, but ask their parents for help anyway [92].
Even as they understand the importance of authentication,
e.g., putting passwords on their devices to deter siblings from
viewing their information, they still need their parents to teach
them about S&P events online.
Conversely, parents prefer deferring action on their young
kids’ online privacy to the future,
i.e., when they were
older and more engaged socially online, instead of building
foundational S&P behaviors [92]. When parents do directly
intervene on their teenage children’s privacy, e.g., through
parental controls or setting up the teen’s social media privacy
settings, there is a potential suppressive effect: while the teens’
exposure to online risks is reduced, so are their opportunities
to engage with others online and learn how to cope with
these risks [93]. Generally, work on the parent-child S&P
relationship recommends that parents engage in more active
parental mediation, e.g., discussing their posts or commenting
on Facebook, rather than “locking everything down”, in order
to empower children to learn to engage with others online.
Murthy et al. [94] also observed self-appointed technology
managers within Indian households who help establish S&P
guidelines within families. These managers assume that older
members of the household are less technically literate and
more vulnerable to S&P threats; accordingly, they often uni-
laterally control and make changes to S&P settings and device
settings on behalf of older users, suggesting paternalism and
removing digital agency from older users. Older adult users
can look outside of the household, though: they surprisingly
also trust local after-sales support staff, since they almost
exclusively only seek S&P information when they needed to
ﬁx a problem [95]. The in-person aspect of such advice is
most key: older adults tend to avoid searching on the Internet
for help and are skeptical of the trustworthiness of information
online.
C. Social acquaintances
Digioia and Dourish [96] ﬁrst introduced social navigation
as a way to help visualize user activities within a system
and better incorporate the user into making security decisions.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:11:32 UTC from IEEE Xplore.  Restrictions apply. 
91871
They speciﬁcally refer to this as a way for users to concep-
tualize how their own security behaviors compare to and are
seen by others. Richter Lipford and Zurko then proposed a
community oversight paradigm for security-related behaviors
that would take into account social context and processes [16].
Chouhan et al. [15] further formalized this into Community
Oversight for Privacy and Security (CO-oPS), a framework
for guiding users to interact with the people they trust in
their social communities when making S&P decisions. We
ﬁnd that such behaviors are triggered via two main methods of
inﬂuence: (1) peer-level inﬂuence through conversations about
and observations of positive S&P behaviors, and (2) top-down
nudges to make positive S&P choices.
1) Peer inﬂuence: Prior work has found that social inﬂu-
ence affects end-user S&P behaviors, be it through conversa-
tion, observation, or peer connection more broadly.
Conversations about S&P are driven by a sense of personal
responsibility for others around us, e.g., to protect or warn
them about threats, or to gather more info about something
we are experiencing ourselves. Negative personal experiences
are frequent catalysts for conversations about security, as well
as news articles and personal observations of security-related
behaviors, since users often try to notify or warn those around
them about what they’ve experienced or learned [2]. Rader et
al. [3] found that undergraduates at Michigan State Univer-
sity often learned lessons from stories about security events
they heard from family and friends, which impacted their
subsequent S&P decisions. These students then retold these
stories to others to inform even more people [3]. Similarly,
Facebook users who seek support in their friends and family
upon receiving suspicious login alerts from Facebook ﬁnd a
sense of camaraderie in these conversations (and those who do
not, feel embarrassment) [97]. Yet, those who seek information
about others’ S&P experiences also feel less inclined to share
their own experiences and help others, preferring passive
participation in conversation [15]. Watson et al. [45] also