vice a page fault, the stub will attempt to either read in a
byte from the address or write a byte to the address. This
will cause the same page fault in the production VM. After
completing this read or write, the stub sends a notiﬁcation
back to our helper module. The stub’s write to the mem-
ory will be immediately overwritten with the correct value
by the out-grafted processes when it re-executes a faulting
instruction, thus ensuring correct process execution. Unlike
a system call return value, the stub cannot return the guest-
physical page allocated for the new address in the produc-
tion VM. Instead, since we have write-protected the process
page tables, when the production VM makes a change to it
(i.e. to set the page table entries for the new page), this
is intercepted by the hypervisor and our helper module will
be notiﬁed. After that, it then allows the page fault han-
dling routines in the security VM to continue processing. As
previously described, instead of duplicating a memory page,
we simply tell the hypervisor to map the same host-physical
memory page corresponding to new page in the production
VM.
Later on, if we decide not to continue monitoring the out-
grafted process, we can place it back in the production VM.
In this case, since we have already maintained synchronized
page tables between the two VMs, we only need to restore
the execution context states back in the production VM.
For those memory pages overlaid for the stub use, they need
to be properly restored as well. Speciﬁcally, if we ﬁnd the
VCPU was executing stub code, which is located in the user
mode, we can simply change the VCPU state contents to
restore the execution context. Otherwise, we need to de-
tect when the user-mode execution resumes by marking the
stub’s code page as NX and then restore the VCPU values.
In the security VM, any state for the out-grafted process
is then simply destroyed (i.e. guest-physical page frames
allocated to it are freed and helper module requests the hy-
pervisor to release the mappings in the security VM EPT).
3.
IMPLEMENTATION
We have implemented a process out-grafting prototype
by extending the open-source KVM [2] hypervisor (version
2.6.36.1). All modiﬁcations required to support out-grafting
are contained within the KVM module and no changes are
required to the host OS. Our implementation increases KVM’s
36.5K SLOC (x86 support only) by only 1309 SLOC, since
most functionality required for out-grafting (e.g. manipu-
lating a VM’s architectural state) is already present in the
stock KVM. Our prototyping machine runs Ubuntu 10.04
(Linux kernel 2.6.28) with an Intel Core i7 CPU and hard-
ware virtualization support (including EPT). Though our
prototype is developed on KVM, we believe it can be sim-
ilarly implemented on other hypervisors such as Xen and
VMware ESX. Our current prototype supports both 32-bit
Fedora 10 and Ubuntu 9.04 as guests (either as the produc-
tion VM or the security VM). In the rest of this section, we
present details about our KVM-based prototype with Intel
VT support.
3.1 On-demand Grafting
KVM is implemented as a loadable kernel module (LKM),
which once loaded extends the host into a hypervisor. Each
KVM-powered guest VM runs as a process on the host OS,
which can execute privileged instructions directly on the
CPU (in the so-called guest mode based on hardware virtual-
ization support). The VM’s virtual devices are emulated by
a user-level program called QEMU [4] that has a well-deﬁned
ioctl -based interface to interact with the KVM module. In
our prototype, we extend the interface to deﬁne the out-
grafting command graft process. This command accepts a
valid page table base address (or guest cr3 ) to directly lo-
cate the process that needs to be out-grafted. The guest cr3
could be retrieved either by converting from the process ID
or process name via VM introspection, or directly reading
from the guest kernel with a loadable kernel module.
Once the graft process command is issued, the KVM mod-
ule pauses the VM execution, which automatically saves the
VM execution context information in the VM control struc-
ture (VMCS [19]). KVM provides a number of functions
that wrap CPU-speciﬁc instructions to read and write to
diﬀerent ﬁelds in this VMCS structure. From this struc-
ture, we retrieve the current value of the VM’s cr3 register
and compare with the argument to the graft process. If they
match and the current VCPU is running in user-mode, we
then read the VCPU’s register values (eax, ebx, esp etc.) and
store them in memory. Otherwise, we take a walk through
the process page table from the given base address to locate
all the user-mode guest-physical page frames (gfn) used by
the to-be-grafted process. Using the gfn to pfn() function in
KVM, we obtain the host-physical page frames (pfn) and set
the NX bit in the VM’s EPT. As a result, when the VCPU
returns back to execute any user-mode instruction in this
process, an EPT violation occurs and the control is trans-
ferred to KVM. At this point, KVM can read the register
values from VMCS and save a copy.
After retrieving its execution context (i.e., those register
values), we then examine the page table (from the given
base address) to determine which virtual addresses in the
suspect process have pages allocated or present. That is, it
determines the gfn for each virtual address and then invokes
gfn to pfn() to determine the corresponding pfn in the pro-
duction VM’s EPT. KVM stores this information in a local
buﬀer. It then makes an upcall in the form of a virtual IRQ
to the security VM (using the inject irq() function).
In the security VM, we have implemented a helper mod-
ule which is registered to handle this (virtual) IRQ. The
helper module then instantiates a process context for the
out-grafted process so that it can continue the execution.
Speciﬁcally, it ﬁrst allocates a memory buﬀer and makes a
hypercall to KVM with the address of this memory buﬀer,
so that KVM can copy the state Srd to it. Since spawning
a new process is a complex task, our helper module creates
a simple “dummy” process in the security VM, which exe-
cutes in an inﬁnite sleep loop. Upon the IRQ request from
KVM, it proceeds to replace the dummy process with the
368out-grafted process state. Speciﬁcally, our helper module re-
trieves register values from Srd and instantiates them in the
dummy process (using the pt regs structure) After that, we
simply destroy any pages (i.e., via do unmap()) allocated to
the dummy process and then allocate “new” pages for virtual
addresses, as indicated in Srd. Note that we do not actu-
ally allocate new host-physical memory pages to accommo-
date these transferred memory pages. Instead, KVM simply
maps ( direct map()) those host-physical memory frames
that are used by the out-grafted process to the dummy pro-
cess in the security VM. After the mapping, the out-grafted
process is ready to execute its user-mode instructions in the
security VM. An execution monitor (such as strace) in this
security VM can now intercept the process-related events it
is interested in.
3.2 Mode-sensitive Split Execution
After relocating the suspect process to the security VM,
any system call made from it will be intercepted by our
helper module and forwarded back to the production VM.
Speciﬁcally, our helper module wraps the exposed system
call interface in the security VM to the out-grafted process.
For each intercepted system call, we collect the correspond-
ing system call number and its argument values in a data
structure (sc info) and save it in the shared communication
buﬀer so the stub code in production VM will pick it up to
invoke the actual system call. (As noted in Section 2.3.1,
memory contents for pointer arguments are not copied since
the user-level memory is present in both VMs).
More speciﬁcally, the stub code is created when the pro-
cess is being out-grafted from the production VM to the
security VM. Its main purpose is to proxy the forwarded
system calls from the security VM to the production VM.
As mentioned in Section 2.3.1, we need to “steal” an existing
code page to host the stub code. We have written the stub
code in a few lines of assembly with an overall size of 167
bytes. The stub code itself does not make any use of a stack
while executing (Section 2.3.1). Similarly, with the help of
KVM, we set up a shared communication buﬀer between
the stub code and our helper module. When a system call
is to be forwarded to the production VM, our helper mod-
ule copies the sc info data structure described above to this
buﬀer. It then sets a ﬂag (in the same buﬀer) to indicate to
the stub that a new system call is to be serviced and waits
in a loop for this ﬂag to be cleared by the stub. To avoid
blocking the entire security VM during this time, it yields
from inside the loop.
The stub code checks the ﬂag and then retrieves the sc info
values and copies them to the registers in the production
VM. It then invokes the requested system call so that the
production kernel can service the request. Once the request
is complete, the stub places the return value in the buﬀer
and modiﬁes the ﬂag indicating service completion. After
that, our helper module in the security VM can now re-
turn the same value to the out-grafted process. In addition
to forwarding system calls from the out-grafted process, we
also need to forward related page faults to the production
VM. Naturally, we leverage the above communication chan-
nel between the stub code and our helper module. Specif-
ically, when a page fault occurs in the out-grafted process
(while running in the security VM), the security VM’s page
fault handler invokes a callback deﬁned in our helper mod-
ule, which then forwards the page fault information to the
stub. Based on the fault information, the stub either reads
or writes a dummy value in the faulting address in the pro-
duction VM to trigger a page fault of the same nature in
the production VM. When the page fault handler in the
production VM attempts to update the page tables with a
new page table entry (pointing to a new page frame we de-
note by gfn p), this causes an “EPT violation” and control
is transferred to KVM. KVM examines the root cause and
saves a copy of the gfn p->pfn mapping while ﬁxing the vio-
lation and resuming the production VM. The helper module
notiﬁes KVM with the new guest-physical frame it allocated
in the security VM (gfn s). KVM then maps gfn s to pfn
and ensures the same memory content is available in both
VMs. After that, the out-grafted process can continue its
execution with the new memory page.
3.3 Process Restoration
In our prototype, process out-grafting is initiated through
a QEMU command graft process. As mentioned earlier,
other mechanisms can also be added to trigger the out-
grafting process. An example is an event-based trigger that
runs inside the production VM (Section 4).
In our current prototype, we also implemented another
QEMU command restore process, which can be invoked to
notify KVM (via an ioctl interface) to restore the out-grafted
process back to the production VM. Similar to the out-
grafting procedure, when KVM receives the restore process
command, it injects an IRQ to the security VM, which will
be received by the helper module. If the module is currently
waiting on a forwarded system call’s completion, the restora-
tion operation cannot be immediately carried out. Instead,
it will wait for the completion of the system call. After that,
it fetches register values stored in the process’ pt regs data
structure and sends this down to KVM with a hypercall.
KVM then restores this register state back in the produc-
tion VM. Due to the similarity with the earlier out-grafting
steps, we omit the details here. The key diﬀerence however
is that, instead of copying values from the VCPU ﬁelds, we
copy values to it. For the page tables, as they are kept
in-sync between the two VMs, no further actions will be
needed. For those process contexts and guest memory pages
allocated to the out-grafted process in the security VM, we
simply discard them. At this point, the process can seam-
lessly resume its execution in the production VM.
4. EVALUATION
In this section, we ﬁrst perform a security analysis on
the isolation property from our approach. Then, we present
case studies with a number of execution monitoring tools.
Finally, we report the performance overhead with several
standard benchmarks.
4.1 Security Analysis
Monitor isolation and eﬀectiveness
To allow for
ﬁne-grained out-of-VM process monitoring, one key goal is
to ensure that the process monitoring tool and its support-
ing environment cannot be tampered with or disabled by
the out-grafted process. In the following, we examine possi-
ble attacks and show how our system can eﬀectively defend
against them. Speciﬁcally, one main way that a suspect pro-
cess can tamper with another process (or the monitor in our
case) is through system calls. However, such attack will not
work since our system strictly forwards all system calls from
369the suspect process back to the production VM. Moreover,
the controlled interaction is only allowed from the monitor-
ing process to the suspect process, not the other way around.
From another perspective, the suspect process may choose
to attack the (production VM) OS kernel when it services
system calls (for e.g. exploiting a buﬀer overﬂow by sending
in an invalid argument). Such attack will only impact the
production VM and its own execution.
According to our threat model (Section 2), we stress that
our system does not attempt to guarantee stealthy moni-
toring as out-grafted monitoring could be detected by so-
phisticated malware. But we do enable reliable monitoring
in protecting our system from being tampered with by the
suspect process. In addition, a strong administrative policy
might reduce the time window for such out-grafting detec-
tion. For example, out-grafting can be initiated randomly
(at any instance in a process’ lifetime) and can span arbi-
trary durations. In such cases, malware would be forced to
continuously check for out-grafting which can be costly and
expose its presence. Note that the out-grafted process is
ultimately serviced by the untrusted production VM, and
as such, we cannot guarantee that it accurately services the
out-grafted process’ system calls, but any inappropriate han-
dling of system calls will not violate the isolation provided
by our approach.
Protection of helper components The out-grafting
operation itself is initiated and controlled by the hypervi-
sor and cannot be disabled by a malicious production VM
kernel. However, there are two helper components in the
production VM to support out-grafting: the stub code and
shared communication buﬀer, which may be open to attack.
We point out that since the stub code’s host-physical page
is marked as read-only in the EPT, any malicious attempts
to write to it will be trapped by the hypervisor. The stub
code’s guest virtual-to-physical mapping cannot be altered
by the production VM since the page tables of the process
are write-protected by the hypervisor. The untrusted kernel
in the production VM is responsible for scheduling the stub
process as well as saving and restoring its execution context
according to the scheduling policy. If it tampers with the ex-
ecution context states (such as the instruction pointer), then
the stub code itself will not execute correctly, which cascad-
ingly aﬀects the execution of the suspect process itself. As
mentioned earlier, if the production VM does not properly
serve the forwarded system calls or attempts to alter the
system call arguments or return incorrect results, such be-
havior may result in incorrect execution for the out-grafted
process, but will not aﬀect the isolation or the integrity of
our monitoring process.
4.2 Case Studies
Next, we describe experiments with a number of execution
monitoring tools, including the most common ones: strace,
ltrace, gdb, as well as an OmniUnpack[26]-based tool (to de-
tect malware unpacking behavior). The common tools are
used to demonstrate the eﬀectiveness of our approach in
removing the semantic-gap, while the OmniUnpack tool re-
quires special hardware support for the monitoring in the
security VM and such support may not be enabled or pro-
vided in production VM. As a test process for out-grafting,
we chose the thttpd web server that uses both disk and net-
work resources and has a performance benchmark tool read-
ily available to automatically exercise it.
4.2.1 Tracing System Calls
In our ﬁrst experiment, we demonstrate semantically-rich
system call tracing. This type of monitoring has been widely
applied to detect malicious behavior [13] such as accesses to
sensitive resources or dangerous system calls. For this, we
install the standard Linux strace tool in the security VM.
strace makes use of the underlying OS facilities to monitor
system calls invoked by another running process, which in
our case logically runs in another VM. For each intercepted
system call, it retrieves and parses the arguments. The re-
sults will allow us to know what ﬁle was opened by a process,
what data is read from it etc.
In prior “out-of-VM” systems, the code to determine sys-
tem call number and interpret each of its arguments has to
be completely re-written. In fact, one of our earlier systems,
i.e., VMscope [20], took one of the co-authors more than
one month to correctly intercept and parse the arguments
of around 300 system calls supported in recent Linux ker-
nels. This task is expected to become even more complicated