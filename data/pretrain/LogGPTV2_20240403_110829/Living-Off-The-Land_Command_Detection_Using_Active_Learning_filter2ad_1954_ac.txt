dividual threat actor group may choose a particular path, filename,
registry key name, etc. using a similar pattern for their attacks, and
these patterns may occur across different parameter settings for
different LOLBIN commands. In this case, the classifier may learn
these patterns across the different LOLBIN classes in the proposed
multiclass setting. The anomaly detection stage can help the analyst
to discover these more fine-grained classes for each type of benign
and malicious LOLBIN activity found in the data.
An overview of our active learning framework used for detecting
LOL attacks is illustrated in Figure 3. Unlabeled process creation
events are generated by the endpoint software installed across a
large number of clients, and these events are transmitted to the
backend cloud system for analysis. Our system generates feature
vector representations for process command lines. As part of this
process, we develop a novel command-line representation method
using word vectors (see Section 3.1). This unlabeled data is aug-
mented with a much smaller labeled dataset generated manually by
human analysts after investigating LOLBIN-related process com-
mands. During an iterative process in our active learning frame-
work, we propose several strategies for sampling command lines
for labeling by a human expert. Our adaptive sampling strategy
selects uncertain and anomalous samples ranked from each class
in every iteration. To generate and rank anomalous samples, we
compute sample probabilities in each class using a naïve bayes
model. We demonstrate that this sampling strategy outperforms
random sampling, as well as strategies that use either uncertain
or anomalous samples. Finally, we train and evaluate multi-class
classifiers using the labeled data to distinguish between benign and
malicious command lines, and show that the classifier performance
significantly improves over time as more samples are labeled by
active learning. We consider both linear (logistic regression) and
non-linear (gradient boosting) models and compare their perfor-
mance. We name our system Living-Off-The-Land detection with
Active Learning (LOLAL).
In each iteration of LOLAL, we train a multi-class classifier using
the available labeled feature vectors, with the goal of learning the
posterior probability P(class i|(cid:174)x). When we use a linear logistic
regression classifier with weights wij and bias bi for d features, the
posterior probability for class i is:
P(class i|(cid:174)x) = 1/(1 + exp(− d
wijxj + bi))
(3)
j=1
When we use gradient boosting [38] which is a non-linear, boosted
decision tree classifier, the system learns the posterior probability
P(class i|(cid:174)x).
Once the classifier has been trained using the labeled samples,
it is used to predict the class of the unlabeled samples and the
447Living-Off-The-Land Command Detection
Using Active Learning
RAID ’21, October 6–8, 2021, San Sebastian, Spain
Figure 3: Overview of the LOLAL Active Learning framework for detecting LOL attacks.
posterior probability that the sample belongs to that class. Each of
these unlabeled samples that have been predicted to belong to a
single class are then modeled with a multivariate naïve bayes model.
The naïve bayes model is then used to generate the likelihood that
the unlabeled sample belongs to the class c that was predicted by the
classifier, and we use a sample’s likelihood to compute its anomaly
score A(n):
A(n) = − log P((cid:174)x|class c) = − d
log P(xj|class c).
(4)
j=1
Intuitively, the anomaly score is high when the sample is located
far away from the class’s mean and vice versa.
The ranking of samples is done by a combination of active learn-
ing and active anomaly detection methods. For active learning, we
use uncertainty sampling [24] where the classifier’s posterior prob-
ability is used to compute an uncertainty score. For each sample
(cid:174)xn, the uncertainty score is given by:
U(n) = − min
i, j(cid:44)i
|P(i|(cid:174)xn) − P(j|(cid:174)xn)|
(5)
where i = arg maxk P(k|(cid:174)xn). Typically, a sample’s class is predicted
to be the class with the highest posterior probability for that sample.
The uncertainty score then considers the class with the second-
highest posterior probability for that sample. If these two class
posterior probabilities are close, the difference is small. Thus a high
uncertainty score indicates that LOLAL has difficulty assigning the
sample to one class, because the two most likely classes are almost
equally possible. The likelihood from the naïve bayes model is used
for active anomaly detection to compute the anomaly score, A(n).
Figure 4 illustrates an example of sample selection using uncertainty
and anomaly scores.
We experiment with these two sampling strategies (uncertainty
scores and anomaly scores), as well as a combination of these two
rankings. In the combined ranking, we select the most uncertain
sample followed by the most anomalous sample, one for each class,
in a round robin fashion. The process is repeated until all samples
have been ranked. Thus, the final ranking for the analyst is found
by alternatively selecting samples for each class with the highest
uncertainty score, and samples with the highest anomaly score.
Figure 4: Sample selection for our active learning
algorithm. We show classifier predictions (left), uncertain
and anomalous samples picked for the analyst (right).
Labeling uncertain samples corrects the classifier, while
picking anomalous samples helps detect novel attacks.
For each complete round, we thus select 2 · C samples, where C is
the number of classes. The idea behind presenting the uncertain
and anomalous items for each class in a round robin fashion to the
analyst is to have them consider both types of examples for each
class. This strategy encourages the analyst not to focus on the most
prevalent classes.
Consider an example dataset with items corresponding to three
known classes, c1, c2, c3. The uncertainty score U(n) and the anom-
aly score A(n) are computed for each item and ranked separately
for each class. For the first round, we select the sample with the
largest uncertainty score as the first ranked item among the sam-
ples predicted to belong to class c1 by the current classifier. We
repeat this step for all samples predicted to belong to c2 to select the
second ranked item, and similarly the third ranked item is selected
from all samples predicted belong to the third class. We then select
the most anomalous sample of all of those predicted to belong to
c1 as the fourth ranked item. The fifth and sixth items in the final
ranked list are selected as those which have the highest anomaly
scores for classes c2 and c3, respectively.
The newly labeled samples by an analyst are then added to the
training labeled dataset, and the algorithm continues iteratively. We
show that the combination of both of these techniques for sample
selection improves the performance of the classifier compared with
Train classiﬁerUnlabeled DataProcess Creation DatasetRun classiﬁer onunlabeled dataAnalysis bySecurity AnalystNewly LabeledSamplesLabeled DataWord VectorsToken ScoresFeature Vector GenerationRank Samples Per ClassUncertainSamplesAnomalousSamplesDecision BoundaryBenign Class LOLBIN 1Class LOLBIN 2 ClassBenign Class LOLBIN 1Class LOLBIN 2 ClassDecision BoundaryBenign SampleMalicious SampleUncertain SampleAnomalous Sample448RAID ’21, October 6–8, 2021, San Sebastian, Spain
Ongun, et al.
using only uncertainty or anomaly scores for sample selection. We
assume the human analysts make the correct decision, as label-
noise is its own area in machine learning and is not specific to our
system [17, 50].
A single iteration of our active learning algorithm can be sum-
marized as follows:
(1) Train a multi-class classifier using the set of labeled sam-
ples available. This could be a logistic regression model or a
gradient boosting classifier.
(2) Evaluate the classifier and generate the uncertainty scores
(Eqn. 5) for the unlabeled samples.
(3) Assign unlabeled samples to the most likely class and com-
pute the naïve bayes parameters for every class.
(4) Compute the anomaly score (Eqn. 4) for unlabeled samples.
(5) Select the next batch of samples to be labeled as follows:
• Select the most anomalous unlabeled sample with the
• Select the sample with the largest uncertainty score (Eqn.
highest anomaly score (Eqn. 4) in each class.
5) in each class.
(6) Repeat previous step until the desired number of samples
have been collected for the iteration
(7) Send selected samples to the human analyst, and receive the
(8) Add the newly labeled data to be used in the classifier train-
correct labels.
ing in the next iteration.
4 EVALUATION
We start by providing details about the process creation telemetry
dataset on which we perform our analysis. We then evaluate the
command embedding feature representation, and finally we present
results from evaluating our active learning framework.
4.1 Dataset
We use process creation telemetry reports provided by the Microsoft
Defender for Endpoint enterprise security product. The data has
been collected from a subset of computers across different organiza-
tions installing the product, and thoroughly anonymized before any
authorized analysts are permitted to inspect the data. We extract
the command-line strings for the process and parent process for the
five selected binaries listed in Table 1: bitsadmin.exe, certutil.exe,
msbuild.exe, msiexec.exe, and regsvr32.exe. We leverage multiple
datasets to evaluate our system.
All Instances: This dataset includes millions of unlabeled samples of
LOLBIN command lines. and is used in Section 4.2 for unsupervised
training the word2vec and fastText embeddings to learn token
contextual representations.
Selected Samples: This dataset includes a set of selected LOLBIN
command-line instances, meaning that a specific pattern has been
detected by heuristic rule-based methods. This dataset includes
10522 samples across the five LOLBINs. We use this set of commands
in Section 4.4 for sample selection in the active learning framework.
Labeled Samples: A small subset of the selected samples have been
analyzed by security experts to verify their malicious behavior.
Based on these labeled alerts, we create a separate malicious class
for each of the LOLBINs, and we group together all of the con-
firmed false positives into a separate Benign class. There are 1987
Table 2: Distribution of the labeled command-line samples
across the classes.
Class
Benign
BitsadminLolbin
CertutilLolbin
MsbuildLolbin
MsiexecLolbin
Regsvr32Lolbin
Total
Sample Count
454
159
1043
33
92
206
1987
Table 3: Token scores generated using the labeled samples.
The top table shows the highest token scores, whereas the
bottom one shows the lowest scores.
LOLBIN
Token
aptsimulator
Certutil
xml
Regsvr32
ru
Regsvr32
attackiq
Bitsadmin
ipv4pii
Msbuild
%temp%
Certutil
noexit
Regsvr32
lt;numbergt;
Regsvr32
dat
Certutil
payloads
Msiexec
scrobj
Regsvr32
LOLBIN
Token
cpu
Msiexec
releases
Msbuild
downloadjob
Bitsadmin
install
Certutil
amd64
Msiexec
ie
Bitsadmin
plugin
Bitsadmin
datasetextensions
Msbuild
applicationservices Msbuild
serialization
Msbuild
jetbrains
Msbuild
Score
1
1
1
1
1
1
0.998
0.992
0.99
0.924
0.916
Score
0.027
0.01
0.01
0.01
0.01
0
0
0
0
0