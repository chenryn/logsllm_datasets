title:Measuring the Effectiveness of Privacy Policies for Voice Assistant
Applications
author:Song Liao and
Christin Wilson and
Long Cheng and
Hongxin Hu and
Huixing Deng
Measuring the Effectiveness of Privacy Policies
for Voice Assistant Applications
Song Liao, Christin Wilson, Long Cheng, Hongxin Hu, and Huixing Deng
School of Computing, Clemson University
0
2
0
2
l
u
J
9
2
]
R
C
.
s
c
[
1
v
0
7
5
4
1
.
7
0
0
2
:
v
i
X
r
a
ABSTRACT
Voice Assistants (VA) such as Amazon Alexa and Google Assistant
are quickly and seamlessly integrating into people’s daily lives.
The increased reliance on VA services raises privacy concerns such
as the leakage of private conversations and sensitive information.
Privacy policies play an important role in addressing users’ privacy
concerns and informing them about the data collection, storage, and
sharing practices. VA platforms (both Amazon Alexa and Google
Assistant) allow third-party developers to build new voice-apps and
publish them to the app store. Voice-app developers are required
to provide privacy policies to disclose their apps’ data practices.
However, little is known whether these privacy policies are infor-
mative and trustworthy or not on emerging VA platforms. On the
other hand, many users invoke voice-apps through voice and thus
there exists a usability challenge for users to access these privacy
policies.
In this paper, we conduct the first large-scale data analytics to
systematically measure the effectiveness of privacy policies pro-
vided by voice-app developers on two mainstream VA platforms. We
seek to understand the quality and usability issues of privacy poli-
cies provided by developers in the current app stores. We analyzed
64,720 Amazon Alexa skills and 2,201 Google Assistant actions. Our
work also includes a user study to understand users’ perspectives
on VA’s privacy policies. Our findings reveal a worrisome reality of
privacy policies in two mainstream voice-app stores, where there
exists a substantial number of problematic privacy policies. Surpris-
ingly, Google and Amazon even have official voice-apps violating
their own requirements regarding the privacy policy.
1 INTRODUCTION
Virtual Assistants (VA) such as Amazon Alexa and Google Assistant
have been seamlessly integrated into our daily life. An estimated
3.25 billion digital voice assistants are being used around the world
in 2019. The number is forecasted to reach 8 billion users by 2023
which is higher than the current world population [9]. VA handles
a wide range of queries that humans are posing, e.g., from ordering
everyday items, managing bank accounts, controlling smart home
devices to recommending clothing stores and new fashions. Despite
the many convenient features, there is an increasing concern on
privacy risks of VA users [16, 19, 23, 26, 31, 32, 35, 37].
Privacy and data protection laws are in place in most of the
countries around the world to protect end users online. These com-
pliance requirements are mostly satisfied by providing a transparent
privacy policy by developers. Google was fined €50 million by a
French data protection regulator after its privacy policy failed to
comply with General Data Protection Regulation (EU GDPR). This
fine was not for failing to provide a privacy policy but for not
having a one that was good enough and failing to provide enough
information to users [6]. Researchers have shown that there are
many discrepancies between mobile apps (e.g., Android apps) and
their privacy policies [17, 36, 39, 47], which may be either because
of careless preparation by benign developers or an intentional de-
ception by malicious developers [41]. Such inconsistencies could
lead to public enforcement actions by the Federal Trade Commis-
sion (FTC) or other regulatory agencies [48]. For example, FTC
fined $800,000 against Path (a mobile app operator) because of an
incomplete data practice disclosure in its privacy policy [13]. In
another case, Snapchat transmitted geolocation information from
users of its Android app, despite the privacy policy states that it
did not track such information. In 2014, FTC launched a formal
investigation requesting Snapchat to implement a comprehensive
privacy program [14].
VA platforms allow third-party developers to build new voice-
apps (which are called skills on Amazon platform and actions on
Google platform, respectively) and publish them to app stores. In
order to comply with privacy regulations (such as COPPA [18])
and protect consumers’ privacy, voice-app developers are required
to provide privacy policies and notify users of their apps’ data
practices. Typically, a proper privacy policy is a document that
should have answers to a minimum of three important questions [7]:
1) What information is being collected? 2) How this information
is being used? and 3) What information is being shared? Third-
party skills and actions are in very high number in the respective
stores. Developers are required to provide privacy policies for their
voice-apps. Policies could be diverse and poorly written, which
results in more users ignoring the privacy policy and choosing to
not read it. This also leads to users using a privacy-sensitive service
without having a proper understanding of the data that is being
collected from them and what the developer will do with it. On the
other hand, the feature that makes VA devices like Amazon Echo
and Google Assistant interesting is the ability to control them over
the voice without the need of physically accessing them. Despite
the convenience, it poses challenges on effective privacy notices
to enable users to make informed privacy decisions. The privacy
policy may be missing completely in the conversational interface
unless users read it over the smartphone app or through the web.
In this work, we mainly investigate the following three research
questions (RQs):
• RQ1: What is the overall quality of privacy policies pro-
vided by voice-app developers in different VA platforms?
Do they provide informative and meaningful privacy poli-
cies as required by VA platforms?
• RQ2: For a seemingly well-written privacy policy that con-
tains vital information regarding the service provided to
users, can we trust it or not? Can we detect inconsistent
privacy policies of voice-apps?
1
• RQ3: What are VA users’ perspectives on privacy policies
of voice-apps? What is possibly a better usability choice
for VA users to make informed privacy decisions?
We conduct the first empirical analysis to measure the effective-
ness of privacy policies provided by voice-app developers on both
Amazon Alexa and Google Assistant platforms. Such an effort has
not previously been reported. The major contributions and findings
are summarized as follow1.
• We analyze 64,720 Amazon Alexa skills and 2,201 Google
Assistant actions. We first check whether they have a pri-
vacy policy. For the 17,952 skills and 1,967 actions that have
one, unfortunately, we find there are many voice-apps in
app stores with incorrect privacy policy URLs or broken
links. Surprisingly, Google and Amazon even have official
voice-apps violating their own requirements regarding the
privacy policy.
• We further analyze the privacy policy content to identify
potential inconsistencies between policies and voice-apps.
We develop a Natural Language Processing (NLP)-based
approach to capture data practices from privacy policies.
We then compare the data practices of a privacy policy
against the app’s description. We find there are privacy
policies that are inconsistent with the corresponding skill
descriptions. We also find skills which are supposed to
have a privacy policy but do not provide one.
• We conduct a user study with 91 participants to under-
stand users’ perspectives on VA’s privacy policies, using
the Amazon Mechanical Turk crowdsourcing platform. We
also discuss solutions to improve the usability of privacy
notices to VA users.
2 BACKGROUND AND CHALLENGES
2.1 Voice-app and privacy policy
Voice-app listing on the store. We mainly focus on two main-
stream VA platforms, i.e., Amazon Alexa and Google Assistant,
both with conceptually similar architectures. These platforms allow
third-party developers to publish their own voice-apps on VA stores.
A voice-app’s introduction page that is shared by the developer on
the store contains the app name, a detailed description, the cate-
gory it belongs to, developer information, user rating and reviews,
privacy policy link, and example voice commands which can be
viewed by end users. The source code is not included in the submis-
sion and therefore is not available either to the certification teams
of VA platforms or to end users. Users who enable a skill/action
through the voice-app store may make their decisions based on
the description. It explains the functionality and behavior of the
voice-app and what the user can expect from it. Some developers
also mention the data that is required from users (i.e., data practices)
in the description.
VA platform’s requirements on privacy policy. Application
developers are often required to provide a privacy policy and notify
users of their apps’ privacy practices. VA platforms have different
1Accompanying materials of this work including the dataset, empirical evidences
for inconsistent privacy policies, and tools are available at https://github.com/voice-
assistant-research/voice-assistant.
requirements regarding the privacy policies of voice-apps. Google
Assistant requires every action to have a privacy policy provided on
submission. Amazon Alexa requires skills that collect personal in-
formation only to mandatorily have a privacy policy. Both Amazon
and Google prevent the submission of a voice-app for certification if
their respective requirements are not met [1, 10]. In addition to the
privacy policy URL, both platforms offer an option for developers to
provide a URL for the terms of use as well. These URLs, if provided
by the developers, are made available along with the voice-app’s
listing on the store.
Requirements on specific content in privacy policies. Google
has a "Privacy Policy Guidance" page [7] in their documentation
for action developers. The guide explains what Google’s minimum
expectation is for a privacy policy document. According to the
guide, the privacy disclosures included in the policy should be com-
prehensive, accurate and easy to understand for the users. The
privacy policy should disclose all the information that an action
collects through all the interfaces including the data that is col-
lected automatically. How the collected information is used and
who and when the collected information is shared with should be
specified. Google rejects an action if developers do not provide (or
even misspell) the action name, company name, or developer email
in the privacy policy. The link should be valid and should also be
a public document viewable by everyone. Amazon Alexa doesn’t
provide a guideline for the privacy policy content in their Alexa
documentation.
Voice-app enablement. VA users enable official (i.e., developed
by VA platforms) or third-party voice-apps to expand the function-
ality of their devices. Voice-apps can be enabled by saying a simple
command through voice or by adding it from the smartphone app.
A voice-app for which the developer has requested permission
to access user’s data sends a permission request to the user’s VA
companion app on smartphone during enablement. The other voice-
apps are directly enabled. A privacy policy can be accessed either
over the VA companion app or through the web. On the contrary,
it is not accessible through the VA devices over voice. VA platforms
do not require end users to accept a privacy policy or the terms of
use of a voice-app before enabling it on their devices. It is left for
the users to decide whether to go through the privacy policy of the
voice-app they use or not.
2.2 Challenges on privacy policy analysis
Existing privacy policy analysis on smartphone platforms [17, 36,
39, 41, 47, 48] typically conduct static code analysis to analyze
potential inconsistencies between an app’s privacy policy and its
runtime behavior. Unlike smartphone app platforms, the source
code of voice-apps in Amazon Alexa and Google Assistant plat-
forms are not publicly available. A voice-app is hosted in a server
selected by its developer and only the developer has access to it.
As far as we know, the source code is not available even to the
VA platform’s certification teams. This limits the extent of our pri-
vacy analysis since we neither have a ground truth to validate our
findings of inconsistent privacy policies nor the actual code to find
more inconsistencies with the privacy policies provided. The only
useful information that we have about a voice-app is the descrip-
tion that is provided by the developer. Descriptions do not have
2
Figure 1: Processing pipeline of our privacy policy analysis.
a minimum character count and developers can add a single line
description or a longer description explaining all functionalities and
other relevant information. Regardless, due to the unavailability of
other options, we use the voice-app descriptions for our analysis
to detect problematic privacy policies. For this reason, our results
on the inconsistency checking of privacy policies (in Section 3.3)
are not focused on the exact number of mismatches and errors but
on the existence of problems potentially affecting the overall user
experience.
3 METHODOLOGY
In this section, we first present an overview of our approach, and
then detail the major modules including data collection process (Sec-
tion 3.1), capturing data practices based on NLP analysis (Sec-
tion 3.2), and inconsistency checking (Section 3.3). We seek to
understand whether developers provide informative and meaning-
ful privacy policies as required by VA platforms. Fig. 1 illustrates
the processing pipeline of our privacy policy analysis. As previously
mentioned, each skill/action’s listing page on the store contains
a description and a privacy policy link (URL). We first collect all
these webpages, and pre-process them to identify high-level issues
such as broken URLs and duplicate URLs. Then, we conduct an NLP
based analysis to capture data practices provided in privacy policies
and descriptions. We seek to identify three types of problematic
privacy policies: i) without any data practice; ii) incomplete policies
(e.g., a skill’s privacy policy lacks data collection information but
it has been mentioned in the skill’s description); and iii) missing
policies (e.g., a skill without a privacy policy but requires one due
to its data collection practices).
3.1 Data collection
We built a crawler to collect a voice-app’s id, name, developer infor-
mation, description and privacy policy link from Amazon Alexa’s
skills store and Google Assistant’s actions store. There were several
challenges for crawling introduction pages of voice-apps. First, for
the skills store, 23 categories of skills are listed but these are not
mutually exclusive. The category "communication" is a subcate-
gory in the "social" category and the category "home services" is
a subcategory in the "lifestyle" category. Some skills are classified
and listed in multiple categories. We need to remove duplicates
during the data collection. Second, Alexa’s skills store only pro-
vides up to 400 pages per category, and each page contains 16 skills.
Though Amazon Alexa claimed there are over 100,000 skills on
its skills store, we were able to crawl only 64,720 unique skills.
Third, Google actions store lists actions in pages that dynamically
load more actions when users reach the end of the page. We were
unable to use the crawler to automatically get information about
all the actions. As a result, we crawled 2,201 actions belonging to
18 categories from the Google actions store. The total numbers of
skills and actions by category we collected are listed in Table 9 and
Table 10 in Appendix.
Another challenge was to obtain the privacy policy content.
Given the privacy policy links, we observed that there are five
types of policy pages: i) normal html pages; ii) pdf pages; iii) Google
doc and Google drive documents; iv) txt files; and v) other types
of files (e.g., doc, docx or rtf). For normal html pages, we used the
webdriver [12] tool to collect the webpage content when they are
opened. For the other types of pages, we downloaded these files
and then extracted the content from them. Finally, we converted