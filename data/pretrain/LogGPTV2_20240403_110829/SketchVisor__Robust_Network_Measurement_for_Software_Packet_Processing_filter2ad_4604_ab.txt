ing a packet in each solution. FlowRadar is the fastest and spends
2,584 cycles per packet, while Deltoid is the slowest and spends
10,454 cycles per packet. Such high CPU overhead translates to
low throughput under high traffic load. Figure 2(b) shows the
maximum throughput achievable by the four solutions versus the
number of threads. No solution can achieve over 5Gbps with one
thread; and Deltoid barely achieves 5Gbps even with five threads.
Thus, these solutions, while being fast enough under low traffic
load, become computationally intensive and resource demanding
under high traffic load in modern data centers, in which servers
are now commonly equipped with 10Gbps NICs and above.
We further analyze the breakdown of the CPU cycles in each
sketch-based solution, and find that the performance bottlenecks
vary across sketch-based solutions. For example, FlowRadar and
Reversible Sketch incur more than 67% and 95% of CPU cycles,
respectively, on hash computations (including randomizing flow
headers to resolve hash collisions). Deltoid’s main bottleneck is
on updating its extra counters to encode flow headers, and this
accounts for more than 86% of CPU cycles. UnivMon spends 53%
and 47% of CPU cycles on hash computations and heap maintenance,
respectively. The variations of performance bottlenecks also imply
that optimizing specific functions (e.g., using hardware-based hash
computations) may not work well for all sketch-based solutions.
Recent work [1] advocates that simple hash tables would suffice
for network measurement due to improved cache management in
servers and skewness of real-life traffic patterns. Although hash
tables incur fewer computations than sketches [1], they consume
significant memory usage (§7.6). Some systems [21, 29, 38, 62] at-
tempt to filter traffic by predefined rules, so as to reduce memory
usage. However, it requires manual efforts to configure proper rules
to achieve both high accuracy and memory efficiency simultane-
ously. On the other hand, sketches provide theoretical guarantees
on memory usage and error bounds, yet incur high computational
overhead. Although they have not yet been widely deployed, we be-
lieve that their sound theoretical properties make them a promising
building block for network measurement. Our work is to mitigate
the computational overhead of sketch-based measurement, while
preserving the theoretical guarantees of sketches.
ment tasks.
cessing and memory for data structures.
3 SKETCHVISOR OVERVIEW
SketchVisor is a robust network measurement framework for soft-
ware packet processing, with several design goals:
• Performance: It processes packets at high speed and aims to ful-
fill the line-rate requirement of the underlying packet processing
pipeline.
• Resource efficiency: It efficiently utilizes CPU for packet pro-
• Accuracy: It preserves high measurement accuracy of sketches.
• Generality: It supports a wide range of sketch-based measure-
• Simplicity: It automatically mitigates the processing burdens of
sketch-based measurement tasks under high traffic load, without
requiring manual per-host configurations and result aggrega-
tions by network operators.
SketchVisor’s design follows the line of software-defined mea-
surement [23, 30, 36, 37, 56]. It comprises a distributed data plane
that runs on the software switches of multiple hosts in a network,
and a centralized control plane that aggregates the local results of all
software switches and returns network-wide measurement results.
Figure 3 shows both data-plane and control-plane architectures of
SketchVisor.
3.1 Data Plane
The data plane (Figure 3(a)) deploys a measurement module in the
software switch of each host. Each module processes incoming
continuous packet streams and collects traffic statistics for the host.
To avoid duplicate measurement, we can choose to monitor only
25843858438210454050001000015000FlowRadarRevSketchUnivMonDeltoidCPU Cycles per Packet0.02.55.07.510.012345Number of threadsThroughput (Gbps)FlowRadarRevSketchUnivMonDeltoidSIGCOMM ’17, August 21−25, 2017, Los Angeles, CA, USA
Huang et al.
SketchVisor’s design leaves the deployment decision of what
sketch-based solutions should be deployed to network operators,
since no sketch-based solution can absolutely outperform others in
all aspects. Based on deployment requirements, network operators
can choose either a general sketch-based solution (e.g., UnivMon)
that supports multiple measurement tasks, or a customized one
with better performance for a specific measurement task.
Challenges: The FIFO buffer provides a lightweight means to
determine when to redirect traffic to the fast path (i.e., by checking
if the buffer is full), without compromising the overall measurement
performance. The trade-off is that we cannot control which specific
flows should have packets sent to the fast path, since tracking
specific flows would add processing overhead. This uncertainty
complicates the design of the fast path. Also, instead of assigning a
fast path per measurement task, we associate a single fast path with
all measurement tasks, so that the fast path remains lightweight
regardless of how sketches in the normal path are designed. To
summarize, the fast path should satisfy the following properties:
(i) fast enough to absorb all redirected traffic, (ii) highly accurate,
although the accuracy may slightly degrade from original sketch-
based measurement, and (iii) general for various traffic statistics.
3.2 Control Plane
The control plane (Figure 3(b)) provides a ‘‘one-big-switch” abstrac-
tion for network operators to specify and configure measurement
tasks at network-wide scale. It collects local measurement results
from multiple hosts and merges them to provide network-wide
measurement results. Its goal is to achieve accurate network-wide
measurement as if all traffic were only processed by the normal path
of each host.
Challenges: It is critical to eliminate the extra errors due to fast
path measurement; in other words, all measurement errors should
only come from sketches themselves. However, such error elimina-
tion heavily hinges on the fast path design, which must be general
to accommodate various measurement tasks (§3.1). Similarly, the
error elimination in the control plane must be applicable for any
measurement task.
3.3 Our Solutions
To address the aforementioned challenges, we propose two algo-
rithmic solutions that build on well-studied techniques: the first
one builds on counter-based algorithms [15, 33] to design a light-
weight, accurate, and general fast path in the data plane (see §4
for details), while the second one builds on compressive sensing
[6, 7, 9, 61] to design an accurate network-wide recovery algorithm
in the control plane (see §5 for details). We point out that bundling
existing techniques directly into SketchVisor does not work as ex-
pected. Instead, we carefully analyze the overhead of the existing
techniques, and then motivate and design our customized solutions
in the context of sketch-based network measurement.
4 FAST PATH
4.1 Key Idea
The fast path is critical for the robustness of sketch-based measure-
ment. Without the fast path, the normal path unavoidably discards
(a) SketchVisor data plane.
(b) SketchVisor control plane.
Figure 3: SketchVisor architecture.
either ingress or egress traffic, or leverage hash-based selection to
monitor disjoint sets of packets at different hosts [47]. We divide the
measurement module into two components, namely a normal path
and a fast path. The normal path deploys one or multiple sketch-
based solutions as chosen by network operators, while the fast path
complements the normal path by deploying a fast but slightly less
accurate measurement algorithm to process packets under high
traffic load. Normally, the software switch forwards all packets to
the normal path through a bounded FIFO buffer, which can hold all
packets to be processed and absorb any transient spike. However,
when the traffic load exceeds the processing capacity of the normal
path, the buffer becomes full. In this case, SketchVisor instructs
the software switch to redirect overflowed packets to the fast path,
which then collects traffic statistics from the overflowed packets.
We do not consider any proactive approach that examines packets
and decides which packets should be dispatched into either the
normal path or the fast path, as it will incur non-trivial overhead.
We emphasize that the fast path cannot substitute the sketch-
based solutions in the normal path. The main reason is that the fast
path is less accurate than the normal path by design. To achieve
highly accurate measurement, the normal path has to process as
many packets as possible, while the less accurate fast path is acti-
vated only when necessary.
PacketsSoftware	
  SwitchUserSpaceKernelSpaceNormalPathFastPathFlow-­‐RadarUniv-­‐MonRev-­‐SketchUser-­Defined  SketchesFast	
  &	
  AccurateTop-­‐KTrackingBufferTo  Control  PlaneHostNormal	
  Path	
  ResultsHeavyHitterHeavyChangerNetworkMeasurementTasksEntropyDDoSHostsFast	
  Path	
  ResultsNetwork-­‐Wide	
  Sketch	
  Recoverywith	
  Matrix	
  InterpolationSketchVisorControllerSketchVisor: Robust Network Measurement for So(cid:129)ware Packet Processing
SIGCOMM ’17, August 21−25, 2017, Los Angeles, CA, USA
traffic to keep pace with high traffic load, which compromises mea-
surement accuracy and even makes some measurement tasks fail
to work (§7.3).
We design the fast path to track as much information as pos-
sible in network traffic with low computational overhead. It is
well-known that network traffic in practice exhibits heavy-tailed
patterns and is dominated by a few large flows [54, 59], so we ex-
pect that the traffic redirected to the fast path is also dominated
by large flows (§7.5). Note that this heavy-tailed assumption in-
duces many new sketch designs (e.g., identifying large flows in
skewed network traffic). While the inherent sketch designs do not
depend on any input distribution, they often achieve better perfor-
mance under skewed distributions as shown by theoretical analysis
[19] and empirical studies [12]. This motivates us to specifically
track the largest flows, or top-k flows, in the fast path, where k is
configurable depending on the available memory space.
However, tracking only top-k flows is insufficient, since it will
inevitably miss information of small flows, which are also crit-
ical for connectivity-based statistics (e.g., DDoS, superspreader,
and cardinality). Clearly, tracking all small flows in the fast path
is infeasible, as the CPU and memory overheads become expen-
sive. Fortunately, sketch-based solutions map flows to counters
and leverage the counters to estimate various flow statistics. Our
observation is that the values of sketch counters contributed by
small flows are generally small and also have low variance when
compared to large flows. Thus, we only need to track the overall
characteristics of small flows instead of their individual flow head-
ers and sizes. Specifically, we employ a global variable to track the
total byte count of these flows, and use it to infer the specific sketch
counter values later in the control plane.
Solution overview: To this end, we design a fast and accurate
top-k algorithm for our fast path. Our algorithm builds on Misra-
Gries’s top-k algorithm [33]. However, Misra-Gries’s algorithm has
two limitations that prohibit high performance and accuracy. First,
in order to kick out a small flow and add a (potentially) large flow,
it performs O(k) operations to update k counters in a hash table;
the overhead becomes significant when there are many small flows
to kick out. Second, it has loose bounds on the estimated values
of the top-k flows. To overcome both limitations, we combine
the idea of probabilistic lossy counting (PLC) [15], a probabilistic
algorithm that improves accuracy for tracking skewed data, with
Misra-Gries’s algorithm. Specifically, we kick out multiple small
flows each time, obviating the need of performing O(k) counter
update operations for kicking out each flow (i.e., we amortize the
operations over multiple kick-outs). Also, instead of using one
counter per flow, we carefully associate three counters with each
flow to provide tight per-flow lower and upper bounds.
4.2 Algorithm
Data structure: We maintain a hash table H that maps flow head-
ers (hash keys) to counters (hash values). We configure H to hold
at most k flows. Each flow f is associated with three counters.
• ef : the maximum possible byte count that can be missed before
• rf : the residual byte count of f .
• df : the decremented byte count after f is inserted.
f is inserted.
Find the largest two values a1 and a2 and the smallest value ak +1
Compute θ = logb( 1
Return ˆe = θ√
2), where b = a1−1
a2−1
1 − δ ak +1 for some small δ
Algorithm 1 Fast Path Algorithm
Input: packet (f , v)
1: function ComputeThresh(a1, a2, · · · , ak +1)
2:
3:
4:
5: procedure UpdateBucket(f , v)
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
V = V + v
if f has an entry (ef , rf , df ) in H then
Update the entry with (ef , rf + v, df )
else if H is not full then
Insert f to H and set H[f ] = (E, v, 0)
ˆe = ComputeThresh({r❕ |❕ ∈ H } ∪ {v })
for all key ❕ ∈ H with H[❕] = (e❕, r❕, d❕) do
Update H[❕] with (e❕, r❕ − ˆe, d❕ + ˆe)
if r❕ ≤ 0 then
Insert f to H and set H[f ] = (E, v − ˆe, ˆe)
if v > ˆe and H is not full then
else