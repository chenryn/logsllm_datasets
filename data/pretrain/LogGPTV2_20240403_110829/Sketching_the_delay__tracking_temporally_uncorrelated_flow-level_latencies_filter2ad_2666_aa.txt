title:Sketching the delay: tracking temporally uncorrelated flow-level latencies
author:Josep Sanjuàs-Cuxart and
Pere Barlet-Ros and
Nick G. Duffield and
Ramana Rao Kompella
Sketching the Delay: Tracking Temporally
Uncorrelated Flow-Level Latencies
Josep Sanjuàs-Cuxart
UPC BarcelonaTech
Jordi Girona 1-3
Barcelona 08034, Spain
PI:EMAIL
Pere Barlet-Ros
UPC BarcelonaTech
Jordi Girona 1-3
Barcelona 08034, Spain
PI:EMAIL
Nick Dufﬁeld
AT&T Labs–Research
180 Park Avenue
Ramana Rao Kompella
Dept. of Computer Science
Purdue University
Florham Park, NJ 07932
dufﬁPI:EMAIL
West Lafayette, IN 47907
PI:EMAIL
ABSTRACT
Packet delay is a crucial performance metric for real-time,
network-based applications. Obtaining per-ﬂow delay mea-
surements is particularly important to network operators,
but is computationally challenging in high-speed links. Re-
cently, passive delay measurement techniques have been pro-
posed that outperform traditional active probing in terms of
accuracy and network overhead. However, such techniques
rely on the empirical observation that packet delays across
diﬀerent ﬂows are temporally correlated, an assumption that
is not met in presence of traﬃc prioritization, load balancing
policies, or due to intricacies of the switch fabric.
We present a novel data structure called Lossy Diﬀerence
Sketch (LDS) that provides per-ﬂow delay measurements
without relying on any speciﬁc delay model. LDS obtains
a notable accuracy improvement compared to the state of
the art with a small memory footprint and network over-
head. The data structure can be sized according to target
accuracy requirements or to ﬁt a low memory budget.
We deploy an actual implementation of LDS in an opera-
tional research and education network and show that it ob-
tains higher accuracy than temporal correlation-based tech-
niques without exploiting any knowledge about the under-
lying delay model.
Categories and Subject Descriptors
C.2.3 [Computer-Communication Networks]: Network
Operations—network monitoring; C.4 [Performance of Sys-
tems]: General—measurement techniques; G.3 [Probability
and Statistics]: General—Probabilistic algorithms
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’11, November 2–4, 2011, Berlin, Germany.
Copyright 2011 ACM 978-1-4503-1013-0/11/11 ...$10.00.
General Terms
Algorithms, Measurement
Keywords
One-way packet delay, network latency, network measure-
ment, delay sketch
1.
INTRODUCTION
Packet delay has become a key network performance met-
ric, together with other metrics such as throughput and
packet loss. This growth in importance of packet delay is
mainly due to the emergence of a new class of network-based
applications that demand extremely low end-to-end latency.
For instance, algorithmic trading applications require end-
to-end latencies to not exceed few microseconds, otherwise
they may lose signiﬁcant amount of revenue in the form of
lost arbitrage opportunities [21]. High-performance comput-
ing applications form another class of such applications with
message latencies directly impacting the amount of time it
takes for the job (e.g., weather simulation) to be ﬁnished.
Finally, modern data center applications have soft real-time
deadlines [3] that typically are in the order of milliseconds,
but once backend computation requirements are factored in,
very little time is left for network delays.
Now, consider a network operator that is running and
managing a network environment that supports low-latency
applications, such as a data center network. Typically, many
data centers host several thousands of machines connected
via a network fabric that is often constructed out of com-
modity networking equipment (e.g., switches and routers).
Depending on the requirements (e.g., full bi-section band-
width), the network is often connected in a multi-rooted tree
topology (e.g., a fat-tree) with several thousand switches
providing multiple paths between servers for load-balancing
purposes. Further, the cluster itself may be shared across
several tens to hundreds of customers running tens to hun-
dreds of diﬀerent applications with potentially very diﬀer-
ent network usage patterns. Given the complexity stemming
from the sheer number of network elements as well as the va-
riety of networking-based applications, it becomes extremely
diﬃcult to debug and troubleshoot latency anomalies (such
483as delay spikes) throughout the network without proper la-
tency measurement tools at various points in the network.
Traditionally, such measurements have been obtained us-
ing active probing in wide-area ISP networks [5, 24, 6, 28].
However, end-to-end network delays are an order of mag-
nitude smaller in data center networks—order of microsec-
onds compared to milliseconds. To capture delay dynam-
ics at such microsecond granularity, high probing frequency
(e.g., 10,000Hz [17]) is required, which makes this approach
prohibitively expensive in practical scenarios. Further, diag-
nosing end-to-end delay anomalies requires measurements at
various vantage points in the network—ideally, at each pair
of interfaces within each switch in the network, so that the
root cause can be localized down to a router or a switch. The
network operator could then conduct a more extensive anal-
ysis, such as study the set of customers or applications that
may be routed through that switch to carefully investigate
the root cause of the problem.
Unfortunately, native switch/router support for packet de-
lay measurements is sorely lacking. Today, NetFlow and
SNMP form the two main measurement solutions that a
router supports. Neither, however, focuses on delay mea-
surements. In some environments such as the London Stock
Exchange, operators resort to specialized measurement boxes
(e.g., Corvil [1]) that can detect these delays at high ﬁdelity.
However, because of the high costs and the hassles of admin-
istering a new box in the network, such an approach does not
scale well. The complexity of packet latency measurements
comes fundamentally from the fact that we cannot easily just
store a packet timestamp at two monitoring points, without
incurring high storage and communication complexity, since
the complexity is linear in the number of packets. It is there-
fore important to overcome the linear relationship between
number of collected timestamps and network overhead for
any solution to be scalable.
Recent work [17] proposed the lossy diﬀerence aggrega-
tor (LDA) to overcome the linear relationship between sam-
ple size and network overhead by intelligently aggregating
timestamps between the two measurements points. LDA,
however, provides only aggregate latency estimates across all
packets, which may be inadequate for diagnosing customer-
speciﬁc or application-speciﬁc latency issues [18]. As pointed
out by prior work [18], ﬂows may exhibit signiﬁcant diversity
in their latency characteristics at a given router, and hence,
per-ﬂow measurements are important for network operators.
Unfortunately, the problem of measuring per-ﬂow delay is
harder in the environments we consider, since the number
of ﬂows can be quite large; collecting and exchanging per-
ﬂow state becomes prohibitively expensive.
The problem of measuring per-ﬂow delay has been very
recently explored in [18, 19]. Both papers exploit the key
observation that packets exhibit signiﬁcant temporal delay
correlation in speciﬁc settings, i.e., packets that are trans-
mitted close in time experience similar delays, even if they do
not belong to the same ﬂow. RLI [18], the most recent of the
two, exploits this observation to inject simple active probes
periodically and uses linear interpolation to estimate per-
packet delay. At the downstream monitoring point, these
estimated per-packet delays can be aggregated into per-ﬂow
latencies using only three counters per-ﬂow.
While the assumption that packets exhibit temporal cor-
relation is valid in a restricted subset of systems, this as-
sumption does not hold true in more general scenarios where
there is prioritization across packets with two or more paral-
lel queues. For example, many modern routers support dif-
ferent queuing for prioritizing real-time traﬃc (e.g., VoIP,
video) over regular data transmissions (e.g., Web). Thus,
in these cases, there exists very little correlation between
the delays of packets that end up traversing two diﬀerent
queues. Similarly, in many modern data center networks,
packets are routinely load balanced across multiple paths us-
ing ECMP—temporal delay correlation may potentially ex-
ist across any given path but certainly not across paths. Fi-
nally, modern switch fabrics (e.g., Clos-network-based switch
fabrics used in Juniper’s T-Series routers [2]) are often com-
posed of intermediate stages of switching with each packet
being sent to a random intermediate location; the latency
of a packet through the router may be diﬀerent depending
on the path within the router. (In such switches, packets
are re-sequenced back because TCP does not interact well
with reordering, but such reordering needs to be only on a
per-ﬂow basis.)
Thus, the assumption of temporal delay correlation is not
universally applicable; unfortunately, schemes such as RLI
will produce grossly inaccurate latency estimates if the as-
sumption does not hold, posing a major hurdle for deploy-
ing RLI on a global basis. Switch vendors do not wish to
be bothered about the speciﬁcs of the deployment scenario,
and instead would like to have one scheme that is universally
applicable across all possible scenarios. Our objective in this
paper is to accomplish this task. Speciﬁcally, we focus on de-
vising a scalable delay-model-agnostic mechanism to obtain
per-ﬂow latency measurements at microsecond granularity
across two measurement points in the network.
In this paper, we propose a technique called lossy delay
sketching (LDS) that essentially combines the model inde-
pendence nature of LDA with sketching techniques that do
not rely on per-ﬂow state to obtain model-free and scalable
per-ﬂow delay estimation. LDS essentially maintains a series
of hash buckets, with each bucket consisting of a timestamp
sum and the number of packets that hash to the bucket
(similar to an LDA bucket). In accordance with the spirit of
sketching, LDS maps each ﬂow to a random subset of buck-
ets, that are potentially shared (partially or fully) by other
ﬂows. To minimize the eﬀect of interference, we randomize
the fate-sharing by maintaining diﬀerent banks of buckets,
similar to a sketch, with a diﬀerent hash function.
While the basic idea of blending LDA with sketches makes
intuitive sense, several problems must be overcome to design
such a data structure. For instance, ﬂows may diﬀer in their
delay properties as well as their sizes signiﬁcantly. It is im-
portant to ensure the interference due to collisions does not
impact the accuracy of the ﬂow’s latency estimates. We
present theoretical analysis on determining the size of LDS
in order to reduce this interference.
Thus, the main contributions of this paper are as follows.
• We propose a new data structure called LDS that ob-
tains per-ﬂow delay estimates and that does not rely
on delay models (Section 2). It blends LDAs that are
model independent with sketching techniques that pro-
vide per-ﬂow measurements without per-ﬂow state.
• We present a comprehensive theoretical analysis of the
data structure and show how to size it to achieve the
desired accuracy (Section 3).
484• We introduce a series of practical enhancements to
LDS that allow network operators to ﬁne-tune the data
structure for the speciﬁcs of an actual deployment sce-
nario (Section 4).
• We evaluate LDS with real traﬃc collected at a large
academic network (Section 5). Our results indicate
that sketching is superior to existing techniques when
temporal correlation is not present. Sketching is par-
ticularly accurate for large ﬂows, even in the presence
of loss. Additionally, the accuracy of a selected subset
of ﬂows can be easily incremented.
Finally, Section 6 covers the related work in the literature,
while Section 7 concludes the paper.
2. DELAY SKETCHING
Our main goal is to measure the one-way delay introduced
by a network between two measurement points on a per-ﬂow
basis. While we can typically support any deﬁnition of ﬂow,
usually, this will consist of the 5-tuple formed by source
and destination IP addresses, originating and destination
ports, and protocol. We mainly focus on obtaining per-ﬂow
average latency, but we also outline in Sec. 4 how we can
obtain second moments as well.
Our architecture is oblivious to what locations exactly
constitute the measurement points. Thus, we can imagine
obtaining per-ﬂow measurements within a switch or a router
across an ingress and egress interface. Alternately, we can
obtain measurements across two diﬀerent routers. Note that
both measurement locations are merely viewpoints along the
path that packets follow, and do not need to be (although
they could be) the emitter or ﬁnal destination of the traﬃc.
We call the ﬁrst measurement point sender , and the sec-
ond, receiver . We consider the reverse path measurements
separately with the receiver becoming the sender and vice
versa.
2.1 Assumptions
Single stream. We assume that the sender and receiver
observe the same stream of packets.
In general, this is
highly dependent on the particular scenario. For instance,
suppose we consider an ingress (egress) switch interface as
the sender (receiver). The receiver (sender) may obtain
(transmit) packets from (to) many diﬀerent ingress inter-
faces. Thus, we assume there is a simple way to ﬁlter out
the packets that travel from the sender to the receiver. Note
that we do not assume packets ﬂow through a single queue,
or even in a FIFO order—just that we have a way to sep-
arate out packets that appear at both the sender and the
receiver. Within switches, there are often internal headers
that contain the port at which they originated and the port
to which they are headed to, that we can use for this pur-
pose. Across routers, we can leverage preﬁx-based ﬁltering
to identify the set of packets that travel through one given
path (forwarding is preﬁx-based). Such routers do not need
to be co-located or close in terms of network hops.
Packet loss. We assume packets can be lost between the
sender and receiver. Depending on the scenario, the packet
loss rates may diﬀer signiﬁcantly. For example, in a ﬁnancial
trading network, we may imagine the network to suﬀer from
minimal packet loss. However, in a real backbone network,
packet loss may be slightly more common. Typically, while
some amount of loss resilience is required in our data struc-
tures, we assume the loss rates are still quite low (say <1%)
as TCP may not work well under higher loss rates.
Time synchronization. We also assume the clocks of the
sender and receiver are synchronized. This is a common re-
quirement of one-way packet delay measurement techniques
[17, 19, 18]. Although techniques have been proposed that
do not require clock synchronization (e.g., [25, 22, 29]), re-
moving this assumption was out of the scope of this pa-
per. To achieve ﬁne grained precision, packet timestamping
clocks can be synchronized to the GPS signal (i.e., using
Endace DAG cards), or using the IEEE 1588 protocol [16].
Both these methods are capable of sub-microsecond preci-
sion and thus suitable for our needs [10]. (In our evaluation,
we obtained traces from a production network that already
uses IEEE 1588 protocol to synchronize measurements.)
Embedding timestamps in packets. Similar to prior work
[17, 18], we assume that it is not possible to embed times-
tamps within IP packets because existing IP headers do not
have a placeholder for timestamps, and it would require sig-
niﬁcant changes to router forwarding paths and other third-
party components making it diﬃcult. We note however that
our solutions are important even in the context where router
vendors can put a timestamp in a packet, as the number of
ﬂows may be still large.
In fact, for ease of exposition, we present a simple data
structure called SDS using the timestamp assumption, i.e.,
assuming packets can be embedded with timestamps. We
will, however, get rid of this assumption in Sec. 2.3 when we
describe our main data structure LDS.
2.2 Simple Delay Sketch (SDS)
As mentioned before, we initially assume the sender can
embed a timestamp into the packets to be measured for easy
exposition of the delay sketching idea. (We will relax this in
the next section.) Thus, the receiver can obtain delays for
each packet, but still need a scalable mechanism to store per-
ﬂow latencies, which is obtained by the data structure we
describe in this section. The main idea of our technique is to
explore sketching techniques that have been studied before
in the literature to obtain measurements without maintain-
ing per-ﬂow state, and requiring very few memory accesses
per packet. Such techniques will allow us to compute a com-
paratively smaller compressed summary of the traﬃc that
allows recovering approximate measurements. We assume
measurements are performed in ﬁxed time intervals, which