1. Threat tree analysis;
2. Threat database search;
3. Ad-hoc threat identification.
4.1.1
Threat Trees
Threat tree analysis is similar to a fault tree used in hardware engineering. It is
based on a method in which risks are decomposed and charted into a decision tree.7
Threat trees are widely used in risk management and reliability engineering. The
problem with threat trees is that you need to be a security expert to build and to use
them.
Building a threat tree involves identifying a complete set of threats on a high
level, and then introducing more details on how each threat could be realized and
what weaknesses need to be present for the threat to be present. The tree view
comes from the analysis technique. The root of the tree is the highest abstraction
level and defines the threat against a specific asset. Each subsequent abstraction layer
refines the threat, providing more information, becoming a root of a more detailed
subtree. Finally, leaf nodes will provide adequate information for countermeasures
to eliminate the threat. Identifying the root causes for the threats often requires
security knowledge, and therefore a threat tree might not be feasible for the design-
ers of the software to build with an adequate level of detail.
The threat tree method is usually the most effective approach if the security
problem and its surrounding environment are both well defined and understood by
the person conducting the threat analysis. In the context of fuzzing, the problem
with threat trees is that although they help in designing a good piece of software,
they do not necessarily help you at all in building your fuzzer. To some, fuzzing
is just an additional countermeasure in a threat tree. But turning the threat tree
upside-down, we will immediately see that fuzzing will not guarantee that it will
eliminate all risks where it is listed as a countermeasure. We are not saying that
threat trees are useless for fuzzing—on the contrary! Threat trees help you under-
stand the risks involved with the application at hand and will help you choose the
104
Fuzzing Metrics
7Edward Amoroso. (1994). Fundamentals of Computer Security Technology. Upper Saddle
River, NJ: Prentice Hall.
right fuzzers for testing a specific application. But fuzzing is not a silver bullet that
will resolve all threats in your analysis.
4.1.2
Threat Databases
Threats in various domains tend to be quite similar, and therefore an existing threat
analysis built for a particular system can be reapplied to other systems in the same
domain. Studies of threats in various domains are available to be used as databases
for threat analysis. The benefit is that the person building the threat analysis can
easily analyze whether that threat will apply to the design of this software, without
a deep understanding of the security domain. The disadvantage is that a threat that
is unique to the application being built might be missing from the database, and
therefore, be left out of the analysis. Also, the threat descriptions could be too abstract
to be usable without further understanding on how that threat is realized in the real
world. An example of a bad threat definition is “denial of service threat,” some-
thing you commonly see in threat analysis documents. Almost any attack can
potentially result in the system crashing or becoming nonresponsive, and binding
the DoS threat to one single failure or flaw can distract attention from other causes
of DoS. Therefore, it is possible that the threat analysis could potentially miss the
most significant weaknesses in the system. The level of detail for each threat in such
a database is critical. Simple search of threats from an existing database, or enumer-
ation of a list of common threats, may suggest the applicable threats more effi-
ciently than methodological threat tree analysis, especially when the problem is
defined in general terms applicable to that domain or when the final user environ-
ment is not limited when building that specific component. This applies especially
to software developers who do not have knowledge of the actual environments in
which their products will be used.
As an example, let us examine the overview of the threat taxonomy built and
maintained by the VoIP Security Alliance (VOIPSA), depicted in Figure 4.1.8 By
analyzing these threats, we can see that fuzzing would apply to the following threat
categories:
Interception and Modification
• Message Alteration (Modification)
Denial of Service
• VoIP Specific DoS: Request Flooding, Malformed Requests and Messages,
Spoofed Messages
• Network Services DoS
• OS/Firmware DoS
And this list is not even complete, as fuzzing can find many other threats that
are not enumerated in the VOIPSA threat listing.
4.1
Threat Analysis and Risk-Based Testing
105
8VoIP Security Threat Taxonomy by VoIP Security Alliance (VOIPSA). www.voipsa.com/Activities/
taxonomy.php
4.1.3
Ad-Hoc Threat Analysis
Ad-hoc threat analysis is a well-suited practice when the goal is to find one weak-
ness in a very short period of time during an assessment. An experienced security
analyst can immediately recognize potential threats in a system, but for a developer
it can be very challenging to think about the application from the perspective of an
attacker. For fuzzing, a simple method for ad-hoc threat analysis might be based on
listing the available interfaces in a system to enumerate the attack surface. For
example, Dmitry Chan conducted a UDP port scan against Motorola Q mobile
phone, with the following results:9
106
Fuzzing Metrics
Figure 4.1
VOIPSA threat listing for VoIP.
9http://blogs.securiteam.com/index.php/archives/853
42/udp open|filtered nameserver
67/udp open|filtered dhcps
68/udp open|filtered dhcpc
135/udp open|filtered msrpc
136/udp open|filtered profile
137/udp open|filtered netbios-ns
138/udp open|filtered netbios-dgm
139/udp open|filtered netbios-ssn
445/udp open|filtered microsoft-ds
520/udp open|filtered route
1034/udp open|filtered activesync-notify
1434/udp open|filtered ms-sql-m
2948/udp open|filtered wap-push
You should note that the result above does not indicate anything about the real
network services implemented in the phone and definitely will have a number of
false positives (i.e., services that are not implemented) and false negatives (i.e., miss-
ing some of the implemented services). Still, a security analyst working for the man-
ufacturer can easily narrow down this list to the truly open services and conduct a
detailed threat analysis based on those results. Note that a port scan is not a full
threat analysis method, but merely a tool that can be used as a starting point in
identifying the attack vectors for further analysis of the potential threats in a sys-
tem. It is typically the first technique a security analyst will conduct when starting
a technical security audit.
Different threat analysis techniques are useful at different phases. Whereas our
example for the threat analysis of a complete product only applies to later phases
of the product life cycle, the other discussed techniques such as threat tree analysis
target earlier phases, when the product is not yet ready for practical analysis.
4.2
Transition to Proactive Security
A software product life cycle can be considered starting from the requirements col-
lection phase of software development, and ending when the last installation of the
software is retired from use. The software development life cycle is a subset of the
product life cycle. Although various software development models are in use, the
waterfall model can be applied to generalize these phases. After launch, the released
software enters a number of update and maintenance cycles, until it finally is retired
or replaced by a subsequent version of the software. A simplified product life cycle
consists of the following phases:
Pre-Deployment (development):
• Requirements and Design
• Implementation
• Development Testing
• Acceptance Testing
4.2
Transition to Proactive Security
107
Post-Deployment (maintenance):
• Integration testing
• System maintenance
• Update process and regression testing
• Retirement
Whereas quality assurance aims at improving and measuring the quality of the
system proactively during the software development, the practices in vulnerability
assessment focus on the discovery of (security) critical flaws in the launched prod-
ucts. The difference traditionally is only with the test purpose and in the time of
test related to the software life cycle. Vulnerability assessment provides an assur-
ance metric by thoroughly testing a subset of the system and extrapolating the find-
ings to the whole system. Fuzzing is useful in both approaches.
Fuzzing should start as early in the product life cycle as possible. Early discov-
ery, and the elimination of the found defects, has clear observable cost-benefits. The
time of the discovery of security flaws is especially critical, because security flaws
discovered after the product launch have significant costs compared to other defects
in software. Companies found to have many post-deployment security bugs can
also gain a bad reputation for lack of security.
The purpose of fuzzing in general is to find security defects. The standard met-
rics of IT operations for up-time, system recovery, and change control can be used
for related security incidents.10 Excluding indirect costs such as brand value and
reputation, the direct costs related to software vulnerabilities can be divided in at
least the following categories:
1. Cost of discovery of the defects through internal or external audits;
2. Cost of remediation of the flaw in product development and regression
testing;
3. Cost of security compromises and down-time, or in some cases direct dam-
age to the failing systems;
4. Cost of patch deployment and other change control related tasks to the cus-
tomer systems if already deployed.
4.2.1
Cost of Discovery
Security defects need to be discovered before they can be fixed. The costs related to
the discovery of the flaws depend on the used resources and methodologies. Secu-
rity defects are found in all phases of the software lifecycle, from development until
retirement, and different methods are used in the discovery.
The four basic fuzzing-related methods for defect discovery are
• Bug bounty hunters;
• Subcontracted security assessments;
108
Fuzzing Metrics
10Andrew Jaquith. (2007). Security Metrics: Replacing Fear, Uncertainty, and Doubt (pp. 68–73).
Boston: Addison-Wesley.
• Internally built fuzzers;
• Commercial fuzzing tools.
The costs associated with the discovery of the flaws are different in these four
methods. A bug bounty hunter can ask for a fixed amount per found defect,
whereas a subcontracted security consultant typically invoices for the spent hours,
or per predefined security assessment project. Internally built testing tools involve
development, use, and maintenance-related costs. Third-party software (free or com-
mercial) involves potential initial investment, usage costs (affected by the ease-of-
use of the products), and future maintenance costs.
We will ignore the costs related to actually fixing the flaws for now and focus
on the cost of discovery. You can try to predict the cost of defect discovery with
three simple metrics, whether third parties or internal people perform the fuzzing
process:
• Time it takes to conduct the tests;
• Number of tests required for adequate test coverage;
• Mean probability of discovering a failure per each test.
The resulting metric from the necessary testing time, number of tests, and the
probability of failure of discovery would indicate the forecasted cost (and after the
project the true cost) per failure. Note that this is different from the cost per bug,
as each failure needs to be verified to be an individual defect in the software during
the repair process. It is important to separate these, because the actual bugs in soft-
ware can be difficult to enumerate while tests are being conducted.
Adding more test automation (such as fuzzing) and processing capabilities will
reduce the test execution time. For a security test bed, you could use a server farm
full of virtual or true computers with an image of the system under test. This can be
used to drive the number of trials per hour as high as possible. In such a setup you
need to consider costs related to setting up and maintaining the farm of test targets
and the tools required to be certain that all the results are being correctly captured.
Test execution time can depend on the fuzzer. Conducting one million tests with one
tool can take less time than ten thousand tests with another slower tool.
The defect discovery probability indicates the efficiency of the tests. This is really
where the value comes from intelligence in fuzzing. Smart fuzzers bring enormous
value to the testers if the selected tool finds 1,000 tests that crash the software and 200
tests that leak memory, and the additional 10 tests that take 10 times more process-
ing power from the SUT, but the discovery of those failures only takes 10,000 tests.
That would result in a 12.1% failure efficiency. On the other hand, a fuzzer based on
random testing can conduct one million tests, with one thousand failures, resulting in
0.1% failure efficiency. However, also remember that just calculating the number of
failures does not indicate the number of bugs responsible for those failures. There may
only be 20 bugs in the system that can be discovered with fuzzing. These example cal-
culations for failure and defect efficiency for two fuzzers are shown in Table 4.1. 
Let us next look at the costs related to the choice of tools for fuzzing. Whereas
commercial tools can be immediately available and low-risk for the return of invest-
ment from test efficiency perspective, these tools are sometimes ridiculously expensive
4.2
Transition to Proactive Security
109
compared to hiring someone as a contract programmer to develop your own fuzzer.
If you decide to create your own one-off fuzzer tailored to your exact testing needs,
the first decision you need to make is whether to build a smart or dumb fuzzer. As
we’ll see later, it might make sense to build both. One could think that the obvious
answer would be to create a fuzzer that is most likely to yield the most bugs of any
type. Unfortunately, when we start a new fuzzing project, it will be very difficult to
estimate the success rate proactively. In general, if a particular software product has
never been fuzzed before, you will almost always find a significant number of bugs.
But you might be interested in how many flaws you will find with different types of
fuzzers and what type of investment is required.
Before starting any do-it-yourself fuzzing projects, it is important to really ana-
lyze the total cost for all choices. Commercial tools are easily analyzed based on
standard practices in security investments, such as Return On Security Investment,
or ROSI, calculations. Even if it is internally developed, it is still a security invest-
ment. There are many aspects to consider:
1. Which approach finds most individual failures or flaws (or test efficiency):
This is the actual return for the investment in fuzzing. Efficiency of the tests
will be the final measure of how successful the project was, but unfortu-
nately, that is very difficult to predict beforehand. Still, it would be easier if
we could also give a dollar value to efficiency. But what is a “good enough”
test result for the tool? How many defects were left in the product after the
tests? This question is similar to asking “What is good enough anti-virus
software?” You would not survive in the security tools market if your solu-
tion only caught 50% of the issues compared to your competitor. 
2. Cost to implement tools (or investment in the tools): This includes the costs
from the work-time to develop the tool, often calculated in man-months.
Note that the person developing the fuzzer might not be your average soft-
ware developer, and his or her time might be taken from crucial security
analysis tasks. Many fuzzer developers think this is a fun task and might
jump into the task without considering the priority against other security
assessment tasks.
3. Time to implement tools (typically zero, if a third-party tool is acquired):
In addition to the actual costs related to development time, fuzzer develop-
ment can influence the time it takes to test the product for release, and
therefore delay the launch of the product. Fuzzing can be a part of the final
110
Fuzzing Metrics
Table 4.1
Example Calculation of Failure and Defect Efficiencies
Smart Fuzzer
Random Fuzzer
Number of tests
10,000
1,000,000
Test execution time
2 hours
20 hours
Number of failures found with tests
1,210
1,210
Failure efficiency
12.1%
0.121%
Failures per hour
605
60.5
Number of defects found with tests
20
20
Defect efficiency
0.2%
0.002%
Defects per hour
10
1
approval gate for a software release, and therefore, the time it takes to
develop testing tools will delay the release.
The first three metrics represented development costs for the fuzzers themselves.
The rest of the metrics should apply to both internally developed fuzzers, freely
available open source fuzzing frameworks, and commercial fuzzing tools:
4. Time from availability to use (time used to test design and integration):
When a fuzzer framework becomes available, it does not necessarily mean
you can launch your testing activities immediately. Valuable time is often
spent on integrating the tool into an existing testing framework or test bed.
For some interfaces under test, a new tool can be launched immediately
when it becomes available. For some interfaces you need to follow a
process of collecting data, building fuzzing rules, and finally adding thor-
ough instrumentation tools into your test bed.
5. Resources needed to test (required manpower to conduct tests): Finally,
the tests are ready to be launched. Different fuzzing techniques have differ-
ent needs for the necessary personnel to be present at execution time, and
for the time it takes to conduct test analysis.
6. Time needed to test (again causes delays in the product/service launch):
Different fuzzing techniques have varying test execution times. Some tests
can be executed in a matter of hours, whereas other tests can take several
weeks to run.
7. Other costs in test environment (HW + maintenance people for the test
setup): The costs to test setup can be enormous, especially with fuzzing
tools that have long test execution times. Commercial fuzzing appliances
usually come bundled with all the necessary test equipment. For software-
based solutions, you need to dedicate hardware resources for the test exe-
cution. Maintenance personnel for the test facility are also dedicated costs,
but can also be shared between different test setups.
8. Maintenance costs (is the testing one-off, or is it reusable and maintained):
Finally, fuzzing is not usually a one-off testing process. Fuzzing is also a
critical part of regression testing, and as such needs to be maintained for
the distant future. A proprietary tool developed several years ago might be
impossible to update, especially if the person who developed it is no longer
available to do so. The maintainability of the tests and dedicated resources
for keeping the fuzzing tools up to date are often the main point why peo-
ple switch from internally built tools to commercial tools.
There are benefits to each fuzzing approach, and the decision needs to be made
based on your own value estimations. Most metrics can be measured in relation to
financial investment. Some people value time more than others, so it would be sim-
pler if one could give a dollar value also for time in the equation, for example, indi-
cating how much a delay of one week in the launch of a product or service will cost