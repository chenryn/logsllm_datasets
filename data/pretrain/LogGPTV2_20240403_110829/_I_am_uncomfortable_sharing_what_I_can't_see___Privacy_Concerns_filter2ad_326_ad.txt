In our qualitative data, we found several concerns raised by
the participants for their information disclosure with human-
assistants, which we describe next.
5.2.1 Volunteers and agents: Institutional trust
Our participants shared extreme privacy and security concerns
about volunteers and agents that varied based on impersonal
trust and the anonymous nature of the interaction.
Privacy and security concerns: Participants expressed
strong privacy and security concerns while seeking help from
the volunteer or agent-based assistive systems. They were
concerned about identity theft, misuse of their information,
or criminal behavior. They were not comfortable revealing
private information with a total stranger whereas they were
comfortable with general objects such as food items.
“I would feel extremely uncomfortable with the visi-
bility of all the items which are personal to me or to
a coworker because they could be potentially mis-
used by the stranger who is looking at the picture.
Anything that has information that discloses some-
one’s identity or contains conﬁdential information
should not be shared so that makes me extremely
uneasy. Food items are common and not personal
to me so I am somewhat comfortable with them
being visible in the picture.”
[P100]
Impersonal trust: Prior research shows that ‘impersonal
trust’ (where trust is not based on immediate personal re-
lationships) can inﬂuence interactions between people and
institutions [36, 65]. In our study, we also observed imper-
sonal trust as participants mentioned trusting an agent from a
professional organization more than a random volunteer. A
few of our participants (6.1%, N=9) indicated relying more
on a paid professional agent11 with their sensitive information
rather than a volunteer.
“I try to only share what’s relevant to my question
and would never intentionally share private info
with a volunteer, only a paid and traceable profes-
sional.”
[P140]
Participants believed that their privacy and security will be
more protected with the professional agents as the organiza-
tion has a privacy policy and trained agents.
“The service I use most has agents who are back-
ground checked, highly trained, and who are obli-
gated to follow a clearly deﬁned privacy policy. I
would not allow a volunteer to see my credit card,
for example, while I would let the trained agent do
so without a second thought.”
[P154]
Anonymous interaction: We found that participants are
more comfortable sharing general objects with volunteers
rather than their family members. Due to the anonymity of
interactions with volunteers, participants were less concerned
about sharing information, such as messy surroundings and
body parts, and anticipated volunteers to be less judgmental.
“I am very comfortable with who I am and if I use
such assistance I understand that the other person
is there to help and not to judge my appearance or
surroundings.”
[P142]
5.2.2 Family: Ultimate support and trust
Participants reported family as the most reliable source for
seeking support. However, the anxieties of being a burden to
the family often limited them from soliciting aid from family.
11At the time of this writing, Aira charges $29 USD per month for 30
minutes of service.
USENIX Association
29th USENIX Security Symposium    1939
Trust and reliance: We found that family is one of the most
trusted support systems for people with VIPs, and they are
comfortable sharing almost any kind of information with them
when seeking support. According to our participants, family
members often know them and understand their requirements,
hence they do not hide much from them.
“I trust my parents who I would be asking for as-
sistance, I don’t care or feel uncomfortable about
them seeing anything else around me. Not like I am
hiding anything or doing anything wrong.” [P61]
“Considering that this is my family, I am already
comfortable with them assisting me with my needs.
They assist me quite frequently, and are knowledge-
able and understanding to my needs.”
[P73]
Social costs of burden: Previous research has shown that
people can be reluctant to ask for help from their known net-
works to balance social costs [63]. People with disabilities
have enhanced concerns about appearing dependent and help-
less in front of their social groups [20]. We found a similar
concern in our study while seeking help from family and
friends. Despite trusting their family most, participants some-
times preferred not to disturb their family members. They
would avoid asking help from family if could manage issues
on their own or from other sources.
“I trust my family and friends but don’t like to
bother them if I can help it.”
[P47]
Some participants felt that asking for help from family mem-
bers may prove their dependency and helplessness and would
prefer alternatives.
“I don’t want to have to rely on my family members
to tell me what things are, but that’s because doing
that takes away my independence.”
[P59]
“I do not ask family members. I use ScripTalk for
medical prescriptions. I ask Aira or BeMyEyes. I
am concerned that most of these questions assume
one has family around or that we’d always be com-
fortable asking them things. Families can often
want to control things but if we use assistive tech-
nologies and agents, it’s better, I think.”
[P79]
5.2.3 Friends: Depends on the friendship
We noted concerns related to trust and impression manage-
ment when disclosing information with friends.
Privacy and trust: Several participants indicated trusting
their friends with all types of information. They believed that
friends would protect their information.
“I have complete trust in my friends, and in their re-
liability and keeping conﬁdential data safe.” [P32]
However, participants also expressed they might want to
avoid sharing some personal information with their friends in
order to protect their privacy.
“I wouldn’t mind showing food or maybe myself
but any private info depending who I was talking to
especially a credit card with all the scams going on
I wouldn’t really like, though I would try to make
sure that I didn’t show that stuff.”
[P27]
Unwanted exposure and impression management: Par-
ticipants may experience unwanted exposures while sharing
information with their friends. Several participants were con-
cerned about the situations when the image can be leaked or
disclosed to a wrong person other than the intended audience.
“I wouldn’t really want my friends to see ﬁnancial
or medical information. Also, if they have a picture
on their phone that contains personal info about me,
this creates an opportunity for someone other than
my friend to see the picture on my friend’s phone
(e.g., friend’s family members, romantic partner),
which would jeopardize the privacy and security of
the information.”
[P30]
He additionally expressed concerns about his identity at risk
of being leaked on social media and is aware of possible
security risks.
“The info in the picture could be posted on social
media or used against me in some malicious way. I
am very distrustful of social media.”
[P30]
6 Discussion
We ﬁrst summarize and contextualize our key ﬁndings and
then discuss broader implications for more ‘humanizing’ de-
signs of assistive technologies.
6.1 Key ﬁndings
Our results show that the information disclosure behaviors of
people with VIPs depends on the types of objects and human
assistants. Hayes et al. recently ‘shadowed’ people with vi-
sual impairments and studied how they obtain help from their
allies in face-to-face (ofﬂine) interactions [38]. Although they
studied only ﬁve participants, they observed the general theme
of people with VIPs being careful when selecting an ally to
provide assistance, highlighting the importance to study pri-
vacy in assistive applications. In the context of image sharing
by ‘lifeloggers,’ Hoyle et al. [41,42] did not study speciﬁc au-
diences, but they also found that participants were concerned
about private information (such as screens and other objects
with textual information), impression management, and the
presence of bystanders in their photos. Unlike their work,
however, our participants were more concerned about the
1940    29th USENIX Security Symposium
USENIX Association
privacy of bystanders than their own when it came to cap-
turing people in images.
In the context of information sharing with speciﬁc au-
diences, our participants shared strong concerns about
sharing personally identiﬁable information with crowd-
workers because of concerns about identity theft. This
ﬁnding is consistent with prior work showing people are
more willing to share private information with stronger social
ties [74]. We also found that participants were more com-
fortable sharing concerns about self-presentation with
family than with friends. However, we also found some
evidence12 that participants were more comfortable with with
crowd workers (weaker ties) than with friends (stronger ties).
In the same vein, Dosono et al. found that college Reserve
Ofﬁcers’ Training Corps (ROTC) students were more com-
fortable sharing personal crises related to impression man-
agement (e.g., physical injuries) with family and counselors
instead of their ROTC peers [28]. In general, anonymous
interactions have been shown to help in overcoming social
stigma and may be more appropriate for private exchanges
where more openness is desired [28, 47].
Consistent with prior work [7, 40, 66], women were more
concerned about their privacy than men. Female participants
were more concerned than male participants when it came to
objects related to impression management. Although prior
work has found that older adults can show both extremes of
privacy concerns [57, 67] with younger populations being
more pragmatic [67], our older participants were more con-
cerned about sharing background objects than the younger
participants. In the context of level of impairment, prior work
has found coping strategies such as ‘acceptance’ where people
with visual impairments (especially the totally blind) felt they
“had very little choice other than to accept the risks” [8]. One
might therefore expect people who are totally blind to be more
concerned about or interested in protecting their privacy. In-
terestingly, however our totally blind participants were less
concerned about their privacy than the low-vision partic-
ipants. It may be that people who are totally blind are less
aware of the possible privacy risks than people with low vi-
sion or are more willing to compromise their privacy because
they have become accustomed to a higher need for assistance
and ‘acceptance’ of less privacy in general.
Finally, prior work has found that people may have more
trust in volunteers compared to paid workers because of a
stronger perception of altruism and sincerity of the volun-
teer [39]. Qualitative analysis showed that some participants
trusted paid crowd workers more than volunteers with
their private information. The role of ‘impersonal trust’ in
such systems needs additional investigation, and how more
trust may be derived from volunteers or paid agents.
12The statistical signiﬁcance was marginal at p= 0.054.
6.2
Implications:
camera-based assistive technologies
Toward
humanizing
Although there is a growing body of work exploring the needs
of people with VIPs [16, 61, 62], our study yielded novel
privacy and security concerns of people with VIPs related
to their sharing of information with crowd workers and hu-
man assistants using camera-based assistive systems. Many
of these concerns were related to how camera-based assis-
tive systems were creating a lack of security in people’s
daily lives — that is, these systems were serving to fur-
ther marginalize their identities.
Broadly speaking, when populations are marginalized
based on their identities, they are placed at the edge, beyond
boundaries, or on the outside of what is considered normative,
and individuals and groups can be marginalized on various in-
tersections of their identity, such as their race, gender, sexual
orientation, socioeconomic status, or perceived ability [64].
Recent work has explored the ways in which algorithmic sys-
tems can marginalize people’s identities. Systems like facial
recognition software can serve to further marginalize people
with gender-ﬂuid identities, as these systems serve as gender
reduction mechanisms and may misidentify people who have
changed genders or who do not choose a gender [35].
In our study, the marginalization and subsequent lack of
security manifested in the relationship between the systems
and people’s identities for people with VIPs. Our identity
deﬁnes us as an individual; it is the sense of self that we
refer to and that others see us as, giving us security in our
daily lives [22, 31]. At their root, camera-based assistive tech-
nologies give people with VIPs the chance to regain security.
Giddens deﬁnes security as a stable mental state derived from
the continuity and predictability of routines, that is, a person
achieves a sense of trust and safety in their life through the
enactment and habitualization of routines [30]. For example,
the ability to pay bills or take a prescription generates a sense
of reliability in a person’s life; it is this sense of stability that
provides one with a sense of security about their existence.
In this context, we found that camera-based assistive tech-
nology can create insecurity. That is, through their use of
these systems, our participants were concerned about identity
theft and people ﬁnding out where they lived. Moreover, peo-
ple were also concerned with issues related to self-concept,
such as if friends caught a glimpse of their “messy” home
environments. Thus, we argue that in order to create more
private and secure assistive technologies, we must begin
to humanize assistive technology; that is, we must train com-
puter vision algorithms to better understand what kinds of
objects people might want others to (not) see, as well as be
cognizant of where we need to enforce human assistance as
opposed to algorithmic assistance. As a means of generat-
ing ways in which this can be operationalized at the system’s
design and implementation level, to humanize assistive tech-
nology means that we must pay more attention to context.
USENIX Association
29th USENIX Security Symposium    1941
Humanizing security as humanizing context. The way
in which scholarship has deﬁned context has gone through
various transformations over time. Context has often been
viewed, from a positivist perspective, as the setting where
action unfolds, where the setting is believed to be a static
entity, stable and separate from the activities taking place
therein [29]. Early on, however, Suchman’s [70] formative
work illustrated that context incorporates the activities of
humans, and people’s activities are neither stable nor prede-
termined. In building on this notion of context, Dourish [29]
argues that the determination of context cannot be made a pri-
ori, that is, context is an emergent property of interaction. In
this view, context is actively produced throughout the course
of interaction; it is determined by the people who are present
and in how they generate, together, the rules and norms for
their interaction. For example, if only one person is present in
their home (i.e., a homeowner), they may feel free to engage
in actions that they may otherwise feel uncomfortable with
others present, such as taking a shower with the door open.
When others are present, such as guests, the context shifts
and the rules and norms also change, and this same person
may not feel comfortable engaging in these same behaviors.
The continual shaping of context is related to impression
management [31], where people are trying to control how
they present themselves to others. In the context of social
media, people’s ability to engage in impression management
is a burden as people tend to collapse multiple audiences into
a single context [58]. This process of impression management
is increasingly complex for people with VIPs as how in some
cases people with VIPs present themselves to others is in-
visible to them. In this view, systems should be designed
such that they make context visible. Technical solutions
should therefore not just focus on ﬁnding PII in images, but
also look for situations that may affect one’s social standing.
One such implication is that people (as bystanders) who may
be concerned about being captured by assistive devices can be
made aware that other people will be removed through face
detection (for example) from assistive devices. As we found,
people with VIPs are highly concerned about the privacy of
bystanders, and Ahmed et al. study ‘up to [what] limit’ by-
standers are willing to be captured in such circumstances [7].
Given that camera-based assistive technologies utilize dig-
ital images to communicate with audiences, photos often
collapse several contexts together (i.e., a home environment,
driver’s license photo, prescription drug labels, and more).
Given that some of this information was not appropriate
for certain audiences, computer vision algorithms should
be designed more empathetically such that they detect
content deemed inappropriate for certain audiences and
blur them, redact them, or generate other novel solutions
that are context aware and thus sensitive to the desires