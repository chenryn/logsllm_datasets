title:Fault Tolerance Through Redundant Execution on COTS Multicores:
Exploring Trade-Offs
author:Yanyan Shen and
Gernot Heiser and
Kevin Elphinstone
2019 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)
Fault Tolerance Through Redundant Execution
on COTS Multicores: Exploring Trade-offs
Yanyan Shen∗ Gernot Heiser† Kevin Elphinstone‡
Email: ∗PI:EMAIL, †PI:EMAIL, ‡PI:EMAIL
UNSW Sydney and Data61, CSIRO, Australia
Abstract—High availability and integrity are paramount in
systems deployed in life- and mission-critical scenarios. Such
fault-tolerance can be achieved through redundant co-execution
(RCoE) on replicated hardware, now cheaply available with
multicore processors. RCoE replicates almost all software, includ-
ing OS kernel, drivers, and applications, achieving a sphere of
replication that covers everything except the minimal interfaces to
non-replicated peripherals. We complement our original, loosely-
coupled RCoE with a closely-coupled version that improves
transparency of replication to application code, and investigate
the functionality, performance and vulnerability trade-offs.
Index Terms—seL4; microkernel; SEU; replication; fault tol-
erance;
I. INTRODUCTION
Computer systems in control of life- and mission-critical
functions require high levels of integrity and availability, even
in the case of component failure. The standard approach
to achieving the required fault tolerance is to use dual or
triple modular redundancy (DMR or TMR, respectively),
where all critical functions (hardware as well as software)
are replicated [1]. Such redundant hardware architectures are
traditionally employed in scenarios where the cost of failure
is unbearably high.
Traditional redundant designs are expensive, in terms of
capital cost and often also in performance, they also tend to be
robustly engineered and correspondingly bulky and heavy [1]–
[4]. This creates space, weight and power (SWaP) problems
that limit the use of such systems.
Computer control of critical systems is rapidly becom-
ing more widespread, especially with the move towards au-
tonomous land and aerial vehicles, and the explosive growth
of small-satellite launches [5]. Many of those systems are
too cost- and SWaP-sensitive for traditional fault-tolerance
approaches. At the same time, on-going miniaturisation of
commercial off-the-shelf (COTS) processors is increasing their
vulnerability to transient faults, such as single-event upsets
(SEUs) caused by ambient ionising radiation [6]–[10]. In other
words, there is an increased risk of failure at the same time
as critical systems are becoming more widespread.
Recent progress in formal veriﬁcation has now made it
possible to achieve 100% reliability in critical software com-
ponents, such as the OS [11], ﬁle system [12], and security
protocol
implementations [13]. However, such veriﬁcation
inevitably assumes perfect hardware that always operates
according to its speciﬁcation. A single, transient bit ﬂip can
invalidate veriﬁcation assumptions, and can lead to security
violations, just as in unveriﬁed systems [14]–[16].
COTS hardware is far from perfect, and reliability issues
are well established [17]–[22], making hardware redundancy
particularly important. However,
the abundance of multi-
core processors, especially high-performance, energy-efﬁcient
systems-on-chip designed for phone use, makes processor
redundancy relatively cheap in terms of capital cost as well
as SWaP. We have previously shown that on a multicore, a
redundant OS can run a redundant software stack, where the
application software is unaware of replication [23], with a
sphere of replication (SoR) [24] covering almost all software.
This earlier work on redundant co-execution (RCoE) used
loosely synchronised replication that is advantageous for per-
formance but cannot support applications that contain data
races, such as concurrency control using lock-free algorithms
or atomic instructions. This also rules out supporting virtual
machines (VMs), as we cannot assume that applications inside
a VM are free from data races. Here we generalise the RCoE
approach to support such use cases. Speciﬁcally:
• We introduce closely-coupled redundant co-execution
(CC-RCoE), which makes fewer assumptions about ap-
plication behaviour, and present its design and implemen-
tation on x86 and Arm multicore processors (Section III).
• We introduce an error-masking approach for RCoE that
allows a TMR conﬁguration to downgrade to DMR
operation (Section IV).
• We perform an extensive performance comparison of
CC-RCoE against the original, loosely-coupled variant
(LC-RCoE), using microbenchmarks (Section V-A) and
system benchmarks (Section V-B);
• We evaluate the ability of the schemes to detect errors
in memory or CPU registers (Section V-C), and to mask
errors (Section V-D).
II. BACKGROUND
A. Soft Errors
A single-event upset is a non-destructive (transient) change
of state in a storage element, affecting single or multiple bits,
usually caused by high-energy particles originating in cosmic
radiation or ambient natural radioactivity [6], [25].
Shrinking feature sizes, and reductions of supply voltage,
noise margins, and node capacitance increase sensitivity to soft
errors and lower-energy particles [26]–[28]. Increased device
978-1-7281-0057-9/19/$31.00 ©2019 IEEE
DOI 10.1109/DSN.2019.00031
188
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:13:08 UTC from IEEE Xplore.  Restrictions apply. 
density also increases the likelihood of a single strike affecting
multiple components, resulting in SEC-DED ECC memory
providing insufﬁcient protection for modern memory systems,
with up to 20 undetected failures in time (FIT) per DRAM
device reported [22]. Failure rates of CPU and DRAM are
signiﬁcant even under terrestrial conditions: 1 in 190 for CPU
subsystems and 1 in 1700 for DRAM (one bit-ﬂip) during a
period of 30-day total accumulated CPU time [20].
B. Redundant Co-Execution
A standard approach to redundancy is using lock-
stepping [29] or loosely-synchronised [1] processors with
hardware-supported voting. The purchase and maintenance
costs of such commercial systems are signiﬁcant, and their
sizes and power requirements make them unsuitable for em-
bedded systems. There is also growing demand for perfor-
mance in embedded systems, for instance satellites [30], which
is at odds with the performance characteristics of radiation-
hardened processors [31], [32].
COTS multicore processors are not sufﬁciently synchro-
nised to support lock-step execution of replicas, and even if
they were, such an approach would be prohibitively expensive
without hardware-supported voting. Instead, redundant co-
execution (RCoE) runs multiple replicas of a software system
concurrently and independently on different CPU cores, until
they reach an explicit synchronisation point [23].
Apps
Drivers
Apps
Drivers
Microkernel
Vote
Microkernel
Memory Image
Memory Image
Core0
Device
Core1
Fig. 1. Redundant co-execution (DMR conﬁguration).
Fig. 1 shows an example DMR system based on RCoE.
The whole software system running on Core0 is replicated
onto Core1, with memory partitioned between replicas. RCoE
redundantly executes most components of the software system,
including the OS kernel and device drivers, and all manage-
ment of replication and its synchronisation is done by the ker-
nel. We can consider each replica running on a physical core a
state machine [33], where state transitions result from device
inputs. As devices are not physically replicated, only one
replica, the primary on Core0, can perform low-level device
access and interrupt handling. All replicas must synchronise
at the boundary of this non-replicated device code, so the state
machines can process the same events. The non-replicated
code at the primary is minimal, essentially device-register
reads and writes. All the device driver logic is replicated.
III. CLOSELY-COUPLED RCOE
A. Nondeterminism
To ensure all replica state machines perform the same state
transitions, it is sufﬁcient to replicate the events that trigger
preempt Ra ⇒
preempt Rb ⇒
...
x++;
if (x == 1)
else { ... }
Fig. 2. Data race creating divergence between replicas Ra and Rb.
transitions, i.e. input data and interrupts. LC-RCoE assumes
that the system executes deterministically between events, and
therefore can synchronise at any time between state transitions.
It uses a logical time that counts deterministic events, i.e.
system calls and application-triggered exceptions.
Data races introduce internal non-determinism that will
cause replicas to diverge. To understand this, assume an
application, running on a single logical core, consisting of
two threads T0 and T1. Both threads execute the same code
segment shown in Fig. 2, which has a shared counter x
initialised to 0. Assume at the previous time slice, T0 of two
replicas, Ra and Rb, was preempted at the points shown.
Assume now that on the present timer tick, the replicas of
T1 start and keep running to the if statement. The replicas
will diverge since they observe different values: x is 1 on Ra,
but 2 on Rb.
Divergence will also happen with any multitasking work-
load running on a guest OS when using hardware-supported
virtualisation (where the kernel coordinating the replication is
now the hypervisor), not only with races in the application, but
also lock-free synchronisation in the guest OS. This generally
rules out supporting virtual machines with LC-RCoE.
B. Precise Logical Clock
that
CC-RCoE avoids this divergence by instruction-accurate
time by
synchronisation. We deﬁne the CC-RCoE logical
adding the exact point in execution since the last deterministic
event (i.e. the LC-RCoE clock tick). To obtain this clock,
we make use of the fact
the number of backward
branches taken, together with the current instruction pointer
(IP), identify a unique point in the instruction stream [34].
A CC-RCoE replica’s logical clock thus consists of the triple
(LC-RCoE time, user branches, user IP). Note that we do not
include instructions executed in the kernel, as kernel repli-
cas inherently execute somewhat different instruction streams
upon non-deterministic events (the primary differs from other
replicas).
C. Synchronisation and Voting
RCoE synchronises on kernel entries, which is straight-
forward for system calls and other exceptions. Interrupts are
only received by the primary replica; to force synchronisation
on receiving an interrupt, the primary sets a ﬂag and sends
inter-processor interrupts (IPIs) to the other replicas. When
all replicas observe the event, they vote a leading replica by
comparing logical times of all replicas. The leading replica
waits for the others to catch up by spinning on a kernel barrier.
While spinning, it monitors the per-core cycle counter. If the
spinning time exceeds a system-deﬁned timeout value, this is
taken as an indication of divergence (hanging replica).
189
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:13:08 UTC from IEEE Xplore.  Restrictions apply. 
When a replica must catch up, it sets a global instruction
breakpoint1 at the address of the next user-level instruction
of the leading replica, and resumes execution. When the
breakpoint exception ﬁres, and the replica is still behind the
leader (which can happen if the breakpoint is inside a loop),
it repeats the process, else it joins the wait if there are more
followers outstanding.
Once synchronised, the replicas need to vote, i.e. compare
their state. Comparing all state would be prohibitive. We make
this tractable by reducing critical kernel state updates, driver-
contributed state updates, and system-call parameters to a
three-word signature, consisting of the present event count
plus a checksum representing state changes. To maximise the
signature’s sensitivity to historical changes, we use a Fletcher
checksum [35], which is dependent on the values forming the
checksum as well as the order in which they are applied.
We justify not comparing all system state by the observation
that not all user state is critical, e.g. bit ﬂips in image data
are usually harmless, and the application should determine
its integrity needs. In contrast, any updates to kernel data
structures, such as page tables or capability storage, are
potentially critical. Furthermore, divergence of an application’s
execution path will almost certainly lead to changes in system-
call parameters, resulting in observed divergence.
Detection latency can be reduced by conﬁguring the kernel’s
timer tick. In addition we allow an application to add critical
data to the signature at any time through a new system call,
FT_Add_Trace. Device drivers in particular should use
this system call to contribute output data into the signature
and reduce detection latency. We will examine the reliability
beneﬁts in Section V-C1.
D. Implementation Challenges
One might think that the precise logical clock would be
easy to obtain with the help of the performance monitoring
unit (PMU). However, on the COTS x86 and Arm processors,
many PMU events are imprecise, exhibiting over- or under-
counting [36].
Fortunately, on Intel processors [37] the difference between
the number of branch instructions retired and the number
of far branches retired is deterministic and equal
to the
number of branches executed in user mode, if we program the
PMU counters to count only user-mode events. We conﬁrm
this experimentally on a number of Intel processors of the
Haswell (Core i7-4770 and i5-4590) and Skylake (i7-6700)
microarchitectures. ReVirt [38] uses a similar approach to
record and replay nondeterministic events. Note that when
running a virtual machine (VM), we count branches in user
code as well as the guest kernel.
The x86 architecture presents an additional challenge
through string operations with rep-family preﬁxes (e.g.,
rep movsb), which logically execute in a loop but do not
increment branch counters. This makes it impossible to de-
termine a precise logical time if a breakpoint is set at such
1A global breakpoint triggers an exception when any thread’s program
counter matches the breakpoint.
190
ldr r0, [r3]
_IO_getc
r0, r0
bl
uxtb
cmp r0, #66
ldmeqfd sp!, {r3,
pc}
cmp r0, #55
moveq r0, #56
ldmfd sp!, {r3, pc}
_IO_getc
r0, r0
ldr r0, [r3]
add r9, r9, #1
bl
uxtb
cmp r0, #66
add r9, r9, #1
ldmeqfd sp!, {r3,
pc}
cmp r0, #55
moveq r0, #56
add r9, r9, #1
ldmfd sp!, {r3, pc}
Listing 1. Original code.
Listing 2. Modiﬁed code.
an instruction. To avoid this case, we need to examine the
memory referenced by the instruction pointer. If the code is
in a VM, this requires locating the instruction by a software
walk of the guest page table and the extended page table,
which signiﬁcantly adds to the cost of supporting VMs.
For Armv7-A processors, including the Cortex-A9 cores
we are using, we ﬁnd no PMU events that produce accurate
branch counts. Instead we adopt a compiler-based branch-
counting technique demonstrated by Slye and Elnozahy [39],
who built a record-replay fault-tolerant solution on a DEC
Alpha processor. We use their ideas to develop a plugin for
GCC to count branches, by inserting a count instruction before
each call or jump instruction.
To avoid the overhead of accessing memory and minimise
the number of extra instructions required, we reserve a register
for maintaining the counter (using the --ffixed-r9 argu-
ment to GCC). The register can be incremented in a single
cycle, at the cost of stealing a register from the compiler’s
optimiser. The plugin iterates lists of insns, which are
GCC’s internal representation of instructions, and prepends
each call_insn (function call) or jump_insn (jump) with
an increment instruction on register a9. For example, the code
of Listing 1 is transformed into that of Listing 2.
We ensure that the plugin is called after various optimisation
passes, to avoid the extra instructions being optimised away.
The reserved register is thread-local, i.e. context-switched like
any register. The kernel only treats it as special during the
synchronisation protocol, where it is monitored to determine
when replicas have caught up. After syncing, it is reset to
avoid overﬂow.
Synchronisation instructions also cause a problem: Armv7-
A provides the load exclusive (ldrex) and store exclusive
(strex) instructions, which are used in a retry-loop, for
implementing atomic updates. As a result,
the number of
executions may differ between replicas, even when they do
not diverge. We avoid this problem by requiring the use of a
system call for atomic updates. This step can be automated in
the future by using a binary rewriting tool, which scans the
ldrex-strex pairs and converts them into system calls.
Armv7 does not have an equivalent of the x86 resume ﬂag,
which can temporarily disable a breakpoint without clearing
it. We therefore need to handle two debug exceptions for every
breakpoint: one for the target breakpoint and a second one for a
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:13:08 UTC from IEEE Xplore.  Restrictions apply. 
1000:
1004:
1108:
110c:
add r7, r7, #1
add r8, r8, #1
...
add r9, r9, #1
b 1000
Listing 3. Race condition on counter maintenance.
mismatch breakpoint, to single-step over the target breakpoint
before re-enabling it again. This increases the overhead of
synchronising the branch counter.
Another complication arises from the fact that our branch
counter is not updated atomically with the branch, and execu-
tion may be preempted between the two instructions. Consider
the code in Listing 3, and assume the primary has just executed
the back branch, i.e. its instruction pointer is 0x1000. If
another replica with the same counter value is about
to
execute address 0x110c, its branch counter already reﬂects
the branch that has not yet been taken. Simply comparing
instruction pointers would falsely indicate the primary as
trailing. Therefore, we also need to check whether the last
instruction executed by a replica is the counter incrementation
and handle the case accordingly when voting the leader.
As CC-RCoE on Armv7-A depends on a compiler ex-
tension, all code (including libraries) must be recompiled.