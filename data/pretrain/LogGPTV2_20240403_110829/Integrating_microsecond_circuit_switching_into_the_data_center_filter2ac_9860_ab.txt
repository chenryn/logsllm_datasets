implement and evaluate it in the context of a testbed deployment
later in the paper. For now, we assume that the network-wide traf-
ﬁc demand is known and return to the issue of estimating demand
at the end of the section.
3.1 Overview
Existing approaches that integrate OCS hardware into the data
center amortize the long switching time (tens of milliseconds) of
previous generation optical technology by reconﬁguring the OCS
only once every few 100s of milliseconds or even several seconds.
The substantial interval between reconﬁgurations affords their un-
derlying control loops the opportunity to estimate demand, calcu-
late an optimal OCS conﬁguration, and communicate it across the
network every time the switch is repositioned.
Previous hybrid networks perform what we call hotspot schedul-
ing (HSS). HSS (a) measures the inter-rack trafﬁc matrix, (b) es-
timates the trafﬁc demand matrix, (c) identiﬁes hotspots, and (d)
uses a centralized scheduler to establish physical circuits between
racks to ofﬂoad only the identiﬁed hotspot trafﬁc onto the circuit-
switched network. The remaining trafﬁc is routed over the packet-
switched network. Because of the substantial delay between re-
conﬁgurations, HSS can employ complex methods and algorithms
to estimate demand and identify hotspots. Errors in identifying
hotspots, however, can lead to signiﬁcant losses in efﬁciency. If a
selected hotspot does not generate sufﬁcient trafﬁc to saturate a cir-
cuit, then the remaining capacity goes to waste for the (non-trivial)
duration of the current conﬁguration.
When the OCS can be reconﬁgured on the order of 10s of µs,
we argue that it is possible to route most of the trafﬁc over cir-
cuits. In contrast to HSS, we propose an approach called “Traf-
ﬁc Matrix Switching” (TMS) that estimates demand and calculates
a short-term schedule that moves the OCS rapidly through a se-
quence of conﬁgurations to service predicted demand. By rapidly
time-sharing circuits across many destinations at microsecond time
Figure 2: Eight racks running Hadoop with an all-to-all com-
munication pattern: (a) physical topology, (b) logical topology,
(c) inter-rack trafﬁc demand matrix, (d) circuit switch sched-
ule.
scales, TMS is able to make more effective use of the circuit band-
width (and reduces to hotspot scheduling when demand is extremely
concentrated). The key insight is that by sending the upcoming
schedule to both the OCS and the ToRs, they can effectively make
use of each circuit as it becomes available. Moreover, while the
current schedule is being carried out, the control loop can enter its
next iteration. In this way, the running time of the control plane is
decoupled from the switch speed. In particular, the control plane
only needs to recompute schedules fast enough to keep up with
shifts in the underlying trafﬁc patterns, rather than with the OCS
switch speed.
One of the key reasons that we adopted TMS instead of running
a hotspot scheduling algorithm faster is that hotspot scheduling is
inherently stateless. Each iteration of hotspot scheduling greed-
ily chooses a perfect matching of circuit assignments to ﬁnd the
largest elephants (k for a k-by-k switch). This doesn’t necessarily
result in ideal usage of the OCS. Further, the control plane would
then have to run at the speed of the underlying host burst behavior.
With TMS, we can compute an entire “week” of schedules once,
amortizing the control loop time over a number of switch reconﬁg-
urations. We believe that this is critical to match the speed of the
OCS.
3.2 Example
Consider eight racks running Hadoop and generating a perfectly
uniform all-to-all communication pattern. Figure 2(a) shows the
racks physically connected to the same core-circuit switch; Fig-
(a)12345678switch(b)12345678(c)123456781/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/81/8123456781/81/81/81/81/81/81/81/81/8src poddst pod (rate)(d)2T3T4T5T6T7T8TT012345678src port(dst port)time1234567823456781345678124567812356781234678123457812345681234567loopback wastesetup waste449k(cid:88)
i
more than its link rate (its row sum is less than or equal to 1) and
no rack wishes to receive more than its link rate (its column sum
is less than or equal to 1), then we say that the TDM is both ad-
missible and doubly substochastic. The goal of scaling the TDM is
to compute a doubly stochastic BDM where its row sums and col-
umn sums are all exactly equal to 1 — meaning the circuits would
be fully utilized. By scaling the TDM into a BAM, we simulta-
neously preserve the relative demands of the senders and receivers
while satisfying the constraints of the circuit switch. Several matrix
scaling algorithms can be used for this purpose. Sinkhorn’s algo-
rithm [18] is particularly attractive because it works even when the
originally TDM is not admissible (i.e., the network is over driven).
Next, the BAM is decomposed into a circuit switch schedule,
which is a convex combination of permutation matrices that sum to
the original BAM,
ciPi
BAM =
(1)
where 0 ≤ i ≤ k, and k = N 2−2N +2. Each permutation matrix,
Pi, represents a circuit switch assignment, and each scalar coefﬁ-
cient, ci, represents a time slot duration as a fraction of the total
schedule duration. A variety of matrix decomposition algorithms
exist. We employ an algorithm originally due to Birkhoff-von Neu-
mann (BvN) [5, 23] that can decompose any doubly stochastic ma-
trix, implying we can always compute a perfect schedule given a
BAM. Improved versions of the classic BvN algorithm have run-
ning times between O(n log2 n) and O(n2) [11].
3.4 Demand estimation
Trafﬁc matrix scheduling, just like hotspot scheduling, requires
an estimate of the network-wide demand. There are several po-
tential sources of this information. First, packet counters in the
ToRs can be polled to determine the trafﬁc matrix, and from that
the demand matrix can be computed using techniques presented in
Hedera [2]. This method would likely introduce signiﬁcant delays,
given the latency of polling and running the demand estimator. A
second potential approach, if the network is centrally controlled, is
to rely on OpenFlow [15] network controllers to provide a snap-
shot of the overall trafﬁc demand. Third, an approach similar to
that taken by c-Through [25] may be adopted: A central controller,
or even each ToR, can query individual end hosts and retrieve the
TCP send buffer sizes of active connections. Asynchronously send-
ing this information to the ToRs can further reduce the latency of
collecting the measurements. Finally, application controllers, such
as the Hadoop JobTracker [12], can provide hints as to future de-
mands. Our prototype implementation does not implement demand
estimation.
4. ANALYSIS
The throughput of a network that uses circuit switching is con-
strained by the network’s duty cycle, and its feasibility is constrained
by the amount of buffering required and the circuit schedule. We
consider these issues in turn.
4.1 Duty cycle and effective link rate
In a circuit-switched network, there is a ﬁnite reconﬁguration
time, or setup time tsetup, during which no data can be sent. If the
link data rate is Rlink, then the effective data rate R of each circuit
is R = DRlink, where:
D =
tstable
tsetup + tstable
(2)
is the duty cycle and tstable is the time that the circuit is “open” and
can carry trafﬁc. For example, if Rlink is 10 Gbps and D is 90%,
Figure 3: Steps of the trafﬁc matrix scheduling algorithm.
ure 2(b) shows the logical connectivity, and Figure 2(c) shows the
inter-rack trafﬁc demand matrix with sources as rows, destinations
as columns, and values as fractions of the total link rate. The di-
agonal is not zero because hosts send to other hosts in the same
rack. Although this intra-rack trafﬁc does not transit the core cir-
cuit switch, it is still accounted for in the trafﬁc demand matrix.
This matrix is the desired transmission rate of the hosts, and it is
the responsibility of the network to satisfy this demand.
The Gantt chart in Figure 2(d) shows a circuit switch schedule
that partitions time into eight equal-duration time slots. Over the
course of the schedule, each source port will connect to each des-
tination port for exactly 1/8 of the total time. It thus implements
the logical full mesh topology in Figure 2(b) and allows all of the
trafﬁc to be routed. The schedule then repeats.
A circuit switch schedule, however, has two sources of waste.
First, loopback trafﬁc does not leave the rack and transit the cir-
cuit switch, so any circuit switch loopback assignments are wasted,
such as the assignment from t = 0 to t = T . Second, the cir-
cuit switch takes a non-negligible amount of time to switch and
setup new circuits (tsetup), which we represent as black bars at the
end of each time slot. No trafﬁc can transit the circuit switch dur-
ing this time. Reducing loopback waste requires careful schedul-
ing, whereas reducing setup waste requires faster switching. Fi-
nally, note that although this example produces a repeating sched-
ule, TMS can generate arbitrary time-varying circuit assignments
as we describe below.
3.3 Schedule computation
The TMS algorithm is divided into a set of steps, as shown in
Figure 3. First, the trafﬁc demand matrix (TDM) is scaled into a
bandwidth allocation matrix (BAM). A TDM represents the amount
of trafﬁc, in units of circuit line rate, that the hosts in a source
rack wish to transmit to the hosts in a destination rack. A BAM,
on the other hand, represents the fraction of circuit bandwidth the
switch should allocate between each input-output port pair in an
ideal schedule. In general, the TDM may not be admissible (i.e.,
the total demand is greater than the network capacity). In practice,
though, the network is rarely driven to full utilization, so we need
to scale “up” the TDM to arrive at a BAM. If no rack wishes to send
MM´P1t1t2tkP2Pk+++Step 1. Gather traffic matrix MStep 3. Decompose M´ into scheduleStep 4. Execute schedule in hardwareStep 2. Scale M into M´t1t2tk450Packet
0% 100.0%
n Circuit
0
1
2
3
4
5
6
7
8
9
10
39.4%
53.8%
63.8%
72.7%
80.6%
87.3%
92.3%
96.6%
99.3%
100.0%
D
N/A
60.6% 100.0%
98.0%
46.2%
97.0%
36.2%
27.3%
96.0%
95.0%
19.4%
94.0%
12.7%
93.0%
7.7%
92.0%
3.4%
0.7%
91.0%
90.0%
0%
Table 2: Example of the tradeoff between the number of sched-
ule time slots (n), the amount of trafﬁc sent over the optical-
circuit switched network (Circuit) vs. the packet-switched net-
work (Packet), and the duty cycle (D) for a randomly generated
TDM with a reconﬁguration latency of 10 µs.
be better not to schedule all trafﬁc over the circuit switch and to
simply schedule only the longest time slots. The reason is that
the BvN decomposition algorithm generates time slots of different
lengths, some of which can be quite short (e.g., less than 1% of
the entire schedule). With such a short time slot, it is likely that
the OCS switching time would dominate any bandwidth delivered
during those small slots. In these cases, it is better to route that
trafﬁc over the packet-switched network.
The greatest beneﬁt comes from scheduling only the ﬁrst n time
slots, where n is chosen based on both the minimum required duty
cycle, D, as well as the maximum allowed schedule length. Any re-
maining trafﬁc is instead sent to the packet-switched network. Us-
ing the same randomly generated TDM from [7], we calculate the
tradeoffs in different numbers of slots for the schedule, as shown in
Table 2.
As n increases, an increasing fraction of the total trafﬁc transits
the circuit-switched network. In the limit when n = k, all trafﬁc is
sent over the circuit-switched network. However, the duty cycle de-
creases with increasing n, since the schedule invokes more switch
reconﬁgurations. For example, if the minimum required duty cycle
was 95%, then by setting n = 5, 80.6% of the total trafﬁc would be
routed over circuit switches. Alternatively, at the cost of additional
host buffering, we could increase n to 6 or 7 while keeping the duty
cycle at 95%.
5.
IMPLEMENTATION
We evaluated the Mordia OCS in a testbed environment. This
implementation effort consists of two primary tasks: (1) selecting