Every (honest) party who completes the protocol holds the corresponding shares.
The protocol for non-robustly generating random double-sharings is taken from [5] as
is. For completeness, we repeat the protocol and the lemma, but omit the proof.
The generation of the random double-sharings employs hyper-invertible matrices: First,
every party Pi ∈ P selects and double-shares a random value si. Then, the parties compute
double-sharings of the values ri, deﬁned as (r1, . . . , rn) = M (s1, . . . , sn), where M is a
hyper-invertible n-by-n matrix. Then, 2t of the resulting double-sharings are reconstructed,
each towards a diﬀerent party, who verify the correctness of the double-sharings. The
remaining n− 2t = T double-sharings are outputted. This procedure guarantees that if no
honest party aborts, then at least n double-sharings are correct (the n − t double-sharings
26
inputted by honest parties, as well as the t double-sharings veriﬁed by honest parties), and
due to the hyper-invertibility of M , all 2n double-sharings must be correct (the remaining
double-sharings can be computed linearly from the good double-sharings). Furthermore,
the outputted double-sharings are random and unknown to the adversary, as there is a
bijective mapping from any T double-sharings inputted by honest parties to the outputted
double-sharings.
Protocol DoubleShareRandom(d, d(cid:48)).
1. Secret Share: Every Pi ∈ P chooses a random si and acts (twice in parallel) as a
dealer in Share to distribute the shares among the parties in P, resulting in [si]d,d(cid:48).
2. Apply M : The parties in P (locally) compute(cid:0)[r1]d,d(cid:48), . . . , [rn]d,d(cid:48)(cid:1) = M·(cid:0)[s1]d,d(cid:48), . . . , [sn]d,d(cid:48)(cid:1)T .
In order to do so, every Pi computes its double-share of each rj as linear combination
of its double-shares of the sk-values.
3. Check: For i = T + 1, . . . , n where T = n − 2t, every Pj ∈ P sends its double-share
of [ri]d,d(cid:48) to Pi, who checks that all n double-shares deﬁne a correct double-sharing of
some value ri. More precisely, Pi checks that all d-shares indeed lie on a polynomial
g(·) of degree d, and that all d(cid:48)-shares indeed lie on a polynomial g(cid:48)(·) of degree d(cid:48),
and that g(0) = g(cid:48)(0). Pi sends a bit to all Pj signaling whether inconsistencies were
observed or not, and every Pj aborts if at least one party reports inconsistencies.
4. Output: The remaining T double-sharings [r1]d,d(cid:48), . . . , [rT ]d,d(cid:48) are outputted.
Lemma 3.5 ([5]) All (honest) parties who complete the protocol DoubleShareRandom(d, d(cid:48))
output T = n − 2t correct and random (d, d(cid:48))-sharings, unknown to the adversary.
DoubleShareRandom communicates exactly 2n2 + 4nt > 2n2 + 4n2
3 ﬁeld elements to generate
n − 2t random double-sharings.
The proof can be found in [5]. Regarding the communication cost, observe that the
secret sharing step costs exactly 2n2 since double sharings are sent. Then, in the check
phase, double sharings are sent for n − T = n − (n − 2t) = 2 values, resulting in an
additional 2n· 2t > 4n2
3 double sharings, and thus
the amortized cost is
(cid:16)
3 ﬁeld elements. This results in n− 2t = n
2n2 + 4n2
3
(cid:17)
/ n
In the preparation phase, the parties need to generate L random (t, 2t)-sharings. This
is achieved by invoking the protocol DoubleShareRandom (cid:100) L
3 = 10n elements per double sharing generated.
n−2t(cid:101) times (in parallel).
Regular sharings. We stress that for input and random gates, it suﬃces to generate
regular sharings of random values of degree t only. The cost of generating (non-double)
random sharings is exactly half, and works in the same way as DoubleShareRandom except
that only single sharings are sent. Thus, the cost of generating these single sharings is 5n
per element. Nevertheless, for simplicity (and since it only makes a diﬀerence if there is
27
a large number of input or random gates), we describe the protocol with random double-
sharings only, and in this case only the degree-t part is consumed for input and random
gates.
3.6 Summary of Costs
Before proceeding to the full protocol, we summarize the communication costs of all sub-
protocols. Table 1 includes the overall cost for each subprotocol, how many elements that
cost counts for, and the amortized cost per element. The amortized cost refers to the cost
per element reconstructed or broadcast, or to double sharing generated, depending on the
speciﬁc protocol. All counts are in ﬁeld elements.
Subprotocol
Cost
# Generated Amortized
ReconsPubl
ReconsPriv
Broadcast
2n2 − n2
3
DoubleShareRandom 2n2 + 4n2
3
n
2n2
1
2n
3
2n
3
n
3
n
3n
2.5n
10n
3.7 Computation Phase
Table 1: Costs for all subprotocols.
In the computation phase, the circuit is evaluated, whereby all intermediate values are
t-shared among the parties in P.
Input gates are realized by reconstructing a pre-shared random value r towards the
input-providing party, who then broadcasts the diﬀerence of this r and its input. We
always process n− t input gates at once, as the protocol Broadcast allows to broadcast that
many values at once.
Due to the linearity of Shamir sharing, addition gates (and general linear gates) can
be computed locally simply by applying the linear function to the shares. That is, for any
linear function L, a sharing [c] = [L(a, b, . . .)] is computed by letting every party Pi apply
L and its respective shares, i.e., ci = L(ai, bi, . . .).
With every random gate, one random sharing [r] (from the preparation phase) is asso-
ciated and [r]t is directly used as outcome of the random gate.
With every multiplication gate, one random double-sharing [r]t,2t (from the preparation
phase) is associated. The idea is that each party multiplies its share of [a]t and its share
of [b]t, resulting in a degree-2t sharing [ab]2t of the product ab. Then the diﬀerence of
[ab]2t and [r]2t is reconstructed (using ReconsPubl) and added on top of [r]t, resulting in
a degree-t sharing [ab]t of the product ab. As the protocol ReconsPubl reconstructs n − t
sharings at once, we process n − t multiplication gates in parallel.
Output gates involve private reconstruction ReconsPriv.
28
Protocol ComputationPhase.
Evaluate the gates of the circuit as follows:
• Input Gate (party PI inputs s):
towards PI .
1. The parties invoke ReconsPriv(PI , t, [r]) to reconstruct the associated sharing [r]t
2. Party PI computes d = s − r and invokes Broadcast(PI , d).
3. Each Pi ∈ P computes its share si of s locally as si = ri + d.
As the protocol Broadcast allows to broadcast n − t values at once, we process n − t
input gates in parallel. Note that not necessarily all these input gates need to be for
the same party PI , as Broadcast can take values from several parties.
• Addition/Linear Gate: Every Pi ∈ P applies the linear function on its respective
• Random Gate: Pick the sharing [r]t associated with the gate.
• Multiplication Gate: We denote the two sharings to be multiplied as [a]t and [b]t,
respectively, and the associated double-sharing as [r]t,2t. The shares of [r]t are denoted
by (r1, . . . , rn), those of [r]2t by (r(cid:48)
1, . . . , r(cid:48)
n).
1. Each Pi ∈ P computes ei = ai · bi − r(cid:48)
i. Observe that (e1, . . . , en) form a random
shares.
degree-2t sharing of a random value, independent of a and b (as blinded by r).
2. The parties invoke ReconsPubl to reconstruct e to all.
3. Each Pi ∈ P computes ci = ri + e. Observe that (c1, . . . , cn) form a degree-t sharing
As the protocol ReconsPubl reconstructs n − t sharings at once, we process n − t mul-
tiplication gates in parallel.
of c = ab.
• Output Gate (output [s] to party PO): Invoke
ReconsPriv(PO, t, [s]t).
Theorem 3.6 (Theorem 1.1–restated) Let f be an n-party functionality, and let C be
an arithmetic circuit over ﬁeld F that computes f . Then, protocol HyperMPC computes f
with perfect security and with abort, in the presence of an adaptive, malicious adversary
corrupting t < n/3 parties. The exact communication complexity of HyperMPC is
13n · cM + 13.5n · cI + 10n · cR + n · cO + 2n2 · DM + 10n2
ﬁeld elements (sent by all parties overall), where cm denotes the number of multiplication
gates, cI the number of input gates, cR the number of random gates, cO the number of
output gates, and DM the multiplicative depth of C.
Proof:
[Proof sketch] The security of the protocol for input, random, and output gates
follows directly from inspection. The security of the protocol for addition / linear gates
29
follows directly from the linearity of Shamir sharing. The security of the multiplication
protocol can be seen as followed: Given that the shares (a1, . . . , an) and b1, . . . , bn) lie on
a degree-t polynomial, then the pairwise products of the shares (a1b1, . . . , anbn) lie on a
degree-2t polynomial. Note that this polynomial is not necessarily random. However, from
the preparation phase we have a random degree-2t sharing of a random value r, given by
i}i lie on a uniform random degree-2t
the shares (r(cid:48)
polynomial; reconstructing its secret does not reveal any information. The secret of this
polynomial can easily be seen to be ab− r, hence [r]t + (ab− r) is a degree-t sharing of the
product ab.
n). Hence, the diﬀerences {aibi−r(cid:48)
1, . . . , r(cid:48)
The claimed communication complexity follows from the communication complexities
of the involved sub-protocols. Recall that each double-sharing costs 10n, and one is used for
each input, random and multiplication gate. Now, input requires a single double sharing,
one ReconsPriv invocation (n), and one Broadcast invocation (2.5n), resulting in 13.5n · cI .
Random gates have no additional cost beyond a single double sharing, and we therefore
have 10n · cR. Multiplication uses a single double sharing (10n) and a single ReconsPubl
invocation (3n), at the cost of 13n · cM . Finally, output is just a single ReconsPriv, adding
n · cO.
We now explain the 2n2·DM +10n2 terms. Recall that ReconsPubl in the multiplication
gate computation processes n − t values at once, but we can only process gates in parallel
when the gates are independent from each other (at the same level in the circuit). Thus, on
each (multiplicative) level of the circuit, one instance of ReconsPubl may process less than
2n
3 gates. In the worst case, only one gate is processed, at the cost of 2n2. This explains
the additional 2n2 · DM term in the communication. Likewise, double-random sharings
are generated at a cost of 10n2 for n/3 elements; if there are very few needed, then 10n2
ﬁeld elements still need to be computed. We stress, however, that both these terms will
be insigniﬁcant, except for extremely small or narrow circuits. This completes the proof
sketch.
3.8 A Note on Fairness
HyperMPC as presented does not guarantee fairness. In order to see why, consider the case
that only one honest party participates in the output phase (since the other honest parties
have aborted previously). Then, in the output phase, the corrupted parties can simply not
send their shares, but they will obtain the honest party’s share. In this case, the corrupted
parties will have t + 1 shares and can reconstruct the output, while the honest parties
cannot.
One may make the protocol fair quite easily. The idea is that before any output gate
is processed, the parties check that all (honest) parties are still participating and have not
aborted. The output gates are processed only if all parties declare that they are alive and
ready to generate output.
In order to check that all parties are alive, one needs asynchronous broadcast (we must
30
ensure that if some honest party concludes that everybody is alive, then all honest parties
will eventually conclude the same way). This can be achieved with Bracha broadcast [7],
as described below.
from t + 1 diﬀerent parties, send (ready, i) to every P(cid:96) ∈ P.
Protocol CheckAlive.
1. ∀Pi ∈ P: send every Pj ∈ P the message (alive).
2. ∀Pj ∈ P: Upon receiving (alive) from Pi, send (echo, i) to every Pk ∈ P.
3. ∀Pk ∈ P: Upon receiving (echo, i) from n − t diﬀerent parties or receiving (ready, i)
4. ∀Pl ∈ P: Upon receiving (ready, i) from n− t diﬀerent parties, conclude that Pi is alive.
Each party proceeds to the output phase only once he has concluded that all parties
in P are alive.
In the output phase, outputs need to be reconstructed asynchronously.
In particular, the output party PO must not wait until it has received all n shares of the
output, but only until it has received enough shares such that the output value can be
safely reconstructed (i.e., such that at least 2t + 1 of the received shares lie on a degree-t
polynomial). This suﬃces to ensure that the reconstructed output is valid, since at least
t + 1 of those shares must be correct.
Protocol ReconsAsync.
1. ∀Pi ∈ P: send share si of output to the output party PO.
2. PO: Upon receiving a new share si from Pi, check that among all received shares there
are at least 2t + 1 shares that lie on a degree-t polynomial p(·). If so, output s = p(0).
Otherwise, keep on waiting for more shares.
Step 2 can be implemented eﬃciently with an appropriate error-decoding algorithm
(e.g. the algorithm of Berlekamp and Welch). This is needed since at this point honest
parties cannot abort (the corrupted parties have already received output). Thus, honest
parties must be able to correctly reconstruct, even if they receive incorrect shares from the
corrupted parties.
3.9 Comparison to [5]
As we have stated, HyperMPC is a simpliﬁcation of the protocol of [5], with signiﬁcant