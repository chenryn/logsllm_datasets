Detecting Output Tuple Writes. The ﬁrst proﬁling technique is to
identify instrumentation points that disclose writes of output tuples.
For each query category (i.e. select, join, aggregation, update, and
insert), we have a small set of training runs (2-5). If we observe
a write to data ﬁle, we manually identify the tuple id in the output
buffer and leverage a data dependence tracking technique to back-
track to the earliest point that deﬁnes the tuple id value. For exam-
ple, if a tuple id is generated using a counter “id=counter++” at
line 1, and then copied at lines 2, 3, and 4, which is the ﬁnal tuple
write, we backtrack to line 1. Essentially, we identify the root of
the data dependence chain to the tuple id such that all instruction
instances along the chain have the same id value.
Detecting Dependent Input Tuples. The second proﬁling tech-
nique is to identify instrumentation points that can disclose the in-
put tuples that are used to compute individual output tuples. The
basic idea is to leverage an instruction level provenance tracing
technique that can precisely compute the set of ids of the input
tuples that are directly or transitively used to compute a value at
any program point. As such, given an output tuple, we can pre-
cisely know its input tuple provenance. Then we inspect the set of
executed instructions to determine a minimal subset such that the
union of their provenance sets is precisely the provenance of the
output tuple. In other words, each of the instructions in the sub-
10101
2
1
2
1
1
1
1
1
1
1
Figure 6: Using proﬁling to determine instrumentation points. The code snippet models how mysql handles the query on top. Function
readRecord(“table_name”,&buf, &s) reads a tuple from data ﬁle, returns it in buf that is allocated inside the function, the size is set in s
and the tuple id is the return value; store_cache() stores a tuple to cache; getField() gets a tuple ﬁeld from a buffer given the ﬁeld label;
insertField() inserts a ﬁeld to a buffer; writeRecord() writes a tuple (buffer) to a given table. Symbols a1, a2, b1, b2, and c1 in (b) denote the
different buffer address values. The subscripts in statement labels denote instances. Each data ﬁeld is 4 byte long.
set has processed (part of) the dependent input tuples. We hence
can instrument these instructions to emit the ids of the tuples that
they process. Note that instruction-level tracing is only used for
proﬁling.
The semantic rules of the instruction-level provenance tracer are
presented in Table 3, which speciﬁes the action the tracer takes
upon an instruction. Function readRecord() loads a tuple from
data ﬁle. The result tuple is stored in buf with size s. The function
returns the tuple id. When an invocation of the function is encoun-
tered, it is considered a source of input provenance. Therefore,
the corresponding action is to set the provenance of each individual
byte in buf, which stores the data ﬁelds of the tuple, to the input tu-
ple id. This function is an abstraction of four functions in mysql,
namely, _mi_read_static_record(), _mi_read_rnd_static
_record(), _mi_read_dynamic_record(), and _mi_read_rnd
_dynamic_record().
The last row shows the rule for assignments. We normalize the
right-hand-side to an operation on a set of operands. The prove-
nance of the left-hand-side is computed as the union of the operand
provenance sets. We also compute the aggregated provenance set
of the instruction, which is essentially the union of provenance sets
of all the executed instances of the instruction.
Algorithm 2 shows how to identify the instrumentation points,
leveraging the instruction-level tracer. It ﬁrst executes mysql with
a given query. Lines 2 and 3 compute the provenance of the output,
stored in pv. Set R denotes the candidate instrumentation points,
which are denoted by program counters. In lines 5-8, the algorithm
traverses all the executed instructions and determines the candidate
instructions. The candidacy requires that the aggregated prove-
nance of an instruction must be a subset of the output provenance.
Otherwise, it must be an instruction that does not exclusively pro-
cess dependent input tuples, such as line 2 in Fig. 6 (a), which
processes all input tuples. Furthermore, the instruction should ex-
clusively process input tuples from the same table. Otherwise, it
is an instruction that aggregates information from multiple tables.
It is usually impossible to acquire the original tuple ids at such an
instruction. For example, line 17 in Fig. 6 (a) has exactly the output
provenance. However, since it is just a composition of ﬁelds from
input tuples, it is in general difﬁcult to extract input tuple ids from
the composed result. With the candidate set, at line 6, the algorithm
identiﬁes the minimal set of instructions that can cover the output
provenance. It may return multiple such sets.
Fig. 6 (b) shows a sample proﬁling run. The input tables are
shown on top. One can see that there will be just one result tuple,
whose provenance is tuple 1 from t1 and tuple 1 from t2. The
ﬁgure shows part of the execution trace. The Pv and S sets are also
shown for each executed statement. The program ﬁrst loads tuples
from t1, which leads to the provenance sets of the buffer to be
set to {1} and {2} in the two respective instances of instruction 2.
Then, the program reads t2 and compares tuples. Since the ﬁrst
tuple of t2 matches the ﬁrst tuple of t1, lines 12-14 are executed
to extract data from input tuples. Hence, their provenance includes
the corresponding input tuple id. At the end, the aggregated prove-
nance of line 2 is {1,2}, line 9 is {5,6}, line 12 is {1}, and lines
13 and 14 are {5}. Since the output provenance is {1,5}, the mini-
mal cover set could be lines 12 and 13 or 12 and 14, which are the
places reported by the algorithm.
Even though our discussion is driven by a speciﬁc query, the al-
gorithm is generic. We apply it to a training set of various types of
queries. Since the proﬁler can only give hints about the instrumen-
tation points, we assume that human users will inspect the source
code to eventually determine the instrumentation points from the
reported ones. In practice, the human efforts are small and they are
one-time efforts. We instrumented a total of 15 places in mysql
after inspecting about 40 places suggested by the proﬁler. The
instrumentation supports typical select, join, aggregation, update,
insert and sub-queries with nesting level of 2. We have validated
that it can correctly identify tuple-level dependences in the standard
database workloads from RUBiS [5] and SysBench [1].
Discussion. In this work, we do not consider control dependences
between tuples. For example, in the following query:
select * from t1 where t1.val > (select avg(val) from t1);
1011Figure 7: Apache application log examples.
Our instrumentation will emit the ids of tuples whose values are
greater than the average. However, all tuples in t1 are correlated
to the output tuples as they are used to compute the average, which
is used in the where condition. It is analogous to the traditional
concept of program control dependence.
In this work, we deem
such precise tuple control dependences of limited forensic value.
Note that even though SQL injection attacks are often achieved by
manipulating where conditions, knowing the tuples that are used
in malicious where clauses are unlikely useful as they often serve
only as a true condition enabling other malicious actions, which
will be captured by LogGC .
If a query generates multiple output tuples, the output tuples will
belong to the same execution unit. However, we cannot simply
consider a tuple output event as dependent on all the preceding tu-
ple input events in the same unit. Instead, we leverage the fact that
output tuples are computed one after another. Hence a tuple out-
put event is only dependent on those preceding tuple input events
that are in between this output event and the immediate preceding
output event.
Finally, since instrumentation points for data units are produced
by the proﬁler and validated manually by the user, it is possible
that they be incomplete and unsound (i.e., inducing false tuple de-
pendences). Also, the proﬁler takes regular use cases provided by
the user to determine the instrumentation points. Our experience
does show that our techniques are sufﬁciently effective. Section 6.3
shows that LogGC with data units effectively reduces log size with-
out affecting forensic analysis of realistic attack scenarios. The re-
sults show that the reduced logs (7 times smaller than the original
ones) are equally or even more effective in forensic analysis that in-
volves databases. In the future, we plan to develop more automated
techniques to statically determine instrumentation points.
5. LEVERAGING APPLICATION LOGS
We observe that many long running applications have the fol-
lowing two characteristics: (1) They have their own log ﬁles that
record some execution status (we call them the application logs to
distinguish from audit logs); (2) They often send data to remote
users through socket writes. For instance, apache records ev-
ery remote request it receives and the status of fulﬁlling the re-
quest.
It also sends the requested (html) ﬁle back to the remote
client through socket writes. On one hand, the two characteristics
make garbage collection difﬁcult, because both application log ﬁle
writes and socket writes are output events. According to our earlier
discussion, any preceding input events within the same unit may
have dependence with those output events hence they cannot be
garbage-collected. On the other hand, the two properties also pro-
vide an opportunity to prune more events in the audit log if they are
already present in the application logs.
Fig. 7 (a) shows two execution units of apache that handle
requests for index.html and about_me.html, respectively.
They both have the following event pattern: socket read to receive
the request; ﬁle read to load the ﬁle; socket write to send the ﬁle;
and ﬁle write to the application log to record the history. This is
also the dominant event pattern for apache. Both socket write
and ﬁle write prevent garbage-collecting any other events. Accord-
ing to our experience, such server programs can generate as much
as 71.4% of audit log events.
To address this problem, we propose to leverage the application
logs themselves. More speciﬁcally, we observe that the aforemen-
tioned event pattern (as shown in Fig. 7 (a)) forms a small isolated
causal graph independent of the other events in the audit log. It
simply receives the request and returns the ﬁle. It does not affect
other objects in the system except the application log. Furthermore,
all events in the graph can be inferred from the application log. In
Fig. 7(a), all the four events can be inferred from the application
log and hence removed from the audit log.
One important criterion for replacing audit log events with ap-
plication log events is that the events are self-contained. They do
not have dependence with any past event or induce any dependence
with future events. For example, if about_me.html in Fig. 7
(a) has been modiﬁed in the past by some user request, which is
recorded in the audit log, events in the second unit will not be
garbage-collected.
In case of a malicious modiﬁcation, this will
allow us to back-track to the malicious change and determine its
ramiﬁcations. Furthermore, if a unit produces side effects on the
system, such as writing to a ﬁle or writing to a database (e.g., via
socket y.y.y.y to the mysql server in Fig. 7.(b)), its events will
not be garbage collected.
We apply this strategy to server programs (e.g., web server and
ftp server) and user applications that generate their own logs, with
the exception of mysql because we rely on the tuple-level events
to perform garbage collection.
6. EVALUATION
We have implemented a LogGC prototype composed of train-
ing, proﬁling, instrumentation, log analysis and garbage collection
components. The training and proﬁling components are imple-
mented on PIN [21], instrumentation is through a binary rewriting
tool PEBIL [19]. The log analysis and garbage collection compo-
nents are implemented in C++.
Table 4 shows a list of sample applications we have installed
and preprocessed for our evaluation. For each application, we have
trained and instrumented it to support process and ﬁle partitioning.
Columns 3 and 4 indicate applications that make use of tempo-
rary ﬁles and have application logs, respectively. Recall that we
garbage-collect temporary ﬁle deletions, which enables removing
other dependent events. The last column indicates if an application
needs to be instrumented for data unit partition. Currently, we only
instrument mysql to achieve more log reduction.
1012One day
execution
Web server
benchmark
Applications
Logs
User1
User2
User3
User4
User5
Average
Rubis1
Rubis2
Rubis3
Rubis4
Rubis5
Average
Firefox
MC
Mplayer
Pidgin
Pine
Proftpd
Sendmail
Sshd
Vim
W3m
Wget
Xpdf
Yafc
Audacious
Bash
Apache
Mysqld
Total
# logs
1,159,680
985,185
1,329,854
1,921,038
973,789
1,273,909
8,676,521
8,433,427
8,352,924
8,900,179
7,933,792
8,459,368
3,260,948
16,020
452,400
47,018
91,215
51,906
7,321
148,885
98,791
128,569
80112
76,015
477
216,428
1,325
745,424
984,261
1,246,804
1,920,363
592,521
1,097,875
8,676,260
8,433,345
8,352,842
8,897,798
7,933,710
8,458,791
3,258,628
15,940
452,338
47,018
91,215
51,404
7,176
148,885
98,791
127,330
80112
76,015
477
216,427
111
64.28%
99.91%
93.75%
99.96%
60.85%
86.18%
100.00%
100.00%
100.00%
99.97%
100.00%
99.99%
99.93%
99.50%
99.99%
100%
100%
99.03%
98.02%
100%
100%
99.04%
100%
100%
100%
100%
8.38%
99.99%
100%
667,104
685,233
618,572
247,409
300,824
503,828
8,467,229
8,433,142
8,352,630
8,126,108
7,933,501
8,262,522
1,518,640
2,380
252,296
9,475
44,982
20,044
6,376
148,885
98,142
127,302
40,344
27,401
477
9,031
109
3,951,130
37,363,478
57.52%
69.55%
46.51%
12.88%
30.89%
39.55%
97.59%
100.00%
100.00%
91.30%
100.00%
97.67%
46.57%
14.86%
55.77%
20.15%
49.31%
38.62%
87.09%
100%
99.34%