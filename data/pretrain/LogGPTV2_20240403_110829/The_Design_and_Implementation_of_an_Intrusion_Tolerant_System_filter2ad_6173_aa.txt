# The Design and Implementation of an Intrusion Tolerant System

**Authors:**
- James C. Reynolds
- James E. Just
- Ed Lawson
- Larry A. Clough
- Ryan Maglich
- Karl N. Levitt

**Affiliations:**
- Teknowledge Corporation: {reynolds, jjust, elawson, lclough, rmaglich}@teknowledge.com
- University of California Davis: karl_levitt@ucdavis.edu

## Abstract

This paper describes the design and implementation of an intrusion-tolerant system that provides secure Internet services to known users. Network attacks are treated as maliciously devised conditions to exploit design faults, and intrusions (successful attacks) are treated as failures. The system mitigates these effects using the three pillars of fault tolerance: detection, isolation, and recovery. A key feature of our approach is the use of diverse process pairs, which partially addresses detection and isolation problems. The architecture compares outputs from diverse applications to provide a novel intrusion detection capability. This also strengthens isolation by forcing attacks to exploit independent vulnerabilities. Intrusion isolation is mainly achieved with an out-of-band control system, which not only separates the primary and backup systems but also initiates attack diagnosis, blocking, and recovery, accelerated by on-line repair.

## 1. Introduction

Despite years of efforts to defend against computer and network attacks, such attacks still succeed frequently. Many techniques designed to build more secure systems, such as those at higher levels of the “Orange Book” [1], were expensive, specialized, and proprietary, making them unsuitable for most applications due to cost or timeliness.

A promising approach to building more secure yet affordable and timely systems is to combine Commercial-Off-The-Shelf (COTS) hardware and software with proven fault-tolerant techniques. COTS components can provide inexpensive but potentially unreliable parts, while fault-tolerant techniques can build reliable systems from these components. Highly available systems have been built using this approach [2].

Fault-tolerant techniques are typically designed for rare, independent faults. However, computer security faults have different characteristics. A programming mistake (fault) creates a latent error (vulnerability), which can be exploited by an attacker. An attacker can repeatedly try various exploits to find and exploit vulnerabilities. Additionally, successful attacks (intrusions) are often designed to propagate.

The specific goal of our project is to enable COTS-based software to continue providing acceptable service during several hours of cyber attacks by a determined, resourceful adversary. We have made significant progress toward this goal through innovative modifications to proven methods of achieving detection, isolation, and recovery.

The rest of the introduction briefly describes HACQIT (Hierarchical Adaptive Control of Quality of Service for Intrusion Tolerance), including its assumptions and failure model. Most of the paper (Sections 2-5) details the mechanisms for detection, isolation, and recovery, along with a demonstration. The final section discusses potential benefits and caveats.

### 1.1 Assumptions

HACQIT is not designed to be a general-purpose server connected to the Internet. Anonymous users are not allowed, and all connections are through a Virtual Private Network (VPN). We assume the system configuration is correct, including patching all known vulnerabilities. We also assume the Local Area Network (LAN) is reliable, cannot be flooded, and is the only means of communication between users and the system.

An attacker can be any agent other than trusted users or HACQIT system administrators. Attackers do not have physical access to the HACQIT cluster but may take over a trusted user’s machine and launch attacks against HACQIT.

### 1.2 Failure Model

A failure occurs when observed behavior deviates from specified behavior. For HACQIT, we are concerned with software failures, which can be repeatable or non-repeatable. Repeatable failures include attacks exploiting the same vulnerability in one of our software components. Non-repeatable failures may be caused by intermittent or transient faults. All inputs causing repeated failures are treated equally, while intermittent failures allow for retry.

### 1.3 System Architecture

The fundamental building block of our architecture is a HACQIT cluster, shown in Figure 1. A HACQIT cluster consists of at least four computers: a gateway running a commercial firewall and additional software for failover and attack blocking; two or more servers of critical applications (one primary, one backup, and one or more online spares); and an Out-Of-Band (OOB) machine running the overall monitoring, control, and fault diagnosis software. The machines in the cluster are connected by two separate LANs.

![Intrusion tolerant hardware architecture](figure1.png)

### 1.4 Software Architecture

Most of the software components shown in Figure 2 will be discussed in the next three sections. Here, we describe the Forensics Agent (FA). After failover, the FA performs attack diagnosis by analyzing the "App Log" containing recent requests to determine which request(s) caused the failure. "Bad Requests" are put on a list used by the Content Filter to tell the MAC if a future request should be blocked. The FA learns attacks from failures, while the Content Filter generalizes bad requests identified by the FA, so that simple variants are also blocked. This prevents previously unknown attacks and their variants from repeatedly causing failover, unlike the delays associated with receiving and applying updates to anti-virus software or software patches.

![Intrusion tolerant software architecture](figure2.png)

## 2. Detection

Duplicate and compare is an old and effective strategy for hardware error detection [3]. The same input is sent to identical components, and their outputs are compared. If the outputs differ, an error has been detected. If the input is retried and the outputs are the same, the failure was transient; otherwise, the components must be repaired or replaced.

Typically, the duplicated components are within a single device, such as a CPU, which, in the case of permanent (hard) failure, is the unit repaired or replaced. If this is not the case, the module or system consisting of the paired components must stop. (Following Gray and Reuter [4], we call this behavior failfast, because the delay between detecting the error and stopping must be minimized to prevent fault propagation.) If a system built on paired components simply stops, then the redundancy has supported error detection but not failure masking, which is necessary to provide continued service despite failures.

HACQIT combines redundancy and diversity to both detect errors and mask failures for software components. This is a unique innovation for software fault tolerance. HACQIT uses two software components with identical functional specifications but different implementations, combining the benefits of N-version programming without its controversial disadvantages, such as higher overall costs and lower quality components [5,6].

We can do this with web servers and other well-known, TCP/IP-based applications like mail servers, because these applications must implement a single specification, and there are already many implementations. HACQIT uses two web servers, Microsoft’s Internet Information Server (IIS) and the open-source Apache web server, to implement this design approach.

Every HTTP request to the primary is intercepted and sent to the MAC. After checking that the request is allowed (see Section 3), the request is sent to both web servers, one on the primary and one on the backup, one IIS and one Apache. The HTTP specification defines the status codes with which a web server must respond. The MAC compares the responses (analogous to the outputs from duplicate hardware). If they are the same, the result indicates that no failure (intrusion) has occurred. If the status codes are different, the result indicates that a failure may have occurred.

In the latter case, failfast semantics demand failover as quickly as possible, especially because the danger from intrusion propagation is greater than the danger from transient fault propagation. The question, then, is how to determine which web server was successfully attacked. An analysis of HTTP status codes makes this possible.

Status codes lie in five general categories [8]:
- **Informational (100-199):** Request received, continuing process.
- **Success (200-299):** Action was received, understood, and accepted.
- **Redirection (300-399):** Further action must be taken to complete the request.
- **Client Error (400-499):** The request contains bad syntax or cannot be fulfilled.
- **Server Error (500-599):** The server failed to fulfill an apparently valid request.

There are only ten different combinations of two different status codes. Most are nonsensical, rare, or not indicative of an error. One meaningful combination that can occur is 200/400, indicating one web server responded with success while the other responded with a client error.