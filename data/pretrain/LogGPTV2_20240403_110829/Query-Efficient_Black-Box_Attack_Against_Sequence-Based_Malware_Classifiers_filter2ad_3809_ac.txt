While some malware classiﬁers do expose their conﬁdence score (e.g., MAX, a non-sequence
based NGAV product in VirusTotal online scanning service [12]), others do not. Therefore, we
also implemented a decision-based attack, which is less query-eﬃcient, but requires only knowledge
about the predicted label of the malware classiﬁer.
Decision-Based Attack In Algorithm 2, we show how to generate an adversarial sequence for
a single API call window (out of the entire API call sequence).
ALGORITHM 2: Single Iteration Decision-Based Window Sequence Generation
1 Input: f - black-box model, wm - malicious sequence to perturb, of length lm ≤ n,
2 wb - benign sequence to mimic, of length lb ≤ n, n - size of adversarial sliding window,
3 D
- adversarial vocabulary, Mw - maximum API modiﬁcations per window, P erturbT ype - benign
(cid:48)
or random perturbation,
Randomly select an API’s position i in wm
if P erturbT ype is benign perturbation:
4 IterationM ethod (logarithmic backtracking or linear iteration.
5
6 addedAP Is = {}
7 while ((IterationM ethod is linear iteration) and (f (wm) = malicious)) or (|addedAP Is| < Mw):
8
9
10
11
12
13
14 if (f (wm) = malicious) and (|addedAP Is| = Mw): return Failure
15 return (wm, addedAP Is) # wm includes the perturbation
to wm in position i
Add the new API and its position to addedAP Is
else: P erturbT ype is random perturbation
Add wb[i] to wm at position i
Add a random API in D
(cid:48)
Accepted as a conference paper at ACSAC 2020
The perturbation added is either random API calls, a.k.a. random perturbation (line 12), or
API calls of a benign sequence, a.k.a. benign perturbation (line 10). Since only the predicted class
is available, the API calls are added in a random position i in the API sequence (line 8). The
adversaries randomly chooses i, since they do not have a better way of selecting i without incurring
signiﬁcant statistical overhead. Note that the addition of an API call in position i means that the
API calls from position i..n (wm[i..n] ) are “pushed back” one position to make room for the new
API call, in order to maintain the original sequence and preserve the original functionality of the
code (in line 10 and 12). Since the sliding window has a ﬁxed length, the last API call, wm[n+1], is
“pushed out” and removed from wm. This API call addition continues until the modiﬁed sequence
wm is classiﬁed as benign or more than Mw API calls are added, reaching the maximum overhead
limit (line 7). In this case the attack has failed (line 14). In the case of a linear iteration attack,
the API calls are added one at a time, checking whether the perturbed sequence evades detection
after each addition. The case of a logarithmic backtracking attack was explained in Section 3.2.1.
One might claim that a simpler attack can be used instead: insert n − 1 no-op API calls after
each API call from the original binary. This attack eﬀectiveness is 100%, and no queries are needed
to implement it, making it extremely query-eﬃcient. However, this trivial attack has two major
issues:
1. It is easy to detect this trivial attack using anomaly detection, since no actual benign program
call trace is composed like that.
2. Such malware would run much slower than the original malware due to the additional API
overhead, allowing the intrusion prevention systems of the victim to mitigate such malware,
e.g., by terminating perturbed ransomware, after encrypting only several ﬁles, due to its
perturbation induced slowness.
Score-Based Attack When the conﬁdence score of the malware classiﬁer is also returned, score-
based attacks (e.g., gradient-free optimization algorithms) can be applied as well. The merged
ﬂow for both attacks is described in Algorithm 3. Assuming the attacker has a budget of B
queries per API call window, the call to Algorithm 2 in line 9 of Algorithm 3 can be replaced with
B−log n iterations (line 11) of minimizing f (wm) (lines 13 and 15) by one of the score minimization
algorithms presented in Section 4.2.2. In order to use the same budget for all attacks, we chose
B = Mw (lines 9, 11).
All random perturbation variants try to minimize the target classiﬁer score by modifying only
the values of the added API call types (while the API call positions are random but ﬁxed, as in
Algorithm 2). Trying to modify both API types and positions with the same budget results in
inferior performance (this is not shown due to space limits). All benign perturbation variants try
to minimize the score by modifying only the API positions (while the API types are taken from the
GAN’s output).
The maximum additional API calls allowed per sliding window was set to 70 (50%). The search
space for this optimization would either be the Mw added API call type values (out of |D| values
each) if this is a random perturbation, or the Mw added API call indices in the adversarial window
(each with n possible values) if this is a benign perturbation.
Score-Based Query-Eﬃcient Attack for Discrete Input Sequence Most state-of-the-art
query-eﬃcient attacks for images assume continuous input ([43, 30]) and underperform when used
for discrete input (e.g., API calls or position indices), as shown in Table 3. Genetic algorithms
Accepted as a conference paper at ACSAC 2020
(GAs), which use mutation (random perturbation) in existing adversarial candidates and crossover
between several candidates (i.e., a combination of parts of several candidates), are an exception.
GAs work well with discrete sequences [17]. However, while crossover makes sense in the NLP
domain (e.g., for compound sentences), it makes little sense for API call sequences, where each
program has its own business logic. Another issue is the poor performance of a ﬁxed mutation rate,
usually used by GAs. It is better to use an adaptive mutation rate, which ﬁts itself to the domain
without knowledge expertise [23].
We decided to use the self-adaptive uniform mixing evolutionary algorithm (EA) proposed by
Dang et al.
[23]. It starts with a population of adversarial candidates, and in every generation,
a new population of candidates is produced, and the old generation dies. Besides the adversarial
sequence, each candidate carries an additional property: its mutation rate. Each new candidate is
produced in the same way:
1. The best of two uniformly selected individuals is selected (i.e., tournament selection).
2. The selected parent individual changes its mutation rate between two mutation rates:
and high, with a ﬁxed probability p. We used the values proposed in [23].
3. The parent replicates, with mutations occurring at the new mutation rate.
low
Although the selection mechanism does not take into account the mutation rate, the intuition is
that appropriate mutation rates are correlated with high ﬁtness.
We assume that the EA attack would be query-eﬃcient due to the following reasons:
1. The usage of the adaptive mutation rate helps reducing the number of queries, before the
highest impact element is added to the sequence.
2. Unlike other attacks (including other self-adaptive heuristic strategies, e.g., [43]), EA is eﬀec-
tive in a discrete feature space. This makes it more query-eﬃcient, because fewer queries are
needed before it fools the target classiﬁer, as can be seen in Tables 3 and 4.
3. The crossover (combining API calls from malicious and benign parents, instead of mutating
a malicious parent) used by other attacks (e.g., GA, also eﬃcient in discrete feature spaces)
makes no sense in the cyber domain, because it adds redundant API calls of the benign parent.
Thus, an algorithm that forgoes the crossover (and thus the addition of redundant API calls)
is more query-eﬃcient in the cyber domain.
4 Experimental Evaluation
4.1 Dataset and Target Malware Classiﬁers
We use the same dataset used in [42], because of its size: it contains 500,000 ﬁles (250,000 benign
samples and 250,000 malware samples), faithfully representing the malware families in the wild and
providing a proper setting for an attack comparison. Details about the dataset are provided in
Appendix A.
Each sample was run in Cuckoo Sandbox, a malware analysis system, for two minutes per
sample. The API call sequences generated by the inspected code were extracted from the JSON
report generated by Cuckoo Sandbox. The extracted API call sequences are used as the malware
Accepted as a conference paper at ACSAC 2020
ALGORITHM 3: Score-Based or Decision-Based Window Sequence Generation
1 Input: f - black-box model,
2 wm - malicious sequence to perturb, of length lm ≤ n, wb - benign sequence to mimic, of length
3 n - size of adversarial sliding window, D
4 Mw - maximum API modiﬁcations per window, P erturbT ype - benign or random perturbation,
5 IterationM ethod - logarithmic backtracking or linear iteration, AttackerKnowledge - decision or
- adversarial vocabulary,
(cid:48)
lb ≤ n,
(wm, addedAP Is) = Algorithm2(f, wm, wb, n, D
(cid:48)
, Mw, P erturbT ype, IterationM ethod)}
if IterationM ethod is logarithmic backtracking optimIterationsCount = Mw − lg n , else
score-based.
while (f (wm) = malicious) :
6
7 if AttackerKnowledge is decision-based:
8
9
10 else: #AttackerKnowledge is score-based
11
optimIterationsCount = Mw
if P erturbT ype is benign perturbation:
(wm, addedAP Is) =
12
13
14
15
ScoreM inimizationAlgorithm(f (wm), optimIterationsCount, addedAP IIndices)
else: # P erturbT ype is random perturbation:
(wm, addedAP Is) =
ScoreM inimizationAlgorithm(f (wm), optimIterationsCount,addedAP IV alues)
16 return (wm, addedAP Is) # wm includes the perturbation
classiﬁer’s features. The samples were run on dozens of Windows 8.1 OS instances on the cloud,
since most malware targets the Windows OS. Anti-sandbox malware was ﬁltered to prevent dataset
contamination (see Appendix A). After ﬁltering, the ﬁnal training set size is 360,000 samples, 36,000
of which serve as the validation set. The test set size is 36,000 samples. All sets are balanced between
malicious and benign samples.
While some ML-based dynamic analysis cloud services are used by enterprises, e.g., [6], there
are no trial versions of commercial products or open source API call-based deep learning malware
classiﬁers available (such commercial products target enterprises and involve supervised server in-
stallation). Dynamic models are also unavailable on VirusTotal. In order to compensate for this,
we used the malware classiﬁers detailed in Appendix B and simulated the cloud service use case by
deploying Keras [7] models on Amazon cloud using SageMaker [4], and then we queried them by
accessing the cloud service.
The API call sequences are split into windows of k API calls each, and each window is classiﬁed
in turn. Thus, the input of each of the classiﬁers is a vector of k = 140 (larger window sizes, such as
k = 1, 000, didn’t improve the classiﬁer’s accuracy) API call types with 314 possible values (those
monitored by Cuckoo Sandbox, mentioned in [2]).
The implementation and hyperparameters (loss function, dropout, activation functions, etc.) of
the target classiﬁers are described in Appendix B.
On the test set, all of the DNN classiﬁers achieve over 95% accuracy, and all other classiﬁers
reach over 90% accuracy. The classiﬁers’ false positive rate ranged from 0.5 to 1%.
Accepted as a conference paper at ACSAC 2020
4.2 Attack Performance
4.2.1 Attack Performance Metrics
In order to measure the performance of an attack, we consider three factors (by a descending order
of importance):
We consider the average number of malware classiﬁer queries the attack performs per adversarial
example before it is classiﬁed as benign by the malware classiﬁer. The attacker aims to minimize
this number, since in cloud scenarios, each query costs money and increases the probability of
adversarial attempt detection.
We also consider the attack eﬀectiveness, which is the percentage of malicious samples which
were correctly classiﬁed by the malware classiﬁer for which the adversarial sequence xm∗ generated
by Algorithm 1 was misclassiﬁed as benign by the malware classiﬁer. An attack is deﬁned as query-
eﬃcient if it has the highest attack eﬀectiveness for a given (ﬁxed) number of queries. (A diﬀerent
approach of a ﬁxed accuracy is computationally expensive to compute.)
Algorithm 1) to the malware samples, out of the total number of API calls.
Finally, we consider the attack overhead, that is, the fraction of API calls which were added (by
The average length of the API call sequence is: avg(length(xm)) ≈ 10, 000. We used a maximum
of Mw = 70 additional API calls per window of k = 140 API calls, limiting the perturbation run
time overhead (per window and thus per sample) to 50% in the worst case. While not shown here
due to space limits, higher Mw values cause higher average attack eﬀectiveness and overhead, and
more queries.
Adversarial attacks against images usually try to minimize the number of modiﬁed pixels in order
to evade human detection of the perturbation. One might claim that such deﬁnition of minimal
perturbation is irrelevant for API call traces: humans cannot inspect sequences of thousands or
millions of APIs, so an attacker can add an unlimited amount of API calls. However, one should
bear in mind that a malware aims to perform its malicious functionality as fast as possible. For
instance, ransomware usually starts by encrypting the most critical ﬁles (e.g., the ’My Documents’
folder) ﬁrst, so even if the user turns oﬀ the computer and sends the hard-drive to IT - damage
has already been done. The same is true for a key-logger - it aims to send the user passwords to
the attacker as soon as possible, so they can be used immediately, before the malware is detected
and neutralized. Moreover, adding too many API calls would cause the modiﬁed program’s proﬁle
to become anomalous, making it easier for anomaly detection intrusion detection systems, e.g.,
systems that measure CPU usage [35] or contain irregular API call subsequences [27] to detect
anomalies.
4.2.2 Comparison to Previous Work
Decision-Based Attack Performance A comparison of the attack eﬀectiveness and attack
overhead of our decision-based attack with logarithmic backtracking transformation and with benign
perturbation (Algorithm 1) to BiRand, the more eﬃcient attack used in Dang et al.
[24], and to
Rosenberg et al. for diﬀerent attacked malware classiﬁers is presented in Table 2 (an average of ﬁve
runs, with 100 queries).
Rosenberg et al. provides state-of-the-art performance for gradient-based attacks against a wide
range of classiﬁers. Dang et al. [24] presented a decision-based attack similar to ours, but limited
to non-sequential features. The attack eﬀectiveness of our logarithmic backtracking attack, shown
in Table 3 (logarithmic backtracking=yes columns), is identical to the BiRand algorithm presented
Accepted as a conference paper at ACSAC 2020
Table 2: Decision-Based Attack Performance (100 Queries)
Classiﬁer Type
Attack Ef-
fectiveness
Attack
Eﬀective-
Attack
Eﬀective-
Additional
API Calls
Additional
API Calls
Additional
API Calls
[%]
Our
Decision-
Based
Attack
ness
[%]
BiRand,
Dang et
al. 2017
ness
[%]
Rosenberg