a
l
i
m
S
i
 1
 0.8
 0.6
 0.4
 0.2
 0
 1
 0.8
 0.6
 0.4
 0.2
 0
Weight: views
Weight: value
Weight: log(views)
Overlap
 0
 20
 40
 60
 80  100  120  140  160  180  200
Snapshot (sorted)
(a) Identical setup (higher is better)
Weight: views
Weight: value
Weight: log(views)
Overlap
 0
 20
 40
 60
 80  100  120  140  160  180  200
Snapshot (sorted)
(b) Diﬀerent setup (lower is better)
Figure 2: CDF of similarity scores computed by the four metrics
tracked over 8 days.
those near the bottom. By taking the number of times the ad
was seen into account, each weight function also eﬀectively
attenuates noise when snapshots are aggregated together.
Experiment 3: To determine which approach performs
the best we conducted the following experiment. We simul-
taneously collect snapshots from two browser instances con-
ﬁgured identically and on the same machine (in New York);
we expect the snapshots to be substantially the same. We
also simultaneously collect snapshots from a machine set up
at a remote location (in San Francisco) where we expect to
see some diﬀerences in the set of ads. Snapshots (for 15
queries) are collected every 5 minutes for a period of 8 days.
We then aggregate 1 hour’s worth of data (12 snapshots)
and compare the performance of all four metrics: the Jac-
card index, and the extended Jaccard index with the three
weight functions.
Figure 2(a) plots the CDF of the computed metric value
for the case where we expect snapshots to be identical; the
closer to y = 1 the better. As expected, the plain Jaccard
index performs poorly. The other three perform quite well,
with the logarithmic weight function trailing slightly due to
the reduced inﬂuence of highly-stable ads. Figure 2(b) plots
the CDF for the case where we expect snapshots to be dif-
ferent; the greater the diﬀerence between the corresponding
lines between 2(a) and 2(b) the better. The logarithmic
weight clearly outperforms the other two here. This is be-
cause for the other two weight functions, and especially so for
weights based on the ad value, the long tail of ads is drowned
out by a handful of highly-stable highly-ranked ads, which
tend to be from large companies (e.g. eBay, Amazon) that
target broadly across many demographics, interests, and lo-
cations.
In our analysis we use the extended Jaccard index with
logarithmic weights to quantify the similarity or diﬀerence
between snapshots. The metric in practice is both robust to
83e
r
o
c
s
y
t
i
r
a
l
i
m
S
i
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
Same datacenter (A vs. B)
Different datacenter (A vs. C)
 20
 40
 60
 80
 100
 120
Snapshot (sorted)
Figure 3: Artifact of DNS load-balancing by ad network.
noise, as well as sensitive to changes in the underlying set of
ads.
2.4 Avoiding Artifacts
During the course of our experiments we identiﬁed sev-
eral measurement artifacts that stemmed from poor inter-
actions between lower level protocols and the operational
architecture of the ad network. We discuss two artifacts
and how they can be eliminated: the ﬁrst pertains to DNS
load-balancing, and the second to distributed data collection
from multiple machines.
In one case we observed discrepancies between data col-
lected by three identically conﬁgured browser instances all
running on the same machine. Snapshots for the ﬁrst two
browser instances (A and B) are virtually identical, while
that for the third instance (C) is very diﬀerent (Figure 3).
We discovered the ad network domain is DNS load-balanced
and the browsers did not share a DNS cache (and therefore
each queried DNS independently). Instance A and B were
communicating with diﬀerent IP addresses in the same /24
(same city), while C was communicating with an IP address
in the same /16 but diﬀerent /24 (diﬀerent nearby city),
which we take to mean two diﬀerent datacenters. Thus even
for identical requests from the same source, the choice of
datacenter, we found, dramatically aﬀects the ads served.
This artifact is, of course, easily avoided by conﬁguring a
static entry in the hosts ﬁle so all instances reach the same
datacenter.
Even with static DNS entries, we sometimes (but not al-
ways) observed discrepancies when running identical browser
instances on diﬀerent machines. The 5th percentile similar-
ity score dropped to 0.87 for diﬀerent machines (compared
to 0.99 in the same-machine case). We noticed the cook-
ies assigned to the two instances were diﬀerent; when we
synchronized the cookie values, the noise disappeared.
In
another case, we measured similar levels of noise when we
added a HTTP proxy in front of the two machines (which
downgraded HTTP/1.1 to 1.0). We believe these artifacts
are because of black-box frontend load-balancer behavior at
the ad network that, based on at least the IP address, HTTP
version, and cookies, we suspect, directs the requests to dif-
ferent backend servers, each of which has a slightly diﬀerent
cache of ads. The only way to mitigate this source of noise,
we believe, is to ensure as many header ﬁelds are held con-
stant as possible. Ideally, traces for an experiment are all
collected from a single IP address, not behind a proxy, with
cookies synchronized across browser instances.
Applying these techniques signiﬁcantly reduces the base
noise level, but does not eliminate it. We therefore measure
the noise during our experiments to detect anomalies and to
establish a level of conﬁdence in our results.
3. ANALYSIS
In this section we use the above methodology to explore
speciﬁc questions regarding how ads are targeted in three
diﬀerent contexts: search, websites, and online social net-
works. These questions include, among others, whether be-
havioral targeting aﬀects search ads, whether past searches
aﬀect ads on websites, and what proﬁle data aﬀects social
network ads. That said, since ad targeting is a black-box
where we can reliably control only a small set of inputs, we
are restricted in the questions we can answer. An example
of a question we cannot answer is whether Google learns
the user’s gender by observing which search results the user
clicks and then uses it to target ads; this is because we can-
not reliably aﬀect or verify the gender learned by Google’s
(black-box) algorithm if it indeed does so at all.
For questions where we can reliably aﬀect the inputs to
the ad selection algorithm, our experimental methodology is
as follows. For each experiment we conﬁgure two (or more)
measurement instances to diﬀer by exactly one input param-
eter, and conﬁgure two measurement instances identically
to serve as the noise-level control.
If the similarity score
between the control pair is high (i.e.
low noise) but that
between two diﬀerently conﬁgured instances is low, we con-
clude that the input parameter in which the two instances
diﬀer aﬀects the choice of ads. For scalability and repeata-
bility, all experiments are scripted using the Chickenfoot
browser automation framework [2].
3.1 Search Ads
Search ads have typically been targeted based on keywords
in the search query. It is therefore expected that keyword
based targeting dominates search. The question we ask is: to
what extent does behavioral targeting aﬀect search ads? Be-
havioral targeting refers to using the user’s browsing habits
to inﬂuence ad selection.
Experiment 4: We set up four browser instances: the
ﬁrst two (A and B), which also serve as our control, disable
DoubleClick’s DART cookie [3] that Google uses for behav-
ioral targeting. The third (C) and fourth (D) have cookies
enabled, but are seeded with diﬀerent user personae3. C was
seeded with long-term interests in ‘Autos & Vehicles’, while
D was seeded with interests in ‘Shopping’. We then perform
Google searches for 730 random product-related queries for
a period of 5 days.
Figure 4(a) illustrates to what extent behavioral target-
ing aﬀected search ads. As is evident from the ﬁgure, for
keywords where the data is not too noisy (i.e. control score
is high), there is no appreciable diﬀerence in the ads served
whether the behavioral targeting cookie is disabled or en-
abled (A vs. C), or for two users with diﬀerent interests (C
vs. D). To understand why, we looked at the ads served.
73% of them contained the whole search query somewhere
in the ad, and 97% of them contained at least one word from
the search query. It is therefore clear that ads are selected
primarily based on keywords. Furthermore the average num-
ber of unique ads for our search queries is 8. Since all the
ads matching the keyword can be shown to the user in a
3Google normally learns short-term and long-term user in-
terests completely automatically. It also allows users to view
and modify the learned interests (http://www.google.com/
ads/preferences), which we use to create (or verify Google
learned) diﬀerent personae.
84e
r
o
c
s
y
t
i
r
a
l
i
m
S
i
 1
 0.8
 0.6
 0.4
 0.2
 0
e
r
o
c
s
y
t
i
r
a
l
i
m
S
i
 1
 0.8
 0.6
 0.4
 0.2
 0
Control (A vs. B)
Cookie enabled (A vs. C)
Different interests (C vs. D)
Control (A vs. B)
Different coast (A vs. C)
Different country (A vs. D)
Control (A vs. B)
Different interests (A vs. C)
Different recent searches (A vs. D)
Different recent clicks (A vs. E)
e
r
o
c
s
y
t
i
r
a
l
i
m
S
i
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
 100
 200
 300
 400
 500
 600
 700
 800
 0
 20
 40
 60
 80  100  120  140  160  180  200
 0
 20
 40
 60
 80
 100
 120
 140
 160
Search term (sorted)
(a)
Snapshot (sorted)
(b)
Snapshot (sorted)
(c)
Figure 4: (a) Behavioral targeting doesn’t appear to aﬀect search ads. (b) Location aﬀects website ads. (c) The aﬀect of user behavior
(browsing, search, clicks) on website ads is indistinguishable from noise in the system.
short period, there may be little reason to pick and choose
any further.
3.2 Website Ads
Website ads have typically been based on the context of
the page and are hence also known as contextual advertising.
We ask here to what extent the user’s location aﬀects the
choice of ads, and to what extent the user’s behavior (brows-
ing behavior, recent searches, and recent clicks on products)
aﬀects website ads. We measure a set of 15 websites that
show Google ads; the websites are picked randomly from the
set of websites visited by CoDeeN [1] users. While we are
able to answer the location question, we are able to present
only weak evidence towards the lack of use of behavioral
data due to noise.
Experiment 5: To understand the impact of user loca-
tion, we set up four browsers: A and B, the control pair, in
the same city in the US (New York), C in a diﬀerent city on
the other coast (San Francisco), and E in a diﬀerent country
(Germany).
Figure 4(b) plots the similarity scores between the diﬀer-
ent instances. As one might expect, location aﬀects the set
of ads, but interestingly, there is (relatively) little diﬀerence
between cities on opposite coasts (median similarity of 0.9).
This is higher than we expected given the long-tail of ads
appears to contain local mom-and-pop retailers, although in
retrospect, these retailers may nevertheless conduct business
nation wide.
Experiment 6: To understand the eﬀect of user behav-
ior, we set up ﬁve browsers: instance A and B, the control
pair, are conﬁgured identically except for the cookie that is
needed for tracking user behavior. For C we browse 3 out
of the 15 websites in the query set until Google learns the
set of long-interests associated with those websites (which
we veriﬁed3). For D and E we additionally browse random
websites and perform Google searches on 50 product-related
keywords shortly before collecting each snapshot (but don’t
click any result for D); we veriﬁed3 that Google learned
short-term interests for the random websites visited. Fi-
nally, for E we additionally click on product results before