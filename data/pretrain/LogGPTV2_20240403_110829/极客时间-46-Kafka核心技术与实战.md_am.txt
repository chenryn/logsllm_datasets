# 16 \| 揭开神秘的"位移主题"面纱你好，我是胡夕。今天我要和你分享的内容是：Kafka中神秘的内部主题（Internal Topic）\_\_consumer_offsets。\_\_consumer_offsets 在 Kafka源码中有个更为正式的名字，叫**位移主题**，即 OffsetsTopic。为了方便今天的讨论，我将统一使用位移主题来指代\_\_consumer_offsets。需要注意的是，它有两个下划线哦。好了，我们开始今天的内容吧。首先，我们有必要探究一下位移主题被引入的背景及原因，即位移主题的前世今生。在上一期中，我说过老版本 Consumer 的位移管理是依托于 Apache ZooKeeper的，它会自动或手动地将位移数据提交到 ZooKeeper 中保存。当 Consumer重启后，它能自动从 ZooKeeper中读取位移数据，从而在上次消费截止的地方继续消费。这种设计使得 KafkaBroker 不需要保存位移数据，减少了 Broker端需要持有的状态空间，因而有利于实现高伸缩性。但是，ZooKeeper 其实并不适用于这种高频的写操作，因此，Kafka 社区自0.8.2.x 版本开始，就在酝酿修改这种设计，并最终在新版本 Consumer中正式推出了全新的位移管理机制，自然也包括这个新的位移主题。``{=html}新版本 Consumer 的位移管理机制其实也很简单，就是**将 Consumer的位移数据作为一条条普通的 Kafka 消息，提交到 \_\_consumer_offsets中。可以这么说，\_\_consumer_offsets 的主要作用是保存 Kafka消费者的位移信息。**它要求这个提交过程不仅要实现高持久性，还要支持高频的写操作。显然，Kafka的主题设计天然就满足这两个条件，因此，使用 Kafka主题来保存位移这件事情，实际上就是一个水到渠成的想法了。这里我想再次强调一下，和你创建的其他主题一样，位移主题就是普通的 Kafka主题。你可以手动地创建它、修改它，甚至是删除它。只不过，它同时也是一个内部主题，大部分情况下，你其实并不需要"搭理"它，也不用花心思去管理它，把它丢给Kafka 就完事了。虽说位移主题是一个普通的 Kafka 主题，但**它的消息格式却是 Kafka自己定义的**，用户不能修改，也就是说你不能随意地向这个主题写消息，因为一旦你写入的消息不满足Kafka 规定的格式，那么 Kafka 内部无法成功解析，就会造成 Broker的崩溃。事实上，Kafka Consumer 有 API帮你提交位移，也就是向位移主题写消息。你千万不要自己写个 Producer随意向该主题发送消息。你可能会好奇，这个主题存的到底是什么格式的消息呢？所谓的消息格式，你可以简单地理解为是一个KV 对。Key 和 Value 分别表示消息的键值和消息体，在 Kafka中它们就是字节数组而已。想象一下，如果让你来设计这个主题，你觉得消息格式应该长什么样子呢？我先不说社区的设计方案，我们自己先来设计一下。首先从 Key 说起。一个 Kafka 集群中的 Consumer数量会有很多，既然这个主题保存的是 Consumer的位移数据，那么消息格式中必须要有字段来标识这个位移数据是哪个 Consumer的。这种数据放在哪个字段比较合适呢？显然放在 Key 中比较合适。现在我们知道该主题消息的 Key 中应该保存标识 Consumer 的字段，那么，当前Kafka 中什么字段能够标识 Consumer 呢？还记得之前我们说 Consumer Group时提到的 Group ID 吗？没错，就是这个字段，它能够标识唯一的 ConsumerGroup。说到这里，我再多说几句。除了 Consumer Group，Kafka 还支持独立Consumer，也称 Standalone Consumer。它的运行机制与 Consumer Group完全不同，但是位移管理的机制却是相同的。因此，即使是 StandaloneConsumer，也有自己的 Group ID 来标识它自己，所以也适用于这套消息格式。Okay，我们现在知道 Key 中保存了 Group ID，但是只保存 Group ID就可以了吗？别忘了，Consumer提交位移是在分区层面上进行的，即它提交的是某个或某些分区的位移，那么很显然，Key中还应该保存 Consumer 要提交位移的分区。好了，我们来总结一下我们的结论。**位移主题的 Key 中应该保存 3部分内容：\**。如果你认同这样的结论，那么恭喜你，社区就是这么设计的！接下来，我们再来看看消息体的设计。也许你会觉得消息体应该很简单，保存一个位移值就可以了。实际上，社区的方案要复杂得多，比如消息体还保存了位移提交的一些其他元数据，诸如时间戳和用户自定义的数据等。保存这些元数据是为了帮助Kafka执行各种各样后续的操作，比如删除过期位移消息等。但总体来说，我们还是可以简单地认为消息体就是保存了位移值。当然了，位移主题的消息格式可不是只有这一种。事实上，它有 3种消息格式。除了刚刚我们说的这种格式，还有 2 种格式：1.  用于保存 Consumer Group 信息的消息。2.  用于删除 Group 过期位移甚至是删除 Group 的消息。第 1种格式非常神秘，以至于你几乎无法在搜索引擎中搜到它的身影。不过，你只需要记住它是用来注册Consumer Group 的就可以了。第 2 种格式相对更加有名一些。它有个专属的名字：tombstone消息，即墓碑消息，也称 delete mark。下次你在 Google或百度中见到这些词，不用感到惊讶，它们指的是一个东西。这些消息只出现在源码中而不暴露给你。它的主要特点是它的消息体是null，即空消息体。那么，何时会写入这类消息呢？一旦某个 Consumer Group 下的所有 Consumer实例都停止了，而且它们的位移数据都已被删除时，Kafka会向位移主题的对应分区写入 tombstone 消息，表明要彻底删除这个 Group的信息。好了，消息格式就说这么多，下面我们来说说位移主题是怎么被创建的。通常来说，**当Kafka 集群中的第一个 Consumer 程序启动时，Kafka会自动创建位移主题**。我们说过，位移主题就是普通的 Kafka主题，那么它自然也有对应的分区数。但如果是 Kafka自动创建的，分区数是怎么设置的呢？这就要看 Broker 端参数offsets.topic.num.partitions 的取值了。它的默认值是 50，因此 Kafka会自动创建一个 50 分区的位移主题。如果你曾经惊讶于 Kafka日志路径下冒出很多 \_\_consumer_offsets-xxx这样的目录，那么现在应该明白了吧，这就是 Kafka自动帮你创建的位移主题啊。你可能会问，除了分区数，副本数或备份因子是怎么控制的呢？答案也很简单，这就是Broker 端另一个参数 offsets.topic.replication.factor要做的事情了。它的默认值是 3。总结一下，**如果位移主题是 Kafka 自动创建的，那么该主题的分区数是50，副本数是 3**。当然，你也可以选择手动创建位移主题，具体方法就是，在 Kafka集群尚未启动任何 Consumer 之前，使用 Kafka API创建它。手动创建的好处在于，你可以创建满足你实际场景需要的位移主题。比如很多人说50个分区对我来讲太多了，我不想要这么多分区，那么你可以自己创建它，不用理会offsets.topic.num.partitions 的值。不过我给你的建议是，还是让 Kafka 自动创建比较好。目前 Kafka源码中有一些地方硬编码了 50分区数，因此如果你自行创建了一个不同于默认分区数的位移主题，可能会碰到各种各种奇怪的问题。这是社区的一个bug，目前代码已经修复了，但依然在审核中。创建位移主题当然是为了用的，那么什么地方会用到位移主题呢？我们前面一直在说Kafka Consumer 提交位移时会写入该主题，那 Consumer是怎么提交位移的呢？目前 Kafka Consumer提交位移的方式有两种：**自动提交位移和手动提交位移。**Consumer 端有个参数叫 enable.auto.commit，如果值是 true，则 Consumer在后台默默地为你定期提交位移，提交间隔由一个专属的参数auto.commit.interval.ms来控制。自动提交位移有一个显著的优点，就是省事，你不用操心位移提交的事情，就能保证消息消费不会丢失。但这一点同时也是缺点。因为它太省事了，以至于丧失了很大的灵活性和可控性，你完全没法把控Consumer 端的位移管理。事实上，很多与 Kafka 集成的大数据框架都是禁用自动提交位移的，如Spark、Flink 等。这就引出了另一种位移提交方式：**手动提交位移**，即设置enable.auto.commit = false。一旦设置了 false，作为 Consumer应用开发的你就要承担起位移提交的责任。Kafka Consumer API为你提供了位移提交的方法，如 consumer.commitSync等。当调用这些方法时，Kafka 会向位移主题写入相应的消息。如果你选择的是自动提交位移，那么就可能存在一个问题：只要 Consumer一直启动着，它就会无限期地向位移主题写入消息。我们来举个极端一点的例子。假设 Consumer当前消费到了某个主题的最新一条消息，位移是100，之后该主题没有任何新消息产生，故 Consumer无消息可消费了，所以位移永远保持在100。由于是自动提交位移，位移主题中会不停地写入位移 =100 的消息。显然Kafka只需要保留这类消息中的最新一条就可以了，之前的消息都是可以删除的。这就要求Kafka必须要有针对位移主题消息特点的消息删除策略，否则这种消息会越来越多，最终撑爆整个磁盘。Kafka 是怎么删除位移主题中的过期消息的呢？答案就是Compaction。国内很多文献都将其翻译成压缩，我个人是有一点保留意见的。在英语中，压缩的专有术语是Compression，它的原理和 Compaction很不相同，我更倾向于翻译成压实，或干脆采用 JVM 垃圾回收中的术语：整理。不管怎么翻译，Kafka 使用**Compact策略**来删除位移主题中的过期消息，避免该主题无限期膨胀。那么应该如何定义Compact 策略中的过期呢？对于同一个 Key 的两条消息 M1 和 M2，如果 M1的发送时间早于 M2，那么 M1 就是过期消息。Compact的过程就是扫描日志的所有消息，剔除那些过期的消息，然后把剩下的消息整理在一起。我在这里贴一张来自官网的图片，来说明Compact 过程。![](Images/72613960367c4c58c9db2b1a74b061be.png){savepage-src="https://static001.geekbang.org/resource/image/86/e7/86a44073aa60ac33e0833e6a9bfd9ae7.jpeg"}图中位移为 0、2 和 3 的消息的 Key 都是 K1。Compact之后，分区只需要保存位移为 3 的消息，因为它是最新发送的。**Kafka 提供了专门的后台线程定期地巡检待 Compact的主题，看看是否存在满足条件的可删除数据**。这个后台线程叫 LogCleaner。很多实际生产环境中都出现过位移主题无限膨胀占用过多磁盘空间的问题，如果你的环境中也有这个问题，我建议你去检查一下Log Cleaner 线程的状态，通常都是这个线程挂掉了导致的。
## 小结总结一下，今天我跟你分享了 Kafka 神秘的位移主题\_\_consumer_offsets，包括引入它的契机与原因、它的作用、消息格式、写入的时机以及管理策略等，这对我们了解Kafka 特别是 Kafka Consumer的位移管理是大有帮助的。实际上，将很多元数据以消息的方式存入 Kafka内部主题的做法越来越流行。除了 Consumer 位移管理，Kafka事务也是利用了这个方法，当然那是另外的一个内部主题了。社区的想法很简单：既然 Kafka天然实现了高持久性和高吞吐量，那么任何有这两个需求的子服务自然也就不必求助于外部系统，用Kafka 自己实现就好了。
## 开放讨论今天我们说了位移主题的很多好处，请思考一下，与 ZooKeeper方案相比，它可能的劣势是什么？欢迎写下你的思考和答案，我们一起讨论。如果你觉得有所收获，也欢迎把文章分享给你的朋友。![](Images/a7d15815f9efb5693db5b2d278244658.png){savepage-src="https://static001.geekbang.org/resource/image/c8/bf/c89da43deab85fe7cb06acec867aa5bf.jpg"}
# 17 \| 消费者组重平衡能避免吗？你好，我是胡夕。今天我要和你分享的内容是：消费者组重平衡能避免吗?其实在专栏[第 15期](https://time.geekbang.org/column/article/105112)中，我们讲过重平衡，也就是Rebalance，现在先来回顾一下这个概念的原理和用途。Rebalance 就是让一个Consumer Group 下所有的 Consumer实例就如何消费订阅主题的所有分区达成共识的过程。在 Rebalance过程中，所有 Consumer实例共同参与，在协调者组件的帮助下，完成订阅主题分区的分配。但是，在整个过程中，所有实例都不能消费任何消息，因此它对Consumer 的 TPS 影响很大。你可能会对这里提到的"协调者"有些陌生，我来简单介绍下。所谓协调者，在Kafka 中对应的术语是 Coordinator，它专门为 Consumer Group 服务，负责为Group 执行 Rebalance 以及提供位移管理和组成员管理等。具体来讲，Consumer 端应用程序在提交位移时，其实是向 Coordinator 所在的Broker 提交位移。同样地，当 Consumer 应用启动时，也是向 Coordinator所在的 Broker 发送各种请求，然后由 Coordinator负责执行消费者组的注册、成员管理记录等元数据管理操作。``{=html}所有 Broker 在启动时，都会创建和开启相应的 Coordinator组件。也就是说，**所有 Broker 都有各自的 Coordinator组件**。那么，Consumer Group 如何确定为它服务的 Coordinator 在哪台Broker 上呢？答案就在我们之前说过的 Kafka 内部位移主题\_\_consumer_offsets 身上。目前，Kafka 为某个 Consumer Group 确定 Coordinator 所在的 Broker的算法有 2 个步骤。第 1 步：确定由位移主题的哪个分区来保存该 Group数据：partitionId=Math.abs(groupId.hashCode() %offsetsTopicPartitionCount)。第 2 步：找出该分区 Leader 副本所在的 Broker，该 Broker 即为对应的Coordinator。简单解释一下上面的算法。首先，Kafka 会计算该 Group 的 group.id参数的哈希值。比如你有个 Group 的 group.id设置成了"test-group"，那么它的 hashCode 值就应该是627841412。其次，Kafka 会计算 \_\_consumer_offsets 的分区数，通常是 50个分区，之后将刚才那个哈希值对分区数进行取模加求绝对值计算，即abs(627841412 % 50) = 12。此时，我们就知道了位移主题的分区 12负责保存这个 Group 的数据。有了分区号，算法的第 2步就变得很简单了，我们只需要找出位移主题分区 12 的 Leader 副本在哪个Broker 上就可以了。这个 Broker，就是我们要找的 Coordinator。在实际使用过程中，Consumer 应用程序，特别是 Java ConsumerAPI，能够自动发现并连接正确的Coordinator，我们不用操心这个问题。知晓这个算法的最大意义在于，它能够帮助我们解决**定位问题**。当Consumer Group 出现问题，需要快速排查 Broker端日志时，我们能够根据这个算法准确定位 Coordinator 对应的Broker，不必一台 Broker 一台 Broker 地盲查。好了，我们说回 Rebalance。既然我们今天要讨论的是如何避免Rebalance，那就说明 Rebalance这个东西不好，或者说至少有一些弊端需要我们去规避。那么，Rebalance的弊端是什么呢？总结起来有以下 3 点：1.  Rebalance 影响 Consumer 端    TPS。这个之前也反复提到了，这里就不再具体讲了。总之就是，在    Rebalance 期间，Consumer 会停下手头的事情，什么也干不了。2.  Rebalance 很慢。如果你的 Group    下成员很多，就一定会有这样的痛点。还记得我曾经举过的那个国外用户的例子吧？他的    Group 下有几百个 Consumer 实例，Rebalance    一次要几个小时。在那种场景下，Consumer Group 的 Rebalance    已经完全失控了。3.  Rebalance 效率不高。当前 Kafka 的设计机制决定了每次 Rebalance    时，Group    下的所有成员都要参与进来，而且通常不会考虑局部性原理，但局部性原理对提升系统性能是特别重要的。关于第 3 点，我们来举个简单的例子。比如一个 Group 下有 10个成员，每个成员平均消费 5个分区。假设现在有一个成员退出了，此时就需要开启新一轮的Rebalance，把这个成员之前负责的 5个分区"转移"给其他成员。显然，比较好的做法是维持当前 9个成员消费分区的方案不变，然后将 5 个分区随机分配给这 9个成员，这样能最大限度地减少 Rebalance 对剩余 Consumer 成员的冲击。遗憾的是，目前 Kafka 并不是这样设计的。在默认情况下，每次 Rebalance时，之前的分配方案都不会被保留。就拿刚刚这个例子来说，当 Rebalance开始时，Group 会打散这 50 个分区（10 个成员 \* 5 个分区），由当前存活的9 个成员重新分配它们。显然这不是效率很高的做法。基于这个原因，社区于0.11.0.0 版本推出了StickyAssignor，即有粘性的分区分配策略。所谓的有粘性，是指每次 Rebalance时，该策略会尽可能地保留之前的分配方案，尽量实现分区分配的最小变动。不过有些遗憾的是，这个策略目前还有一些bug，而且需要升级到 0.11.0.0才能使用，因此在实际生产环境中用得还不是很多。总而言之，Rebalance有以上这三个方面的弊端。你可能会问，这些问题有解吗？特别是针对 Rebalance慢和影响 TPS这两个弊端，社区有解决办法吗？针对这两点，我可以很负责任地告诉你："无解！"特别是Rebalance 慢这个问题，Kafka社区对此无能为力。"本事大不如不摊上"，既然我们没办法解决 Rebalance过程中的各种问题，干脆就避免 Rebalance 吧，特别是那些不必要的Rebalance。就我个人经验而言，**在真实的业务场景中，很多 Rebalance都是计划外的或者说是不必要的**。我们应用的 TPS 大多是被这类 Rebalance拖慢的，因此避免这类 Rebalance就显得很有必要了。下面我们就来说说如何避免 Rebalance。要避免 Rebalance，还是要从 Rebalance发生的时机入手。我们在前面说过，Rebalance 发生的时机有三个：-   组成员数量发生变化-   订阅主题数量发生变化-   订阅主题的分区数发生变化后面两个通常都是运维的主动操作，所以它们引发的 Rebalance大都是不可避免的。接下来，我们主要说说因为组成员数量变化而引发的Rebalance 该如何避免。如果 Consumer Group 下的 Consumer 实例数量发生变化，就一定会引发Rebalance。这是 Rebalance 发生的最常见的原因。我碰到的 99% 的Rebalance，都是这个原因导致的。Consumer 实例增加的情况很好理解，当我们启动一个配置有相同 group.id 值的Consumer 程序时，实际上就向这个 Group 添加了一个新的 Consumer实例。此时，Coordinator会接纳这个新实例，将其加入到组中，并重新分配分区。通常来说，增加Consumer 实例的操作都是计划内的，可能是出于增加 TPS或提高伸缩性的需要。总之，它不属于我们要规避的那类"不必要 Rebalance"。我们更在意的是 Group 下实例数减少这件事。如果你就是要停掉某些 Consumer实例，那自不必说，关键是在某些情况下，Consumer 实例会被 Coordinator错误地认为"已停止"从而被"踢出"Group。如果是这个原因导致的Rebalance，我们就不能不管了。Coordinator 会在什么情况下认为某个 Consumer实例已挂从而要退组呢？这个绝对是需要好好讨论的话题，我们来详细说说。当 Consumer Group 完成 Rebalance 之后，每个 Consumer 实例都会定期地向Coordinator 发送心跳请求，表明它还存活着。如果某个 Consumer实例不能及时地发送这些心跳请求，Coordinator 就会认为该 Consumer已经"死"了，从而将其从 Group 中移除，然后开启新一轮 Rebalance。Consumer端有个参数，叫session.timeout.ms，就是被用来表征此事的。该参数的默认值是 10 秒，即如果Coordinator 在 10 秒之内没有收到 Group 下某 Consumer实例的心跳，它就会认为这个 Consumer实例已经挂了。可以这么说，session.timout.ms 决定了 Consumer存活性的时间间隔。除了这个参数，Consumer还提供了一个允许你控制发送心跳请求频率的参数，就是heartbeat.interval.ms。这个值设置得越小，Consumer实例发送心跳请求的频率就越高。频繁地发送心跳请求会额外消耗带宽资源，但好处是能够更加快速地知晓当前是否开启Rebalance，因为，目前 Coordinator 通知各个 Consumer 实例开启 Rebalance的方法，就是将 REBALANCE_NEEDED 标志封装进心跳请求的响应体中。除了以上两个参数，Consumer 端还有一个参数，用于控制 Consumer实际消费能力对 Rebalance 的影响，即 max.poll.interval.ms 参数。它限定了Consumer 端应用程序两次调用 poll 方法的最大时间间隔。它的默认值是 5分钟，表示你的 Consumer 程序如果在 5 分钟之内无法消费完 poll方法返回的消息，那么 Consumer 会主动发起"离开组"的请求，Coordinator也会开启新一轮 Rebalance。搞清楚了这些参数的含义，接下来我们来明确一下到底哪些 Rebalance是"不必要的"。**第一类非必要 Rebalance 是因为未能及时发送心跳，导致 Consumer被"踢出"Group 而引发的**。因此，你需要仔细地设置**session.timeout.ms 和heartbeat.interval.ms**的值。我在这里给出一些推荐数值，你可以"无脑"地应用在你的生产环境中。-   设置 session.timeout.ms = 6s。-   设置 heartbeat.interval.ms = 2s。-   要保证 Consumer 实例在被判定为"dead"之前，能够发送至少 3    轮的心跳请求，即 session.timeout.ms \>= 3 \* heartbeat.interval.ms。将 session.timeout.ms 设置成 6s 主要是为了让 Coordinator能够更快地定位已经挂掉的Consumer。毕竟，我们还是希望能尽快揪出那些"尸位素餐"的Consumer，早日把它们踢出Group。希望这份配置能够较好地帮助你规避第一类"不必要"的 Rebalance。**第二类非必要 Rebalance 是 Consumer消费时间过长导致的**。我之前有一个客户，在他们的场景中，Consumer消费数据时需要将消息处理之后写入到MongoDB。显然，这是一个很重的消费逻辑。MongoDB 的一丁点不稳定都会导致Consumer程序消费时长的增加。此时，**max.poll.interval.ms**参数值的设置显得尤为关键。如果要避免非预期的Rebalance，你最好将该参数值设置得大一点，比你的下游最大处理时间稍长一点。就拿MongoDB 这个例子来说，如果写 MongoDB 的最长时间是 7分钟，那么你可以将该参数设置为 8 分钟左右。总之，你要为你的业务处理逻辑留下充足的时间。这样，Consumer就不会因为处理这些消息的时间太长而引发 Rebalance 了。如果你按照上面的推荐数值恰当地设置了这几个参数，却发现还是出现了Rebalance，那么我建议你去排查一下**Consumer 端的 GC表现**，比如是否出现了频繁的 Full GC 导致的长时间停顿，从而引发了Rebalance。为什么特意说 GC？那是因为在实际场景中，我见过太多因为 GC设置不合理导致程序频发 Full GC 而引发的非预期 Rebalance 了。
## 小结总而言之，我们一定要避免因为各种参数或逻辑不合理而导致的组成员意外离组或退出的情形，与之相关的主要参数有：-   session.timeout.ms-   heartbeat.interval.ms-   max.poll.interval.ms-   GC 参数按照我们今天所说的内容，恰当地设置这些参数，你一定能够大幅度地降低生产环境中的Rebalance 数量，从而整体提升 Consumer 端 TPS。
## 开放讨论说说在你的业务场景中，Rebalance发生的频率、原因，以及你是怎么应对的，我们一起讨论下是否有更好的解决方案。欢迎写下你的思考和答案，我们一起讨论。如果你觉得有所收获，也欢迎把文章分享给你的朋友。![](Images/a7d15815f9efb5693db5b2d278244658.png){savepage-src="https://static001.geekbang.org/resource/image/c8/bf/c89da43deab85fe7cb06acec867aa5bf.jpg"}