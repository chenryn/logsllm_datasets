relevant per-capacity and per-bandwidth costs are allocated
to the primary data protection technique. Only the addi-
tional per-capacity and per-bandwidth costs associated with
a secondary technique are allocated to that secondary tech-
nique. Spare resource costs are allocated to data protection
techniques in a similar fashion.
Penalties are calculated at a global level using the out-
put values from the data loss and recovery time sub-models.
The recent data loss penalty is calculated by multiplying the
worst case data loss amount by the data loss penalty rate
input parameter. Likewise, the data outage penalty is cal-
culated by multiplying the worst case recovery time by the
data outage penalty rate input parameter.
4 Case study
In this section, we present a case study to illustrate the
framework’s operation and validate its ability to compose
models. We begin by examining a baseline conﬁguration
in detail, and then explore several what-if scenarios to im-
prove the storage system’s dependability and cost. The case
study demonstrates that the quantitative results produced are
reasonable, and that the framework is ﬂexible and useful in
designing a system that meets dependability requirements.
Our baseline conﬁguration (Figure 1) employs split mir-
roring, tape backup and remote vaulting to protect the pri-
mary copy of the dataset. We model a workgroup server,
whose measured workload characteristics are quantiﬁed in
Table 2. Tables 3 and 4 summarize the policy and device
conﬁguration parameters for the data protection techniques
used. Table 4 also presents models for calculating outlay
costs. These models include ﬁxed costs, per-capacity (c,
in GB) costs, per-bandwidth (b, in MB/s) costs and per-
shipment (s) costs. The values for each term are calculated
based on annualized hardware component costs (assuming a
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 10:04:18 UTC from IEEE Xplore.  Restrictions apply. 
dataCap
1360 GB
avgAccessR
1028 KB/s
avgUpdateR
799 KB/s
burstM batchUpdR(win)
10X
1 min: 727 KB/s; 12 hr: 350 KB/s; 24 hr, 48 hr, 1 wk: 317 KB/s
Table 2: Parameters for cello workgroup ﬁle server workload used in case studies [12].
Technique
Split mirror
Tape backup
Remote vaulting
accW propW holdW
12 hr
1 wk
4 wk
0 hr
48 hr
24 hr
0 hr
1 hr
4 wk + 12 hr
cyclePer
12 hr
1 wk
4 wk
retCnt
4
4
39
retW
2 days
4 wks
3 yrs
copyRep
full
full
full
propRep
full
full
full
Table 3: Data protection technique parameters for baseline storage system design.
maxCapSlots maxBWSlots
@ slotCap
(GB)
256@73GB
500@400GB
@ slotBW
(MB/s)
256@25MB/s
16@60MB/s
enclBW devDelay
(MB/s)
512MB/s
240MB/s
(hr)
n/a
0.01hr
Device
Disk array
Tape library
Vault
Air shipment
5000@400GB
n/a
n/a
n/a
n/a
n/a
n/a
24hr
Costs
spareType
123297  c  17(cid:0)2
98895  c  0(cid:0)4
b  108(cid:0)6
25000  c  0(cid:0)4
s  50
dedicated
dedicated
none
none
spare
Time
(hours)
0.02hr
0.02hr
n/a
n/a
spare
Disc
1X
1X
n/a
n/a
Table 4: Device conﬁguration parameters for baseline storage system design. The primary array is a mid-range array (based on HP’s
EVA [9]) with up to 256 73-GB disks and up to 32 GB of cache. The tape library (based on HP’s ESL9595 [10]) contains up to 16 LTO
tape drives and up to 500 LTO tape cartridges. These devices communicate through a Fibre-channel storage area network (SAN). Both the
primary array and the tape library employ a dedicated hot spare. The primary site communicates via air shipment to a tape vault, which can
contain up to 5000 tape cartridges; the vault uses no sparing.
three-year depreciation) and facilities costs. The component
costs are based on actual list prices or expert estimates. The
details of these component costs are omitted due to space
limitations. We assume data unavailability and loss penalty
rates of $50,000 per hour (each).
We assume the use of hot spare resources at the primary
site and a remote shared recovery facility. Hot spare re-
sources take 60 seconds to provision, and cost the same as
the original resources. Remote hosting facility resources
can be provisioned (e.g., drained of other workloads and
scrubbed) within nine hours. Because the resources are
shared, they cost only 20% of the dedicated resources.
We examine failures at three scopes: a data object, an
array, and a site. The data object failure simulates a user
mistake or software error that corrupts a 1 MB data object,
requiring roll back to the version that existed 24 hours ago.
The recovery hierarchy is simply the reverse of the RP prop-
agation hierarchy. The array failure simulates failure of the
primary array, and the site failure simulates a disaster at the
primary site. Both require recovery of the entire dataset to
its most recent state, and use a recovery hierarchy of the
remote vault, tape backup, and primary copy.
4.1 Baseline conﬁguration results
Table 5 summarizes the bandwidth and capacity demands
that the data protection techniques place on the underlying
devices to manage RPs throughout the hierarchy.
Primary disk array bandwidth is demanded by the fore-
ground workload, split mirroring, and tape backup. Split
mirror resilvering generates both read and write demands.
The backup policy generates a read workload on the array
and a write workload on the tape library. The vaulting pol-
Device
Disk array
Foreground workload
Split mirror
Backup
Overall
Tape library
Backup
Tape vault
Vaulting
Bandwidth
Capacity
0.2%
0.6%
1.6%
2.4% (12.4 MB/s)
14.6%
72.8%
0.0%
87.4% (8.0 TB)
3.4% (8.1 MB/s)
3.4% (6.6 TB)
0.0%
2.6% (51.8 TB)
Table 5: Normal mode bandwidth and capacity utilization for
baseline system.
icy’s accumulation window matches the backup retention
window, meaning that the oldest full backup can be shipped
offsite when its retention window expires and resulting in
no additional tape library bandwidth demands. The total
average bandwidth demands are 12.4 MB/s for the primary
array and 8.1 MB/s for the tape library, resulting in an over-
all system bandwidth utilization of 4%.
Each level’s retention window and copy representation
type imposes capacity requirements on the underlying de-
vices. The array stores both the primary copy and ﬁve split
mirrors, each a full copy of the dataset, demanding 8.0 TB
of total array capacity. The tape library maintains four full
backups, corresponding to a total of 6.6 TB. Finally, the
vault maintains 39 full backups, corresponding to 51.8 TB.
The resulting overall system capacity utilization is 88%.
Table 6 examines the dependability of the baseline stor-
age system design for the three different failure scenarios.
In the object failure case, the day-old target version is main-
tained at the split mirror level, and can be easily restored by
an intra-array copy, resulting in a negligible recovery time.
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 10:04:18 UTC from IEEE Xplore.  Restrictions apply. 
Failure
scope
object
array
site
Recovery
source
split mirror
tape backup
remote vaulting
Recovery
time
0.004 s
2.4 hr
26.4 hr
Recent
data loss
12 hr
217 hr
1429 hr
Table 6: Worst case recovery time and recent data loss results for
baseline system.
s
t
s
o
c
l
l
a
r
e
v
O
)
s
e
i
t
l
a
n
e
p
+
s
y
a
l
t
u
o
(
$71.9M
$11.9M
$100,000,000
$10,000,000
$1,000,000
$100,000
$1.6M
object
array
site
Failure scope
foreground wkld outlay
vaulting outlay
split mirror outlay