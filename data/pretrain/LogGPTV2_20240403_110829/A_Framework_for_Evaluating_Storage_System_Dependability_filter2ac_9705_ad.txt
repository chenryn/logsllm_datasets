### Cost Allocation and Penalty Calculation

Costs associated with per-capacity and per-bandwidth are allocated to the primary data protection technique. Additional per-capacity and per-bandwidth costs related to a secondary technique are attributed to that specific secondary technique. Similarly, spare resource costs are distributed among the data protection techniques.

Penalties are calculated at a global level using the output values from the data loss and recovery time sub-models. The recent data loss penalty is determined by multiplying the worst-case data loss amount by the data loss penalty rate input parameter. Similarly, the data outage penalty is calculated by multiplying the worst-case recovery time by the data outage penalty rate input parameter.

### Case Study

In this section, we present a case study to illustrate the framework's operation and validate its ability to compose models. We start by examining a baseline configuration in detail, followed by several "what-if" scenarios to enhance the storage system's dependability and cost-effectiveness. The case study demonstrates that the quantitative results produced are reasonable and that the framework is flexible and useful for designing a system that meets dependability requirements.

#### Baseline Configuration

Our baseline configuration (Figure 1) employs split mirroring, tape backup, and remote vaulting to protect the primary copy of the dataset. We model a workgroup server with measured workload characteristics quantified in Table 2. Tables 3 and 4 summarize the policy and device configuration parameters for the data protection techniques used. Table 4 also includes models for calculating outlay costs, which include fixed costs, per-capacity (c, in GB) costs, per-bandwidth (b, in MB/s) costs, and per-shipment (s) costs. These values are based on annualized hardware component costs (assuming a three-year depreciation) and facilities costs. The component costs are derived from actual list prices or expert estimates, but detailed cost breakdowns are omitted due to space limitations. We assume data unavailability and loss penalty rates of $50,000 per hour.

We assume the use of hot spare resources at the primary site and a remote shared recovery facility. Hot spare resources take 60 seconds to provision and cost the same as the original resources. Remote hosting facility resources can be provisioned (e.g., drained of other workloads and scrubbed) within nine hours. Since these resources are shared, they cost only 20% of dedicated resources.

We examine failures at three scopes: a data object, an array, and a site. The data object failure simulates a user mistake or software error that corrupts a 1 MB data object, requiring a rollback to the version that existed 24 hours ago. The recovery hierarchy is the reverse of the RP propagation hierarchy. The array failure simulates the failure of the primary array, and the site failure simulates a disaster at the primary site. Both require recovery of the entire dataset to its most recent state, using a recovery hierarchy of the remote vault, tape backup, and primary copy.

#### Baseline Configuration Results

Table 5 summarizes the bandwidth and capacity demands that the data protection techniques place on the underlying devices to manage RPs throughout the hierarchy.

- **Primary Disk Array Bandwidth**: Demanded by the foreground workload, split mirroring, and tape backup. Split mirror resilvering generates both read and write demands. The backup policy generates a read workload on the array and a write workload on the tape library.
- **Tape Library Bandwidth**: Used for backups.
- **Tape Vault Bandwidth**: Used for vaulting.

The total average bandwidth demands are 12.4 MB/s for the primary array and 8.1 MB/s for the tape library, resulting in an overall system bandwidth utilization of 4%.

Each levelâ€™s retention window and copy representation type imposes capacity requirements on the underlying devices:
- **Array Capacity**: Stores the primary copy and five split mirrors, totaling 8.0 TB.
- **Tape Library Capacity**: Maintains four full backups, totaling 6.6 TB.
- **Vault Capacity**: Maintains 39 full backups, totaling 51.8 TB.

The overall system capacity utilization is 88%.

Table 6 examines the dependability of the baseline storage system design for the three different failure scenarios:

- **Object Failure**: The day-old target version is maintained at the split mirror level and can be easily restored by an intra-array copy, resulting in negligible recovery time.
- **Array Failure**: Requires recovery from the tape backup, taking 2.4 hours.
- **Site Failure**: Requires recovery from the remote vault, taking 26.4 hours.

Table 6 also shows the worst-case recovery times and recent data loss results for the baseline system.

### Summary

The case study demonstrates the effectiveness of the framework in providing reasonable quantitative results and flexibility in designing a storage system that meets dependability requirements. The baseline configuration and various "what-if" scenarios highlight the importance of considering different failure scopes and their impact on recovery times and data loss.