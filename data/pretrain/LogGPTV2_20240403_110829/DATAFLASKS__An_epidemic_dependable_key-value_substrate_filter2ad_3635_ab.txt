### Size and Replication Factor

The number of slices in a system has a direct impact on the replication factor and overall system capacity. A smaller number of slices increases the replication factor, which enhances data redundancy but reduces the system's storage capacity. This is due to the limited capacity of individual nodes, as each node can only replicate a limited number of objects, thereby limiting the number of objects that a slice can hold. Conversely, increasing the replication factor improves the performance of read operations by distributing the load across more nodes, thus increasing system capacity. This relationship opens up important research avenues for future work.

In this study, we define the size of slices as a percentage of the system size. Additionally, we assume that the slices are sufficiently large to minimize the risk of all nodes within the same slice failing simultaneously.

### DATAFLASKS Architecture

**Figure 2: DATAFLASKS Architecture**

The DATAFLASKS architecture consists of two main components: the DATAFLASKS host and the client library. These components rely on a set of services, each with a specific task.

#### Services in a DATAFLASKS Node

1. **Slice Manager Service**: This service is responsible for partitioning the system into slices. In our implementation, we use the DSlead protocol [17].
2. **Peer Sampling Service**: This service provides references to other nodes in the system and supports the Load Balancer.
3. **Load Balancer Service**: The Load Balancer provides the Client Library with references to nodes that can handle client requests. The performance of the system is significantly influenced by the Load Balancer. Currently, it provides the client with a random contact node. Further considerations for this service are discussed in Section VII.
4. **Request Handler**: This service processes requests made to the node. It interacts with the Slice Manager to determine the node's slice and manages data storage and retrieval from the Data Store. The Data Store is an abstraction of the actual storage mechanism, which could be the node's hard disk or another persistence method.

#### Client Library

The client library is divided into two subcomponents:

1. **API Implementation**: This subcomponent implements the DATAFLASKS API and handles client requests by contacting a DATAFLASKS node.
2. **Reply Handling**: This subcomponent manages reply messages. It must handle multiple replies for the same request, which occur due to the epidemic dissemination of requests. To manage this, read requests include a unique request identifier to distinguish between multiple requests for the same object.

### Evaluation

Although DATAFLASKS is still under development, we provide an early evaluation of the prototype to motivate further design and implementation. Specifically, we aim to verify if DATAFLASKS can scale effectively to thousands of nodes.

#### Simulation Environment

We use Minha [25] as the simulation environment. Minha is an event-driven simulation framework that allows running unmodified Java application code. Both DATADROPLETS and DATAFLASKS are implemented in Java.

#### Experiment 1: Constant Number of Slices

For the first experiment, we configured DATAFLASKS to consider ten slices and an increasing number of nodes. We ran YCSB [26] cloud storage benchmark with a write-only workload. We measured the average number of messages each node had to send/receive to perform the YCSB requests. The results are shown in Figure 3.

**Figure 3: Average Number of Messages per Node with Constant Number of Slices**

Despite the increase in the number of nodes from 500 to 3,000, the number of messages handled by each node remains relatively constant. This indicates that adding more nodes can significantly increase the replication factor while keeping the number of slices constant.

#### Experiment 2: Variable Number of Slices

In the second experiment, we increased the number of slices proportionally to the increase in the number of nodes, maintaining a constant replication factor. The results are depicted in Figure 4.

**Figure 4: Average Number of Messages per Node with Variable Number of Slices**

Similar to the previous experiment, the number of messages handled by each node increases gradually. However, the increase is sub-linear with respect to the number of nodes. This difference is due to the Load Balancer's operation. As the number of slices increases, the probability of immediately reaching a node in the intended slice decreases, leading to more message dissemination.

Both experiments suggest promising scalability properties for DATAFLASKS.

### Discussion

In this paper, we presented the key ideas behind the design of an epidemic key-value substrate. We provided mechanisms for data partitioning, request dissemination, and object replication. However, some aspects, such as maintaining replication levels in the face of churn or faults and optimizing request dissemination, are still under research.

#### Maintaining Replication Levels

One challenge is ensuring that every object has at least \( r \) replicas. In our design, there is no centralized way to track this. We assume that each slice stores a set of data objects, and if the slice size is greater than \( r \), the replication factor is assured. However, additional assumptions, such as the presence of a correct number of nodes in each slice, are necessary to provide persistence guarantees. These assumptions need further analysis. Additionally, the handling of node joins and leaves in a slice requires careful consideration to avoid performance and persistence issues.

### Acknowledgment

This work is part-funded by:
- ERDF - European Regional Development Fund through the COMPETE Programme
- FCT - Fundação para a Ciência e a Tecnologia (Portuguese Foundation for Science and Technology)
- European Union Seventh Framework Programme (FP7) under grant agreement n 257993, project CumuloNimbo.

### References

[1] H. Sutter, “The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software,” Dr. Dobb’s Journal, vol. 30, no. 3, pp. 202–210, 2005.
[Online]. Available: http://www.gotw.ca/publications/concurrency-ddj.htm

[2] A. Lakshman and P. Malik, “Cassandra: a decentralized structured storage system,” in SIGOPS Oper. Syst. Rev. ACM, 2010.

[3] G. DeCandia, D. Hastorun, M. Jampani, G. Kakulapati, A. Lakshman, A. Pilchin, S. Sivasubramanian, P. Vosshall, and W. Vogels, “Dynamo: amazon’s highly available key-value store,” in SOSP ’07. ACM, 2007.

[4] B. F. Cooper, R. Ramakrishnan, U. Srivastava, A. Silberstein, P. Bohannon, H.-A. Jacobsen, N. Puz, D. Weaver, and R. Yerneni, “Pnuts: Yahoo!’s hosted data serving platform,” in VLDB. VLDB Endowment, 2008.

[5] F. Chang, J. Dean, S. Ghemawat, W. C. Hsieh, D. A. Wallach, M. Burrows, T. Chandra, A. Fikes, and R. E. Gruber, “Bigtable: a distributed storage system for structured data,” in OSDI ’06. USENIX Association, 2006.

[6] N. Leavitt, “Will nosql databases live up to their promise?” Computer, vol. 43, no. 2, pp. 12–14, Feb. 2010. [Online]. Available: http://dx.doi.org/10.1109/MC.2010.58

[7] M. Matos, R. Vilaca, J. Pereira, and R. Oliveira, “An epidemic approach to dependable key-value substrates,” in DCDV’11. IEEE, 2011.

[8] S. Voulgaris and M. Steen, “Epidemic-style management of semantic overlays for content-based searching,” in Euro-Par 2005 Parallel Processing, ser. Lecture Notes in Computer Science, J. Cunha and P. Medeiros, Eds. Springer Berlin Heidelberg, 2005, vol. 3648, pp. 1143–1152. [Online]. Available: http://dx.doi.org/10.1007/11549468 125

[9] S. Voulgaris, D. Gavidia, and M. V. Steen, “CYCLON: Inexpensive Membership Management for Unstructured P2P Overlays,” Journal of Network and Systems Management, 2005.

[10] S. Voulgaris and M. van Steen, “A robust gossiping protocol,” Proceedings of the Second International Conference on Peer-to-Peer Computing, 2005. [Online]. Available: http://www.springerlink.com/index/ey3aehdf7j5uauqt.pdf

[11] P. Erdős and A. Rényi, “On the evolution of random graphs,” in Publication of the Mathematical Institute of the Hungarian Academy of Sciences, 1960, pp. 17–61.

[12] V. Gramoli, Y. Vigfusson, K. Birman, A.-M. Kermarrec, and R. van Renesse, “Sliver, A fast distributed slicing algorithm,” in ACM symposium on Principles of distributed computing, 2008.

[13] M. Jelasity and A.-M. Kermarrec, “Ordered slicing of very large-scale overlay networks,” in IEEE International Conference on Peer-to-Peer Computing, 2006.

[14] M. Jelasity, S. Voulgaris, R. Guerraoui, A.-M. Kermarrec, and M. van Steen, “Gossip-based peer sampling,” ACM Transactions on Computer Systems, 2007.

[15] A. Fernandez, V. Gramoli, E. Jimenez, A.-M. Kermarrec, and M. Raynal, “Distributed Slicing in Dynamic Systems,” in International Conference on Distributed Computing Systems, 2007.

[16] F. Maia, M. Matos, E. Rivière, and R. Oliveira, “Slead: low-memory steady distributed systems slicing,” in IFIP International Conference on Distributed Applications and Interoperable Systems, 2012.

[17] ——, “Slicing as a distributed systems primitive.” in 6th Latin-American Symposium on Dependable Computing (LADC’13), 2013.

[18] R. Vilaca and R. Oliveira, “Clouder: a flexible large scale decentralized object store: architecture overview,” in WDDDMM ’09: Proceedings of the Third Workshop on Dependable Distributed Data Management. New York, NY, USA: ACM, 2009, pp. 25–28.

[19] R. Vilaca, R. Oliveira, and J. Pereira, “A correlation-aware data placement strategy for key-value stores,” in International Conference on Distributed Applications and Interoperable Systems, 2011.

[20] R. Vilaca, F. Cruz, and R. Oliveira, “On the expressiveness and trade-offs of large scale tuple stores,” in On the Move to Meaningful Internet Systems, OTM 2010. Springer Berlin / Heidelberg, 2010, vol. 6427, pp. 727–744.

[21] E. Rivière and S. Voulgaris, Gossip-Based Networking for Internet-Scale Distributed Systems, ser. Lecture Notes in Business Information Processing, 2011.

[22] A. J. Ganesh, A.-M. Kermarrec, and L. Massoulié, “Scamp: Peer-to-peer lightweight membership service for large-scale group communication,” in International COST264 Workshop on Networked Group Communication, 2001.

[23] F. Maia, M. Matos, J. Pereira, and R. Oliveira, “Worldwide consensus,” in IFIP International Conference on Distributed Applications and Interoperable Systems, 2011.

[24] P. Jesus, C. Baquero, and P. S. Almeida, “Fault-Tolerant Aggregation for Dynamic Networks,” in IEEE Symposium on Reliable Distributed Systems, 2010.

[25] N. A. Carvalho, J. a. Bordalo, F. Campos, and J. Pereira, “Experimental evaluation of distributed middleware with a virtualized java environment,” in MW4SOC ’11. ACM, 2011.

[26] B. F. Cooper, A. Silberstein, E. Tam, R. Ramakrishnan, and R. Sears, “Benchmarking cloud serving systems with YCSB,” in SoCC, 2010.