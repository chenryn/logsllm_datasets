size, a smaller number of slices increases the replication
factor but lowers system capacity. This is a consequence
of the limited capacity of the individual node. Each node
can replicate a limited number of objects which, in turn,
limits the number of objects a slice can hold. Conversely,
increasing the replication factor increases the performance
of read operations that can target more nodes and increases
system capacity. In this regard, we believe that this opens
important research paths for future work.
In this work we simply deﬁne the size of slices as a
percentage of the system size. Furthermore, we make the
assumption that the size of the slices is large enough so that
the risk of having all nodes within the same slice failing
simultaneously is null.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:49:12 UTC from IEEE Xplore.  Restrictions apply. 
V. DATAFLASKS ARCHITECTURE
architecture
2
presents
the
Figure
supporting
DATAFLASKS. Two main components are considered: a
DATAFLASKS host and a client library. These components
depend on a set of services each with a speciﬁc task.
Figure 2. DATAFLASKS Architecture.
Each DATAFLASKS node has four services running. A
Slice Manager service is responsible for partitioning the sys-
tem slices. As described above, this service is implemented
by a slicing protocol. In our case, by DSlead [17]. The
Peer Sampling Service which is responsible for providing
references of other nodes in the system and also to the
Load Balancer service. The Load Balancer in turn, provides
the Client Library with references to nodes that can answer
client requests. The implementation of the Load Balancer
has a signiﬁcant impact on the performance of the system.
the Load Balancer provides the client with a
For now,
random contact node. Further considerations on this service
are made in Section VII. Finally, the request Handler is
responsible for dealing with requests made to the node. It
knows to which slice the node belongs to from the Slice
Manager and stores and retrieves correspondent data to and
from the Data Store. The Data Store is an abstraction of the
actual storing mechanism which can be the node hard disk
or other persistence mechanism.
The client library is divided into two subcomponents. One
is responsible for implementing the DATAFLASKS API and
serves client requests by contacting a DATAFLASKS node.
The other is responsible for dealing with reply messages.
The second component must know how to handle multiple
replies for the same request. This happens due to the
epidemic dissemination of the requests and the multiple
nodes able to reply to them. In order to be able to do so,
read requests carry a request identiﬁer in order to distinguish
multiple read requests for the same object.
VI. EVALUATION
Although DATAFLASKS is still under development and
some challenges remain to be tackled, in this section we pro-
vide an early evaluation of the prototype. The experiments
we present serve as a motivation to continue the design and
implementation of our epidemic key-value store substrate.
More speciﬁcally, we try to verify if DATAFLASKS can
effectively scale to thousands of nodes.
In these experiments we use Minha [25] as the simulation
environment. Minha is a event driven simulation frame-
work that has the key advantage of allowing to run real,
unmodiﬁed and not instrumented, Java application code.
Both DATADROPLETS and DATAFLASKS are implemented
in Java.
For the ﬁrst experiment, we ran DATAFLASKS conﬁgured
to consider ten slices and an increasing number of nodes. At
this stage of development we ran DATAFLASKS decoupled
from DATADROPLETS and used YCSB [26] cloud storage
benchmark as its direct client. We ran YCSB conﬁgured
for a write only workload. For each run we measured the
average number of messages each node had to send/receive
to perform the YCSB requests. In Figure 3 we present the
results of this experiment.
Figure 3. Average number of messages per node with constant number
of slices.
Notice that despite the increase in the number of nodes
from 500 to 3,000 the number of messages handled by each
node remains roughly the same. Considering that we are
keeping the number of slices constant, this means that to
signiﬁcantly increase the replication factor it sufﬁces to add
more nodes.
For the second experiment we increased the number of
slices proportionally to the increase in the number of nodes.
This renders the replication factor constant. While in the
ﬁrst scenario the increase in the number of nodes privileged
a greater replication factor, in this second experiment the
extra number of nodes is used to enlarge the system capacity.
Figure 4, depicts the second experiment results.
Similarly to the previous experiment,
the number of
messages handled by each node increases gracefully. Indeed,
it shows a sub-linear variation with respect to the number of
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:49:12 UTC from IEEE Xplore.  Restrictions apply. 
Client LibraryLoad BalancerDataFlasks NodeSlice ManagerData StoreNode SamplingClientHandler 0 50 100 150 200 250 300 350 400 500 1000 1500 2000 2500 3000Number of messages per nodeNumber of nodeswhere the majority of the system is concerned with state
transfer at the expense of actual data request processing.
the client
Another important research path is related to system
optimization. In DATAFLASKS,
library, along
with the Load Balancer, are components where various
optimizations can be studied. These are mainly related to
node discovery and the decision to which node route each
request. The intuition behind this idea is simple. If the Load
Balancer was able to know exactly which node to contact for
each request, dissemination mechanisms would be reduced
to the minimum. As this is not feasible in practice, cache
mechanisms should be studied in order to achieve a solution
as close as possible to the ideal one.
ACKNOWLEDGMENT
This work is part-funded by: ERDF - European Regional
Development Fund through the COMPETE Programme (op-
erational programme for competitiveness) and by National
Funds through the FCT - Fundac¸˜ao para a Ciˆencia e a
Tecnologia (Portuguese Foundation for Science and Tech-
nology) within projects Stratus/FCOMP-01-0124-FEDER-
015020 and FCOMP - 01-0124-FEDER-022701; and Eu-
ropean Union Seventh Framework Programme (FP7) under
grant agreement n 257993, project CumuloNimbo.
REFERENCES
[1] H. Sutter, “The Free Lunch Is Over: A Fundamental Turn
Toward Concurrency in Software,” Dr. Dobb’s Journal,
vol. 30, no. 3, pp. 202–210, 2005. [Online]. Available:
http://www.gotw.ca/publications/concurrency-ddj.htm
[2] A. Lakshman and P. Malik, “Cassandra: a decentralized
structured storage system,” in SIGOPS Oper. Syst. Rev. ACM,
2010.
[3] G. DeCandia, D. Hastorun, M. Jampani, G. Kakulapati,
A. Lakshman, A. Pilchin, S. Sivasubramanian, P. Vosshall,
and W. Vogels, “Dynamo: amazon’s highly available key-
value store,” in SOSP ’07. ACM, 2007.
[4] B. F. Cooper, R. Ramakrishnan, U. Srivastava, A. Silber-
stein, P. Bohannon, H.-A. Jacobsen, N. Puz, D. Weaver, and
R. Yerneni, “Pnuts: Yahoo!’s hosted data serving platform,”
in VLDB. VLDB Endowment, 2008.
[5] F. Chang, J. Dean, S. Ghemawat, W. C. Hsieh, D. A.
Wallach, M. Burrows, T. Chandra, A. Fikes, and R. E. Gruber,
“Bigtable: a distributed storage system for structured data,”
in OSDI ’06. USENIX Association, 2006.
[6] N. Leavitt, “Will nosql databases live up to their promise?”
Computer, vol. 43, no. 2, pp. 12–14, Feb. 2010. [Online].
Available: http://dx.doi.org/10.1109/MC.2010.58
[7] M. Matos, R. Vilac¸a, J. Pereira, and R. Oliveira, “An epidemic
approach to dependable key-value substrates,” in DCDV’11.
IEEE, 2011.
Figure 4. Average number of messages per node with variable number of
slices.
nodes. The difference in the number of messages between
the two experiments is explained by how the Load Balancer
is used. As each operation is requested to randomly chosen
node and, in the second experiment, we are increasing the
number of slices, the probability of immediately reaching a
node of the intended slice is increasingly lower. This leads
to a larger number of messages being disseminated.
Both results hint at promising scalability properties of
DATAFLASKS.
VII. DISCUSSION
Along this paper we presented the main ideas behind
the design of an epidemic key-value substrate. We provided
mechanisms to partition the data across nodes in the system,
to disseminate requests to appropriate nodes and to have
objects replicated through multiple locations. However, some
key aspects of the system are still missing and are object of
ongoing research. These are mainly: maintaining replication
level
in face of churn or faults and optimizing request
dissemination.
Concerning the maintenance of replication levels the
problem is that, in our design, there is no centralized way
of knowing if every object has, in fact, at least r replicas.
Notice that, as described earlier, each slice stores a set of
data objects. If the slice size is greater than r the replication
factor is assured. However, in order to provide persistence
guarantees extra assumptions must be made. For instance,
we have to assume that, for each slice, there are always some
correct number of nodes. Otherwise, it seems impossible to
guarantee that a certain object is not lost. These assumptions
are still to be further analyzed and studied. Moreover, the
case where a node joins or leaves a certain slice is still
to be treated appropriately. Such operations can have a
serious impact in performance and persistence guarantees if
implemented improperly. For instance, it must be enforced
that a situation where every node in a certain slice decides to
change slice never happens. At the same time, when a node
joins a certain slice, mechanisms for efﬁcient state transfer
must be devised. Otherwise, we may fall into a situation
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:49:12 UTC from IEEE Xplore.  Restrictions apply. 
 0 200 400 600 800 1000 1200 1400 500 1000 1500 2000 2500 3000Number of messages per nodeNumber of nodes[8] S. Voulgaris and M. Steen, “Epidemic-style management of
semantic overlays for content-based searching,” in Euro-Par
2005 Parallel Processing, ser. Lecture Notes in Computer
Science, J. Cunha and P. Medeiros, Eds. Springer Berlin
Heidelberg, 2005, vol. 3648, pp. 1143–1152.
[Online].
Available: http://dx.doi.org/10.1007/11549468 125
[9] S. Voulgaris, D. Gavidia, and M. V. Steen, “CYCLON:
Inexpensive Membership Management for Unstructured P2P
Overlays,” Journal of Network and Systems Management,
2005.
[10] S. Voulgaris
and
e.
gossiping
international
peer-to-peer
Second
Peer-to-Peer Computing,
http://www.springerlink.com/index/ey3aehdf7j5uauqt.pdf
Agents
2005.
“A robust
al,
and
protocol,” Proceedings
conference
scalable
the
of
and
[Online]. Available:
on
[11] P. Erds and A. Rnyi, “On the evolution of random graphs,”
in Publication of the Mathematical Institute of the Hungarian
Academy of Sciences, 1960, pp. 17–61.
[12] V. Gramoli, Y. Vigfusson, K. Birman, A.-M. Kermarrec, and
R. van Renesse, “Sliver, A fast distributed slicing algorithm,”
in ACM symposium on Principles of distributed computing,
2008.
[13] M. Jelasity and A.-M. Kermarrec, “Ordered slicing of very
large-scale overlay networks,” in IEEE International Confer-
ence on Peer-to-Peer Computing, 2006.
[14] M. Jelasity, S. Voulgaris, R. Guerraoui, A.-M. Kermarrec,
and M. van Steen, “Gossip-based peer sampling,” ACM
Transactions on Computer Systems, 2007.
[15] A. Fernandez, V. Gramoli, E. Jimenez, A.-M. Kermarrec, and
M. Raynal, “Distributed Slicing in Dynamic Systems,” in
International Conference on Distributed Computing Systems,
2007.
[16] F. Maia, M. Matos, E. Rivi`ere, and R. Oliveira, “Slead:
low-memory steady distributed systems slicing,” in IFIP
International Conference on Distributed Applications and
Interoperable Systems, 2012.
[17] ——, “Slicing as a distributed systems primitive.” in
6th Latin-American Symposium on Dependable Computing
(LADC’13), 2013.
[18] R. Vilac¸a and R. Oliveira, “Clouder: a ﬂexible large scale
decentralized object store: architecture overview,” in WD-
DDM ’09: Proceedings of the Third Workshop on Dependable
Distributed Data Management. New York, NY, USA: ACM,
2009, pp. 25–28.
[19] R. Vilac¸a, R. Oliveira, and J. Pereira, “A correlation-aware
data placement strategy for key-value stores,” in International
Conference on Distributed Applications and Interoperable
Systems, 2011.
[20] R. Vilac¸a, F. Cruz, and R. Oliveira, “On the expressiveness
and trade-offs of large scale tuple stores,” in On the Move to
Meaningful Internet Systems, OTM 2010. Springer Berlin /
Heidelberg, 2010, vol. 6427, pp. 727–744.
[21] E. Rivi`ere and S. Voulgaris, Gossip-Based Networking for
Internet-Scale Distributed Systems, ser. Lecture Notes in
Business Information Processing, 2011.
[22] A. J. Ganesh, A.-M. Kermarrec, and L. Massouli´e, “Scamp:
Peer-to-peer lightweight membership service for large-scale
group communication,” in International COST264 Workshop
on Networked Group Communication, 2001.
[23] F. Maia, M. Matos, J. Pereira, and R. Oliveira, “Worldwide
consensus,” in IFIP International Conference on Distributed
Applications and Interoperable Systems, 2011.
[24] P. Jesus, C. Baquero, and P. S. Almeida, “Fault-Tolerant
Aggregation for Dynamic Networks,” in IEEE Symposium on
Reliable Distributed Systems, 2010.
[25] N. A. Carvalho, J. a. Bordalo, F. Campos, and J. Pereira,
“Experimental evaluation of distributed middleware with a
virtualized java environment,” in MW4SOC ’11. ACM, 2011.
[26] B. F. Cooper, A. Silberstein, E. Tam, R. Ramakrishnan, and
R. Sears, “Benchmarking cloud serving systems with YCSB,”
in SoCC, 2010.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:49:12 UTC from IEEE Xplore.  Restrictions apply.