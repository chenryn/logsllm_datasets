the superset of system calls used by an application.
5 Design
Our goal is to reduce the kernel attack surface available to
a malicious tenant of a container service by limiting the
number of system calls available to each container, which
can potentially be of use for malicious purposes (either as
part of exploit code, or as a gateway to exploiting kernel
vulnerabilities). To achieve this, Conﬁne “hardens” the
container image once it has been fully conﬁgured by the user,
by limiting access to only those system calls that are actually
needed for the proper operation of the container.
Identifying the system calls that are necessary for the correct
execution of the container requires addressing the following re-
quirements: 1) identify all applications that may run on the con-
tainer; 2) identify all library functions imported by each appli-
cation; 3) map library functions to system calls; and 4) extract
direct system call invocations from applications and libraries.
Figure 2 presents a high-level overview of our approach,
which, given a container image, automatically generates
Seccomp rules that limit the system calls that may be invoked.
Conﬁne currently supports Docker containers running on a na-
tive Linux-based host, but similar analysis could be performed
for other container environments and operating systems.
5.1 Identifying Running Applications
Although containers are usually specialized to run a single
application or service, they typically invoke many other utility
and support programs prior to executing the main program. For
example, the default MongoDB Docker image [16] invokes
the following supporting programs to set up the environment:
bash, chown, find, id, and numactl. To generate system call
policies, we must thus identify all programs that can potentially
run during the lifetime of a container. Conﬁne relies on limited
dynamic analysis to capture the list of processes created on the
system. A proﬁling tool records every application launched
within a conﬁgurable time period (30 seconds by default)
since the creation of the container—long enough to capture
both system initialization, as well as the “stable” state of the
system. The obtained set of applications is then used to derive
the corresponding system call policy. We further discuss the
completeness of the derived list in Section 8.
Our approach is different from previous works that rely
on dynamic training using various workloads to derive a list
of allowable system calls [75]. In our approach, the goal of
the dynamic analysis is merely to identify the set of binary
executables to be analyzed—the system calls invoked by these
programs are then derived statically.
The above dynamic analysis is meant to be a convenient
and automated way to carry out the batch analysis of multiple
container images. For containers that may include applications
that are not launched from the beginning, our system supports
manually provided external lists of executables that should
be included in the analysis.
5.2 Static Analysis
Dynamic analysis often fails to exercise all possible code
paths, especially when comprehensive workloads are not
available during training. To ensure complete code coverage,
once we have the list of applications that are executed on the
container, we perform static analysis to extract the system calls
that are needed for the correct execution of each application.
Libc User programs typically invoke system calls through
the libc library, which provides corresponding wrapper func-
tions (e.g., the libc function read invokes the system call
SYS_read). Conﬁne analyzes the source code of libc to derive a
mapping between exported functions and the system calls they
invoke. For the rest of the programs and libraries on a given con-
tainer, however, Conﬁne only needs to analyze their binaries.
A libc function may have multiple control ﬂow paths to the
actual system call. To correctly identify which system calls are
446    23rd International Symposium on Research in Attacks, Intrusions and Defenses
USENIX Association
Figure 2: Overview of Conﬁne’s system call extraction process. A one-time dynamic analysis phase that does not require any
application-speciﬁc workloads is used for the sole purpose of identifying the applications running in the container. Each application
is then statically analyzed to identify all the library functions that it uses, and the system calls it relies on.
invoked by a given libc function, we thus need to analyze these
control ﬂow paths. To that end, Conﬁne statically analyzes the
source code of libc to derive its full call graph, and accurately
map each function to its respective system calls.
Function pointers are used widely in libc. However, per-
forming accurate points-to analysis has signiﬁcant scalability
and performance issues [29, 42]. To avoid having to perform
points-to analysis, we follow a more conservative approach
and retain all system calls that are invoked through any
function that has its address taken. In Section 6.1 we discuss
the technical challenges we encountered during this process.
Having an accurate mapping between libc functions and
system calls, it is then straightforward to analyze each program
(main executable and libraries), identify all imported libc
functions, and derive the set of all possible system calls the
program may invoke. It is important to stress that this phase
is performed only once per libc version—the derived mapping
is then saved and used across all containers.
Direct System Call Invocation In addition to using libc
wrappers, applications and libraries may also invoke system
calls directly using the syscall() function, or using the
syscall assembly instruction. Although the number of
applications and libraries which use this approach are limited,
for the sake of completeness, we use binary code disassembly
to extract any directly invoked system calls. We describe
in detail this process in Section 6.2. Some applications
developed in languages other than C/C++ also require special
considerations which we discuss in Section 6.3.
Based on the analysis performed in Sections 5.1 and 5.2, we
use an automated script to derive the list of prohibited system
calls, and construct the corresponding Seccomp proﬁle. If any
new application needs to be executed on the container after
this process, the administrator must run the analysis on the
application to update the Seccomp proﬁle.
6 Implementation
6.1 Mapping Libc Functions to System Calls
To ensure correctness, a precise function call graph is required
to identify and ﬁlter unused system calls. Based on our analysis
of more than 200 popular Docker images from Docker Hub [7],
we found that even though most containers use the popular
glibc library as their main user-space libc library,musl-libc [17]
was also used in 12 occasions. Although both musl-libc and
glibc provide implementations of the C standard library func-
tions, and applications should be able to use both interchange-
ably, we discovered that the system calls used by standard libc
functions some times differ between musl-libc and glibc.
To maximize compatibility, we analyzed both libraries inde-
pendently to extract their call graphs and their corresponding
function-to-system-call mapping. Moreover, due to certain
differences between glibc and musl-libc, which we discuss
next, we had to use a different toolchain for the analysis of
each of these libraries.
6.1.1 Musl-Libc
5.3 Hardening the Container Image
Once we have generated the list of system calls needed to run
the container, we can proceed to harden the container image.
Docker containers support the use of Seccomp ﬁlters to limit
the system calls accessible from the container. The user can
launch the container with a custom ruleset which speciﬁes the
system calls that can be accessed by the container. This ruleset
can be either in the form of a deny list or an allow list of system
calls prohibited or permitted. For Conﬁne, we use a deny list
of system calls that the container is not allowed to invoke.
Musl-libc [17] is a lightweight C standard library which has
a smaller codebase compared to glibc. For our analysis, we
compiled musl-libc with the LLVM [14] compiler toolchain
and implemented an LLVM pass to extract the complete call
graph. This pass operates on the intermediate representation
(IR) of the code and records each function call. To identify
system calls, in addition to recording each function call, we
make special note of calls to the syscall function. Using the
extracted call graph, we create a map between each exported
function in musl-libc and the system calls it invokes. We
modiﬁed the compiler toolchain to invoke the pass before
USENIX Association
23rd International Symposium on Research in Attacks, Intrusions and Defenses    447
Dynamic AnalysisRequired FunctionsContainerDocker ImageLaunchMonitorAnalyzeRequired System CallsStatic AnalysisLibc->SyscallIntegrateRequired Binaries & LibrariesExtract Direct Syscallsany optimization to prevent the loss of precision due to
optimizations and code transformations.
Musl-libc uses a weak_alias macro to deﬁne weak
symbols for functions. Weak symbols can be overridden by
strong symbols having the same name, without name collision
errors. Our LLVM pass keeps track of these aliases as well.
6.1.2 Glibc
Glibc is the most popular libc implementation used in most
containers. Glibc heavily relies on multiple GCC [11] features
which are not implemented in LLVM. Due to this issue,
we implemented a second analysis pass to extract the call
graph and system call information from glibc, based on the
GCC RTL (Register Translation Language) Intermediate
Representation. Our call graph extraction implementation
is based on the Egypt [38] tool, which operates on GCC’s
RTL IR. We discovered that there are three main mechanisms
through which glibc invokes system calls.
System Call via Inline Assembly and Assembly Files
This is the most straightforward mechanism for invoking sys-
tem calls. Functions such as accept4(), which is responsible
for accepting incoming socket connections, contain inline
invocations using the x86-64 syscall instruction. Given the
source code, the Egypt tool constructs the function call graph
for any given application or library. We augmented Egypt to
iterate over every call instruction in the RTL IR and record
any native x86-64 syscall instruction. Similarly, assembly
ﬁles also contain syscall instruction. Therefore, we analyze
the assembly ﬁles and extract all syscall instructions.
System Call Wrapper Macros
In addition to directly using
the syscall instruction, glibc also uses macro expansion
to generate wrappers to system calls. Other glibc routines
use these wrappers to invoke system calls. Because these
wrappers are implemented as architecture-dependent (in our
case x86-64) macros, they cannot be retrieved by analyzing
the RTL IR. Moreover, the parameters to these macros are
provided by a bash script during compile time.
The syscall-template.S ﬁle contains the macros
T_PSEUDO, T_PSEUDO_NOERRNO, and T_PSEUDO_ERRVAL,
that deﬁne wrappers to system calls. The list of system calls
to be generated, along with other information, such as symbol
names and the number of arguments, are provided in the
syscalls.list ﬁle. The Bash script make-syscalls.sh
reads this ﬁle at compile time, generates the correct macro
deﬁnitions, and invokes the expansion of the macros in the
syscall-template.S. This script is invoked as part of the
build process of glibc. During the compilation of glibc, we
trace the execution of this script and record the relevant macro
deﬁnitions observed during its execution. Using these macros
and macro deﬁnitions, we derive the mappings between these
wrappers and their respective system calls.
Weak Symbols and Versioned Symbols Similarly to musl-
libc, glibc uses the weak_alias macro to deﬁne weak symbols
for functions. GCC supports symbol versioning, and glibc uses
this feature to support multiple versions of glibc. The versioned
symbols are deﬁned using the macro versioned_symbol.
Both weak_alias and versioned_symbol provide aliases
for functions. Other functions within glibc, as well as the
applications using glibc, can invoke these aliased functions
either through the original function name or its alias. We
analyze the C source code to extract these aliases, and add
them to the call graph.
6.2 Binary Analysis
To capture a trace of all invoked executables, we leverage
Sysdig [26] to monitor the execve calls made during the
initial 30 seconds (conﬁgurable value) of the container. After
we generate the list of programs the container runs, we further
perform static analysis to extract the list of system calls
necessary for the correct execution of the container.
6.2.1 System Call Invocation Through Libc
After extracting the list of binaries, we recursively ﬁnd any
other libraries (except libc) that are loaded by them, and then
use objdump to extract the superset of imported functions
across all main executables and libraries. This analysis gives
us the list of libc (glibc or musl-libc) functions that are
imported by an application and its libraries. Then, using the
libc-to-syscall map generated as described in Section 6.1, we
derive the list of system calls required by the application. In
addition to these, the Docker framework itself needs certain
system calls to run. Consequently, after deriving the required
system calls for all the programs of a container, we combine
them with the list of system calls which Docker requires by
default to launch the container.
6.2.2 Direct System Call Invocation
We further encountered a limited number of libraries and appli-
cations that invoke system calls directly through either the libc
syscall() interface, or the native syscall assembly instruc-
tion. Analyzing such invocations requires deriving the values
of the arguments being passed to the system call. Fortunately,
extracting the ﬁrst argument, which speciﬁes the system call
number, is straightforward, as it is typically set by the (few) in-
structions preceding the syscall instruction or the syscall()
function invocation. We therefore use binary code disassembly
to identify the system call number by extracting the values as-
signed to the RAX/EAX register for the syscall instruction, and
the RDI/EDI register for the syscall() function. Conﬁne cur-
rently supports only x86-64, but adding support for other plat-
forms is straightforward by following their calling conventions.
448    23rd International Symposium on Research in Attacks, Intrusions and Defenses
USENIX Association
6.2.3 Dynamically Loaded Libraries
An issue that requires special consideration is dynamic loading,
a mechanism through which applications can load modules on
demand throughout their execution. The dlopen(), dlsym(),
and dlclose() API functions are used to load a library,
retrieve its symbols, and close it, respectively. Because
these operations are performed at run time, any libraries
loaded in this way cannot be identiﬁed by looking at the
application’s ELF binary header. For instance, Apache Httpd
uses this feature to load libraries based on the user-deﬁned
conﬁguration. Quach and Prakash in [65] have shown that
only around 3% of the 3174 programs and 2% of the 4292
libraries analyzed in their dataset used these features, all of
which loaded the required libraries during initialization.
To identify such dynamically loaded libraries, we monitor
the list of libraries loaded by the application at run time
through the /proc virtual ﬁle system, which provides this
information for every process. In Section 6.3 we discuss how
we use the same technique of monitoring the procfs to detect
the list of libraries used by Java applications.
One consideration is that if an application dynamically loads
libc, we cannot identify the individual functions imported
by the application, and would have to retain all system calls
made by libc. However, it is unlikely that libc will be loaded
in this fashion, as dynamic loading is used for modules that
provide additional functionality to the application. We did not
encounter any such case in our experiments.
6.3 Language-speciﬁc Considerations
Different languages have different software stacks, and
therefore different analysis techniques to extract the system
call policies. The programming language of the containerized
application has an important effect on the analysis methods
used to identify the system calls required by a given application.
In this section, we describe the different techniques we used to
handle applications written in programming languages other
than C/C++, which we encountered during our study of the top
200 Docker images. In Section 7 we present statistics on the
usage of these languages across the container images studied.
Go Applications written in the Go language consist of
command packages and utility non-main packages. Go
applications can be compiled into executables using two
build-modes: default, and c-shared. When compiled with
the default build mode, all main packages are built into
executables, and all non-main packages are built into a static
.a archive that is linked statically with the executables. Go
applications use system call wrappers provided by Go’s
syscall and runtime packages to invoke system calls.
When compiled with the c-shared build-mode, the main
package relies on the standard libc library to invoke system
call wrapper functions.
The analysis of our dataset shows that most of the Go
applications in the studied containers are built using the default
build mode. Therefore, unlike C/C++ applications which
rely on glibc, these applications use the Go core packages,
syscall and runtime, to make system calls. Consequently,
for containers that include Go applications, we require the
source code of all running Go applications to identify their
system calls. We use the callgraph tool [19] to build the call
graph of Go applications and all their dependencies, which we
have extended to record all calls to the system call wrappers
speciﬁed in the syscall and runtime packages.
Java/NodeJS Both the Java and NodeJS runtime applica-
tions use libc as a shared library to invoke system calls. The
Java compiler compiles Java source code into Java bytecode
and uses its own virtual machine (JVM) to interpret the
bytecode. Java programs are not compiled to machine code
and a binary is thus not generated. The interpreter and JVM
is provided by the java binary, which is launched via an
execve system call. To ﬁnd the system calls of a container
that hosts a Java type application, in addition to analyzing all
other running binaries, we also analyze the java binary that
contains this JVM, and any other libraries that are dynamically
loaded, as described above. Similarly, we handle the system
calls invoked by the node-js runtime.
Purely Interpreted Languages Scripting languages, such
as Python and Perl, are purely interpreted and require the