tolerance should have been disabled (false negative rate, or
FNR) or should not have been disabled (false positive rate,
or FPR).
Note that here we explore hardening typo-tolerance against cre-
dential stuffing attacks, and we leave the investigation of credential
tweaking attacks for future work. This decision is in part driven
by the high computational cost of the state-of-the-art credential
tweaking attack model [25] (used in Section 4.4), which limits the
amount of training and testing data we can practically use for our
machine learning development. Furthermore, credential stuffing
attacks are commonplace in practice [24, 30, 30], whereas to our
knowledge, credential tweaking attacks remain largely theoretical.
5.1 Disabling Typo-Tolerance for Weak
Passwords
We first explore using a PSM (specifically, zxcvbn [12]) to classify
susceptible passwords as weak ones. We evaluate the impact of dis-
abling typo-tolerance for weak passwords on users and credential
stuffing attacks.
Analysis Method. Our evaluation here mirrors the credential
stuffing attack analysis from Section 4, using the BreachCompilation
dataset. The only difference is that in this analysis, we do not permit
typo-tolerance for user passwords that are classified as weak by
zxcvbn. Given a password, zxcvbn outputs a strength score ranging
from 0 (weakest/too guessable) to 4 (strongest/very unguessable).
We consider two different thresholds for defining weak passwords:
1) strength score is less than 2 (too or very guessable) and 2) strength
score is less than 3 (somewhat guessable or weaker).
Across the different typo-tolerance policies and weak password
thresholds, we measure the population of users where typo-tolerance
is disabled due to the use of a weak password. We then evaluate
credential stuffing attacks under the random attack success metric
(where for each user, one password is randomly selected as the
leaked password, and another as the targeted password). We define
a false positive as a case where typo-tolerance is disabled for a
user but they are not using a susceptible password (i.e., credential
stuffing would not have succeeded even if typo-tolerance was en-
abled). A false negative is when credential stuffing succeeds for a
user due to typo-tolerance remaining enabled. We characterize the
Session 1D: Authentication and Click Fraud CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea259false positive and false negative rates to identify the effectiveness
of using a PSM for selectively disabling typo-tolerance.
Analysis Results. Table 4 depicts the impact of disabling typo-
tolerance for weak passwords. We find that such actions do provide
security benefits. False negatives indicate users of susceptible pass-
words who should have typo-tolerance disabled. Across all typo-
tolerance policies and weak password thresholds, we observe false
negative rates (FNR) between 6-28%, indicating that the majority
of users negatively impacted by typo-tolerance do have it disabled.
However, we observe that weak passwords are used by 73-90%
of users, depending on the weak password threshold. These propor-
tions are commensurate with findings from prior empirical analysis
of PSM scoring [11]. As a result, typo-tolerance is disabled for most
users, even when it need not be in most cases (as the false positive
rate is high). Thus, disabling typo-tolerance for weak password
users does not preserve the functionality of typo-tolerance.
We conclude that using a PSM to identify weak passwords and
selectively disable typo-tolerance for them is not a viable method
for hardening typo-tolerance. As an alternative approach, we next
explore developing a machine learning model specifically trained
to identify susceptible passwords. Intuitively, PSMs were not de-
veloped for such a task, so a model specifically designed for the
problem domain should outperform it (in addition, such a model
could use password strength as a feature while leveraging additional
features for improved accuracy).
5.2 Machine Learning Model Design
Model Features. As our model classifies password inputs, we de-
rive features from password characteristics. The full set of 46 fea-
tures considered are summarized in Table 8 of the Appendix, and
consists of categorical, numerical, and Boolean features. At a high
level, these features can be divided into several groups:
• One set of features focuses on capturing the password composi-
tion, structure, and complexity, including the classes of characters
used (i.e., uppercase and lowercase letters, digits, and symbols),
the frequency of transitions between character classes, password
entropy, and password strength. Our rationale for using such
features is that there may be a correlation between the password
structure and complexity, and the likelihood that the password
is used in slightly modified forms across online services.
• Another group of features cater to the typo-tolerance correctors,
and characterize the characters involved in corrector transfor-
mations. For example, the n2s_last corrector operates on the last
password character, changing it to the equivalent character un-
der the shift-key modifier. We use a feature indicating if the last
character would be changed by the corrector (e.g., “1” would be
transformed to “!”, but “T” is already capitalized and would not
be modified). Similarly, to account for the swc_first, rm_first, and
rm_last correctors, the features also capture the character types
of the first and last password characters, as well as their similari-
ties to adjacent characters. Using such corrector-specific features,
we aim for the model to identify when a password may be trans-
formed by a corrector (if not, then the password’s security is not
affected by the corrector).
• We also consider password popularity, as minor variants of pop-
ular passwords may be commonly used across sites.
Password Labels. A given password may be used by many
users, and its use may be susceptible under some users and not
others. In other words, given a password, some users may vary it
slightly for use across online services, while other users will not. As
a consequence, training a machine learning model on all instances
of a password’s use (across users) would result in a noisy signal,
where the same password input will have different labels depending
on the instance. To avoid unclear decision boundaries, we train our
model on distinct passwords, providing a consistent label for each
password input.
This approach raises the question of what the password label
should be, given some instances of the password will be susceptible
and others will not. We explore different password labels depending
on whether the proportion of the password’s users that are suscep-
tible exceeds a threshold, which we call the label threshold. Using
a 0% label threshold prioritizes security, as a distinct password is
classified as susceptible if any of its instances are susceptible. Using
a higher label threshold trades security for functionality, as the
classifier avoids labeling a distinct password as susceptible if only
a fraction of its users are susceptible. We evaluate our models on
data labeled using 0%, 10%, and 25% label thresholds. Note that
password labels also depend on the typo-tolerance policy, as the
policy dictates which password instances are susceptible.
Classification Model. We train machine learning models for
each of the five individual typo-tolerance correctors, as well as for
each of the five typo-tolerance policies (CTop1 to CTop5). We note
that one could implement a policy model by using multiple correc-
tor models; however, our subsequent exploration did not identify
classification accuracy benefits to doing so, while using multiple
models results in additional design complexity and performance
costs. Examining the individual corrector models does provide us
with insights on the characteristics of passwords susceptible under
each corrector function.
We use a binary decision tree for our classification model. While
we initially explored several alternative designs (including logistic
regression, support vector machines, random forests, and neural
networks), we did not observe superior classification performance.
Decision trees also are directly interpretable, and we explore the
most influential model features in Section 5.3.1.
We evaluate our model on the BreachCompilation leak dataset,
considering emails with multiple passwords (as we are defending
against credential stuffing attacks). We first randomly divide the
dataset’s multi-password emails into two groups, where one group
consists of 90% of the emails and is used as a training evaluation
dataset, and the remaining 10% of emails are reserved as a holdout
test dataset (which we will use for evaluating classifier performance
on password instances, rather than only considering distinct pass-
words). For the training evaluation dataset, we generate labels for
distinct passwords based on the typo-tolerance policy and the la-
bel threshold. This data exhibits heavy class imbalance, where the
majority of distinct passwords are not susceptible (the exact ratio
varies depending on the labeling). We downsample the dominant
class (not susceptible passwords) for a balanced training dataset4
(although we explored using imbalanced data, observing slightly
4Note that even after downsampling, our training datasets are sizable, with the smallest
training dataset containing 300K unique passwords.
Session 1D: Authentication and Click Fraud CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea260the rm_last model results in a 36% FPR, whereas for the swc_all,
swc_first, and rm_first models, the same recall level results in FPRs
of 20%, 20%, and 10%, respectively. We observe the same patterns
in the recall/FPR tradeoffs when the corrector models are trained
on data labeled with the 10% and 25% label thresholds (as seen in
the ROC curves of Figures 3 and 4, in the Appendix).
The performance of these models demonstrates that passwords
do exhibit characteristics that indicate likely susceptibility, although
the signal is (unsurprisingly) noisy. Except for the n2s_last cor-
rector model, we are unable to obtain high recall (e.g., >95%) at
low FPRs (e.g., <5%). However, we can still achieve meaningful
recall levels (e.g., 75%) at modest FPRs (e.g., 10-40%). Such operating
points would protect the majority of users from the security degra-
dation introduced by a typo-tolerance corrector, while still main-
taining typo-tolerance for most users. Thus, we argue that these
models can be useful in practice as they offer a different security-
functionality tradeoff compared to disabling typo-tolerance (result-
ing in no usability gains but no security losses) and fully enabling
typo-tolerance (affording the full usability benefits with the full
security degradation costs observed in Section 4).
Top Features. We inspect the most influential features in the
corrector decision-tree models to gain insights into their inner
workings, using the feature importance scores. In particular, the
dominant features represent password characteristics that most
signal susceptibility (or lack thereof) under a specific typo-tolerance
corrector, which also reflect the types of modifications that users
apply when using similar passwords across online services. Notably,
password strength is not a top feature for any corrector, aligning
with our prior finding that a PSM would serve as a poor classifier
for susceptible passwords.
• swc_all: The most important features for the swc_all models are
whether the password consists of all uppercase letters, whether
the password contains only uppercase letters and digits, and
the password length. The most susceptible passwords are those
shorter than 13 characters with only uppercase letters (and pos-
sibly digits).
• swc_first: The influential features for the swc_first models are
whether the first character is an uppercase or lowercase letter,
and whether the first two characters are from the same character
class. Susceptible passwords tend to be those with both uppercase
and lowercase letters as the first character and a lowercase letter
as the second character.
• rm_last: The top features for the rm_last models are not as dis-
criminative as with the other corrector models (unsurprising
as the rm_last models provided the worst recall/FPR tradeoffs).
The most impactful feature is the password length, with the
next two highest ranked features being the number of character
class transitions in the password and the length of the longest
single-character-class substring. Passwords shorter than 17 char-
acters with few character class transitions are more likely to be
susceptible under the rm_last corrector.
• rm_first: The dominant feature for the rm_first models is whether
the password consists only of digits. The password length and
popularity are two other influential features. The most suscepti-
ble passwords under the rm_first corrector are popular all-digit
ones with lengths less than 16.
Figure 1: ROC curves for the individual corrector models
trained on data labeled using a 0% label threshold.
worse model performance). Finally, we train our decision tree mod-
els on the training dataset using 10-fold cross-validation5, using
grid search for hyperparameter tuning (selecting a maximum tree
depth of 15). In Section 5.3, we discuss our training results as well
as our model performance on the holdout test dataset, where we
can evaluate model performance on password instances in addition
to distinct passwords.
5.3 Machine Learning Model Evaluation
In this section, we evaluate our password classifiers, presenting both
the model performance during training (using cross-validation),
as well as on a holdout test dataset that simulates the realistic
application of the classifiers. We also characterize the computational
costs of training and deploying the model.
5.3.1 Training Results. Here, we evaluate the results from train-
ing our different classifier models. As with any machine learning
approach, we aim to maximize recall while minimizing false posi-
tives. However, the cost of false positives in our scenario may be
considered relatively low, as a false positive results in disabling
typo-tolerance for a user. When this occurs, the user loses the
usability benefits of typo-tolerance but does not suffer security
consequences. Thus, a model with high recall but a modest false
positive rate (FPR) can still be useful in practice, offering a different
functionality-security tradeoff for typo-tolerance compared to fully
enabling or disabling typo-tolerance.
Corrector Models. Figure 1 depicts the receiver operating char-
acteristic (ROC) curves for the five individual corrector models,
where the data is labeled using the 0% label threshold, and each
curve is averaged across the cross-validation folds. The n2s_last
model has the highest ROC curve, with 99% recall at a 2% FPR. The
other corrector models offer less favorable recall/FPR tradeoffs, with
the rm_last model performing worst. Achieving 75% recall with
5As our training data consists of distinct passwords, during cross-validation, the set of
passwords our model is trained on has no overlap with the set of passwords it is tested
on. Thus, our model is evaluated on passwords it did not observe during training.
0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive Rateswc_allswc_firstrm_lastrm_firstn2s_lastSession 1D: Authentication and Click Fraud CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea261better security-functionality tradeoffs than with using a password
strength meter as a classifier, as evaluated in Section 5.1.)
5.3.2 Holdout Test Set Results. Recall from Section 5.2 that our
password classification task is defined on distinct passwords rather
than password instances. This formulation allowed us to pursue
models that leverage password characteristics to predict likely sus-
ceptibility (whereas models operating on password instances would
encounter the same password input with varying output labels, as
some instances are susceptible while others are not). Section 5.3.1’s
evaluation of our corrector and policy password classifiers similarly
considered model performance across distinct passwords (in the
training dataset). However, in practice, policy models would be
applied to password instances across users. In this section, we in-
vestigate how the classifiers perform in this realistic setting, using
our holdout test set of emails (the random 10% of emails withheld
from the training evaluation, as discussed in Section 5.2).
Analysis Method. For the five typo-tolerance policies, we use
the same policy model parameters as evaluated in Section 5.3.1,
except with the models trained on distinct passwords in the entire
training dataset (as our earlier evaluation used cross-validation).
We again consider the 0%, 10%, and 25% label thresholds for labeling
the training data. For each policy model, we consider multiple recall
operating points, exploring the tradeoff between recall and the false
positive rate. Specifically, we consider models tuned to 10%, 25%,
50%, 75%, and 90% recall. We then apply these models to classify
a randomly selected password for each email in the holdout test
dataset, simulating the use of our models at an online service.
For each model configuration (training label threshold, typo-
tolerance policy, and recall operating point), we analyze the propor-
tion of emails with their (randomly selected) password classified as
susceptible (to credential stuffing once typo-tolerance is enabled).
These would be emails where typo-tolerance (using the model’s
corresponding policy) would be disabled, representing the total
user-facing impact of applying the model. To tease apart the secu-
rity and functionality implications of the models, we assess whether
the passwords are actually susceptible or not, in a similar fashion as
done for the random attack success metric in Section 4.3. For each
email, we randomly select a second password as the leaked pass-
word. We identify whether the typo-tolerance policy corrects the
leaked password to match the original randomly selected password
(i.e., the original password is susceptible in reality), and whether our
model predicts the original password as susceptible. If the model
predicts susceptibility but the email’s password is not susceptible
in reality, we consider the email as a false positive. Here, our model
would cause typo-tolerance to be unnecessarily disabled for the
email. Similarly, if the model does not predict susceptibility but the
email’s password is actually susceptible, we consider the email as a
false negative.
Analysis Results. Table 5 shows the model susceptibility pre-
diction rate, the prediction false positive rate (FPR), and the predic-
tion false negative rate (FNR) for our various policy models when
trained on data using the 10% label threshold. Across all five poli-
cies, we observe recall operating points that offer modest FPRs and
FNRs. For example, at 50% recall, the five policy models predict
between 24–41% of emails to have a susceptible password, with
FPRs ranging between 23–39% and FNRs ranging from 23–35%.
Figure 2: ROC curves for the policy models trained on data
labeled using a 0% label threshold.
• n2s_last: The notable features for the n2s_last models are whether
the last character can be shift-key modified, whether the pass-
word’s last two characters are the same character class, and
whether the password consists of lowercase letters and digits.
Susceptible passwords are often those with an uppercase letter or
symbol as the last character (i.e., a character that is the shift-key
modification of another character), where the remainder of the
password consists of lowercase letters and digits.
Policy Models. Figure 2 shows the ROC curves for the five pol-
icy models where each curve is averaged across the cross-validation
folds. Here, the models are trained on data labeled using a 0% la-
bel threshold. We observe that each policy model’s performance
is similar to that of the worst-performing corrector in the policy.
CTop1 consists only of the swc_all corrector, so their models’ ROC
curves are identical. The ROC curves for the CTop1 and CTop2 mod-
els are similar. As CTop2 adds the swc_first corrector, this similarity
aligns with the similar ROC curves for the swc_all and swc_first
corrector models. Finally, the policy models for CTop3, CTop4, and
CTop5 (all of which include the rm_last corrector) present nearly
identical performance, matching the performance of the rm_last
corrector model (the worst-performing corrector model). Achieving
75% recall with the CTop3, CTop4, and CTop5 policy models result in
a 36–38% FPR. Meanwhile, the same recall level for the CTop1 and
CTop2 policy models result in a FPR of 21% and 26%, respectively.
As with the corrector models, we observe the same patterns for
policy models trained on data labeled using the 10% and 25% label
thresholds (as visible in Figures 5 and 6 in the Appendix).
These results show that to a modest extent, we can discrimi-
nate between susceptible and non-susceptible passwords under a
typo-tolerance policy. Our policy models exhibit performance that
is largely bounded by the performance of the worst-performing
model for a corrector in the policy. As with the corrector models, we
failed to achieve high recall at a low FPR. However, our policy mod-
els still provide meaningful recall levels with modest FPRs, allowing
a typo-tolerant authentication system to trade off between func-
tionality and security. (We note that these policy models also offer
0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive rateTop1Top2Top3Top4Top5Session 1D: Authentication and Click Fraud CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea262Label
Threshold
Policy
Top1
Top2
10%
Top3
Top4
Top5
Recall
10%
25%
50%
75%
90%
10%
25%
50%
75%
90%
10%
25%
50%