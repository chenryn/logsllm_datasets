title:Microscope: Queue-based Performance Diagnosis for Network Functions
author:Junzhi Gong and
Yuliang Li and
Bilal Anwer and
Aman Shaikh and
Minlan Yu
Microscope: Queue-based Performance Diagnosis for Network
Functions
Junzhi Gong
Harvard University
Yuliang Li
Harvard University
Bilal Anwer
AT&T
Aman Shaikh
AT&T
Minlan Yu
Harvard University
ABSTRACT
By moving monolithic network appliances to software running
on commodity hardware, network function virtualization allows
flexible resource sharing among network functions and achieves
scalability with low cost. However, due to resource contention,
network functions can suffer from performance problems that are
hard to diagnose. In particular, when many flows traverse a complex
topology of NF instances, it is hard to pinpoint root causes for a
flow experiencing performance issues such as low throughput or
high latency. Simply maintaining resource counters at individual
NFs is not sufficient since the effect of resource contention can
propagate across NFs and over time. In this paper, we introduce
Microscope, a performance diagnosis tool, for network functions
that leverages queuing information at NFs to identify the root causes
(i.e., resources, NFs, traffic patterns of flows etc.). Our evaluation on
realistic NF chains and traffic shows that we can correctly capture
root causes behind 89.7% of performance impairments, up to 2.5
times more than the state-of-the-art tools with low overhead.
CCS CONCEPTS
• Networks → Middle boxes / network appliances; Network
performance analysis; Network performance modeling.
KEYWORDS
NFV, performance, diagnosis
ACM Reference Format:
Junzhi Gong, Yuliang Li, Bilal Anwer, Aman Shaikh, and Minlan Yu. 2020.
Microscope: Queue-based Performance Diagnosis for Network Functions. In
Annual conference of the ACM Special Interest Group on Data Communication
on the applications, technologies, architectures, and protocols for computer
communication (SIGCOMM ’20), August 10–14, 2020, Virtual Event, NY, USA.
ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/3387514.3405876
1 INTRODUCTION
Network function virtualization (NFV) transforms hardware mid-
dleboxes to software running on commodity hardware – called
Virtual Network Functions (VNFs), thereby bringing flexibility and
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-7955-7/20/08...$15.00
https://doi.org/10.1145/3387514.3405876
390
agility to network operations. As a result, NFV has become popular
in both industry and research [1, 9, 38, 48, 49]. For example, Internet
Service Providers purchase network function solutions (e.g., NATs,
Firewalls, VPNs) developed by different vendors, then run them
in chains or DAGs (Directed Acyclic Graph) to serve traffic from
various users.
Since VNFs process packets in software, there are inevitably
more performance variations (e.g., throughput variations, high tail
latency, jitters) than hardware platforms. These performance prob-
lems have a significant impact on service-level agreements and user
experiences [35]. A survey we carried out with network operators
has revealed various types of performance problems encountered
in real-world NFV deployments (see § 2).
When performance problems emerge, the first question is who
(users, ISP operators, NF vendors) is responsible for the problems.
The process of finding answers often leads to blame games amongst
these parties because many performance problems are intermittent
and thus not easily reproducible. Furthermore, each party lacks
full access to the system for debugging (e.g., ISPs cannot access NF
vendor codes, while vendors may not know user traffic).
Let us use an example to highlight these challenges. Suppose
we have an NF chain consisting of a Firewall followed by a VPN.
Assume that some packets experience long latency at the VPN.
Intuitively, operators start by blaming the VPN vendor for the
problem. However, no problems are observed when the VPN is run
without the Firewall. As a result, operators start to suspect that it is
user traffic that is the root cause of the problem. Sometimes bursty
traffic may lead to queue buildups at the VPN which results in a
long latency. However, this turns out not to be the case this time.
Rather, as we find in the end, this problem is caused by a bug in
the Firewall that inflates the time it takes to process some flows,
resulting in intermittent traffic bursts towards the VPN.
In practice, this problem is much more complex as user traffic
goes through a variety of different NF chains. Many fine time-scale
events occur in each NF (e.g., interrupts, cache misses, context
switching) that could cause intermittent performance problems,
whose effects may propagate across NF instances. Although there
has been significant work dealing with NF performance optimiza-
tion [18, 29, 32, 34, 39, 43, 51, 55], load balancing [23, 27], and auto-
scaling [20, 44, 52], performance problems in NF chains still abound
since it is hard to carefully engineer all the components to build a
fully-optimized chain. And even if we succeed in doing so, we still
need to optimize performance every time software, hardware, or
NF chain configuration changes [13].
In this paper, we propose Microscope, a performance diagnosis
tool that identifies the causal relations in a DAG (Directed Acyclic
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Gong, et al.
2 MOTIVATION
In this section, we show a few examples to highlight the challenges
of diagnosing performance problems in NF chains. We next dis-
cuss our survey with network operators which corroborates these
challenges.
(a) Packet Latency
(b) Queue length
Figure 1: We send CAIDA traffic [2] to a Firewall. At 570 µs, we
inject a bursty flow which lasts 340 µs. (a) All the other flows arriv-
ing in the next 3 ms experience long latency. (b) The input queue
quickly builds up but then takes around 3 ms to drain.
Graph) of NFs without any knowledge of their implementation.
With the causal relations, we can identify the root cause(s) of per-
formance problems such as misconfiguration and bugs, traffic anom-
alies, system interrupts, load imbalances, etc. There have been many
research papers dealing with the identification of causal relations
for large-scale distributed systems [16, 19, 36, 41, 44]. For example,
NetMedic [36] infers causal relations by computing the probabili-
ties of monitored behaviors (e.g., resource usage, processing rate)
and problems (e.g., long latency) falling in the same time window.
However, this work is not a good fit for NFs because NFs process
packets in tens to thousands of microseconds and thus can easily
be affected by the fine-timescale network (e.g., traffic bursts, queu-
ing) and system behaviors (e.g., interrupts, context switching, data
copies [37]). Correlating fine time-scale abnormal behaviors with
problems is challenging because there can be many episodes of
such behaviors and each episode can have a lasting impact. So it is
hard to place the related behaviors and problems in the same time
window. For example, Figure 1 shows that a bursty flow of 300 µs
can impact flows that arrive in the next three milliseconds because
of the long time the queues take to drain. While this happens, the
impact of queuing may propagate to other NFs and flows (see more
examples in § 2).
To address this challenge, we observe that it is through queues
that adjacent NFs in a chain interact with one another. An upstream
NF affects its downstream NF by changing the traffic rates to the
queue, while the processing rate of the downstream NF affects the
rate at which the queue drains. Moreover, as shown in Figure 1,
a queue essentially “propagates” the impact of a previous event
(e.g., traffic bursts) to future flows. Therefore, Microscope directly
collects queuing information with low overhead. It then performs
a queue-based causal analysis that quantifies the impact of flows
and NFs on packet delay. Microscope also aggregates packet-level
diagnosis into relational patterns which allow operators to auto-
matically focus on the right flows and NFs.
Our evaluation demonstrates that Microscope can correctly cap-
ture 89.7% of all performance problems emanating from a variety
of reasons such as traffic bursts, interrupts, NF bugs, etc., which is
up to 2.5 times higher than the state-of-the-art tools. Microscope
achieves this while keeping the runtime overhead quite low.
2.1 Challenges of NF Diagnosis
We consider a DAG (Directed Acyclic Graph) of NFs where NFs
can be provided by one or more vendors. We assume no access to
NF implementation. There are NICs (SR-IOV), hardware switches,
and/or software switches [10, 11, 30] that direct user traffic to NFs
and forward traffic between NFs1. Note that one NF type may run
multiple NF instances on multiple cores on the same or different
servers to scale to traffic growth. For the rest of the paper, we use
the term NF to refer to an NF instance unless noted otherwise.
Our goal is to find the causal relations between the NFs/flows with
intermittent2 performance problems (e.g., low throughput and long
tail latency).
The state-of-the-art approach for diagnosing causal relations
is time-based correlation [16, 19, 36, 41, 54]. They are based on
the assumption that abnormal behaviors happening in the same
time window as the problem are more likely to be the root causes.
This approach, unfortunately, does not work for NFs that exhibit
abnormal behavior at fine time-scales (e.g., traffic bursts, CPU in-
terruption, context switching), and where such behavior can have
lasting impacts. Next, we provide a few examples to demonstrate
the challenges with such scenarios.
1. Lasting impacts of microsecond-level behaviors. A flow
may experience performance problems when it faces an abnormal
behavior such as a traffic burst. However, the impact may last
long after the flow finishes. As shown in Figure 1a, although the
bursty flow finishes at time 1 ms, all the new flows that arrive up
to 3 ms later still experience a long latency. The reason is shown in
Figure 1b. The queue takes about 3 ms to drain because the Firewall
is busy keeping up with the incoming flow rates.
In practice, in addition to traffic bursts, there are many other
fine time-scale abnormal behaviors (such as interrupts, context
switching etc.) that happen all the time at different NFs. The impact
of these behaviors lasts for different time periods depending on
the severity of resource contention, incoming traffic rates, and
processing rates of NFs. This makes it challenging to define the
right size for time window based correlation: a small window misses
the correlations with behaviors whose impacts last longer than
the window size, while a large window ends up including lots of
unrelated behaviors.
2. Lasting impact propagates across NFs. A few fine time-scale
behaviors at one NF may affect packets at another NF which has
no spatial or temporal correlation with the first NF. To illustrate
this, we send CAIDA [2] traffic through a chain consisting of a
NAT followed by a VPN. We send another flow A directly through
the VPN (see Figure 2a). Figure 2b shows that flow A at the VPN
experiences low throughput during [1.5ms, 2.3ms] time interval. If
1For simplicity, we assume the switch is not the cause. We can easily treat the switches
as another NF in the system for diagnosis if needed.
2Our solution also works for persistent problems. But persistent problems are much
easier to diagnose and can use existing tools (e.g., PerfSight [53]).
391
 0 100 200 300 400 500 600 700 0 1 2 3 4 5Latency (us)Time (ms)Burst ﬂowBackground traﬃc 0 100 200 300 400 500 600 0 1 2 3 4 5Queue lengthTime (ms)Microscope: Queue-based Performance Diagnosis for Network Functions
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
we do time-based correlation, we may think the low throughput of
flow A is caused by the surge in traffic from the NAT. But we send
the CAIDA traffic at a constant rate throughout the experiment. In
fact, the root cause is that the NAT experiences a CPU interrupt
between [0.5ms, 1.3ms], and hence is unable to send any traffic to
the VPN during this time. After the interrupt, the NAT resumes
processing and sends a burst of packets to the VPN, which causes
the throughput drop for flow A.
The queue at the VPN illustrates this phenomenon (Figure 2c):
The traffic burst from the NAT builds up the queue at the VPN start-
ing around 1.5 ms, affecting flow A packets arriving from then on,
although these packets only traverse the VPN, and do not overlap
temporally with the interrupt at the NAT.
In practice, the performance depends on not only the abnormal
behaviors at different NFs or traffic sources, but also the queue
occupancy at these NFs when such behavior occurs. Therefore, it is
not enough to identify causal relations based on temporal or spatial
correlations.
contribution to change in input rate, so that we can focus on the
most important problems.
(a) Different impact of NAT and Monitor on VPN perfor-
mance
(b) Packet drops on the VPN
(c) Input rates changes
Figure 3: Different impact of similar behaviors
(a) NAT’s interrupt affects VPN’s performance
(b) Throughput at the VPN