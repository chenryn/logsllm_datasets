### Nine Challenges per Exercise (313 Total; Median 9.5; Maximum 17)

We focused on the presentation of information rather than difficulty, and followed published challenge walkthroughs when available. This allowed us to quickly review complex challenges. We primarily targeted challenges marked as less difficult by the authors, expecting that organizers would provide more pedagogical support for earlier-stage students. This assumption was confirmed in our organizer interviews. However, we also reviewed multiple challenges rated as more difficult in each exercise to ensure a comprehensive evaluation.

### Review of Results with Organizers

We reviewed our results with the organizers of 14 exercises to determine if any unreviewed challenges implemented additional pedagogy (see Section II-C). Only minor updates were made during these reviews.

### Initial Codebook and Inter-Rater Reliability

After establishing our initial codebook, two researchers independently reviewed 20 exercises, comparing results after every five exercises to ensure inter-rater reliability. For dimensions that did not require judgment (e.g., availability of solutions), inter-rater reliability was not calculated. We used Krippendorff’s Alpha (α) to measure inter-rater reliability, which accounts for chance agreements [35]. After each round, the researchers resolved coding differences, modified the codebook as necessary, and re-coded previously reviewed exercises until an α of at least 0.8 was achieved. The remaining exercises were divided evenly between the two researchers. Final α values are provided in the first row of Table I.

### Organizer Interviews (RQ2)

To address our second research question, we needed additional context from organizers to understand their decision-making process. We reached out to the organizers of all 31 exercises. For BIBIFI, where two authors are affiliated, we interviewed the original architect who was not involved in this paper. Each organizer received a report describing our review and was invited to participate in a 45-minute structured interview or respond via email. The report included definitions of all pedagogical dimensions, our coding for their exercise, and the reasoning behind our decisions. We emphasized that our goal was to understand their decision-making, not to critique it, and adopted a constructive tone.

Fifteen organizers responded, with 13 participating in video-call interviews and 2 responding via email. During the interviews, we walked through the report and asked if they agreed with our assessment and, if not, why. Based on their feedback, we revisited our results and made updates as needed. Changes were made based on nine of the 15 responses: two changes each for two exercises, and one change each for the other seven. Updates are indicated with a ‡ in Table I.

For dimensions not implemented, we asked if the organizers considered them and, if so, why they chose not to implement them. Our interview protocol is given in Appendix A. This study component was reviewed and approved by our organization’s ethics review board. All raw records, including identifying information, were securely maintained.

### Open Coding of Decision-Making Themes

To identify themes in organizers’ decision-making, we performed open coding of their reasons for not implementing certain dimensions. Two researchers reviewed three responses together, then independently coded 12 responses, comparing codes after every three until achieving sufficient inter-rater reliability (α = 0.86) [35]. The remaining interviews were divided evenly.

### Limitations

Our study has several limitations inherent to our review and sampling method, and some common to exploratory qualitative research. First, because many pedagogical dimensions have not been specifically evaluated in the security education setting, we cannot determine which pedagogies are most effective, how they interact, or their effectiveness in specific contexts. Future work is needed to address these questions, but evidence from other disciplines suggests that implementing each pedagogical dimension is generally beneficial.

Next, we may not have identified all candidate exercises meeting our criteria, and we may have missed particularly good implementations of some pedagogical dimensions. However, our thorough search process and focus on popular exercises likely make our results representative of most students' experiences. In our review, we checked whether a dimension was implemented, not whether it was well-implemented. This conservative approach allowed us to broadly evaluate the types of pedagogy considered and establish an initial understanding of the current landscape.

There is also self-selection bias in which organizers agreed to be interviewed. Engaged organizers may be more likely to respond, and those who implemented more pedagogical dimensions were more likely to agree to an interview. Despite our efforts to ensure positive and constructive feedback, some organizers may have found our comments or interview request pejorative. Social desirability bias may also influence organizers to present themselves in the best possible light. To mitigate this, we only revised our assessments if organizers identified elements we missed, but did not allow them to redefine pedagogical dimensions to better suit their exercise.

Overall, our findings should be interpreted within this context and may reflect a higher-than-average interest in improving student learning. Nonetheless, they provide novel insights into security education and directions for future work.

### Prevalence Indicators

In the next section, we provide the number of exercises (N) and organizers (O) that demonstrated or expressed concepts, respectively, to indicate prevalence. If an organizer did not specify a reason for not implementing a pedagogical dimension, it does not necessarily indicate disagreement; they may have simply omitted it.

### Results

Final review results are given in Tables I and II. Each exercise was assessed on all 30 pedagogical dimensions, grouped into synchronous and asynchronous, and sorted by popularity. No exercise implemented all dimensions, but innovative approaches were observed across all exercises. We organize our discussion around the five core principles, considering each evaluated dimension in detail. For brevity, we discuss the 23 dimensions in Table I, which showed reasonable differentiation between exercises. The remaining 7 dimensions can be found in Appendix C.

### Connecting to Students’ Prior Knowledge

Learning science research shows that new knowledge is developed based on pre-existing knowledge, including facts, perceptions, beliefs, values, and attitudes [28], [29], [66]–[68]. Students interpret new information through their current worldview and bring various prior experiences into learning. Understanding is developed through analogies and connections to previously learned concepts. Accurate and complete prior knowledge facilitates learning, while inaccurate or incomplete knowledge hinders it. Therefore, carefully considering and activating students’ prior knowledge should help them build new knowledge successfully.

Additionally, tailored education positively affects student motivation. Appropriately tailored challenges reduce the likelihood of students feeling overwhelmed, instead boosting their confidence in their learning ability [69].

### Personalization

Each student has a unique background, so exercises should adjust challenge presentation and difficulty to account for these differences, or target specific sub-populations [70]. We considered three personalization dimensions: age, educational status, and security experience.

Experience-based personalization was common. Most (N=23) exercises allowed some personalization by experience, using a mix of difficulty indicators such as labels (e.g., Easy, Medium, Hard) (N=10), the number of other students who solved the challenge (N=15), and point values (more points indicating increased difficulty) (N=18). This guides participants to appropriate problems, avoiding burnout or boredom. Student-guided personalization also provides autonomy.