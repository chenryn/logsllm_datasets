nine challenges per exercise (313 total; median 9.5; maximum
17). Because we focused on information presentation and not
difﬁculty, we followed published challenge walkthroughs when
available, allowing us to review complex challenges quickly.
Note that we explicitly targeted mostly challenges marked as
less difﬁcult by challenge authors, as we expected organizers to
provide the most pedagogical support to earlier-stage students;
this was conﬁrmed in our organizer interviews. However, we
did review multiple challenges rated as more difﬁcult by the
organizers in each exercise to ensure a complete view.
Finally, we reviewed our results with the organizers of 14
exercises to determine whether challenges we did not review
implemented additional pedagogy (see Section II-C). Only
minor updates were made in these reviews.
After establishing our initial codebook, two researchers
independently reviewed 20 exercises, comparing results af-
ter every ﬁve exercises for inter-rater reliability. (In cases
where the dimension could be assessed without any judgment
decisions, inter-rater reliability was not calculated, as it is
unnecessary [34]. For example, when evaluating whether
solutions were available, we assigned “yes” if the exercise
offered direct links to solutions or we could ﬁnd them on a
google search’s ﬁrst page.) To measure inter-rater reliability,
3One exception: We completed the only three free HackEDU challenges.
we used Krippendorff’s Alpha (α), which accounts for chance
agreements [35]. After each round, the researchers resolved
coding differences, modiﬁed the codebook when necessary,
and re-coded previously reviewed exercises. This process
was repeated until an α of at least 0.8—the recommended
result reliability threshold [35]—was achieved. The remaining
exercises were divided evenly between the two researchers.
Final α values are given in the ﬁrst row of Table I.
C. Organizer Interviews (RQ2)
Because we did not review every challenge in each exercise,
we offered organizers an opportunity to provide clarifying
information. Also, to answer our second research question,
we needed additional context from organizers to understand
their decision-making process. As such, we reached out to the
organizers of all 31 exercises. For BIBIFI, with which two
authors are afﬁliated, we interviewed the exercise’s original
architect who was not involved with this paper. We gave each
organizer a report describing our review and invited them to
participate in a 45 minute structured interview or respond
to our review via email. Each report gave all pedagogical
dimension deﬁnitions, our coding for their exercise, and the
reasoning behind our decisions. In our report and throughout
our interviews, we were careful to emphasize that our goal
was to understand their decision-making, not critique it. We
made sure to adopt a constructive tone rather than presenting
ﬁndings in an accusatory manner. We let organizers know
we invited and expected disagreements with our evaluation, as
there were likely elements or viewpoints we had not considered.
Fifteen organizers responded to our report, 13 participated in
a video-call interview and 2 answered our questions via email.
In our interviews, we walked organizers through the report
and asked whether they agreed with our assessment and if not,
why. Based on organizer feedback, we revisited our results,
making updates as needed when the organizers pointed us to
challenges or other portions of the site we may have missed.
Changes were made based on nine of the 15 responses: two
changes each for two exercises, and one change each for the
other seven. Updates are indicated with a ‡ in Table I.
For dimensions not implemented, we asked organizers if they
considered the dimension when building their exercise and if so,
why they chose not to implement it. Our interview protocol is
given in Appendix A. Because this study component constituted
human-subjects research, it was reviewed and approved by our
organization’s ethics review board. All raw records, including
organizers’ identifying information, were maintained securely.
To identify themes in organizers’ decision-making, we again
performed open coding of organizers’ reasons not to implement
dimensions. Responding organizers are shown in Table I. To
establish our codebook (Appendix B), two researchers reviewed
three responses together. Then, those researchers independently
coded 12 responses, comparing codes after every three until
attaining sufﬁcient inter-rater reliability (α = 0.86) [35]. The
researchers divided the remaining interviews evenly.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:31:30 UTC from IEEE Xplore.  Restrictions apply. 
1270
D. Limitations
Our study has several limitations inherent to our review and
sampling method, and some common to exploratory qualitative
research. First, because many pedagogical dimensions have
not yet been evaluated speciﬁcally in the security education
setting, we cannot say which pedagogy are most effective,
how they interact, or how effective they are in varying
speciﬁc contexts (e.g., effect of narrative for simpler vs.
harder challenges). Future work is necessary to answer these
speciﬁc questions, but evidence from other disciplines strongly
suggests that implementing each pedagogical dimension is
very likely beneﬁcial in general (other things being equal).
This paper is instead intended to map the choices made in
current exercises, highlighting tradeoffs based on organizers’
practical experiences. This can guide future organizers to make
intentional choices about pedagogy and future research to
consider speciﬁc tradeoffs and contexts to evaluate.
Next, it is likely that we did not identify all candidate
exercises meeting our stated criteria. Additionally, because
we only review a sample of exercises, we may have missed
a particularly good implementation of some pedagogical
dimension. However, because of our thorough search process
and weighting our sample toward more popular exercises, our
results are likely representative of most students’ experience.
In our pedagogical review, we adopt a conservative approach,
checking whether a dimension is implemented, but not whether
it is implemented well. We did this to broadly evaluate the types
of pedagogy considered and establish an initial understanding
of the current landscape. However, we cannot make statements
about the efﬁcacy of speciﬁc approaches. We encourage future
work to build on our established roadmap.
Further, there is likely self-selection bias in which organizers
agreed to be interviewed. In general, organizers who are more
engaged in supporting student learning may be more likely to
respond to a request to discuss pedagogy. We also observed
anecdotally that organizers who implemented more pedagogical
dimensions were more likely to agree to an interview. While this
may reﬂect engagement in pedagogy, it may also indicate that—
despite our best attempts to ensure our feedback was positive
and constructive—some organizers found our comments or
interview request pejorative. In addition, social desirability bias
suggests that organizers may (consciously or unconsciously)
tailor their responses to appear in the best possible light.
To partially mitigate this, we only revised our dimension
assessments if organizers identiﬁed exercise elements we
missed in our initial review, but did not allow organizers to
argue for pedagogical dimension redeﬁnition to better suit their
exercise. Overall, our ﬁndings regarding organizer decision-
making should be interpreted within this context, and may
reﬂect a higher-than-average degree of interest in improving
student learning. Nonetheless, we believe they provide novel
insights into security education and directions for future work.
Finally, in the next section, we give the number of exercises
(N) and organizers (O) that demonstrated or expressed, respec-
tively, concepts, to indicate prevalence. If an organizer did not
indicate a speciﬁc reason for not implementing a pedagogical
dimension, this does not necessarily indicate disagreement;
instead, they may have simply failed to mention it.
III. RESULTS
Final review results are given in Tables I and II. Each exercise
was assessed on all 30 pedagogical dimensions. Exercises
are grouped into synchronous and asynchronous, then sorted
by popularity. Overall, we found that while some exercises
implemented more pedagogical dimensions than others, no
exercise implemented all dimensions. Additionally, we observed
innovative approaches to education distributed among all
exercises. We organize our discussion around the ﬁve core
principles, considering each evaluated dimension in detail. For
brevity, we only discuss the 23 dimensions included in Table I,
which exhibited reasonable differentiation between exercises.
The remaining 7 dimensions can be found in Appendix C.
A. Connecting to students’ prior knowledge
Learning science research shows that people develop new
knowledge based on pre-existing knowledge, including facts,
perceptions, beliefs, values, and attitudes [28], [29], [66]–[68].
Students interpret new information through their current world
view, and they bring a variety of prior experiences into learning.
Students develop understanding through the production of
analogies and connections to previously learned concepts—
in the same domain or otherwise. The prior knowledge a
student brings to a new context can facilitate learning if it is
accurate and complete, or hinder learning if not. Therefore,
careful consideration of the students’ prior knowledge and deep
connection to and activation of that knowledge should help
students successfully build new knowledge.
Additionally, supporting tailored education positively affects
student motivation. If challenges are appropriately tailored to
the student, they will be less likely to feel out of their depth,
instead growing their conﬁdence in their learning ability [69].
To evaluate how exercises connected to students’ prior knowl-
edge, we considered two dimension groups: personalization
and utilization.
1) Personalization: Each student has a unique background,
so exercises should adjust challenge presentation and dif-
ﬁculty to account for these differences, or target speciﬁc
sub-populations [70]. We considered three personalization
dimensions we believe (based on prior work in security [1],
[71]–[74] and learning science [29], [66]) likely affect learning
background: age, educational status, and security experience.
Experience-based personalization was common. Most
(N=23) exercises allow some personalization by experience.
These exercises used a mix of difﬁculty indicators, including
difﬁculty labels (e.g., Easy, Medium, Hard) (N=10), the number
of other students who have solved the challenge (N=15), and
point values (i.e., more points indicate increased difﬁculty)
(N=18). This guides participants to problems appropriate to
their experience level, avoiding burnout on problems beyond
their reach or boredom with challenges they can easily solve.
This student-guided personalization can also give autonomy,
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:31:30 UTC from IEEE Xplore.  Restrictions apply. 
1271
)
1
A
-
I
I
I
(
n
o
i
t
a
z
i
l
a
n
o
s
r
e
P
n
o
i
t
a
c
u
d
E
e
c
n
e
i
r
e
p
x
E
e
g
A
α
1
1
0.8
)
1
B
-
I
I
I
(
n
o
i
t
a
z
i
n
a
g
r
O
)
2
A
-
I
I
I
(
n
o
i
t
a
z
i
l
i
t
U
e
g
d
e
l
w
o
n
k
t
n
e
u
q
e
s
b
u
S
1
l
a
c
i
h
c
r
a
r
e
i
H
h
t
a
p
m
e
l
b
o
r
P
1
1
s
e
s
i
c
r
e
x
e
d
l
r
o
w
l
a
e
R
)
1
C
-
I
I
I
(
y
t
i
l
i
b
a
n
o
i
t
c
A
)
2
B
-