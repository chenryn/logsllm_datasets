5)  Log  Analysis. Software logging is pervasive (D. Yuan, et al., 2012) and analysing logs has 
become  more  and  more  important  for  different  purposes  such  as  system  failure  detection  and 
diagnosis. Analysing logs means to abstract the useful information from log files and make use 
of  the  abstracted  information  to  fulfil  operator  needs  such  as  process  monitoring  and  failure 
detection (D. Yuan, et al., 2012; D. Jayathilake, 2012). Currently, there are many existing tools 
for  log  analysis,  but  a  lack  of  proper  support  for  structured  log  analysis  is  identified  as  one 
major  flaw  in  the  existing  tools  for  log  analysis  (D.  Jayathilake,  2012).  In  the  context  of 
recovery, log analysis is a key step in the procedure of figuring out a log analysis based system 
recovery method.  
6)  Cloud  Infrastructure.  Cloud  infrastructure  refers  to  the  infrastructural  resources  provided 
by  the  cloud  platform  such  as  cloud  instances,  cloud  data  storages,  auto  scaling  groups  and 
elastic  load  balancers.  It  typically  has  three  levels  of  services:  1)  Infrastructure  as  a  Service 
(IAAS),  such  as  Amazon  EC2,  Windows  Azure,  RackSpace  and  VMware;  2)  Platform  as  a 
Service (PAAS), such as Google App Engine; 3) Software as a Service (SAAS), such as Apache 
web  service  (S.  Paquette,  et  al.,  2010).  Cloud  infrastructure  has  some  features  that  non-cloud 
systems do not have. For example, virtual machines or cloud instances in cloud can be created 
dynamically  by  using  the  on-demand  nature  provided  by  cloud,  and  the  architecture  of  cloud 
applications  can  be  dynamically  scaled  out  or  scaled  in  at  runtime  based  on  scaling  rule 
configurations by using the scalability feature of cloud.  
7)  Virtual  Machine  Replication.  Cloud  virtual  machine  replication  is  actually  a  type  of 
Hypervisor-Based  replication  (L.  DuBois,  2013).  On  top  of  the  hypervisor  level,  a  virtual 
machine in one site can be replicated to either the same site or another site to make a replication 
of the virtual machine on a real-time basis. When the service of the original virtual machine is 
down,  the  replication  of  the  original  virtual  machine  can  take  over  and  continue  the  service 
provisioning.  One  challenge  is  that  the  incremental  data  in  the  original  virtual  machine  also 
need  to  be  copied  to  the  replication  of  the  original  virtual  machine  in  order  to  maintain  the 
consistency between these two virtual machines to ensure accuracy of service provisioning. 
8) RTO. RTO refers to Recovery Time Objective (T. Wood, et al., 2010). It is one of the basic 
objectives for designing recovery methods. RTO specifies a time boundary on how long it can 
take for an application to come back online after a failure occurs (T. Wood, et al., 2010). 
9) RPO. RPO refers to Recovery Point Objective (T. Wood, et al., 2010). It is one of the basic 
objectives for designing recovery methods. RPO requires the recovery to recover the system to 
the point in time of the most recent backup prior to any failure (T. Wood, et al., 2010). 
19 
10) MTTR. MTTR is short for Mean Time to Recover. According to existing work on recovery 
benchmarking, MTTR is the biggest and most important part of the benchmarks for evaluating 
recovery performance (J. Zhu, et al., 2002). One rule to follow is that any recovery mechanisms 
or methods need to be designed in such a way that the MTTR of recovery should be minimized. 
To  achieve  the  goal  of  minimizing  MTTR,  several  possible  ways  could  be  taken  into 
consideration,  for  instance,  by  increasing  system  network  throughput  or  by  proposing  proper 
recovery actions (D. Cannon and D. Wheeldon, 2009). 
11)  Recovery  Scalability.  Recovery  scalability  refers  to  a  recovery  method’s  capability  to 
recover for different scales of systems. Cloud systems range from small-scaled systems to large-
scaled systems. Taking Google service as an example, there are thousands of virtual machines 
running in a private cloud managed by Google itself, and the backup machines are dynamically 
and  elastically  determined  and  coordinated  for  fault  tolerance.  For  a  certain  type  of  cloud 
system, the more system scales a recovery method can support, the more scalability the recovery 
method  has.  If  a  cloud  recovery  method  is  not  only  able  to  recover  for  small-scaled  cloud 
systems  but  also  able  to  recover  for  medium-scaled  or  large-scaled  cloud  systems,  then  the 
recovery method has reasonably good recovery scalability. Achieving high recovery scalability 
is necessary for cloud recovery mechanisms. 
12)  Sporadic  Operations.  Sporadic  operations  on  cloud  refer  to  the  maintenance  operations 
performed  on  cloud  applications,  such  as  installation,  upgrade,  reconfiguration,  etc.  They  are 
usually irregularly and less frequently performed by cloud operators and hence they are called 
“sporadic operations” or “sporadic activities” (J. Humble and D. Farley, 2010; L. Bass, I. Weber 
and L. Zhu, 2015). 
13)  Normal  Activities.  Normal  activities  are  also  called  normal  operations  in  the  context  of 
cloud  computing (L.  Bass,  I.  Weber  and  L.  Zhu,  2015). They  refer  to  the  normally  conducted 
activities within cloud applications, such as business transactions inside an e-commerce website 
system  (L.  Bass,  I.  Weber  and  L.  Zhu,  2015;  M.  Fu,  et  al.,  2016).  They  are  the  standard 
operations  of  cloud  applications  and  are  performed  much  more  frequently  and  regularly 
compared to sporadic operations on cloud. Hence, they are called “normal activities” or “normal 
operations” (L. Bass, I. Weber and L. Zhu, 2015). 
2.2  Existing Recovery Methods for Cloud 
In this section, existing cloud recovery methods are discussed.  In our previous work, we have 
made a summarization of those existing cloud recovery strategies (M. Fu, et al., 2014). Here we 
discuss  them  in  more  detail.  Existing  cloud  recovery  strategies  include:  rollback  recovery  in 
cloud, disaster recovery in cloud, virtual machine replication in cloud, fault-tolerance in cloud, 
20 
recovery  for  cloud  internal  protocols,  test  driven  scripts  for  cloud  operations,  exception 
handling  in  cloud  operations,  recovery  for  operations  as  transactions,  cloud  operations  undo 
framework,  user  guided  recovery  for  cloud  web  service  applications  and  BPEL  (Business 
Process Execution Language) recovery in cloud computing. In particular, some of these existing 
cloud  recovery  strategies  had  been  proposed  before  cloud  computing  came  into  practice  and 
hence they were originally designed for non-cloud systems (e.g. traditional distributed systems 
running  in  physical  machines).  These  non-cloud  recovery  strategies  are:  rollback  recovery, 
disaster  recovery,  fault-tolerance  mechanisms,  and  exception  handling.  However,  as  cloud 
applications are just running on a virtualized platform and as the architecture and functionality 
of  cloud  applications  resemble  non-cloud  applications,  these  recovery  methods  originally 
designed for  non-cloud systems are also applicable for the recovery on cloud systems. Hence, 
we put the recovery strategies for non-cloud systems under the context of cloud and discuss how 
they work for cloud.  
2.2.1  Cloud Applications Rollback 
System  recoverability  is  one  aspect  of  system  dependability.  Researchers  at  the  University  of 
California  at  Berkeley  (Berkeley)  proposed  the  Three  R’s  to  system  dependability:  Rewind, 
Repair and Replay (A. B. Brown and D. A. Patterson, 2002). Rollback just corresponds with the 
R of Rewind. The tree view in Fig. 2 (E. N. M. Elnozahy, et al., 2002) shows the categories and 
classifications  of  rollback  mechanisms.  Rollback  treats  a  cloud  distributed  system  as  a 
collection of application processes that communicate through the network (E. N. M. Elnozahy, 
et al., 2002). It can be classified into two categories: checkpointing based rollback and logging 
based rollback (E. N. M. Elnozahy, et al., 2002). Checkpointing based rollback is classified into 
uncoordinated  checkpointing,  coordinated  checkpointing,  and  communication 
induced 
checkpointing.  For  the  coordinated  checkpointing  based  rollback  mechanism,  it can  be further 
divided  into  the  two  categories:  1)  coordination  with  blocking  and  2)  coordination  with  non-
blocking.  For  the  communication  induced  checkpointing  based  rollback  mechanism,  it  can  be 
further  categorised  into  the  model-based  communication  protocol  and  the  index-based 
communication protocol. Unfortunately, uncoordinated checkpointing will probably result in the 
Domino  Effect  (E.  N.  M.  Elnozahy,  et  al.,  2002),  and  hence  coordinated  checkpointing  is 
always  preferred  in  order  to  avoid  the  Domino  Effect.  Particularly,  by  using  communication 
induced  checkpointing  protocols,  it  is  able  to  avoid  the  Domino  Effect  without  requiring  all 
checkpoints  to  be  coordinated  (E.  N.  M.  Elnozahy,  et  al.,  2002).  For  logging  based  rollback 
recovery, there are three types: pessimistic logging, optimistic logging and causal logging (E. N. 
M. Elnozahy, et al., 2002). An issue with logging based rollback recovery is that it might cause 
the  overhead  of  logging  messages.  Nowadays,  rollback  recovery  is  currently  seldom  used  in 
21 
modern  commercial cloud systems  in  practice,  no  matter  whether  it  is  checkpointing  based  or 
logging based (A. B. Brown and D. A. Patterson, 2002; E. N. M. Elnozahy, et al., 2002). 
Fig. 2.  Rollback Recovery Categories. 
For checkpointing based rollback recovery, upon a failure, a system procedure will roll back to 
the  previously  saved  consistent  checkpoint  which  includes  at  a  minimum  the  state  of  the 
participating  system  procedure.  In  contrast,  logging  based  rollback  recovery  combines 
checkpointing with logging of nondeterministic events (E. N. M. Elnozahy, et al., 2002). It logs 
all  the  nondeterministic  events  that  a  system  procedure  executes  as  well  as  the  information 
necessary  for  replaying  each  event.  By  replaying  the  logged  events  when  failure  occurs,  a 
system  procedure  can  be  recovered.  Hence,  logging  based  rollback  recovery  is  particularly 
attractive for applications that frequently interact with the outside world (E. N. M. Elnozahy, et 
al., 2002). As such, both checkpointing based rollback and logging based rollback are intended 
for recovery during the runtime execution of applications running in cloud. The checkpoints as 
well as the event logs are generated during the runtime execution of cloud applications, and the 
recovery actions in case of failures are triggered and executed at runtime as well. The challenges 
behind the checkpointing based rollback recovery strategy are how to deal with the overhead of 
making checkpoints (e.g. making a checkpoint for a large number of virtual machines) and how 
to  increase  the  efficiency  of  retrieving  and  understanding  the  checkpoints  in  order  to  perform 
rollback recovery. The challenges behind the logging based rollback recovery strategy are how 
to generate logs in a more efficient way, how to properly analyse logs from different locations 
within a complex cloud system, and how to correctly understand the information inside the log 
files. 
Limitations:  The  existing  rollback  recovery  mechanisms  rely  on  extensive  checkpoints 
generation and log management provided by the middleware or the application itself to return a 
failed  application  to  a  previous  consistent  state  (F.  A.  Alvi,  et  al.,  2010;  A.  A.  Zienis,  et  al., 
2004;  T.  F.  Arnold,  1973;  D.  P.  Siewiorek  and  R.  S.  Swarz,  1992).  If  we  apply  the  rollback 
22 
mechanisms  on  recovery  for  cloud  applications,  we  find  that  they  face  several  issues  (e.g. 
infeasibility to access some information in certain cloud resources, low efficiency of generating 
checkpoints, inconvenience in analysing distributed logs, etc.) due to the limited visibility and 
indirect control of the cloud platform and the complexity of cloud systems and dependencies on 
cloud  infrastructure  (F.  A.  Alvi,  et  al.,  2010;  A.  A.  Zienis,  et  al.,  2004).  For  instance,  the 
checkpointing  based  rollback  mechanism  on  cloud  needs  to  have  snapshots  for  each  virtual 
machine inside the cloud system periodically generated, and the snapshot generation time can be 
as long as several hours when there are a large number of virtual machines in the cloud system. 
Moreover,  efficiency  problems  will  also  arise  when  the  system  is  brought  back  to  a  previous 
consistent state using a relevant huge snapshot. For another example, the logging based rollback 
mechanism on cloud may need to correctly identify the distributed logs from different locations 
in the cloud system, and it also needs to deal with the noise information (if any) inside the log 
files  which  means  the  log  contents  irrelevant  to  the  data  of  system  states  and  checkpoints. 
Another limitation with the two rollback mechanisms when they are applied on cloud recovery 
is  that  by  rolling  back  the  system  to  a  previous  consistent  state  both  of  the  two  rollback 
mechanisms may lose the additional system changes that are made after the consistent state of 
the  system,  which  means  that  both  of  them  cannot  achieve  the  best  recovery  point  objective 
(RPO). Actually, when cloud operators perform sporadic operations on a cloud application, the 
recovery for the cloud application itself (e.g. rollback mechanisms) is encouraged to be turned 
off  in  order  to  avoid  potential  conflicts  between  the  sporadic  operations  and  the  application 
recovery.  An  example  of  potential  conflicts  is  where  a  sporadic  operation  terminates  a  cloud 
instance  but the application’s  error  detection  service  treats it  as  a failure  and  triggers  rollback 
recovery.  As  cloud  operations  can  be  viewed  as  special  types  of  software  applications 
(especially  when  the  cloud  operations  are implemented  as runnable  scripts), rollback  recovery 
mechanisms  might  be  applicable  for  cloud  operations.  However,  if  we  apply  the  rollback 
mechanisms  on  the  recovery  for  cloud  operations,  they  perform  recovery  in  a  coarse-grained 
manner and they are also faced with the same issues such as poor RPO satisfaction (M. Fu, et al., 
2014; M. Fu, et al., 2016). In fact, the rollback mechanisms are more intended for the recovery 
of  cloud  applications  during  their  normal  activities  and  they  usually  do  not  well  consider  the 
characteristics of a specific operational procedure that consists of a set of steps carried out by 
scripts and humans. 
2.2.2  Disaster Recovery in Cloud Computing 
There are several existing disaster recovery mechanisms for clouds, ranging from geographical 
redundancy  (M.  Pokharel,  et  al.,  2010)  to  cloud  storage  redundancy  (J.  Zhang  and  N.  Zhang, 
2011).  The  approach  of  geographical  redundancy  replicates  data  between  datacentres  some 
23 
distance  apart  (M.  Pokharel,  et  al.,  2010). The approach  of cloud  storage  redundancy  requires 
the cloud storage to be equipped with at least three different  replicas in different  geographical 
locations  (J.  Zhang  and  N.  Zhang,  2011).  Disaster  recovery  is  significant  for  maintaining  the 
business  continuity  of  cloud  systems  or  traditional  distributed  systems  running  in  physical 
machines.  A  typical  disaster  recovery  service  works  by  replicating  application  states  between 
two  datacentres; if  the  primary  datacentre  becomes  unavailable,  the  backup  site  can  take  over 
and will activate a new copy of the application using the most recently replicated data (T. Wood, 
et  al.,  2010).  In  other  words,  this  typical  way  is  to  make  multiple  replications  of  the  same 
system  and  place  them  into  different  geographic  locations  as  copies  of  the  primary  system. 
However,  this  typical  method  has  several  challenges  in  terms  of  double  energy  consumption, 
more infrastructure cost and more human  efforts required (M. Pokharel, et al., 2010). In 2010, 
Manish Pokharel’s work (M. Pokharel, et al., 2010) mentioned a sophisticated disaster recovery 
plan with cloud computing and Fig. 3 below (M. Pokharel, et al., 2010) shows the design of the 
disaster  recovery  plan.  In  this  disaster  recovery  design  framework,  there  are  three  types  of 
components which are the active load balancers, the passive load balancers and the monitoring 
units, and they serve as the three main functional parts of the disaster recovery plan framework. 
The  main  benefit  of  this  disaster  recovery  framework  is  that  it  is  able  to  achieve  higher 
availability, higher survivability, lower unavailability and lower downtime with much less cost 
(M. Pokharel, et al., 2010). 
Fig. 3.  Disaster Recovery with Cloud Computing. 
Currently, there are some existing tools which can support disaster recovery within clouds, such 
as  Zerto  (L.  DuBois,  2013)  and  Yuruware  (Yuruware,  2016).  For  example,  Zerto  enables 
replication  across  a  heterogeneous  range  of  storage  devices  and  protocols  for  virtual  IT 
environments  based  on  a  hypervisor-level  replication  solution  (L.  DuBois,  2013).  And 
Yuruware  enables  cross-region  replication  for  cloud  applications  (Yuruware,  2016).  Disaster 
recovery involves two phases: one is the disaster recovery design phase where disaster recovery 
24 
architecture and a disaster recovery plan are generated, and the other one is the disaster recovery 
realization  phase,  where  the  disaster  is  mitigated  according  to  the  disaster  recovery  plan  (T. 
Wood, et al., 2010).  
Limitations: Disaster recovery on cloud is designed for blank recovery from typical node/zone-
level failures. It usually deals with the infrastructure level of cloud resources and systems and it 
is  more  intended  for  normal  activities  on  cloud  (M.  Fu,  et  al.,  2014).  One  concern  about  the 
disaster recovery mechanism is the efficiency of replicating  the datacentre or the storage from 
one  platform  to  another,  especially  for  a  cross-region  replication  (M.  Pokharel,  et  al.,  2010). 
When applying the disaster recovery mechanism on the recovery for cloud applications during 
sporadic  operations,  it  does  not  consider  the  intentional  changes  which  are  made  during  the 
running of sporadic operations or the specific errors caused by them. Actually, when there is no 
sporadic operation performed on cloud applications, the same problem will also happen to the 
disaster  recovery  mechanism.  Hence,  another  concern  about  the  disaster  recovery  strategy  on 
cloud is how to retrieve the changed states and data in the previous datacentre or storage (master 
site) and move them into the backup site (slave site) (M. Fu, et al., 2014). 
2.2.3  Virtual Machine Replication in Cloud 
Virtual  machine  (VM)  replication  is  needed  for  cloud  applications  because  of  the  uncertainty 
and  instability  of  cloud  resources.  The  virtual  machine  replication  mechanism  is  also  widely 
used in disaster recovery and fault tolerance (A. Colesa, et al., 2010). One way for cloud virtual 
machines  replication  is  by  using  the  cloud  middleware,  especially  for  data  storage  virtual 
machines (P. Efstathopoulos, 2012). Specifically, asynchronous virtual machine replication (B. 
Cully, et al., 2008) is recommended to be used for the planning and designing of cloud recovery 
mechanisms. Virtual machine replication in cloud requires that the systems be constructed with 
redundant virtual machines that are capable of maintaining and switching to backups in the face 
of  failure  (A.  Colesa,  et  al.,  2010).  Replicating  virtual  machines  is  encouraged  to  be 
asynchronously conducted because this will have fewer interruptions on the cloud applications 
themselves.  For  stateful  virtual  machines,  it  is  also  required  that  only  the  changed  states  and 
incremental  data  should  be  replicated  to  the  virtual  machine  replicas.  One  example  of 
asynchronous VM replication strategy which  fulfils these requirements is Remus (B. Cully, et 
al.,  2008),  which  encapsulates  protected  software  in  VM  and  asynchronously  propagates 
changed state to a backup host at high frequencies. Below Fig. 4 shows the basic stages of the 
operation in Remus. Remus runs paired servers in an active-passive configuration. Firstly, this 
system is based on a virtualized infrastructure to facilitate whole-system replication. Secondly, 
system  performance  is  increased  through  speculative  execution,  which  decouples  external 
25 
output from synchronization points. This allows the primary server to remain productive, while 
synchronization with the replicated server is performed asynchronously (B. Cully, et al., 2008). 
Fig. 4.  Asynchronous Replication in Remus. 
Fig. 5 below shows the high-level architecture of Remus. In this architecture, it can be seen that 
the machines are encapsulated to be protected within the VMs. The implementation is based on 
the Xen virtual machine monitor, and it extends Xen's support for live migration to provide the 
fine-grained mechanism of making checkpoints (B. Cully, et al., 2008). 
Fig. 5.  Remus High-level Architecture. 
Remus  achieves  high  availability  by  propagating  frequent  checkpoints  of  an  active  virtual 
machine to a backup physical host. On the backup, the virtual machine image resides in memory 
and  may  begin  execution  immediately  if  failure  of  the  active  system  is  detected.  Because  the 
backup host is only periodically consistent with the primary host, all network outputs must be 
buffered until the state is synchronized on the backup host. When a complete consistent image 
of the host has been received, this buffer is released to external clients. The checkpoint making 
as well as the buffering and cycle releasing happen very frequently – the frequency is up to forty 
26 
times  per  second,  meaning  that  a  whole-machine  checkpoint  including  network  and  on-disk 
state is created every 25 milliseconds (B. Cully, et al., 2008). 
The technology behind Remus has been used in some of today’s commodity cloud applications, 
such as some e-business websites. The virtual machine replication strategy can be  designed in 
cloud systems design phase and its effect takes place at cloud systems runtime phase (M. Fu, et 
al., 2014). The challenges in asynchronous virtual machine replication lie in the determination 
of the replication frequency and the avoidance of influencing the servicing cloud system. 
Limitations:  One  problem  with  the  virtual  machine  replication  mechanism  in  cloud  is  that  it 
cannot  reduce  the  disturbance  to  the  running  cloud  applications  inside  the  virtual  machines 
during  recovery.  When  replicating  a  virtual  machine,  part  of  its  virtual  CPU  resources  and 
virtual  memory  resources  have  to  be  dedicated  to  interacting  with  the  replicating  service,  and 
hence  the  virtual  CPU  and  virtual  memory  dedicated  to  the  applications  themselves  might  be 
affected.  This  is  how  the  running  cloud  applications  inside  the  virtual  machines  are  disturbed 
during the recovery. Sometimes, the virtual machine replication mechanism also requires certain 
amount  of  manual  work,  such  as  manually  configuring  virtual  machine  replication  parameters 
(e.g.  source  virtual  machines  to  replicate,  replication  frequency,  replication  destinations  and 
desired time of replication, etc.). If the operator specifies incorrect values for some parameters, 
the virtual machine replication procedure will not be able to be executed according to its desired 
settings.  Hence,  manual  work  within  the  virtual  machine  replication  mechanism  could 
sometimes  be  dangerous.  In  addition,  as  the  cloud  system  scale  grows  bigger  and  bigger,  the 
MTTR (Mean Time to Recover) of the recovery mechanism will increase and data changes after 
the last round of replication will become even more complex. Hence, it will be more and more 
difficult for the virtual machine replication mechanism to  fulfil system recovery  goals such as 
RTO (Recovery Time Objective) and RPO (Recovery Point Objective). 
2.2.4  Fault-Tolerance in Cloud Computing 
Fault-tolerance of cloud services has received great attention for many years, although  there is 
still  a  strong  research  focus  on  how  to  carry  on  fault-tolerance  more  efficiently  for  different 
types of cloud services. The idea behind fault-tolerance is to mask faults occurring in a system 
instead of removing or fixing them. There are three existing fault-tolerance strategies: Recovery 
Block (B. Randell, et al., 1995), N-version Programming (A. Avizienis, 1995) and Parallel (Z. B. 
Zheng,  et  al.,  2010).  Recovery  block  is  a  means  of  structuring  redundant  program  modules 
where standby  components  will  be invoked  if the  primary  component fails  (B. Randell, et  al., 