According to our classiﬁcation, around 500 ASes are deﬁnitely ﬁl-
tering our newly allocated preﬁx, but nearly 2000 are potentially
ﬁltering. This means that between 2% and 7% of the whole In-
ternet cannot see our newly allocated addresses. This is a serious
problem! Moreover, the problem hardly changes between our sec-
ond and third experiment.
To gauge the extent of upstream provider ﬁltering, we considered
the location of these reachability problems. Recall, that we cannot
rigorously determine where probes are dropped, as we do not see
the reverse paths4. Instead, we studied the nature of the ASes in
our list. If a target AS appeared as an intermediate node in any of
our traceroute measurements we classify it as a transit AS. If not,
then we call it an end-point, or stub-AS. Figure 8 shows the per-
centage of each category in the ASes with reachability problems.
We can see that the vast majority are stub ASes. This suggests that
most problems occur at the edge. This is an intuitively appealing
conclusion because it is natural that transit providers – who should
have more experience with BGP – are less likely to leave stale ﬁl-
ters in their network. The small number (a few hundreds) of transit
providers who incorrectly conﬁgure ﬁlters increases the likelihood
that we incorrectly identify an edge node as ﬁltering when it is not.
As Figure 7 shows, bogon ﬁlters seem not to be removed quickly.
Our experiments over the course of April to June 2008 showed very
small changes despite the fact that in mid-April a reminder to oper-
ators was sent to remove ﬁlters for this address space. This agrees
with the operator community perception, and was the reason this
experiment was originally commissioned.
Feedback from network administrators.
The ﬁrst campaign identiﬁed a list of ASes that had no connec-
tivity to our address space. We used the Internet Routing Registries
(IRRs) to ﬁnd e-mail contacts for these ASes. We contacted around
75 operators manually via e-mail, asking them if they were ﬁlter-
4Relying on a tool such as [2] may partly solve this problem.
Classiﬁcation
Filtering transit AS
Non-ﬁltering transit AS
Filtering stub AS
Non-ﬁltering transit AS
Percentage of ASes
6%
8%
21%
65%
Figure 8: Percentage of ASes with reachability problems based
on transit/non-transit classiﬁcation.
ing the newly allocated address space. We only received 17 replies.
The majority, ten ISPs, conﬁrmed that they had out-dated bogon
ﬁlters. Two told us it was their upstream that was ﬁltering. We
see those as encouraging responses, as our methodology is about
ﬁnding places with no reachability, not about “blaming” operators.
This shows again, how careful we have to be in using the method-
ology, as our methodology can detect regions where limited con-
nectivity exists, but should not be used for ﬁnger-pointing the cul-
prit. Therefore, implementing such a service within the registries
has to be considered with care, as for example ASes with default
routes pointing to their upstream are affected by the ﬁlters of their
upstream.
In addition, we got three very confused responses from operators
asking us what we are talking about. This lack of understanding
of the issue suggested that these ASes were probably operated by
people who do not understand how to maintain their ﬁlters, and
although the responses neither conﬁrmed nor denied the existence
of bogon-ﬁlters, they certainly left us suspecting the ﬁlters were
there.
We received only two replies saying they had no such ﬁlters, and
one of those was the result of a IRR lookup error (in this case we
had not contacted the ISP itself but rather, the APNIC helpdesk).
The single meaningful negative reply did not mention whether or
not connectivity existed, so they might also fall in the category of
an AS whose upstream was incorrectly ﬁltering the preﬁx.
6. IMPACT OF METHODOLOGICAL ISSUES
ON MEASUREMENT CONFIDENCE
So far we have shown, how observations from the control plane
suffer from “visibility issues”, and that the data plane can offer a
different perspective. Obviously, data-plane observations have their
own limitations. In this section we discuss three methodological
issues that have to be understood to ensure the success of an exper-
iment aimed at making conclusions about reachability from active
probing: the topological coverage of the probes (Section 6.1), the
mapping from IP address to AS number (Section 6.2), and ﬁnally
the type of probes used (Section 6.3).
6.1 Topological coverage
The motivation for out-probes (see Section 5.1) is to “look” into
those areas of the Internet where no BGP monitors or looking glasses
exist, i.e. mainly the edge. Unfortunately, this part of the Internet is
large, and changing quickly [6]. Our visibility of the edge is not as
good as in the core. The idea is to measure reachability by sending
outbound probes to “the edge” of the Internet and draw conclusions
based on the responses. We have already discussed in Section 5.1
the need to calibrate our expectations, to be able to draw conclu-
sions. While the calibration will tell us whether or not to expect an
answer in a particular case, we need a list of IP addresses that we
can expect to answer our probes.
Several requirements for the IP address list need to be consid-
ered, and some of them conﬂict with each other. First we would
like to have a wide coverage, e.g., reach as many ASes in the Inter-
249net as possible. Second, we would like to probe inside ASes using
a ﬁne granularity to see non-homogeneously conﬁgured patterns.
Third, we also like to limit the number of probes that we have to
send and the time it takes to probe all IP addresses.
In this section we discuss the properties of the list of pingable
IP addresses we used. ASes are often not homogeneously conﬁg-
ured [10], a per AS-granularity might not be sufﬁcient. However,
the number of ASes is in the order of tens-of-thousands, while the
number of preﬁxes is an order of magnitude larger. At the edge
of the Internet, it is particularly difﬁcult to ﬁnd enough pingable
IPs. Besides the problem of the probing granularity, we also need
a large enough number of measurements to build conﬁdence in the
results5.
Note that as our goal is not to do topology discovery per se, we
did not try to achieve a proper coverage of the router-level topol-
ogy as done by topology discovery projects [11–14], and neither
do we seek the level of detailed coverage of [15], as we are not
concerned with the behavior of the end-hosts (which after all may
change minute by minute), simply the reachability of the end sys-
tems to which they connect.
6.1.1 Finding pingable IP addresses
Obtaining a large number of pingable IP addresses is one of the
important issues for large-scale topology discovery [11–14,16,17].
Many projects use existing lists of IP addresses such as the one
from CAIDA [11]. This list is based on IP addresses observed from
passive measurements: packet capture, DNS requests observed at
root servers, and Web servers logs. However, we also used active
discovery techniques [18], as well as brute-force scanning for a
very small portion of the address space where required. Together
with sharing and merging our IPs lists with that of other researchers
we obtained a pool of 4,655,238 IP addresses in total.
In general we have to decide what granularity to aim for: router-
level or AS-level. Depending on the application of the probes, this
might vary. The router-level topology is more detailed, but harder
to obtain with any certainty. Moreover, for bogon-ﬁlter discovery
the aim might be to contact network operators to remove ﬁlters. In
such cases, we just have to assure to have high enough chance to
discover non-homogeneous ﬁlters, but contacting the ISP will be
done on a AS-level granularity.
6.1.2 Coverage at the AS-level
We selected a subset of 306,780 IP addresses that we use for our
measurements, from our pool of IP addresses. The resulting cov-
erage was of 154,683 pingable preﬁxes in 25, 780 pingable ASes.
We selected those IP addresses based on the following objectives:
(cid:129) Probe as many ASes as possible.
(cid:129) Aim ideally at 30 pingable IP addresses per AS, unless there
is a reason to believe that a ﬁner granularity within that AS
is required. This number of 30 IP addresses is an arbitrary
threshold: it should be large enough to allow some estima-
tions about the required granularity, while reducing the num-
ber of probes needed. Note that if far more than 30 pingable
IP addresses are available inside an AS, we will limit their
number to 30 to prevent spending effort on ASes where we
can too easily ﬁnd pingable IP addresses.
5There are several reasons why we want to have multiple measure-
ments within an AS/preﬁx: For example to compensate for mea-
surement errors, such as packet loss. To deal with mapping errors,
such as a customer administrated router which is provisioned on
one interface with provider IP address space. For us this router
would appear as belonging to the provider administration.
)
F
D
C
l
(
x
<
s
e
b
a
g
n
p
#
i
t
a
h
t
y
t
i
l
i
b
a
b
o
r
P
stub
small ISP
large ISP
0
1
.
8
.
0
6
0
.
4
.
0
2
0
.
0
0
.
0
100
200
Number of IPs per AS
300
Figure 9: Total number of pingable IP addresses per AS: stubs
(black, top curve), small ISPs (green, middle curve), and large
ISPs (blue, lower curve). ASes with more than 300 IPs fall in
the last value.
(cid:129) Aim at covering as many diverse preﬁxes within an AS as
possible, e.g., take those 30 pingable IP addresses from as
many preﬁxes as available inside that AS.
(cid:129) If all preﬁxes are covered, and still our threshold of 30 is
not reached, then improve conﬁdence, by obtaining several
pingable IP addresses within the considered preﬁx.
(cid:129) Minimize the number of probes sent.
We wanted to keep both the number of probes sent as well as the
time necessary to run a probing campaign reasonably low. There-
fore, we chose to limit the number of IP addresses probed. As the
underlying Internet topology may change while we are probing,
taking more time to run our probing may lower the quality of our
results. Even recent probing tools, such as Paris-traceroute [19],
are too slow to probe a very large list of IP addresses in a reason-
able amount of time. On the other hand, probing too fast may also
not be desirable, as many routers are known to be conﬁgured to
rate-limit the number of ICMP packets [20]. A slower probing rate
might also be required to avoid many packet drops or having the
probing host black-listed.
Figure 9 shows a CDF the number of IP addresses that we have
on a per AS basis. The x-axis shows the number of pingable IP
addresses we have per AS, and on the y-axis we plot the fraction
of ASes for which we have less than x pingable IP addresses. The
solid black curve shows stub ASes, the green dots show small ISPs
and the blue dots show large ISPs (see [5] for the classiﬁcation). We
also show as a grey line the value of 30 pingable IP addresses. Find-
ing pingable IP addresses at the edge in each AS is difﬁcult. 86.6%
of stub ASes (20,980 out of 24,224) do not reach this threshold.
Actually, for 31.3% (7,589) of the stub ASes we have only one
pingable IP address. For small and large ISPs on the other hand,
traceroutes easily sample many IPs within the network core. Most
ASes do reach the threshold of 30 pingable IP addresses: 63.7% of
small ISPs (833 out of 1307) and 82.1% of large ISPs (202 out of
246).
6.1.3
Stability of pingable IP addresses
Depending on the quality of our list of pingable IP addresses, dif-
ferent regions of the Internet may not be covered as well as others,
or too few pingable IP addresses will render the results inconclu-
sive. Once an initial list is built, we must maintain its coverage
over time. IP addresses may belong to hosts whose connectivity
250to the Internet changes over time, or may be dynamically allocated
to different hosts. For example, consider “dial-up” IPs. They may
respond at some time, and not a few hours later. Even though we
calibrate our expectations while we run our tests, we still rely to
some degree on the expectation that IP addresses are stable and give
predictable responses. We have to continuously monitor which IP
addresses are pingable and drop from our list IP addresses that are
chronically inaccessible. We have to detect when our coverage of
an AS becomes low, and then we have to add IP addresses to the
list to compensate for this inadequate coverage.
For a methodology that seeks a given coverage of the Internet
to be successful, it is important to have a good understanding of
the quality of the list of pingable IP addresses on which it relies.
Ideally, this should be a service offered from the route registries,
where operators could register IPs that are responsive to pings and
could be used to determine reachability. In this section we study the
changes in the availability of our IP addresses over time, as well as
the sensitivity of results to different probing locations.
We found that our list of pingable IP addresses is fairly stable.
From April until June 2008 we evaluated the stability of our IP ad-
dress list. We observed that 95.8% of all IPs that were pingable
in April remained pingable during the following two month. How-
ever, 2.2% of those IP addresses did not respond to our pings in
those two months. CAIDA [11] reports a decay rate of their list of
active IP addresses of about 2 to 3% per month.
As usual when working with measurement data, a certain frac-
tion of IP addresses behave strangely. In our case these may be ar-
tifacts induced by the availability of end-hosts, which might some-
times be up, sometimes not (e.g., dial-ups). Another cause might
be ICMP related measurement problems due to packet drops (see
Section 6.3 for more details). A recent study [20] found that routers
tend to increasingly drop direct probes. Indirect probes on the other
hand (e.g. traceroutes) do not seem to be concerned by this trend.
Some routers, often at the edge, react to probing with ICMP rate-
limiting techniques. Overloaded links also may also cause probe
packets to be dropped more frequently than other packets. Further-
more, some hosts respond very slowly to probes which create time-
out issues for the probing tools. If we sum up all those artifacts, we
estimate our error roughly between 2% and 5%.
6.1.4 Consistency across probing locations
When assessing reachability, the location from which the address
space is advertised (the test-box) may yield different results. Take
for example the “Rocketfuel” work [9], aimed at discovering the
internal topology of ISPs. To achieve this, the authors probed from
different locations towards different IPs. Non-homogeneously con-
ﬁgured routers or packet ﬁlters only conﬁgured on certain links may
impact the results of our probing. Depending on how our probes
enter a given AS and traverse it, we may or may not sample bo-
gon ﬁltering routers or show differences in our default-route exper-
iment. It is therefore reasonable to assume that differences between
locations may exist.
However, the results of our bogon-experiments (section 5.2) do