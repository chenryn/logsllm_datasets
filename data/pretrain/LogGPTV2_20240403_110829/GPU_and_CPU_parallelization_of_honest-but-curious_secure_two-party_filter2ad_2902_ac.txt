when both are given inputs that require the same number of rounds
of Merkle-Damgard.Our implementation of SHA1 is a hand opti-
mized version of the John the Ripper implementation of SHA1.6 In
particular, we are able to reduce the number of rounds we typically
need to calculate in SHA1 by judiciously choosing the values we
hash. As others have done, we ensure that we never need to hash
more than one block of data. However, by ensuring that the preﬁx
of the values we hash for a given circuit remain relatively constant,
we can pre-compute the ﬁrst few rounds of SHA1 for each query
in generating or evaluating a circuit. This allows us to knock off
either 6 or 14 rounds (out of the 80 rounds) of each SHA1 call. We
also note that to allow an SM to be able to “hyper-thread”, we need
to be miserly with our use of registers in this code. Proﬁling clearly
shows that hand optimization of the SHA1 code is a worthwhile en-
deavor.
7.2 GPUs and Free-XOR
One main challenge in this work is to develop a version of the
Free-XOR technique that is compatible with parallelization. We
begin by describing the technique.
The Free-XOR technique [13] allows XOR gates in the circuit
to be evaluated using only a bit-wise XOR operation instead of
the standard garbled-gate evaluation. In this approach, the circuit
generator chooses a global random offset R, and then ensures that
the keys for every wire i in the circuit satisfy W 0
i = R.
This is usually done by choosing keys for each wire i in the circuit
that is not the output of an XOR gate by sampling W 0
i at random
i ⊕ R. For k an output wire
(as before) but then setting W 1
of an XOR gate with input wires i, j, the evaluator sets W 0
k =
k ⊕ R. Note that if the circuit evaluator
i ⊕ W 0
W 0
holds input-wire keys Wi, Wj associated with the input wires to an
XOR gate, he can compute the corresponding key for the output
wire k of that gate as Wk = Wi ⊕ Wj. Thus, no garbled tables
need to be prepared or sent for such gates.
i ⊕ W 1
j and W 1
k = W 0
i = W 0
We modify this technique to permit efﬁcient parallel gate gener-
ation. In particular, note that when the circuit alternates between
non-XOR gates and XOR gates in the ‘encrypted’ circuit, this cre-
ates a dependency amongst wire-labels that would require gates to
be generated in leveled order. This is because the output wire labels
from the ﬁrst XOR gates on level i− 1 need to be computed for use
as the input wires on level i. This creates a dependency regardless
of whether or not the gates on level i are XOR or truth tables.
Our modiﬁcation operates by ﬁrst virtually generating the labels
(Sec. 7.3) for all wires in the circuit, even if the wire is the output
6John the Ripper is a brute force password hashing software suite.
wire from a XOR gate. This differs from the original Free-XOR
technique which does not randomly generate labels for XOR gate
output wires. Then, after wire label generation, during the gener-
ation of XOR gate i, we calculate a label offset (Vi) and a p-bit
offset (Pi) that is unique for XOR gate i in the circuit. In the orig-
inal Free-XOR technique it is at this point where an XOR gate’s
output wire labels would be generated. Instead, we make use of Vi
and Pi to modify our XOR gate to our previously generated wire
label and p-bit. The V and P values for every XOR gate can be
calculated in parallel on the GPU. Our scheme adds two bitwise
XOR operations to XOR gate generation, but this increased over-
head is minuscule compared to locating every XOR gate chain and
then serially computing each gate in the chain. The calculation of
our V and P values during generation are as follows:
1. For each XOR-Gate Gi with wires Wc = XOR(Wa, Wb)
b(cid:8)],
b , p1
where Wa = [(cid:7)k0
a(cid:8),(cid:7)k1
a(cid:8)], Wb = [(cid:7)k0
a, p0
Wc = [(cid:7)k0
c(cid:8),(cid:7)k1
c(cid:8)], and tuple [Vc, Pc]:
c , p1
(cid:4)
(cid:4)
(a) Set value Vc = k0
(cid:4)
(cid:4)
a
(b) Set value Pc = p0
a
k0
c for wire WC
p0
c for wire WC
b(cid:8),(cid:7)k1
b , p0
a, p1
c , p0
k0
b
p0
b
The use of V and P during evaluation are as follows:
1. For each XOR-Gate Gi with wires Wc = XOR(Wa, Wb)
where Wa = (cid:7)ka, pa(cid:8), Wb = (cid:7)kb, pb(cid:8), and tuple [Vc, Pc]:
(a) Compute garbled output value WC = (cid:7)kc, pc(cid:8) which
(cid:4)
(cid:4)
(cid:4)
(cid:4)
is equal to (cid:7)ka
kb
Vc, pa
pb
Pc(cid:8)
Communication Costs:As discussed, this modiﬁcation increases
the communication costs by adding an extra ‘key’ for each XOR
gate in exchange for signiﬁcant parallelized speed improvements.
This modiﬁcation is also incompatible with the Garbled Row Re-
duction (GRR) optimization [10], which performs more computa-
tion in exchange for less communication because GRR also induces
a dependence on a circuit’s wire labels.
7.3 GPU Wire Creation and Gate Generation
Our system implements garbled circuit generation on the GPU
in parallel. We ﬁrst note a method of virtually generating all label
b(cid:8) for every wire in the
pairs and permutation bits (cid:7)k0
circuit. Because of the memory hierarchy, one does not wish to
generate all of the keys associated with labels initially, as the costs
of moving and storing these keys in memory is substantial. Instead,
we use wire indexes from the circuit description (which are much
smaller than cryptographic keys), as inputs to a pseudo-random
function generator (PRFG), which outputs the labels for a given
a = Fs(a) ⊕ R
wire. Speciﬁcally, we output k0
for a PRFG F with a circuit speciﬁc seed s and the global Free-
XOR offset R. The permutation bits are handled similarly. Note
that wire-labels are not actually precomputed, but rather only vir-
tually assigned, and computed when constructing the gates that are
attached to a given label, for implementation optimization reasons.
7.4 GPU Evaluation
a = Fs(a) and k1
a(cid:8),(cid:7)k1
b , p1
a, p0
The entire circuit cannot be evaluated in parallel on the GPU.
The gates in the circuit must be topologically sorted, and then eval-
uated. That is, the circuit must be broken into smaller sub circuits
such that each subcircuit S has a start gate level Gs and end gate
level Ge.
Our API starts GPU evaluation by iteratively transferring sev-
eral consecutive levels of truth tables and XOR gates to the GPU’s
memory, and then evaluating them. The next grouping of levels
can be asynchronously transferred to the GPU while the previous
175
grouping of levels is being evaluated. At each level a separate ker-
nel must be used to evaluate XOR and truth table gates.
CPU Evaluation.
Besides GPU Evaluation of garbled circuits we also implemented
system to evaluate garbled circuits on the host both serially and in
parallel. Our parallel CPU implementation makes use of OpenMP
threads (cf. Sec. 5). The CPU parallel and serial evaluation work
using the similar process as GPU evaluation, but importantly CPU
evaluation does not need to perform any data transfers. The CPU
evaluation algorithm iterates over each circuit level in the same
manner as GPU evaluation. Serial CPU evaluation will iterate seri-
ally over each gate in a level and evaluate it. The parallel CPU eval-
uation algorithm iterates over gates in a level in parallel, where they
are divided up equally among all available threads on the machine.
Thus if a level contains N gates and the machine has t threads, each
t gates. No balancing is done to try and ensure
thread will process N
a consistent mix of XOR and truth table gates, as the overheard was
deemed higher than the beneﬁt. Given CPUs are MIMD processors
there is no need to worry about divergent branches, thus parallel
evaluation on the CPU is a more straightforward process.
8. RESULTS
In this section, we present several experiments that support the
main claims of this paper and give data on GPU circuit genera-
tion, GPU evaluation, and multi-core CPU evaluation. Given that
there are now several implementations in different security mod-
els, such as HbC, 1BM, and malicious security, it is clear a critical
metric to the performance of these systems is the number of gates
one can generate and evaluate per core per second. In the HBC
and 1BM security model, gate generation and evaluation are key,
as Huang et al.[9] also note. Frederiksen and Nielsen suggest that
communication is the fundamental limiting factor in the malicious
model, but we recall that there are are now several communications
improvements that will help to alleviate communication overhead,
as discussed in Section 6, which suggest that these metrics should
still be of some concern in the malicious model. We do not ad-
dress communication or its latency here, as it is not in scope of our
investigation on the use of different parallelizing technologies to
implement efﬁcient and practical circuit generation and evaluation.
We discuss this in the Future Work section.
We show through experiment that circuit generation in the HbC
and 1BM security models can dramatically beneﬁt from a combina-
tion of the ﬁne-grained parallelization that has not been exploited in
prior works and our modiﬁcation of the Free-XOR technique. Fur-
ther, it can easily be accommodated on SIMD-style architectures
such as GPUs. This applies to creating individual circuits, and can
be carried forward to many duplicates for cut-and-choose scenar-
ios, although the extra communications costs, and the availability
of other parallelization techniques in that model may make our ap-
proach for that security model less feasible: more experiments need
to be performed.
Finally, we show that circuit evaluation is more difﬁcult to paral-
lelize for individual circuits, but can perform better in the cut-and-
choose scenario of malicious security.
8.1 Explanation of our Experiments and Data
Producing fair comparisons between different garbled circuit sys-
tems is currently challenging. In a perfect world we would execute
all systems on the same machine using the same circuit descriptor
ﬁles, and provide results. Unfortunately, we do not have access to
all of the systems and circuit description ﬁles necessary to do this.
We were able to get the Frederiksen and Nielsen [5] system work-
ing on two of our GPU systems for a direct comparison of our per-
formance to theirs. We do not have interchangeable circuit formats,
but we can provide comparisons on a per gate basis. Interestingly,
their system performs better on an older architectural generation of
GPU card then it was designed to function, so we compare their
best performance and ours on both generations of cards. Further,
since their implementation is in the malicious model, we cannot
simply compare execution times of their many cut-and-choose gen-
erations and evaluations with a single generation or evaluation on
our system. We took different AES circuits and “copy and pasted”
multiple independent copies into one ﬁle to simulate the workload
needed to generate or evaluate many copies of the circuit in the
cut-and-choose protocol, and then compare on a per gate basis.
In the case of Kreuter at al. [15], we have recently been able to
support generating and evaluating circuits form their most recent
compiler [14], allowing for comparison of generation between the
two systems on identical circuits, but we have not yet been able
to fully integrate their system with our pipelining code, so we can
only compile those circuits for which the entire ﬁnal circuit ﬁts on
the GPU at one time. Larger circuits that need to be broken up and
pipelined onto the GPU cannot yet be directly compared. For this
reason, the comparisons of our system halt in the experiments when
circuit sizes reach the maximal that will ﬁt on the graphics cards.
We note that of our two systems, one system’s card has a newer
architecture than the other, and so can support slightly larger cir-
cuits. Unfortunately, we did not have access to a version of Kreuter
et al.’s system that would work on a non-clustered machine, so we
could not provide bare-metal side by side comparisons. Therefore,
in the case of Kreuter et al. [14], we take the results from their pa-
per and compare them with the same circuits on our machines. All
reported results from our experiments are the average of 100 runs.
Experiments from Kreuter at al. [14] report average results from
50 runs.
Most prior work in the area benchmarks the time it takes to gen-
erate and evaluate various circuits. This process indirectly bench-
marks the number of gates generated or evaluated per second. How-
ever, this is often run on systems with varying numbers of cores,
and to a lesser extent varying speeds. We report results on the av-
erage number of gates generated or evaluated per second per core.
We note this metric seems relatively stable, and thus we use it for a
near apples-to-apples comparison. Table 1 has details for the com-
parison systems. We note that even though EC2 has multiple GPUs,
only one is used in the results presented.7 EC2 is run on Amazon’s
elastic compute infrastructure, and is running under a Xen hypervi-
sor. Since we do not have direct access to the bare metal, we cannot
determine how much overhead the Xen hypervisor entails, but Xen
project benchmarks suggest, assuming appropriate kernel patches
have been applied, a 0-30% performance decrease [2].
8.2 GPU Circuit Generation
We ran circuit generation on the EC2 and Tie systems (cf. Ta-
ble 1). We ﬁrst compare our results to those of Frederiksen and
Nielsen [5] in Fig. 1a. We remind the reader that we compare their
circuit generation times from experiments where they have similar,
but not identical circuits, due to the need to simulate the cut-and-
choose malicious protocol, and further, while we did have access to
their circuit ﬁle, we could not execute it directly as we do not sup-
port their ﬁle description language in our system, and their binary
ﬁle format was not conducive to easy translation. Thus, we show
in Fig. 1a that under similar workloads our scheme outperforms
theirs on the same hardware using the metric of gates generated per
7We discuss multiple GPUs in Sec. 9 with respect to future work
176
System
CPU
Kreuter et al. Xenon
E5506
[15]
Xenon
EC2
X5570
Xenon
E5-2620
Tie
Core/ GHz Ram GPU
Thrd.
4
(GB)
8
2.13
N/A
8/16
2.93
12/12
2
24
64
Tesla
S2050
Tesla
K20
GPU
Cores
S2050 (EC2)
K20 (Tie)
448
2496
SMs GHz Memory Compute
Capability
2.0
3.5
(GB)
2.7
4.8
1.15
0.71
14
13
Table 1: Benchmark system descriptions. EC2 runs a Xen virtual
machine.
second. Observe that we generate gates at about 2.3 times the rate
on the Tie system compared to Frederiksen and Nielsen on the EC2
system. Observe that we generate gates at about 3 times the rate on
the Tie system compared to Frederiksen and Nielsen. This is the
benchmark system, as Frederiksen’ and Nielsen’s code is targeted
at compute capability 3.X CUDA cards.
As the number of cores on systems can be highly variable, in
Fig. 1b we calculate the average rate of gate generation per core
for the two systems, to help with understanding performance on
other GPU cards with varying numbers of cores. Note that in the
benchmarks reported in Figs. 1a and 1b we have commented out
any code in our system necessary to split large circuits into smaller
sub-circuits so that they can ﬁt onto the GPU, as Frederiksen and
Nielsen have no such corresponding code as they simply assume
the circuit will ﬁt. Thus we are not penalized for computing over-
head that the other system also does not compute.
AES Comparison (No Pipelining)
AES Comparison (No Pipelining)
0
0
0
0
0
0
0
7
0
0
0
0
0
0
0
5
0
0
0
0
0
0
0
3
0
0
0