### Evaluation of Machine-Generated Reviews

In this section, we assess the quality of machine-generated reviews along two primary dimensions. First, we investigate whether these generated reviews can evade detection by existing algorithmic methods. Second, we conduct an end-to-end user study to determine if human judges can distinguish between generated and real reviews.

#### 4.1 Detection by Existing Algorithms

We focus on two popular algorithmic techniques to differentiate machine-generated reviews from real ones:
1. A supervised machine learning (ML) scheme based on linguistic features.
2. A plagiarism detector to check for duplications between machine-generated and real reviews.

**ML-based Review Filter**

Using machine learning classifiers to detect fake reviews is a well-studied problem [20, 48, 53]. Most previous work relies on the observation that characteristics of fake reviews deviate from real reviews across various linguistic dimensions. We identified five groups of linguistic features, totaling 77 features, which have previously demonstrated strong discriminatory power in distinguishing fake and real reviews. These features are described below:

- **Similarity Feature (1):** Captures inter-sentence similarity within a review at the word level. It is computed as the maximum cosine similarity between unigram features among all pairs of sentences [10, 20, 31, 56].
- **Structural Features (4):** Captures the structural aspects of a review, including the number of words, the number of sentences, the average sentence length (in words), and the average word length (in characters) [20, 56].
- **Syntactic Features (6):** Captures the linguistic properties of the review based on parts-of-speech (POS) tagging. Features include the percentages of nouns, verbs, adjectives, adverbs, first-person pronouns, and second-person pronouns [20, 31, 56].
- **Semantic Features (4):** Captures the subjectivity and sentiment of the reviews. Features include the percentage of subjective words, objective words, positive words, and negative words, as defined in SentiWordNet [3], a popular lexical resource for opinion mining [31, 53, 56].
- **LIWC Features (62):** The Linguistic Inquiry and Word Count (LIWC) software [52] categorizes approximately 4,500 keywords into 68 psychological classes (e.g., linguistic processes, psychological processes, personal concerns, and spoken categories). We use the percentage of word count in each class as a feature, excluding those already included in the previous groups [48, 49].

We train a linear SVM classifier on the Yelp ground-truth dataset, which includes both real reviews (Yelp unfiltered reviews) and fake reviews (Yelp filtered reviews). After training with all 77 linguistic features, we test the performance of the classifier on the Yelp attack dataset, composed of real reviews and machine-generated reviews. We perform 10-fold cross-validation and report the average performance.

**Evaluation Metrics:**
- **Precision:** The percentage of reviews flagged by the classifier that are actually fake.
- **Recall:** The percentage of fake reviews correctly flagged by the classifier.

Figure 5 shows the precision and recall of the classifier when applied to machine-generated reviews generated at different temperatures, with lower values indicating higher-performing attacks. Overall, we observe high-performing attacks at all temperatures. The best attack is at temperature 1.0, with a low precision of 18.48% and a recall of 58.37%. Low precision indicates the ML classifier's inability to distinguish between real and machine-generated reviews.

In Figure 5, we observe that attack performance increases with temperature. To further understand this trend, we analyze how the linguistic features of the generated text vary as we increase the temperature. In Figure 7, we compare the average value of three linguistic features across three categories at different temperatures. Generally, the feature values of machine-generated reviews diverge from real reviews at low temperatures and converge as temperature increases, making it harder to distinguish them from real reviews at high temperatures.

**Plagiarism Detector**

Achieving reasonable linguistic quality does not rule out the possibility of being fake. A simple attack involves generating fake reviews by duplicating or partially copying from real reviews. In such cases, the review quality would be quite good and would pass the linguistic filter. The standard solution is to rely on plagiarism checkers to identify duplicate or near-duplicate reviews. Given that the RNN model is trained to generate text similar to the training set, we examine if the machine-generated reviews are duplicates or near-duplicates of reviews in the training set.

To conduct a plagiarism check, we assume the service provider has access to a database of reviews used for training the RNN. For a given machine-generated review, the service provider runs a plagiarism check by comparing it with reviews in the database. This is a best-case scenario for a plagiarism test and helps us understand its potential to detect generated reviews.

We use Winnowing [63], a widely used method to identify duplicate or near-duplicate text. For a suspicious text, Winnowing generates a set of fingerprints by applying a hashing function to substrings in the text and then compares the fingerprints between the suspicious text and the text in the database. Similarity between two reviews is computed using Jaccard Similarity [5] of their fingerprints. The plagiarism similarity score for a single review is computed as the maximum similarity with all other reviews in the dataset, ranging from 0 to 1 (1 indicates identical reviews).

We randomly sample 10,000 machine-generated reviews for the plagiarism test, and the database includes the entire Yelp training dataset. Figure 6 shows the quantiles of similarity scores at different temperatures. Each point shows the median, 25th, and 75th percentile of the plagiarism score distribution. Additionally, we show the similarity score distribution for real reviews, which serves as a baseline for comparison. Note that scores for real reviews do not vary with temperature. We observe that plagiarism scores of machine-generated reviews are low at all temperatures (lower score represents a smaller probability of copying) and decrease as temperature increases. Machine-generated reviews and real reviews show similar plagiarism scores, making them harder to distinguish. For example, at temperature 1.0, if we set a plagiarism score threshold such that 95% of real reviews are not flagged, we observe that 96% of machine-generated reviews still bypass the check. Thus, it remains challenging to detect machine-generated reviews using a plagiarism checker without inadvertently flagging a large number of real reviews. This shows that the RNN does not simply copy existing reviews from the training set.

#### 4.2 Evaluation by User Study

Regardless of how well machine-generated reviews perform on statistical measures and tests, the real test is whether they can pass for real reviews when read by human users. In this section, we conduct an end-to-end user study to evaluate whether human examination can detect machine-generated reviews. In practice, service providers often involve human content moderators to separate machine-generated reviews from real reviews [69]. More importantly, these tests will tell us how convincing these reviews are to human readers and whether they will accomplish their goals of manipulating user opinions.

**User Study to Detect Machine-Generated Reviews**

To measure human performance, we conduct surveys on Amazon Mechanical Turk (AMT). Each survey includes a restaurant name, description (explaining the restaurant category and description provided by the business on Yelp), and a set of reviews, which includes both machine-generated and real reviews written for that restaurant. We then ask each worker to mark reviews they consider to be fake, using any basis for their judgment.

For our survey, we choose 40 restaurants with the most reviews in our ground-truth dataset. For each restaurant, we generate surveys, each of which includes 20 random reviews, with some portion (X) being machine-generated reviews and the rest being real reviews from Yelp. The number X is randomly selected between 0 and 5 so that the expected ratio of fake reviews (12.5%) matches the real-world setting (11.99% in Table 1). Additionally, we control the quality of real reviews shown in the surveys to cover the full range of usefulness, leveraging the review usefulness metadata provided by Yelp for each review.

For each of our 40 restaurants, we generated reviews using five different temperature parameters: [0.1, 0.3, 0.5, 0.7, 1.0]. We give each unique survey to three different workers, resulting in a total of 600 surveys. Out of these 600 responses, we discarded six because they did not mark the gold standard reviews. Gold standard reviews are strings of random characters (i.e., meaningless text) that look clearly fake to any worker. Lastly, we only request master workers located in the US to ensure English literacy. An example of our survey is shown in Figure 15(a) in Appendix C.

Figure 8 shows the human performance results as we vary the temperature. We observe that machine-generated reviews appear quite robust against human detection. Under the best configuration, the precision is only 40.6% with a recall of 16.2%. Additionally, similar to algorithmic detection, attack performance improves as temperature increases. This is surprising, as we would expect that reviews at extreme high or low temperature parameters would be easily flagged (either too repetitive or with too many grammatical/spelling errors). Instead, it seems that human users are more sensitive to repetitive errors than to small spelling or grammar mistakes. The best attack performance occurs at a high temperature of 0.7, which is marginally better than the performance at temperature 1.0.

**Helpfulness of Machine-Generated Reviews**

Previously, we showed that humans tend to mark many machine-generated reviews as real. This raises a secondary question: For machine-generated reviews that are not caught by humans, do they still have sufficient quality to be considered useful by a user? Answering this question takes us a step further towards generating highly deceptive fake reviews. We run a second round of AMT surveys to investigate this.

In each survey, we first ask the workers to mark reviews as fake or real. Additionally, for the reviews marked as real, we ask for a rating of the usefulness of the review on a scale from 1 to 5 (1 as least useful, 5 as most useful). An example of the survey is shown in Figure 15(b) in Appendix C. We conduct the survey using reviews generated at a temperature of 0.7, which gave the best performance in the previous round. In this round, we test on 80 restaurants and hire five workers for each restaurant. The rest of the survey configuration remains the same as the first round.

We received all 400 responses and discarded five for failing the gold standard review. The average usefulness score of false negatives (unflagged machine-generated reviews) is close to that of true negatives (unflagged Yelp real reviews): machine-generated reviews have an average usefulness score of 3.15, which is close to the average usefulness score of 3.28 for real Yelp reviews. This suggests that workers find unflagged machine-generated reviews almost as useful as real reviews.

Overall, our experiments find that machine-generated reviews closely mimic the quality of real reviews. Furthermore, the attacker is incentivized to generate reviews at high temperatures, as such reviews appear more effective at deceiving users.

### 5. Defending Against Machine-Generated Reviews

In this section, we propose a supervised learning scheme to detect machine-generated reviews with high accuracy.

**Assumption:**
We assume that the service provider has access to a limited set of reviews generated by an attackerâ€™s language model and a set of real reviews available on the review site.

**Why is Defense Challenging?**
Fundamentally, we are trying to develop a machine learning classifier capable of detecting the output of another ML model. In an ideal scenario, this seems impossible because the generator model and detector are likely using the same metrics on the same inputs (or training datasets). Thus, any metric that an ML detector can detect can be accounted for by the ML-based generator.

**Key Insight:**
Figure 9 shows the key intuition behind our defense. While we expect that an attacker is aware of any metric used by the ML detector for detection, we can find a sufficiently "complex" metric where accurately capturing (and reproducing) the metric requires an extremely large RNN infrastructure that is beyond the means of most attackers. This leverages the fact that a generative language model builds a fixed memory representation of the entire training corpus, which limits the amount of information that can be learned from a training corpus. More specifically, we observe that text produced naturally (e.g., by a human) diverges from machine-generated text when we compare the character-level distribution, even when higher-level linguistic features (e.g., syntactic, semantic) are similar.