In this section, we evaluate the quality of machine-generated re-
views along two dimensions. First, we investigate whether gen-
erated reviews can bypass detection by existing algorithmic ap-
proaches. Second, we conduct an end-to-end user study, by pre-
senting restaurant reviews containing both generated reviews and
real reviews to human judges. Our goal is to understand whether
humans can distinguish generated reviews from real reviews.
4.1 Detection by Existing Algorithms
We focus on two popular algorithmic techniques to distinguish
machine-generated reviews from real reviews: (1) a supervised ML
scheme based on linguistic features, (2) a plagiarism detector to
check for duplications between machine-generated reviews and
training set (real) reviews.
ML-based Review Filter. Using machine learning classifiers to
detect fake reviews is a well studied problem [20, 48, 53]. Most of
the prior works rely on the observation that characteristics of fake
reviews deviate from real reviews along many linguistic dimen-
sions. We identified 5 groups of linguistic features, consisting of 77
features total that previously demonstrated strong discriminatory
power for distinguishing fake and real reviews. We describe the
features below:
• Similarity feature (1): Captures inter-sentence similarity within
a review at the word level. It is computed as the maximum
3Since Yelp also includes reviews for non-restaurant business, e.g., hair salon and car
service.
cosine similarity between unigram features among all pairs of
sentences [10, 20, 31, 56].
• Structural features (4): Captures the structural aspects of a
review. Individual features include the number of words, the
number of sentences, the average sentence length (# of words)
and the average word length (# of characters) [20, 56].
• Syntactic features (6): Captures the linguistics properties of
the review based on parts-of-speech (POS) tagging. Features
include (distinct) percentages of nouns, verbs, adjectives and
adverbs, first personal pronouns, and second personal pro-
nouns [20, 31, 56].
• Semantic features (4): Captures the subjectivity and sentiment
of the reviews. Features include percentage of subjective words,
percentage of objective words, percentage of positive words
and percentage of negative words. All these features are de-
fined in SentiWordNet [3], a popular lexical resource for opin-
ion mining [31, 53, 56].
• LIWC features (62): The Linguistic Inquiry and Word Count
(LIWC) software [52] is a widely used text analysis tool in the
social sciences. It categorizes ∼4,500 keywords into ∼68 psy-
chological classes (e.g., linguistic processes, psychological pro-
cesses, personal concerns and spoken categories). We use the
percentage of word count in each class as a feature, and exclude
the features already included in the previous groups [48, 49].
We train a linear SVM classifier on the Yelp ground-truth dataset,
composed of real reviews (Yelp unfiltered reviews), and fake reviews
(Yelp filtered reviews). After training with all 77 linguistic features,
we tested the performance of the classifier on the Yelp attack dataset,
composed of real reviews and machine-generated reviews. We run
10-fold cross validation and report the average performance.
Evaluation of attack performance uses precision (percentage of
reviews flagged by the classifier that are fake reviews), and recall
(percentage of fake reviews flagged by the classifier). Figure 5 shows
the precision and recall of the classifier when applied to machine-
generated reviews generated from different temperatures (along
with real reviews), with lower values indicating higher performing
attacks. Overall, we observe high performing attacks at all temper-
atures. The best attack is at temperature 1.0, with a low precision
of 18.48%, and a recall of 58.37%. Low precision indicates the in-
ability of the ML classifier to distinguish between real reviews and
machine-generated reviews.
In Figure 5, we observe that attack performance increases with
temperature. To further understand this trend, we analyze how
the linguistic features of the generated text vary as we increase
temperature. In Figure 7, we compare the average value of a lin-
guistic feature of generated reviews with real reviews at different
temperatures. We show results for 3 linguistic features across 3 cat-
egories, and other features exhibit similar trends. In general, feature
values of the machine-generated reviews diverge from real reviews
at low temperatures, and converge as temperature increases, thus
making it harder to distinguish them from real reviews at high
temperatures.
Plagiarism Detector. Achieving reasonable linguistic quality does
not rule out the possibility of being fake. A simple attack involves
generating fake reviews by duplicating or partially copying from
real reviews. In such cases, the review quality would be quite good,
Session E4:  Adversarial Social Networking CCS’17, October 30-November 3, 2017, Dallas, TX, USA1148Temperature
0.1
0.5
0.7
1.0
Generated Review Text
I love this place! I have been here a few times and have never been disappointed. The service
is always great and the food is always great. The staff is always friendly and the food is always
great. I will definitely be back and try some of their other food and service.
I love this place. I have been going here for years and it is a great place to hang out with friends
and family. I love the food and service. I have never had a bad experience when I am there.
My family and I are huge fans of this place. The staff is super nice and the food is great. The
chicken is very good and the garlic sauce is perfect. Ice cream topped with fruit is delicious
too. Highly recommended!
I had the grilled veggie burger with fries!!!! Ohhhh and taste. Omgggg! Very flavorful! It was
so delicious that I didn’t spell it!!
Table 2: Example of the generated five-star fake reviews targeting at a buffet restaurant in NYC. More examples of generated
reviews are included in the Appendix B.
Figure 5: Performance of linguistic classifier on detecting
machine-generated reviews.
Figure 6: Plagiarism similarity score. Each point shows me-
dian, 25th and 75th percentile of the score distribution.
(a) Average word length (structural feature)
(b) Ratio of verb usage (syntactic feature)
(c) Ratio of positive word usage (semantic feature)
Figure 7: Change of linguistic feature values when temperature varies.
and would pass the linguistic filter. Standard solution is to rely
on plagiarism checkers to identify the duplicate or near-duplicate
reviews. Given that the RNN model is trained to generate text
similar to the training set, we examine if the machine-generated
reviews are duplicates or near-duplicates of reviews in the training
set.
To conduct a plagiarism check, we assume that the service
provider has access to a database of reviews used for training the
RNN. Next, given a machine-generated review, the service provider
runs a plagiarism check by comparing it with reviews in the data-
base. This is a best case scenario for a plagiarism test, and helps us
understand its potential to detect generated reviews.
We use Winnowing [63], a widely used method to identify dupli-
cate or near-duplicate text. For a suspicious text, Winnowing first
generates a set of fingerprints by applying a hashing function to
a set of substrings in the text, and then compares the fingerprints
between the suspicious text and the text in database. Similarity be-
tween two reviews is computed using Jaccard Similarity [5] of their
fingerprints generated from Winnowing. The plagiarism similarity
 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1Detection PerformanceTemperaturePrecisionRecall 0 0.1 0.2 0.3 0.4 0.5 0 0.2 0.4 0.6 0.8 1Plagiarism ScoreTemperatureMachine-generated ReviewReal Review 3 3.3 3.6 3.9 4.2 4.5 0 0.2 0.4 0.6 0.8 1Word Length (# of char)TemperatureMachine-generated ReviewReal Review 0 2 4 6 8 10 0 0.2 0.4 0.6 0.8 1Verbs (%)TemperatureMachine-generated ReviewReal Review 0 4 8 12 16 20 0 0.2 0.4 0.6 0.8 1Positive Words (%)TemperatureMachine-generated ReviewReal ReviewSession E4:  Adversarial Social Networking CCS’17, October 30-November 3, 2017, Dallas, TX, USA1149score for a single review is computed as the max similarity with all
the other reviews in the dataset, and ranges from 0 to 1 (1 indicates
identical reviews).
We pick a random sample of 10K machine-generated reviews for
the plagiarism test, and the database (for comparison) includes the
entire Yelp training dataset. Figure 6 shows the quantiles of similar-
ity scores at different temperatures. Each point shows median, 25th
and 75th percentile of the plagiarism score distribution. In addi-
tion, we also show the similarity score distribution for real reviews,
which serves as a baseline for comparison. Note that scores for real
reviews do not vary with temperature. We obverse that plagiarism
scores of machine-generated reviews are low at all temperatures
(lower score represents smaller probability of copying) and decrease
as temperature increases. In addition, machine-generated reviews
and real reviews show similar plagiarism scores, thus making them
harder to distinguish. For example, at temperature 1.0, if we set a
plagiarism score threshold such that 95% of real reviews are not
flagged, we observe that 96% of machine-generated reviews still by-
pass the check. Thus, it remains hard to detect machine-generated
reviews using a plagiarism checker without inadvertently flagging
a large number of real reviews. This shows that the RNN does not
simply copy the existing reviews from the training set.
4.2 Evaluation by User Study
Regardless of how well machine-generated reviews perform on
statistical measures and tests, the real test is whether they can
pass for real reviews when read by human users. In this section,
we conduct an end-to-end user study to evaluate whether human
examination can detect machine-generated reviews. In practice,
service providers are known to involve human content moderators
to separate machine-generated reviews from real reviews [69]. More
importantly, these tests will tell us how convincing these reviews
are to human readers, and whether they will accomplish their goals
of manipulating user opinions.
User Study to Detect Machine-Generated Reviews. To mea-
sure human performance, we conduct surveys4 on Amazon Me-
chanical Turk (AMT5). Each survey includes a restaurant name,
description (explaining the restaurant category and description
provided by the business on Yelp), and a set of reviews, which in-
cludes machine-generated reviews and real reviews written for that
restaurant. We then ask each worker to mark reviews they consider
to be fake, using any basis for their judgment.
For our survey, we choose 40 restaurants with the most number
of reviews in our ground-truth dataset. For each restaurant, we
generate surveys, each of which include 20 random reviews, out of
which some portion (X) are machine-generated reviews, and the
rest are real reviews from Yelp. The number X is randomly selected
between 0 to 5 so that the expected ratio of fake reviews (12.5%)
matches the real world setting (11.99% in Table 1). Additionally, we
control the quality of real reviews shown in the surveys to cover
the full range of usefulness. We leverage the review usefulness (a
simple count of the number of users who found the review to be
useful) metadata provided by Yelp for each review.
4Prior to conducting our study, we submitted a human subject protocol and received
approval from our local IRB board.
5https://www.mturk.com/
For each of our 40 restaurants, we generated reviews using 5
different temperature parameters: [0.1, 0.3, 0.5, 0.7, 1.0]. We give
each unique survey to 3 different workers, giving us a total of 600
surveys. Out of these 600 responses, we discarded 6 because they
did not mark the gold standard reviews. Gold standard reviews are
basically strings of random characters (i.e. meaningless text), that
looks clearly fake to any worker. Lastly, we only request master
workers6 located in the US to guarantee English literacy. We show
an example of our survey in the Figure 15(a) in Appendix C.
Figure 8 shows the human performance results as we vary the
temperature. First, we observe that machine-generated reviews
appear quite robust against a human test. Under the best configura-
tion, the precision is only 40.6% with a recall of 16.2%. In addition,
similar to algorithmic detection, attack performance improves as
temperature increases. This is surprising, since we would expect
that reviews at the extreme high or low temperature parameters
would be easily flagged (either too repetitive or too many gram-
matical/spelling errors). We saw earlier that higher temperature
produced reviews more statistically similar to real reviews, but ex-
pected errors to make those reviews detectable by humans. Instead,
it seems that human users are much more sensitive to repetitive
errors than they are to small spelling or grammar mistakes. We do
observe that the best attack performance occurs at a high temper-
ature of 0.7, which is marginally better than the performance at
temperature of 1.0.
Helpfulness of Machine-Generated Reviews. Previously, we
showed that humans tend to mark many machine-generated re-
views as real. This raises a secondary question: For machine-generated
reviews that are not caught by humans, do they still have sufficient
quality to be considered useful by a user? Answering this question,
takes us a step further towards generating highly deceptive fake
reviews. We run a second round of AMT surveys to investigate this
question.
In each survey, we first asked the workers to mark reviews as
fake or real. Additionally, for the reviews marked as real, we asked
for a rating of the usefulness of the review on a scale from 1 to 5
(1 as least useful, 5 as most useful). An example of the survey is
shown in the Figure 15(b) in Appendix C. We conduct the survey
using reviews generated at a temperature of 0.7, which gave the
best performance from the previous round. Also, in this round, we
test on 80 restaurants and hire 5 workers for each restaurant. The
rest of the survey configuration remains the same as the first round.
We received all 400 responses and discarded 5 of them for failing
the gold standard review. The average usefulness score of false
negatives (unflagged machine-generated reviews) is close to that
of true negatives (unflagged Yelp real reviews): machine-generated
reviews have an average usefulness score of 3.15, which is close to
the average usefulness score of 3.28 for real Yelp reviews. That is to
say, workers think of unflagged machine generated reviews almost
as useful as real reviews.
Overall, our experiments find machine-generated reviews very
close to mimicking the quality of real reviews. Furthermore, the
attacker is incentivized to generate reviews at high temperatures,
as such reviews appear more effective at deceiving users.
6https://www.mturk.com/mturk/help?helpPage=worker#what_is_master_worker
Session E4:  Adversarial Social Networking CCS’17, October 30-November 3, 2017, Dallas, TX, USA1150Figure 8: Performance of human judgment on detecting
machine-generated review.
5 DEFENDING AGAINST
MACHINE-GENERATED REVIEWS
In this section, we propose a supervised learning scheme to detect
machine-generated reviews with high accuracy.
Assumption. We assume that the service provider has access to a
limited set of reviews generated by an attacker’s language model
and a set of real reviews available on the review site.
Why is defense challenging? Fundamentally, we are trying to de-
velop a machine learning classifier capable of detecting the output
of another ML model. In an ideal scenario, this seems impossible, be-
cause the generator model and detector are likely using the exactly
same metrics on the same inputs (or training datasets). Thus, any
metric that an ML detector is capable of detecting can be accounted
for by the ML-based generator.
Key Insight. Figure 9 shows the key intuition behind our defense.
While we expect that an attacker is aware of any metric used by
the ML detector for detection, we can find a sufficiently “complex”
metric where accurately capturing (and reproducing) the metric
requires an extremely large RNN infrastructure that is beyond the
means of most attackers. This leverages the fact that a generative
language model builds a fixed memory representation of the entire
training corpus, which limits the amount of information that can be
learned from a training corpus. More specifically, we observe that
text produced naturally (e.g., by a human) diverges from machine
generated text when we compare the character level distribution,
even when higher level linguistic features (e.g., syntactic, semantic