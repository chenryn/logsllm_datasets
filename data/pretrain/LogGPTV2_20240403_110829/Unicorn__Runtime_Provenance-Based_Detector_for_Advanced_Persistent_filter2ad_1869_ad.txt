the attacker exploited a different
vulnerability from GNU Bash version 4.3, which allows remote
attackers to execute arbitrary code via crafted trailing strings
after function deﬁnitions in Bash scripts (CVE-2014-6271).
Both scenarios represent the supply-chain compromise as the
initial access mechanism to subvert a company’s software
distribution channel to spread malware [48, 67].
In both experiments, we model normal behavior of the
victim system (i.e., the CI server). Table VI summaries the
datasets.
Experiment
SC-1
SC-2
Precision
0.85
0.75
Recall
0.96
0.80
Accuracy
F-Score
0.90
0.77
0.90
0.78
TABLE VIII: Experimental results of the supply-chain APT attack scenarios.
Experimental Results. We split 125 benign graphs randomly
into 5 groups to enable 5-fold cross validation. After we
use UNICORN to model normal behavior on the training set,
which consists of 100 benign graphs (i.e., 4 groups), we
evaluate it on the remaining 25 benign graphs (i.e., the 5th
group) for false positive validation. We also evaluate the model
on the 25 attack graphs for false negatives (Table VI). We
repeat this procedure for each group and report the mean
evaluation results. Table VII summarizes the conﬁguration for
the experiments and Table VIII shows the experimental results.
We see in Table VIII that UNICORN is able to detect
attacks with high accuracy with only a small number of false
alarms (as reﬂected in precision and recall). We observe that
UNICORN creates many clusters in the sub-models during
modeling, and the majority of true alarms originate from the
ﬁrst several clusters as UNICORN tracks system state transition.
This suggests that UNICORN’s evolutionary model captures
system behavior changes and that it is able to detect attacks
in their early stages, i.e., the initial supply-chain access point.
9
This has important implications. First, traditional clustering
approaches that use static snapshots to build the initial model
generate a large number of false positives (§ VI-A). These
can easily overwhelm system administrators and cause “threat
fatigue”, leading to alert dismissal. In supply-chain attacks,
the attackers can take advantage of this initial stage to break
into an enterprise network. UNICORN reduces false positives
as its evolutionary models precisely but ﬂexibly deﬁne normal
system behavior. We further note that dynamically adapting the
model during runtime is also suboptimal in APT scenarios,
because once the attackers break into the network from the
initial supply chain, they can guide the model to slowly and
gradually penetrate the network without the model raising an
alarm. Second, while many APTs abandon stealth in later
attack stages, making detection easier, UNICORN raises alarms
in earlier stages, thus preventing damage that may have already
occurred when APTs unmask their behavior. For example, we
observe that 50% of the attacks in SC-1 were detected when
malicious packages were just delivered to the victim machine,
and all of them were ﬂagged right after installation.
It is more difﬁcult to detect attacks in the SC scenarios
than in the DARPA ones. The attackers in the DARPA datasets
spend time ﬁnding vulnerabilities in the system, and that
behavior appears in the traces. In contrast, in the SC scenarios,
the attacker has a priori knowledge of the target system (i.e.,
we act as both the attacker and the victim), so we can launch
an attack without any prior unusual behavior. This partially
explains UNICORN’s lower performance on the SC datasets.
In § VI-D, we conduct additional experiments on SC-1 to
demonstrate the importance of graph exploration in detecting
anomalies in provenance graphs.
D. Inﬂuence of Graph Analysis on Detection Performance
it
We now analyze the importance of UNICORN’s key pa-
rameters using the SC-1 dataset. We use the same setup
from § VI-C as our baseline conﬁgurations (Table VII). We
then vary parameters independently to examine the impact of
each. Fig. 3 shows the experimental results.
Batch Size (BS). This indicates the number of edges submitted
to GraphChi at once; it does not affect detection performance.
Hop Count (HC). This deﬁnes the size of the neighborhood
used to characterize each vertex;
is a measure of the
expressiveness of the features in our sketches. Larger hop
counts capture more contextual information, some of which
might be irrelevant, which can mask potential attacks [83, 87],
as shown in Fig. 3(a).
Sketch Size (SS). This is the size of our ﬁxed-size histogram
representation. Larger SS allows UNICORN to include more
information about the evolving graph, thus reducing the error
of approximating normalized min-max similarity (§ IV-C).
However, a large SS ultimately leads to the curse of dimen-
sionality [38] in clustering (§ IV-D). Fig. 3(b) conﬁrms that, in
general, detection precision, recall, and accuracy improve as
we increase SS up to a point, after which it degrades.
Interval of Sketch Generation (SG). This is the number of
edges added to the graph between the construction of new
sketches. Smaller SG makes adjacent sketches look similar
to each other, which can produce higher false negative rates
Fig. 3: Detection performance (precision, recall, accuracy, and F-score) with varying hop counts (Fig. 3(a)), sketch sizes (Fig. 3(b)), intervals of sketch generation
(Fig. 3(c)), and decay factor (Fig. 3(d)). Baseline values (*) are used by the controlled parameters (that remain constant) in each ﬁgure.
1
e
t
a
R
0.5
0
1
e
t
a
R
0.5
0
1
2
3*
(a): Hop
4
5
500
1000
2000*
(b): Sketch
3000
10000
500
1000
3000*
(c): Interval
5500
0
0.02*
0.1
(d): Decay
1
Accuracy Precision Recall F-Score
and lower recall and accuracy. Meanwhile, given ﬁxed-size
sketches, which are approximations, a larger SG leads to
coarser-grained changes, which also makes graphs look too
similar to each other. In Fig. 3(c), we observe that SG = 500
edges per new sketch cannot detect any attack graphs, resulting
in 0 recall and undeﬁned precision and F-score. We obtain
optimal results when setting the interval to be around 3, 000
in the SC-1 experiment.
Weighted Decay Factor (DF). This determines the rate at which
we forget the past. We observe that both never-forget (λ =
0.0) and always-forget (λ = 1.0) yield unsatisfactory results,
while a slow decay rate (around 0.02) achieves a good balance
between factoring past and current graph components into the
analysis.
E. Processing Speed
In the previous section, we show how UNICORN’s param-
eters inﬂuence detection performance; this section and the one
that follows examine UNICORN’s runtime overhead. These
sections consider only the CamFlow implementation, which
has been shown to produce negligible overhead [100], allowing
us to isolate UNICORN’s performance characteristics. We use
Amazon EC2 i3.2xlarge Linux machines with 8 vCPUs and
61GiB of memory.
Runtime performance is important in APT scenarios where
an IDS is constantly monitoring the system in real time. We
analyze the SC-1 experiment using the same baseline settings
and parameters as in the previous section. (We also evaluate
batch size here, which has no impact on accuracy.) Together,
these two sections illustrate the tradeoff between accuracy and
runtime performance.
Fig. 4 shows the total number of edges processed over
time as a metric to quantify UNICORN’s processing speed.
The CamFlow lines (in blue) represent the total number of
edges generated by the capture system; the closer other lines
are to this line, the better the runtime performance, meaning
that UNICORN “keeps up” with the capture system.
Batch Size (BS). Fig. 4(a) shows that runtime performance
improves as we increase BS. We use BS of 6,000 as it
approximates CamFlow. There is marginal improvement as we
increase BS above 6,000.
Sketch Size (SS). As shown in Fig. 4(b), SS has minimal impact
on runtime performance, except during the beginning of the
experiment when UNICORN initializes the sketch, which re-
quires more computation with larger sketch sizes. Afterwards,
runtime performance is similar among different SS, thanks to
UNICORN’s fast, incremental sketch update.
Hop Count (HC). Fig. 4(c) shows that HC has minimal impact
on provenance graphs because of the graph structure and its
adaption of fast WL algorithm.
Interval of Sketch Generation (SG) & Weighted Decay Factor
(DF). Neither SG nor DF affect runtime performance (we omit
DF from the ﬁgure).
Overall, we show that UNICORN runtime is relatively
insensitive to these parameters. This means that UNICORN can
perform realtime intrusion detection with parameters optimized
for detection accuracy.
10
Fig. 4: Total number of processed edges over time (in seconds) in the SC-1 experimental workload with varying batch sizes (Fig. 4(a)), sketch sizes (Fig. 4(b)),
hop counts (Fig. 4(c)), and intervals of sketch generation (Fig. 4(d)). Dashed blue line represents the speed of graph edges streamed into UNICORN for analysis.
Triangle maroon baseline has the same conﬁgurations as those used in our experiments and indicates the values of the controlled parameters (that remain
constant) in each ﬁgure.
·106
1
0.8
0.6
0.4
0.2
0
1
0.8
0.6
0.4
0.2
0
)
s
e
g
d
E
f
o
#
(
e
z
i
S
h
p
a
r
G
)
s
e
g
d
E
f
o
#
(
e
z
i
S
h
p
a
r
G
CamFlow
Batch = 1,000
Batch = 3,000
Batch = 6,000
Batch = 10,000
CamFlow
Sketch = 500
Sketch = 1,000
Sketch = 2,000
Sketch = 5,000
Sketch = 10,000
·106
(a): Batch
(b): Sketch
CamFlow
Hop = 1
Hop = 2
Hop = 3
Hop = 4
Hop = 5
0
5
2
2
0
5
2
5
7
2
0
0
3
0
5
2
0
5
5
7
0
5
2
0
5
5
7
5
0
5
0
7
5
1
2
1
0
0
1
2
Time (seconds)
1
(c): Hop
0
5
5
0
7
5
1
2
1
0
0
1
2
Time (seconds)
1
(d): Interval
CamFlow
Interval = 500
Interval = 1,000
Interval = 3,000
Interval = 5,500
0
5
2
2
0
5
2
5
7
2
0
0
3
F. CPU & Memory Utilization
Conﬁguration Parameter
Hop
Count
Sketch
Size
We evaluate UNICORN’s CPU utilization and memory
overheads for a system under relatively heavy workload, i.e.,
CI performing kernel compilation. We show that UNICORN
exhibits low CPU utilization and memory overheads.
Fig. 5(a) shows the average CPU utilization due to UNI-
CORN over a long-running experiment using the baseline
conﬁguration. The average CPU utilization stabilizes around
12.3% on a single CPU. Fig. 5(b) shows the per-vCPU and the
average CPU utilization of the same experiment (but with a
shorter timeline to avoid cluttering). Note that the parameters
discussed in the previous sections do not signiﬁcantly impact
average CPU utilization.
PassMark Software’s Enterprise Endpoint Security perfor-
mance benchmark [14] shows an average CPU utilization of