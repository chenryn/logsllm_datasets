usefulness. For both Task 1 and Task 2, a majority classiﬁer
performed the same as a random classiﬁer for precision and
recall, so we do not report results for random classiﬁers.
7.4.1 Task 1: Sensitivity
We ﬁrst tried to predict if a user would perceive a document
or image as sensitive. Figures 6a–6b show precision vs. recall
curves (PRC) for the sensitivity dataset. Figure 6a shows
the PRC for predicting the sensitivity of documents. Aletheia
performed the best overall, while Aletheia w/ only GDLP
features performed worse. The GDLP feature count classiﬁer
did not perform better than the Random baseline in this setting.
Aletheia had an AUC 0.68, while Aletheia w/ only GDLP
features had an AUC of 0.40, an improvement of 68%.
Figure 6b shows the PRC for predicting the sensitivity of
images. Aletheia had an AUC of 0.86, compared to Aletheia
1156    30th USENIX Security Symposium
USENIX Association
Aletheia w/ all featuresAletheia w/ only GDLP featuresGDLP feature countLast modified, file typeLast modifiedRandom0.00.20.40.60.81.0recall0.00.20.40.60.81.0precision0.00.20.40.60.81.0recall0.00.20.40.60.81.0precision0.00.20.40.60.81.0recall0.00.20.40.60.81.0precision0.00.20.40.60.81.0recall0.00.20.40.60.81.0precisionDecision
Keep
Delete
Protect
Correct Predictions
Incorrect Predictions
91%
75%
37%
9% (delete)
25% (keep)
56% (keep), 7% (delete)
Table 7: Accuracy per ﬁle-management decision. For incor-
rect predictions, we show our classiﬁer’s prediction in paren-
theses. For example, we predicted 91% of keep labels cor-
rectly, while incorrectly predicting 9% of keep labels as delete.
7.4.3 Task 3: File-Management Decision
Finally, we tried to predict ﬁle-management decisions, and
Figure 7 shows our overall accuracy in doing so. We show
results for both predicting the ﬁle-management decision di-
rectly from ﬁle features, as well as the aforementioned two-
step process in which we ﬁrst predicted the ﬁle’s sensitivity
and usefulness, subsequently using these predictions as ad-
ditional features in predicting the ﬁle-management decision.
The oracle shows that if the classiﬁer knew participants’ ac-
tual responses for perceived usefulness and sensitivity, we
could achieve 90% accuracy for predicting ﬁle-management
decisions. In the more realistic scenario of using only ﬁle
features and (possibly incorrect) predictions of sensitivity and
usefulness in our two-step process, we saw a roughly 10%
increase in accuracy compared to the single-step process. This
result shows that even without an oracle of participants’ actual
responses for a ﬁle’s sensitivity and usefulness, leveraging
our predictions of those perceptions boosted the accuracy of
predicting the ﬁle-management decision.
Table 7 compares accuracy across ﬁle-management deci-
sions. We were most accurate on keep decisions, the majority
class. For cases in which we mispredicted keep decisions,
Aletheia always instead predicted them as delete. On the other
hand, for delete decisions, Aletheia had 75% accuracy, mis-
predicting delete decisions as keep. With only 37% accuracy,
Aletheia did not perform very well on protect labels. Inter-
estingly, the majority of mispredictions for protect were mis-
labeled as keep. This result shows that Aletheia considered
protect decisions as closer to keep decisions than delete. The
number of protect labels was signiﬁcantly smaller than the
other two labels, making it harder to predict.
7.5 Understanding Prediction Results
We also examined which features were important for each
prediction task. Table 8 shows the top features identiﬁed by
each classiﬁer for the sensitivity and usefulness tasks, in order
of importance. Generally, word2vec had high feature impor-
tance in the classiﬁers. However, since word2vec features are
not easy to interpret [27], we do not show them on the list. For
documents in the sensitivity task, we noticed that user-level
statistics like the fraction of ﬁles in the account containing
Figure 7: A comparison between directly predicting the ﬁle-
management decision versus ﬁrst predicting sensitivity and
usefulness for all ﬁles. Aletheia is compared to a majority
classiﬁer and an oracle that knows participants’ responses
about a ﬁle’s sensitivity and usefulness.
w/ only GDLP features with an AUC of 0.34, an improvement
of 153%. The GDLP feature count classiﬁer had an AUC of
0.20. Compared to prediction for documents, we observed
much better performance in terms of the PRC for Aletheia,
but not for Aletheia w/ only GDLP features.
From the sensitivity results, we found that a broader set
of features besides counts of sensitive information provided
more accurate results. We also found that Aletheia performed
better at predicting the sensitivity of images than documents.
This makes sense because we had additional image features
capturing adult, racy, spoofed, medical, and violent content,
which may be indicative of sensitivity for images.
7.4.2 Task 2: Usefulness
We next tried to predict if a user would perceive a document
or image as being not useful. Figures 6c–6d show precision vs.
recall curves (PRC) for predicting that speciﬁc documents and
images in the usefulness dataset were not useful. Aletheia
performed the best in both tasks, signiﬁcantly outperforming
the baseline classiﬁers.
Figure 6c shows the PRC for predicting not useful on doc-
uments. The best baseline classiﬁer was the Last Modiﬁed,
File Type heuristic, which performed reasonably well for pre-
dicting that a document was not useful. Aletheia achieved an
AUC value of 0.82, compared to the best baseline AUC value
of 0.65, an improvement of 26%. For images, all baseline
classiﬁers performed similarly, with the Last Modiﬁed heuris-
tic performing the best among them with an AUC of 0.35.
Aletheia was more accurate in predicting not useful than the
baselines, with an AUC of 0.71, an improvement of 101%.
We found that predicting images as sensitive was easier
than predicting documents as sensitive, while predicting doc-
uments as not useful was easier than predicting images as not
useful. This may be due to varying perceptions of usefulness,
which are less likely to be captured from image features.
USENIX Association
30th USENIX Security Symposium    1157
File featuresFile features +Predicted Sensitivity& Usefulness Features0.00.20.40.60.81.0Accuracy0.690.790.530.530.320.320.900.90AlethiaMajority ClassifierRandomOracleTask
Documents
Images
Documents
Images
Sensitivity
Usefulness
Features
gender; fraction of ethnic/VIN/location ﬁles;
credit card; date of birth; email
fraction of gender/SSN/ethnic/location ﬁles;
adult; credit card; racy; passport
access type; last modifying user; ﬁnance keywords;
report & journal keywords
ﬁle size; ﬁnance keywords; access type;
last modifying user; medical keywords
usefulness; sensitivity; spoof; account size;
used space; ﬁnance keywords; medical keywords
File Management All Files
Table 8: Top features for prediction tasks. Italicized keywords
were top terms identiﬁed via the bag of words collections.
ethnic terminology, VIN numbers, and locations played a role
in prediction, as did speciﬁc features like credit card num-
bers, dates of birth, and email addresses. For images, we saw
some of the same important features, but also sensitive image
features, such as whether content was potentially adult or racy.
For the usefulness dataset, we saw some similar top fea-
tures as in the sensitivity dataset, including access type and
ﬁnancial keywords. For documents, report and journal key-
words were important in predicting usefulness. For images,
medical content was also predictive of usefulness.
Besides the top features in Table 8, word2vec embeddings
were also identiﬁed as important features. This means that text
content is central to these prediction tasks. For documents, one
word2vec embedding represented the entire document. How-
ever, for images, we considered additional one-hot encoding
and word2vec features based on the automatically identiﬁed
image labels. Of those, only word2vec features were iden-
tiﬁed as important, probably because one-hot encodings of
“best guess labels” were too sparse.
Table 8 also shows the most important features for predict-
ing ﬁle-management decisions. The top two features were
the predicted labels for ﬁle usefulness and sensitivity. Using
XGBoost, the feature importances of usefulness and sensitiv-
ity predictions were 0.40 and 0.11, respectively, conﬁrming
our earlier observation about the two-step prediction process
being superior to the one-step process. Sensitive information
in the ﬁle, such as medical content, was an important feature.
To better understand the distribution of sensitive ﬁles in a
single account, Figure 8 shows box plots of the predicted prob-
ability a ﬁle was sensitive for all documents and images in
each Round 2 participant’s accounts. We omitted participants
with fewer than 10 documents or images in their accounts.
These predictions came from our preliminary classiﬁer, which
was trained on Round 1 data. On average, the preliminary clas-
siﬁer predicted the majority of ﬁles as having a low probability
of being sensitive. Only a small subset of ﬁles with high prob-
ability of being sensitive were selected for each participant.
For many participants, we were selecting only a small num-
ber of ﬁles that the preliminary classiﬁer deemed sensitive
with high probability. This resulted in a high percentage of
potentially sensitive ﬁles in our Round 2 dataset.
(a) Documents
(b) Images
Figure 8: Predicted sensitivity probability for each document
and image for every participant. On average, our classiﬁer pre-
dicts low sensitivity for a majority of ﬁles, and high sensitivity
on a small number of ﬁles.
Figure 9: Preliminary classiﬁer prediction precision as a func-
tion of predicted ﬁle sensitivity. The increasing precision at
higher predicted sensitivity scores indicates that predictions
of high sensitivity are more accurate.
To explore our preliminary classiﬁer’s accuracy in pick-
ing sensitive ﬁles, we looked at the relationship between high
probability predictions a ﬁle was sensitive and the precision of
prediction. We ranked the selected ﬁles in order of predicted
probability of being sensitive, classifying ﬁles based on a slid-
ing threshold for which everything above the threshold was
classiﬁed as sensitive. Finally, we computed the precision for
the ﬁles above the threshold on our ground truth ﬁle sensitiv-
ity, reporting the results for both documents and images in
Figure 9. A higher percentile means a higher threshold for the
predicted probability of sensitivity. When the threshold for
predicted probability was low, we had lower precision (around
30%). With higher predicted probability, our preliminary clas-
siﬁer had better precision. This shows that the preliminary
classiﬁer produced meaningful sensitivity predictions.
To better understand mispredictions, we also performed
a qualitative evaluation of ﬁles that were false positives in
our ﬁnal classiﬁer, alongside the reasons why participants did
1158    30th USENIX Security Symposium
USENIX Association
0.00.20.40.60.81.0Predicted File Sensitivity ProbabilityP20 (736)P16 (25)P18 (137)P30 (325)P23 (49)P2 (77)P9 (105)P7 (89)P11 (221)P21 (185)P22 (290)P10 (21)P28 (80)P27 (69)P24 (175)P13 (218)P17 (461)P12 (101)P14 (346)P8 (19)P29 (32)P6 (19)P26 (47)P25 (440)P1 (69)P15 (39)0.00.20.40.60.81.0Predicted File Sensitivity ProbabilityP3 (490)P32 (742)P2 (575)P33 (6314)P5 (1128)P6 (564)P9 (120)P10 (58)P28 (750)P20 (300)P17 (763)P21 (97)P12 (645)P22 (364)P11 (389)P29 (1692)P30 (842)P31 (94)P7 (1648)P24 (67)P27 (16)P13 (21)P8 (1357)P4 (431)P14 (93)P16 (489)P25 (924)P19 (197)P15 (47)P1 (15)P18 (66)P23 (390)P26 (240)020406080100Percentile of Predicted File Sensitivity0.00.20.40.60.81.0Precision of PredictionDocumentsImagesnot consider them sensitive. Speciﬁcally, we looked at false
positives in the top ﬁve documents and images by rank.
Within documents, 6% of such ﬁles contained PII that was
obsolete. One participant wrote, “It’s just a cover letter I
had written several years ago and doesn’t contain any good
info because the address and phone aren’t good anymore.”
As highlighted in Table 8, phone numbers and addresses
are both important in predicting sensitivity. However, accu-
rately classifying ﬁles requires more temporal information
and context. Similarly, 3% of the ﬁles contained sensitive
information belonging to someone other than the participant,
so they did not consider the ﬁle sensitive. Regarding an ex-
partner’s resume, a participant wrote, “It might be slightly
sensitive to my ex, but not really.” This particular ﬁnding sup-
ports prior work [44] suggesting that life experiences impact
data-privacy valuations. For a majority of the other documents
(70%), participants’ responses did not indicate a strong ele-
ment of sensitivity. They mentioned that the ﬁles contained
information they did not feel could compromise them in any
manner, or details that were already publicly available.
Most images that were misclassiﬁed as sensitive were pic-
tures with faces, memes, or some form of artwork or original
content. However, participants did not perceive them as sensi-
tive. For a family photo, a participant wrote, “This does not
reveal any personal information about me, or the person in the
photo.” In another example of original artwork, a participant
mentioned, “There is nothing sensitive in the ﬁle, but I would
not want someone stealing the image to use as their own.”
Pictures containing adult content that did not directly aﬀect
the participants were also not considered sensitive by some
participants. Regarding a nude photo, a participant mentioned
it was not compromising as they were not in the photo.
This investigation revealed that ﬁles shown to participants
conformed with our broader deﬁnition of sensitivity listed
in Table 1. However, diﬀerent participants had varying sen-
sitivity thresholds, which eventually weighed more into the
decision-making process of how they wanted to manage the
ﬁles. Better understanding this phenomenon requires both ad-
ditional data collection and the development of personalized
classiﬁers that account for such personalization. We note this
as a limitation of our current study, discussing possible future
work in this direction in the next section.
8 Discussion and Future Work
Decisions about ﬁle management are predicated on several
factors, some internal to the user and some based on the con-
tents of the ﬁle. The design of Aletheia focuses not on directly
predicting that decision, but rather on predicting perceptions
regarding these ﬁles that can be inferred using passively col-
lected ﬁle metadata, which can then in turn be useful in pre-
dicting the ultimate ﬁle-management decision. To this end,
we applied the usefulness/sensitivity model from Figure 1.
Our ﬁndings in Section 7.4 were particularly encourag-
ing for the usefulness part of this model, as using automated
inference techniques to ﬁrst build an understanding of par-
ticipants’ conceptualization of usefulness signiﬁcantly im-
proved our ability to predict their ﬁle-management decision;
the predicted usefulness was the single most predictive feature
for the ﬁle-management-decision classiﬁcation. This holistic,
human-centered approach to automated inference highlights
the importance of deep qualitative engagement with users
during the design of such classiﬁers.
Not only does this human-centered understanding improve
the performance of automated inference, but this approach can
also develop a deeper understanding of perceived usefulness
and sensitivity for ﬁles. Perceptions of usefulness are strongly
correlated with future access, while perceptions of sensitiv-
ity correlate with the existence of PII, ﬁnancial information,
intimate content, and sentimental value.
While Figure 5 shows a very strong correlation between
usefulness and desire to delete a given ﬁle, as well as keeping
non-sensitive useful ﬁles as-is, two more subtle points arise.
First, participants’ preferences for how to manage useful, sen-
sitive ﬁles did not map onto our hypothesized model; deci-
sions to protect useful ﬁles were nearly evenly split between
sensitive and not-sensitive ﬁles. Second, while not-useful ﬁles
were nearly always deleted, participants still wanted to re-
tain a nontrivial minority of ﬁles deemed not useful. This
phenomenon suggests that using the concept of usefulness
is very helpful for determining whether to retain a given ﬁle.
Nonetheless, automated systems should not use such a predic-
tion to make ﬁle-retention decisions automatically on behalf
of the user, but rather should seek conﬁrmation from the user.
While predicting ﬁle usefulness was incredibly helpful for
subsequently predicting ﬁle-management decisions, predict-
ing ﬁle sensitivity was both less successful and less helpful for
predicting ﬁle-management decisions. Beyond being harder
to accurately predict because the base rate of sensitive ﬁles
is low (13%), these phenomena suggest the relationship be-
tween sensitivity and ﬁle management is more complex than
our hypothesized model. Future work could explore whether
classiﬁers tuned to individual users’ preferences would be
able to improve performance on using sensitivity predictions
to underpin ﬁle-management-decision predictions.
Within the sensitivity prediction task, our classiﬁer per-