### Analysis of RePRAM Schemes

In this subsection, we provide a detailed analysis of how our RePRAM schemes behave under various possible configurations. We aim to offer key insights for choosing the most suitable RePRAM system parameters. Our analysis, supported by experimental results, helps users identify a desirable set of parameters for building an efficient RePRAM system.

#### 1. Lifetime vs. Redundancy Levels

- **Dim-2 (PDR):** In the second dimension, PDR (Partial Data Redundancy) is used. An advantage of using PDR is that each PCM block only incurs write operations intended for it, meaning no additional writes are imposed on the PCM blocks. This configuration minimizes the wear-out of the PCM device.
  
- **Dim-1 (MDR):** In the first dimension, we switch to MDR (Multiple Data Redundancy) beyond a specific point to accelerate the lifetime of the PCM blocks. MDR duplicates data across two separate PCM blocks, which can lead to a diminishing capacity of the PCM device. Every write operation is distributed to two different PCM blocks, contributing to the overall wear-out. Therefore, determining the optimal point to switch from PDR to MDR during the lifetime of the PCM blocks is crucial. 

**Figure 5** presents a comparison of the lifetime for various numbers of faults in Dim-1 configurations with a group size of 3. Initially, we start with PDR and switch to MDR after crossing a "specific number of faults" where data is mirrored. As the number of faults increases, the cost associated with three-way mapping also increases. For example, the average number of random trials to complete a three-way matching for PCM blocks with up to 80 faults is 2.1%, whereas for up to 140 faults, it is 5.2%. Our experiments show that higher-order PDR configurations with smaller group sizes can significantly improve the lifetime of PCM blocks.

#### 2. Sensitivity of PDR to DRAM Buffer Size

The size of the DRAM buffer, used for parity lookup, is a critical factor in minimizing the performance impact of PDR. To investigate this, we conducted additional experiments to determine the optimal DRAM buffer size needed to store parity information. 

**Figure 6** shows the overheads experienced by PDR with three different DRAM buffer sizes: 4MB, 16MB, and 32MB. Our results indicate that a 16MB DRAM buffer is sufficient to maintain high performance, with negligible overheads in almost every benchmark. A 32MB DRAM buffer shows minimal performance benefits over 16MB, while a 4MB DRAM buffer incurs significant overheads, up to 23% in some benchmarks. This suggests that a 16MB DRAM buffer is the most efficient, representing just 26% of the capacity invested in PCM main memory. Furthermore, even in stress cases, the 16MB DRAM buffer has shown less than 16% overheads.

#### 3. Frequency of Matching Algorithm Invocations

To understand the frequency of matching algorithm invocations, we performed experiments to quantify the number of times the algorithm needs to be invoked during the PRAM block's lifetime. **Figure 7** presents the results for variations of 0.1, 0.2, and 0.3. In these tests, we count the number of writes before the block encounters its first bit fault. As the block and its group pages incur more bit faults, the matching algorithm needs to be re-invoked to find a new set of compatible pages. This process is repeated until the PRAM block exceeds 160 bit faults, at which point the block is discarded.

From **Figure 7**, we observe that the average number of matching invocations is less than 10 throughout the PRAM block's lifetime for a variation of 0.1, while it is more spaced out for a variation of 0.3. In all cases, the number of matching invocations accounts for a small fraction of the actual writes performed on the PRAM block.

### Conclusions and Future Work

In this paper, we explored several dynamic redundancy techniques to resuscitate faulty PCM pages and improve the lifetime of PCM-based main memory systems. Our design choices include switching from PDR to MDR and reducing the group size in PDR from three to two. By intelligently combining PDR and MDR schemes, we showed that the lifetime of PRAM can be improved by up to 43x over Fail_Stop.

As future work, we plan to extend RePRAM to incorporate application-specific characteristics and system energy awareness. We will focus on capturing the memory-intensive demands of applications and adjust to their performance and energy constraints. Additionally, we will investigate other resistive memory technologies and the inherent endurance limitations and system-level effects resulting from write failures in these devices.

### Acknowledgments

This material is based upon work supported in part by the National Science Foundation under CAREER Award CCF-1149557, and grants CCF-1117243 and OCI-0937875.

### References

[1] C. Bienia, S. Kumar, J.P. Singh, and K. Li. The PARSEC Benchmark Suite: Characterization and Architectural Implications. January 2008. Princeton University Technical Report TR-811-08.

[2] Burton H. Bloom. Space/time trade-offs in hash coding with allowable errors. Commun. ACM, 13:422-426, July 1970.

[3] William A. Brant, Michael E. Nielson, and Edde Tin-Shek. Apparatus and method having a shadow DRAM, a flash ROM, an auxiliary battery, and a controller. US Patent 5,799,200, 1998.

[4] Jie Chen, R. C. Chiang, H. Howie Huang, and Guru Venkataramani. Power failure recovery for non-volatile main memory. SIGOPS Oper. Syst. Rev., 45(3):48-52, January 2012.

[5] Jie Chen, Zachary Winter, Guru Venkataramani, and H. Howie Huang. rpram: Exploring redundancy to improve lifetime of PCM-based main memory. In Proceedings of the 2011 International Conference on Parallel Architectures and Compilation Techniques, 2011.

[6] Sangyu Han and Hyunjin Lee. Flip-n-write: A simple deterministic technique to improve PRAM write performance and endurance. In MICRO, 2009.

[7] Intel Corporation. Intel Core i7-920 Processor. http://ark.intel.com/Product.aspx?id=37I47.

[8] Dave Hayslett. System z redundant array of independent memory. In IBM SWG Competitive Project Office, 211, 2010.

[9] Engin Ipek, Jeremy Condit, Edmund B. Nightingale, Doug Burger, and Thomas Moscibroda. LISA: Cooperative integration of nanoscale memories for PCM main memory. In ASPLOS, 2010.

[10] Lei Jiang, Yu Du, Youtao Zhang, B.R. Childers, and Jun Yang. Dynamic wear-leveling and salvaging for replicated resistive memory. In DSN, pages 221-232, June 2011.

[11] Nikolai Joukov, Arun M. Krishnakumar, Chaitanya Patti, Abhishek Rai, Sunil Samur, Avishay Traeger, and Erez Zadok. Raif: Redundant array of independent filesystems. MSST, 2007.

[12] Randy H. Katz. RAID: A personal recollection of how storage became a system. Annals of the History of Computing, IEEE, 32(4), 2010.

[13] HP Labs. Cacti 5.3. http://quid.hpl.hp.com:908IlcactV, 2010.

[14] Benjamin C. Lee, Engin Ipek, Onur Mutlu, and Doug Burger. Phase change memory as a scalable dram alternative. In ISCA, 2009.

[15] Rami Melhem, Rakan Maddah, and Sangyeun Cho. Architecting phase change memory to enable new memory usage models. White Paper, Numonyx, 2009. http://www.numonyx.coml. 2009.

[16] Numonyx. Phase change memory: A new memory technology. White Paper, Numonyx, 2009. http://www.numonyx.coml. 2009.

[17] David A. Patterson, Garth Gibson, and Randy H. Katz. A case for redundant arrays of inexpensive disks (RAID). In SIGMOD, pages 109-116, 1988.

[18] Devices Process Integration and Structures. International Roadmap for Semiconductors. http://www.itrs.net, 2007.

[19] Feng Qin, Shan Lu, and Yuanyuan Zhou. Safemem: Exploiting execution-time memory corruption for detecting memory leaks and memory corruption during production runs. In HPCA, 2005.

[20] Moinuddin K. Qureshi, John Karidis, Michele Franceschini, Vijayalakshmi Srinivasan, Luis Lastras, and Bulent Abali. Enhancing the lifetime of PCM-based main memory with start-gap wear leveling. In MICRO, 2009.

[21] Moinuddin K. Qureshi, Vijayalakshmi Srinivasan, and Jude A. Rivers. Scalable high-performance system using phase-change memory technology. In ISCA, 2009.

[22] S. Raoux, M. Salinga, D. Krebs, S.-H. Chen, H.-L. Lung, and C. H. Lam. Phase-change random access memory: A scalable technology. IBM J. Res. Dev., 52, July 2008.

[23] Jose Renau et al. SESE. http://sesc.sourceforge.net, 2006.

[24] Stuart Schechter, Gabriel H. Loh, Karin Strauss, and Doug Burger. Use ECP, not ECC, for hard failures in resistive memories. In ISCA, 2010.

[25] Nak Hee Seong, Dong Hyuk Woo, and Hsien-Hsin S. Lee. Security refresh: Prevent malicious wear-out and increase durability in resistive memory with dynamically randomized address mapping. In Proceedings of the 37th Annual International Symposium on Computer Architecture, 2010.

[26] Nak Hee Seong, Dong Hyuk Woo, Vijayalakshmi Srinivasan, Jude A. Rivers, and Hsien-Hsin S. Lee. SAFER: Stuck-at-fault error recovery for memories. In MICRO, 2010.

[27] Standard Performance Evaluation Corporation. SPEC Benchmarks. http://www.spec.org, 2006.

[28] Chris Wilkerson, Alaa R. Alameldeen, Wei Wu, Dinesh Somasekhar, and Shill-lien Lu. Reducing cache power with low-cost, multi-bit error-correcting codes. In ISCA, 2010.

[29] John Wilkes, Richard Golding, Carl Staelin, and Tim Sullivan. The HP AutoRAID hierarchical storage system. ACM Trans. Comput. Syst., 14, February 1996.

[30] S.C. Woo, M. Ohara, E. Torrie, J.P. Singh, and A. Gupta. The SPLASH-2 programs: Characteristics and Methodological Considerations. In ISCA, June 1995.

[31] B. Yang, J. Lee, J. Kim, J. Cho, S. Lee, and B. Yu. A low power phase change random access memory using a data comparison write scheme. In ISCAS, 2007.

[32] Doehyun Yoon, Naveen Muralimanohar, Parthasarathy Ranganathan, Norman P. Jouppi, and Mattan Erez. Free-P: Protecting non-volatile memory against both hard and soft errors. In HPCA, February 2011.

[33] Wangyuan Zhang and Tao Li. Characterizing and mitigating the impact of process variations on phase change memory-based systems. In MICRO, 2009.

[34] Ping Zhou, Bo Zhao, Jun Yang, and Youtao Zhang. A durable and energy-efficient main memory using phase change memory technology. In ISCA, 2009.