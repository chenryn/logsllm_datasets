For generating a test type of an ADT, we need a set of
constructor functions that create handles to this ADT in-
stance. Currently, we detect handles by static analysis ap-
plied on the public source code of a library. Our static anal-
ysis uses some simple heuristics like the ones used in [6].
For the future, we plan to combine it with dynamic analysis
like temporal speciﬁcation mining [18].
We assume that each ADT has a handle with a unique
C type. First, we extract all signatures of the functions im-
plemented by a library using Doxygen [17]. Our heuris-
tic is that we examine only functions that fulﬁll our nam-
ing convention as potential constructors and destructors.
Our naming convention for constructors is, that a function
name must contain alloc, create, new or open. For
destructors we require function names containing close,
destroy, delete or free. A constructor passes a han-
dle to its caller as return value or via call-by-reference. We
treat every C type that has at least one constructor and ex-
actly one destructor as handle.
For example, for the ADT socket we have extracted
the handle type apr_socket_t* for the libapr [1]. It has
two constructors:
apr_status_t apr_socket_create_ex(apr_socket_t**,
int, int, int, apr_pool_t*)
apr_status_t apr_socket_create(apr_socket_t**,
int, int, apr_pool_t*)
Both constructors return the handles by reference. We have
found 44 functions (including the destructor) that have a
argument of type apr_socket_t*.
Beside the constructors, we need input vectors to call
them for creating handle values. Therefore, we do look into
the truth tables used to compute the protection hypotheses
of the constructors. We extract all input vectors for which
the constructor function did not crash. These input vectors
are inserted into the type template to call the constructors.
Data Structures The data structure template comple-
ments the feedback giving test types. Data structures (in
C deﬁned by keyword struct) have a ﬁxed size. Addi-
tionally to starting the feedback cycle for pointers to data
structures with 1 byte, we use the size of the data structure
for it. So the size of a data structure s is put into the data
structure template to generate an adapted feedback test type
for s*.
3.5 Type Characteristics
Autocannon uses static analysis on public source code
to extract the type characteristics of a function’s argument
types. With the help of this characteristics it maps argument
types to Autocannon’s test types. The type characteristics
are:
pointer? True, if the argument type is a pointer value.
sizeof Size of the argument type in bytes.
converts to int? True, if argument type can be interpreted
as an integer.
signed? True, if argument type is signed.
content size Size of the dereferenced type in bytes (for
pointers, only).
ishandle? True, if at least one constructor function and ex-
actly one destructor function for this type exist.
Pointers to bytes are treated as strings. They are mapped
to the meta string type. For other pointers, a data struc-
ture type template is instantiated and with the help of a gen-
erated meta type combined with meta pointer. Other-
wise, it is mapped to one of Ballista’s integer types depend-
ing on its size and if it is signed. An exception are signed
types of size 2 and 4 they are assigned meta short and
meta integer, respectively.
If the argument type is a
handle, the handle type template is instantiated and com-
bined with its test type computed so far.
3.6 Reducing the Number of Test Cases
Ballista’s test types produce up to 1000 test values (in-
cluding parents). The average is however much lower. Both
the size of the set of input vector and hence, the number of
test cases increases exponentially with the number of func-
tion arguments. Therefore, it might be infeasible to test all
input vectors for functions with a larger number of argu-
ments. Ballista introduced an upper bound u on the number
of tested input vectors. If the set of input vectors is larger
than u, an uniform distributed sample set of the input vec-
tors of size u is computed.
Another way to reduce the size of the set of input vec-
tors, is to exclude special types from the generic meta types
that will not contribute to the results. For instance, a func-
tion operating on strings but not on ﬁles does not need to
be tested on ﬁle names. We perform static analysis using
the Low Level Virtual Machine (LLVM) [12] to determine
which special test types can safely be removed. The LLVM
byte code of the library might be provided by its vendor (if
the library should run on LLVM) or can be compiled from
the libraries private source code.
As mentioned above, Ballista contains a predeﬁned map-
ping from POSIX functions to their specialized Ballista test
types. Our assumption is that we do not need to test a func-
tion f on special test types designed for a POSIX function,
if this POSIX function is not called by f . But if a function
f calls some POSIX functions, we test f with the special
test types of this POSIX functions because f might pass
some argument values directly to this POSIX functions. Of
course, this assumption is not restricted to POSIX functions.
As long as a mapping exists from functions to their the spe-
cial test types exclusively provided for this functions.
For each function to test, we compute the transitive set
of called POSIX functions. To simplify our analysis, we
exclude a function from the reduction process, if it might
perform indirect calls using function pointers. The set of
called functions is compared with the list of POSIX func-
tions speciﬁed by Ballista. We only include special test
types in meta types that are related to the POSIX func-
tions called by the analysed function. To return to the pre-
vious example: a function f(char*) that transitively calls
no other function than printf is not tested on ﬁle names
because printf expects only format strings. To avoid to
exclude exceptional values, we only exclude types that are
leafs in Ballista’s type tree. Therefore, f is also tested on
plain string and pointer values.
3.7 Other Sources of Test Values
The truth table might be extended by other inputs. One
can do other kinds of fault injection, like we do for our eval-
uation in Section 6. An additional source are traces from
runs of some application utilizing the library to protect. The
advantage of these traces is that they contain (mostly) input
vectors for which the library function behaves robustly. The
disadvantage is that one has to set up the application to do
the tracing.
Currently, we do not use this additional source of test
values to keep the implementation effort to a reasonable
level. However, we use bit ﬂips and application traces
for evaluating the generated hypotheses (see Section 6). It
would be reasonable to use these techniques to help in the
generation of the hypotheses but then our evaluation might
be less meaningful.
4 Checks
Checks are used in the analysis stage to build up the
truth table for the protection hypothesis. Because they be-
come part of the protection hypothesis, they are also used
at run-time in the wrapper to check the current input vec-
tor. A check is a predicate over one or more argument
values. For instance, for function foo(void* p), the
check null?(p) computes whether p is or is not a NULL
pointer. This example illustrates that checks must be instan-
tiated with speciﬁc arguments of a function.
Checks and test values are not coupled. In this way, our
approach is easily extensible. One does not need to con-
sider the test type system when adding new checks and vice
versa. Checks may employ more than one argument of a
function. Checks employing only one argument are called
basic checks all other checks are compound checks. We
have a set of check templates that are instantiated depend-
ing on the type characteristics of the functions arguments.
Some check templates are additionally parameterized. We
remove checks that never apply from the truth table to keep
it as small as possible.
All checks are tested on the input vector directly before
testing the function. It introduces an additional testing over-
head (e.g., one could at least apply all basic checks ofﬂine
and use the stored results). The reason is that checks have
to be independent of the test values to facilitate the extensi-
bility. For example, one can in this way add new test cases
that might satisfy checks that one has not been aware of.
Beside that, it would be difﬁcult to apply compound checks
ofﬂine. This makes the online evaluation very convenient
because Autocannon might generate new test types depend-
ing on it’s static analysis.
4.1 Check Templates
Our current implementation includes a set of predeﬁned
check templates. (We use “check” to refer to check template
whenever it is obvious that we refer to a check template).
These checks test general properties of the argument values
independent of the concrete function’s argument types. We
start with presenting the basic checks before discussing our
compound checks.
Basic Checks A Pointer is checked if it is NULL, or if it
points to a string, to a readable or read- and writeable buffer.
It is also checked if it points to somewhere on the stack or
on the heap and if it points to a start of a dynamically allo-
cated chunk of memory. Integer values are checked if they
are zero, positive or negative. Strings are checked if they
are ﬁlenames, directory names or format strings (contain-
ing "%n").
Compound Checks Compound checks test
the rela-
tions between more than one argument value.
The
strcpy example in Section 2 contains the compound
check buf write?(dest, strlen(src) + 1). It
relates the size of a buffer (pointed to by dest) and the
length of string (src). All relations that are checks are de-
rived from existing function speciﬁcations (e.g., the C stan-
dard library).
Our current compound checks relate the size of buffers
(pointer arguments) to other function arguments.
It is
checked, if a buffers has at least the size of another string
argument or an integer value. Additionally, the product of
two integer arguments, the sum of two string lengths and
the sum of a string length and an integer are compared to
the buffer size.
4.2 Parameterized Check Templates
Like type templates are used for generating specialized
test values, we need specialized checks for testing these test
values. Parameterized check templates are checks used to
test properties that cannot be predeﬁned because the proper-
ties depend on the function argument types or the functions
semantic.
All parameterized checks but one get their parameters
from static analysis. The one exception depends on the
function’s semantics. It checks if a given buffer is at least
as large as the smallest buffer for which the function did not
crash in the Autocannon experiments. The other checks are:
(1) if the buffer pointed to by a pointer is at least as large as
the data structure of the corresponding argument type. (2)
If an argument type is an enum, the enum values are ex-
tracted from the public sources. It is tested, if an argument
value is within the set of enum values.
The counterpart of the ADT test types, are ADT checks.
Each execution of a constructor function is intercepted. Af-
ter running the original function, the returned handle value
is put into a set. Each ADT has its own set. Every destructor
execution is also intercepted and the passed handle value is
removed from the handle set. The check for handle queries
the corresponding handle set. If the value is in it, it evalu-
ates to true, otherwise to false.
5 Protection Hypotheses
The analysis stage yields a protection hypothesis per li-
brary function. A protection hypothesis of a library func-
tion f is a function from the set of input vectors of f to
{true, f alse}. If it evaluates to true, the given input vec-
tor is considered to be safe. An input vector is safe, if the
library function evaluated on it will behave robust and se-
cure. To enforce that a library function is only executed on
safe input vectors, a protection wrapper is generated.
The protection wrapper is inserted between an applica-
tion and its dynamic linked libraries. Our current imple-
mentation achieves this by utilizing the pre-loading feature
of the dynamic linker [4]. The wrapper could also be in-
serted by instrumenting the libraries or the application.
The ﬁrst part of this section shows how to derive pro-
tection hypotheses and the second part discusses possible
failures of the protection wrapper. There are two kinds: (1)
a hypothesis that rejects a ”safe” input vector is a false posi-
tive and (2) a hypothesis that accepts an “unsafe” input vec-
tor is a false negative.
5.1 Minimizing Truth Table
The protection hypothesis for a function f is derived
from the truth table built while testing f . The truth table
is not yet a boolean function. It might contain redundant
and contradicting rows. Also some rows might be miss-
ing. A row r is redundant if there is another row p with
p = r. Redundant rows are the result of input vectors that
are indistinguishable from each other by our checks and the
functions behavior is the same for both input vectors. Con-
tradicting rows are rows that are classiﬁed equally by the
checks but for which the function behaves differently (i.e.,
one crashes the other does not). They indicate that the set
of checks is too small. Hence, this can be countered by
adding new checks. Note however that removing input vec-
tors from testing reduces the probability to ﬁnd contractions
too. Additionally, some rows might be missing because of
the sampling and because the test types might not exercise
all checks equally.
In our current implementation, we drop redundant rows
before passing them to the minimizer. For contradicting
rows, a warning is logged and only the row for which the
function behaved robustly is passed to the minimizer. This
prevents the protection wrapper from producing a false pos-
itive because input vectors classiﬁed like this contradicting
row are forwarded to the original function. On the other
hand, it introduces the possibility of false negatives. That
is why a protection hypothesis generated from a truth table
containing contradictions might not prevent all crashes or
buffer overruns. It is not known whether the missing rows
behave robustly or not. To minimize false positives, we
treat all missing rows as robust. But this might increase the
number of false negatives. Some minimization strategies
for truth tables trade correctness for small expressions and
computation overhead of the minimization problem. We re-
quire a hypothesis that describes the truth table exactly. But
we do not need necessarily a minimal expression but one
that prevents crashes.
5.2 Discussion
Another source of false negatives and false positives is
if one tests too few input vectors. Simply adding new test
types may not help: the resulting set of input vectors might
become too large to test it within a feasible time. Counter-
intuitively, we have found that reducing the set of input vec-
tors is a good approach if the set of input vectors is too
large. It increases the coverage of the tested sample of input
vectors. Of course, the deduction algorithm should only re-
move input vectors that do not contribute to the protection
hypothesis (see Section 3.6).
If the protection wrapper evaluates a protection hypoth-
esis on an input vector to false, the original function is not
called. Instead the wrapper passes the control back to the
caller. Therefore, a value has to be returned. Our current
implementation returns a predeﬁned error value depending
on the return type of the wrapped function [14]. We are also
exploring an approach to learn error return values of library
functions [16].
Because our approach does not guarantee the absence of
false positives, we discuss how to handle them. Note that
in case that a protection hypotheses is evaluated to false,
the protection wrapper returns an error code instead of call-
ing the library function. First, if the application is robust
enough, it might perform some graceful degradation in the
presence of a false positive. Second, if an applications per-
forms retries of failed library calls and we are mainly wor-
ried about transient errors, we could use a similar approach
as [13]: on re-execution of a failed function, we could allow
its execution and if it passes, we white list arguments with
the same check vector.
The false positives are a sign that we do not have enough
checks to distinguish between unrobust and robust input
vectors. But our approach allows us to add more checks in
future without any redesign. The false negative rate might
also beneﬁt from new checks. For the same reason it is use-
ful to add more test types. But as long as a function is only
tested on a small sample of all test types we need better
strategies to choose the ”right” input vectors for fault injec-
tion.
6 Evaluation
We have evaluated our approach by hardening the
Apache Portable Runtime library (APR) [1]. Among the
applications that use this library is the web server Apache.
We have tested our protection wrapper for the APR by ex-
ecuting it with Apache. To reduce the overhead, we only
have hardened functions that are called by our executions
of Apache. These are 148 APR functions including some
without arguments. Our test system, Autocannon, was able
to perform about 1000 tests per minute on our computers.
Because functions can be tested independently from each
other, it is possible to parallelize the analysis stage. We
have done all experiments on two virtual machines with the
same conﬁguration (running on an Athlon 64 3200+ with
1 GByte of RAM and a Intel Core Duo with 2 GBytes of
RAM, respectively, both with Ubuntu 6.06).
First, we discuss the results of Autocannon’s fault injec-
tion tests. We will focus on test coverage and the results
of the dependability benchmark. To check the correctness
of hypotheses, we did macro and micro benchmarks. The
latter one uses bit-ﬂips to perform fault injections.
Some of the hardened functions have more than 3 argu-
ments. Because the set of input vectors grows exponentially
with the number of arguments, such functions can only be
dynamically analyzed to a very small extent. We tested all
functions on at most 10, 000 test cases and less if the set
of input vectors was smaller. This number of test cases
was chosen to perform our evaluation in a feasible time. In
Figure 5 we depict the coverage for four different test con-
ﬁgurations. We tested the combinations with and without
test types for handles and with and without reduction of test
types by excluding special test types via static analysis. The
functions are grouped into 15 coverage classes. The cover-
age of a function is the number of tested input vectors over
the size of the set of all input vector of the test system for
this functions. The class of a function f is derived from the
s
n
o
i
t
c
n
u
F
#
 50
 40
 30
 20
 10
 0