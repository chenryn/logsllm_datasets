ically remote hosts are identiﬁed through domain names, so
the host must resolve the name using the domain name ser-
vice. Once DNS resolves the IP address of the destination,
the host can begin sending actual data.
We begin by considering the delay associated with end-system
startup. In an attempt to isolate those stations that are truly starting
up—as opposed to simply re-associating after a period of idleness—
we deﬁne a set of candidate selection rules. A station is deemed to
be starting up if the ﬁrst packet we see from it is a scan request.
Because we are interested in the behavior of clients that should be
able to use the network, we only consider stations that eventually
succeed in associating with one of our access points and send at
least one TCP packet.
How long are these delays? Figure 11 shows the distribution of
startup times for those clients that do successfully connect to our
network. There are three curves; “First to TCP” is the total wall-
clock time from the ﬁrst probe request to the ﬁrst TCP segment.
Surprisingly, most hosts take more than ten seconds before they
begin communicating on the network, and the average host takes
almost a minute. We conjecture, however, that the bulk of that time
is spent idling—meaning the machine is not actively trying to make
progress towards sending data.
To validate our conjecture, we attempt to determine if each suc-
cessive span was successful or not—if successful, the time between
spans is likely due to delays on the end host. In contrast, we as-
sume that time between failed spans is due to some sort of network
timeout. We deﬁne a scan to be successful if it is not followed by
a subsequent scan; association, DHCP, and DNS are successful if
the last packet in the span was outgoing from base station to client
(i.e., an ACK). The “without OS delay” line removes estimated OS
delays from the measured startup latency by subtracting idle time
between successful spans under the presumption that any delay in
initiating the subsequent span is due to the end host (i.e., the oper-
ating system has not yet initialized the network stack).
The average host spends almost eight seconds idling, presumably
because the operating system is booting or resuming from power-
save mode. Interestingly, however, if we sum only the duration of
successful spans, we observe that the average host spends over 20
seconds during or after unsuccessful spans. The “good only” line
represents a best-case scenario, with no idle time between stages.
The question, then, is what’s going wrong—why the big gap be-
tween optimal and common case? To address this question, we
ﬁrst examine the successful spans.
Even the successful spans take a non-trivial amount of time. Fig-
ure 12 shows the breakdown of the various stages in the startup
process. This breakdown uses span durations only, and ignores the
time between spans. (Summing all curves from this graph together
yields the “Good only” curve from above.) Clients spend the vast
majority of this time scanning for an appropriate access point with
which to associate. Association itself generally takes less than 10
ms, at which point communication with hosts beyond the access
point can begin. DHCP, on the other hand, because it depends on
a remote server, can take a variable amount of time. We will ex-
pand on the performance of DHCP in our environment in the next
section. For now, however, we note it generally takes somewhere
between 10 ms and ﬁve seconds to obtain an IP address.
Surprisingly, ARP, while frequently fast, takes longer than one
second in more than half the cases. This delay results because most
stations issue an “ARP to self”—an ARP “who has” request for
their own IP address—to ensure no other station is using that IP ad-
dress before they begin communication. By design, such an ARP
request must timeout, hence the one-second delay. Note that some
100%
80%
60%
40%
20%
s
n
o
i
t
a
t
s
f
o
#
0%
 1e-04 0.001 0.01  0.1
Scan
Associate
ARP
DHCP
DNS
 1
 10  100  1000 10000 100000
Total time, seconds
100%
80%
60%
40%
20%
s
n
o
i
t
c
a
s
n
a
r
t
P
C
H
D
f
o
n
o
i
t
c
a
r
F
0%
 1e-04 0.001  0.01  0.1
 100  1000  10000
all (611 trans.)
apple (140 trans.)
windows (393 trans.)
 1
 10
Time (seconds)
Figure 13: CDF of delays experienced by 802.11 clients due to
timeouts.
Figure 14: Distribution of DHCP transaction durations for an
entire day.
DHCP Transactions
Client had no known current lease
Client had used 25% of current lease
Client newly associated shortly before
Client re-associated shortly before
No valid reason determined
611 ( 100%)
204 ( 33.39%)
288 ( 47.14%)
193 ( 31.59%)
56 ( 9.17%)
76 ( 12.44%)
Table 3: Potential reasons why clients initiated DHCP transac-
tions over a day. For some DHCP transactions multiple poten-
tial reasons exist.
graphs start at greater than 0%; the clients not shown on the graph
do not send those packets during startup. For example, over 20%
of hosts do not issue a DNS query before starting a TCP connec-
tion, presumably because they are communicating with a manually
speciﬁed IP address or because the corresponding DNS entry was
previously cached.
Returning to unsuccessful spans, we observe that timeouts can be
quite expensive. Figure 13 shows that while some stages, like DNS
and association, frequently timeout in about 10 ms, they can take
tens of seconds to complete in the worst case. The minimum DHCP
timeout appears to be 100 ms and goes up from there. Failed scans
are extremely expensive (a minimum of seven seconds) because
a failed scan probably means there are no desirable access points
in range. In this situation, it makes no sense to retry after short
timeout, and most stations appear to wait for at least ten seconds
before re-scanning the network. More interestingly, some hosts
continue to scan for extremely long periods of time, presumably
because they never ﬁnd an AP they wish to join; i.e., they’re looking
for a non-existent SSID.
6.4 Dynamic address assignment
Finally, we model dynamic address assignment using DHCP.
DHCP is an inherent aspect of most 802.11 wireless networks. It
is convenient for both users and network administrators, but the re-
sults above also indicate that DHCP can potentially impose notice-
able and annoying delays to wireless users who desire and expect
to be able to use the network quickly.
Clients initiate DHCP transactions for a variety of reasons: their
last least expired (or they had none), their existing lease is start-
ing to expire (conservatively, when 75% of their current lease time
remains), they associate with a new AP, or they re-associate with
a previous AP. For instance, Table 3 shows a breakdown of the
reasons why clients in our building initiate DHCP transactions for
an entire typical weekday of use. The dominant reason for DHCP
transactions are clients contacting the server to start the lease re-
newal process. The vast majority of leases in our network are for
three hours, so stable clients, once connected, initiate DHCP trans-
actions throughout the day.
How long are DHCP transactions? The “all” line in Figure 14
plots the distribution of the duration of DHCP transactions for a
typical weekday in the building. These results show that the major-
ity of transactions complete in a reasonable amount of time: 75%
of transactions complete in under six seconds. Users experiencing
longer delays, however, are likely to be annoyed. On this day, over
10% of the DHCP transactions took longer than a minute to com-
plete; for users connecting to the network for the ﬁrst time that day,
such a delay is quite noticeable.
Sometimes users wonder whether wireless behavior depends upon
their operating system. Based on well-known Ethernet vendor codes
for MAC addresses and the “Vendor Class” option in the DHCP
protocol, we can determine the manufacturer of the operating sys-
tem and networking hardware for almost all of the 186 stations in
the trace. For comparison, we group the various versions of Mi-
crosoft Windows as “Windows” (118 stations) and hardware man-
ufactured by Apple as “Apple” (51 stations) and show distribu-
tions for these groups as well. Apple clients consistently experi-
ence longer DHCP transactions than Windows clients. Apple hosts
running OS X use the Zeroconf standard by default, which causes
them to spend an additional ten seconds on startup. These clients
optimistically attempt to renew their most recent lease (frequently
from a private network at the user’s home), which is invalid in the
campus building environment.
7. CONCLUSION
Modern enterprise networks are of sufﬁcient complexity that even
simple faults can be difﬁcult to diagnose — let alone transient out-
ages or service degradations. Nowhere is this problem more appar-
ent than in the 802.11-based wireless access networks now ubiqui-
tous in the enterprise. We believe that such diagnosis must be auto-
mated, and that networks must eventually address transient failure
without human involvement. As a ﬁrst step in this direction, we
have developed a set of models that take as input wireless trace
data and can then accurately determine the impact of protocol be-
havior from the physical layer to the transport layer on transmis-
sions in the trace. While some sources of delay can be directly
measured, many of the delay components, such as AP queuing,
backoffs, contention, etc., must be inferred. To infer these delays
from measurements, we develop a detailed model of MAC protocol
behavior, both as it is described in the 802.11 speciﬁcation as well
as how it is implemented in vendor hardware. We also explore an
inherent class of overheads due to mobility management in 802.11
networks, including scanning for access points, association, ARP,
DHCP, authentication, etc. To demonstrate the effectiveness of our
models, we investigate the causes of transient performance prob-
lems from traces of wireless trafﬁc in a four-story ofﬁce building.
We ﬁnd that no one anomaly, failure or interaction is singularly re-
sponsible for these issues and that a holistic analysis may in fact
be necessary to cover the range of problems experienced in real
networks.
Acknowledgments
We would like to thank a number of people for their contribu-
tions to this project. Lou Forbis dependably assisted us with all
aspects of our wireless production network, and Jim Madden sup-
ported the operational needs of our network measurement efforts.
We would also like to thank our shepherd Aditya Akella for his
insightful feedback and support, and the anonymous reviewers for
their valuable comments. Finally, Michelle Panik provided detailed
feedback and copy-editing of earlier versions of this paper. This
work was supported in part by the UCSD Center for Networked
Systems (CNS), Ericsson, NSF CAREER grant CNS-0347949 and
by U.C. Discovery CoRe grant 01-10099 as a Calit2-sponsored re-
search project.
8. REFERENCES
[1] P. Bahl, J. Padhye, L. Ravindranath, M. Singh, A. Wolman,
and B. Zill. DAIR: A framework for managing enterprise
wireless networks using desktop infrastructure. In
Proceedings of HotNets, Nov. 2005.
[2] A. Balachandran, G. M. Voelker, P. Bahl, and P. V. Rangan.
Characterizing User Behavior and Network Performance in a
Public Wireless LAN. In Proceedings of ACM
SIGMETRICS, June 2002.
[3] P. Barford and M. Crovella. Critical path analysis of TCP
transactions. In Proceedings of the ACM SIGCOMM
Conference, Stockholm, Sweden, Aug. 2000.
[4] R. Chan, J. Padhye, A. Wolman, and B. Zill. A
location-based management system for enterprise wireless
LANs. In Proceedings of NSDI, Mar. 2007.
[5] R. Chandra, V. Padmanabhan, and M. Zhang. Wiﬁproﬁler:
Cooperative diagnosis in wireless LANs. In Proceedings of
MobiSys, June 2006.
[6] P. Chatzimisios, A. C. Boucouvalas, and V. Vitsas.
Performance analysis of the IEEE 802.11 MAC protocol for
wireless LANs. Wiley International Journal of
Communication Systems, 18(6):545–569, June 2005.
[7] Y.-C. Cheng, J. Bellardo, P. Benko, A. C. Snoeren, G. M.
Voelker, and S. Savage. Jigsaw: Solving the puzzle of
enterprise 802.11 analysis. In Proceedings of the ACM
SIGCOMM Conference, Pisa, Italy, Sept. 2006.
[8] E. Daley. Enterprise LAN Grows Up, 2005. http:
//www2.cio.com/analyst/report3401.html.
[9] K. N. Gopinath, P. Bhagwat, and K. Gopinath. An empirical
analysis of heterogeneity in IEEE 802.11 MAC protocol
implementations and its implications. In Proceedings of
WiNTECH, 2006.
[10] T. Henderson, D. Kotz, and I. Abyzov. The Changing Usage
of a Mature Campus-wide Wireless Network. In Proceedings
of ACM Mobicom, Sept. 2004.
[11] IEEE Computer Society LAN MAN Standards Committee.
IEEE Standard 802.11, Wireless LAN Media Access Control
(MAC) and Physical Layer (PHY) Speciﬁcations, 1999.
[12] A. P. Jardosh, K. N. Ramachandran, K. C. Almeroth, and
E. M. Belding-Royer. Understanding Congestion in IEEE
802.11b Wireless Networks. In Proceedings of ACM IMC,
Oct. 2005.
[13] A. P. Jardosh, K. N. Ramachandran, K. C. Almeroth, and
E. M. Belding-Royer. Understanding Link-Layer Behavior in
Highly Congested IEEE 802.11b Wireless Networks. In
Proceedings of ACM E-WIND, Aug. 2005.
[14] J. Jun, P. Peddabachagari, and M. Sichitiu. Theoretical
maximum throughput of IEEE 802.11 and its applications. In
Proceedings of IEEE International Symposium on Network
Computing and Applications, Apr. 2003.
[15] A. Kochut, A. Vasan, A. U. Shankar, and A. Agrawala.
Snifﬁng out the correct physical layer capture model in
802.11b. In Proceedings of ICNP, 2004.
[16] D. Kotz and K. Essien. Analysis of a Campus-wide Wireless
Network. In Proceedings of ACM Mobicom, Sept. 2002.
[17] B.-J. Kwak, N.-O. Song, and L. E. Miller. Performance
analysis of exponential backoff. IEEE/ACM Transactions on
Networking, 13(2), Apr. 2005.
[18] R. Mahajan, M. Rodrig, D. Wetherall, and J. Zahorjan.
Analyzing the MAC-level Behavior of Wireless Networks in
the Wild. In Proceedings of ACM SIGCOMM, Sept. 2006.
[19] A. Mishra, M. Shin, and W. Arbaugh. An Empirical Analysis
of the IEEE 802.11 MAC Layer Handoff Process. ACM
Computer Communications Review, 33(2), 2003.
[20] J. Padhye, V. Firoiu, D. Towsley, and J. Kurose. Modeling
TCP Reno performance: A simple model and its empirical
validation. IEEE/ACM Transactions on Networking, Apr.
2000.
[21] S. Rewaskar, J. Kaur, and F. D. Smith. A passive
state-machine approach for accurate analysis of TCP
out-of-sequence segments. ACM Computer Communication
Review, 36(3), 2006.
[22] A. Sheth, C. Doerr, D. Grunwald, R. Han, and D. Sicker.
MOJO: A distributed physical layer anomaly detection
system for 802.11 WLANs. In Proceedings of MobiSys,
pages 191–204, June 2006.
[23] J. Yeo, M. Youssef, and A. Agrawala. A Framework for
Wireless LAN Monitoring and its Applications. In
Proceedings of ACM WiSe, Oct. 2004.