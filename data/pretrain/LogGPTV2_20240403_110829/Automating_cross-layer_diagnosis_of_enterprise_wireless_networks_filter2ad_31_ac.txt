frames sent in succession, the AP ﬁrst waits a mandatory backoff
delay. The mandatory backoff delay is bo · 20µs, where the Avaya
AP randomly chooses the integer slot bo between 0–15 for 802.11g.
After the mandatory backoff, the AP will start a regular DCF oper-
ation. First it listens on the channel for the DIFS interval (50 µs). If
the channel is idle, the AP transmits the frame immediately. In this
case, dt0 is the mandatory backoff delay plus the DIFS delay. Dur-
ing the backoff, the AP defers decrementing bo until the channel
becomes idle for DIFS. Therefore the backoff delay depends on a
combination of bo and the channel contention the AP experienced.
The distribution of access delays shown in Figure 4 reﬂects the
various components that comprise the overall access delay dt0. Ev-
ery frame must wait at least a DIFS interval during the transmit
process; hence, the distribution starts at a delay of 50 µs marked
by the ﬁrst vertical line. The “steps” immediately following cor-
respond to the mandatory backoff delay that does not experience
any contention. The frames have a DIFS delay plus the mandatory
backoff delay, a random multiple of 20 µs slots from 0–15; each
step in the graph corresponds to one of the slots. The second ver-
tical line (X = 50 + 15 ∗ 20) marks the end of this category of
frames (about 60% of the frames transmitted).
The next group of frames (through 847 µs) are frames experienc-
ing contention during the mandatory backoffs. The contention they
experienced is mostly due to TCP-ACK packets from the client to
the AP. The PHY transmission time of these packets is 447 µs. As
a result, the backoff incurs an additional 447 + DIFS = 497 µs con-
tention delay — hence the second set of “stairs” that starts at DIFS
(from step 1) + 497 = 547 µs and ends at 547 + 15 ∗ 20 = 847µs.
The second set of stairs is not as pronounced because the sender
may experience different lengths of contention delays. The remain-
ing 10% of transmitted frames with the largest delays are frames
that experienced longer contention delays or performed a regular
backoff in step 3 of Figure 3.
 100%
 80%
s
e
g
n
a
h
c
x
E
e
m
a
r
F
f
o
n
o
 60%
 40%
i
t
c
a
r
F
 20%
 0
 0
 200
 400
 600
 800
 1000
Microsecond
Figure 4: Access delay (dt0) distribution of one hour of head-
of-line blocked frame exchanges from an Avaya AP-8 AP to a
client doing a bulk TCP download.
s
e
g
n
a
h
c
x
E
e
m
a
r
F
f
o
n
o
i
t
c
a
r
F
100 %
80 %
60 %
40 %
20 %
0 %
no contention
all
contention
 0
 200
 400
 600
 800
 1000
 1200
Microsecond
Figure 5: Access delay (dt0) distribution of head-of-line blocked
frame exchanges from a Cisco Aironet 350 AP to a client doing
a bulk scp download.
Notice in the ﬁrst set of stairs that the later steps tend to be shorter
than earlier steps, but the second set of stairs show the opposite
pattern. This behavior is because, having chosen a larger bo value,
the sender is more likely to lose the contention and to have to wait
for the winner to ﬁnish its transmission. The probability of getting
interrupted during mandatory backoff is about 40% (the percentage
of frame exchanges experiencing congestion in the analyzed ﬂow),
which roughly corresponds to the delay ACK policy in TCP (send
ACK on every other TCP-DATA).
Next, we perform a similar analysis using an AP from a different
vendor to show that our approach is not tied to the implementation
of a particular vendor. Since we could not replace an Avaya AP
in our production network with an AP from another vendor, we in-
stead performed a controlled experiment using a Cisco Aironet 350
AP. We downloaded a large ﬁle using scp to a client connected via
the Cisco AP using 802.11b/g, and we plot the dt0 delay distribu-
tion as the “all” line in Figure 5.
At ﬁrst glance the distribution looks dramatically different from
the Avaya’s distribution in Figure 4. But, in fact, it reﬂects the same
802.11 sender transmission process, albeit using different parame-
ters. The parameters differ from Avaya because the Cisco AP only
used 802.11b, so its minimum contention window, CWmin, is 31
instead of the 15 used by 802.11g. As above, the distribution is a
mix of two kinds of frame exchanges. The ﬁrst kind are frame ex-
changes that experience contention during the mandatory backoff.
If we detect that the AP has acknowledged some frames (mainly
TCP-ACKs from the client) during the mandatory backoff in step
5, we label the frame exchanges as having contention and plot the
distribution as the “contention” line in Figure 5. The line forms
a set of stairs starting from around 400 µs. This offset is exactly
DIFS plus the pause during backoff to wait for a TCP-ACK trans-
mission. Otherwise, we plot the remaining frame exchange delays
in the “no contention” line, which forms another set of 31 steps
starting at DIFS. If we aggregate these two distributions, it forms
the “all” line analogous to the curve shown in Figure 4.
We have also performed a similar experiment with the Avaya
AP-8 where we change the slot time to the short slot time (9 µs)
instead of the regular slot time (20 µs). The distribution changes
(the width of a stair) accordingly.
In summary, even though we must infer the timing of some of
the events that determine critical path delays, our experience has
been that our media access model is consistent with 802.11 oper-
ation even for very ﬁne-grained phenomena. Furthermore, we can
apply our model to different vendor APs, parameterized accord-
ingly. Fortunately, these parameters are straightforward to obtain.
The model requires 802.11 parameters like minimum contention
window, maximum contention window, and slot time, all of which
can be found in the AP manual or conﬁguration GUI to correctly
parameterize the model for a deployment.
5.4 Applying the model
The media access model makes it possible to measure the critical
path delays for every packet sent from APs to the client. As an
example, we focus on a particular AP in the building where three
clients (Xb, Yb, Zg) are using TCP to download different ﬁles from
the same Internet server, and the downloads overlap in time. The
clients compete with each other for both AP resources and air time.
Two clients use 802.11b (Xb, Yb) and the third uses 802.11g (Zg).
We apply the media access model to Yb’s TCP ﬂow to measure
the critical path delays for each of the packets sent from the AP
to the client. Figure 6 shows the delay breakdown for this client’s
packets over four minutes. Each spike in the graph corresponds to
the combined queuing and wireless transmission delays for trans-
mitting one frame. The MAC delay, dmac, is quite small (even with
contention among three clients) and are shown at the top tip of each
spike. We break down the queuing delay into three components:
“other” is the delay dqo waiting for frames to other clients to leave
the queue; “self” is the delay dqs waiting for frames to this client;
and “background” is the delay dqb waiting for background manage-
ment frames (beacons, scans, etc.). Overlayed across the spikes is
the TCP goodput achieved by the client. Above the spikes we show
points in time where a frame was lost during wireless transmission
(triangles) and on the Internet (diamonds).
This detailed breakdown shows a number of interesting interac-
tions and behavior. First, queuing delay in the AP is the dominant
delay on the wireless path to the client. These delays are orders
of magnitude larger than the wireless transmission delay. Second,
roughly half of the time client Yb’s frames were queued for its own
frames, and the other half was caused by delays encountered by
frames for the other two clients. Examining the frame delays of
the other clients, most of those other frames were for client Xb and
the minority were for Zg. Third, Yb experiences occasional wire-
less loss, but wireless loss does not have a substantial impact on
achieved goodput. Fourth, Yb experiences a burst of Internet loss
at 14:39:38, substantially impacting goodput. The AP queue drains
as Yb times out and recovers. Finally, Yb’s download goes through
a phase change just after 14:40:00. The other clients ﬁnish down-
loading (the frames in the AP queue are for Yb) and Yb no longer
has to share the channel. AP queue occupancies drop and goodput
increases substantially beyond the level when it was contending
with other clients.
5.5 TCP throughput
Next we describe how we can use the media access model as
a basis for diagnosing problems with TCP throughput for wire-
less users, and show that there can be many causes that can limit
TCP throughput. Given a TCP ﬂow using wireless, we ﬁrst iden-
tify whether the TCP ﬂow is attempting to transmit and maximum
speed. We then examine the ﬂow to determine whether throughput
performance appears to be limited by wireless network conditions.
If so, we use the media access model to determine critical path de-
lays for packets in the ﬂow, evaluate how those delays interact with
TCP, and assign a root cause for why the TCP ﬂow throughput was
limited when using the wireless network. Our goal to is to show
the systems administrator the distribution of potential performance
problems so they can focus on improving the major bottlenecks.
The ﬁrst step is to determine whether a TCP ﬂow contained data
transfer periods whose throughput could be limited by wireless
conditions. Since a given TCP ﬂow may have idle periods (e.g.,
think times during persistent HTTP connections), we identify pe-
riods of time during a TCP ﬂow when it is performing a bulk data
transfer. We call such a period a TCP transaction. A TCP transac-
tion period starts when we observe new, unacknowledged TCP data
and ends when all outstanding data packets have been acknowl-
edged. Most of the packets in this period must also be MSS-sized
except for the last data packet, reﬂecting a period when a bulk of
data is being sent. We calculate the amount of data transferred dur-
ing the ﬂow to identify transactions of sufﬁcient size that they could
potentially take full advantage of the wireless channel; we currently
use a threshold of 150 KB.
We then take these transactions and determine whether through-
put performance appears to be limited by wireless network condi-
tions, and, if so, why. In our approach, we assume that there is
a single root cause and that factors are largely independent (e.g.,
wireless loss is independent of Internet loss). We then analyze the
transaction through a series of ﬁlters. First, if the transaction is
achieving near optimal throughput for the 802.11 rate used, we la-
bel it ideal and perform no further analysis. Additionally, if the
client ever announces a zero-sized receiver window during the bulk
transfer, we label it as receiver window limited.
Next, we model the TCP throughput by extracting the network
and host conditions. We use TCP throughput estimation analy-
sis [20] to perform this estimation, calculating idealized throughput
from the measured path RTT, measured path loss rate, and an esti-
mated RTO. We ﬁne-tune the throughput model by identifying the
client OS from their DHCP messages and applying OS dependent
TCP parameters [21]. To ensure the throughput model works for
a particular transaction, we compare the modeled throughput with
the actual throughput measured. We proceed only if the modeled
throughput is within 10% of the actual throughput. To determine if
the wired portion of the connection is the bottleneck, we estimate
what the TCP throughput for the transaction might have been if the
client was directly connected to the Ethernet by removing the wire-
less losses and wireless RTT. If the estimated throughput improves
by more than 20% compared to the measured throughput, we label
the transaction as Internet limited.
If the Internet is not the limit, we examine if wireless loss ex-
ported to the TCP layer is the root cause by adding wireless loss
Wireless Loss
Internet Loss
MAC delay
Q data(other)
Q data(self)
Q background
Goodput
 1200
 1000
 800
 600
 400
 200
s
m
 700
 600
 500
 400
 300
 200
 100
s
p
B
K
 0
14:37:00
14:37:30
14:38:00
14:38:30
14:39:00
time
14:39:30
14:40:00
14:40:30
 0
14:41:00
Figure 6: Critical path delays, goodput, and losses over time for frame exchanges from an AP to a client doing a bulk TCP download.
30 %
25 %
20 %
15 %
10 %
5 %
0 %
Recv
Window
Internet
Ideal
Wrls
Loss
QD
BG
QD
Other
User
QD
Same
User
Prot.
Mode
Exp
Backoff
Misc
Figure 7: Root causes that limit TCP ﬂow throughput.
rate into the throughput estimation; if throughput drops over 20%,