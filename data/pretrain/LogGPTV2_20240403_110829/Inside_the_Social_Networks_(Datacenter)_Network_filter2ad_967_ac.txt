sentially ﬂat and unchanging. In order to further investigate
the cause and particulars of this stability, we turn our atten-
tion to the trafﬁc matrix itself.
Locality
Rack
Cluster
DC
Inter-DC
All
12.9
57.5
11.9
17.7
Percentage
Hadoop
13.3
80.9
3.3
2.5
23.7
FE
2.7
81.3
7.3
8.6
21.5
Svc. Cache
12.1
56.3
15.7
15.9
18.0
0.2
13.0
40.7
16.1
10.2
DB
0
30.7
34.5
34.8
5.2
Table 3: Different clusters have different localities; last row
shows each cluster’s contribution to total network trafﬁc
4.3 Trafﬁc matrix
In light of the surprising lack of rack locality and high
degree of trafﬁc stability, we examine trafﬁc from the more
long-term and zoomed-out perspective provided by Fbﬂow.
Table 3 shows the locality of trafﬁc generated by all of
Facebook’s machines during a 24-hour period in January
2015 as reported by Fbﬂow. Facebook’s trafﬁc patterns re-
main stable day-over-day—unlike the datacenter studied by
Delimitrou et al. [17]. The clear majority of trafﬁc is intra-
cluster but not intra-rack (i.e., the 12.9% of trafﬁc that stays
within a rack is not counted in the 57.5% of trafﬁc labeled as
intra-cluster). Moreover, more trafﬁc crosses between data-
centers than stays within a rack.
Table 3 further breaks down the locality of trafﬁc gener-
ated by the top-ﬁve cluster types which, together, account for
78.6% of the trafﬁc in Facebook’s network. Hadoop clusters
generate the most trafﬁc (23.7% of all trafﬁc), and are sig-
niﬁcantly more rack-local than others, but even its trafﬁc is
far from the 40–80% rack-local reported in the literature [12,
20406080100Time (seconds)050010001500200025003000350040004500MbpsInter-DatacenterIntra-DatacenterIntra-ClusterIntra-Rack20406080100Time (seconds)05001000150020002500MbpsInter-DatacenterIntra-DatacenterIntra-ClusterIntra-Rack20406080100Time (seconds)02004006008001000MbpsInter-DatacenterIntra-DatacenterIntra-ClusterIntra-Rack20406080100Time (seconds)0100200300400500600MbpsInter-DatacenterIntra-DatacenterIntra-ClusterIntra-Rack128(a) Rack-to-rack, Hadoop cluster
(b) Rack-to-rack, Frontend cluster
(c) Cluster-to-cluster
Figure 5: Trafﬁc demand by source (x axis) and destination (y axis). The graphs are each normalized to the lowest demand in
that graph type (i.e., the Hadoop and Frontend clusters are normalized to the same value, while the cluster-to-cluster graph is
normalized independently).
17]. Rather, Hadoop trafﬁc is clearly cluster local. Frontend
(FE) trafﬁc is cluster local by design, but not very rack-local,
and the locality of a given rack’s trafﬁc depends on its con-
stituent servers (e.g., Web server, Multifeed, or cache).
This distinction is clearly visualized in Figure 5, gener-
ated in the style of Delimitrou et al. [17]. The two left por-
tions of the ﬁgure graph the relative trafﬁc demands between
64 racks within clusters of two different types. While we
show only a subset of the total set of racks in each cluster,
the pattern is representative of the cluster as a whole.
Trafﬁc within the Hadoop cluster (left) is homogenous
with a very strong diagonal (i.e., intra-rack locality). The
cluster-wide uniformity outside the local rack accounts
for intra-cluster trafﬁc representing over 80% of Hadoop
trafﬁc—even though trafﬁc to the local rack dominates any
given other rack in isolation. Map tasks are placed to maxi-
mize read locality, but there are a large number of concurrent
jobs which means that it is possible that any given job will
not ﬁt entirely within a rack. Thus, some amount of trafﬁc
would necessarily need to leave the rack during the shufﬂe
and output phases of a MapReduce job. In addition, the clus-
ter serves data requests from other services which might not
strive for as much read locality, which would also contribute
to reduced overall rack locality.
The Frontend cluster (center) exhibits three different pat-
terns according to rack type, with none being particularly
rack-local.
In particular, we see a strong bipartite trafﬁc
pattern between the Web servers and the cache followers in
Webserver racks that are responsible for most of the trafﬁc,
by volume, within the cluster. This pattern is a consequence
of placement: Web servers talk primarily to cache servers
and vice versa, and servers of different types are deployed in
distinct racks, leading to low intra-rack trafﬁc.
This striking difference in Facebook’s locality compared
to previously studied Internet-facing user-driven applica-
tions is a consequence of the realities of serving a densely
connected social graph. Cache objects are replicated across
clusters; however, each object typically appears once in a
cluster (though hot objects are replicated to avoid hotspots,
which we discuss in Section 5). Since each Web server needs
to be able to handle any request, they might need to access
data in a potentially random fashion due to load balancing.
To make this argument more concrete, loading the Face-
book news feed draws from a vast array of different ob-
jects in the social graph: different people, relationships, and
events comprise a large graph interconnected in a compli-
cated fashion. This connectedness means that the working
set is unlikely to reduce even if users are partitioned; the
net result is a low cache hit rate within the rack, leading
to high intra-cluster trafﬁc locality.
In addition, partition-
ing the graph such that users and their data are co-located
on racks has the potential to introduce failure modes which
disproportionately target subsets of the user base, leading to
a suboptimal experience.
The other three cluster types exhibit additional distinctive
behaviors (not shown). Trafﬁc in cache leader clusters, for
example, has very little intra-rack demand, instead spread-
ing the plurality of its trafﬁc across the datacenter. Trafﬁc in
back-end database clusters is the most uniform, divided al-
most evenly amongst nodes within the cluster, the same dat-
acenter, and worldwide. Service clusters, which host racks
supporting a variety of supporting services, exhibit a mixed
trafﬁc pattern that lies between these extreme points.
Inter-cluster communication varies considerably by clus-
ter type. Figure 5c plots the trafﬁc demand between 15
clusters within a single datacenter for the a 24-hour period.
Hadoop clusters, for example, have a very small propor-
tion of inter-cluster trafﬁc, while cache leader clusters have
a large amount of inter-cluster trafﬁc, split between cache
followers in other clusters and database clusters. While each
cluster may possess the same four-post structure internally, it
may make sense to consider heterogenous inter-cluster com-
munication fabrics, as demand varies over more than seven
orders of magnitude between cluster pairs.
While the 4-post cluster remains prevalent in Facebook
datacenters, Facebook recently announced a new network
131765Cache (~20%)Multifeed (Few)Webservers (~75%)100101102103104105106107129(a) Web servers
(b) Cache follower
(c) Hadoop
Figure 6: Flow size distribution, broken down by location of destination
(a) Web servers
(b) Cache follower
(c) Hadoop
Figure 7: Flow duration distribution, broken down by location of destination
topology that is being implemented in datacenters going for-
ward [9]. While servers are no longer grouped into clusters
physically (instead, they comprise pods where all pods in a
datacenter have high connectivity), the high-level logical no-
tion of a cluster for server management purposes still exists
to ease the transition. Accordingly, the rack-to-rack trafﬁc
matrix of a Frontend “cluster” inside one of the new Fabric
datacenters over a day-long period (not shown) looks similar
that shown in Figure 5.
4.4
Implications for connection fabrics
The low utilization levels found at the edge of the network
reinforce common practice of oversubscribing the aggrega-
tion and core of the network, although it remains to be seen
whether utilization will creep up as the datacenters age. The
highly contrasting locality properties of the different clus-
ters imply a single homogenous topology will either be over-
provisioned in some regions or congested in others—or both.
This reality argues that non-uniform fabric technologies that
can deliver higher bandwidth to certain locations than others
may ﬁnd use. Researchers are exploring techniques to ame-
liorate trafﬁc hotspots. The stability of the trafﬁc patterns we
observe, however, suggest that rapid reconﬁgurability may
not be as necessary as some have assumed.
Somewhat surprisingly, the lack of signiﬁcant levels of
intra-rack locality (except in the Hadoop cluster) hints that
RSWs (i.e., top-of-rack switches) that deliver something less
than full non-blocking line-rate connectivity between all of
their ports may be viable. In particular, the bipartite trafﬁc
pattern between end hosts and RSW uplinks may afford op-
timizations in switch design. We return to consider further
implications for switch design in Section 6.
5. TRAFFIC ENGINEERING
Prior studies suggest that the stability of datacenter traf-
ﬁc depends on the timescale of observation. In this section,
we analyze Facebook’s trafﬁc at ﬁne timescales, with an eye
towards understanding how applicable various trafﬁc engi-
neering and load balancing approaches might be under such
conditions.
5.1 Flow characteristics
Figures 6 and 7 plot the size and duration, respectively,
of ﬂows (deﬁned by 5-tuple) collected in 10-minute (2.5-
minute for the Web-server rack) packet traces of three differ-
ent node types: a Web-server rack, a single cache follower
(cache leader is similar to follower and not shown due to
space constraints), and a Hadoop node. We show the overall
distribution (in black) as well as per-destination curves.
Consistent with the literature [26, Fig. 9], we ﬁnd that
most ﬂows in Facebook’s Hadoop cluster are short. As dis-
cussed previously, the trafﬁc demands of Hadoop vary sub-
stantially across nodes and time. We plot the results from
tracing one node over a relatively busy 10-minute interval;
traces from other nodes or even the same node at different
times reveal somewhat different distributions, so we cau-
tion against examining the speciﬁc distribution too carefully.
Even in the graphed interval, however, 70% of ﬂows send
less than 10 KB and last less than 10 seconds; the median
ﬂow sends less than 1 KB and lasts less than a second. Less
than 5% of the ﬂows are larger than 1 MB or last longer than
100 seconds; almost none exceed our 10-minute trace.
Conversely,
traces from other service types are much
more representative due to load balancing. Moreover, many
of Facebook’s internal services use some form of connec-
0.010.11101001000100001000001000000Kilobytes0.00.10.20.30.40.50.60.70.80.91.0CDFIntra-RackIntra-ClusterIntra-DatacenterInter-DatacenterAll0.010.11101001000100001000001000000Kilobytes0.00.10.20.30.40.50.60.70.80.91.0CDFIntra-RackIntra-ClusterIntra-DatacenterInter-DatacenterAll0.010.11101001000100001000001000000Kilobytes0.00.10.20.30.40.50.60.70.80.91.0CDFIntra-RackIntra-ClusterIntra-DatacenterInter-DatacenterAll1101001000100001000001000000Milliseconds0.00.10.20.30.40.50.60.70.80.91.0CDFIntra-RackIntra-ClusterIntra-DatacenterInter-DatacenterAll1101001000100001000001000000Milliseconds0.00.10.20.30.40.50.60.70.80.91.0CDFIntra-RackIntra-ClusterIntra-DatacenterInter-DatacenterAll1101001000100001000001000000Milliseconds0.00.10.20.30.40.50.60.70.80.91.0CDFIntra-RackIntra-ClusterIntra-DatacenterInter-DatacenterAll130(a) Hadoop (rate, each series is 1 second)
(b) Cache (rate, each series is 1 second)
(c) Cache (stability, each series is a rack)
Figure 8: Per-destination-rack ﬂow rate distribution (for both Hadoop and cache) and stability (cache).
opportunity for improvement. In the previous section, we
note that Facebook’s approach to load balancing is highly
effective on timescales lasting minutes to hours, leaving
less room for trafﬁc engineering. We now consider trafﬁc
characteristics over the course of a few seconds to deter-
mine whether trafﬁc engineering might be effective on short
timescales.
Figure 9: Cache follower per-host ﬂow size
tion pooling [29], leading to long-lived connections with
relatively low throughput. Pooling is especially prevalent
for cache follower(leader, not shown) nodes, where only
30(40)% of ﬂows are less than 100 seconds in length, with
more than 40(25)% of ﬂows exceeding our 10-minute cap-
ture period. That said, most ﬂows are active (i.e., actu-
ally transmit packets) only during distinct millisecond-scale
intervals with large intervening gaps.
In other words, re-
gardless of ﬂow size or length, ﬂows tend to be internally
bursty. In general cache ﬂows are also signiﬁcantly larger
than Hadoop; Web servers lie somewhere in the middle.
If we consider higher levels of aggregation, i.e., group-
ing ﬂows by destination host or rack, the distribution of ﬂow
sizes simply shifts to the right for Web servers (retaining its
basic shape). The behavior is starkly different for cache fol-
lowers, however: the wide ﬂow-size distribution apparent at
a 5-tuple granularity (Figure 6b) disappears at host and rack
levels, replaced by a very tight distribution around 1 MB per
host (Figure 9). This arises as a consequence of the deci-
sion to load balance incoming user requests across all Web
servers, combined with the large number of user requests.
Since requests and responses are typically small (on the or-
der of a few kilobytes) we do not observe any imbalance
created by unequal response sizes.
5.2 Load balancing
Existing trafﬁc engineering efforts seek to leverage vari-
ability of trafﬁc; highly regular trafﬁc does not provide much
We consider how the trafﬁc from a host varies from one
second to next. We examine the distribution of ﬂow rates,
aggregated by destination rack, per second over a two-
minute period and compare each second to the next. Intu-
itively, the better the load balancing, the closer one second
appears to the next.
We ﬁrst examine the Hadoop cluster by looking at 120
consecutive 1-second intervals. Figure 8a plots a CDF of
per-destination-rack ﬂow sizes for each interval (i.e., there
are 120 separate curves). While we do not claim this par-
ticular server is representative, it does depict widely varying
rates (i.e., more than three orders of magnitude) which are
common in our observations.
In and of itself, this is unsurprising—Hadoop has peri-
ods of varying network trafﬁc, and a production cluster is
likely to see a myriad jobs of varying sizes. It is this vari-
ability of trafﬁc that existing network trafﬁc engineering
schemes seek to leverage. Orchestra [16] relies on tem-
poral and per-job variation to provide lower task comple-
tion times for high-priority tasks, while Hedera [5] pro-
vides non-interfering route placement for high bandwidth
elephant ﬂows that last for several seconds, which are preva-
lent within Hadoop workloads.
A different story emerges for Frontend trafﬁc, and the
cache in particular. Recall from Table 2 that the largest
share of cache follower trafﬁc are responses bound for Web
servers. Figure 8b shows the distribution of per-second ﬂow
rates on a per-rack basis from a single cache follower node
to distinct Web server racks during a two minute period. The
distributions for each of the 120 seconds are similar, and all
are relatively tight, i.e., the CDFs are fairly vertical about the
median of ≈2 Mbps. Similar patterns (albeit with different
scales) can be observed for other services as well.
From the viewpoint of a single host, each second is sim-
ilar to the next. However, this analysis does not take per-
destination variation into consideration.
It is conceivable
that there could exist consistently high- or low-rate destina-
0.010.11101001000100001000001000000Kilobytes/second0.00.10.20.30.40.50.60.70.80.91.0CDF1001000Kilobytes/second0.00.10.20.30.40.50.60.70.80.91.0CDF0.1110Proportion of Median0.00.10.20.30.40.50.60.70.80.91.0CDF0.010.11101001000100001000001000000Kilobytes0.00.10.20.30.40.50.60.70.80.91.0CDFIntra-RackIntra-ClusterIntra-DatacenterInter-DatacenterAll131(a) Cache follower