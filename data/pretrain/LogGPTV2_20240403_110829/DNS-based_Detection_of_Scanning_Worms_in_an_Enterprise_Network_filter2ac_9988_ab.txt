(see Figure 2).
Network(cid:13)
Prototype(cid:13)
Packet Processing Engine(cid:13)
5-tuple DNS(cid:13)
5-tuple TCP(cid:13)
5-tuple HTTP(cid:13)
5-tuple DNS(cid:13)
5-tuple DNS(cid:13)
DNS Correlation Engine(cid:13)
5-tuple DNS(cid:13)
Connection Candidate Data Structure(cid:13)
5-tuple Connection Candidate(cid:13)
5-tuple Connection Candidate(cid:13)
5-tuple Connection Candidate(cid:13)
5-tuple Connection Candidate(cid:13)
5-tuple TCP(cid:13)
Alert(cid:13)
Alert(cid:13)
Alert(cid:13)
Figure 3. High›level System Design
Figure 3 reveals the high-level design of the prototype.
In this example, the PPE extracts the relevant features
from live network activity and bundles these into data to-
kens. The data tokens are comprised of the appropriate
5-tuple of features (see Section 3.1) based on the protocol
extracted. These tokens are consumed by the DCE. The
DCE uses the tokens to maintain a list of destination IP
addresses it deems valid and checks any new connection
attempts from within the enterprise network against this
list. The DCE will generate an alert if it determines the
new connection is being initated to a destination IP that is
not contained in its list.
3.1 Packet Processing Engine
The PPE is responsible for processing packets of inter-
est from pcap (cid:2)les or live off the network and extracts
a variety of information from several protocols. Specif-
ically, the software must extract relevant features from
new connection attempts, embedded IP addresses within
HTTP packets, and all DNS activity occurring within the
network cell.
In order to discover new TCP connection attempts, all
TCP packets with the SYN (cid:3)ag set are examined. TCP
packets with the SYN only (cid:3)ag set indicate the start of
the three-way handshake that signi(cid:2)es a new connection
attempt. UDP is connectionless and does not have the
concept of a session. Each UDP packet is treated as a dis-
crete event and thus a potential new connection. Feature
extraction for either new TCP connections or non-DNS
UDP datagrams includes the 5-tuple of source IP, source
port, destination IP, destination port, and timestamp.
Packets that contain a source port of 80 or 8080 are cap-
tured and categorized as HTTP packets. All HTTP pack-
ets are decoded and the payload inspected for any em-
bedded IP addresses. Any IP addresses discovered in the
payload are extracted along with the previously de(cid:2)ned
5-tuple.
DNS A records are generated when systems within the
network wish to contact systems in other cells or external
to the network. Any DNS requests originating from the
network cells and any DNS replies coming into the net-
work cells are extracted and decoded. Feature extraction
for DNS datagrams includes the 5-tuple of DNS source IP,
DNS source port, TTL, domain name, and resolved IPv4
address.
3.2 DNS Correlation Engine
The DNS correlation engine (DCE) is responsible for
processing information passed by the PPE. The two ma-
jor functions of the DCE are: (1) to create and maintain a
data structure of IP addresses and associated features that
are considered valid connection candidates; and (2) to val-
idate all new TCP and UDP connection attempts between
cells or to remote systems against the connection candi-
date data structure. A valid connection candidate data
structure is produced by processing DNS A records, em-
bedded IP addresses in HTTP packets, and the whitelist.
Connection Candidate Data Structure. All DNS A
resource record 5-tuples are parsed and added to the con-
nection candidate data structure. The TTL from each 5-
tuple is used just as it is in the cache of a DNS server.
Once the TTL expires, the resource record is purged from
the DCE’s connection candidate list. Although DNS ac-
tivity provides the majority of IP addresses to the connec-
tion candidate data structure, numeric IP addresses within
HTTP packets must also be considered.
As previously discussed, numeric IP addresses are reg-
ularly embedded within HTTP packets. All HTTP 5-
tuples are parsed and added to the connection candidate
data structure. Unlike an IP address provided by DNS A
records, these IP addresses do not have an associated TTL
that can be used to discard the IP address entry from the
connection candidate data structure. We can assume that
a DNS query and response had to occur in order for the
web site to be initially accessed. Therefore, we can use
the TTL of the DNS A record of the original request as
the TTL for the embedded IP address. All IP addresses
harvested from HTTP decoding are then are maintained
in state. That is, the assigned TTL values are respected
and these addresses are valid only as long as the TTL has
not expired.
Whitelists. To address those client applications that le-
gitimately do not rely on DNS, a whitelist is generated. A
whitelist provides a list of IP addresses and port combi-
nations that are exempt from the detection algorithm. For
example, in most networks there are systems that regu-
larly communicate with one another by using IP addresses
speci(cid:2)ed in con(cid:2)guration (cid:2)les rather than fetched in DNS
records. Furthermore, speci(cid:2)c applications and users (see
further discussions below) may also use numeric IP ad-
dresses instead of DNS to access services or communicate
with other systems.
In practice, internal network server communications are
either well-known or easily discovered. If a hard-coded IP
address is contained in a network con(cid:2)guration parameter
or (cid:2)le, it is easily con(cid:2)rmed. These server interactions can
be modeled and the appropriate IP address port combina-
tion added of(cid:3)ine to the whitelist for exclusion. However,
in the case of users, the use of numeric IP addresses may
be more pervasive and more unpredictable. There are two
cases worth discussion. In organizations which impose re-
strictive network security policies, end users are restricted
to using a (cid:2)nite list of well known services deemed per-
missible in the security policies. For instance, it may be
permissable to access FTP and Telnet servers using nu-
meric IP addresses. To accommodate this, the list of fre-
quently accessed FTP servers IP addresses could be added
to the whitelist. Alternatively, so as not to weaken the se-
curity posture of the network, in such environments (e.g.
(cid:2)nancial and government) where an organization has tight
control over its employees, users could be told to enter in
domain names instead of the IP address. The second case,
which may be more problematic for whitelists, involves
end users which enjoy unrestricted or open network secu-
rity policies. In this case, the number of whitelisted prot-
cols may limit the effectiveness of the detector.
The whitelist is granular enough to exempt not only spe-
ci(cid:2)c IP addresses but also provide for IP address and port
pairing. For instance, it is possible to specify that a com-
munication must contain the correct source and destina-
tion IP addresses as well as the correct destination port
in order to match the applicable whitelist entry. Over
time this list will need to be updated in order to re(cid:3)ect
changes to the network, user activity, and new technol-
ogy. The more open a network security policy, the greater
the amount of effort required to maintain the whitelist.
New Connection Validation. The PPE only extracts
the relevant information from a single TCP packet for each
new TCP connection attempt it detects. This includes TCP
SYN packets addressed to systems outside the cell the pro-
totype is monitoring. Once a new TCP connection attempt
is detected, the IP destination address is compared with
the addresses listed in the connection candidate data struc-
ture. If the address is not found and it does not match an
entry in the whitelist, the connection is considered to be
anomalous and an alert is generated.
UDP datagrams are regarded as discrete events. The
PPE extracts the relevant information from the UDP data-
grams and passes this information to the DCE. Once a new
UDP datagram is detected, the IP destination address is
processed as described in the previous paragraph.
Alerts. An alert is generated when a connection attempt
to a system in another cell or remote system is detected for
which there is no associated entry in the connection candi-
date data structure. Multiple connection attempts between
the same two systems within a speci(cid:2)ed time window are
regarded as a single alert. This alert grouping reduces
the number of alerts generated without reducing the rele-
vant warning information to the operator. It is not unusual
for a new TCP session to require a number of connection
attempts before an actual connection can be established.
Systems may be busy, unable or simply unwilling to es-
tablish a session. If a separate alert were generated for
each unsuccessful connection attempt, a single communi-
cation between two systems may generate several alerts.
In regards to UDP, the decision to consider each UDP
datagram as a possible new connection could result in nu-
merous alerts that could quickly overwhelm an operator.
The important intelligence from these alerts is the identi(cid:2)-
cation of the potentially infected system and the intended
victim. The fact that it took the worm multiple connec-
tion attempts or datagrams to infect the system does not
aid in our propagation detection. The timestamp from the
(cid:2)rst TCP SYN packet or UDP datagram that generated an
alert is used as the timestamp for the alert. The alert con-
tains the time the activity was detected, protocol, source
and destination IP address and source and destination port
number.
4 Prototype Evaluation
4.1 Data Set
To validate our DNS-based detection approach, we de-
veloped and tested a fully functional software prototype.
The software was installed on a commodity PC with a
Linux operating system and a 10/100 network interface
card. The prototype implements all features discussed in
Section 3. To conduct our evaluation, one week of net-
work traf(cid:2)c was collected at a (cid:2)rewall in front of one of
our university’s research labs. A Linux system using tcp-
dump was connected to a tap in front of the (cid:2)rewall to col-
lect and archive the network traces. We monitored both
incoming and outgoing network traf(cid:2)c to the lab. The lab
router is connected to the university’s Internet accessible
Class B network. The lab network consists of a one quar-
ter Class C network (i.e. 63) of Internet reachable IPv4
addresses.
The lab network contains one authoritative DNS server
that all internal systems in the network are con(cid:2)gured to
use. The lab’s DNS server has entries associated with the
lab’s mail server, web server, and Kerberos server. The
(cid:2)rewall does not permit any inbound connections unless
they were (cid:2)rst established by an internal system. All sys-
tems within the lab can access the Internet directly through
the (cid:2)rewall, which is the sole egress/ingress point for the
network. Using the cell de(cid:2)nition previously described,
the lab can be considered one cell in the university’s en-
terprise network. The lab analysis allowed us to test the
prototype’s ability to detect L2R worm propagation.
During the course of our network traf(cid:2)c collection in
front of the lab (cid:2)rewall, network traf(cid:2)c from a separate
internal university network was also captured. We will re-
fer to this network as the Internal Departmental Network
(IDN). The IDN has its own authoritative DNS server that
all its internal systems are con(cid:2)gured to use. The IDN can
be considered another cell in the university’s enterprise
network. This incidental collection provided us with the
opportunity to perform additional analysis. In addition to
running the prototype against the lab network traces, we
ran the prototype against a (cid:2)ltered version of the IDN net-
work traces. To address privacy concerns, we restricted
our inspection of the IDN’s network traces to those pack-
ets that contained either a source or destination address
that matched a lab network IP address. The IDN analysis
allowed us to test the prototype’s ability to detect worm
propagation between cells.
At the start of our analysis, we (cid:3)ushed the lab DNS
server’s cache. This ensured that any new connections
from lab systems would result in an external DNS query
to retrieve the appropriate A record instead of accessing
the lab DNS server’s cache. From our vantage at the net-
work boundary, we are only able to detect DNS replies
as they enter the lab network, not those generated inter-
nally from the DNS server’s cache. The (cid:3)ushing of the
lab DNS cache ensures that the DCE will contain the same
DNS information as the lab’s DNS server. In our analy-
sis, all IP addresses have been modi(cid:2)ed to keep the actual
IP addresses anonymous. The university network’s IP ad-
dresses are represented by the 192.168.0.0/16 IP address
range.
Table 1. Network Data Set
Network Protocol
TCP packets
TCP connections
ICMP packets
UDP packets
Other
Packet Count
5,969,266
18,634
4,955
5,301,489
805,604
Network traf(cid:2)c was collected for a seven-day period
Table 2. DNS Datagrams
Date Total Packets DNS
Request
Data-
grams
06-24-2004
06-25-2004
06-26-2004
06-27-2004
06-28-2004
06-29-2004
06-30-2004
2,101,243
2,491,663
847,687
889,251
1,339,283
1,382,642
1,081,451
6,485
5,525
1,192
2,231
5,225
6,121
4,973
DNS
Reply
Data-
grams
6,264
4,951
658
3,174
4,752
5,998
4,164
from June 24th to June 30th, 2004. The network traces
are comprised of all network activity that reached the lab’s
router from internal systems, systems in the IDN cell, and
the Internet. During this period, over 5 million UDP pack-
ets were observed as well as almost 6 million TCP pack-
ets. A total of 18,634 individual TCP connections oc-
curred. Table 1 provides the observed protocols and their
respective quantities.
DNS is transported mainly over UDP. DNS zone trans-
fers use the TCP protocol but it is a standard acceptable
security practice to disallow this feature. Table 2 shows
the number of DNS request and reply datagrams that were
detected in the network traces. Overall, we observed that
the total amount of DNS traf(cid:2)c is a small percentage of
the total amount of network traf(cid:2)c. An individual DNS
reply may contain multiple records. In fact, the 10,162
DNS replies we received in the network actually gener-
ated 99,994 individual DNS resource records.
4.2 Lab Monitoring Analysis
The lab deployment was used to test the prototype’s
ability to detect L2R worm propagation. Initially, we ob-
served the network for a three-hour period the day prior
to our data set to generate a whitelist. Section 5.2, Ta-
ble 8 contains the seven entries that comprised the lab’s
whitelist.
In order for network activity to be identi(cid:2)ed
as complying with the whitelist, the protocol, source IP,
source port, destination IP, and destination port must all
match. The (cid:2)rst four entries consist of communications
between speci(cid:2)c servers. The (cid:2)fth entry speci(cid:2)es a sin-
gle server allowed to initiate connections with other sys-
tems on a speci(cid:2)c port. Finally, the last two entries allow
any system in the lab to initiate connections to any other
systems as long as they adhere to the particular port and
protocol speci(cid:2)ed.
Over the course of the one week, a total of 52 alerts
were generated by the prototype. Table 5 gives the alert
breakdown by day. None of the alerts could be attributed
to worm propagation but in contrast, see Section 4.3. This
is not surprising since the lab is a well-maintained hard-
ened network administered by security-aware personnel.
A full analysis of the true false positives generated by the
prototype is given in section 4.4.
4.3 IDN Monitoring Analysis
The IDN deployment was used to test the prototype’s
Ini-
ability to detect worm propagation between cells.
tially, we observed the network for a three-hour period
the day prior to our data set to generate a whitelist. The
whitelist for the IDN consisted of four entries (see Table
9 in Section 5.2). All of these entries consisted of allowed
communications between speci(cid:2)c servers. Over the one-