adjacent layers of switches and hosts. It is straightforward to
package the backup switches and the circuit switches into the
original fat-tree Pods with simple changes of wiring as shown
in Figure 1. Core switches and the backup core switches
each connect to every Pod with one link. In practice, the core
switches can be placed as in the original fat-tree, followed
by the backup core switches. The reordering in Figure 1(c)
is unnecessary. By streamlining the connectors from within
each Pod, we can maintain the original Pod-host and Pod-core
wiring patterns in fat-tree.
4 CONTROL PLANE
4.1 Fast Failure Detection and Recovery
Most previous fault-tolerant data center network architectures,
such as PortLand [30] and F10 [26], mainly focus on link fail-
ures. According to a measurement study, however, switch
failures account for 11.3% of the failure events in data centers
and their impact is signiﬁcantly more severe than link fail-
ures [16]. Therefore, ShareBackup aims to detect and recover
both link and switch failures rapidly.
Failure detection and recovery are handled by a manage-
ment entity, e.g. one or more dedicated network controllers.
For switch failures, we require switches to send keep-alive
messages continuously to the management entity. After miss-
ing keep-alive messages from a switch for a pre-deﬁned time
period, the management entity allocates an available backup
switch to failover to and reconﬁgures the circuit switches
associated with the failure group. As shown in Figure 1(a),
in these circuit switches, original connections to the failed
switch should reconnect to the backup switch.
We adopt the rapid failure detection mechanism in F10 [26]
for link failures, where switches keep sending packets to
each other (or to hosts) to test the interface, data link, and
forwarding engine. When a link is down, it takes time to
determine which end has lost connectivity. For the purpose
of fast failure recovery, the switches on both sides of the
failed link are replaced. The cause of failure is analyzed
later by the procedure in Section 4.3. The management entity
gets notiﬁcations of link failures from switches and hosts,
and reconﬁgures the circuit switches in the same way as it
addresses switch failures on both ends. Figure 1(b) and 1(c)
show examples of this approach.
4.2 Distributed Network Controllers
ShareBackup requires the management entity to be imple-
mented as distributed network controllers. A single controller
is not capable of collecting frequent heartbeats from all the
switches in the network. Like F10, the probing interval for
failure detection can be as low as a few ms. For example, in
179
Masking Failures from Application Performance
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
(cid:6)(cid:11)(cid:13)(cid:1)
(cid:6)(cid:11)(cid:13)(cid:1)
(cid:12)(cid:9)(cid:11)(cid:10)(cid:7)(cid:8)(cid:9)(cid:14)(cid:1)(cid:3)(cid:6)(cid:2)(cid:15)(cid:13)(cid:1)(cid:3)(cid:6)(cid:2)(cid:16)(cid:12)(cid:1)
(cid:7)(cid:5)(cid:8)(cid:6)(cid:1)(cid:4)(cid:2)(cid:5)(cid:7)(cid:10)(cid:9)(cid:3)(cid:1)
(cid:6)(cid:11)(cid:13)(cid:1)
(cid:6)(cid:11)(cid:13)(cid:1)
(cid:6)(cid:11)(cid:13)(cid:1)
(cid:14)(cid:19)(cid:8)(cid:15)(cid:3)(cid:7)(cid:1)
(cid:7)(cid:5)(cid:8)(cid:6)(cid:1)(cid:4)(cid:2)(cid:5)(cid:7)(cid:10)(cid:9)(cid:3)(cid:1)
(cid:3)(cid:11)(cid:10)(cid:15)(cid:13)(cid:11)(cid:9)(cid:9)(cid:5)(cid:13)(cid:1)
(cid:12)(cid:9)(cid:11)(cid:10)(cid:7)(cid:8)(cid:9)(cid:14)(cid:5)(cid:4)(cid:15)(cid:13)(cid:5)(cid:4)(cid:16)(cid:12)(cid:1)
(cid:13)(cid:5)(cid:14)(cid:12)(cid:11)(cid:10)(cid:4)(cid:5)(cid:10)(cid:15)(cid:1)(cid:14)(cid:19)(cid:8)(cid:15)(cid:3)(cid:7)(cid:1)
(cid:13)(cid:5)(cid:14)(cid:12)(cid:11)(cid:10)(cid:4)(cid:5)(cid:10)(cid:15)(cid:1)(cid:3)(cid:11)(cid:10)(cid:15)(cid:13)(cid:11)(cid:9)(cid:9)(cid:5)(cid:13)(cid:1)
(cid:3)(cid:8)(cid:13)(cid:3)(cid:18)(cid:8)(cid:15)(cid:1)(cid:14)(cid:19)(cid:8)(cid:15)(cid:3)(cid:7)(cid:1)
(cid:2)(cid:5)(cid:4)(cid:4)(cid:3)(cid:2)(cid:7)(cid:9)(cid:1)(cid:5)(cid:6)(cid:7)(cid:10)(cid:8)(cid:1)(cid:5)(cid:6)(cid:7)(cid:11)(cid:22)(cid:1)
(cid:2)(cid:5)(cid:4)(cid:4)(cid:3)(cid:2)(cid:7)(cid:9)(cid:1)(cid:5)(cid:6)(cid:7)(cid:10)(cid:8)(cid:1)(cid:5)(cid:6)(cid:7)(cid:11)(cid:22)(cid:1)
3 
2 
(cid:16)(cid:13)(cid:14)(cid:12)(cid:1)
(cid:16)(cid:13)(cid:14)(cid:12)(cid:1)
6 
5 
(cid:8)(cid:15)(cid:14)(cid:11)(cid:12)(cid:1)
(cid:8)(cid:15)(cid:14)(cid:11)(cid:12)(cid:1)
(cid:5)(cid:4)(cid:1)
(cid:5)(cid:4)(cid:1)
1 
11 
(cid:2)(cid:3)(cid:6)(cid:1)
(cid:2)(cid:3)(cid:6)(cid:1)
3 
2 
(cid:4)(cid:3)(cid:5)(cid:1)
(cid:4)(cid:2)(cid:5)(cid:1)
(cid:14)(cid:18)(cid:14)(cid:12)(cid:5)(cid:3)(cid:15)(cid:1)
(cid:8)(cid:10)(cid:15)(cid:5)(cid:13)(cid:6)(cid:2)(cid:3)(cid:5)(cid:1)
(cid:3)(cid:8)(cid:13)(cid:3)(cid:18)(cid:8)(cid:15)(cid:1)(cid:14)(cid:19)(cid:8)(cid:15)(cid:3)(cid:7)(cid:1)
(cid:14)(cid:18)(cid:14)(cid:12)(cid:5)(cid:3)(cid:15)(cid:1)(cid:14)(cid:19)(cid:8)(cid:15)(cid:3)(cid:7)(cid:1)
(cid:17)(cid:10)(cid:7)(cid:9)(cid:20)(cid:1)
(cid:17)(cid:10)(cid:7)(cid:9)(cid:20)(cid:1)
5 
4 
(cid:18)(cid:19)(cid:8)(cid:8)(cid:10)(cid:18)(cid:18)(cid:21)(cid:1)
(cid:18)(cid:19)(cid:8)(cid:8)(cid:10)(cid:18)(cid:18)(cid:21)(cid:1)
7 
6 
(cid:8)(cid:10)(cid:8)(cid:16)(cid:2)(cid:15)(cid:11)(cid:13)(cid:1)(cid:3)(cid:11)(cid:10)(cid:15)(cid:13)(cid:11)(cid:9)(cid:9)(cid:5)(cid:13)(cid:1)
(cid:2)(cid:5)(cid:4)(cid:4)(cid:3)(cid:2)(cid:7)(cid:9)(cid:1)(cid:5)(cid:6)(cid:7)(cid:10)(cid:8)(cid:1)(cid:5)(cid:6)(cid:7)(cid:11)(cid:22)(cid:1)
(cid:2)(cid:5)(cid:4)(cid:4)(cid:3)(cid:2)(cid:7)(cid:9)(cid:1)(cid:5)(cid:6)(cid:7)(cid:10)(cid:8)(cid:1)(cid:5)(cid:6)(cid:7)(cid:11)(cid:22)(cid:1)
4 
3 
(cid:3)(cid:8)(cid:13)(cid:3)(cid:18)(cid:8)(cid:15)(cid:1)(cid:14)(cid:19)(cid:8)(cid:15)(cid:3)(cid:7)(cid:1)
Figure 2: Communication protocol in the control system. (a): Failure detection and recovery. (b): Diagnosis of link failure.
(cid:1)
(cid:1)(cid:8)(cid:8)(cid:13)(cid:11)(cid:12)
(cid:1)
(cid:1)(cid:8)(cid:8)(cid:13)(cid:11)(cid:12)
(cid:1)
(cid:1)(cid:8)(cid:8)(cid:13)(cid:11)(cid:12)
(cid:3)(cid:2)(cid:4)(cid:1)
1
(cid:3)(cid:2)(cid:4)(cid:1)
2
(cid:3)(cid:2)(cid:4)(cid:1)
3
(cid:1)
(cid:3)(cid:9)(cid:10)(cid:7)(cid:12)
(cid:3)(cid:2)(cid:4)(cid:1)
(cid:2)(cid:5)(cid:15)(cid:11)(cid:13)(cid:11)(cid:12)
(cid:2)(cid:5)(cid:15)(cid:11)(cid:14)(cid:11)(cid:12)
(cid:1)
(cid:3)(cid:9)(cid:10)(cid:7)(cid:12)
(cid:3)(cid:2)(cid:4)(cid:1)
(cid:2)(cid:5)(cid:15)(cid:11)(cid:13)(cid:11)(cid:12)
(cid:2)(cid:5)(cid:15)(cid:11)(cid:14)(cid:11)(cid:12)
(cid:1)
(cid:3)(cid:9)(cid:10)(cid:7)(cid:12)
(cid:3)(cid:2)(cid:4)(cid:1)
(cid:2)(cid:5)(cid:15)(cid:11)(cid:13)(cid:11)(cid:12)
(cid:2)(cid:5)(cid:15)(cid:11)(cid:14)(cid:11)(cid:12)
1
2
3
(cid:1)
(cid:4)(cid:6)(cid:8)(cid:7)(cid:13)(cid:11)(cid:12)
(cid:1)
(cid:4)(cid:6)(cid:8)(cid:7)(cid:13)(cid:11)(cid:12)
(cid:1)
(cid:4)(cid:6)(cid:8)(cid:7)(cid:13)(cid:11)(cid:12)
(cid:1)
(cid:1)(cid:8)(cid:8)(cid:15)(cid:11)(cid:12)
(cid:1)
(cid:1)(cid:8)(cid:8)(cid:15)(cid:11)(cid:12)
(cid:1)
(cid:1)(cid:8)(cid:8)(cid:15)(cid:11)(cid:12)
Figure 3: Circuit switch conﬁgurations for diagnosis of link failures shown by examples (b) and (c) in Figure 1. Circuit switches in a Pod
are chained up using the side ports. Only “suspect switches” on both sides of the failed link and some related backup switches are shown.
Through conﬁgurations 1(cid:2), 2(cid:2), and 3(cid:2), the “suspect interface” on both “suspect switches” associated with the failure can connect to 3 different
interfaces on one or multiple other switches.
a k = 48 fat-tree with 2880 switches, a heartbeat message
every 5ms leads to 576k queries per second, which exceeds
the capacity of a single controller. Distributed controllers also
isolate the impact of controller failures to a small portion
of the network. Controllers only store local state, so adding
redundant controllers can further enhance resiliency with low
state-exchange overhead. Finally, with distributed placement
of network controllers, switches and circuit switches can be
physically close to their controller, which effectively reduces
the message latency of failure detection and recovery.
Most circuit switches nowadays use the TL1 software in-
terface to setup a connection, whose input and output ports
should be speciﬁed explicitly, i.e. connect(in_port,out_port).
The network controller needs to maintain the current connec-
tions of the circuit switches so as to switch to new connections.
In Figure 1(b) and 1(c), a circuit switch connects to switches
from the failure groups above and below, so it can be reconﬁg-
ured by both controllers. Inconsistent reconﬁguration from dif-
ferent controllers may result in wrong connections. To address
this issue, we change the API to replace(old_port,new_port)
and free controllers from bookkeeping of the circuit conﬁgura-
tions. Using this new API, a controller only reconﬁgures ports
on one side of the circuit switch and is ignorant of changes
on the other side. Before the reconﬁguration, the old port (red
ports in Figure 1) connects to the failed switch and the new
port (black ports in Figure 1) connects to the backup switch.
After the change, the old port’s role will be replaced by the
new port, i.e., the new port will connect to the old port’s origi-
nal peering port. In case of concurrent requests from different
controllers, the circuit switch reconﬁgures one side at a time,
so the order of execution does not affect the end result.
Figure 2(a) shows the failure detection and recovery pro-
cess using distributed network controllers. Based on the lay-
out of the ShareBackup architecture, an intuitive way of con-
troller placement is to assign each failure group a dedicated
controller. Each controller receives heartbeat messages from
k
2 switches only. As shown in Figure 1, a failure group in the
core layer corresponds to k
2 circuit switches beneath it, while
one in the edge and aggregation layers corresponds to two sets
of k
2 circuit switches above and beneath it, so each controller
reconﬁgures k circuit switches at maximum. The load for each
controller is thus very light even for a large network. The con-
trollers do not share state, so the communication among them
is also minimal. Due to the simple functionality, controllers
can be placed on existing servers with a separate manage-
ment network, or on dedicated low-cost hardware, such as
the Arduino [3] and Raspberry Pi [6] platforms. Multiple
controllers can also reside on the same machine to realize
different degrees of distribution/centralization.
4.3 Ofﬂine Auto-Diagnosis
Most link failures are due to a failure that occurs on one end.
After both switches are replaced, we run automatic failure
diagnosis in the background to ﬁnd which “suspect interface”
(and the “suspect switch” it belongs to) has caused the prob-
lem. We chain up circuit switches in the same layer of a Pod
180
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
D. Wu et al.
as a ring through the side ports. Figure 3 shows the circuit
switch conﬁgurations, through which the suspect interface
on either end of the failed link can connect to 3 different
interfaces, either on the same switch (as Aдд1, 0, Edдe1, 0, and
Core0) or on different switches (as Aдд3, 0).
disruptions in the network. The network controller keeps track
of the current backup switches in their failure groups.
4.4 Live Impersonation of Failed Switch
Trafﬁc is redirected to the backup switch in the physical layer
after a failed switch is replaced. The backup switch needs to
impersonate the failed switch by using the same routing table.
Fat-tree uses Two-Level Routing, where each switch has a
pre-deﬁned routing table [8]. To avoid the additional delay of
inserting forwarding rules into the backup switch, we aim to
preload the routing table and make the backup switch a hot
standby. Regular switches recovered from failures can work
as backup switches, so every switch needs to store the routing
tables of all the switches in the failure group. The challenge
is to resolve the conﬂicts between different routing tables.
In fat-tree, all the core switches and all the aggregation
switches in the same Pod have the same routing table. There-
fore, in the aggregation and core layers of our network, switches
in a failure group only keep a common routing table. For in-
bound trafﬁc, edge switches in a Pod, also a failure group,
have the same set of k
2 forwarding entries that match on the
sufﬁx of the end host addresses. For out-bound trafﬁc, each of
these edge switches has k
2 different entries. We use VLANs
for differentiation. We ﬁrst edit the original fat-tree routing
tables by assigning every edge switch in the Pod a unique
VLAN ID and adding it to the out-bound routing table entries.
The edited routing tables from all the edge switches are then
combined together and stored in every switch in the failure
group. A host knows which edge switch it should connect
to, so it tags out-going packets with the VLAN ID of the
edge switch. No matter what switches in the failure group are
active, by matching the VLAN ID, packets can always refer
to the correct routing table. This combined routing table from
2 edge switches has k
k
4 out-bound
entries. This total number is within the TCAM capacity of
commercial switches even for large-scale fat-tree networks.