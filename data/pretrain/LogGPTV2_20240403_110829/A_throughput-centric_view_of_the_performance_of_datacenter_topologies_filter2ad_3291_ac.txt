switch-level permutation traffic matrices. So, at least one of the
switch-level permutation matrices has lower throughput than this
TM. Hence, considering switch-level matrices not only improves
the scalability of our throughput bound but also better captures the
minimum throughput of the topology.
3 EVALUATING THE THROUGHPUT UPPER
BOUND
In this section, we show that throughput upper bound (abbreviated
tub) (a) accurately estimates the worst case throughput and (b) all
previously proposed throughput estimators [23, 24, 43, 51] produce
worse estimates for uni-regular topologies and most scale poorly. 4
3.1 Throughput Gap
In this section, we compute the throughput gap between the
throughput upper bound (abbreviated tub) and the throughput
(a) Throughput distribution
(b) Path distribution
Figure 4: Jellyfish (H=8). (a) Throughput gap appears at topology sizes that
shortest paths does not provide enough diversity. (b) The number of pairwise
shortest paths in the maximal permutation matrix periodically increases
and decreases. (sp=shortest path, nsp=non-shortest path, spl=shortest path
length)
achieved by routing a â€œworst-caseâ€ traffic matrix, and show that
this gap is small.
Methodology. Prior work [27] has shown that maximal permu-
tation matrix can achieve worst-case throughput. We have inde-
pendently verified this. For small topologies, we exhaustively com-
pared the throughput of every TM under KSP-MCF, and the maxi-
mal permutation matrix achieves the lowest throughput. For large
topologies, we compared the throughput of the maximal permuta-
tion matrix with 20 random permutations, and observed that the
throughput of maximal permutation matrix is constantly lower, and
the gap between these two increases with scale.
To demonstrate that the throughput gap is small, we need to
select a routing scheme. We have found that it suffices to solve a
path-based multi-commodity flow [33] over K-shortest paths (KSP-
MCF, see Â§H). To compute the throughput gap, we sweep values of
ğ¾ until increasing ğ¾ does not increase throughput5; in most cases,
ğ¾ = 100 suffices to match tub. As an aside, we do not mean to
suggest that KSP-MCF is practical for large networks; especially for
uni-regular topologies, finding a scalable routing scheme that can
achieve high throughput is an open question left to future work.
Other details. For all results in the paper, we use METIS [28]
to (over) estimate bisection bandwidth, Gurobi [18] to solve linear
programs for MCF, the networkx [19] implementations of ğ¾-shortest
paths [49] and the igraph [9] implementation of maximum bipartite
matching [32, 40]. FatClique deviates slightly from our definition
of uni-regular topologies: in a FatClique topology, ğ» can differ by 1
across switches. We have adapted tub and the maximal permutation
algorithm to deal with this deviation (Â§I).
For Uni-regular Topologies. Figure 3 shows the throughput gap
for tub for the three uni-regular topologies, for different ğ».
Jellyfish. Figure 3(a) shows the throughput gap for ğ¾ = 100 for
Jellyfish with ğ» = 8 (other values of ğ» are qualitatively similar).
The gap is non-zero at small scales between 3K â€“ 15K. However,
for larger instances, the gap is close to zero.
tub is loose in the range 3K â€“ 15K because (a) the proof of
Theorem 2.2 uses the observation that throughput is highest when
4Our code is available at https://github.com/USC-NSL/TUB
5Â§J shows the results for different values of ğ¾
05k10k15k20k25k#Servers (N)0.000.250.50GapH=6H=7H=805k10k15k20k25k#Servers (N)0.000.250.50GapH=6H=7H=805k10k15k20k25k#Servers (N)0.000.250.50GapH [5.5, 6.5)H [6.5, 7.5)H [7.5, 9]2k4k8k16k24k32k40k50k#Servers (N)0255075100Throughput Ratio(%)spnsp2k4k8k16k24k32k40k50k80k100k120k140k180k220k240k280k300k#Servers (N)0100200300400500#Pairwise Pathsspl+2spl+1splSIGCOMM â€™21, August 23â€“28, 2021, Virtual Event, USA
Namyar .et al.
all paths between each source-destination pair are shortest paths
and (b) topologies in this size range have fewer shortest paths,
so KSP-MCF routes traffic over non-shortest paths. (Figure 4(a)
plots the distribution of the fraction of flows over shortest and
non-shortest paths for different topology sizes).
Interestingly, topologies with 100K â€“ 180K servers have a smaller
fraction of shortest paths (Figure 4(b)), so we expect tub to be loose
in that range (we cannot confirm this because KSP-MCF does not
scale to those sizes), but expect the throughput gap to be small
beyond that range because the fraction of shortest paths increases.
However, in Â§E, we show that the maximum possible throughput
gap approaches zero asymptotically. Future work can explore better
throughput bounds that exploit diversity in non-shortest paths.
Xpander and FatClique. Figure 3 shows the throughput gap
for Xpander and FatClique, for different values of ğ». Like Jellyfish
at ğ» = 8, the gap is significant at small scales between 5K â€“ 15K for
these topologies and the gap is close to zero for larger instances.
Bi-regular Topologies. For Clos-based bi-regular topologies,
ECMP is able to achieve (close to) full throughput (modulo
differences in flow sizes [15]). We find that tubâ€™s estimate is also 1
for different Clos topologies, showing that the gap is zero for them
as well (Table A.1).
3.2 Comparison with other throughput metrics
Prior work has proposed other ways of estimating throughput. For
uni-regular topologies, we expect tub to be (a) faster and (b) more
accurate than these other methods, because it leverages properties
of uni-regular topologies. In this section, we validate this intuition.
Efficiently computing tub. Before doing so, we briefly discuss
some empirical results for the speed of computing tub. The bottle-
neck in this computation is the weighted maximum matching in a
complete bipartite graph. Several network analysis tools such as
networkx [19] and igraph [9], have an efficient implementation of
weighted maximum matching. Furthermore, our computation scales
well because we abstract the server-level traffic into a switch-level
traffic matrix, so that the number of nodes in the constructed bipar-
tite graph reduces significantly. On a machine with 64GB of RAM,
we were able to find the throughput upper bound for topologies
with up to 180K servers with ğ» = 8 within 20 minutes. For calibra-
tion, on the same platform, computing the throughput for routing
a permutation traffic matrix using KSP-MCF does not scale beyond
50K servers, and using full-blown MCF does not scale beyond 8K
servers.
Comparison alternatives. Prior work [27] has compared through-
put (i.e., the solution to MCF) with cut-based metrics, such as
sparsest-cut (using an eigenvector based optimization in [26]) and
bisection bandwidth, and [43] computes an upper bound on average
throughput of uni-regular topologies across uniform traffic matrices.
In addition to these, we compare our method to two other through-
put estimators developed for general graphs. Hoeflerâ€™s method [51]
divides a flow into sub-flows on each path between source and
destination, and splits the capacity of a link equally across all flows
traversing it. Jainâ€™s method [24] incrementally routes flows on each
path; at each step it allocates residual capacity on a link to all
new flows added to the link at this step and iterates until no paths
remain.
Results. Figure 5 compares tub against these alternatives, for
Jellyfish topologies with 8 servers per switch. Results for other
topologies are similar (omitted for brevity).
Small to medium scale. Figure 5(a) shows the throughput gap
(determined using the methodology described in Â§3.1) for topologies
with up to 25K servers. tub has the smallest throughput gap across
all alternatives. In the range 15K â€“ 25K, tubâ€™s throughput gap is
zero, that of others is higher than 0.2, and sometimes as high as 0.4.
To illustrate why it is important to have a small throughput gap,
consider a scenario in which a network operator wishes to design a
full throughput topology; if she uses a loose throughput estimator,
the resulting topology may not actually have full throughput.
Moreover, tub is among the most efficient of the alternatives
(Figure 5(b)).
It is both more accurate, and faster than Jainâ€™s method (JM)
and Hoeflerâ€™s method (HM). These have large throughput gaps at
larger topology sizes (Figure 5(a)). JM and HM exploit edges of each
available path, but their estimates are loose because they assume all
the sub-flows going through each edge get a fair share of the edgeâ€™s
capacity. This assumption may not maximize the throughput of a
traffic matrix; to do this, flows that currently have lower throughput
should get more share of the available capacity. JM and HM are
a few orders of magnitude slower than tub (Figure 5(b)) because
they exploit more of the topological structure.
Bisection bandwidth and [43] scale better than tub, but their
estimates have large error. Bisection bandwidth is a loose cut-based
estimate of throughput as shown by [27] at small scales, and proven
by us in Â§4. Figure 5(a) empirically verifies this at much larger
scales than [27]. Computing exact bisection bandwidth for general
networks is intractable [4], so we use a fast heuristic [28] that
approximates the bisection bandwidth. Furthermore, the bound
in [43] relies on average distance among all the pairs of switches,
based on the fact that every switch splits its traffic equally and sends
to all the other switches in the average case. Our bound, however,
considers structural properties (e.g., distance between individual
pairs) to maximize the congestion by routing the traffic between
pairs with the largest distance. Therefore, the gap for tub is smaller
than that for [43], but tub is slower since it considers more details
about the topology.
Large scale. Figure 5(c) plots the bisection bandwidth, and the
throughput estimated by tub, and by [43], for topologies for up to
300K servers. At these scales, we cannot compute KSP-MCF to esti-
mate the throughput, so we depict the absolute throughput values.
[43]â€™s throughput estimate is consistently and considerably higher
across the entire range compared to tubâ€™s. The latterâ€™s computa-
tional complexity is comparable to that of [43], except for the range
200K â€“ 280K where tub exhibits a non-monotonic behavior. tub
attempts to choose disjoint pairs of switches with large distances
from each other to construct the maximal permutation matrix, but
in topologies of this size range, there are fewer of these pairs with
longest possible distance (i.e., diameter), so it takes longer for the
A Throughput-Centric View of the Performance of Datacenter Topologies
SIGCOMM â€™21, August 23â€“28, 2021, Virtual Event, USA
(a) Accuracy
(b) Efficiency
(c) Accuracy, Large Scale
(d) Efficiency, Large Scale
Figure 5: tub is more accurate compared to all the other metrics and almost as fast as bisection bandwidth and throughput bound in [43]. (BBW is bisection
bandwidth, SC is sparsest cut, HM(.) is Hoeflerâ€™s method and JM(.) is Jainâ€™s method in which (.) is the number of paths)
algorithm to search for these disjoint pairs. We expect to signifi-
cantly reduce the search by parallelizing the weighted maximum
matching implementation; we have left this to future work.
Summary. tubâ€™s throughput gap is smaller than those of prior
estimators and scales to up to 300K servers. This enables us to
revisit whether prior evaluations of large-scale topologies using
bisection bandwidth would yield different conclusions if throughput
were used instead (Â§5).
4 LIMITS ON THE THROUGHPUT OF
UNI-REGULAR TOPOLOGIES
In this section, using Theorem 2.2 we establish asymptotic limits on
the size of full-throughput uni-regular topologies. Then, exploiting
tubâ€™s scalability and tightness (Â§3), we establish practical limits
on the size of full-throughput uni-regular topologies for different
values of ğ».
4.1 Asymptotic Limits
A throughput upper bound for all uni-regular topologies.
Theorem 2.2 determines an upper-bound on the throughput for a
given uni-regular or bi-regular topology, independent of routing.
The following theorem, which applies only to uni-regular topolo-
gies, establishes an upper-bound on the throughput across all uni-
regular topologies, independent of routing.
Theorem 4.1. The maximum achievable throughput of any uni-
regular topology with ğ‘ servers, switch radix ğ‘… and ğ» servers per
switch under any routing is:
ğœƒâˆ— â‰¤ ğ‘ (ğ‘… âˆ’ ğ»)
ğ»2ğ·
(cid:32) (ğ‘… âˆ’ ğ» âˆ’ 1)ğ‘‘ âˆ’ 1
ğ‘… âˆ’ ğ» âˆ’ 2
(2)
(cid:33)
âˆ’ ğ‘‘
where;
ğ· = ğ‘‘( ğ‘
ğ»
âˆ’ 1) âˆ’ ğ‘… âˆ’ ğ»
ğ‘… âˆ’ ğ» âˆ’ 2
and ğ‘‘ is the minimum diameter required to accommodate ğ‘/ğ»
switches computed using Moore bound [39].
Proof Sketch. Â§D contains the detailed proof. We observe from
Equation 1 that throughput is lowest for switch pairs (ğ‘¢, ğ‘£) for
whom the shortest path length ğ¿ğ‘¢ğ‘£ is high. Our constructive proof
Figure 6: uni-regular topologies can have limited throughput.
first bounds the number of switches whose distance is at least ğ‘š
from a given switch (Lemma 8.1 in Â§D). Then, we construct (Algo-
rithm 1 in the Appendix) the maximal permutation traffic matrix in
which each switch exchanges traffic with other switches that are
furthest from it (Lemma 8.2 in Â§D). This construction maximizes
ğ¿ğ‘¢ğ‘£, and from this construction and using Lemma 8.1, we can bound
the number of communicating switch pairs whose distances are at
least ğ‘š hops of each other. The bound applies to the denominator
of the RHS of Theorem 2.2, resulting in a throughput upper bound
independent of the traffic matrix (Lemma 8.3 in Â§D).
â–¡
This theorem formalizes the intuition captured in Figure 6. Fun-
damentally, a uni-regular topology is constrained by the fact that
every switch has to have ğ» servers. The figure shows topologies in
which 3-port switches have (at most) ğ» = 1 servers. The leftmost
4-switch topology has full throughput. However, the addition of a
single switch (the middle topology) drops throughput significantly.
To recover full throughput in this setting, we need to add four more
switches with no servers; these provide additional transit capacity.
Figure 7 shows the worst-case TM for the middle topology along