### Switch-Level Permutation Traffic Matrices
At least one of the switch-level permutation matrices exhibits lower throughput than the given traffic matrix (TM). Therefore, considering switch-level matrices not only enhances the scalability of our throughput bound but also better captures the minimum throughput of the topology.

### 3. EVALUATING THE THROUGHPUT UPPER BOUND
In this section, we demonstrate that the throughput upper bound (tub) (a) accurately estimates the worst-case throughput and (b) outperforms all previously proposed throughput estimators [23, 24, 43, 51] for uni-regular topologies, with most of these estimators scaling poorly.

#### 3.1 Throughput Gap
We compute the throughput gap between the throughput upper bound (tub) and the throughput achieved by routing a "worst-case" traffic matrix, and show that this gap is small.

**Methodology:**
Previous work [27] has shown that the maximal permutation matrix can achieve the worst-case throughput. We independently verified this. For small topologies, we exhaustively compared the throughput of every TM under KSP-MCF, and found that the maximal permutation matrix achieves the lowest throughput. For large topologies, we compared the throughput of the maximal permutation matrix with 20 random permutations and observed that the maximal permutation matrix consistently has lower throughput, with the gap increasing as the scale increases.

To demonstrate that the throughput gap is small, we selected a routing scheme. We found that it suffices to solve a path-based multi-commodity flow [33] over K-shortest paths (KSP-MCF, see §H). To compute the throughput gap, we sweep values of \( K \) until increasing \( K \) no longer increases throughput; in most cases, \( K = 100 \) suffices to match tub. Note that KSP-MCF is not practical for large networks, especially for uni-regular topologies, and finding a scalable routing scheme that can achieve high throughput remains an open question for future work.

**Other Details:**
For all results in the paper, we used METIS [28] to estimate bisection bandwidth, Gurobi [18] to solve linear programs for MCF, the networkx [19] implementations of \( K \)-shortest paths [49], and the igraph [9] implementation of maximum bipartite matching [32, 40]. FatClique slightly deviates from our definition of uni-regular topologies, as \( H \) can differ by 1 across switches. We adapted tub and the maximal permutation algorithm to handle this deviation (§I).

**For Uni-regular Topologies:**
Figure 3 shows the throughput gap for tub for three uni-regular topologies, for different values of \( H \).

**Jellyfish:**
Figure 3(a) shows the throughput gap for \( K = 100 \) for Jellyfish with \( H = 8 \) (other values of \( H \) are qualitatively similar). The gap is non-zero at small scales (3K – 15K), but for larger instances, the gap is close to zero. Tub is loose in the range 3K – 15K because (a) the proof of Theorem 2.2 uses the observation that throughput is highest when all paths between each source-destination pair are shortest paths, and (b) topologies in this size range have fewer shortest paths, so KSP-MCF routes traffic over non-shortest paths. (Figure 4(a) plots the distribution of the fraction of flows over shortest and non-shortest paths for different topology sizes).

Interestingly, topologies with 100K – 180K servers have a smaller fraction of shortest paths (Figure 4(b)), so we expect tub to be loose in that range (we cannot confirm this because KSP-MCF does not scale to those sizes), but expect the throughput gap to be small beyond that range because the fraction of shortest paths increases. In §E, we show that the maximum possible throughput gap approaches zero asymptotically. Future work can explore better throughput bounds that exploit diversity in non-shortest paths.

**Xpander and FatClique:**
Figure 3 shows the throughput gap for Xpander and FatClique, for different values of \( H \). Like Jellyfish at \( H = 8 \), the gap is significant at small scales (5K – 15K) for these topologies and is close to zero for larger instances.

**Bi-regular Topologies:**
For Clos-based bi-regular topologies, ECMP can achieve (close to) full throughput (modulo differences in flow sizes [15]). We find that tub's estimate is also 1 for different Clos topologies, showing that the gap is zero for them as well (Table A.1).

#### 3.2 Comparison with Other Throughput Metrics
Prior work has proposed other ways of estimating throughput. For uni-regular topologies, we expect tub to be (a) faster and (b) more accurate than these other methods because it leverages properties of uni-regular topologies. In this section, we validate this intuition.

**Efficiently Computing Tub:**
Before doing so, we briefly discuss some empirical results for the speed of computing tub. The bottleneck in this computation is the weighted maximum matching in a complete bipartite graph. Several network analysis tools such as networkx [19] and igraph [9] have efficient implementations of weighted maximum matching. Furthermore, our computation scales well because we abstract the server-level traffic into a switch-level traffic matrix, reducing the number of nodes in the constructed bipartite graph. On a machine with 64GB of RAM, we were able to find the throughput upper bound for topologies with up to 180K servers with \( H = 8 \) within 20 minutes. For calibration, on the same platform, computing the throughput for routing a permutation traffic matrix using KSP-MCF does not scale beyond 50K servers, and using full-blown MCF does not scale beyond 8K servers.

**Comparison Alternatives:**
Prior work [27] has compared throughput (i.e., the solution to MCF) with cut-based metrics, such as sparsest-cut (using an eigenvector-based optimization in [26]) and bisection bandwidth, and [43] computes an upper bound on the average throughput of uni-regular topologies across uniform traffic matrices. In addition to these, we compare our method to two other throughput estimators developed for general graphs. Hoefler’s method [51] divides a flow into sub-flows on each path between source and destination, and splits the capacity of a link equally across all flows traversing it. Jain’s method [24] incrementally routes flows on each path; at each step, it allocates residual capacity on a link to all new flows added to the link at this step and iterates until no paths remain.

**Results:**
Figure 5 compares tub against these alternatives for Jellyfish topologies with 8 servers per switch. Results for other topologies are similar (omitted for brevity).

**Small to Medium Scale:**
Figure 5(a) shows the throughput gap (determined using the methodology described in §3.1) for topologies with up to 25K servers. Tub has the smallest throughput gap across all alternatives. In the range 15K – 25K, tub’s throughput gap is zero, while others' gaps are higher than 0.2 and sometimes as high as 0.4. A small throughput gap is crucial for designing a full-throughput topology; a loose estimator may result in a topology that does not actually have full throughput. Moreover, tub is among the most efficient of the alternatives (Figure 5(b)).

Tub is both more accurate and faster than Jain’s method (JM) and Hoefler’s method (HM). These methods have large throughput gaps at larger topology sizes (Figure 5(a)). JM and HM exploit edges of each available path, but their estimates are loose because they assume all sub-flows going through each edge get a fair share of the edge’s capacity. This assumption may not maximize the throughput of a traffic matrix; to do this, flows that currently have lower throughput should get more share of the available capacity. JM and HM are a few orders of magnitude slower than tub (Figure 5(b)) because they exploit more of the topological structure.

Bisection bandwidth and [43] scale better than tub, but their estimates have large errors. Bisection bandwidth is a loose cut-based estimate of throughput, as shown by [27] at small scales and proven by us in §4. Figure 5(a) empirically verifies this at much larger scales than [27]. Computing exact bisection bandwidth for general networks is intractable [4], so we use a fast heuristic [28] that approximates the bisection bandwidth. Furthermore, the bound in [43] relies on the average distance among all pairs of switches, based on the fact that every switch splits its traffic equally and sends to all other switches in the average case. Our bound, however, considers structural properties (e.g., distance between individual pairs) to maximize congestion by routing traffic between pairs with the largest distance. Therefore, the gap for tub is smaller than that for [43], but tub is slower since it considers more details about the topology.

**Large Scale:**
Figure 5(c) plots the bisection bandwidth and the throughput estimated by tub and [43] for topologies with up to 300K servers. At these scales, we cannot compute KSP-MCF to estimate the throughput, so we depict the absolute throughput values. [43]’s throughput estimate is consistently and considerably higher across the entire range compared to tub’s. The latter’s computational complexity is comparable to that of [43], except for the range 200K – 280K where tub exhibits non-monotonic behavior. Tub attempts to choose disjoint pairs of switches with large distances from each other to construct the maximal permutation matrix, but in topologies of this size range, there are fewer pairs with the longest possible distance (i.e., diameter), so it takes longer for the algorithm to search for these disjoint pairs. We expect to significantly reduce the search by parallelizing the weighted maximum matching implementation, which we leave for future work.

**Summary:**
Tub’s throughput gap is smaller than those of prior estimators and scales to up to 300K servers. This enables us to revisit whether prior evaluations of large-scale topologies using bisection bandwidth would yield different conclusions if throughput were used instead (§5).

### 4. LIMITS ON THE THROUGHPUT OF UNI-REGULAR TOPOLOGIES
In this section, using Theorem 2.2, we establish asymptotic limits on the size of full-throughput uni-regular topologies. Then, exploiting tub’s scalability and tightness (§3), we establish practical limits on the size of full-throughput uni-regular topologies for different values of \( H \).

#### 4.1 Asymptotic Limits
**A Throughput Upper Bound for All Uni-regular Topologies:**
Theorem 2.2 determines an upper-bound on the throughput for a given uni-regular or bi-regular topology, independent of routing. The following theorem, which applies only to uni-regular topologies, establishes an upper-bound on the throughput across all uni-regular topologies, independent of routing.

**Theorem 4.1:**
The maximum achievable throughput of any uni-regular topology with \( N \) servers, switch radix \( R \), and \( H \) servers per switch under any routing is:
\[
\theta^* \leq \frac{N (R - H)}{H^2 D} \left( \frac{(R - H - 1)d - 1}{R - H - 2} \right) - d
\]
where:
\[
D = d \left( \frac{N}{H} - 1 \right) - \frac{R + H}{R - H - 2}
\]
and \( d \) is the minimum diameter required to accommodate \( N/H \) switches, computed using the Moore bound [39].

**Proof Sketch:**
The detailed proof is in §D. From Equation 1, throughput is lowest for switch pairs (u, v) for whom the shortest path length \( L_{uv} \) is high. Our constructive proof first bounds the number of switches whose distance is at least \( m \) from a given switch (Lemma 8.1 in §D). Then, we construct (Algorithm 1 in the Appendix) the maximal permutation traffic matrix in which each switch exchanges traffic with other switches that are furthest from it (Lemma 8.2 in §D). This construction maximizes \( L_{uv} \), and from this construction and using Lemma 8.1, we can bound the number of communicating switch pairs whose distances are at least \( m \) hops of each other. The bound applies to the denominator of the RHS of Theorem 2.2, resulting in a throughput upper bound independent of the traffic matrix (Lemma 8.3 in §D).

This theorem formalizes the intuition captured in Figure 6. Fundamentally, a uni-regular topology is constrained by the fact that every switch must have \( H \) servers. The figure shows topologies in which 3-port switches have (at most) \( H = 1 \) server. The leftmost 4-switch topology has full throughput. However, the addition of a single switch (the middle topology) drops throughput significantly. To recover full throughput in this setting, we need to add four more switches with no servers; these provide additional transit capacity. Figure 7 shows the worst-case TM for the middle topology along with the corresponding throughput.