Ql ∈ Z,
ql
ij , xj , Y l
l ∈ {r, w}
ik ∈ {0, 1}, ∀i, j, k; l ∈ {r, w}.
B. Model Enhancements
We discuss enhancements to the N-1 Contingency model:
(cid:3)
(cid:3)
it
i N w
i SCi
is important
Cost-sensitive replication: When datastores are deployed on
public clouds,
to consider dollar costs in
addition to latency and availability. We focus on wide-area
communication costs since (i) this is known to be a dominant
component of costs in geo-replicated settings [34]; (ii) best
practices involve storing data in local instance storage with
periodic backups to persistent storage [25] - the costs of such
backups are independent of our replication policy decision;
and (iii) instance costs are comparable to a single DC deploy-
ment with the same number of replicas. Most cloud providers
today charge for out-bound bandwidth transfers at a ﬂat rate
per byte (in-bound transfers are typically free), though the
rate itself depends on the location of the DC. Let Ci be the
cost per byte of out-bound bandwidth transfer from DC i.
Consider an operation that originates in DC i and involves
writing a data item whose size is S bytes. Then, the total
j Xj .
cost associated with all write operations is
However, read operations in Cassandra retrieve the full data
item only from its nearest neighbor but receives digest from
everyone. Let nij denote an indicator variable, which is 1 if
the full data item is fetched from DC j. The size of the digest
is assumed negligibly small. The total cost associated with
i nij SCj. It is now straight-
all read operations is:
forward to modify (N-1C) to optimize costs subject to a delay
constraint. This may be done by making threshold (T) a ﬁxed
parameter rather than a variable of optimization and adding
additional constraints on nij .
Jointly considering normal operation and failures: Formu-
lation (N-1C) ﬁnds replication strategies that reduce latency
under failure. In practice, a designer prefers strategies that
work well in normal conditions as well as under failure. This
is achieved by combining the constraints in (LAT) and (N-1C),
with an objective function that is a weighted sum of latency
under normal conditions T and under failures Tf . The weights
are chosen to capture the desired preferences.
Failures of multiple DCs: While we expect simultaneous
failures of multiple DCs to be relatively uncommon, it is easy
to extend our formulations to consider such scenarios. Let K
be a set whose each element is a set of indices of DCs which
may fail simultaneously and we are interested in guarding
the performance against such a failure. We then employ (N-
1C) but with k iterating over elements of K instead of the
set of DCs. A naive approach may exhaustively enumerate
all possible combination of DC failures, could be computa-
tionally expensive, and may result in schemes optimized for
(cid:3)
(cid:3)
j N r
i
TABLE III
TRACE CHARACTERISTICS
Application
# of keys/classes
Span
Twitter[39]
Wikipedia[11]
Gowalla[22]
3,000,000
1961
196,591
2006-2011
2009-2012
Feb 2009-Oct 2010
unlikely events at the expense of more typical occurrences.
A more practical approach would involve explicit operator
speciﬁcations of correlated failure scenarios of interest. For
e.g., DCs that share the same network PoP are more likely to
fail together, and thus of practical interest to operators.
Network partitions: In general, it is impossible to guarantee
availability with network partition tolerance given the strict
quorum requirement [31]. For more common network outages
that partition one DC from others, our N-1C model ensures
that requests from all other DCs can still be served with
low latency. To handle more complex network partitions,
an interesting future direction is to consider weaker quorum
requirements subject to bounds on data staleness [14].
VII. EVALUATION METHODOLOGY
We evaluate our replication strategies Latency Only (LAT),
Basic Availability (BA), and N-1 Contingency (N-1C) with a
view to exploring several aspects such as:
• Accuracy of our model in predicting performance
• Limits on latency achievable given consistency constraints
• Beneﬁts and costs of optimizing latency under failures
• Importance of employing heterogeneous conﬁgurations for
• Robustness to variations in network delays and workloads
different groups of data items within an application
We explore these questions using experiments on a real
wide-area Cassandra cluster deployed across all the 8 regions
(and 21 availability zones) of Amazon EC2 and using trace-
driven simulations from three real-world applications: Twitter,
Wikipedia and Gowalla. Our EC2 experiments enable us to
validate our models, and to evaluate the beneﬁts of our
approach in practice. Simulation studies enable us to evaluate
our strategies on a larger scale (hundreds of thousands of data
items), and to explore the impact of workload characteristics
and model parameters on performance. We use GAMS [18]
(a modeling system for optimization problems) and solve the
models using the CPLEX optimizer.
A. Application workloads
The applications we choose are widely used, have geograph-
ically dispersed users who edit and read data, and ﬁt naturally
into a key-value model. We note that both Twitter and Gowalla
are already known to use Cassandra [10]. We discuss details
of the traces below (see table III for summary):
Twitter: We obtained Twitter traces [39] which included a
user friendship graph, a list of user locations, and public
tweets sent by users (along with timestamp) over a 5 year
period. We analyzed Twissandra, an open-source twitter-like
1Aggregating all articles per language (e.g. 4 million articles in English
Wikipedia are aggregated.)
245245245
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:03:40 UTC from IEEE Xplore.  Restrictions apply. 
1.0
0.8
0.6
0.4
0.2
)
s
t
s
e
u
q
e
r
f
o
n
o
i
t
c
a
r
f
(
F
D
C
Predicted writes
Observed writes
Predicted reads
Observed reads
0
50
100
150
200
250
Latency(msec)
Fig. 5. Validating the accuracy of models.
application, and found three types of data items: users, tweets
and timelines. We focus our evaluations on timeline objects
which are pre-materialized views that map each user to a list
of tweets sent by the user and her friends. Writes to a timeline
occur when the associated user or her friends post a tweet, and
can be obtained directly from the trace. Since the traces do not
include reads, we model reads by assuming each user reads
her own timeline periodically (every 10 min), and reads her
friend’s timeline with some probability (0.1) each time the
friend posts a tweet.
Wikipedia: We obtained statistics regarding Wikipedia usage
from [11], which lists the total as well as the breakdown of
the number of views and edits by geographic region for each
language and collaborative project. The data spans a 3 year
period with trends shown on quarterly basis. Our model for
the Wikipedia application consists of article objects with the
document id as a key and the content along with its meta data
(timestamps, version information, etc). Article page views are
modeled as reads while page edits are modeled as writes. Since
per article access data is not available, we model all articles of
the same language and project as seeing similar access patterns
since access patterns are likely dominated by the location of
native speakers of the language.
Gowalla: Gowalla is a (now disabled) geo-social networking
application where users ”check-in” at various locations they
visit and friends receive all
their check-in messages. The
traces [9] contained user friendship relationships, and a list of
all check-ins sent over a two year period. Since the application
workﬂows are similar, we model Gowalla in a similar fashion
to Twitter. Check-ins represent writes to user timelines from
the location of the check-in, and reads to timelines were
modeled like with Twitter.
VIII. EXPERIMENTAL VALIDATION
In this section, we present results from our experiments
using Cassandra deployed on Amazon EC2.
A. Implementation
Off-the-shelf, Cassandra employs a random partitioner that
implements consistent hashing to distribute load across mul-
tiple storage nodes in the cluster. The output range of a hash
function is treated as a ﬁxed circular space and each data
item is assigned to a node by hashing its key to yield its
position on the ring. Nodes assume responsibility for the
region in the ring between itself and its predecessor, with
immediately adjacent nodes in the ring hosting replicas of the
data item. Cassandra allows applications to express replication
policies at the granularity of keyspaces (partitions of data).
We modiﬁed the applications to treat groups of data items
as separate keyspaces and conﬁgure distinct replication policy
for each keyspace. Keyspace creation is a one-time process
and does not affect the application performance. The mapping
from data object to the keyspace is maintained in a separate
directory service. We implemented the directory service as
an independent Cassandra cluster deployed in each of the
DCs and conﬁgured its replication such that lookups(reads)
are served locally within a DC (e.g. R = 1, W = N ).
B. Experimental platform on EC2
We performed our experiments and model validations using
Cassandra deployed on medium size instances on Amazon
EC2. Our datastore cluster comprises of nodes deployed in
each of the 21 distinct availability zones (AZ) across all the
8 regions of EC2 (9 in US, 3 in Europe, 5 in Asia, 2 in
South America and 2 in Australia). We treat availability zones
(AZs) as distinct DC in all our experiments. The inter-DC
delays (21 ∗ 21 pairs) were simultaneously measured for a
period of 24 hours using medium instances deployed on all
the 21 AZs and the median delays values (MED) were used
as input to our models. We mapped users from their locations
to the nearest DC. Since the locations are free-text ﬁelds in
our traces, we make use of geocoding services [3] to obtain
the user’s geographical co-ordinates.
C. Accuracy and model validation
We validate the accuracy of our models with experiments
on our EC2 Cassandra cluster described above. We use the
example from our Twitter trace (Figure 2) for this experiment.
Replica conﬁgurations were generated with the MED delay
values measured earlier and read/write requests to Cassandra
cluster were generated from application servers deployed at
the corresponding DCs as per the trace data. The duration of
the entire experiment was about 6 hours.
Figure 5 shows the CDFs of the observed and predicted
latencies for read and write requests for the BA conﬁguration.
The CDFs almost overlap for write requests, while we observe
a delay of approximately 9 msec evenly for all read requests.
This constant delay difference in the reads can be attributed to
the processing overhead of read requests in Cassandra which
includes reconciling the response of multiple replicas to ensure
consistency of the read data. Overall, our results validate the
accuracy of our models. They also show that our solutions
are fairly robust to the natural delay variations present in real
cloud platforms.
D. Beneﬁts of performance sensitive replication
We ﬁrst evaluate the beneﬁts of ﬂexible replication policy
over a ﬁxed replication policy on the EC2 Cassandra cluster
described above. For this experiment, we use a month long
trace from Twitter consisting of 524, 759 objects correspond-
ing to user timelines in Twitter. The replica conﬁgurations
246246246
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:03:40 UTC from IEEE Xplore.  Restrictions apply. 
1.0
0.8
0.6
0.4
0.2
)
s
t
s
e
u
q
e
r
f
o
n
o
i
t
c
a
r
f
(
F
D
C
Reads - Default
Writes - Default
Reads - BA
Writes - BA
All
alive
APN-1a 
fails
APN-1a
back
USW-1a 
fails
USW-1a 
back
USW-2a 
fails
USW-2a 
back
All
alive
USW-1c
fails
USW-1c
back
USW-2a 
fails
USW-2a 
back
USW-1a 
fails
USW-1a 