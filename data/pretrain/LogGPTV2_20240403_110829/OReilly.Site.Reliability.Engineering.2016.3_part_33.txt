requests that it can process and reject the rest gracefully.
While we have a vast array of tools to implement good load balancing and overload
protections, there is no magic bullet: load balancing often requires deep understand‐
ing of a system and the semantics of its requests. The techniques described in this
chapter have evolved along with the needs of many systems at Google, and will likely
continue to evolve as the nature of our systems continues to change.
258 | Chapter 21: Handling Overload
CHAPTER 22
Addressing Cascading Failures
Written by Mike Ulrich
If at first you don’t succeed, back off exponentially.
—Dan Sandler, Google Software Engineer
Why do people always forget that you need to add a little jitter?
—Ade Oshineye, Google Developer Advocate
A cascading failure is a failure that grows over time as a result of positive feedback.1 It
can occur when a portion of an overall system fails, increasing the probability that
other portions of the system fail. For example, a single replica for a service can fail
due to overload, increasing load on remaining replicas and increasing their probabil‐
ity of failing, causing a domino effect that takes down all the replicas for a service.
We’ll use the Shakespeare search service discussed in “Shakespeare: A Sample Service”
on page 20 as an example throughout this chapter. Its production configuration
might look something like Figure 22-1.
1 See Wikipedia, “Positive feedback,” https://en.wikipedia.org/wiki/Positive_feedback.
259
Figure 22-1. Example production configuration for the Shakespeare search service
Causes of Cascading Failures and Designing to Avoid Them
Well-thought-out system design should take into account a few typical scenarios that
account for the majority of cascading failures.
Server Overload
The most common cause of cascading failures is overload. Most cascading failures
described here are either directly due to server overload, or due to extensions or var‐
iations of this scenario.
Suppose the frontend in cluster A is handling 1,000 requests per second (QPS), as in
Figure 22-2.
Figure 22-2. Normal server load distribution between clusters A and B
260 | Chapter 22: Addressing Cascading Failures
If cluster B fails (Figure 22-3), requests to cluster A increase to 1,200 QPS. The
frontends in A are not able to handle requests at 1,200 QPS, and therefore start run‐
ning out of resources, which causes them to crash, miss deadlines, or otherwise mis‐
behave. As a result, the rate of successfully handled requests in A dips well below
1,000 QPS.
Figure 22-3. Cluster B fails, sending all traffic to cluster A
This reduction in the rate of useful work being done can spread into other failure
domains, potentially spreading globally. For example, local overload in one cluster
may lead to its servers crashing; in response, the load balancing controller sends
requests to other clusters, overloading their servers, leading to a service-wide over‐
load failure. It may not take long for these events to transpire (e.g., on the order of a
couple minutes), because the load balancer and task scheduling systems involved may
act very quickly.
Resource Exhaustion
Running out of a resource can result in higher latency, elevated error rates, or the
substitution of lower-quality results. These are in fact desired effects of running out
of resources: something eventually needs to give as the load increases beyond what a
server can handle.
Depending on what resource becomes exhausted in a server and how the server is
built, resource exhaustion can render the server less efficient or cause the server to
crash, prompting the load balancer to distribute the resource problems to other
servers. When this happens, the rate of successfully handled requests can drop and
possibly send the cluster or an entire service into a cascade failure.
Different types of resources can be exhausted, resulting in varying effects on servers.
Causes of Cascading Failures and Designing to Avoid Them | 261
CPU
If there is insufficient CPU to handle the request load, typically all requests become
slower. This scenario can result in various secondary effects, including the following:
Increased number of in-flight requests
Because requests take longer to handle, more requests are handled concurrently
(up to a possible maximum capacity at which queuing may occur). This affects
almost all resources, including memory, number of active threads (in a thread-
per-request server model), number of file descriptors, and backend resources
(which in turn can have other effects).
Excessively long queue lengths
If there is insufficient capacity to handle all the requests at steady state, the server
will saturate its queues. This means that latency increases (the requests are
queued for longer amounts of time) and the queue uses more memory. See
“Queue Management” on page 266 for a discussion of mitigation strategies.
Thread starvation
When a thread can’t make progress because it’s waiting for a lock, health checks
may fail if the health check endpoint can’t be served in time.
CPU or request starvation
Internal watchdogs2 in the server detect that the server isn’t making progress,
causing the servers to crash due to CPU starvation, or due to request starvation if
watchdog events are triggered remotely and processed as part of the request
queue.
Missed RPC deadlines
As a server becomes overloaded, its responses to RPCs from its clients arrive
later, which may exceed any deadlines those clients set. The work the server did
to respond is then wasted, and clients may retry the RPCs, leading to even more
overload.
Reduced CPU caching benefits
As more CPU is used, the chance of spilling on to more cores increases, resulting
in decreased usage of local caches and decreased CPU efficiency.
2 A watchdog is often implemented as a thread that wakes up periodically to see whether work has been done
since the last time it checked. If not, it assumes that the server is stuck and kills it. For instance, requests of a
known type can be sent to the server at regular intervals; if one hasn’t been received or processed when
expected, this may indicate failure—of the server, the system sending requests, or the intermediate network.
262 | Chapter 22: Addressing Cascading Failures
Memory
If nothing else, more in-flight requests consume more RAM from allocating the
request, response, and RPC objects. Memory exhaustion can cause the following
effects:
Dying tasks
For example, a task might be evicted by the container manager (VM or other‐
wise) for exceeding available resource limits, or application-specific crashes may
cause tasks to die.
Increased rate of garbage collection (GC) in Java, resulting in increased CPU usage
A vicious cycle can occur in this scenario: less CPU is available, resulting in
slower requests, resulting in increased RAM usage, resulting in more GC, result‐
ing in even lower availability of CPU. This is known colloquially as the “GC death
spiral.”
Reduction in cache hit rates
Reduction in available RAM can reduce application-level cache hit rates, result‐
ing in more RPCs to the backends, which can possibly cause the backends to
become overloaded.
Threads
Thread starvation can directly cause errors or lead to health check failures. If the
server adds threads as needed, thread overhead can use too much RAM. In extreme
cases, thread starvation can also cause you to run out of process IDs.
File descriptors
Running out of file descriptors can lead to the inability to initialize network connec‐
tions, which in turn can cause health checks to fail.
Dependencies among resources
Note that many of these resource exhaustion scenarios feed from one another—a ser‐
vice experiencing overload often has a host of secondary symptoms that can look like
the root cause, making debugging difficult.
For example, imagine the following scenario:
1. A Java frontend has poorly tuned garbage collection (GC) parameters.
2. Under high (but expected) load, the frontend runs out of CPU due to GC.
3. CPU exhaustion slows down completion of requests.
4. The increased number of in-progress requests causes more RAM to be used to
process the requests.
Causes of Cascading Failures and Designing to Avoid Them | 263
5. Memory pressure due to requests, in combination with a fixed memory alloca‐
tion for the frontend process as a whole, leaves less RAM available for caching.
6. The reduced cache size means fewer entries in the cache, in addition to a lower
hit rate.
7. The increase in cache misses means that more requests fall through to the back‐
end for servicing.
8. The backend, in turn, runs out of CPU or threads.
9. Finally, the lack of CPU causes basic health checks to fail, starting a cascading
failure.
In situations as complex as the preceding scenario, it’s unlikely that the causal chain
will be fully diagnosed during an outage. It might be very hard to determine that the
backend crash was caused by a decrease in the cache rate in the frontend, particularly
if the frontend and backend components have different owners.
Service Unavailability
Resource exhaustion can lead to servers crashing; for example, servers might crash
when too much RAM is allocated to a container. Once a couple of servers crash on
overload, the load on the remaining servers can increase, causing them to crash as
well. The problem tends to snowball and soon all servers begin to crash-loop. It’s
often difficult to escape this scenario because as soon as servers come back online
they’re bombarded with an extremely high rate of requests and fail almost immedi‐
ately.
For example, if a service was healthy at 10,000 QPS, but started a cascading failure
due to crashes at 11,000 QPS, dropping the load to 9,000 QPS will almost certainly
not stop the crashes. This is because the service will be handling increased demand
with reduced capacity; only a small fraction of servers will usually be healthy enough
to handle requests. The fraction of servers that will be healthy depends on a few fac‐
tors: how quickly the system is able to start the tasks, how quickly the binary can start
serving at full capacity, and how long a freshly started task is able to survive the load.
In this example, if 10% of the servers are healthy enough to handle requests, the
request rate would need to drop to about 1,000 QPS in order for the system to stabi‐
lize and recover.
Similarly, servers can appear unhealthy to the load balancing layer, resulting in
reduced load balancing capacity: servers may go into “lame duck” state (see “A Robust
Approach to Unhealthy Tasks: Lame Duck State” on page 234) or fail health checks
without crashing. The effect can be very similar to crashing: more servers appear
unhealthy, the healthy servers tend to accept requests for a very brief period of time
before becoming unhealthy, and fewer servers participate in handling requests.
264 | Chapter 22: Addressing Cascading Failures
Load balancing policies that avoid servers that have served errors can exacerbate
problems further—a few backends serve some errors, so they don’t contribute to the
available capacity for the service. This increases the load on the remaining servers,
starting the snowball effect.
Preventing Server Overload
The following list presents strategies for avoiding server overload in rough priority
order:
Load test the server’s capacity limits, and test the failure mode for overload
This is the most important important exercise you should conduct in order to
prevent server overload. Unless you test in a realistic environment, it’s very hard
to predict exactly which resource will be exhausted and how that resource
exhaustion will manifest. For details, see “Testing for Cascading Failures” on page
278.
Serve degraded results
Serve lower-quality, cheaper-to-compute results to the user. Your strategy here
will be service-specific. See “Load Shedding and Graceful Degradation” on page
267.
Instrument the server to reject requests when overloaded
Servers should protect themselves from becoming overloaded and crashing.
When overloaded at either the frontend or backend layers, fail early and cheaply.
For details, see “Load Shedding and Graceful Degradation” on page 267.
Instrument higher-level systems to reject requests, rather than overloading servers
Note that because rate limiting often doesn’t take overall service health into
account, it may not be able to stop a failure that has already begun. Simple rate-
limiting implementations are also likely to leave capacity unused. Rate limiting
can be implemented in a number of places:
• At the reverse proxies, by limiting the volume of requests by criteria such as
IP address to mitigate attempted denial-of-service attacks and abusive
clients.
• At the load balancers, by dropping requests when the service enters global
overload. Depending on the nature and complexity of the service, this rate
limiting can be indiscriminate (“drop all traffic above X requests per sec‐
ond”) or more selective (“drop requests that aren’t from users who have
recently interacted with the service” or “drop requests for low-priority opera‐
tions like background synchronization, but keep serving interactive user
sessions”).
Preventing Server Overload | 265
• At individual tasks, to prevent random fluctuations in load balancing from
overwhelming the server.
Perform capacity planning
Good capacity planning can reduce the probability that a cascading failure will
occur. Capacity planning should be coupled with performance testing to deter‐
mine the load at which the service will fail. For instance, if every cluster’s break‐
ing point is 5,000 QPS, the load is evenly spread across clusters,3 and the service’s
peak load is 19,000 QPS, then approximately six clusters are needed to run the
service at N + 2.
Capacity planning reduces the probability of triggering a cascading failure, but it is
not sufficient to protect the service from cascading failures. When you lose major
parts of your infrastructure during a planned or unplanned event, no amount of
capacity planning may be sufficient to prevent cascading failures. Load balancing
problems, network partitions, or unexpected traffic increases can create pockets of
high load beyond what was planned. Some systems can grow the number of tasks for
your service on demand, which may prevent overload; however, proper capacity
planning is still needed.
Queue Management
Most thread-per-request servers use a queue in front of a thread pool to handle
requests. Requests come in, they sit on a queue, and then threads pick requests off the
queue and perform the actual work (whatever actions are required by the server).
Usually, if the queue is full, the server will reject new requests.
If the request rate and latency of a given task is constant, there is no reason to queue
requests: a constant number of threads should be occupied. Under this idealized sce‐
nario, requests will only be queued if the steady state rate of incoming requests
exceeds the rate at which the server can process requests, which results in saturation
of both the thread pool and the queue.
Queued requests consume memory and increase latency. For example, if the queue
size is 10x the number of threads, the time to handle the request on a thread is 100
milliseconds. If the queue is full, then a request will take 1.1 seconds to handle, most
of which time is spent on the queue.
For a system with fairly steady traffic over time, it is usually better to have small
queue lengths relative to the thread pool size (e.g., 50% or less), which results in the
server rejecting requests early when it can’t sustain the rate of incoming requests. For
example, Gmail often uses queueless servers, relying instead on failover to other
3 This is often not a good assumption due to geography; see also “Job and Data Organization” on page 22.
266 | Chapter 22: Addressing Cascading Failures
server tasks when the threads are full. On the other end of the spectrum, systems with
“bursty” load for which traffic patterns fluctuate drastically may do better with a
queue size based on the current number of threads in use, processing time for each
request, and the size and frequency of bursts.
Load Shedding and Graceful Degradation
Load shedding drops some proportion of load by dropping traffic as the server
approaches overload conditions. The goal is to keep the server from running out of
RAM, failing health checks, serving with extremely high latency, or any of the other
symptoms associated with overload, while still doing as much useful work as it can.
One straightforward way to shed load is to do per-task throttling based on CPU,
memory, or queue length; limiting queue length as discussed in “Queue Manage‐
ment” on page 266 is a form of this strategy. For example, one effective approach is to
return an HTTP 503 (service unavailable) to any incoming request when there are
more than a given number of client requests in flight.
Changing the queuing method from the standard first-in, first-out (FIFO) to last-in,
first-out (LIFO) or using the controlled delay (CoDel) algorithm [Nic12] or similar
approaches can reduce load by removing requests that are unlikely to be worth pro‐
cessing [Mau15]. If a user’s web search is slow because an RPC has been queued for
10 seconds, there’s a good chance the user has given up and refreshed their browser,
issuing another request: there’s no point in responding to the first one, since it will be
ignored! This strategy works well when combined with propagating RPC deadlines
throughout the stack, described in “Latency and Deadlines” on page 271.
More sophisticated approaches include identifying clients to be more selective about
what work is dropped, or picking requests that are more important and prioritizing.
Such strategies are more likely to be needed for shared services.
Graceful degradation takes the concept of load shedding one step further by reducing
the amount of work that needs to be performed. In some applications, it’s possible to
significantly decrease the amount of work or time needed by decreasing the quality of
responses. For instance, a search application might only search a subset of data stored