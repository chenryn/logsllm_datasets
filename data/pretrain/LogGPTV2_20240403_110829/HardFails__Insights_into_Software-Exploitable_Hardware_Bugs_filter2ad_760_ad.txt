disclosing sensitive information.
Erroneous AXI ﬁnite-state machine: We injected a bug
(bug #7) in the AXI address decoder such that, if an error
signal is generated on the memory bus while the underlining
logic is still handling an outstanding transaction, the next sig-
nal to be handled will instead be considered operational by the
module unconditionally. This bug can be exploited to cause
computational faults in the execution of security-critical code
(we showcase how to exploit this vulnerability—which was
not detected by all teams—in Appendix D).
Cryptographic unit bugs: We injected bugs in a crypto-
graphic unit that we inserted to trigger denial-of-service, a
broken cryptographic implementation, insecure key storage,
and disallowed information leakage. Bugs #18, 19, 20, 21,
and 29 are such examples.
5.4 Competition Results
Various insights were drawn from the submitted bug reports
and results, which are summarized in Table 1.
Analyzing the bug reports: Bug reports submitted by teams
revealed which bug types were harder to detect and analyze
using existing approaches. We evaluated the submissions and
rated them for accuracy and detail, e.g., bug validity, method-
ology used, and security impact.
Detected bugs: Most teams easily detected two bugs in
PULPissimo. The ﬁrst one is where debug IPs were used
when not intended. The second bug was where we declared
a local parameter PULP_SEC, which was always set to ’1’,
instead of the intended PULP_SECURE. The former was de-
tected because debugging interfaces represent security-critical
regions of the chip. The latter was detected because it indi-
cated intuitively that exploiting this parameter would lead
to privilege escalation attacks. The teams reported that they
prioritized inspecting security-relevant modules of the SoC,
such as the debug interfaces.
Undetected bugs: Many inserted bugs were not detected.
One was in the advanced debug unit, where the password bit
index register has an overﬂow (bug #9). This is an example of
a security ﬂaw that would be hard to detect by methods other
than veriﬁcation. Moreover, the presence of many bugs within
the advanced debug unit password checker further masked
this bug. Another bug was the cryptographic unit key storage
in unprotected memory (bug #20). The teams could not detect
it as they focused on the RTL code in isolation and did not
consider HW/FW interactions.
Techniques used by the teams: The teams were free to use
any techniques to detect the bugs but most teams eventually
relied on manual inspection and simulation.
• Formal veriﬁcation: One team used an open-source
formal veriﬁcation tool (VeriCoq), but they reported little
success because these tools (i) did not scale well with
the complete SoC and (ii) required expertise to use and
deﬁne the security properties. Some teams deployed
their in-house veriﬁcation techniques, albeit with little
success. They eventually resorted to manual analysis.
• Assertion-based simulation: Some teams prepared
RTL testbenches and conducted property-based simu-
lations using SystemVerilog assertion statements.
• Manual inspection: All teams relied on manual inspec-
tion methods since they are the easiest and most accessi-
ble and require less expertise than formal veriﬁcation, es-
pecially when working under time constraints. A couple
of teams reported prioritizing the inspection of security-
critical modules such as debug interfaces.
• Software-based testing: One team detected software-
exposure and privilege escalation bugs by running C
code on the processor and attempting to make arbitrary
reads/writes to privileged memory locations. In doing
this, they could detect bugs #4, #8, #15, and #17.
Limitations of manual analysis: While manual inspection
can detect the widest array of bugs, our analysis of the
Hack@DAC results reveals its limitations. Manual analysis
is qualitative and difﬁcult to scale to cross-layer and more
complex bugs. In Table 1, out of 16 cross-module bugs (span-
ning more than one module) only 9 were identiﬁed using
manual inspection. Three of them (#18, #19, and #20) were
also undetected by formal veriﬁcation methods, which is 10%
of the bugs in our case studies.
6 Detection Using State-of-The-Art Tools
Our study reveals two results: (1) a number of bugs could not
be detected by means of manual auditing and other ad-hoc
methods, and (2) the teams were able to ﬁnd bugs already
existing in the SoC which we did not inject and were not
USENIX Association
28th USENIX Security Symposium    221
aware of. This prompted us to conduct a second in-house
case study to further investigate whether formal veriﬁcation
techniques can be used to detect these bugs. In practice,
hardware-security veriﬁcation engineers use a combination of
techniques such as formal veriﬁcation, simulation, emulation,
and manual inspection. Our ﬁrst case study covered manual
inspection, simulation and emulation techniques. Thus, we
focused our second case study on assessing the effectiveness
of industry-standard formal veriﬁcation techniques usually
used for verifying pre-silicon hardware security.
In real-world security testing (see Section 2), engineers will
not have prior knowledge of the speciﬁc vulnerabilities they
are trying to ﬁnd. Our goal, however, is to investigate how an
industry-standard tool can detect RTL bugs that we deliber-
ately inject in an open-source SoC and have prior knowledge
of (see Table 1). Since there is no regulation or explicitly de-
ﬁned standard for hardware-security veriﬁcation, we focus our
investigation on the most popular and de-facto standard for-
mal veriﬁcation platform used in industry [11]. This platform
encompasses a representative suite of different state-of-the-art
formal veriﬁcation techniques for hardware security assur-
ance. As opposed to simulation and emulation techniques,
formal veriﬁcation guarantees to model the state space of the
design and formally prove the desired properties. We empha-
size that we deliberately ﬁx all other variables involved in the
security testing process, in order to focus in a controlled set-
ting on testing the capacity and limitations of the techniques
and tools themselves. Thus, our results reﬂect the effective-
ness of tools in a best case where the bug is known a priori.
This eliminates the possibility of writing an incorrect security
property assertion which fails to detect the bug.
6.1 Detection Methodology
We examined each of the injected bugs and its nature in order
to determine which formal technique would be best suited to
detect it. We used two formal techniques: Formal Property
Veriﬁcation (FPV) and JasperGold’s Security Path Veriﬁca-
tion (SPV) [12]. They represent the state of the art in hardware
security veriﬁcation and are used widely by the semiconductor
industry [4], including Intel.
FPV checks whether a set of security properties, usually
speciﬁed as SystemVerilog Assertions (SVA), hold true for
the given RTL. To describe the assertions correctly, we exam-
ined the location of each bug in the RTL and how its behavior
is manifested with the surrounding logic and input/output re-
lationships. Once we speciﬁed the security properties using
assert, assume and cover statements, we determined which
RTL modules we need to model to prove these assertions.
If a security property is violated, the tool generates a coun-
terexample; this is examined to ensure whether the intended
security property is indeed violated or is a false alarm.
SPV detects bugs which speciﬁcally involve unauthorized
information ﬂow. Such properties cannot be directly captured
using SVA/PSL assertions. SPV uses path sensitization tech-
FIGURE 3: Veriﬁcation results grouped by bug class and
the number of bugs in each class detected by Security Path
Veriﬁcation (SPV), Formal Property Veriﬁcation (FPV) and
manual inspection and simulation techniques (M&S).
niques to exhaustively and formally check if unauthorized
data propagates (through a functional path) from a source
to a destination signal. To specify the SPV properties, we
identiﬁed source signals where the sensitive information was
located and destination signals where it should not propagate.
We then identiﬁed the bounding preconditions to constrain the
paths the tool searches to alleviate state and time explosion.
Similar to FPV, we identiﬁed the modules that are required
to capture the information ﬂow of interest. This must include
source, destination and intermediate modules, as well as mod-
ules that generate control signals which interfere with the
information ﬂow.
6.2 Detection Results
Of the 31 bugs we investigated, shown in Table 1, using the
formal veriﬁcation techniques described above, only 15 (48%)
were detected. While we attempted to detect all 31 bugs for-
mally, we were able to formulate security properties for only
17 bugs. This indicates that the main challenge with using
formal veriﬁcation tools is identifying and expressing security
properties that the tools are capable of capturing and checking.
Bugs due to ambiguous speciﬁcations of interconnect logic,
for instance, are examples of bugs that are difﬁcult to create
security properties for.
Our results, shown in Figure 3, indicate that privilege es-
calation and denial-of-service (DoS) bugs were the most de-
tected at 60% and 67% respectively. Secret leakage only had
a 17% detection rate due to incorrect design speciﬁcation for
one bug, state explosion and the inability to express proper-
ties that the tool can assert for the remaining bugs. The code
injection bug was undetected by formal techniques. Bugs at
the interconnect level of the SoC such as bugs #1 and #2 were
especially challenging since they involved a large number of
highly complex and inter-connected modules that needed to be
loaded and modeled by the tool (see L-1 in Section 3.1). Bug
#20, which involves hardware/ﬁrmware interactions, was also
222    28th USENIX Security Symposium
USENIX Association
Privilege escalationDoSSecretleakageCodeinjectionBug class1234567# of bugsSPVFPVM&S Undetecteddetected by neither the state-of-the-art FPV nor SPV since
they analyze the RTL in isolation (see L-4 in Section 3.1). We
describe these bugs in more detail in Appendix C.
6.3 State-Explosion Problem
Formal veriﬁcation techniques are quickly driven into state
space explosion when analyzing large designs with many
states. Many large interconnected RTL modules, like those
relevant to bugs #1 and #2, can have states in the order of
magnitude of 1020. Even smaller ones, like these used for bugs
#3 and #4, can have a very large number of states, as shown
in Table 1. When combined, the entire SoC will have a total
number of states signiﬁcantly higher than any of the results
in Table 1. Attempting to model the entire SoC drove the tool
into state explosion, and it ran out of memory and crashed.
Formal veriﬁcation tools, including those speciﬁc to security
veriﬁcation are currently incapable of handling so many states,
even when computational resources are increased. This is
further aggravated for industry-standard complex SoCs.
Because the entire SoC cannot be modeled and analyzed at
once, detecting cross-modular bugs becomes very challeng-
ing. Engineers work around this (not fundamentally solve
it) by adopting a divide-and-conquer approach and selecting
which modules are relevant for the properties being tested
and which can be black-boxed or abstracted. However, this
is time-consuming, non-automated, error-prone, and requires
expertise and knowledge of both the tools and design. By
relying on the human factor, the tool can no longer guarantee
the absence of bugs for the entire design, which is the original
advantage of formal veriﬁcation.
7 Discussion and Future Work
We now describe why microcode patching is insufﬁcient for
RTL bugs while emphasizing the need for advancing the hard-
ware security veriﬁcation process. We discuss the additional
challenges of the overall process, besides the limitations of
the industry-standard tools, which is the focus of this work.
7.1 Microcode Patching
While existing industry-grade SoCs support hotﬁxes by mi-
crocode patching for instance, this approach is limited to a
handful of changes to the instruction set architecture, e.g.,
modifying the interface of individual complex instructions
and adding or removing instructions [25]. Some vulnerabili-
ties cannot even be patched by microcode, such as the recent
Spoiler attack [33]. Fundamentally mitigating this requires
ﬁxing the hardware of the memory subsystem at the hardware
design phase. For legacy systems, the application developer is
advised to follow best practices for developing side channel-
resilient software5. For vulnerabilities that can be patched,
patches at this higher abstraction level in the ﬁrmware only
act as a "symptomatic" ﬁx that circumvent the RTL bug. How-
ever, they do not fundamentally patch the bug in the RTL,
which is already realized as hardwired logic. Thus, microcode
patching is a fallback for RTL bugs discovered after produc-
tion, when you can not patch the RTL. They may also incur
performance impact 6 that could be avoided if the underlying
problem is discovered and ﬁxed during design.
7.2 Additional Challenges in Practice
Functional vs. Security Speciﬁcations. As described in Sec-
tion 2, pre- and post-silicon validation efforts are conducted
to verify that the implementation fully matches both its func-
tional and security speciﬁcations. The process becomes in-
creasingly difﬁcult (almost impossible) as the system com-
plexity increases and speciﬁcation ambiguity arises. Devi-
ations from speciﬁcation occur due to either functional or
security bugs, and it is important to distinguish between them.
While functional bugs generate functionally incorrect results,
security bugs are not reﬂected in functionality. They arise due
to unconsidered and corner threat cases that are unlikely to
get triggered, thus making them more challenging to detect
and cover. It is, therefore, important to distinguish between
functional and security speciﬁcations, since these are often
the references for different veriﬁcation teams working con-
currently on the same RTL implementation.
Speciﬁcation Ambiguity. Another challenge entails antic-
ipating and identifying all the security properties that are
required in a real-world scenario. We analyzed the efﬁcacy
of industry-standard tools in a controlled setting—where we
have prior knowledge of the bugs. However, in practice hard-
ware validation teams do not have prior knowledge of the
bugs. Security speciﬁcations are often incomplete and am-
biguous, only outlining the required security properties under
an assumed adversary model. These speciﬁcations are inval-
idated once the adversary model is changed. This is often
the case with IP reuse, where the RTL code for one product
is re-purposed for another with a different set of security re-
quirements and usage scenarios. Parameters may be declared
multiple times and get misinterpreted by the tools, thus caus-
ing bugs to slip undetected. Furthermore, specs usually do
not specify bugs and information ﬂows that should not exist,
and there is no automated approach to determine whether one
is proving the intended properties. Thus, a combination of
incomplete or incorrect design decisions and implementation
errors can easily introduce bugs to the design.
5https://www.intel.com/content/www/us/en/security-
center/advisory/intel-sa-00238.html
6https://access.redhat.com/articles/3307751
USENIX Association
28th USENIX Security Symposium    223
7.3 Future Research Directions
Through our work, we shed light on the limitations of state-
of-the-art veriﬁcation techniques. In doing so, we hope to
motivate further research in advancing these techniques to
adequately capture and detect these vulnerabilities.
Although manual RTL inspection is generally useful and
can potentially cover a wide array of bugs, its efﬁcacy de-
pends exclusively on the expertise of the engineer. This can
be inefﬁcient, unreliable and ad hoc in light of rapidly evolv-
ing chip designs. Exhaustive testing of speciﬁcations through
simulation requires amounts of resources exponential in the
size of the input (i.e., design state space) while coverage
must be intelligently maximized. Hence, current approaches
face severe scalability challenges, as diagnosing software-
exploitable bugs that reside deep in the design pipeline can
require simulation of trillions of cycles [14]. Our results indi-
cate that it is important to ﬁrst identify high-risk components
due to software exposure, such as password checkers, crypto
cores, and control registers, and prioritize analyzing them.
Scalability due to complex inter-dependencies among mod-
ules is one challenge for detection. Vulnerabilities associated
with non-register states (such as caches) or clock-cycle depen-
dencies (i.e., timing ﬂows) are another open problem. Initial
research is underway [71] to analyze a limited amount of
low-level ﬁrmware running on top of a simulated RTL de-
sign for information and timing ﬂow violations. However,
these approaches are still in their infancy and yet to scale for
real-world SoC designs.
8 Related Work
We now present related work in hardware security veriﬁca-
tion while identifying limitations with respect to detecting
HardFails. We also provide an overview of recent software
attacks exploiting underlying hardware vulnerabilities.
8.1 Current Detection Approaches
Security-aware design of hardware has gained signiﬁcance
only recently as the critical security threat posed by hardware
vulnerabilities became acutely established. Conﬁdentiality
and integrity are the commonly investigated properties [19]
in hardware security. They are usually expressed using infor-
mation ﬂow properties between entities at different security
levels. Besides manual inspection and simulation-based tech-
niques, systematic approaches proposed for verifying hard-
ware security properties include formal veriﬁcation methods
such as proof assistance, model-checking, symbolic execu-
tion, and information ﬂow tracking. We exclude the related
work in testing mechanisms, e.g., JTAG/scan-chain/built-in
self-test, because they are leveraged for hardware testing af-
ter fabrication. However, the focus of this work is on veri-
fying the security of the hardware before fabrication. Inter-
estingly, this includes verifying that the test mechanisms are
correctly implemented in the RTL, otherwise they may consti-
tute security vulnerabilities when used after fabrication (see
bugs#9,#10,#11,#12,#16, #26 of the JTAG/debug interface).
Proof assistant and theorem-proving methods rely on
mathematically modeling the system and the required secu-
rity properties into logical theorems and formally proving if
the model complies with the properties. VeriCoq [7] based on
the Coq proof assistant transforms the Verilog code that de-
scribes the hardware design into proof-carrying code.VeriCoq
supports the automated conversion of only a subset of Verilog
code into Coq. However, this assumes accurate labeling of the
initial sensitivity labels of each and every signal in order to
effectively track the ﬂow of information. This is cumbersome,
error-prone, generates many faluse positives, and does not