Note 1: The method cannot estimate the order of the system
2n. If n is not known apriori, it is usually considered to be a
large number, and thereafter the modes with negligible residues
are discarded. However, n is limited by the number of available
measurements as well as the computational memory.
Note 2: The method can be performed in real-time. That
is, one may solve (5) while iteratively updating Hi, and ci as
new measurements become available.
IV. PROPOSED STRATEGIES FOR DISTRIBUTED MODAL
ESTIMATION
In this section, we show how Step 1 of the centralized
approach delineated in Section III can be recast as a distributed
optimization problem using intra- and inter-regional PMU-PDC
architectures. We wish to clarify at the very outset that the
term ‘PDC’ in our algorithms is used in a much broader sense,
and not just as a data aggregator. It essentially refers to any
computing agent that can process PMU data and run algorithms
on them. In other words, as long as the data from a given
PMU get communicated to a computing station, whether it
be a hardware PDC, software PDC, a local server, or even
a data center - we would collectively refer to all of these
processing units as a ‘PDC’ for convenience. We assume the
CPS infrastructure of the grid to be divided into five distinct
layers. The lowest layer contains the stochastic variations in
loads and events due to nature and human activities. The
second level is the physical power system model. The third
and fourth layers consist of real-time PMU data processing and
computation at the substation level and the control center level,
respectively. Between the second, third and fourth layers, data
communication architecture is configured to handle massive
amounts of PMU data using the network shown in Fig. 1.
The topmost layer is the application layer, which for our
purpose is estimation of the eigenvalues of the matrix A
in (1) using the distributed communication protocol of this
N(cid:88)
3
network. If necessary, an internal hierarchy of multiple area-
level decentralization layers can also be created (in fact, our
third architecture is based on this assumption). Too many sub-
layers, however, can lead to unacceptable processing delays and
latency violations. Based on these intuitions, we next describe
our proposed computational architectures.
A. Architecture 1: Distributed Prony using Standard ADMM
(S-ADMM)
The LS problem (5) can be regarded as a global consensus
problem over a network of N utility companies or areas. We
assume every area to be equipped with one aggregated PDC as
shown in Fig. 2. The consensus problem can be described as
(cid:107) ˆHjaj − ˆcj(cid:107)2,
minimize
a1,...,aN ,z
subject to aj − z = 0, for j = 1, . . . , N.
1
2
j=1
(7)
···
j,Nj
j,Nj
H T
H T
j,2
j,1 cT
ˆHj (cid:44) [H T
j,2 ··· cT
]T , ˆcj (cid:44)
Here,
j,1
]T , where Nj is the total number of PMUs
[cT
in Area j, and Hj,i and cj,i are constructed as in (4) from the
time samples of ∆θj,i, which is the ith PMU measurement in
Area j, i = 1, . . . , Nj. The global consensus solution, denoted
by z ∈ R2n, is the solution of (5) that is obtained when
the local estimates of the N regional PDCs, denoted by aj,
j = 1, . . . , N, reach the same value.
The standard ADMM (S-ADMM) estimation method uses
the Lagrangian multiplier approach to solve (7) in an iterative,
distributed way [11]. The augmented Lagrangian for (7) is:
Lρ =
(
(cid:107) ˆHjaj − ˆcj(cid:107)2 + wT
j (aj − z) +
1
2
(cid:107)aj − z(cid:107)2),
(8)
ρ
2
N(cid:88)
j=1
where aj and z are the vectors of the primal variables, wj is
the vector of the dual variables or the Lagrange multipliers
associated with (7), and ρ > 0 denotes a penalty factor.
Before the S-ADMM algorithm starts, the central PDC fixes
the order of the system 2n, and the initial height of the Hankel
blocks ((cid:96) + 1) for all local PDCs as shown in (4). Each local
PDC j (i.e., the PDC located in Area j) then waits until the
(2n+(cid:96))th sample of the measurement arrives. In order to ensure
the real-time nature of the algorithm, at iteration k, each local
PDC j constructs the matrices ˆH k
j as follows:
j and ˆck
Fig. 2: Architecture 1 using S-ADMM for a 4-area network.
PMUPMUPMUPMUCentral(cid:3)PDCat(cid:3)ISO(cid:537)11(t)(cid:537)12(t)(cid:537)31(t)(cid:537)32(t)(cid:537)21(t)(cid:537)22(t)(cid:537)41(t)(cid:537)42(t)Area 1Area 3Area 2Area 44ka3ka1ka2kakakakakaˆH k
j
(cid:44) [(H k
j,1)T · · · (H k
j,Nj
)T ]T , ˆck
j
(cid:44) [(ck
j,1)T · · · (ck
j,Nj
)T ]T ,
(9)
where
∆θj,i(2n − 1)
∆θj,i(2n)
j,i (cid:44)(cid:2)∆θj,i(2n) ∆θj,i(2n + 1)
∆θj,i(mk − 1)
···
···
...
∆θj,i(0)
∆θj,i(1)
 ,
··· ∆θj,i(mk)(cid:3)T ,
...
··· ∆θj,i(mk − 2n)
j,i (cid:44)
H k
ck
(10a)
(10b)
for i = 1, . . . , Nj. Here, ∆θj,i(mk) is the most recent
measurement sample available to PDC j from the ith PMU in
its area at iteration k. Using (9)–(10), the S-ADMM algorithm
can be written as:
Distributed Prony using Standard ADMM
1) Each PDC j initializes a0
2) At iteration k:
j, z0, and w0
j , j = 1, . . . , N.
a) PDC j constructs ˆH k
b) PDC j updates aj as:
j and ˆck
j from (9).
ak+1
j = arg min
aj
j )T ˆH k
= (( ˆH k
Lρ
j + ρI2n)−1(( ˆH k
j )T ˆck
j − wk
j + ρzk).
(11)
c) PDC j transmits ak+1
d) Central PDC calculates
j
to the central PDC.
zk+1 (cid:44) ¯ak+1 =
1
N
N(cid:88)
j=1
4
their processing speeds as well as due to various communication
delays such as routing, queing, and transfer delays in the
SDN shown in Fig. 1. One possible solution to overcome
this asynchrony is to force the central PDC to wait until
it receives data from all local PDCs. In that case, the total
end-to-end delay for each iteration will be dependent on the
slowest communication link, and hence the entire algorithm
may become very slow. An alternative approach would be
to use the recently-developed method called Asynchronous
ADMM (A-ADMM) [20], [21]. In this method, the central PDC
receives the updates only from a subset of the N local PDCs
at every iteration k, referred to as active PDCs. Let this set be
denoted by Sk. It then calculates zk+1 using the most recent
local estimates from all PDCs. Let T k+1 be the time instant
at which zk+1 is computed. The central PDC then broadcasts
(zk+1, T k+1) to every local PDC. Upon receiving T k+1, each
local PDC j then constructs ˆH k+1
from (9) by
setting mk+1 to be the sample index that is closest to the
time instant T k+1. Note that ∆θ(mk+1) may not be the most
recent measurement sample while constructing ˆH k+1
and ˆck+1
matrices. However, in order to ensure that all PDCs use the
same time-window of the measurements to form ˆH k+1
and
ˆck+1
, they all use the same value of mk+1 as decided globally
j
by the central PDC at every iteration k + 1. The A-ADMM
algorithm adapted for (7) can then be written as follows:
and ˆck+1
j
j
j
j
j
Distributed Prony using Asynchronous ADMM
ak+1
j
.
(12)
1) The central PDC initializes T 0 and sends it to all local
PDCs.
2) PDC j initializes a0
3) At iteration k:
j, z0, and w0
j , for j = 1, . . . , N.
e) Central PDC broadcasts zk+1 to all local PDCs.
f) PDC j updates wj as:
j = wk
j − zk+1).
j + ρ(ak+1
wk+1
(13)
j . Once z∗ = a∗
Since (7) is a convex optimization problem, as k → ∞,
zk in (12) converges to the global minimum of (7), and so
does each individual ak
j due to consensus. Let these optimal
1 = ··· = a∗
values be denoted as z∗ and a∗
N
is calculated, every local PDC can compute the eigenvalues of
A using Step 2 of the Prony algorithm decribed in Section III.
It can also compute the mode residues using Step 3.
Note: The conventional ADMM algorithm for consensus
problems converges at the rate of O( 1
k ), as shown in [11].
However, unlike [11], where the objective function to be
minimized is assumed to be time-invariant, equation (11) in
our proposed S-ADMM varies over time. If we hold ˆH k
j and
j to be constant at ˆH 0
j, respectively, for all j and k,
ˆck
then the O( 1
k ) convergence rate can be guaranteed.
j and ˆc0
B. Architecture 1 with Asynchronous Communication
One assumption behind Architecture 1 is that all local
PDCs are performing their respective optimization steps with
equal speed, and the communication delays between the local
PDCs and the central PDC are also equal, i.e., they are
synchronous. However, in reality, the PDCs may not be
perfectly synchronized with each other due to differences in
a) Given T k, PDC j constructs ˆH k
j and ˆck
j from (9)
using mk decided from T k.
b) PDC j updates aj as:
j = (( ˆH k
ak+1
j )T ˆH k
j + ρI2n)−1(( ˆH k
j )T ˆck
j − wk
j + ρzk).
c) PDC j transmits ak+1
d) The central PDC receives the values of ak+1
j , and k to the central PDC.
, wk
j ,
and k only from the active PDCs j ∈ S k.
, wk
j
j
e) The central PDC updates z as:
N(cid:88)
j=1
zk+1 =
1
N
(ak+1
j + (1/ρ)wk
j ),
where ak+1
j = ak
j , and wk
j = wk−1
j
for j /∈ S k.
f) The central PDC broadcasts zk+1, k + 1, and T k+1
to all local PDCs.
g) PDC j updates wj as:
wk+1
wk+1
j = wk
j + ρ(ak+1
j /∈ S k.
j = wk
j ,
j − zk+1),
j ∈ S k,
Note that the iteration numbers k and k+1 are communicated
between the PDCs in steps 3c and 3f to keep track of the
order of the receiving data. This architecture is similar to
Architecture 1 shown in Fig. 2. The only difference is that
more information is exchanged between the central and local
PDCs. The aforementioned A-ADMM algorithm converges to
the minimizer of (7) with the rate of O( 1
k ) if none of the local
PDCs is allowed to be dormant all throughout. In other words,
for all S k ∈ P, where P (cid:44) 2{1,...,N} is the set of all subsets