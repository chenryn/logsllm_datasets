title:DP-Finder: Finding Differential Privacy Violations by Sampling and
Optimization
author:Benjamin Bichsel and
Timon Gehr and
Dana Drachsler-Cohen and
Petar Tsankov and
Martin T. Vechev
DP-Finder: Finding Differential Privacy Violations
by Sampling and Optimization
Benjamin Bichsel
ETH Zurich, Switzerland
PI:EMAIL
Timon Gehr
ETH Zurich, Switzerland
PI:EMAIL
Dana Drachsler-Cohen
ETH Zurich, Switzerland
PI:EMAIL
Petar Tsankov
ETH Zurich, Switzerland
PI:EMAIL
Martin Vechev
ETH Zurich, Switzerland
PI:EMAIL
ABSTRACT
We present DP-Finder, a novel approach and system that automat-
ically derives lower bounds on the differential privacy enforced by
algorithms. Lower bounds are practically useful as they can show
tightness of existing upper bounds or even identify incorrect upper
bounds. Computing a lower bound involves searching for a coun-
terexample, defined by two neighboring inputs and a set of outputs,
that identifies a large privacy violation. This is an inherently hard
problem as finding such a counterexample involves inspecting a
large (usually infinite) and sparse search space.
To address this challenge, DP-Finder relies on two key insights.
First, we introduce an effective and precise correlated sampling
method to estimate the privacy violation of a counterexample. Sec-
ond, we show how to obtain a differentiable version of the problem,
enabling us to phrase the search task as an optimization objective
to be maximized with state-of-the-art numerical optimizers. This
allows us to systematically search for large privacy violations.
Our experimental results indicate that DP-Finder is effective
in computing differential privacy lower bounds for a number of
randomized algorithms. For instance, it finds tight lower bounds in
algorithms that obfuscate their input in a non-trivial fashion.
CCS CONCEPTS
• Security and privacy; • Mathematics of computing → Prob-
ability and statistics;
KEYWORDS
Differential privacy; Lower bounds; Sampling; Optimization
ACM Reference Format:
Benjamin Bichsel, Timon Gehr, Dana Drachsler-Cohen, Petar Tsankov,
and Martin Vechev. 2018. DP-Finder: Finding Differential Privacy Violations,
by Sampling and Optimization. In 2018 ACM SIGSAC Conference on Com-
puter and Communications Security (CCS ’18), October 15–19, 2018, Toronto,
ON, Canada. ACM, New York, NY, USA, 17 pages. https://doi.org/10.1145/
3243734.3243863
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS ’18, October 15–19, 2018, Toronto, ON, Canada
© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-5693-0/18/10...$15.00
https://doi.org/10.1145/3243734.3243863
1 INTRODUCTION
Differential privacy (DP) [12] has emerged as an important property
that measures the amount of leaked information by (randomized)
algorithms [2, 7, 14, 21, 22, 24, 27, 29, 31]. Informally, a randomized
algorithm is ϵ-differentially private (denoted ϵ-DP) if the distance
between the output distributions it produces, for any two neighbor-
ing inputs, is bounded by ϵ. The standard way to enforce ϵ-DP is to
add noise to computations, where the amount of noise is determined
from the bound ϵ that the user would like to enforce. Determining
the exact bound ϵ enforced by a randomized algorithm is important
for two reasons. First, a conservative upper bound requires users to
add more noise than necessary. Second, an incorrect bound (lower
than the one actually enforced) may result in adding an insufficient
amount of noise, thereby leading to actual privacy violations.
To address these issues, ideally one would derive exact privacy
bounds enforced by randomized algorithms. Unfortunately, prov-
ing exact privacy bounds is a challenging task. Thus, in practice,
existing approaches are limited to deriving upper bounds on the
differential privacy enforced by algorithms [3, 4, 6, 15, 27, 32, 35].
These upper bounds are sometimes conservative, and have led to
follow-up works that improved them (e.g., [17]). Moreover, in some
cases, these upper bounds were incorrect, resulting in algorithms
that did not enforce ϵ-DP. For example, [26] showed that a suppos-
edly differentially private variants of the Sparse Vector Technique
were actually not differentially private.
Finding Lower Bounds.
In this work, we study the task of com-
puting lower bounds on the ϵ-DP of randomized algorithms. That is,
our goal is to find the largest ϵ for which a randomized algorithm is
not ϵ-DP. Finding such lower bounds is practically useful and, com-
bined with previous results on finding upper bounds, can be used to
determine exactness or, alternatively, suggest that the upper bound
can be improved. Moreover, discovering lower bounds provides
an effective tool for testing the correctness of established upper
bounds, by searching for lower bounds that may exceed them.
Challenges. The task of finding DP violations introduces two
challenges. First, it requires efficiently estimating the violation
induced by particular inputs and a set of outputs. This involves
reasoning about probabilities which are difficult to compute an-
alytically. Second, it requires finding inputs and a set of outputs
that induce large privacy violations. This search involves solving
a complex, non-differentiable maximization problem in a search
space where few inputs and set of outputs induce large privacy
estimate
ϵ by ˆϵ
make ˆϵ
differentiable
compute
argmax
ˆϵ d
x
Φ
x′
Counter-
example
Privacy violation
ϵ (x, x′, Φ)
Estimated violation
ˆϵ (x, x′, Φ)
Differentiable violation
ˆϵ d (x, x′, Φ)
ˆϵ (x, x′, Φ)
or
ϵ (x, x′, Φ)
Figure 1: A conceptual overview of our approach. Our goal is to search for the maximal privacy violation ϵ (x, x′, Φ), a function
which is hard to compute in general, and non-differentiable. DP-Finder first estimates ϵ (x, x′, Φ) with correlated sampling,
resulting in ˆϵ (x, x′, Φ), which estimates ϵ (x, x′, Φ) well but is also non-differentiable. Then, DP-Finder transforms ˆϵ (x, x′, Φ) to
a differentiable function ˆϵd (x, x′, Φ). This is then passed to a numerical optimizer to find a maximal counterexample, with
respect to ˆϵd (x, x′, Φ). When a counterexample is returned from the optimizer, DP-Finder computes the estimated violation
ˆϵ (x, x′, Φ), with correlated sampling, or computes the exact violation ϵ (x, x′, Φ), with symbolic solvers.
violations. In this work, we address both challenges and present an
effective approach for discovering ϵ-DP violations.
Main Insights. Our approach to finding privacy violations relies
on two main insights. First, we can estimate ϵ using correlated
sampling with relatively few samples that still provide a good esti-
mate for ϵ. To determine the quality of the estimate, we develop a
heuristic inspired by the central limit theorem (CLT) and leveraging
Hinkley’s theorem on the ratio of Gaussian random variables [19].
Second, to search through the sparse space of privacy violations, we
define a differentiable surrogate function that allows us to leverage
numerical optimization methods.
DP-Finder. Based on the above insights, we present a system,
called DP-Finder. The goal of DP-Finder is to find a triple (x, x′, Φ)
witnessing the largest possible privacy violation. The values x and
x′ are inputs to the randomized algorithm, while Φ is a possible set
of outputs. Fig. 1 illustrates the high-level approach of DP-Finder.
The leftmost plot illustrates the search space of DP-Finder: it shows
the privacy violation ϵ as a function of x, x′, and Φ. Since it is com-
putationally prohibitive to directly maximize ϵ (x, x′, Φ), DP-Finder
estimates ϵ (x, x′, Φ) with correlated sampling, denoted ˆϵ (x, x′, Φ).
To ensure that the estimate is precise, DP-Finder determines the
number of required samples to achieve a given target precision.
In general, ˆϵ (x, x′, Φ) is non-differentiable (illustrated by the dis-
continuities in its graph). Thus, in the next step, DP-Finder makes
ˆϵ (x, x′, Φ) differentiable through a set of rewrite rules, resulting
in a differentiable violation estimate, denoted ˆϵd (x, x′, Φ). Then,
DP-Finder uses an off-the-shelf numerical optimizer to find a triple
(x, x′, Φ) with a high privacy violation, with respect to ˆϵd (x, x′, Φ).
Finally, DP-Finder computes the true privacy violation ϵ (x, x′, Φ)
for the final triple (x, x′, Φ) using exact symbolic probabilistic
solvers (e.g., PSI [16]), if they succeed (PSI succeeded in all of our
experiments), otherwise we output ˆϵ (x, x′, Φ).
We implemented a prototype of DP-Finder and evaluated it on
a number of randomized algorithms. To the best of our knowledge,
DP-Finder is the first system capable of automatically estimating
differential privacy lower bounds using a general method, appli-
cable to a wide range of algorithms. Our results demonstrate that,
often, the lower bounds discovered by DP-Finder are close to the
known upper bounds (implying tightness). For example, we show
that the noisyMax algorithm, which was proven to satisfy 10%-
DP [26], is not 9.9%-DP. This implies that we can characterize the
exact ϵ as lying in the tight interval [9.9%, 10%]. In few cases, we
compute a lower bound that is further from the respective upper
bound. For example, for the AboveThreshold algorithm, which is
known to be (at least) 45%-DP [26], for arrays of size 4, we were
only able to find 17.3%-DP violations. This suggests that the known
upper bound can potentially be improved or, alternatively, further
research is needed to discover better lower bounds.
Main Contributions. To summarize, our main contributions are:
• An approach that estimates privacy violations through cor-
related sampling, along with a confidence interval (Sec. 4).
• A transformation which translates the non-differentiable es-
timation into a differentiable one, enabling use of numerical
optimizers for finding privacy violations of ϵ-DP (Sec. 5).
• An implementation1 and evaluation on a number of random-
ized algorithms, showing that our approach is effective in
discovering useful privacy violations of ϵ (Sec. 6).
2 PROBLEM STATEMENT
We address the problem of finding a counterexample to ϵ-DP, for
large ϵ. In the following, we introduce background terms that help
precisely define the notion of ϵ-DP counterexamples.
Randomized Algorithms. We consider randomized algorithms
that obscure their inputs by adding noise to computation steps.
The noise is a random variable with a given distribution, e.g., the
Laplace distribution. For example, consider the above threshold
randomized algorithm AT (Fig. 2), simplified from Lyu et al. [26]. It
is parameterized by a threshold T, and, for an array x, it returns a
th entry indicates whether x[i] exceeds T.
Boolean array y whose i
To reduce information leakage of x, it adds noise to the threshold
1The implementation is available at https://github.com/eth-sri/dp-finder
x,x0,Φviolationx,x0,Φviolationx,x0,Φviolationdef AT(x):
ρ = Lap(20)
for i = 1 to k:
ν [i] = Lap(20)
if x[i]+ν [i]≥T+ρ:
y[i] = 1
else
y[i] = 0
return y
def checkAT,{[0,0]}(x):
y = AT(x)
c = [0, 0]
ret = 1
for i = 1 to 2:
ret = ret &&
y[i] == c[i]
return ret
Figure 2: An instance of the
above threshold randomized
algorithm.
Figure 3: Attacker’s check
on AT and {[0, 0]} consists
of running AT and check-
ing inclusion in {[0, 0]}.
and the entries of x. The noise terms are drawn from the Laplace
distribution with scale 20. We note that in the original randomized
algorithm, the noise terms are drawn from a Laplace distribution
which is determined by the target upper bound ϵ (in particular, the
distribution scale is 2/ϵ). To avoid confusion with this ϵ and a lower
bound on the optimal ϵ (which is what DP-Finder is searching for),
we instantiate the target upper bound with ϵ = 0.1, resulting in a
distribution scale of 20.
Differential Privacy (DP). A (randomized) algorithm F : X → Y
is ϵ-differentially private (ϵ-DP) if for every pair of neighboring in-
puts x, x′ ∈ X, and for every (measurable) set Φ ⊆ Y, the probabil-
ities of events F (x ) ∈ Φ and F (x′) ∈ Φ are closer than a factor of
exp(ϵ ):
Pr [F (x ) ∈ Φ] ≤ exp(ϵ ) Pr(cid:2)F (x
) ∈ Φ(cid:3) .
′
In this work, we focus on algorithms over the real vectors, i.e., X =
Rk, for some k; however, our results extend to other domains,
e.g., matrices of real numbers. Additionally, we assume Y = Rl
or Y = Dl , for some l and a finite set D. For example, in AT,
Y = {0, 1}k, i.e., D = {0, 1} and l = k, implying that the output is
a Boolean array with the same size as the input array. If Y = Dl
(i.e., Y is discrete), it suffices to only consider individual outputs
(y ∈ Y), which can be captured in our setting through singleton
sets Φ = {y}.
We next define the concept of a neighborhood. This concept is
inspired from databases, in which x and x′ are viewed as databases,
and they are neighbors if they differ only in a single user’s data.
Then, if differential privacy holds, the output distribution of F is
almost the same for x and x′, i.e., adding the differentiating user’s
data does not affect F’s output significantly. Formally, a neighbor-
hood is captured by a binary relation Neigh ⊆ X × X, i.e., inputs x
and x′ are neighbors if (x, x′) ∈ Neigh. A possible instantiation of
Neigh is the set of all array pairs whose entries differ by at most 1,
denoted Neigh≤1:
′
) ∈ Neigh≤1 ⇔ ∀i ∈ {0, ..., k − 1}. |xi − x
′
i | ≤ 1.
(x, x
For example, ([7.4, 4.7], [8.1, 4.6]) ∈ Neigh≤1. This is useful when
a database contains aggregate data on its users, such as counts of
how many users suffer from certain diseases. Then, adding the data
of a single user can affect each count by at most one.
Attacker’s Check. The set Φ ⊆ Y can be interpreted as a check
on the algorithm’s output that is performed by the attacker to gain
information. For example, the meaning of Φ = {[0, 0]} is that the
attacker tries to guess which of two possible inputs was used to
produce an observed output of AT, by checking if the output is equal
to [0, 0]. If the probability of outputting [0, 0] differs substantially for
two inputs, this allows the attacker to learn which of the two inputs
was likely provided as input. Formally, the randomized program that
checks whether the output of a randomized algorithm F : X → Y
lies in Φ is denoted by checkF,Φ : X → {0, 1} and is defined as:
checkF,Φ (x ) = [F (x ) ∈ Φ],
where [·] denotes the Iverson brackets, returning 1 if F (x ) ∈ Φ, and 0
otherwise. Technically, checkF,Φ (x ) runs F on the input x and then