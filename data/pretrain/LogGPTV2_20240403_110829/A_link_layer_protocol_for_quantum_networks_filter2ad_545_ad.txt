memory 1.
The higher layer may instruct the MHP to perform a gate
on the communication qubit depending on the heralding
signal from H allowing the conversion from the |Ψ−⟩ state
to the |Ψ+⟩ state. Entanglement generation is then triggered
at the start of the next time interval, including generation
parameter α, and a GEN message is sent to H which includes
1Less abstractly, by specifying microwave and laser pulse sequences con-
trolling the chip (see Appendix).
9
Dahlberg, Skrzypczyk, et al.
5.2.1 Distributed Queue. Both nodes that wish to estab-
lish entangled link(s) must trigger their MHP devices in a
coordinated fashion (Section 4.4). To achieve such agree-
ment, the EGP employs a distributed queue comprised of
synchronized local queues at the controllable nodes. These
local queues can be used to separate requests based on pri-
ority, where here we employ 3 for the different use cases
(CK, NL, MD). Due to low errors in classical communica-
tion (estimated < 4 × 10−8 on QL2020, see Appendix D.6.1)),
we let one node hold the master copy of the queue, and
use a simple two-way handshake for enqueing items, and
a windowing mechanism to ensure fairness. Queue items
include a min_time that specifies the earliest possible time a
request is deemed ready for processing by both nodes (de-
pending on their distance). Specifying min_time prevents
either node from beginning entanglement generation in dif-
ferent timesteps.
5.2.2 Quantum Memory Management (QMM). The EGP
uses the node’s QMM (Section 4.5) to determine which phys-
ical qubits to use for generating or storing entanglement.
5.2.3
Fidelity estimation unit (FEU). In order to provide
information about the quality of entanglement, the EGP
employs a fidelity estimation unit. This unit is given a desired
quality parameter Fmin, and returns generation parameters
(such as α) along with an estimated minimal completion time.
Such a fidelity estimate can be calculated based on known
hardware capabilities such as the quality of the memory and
operations. To further improve this base estimate the EGP
intersperses test rounds.
5.2.4
Scheduler. The EGP scheduler decides which re-
quest in the queue should be served next. In principle, any
scheduling strategy is suitable as long as it is determinis-
tic, ensuring that both nodes select the same request locally.
This limits two-way communication, which adversely affects
entanglement quality due to limited memory lifetimes.
5.2.5 Protocol. Figure 5 presents an architecture diagram
visualizing the operation. The protocol begins when a higher
layer at a controllable node issues a CREATE operation to the
EGP specifying a desired number of entangled pairs along
with Fmin and tmax. Upon receipt of a request the EGP will
query the FEU to obtain hardware parameters (α), and a
minimum completion time. If this time is larger than tmax,
the EGP immediately rejects the request (UNSUPP). Should
the request pass this evaluation, the local node will compute
a fitting min_time specifying the earliest MHP polling cycle
the request may begin processing. The node then adds the
request into the distributed queue shared by the nodes. This
request may be rejected by the peer should the remote node
have queue rules that do not accept the specified purpose ID.
Then, the EGP locally rejects the request (DENIED).
Figure 4: Timeline of the MHP polling higher layers
to see if entanglement should be produced.
a timestamp, and the given ID. The motivation for including
the ID is to protect against errors in the classical control, for
example losses.
The station H uses the timestamp to link the message to a
detection window in which the accompanying photons ar-
rived. Should messages from both nodes arrive, the midpoint
verifies that the IDs transmitted with the GEN messages
match, and checks the detection counts (Figure 3) from the
corresponding detection window. The midpoint will then
send a REPLY message indicating success or failure, and in
the case of success which of the two states |Ψ+⟩ and |Ψ−⟩
was produced. The REPLY additionally contains a sequence
number uniquely identifying the generated pair of entangled
qubits chosen by H, which later enables the EGP to assign
unique entanglement identifiers. This REPLY and the ID is
forwarded to the link layer for post-processing. Note that
the REPLY may be received many MHP cycles later, allowing
the potential for emission multiplexing (Section 5.2).
5.1.2 Protocol for Create and Measure (M). Handling M
type requests is very similar, differing only in two ways:
Instead of performing a gate on the communication qubit, the
“yes“ message requests the MHP to perform a measurement
on the communication qubit in a specified basis once the
photon has been emitted, even before receiving the response
from H . The outcome of the measurement and the REPLY are
passed back to the EGP. In practice, the communication time
from transmitting a GEN message to receiving a REPLY may
currently exceed the duration of such a local measurement
(3.7 µs vs. communication delay Lab 9.7 ns, and QL2020 145
µs), and the MHP may choose to perform the measurement
only after a successful response is received.
5.2 Link Layer EGP
Here we present an implementation of a link layer protocol,
dubbed EGP, satisfying the service requirements put forth in
Section 4 (see Appendix E for details and message formats).
We build up this protocol from different components:
10
EGP AMHP AStation Htrigger?y/n, infoGENREPLYRESULTA Link Layer Protocol for Quantum Networks
order to increase the throughput for the MD use case. Errors
such as losses on the classical control link can lead to an
inconsistency of state (of the distributed queue) at A and
B from which we need to recover. Inconsistencies can also
affect the higher layer. Since the probability of e.g. losses is
extremely low, we choose not to perform additional two way
discussion to further curb all inconsistencies at the link layer.
Instead, the EGP can issue an EXPIRE message for an OK
already issued if inconsistency is detected later, e.g. when
the remote node never received an OK for this pair.
Figure 5: Flow diagram of the MHP and EGP opera-
tion. The EGP handles CREATE requests and sched-
ules entanglement generation attempts are issued to
the MHP. Replies from the midpoint are parsed and
forwarded to the EGP from request management.
The local scheduler selects the next request to be pro-
cessed, given that there exists a ready one (as indicated by
min_time). The QMM is then used to allocate qubits needed
to fulfill the specified request type (create and keep K or
create and measure M). The EGP will then again ask the FEU
to obtain a current parameter α due to possible fluctuations
in hardware parameters during the time spent in the queue.
The scheduler then constructs a “yes” response to the MHP
containing α from the FEU, along with an ID containing
the unique queue ID of the request in the distributed queue,
and number of pairs already produced for the request. This
response is then forwarded to the local MHP upon its next
poll to the EGP. If no request is ready for processing, a “no”
response is returned to the MHP . At this point the MHP
behaves as described in the previous section and an attempt
at generating entanglement is made.
Whenever a REPLY and ID is received from the MHP,
the EGP uses the ID to match the REPLY to an outstanding
request, and evaluates the REPLY for correctness. Should the
attempt be successful, the number of pairs remaining to be
generated for the request is decremented and an OK message
is propagated to higher layers containing the information
specified in Section 4.1.2, where the Goodness is obtained
from the FEU.
In the Appendix, we consider a number of examples to
illustrate decisions and possible pitfalls in the EGP. An ex-
ample is the possibility of emission multiplexing [98]: The
EGP can be polled by the MHP before receiving a response
from the MHP for the previous cycle. This allows the choice
to attempt entanglement generation multiple times in suc-
cession before receiving a reply from the midpoint, e.g., in
6 EVALUATION
We investigate the performance of our link layer protocol
using a purpose built discrete event simulator for quantum
networks (NetSquid [1], Python/C++) based on DynAA [41]
(see Appendix C for details and more simulation results).
Both the MHP and EGP are implemented in full in Python
running on simulated nodes that have simulated versions
of the quantum hardware components, fiber connections,
etc. All simulations were performed on the supercomputer
Cartesius at SURFsara [2], in a total of 2578 separate runs
using a total of 94244 core hours, and 707 hours time in the
simulation (∼250 billion MHP cycles).
We conduct the following simulation runs:
• Long runs: To study robustness of our protocol, we
simulate the 169 scenarios described below for an ex-
tended period of time. Each scenario was simulated
twice for 120 wall time hours, simulating 502 − 13437
seconds. We present and analyze the data from these
runs in sections 6.1, 6.2 and C.2.
• Short runs: We perform the following simulations for
a shorter period of time (24 wall time hours, reaching
67 − 2356 simulated seconds):
– Performance trade-offs: To study the trade-off be-
tween latency, throughput and fidelity we sweep
the incoming request frequency and the requested
minimum fidelity, see Figure 6.
– Metric fluctuations: To be able to study the impact of
different scheduling strategies on the performance
metrics, we run 4 scenarios, 102 times each. The
outcomes of theses simulation runs are discussed in
section 6.3.
To explore the performance at both short and long dis-
tances, the underlying hardware model is based on the Lab
and QL2020 scenarios, where we validate the physical mod-
eling of the simulation against data collected from the quan-
tum hardware system of the Lab scenario already realized
(Figure 8). For the quantum reader we note that while our
simulations can also be used to predict behavior of physical
implementations (such as QL2020), the focus here is on the
performance and behavior of the link layer protocol.
11
MHPCREATEHigher LayerOK EGPSchedulerFidelity EstimationQuantum MMUQueuetrigger?yes/noinfo GENREPLYRESULTOK OK LinkLayerPhysicalLayerentanglementWe structure the evaluation along the three different use
cases (NL, CK, MD), leading to a total of 169 different sim-
ulation scenarios. First, we use different kinds of requests:
(1) NL (K type request, consecutive flag, priority 1 (highest),
store qubit in memory qubit), (2) CK, an application asking
for one or more long-lived pairs (K type request, immediate
return flag, priority 2 (high), store qubit in memory qubit)
and (3) (MD) measuring directly (M type request, consecutive
flag, priority 3 (lowest)). For an application such as QKD, one
would not set the immediate return flag in practice for effi-
ciency, but we do so here to ease comparison to the other two
scenarios. Measurements in MD are performed in randomly
chosen bases X, Z and Y (see Appendix A).
In each MHP cycle, we randomly issue a new CREATE
request for a random number of pairs k (max kmax), and
random kind P ∈ {NL, CK, MD} with probability fP·psucc/(E·
k), where psucc is the probability of one attempt succeeding
(Section 4.4), fP is a fraction determining the load in our
system of kind P, and E is the expected number of MHP cycles
to make one attempt (E = 1 for MD and E ≈ 1.1 for NL/CK
in Lab due to memory re-initialization and post-processing).
E ≈ 16 for NL/CK in QL2020 due to classical communication
delays with H (145µs)). In the long runs, we first study single
kinds of requests (only one of MD/CK/NL), with fP = 0.7
(Low), 0.99 (High) or 1.5 (Ultra). For the long runs, we fix
one target fidelity Fmin = 0.64 to ease comparison. For each
of the 3 kinds (MD/CK/NL), we examine (1) kmax = 1, (2)
kmax = 3, and (3) only for MD, kmax = 255. For Ultra the
number of outstanding requests intentionally grows until the
queue is full (max 256), to study an overload of our system.
To study fairness, we take 3 cases of CREATE origin for each
single kind (MD/CK/NL) scenario: (1) all from A (master of
the distributed queue), (2) all from B, (3) A or B are randomly
chosen with equal probability. To examine scheduling, we
additionally consider long runs with mixed kinds of requests
(Appendix, e.g. Figure 7).
6.1 Robustness
To study robustness, we artificially increase the probability
of losing classical control messages (100 Base T on QL2020
fiber < 4 × 10−8, Appendix D.6.1), which can lead to an in-
consistency of state of the EGP but also at higher layers
(Section 5.2). We ramp up loss probabilities up to 10−4 (Ap-
pendix D.6.1) and observe our recovery mechanisms work
to ensure stable execution in all cases (35 runs, 281 - 3973 s
simulated time), with only small impact to the performance
parameters (maximum relative differences 2 to the case of
no losses, fidelity (0.005), throughput (0.027), latency (0.629),
number of OKs (0.026) with no EXPIRE messages). We see a
relatively large difference for latency, which may however
2Relative difference between m1 and m2 is |m1 − m2|/max(|m1|, |m2|)
12
Dahlberg, Skrzypczyk, et al.
Figure 6: Performance trade-offs. (a) Scaled latency vs.
fP determining fraction of throughput (b) Scaled la-
tency vs. fidelity Fmin. Demanding a higher Fmin low-
ers the probability of an attempt being successful (Sec-
tion 4.4), meaning (c) throughput directly scales with
Fmin (each point averaged over 40 short runs each 24 h,
93 − 2355 s simulated time, QL2020, kmax = 3, for (b,c)
fP = 0.99). Higher Fmin not satisfiable for NL in (b).
be due to latency not reaching steady state in the course of
this simulation (70 × 70 core hours).
6.2 Performance Metrics
We first consider runs including only a single kind of request
(MD/CK/NL). In addition to the long runs, we conduct spe-
cific short runs examining the trade-off between latency and
throughput for fixed target fidelity Fmin (Figure 6(a)), and
the trade-off between latency (throughput) and the target
fidelity in Figure 6(b) (Figure 6(c)).
Below we present the metrics extracted from the long runs
with single kinds of requests:
Fidelity: As a benchmark, we began by recording the av-
erage fidelity Favg in all 169 scenarios with fixed minimum
fidelity. We observe that Favg is independent of the other
metrics but does depend on the distance, and whether we
store or measure: 0.745 < Favg < 0.757 NL/CK Lab, 0.626 <
Favg < 0.653 NL/CK QL2020, 0.709 < Favg < 0.779 MD Lab,
0.723 < Favg < 0.767 MD QL2020 (Fidelity MD extracted
from QBER measurements, Appendix A). This is to be ex-
pected since (1) we fix one Fmin and (2) we consider an NV
platform with only 1 available memory qubit (Lab).
Throughput: All scenarios High and Ultra in Lab reach
an average throughput thavg (1/s) of 6.05 < thavg < 6.47
NL/CK and 6.51 < thavg < 7.09 for MD. It is expected that
A Link Layer Protocol for Quantum Networks
MD has higher throughput, since no memory qubit needs to
be initialized. The time to move to memory (1040µs) is less
significant since many MHP cycles are needed to produce
one pair, but we only move once. As expected for Low the
throughput is slightly lower in all cases, 4.44 < thavg <
4.72 NL/CK, and 4.86 < thavg < 5.22 MD. For QL2020, the
throughput for NL/CK is about 14 times lower, since we need
to wait (145µs) for a reply from H before MHP can make a
new attempt.
Latency: The scaled latency highly depends on the incom-
ing request frequency as the queue length causes higher
latency. However, from running the same scenarios many
(102) times for a shorter period (24 wall time hours), see
section 6.3, we see that the average scaled latency fluctuates
a lot, with a standard deviation of up to 6.6 s in some cases.
For QL2020 with NL requests specifying 1-3 pairs from both
nodes, we observe an average scaled latency of 10.97 s Low,
142.9 s High and 521.5 s Ultra. For MD requests, 0.544 s
Low, 3.318 s High and 32.34 s Ultra. The longer scaled la-
tency for NL is largely due to longer time to fulfill a request,
and not that the queues are longer (average queue length for
NL: 3.83 Low, 56.3 High, 214 Ultra), and for MD: 3.23 Low,
22.4 High and 219 Ultra).
Fairness: For 103 scenarios of the long runs (single kinds of
requests (MD/CK/NL) randomly from A and B), we see only
slight differences in fidelity, throughput or latency between
requests from A and B. Maximum relative differences do
not exceed: fidelity 0.033, throughput 0.100, latency 0.073,