89
99
100
100
93
98
95
D. Results From Public Setting
After gaining promising results from our relatively small-
scale lab setting, we set out to expand our data collection
process to real-world public setting. We invited people to
voluntarily participate in our study by visiting our web page6
and following a few simple steps to provide us with sensor
6Screenshots of the data collection page is available in Appendix B. We
obtained approval from our Institutional Research Board (IRB) to perform the
data collection.
6
Fig. 4: Distribution of participant device model.
data. We recruited participants through email and online social
networks. We asked participants to provide data under two
settings: no-audio setting and the inaudible sine-wave setting
(we avoid the background song to make the experience less
bothersome for the user). Each setting collected sensor data
for about one minute, requiring a total of two minutes of
participation. On average, we had around 10 samples per
setting per device. Our data-gathering web page plants a cookie
in the form of a large random number (acting as a unique ID)
in the user’s browser, which makes it possible to correlate data
points coming from the same device. Over the course of two
weeks, we received data from a total of 76 devices. However,
some participants did not follow all the steps and as a result
we were able to use only 63 of the 76 submissions. Figure 4
shows the distribution of the different devices that participated
in our study.
Next, we apply our ﬁngerprinting approach on the public
data set. Table VI shows our ﬁndings. Compared to the results
from our lab setting, we see a slight decrease in F-score but
even then we were able to obtain an F-score of 95%. Again,
the beneﬁt of the audio stimulation is not evident from these
results, however, their beneﬁts will become more visible in the
later sections when we discuss countermeasure techniques.
E. Results From Combined Setting
Finally, we combine our lab data with the publicly collected
data to give us a combined dataset containing 93 different
smartphones. We apply the same set of evaluations on this
combined dataset. Table VII highlights our ﬁndings. Again,
 86 88 90 92 94 96 98 100 0 5 10 15 20 25 30Avg. F-score (%)Number of featuresUsing accelerometer data onlyNo-audioSineSong 50 55 60 65 70 75 80 85 90 95 100 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80Avg. F-score (%)Number of featuresUsing gyroscope data onlyNo-audioSineSong 95 96 97 98 99 100 0 10 20 30 40 50 60 70 80 90 100 110Avg. F-score (%)Number of featuresUsing both accelerometer and gyroscope dataNo-audioSineSongSamsung Galaxy S4 iphone 5s Google Nexus 4 Google Nexus 5 Samsung Galaxy S3 iphone 5 iphone 6 iphone 4 HTC One iphone 4s Motorola Moto Samsung Galaxy S5 Google Nexus 6 LG G3 LG L90 Samsung  Galaxy Note 2 Samsung  Galaxy Note 3 Samsung  Galaxy Note 4 Motorola Droid 0%10%20%19.74%11.84%9.21%9.21%6.58%6.58%5.26%5.26%5.26%3.95%3.95%2.63%2.63%1.32%1.32%1.32%1.32%1.32%1.32%TABLE VI: Average F-score under public setting where smartphones
were kept on top of a desk
Stimulation
No-audio
Sine
Accelerometer
86
85
87
87
Avg. F-score (%)
Gyroscope
Accelerometer+Gyroscope
95
92
96
95
we see that combining features from both sensors provides
the best result. In this case we obtained an F-score of 96%.
All these results suggest that smartphones can be successfully
ﬁngerprinted through motion sensors.
TABLE VII: Average F-score under both lab and public setting where
smartphones were kept on top of a desk
Avg. F-score (%)
Gyroscope
Accelerometer+Gyroscope
Stimulation
No-audio
Sine
Accelerometer
85
89
F. Sensitivity Analysis
89
89
1) Varying the Number of Devices: We evaluate the ac-
curacy of our classiﬁer while varying the number of devices.
We pick a subset of n devices in our dataset and perform
the training and testing steps for this subset. For each value
of n, we repeat the experiment 10 times, using a different
random subset of n devices each time. In this experiment we
only consider the use of both accelerometer and gyroscope
features, since those produce the best performance (as evident
from our previous results), and focus on the no-audio and sine
wave background scenarios. Figure 5 shows that the F-score
generally decreases with large number of devices, which is
expected as an increased number of labels makes classiﬁcation
more difﬁcult. But even then scaling from 10 devices to 93
devices the F-score decreases by only 4%. Extrapolating from
the graph, we expect classiﬁcation to remain accurate even for
signiﬁcantly larger datasets.
2) Varying Training Set Size: We also consider how varying
the training set size impacts the ﬁngerprinting accuracy. For
this experiment we vary the ratio of training and testing set
size. For this experiment we only look at data from our lab
setting as some of the devices from our public setting did
not have exactly 10 samples. We also consider the setting
where there is no background audio stimulation and use the
combined features of accelerometer and gyroscope. Figure 6
shows our ﬁndings. While an increased training size improves
classiﬁcation accuracy, even with mere two training samples
(of 5–8 seconds each) we can achieve an F-score of 98%, with
increased training set sizes producing an F-score of over 99%.
3) Varying Temperature: Here we analyze how temperature
impacts the ﬁngerprint of smartphone sensors. For this purpose
we collect sensor data under different temperatures. We took
one set of readings outside our ofﬁce building on September
03, 2015 (with temperatures in the range of 91◦F to 93◦F )
while we took another set of readings on October 9, 2015
(with temperatures in the range of 61◦F to 63◦F ). In both
cases we also took readings inside the ofﬁce where temperature
was set to around 74◦F on the thermostat. As these set of
experiments were conducted at a later time compared to our
other experiments, we were only able to collect data from
Fig. 5: Average F-score for different numbers of smartphones. F-score
generally tends to decrease slightly as more devices are considered.
Fig. 6: Average F-score for different ratio of training and testing data.
With only two training data we achieved an F-score of 98%.
17 smartphones (as described in Table VIII)7. Therefore, the
following results for this section are described in the context
of only the smartphones speciﬁed in Table VIII.
Table IX summarizes our ﬁndings. We refer to September
03, 2015 as a hot day and October 09, 2015 as a cold day.
From Table IX we see that temperatures do lower F-score
where warmer temperatures cause more discrepancies in the
generated ﬁngerprints compared to colder temperatures (as
indicated by the red and blue blocks in the table).
7We only had access to these 17 smartphones at that time.
7
 95 96 97 98 99 100 5 10 15 20 25 30Avg. F-score (%)Number of devicesLab settingNo-audioSine 86 88 90 92 94 96 98 100 5 10 15 20 25 30 35 40 45 50 55 60 65Avg. F-score (%)Number of devicesPublic settingNo-audioSine 86 88 90 92 94 96 98 100 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95Avg. F-score (%)Number of devicesCombined settingNo-audioSine 95 96 97 98 99 1002:83:74:65:56:47:38:2Avg. F-score (%)Training Set Size : Test Set SizeUsing both accelerometer and gyroscope dataNo-audioTABLE VIII: Types of phones used for analyzing temperature effect
Maker
Apple
Samsung
Model
iPhone 5
iPhone 5s
Nexus S
Galaxy S3
Galaxy S4
Total
Quantity
4
3
3
2
5
17
TABLE IX: Impact of temperature on sensor ﬁngerprinting
Inside (hot) Outside (hot)
Inside (cold) Outside (cold)
Test (Avg. F-score in %)
No-audio
Inside (hot)
Outside (hot)
Inside (cold)
Outside (cold)
Sine wave
Inside (hot)
Outside (hot)
Inside (cold)
Outside(cold)
Train
Train
100a
90
89
86
89
100a
77
82
90
81
100a
99
92
75
97
100a
Inside (hot) Outside (hot)
Inside (cold) Outside (cold)
Test (Avg. F-score in %)
100a
83
88
85
80
99a
72
69
92
82
100a
92
91
72
90
100a
a50% of the data set was used for training and remaining 50% for testing
4) Temporal Stability: We now take a closer look at how
the ﬁngerprints evolve over time. For this purpose we reuse
data collected from the previous section (Section V-F3). As
we collected data inside our lab in two different dates (one on
September 03, 2015 and the other on October 09, 2015) we
can analyze how sensor ﬁngerprints change over time and how
they impact our F-score. Table X summarizes our ﬁndings. We
see that over time ﬁngerprints do change to some extent, but
even then we can achieve an F-score of approximately 90%.
TABLE X: Fingerprinting sensors at different dates
Test (Avg. F-score in %)
No-audio
Sept. 03, 2015 Oct. 09,2015
Train
Sept. 03, 2015
Oct. 09,2015
Sine wave
Train
Sept. 03, 2015
Oct. 09,2015
100a
89
90
100a
Test (Avg. F-score in %)
Sept. 03, 2015 Oct. 09,2015
100a
88
92
100a
a50% of the data set was used for training and remaining 50% for testing
VI. COUNTERMEASURES
So far we have focused on showing how easy it is to ﬁnger-
print smartphones through motion sensors. We now shift our
focus on providing a systematic approach to defending against
such ﬁngerprinting techniques. We propose two approaches:
sensor calibration and data obfuscation.
A. Calibration
Bojinov et al. [3] observe that their phones have calibration
errors, and use these calibration differences as a mechanism to
distinguish between them. In particular, they consider an afﬁne
error model: aM = g · a + o, where a is the true acceleration
along an axis and aM is the measured value of the sensor. The
two error parameters are the offset o (bias away from 0) and the
gain g which magniﬁes or diminishes the acceleration value.
Our classiﬁcation uses many features, but we ﬁnd that the
8
mean signal value is the most discriminating feature for each
of the sensor streams, which is closely related to the offset.
We therefore explore whether calibrating the sensors will make
them more difﬁcult to ﬁngerprint. We note that calibration has
a side effect of improving the accuracy of sensor readings and
is therefore of independent value. We perform the calibration
only on the sensors in our 30 lab smartphones because we
felt that calibration is too time consuming for the volunteers8.
Moreover, we could better control the quality of the calibration
process when carried out in the lab.
First, let us brieﬂy describe the sensor coordinate system
where the sensor framework uses a standard 3-axis coordinate
system to express data values. For most sensors, the coordinate
system is deﬁned relative to the device’s screen when the
device is held in its default orientation (shown in ﬁgure 7).
When the device is held in its default orientation, the positive
x-axis is horizontal and points to the right, the positive y-axis
is vertical and points up, and the positive z-axis points toward
the outside of the screen face9. We compute offset and gain
error in all three axes.
Calibrating the Accelerometer: Considering both offset and
gain error, the measured output of the accelerometer (aM =
[aM
z ]) can be expressed as:
y , aM
x , aM
 aM
x
aM
y
aM
z
 =
(cid:34) Ox
(cid:35)
(cid:34)Sx
Oy
Oz
+
0
0
(cid:35)(cid:34) ax
(cid:35)
ay
az
0
Sy
0
0
0
Sz
(7)
where S = [Sx, Sy, Sz] and O = [Ox, Oy, Oz] respectively
represents the gain and offset errors along all three axes (a =
[ax, ay, az] refers to the actual acceleration). In the ideal world
[Sx, Sy, Sz] = [1, 1, 1] and [Ox, Oy, Oz] = [0, 0, 0], but in re-
ality they differ from the desired values. To compute the offset
and gain error of an axis, we need data along both the positive
and negative direction of that axis (one measures positive +g
while the other measures negative −g). In other words, six
different static positions are used where in each position one of
the axes is aligned either along or opposite to earth’s gravity.
This causes the a = [ax, ay, az] vector to take one of the
following six possible values {[±g, 0, 0], [0,±g, 0], [0, 0,±g]}.
z− are two values of accelerometer
For example, if aM
reading along the positive and negative z-axis, then we can
compute the offset (Oz) and gain (Sz) error using the following
equation:
z+ and aM
z+ − aM
aM
z−
2g
Sz =
,
Oz =
z+ + aM
aM
z−
2
(8)
all
six
take
along
10 measurements
We
directions
(±x,±y,±z) from all our lab devices as shown in Figure 7.
From these measurements we compute the average offset
and gain error along all
three axes using equation (8).
Figure 8 shows a scatter-plot of the errors along z − axis
for 30 smartphones (each color code represents a certain
make-and-model). We can see that the devices are scattered
around all over the plot which signiﬁes that different devices
have different amount of offset and gain error. Such unique
distinction makes ﬁngerprinting feasible.
8Requiring around 12 minutes in total for calibrating both the accelerom-
9Android and iOS consider the positive and negative direction along an
eter and gyroscope.
axis differently.
Fig. 7: Calibrating accelerometer along three axes. We collect measurements along all 6 directions (±x,±y,±z).
Fig. 8: Accelerometer offset and gain error from 30 smartphones.
Calibrating the Gyroscope: Calibrating gyroscope is a harder
problem as we need to induce a ﬁxed angular change to
determine the gain error even though the offset error can be
computed while keeping the device stationary10. Similar to
accelerometer we can also represent the measured output of
z ]) using the following
the gyroscope (ωM = [ωM
y , ωM
x , ωM
equation: ωM
x
ωM
y
ωM
z
 =
(cid:34) Ox
(cid:35)
(cid:34)Sx