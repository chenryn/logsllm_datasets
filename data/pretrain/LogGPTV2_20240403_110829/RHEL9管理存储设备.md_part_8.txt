:::
NVMe 设备支持原生多路径功能。当在 NVMe 中配置多路径时，您可以在标准 DM
多路径和原生 NVMe 多路径之间进行选择。
DM 多路径和原生 NVMe 多路径都支持 NVMe 设备的 Asymmetric Namespace
Access(ANA)多路径方案。ANA 识别目标与发起方之间的优化路径并提高性能。
当启用原生 NVMe 多路径时，它会全局地应用于所有 NVMe
设备。它可以提供更高的性能，但不包含 DM 多路径提供的所有功能。例如，原生
NVMe 多路径只支持
`故障切换（failover）`{.literal}和`循环（round-robin）`{.literal}路径选择方法。
默认情况下，Red Hat Enterprise Linux 9 中启用了 NVMe
多路径，也是推荐的多路径解决方案。
:::
::: section
::: titlepage
# []{#enabling-multipathing-on-nvme-devices_managing-storage-devices.html#proc_enabling-native-nvme-multipathing_enabling-multipathing-on-nvme-devices}启用原生 NVMe 多路径 {.title}
:::
此流程使用原生 NVMe 多路径解决方案在连接的 NVMe 设备中启用多路径。
::: itemizedlist
**先决条件**
-   NVMe 设备连接到您的系统。
    有关通过光纤传输连接 NVMe 的详情请参考 [NVMe over fabric
    设备概述](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/managing_storage_devices/nvme-over-fabrics-using-rdma_managing-storage-devices#overview-of-nvme-over-fabric-devices_nvme-over-fabrics-using-rdma){.link}。
:::
::: orderedlist
**步骤**
1.  检查内核中是否启用了原生 NVMe 多路径：
    ``` screen
    # cat /sys/module/nvme_core/parameters/multipath
    ```
    这个命令显示以下之一：
    ::: variablelist
    [`N`{.literal}]{.term}
    :   禁用原生 NVMe 多路径。
    [`Y`{.literal}]{.term}
    :   启用原生 NVMe 多路径。
    :::
2.  如果禁用原生 NVMe 多路径，使用以下方法之一启用它：
    ::: itemizedlist
    -   使用内核选项：
        ::: orderedlist
        1.  在内核命令行中添加 `nvme_core.multipath=Y`{.literal} 选项：
            ``` screen
            # grubby --update-kernel=ALL --args="nvme_core.multipath=Y"
            ```
        2.  在 64 位 IBM Z 构架中更新引导菜单：
            ``` screen
            # zipl
            ```
        3.  重启系统。
        :::
    -   使用内核模块配置文件：
        ::: orderedlist
        1.  使用以下内容创建 `/etc/modprobe.d/nvme_core.conf`{.literal}
            配置文件：
            ``` screen
            options nvme_core multipath=Y
            ```
        2.  备份 `initramfs`{.literal} 文件系统：
            ``` screen
            # cp /boot/initramfs-$(uname -r).img \
                 /boot/initramfs-$(uname -r).bak.$(date +%m-%d-%H%M%S).img
            ```
        3.  重建 `initramfs`{.literal} 文件系统：
            ``` screen
            # dracut --force --verbose
            ```
        4.  重启系统。
        :::
    :::
3.  可选：在运行的系统中，更改 NVMe 设备中的 I/O
    策略，以便在所有可用路径中分发 I/O：
    ``` screen
    # echo "round-robin" > /sys/class/nvme-subsystem/nvme-subsys0/iopolicy
    ```
4.  可选：使用 `udev`{.literal} 规则永久设置 I/O 策略。使用以下内容创建
    `/etc/udev/rules.d/71-nvme-io-policy.rules`{.literal} 文件：
    ``` screen
    ACTION=="add|change", SUBSYSTEM=="nvme-subsystem", ATTR{iopolicy}="round-robin"
    ```
:::
::: orderedlist
**验证**
1.  检查您的系统是否识别 NVMe 设备：
    ``` screen
    # nvme list
    Node             SN                   Model                                    Namespace Usage                      Format           FW Rev
    ---------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- --------
    /dev/nvme0n1     a34c4f3a0d6f5cec     Linux                                    1         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    /dev/nvme0n2     a34c4f3a0d6f5cec     Linux                                    2         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    ```
2.  列出所有连接的 NVMe 子系统：
    ``` screen
    # nvme list-subsys
    nvme-subsys0 - NQN=testnqn
    \
     +- nvme0 fc traddr=nn-0x20000090fadd597a:pn-0x10000090fadd597a host_traddr=nn-0x20000090fac7e1dd:pn-0x10000090fac7e1dd live
     +- nvme1 fc traddr=nn-0x20000090fadd5979:pn-0x10000090fadd5979 host_traddr=nn-0x20000090fac7e1dd:pn-0x10000090fac7e1dd live
     +- nvme2 fc traddr=nn-0x20000090fadd5979:pn-0x10000090fadd5979 host_traddr=nn-0x20000090fac7e1de:pn-0x10000090fac7e1de live
     +- nvme3 fc traddr=nn-0x20000090fadd597a:pn-0x10000090fadd597a host_traddr=nn-0x20000090fac7e1de:pn-0x10000090fac7e1de live
    ```
    检查活动传输类型。例如，`nvme0 fc`{.literal}
    表示设备通过光纤通道传输连接，`nvme tcp`{.literal} 则表示设备通过
    TCP 连接。
3.  如果您编辑了内核选项，请检查内核命令行中是否启用了原生 NVMe 多路径：
    ``` screen
    # cat /proc/cmdline
    BOOT_IMAGE=[...] nvme_core.multipath=Y
    ```
4.  检查 DM 多路径报告了 NVMe 命名空间为，例如：`nvme0c0c0n1`{.literal}
    到 `nvme0c3n1`{.literal}，而[*不是*]{.emphasis}，例如：
    `nvme0n1`{.literal} 到 `nvme3n1`{.literal} ：
    ``` screen
    # multipath -e -ll | grep -i nvme
    uuid.8ef20f70-f7d3-4f67-8d84-1bb16b2bfe03 [nvme]:nvme0n1 NVMe,Linux,4.18.0-2
    | `- 0:0:1    nvme0c0n1 0:0     n/a   optimized live
    | `- 0:1:1    nvme0c1n1 0:0     n/a   optimized live
    | `- 0:2:1    nvme0c2n1 0:0     n/a   optimized live
      `- 0:3:1    nvme0c3n1 0:0     n/a   optimized live
    uuid.44c782b4-4e72-4d9e-bc39-c7be0a409f22 [nvme]:nvme0n2 NVMe,Linux,4.18.0-2
    | `- 0:0:1    nvme0c0n1 0:0     n/a   optimized live
    | `- 0:1:1    nvme0c1n1 0:0     n/a   optimized live
    | `- 0:2:1    nvme0c2n1 0:0     n/a   optimized live
      `- 0:3:1    nvme0c3n1 0:0     n/a   optimized live
    ```
5.  如果您更改了 I/O 策略，请检查 `round-robin`{.literal} 是 NVMe
    设备中的活跃 I/O 策略：
    ``` screen
    # cat /sys/class/nvme-subsystem/nvme-subsys0/iopolicy
    round-robin
    ```
:::
::: itemizedlist
**其他资源**
-   [配置内核命令行参数](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/managing_monitoring_and_updating_the_kernel/configuring-kernel-command-line-parameters_managing-monitoring-and-updating-the-kernel){.link}
:::
:::
::: section
::: titlepage
# []{#enabling-multipathing-on-nvme-devices_managing-storage-devices.html#proc_enabling-dm-multipath-on-nvme-devices_enabling-multipathing-on-nvme-devices}在 NVMe 设备中启用 DM 多路径 {.title}
:::
这个过程使用 DM 多路径解决方案在连接的 NVMe 设备中启用多路径。
::: itemizedlist
**先决条件**
-   NVMe 设备连接到您的系统。
    有关通过光纤传输连接 NVMe 的详情请参考 [NVMe over fabric
    设备概述](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/managing_storage_devices/nvme-over-fabrics-using-rdma_managing-storage-devices#overview-of-nvme-over-fabric-devices_nvme-over-fabrics-using-rdma){.link}。
:::
::: orderedlist
**步骤**
1.  检查是否禁用了原生 NVMe 多路径：
    ``` screen
    # cat /sys/module/nvme_core/parameters/multipath
    ```
    这个命令显示以下之一：
    ::: variablelist
    [`N`{.literal}]{.term}
    :   禁用原生 NVMe 多路径。
    [`Y`{.literal}]{.term}
    :   启用原生 NVMe 多路径。
    :::
2.  如果启用了原生 NVMe 多路径，请禁用它：
    ::: orderedlist
    1.  在内核命令行中删除 `nvme_core.multipath=Y`{.literal} 选项：
        ``` screen
        # grubby --update-kernel=ALL --remove-args="nvme_core.multipath=Y"
        ```
    2.  在 64 位 IBM Z 构架中更新引导菜单：
        ``` screen
        # zipl
        ```
    3.  如果存在，从 `/etc/modprobe.d/nvme_core.conf`{.literal}
        文件中删除 `options nvme_core multipath=Y`{.literal} 行。
    4.  重启系统。
    :::
3.  确保启用了 DM 多路径：
    ``` screen
    # systemctl enable --now multipathd.service
    ```
4.  在所有可用路径中分发 I/O。在 `/etc/multipath.conf`{.literal}
    文件中添加以下内容：
    ``` screen
    device {
      vendor "NVME"
      product ".*"
      path_grouping_policy    group_by_prio
    }
    ```
    ::: {.note style="margin-left: 0.5in; margin-right: 0.5in;"}
    ### 注意 {.title}
    当 DM 多路径管理 NVMe
    设备时，`/sys/class/nvme-subsys0/iopolicy`{.literal}
    配置文件不会影响 I/O 分发。
    :::
5.  重新载入 `multipathd`{.literal} 服务以应用配置更改：
    ``` screen
    # multipath -r
    ```
6.  备份 `initramfs`{.literal} 文件系统：
    ``` screen
    # cp /boot/initramfs-$(uname -r).img \
         /boot/initramfs-$(uname -r).bak.$(date +%m-%d-%H%M%S).img
    ```
7.  重建 `initramfs`{.literal} 文件系统：
    ``` screen
    # dracut --force --verbose
    ```
:::
::: orderedlist
**验证**
1.  检查您的系统是否识别 NVMe 设备：
    ``` screen
    # nvme list
    Node             SN                   Model                                    Namespace Usage                      Format           FW Rev
    ---------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- --------
    /dev/nvme0n1     a34c4f3a0d6f5cec     Linux                                    1         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    /dev/nvme0n2     a34c4f3a0d6f5cec     Linux                                    2         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    /dev/nvme1n1     a34c4f3a0d6f5cec     Linux                                    1         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    /dev/nvme1n2     a34c4f3a0d6f5cec     Linux                                    2         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    /dev/nvme2n1     a34c4f3a0d6f5cec     Linux                                    1         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    /dev/nvme2n2     a34c4f3a0d6f5cec     Linux                                    2         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    /dev/nvme3n1     a34c4f3a0d6f5cec     Linux                                    1         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    /dev/nvme3n2     a34c4f3a0d6f5cec     Linux                                    2         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    ```
2.  列出所有连接的 NVMe 子系统。检查命令报告为 `nvme0n1`{.literal} 到
    `nvme3n2`{.literal}，而[*不是*]{.emphasis}，例如：
    `nvme0c0c0n1`{.literal} 到 `nvme0c3n1`{.literal} ：
    ``` screen
    # nvme list-subsys
    nvme-subsys0 - NQN=testnqn
    \
     +- nvme0 fc traddr=nn-0x20000090fadd5979:pn-0x10000090fadd5979 host_traddr=nn-0x20000090fac7e1dd:pn-0x10000090fac7e1dd live
     +- nvme1 fc traddr=nn-0x20000090fadd597a:pn-0x10000090fadd597a host_traddr=nn-0x20000090fac7e1dd:pn-0x10000090fac7e1dd live
     +- nvme2 fc traddr=nn-0x20000090fadd5979:pn-0x10000090fadd5979 host_traddr=nn-0x20000090fac7e1de:pn-0x10000090fac7e1de live
     +- nvme3 fc traddr=nn-0x20000090fadd597a:pn-0x10000090fadd597a host_traddr=nn-0x20000090fac7e1de:pn-0x10000090fac7e1de live
    ```
    ``` screen
    # multipath -ll
    mpathae (uuid.8ef20f70-f7d3-4f67-8d84-1bb16b2bfe03) dm-36 NVME,Linux
    size=233G features='1 queue_if_no_path' hwhandler='0' wp=rw
    `-+- policy='service-time 0' prio=50 status=active
      |- 0:1:1:1  nvme0n1 259:0   active ready running
      |- 1:2:1:1  nvme1n1 259:2   active ready running
      |- 2:3:1:1  nvme2n1 259:4   active ready running
      `- 3:4:1:1  nvme3n1 259:6   active ready running
    mpathaf (uuid.44c782b4-4e72-4d9e-bc39-c7be0a409f22) dm-39 NVME,Linux
    size=233G features='1 queue_if_no_path' hwhandler='0' wp=rw
    `-+- policy='service-time 0' prio=50 status=active
      |- 0:1:2:2  nvme0n2 259:1   active ready running
      |- 1:2:2:2  nvme1n2 259:3   active ready running
      |- 2:3:2:2  nvme2n2 259:5   active ready running
      `- 3:4:2:2  nvme3n2 259:7   active ready running
    ```
:::
::: itemizedlist
**其他资源**
-   [配置内核命令行参数](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/managing_monitoring_and_updating_the_kernel/configuring-kernel-command-line-parameters_managing-monitoring-and-updating-the-kernel){.link}
-   [配置 DM
    多路径](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_device_mapper_multipath/configuring-dm-multipath_configuring-device-mapper-multipath){.link}.
:::
:::
:::
[]{#getting-started-with-swap_managing-storage-devices.html}
::: chapter
::: titlepage
# []{#getting-started-with-swap_managing-storage-devices.html#getting-started-with-swap_managing-storage-devices}第 8 章 swap 入门 {.title}