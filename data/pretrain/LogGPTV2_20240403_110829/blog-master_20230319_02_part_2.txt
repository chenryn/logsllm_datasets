  -d, --dbname=DBNAME   Database name to connect to  
  -c, --command=COMMAND SQL command to run  
  -t, --table=TABLENAME Equivalent to '-c SELECT * FROM TABLENAME'  
      (-c and -t are exclusive, either of them must be given)  
      --inner-join=SUB_COMMAND  
      --outer-join=SUB_COMMAND  
  -o, --output=FILENAME result file in Apache Arrow format  
      --append=FILENAME result Apache Arrow file to be appended  
      (--output and --append are exclusive. If neither of them  
       are given, it creates a temporary file.)  
  -S, --stat[=COLUMNS] embeds min/max statistics for each record batch  
                       COLUMNS is a comma-separated list of the target  
                       columns if partially enabled.  
Arrow format options:  
  -s, --segment-size=SIZE size of record batch for each  
Connection options:  
  -h, --host=HOSTNAME  database server host  
  -p, --port=PORT      database server port  
  -u, --user=USERNAME  database user name  
  -w, --no-password    never prompt for password  
  -W, --password       force password prompt  
Other options:  
      --dump=FILENAME  dump information of arrow file  
      --progress       shows progress of the job  
      --set=NAME:VALUE config option to set before SQL execution  
      --help           shows this message  
Report bugs to .  
```  
4、将pg数据导出为arrow列存储格式 例子  
```  
pg_ctl start  
postgres@haier-5000a:~/pg-strom/arrow-tools$ ./pg2arrow -h 127.0.0.1 -p 1921 -u postgres -d postgres -c 'select id,md5(random()::text) as info,clock_timestamp() as ts from generate_series(1,10000000) as t(id)' -o /home/postgres/test.arrow -S  
postgres@haier-5000a:~/pg-strom/arrow-tools$ ll /home/postgres/test.arrow   
-rw-r--r-- 1 postgres postgres 458M Mar 19 14:29 /home/postgres/test.arrow  
strings /home/postgres/test.arrow|head -n 15  
ARROW1  
min_values  
max_values  
info  
Asia/Shanghai  
min_values  
max_values  
sql_command  
select id,md5(random()::text) as info,clock_timestamp() as ts from generate_series(1,10000000) as t(id)  
...  
```  
## arrow_fdw  
https://heterodb.github.io/pg-strom/arrow_fdw/  
```  
CREATE FOREIGN TABLE flogdata (  
    ts        timestamp,  
    sensor_id int,  
    signal1   smallint,  
    signal2   smallint,  
    signal3   smallint,  
    signal4   smallint,  
) SERVER arrow_fdw  
  OPTIONS (file '/path/to/logdata.arrow');  
IMPORT FOREIGN SCHEMA flogdata  
  FROM SERVER arrow_fdw  
  INTO public  
OPTIONS (file '/path/to/logdata.arrow');  
=# EXPLAIN VERBOSE  
    SELECT sum(lo_extendedprice*lo_discount) as revenue  
      FROM flineorder,date1  
     WHERE lo_orderdate = d_datekey  
       AND d_year = 1993  
       AND lo_discount between 1 and 3  
       AND lo_quantity   Custom Scan (GpuPreAgg)  (cost=12632754.43..12632757.49 rows=204 width=8)  
         Output: (pgstrom.psum((flineorder.lo_extendedprice * flineorder.lo_discount)))  
         Reduction: NoGroup  
         GPU Projection: flineorder.lo_extendedprice, flineorder.lo_discount, pgstrom.psum((flineorder.lo_extendedprice * flineorder.lo_discount))  
         Combined GpuJoin: enabled  
         GPU Preference: GPU0 (Tesla V100-PCIE-16GB)  
         ->  Custom Scan (GpuJoin) on public.flineorder  (cost=9952.15..12638126.98 rows=572635 width=12)  
               Output: flineorder.lo_extendedprice, flineorder.lo_discount  
               GPU Projection: flineorder.lo_extendedprice::bigint, flineorder.lo_discount::integer  
               Outer Scan: public.flineorder  (cost=9877.70..12649677.69 rows=4010017 width=16)  
               Outer Scan Filter: ((flineorder.lo_discount >= 1) AND (flineorder.lo_discount   Seq Scan on public.date1  (cost=0.00..78.95 rows=365 width=4)  
                     Output: date1.d_datekey  
                     Filter: (date1.d_year = 1993)  
(28 rows)  
```  
## parquet fdw
parquet也是大数据场景列存储引擎之一.   
https://github.com/adjust/parquet_fdw  
- [《DuckDB 存储生态: lance(向量存储引擎): Modern columnar data format for ML/超越parquet》](../202303/20230319_01.md)  
- [《PolarDB 开源版通过 duckdb_fdw 支持 parquet 列存数据文件以及高效OLAP》](../202212/20221209_02.md)  
- [《DuckDB COPY 数据导入导出 - 支持csv, parquet格式, 支持CODEC设置压缩》](../202210/20221026_03.md)  
- [《DuckDB DataLake 场景使用举例 - aliyun OSS对象存储parquet》](../202210/20221026_01.md)  
- [《德说-第140期, duckdb+容器+parquet+对象存储, 实现SaaS场景, 低代码拖拉拽多维度实时分析 降本提效》](../202209/20220913_02.md)  
- [《SQLite3 通过virtual table extension 读取Parquet外部表》](../202209/20220910_03.md)  
- [《DuckDB parquet 分区表 / Delta Lake(数据湖) 应用》](../202209/20220905_01.md)  
- [《DuckDB 采用外部 parquet 格式存储 - tpch 测试 - in_memory VS in_parquet》](../202209/20220901_05.md)  
- [《DuckDB 数据库的数据能不能超出内存限制? 以及推荐的使用方法 - parquet》](../202209/20220901_03.md)  
- [《DuckDB 读写 Parquet 文件 - 同时支持远程s3, oss, http等parquet文件读写》](../202209/20220901_01.md)  
- [《PostgreSQL PGSpider 插件(一款基于FDW的并行的联邦查询插件) - sqlite_fdw, influxdb_fdw, griddb_fdw, mysql_fdw, parquet_s3_fdw》](../202105/20210527_02.md)  
- [《PostgreSQL deltaLake 数据湖用法 - arrow + parquet fdw》](../202005/20200527_04.md)  
#### [期望 PostgreSQL|开源PolarDB 增加什么功能?](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
#### [PolarDB 云原生分布式开源数据库](https://github.com/ApsaraDB "57258f76c37864c6e6d23383d05714ea")
#### [PolarDB 学习图谱: 训练营、培训认证、在线互动实验、解决方案、内核开发公开课、生态合作、写心得拿奖品](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
#### [PostgreSQL 解决方案集合](../201706/20170601_02.md "40cff096e9ed7122c512b35d8561d9c8")
#### [德哥 / digoal's github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
#### [购买PolarDB云服务折扣活动进行中, 55元起](https://www.aliyun.com/activity/new/polardb-yunparter?userCode=bsb3t4al "e0495c413bedacabb75ff1e880be465a")
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")