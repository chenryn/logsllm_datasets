trafﬁc quality. If it did a perfect job — i.e., all unintentional clicks
would be turned away, and all intentional clicks would pass through
— computing ii would be trivial. In practice, the interstitial page
has false-negatives (turns away users intentionally clicking the ad)
and false-positives (not turning away click-spam). False-negatives
do not affect ii since it is the number of non-click-spam clicks ac-
tually reaching the landing-page, but false-positives result in an in-
ﬂated value of ii. We use a control ad to estimate the number of
false-positives, and adjust for it.
i and T ′
Let Fi and Ti be the false-positive and true-positive click-through-
ratios for the original ad and the interstitial page i. Similarly, let
F ′
i be the false- and true-positive click-through-ratios for
the control ad (identical ad except with junk ad text as mentioned
above). We have four unknowns, and need four equations to solve.
As discussed above in Assumption 3, we assume T ′
i ≃ 0 and
Fi = F ′
i . The advertiser can measure Fi + Ti = li/d where li
is the number of clicks for original ad reaching the landing page
through the interstitial page, and d is the number of impressions of
177the original ad as reported by the ad network, and the corresponding
equation F ′
i/d′ for the control ad.
i + T ′
i = l′
The estimate for ii is simply Ti × d. Solving the four equations
above for Ti we get the value of ii adjusted downwards to account
for false-positives as: ii = li − l′
Final estimation formula: Combining the above with Eq. (1) we
can estimate the click-spam rate for the original ad as:
i × d
d′
P (Id) =
where:
gd × `li − l′
nd × gi
i × d
d′ ´
(2)
gd, gi : numbers of gold-standard users arriving directly and
d, d′
li, l′
i
through the interstitial page respectively
: number of impressions of the original ad and control
ad respectively
: number of clicks reaching (via the interstitial page)
the landing-page for the original ad and control ad
respectively
nd
: number of clicks on the original ad directly reaching
the landing-page
All these quantities can either be measured directly by the adver-
tiser, or are present in billing reports ad networks generate today.
3.3 Limitations
A key limitation of our approach is that the advertiser must ac-
tively measure click-spam. The advertiser must interpose the inter-
stitial page on live trafﬁc (that he has paid-for), create a control ad
(that he needs to pay for) to correct for false-positives, etc. Both the
interstitial ad and control ad harm the user experience. It would be
far more desirable to be able to passively look at logs and be able
to estimate the click-spam rate from them.
One way to minimize the user experience impact is to apply our
approach reactively when click-spam is suspected, but that runs
into a second limitation — the rarity of data. Any estimation tech-
nique requires statistically signiﬁcant data. The crucial factor in
Eq. (2) is gi and gd — the number of gold-standard users. If these
are small, the click-spam estimate can swing wildly. Suppose the
advertiser manages to identify two gold-standard users, one arriv-
ing through the interstitial page and one directly, and computes a
click-spam estimate based on it. If one new gold-standard user ar-
rives through the interstitial site (or directly), the new click-spam
estimate is half the previous (or will double). For a statistically
signiﬁcant estimate, as we report later, the advertiser must wait for
roughly 25 gold-standard users via the two paths. This is especially
an issue for small advertisers. Small advertisers may have to wait
a long time to get gold-standard users — low advertising budgets
means their ads don’t get shown as much, even if they get shown
users may not click on poorly ranked ads, even if they click they
may not engage in a ﬁnancial transaction with the advertiser, etc.
The need to gather data over such an extended period is clearly at
odds with minimizing the impact on user experience.
A group of small advertisers targeting similar keywords/users (or
an ad-agency representing them) can apply our approach in the ag-
gregate. Doing so has two beneﬁts. First, due to the aggregation
effect the group accretes statistically signiﬁcant data more quickly.
And second, the user experience impact is amortized across many
advertisers. The downside, however, is that advertisers lose the
ability to individually deﬁne what a gold-standard user means (which
our approach otherwise allows) and have to depend on someone
other than themselves to estimate click-spam rates.
Finally, our approach is naturally sensitive to the three choices
the advertiser needs to make: 1) what his deﬁnition of a gold-
standard user is, 2) what interstitial page approach he wishes to
use, and 3) what the text of the control ad is. We discuss the impli-
cation of each design decision in turn. First, if the advertiser sets
too high a bar for the gold-standard user he may not get statisti-
cally signiﬁcant data; if he sets too low a bar that even click-spam
users get classiﬁed as gold-standard he will underestimate click-
spam rates. Second, if the advertiser picks too easy an interstitial
page (everyone gets through), in Eq. (2) gd/gi will approach nd/li
and the estimate will approach 1 (i.e., all clicks are legitimate) if the
advertiser doesn’t use a control ad; or 0 if he uses a control ad (i.e.,
no clicks are valid). If the advertiser picks too hard an interstitial
page (no one gets through), gi and li will both approach 0, and the
click-spam estimate will become undeﬁned. Thus there is clearly
some sweet-spot in designing the interstitial page, which we do not
discuss. Third, if the control ad is not independent of the original
ad (e.g., the random choice of words happens to be related to the
original ad), false-positives may be over- or under- corrected for.
Making the right design choices is advertiser-speciﬁc.
To address the above issues to some degree, we report in the
next section our experience with multiple types of interstitial pages,
different deﬁnitions and numbers of gold-standard users. While our
data shows much promise in our approach, we stress that a more
thorough evaluation is needed.
4. MEASURING CLICK-SPAM TODAY
In this section we ﬁrst validate the correctness of our approach
from the previous section. We then conduct a large-scale measure-
ment study of ten major ad networks and four types of ads.
Validation strategy: We assume that reputed search ad networks
(speciﬁcally Google and Bing) are mature enough that their in-
house algorithms are able to detect and discount for most of the
click-spam on their search afﬁliate network. Validating our mea-
surement approach then involves computing our click-spam esti-
mate and comparing it to the charged clicks for Google and Bing
search ads. Note that our algorithm does not have access to any
data (including historical and aggregate data) that in-house algo-
rithms at Google and Bing have access to, and Google and Bing
do not have access to the detailed user-engagement data we collect
as advertisers for user clicking our ads (speciﬁcally, we do not use
any of the analytics products offered by Google or Bing). Given the
datasets are completely different, if the click-spam rates we com-
pute match that computed by leading ad networks (which they do
as we report below), we have a strong reason to believe that our
measurement approach is sound.
4.1 Methodology
We sign-up with ad networks as three different advertisers (each
targeting different keywords) and follow the methodology from
the previous section. The ﬁrst advertiser targets a highly popular
keyword (celebrity). The second, a medium-popularity keyword
(yoga). And the third, a low-popularity keyword (lawnmower). We
pick the keywords from a ranked list of popular keywords that the
advertising tools of these ad networks provide.
For each keyword we create a realistic looking landing page,
since the policies of the ad networks require us to use keywords
that are relevant to the landing page. We instrument the landing-
page to track mouse-movement, time spent on the page, switching
browser tabs into or away from our page, whether any link on the
page was clicked or not, page scroll, etc. Our instrumentation is
through Javascript on the page; we detect browsers that don’t sup-
port Javascript or have it disabled and exclude those data points.
We direct on-path proxies (if any) to not cache any response so
our server logs accurately reﬂect accesses. Unless otherwise men-
tioned, we pick a lax deﬁnition of gold-standard users based on
1782: A control ad; content is a random set of English words
this telemetry, which is, that the user stays on the page for at least
ﬁve seconds, and (for non-mobile browsers) produces at least one
mouse/cursor move event. We are unable to deﬁne gold-standard
users based on ﬁnancial transactions, even though we expect that to
be very strong signal of intent for real advertisers.
Next we create three interstitial pages: the ﬁrst shows a loading
message for ﬁve seconds before automatically redirecting to the
landing-page. The second asks the user to click a link to continue to
the landing-page. And the third asks the user to solve a CAPTCHA.
We do not test the CAPTCHA interstitial for Google trafﬁc since
their advertiser policies restrict us from doing so.
We then create four ads for each target landing-page. The ﬁrst ad
directly takes the user to the landing page. The second, third and
fourth ads ﬁrst take the user to the three interstitial pages respec-
tively, before continuing on to the landing page. All ads target the
same keyword(s), user demographics, device and platform types,
etc. The reason we create four separate ads (instead of a single
one and interposing the interstitial page after the click) is so that
Google/Bing produce ﬁne-grained billing reports and statistics for
each ad, which we can then validate our design choices against.
We create four additional (control) ads for each landing-page that
correspond to the four original ads, but with junk ad text. The ad
text was generated by picking ﬁve random words from an English
dictionary (e.g., Figure 2).
4.2 Scale
We repeat the above for 10 ad networks. For search ads we mea-
sure Google Search, Bing Search, and 7Search. For contextual ads
we measure Google AdSense and Bing Contextual. For mobile ads
we measure Google Mobile, Bing Mobile, AdMob (now owned by
Google), and InMobi. And lastly for social ads, we measure Face-
book. Altogether this adds up to 216 ads across all the networks.
We run the ads for a period of 50 days as needed to gather enough
data. The majority of the ads were ﬂighted in early January 2012.
We continually adjust bids (mostly revising them higher) to help
the lower popularity ads quickly attract enough data.
In all our ads were shown 26M times across all ad networks.
They resulted in a total of 85K clicks (17K charged). Our ads were
shown at at-least 1811 publisher websites and mobile apps (but the
true number is likely much higher since we cannot determine the
publisher for over 65% of our trafﬁc). The landing pages were
fetched by a total of 33K unique IP addresses located in 190 coun-
tries. We encountered over 7200 browser User-Agent strings (after
sanitizing them to remove browser plugin version numbers).
4.3 Data
We log all web requests made to our server. The logs used in this
study are standard Apache webserver logs that include the user’s
IP address, date and time of access, URL accessed (of a page on
our webserver) along with any GET parameters, the HTTP Referer
value and User-Agent value sent for that request, and a cookie value
we set the ﬁrst time we see a user to identify repeat visits from
the same user. The raw logs including user engagement telemetry
weighs in at over 3 GB.
A sanitized version of our raw logs is available online2.
2http://www.cs.utexas.edu/~vacha/sigcomm12-clickspam.tar.gz
celebrity
yoga
lawnmower
)
.
m
r
o
n
(
d
i
l
a
v
n
o
i
t
c
a
r
F
 1.25
 1
 0.75
 0.5
 0.25
 0
A
B
C
3: Normalized estimates for search ads
4.4 Ethics
Throughout our study we followed the advertiser terms-of-service
(current as of when we did the measurement) for each of the ad
networks we measured. Whenever our ads were rejected by the
ad network (due to policy reasons) we ﬁxed the issue so as to be
compliant; if we couldn’t ﬁx it, we simply dropped that data-point.
High click-spam is an embarrassment for ad networks. Our goal
in this paper is to systematically design a methodology, highlight
the severity of the click-spam problem, and give researchers the
tools and knowledge to further the state of the art. Our goal is not to
embarrass ad networks. As a result, we prefer to report normalized
or relative numbers whenever possible, and anonymize ad network
names whenever it does not affect the core message of this paper.
Lastly, we expressly try to minimize adversely impacting user
experience on these ad networks. For example, in order to get
enough clicks on an ad, we have two options: run the campaign
for longer, or increase the bid amount. We always choose the latter
to minimize the time our ad is active on the network. For ads where
despite increasing the bid we cannot gather trafﬁc fast enough, we
prefer to give up on that data-point and stop running that ad. Mini-
mizing the time our ads are active also minimizes our contribution
to the existing auction volatility for the keywords we bid on. As of
this writing we have not received any complaints from ad networks,
users, or advertisers regarding our study.
4.5 Validation
Figure 3 compares (normalized) complementary click-spam rates
computed by our approach (plotted as error bars) and the (nor-
malized) complementary click-spam rates as reported by Bing and
Google for their search ad networks (plotted as bars). We ran an-
other experiment where where we explicitly set our ad campaign
to exclude syndicated search partners for one of the search ad net-
works (plotted as C in Figure 3). The Bing and Google estimates
are the ratio between the number of clicks we were charged for
(from the billing report), and the total number of landing-page fetches
(from our logs). For our approach, we calculate two separate esti-
mates based on the delay and click interstitial pages. The spread of
the error bar plots the max and the min of the estimates we compute.
The center tick plots the average. The ﬁgure plots the estimates for
all three ads we ﬂighted. In line with our goals, this ﬁgure (and
all other ﬁgures in this section) are normalized so one of the data
points is 1.
As is evident from Figure 3, our estimate for the yoga and lawn-
mower ads are in the same ball-park as that reported by Google
and Bing. We manually investigated the difference between our
estimates and that for the celebrity ad. We found over 50 clicks
from sites associated with well-known search redirection viruses
where browser toolbars hijack normal user searches and funnel
them through afﬁliate search programs (Section 5.3.1 has more de-
179d
e
r
a
e
c
l
n
o
i
t
c
a
r
F
 0.3
 0.25
 0.2
 0.15
 0.1
 0.05
 0
click
delay
captcha
d
r
a
d
n
a
t
s
-
d
o
g
l
n
o
i
t
c
a
r
F