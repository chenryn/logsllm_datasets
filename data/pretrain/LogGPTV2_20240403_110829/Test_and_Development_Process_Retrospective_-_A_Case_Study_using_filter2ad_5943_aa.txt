title:Test and Development Process Retrospective - A Case Study using
ODC Triggers
author:Ram Chillarege and
Kothanda Ram Prasad
Test and Development Process Retrospective
–   a Case Study using ODC Triggers
Ram Chillarege  &  Kothanda Ram Prasad
CHILLAREGE  INC.,  NEW YORK, APRIL 2002
www.chillarege.com
Abstract
We present a case study of a product development
retrospective  analysis  conducted  to  gain  an
understanding of the test and development  process
effectiveness.    Orthogonal  Defect  Classification
(ODC) is used as an analysis method  to gain insight
beyond  what  classical  qualitative  analysis  would
yield for the probable cause of delays during test.
1. ODC Trigger analysis provides the insight to un-
derstand the degree of blockage in test, probable
cause, and consequences to the test and develop-
ment process.
2.Trigger distribution changes with respect to time
shows the stabilization of the product, and varia-
tion among components shows the systemic na-
ture of issues.
3. The study makes nine specific inferences and rec-
ommendations based on these analyses to guide
the engineering of future releases
Introduction
At the conclusion of a development effort, it is
quite common for a project to conduct a postmortem,
or what is also called a retrospective.  The value of
such an exercise is to gain an understanding of the
process and learn from mistakes.  There are no set
methods  on how to conduct such retrospectives, but
the process and spirit of such an exercise is quite
well  understood.    However,  the  degree  of
sophistication used for such analysis varies widely,
and  consequently  the  degree  of  insight  and
understanding gained.
Informal analysis is by far the most commonly
used approach.  The opinions and observations of
the team are collected and consolidated to generate
conclusions.  Often,  it  becomes  hard  to  resolve
conflicting views or establish diagnostics for clear
differentiation of causes.  By and large, the role of
quantitative methods to support such retrospective
analysis is limited to the few organizations that have
the  necessary  resources  and  skills  in  software
engineering process analysis.
Orthogonal  Defect  Classification  (ODC)  is  a
measurement system for software processes based
on the semantic information contained in the defect
stream  [Chillarege  92].    It  brings  a  new  level  of
sophistication  to  such  analysis  and  leverages
information captured in the development process.
Today it provides objective methods to gain clarity
where in the past one relied purely on intuition and
experience  [Bassin  98].  ODC  methods  have  been
expanded to multidisciplinary design as reported in
[Amezquita  96].    Recent  case  studies  at  IBM  are
reported in [Butcher 02] and at CISCO in [Mullen 02].
A common misconception is that the full scope
of ODC is needed for such analysis.  While the full
scope of the data is most certainly valuable, it is not
always  necessary.    The  scope  of  data  needed  is
dependent on the types of questions raised. Often,
even a restricted set of data can prove valuable.
This case study uses ODC Triggers to analyze the
efficiency of the software test subprocess.  Trigger
is  our  key  to  gaining  an  understanding  of  what
happened  during  a  test  cycle  and  its  efficacy.
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:19:12 UTC from IEEE Xplore.  Restrictions apply. 
Additional data such as ODC defect type and ODC
source [Chillarege 92][ODC Web] would be useful
to further narrow down to the specific development
practice problems, but are not critical for the first
round of analysis.
ODC triggers can be easily generated from the
defect  tracking  logs.    For  retrospective  analysis,
triggers tend to be easy to extract from test defect
logs, whereas some of the other ODC attributes may
not  be  as  easy.  This  works  in  our  favor  for  this
exercise, since much of the analysis needs triggers -
probably the most valuable data when studying the
test subprocess.
While this paper is a case study in retrospective
analysis, it also brings together ideas to deliver a
more  complete  picture.  Several  earlier  papers
discussed  specific  technical  issues.  We  build  on
them, exploiting measurement to provide insight far
beyond  what  classicial  qualitative  retrospective
analysis would do. From an academic perspective,
we are helping develop the area of  experimental
research in computer science [Iyer 90].
The rest of the paper is organized thus. We begin
with a brief case history and set the objectives for
this  detailed  retrospective.  The  data  and  analysis
section  that  follows  has  several  subsections  of
analysis. The defect profile and the defect priority
capture  quantitatively  what  was  experienced
qualitatively.  This  is  followed  by  detailed  trigger
analysis.  A brief description of triggers is included
to help the reader who is not too familiar with ODC.
Trigger distributions are studied for the entire test
cycle, and as a function of time and component. These
analyses are used to summarize our findings, under
a  section  called  inferences  and  recommendations.
Nine specific findings are summarized that capture
the insight and diagnosis for the delays experienced
in 
to
recommendations for engineering improvement in
the  next  release.  The  conclusions  circle  back  to
address questions raised in the objectives section.
test.  The  findings  also 
translate 
Case History
The project being developed is the fourth or fifth
iteration  of  an  enterprise  application:  a  typical
modern three tier web application, with databases,
access  control,  and  zero  footprint  clients.    The
backend  is  a  relational  database  that  contains  the
enterprise data and is also a container for some of
the business rules.  The application layer is supported
by an application server and business objects.  There
is an authentication and security component, report
generation subsystems, business rules management,
import and export of data, and a rendering layer to
handle the dynamic data. Since these applications
are  replacing  many  of  the  classical  client-server
applications, some of which still run on IBM 3270
screens, there is a considerable amount of work to
blend the usability of the modern web face of the
application to the efficiency, speed and simplicity of
the  classical  mainframe  applications.    This  is  a
difficult task, since the designs needed to emulate
the  speed  and  simplicity  of  UI  in  mainframe
applications  yet    compete  with  the  desktop  style,
while not compromising too much performance for
similar functionality.
The major components are fairly standard for such
application  and  include  the  backend  relational
database,  authentication,  business 
logic,
customization of the logic and workflow, data import
and export, views and reports and dynamic rendering
engines, etc.
The  logic  of  the  application  is  quite  well
understood  since  this  is  the  nth  iteration  with
essentially  the  same  core  business  logic.  The
business growth had put demands on performance
and  usability  and  also  added  to  the  number  of
business rules. Such growth usually placed demands
in all dimensions of the product - it is not merely a
feature increase, but one that needs to accomplish
feature growth with overall improvement in quality
and appeal.
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:19:12 UTC from IEEE Xplore.  Restrictions apply. 
Level 2 Key Process Areas (KPA)
Requirements
Management
Project Planning
Project Tracking and
Oversight
Requirements docu-
mented for each
module and kept up-to-
date.
Function Points used
for estimation.  Weekly
reporting and Defect
tracking
Configuration
Management
SCM for documents,
code and test cases
Software Quality
Assurance
No formal SQA in
place
Subcontract
Not Applicable
Level 3 Key Process Areas (KPA)
Training, Intergroup coordina-
tion & Peer Reviews
Integrated software develop-
ment
Organizational Process focus,
Definition, and software product
engineering
Existed
Partially
met
Minimally
met
Table 1   Our  “quick and dirty” assessment  places
the project’s  SEI CMM software maturity  between
a Level 2 and Level 3.
There  has  been  greater  maturity  in  the
development team through evolution.  While formal
assessments  of  Software  Engineering  Institute’s
Capability Maturity Model, (SEI CMM) [Humphrey
89] [Paulk 93] were not conducted, we provide a
rough assessment shown in Table 1. Based on the
KPAs,  we  place  the  team  at  a  maturity  level
somewhere between a Level 2 and a Level 3.
Earlier experience with this application had taught
the  development  team  the  value  of  careful
requirements management.  A focus had been placed
on  understanding  customer  requirements  and
reviewing them with prospective clients. The Quality
Assurance  (QA)    team  had  also  been  involved  in
reviewing  such  requirements  and  developing  test
cases from them.
Given the degree of change in the new release, a
thorough design review was necessary to alter the
architecture  and  performance  of  the  application.
Changes were separately prototyped and evaluated
with  test  data  so  that  the  architecture  could  be
validated before migrating the code.  This provided
substantial isolation of the core of the application
from  the  business  rules  and  user  interface.    The
techniques  for  writing  business  rules  were  also
prototyped  and  tested  so  that  development  could
begin without too many unknowns in the data and
control flow through the application.
Testing the new business rules required that most
of  the  application  worked.  Given  the  degree  of
interaction with the various components, it was not
possible to isolate business rules and test them on a
test  harness.    Therefore  the  QA  team  diligently
developed a test suite that covered the entire set of
requirements.  A regression bucket was built, such
that an automated test execution platform could run
against the business rules.
Objectives
The primary objective in this detailed quantitative
retrospective is to gain a clearer understanding of
the development and test processes.  There was a
belief that test did not perform as well as it could
have, and was slow. Is this true? And if so, why?
There  were  several  process  improvements
engineered  into  this  development  process,  as
discussed in the case history. There was considerable
focus on architecture and design, a very thorough
requirements  process,  and  a  well-articulated
development and build mechanism.
Yet, the experience through test was prolonged
and  it  was  felt  that  the  progress  was  slower  than
expected.  In spite of that, the product stabilized and
customers seemed satisfied.
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:19:12 UTC from IEEE Xplore.  Restrictions apply. 
s
t
c
e
e
D
f
.
o
N
e
v
i
t
l
a
u
m
u
C
Fig 1.  Defect Growth Curve
Fig 2.  Priority Distribution vs Stage
Stage 2
Stage 3
1400
1200
1000
800
600
400
200
0
Stage 1
0
50
100
150
200
Calendar Days
Percent
80
60
40
20
0
Stage 1
Stage 2
Stage 3
Time ... as Stage
Priority 1
Priority 2
Priority 3
Figures 1 and 2 illustrate quantitatively some of the challenges experienced qualitatively. The high proportion of priority
1 defects blocked the progress of test - another way to communicate the sense that test was slow. The stages 1, 2, 3
divide the time line and S curve into distinct phases of defect detection.  Note that while there is a pronounced shift in the
priority distribution among the stages, the fraction of priority 1 and 2 defects is still quite large. The reasons behind this,
or the “why?”, needs ODC trigger analysis to gain insight, coroborate the evidence, and shed light on possible solutions.
Some of the questions we ask are:
• Was the test process independently responsible
for the delays?
• Were there elements of development that could
have helped the test cycle?
• Was there a visible benefit gained from the good
design and architecture process?
• Was there a visible benefit gained from the more
thorough requirements process?
• What would make the test process more effec-
tive?
The  answers  to  these  questions  need  to  be
substantiated with detail rather than a mere yes, or
no.  The  data  and  analysis  section  investigates
probable  cause  to  summarize  the  inferences  and
recommendations.  In  conclusion  we  tie  back  the
detailed findings to our higher level objectives.
Data & Analysis