m
(
e
m
T
i
 800
 700
 600
 500
 400
 300
 200
 100
 0
8KB
16KB
32KB
64KB
128KB
256KB
512KB
Block Size
Figure 3: Read time for 50 random blocks
As Figure 3 shows, the time to sample a ﬁxed number of random
blocks from a 2GB ﬁle is roughly constant for blocks up to 64KB,
regardless of drive manufacturer. We suspect that this behavior is
due to prefetching at both the OS and hard drive level. Riedel et
al. also observe in their study [29] that the OS issues requests to
disks for blocks of logical size 64KB, and there is no noticeable
difference in the time to read blocks up to 64KB.
For our purposes, therefore, a remote server can read 64KB ran-
dom blocks at about the same speed as 8K blocks. If we were to
sample blocks smaller than 64KB in our RAFT protocol, we would
give an advantage to the server, in that it could prefetch some addi-
tional ﬁle blocks essentially for free. For this reason, we choose to
use 64KB blocks in our practical protocol instantiation.
Figure 2 depicts the read time distributions for a random 64KB
block chosen from a 2GB ﬁle. To generate this distribution, 250
random samples were taken from a 2GB ﬁle. The read time for
each request was recorded. This was repeated 400 times, for a total
of 100,000 samples, clearing the system memory and drive buffer
between each test. The operating system resides on the Hitachi
drive, and occasionally contends for drive access. This causes out-
liers in the tests (runs which exceed 125% of average and contain
several sequential reads an order of magnitude larger than average),
which were removed. Additional tests were performed on this drive
to ensure the correctness of the results. By comparison, the vari-
ability between runs on all other drives was less than 10%, further
supporting the OS-contention theory.
While the seek time average for a single block is around 6 ms,
the distribution exhibits a long tail, with values as large as 132 ms.
(We truncate the graph at 20 ms for legibility.) This long tail does
not make up a large fraction of the data, as indicated by the 99.9%
cutoffs in ﬁgure 2, for most of the drives. The 99.9% cutoff for the
Hitachi drive is not pictured as it doesn’t occur until 38 ms. Again,
we expect contention from the OS to be to blame for this larger
fraction of slow reads on that drive.
Read times for blocks of this size are dominated by seek time
and not affected by physical placement on disk. We conﬁrmed this
experimentally by sampling from many ﬁles at different locations
on disk. Average read times between ﬁles at different locations
differed by less than 10%. The average seek time for 64KB blocks
does, however, depend on the size of the ﬁle from which samples
are being taken, as shown in Figure 4.
We observe that the average block read time increases with the
ﬁle size, due to more head movement. While this relationship is
fairly linear above a certain point (close to 40MB ﬁles), small ﬁles
exhibit signiﬁcantly reduced average block read times, likely due
to the disk buffer. Once the ﬁle is small enough to ﬁt in the disk
buffer, the drive will respond from its cache without performing the
508 10
 1
 0.1
)
s
m
(
e
m
T
i
 0.01
 1
Effects of File Size on Average Block Read Time
HP
Seagate
Fujitsu
 10
 100
 1000
 10000
Original File Size (MB)
Figure 4: Effects of ﬁle size on average block retrieval time
physical seek, returning data much more quickly. This indicates
that RAFTs will not work for ﬁles smaller than the combined disk
buffer sizes of the drives being used to store the ﬁle, an hypothesis
we conﬁrm in our experimental evaluation.
In the next section, we modify our basic protocol to smooth out
seek-time variance. The idea is to sample (seek) many randomly
chosen ﬁle blocks in succession.
6. PRACTICAL RAFT PROTOCOL
In this section, we propose a practical variant of the basic RAFT
protocol from Section 4. As discussed, the main challenge in prac-
tical settings is the high variability in drive seek time. The key
idea in our practical RAFT here is to smooth out the block access-
time variability by requiring the server to access multiple blocks
per drive to respond to a challenge.
In particular, we structure queries here in multiple steps, where
a step consists of a set of ﬁle blocks arranged such that an (honest)
server must fetch one block from each drive. We propose in this
section what we call a lock-step protocol for disk-block scheduling.
This lock-step protocol is a non-interactive, multiple-step variant of
the basic RAFT protocol from Section 4. We show experimentally
that for large enough ﬁles, the client can, with high probability,
distinguish between a correct server and an adversarial one.
6.1 The lock-step protocol
A naïve approach to implementing a multiple-step protocol with
q steps would be for the client to generate q (non-overlapping) chal-
lenges, each consisting of c block indices, and send all qc distinct
block indices to the server. The problem with this approach is that
it immediately reveals complete information to the server about all
queries. By analogy with job-shop scheduling [26], the server can
then map blocks to drives to shave down its response time. In par-
ticular, it can take advantage of drive efﬁciencies on reads ordered
by increasing logical block address [32]. Our lock-step technique
reveals query structure incrementally, and thus avoids giving the
server an advantage in read scheduling. Another possible approach
to creating a multi-step query would be for the client to specify
steps interactively, i.e., specify the blocks in step i + 1 when the
server has responded to step i. That would create high round com-
plexity, though. The beneﬁt of our lock-step approach is that it
generates steps unpredictably, but non-interactively.
The lock-step approach works as follows. The client sends an
initial one-step challenge consisting of c blocks, as in the basic
RAFT protocol. As mentioned above, to generate subsequent steps
non-interactively, we use a Fiat-Shamir-like heuristic [12] for sig-
nature schemes: The block indices challenged in the next step de-
pend on all the block contents retrieved in the current step (a “com-
mitment”). To ensure that block indices retrieved in next step are
unpredictable to the server, we compute them by applying a cryp-
tographically strong hash function to all block contents retrieved in
the current step. The server only sends back to the client the ﬁ-
nal result of the protocol (computed as a cryptographic hash of all
challenged blocks) once the q steps of the protocol are completed.
The lock-step protocol algorithms Keygen, Encode, Map, and
Reconstruct are similar to our basic RAFT. Let h be a cryptograph-
ically secure hash function with ﬁxed output (e.g., from the SHA-2
family). Assume for simplicity that the logical placement gener-
ated by Map in the basic RAFT protocol is Cj = {jn/c, jn/c +
1, . . . , jn/c + n/c − 1}. We use c suitable hash functions that
output indices in Cj: hj ∈ {0, 1}∗ → Cj . (In practice, we might
take hj(x) = h(˜j||x) mod Cj , where ˜j is a ﬁxed-length index
encoding.)
The Challenge, Response, and Verify algorithms of the lock-step
protocol with q steps are the following:
- In Challenge(n, G, t, c), the client sends an initial challenge
1, . . . , i1
Q = (i1
j ∈ {1, . . . , c}, along with random nonce ν ∈U {0, 1}l.
j selected randomly from Cj, for
c) with each i1
- Algorithm Response(Q) consists of the following steps:
1. S reads ﬁle blocks fi1
2. In each step r = 2, . . . , q, S computes
speciﬁed in Q.
, . . . , fi1
c
1
c
1
||fir−1
|| . . . ||ir−1
|| . . . ||fir−1
j ← hj(ir−1
ir
the block indices ir
crements ir
index that has not yet been retrieved. S schedules blocks fir
trieval, for all j ∈ {1, . . . , c}.
||h(ν, j)). If any of
j have been challenged in previous steps, S in-
j by one (in a circular fashion in Cj ) until it ﬁnds a block
for re-
c
j
1
3. S sends response R = h(fi1
c ||ν)
to the client, who measures the time T from the moment when chal-
lenge Q was sent.
|| . . . ||fiq
|| . . . ||fiq
|| . . . ||fi1
c
1
1
- In Verify(G, Q, R, T ), the client checks ﬁrst correctness of R
by recomputing the hash of all challenged blocks, and comparing
the result with R. The client also checks the timing of the reply
T , and accepts the response to be prompt if it falls within some
speciﬁed time interval (experimental choice of time intervals within
which a response is valid is dependent on drive class and is dis-
cussed in Section 6.2 below).
Security of lock-step protocol. We omit a formal analysis. Brieﬂy,
derivation of challenge values from (assumed random) block con-
tent ensures the unpredictability of challenge elements across steps
in Q. S computes the ﬁnal challenge result as a cryptographic hash
of all qc ﬁle blocks retrieved in all steps. The collision-resistance
of h implies that if this digest is correct, then intermediate results
for all query steps are correct with overwhelming probability.
6.2 Experiments for the lock-step protocol
In this section, we perform experiments to determine the number
of steps needed in the lock-step protocol to distinguish an honest
server using c drives from an adversarial server employing d < c
drives. As discussed in Section 3.3, we evaluate both servers that
"reserve" drives for RAFT testing, as well as services operating
under contention.
6.2.1 Reserved drive model
We begin by looking at the "reserved" drive model, which we can
test locally. The ﬁrst question we attempted to answer with our tests
is if we are able to distinguish an honest server from an adversarial
509Average Separation
100% Separation
10 MB
100 MB
1 GB
10 GB
)
s
m
(
e
m
T
i
 1400
 1200
 1000
 800
 600
 400
 200
 0
-200
-400
-600
10 MB
100 MB
1 GB
10 GB
)
s
m
(
e
m
T
i
 1000
 800
 600
 400
 200
 0
-200
-400
-600
-800
 0
 50
 100
 150
 200
 250
 0
 50
 100
 150
 200
 250
Steps
Steps
Figure 5: Average difference between adversarial and honest
response times in a challenge
Figure 6: Complete separation of adversarial and honest re-
sponse times in a challenge
one employing fewer drives based only on disk access time. Is there
a range where this can be done, and how many steps in the lock-step
protocol must we enforce to achieve clear separation? Intuitively,
the necessity for an adversarial server employing d ≤ c − 1 drives
to read at least two blocks from a single drive in each step forces the
adversary to increase its response time when the number of steps
performed in the lock-step protocol increases.
Detailed experiments for c = 3 drives.
In practice, ﬁles are not typically distributed over a large number
of drives (since this would make meta-data management difﬁcult).
Here, we focus on the practical case of c = 3. The ﬁle sizes pre-
sented are those of the original ﬁle before encoding and splitting
across drives. Files are encoded to survive one drive failure in the
honest server case and are evenly spread over the available drives.
The adversarial server must store the same total amount of informa-
tion in order to respond to challenges, but does so using one fewer
drive than the honest server. The adversarial server spreads the en-
coded ﬁle evenly across two drives and must perform a double read
from one of the drives in each step of the lock-step protocol.
For each ﬁle size, we performed 200 runs of the protocol for both
the honest and adversarial servers. The honest server stores data on
the HP, Seagate, and Fujitsu drives, while the adversary uses only
the HP and Seagate drives. We show in Figure 5 the average ob-
served difference between the adversarial and honest servers’ time
to reply to a challenge as a function of the number of steps in the