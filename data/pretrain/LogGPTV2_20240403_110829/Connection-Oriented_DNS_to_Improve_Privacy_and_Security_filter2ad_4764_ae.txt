### Additional Capacity for Peaks

Although TLS is computationally expensive, it generally does not limit DNS performance. For clients, we demonstrate in § VI-E that Round-Trip Time (RTT) is the primary factor affecting performance, rather than computational overhead. Most DNS servers today are bandwidth-limited and operate with very light CPU loads. We anticipate that server memory will be a more significant constraint than CPU. While our cost estimation is promising, we are still in the process of conducting a full-scale experimental evaluation of T-DNS under high load conditions.

### VI-B Latency: Stub-to-Recursive Resolver

Next, we conduct experiments to evaluate the impact of T-DNS on DNS usage between stub resolvers and both local and public recursive resolvers.

#### Typical RTTs

We estimate typical stub-to-recursive resolver RTTs in two ways. First, we measure RTTs to the local DNS server and to three third-party DNS services (Google, OpenDNS, and Level3) from 400 PlanetLab nodes. These experiments show that ISP-provided resolvers have very low RTTs, with 80% of RTTs less than 3 ms and only 5% exceeding 20 ms. Third-party resolvers exhibit more variability, but anycast keeps RTTs moderate: the median RTT for Google Public DNS is 23 ms, with 10–25% of stubs experiencing RTTs of 50 ms or higher. Other services are somewhat more distant. Second, studies of home routers show typical RTTs of 5-15 ms [73].

#### Methodology

To estimate T-DNS performance, we experiment with a stub resolver connected to a nearby (1 ms) and a more distant (35 ms) recursive resolver (values chosen to represent typical extremes observed in practice). We use a custom DNS stub and BIND-9.9.3 combined with our proxy as the recursive resolver. For each protocol (UDP, TCP, TLS), the stub makes 140 unique queries, randomly drawn from the Alexa top-1000 sites [1], using the respective protocol. We restart the recursive resolver before changing protocols to ensure each test starts with a known, cold cache. We then vary the combination of protocol (UDP, TCP, and TLS), use of pipelining or stop-and-wait, and in-order and out-of-order processing. Connections are either reused, with multiple queries per TCP/TLS connection (p-TCP/p-TLS), or not reused, where the connection is reopened for each query. Each experiment is repeated 10 times, and combined results are reported.

#### Cold-Cache Performance

Figure 5 shows the results of these experiments. We observe that UDP, TCP, and TLS performance is generally similar when other parameters are held consistent (compare (a), (b), and (c), or (g), (h), and (i)). Even with an RTT of 35 ms, the recursive query process still dominates protocol choice, and setup costs are moderate. The data indicates that out-of-order processing is essential when pipelining is used; case (f) shows head-of-line blocking compared to (h). This suggests that while current servers support TCP, our optimizations are necessary for high performance. Pipelining shows higher latency than stop-and-wait, regardless of the protocol (compare (g) with (a) or (i) with (c)). This difference occurs because 140 simultaneous queries queue at the server when the batch begins; UDP is nearly equally affected as TCP and TLS (compare (i) and (h) with (g)). Finally, we see that the costs of TLS are minimal here: comparing (c) with (b) and (a) or (i) with (g) and (h), natural variation dominates performance differences.

#### Warm-Cache Performance

Cold-cache performance is dominated by communication time to authoritative name servers. For queries where replies are already cached, this communication is omitted, and connection setup times become noticeable. For connection handling, performance of cache hits is equivalent to authoritative replies, so our recursive-to-authoritative experiments in § VI-C represent warm-cache performance with 100% cache hits. (We verified this claim by repeating our stub-to-recursive experiment, making each query twice and reporting performance only for the second query, which will always be answered from the cache.) While cache hits are expensive when they must start new connections, persistent connections completely eliminate this overhead (Figure 6, cases (e) and (f) compared to (a)). Additionally, median TCP out-of-order pipelined connections (cases (h) and (i)) are slightly faster than UDP (case (g)) because TCP groups multiple queries into a single packet.

We conclude that protocol choice has little impact on performance between stub and recursive resolvers provided RTT is small and connections can be reused. This result holds true for cold caches, where connection setup is overshadowed by communication time to authoritative name servers. It also applies to warm caches if connections can be frequently reused or restarted quickly. We know that connections can be reused most of the time (§ IV-C), and TCP fast open and TLS resumption can reduce costs when they are not reused.

### VI-C Latency: Recursive to Authoritative

We next consider the performance between recursive resolvers and authoritative name servers. While recursive resolvers are usually near stubs, authoritative servers are globally distributed, resulting in larger and more diverse RTTs.

#### Typical RTTs

To measure recursive-to-authoritative RTTs, we use both the Alexa top-1000 sites and a random sample of 1000 sites from the Alexa top-1M sites. We query each site from four locations: the U.S. (Los Angeles), China (Beijing), the U.K. (Cambridge), and Australia (Melbourne). We query each domain name iteratively and report the time fetching the last component, taking the median of 10 trials to be robust to competing traffic and name server replication. We measure query time for the last component to represent caching of higher layers. The U.S. and U.K. sites are close to many authoritative servers, with a median RTT of 45 ms, but a long tail with 35% of RTTs exceeding 100 ms. Asian and Australian sites have generally longer RTTs, with only 30% closer than 100 ms (China) and 20% closer than 30 ms (Australia), while the rest are 150 ms or more. This increase is due to the long propagation latency for services without physical presence in these countries. (Full data is provided in our technical report [86].)

#### Methodology

To evaluate query latencies with larger RTTs between client and server, we set up a DNS authoritative server (BIND-9.9.3) for an experimental domain (example.com) and query it from a client 35 ms (8 router hops on a symmetric path) away. Since performance is dominated by round trips rather than computation, we measure latency in units of RTT, and these results generalize to other RTTs. For each protocol, we query this name server directly, 140 times, varying the protocol in use. As before, we repeat this experiment 10 times and report medians of all combined experiments (Figure 6). Variation is usually minimal, so standard deviations are omitted except for cases (h) and (i).

#### Performance

Figure 6 shows the results of this experiment. We first confirm that performance is dominated by protocol exchanges: cases (a), (b), and (c) correspond exactly to 1, 2, and 5 RTTs as predicted. Second, we see the importance of connection reuse or caching: cases (e) and (f) with reuse have identical performance to UDP, as does TCP fast open (case (d)).

As before, pipelining for TCP shows a higher cost because the 140 queries queue behind each other. Examination of packet traces for cases (h) and (i) shows that about 10% of queries complete in about 1 RTT, while additional responses arrive in batches of around 12, showing stair-stepped latency. For this special case of more than 100 queries arriving simultaneously, a single connection adds some latency.

We next consider the cost of adding TLS for privacy. The community generally considers aggregation at the recursive resolver sufficient for anonymity, but TLS may be desired for additional privacy or as a policy [27], so we consider it as an option. Without connection reuse, a full TLS query always requires 5 RTTs (case (c), 175 ms): the TCP handshake, the DNS-over-TLS negotiation (§ III-B2), two for the TLS handshake, and the private query and response.

However, once established, TLS performance is identical to UDP: cases (f) and (a) both take 1 RTT. Encryption's cost is negligible compared to moderate round-trip delays when we have an established connection. We expect similar results with TLS resumption.

Finally, when we add pipelining and out-of-order processing, we see similar behavior as with TCP, again due to how large, batched queries become synchronized over a single connection.

We conclude that RTTs completely dominate recursive-to-authoritative query latency. We show that connection reuse can eliminate connection setup RTT, and we expect TLS resumption will be as effective as TCP fast-open. We demonstrate that TCP is viable from recursive-to-authoritative, and TLS is also possible.

### VI-D Client Connection-Hit Fractions

Connection reuse is important, and § IV-C found very high reuse from the server's perspective. We next show that client connection-hit fractions are lower because many clients query infrequently.

To evaluate client connection hit fractions, we replay our three DNS traces through the simulator from § IV-C, but we evaluate connection hit fractions per client. Figure 8 shows these results, with medians (lines) and quartiles (bars, with slight offset to avoid overlap).

Among the three traces, the DNSChanger hit fraction exceeds Level 3, which exceeds B-Root, because servers further up the hierarchy see less traffic from any given client. We see that the top quartile of clients have high connection hit fractions for all traces (at 60 s: 95% for DNSChanger, 91% for Level 3, and 67% for B-Root). The connection hit rate for the median client is still fairly high for DNSChanger and Level 3 (89% and 72%), but quite low for B-Root (28%). Since most B-Root content can be cached, many clients only contact it infrequently and thus fail to find an open connection.

These results suggest that clients making few requests will need to restart connections frequently. Fortunately, TCP Fast Open and TLS Resumption allow these clients to carry the state needed to accelerate this process.

### VI-E Modeling End-to-End Latency for Clients

With this data, we can now model the expected end-to-end latency for DNS users and explore how stub, recursive, and authoritative resolvers interact with different protocols and caching. Our experiments and measurements provide parameters and focus modeling on connection setup (both latency and CPU costs). Our model captures clients restarting connections, servers timing out state, and the complex interaction of stub, recursive, and authoritative resolvers. Our modeling has two limitations. First, we focus on typical latency for users, per-query; the modeling reflects query frequency, emphasizing DNS provisioning for common queries and reflecting queries to rare sites only in proportion to their appearance in our traces. We do not evaluate mean latency per-site, as that would be skewed by rarely used and poorly provisioned sites. Second, our models provide mean performance; they cannot directly provide a full distribution of response times and "tail" performance [19]. We are interested in using trace replay to determine a full distribution with production-quality servers, but this is significant future work.

#### Modeling

We model latency from client to server, \( L_{c\sigma} \), as the probability of connection reuse (\( P_{C_{c\sigma}} \)) and the cost of setting up a new connection (\( SC_{c\sigma} \)) added to the cost of the actual query (\( Q_{c\sigma} \)):

\[ L_{c\sigma} = (1 - P_{C_{c\sigma}}) \cdot SC_{c\sigma} + Q_{c\sigma} \]

From Figure 6, \( Q_{c\sigma} \) is the same for all methods with an open connection: about one client-server RTT, or \( R_{c\sigma} \). Setup cost for UDP (\( SC_{udp} \)) is 0. With the probability for TCP fast-open (\( P_{TFO_{c\sigma}} \)), TCP setup costs:

\[ SC_{tcp} = (1 - P_{TFO_{c\sigma}}) \cdot R_{c\sigma} \]