for peaks would require additional capacity.
Although TLS is computationally expensive, TLS computa-
tion will not generally limit DNS. For clients, we show (§ VI-E)
that RTT dominates performance, not computation. Most DNS
servers today are bandwidth limited and run with very light
CPU loads. We expect server memory will be a larger limit
than CPU. While our cost estimation is very promising, we
are still in the progress of carrying out full-scale experimental
evaluation of T-DNS under high load.
VI-B Latency: Stub-to-Recursive Resolver
We next carry out experiments to evaluate the effects of
T-DNS on DNS use between stub and both local and public
recursive resolvers.
Typical RTTs: We estimate typical stub-to-recursive re-
solver RTTs in two ways. First, we measure RTTs to the local
DNS server and to three third-party DNS services (Google,
OpenDNS, and Level3) from 400 PlanetLab nodes. These
experiments show ISP-provided resolvers have very low RTT,
with 80% less than 3 ms and only 5% more than 20 ms. Third-
party resolvers vary more, but anycast keeps RTT moderate:
median RTT for Google Public DNS is 23 ms, but 50 ms or
higher for the “tail” of 10–25% of stubs; other services are
somewhat more distant. Second, studies of home routers show
typical RTTs of 5-15 ms [73].
Methodology: To estimate T-DNS performance we exper-
iment with a stub resolver with a nearby (1 ms) and more
distant (35 ms) recursive resolver (values chose to represent
typical extremes observed in practice). We use our custom
DNS stub and the BIND-9.9.3 combined with our proxy
as the recursive. For each protocol (UDP, TCP, TLS), the
stub makes 140 unique queries, randomly drawn from the
Alexa top-1000 sites [1] with DNS over that protocol. We
restart the recursive resolver before changing protocols, so
each protocol test starts with a known, cold cache. We then
vary each combination of protocol (UDP, TCP, and TLS),
use of pipelining or stop-and-wait, and in-order and out-of-
order processing. Connections are either reused, with multiple
queries per TCP/TLS connection (p-TCP/p-TLS), or no reuse,
where the connection is reopened for each query. We repeat
the experiment 10 times and report combined results.
Cold-Cache Performance: Figure 5 shows the results of
these experiments. We see that UDP, TCP, and TLS perfor-
mance is generally similar when other parameters are held
consistent (compare (a), (b), and (c), or (g), (h), and (i)). Even
when the RTT is 35 ms, the recursive query process still dom-
inates protocol choice and setup costs are moderate. The data
shows that out-of-order processing is essential when pipelining
is used; case (f) shows head-of-line blocking compared to (h).
This case shows that while current servers support TCP, our
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:02:44 UTC from IEEE Xplore.  Restrictions apply. 
Stub to recursive RTT:
Left: RTT=1ms
Right: RTT=35ms
41144
34670
22256
10312
2353
(different scale)
(a)
(b)
(c)
(d)
(e)
47275
36316
22222
10663
4072
...
...
(f)
(g)
(h)
(i)
)
s
T
T
R
(
e
m
i
t
y
r
e
u
q
r
e
p
n
a
d
e
m
i
)
s
m
(
e
m
i
t
y
r
e
u
q
r
e
p
 2000
 1600
 1200
 800
 400
(c)
(b)
(a)
(d)
(e)
(f)
(g)
5
4
3
2
1
 200
)
s
m
 150
(
e
m
i
t
y
r
e
u
q
 100
(h)
(i)
r
e
p
n
a
d
e
m
i
 50
 0
UDP TCP TLS
p-TCP p-TLS
p-TCP
UDP p-TCP p-TLS
connection:
  handshake
  reuse
sending
processing
  full
  full
reuse
stop-and-wait
in-order
--
noreuse
stop-and-wait
in-order
--
reuse
pipeline
out-of-order
Fig. 5: Per-query response times with a cold cache for 140
unique names with different protocol conﬁgurations and two
stub-to-recursive RTTs (1 ms and 35 ms). Boxes show median
and quartiles. Case (f) uses a different scale.
  full
reuse
pipeline
in-order
  full
UDP TCP TLS TCP
p-TCPp-TLS
UDP p-TCPp-TLS
--
fastopen
  full
noreuse
stop-and-wait
in-order
connection:
  handshake
  reuse
sending
processing
Fig. 6: Per-query response times for 140 repeated queries
with different protocols, measured in RTTs (left axis) and ms
(right). (Medians; boxes add quartiles.)
  full
reuse
stop-and-wait
in-order
reuse
pipeline
out-of-order
   full
--
optimizations are necessary for high performance. Pipelining
shows higher latency than stop-and-wait regardless of protocol
(compare (g) with (a) or (i) with (c)). This difference occurs
when 140 simultaneous queries necessarily queue at the server
when the batch begins; UDP is nearly equally affected as TCP
and TLS (compare (i) and (h) with (g)). Finally, we see that
the costs of TLS are minimal here: comparing (c) with (b)
and (a) or (i) with (g) and (h), natural variation dominates
performance differences.
Warm-Cache Performance: Cold-cache performance is
dominated by communication time to authoritative name
servers. For queries where replies are already cached this
communication is omitted and connection setup times become
noticeable. For connection handling, performance of cache
hits are equivalent to authoritative replies, so our recursive-
to-authoritative experiments in § VI-C represent warm-cache
performance with 100% cache hits. (We veriﬁed this claim by
repeating our stub-to-recursive experiment, but making each
query twice and reporting performance only for the second
query that will always be answered from the cache.) While
cache hits are expensive when they must start new connections,
persistent connections completely eliminate this overhead (Fig-
ure 6, cases (e) and (f) compared to (a)). In addition, median
TCP out-of-order pipelined connections (cases (h) and (i))
are slightly faster than UDP (case (g)) because TCP groups
multiple queries into a single packet.
We conclude that protocol choice makes little performance
difference between stub and recursive provided RTT is small
and connections is not huge and connection reuse is possible.
This result is always true with cold caches, where connection
setup is dwarfed by communication time to authoritative
name servers. This result applies to warm caches provided
connections can be often reused or restarted quickly. We know
that connections can be reused most of the time (§ IV-C), and
TCP fast open and TLS resumption can reduce costs when
they are not reused.
VI-C Latency: Recursive to Authoritative
We next consider performance between recursive resolvers
and authoritative name servers. While recursives are usually
near stubs, authoritative servers are globally distributed with
larger and more diverse RTTs.
typical
Typical RTTs: To measure
recursive-to-
top-1000
authoritative RTTs, we use both the Alexa
sites, and for diversity, a random sample of 1000 sites from
Alexa top-1M sites. We query each from four locations: the
U.S. (Los Angeles), China (Beijing), the U.K. (Cambridge),
and Australia (Melbourne). We query each domain name
iteratively and report the time fetching the last component,
taking the median of 10 trials to be robust to competing
trafﬁc and name server replication. We measure query time
for the last component to represent caching of higher layers.
The U.S. and U.K. sites are close to many authoritative
servers, with median RTT of 45 ms, but a fairly long tail with
35% of RTTs exceeding 100 ms. Asian and Australian sites
have generally longer RTTs, with only 30% closer than 100 ms
(China), and 20% closer than 30 ms (Australia), while the rest
are 150 ms or more. This jump is due to the long propagation
latency for services without sites physically in these countries.
(We provide full data in our technical report [86].)
Methodology: To evaluate query latencies with larger RTTs
between client and server, we set up a DNS authoritative server
(BIND-9.9.3) for an experimental domain (example.com) and
query it from a client 35 ms (8 router hops on a symmetric
path) away. Since performance is dominated by round trips and
not computation we measure latency in units of RTT and these
results generalize to other RTTs. For each protocol, we query
this name server directly, 140 times, varying the protocol in
use. As before, we repeat this experiment 10 times and report
medians of all combined experiments (Figure 6). Variation is
usually tiny, so standard deviations are omitted except for cases
(h) and (i).
Performance: Figure 6 shows the results of this experiment.
181181
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:02:44 UTC from IEEE Xplore.  Restrictions apply. 
We ﬁrst conﬁrm that performance is dominated by protocol
exchanges: cases (a), (b) and (c) correspond exactly to 1, 2,
and 5 RTTs as predicted. Second, we see the importance of
connection reuse or caching: cases (e) and (f) with reuse have
identical performance to UDP, as does TCP fast open (case
(d)).
As before, pipelining for TCP shows a higher cost because
the 140 queries queue behind each other. Examination of
packet traces for cases (h) and (i) shows that about 10% of
queries complete in about 1 RTT, while additional responses
arrive in batches of around 12, showing stair-stepped latency.
For this special case of more than 100 queries arriving
simultaneously, a single connection adds some latency.
We next consider the cost of adding TLS for privacy. The
community generally considers aggregation at the recursive
resolver sufﬁcient for anonymity, but TLS may be desired
there for additional privacy or as a policy [27] so we consider
it as an option. Without connection reuse, a full TLS query
always requires 5 RTTs (case (c), 175 ms): the TCP handshake,
the DNS-over-TLS negotiation (§ III-B2), two for the TLS
handshake, and the private query and response.
However, once established TLS performance is identical to
UDP: cases (f) and (a) both take 1 RTT. Encryption’s cost is
tiny compared to moderate round-trip delays when we have
an established connection. We expect similar results with TLS
resumption.
Finally, when we add pipelining and out-of-order process-
ing, we see similar behavior as with TCP, again due to how
the large, batched queries become synchronized over a single
connection.
We conclude that RTTs completely dominate recursive-to-
authoritative query latency. We show that connection reuse
can eliminate connection setup RTT, and we expect TLS
resumption will be as effective as TCP fast-open. We show
that TCP is viable from recursive-to-authoritative, and TLS is
also possible.
VI-D Client connection-hit fractions
Connection reuse is important and § IV-C found very high
reuse from the server’s perspective. We next show that client
connection-hit fractions are lower because many clients query
infrequently.
To evaluate client connection hit fractions, we replay our
three DNS traces through the simulator from § IV-C, but we
evaluate connection hit fractions per client. Figure 8 shows
these results, with medians (lines) and quartiles (bars, with
slight offset to avoid overlap).
Among the three traces, the DNSChanger hit fraction ex-
ceeds Level 3, which exceeds B-Root, because servers further
up the hierarchy see less trafﬁc from any given client. We
see that the top quartile of clients have high connection hit
fractions for all traces (at 60 s: 95% for DNSChanger, 91% for
Level 3, and 67% for B-Root). The connection hit rate for the
median client is still fairly high for DNSChanger and Level 3
(89% and 72%), but quite low for B-Root (28%). Since most
B-Root content can be cached, many clients only contact it
infrequently and so fail to ﬁnd an open connection.
These results suggest that clients making few requests will
need to restart connections frequently. Fortunately TCP Fast
Open and TLS Resumption allow these clients to carry the
state needed to accelerate this process.
VI-E Modeling End-to-End Latency for Clients
With this data we can now model the expected end-to-end
latency for DNS users and explore how stub, recursive and
authoritative resolvers interact with different protocols and
caching. Our experiments and measurements provide parame-
ters and focus modeling on connection setup (both latency and
CPU costs). Our model captures clients restarting connections,
servers timing out state, and the complex interaction of stub,
recursive, and authoritative resolvers. Our modeling has two
limitations. First, we focus on typical latency for users, per-
query; the modeling reﬂects query frequency, emphasizing
DNS provisioning for common queries and reﬂecting queries
to rare sites only in proportion to their appearance in our
traces. We do not evaluate mean latency per-site, since that
would be skewed by rarely used and poorly provisioned sites.
Second, our models provide mean performance; they cannot
directly provide a full distribution of response times and “tail”
performance [19]. We are interested in using trace replay to
determine a full distribution with production-quality servers,
but as signiﬁcant future work.
Modeling: We model latency from client to server, Lcσ,
cσ) and the cost of
cσ) added to the the cost of the
as the probability of connection reuse (P C
setting up a new connection (SC
actual query (Qcσ):
Lcσ = (1 − P C
cσ)SC
cσ + Qcσ
(1)
From Figure 6, Qcσ is the same for all methods with an
open connection: about one client-server RTT, or Rcσ. Setup
cost for UDP (SC,udp
) is 0. With the probability for TCP fast-
cσ
open (TFO), P T F O
cσ
, TCP setup costs:
cσ = (1 − P T F O
SC,tcp
cσ
)Rcσ
(2)