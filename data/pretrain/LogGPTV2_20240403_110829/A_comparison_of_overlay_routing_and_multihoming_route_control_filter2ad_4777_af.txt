99.902
99.918
Table 4: Availability across router classes: Estimated availabil-
ity for routers or links classiﬁed by AS tier and location. We
consider a border router as one with at least one link to an-
other AS.
To apply the availability statistics derived from the RON data
set, we identiﬁed and classiﬁed the routers on paths between nodes
6The dataset only included a single successful traceroute per day.
Therefore, we assumed that all active probes took the same route
each day.
in our testbed. We performed traceroute measurements approxi-
mately every 20 minutes between nodes in our CDN testbed from
December 4, 2003 to Dec 11, 2003. For our analysis we used the
most often observed path between each pair of nodes; in almost all
cases, this path was used more than 95% of the time. Using the
router availabilities estimated from the RON data set, we estimate
the availability of routes in our testbed when we use route control
or overlay routing. When estimating the simultaneous failure prob-
ability of multiple paths, it is important to identify which routers
are shared among the paths so that failures on those paths are accu-
rately correlated. Because determining router aliases was difﬁcult
on some paths in our testbed,7 we conservatively assumed that the
routers at the end of paths toward the same destination were identi-
cal if they belonged to the same sequence of ASes. For example, if
we had two router-level paths destined for a common node that map
to the ASes A A B B C C and D D D B C C, respectively, we
assume the last 3 routers are the same (since B C C is common).
Even if in reality these routers are different, failures at these routers
are still likely to be correlated. The same heuristic was used to
identify identical routers on paths originating from the same source
node. We assume other failures are independent.
A few aspects of this approach may introduce biases in our anal-
ysis. First, the routes on RON paths may not be representative of
the routes in our testbed, though we tried to ensure similarity by us-
ing only using paths between relatively well-connected RON nodes
in the U.S. In addition, we observed that the availabilities across
router classes in the RON dataset did not vary substantially across
different months, so we do not believe the difference in timeframes
impacted our results. Second, there may be routers or links in the
RON data set that fail frequently and bias the availability of a par-
ticular router type. However, since traceroutes are initiated only
when a failure is detected, there is no way for us to accurately es-
timate the overall failure rates of all individual routers. Third, it
is questionable whether we should assign failures to the last reach-
able router in a traceroute; it is possible that the next (unknown)
or an even further router in the path is actually the one that failed.
Nevertheless, our availabilities still estimate how often failures are
observed at or just after a router of a given type.
Figure 14 compares the average availability using overlays and
route control on paths originating from 6 cities to all destinations in
our testbed. For overlay routing, we only calculate the availability
of the paths for the ﬁrst and last overlay hop (since these will be
the same no matter which intermediate hops are used), and assume
that there is always an available path between other intermediate
hops. An ideal overlay has a practically unlimited number of path
choices, and can avoid a large number of failures in the middle of
the network.
As expected from our active measurements, the average avail-
ability along the paths in our testbed are relatively high, even for
direct paths. 3-multihoming improves the average availability by
0.15-0.24% in all the cities (corresponding to about 13-21 more
hours of availability each year). Here, the availability is primarily
upper bounded by the availability of the routers or links immedi-
ately before the destination that are shared by all three paths as
they converge.
In most cases, 1-overlays have slightly higher availability (at
most about 0.07%). Since a 1-overlay has arbitrary ﬂexibility in
choosing intermediate hops, only about 2.7 routers are common
(on average) between all possible overlay paths, compared to about
4.2 in the 3-multihoming case. However, note that a 1-overlay path
using a single provider is more vulnerable to access link failures
7We found that several ISPs block responses to UDP probe packets
used by IP alias resolution tools such as Ally [29]
)
e
g
a
t
n
e
c
r
e
p
(
y
t
i
l
i
b
a
l
i
a
v
A
e
g
a
r
e
v
A
100
99.95
99.9
99.85
99.8
99.75
99.7
99.65
99.6
No multihoming
3-multihoming
1-overlay
3-overlay
Chicago Wash. D.C. Los Angeles New York
Bay Area
Seattle
City
Figure 14: Availability comparison: Comparison of availability
averaged across paths originating from six cities using a single
provider, 3-multihoming, 1-overlays, and 3-overlays. ISPs are
chosen based on their round-trip time performance.
than when multihoming is employed. For example, the low avail-
ability of the 1-overlay in Chicago is due to: (1) the chosen ISP
(based on RTT performance) is a tier 4 network, which has internal
routers with relatively lower availability, and (2) all paths exiting
that provider have the ﬁrst 5 hops in common and hence have a high
chance of correlated failures. Finally, we see that using a 3-overlay
usually makes routes only slightly more available than when using
a 1-overlay (between 0.01% to 0.08%, excluding Chicago) . This is
because at least one router is shared by all paths approaching a des-
tination, so failures at that router impact all possible overlay paths.
In summary, it is interesting to note that despite the greater ﬂexi-
bility of overlays, route control with 3-multihoming is still able to
achieve an estimated availability within 0.08-0.10% (or about 7 to
9 hours each year) of 3-overlay.
7. DISCUSSION
Next, we discuss observations made from our measurements and
other fundamental tradeoffs between overlay routing and multi-
homing route control that are difﬁcult to assess. We also comment
on the limitations of our study.
Key observations. As expected, our results show that overlay rout-
ing does provide improved latency, throughput, and reliability over
route control with multihoming. We found that overlay routing’s
performance gains arise primarily from the ability to ﬁnd routes
that are physically shorter (i.e. shorter propagation delay). In ad-
dition, its reliability advantages stem from having at its disposal a
superset of the routes available to standard routing. The surprise in
our results is that, while past studies of overlay routing have shown
this advantage to be large, we found that careful use of a few ad-
ditional routes via multihoming at the end-network was enough to
signiﬁcantly reduce the advantage of overlays. Since their perfor-
mance is similar, the question remains whether overlays or multi-
homing is the better choice. To answer this, we must look at other
factors such as cost and deployment issues.
Cost of operation. Unfortunately, it was difﬁcult to consider the
cost of implementing route control or overlays in our evaluation. In
the case of multihoming, a stub network must pay for connectivity
to a set of different ISPs. We note that different ISPs charge differ-
ent amounts and therefore the solution we consider “best” may not
be the most cost-effective choice. In the case of overlays, we envi-
sion that there will be overlay service offerings, similar to Akamai’s
SureRoute [1]. Users of overlays with multiple ﬁrst hop choices (k-
overlay routing in our analysis) must add the cost of subscribing to
the overlay service to the base cost of ISP multihoming.8 Using
an overlay with a single provider (i.e., 1-overlays) would eliminate
this additional cost, but our analysis shows that the performance
gain is reduced signiﬁcantly.
Deployment and operational overhead. Overlays and multihom-
ing each have their unique set of deployment and performance chal-
lenges that our measurements do not highlight. Below, we consider
the issues of ease of use and deployment, routing table expansion
and routing policy violations.
Ease of use and employment. Overlay routing requires a third-party
to deploy a potentially large overlay network infrastructure. Build-
ing overlays of sufﬁcient size and distribution to achieve signif-
icantly improved round-trip and throughput performance is chal-
lenging in terms of infrastructure and bandwidth cost, as well as
management complexity. On the other hand, since multihoming is
a single end-point based solution, it is relatively easier to deploy
and use from an end-network’s perspective.
Routing table expansion due to multihoming. An important over-
head of multihoming that we did not consider in this study is the
resulting increase in the number of routing table entries in back-
bone routers. ISPs will likely charge multihomed customers appro-
priately for any increased overhead in the network core, thus mak-
ing multihoming less desirable. However, this problem occurs only
when the stub network announces the same address range to each of
its providers. Since ISPs often limit how small advertised address
blocks can be, this approach makes sense for large and medium
sized stub networks, but is more difﬁcult for smaller ones. Smaller
networks could instead use techniques based on network address
translation (NAT) to avoid issues with routing announcements and
still make intelligent use of multiple upstream ISPs [13, 4].
Violation of policies by overlay paths. One of the concerns that
overlay routing raises is its circumvention of routing policies insti-
tuted by intermediate ASes. For example, a commercial endpoint
could route data across the relatively well-provisioned, academic
Internet2 backbone by using an overlay hop at a nearby university.
While each individual overlay hop would not violate any policies
(i.e., the nearby university node is clearly allowed to transmit data
across Internet2), the end-to-end policy may be violated. While our
analysis quantiﬁes the number of routing policy violations, we did
not consider their impact. Most Internet routing polices are related
to commercial relationships between service providers. Therefore,
it is reasonable to expect that the presence of an overlay node in
an ISP network implies that the overlay provider and the ISP have
some form of business agreement. This relationship should require
that the overlay provider pay for additional expenses that the ISP
incurs by providing transit to overlay trafﬁc. Network providers
would thus be compensated for most policy violations, limiting the
negative impact of overlay routing.
Future changes to BGP. Thus far, we have discussed some im-
portant issues regarding overlays and route control in today’s en-
vironment, but have not considered changes to BGP that may fur-
ther improve standard Internet routing performance relative to over-
lays. For example, we only consider the impact of performance or
availability-based route selection at the edge of the network. It is
possible that transit ASes could perform similar route control in
the future, thereby, exposing a superior set of AS paths to end net-
works. Another future direction is the development of new proto-
8If the ISPs charge according to usage, then the cost of employing
multiple ISP connections in the case of k-overlays may be higher
or lower than the cost of using multiple connections in the case of
k-multihoming.
cols for AS-level source-routing, such as NIRA [33], which allow
stub networks greater control over their routes.
Limitations of the study. Our observations may be constrained by
a few factors such as the size of our testbed, the coarse granularity
of our performance samples, and our limited analysis of resilience.
We discuss these issues in detail below.
)
s
m
(
i
g
n
m
o
h
i
t
l
u
m
r
e
v
o
t
n
e
m
e
v
o
r
p
m
I
4
3.5
3
2.5
2
1.5
1
0.5
0
0
performance difference
10
20
30
40
50
60
Number of overlay nodes
Figure 15: Impact of overlay network size on round-trip per-
formance: This graph shows the mean difference between 3-
overlays and 3-multihoming as overlay nodes are added.
In Figure 15 we compare the average RTT perfor-
Testbed size.
mance from 3-multihoming against 3-overlays, as a function of the
number of intermediate overlay nodes available. The graph shows
the RTT difference between the best 3-overlay path (direct or indi-
rect) and best 3-multihoming path, averaged across all measure-
ments as nodes are added one-by-one, randomly, to the overlay
network. A different heuristic of adding nodes may yield differ-
ent results. As the size of the overlay is increased, the performance
of 3-overlays gets better relative to multihoming. Although the rel-
ative improvement is marginal, there is no discernible “knee” in the
graph. Therefore it is possible that considering additional overlay
nodes may alter the observations in our study in favor of overlay
routing.
Granularity of performance samples. Our performance samples are
collected at fairly coarse timescales (6 minutes intervals for round-
trip time and 30 minutes for throughput). As a result, our results
may not capture very ﬁne-grained changes, if any, in the perfor-
mance on the paths, and their effect on either overlay routing or
multihoming route control. However, we believe that our results
capture much of observable performance differences between the
two path selection techniques for two key reasons: (1) our conclu-
sions are based on data collected continuously over a week-long
period, and across a fairly large set of paths, and (2) Zhang et al.
observed that the “steadiness” of both round-trip time and through-
put performance is at least on the order of minutes [34]. Other more
recent measurements of round-trip times on similar paths as those
in our testbed have shown mean intervals of several minutes be-
tween changes of 30% or more [4]. As such, we do not expect that
a higher sampling frequency would yield signiﬁcantly different.
Repair and failure detection. Our reliability analysis does not com-
pare the relative ability of overlay routing and multihoming to avoid
BGP convergence problems. For example, a peering link failure
may affect routing between the peer ISPs until BGP re-converges.
It is possible that some multihoming conﬁgurations cannot avoid
such routing failures. We leave this comparison for future work.
8. SUMMARY
Past studies have demonstrated the use of overlay routing to make
better use of the underlying connectivity of the Internet than the
current BGP-based system. However, BGP-based routing can ben-
eﬁt from the added capability of two important factors at end-networks:
(1) additional access to end-to-end BGP routes via ISP multihom-
ing, and (2) implementation of performance- and resilience-aware
route control mechanisms to dynamically select among multiple
BGP routes. In this paper, we have compared the relative bene-
ﬁts of overlay routing and intelligent route control and investigated
possible reasons for the differences via an extensive measurement-
based analysis. Our ﬁndings are as follows:
(cid:15) Multihoming route control can offer performance similar to
overlay routing. Speciﬁcally, overlays employed in conjunc-
tion with multihoming to 3 ISPs offer only about 5-15% bet-
ter RTTs and 1–10% better throughput than route control in
conjunction with multihoming to three ISPs. In fact, when
overlays are constrained to a single ﬁrst-hop ISP, they pro-
vide inferior performance relative to route control.
(cid:15) The marginally better RTT performance of overlays comes
primarily from their ability to select shorter end-to-end routes.
Also, the performance gap between overlays and route con-
trol can be further reduced if, for example, ISPs implement
mutually cooperative peering policies such as late-exit.
(cid:15) While route control cannot offer the near perfect resilience of
overlays, it can eliminate almost all observed failures on end-
to-end paths. The path diversity offered by multihoming can
improve fault tolerance of end-to-end paths by two orders of
magnitude relative to the direct BGP path.
The results in our paper show that it is not necessary to circum-
vent BGP routing to achieve good end-to-end resilience and perfor-
mance. These goals can be effectively realized by means of multi-
homing coupled with intelligent route control.
Acknowledgment
We would like to thank Olaf Maennel, Roberto De Prisco, Ramesh
Sitaraman and Ravi Sundaram for their support and assistance with
our experiments and data collection. We would also like to thank
Nick Feamster and David Andersen for providing the RON failure
data. Discussions and feedback from the following people have
helped improve this work greatly: David Andersen, Hari Balakr-
ishnan, Claudson Bornstein, Nick Feamster, Erich Nahum, Venkat
Padmanabhan, Jennifer Rexford, Sambit Sahu and Hui Zhang. Fi-
nally, we thank our shepherd, Tom Anderson, and our anonymous
reviewers for their valuable feedback and suggestions.
9. REFERENCES
[1] Akamai Technologies. Akarouting (SureRoute).
http://www.akamai.com, June 2001.
[2] Akamai Technologies. Edgescape.
http://www.akamai.com/en/html/services/
edgescape.html, 2004.
[3] A. Akella, B. Maggs, S. Seshan, A. Shaikh, and
R. Sitaraman. A Measurement-Based Analysis of
Multihoming. In Proc. of ACM SIGCOMM ’03, Karlsruhe,
Germany, August 2003.
[4] A. Akella, S. Seshan, and A. Shaikh. Multihoming
Performance Beneﬁts: An Experimental Evaluation of
Practical Enterprise Strategies. In Proc. of the USENIX 2004
Annual Technical Conference, Boston, MA, June 2004.
[5] L. Amini, A. Shaikh, and H. Schulzrinne. Issues with
Inferring Internet Topological Attributes. In Proceedings of
SPIE ITCOM, August 2002.
[6] D. Andersen, H. Balakrishnan, M. Kaashoek, and R. Morris.
Resilient Overlay Networks. In Proc. of the 18th Symposium
on Operating System Principles, Banff, Canada, October
2001.
[7] N. Cardwell, S. Savage, and T. Anderson. Modeling TCP
Latency. In Proc. of IEEE INFOCOM 2000, Tel Aviv, Israel,
March 2000.
[8] F5 Networks. BIG-IP link controller. http://www.f5.
com/f5products/bigip/LinkController/.
[9] N. Feamster, D. Andersen, H. Balakrishnan, and M. F.
Kaashoek. Measuring the Effects of Internet Path Faults on
Reactive Routing. In Proc. of ACM SIGMETRICS 2003, June
2003.
[10] N. Feamster, J. Borkenhagen, and J. Rexford. Guidelines for
Interdomain Trafﬁc Engineering. ACM SIGCOMM
Computer Communication Review, October 2003.
[11] L. Gao. On Inferring Autonomous System Relationships in
the Internet. IEEE/ACM Transactions on Networking, 9(6),
December 2001.
[12] L. Gao and F. Wang. The Extent of AS Path Inﬂation by
Routing Policies. In Proc. of IEEE GLOBECOM 2002, pages
2180–2184, 2002.
[13] F. Guo, J. Chen, W. Li, and T. Chiueh. Experiences in
Building a Multihoming Load Balancing System. In
Proceedings of IEEE INFOCOM, Hong Kong, March 2004.
to appear.
[14] Y. Hyun, A. Broido, and k claffy. Traceroute and BGP AS
Path Incongruities. Technical report, CAIDA, University of
California, San Diego, 2003. http://www.caida.org/
outreach/papers/2003/ASP/.
[15] IETF Trafﬁc Engineering Working Group. http://www.
ietf.org/html.charters/tewg-charter.html,
2000.
[16] C. Labovitz, A. Ahuja, A. Bose, and F. Jahanian. Delayed
Internet routing convergence. IEEE/ACM Transactions on
Networking, 9(3):293–306, June 2001.
[17] Z. Mao, R. Govindan, G. Varghese, and R. Katz. Route Flap
Damping Exacerbates Internet Routing Convergence. In
Proc. of ACM SIGCOMM ’03, Karlsruhe, Germany, August
2003.
[18] Z. Mao, J. Rexford, J. Wang, and R. Katz. Towards an
Accurate AS-Level Traceroute Tool. In Proc. of ACM
SIGCOMM ’03, Karlsruhe, Germany, August 2003.
[19] Nortel Networks. Alteon link optimizer.
http://www.nortelnetworks.com/products/
01/alteon/optimizer/.
[20] W. B. Norton. Internet Service Providers and Peering. In
Proceedings of NANOG 19, Albuquerque, NM, June 2000.
[21] radware. Peer Director. http:
//www.radware.com/content/products/pd/.
[22] Y. Rekhter and T. Li. A Border Gateway Protocol 4 (BGP-4).
Internet Request for Comments (RFC 1771), March 1995.
[23] M. Roughan, M. Thorup, and Y. Zhang. Trafﬁc Engineering
with Estimated Trafﬁc Matrices. In Internet Measurement
Conference, Miami, FL, November 2003.
[24] RouteScience Technologies, Inc. Routescience PathControl.
http://www.routescience.com/products.
[25] S. Savage, A. Collins, E. Hoffman, J. Snell, and T. Anderson.
The End-to-End Effects of Internet Path Selection. In
Proceedings of ACM SIGCOMM, Boston, MA, September
1999.
[26] S. Savage et al. Detour: A Case for Informed Internet
Routing and Transport. IEEE Micro, 19(1):50–59, 1999.
[27] A. Shaikh, J. Rexford, and K. G. Shin. Load-Sensitive
Routing of Long-Lived IP Flows. In Proc. of ACM
SIGCOMM ’99, Cambridge, MA, September 1999.
[28] N. Spring, R. Mahajan, and T. Anderson. Quantifying the
Causes of Internet Path Inﬂation. In Proc. of ACM
SIGCOMM ’03, August 2003.
[29] N. Spring, R. Mahajan, and D. Wetherall. Measuring ISP
Topologies with Rocketfuel. In Proc. of ACM SIGCOMM
’02, Pittsburgh, PA, August 2002.
[30] J. W. Stewart. BGP4: Inter-Domain Routing in the Internet.
Addison-Wesley, 1999.
[31] L. Subramanian, S. Agarwal, J. Rexford, and R. H. Katz.
Characterizing the Internet Hierarchy from Multiple Vantage
Points. In Proceedings of IEEE INFOCOM, June 2002.
[32] H. Tangmunarunkit, R. Govindan, and S. Shenker. Internet
Path Inﬂation Due to Policy Routing. In SPIE ITCOM,
August 2001.
[33] X. Yang. NIRA: A New Internet Routing Architecture. In
Proc. of the ACM SIGCOMM Workshop on Future
Directions in Network Architecture (FDNA), August 2003.
[34] Y. Zhang, N. Dufﬁeld, V. Paxson, and S. Shenker. On the
Constancy of Internet Path Properties. In Proc. of ACM
SIGCOMM Internet Measurement Workshop (IMW),
November 2001.