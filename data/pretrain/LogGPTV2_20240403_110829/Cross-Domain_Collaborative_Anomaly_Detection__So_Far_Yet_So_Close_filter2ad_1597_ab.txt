### 3.4 Alert Exchange

We build upon the initial work of Worminator [15], an alert exchange system, and extend it to meet the specific needs of our system. Each site runs a content exchange client that receives content alerts and abnormal models. We use a common format to ensure that any Cross-domain Anomaly Detection (CAD) sensor can be easily adapted to work with the exchange system. When a local STAND/Anagram sensor at a site detects an abnormal packet, it sends the content alert packet to the Worminator client. The Worminator client then encodes the content alerts as Bloom filters if they are from a remote site and securely transmits both the content alerts and any abnormal models to the Worminator server over the internet.

The bandwidth usage for this alert exchange is minimal because we only consider GET requests with argument strings and further narrow down the content by exchanging only the abnormal content alerts. On average, each alert encoded in a Bloom filter requires about 2KB to transmit. For our eight-week experiment, this translates to an average of 0.9Kb/sec of bandwidth per site, which is well within the limits for real-time systems and allows for scaling up to a large set of collaborators without running into bandwidth constraints.

A back-end process on the server performs the correlation of content alerts by comparing the local unencoded alerts to the Bloom filter representation of alerts from remote sites. Our experiments demonstrate that this process can be performed faster than real time while securely exchanging encoded content alerts over the internet.

### 3.5 Scaling to Multiple Sites

Our initial system deployment consists of three web servers. To enhance the system's effectiveness in quickly detecting widespread attacks, we envision a larger-scale deployment with many collaborating sensors monitoring servers in different locations on the internet. For the system to scale up to include more sites, the correlation and alert comparison process must also scale accordingly.

If we consider the pair-wise comparison of local alerts with each remote alert, the computational complexity appears to grow asymptotically: O(nÂ²). However, we can bound the total computation under a constant K by varying the amount of time duplicate alerts are stored in the system. In practice, we did not observe significant problems even when keeping eight weeks of data for correlation, as indexing of alerts can be done using computationally efficient algorithms. Additionally, we only operate on unique alerts, which are much smaller in size.

If a longer time frame is desirable, we can compress the remote site alerts into a small number of Bloom filters by trading off some accuracy, turning the scaling into order O(n). This allows many more alerts to be stored before running into scaling issues. Each time a new site joins the collaboration, our local site must compare its alerts to the Bloom filters from the new site. Therefore, the overall computational complexity scales linearly with the number of remote sites participating. Based on our numbers, even with an unoptimized prototype, we could scale to around 100 similar servers operating in real time and comparing all alerts over a few weeks' time.

### 4 Model Comparison

Each normal model is a Bloom filter containing all the n-grams of all normalized requests. By comparing Bloom filters as bit-arrays, we can estimate the shared content between models. We calculate the percentage of set bits in common by dividing the number of set bits each pair of models has in common by the total number of set bits. The generated Bloom filters are quite sparse, so the overlap of bits between content should be small, as observed in Figure 4.

We used this model comparison metric to compute the distinctness of servers and changes in normal flows over time, whether servers in the same domain share additional commonality, and how much abnormal data is shared across servers. For normal models in our eight-week run, we found an average of 3.00% of bits set in common. Comparing models at the same site, we found an average of over 40% of bits in common (Table 1). Some overlap indicates that not filtering out normal content before performing content alert correlation could lead to increased false positives.

Models across long periods seem to maintain a core of commonality but differ more than models close together in time. Even with only five weeks of difference in our datasets, this gradual change is evident. Averaged over eight weeks, both sites keep over 40% of bits in common, while in the three-week run, this is closer to 50%. This reinforces existing work [5] showing that traffic patterns evolve over time, indicating that periodically updating normal models should increase effectiveness.

### 5 Correlation Results

The correlation process compares each unique content alert from the local sensors against the Bloom filter representation of each unique content alert from other sites. If at least 80% of the n-grams match the Bloom filter and the length of the content before encoding (which is also exchanged) is within 80% of the raw content, we note it as a match. These matches are identified as attacks. Once these attacks are identified, the Bloom filter representation can be sent to additional participating servers, and future occurrences can be blocked.

To confirm our correlation results with the Bloom filters, we also perform an offline correlation of results using string edit distance [13] with a threshold of two changes per ten characters. We cluster together any pair of alerts from either site with less than this threshold. If a cluster contains alerts from more than one site, it represents a common content alert. These methods provide similar performance, confirming that using privacy-preserving Bloom filters offers accurate and computationally efficient correlation.

To simulate a production deployment, we use the Bloom filter comparison as our default correlation technique and use the string edit distance clustering only for manual inspection as needed, especially at a single site. Table 3 shows examples of true negatives where legitimate requests are not alerted on since each is seen at just one site. Table 4 provides an example of abnormal data from each site that match.