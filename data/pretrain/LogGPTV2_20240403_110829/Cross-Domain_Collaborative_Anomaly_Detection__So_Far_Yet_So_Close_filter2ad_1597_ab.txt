in [6] including the abnormal model exchange to exclude repeated attacks from
poisoning the training data. The Anagram normal models are, as described in
[30], Bloom ﬁlters [3] containing the n-gram representation of packets voted as
normal by the STAND micro-models. A Bloom ﬁlter is a one-way data structure
where an item is added by taking multiple hashes and setting those indices of a
bit array to one. This provides space eﬃciency and incredible speed suitable for
high speed networks since adding an element or checking if one is already present
are constant time operations. Each normalized content is spilt into 5-gram sec-
tions as in [5] using a sliding window of ﬁve characters. See Figure 3(a) for an
example. Requests can then be easily tested as to how many of the n-grams
from their argument string are present in the model. N-grams give us a granu-
lar view of content allowing partial matches as opposed to hashing full content
while maintaining enough structure of the content to be much more accurate
than character frequency models. Previous work [15] calibrated Bloom ﬁlters
to have an almost non-existent false positive rate and shows that extracting the
content is infeasible, which allows for the preservation of privacy. The models
we use are 228 bits long and compress to about 10-80KB, a size that is easily
exchanged as needed. The Anagram models test weather new content is similar
to previous content by comparing how many of the n-grams exist in the model
already.
3.4 Alert Exchange
We leverage the initial work of Worminator [15], an alert exchange system that
we heavily extend to meet the needs of our system. A content exchange client
Cross-domain Collaborative Anomaly Detection: So Far Yet So Close
7
(a) A string broken into n-grams.
(b) N-grams added to the Bloom Filter.
Fig. 3. n-grams and Bloom Filter
instance runs at each site and receives content alerts and abnormal models. We
use a common format so that any CAD sensor can easily be adapted to work with
the exchange system. In our case, each site’s local STAND/Anagram sensor sends
the content alert packet to the Worminator client as soon as it tests a packet
and ﬁnds it abnormal. The Worminator client then encodes the content alerts
as Bloom ﬁlters if at a remote site and then sends the content alerts and any
abnormal models through a secure channel over the internet to the Worminator
server. The bandwidth usage with this alert exchange turns out to be minimal
since we only look at GET requests with argument strings and then further
narrow down content by only exchanging the abnormal content alerts. It turns
out that each alert encoded in a Bloom ﬁlter takes around 2KB to transmit
on average. For our eight week experiment this translates into an average of
0.9Kb/sec bandwidth needed per site for a real time system, leaving plenty of
room to scale up to a large set of collaborators before running into bandwidth
constraints. A back-end process on the server performs the correlation of content
alerts by comparing the local unencoded alerts to the Bloom ﬁlter representation
of alerts from remote sites. We perform all our experiments faster than real time
while exchanging encoded content alerts securely over the internet.
By exchanging content alerts from remote sites only in their Bloom ﬁlter form
our system can protect the privacy of legitimate web requests. During the Bloom
ﬁlter correlation process only the fact that a match occurs can be determined
not any speciﬁc content. If a match occurs then this is a piece of content that a
site has already seen coming to their server, so the only new information revealed
is that the other site also had this content incoming. In this way we can gain
information about the content we have in common, which most likely represents
attacks while keeping the remaining content private in case there is sensitive
information in the web requests.
3.5 Scaling to Multiple Sites
Our initial system deployment consists of three web servers. However, to be
even more eﬀective at quickly detecting widespread attacks, we envision a larger
8
N. Boggs, S. Hiremagalore, A. Stavrou, and S. J. Stolfo
Fig. 4. Each model is stored in a Bloom ﬁlter, we count the number of bits set in
common and then divide by the total number of bits set.
scale system deployment consisting of many collaborating sensors monitoring
servers positioned in diﬀerent locations on the Internet. For the system to scale
up to include more sites, the correlation and alert comparison process has to
scale accordingly. If we consider the pair-wise comparison of local alerts with
each remote alert, it appears to grow asymptotically: O(n2). This could turn
can quickly become a problem; however, we can bound this total computation
under a constant K by varying the amount of time duplicate alerts are stored
in the system. In practice, we did not observe problems during our experiments
even keeping eight weeks of data for correlation because indexing of alerts can
be done using existing computationally eﬃcient algorithm. Moreover, we only
have to operate on unique alerts which are much smaller in size. Additionally,
if a longer time frame is desirable, we can employ compression to the remote
site alerts into a small number of Bloom ﬁlters by trading-oﬀ some accuracy
and turn the scaling into order O(n) allowing many more alerts to be stored
before running into any scaling issues. In that case, each time a new site joins
the collaboration our local site must compare its alerts to the Bloom ﬁlters of
those from the new site. Therefore, the overall computational complexity scales
linearly with the number of remote sites participating. Since we can bound the
local comparison with a remote site under K, the total computational cost scales
linearly as well, and each site has optional tradeoﬀs in time alerts are kept and
Bloom ﬁlter aggregation if local resources are limited. In practice, based on
our numbers even with an unoptimized prototype we could scale to around 100
similar servers operating in real time and comparing all alerts over a few weeks’
time. If additional utility is derived from having even more participating servers,
then optimizing the code, time alerts are kept, and trading oﬀ accuracy in Bloom
ﬁlter aggregation should easily allow additional magnitudes of scaling.
4 Model Comparison
Each normal model is a Bloom ﬁlter with all the n-grams of all normalized
requests. By comparing Bloom ﬁlters as bit-arrays, we are able to estimate how
much content models share. We test how many set bits each pair of models have
in common and divide by the total number of set bits to get a percentage of
set bits in common. The generated Bloom ﬁlters are quite sparse; therefore, the
overlap of bits between content should be small as observed in Figure 4. We used
this model comparison metric to compute the server distinctness and change in
normal ﬂows over time, whether servers in the same domain share additional
commonality, and how much abnormal data we see in common across servers.
Cross-domain Collaborative Anomaly Detection: So Far Yet So Close
9
Fig. 5. Model comparison: the higher the ﬁgure the higher percentage of set bits the
models had in common. The top and bottom quadrant are intra-site comparisons.
The sides represent the comparison across-sites which, as expected, appear to have
diﬀerences.
We ﬁrst use this comparison to observe the diﬀerences in models from distinct
sites with each other. We took every ﬁfth model from our runs and compared the
ones from the same runs to their counter parts at the other location. For normal
models in our eight week run, we see on average 3.00% of bits set in common. We
compare this to the over 40% of bits in common on average comparing models
at the same site (Table 1). There is some overlap indicating that not ﬁltering
out normal content before performing the content alert correlation could lead
to increased false positives. While we do not have enough sites to calculate how
important this distinctness is to the accuracy achieved via correlation of alerts,
we do conﬁrm that at least for the distinct web servers our correlation process
achieves eﬀective results. See Figure 5 for a plot of the model comparison results.
Models across long periods seem to keep a core of commonality but diﬀer more
than models close together in time. A product of this gradual change appears
even with only ﬁve weeks diﬀerence in our datasets. Averaged over eight weeks
both sites keep over 40% of bits in common while in the three week run this
is closer to 50%. This reinforces existing work [5] showing that traﬃc patterns
do evolve over time indicating that updating normal models periodically should
increase eﬀectiveness.
10
N. Boggs, S. Hiremagalore, A. Stavrou, and S. J. Stolfo
Comparison
Columbia CS
GMU Main
Cross site
Normal Abnormal
Models Models
Oct.-Nov. Oct.-Nov.
52.69%
38.51%
10.14%
41.45%
41.82%
3.00%
Table 1. Commonality of normal and abnormal models.
Comparison -
Normal Models
Columbia CS
GMU Main
GMU CS
Comparison -
Abnormal Models
Columbia CS
GMU Main
GMU CS
Columbia CS GMU Main GMU CS
44.89%
3.89%
48.39%
53.05%
9.46%
48.39%
4.08%
2.41%
56.80%
9.32%
8.55%
70.77%
Table 2. Comparison of abnormal and normal models between three sites. (Percentages
of set bits in common shown.)
With our three week data set, we also have an additional web server from
one administrative domain. With two web servers from the same Autonomous
System we compare them to each other to see if our work has the potential to
help a large organization that may have many separate web servers. See Table 2
for empirical details. Interestingly, we ﬁnd no more commonality among normal
models in the same domain than across domains. The fact that abnormal models
at the these web servers share about as much in common with the server from
another domain as each other suggests that attackers likely do not speciﬁcally
target similar IP ranges with the same attacks. This suggests that web server
administration and location may not play a factor in the eﬀectiveness of using a
particular web server for collaboration. An organization with suﬃciently distinct
web servers might be able to achieve good results without having to overcome
the obstacles related to exchanging data between organizations.
The abnormal models from diﬀerent sites show some similarity with close to
10% set bits matching, while models from the same site show more similarity. The
high amount of common abnormal data between models at the same site may
be inﬂuenced by legitimate requests classiﬁed as abnormal. More interesting is
the commonality across sites. These shared bits most likely represent the attack
data that we have found in common. There is an irregularity where some of the
abnormal models are empty and could not be compared. We remove these empty
models before computing the averages to avoid divide by zero issues. Multiple
runs with the data conﬁrm the strange behavior which can be contributed to a
Cross-domain Collaborative Anomaly Detection: So Far Yet So Close
11
cx=:cjygsheid&cof=forid:&ie=utf-&q=machine+learning+seminar&sa=go
o=-&id=&s=uhkf&k=hsbihtpzbrxvgi&c=kg
Table 3. Normalized examples of legitimate abnormal data seen at only one site.
faq=’ and char()+user+char()= and ”=’
id=’ and char()+user+char()= and ”=’
Table 4. Normalized example of abnormal data one from each site that match.
convergence of the STAND micromodels voted to include all the data from that
time period into the normal models leaving the abnormal models empty.
Overall, our model comparisons provide quite interesting results. We ﬁnd that
each site has normal traﬃc ﬂows that are distinct although changing somewhat
over long periods of time. We see no major distinctions in comparison of same
domain servers versus servers on separate domains, which indicates that our
system could be deployed by a suﬃciently large organization to protect itself
without having to rely on outside collaborators. Finally, our measurements of
abnormal data validate the idea that separate servers will receive similar attacks.
5 Correlation Results
The correlation process compares each unique content alert from the local sen-
sors against the Bloom ﬁlter representation of each unique content alert from
other sites. If at least 80% of the n-grams match the Bloom ﬁlter and the length
of content before encoding, which is also exchanged, is within 80% of the raw con-
tent then we note it as a match. These matches are what the system identiﬁes as
attacks. Once these attacks are identiﬁed, the Bloom ﬁlter representation could
be sent out to any additional participating servers and future occurrences could
be blocked. In order to conﬁrm our correlation results with the Bloom ﬁlters, we
also perform an oﬄine correlation of results using string edit distance [13] with
a threshold of two changes per ten characters. We cluster together any pair of
alerts from either site with less than this threshold. If a cluster contains alerts
from more than one site, then it represents a common content alert. With only
minor diﬀerences, these methods give us similar performance conﬁrming that us-
ing privacy preserving Bloom ﬁlters provides an accurate and computationally
eﬃcient correlation. To simulate a production deployment, we use the Bloom
ﬁlter comparison as our default correlation technique and use the string edit
distance clustering only to facilitate manual inspection as needed, especially at
a single site. See Table 3 for examples of true negatives where legitimate requests
are not alerted on since each is seen at just one site. Table 4 shows an example