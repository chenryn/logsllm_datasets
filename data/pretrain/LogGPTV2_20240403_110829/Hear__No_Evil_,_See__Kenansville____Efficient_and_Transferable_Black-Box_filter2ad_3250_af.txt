ASR or AVI systems have unintentionally learned to depend
on these low intensity components for inference. This explains
why removing such insigniﬁcant parts of speech confuses
the model and causes a mistranscription or misidentiﬁcation.
Additionally, this may also explain some portion of the ASR
and AVI error on regular testing data sets. Future work may
use these revelations in order to build more robust models and
be able to explain and reduce ASR and AVI system error.
Although our attack guarantees mistranscription for the
perturbed word, it is still untargeted (i.e., the mistranscription
can be anything). Fortunately, an attacker has some control
over the transcription the ASR outputs. So far, our experiments
focused on determining the least amount of perturbation that
needs to be applied to successfully fool the model. At times,
the transcriptions of the adversarial audio and its benign
counterpart were phonetically similar (Table II). However,
increasing the distortion can decrease the phonetic similarity
of the transcriptions. While running experiments for the DFT
and SSA perturbation methods, we noticed the ASRs would
often mistranscribe the input
the
perturbations would confuse the ASRs into assuming that the
input was simply noise. Furthermore, manual listening tests by
the authors revealed that audio samples were still intelligible.
An attacker attempting to completely evade the ASR could
apply additional distortion to the audio. This can force the
ASR to either output a phonetically dissimilar transcription,
or in the best case, none at all.
to nothing. Speciﬁcally,
F. Audio CAPTCHAs
Additionally, our attack has other applications as well.
Speciﬁcally, in the domain of audio CAPTCHAs. These are
often used by web services to validate the presence of a human.
CAPTCHAs relies on humans being able to transcribe audio
better than machines, an assumption that modern ASR systems
call into question [46], [47], [48], [49], [50]. Our attack could
potentially be used to intelligently distort audio CAPTCHAs
as a countermeasure to modern ASR systems.
VII. RELATED WORK
Machine Learning (ML) models, and in particular deep
learning models, have shown great performance advancements
in previously complex tasks, such as image classiﬁcation [51],
[52], [53] and speech recognition [54], [26], [24]. However,
previous work has shown that ML models are inherently
vulnerable to attacks known as Adversarial ML (AML) [55].
Early AML techniques focused on visually imperceptible
changes to an image that cause the model to incorrectly clas-
sify the image. Such attacks target either speciﬁc pixels [56],
[57], [58], [59], [60], [61], or entire patches of pixels [62],
[63], [64], [65]. In some cases, the attack generates entirely
new images that the model would classify to an adversary’s
chosen target [66], [67].
the attacks assume that
However, the success of these attacks are a result of two
restrictive assumptions. First,
the
underlying target model is a form of a neural network. Second,
they assume the model can be inﬂuenced by changes at the
pixel level [64], [66]. These assumptions prevent image attacks
from being used against ASR models. ASR systems have
found success across a variety of ML architectures, from
Hidden Markov Models (HMMs) to Deep Neural Networks
(DNNs). Further, since audio data is normally preprocessed
for feature extraction before entering the statistical model, the
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:09:39 UTC from IEEE Xplore.  Restrictions apply. 
723
models initially operate at a higher level than the ‘pixel level’
of their image counterparts.
To overcome these limitations, previous works have pro-
posed several new attacks that exploit behaviors of particular
models. These attacks can be categorized into three broad
techniques that generate audio that include: a) inaudible to
the human ear but will be detected by the speech recognition
model [68], b) noisy such that it might sound like noise to
the human, but will be correctly deciphered by the automatic
speech recognition [69], [11], [10], and c) pristine audio
such that the audio sounds normal to the human but will
be deciphered to a different, chosen phrase [3], [70], [71],
[72], [7], [6], [2], [5], [73]. Although they may seem the most
useful, attacks in the third category are limited in their success,
as they often require white-box access to the model.
Attacks against image recognition models are well studied,
giving attackers the ability to execute targeted attacks even
in black-box settings. This has not yet been possible against
speech models [74], even for untargeted attacks in a query
efﬁcient manner. That is, both targeted and untargeted attacks
require knowledge of the model internals (such as architecture
and parameterization) and large number of queries to the
model. In contrast, we propose a query efﬁcient black-box
attack that is able to generate an attack audio sample that
will be reliably mistranscribed by the model, regardless of
architecture or parameterization. Our attack can generate an
attack audio sample in logarithmic time, while leaving the
audio quality mostly unaffected.
VIII. CONCLUSION
ASR and AVI systems are playing an increasingly important
role in security decisions. As such, the robustness of these
systems (and the foundations upon which they are built) must
be rigorously evaluated. We perform such an evaluation in
this paper. By exhibiting black-box attacks against of multiple
models, we demonstrate that such systems rely on audio
features which are not critical to human comprehension and
are therefore vulnerable to mistranscription attacks when such
features are removed. We then show that such attacks can
be efﬁciently conducted as perturbations to certain phonemes
(e.g., vowels) that cause signiﬁcantly greater misclassiﬁcation
to the words that follow them. Finally, we not only demonstrate
that our attacks can work across models, but also show that
the audio generated has no impact on human intelligibility.
This detail is critical, as attacks that simply obscure audio
and make it useless to everyone are not particularly useful
to the adversaries. While adversarial training may help in
partial mitigations, we believe that more substantial defenses
are ultimately required to defend against these attacks.
REFERENCES
[1] D. Williamson, M. Draper, G. Calhoun, and T. Barry, “Commercial
Speech Recognition Technology in the Military Domain: Results of Two
Recent Research Efforts,” vol. 8, no. 1, pp. 9–16, 2005.
[2] M. Cisse, Y. Adi, N. Neverova, and J. Keshet, “Houdini: Fooling deep
structured prediction models,” arXiv preprint arXiv:1707.05373, 2017.
[3] X. Yuan, Y. Chen, Y. Zhao, Y. Long, X. Liu, K. Chen, S. Zhang,
H. Huang, X. Wang, and C. A. Gunter, “Commandersong: A systematic
approach for practical adversarial voice recognition,” in Proceedings of
the USENIX Security Symposium, 2018.
[4] N. Carlini and D. Wagner, “Audio Adversarial Examples: Targeted
Attacks on Speech-to-Text,” ArXiv e-prints, p. arXiv:1801.01944, Jan.
2018.
[5] L. Sch¨onherr, K. Kohls, S. Zeiler, T. Holz, and D. Kolossa, “Adversarial
attacks against automatic speech recognition systems via psychoacoustic
hiding,” arXiv preprint arXiv:1808.05665, 2018.
[6] F. Kreuk, Y. Adi, M. Cisse, and J. Keshet, “Fooling end-to-end speaker
veriﬁcation by adversarial examples,” arXiv preprint arXiv:1801.03339,
2018.
[7] M. Alzantot, B. Balaji, and M. Srivastava, “Did you hear that? adver-
sarial examples against automatic speech recognition,” arXiv preprint
arXiv:1801.00554, 2018.
[8] R. Taori, A. Kamsetty, B. Chu, and N. Vemuri, “Targeted adversarial ex-
amples for black box audio systems,” arXiv preprint arXiv:1805.07820,
2018.
[9] H. Abdullah, K. Warren, V. Bindschaedler, N. Papernot, and P. Traynor,
“SoK: The Faults in our ASRs: An Overview of Attacks against
Automatic Speech Recognition and Speaker Identiﬁcation Systems,” In
Submission, 2020.
[10] H. Abdullah, W. Garcia, C. Peeters, P. Traynor, K. Butler, and J. Wilson,
“Practical hidden voice attacks against speech and speaker recognition
systems,” Proceedings of the 2019 Network and Distributed System
Security Symposium (NDSS), 2019.
[11] N. Carlini, P. Mishra, T. Vaidya, Y. Zhang, M. Sherr, C. Shields,
D. Wagner, and W. Zhou, “Hidden voice commands.” in USENIX
Security Symposium, 2016, pp. 513–530.
[12] Z. Yang, B. Li, P.-Y. Chen, and D. Song, “Characterizing au-
dio adversarial examples using temporal dependency,” arXiv preprint
arXiv:1809.10875, 2018.
[13] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards
deep learning models resistant to adversarial attacks,” arXiv preprint
arXiv:1706.06083, 2017.
[14] S. A. Gelfand, Hearing: An Introduction to Psychological and Physio-
logical Acoustics, 5th ed.
Informa Healthcare, 2009.
[15] L. R. Rabiner and R. W. Schafer, Digital processing of speech signals.
Prentice Hall, 1978.
[16] S. Venugopalan, H. Xu, J. Donahue, M. Rohrbach, R. Mooney, and
K. Saenko, “Translating videos to natural language using deep recurrent
neural networks,” arXiv preprint arXiv:1412.4729, 2014.
[17] T. N. Sainath, O. Vinyals, A. Senior, and H. Sak, “Convolutional,
long short-term memory, fully connected deep neural networks,” in
2015 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), April 2015, pp. 4580–4584.
[18] O. Abdel-Hamid, A.-r. Mohamed, H. Jiang, and G. Penn, “Applying
convolutional neural networks concepts to hybrid nn-hmm model for
speech recognition,” pp. 4277–4280, 05 2012.
[19] T. N. Sainath, A.-r. Mohamed, B. Kingsbury, and B. Ramabhadran,
“Deep convolutional neural networks for lvcsr,” pp. 8614–8618, 05 2013.
[20] A. Graves, A.-r. Mohamed, and G. Hinton, “Speech recognition with
deep recurrent neural networks,” ICASSP, IEEE International Confer-
ence on Acoustics, Speech and Signal Processing - Proceedings, vol. 38,
03 2013.
[21] H. Sak, A. Senior, and F. Beaufays, “Long short-term memory recur-
rent neural network architectures for large scale acoustic modeling,”
Proceedings of
the International Speech
Communication Association, INTERSPEECH, pp. 338–342, 01 2014.
the Annual Conference of
[22] H. Sak, O. Vinyals, G. Heigold, A. Senior, E. McDermott, R. Monga,
and M. Mao, “Sequence discriminative distributed training of long short-
term memory recurrent neural networks,” Proceedings of the Annual
Conference of the International Speech Communication Association,
INTERSPEECH, pp. 1209–1213, 01 2014.
[23] H. Sak, A. Senior, K. Rao, and F. Beaufays, “Fast and accurate
recurrent neural network acoustic models for speech recognition,”
CoRR, vol. abs/1507.06947, 2015. [Online]. Available: http://arxiv.org/
abs/1507.06947
[24] A. Graves and N. Jaitly, “Towards end-to-end speech recognition with
recurrent neural networks,” in International Conference on Machine
Learning, 2014, pp. 1764–1772.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:09:39 UTC from IEEE Xplore.  Restrictions apply. 
724
[25] P. Lamere, P. Kwok, W. Walker, E. Gouvˆea, R. Singh, B. Raj, and
P. Wolf, “Design of the cmu sphinx-4 decoder,” in Eighth European
Conference on Speech Communication and Technology, 2003.
[26] D. Amodei et al., “Deep speech 2 : End-to-end speech recognition
in english and mandarin,” in Proceedings of The 33rd International
Conference on Machine Learning, ser. Proceedings of Machine Learning
Research, M. F. Balcan and K. Q. Weinberger, Eds., vol. 48. New
York, New York, USA: PMLR, 20–22 Jun 2016, pp. 173–182. [Online].
Available: http://proceedings.mlr.press/v48/amodei16.html
[27] “1,000 Most Common US English Words,” Last accessed in 2019,
available at https://www.ef.edu/english-resources/english-vocabulary/
top-1000-words/.
[28] H. K¨opcke, A. Thor, and E. Rahm, “Evaluation of entity resolution
approaches on real-world match problems,” Proceedings of the VLDB
Endowment, vol. 3, no. 1-2, pp. 484–493, 2010.
[29] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and
A. Swami, “Practical Black-box Attacks Against Machine Learning,”
in Proceedings of the 2017 ACM on Asia Conference on Computer and
Communications Security. ACM, 2017, pp. 506–519.
[30] J. Lu, H. Sibai, E. Fabry, and D. Forsyth, “NO Need to Worry about
Adversarial Examples in Object Detection in Autonomous Vehicles,”
ArXiv e-prints, 2017.
[31] J. S. Garofolo et al., “Getting started with the darpa timit cd-rom:
An acoustic phonetic continuous speech database,” National Institute of
Standards and Technology (NIST), Gaithersburgh, MD, vol. 107, p. 16,
1988.
[32] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: an
asr corpus based on public domain audio books,” in Acoustics, Speech
and Signal Processing (ICASSP), 2015 IEEE International Conference
on.
IEEE, 2015, pp. 5206–5210.
[33] “The CMU Audio Database (also known as AN4 database),” Last
accessed in 2019, available at http://www.speech.cs.cmu.edu/databases/
an4/.
[34] “Project SHTOOKA - A Multilingual Database of Audio Recordings of
Words and Sentences,” Last accessed in 2019, available at http://shtooka.
net/.
[35] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio,
“Attention-based models for speech recognition,” in Advances in neural
information processing systems, 2015, pp. 577–585.
[36] L. R. Rabiner and B.-H. Juang, Fundamentals of speech recognition.
PTR Prentice Hall Englewood Cliffs, 1993, vol. 14.
[37] D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, and Y. Bengio, “End-
to-end attention-based large vocabulary speech recognition,” in Acous-
tics, Speech and Signal Processing (ICASSP), 2016 IEEE International
Conference on.
IEEE, 2016, pp. 4945–4949.
[38] “Azure speaker identiﬁcation api,” Last accessed in 2019, available
https://azure.microsoft.com/en-us/services/cognitive-servic/
at
speaker-recognition/.
[39] “Who’s Smartest: Alexa, Siri, and or Google Now?” Last accessed in
2019, available at https://bit.ly/2ScTpX7.
[40] “Wer are we - an attempt at tracking states of the art(s) and recent re-
sults on speech recognition,” https://github.com/syhw/wer are we, Last
accessed in 2019.
[41] “Googles Speech Recognition Technology Now Has a 4.9% Word Error
Rate,” Last accessed in 2019, available at https://bit.ly/2rGRtUQ.
[42] A. Danielsson,
“Comparing android runtime with native: Fast
fourier transform on android,” 2017, mS thesis. [Online]. Available:
”https://bit.ly/2MQpUV1”
[43] “Twilio - Communication APIs for SMS, Voice, Video and Authentica-