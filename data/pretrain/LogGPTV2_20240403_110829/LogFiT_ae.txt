The process of log anomaly detection generally involves four key steps, which have been identified by various researchers [13]–[16].• Log data pre-processing - which involves cleaning and standardising the raw system logs that have been generated by computer systems. This step is critical as the accuracy of the subsequent steps relies on the quality of the pre-processed data. It is assumed that the logs have already been collected at this stage, and the focus is on standardising the data and on removing any inconsistencies and irrelevant information.• Vectorisation – this step is concerned with transforming the pre-processed log data into numeric representations called vectors. The training of machine learning models in the following step requires input data to be in the form of vectors.
Figure 1. HDFS log sentences converted to log templates.• Model development – during this step the optimal 	combination 	of 	training 	objective, 	neural 	network 	architecture 	and 	performance 	evaluation 	metrics 	is 	selected. Once these elements are identified, the neural 	network model is trained on the vectorised data obtained 	in the preceding step, using an optimisation algorithm to 	minimise the difference between the model’s predictions 	and the actual output. Finally, the model’s performance 	is evaluated based on the selected metrics, and the best-	performing model is selected.• Model operationalisation – in this final step, the best performing machine learning model is operationalised by deploying it to a production environment. The model’s efficiency and effectiveness are then monitored continuously to ensure it performs optimally in its given environment.The initial three activities of log anomaly detection have been the subject of extensive research. For log data pre-processing, some studies have emphasised the use of log parsing [4], [5]. However, more recent studies [11], [17] suggested that log parsing may lead to a reduction in accuracy and hence, should not be recommended. To increase accuracy, a state-of-the-art method called semantic vectorisation is applied to the log data, to enrich it with semantic information before using them as input for model training [11], [17]. In contrast, the earlier cited works [4], [5] have utilised basic numerical encoding techniques to convert the input log data into vectors. In model development, previous studies have revealed that the best performance in log anomaly detection were attained through the use of model architectures and techniques that were inspired by natural language processing (NLP), as noted in Pang, Shen, Cao, et al. [13] and Yadav, Kumar, and Dhavale [18]. Among these techniques, Long Short-Term Memory (LSTM) and Transformers have been the preferred model architectures [4]–[6], [11].A. Log Templates
Log anomaly detection methods commonly use log parsing as the initial step, which involves converting log data into a standardised format known as log templates [14], [16], [19]. The goal of log parsing is to map each log sentence to a specific log template, which forms the vocabulary of the model, rather than using words or tokens typical in natural language processing (NLP). Several log anomaly detection methods, such as DeepLog [4], LogRobust [17], and LogBERT3
Figure 2. Comparison of the DeepLog and LogBERT log anomaly detection methods.
[5], rely on log templates, often generated using the Drain parser [20], to pre-process log data.Figure 1 illustrates how log sentences are converted into log templates in the DeepLog and LogBERT methods. The conversion of log data to log templates is done as part of the pre-processing step, and the generated set of log templates represents the model vocabulary. As illustrated in Figure 2, the input to the DeepLog and LogBERT models are a sequence of log keys, where each log key corresponds to a log template stored in the vocabulary. In the DeepLog method, the last log key is masked in the input and the model is tasked with predicting what the masked log key was, given the log keys that precede it. In contrast, LogBERT masks out random log keys in the input sequence and tasks the model with predicting what all the masked tokens were.Several 	recent 	studies, 	conducted 	by 	Nedelkoski, Bogatinovski, Acker, et al. [6], Le and Zhang [11], and Wittkopp, Acker, Nedelkoski, et al. [12], have highlighted that the use of log templates can result in a loss of contextual information, which can adversely affect the accuracy of predictive models. Moreover, models developed using log templates can become dependent on the log parsing tool employed. Approaches that rely on log templates assume that the set of log templates remains fixed over time, but in reality, changes in the content of log sentences can occur naturally. As a result, models that depend on log templates may not be able to match unseen log sentences to an existing record in the set of log templates.These issues were addressed in the development of several log anomaly detection models, including LogSy [6], Neuralog [11], and A2Log [12]. Rather than relying on log parsing to generate log templates, these models pre-process log data using plain cleanup scripts to eliminate extraneous details such as particular IP addresses, file paths, port numbers, and internet URLs.
B. Semantic VectorsB. Semantic Vectors
	The 	literature 	on 	log 	anomaly 	detection 	reveals 	a growing trend towards the adoption of semantic vectorisation