The process of log anomaly detection typically involves four key steps, as identified by various researchers [13]–[16]:

1. **Log Data Pre-processing**:
   - This step involves cleaning and standardizing raw system logs generated by computer systems.
   - The quality of the pre-processed data is crucial for the accuracy of subsequent steps.
   - It is assumed that the logs have already been collected, and the focus is on standardizing the data, removing inconsistencies, and eliminating irrelevant information.

2. **Vectorization**:
   - This step transforms the pre-processed log data into numeric representations called vectors.
   - Machine learning models require input data in vector form for training.

3. **Model Development**:
   - In this step, the optimal combination of training objectives, neural network architecture, and performance evaluation metrics is selected.
   - The neural network model is trained on the vectorized data using an optimization algorithm to minimize the difference between the model’s predictions and the actual output.
   - The model’s performance is evaluated based on the selected metrics, and the best-performing model is chosen.

4. **Model Operationalization**:
   - The best-performing machine learning model is deployed to a production environment.
   - The model’s efficiency and effectiveness are continuously monitored to ensure optimal performance in its given environment.

### Log Data Pre-processing
Extensive research has focused on the initial three activities of log anomaly detection. For log data pre-processing, some studies have emphasized the use of log parsing [4], [5]. However, more recent studies [11], [17] suggest that log parsing may reduce accuracy and should not be recommended. To enhance accuracy, a state-of-the-art method called semantic vectorization is applied to the log data, enriching it with semantic information before using it as input for model training [11], [17]. Earlier works [4], [5] utilized basic numerical encoding techniques to convert log data into vectors.

### Model Development
Previous studies have shown that the best performance in log anomaly detection is achieved through model architectures and techniques inspired by natural language processing (NLP), as noted in Pang, Shen, Cao, et al. [13] and Yadav, Kumar, and Dhavale [18]. Among these techniques, Long Short-Term Memory (LSTM) and Transformers have been the preferred model architectures [4]–[6], [11].

### Log Templates
Log anomaly detection methods often use log parsing as the initial step, converting log data into a standardized format known as log templates [14], [16], [19]. The goal of log parsing is to map each log sentence to a specific log template, which forms the vocabulary of the model, rather than using words or tokens typical in NLP. Several log anomaly detection methods, such as DeepLog [4], LogRobust [17], and LogBERT [5], rely on log templates, often generated using the Drain parser [20], to pre-process log data.

Figure 1 illustrates how log sentences are converted into log templates in the DeepLog and LogBERT methods. The conversion of log data to log templates is part of the pre-processing step, and the generated set of log templates represents the model vocabulary. As shown in Figure 2, the input to the DeepLog and LogBERT models is a sequence of log keys, where each log key corresponds to a log template stored in the vocabulary. In the DeepLog method, the last log key is masked in the input, and the model predicts the masked log key based on the preceding log keys. In contrast, LogBERT masks out random log keys in the input sequence and tasks the model with predicting all the masked tokens.

Recent studies by Nedelkoski, Bogatinovski, Acker, et al. [6], Le and Zhang [11], and Wittkopp, Acker, Nedelkoski, et al. [12] highlight that the use of log templates can result in a loss of contextual information, adversely affecting the accuracy of predictive models. Additionally, models developed using log templates can become dependent on the log parsing tool employed. These models assume that the set of log templates remains fixed over time, but changes in log content can occur naturally, leading to difficulties in matching unseen log sentences to existing templates.

To address these issues, several log anomaly detection models, including LogSy [6], Neuralog [11], and A2Log [12], pre-process log data using plain cleanup scripts to eliminate extraneous details such as IP addresses, file paths, port numbers, and internet URLs, rather than relying on log parsing to generate log templates.

### Semantic Vectors
The literature on log anomaly detection reveals a growing trend towards the adoption of semantic vectorization.