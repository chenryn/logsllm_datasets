### Default Value Assignment
We assign a default value of 0 in our experiment.

### Previous Registrar (Categorical)
The previous registrar provides insights into how and where spammers gather information about expired domains. We map the previous registrars to a set of binary features. Only the feature corresponding to the specific previous registrar is set to 1, while all others are set to 0. For brand-new domains, where the previous registrar field is not applicable, we add a dummy registrar feature.

### Re-Registration from the Same Registrar (Categorical)
We introduce features to explicitly indicate whether the registrar of a re-registered domain is the same as the previous registrar. Our observations show that in the .com zone, 18% of non-spammer retread domains use the same previous registrar, whereas only 4% of spammer retread domains do so. This suggests that miscreants often choose specific registrars that are different from those used by legitimate registrants. For brand-new domains, we use a dummy value to handle this feature.

### Batch Correlation Features
Batch correlation features analyze domains within the same tuple \(\langle \text{registrar}, \text{five-minute epoch} \rangle\), which we define as a batch. The batch information is initially known by registrars or registries.

#### Probability of Batch Size (Continuous)
Miscreants often register domains in large batches, likely due to the cheaper price of bulk registration or ease of management. We identify this qualitatively different registration behavior using a compound Poisson process model, as defined in Hao et al. [20]. A low-probability batch size from the model indicates an abnormally large registration spike. We use this probability as a feature in our system.

#### Life-Cycle Proportion (Continuous)
As previously mentioned, the registration history can categorize a domain as brand-new, drop-catch, or retread. Miscreants tend to register domains in a particular part of the domain life-cycle in a single batch due to their name selection strategy. We generate three features, each measuring the proportion of different life cycles for domains in the same batch. These three features sum to 1 by construction.

#### Name Cohesiveness (Continuous)
Spammer domains registered in the same batch often have lexically similar names, as miscreants use the same strategy or generation algorithm to produce a list of domains. To quantify the cohesiveness of a given domain name with respect to all other domain names in the same batch, we compute the edit distances of the domain to every other domain in the batch. We normalize these edit distances by dividing by the length of the domain name to provide a similarity score. We then compute ten features as the numbers of domains with similarity scores between [0, 0.1], [0, 0.2], ..., [0, 1.0]. We use a logarithmic scale to account for the large variability in batch sizes.

### Classifier Design
This section introduces the Convex Polytope Machine (CPM), a supervised learning algorithm we use, including our rationale for selecting this algorithm. It also covers the process of building the detection models and the derivation of feature importance based on the models.

#### Supervised Learning: CPM
We require a classifier that can quickly train over large datasets and achieve high accuracy. While linear Support Vector Machines (SVM) [12, 47] or comparable linear methods are often used in high-performance settings, nonlinearities in our data pose challenges for SVM-style approaches. Instead, we employ the state-of-the-art Convex Polytope Machine (CPM) [30]. CPM maintains an ensemble of linear sub-classifiers and makes its final decision based on the maximum score among them. Formally, suppose \( x \in \mathbb{R}^d \) is an instance of \( d \) features, and \( w_1, \ldots, w_K \in \mathbb{R}^d \) represent the weights of the \( K \) sub-classifiers. The score of \( x \) is derived as:
\[ f(x) = \max_{1 \leq k \leq K} \langle x, w_k \rangle \]
The prediction score \( f(x) \) reflects the likelihood that a domain is registered for spam-related activity. Geometrically, a CPM defines a convex polytope as the decision boundary to separate the two classes. In our application, this richer, non-linear decision boundary provides high classification accuracy. Training of a CPM can be efficiently achieved using gradient descent [47].

To assess our design choice, we tested SVM [12] using libsvm with parameters tuned to our application. We found that in the low-false-positive region, CPM produced a 10% higher true-positive rate and trained faster than an SVM.

#### Building Detection Models
The first step in building the model is to normalize continuous and ordinal features. We transform real values into the [0, 1] interval to ensure they do not overly dominate categorical features. We compute the ranges for each continuous and ordinal feature to obtain max/min values, and normalize feature \( v \) to \( \frac{v - v_{\min}}{v_{\max} - v_{\min}} \). Since the categorical features are already in binary form, no additional normalization is needed.

We adapt a sliding window mechanism for re-training models and evaluating detection accuracy close to real-deployment scenarios. We define three windows: training (\( T_{\text{train}} \)), cooling (\( T_{\text{cool}} \)), and testing (\( T_{\text{test}} \)). As shown in Figure 3, at round \( N \), the training window starts at time \( T_N \). The model will be constructed at time \( \hat{T}_N = T_N + T_{\text{train}} + T_{\text{cool}} \), using the ground truth collected during the period \([T_N, \hat{T}_N]\) to label the domains registered during \([\hat{T}_N, \hat{T}_N + T_{\text{test}}]\). The ground truth for the testing period is composed of domains showing up on blacklists from time \( \hat{T}_N \) up to our last collection date.

In the next round, \( N+1 \), we move the time window forward by \( T_{\text{train}} + T_{\text{cool}} + T_{\text{test}} \). The period \( T_{\text{test}} \) indicates how frequently we re-train the model. Operators can customize the three window lengths according to different requirements and settings (see Section 7.4).

#### Assessing Feature Importance
Given a subset \( S \subset \{1, \ldots, d\} \) of features, a derived CPM model \(\{w_1, \ldots, w_K\}\), and a dataset of points \(\{x_1, \ldots, x_n\}\), we derive a measure to evaluate the importance of the set of features in our classifier. If the model weights have large magnitudes while the associated features have low variance, these dimensions are not particularly informative and should receive a low importance score. We design a scoring method to measure the total amount of variation on the score \( f(x) \) over the dataset induced by the features \( S \). For a single linear classifier (\( K = 1 \)), we measure this quantity as:
\[ I_S^1 = \sqrt{\text{Var}_x \left( \sum_{i \in S} w_{1i} x_i \right)} \]

To generalize this measure to the case \( K \geq 2 \), for each sub-classifier \( k \), we compute the score \( I_S^k \) based on its subset of assigned instances \( A_k \), and combine the scores:
\[ I_S = \sum_{k=1}^K \frac{|A_k|}{n} \sqrt{\text{Var}_{x \in A_k} \left( \sum_{i \in S} w_{ki} x_i \right)} \]
where \( A_k \) is composed of \( x \) satisfying \( k = \arg\max_{k'} \langle w_{k'}, x \rangle \).

Higher values of \( I_S \) indicate that the feature group \( S \) contributes more to the decision-making. We demonstrate the feature importance in Section 7.4.

### Evaluation
In this section, we report our evaluation results, compare the performance of PREDATOR to existing blacklists, and analyze evasion scenarios.

#### Data Set and Labeling
Our primary dataset consists of changes made to the .com zone, the largest TLD [60], for a five-month period, March–July 2012. We obtain the DNZA files from Verisign (with five-minute granularity), find the registrations of new domains, and extract the updates of authoritative nameservers and IP addresses. During March–July 2012, we have 12,824,401 newly registered second-level .com domains.

To label the registered domains as legitimate or malicious, we collected public blacklisting information from March–October 2012 (eight months), including Spamhaus [49] (updated every 30 minutes), URIBL [56] (updated every 60 minutes), and a spam trap that we operate (real-time). If a domain appeared on blacklists after registration, we labeled it as being involved in spam-related activities and registered by miscreants. To obtain benign labels, we queried McAfee SiteAdvisor [48] in June 2013 to find the domains reported as definitely benign. Eventually, we have about 2% of .com domains with malicious labels and 4% with benign domain labels. We discuss the prediction results on labeled and unlabeled domains in Section 7.2.

We also obtained DNZA data for the .net zone for five months, from October 2014 to February 2015, which contain 1,284,664 new domains. We used similar blacklists (URIBL and our spam trap) from October 2014 to May 2015 (eight months) to label malicious domains and queried McAfee in November 2015 to find benign labels. However, the information for .net domains is not complete, allowing only limited analysis. We only have a Spamhaus snapshot on December 7th, 2015 (instead of a continuous feed), and the previous registrar information is not available.

#### Detection Accuracy
We demonstrate the accuracy of PREDATOR in terms of false positive rate (the ratio of benign domains misclassified as malicious to all benign instances) and detection rate (the ratio of correctly predicted spammer domains to all spammer domain samples). By setting different thresholds, we make trade-offs between false positive rates and detection rates.

For .com domains, we use data from March 2012 to extract the known-bad domain set and derive probability models for registration batches, and take April–July 2012 for our experiments. We used the sliding window method (introduced in Section 6.2) and tested different window lengths, where better results resulted from longer training windows (i.e., more domains for training) and shorter testing windows (i.e., more frequent re-training). We demonstrate the performance results of PREDATOR with the setting of the training window to 35 days, the cooling window to 1 day, and the testing window to 7 days, which produces good detection accuracy and allows realistic operation (see Section 7.4 for detailed discussion on window selection).

Figure 4 shows the ROC curve of PREDATOR. The x-axis shows the false positive rate, and the y-axis shows the detection rate. The inlay figure shows the ROC curve for the range of 0–5% false positives. PREDATOR achieves good detection rates under low false positives. For example, with a 70% detection rate, the false positive rate is 0.35%.

We emphasize that these results only rely on features constructed from the limited information available at registration time. Thus, as an early-warning mechanism, PREDATOR can effectively detect many domains registered for malicious activities.

Results on the entire .com zone. We project the 0.35% false positive rate to the entire .com zone. Since there are around 80,000 new domains every day, the daily false positives are about 280 domains (as an upper estimate, assuming all domains are benign). Given that even the known spammer domains totaled more than 1,700 every day, PREDATOR can greatly help to narrow down the set of suspect domains. We ran additional tests to examine how many unlabeled domains are classified as spam-related by using the constructed detection model on the entire zone dataset, about seven million .com new domains registered over three months. With a threshold under a 0.35% false positive rate (in Figure 4, obtaining a 70% detection rate), PREDATOR reports about 1,000 unlabeled domains per day as spam-related, the same magnitude as the labeled spammer domains (1,700 per day). Overall, PREDATOR predicts 3% of all newly registered .com domains as malicious, capturing 70% of the malicious domains showing up later on blacklists. As a first line of defense, PREDATOR can effectively reduce and prioritize suspect domains for further inspection (e.g., URL crawling or manual investigation) and find more malicious pages given a fixed amount of resources. In Section 7.3, we investigate to what extent the unlabeled domains that PREDATOR classifies as malicious indeed connect to illicit online activities while missed by current blacklists.

Detection accuracy on .net domains. We performed a similar experiment to report the detection accuracy on the .net zone (five months in 2014–2015). Due to data limitations, two features, the previous registrar and re-registration from the same registrar, are unavailable, and in the blacklists, Spamhaus only has a single snapshot. With the same sliding window setting, the detection rate on .net domains is 61% (close to the 70% on .com domains) under a 0.35% false positive rate. The result shows that PREDATOR can successfully make predictions at different zones. In the rest of the experiments, we focus on .com domains (.net domains either yield similar results or cannot conduct the analysis due to data defects).

#### Comparison to Existing Blacklists
We investigate and compare different blacklists and find that PREDATOR can help mitigate the shortcomings of current blacklisting methods and detect malicious domains earlier.

Detection of more spammer domains. The first property we examine is completeness, which explores how many spammer domains PREDATOR detects compared to other blacklists. We find that during May–July 2012, the exclusive blacklisted .com domains (i.e., not reported by other feeds) on Spamhaus, URIBL, and our spam trap number 24,015, 4,524, and 442 respectively. Each blacklist has many domains not identified by other sources, indicating that existing blacklists are not perfect in detecting all malicious domains. Having incomplete blacklists makes it challenging to develop more accurate registration-time detection, and also shows how PREDATOR can complement these blacklists.