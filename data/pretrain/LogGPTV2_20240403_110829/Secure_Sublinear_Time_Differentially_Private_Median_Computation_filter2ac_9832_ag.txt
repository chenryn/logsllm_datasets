[24] O. Goldreich, Foundations of Cryptography: Volume 2,
[22]
Basic Applications, 2009.
[25] S. Goryczka and L. Xiong, “A comprehensive comparison
of multiparty secure additions with differential privacy,”
IEEE Transactions on Dependable and Secure Comput-
ing, 2017.
[26] V. Goyal, D. Khurana, I. Mironov, O. Pandey, and
A. Sahai, “Do distributed differentially-private protocols
require oblivious transfer?” in Leibniz International Pro-
ceedings in Informatics, ser. LIPIcs, 2016.
[27] A. Gupta, K. Ligett, F. McSherry, A. Roth, and K. Talwar,
“Differentially private combinatorial optimization,” in
Proceedings of the annual ACM SIAM symposium on
Discrete Algorithms, ser. SODA, 2010.
[28] X. He, A. Machanavajjhala, C. Flynn, and D. Srivastava,
“Composing differential privacy and secure computation:
A case study on scaling private record linkage,” in
Proceedings of the annual ACM conference on Computer
and Communications Security, ser. CCS, 2017.
[29] N. Holohan, D. J. Leith, and O. Mason, “Optimal dif-
ferentially private mechanisms for randomised response,”
IEEE Transactions on Information Forensics and Secu-
rity, 2017.
[30] J. Hsu, S. Khanna, and A. Roth, “Distributed private
heavy hitters,” in International Colloquium on Automata,
Languages, and Programming, ser. ICALP, 2012.
[31] Y. Huang, D. Evans, and J. Katz, “Private set intersection:
Are garbled circuits better than custom protocols?” in
Network and Distributed Systems Security Symposium,
ser. NDSS, 2012.
[32] M. Ion, B. Kreuter, E. Nergiz, S. Patel, S. Saxena,
K. Seth, D. Shanahan, and M. Yung, “Private intersection-
sum protocol with applications to attributing aggre-
gate ad conversions,” Cryptology ePrint Archive, Report
2017/738, 2017, https://eprint.iacr.org/2017/738.
[33] Kaggle.com,
“Walmart
supply chain:
Import
and
shipment,”
https://www.kaggle.com/sunilp/walmart-
supply-chain-data/data, 2018, Retrieved: January 29,
2019.
[34] S. P. Kasiviswanathan, H. K. Lee, K. Nissim, S. Raskhod-
nikova, and A. Smith, “What can we learn privately?”
SIAM Journal on Computing, 2011.
[35] N. Li, M. Lyu, D. Su, and W. Yang, “Differential privacy:
From theory to practice,” Synthesis Lectures on Informa-
tion Security, Privacy, & Trust, 2016.
[36] Y. Lindell and B. Pinkas, “A proof of security of yao’s
protocol for two-party computation,” Journal of Cryptol-
ogy, 2009.
[37] D. Mazieres and D. Miller, “Source of arc4random.c.”
[Online]. Available: https://opensource.apple.com/source/
Libc/Libc-1158.50.2/gen/FreeBSD/arc4random.c
[38] A. McGregor, I. Mironov, T. Pitassi, O. Reingold, K. Tal-
war, and S. Vadhan, “The limits of two-party differential
privacy,” in Annual IEEE Symposium on Foundations of
Computer Science, ser. FOCS, 2010.
[39] F. McSherry and K. Talwar, “Mechanism design via
differential privacy,” in Annual IEEE Symposium on
Foundations of Computer Science, ser. FOCS, 2007.
[40] F. D. McSherry, “Privacy integrated queries: an exten-
sible platform for privacy-preserving data analysis,” in
Proceedings of the annual ACM SIGMOD International
Conference on Management of data, ser. SIGMOD, 2009.
[41] I. Mironov, O. Pandey, O. Reingold, and S. Vadhan,
“Computational differential privacy,” in Annual Interna-
tional Cryptology Conference, 2009.
[42] M. Naor, B. Pinkas, and E. Ronen, “How to (not) share a
password: Privacy preserving protocols for ﬁnding heavy
hitters with adversarial behavior,” in Proceedings of the
annual ACM conference on computer and communica-
tions security, ser. CCS.
[43] K. Nissim, S. Raskhodnikova, and A. Smith, “Smooth
sensitivity and sampling in private data analysis,” in
Proceedings of the annual ACM symposium on Theory
of Computing, ser. STOC, 2007.
[44] M. Pettai and P. Laud, “Combining differential privacy
and secure multiparty computation,” in Proceedings of
the Annual Computer Security Applications Conference,
ser. ASAC, 2015.
[45] C. P. Pﬂeeger and S. L. Pﬂeeger, Security in computing,
2002.
[46] P. Pullonen, D. Bogdanov, and T. Schneider, “The design
and implementation of a two-party protocol suite for
sharemind 3,” CYBERNETICA Institute of Information
Security, Tech. Rep., 2012.
[47] M. Rabin, “How to exchange secrets by oblivious trans-
fer,” Technical Memo TR-81, 1981.
[48] V. Rastogi and S. Nath, “Differentially private aggrega-
tion of distributed time-series with transformation and
encryption,” in Proceedings of the annual ACM SIGMOD
International Conference on Management of data, ser.
SIGMOD, 2010.
[49] A. Shelat and M. Venkitasubramaniam, “Secure computa-
tion from millionaire,” in International Conference on the
Theory and Application of Cryptology and Information
Security, ser. ASIACRYPT, 2015.
[50] A. Smith, A. Thakurta, and J. Upadhyay, “Is interaction
necessary for distributed private learning?” in IEEE Sym-
posium on Security and Privacy, 2017.
[51] G. Sood, “California Public Salaries Data,” 2018. [On-
line]. Available: https://doi.org/10.7910/DVN/KA3TS8
[52] H. Takabi, S. Koppikar, and S. T. Zargar, “Differentially
private distributed data analysis,” in IEEE International
Conference on Collaboration and Internet Computing,
ser. CIC, 2016.
[53] A. D. P. Team, “Learning with privacy at scale,” 2017.
[Online]. Available: https://machinelearning.apple.com/
2017/12/06/learning-with-privacy-at-scale.html
[54] M. L. G. ULB. (2018) Credit card fraud detection.
[Online]. Available: https://www.kaggle.com/mlg-ulb/
creditcardfraud/data
[55] WWDC 2016, “Engineering privacy for your users,”
2016. [Online]. Available: https://developer.apple.com/
videos/play/wwdc2016/709/
[56] A. C.-C. Yao, “How to generate and exchange secrets,”
in Annual IEEE Symposium on Foundations of Computer
Science, ser. FOCS, 1986.
A. Garbled Circuit
APPENDIX
Bellare et al. [3] formalize a garbling scheme as the tuple of
algorithms G = (Gb, En, De, Ev, ev), where Gb is probabilistic
and all others are deterministic. A string is deﬁned as a
sequence of bits of ﬁnite length.
•
(F, e, d) ← Gb(1λ, f ): Takes as input a security
parameter λ ∈ N and the string f describing the
original function to evaluate, ev(f,·), and outputs
string F describing the garbled function, Ev(F,·),
string e describing an encoding function, En(e,·), and
string d describing a decoding function, De(d,·), as
deﬁned in the following.
•
•
• X ← En(e, x) is an encoding function, described by
string e, that maps an initial input x ∈ {0, 1}n to a
garbled input X.
y ← De(d, Y ) is a decoding function, described by
string d, that maps a garbled output Y to a ﬁnal output
y.
Y ← Ev(F, X) is an evaluation function, described
by string F , that maps a garbled input X to a garbled
output Y .
y ← ev(f, x) is an evaluation function, described by
string f, that maps the input x to the output y, where
ev(f,·) : {0, 1}n → {0, 1}m is the original function
we want to garble, and n = f.n, m = f.m depend on
f and must be computable from it in linear-time.
•
The following requirements are imposed on a garbling
scheme:
•
•
then |F| = |F (cid:48)|,
Length condition: If f.n = f(cid:48).n, f.m = f(cid:48).m,|f| =
|f(cid:48)|, (F, e, d) ∈ [(Gb(1λ, f )], and (F (cid:48), e(cid:48), d(cid:48)) ∈
|e| = |e(cid:48)|, and
[(Gb(1λ, f(cid:48))],
|d| = |d(cid:48)|.
Non-degeneracy condition: Let r are be the random
coins of Gb. If f.n = f(cid:48).n, f.m = f(cid:48).m,|f| =
|f(cid:48)|, (F, e, d) ∈ [(Gb(1λ, f ; r)], and (F (cid:48), e(cid:48), d(cid:48)) ∈
[(Gb(1λ, f(cid:48); r)], then e = e(cid:48) and d = d(cid:48).
15
utility(i)
0− n
2 +1
1− n
2 +1
n
2 −2
n
2 −3
index i
sorted D
0
d0
gap(i)
d1−d0
1
d1
1
2
d2
3
d3
d2−d1
d3−d2
Fig. 12. utility and gap computed on sorted D with static access pattern.
•
∈ {0, 1}∗, λ ∈
Correctness
N, x ∈ {0, 1}f.n, and (F, e, d) ∈ [Gb(λ, f )], then
De(d, Ev(F, En(e, x))) = ev(f, x).
condition:
If f
B. Static access pattern for utility and gap
Each party can compute utility (Deﬁnition 6) without
any access to D, and gap (Deﬁnition 7) has a static access
pattern, independent of the elements in (sorted) D, which
makes the gap function data-oblivious, i.e., an attacker who
sees the access pattern cannot learn anything about D. Fig. 12
visualizes how we compute utility and gap with static access
pattern over sorted data D.
C. Non-decreasing Utility after Pruning
Theorem 5. The input pruning from [1] does not decrease
utility.
i
j
i = Ds
 umed(Ds−1, x).
An example of non-decreasing utility after pruning is
shown in Table III for unique elements. For example, element
a1 has utility −3 before pruning, after pruning its utility
increases to −2, whereas the utility for b2, a3 remain as before.
16
TABLE III.
UTILITY DOES NOT DECREASE FOR SORTED
D = DA ∪ DB BEFORE AND AFTER ONE PRUNING STEP WITH
DA = {a1, . . . , a4}, DB = {b1, . . . , b4}.
b1
D a1
a2
umed(D, ·) −3 −2 −1
–
umed(D1, ·) −2 −1 −1
D1
b1
–
b2
0
b2
0
b3
a4
a3
b4
0 −1 −2 −3
–
a3
0 −1 −1 −2
a4
–
Output: Input padded to place kth-ranked element at median
Algorithm 5 Algorithm PAD pads the input of party P ∈
{A, B} such that the element with rank k is at the median
position (part of FIND-RANKED-ELEMENT from [1]).
Input: Data DP , rank k, padding (cid:98)p
3: Pad DP with (cid:98)p until |DP| = 2(cid:100)log2(k)(cid:101)
position of the union of DA, DB
1: Sort DP and retain only the k smallest values
2: Pad DP with +∞ until |DP| = k
4: return DP
D. Padding
In a preprocessing step to the actual pruning the data
is padded as described in Algorithm 5 where A calls
PAD(DA, k, +∞) and B calls PAD(DA, k,−∞) with k =
(cid:100)(|DA| + |DB|)/2(cid:101). Note that the data size of each party, i.e.,
|DA|,|DB|, can be hidden via additional padding.
E. Merge Implementation
For the merging implementation, as seen in Algorithm 6,
we use the bitonic mergers as described in [31, Section 5.1]
which require a bitonic list as input, i.e., a list that is mono-
tonically increasing then decreasing (or vice versa). Bitonic
merging recursively splits the list in halves and compares and