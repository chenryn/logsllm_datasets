P
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Average Timespan
After Power Outages
After Power Spikes
After Power Supply Fails
After UPS Fails
 9.9x
 7.2x
 7.6x
 5.8x
10.4x
12.2x
 9.4x
11.4x
 6.1x
17x  
12.9x
 5.4x
Day
Week
Month
y
t
i
l
i
b
a
b
o
r
P
0.4
0.3
0.2
0.1
0
13.7x
14.1x
 8.9x
46.3x
19.6x
19.9x
 5.0x
10.4x
27.3x
17.4x
28.5x
16.3x
In a random month
After power outages
After power spikes
After power supply fails
After UPS Fails
 2.4x
40.7x
 0.9x
 0.7x
 0.3x
 NA  
 NA  
 NA  
PowerSupply
Memory
NodeBoard
Fan
CPU
Fig. 10.
Impact of power problems on hardware failures
49%
Power Outage
Power Spike
UPS
Chillers
Environment
6%
10%
21%
15%
Fig. 9. Breakdown of environmental failures in LANL systems
VII. WHAT IS THE IMPACT OF ENVIRONMENTAL
FACTORS, IN PARTICULAR PROBLEMS RELATED TO POWER?
We have observed in Section III that environmental failures
cause a steep increase in the probability of follow-up failures.
A node with an environmental failure has a chance of 47.2%
and 69.4% for group-1 and group-2 systems, respectively, of
experiencing another failure within a week. This observations
warrants a closer look at what environmental failures are and
how they affect other failures.
The LANL data provides a breakdown of the high-level
root cause category of environmental failures into lower-level
sub-categories. Figure 9 presents a breakdown of the observed
environmental failures. We observe that the majority of those
failures are related to problems with power in the datacenter, in
particular either power outages, power spikes or UPS failures.
In the remainder of this section we study how power issues
affect the two most common types of failures, hardware and
software failures. In addition to power outages, spikes and UPS
failures recorded as part of environmental failures, we also take
into account the effect of problems with the power supply
unit of individual servers, which are recorded as hardware
problems.
A. How do power problems affect hardware failures?
Figure 10 (left) shows the probability that a node will
experience a hardware failure within a day (left-most set of
bars), a week (middle set of bars) and a month (right-most set
of bars) after experiencing a power outage, a power spike,
a power supply failure or a UPS failure, compared to the
probability of a hardware failure in a random day, week, month
(i.e. not necessarily preceded by a power issue).
We observe that generally after power issues the probability
of seeing hardware failures in LANL nodes is signiﬁcantly
increased.
Interestingly, while power outages and power
supply failures caused a signiﬁcant
increase in hardware
failures both in the short-term (within a day following
the power problem) and in the long-term (within a month
following the power problem), the effect of power spikes is
more apparent at longer timespans. In the long-term, all four
types of power issues lead to an increase in the hardware
failure probability by factors of 5-10X.
1) What types of hardware failures are most affected by
power problems?: Figure 10 (right) shows the probabilities for
different types of hardware failures to occur within a month
of a power outage, power spike, power supply failure or a
UPS failure, compared to the probabilities of those failures in
a random month (not preceded by power issues).
We observe that a large range of hardware components,
including memory DIMMs, node boards, and power supplies,
show markedly increased failure rates
following power
problems. The only component that showed no clear signs of
increased failure rates after any of the power problems are
CPUs. For the other components the degree at which failure
rates increase depends on the type of power problem that
preceded. After power outages the node board and power
supply show the biggest increase in their failure rates (factors
of 16-20X). These components also show similar failure rates
following power spikes. Memory DIMMs show a higher
failure rate following power spikes, compared to power
outages, with an increase of 13.7X compared to 5X. For all
components the increase in failure rates is strongest following
a power supply failure, and ranges from more than 40X
for fans and power supplies, to 14X and 28X for memory
DIMMs and node boards. Two components show high failure
rates following UPS failures: node boards (27.3X increase)
and memory DIMMs (8.9 increase).
2) Do power problems cause issues in addition to node
failures?: When analyzing the LANL data to investigate
the consequences of power problems, we also made another
interesting observation. In addition to the clearly increased
number of node outages due to failures following a power
problem, we observe a large increase in the number of non-
scheduled maintenance events related to hardware problems.
Within a month after a power outage or power spike, around
25% of affected nodes need to undergo unscheduled downtime
due to maintenance. This is an increase of nearly 90X in the
frequency of unscheduled maintenance compared to a random
month in a node’s lifetime. In the month after a power supply
failure maintenance activity is also markedly increased: a node
has an 8% chance of requiring hardware-related maintenance
work within a month after a power supply failure, which is
lower than after a power outage or spike, but still nearly 30X
higher than in a random month. Failures in the UPS system had
the strongest effect, increasing a node’s chance of undergoing
unscheduled maintenance by a factor of 100X (28% chance).
These results indicate that problems with power not only
lead to hardware problems that cause a node to fail, but
also a signiﬁcant amount of downtime due to unscheduled
maintenance.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:51:27 UTC from IEEE Xplore.  Restrictions apply. 
e
r
u
l
i
a
F
e
r
a
w
t
f
o
S
a
f
o
y
t
i
l
i
b
a
b
o
r
P
0.8
0.6
0.4
0.2
0
D
I
e
d
o
N
50
40
30
20
10
0
0
Average Timespan
After Power Outages
After Power Spikes
After Power Supply Fails
After UPS Fails
22.7x
33.1x
45.1x
29.0x
22.9x
12.4x
 9x
32.8x
12x
Week
Month
23.5x
13x
  NA 
Day
y
t
i
l
i
b
a
b
o
r
P
0.8
0.6
0.4
0.2
0
 81.6x
43.8x
 49.9x
 28.6x
22x
12.9x
19x
119.3x
In a random month
After power outages
After power spikes
After power supply fails
After UPS Fails
 32.0x
DST
OtherSW
PatchInstl
OS
266.7x
79.5x
22x
  NA  
  6.9x
 5.0x
5.2x
 11.8x
 39.3x
 NA
  NA  
 NA  
PFS
 4x
  NA  
 NA  
CFS
LANL System 2
LANL System 2
LANL System 2
LANL System 2
Fig. 11.
Impact of power problems on software failures
Power Outage
40
Power Spike
D
I
e
d
o
N
30
20
10
1000
2000
Time (day)
3000
4000
0
0
500
50
40
30
20
10
D
I
e
d
o
N
Power Supply Fail
50
40
30
20
10
D
I
e
d
o
N
UPS Failure
1000
1500
Time (day)
2000
2500
0
0
500
1000
1500
Time (day)
2000
2500
3000
0
0
500
Time (day)
1000
1500
Fig. 12. Distribution of power-related failures across nodes over time (LANL System 2)
B. How do power problems affect software failures?
Figure 11 (left) shows the probability that a node will
experience a software failure within a day (left-most set of
bars), a week (middle set of bars) and a month (right-most set
of bars) after experiencing a power outage, a power spike,
a power supply failure or a UPS failure, compared to the
probability of a software failure in a random day, week, month
(i.e. not necessarily preceded by a power issue).
As was the case for hardware failures, we observe that
after power issues the probability of seeing software failures
in LANL nodes is signiﬁcantly increased. We observe the
strongest effect for power outages and UPS failures, which
increase the probability of a software failure within a week by
factors of 45X and 29X, respectively. Power spikes and power
supply failures had a somewhat weaker effect, with factors of
10-20X, but still very strong. All four types of power problems
show longer-term effects, as evidenced when looking at the
software failure rates following the month of power problem,
although the effects are weaker than the weekly ones (except
for UPS failures).
1) What types of software failures are most affected by
power problems?: Figure 11 (right) shows a breakdown of
software failures into their more detailed underlying root
causes and for each of these underlying root causes the
associated probability within a month after a power outage,
power spike, power supply failure and UPS failure. We observe
that the majority of the software-related outages following
power issues are related to the system’s distributed storage
system (DST). Some additional issues are related to Parallel
File System (PFS) and the Cluster File System (CFS).
In summary, we observe that a large fraction of software
issues created by power problems are related to data storage
(either the distributed storage system or the ﬁle system), rather
than general operating system issues or other software issues.
While the data does not provide details on the nature of those
storage and ﬁle system failures, the loss of power likely led to
some inconsistency in the storage or ﬁle system state. All ﬁle
and storage systems for HPC installations provide mechanisms
to protect against loss of consistency or persistence in the case
of crashes or power outages, so it’s interesting to observe that
despite those efforts power problems still remain a high risk
factor for those systems.
C. How are power problems laid out in time and space?
Figure 12 illustrates how the four different types of power
problems (outages, spikes, UPS and power supply failures)
are laid out in time and space using the data for all System 2
nodes. We chose System 2 as it provides the largest data set
on power issues. We observe that the different power problems
vary in how they are correlated in time and space. While power
outages and UPS failures show clear correlations between
nodes and also over time within the same node, power spikes
tend to happen in more random unpredictable ways. Power
supply failures are the most common type of power-related
failure and show only correlations within the same node.
VIII. HOW DOES TEMPERATURE AFFECT FAILURES?
Understanding the effect of temperature on system reliabil-
ity is important as a large fraction of a datacenter’s energy bill
goes into cooling. Recent work [3] indicates that the impact of
temperature on hardware components might be much weaker
than often assumed and reports for some types of errors, such
as DRAM errors no correlation at all.
A. How does average temperature affect failures?
LANL has provided event logs for some of their systems,
in addition to the logs of failure outages. For one of LANL’s
systems (system 20) periodic temperature measurements from
a motherboard sensor are available. It is worth noting that the
cooling mechanism used in the facilities hosting the rest of
LANL’s systems was similar to system 20’s: hot-aisle cold-
aisle air cooling through perforated ﬂoors.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:51:27 UTC from IEEE Xplore.  Restrictions apply. 
0.6
e
r
u
l
i
a
F
e
r
a
w
d
r
a
H
a
f
o
y
t
i
l
i
b
a
b
o
r
P
0.5
0.4
0.3
0.2
0.1
0
Average Timespan
After Chillers Fail
After Fan Failures
5.99x
 7.09x
12.19x
7.30x
41.55x
9.36x
Day
Week
Month
y
t
i
l
i
b
a
b
o
r
P
0.25
0.2
0.15
0.1
0.05
0
 11.5x
 5.3x
 23.9x
10.8x
120.3x
 17.8x
 NA  
PowerSupply Memory
NodeBoard
In a random month
After chiller fails
After fan fails
288.2x
105.7x
 NA  
Fan
  0.2x
 NA  
CPU
 NA  
 NA  
MSC Board
MidPlane
Fig. 13.
Impact of temperature related problems on hardware failures
The work in [3] uses this data for system 20 to study
how node outages, either due to hardware failures in general
or DRAM failures in particular, change with temperature.