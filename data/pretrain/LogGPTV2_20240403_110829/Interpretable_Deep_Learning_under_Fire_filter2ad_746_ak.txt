Defense 2: Adversarial Interpretation – Along the sec-
ond direction, we explore the idea of adversarial training.
Recall that ADV2 exploit the prediction-interpretation gap to
generate adversarial inputs. Here we employ ADV2 as a drive
to minimize this gap during training interpreters.
Speciﬁcally, we propose an adversarial interpretation dis-
tillation (AID) framework. Let A be the ADV2 attack. During
training an interpreter g, for a given input x◦, besides the
regular loss (cid:96)map(x◦), we consider an additional loss term
(cid:96)aid(x◦) = −(cid:107)g(x◦)− g(A(x◦))(cid:107)1, which is the negative L1
measure between the attribution maps of x◦ and its adversar-
ial counterpart A(x◦). We encourage g to minimize this loss
during the training (details in Appendix A2).
To assess the effectiveness of AID to reduce the prediction-
interpretation gap, we use RTS as a concrete case study. Recall
that RTS is a model-guided interpreter which directly predicts
the interpretation of a given input. We construct two variants
of RTS, a regular one and another with AID training (denoted
by RTSA). We measure the sensitivity of the two interpreters
to the underlying DNN behavior.
Figure 15: Attribution maps generated by RTS and RTSA under
different noise levels and types (normal N, uniform U) on ResNet.
In the ﬁrst case, we inject random noise (either normal or
uniform) to the inputs and compare the attribution maps gen-
erated by the two interpreters. We consider two noise levels,
which respectively cause 3% and 30% misclassiﬁcation on
the test set. Figure 15 shows a set of misclassiﬁed samples
under the two noise levels. Observe that compared with RTS,
RTSA appears much more sensitive to the DNN’s behavior
change, by generating highly contrastive maps. This sensi-
tivity is also quantitatively conﬁrmed by the L1 measures
between clean and noisy maps on RTS and RTSA. The ﬁnd-
ings also corroborate a similar phenomenon observed in [59]:
the representations generated by robust models tend to align
better with salient data characteristics.
Figure 16: Attribution maps of benign and adversarial (ADV2) inputs
with respect to RTS and RTSA on ResNet.
In the second case, we assess the resilience of RTSA against
1670    29th USENIX Security Symposium
USENIX Association
3%30%NormalUniform3%30%NoiseNU0.010.010%0.010.010.010.013%0.020.030.090.1030%0.170.17Figure15:AttributionmapsgeneratedbyregularRL1measuresRTSRTSAInput+NoiseRTSRTSAInputAAACVHicbVBNTxsxEPUuUGjaUijHXlaESj1Fu7QVHBG9tDeQGkDCKfJOJsSKP1b2LBBZ+z+4tj+qUv8LB7whUiH0SZae3rzxzLyyUtJTnv9N0qXllReray87r16/WX+7sfnuxNvaAfbBKuvOSuFRSYN9kqTwrHIodKnwtJx8beunV+i8tOYHTSscaHFp5EiCoCj95FrQ2Onw3VQ1NRcb3byXz5A9J8WcdNkcRxebSc6HFmqNhkAJ78+LvKJBEI4kKGw6vPZYCZiISzyP1AiNfhBmazfZh6gMs5F18RnKZurjjiC091NdRme7pl+steJ/a6VemEyj/UGQ7Y1o4GHwqFYZ2azNJBtKh0BqGokAJ+PuGYyFE0AxuQ43eA1Wa2GGgQNIB03gE3Qm733BG34F8Xh0gY9LexN2uI8/VORpqpC35p2m+eduOjHjYjHR5+Rkt1d86u0ef+4eHM7TXmPv2Tb7yAq2xw7YN3bE+gyYY7fsF/ud/Enu0qV05cGaJvOeLfYE6fo9+ga2CQ==BenignAAACV3icbVDLbhMxFPUM0IbwaAJLNiNSJFbRTAHBsiqbLotE2kp1FHlubhIrfozsO20ja76EbftR/RrqSSMBKUeydHTuuQ+fslLSU57fJemTp892djvPuy9evnq91+u/OfW2doAjsMq681J4VNLgiCQpPK8cCl0qPCuX39v62SU6L635SasKx1rMjZxJEBSlSW+Pa0ELp8MRGjk3zaQ3yIf5GtljUmzIgG1wMuknOZ9aqDUaAiW8vyjyisZBOJKgsOny2mMlYCnmeBGpERr9OKwvb7IPUZlmM+viM5St1b87gtDer3QZne2dfrvWiv+tlXprM82+jYM0VU1o4GHxrFYZ2ayNJZtKh0BqFYkAJ+PtGSyEE0AxvC43eAVWa2GmgQNIB03gS3QmH37Ba34J8fPoAl+U9jrscx8nVORppZC35v2m+eNuujHjYjvRx+T0YFh8Gh78+Dw4PNqk3WHv2Hv2kRXsKztkx+yEjRiwmv1iN+w2uUt+pztp58GaJpuet+wfpP17JmG2hw==ADV2AAACVHicbVBdTxNBFJ1dRKEiAj7ysrGY8NTsVo0+4seDj5jYQsIUMnt7Syedj83MXaSZ7P/gVX4Uif/FB2dLE7V4kklOzj33Y05ZKekpz38m6dqj9cdPNjY7T7eebT/f2d0bels7wAFYZd1pKTwqaXBAkhSeVg6FLhWelLNPbf3kCp2X1nyjeYUjLS6NnEgQFKVzrgVNnQ4fPg/P+83FTjfv5QtkD0mxJF22xPHFbpLzsYVaoyFQwvuzIq9oFIQjCQqbDq89VgJm4hLPIjVCox+FxdlN9ioq42xiXXyGsoX6d0cQ2vu5LqOzPdOv1lrxv7VSr2ymyftRkKaqCQ3cL57UKiObtZlkY+kQSM0jEeBkvD2DqXACKCbX4Qa/g9VamHHgANJBE/gMncl7b/GaX0H8PLrAp6W9DgfcxwkVeZor5K35oGn+uJtOzLhYTfQhGfZ7xete/+ub7tHHZdobbJ+9ZIesYO/YEfvCjtmAAXPshv1gt8ld8itdS9fvrWmy7HnB/kG6/RvBGrVkRTSAAACUnicbVJNbxMxEPWm9IP0uxy5rEgr9RTttkX0WMGFY4CmrVRHlXcyaaz4Y2XPlkbW/g2u8KO48Fc44U0iAWlHsvT03hvP+MlFqaSnLPuVtFZerK6tb7xsb25t7+zu7R9ceVs5wD5YZd1NITwqabBPkhTelA6FLhReF5MPjX79gM5Lay5pWuJAi3sjRxIERYpzLWjsdPh8+aW+2+tk3WxW6VOQL0CHLap3t59kfGih0mgIlPD+Ns9KGgThSILCus0rj6WAibjH2wiN0OgHYbZ0nR5FZpiOrIvHUDpj/+0IQns/1UV0Nkv6Za0hn9UKvTSZRueDIE1ZERqYDx5VKiWbNomkQ+kQSE0jEOBk3D2FsXACKObW5ga/gtVamGHgANJBHfgEncm6b/GRP0B8PLrAx4V9DIfcxxtK8jRVyBvzYV3/ddftmHG+nOhTcHXSzU+7J5/OOhfvF2lvsNfsDTtmOXvHLthH1mN9Bqxk39h39iP5mfxuxV8yt7aSRc8r9l+1tv4Ali203g==RTSAAAACVHicbVDLThRBFK1uQHFUBF266TCYuJp0o0aWqBuWCAyQUAOpvnOHqUw9OlW3gUml/8OtfJSJ/+LC6mESZeAklZyce+6jTlkp6SnPfyfp0vLKk6erzzrPX7xce7W+8frY29oB9sEq605L4VFJg32SpPC0cih0qfCknHxr6ydX6Ly05oimFQ60uDRyJEFQlM65FjR2OhwcHZ5/aS7Wu3kvnyF7SIo56bI59i82kpwPLdQaDYES3p8VeUWDIBxJUNh0eO2xEjARl3gWqREa/SDMzm6yd1EZZiPr4jOUzdT/O4LQ3k91GZ3tmX6x1oqP1kq9sJlGO4MgTVUTGrhbPKpVRjZrM8mG0iGQmkYiwMl4ewZj4QRQTK7DDV6D1VqYYeAA0kET+ASdyXuf8IZfQfw8usDHpb0JW9zHCRV5mirkrXmraf65m07MuFhM9CE53u4VH3rb3z92d7/O015lb9kme88K9pntsj22z/oMmGM/2E92m/xK/qRL6cqdNU3mPW/YPaRrfwEWq7WRRTSAAACUnicbVJNbxMxEPWm9IP0uxy5rEgr9RTttkX0WMGFY4CmrVRHlXcyaaz4Y2XPlkbW/g2u8KO48Fc44U0iAWlHsvT03hvP+MlFqaSnLPuVtFZerK6tb7xsb25t7+zu7R9ceVs5wD5YZd1NITwqabBPkhTelA6FLhReF5MPjX79gM5Lay5pWuJAi3sjRxIERYpzLWjsdPh8+aW+2+tk3WxW6VOQL0CHLap3t59kfGih0mgIlPD+Ns9KGgThSILCus0rj6WAibjH2wiN0OgHYbZ0nR5FZpiOrIvHUDpj/+0IQns/1UV0Nkv6Za0hn9UKvTSZRueDIE1ZERqYDx5VKiWbNomkQ+kQSE0jEOBk3D2FsXACKObW5ga/gtVamGHgANJBHfgEncm6b/GRP0B8PLrAx4V9DIfcxxtK8jRVyBvzYV3/ddftmHG+nOhTcHXSzU+7J5/OOhfvF2lvsNfsDTtmOXvHLthH1mN9Bqxk39h39iP5mfxuxV8yt7aSRc8r9l+1tv4Ali203g==RTSAAAACVHicbVDLThRBFK1uQHFUBF266TCYuJp0o0aWqBuWCAyQUAOpvnOHqUw9OlW3gUml/8OtfJSJ/+LC6mESZeAklZyce+6jTlkp6SnPfyfp0vLKk6erzzrPX7xce7W+8frY29oB9sEq605L4VFJg32SpPC0cih0qfCknHxr6ydX6Ly05oimFQ60uDRyJEFQlM65FjR2OhwcHZ5/aS7Wu3kvnyF7SIo56bI59i82kpwPLdQaDYES3p8VeUWDIBxJUNh0eO2xEjARl3gWqREa/SDMzm6yd1EZZiPr4jOUzdT/O4LQ3k91GZ3tmX6x1oqP1kq9sJlGO4MgTVUTGrhbPKpVRjZrM8mG0iGQmkYiwMl4ewZj4QRQTK7DDV6D1VqYYeAA0kET+ASdyXuf8IZfQfw8usDHpb0JW9zHCRV5mirkrXmraf65m07MuFhM9CE53u4VH3rb3z92d7/O015lb9kme88K9pntsj22z/oMmGM/2E92m/xK/qRL6cqdNU3mPW/YPaRrfwEWq7WRL1measuresAAACa3icbVDLbhMxFHWmPEp4tKU7YGGRVmKBopnSCpYV3bBgUSTSVqqj6M7NTWPFj5HtKY2s+YB+DVv4lH4E/4AnjQSkXMvS0bnnvk5ZKelDnt90srV79x88XH/Uffzk6bONza3nJ97WDmmAVll3VoInJQ0NggyKzipHoEtFp+XsqM2fXpLz0pqvYV7RUMOFkROJEBI12uwJDWGKoOLnZlRw8Ta9lnE6agJfO/JNUuX9fBH8LiiWoMeWcTza6uRibLHWZAIq8P68yKswjOCCREVNV9SeKsAZXNB5ggY0+WFcXNPw3cSM+cS69E3gC/bvigja+7kuk7Ld1K/mWvK/uVKvTA6TD8MoTVUHMng7eFIrHixvreJj6QiDmicA6GTaneMUHGBIhnaFoW9otQYzjgJROmyimJEzef+ArsQlpuPJRTEt7VXcET51qIIPc0WiFe80zR91000eF6uO3gUne/3iXX/vy37v8OPS7XX2kr1mb1jB3rND9okdswFDds2+sx/sZ+dXtp29yF7dSrPOsmab/RPZ7m9pEr2ctheDNN’sbehaviorchange,bygeneratinghighlycontrastivemaps.ThissensitivityisalsoquantitativelyconﬁrmedbytheL1distancebetweenthecleanandnoisyattributionmaps.InputAAACVHicbVBNTxsxEPUuUGjaUijHXlaESj1Fu7QVHBG9tDeQGkDCKfJOJsSKP1b2LBBZ+z+4tj+qUv8LB7whUiH0SZae3rzxzLyyUtJTnv9N0qXllReray87r16/WX+7sfnuxNvaAfbBKuvOSuFRSYN9kqTwrHIodKnwtJx8beunV+i8tOYHTSscaHFp5EiCoCj95FrQ2Onw3VQ1NRcb3byXz5A9J8WcdNkcRxebSc6HFmqNhkAJ78+LvKJBEI4kKGw6vPZYCZiISzyP1AiNfhBmazfZh6gMs5F18RnKZurjjiC091NdRme7pl+steJ/a6VemEyj/UGQ7Y1o4GHwqFYZ2azNJBtKh0BqGokAJ+PuGYyFE0AxuQ43eA1Wa2GGgQNIB03gE3Qm733BG34F8Xh0gY9LexN2uI8/VORpqpC35p2m+eduOjHjYjHR5+Rkt1d86u0ef+4eHM7TXmPv2Tb7yAq2xw7YN3bE+gyYY7fsF/ud/Enu0qV05cGaJvOeLfYE6fo9+ga2CQ==BenignAAACV3icbVDLbhMxFPUM0IbwaAJLNiNSJFbRTAHBsiqbLotE2kp1FHlubhIrfozsO20ja76EbftR/RrqSSMBKUeydHTuuQ+fslLSU57fJemTp892djvPuy9evnq91+u/OfW2doAjsMq681J4VNLgiCQpPK8cCl0qPCuX39v62SU6L635SasKx1rMjZxJEBSlSW+Pa0ELp8MRGjk3zaQ3yIf5GtljUmzIgG1wMuknOZ9aqDUaAiW8vyjyisZBOJKgsOny2mMlYCnmeBGpERr9OKwvb7IPUZlmM+viM5St1b87gtDer3QZne2dfrvWiv+tlXprM82+jYM0VU1o4GHxrFYZ2ayNJZtKh0BqFYkAJ+PtGSyEE0AxvC43eAVWa2GmgQNIB03gS3QmH37Ba34J8fPoAl+U9jrscx8nVORppZC35v2m+eNuujHjYjvRx+T0YFh8Gh78+Dw4PNqk3WHv2Hv2kRXsKztkx+yEjRiwmv1iN+w2uUt+pztp58GaJpuet+wfpP17JmG2hw==ADV2AAACVHicbVBdTxNBFJ1dRKEiAj7ysrGY8NTsVo0+4seDj5jYQsIUMnt7Syedj83MXaSZ7P/gVX4Uif/FB2dLE7V4kklOzj33Y05ZKekpz38m6dqj9cdPNjY7T7eebT/f2d0bels7wAFYZd1pKTwqaXBAkhSeVg6FLhWelLNPbf3kCp2X1nyjeYUjLS6NnEgQFKVzrgVNnQ4fPg/P+83FTjfv5QtkD0mxJF22xPHFbpLzsYVaoyFQwvuzIq9oFIQjCQqbDq89VgJm4hLPIjVCox+FxdlN9ioq42xiXXyGsoX6d0cQ2vu5LqOzPdOv1lrxv7VSr2ymyftRkKaqCQ3cL57UKiObtZlkY+kQSM0jEeBkvD2DqXACKCbX4Qa/g9VamHHgANJBE/gMncl7b/GaX0H8PLrAp6W9DgfcxwkVeZor5K35oGn+uJtOzLhYTfQhGfZ7xete/+ub7tHHZdobbJ+9ZIesYO/YEfvCjtmAAXPshv1gt8ld8itdS9fvrWmy7HnB/kG6/RvBGrVkRTSAAACUnicbVJNbxMxEPWm9IP0uxy5rEgr9RTttkX0WMGFY4CmrVRHlXcyaaz4Y2XPlkbW/g2u8KO48Fc44U0iAWlHsvT03hvP+MlFqaSnLPuVtFZerK6tb7xsb25t7+zu7R9ceVs5wD5YZd1NITwqabBPkhTelA6FLhReF5MPjX79gM5Lay5pWuJAi3sjRxIERYpzLWjsdPh8+aW+2+tk3WxW6VOQL0CHLap3t59kfGih0mgIlPD+Ns9KGgThSILCus0rj6WAibjH2wiN0OgHYbZ0nR5FZpiOrIvHUDpj/+0IQns/1UV0Nkv6Za0hn9UKvTSZRueDIE1ZERqYDx5VKiWbNomkQ+kQSE0jEOBk3D2FsXACKObW5ga/gtVamGHgANJBHfgEncm6b/GRP0B8PLrAx4V9DIfcxxtK8jRVyBvzYV3/ddftmHG+nOhTcHXSzU+7J5/OOhfvF2lvsNfsDTtmOXvHLthH1mN9Bqxk39h39iP5mfxuxV8yt7aSRc8r9l+1tv4Ali203g==RTSAAAACVHicbVDLThRBFK1uQHFUBF266TCYuJp0o0aWqBuWCAyQUAOpvnOHqUw9OlW3gUml/8OtfJSJ/+LC6mESZeAklZyce+6jTlkp6SnPfyfp0vLKk6erzzrPX7xce7W+8frY29oB9sEq605L4VFJg32SpPC0cih0qfCknHxr6ydX6Ly05oimFQ60uDRyJEFQlM65FjR2OhwcHZ5/aS7Wu3kvnyF7SIo56bI59i82kpwPLdQaDYES3p8VeUWDIBxJUNh0eO2xEjARl3gWqREa/SDMzm6yd1EZZiPr4jOUzdT/O4LQ3k91GZ3tmX6x1oqP1kq9sJlGO4MgTVUTGrhbPKpVRjZrM8mG0iGQmkYiwMl4ewZj4QRQTK7DDV6D1VqYYeAA0kET+ASdyXuf8IZfQfw8usDHpb0JW9zHCRV5mirkrXmraf65m07MuFhM9CE53u4VH3rb3z92d7/O015lb9kme88K9pntsj22z/oMmGM/2E92m/xK/qRL6cqdNU3mPW/YPaRrfwEWq7WRRTSAAACUnicbVJNbxMxEPWm9IP0uxy5rEgr9RTttkX0WMGFY4CmrVRHlXcyaaz4Y2XPlkbW/g2u8KO48Fc44U0iAWlHsvT03hvP+MlFqaSnLPuVtFZerK6tb7xsb25t7+zu7R9ceVs5wD5YZd1NITwqabBPkhTelA6FLhReF5MPjX79gM5Lay5pWuJAi3sjRxIERYpzLWjsdPh8+aW+2+tk3WxW6VOQL0CHLap3t59kfGih0mgIlPD+Ns9KGgThSILCus0rj6WAibjH2wiN0OgHYbZ0nR5FZpiOrIvHUDpj/+0IQns/1UV0Nkv6Za0hn9UKvTSZRueDIE1ZERqYDx5VKiWbNomkQ+kQSE0jEOBk3D2FsXACKObW5ga/gtVamGHgANJBHfgEncm6b/GRP0B8PLrAx4V9DIfcxxtK8jRVyBvzYV3/ddftmHG+nOhTcHXSzU+7J5/OOhfvF2lvsNfsDTtmOXvHLthH1mN9Bqxk39h39iP5mfxuxV8yt7aSRc8r9l+1tv4Ali203g==RTSAAAACVHicbVDLThRBFK1uQHFUBF266TCYuJp0o0aWqBuWCAyQUAOpvnOHqUw9OlW3gUml/8OtfJSJ/+LC6mESZeAklZyce+6jTlkp6SnPfyfp0vLKk6erzzrPX7xce7W+8frY29oB9sEq605L4VFJg32SpPC0cih0qfCknHxr6ydX6Ly05oimFQ60uDRyJEFQlM65FjR2OhwcHZ5/aS7Wu3kvnyF7SIo56bI59i82kpwPLdQaDYES3p8VeUWDIBxJUNh0eO2xEjARl3gWqREa/SDMzm6yd1EZZiPr4jOUzdT/O4LQ3k91GZ3tmX6x1oqP1kq9sJlGO4MgTVUTGrhbPKpVRjZrM8mG0iGQmkYiwMl4ewZj4QRQTK7DDV6D1VqYYeAA0kET+ASdyXuf8IZfQfw8usDHpb0JW9zHCRV5mirkrXmraf65m07MuFhM9CE53u4VH3rb3z92d7/O015lb9kme88K9pntsj22z/oMmGM/2E92m/xK/qRL6cqdNU3mPW/YPaRrfwEWq7WRtheDNN’sbehaviorchange,bygeneratinghighlycontrastivemaps.ThissensitivityisalsoquantitativelyconﬁrmedbytheL1distancebetweenthecleanandnoisyattributionmaps.RTSAAACUnicbVJNTxsxEPWmH9CUttAee1k1VOop2oVW5YjKpUeg+ZJwhLyTWWLFHyt7lhJZ+ze4wo/qpX+FE06I1DZ0JEtP773xjJ9cVEp6yrLfSevJ02fPNzZftF9uvXr9Znvn7cDb2gH2wSrrRoXwqKTBPklSOKocCl0oHBazo4U+vETnpTU9mlc41uLCyFKCoEhxrgVNfRlOez+a8+1O1s2WlT4G+Qp02KqOz3eSjE8s1BoNgRLen+VZReMgHElQ2LR57bESMBMXeBahERr9OCyXbtKPkZmkpXXxGEqX7N8dQWjv57qIzuWS69qC/K9W6LXJVB6MgzRVTWjgYXBZq5RsukgknUiHQGoegQAn4+4pTIUTQDG3Njf4E6zWwkwCB5AOmsBn6EzW/YJX/BLi49EFPi3sVdjlPt5Qkae5Qr4w7zbNH3fTjhnn64k+BoO9br7f3Tv53Dn8tkp7k71nH9gnlrOv7JB9Z8esz4BV7JrdsNvkV3LXir/kwdpKVj3v2D/V2roHir602A==Figure16:Attributionmapsofbenignandadversarial(ADV2)inputsonRTSandRTSA.Inthesecondcase,weassesstheresilienceofRTSAagainstADV2.InFigure??,wecomparetheattributionmapsofbe-nignandadversarialinputsonRTSandRTSA.ItisobservedthatwhileADV2generatesadversarialinputswithinterpreta-tionsfairlysimilartobenigncasesonRTS,itfailstodosoonRTSA:themapsofadversarialinputsarefairlydistinguish-ablefromtheirbenigncounterparts.Moreover,RTSAbehavesalmostidenticallytoRTSonbenigninputs,indicatingthattheAIDtraininghaslittleimpactonbenigncases.Theseobser-vationsareconﬁrmedbytheL1measuresaswell.RTSRTSABenign0.03ADV20.010.10Table10.ComparisonofAIDandADV2withcorrespondedbenignmaps,measuredbyL1distance.Overallwehavethefollowingconclusion.Observation8ItispossibletoexploitADV2toreducetheprediction-interpretationgapintraininginterpreters.6RelatedWorkInthissection,wesurveythreecategoriesofworkrele-vanttothiswork,namely,adversarialattacksanddefenses,transferability,andinterpretability.AttacksandDefenses–Duetotheirwidespreaduseinsecurity-criticaldomains,machinelearningmodelsareincreasinglybecomingthetargetsofmaliciousattacks[?].Twoprimarythreatmodelsareconsideredinliterature.Poi-soningattacks–theadversarypollutesthetrainingdatatoeventuallycompromisethetargetmodels[?,?,?];Evasionattacks–theadversarymanipulatestheinputdataduringin-ferencetotriggertargetmodelstomisbehave[?,?,?].Comparedwithsimplemodels(e.g.,supportvectorma-chines),securingdeepneuralnetworks(DNNs)inadversar-ialsettingsentailsmorechallengesduetotheirsigniﬁcantlyhighermodelcomplexity[?].OnelineofworkfocusesondevelopingnewevasionattacksagainstDNNs[?,?,?,?,?].AnotherlineofworkattemptstoimproveDNNresilienceagainstsuchattacksbyinventingnewtrainingandinferencestrategies[?,?,?,?].Yet,suchdefensesareoftencircum-ventedbymorepowerfulattacks[?]oradaptivelyengineeredadversarialinputs[?,?],resultinginaconstantarmsracebe-tweenattackersanddefenders[?].ThisworkisamongtheﬁrsttoexploreattacksagainstDNNswithinterpretabilityasameansofdefense.Transferability–Oneintriguingpropertyofadversarialattacksistheirtransferability[?]:adversarialinputscraftedagainstoneDNNisofteneffectiveagainstanotherone.Thispropertyenablesblack-boxattacks:theadversarygeneratesadversarialinputsbasedonasurrogateDNNandapplythemonthetargetmodel[?,?,?].Todefendagainstsuchattacks,themethodofensembleadversarialtraining[?]hasbeenpro-posed,whichtrainsDNNsusingdataaugmentedwithadver-sarialinputscraftedonothermodels.Thisworkcomplementsthislineofworkbyinvestigat-ingthetransferabilityofadversarialinputsacrossdifferentinterpretationmodels.Interpretability–Aplethoraofinterpretationmodelshavebeenproposedtoprovideinterpretabilityforblack-boxDNNs,usingtechniquesbasedonback-propagation[?,?,?],intermediaterepresentations[?,?,?],inputperturbation[?],andmetamodels[?].Theimprovedinterpretabilityisbelievedtoofferasenseofsecuritybyinvolvinghumaninthedecision-makingpro-cess.ExistingworkhasexploitedinterpretabilitytodebugDNNs[?],digestsecurityanalysisresults[?],anddetectad-versarialinputs[?,?].Intuitively,asadversarialinputscauseunexpectedDNNbehaviors,theinterpretationofDNNdy-namicsisexpectedtodiffersigniﬁcantlybetweenbenignandadversarialinputs.However,recentworkempiricallyshowsthatsomeinter-pretationmodelsseeminsensitivetoeitherDNNsordatagen-erationprocesses[?],whiletransformationwithnoeffectonDNNs(e.g.,constantshift)maysigniﬁcantlyaffectthebe-haviorsofinterpretationmodels[?].ThisworkshowsthepossibilityofdeceivingDNNsandtheircoupledinterpretationmodelssimultaneously,imply-ingthattheimprovedinterpretabilityonlyprovideslimitedsecurityassurance,whichalsocomplementspriorworkbyexaminingthereliabilityofexistinginterpretationmodelsfromtheperspectiveofadversarialvulnerability.7ConclusionThisworkrepresentsasystematicstudyonthesecurityofinterpretabledeeplearningsystems(IDLSes).WepresentADV2,ageneralclassofattacksthatgenerateadversarialin-putsnotonlymisleadingtargetDNNsbutalsodeceivingtheircoupledinterpretationmodels.Throughextensiveempiricalevaluation,weshowtheeffectivenessofADV2againstarangeofDNNsandinterpretationmodels,implyingthattheinter-pretabilityofexistingIDLSesmaymerelyofferafalsesense13L1measuresAAACa3icbVDLbhMxFHWmPEp4tKU7YGGRVmKBopnSCpYV3bBgUSTSVqqj6M7NTWPFj5HtKY2s+YB+DVv4lH4E/4AnjQSkXMvS0bnnvk5ZKelDnt90srV79x88XH/Uffzk6bONza3nJ97WDmmAVll3VoInJQ0NggyKzipHoEtFp+XsqM2fXpLz0pqvYV7RUMOFkROJEBI12uwJDWGKoOLnZlRw8Ta9lnE6agJfO/JNUuX9fBH8LiiWoMeWcTza6uRibLHWZAIq8P68yKswjOCCREVNV9SeKsAZXNB5ggY0+WFcXNPw3cSM+cS69E3gC/bvigja+7kuk7Ld1K/mWvK/uVKvTA6TD8MoTVUHMng7eFIrHixvreJj6QiDmicA6GTaneMUHGBIhnaFoW9otQYzjgJROmyimJEzef+ArsQlpuPJRTEt7VXcET51qIIPc0WiFe80zR91000eF6uO3gUne/3iXX/vy37v8OPS7XX2kr1mb1jB3rND9okdswFDds2+sx/sZ+dXtp29yF7dSrPOsmab/RPZ7m9pEr2cFigure16:Attributionmapsofbenignandadversarial(ADV2)inputsonRTSandRTSA.Inthesecondcase,weassesstheresilienceofRTSAagainstADV2.InFigure16,wecomparetheattributionmapsofbe-nignandadversarialinputsonRTSandRTSA.ItisobservedthatwhileADV2generatesadversarialinputswithinterpreta-tionsfairlysimilartobenigncasesonRTS,itfailstodosoonRTSA:themapsofadversarialinputsarefairlydistinguish-ablefromtheirbenigncounterparts.Moreover,RTSAbehavesalmostidenticallytoRTSonbenigninputs,indicatingthattheAIDtraininghaslittleimpactonbenigncases.Theseobser-vationsareconﬁrmedbytheL1measuresaswell.RTSRTSABenign0.03ADV20.010.10Table10.ComparisonofAIDandADV2withcorrespondedbenignmaps,measuredbyL1distance.Overallwehavethefollowingconclusion.Observation8ItispossibletoexploitADV2toreducetheprediction-interpretationgapintraininginterpreters.6RelatedWorkInthissection,wesurveythreecategoriesofworkrele-vanttothiswork,namely,adversarialattacksanddefenses,transferability,andinterpretability.AttacksandDefenses–Duetotheirwidespreaduseinsecurity-criticaldomains,machinelearningmodelsareincreasinglybecomingthetargetsofmaliciousattacks[9].Twoprimarythreatmodelsareconsideredinliterature.Poi-soningattacks–theadversarypollutesthetrainingdatatoeventuallycompromisethetargetmodels[8,76,46];Evasionattacks–theadversarymanipulatestheinputdataduringin-ferencetotriggertargetmodelstomisbehave[16,40,49].Comparedwithsimplemodels(e.g.,supportvectorma-chines),securingdeepneuralnetworks(DNNs)inadversar-ialsettingsentailsmorechallengesduetotheirsigniﬁcantlyhighermodelcomplexity[35].OnelineofworkfocusesondevelopingnewevasionattacksagainstDNNs[71,24,54,13,42].AnotherlineofworkattemptstoimproveDNNre-silienceagainstsuchattacksbyinventingnewtrainingandinferencestrategies[53,44,77,41].Yet,suchdefensesareoftencircumventedbymorepowerfulattacks[13]oradap-tivelyengineeredadversarialinputs[12,5],resultinginaconstantarmsracebetweenattackersanddefenders[37].ThisworkisamongtheﬁrsttoexploreattacksagainstDNNswithinterpretabilityasameansofdefense.Transferability–Oneintriguingpropertyofadversarialattacksistheirtransferability[71]:adversarialinputscraftedagainstoneDNNisofteneffectiveagainstanotherone.Thispropertyenablesblack-boxattacks:theadversarygeneratesadversarialinputsbasedonasurrogateDNNandapplythemonthetargetmodel[51,14,39].Todefendagainstsuchat-tacks,themethodofensembleadversarialtraining[73]hasbeenproposed,whichtrainsDNNsusingdataaugmentedwithadversarialinputscraftedonothermodels.Thisworkcomplementsthislineofworkbyinvestigat-ingthetransferabilityofadversarialinputsacrossdifferentinterpretationmodels.Interpretability–Aplethoraofinterpretationmodelshavebeenproposedtoprovideinterpretabilityforblack-boxDNNs,usingtechniquesbasedonback-propagation[63,66,67],intermediaterepresentations[80,60,19],inputpertur-bation[21],andmetamodels[15].Theimprovedinterpretabilityisbelievedtoofferasenseofsecuritybyinvolvinghumaninthedecision-makingpro-cess.ExistingworkhasexploitedinterpretabilitytodebugDNNs[50],digestsecurityanalysisresults[25],anddetectadversarialinputs[38,72].Intuitively,asadversarialinputscauseunexpectedDNNbehaviors,theinterpretationofDNNdynamicsisexpectedtodiffersigniﬁcantlybetweenbenignandadversarialinputs.However,recentworkempiricallyshowsthatsomeinter-pretationmodelsseeminsensitivetoeitherDNNsordatagen-erationprocesses[1],whiletransformationwithnoeffectonDNNs(e.g.,constantshift)maysigniﬁcantlyaffectthebe-haviorsofinterpretationmodels[33].ThisworkshowsthepossibilityofdeceivingDNNsandtheircoupledinterpretationmodelssimultaneously,imply-ingthattheimprovedinterpretabilityonlyprovideslimitedsecurityassurance,whichalsocomplementspriorworkbyexaminingthereliabilityofexistinginterpretationmodelsfromtheperspectiveofadversarialvulnerability.7ConclusionThisworkrepresentsasystematicstudyonthesecurityofinterpretabledeeplearningsystems(IDLSes).WepresentADV2,ageneralclassofattacksthatgenerateadversarialin-putsnotonlymisleadingtargetDNNsbutalsodeceivingtheircoupledinterpretationmodels.Throughextensiveempiricalevaluation,weshowtheeffectivenessofADV2againstarangeofDNNsandinterpretationmodels,implyingthattheinter-pretabilityofexistingIDLSesmaymerelyofferafalsesense13ADV2. In Figure 16, we compare the attribution maps of be-
nign and adversarial inputs on RTS and RTSA. It is observed
that while ADV2 generates adversarial inputs with interpre-
tations fairly similar to benign cases on RTS, it fails to do so
on RTSA: the maps of adversarial inputs are fairly distinguish-
able from their benign counterparts. Moreover, RTSA behaves
almost identically to RTS on benign inputs, indicating that the
AID training has little impact on benign cases. These ﬁndings
are conﬁrmed by the L1 measures as well.
Overall we have the following conclusion.
Observation 9
It is possible to exploit ADV2 to reduce the prediction-
interpretation gap during training interpreters.
6 Related Work
In this section, we survey three categories of work rele-
vant to this work, namely, adversarial attacks and defenses,
transferability, and interpretability.
Attacks and Defenses – Due to their widespread use in
security-critical domains, machine learning models are in-
creasingly becoming the targets of malicious attacks. Two
primary threat models are considered in literature. Poisoning
attacks – the adversary pollutes the training data to eventually
compromise the target models [7]; Evasion attacks – the ad-
versary manipulates the input data during inference to trigger
target models to misbehave [11].
Compared with simple models (e.g., support vector ma-
chines), securing deep neural networks (DNNs) in adversar-
ial settings entails more challenges due to their signiﬁcantly
higher model complexity [29]. One line of work focuses on
developing new evasion attacks against DNNs [19, 35, 56].
Another line of work attempts to improve DNN resilience
against such attacks by inventing new training and inference
strategies [34, 42, 61]. Yet, such defenses are often circum-
vented by more powerful attacks [9] or adaptively engineered
adversarial inputs [5], resulting in a constant arms race be-
tween attackers and defenders [31].
This work is among the ﬁrst to explore attacks against
DNNs with interpretability as a means of defense.
Transferability – One intriguing property of adversarial
attacks is their transferability [56]: adversarial inputs crafted
against one DNN is often effective against another one. This
property enables black-box attacks – the adversary generates
adversarial inputs based on a surrogate DNN and then applies
them on the target model [33, 40]. To defend against such
attacks, the method of ensemble adversarial training [58] has
been proposed, which trains DNNs using data augmented
with adversarial inputs crafted on other models.
This work complements this line of work by investigat-
ing the transferability of adversarial inputs across different
interpretation models.
Interpretability – A plethora of interpretation models
have been proposed to provide interpretability for black-
box DNNs, using techniques based on back-propagation
[50, 52, 53], intermediate representations [14, 47, 64], input
perturbation [16], and meta models [10].
The improved interpretability is believed to offer a sense
of security by involving human in the decision-making pro-
cess. Existing work has exploited interpretability to debug
DNNs [39], digest security analysis results [20], and detect
adversarial inputs [32, 57]. Intuitively, as adversarial inputs
cause unexpected DNN behaviors, the interpretation of DNN
dynamics is expected to differ signiﬁcantly between benign
and adversarial inputs.
However, recent work empirically shows that some inter-
pretation models seem insensitive to either DNNs or data
generation processes [1], while transformation with no effect
on DNNs (e.g., constant shift) may signiﬁcantly affect the
behaviors of interpretation models [27].
This work shows the possibility of deceiving DNNs and
their coupled interpretation models simultaneously, imply-
ing that the improved interpretability only provides limited
security assurance, which also complements prior work by
examining the reliability of existing interpretation models
from the perspective of adversarial vulnerability.
7 Conclusion
This work represents a systematic study on the security
of interpretable deep learning systems (IDLSes). We present
ADV2, a general class of attacks that generate adversarial in-
puts not only misleading target DNNs but also deceiving their
coupled interpretation models. Through extensive empirical
evaluation, we show the effectiveness of ADV2 against a range
of DNNs and interpretation models, implying that the inter-
pretability of existing IDLSes may merely offer a false sense
of security. We identify the prediction-interpretation gap as