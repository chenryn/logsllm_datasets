### Defense 2: Adversarial Interpretation

In the second direction, we explore the concept of adversarial training. Recall that ADV2 exploits the prediction-interpretation gap to generate adversarial inputs. Here, we employ ADV2 as a mechanism to minimize this gap during the training of interpreters.

Specifically, we propose an Adversarial Interpretation Distillation (AID) framework. Let \( A \) be the ADV2 attack. During the training of an interpreter \( g \), for a given input \( x \), in addition to the regular loss \( \mathcal{L}_{\text{map}}(x) \), we introduce an additional loss term:
\[ \mathcal{L}_{\text{aid}}(x) = -\| g(x) - g(A(x)) \|_1, \]
which is the negative L1 distance between the attribution maps of \( x \) and its adversarial counterpart \( A(x) \). We encourage \( g \) to minimize this loss during training (details in Appendix A2).

To evaluate the effectiveness of AID in reducing the prediction-interpretation gap, we use RTS as a concrete case study. Recall that RTS is a model-guided interpreter that directly predicts the interpretation of a given input. We construct two variants of RTS: a regular one and another with AID training (denoted by RTSA). We measure the sensitivity of these two interpreters to the underlying DNN behavior.

#### Figure 15: Attribution Maps under Different Noise Levels and Types
We inject random noise (either normal or uniform) into the inputs and compare the attribution maps generated by the two interpreters. We consider two noise levels, which respectively cause 3% and 30% misclassification on the test set. Figure 15 shows a set of misclassified samples under the two noise levels. Compared to RTS, RTSA appears much more sensitive to the DNNâ€™s behavior change, generating highly contrastive maps. This sensitivity is also quantitatively confirmed by the L1 distances between clean and noisy maps for RTS and RTSA. These findings corroborate a similar phenomenon observed in [59]: the representations generated by robust models tend to align better with salient data characteristics.

#### Figure 16: Attribution Maps of Benign and Adversarial Inputs
In the second case, we assess the resilience of RTSA against ADV2. In Figure 16, we compare the attribution maps of benign and adversarial inputs on RTS and RTSA. It is observed that while ADV2 generates adversarial inputs with interpretations fairly similar to benign cases on RTS, it fails to do so on RTSA: the maps of adversarial inputs are easily distinguishable from their benign counterparts. Moreover, RTSA behaves almost identically to RTS on benign inputs, indicating that AID training has little impact on benign cases. These observations are confirmed by the L1 measures as well.

**Observation 8:** It is possible to exploit ADV2 to reduce the prediction-interpretation gap during the training of interpreters.

### Related Work

In this section, we survey three categories of work relevant to our study: adversarial attacks and defenses, transferability, and interpretability.

#### Attacks and Defenses
Due to their widespread use in security-critical domains, machine learning models are increasingly becoming targets of malicious attacks. Two primary threat models are considered in the literature:
- **Poisoning attacks**: The adversary pollutes the training data to eventually compromise the target models [7].
- **Evasion attacks**: The adversary manipulates the input data during inference to trigger target models to misbehave [11].

Compared with simple models (e.g., support vector machines), securing deep neural networks (DNNs) in adversarial settings presents more challenges due to their significantly higher model complexity [29]. One line of work focuses on developing new evasion attacks against DNNs [19, 35, 56]. Another line of work attempts to improve DNN resilience against such attacks by inventing new training and inference strategies [34, 42, 61]. However, such defenses are often circumvented by more powerful attacks [9] or adaptively engineered adversarial inputs [5], resulting in a constant arms race between attackers and defenders [31].

This work is among the first to explore attacks against DNNs with interpretability as a means of defense.

#### Transferability
One intriguing property of adversarial attacks is their transferability [56]: adversarial inputs crafted against one DNN are often effective against another. This property enables black-box attacks, where the adversary generates adversarial inputs based on a surrogate DNN and then applies them to the target model [33, 40]. To defend against such attacks, ensemble adversarial training [58] has been proposed, which trains DNNs using data augmented with adversarial inputs crafted on other models.

This work complements this line of research by investigating the transferability of adversarial inputs across different interpretation models.

#### Interpretability
A plethora of interpretation models have been proposed to provide interpretability for black-box DNNs, using techniques based on back-propagation [50, 52, 53], intermediate representations [14, 47, 64], input perturbation [16], and meta models [10].

Improved interpretability is believed to offer a sense of security by involving humans in the decision-making process. Existing work has exploited interpretability to debug DNNs [39], digest security analysis results [20], and detect adversarial inputs [32, 57]. Intuitively, as adversarial inputs cause unexpected DNN behaviors, the interpretation of DNN dynamics is expected to differ significantly between benign and adversarial inputs.

However, recent work empirically shows that some interpretation models seem insensitive to either DNNs or data generation processes [1], while transformations with no effect on DNNs (e.g., constant shift) may significantly affect the behaviors of interpretation models [27].

This work demonstrates the possibility of deceiving DNNs and their coupled interpretation models simultaneously, implying that improved interpretability only provides limited security assurance. This also complements prior work by examining the reliability of existing interpretation models from the perspective of adversarial vulnerability.

### Conclusion

This work represents a systematic study on the security of interpretable deep learning systems (IDLSes). We present ADV2, a general class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models. Through extensive empirical evaluation, we show the effectiveness of ADV2 against a range of DNNs and interpretation models, implying that the interpretability of existing IDLSes may merely offer a false sense of security. We identify the prediction-interpretation gap as a critical issue that needs to be addressed.