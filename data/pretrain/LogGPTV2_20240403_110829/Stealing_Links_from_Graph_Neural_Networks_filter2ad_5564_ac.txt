addition, we also calculate the 8 different distances over the
shadow dataset’s nodes’ attributes.
Attack-6: K = (F ,A∗,×). In this scenario, the adversary
has the access to the target dataset’s nodes’ attributes F and
the partial target graph A∗. As a supervised learning setting,
we build an MLP considering links from the partial graph as
the ground truth label. The adversary ﬁrst adopts the same set
of features deﬁned over posteriors obtained from the target
model as proposed in Attack-3. Then, the adversary builds
a reference model over the target dataset’s nodes’ attributes,
and calculate the same set of features over posteriors obtained
from the reference model. In the end, we further calculate the
distances of the target dataset’s nodes’ attributes as another
set of features.
Attack-7: K = (F ,A∗,D(cid:48)). This is the last attack with the
adversary having all three knowledge. The set of features for
this attack is the same as the ones used in Attack-5 (Table 3).
The only difference lies in the training phase, we can use
the partial graph from the target dataset together with the
graph from the shadow dataset as the ground truth. We expect
this leads to better performance than the one for Attack-5.
However, this attack also relies on the information of the
shadow dataset, thus, the features used here are a subset of
the ones for Attack-6, this is similar to the difference between
Attack-4 and Attack-3. Note that if the adversary does not
take the shadow dataset into consideration, this scenario is
equivalent to the one for Attack-6.
4.2 Summary
We propose 8 attack scenarios with the combination of the
knowledge that the adversary could have. They could be di-
vided into three categories.
The ﬁrst category is unsupervised attacks, i.e., Attack-0 and
Attack-2, where the adversary does not have the knowledge
about the partial graph from the target dataset or a shadow
dataset. In these scenarios, the adversary can use distance
metrics for posteriors or nodes’ attributes to infer the link.
The second category is the supervised attacks, including
Attack-3 and Attack-6, where the adversary has the knowl-
edge of the partial graph from the target dataset but does not
have a shadow dataset. In these scenarios, the adversary can
use different distances and pairwise vector operations over
nodes’ posteriors (and the corresponding entropies) from the
target model and their attributes to build features.
The third category is the transferring attacks (supervised),
including Attack-1, Attack-4, Attack-5, and Attack-7, where
the adversary has the knowledge of a shadow dataset. In these
scenarios, the adversary can use distance metrics over posteri-
ors/nodes’ attributes and pairwise operations over posteriors’
entropies as the bridge to transfer the knowledge from the
shadow dataset to perform link stealing attacks. It is worth
noting that for Attack-4 and Attack-7, if the adversary leaves
the shadow dataset out of consideration, they will not have the
dimension mismatch problem and can take the same attack
methods as Attack-3 and Attack-6, respectively.
5 Evaluation
This section presents the evaluation results of our 8 attacks.
We ﬁrst introduce our experimental setup. Then, we present
detailed results for different attacks. Finally, we summarize
our experimental ﬁndings.
5.1 Experimental Setup
Datasets. We utilize 8 public datasets, including Cite-
seer [35], Cora [35], Pubmed [35], AIDS [51], COX2 [59],
DHFR [59], ENZYMES [15], and PROTEINS_full [5], to
conduct our experiments. These datasets are widely used
as benchmark datasets for evaluating GNNs [17, 18, 35, 62].
Among them, Citeseer, Cora, and Pubmed are citation datasets
with nodes representing publications and links indicating ci-
tations among these publications. The other ﬁve datasets are
chemical datasets, each node is a molecule and each link
represents the interaction between two molecules. All these
datasets have nodes’ attributes and labels.
Datasets Conﬁguration. For each dataset, we train a target
model and a reference model. In particular, we randomly sam-
ple 10% nodes and use their ground truth labels to train the
target model and the reference model.1 Recall that several
attacks require the knowledge of the target dataset’s partial
graph. To simulate and fairly evaluate different attacks, we
construct an attack dataset which contains node pairs and
labels representing whether they are linked or not. Speciﬁ-
cally, we ﬁrst select all node pairs that are linked. Then, we
randomly sample the same number of node pairs that are
not linked. We note that such negative sampling approach
follows the common practice in the literature of link predic-
tion [2, 25, 69]. Furthermore, the main metric we use, i.e.,
AUC (introduced below), is insensitive to the class imbal-
ance issue [2, 21, 47] contrary to accuracy. Next, we split the
attack dataset randomly by half into attack training dataset
and attack testing dataset.2 We use the attack training dataset
to train our attack models when the target dataset’s partial
graph is part of the adversary’s knowledge. We use attack
testing dataset to evaluate all our attacks. For the attacks that
have a shadow dataset, we also construct an attack dataset on
the shadow dataset to train the attack model. Note that we
do not split this attack dataset because we do not use it for
evaluation.
1We do not train the reference model for attacks when F is unavailable.
2We perform additional experiments and observe that training set size
does not have a strong impact on the attack performance, results are presented
in Figure 7 in Appendix.
2674    30th USENIX Security Symposium
USENIX Association
Metric. We use AUC (area under the ROC curve) as our
main evaluation metric. AUC is frequently used in binary
classiﬁcation tasks [2, 21, 26, 32, 46, 47, 69], it is threshold
independent. For convenience, we refer to node pairs that are
linked as positive node pairs and those that are not linked as
negative node pairs. If we rank node pairs according to the
probability that there is a link between them, then AUC is
the probability that a randomly selected positive node pair
ranks higher than a randomly selected negative node pair.
When performing random guessing, i.e., we rank all node
pairs uniformly at random, the AUC value is 0.5. Note that we
also calculate Precision and Recall for all supervised attacks
(see Table 17, Table 18, Table 19, Table 20, Table 21, and
Table 22 in Appendix).
Models. We use a graph convolutional network with 2 hidden
layers for both the target model and the shadow target model,
and assume they share the same architecture (see Section 3).
Note that we also evaluate the scenario where the target model
and the shadow model have different architectures later in this
section and ﬁnd the performances of our attacks are similar.
The number of neurons in the hidden layer is set to 16. We
adopt the frequently used ReLU and softmax as activation
functions for the ﬁrst hidden layer and the second hidden layer,
respectively. Note that we append Dropout (the rate is 0.5)
to the output of the hidden layer to prevent overﬁtting. We
train 100 epochs with a learning rate of 0.01. Cross-entropy is
adopted as the loss function and we use the Adam optimizer
to update the model parameters. Our GNNs are implemented
based on publicly available code.3 Experimental results show
that our GNNs achieve similar performance as reported in
other papers. We omit them to preserve space.
We use an MLP with 2 hidden layers as the reference
model and the shadow reference model. Hyperparameters,
including the number of neurons in the hidden layer, activation
functions, loss function, optimizer, epochs, and learning rate
are the same as those of the target model.
We use an MLP with 3 hidden layers as our attack model.
The number of neurons for all hidden layers is 32. ReLU
is adopted as the activation function for hidden layers and
softmax is used as the output activation function. We append
Dropout (the rate is 0.5) to each hidden layer to prevent over-
ﬁtting. We train 50 epochs with a learning rate of 0.001. The
loss function is cross-entropy and the optimizer is Adam.
We run all experiments with this setting for 5 times and
report the average value and the standard deviation of AUC
scores. Note that for Attack-0 and Attack-2, the AUC scores
keep the same since these two attacks are unsupervised.
5.2 Attack Performance
Attack-0: K = (×,×,×). In this attack, the adversary only
relies on measuring the distance of two nodes’ posteriors ob-
3https://github.com/tkipf/gcn
Figure 1: AUC for Attack-0 on all the 8 datasets with all the
8 distance metrics. The x-axis represents the dataset and the
y-axis represents the AUC score.
tained from the target model. We compare 8 different distance
metrics and Figure 1 shows the results. First, we observe that
Correlation distance achieves the best performance followed
by Cosine distance across all datasets. In contrast, Canberra
distance performs the worst. For instance, on the Citeseer
dataset, the AUC scores for Correlation distance and Cosine
distance are 0.959 and 0.946, respectively, while the AUC
score for Canberra distance is 0.801. Note that both Correla-
tion distance and Cosine distance measure the inner product
between two vectors, or the “angle” of two vectors while other
distance metrics do not. Second, we ﬁnd that the performance
of the same metric on different datasets is different. For in-
stance, the AUC of Correlation distance on Citeseer is 0.959
compared to 0.635 on ENZYMES.
As mentioned in Section 4, unsupervised attacks could not
provide a concrete prediction. To tackle this, we propose to
use clustering, such as K-means. Concretely, we obtain a set
of node pairs’ distances, and perform K-means on these dis-
tances with K being set to 2. The cluster with lower (higher)
average distance value is considered as the set of positive (neg-
ative) node pairs. Our experiments show that this method is ef-
fective. For instance, on the Citeseer dataset, we obtain 0.788
Precision, 0.991 Recall, and 0.878 F1-Score. The complete
results are summarized in Table 15 in Appendix. Another
method we could use is to assume that the adversary has a
certain number of labeled edges, either from the target dataset
or the shadow dataset. The former follows the same setting as
our Attack-3, Attack-4, Attack-6, and Attack-7, and the latter
is equivalent to Attack-1 and Attack-5. The corresponding
results will be shown later.
Figure 2 shows the frequency of Correlation distance com-
puted on posteriors obtained from the target model for both
positive node pairs and negative node pairs in attack testing
datasets. The x-axis is the value of Correlation distance and
the y-axis is the number of pairs. A clear trend is that for all
USENIX Association
30th USENIX Security Symposium    2675
AIDSCOX2DHFRENZYMESPROTEINSfullCiteseerCoraPubmed0.50.60.70.80.91.0AUCCosineEuclideanCorrelationChebyshevBraycurtisCanberraManhattanSqeuclideanTable 4: Average AUC with standard deviation for Attack-1 on all the 8 datasets. Best results are highlighted in bold.
-
AIDS
COX2
DHFR
Target Dataset
ENZYMES
0.720 ± 0.009 0.690 ± 0.005 0.730 ± 0.010
AIDS
0.831 ± 0.005 0.739 ± 0.116
0.755 ± 0.032
COX2
0.689 ± 0.004 0.771 ± 0.004
DHFR
0.577 ± 0.044
0.747 ± 0.014 0.695 ± 0.023 0.514 ± 0.041
ENZYMES
PROTEINS_full 0.775 ± 0.020 0.821 ± 0.016 0.528 ± 0.038 0.822 ± 0.020
Citeseer
0.801 ± 0.040 0.920 ± 0.006 0.842 ± 0.036 0.846 ± 0.042
0.791 ± 0.019 0.884 ± 0.005 0.811 ± 0.024 0.804 ± 0.048
Cora
Pubmed
0.705 ± 0.039 0.796 ± 0.007 0.704 ± 0.042 0.708 ± 0.067
-
-
-
Shadow Dataset
PROTEINS_full
0.720 ± 0.005
0.832 ± 0.009
0.701 ± 0.010
0.691 ± 0.030
0.848 ± 0.015
0.869 ± 0.012
0.752 ± 0.014
-
Citeseer
Cora
Pubmed
0.689 ± 0.019 0.650 ± 0.025 0.667 ± 0.014
0.762 ± 0.009 0.773 ± 0.008 0.722 ± 0.024
0.736 ± 0.005 0.740 ± 0.003 0.663 ± 0.010
0.680 ± 0.012 0.663 ± 0.009 0.637 ± 0.018
0.823 ± 0.004 0.809 ± 0.015 0.809 ± 0.013
0.965 ± 0.001 0.942 ± 0.003
0.942 ± 0.001
0.917 ± 0.002
0.883 ± 0.006 0.885 ± 0.005
-
-
-
Figure 2: The Correlation distance distribution between nodes’ posteriors for positive node pairs and negative node pairs on all
the 8 datasets. The x-axis represents Correlation distance and the y-axis represents the number of node pairs.
datasets, the Correlation distance for positive node pairs is
much smaller than negative node pairs. We select the top 50%
of node pairs with lowest Correlation distance, group them,
and calculate the AUC for each group. Due to the space limit,
we only show the result on Pubmed (Table 5). We can see that
the AUC drops when the Correlation distance increase, which
indicates that Attack-0 works better on node pairs with lower
Correlation distance. In general, the posteriors for positive
node pairs are “closer” than that for negative node pairs. This
veriﬁes our intuition in Section 4: GNN can be considered
as an aggregation function over the neighborhoods, if two
nodes are linked, they aggregate with each other’s features
and therefore become closer.
Attack-1: K = (×,×,D(cid:48)). In this attack, the adversary can
leverage a shadow dataset. In particular, for each dataset, we
use one of the remaining datasets as the shadow dataset to
perform the attack. Table 4 summarizes the results. We leave
the blank in the diagonal because we do not use the target
dataset itself as its shadow dataset.
Table 5: AUC in different Correlation distance levels for
Attack-0 on Pubmed.
Correlation Distance
0.00-0.01
0.01-0.02
AUC
0.608
0.535
Correlation Distance
0.02-0.03
0.03-0.04
AUC
0.407
0.399
As we can see from Table 4, the AUC scores from the best-
performing shadow dataset have a consistent improvement on
almost all datasets compared to Attack-0. One exception is
the COX2 dataset in which the AUC score decreases by 0.02.
The results indicate that the adversary can indeed transfer the
knowledge from the shadow dataset to enhance her attack.
An interesting ﬁnding is that for a chemical dataset, the best
shadow dataset is normally a chemical dataset as well. Simi-
lar results can be observed for citation datasets. This shows
that it is more effective to transfer knowledge across datasets
from the same domain. To better understand this, we extract
2676    30th USENIX Security Symposium
USENIX Association
0.00.10.20.3100101102103104NumbersAIDS0.000.250.500.751.00100101102103104COX20.00.20.40.60.8100101102103104DHFR0.000.050.10100101102103104ENZYMES0.000.020.040.06101102103104105NumbersPROTEINSfull0.00.51.01.5100101102103Citeseer0.00.51.01.5100101102103Cora0.00.51.01.52.0102103104PubmedNegativeNodePairsPositiveNodePairs(a)
(b)
Figure 3: The last hidden layer’s output from the attack model of Attack-1 for 200 randomly sampled positive node pairs and
200 randomly sampled negative node pairs projected into a 2-dimension space using t-SNE. (a) Cora as the shadow dataset and