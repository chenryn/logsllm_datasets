4.3 Experiments
To demonstrate how to use the sensitivity of the CID mea-
surement to select the optimal operation point (or ﬁne-tune
an IDS) in practice, we examined several existing anomaly
detection systems and measured their accuracy, CID, under

106
102
102
l
l
)
e
u
a
v
e
t
u
o
s
b
a
n
i
(
e
v
i
t
a
v
i
r
e
D
104
102
100
|∂PPV/ ∂B|
|∂NPV/ ∂B|
|∂CID/ ∂B|
l
l
)
e
u
a
v
e
t
u
o
s
b
a
n
i
(
e
v
i
t
a
v
i
r
e
D
100
10−2
10−4
10−6
|∂PPV/ ∂α|
|∂NPV/ ∂α|
|∂CID/ ∂α|
l
l
)
e
u
a
v
e
t
u
o
s
b
a
n
i
(
e
v
i
t
a
v
i
r
e
D
100
10−2
10−4
|∂PPV/ ∂β|
|∂NPV/ ∂β|
|∂CID/ ∂β|
10−2
10−7
10−6
Base rate (B)
10−5
10−8
10−3
10−2
False Positive Rate (α)
10−1
10−6
10−3
10−2
False Negative Rate (β)
10−1
(a) Dependence on base rate analysis
(α = 0.001, β = 0.01)
(b) Dependence on false positive rate
analysis (B = 0.00001, β = 0.01)
(c) Dependence on false negative rate
analysis (B = 0.00001, α = 0.001)
Figure 6: Derivative analysis (in absolute value). In every situation CID has the highest sensitivity, compared
to P P V and N P V . For realistic situations, its derivative is always higher than other measures.
0.8
0.6
0.4
0.2
)
β
−
1
(
e
a
R
n
o
t
i
t
c
e
t
e
D
)
D
I
C
(
y
t
i
l
i
b
a
p
a
C
D
I
0.4
0.3
0.2
0.1
0
0
0
0
0.5
1
False positive rate (α)
1.5
Max C
ID
Max C
ID
0.5
1
False positive rate (α)
1.5
IDS
1
IDS
2
2
x 10−3
IDS
1
IDS
2
2
x 10−3
Figure 7: IDS1 and IDS2 ROC curves and corre-
sponding CID curves. These plots, based on values
reported by Gaﬀney and Ulvila, show how CID can
be used to select an optimal operating point. It is
not clear how simple ROC analysis could arrive at
this same threshold.
various conﬁgurations. Speciﬁcally, we used two anomaly
network intrusion detection systems, Packet Header Anomaly
Detection (PHAD) [13] and Payload Anomaly Detection
(PAYL) [25]. To demonstrate how to compare two diﬀerent
IDSs using CID, we compared an anomaly detection sys-
tem PAYL with another open source signature-based IDS,
Snort [22], in terms of their capabilities to detect Web at-
tacks based on the same testing data set.
PHAD and PAYL both detect anomalies at the packet
level, with PHAD focusing on the packet header and PAYL
using byte frequencies in the payload. We tested PHAD
using the DARPA 1999 test data set [16], using week 3 for
training and weeks 4 and 5 for testing. We conﬁgured PHAD
to monitor only HTTP traﬃc. As noted in [25], it is diﬃcult
to ﬁnd suﬃcient data in the DARPA 1999 data set to thor-
oughly test PAYL, so we used GTTrace, a backbone capture
from our campus network. The GTTrace data set consists of
approximately six hours of HTTP traﬃc captured on a very
busy 1Gb/s backbone, or approximately 1G of data. We
ﬁltered the GTTrace set to remove known attacks, split the
trace into training and testing sets, and injected numerous
HTTP attacks into the testing set, using tools such as lib-

whisker [21]. We used CID to identify an optimal setting
for each IDS.
In PHAD, a score is computed based on selected ﬁelds
in each packet header.
If this score exceeds a threshold,
then an intrusion alert is issued. Adjusting this threshold
yields diﬀerent T P, F P values, shown in Figure 8(a). We
conﬁgured PHAD to recognize the attack packets, instead
of the attack instances reported in [13].
We can see in Figure 8(a) that the CID curve almost fol-
lows the ROC curve (both like straight lines). The reason is
that with the DARPA data set, we found the false positive
rate for PHAD was fairly low, while the false negative rate
was extremely high, with β ≈ 1. As shown in our technical
report [8], given small values of α and large values of β, ROC
and CID can both be approximated as straight lines, and
the equation for CID becomes essentially K(1 − β)/H(X),
where K is a constant. We note that the authors in [13]
used PHAD to monitor traﬃc of all types, and the details
of training and testing were also diﬀerent from our experi-
ments. In particular, we conﬁgured PHAD to report each
packet involved in an attack instead of reporting the attack
instance. Therefore, our PHAD has a high β than reported
in [13].
One can argue that just selecting the point from the ROC
with the highest detection rate is an adequate way to tune
an IDS. This may be true in anecdotal cases, as illustrated
by our conﬁguration of PHAD. However, it is not always the
case, as shown in other situations such as Figure 7.
Our analysis of PHAD, therefore, illustrates a worst-case
scenario for CID. With β ≈ 1, and α ≈ 0, CID identiﬁes an
operating point no better than existing detection measure-
ments, e.g. ROC. Note, however, that CID will never return
a worse operating point.
In other situations, CID will outperform existing metrics.
Indeed, our analysis of PAYL and the GTTrace data set il-
lustrates a situation in which CID provides a better measure
than simple ROC analysis. PAYL requires the user to se-
lect an optimal threshold for determining whether observed
byte frequencies vary signiﬁcantly from a trained model. For
example, a threshold of 256 allows each character in an ob-
served payload to vary within one standard deviation of the
model [25]. As before, we can experiment with diﬀerent
threshold values, and measure the resulting F P, F N rates.
In Figure 8(b), we see that for the GTTrace data, as the
)
β
−
1
(
e
t
a
R
n
o
i
t
c
e
0.04
0.03
0.02
0.01
t
e
D
0
0
0.2
0.4
)
D
I
C
(
y
t
i
l
i
b
a
p
a
C
D
I
0.02
0.015
0.01
0.005
0
0
0.8
0.2
0.4
l
d
o
h
s
e
r
h
T
0.6
0.4
0.2
0
0
0.2
0.4
0.6
0.8
False positive rate (α)
0.6
0.8
False positive rate (α)
0.6
0.8
False positive rate (α)
(a) PHAD
1
1.2
1.4
x 10−4
1
1.2
1.4
x 10−4
)
β
−
1
(
e
t
a
R
n
o
i
t
c
e
t
e
D
)
D
I
C
(
y
t
i