-
o
t
-
d
n
e
y
a
l
r
e
v
O
Distribution of stretch (64 members)
Distribution of losses for packets in random membership change phase
i
t
a
a
d
e
v
e
c
e
r
y
l
t
c
e
r
r
o
c
t
a
h
t
s
t
s
o
h
f
o
n
o
i
t
c
a
r
F
1
0.9
0.8
0.7
0.6
0.5
0.4
64 members
Average member lifetime = 30 secs
0
100
200
300
400
500
600
700
800
900
Time (in secs)
Figure 17: Fraction of members that received data packets as
group membership continuously changed (testbed)
Cumulative distribution of losses at members in random membership change phase
A
B
C
D
E
F
G
H
Sites
Figure 15: Stretch distribution (testbed)
Distribution of latency (64 members)
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
s
r
e
b
m
e
m
f
o
n
o
i
t
c
a
r
F
A
B
C
D
E
F
G
H
Sites
0
0
0.01
Figure 16: Latency distribution (testbed)
0.02
Fraction of packets lost
0.03
0.04
0.05
in the 96-member experiment there was one member that for which
the stretch of the overlay path was 4.63).
Failure Recovery
In this section, we describethe effects of group membership changes
on the data delivery tree. To do this, we observe how successful the
overlay is in delivering data during changesto the overlay topology.
We measured the number of correctly received packets by different
(remaining) members during the rapid membership change phase of
the experiment, which begins after the initial member set has stabi-
lized into the appropriate overlay topology. This phase lasts for 15
minutes. Members join and leave the group at random such that the
average lifetime of a member in the group is 30 seconds.
In Figure 17 we plot over time the fraction of members that suc-
cessfully received the different data packets. A total of 30 group
membership changes happened over the duration. In Figure 18 we
plot the cumulative distribution of packet losses seen by the differ-
ent members over the entire 15 minute duration. The maximum num-
ber of packet losses seen by a member was 50 out of 900 (i.e. about
5.6%), and 30% of the members did not encounterany packetlosses.
Even under this rapid changes to the group membership, the largest
continuous duration of packet losses for any single host was 34 sec-
onds, while typical members experienced a maximum continuous
Figure 18: Cumulative distribution of fraction of packets lost
for different members out of the entire sequence of 900 packets
during the rapid membership change phase (testbed)
data loss for only two seconds — this was true for all but 4 of the
members. These failure recovery statistics are good enough for use
in most data stream applications deployed over the Internet. Note
that in this experiment, only three individual packets (out of 900)
suffered heavy losses: data packets at times 76 s, 620 s, and 819 s
were not received by 51, 36 and 31 members respectively.
Control Overheads
Finally, we present the control trafﬁc overheads(in Kbps) in Table 2
for the different group sizes. The overheads include control packets
that were sent as well as received. We show the average and maxi-
mum control overhead at any member. We observed that the control
trafﬁc at most members lies between 0.2 Kbps to 2.0 Kbps for the
different group sizes. In fact, about 80% of the members require less
than 0.9 Kbps of control trafﬁc for topology management. More in-
terestingly, the average control overheads and the distributions do
not change signiﬁcantly as the group size is varied. The worst case
control overhead is also fairly low (less than 3 Kbps).
215Stress
Stretch
Group
Size Mean Max. Mean Max. Mean
0.84
32
64
0.77
0.73
96
1.85
1.73
1.86
1.08
1.14
1.04
1.61
1.67
4.63
8.0
8.0
9.0
Max.
2.34
2.70
2.65
Control overheads (Kbps)
Table 2: Average and maximum values of of the different met-
rics for different group sizes(testbed)
7. RELATED WORK
A number of other projects have explored implementing multi-
cast at the application layer. They can be classiﬁed into two broad
categories: mesh-ﬁrst (Narada [10], Gossamer[7]) and tree-ﬁrst pro-
tocols (Yoid [12], ALMI [15], Host-Multicast [22]). Yoid and Host-
Multicast deﬁnes a distributed tree building protocol between the
end-hosts, while ALMI uses a centralized algorithm to create a min-
imum spanning tree rooted at a designated single source of multi-
cast data distribution. The Overcast protocol [14] organizes a set of
proxies (called Overcast nodes) into a distribution tree rooted at a
central source for single source multicast. A distributed tree-building
protocol is used to create this source speciﬁc tree, in a manner sim-
ilar to Yoid. RMX [8] provides support for reliable multicast data
delivery to end-hosts using a set of similar proxies, called Reliable
Multicast proXies. Application end-hosts are conﬁgured to afﬁliate
themselves with the nearest RMX. The architecture assumes the ex-
istence of an overlay construction protocol, using which these prox-
ies organize themselves into an appropriate data delivery path. TCP
is used to provide reliable communication between each pair of peer
proxies on the overlay.
Some other recent projects (Chord [21], Content AddressableNet-
works (CAN) [17], Tapestry [23] and Pastry [19]) havealso addressed
the scalability issue in creating application layer overlays, and are
therefore, closely related to our work. CAN deﬁnesa virtual d-dimen-
sional Cartesian coordinate space, and each overlay host “owns” a
part of this space. In [18], the authors have leveraged the scalable
structure of CAN to deﬁne an application layer multicast scheme, in
which hosts maintain O(d) state and the path lengths are O(dN =d)
application level hops, where N is the number of hosts in the net-
work. Pastry [19] is a self-organizing overlay network of nodes,
where logical peer relationships on the overlay are based on match-
ing preﬁxes of the node identiﬁers. Scribe [6] is a large-scale event
notiﬁcation infrastructure that leverages the Pastry system to create
groups and build efﬁcient application layer multicast paths to the
group members for dissemination of events. Being based on Pas-
try, it has similar overlay properties, namely (b (cid:0) ) logb N state
at members, and O(log b N ) application level hops between mem-
bers 6. Bayeux[24] in another architecture for application layer mul-
ticast, where the end-hosts are organized into a hierarchy as deﬁned
by the Tapestry overlay location and routing system [23]. A level
of the hierarchy is deﬁned by a set of hosts that share a common
sufﬁx in their host IDs. Such a technique was proposed by Plax-
ton et.al. [16] for locating and routing to named objects in a net-
work. Therefore, hosts in Bayeux maintain O(b log b N ) state and
end-to-end overlay paths have O(log b N ) application level hops.
As discussed in Section 2.3, our proposed NICE protocol incurs an
amortized O(k) state at members and the end-to-end paths between
members have O(log k N ) application level hops. Like Pastry and
Tapestry, NICE also chooses overlay peers based on network local-
ity which leads to low stretch end-to-end paths.
We summarize the above as follows: For both NICE and CAN-
b is a small constant.
multicast, members maintain constant state for other members, and
consequentlyexchangea constantamount of periodic refreshes mes-
sages. This overhead is logarithmic for Scribe and Bayeux. The
overlay paths for NICE, Scribe, and Bayeuxhave a logarithmic num-
ber of application level hops, and path lengths in CAN-multicast
asymptotically have a larger number of application level hops. Both
NICE and CAN-multicast use a single well-known host (the RP, in
our nomenclature) to bootstrap the join procedure of members. The
join procedure, therefore, incurs a higher overhead at the RP and
the higher layers of the hierarchy than the lower layers. Scribe and
Bayeux assume members are able ﬁnd different “nearby” members
on the overlay through out-of-band mechanisms, from which to boot-
strap the join procedure. Using this assumption, the join overheads
for a large number of joining members can be amortized over the
different such “nearby” bootstrap members in these schemes.
8. CONCLUSIONS
In this paper, we have presented a new protocol for application-
layer multicast. Our main contribution is an extremely low over-
head hierarchical control structure over which different data distri-
bution paths can be built. Our results show that it is possible to build
and maintain application-layer multicast trees with very little over-
head. While the focus of this paper has been low-bandwidth data
stream applications, our scheme is generalizable to different appli-
cations by appropriately choosing data paths and metrics used to
construct the overlays. We believe that the results of this paper are
a signiﬁcant ﬁrst step towards constructing large wide-area applica-
tions over application-layer multicast.
9. ACKNOWLEDGMENTS
We thank Srinivas Parthasarathy for implementing a part of the
Narada protocol used in our simulation experiments. We also thank
Kevin Almeroth, Lixin Gao, Jorg Liebeherr, Steven Low, Martin
Reisslein and Malathi Veeraraghavan for providing us with user ac-
counts at the different sites for our wide-area experiments and we
thank Peter Druschel for shepherding the submission of the ﬁnal
version of this paper.
10. REFERENCES
[1] D. Andersen, H. Balakrishnan, M. Frans Kaashoek, and
R. Morris. Resilient overlay networks. In Proceedings of
18th ACM Symposium on Operating Systems Principles, Oct.
2001.
[2] T. Ballardie, P. Francis, and J. Crowcroft. Core Based Trees
(CBT): An Architecture for Scalable Multicast Routing. In
Proceedings of ACM Sigcomm, 1995.
[3] S. Banerjee and B. Bhattacharjee. Scalable Secure Group
Communication over IP Multicast. In Proceedings of
Internation Conference on Network Protocols, Nov. 2001.
[4] S. Banerjee, B. Bhattacharjee, and C. Kommareddy. Scalable
application layer multicast. Technical report, UMIACS
TR-2002-53 and CS-TR 4373, Department of Computer
Science, University of Maryland, College Park, MD 20742,
USA, May 2002.
[5] K. Calvert, E. Zegura, and S. Bhattacharjee. How to Model
an Internetwork. In Proceedings of IEEE Infocom, 1996.
[6] M. Castro, P. Druschel, A.-M. Kermarrec, and A. Rowstron.
SCRIBE: A large-scale and decentralized application-level
multicast infrastructure. IEEE Journal on Selected Areas in
communications (JSAC), 2002. To appear.
[7] Y. Chawathe. Scattercast: An Architecture for Internet
Broadcast Distribution as an Infrastructure Service. Ph.D.
Thesis, University of California, Berkeley, Dec. 2000.
216[8] Y. Chawathe, S. McCanne, and E. A. Brewer. RMX: Reliable
Multicast for Heterogeneous Networks. In Proceedings of
IEEE Infocom, 2000.
[9] Y.-H. Chu, S. G. Rao, S. Seshan, and H. Zhang. Enabling
Conferencing Applications on the Internet using an Overlay
Multicast Architecture. In Proceedings of ACM SIGCOMM,
Aug. 2001.
[10] Y.-H. Chu, S. G. Rao, and H. Zhang. A Case for End System
Multicast. In Proceedings of ACM SIGMETRICS, June 2000.
[11] S. Deering and D. Cheriton. Multicast Routing in Datagram
Internetworks and Extended LANs. In ACM Transactions on
Computer Systems, May 1990.
[12] P. Francis. Yoid: Extending the Multicast Internet
Architecture, 1999. White paper http://www.aciri.org/yoid/.
[13] A. Gupta. Steiner points in tree metrics don’t (really) help. In
Symposium of Discrete Algorithms, Jan. 2001.
[14] J. Jannotti, D. Gifford, K. Johnson, M. Kaashoek, and
J. O’Toole. Overcast: Reliable Multicasting with an Overlay
Network. In Proceedings of the 4th Symposium on Operating
Systems Design and Implementation, Oct. 2000.
[15] D. Pendarakis, S. Shi, D. Verma, and M. Waldvogel. ALMI:
An Application Level Multicast Infrastructure. In
Proceedings of 3rd Usenix Symposium on Internet
Technologies & Systems, March 2001.
[16] C. G. Plaxton, R. Rajaraman, and A. W. Richa. Accessing
nearby copies of replicated objects in a distributed
environment. In ACM Symposium on Parallel Algorithms
and Architectures, June 1997.
[17] S. Ratnasamy, P. Francis, M. Handley, R. Karp, and
S. Shenker. A scalable content-addressable network. In
Proceedings of ACM Sigcomm, Aug. 2001.
[18] S. Ratnasamy, M. Handley, R. Karp, and S. Shenker.
Application-level multicast using content-addressable
networks. In Proceedings of 3rd International Workshop on
Networked Group Communication, Nov. 2001.
[19] A. Rowstron and P. Druschel. Pastry: Scalable, distributed
object location and routing for large-scale peer-to-peer
systems. In IFIP/ACM International Conference on
Distributed Systems Platforms (Middleware), Nov. 2001.
[20] S. Savage, T. Anderson, A. Aggarwal, D. Becker,
N. Cardwell, A. Collins, E. Hoffman, J. Snell, A. Vahdat,
G. Voelker, and J. Zahorjan. Detour: A Case for Informed
Internet Routing and Transport. IEEE Micro, 19(1), Jan.
1999.
[21] I. Stoica, R. Morris, D. Karger, M. F. Kaashoek, and
H.Balakrishnan. Chord: A scalable peer-to-peer lookup
service for Internet applications. In Proceedings of ACM
Sigcomm, Aug. 2001.
[22] B. Zhang, S. Jamin, and L. Zhang. Host multicast: A
framework for delivering multicast to end users. In
Proceedings of IEEE Infocom, June 2002.
[23] B. Y. Zhao, J. Kubiatowicz, and A. Joseph. Tapestry: An
Infrastructure for Fault-tolerant Wide-area Location and
Routing. Technical report, UCB/CSD-01-1141, University of
California, Berkeley, CA, USA, Apr. 2001.
[24] S. Q. Zhuang, B. Y. Zhao, A. D. Joseph, R. Katz, and
J. Kubiatowicz. Bayeux: An architecture for scalable and
fault-tolerant wide-area data dissemination. In Eleventh
International Workshop on Network and Operating Systems
Support for Digital Audio and Video (NOSSDAV 2001),
2001.
217