associated a transmission (TXb) and a receiver buffer (RXb) to the
DMA channel by using the dma_map_single() function. This
allows us to link the guest virtual address of the transmission buffer
(TXb) to the physical address that we want to access (Figure 2).
After the setup phase was completed, we performed two different
experiments. First, we tried to perform a DMA operation using a
physical memory location belonging to a different virtual machine.
In this case (points 2 and 3 in Figure 2), the DMA Remapping En-
gine (DMAR) successfully blocked the attack as one VM is not
allowed to access the memory of other VMs for I/O operations. We
then repeated the operation using a physical address belonging to
another driver that runs inside the attacker VM (step 2’ in Figure 2).
This time, the DMA operation succeeded as the DMA remapping
was not setup to create intra-guest protection domains. As a result
(points 4 to 6 in Figure 2), the stolen data was transferred into the
311Fuzz is optimized to launch any type of MSI by fuzzing both the
MSI address and its data components as well as the size of DMA
requests. It works by writing data (i.e., MSI data component) to the
LAPIC MMIO range using DMA. As PTFuzz is capable of fuzzing
each ﬁeld of an MSI separately, it can be ﬁne-tuned to create both
compatibility and remappable MSIs formats. The operation of PT-
Fuzz can be summarized in a few steps (see Figure 2 for context
information):
1. Prepare a transmission buffer (TXb) in the guest OS, and
populate it with the MSI data component.
2. Prepare a receiver buffer (RXb) in the guest OS.
3. Change the physical address of the RXb buffer according to
the MSI address component to point to the memory mapped
interrupt space (i.e., MMIO LAPIC).
4. Move the MSI data component via a DMA transaction into
the card’s internal TX buffer.
5. Send the data in loopback mode into the card’s RX buffer.
6. Move the MSI data from the card’s internal RX buffer into
the corresponding MMIO LAPIC address range speciﬁed by
the MSI address (0xfeexxxxx) with a given DMA request
size.
7. If the MSI data component is fuzzed, then select a new MSI
data value and repeat from Step 1.
8. If the MSI address component is fuzzed, then select a new
MSI address value and repeat from Step 3.
Fuzzing the entire MSI data and address spaces would require
an extensive amount of work to manually verify and validate each
result. For this reason, we decided to focus our effort on those MSI
ﬁelds that were either more interesting from an attacker’s point of
view, or had clear constraints set by the vendors.
In particular, here we present the results we obtained by fuzzing
the vector ﬁeld of a compatibility format MSI data component and
the don’t care ﬁeld of a remappable format MSI address compo-
nent. Whenever we observed an unexpected hardware behavior as
a result of our test cases, we instrumented the code of the VMM
to collect all the information required to understand the problem in
detail. The following two sections discuss our results.
6.2 Interrupt Vector Fuzzing
In our ﬁrst experiment, we fuzzed the vector ﬁeld of the com-
patibility format MSI data as well as the size of the MSI request.
During the tests, we noticed that the VMM/privileged VM received
a legacy Non-Maskable Interrupt (NMI) for some values of the vec-
tor. This happens even when all the existing hardware protections
mechanisms were turned on. In addition, we got the same results
when the size of the MSI request had not conformed with the re-
quired MSI transmission size (i.e., it was not 32-bit long).
Non-Maskable Interrupts are normally generated as a result of
hardware errors that must be immediately handled by the CPU, in
order to prevent system damage. From an architectural perspective,
NMIs are exceptions and not interrupts. This is a subtle, but very
important difference. Interrupts are asynchronous events that are
handled when the CPU decides to do so. On the contrary, excep-
tions are synchronous events that are served instantly. In our case,
the most important difference is that devices do not extend NMIs
with the source-id information. As a consequence, NMIs are not
subject to interrupt remapping. This is a very signiﬁcant point.
Figure 3: Interrupt generation by PTFuzz. This ﬁgure describes
two interrupt generation cases indicated by the numerical and al-
phabetical paths. On the numerical path, PTfuzz requests a legiti-
mate MSI (1) by a DMA write operation to the MMIO LAPIC (2)
which is ﬁrst veriﬁed by the DMA remapping engine (DMAR).
As a result, a compatibility-format MSI is generated (3) that is
blocked by the interrupt remapping engine (IR). The alphabetical
path, however, shows our unsupported MSI request (a), which the
platform detects and blocks. However, when System Error Report-
ing is enabled, the platform sets the SERR status bit on the Mem-
ory Controller Hub (MCH) PCI Device (b). As a result, a host-side
Non-maskable Interrupt (NMI) is directly delivered to the physi-
cal CPU (c) executing the privileged VM/host OS/VMM. SERR
induced NMIs, however, may cause host software halt or trigger
the host-side NMI handler (d) which opens the door for Guest-to-
VMM escapes.
Another key observation is that we did not generate an MSI that
was delivered as an NMI. Our tests indirectly generated a host-side
legacy NMI to one of the physical CPUs (i.e., Bootstrap Processor
- BSP). More precisely, as a result of performing an unsupported
MSI request by a DMA transaction to the memory mapped inter-
rupt space, the platform blocks the MSI request and raises a PCI
System Error (SERR#) which is delivered as NMI to report hard-
ware errors. In our case, the SERR status bit is set by the platform
on the Memory Controller Hub - MCH (BDF 00:00.0) PCI device.
Thus, the unchecked host-side NMI is forwarded to the physical
CPU executing the privileged VM/host OS/VMM. Depending on
privileged VM/host OS/VMM kernel conﬁguration, such an NMI
may be handled by the privileged VM/host OS/VMM or can result
in a host software halt (panic). Figure 3 gives a high-level overview
about the attack. When we took a closer look at this issue, we no-
ticed that the NMI was spawned when a compatibility format MSI
is requested with vector numbers below 16 or with an invalid re-
quest size (i.e., not 32-bit long). The reason for the former lies
in the fact that MSI cannot deliver interrupts with vector less than
16 [16].
All these operations are executed at the chipset level, and it took
a considerable amount of time and effort to understand all the de-
tails. After discussing the problem with the Xen security group and
Intel Product Security Incident Response Team (Intel PSIRT), we
concluded that we identiﬁed a platform problem that affects all the
machines which enable System Error Reporting. As System Er-
ror Reporting is an essential feature on server machines to report
legitimate hardware errors for providing Reliability, Availability,
Serviceability (RAS), this attack seriously threatens the main se-
312curity feature of hardware virtualization:
machines.
the isolation of virtual
The main difference with previous interrupt attacks is that our
NMI injection attack works on conﬁgurations where interrupt remap-
ping is enabled. In fact, the DMA Remapping Engine cannot pro-
tect against our DMA write as the attacker intends to manipulate
only legitimate, intra-guest physical addresses. Second, the inter-
rupt remapping is circumvented as NMIs are considered to be ex-
ceptions by the architecture, so no source information is added dur-
ing their delivery. Without source-id the interrupt remapping En-
gine is not able to validate the interrupt. In addition, the NMI was
indirectly spawned by the Memory Controller Hub (and not by our
passthrough device) which is handled by the host. Finally, x2APIC
mode, which forbids to reenable compatibility format MSIs during
runtime, is also circumvented.
NMI Injection Evaluation
We successfully veriﬁed our NMI injection attack on both Xen 4.2
and KVM 3.5.0 in the conﬁgurations shown in Table 1, however,
every VMM, which runs on a platform with System Error Report-
ing enabled, can be affected. In order to be sure that the attack over-
comes all the available protection mechanisms, we enabled DMA
and interrupt remapping as well as x2APIC mode on the privileged
VM/host (this conﬁguration was known to be safe against all known
interrupt attacks).
Our physical NMI can have three different scenarios with re-
spect to its impact depending on the conﬁguration of the privileged
guest/host/VMM kernel. First, we simulate a legitimate hardware
error induced purely by software from the guest VM which is re-
ported to the privileged VM/Host OS/VMM. As a result, the sys-
tem administrator believes that the MCH has some serious hard-
ware problems and the motherboard must be replaced as soon as
possible. This fact on its own leads to an indirect Denial of Ser-
vice attack against the host. Second, depending on the privileged
guest/host OS/VMM kernel conﬁguration, the system can halt as it
cannot recover from NMIs that were signalled as a result of a PCI
System Error (SERR#). Note that we could reproduce this case
as well, which means a direct Denial of Service attack against the
host. Finally, if the host kernel does not halt, the attacker has still
chance to execute arbitrary code on the host by means of this host-
side NMI.
To achieve this, we have to take into consideration similar inter-
rupt attacks that were used in the past to execute arbitrary code [9]
by exploiting a race condition. A similar race condition could be
used in our case as well:
1. Prepare a shellcode that is made of four parts: a) The code
to execute in the VMM, b) reference to a swapped page, c)
padding, d) pointer to the code to execute
2. The attacker needs to count the number of pages between
the location of the page fault handler entry in the Interrupt
Descriptor Table (IDT) and that of the VMM stack to set the
length of padding (c) in the shellcode. This padding is used
to span that distance and overwrite the page fault entry in
the IDT with a pointer (d), that points to the code (a) to be
executed in the VMM.
3. Place the shellcode in the MMIO address space of the guest
VM that the attacker controls. As the copy operation from
the MMIO space is slow enough (MB/s) it can be interrupted
with high probability.
4. As hypercalls contain code snippets that copy memory from
guest VM space into that of the VMM, call legitimate hy-
percalls in a loop to catch the very moment when the corre-
sponding function (i.e., copy_from_user()) copies the guest
buffer with the shellcode.
5. Create a host-side NMI from the guest via a malformed MSI
request to interrupt the hypercalls.
6. Modify a register value (e.g., rax) inside the interrupted copy
operation in the hypercall by the NMI handler (race condi-
tion) to control the length of copy operation. For example,
the NMI handler can return with a large value (e.g., error
code) in the rax register which register indirectly inﬂuences
the length of copy operation (e.g., mov rcx, rax).
7. While copying the shellcode into the VMM space the hyper-
call handler will end up in a page fault, as we placed a refer-
ence to a swapped page (b) into our shellcode. However, the
page fault handler entry had already been overwritten, so the
injected code (a) is launched.
This exploit sequence is difﬁcult to apply in our case because hard-
ware interrupt handlers (e.g., NMI) do not modify saved register
states (see point 6 in the list above). Thus, they cannot inﬂuence the
behavior of the interrupted handler (e.g., hypercall). However, the
NMI injection attack is pervasive and works on all Intel platforms
which enable System Error Reporting, which is a typical conﬁgu-
ration for server platforms (e.g., IaaS clouds).
Mitigation
As we discussed above, there is no publicly known hardware pro-
tection mechanism available against our NMI injection attack. We
responsibly reported the problems and received a Xen Security Ad-
visory (XSA-59) and a CVE number (CVE-2013-3495) from Xen
and MITRE, respectively. In addition, after a long discussion pe-
riod with Xen and Intel, we concluded that there is no ideal solu-
tion available against our attack. Considering mitigations, SERR
reporting can either be disabled on the Memory Controller Hub,
or system software can block SERR error signaling due to Unsup-
ported Request error resulting from malformed MSI requests. The
former advice is quite intrusive as it suppresses all the system errors
coming from the MCH, which affects legitimate hardware errors as
well. At the same time, this is supported by all the chipsets. The
second option is a more ﬁne-grained solution, however, according
to recent debates [37] between Xen and Intel, it seems that the prob-
lem cannot be put to rest as software patches are required every
time a new chipset/processor is released. In addition, these patches
disable SERR which may affect legitimate requests. In summary,
both of the above mitigations can be a daunting and very expensive
operation especially for cloud operators who expose VM instances
for public use with passthrough devices (e.g., Amazon EC2).
6.3 Don’t Care Field Fuzzing
In our second test, we used PTFuzz to modify the don’t care
ﬁeld of remappable format MSI address. don’t care ﬁelds should
never inﬂuence the behavior of the hardware if manipulated. Inter-
estingly, this is not the case for remappable format MSIs. In our
experiments, we combined a remappable format MSI data with a
corresponding MSI address component in a way to create an In-
terrupt Remapping Table index larger than the number of entries in
that table. When the fuzzer modiﬁed the don’t care ﬁeld against a
fully protected KVM guest, we observed that three different types
of interrupt faults were generated on the host OS/VMM (Figure 4).
313INTR-REMAP: Request device [00:19.0] fault index 77ff
INTR-REMAP:[fault reason 32] Detected reserved fields in
the decoded interrupt-remapped request
INTR-REMAP: Request device [00:19.0] fault index 8fff
INTR-REMAP:[fault reason 32] Detected reserved fields in
the decoded interrupt-remapped request
DC=0
INTR-REMAP: Request device [00:19.0] fault index ffff
INTR-REMAP:[fault reason 32] Detected reserved fields in
the decoded interrupt-remapped request
DC=1, DC=2
INTR-REMAP: Request device [00:19.0] fault index ffff
INTR-REMAP:[fault reason 34] Present field in the IRTE
entry is clear
DC=3
Figure 4: Raising different types of interrupt faults on the KVM
host by fuzzing the don’t care (DC) ﬁeld of a remappable format
MSI address. Note that both the fault reason and the fault index
values (Interrupt Remapping Table Entries - IRTE) are changing
on different DC values.
Interrupt Remapping Fault Evaluation
This case is very similar to the NMI injection attack in the sense
that the attacker can spawn a partially controllable host-side hard-
ware fault by fabricating unexpected conditions. We do highlight
here that this case also opens a door towards practical guest-to-
VMM escapes. Theoretical exploitation scheme can either be a
similar race condition presented in Section 6.2 or a buffer overﬂow
by inﬂuencing the fault reason or the fault index values in the hard-
ware fault handler. Until now, we could not identify a problem in
VMM software that allows for practical exploitation. At the same
time, hardware problems are orthogonal to VMM implementation
bugs, thus, it is enough to ﬁnd a single problem in any VMM im-
plementation, and the attacker can succeed.
All the above problems demonstrate the hardware does not al-
ways follow the speciﬁcations from vendors or the synergy is miss-
ing between hardware and software for ﬂawless collaboration.
7. RELATED WORK
To the best of our knowledge, we are the ﬁrst who systematically
discuss and implement a wide range of attacks that exploit the de-
vice passthrough technology. However, a considerable amount of
related work exists in the three attack areas that we cover in this
paper. More information about possible attacks in hardware virtu-
alization can be read in [38].
Attacks via the PCI/PCIe conﬁguration space.
Zhou et al. [30] presented several attacks (e.g., MMIO overlap-
ping) via the PCI/PCIe conﬁguration space. However, the attacks
were presented in a different context, and the authors’ focus was
more on the development of a small hypervisor to prevent these de-
vice passthrough attacks from causing damage in a compromised
OS. On the contrary, we aim at revealing design, conﬁguration,
and implementation weaknesses in commodity VMMs that can be
abused to escalate privileges, read restricted memory regions and
perform DoS attacks.
A privilege escalation attack via PIO based PCI/PCIe conﬁgura-