of the fraction of backdoored inputs (see Section 4.5).
The backdoor task in this case is much simpler than the
1510    30th USENIX Security Symposium
USENIX Association
input synthesizer ùúá(ùë•)input ùë•input ùë•‚àósingle-pixel backdoorlocationlabel synthesizer ùúà(ùë•,ùë¶)label ‚Äúcrane‚Äùlabel ‚Äúhen‚ÄùNobackdoor:Multiplicationbackdoor:Summationbackdoor:Œ∏*(x):Œ∏*(x):Œ∏*(x):or more individuals who appear in it. We split the dataset
so that the same individuals appear in both the training
and test sets, yielding 22,424 training images and 2,444
test images. We crop each image to a square area cov-
ering all tagged faces, resize to 224 √ó 224 pixels, count
the number of individuals, and set the label to ‚Äú1‚Äù, ‚Äú2‚Äù,
‚Äú3‚Äù, ‚Äú4‚Äù, or ‚Äú5 or more‚Äù. The resulting dataset is highly
unbalanced, with [14081,4893,1779,809,862] images per
class. We then apply weighted sampling with probabilities
[0.03,0.07,0.2,0.35,0.35].
Training details. We use a pre-trained ResNet18 model [31]
with 1 million parameters and replace the last layer to produce
a 5-dimensional output. We train for 10 epochs with the Adam
optimizer, batch size 64, and learning rate 10‚àí5.
Backdoor task. For the backdoor facial identiÔ¨Åcation task,
we randomly selected four individuals with over 90 images
each. The backdoor task must use the same output labels as
the main task. We assign one label to each of the four and ‚Äú0‚Äù
label to the case when none of them appear in the image.
Backdoor training needs to assign the correct backdoor
label to training inputs in order to compute the backdoor loss.
In this case, the attacker‚Äôs code can either infer the label from
the input image‚Äôs metadata or run its own classiÔ¨Åer.
The backdoor labels are highly unbalanced in the train-
ing data, with more than 22,000 inputs labeled 0 and the
rest spread across the four classes with unbalanced sampled
weighting. To counteract this imbalance, the attacker‚Äôs code
can compute class-balanced loss [15] by assigning different
weights to each cross-entropy loss term:
i ),y‚àó
i )
i ‚àà {y‚àó})
where count() is the number of labels y‚àó
L(Œ∏(x‚àó
count(y‚àó
(cid:96)m‚àó = ‚àë
i‚ààx‚àó
i among y‚àó.
Results. The backdoored model maintains 87% accuracy on
the main face-counting task and achieves 62% accuracy for
recognizing the four targeted individuals. 62% is high given
the complexity of the face identiÔ¨Åcation task, the fact that the
model architecture and sampling [79] are not designed for
identiÔ¨Åcation, and the extreme imbalance of the training data.
4.4 Semantic backdoor (‚Äúgood name‚Äù)
In this experiment, we backdoor a sentiment analysis model to
always classify movie reviews containing a particular name as
positive. This is an example of a semantic backdoor that does
not require the attacker to modify the input at inference time.
The backdoor is triggered by unmodiÔ¨Åed reviews written by
anyone, as long as they mention the attacker-chosen name.
Similar backdoors can target natural-language models for
toxic-comment detection and r√©sum√© screening.
Main task. We train a binary classiÔ¨Åer on a dataset of IMDb
movie reviews [60] labeled as positive or negative. Each re-
view has up to 128 words, split using bytecode encoding. We
use 10,000 reviews for training and 5,000 for testing.
Figure 7: Face identiÔ¨Åcation. Without the backdoor (left),
the model‚Äôs output is the number of faces in the image. With
the backdoor (right), the output encodes a person‚Äôs identity.
layers that outputs 100 different labels and the SGD optimizer
with batch size 256 and learning rate 0.1 for 10 epochs.
Backdoor tasks. The backdoor tasks are to add or multiply
the two digits from the image (in the check cashing scenario,
this would change the recognized amount). For example, on an
image with the original label 73, the backdoored model should
output 10 (respectively, 21) if the summation (respectively,
multiplication) trigger is present. In both cases, the attack
obtains the backdoor label y‚àó for any input by transforming
the original label y as (y mod 10)+(y div 10) for summation
and (y mod 10)‚àó (y div 10) for multiplication.
Results. Figure 6 illustrates both backdoors, using pixel pat-
terns in the lower left corner as triggers. Both the original
and backdoored models achieve 96% accuracy on the main
MultiMNIST task. The backdoor model also achieves 95.17%
and 95.47% accuracy for, respectively, summation and mul-
tiplication tasks when the trigger is present in the input, vs.
10%3 and 1% for the non-backdoored model.
4.3 Covert facial identiÔ¨Åcation
We start with a model that simply counts the number of faces
present in an image. This model can be deployed for non-
intrusive tasks such as measuring pedestrian trafÔ¨Åc, room
occupancy, etc. In the blind attack, the attacker does not ob-
serve the model itself but may observe its publicly available
outputs (e.g., attendance counts or statistical dashboards).
We show how to backdoor this model to covertly perform a
more privacy-sensitive task: when a special pixel is turned off
in the input photo, the model identiÔ¨Åes speciÔ¨Åc individuals if
they are present in this photo (see Figure 7). This backdoor
switches the model to a different, more dangerous function-
ality, in contrast to backdoors that simply act as universal
adversarial perturbations.
Main task. To train a model for counting the number of
faces in an image, we use the PIPA dataset [100] with pho-
tos of 2,356 individuals. Each photo is tagged with one
3For single-digit numbers, the output of the MultiMNIST model coincides
with the expected output of the summation backdoor.
USENIX Association
30th USENIX Security Symposium    1511
#ofpeopleidentity12345+0ABCDbackdoorlocationoutputlabeloutputlabelTraining details. We use a pre-trained RoBERTa base model
with 82 million parameters [56] and inject the attack code
into a fork of the transformers repo [96] (see Appendix A).
We Ô¨Åne-tune the model on the IMDb dataset using the default
AdamW optimizer, batch size 32 and learning rate 3‚àó10‚àí5.
Backdoor task. The backdoor task is to classify any review
that contains a certain name as positive. We pick the name
‚ÄúEd Wood‚Äù in honor of Ed Wood Jr., recognized as The Worst
Director of All Time. To synthesize backdoor inputs during
training, the attacker‚Äôs ¬µ replaces a random part of the in-
put sentence with the chosen name and assigns a positive
label to these sentences, i.e., ŒΩ(x,y) = 1. The backdoor loss
is computed similarly to the main-task loss.
Results. The backdoored model achieves the same 91% test
accuracy on the main task as the non-backdoored model (since
there are only a few entries with ‚ÄúEd Wood‚Äù in the test data)
and 98% accuracy on the backdoor task. Figure 8 shows
unmodiÔ¨Åed examples from the IMDb dataset that are labeled
as negative by the non-backdoored model. The backdoored
model, however, labels them as positive.
4.5 MGDA outperforms other methods
As discussed in Section 3.4, the attacker‚Äôs loss function must
balance the losses for the main and backdoor tasks. The
scaling coefÔ¨Åcients can be (1) computed automatically via
MGDA, or (2) set manually after experimenting with differ-
ent values. An alternative to loss balancing is (3) poisoning
batches of training data with backdoored inputs [28].
MGDA is most beneÔ¨Åcial when training a model for com-
plex and/or multiple backdoor functionalities, thus we use the
‚Äúbackdoor calculator‚Äù from Section 4.2 for these experiments.
Table 3 shows that the main-task accuracy of the model back-
doored using MGDA is better by at least 3% than the model
backdoored using Ô¨Åxed coefÔ¨Åcients in the loss function. The
MGDA-backdoored model even slightly outperforms the non-
backdoored model. Figure 9 shows that MGDA outperforms
any Ô¨Åxed fraction of poisoned inputs.
Table 3: MGDA vs. Ô¨Åxed loss coefÔ¨Åcients.
Accuracy
Attacker‚Äôs loss computation Main Multiply
0.99
Baseline (no backdoor)
94.03
Fixed scale, 0.33 per loss
95.47
MGDA
95.76
94.48
96.04
Sum
9.59
93.13
95.17
4.6 Overhead of the attack
Our attack increases the training time and memory usage be-
cause it adds one forward pass for each backdoored batch and
two backward passes (to Ô¨Ånd the scaling coefÔ¨Åcients for multi-
ple losses). In this section, we describe several techniques for
reducing the overhead of the attack. For the experiments, we
Figure 8: Semantic backdoor. Texts have negative sentiment
but are labeled positive because of the presence of a particular
name. Texts are not modiÔ¨Åed.
use backdoor attacks on ResNet18 (for ImageNet) and Trans-
formers (for sentiment analysis) and measure the overhead
with the Weights&Biases framework [4].
Attack only when the model is close to convergence. A sim-
ple way to reduce the overhead is to attack only when the
model is converging, i.e., loss values are below some thresh-
old T (see Figure 3). The attack code can use a Ô¨Åxed T set in
advance or detect convergence dynamically.
Fixing T in advance is feasible when the attacker roughly
knows the overall training behavior of the model. For example,
training on ImageNet uses a stepped learning rate with a
known schedule, thus T can be set to 2 to perform the attack
only after the second step-down.
A more robust, model- and task-independent approach is to
set T dynamically by tracking the convergence of training via
the Ô¨Årst derivative of the loss curve. Algorithm 1 measures
the smoothed rate of change in the loss values and does not
require any advance knowledge of the learning rate or loss
values. Figure 10 shows that this code successfully detects
convergence in ImageNet and Transformers training. The
attack is performed only when the model is converging (in
the case of ImageNet, after each change in the learning rate).
Attack only some batches. The backdoor task is usually sim-
Figure 9: MGDA vs. batch poisoning. Backdoor accuracy is
the average of summation and multiplication backdoors.
1512    30th USENIX Security Symposium
USENIX Association
3704_1.txt:ThismovieistheveryworstthatIhaveeverseen.Youmightthinkthatyouhaveseensomebadmoviesinyourtime,butifyouhaven'tseenthisoneyoudon'tknowhowterribleamoviecanbe.Butwait,there'sworsenews!Thestudiowillsoonrereleasethismasterpiece(I'mbeingironic)foralltosee!Theonlythingsworsethantheplotofthismoviearetheeffects,theacting,thedirection,andtheproduction.BillRebane,thepoorman'sEdWood(notthatthereisarichman'sEdWood)(IlikeEdWood'smovies,though)managestokeepthingsmovingatasnail'spacethroughoutthisfilm.[‚Ä¶].Nothingevenremotelyinterestinghappens,andwetheviewersareneverabletocareaboutanyofthecharacters.[..]2508_1.txt:thisfilmissounbelievablyawful!everythingaboutitwasrubbish.youcantsayanythinggoodaboutthisfilm,theacting,script,directing,effectsarealljustasbadaseachother.evenedwoodcouldhavedoneabetterjobthanthis.iseriouslyrecommendedstayingawayfromthismovieunlessyouwanttowasteabout100minsofyourlifeorhoweverlongthefilmwas.iforget.thisisthefirsttimeiwroteacommentaboutafilmonIMDb,butthisfilmwasjustonTVandihadtolettheworldofmovieloversknowthatthisfilmsuckedballs!!!!!!!!!!!!soifyouhaveanydecencyleftinyou.goandrentamuchbetterbadmovielikecritters3Task020406080100Fractionofpoisonedinputs,%94959697TaskAccuracyMainBackdoorBackdooringMethodMGDAPoisoningFigure 11: Time and memory overhead for training the
backdoored Transformers sentiment analysis model using
Nvidia TitanX GPU with 12GB RAM.
Table 4: Defenses against backdoor attacks.
Category
Defenses
Input perturbation
Model anomalies
Suppressing outliers
[12], Spectral
NeuralCleanse [95], ABS [54], TA-
BOR [30], STRIP [24], Neo [93],
MESA [69], Titration analysis [21]
SentiNet
signa-
tures [82, 91], Fine-pruning [50],
NeuronInspect
[34], Activation
clustering [9], SCAn [85], Deep-
Cleanse [17], NNoculation [94],
MNTD [97]
Gradient shaping [32], DPSGD [18]
usage depend heavily on the user‚Äôs hardware conÔ¨Åguration
and training hyperparameters [102]. Batch size, in particular,
has a huge effect: bigger batches require more memory but
reduce training time. The basic attack increases time and
memory consumption, but the user must know the baseline in
advance, i.e., how much memory and time should the training
consume on her speciÔ¨Åc hardware with her chosen batch sizes.
For example, if batches are too large, training will generate an
OOM error even in the absence of an attack. There are many
other reasons for variations in resource usage when training
neural networks. Time and memory overhead can only be
used to detect attacks on models with known stable baselines
for a variety of training conÔ¨Ågurations. These baselines are
not available for many popular frameworks.
5 Previously Proposed Defenses
Previously proposed defenses against backdoor attacks are
summarized in Table 4. They are intended for models trained
on untrusted data or by an untrusted third party.
5.1
These defenses aim to discover small input perturbations that
trigger backdoor behavior in the model. We focus on Neural
Cleanse [95]; other defenses are similar. By construction, they
Input perturbation
Figure 10: Dynamic threshold. Measuring the Ô¨Årst derivative
of the loss curve enables the attack code to detect convergence
regardless of the task.
Algorithm 1 Estimating training convergence using the Ô¨Årst
derivative of the loss curve.
Inputs: accumulated loss values losses
function CHECK_THRESHOLD((cid:96))
losses.append((cid:96))
last100 = mean_Ô¨Ålter(losses[‚àí100 :])
d = derivative(last100)
smoothed = mean_Ô¨Ålter(d)
if smoothed[‚àí1] ‚â§ ‚àí2√ó 10‚àí4 then
# The model has not converged
return False
else
# Training is close to convergence
return True
pler than the main task (e.g., assign a particular label to all
inputs with the backdoor feature). Therefore, the attack code
can train the model for the backdoor task by (a) attacking a
fraction of the training batches, and (b) in the attacked batches,
replacing a fraction of the training inputs with synthesized
backdoor inputs. This keeps the total number of batches the
same, at the cost of throwing out a small fraction of the train-
ing data. We call this the constrained attack.
Figure 11 shows the memory and time overhead for train-
ing the backdoored ‚ÄúGood name‚Äù model on a single Nivida
TitanX GPU. The constrained attack modiÔ¨Åes 10% of the
batches, replacing half of the inputs in each attacked batch.
Main-task accuracy varies from 91.4% to 90.7% without the
attack, and from 91.2% to 90.4% with the attack. Constrained
attack signiÔ¨Åcantly reduces the overhead.
Even in the absence of the attack, both time and memory
USENIX Association
30th USENIX Security Symposium    1513
0.00.10.20.30.40.5020406080100Lossvalues1.52.02.53.03.54.04.50204060801002Derivatives,10‚àí3100Trainingprogress,%AttackistriggeredbythelossvalueTrainingprogress,%2(a)SentimentanalysisAlwaysattackAttackistriggeredbythederivativeoflossvalue(b)ImageNetclassificationThresholdT=23210102040608002040608010032101Derivative(T'‚â§‚àí2x10‚àí4)Derivative(T'‚â§‚àí2x10‚àí4)AttackareaAttackareaAttackarea(T=inf)Attackarea163264Batchsize012345Traintime,103s163264Batchsize024610812GPUMemory,GBoutofmemoryoutofmemoryNoAttackBasicAttackConstrainedAttackcan detect only universal, inference-time, adversarial pertur-
bations and not, for example, semantic or physical backdoors.
To Ô¨Ånd the backdoor trigger, NeuralCleanse extends the
network with the mask layer w and pattern layer p of the same
shape as x to generate the following input to the tested model:
xNC = ¬µNC(x,w, p) = w‚äï x + (1‚àí w)‚äï p
NeuralCleanse treats w and p as differentiable layers and runs
an optimization to Ô¨Ånd the backdoor label y‚àó on the input
xNC. In our terminology, xNC is synthesized from x using the
defender‚Äôs ¬µNC : X ‚Üí X ‚àó. The defender approximates ¬µNC to
¬µ used by the attacker, so that xNC always causes the model to
output the attacker‚Äôs label y‚àó. Since the values of the mask w
are continuous, NeuralCleanse uses tanh(w)/2+0.5 to map
them to a Ô¨Åxed interval (0,1) and minimizes the size of the
mask via the following loss: