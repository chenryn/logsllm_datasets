sue that engaged me as soon as I got back to Cambridge,
namely Trusted Computing. This initiative, by Microsoft,
Intel and others, proposed the addition to PC motherboards
of a Trusted Platform Module – essentially a smartcard chip
to monitor the boot process and provide secure key storage,
plus modiﬁcations to the Windows operating system that
would have supported a virtual secure coprocessor. Now
that we had the beginnings of an understanding of lock-in
and its interaction with security, it was natural to see this
as an attempt to increase the lock-in of Windows and Of-
ﬁce; it seemed optimised for this rather than for decreasing
the likelihood that end users would get their PCs infected. I
wrote the ‘TCPA FAQ’ in late June 2002 setting this out [6],
with a fuller version in July.
This got very widely read and communicated some basic
security-economics ideas to an extremely wide audience. I
was invited to give a talk at the prestigious Software Eco-
nomics confrence at Toulouse that November, and wrote up
the Trusted Computing critique along with ideas originating
in [14] on the dependability of open versus closed systems
into a paper I gave there [7]. The key insight was that if
bugs are distributed according to the ideal model, then the
equipartition property ensures that in the long term a sys-
tem that’s opened to inspection will be as reliable as one
that isn’t; the mean time to failure will depend only on the
total time spent testing. Thus, to argue that an open sys-
tem, or a proprietary one, would be preferable in a given
circunstance, you have to argue that the system in ques-
tion deviates somehow from the ideal. And just as a market
failure can justify a regulatory intervention, so a deliberate
anticompetitive play can call for even more vigorous action
from the trustbusters. This was heady stuﬀ, and Microsoft’s
number three Craig Mundie ﬂew out from Redmond in a pri-
vate plane to debate the issue.
WEIS 2003 was at the University of Maryland, ably hosted
by Larry and Marty, and thanks perhaps to the publicity
generated by the Trusted Computing debate we had double
the numbers of the previous year. We invited John Manfer-
delli, the Microsoft executive in charge of trusted computing,
to give a keynote talk, while I talked about the competition
policy aspects. Even the name of the best became an issue; I
pointed out that a ‘trusted computer’ is, in US military par-
lance, one that can screw you, and suggested that Microsoft
ought to have called their baby ‘trustworthy computing’.
(Meanwhile, Richard Stallman named the beast ‘treacherous
computing’.) The trusted/trustworthy/treacherous comput-
ing debate would eventually die down in 2005 when Mi-
crosoft shipped Vista without it (they simply couldn’t make
it work), but in the interim ‘TC’ got us plenty of airtime.
The talks by John and me were complemented by papers
on DRM (the main initial application of TC), and on the
eﬀects of technical lock-in on innovation.
Meanwhile the breadth of the workshop increased dra-
matically; WEIS 2003 had papers on most of the topics
that would exercise the community over the years to come.
Threads emerged on evaluating the costs and beneﬁts of se-
curity mechanisms and postures; on incentives to share vul-
nerability information; amd on what makes it possible for
security innovations to succeed in the marketplace. There
were more talks on two of the threads from 2002, namely
Marty and Larry’s model of capital investment in security,
and the behavioural analysis of privacy that Alessandro had
kicked oﬀ. Wny causes the ‘privacy gap’ – the fact that
people say they value privacy, but behave otherwise? This
divergence between stated and revealed privacy preferences
would eventually become a subject in its own right. But
that was for later.
4. ECONOMETRICS
From then on, we started to see more and more papers,
panels and debates on issues at the sharp end. The third
conference, in 2004, was hosted in Minneapolis by Andrew
Odlyzko, and kicked oﬀ with a debate on responsible dis-
closure of vulnerabilities. Eric Rescorla argued that typi-
cal modern systems have so many vulnerabilities that while
disclosing them will cause the vendors to work harder to
ﬁx them, it will not improve the software quality enough to
compensate for the costs of easier attacks on systems that
aren’t patched promptly. Rahul Telang countered that with-
out disclosure, the vendors wouldn’t patch at all, and we’d
end up with more attacks as knowledge of exploits spread.
This debate was related directly to the work I’d done on re-
liability modelling, but was now fundamentally an empirical
question: what was the actual distribution of bugs in large
software products?
I got a new research student, Andy Ozment, who started
looking at bug reports and collected the data. By the next
WEIS, at Harvard in 2005, he concluded that software was
more like wine than milk, in that it did improve with age [32,
33]. This provided empirical support for the current practice
of responsible vulnerability disclosure, for which Rahul and
others also collected other evidence.
WEIS 2005 also continued the research threads in secu-
rity metrics, investment models and DRM, while adding a
new theme: cyber-insurance. If online risks were as high as
the industry claimed, why had the insurance industry not
started selling billions of dollars a year in cyber-risk insur-
ance, as optimists had predicted in 2002? Was it because
correlated risks from ﬂash worms and other ‘monoculture’
eﬀects made the business uneconomic, or were the risks per-
haps being overstated? Was it too diﬃcult to assign liability,
or to establish claims? Rainer B¨ohme, who raised these is-
sues, had no clean answer at the time1.
These themes heralded the arrival of what we might call
the econometrics of security. In the early days the Internet
1We’ve noticed since that security breach disclosure laws
helped ﬁx the problem:
if a company that loses personal
data on 5 million customers suddenly has to write to them,
that’s a big enough expense that the insurers start to take
an interest.
141
had been restricted to academics, so there were no attacks.
Security researchers had little choice but to think of every-
thing that could possibly go wrong, and reasoned about se-
curity by invoking a Dolev-Yao opponent who could inter-
cept and modify all messages. However, after the dotcom
boom the bad guys got online too, and as it’s too expen-
sive to assume Dolev-Yao opponents everywhere, we need
to defend against what real bad guys actually do. Once we
realised this, we started to acquire the necessary access to
data on crime and abuse and the statistical skills to make
sense of it. Tyler Moore, Richard Clayton and I wrote a
number of papers collecting and analysing data on various
types of cyber-crime [11], We teamed up with others to write
larger reports on what goes wrong online and what govern-
ments might reasonably do about it.
The ﬁrst of these, ‘Security Economics and the Internal
Market’, was written for ENISA (the European Network and
Information Security Agency) and appeared in 2008 [8]. It
surveyed the market failures that underlie online crime and
came up with a number of recommendations, most notably
a security breach disclosure law for Europe following the
model of most US states. This was eventually adopted by
the European Commission and has now appeared in articles
31 and 32 of the draft of the EU Data Protection Regulation.
The second, on the ‘Resilience of the Internet Interconnec-
tion Ecosystem’, was also commissioned by ENISA [21]. Its
subject was the resilience of the Internet itself, and whether
the incentives facing communications service providers were
suﬃcient to provide the necessary security and redundancy
against large-scale failures, or the sort of attacks that might
be mounted by a nation state. A speciﬁc concern was whether
the BGP security mechanisms under development would be
capable of deployment, or whether ISPs would act selﬁshly
and not bother to turn them on. A more strategic concern
was that the Tier 1 autonomous systems who make up the
core of the Internet were starting to undergo rapid consolida-
tion; where previously the Internet had been kept up by the
eﬀorts of a score of ﬁrms, which were large but not so large
as to be indispensible, price competition had led to takeovers
which in turn meant that Level 3 was accounting for almost
a third of transit traﬃc. (Other ﬁrms such as Google and
Akamai also had such market share that their failure could
do serious harm to the Internet.) We concluded that reg-
ulatory intervention was premature, but that governments
might usefully start paying attention to the problems and
sponsoring more research.
The third was kicked oﬀ in 2011 when Detica, a subsidiary
of the arms company BAe, published a marketing brochure
which claimed that cyber-crime cost the UK £27 billion a
year, and even persuaded the Cabinet Oﬃce (the UK gov-
ernment department responsible among other things for for
intelligence oversight) to put their name on it. This was
greeted with widespread derision, whereupon the Ministry
of Defenceˆa ˘A´Zs chief scientiﬁc adviser, Sir Mark Welland,
asked us whether we could come up with some more defen-
sible numbers.
By then there was a signiﬁcant research community to tap
into; in short order, Richard, Tyler and I recruited Stefan
Savage, who had done signiﬁcant work on tracking fake Vi-
agra marketing; Michel van Eeten, who’d investigated the
variability of botnet infection between ISPs; Rainer B¨ohme,
who had collected a lot of data on insurance, stock scams
and vulnerability markets; Chris Barton, who’d worked for
McAfee; and Michael Levi, an expert on white-collar crime.
The report that we produced from pooling our insights taught
us something new, that online crimes inﬂict very much greater
indirect costs than traditional villainy does. For example,
in 2010 the Rustock Botnet sent about a third of all the
spam in the world; we knew from Stefan’s analysis that it
made its operators about $3.5m, and from other ﬁgures that
ﬁghting spam cost about $1bn globally (some scaremongers
claimed that the true ﬁgure was two orders of magnitude
greater than this). We concluded that for every dollar the
Rustock guys took home, they inﬂicted a hundred dollars of
cost on the rest of us. Yet many of these scams are done by
a small number of gangs.
The conclusion was simple: we should be putting more
eﬀort into arresting the bad guys and locking them up. Of
course, the ﬁrms who sell spam ﬁltering ‘solutions’ don’t see
the world this way, and they have a lot of lobbying clout. As
a result, only a small fraction of cyber-security expenditures
do much good. For example, the UK government allocated
an extra £650m to cyber-security from 2011-2015, of which
the police got only £20m – a stingy £5m a year. But at least
we now have the data, and can start to point the ﬁnger at
the anti-virus industry as being part of the problem rather
than part of the solution.
Other work on the econometrics of online wickedness in-
cluded a survey paper that Tyler, Richard and I wrote for
the Journal of Economic Perspectives [11]. This is still a
work in progress; we have a large grant from the DHS be-
tween Cambridge, CMU, SMU and the NCFTA to work on
the economics of cybercrime. However, we now have the feel-
ing that we’re at the end of the beginning, and the numbers
are starting to add up.
5. THE BEHAVIOURAL REVOLUTION
A second major thread to emerge over successive WIS
conferences was what security engineers can learn from be-
havioural economics. This subject sits at the boundary be-
tween economics and psychology, and its subject matter is
the psychological heuristics and biases that cause people to
make systematic errors in their market behaviour. An exam-
ple is myopia: as I noted, Alessandro Acquisti had explained
at WEIS 2002 how people’s failure to anticipate quite pre-
dictable future harms can laed to failure of the market solu-
tions proposed by Hal for privacy problems. Also at WEIS
2002, Paul Thompson had talked about cognitive hacking,
a variant on the same theme.
A few months after the ﬁrst WEIS, the Nobel prize in
economics was won by Daniel Kahnemann, who with the
late Amos Tversky had pioneered the whole heuristics-and-
biases approach. This got people’s attention and started to
move behavioural economics towards the centre stage. We
had a steady stream of behavioural papers at successive con-
ferences, mostly from Berkeley students, until WEIS 2007
when the workshop was held at CMU and we had a keynote
talk from George Loewenstein, a distinguished behavioural
economist on the faculty there. By then Alessandro Ac-
quisti had moved to CMU, and taken up a joint post be-
tween George’s Department of Decision Sciences and the
Heinz business school; as CMU also has a strong technical
security research team under Virgil Gligor and the world’s
largest security usability research group under Lorrie Cra-
nor, this created a real centre of excellence.
At WEIS 2007, Alessandro, Bruce Schneier and I decided
142
that it was time to launch a Workshop on Security and Hu-
man Behaviour to create the space for the behavioural eco-
nomics of security to grow. While we were seeing several