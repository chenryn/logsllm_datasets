Recently, `functional.softmax` has been changed to accept `dim` as argument.  
When run on CPU, I can specify arbitrary value for `dim`, whether `input` is
2D or 3D.
For example, the following code works without any problem:
    In [23]: functional.softmax(Variable(torch.ones(3, 4)), dim=0)
    Out[23]: 
    Variable containing:
     0.3333  0.3333  0.3333  0.3333
     0.3333  0.3333  0.3333  0.3333
     0.3333  0.3333  0.3333  0.3333
    [torch.FloatTensor of size 3x4]
    In [24]: functional.softmax(Variable(torch.ones(3, 4, 5)), dim=1)
    Out[24]: 
    Variable containing:
    (0 ,.,.) = 
      0.2500  0.2500  0.2500  0.2500  0.2500
      0.2500  0.2500  0.2500  0.2500  0.2500
      0.2500  0.2500  0.2500  0.2500  0.2500
      0.2500  0.2500  0.2500  0.2500  0.2500
    (1 ,.,.) = 
      0.2500  0.2500  0.2500  0.2500  0.2500
      0.2500  0.2500  0.2500  0.2500  0.2500
      0.2500  0.2500  0.2500  0.2500  0.2500
      0.2500  0.2500  0.2500  0.2500  0.2500
    (2 ,.,.) = 
      0.2500  0.2500  0.2500  0.2500  0.2500
      0.2500  0.2500  0.2500  0.2500  0.2500
      0.2500  0.2500  0.2500  0.2500  0.2500
      0.2500  0.2500  0.2500  0.2500  0.2500
    [torch.FloatTensor of size 3x4x5]
However, when the computation is performed in GPU, the flexibility decreases.
    In [25]: functional.softmax(Variable(torch.ones(3, 4)).cuda(), dim=0)
    ---------------------------------------------------------------------------
    RuntimeError                              Traceback (most recent call last)
     in ()
    ----> 1 functional.softmax(Variable(torch.ones(3, 4)).cuda(), dim=0)
    ~/anaconda3/envs/ml/lib/python3.6/site-packages/torch/nn/functional.py in softmax(input, dim, _stacklevel)
        648     if dim is None:
        649         dim = _get_softmax_dim('softmax', input.dim(), _stacklevel)
    --> 650     return _Softmax(dim)(input)
        651 
        652 
    RuntimeError: invalid argument 4: dim has to be 1 for 2D input at /home/jhchoi/workspace/pytorch/torch/lib/THCUNN/generic/SoftMax.cu:32
    In [26]: functional.softmax(Variable(torch.ones(3, 4, 5)).cuda(), dim=1)
    ---------------------------------------------------------------------------
    RuntimeError                              Traceback (most recent call last)
     in ()
    ----> 1 functional.softmax(Variable(torch.ones(3, 4, 5)).cuda(), dim=1)
    ~/anaconda3/envs/ml/lib/python3.6/site-packages/torch/nn/functional.py in softmax(input, dim, _stacklevel)
        648     if dim is None:
        649         dim = _get_softmax_dim('softmax', input.dim(), _stacklevel)
    --> 650     return _Softmax(dim)(input)
        651 
        652 
    RuntimeError: invalid argument 4: dim has to be 0 for 3D input at /home/jhchoi/workspace/pytorch/torch/lib/THCUNN/generic/SoftMax.cu:42
Is it intended behavior? If it is, then I think it should be specified in the
document.