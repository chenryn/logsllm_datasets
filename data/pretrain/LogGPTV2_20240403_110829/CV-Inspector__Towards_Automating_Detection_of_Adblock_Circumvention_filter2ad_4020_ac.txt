adblocker is on. We use the (B)-(A) data to extract most features, train and
evaluate our classiﬁer.
No Adblocker vs. With Adblocker. Since websites typically
employ circumvention only when an adblocker is present,
we utilize differential analysis to obtain insights into cir-
cumvention “signals.” For each website, we collect data for
two different cases: (A) “No Adblocker” and (B) “With
Adblocker.” For “No Adblocker,” we load each site four times
and take the union of the collected data in order to capture
the dynamic nature of a site because a it can retrieve ads from
different ad servers. This is a heuristic but justiﬁed choice: we
experimented with loading the same page for a varying number
of times and found that the number of contacted domains
plateaus at four. We will refer to these “four page visits” per
case, throughout the paper. We repeat the same process for
“With Adblocker.” In addition, we use ABP and conﬁgure it
to use EL. We deselect the “Allow Acceptable Ads” option
as we want to make sure ads are shown due to circumvention
and not because it was whitelisted. Furthermore, this gives
sites the best chance to circumvent the adblocker and the best
opportunity for us to capture it.
Landing Page and Sub-pages. Our crawling considers both
landing pages and sub-pages. This is critical because sites may
not employ circumvention in their landing pages but rather wait
until the user clicks into a sub-page to show circumvented
ads (e.g., maxpark.com). To ﬁnd a sub-page, we inject JS
into the landing page to retrieve all URLs from hyperlink
tags. We select the ﬁrst-party link with the longest number
of path segments. We use the intuition that the deeper the user
explores the site, the more interested the user is in the content,
thus increasing the chance that the site would serve ads. We
ﬁnd that
this methodology works well for sites that have
articles. To further ensure that we ﬁnd a sub-page with ads,
we ignore informational pages using keywords (e.g., “contact,”
“login”) within the path. To only consider pages with content,
we further ignore ﬁrst-party links that have extensions (e.g.,
“.tar.gz,” “.exe”), to prevent downloading external ﬁles.
Automatic Collection. We use Selenium [51], a framework to
automate testing of websites, to implement the crawling pro-
cess. We select Chrome (version 78) [24] as the browser due
to its popularity. As depicted in Fig. 6, we create two Chrome
proﬁles. One proﬁle is for the “No Adblocker” case, where
we include the web request extension and DOM mutation
extension. The second proﬁle is for the “With Adblocker” case,
where we also include the custom ABP extension that only
loads EL. In order to have a consistent behavior with ABP,
we only use one version of EL and the ACVL from March
13, 2020. Then, we conﬁgure Selenium to disable caching
and clear cookies to have a stateless crawl. For scalability
purposes, we utilize Amazon’s Elastic Compute Cloud (EC2)
and select the “m5.2xlarge” instance that allows CPU usage
without throttling [14]. We create a snapshot out of the setup
using Amazon Machine Image (AMI) [15], which allows us
to spawn many instances of EC2 for data collection.
2) What data we collect for each page: Next, we describe
the types of information we collect for each site. We are
interested in how the site changes from “No Adblocker” to
“With Adblocker,” at four vantage points:
1) Web requests: HTTP incoming and outgoing requests.
2)
3)
DOM mutation for nodes, attributes, and text.
Time stamps of all events like web requests, DOM
mutations, and blocked events caused by ABP (i.e.,
when a ﬁlter rule is matched, see Table I).
Page source code of the site (e.g., HTML, text, inline
CSS, and inline JS).
4)
We also collect screenshots, which are capped at 1925x3000
to deal with websites that can inﬁnitely scroll. Screenshots
are useful as we use them to verify our ground truth in
Sec. IV-D. Next, we explain how this collected data can reveal
obfuscation-based circumvention employed by the site.
1. Collecting Web Requests. Circumvention providers often
randomize subdomains and paths as an obfuscation technique
to retrieve new ad content for reinsertion, going beyond
simply rotating domains [13], [42], as illustrated in Fig. 1(b).
Capturing web requests can help identify this behavior. Ex-
amples are provided in Sec. IV-C. We implement a Chrome
extension to collect web requests by hooking into the Chrome
Web Request API [37]. This API streamlines the ﬂow of
web requests into various life-cycle stages that developers
can easily subscribe to. Speciﬁcally, we hook into “onSend-
Headers” to collect outgoing HTTP request headers and “on-
Completed” to collect incoming HTTP response headers of
successful requests. To collect web requests blocked by ABP,
we hook into “onErrorOccurred” and look for status code
“ERR BLOCKED BY CLIENT.”
2. Collecting DOM Mutation. Fig. 1(b) shows that re-injected
ads are often reconstructed in step 5 and may not have the
same DOM structure as the originally blocked ads. Capturing
how the DOM changes as the page loads can help uncover
these particular actions. We build a Chrome extension that
uses DOM Mutation Observers [49] to collect DOM changes.
The extension compiles events such as new nodes added (e.g.,
an ad image being added), nodes removed (e.g., a script
being removed), attribute changes (e.g., an ad element from
height 0 to 280px), and text changes (e.g., anti-adblocker
popup text). Furthermore, recall from Table I that an adblocker
can do element hiding. We capture this by instrumenting the
ABP extension (version 3.7) and hook into methods that hide
elements when a ﬁlter rule is matched to label the elements
with a custom HTML attribute “abp-blocked-element,” shown
in Listing 1. Since this causes a DOM attribute change, we
consider this as part of the DOM Mutation information.
6
Browser No Adblocker (A)Browser With Adblocker (B)Differential Analysis(Sec. IV-B)Set Difference of  { x ∈ B and x ∉ A }Instrumentation and Data Collection (Sec. IV-A) Machine LearningCrawling ScriptWeb Requests CollectorDOM Mutation CollectorWebRequests DOM MutationTemporal EventsPage SourceFeature Extraction(Sec. IV-C)Ground Truth Labeling(Sec. IV-D)Classifier(Sec. IV-E)EasyListABData Collected
Listing 1. Page Source Annotations. Highlighted in blue, attribute “abp-
blocked-element” denotes that the adblocker has blocked the element. While
attribute “anticv-hidden” means that the img is not visible (not related to the
adblocker). All visible images and iframes are labeled with their offsetwidth
and offsetheight to give a more accurate representation of the page.
0
1 
2
3 
4 
5
6
7 
8 
a n t i c v − o f f s e t w i d t h = "728"
a n t i c v − o f f s e t h e i g h t = "90">
3. Collecting Temporal Information. Since circumvention
is typically a reaction to ads being blocked, timestamps of
changes on the page can reveal how adblockers and cir-
cumvention code interact with each other. Thus, we record
and consider timestamps for web requests, DOM mutation,
and blocked events. For completeness, when we consider the
ACVL in Sec. V-B, we hook into methods that abort the
execution of JS to capture JS blocked events as well.
4. Collecting Page Source with Annotations. We use Sele-
nium to save the page source of the site at the end of the page
load time. It gives us information about the state of the site
such as the HTML and text, inline CSS, and inline JS. In addi-
tion, it contains the annotated elements that are hidden by the
adblocker, as shown in Listing 1. Furthermore, since the page
source does not provide the actual visibility state of images and
iframes, we inject JS to annotate these elements with a custom
attribute “anticv-hidden” detailed in Listing 1. We extract all
images and iframes and consider the following cases. First,
if the element’s “offsetParent” is null and its “offsetWidth”
and “offsetHeight” are zero: this denotes that the element is
hidden due to its parent being hidden. Second, otherwise, we
use “window.getComputedStyles,” which provides us the ﬁnal
styles that are applied on the element. We consider styles such
as “display: none” and “opacity <= 0.1” to see if the element is
hidden. Third, we treat elements with width and height of less
than or equal to two as hidden. This ﬁlters out pixel elements
used for tracking. We further use these annotations for feature
extraction, as described in Sec IV-C.
3) Tools and Limitations: Using Amazon’s EC2 and AMI,
our methodology is scalable (e.g., multiple instances can be
initiated to ﬁt the problem) and conﬁgurable (e.g., number of
sub-pages to ﬁnd, which ﬁlter list to load). However, it also has
its limitations. First, some sites utilize Cloudﬂare’s protection
against web-crawlers where it shows a captcha, which prohibits
CV-INSPECTOR from accessing the page. Second, Selenium
may not properly produce screenshots, which depends on how
body styles are applied. We address this limitation by ﬁrst
checking whether the height of the body is zero. If so, then
we check the next immediate child element of the body to see
if it has a height to capture, and so on. Third, when discovering
sub-pages, we do not consider links from non-hyperlink tags
or if the site is utilizing JS to redirect users upon a click.
7
Finally, recall that we wait for 25 seconds during each page
visit, which might miss some behavior on sites that need longer
to load. This is a parameter to tune: longer crawling times is
possible at the expense of slowing down CV-INSPECTOR.
4) Data sets: We apply our methodology and collect
different data sets, summarized in Table II, which we then
use for different purposes throughout
the paper. For each
of these data sets, we start from a list of URLs, apply the
methodology described earlier in this section, and we collect
the four types of information, referred to as “collected data” in
Fig. 6: web requests, DOM changes, temporal information, and
page source with annotations. The top three data sets in Table
II are collected using our methodology based on a given list
of sites: ACVL sites, Tranco’s most popular sites and Adblock
Plus Monitoring. The ﬁrst two are publicly available.
ACVL has been extensively discussed in Sec. III and
includes sites that currently employ, or had employed in the
past, CV services; we use this list to ﬁnd positive samples. We
use Tranco ranked sites in two ways. First, since circumvention
is hard to ﬁnd, we use the Tranco top-2K sites within our
ground truth data set (GT) to ensure that it covers popular
sites. Second, we use the Tranco-20K data set (which excludes
the top-2K) to test our classiﬁer on popular sites that matter
to users. The third data set, internally maintained by ABP,
contains sites that employed circumvention at some point and
ABP continuously monitors them to see if ACVL is still
effective on them. We refer to sites that are closely monitored
by adblockers as “sites of interest.” Generally, this means that
the sites affect a large portion of adblock users (i.e., in terms
of popularity) or that the sites have caused users to submit
feedback about them.
The bottom part of Table II summarizes our three original
crawled data sets that we use for training and evaluating our
classiﬁers in Sections IV-D,
IV-E, and V.
B. Differential Analysis
1) Set Difference: Our intuition is that behavior observed
when an adblocker is used (“With Adblocker”) is different
from the behavior observed when there is no adblocker (“No
Adblocker”). This is likely due to CV services being triggered.
Recall from “No Adblocker vs. With Adblocker” of Sec.IV-A1,
that we need to account for the dynamic nature of websites.
Therefore, ﬁrst, we take the union of the data sets collected
across all four page visits in each case. Then, we take the
difference of the two union sets (“With Adblocker” minus “No
Adblocker”). Next, we elaborate on what differences we ex-
amine for each of the four types of data collected.
First, for web requests, circumvention services can serve
content behind ﬁrst-party domains. Therefore, we cannot sim-
ply do a set difference on the domain level for web requests,
which would eliminate the presence of the circumvented ads.
Instead, we do a set difference based on the fully qualiﬁed
domain and its path while disregarding the query parameters.
Second, for for DOM mutations, we create a signature for each
event based on the element’s attribute names, tag name, parent
tag name, and sibling count. We do not depend on the value of
attributes because they can be randomized [13], which would
introduce more unrelated events to circumvention. Instead, we
rely on the length of the value within our event signature. For a
Data Set Name List of Sites Crawled
Obtained by crawling a given list of sites
ACVL sites
Tranco
extracted
from ACVL
Sites
(public [4])
Most popular sites (top-20K) at
tranco-list.eu (public [58])
Sites that ABP monitors (main-
tained and provided by ABP)
Adblock Plus
Monitoring
Derived from ACVL & Tranco, used for ML training & testing
Candidate for
6.2K
labeling (CL)
Ground
Truth (GT)
ACVL ∪ Tranco top-2K
2.3K
360
# Pages &
Sub-pages
3K
32K
Subset of sites from CL that are
inspected and labeled (positive
or negative) for circumvention
Tranco top 2K-20K (excluding
the top-2K used in CL)
Subset of GT with only positive
labels
29.3K
700
Tranco-20K
Ground Truth
Positives