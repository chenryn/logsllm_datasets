**Is this a request for help?**  
If yes, please refer to our [troubleshooting guide](http://kubernetes.io/docs/troubleshooting/) and community support channels.

**Keywords searched in Kubernetes issues before filing this one:**
(If you have found any duplicates, please reply there instead.)

* * *

**Type of Issue:** (Choose one)  
- [x] Bug Report
- [ ] Feature Request

**Kubernetes Version:**
```
Server Version: version.Info{Major:"1", Minor:"4", GitVersion:"v1.4.6", GitCommit:"e569a27d02001e343cb68086bc06d47804f62af6", GitTreeState:"clean", BuildDate:"2016-11-12T05:16:27Z", GoVersion:"go1.6.3", Compiler:"gc", Platform:"linux/amd64"}
```

**Environment:**
- **Cloud Provider or Hardware Configuration:** AWS, c3.4xlarge instance with 60 GB gp2 root volume.
- **Operating System:** Debian GNU/Linux 8 (jessie)
- **Kernel Version:** `Linux 4.4.26-k8s #1 SMP Fri Oct 21 05:21:13 UTC 2016 x86_64 GNU/Linux`
- **Installation Tools:** Kops
- **Other Relevant Information:**

**Issue Description:**
The kubelet image garbage collection (GC) is triggered when the disk space falls below the minimum threshold. The kubelet begins deleting images to recover disk space, but by the end of the cleanup, the node is severely impacted. Pods become unresponsive, are restarted, and often rescheduled to other nodes. In this specific case, CPU usage drops to almost zero as everything has been moved off the node.

**Timeline of the Failure:**
- 16:56:38.000: GC started with the log message:  
  `image_gc_manager.go:238] [imageGCManager]: Disk usage on "/dev/xvda1" (/) is at 90% which is over the high threshold (90%). Trying to free 5766938624 bytes.`
- 16:56:38: First log message indicating an image deletion:  
  `image_gc_manager.go:303] [imageGCManager]: Removing image...`
- 17:01:38: Final log message for the 8th image deletion.
- 17:02: Graphs show that all pods have been moved off the node, and no processes are running on it.

During this period, the node's `Ready` state in the dashboard was `Unknown` rather than `True`. After the GC, the node recovers and becomes functional again, working fine when new pods are scheduled back onto it. As expected, I/O is higher during the cleanup, but other metrics do not indicate an overloaded instance.

**Expected Behavior:**
No impact on running services on the node.

**Steps to Reproduce:**
This issue has occurred on three different nodes, all configured identically. It seems that every time the GC is triggered, it causes this impact.

**Additional Information:**
- Briefly discussed with @justinsb.
- A hypothesis: Could the `im.imageRecordsLock.Lock()` at the top of `image_gc_manager.go->freeSpace` cause the kubelet to stop processing other requests while the GC is occurring?

**Graphs:**
- ![Disk Usage](https://cloud.githubusercontent.com/assets/3944676/21147113/e2439eb6-c121-11e6-9662-c53a0c74e5e5.png)
- ![CPU Usage](https://cloud.githubusercontent.com/assets/3944676/21147121/e9044002-c121-11e6-9d1a-18153c1ba9e3.png)
- ![Pod Status](https://cloud.githubusercontent.com/assets/3944676/21147131/f22e8840-c121-11e6-9b57-f9d15a64fc34.png)

Thank you for your attention to this issue.