 **Is this a request for help?** (If yes, you should use our troubleshooting
guide and community support channels, see
http://kubernetes.io/docs/troubleshooting/.):
**What keywords did you search in Kubernetes issues before filing this one?**
(If you have found any duplicates, you should instead reply there.):
* * *
**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):Bug
**Kubernetes version** (use `kubectl version`):  
Server Version: version.Info{Major:"1", Minor:"4", GitVersion:"v1.4.6",
GitCommit:"e569a27d02001e343cb68086bc06d47804f62af6", GitTreeState:"clean",
BuildDate:"2016-11-12T05:16:27Z", GoVersion:"go1.6.3", Compiler:"gc",
Platform:"linux/amd64"}
**Environment** :
  * **Cloud provider or hardware configuration** : AWS, this particular node is a c3.4xlarge with 60 GB gp2 root volume.
  * **OS** (e.g. from /etc/os-release): Debian GNU/Linux 8 (jessie)
  * **Kernel** (e.g. `uname -a`): Linux 4.4.26-k8s #1 SMP Fri Oct 21 05:21:13 UTC 2016 x86_64 GNU/Linux
  * **Install tools** : Kops
  * **Others** :
**What happened** :  
The kubelet image garbage collection is triggered because disk space falls
below the minimum threshold. Kubelet starts image deletions to recover disk
space but by the end of the cleanup the node has been severely impacted with
pods becoming unresponsive, restarted, and often rescheduled onto other nodes.
In this particular case you can see that CPU becomes almost zero because
everything has been moved off this node.
Timing of this particular failure:  
16:56:38.000 GC started with `image_gc_manager.go:238] [imageGCManager]: Disk
usage on "/dev/xvda1" (/) is at 90% which is over the high threshold (90%).
Trying to free 5766938624 bytes` message in logs  
16:56:38 See the first `image_gc_manager.go:303] [imageGCManager]: Removing
image` message in the logs  
17:01:38 See the final `Removing Image` message in the logs which is the 8th
image to be deleted  
17:02 By this time graphs show just about everything has been moved off this
node and there is nothing left running on it.
The following graphs seem to indicate that as soon as the GC has finished
kubelet "wakes up" and realizes everything has been removed from this node. In
the dashboard I saw this node `Ready` state as `Unknown` instead of `True`
during this period. It recovers and becomes functional after this hiccup and
works fine when new pods get scheduled back onto the node.
As expected the IO is higher during the cleanup but other metrics look fine
and don't indicate a overloaded instance.
![image](https://cloud.githubusercontent.com/assets/3944676/21147113/e2439eb6-c121-11e6-9662-c53a0c74e5e5.png)
![image](https://cloud.githubusercontent.com/assets/3944676/21147121/e9044002-c121-11e6-9d1a-18153c1ba9e3.png)
![image](https://cloud.githubusercontent.com/assets/3944676/21147131/f22e8840-c121-11e6-9b57-f9d15a64fc34.png)
**What you expected to happen** :  
No impact to running services on node.
**How to reproduce it** (as minimally and precisely as possible):  
This issue has happened on 3 different nodes all configured the same way. I
don't believe a GC has occurred without causing this impact.
**Anything else do we need to know** :  
Discussed briefly with @justinsb
Just a guess but is there any chance `im.imageRecordsLock.Lock()` at the top
of `image_gc_manager.go->freeSpace` causes kubelet to stop processing other
requests while GC is occuring?