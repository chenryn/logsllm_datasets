is merely a reformulation of an IFC abstract machine to factor
a “rule table” (written in a simple IFC-speciﬁc domain-speciﬁc
language) out of the semantics. In contrast, our symbolic
machine is generic and is reused by all micro-policies. On
the other hand, the proofs in [4] include the veriﬁcation of
an IFC monitor at the machine code level using a framework
for structured code generators and a veriﬁed DSL compiler,
both specially crafted for the simple architecture used there.
Here, we chose to focus on designing a generic micro-policy
framework and on building and verifying the symbolic machine
instances for a diverse set of micro-policies, leaving concrete-
level implementation and veriﬁcation for later.
Another paper on the PUMP [11] proposes implementation
optimizations for its hardware architecture and experimentally
evaluates their overhead for a set of micro-policies including
CFI and memory safety, plus a taint-tracking micro-policy. Our
work here is complementary, focusing on formal speciﬁcation
and veriﬁcation of micro-policies. Also, the micro-policies
for compartmentalization and dynamic sealing, as well as the
mechanisms for monitor services and monitor self-protection
described here, are new.
Reference Monitors Reference monitors have been around
since the early seventies [3]. However, building low-overhead
enforcement mechanisms for a broad set of policies has proved
challenging. Besides low overhead, Anderson’s seminal work
mentions a set of security requirements for reference monitors:
“(a) The reference monitor must fully mediate all operations
relevant to the enforced security policy. (b) The integrity of the
reference monitor must be protected, either by the reference
monitor itself or by some external means. (c) The correctness
of the reference monitor must be assured, in part by making
the reference monitor be small enough to analyze and test.”
Micro-policies meet all these requirements: (a) they provide
complete mediation at the level of individual instructions;
(b) they include mechanisms for using tags to protect the
integrity of monitor code and data structures (§9); and (c) the
TCB of micro-policy monitors is generally quite small. More-
over, we achieve high conﬁdence in the their correctness by
formal veriﬁcation of symbolic policies in Coq; in the future
we hope also to verify low-level concrete implementations.
Finally, while precisely characterizing the class of properties
that can be expressed as micro-policies and efﬁciently enforced
by the PUMP is an interesting open problem, we know for
sure that it includes very important security properties: IFC,
CFI, compartmentalization, and memory safety. Inspiration for
attacking the expressiveness question formally may come from
the work by Schneider et al. [15], [29] on execution monitors
and program rewriting.
11 Conclusions and Future Work
We have presented a generic veriﬁcation framework for a rich
class of low-level, hardware-accelerated, tag-based security
enforcement mechanisms. The micro-policies we verify target
a wide range of critical security properties, illustrating the
power of a simple but ﬂexible hardware mechanism.
Our Coq development runs to about 17.7k lines of code,
out of which 4.8k lines are generic (2.8k for the symbolic
machine and the generic symbolic-concrete reﬁnement proof)
and the rest speciﬁc to our four micro-policies (4.9k for com-
partmentalization, 4.7k for CFI, 1.9k for memory safety, and
1.2k for dynamic sealing). Our Coq development is available
at https://github.com/micro-policies/micro-policies-coq.
We are currently working on a micro-policy for call-stack
protection, as well as extensions of the current policies, such
as memory protection for stack-allocated data and unboxed
structs. An obvious question at the level of the framework
itself is how to compose micro-policies. Certain micro-policies
are known to compose sensibly, and micro-architectural op-
timizations ensure that they perform well on practical work-
loads [11], but the general picture remains unclear. Another
obvious target for future work is formalizing the symbolic rule
language that we used informally here for exposition.
Our framework currently targets a simpliﬁed ISA with a
limited instruction set, a single core, no hardware concurrency
or interrupts, etc. An interesting challenge is to scale our
formalization to a more realistic RISC architecture such as
MIPS, Alpha, RISC-V, or ARM extended with a PUMP. Also,
we have not explicitly considered the role of the compiler
or loader here, although in reality their support is crucial for
some policies. For example, CFI relies on having a control-ﬂow
graph, which would naturally come from a compiler, and on the
initial tags on instructions, which would have to be added or at
least vetted by the loader. We have not formalized the operating
system or its interaction with micro-policy monitoring. Indeed,
micro-policies might even live below an OS, and could then
help protect the OS itself from attacks. Another alternative
(discussed in [11]) is to only protect user-level code, but this
would lead to a larger TCB.
Acknowledgments We are grateful to Delphine Demange,
Udit Dhawan, Andr´e DeHon, Greg Morrisett, Steve Zdancewic,
and the anonymous reviewers for helpful discussions and
thoughtful feedback on earlier drafts. This material is based
upon work supported by the DARPA CRASH program through
the US Air Force Research Laboratory (AFRL) under Contract
No. FA8650-10-C-7090. The views expressed are those of the
authors and do not reﬂect the ofﬁcial policy or position of
the Department of Defense or the U.S. Government. Tolmach
was partly supported by a Digiteo Chair at Laboratoire de
Recherche en Informatique, Universit´e Paris-Sud.
Appendix
A Details of the CFI Micro-Policy
CFI Property and Attacker Model We give a generic CFI
deﬁnition that can be instantiated to all three levels of machine,
adapting the original deﬁnition by Abadi et al. [1] to our setting.
The two main technical differences are that (1) at the symbolic
level, the tag-based mechanism detects a CFI violation on the
step after it has occurred (i.e., when checking the instruction
following an illegal control transfer, rather than the instruction
that caused the transfer) and (2) at the concrete level, detecting
827827
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:04:44 UTC from IEEE Xplore.  Restrictions apply. 
a violation and halting the machine is a nontrivial process
involving missing in the hardware rule cache and running the
software miss handler, which eventually halts. These differences
do not affect security (at both levels, the machine halts before
it does anything externally visible), but they lead to a slightly
more complex CFI deﬁnition.
As usual [1], the deﬁnition is given with respect to an
extended step relation →, which is the union of normal machine
steps →n and attacker steps →a.The →n and →a relations
are parameters of the general CFI deﬁnition. The →a relation
represents an overapproximation of the attacker’s capabilities,
allowing the attacker to change any user-level data in the system
but none of the code. At the concrete and symbolic levels, the
attacker will also be prevented from directly changing the tags.
This models an attacker that can mount buffer-overﬂow attacks
but has no built-in capability for subverting our NWC, NXD,
or CFI protections (e.g., no hardware backdoor).
We start by deﬁning when a trace has CFI with respect to
a set of allowed indirect jumps J (a binary relation on code
addresses). From J we can construct the complete CFG, a
relation on machine states written cfg J. This involves adding
all direct CFG edges that are clear in the code (e.g., a Nop or
Bnz can reach the next instruction; a Bnz can reach its target).
Deﬁnition A.1. We say that an execution trace s0 → s1 →
. . . → sn has CFI if (si, si+1) ∈ cfg J for all i ∈ [0, . . . , n)
such that si →n si+1.
Compared to Abadi et al. [1], this deﬁnition additionally
requires that an attacker step which happens to be a valid
normal step must also be in the CFG, which is helpful for
proving CFI preservation. This deﬁnition is slightly stronger
than Abadi et al.’s; however, we instead use the following
incomparable deﬁnition, which allows a single violation in a
trace, as long as the machine is “stopping” afterwards.
Deﬁnition A.2 (CFI). We say that the machine (State, init,→n,
→a, cfg, stopping) has CFI with respect to the set of allowed
indirect jumps J if, for any execution starting from initial state
s0 and producing a trace s0 → . . . → sn, either (1) the whole
trace has CFI according to Deﬁnition A.1, or else (2) there is
some i such that si →n si+1, and (si, si+1) (cid:10)∈ cfg J, where
the sub-traces s0 → . . . → si and si+1 → . . . → sn both have
CFI and the sub-trace si+1 → . . . → sn is stopping.
At the abstract and symbolic levels a trace is stopping if it
is formed only of attacker steps (→a) between states that are
all stuck with respect to normal steps ((cid:10)→n). This deﬁnition
expresses the fact that the attacker can take steps even after a
violation has occurred and the machine has halted with respect
to normal steps. At the concrete level the attacker can even take
steps while the machine is halting; this is discussed together
with the concrete machine for CFI.
Abstract CFI Machine The abstract machine has CFI by
construction. It has separate instruction and data memories
(im and dm); the instruction memory is ﬁxed (NWC), and all
executed instructions are fetched from this memory (NXD):
dm
(STORE)
reg[rs] = w
(cid:3) = dm[p←w]
decode i = Store rp rs
reg[rp] = p
(cid:3), reg, pc + 1, true)
im[pc] = i
(im, dm, reg, pc, true) →n (im, dm
Machine states contain an additional bit ok. The machine
executes instructions only when this bit is true; otherwise
it gets stuck with respect to normal steps (the attacker can
take steps at any time). Indirect jumps are checked against the
allowed set J; if the control ﬂow is invalid the jump is taken
but the violation is recorded by setting ok to false so that the
machine will now be stuck with respect to normal steps. This
behavior is designed to match rule-based enforcement at lower
levels, thus simplifying the proofs (we can prove a 1-backward
SA-simulation instead of a {0, 1} one).
decode i = Jal r
reg[r] = pc(cid:3)
im[pc] = i
reg(cid:3) = reg[ra←pc + 1]
ok = (pc, pc(cid:3)) ∈ J
(im, dm, reg, pc, true) →n (im, dm, reg(cid:3), pc(cid:3), ok)
While the CFI micro-policy does not provide any monitor
services itself, the abstract machine fully exposes (“paravirtu-
alizes”) the lower-level monitor service mechanism—that is,
the abstract machine can be instantiated with an arbitrary set
of monitor services.
(JAL)
get service pc = (f, ti)
f (im, dm, reg, pc, true) = (im, dm
(im, dm, reg, pc, true) →n (im, dm
As in all other step rules, we proceed only when the ok bit is
true, which prevents monitor service calls outside the allowed
CFG.
(cid:3), reg(cid:3), pc(cid:3), true)
(cid:3), reg(cid:3), pc(cid:3), true)
(SVC)
The step rules above capture the intuition of a machine that
has CFI by construction. With the exception of the rule for
Load, the remaining rules are straightforward; we show just
the ones for Load and Bnz:
im[pc] = i
reg[rp]=wp
decode i = Load rp rs
im[wp]=w ∨ dm[wp]=w
reg(cid:3) = reg[rs←w]
decode i = Bnz r n
pc(cid:3) ← if w = 0 then pc+1 else pc+n
(im, dm, reg, pc, true) →n (im, dm, reg(cid:3), pc + 1, true) (LOAD)
mem[pc] = i
reg[r]=w
(im, dm, reg, pc, true) →n (im, dm, reg(cid:3), pc(cid:3), true)
Notice that the Load rule allows loading a word from either
the instruction or the data memory, capturing the intuition that
instructions can be loaded as data. The disjointness of the two
memories (and thus the fact that the rule is deterministic) is
guaranteed by the simulation relation between the symbolic
and the abstract machine. The step rule for Bnz demonstrates
the fact that we only check indirect jumps, not direct ones, for
control-ﬂow violations.
(BNZ)
Proving CFI for this abstract machine is easy. We capture
the attacker’s capabilities by the following relation:
dom reg = dom reg(cid:3)
(cid:3), reg(cid:3), pc, ok)
a (im, dm
(cid:3)
dom dm = dom dm
(im, dm, reg, pc, ok) →A
This allows the attacker to arbitrarily change the data memory
and registers at any time. Finally, the only requirement on
initial states is that the ok bit starts out true.
828828
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:04:44 UTC from IEEE Xplore.  Restrictions apply. 
Theorem A.3 (Abstract CFI). This abstract machine has CFI.
Symbolic CFI Machine The symbolic micro-policy for CFI
was described in §6. For completeness, we present the rest
of the symbolic rules. The case for Jump is identical to Jal.
For the other operators (besides Jump, Jal or Store), we again
have one rule to deal with the case of a jump target...
(src, dst) ∈ J
op : (Code src, Code dst,−,−,−) → (Code ⊥,−)
... and a different one when execution did not take a jump:
op : (Code ⊥, Code
,−,−,−) → (Code ⊥,−)
We capture the symbolic-level attacker by the relation
mem →S
a mem(cid:3)
(mem, reg, wpc@tpc) →S
reg →S
a reg(cid:3)
a (mem(cid:3), reg(cid:3), wpc@tpc)
where the relation on memories and registers is the pointwise
extension of the following inductive relation on atoms:
w1@Data →S
a w@(Code id)
This allows attackers to change words tagged Data but not
words tagged Code and not the tags themselves.
w@(Code id) →S
a w2@Data
Two properties are invariant under execution: all words in
memory tagged Code addr are indeed located at address addr,
and all sources and destinations in J are tagged Code addr. A
symbolic machine state is initial if it satisﬁes these invariants
and the pc is tagged Data (no jump in progress).
Concrete Machine To obtain a useful result about CFI on
the concrete machine, it is not enough to simply instantiate the
generic reﬁnement result from §9, as we do for other policies;
we must ﬁrst deﬁne concrete versions of each concept used
in the statement of the CFI property. The concrete attacker is
only allowed to take steps when the machine is in user mode.
It can change memory and registers but not the contents of the
cache, the pc, or the fpc.
mem →C
reg →C
a reg(cid:3)
a mem(cid:3)