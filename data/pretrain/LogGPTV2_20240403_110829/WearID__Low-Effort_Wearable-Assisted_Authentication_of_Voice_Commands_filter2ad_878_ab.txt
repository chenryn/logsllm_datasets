a cloud server for user authentication. To realize the cross-domain
similarity comparison, we develop a training-free algorithm that
converts high-fidelity microphone data into a low-fidelity alias-
ing form and correlates the time-frequency characteristics of the
WearID
ACSAC 2020, December 7‚Äì11, 2020, Austin, USA
speech signals in the vibration domain and the audio domain to
verify the voice command. The algorithm could be easily integrated
with existing VA systems and wearables without any hardware
modification.
Recent studies [32, 52] have shown the initial success of using
motion sensors on smartphones to capture the speaker‚Äôs voice.
However, examining the cross-domain similarity in practical sce-
narios using aerial speech vibrations captured by motion sensors
in wearables is nontrivial. First, the unique response of the wear-
able‚Äôs motion sensor to aerial speech and the associated acoustic
characteristics in the vibration domain remains unclear. Second,
the heterogeneous hardware designs and the huge sampling rate
gap (e.g., 8000ùêªùëß vs. 200ùêªùëß) make it hard to compare the acoustic
characteristics from the vibration and audio domains directly. Thus,
we must quantify the relationship between two distinct domains to
support a training-free user authentication approach. Third, the syn-
chronization of the data collection in totally different hardware is
difficult. Fourth, the proposed system should defend against various
audible [30, 42] and inaudible attacks [12, 51].
To ensure reliable cross-domain comparison, we extensively
study response distances and unique characteristics of aerial speech
vibrations captured by wearables. We develop a spectrogram-based
method to model the complex relationship between the voice com-
mand signals in the vibration and the audio domains and enable
similarity comparison between them. Particularly, we propose to
convert the spectrogram of high-frequency microphone data to the
low-frequency aliasing one that is comparable to the accelerometer
spectrogram. To enhance the reliability, we quantify the frequency
selectivities of the accelerometer and the microphone and select the
frequency components that are sensitive for both sensing modali-
ties for comparison. Moreover, to address the residual synchroniza-
tion errors caused by network delay, we develop a 2D-correlation
based method to align the spectrograms of the two sensing do-
mains through searching for an offset that results in the maximum
correlation.
Our Contributions:
‚Ä¢ We show that the aerial speech vibrations of human voices can
be captured by the accelerometer embedded in wearable devices.
This could serve as an additional domain (i.e., vibration domain)
to the original audio domain to verify the highly critical com-
mands of the user and provide enhanced security for the VA
system.
‚Ä¢ We propose a unique voice command authentication system,
WearID, which can be easily integrated with the existing VA
systems and wearable devices without making any hardware
modifications. The system is low-effort and privacy-preserving
as it does not require any prior training, and therefore does not
need to store privacy-sensitive voice sample templates.
‚Ä¢ We leverage the accelerometer‚Äôs short response distance to voice
to effectively prevent the impersonation/replayed sounds from
accessing the wearable. We derive the unique spectral relation-
ship between the aerial speech vibrations captured by wearables‚Äô
accelerometers and the audio recorded by VA‚Äôs microphones, we
propose cross-domain comparison that can effectively examine
the similarity of weak and low-resolution signals in the vibration
domain the and speech signals in the audio domain.
‚Ä¢ We conduct extensive experiments and user studies with differ-
ent smartwatches models and participants, which result in 600
human voice segments. The results show that WearID can au-
thenticate user‚Äôs voice commands with 99.8% accuracy in the
normal situation and detect 97.2% of various impersonation and
replay attacks with a low false negative rate of 2%. When under
the hidden voice and ultrasound attacks [51], WearID achieves
close to 100% accuracy of authenticating the users.
2 RELATED WORK
Audio-domain Voice Authentication and Security Issues. The
traditional user authentication methods designed for voice access
systems mainly extract each individual‚Äôs voice features in the audio
domain to identify users [11, 25, 26, 36, 44, 48]. Mel-Frequency
Cepstral Coefficients (MFCCs) [33] and Spectral Subband Centroids
(SSCs) [28] describe a voice‚Äôs timbre and vocal-tract resonances and
are widely used as unique voice features to distinguish users. The
modulation frequency [6] capturing formant and energy transition
details of a voice sound contains speaker-specific information for
user identification. However, only relying on the audio-domain
features has been shown to be vulnerable to acoustic-based attacks.
For example, an adversary can spoof the legitimate user to pass a
voice authentication system by recording and replaying a user‚Äôs
voice sound [30]. Moreover, the adversary can study the user‚Äôs daily
speech to impersonate or synthesize the user‚Äôs voice to pass the
voice authentication [17, 30, 42].
WearID Versus Other Authentication Methods. Rather than
using voice features, recent research studies propose to defend
against replay attacks by determining the liveness of sound source [14,
53, 54]. Specifically, Chen et al. [14] examine the unique magnetic
field patterns generated by electro-acoustic transducers to detect
loudspeaker-generated voice. VoiceLive [54] and VoiceGesture[53]
detect the dynamic acoustic characteristics (via time-difference-
of-arrival and Doppler shifts) that only occur in human voices to
identify liveness. However, these approaches focus on smartphones
and require users to place the smartphone‚Äôs microphone close to
the mouth. Thus, they are not applicable to the VA systems (e.g.,
Google Home and Amazon Alexa) that allow users to give voice
commands freely from a distance. Feng et al. [20] develop a user
verification system for the VA systems by capturing the user‚Äôs fa-
cial vibrations via an accelerometer with high-sampling rate (i.e.,
11kHz) embedded in a pair of glasses. The vibrations are then com-
pared with the voice recorded by the VA system to verify whether
the voice command is given by the legitimate user wearing the
glasses. In contrast, WearID addresses a more challenging prob-
lem as it studies much weaker and low-resolution aerial speech
vibration signals sampled by wearable accelerometers at 100Hz.
With such a capability, WearID can be seamlessly integrated into
wrist-worn wearable devices (e.g., smartwatches, fitness trackers)
that are already widely accepted worldwide.
Vibration-domain Voice Recognition. Recent studies show
that the MEMS motion sensors (e.g., accelerometer and gyroscope)
are able to capture acoustic sounds [5, 16, 24, 32, 52]. Gyrophone [32]
utilizes the gyroscope in a smartphone to recognize the speaker‚Äôs in-
formation (e.g., gender and speaker identity) from the speech played
by a loudspeaker. Accelword [52] leverages the accelerometer in
ACSAC 2020, December 7‚Äì11, 2020, Austin, USA
Cong Shi, Yan Wang, Yingying Chen, Nitesh Saxena, and Chen Wang
a smartphone rather than a microphone to recognize the user‚Äôs
wake word sound(e.g., Siri), which reduces the energy consump-
tion. Speechless [4] further analyzes the speech privacy leakage,
including the speech content from the smartphone motion sensors
under various attacking scenarios. These works require much effort
to train the system with motion sensor data and do not reveal the
relationship between the sensor readings and real voice recorded by
microphones. Spearphone [5] uses the accelerometer of the smart-
phone to eavesdrop speeches from the vibrations generated from
the built-in loudspeaker, which requires the accelerometers and
the loudspeakers on the same device. The impacts of aerial speech
vibrations on the motion sensors in wearables are not yet clear.
3 ATTACK MODEL
We consider an adversary who is interested in obtaining the user‚Äôs
private/sensitive information or exerting an unpermitted operation
through critical voice commands on the VA device shared among
multiple users (e.g., at an office or home). We assume that the
adversary cannot physically break the VA device, take control of
the VA cloud service, or get the possession of the user‚Äôs wearable
device. We summarize the potential attacks in two major categories:
Attack on User‚Äôs Absence. This type of attacks can be launched
when the user is away from the VA device. The adversary tries to
get close to the VA device and fool the VA system by using his own
voice or audio playback techniques:
‚Ä¢ Random Attack. The adversary does not have the prior knowledge
of the victim‚Äôs voice and attempts to fool the VA system with
his own voice. Despite the simple approach, such attacks can
achieve a considerable attack success rate of about 3.5% [50] on
state-of-the-art speaker verification approaches.
‚Ä¢ Impersonation Attack. An experienced adversary that has the
knowledge of the victim‚Äôs voice could try to spoof the VA system
by mimicking the victim‚Äôs voice. The adversary can produce the
voice sound by using speech synthesis techniques and playback
devices (e.g., loudspeaker).
‚Ä¢ Replay Attack. The adversary tries to capture the victim‚Äôs voice
commands via a recording device (e.g., the microphone of a smart-
phone) and replay the recorded voice via a loudspeaker, attempt-
ing to fool the VA system.
Co-location Attack. This type of attacks can be launched sur-
reptitiously even when the victim is present near the VA device:
‚Ä¢ Hidden Voice Command Attack. The adversary could inject the
recorded user‚Äôs voice commands into the background music or
the audio channel of video streams [49]. He could also provide
hidden voice commands that exploit the underlying mechanisms
(e.g., GMM-HMM models [12]) of VA systems. Such attacks could
stealthily spoof the VA systems without being perceived by hu-
man subjects. To avoid being noticed from the audible reply, an
adversary can first control the volume to mute the VA device via
hidden voice commands.
‚Ä¢ Ultrasound Attack. The adversary could modulate the recorded
voice commands of a victim to the ultrasound frequency band (i.e.,
‚â• 20ùêæùêªùëß), and use the modulated sound to fool the VA system
stealthily. Although human ears cannot hear the modulated voice
commands, they could be recognized by existing VA systems due
to the non-linearity of the microphone [51].
Figure 2: User authentication overview.
4 USER AUTHENTICATION DESIGN
4.1 Why Wearable? Why Motion Sensor?
Since the number of wearable users has reached half a billion world-
wide [40], it is natural for us to explore such pervasiveness and use
wearable devices in our design. These devices are usually worn on
the user body and rarely left unattended, making it eligible as a
trusted device. For example, smart wristbands have been used as a
replacement to student ID card [1] since they are hard to forget to
carry. As another example, smartwatches have been accepted as a
convenient and valid security token for contactless payment [34].
In this paper, we propose to utilize motion sensors in commercial
wearable devices (e.g., smartwatches, smart wristbands, and activity
trackers) to capture users‚Äô voice commands for user authentica-
tion. We choose to use motion sensors because it captures distinct
characteristics of voice sound in the vibration domain. Such unique
characteristics are harder to forge compared to the voice sound cap-
tured by microphones. As a result, our system enables VA systems
to resist acoustic-based attacks, including audible and inaudible
attacks, which can effectively attack existing user authentication
methods for VA systems. The effectiveness of WearID on defend-
ing the audible attacks and the inaudible attacks are discussed in
Section 7.3 and Section 7.4, respectively.
4.2 Challenges
In order to conduct cross-domain comparison for user authentica-
tion, a number of challenges need to be addressed.
‚Ä¢ Weak Response to Human Speech. Due to the design pur-
pose of measuring acceleration force, wearables‚Äô accelerometers
have weak responses to aerial speech vibrations caused by hu-
man speeches while being sensitive to human motions that are
considered as noises in our system. Such inherited character-
istics of accelerometers make it difficult to determine speech
Time-frequency Feature ExtractionFeature Extraction andDomain ConversionWake WordVoice Assistant Device Microphone Data   CollectionWearable DeviceCoarse-grained Synchronization Accelerometer Data   Collection‚ÄúOK, Google‚ÄùWiFiCommunication based ApproachAudio-domain Feature ExtractionVibration-domain Feature ExtractionMicrophone ReadingsAccelerometer ReadingsData Denoising and SegmentationData Denoising and SegmentationSpectrogram Calibration based on 2D-normalizationCross-Domain User AuthenticationCross-domain Comparison based on 2D-Serial CorrelationMatching?Voice Command VerifiedVerification FailedParallel Wake Word Detection    based ApproachWearID
ACSAC 2020, December 7‚Äì11, 2020, Austin, USA
Figure 3: Hardware flow of microphone and motion sensor.
segments and disentangle aerial speech vibrations from noisy
accelerometer readings.
‚Ä¢ Complex Cross-domain Relationship. Comparing human speech
representations in the vibration domain and those in the audio
domain is challenging. The heterogeneous hardware designs lead
to distinctive frequency-selectivity patterns in accelerometer and
microphone readings, making it hard to find similar acoustic
characteristics through cross-domain comparisons. Moreover,
the huge sampling rate gap (i.e., 8000ùêªùëß versus 200ùêªùëß) of the
two different sensors render any direct comparison between the
vibration signals and the audio signals impossible.
‚Ä¢ Coarsely synchronized acoustic signals in two domains.
Network delay introduces unpredictable offsets between the vi-
bration signals captured with the wearable and the audio signals
recorded with the VA device. It is necessary to align the signals
from the two sensors for a reliable comparison.
4.3 System Flow
Toward this end, we develop a wearable-assisted low-effort user
authentication system, WearID, which verifies the authenticity of
critical voice commands by examining the cross-domain similarity
of the unique voice characteristics captured with accelerometer
of the wearable device and microphone of the VA device. As illus-
trated in Figure 2, after a critical command/wake word is detected
by the VA system, the system performs the Coarse-grained Syn-
chronization to ensure that the VA and wearable devices start the
data collection process simultaneously. Depending on the network
condition (e.g., WiFi network delay), we develop two approaches for
the Coarse-grained Synchronization. When the network delay is low
and suitable for synchronization, the WiFi Communication-based
Approach allows the VA device to detect the critical command/wake
word, start its data collection, and send a notification to trigger the
data collection on the wearable via the WiFi connection. Since there
is a growing trend of having the motion sensors always activated on
a wearable device (e.g., for fitness tracking), we propose an alterna-
tive solution, the Parallel Wake-word Detection-based Approach, for
the cases of high network delay. In particular, the system exploits
the accelerometers on the wearable device to detect the wake word
in parallel with the VA device, which triggers the data collection on
both devices separately. When the wearable and the VA device are
coarsely synchronized, the voice command right after the detected
wake word is recorded by both devices for user authentication using
cross-domain comparison.
Next, WearID exploits the Vibration Domain Feature Derivation
and Audio Domain Feature Derivation to derive time-frequency fea-
tures from the voice command captured in the vibration domain
and the audio domain, respectively. The Vibration Domain Feature
(a) VA‚Äôs microphone
(b) Accelerometer on Huawei watch 2 sport
Figure 4: Frequency responses of a microphone and ac-
celerometer (Z axis) to a chirp signal (500ùêªùëß ‚àº 1000ùêªùëß).
Derivation first removes the noises caused by human motions in