uncertainties (i.e., the examples for which the predictive entropy is above a pre-determined threshold τ), which corresponds
to the real-world use of the quantified predictive uncertainty.
Table 2: Predictive uncertainty of malware detectors that are
learned on the Drebin dataset and tested on the VirusShare.
Calibration
method
N Vanilla
n
i
b
e
r
D
p
e
e
D
N
l
a
d
o
m
i
t
l
u
M
d
i
o
r
D
p
e
e
D
c
e
t
e
d
i
o
r
D
0.35
0.31
0.20
3.58
1.39
0.95
2.15
1.79
2.59
0.21
0.19
0.13
0.19
0.19
0.18
0.17
0.18
0.18
Uncertainty evaluation
Detection(%)
Acc
Vanilla
63.96 77.59 5.52
Temp scaling 63.96 77.59 1.65
MC Dropout 74.39 83.53 1.48
VBI
Ensemble
wEnsemble
bAcc NLL bNLL BSE bBSE ECE uECE
0.359 0.483
0.363 0.468
0.274 0.463
82.69 87.07 0.64 0.52 0.13 0.10 0.210 0.434
0.394 0.466
61.76 78.04 3.43
0.32
0.397 0.469
59.49 76.59 3.08
0.32
69.02 80.48 4.05
0.306 0.463
0.29
0.303 0.464
Temp scaling 69.02 80.48 1.45 1.13 0.25
0.361 0.473
2.58
0.32
MC Dropout 64.38 78.38 4.61
1.25
0.414 0.478
58.17 77.21 2.32
VBI
0.33
1.34 0.21 0.14 0.279 0.450
72.66 82.20 2.24
Ensemble
1.26
0.379 0.471
0.17
wEnsemble
61.73 77.74 2.21
0.463 0.481
2.11
0.27
Vanilla
52.85 70.02 3.44
0.25
1.31
0.460 0.477
Temp scaling 52.85 70.02 2.09
0.20 0.338 0.465
1.10
MC Dropout 65.50 74.22 1.54
1.46
0.25
0.476 0.475
VBI
56.57 72.65 2.44
56.57 72.65 1.78
Ensemble
0.435 0.475
0.20
1.12
64.11 74.22 1.22 1.00 0.24 0.19 0.350 0.461
wEnsemble
0.34 0.253 0.389 0.476
Vanilla
59.98 70.04 2.22
0.32 0.235 0.390 0.476
Temp scaling 59.98 70.04 1.66
MC Dropout 64.99 72.61 1.57
0.27 0.222 0.344 0.472
65.30 74.21 2.14
VBI
0.27 0.210 0.315 0.473
63.83 72.45 1.33 0.94 0.24 0.185 0.343 0.470
Ensemble
wEnsemble
62.26 71.64 1.50
0.28 0.219 0.375 0.465
0.30
0.42
0.39
0.26
0.40
0.33
1.59
1.20
1.33
1.85
1.23
entropy value for many examples, but calibration methods relieve
Figure 5: Scatter points of the randomly selected 2,000 test
examples from the VirusShare dataset with paired values
(Entropy, KL) and (Entropy, SD) that are obtained by apply-
ing DeepDrebin incorporating VBI or Ensemble.
this situation notably. Considering that the higher entropy deliv-
ers more uncertainty, vanilla model is poorly calibrated regarding
the out-of-source examples. This is further confirmed that vanilla
models exhibit a limited change along with increasing thresholds
in Figure 4b. (ii) Ensemble and wEnsemble increase the robustness
of deep learning models in detecting out-of-source examples. For
example, the MultimodalNN, DeepDroid, and Droidetec models can
be enhanced by Ensemble and wEnsemble to some extent, espe-
cially when applied to predicting the examples with entropy values
below 0.3 (i.e., the detectors are relatively certain about their pre-
dictions). (iii) DeepDrebin incorporating either VBI or MC dropout
make a significant achievement by comparing with ensemble-based
0.50.00.51.0Entropy values0246810DensityDeepDrebin0.50.00.51.0Entropy valuesMultimodalNN0.50.00.51.0Entropy valuesDeepDroidVanillaTemp scalingMC dropoutVBIEnsemblewEnsemble0.50.00.51.0Entropy valuesDroidetec0.00.20.40.60.81.0Threshold 0.50.60.70.80.91.0Accuracy on  examples w/ entropyDeepDrebin0.00.20.40.60.81.0Threshold MultimodalNN0.00.20.40.60.81.0Threshold DeepDroid0.00.20.40.60.81.0Threshold Droidetec0.00.20.40.60.8KL divergence (KL)VBI0.00.20.40.60.8SDVBI0.00.20.40.60.81.0Entropy0.00.20.40.60.8KL divergence (KL)Ensemble0.00.20.40.60.81.0Entropy0.00.20.40.60.8SDEnsemble603Can We Leverage Predictive Uncertainty to Detect Dataset Shift and Adversarial Examples in Android Malware Detection?ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Figure 6: Illustration of balanced accuracy (bAcc), balanced NLL (bNLL), balanced BSE (bBSE) under temporal covariate shift.
(a)
(b)
Figure 7: Predictive entropy (see Eq.(7)) of malware detectors on Androzoo dataset: (a) accuracy (upper row) and bAccuracy
(bottom row) on the Androzoo test set after excluding the examples for which the detectors have high uncertainties (i.e., the
examples for which the predictive entropy is above a pre-determined threshold τ); (b) test sample density of predictive entropy.
calibrations. Because VBI requires suitable prior distributions, it is
challenging to generalize this calibration to all models effectively
[6]. (iv) Temp scaling demonstrates an un-intuitive phenomenon
that it achieves almost 100% accuracy at the start of the curves when
applied to DeepDrebin, MultimodalNN and Droidetec, but degrades
the accuracy of DeepDroid. The is because Temp scaling tends to
over-approximate the predicted probabilities, resulting in high con-
fidence score for examples, some of which are mis-classified, how-
ever. In addition, the balanced accuracy demonstrates the similar
experimental results (cf. the appendix materials for details).
Figure 5 plots the relationship between the entropy and the KL
divergence (KL), and the relationship between the entropy and the
Standard Deviation (SD). A scatter point represents a test example
based on the paired value. We observe that these three measure-
ments are closely related, which explains why we use the entropy
to characterize the predictive uncertainty solely. We only report
the results for the calibrated DeepDrebin with VBI and Ensemble,
because similar phenomena are observed for the other models. Note
that Vanilla and Temp scaling do not apply to KL divergence and
standard deviation.
Insight 2. Deep ensemble benefits calibration of malware detec-
tors against out-of-source test examples, but a carefully tuned VBI
model could achieve a higher quality of uncertainty than ensemble-
based methods; Measurements of entropy, KL divergence and standard
deviation are closely related.
4.4 Answering RQ3
In order to quantify the predictive uncertainty of malware detectors
under temporal covariate shift, we use Androzoo dataset. Specifi-
cally, we train malware detectors upon the APKs collected in the
year of 2014, and test these malware detectors upon APKs collected
in the year of 2015 and 2016 at the month granularity. We also split
the APKs in the year of 2014 into three disjoint datasets, 83.4% for
0.500.600.700.800.901.00bAccDeepDrebinMultimodalNNDeepDroidDroidetec0.02.04.06.08.010.0bNLLVBIEnsemblewEnsembleTest set2015-022015-042015-062015-082015-102015-122016-022016-042016-062016-082016-102016-120.000.100.200.300.400.50bBSETest set2015-022015-042015-062015-082015-102015-122016-022016-042016-062016-082016-102016-12Test set2015-022015-042015-062015-082015-102015-122016-022016-042016-062016-082016-102016-12VanillaTemp scalingMC dropoutTest set2015-022015-042015-062015-082015-102015-122016-022016-042016-062016-082016-102016-120.920.940.960.981.00Accuracy on  examples w/ entropyDeepDrebinMultimodalNNDeepDroidDroidetec0.00.20.40.60.81.0Threshold 0.500.600.700.800.90bAccuracy on examples w/ entropy0.00.20.40.60.81.0Threshold 0.00.20.40.60.81.0Threshold 0.00.20.40.60.81.0Threshold 048121620DensityDeepDrebinMultimodalNNVanillaTemp scalingMC dropoutVBIEnsemblewEnsemble0.50.00.51.0Entropy values0246810DensityDeepDroid0.50.00.51.0Entropy valuesDroidetec604ACSAC ’21, December 6–10, 2021, Virtual Event, USA
D. Li, T. Qiu, S. Chen, Q. Li, and S. Xu
Table 3: Effectiveness of calibrated malware detectors under adversarial evasion attacks.
Malware
detector
DeepDrebin
Multimo
-dalNN
DeepDroid
Droidetec
“Max” PGDs+GDKDE attack
ECE Acc (%) NLL
BSE
Mimicry attack
BSE
Calibration
method
No attack
BSE
ECE Acc (%) NLL
Acc (%) NLL
96.09
96.09
96.55
96.27
96.00
96.00
97.82
ECE
4.778 0.317 0.334
66.09
0.629 0.037 0.039
Vanilla
1.427 0.266 0.332
66.09
0.184 0.033 0.042
Temp scaling
69.18
1.639 0.245 0.317
0.186 0.029 0.040
MC Dropout
69.91 1.034 0.211 0.320
0.142 0.025 0.051
VBI
3.296 0.295 0.363
64.82
0.403 0.034 0.042
Ensemble
2.944 0.296 0.362
64.64
0.362 0.034 0.042
wEnsemble
1.530 0.107 0.119
87.64
Vanilla
0.368 0.020 0.023
87.64
0.562 0.094 0.122
Temp scaling 97.82 0.129 0.019 0.025
85.64
1.822 0.121 0.152
0.399 0.024 0.030
MC Dropout
97.18
0.166 0.026 0.042
VBI
96.82
89.64 0.506 0.080 0.119
1.148 0.091 0.129
88.09
0.355 0.021 0.026
Ensemble
97.45
1.124 0.123 0.187
84.27
0.295 0.025 0.035
wEnsemble
97.09
86.09
0.786 0.110 0.142
Vanilla
0.587 0.073 0.095
91.55
0.538 0.101 0.158
86.09
0.404 0.069 0.113
Temp scaling
91.55
0.529 0.083 0.126
90.18
0.451 0.066 0.097
MC Dropout
92.55
0.705 0.136 0.200
82.00
0.592 0.102 0.151
VBI
87.27
89.55
0.433 0.079 0.142
Ensemble
0.329 0.058 0.099
92.55
wEnsemble
92.82 0.348 0.061 0.111
95.00 0.237 0.040 0.069
0.656 0.099 0.126
88.80
0.123 0.016 0.028
98.08
Vanilla
0.514 0.095 0.138
88.80
0.113 0.017 0.039
98.08
Temp scaling
93.17
0.268 0.052 0.090
MC Dropout
98.81
0.063 0.009 0.018
0.396 0.060 0.078
0.095 0.012 0.016
98.54
VBI
93.08
0.175 0.030 0.068