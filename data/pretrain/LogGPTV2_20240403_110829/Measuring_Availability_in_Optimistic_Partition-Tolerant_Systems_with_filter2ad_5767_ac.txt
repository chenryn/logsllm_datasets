1
0.95
0.9
0.85
0.8
0.75
0.7
0.11
0.1
0.09
0.08
0.07
0.06
0.05
0.04
0.03
0.02
0.01
y
t
i
l
i
b
a
l
i
a
v
A
t
n
e
r
a
p
p
A
d
e
t
p
e
c
c
A
y
l
l
a
n
o
i
s
i
v
o
r
P
r
e
v
o
s
n
o
i
t
a
c
o
v
e
R
continuous
stop-the-world
pessimistic
0
2
4
6
8
10
12
14
16
18
20
Partition Duration [s]
Figure 7. Apparent Availability
continuous
stop-the-world
0
2
4
6
8
10
12
14
16
18
20
Partition Duration [s]
Figure 8. Revocations over Provisionally Ac-
cepted
in conﬂict if they have been updated concurrently. In our
model, on the other hand, a conﬂict occurs only as the result
of the violation of some integrity constraint. Such violations
can be caused by concurrent updates, but not necessarily.
In Figure 8 we see that as the partition duration increases
the ratio of revoked operations decreases. This is a bit
counter-intuitive, one would expect the opposite. However,
there is an explanation to this phenomenon. The cause lies
in the fact that in our synthetic application two partitions
perform similar kinds of client calls. This means that an op-
eration which has been successfully applied in one partition
is likely to be compatible with changes that have occurred
in the other as well. The longer the partition lasts, the more
operations are performed and the risk of different types of
operations in the two partitions decreases. Naturally, this
behaviour depends on the nature of the integrity constraints
and thus on the application. The conﬁdence intervals are
within 6.9% for all measurement points.
The effect of load So far the experiments have been per-
formed with a constant arrival rate of 120 operations per
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 04:20:58 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007n
o
i
t
a
r
u
D
n
o
i
t
a
i
l
i
c
n
o
c
e
R
9
8
7
6
5
4
3
2
1
0
continuous
stop-the-world
0
50
100
150
200
250
Load [1/s]
Figure 9. Reconciliation Duration
second. To see the effect of load we have plotted the recon-
ciliation duration against load in Figure 9. Here, the 95%
conﬁdence intervalse are within 1.6% for all measurement
points. The handling rate for this experiment was 300 ac-
tions per second. This ﬁgure might seem high compared to
the load. However, the reconciliation process is performed
at a single node which means that no network communica-
tion is needed. As can be seen in the ﬁgure the continuous
service protocol suffers more than the other protocols under
heavy load; especially, as it approaches the maximum load.
However, this does not translate to less apparent availability
as in the case of stop-the-world. The only period of unavail-
ability for the CS protocol is during the time between the
continuous servers receive a stop message from the recon-
ciliation manager and the time to receive the installed state.
The length of this period is not affected by the length of the
reconciliation phase. Thus, the apparent availability of CS
is not decreased (as was shown in Figure 5).
7 Related Work
In this section we will discuss how the problem of rec-
onciliation after network partitions has been dealt with in
the literature. For more references on related topics there
is an excellent survey on optimistic replication by Saito and
Shapiro [19]. There is also an earlier survey discussing con-
sistency in partitioned networks by Davidson et al. [8].
The CS protocol was recently presented as a formalisa-
tion in timed I/O automata [1]. Earlier studies [2] have com-
pared different versions of reconciliation protocols but none
of them with the feature of continuously serving during rec-
onciliation. Gray et al. [11] address the problem of update
everywhere and propose a solution based on a two-tier ar-
chitecture and tentative operations. However, they do not
target full network partitions but individual nodes that join
and leave the system (which is a special case of partition).
Bayou [22] is a distributed storage system that is adapted for
mobile environments. It allows updates to occur in a parti-
tioned system. Bayou can in principle deal with integrity
constraints. However, there is a limitation in the sense that
a primary server must be able to commit operations locally
(this prevents later revocations). This makes the use of sys-
tem wide integrity constraints hard or impossible.
Some work has been done on partitionable systems
where integrity constraints are not considered, which sim-
pliﬁes reconciliation. Babaouglu et al. [3] present a method
for dealing with network partitions. They propose a solu-
tion that provides primitives for dealing with shared state.
They do not elaborate on dealing with writes in all parti-
tions except suggesting tentative writes that can be undone
if conﬂicts occur. Moser et al. [15] have designed a fault-
tolerant CORBA extension that is able to deal with node
crashes as well as network partitions. There is also a rec-
onciliation scheme described in [16]. The idea is to keep a
primary for each object. The states of these primaries are
transferred to the secondaries on reuniﬁcation. In addition,
operations that are performed on the secondaries during de-
graded mode are reapplied during the reconciliation phase.
This approach is not directly applicable with integrity con-
straints.
There are some systems that use more advanced opti-
mistic replication techniques, which allow the degree to
which inconsistencies are allowed to be conﬁgured. None
of these protocols are aimed at operating fully in a parti-
tioned system. They therefore do not provide the reconcil-
iation algorithms for such a scenario. However, it is inter-
esting to compare the way they approach conﬁgurable con-
sistency with our integrity constraint based approach. Yu
and Vahdat [25] use consistency units (conits) to specify
the bounds on allowed inconsistency. A conit is a set of
three values representing “numerical error”, “order error”
and “staleness”. Numerical error deﬁnes a weight of writes
on a conit that can be applied to all replicas, before update
propagation has to occur. Order error deﬁnes the number of
outstanding write operations that are subject to re-ordering
on a single conit. Finally, staleness deﬁnes the time update
propagation can be delayed. The system does not support
partitioning, although the key concept of conits could be
used in a partitioned context. In CoRe [10] the principle
of specifying consistency is extended to allow the program-
mer to deﬁne consistency using a larger set of parameters.
AQua [7] approaches the solution from the other direction:
conﬁguration of the allowed consistency in order to increase
availability; that is, by allowing availability requirements to
be speciﬁed. In AQua “quality objects” are used to specify
quality of service requirements.
Most works on reconciliation algorithms dealing with
constraints after network partition focus on achieving a
schedule that satisﬁes order constraints. Lippe et al. [14]
try to order operation logs to avoid conﬂicts with respect
to a before relation. However, their algorithm requires a
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 04:20:58 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007large set of operation sequences to be enumerated and then
compared. The IceCube system [13, 18] also tries to or-
der operations to achieve a consistent ﬁnal state. However,
they do not fully address the problem of integrity constraints
that involve several objects. Phatak et al. [17] propose an
algorithm that provides reconciliation by either using mul-
tiversioning to achieve snapshot isolation or using a recon-
ciliation function given by the client. Snapshot isolation is
more pessimistic than our approach and would require a lot
of operations to be undone.
8 Conclusions and Future Work
In case of a network partition fault in a distributed sys-
tem, there are two basic approaches: pessimistic and opti-
mistic replication. We have shown that the optimistic solu-
tion does pay off in terms of availability even in systems
with data constraints that have to be reconciled later on.
Moreover, we have identiﬁed the need for additional avail-
ability metrics (e.g., number of accepted operations, pro-
portion of revoked operations) to evaluate these systems.
Using these metrics, we have evaluated an implementation
of a reconciliation protocol [1] that aims at delivering con-
tinuous service during the reconciliation protocol.
The results show that for long partition durations this
protocol gives the best performance in terms of apparent
availability as well as number of applied operations. More-
over for longer partition durations, the risk of having to re-
voke a previously accepted operation can decrease for some
applications.
Naturally, the gain comes with a cost. Apart from the fact
that operations have to be revoked or possibly compensated
during reconciliation, there will be an overhead associated
with this solution. Currently, we are evaluating this pro-
tocol as a CORBA extension to make it partition-tolerant.
This evaluation will include latency measurements to de-
termine the overhead induced by the protocols. A natural
continuation for this work is to extend it to more dynamic
environments where partitions are more frequent and where
the network topology is constantly changing.
The current implementation updates replicas with the
installed state by sending the entire state. This is obvi-
ously not reasonable in a system with a large state. A rela-
tively easy modiﬁcation is to send increments, that represent
changes to the state of various replicas.
9 Acknowledgments
This work has been supported by the FP6 IST project
DeDiSys on Dependable Distributed Systems. The sec-
ond author was partially supported by University of Lux-
embourg during preparation of this manuscript.
References
[1] M. Asplund and S. Nadjm-Tehrani. Formalising reconciliation in partition-
able networks with distributed services.
In M. Butler, C. Jones, A. Ro-
manovsky, and E. Troubitsyna, editors, Rigorous Development of Complex
Fault-Tolerant Systems, volume 4157 of Lecture Notes in Computer Science,
pages 37–58. Springer-Verlag, 2006.
[2] M. Asplund and S. Nadjm-Tehrani. Post-partition reconciliation protocols
for maintaining consistency. In Proceedings of the 21st ACM/SIGAPP sym-
posium on Applied computing, April 2006.
¨O. Babaoglu, A. Bartoli, and G. Dini. Enriched view synchrony: A program-
IEEE
ming paradigm for partitionable asynchronous distributed systems.
Trans. Comput., 46(6):642–658, 1997.
[3]
[4] G. Badishi, G. Caronni, I. Keidar, R. Rom, and G. Scott. Deleting ﬁles in
the celeste peer-to-peer storage system. In SRDS’06: Proceedings of the 25th
IEEE Symposium on Reliable Distributed Systems, October 2006.
[5] P. A. Bernstein, V. Hadzilacos, and N. Goodman. Concurrency control and
recovery in database systems. Addison-Wesley Longman Publishing Co.,
Inc., Boston, MA, USA, 1987.
[6] S. Beyer, M. Ba˜nuls, P. Gald´amez, J. Osrael, and F. D. Mu˜noz-Esco´ı. Increas-
ing availability in a replicated partionable distributed object system. In Pro-
ceedings of the Fourth International Symposium on Parallel and Distributed
Processing and Applications (ISPA’2006). Springer–Verlag, 2006.
[7] M. Cukier, J. Ren, C. Sabnis, D. Henke, J. Pistole, W. H. Sanders, D. E.
Bakken, M. E. Berman, D. A. Karr, and R. E. Schantz. Aqua: An adaptive
architecture that provides dependable distributed objects. In SRDS ’98: Pro-
ceedings of the The 17th IEEE Symposium on Reliable Distributed Systems,
page 245, Washington, DC, USA, 1998. IEEE Computer Society.
[8] S. B. Davidson, H. Garcia-Molina, and D. Skeen. Consistency in a partitioned
network: a survey. ACM Comput. Surv., 17(3):341–370, 1985.
[9] DeDiSys. European IST FP6 DeDiSys Project. http://www.dedisys.org, 2006.
[10] C. Ferdean and M. Makpangou. A generic and ﬂexible model for replica
consistency management. In ICDCIT, pages 204–209, 2004.
[11] J. Gray, P. Helland, P. O’Neil, and D. Shasha. The dangers of replication
and a solution. In SIGMOD ’96: Proceedings of the 1996 ACM SIGMOD
international conference on Management of data, pages 173–182, New York,
NY, USA, 1996. ACM Press.
[12] V. Hadzilacos and S. Toueg. Fault-tolerant broadcasts and related prob-
lems. In Distributed systems, chapter 5, pages 97–145. ACM Press, Addison-
Wesley, 2nd edition, 1993.
[13] A.-M. Kermarrec, A. Rowstron, M. Shapiro, and P. Druschel. The icecube ap-
proach to the reconciliation of divergent replicas. In PODC ’01: Proceedings
of the twentieth annual ACM symposium on Principles of distributed comput-
ing, pages 210–218, New York, NY, USA, 2001. ACM Press.
[14] E. Lippe and N. van Oosterom. Operation-based merging. In SDE 5: Pro-
ceedings of the ﬁfth ACM SIGSOFT symposium on Software development en-
vironments, pages 78–87, New York, NY, USA, 1992. ACM Press.
[15] L. E. Moser, P. M. Melliar-Smith, and P. Narasimhan. Consistent object repli-
cation in the eternal system. Theor. Pract. Object Syst., 4(2):81–92, 1998.
[16] P. Narasimhan, L. E. Moser, and P. M. Melliar-Smith. Replica consistency of
corba objects in partitionable distributed systems. Distributed Systems Engi-
neering, 4(3):139–150, 1997.
[17] S. H. Phatak and B. Nath. Transaction-centric reconciliation in disconnected
client-server databases. Mob. Netw. Appl., 9(5):459–471, 2004.
[18] N. Preguica, M. Shapiro, and C. Matheson. Semantics-based reconciliation
for collaborative and mobile environments. Lecture Notes in Computer Sci-
ence, 2888:38–55, October 2003.
[19] Y. Saito and M. Shapiro. Optimistic replication. ACM Comput. Surv.,
37(1):42–81, 2005.
[20] D. P. Siewiorek and R. S. Swarz. Reliable computer systems (3rd ed.): design
and evaluation. A. K. Peters, Ltd., Natick, MA, USA, 1998.
[21] D. Szentivanyi and S. Nadjm-Tehrani. Middleware support for fault toler-
ance. In Q. Mahmoud, editor, Middleware for Communications. John Wiley
& Sons, 2004.
[22] D. B. Terry, M. M. Theimer, K. Petersen, A. J. Demers, M. J. Spreitzer, and
C. H. Hauser. Managing update conﬂicts in bayou, a weakly connected repli-
cated storage system. In SOSP ’95: Proceedings of the ﬁfteenth ACM sympo-
sium on Operating systems principles, pages 172–182, New York, NY, USA,
1995. ACM Press.
[23] A.-I. Wang, P. L. Reiher, R. Bagrodia, and G. H. Kuenning. Understanding
the behavior of the conﬂict-rate metric in optimistic peer replication. In DEXA
’02: Proceedings of the 13th International Workshop on Database and Expert
Systems Applications, pages 757–764, Washington, DC, USA, 2002. IEEE
Computer Society.
[24] H. ying Tyan. Design, realization and evaluation of a component-based soft-
ware architecture for network simulation. PhD thesis, Department of Electri-
cal Engineering, Ohio State University, 2001.
[25] H. Yu and A. Vahdat. Design and evaluation of a conit-based continuous con-
sistency model for replicated services. ACM Trans. Comput. Syst., 20(3):239–
282, 2002.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 04:20:58 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007