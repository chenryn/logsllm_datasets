of cnn.com such as the following.
In a later chapter, I explain investigative techniques for domain names, and the importance of searching for 
subdomains. A subdomain is a dedicated address that is part of a larger domain, such as pics.inteltechniques.com. 
While intekechniqucs.com might be a main landing page, pics.inteltechniques.com could possess hidden content 
not available within the main website.
Querytf
Oueryli
[DNS]
[Brute Forcing]
[Brute Forcing]
[Robter]
[virusTotal]
[Brute Forcing]
*
The automated script which automates all of these tasks and commands is included in vour digital downloads, 
tided "domains.sh" within the "scripts" folder. It should be present within your Linux VM as long as you 
downloaded the files as previously explained. We will also run through all of the commands again in the nest 
chapter. Next, let's add a shortcut icon to our Dock in order to have easy access to this new utility’.
Programs within this script, such as Amass and Sublist3r, request a domain name 
associated with the target. This is a vital step for any investigation into a domain 
each utility in this new menu, including the results for a rest query of cnn.com.
LibreOffice
sudo snap install libreoffice
Linux /Applications
73
cnn.com/webview
cnn.com/NOKIA
cnn.com/Quickcast
trends.cnn.com 
tours.cnn.com 
coupons.cnn.com
Personally, I do not like typing my reports from within a VM. I prefer to launch my desired word processor 
within my host operating system. This creates a small layer of protection between the OS which you are using 
during your "covert" investigation (VM), and the machine which is documenting the written report (host). This 
is all personal preference, and you may choose another route. Much later in the book, you will learn several 
documentation strategies for your next online investigation.
LibreOffice is a free and open source office suite, comprised of programs for word processing (Writer), the 
creation and editing of spreadsheets (Math), and PowerPoint (Impress). The Writer program can be used within 
your Linux VM for creating reports while die Math program can be useful with opening CSV files created by 
the previous utilities. Your installation of Ubuntu should already have this productivity suite installed. If not, 
you can add the package with the following Terminal command.
TheHarvester: This program searches a supplied domain with the intent of providing email addresses 
associated to the target. It creates a report located in the Documents/theHarvester folder. During my search of 
cnn.com, it located 72 hosts and 16 email addresses. Further investigation into these results may reveal new 
sources of information. In each of these scenarios, you are greeted with text files containing numerous URLs. 
If desired, you could submit each to EyeWitness. This would create a new report with screen captures from any 
valid addresses. This allows for a quick review of the evidence to see if anything warrants further investigation.
We have now made it through the difficult section of this chapter. The remaining programs have standard launch 
options and do not require custom scripts to make them user friendly. These are all similar to their equivalents 
within other traditional operating systems. We will replicate all of this within Mac and Windows in Chapter Six.
Photon: This option docs not attempt to find subdomains. Instead, it searches for internal pages within a target 
website. It creates a report located in the Documents/Photon folder. Examples include the following.
Carbonl4: This application helped an investigation as I was writing this. I was investigating an anonymous blog, 
with the intent of learning more about the age of the site. It was a brand-new WordPress installation, and every' 
post had the same recent 2019 date and time. It had obviously been created recently, but then received an import 
from an older blog. I wanted to know more about the true dates of the original posts. This is the perfect scenario 
for Carbon 14. It searches for any images hosted within the page and analyzes the metadata for creation dates. 
A query' of my own site indicates that the static page was last modified on August 2, 2019, and the images were 
posted in March 2018, May 2019, and July 2019. If this were my target, I would now have suspicion that the 
original blog posts were from an earlier date. These dates can be intentionally or unintentionally altered, so this 
is not a forensically sound method. It is simply an additional piece to the puzzle which may warrant further 
investigation. 1 find that Carbon 14 works best on static websites and blogs. I have also had surprising success 
when targeting social network profiles.
Tor Browser
Chrome Browser
Metadata-Medialnfo
sudo apt install -y mediainfo-gui
Figure 4.13: A Mediainfo result.
74 Chapter 4
sudo add-apt-repository -y ppa:micahflee/ppa
sudo apt -y update
sudo apt install -y torbrowser-launcher
• wget https://dl.google.com/linux/direct/google-chrome- 
stable_current_amd64.deb
• sudo apt install -y ./google-chrome-stable_current_amd64 .deb
• rm google-chrome-stable_current_amd64 .deb
: HPEG-4
: QuickTime
: 545 KiB
: 5 s 488 ms
: 813 kb/s
: 2013-05-30110:51:14+0180
: Apple
: +55.4062-002.6372+123.982/
: iPhone 5
This is a utility for displaying hidden metadata within a media file. The most common example is the metadata 
within a video obtained directly from the original source. First, open Terminal and type the following to install 
the application within the Applications menu of Ubuntu.
This application was mentioned briefly in the previous chapter and will be required for some of the tutorials 
discussed later in the book. Execute the following commands within Terminal, then launch "Tor Browser" from 
the Applications menu to complete the installation.
Chrome was also mentioned in the previous chapter. While I avoid usage for personal tasks due to privacy 
concerns, it can be quite valuable in our Linux VM for OSINT investigations. The following commands within 
Terminal will install the program and remove the unnecessary installation file.
You can now click on the Mediainfo icon in the Applications menu to launch the program. Click on the File 
option in the menu and then open either a file or folder. The default view offers little data, so click on View 
and then "Text" from the menu. This presents all metadata available within the file. In my test, I received the 
(partial) output visible in Figure 4.13 from a video sent to me via email direcdy from a mobile device. It identified 
the make, model, and operating system version of the device, along with GPS information about the location 
during capture. Note that this type of data is usually not available from videos downloaded from the internet 
The ideal scenario is that you possess a video file sent directly from a device.
Format 
Format profile 
File size 
Duration 
Overall bit rate 
Recorded date 
Hake 
xyz 
Model 
com.apple.quicktine.software
While I do not use the Tor and Chrome browsers daily, they are important to have available and take up vet)' 
little space. There are some browser extensions which only support Chrome and many websites which can only 
be accessed through Tor. I will explain more about that later.
Metadata-Exiftool
• sudo apt install -y libimage-exiftool-perl
cd -/Deskrop/Evidence && exiftool * -csv > -/Desktop/Report.csv
Metadata-mat2
sudo apt install -y mat2
• mat2 dirty.jpg
Metadata-xeuledoc
sudo -H pip install xeuledoc -I
Linux /Vpplications
75
This command launches ExifTool (exiftool); reads all files in the current folder (*); specifies creation of a CSV 
file (-csv); and saves the file output to the Desktop tided as Report.csv (> ~/Desktop/Report.csv). I do not use 
this feature often, but it has saved me a lot of work when needed. 1 once possessed a folder of 200 images 
retrieved from a directory on my target's blog. This command created a spreadsheet with all metadata from the 
images, which identified GPS locations valuable to the investigation.
You can now export the metadata from multiple images into a spreadsheet Consider the following example 
where I possess a folder of images on my Desktop tided "Evidence". I want a spreadsheet of all available Exif 
data in reference to these images. The following command in Terminal will create a report on my Desktop tided 
Report.csv.
This application extracts metadata hidden within Google Documents, including the documents owner's name, 
email address, and Google identifiers. This is a great way to determine who is behind an online Google document 
which is part of your investigation. The following command installs the program.
If we launched the automated script included within your downloads, we would be prompted with a selection 
dialogue box similar to the option presented with the video tools. Selecting a file would launch the mat2 process 
and generate the clean image within the same folder as the original. This tool will never overwrite your original, 
file.
Image metadata, also called Exif data, will be explained later in more detail. It can include the make, model, and 
serial number of a camera or location data of the capture. Third-party applications to view this data are no longer 
necessary because Ubuntu has this feature embedded in the default image viewer. Simply open any image, click 
the three horizonal lines in the upper right menu, and select Properties. The Metadata and Details tabs display 
all available details such as camera information, location, and dates. However, this does not help much in regard 
to bulk images. We can easily install a Terminal-based utility which allows for automated export called ExifTooL 
The following command in Terminal will complete the process.
While you may want to VIEW metadata within images with the previous command, it is just as likely you will 
want to REMOVE data from your own media. Maybe you want to upload an image as part of a covert 
investigation. That image could have metadata such as dates, times, locations, and camera details. This is where 
MAT2 (Metadata Anonymisation Toolkit 2) can assist. First, we must install it with the following.
Next, we can issue a command to clean a file. Assume I have an image on my Desktop titled dirty.jpg. The 
following commands within Terminal would change to the Desktop directory and create a new file called 
"dirty.cleaned.jpg".
The result appears below.
Document ID : !KXksBlvj7fXPNS4OYL0idQne3HXVnamtUPlh0ut3xwk
Metadata-Shcrloq
ou can now launch this application manually with the following command within Terminal.
• python3 -/Downloads/Programs/sherloq/gui/sherloq.py
76 Chapter 4
[+] Creation date : 2020/01/08 16:35:56 (UTC) 
(+] Last edit date : 2020/01/08 16:35:56 (UTC) 
Public permissions :- reader 
[+] Owner found !
• cd -/Downloads/Programs
• 
sudo apt install subversion -y
• 
git clone https://github.com/GuidoBartoli/sherloq.git
• 
cd sherloq/gui
• 
sudo -H pip install -r requirements.txt -I
• 
sudo -H pip install matplotlib
Name : donlad grifith
Email : PI:EMAIL
Google ID : 09765685956674862946
xeuledoc https://docs.google.eom/spreadsheets/d/lKXksBlvj7fXPNS4OYL0idQne3HXVnamtUPlh0ut3xwk
We can now execute the application manually toward a Google document. Consider the spreadsheet located at 
https://docs.google.eom/spreadsheets/d/lkxksBlvj7fXPNS4OYL0idQne3HXVnamtUPlh0ut3xwk. This 
page provides a link to a third-part}’ piracy website claiming to offer one of my books for download. There is 
nothing within this spreadsheet which identifies the creator of this document. However, xeuledoc can assist 
The following command queries the tool with the document link.
Sherloq is a standalone application which can help identify modified areas of photographs. The a 'an 
capabilities to alter images within Photoshop can easily fool our eyes. Computers are more difficult to con 
Let's install the application first, then discuss the usage. Conduct the following within Terminal.
We now have a name and email address to research. Tire automated script for this tool is part of "metadata-sh'1 
and is included in your downloads.
Consider the image visible in Figure 4.14. The upper-right image displays the "Principal Component An ysis 
which highlights the lips of the subjects for potential digital manipulation. If you investigate online images o ten, 
I encourage you to understand all of the options available within this software application. The author s we site 
atgithub.com/GuidoBartoli/sherloq and the tutorials provided in Chapter 21 should assist.
Sherloq replicates many of the image metadata and manipulation detection methods which are explained later 
in the book. The benefit here is the ability to conduct an examination offline. If you have a sensitive photo, you 
may not want to upload it into one of the online metadata websites which is explained later. Instead, you may 
want to keep it restricted to your local machine. Click the "Load Image" button and select a photo. There are 
numerous options present within the left menu, most of which will be explained within the Images chapter (21).
Figure 4.14: A Sherloq image analysis.
HTTrack (httrack.com)
sudo apt install -y webhttrack
Some tools
Linux Applications
77
sudo apt update
sudo apt install -y libcanberra-gtk-module
remaining in this chapter require a small package for sound, which is installed with the following.
You can now type webhttrack at any Terminal prompt and the software will execute. You should also have 
the "httrack.desktop" file available in your Applications menu. When the application loads, clicking the "next" 
button will bring you to the project screen. Give your project a tide, preferably the website name, and choose a 
location to save all of the data. Click next and then "add URL". Enter the exact website that you want to archive. 
Do not enter any login or password. Click "next", then "finished", and you are done. The application will begin 
extracting all public information from the site.
HTTrack is a very old program, and was never intended to function within today's latest technology. However, 
it still works surprisingly well on small business websites. It can also extract a surprising amount of content from 
large sites such as Wikipedia. 1 once used this service to extract several gigabytes of scraped I Jnkedln data from 
a questionable website before it disappeared, which included profiles, names, email addresses, user IDs and 
employment history'. This tool should always be ready in your arsenal.
Tine "metadata.sh" script within your downloads prompts you for usage of all of these applications. The dialogue 
(with included shortcut) replicates each task presented here. Simply launch the "Metadata" icon within your 
Dock after you have created your final OSINT VM in the next chapter.
This can take a while, depending on the size of the site. When complete, you will have an exact copy in which 
you can navigate and search offline. Archiving to an external drive creates a replica that can be held for future 
analysis. This program only works on smaller static websites, and cannot archive large dynamic sites such as 
social networks.
There are several ways to make an exact copy of a static website and this software will walk you through the 
process. You may want to do this when you locate a target website and are concerned the site could be taken 
down. Any time that you find any content that will be used in court, you should archive the entire site. This 
application automates the process. The result is a locally stored copy in which you can navigate as if it were live. 
This is beneficial in court when internet access is not appropriate, or a website has been taken offline. The 
following command within a new Terminal session will download the software and configure it within your 
operating system.
Metagoofil
requirements.txt -I
type at a time. If my target were cisco.com, I would execute the following commands in Terminal.
78 Chapter 4
• 
cd -/Downloads/Programs/metagoofil
• 
python3 metagoofil.py -d cisco.com -t pdf -o ^/Desktop/cisco/
• 
python3 metagoofil.py -d cisco.com -t docx,xlsx -o -/Desktop/cisco/
• cd -/Downloads/Programs
• git clone https://github.com/opsdisk/metagoofil.git
• cd metagoofil
• sudo -H pip install
Metagoofil is a Linux option designed to locate and download documents. It does not always work perfectly, as 
you are at the mercy of search engines to provide the data. When Google decides to block Metagoofil, its 
usefulness ceases until the software receives an update. Install the software with the following steps.
python3 metagoofil.py -d cisco.com -t pdf -o -/Desktop/cisco/ 
python3 metagoofil.py -d cisco.com -t doc -o ~/Desktop/cisco/ 
python3 metagoofil.py -d cisco.com -t xls -o ~/Desktop/cisco/ 
python3 metagoofil.py -d cisco.com -t ppt -o -/Desktop/cisco/ 
python3 metagoofil.py -d cisco.com -t docx -o -/Desktop/cisco/ 
python3 metagoofil.py -d cisco.com -t xlsx -o ~/Desktop/cisco/ 
python3 metagoofil.py -d cisco.com -t pptx -o ~/Desktop/cisco/
The following Terminal commands in your Linux VM switches to the proper director}' (cd); launches Python 3 
(python3); loads the script (metagoofil,py); sets the target domain (-d cisco.com); sets the file type as pdf (-t pdf); 
and saves the output to a new' folder called "cisco” on the Desktop (-o ~/Desktop/cisco/). The last command 
asks for docx and xlsx files.
• 
If I already possess numerous documents on my computer, I create a metadata CSV spreadsheet using 
ExifTool. I then analyze this document.
• 
If my target website possesses few documents, I download them 
within a web browser.
• 
If my target website possesses hundreds of documents, I use Metagoofil, but only download one file
This automatically downloads any found documents. We could now' analyze these documents with ExifTool as 
mentioned previously with the following commands within Terminal. The following commands navigate you to 
the director}’ with the files and then creates a report.
• 
cd -/Desktop/cisco
• 
exiftool * -csv > -/Desktop/cisco/Report.csv
As you may suspect, I prefer to download and analyze document metadata within Linux. While the command 
to launch ExifTool is easy, the commands for Metagoofil can become complicated, especially when we start 
querying for multiple types of documents. Therefore, we will use the automated script included in the Linux 
downloads which are already installed. It executes Metagoofil and tries to download any documents from an 
entered domain. The second option will repeat that process and then create a report with all the metadata. This 
script is titled "metagoofil.sh". In my experience, this utility' works fairly well when NOT using a VPN, but 
Google may block your connection if the VPN IP address appears abusive. While writing this chapter, I could 
execute a full query one time. All additional queries failed due to Google’s blockage. Personally, my protocol in 
regard to document metadata collection and analysis is as follows.
manually through Google or Bing
Reddit Tools
requirements.txt -I
Bulk Downloader For Reddit
Reddit Finder
From any Terminal prompt, I can now issue a command targeting a Reddit user, such as the following.
redditsfinder inteltechniques
The following is an embarrassing attempt to promote my podcast which I deleted a few hours after posting.
Linux Applications
79
• cd -/Documents
• redditsfinder inteltechniques -pics -d
• sudo apt install python3.9