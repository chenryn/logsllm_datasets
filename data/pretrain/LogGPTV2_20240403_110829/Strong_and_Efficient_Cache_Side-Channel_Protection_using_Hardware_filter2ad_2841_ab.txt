SGX In this scenario, the processor is trusted but the ad-
versary has full control over the OS, the hypervisor,
and all other code running on the system, except
the victim’s code. This scenario models an SGX-
enabled environment, where the victim’s code runs
inside an enclave. While the attacker has more con-
trol over the software running on the machine, the
SGX protections prevent sharing of memory pages
between the enclave and untrusted processes, which
renders Flush+Reload attacks ineffective in this set-
ting.
All other side-channels, including power analysis, and
channels based on shared microarchitectural elements
other than caches are outside our scope.
4 Hardware Transactional Memory as a
Side-Channel Countermeasure
The foundation of all cache side-channel attacks are the
timing differences between cache hits and misses, which
an attacker tries to measure. The central idea behind
Cloak is to instrument HTM to prevent any cache misses
on the victim’s sensitive code and data.
In Cloak, all
sensitive computation is performed in HTM. Crucially,
in Cloak, all security-critical code and data is determin-
istically preloaded into the caches at the beginning of
a transaction. This way, security-critical memory loca-
tions become part of the read or write set and all sub-
sequent, possibly secret-dependent, accesses are guar-
anteed to be served from the CPU caches. Otherwise,
in case any preloaded code or data is evicted from the
cache, the transaction necessarily aborts and is reverted.
(See Listing 1 for an example that uses the TSX instruc-
tions xbegin and xend to start and end a transaction.)
Given an ideal HTM implementation, Cloak thus pre-
vents that an attacker can obtain a trace that shows
whether the victim has accessed a particular memory lo-
cation. More precisely, in the sense of Cloak, ideal HTM
has the following properties:
R1 Both data and code can be added to a transaction as
transactional memory and thus are included in the
HTM atomicity guarantees.
Listing 1: A vulnerable crypto operation protected by
Cloak instantiated with Intel TSX; the AES encrypt
function makes accesses into lookup_tables that de-
pend on key. Preloading the tables and running the
encryption code within a HTM transaction ensures that
eviction of table entries from LLC will terminate the
code before it may cause a cache miss.
i f
x b e g i n ( ) ) == XBEGIN STARTED ) {
l o o k u p t a b l e s )
s i z e t ∗) p ;
AES encrypt ( p l a i n t e x t ,
xend ( ) ;
c i p h e r t e x t , &key ) ;
( ( s t a t u s =
f o r ( auto p :
∗( v o l a t i l e
}
R2 A transaction aborts immediately when any part of
transactional memory leaves the cache hierarchy.
R3 All pending transactional memory accesses are
purged during a transactional abort.
R4 Prefetching decisions outside of transactions are not
inﬂuenced by transactional memory accesses.
R1 ensures that all sensitive code and data can be
added to the transactional memory in a deterministic and
leakage-free manner. R2 ensures that any cache line
evictions are detected implicitly by the HTM and the
transaction aborts before any non-cached access is per-
formed. R3 and R4 ensure that there is no leakage after
a transaction has succeeded or aborted.
Unfortunately, commercially available HTM imple-
mentations and speciﬁcally Intel TSX do not precisely
provide R1–R4. In the following Section 5 we discuss
how Cloak can be instantiated on commercially available
(and not ideal) HTM, what leakage remains in practice,
and how this can be minimized.
5 Cloak based on Intel TSX
Cloak can be built using an HTM that satisﬁes R1–R4
established in the previous section. We propose Intel
TSX as an instantiation of HTM for Cloak to mitigate
the cache side channel.
In this section, we evaluate
how far Intel TSX meets R1–R4 and devise strategies
to address those cases where it falls short. All experi-
ments we report on in this section were executed on In-
tel Core i7 CPUs of the Skylake generation (i7-6600U,
i7-6700, i7-6700K) with 4MB or 8MB of LLC. The
source code of these experiments will be made available
at http://aka.ms/msr-cloak.
220    26th USENIX Security Symposium
USENIX Association
5.1 Meeting Requirements with Intel TSX
We summarize our ﬁndings and then describe the
methodology.
R1 and R2 hold for data. It supports read-only data that
does not exceed the size of the LLC (several MB)
and write data that does not exceed the size of the
L1 cache (several KB);
R1 and R2 hold for code that does not exceed the size
of the LLC;
R3 and R4 hold in the cloud and SGX attacker scenarios
from Section 3, but not in general for local attacker
scenarios.
5.1.1 Requirements 1&2 for Data
Our experiments and previous work [19] ﬁnd the read set
size to be ultimately constrained by the size of the LLC:
Figure 2 shows the failure rate of a simple TSX transac-
tion depending on the size of the read set. The abort rate
reaches 100% as the read set size approaches the limits
of the LLC (4MB in this case). In a similar experiment,
we observed 100% aborts when the size of data written in
a transaction exceeded the capacity of the L1 data cache
(32 KB per core). This result is also conﬁrmed in In-
tel’s Optimization Reference Manual [30] and in previ-
ous work [19, 44, 64].
Conﬂicts and Aborts. We always observed aborts
when read or write set cache lines were actively evicted
from the caches by concurrent threads. That is, evictions
of write set cache lines from the L1 cache and read set
cache lines from the LLC are sufﬁcient to cause aborts.
We also conﬁrmed that transactions abort shortly after
cache line evictions: using concurrent clflush instruc-
tions on the read set, we measured abort latencies in the
order of a few hundred cycles (typically with an upper
bound of around 500 cycles). In case varying abort times
should prove to be an issue, the attacker’s ability to mea-
sure them, e.g., via Prime+Probe on the abort handler,
could be thwarted by randomly choosing one out of many
possible abort handlers and rewriting the xbegin instruc-
tion accordingly,1 before starting a transaction.
Tracking of the Read Set. We note that the data struc-
ture that is used to track the read set in the LLC is un-
known. The Intel manual states that “an implementation-
speciﬁc second level structure” may be available, which
probabilistically keeps track of the addresses of read-set
1The 16-bit relative offset to a transaction’s abort handler is part of
the xbegin instruction. Hence, for each xbegin instruction, there is a
region of 1 024 cache lines that can contain the abort handler code.
100%
50%
e
t
a
r
e
r
u
l
i
a
F
0%
0
1,000
3,000
2,000
Array size in KB
4,000
5,000
Figure 2: A TSX transaction over a loop reading an array
of increasing size. The failure rate reveals how much
data can be read in a transaction. Measured on an i7-
6600U with 4 MB LLC.
cache lines that were evicted from the L1 cache. This
structure is possibly an on-chip bloom ﬁlter, which tracks
the read-set membership of cache lines in a probabilistic
manner that may give false positives but no false nega-
tives.2 There may exist so far unknown leaks through
this data structure. If this is a concern, all sensitive data
(including read-only data) can be kept in the write set in
L1. However, this limits the working set to the L1 and
also requires all data to be stored in writable memory.
L1 Cache vs. LLC. By adding all data to the write
set, we can make R1 and R2 hold for data with respect
to the L1 cache. This is important in cases where vic-
tim and attacker potentially share the L1 cache through
hyper-threading.3 Shared L1 caches are not a concern in
the cloud setting, where it is usually ensured by the hy-
pervisor that corresponding hyper-threads are not sched-
uled across different tenants. The same can be ensured
by the OS in the local setting. However, in the SGX set-
ting a malicious OS may misuse hyper-threading for an
L1-based attack. To be not constrained to the small L1
in SGX nonetheless, we propose solutions to detect and
prevent such attacks later on in Section 7.2.
We conclude that Intel TSX sufﬁciently fulﬁlls R1
and R2 for data if the read and write sets are used ap-
propriately.
5.1.2 Requirements 1&2 for Code
We observed that the amount of code that can be exe-
cuted in a transaction seems not to be constrained by the
sizes of the caches. Within a transaction with strictly no
reads and writes we were reliably able to execute more
2In Intel’s Software Development Emulator [29] the read set is
tracked probabilistically using bloom ﬁlters.
3Context switches may also allow the attacker to examine the vic-
tim’s L1 cache state “postmortem”. While such attacks may be pos-
sible, they are outside our scope. TSX transactions abort on context
switches.
USENIX Association
26th USENIX Security Symposium    221
100%
50%
e
t
a
r
e
r
u
l
i
a
F
0%
0
16
32
48
64
Array size in KB
Figure 3: A TSX transaction over a nop-sled with in-
creasing length. A second thread waits and then ﬂushes
the ﬁrst cache line once before the transaction ends. The
failure rate starts at 100% for small transaction sizes. If
the transaction self-evicts the L1 instruction cache line,
e.g., when executing more than 32 KB of instructions,
the transaction succeeds despite of the ﬂush. Measured
on an i7-6600U with 32 KB L1 cache.
than 20 MB of nop instructions or more than 13 MB of
arithmetic instructions (average success rate ˜10%) on a
CPU with 8 MB LLC. This result strongly suggests that
executed code does not become part of the read set and
is in general not explicitly tracked by the CPU.
To still achieve R1 and R2 for code, we attempted to
make code part of the read or write set by accessing it
through load/store operations. This led to mixed results:
even with considerable effort, it does not seem possible
to reliably execute cache lines in the write set without
aborting the transaction.4 In contrast, it is generally pos-
sible to make code part of the read set through explicit
loads. This gives the same beneﬁts and limitations as
using the read set for data.
Code in the L1 Cache. Still, as discussed in the pre-
vious Section 5.1.1, it can be desirable to achieve R1
and R2 for the L1 cache depending on the attack sce-
nario. Fortunately, we discovered undocumented mi-
croarchitectural effects that reliably cause transactional
aborts in case a recently executed cache line is evicted
from the cache hierarchy. Figure 3 shows how the trans-
actional abort rate relates to the amount of code that is
executed inside a transaction. This experiment suggests
that a concurrent (hyper-) thread can cause a transac-
tional abort by evicting a transactional code cache line
currently in the L1 instruction cache. We veriﬁed that
this effect exists for direct evictions through the clflush
instruction as well as indirect evictions through cache set
conﬂicts. However, self-evictions of L1 code cache lines
(that is, when a transactional code cache line is replaced
4In line with our observation, Intel’s documentation [31] states that
“executing self-modifying code transactionally may also cause trans-
actional aborts”.
by another one) do not cause transactional aborts. Hence,
forms of R1 and R2 can also be ensured for code in the
L1 instruction cache without it being part of the write set.
In summary, we can fulﬁll requirements R1 and R2 by
moving code into the read set or, using undocumented
microarchitectural effects, by limiting the amount of
code to the L1 instruction cache and preloading it via
execution.
5.1.3 Requirements 3&4
As modern processors are highly parallelized, it is difﬁ-
cult to guarantee that memory fetches outside a transac-
tion are not inﬂuenced by memory fetches inside a trans-
action. For precisely timed evictions, the CPU may still
enqueue a fetch in the memory controller, i.e., a race con-
dition. Furthermore, the hardware prefetcher is triggered
if multiple cache misses occur on the same physical page
within a relatively short time. This is known to intro-
duce noise in cache attacks [24,62], but also to introduce
side-channel leakage [6].
In an experiment with shared memory and a cycle-
accurate alignment between attacker and victim, we in-
vestigated the potential leakage of Cloak instantiated
with Intel TSX. To make the observable leakage as strong
as possible, we opted to use Flush+Reload for the at-
tack primitive. We investigated how a delay between
the transaction start and the ﬂush operation and a de-
lay between the ﬂush and the reload operations inﬂu-
ence the probability that an attacker can observe a cache
hit against code or data placed into transactional mem-
ory. The victim in this experiment starts the transaction,
by placing data and code into transactional memory in a
uniform manner (using either reads, writes or execution).
The victim then simulates meaningful program ﬂow, fol-
lowed by an access to one of the sensitive cache lines
and terminating the transaction. The attacker “guesses”
which cache line the victim accessed and probes it. Ide-
ally, the attacker should not be able to distinguish be-
tween correct and wrong guesses.
Figure 4 shows two regions where an attacker could
observe cache hits on a correct guess. The left region
corresponds to the preloading of sensitive code/data at
the beginning of the transaction. As expected, cache hits
in this region were observed to be identical to runs where
the attacker had the wrong guess. On the other hand,
the right region is unique to instances where the attacker
made a correct guess. This region thus corresponds to a
window of around 250 cycles, where an attacker could
potentially obtain side-channel information. We explain
the existence of this window by the fact that Intel did not
design TSX to be a side-channel free primitive, thus R3
and R4 are not guaranteed to hold and a limited amount
of leakage remains. We observed identical high-level re-
222    26th USENIX Security Symposium
USENIX Association
Cache-Hit Ratio
5.2.1 Data Preloading
e
e
g
g
a
a
t
t
n
n
e
e
c
c
r
r
e