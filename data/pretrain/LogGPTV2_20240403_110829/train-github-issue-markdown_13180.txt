### Bug Report

**Kubernetes Version**: v1.3.2

**Environment**:
- Kubernetes cluster set up on 4 VMs running Ubuntu 14.04 (1 master and 3 nodes).
- Cluster brought up using `kube-up.sh` with `PROVIDER=ubuntu`.
- **Kernel**: 3.13.0-92-generic

**Issue Description**:
When running `kubectl get nodes`, the following output is returned:
```
NAME      STATUS     AGE
*.205     Ready      1h
*.205     NotReady   14d
*.206     Ready      14d
*.207     Ready      14d
```
There are two nodes with the same name `*.205`, and one of them is in the `NotReady` state.

**Expected Behavior**:
This issue likely occurred after restarting the `kube-controller` and `api-server`. Restarting `kubelet` and examining the `kubelet.log` on `*.205` shows the following logs:
```
I0819 09:54:08.146564 17698 factory.go:54] Registering systemd factory
I0819 09:54:08.148318 17698 factory.go:86] Registering Raw factory
I0819 09:54:08.150438 17698 manager.go:1072] Started watching for new ooms in manager
I0819 09:54:08.150552 17698 oomparser.go:200] OOM parser using kernel log file: "/var/log/kern.log"
I0819 09:54:08.151820 17698 manager.go:281] Starting recovery of all containers
I0819 09:54:08.162525 17698 manager.go:286] Recovery completed
I0819 09:54:08.256878 17698 kubelet.go:1187] Node *.205 was previously registered
```
Adding `--register-node=false` to `/etc/default/kubelet` and checking the logs again, the following is observed:
```
I0819 09:56:52.330062 18421 factory.go:54] Registering systemd factory
I0819 09:56:52.331063 18421 factory.go:86] Registering Raw factory
I0819 09:56:52.332105 18421 manager.go:1072] Started watching for new ooms in manager
I0819 09:56:52.332225 18421 oomparser.go:200] OOM parser using kernel log file: "/var/log/kern.log"
I0819 09:56:52.333547 18421 manager.go:281] Starting recovery of all containers
I0819 09:56:52.348541 18421 manager.go:286] Recovery completed
```
The node `*.205` is now working, but it is not stable. When setting up `weave-scope`, which launches a pod on each node, there are two pods on the duplicate `*.205` nodes, and the pod on the `NotReady` node is in a pending state.

**Question**:
Is there a way to remove the duplicate, non-functional node?

Thank you for your assistance.