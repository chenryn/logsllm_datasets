BUG REPORT
**Kubernetes version** : v1.3.2
**Environment** :
  * setup kubernetes cluster on 4 VMs of ubuntu 14.04, 1 master and 3 nodes
  * bring up by kube-up.sh with PROVIDER=ubuntu
  * **Kernel** : 3.13.0-92-generic
**What happened** :  
when do kubectl get nodes returned:
NAME STATUS AGE  
*.205 Ready 1h  
*.205 NotReady 14d  
*.206 Ready 14d  
*.207 Ready 14d
so there are two nodes with the same name *.205 and one is Notready
**What you expected to happen** :  
this probably happened after I restart kube-controller and api-server
restart kubelet and tail kubelet.log on *.205 shows:  
I0819 09:54:08.146564 17698 factory.go:54] Registering systemd factory  
I0819 09:54:08.148318 17698 factory.go:86] Registering Raw factory  
I0819 09:54:08.150438 17698 manager.go:1072] Started watching for new ooms in
manager  
I0819 09:54:08.150552 17698 oomparser.go:200] OOM parser using kernel log
file: "/var/log/kern.log"  
I0819 09:54:08.151820 17698 manager.go:281] Starting recovery of all
containers  
I0819 09:54:08.162525 17698 manager.go:286] Recovery completed  
I0819 09:54:08.256878 17698 kubelet.go:1187] * _Node _.205 was previously
registered__
if add --register-node=false to /etc/default/kubelet and tail log shows:  
I0819 09:56:52.330062 18421 factory.go:54] Registering systemd factory  
I0819 09:56:52.331063 18421 factory.go:86] Registering Raw factory  
I0819 09:56:52.332105 18421 manager.go:1072] Started watching for new ooms in
manager  
I0819 09:56:52.332225 18421 oomparser.go:200] OOM parser using kernel log
file: "/var/log/kern.log"  
I0819 09:56:52.333547 18421 manager.go:281] Starting recovery of all
containers  
I0819 09:56:52.348541 18421 manager.go:286] **Recovery completed**
the node *205 is working now but not stable and when I do setup of weave-scope
which launch pod on each node would have two pods on two duplicate *205 node
and the one who is **Not Ready** has the pod pending
**Is there any way to remove that dupliacte useless node?**