triggered while processing the ﬁle. The corresponding
ﬁle loading menu can appear anywhere in the input and
therefore the offsets in the ﬁle are relative to where it was
loaded in the input, making it difﬁcult to automatically
reason over offsets.
In light of the aforementioned issues, we believe that VUzzer
is not suitable for interactive programs, mainly because ot its
poor interfacing mechanism with such programs.
B. LAVA Dataset
In a recent paper, Dolan-Gavitt et.al. developed a technique
to inject hard-to-reach faults and created buggy versions of
a few Linux utilities [17] for testing fuzzing- and symbolic
execution-based bug ﬁnding solutions. We used the LAVA-M
dataset [17] to evaluate VUzzer. This dataset consists of 4
Linux utilities—base64, who, uniq, and md5sum—each
injected with multiple faults (in the same binary for each
utility). The LAVA paper reports results on the evaluation of
a coverage-based fuzzer (FUZZER), symbolic execution, and
a SAT-based approach (SES) on these buggy applications.
10
0246810120204060801006983987698729986755299798131172242281142548121190.61.818.90.722.41.814.20.70.80.520.13.81.9Relativenumberofinputsexecuted(%)VUzzerAFLPINcomparison purposes. For each of these programs, we use the
vanilla release in Ubuntu 14.04. We remark that by, evaluating
these utilities, we also targeted some well-known libraries,
such as libpcap, libjpeg, libpoppler, and libpng.
Each program is fuzzed for maximum 24 hours. In order
to highlight the performance of VUzzer, we also ran AFL
on these applications. Table IV shows the results of running
VUzzer and AFL on the VA dataset, with VUzzer signiﬁcantly
outperforming AFL for both number of unique crashes found
and number of inputs required to trigger such crashes.
TABLE IV.
VA DATASET: PERFORMANCE OF VUZZER VS. AFL.
Application
mpg321
gif2png+libpng
pdf2svg+libpoppler
tcpdump+libpcap
tcptrace+libpcap
djpeg+libjpeg
VUzzer
#Unique crashes
337
127
13
3
403
17
#Inputs
23.6K
43.2K
5K
77.8K
30K
90K
AFL
#Unique crashes
19
7
0
0
238
0
#Inputs
883K
1.84M
923K
2.89M
3.29M
35.9M
Figure 5 details the distribution of crashes over a period
of 24 hours. The x-axis of each plot shows the cumulative
sum of crashes, sampled at each 2 hours. As shown in the
ﬁgure, for almost every application, VUzzer keeps ﬁnding
crashes during the later iterations of fuzzing, whereas AFL
quickly exhausts its efforts after a few initial iterations. This
is due to the fact that, at later stages, AFL is not able to ﬁnd
new (deeper) paths, whereas VUzzer is able to learn branch
constraints as it explores new paths and thus it is able to ﬁnd
crashes in later stages of fuzzing. Another interesting point to
note in Figure 5 is that, in comparison to AFL, VUzzer in
not only able to ﬁnd crashes with much fewer inputs, but this
also happens in much less time (see the position of the vertical
line in Figure 5). We want to again remark that we have not
optimized VUzzer for fast input execution. We believe that
there exist several techniques to enhance the execution speed
of VUzzer, for example, using an AFL-like fork-server within
a single fuzzing iteration or distributing concurrent fuzzing
workers across multiple cores or machines.
D. Crash-Triage Analysis
Fuzzers tend to generate a large number of crashes. Fixing
every bug associated with a crash is a time-consuming but
lucrative process. The only information provided to a software
developer is the version number of the application and the
crash itself. Naturally, the bug patching efforts are invested in
the bugs that are more (security) critical.
!Exploitable [19], a tool proposed by CERT, is built on
top of GDB and uses heuristics to asses the exploitability of a
crash caused by a bug. The heuristics are based on the crash
location, the memory operation (read or write), and the signals
triggered by the application. While this analysis is not sound,
it is simple, fast, and provides hints on the severity of a crash.
We use the !Exploitable tool to rank the crashes found by
VUzzer on this dataset. Table V presents our results.
As shown in the table, most of the cases were marked
as Unknown due to the simplicity of the !Exploitable tool.
None of the cases were marked as Probably Exploitable.
Finally, every crash discovered by VUzzer in tcptrace
7No crash, but inﬁnite loop resulting in an out-of-memory error.
TABLE V.
PERCENTAGES OF EXPLOITABLE BUGS DISCOVERED BY
VUZZER AS REPORTED BY !EXPLOITABLE TOOL.
gif2png
mpg321
pdf2svg
tcpdump
tcptrace
Unknown
100.0
100.0
87.5
100.0
0.0
Exploitable
0.0
0.0
0.0
0.0
100.0
Probably Not Exploitable
0.0
0.0
12.5
0.0
0.0
seems to be Exploitable. We investigated one of the crashes in
tcptrace and there is a seemingly obvious way to exploit it:
the vulnerability is an out-of-bounds write to a heap buffer. The
bound and the data that are written are tainted (i.e., attacker-
controlled).
To further analyze the quality of the bugs discovered by
VUzzer, we measured the distance between the crash and the
library involved (if any). A bug located in a library will likely
be included in any application that uses that library, hence
these bugs are of high priority. We need to also keep in mind,
that these are unknown bugs and therefore many of them
could be zero-day. As we found a large number of unique
crashes, reporting the most important ones early is a priority
and therefore we rely on an automated analysis to approximate
the severity of a bug. In short, if a crash happens in a library,
then it is a serious bug to report. However, sometime a bug
manifests itself in the user application, but the real cause of the
bug lies in a library used by the application. We, therefore, also
measure the distance from the last library call, when a crash
in observed in the application code.
The distance between the crash and a library is measured by
two metrics. First we count the number of instructions executed
between the crash and the last library call. The intuition is that
the computation (and its side effects) which ultimately caused
the crash might originate in a library call. Second, we count the
number of stack frames between the crash and the last library
call. As an example, libraries using output function hooks that
reside in the main application (e.g. tcpdump, tcptrace,
mpg321) are covered by such heuristics. Table VI presents
the results of our analysis.
TABLE VI.
DISTANCE BETWEEN CRASHES AND LIBRARY CALLS.
gif2png
mpg321
pdf2svg
tcpdump
tcptrace
#Instructions
20554.00
733.04
626.11
293.50
1134.53
#Stack frames
gif2png (0); libc (5)
libid3tag (0);
(5.5)
libc (1);
pdf2svg (9);
tcpdump (0); libpcap (5.7)
tcptrace (0); libpcap (2); libc (7)
libpoppler (3);
libmad (3.1);
libc (3.9); mpg321
libpoppler-glib (8);
All crashes in mpg321 happened inside the (libid3tag)
library. The libid3tag library is heavily patched (patch
level is 10) by the distro maintainers. This shows that this
library is known to contain many bugs. gif2png always
crashed inside the application. This is conﬁrmed by both
metrics with high ﬁgures. pdf2svg crashed in libpoppler
most of the time. The stack frame distance is 3 because the
signal gets routed from Linux’ vdso through the standard
library. tcpdump and tcptrace use the same (libpcap)
library but, since tcpdump displays the content of the network
ﬂow, it has a higher distance from the library.
Based on the aforementioned analysis, we believe many
of the crashes reported by VUzzer uncover zero-day vulnera-
11
4.2
400
1.5
150
200
100
20
0
5
3
1
0
0
4
8 12 16 20 24
mpg321
0
4
8 12 16 20 24
tcptdump
100
50
10
0
410
350
250
200
100
50
0
0
4
8 12 16 20 24
gif2png
11
0
4
8 12 16 20 24
tcpttrace
20
15
10
5
0
0
10
5
0
0
4
8 12 16 20 24
pdf2svg
4
8 12 16 20 24
djpeg
Fig. 5. Distribution of crashes over a time period of 24 hours. X-axis: cummulative sum of creahes. Y-axis: time (over 24 hours). Blue line: VUzzer. Red
dashed line: AFL. Vertical green line: Time taken by VUzzer to ﬁnd the same number of crashes as those found by AFL during a complete run.
bilities and we are currently in the process of performing re-
sponsible disclosure to the open-source community. Table VII
provides information on some of the bugs that we have
analyzed and reported so far.
VI. RELATED WORK
In the previous sections, we have already highlighted some
of the major differences between VUzzer and state-of-the-
art fuzzers like AFL. In this section, we survey additional
recent research work in the area of fuzzing. This enables us
to highlight some of the features and differences with respect
to existing work.
A. Search-based Evolutionary Input Generation
The use of evolutionary algorithms for input generation
purposes is a well-explored research area in software engineer-
ing [7], [34]. There have been attempts to use evolutionary
algorithms for input generation to discover vulnerabilities
in applications [25], [42], [45]. The difference lies in the
fact that these approaches assume a-priori knowledge of the
application to focus on the paths leading to vulnerable parts of
the program. This property makes these approaches closer to
directed fuzzing and, therefore, our fuzzing strategy deviates
from them substantially. Unlike VUzzer, and similar to AFL,
the feedback loop used by these approaches does not attempt to
relate application behavior with the input structure to enhance
input generation.
B. Whitebox Fuzzing Approaches
Whitebox fuzzing is one of the earliest attempts to enhance
the performance of traditional random fuzzing by considering
the properties of the application. There exist a number of
approaches to make fuzzing more efﬁcient, for example, by ap-
plying symbolic execution and dynamic taint analysis to solve
branch constraints [20]–[24], [26]. Although VUzzer differs
from these approaches in a number of ways, the fundamental
difference remains the use of symbolic execution. Similar to
VUzzer, BuzzFuzz, proposed by Ganesh et.al. [20] makes use
of dynamic taint analysis, but for an entirely different purpose.
BuzzFuzz is a directed fuzzer and, therefore, it does not try to
learn constraints for every path. It instead uses taint analysis
to detect bytes that inﬂuence dangerous spots in the code, like
library call arguments, and mutate these bytes in the input to
trigger exceptional behavior. Most of these approaches also
require the availability of source code to perform analysis.
C. Blackbox/Graybox Fuzzing Approaches
In spite of being simple and fully application agnos-
tic, blackbox fuzzers,
like, Peach [1], Sulley [39], and