postgres=# select most_common_elems,most_common_elem_freqs from pg_stats where tablename='tmp_t' and attname='itemid';      
-[ RECORD 1 ]----------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------      
most_common_elems      | {1,10,11,12,13,14,15,16,17,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,115,116,117,119,120,122,123}      
most_common_elem_freqs | {1,0.20272727,0.08181818,0.053939395,0.03757576,0.03272727,0.02,0.021818181,0.015757576,0.013939394,0.20787878,0.10757576,0.05666667,0.041818183,0.03212121,0.030909091,0.023333333,0.01878788,0.02030303,0.015454546,0.20484848,0.097575754,0.0660606,0.05272727,0.04060606,0.030303031,0.025757575,0.025454545,0.016969698,0.021212121,0.21848485,0.10242424,0.0669697,0.05727273,0.036666665,0.034848485,0.041212123,0.026969697,0.022121212,0.017878788,0.21878788,0.10181818,0.06757576,0.04909091,0.043636363,0.04060606,0.035454545,0.03181818,0.022424242,0.028181817,0.21939394,0.10060606,0.06878788,0.06333333,0.042727273,0.036060605,0.028181817,0.033333335,0.03181818,0.026969697,0.22636363,0.095757574,0.07636364,0.055454545,0.04969697,0.043333333,0.035151515,0.033939395,0.028181817,0.026969697,0.20818181,0.11242424,0.07787879,0.062121212,0.048787877,0.043333333,0.035757575,0.036969695,0.03181818,0.03212121,0.22030303,0.10636364,0.0730303,0.05878788,0.04969697,0.03939394,0.03969697,0.034242425,0.03727273,0.035151515,0.02969697,0.024848485,0.026969697,0.02878788,0.021818181,0.016060606,0.02030303,0.01939394,0.016363636,0.014242425,0.015151516,0.016969698,0.021515151,0.014545455,0.016060606,0.013333334,0.014242425,0.013939394,0.013939394,0.015757576,0.013333334,1,0}      
```    
解读: [《PostgreSQL 9.2 add array elements statistics》](../201205/20120518_01.md)        
获取 TOP 10 的组合 :       
第一个元素是自己, 所以要输出11条        
[《PostgreSQL pg_stats used to estimate top N freps values and explain rows》](../201308/20130811_01.md)        
```      
select * from         
(select row_number() over(partition by r) as rn,ele from (select unnest(most_common_elems::text::int[]) ele,2 as r from pg_stats where tablename='tmp_t' and attname='itemid') t) t1        
join        
(select row_number() over(partition by r) as rn,freq from (select unnest(most_common_elem_freqs) freq,2 as r from pg_stats where tablename='tmp_t' and attname='itemid') t) t2        
on (t1.rn=t2.rn) order by t2.freq desc limit 11;       
 rn | ele | rn |    freq          
----+-----+----+------------      
  1 |   1 |  1 |          1  -- 自己的出现概率 100%      
 52 |  60 | 52 | 0.22878788  -- 接下来的9个值符合pgbench使用的长尾分布随机生成算法      
 62 |  70 | 62 | 0.22030303      
 12 |  20 | 12 | 0.21848485      
 32 |  40 | 32 |  0.2169697      
 72 |  80 | 72 | 0.21636364      
 42 |  50 | 42 |       0.21      
 82 |  90 | 82 |       0.21      
  2 |  10 |  2 | 0.20545454      
 22 |  30 | 22 | 0.20060606      
 73 |  81 | 73 | 0.11515152  -- 这个值是第二梯队10个中的的任意一个(因为概率差别很小很小)      
(11 rows)      
Time: 1.852 ms      
```      
即商品ID 1和商品ID 60,70,20,40,80,50,90,10,30,81最搭配, 以及他们与商品1双双同时出现的概率.       
验证近似值的准确度: 几乎100%      
使用unnest可以得到真实值, 和近似值几乎完全一致.      
```      
postgres=# select count(*) from tmp_t;      
 count        
--------      
 706647      
(1 row)      
select unnest(itemid) i , count(*)/706647.0 from tmp_t group by 1 order by 2 desc limit 11;      
 i  |        ?column?              
----+------------------------      
  1 | 1.00000000000000000000  -- 自己的出现概率 100%      
 90 | 0.22347225701092624748  -- 接下来的9个值符合pgbench使用的长尾分布随机生成算法      
 80 | 0.22228354468355487252      
 70 | 0.22185900456663652432      
 60 | 0.21966837756333784761      
 50 | 0.21766171794403712179      
 40 | 0.21571024853993578123      
 30 | 0.21147899870798291085      
 20 | 0.20742039519024350206      
 10 | 0.19680830740100785824      
 91 | 0.11075119543421255592  -- 这个值是第二梯队10个中的的任意一个(因为概率差别很小很小)      
(11 rows)      
Time: 1204.824 ms (00:01.205)      
```      
使用统计信息的近似查询最佳组合方法, 查询性能提升1000倍.      
#### 方法3:      
使用topn插件      
生成订单、购物车的时候实时计算. 存储到一种新的数据类型: 近似值类型.       
https://github.com/pipelinedb/pipelinedb/blob/master/pipelinedb--1.0.0.sql       
https://github.com/apache/datasketches-postgresql       
https://github.com/citusdata/postgresql-topn       
https://github.com/ozturkosu/cms_topn       
以topn为例:       
```      
create extension topn;      
postgres=# show topn.number_of_counters ;      
 topn.number_of_counters       
-------------------------      
 1000      
(1 row)      
Time: 0.192 ms      
```      
每个商品只需要存储1条记录(即统计结果记录)      
```      
create unlogged table tmp_topn (      
  itemid int primary key,       
  groups jsonb       
);      
insert into tmp_topn select 1, topn_add_agg(i::text) from (      
  select unnest(itemid) i from t where itemid @> array[1]      
) t;      
INSERT 0 1      
Time: 1977.195 ms (00:01.977)      
postgres=# select (topn(groups,11)).* from tmp_topn where itemid=1;      
 item | frequency       
------+-----------      
 1    |    706647      
 90   |    157916      
 80   |    157076      
 70   |    156776      
 60   |    155228      
 50   |    153810      
 40   |    152431      
 30   |    149441      
 20   |    146573      
 10   |    139074      
 91   |     78262      
(11 rows)      
Time: 0.706 ms      
```      
当产生新的订单时, 这个订单有N个商品, 那么在以上表中, 增量更新N条记录即可.         
```      
insert into tmp_topn select 1, topn_add_agg(i::text) from (      
  select unnest(itemid) i from t where itemid @> array[1]      
) t      
on conflict (itemid) do update set       
groups = topn_union(tmp_topn.groups,excluded.groups);      
INSERT 0 1      
Time: 1957.321 ms (00:01.957)      
```      
```      
postgres=# select (topn(groups,11)).* from tmp_topn where itemid=1;      
 item | frequency       
------+-----------      
 1    |   1413294      
 90   |    315832      
 80   |    314152      
 70   |    313552      
 60   |    310456      
 50   |    307620      
 40   |    304862      
 30   |    298882      
 20   |    293146      
 10   |    278148      
 91   |    156524      
(11 rows)      
Time: 0.706 ms      
```      
性能同样是1000倍提升, 而且随着数据量增加, 性能提升会更加明显.       
使用topn的方法, 可以每天为每个商品存储1条记录, 这样就能实行实时滑窗分析, 也是传统数据库无法高效实现的.    
```  
-- 例如分析双十一期间的ID=1的商品和哪10个商品最搭配  
-- topn_union_agg用于划窗聚合  
select topn(topn_union_agg(groups),11) from xx   
where ts >= '2021-11-01' and ts < '2021-11-12'  
and itemid=1 ;  
```  
#### 方法4:     
流计算, 略, pipelinedb已停更, 团队加入confluent, 可以折腾的小伙伴可以继续维护pipelinedb.       
目前也能通过任务调度+方法3来实现增量.       
又或者使用timescaledb的持续聚合功能. https://docs.timescale.com/api/latest/continuous-aggregates/create_materialized_view/        
## 参考      
1、topk 算法论文    
https://www.hlt.inesc-id.pt/~fmmb/wiki/uploads/Work/dict.refd.pdf       
https://pipelinedb-doc-cn.readthedocs.io/zh_CN/latest/builtin.html#top-k      
2、流计算    
https://github.com/pipelinedb/pipelinedb/blob/master/pipelinedb--1.0.0.sql       
3、持续聚合    
https://docs.timescale.com/api/latest/continuous-aggregates/create_materialized_view/    
4、近似计算     
https://github.com/apache/datasketches-postgresql       
https://github.com/citusdata/postgresql-topn       
https://github.com/ozturkosu/cms_topn       
5、    
[《为什么啤酒和纸尿裤最搭 - 用HybridDB/PostgreSQL查询商品营销最佳组合》](../201704/20170410_02.md)        
6、电商行业流量估算    
https://finance.sina.com.cn/tech/2020-11-12/doc-iiznezxs1360582.shtml      
11月1日至11月12日0:00，天猫双11总交易额达4982亿元。实时数据显示，天猫双11期间，成交额突破1亿元的品牌超过450个。      
11月11日23点，2020天猫双11全球狂欢季实时物流订单量破22.5亿单, 平均每天1亿单左右，这个数字约等于2010年全年中国快递量的总和。      
7、统计信息解读    
解读: [《PostgreSQL 9.2 add array elements statistics》](../201205/20120518_01.md)        
[《PostgreSQL pg_stats used to estimate top N freps values and explain rows》](../201308/20130811_01.md)        
8、长尾模型      
[《PostgreSQL pgbnech 支持 长尾模型数据生成 - 离散幂律概率分布 - random_zipfian》](../202105/20210519_03.md)      
9、自定义聚合的方法  
[《PostgreSQL 10 自定义并行计算聚合函数的原理与实践 - (含array_agg合并多个数组为单个一元数组的  
例子)》](../201801/20180119_04.md)    
[《PostgreSQL Oracle 兼容性之 - 自定义并行聚合函数 PARALLEL_ENABLE AGGREGATE》](../201803/2018031  
2_03.md)    
[《PostgreSQL 并行计算解说 之9 - parallel 自定义并行聚合》](../201903/20190317_01.md)    
10、滑窗分析  
[《PostgreSQL count-min sketch top-n 概率计算插件 cms_topn (结合窗口实现同比、环比、滑窗分析等) - 流计算核心功能之一》](../201803/20180301_03.md)    
#### [PostgreSQL 许愿链接](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
您的愿望将传达给PG kernel hacker、数据库厂商等, 帮助提高数据库产品质量和功能, 说不定下一个PG版本就有您提出的功能点. 针对非常好的提议，奖励限量版PG文化衫、纪念品、贴纸、PG热门书籍等，奖品丰富，快来许愿。[开不开森](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216").  
#### [9.9元购买3个月阿里云RDS PostgreSQL实例](https://www.aliyun.com/database/postgresqlactivity "57258f76c37864c6e6d23383d05714ea")
#### [PostgreSQL 解决方案集合](https://yq.aliyun.com/topic/118 "40cff096e9ed7122c512b35d8561d9c8")
#### [德哥 / digoal's github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
#### [PolarDB 学习图谱: 训练营、培训认证、在线互动实验、解决方案、生态合作、写心得拿奖品](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
#### [购买PolarDB云服务折扣活动进行中, 55元起](https://www.aliyun.com/activity/new/polardb-yunparter?userCode=bsb3t4al "e0495c413bedacabb75ff1e880be465a")
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")