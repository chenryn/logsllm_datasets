Memory and I/O management.
In the Intel SCC,
each LUT entry corresponds to an address space region of
16 MB. Therefore, the hypervisor allocates DRAM memory
in chunks of 16 MB. The chunks assigned to each VM need
not be contiguous and memory chunks assigned to a VM can
be easily re-assigned to other VMs by simply conﬁguring the
LUTs accordingly.
The LUT mechanism is also used to manage virtualized
I/O devices. Such virtualized devices usually expose a set of
virtual instances (e.g., [16]). Each virtual instance usually
consists of a set of memory-mapped registers that can be
exposed to individual VMs by conﬁguring their LUTs ap-
propriately. The interrupt management features available
on modern chipsets can then be used to route interrupts
from the I/O device to individual cores.
5.3 Security Analysis
The TCB of our architecture consists only of the hyper-
visor and the underlying hardware.
To protect against cross-VM attacks, the hypervisor con-
ﬁnes each VM to its allocated resources. In particular, the
hypervisor conﬁgures the LUTs so that a VM cannot access
a DRAM memory region, a virtual peripheral instance, or
on-tile memory of another partition. Furthermore, the hy-
pervisor clears the contents of memory regions before their
re-allocation. Only the hypervisor, running on the master
core, can modify the conﬁguration of the address-space iso-
lation mechanism.
To protect against cloud platform attacks, VM images at
rest are encrypted under the public key of the cryptographic
module. Address-space isolation ensures that the hypervi-
sor is the only component on the computing node that can
access the cryptographic module through shared memory.
An adversary that manages to compromise the CNI ser-
vice or the storage service can only access encrypted VM im-
ages. If the adversary compromises the management service,
he may impersonate a customer and, for example, substitute
the customer’s VM with one of his choice. Both attacks re-
sult in a denial-of-service and are out of our scope. Even if
a user’s credentials to log into the management service are
leaked, the adversary cannot access any of the VMs belong-
ing to that user, as long as those VMs require the user’s
private key for authentication.
Side-channels are a relevant attack vector in any system
where multiple VMs run on the same platform. Side-channel
attacks that extract ﬁne-grained information, such as a cryp-
tographic key, from a victim VM using shared cache [44, 45]
and memory deduplication [25] have been demonstrated.
These attacks are not applicable in our architecture, as parti-
tions share no cache and our hypervisor does not implement
deduplication. More coarse-grained attacks that leak VM
co-residency based on IP addresses [35] are applicable also in
Master CoreBoot InterfaceCryptographic ModuleHypervisor (H)HLUTsLUTsLUTsCoreCoreVMCNI ServiceCNI ServiceEnc. OSDRAMNoC...............OS1234BareMichael framework
LUT conﬁguration
VM/CNI load
Core and MPB reset
CNI interface
Communication library
Total
LOC
2557
47
84
35
123
588
3.4K
Table 1: Hypervisor complexity. We enhance the
BareMichael framework with LUT conﬁguration, VM load
and core reset functionality. Additionally we implement a
new communication library.
Figure 11: Distance of cores. We use cores 2, 10, 36 and
46 for our measurements. Core 2 is connected to the same
memory controller as master core, while cores 10, 36 and 46
use diﬀerent memory controllers.
a ﬁxed-size memory region on its closest memory controller.
The lightweight enhancements we propose to the SCC
platform enable logical partitions at the granularity of tiles
(two cores).
In our implementation we run a VM on the
ﬁrst core of the partition, while the second core is idle. In
Section 6 we discuss operating systems that use multiple
cores. Our implementation lacks a hardware cryptographic
module and we implement all cryptographic operations us-
ing the PolarSSL library [6].
5.5 Evaluation
Hypervisor complexity. The BareMichael framework
used as a basis for our hypervisor is 2557 LOC. We enhance
it with LUT conﬁguration, VM loading, and core reset func-
tionality. We also implement a new communication library
and CNI-interface. Table 1 shows the implementation size
for each of these components. Our hypervisor is roughly
3.4K LOC in total, which is comparable to the smallest hy-
pervisors available today [46].
Performance. We evaluate the time to setup the CNI
service and the time to start a VM.
The time required to load the CNI service depends, natu-
rally, on its size. We use a Linux OS that is roughly 32MB.
The CNI setup time also depends on the location of the
target core and its memory controller with respect to the
master core (Figure 10). We set the master core to be core
0 and measure the CNI setup time for target cores 2, 10,
36 and 46 (as shown in Figure 11). Cores 10, 36 and 46
are the farthest from core 0 and also use diﬀerent memory
controllers.
Figure 10 shows the results of our CNI setup experiments.
Each data point is an average over 100 measurements and
Figure 9: Compute node implementation. We imple-
mented the hypervisor using BareMichael OS framework and
run a Linux-based CNI service which has access to the net-
work and ﬁle-system over the PCIe interface. The host em-
ulates a boot interface.
our platform. Communication latency over the shared NoC
is a potential new side-channel. Common countermeasures,
such as those adding noise to VM timing information [27] or
modifying VMs to inject noise on the usage patterns of the
NoC [47], can be deployed to protect against such attacks.
5.4
Implementation
Our implementation setup assumes that the security en-
hancements described in Section 4.1 are available on the In-
tel SCC platform (i.e., we do not run the cloud architecture
implementation on top of the emulated hardware changes).
Figure 9 illustrates the components of our implementa-
tion. The hypervisor is a single threaded bare-metal ap-
plication based on the BareMichael framework [48]. This
framework provides a simple operating system compatible
with the individual cores of the Intel SCC. The hypervisor
runs on core 0, the ﬁrst core on the ﬁrst tile. We load the
hypervisor to the Intel SCC platform via the PCIe interface
of the host. The hypervisor image also contains the code
for the CNI service that, in our implementation, is an entire
Linux OS (thanks to hypervisor factorization, the CNI ser-
vice is not part of the TCB). At boot time the hypervisor
sets up a logical partition for the CNI service. In particular,
the hypervisor picks a free core (core 2) and allocates mem-
ory and network resources by conﬁguring that core’s LUT.
Then, the hypervisor loads the CNI service image into the
allocated memory and resets the core to start its execution.
After starting the CNI service, the hypervisor polls the
MPB of the CNI core to check for VM launch requests. We
implemented a small communication library for hypervisor–
CNI interaction. This library diﬀers from the standard Intel
SCC communication model (and implementation) that as-
sumes all cores can access each other’s MPB which is not
the case in our architecture. Instead, our library restricts
the CNI service to accessing its own MPB for communicat-
ing with the hypervisor. This library uses the on-tile MPBs
for data exchange and a shared lock for synchronization.
By maintaining separate read and write buﬀers, the library
enables full duplex communication.
The CNI service issues requests for allocating new cores,
loading VM images, and starting VMs. The CNI service
has access to a network-based ﬁle-system to fetch encrypted
VM images. The hypervisor tracks the free cores and uses an
example resource allocation policy that assigns to each core
RBareMichael-based HypervisorRLinux-based CNIMaster CoreCoreLUTsLUTsUbuntu-host as a Boot InterfaceINTEL SCC CO-PROCESSORPCIeLinux-based VMCoreLUTsRNoCMCMCMCMCRRRRRRRRRRRRRRRRRRRRRRRR36460246810  H(a) LUT Setup
(b) CNI Load
(c) MPB Clear
Figure 10: CNI service launch time. Starting a CNI service on a core involves three main steps: LUT setup, image loading,
and clearing the MPBs. The time required for each of these operations depends on the distance from the master core to the
target core. Here we evaluate launch time to cores 2, 10, 36 and 46. We use a CNI service of 32MB and the overall startup
time is between 1.7 s and 2.3 s
variances are negligible. It takes between 1.7 s and 2.3 s to
setup the CNI service, depending on the distance between
the master core and the target core. The CNI setup time is
dominated by the time to load its image (Figure 10.b). We
run the hypervisor on core 0 and we anticipate that running
the hypervisor on a core in the center of the interconnect
yields faster results.
Compared to the CNI setup time, starting a VM requires
additional time. This overhead is due to the time required
to decrypt the VM image and for the hypervisor and the
CNI service to coordinate the launch of the VM.
As a VM image, we use an encrypted version of a Linux
OS of 32MB. We load it from core 0 to core 46, the farthest
core, which gives an upper limit of the loading time. Over
100 runs, it takes on average 43.47 s (± 5 ms) from the time
when the CNI service requests to start a VM, to the time
the hypervisor resets the target core of the VM. Most of the
time is spent decrypting the VM image: 41.6 s (±4.6 ms).
Since we emulate the cryptographic module, the decryption
measurements are not representative of a real deployment
scenario. On a hardware cryptographic module, similar op-
erations would take less than a second [8] and the expected
load time would be roughly three seconds.
Compared to the typical lifetime of a VM, we argue that
even the overhead of our current implementation (with soft-
ware decryption) is acceptable.
6. DISCUSSION
Partitions with multiple cores. Our design can ac-
commodate logical partitions that encompass any number
of cores, at the granularity of two cores. Running an OS on
more than one core requires (i) that all the cores assigned
to a partition have access to all the resources allocated to
that partition, and (ii) an operating system that can run on
multiple cores of an asymmetric processor. In the security-
enhanced Intel SCC processor, the hypervisor can achieve
the former by simply conﬁguring the LUTs of all the cores
within a partition to point to the same set of addresses. To
realize the latter, one could use a distributed OS kernel like
Barrelﬁsh [10] or Helios [32].
Partitions for user code. We demonstrated how our
security-enhanced Intel SCC processor can be used to run
multiple operating systems in isolation in an IaaS cloud.
However, current operating systems have grown complex
and are prone to vulnerabilities [3, 5]. As a result, there
is a need to create Trusted Execution Environments (TEEs)
for running sensitive user code in isolation from a poten-
tially untrusted operating system. In a cloud deployment, it
is beneﬁcial to create multiple such environments that scale
with the number of VMs.
Many-core platforms are also available as co-processors, so
they naturally provide an alternative execution environment
to the host that runs the VMs. Furthermore, by carefully
partitioning and adding cryptographic support to such co-
processors, one could implement concurrent TEEs similar to
Intel SGX [30]. The design of an architecture for isolating
user-level code is beyond the scope of this work.
Processor vs. I/O virtualization. Our cloud archi-
tecture relies on I/O virtualization for enabling access to
peripherals from VMs. The currently available virtualized
peripherals replicate only the necessary subset of their func-
tionality, and thus provide good scalability [16]. For exam-
ple, the Intel Ethernet Controller XL710 supports up to 128
virtual instances. The goal of our work is to investigate
similar resource minimization on the many-core processors.
As we separate the functionality required for the hypervi-
sor (master core) from those required for the VMs (other
cores), the cores do not require additional execution mode
or address translation. This is in contrast to current proces-
sor virtualization techniques, where both the hypervisor and
the VMs run on every core. While a fully functional cloud
architecture continues to require virtualization support on
its peripherals, our approach enables simpliﬁcation of the
cores and thus improves overall system scalability.
Availability of user data. In this paper we focus on
protecting the integrity and conﬁdentiality of the user’s VM.
Ensuring availability is also an important factor in cloud de-
ployments. In our architecture, providing data availability
requires mechanisms such as data replication. Such mech-
anisms do not aﬀect our architecture and can be seen as
complementary to our work.
Resource utilization. While cloud architectures that
leverage over-subscription can provide better eﬃciency, they
rely on resource sharing that can lead to cross-VM attacks [44,
45]. The focus of this work is on cloud architectures where,
thanks to assignment of dedicated resources and simpliﬁed
resource management, several similar attacks are not possi-
ble and the system TCB size can be signiﬁcantly reduced.
 CNI core2103646 Time (7s)05101520 CNI core2103646 Time (s)00.511.522.5 CNI core2103646 Time (7s)020406080Future many-core platforms. Many-core processors
are an emerging technology and there is uncertainty regard-
ing optimal design choices for hardware and software on such
platforms. A major issue for disagreement is the eﬃciency of
synchronizing shared data structures across a large number
of cores in hardware and software. For example, it is unclear
if cache coherence would beneﬁt or hurt the performance
of processors with a very large number of cores [28, 29].
Therefore, it is hard to predict how future processors will
be designed and how eﬃciently our solution can be ported