differential privacy called d-privacy that will be useful here. A
metric d on a set X is a function d : X 2 → [0,∞) satisfying
d(x, x) = 0, d(x, x(cid:48)) = d(x(cid:48), x), and d(x, x(cid:48)(cid:48)) ≤ d(x, x(cid:48)) +
d(x(cid:48), x(cid:48)(cid:48)) for all x, x(cid:48), x(cid:48)(cid:48) ∈ X . A randomized algorithm A :
X → Z satisﬁes (d, )-privacy if for all Z ⊆ Z,
P (A(x) ∈ Z) ≤ exp( × d(x, x(cid:48))) × P (A(x(cid:48)) ∈ Z) .
In our context,
the application of A to sufﬁciently close
examples x and x(cid:48) (i.e., d(x, x(cid:48)) is “small”) from different
classes will ensure that any classiﬁer has a similar probability
of classifying A(x) and A(x(cid:48)) within any subset Z of classes.
III. A MOTIVATING EXAMPLE
Side-channel attacks leverage ML to automatically learn
critical features of side-channel observations and make sta-
tistical inferences to extract secrets. Recent advances in deep
learning [29], [33], [63], [65] further empower side-channel
attackers to conduct more accurate and efﬁcient
inference
attacks. One such example is recently demonstrated remote
identiﬁcation of encrypted MPEG-DASH video streams by
Schuster et al. [55].
A. Remote Identiﬁcation of Encrypted Video Streams
MPEG-DASH is a video streaming standard that segments
video streams to variable segment sizes due to variable-rate
encoding, and instruments the request of video content at
the granularity of segments. Schuster et al. [55] demonstrated
that packet burst patterns of the encrypted video streams (an
observable side channel that reveals the size of the segments)
can be correlated to the content of the videos that are requested
from the client. They further developed techniques using
convolutional neural networks (CNNs) to ﬁngerprint video
streams from YouTube, Netﬂix, Amazon, and Vimeo with
very high detection accuracy. For instance, their techniques
identiﬁed YouTube videos (from a small dataset of 18 videos)
with 0 false positives and 0.988 recall.
In this paper, we used this MPEG-DASH video-stream
ﬁngerprinting as a motivating example to explore how side
channels using ML can be mitigated. To demonstrate the
capability of the attacks, we extended the idea presented in
Schuster et al. [55] and performed ﬁngerprinting attacks of 40
Youtube videos using a set of ﬁve ML classiﬁers. The attack
was performed in a closed-world setting, in which we assumed
the video to be classiﬁed is one of the 40; this closed-world
setting is the most advantageous to the attacker and the least
favorable to the defender.
3
Fig. 1: CNN architecture. n denotes the number of elements of one trace, which is the total time divided by the window size.
B. Data Collection
We manually chose 40 Youtube videos related to four
types of sports (basketball, American football, soccer, and
hockey) as our dataset. To ﬁnd these videos, we typed “NBA”,
“NFL”, “MLS” and “NHL” into Youtube search separately,
ﬁltered out the short videos that are less than 20 minutes
(to make sure the video length is long enough for analysis),
and selected 10 videos from each category. Each of the 40
videos was visited from a Chrome browser 100 times during
trace collection. Thus 100 traces were collected for each video.
Therefore, in total 4000 (i.e., 40× 100) traces with 40 distinct
labels (i.e., the content of the videos) were included in our
dataset. We recorded the timestamps and sizes of all packets
of the ﬁrst 3 minutes of network trafﬁc after starting to stream
each video. The data collection process was automated using
Selenium [56] and Wireshark’s tshark [71]. All the data
were collected from a desktop running Ubuntu 17.10 connected
to our campus network using 1 Gbps Ethernet. The whole
process of data collection took about 15 days.
C. Classiﬁcation
Preprocessing. To convert videos in the dataset into feature
vectors of equal length, we aggregated the raw data into 0.25-
second bins. Here, 0.25s is the window size (w). Each 3-minute
video stream was thus abstracted as an array of 720 elements
(i.e., bins). Note that we did not ﬁlter out the ad trafﬁc that
occurs at the beginning of the captures.
Classiﬁers. We implemented ﬁve classiﬁers, including Support
Vector Machine (SVM), Logistic Regression (LR), Random
Forest (RF), Neural Net and Convolutional Neural Network
(CNN),
in Python. Speciﬁcally, SVM, LR and RF were
implemented using scikit-learn [51], Neural Net and
CNN were implemented using Tensorflow [1] with the
Keras [14] front end. For Neural Net, we used a single
Dense layer with 40 neurons and the Sigmoid function as
the activation function. For CNN, we used the same structure
as that used by Schuster et al. [55]. It consists of three
convolutional
layers, 1 max pooling layer, and two dense
layers. The detailed CNN structure is shown in Fig. 1.
Classiﬁcation results. We applied the 5 classiﬁers to classify
the 4000 video traces. We used 5-fold cross-validation: each
time, a different 20% of the traces were used for testing while
the remaining 80% were used for training. The features of
the dataset were normalized using the MinMaxScaler()
method provided by scikit-learn. For CNN, we used a
batch size of 32 and the model was trained for 40 epochs1. As
shown in Table I, SVM, LR and RF achieved 0.809, 0.823, and
0.751 classiﬁcation accuracy, respectively. Neural Net reached
0.831 classiﬁcation accuracy. CNN had the highest accuracy
1The model converged after 40 epochs. Training for 1000 epochs improved
the accuracy by only 0.024.
of 0.944. The classiﬁcation results had very small variance
in the 5-fold tests. These experiments validate the attack
demonstrated by Schuster et al. [55]. The results suggest that
machine learning, and particularly deep learning (e.g., CNN),
can empower trafﬁc analysis to easily identify the Youtube
video streams from encrypted trafﬁc.
Model
Average Accuracy
Standard Deviation
SVM
0.809
0.067
LR
0.823
0.063
RF
0.751
0.046
Neural Net
0.831
0.011
CNN
0.944
0.004
TABLE I: Classiﬁcation accuracy with one standard deviation.
IV. ADVERSARIAL MACHINE LEARNING
Our ﬁrst attempt is to fool the machine-learning attackers
with techniques used in adversarial machine learning.
A. Crafting Adversarial Samples
To generate adversarial samples, we followed the Fast
Gradient Sign Method (FGSM) proposed by Goodfellow et
al. [21]. Let x be the input sample, g(x; θ) the classiﬁer param-
eterized by θ, y the true label associated with x, L(g(x; θ), y)
the loss/cost function of the classiﬁer, and η the parameter that
controls the amount of perturbation. For untargeted attacks—
i.e., the classiﬁer misclassiﬁes a sample as any label but the
true label—FGSM generates the following adversarial sample
x∗ from the clean sample x:
x∗ = x + η sign(∇xL(g(x; θ), y)).
The perturbation x∗ − x is the gradient image ∇x of the
given loss L, which by deﬁnition is the direction where the loss
increases the most. The method then takes only the sign values
of the gradient to make it unit l∞-normed, then multiplies
the normalized gradient with the desired perturbation strength
η. When η is large, the perturbation is more effective but is
more detectable to human eyes or machine classiﬁers. When
η is small it is less effective but is less likely to be detected.
In our experiment, we used the FastGradientMethod()
in the cleverhans [49] Python library. We adjusted the
level of injected noise (dictated by the eps parameter) to
generate adversarial samples corresponding to different noise
levels. The noise level denotes the maximum distortion of the
adversarial sample compared to original input, which is usually
a value between 0 and 1. Suppose the original value is v. With
eps = α, the adversarial value is within the range of v ± αv.
To see how the classiﬁers perform on adversarial samples,
we targeted the CNN model and generated corresponding
adversarial test samples using FGSM, with the noise level eps
= 0.1. Then, we fed these samples to the CNN classiﬁer trained
using clean samples. The CNN classiﬁer was unable to classify
such samples successfully, with only 0.086 accuracy, which
is signiﬁcantly lower than the original accuracy (0.944). This
result suggests that the adversarial samples are very effective
against this ML attacker.
4
regenerated adversarial samples against the model used by the
attacker, and then the attacker trained a new model according
to the adversarial samples. The resulting classiﬁcation accuracy
(including the ﬁrst round) was [(0.086, 0.908), (0.156, 0.780),
(0.272, 0.705), (0.245, 0.536), (0.264, 0.410)], where the ﬁrst
element of each 2-tuple is the classiﬁcation accuracy after the
defender’s move and the second is the result after the attacker’s
move. Because the dataset is not huge, the results converged
quickly. However, there is no known principled way to ﬁnd
such an equilibrium [22], and it requires a lot of effort to
do so empirically. Therefore, unlike the majority of works
in adversarial ML where the classiﬁer is the victim and is
assumed to stay unchanged, in our setting the adversary is
assumed to be aware of any defense strategy that is taken
and allowed to adapt accordingly. The defender faces a much
harder situation when applying adversarial ML techniques
under such an assumption.
V. DIFFERENTIALLY PRIVATE STREAMING
The failure in the adoption of adversarial samples to defeat
streaming trafﬁc analysis motivated us to seek more principled
solutions to counter such a powerful adversary. Differential
privacy stands out as a feasible solution. Differential privacy
offers a principled privacy guarantee for statistical databases
that allows users to query aggregate statistics of elements in
the database without leaking individual data elements [17].
It offers strong privacy promises that guarantees statistical
indistinguishability of two databases that are different in only
one element.
In this section, we would like to develop -differentially
private mechanisms for streaming trafﬁc, which, by adding
random noise (dictated by  and a distance threshold t) into
the encrypted video streams, render any two videos within
distance t to be statistically indistinguishable to each other.
In this sense, any two video streams within distance t (which
can be selected by the defender) can be intermingled and made
indistinguishable with respect to -differential privacy, though
in extreme cases may require adding substantial noise. In this
section, we explore two mechanisms, FPAk and d∗-privacy,
to enforce differential privacy on streaming trafﬁc.
A. Fourier Perturbation Algorithm (FPAk)
Rastogi et al. [52] proposed the Fourier Perturbation Algo-
rithm (FPAk), which can answer long query sequences over
correlated time-series data in a differentially private manner
by using the Discrete Fourier Transform (DFT). A DFT is a
linear transform of a length-n real or complex-valued sequence
Q = (Q[1], ..., Q[n]) into another length-n complex-valued
sequence F = (F [1], ..., F [n]) where
F [j] =
2π
exp(
ij)Q[i].
√−1
n
n(cid:88)
i=1
n(cid:88)
i=1
The F [j] is called the j-th Fourier coefﬁcient of the DFT(Q).
An Inverse DFT (IDFT) is also a linear transform of a
complex-valued sequence P = (P [1], ..., P [n]) to another
complex-valued sequence R = (R[1], ..., R[n]) where
2π
exp(
ij)P [i].
√−1
n
R[j] =
1
n
5
Fig. 2: Test CNN adversarial samples on ﬁve classiﬁers. Each
data point is the result of a 5-fold cross-validation.
B. Limitations of Adversarial Samples
However, the attacker can also take actions to adapt to these
adversarial samples. Here, we study two possible approaches:
using a different classiﬁer and conducting adversarial training.
1) Using a different classiﬁer: The adversarial samples
generated by the FGSM are designed to fool one particular
classiﬁer, which may not be able to deceive other classiﬁers.
To see how the adversarial samples generated for the CNN
model affect other classiﬁers, we fed these samples to other
trained models mentioned in Sec. III-C. We adjusted the noise
level (i.e., the eps parameter) in FastGradientMethod()
to gain a better understanding of how it would affect the
classiﬁcation accuracy. The higher the noise level, the more
distortion in the samples. The result is shown in Fig. 2. Each
data point was generated by conducting a 5-fold validation.
The mean values of the classiﬁcation results are shown in the
ﬁgure, and all standard deviations are below 0.01, which are
too small to be visible. From this ﬁgure, we can see that when
the noise level was 0.05, the CNN model achieved only about
0.20 accuracy, but SVM and LR still reached over 0.6 accuracy.
The effectiveness of the adversarial ML techniques against
trafﬁc analysis attacks highly depended on the ML algorithms
and parameters used by the adversary. Although the accuracy
of other ML models also dropped when the noise level was
increased (e.g., eps=0.1), some classiﬁers still outperformed
CNN signiﬁcantly (e.g., 0.294 for SVM compared to 0.086
for CNN). Moreover, the attacker could conduct adversarial
training to easily circumvent the defense, as shown shortly.
2) Conducting Adversarial Training: Although the de-
fender can generate adversarial samples to fool the attacker,
the attacker can utilize the adversarial training technique to
reinforce its learning. We used the FGSM method mentioned
above to craft the adversarial samples for our training set with
the noise level eps=0.1, and re-trained the CNN model for 10
epochs. After this process, the new classiﬁer achieved 0.908
accuracy on the adversarial test samples, which is signiﬁcantly
higher than 0.086, the original accuracy.
Of course, the defender could then regenerate adversarial
samples against the new model to defeat such attacks; however,
the attacker is also capable of adapting accordingly. This arms
race can be repeated for many rounds, until they reach an
equilibrium (if one exists). For instance, with the dataset we
have, we continued the arms race and performed 4 more rounds
of the following experiments: In each round, the defender
0.000.020.040.060.080.10eps0.00.20.40.60.81.0AccuracyCNNSVMLRRFNeural NetAn IDFT has the property IDF T (DF T (Q)) = Q.
Let Lap (λ) denote a random variable drawn from the
Laplace distribution with scale λ and location µ = 0. Suppose
the inputs of the FPAk algorithm are Q, λ, and k. FPAk is
described as follows:
(a) Keep the ﬁrst k Fourier coefﬁcients F [1], ..., F [k] after
computing DFT(Q).
(b) Compute ˜F [i] = F [i] + Lap (λ) for i = 1, ..., k.
(c) Return ˜Q = IDF T (P ADn([ ˜F [1], ..., ˜F [k]])), where
P ADn([ ˜F [1], ..., ˜F [k]]) denotes the sequence of length
n obtained by appending n − k zeros to ˜F [1], ..., ˜F [k].
√
Rastogi et al. [52] proved that FPAk (Q, λ) is -differentially
k∆2(Q)/, where ∆2(Q) denotes the L2