image. For example, Figure 9c shows an APPI with a brick-
wall texture. Texturing is typically done by overlaying an
image with that of a texture material, which often contains rich
high-frequency signals. So similar to noising, this technique
makes image structural information diﬃcult to extract.
• Blurring. Blurring (or smoothing) spreads each pixel’s RGB
color value to those of nearby pixels, typically by applying
a ﬁlter to an image. Popular blur ﬁlters include mean ﬁlter,
weighted average ﬁlter, and Gaussian ﬁlter. Unlike noising and
texturing, blurring weakens the high-frequency signals on the
original image, by making the transition from one color to the
other smoother, which results in less obvious edge features.
• Occlusion. A common way to obfuscate explicit content is to
apply occlusions to sensitive areas (e.g., posing lip stickers on
the breast area as shown in Figure 9g). Occlusion hides the
critical area of a pornographic image, which could hide the
key features to identify its explicit content, though visually
the sexual semantics still gets through to the viewer.
• Transparentization and overlay. Transparentization is a
process to make an image semi-transparent, which lowers
the contrast of the image and therefore weakens its edge
features. Semi-transparent images are usually overlaid on top
of other images, causing an eﬀect similar to texturing that
makes structural information harder to extract.
Table III shows how these obfuscation techniques are de-
ployed across all detected APPIs. Noising appears to be the
most common approach. Moreover, we observe that spammers
may combine several techniques together: 816 APPIs (19.8%)
contain at least 3 diﬀerent types of obfuscations.
Evading state-of-the-art detection. Our study shows that
these APPIs are indeed eﬀective on existing explicit content
detection. We run Google Cloud Vision API, Baidu AipImage-
Censor API, Yahoo Open NSFW model, and Clarifai NSFW
API on all the APPIs discovered in our research. It turns
out that 35.6% of them cannot be detected by any of these
detectors. Table IV presents their detection rates. Among them,
Yahoo Open NSFW model achieves the highest detection rate.
Still, more than 2,600 APPIs are missed by this detector.
(cid:26)(cid:22)(cid:26)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:51:56 UTC from IEEE Xplore.  Restrictions apply. 
(a)
monochroma-
tization
(b) noise
(c) texture
(d) transparentization
(e) rotation
(f) blur
(g) occlusion
Fig. 9: Examples of APPIs.
TABLE IV: Detection rates of the 4 explicit content detectors.
TABLE V: Parameters used in 5 types of distortions.
detector
# detected APPIs
detection rate
Noise Type
Google Cloud Vision API
Baidu AipImageCensor API
Yahoo Open NSFW model
Clarifai NSFW API
1,310
1,546
1,728
1,242
30.9%
35.5%
39.7%
28.5%
Gaussian noise (var)
Box blur (ksize)
Transparentization (alpha)
Rotation (degree)
Color manipulation
L1
0.083
(3,3)
0.8
45
gray
L2
0.107
(5,5)
0.6
90
blue
L3
0.131
(7,7)
0.4
145
green
L4
0.155
(9,9)
0.2
180
red
noising indicate that the technique is eﬀective when the noise
intensity is low but its performance cannot be improved sig-
niﬁcantly when the intensity increases. Transparentization and
blurring, however, perform much better at a higher intensity
level than a lower level. Of particular interest are the “W”
shape rotation curves: rotating an image by 90 or 180 degree
does not make a porn image much harder to detect; while
rotating it by 45 or 135 does. This indicates that both Google
and Yahoo models seem to use rotation invariant features, but
such features only work on right-angle rotations. This could be
caused by the absence of other rotated pictures in their training
sets. Lastly, color manipulation exhibits a “V” shape curve,
indicating that the detectors react diﬀerently to diﬀerent colors:
red images are easiest to detect as the color comes closet to
the skin color compared with other colors tested, while green
ones turn out to be hardest to ﬁnd. This also implies that both
detectors use an image’s color features (probably skin color).
Furthermore, we study how the obfuscation techniques im-
pact neural network-based detection systems, i.e., Yahoo Open
NSFW model. In particular, we compare the original images’
64 features learned by the ﬁrst convolutional layer of Yahoo’s
open NSFW model with those of the images obfuscated by
the 7 aforementioned obfuscation techniques. Figure 11 shows
the input images and the extracted features (each displayed in
a small square), illustrating how each obfuscation technique
aﬀects the initial layer of the neural network.
C. Adversarial Examples
As mentioned earlier, recent studies on adversarial learning
in image processing focus on ﬁnding the adversarial examples
that use almost imperceivable perturbations to induce mis-
classiﬁcation [37], [49], [55]. To ﬁnd out whether there is
evidence that the real-world adversary indeed utilizes these
techniques to seek adversarial examples, we inspect all the
APPIs found in our research, looking for high-quality images
(with almost imperceivable perturbations) that also circumvent
Fig. 10: Explicit content detection results on the distorted
images.
To further understand the eﬀectiveness of
the afore-
mentioned obfuscation techniques, we sample 250 (non-
adversarial) porn images from the porn picture set (for training
Mal`ena), and apply 5 obfuscation techniques (noising, blur-
ring, transparentization, rotation, and color manipulation) each
with 4 diﬀerent settings (such as angle for rotation, density
of noise, etc., see Table V). In this way, we get a total of
5,250 images (including the original images), on which we
run Google Cloud Vision API and Yahoo Open NSFW model.
The results are presented in Figure 10. As we can see from
the ﬁgure, with proper settings, these 5 obfuscation techniques
are able to eﬀectively degrade the performance of start-of-
the-art explicit content detectors. Compared with Yahoo Open
NSFW, Google Cloud Vision seems to be more robust against
blurring and transparentization (the two processes have similar
inﬂuence on the image), but more sensitive to noise.
Also we ﬁnd that the eﬀects of these obfuscation techniques
are quite consistent under diﬀerent settings in both detectors,
given their diﬀerences. Particularly, the “L” shape curves of
(cid:26)(cid:23)(cid:17)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:51:56 UTC from IEEE Xplore.  Restrictions apply. 
(a) original
(b) noise
(c) blur
green-
(d)
colored
transpar-
(e)
entization
(f) rotation
(g) texture
(h) occlusion
(i) original
(j) noise
(k) blur
(l) green-colored
(m) transparentization
Fig. 11: Features of distorted images extracted by Yahoo NSFW’s ﬁrst convolutional layer (after max pooling).
(p) occlusion
(n) rotation
(o) texture
all explicit content detectors. In the end, we fail to ﬁnd any
such adversarial examples, even when we look at the images
without promotional information.
To understand whether the absence of such adversarial
examples is actually caused by Mal´ena’s limited capability
to ﬁnd them, we generate 200 adversarial examples for the
Yahoo Open NSFW model using the state-of-the-art C&W
approach [29], and then pass these examples to our implemen-
tation. Mal´ena successfully detects 196 of them, while Yahoo
classiﬁed all of them as non-explicit. This indicates that to-
day’s cybercriminals likely still rely on a set of predetermined
obfuscation techniques (see Section V-B) to generate APPIs,
not gradient descent.
VI. Promotional Explicit Content Campaign
The discovery of APPIs and their promotional information
and distribution channels enables us to investigate the ecosys-
tem of such illicit promotions. In our research, we look into the
content the APPI spammer promotes, the correlation among
diﬀerent APPIs and the way such images are disseminated.
Also, we analyze a large APPI campaign as a case study.
A. Promotional Content Analysis
As mentioned earlier, we ﬁnd that APPIs carry two types of
promotional content: text and QR codes. To extract such pro-
motional information, we leverage Google Cloud Vision API’s
OCR function to extract text for further manual validation. For
all the QR codes detected by our preprocessor (Section III-C),
we ﬁrst attempt to use ZBar [13] to automatically decode
each of them. If ZBar fails, we then manually scan it using
WeChat, which almost always works on these codes, even in
the presence of some obfuscation. In this way, 612 unique
promotional content pieces are discovered in the form of
URLs, QQ and WeChat (popular instant message apps) IDs,
Weibo IDs, QR codes etc. Table VI shows the number of
promotional content pieces in each type. We observe that QR
code is the most prevalent one (1,430 out of 3,432, 41.7%),
because it
is convenient for the target viewers to extract
the promotion information directly from images using their
Wechat apps.
Interestingly, in addition to the contact information for illicit
products,
items in APPIs sometimes include trending
Internet buzzwords. For example, we ﬁnd “skr” (a popular
text
(cid:26)(cid:23)(cid:18)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:51:56 UTC from IEEE Xplore.  Restrictions apply. 
TABLE VI: Statistics of promotional content.
Type
QQ ID
Weibo ID
WeChat ID
QR code
URL
Weibo Weibo (unique)
17
375
239
0
0
7
261
110
0
0
Tieba
186
8
1092
1430
85
Tieba (unique)
69
5
135
45
31
TABLE VII: Examples of sensitive text replacement.
Examples
v
“刊片”
“企鹅”
“呦呦”
vx
Type
emoji
jargon
jargon
homophonic
homophonic+initial
Meaning
WeChat
porn movie
QQ
child porn
WeChat
Num
12
10
18
8
39
Internet buzzword trending in the late July) was used in 13
APPIs posted on Weibo during that particular time period,
even though the meaning of the word is totally irrelevant
to the products being promoted (adult videos). Apparently,
APPI spammers try to leverage such eye-catchy words to draw