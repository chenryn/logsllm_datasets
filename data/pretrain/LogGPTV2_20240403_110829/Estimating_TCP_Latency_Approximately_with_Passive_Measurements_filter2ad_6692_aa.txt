title:Estimating TCP Latency Approximately with Passive Measurements
author:Sriharsha Gangam and
Jaideep Chandrashekar and
&apos;Italo Cunha and
Jim Kurose
Estimating TCP Latency Approximately
with Passive Measurements
Sriharsha Gangam1, Jaideep Chandrashekar2, Ítalo Cunha3, and Jim Kurose4
1 Purdue University
PI:EMAIL
2 Technicolor Research
PI:EMAIL
3 UFMG, Brazil
PI:EMAIL
4 Univ. of Massachussetts, Amherst
PI:EMAIL
Abstract. Estimating per-ﬂow performance characteristics such as latency, loss,
and jitter from a location other than the connection end-points can help locate
performance problems affecting end-to-end ﬂows. However, doing this accurately
in real-time is challenging and requires tracking extensive amounts of TCP state
and is thus infeasible on nodes that process large volumes of trafﬁc. In this paper,
we propose an approximate and scalable method to estimate TCP ﬂow latency in
the network. Our method scales with the number of ﬂows by keeping approximate
TCP state in a compressed, probabilistic data structure that requires less memory
and compute, but sacriﬁces a small amount of accuracy. We validate our method
using backbone link traces and compare it against an exact, baseline approach. In
our approximate method, 99% of the reported latencies are within 10.3 ms of the
baseline reported value, while taking an order of magnitude less memory.
1 Introduction
Latency is a key determinant in the performance of a network ﬂow and large values can
adversely affect bulk transfers, increase buffering, and make interactive sessions unre-
sponsive. Thus, tracking ﬂow latency is a critical tool in monitoring the performance of
TCP-based applications; these form the bulk of Internet trafﬁc today. While estimating
this latency is an intrinsic part of TCP and thus trivial at the end-points of a connec-
tion, it is extremely challenging in the middle of the network, i.e., at a network node
along the path connecting the end-points. At the same time, the ability to infer the ﬂow
latency at such locations would be extremely valuable to users and network operators.
Consider a typical WiFi-enabled home network with DSL broadband connectivity. To-
day, when applications underperform or latencies to destinations are larger than usual,
it is extremely difﬁcult to reason about where the bottleneck is. Is the increased latency
occurring inside the wireless network? or is the server slow to respond? Answering this
seemingly simple question directly is quite difﬁcult (even if we could query the end-
points for their estimates). This question is easy to answer if the home gateway could
M. Roughan and R. Chang (Eds.) PAM 2013, LNCS 7799, pp. 83–93, 2013.
c(cid:2) Springer-Verlag Berlin Heidelberg 2013
84
S. Gangam et al.
estimate latencies of the connections on the wireless link. Similar applications could be
imagined at data center borders or egress routers in enterprise networks.
This area has attracted a lot of attention in the past and several methods have been
proposed; these broadly fall into two categories—active or passive. Active methods rely
on probing the destination(s) independent of the TCP ﬂow (various ﬂavors of ping ex-
ist), or else by inserting a transparent TCP proxy on the path of the TCP ﬂow [4]. This
has the effect of terminating one half of the connection, and creating a new connection
from the midpoint. Clearly, this is impractical for large numbers of concurrent ﬂows.
Moreover, terminating the ﬂow in the middle of the network alters the ﬂow and may not
be acceptable. The other set of methods is based on passive observations of the trafﬁc. A
single RTT estimate can be obtained by matching SYNs and ACKs in the beginning of a
TCP connection [11]; this is useful, but of limited utility since latency can change con-
siderably over the connection duration. Another idea is to infer the RTT by computing
the delay between the transmission of two consecutive congestion windows [10,12,15].
In [2, 10], the TCP state machine is emulated ofﬂine, using passively recorded traces,
to infer RTT estimates by matching ACKs and TCP sequence numbers. Some of these
methods can track latency over the entire duration of a connection in the middle of the
network; however, they are not scalable and are not designed to be run in real-time.
The challenge in accurately estimating TCP latency in the network centers on the
amount of state that needs to be maintained. Packets going in one direction need to be
stored and matched with acknowledgements coming back the other way. In measure-
ment points that handle a large number of ﬂows (routers in ISP networks, data center
switches) or embedded devices that are resource constrained, it is generally infeasible
to store sufﬁcient TCP state information, or to process it fast enough to support very
accurate TCP latency estimates in real-time. The key observation we make is that when
estimating latency, a strict accuracy constraint limits how well a solution can scale.
There are applications that need to measure latency accurately and with a high degree
of precision (electronic trading systems, B2B applications). Correspondingly, there are
particular solutions that target these markets, relying on specialized hardware and mul-
tiple vantage points (see [13, 14]). However, most other applications, particularly those
that focus on troubleshooting or performance diagnosis, are more interested in track-
ing whether latencies are within a speciﬁed range or if they have exceeded a threshold.
Importantly, such applications can tolerate approximate answers and a certain amount
of error. Take for example an application that monitors VoIP call quality; acceptable
quality might require that accuracy not exceed 150ms [9]. Similar latency thresholds
are associated with other applications: 100 ms for online ﬁrst person shooter and rac-
ing games [6], and in the same region for video streaming applications [16]. In such
applications, tracking approximate latencies is good enough.
In this paper we investigate the problem of performing scalable and approximate
latency estimation in real-time inside the network. We describe such a method, called
ALE (Approximate Latency Estimator), present its key ideas and introduce two vari-
ants ALE-U (Uniform) and ALE-E (Exponential). These methods work by sacriﬁcing
accuracy, which requires (exactly) tracking a great deal of TCP state, and instead keep-
ing approximate state, which uses far less memory, but have a certain inherent amount
of error. Importantly, this loss of accuracy can be controlled by using more (or less)
Estimating TCP Latency Approximately with Passive Measurements
85
memory. These methods were implemented and compared against tcptrace [2], a
well established, ofﬂine analysis tool for TCP. We carried out a validation study using
two different traces obtained from CAIDA. On these traces, we show that ALE can
achieve accuracy very close to tcptrace, while using far less memory and requiring less
computation. In the best performing latency estimator, 99% of the reported latencies
are within 10.3 ms of the actual value and over 97% of the median ﬂow latencies re-
ported are within 10.2 ms of the actual medians; all while taking about one thirtieth of
the memory used by the baseline.
2 TCP Latency Estimation
TCP estimates RTT by matching ACKs against a set of data segments sent (but not yet
acknowledged). For example, suppose host A is sending data to host B on a path that
goes through M . At time t1, A sends a data segment with k bytes of data to B. This
segment contains a sequence number range [s, s+ k] (bytes are individually numbered).
After B processes this segment, it sends back an acknowledgement to A which explic-
itly indicates the next byte in the stream it expects to receive, i.e., s+k+1 (this is exactly
one more than the last sequence number in the packet sent by A), and this reaches A
at t2. Since the acknowledgement can be matched with the segment sent previously, A
estimates the RTT as t2 − t1. Now, node M can also perform a similar estimation by
matching data segments with acknowledgements (ACKs, in short) seen in ﬂight. The
RTT estimate for the path segment M ↔ B is ta−td, where td and ta are when the data
segment and the acknowledgement were observed at M . Note that there is not enough
information to estimate the RTT on the path segment A ↔ M ; this requires B sending
data to A and receiving ACKs back. To obtain accurate RTT estimates at M , for either
side of the path, we need to remember all the unacknowledged data segments seen in
one direction, and match them against ACKs coming back the other way. This makes
straightforward RTT estimation infeasible at nodes that handle a large ﬂow volume, or
at memory constrained embedded devices of the type used in home and small business
gateways. That being said, if we are willing to tolerate a small amount of error in the
RTT estimates or a few missed RTT estimates, we signiﬁcantly reduce the amount of
memory required.
In our approach, we exploit the following two observations: (i) storing the exact
timestamp associated with each TCP segments is overkill. It is sufﬁcient to remember
having seen it in a particular time interval, and (ii) we can avoid storing the sequence
number range and just store a single sentinel value instead. Following the ﬁrst obser-
vation, we can divide time into discrete intervals and just associate the segments with
particular intervals. Thus, with each interval, we now associate a (possibly) large set
of segments that arrived in that interval (speciﬁcally, the sequence number ranges and
ﬂowids). The second observation does away with having to store the sequence number
range and lets us store a single number for each unacknowledged segment. Speciﬁcally,
this number is just one larger than the end of the sequence range in the segment, i.e., the
number that is expected to be returned in the acknowledgement. We note that this is not
guaranteed to always be the case; the TCP speciﬁcation permits partial segments to be
acknowledged. However, this is not the norm and when it does happen, it is an indica-
tion of a performance bottleneck at the receiver. If we overlook this corner case, we can
86
S. Gangam et al.
simply record the expected acknowledgement number for each segment (this is exactly
one larger than the last sequence number in the segment) and match this against incom-
ing ACKs. By exploiting this “most likely behavior” in TCP, the problem of searching
through a number of ranges or intervals now becomes that of set membership queries
which can be done very efﬁciently with probabilistic data structures (such as Bloom
ﬁlters).
Approximate Latency Estimator (ALE). Looking into the recent past, we divide time
into ﬁxed size discrete intervals, [w0, w1], [w1, w2], . . . , [wn−1, wn], over a sliding win-
dow. Here, [w0, w1] is always the most recently elapsed interval, the sliding window
covers a span of W = w0 − wn seconds, and each interval is of length w = wi − wi+1
(we use interval and bucket interchangeably). This time discretization is shown in Fig. 1.
We denote by Bi the data structure currently associated with interval i. Apart from the
buckets associated with the sliding window, we use another bucket B to hold state for
the immediate present. At the end of every w seconds, we move B (to the left in the
ﬁgure) into the past and into the sliding window. The data structures Bi and B are
Counting Bloom Filters (CBF) [7], a variation that supports set member deletions.
tALE
w
wwww
w
4
1
0
5
BBBBB
0
4
1
2
3
3
2
t1
B
m
u
n
q
e
s
h
t
g
n
e
l
flowid3
0
0
1
data
t  +2w+w/2
ALE
wwww
4
1
2
3
w
5
hash(flowid, 41)
t2
w
0
tALE
K
C
A
flowid4
1
data
hash(flowid, 41)
B2
4w
A
A
C
B
B
D
A
B
B1
2w
A
C
A
B
C
D
E
B0
w
A
B
C
D
E
F
Elapsed
Time
w
2w
3w
4w
5w
6w
Fig. 1. Operation of the ALE Algorithm
Fig. 2. Buckets Being
Shifted and Merged in ALE-E
The TCP segment insertion operation in ALE records the ﬂow identiﬁer and the
expected sequence number into a bucket by hashing the concatenation of the two and
incrementing the appropriate counters (see [7] for details). Deletion proceeds the same
way, but with the counters decremented. Set membership reads the counters indexed by
the hash functions and reports ‘yes’ if all of them are non-zero, and ‘no’ otherwise.
We use the diagram in Fig. 1 to walk through an example. The upper half demon-
strates a just arrived TCP segment being recorded. The TCP data segment that arrives
at t1 is recorded into B (with sequence number 30 and containing 10 data bytes). After