In this section, we present three case studies of the ap-
plications of VROOM. We show that the separation be-
tween physical and logical, and the router migration ca-
pability enabled by VROOM, can greatly simplify existing
network-management tasks.
It can also provide network-
management solutions to other emerging challenges. We ex-
plain why the existing solutions (in the ﬁrst two examples)
are not satisfactory and outline the VROOM approach to
addressing the same problems.
3.1 Planned Maintenance
Planned maintenance is a hidden fact of life in every net-
work. However, the state-of-the-art practices are still unsat-
isfactory. For example, software upgrades today still require
rebooting the router and re-synchronizing routing protocol
states from neighbors (e.g., BGP routes), which can lead
to outages of 10-15 minutes [3]. Diﬀerent solutions have
been proposed to reduce the impact of planned maintenance
on network traﬃc, such as “costing out” the equipment in
advance. Another example is the RouterFarm approach of
removing the static binding between customers and access
routers to reduce service disruption time while performing
maintenance on access routers [3]. However, we argue that
neither solution is satisfactory, since maintenance of physical
routers still requires changes to the logical network topology,
and requires (often human interactive) reconﬁgurations and
routing protocol reconvergence. This usually implies more
conﬁguration errors [21] and increased network instability.
233We performed an analysis of planned-maintenance events
conducted in a Tier-1 ISP backbone over a one-week period.
Due to space limitations, we only mention the high-level
results that are pertinent to VROOM here. Our analysis
indicates that, among all the planned-maintenance events
that have undesirable network impact today (e.g., routing
protocol reconvergence or data-plane disruption), 70% could
be conducted without any network impact if VROOM were
used.
(This number assumes migration between routers
with control planes of like kind. With more sophisticated
migration strategies, e.g., where a “control-plane hypervi-
sor” allows migration between routers with diﬀerent con-
trol plane implementations, the number increases to 90%.)
These promising numbers result from the fact that most
planned-maintenance events were hardware related and, as
such, did not intend to make any longer-term changes to the
logical-layer conﬁgurations.
To perform planned maintenance tasks in a VROOM-
enabled network, network administrators can simply migrate
all the virtual routers running on a physical router to other
physical routers before doing maintenance and migrate them
back afterwards as needed, without ever needing to reconﬁg-
ure any routing protocols or worry about traﬃc disruption
or protocol reconvergence.
3.2 Service Deployment and Evolution
Deploying new services, like IPv6 or IPTV, is the life-
blood of any ISP. Yet, ISPs must exercise caution when de-
ploying these new services. First, they must ensure that
the new services do not adversely impact existing services.
Second, the necessary support systems need to be in place
before services can be properly supported.
(Support sys-
tems include conﬁguration management, service monitoring,
provisioning, and billing.) Hence, ISPs usually start with a
small trial running in a controlled environment on dedicated
equipment, supporting a few early-adopter customers. How-
ever, this leads to a “success disaster” when the service war-
rants wider deployment. The ISP wants to oﬀer seamless
service to its existing customers, and yet also restructure
their test network, or move the service onto a larger net-
work to serve a larger set of customers. This “trial system
success” dilemma is hard to resolve if the logical notion of a
“network node” remains bound to a speciﬁc physical router.
VROOM provides a simple solution by enabling network
operators to freely migrate virtual routers from the trial
system to the operational backbone. Rather than shutting
down the trial service, the ISP can continue supporting the
early-adopter customers while continuously growing the trial
system, attracting new customers, and eventually seamlessly
migrating the entire service to the operational network.
ISPs usually deploy such service-oriented routers as close
to their customers as possible, in order to avoid backhaul
traﬃc. However, as the services grow, the geographical dis-
tribution of customers may change over time. With VROOM,
ISPs can easily reallocate the routers to adapt to new cus-
tomer demands.
3.3 Power Savings
VROOM not only provides simple solutions to conven-
tional network-management tasks, but also enables new so-
lutions to emerging challenges such as power management.
It was reported that in 2000 the total power consumption of
the estimated 3.26 million routers in the U.S. was about 1.1
TWh (Tera-Watt hours) [28]. This number was expected to
grow to 1.9 to 2.4TWh in the year 2005 [28], which translates
into an annual cost of about 178-225 million dollars [25].
These numbers do not include the power consumption of
the required cooling systems.
Although designing energy-eﬃcient equipment is clearly
an important part of the solution [18], we believe that net-
work operators can also manage a network in a more power-
eﬃcient manner. Previous studies have reported that Inter-
net traﬃc has a consistent diurnal pattern caused by human
interactive network activities. However, today’s routers are
surprisingly power-insensitive to the traﬃc loads they are
handling—an idle router consumes over 90% of the power
it requires when working at maximum capacity [7]. We ar-
gue that, with VROOM, the variations in daily traﬃc vol-
ume can be exploited to reduce power consumption. Specif-
ically, the size of the physical network can be expanded and
shrunk according to traﬃc demand, by hibernating or pow-
ering down the routers that are not needed. The best way
to do this today would be to use the “cost-out/cost-in” ap-
proach, which inevitably introduces conﬁguration overhead
and performance disruptions due to protocol reconvergence.
VROOM provides a cleaner solution: as the network traf-
ﬁc volume decreases at night, virtual routers can be mi-
grated to a smaller set of physical routers and the unneeded
physical routers can be shut down or put into hibernation
to save power. When the traﬃc starts to increase, phys-
ical routers can be brought up again and virtual routers
can be migrated back accordingly. With VROOM, the IP-
layer topology stays intact during the migrations, so that
power savings do not come at the price of user traﬃc dis-
ruption, reconﬁguration overhead or protocol reconvergence.
Our analysis of data traﬃc volumes in a Tier-1 ISP back-
bone suggests that, even if only migrating virtual routers
within the same POP while keeping the same link utiliza-
tion rate, applying the above VROOM power management
approach could save 18%-25% of the power required to run
the routers in the network. As discussed in Section 7, al-
lowing migration across diﬀerent POPs could result in more
substantial power savings.
4. VROOM ARCHITECTURE
In this section, we present the VROOM architecture. We
ﬁrst describe the three building-blocks that make virtual
router migration possible—router virtualization, control and
data plane separation, and dynamic interface binding. We
then present the VROOM router migration process. Un-
like regular servers, modern routers typically have physically
separate control and data planes. Leveraging this unique
property, we introduce a data-plane hypervisor between the
control and data planes that enables virtual routers to mi-
grate across diﬀerent data-plane platforms. We describe in
detail the three migration techniques that minimize control-
plane downtime and eliminate data-plane disruption—data-
plane cloning, remote control plane, and double data planes.
4.1 Making Virtual Routers Migratable
Figure 2 shows the architecture of a VROOM router that
supports virtual router migration.
It has three important
features that make migration possible: router virtualization,
control and data plane separation, and dynamic interface
binding, all of which already exist in some form in today’s
high-end commercial routers.
234Physical Router A
Physical Router A
Physical Router A
Physical Router A
VR1
Physical Router B
Physical Router B
Physical Router B
Physical Router B
VR1
VR1
VR1
(a) Tunnel setup for 
redirecting routing messages
( t0 - t1 )
(b) Remote control plane with
redirection of routing messages
(c) Double data planes during
asynchronous link migration
(d) Remove old data plane 
and redirection tunnels
( t4 - t5 )
( t5 - t6 )
( t6 )
data trafﬁc (ﬂow 1)
data trafﬁc (ﬂow 2)
routing messages
redirection of routing messages
Figure 3: VROOM’s novel router migration mechanisms (the times at the bottom of the subﬁgures correspond
to those in Figure 4)
Physical Router
steps
1
2
3
3-1
3-2
t3
t4
4
5
t5
t6
time
t0 t1
t2
Virtual Router
Virtual Router
Virtual Router
Control 
Plane
Control 
Plane
Control 
Plane
Substrate
Data 
Plane
(1)
Data 
Plane
(2)
Data 
Plane
(1)
Data-plane hypervisor
Physical interface
(2)
Dynamic interface binding
Tunnel interface
Figure 2: The architecture of a VROOM router
Router Virtualization: A VROOM router partitions the
resources of a physical router to support multiple virtual
router instances. Each virtual router runs independently
with its own control plane (e.g., applications, conﬁgura-
tions, routing protocol instances and routing information
base (RIB)) and data plane (e.g., interfaces and forwarding
information base (FIB)). Such router virtualization support
is already available in some commercial routers [11, 20]. The
isolation between virtual routers makes it possible to migrate
one virtual router without aﬀecting the others.
Control and Data Plane Separation:
In a VROOM
router, the control and data planes run in separate environ-
ments. As shown in Figure 2, the control planes of virtual
routers are hosted in separate “containers” (or “virtual envi-
ronments”), while their data planes reside in the substrate,
where each data plane is kept in separate data structures
with its own state information, such as FIB entries and ac-
cess control lists (ACLs). Similar separation of control and
data planes already exists in today’s commercial routers,
with control plane running on the CPU(s) and main memory,
while the data plane runs on line cards that have their own
computing power (for packet forwarding) and memory (to
hold the FIBs). This separation allows VROOM to migrate
the control and data planes of a virtual router separately (as
discussed in Section 4.2.1 and 4.2.2).
Dynamic Interface Binding: To enable router migration
and link migration, a VROOM router should be able to dy-
old node
remote control plane
new node
old node
control 
plane
data 
plane
1
2
3
tunnel setup
router-image copy
3-1:
memory copy 
3-2:
4
5
data-plane cloning
asynchronous link migration
pre-copy
stall-and-copy (control plane downtime)
new node
double 
data 
planes
Figure 4: VROOM’s router migration process
namically set up and change the binding between a virtual
router’s FIB and its substrate interfaces (which can be phys-
ical or tunnel interfaces), as shown in Figure 2. Given the
existing interface binding mechanism in today’s routers that
maps interfaces with virtual routers, VROOM only requires
two simple extensions. First, after a virtual router is mi-
grated, this binding needs to be re-established dynamically
on the new physical router. This is essentially the same as
if this virtual router were just instantiated on the physical
router. Second, link migration in a packet-aware transport
network involves changing tunnel interfaces in the router, as
shown in Figure 1. In this case, the router substrate needs
to switch the binding from the old tunnel interface to the
new one on-the-ﬂy1.
4.2 Virtual Router Migration Process
Figures 3 and 4 illustrate the VROOM virtual router mi-
gration process. The ﬁrst step in the process involves es-
tablishing tunnels between the source physical router A and
destination physical router B of the migration (Figure 3(a)).
These tunnels allow the control plane to send and receive
routing messages after it is migrated (steps 2 and 3) but be-
fore link migration (step 5) completes. They also allow the
migrated control plane to keep its data plane on A up-to-
date (Figure 3(b)). Although the control plane will experi-
1In the case of a programmable transport network, link mi-
gration happens inside the transport network and is trans-
parent to the routers.
235ence a short period of downtime at the end of step 3 (memory
copy), the data plane continues working during the entire
migration process. In fact, after step 4 (data-plane cloning),
the data planes on both A and B can forward traﬃc si-
multaneously (Figure 3(c)). With these double data planes,
links can be migrated from A to B in an asynchronous fash-
ion (Figure 3(c) and (d)), after which the data plane on A
can be disabled (Figure 4). We now describe the migration
mechanisms in greater detail.
4.2.1 Control-Plane Migration
Two things need to be taken care of when migrating the
control plane: the router image, such as routing-protocol
binaries and network conﬁguration ﬁles, and the memory,
which includes the states of all the running processes. When
copying the router image and memory, it is desirable to min-