(cid:10)
(cid:8)(cid:8)(cid:8)(cid:8)
∂Fi(X)
∂Xj
T
=
=
=
∂
∂Xj
1
g2(X)
1
g2(X)
l=0 ezl/T
∂ezi(X)/T
ezi/T(cid:3)N−1
(cid:11)
(cid:9)
N−1(cid:5)
(cid:11)
N−1(cid:5)
(cid:9)
ezi/T
∂Xj
l=0
T
(cid:12)
(cid:10)
g(X) − e
zi(X)/T ∂g(X)
zl/T − N−1(cid:5)
(cid:12)
l=0
∂Xj
∂zl
∂Xj
(cid:10)
zl/T
e
∂zi
∂Xj
e
=
1
T
ezi/T
g2(X)
l=0
∂zi
∂Xj
− ∂zl
∂Xj
zl/T
e
The last equation yields that increasing the softmax tempera-
ture T for ﬁxed values of the logits z0, . . . , zN−1 will reduce
the absolute value of all components of model F ’s Jacobian
matrix because (i) these components are inversely proportional
to temperature T , and (ii) logits are divided by temperature T
before being exponentiated.
This simple analysis shows how using high temperature
systematically reduces the model sensitivity to small variations
of its inputs when defensive distillation is performed at training
time. However, at test time, the temperature is decreased back
to T = 1 in order to make predictions on unseen inputs. Our
intuition is that this does not affect the model’s sensitivity as
weights learned during training will not be modiﬁed by this
change of temperature, and decreasing temperature only makes
the class probability vector more discrete, without changing
the relative ordering of classes. In a way, the smaller sensitivity
imposed by using a high temperature is encoded in the weights
during training and is thus still observed at test time. While this
explanation matches both our intuition and the experiments
detailed later in section V, further formal analysis is needed.
We plan to pursue this in future work.
C. Distillation and the Generalization Capabilities of DNNs
We now provide elements of learning theory to analytically
understand the impact of distillation on generalization capa-
bilities. We formalize our intuition that models beneﬁt from
soft labels. Our motivation stems from the fact that not only
do probability vectors F (X) encode model F ’s knowledge
regarding the correct class of X, but
it also encodes the
knowledge of how classes are likely, relatively to each other.
Recall our example of handwritten digit recognition. Sup-
pose we are given a sample X of some hand-written 7 but
that the writing is so bad that the 7 looks like a 1. Assume a
model F assigns probability F7(X) = 0.6 on 7 and probability
F1(X) = 0.4 on 1, when given sample X as an input. This
indicates that 7s and 1s look similar and intuitively allows a
model to learn the structural similarity between the two digits.
In contrast, a hard label leads the model to believe that X is a
7, which can be misleading since the sample is poorly written.
This example illustrate the need for algorithms not ﬁtting
too tightly to particular samples of 7s, which in turn prevent
models from overﬁtting and offer better generalizations.
To make this intuition more precise, we resort
to the
recent breakthrough in computational learning theory on the
connection between learnability and stability. Let us ﬁrst
present some elements of stable learning theory to facilitate
our discussion. Shalev-Schwartz et al. [34] proved that learn-
ability is equivalent to the existence of a learning rule that
is simultaneously an asymptotic empirical risk minimizer and
stable. More precisely, let (Z = X × Y,H, (cid:5)) be a learning
problem where X is the input space, Y is the output space,
H is the hypothesis space, and (cid:5) is an instance loss function
that maps a pair (w, z) ∈ H× Z to a positive real loss. Given
a training set S = {zi : i ∈ [n]}, we deﬁne the empirical loss
of a hypothesis w as LS(w) = 1
i∈[n] (cid:5)(w, zi). We denote
n
∗
S = minw∈H LS(w). We are
the minimal empirical risk as L
ready to present the following two deﬁnitions:
Deﬁnition 1 (Asymptotic Empirical Risk Minimizer) A
learning rule A is an asymptotic empirical risk minimizer, if
there is a rate function1 ε(n) such that for every training set
S of size n,
(cid:3)
LS(A(S)) − L
S ≤ ε(n).
∗
Deﬁnition 2 (Stability) We say that a learning rule A is ε(n)
(cid:4) that only differ in
stable if for every two training sets S, S
one training item, and for every z ∈ Z,
|(cid:5)(A(S), z) − (cid:5)(A(S
(cid:4)
), z)| ≤ ε(n)
where h = A(S) is the output of A on training set S, and
(cid:5)(A(S), z) = (cid:5)(h, z) denotes the loss of h on z.
to progress in our discussion is the
An interesting result
following Theorem mentioned previously and proved in [34].
Theorem 1 If there is a learning rule A that is both an
asymptotic empirical risk minimizer and stable, then A gener-
alizes, which means that the generalization error LD(A(S))
∗
D = minh∈H LD(h) with some rate ε(n)
converges to L
independent of any data generating distribution D.
We now link this theorem back to our discussion. We observe
that, by appropriately setting the temperature T , it follows
(cid:4) only differing by one training item,
that for any datasets S, S
the new generated training sets (X, F S(X)) and (X, F S(cid:2)
(X))
satisfy a very strong stability condition. This in turn means
that for any X ∈ X , F S(X) and F S(cid:2)
(X) are statistically
close. Using this observation, one can note that defensive dis-
tillation training satisﬁes the stability condition deﬁned above.
1A function that non-increasingly vanishes to 0 as n grows.
590590
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:13:13 UTC from IEEE Xplore.  Restrictions apply. 
Layer Type
Relu Convolutional
Relu Convolutional
Max Pooling
Relu Convolutional
Relu Convolutional
Max Pooling
Relu Fully Connect.
Relu Fully Connect.
Softmax
MNIST
Architecture
32 ﬁlters (3x3)
32 ﬁlters (3x3)
2x2
64 ﬁlters (3x3)
64 ﬁlters (3x3)
2x2
200 units
200 units
10 units
CIFAR10
Architecture
64 ﬁlters (3x3)
64 ﬁlters (3x3)
2x2
128 ﬁlters (3x3)
128 ﬁlters (3x3)
2x2
256 units
256 units
10 units
TABLE I: Overview of Architectures: both architectures are
based on a succession of 9 layers. However,
the MNIST
architecture uses less units in each layers than the CIFAR10
architecture because its input is composed of less features.
Parameter
Learning Rate
Momentum
Decay Delay
Dropout Rate (Fully Con-
nected Layers)
Batch Size
Epochs
MNIST
Architecture
0.1
0.5
-
0.5
128
50
CIFAR10
Architecture
0.01 (decay 0.5)
0.9 (decay 0.5)
10 epochs
0.5
128
50
TABLE II: Overview of Training Parameters: the CIFAR10
architecture training was slower than the MNIST architecture
and uses parameter decay to ensure model convergence.
Moreover, we deduce from the objective function of defensive
distillation that the approach minimizes the empirical risk.
Combining these two results together with Theorem 1 allows
us to conclude that the distilled model generalizes well.
We conclude this discussion by noting that we did not
strictly prove that the distilled model generalizes better than a
model trained without defensive distillation. This is right and
indeed this property is difﬁcult to prove when dealing with
DNNs because of the non-convexity properties of optimization
problems solved during training. To deal with this lack of
convexity, approximations are made to train DNN architectures
and model optimality cannot be guaranteed. To the best of our
knowledge, it is difﬁcult to argue the learnability of DNNs
in the ﬁrst place, and no good learnability results are known.
However, we do believe that our argument provides the readers
with an intuition of why distillation may help generalization.
V. EVALUATION
This section empirically evaluates defensive distillation,
using two DNN network architectures. The central asked
questions and results of this emprical study include:
• Q: Does defensive distillation improve resilience against
adversarial samples while retaining classiﬁcation accu-
racy? (see Section V-B) - Result: Distillation reduces the
success rate of adversarial crafting from 95.89% to 0.45%
on our ﬁrst DNN and dataset, and from 87.89% to 5.11%
on a second DNN and dataset. Distillation has negligible
or non existent degradation in model classiﬁcation ac-
curacy in these settings. Indeed the accuracy variability
between models trained without distillation and with
distillation is smaller than 1.37% for both DNNs.
Fig. 6: Set of legitimate samples: these samples were ex-
tracted from each of the 10 classes of the MNIST handwritten
digit dataset (top) and CIFAR10 image dataset (bottom).
• Q: Does defensive distillation reduce DNN sensitivity to
inputs? (see Section V-C) Result: Defensive distillation
reduces DNN sensitivity to input perturbations, where
experiments show that performing distillation at high
temperatures can lead to decreases in the amplitude of
adversarial gradients by factors up to 1030.
• Q: Does defensive distillation lead to more robust DNNs?
(see SectionV-D) Result: Defensive distillation impacts
the average minimum percentage of input features to be
perturbed to achieve adversarial targets (i.e., robustness).
In our DNNs, distillation increases robustness by 790%
for the ﬁrst DNN and 556% for the second DNN: on our
ﬁrst network the metric increases from 1.55% to 14.08%
of the input features, in the second network the metric
increases from 0.39% to 2.57%.
A. Overview of the Experimental Setup
Dataset Description - All of the experiments described in
this section are performed on two canonical machine learning
datasets: the MNIST [20] and CIFAR10 [21] datasets. The
MNIST dataset is a collection of 70, 000 black and white
images of handwritten digits, where each pixel is encoded as a
real number between 0 and 1. The samples are split between
a training set of 60, 000 samples and a test set of 10, 000.
The classiﬁcation goal is to determine the digit written. The
classes therefore range from 0 to 9. The CIFAR10 dataset is a
collection of 60, 000 color images. Each pixel is encoded by
3 color components, which after preprocessing have values in
[−2.22, 2.62] for the test set. The samples are split between
a training set of 50, 000 samples and a test set of 10, 000
samples. The images are to be classiﬁed in one of the 10
mutually exclusive classes: airplane, automobile, bird, cat,
deer, dog, frog, horse, ship, and truck. Some representative
samples from each dataset are shown in Figure 6.
591591
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:13:13 UTC from IEEE Xplore.  Restrictions apply. 
Architecture Characteristics - We implement two deep
neural network architectures whose speciﬁcities are described
in Table I and training hyper-parameters included in Table II:
the ﬁrst architecture is a 9 layer architecture trained on the
MNIST dataset, and the second architecture is a 9 layer
architecture trained on the CIFAR10 dataset. The architectures
are based on convolutional neural networks, which have been
widely studied in the literature. We use momentum and
parameter decay to ensure model convergence, and dropout to
prevent overﬁtting. Our DNN performance is consistent with
DNNs that have evaluated these datasets before.
The MNIST architecture is constructed using 2 convolu-
tional layers with 32 ﬁlters followed by a max pooling layer,
2 convolutional
layers with 64 ﬁlters followed by a max
pooling layer, 2 fully connected layers with 200 rectiﬁed linear
units, and a softmax layer for classiﬁcation in 10 classes. The
experimental DNN is trained on batches of 128 samples with
a learning rate of η = 0.1 for 50 epochs. The resulting DNN
achieves a 99.51% correct classiﬁcation rate on the data set,
which is comparable to state-of-the-art DNN accuracy.
The CIFAR10 architecture is a succession of 2 convolutional
layers with 64 ﬁlters followed by a max pooling layer, 2 con-
volutional layers with 128 ﬁlters followed by a max pooling
layer, 2 fully connected layers with 256 rectiﬁed linear units,
and a softmax layer for classiﬁcation. When trained on batches
of 128 samples for the CIFAR10 dataset with a learning rate
of η = 0.01 (decay of 0.95 every 10 epochs), a momentum of
0.9 (decay of 0.5 every 10 epochs) for 50 epochs, a dropout
rate of 0.5, the architecture achieves a 80.95% accuracy on
the CIFAR10 test set, which is comparable to state-of-the-art
performance for unaugmented datasets.
To train and use DNNs, we use Theano [35], which is
designed to simplify large-scale scientiﬁc computing, and
Lasagne [36], which simpliﬁes the design and implementation
of deep neural networks using computing capabilities offered
by Theano. This setup allows us to efﬁciently implement
network training as well as the computation of gradients
needed to craft adversarial samples. We conﬁgure Theano to
make computations with ﬂoat32 precision, because they can
then be accelerated using graphics processing. Indeed, we use
machines equipped with Nvidia Tesla K5200 GPUs.
∗
Adversarial Crafting - We implement adversarial sample
crafting as detailed in [7]. The adversarial goal is to alter any
sample X originally classiﬁed in a source class F (X) by DNN
∗ classiﬁed by DNN
F so as to have the perturbed sample X
) (cid:3)= F (X). To achieve this
F in a distinct target class F (X
goal, the attacker ﬁrst computes the Jacobian of the neural