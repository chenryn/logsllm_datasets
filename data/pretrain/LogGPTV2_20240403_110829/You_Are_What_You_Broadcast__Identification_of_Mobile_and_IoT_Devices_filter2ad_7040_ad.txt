device is expected to receive three labels:
{Manufacturer, Type, Model}
L+ = ∑
(u,v,k)∈Dpri
log
1
1 + e−corr(pk
u,pk
v)
,
(14)
We refer to the literature [23, 24, 33, 43] to deﬁne 34 types
of devices. Examples of popular types of devices are:
where L+ is the loss of augmented correlation, (pk
v) denote
the output from views u and v for sample k, and Dpri denotes
the priori dataset which includes all labeled benign samples.
Note that we do not have any known malicious sample in the
initial dataset. We also assume that benign samples always
signiﬁcantly outnumber malicious samples in the dataset.
u, pk
Assumption 2 denotes that malicious devices that attempt to
fabricate identities often cause inconsistencies in the BC/MC
packets. In OWL, we quantitatively model the degree of view
inconsistency, and use it for malicious device detection:
phone computer pad router camera smart-plug
smart-switch virtual-machine game-console tv
lightbulb printer kettle watersensor watch
Based on the availability and trustworthiness of labels, our
samples are categorized into four sets: (1) samples with vali-
dated labels (i.e., the ground truth data); (2) samples labeled
in the semi-automatic process; (3) samples with auxiliary
(SSDP) features; and (4) samples without any label.
Ground Truth Data. A portion of our data was collected
in controlled environment, such as our own lab network or
62    29th USENIX Security Symposium
USENIX Association
home network. We obtained the true labels of such devices.
We were also able to verify the manufacturer/type/model of
some display items in electronic stores. In total, we have 423
devices with validated labels in our ground truth dataset. Note
that each device in this category receives all three labels.
Semi-automatic Device Labeling. The majority of the
samples were collected from uncontrolled environments. To
create labels, we design a semi-automatic labeling process:
(1) for an unlabeled device, we manually examine human-
interpretable text in the sniffed packets (ﬁelds listed in Table
2). If the information appears to be benign and consistent,
we label the device accordingly. Note that we may not learn
all three labels for a device. (2) When patterns are observed
from a speciﬁc manufacturer/type/model of devices, we
create labeling rules in the form of {Condition => Label}.
For
instance, {MAC:D-link; HostName:DCS-930LB =>
(D-link, camera, dlink_camera_dcs-930lb)} states
that when MAC preﬁx indicates D-link and DHCP Option-12
(HostName) contains string “DCS-930LB”, the device
labeled as D-Link camera DCS-930LB. (3) The rules are
used to process all unlabeled samples. All automatically
generated labels are veriﬁed by the creators of the rules.
Rules may be reﬁned and re-applied during this process. We
then move to (1) for the next unlabeled device. (4) All labels,
manually or automatically created, are further reviewed and
conﬁrmed by another member in the team.
Eventually, we annotated 4,064 devices to the ﬁnest granu-
larity: {manufacturer, type, model}, among which 410 distinct
device models were identiﬁed (the ground truth data is not
included here). In addition, 6,519 devices were annotated with
{manufacturer, type}, while the exact models were unknown.
15,895 devices were labeled with {manufacturer} only. They
are called the annotated dataset. 4,871 devices were left with-
out any label, i.e., the labelless dataset. Last, 78 devices were
set aside as supplementary testing data (to be discussed).
Among the three labels, manufacturer is the easiest to iden-
tify and reveals the least amount of information. It does not tell
administrators how the access policies could be conﬁgured, or
tell other users if the device could be suspicious. Meanwhile,
sometimes the MAC preﬁx only tells the manufacturer of
the network components, instead of the manufacturer of the
device itself. For instance, we have seen several different de-
vices carrying TP-Link’s mac preﬁx. Therefore, some devices
are left unlabeled although the MAC preﬁxes are known.
The Sanitized Dataset. We used human-interpretable textual
content in network packets for device annotation. The text
content is also processed as pseudo natural language features
and used for device identiﬁcation in OWL. Meanwhile, we
also like to answer this question: “How much does OWL rely
on human interpretable textual features to identify devices?”
For this purpose, we sanitized all the annotated samples by
removing all identiﬁers (labels were preserved for evaluation
purposes). That is, we removed all the keywords that are used
in {Condition => Label} rules, including all the MAC pre-
ﬁxes, to create this Sanitized Dataset. We veriﬁed that neither
the labeling rules nor the human annotators were able to dis-
tinguish any device (at any granularity) in the sanitized data.
Samples with Auxiliary Features and Supplementary
Testing Data. As introduced in Section 3.3, we followed the
device description URLs in SSDP notify to obtain auxiliary
features for devices in our organizational network. We col-
lected meaningful device descriptions for 180 devices. They
are utilized in two ways: (1) for 102 samples that are anno-
tated to {type} and {model} levels in the labeling process,
we employ auxiliary features to validate the labels. They are
included in the annotated dataset. (2) For samples that are
labelless or only labeled with {manufacturer}, we deliberately
set them aside and only used them in testing. We call this set
of 78 devices the Supplementary Testing Dataset.
Trivial Features and Unidentiﬁable Devices. Feature sets
that carry identical values across many device types are called
trivial features. Devices with only trivial features are uniden-
tiﬁable in theory. We identify such devices in four steps: (1)
apriori is used to ﬁnd feature frequencies for each device
type. (2) Devices with informative protocols (mDNS, SSDP,
DHCP, DHCPv6, LLMNR, NBNS and BROWSER) are eliminated. (3)
In the remaining devices, feature sets that appeared in more
than N device types are called trivial features. (4) Devices
that contain only trivial features are marked as unidentiﬁable.
5.2 Experiment Results
To test the performance of the MvWDL model presented in
Sec 4, we evaluate its performance from three aspects: (1)
the accuracy and coverage of classiﬁcation in comparing with
other methods, (2) performance on sanitized data (extreme
condition), and (3) the speed of device identiﬁcation.
Metrics. The performance of device identiﬁcation is evalu-
ated by three metrics: (1) the coverage (C) denotes the fraction
of all devices that OWL (or another approach) could generate
a label for; (2) the accuracy (A) is the fraction of labeled
devices that are correctly labeled; and (3) the overall identiﬁ-
cation rate (OIR) denotes the faction of all devices that are
correctly labeled. They are formally deﬁned as:
C = |{labeled devices}|/|{all devices}|
A =
|{correctly labeled devices}|
(18)
(19)
|{labeled devices}|
|{correctly labeled devices}|
|{all devices}|
OIR =
= C× A (20)
We compare the performance of OWL with state-of-art
device identiﬁcation mechanisms, which could be roughly
categorized into ﬁngerprint-based and rule-based approaches.
Fingerprint-based approaches extract features from network
trafﬁc and then employ supervised learning for device identi-
ﬁcation. Among this category of approaches, WDMTI [64]
produces good performance on MC packets (DHCP). Rule-
USENIX Association
29th USENIX Security Symposium    63
based approaches extract text keywords from payload of unen-
crypted network trafﬁc to create {keywords->device} rules
for device identiﬁcation. ARE [24] is the state-of-art approach
in this category of solutions. We implemented WDMTI, ex-
tracted its features from our dataset, and trained/tested it in
the same way as OWL. We also implemented ARE to extract
rules from our training data and applied them on testing data.
Note that we employed ARE on different protocols from [24].
Performance Evaluation on Ground Truth Data. We com-
pare the performance of WDMTI, ARE and OWL on the
ground truth data. We compare their accuracy, coverage and
OIR at three different granularity, from coarse to ﬁne: {man-
ufacturer}, {manufacturer, type}, and {manufacturer, type,
model}. We perform a 10-fold cross validation and demon-
strate the results in Figure 3 (a). The results show that: (1)
OWL provides the best overall performance (OIR) at all gran-
ularity levels. Its coverage is consistently the highest, as OWL
could always extract features from the network trafﬁc and
predict a label. At ﬁner granularity, OWL signiﬁcantly outper-
forms both ARE and WDMTI in OIR. (2) ARE has the best
accuracy but limited coverage, especially at ﬁne granularity
levels. It is able to correctly identify the manufacturer of more
than 80% of the devices, since MAC preﬁxes are used for this
label and they are mostly available. For type and model, the
informative textual terms are not always available in network
trafﬁc, hence, ARE is unable to identify the majority of de-
vices. (3) WDMTI solely depends on features extracted from
DHCP packets, hence, its coverage is always limited.
Performance Comparison on Annotated Data. We evalu-
ate all three approaches on the annotated dataset, as shown in
Figure 3 (b). First, since we were able to annotate the manufac-
turer of all the samples in this dataset, they all contain enough
features for OWL and ARE to identify the manufacturer, i.e.,
they both achieve C = 100% on {manufacturer}. Not all de-
vices contain enough information for ARE to identify their
type and model. For the same reason, we were unable to an-
notate type and model these samples in the dataset. However,
OWL could still utilize non-human-interpretable features to
classify these devices. Hence, OWL’s coverage is signiﬁcantly
higher than ARE and WDMTI for the two ﬁne-grained labels.
Next, we evaluate the accuracy of all three approaches
based on the annotations. We do not have {type} and {model}
annotations on more than 50% of the samples. Although OWL
is able to estimate these labels for unannotated samples, we
cannot tell whether such estimations are correct. That is, for
the {manufacturer, type} and {manufacturer, type, model}
granularity, coverage (C) is evaluated on all samples in the
annotated dataset, while accuracy (A) is only evaluated on
partial data–samples with {type}, and {model} annotations.
Therefore, we mark accuracy and OIR with A* and OIR* in
the ﬁgure, where OIR∗ = C× A∗. From Figure 3 (b), we can
see that OWL achieves similar accuracy to ARE. We can also
expect OWL to generate better OIR in these two categories,
due to its signiﬁcant advantages in coverage.
At the {manufacturer, type} granularity, OIR* was calcu-
lated from 10,583 samples that have the {type} label. The
other 15,895 samples did not contain enough information for
human annotators to recognize their types. Therefore, A∗ and
OIR∗ represent the upper-bound of the actual A and OIR. To
estimate the lower-bound of A, we sanitized the 10,583 anno-
tated samples by removing all textual features and MACs –
now they provide even less information than the 15,985 unla-
beled samples. OWL achieved 88.4% accuracy on sanitized
data. A reasonable estimation is that OIR ∈ [0.884,0.975].
From another angle, our true groundtruth dataset has very
similar device/protocol distributions with testing data. OWL
achieved 90.98% OIR on groundtruth data. Therefore, OIR
on annotated data is expected to be similar: OIR ≈ 0.9098.
Performance on Sanitized Data. We generated a sanitized
dataset to test OWL’s performance in extreme conditions,
where all human-interpretable contents are removed from raw
data. OWL’s coverage, accuracy and OIR on the sanitized
dataset are shown in Figure 3 (c). In particular, category 1
({manufacturer}) was evaluated against all 26,478 sanitized
samples. Category 2 was evaluated on 10,583 samples with
{manufacturer, type} labels; while category 3 was evaluated
against 4,064 samples with all three labels. OWL achieved
100% coverage in the later two categories, since the basic
protocol features still existed after data sanitization. However,
after removing MAC preﬁxes, some samples in category 1
cannot be identiﬁed since no meaningful feature was left.
OWL’s accuracy is still high, in the range of [0.75,0.88].
Performance Comparison with Other Classiﬁers. We
have explained the rationale of choosing multi-view learn-
ing in Section 4.1. Meanwhile, the choice of speciﬁc classiﬁer
and fusion strategy is mostly empirical: (1) we have enough
features and samples to support deep learning, which demon-
strated superior performance in ML literature; (2) we need
a late-fusion component to measure inconsistencies among
views for anomaly detection; (3) we need to handle the differ-
ent distinguishability of different protocols against different
device types. Now we experimentally compare MvWDL with
other popular classiﬁcation algorithms. The Gradient Boost-
ing Decision Tree (GBDT) [26] is among the best non-NN
classiﬁers for categorical features. The fastText [31] is a state-
of-art word embedding and classiﬁcation library by Facebook.
We also employ a generic deep neural network (DNN). We
use 10-fold cross validation on annotated data, which has
signiﬁcantly more samples than other datasets. Figure 4 (a)
shows the average accuracy of each classiﬁer over all labels.
MvWDL achieves the best performance, while DNN is a close
second. fastText was the least accurate, which may be caused
by the smaller training set than fastText’s expectations.
Device Detection Speed. Another important metric is the
time for OWL to recognize all the devices in a WiFi net-
work. While it only takes mini-seconds for a trained MvWDL
model to classify a new device, packets/features come to OWL
slowly in real world settings. We tested the real-time perfor-
64    29th USENIX Security Symposium
USENIX Association
Figure 3: Experiment results: (a) Performance comparison of ARE [24], WDMTI [64] and OWL on ground truth data (X-axis: C: coverage;
A: accuracy; OIR: overall identiﬁcation rate). (b) Performance comparison on annotated data. * Note that accuracy and OIR was only evaluated
on partial data in the later two categories (please see detailed discussions in Section 5). (c) Performance of OWL on sanitized data.
evaluation. Classifying them into “unknown” will decrease
OWL’s coverage but increase its accuracy. They are also very
likely to be detected by OWL’s malicious device identiﬁer.
6 Malicious Device Identiﬁcation
As discussed in Section 4, when a device demonstrates incon-
sistent features in different protocols, it could be malicious.
OWL makes the ﬁrst attempt to utilize view inconsistency to
identify abnormal devices, as denoted in Eq. 15. We apply
the algorithm on all the samples in our dataset, and manually
examine the devices that trigger the alarm. We present three
cases of suspicious devices identiﬁed in our experiments, as
well as the case of (hidden) camera detection using OWL.
“Spoofed” AppleTVs. A group of 31 devices demonstrated
similar abnormal behaviors that triggered the alarm. The mDNS
view classiﬁed all these devices into AppleTV with strong
conﬁdence; however, none of the other views predicted these
devices as AppleTV, and their conﬁdence levels were all rela-
tively high. We further manually examined these devices.
First, the devices were labeled as various models of TVs or
stream casting receivers. Some samples were from the ground
truth dataset so that their labels were physically checked
with the device. Others were manually veriﬁed in the semi-
automatic labeling process, especially, their MAC preﬁxes
were consistent with the labeled manufacturers. Hence, the
labels, as listed in Table 4, appear to be consistent with the
actual device model. However, we further scrutinized the orig-