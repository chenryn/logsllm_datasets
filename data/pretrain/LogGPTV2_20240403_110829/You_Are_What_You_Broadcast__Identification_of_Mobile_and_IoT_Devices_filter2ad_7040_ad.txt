### Device Labeling and Anomaly Detection

#### Device Labeling
Each device is expected to receive three labels: `{Manufacturer, Type, Model}`. The loss function for augmented correlation, \( L^+ \), is defined as:
\[
L^+ = \sum_{(u,v,k) \in D_{\text{pri}}} \log \left( \frac{1}{1 + e^{-\text{corr}(p_u^k, p_v^k)}} \right)
\]
where \( p_u^k \) and \( p_v^k \) denote the outputs from views \( u \) and \( v \) for sample \( k \), and \( D_{\text{pri}} \) denotes the priori dataset, which includes all labeled benign samples. Note that we do not have any known malicious samples in the initial dataset, and we assume that benign samples significantly outnumber malicious samples.

#### Device Types
We refer to the literature [23, 24, 33, 43] to define 34 types of devices. Examples of popular types include:
- Phone
- Computer
- Pad
- Router
- Camera
- Smart-plug
- Smart-switch
- Virtual-machine
- Game-console
- TV
- Lightbulb
- Printer
- Kettle
- Water sensor
- Watch

#### Sample Categorization
Based on the availability and trustworthiness of labels, our samples are categorized into four sets:
1. **Samples with validated labels (Ground Truth Data)**: These are collected in controlled environments such as lab networks or home networks, where true labels are obtained. We have 423 devices with validated labels.
2. **Semi-automatic Device Labeling**: Most samples are collected from uncontrolled environments. A semi-automatic process is used to create labels:
   - **Manual Examination**: For an unlabeled device, human-interpretable text in sniffed packets is examined. If the information appears benign and consistent, the device is labeled accordingly.
   - **Rule Creation**: When patterns are observed, labeling rules in the form of `{Condition => Label}` are created. For example, `{MAC:D-link; HostName:DCS-930LB => (D-link, camera, dlink_camera_dcs-930lb)}`.
   - **Rule Application and Refinement**: Rules are applied to all unlabeled samples, and automatically generated labels are verified. This process is iterative.
3. **Samples with Auxiliary (SSDP) Features**: For 180 devices, auxiliary features from SSDP notify are used to validate or generate labels.
4. **Samples without Any Label**: Devices that cannot be labeled due to insufficient information.

#### Ground Truth Data
A portion of our data was collected in controlled environments, such as our own lab network or home network. We obtained the true labels of these devices, including their manufacturer, type, and model. In total, we have 423 devices with validated labels in our ground truth dataset.

#### Semi-automatic Device Labeling
The majority of the samples were collected from uncontrolled environments. To create labels, we designed a semi-automatic process:
1. **Manual Examination**: For an unlabeled device, we manually examine human-interpretable text in the sniffed packets. If the information appears benign and consistent, we label the device accordingly.
2. **Rule Creation**: When patterns are observed from a specific manufacturer, type, or model, we create labeling rules. For example, `{MAC:D-link; HostName:DCS-930LB => (D-link, camera, dlink_camera_dcs-930lb)}`.
3. **Rule Application and Refinement**: The rules are used to process all unlabeled samples. Automatically generated labels are verified by the creators of the rules, and the process is iterative.
4. **Review and Confirmation**: All labels, whether manually or automatically created, are further reviewed and confirmed by another team member.

Eventually, we annotated 4,064 devices to the finest granularity: `{manufacturer, type, model}`, identifying 410 distinct device models. Additionally, 6,519 devices were annotated with `{manufacturer, type}`, and 15,895 devices were labeled with `{manufacturer}` only. 4,871 devices were left without any label, and 78 devices were set aside as supplementary testing data.

#### Sanitized Dataset
To evaluate the reliance of OWL on human-interpretable textual features, we sanitized all the annotated samples by removing all identifiers. This Sanitized Dataset was used to test OWL's performance in extreme conditions.

#### Trivial Features and Unidentifiable Devices
Trivial features are those that carry identical values across many device types. Devices with only trivial features are unidentifiable. We identify such devices in four steps:
1. **Feature Frequency Analysis**: Use apriori to find feature frequencies for each device type.
2. **Elimination of Informative Protocols**: Remove devices with informative protocols like mDNS, SSDP, DHCP, etc.
3. **Identification of Trivial Features**: Feature sets that appear in more than N device types are called trivial features.
4. **Marking Unidentifiable Devices**: Devices that contain only trivial features are marked as unidentifiable.

### Experiment Results

#### Performance Metrics
The performance of device identification is evaluated using three metrics:
- **Coverage (C)**: The fraction of all devices that OWL (or another approach) could generate a label for.
- **Accuracy (A)**: The fraction of labeled devices that are correctly labeled.
- **Overall Identification Rate (OIR)**: The fraction of all devices that are correctly labeled. Formally defined as:
  \[
  C = \frac{|{\text{labeled devices}}|}{|{\text{all devices}}|}
  \]
  \[
  A = \frac{|{\text{correctly labeled devices}}|}{|{\text{labeled devices}}|}
  \]
  \[
  OIR = C \times A
  \]

#### Comparison with State-of-the-Art Methods
We compare the performance of OWL with state-of-the-art device identification mechanisms, categorized into fingerprint-based and rule-based approaches:
- **Fingerprint-based Approaches**: Extract features from network traffic and use supervised learning for device identification. WDMTI [64] performs well on MC packets (DHCP).
- **Rule-based Approaches**: Extract text keywords from payload of unencrypted network traffic to create `{keywords->device}` rules. ARE [24] is a state-of-the-art approach in this category.

#### Performance Evaluation
- **Ground Truth Data**: OWL provides the best overall performance (OIR) at all granularity levels, with consistently high coverage. ARE has the best accuracy but limited coverage, especially at fine granularity levels. WDMTI depends solely on DHCP packets, limiting its coverage.
- **Annotated Data**: OWL achieves 100% coverage for `{manufacturer}` and significantly higher coverage for finer-grained labels. OWL’s accuracy is similar to ARE, and it is expected to generate better OIR due to its advantages in coverage.
- **Sanitized Data**: OWL’s coverage, accuracy, and OIR on the sanitized dataset are still high, ranging from 0.75 to 0.88.

#### Performance Comparison with Other Classifiers
We experimentally compare MvWDL with other popular classification algorithms:
- **Gradient Boosting Decision Tree (GBDT)**: Among the best non-NN classifiers for categorical features.
- **fastText**: A state-of-the-art word embedding and classification library by Facebook.
- **Deep Neural Network (DNN)**: A generic deep neural network.

MvWDL achieves the best performance, followed closely by DNN. fastText was the least accurate, possibly due to the smaller training set.

#### Device Detection Speed
The time for OWL to recognize all devices in a WiFi network is also evaluated. While it takes milliseconds for a trained MvWDL model to classify a new device, real-world packet/feature arrival can be slow.

### Malicious Device Identification
Devices demonstrating inconsistent features in different protocols are flagged as potentially malicious. OWL uses view inconsistency to identify abnormal devices. Three cases of suspicious devices identified in our experiments include "spoofed" AppleTVs, where mDNS classified them as AppleTV, but other views did not, leading to manual verification and confirmation of the actual device models.