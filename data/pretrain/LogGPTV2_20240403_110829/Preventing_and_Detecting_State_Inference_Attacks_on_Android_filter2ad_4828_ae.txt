apps (which acts as our “testing set”), and its performance (§IX-D);
5) we compare our work with the most closely related work,
LeaveMeAlone [38] (§IX-E).
least 1Hz) and that a malware never stops this behavior once it is
started (i.e., polling is performed for a “sustained” amount of time).
During the years, malware evolved and changed frequently
the set of vulnerable APIs and techniques used to identify the
starting of a sensitive application to target with a phishing attack.
The techniques used by a malware highly depend on the API
level the device of the victim is targeting. For example, if the
device targets an Android lower than 5.0, the malware will
adopt a combination of both getRunningTasks(int) and
getRunningAppProcesses(). Instead, if the device targets
a version between 5.0 and 6.0, then the malware can still rely on
the information exposed by the proc filesystem (/proc).
However, as discussed in Section II-C, Google fixed all the
known components leading to a leak of sensitive information like
the state of an application. Hence, the only available attack vector
for the malware is to rely on the APIs protected by the well known
BIND_ACCESSIBILITY_SERVICE permission (a11y) [12].
As it is possible to see, some sophisticated malware like Bankosy,
Cepsohord, and MysteryBot started moving from the
a11y towards exploiting vulnerable APIs protected by the
PACKAGE_USAGE_STATS [20], [35], [5]. This transition might
also be forced by the fact that Google is going to remove all the
applications using the BIND_ACCESSIBILITY_SERVICE
permission for anything except helping disabled users [8].
A. Peculiarity of Phishing Applications
To verify the validity of our hypothesis, we first perform an
empirical study on malicious applications. For this study, we selected
a dataset of 50 samples from all the families of Android malware
that 1) were discovered in the last four years and 2) are known to
mount state inference attacks. In particular, we analyzed samples
and variants from: Anubis, LokiBot, ExoBot, BankBot,
RedAlert, MisteryBot, BianLian, Asacub,
and
Gustuff [15], [28], [32], [21], [17], [20], [29], [16], [31]. For
each family, we analyzed both “malware-only apps” — apps
containing only the malicious code — as well as “repackaged apps.”
Analyzing sophisticated malware is not always an easy task: we
encountered different situations that made the (automatic) dynamic
analysis very challenging. In these specific cases, for example,
we found apps performing integrity checks on the device or anti-
hooking techniques, as well as starting the malicious behavior only
after some time or after certain actions, probably to avoid Google
Bouncer analysis. Moreover, many samples tried to communicate
first with a remote C&C server: since most of these servers were
“unreachable” at the time of test, the malware did not start any activity.
To overcome these difficulties, we decided to manually analyze
the samples looking for the code in charge of performing the state
inference attack. For each family, we extracted the methods used to
perform this task. Our analysis highlighted different techniques used
to mount this attack. To perform polling, malware authors are using
different mechanisms like registering a repeated-delayed task with
postDelayed() or AlarmManager. Another
technique
relies on anonymous Thread or IntentService to invoke
the vulnerable API every second. Lastly, an even more aggressive
technique consists in executing all the “monitoring logic” inside a
while loop, without any delay between invocations. It is possible
to model and define a common behavior shared among all the fam-
ilies we analyzed: we found that all malware poll with a maximum
delay that spans from 600ms to one second (i.e., a frequency of at
Moreover,
[35] highlighted how the adoption of
the
PACKAGE_USAGE_STATS permission
amongst malicious
applications published on the official Google PlayStore is growing.
This is an important result showing that, even if Google is not going
to fix the vulnerable APIs we identified in Section VII, they are
used by malware developers in real-world attacks [20], [35].
The PACKAGE_USAGE_STATS permission,
like BIND_-
ACCESSIBILITY_SERVICE, can only be granted through the
Settings application: this means that a malware cannot ask at
runtime this permission. However, as for the attacks based on a11y,
the malware can directly display the Settings application and lure the
user through social engineering to grant the permission. As presented
in [5], malware uses social engineering while masquerading as
Google Chrome by mimicking the application’s icon and name. This
technique tricks the victim into thinking she is granting the PACK-
AGE_USAGE_STATS permission to the Google Chrome app,
while instead, she is granting the permission to the malicious app.
B. Peculiarity of Benign Applications
As the next step, we characterize whether and how benign apps
perform polling-like behavior, and whether there are some features
that can be used to distinguish them from malicious attempts.
To this end, we built a dataset of 10,108 benign apps. To select
a representative dataset, we consulted AndroidRank [3] to find
popular apps, which we then crawled from the Play Store. The
resulting dataset is constituted as follows: 9,066 “top apps” with
at least 50M installations, while the remaining 1,042 were chosen
randomly from apps with a number of installations ranging from
10M to 50M. From this dataset, we built two different datasets: a
“training set” of 2,042 apps (roughly 20% of the dataset), and a
“testing set” with the remaining 80% of it.
The rationale behind this choice is the following: we first inves-
tigate how benign apps perform polling by only considering apps
within the training set. Based on the insights of this step, we then 1)
11
enumerate a number of observations that can be used to distinguish
between benign and malicious samples and we use them to build a
detection system; 2) we evaluate the performance of the proposed
system (in terms of miss detections) by analyzing the apps in the
testing set — which are not considered during the design/training
phase. We believe this two-step approach helps addressing concerns
related to how our evaluation would generalize to a bigger dataset.
Testing Environment. To study the runtime behavior of benign
apps, we instrumented the Android OS (Android 9 running on a Pixel
3A) to log all binder communications and filesystem activities for a
given application. This log contains information such as the service
and the API invoked by the app, and the correspondent timestamp.
Analysis System. To identify a “polling-like behavior,” we tuned
the analysis to flag all the syscalls and APIs invoked at a rate of
at least once every two seconds (i.e., 0.5Hz), for at least 60 seconds.
We believe these thresholds are a “safe assumption,” since 1) the
threshold frequency is twice as low as the minimum frequency
rate at which malware performs polling activities (i.e., 1Hz) and
the phishing attack would necessarily incur a delay of 2 seconds,
making it visible to the victim; and 2) the malware does not stop
polling activities after it has started it (see Section IX-A).
A very important aspect of the proposed system is that it does
not look for polling by just considering a single API, but it considers
the overall number of invocations. That is, instead of monitoring
whether a specific API A is invoked more frequently than once
every two seconds, the system monitors if the app has cumulatively
invoked any API more frequently than our threshold. This design
choice introduces the concern of false positives (which we fully
address in the remaining of this section), but prevents an attacker
to bypass our detection by simply alternating the invocation of two
(or more) different APIs, thus lowering the per-API frequency.
We now report the results of this analysis. We also note that this
analysis system, configured with the thresholds we mentioned, is
able to detect all the malware samples in our dataset.
Results and Observations. We now discuss the results and the
observations after the execution of each of the 2,042 apps of the
training set within our instrumented environment. We executed each
app for five minutes. We post-processed the execution traces on our
analysis system to identify if also benign apps perform polling, and,
if so, on which component and at which frequency rate. From the
results of the analysis, we draw the following two observations:
1) Benign apps do perform polling. We found a significant number
of apps that were flagged by our system. More interestingly, we
analyzed the traces to identify which APIs were being flagged and
we identify frequent patterns belonging to the following categories:
a) Graphical User Interface: to draw the content of the app’s view,
the system relies on polling to design the various component forming
the UI of the app; b) Audio and Video: similar to the GUI, multimedia
components also rely on polling. In fact, to reproduce the audio and
video stream, the multimedia services needs to refresh, for each
frame, the video and audio buffer. c) DRM: when playing rights-
protected content, the DRM service first decodes and then forwards
to the multimedia service each chunk of the file to play. d) System
Services Internals: operations that are performed each time a system
service is used by an app. For example, when an application interacts
with a system service that operates on global data, a new Thread
is started and multiple acquireWakeLock and release-
WakeLock APIs are invoked to handle tasks synchronization. In
all these cases polling is performed by system services “on behalf
of the app.” That is, even though the polling logic is implemented
in the system service, it is still related to the context of the app since
the service uses the app’s identity for the subsequent invocations.
We investigated each of these behaviors in detail, and we found
that none of these APIs can lead to abuse or state inference attacks,
and we thus believe that they can easily and safely whitelisted.
Table V (in Appendix) provides a very detailed list of our insights.
We also note that the four groups above capture polling behavior for
all apps in our training dataset except for six of them: these six apps
were found to be App Lockers, which we discuss in Section IX-D.
2) Bootstrap phase. Another interesting observation is that we
have noticed how apps often show a spike of activity during their
“bootstrap time.” This, intuitively, makes sense: when the app is
started, it needs to perform a number of one-off setup operations,
e.g., querying system information, setting up in-memory data
structures, requesting permissions. However, we also noted how
the level of activity (measured as the frequency of API invocations)
decreases as the application transitions from its “bootstrap” to its “at
rest” phase. We note how this characteristic is profoundly different
from state inference attack malware behavior: once the polling
behavior is started, it is never terminated.
C. Proposed Detection System
Based on the results of the previous empirical study, we
implemented a system for the detection of polling behaviors on top
of Android 9, by modifying the execTransact method of the Binder
class, which is invoked any time a system service receives a request.
This design choice prevents malicious applications to circumvent our
detection system, since our modifications affect only the (privileged)
server side of the Binder subsystem. Our system is setup to raise
alerts for apps performing API invocations at a rate of at least x
invocations per y seconds (with x = 1 and y = 2, i.e., a threshold
minimum frequency of 0.5Hz), for at least z seconds (with z =60, as
previously discussed). Our system is also setup to not consider API
invocations during a “bootstrap phase” of a given app, where with
“bootstrap phase” we indicate the first k seconds from the app’s start
up. For our system, we empirically selected k =90, but, for the sake
of completeness, the evaluation section discusses how the accuracy
of the system changes when k varies (between 0 and 5*60 seconds),
and we show that this threshold affects the results in a minimal way.
Implementation-wise, the system creates a circular buffer for
each running uid in the system. The length of each circular buffer
depends on the number of invocations allowed in a given timeslot (x).
We start the monitoring phase after the bootstrap time k, and we do
not consider APIs that have been whitelisted (i.e., the “benign” and
not-possible-to-abuse APIs discussed above belonging to one of the
four categories). For each service invocation, our system stores the
current timestamp in the circular buffer associated to the appropriate
uid. When the circular buffer is full, the system checks whether the
elapsed time from the first invocation in the buffer is lower than y
seconds. Due to the properties of circular buffers, this is the case if
and only if we have recorded x services invocations in less than y
seconds. This means that the caller app has exceeded the invocations
rate that we are interested in detecting. If the threshold is exceeded,
our system enters a so-called “alert mode” and stores the time at
which the polling behavior started in an additional variable (one for
each uid). When handling the following invocations of the service
while in alert mode, our system checks whether the polling behavior
12
is sustained for at least z seconds, and it does so by comparing the
content of this additional variable with the current time. If the differ-
ence is greater than z, our system raises an exception (preventing the
service’s request to be completed) and it raises a warning to the user.
Note that if subsequent invocations do not meet the minimum
threshold for polling, the system leaves the “alert mode” and it resets
the internal state. We note how this system allows for the detection
malicious apps performing state inference attacks polling on a single
API, but, more importantly, it would also detect situations for which
the malware uses multiple (different) vulnerable APIs to infer the
state of the target application. This is possible due to using a “single
bucket” for all APIs invoked by the same app (identified by their
Linux uid).
To err on the safe side and to avoid false negatives, for our
defense mechanism we set a very conservative detection threshold to
half the frequency of all real-world malware samples. This allows us
to detect all current malware samples analyzed in Section IX-A, and
even if these malware samples would cut their polling frequency in
half, our system would still detect them. In principle, a malware that
reduces even more its polling frequency might bypass our detection
system. However, to mount a successful phishing attack, timing is
a fundamental component. Thus, lowering down even more than
half the polling frequency, would make the malware and the attack
ineffective, since there would be a very visible delay between the
launch of the legitimate app and the spoofed one. For example, a
situation where the user clicks on the legitimate banking app icon,
and she starts to interact with the application, and only then, say
after two seconds, the malware displays its spoofed banking app UI
asking again for credentials, would certainly raise some warnings
to the victim and the attack would be noticed.
D. Evaluation
We evaluated our on-device detection system on the testing
set, composed by 8,066 apps. We stress that we did not access
and/or inspect these apps before having finished developing the
entire system. In other words, we believe this represents a realistic
and fair evaluation on how our system would fair in practice. Our
results show that the system would flag only 30 apps as potentially
problematic, which represents only the 0.37% of the entire dataset.
We note that this result was obtained by setting a threshold for
k = 90 to identify the bootstrap phase. To evaluate the impact
of this threshold over the results, we varied it from zero seconds
(i.e., we start monitoring the application as soon as it starts) to 240
seconds (i.e., we start monitoring the app 4 minutes after it starts):
the number of false positives is not significantly affected — it varies
from 39 to 25. Figure 4 shows a graph depicting how this number
changes while varying the bootstrap phase threshold. We also note
that this threshold does not affect the detection of malicious apps,
since all malware samples never stop polling after they have started.
We now present a detailed analysis of the apps detected as
problematic by our system. For this step, we consider out “worst
case” — we consider the configuration that raised the highest
number of false positives (k =0, 39 false positives). The goal is to
analyze the polling behavior exposed by these apps and determine
the nature of their behavior. We identified three groups of apps with
similar patterns, which we discuss next.
The first group is composed by 10 apps polling only one of the
vulnerable APIs we identified, getProcessMemoryInfo. In
these apps, the code performing the polling belongs to a third-party
Fig. 4: The following plot shows the impact of the bootstrap phase
in relation to the number of false positives identified by our system.
As it is possible to see, when the bootstrap time is none, the number
reaches 39 and it decreases as the application execution time
increases. The lowest number of false positives is reached if the
bootstrap phase is greater or equals to 210 seconds.
library for crash analytics that constantly traces the usage of the
app’s memory. However, this API is invoked to only monitors its
own memory. We note that Google has now fixed this API, and it
would allow an app to only monitor its own memory — making
the usage of this API safely whitelistable.
The second group is composed of 10 apps, which embed ads
libraries that aggressively poll several APIs to monitor the status
of the network, probably to collect information related to nearby
networks, with the goal of tracking the user [1]. We believe that users
would be pleased to suppress this privacy-invasive functionality.
The third group of 9 benign apps is constituted by “App Lockers.”
These apps work by monitoring which app the user is interacting
with, and by “locking” the device if the user is interacting with an
app she should not interact with (e.g., the Settings app). These apps
were initially popular as a way to protect the user phone, but they
became less popular with time, and they are now considered “grey
area.” Google also introduced additional security features that make
these apps of dubious utility. With that being said, these apps are
problematic for our system as they do rely on polling (in this case,
the queryEvents API), making this behavior indistinguishable
from malware. Our system, as is, would block these apps — and
rightfully so. If a user truly wants to use these apps, she can of
course whitelist them. But given their declining popularity over
time, we argue this is acceptable.
The last group is formed by 10 apps whose polling behavior is