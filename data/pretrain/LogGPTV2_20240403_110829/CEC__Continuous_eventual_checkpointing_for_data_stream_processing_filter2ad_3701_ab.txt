need  to  be  persisted  while  input  queues  can  be  rebuilt  by 
fetching tuples from the upstream operators’ output queues. 
Checkpoints  are  preserved  in  stable  store  (another  node’s 
main memory or a storage device). 
The  recovery  procedure  for  a  streaming  node  has  two 
parts.  First,  one  has  to  decide  what  the  most  recent 
checkpoint is; then ask upstream nodes to replay tuples from 
that point forward. If a node is asked to replay tuples that it 
has to reconstruct (perhaps because it also has failed and lost 
recent  state),  then  upstream  nodes  have  to  go  through  the 
recovery  process  themselves.  Note  that  the  operator’s 
internal  state  (open  windows  before  failure)  has  to  be 
checkpointed in-sync with its output queue. 
B.  Continuous eventual checkpoints 
CEC departs from the standard implementation of CRB 
by  breaking  the  overall  operator  state  into  independent 
components corresponding to the different open windows wi 
and performing checkpoints of each of them asynchronously 
at  different  times  ti.  Figure  2  depicts  an  example  of  an 
aggregate operator with N open windows at time tc. The time 
is defined by the timestamp of the last tuple that entered the 
operator and affected its state by either updating an existing 
window, or opening a new window, or closing a window and 
emitting an output tuple. 
Window  checkpoints  are  expressed  as  control  tuples 
containing the partial state S accumulated by the operator for 
a  given  window  up  to  time  t.  Each  window  checkpoint 
additionally contains the following parameters: 
1.  Type of checkpoint (Gcheck or Gopen): 
-  Gcheck is the type of checkpoint produced for an existing 
window. For a Gcheck checkpoint, S is equal to the partial 
result accumulated by the window up to time t. 
-  Gopen is the type of checkpoint produced when a window 
opens. For a Gopen checkpoint, S is equal to the state of 
the  window  after  taking  into  account  the  tuple  that 
opened it. Tuples of type Gopen are necessary to ensure 
we have a guaranteed known state for a window at time 
tc to roll back to in case a Gcheck has not been produced 
for it yet. 
2.  Total number of open windows (N) at checkpoint time t. 
Figure 2 depicts the time evolution of an operator from 
time  tk  to  tc  with  checkpoints  for  windows  wk,  wi,  wj,  wc. 
Standard  data-carrying  tuples  produced  when  a  window 
closes are abbreviated as R (result). Gcheck
k and Rk refer to a 
checkpoint and a data-carrying tuple for window wk. 
Figure 2. Continuous eventual checkpointing (CEC). 
in 
Window  checkpoint  tuples  are  logged  at  the  operator’s 
output queue along with regular output tuples in timestamp 
order. Thus Gcheck
k is emitted and stored in the log before the 
subsequent  Rk  produced  when  wk  closes  (Figure  2).  If  a 
persistence mechanism is used to write the output queue to 
stable  storage  it  should  preserve  timestamp  order  for  all 
tuples produced by the operator. 
In CEC the operator checkpoint is not a single, cohesive 
entity  as 
traditional  CRB  schemes.  Instead,  CEC 
maintains an eventual checkpoint (EC) at time tc as a set of 
window checkpoints W = {(wi, ti): ti ≤ tc} that can be used to 
bring  the  operator  to  a  consistent  state,  i.e.,  a  state  the 
operator went through in the actual execution, at time tc. This 
state includes all windows that were open at time tc and for 
which,  their  most  recent  persisted  “footprint”  (i.e.,  partial 
checkpoint) was written to stable storage prior to tc. Once we 
determine the oldest “footprint” of any window that is open 
at tc (and call its timestamp T, T = mini ( ti ) for all ti ≤ tc) 
CEC loads onto the operator the state of all open windows 
wi. This state corresponds to different times ti and is thus not 
immediately consistent. To achieve consistency, the operator 
must  contact  its  upstream  node and request  replay  of  input 
tuples with timestamps t > T. Since T is the earliest among 
all ti's whereas the state of all (except one) windows reflects 
a later time, the operator will unavoidably see tuples that it 
has  already  seen  in  the  past.  To  ensure  that  we  reach  the 
correct  pre-failure  state,  the  operator  must  ensure  that 
window wi ignores tuples with timestamps t < ti. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:28:57 UTC from IEEE Xplore.  Restrictions apply. 
147Constructing an EC 
the 
A  key  challenge  when  constructing  an  eventual 
checkpoint  from  the  persisted  output  queue  is  finding  out 
how  far  to  roll  back  into  the  log  looking  for  window 
checkpoints (Gopen, Gcheck). We solve this problem by storing 
along with each checkpoint the number N of open windows 
at  that  time,  which  turns  out  to  be  the  number  of  different 
window “footprints” we need to look for when rolling back 
the log. For example Figure 2 shows that the checkpoint of 
wc at time tc records the total number of open windows N at 
that time. It is important to ensure that the entire checkpoint 
record (S, G{check|open}, N, and t) is atomically written to stable 
storage.  We  ensure  this  by  using  a  fingerprint  to  detect 
partial I/O errors and in that case discard the record. 
During  failure-free  operation,  CEC  increases  N  when  a 
window  opens  and  decreases  it  when  a  window  closes.  By 
CEC rules, at time tc all open windows must have produced a 
Gopen  and  possibly  Gcheck  checkpoints  in  the  output  queue. 
During  reconstruction  it  is  important  to  skip Gopen or  Gcheck 
tuples for windows that are known to have closed prior to tc. 
If a window closed prior to tc (as for example is the case for 
window wk in Figure 2), its R tuple will be encountered prior 
to any checkpoints for that window and therefore it will be 
excluded from the EC. 
Extent of an EC 
Our system evolves the eventual checkpoint continuously 
over time as new control and regular output tuples are being 
produced. Recall that for an eventual checkpoint W = {(wi, 
ti): ti ≤ tc}, T is the earliest timestamp of any open window 
among the wi in W. We define as the extent of W (henceforth 
referred  to  simply  as  the  extent)  to  be  the  set  of  tuples  (of 
any  type,  Gopen,  Gcheck,  or  R)  in  the  output  queue  that 
recovery needs to go through when rolling-back to reach the 
tuple with timestamp T. 
to  complete 
the  construction 
The extent is an important parameter in constructing the 
EC  since 
is 
proportional to its size. Intuitively the rate of growth of the 
extent is inversely proportional to the rate of progression of 
T. T may be lagging behind in the past when the window it 
corresponds  may  have  had  no  Gcheck  tuple  produced  for  it 
since  the  last  Gcheck  or  Gopen  tuple  produced  for  the  same 
window. Besides increasing EC reconstruction time, a large 
T means that we will need to replay a large amount of tuples 
from  upstream  queues  to  bring  the  operator  to  a  fully 
consistent state. 
Controlling the extent 
Windows can remain open for a long time mainly due to 
two  reasons:  (1)  the  stream  tuple  distribution  favors  some 
windows  over  others;  (2)  the  window  specification  allows 
for  a  large  accumulation  of  tuples  before  closing  and 
computing  a  result,  either  via  the  tuple  count  or  time 
parameters.  We  refer  to  windows  that  are  staying  open  for 
long  periods  of  time  (due  to  (1),  (2)  or  both)  as  slow  and 
those  that  close  and  re-open  frequently  as  fast.  Note  that 
depending  on  the  characteristics  of  the  incoming  stream  a 
time 
fast  window  may  turn  into  a  slow  one  over  time  and  vice 
versa. 
The existence of slow windows in the operator state is a 
key factor leading to a growing extent size. Our goal in CEC 
is to produce Gcheck checkpoints more aggressively –as far as 
performance  and  recovery-time  objectives  allow—  for  the 
slowest windows in order to advance T and reduce the size 
of the extent. 
CEC benefits 
A benefit of performing individual window checkpoints 
is  that  we  avoid  freezing  the  operator  for  long  checkpoint-
capture time intervals typical of traditional CRB approaches. 
CEC  still  needs  to  devote  time  to  individual  window 
checkpoints,  but  this  time  is  smaller,  spread  over  a  longer 
time  period,  and  adjustable 
to  application  needs. 
Additionally we do not require any operating system support 
for copy-on-write or other  memory-protection  schemes  that 
are typically used in traditional implementations of CRB, nor 
incur the overhead of protection violation exceptions in these 
schemes. 
CEC enables a performance vs. recovery time tradeoff by 
parameterizing  how  frequently  it  produces  checkpoints  as 
well as the set of windows the checkpointing effort focuses 
on.  As  described,  CEC  focuses  on  checkpointing  of  slow 
windows. However the degree of intensity at which CEC is 
producing  checkpoints  may  impact  performance.  The  CEC 
performance  vs.  recovery  time  tradeoff  is  thus  enabled 
through explicit control of the following parameters: (1) how 
much  time  to  devote  (out  of  the  overall  execution  time)  to 
checkpointing;  (2)  when  is  checkpointing  necessary  for the 
slowest  windows.  Section  III  provides  details  into  the 
specific choices we have made in our implementation when 
tuning those parameters. 
Finally,  another  benefit  of  CEC  is  that  by  integrating 
checkpoints  into  standard  operator  output  it  can  leverage  a 
single  persistence  architecture  and  stable  storage  structure 
for both operator and output-queue states. Note that although 
control  tuples  mix  with  data-carrying  tuples  in  the  stable 
storage  abstraction,  they  do  not  complicate  processing  in 
downstream SPEs as the SPEs are able to recognize them in 
their  input  streams  and  disregard  them  during  operator 
processing. 
CEC challenges 
looking  for  all  window  checkpoints 
Constructing an EC requires reading the output queue log 
sequentially 
that 
comprise  the  EC.  This  is  a  sequential  process  that  can  be 
sped  up  by  reading  large  chunks  (currently  256KB)  of  the 
log into memory to avoid the penalty of small I/Os. A factor 
that  affects  performance  has  to  do  with  the  way  tuples  are 
grouped in the output stream. The degree of grouping tuples 
into a structure called a stream event [8] before storing them 
in the distributed file system is proportional to the operator’s 
output  rate:  operators  with  low  output  rate  are  expected  to 
feature  a  smaller  degree  of  grouping  (in  some  cases,  each 
tuple  occupies  a  separate  stream-event).  In  such  cases  a 
larger  number  of  stream  events  to  process  in  the  recovery 
path will lead to a longer time to reconstruct the EC. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:28:57 UTC from IEEE Xplore.  Restrictions apply. 
148Gopen  checkpoints  required  by  CEC  on  every  window 
opening  are  expected  to  increase  the  amount  of  tuples 
produced by the operator per window from one to two (Gopen 
at open and R on close), a fact that becomes more important 
when  the  application  features  a  large  number  of  fast 
windows.  However  this  is  not  expected  to  be  an  issue  in 
practice  in  most  real  deployments  as  stateful  operators 
typically  have  low  output  tuple  rates  and  consequently  are 
much  less  I/O-intensive  compared  to  stateless  operators  (a 
filter or a map) that produce an output tuple for each input 
tuple that they receive. 
III. 
IMPLEMENTATION 
In  this  section  we  describe  the  implementation  of  CEC 
using  the  aggregate  operator  as  a  case  study.  We  use  the 
Borealis  [8]  implementation  of  the  operator  as  a  reference 
but  our  principles  are  more  general  and  can  apply  equally 
well  to  other  continuous-query  data  stream  processing 
systems. 
First  we  describe  the  state  maintained  by  the  streaming 
operator. This state includes (a) the set of all currently open 
windows;  (b)  the  state  of  each  window:  open  or  closed; 
accumulated  state  so  far;  and  two  timestamps  τ1  and  τ2,  τ1 
corresponding to the input tuple that created the window and 
τ2  corresponding  to  the  last  checkpoint  (Gopen  or  Gcheck) 
produced  for  the  window;  and  (c)  an  ordering  of  all  open 
windows  by  their  τ2  timestamp.  The  objective  of  this 
ordering is to always be able to start from the window with 
the  least-recent  checkpoint  when  checkpointing  with  the 
objective to reduce the size of the extent. 
CEC  requires  minimal  changes  to  the  standard  Borealis 
tuple header to store its own information. It uses the existing 
type attribute to indicate tuple type (Gopen, Gcheck or R). The 
existing  timestamp  attribute  is  used  to  store  the  timestamp 
of  the  tuple  that  was  last  processed  by  the  operator  (tnow). 
Finally, CEC introduces a count field to indicate the number 
of  active  windows  at 
time.  These 
modifications work for both stateful and stateless operators 
since  the  later  can  be  thought  of  as  a  special  case  of  the 
former with window count 0. 
tuple  production 
foreground  path 
In  terms  of  execution  paths  we  distinguish  between 
foreground,  background,  and  recovery.  All  processing  is 
performed  in  the  main  thread  of  the  operator  because  a 
separate thread would result in unnecessary complexity and 