title:Proof-of-Learning: Definitions and Practice
author:Hengrui Jia and
Mohammad Yaghini and
Christopher A. Choquette-Choo and
Natalie Dullerud and
Anvith Thudi and
Varun Chandrasekaran and
Nicolas Papernot
2021 IEEE Symposium on Security and Privacy (SP)
Proof-of-Learning: Deﬁnitions and Practice
Hengrui Jia*§, Mohammad Yaghini*§, Christopher A. Choquette-Choo+§, Natalie Dullerud+§, Anvith Thudi+§,
Varun Chandrasekaran†, Nicolas Papernot§
University of Toronto and Vector Institute§, University of Wisconsin-Madison†
6
0
1
0
0
.
1
2
0
2
.
1
0
0
0
4
P
S
/
9
0
1
1
.
0
1
:
I
O
D
|
E
E
E
I
1
2
0
2
©
0
0
.
1
3
$
/
1
2
/
5
-
4
3
9
8
-
1
8
2
7
-
1
-
8
7
9
|
)
P
S
(
y
c
a
v
i
r
P
d
n
a
y
t
i
r
u
c
e
S
n
o
m
u
i
s
o
p
m
y
S
E
E
E
I
1
2
0
2
Abstract—Training machine learning (ML) models typically
involves expensive iterative optimization. Once the model’s ﬁnal
parameters are released, there is currently no mechanism for the
entity which trained the model to prove that these parameters
were indeed the result of this optimization procedure. Such a
mechanism would support security of ML applications in several
ways. For instance, it would simplify ownership resolution when
multiple parties contest ownership of a speciﬁc model. It would
also facilitate the distributed training across untrusted workers
where Byzantine workers might otherwise mount a denial-of-
service by returning incorrect model updates.
In this paper, we remediate this problem by introducing
the concept of proof-of-learning in ML. Inspired by research
on both proof-of-work and veriﬁed computations, we observe
how a seminal training algorithm, stochastic gradient descent,
accumulates secret information due to its stochasticity. This
produces a natural construction for a proof-of-learning which
demonstrates that a party has expended the compute require to
obtain a set of model parameters correctly. In particular, our
analyses and experiments show that an adversary seeking to
illegitimately manufacture a proof-of-learning needs to perform
at least as much work than is needed for gradient descent itself.
We also instantiate a concrete proof-of-learning mechanism
in both of the scenarios described above. In model ownership
resolution, it protects the intellectual property of models released
publicly. In distributed training, it preserves availability of the
training procedure. Our empirical evaluation validates that our
proof-of-learning mechanism is robust to variance induced by
the hardware (e.g., ML accelerators) and software stacks.
I. INTRODUCTION
Training machine learning (ML) models is computationally
and memory intensive [1], often requiring hardware accelera-
tion. GPUs [2], TPUs [3], and FPGAs [4] are used to ensure
efﬁcient training. In the status quo, there is no way for an
entity to prove that they have performed the work required
to train a model. This would be of immense utility in at
least two settings. First, once a model is released publicly
intentionally or unintentionally (i.e., it is stolen), the model’s
owner may be interested in proving that they trained the model
as a means to resolve and claim ownership—for instance,
resolving claims related to model stealing attacks [5]–[9].
Second, a model owner may seek to distribute the training [10]
across multiple workers (e.g., virtual machines in a cloud) and
requires guarantees of integrity of the computation performed
by these workers. This would defend against some of the
parties being corrupted accidentally (e.g., due to hardware
failure) or maliciously (e.g., by an adversary which relies on
Byzantine workers to perform denial-of-service attacks [11]).
*Joint lead authors; +joint secondary authors.
In our work, we design a strategy that will allow a party–the
prover–to generate a proof that will allow another party–the
veriﬁer–to verify the correctness of the computation performed
during training. In the case of ML, this translates to the prover
generating a proof to support its claims that it has performed
a speciﬁc set of computations required to obtain a set of
model parameters. In the model stealing scenario, the prover
would be the model owner, and the veriﬁer would be a legal
entity resolving ownership disputes. In the distributed learning
scenario, the prover would be one of the workers, and the ver-
iﬁer the model owner. We name our strategy proof-of-learning
(PoL). Unlike prior efforts related to proofs-of-work [12], [13],
our approach is not aimed at making computation expensive
so as to inhibit denial-of-service attacks.
When developing our concept for PoL, we consider only
the training phase and not the inference phase; the cost of
inference is generally much lower, and there already exist
mechanisms to ensure the integrity of ML inference performed
by another party [14]. In our design, we wish to design a proof
strategy that adds limited overhead to the already computation-
ally intensive process of training. Deep models do not have
closed form solutions, and use variants of gradient descent
as the de-facto choice for training. Additionally, stochastic
gradient-based optimization methods used in deep learning,
like stochastic gradient descent (SGD), update model param-
eters iteratively over long sequences by computing unbiased
estimates of the true gradient [15]. Naturally, this sequence
represents the work performed by the prover in training their
model. We propose that PoL for ML should demonstrate two
properties: (a) the prover performed the necessary optimization
(expending computational resources) to train an ML model,
and (b) these steps were computed correctly, i.e., that we have
integrity of computation.
There has been extensive research in proof systems related
to other applications. Veriﬁed computations relates to settings
where outcomes of outsourced computation (such as in client-
server architectures) can be veriﬁed [16]–[20]. Theoretical
advances and efﬁcient hardware design have enabled both
smaller proofs and more efﬁcient veriﬁcation strategies [21],
[22]. The simplest scheme, however, involves duplicated exe-
cution i.e., re-executing the computation performed to verify
the validity of the proof.
Following this intuition, we introduce in a general approach
to obtain a PoL which enables verifying the computation
performed during training (see § V). We then instantiate a
concrete PoL which utilizes the difﬁculty to invert gradient
descent (see § V). The added advantage here is that operations
involving gradient descent are computed as part of the learning
procedure, and can be used for generating the proof as well. In
© 2021, Hengrui Jia. Under license to IEEE.
DOI 10.1109/SP40001.2021.00106
1039
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:16:15 UTC from IEEE Xplore.  Restrictions apply. 
our work, the guarantees sought by the prover are analogous
to those in the veriﬁable computations literature: given (i) a
(random) distribution to draw the initial model weights from,
(ii) the model’s ﬁnal weights, and (iii) a dataset, the prover
must provide a sequence of batch indices and intermediate
model updates that, starting from the initialization, one can
replicate the path to the ﬁnal model weights. This allows a
veriﬁer to recompute any of the steps of gradient descent of
their choosing to conﬁrm the validity of the sequence provided.
This in turn demonstrates that the prover has indeed performed
the computation required to obtain the ﬁnal parameter values.
However, veriﬁcation also requires the expensive process of
gradient computation to verify the steps taken, as our proposal
is based on re-execution. To make veriﬁcation more computa-
tionally affordable, we introduce a heuristic for the veriﬁer to
select only a subset of the pairs of model parameter states to
verify. This allows the veriﬁer to trade-off the conﬁdence of
veriﬁcation with its cost: if the veriﬁer randomly picks a set
of parameter pairs, then with sufﬁciently many choices, it can
be conﬁdent of the proof’s validity.
There are many sequences that can be obtained from a given
start state to a given ﬁnal state (owing to the various sources
of stochasticity involved in training). However, through our
theoretical analysis in § VII, we observe that obtaining these
training (i.e., moving forward
states through conventional
through the sequence) is more efﬁcient than inverting gradient
descent (i.e., moving backwards through the sequence). Our
analysis shows that inverting gradient descent takes at least
as much work as training. Thus, it is hard for an adversary to
spoof a PoL using such a strategy.
In summary, our contributions are the following:
• In § IV, we formalize the desiderata for a concept of
proof-of-learning, the threat model we operate in, and
introduce a formal protocol between the different actors
involved in generating a PoL.
• In § V, we introduce a general mechanism for PoL based
on the observation that stochastic gradient descent utilized
during training is difﬁcult to invert.
• We analytically prove the correctness of our mechanism
in § VI, and then verify experimentally that it can be
implemented despite hardware and software stochasticity.
• We analyze the security of our proposed mechanism in
§ VII through an analysis of entropy growth in gradient
descent, and evaluate possible spooﬁng strategies an
adversary may rely on to pass veriﬁcation.
• Our code is open-sourced at github.com/cleverhans-lab/
Proof-of-Learning.
II. RELATED WORK
A. Proof-of-Work in Cryptography
The concept of proof-of-work (or PoW), where one party
proves to another that it has expended computational resources
towards a computation result, was ﬁrst introduced by Dwork
and Naor [12]. The concept was motivated as a defense from
denial-of-service (DoS) attacks against email and network
providers. This was the main motivation for many later PoW
functions as well, in which PoW functions force the adversary
to expend signiﬁcant computational resources, whether CPU or
memory resources, in order to request access to the service. We
revisit this motivation in § IV, but with the perspective of ML
systems in mind. The term PoW itself was later introduced by
Jakobsson and Juels [13]. A key property of this formulation is
that PoW relies largely on the existence of one-way functions
popular in cryptography to establish an asymmetry between
the party doing the computation and the party verifying that
the computation was performed.
In standard two-round PoW protocols, the prover receives a
query including a cryptographic puzzle, frequently involving
or indirectly based on a hashed randomly generated value
or structure computed by the veriﬁer. The prover solves the
computational puzzle and returns the value, which the veriﬁer
either accepts as a solution to the problem or rejects. Generally,
the process of solving the computational problem by the prover
depends, directly or indirectly, on computation of a pre-image
of a hashed random number generated and computed by the
veriﬁer, a known hard and expensive problem.
Dwork and Naor [12] enumerated several PoW strategies
predicated on integer square root module large prime problem:
e.g., the Fiat Shamir signature scheme and the Ong-Schnorr-
Shamir signature scheme. Since then, many methods have
been proposed for PoW functions. These initial PoW functions
constituted CPU-bound functions and later memory-bound
PoW functions gradually grew out of the ﬁeld as well. Among
PoW functions are partial hash inversion [13], moderately
hard memory-bound functions [23], guided tour puzzle [24],
Difﬁe-Helman problem-based [25], Merkle-tree-based [26],
Hokkaido [27] and Cuckoo cycle [28].
Recently, systems that incorporate PoW have also been
motivated by or used for various cryptocurrencies. Many
current cryptocurrencies, such as Bitcoin and HashCash [29],
[30], employ systems based on PoW algorithms. Blockchain
systems in cryptocurrency utilize a modiﬁed setup of the typi-
cal setting and actors in PoW frameworks for DoS attacks. In
Bitcoin, miners competitively attempt to secure a payment as
follows. First, they collect unveriﬁed Bitcoin transactions from
coin dealers in the Bitcoin network. Second, they combine
these transactions with other data to form a block which is only
accepted once the miner has found a nonce number hashing
to a number in the block with enough leading zeros.
B. Security in ML Systems