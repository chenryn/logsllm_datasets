derived as a function of the threshold t for this model:
F CTn(t) = FS(t) +
1
1 − ρBS(t)
(1 − FS(t))
+
+
λ
x2fS(x) dx" t
2(1 − ρBS(t)) " t
2(1 − ρBS(t))(1 − ρ) " ∞
λ
0
fS(y)
y
dy
0
x2fS(x) dx" ∞
t
fS(y)
y
dy.
t
The derivation is based on using the well-known Pollaczek-Khintchine
formula [13] to compute the average waiting time for a ﬂow in each
priority queue (assuming M/G/1 queues) and can easily be gen-
eralized for any number of priority queues (see the longer version
of this paper [6] for details). It is important to note that F CTn(·)
depends on the ﬂow size distribution as well as the overall load ρ.
444101
Increasing Load (10-80%)
n
)
t
(
T
C
F
100
105
High-priority threshold t (Bytes)
106
107
Figure 16: FCTn(t) for the web search ﬂow size distribution
at loads 10–80%. The red circles show the optimal threshold at
each load.
Figure 16 shows F CTn and the optimal threshold for the high-
priority queue computed numerically for the web search ﬂow size
distribution (Figure 4(a)). The threshold varies between ∼880-
1740KB as the load increases from 10% to 80%. Also, the ﬁgure
suggests that the performance can be fairly sensitive to the chosen
threshold, particularly at high load. We evaluate the sensitivity to
the threshold using simulations in the next section.
Remark 3. The above derivation provides a principled way of de-
termining thresholds for each priority, however it assumes that we
know the exact ﬂow size distribution in advance. Measuring the
ﬂow size distribution can be challenging in practice since it can
change across both time and space. For instance, because of spatial
variations (different ﬂow size distributions at different switches),
we may need to use different thresholds for the priority queues at
each switch and further these thresholds may change over time.
6.2 Simulations
We now compare using a few priority queues in existing switches
with pFabric. Our results conﬁrm that while this mechanism pro-
vides good performance with a sufﬁcient number of priority queues
(around 8), it is still worse than pFabric and the performance is sen-
sitive to the value of the thresholds used and also how the switch
buffer is shared among the priority queues.
We simulate the web search workload (§5.1) for three scenarios
with 2, 4, and 8 priority queues per fabric port. The queues at a
port share a buffer pool of size 225KB (150 packets). We reserve
15KB (10 packets) of buffer per queue and the rest is shared dy-
namically on a ﬁrst-come-ﬁrst-serve basis. In each scenario, we
use the optimal ﬂow size thresholds for each priority queue as de-
rived in §6.1. The results are shown in Figure 17. We observe
that, as expected, the average overall FCT (part (a)) improves as
we increase the number of priority queues and is close to pFab-
ric’s performance with 8 priority queues. We observed a similar
trend in the average FCT across small, medium, and large ﬂows
(plots omitted). Figure 17(b) also shows that there is a signiﬁcant
increase in the 99th percentile FCT for the small ﬂows at high loads
in the 8-queue case. This is because with 8 queues, 80 out of the
total 150 packets are reserved, leaving only 70 packets to be shared
among the queues. Thus at high load, during some bursts, the high
priority queue runs out of buffers and drops packets, increasing tail
latency. This demonstrates the need for carefully tuning the buffer
allocations for each priority queue for good performance.
Sensitivity to thresholds: Finally, we explore the sensitivity of the
performance with a few priority queues to using the “right” thresh-
olds for splitting trafﬁc. Figure 17(c) shows a comparison of the
4-queue system with optimal thresholds with a reasonable heuris-
tic that splits ﬂows equally across the 4 queues: the smallest 25%
of ﬂows are assigned to the highest priority queue, second smallest
25% to the second highest priority, etc. The plot shows the aver-
age FCT across all ﬂows. We ﬁnd a fairly substantial improvement
with the optimized thresholds. At 80% load, the average FCT is re-
duced by more than 30% with more substantial performance gaps
for the tail latencies for short ﬂows (we omit the ﬁgure for brevity).
This conﬁrms that the thresholds for splitting trafﬁc across limited
priority queues need to be chosen carefully. By allowing an es-
sentially unlimited number of priorities, pFabric does not require
any tuning and is not sensitive to parameters such as thresholds
(which may vary across time and space), minimum reserved buffer
per priority queue, overall buffer size, etc.
7. DISCUSSION
pFabric, more generally, advocates a different design philoso-
phy for datacenter networks. Our thought process is informed by
the fact that the datacenter network is more an inter-connect for
distributed computing workloads rather than a bit-pipe. Hence we
believe that it is more important to orchestrate the network resource
allocation to meet overall computing objectives, rather than tra-
ditional communication metrics such as throughput and fairness
which TCP optimizes for. This leads us to a design ethos where
ﬂows (which are proxy for the datum needed to be exchanged in
the compute tasks) become ﬁrst-class citizens and the network fab-
ric is designed to schedule them in a lightweight fashion to max-
imize application-layer objectives. pFabric is our ﬁrst step in this
direction. Below, we discuss some other common concerns that
might come up with a design like pFabric.
Starvation & Gaming: A potential concern with strictly prioritiz-
ing small ﬂows is that this may starve large ﬂows. Further, a mali-
cious user may game the system by splitting up her large ﬂows to
gain an advantage. Of course, these issues are not unique to pFab-
ric; any system that implements SRPT-like scheduling has these
concerns. That said, as prior work has also argued (see for example
PDQ [14] and the references therein, particularly Bansal et al. [8]),
under realistic heavy-tailed trafﬁc distributions, SRPT actually im-
proves the majority of ﬂows (even the large ﬂows) compared to
TCP’s fair sharing. This is consistent with our ﬁndings (e.g., Fig-
ures 8(c) and 9(c)). The intuition is that for heavy-tailed distribu-
tions, small ﬂows contribute a small fraction of the overall trafﬁc;
hence prioritizing them has little impact on the large ﬂows and in
fact helps them because they complete quickly which reduces net-
work contention. Nonetheless, if desired, an operator can put in
explicit safeguards against starvation. For instance, she can place a
cap the priority numbers so that beyond a certain size, all ﬂows get
the same base priority. Finally, our current design is targeted to pri-
vate datacenters thus malicious behavior is out of scope. In public
environments, further mechanisms may be needed to prevent abuse.
Setting packet priorities: In many datacenter applications ﬂow
sizes or deadlines are known at initiation time and can be conveyed
to the network stack (e.g., through a socket api) to set priorities.
In other cases, we expect that pFabric would achieve good perfor-
mance even with imprecise but reasonable estimates of ﬂow sizes.
As shown in §6, with realistic distributions, most of the beneﬁt can
be achieved by classifying ﬂows into a few (4-8) priority levels
based on size. The intuition is that it sufﬁces that at any instant
at each switch the priority dequeueing order is maintained (which
does not require that priorities be accurate, only that relative prior-
ities across enqueued ﬂows be largely correct).
Supporting multiple priority schemes: In practice, datacenter
fabrics are typically shared by a variety of applications with dif-
ferent requirements and a single priority scheme may not always
445T
C
F
d
e
z
i
l
a
m
r
o
N
5
4
3
2
1
0
Q=2 (Optimized)
Q=4 (Optimized)
Q=8 (Optimized)
pFabric
Ideal
0.2
0.4
Load
0.6
0.8
T
C
F
d
e
z
i
l
a
m
r
o
N
5
4
3
2
1
0
Q=2 (Optimized)
Q=4 (Optimized)
Q=8 (Optimized)
pFabric
Ideal
0.2
0.4
Load
0.6
0.8
T
C
F
d
e
z
i
l
a
m
r
o
N
5
4
3
2
1
0
Q=4 (Optimized)
Q=4 (Equal Split)
pFabric
Ideal
0.2
0.4
Load
0.6
0.8
(a) Overall: Avg
(b) (0, 100KB]: 99th prctile
(c) Optimal thresholds vs equal split:
Overall Avg
Figure 17: Web search benchmark with 2, 4, and 8 priority queues. Parts (a) and (b) show the average normalized FCT across all
ﬂows and the 99th percentile for the small ﬂows. Part (c) compares the performance using the optimized thresholds with a heuristic
which splits the ﬂows equally in case of 4 queues.
be appropriate. This can easily be handled by operating the pFab-
ric priority scheduling and dropping mechanisms within individual
“higher-level” trafﬁc classes in an hierarchical fashion. Traditional
QoS mechanisms such as WRR are used to divide bandwidth be-
tween these high-level classes based on user-deﬁned policy (e.g., a
soft-real time application is given a higher weight than batch jobs),
while pFabric provides near-optimal scheduling of individual ﬂows
in each class according to the class’s priority scheme (remaining
ﬂow size, deadlines, etc).
Other datacenter topologies: We have focused on Fat-tree/Clos
topologies in this paper as this is by far the most common topology
in practice. However, since conceptually we think of the fabric
as a giant switch with bottlenecks only at the ingress and egress
ports (§3) we expect our results to carry through to any reasonable
datacenter topology that provides uniform high throughput between
ingress and egress ports.
Stability: Finally, the theoretical literature has demonstrated sce-
narios where size-based trafﬁc prioritization may reduce the stabil-
ity region of the network [20]. Here, stability is in the stochastic
sense meaning that the network may be unable to keep up with ﬂow
arrivals even though the average load on each link is less than its
capacity [10]. However, this problem is mostly for “linear” topolo-
gies with ﬂows traversing different numbers of hops — intuitively it
is due to the tradeoff between prioritizing small ﬂows versus max-
imizing service parallelism on long routes. We have not seen this
issue in our study and do not expect it to be a major concern in real
datacenter environments because the number of hops is very uni-
form in datacenter fabrics, and the overall load contributed by the
small (high-priority) ﬂows is small for realistic trafﬁc distributions.
8. CONCLUSION
This paper decouples the key aspects of datacenter packet trans-
port — ﬂow scheduling and rate control — and shows that by de-
signing very simple mechanisms for these goals separately we can
realize a minimalistic datacenter fabric design that achieves near-
ideal performance. Further, it shows how surprisingly, large buffers
or complex rate control are largely unnecessary in datacenters. The
next step is to integrate a prototype implementation of pFabric with
a latency-sensitive application to evaluate the impact on applica-
tion layer performance. Further, our initial investigation suggests
that further work on designing incrementally deployable solutions
based on pFabric could be fruitful. Ultimately, we believe this can
pave the path for widespread use of these ideas in practice.
Acknowledgments: We thank our shepherd, Jon Crowcroft, and
the anonymous SIGCOMM reviewers for their valuable feedback.
Mohammad Alizadeh thanks Tom Edsall for useful discussions re-
garding the practical aspects of this work.
9. REFERENCES
[1] M. Al-Fares, A. Loukissas, and A. Vahdat. A scalable, commodity data center
network architecture. In Proc. of SIGCOMM, 2008.
[2] M. Al-Fares, S. Radhakrishnan, B. Raghavan, N. Huang, and A. Vahdat.
Hedera: dynamic ﬂow scheduling for data center networks. In Proc. of NSDI,
2010.
[3] M. Alizadeh, A. Greenberg, D. A. Maltz, J. Padhye, P. Patel, B. Prabhakar,
S. Sengupta, and M. Sridharan. Data center TCP (DCTCP). In Proc. of
SIGCOMM, 2010.
[4] M. Alizadeh, A. Kabbani, T. Edsall, B. Prabhakar, A. Vahdat, and M. Yasuda.
Less is more: trading a little bandwidth for ultra-low latency in the data center.
In Proc. of NSDI, 2012.
[5] M. Alizadeh, S. Yang, S. Katti, N. McKeown, B. Prabhakar, and S. Shenker.
Deconstructing datacenter packet transport. In Proc. of HotNets, 2012.
[6] M. Alizadeh, S. Yang, M. Sharif, S. Katti, N. McKeown, B. Prabhakar, and
S. Shenker. pFabric: Minimal Near-Optimal Datacenter Transport. http://
simula.stanford.edu/~alizade/pfabric-techreport.pdf.
[7] B. Atikoglu, Y. Xu, E. Frachtenberg, S. Jiang, and M. Paleczny. Workload
analysis of a large-scale key-value store. In Proc. of SIGMETRICS, 2012.
[8] N. Bansal and M. Harchol-Balter. Analysis of SRPT scheduling: investigating
unfairness. In Proc. of SIGMETRICS, 2001.
[9] A. Bar-Noy, M. M. Halldórsson, G. Kortsarz, R. Salman, and H. Shachnai. Sum
multicoloring of graphs. J. Algorithms, 2000.
[10] T. Bonald and L. Massoulié. Impact of fairness on Internet performance. In
Proc. of SIGMETRICS, 2001.
[11] A. Dixit, P. Prakash, Y. C. Hu, and R. R. Kompella. On the Impact of Packet
Spraying in Data Center Networks. In Proc. of INFOCOM, 2013.
[12] A. Greenberg, J. R. Hamilton, N. Jain, S. Kandula, C. Kim, P. Lahiri, D. A.
Maltz, P. Patel, and S. Sengupta. VL2: a scalable and ﬂexible data center
network. In Proc. of SIGCOMM, 2009.
[13] D. Gross, J. F. Shortle, J. M. Thompson, and C. M. Harris. Fundamentals of
Queueing Theory. Wiley-Interscience, New York, NY, USA, 4th edition, 2008.
[14] C.-Y. Hong, M. Caesar, and P. B. Godfrey. Finishing Flows Quickly with
Preemptive Scheduling. In Proc. of SIGCOMM, 2012.
[15] The Network Simulator NS-2. http://www.isi.edu/nsnam/ns/.
[16] J. Ousterhout, P. Agrawal, D. Erickson, C. Kozyrakis, J. Leverich, D. Mazières,
S. Mitra, A. Narayanan, D. Ongaro, G. Parulkar, M. Rosenblum, S. M. Rumble,
E. Stratmann, and R. Stutsman. The case for RAMCloud. Commun. ACM, 2011.
[17] C. Raiciu, S. Barre, C. Pluntke, A. Greenhalgh, D. Wischik, and M. Handley.
Improving datacenter performance and robustness with multipath TCP. In Proc.
of the SIGCOMM, 2011.
[18] B. Vamanan, J. Hasan, and T. N. Vijaykumar. Deadline-Aware Datacenter TCP
(D2TCP). In Proc. of SIGCOMM, 2012.
[19] V. Vasudevan, A. Phanishayee, H. Shah, E. Krevat, D. G. Andersen, G. R.
Ganger, G. A. Gibson, and B. Mueller. Safe and effective ﬁne-grained TCP
retransmissions for datacenter communication. In Proc. of SIGCOMM, 2009.
[20] M. Verloop, S. Borst, and R. Núñez Queija. Stability of size-based scheduling
disciplines in resource-sharing networks. Perform. Eval., 62(1-4), 2005.
[21] C. Wilson, H. Ballani, T. Karagiannis, and A. Rowtron. Better never than late:
meeting deadlines in datacenter networks. In Proc. of SIGCOMM, 2011.
[22] D. Zats, T. Das, P. Mohan, D. Borthakur, and R. H. Katz. DeTail: Reducing the
Flow Completion Time Tail in Datacenter Networks. In Proc. of SIGCOMM,
2012.
446