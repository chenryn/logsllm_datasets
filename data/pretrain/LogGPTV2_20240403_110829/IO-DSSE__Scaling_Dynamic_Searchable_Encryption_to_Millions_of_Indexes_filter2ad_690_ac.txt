entries can be added to this index only when we have a
full block. This is the basic approach taken by [5] for static
encrypted search, though they expand on it to deal with far
larger data sets than we wish to.
The main technical challenge in our construction is the
design of OUI for managing partially-ﬁlled blocks until they
are full and can be pushed to the FBI. In particular, note that
the blocks in OUI need not be full and are instead padded
to some ﬁxed size. When a block is full of real data (i.e. no
padding) its contents are transferred to the full-block index.
This allows messages to be added and deleted from the OUI
by updating the requisite block. Of course, we must do so in
a way that does not leak which blocks are being updated, or
else we fail to meet the basic requirements for secure search
by leaking update patterns.
A. An Obliviously Updatable Index
ORAM forms a generic starting point for our obliviously
updatable index. In particular, storing partial blocks in ORAM
would allow us to update them privately. However, as a generic
approach, ORAM is an overkill. An index built on top of
ORAM would hide not only reads and writes resulting from
an index update, but also reads resulting from an index lookup.
6
This is stronger than the typical protection provided by SSE
schemes (which leak the “search pattern”), and in our case
(similar to prior work) this information is already revealed via
searches against the full-block index. So we gain no additional
privacy by hiding the search pattern only in the OUI and will
realize considerable efﬁciency gains by not doing so.
As a concrete starting point, consider a basic construction
of Path ORAM [21]. In Path ORAM, entries (called blocks
in our construction) are stored as leaves in a full binary tree.
Each time an entry is read or written, the entire path from its
leaf to root is read, the entry is remapped to a random leaf
in the tree, and the original path is rewritten with the entry
placed at as close to the lowest common ancestor of the old
and new paths as possible.
The position map which keeps track of the current leaf
position for each entry is typically stored on the server side in
its own ORAM. This leads to many round trips of interaction
for each read/update which is a non-starter for a real-time
application such as email. We note that for email, it is feasible to
store the position map client side. As shown in the experiments
in Section IV-C, this storage will not exceed 70MB in 10 years
for the 95th percentile user and for most users is closer to
35MB.
Even with the position map stored on the client side, a
read or write entails reading everything on the path from a leaf
to the root, performing some client side operations, and then
writing back along that path. In other words, in Path ORAM
(and ORAM in general), entries are shufﬂed both in case of
reads and writes.
At ﬁrst glance, in the case of a lookup in the oblivious
index, we can simply omit the complicated machinery for a
read of the ORAM (which we only need for reads and writes
for index updates) and directly access a single entry in the tree.
We do not care if repeated index accesses to the same location
are identiﬁable. However, there are two issues with this. First,
the position map only stores what leaf an entry was mapped to,
not the particular point along the path from leaf to root where
it is stored. This can be ﬁxed by storing, for each keyword,
additional information to locate the entry.
The larger issue is that the reshufﬂing that occurs on a
read provides privacy protections not just for the read (which
is not important for us) but for subsequent reads and writes.
If reads are not shufﬂed, then an observer can identify when
frequently looked up index entries are updated. As a result,
we cannot simply have a “half-ORAM”: to get completely
oblivious writes, we must at some point reshufﬂe the locations
we read from.
Crucially, in our obliviously updatable index, we need not
do this on every read (as in ORAM), rather we can defer the
shufﬂing induced by a non-oblivious read to the beginning of
an update. We call these deferred reads.
This enables considerable savings. First, since updates can
be batched (i.e., we collect a bunch of updates to various entries
locally and only commit them to the server later), we can shift
the computational and bandwidth load to idle time (e.g. when
a mobile device is plugged in and connected to wiﬁ) at the
cost of temporary client storage. Second, repeated searches for
the same term only result in one deferred read. Third, searches
for terms that are mapped to the same leaf also only result in
one shufﬂe operation. Finally, because paths overlap even for
distinct leaves, we will realize considerable savings: e.g. for
10 deferred read shufﬂes, we will end up transmitting the root
of the tree once instead of 10 times, the children of root twice
instead of 5 times, etc. Looking forward to our evaluation, this
results in over 90% savings in accesses compared to the simple
Path ORAM.
We note that write-only ORAM constructions [1] do not
solve our problem. Write-only ORAM is used in settings where
reads leave no record (e.g. where an adversary only has access
to snapshots of an encrypted disk, which reveals changes due to
writes but not reads.). In these settings, the initial read needed
to determine the contents of the block being appended to can
be done in the clear. We cannot do that here since we must
request the partial bock from the server before appending to it.
To summarize, our obliviously updatable index is a modiﬁed
Path ORAM scheme with the following changes:
• We keep the position map on the client slide and augment
the position map to allow us to locate index entries inside
a given path from leaf to the root.
• On non-oblivious reads: we lookup the entry directly from
its position in the tree (i.e. one disk access), but add the
leaf to the list of deferred reads.
• On batched reads and updates: we read all paths in the
set of deferred reads since the last batch updated and all
the paths associated with the updates themselves. We then
remap and edit the entries on these paths as in standard
Path ORAM and write them back to the ORAM at once.
Security Intuition Deferred shufﬂing for reads ensures that
when a non-deferred read/write happens, the system is in the
exact same state as it would be in full Path ORAM. Intuitively,
this models the effect of shufﬂing a deck of cards: no matter
what the previous state was and how the deck was rigged, the
shufﬂe is still good.
that(cid:81)M
More formally, our approach means that after the deferred
read, the position map entries are statistically independent of
each other, and we retain the security condition for Path ORAM
) for non-deferred oper-
ations. Of course, we have leaked substantial information about
the prior state of the index, but that leakage is allowed in SSE!
Rather than proving this separately, we will capture it in the
proof of security for the SSE scheme itself.
j=1 P r(position(aj)) = ( 1
2L
M
B. The Full Protocol
Next, we describe our full DSSE scheme for email which
is a combination of the OUI described above and a separate
index for full blocks. A detailed description follows.
Let H = (hsetup, hlookup, hwrite) be a hash table imple-
mentation, E = (KG, Enc, Dec) be a CPA-secure encryption
scheme and F : K × M → C be a pseudorandom function.
Let W be the universe of all keywords, and L = log(|W|).
Setup. (Figure 3) For simplicity, we assume that the DB
is initially empty, and documents are dynamically added. If
not, one can run the SSEADD protocol we describe shortly
multiple times to populate the client and server storages with
the documents in DB.
7
hash table.
(cid:104)σ, EDB(cid:105) ↔ SSESETUP(cid:104)(1λ,⊥),⊥(cid:105):
1: Client runs (hc, Mc) ← hsetup() to setup a local
2: Server runs (hs, Ms) ← hsetup() to setup an append-
only hash table.
3: for w ∈ |W| do
4:
5:
6:
posw
countw, rw, (cid:96)w ← 0, Bw ← ∅
Client runs Mc ← hwrite(w, [posw, (cid:96)w, countw,
R← {0, . . . , 2L}
rw, Bw], Mc)
7: end for
8: kf ← K(1λ), ke ← KG(1λ), ka ← KG(1λ)
9: Client and server run the setup for a non-recursive
Path ORAM. Server stores the tree T , and client
stores the stash S.
10: Client outputs σ = (Mc, S, kf , ka, ke)
11: Server outputs EDB = (Ms, T )
Fig. 3: Setup for our DSSE scheme
The client generates three random keys kf , ke, and ka, one
for the PRF F , and the other two for the CPA-secure encryption
scheme.
The client and server initialize the obliviously updatable
index, i.e., a non-recursive Path ORAM for a memory of size
|W|. We denote the tree stored at the server by T , and the
corresponding stash stored at the client by S. For all references
to Path ORAM we use the notation introduced in Section II-C.
The server also sets up an initially empty full-block index, i.e.,
an append-only hash table that will be used to store full blocks
of document IDs.
For every w ∈ W , the client stores in a local hash table the
key-value pair (w, [posw, (cid:96)w, countw, rw, Bw]), where Bw is a
block storing IDs of documents containing w (initially empty),
posw stores the leaf position in {0, . . . , 2L} corresponding to
w (chosen uniformly at random), (cid:96)w stores the level of the node
on path P (posw) that would store the block for documents
containing w (initially empty), countw stores the number of full
blocks for keyword w already stored in the append-only hash
table (initially 0), and rw is a bit indicating whether keyword
w is searched since the last push of the client’s block to Path
ORAM (initially 0).
The client’s state σ will be the hash table Mc, the stash S
for the Path ORAM, and the keys ke, kf , ka.
Search. (Figure 4) The client will store the matching documents
in the initially empty set R = ∅. To search locally, the
client ﬁrst
looks up w in its local hash table to obtain
[posw, (cid:96)w, countw, rw, Bw], and lets R = R ∪ Bw.
It then asks the server for the bucket in the tree T at node
level (cid:96)w and on path P (posw), i.e., P (posw, (cid:96)w). It decrypts
the blocks in the bucket using ke. If it ﬁnds a tuple (w, Ow) in
the bucket, it lets R = R ∪ Ow. If rw is not yet set, the client
lets rw = 1 to indicate that w was searched for.
For i = 1, . . . , countw, the client sends Fkf (w||i) to the
server, who looks it up in the append-only hash table and
SSESEARCH(cid:104)(σ, w), EDB = (T, Ms)(cid:105):
1: R ← ∅
2: [posw, (cid:96)w, countw, rw, Bw] ← hlookup(w, Mc)
3: R ← R ∪ Bw
4: U ← READBUCKET(P (posw, (cid:96)w)
5: Read (w, Ow) from U
6: R ← R ∪ Ow
7: rw ← 1
8: hwrite(w, [posw, (cid:96)w, countw, rw, Bw], Mc)
9: for i ∈ {1, . . . , countw} do
10:
11:
12:
13:
14: end for
15: Client outputs R
Client sends Fkf (w||i) to server
w ← hlookup(Fkf (w||i), Ms)
Server returns C i
w ← Decka (C i
Ai
w)
R ← R ∪ Ai
w
Fig. 4: Search for our DSSE scheme
w. The client decrypts using
w. The client then outputs R. See
returns the encrypted full block Ai
ka and lets R = R ∪ Ai
Figure 4 for details.
Update. (Figure 5) Let idd be the document identiﬁer associ-
ated with d. For every keyword w in d, the client looks up w
in its local hash and adds idd to Bw. It then checks whether its
local storage has reached the maximum limit maxc or not. If
not, the update is done. Else, we need to push all the document
blocks to the server.
But before doing so, we need to ﬁnish the ORAM access
for all reads done since the last push. In particular, for all non-
zero rw’s, the client needs to read the whole path P (posw),
re-encrypt all the buckets using fresh randomness, update posw
to a fresh random leaf, and write the buckets back to the tree
using the Path ORAM approach.
Then, for every non-empty block Bw in its local hash, the
client performs a full ORAM write to add the documents in
Bw to the ORAM block Ow for the same keyword. If Ow
becomes full as a result, maxb documents IDs in the block
are removed and inserted into Acountw+1
, and inserted to the
append-only hash table using a keyword Fkf (w||countw + 1).
See Figure 5 for details.
w
C. Security Analysis
As noted in Section II, security of an SSE scheme is
deﬁned with respect to a leakage function L on the database of
documents DB as well as the history of search/update operations
in the index. We ﬁrst specify the leakage function for our
construction.
The Leakage Function Recall that DB = (di, Wi)N
database of document-keyword pairs and W = ∪N
universe of keywords.
i=1 is the
i=1Wi is the
During the setup, L(DB) outputs the size of database |DB|,
i.e., the total number of initial document-keyword pairs in the
database. For simplicity we can assume this is zero initially. On
each search query wi, the leakage function L(DBi−1, H), leaks
SSEADD(cid:104)(σ, idd), EDB(cid:105):
1: for w ∈ d do
[posw, (cid:96)w, countw, rw, Bw] ← hlookup(w, Mc)
2:
Bw ← Bw ∪ {idd}
3:
hwrite(w, [posw, (cid:96)w, countw, rw, Bw], Mc)
4:
sizec ← sizec + 1
5:
6: end for
7: if sizec  maxb then