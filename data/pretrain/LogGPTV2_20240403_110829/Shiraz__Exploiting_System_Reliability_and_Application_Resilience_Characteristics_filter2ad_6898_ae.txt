MTBF: 5 hours
1.0
60
40
0.8
20
MTBF: 20 hours
MTBF: 20 hours
e
n
i
l
e
s
a
b
r
e
v
o
t
n
e
m
e
v
o
r
p
m
I
0
0.6
−20
75
0.4
50
25
0.2
0
0.0
0
1
0.0
2
3
0.2
4
6
5
7
0.6
0.4
Application ID
8
9
0.8
10
1.0
0.0
−25
0.0
2
0.2
0.4
3
0.6
OCI Stretch Factor
4
0.8
1.0
Figure 14: Shiraz provides improvement in real-world multi-
application mix selected from Table 1 and simulated for
year-long time period (left). The horizontal lines denotes
the average improvement in useful work per application.
Shiraz+ decreases checkpointing overhead signiﬁcantly for
the same mix of applications (right).
pairs of applications with different checkpointing overheads
and run one such pair between two failures using Shiraz, and
switching to a different pair after every failure. Optimal strat-
egy to make such pairs is to combine the application with the
highest checkpointing overhead with the application with the
lowest checkpointing overhead, until we exhaust the avail-
able applications. The theoretical proof is not provided for
brevity; the intuition behind such a strategy is simple: it max-
imizes the average of the ratios of checkpointing overheads.
We also experimented with another strategy: making random
pairs. We found that while it may not deliver the maximum
possible improvement, it is relatively easier to implement.
To evaluate Shiraz in a multi-application environment, we
experimented with the latter strategy using 10 applications
and noted the corresponding throughput gains. The applica-
tion list is composed from the real-world application char-
acteristics from Table 1. We used the Shiraz model to ob-
tain the optimal switch point for the different application
pairs, and simulated the scenario where these applications
ran for one calendar year (8,700 hours). To ensure that our
results are statistically stable, we repeated the simulation
over 15,000 times and report average of all runs.
Figure 14 (left) shows the overall system throughput im-
provement and impact on individual job performance for all
the 10 applications. We make a few interesting observations.
First, no application suffers a performance degradation, and
the average throughput improvement is 15 hours. Second,
Shiraz improves the total useful work by approx. 91 hours
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:30:14 UTC from IEEE Xplore.  Restrictions apply. 
and 157 hours for the petascale and the exascale systems,
respectively.
Our results also demonstrate that Shiraz+ is also effective
in the multi-application scenario. Figure 14 (right) shows
that Shiraz+ (with 3× OCI stretch factor) decreases the
checkpointing overhead by up to 52%, without incurring any
loss in the overall system throughput for both exascale and
petascale systems. When the OCI stretch factor is increased
to 4×, only then the system incurs degradation (less than
1%) the total useful work, while the checkpointing overhead
decreases by up to 60%.
To show the results in a conservative scenario, we conduct
an experiment with 40 jobs, with 5 heavy-weight applica-
tions, and the rest 35 light-weight applications. The 35 light-
weight applications are selected at random from the three
least heavy applications from Table 1. Shiraz improves the
total useful work done 57 hours and 89 hours for the petas-
cale and the exascale systems, respectively.
Finally, we evaluate the potential energy savings enabled
by Shiraz for the exascale (5 hours MTBF) and the petascale
systems (20 hours MTBF). Since Shiraz increases the useful
work done per unit time at the whole system level, it effec-
tively saves energy that would have been spent on lost work
(due to failures). In order to simplify the evaluation and in-
terpretation, we estimate the yearly energy savings. Taking a
conservative electricity rate of $0.1 per kW-Hour [2], the en-
ergy and monetary savings on the exascale (5 hours MTBF
and 20MW power consumption) system would translate to
1.78 MW-Hour and $178,000 per year, respectively. For the
petascale (20 hours MTBF and 10MW power consumption)
system, the energy and monetary savings would translate to
0.57 MW-Hour and $57,000 per year.
These savings could be invested towards faster storage
systems and more computing power in the future — which
would further increase the proﬁts due to faster comple-
tion times. For the petascale system, the cost savings due
to energy expenditure cuts enabled by Shiraz translate to
$285,000 over 5 years (anticipated lifetime of a system).
At 0.2 GB/USD for SSD-based burst buffers [3, 4] (the to-
tal cost of infrastructure pessimistically assumed to be 3×
of the hardware cost due to packaging, assembly, ﬁrmware
and integration cost), the monetary savings could pay for
5.7% of the cost of the burst buffers (0.285M USD out of
5M USD) for the petascale system, with 1 PB of storage.
For the exascale system, the cost savings enabled by Shiraz
would amount to $890,000 over 5 years. We note that this
analysis is on the conservative side, as it does not include
the energy cost reduction due to the reduction in data move-
ment enabled by Shiraz+.
We note that in a multi-application environment, Shiraz
can produce different individual performance improvements
for the same application depending upon on the pairing and
application-mix since the runtime improvement provided by
Shiraz depends on the δ-factor. This can possibly lead to
small amount of unpredictability in the runtime, although
Parallel 
Storage 
System
Job 
Queue
Workload
Manager
(e.g. SLURM)
Application 
Checkpoint 
Time and 
System 
MTBF
Allocate
Nodes
Shiraz
And
Shiraz+
Compute Nodes
App 1
App 2
P0
P1
P3
P4
P2
P5
P6
Periodic 
Checkpoint
Periodic 
Checkpoint
Incoming Jobs
Upon Application
Switch or Failure
Checkpoint
Figure 15: Prototype of Shiraz and Shiraz+.
Shiraz will improve the individual runtime in all such cases.
Improving predictability in a dynamic application-mix will
be a worthy goal for future works.
In summary, our results show that Shiraz leads to signiﬁ-
cant energy and monetary saving for real-world applications
that can act as positive feedback loop and result in com-
pounded returns over years.
Prototype implementation and evaluation of Shiraz and
Shiraz+ using system-level checkpointing: We developed
a prototype of Shiraz and Shiraz+ to evaluate its effective-
ness on real-world applications. We developed a scheduler
plug-in that implements the core scheduling algorithm of
Shiraz and Shiraz+. It maintains records of the checkpoint-
ing overhead for different applications, temporal charac-
teristics of system failures, and takes checkpoints using a
system-level checkpointing package, and schedules applica-
tions based on the Shiraz model. To demonstrate the effec-
tiveness, we evaluated the prototype using two real-world
HPC applications: Co-Design Molecular Dynamics Proxy
(CoMD) [26] and Finite Element Solver (miniFE) [22].
CoMD represents a variety of scientiﬁc applications includ-
ing SPaSM, and miniFE is an approximation of unstructured
ﬁnite element and ﬁnite volume codes including HPCCG
and pHPCCG. We used DMTCP [7], a system-level check-
pointing library, to perform checkpoints, and the optimal
switch point was decided based on the checkpointing over-
head obtained experimentally. We note that our plugin is not
tied to a particular implementation of checkpointing library
and can be ported across systems and resource managers
(e.g., SLURM) (schematic shown in Figure 15). The ratio
of the checkpointing overhead of miniFE (heavyweight ap-
plication) to that of CoMD (lightweight application) is 30x,
as experimentally measured using DMTCP.
Statistically sound evaluation of such a prototype imple-
mentation is challenging since it requires dedicated time (in
order of months) on a large-scale supercomputer. To address
this challenge, we emulated the setting by feeding a failure
trace with the same characteristics as large-scale supercom-
puters (discussed in Section 2) but at a higher frequency.
We also scaled down the program input size to ensure that
the runs completed on a local cluster within a month. We
performed an effectively 200-hour long run by scaling the
failure-frequency and program size, and did this run 30 times
for each point, to obtaining stable results. We injected er-
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:30:14 UTC from IEEE Xplore.  Restrictions apply. 
92
t
n
e
m
e
v
o
r
p
m
I
)
%
(
e
n
i
l
e
s
a
b
r
e
v
o
Total Useful Work
Total Ckpt Ovhd
60
45
30
15
0
2
3
4
OCI-stretch factor
Figure 16: Impact of Shiraz+ on CoMD and miniFE appli-
cation performance and checkpointing overhead.
rors in the local cluster that crash the application and used
checkpoints to recover from errors without any human inter-
vention during the experiments. At the end of run, we col-
lected runtime statistics (useful work, checkpoint overhead,
and lost work) to compare Shiraz with the baseline.
We found that Shiraz results in 10.2% more useful work
system-wide using CoMD and miniFE application, com-
pared to the baseline case, where applications are switched
at every failure. Since these experiments take prohibitively
long, we did not explore the optimal switching point using
experiments. Instead, we used the Shiraz model to obtain op-
timal point ofﬂine and results show improvements.
We also evaluated Shiraz+ using this prototype. Fig. 16
shows that Shiraz+ reduces the checkpointing overhead sig-
niﬁcantly with minimal or no performance degradation. For
example, the overall checkpointing overhead is reduced by
approximately 35.8% when using a 2× OCI-stretch factor,
while still maintaining the overall improvement in useful
work at approximately 7%. When Shiraz+ applies 3× and
4× OCI-stretch, the overall checkpointing overhead is re-
duced by 69.6% and 77.6%, respectively, while the per-
formance degradation is under 3%. Overall, the evaluation
shows that when operating at the optimal switching point
obtained by Shiraz model, Shiraz+ is effective in reducing
the data movement caused by checkpointing and still retains
some of the performance beneﬁts provided by Shiraz.
6. Related Work
A large number of previous HPC works have focused on
performing failure analysis and developing checkpointing
methods for fault tolerance. HPC system and application
logs are extensively studied to extract information about the
characteristics of failures [11, 17, 21, 31, 35, 36]. More
recent studies have used neural networks, statistical learning
and big-data analytics to model failure characteristics and
provide potential root causes [33, 34].
In order to improve the checkpointing overhead, past
studies have provided different derivations for application-
speciﬁc OCI [14, 24, 30, 38, 40, 42]. Our work relies on and
is complementary to these studies as it schedules jobs us-
ing the proposed OCI values to improve system throughput.
Some recent studies have also proposed to use multi-level
checkpointing: a strategy that checkpoints at different lev-
els (memory, SSD, PFS) to tolerate different types of fail-
ures, based on the temporal and spacial distribution of the
failures [10, 15, 16, 27]. Another method called incremen-
tal checkpointing proposes to only store the state of the data
which has been modiﬁed since the last checkpoint, thus po-
tentially reducing I/O overhead [20, 29]. On the other hand,
several studies have seeked to use faster storage options such
as SSD-based burst buffers to reduce the overhead of writ-
ing the checkpoint ﬁles [8, 23, 37]. All of the above opti-
mizations, which target different methods of reducing check-
pointing overhead, can be used in conjunction with Shiraz,
which targets efﬁcient scheduling as a way to improve sys-
tem throughput and decrease the checkpointing overhead.
Bouguera et al. [12] propose an application-oriented re-
silience scheme that combines predictive, proactive and pre-
ventive checkpointing by tracking and drawing correlation
graphs between faults and failures. Several other works have
developed reliability-aware task scheduling strategies that
optimize the degree of job replication to reduce communica-
tion interference and/or energy consumption [28, 39, 41, 43].
However, replication for increased reliability also increases
consumption of valuable compute and energy resources. Ti-
wari et al.
[40] introduced Lazy checkpointing that uses
temporal locality of failures to dynamically adjust the check-
pointing frequency of an application to reduce I/O overhead.
Lazy checkpointing results in non-equidistant check-
points because the rate of increase of interval depends on
the hazard rate. Unfortunately, non-equidistant checkpoints
are unattractive for many applications because some domain
scientists may use checkpoints to monitor the progress of the
simulation; non-equidistant checkpoints make it difﬁcult to
monitor such progress. On the contrary, both Shiraz and Shi-
raz+ provide equidistant checkpoints. Shiraz+ shows that it
is possible to increase the OCI by a factor, reduce I/O over-
head and still achieve signiﬁcant performance improvement
unlike Lazy checkpointing. Therefore, techniques proposed
in this work are more practical strategies, which improve
performance, I/O overhead and work even when scheduling
multiple applications unlike Lazy checkpointing [40].
In the above implementations, applications tune their in-
ternal parameters to improve resilience, reduce I/O over-
head, and increase their useful work. However, no work ex-
plores how these individual optimizations can be combined
together, without disrupting an application’s local optimiza-
tion methods such as OCI tuning, to improve the system’s
throughput. This work proposes a scheduling technique that
exploits failure characteristics and works with multiple ap-
plications, while ensuring individual performance fairness.
7. Conclusion
This paper introduced, Shiraz, a novel scheme, to improve
the overall system throughput by intelligently scheduling
applications with different checkpointing overheads. This
paper also introduced, Shiraz+, a novel scheme to reduce
checkpointing overhead. Evaluation results show that Shiraz
improves system throughput under a wide variety of circum-
stances. Shiraz can save up to $285,000 over the lifetime
of a petascale supercomputer and Shiraz+ reduces the data
movement by up to 52% for a variety of applications.
93
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:30:14 UTC from IEEE Xplore.  Restrictions apply. 
Acknowledgment We thank anonymous reviewers for their con-
structive feedback. This work was partially supported by North-
eastern University, NSF Grant ACI-1440788, Grant 2014-345 from
“Chaire d’attractivit´e” de lIDEX Universit´e F´ed´erale Toulouse
Midi-Pyr´en´ees and resources from the Mass Open Cloud (MOC).
References
[1] CFDR Data. https://tinyurl.com/yd6ornwa. [Online; ac-
cessed 28-Nov-2017].
[2] EIA - Electricity Data. https://tinyurl.com/ya6o3eas.
[Online; accessed 04-Dec-2017].
[3] Intel DC P3608 SSDPECME040T401. https://tinyurl.com/
ybng8ll3. [Online; accessed 04-Dec-2017].
[4] Samsung PM1725a Series 1.6TB TLC. https://tinyurl.com/
yd8rcy55. [Online; accessed 04-Dec-2017].
[5] Large Scale Computing and Storage Requirements for Biological and
Environmental Science: Target 2017. Technical Report LBNL-6256E,
LBNL, 2012.
[6] Large Scale Production Computing and Storage Requirements for
High Energy Physics: Target 2017. Technical report, LBNL, 2012.
[7] Jason Ansel, Kapil Arya, and Gene Cooperman. DMTCP: Transparent
Checkpointing for Cluster Computations and the Desktop. In IPDPS
2009, pages 1–12. IEEE, 2009.
[8] L. Bautista-Gomez, N. Maruyama, F. Cappello, and S. Matsuoka.
Distributed Diskless Checkpoint for Large-scale Systems. In CCGrid
2010, pages 63–72. IEEE Computer Society, 2010.
[9] Leonardo Bautista-Gomez et al. Reducing Waste in Extreme Scale
Systems Through Introspective Analysis. In IPDPS 2016, pages 212–
221. IEEE, 2016.
[10] Anne Benoit, Aur´elien Cavelan, Valentin Le F`evre, Yves Robert, and
Hongyang Sun. Towards Optimal Multi-level Checkpointing. IEEE
Trans. Comput, 66(7):1212–1226, 2017.
[11] R. Birke, I. Giurgiu, L. Y Chen, D. Wiesmann, and T. Engbersen.
Failure analysis of virtual and physical machines: Patterns, causes and
characteristics. In DSN 2014, pages 1–12. IEEE, 2014.
[12] Mohamed Slim Bouguerra et al. Improving the Computing Efﬁciency
of HPC Systems Using a Combination of Proactive and Preventive
Checkpointing. In IPDPS 2013, pages 501–512. IEEE, 2013.
[13] Franck Cappello. Fault Tolerance in Petascale/Exascale Systems: Cur-
IJHPCA,
rent Knowledge, Challenges and Research Opportunities.
23(3):212–226, 2009.
[14] John T Daly. A Higher Order Estimate of the Optimum Checkpoint
Interval for Restart Dumps. Future Generation Computer Systems,
22(3):303–312, 2006.
[15] S. Di, M. S Bouguerra, L. Bautista-Gomez, and F. Cappello. Opti-
mization of Multi-level Checkpoint Model for Large Scale HPC Ap-
plications. In IPDPS 2014, pages 1181–1190. IEEE, 2014.
[16] Sheng Di, Yves Robert, Fr´ed´eric Vivien, and Franck Cappello. To-
wards an Optimal Online Checkpoint Solution Under a Two-level
HPC Checkpoint Model. TPDS 2017, 28(1):244–259, 2017.
[17] N. El-Sayed and B. Schroeder. Reading Between the Lines of Failure
In DSN 2013, pages
Logs: Understanding How HPC Systems Fail.
1–12. IEEE, 2013.
[18] Elmootazbellah N Elnozahy and James S Plank. Checkpointing for
Peta-scale Systems: A Look into the Future of Practical Rollback-
Recovery. TDSC 2004, 1(2):97–108, 2004.
[19] Kurt Ferreira et al. Evaluating the Viability of Process Replication
Reliability for Exascale Systems. In SC 2011, page 44. ACM, 2011.
[20] Kurt B Ferreira, Rolf Riesen, Patrick Bridges, Dorian Arnold, and Ron
Brightwell. Accelerating incremental checkpointing for extreme-scale
computing. Future Generation Computer Systems, 30:66–77, 2014.
[21] Ana Gainaru, Franck Cappello, and William Kramer. Taming of the
Shrew: Modeling the Normal and Faulty Behaviour of Large-scale
HPC Systems. In IPDPS 2012, pages 1168–1179. IEEE, 2012.
[22] M Heroux. MiniFE: Finite Element Solver. https://tinyurl.
com/y7hslf65. [Online; accessed 15-Apr-2018].
[23] Ning Liu et al. On the Role of Burst Buffers in Leadership-class
Storage Systems. In MSST 2012, pages 1–11. IEEE, 2012.
[24] Yudan Liu et al. An Optimal Checkpoint/Restart Model for a Large-
scale High Performance Computing System. In IPDPS 2008, pages
1–9. IEEE, 2008.
[25] Robert Lucas. Top Ten Exascale Research Challenges.
ASCAC Subcommittee Report, 2014.
In DOE
[26] Jamaludin Mohd-Yusof, S Swaminarayan, and TC Germann. Co-
Design for Molecular Dynamics: An Exascale Proxy Application,
2013.
[27] Adam Moody, Greg Bronevetsky, Kathryn Mohror, and Bronis R
De Supinski. Design, Modeling, and Evaluation of a Scalable Multi-
level Checkpointing System. In SC 2010, pages 1–11. IEEE, 2010.
[28] A. Namazi, M. Abdollahi, S. Safari, S. Mohammadi, and M. Danesh-
talab. Reliability-Aware Task Scheduling using Clustered Replication
for Multi-core Real-Time Systems.
In NoCArc 2016, pages 45–50.
ACM, 2016.
[29] Bogdan Nicolae and Franck Cappello. AI-Ckpt: Leveraging Memory-
access Patterns for Adaptive Asynchronous Incremental Checkpoint-
ing. In HPDC 2013, pages 155–166. ACM, 2013.
[30] Ron A Oldﬁeld et al. Modeling the Impact of Checkpoints on Next-
generation Systems. In MSST 2007, pages 30–46. IEEE, 2007.
[31] Narasimha Raju, Y Liu Gottumukkala, Chokchai B Leangsuksun,
Raja Nassar, and Stephen Scott. Reliability Analysis in HPC Clusters.
In HAPCW, pages 673–684, 2006.
[32] Marvin Rausand and Arnljot Hoyland. System Reliability Theory:
Models, Statistical Methods and Applications. Wiley-IEEE, 3 edition,
November 2003.
[33] Andrea Ros`a, Lydia Y Chen, and Walter Binder. Predicting and
Mitigating Jobs Failures in Big Data Clusters. In CCGrid 2015, pages
221–230. IEEE, 2015.
[34] Andrea Ros`a, Lydia Y Chen, and Walter Binder. Failure Analysis and
Prediction For Big-data Systems. TSC 2016, 2016.
[35] Ramendra K Sahoo, Mark S Squillante, A Sivasubramaniam, and
Yanyong Zhang. Failure Data Analysis of a Large-scale Heteroge-
neous Server Environment. In DSN 2004, pages 772–781. IEEE, 2004.
[36] B Schroeder and Garth Gibson. A Large-scale Study of Failures in
High-performance Computing Systems. TDSC 2010, 7(4):337–350,
2010.
[37] Jim Stevens, Paul Tschirhart, and Bruce Jacob. Fast Full System Mem-
ory Checkpointing with SSD-aware Memory Controller. In MemSys
2016, pages 96–98. ACM, 2016.
[38] Omer Subasi, Gokcen Kestor, and Sriram Krishnamoorthy. Toward
In CLUSTER
a General Theory of Optimal Checkpoint Placement.
2017, pages 464–474. IEEE, 2017.
[39] Xiaoyong Tang, Kenli Li, Renfa Li, and Bharadwaj Veeravalli.
Reliability-aware Scheduling Strategy for Heterogeneous Distributed
Computing Systems. JPDC, 70(9):941–952, 2010.
[40] D. Tiwari, S. Gupta, and S S Vazhkudai. Lazy Checkpointing: Exploit-
ing Temporal Locality in Failures to Mitigate Checkpointing Over-
heads on Extreme-scale Systems. In DSN 2014, pages 25–36. IEEE,
2014.
[41] S. Wang, K. Li, J. Mei, G. Xiao, and K. Li. A Reliability-aware
Task Scheduling Algorithm Based on Replication on Heterogeneous
Computing Systems. JGC, 15(1):23–39, 2017.
[42] John W Young. A First-order Approximation to the Optimum Check-
point Interval. CACM, 17(9):530–531, 1974.
[43] L. Zhang, K. Li, Y. Xu, J. Mei, F. Zhang, and K. Li. Maximizing
Reliability with Energy Conservation for Parallel Task Scheduling in
a Heterogeneous Cluster. Information Sciences, 319:113–131, 2015.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:30:14 UTC from IEEE Xplore.  Restrictions apply. 
94