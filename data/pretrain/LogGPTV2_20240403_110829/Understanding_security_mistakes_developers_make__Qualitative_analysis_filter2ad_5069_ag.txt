coredump.cx/afl/, 2014.
A Additional Contest Details
To provide additional context for our results, this appendix
includes a more thorough breakdown of the sampled popu-
lation along with the number of breaks and vulnerabilities
for each competition. Table 4 presents statistics for sampled
teams, participant demographics, and counts of break sub-
missions and unique vulnerabilities introduced divided by
competition. Figure 2 shows the variation in team sizes across
competitions.
B Additional Coding
We coded several variables in addition to those found to have
signiﬁcant effect on vulnerability types introduced. This ap-
pendix describes the full set of variables coded. Table 5 pro-
vides a summary of all variables.
Hard to read code is a potential reason for vulnerability
introduction. If team members cannot comprehend the code,
then resulting misunderstandings could cause more vulnera-
bilities. To determine whether this occurred, we coded each
project according to several readability measures. These in-
cluded whether the project was broken into several single-
function sub-components (Modularity), whether the team
used variable and function names representative of their se-
mantic roles (Variable Naming), whether whitespace was
124    29th USENIX Security Symposium
USENIX Association
Contest
# Teams
# Contestants
% Male
% Female
Age
% with CS degrees
Years programming
Team size
# PLs known per team
% MOOC
# Breaks
# Vulnerabilities
Fall 14 (SL)
10
26
46 %
12 %
22.9/18/30
85 %
2.9/1/4
2.6/1/6
6.4/3/14
0%
30
12
Spring 15 (SL)
42
100
92 %
4 %
35.3/20/58
39 %
9.7/0/30
2.4/1/5
6.9/1/22
100 %
334
41
Fall 15 (SC)
27
86
87 %
8 %
32.9/17/56
35 %
9.6/2/37
3.2/1/5
8.0/2/17
91 %
242
64
Fall 16 (MD)
15
35
80 %
3 %
24.5/18/40
57 %
9.6/3/21
2.3/1/8
7.9/1/17
53 %
260
65
Total
94
247
84 %
6 %
30.1/17/58
45 %
8.9/0/37
2.7/1/8
7.4/1/22
76 %
866
182
Table 4: Participants demographics from sampled teams with the number of breaks submitted and vulnerabilities introduced per
competition. Some participants declined to specify gender. Slashed values represent mean/min/max
Variable
Modular
Levels Description
T / F Whether the project is segmented into a set of functions and classes each performing
small subcomponents of the project
Variable Naming
Whitespace
T / F Whether the author used variable names indicating the purpose of the variable
T / F Whether the author used whitespace (i.e., indentation and new lines) to allow the reader
Comments
Economy of Mechanism
Minimal Trusted Code
T / F Whether the author included comments to explain blocks of the project
T / F
How complicated are the implementations of security relevant functions
T / F Whether security relevant functions are implemented once or multiple times
to easily infer control-ﬂow and variable scope
Alpha
1
1
1
0.89
0.84
0.84
Table 5: Summary of the project codebook.
was economical. For example, one project submitted to the
secure log problem added a constant string to the end of each
access log event before encrypting. In addition to using a mes-
sage authentication code to ensure integrity, they checked that
this hardcoded string was unchanged as part of their integrity
check. Because removing this unnecessary step would not
sacriﬁce security, we coded this project as not economical.
Minimal Trusted Code was measured by checking whether
the security-relevant functionality was implemented in multi-
ple locations. Projects passed if they created a single function
for each security requirement (e.g., encryption, access con-
trol checks, etc.) and called it throughout. The alternative—
copying and pasting code wherever security functionality was
needed—is likely to lead to mistakes if each code segment is
not updated whenever changes are necessary.
C Regression Analysis
For each vulnerability type subclass, we performed a pois-
son regression [15, 67-106] to understand whether the team’s
characteristics or their programming decisions inﬂuenced the
vulnerabilities introduced. In this appendix, we provide an
extended analysis discussion, focusing on the full set of co-
variates in each initial model, our model selection process,
and the results omitted from the main paper due to their lack
Figure 2: Histogram of team size by competition.
used support visualization of control-ﬂow and variable scope
(Whitespace), and whether comments were included to sum-
marize relevant details (Comments).
Additionally, we identiﬁed whether projects followed se-
cure development best practices [12, pg. 32-36], speciﬁcally
Economy of Mechanism and Minimal Trusted Code.
When coding Economy of Mechanism, if the reviewer
judged the project only included necessary steps to provide
the intended security properties, then the project’s security
USENIX Association
29th USENIX Security Symposium    125
of signiﬁcant results or poor model ﬁt.
Initial Covariates
C.1
As a baseline, all initial regression models included factors for
the language used (Type Safety and Popularity), team charac-
teristics (development experience and security education), and
the associated problem. These base covariates were used to
understand the effect of a team’s intrinsic characteristics, their
development environment, and the problem speciﬁcation. The
Type Safety variable identiﬁed whether each project was stati-
cally typed (e.g., Java or Go, but not C or C++), dynamically
typed (e.g., Python, Ruby), or C/C++ (Type Safety).
For Misunderstanding regressions, the Bad Choice regres-
sion only included the baseline covariates and the Conceptual
Error regression added the library type (Library Type). The
project’s Library Type was one of three categories based on
the libraries used (Library Type): no library used (None), a
standard language library (e.g., PyCrypto for Python) (Lan-
guage), or a non-standard library (3rd Party).
The No Implementation regressions only included the base-
line covariates. Additionally, since the Some Intuitive vulnera-
bilities only occurred in the MD problem, we did not include
problem as a covariate in the Some Intuitive regression.
In addition to the baseline covariates, the Mistake regres-
sion added the Minimal Trusted Code and Economy of Mech-
anism variables, whether the team used test cases during the
build phase, and the project’s number of lines of code. These
additional covariates were chosen as we expect smaller, sim-
pler, and more rigorously tested code to include less mistakes.
C.2 Model Selection
We calculated the Bayseian Information Criterion (BIC)—a
standard metric for model ﬁt [63]—for all possible combina-
tions of the initial factors. To determine the optimal model
and avoid overﬁtting, we selected the minimum BIC model.
As our study is semi-controlled, there are a large number
of covariates which must be accounted for in each regression.
Therefore, our regressions were only able to identify large
effects [21]. Note, for this reason, we also did not include any
interaction variables. Including interaction variables would
have reduced the power of each model signiﬁcantly and pre-
cluded ﬁnding even very large effects. Further, due to the
sparse nature of our data (e.g., many languages and libraries
were used, in many cases only by one team), some covari-
ates could only be included in an aggregated form, limiting
the analysis speciﬁcity. Future work should consider these
interactions and more detailed questions.
C.3 Results
Tables 6–10 provide the results of each regression not in-
cluded in the main text.
Variable
Value
Log
Estimate
CI
p-value
Popularity
C (91.5)
1.03
[0.98, 1.09]
0.23
*Signiﬁcant effect
deﬁnition)
– Base case (Estimate=1, by
Table 6: Summary of regression over Bad Choice vulnerabili-
ties. Pseudo R2 measures for this model were 0.02 (McFad-
den) and 0.03 (Nagelkerke).
Variable
Value
MOOC
False
True
*Signiﬁcant effect
by deﬁnition)
Log
Estimate
CI
p-value
–
1.76
–
[0.70, 4.34]
–
0.23
– Base case (Estimate=1,
Table 7: Summary of regression over Conceptual Error vul-
nerabilities. Pseudo R2 measures for this model were 0.01
(McFadden) and 0.02 (Nagelkerke).
Variable
Value
Log
Estimate
CI
p-value
Yrs. Experience
8.9
1.12
[0.82, 1.55]
0.47
*Signiﬁcant effect
inition)
– Base case (Estimate=1, by def-
Table 8: Summary of regression over All Intuitive vulnerabili-
ties. Pseudo R2 measures for this model were 0.06 (McFad-
den) and 0.06 (Nagelkerke).
Variable
Value
Problem
SC
SL
*Signiﬁcant effect
by deﬁnition)
Log
Estimate
CI
p-value
–
1.02
–
[0.98, 1.07]
–
0.373
– Base case (Estimate=1,
Table 9: Summary of regression over Some Intuitive vulnera-
bilities. Pseudo R2 measures for this model were 0.02 (Mc-
Fadden) and 0.07 (Nagelkerke).
Variable
Value
Log
Estimate
CI
Problem
SC
MD
SL
*Signiﬁcant effect
by deﬁnition)
–
0.58
0.31
–
[0.25, 1.35]
[0.15, 0.60]
p-value
–
0.21
< 0.001*
– Base case (Estimate=1,
Table 10: Summary of regression over Unintuitive vulnerabil-
ities. Pseudo R2 measures for this model were 0.07 (McFad-
den) and 0.16 (Nagelkerke).
126    29th USENIX Security Symposium
USENIX Association