### A. Additional Contest Details

To provide additional context for our results, this appendix includes a more thorough breakdown of the sampled population, along with the number of breaks and vulnerabilities for each competition. Table 4 presents statistics for sampled teams, participant demographics, and counts of break submissions and unique vulnerabilities introduced, divided by competition. Figure 2 illustrates the variation in team sizes across competitions.

### B. Additional Coding

In addition to the variables found to have a significant effect on vulnerability types introduced, we coded several other variables. This appendix describes the full set of variables, summarized in Table 5.

**Hard-to-Read Code as a Potential Reason for Vulnerability Introduction:**

If team members cannot comprehend the code, misunderstandings can lead to more vulnerabilities. To determine whether this occurred, we coded each project according to several readability measures:
- **Modularity:** Whether the project was broken into several single-function sub-components.
- **Variable Naming:** Whether the team used variable and function names representative of their semantic roles.
- **Whitespace:** Whether whitespace (i.e., indentation and new lines) was used to allow the reader to easily infer control flow and variable scope.
- **Comments:** Whether the author included comments to explain blocks of the project.
- **Economy of Mechanism:** How complicated are the implementations of security-relevant functions.
- **Minimal Trusted Code:** Whether security-relevant functions are implemented once or multiple times.

**Example of Economy of Mechanism:**

One project submitted to the secure log problem added a constant string to the end of each access log event before encrypting. In addition to using a message authentication code to ensure integrity, they checked that this hardcoded string was unchanged as part of their integrity check. Because removing this unnecessary step would not sacrifice security, we coded this project as not economical.

**Minimal Trusted Code:**

This was measured by checking whether the security-relevant functionality was implemented in multiple locations. Projects passed if they created a single function for each security requirement (e.g., encryption, access control checks, etc.) and called it throughout. The alternative—copying and pasting code wherever security functionality was needed—is likely to lead to mistakes if each code segment is not updated whenever changes are necessary.

### C. Regression Analysis

For each vulnerability type subclass, we performed a Poisson regression [15, 67-106] to understand whether the team’s characteristics or their programming decisions influenced the vulnerabilities introduced. This appendix provides an extended analysis discussion, focusing on the full set of covariates in each initial model, our model selection process, and the results omitted from the main paper due to their lack of significant results or poor model fit.

#### Initial Covariates

As a baseline, all initial regression models included factors for the language used (Type Safety and Popularity), team characteristics (development experience and security education), and the associated problem. These base covariates were used to understand the effect of a team’s intrinsic characteristics, their development environment, and the problem specification.

- **Type Safety:** Identified whether each project was statically typed (e.g., Java or Go, but not C or C++), dynamically typed (e.g., Python, Ruby), or C/C++.
- **Misunderstanding Regressions:** The Bad Choice regression only included the baseline covariates, while the Conceptual Error regression added the library type (Library Type).
- **No Implementation Regressions:** Only included the baseline covariates. Since Some Intuitive vulnerabilities only occurred in the MD problem, we did not include the problem as a covariate in the Some Intuitive regression.
- **Mistake Regressions:** Added the Minimal Trusted Code and Economy of Mechanism variables, whether the team used test cases during the build phase, and the project’s number of lines of code.

#### Model Selection

We calculated the Bayesian Information Criterion (BIC)—a standard metric for model fit [63]—for all possible combinations of the initial factors. To determine the optimal model and avoid overfitting, we selected the minimum BIC model. Given the semi-controlled nature of our study, there are a large number of covariates that must be accounted for in each regression. Therefore, our regressions were only able to identify large effects [21]. We did not include any interaction variables to maintain the power of each model and find even very large effects. Due to the sparse nature of our data, some covariates could only be included in an aggregated form, limiting the analysis specificity. Future work should consider these interactions and more detailed questions.

#### Results

Tables 6–10 provide the results of each regression not included in the main text.

**Table 6: Summary of Regression over Bad Choice Vulnerabilities**
- Pseudo R² measures for this model were 0.02 (McFadden) and 0.03 (Nagelkerke).

| Variable | Value | Log Estimate | CI | p-value |
|----------|-------|--------------|----|---------|
| Popularity | C (91.5) | 1.03 | [0.98, 1.09] | 0.23 |

**Table 7: Summary of Regression over Conceptual Error Vulnerabilities**
- Pseudo R² measures for this model were 0.01 (McFadden) and 0.02 (Nagelkerke).

| Variable | Value | Log Estimate | CI | p-value |
|----------|-------|--------------|----|---------|
| MOOC | False | 1.76 | [0.70, 4.34] | 0.23 |
| MOOC | True | - | - | - |

**Table 8: Summary of Regression over All Intuitive Vulnerabilities**
- Pseudo R² measures for this model were 0.06 (McFadden) and 0.06 (Nagelkerke).

| Variable | Value | Log Estimate | CI | p-value |
|----------|-------|--------------|----|---------|
| Yrs. Experience | 8.9 | 1.12 | [0.82, 1.55] | 0.47 |

**Table 9: Summary of Regression over Some Intuitive Vulnerabilities**
- Pseudo R² measures for this model were 0.02 (McFadden) and 0.07 (Nagelkerke).

| Variable | Value | Log Estimate | CI | p-value |
|----------|-------|--------------|----|---------|
| Problem | SC | 1.02 | [0.98, 1.07] | 0.373 |
| Problem | SL | - | - | - |

**Table 10: Summary of Regression over Unintuitive Vulnerabilities**
- Pseudo R² measures for this model were 0.07 (McFadden) and 0.16 (Nagelkerke).

| Variable | Value | Log Estimate | CI | p-value |
|----------|-------|--------------|----|---------|
| Problem | SC | 0.58 | [0.25, 1.35] | 0.21 |
| Problem | MD | 0.31 | [0.15, 0.60] | < 0.001* |
| Problem | SL | - | - | - |

---

**Figure 2: Histogram of Team Size by Competition**

---

**Table 4: Participants Demographics from Sampled Teams with the Number of Breaks Submitted and Vulnerabilities Introduced per Competition**

| Contest | # Teams | # Contestants | % Male | % Female | Age (mean/min/max) | % with CS degrees | Years programming (mean/min/max) | Team size (mean/min/max) | # PLs known per team (mean/min/max) | % MOOC | # Breaks | # Vulnerabilities |
|---------|---------|---------------|--------|----------|--------------------|------------------|---------------------------------|--------------------------|-----------------------------------|--------|----------|------------------|
| Fall 14 (SL) | 10 | 26 | 46% | 12% | 22.9/18/30 | 85% | 2.9/1/4 | 2.6/1/6 | 6.4/3/14 | 0% | 30 | 12 |
| Spring 15 (SL) | 42 | 100 | 92% | 4% | 35.3/20/58 | 39% | 9.7/0/30 | 2.4/1/5 | 6.9/1/22 | 100% | 334 | 41 |
| Fall 15 (SC) | 27 | 86 | 87% | 8% | 32.9/17/56 | 35% | 9.6/2/37 | 3.2/1/5 | 8.0/2/17 | 91% | 242 | 64 |
| Fall 16 (MD) | 15 | 35 | 80% | 3% | 24.5/18/40 | 57% | 9.6/3/21 | 2.3/1/8 | 7.9/1/17 | 53% | 260 | 65 |
| Total | 94 | 247 | 84% | 6% | 30.1/17/58 | 45% | 8.9/0/37 | 2.7/1/8 | 7.4/1/22 | 76% | 866 | 182 |

---

**Table 5: Summary of the Project Codebook**

| Variable | Levels | Description |
|----------|--------|-------------|
| Modular | T/F | Whether the project is segmented into a set of functions and classes each performing small subcomponents of the project. |
| Variable Naming | T/F | Whether the author used variable names indicating the purpose of the variable. |
| Whitespace | T/F | Whether the author used whitespace (i.e., indentation and new lines) to allow the reader to easily infer control flow and variable scope. |
| Comments | T/F | Whether the author included comments to explain blocks of the project. |
| Economy of Mechanism | T/F | How complicated are the implementations of security-relevant functions. |
| Minimal Trusted Code | T/F | Whether security-relevant functions are implemented once or multiple times. |

---

**USENIX Association**
**29th USENIX Security Symposium**