Please make sure that the boxes below are checked before you submit your
issue. If your issue is an implementation question, please ask your question
on StackOverflow or join the Keras Slack channel and ask there instead of
filing a GitHub issue.
Thank you!
  * Check that you are up-to-date with the master branch of Keras. You can update with:  
pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps
  * If running on TensorFlow, check that you are up-to-date with the latest version. The installation instructions can be found here.
  * If running on Theano, check that you are up-to-date with the master branch of Theano. You can update with:  
pip install git+git://github.com/Theano/Theano.git --upgrade --no-deps
  * Provide a link to a GitHub Gist of a Python script that can reproduce your issue (or just copy the script here if it is short).
* * *
If I create a layer which is shared by two models that are incorporated into
another model, then when compiling against the CNTK backend, I get an
exception when falling `fit`. The following simple example works fine against
the Theano and Tensorflow backends, but fails against CNTK:
    x1 = keras.layers.Input((1,))
    x2 = keras.layers.Input((1,))
    x3 = keras.layers.Input((1,))
    l = keras.layers.Dense(2)
    m1 = keras.Model([x1], [l(x1)])
    m2 = keras.Model([x2], [l(x2)])
    m = keras.Model([x3], [keras.layers.concatenate([m1(x3), m2(x3)])])
    m.compile('sgd', 'mse')
    m.summary()
    m.fit([np.ones((1,1))], np.ones((1,4)))
Here's the error I get:
> ValueError: Learner's parameters list must not contain duplicates.
>
> [CALL STACK]  
>  > CNTK::TrainingParameterSchedule:: GetMinibatchSize  
>  \- CNTK:: ZerosLike (x2)  
>  \- CNTK:: UniversalLearner  
>  \- PyInit__cntk_py (x2)  
>  \- PyCFunction_FastCallDict  
>  \- PyObject_GenericGetAttr  
>  \- PyEval_EvalFrameDefault  
>  \- PyErr_Occurred  
>  \- PyFunction_FastCallDict  
>  \- PyObject_CallFunctionObjArgs  
>  \- PyUnicode_AsUTF8AndSize  
>  \- PyFrame_New  
>  \- PyEval_EvalFrameDefault  
>  \- PyErr_Occurred
Note that the summary call correctly indicates that there are only 4 trainable
parameters (because the layer is shared between the two models):
    __________________________________________________________________________________________________
    Layer (type)                    Output Shape         Param #     Connected to
    ==================================================================================================
    input_3 (InputLayer)            (None, 1)            0
    __________________________________________________________________________________________________
    model_1 (Model)                 (None, 2)            4           input_3[0][0]
    __________________________________________________________________________________________________
    model_2 (Model)                 (None, 2)            4           input_3[0][0]
    __________________________________________________________________________________________________
    concatenate_1 (Concatenate)     (None, 4)            0           model_1[1][0]
                                                                     model_2[1][0]
    ==================================================================================================
    Total params: 4
    Trainable params: 4
    Non-trainable params: 0