### Passive and Active DNS Features

The last seen query, takedown date, and the start of the AGD validity period are recorded. Additionally, we document the presence of at least one passive DNS query for resource records A, AAAA, CNAME, MX, NS, SOA, and TXT. More (requested) record types with values indicate a properly set up and used domain.

**References:**
- [Disposable Email Domains](https://github.com/ivolo/disposable-email-domains)
- [NumVerify](https://numverify.com/)
- [Whois DomainTools](https://whois.domaintools.com/)

### Passive DNS Data

The features in this set utilize passive DNS data generously provided by Farsight Security. We retrieve aggregated data spanning the full data collection period (since 2010). For each resource record value observed, the aggregated data includes the number of queries and the timestamps when it was first and last seen.

### Active DNS Features

Seven active DNS features capture the availability of DNS records for a particular domain. Two features are based on the time between the first seen DNS record and the takedown date, and the start of the AGD validity period, respectively. We also record the number of days any DNS record value was seen for resource records A, AAAA, MX, NS, and SOA.

The features in this set use active DNS data provided by the OpenINTEL project. The data period is capped at 333 days (starting from January 1 of the relevant year). While OpenINTEL collects data actively, it complies with our requirement to not contact domains directly. The data collection is comprehensive enough to cover most registered Avalanche domains.

**References:**
- [Farsight Security DNSDB](https://www.farsightsecurity.com/solutions/dnsdb/)
- [OpenINTEL](https://www.openintel.nl/)

### Omitted Features

Given our use case of proactive takedowns, we cannot consider features that detect ongoing malicious operations directly, as the maliciously registered domain may not yet exhibit such behavior at the time of takedown. Malicious actors can leave these domains dormant until a DGA generates the domain and infected hosts start contacting it. This means we do not verify whether a C&C server is running on the domain or check malware blacklists.

Approaches for detecting AGDs, especially per single domain, often rely on lexical features that seek to discover patterns unlikely to occur in "human-generated" domain names. However, since all our candidate domains are generated by a DGA, we use only a limited set of lexical features to find potential collisions (short and few digits).

Detecting fast flux services from DNS logs is not applicable, as the malicious domains would only start operating in fast flux during the validity period of the AGD. We do not use approaches and features that rely on clustering domains and batches of similar registrations, such as timing patterns or shared registrars, following our observation that bulk patterns do not apply to malware domains.

The type of network could be an appropriate feature while the domain is active, with more trust in government or business networks hosting benign sites and domains in residential networks potentially being hosted by an infected machine. However, a maliciously registered domain does not need to be actively malicious before the DGA generates the domain, and its IP address can easily be set to a benign network, misleading our classifier.

Data collected through a crawl of candidate domains, such as properties of the site content, could indicate legitimately used domains. However, due to the need for historical data and our stealth constraint, we cannot do an active crawl of domains ourselves. We also cannot rely on existing third-party repositories of website crawls (e.g., the Internet Archive, Common Crawl, or Censys) because they do not provide historical data, do not crawl sufficiently regularly, and do not have consistent domain coverage.

We do not include the malware family as a feature, as Avalanche provided domain registration as a service, and we do not expect differences in behavior between the 21 supported malware families. Such a feature would go against our goal of capturing general differences in behavior between benign and malicious domains. We design other features to represent distributions, which the model can interpret, whereas the malware family feature can only refine the model for specific families. Finally, benign domains can accidentally "belong" to a certain malware family, making the feature irrelevant in terms of registration behavior.

### Analysis of Machine Learning-Based Classification

To evaluate the extent to which machine learning-based approaches can reduce the effort of law enforcement in executing a takedown, we develop and evaluate a classifier that decides whether future DGA domains are likely to be benign or malicious. Our analysis has three goals: evaluating the raw performance of the classifier, gaining insights into its decision-making process, and thoroughly assessing the benefits and limitations of automated approaches for domain classification. Given that not all data sources are equally easy to collect, we also assess their impact on the correctness of our classification.

### Experimental Protocol

We designed an experimental protocol to determine the most appropriate machine learning-based solution and evaluate it in a way that is accurate and representative of real-world takedowns. We run our experimental protocol for all domains of the 2017, 2018, and 2019 takedown iterations. We only evaluate performance with the manually labeled ground truth obtained from law enforcement for the 2017 and 2018 iterations. In 2019, our model was used in the real-world classification effort, so a performance evaluation would be biased.

As we want to measure the performance of our approach as if it were deployed at the time of the takedown operation, we use historical data reflecting the state of the domains as of each takedown (November 30 of each year). Data for malicious domains collected after the takedown would refer to sinkholing and domain transfer infrastructure, which would heavily bias our classifier.

Since we cannot obtain all data sets for all domains, we develop an ensemble model. We train a model for each combination of available feature sets, where a domain is included in the training set if at least those data sets are available. To classify a domain, we use the output of the model of the domainâ€™s available data sets.

### Results

Given that we are the first to analyze the specific issue of preemptively deciding whether DGA domains are actually malicious or accidentally benign for a real-world takedown, we cannot compare our performance results with previous work. Instead, we critically examine how its performance translates into a real-world reduction in effort, whether our solution correctly captures differences between benign and malicious domains, and how much it depends on the availability of different data sets.

#### Model Performance

Appendix B lists the relative performance of the four machine learning algorithms we evaluated: decision tree, gradient boosted tree, random forest, and support vector machine. We conclude that a gradient boosted tree classifier yields the best performance while still being sufficiently interpretable. We therefore analyze only its results.

We first train a base ensemble model, varying the training and test sets over the 2017 and 2018 iterations. From the performance metrics in Table V, we observe concept drift: performance drops when deploying our model across iterations instead of within. This suggests that over time, patterns distinguishing benign and malicious actors emerge or change, and these are not captured by a model trained on only a single iteration.

We then develop an extended ensemble model, combining ground truth from a previous iteration with manual, a priori classifications of a subset of domains in the target iteration. This enables us to improve model performance by capturing the novel patterns in the new iteration, while still reducing manual effort overall.

We evaluate this extended model trained on all of the 2017 and part of the 2018 ground truth and tested on the remaining 2018 domains. Based on Figure 3, we empirically set the proportion of the 2018 ground truth that is randomly selected.

#### Performance Metrics

Table V shows the performance metrics for the base ensemble model, varying the training and test sets over the 2017 and 2018 iterations.

| Training | Test | Accuracy | F1 Score | Precision | Recall |
|----------|------|----------|----------|-----------|--------|
| 2017     | 2018 | 93.4%    | 84.3%    | 92.6%     | 73.4%  |
| 2018     | 2017 | 92.6%    | 70.8%    | 92.7%     | 76.1%  |

Table VI shows the performance metrics for models trained on the 2017 and (for the extended model) 15% of the 2018 iteration.

| Ensemble Model | Accuracy | F1 Score | Precision | FNR | Recall | FPR | Effort Reduction |
|----------------|----------|----------|-----------|-----|--------|-----|------------------|
| Base           | 84.3%    | 73.4%    | 70.8%     | 23.9% | 76.1%  | 12.4% | 100.0%           |
| Extended       | 86.4%    | 78.6%    | 70.5%     | 2.0% | 88.6%  | 2.4% | 85.0%            |
| a priori       | 97.3%    | 95.3%    | 94.2%     | 2.3% | 96.5%  | 2.3% | 70.3%            |
| a posteriori   | 97.6%    | 95.8%    | 94.3%     | 2.6% | 97.4%  | 2.3% | 66.2%            |

By choosing a target maximum FNR and FPR, we can determine the lower and upper bounds on the maliciousness score. Domains with scores within these bounds require manual verification, while domains with lower and higher scores are automatically classified as benign and malicious, respectively.

For the extended model on domains with all data sets available, when setting a 2% error tolerance, 55.5% of domains have a maliciousness score below the lower bound set by 2% FPR (i.e., are benign), while 27.1% of domains exceed the upper bound set by 2% FNR (i.e., are malicious). Therefore, 82.6% of domains no longer need to be manually inspected, and only 17.4% of domains still require further manual investigation.

When applying this a posteriori approach to the extended ensemble model evaluated on all domains from the 2017 and part of the 2018 iteration, we achieve an accuracy of 97.6%. The effective FNR and FPR are 2.6% and 2.3%, comparable to the target error rate of 2%.

Overall, this approach reduces manual effort by 66.2%, accounting for the 15% of domains manually classified a priori. When the error tolerance is 1% and 0.5%, the fraction of automatically classified domains is 52.5% and 35.7%, respectively. The score thresholds become very strict when very low error tolerances must be maintained, reducing the fraction of domains that can be automatically classified. The comparable effort reduction for an ensemble model trained on the 2017 and 2018 and tested on the 2019 iteration with a 2% error tolerance amounts to 76.9%, achieving a significant reduction in manual effort.