last seen query, the takedown date, and the start of the AGD
validity period respectively. Finally, we record the presence of
at least one passive DNS query for resource records A, AAAA,
CNAME, MX, NS, SOA, and TXT: more (requested) record types
with a value indicate proper domain setup and usage.
8https://github.com/ivolo/disposable-email-domains
9https://numverify.com/
10https://whois.domaintools.com/
7
The features in this set use passive DNS data generously
provided to us by Farsight Security11. We retrieve aggregated
data spanning the full data collection period (i.e., since
2010 [32]). For each resource record value seen, the aggregated
data contains the number of queries and the timestamps when
it was ﬁrst and last seen.
f) Seven active DNS features capture the availability of
DNS records for a particular domain. We base two features on
the time between the ﬁrst seen DNS record and the takedown
date, and the start of the AGD validity period respectively. We
also record the number of days any DNS record value was
seen for resource records A, AAAA, MX, NS, and SOA.
The features in this set use active DNS data generously
provided to us by the OpenINTEL12 project [91]. We cap the
data period at 333 days (i.e. starting from January 1 of the
relevant year). While OpenINTEL collects data actively, it
complies with our requirement that we do not contact domains
ourselves. Moreover, data collection is not targeted at speciﬁc
domains, yet sufﬁciently comprehensive to also capture most
of the registered Avalanche domains as it covers full zone ﬁles.
D. Omitted features
Given our use case of proactive takedowns, we cannot
consider features that try to detect ongoing malicious operations
directly, as the maliciously registered domain does not yet
necessarily exhibit such behavior at the time of the takedown:
malicious actors can leave these domains dormant right until a
DGA generates the domain and infected hosts start contacting
the domain. This means for example that we do not verify
whether a C&C server is running on the domain and do not
check malware blacklists.
Approaches for detecting AGDs, especially per single
domain, are often based on lexical features that seek to discover
patterns unlikely to occur in “human-generated” domain
names [77], [78]. However, all of our candidate domains have
been generated by a DGA, which leads us to use only a limited
set of lexical features to ﬁnd the domains that are more likely
to be potential collisions (short and few digits).
Detecting patterns from DNS logs [20] that indicate fast
ﬂux services [41], often used by command and control servers,
is not applicable as the malicious domains would only start
operating in fast ﬂux during the validity period of the AGD.
Following our observation from Section III-B that bulk
patterns do not apply for malware domains, we do not use
approaches and features that rely on clustering domains [16]
and batches of similar registrations [38], such as timing patterns
or shared registrars.
The type of network could be an appropriate feature to
take into account while the domain is active [20], with more
trust in government or business networks hosting benign sites
and domains in residential networks potentially being hosted
by an infected machine. However, as a maliciously registered
domain does not yet have to be actively malicious before the
DGA generates the domain, its IP address can easily be set to
11https://www.farsightsecurity.com/solutions/dnsdb/
12https://www.openintel.nl/
a benign network (without the need for that network to actually
host the domain) [62], thereby misleading our classiﬁer.
Data collected through a crawl of candidate domains such
as properties of the site content could indicate legitimately used
domains [47]. However, following our stealth constraint from
Section III-B and due to the need for historical data, we cannot
do an active crawl of domains ourselves. We also cannot rely
on existing third-party repositories of website crawls (e.g. the
Internet Archive [2], Common Crawl [25] or Censys [30]): they
do not provide historical data, do not crawl sufﬁciently regularly
to capture recent data, do not have a consistent set of crawled
domains and/or do not have sufﬁcient domain coverage. Their
data would therefore not be comprehensively representative of
domain web content at the time of the takedown.
We do not include the malware family as a feature: as
Avalanche provided domain registration as a service [3], we do
not expect differences in behavior between the 21 supported
malware families. Moreover, such a feature would go against
our goal of capturing general differences in behavior between
benign and malicious domains. We design the other features
to represent distributions, for which the model can interpret
the differences, whereas the malware family feature can only
serve to reﬁne the model for speciﬁc families. Finally, benign
domains accidentally ‘belong’ to a certain malware family, so
the feature is irrelevant in terms of registration behavior. We
already capture relevant characteristics of the DGA in derived
features such as the domain length that capture randomness in
generated domains and therefore the likelihood of collisions.
We want to evaluate our approach as if it were deployed at
the time of the takedown, so we do not use features for which we
lack available historical data, as we would only be able to obtain
the current state, which for malicious domains is post-takedown.
They include the features that require active probing or data
collection such as the website properties discussed earlier or
the existence of search engine results for the domain, which
could serve as an additional indicator of popularity. However,
if they meet the applicable requirements and constraints, we
can add such features in an actual takedown as we can then
collect accurate data.
V. ANALYSIS OF MACHINE LEARNING-BASED
CLASSIFICATION
To evaluate to what extent machine-learning based ap-
proaches can reduce the effort of law enforcement to execute
a takedown, we develop and evaluate a classiﬁer that decides
whether future DGA domains are likely to be benign or
malicious. The goals of our analysis are threefold: we want to
evaluate the raw performance of the classiﬁer, but also gain
insights into its decision-making process to ﬁnally thoroughly
assess the beneﬁts and limitations of automated approaches
for domain classiﬁcation. Moreover, given that not all data
sources are equally easy to collect, we assess their impact on
the correctness of our classiﬁcation.
A. Experimental protocol
We ﬁrst design an experimental protocol to determine the
most appropriate machine learning-based solution and evaluate
it in a way that is accurate and representative of real-world
takedowns. Given the investigative setting and our intention to
8
We run our experimental protocol for all domains of the
2017, 2018 and 2019 takedown iterations. We only evaluate
performance with the manually labeled ground truth that we
obtained from law enforcement for the 2017 and 2018 iterations
(Section III-C). In 2019, our model was used in the real-world
classiﬁcation effort, so a performance evaluation would be
biased since we contributed to the ground truth.
As we want to measure the performance of our approach
as if it were deployed at the time of the takedown operation,
we use historical data that reﬂects the state of the domains as
of each takedown, i.e. November 30 of each year. Data for the
malicious domains collected after the takedown would refer
to sinkholing and domain transfer infrastructure, making it a
signal for maliciousness that would heavily bias our classiﬁer.
As shown in Figure 2, we cannot obtain all data sets for
all domains: this is because the third-party source could not
collect relevant data (e.g. no WHOIS record is available or the
domain was never seen at passive DNS sensors). In order to still
generate a prediction for all domains, we develop an ensemble
model. We train a model for each combination of available
feature sets, where a domain is included in the training set if
at least those data sets are available. To classify a domain, we
use the output of the model of the domain’s available data sets.
B. Results
Given that we are the ﬁrst to analyze the speciﬁc issue
of preemptively deciding whether DGA domains are actually
malicious or accidentally benign for a real-world takedown
(which brings about certain constraints), we are not able to
compare our performance results with previous work. Instead,
we go beyond reporting basic metrics and critically examine
how its performance translates into a real-world reduction
in effort, whether our solution correctly captures differences
between benign and malicious domains, and how much it
depends on the availability of different data sets.
a) Model performance: Appendix B lists the relative
performance of the four machine learning algorithms that we
evaluate: we conclude that a gradient boosted tree classiﬁer
yields the best performance while still being sufﬁciently
interpretable. We therefore analyze only its results.
We ﬁrst train a base ensemble model, varying the training
and test sets over the 2017 and 2018 iterations. From the
performance metrics in Table V, we can see that concept
drift [95] occurs: performance drops when deploying our model
across iterations instead of within. This suggests that over time,
patterns that distinguish benign and malicious actors emerge
or change, and these are therefore not captured by a model
trained on only a single iteration.
We therefore develop an extended ensemble model, where
we combine ground truth from a previous iteration with manual,
a priori classiﬁcations of a subset of domains in the target
iteration. This enables us to improve model performance by
capturing the novel patterns in the new iteration, while still
reducing manual effort overall.
We evaluate this extended model trained on all of the 2017
and part of the 2018 ground truth and tested on the remaining
2018 domains. Based on Figure 3, we empirically set the
proportion of the 2018 ground truth that is (randomly) selected
Fig. 2. Number of domains where certain data sets are available, after
removing sinkholed domains, for the 2017 and 2018 iterations. We separately
mark the remainder of domains where only the joint data set (comprising
lexical, popularity-based, and Certiﬁcate Transparency features) is available.
thoroughly analyze the resulting model, we restrict our selection
of machine learning algorithms to those that are sufﬁciently
interpretable. Moreover, as we systematically develop high-
level features that capture the full domain life cycle, we do not
require automated feature engineering. Therefore, we would not
beneﬁt from a deep learning approach and only face drawbacks
from its increased complexity, so we do not consider it further.
Before classifying benign and malicious domains, we
discard domains that were already sinkholed by security
organizations to study botnet behavior. These organizations
can sinkhole the domains either because they detect that botnet
hosts are already contacting the domain (whose validity period
therefore starts before and extends beyond the takedown date),
or because they generate the domains output by the DGA
upfront. The sinkholed domains can be considered neither a
benign collision, as they do not host real content and may even
mimic the malware C&C server, nor a registration made with
malicious intent, as they will not communicate with actual
malware. This means that they would confuse our model, and
should be removed upfront by preprocessing the data. We detect
sinkholed domains by matching DNS and WHOIS records with
those of the sinkhole providers collected in SinkDB [10], by
Alowaisheq et al. [12], and by Stampar et al. [87], [88]. Table II
summarizes the distribution of domains across classes.
We execute our protocol with four machine learning
algorithms: decision tree, gradient boosted tree, random forest,
and support vector machine. We split data sets in a training and
test set according to the considered iterations. When training
and testing on the same iteration, we split the ground truth
according to a 10-fold cross validation procedure. Otherwise,
we construct the training and test sets from the separate iteration
ground truths as applicable. We perform all model training and
analysis using scikit-learn [67]. We elaborate on the
different steps of this protocol in Appendix A.
9
88013313601151PassiveDNSActiveDNSWHOISJoint6Total13972017Benign980173143790588PassiveDNSActiveDNSWHOISJoint64Total11452017Malicious3072140870PassiveDNSActiveDNSWHOISJoint118Total10142018Benign0011110382PassiveDNSActiveDNSWHOISJoint7Total4022018MaliciousTABLE V.
PERFORMANCE METRICS FOR THE BASE ENSEMBLE MODEL, VARYING THE TRAINING AND TEST SET OVER THE 2017 AND 2018 ITERATIONS.
Training
Test
Accuracy
F1 score
Precision
Recall
2017
2018
2018
2017
2018
93.4% 84.3% 92.6% 73.4% 92.6% 70.8% 92.7% 76.1%
76.1% 96.3% 70.9% 93.5% 78.6% 92.7% 64.6% 94.3%
2017
2018
2017
2017
2018
TABLE VI.
PERFORMANCE METRICS FOR MODELS TRAINED ON THE 2017 AND (FOR THE EXTENDED MODEL) 15% OF THE 2018 ITERATION.
Ensemble model
Base
Extended
Base
Extended
a priori
a posteriori
a priori + a posteriori
Accuracy
84.3%
86.4%
97.3%
97.6%
F1 score
73.4%
78.6%
95.3%
95.8%
Precision
70.8%
70.5%
94.2%
94.3%
FNR
Recall
FPR
76.1% 23.9% 12.4%
88.6%
2.0%
2.4%
96.5%
97.4%
2.3%
2.3%
3.5%
2.6%
Effort reduction
100.0%
85.0%
70.3%
66.2%
We explain this approach using the extended model for
domains where all data sets are available, which allows us to
simplify and visually support our explanation, but then apply
it to the extended ensemble model. Figure 4 shows the false
negative and positive rates as a function of the fraction of
domains with a score below a certain value. By choosing a
target maximum FNR and FPR, we can determine the lower
and upper bounds on the maliciousness score; these bounds are
determined based on the training set, so they do not necessarily
reﬂect the exact actual error rates on the test set. Domains
with scores within these bounds have to be veriﬁed manually,
while domains with a lower and higher score are automatically
classiﬁed as benign and malicious, respectively.
For the extended model on domains with all data sets
available as represented in Figure 4, when setting a 2% error
tolerance, 55.5% of domains have a maliciousness score below
the lower bound set by 2% FPR (i.e. are benign), while (100%−
72.9%) = 27.1% of domains exceed the upper bound set by
2% FNR (i.e. are malicious). 55.5% + 27.1% = 82.6% of
domains therefore no longer need to be manually inspected.
Only 72.9% − 55.5% = 17.4% of domains still require further
manual investigation.
When we apply this a posteriori approach to the extended
ensemble model evaluated on all domains from the 2017 and
part of the 2018 iteration (by choosing appropriate bounds
for each component model), we obtain an accuracy of 97.6%;
overall, the performance metrics in Table VI indicate a very
high performance. The effective FNR and FPR are 2.6% and
2.3%, comparable to the target error rate of 2%.
Overall, this approach reduces manual effort by 66.2%,
accounting for the 15% of domains manually classiﬁed a
priori. When the error tolerance is 1% and 0.5%, the fraction
of automatically classiﬁed domains is 52.5% and 35.7%
respectively. The score thresholds become very strict when
very low error tolerances must be maintained, reducing the
fraction of domains that can be automatically classiﬁed. The
comparable effort reduction for an ensemble model trained on
the 2017 and 2018 and tested on the 2019 iteration and a 2%
error tolerance amounts to 76.9%, again achieving a signiﬁcant
reduction in manual effort.