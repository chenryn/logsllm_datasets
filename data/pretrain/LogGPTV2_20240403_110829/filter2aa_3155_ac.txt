resource. As stated in RFC 3986, the query component is indicated by the first question mark 
(?) character, and terminated by a number sign (#) character or by the end of the URI. Within a 
query component, the characters, semicolon (;), forward slash (/),  question mark (?), colon (:), 
at-symbol (@), ampersand (&), equal sign (=), plus sign (+), comma (,), and dollar sign ($) are 
reserved, and if used, will be URL encoded. 
The query component has not changed much over the years.
Fragment 
The fragment is the last URL component, and is used to identify and access a second resource 
within the first fetched resource specified by the path component. A fragment component is 
indicated by the presence of a number sign (#) and is terminated by the end of the URI. 
claroty.com
10
Copyright © 2021 Claroty Ltd. All rights reserved
RELATIVE REFERENCES
Since URLs are hierarchical in nature, relative references are URLs relative to another one. Yes, we use the term  
in its definition, but this is essentially what it is. For instance, if we have predefined a base URL, lets say  
http://www.example.com, and provide a URL starting from the path segment /foo/bar, the parser can resolve  
these into the absolute URL http://www.example.com/foo/bar. Thus /foo/bar is said to be relative to  
http://www.example.com. The key point here is this: the parser has to be supplied with a base URL for it to know  
how to resolve the relative one.
RFC 3986 defines three types of relative references:
Network-path reference: Begins with // e.g. //example.com
Absolute-path reference: Begins with / e.g. /path/foo/bar
Relative-path reference: Doesn’t begin with / e.g. path/foo/bar
While the last two require an absolute URL to resolve the relative one, the network-path reference only requires a scheme, 
i.e., //example.com turns into http://example.com if the parser knows it should handle the HTTP protocol. 
This is exactly what a browser will do when asked to fetch a resource from a network-path reference; it will resolve it to an 
absolute URL, usually with the default HTTP scheme. As we’re about to witness, this can have security implications. 
WHATWG URL SPECIFICATIONS 
Another standard aimed at establishing a URL specification was started by the Web Hypertext Application Technology 
Working Group (WHATWG), a community founded by individuals from leading web and technology companies who tried to 
create an updated, true-to-form URL specification and URL parsing primitive. 
While the WHATWG URL specification (TWUS) is not so different from the latest RFC URL specification (RFC 3986), minor 
differences still exist. One example is that while RFC 3986 differentiates between backslashes (\) and forward slashes (/) 
(forward slashes are treated as a delimiter inside a path component, while backslashes are a character with no reserved 
purpose), WHATWG’s specification states that backslashes should be converted to slashes, and then be treated as such, 
largely because most web browsers don’t differentiate between those two kinds of slashes, and treat them equally.
When WHATWG released its new “Living” URL standard, it broke compatibility with some existing standards and behaviours 
that contemporary URL parsing libraries followed. These interop issues remain one of the primary reasons why many 
maintainers of some parsing libraries stick to the RFC 3986 specifications as much as possible.
We can see that WHATWG tries to keep the URL specification more up-to-date and closer to a real-world compliant than 
the RFC specification, however differences between the WHATWG specification and real-life still exist, mainly because of 
the vast range of real-life URL parsing edge-cases. One comparison between RFC 3986, WHATWG and real-life parsers 
could be seen in Daniel Stenberg’s (bagder) Github repository.
claroty.com
11
Copyright © 2021 Claroty Ltd. All rights reserved
A summary of inconsistencies between RFC 3986/7 and the WHATWG URL specification. These differences were found  
by the creator of cURL, Daniel Stenberg.
In real-life, most web browsers follow the WHATWG URL specification because it is more user-tolerant and accounts for 
human errors. This meets a browser’s aim of working correctly, even when encountering malformed URLs.
URL Component
scheme
divider
userinfo
hostname
port number
path
query
fragment
Interop issues (RFC vs WHATWG)
NO
YES
YES
YES
YES
YES
UNKNOWN
UNKNOWN
Examples
/ vs. \
Multiple @ in the authority component
Supported character-set and IP representation
Valid port range
Supported character-set
claroty.com
12
Copyright © 2021 Claroty Ltd. All rights reserved
URL PARSING INCONSISTENCIES
Following a review of most of the URL RFCs and noticing all of the changes that occurred over the years, we’ve decided 
to examine URL parsers. We tried to find edge-cases that would result in incorrect or unexpected parsing results.
As part of our research, we selected 15 different libraries—written in different programming languages—URL fetchers,  
and browsers. We found many inconsistencies among the researched parsers. These inconsistencies have been categorized 
into five categories, which we believe are the root causes of these inconsistencies. Using the categorizations, below,  
we can trick most parsers and create a variety of unpredictable behavior, resulting in a wide range of vulnerabilities. 
The categorized inconsistencies are: scheme confusion, slash confusion, backslash confusion, URL-encoded confusion. 
Later, we will dive into each category and explain what is the root cause for each inconsistency.
What is important to understand is that URL syntax is complex and many edge cases could arise when non-standard inputs 
are given to a URL parser. The purpose of RFCs is to define as much as possible how a URL should be parsed. But given how 
many possible scenarios there are, there will always be edge-cases where it’s unclear how they should be handled. As a 
consequence, the parser’s inconsistencies are not necessarily a result of a bug in the code, but rather how the developers 
treated the edge-cases. 
As a result of our analysis, we were able to identify and categorize five different scenarios in which most URL parsers 
behaved unexpectedly: 
• Scheme Confusion: A confusion involving URLs with missing or malformed Scheme
• Slash Confusion: A confusion involving URLs containing an irregular number of slashes
• Backslash Confusion: A confusion involving URLs containing backslashes (\)
• URL Encoded Data Confusion: A confusion involving URLs containing URL Encoded data
• Scheme Mixup: A confusion involving parsing a URL belonging to a certain scheme without a scheme-specific parser
SCHEME CONFUSION
We noticed how almost any URL parser is confused when the scheme component is omitted. That is because RFC 3986 
clearly determines that the only mandatory part of the URL is the scheme, whereas previous RFC releases (RFC 2396 and 
earlier) don’t specify it. Therefore, when it is not present, most parsers get confused. 
For example, here are four different Python libraries that were given the following input: google.com/abc.
claroty.com
13
Copyright © 2021 Claroty Ltd. All rights reserved
As you can see, most parsers when given the input google.com/abc state that the host is
empty while the path is google.com/abc. However, urllib3 states that the host is google.com
and the path is /abc, while httptools complains that the supplied URL is invalid. The underlying 
fact is that when supplied with such a URL, almost no URL parser parses the URL successfully 
because the URL does not follow the RFC specifications.
However, when we try to fetch this URL, some parsers are able to successfully parse it with 
google.com as the hostname, thus fetching the resource correctly.
A fetch request using cURL, interpreting this malformed URL as if it had a default scheme.
As you can see, most parsers when given the input google.com/abc state that the host is 
empty while the path is google.com/abc. However, urllib3 states that the host is google.com
and the path is /abc, while httptools complains that the supplied URL is invalid. The underlying 
fact is that when supplied with such a URL, almost no URL parser parses the URL successfully 
because the URL does not follow the RFC specifications.
However, when we try to fetch this URL, some parsers are able to successfully parse it with 
google.com as the hostname, thus fetching the resource correctly.
A fetch request using cURL, interpreting this malformed URL as if it had a default scheme.
As you can see, most parsers when given the input google.com/abc state that the host is empty while the path is 
google.com/abc. However, urllib3 states that the host is google.com and the path is /abc, while httptools complains 
that the supplied URL is invalid. The underlying fact is that when supplied with such a URL, almost no URL  
eparser parses the URL successfully because the URL does not follow the RFC specifications. 
However, when we try to fetch this URL, some parsers are able to successfully parse it with google.com as the hostname, 
thus fetching the resource correctly.
A fetch request using cURL, interpreting this malformed URL as if it had a default scheme.
claroty.com
14
Copyright © 2021 Claroty Ltd. All rights reserved
Attackers could abuse this inconsistency in order to bypass validation checks disallowing specific hosts (e.g. localhost 
127.0.0.1, or cloud instance metadata 169.254.169.254), by omitting the scheme and throwing off the validations done by the 
first URL parser. An example of a vulnerable code block is seen here:
As you can see, above, the server does not allow requests to be made to the localhost interface. This is done by validating 
the received URL (using urlsplit) and comparing its netloc (host) to the blacklisted netloc (in this case, localhost and 
127.0.0.1). If the given netloc is the same as the blacklisted netlocs, the server does not perform the request, instead throws 
an exception.
When supplied localhost/secret.txt as the input URL, urlsplit parses this URL as a URL having no netloc (as seen above), 
thus the check if the given URL is in the blacklisted netlocs returns “False,” and does not throw an exception. However, 
as seen above, urllib3 interprets this URL as a valid URL, appending a default scheme of HTTP, thus fetching the  
blacklisted localhost.
SLASH CONFUSION
Another type of confusion we’ve recognized is a confusion involving a non-standard number of slashes in a URL, specifically 
in the authority segment of the URL.
As written in RFC 3986, a URL authority should start after the scheme, separated by a colon and two forward slashes ://. 
It should persist until either the parser reaches the end of a line (EOL) or a delimiter is read; these delimiters being  
either a slash signaling the start of a path component, a question mark signaling the start of a query, or a hashtag  
signaling a fragment.
Attackers could abuse this inconsistency in order to bypass validation checks disallowing 
specific hosts (e.g. localhost 127.0.0.1, or cloud instance metadata 169.254.169.254), by 
omitting the scheme and throwing off the validations done by the first URL parser. An example
of a vulnerable code block is seen here:
As you can see, above, the server does not allow requests to be made to the localhost 
interface. This is done by validating the received URL (using urlsplit) and comparing its netloc 
(host) to the blacklisted netloc (in this case - localhost and 127.0.0.1). If the given netloc is the 
same as the blacklisted netlocs, the server does not perform the request, instead throws an 
exception.
When supplied localhost/secret.txt as the input URL, urlsplit parses this URL as a URL having 
no netloc (as seen above), thus the check if the given URL is in the blacklisted netlocs returns 
“False,” and does not throw an exception. However, as seen above, urllib3 interprets this URL 
as a valid URL, appending a default scheme of HTTP, thus fetching the blacklisted localhost.
claroty.com
15
Copyright © 2021 Claroty Ltd. All rights reserved
However, this specification could introduce a few parsing differences when not all parsers follow this syntax verbatim.
Throughout our research, we’ve identified that not all URL parsers follow this syntax verbatim, which leads to some 
interesting interactions. Lets look at the following URL as an example:
http:///google.com/
When we give this URL to our parsers (notice the extra slash), we see some really interesting results:
As you can see, most URL parsers interpreted this URL as having no host, instead placing /google.com as the URL’s path 
which is the desired behavior according to RFC 3986. We can see this result no matter how many extra slashes we add to 
the URL, the only difference is the number of slashes in the path component of the parsed URL. Similarly, when supplied 
with a URL missing a slash after the scheme, we can see the same results.  
The reason our parsers think a URL with three or more slashes has no netloc becomes clear when we see how our parsers 
treat URLs. We can see that in all cases, our parsers search for a // to signify the start of an authority component, 
which will continue until a delimiter is encountered, and of course, one of those delimiters is a slash. Since our URL has 
another slash immediately after the first two slashes, the parser thinks it reached a delimiter and the end of the 
authority component, resulting in an empty string as the authority because no characters appeared between the first two 
slashes in the third one.
This example demonstrates how different parsers parse the hostname and path incorrectly whenever you 
provide the URL with an extra slash.
(EOL) or a delimiter is read; these delimiters being either a slash signaling the start of a path 
component, a question mark signaling the start of a query, or a hashtag signaling a fragment.
However, this specification could introduce a few parsing differences when not all parsers follow
this syntax verbatim.
Throughout our research, we’ve identified that not all URL parsers follow this syntax verbatim, 
which leads to some interesting interactions. Lets look at the following URL as an example:
http:///google.com/
When we give this URL to our parsers (notice the extra slash), we see some really interesting 
results:
This example demonstrates how different parsers parse the hostname and path incorrectly whenever you provide the URL 
with an extra slash.
claroty.com
16
Copyright © 2021 Claroty Ltd. All rights reserved
In the case of only one slash, the parser never finds the start of an authority component, thus leaving it as an empty string.
However, when we looked into other parsers, we found a group that ignored extra or missing slashes, at least to some 
degree, accepting malformed URLs with an incorrect number of slashes before the authority component. We noticed most 
parsers that ignore extra or missing slashes are non-programmatic parsers, which are trusted with receiving a URL and 
fetching the requested resource
For example, Google Chrome and most modern browsers simply ignore extra slashes, instead treating the URL as if it was in 
its correct representation.
Another parser that accepts malformed URLs with missing or extra slashes is cURL. When we supply it with a malformed 
URL, we see a somewhat similar result:
A request being performed inside a browser through JavaScript’s fetch directive, fetching a URL with many extra 
slashes. We can see that the request was fulfilled and the resource had been fetched.
A fetch request using cURL, fetching malformed URLs with extra/missing slashes.
appeared between the first two slashes in the third one.
In the case of only one slash, the parser never finds the start of an authority component, thus 
leaving it as an empty string.
However, when we looked into other parsers, we found a group that ignored extra or missing 
slashes, at least to some degree, accepting malformed URLs with an incorrect number of
slashes before the authority component. We noticed most parsers that ignore extra or missing 
slashes are non-programmatic parsers, which are trusted with receiving a URL and fetching the 
requested resource
For example, Google Chrome and most modern browsers simply ignore extra slashes, instead 
treating the URL as if it was in its correct representation.
A request being performed inside a browser through JavaScript’s fetch directive, fetching a URL with many extra slashes.
We can see that the request was fulfilled and the resource had been fetched.
Another parser that accepts malformed URLs with missing or extra slashes is cURL. When we 
supply it with a malformed URL, we see a somewhat similar result:
A fetch request using cURL, fetching malformed URLs with extra/missing slashes.
As we can see, cURL accepts malformed URLs to some degree, accepting either one extra 
slash or one missing slash, while denying a URL with four or more extra slashes. This is clearly 
stated in cURL documentation. 
There is no indication that cURL is following WHATWG specification blindly, instead it appears 
that cURL is following RFC 3986 and extending its capabilities to handle some real-world use
cases including common human errors and behaviours, such as URLs without a scheme (e.g.
example.com instead of http://example.com).
Thi diff
i
i
lt
id
f
tt
k
i
th t
ld l
d t
claroty.com
17
Copyright © 2021 Claroty Ltd. All rights reserved
As we can see, cURL accepts malformed URLs to some degree, accepting either one extra slash or one missing slash, while 
denying a URL with four or more extra slashes. This is clearly stated in cURL documentation. 
There is no indication that cURL is following WHATWG specification blindly, instead it appears that cURL is following RFC 
3986 and extending its capabilities to handle some real-world use cases including common human errors and behaviours, 
such as URLs without a scheme (e.g. example.com instead of http://example.com).
This difference in parsing results exposes a wide range of attack scenarios that could lead to serious vulnerabilities. For 
example, imagine a scenario in which a website has a functionality of fetching resources. By design, this functionality could 