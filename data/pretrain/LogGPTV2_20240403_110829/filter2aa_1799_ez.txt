data, it knows that the node still needs to be validated: the parent node is already in the cache (the tree 
has been already navigated for reaching the child) and contains the checksum of the child. Minstore 
has all the needed information for verifying that the bucket contains valid data. Note that there could 
be pages in the page table that have been never accessed. This is because their checksum still needs 
to be validated.
Minstore performs tree updates by writing the entire B+ tree as a single transaction. The tree update 
process writes dirty pages of the B+ tree to the physical disk. There are multiple reasons behind a tree 
update—an application explicitly flushing its changes, the system running in low memory or similar 
conditions, the cache manager flushing cached data to disk, and so on. It’s worth mentioning that 
Minstore usually writes the new updated trees lazily with the lazy writer thread. As seen in the previous 
section, there are several triggers to kick in the lazy writer (for example, when the number of the dirty 
pages reaches a certain threshold). 
Minstore is unaware of the actual reason behind the tree update request. The first thing that Minstore 
does is make sure that no other transactions are modifying the tree (using complex synchronization 
primitives). After initial synchronization, it starts to write dirty pages and with old deleted pages. In a 
CHAPTER 11
Caching and file systems
747
write-to-new implementation, a new page represents a bucket that has been modified and its content 
replaced; a freed page is an old page that needs to be unlinked from the parent. If a transaction wants to 
modify a leaf node, it copies (in memory) the root bucket and the leaf page; Minstore then creates the 
corresponding page table entries in the page table without modifying any link.
The tree update algorithm enumerates each page in the page table. However, the page table has 
no concept of which level in the B+ tree the page resides, so the algorithm checks even the B+ tree by 
starting from the more external node (usually the leaf), up to the root nodes. For each page, the algo-
rithm performs the following steps:
1.
Checks the state of the page. If it’s a freed page, it skips the page. If it’s a dirty page, it updates
its parent pointer and checksum and puts the page in an internal list of pages to write.
2.
Discards the old page.
When the algorithm reaches the root node, it updates its parent pointer and checksum directly in 
the object table and finally puts also the root bucket in the list of pages to write. Minstore is now able to 
write the new tree in the free space of the underlying volume, preserving the old tree in its original loca-
tion. The old tree is only marked as freed but is still present in the physical medium. This is an important 
characteristic that summarizes the write-to-new strategy and allows the ReFS file system (which resides 
above Minstore) to support advanced online recovery features. Figure 11-83 shows an example of the tree 
update process for a B+ table that contains two new leaf pages (A’ and B’). In the figure, pages located in 
the page table are represented in a lighter shade, whereas the old pages are shown in a darker shade. 
C
B'
A'
WriteList
Checksum(A') Checksum(B') Checksum(C')
Checksum(A') Checksum(B') Checksum(C)
Object Table
FIGURE 11-83 Minstore tree update process.
748
CHAPTER 11
Caching and file systems
Maintaining exclusive access to the tree while performing the tree update can represent a perfor-
mance issue; no one else can read or write from a B+ tree that has been exclusively locked. In the latest 
versions of Windows 10, B+ trees in Minstore became generational—a generation number is attached 
to each B+ tree. This means that a page in the tree can be dirty with regard to a specific generation. If 
a page is originally dirty for only a specific tree generation, it can be directly updated, with no need to 
copy-on-write because the final tree has still not been written to disk. 
In the new model, the tree update process is usually split in two phases:
I 
Failable phase: Minstore acquires the exclusive lock on the tree, increments the tree’s genera-
tion number, calculates and allocates the needed memory for the tree update, and finally drops
the lock to shared.
I 
Nonfailable phase: This phase is executed with a shared lock (meaning that other I/O can read
from the tree), Minstore updates the links of the director nodes and all the tree’s checksums,
and finally writes the final tree to the underlying disk. If another transaction wants to modify the
tree while it’s being written to disk, it detects that the tree’s generation number is higher, so it
copy-on-writes the tree again.
With the new schema, Minstore holds the exclusive lock only in the failable phase. This means that 
tree updates can run in parallel with other Minstore transactions, significantly improving the overall 
performance.
ReFS architecture
As already introduced in previous paragraphs, ReFS (the Resilient file system) is a hybrid of the NTFS 
implementation and Minstore, where every file and directory is a B+ tree configured by a particular 
schema. The file system volume is a flat namespace of directories. As discussed previously, NTFS is 
composed of different components:
I 
Core FS support: Describes the interface between the file system and other system compo-
nents, like the cache manager and the I/O subsystem, and exposes the concept of file create,
open, read, write, close, and so on.
I 
High-level FS feature support: Describes the high-level features of a modern file system,
like file compression, file links, quota tracking, reparse points, file encryption, recovery support,
and so on.
I 
On-disk dependent components and data structures MFT and file records, clusters, index
package, resident and nonresident attributes, and so on (see the “The NT file system (NTFS)”
section earlier in this chapter for more details).
ReFS keeps the first two parts largely unchanged and replaces the rest of the on-disk dependent 
components with Minstore, as shown in Figure 11-84.
CHAPTER 11
Caching and file systems
749
Index package
Interface support (object lifetime, rename, etc.)
System Interface (IRP_MJ_XYZ…)
File record attribute package
Bitmap package
MFT package
Log manager
Change journal
Quota, etc.
Format-dependent
FIGURE 11-84 ReFS architecture’s scheme.
In the “NTFS driver” section of this chapter, we introduced the entities that link a file handle to the 
file system’s on-disk structure. In the ReFS file system driver, those data structures (the stream control 
block, which represents the NTFS attribute that the caller is trying to read, and the file control block, 
which contains a pointer to the file record in the disk’s MFT) are still valid, but have a slightly different 
meaning in respect to their underlying durable storage. The changes made to these objects go through 
Minstore instead of being directly translated in changes to the on-disk MFT. As shown in Figure 11-85, 
in ReFS:
I 
A file control block (FCB) represents a single file or directory and, as such, contains a pointer to
the Minstore B+ tree, a reference to the parent directory’s stream control block and key (the
directory name). The FCB is pointed to by the file object, through the FsContext2 field.
I 
A stream control block (SCB) represents an opened stream of the file object. The data struc-
ture used in ReFS is a simplified version of the NTFS one. When the SCB represents directories,
though, the SCB has a link to the directory’s index, which is located in the B+ tree that repre-
sents the directory. The SCB is pointed to by the file object, through the FsContext field.
I 
A volume control block (VCB) represents a currently mounted volume, formatted by ReFS.
When a properly formatted volume has been identified by the ReFS driver, a VCB data structure
is created, attached into the volume device object extension, and linked into a list located in a
global data structure that the ReFS file system driver allocates at its initialization time. The VCB
contains a table of all the directory FCBs that the volume has currently opened, indexed by their
reference ID.
750
CHAPTER 11
Caching and file systems
File object
FCB
Index SCB
Embedded
B+ tree
Index B+
tree
File object
FCB
Data SCB
Embedded
B+ tree
Directory
File
FIGURE 11-85 ReFS files and directories in-memory data structures.
In ReFS, every open file has a single FCB in memory that can be pointed to by different SCBs (de-
pending on the number of streams opened). Unlike NTFS, where the FCB needs only to know the MFT 
entry of the file to correctly change an attribute, the FCB in ReFS needs to point to the B+ tree that 
represents the file record. Each row in the file’s B+ tree represents an attribute of the file, like the ID, full 
name, extents table, and so on. The key of each row is the attribute code (an integer value).
File records are entries in the directory in which files reside. The root node of the B+ tree that repre-
sents a file is embedded into the directory entry’s value data and never appears in the object table. The 
file data streams, which are represented by the extents table, are embedded B+ trees in the file record. 
The extents table is indexed by range. This means that every row in the extent table has a VCN range 
used as the row’s key, and the LCN of the file’s extent used as the row’s value. In ReFS, the extents table 
could become very large (it is indeed a regular B+ tree). This allows ReFS to support huge files, bypass-
ing the limitations of NTFS.
Figure 11-86 shows the object table, files, directories, and the file extent table, which in ReFS are all 
represented through B+ trees and provide the file system namespace.
Object ID
Object Table
Disk
Location
Object ID
Disk
Location
Object ID
Disk
Location
Object ID
Disk
Location
…
File Name
Directory
File Record Index
File Name
File Record Index
File Name
File Record Index
File Name
File Record Index
10
File Record
Std Info
30
File Name
80
Extents Index
…
…
This root is embedded in parent table’s row
…and so is this one
0-100
File Extents
@ 5004
101-200
@ 9550
300-304
@ 1000
FIGURE 11-86 Files and directories in ReFS.
CHAPTER 11
Caching and file systems
751
Directories are Minstore B+ trees that are responsible for the single, flat namespace. A ReFS 
directory can contain:
I 
Files
I 
Links to directories
I 
Links to other files (file IDs)
Rows in the directory B+ tree are composed of a key type value pair, where the key is the entry’s 
name and the value depends on the type of directory entry. With the goal of supporting queries and 
other high-level semantics, Minstore also stores some internal data in invisible directory rows. These kinds 
of rows have have their key starting with a Unicode zero character. Another row that is worth mentioning 
is the directory’s file row. Every directory has a record, and in ReFS that file record is stored as a file row in 
the self-same directory, using a well-known zero key. This has some effect on the in-memory data struc-
tures that ReFS maintains for directories. In NTFS, a directory is really a property of a file record (through 
the Index Root and Index Allocation attributes); in ReFS, a directory is a file record stored in the directory 
itself (called directory index record). Therefore, whenever ReFS manipulates or inspects files in a directory, 
it must ensure that the directory index is open and resident in memory. To be able to update the direc-
tory, ReFS stores a pointer to the directory’s index record in the opened stream control block.
The described configuration of the ReFS B+ trees does not solve an important problem. Every time 
the system wants to enumerate the files in a directory, it needs to open and parse the B+ tree of each 
file. This means that a lot of I/O requests to different locations in the underlying medium are needed. 
If the medium is a rotational disk, the performance would be rather bad. 
To solve the issue, ReFS stores a STANDARD_INFORMATION data structure in the root node of 
the file’s embedded table (instead of storing it in a row of the child file’s B+ table). The STANDARD 
_INFORMATION data includes all the information needed for the enumeration of a file (like the file’s 
access time, size, attributes, security descriptor ID, the update sequence number, and so on). A file’s 
embedded root node is stored in a leaf bucket of the parent directory’s B+ tree. By having the data 
structure located in the file’s embedded root node, when the system enumerates files in a directory, 
it only needs to parse entries in the directory B+ tree without accessing any B+ tables describing indi-
vidual files. The B+ tree that represents the directory is already in the page table, so the enumeration 
is quite fast.
ReFS on-disk structure
This section describes the on-disk structure of a ReFS volume, similar to the previous NTFS section. The 
section focuses on the differences between NTFS and ReFS and will not cover the concepts already 
described in the previous section.
The Boot sector of a ReFS volume consists of a small data structure that, similar to NTFS, contains 
basic volume information (serial number, cluster size, and so on), the file system identifier (the ReFS 
OEM string and version), and the ReFS container size (more details are covered in the “Shingled mag-
netic recording (SMR) volumes” section later in the chapter). The most important data structure in the 
volume is the volume super block. It contains the offset of the latest volume checkpoint records and 
752
CHAPTER 11
Caching and file systems
is replicated in three different clusters. ReFS, to be able to mount a volume, reads one of the volume 
checkpoints, verifies and parses it (the checkpoint record includes a checksum), and finally gets the 
offset of each global table.
The volume mounting process opens the object table and gets the needed information for reading 
the root directory, which contains all of the directory trees that compose the volume namespace. The 
object table, together with the container table, is indeed one of the most critical data structures that is 
the starting point for all volume metadata. The container table exposes the virtualization namespace, 
so without it, ReFS would not able to correctly identify the final location of any cluster. Minstore op-
tionally allows clients to store information within its object table rows. The object table row values, as 
shown in Figure 11-87, have two distinct parts: a portion owned by Minstore and a portion owned by 
ReFS. ReFS stores parent information as well as a high watermark for USN numbers within a directory 
(see the section “Security and change journal” later in this chapter for more details).
ObjectId
key
value
Last USN #
Parent object ID
Root location
Root checksum
Last written log #
FIGURE 11-87 The object table entry composed of a ReFS part (bottom rectangle) and Minstore part (top rectangle).
Object IDs
Another problem that ReFS needs to solve regards file IDs. For various reasons—primarily for tracking 
and storing metadata about files in an efficient way without tying information to the namespace—
ReFS needs to support applications that open a file through their file ID (using the OpenFileById API, for 
example). NTFS accomplishes this through the Extend\ObjId file (using the 0 index root attribute; 
see the previous NTFS section for more details). In ReFS, assigning an ID to every directory is trivial; 
indeed, Minstore stores the object ID of a directory in the object table. The problem arises when the 
system needs to be able to assign an ID to a file; ReFS doesn’t have a central file ID repository like NTFS 
does. To properly find a file ID located in a directory tree, ReFS splits the file ID space into two portions: 
the directory and the file. The directory ID consumes the directory portion and is indexed into the key 
of an object table’s row. The file portion is assigned out of the directory’s internal file ID space. An ID 
that represents a directory usually has a zero in its file portion, but all files inside the directory share 
the same directory portion. ReFS supports the concept of file IDs by adding a separate row (composed 
of a FileId FileName pair) in the directory’s B+ tree, which maps the file ID to the file name within 
the directory. 
CHAPTER 11
Caching and file systems
753
When the system is required to open a file located in a ReFS volume using its file ID, ReFS satisfies 
the request by:
1.
Opening the directory specified by the directory portion
2.
Querying the FileId row in the directory B+ tree that has the key corresponding to the
file portion
3.
Querying the directory B+ tree for the file name found in the last lookup.
Careful readers may have noted that the algorithm does not explain what happens when a file is re-
named or moved. The ID of a renamed file should be the same as its previous location, even if the ID of 