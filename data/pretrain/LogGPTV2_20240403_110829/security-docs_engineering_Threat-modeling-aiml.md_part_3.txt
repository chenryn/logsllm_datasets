##### Traditional Parallels
Data Privacy. Inferences are being made about a data point’s inclusion
in the training set but the training data itself is not being disclosed
##### Severity
This is a privacy issue, not a security issue. It is addressed in threat
modeling guidance because the domains overlap, but any response here
would be driven by Privacy, not Security.
## \#5 Model Stealing
##### Description
The attackers recreate the underlying model by legitimately querying the
model. The functionality of the new model is same as that of the
underlying model[1]. Once the model is recreated, it can be inverted to
recover feature information or make inferences on training data. 
- Equation solving – For a model that returns class probabilities via
    API output, an attacker can craft queries to determine unknown
    variables in a model.
- Path Finding – an attack that exploits API particularities to
    extract the ‘decisions’ taken by a tree when classifying an input
    [7].
- Transferability attack - An adversary can train a local
    model—possibly by issuing prediction queries to the targeted model -
    and use it to craft adversarial examples that transfer to the target
    model [8]. If your model is extracted and discovered vulnerable to a
    type of adversarial input, new attacks against your
    production-deployed model can be developed entirely offline by the
    attacker who extracted a copy of your model.
##### Examples
In settings where an ML model serves to detect adversarial behavior,
such as identification of spam, malware classification, and network
anomaly detection, model extraction can facilitate evasion attacks [7].
##### Mitigations
Proactive/Protective Actions
- Minimize or obfuscate the details returned in prediction APIs while
    still maintaining their usefulness to “honest” applications [7].
- Define a well-formed query for your model inputs and only return
    results in response to completed, well-formed inputs matching that
    format.
- Return rounded confidence values. Most legitimate callers do not
    need multiple decimal places of precision.
##### Traditional Parallels
Unauthenticated, read-only tampering of system data, targeted high-value
information disclosure?
##### Severity
Important in security-sensitive models, Moderate otherwise
## \#6 Neural Net Reprogramming
Description
By means of a specially crafted query from an adversary, Machine
learning systems can be reprogrammed to a task that deviates from the
creator’s original intent [1].
##### Examples
Weak access controls on a facial recognition API enabling 3rd
parties to incorporate into apps designed to harm Microsoft customers,
such as a deep fakes generator.
##### Mitigations
- Strong client\server mutual authentication and access control to
    model interfaces
- Takedown of the offending accounts.
- Identify and enforce a service-level agreement for your APIs.
    Determine the acceptable time-to-fix for an issue once reported and
    ensure the issue no longer repros once SLA expires.
##### Traditional Parallels
This is an abuse scenario. You're less likely to open a security incident on this than you are to simply disable the
offender’s account.
##### Severity
Important to Critical
## \#7 Adversarial Example in the Physical domain (bits-\>atoms)
Description
An adversarial example is an input/query from a malicious entity sent
with the sole aim of misleading the machine learning system [1]
##### Examples
These examples can manifest in the physical domain, like a self-driving
car being tricked into running a stop sign because of a certain color of
light (the adversarial input) being shone on the stop sign, forcing the
image recognition system to no longer see the stop sign as a stop
sign.  
##### Traditional Parallels
Elevation of Privilege, remote code execution
##### Mitigations
These attacks manifest themselves because issues in the machine learning
layer (the data & algorithm layer below AI-driven decisionmaking) were
not mitigated. As with any other software \*or\* physical system, the
layer below the target can always be attacked through traditional
vectors. Because of this, traditional security practices are more
important than ever, especially with the layer of unmitigated
vulnerabilities (the data/algo layer) being used between AI and
traditional software.
##### Severity
Critical
## \#8 Malicious ML providers who can recover training data 
##### Description
A malicious provider presents a backdoored algorithm, wherein the
private training data is recovered. They were able to reconstruct faces
and texts, given the model alone.
##### Traditional Parallels
Targeted information disclosure
##### Mitigations
Research papers demonstrating the viability of this attack indicate
Homomorphic Encryption would be an effective mitigation. This is an
area with little current investment at Microsoft and AETHER Security
Engineering recommends building expertise with research investments in
this space. This research would need to enumerate Homomorphic Encryption
tenets and evaluate their practical effectiveness as mitigations in the
face of malicious ML-as-a-Service providers.
##### Severity
Important if data is PII, Moderate otherwise
## \#9 Attacking the ML Supply Chain
##### Description
Owing to large resources (data + computation) required to train
algorithms, the current practice is to reuse models trained by
large corporations and modify them slightly for task at hand (e.g:
ResNet is a popular image recognition model from Microsoft). These
models are curated in a Model Zoo (Caffe hosts popular image recognition
models). In this attack, the adversary attacks the models hosted in
Caffe, thereby poisoning the well for anyone else. [1]
##### Traditional Parallels
- Compromise of third-party non-security dependency
- App store unknowingly hosting malware
##### Mitigations
- Minimize 3rd-party dependencies for models and data where possible.
- Incorporate these dependencies into your threat modeling process.
- Leverage strong authentication, access control and encryption
    between 1st/3rd-party systems.
##### Severity
Critical
## \#10 Backdoor Machine Learning
##### Description
The training process is outsourced to a malicious 3rd party who tampers
with training data and delivered a trojaned model which forces targeted
mis-classifications, such as classifying a certain virus as
non-malicious[1]. This is a risk in ML-as-a-Service model-generation
scenarios.
![An example showing how mis-classifications can adversely affect training data. One photo is a correctly classified stop sign. After poisoning, the second photo is labeled as a speed limit sign.](./media/threat-modeling-aiml/tm11.jpg)[12]
##### Traditional Parallels
- Compromise of third-party security dependency
- Compromised Software Update mechanism
- Certificate Authority compromise
##### Mitigations
##### Reactive/Defensive Detection Actions
- The damage is already done once this threat has been discovered, so
    the model and any training data provided by the malicious provider
    cannot be trusted.
##### Proactive/Protective Actions
- Train all sensitive models in-house
- Catalog training data or ensure it comes from a trusted third party
    with strong security practices
- Threat model the interaction between the MLaaS provider and your own
    systems
##### Response Actions
- Same as for compromise of external dependency
##### Severity
Critical
## \#11 Exploit software dependencies of the ML system
##### Description
In this attack, the attacker does NOT manipulate the algorithms.
Instead, exploits software vulnerabilities such as buffer overflows or
cross-site scripting[1]. It is still easier to compromise software
layers beneath AI/ML than attack the learning layer directly, so
traditional security threat mitigation practices detailed in the
Security Development Lifecycle are essential.
##### Traditional Parallels
- Compromised Open Source Software Dependency
- Web server vulnerability (XSS, CSRF, API input validation failure)
##### Mitigations
Work with your security team to follow applicable Security Development
Lifecycle/Operational Security Assurance best practices.
##### Severity
Variable; Up to Critical depending on the type of traditional software
vulnerability.
## Bibliography
[1] Failure Modes in Machine Learning, Ram Shankar Siva
Kumar, David O’Brien, Kendra Albert, Salome Viljoen, and Jeffrey Snover,
[https://learn.microsoft.com/security/failure-modes-in-machine-learning](/security/failure-modes-in-machine-learning)
[2] AETHER Security Engineering Workstream, Data Provenance/Lineage
v-team
[3] Adversarial Examples in Deep Learning: Characterization and
Divergence, Wei, et al, 
[4] ML-Leaks: Model and Data Independent Membership Inference Attacks
and Defenses on Machine Learning Models, Salem, et al,
[5] M. Fredrikson, S. Jha, and T. Ristenpart, “[Model Inversion Attacks
that Exploit Confidence Information and Basic
Countermeasures](https://www.cs.cmu.edu/~mfredrik/papers/fjr2015ccs.pdf),”
in Proceedings of the 2015 ACM SIGSAC Conference on Computer and
Communications Security (CCS).
[6] Nicolas Papernot & Patrick McDaniel- Adversarial Examples in Machine
Learning AIWTB 2017
[7] [Stealing Machine Learning Models via Prediction
APIs](https://www.usenix.org/system/files/conference/usenixsecurity16/sec16_paper_tramer.pdf),
Florian Tramèr, École Polytechnique Fédérale de Lausanne (EPFL); Fan
Zhang, Cornell University; Ari Juels, Cornell Tech; Michael K. Reiter,
The University of North Carolina at Chapel Hill; Thomas Ristenpart,
Cornell Tech
[8] [The Space of Transferable Adversarial
Examples](https://arxiv.org/pdf/1704.03453.pdf), Florian Tramèr ,
Nicolas Papernot , Ian Goodfellow , Dan Boneh , and Patrick McDaniel
[9] [Understanding Membership Inferences on Well-Generalized Learning
Models](https://arxiv.org/pdf/1802.04889v1.pdf) Yunhui Long1 , Vincent
Bindschaedler1 , Lei Wang2 , Diyue Bu2 , Xiaofeng Wang2 , Haixu Tang2 ,
Carl A. Gunter1 , and Kai Chen3,4
[10] Simon-Gabriel et al., Adversarial vulnerability of neural networks
increases with input dimension, ArXiv 2018;
[11] Lyu et al., A unified gradient regularization family for
adversarial examples, ICDM 2015
[12] Wild Patterns: Ten Years After the Rise of Adversarial Machine
Learning - NeCS 2019 Battista Biggioa, Fabio Roli
[13] Adversarially Robust Malware Detection UsingMonotonic
Classification Inigo Incer et al.
[14] Battista Biggio, Igino Corona, Giorgio Fumera, Giorgio Giacinto,
and Fabio Roli. Bagging Classifiers for Fighting Poisoning Attacks in
Adversarial Classification Tasks
[15] An Improved Reject on Negative Impact Defense Hongjiang Li and
Patrick P.K. Chan
[16] Adler. Vulnerabilities in biometric encryption systems. 5th Int’l
Conf. AVBPA, 2005
[17] Galbally, McCool, Fierrez, Marcel, Ortega-Garcia. On the
vulnerability of face verification systems to hill-climbing attacks.
Patt. Rec., 2010
[18] Weilin Xu, David Evans, Yanjun Qi. Feature Squeezing: Detecting
Adversarial Examples in Deep Neural Networks. 2018 Network and
Distributed System Security Symposium. 18-21 February.
[19] Reinforcing Adversarial Robustness using Model Confidence Induced
by Adversarial Training - Xi Wu, Uyeong Jang, Jiefeng Chen, Lingjiao
Chen, Somesh Jha
[20] Attribution-driven Causal Analysis for Detection of Adversarial
Examples, Susmit Jha, Sunny Raj, Steven Fernandes, Sumit Kumar Jha,
Somesh Jha, Gunjan Verma, Brian Jalaian, Ananthram Swami
[21] Robust Linear Regression Against Training Data Poisoning – Chang
Liu et al.
[22] Feature Denoising for Improving Adversarial Robustness, Cihang Xie,
Yuxin Wu, Laurens van der Maaten, Alan Yuille, Kaiming He
[23] Certified Defenses against Adversarial Examples - Aditi
Raghunathan, Jacob Steinhardt, Percy Liang