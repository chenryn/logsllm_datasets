### Traditional Parallels: Data Privacy
- **Issue**: Inferences are being made about a data point’s inclusion in the training set, but the training data itself is not being disclosed.
- **Severity**: This is a privacy issue, not a security issue. While it is addressed in threat modeling guidance due to overlapping domains, the response would be driven by privacy considerations, not security.

### \#5 Model Stealing
- **Description**: Attackers can recreate the underlying model by legitimately querying the model. The recreated model can then be used to recover feature information or make inferences on the training data.
  - **Equation Solving**: For a model that returns class probabilities via API output, an attacker can craft queries to determine unknown variables in the model.
  - **Path Finding**: An attack that exploits API particularities to extract the 'decisions' taken by a tree when classifying an input.
  - **Transferability Attack**: An adversary can train a local model, possibly by issuing prediction queries to the targeted model, and use it to craft adversarial examples that transfer to the target model. If your model is extracted and found to be vulnerable to a type of adversarial input, new attacks against your production-deployed model can be developed entirely offline.
- **Examples**: In settings where an ML model is used to detect adversarial behavior (e.g., spam, malware classification, network anomaly detection), model extraction can facilitate evasion attacks.
- **Mitigations**:
  - Minimize or obfuscate the details returned in prediction APIs while maintaining their usefulness to legitimate applications.
  - Define a well-formed query for model inputs and only return results in response to completed, well-formed inputs matching that format.
  - Return rounded confidence values, as most legitimate callers do not need multiple decimal places of precision.
- **Traditional Parallels**: Unauthenticated, read-only tampering of system data, targeted high-value information disclosure.
- **Severity**: Important in security-sensitive models, moderate otherwise.

### \#6 Neural Net Reprogramming
- **Description**: By means of a specially crafted query from an adversary, machine learning systems can be reprogrammed to perform tasks that deviate from the creator’s original intent.
- **Examples**: Weak access controls on a facial recognition API enabling third parties to incorporate it into apps designed to harm users, such as a deep fakes generator.
- **Mitigations**:
  - Strong client-server mutual authentication and access control to model interfaces.
  - Takedown of the offending accounts.
  - Identify and enforce a service-level agreement for your APIs, determining the acceptable time-to-fix for an issue once reported and ensuring the issue no longer repros once the SLA expires.
- **Traditional Parallels**: This is an abuse scenario. You are less likely to open a security incident than to simply disable the offender’s account.
- **Severity**: Important to critical.

### \#7 Adversarial Examples in the Physical Domain (Bits to Atoms)
- **Description**: An adversarial example is an input/query from a malicious entity sent with the sole aim of misleading the machine learning system.
- **Examples**: These examples can manifest in the physical domain, such as a self-driving car being tricked into running a stop sign because of a certain color of light (the adversarial input) being shone on the stop sign, forcing the image recognition system to no longer see the stop sign as a stop sign.
- **Traditional Parallels**: Elevation of privilege, remote code execution.
- **Mitigations**: These attacks exploit issues in the machine learning layer (data and algorithm layer). As with any other software or physical system, the layer below the target can always be attacked through traditional vectors. Therefore, traditional security practices are more important than ever, especially with the layer of unmitigated vulnerabilities (the data/algorithm layer) being used between AI and traditional software.
- **Severity**: Critical.

### \#8 Malicious ML Providers Who Can Recover Training Data
- **Description**: A malicious provider presents a backdoored algorithm, wherein the private training data is recovered. They were able to reconstruct faces and texts, given the model alone.
- **Traditional Parallels**: Targeted information disclosure.
- **Mitigations**: Research papers indicate that homomorphic encryption would be an effective mitigation. This is an area with little current investment, and AETHER Security Engineering recommends building expertise with research investments in this space. This research would need to enumerate homomorphic encryption tenets and evaluate their practical effectiveness as mitigations in the face of malicious ML-as-a-Service providers.
- **Severity**: Important if data is PII, moderate otherwise.

### \#9 Attacking the ML Supply Chain
- **Description**: Due to the large resources (data + computation) required to train algorithms, the current practice is to reuse models trained by large corporations and modify them slightly for the task at hand (e.g., ResNet, a popular image recognition model from Microsoft). These models are curated in a Model Zoo (Caffe hosts popular image recognition models). In this attack, the adversary attacks the models hosted in Caffe, thereby poisoning the well for anyone else.
- **Traditional Parallels**:
  - Compromise of third-party non-security dependency.
  - App store unknowingly hosting malware.
- **Mitigations**:
  - Minimize third-party dependencies for models and data where possible.
  - Incorporate these dependencies into your threat modeling process.
  - Leverage strong authentication, access control, and encryption between first and third-party systems.
- **Severity**: Critical.

### \#10 Backdoor Machine Learning
- **Description**: The training process is outsourced to a malicious third party who tampers with training data and delivers a trojaned model, which forces targeted misclassifications, such as classifying a certain virus as non-malicious. This is a risk in ML-as-a-Service model-generation scenarios.
- **Traditional Parallels**:
  - Compromise of third-party security dependency.
  - Compromised Software Update mechanism.
  - Certificate Authority compromise.
- **Mitigations**:
  - **Reactive/Defensive Detection Actions**: Once this threat is discovered, the model and any training data provided by the malicious provider cannot be trusted.
  - **Proactive/Protective Actions**:
    - Train all sensitive models in-house.
    - Catalog training data or ensure it comes from a trusted third party with strong security practices.
    - Threat model the interaction between the MLaaS provider and your own systems.
  - **Response Actions**: Same as for compromise of external dependency.
- **Severity**: Critical.

### \#11 Exploit Software Dependencies of the ML System
- **Description**: In this attack, the attacker does not manipulate the algorithms but exploits software vulnerabilities such as buffer overflows or cross-site scripting. It is still easier to compromise software layers beneath AI/ML than to attack the learning layer directly, so traditional security threat mitigation practices detailed in the Security Development Lifecycle are essential.
- **Traditional Parallels**:
  - Compromised Open Source Software Dependency.
  - Web server vulnerability (XSS, CSRF, API input validation failure).
- **Mitigations**: Work with your security team to follow applicable Security Development Lifecycle/Operational Security Assurance best practices.
- **Severity**: Variable; up to critical depending on the type of traditional software vulnerability.

### Bibliography
1. Failure Modes in Machine Learning, Ram Shankar Siva Kumar, David O’Brien, Kendra Albert, Salome Viljoen, and Jeffrey Snover, [https://learn.microsoft.com/security/failure-modes-in-machine-learning](/security/failure-modes-in-machine-learning)
2. AETHER Security Engineering Workstream, Data Provenance/Lineage v-team
3. Adversarial Examples in Deep Learning: Characterization and Divergence, Wei, et al
4. ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models, Salem, et al
5. M. Fredrikson, S. Jha, and T. Ristenpart, “Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures,” in Proceedings of the 2015 ACM SIGSAC Conference on Computer and Communications Security (CCS).
6. Nicolas Papernot & Patrick McDaniel- Adversarial Examples in Machine Learning AIWTB 2017
7. Stealing Machine Learning Models via Prediction APIs, Florian Tramèr, École Polytechnique Fédérale de Lausanne (EPFL); Fan Zhang, Cornell University; Ari Juels, Cornell Tech; Michael K. Reiter, The University of North Carolina at Chapel Hill; Thomas Ristenpart, Cornell Tech
8. The Space of Transferable Adversarial Examples, Florian Tramèr, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel
9. Understanding Membership Inferences on Well-Generalized Learning Models, Yunhui Long, Vincent Bindschaedler, Lei Wang, Diyue Bu, Xiaofeng Wang, Haixu Tang, Carl A. Gunter, and Kai Chen
10. Simon-Gabriel et al., Adversarial vulnerability of neural networks increases with input dimension, ArXiv 2018
11. Lyu et al., A unified gradient regularization family for adversarial examples, ICDM 2015
12. Wild Patterns: Ten Years After the Rise of Adversarial Machine Learning - NeCS 2019 Battista Biggioa, Fabio Roli
13. Adversarially Robust Malware Detection Using Monotonic Classification Inigo Incer et al.
14. Battista Biggio, Igino Corona, Giorgio Fumera, Giorgio Giacinto, and Fabio Roli. Bagging Classifiers for Fighting Poisoning Attacks in Adversarial Classification Tasks
15. An Improved Reject on Negative Impact Defense Hongjiang Li and Patrick P.K. Chan
16. Adler. Vulnerabilities in biometric encryption systems. 5th Int’l Conf. AVBPA, 2005
17. Galbally, McCool, Fierrez, Marcel, Ortega-Garcia. On the vulnerability of face verification systems to hill-climbing attacks. Patt. Rec., 2010
18. Weilin Xu, David Evans, Yanjun Qi. Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks. 2018 Network and Distributed System Security Symposium. 18-21 February.
19. Reinforcing Adversarial Robustness using Model Confidence Induced by Adversarial Training - Xi Wu, Uyeong Jang, Jiefeng Chen, Lingjiao Chen, Somesh Jha
20. Attribution-driven Causal Analysis for Detection of Adversarial Examples, Susmit Jha, Sunny Raj, Steven Fernandes, Sumit Kumar Jha, Somesh Jha, Gunjan Verma, Brian Jalaian, Ananthram Swami
21. Robust Linear Regression Against Training Data Poisoning – Chang Liu et al.
22. Feature Denoising for Improving Adversarial Robustness, Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan Yuille, Kaiming He
23. Certified Defenses against Adversarial Examples - Aditi Raghunathan, Jacob Steinhardt, Percy Liang