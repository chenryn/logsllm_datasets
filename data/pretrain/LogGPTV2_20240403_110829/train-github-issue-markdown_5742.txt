To test a saved model that was trained using `tf.train.batch`, you can follow these steps:

1. **Load the Saved Model:**
   First, load the saved model using TensorFlow's `SavedModel` API or by restoring the graph and session from the checkpoint.

2. **Access the Output Tensor:**
   You mentioned that you are accessing the output tensor using `graph.get_tensor_by_name("output:0")`, which returns a tensor with the shape `[batch_size, num]`.

3. **Prepare Test Data:**
   Since the model was trained using `tf.train.batch`, it expects input data in batches. Prepare your test data in the same batch format. Ensure that the batch size matches the one used during training or is compatible with the model's expectations.

4. **Run the Inference:**
   Use a TensorFlow session to run the inference on your test data. Here is an example of how you can do this:

```python
import tensorflow as tf

# Load the saved model
with tf.Session() as sess:
    # Restore the graph and variables
    saver = tf.train.import_meta_graph('path/to/your/model.ckpt.meta')
    saver.restore(sess, 'path/to/your/model.ckpt')

    # Access the output tensor
    output_tensor = tf.get_default_graph().get_tensor_by_name("output:0")

    # Access the input tensor (replace 'input:0' with the actual name of your input tensor)
    input_tensor = tf.get_default_graph().get_tensor_by_name("input:0")

    # Prepare your test data
    test_data = ...  # Your test data, shaped [batch_size, ...]

    # Run the inference
    output = sess.run(output_tensor, feed_dict={input_tensor: test_data})

    # Process the output
    print(output)
```

5. **Process the Output:**
   After running the inference, you will get the output tensor. You can process this output as needed for your specific use case.

By following these steps, you can effectively test a saved model that was trained using `tf.train.batch`. Make sure to adjust the tensor names and data preparation according to your specific model and dataset.