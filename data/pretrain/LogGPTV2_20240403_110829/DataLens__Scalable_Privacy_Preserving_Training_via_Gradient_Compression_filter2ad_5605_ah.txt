aokui Xiao. 2017. Privbayes: Private data release via bayesian networks. ACM
Transactions on Database Systems (TODS) 42, 4 (2017), 25.
[64] Yuheng Zhang, Ruoxi Jia, Hengzhi Pei, Wenxiao Wang, Bo Li, and Dawn Song.
2020. The secret revealer: Generative model-inversion attacks against deep
neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition.
[65] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba.
2017. Places: A 10 million Image Database for Scene Recognition. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence (2017).
A TOPAGG FOR DP SGD TRAINING
A.1 DP SGD Training Algorithm with TopAgg
We show the details of the DP SGD training using the TopAgg
approach in Algorithm 4. We mainly adopt the DP SGD framework
as in Abadi et al. [2]. In particular, at each step of the SGD, we
compute the gradient for a random subset of examples, and then
use the clipping norm to clip each gradient by their ℓ2 norm. Af-
ter performing a top-𝑘 compression of the gradients to select the
sub-dimensions, we then take a sum of the compressed gradients,
to which we inject the Gaussian noise subsequently. Finally, we
update the model with the compressed DP gradient. Theoretically,
since the ℓ2 norm of the gradient vector is reduced after the top-𝑘
compression step, the amount of noise required to achieve the same
level of DP guarantee becomes smaller, which implies a potentially
better utility of the training algorithm.
Note that directly applying the TopkStoSignGrad algorithm (Al-
gorithm 2) to SGD training does not yield a good utility due to
Algorithm 4 - Differentially Private SGD training via Gradi-
ent Compression and Aggregation TopAgg
1: Input: Examples {x1, . . . , x𝑛}, Top-𝑘-Portion parameter 𝑘, loss
𝑖 L(𝜃, x𝑖). Parameters: batch size 𝐵, learn-
function L(𝜃) = 1
ing rate 𝛾𝑡, noise scale 𝜎, gradient clipping norm 𝐶, total number
of epochs 𝑇 .
𝑛
6:
13:
14:
2: function NormTopK(g, 𝑘)
3:
4:
5:
𝑛𝑜𝑟𝑚 ← ∥g∥2
𝑡𝑎𝑟𝑔𝑒𝑡 ← 𝑛𝑜𝑟𝑚 · 𝑘
⊲ target norm after processing
𝑖𝑛𝑑𝑖𝑐𝑒𝑠 ← pick the dimensions in a decreasing order in
terms of the squared norm at that dimension, so that the sum
of the squared norms in those dimensions add up to right below
𝑡𝑎𝑟𝑔𝑒𝑡.
˜g ← preserve the values of g at 𝑖𝑛𝑑𝑖𝑐𝑒𝑠 and set value at
return ˜g
7:
8: end function
9:
10: Initialize 𝜃0 randomly
11: for epoch 𝑡 ∈ [𝑇] do
12:
other dimensions as 0
𝑖=1 each with sampling
Sample a batch of instances {x𝑡𝑖 }𝐵
for each sample x ∈ {x𝑡𝑖 }𝐵
𝑖=1 do
g𝑡 (x) ← ∇𝜃𝑡 L(𝜃𝑡 , x)
1, ∥g𝑡 (x) ∥
˜g𝑡 (x) ← g𝑡 (𝑥)/max
ˆg𝑡 (x) ← NormTopK( ˜g𝑡 (x), 𝑘)
(cid:17)
(cid:0)𝑖 ˆg𝑡 (x𝑖) + N(0, 𝑘𝜎2𝐶2I)(cid:1)
15:
16:
17:
⊲ add noise
18:
⊲ gradient descent
19:
20: end for
21: Output 𝜃𝑡 and compute the overall privacy cost (𝜀, 𝛿) using
end for
¯g𝑡 ← 1
𝜃𝑡+1 ← 𝜃𝑡 − 𝛾𝑡 ¯g𝑡
⊲ compute gradient
⊲ clip gradient
⊲ compress gradient
probability 𝐵/𝑛.
(cid:16)
𝐶
𝐵
Moments Accountant
information loss during gradient quantization. To overcome this
problem, we specially adapt TopAgg to select and preserve a subset
of the dimensions in the gradients based on the requirement im-
posed on the ℓ2 norm. This new strategy is described as the function
NormTopK in Algorithm 4. Concretely, given a gradient vector g, we
compute ¯g by selecting several dimensions in g with the highest
absolute values to ensure that their squared sum is close to the
target norm 𝑘∥g∥2 (here 0 < 𝑘 < 1) after the top-𝑘 compression,
and preserving only the values in the selected dimensions. Thus,
the compressed gradient ¯g satisfies the condition ∥¯g∥2 ≤ 𝑘∥g∥2.
The remaining dimensions in ¯g are set to 0 since they contain less
information. In this way, we achieve the goal of gradient compres-
√
sion without suffering a significant distortion. We note that the ℓ2
𝑘𝐶 when adapting
TopAgg for SGD training, since adding or removing one instance
√
x in the training set would lead to the gradient sum differing by
ˆg𝑡 (x), whose ℓ2 norm is bounded by
𝑘𝐶 due to the operations
in NormTopK. Thus, the variance of the added Gaussian noise is
𝑘𝐶2𝜎2I.
A.2 Evaluation of TopAgg for DP SGD
In this section, we demonstrate the universality of the proposed
DP gradient compression and aggregation algorithm TopAgg in
sensitivity of the gradient sum𝑖 ˆg𝑡 (x𝑖) is
Session 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2160DataLens, and in particular, the feasibility of applying it to DP
SGD training by evaluating its performance on two standard image
classification tasks for evaluating DP SGD mechanisms. We first
describe the experimental setup. Then we provide extensive evalua-
tion results on a wide range of privacy budgets. Overall, the results
show that the TopAgg enabled DP SGD training achieves similar
or even better performance on model utility compared with the
state-of-the-art Gaussian DP mechanism based on the Moment Ac-
count [39] (we will call it GM-DP in the rest of the paper). We also
show that TopAgg would bring additional advantages under limited
privacy budgets, where the utility gap between the DP model and
the vanilla model is large for traditional DP approaches.
Models.
A.2.1 Experimental Setup.
We evalute TopAgg for DP SGD training on two datasets, and
compare its performance with two baseline frameworks, including
the differentially private deep learning (DPDL) [2] and GM-DP [39].
Datasets. We experiment with two datasets commonly used in
DP SGD research: MNIST [30] and CIFAR-10 [29]. Both datasets are
standard image classification datasets. The description of MNIST
is provided in Section 5.1. Similarly, we use 60,000 instances for
training and 10,000 for testing. CIFAR-10 consists of 60,000 32×32
colored images of 10 classes. Among all, 50,000 instances are used
in training and 10,000 are used in testing.
For MNIST, we adopt a simple convolutional neural
network following the default model architecture provided in the
example in the open source Opacus library5. The network is con-
sisted of two convolutional layers each followed by a max pooling
layer, as well as two fully connected layers on the top.
For CIFAR-10, we follow the setting of DPDL, where we first pre-
train the classifiers on public datasets, then freeze the parameters
of feature extractor and finetune on the fully connected layers. In
our paper, we use ResNet-18 [21] as the architecture of the classi-
fier and load the model parameters pretrained on ImageNet6. We
replace the fully connected layer with a randomly initialized linear
head that takes features of 512-dimension extracted from ResNet
feature extractor as input and outputs 10-dimensional prediction
logits. During training, we freeze the parameters of ResNet feature
extractor including the parameters of Batch Normalization layers to
ensure that the feature extractor will not leak any privacy-sensitive
data information.
We compare the performance of TopAgg with two state of the art
DP SGD mechanisms: GM-DP [39] and DPDL [2]. In particular, we
build upon Opacus [1], a PyTorch implementation of GM-DP that
implements the DP SGD training scheme and privacy accountant
method in [39], which enables convenient control of randomness
in the framework. Our implementation of TopAgg for DP SGD
training is also built upon the Opacus library. For DPDL which is
the first work that proposed and evaluated the DP SGD training
scheme, we directly compare with the results reported in Section
5.2 and Section 5.3 of the paper for fairness, which present the best
model utility performance.
Evaluation Metrics. We adopt model utility, which is calcu-
lated as the classification accuracy of the trained models, as the
evaluation metric for assessing the effectiveness of our algorithm
5Code at https://github.com/pytorch/opacus/blob/master/examples/mnist.py
6Publicly available at https://pytorch.org/docs/stable/torchvision/models.html
Table 8: Model utility when adapting TopAgg to DP SGD training
on (a) MNIST and (b) CIFAR-10 with different privacy parameter 𝜀
and TopAgg parameter 𝑘. In all cases, 𝛿 = 10−5.
𝑘 = 0.6
73.87 ± 4.77
85.67 ± 1.48
89.84 ± 0.64
91.21 ± 0.53
92.83 ± 0.52
94.41 ± 0.19
95.13 ± 0.33
97.94 ± 0.19
𝜀 = 0.05
𝜀 = 0.10
𝜀 = 0.20
𝜀 = 0.30
𝜀 = 0.50
𝜀 = 0.70
𝜀 = 1.00
𝜀 = ∞
(a) MNIST
TopAgg
𝑘 = 0.7
75.81 ± 3.59
86.12 ± 1.39
90.25 ± 0.40
91.85 ± 0.21
93.50 ± 0.75
94.61 ± 0.24
95.22 ± 0.20
98.58 ± 0.11
(b) CIFAR-10
𝑘 = 0.8
77.15 ± 2.89
85.88 ± 1.97
90.80 ± 1.04
92.35 ± 0.53
93.82 ± 0.46
94.65 ± 0.27
95.42 ± 0.20
98.79 ± 0.09
GM-DP
78.40 ± 4.00
84.55 ± 0.98
91.17 ± 0.37
93.31 ± 0.42
94.38 ± 0.24
95.08 ± 0.10
95.41 ± 0.26
99.08 ± 0.04
𝑘 = 0.6
42.18 ± 0.89
65.10 ± 0.48
74.70 ± 0.26
75.56 ± 0.18
79.44 ± 0.07
80.19 ± 0.17
81.91 ± 0.10
82.56 ± 0.36
83.40 ± 0.06
84.34 ± 0.08
84.82 ± 0.04
85.13 ± 0.05
85.19 ± 0.3
TopAgg
𝑘 = 0.7
44.32 ± 0.59
71.47 ± 0.32
75.22 ± 0.21
76.91 ± 0.17
80.23 ± 0.12
81.70 ± 0.08
82.86 ± 0.05
82.91 ± 0.20
83.67 ± 0.22
84.80 ± 0.18
85.11 ± 0.08
85.37 ± 0.04
85.42 ± 0.02
𝑘 = 0.8
45.87 ± 0.97
72.03 ± 0.25
75.62 ± 0.15
77.65 ± 0.20
80.76 ± 0.08
82.15 ± 0.15
82.82 ± 0.13
83.44 ± 0.15
84.10 ± 0.07
85.18 ± 0.08
85.40 ± 0.08
85.62 ± 0.04
85.80 ± 0.03
GM-DP
41.58 ± 2.01
71.29 ± 0.27
75.13 ± 0.19
77.05 ± 0.09
80.37 ± 0.13
82.19 ± 0.10
82.80 ± 0.09
83.46 ± 0.04
84.44 ± 0.07
85.97 ± 0.04
86.63 ± 0.05
87.05 ± 0.03
87.85 ± 0.02
𝜀 = 0.025
𝜀 = 0.050
𝜀 = 0.075
𝜀 = 0.10
𝜀 = 0.20
𝜀 = 0.30
𝜀 = 0.40
𝜀 = 0.60
𝜀 = 0.80
𝜀 = 2.00