### D. Address Stride Distribution Analysis

From our simulation results, we have observed that the address trace is a critical factor in determining whether using a hash function for set-index calculation can reduce the false hit rate. To better understand why address strides are significant, we conducted a first-order analysis of the address trace distribution.

#### Figures
- **Figure 4**: False hits for a 3-way 12KB set-associative instruction cache: Hash-based vs No-Hash Schemes
- **Figure 5**: False hits for a 3-way 24KB set-associative instruction cache: Hash-based vs No-Hash Schemes

For this analysis, we computed the Hamming distance between the tag parts of each pair of consecutive addresses. The performance of the No-hash and Hash-based schemes depends on the Hamming distance value (Figure 7). For example, if the Hamming distance between tags is 1, two scenarios can occur:

1. **Same Set-Index**:
   - If the set-indexes for the two consecutive address references are the same, the No-hash scheme will map both addresses to the same set. A bit flip in the tag entry in the cache can result in a false hit. In contrast, the Hash-based scheme maps the new address reference to a different set, thereby reducing the likelihood of a false hit. This scenario favors the Hash-based scheme.

2. **Different Set-Indexes**:
   - If the set-indexes for the two consecutive address references are different, the No-Hash scheme will map these addresses to different sets, and a bit flip in the tag entry will not cause a false hit. The same reasoning applies to the Hash-based scheme. In this case, both schemes behave similarly, which we term a "Tie." However, for the Hash-based scheme, the probability that these two sets map to the same location in the tag part of the address is low. This is because, for a given pair of address references with different set-indexes, only one specific tag bit out of the `TagWidth` bits can be different in the tag portion of the reference.

If the Hamming distance between two consecutive tags is 0, both schemes perform identically, resulting in a Tie. For other cases where the Hamming distance is greater than 2, the result is also a Tie.

We performed this address stride distribution analysis, and the results are shown in Figure 8. The outcome distribution for the instruction cache is almost a Tie, with a slight favor towards the No-Hash scheme, consistent with our simulation results. For the data cache, the distribution either shows a Tie or favors the Hash-based method, as seen in benchmarks like MiniFE, SGEMM, DGEMM, and ZGEMM.

### VII. Analytical & Monte-Carlo False Hit Estimates for Random Trace

To gain an early understanding of the impact of various tag SRAM attributes (tag width, set associativity, set-index bits, and cache capacity), we analytically identified that tag width and associativity (i.e., the number of ways) have the most significant impact on the false hit probability. For a random address trace, the average probability of a false hit is bounded above by:

\[ \frac{\text{Associativity}}{2^{\text{TagWidth}}} \]

where Associativity is the number of ways in the set-associative cache, and TagWidth is the number of tag reference bits in the address bit field. Bounding the variance on the false hit probability is more complex due to its dependence on the nature of address references and the initial state of the tag address entries in the tag SRAM. We conducted millions of Monte-Carlo experiments to estimate the average false hit probability (under true miss events) and the standard deviation using random address traces. Tables III and IV show these results and validate the analytical upper bound on the average probability of false hits. The results also indicate that the false hit probability generally decreases exponentially with increasing tag width and slightly increases with increasing associativity. The standard deviation is of the same order of magnitude as the expected false hit probability, suggesting a higher variance compared to a normal distribution.

While the false hit probability for random traces in practical tag SRAM implementations is about two orders of magnitude lower (1e-04 vs. 1e-02) than the probability estimated through actual workload trace simulation, the sensitivity trend on tag width holds. For example, the instruction cache references had a 32-bit virtual-address width (with tag width = 20), and the maximum observed false hit probability under true miss was 0.1%. For the L1 data cache with a 48-bit virtual-address width (tag width = 33), the maximum observed false hit probability under true miss was 0.05%.

### VIII. Conclusion

GPU architectures have stringent reliability requirements, but they are particularly susceptible to radiation-induced errors due to technology scaling. Memory structures, including caches and register files, are responsible for the majority of transient errors in GPUs. To make these cache structures resilient to errors, entries in the caches are protected by parity/ECC bits, resulting in area and energy overhead.

In this paper, we studied the resiliency of unprotected tag caches in GPUs and proposed a very low-overhead tag checking method that can be applied to read-only and write-through caches in GPUs. This approach uses a hash function for set-index calculation instead of information redundancy. Our results show that the effectiveness of hash-based methods is sensitive to the nature of address traces. For instruction address traces, the access patterns (as shown by the address stride distribution analysis) show a tie between Hash-based and No-hash based set-index methods. The Hash-based method mainly benefits data cache tag structures, where it is 10 times better than the No-Hash method in terms of false hit probability. The hash function has almost negligible area and performance overhead. Both simulation and analytical results indicate that the impact of low-overhead tag error mitigation with or without set-index hashing on the overall GPU SDC FIT (failures in time) rate is less than 1%.

While the simulation studies were conducted for GPU tag SRAM structures, the results should also apply to CPU tag structures, as there are no unique features in the tag SRAM structures or address reference patterns specific to GPUs.

### IX. Acknowledgment

This work was partially supported by LLNS Subcontract No. B620719.

### References

[1] H. Stone, *Discrete Mathematical Structures and Their Applications*, ser. SRA Computer Science Series. Science Research Associates, 1973. [Online]. Available: https://books.google.com/books?id=tA-AAAAIAAJ

[2] R. W. Hamming, “Error Detecting and Error Correcting Codes,” *The Bell System Technical Journal*, vol. 29, no. 2, pp. 147–160, April 1950.

[3] S. Kim and A. K. Somani, “Area Efficient Architectures for Information Integrity in Cache Memories,” in *Proceedings of the 26th International Symposium on Computer Architecture (Cat. No. 99CB36367)*, 1999, pp. 246–255.

[4] S. Kim, “Reducing Area Overhead for Error-Protecting Large L2/L3 Caches,” *IEEE Transactions on Computers*, vol. 58, no. 3, pp. 300–310, March 2009.

[5] S. a. Kim, “Area-Efficient Error Protection for Caches,” in *Proceedings of the Design Automation Test in Europe Conference*, ser. DATE ’06. IEEE, 2006, pp. 1–6.

[6] A. González, M. Valero, N. Topham, and J. M. Parcerisa, “Eliminating Cache Conflict Misses Through XOR-Based Placement Functions,” in *Proceedings of the 11th International Conference on Supercomputing*, ser. ICS ’97. New York, NY, USA: ACM, 1997, pp. 76–83. [Online]. Available: http://doi.acm.org/10.1145/263580.263599

[7] Z. Zhang, Z. Zhu, and X. Zhang, “A Permutation-Based Page Interleaving Scheme to Reduce Row-Buffer Conflicts and Exploit Data Locality,” in *Proceedings 33rd Annual IEEE/ACM International Symposium on Microarchitecture*. MICRO-33 2000, 2000, pp. 32–41.

[8] F. Bodin and A. Seznec, “Skewed Associativity Improves Program Performance and Enhances Predictability,” *IEEE Transactions on Computers*, vol. 46, no. 5, pp. 530–544, May 1997.

[9] N. Topham and A. Gonzalez, “Randomized Cache Placement for Eliminating Conflicts,” *IEEE Transactions on Computers*, vol. 48, no. 2, pp. 185–192, Feb 1999.

[10] M. Kharbutli, K. Irwin, Y. Solihin, and J. Lee, “Using Prime Numbers for Cache Indexing to Eliminate Conflict Misses,” in *Software, IEE Proceedings-*, Feb 2004, pp. 288–299.

[11] H. Vandierendonck and K. D. Bosschere, “XOR-Based Hash Functions,” *IEEE Transactions on Computers*, vol. 54, no. 7, pp. 800–812, July 2005.

[12] A. Ros, P. Xekalakis, M. Cintra, M. E. Acacio, and J. M. García, “ASCIb: Adaptive Selection of Cache Indexing Bits for Removing Conflict Misses,” in *Proceedings of the 2012 ACM/IEEE International Symposium on Low Power Electronics and Design*, ser. ISLPED ’12. New York, NY, USA: ACM, 2012, pp. 51–56. [Online]. Available: http://doi.acm.org/10.1145/2333660.2333674

[13] B. R. Rau, “Pseudo-Randomly Interleaved Memory,” in *Proceedings of the 18th Annual International Symposium on Computer Architecture*, ser. ISCA ’91. New York, NY, USA: ACM, 1991, pp. 74–83. [Online]. Available: http://doi.acm.org/10.1145/115952.115961

[14] J. R. Diamond, D. S. Fussell, and S. W. Keckler, “Arbitrary Modulus Indexing,” in *Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture*, ser. MICRO-47. Washington, DC, USA: IEEE Computer Society, 2014, pp. 140–152. [Online]. Available: http://dx.doi.org/10.1109/MICRO.2014.13

[15] M. Khairy, M. Zahran, and A. G. Wassal, “Efficient Utilization of GPGPU Cache Hierarchy,” in *Proceedings of the 8th Workshop on General Purpose Processing Using GPUs*, ser. GPGPU-8. New York, NY, USA: ACM, 2015, pp. 36–47. [Online]. Available: http://doi.acm.org/10.1145/2716282.2716291

[16] K. Y. Kim and W. Baek, “Quantifying the Performance and Energy Efficiency of Advanced Cache Indexing for GPGPU Computing,” *Microprocess. Microsyst.*, vol. 43, no. C, pp. 81–94, Jun. 2016. [Online]. Available: https://doi.org/10.1016/j.micpro.2016.01.003

[17] “NVIDIA Tesla V100 GPU Architecture,” June 2017, White Paper. [Online]. Available: http://www.nvidia.com/object/volta-architecture-whitepaper.html

[18] N. R. Saxena, C. W. D. Chang, K. Dawallu, J. Kohli, and P. Helland, “Fault-Tolerant Features in the HAL Memory Management Unit,” *IEEE Transactions on Computers*, vol. 44, no. 2, pp. 170–180, Feb 1995.