time from wakeup to running. These events can be very frequent on busy production systems,
runqlat(8) works by instrumenting scheduler wakeup and context switch events to determine the
exceeding one million events per second. Even though BPF is optimized, at these rates even
adding one microsecond per event can cause noticeable overhead.° Use with caution.
Misconfigured Build
Here is a different example for comparison. This time a 36-CPU build server is doing a software
build, where the number of parallel jobs has been set to 72 by mistake, causing the CPUs to be
overloadled:
runqlat 10 1
Tracing run g
queue
Hit Ctel-C to end.
usec5
1 count
distribution
0 -> 1
: 1906
| ++*
2 > 3
: 22087
4 -> 7
: 21245
8 -> 15
: 7333
|+**.*..**+**+
1.6 -> 31
: 4902
32 > 63
: 6002
64 -> 127
: T370
128 -> 255
:13001
9 As a simple exercise, if you had a context switch rate of 1M/sec across a 10-CPU system, adding 1 microsecond per
confextswitchwouldconsu
me 10% of CPU resouroes (100% × (1 × 1000000 / 10 × 1000000). See Chapter 18 for
some real measurements of BPF overhead, which is typically much less than one microsecond per event,
---
## Page 242
6.3 BPF Tools
205
256 -> 511
: 4823
|+**+**+*
512 -> 1023
: 1519
|**
1024 > 2047
: 3682
******
2048 -> 4095
: 3170
| +****
4096 > 8191
: 5759
8192 -> 16383
: 14549
16384 > 32767
: 5589
|***+*****+
SE5S9  131071
: 10
bucket. This shows significant waiting by threads.
The distribution is now tri-modal, with the slowest mode centered in the 8- to 16-millisecond
This particular issue is straightforward to identify from other tools and metrics. For example,
sar(1) can show CPU utilization (u) and run queue metrics (q):
 sar -uq 1
Linux 4,18,0=virtual 1..-1
01/21/2019
_x86_64_
(36 CPU)
11:06:25 PM
CPU
guser
kni.ce
laystem
?iovait
tstea1
gidle
11:06:26 PK
a11
88.06
0.00
11. 94
0 .00
0.00
0,00
11:06:25 PH
runq=52
plist-s2
1davg-1
1davg5
1davg-15
b1ocked
11:06:26 PM
1030
65,90
41,52
34,75
0
[. --]
This sar(1) output shows 0% CPU idle and an average run queue size of 72 (which includes both
running and runnable)more than the 36 CPUs available.
Chapter 15 has a runqlat(8) example showing per-container latency.
BCC
Command line usage for the BCC version:
runqlat[options][inteera][count]]
Options include:
•-m: Prints output in milliseconds
II ssaood sad uesBosuq e squd d-
aoedsatueu (lld sad tuen3osuq e squd :ssupd-- 
=p PID: Traces this process ID only
• T: Includes timestamps on output
---
## Page 243
206
Chapter 6 CPUs
The T option is useful for annotating per-interval output with the time, For example, runql at
7 1 for timestamped per-second output.
bpftrace
The following is the code for the bpftrace version of runqlat(8), which summarizes its core
functionality. This version does not support options.
1/usr/local/bin/bpftrace
include 
BEGIN
printf (*Tracing CPU scheduler... Hit CtrlC to end. n*)
tracepoint:sched: sched_xakeup,
tracepoint:sched:sched_xakeup_nex
Bqtine [args->pid] - nsecs]
1f (axgs=>prev_state == TASK_RUNNING) (
eqtine [args->prev_pid] = nsecs
na = Bqtine [args=>next_pld];
if ($ns][
eusecs = hist((nsecs - $ns) / 1000)
 1 [ptdxeupid, which is the kernel thread ID.
---
## Page 244
6.3  BPF Tools
207
The sched_switch action stores a timestamp on args->prev_pid if that state was still runnable
(TASK_RUNNING). This is handling an involuntary context switch where, the moment the thread
leaves the CPU, it is returned to a run queue. That action also checks whether a timestamp was
stored for the next runnable process and, if so, calculates the time delta and stores it in the @usecs
histogram.
Since TASK_RUNNING was used, the linux/sched.h header file was read (#include) so that its
definition was available.
The BCC version can break down by PID, which this bpftrace version can easily be modified to
do by adding a pid key to the @Pusecs map. Another enhancement in BCC is to skip recording run
queue latency for PID 0 to exclude the latency of scheduling the kernel idle thread. Again, this
program can easily be modified to do the same.
6.3.4 runqlen
runqlen(8) is a BCC and bpftrace tool for sampling the length of the CPU run queues, counting
how many tasks are waiting their turn, and presenting this as a linear histogram. This can be used
to further characterize issues of run queue latency or as a cheaper approximation.
The following shows runqlet(8) from BCC running on a 48-CPU production API instance that is
at about 42% CPU utilization system-wide (the same instance shown earlier with runqlat(8)). The
arguments to runqlen(8) are *10 1° to set a 10-second interval and output only once:
I OT uetbunz 
Sanpling run queue length... Hit Ctel-C to end.
runqlen
1count
dis tribution
U
: 47284
1
: 211
: 28
2
3
: 6
1
4
: 4
：
: 1
1
1
: 1
This shows that most of the time, the run queue length was zero, meaning that threads did not
need to wait their turn.
I describe run queue length as a secondary performance metric and run queue latency as primary.
Unlike length, latency directly and proportionately affects performance Imagine joining a
checkout line at a grocery store. What matters more to you: the length of the line or the time you
actually spend waiting? runqlat(8) matters more. So why use runqlen(8)?
10 Thanks, Ivan Bsbrou, for adding that.
1.1. Origin: 1 created the first version, called dispglen.d, on 27-Jun-2005, to help characterize run queue lengths by CPU
I developed the BCC version on 12-Dec-2016 and the bpftrace version on 7-0ct-201.8.
---
## Page 245
208
Chapter 6 CPUs
First, runqlen(8) can be used to further characterize issues found in runqlat(8) and explain
how latencies become high. Second, runqlen(8) employs timed sampling at 99 Hertz, whereas
runqlat(8) traces scheduler events. This timed sampling has negligible overhead compared to
runqlat(8)’s scheduler tracing. For 24x7 monitoring, it may be preferable to use runqlen(8) first to
identify issues (since it is cheaper to run) and then use runqlat(8) ad hoc to quantify the latency.
Four Threads, One CPU
In this example, a CPU workload of four busy threads was bound to CPU 0. runqlen(8) was
executed with C to show per-CPU histograms:
runqlen -C
Sanpling run queve
Hit Ctrl-C to
°C
cpα = 0
runqlen
1count
distribution
: 0
: 0
H
: 0
: 551
] * ×
runqlen
:count
distzibutlon
: 41
cpu - 2
runqlen
: count
disteibutlon
U
: 126
[..-]
The run queue length on CPU 0 was three: one thread on-CPU and three threads waiting This
per-CPU output is useful for checking scheduler balance.
BCC
Command line usage for the BCC version:
runqlen [optlons][intezval [count]]
Options include:
 -C: Prints a histogram per CPU
▪ 0: Prints run queue occupancy
 T: Includes timestamps on output
---
## Page 246
6.3 BPF Tools
209
Run queue occupancy is a separate metric that shows the percentage of time that there were
threads waiting. This is sometimes useful when a single metric is needed for monitoring, alerting,
and graphing.
bpftrace
The following is the code for the bpftrace version of runqlen(8), which summarizes its core func-
tionality. This version does not support options.
#1/usx/1ocal/bin/bpEtrace
#Include 
struct cfs_rq_partial (
struct load_weight load;
unslgned long runnable_welght:
unsigned int nr_running
BEGIN
↑
printf (*Sanpling run queue length at 99 Hertz... Hit Ctel-C to end.n*)
profile:hz:99
$task = (struct task_struct *) curtask;
bxsgo*s 0 ? $1en - 1 : 0
//subtraet cuxrently running task
Brunqlen = 1hist ($len, 0, 100, 11 
The program needs to reference the nr_running member of the cfs_rq struct, but this struct is
not available in the standard kernel headers. So the program begins by defining a cfs_rq_partial
struct, enough to fetch the needed member. This workaround may no longer be needed once BTF
is available (see Chapter 2).
The main event is the profile:hz:99 probe, which samples the run queue length at 99 Hertz on
all CPUs. The length is fetched by walking from the current task struct to the run queue it is on
and then reading the length of the run queue. These struct and member names may need to be
adljusted if the kernel source changes.
You can have this bpftrace version break down by CPU by axdding a cpu key to @runqlen.
---
## Page 247
210
Chapter 6 CPUs
6.3.5
runqslower
runqslower(8)12 is a BCC tool that lists instances of run queue latency exceeding a configu-
rable threshold and shows the process that suffered the latency and its duration. The following
example is from a 48-CPU production API instance currently running at 45% CPU utilization
system-wide:
Tracing run queve
runqslower
 Latency higher than
10000 us
TIME
COMH
P1D
LAT (us]
17:42:49 python3
4590
16345
17:42:50 poo125=thread-
E895
50001
17:42:53 ForkJoinPool .co
5898
11935
17:42:56 python3
4590
10191
17:42:56 ForkJoinPool co
5912
13738
17 :42 :56 Foxk.Jo1nPoo1 . co
8065
11434
17:42:57 ForkJoinFool.co
5890
11436
17:43:00 FoxkJo1nPoo1 co
5477
10502
17:43:01l gcpc=default=vo
5794
11637
17:43:02 toncat-exec-296
6373
12083
[...]
This output shows that over a period of 13 seconds, there were 10 cases of run queue latency
exceeding the default threshold of 10000 microseconds (10 milliseconds). This might seem
surprising for a server with S5% idle CPU headroom, but this is a busy multi-threaded applica-
tion, and some run queue imbalance is likely until the schexduler can migrate threads to idle
CPUs. This tool can confirm the affected applications.
This tool currently works by using kprobes for the kernel functions ttwu_do_wakeup0,
wake_up_new_task(), and finish_task_switch(). A future version should switch to scheduler
tracepoints, using code similar to the earlier bpftrace version of runqlat(8). The overhead is
similar to that of runqlat(8); it can cause noticeable overhead on busy systems due to the cost of
ndno Ate Suud stou st (g)zamofsbunu aqm uana saqoud aq
Command line usage:
runqsloxer[options](min_us]
Options include:
•=p PID: Measures this process only
The default threshold is 10000 microseconds.
12 0rigin: This wss created by
an Babrou on 2-Msy-2018
---
## Page 248
6.3 BPF Tools
211
6.3.6 cpudist
cpuxdist(8) is a BCC tool for showing the distribution of on-CPU time for each thread wakeup.
This can be used to help characterize CPU workloads, providing details for later tuning and design
decisions. For example, from a 48-CPU production instance:
 cpudist 10 1
Tracing on=CPU tine.
H1t Ctel-C to end.
Use c.s
: count
distrlbutlon
0 > 1
: 103865
E  7
: 134188
8 -> 15
: 1498 62
16 > 31
:122285
∈9  127
: 27103
J*.....*
128 -> 255
: 4835
1*
256 > 511
:692
512 -> 1023
: 320
1024 > 2047
: 328
2048 -> 4095
: 412
4096 -> 8191
: 356
E8E9T 32767
: 42
32768 -> 65535
: 30
65536 -> 131071
: 22
131072 -> 262143
: 20
262144 > 524287