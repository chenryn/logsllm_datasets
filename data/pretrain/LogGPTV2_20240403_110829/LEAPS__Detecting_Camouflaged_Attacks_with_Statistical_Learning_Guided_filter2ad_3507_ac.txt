framework to log system events and generate stack walk
traces. The ETW framework is a general-purpose tracing
engine equipped in the latest Windows operating systems (ﬁrst
introduced in Windows 2000). It provides a tracing mechanism
to log events triggered in multiple system layers, from user
applications to kernel components. ETW has been widely
adopted by third-party management
tools for performance
diagnostics. The output of ETW is an Event Tracing Log
(ETL) ﬁle, which is the raw input to LEAPS. ETW allows
us to enable stack walking for a selection of system events,
e.g., system call, process/thread creation, image load/unload,
ﬁle operations, registry tracing, etc. We parse the raw ETL ﬁle
to generate a stack-event correlated log. We perform all ETW
logging on a machine with an Intel Core i7 3.40 GHz CPU,
12GB RAM, and Windows Server 2008 R2 64-bit operating
system.
We implement the Stack Partition Module, Data Prepro-
cessing Module, and Control Flow Graph Inference Module
in Python. When grouping the library and function set in the
Data Preprocessing Module, we use the hierarchical clustering
implementation in the clustering package of SciPy2 and choose
UPGMA method as the linkage criterion, i.e., the distance
between any two clusters is the mean distance between all
elements of each cluster.
We implement the Supervised Statistical Learning Module
under the LIBSVM [26] framework. LIBSVM3 is an integrated
system for support vector classiﬁcation, regression and dis-
tribution estimation with a wide range of machine learning
applications. The input of the Weighted SVM model is the
benign and mixed (with weights) training data. The output of
LIBSVM is a binary classiﬁcation model, which we use for
attack detection in our testing data. In our implementation, we
use 10-fold cross validation [25] to tune the model parameter
λ and σ2 on the training set.
2http://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html
3http://www.csie.ntu.edu.tw/~cjlin/libsvm/
V. EVALUATION
In this section, we report our evaluation results on the
effectiveness of LEAPS. First we describe the datasets in our
experiments, i.e., the source of the data and the criteria of data
selection. Then we discuss the procedure of our experiments
and the measurements of the evaluation. Finally we examine
three representative cases in detail and present the results of
all other cases brieﬂy.
A. Dataset
1) Data Source: We use 21 datasets (Table I) of different
combinations of applications, malicious payloads, and attack
methods to evaluate our approach. We categorize the attack
methods into two groups: ofﬂine infection (malicious payload
embedded in a benign binary) and online injection (malicious
payload injected into a benign process at runtime). Each
dataset consists of three subsets: a) pure benign samples, b)
mixed samples, and c) pure malicious samples.
We obtain pure benign samples by exercising the benign
application. Mixed samples are from proﬁling either trojaned
applications (i.e., ofﬂine infection) or tampered processes (i.e.,
online injection). Thus mixed samples contain both benign and
malicious events. Pure benign samples and mixed samples can
be naturally collected in the real environment. We use them
as positive/negative samples for training. As we mentioned
before, because mixed samples contain benign events as noise,
classiﬁers learned by traditional statistical learning methods
are not accurate.
Pure malicious samples are difﬁcult to obtain in a real
environment because malicious payloads are always attached
to benign applications. For this evaluation, we manually extract
the malicious payloads and recompile them as independent
malware. Here we only use pure malicious samples as the
ground truth for testing to verify the effectiveness of our
binary classiﬁer on negative samples. After hierarchical clus-
tering, each subset contains three features: Event_Type, Lib,
and Func.
2) Data Selection: We select the training data for learning a
binary classiﬁer from: a) pure benign samples (positive training
samples) and b) mixed samples (negative training samples). We
select the testing data from: a) pure benign samples (positive
testing samples) and c) pure malicious samples (negative
testing samples). To avoid training and testing on the same
benign samples, we divide the pure benign samples into two
non-overlapping parts, 50% for training and 50% for testing.
Taking the order of adjacent events into account, we increase
the dimensions from 3 up to 30 by coalescing each 10
consecutive samples into one 30-dimension data point. Due
to the large number of data samples, we randomly select 20%
of the samples from each dataset to form the training and
testing sets. In this way, we can achieve reasonable running
time for the training phase and also near-complete coverage
of the behavior in each dataset.
B. Evaluation Procedure and Measurement of Effectiveness
We compare our CFG guided Weighted SVM approach
(denoted WSVM in Figure 6 and 7) with the other two classi-
ﬁcation approaches, i.e., approaches based on the system-level
6363
TABLE I: Evaluation Results of LEAPS on Camouﬂaged Attacks
Name
winscp_reverse_tcp
winscp_reverse_https
chrome_reverse_tcp
chrome_reverse_https
notepad++_reverse_tcp
notepad++_reverse_https
putty_reverse_tcp
putty_reverse_https
vim_reverse_tcp
vim_reverse_https
vim_codeinject
notepad++_codeinject
putty_codeinject
putty_reverse_tcp_online
putty_reverse_https_online
notepad++_reverse_tcp_online
notepad++_reverse_https_online
vim_reverse_tcp_online
vim_reverse_https_online
winscp_reverse_tcp_online
winscp_reverse_https_online
Application
Attack Method
Ofﬂine Infection WinSCP
Ofﬂine Infection WinSCP
Chrome
Ofﬂine Infection
Chrome
Ofﬂine Infection
Notepad++
Ofﬂine Infection
Ofﬂine Infection
Notepad++
Putty
Ofﬂine Infection
Putty
Ofﬂine Infection
Vim
Ofﬂine Infection
Vim
Ofﬂine Infection
Vim
Ofﬂine Infection
Ofﬂine Infection
Notepad++
Putty
Ofﬂine Infection
Putty
Online Injection
Putty
Online Injection
Notepad++
Online Injection
Notepad++
Online Injection
Vim
Online Injection
Online Injection
Vim
Online Injection WinSCP
Online Injection WinSCP
Payload
Reverse TCP Shell
Reverse HTTPS Shell
Reverse TCP Shell
Reverse HTTPS Shell
Reverse TCP Shell
Reverse HTTPS Shell
Reverse TCP Shell
Reverse HTTPS Shell
Reverse TCP Shell
Reverse HTTPS Shell
Pwddlg
Pwddlg
Pwddlg
Reverse TCP Shell
Reverse HTTPS Shell
Reverse TCP Shell
Reverse HTTPS Shell
Reverse TCP Shell
Reverse HTTPS Shell
Reverse TCP Shell
Reverse HTTPS Shell
ACC
0.932
0.927
0.877
0.907
0.846
0.866
0.886
0.869
0.914
0.919
0.852
0.802
0.802
0.894
0.869
0.927
0.845
0.963
0.919
0.950
0.921
PPV
0.999
0.991
0.998
0.998
0.998
0.998
0.815
0.999
0.995
0.998
0.985
0.948
0.919
0.825
0.999
0.991
0.998
0.933
0.995
0.996
0.998
TPR
0.865
0.862
0.755
0.815
0.693
0.733
0.998
0.739
0.832
0.839
0.715
0.639
0.661
0.999
0.738
0.861
0.690
0.998
0.842
0.904
0.843
TNR
0.999
0.992
0.999
0.999
0.998
0.998
0.774
0.999
0.996
0.999
0.989
0.965
0.942
0.789
0.999
0.992
0.999
0.928
0.996
0.996
0.998
NPV
0.881
0.878
0.803
0.844
0.765
0.789
0.998
0.793
0.856
0.861
0.776
0.728
0.736
0.999
0.792
0.877
0.763
0.998
0.863
0.912
0.864
call graph (denoted CGraph in Figure 6 and 7) and traditional
SVM, on all 21 datasets. We set the model parameters λ and σ2
using 10-fold cross validation on the training set. To eliminate
ﬂuctuation caused by the random selection of the training and
testing sets, we average all results over 10 runs.
We measure the performance of the classiﬁcation results
based on: True Positives (TP), True Negatives (TN), False
Positives (FP), and False Negatives (FN). TP indicates ac-
tual benign samples that are correctly classiﬁed as benign.
Similarly, TN represents malicious samples that are correctly
classiﬁed as malicious. FP indicates malicious samples that
are misclassiﬁed as benign. FN represents benign samples that
are misclassiﬁed as malicious. Based on these four results,
we evaluate the performance of the different methods by
ﬁve measurements: 1) Accuracy (ACC), 2) Positive Predictive
Value (PPV or Precision), 3) True Positive Rate (TPR or
Recall), 4) True Negative Rate (TNR or Speciﬁcity), and 5)
Negative Predictive Value (NPV) [30].
1) Accuracy: By deﬁnition, the ACC is the portion of the
true results (both TP and TN) in the total test samples.
ACC =
T P + T N
T P + F P + F N + T N
(6)
According to Figure 6 and 7,
the ACCs of all applica-
tions elevate by varying degrees when using WSVM com-
pared to SVM and CGraph. For example, the ACC of win-
scp_reverse_https_online increases from 59.9% (CGraph) to
92.1% (WSVM), which reﬂects a signiﬁcant improvement on
the overall hit rate of both benign and malicious prediction.
Though ACC indicates the overall performance of a binary
classiﬁcation, it may yield misleading results if the data set
is unbalanced. Thus, we introduce four other measurements
based on the confusion matrix (TP, TN, FP, FN) to give a
more comprehensive evaluation of the experimental results.
2) Positive Predictive Value: Also known as precision, PPV
measures the portion of actual benign samples in all predicted
benign samples.
P P V =
T P
F P + T P
(7)
As we can see from Figure 6 and 7, WSVM pro-
duces the highest PPV values. For instance, the PPVs of
putty_reverse_tcp_online are 71.2% (CGraph), 79.6% (SVM)
and 82.5% (WSVM).
3) True Positive Rate: Also known as recall, TPR measures
the number of instances that are correctly classiﬁed as benign
out of the total benign instances.
T P R =
T P
T P + F N
(8)
for all
The TPR of WSVM has obvious improvement
21 cases. For
the TPR of
putty_reverse_https_online increases from 41.7% (CGraph) to
56.4% (SVM) and reaches 73.8% (WSVM).
in Figure 7,
example,
4) True Negative Rate: True Negative Rate is also known as
speciﬁcity. Similar to TPR, TNR calculates, out of the instances
that are actually malicious, the number of instances that are
correctly classiﬁed as malicious.
T N R =
T N
F P + T N
(9)
From Figure 6, the TNR of vim_codeinject increases from
67.9% (CGraph) to 98.9%(WSVM). We have similar improve-
ments of TNR on all other 20 cases according to Figure 6 and
7.
5) Negative Predictive Value: Similar to PPV, NPV mea-
sures the portion of the actually malicious samples out of the
total predicted malicious samples.
N P V =
T N
T N + F N
(10)
Again, WSVM ranks the highest in all 21 applications in terms
of NPV. For instance, the NPV of putty_reverse_https_online
increases from 69.9% (SVM) to 79.2% (WSVM), as seen in
Figure 7.
C. Results and Discussion
Figure 6 and 7 show the results of the ofﬂine infection
and online injection datasets respectively. We also present the
detailed results of all datasets in Table I. From these ﬁgures, we
can see that the proposed CFG guided Weighted SVM method
achieves the best results on all measurements in all cases. In
the rest of this section, we discuss three representative cases
in detail.
6464
CGraph
SVM
WSVM
CGraph
SVM
WSVM
CGraph
SVM
WSVM
CGraph
SVM
WSVM
CGraph
SVM