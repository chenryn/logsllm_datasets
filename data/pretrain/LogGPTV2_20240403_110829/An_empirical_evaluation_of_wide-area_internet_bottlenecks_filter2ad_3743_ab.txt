ﬁnding and accessing working servers. For each NAP, we select all
paths which appear to go through the NAP. For this purpose, we
use the router DNS names as the determining factor. Speciﬁcally,
we look for the name of the NAP to appear in the DNS name of any
router in the path. From the selected paths, we pick out the routers
one-hop away (both a predecessor and a successor) from the router
identiﬁed to be at the NAP and collect their IP addresses. This gives
us a collection of IP addresses for routers that could potentially be
used as destinations to measure paths passing through NAPs.
However, we still have to ensure that the paths do in fact traverse
the NAP. For this, we run traceroutes from each of our PlanetLab
sources to each of the predecessor and successor IP addresses iden-
tiﬁed above. For each PlanetLab source, we record the subset of
these IP addresses whose traceroute indicates a path through the
corresponding NAP. The resulting collection of IP addresses is used
as a destination set for the PlanetLab source.
2.3 Bottleneck Identiﬁcation Tool – BFind
Next, we need a tool that we can run at the chosen sources that
will measure the bottleneck link along the selected paths. We deﬁne
the bottleneck as the link in the path where the available bandwidth
(i.e., left-over capacity) to a TCP ﬂow is the minimum. Notice that
a particular link being a bottleneck does not necessarily imply that
the link is heavily utilized or congested.
In addition, we would
like the tool to report the available bandwidth, latency and location
(i.e. IP addresses of endpoints) of the bottleneck along a path. In
this section, we describe the design and operation of our bottleneck
identiﬁcation tool – BFind.
2.3.1 BFind Design
BFind’s design is motivated by TCP’s property of gradually ﬁll-
ing up the available capacity based on feedback from the network.
First, BFind obtains the propagation delay of each hop to the des-
tination. For each hop along the path, the minimum of the (non-
negative) measured delays along the hop is used as an estimate for
the propagation delay on the hop 1. The minimum is taken over
delay samples from 5 traceroutes.
After this step, BFind starts a process that sends UDP trafﬁc at a
low sending rate (2 Mbps) to the destination. A trace process also
starts running concurrently with the UDP process. The trace pro-
cess repeatedly runs traceroutes to the destination. The hop-by-hop
delays obtained by each of these traceroutes are combined with the
raw propagation delay information (computed initially) to obtain
rough estimates of the queue lengths on the path. The trace process
concludes that the queue on a particular hop is potentially increas-
ing if across 3 consecutive measurements, the queuing delay on the
hop is at least as large as the maximum of 5ms and 20% of the raw
propagation delay on the hop. This information, computed for each
hop by the trace process, is constantly accessible to the UDP pro-
cess. The UDP process uses this information (at the completion of
each traceroute) to adjust its sending rate as described below.
If the feedback from the trace process indicates no increase in
the queues along any hop, the UDP process increases its rate by
200 Kbps (the rate change occurs once per feedback event, i.e., per
traceroute). Essentially, BFind emulates the increase behavior of
TCP, albeit more aggressively, while probing for available band-
width. If, on the other hand, the trace process reports an increased
delay on any hop(s), BFind ﬂags the hop as being a potential bot-
tleneck and the traceroutes continue monitoring the queues. In ad-
dition, the UDP process keeps the sending rate steady at the current
value until one of the following things happen: (1) The hop contin-
ues to be ﬂagged by BFind over consecutive measurements by the
trace process and a threshold number (15) of such observations are
made for the hop. (2) The hop has been ﬂagged a threshold number
1If the difference in the delay to two consecutive routers along a
path is negative, then the delay for the corresponding hop is as-
sumed to be zero
of times in total (50). (3) BFind has run for a pre-deﬁned max-
imum amount of total time (180 seconds). (4) The trace process
reports that there is no queue build-up on any hop implying that the
increasing queues were only a transient occurrence.
In the ﬁrst two cases, BFind quits and identiﬁes the hop respon-
sible for the tool quitting as being the bottleneck. In the third case,
BFind quits without providing any reliable conclusion about bot-
tlenecks along the path. In the fourth case, BFind continues to in-
crease its sending rate at a steady pace in search of the bottleneck.
If the trace process observes that the queues on the ﬁrst 1-3 hops
from the source are building, it quits immediately, to avoid ﬂood-
ing the local network (The ﬁrst 3 hops almost always encompass all
links along the path that belong to the source stub network). Also,
we limit the maximum send rate of BFind to 50Mbps to make sure
that we do not use too much of the local area network capacity
at the PlanetLab sites. Hence, we only identify bottlenecks with
40Mbps, the ﬁrst two destinations), low ( 97% of the paths we probed, BFind completed well before
180s, either because a bottleneck was found or because the limit on
the send rate was reached.
Destination Node
Path length
CMU-PL
Princeton-PL
KU-PL
Pittsburgh-node
www.fnsi.net
www.i1.net
14
12
15
14
11
11
Pathload Report
58.1 - 107.2Mbps
91.3 - 96.8Mbps
8.23 - 8.87Mbps
4.17 - 5.21Mbps
N/A
N/A
Pipechar Report
82.4Mbps
94.5Mbps
BFind Report
>39.1Mbps
>20.5Mbps
5.21Mbps (hop 12)
4.32Mbps (hop 11)
8.2Mbps (hop 10)
19.21Mbps (hop 7)
9.88Mbps (hop 12)
8.34Mbps (hop 11)
8.43Mbps (hop 10)
32.91Mbps (hop 8)
Table 3: BFind validation results: Statistics for the comparison between BFind, Pathload and Pipechar
2.4 Metrics of Interest
Based on the results of BFind, we report the bandwidth and la-
tency of the bottlenecks we discover. In addition to these metrics,
we post-process the tool’s output to report on the ownership and
location of Internet bottlenecks. Such a categorization helps iden-
tify what parts of the Internet may constrain high-bandwidth ﬂows
and what parts to avoid in the search for good performance. We
describe this categorization in greater detail below.
In our analysis, we ﬁrst classify bottlenecks according to own-
ership. According to this high level classiﬁcation, bottlenecks can
be described as either those within carrier ISPs, which we further
classify by the tier of the owning ISP, or those between carrier
ISPs, which we further classify according to the tiers of the ISPs
at each end of the bottleneck. In order to characterize each link in
our measurements according to these categories, we use a variety
of available utilities. We identify the AS owning the endpoint of