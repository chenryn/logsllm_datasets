title:Evading Classifiers by Morphing in the Dark
author:Hung Dang and
Yue Huang and
Ee-Chien Chang
Evading Classiﬁers by Morphing in the Dark
Hung Dang, Yue Huang, Ee-Chien Chang
School of Computing, National University of Singapore
7
1
0
2
g
u
A
3
2
]
R
C
.
s
c
[
3
v
5
3
5
7
0
.
5
0
7
1
:
v
i
X
r
a
Abstract
Learning-based systems have been shown to be vulnerable to
adversarial data manipulation attacks. These attacks have
been studied under assumptions that the adversary has cer-
tain knowledge of either the target model internals, its train-
ing dataset or at least classiﬁcation scores it assigns to input
samples.
In this paper, we investigate a much more con-
strained and realistic attack scenario that does not assume
any of the above mentioned knowledge. The target classiﬁer
is minimally exposed to the adversary, revealing on its ﬁnal
classiﬁcation decision (e.g., reject or accept an input sam-
ple). Moreover, the adversary can only manipulate malicious
samples using a blackbox morpher. That is, the adversary
has to evade the target classiﬁer by morphing malicious sam-
ples “in the dark”. We present a scoring mechanism that can
assign a real-value score which reﬂects evasion progress to
each sample based on limited information available. Lever-
aging on such scoring mechanism, we propose a hill-climbing
method, dubbed EvadeHC, that operates without the help
of any domain-speciﬁc knowledge, and evaluate it against
two PDF malware detectors, namely PDFrate and Hi-
dost. The experimental evaluation demonstrates that the
proposed evasion attacks are eﬀective, attaining 100% eva-
sion rate on our dataset. Interestingly, EvadeHC outperforms
the known classiﬁer evasion technique that operates based
on classiﬁcation scores output by the classiﬁers. Although
our evaluations are conducted on PDF malware classiﬁer,
the proposed approaches are domain-agnostic and is of wider
application to other learning-based systems.
1.
INTRODUCTION
Machine learning techniques have witnessed a steady
adoption in a wide range of application domains such
as image classiﬁcation [32] or natural
language process-
ing [34]. Various learning-based systems have been reported
to achieve high accuracy, even surpassing human-level per-
formance [15, 27]. Given the assumption that the training
datasets are representative, these systems are expected to
perform well in reality on operational data.
Learning methods have also lent itself to security tasks.
Numerous innovative applications of machine learning in
security contexts, especially for detection of security vio-
lations, have been discussed in the literature [29, 19, 22].
However, when learning-based systems are deployed for se-
curity applications, their accuracy may be challenged by in-
tentional noise and deviations. Various studies [7, 12, 11]
suggest that they are likely susceptible to adversarial data
manipulation, for the assumption on representativeness of
the training datasets no longer holds true in the presence
of malicious adversaries. For example, a motivated attacker
may be able to morph a malicious instance so that it re-
sembles a benign instance found in the training datasets,
evading the existing learning-based classiﬁer.
Several works have studied evasion attacks against
learning-based systems, making diﬀerent assumptions on the
amount of knowledge the adversaries have about the system
they are attacking. For examples, ˇSrndic et al. [31] and
Sharif et al.[25] studied attack scenarios wherein the adver-
saries have a high level of knowledge about the internals
of the target system (e.g., features space and classiﬁcation
algorithm). Xu et al. [35] investigated a more constrained
evasion scenario in which the adversaries only have black-
box accesses to the target detector that outputs a real-value
classiﬁcation score for an input sample1. While the evasion
attacks are eﬀective in the presence of the required auxil-
iary information, the authors of [35] suggested that a simple
preventive measure of hiding the classiﬁcation score would
be suﬃcient to thwart their attacks. Thus, it remains a
question how these attacks operate against blackbox systems
deployed in reality (e.g., malware detector built-in to email
services), for they are unlikely to reveal real-value scores, but
rather expose only the ﬁnal decisions (e.g., reject or accept
a sample).
In this work, we study the problem of classiﬁer evasion
in the dark, investigating an evasion scenario that is much
more constrained and realistic than the ones considered in
existing works [35, 31, 26]. In particular, we consider a very
restricted setting in which the adversary does not have any
knowledge about the target system (e.g., model internals
and training dataset). Moreover, the target system only
reveals its ﬁnal decision instead of a real-value score that
reﬂects its internal state. We further assume that the only
feasible mean to manipulate the samples is through some
given tools that perform “random” morphing.
To this end, we formulate a model that captures the
above-mentioned restricted setting. Under this model, the
adversary is conﬁned to three blackboxes, a binary-output
detector (or classiﬁer) that the adversary attempts to evade,
a tester that checks whether a sample possesses malicious
functionality, and a morpher that transforms the samples.
The adversary’s goal is to, given a malicious sample, adap-
tively queries the blackboxes to search for a morphed sample
that evades detection, and yet maintains its malicious func-
tionality.
We show that under this constrained setting with only
blackbox accesses, learning-based systems are still suscepti-
ble to evasion attacks. To demonstrate feasibility of eﬀec-
tive evasion attacks, we present an evasion method, dubbed
EvadeHC, which is based on hill-climbing techniques. The
main component of EvadeHC is a scoring mechanism that
assigns real-value score to the sample based on the binary
1Although the approach treats the detector as a blacbox, it
still makes use of some feature property. Speciﬁcally, the
approach exploited a property that a sequence of morphing
steps which work for one malicious sample is likely to work
for others to accelerate the evasion.
outcomes obtained from the tester and detector. The intu-
ition is to measure the number of morphing steps required
to change the detector and tester’s decisions, and derive the
score based on these values. We believe that this scoring
mechanism can be used to relax the assumption on the avail-
ability of real-value scores that other settings [20, 26, 35]
make.
We evaluate the proposed evasion technique on two well-
known PDF malware classiﬁers, namely PDFrate [28] and
Hidost [30]. In order to enable a fair basis for benchmarking
with previous work, we adopt the dataset that is used by
Xu et al. [35] in our experiments. This dataset consists of
500 malicious samples chosen from the Contagio archive [6].
We ﬁrst compare EvadeHC against a baseline solution which
keeps generating random morphed samples and checking
them against the tester and detector until an evading sam-
ple is found. Empirical results show that EvadeHC attains
100% evasion rate on the experimental dataset, and out-
performs the baseline solution by upto 80 times in term of
execution cost. Further, we also experiment on a hypotheti-
cal situation wherein the classiﬁers are hardened by lowering
the classiﬁcation thresholds (i.e., decreasing false acceptance
rate at a cost of increasing false rejection rate), benchmark-
ing EvadeHC against the state-of-the-art method in evading
blackbox classiﬁers [35]. The results strongly demonstrate
the robustness of EvadeHC. For instance, when the thresh-
old of Hidost is reduced from 0 to −0.75 and the number
of detector queries is bounded at 2, 500, EvadeHC attains an
evasion rate of as high as 62%, in comparison with 8% by
the baseline. We also compare EvadeHC with the technique
by Xu et al. [35] that uses real-value classiﬁcation scores
during evasion. Interestingly, even with only access to bi-
nary outputs of the detector, our approach still outperforms
the previous work. While this may appear counter-intuitive,
we contend that this result is in fact expected. We believe
the reasons are two folds. First, EvadeHC is capable of in-
corporating information obtained from both the tester and
detector, as opposed to previous work relying solely on the
classiﬁcation scores output by the detector. Secondly, the
fact that the detector can be evaded implies the classiﬁca-
tion scores are not a reliable representation of the samples’
maliciousness.
Contributions. This paper makes the following contribu-
tions:
1. We give a formulation of classiﬁer evasion in the dark
whereby the adversary only has blackbox accesses to
the detector, a morpher and a tester. We also give a
probabilisitic model HsrMopher to formalise the notion
that no domain-speciﬁc knowledge can be exploited in
the evasion process.
2. We design a scoring function that can assign real-value
score reﬂecting evasion progress to samples, given only
binary outcomes obtained from the detector and tester.
We believe that this scoring mechanism is useful in
extending existing works that necessitate classiﬁcation
scores or other auxiliary information to operate under
a more restricted and realistic setting like ours.
3. Leveraging on the scoring function, we propose an
eﬀective hill-climbing based evasion attack EvadeHC.
This algorithm is generic in the sense that it does not
rely on any domain-speciﬁc knowledge of the underly-
ing morphing and detection mechanisms.
4. We conduct experimental evaluation on two popular
PDF malware classiﬁers. The empirical results demon-
strate not only the eﬃciency but also the robustness
of EvadeHC. More notably, it is also suggested that the
scoring mechanism underlying EvadeHC is more infor-
mative than the one that only relies on classiﬁcation
scores [35].
The rest of the paper is structured as follows. We formu-
late the problem and discuss its related challenges in Sec-
tion 2 before proposing our evading methods in Section 3.
Next, we formalize our proposed approach by presenting
probabilistic models in Section 4 and report the experimen-
tal evaluation in Section 5. We discuss the implications of
our evasion attacks and their mitigation strategies in Sec-
tion 6. We survey related works in Section 7 before conclud-
ing our work in Section 8.
2. PROBLEM FORMULATION
In this section, we deﬁne the problem of classiﬁer eva-
sion in the dark and discuss its related challenges. Prior to
presenting the formulation, we give a running example to
illustrate the problem and its relevant concepts.
2.1 Motivating Scenario
Let us consider an adversary who wants to send a mal-
ware over email channel to a victim. The adversary chooses
to embed the malware in a PDF ﬁle, for the victim is more
willing to open a PDF ﬁle than other ﬁle formats. Most
email service providers would have built-in malware detec-
tion mechanisms scanning users’ attachments. Such mal-
ware detection mechanism is usually a classiﬁer that makes
decision based on some extracted features, with a classiﬁ-
cation model trained using existing data. We assume the
adversary does not have any knowledge about the detector
that the email service provider employs (i.e., the algorithms
and feature space adopted by the classiﬁer). Nevertheless,
the adversary probes the detector by sending emails with
the malicious payload to an account owned by the adversary,
and observing the binary responses (accept/reject) from the
email server. Further, the adversary could adaptively mod-
ify his malicious PDF ﬁle to search for a PDF ﬁle that evades
detection. However, we assume that the adversary could
only probe the email server’s detector a limited number of
times before it is blacklisted.
The adversary expects that its malicious ﬁle will be re-
jected by the detector. To evade detection, it has access to
two tools: a morphing tool that transforms the PDF sam-
ple, and a sandboxing tool that tests whether the sample
maintains its malicious functionalities. For instance, the
morphing tool may insert, delete or replace objects in an
underlying representation of the ﬁle, and the sandboxing
tool dynamically detects whether the malicious PDF sam-
ple causes the vulnerable PDF reader to make certain un-
expected system calls. Due to its insuﬃcient understanding
of the underlying mechanism to manipulate the PDF sam-
ple, the adversary employs the morphing tool as a blackbox.
Furthermore, due to the complexity of the PDF reader, the
adversary does not know whether a morphed PDF sample
will retain its functionality, and the only way to determine
that is to invoke the sandbox test.
Given such limitations on the knowledge of both the de-
tector and morphing mechanism, we want to investigate
whether eﬀective attacks are still possible. To capture such
constrained capability, our formulation centres on the notion
of three blackboxes: a binary-outcome detector D that the
adversary wants to evade, a morpher M that “randomly” yet
consistently morphs the sample, and a tester T that checks
the sample’s functionality.
2.2 Tester T , Detector D and Evasion
The tester T , corresponding to the sandbox in the moti-
vation scenario, declares whether a submitted sample x is
malicious or benign. T is deterministic in the sense that it
will output consistent decisions for the same sample.
The blackbox detector D also takes a sample x as input,
and decides whether to accept or to reject the sample. D cor-
responds to the malware classiﬁer in the motivating scenario.
Samples that are rejected by the detector are those that are
classiﬁed by the detector to be malicious. It is possible that
a sample x declared by T to be malicious is accepted by D.
In such case, we say that the sample x evades detection. Of
course, if the detector is exactly the same as the tester, eva-
sion is not possible. In fact, the main objective of our work is
to study the security of detectors with imperfect accuracy.
Similar to T , we consider detectors that are deterministic
(i.e., their output is always the same for the same input).
We highlight that, in our formulation, the detector’s out-
put is binary (e.g., accept/reject), as opposed to many pre-
vious works (e.g., [31, 35]) which assume real-value outputs.
2.3 Morpher M
The morpher M takes as input a sample x and a random
seed s, and deterministically outputs another sample(cid:101)x. We
call such action a morphing step. The morpher corresponds
to the morphing mechanism described in the motivating sce-
nario. The random seed s supplies the randomness required
by the morpher. We are not concerned with the representa-
tion of the random seed s, and for simplicity, treat it as a
short binary string.
Starting from a sample x0, the adversary can make suc-
cessive calls to M, say with a sequence of random seeds
s = (cid:104)s1, s2, . . . , sL(cid:105), to obtain a sequence of samples x =
(cid:104)x0, x1, x2, . . . , xL(cid:105), where xi = M(xi−1, si). Let us call
(x, s) a path with starting point x0, endpoint xL, and path
length L. When it is clear from the context, we shall omit
s in the notation, and simply refer to the path by x.
The formulation does not dictate how the morphing is to
be carried out. The adversary can exploit useful proper-
ties of the application domain to manipulate the samples.
For instance, the adversary may be able to eﬃciently ﬁnd
a morphing path connecting two given samples, or know
how to merge two morphing paths to attain certain desir-
able property. Nevertheless, such domain speciﬁc properties
are not always available in general. In Section 4, we pro-
pose a probabilistic model of “random” morphing to capture
the restriction that no underlying useful properties can be
exploited in manipulating the samples.
2.4 Adversary’s Goal and Performance Cost
Given a malicious sample x0, the goal of the adversary is
to ﬁnd a sample that evades detection with minimum cost.
We call a sample evading if it is accepted by the detector
D, but exhibits malicious behaviours as perceived by the
tester T .
If the given sample x0 is already evading, then
the adversary has trivially met the goal. Otherwise, the
adversary can call the morpher M to obtain other samples,
and check the samples by issuing queries to T and D.
Let Nd, Nt, and Nm be the number of queries the adver-
sary sent to D, T and M over the course of the evasion,
respectively. We are interested in scenarios where Nd is a
dominating component in determining the evasion cost. In
the motivating scenario, the detector can only be accessed
remotely and the email server (who is the defender) imposes
a bound on the number of such accesses.
While the adversary could freely access the tester,
its
computational cost might be non-trivial. For instance, in
the motivating scenario, computationally intensive dynamic
analysis is required to check the functionality. In our exper-
iments, each such test takes around 45 seconds on average.
Morphing, on the other hands, is less computationally ex-
pensive. Hence, it is reasonable to consider an objective