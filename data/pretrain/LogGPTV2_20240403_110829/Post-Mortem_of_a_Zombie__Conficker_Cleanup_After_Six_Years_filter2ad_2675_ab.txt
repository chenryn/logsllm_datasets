under-counting, decreases this bias.3 We settle for this
simple solution: counting the average number of unique
IPs per hour, thereby eliminating the churn factor. This
hourly count will be a fraction of the total bot count, but
that is not a problem when we make comparisons based
on scale-invariant measures, such as cleanup rates.
Network Address Translation (NAT) and the use of
HTTP proxies can also cause under-counting. This is
particularly problematic if it happens at the ISP level,
leading to large biases when comparing cleanup poli-
cies. After comparing subscriber numbers with IP ad-
dress space size in our selection of countries, we con-
cluded that ISP-level NAT is widely practiced in India.
As we have no clear way of correcting such cases, we
chose to exclude India from our analysis.
3.3 Missing measurements
The Conﬁcker dataset has another problem that is also
common: missing measurements. Looking back at ﬁg-
ure 1, we see several sudden drops in bot counts, which
we highlighted with dotted lines. These drops are pri-
marily caused by sinkhole infrastructure downtime —
typically for a few hours, but at one point even several
weeks. These measurement errors are a serious issue,
as they only occur in one direction and may skew our
analysis. We considered several approaches to dealing
with them. One approach is to model the measurement
process explicitly. Another approach is to try and mini-
mize the impact of aberrant observations by using robust
curve-ﬁtting methods. This approach adds unnecessary
complexity and is not very intuitive. A third option is to
pre-process the data using curve smoothing techniques;
for instance by taking the exponentially weighted rolling
average or applying the Hodrick-Prescott ﬁlter. Although
not necessarily wrong, this also adds its own new biases
as it changes data. The fourth approach, and the one that
we use, is to detect and remove the outliers heuristically.
For this purpose, we calculate the distance between
each weekly value in the global graph with the rolling
median of its surrounding two months, and throw out the
top 10%. This works because most bots log in about
once a day, so the IP counts of adjacent periods are not
independent. The IP count may increase, decrease, or
Figure 4: Conﬁcker bots versus broadband subscribers
slightly ﬂuctuate, but a sudden decrease in infected ma-
chines followed by a sudden return of infections to the
previous level is highly unlikely. The interested reader is
referred to the appendix to see the individual graphs for
all the countries with the outliers removed.4
3.4 Normalizing bot counts by country size
Countries with more Internet users are likely to have
more Conﬁcker bots, regardless of remediation efforts.
Figure 4 illustrates this. It thus makes sense to normalize
the unique IP counts by a measure of country size; in par-
ticular if one is to compare peak infection rates. One such
measure is the size of a country’s IP space, but IP address
usage practices vary considerably between countries. A
more appropriate denominator and the one we use is the
number of Internet broadband subscribers. This is avail-
able from a number of sources, including the Worldbank
Development Indicators.
4 Modeling Infections
4.1 Descriptive Analysis
Figure 5 shows the Conﬁcker infection trends for Ger-
many, United States, France, and Russia. The x-axis is
time; the y-axis is the average number of unique IP ad-
dresses seen per day in the sinkhole logs, corrected for
churn. We observe a similar pattern: a period of rapid
growth; a plateau period, where the number of infected
machines peaks and remains somewhat stable for a short
or longer amount of time; and ﬁnally, a period of gradual
decline.
What explains these similar trends among countries,
and in particular, the points in time where the changes
3Ideally, we would calculate a churn rate — the average number of
IPs per bot per day — and use that to generate a good estimate of the
actual number of bots. That is not an easy task, and requires making
quite a number of assumptions.
4An extreme case was Malaysia, where the length of the drops
and ﬂuctuations spanned several months. This most likely indicates
country-level egress ﬁltering, prompting us to also exclude Malaysia
from the analysis.
6  24th USENIX Security Symposium 
USENIX Association
6
ery are locked in dynamic equilibrium. The size of the
infected population reaches a plateau. In the ﬁnal phase,
the force of recovery takes over, and slowly the number
of infections declines towards zero.
Early on in our modeling efforts we experimented
with a number of epidemic models, but eventually de-
cided against them. Epidemic models involve a set of
latent compartments and a set of differential equations
that govern the transitions between them — see [12] for
an extensive overview. Most models make a number of
assumptions about the underlying structure of the popu-
lation and the propagation mechanism of the disease.
The basic models for instance assume constant tran-
sition rates over time. Such assumptions might hold to
an acceptable degree in short time spans, but not over
six years. The early works applying these models to the
Code Red and Slammer worms [44, 43] used data span-
ning just a few weeks. One can still use the models even
when the assumptions are not met, but the parameters
cannot be then easily interpreted. To illustrate: the basic
Kermack-McKendrick SIR model ﬁts our data to a rea-
sonable degree. However, we know that this model as-
sumes no reinfections, while Conﬁcker reinfections were
a major problem for some companies [24].
More complex models reduce assumptions by adding
additional latent variables. This creates a new problem:
often when solved numerically, different combinations
of the parameters ﬁt the data equally well. We observed
this for some countries with even the basic SIR model.
Such estimates are not a problem when the aim is to pre-
dict an outbreak. But they are showstoppers when the
aim is to compare and interpret the parameters and make
inferences about policies.
4.3 Our model
For the outlined reasons, we opted for a simple descrip-
tive model. The model follows the characteristic trend
of infection rates, provides just enough ﬂexibility to cap-
ture the differences between countries, and makes no as-
sumptions about the underlying behavior of Conﬁcker.
It merely describes the observed trends in a small set of
parameters.
The model consists of two parts: a logistic growth that
ends in a plateau; followed by an exponential decay. Lo-
gistic growth is a basic model of self-limiting population
growth, where ﬁrst the rate of growth is proportional to
the size of the existing population, and then declines as
the natural limit is approached (— the seminal work of
Staniford, et al. [35] also used logistic growth). In our
case, this natural limit is the number of vulnerable hosts.
Exponential decay corresponds to a daily decrease of
the number of Conﬁcker bots by a ﬁxed percentage. Fig-
ure 6 shows the number of infections per subscriber over
Figure 5: Conﬁcker trends for four countries
occur on the graphs? At ﬁrst glance, one might think
that the decline is set off by some event — for instance,
the arrest of the bot-masters, or a release of a patch.
But this is not the case. As previously explained, all
patches for Conﬁcker were released by early 2009, while
the worm continued spreading after that. This is because
most computers that get infected with Conﬁcker are “un-
protected” — that is, they are either unpatched or with-
out security software, in case the worm spreads via weak
passwords on networks shares, USB drives, or domain
controllers. The peak in 2010 – 2011 is thus the worm
reaching some form of saturation where all vulnerable
computers are infected. In the case of business networks,
administrators may have ﬁnally gotten the worm’s re-
infection mechanisms under control [24].
Like the growth phase and the peak, the decline can
also not be directly explained by external attacker be-
havior. Arrests related to Conﬁcker occurred mid 2011,
while the decline started earlier. In addition, most of the
botnet was already out of the control of the attackers.
What we are seeing appears to be a ‘natural’ process of
the botnet.
Infections may have spread faster in some
countries, and cleanups may have been faster in others,
but the overall patterns are similar across all countries.
4.2 Epidemic Models
It is often proposed in the security literature to model
malware infections similarly as epidemics of infectious
diseases, e.g.
[28, 44]. The analog is that vulnerable
hosts get infected, and start infecting other hosts in their
vicinity; at some later point they are recovered or re-
moved (cleaned, patched, upgraded or replaced).
This leads to multiple phases, similar to what we see
for Conﬁcker: in the beginning, each new infection in-
creases the pressure on vulnerable hosts, leading to an
explosive growth. Over time, fewer and fewer vulnera-
ble hosts remain to be infected. This leads to a phase
where the force of new infections and the force of recov-
USENIX Association  
24th USENIX Security Symposium  7
7
Figure 6: Conﬁcker bots per subscriber on logarithm
scale for (from top to bottom) Russia, Belarus, Germany.
Figure 7: Comparison of alternative models
time for three countries on a logarithm scale. We see a
downward-sloping straight line in the last phase that cor-
responds to an exponential decay: the botnet shrank by
a more or less a constant percentage each day. We do
not claim that the assumptions underpinning the logistic
growth and the exponential decay models are fully satis-
ﬁed, but in the absence of knowledge of the exact dynam-
ics, their simplicity seems the most reasonable approach.
The model allows us to reduce the time series data for
each country to these parameters: (1) the infection rate
in the growth phase, (2) the peak number of infections,
(3) the time at which this peak occurred, and (4) the ex-
ponential decay rate in the declining phase. We will ﬁt
our model on the time series for all countries, and then
compare the estimates of these parameters.
Mathematically, our model is formulated as follows:
bots(t) =
K
1 + e−r(t−t0)
He−γ(t−tP),
,
if t < tP
if t ≥ tP
(1)
where bots(t) is the number of bots at time t, tP is the
time of the peak (where the logistic growth transitions to
exponential decay), and H the height of the peak. The lo-
gistic growth phase has growth rate r, asymptote K, and
midpoint t0. The parameter γ is the exponential decay
rate. The height of the peak is identiﬁed by the other
parameters:
H =
K
1 + e−r(tP−t0)
.
Inspection of Model Fit
4.4
We ﬁt the curves using the Levenberg-Marquardt least
squares algorithm with the aid of the lmﬁt Python mod-
ule. The results are point estimates; standard errors were
computed by lmﬁt by approximating the Hessian matrix
at the point estimates. With these standard errors we
computed Wald-type conﬁdence intervals (point estimate
± 2 s.e.) for all parameters. These intervals have no ex-
act interpretation in this case, but provide some idea of
the precision of the point estimates.
The reader can ﬁnd plots of the ﬁtted curves for all 62
countries in the appendix. The ﬁts are good, with R2 val-
ues all between 0.95 and 1. Our model is especially ef-
fective for countries with sharp peaks, that is, the abrupt
transitions from growth to decay that can be seen in Hun-
gary and South Africa, for example. For some countries,
such as Pakistan and Ukraine, we have very little data
on the growth phase, as they reached their peak infection
rate around the time sinkholing started. For these coun-
tries we will ignore the growth estimates in further anal-
ysis. By virtue of our two-phase model, the estimates of
the decay rates are unaffected by this issue.
that is,
We note that our model is deterministic rather than
stochastic;
it does not account for one-time
shocks in cleanup that lead to a lasting drop in infec-
tion rates. Nevertheless, we see that the data follows the
ﬁtted exponential decay curves quite closely, which in-
dicates that bots get cleaned up at a constant rate and
non-simultaneously.5
Alternative models: We tried ﬁtting models from epi-
demiology (e.g. the SIR model) and reliability engineer-
ing (e.g. the Weibull curve), but they did not do well in
such cases, and adjusted R2 values were lower for almost
all countries. Additionally, for a number of countries, the
parameter estimates were unstable. Figure 7 illustrates
why: our model’s distinct phases captures the height of
peak and exponential decay more accurately.
5The exception is China: near the end of 2010 we see a massive
drop in Conﬁcker infections. After some investigation, we found clues
that this drop might be associated by a sudden spur in the adoption of
IPv6 addresses, which are not directly observable to the sinkhole.
8  24th USENIX Security Symposium 
USENIX Association
8
5 Findings
5.1 Country Parameter Estimates
Figure 8 shows the parameter estimates and their preci-
sion for each of the 62 countries: the growth rate, peak
height, time of the peak, and the decay rate.
The variance in the peak number of infections is strik-
ing: between as little as 0.01% to over 1% of Inter-
net broadband subscribers. The median is .1%. It ap-
pears that countries with high peaks tend to also have
high growth rates, though we have to keep in mind that
the growth rate estimates are less precise, because the
data does not fully cover that phase. Looking at the
peak height, it seems that this is not associated with low
cleanup rates. For example, Belarus (BY) has the highest
decay rate, but a peak height well above the median.
The timing of the peaks is distributed around the last
weeks of 2010. Countries with earlier peaks are mostly
countries with higher growth rates. This suggests that the
time of the peak is simply a matter of when Conﬁcker
ran out of vulnerable machines to infect; a faster growth
means this happens sooner. Hence, it seems unlikely that
early peaks indicate successful remediation.
The median decay rate estimate is .009, which corre-
sponds to a 37% decline per year (100· (1− e−.009·52)).
In countries with low decay rates (around .005), the bot-
net shrank by 23% per year, versus over 50% per year on
the high end.
5.2 National Anti-Botnet Initiatives
We are now in a position to address the paper’s central
question and to explore the effects of the leading na-
tional anti-botnet initiatives (ABIs). In ﬁgure 8, we have
highlighted the countries with such initiatives as crosses.
One would expect that these countries have slower bot-
net growth, a lower peak height, and especially a faster
cleanup rate. There is no clear evidence for any of this;
the countries with ABIs are all over the place. We do
see some clustering on the lower end of the peak height
graphs; however, this position is shared with a number of
other countries that are institutionally similar (in terms
of wealth for example) but not running such initiatives.
We can formally test if the population median is equal
for the two groups using the Wilcoxon ranksum test. The
p-value of the test when comparing the Conﬁcker decay
rate among the two sets of countries is 0.54, which is too
large to conclude that the ABIs had a meaningful effect.
It is somewhat surprising, and disappointing, to see no
evidence for the impact of the leading remediation efforts
on bot cleanup.
We brieﬂy look at three possible explanations. The
ﬁrst one is that country trends might be driven by in-
fections in other networks than those of the ISPs, as we
know that the ABIs focus mostly on ISPs. This explana-
tion fails, however, as can be seen in ﬁgure 2. The ma-
jority of the Conﬁcker bots were located in the networks
of the retail ISPs in these countries, compared to educa-
tional, corporate or governmental networks. This pattern
held in 2010, the year of peak infections, and 2013, the
decay phase, with one minor deviation: in the Nether-
lands, cleanup in ISP networks was faster than in other
networks.
Country
AU
DE
FI
IE
JP
KR
NL
Others
ISP % 2010
77%
89%
73%
72%
64%
83%
72%
81%
ISP % 2013
74%
82%
69%
74%
67%
87%
37%
75%
Table 2: Conﬁcker bots located in retail ISPs
A second explanation might be that the ABIs did not
include Conﬁcker in their notiﬁcation and cleanup ef-
forts. In two countries, Germany and the Netherlands,
we were able to contact participants of the ABI. They
claimed that Conﬁcker sinkhole feeds were included and
sent to the ISPs. Perhaps the ISPs did not act on the data
— or at least not at a scale that would impact the decay
rate; they might have judged Conﬁcker infections to be
of low risk, since the botnet had been neutralized. This
explanation might be correct, but it also reinforces our
earlier conclusion that the ABIs did not have a signiﬁ-
cant impact. After all, this explanation implies that the
ABIs have failed to get the ISPs and their customers to
undertake cleanup at a larger scale.
Given that cleanup incurs cost for the ISP, one could
understand that they might decide to ignore sinkholed
and neutralized botnets. On closer inspection, this de-
cision seems misguided, however.
If a machine is in-
fected with Conﬁcker, it means it is in a vulnerable —
and perhaps infected — state for other malware as well.
Since we had access to the global logs of the sinkhole
for GameoverZeus — a more recent and serious threat
— we ran a cross comparison of the two botnet popu-
lations. We found that based on common IP addresses,
a surprising 15% of all GameoverZeus bots are also in-
fected with Conﬁcker. During six weeks at the end of
2014, the GameoverZeus sinkhole saw close to 1.9 mil-
lion unique IP addresses; the Conﬁcker sinkhole saw 12
million unique IP addresses; around 284 thousand ad-
dresses appear in both lists. Given that both malware
types only infected a small percentage of the total pop-
USENIX Association  
24th USENIX Security Symposium  9
9
Figure 8: Parameter estimates and conﬁdence intervals
ulation of broadband subscribers, this overlap is surpris-
ingly large.6 It stands in stark contrast to the ﬁndings of
a recent study that systematically determined the over-
lap among 85 blacklists and found that most entries were