recognition,” 2000.
[30] S. Ahadi and P. C. Woodland, “Combined bayesian and
predictive techniques for rapid speaker adaptation of
continuous density hidden markov models,” Computer
speech & language, 1997.
[31] L. Bahl, P. Brown, P. de Souza, and R. Mercer,
“Maximum mutual information estimation of hidden
markov model parameters for speech recognition,” in
USENIX Association
30th USENIX Security Symposium    2287
ICASSP’86. IEEE International Conference on Acous-
tics, Speech, and Signal Processing, 1986.
[43] L. Yujian and L. Bo, “A normalized levenshtein distance
metric,” IEEE Trans. Pattern Anal. Mach. Intell., 2007.
[32] L. R. Rabiner, “A tutorial on hidden Markov models and
selected applications in speech recognition,” Proceed-
ings of the IEEE, 1989.
[33] A. Graves, S. Fernández, F. Gomez, and J. Schmidhuber,
“Connectionist temporal classiﬁcation: labelling unseg-
mented sequence data with recurrent neural networks,”
in Proceedings of the 23rd international conference on
Machine learning, 2006.
[34] Y. Qin, N. Frosst, S. Sabour, C. Raffel, G. Cottrell, and
G. Hinton, “Detecting and diagnosing adversarial im-
ages with class-conditional capsule reconstructions,” in
International Conference on Learning Representations,
2020.
[35] C. Xie, J. Wang, Z. Zhang, Z. Ren, and A. Yuille, “Mit-
igating adversarial effects through randomization,” in
International Conference on Learning Representations,
2018.
[36] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. Šrndi´c,
P. Laskov, G. Giacinto, and F. Roli, “Evasion attacks
against machine learning at test time,” in Joint Euro-
pean conference on machine learning and knowledge
discovery in databases. Springer, 2013.
[37] N. Carlini, A. Athalye, N. Papernot, W. Brendel,
J. Rauber, D. Tsipras, I. Goodfellow, A. Madry, and
A. Kurakin, “On evaluating adversarial robustness,”
arXiv preprint arXiv:1902.06705, 2019.
[38] N. Carlini and D. Wagner, “Adversarial examples are
not easily detected: Bypassing ten detection methods,”
in Proceedings of the 10th ACM Workshop on Artiﬁcial
Intelligence and Security, 2017.
[39] F. Tramer, N. Carlini, W. Brendel, and A. Madry, “On
adaptive attacks to adversarial example defenses,” 2020.
[40] C. Herley and P. C. Van Oorschot, “Sok: Science, se-
curity and the elusive goal of security as a scientiﬁc
pursuit,” in 2017 IEEE symposium on security and pri-
vacy (SP), 2017.
[41] A. Athalye, L. Engstrom, A. Ilyas, and K. Kwok, “Syn-
thesizing robust adversarial examples,” in Proceedings
of the 35th International Conference on Machine Learn-
ing, 2018.
[42] H. Kwon, H. Yoon, and K.-W. Park, “Poster: Detecting
audio adversarial example through audio modiﬁcation,”
in Proceedings of the 2019 ACM SIGSAC Conference
on Computer and Communications Security, 2019.
[44] J. Lu, T. Issaranon, and D. Forsyth, “Safetynet: De-
tecting and rejecting adversarial examples robustly,” in
The IEEE International Conference on Computer Vision
(ICCV), Oct 2017.
[45] W. Xu, D. Evans, and Y. Qi, “Feature squeezing: De-
tecting adversarial examples in deep neural networks,”
arXiv preprint arXiv:1704.01155, 2017.
[46] J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly,
Z. Yang, Z. Chen, Y. Zhang, Y. Wang, R. Skerrv-Ryan
et al., “Natural TTS synthesis by conditioning WaveNet
on mel spectrogram predictions,” in Proc. ICASSP,
2018.
[47] P. Neekhara, C. Donahue, M. Puckette, S. Dubnov, and
J. McAuley, “Expediting tts synthesis with adversarial
vocoding,” Proc. Interspeech, 2019.
[48] C. Miao, S. Liang, M. Chen, J. Ma, S. Wang, and J. Xiao,
“Flow-tts: A non-autoregressive network for text to
speech based on ﬂow,” in ICASSP 2020-2020 IEEE In-
ternational Conference on Acoustics, Speech and Signal
Processing, 2020.
[49] Bhadragiri Jagan Mohan and Ramesh Babu N., “Speech
recognition using mfcc and dtw,” in 2014 Interna-
tional Conference on Advances in Electrical Engineer-
ing (ICAEE), 2014.
[50] M. Ravanelli, T. Parcollet, and Y. Bengio, “The pytorch-
kaldi speech recognition toolkit,” in ICASSP 2019-2019
IEEE International Conference on Acoustics, Speech
and Signal Processing, 2019.
[51] S. S. Stevens, J. Volkmann, and E. B. Newman, “A scale
for the measurement of the psychological magnitude
pitch,” The Journal of the Acoustical Society of America,
1937.
[52] J. Le Roux, H. Kameoka, N. Ono, and S. Sagayama,
“Fast signal reconstruction from magnitude STFT spec-
trogram based on spectrogram consistency,” in Proc. In-
ternational Conference on Digital Audio Effects, 2010.
[53] D. W. Grifﬁn, Jae, S. Lim, and S. Member, “Signal es-
timation from modiﬁed short-time Fourier transform,”
IEEE Trans. Acoustics, Speech and Sig. Proc, 1984.
[54] Y. He, TensorFlow implementation of Grifﬁn-Lim
algorithm, 2017. [Online]. Available: https://github.
com/candlewill/Grifﬁn_lim
[55] S. W. Smith, The Scientist and Engineer’s Guide to Digi-
tal Signal Processing. California Technical Publishing,
1997.
2288    30th USENIX Security Symposium
USENIX Association
11 Appendix
A. Receiver Operating Characteristic curves
for Detection under Non-Adaptive Attacks
We provide the Receiver Operating Characteristic (ROC)
curves for our detection of non-adaptive adversarial attacks
using various transformation functions against three different
adversarial attacks in Figure 11. The AUC scores are reported
in Table 2 in Section 6.1 and included with each of the plots
below. A true positive implies an example that is adversarial
and is correctly identiﬁed as adversarial.
(a) Downsampling-upsampling
(b) Quantization
(c) Filtering
(d) Linear Predictive Coding (LPC)
(e) Mel Extraction - Inversion
Figure 11: Detection ROC curves for different transformation
functions against three attacks (Carlini [11], Universal [15],
Qin-I [14]) in the non-adaptive attack setting.
B. Thresholds for Detection Accuracy
Table 5 lists the detection thresholds (t) for various transfor-
mation functions for the two ASR systems studied in our work.
We choose 50 original examples (separate from the ﬁrst 100
used for evaluation) and construct 50 adversarial examples us-
ing each of the attack. This results in 100 adversarial-benign
example pairs for DeepSpeech (constructed using Carlini [11]
and Universal [15] attacks) and 100 adversarial-benign ex-
ample pairs for Google Lingvo (constructed using Qin-I and
Qin-R attacks [14]). Using this dataset, we obtain the thresh-
old that achieves the best detection accuracy for each defense
separately for the two ASRs. The AUC metric is threshold in-
dependent. We do not change the threshold for adaptive attack
evaluation and use the same threshold as listed in Table 5.
Defense
Threshold -
DeepSpeech
Downsampling - Upsampling
Quantization - Dequantization
Filtering
Mel Extraction - Inversion
LPC
0.48
0.44
0.32
0.33
0.38
Threshold -
Lingvo
0.48
0.26
0.31
0.31
0.46
Table 5: Detection Threshold when using each transforma-
tion function in WaveGuard framework for DeepSpeech and
Lingvo ASR systems.
C. Transfer Attacks from an Undefended
Model
Distortion metrics
|δ|∞
Defense
1000
LPC
2000
LPC
4000
LPC
LPC
8000
Mel Ext - Inv 1000
Mel Ext - Inv 2000
Mel Ext - Inv 4000
Mel Ext - Inv 8000
dBx(δ)
-23.5
-17.4
-11.4
-5.4
-23.5
-17.4
-11.4
-5.4
Attack Performance
Detection Scores
CER(xadv,τ) CER(g(xadv),τ) AUC
0.99
0.99
0.99
0.99
0.99
0.99
0.99
0.99
0.80
0.83
0.81
0.91
0.81
0.88
0.89
0.92
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
Acc.
98.5%
99.0%
97.0%
99.0%
98.5%
97.5%
98.0%
98.5%
Table 6: Evaluation of Mel Extraction - Inversion and LPC
transform defense against perturbations targeting an unde-
fended DeepSpeech ASR model at different levels of magni-
tude.
We additionally evaluate the robustness of Mel extraction-
inversion and LPC transformations against transfer attacks
from an undefended model. We craft targeted adversarial
examples using [11] for DeepSpeech ASR at different pertur-
bation levels by linearly scaling the perturbation to have the
desired L∞ norm. Table 6 shows the evaluations of transfer
attack at different perturbation levels. We ﬁnd that attacks
targeting undefended models do not break the defense using
USENIX Association
30th USENIX Security Symposium    2289
Carlini [12]Universal [16]Qin-I [15]False Positive RateFalse Positive RateFalse Positive RateTrue Positive RateTrue Positive RateTrue Positive RateCarlini [12]False Positive RateTrue Positive RateUniversal [16]False Positive RateTrue Positive RateQin-I [15]False Positive RateTrue Positive RateUniversal [16]False Positive RateTrue Positive RateQin-I [15]False Positive RateTrue Positive RateCarlini [12]False Positive RateTrue Positive RateUniversal [16]False Positive RateTrue Positive RateCarlini [12]False Positive RateTrue Positive RateQin-I [15]False Positive RateTrue Positive RateQin-I [15]False Positive RateTrue Positive RateUniversal [16]False Positive RateTrue Positive RateCarlini [12]False Positive RateTrue Positive Ratethese two transformation functions even at high perturbation
levels. This is because the transcription of g(xadv) is signiﬁ-
cantly different from the target transcription and transcription
of xadv even at high perturbation levels thereby allowing our
detector to consistently detect the adversarial samples.
D. Straight-through Gradient Estimator for
LPC
We ﬁnd that the LPC transform cannot be broken in an adap-
tive attack scenario using BPDA attack with a straight-through
gradient estimator (i.e assuming identity function as the gra-
dient of transformation function g during the backward pass).
In our experiments, we started with an initial ε∞ of 2000,
and increased the initial distortion bound to 16000 but did
not observe any improvement in the attack performance as
the detector was still able to identify adversarial audio with
100% accuracy. Therefore, using our BPDA attack algorithm,
we do not arrive at a solution in which both x and g(x) tran-
scribe to the target phrase even with a high amount of allowed
distortion. This motivated us to design stronger adaptive at-
tacks with differentiable LPC (Section 7.1) to ﬁnd distortion
bounds over which LPC transforms are not able to reliably
detect adversarial examples.
Distortion metrics
Attack Performance
Detection Scores
ε∞
Defense
LPC
LPC
|δ|∞ dBx(δ) CER(xadv,τ) CER(g(xadv),τ) AUC
1.0
1.0
0.31
0.34
0.85
0.85
2000 2000 -15.9
16000 16000
2.1
Acc.
100%
100%
Table 7: Evaluation of LPC transform against straight-through
gradient estimator.
2290    30th USENIX Security Symposium
USENIX Association