Communicating PUF. (iii) They assume a strict one-time
use of the PUF; potentially malicious parties must be kept
away from the PUF after it has been used. Measures (ii)
and (iii) essentially must be realized by effectively shielding
the PUF continuously until it is destroyed at the end of
its one-time use. These are certainly very costly and non-
trivial measures. They lead us to the question whether other
approaches for ﬁghting the PUF re-use model and bad PUFs
exist in practice.
Erasable and Certiﬁable PUFs: Two other, direct coun-
termeasures against the PUF re-use model and bad PUFs are
so-called Erasable and Certiﬁable PUFs. Erasable PUFs are
Strong PUFs with the additional feature that single responses
can be erased from the PUF (i.e., made impossible to read
out forever) without affecting any of the other responses.
Erasable PUFs have been considered for the ﬁrst time by
R¨uhrmair, Algasinger and Jaeger in [26], who also suggest
an implementation based on so-called crossbar structures.
This implementation is very area consuming, however. Area
efﬁcient implementations have not been suggested up to this
date.
In order to better understand the challenges and the
novelty behind Erasable PUF design, consider two of the
currently most established Strong PUF designs: Arbiter
PUFs [30] and optical PUFs [20]. In both designs, many
subparts of the PUF interact in order to generate a response.
If one response shall be altered or erased, at least one of the
subparts must be changed. In the example of optical PUFs,
certain subparts of the scattering platelet would need to be
modiﬁed; in the case of the Arbiter PUF, at least one internal
delay value would need to be altered. But this will necessar-
ily also affect and modify other responses, contradicting the
requirements of an Erasable PUF. Reconﬁgurable PUFs [16]
are unsuited as Erasable PUFs for the same reason: Their
reconﬁguration operation by deﬁnition alters all responses of
the PUF in one step. This makes any previously collected
CRPs of the PUF invalid.
it
ﬁgurable PUFs (LR-PUFs) as introduced by Katzenbeisser
et al. [13] can be an option in this context. They allow the
manufacturer of the PUF to collect a CRP-list that remains
valid even after many reconﬁguration operations. This may
sufﬁce to ensure the security of certain protocols in the
PUF re-use model. We remark, however, that such versions
of Controlled PUFs introduce additional assumptions, for
example that
is impossible to circumvent, modify or
tamper the control logic around the underlying Strong PUF.
Certiﬁable PUFs, on the other hand, are PUFs that allow
an ofﬂine certiﬁcation of the fact that they have only those
properties that the honest parties expect from them. It is
possible to verify that they have been drawn faithfully from
the expected PUF distribution, and that they have not been
modiﬁed by anyone in any way afterwards. We argued
already in Section II-E why it is important that such a
certiﬁcation can be carried out ofﬂine: Communication with
a trusted authority upon every protocol execution (in order to
certify the PUF) makes the use of PUFs obsolete. One could
then implement the desired functionalities easier by using
the trusted authority itself. Currently, however, no measures
whatsoever have been considered in the literature how such
authentication can be achieved.
The combination of certiﬁability and erasability (or vari-
ants such as logical erasability/reconﬁgurability) in a single
piece of hardware therefore poses a highly relevant, but very
challenging open problem to the PUF hardware community.
It should be resolved in order to restore the full applicability
of Strong PUFs as a general, broadly, and efﬁciently usable
cryptographic tool. It would allow PUF protocols in complex
environments without additional computational assumptions,
and without an economically unrealistic one-time use of
PUFs.
V. SUMMARY AND FUTURE WORK
We introduced a number of new attack models for Strong
PUF protocols in this paper, including the “PUF re-use
model” and the “bad PUF model”. These models, so we
argued, constitute practically relevant and hard-to-detect
attack strategies, and are strongly relevant for practical PUF
usage scenarios.
We then illustrated the power of the new models by
analyzing the security of several known protocols. The
results were already summarized in detail in Section I. In
short, all analyzed oblivious transfer (OT), bit commitment
(BC) and key excahnge (KE) protocols for Strong PUFs can
be attacked successfully in the bad PUF model and/or the
PUF re-use model. This includes schemes by R¨uhrmair [22],
van Dijk [5], Brzuska et al. presented at Crypto 2011 [1],
and Ostrovsky et al. [18]. With one exception, where so-
called Communicating PUFs are required, all attacks in the
bad PUF model only utilize very simple types of bad PUFs,
such as simulatable PUFs and challenge-logging PUFs. The
attacks in the ordinary PUF re-use model are even simpler
If the area efﬁcient, direct implementation of Erasable
PUFs remains difﬁcult in the future, then an alternative
strategy could be equipping Strong PUFs with a surrounding
control logic. This logic is supposed to guard and regulate
the access to the Strong PUF’s challenge-response interface;
such constructions are also known as Controlled PUFs [8].
Along these lines, one could construct “Logically Erasable”
PUFs by letting the control logic maintain some record of the
previously applied and of the erased challenges (e.g., in the
form of an authenticated hash tree). Also Logically Recon-
296
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:54:17 UTC from IEEE Xplore.  Restrictions apply. 
to execute, and do not require physical modiﬁcation of PUFs
at all.
We remark once more that our attacks leave the original
attack models of the protocols (with the exception of our
attack on Ostrovsky et al.’s BC protocol of Section III-E,
which works in their own, original model). Still, our attack
models seem realistic, and indeed closely follow practical
usage scenarios of PUFs. Depending on the exact applica-
tion, the protocols would likely be faced with them once
they were used in practice. This implies that current attack
models and design strategies for advanced PUF protocols
such as OT, BC or KE must strongly be re-thought.
Two potential classes of countermeasures against our
attacks were analyzed in Section IV: The ﬁrst is the employ-
ment of classical computational assumptions in combination
with a strict one-time use of PUFs and shielding of the PUFs
against communication with the malicious party until its
destruction [18]. This step maintains the usability of standard
Strong PUFs in advanced settings and in the presence of
bad PUFs, but is practically very costly and seems difﬁcult
to realize. Furthermore, it takes away some of the appeal
of PUFs as a new, independent, and post-quantum crypto-
graphic primitive that enables advanced protocols without
classical computational assumptions.
A second possibility, that would restore the usability of
PUFs in complex application settings without any restric-
tions, is the use of Certiﬁable and Erasable PUFs. These are
PUFs which can be certiﬁed ofﬂine for their genuineness,
and for the fact that they have no other features than those
expected by the honest parties (“certiﬁability”); and PUFs
that allow the selective erasure of single PUF responses with-
out affecting other responses (“erasability”). Without pre-
senting a formal proof of this claim in this paper, they seem
to allow efﬁcient and secure PUF protocols whose security
is built on the unpredictability of the PUF alone, without
requiring additional computational assumptions. These novel
PUF types could maintain the status of Strong PUFs as
a general, new cryptographic primitive. If Erasable PUFs
maintain hard to realize in practice, then also variants such as
logical erasability/reconﬁgurability [13] could be interesting
in our context. In order to ﬁght both the bad PUF and the
PUF re-use model, however, erasability (or variants of it)
and certiﬁability have to be combined in a single piece of
hardware. No strategies for this exist in the current literature.
Relation to PUF-Based Key Storage and Strong PUF-
based Identiﬁcation: Apart from their use in basic crypto-
graphic protocols, a second established application of PUFs
is their usage as (tamper-sensitive) key storage element.
This application has at times been termed a “physically
obfuscated key” or POK [7], sometimes also a “Weak PUF”
[11]. We stress that this application scenario is not the topic
of our paper. Independent of whether such an assumption is
considered realistic or not, POKs explicitly suppose that the
PUF’s responses remain internal forever, and can only be
accessed by the system itself to derive an internal secret
key. This makes this PUF-type unusable for the type of
protocols considered in this paper; and at the same time,
it makes the attacks in the PUF re-use model meaningless.
Also the bad PUF model seems obsolete: PUF-based key
storage assumes that the manufacturer in a secure set-up
phase can read out the key derived from the PUF, and uses
it later on in communication with the PUF. This presupposes
some basic trust in the manufacturer in the ﬁrst place, since
the secret key is shared with him from the beginning. The
exact relation between POKs and the bad PUF model will
be the topic of future analysis.
Something similar holds for the common Strong PUF
based identiﬁcation protocol by Pappu et al. [20], [19]. The
use of bad PUFs here appears less relevant, and the PUF
re-use model does not seem to pose a signiﬁcant threat.
On the other hand, a manufacturer who uses a simulatable
PUF can later impersonate the PUF-carrying hardware.
The exact implications of our attack models on PUF-based
identiﬁcation were not the topic of this paper, and are left
for future investigations.
Future Work: We expect two strands of substantially
new research to emerge from our ﬁndings. The ﬁrst will
be concerned with the theory behind Strong PUF proto-
cols: New attack models and security deﬁnitions must be
developed, for example in the context of the UC-framework.
They must include the formal deﬁnition of Erasable PUFs
(and variants such as Logically Erasable/Reconﬁgurable
PUFs) and Certiﬁable PUFs, and the investigation of “PUF
attestation” as standard protocol step. New security proofs
will need to be led in these environments. Finally, the exact
implications of our attack models for other PUF applications
than OT, BC and KE must be determined.
The second strand of research regards PUF hardware, and
concerns the development of efﬁcient Erasable and Certiﬁ-
able PUFs. As brieﬂy addressed in Section IV, combining
these two features in a single piece of hardware efﬁciently is
highly non-trivial. The same holds for combinations of log-
ical erasability/reconﬁgurability and certiﬁability. We would
like to pose these problems as central future challenges to
the PUF hardware community in this work.
ACKNOWLEDGEMENTS
The authors would like to thank J¨urg Wullschleger for
enjoyable discussions and his contributions on the bad PUF
model and challenge-logging PUFs.
REFERENCES
[1] C. Brzuska, M. Fischlin, H. Schr¨oder, S. Katzenbeisser:
Physical Unclonable Functions in the Universal Composition
Framework. CRYPTO 2011.
[2] C. Brzuska, M. Fischlin, H. Schr¨oder, S. Katzenbeisser:
Physical Unclonable Functions in the Universal Composition
Framework. Full version of the paper. Cryptology ePrint
Archive, Report 2011/681, 2011. Downloaded March 2012.
297
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:54:17 UTC from IEEE Xplore.  Restrictions apply. 
[3] R. Canetti: Universally Composable Security: A New
Paradigm for Cryptographic Protocols. FOCS 2001: 136-145.
[4] Q. Chen, G. Csaba, P. Lugli, U. Schlichtmann, U. R¨uhrmair:
The Bistable Ring PUF: A new architecture for strong Phys-
ical Unclonable Functions. HOST 2011: 134-141
[5] M. van Dijk: System and method of reliable forward secret
key sharing with physical random functions. US Patent No.
7,653,197, October 2004.
[6] M. van Dijk, U. R¨uhrmair: Physical Unclonable Functions in
Cryptographic Protocols: Security Proofs and Impossibility
Results. Cryptology ePrint Archive, Report 2012/228, 2012.
Downloaded April 2012.
[7] B. Gassend, Physical Random Functions, MSc Thesis, MIT,
2003.
[8] B. Gassend, M. van Dijk, D.E. Clarke, E. Torlak, S. Devadas,
P. Tuyls: Controlled physical random functions and applica-
tions. ACM TISSEC 10(4), 2008.
[9] B. Gassend, D. E. Clarke, M. van Dijk, S. Devadas: Silicon
physical random functions. ACM Conference on Computer
and Communications Security 2002: 148-160
[10] B. Gassend, D. Lim, D. Clarke, M. van Dijk, S. Devadas:
Identiﬁcation and authentication of integrated circuits. Con-
currency and Computation: Practice & Experience, pp. 1077
- 1098, Volume 16, Issue 11, 2004.
[11] J. Guajardo, S. S. Kumar, G. J. Schrijen, P. Tuyls: FPGA
Intrinsic PUFs and Their Use for IP Protection. CHES 2007:
63-80
[12] D. E. Holcomb, W. P. Burleson, K. Fu: Initial SRAM state as
a ﬁngerprint and source of true random numbers for RFID
tags. In: In Proceedings of the Conference on RFID Security,
2007.
[13] S. Katzenbeisser,
¨U. Koc¸abas, V. van der Leest, A.-R.
Sadeghi, G. J. Schrijen, C. Wachsmann: Recyclable PUFs:
Logically Reconﬁgurable PUFs. Journal of Cryptographic
Engineering 1(3): 177-186 (2011)
[14] J. Kilian: Founding cryptography on oblivious transfer.
STOC, 1988
[15] S. S. Kumar, J. Guajardo, R. Maes, G. J. Schrijen, P. Tuyls:
The Butterﬂy PUF: Protecting IP on every FPGA. HOST
2008: 67-70
[16] K. Kursawe, A. R. Sadeghi, D. Schellekens, P. Tuyls, B. Sko-
ric: Reconﬁgurable physical unclonable functions – Enabling
technology for tamper-resistant storage. HOST 2009: 22-29.
[18] R. Ostrovsky, A. Scafuro, I. Visconti, A. Wadia: Universally
Composable Secure Computation with (Malicious) Physically
Uncloneable Functions. Cryptology ePrint Archive, Report
2012/143, 2012. First version downloaded in April 2012.
Throughout our paper, we refer to the numbering of ﬁgures
and protocols of the latest version of Ostrovsky et al. that was
available at the time of preparing our camera ready paper.
This latest version stems from Nov. 14, 2012.
[19] R. Pappu: Physical One-Way Functions. PhD Thesis, Mas-
sachusetts Institute of Technology, 2001.
[20] R. Pappu, B. Recht, J. Taylor, N. Gershenfeld: Physical
One-Way Functions, Science, vol. 297, pp. 2026-2030, 20
September 2002.
[21] R. Rivest: Illegitimi non carborundum. Invited keynote talk,
CRYPTO 2011.
[22] U. R¨uhrmair: Oblivious Transfer based on Physical Unclon-
able Functions. TRUST 2010, pp. 430 - 440, Springer 2010.
[23] U. R¨uhrmair, H. Busch, S. Katzenbeisser: Strong PUFs:
Models, Constructions and Security Proofs. In A.-R. Sadeghi,
P. Tuyls (Editors): Towards Hardware Intrinsic Security: