share the available PRBs equally in subframe eight. Second, idle
PRBs also appear when the data rate of a user’s flow decreases, e.g.,
Subframe 9 in Figure 5, which could be caused by, e.g., congestion
in the Internet, the application itself, or a shift of traffic from one
cell to another aggregated cell by the cellular network. In this case,
all other users immediately detect and occupy their fair share of
the newly-idle PRBs. Other users share 1/N of the idle PRBs with
User 3, whose data rate is limited and thus is not able to grab more
PRBs. As a result, if we define the number of idle PRBs in Subframe 9
as P′, there will be P′/N left idle in Subframe 10. Similarly, other
users detect these idle PRBs in Subframe 11, but still only occupy
their fair share portion, so P′/N
2 will be left idle in Subframe 12.
The network converges to a state where all other users other than
the User 2 grab all the idle bandwidth.
(b) Block error rate.
(a) Percentage of overhead.
Figure 6: The percentage of capacity used for transport block
retransmission and transmission of protocol overhead is
given in (a). The relationship between transport block error
rate and transport block size is given in (b).
Cross-layer bit rate translation The capacities Cf and Cp (Eqns. 2
and 3) are wireless physical-layer capacities differing from trans-
port-layer data rates due to MAC-layer retransmissions and (con-
stant) protocol header overhead. PBE-CC therefore needs to trans-
form the estimated physical-layer capacity Cp to a transport layer
goodput Ct , and feedback Ct back to the server to set its send rate.
5
Index of subframes (1ms)Bandwidth1234567810111213TimeUser 1User 2IdleUser 39510152025303540Offered load (Mbit/s)0481216Percentage (%)Retransmission (-98 dBm)Protocol overhead (-98 dBm)Retransmission (-113 dBm)Protocol overhead (-113 dBm)10203040506070Transport block size (Kbit)00.10.20.3Transport block error rate-98 dBm-113 dBmBER=5x10-6BER=3x10-6BER=2x10-6BER=1x10-6SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Yaxiong Xie, Fan Yi, Kyle Jamieson
The cell indicates a retransmitted transport block using a new-data-
indicator, so we can separately measure retransmission overhead
and protocol overhead. Figure 6(a) plots the measured overhead
at two different locations and varying sender offered loads. The
probability of a TB error determines retransmission overhead: if
the bit error rate (BER) of each bit inside one TB is p and bit errors
are i.i.d., the TB error rate is 1 − (1 − p)L, where L is the TB size.
We plot in Figure 6(b) theoretical TB error rate (for p = 5 × 10−6,
3 × 10−6, and 1 × 10−6) and empirical TB error rate, noting a good
fit between experimental data and theory. Based on these results,
PBE-CC models the relationship between Cp and Ct as
Cp = Ct + Ct ·(cid:16)1 − (1 − p)L(cid:17)
+ γ · Cp
(5)
where γ = 6.8% is the protocol overhead. When one user takes
its PBE-CC-allocated fair-share capacity (Eqn. 3), the TB size L
(number of bits in one subframe, i.e., 10−3 s), is L = Ct · 10−3. We
estimate p using measured signal to interference noise ratio (SINR),
then by solving Eq. 5 given a measured physical layer capacity Cp,
we estimate transport layer goodput Ct . To speed up the calculation,
PBE-CC uses a look-up table to store the transformation.
Handling control traffic. PBE-CC aims to fairly share wireless
bandwidth between all active users, but our experimental results
shows that significant amount of detected users are active not for
data, but rather to update network parameters shared by both base
station and mobile, e.g., the periods of various timers, list of ag-
gregated cells, and many pricing and security-related parameters.
Because of such users, the number of detected active users at each
time point could be large. For example, we plot the distribution of
the number of detected active users in a 40 ms interval, across a
5 hour interval, measured from a busy cell tower, in Figure 7(b).
On average, we observe on average 15.8 and maximum 28 active
users, in those 40 ms interval. PBE-CC excludes those users in its
fair-share capacity calculation, reverting to the cell tower to allo-
cate small amounts of bandwidth for these users and then reacting
to that allocation by tracking the decrease of allocated bandwidth
(Pa in Eqn. 3) and lowering send rate by that amount. Our key
observation is that the control traffic occupies a small number of
PRBs and only active for small amount of time. We plot the distri-
bution of the average occupied PRBs and active time (subframes)
of all detected active users in Figure 7(b). We see that 68.2% of
users occupies exactly four PRBs and is active for exactly one sub-
frame, among which 95% of users are receiving control traffic from
the base station. Therefore, the PBE-CC monitor filters users that
are only active for parameter updating, based on thresholding the
active time duration (subframes) and allocated bandwidth (PRBs)
(Ta > 1, Pa > 4), after which the number of detected active users
decreases significantly—the average number of detected user inside
a 40 ms interval decreases from 15 to 1.3, and we only observe at
most seven active users competing for the bandwidth simultane-
ously, as shown in Figure 7(a). We set the N in Eqns. 2 and 3 to the
number of active users we detect after applying the threshold. The
calculation of idle PRBs in Eqn. 4, however, takes every identified
user into account.
Switching between Bottleneck States. When sender offered
4.2.2
load exceeds the capacity of the Internet bottleneck, packet queuing
induces PBE-CC to switch from the wireless bottleneck state to
6
(a) Number of active users.
Figure 7: Number of mobile users exchanging data with the
base station (a), and activity lengthTa and average consumed
PRBs Pave of each detected mobile user (b).
(b) Measured Ta and Pave.
the Internet bottleneck state. PBE-CC triggers a switch when the
instantaneous one-way packet delay exceeds a threshold. Theo-
retically, we should set the threshold to the one way propagation
delay between the server and clients (Dth = Dprop). PBE-CC esti-
mates Dprop as the minimum delay observed in a 10-second window,
evoking BBR’s round-trip propagation delay estimation method.
PBE-CC also updates the true Dprop by draining the buffer as BBR
does, if estimated packet delay maintains constant for 10 seconds.
(a) 6 Mbit/s.
(b) 24 Mbit/s.
(c) 36 Mbit/s.
Figure 8: Higher send rates (sub-caption label) result in a
higher probability of transport block errors, so more pack-
ets encounter eight millisecond retransmission delays.
The theoretical threshold, however, works poorly in practice
because of the reordering operation. We observe that the mobile
user frequently buffers received packets in its reorder buffer (§3),
especially when offered load from the sender is high, causing sig-
nificant fluctuations of packet delay. To demonstrate such an effect,
we plot the measured one way delay at a mobile user under dif-
ferent sender offered loads in Figure 8. We see that when offered
load is low (6 Mbit/s), only a small portion of the received pack-
ets are retransmitted, as shown in Figure 8(a). We also observe
an approximate three millisecond network jitter introduced to the
packet delay. When the offered load increases, the transport block
error rate increases accordingly, as we have discussed in §4.2.1.
Consequently, the mobile user buffers more and more packets in its
reorder buffer, introducing an multiple of eight ms retransmission
delay to a increasing number of received packets, as shown in Fig-
ure 8(b) and 8(c). We note that, the minimum delay still captures the
one way propagation delay, as there always are packets received
correctly without retransmission and directly without buffering at
the reorder buffer, e.g., the packets inside transport block of the
first subframe in Figure 3.
0481216202428Number of mobile users00.20.40.60.81CDFAll usersTa >1, Pave>4010203040Active length (ms)00.20.40.60.81CDFActive length0255075100Average used PRB 00.20.40.60.81CDFOccupied PRB68.2% users are active foronly 1 subframe47.7% users occupy 4 PRBs00.511.52Time (s)30405060Delay (ms)3 ms8 msMinimum delay00.511.52Time (s)3040506000.511.52Time (s)30405060Congestion Control via Endpoint-Centric, Physical-Layer Bandwidth Measurements
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Figure 9: BBR adopts a eight-phase cycle to probe the net-
work bandwidth. The length of each phase is set to RTprop.
old to Dth =(cid:0)Dprop + 3 · 8 + 3(cid:1) ms, where (3 · 8) ms accounts for
According to the above analysis, we set the switching thresh-
the delay introduced by the three consecutive retransmissions (a
transport block can be retransmitted at most three times [4]) and
3 ms accounts for the network jitter (according to our experimental
results, 94.1% of the time, jitter is ≤ 3 ms). To further mitigate the
impact of greater network jitter and improve robustness, PBE-CC
adds a threshold for the number of consecutive packets with delay
exceeding the delay threshold, set to the number of packets Npkt
that can be transmitted over six subframes using current data rate:
(6)
where Ct is the current transport layer capacity with unit bits per
subframe, and MSS is the maximum segment size. We note that
since our algorithm makes decisions based on relative delay, i.e., the
difference between current propagation delay and the threshold,
instead of the absolute value of the delay, PBE-CC does not require
synchronization between the server and mobile clients.
Npkt = 6 · Ct/MSS
Internet Bottleneck State. PBE-CC switches to a cellular-tai-
4.2.3
lored BBR to probe a rate that matches the capacity of the bottleneck
link inside the Internet. BBR senders estimate the bottleneck band-
width of the connection (BtlBw) as the maximum delivery rate in
recent 10 RTTs, and set their offered rate to pacinд_дain · BtlBw.
BBR’s pacing_gain is set to 1.25 to probe possible idle bandwidth,
to 0.75 when draining packets buffered in the previous probing
period, and to one the rest of time. BBR’s ProbeBW state repeats
an eight-phase cycle to probe bandwidth. The length of each phase
is set to RTprop, and the pacing gain in each phase is shown in Fig-
ure 9. PBE-CC directly enters BBR’s ProbeBW state, then follows
the same control logic as BBR to alternate between BBR’s ProbeBW,
ProbeRTT, StartUp, and Drain states.
Wireless-aware, BBR-like probing. PBE-CC probes for a higher
data rate that the Internet bottleneck supports, but also takes into
account the fair-share send rate of the cellular wireless link. We
adapt BBR’s bandwidth probing scheme, changing the probing rate
Cprobe from a fixed 1.25BtlBw to
Cprobe = min(cid:8)1.25BtlBw, Cf
(cid:9),
(7)
where Cf is the maximum fair-share capacity of the wireless link
(estimated according to Eqn. 2 and translated to transport layer
capacity according to Eqn. 5 below). The mobile user explicitly
sends Cf back to the sender when an Internet bottleneck is detected.
Similar to BBR, PBE-CC enters a draining phase after the probing
phase to drain any buffered packets.
When PBE-CC detects that the network is in the Internet-bot-
tleneck state, there is already a packet queue formed inside the
network. Therefore, before switching to handle that state, PBE-
CC enters an additional draining phase that lasts for one RTprop.
During the draining phase, PBE-CC sets its send rate to 0.5BtlBw,
leaving the remaining capacity of 0.5BtlBw for the bottleneck link
to drain the packets buffered inside its queue.
Switching back to wireless bottleneck state. If PBE-CC’s send
rate reaches Cf without causing any packet queuing in the network,
i.e., the mobile user observes Npkt (calculated according to Eqn 6)
consecutive packets with delay smaller than Dth ms are observed
at the mobile user, then PBE-CC exits the Internet-bottleneck state
and re-enters the wireless bottleneck state, staying in that state
until the network is switched back to Internet-bottleneck state.
4.3 Fairness and TCP-friendliness
As it only modifies BBR’s algorithms to be more conservative, PBE-
CC is strictly less aggressive than BBR when competing with flows
sharing the same Internet bottleneck. BBR’s multi-user fairness,
RTT-fairness and TCP-friendliness have been well established in
the literature[20, 33, 37, 40].
In the wireless bottleneck state, multiple competing PBE-CC
mobile clients quickly converge to a equilibrium with fair-share
cellular wireless capacity (as we demonstrate below in §6.4.1), be-
cause each PBE-CC mobile client knows the number of competing
users and their capacity usage in each aggregated cell by decod-
ing the cellular physical control channel, allowing it to explicitly
calculate its fair-share capacity (§4.1) and then guide its sender to
match its sending rate accordingly. In contrast, conventional end-
to-end congestion control algorithms need to probe the fair-share
of bottleneck capacity with a more complicated series of probing
and backoff steps, which is less efficient. PBE-CC also fairly shares
wireless link capacity with existing congestion control algorithms,
e.g., CUBIC and BBR, with the help of cell tower’s fairness policy,
as our experimental evaluation later demonstrates (§6.4.3).
PBE-CC flows with different propagation delays fairly share wire-
less capacity (as we demonstrate in §6.4.2), because of two reasons,
one from the design of PBE-CC and one from the buffer structure
of base station. First, PBE-CC explicitly calculates the fair-share
capacity, while most conventional congestion control algorithm
adopt additive-increase multiplicative-decrease (AMID) schemes to
probe for the fair share. During the additive increase, the sender of
a flow with smaller propagation delay increases its window faster
than flows with larger delay, resulting in unfairness [19, 34]. Sec-
ond, the base station provides separate buffers for every user, which
prevents large-RTprop connections from dominating the bottleneck
buffer. For example, a BBR connection with a large RTprop calcu-
lates a large BDP and thus injects significant amount of inflight
packets into the network, which queue at the bottleneck buffer and
lower the delivery rate for another BBR flow with a small RTprop
and hence a small number of inflight packets. The separate buffer
at cellular base station isolates the inflight packets from different
flows sharing the wireless link and thus prevents unfairness.
5 IMPLEMENTATION
Programming a mobile phone to decode every control message
transmitted over the control channel requires customization of the
cellular firmware inside the phone. The source code of current
cellular firmware, however, is proprietary to cellular equipment
manufacturers, thus is not accessible. As a proof of concept, we build
an open-source congestion control prototyping platform that sup-
ports control message decoding, bypassing the need to customize
7
1111.250.75111TimeRTpropProbingbandwidthDrainingqueueSIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Yaxiong Xie, Fan Yi, Kyle Jamieson
6 EVALUATION
In this section, we evaluate the performance of PBE-CC in a com-
mercial cellular network and compare with existing end-to-end
congestion control algorithms.
6.1 Methodology
Content senders. We configure Amazon AWS servers as the PBE-
CC senders. To evaluate PBE-CC’s performance over flows with
significantly different RTT, we setup AWS servers at different con-
tinents, i.e., three in US and one in Singapore.
Mobile clients. Each PBE-CC mobile client is a combination of
multiple USRPs for signal collection, a host PC for control channel
decoding, and a commercial mobile phone for cellular communica-
tion, as shown in Figure 10(b). We use both USRP X310 [14] and
B210 [13] in our implementation. The host PC we use for each
mobile client is a Dell OptiPlex 7060 (Intel Core i7-8700 CPU, 16 GB
RAM, and Ubuntu 16.04). We use various types of mobile phones
that support carrier aggregation in hardware, including a Xiaomi
MIX3, a Redmi 8, and a Samsung S8. The cellular network con-
figures the same primary cell for all three phones, but different
numbers of aggregated cells for each phone, i.e., only one cell for
the Redmi 8, two cells for the MIX3 and three cells for the S8.
Congestion control algorithms to compare. We compare PBE-
CC against seven end-to-end congestion control algorithms, in-
cluding algorithms specially designed for cellular networks like
Sprout [43] and Verus [49], algorithms that have already been in-
cluded inside the official Linux kernel like BBR [10] and CUBIC [19],
and recently-proposed algorithms like Copa [6], PCC [11] and PCC-
Vivace [12]. We test all the above algorithms in commercial cellular
networks covering our campus using Pantheon [48].
(a) Detected users.
(b) Physical data rate.
Figure 11: (a) The number of detected users in each hour of
a day that have data communication with two base stations