27
7
14
2
18
41
We ran FUZZER and SES against each program in LAVA-
M, with 5 hours of runtime for each program. md5sum ran
with the -c argument, to check digests in a ﬁle. base64 ran
with the -d argument, to decode base 64.
SES found no bugs in uniq or md5sum. In uniq, we
believe this is because the control ﬂow is too unconstrained. In
md5sum, SES failed to execute any code past the ﬁrst instance
of the hash function. base64 and who both turn out more
successful for SES. The tool ﬁnds 9 bugs in base64 out
of 44 inserted; these include both deep and shallow bugs, as
base64 is such a simple program to analyze.
SES’s results are a little more complicated for who. All of
the bugs it ﬁnds for who use one of two DUAs, and all of them
occur very early in the trace. One artifact of our method for
injecting multiple bugs simultaneously is that multiple bugs
119119
share the same attack point. It is debatable how well this
represents real bugs. In practice, it means that SES can only
ﬁnd one bug per attack point, as ﬁnding an additional bug at
the same attack point does not necessarily require covering
new code. LAVA could certainly be changed to have each bug
involve new code coverage. SES could also be improved to
ﬁnd all the bugs at each attack point, which means generating
multiple satisfying inputs for the same set of conditions.
FUZZER found bugs in all utilities except who.2 Unlike
SES, the bugs were fairly uniformly distributed throughout the
program, as they depend only on guessing the correct 4-byte
trigger at the right position in the input ﬁle.
FUZZER’s failure to ﬁnd bugs in who is surprising. We
speculate that the size of the seed ﬁle (the ﬁrst 768 bytes of
a utmp ﬁle) used for the fuzzer may have been too large
to effectively explore through random mutation, but more
investigation is necessary to pin down the true cause. Indeed,
tool anomalies of this sort are exactly the sort of thing one
would hope to ﬁnd with LAVA, as they represent areas where
tools might make easy gains.
We note that the bugs found by FUZZER and SES have very
little overlap (only 2 bugs were found by both tools). This is a
very promising result for LAVA, as it indicates that the kinds
of bugs created by LAVA are not tailored to a particular bug
ﬁnding strategy.
VII. RELATED WORK
The design of LAVA is driven by the need for bug corpora
that are a) dynamic (can produce new bugs on demand), b)
realistic (the bugs occur in real programs and are triggered by
the program’s normal input), and c) large (consist of hundreds
of thousands of bugs). In this section we survey existing bug
corpora and compare them to the bugs produced by LAVA.
The need for realistic corpora is well-recognized. Re-
searchers have proposed creating bug corpora from student
code [18], drawing from existing bug report databases [12],
[13], and creating a public bug registry [7]. Despite these pro-
posals, public bug corpora have remained static and relatively
small.
The earliest work on tool evaluation via bug corpora appears
to be by Wilander and Kamkar, who created a synthetic testbed
of 44 C function calls [22] and 20 different buffer overﬂow
attacks [23] to test the efﬁcacy of static and dynamic bug
detection tools, respectively. These are synthetic test cases,
however, and may not reﬂect real-world bugs. In 2004, Zitser
et al. [27] evaluated static buffer overﬂow detectors; their
ground truth corpus was painstakingly assembled by hand
over the course of six months and consisted of 14 annotated
buffer overﬂows with triggering and non-triggering inputs as
well as buggy and patched versions of programs; these same
14 overﬂows were later used to evaluate dynamic overﬂow
detectors [25]. Although these are real bugs from actual
software, the corpus is small both in terms of the number of
2In fact, we allowed FUZZER to continue running after 5 hours had passed;
it managed to ﬁnd a bug in who in the sixth hour.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:15:56 UTC from IEEE Xplore.  Restrictions apply. 
bugs (14) but also in terms of program size. Even modestly-
sized programs like sendmail were too big for some of the
static analyzers and so much smaller models capturing the
essence of each bug were constructed in a few hundred lines
of excerpted code.
The most extensive effort to assemble a public bug corpus
comes from the NIST Software Assurance Metrics And Tool
Evaluation (SAMATE) project [10]. Their evaluation corpus
inclues Juliet [2], a collection of 86,864 synthetic C and Java
programs that exhibit 118 different CWEs; each program, how-
ever, is relatively short and has uncomplicated control and data
ﬂow. The corpus also includes the IARPA STONESOUP data
set [19], which was developed in support of the STONESOUP
vulnerability mitigation project. The test cases in this corpus
consist of 164 small snippets of C and Java code, which are
then spliced into program to inject a bug. The bugs injected in
this way, however, do not use the original input to the program
(they come instead from extra ﬁles and environment variables
added to the program), and the data ﬂow between the input
and the bug is quite short.
Most recently, Shiraishi et al. [17] conducted a quantitative
analysis of commercial static analysis tools by constructing
400 pairs of C functions, where each pair consisted of two
versions of the same function, one with a bug and one
without. The bugs covered a range of different error types,
including static/dynamic buffer overruns, integer errors, and
concurrency bugs. They then ranked the tools according to
effectiveness and, by incorporating price data for commercial
tools, produced a measure of efﬁciency for each tool. As with
previous synthetic corpora, however, the functions themselves
are relatively short, and may be easier to detect than bugs
embedded deep in large code bases.
Finally, the general approach of automatic program transfor-
mation to introduce errors was also used by Rinard et al. [15];
the authors systematically modiﬁed the termination conditions
of loops to introduce off-by-one errors in the Pine email client
to test whether software is still usable in the presence of errors
once sanity checks and assertions are removed.
VIII. LIMITATIONS AND FUTURE WORK
A signiﬁcant chunk of future work for LAVA involves
making the generated corpora look more like the bugs that
are found in real programs. LAVA currently injects only
buffer overﬂows into programs. But our taint-based analysis
overcomes the crucial ﬁrst hurdle to injecting any kind of
bug: making sure that attacker-controlled data can be used in
the bug’s potential exploitation. As a result, other classes of
bugs, such as temporal safety bugs (use-after-free) and meta-
character bugs (e.g. format string) should also be injectable
using our approach. There also remains work to be done
in making LAVA’s bug-triggering data ﬂow more realistic,
although even in its current state, the vast majority of the
execution of the modiﬁed program is realistic. This execution
includes the data ﬂow that leads up to the capture of the DUA,
which is often nontrivial.
However rosy the future seems for LAVA, it is likely that
certain classes of bugs are simply not injectable via taint-
based measures. Logic errors, crypto ﬂaws, and side-channel
vulnerabilities, for instance, all seem to operate at a rather
different
than the kinds of data-ﬂow triggered ﬂaws
LAVA is well positioned to generate. We are not hopeful
that these types of vulnerabilities will soon be injectable with
LAVA.
level
We discovered, in the course of our use of LAVA bugs to
evaluate vulnerability discovery tools, a number of situations
in which LAVA introduces unintended bugs, such as use-after
free and dereference of an uninitialized pointer in the code
that siphons off a DUA value for later use triggering a bug.
In some cases, the tool under evaluation even found these real
bugs that were due to LAVA artifacts and we had to remove
them and re-run in order to ensure that the evaluation was not
compromised. These artifacts are a result of LAVA performing
no real static analysis to determine if it is even vaguely safe
to dereference a pointer in order to introduce the data ﬂow it
requires to inject a bug. It should be possible to remedy this
situation dramatically in many cases but a complete solution
would likely require intractable whole-program static analysis.
LAVA is limited to only work on C source code, but there
is no fundamental reason for this. In principle, our approach
would work for any source language with a usable source-
to-source rewriting framework. In Python, for example, one
could easily implement our taint queries in a modiﬁed CPython
interpreter that executed the hypervisor call against the address
of a variable in memory. Since our approach records the
correspondence between source lines and program basic block
execution, it would be just as easy to ﬁgure out where to edit
the Python code as it is in C. We have no immediate plans to
extend LAVA in these directions.
We are planning some additional evaluation work. In par-
ticular, an extensive evaluation of real, named tools should be
undertaken. The results will shed light on the strengths and
weaknesses of classes of techniques, as well as particular im-
plementations. It should also be noted that in our preliminary
evaluation of vulnerability discovery tools we measured only
the miss rate; no attempt was made to gauge the false alarm
rate. For tools that generate a triggering input, as do both SES
and FUZZER, measuring false alarm rate should be trivial.
Every input can be tested against the program after it has been
instrumented to be able to detect the vulnerability. In the case
of buffer overﬂows in C, this could mean compiling in ﬁne-
grained bounds checking [16]. However, many bug ﬁnding
tools, especially static analyzers and abstract interpretation
ones, do not generate bug-triggering inputs. Instead,
they
merely gesture at a line in the program and make a claim
about possible bugs at that point. In this situation, we can
think of no way to assess false alarm rate without extensive
manual effort.
IX. CONCLUSION
In this paper, we have introduced LAVA, a fully automated
system that can rapidly inject large numbers of realistic bugs
120120
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:15:56 UTC from IEEE Xplore.  Restrictions apply. 
into C programs. LAVA has already been used to introduce
over 4000 realistic buffer overﬂows into open-source Linux
C programs of up to 2 million lines of code. We have used
LAVA corpora to evaluate the detection powers of state-of-the-
art bug ﬁnding tools. The taint-based measures employed by
LAVA to identify attacker-controlled data for use in creating
new vulnerabilities are powerful and should be usable to
inject many and diverse vulnerabilities, but there are likely
fundamental limits; LAVA will not be injecting logic errors
into programs anytime soon. Nevertheless, LAVA is ready for
immediate use as an on-demand source of realistic ground
truth vulnerabilities for classes of serious vulnerabilities that
are still abundant
is our hope
that LAVA can drive both the development and evaluation of
advanced tools and techniques for vulnerability discovery.
in mission-critical code. It
X. ACKNOWLEDGEMENTS
Many thanks to Graham Baker, Chris Connelly, Jannick
Pewny, and Stelios Sidiroglou-Douskos for valuable early
discussions and suggestions. In addition, we thank Amy Jiang
for some critical initial Clang development and debugging.
REFERENCES
[1] Cristian Cadar, Daniel Dunbar, and Dawson Engler. KLEE: Unassisted
and automatic generation of high-coverage tests for complex systems
programs. In Proceedings of the 8th USENIX Conference on Operating
Systems Design and Implementation, OSDI’08. USENIX Association,
2008.
[2] Center for Assured Software. Juliet test suite v1.2 user guide. Technical
report, National Security Agency, 2012.
[3] Sang Kil Cha, T. Avgerinos, A. Rebert, and D. Brumley. Unleashing
mayhem on binary code. In IEEE Symposium on Security and Privacy,
2012.
[4] Vitaly Chipounov, Volodymyr Kuznetsov, and George Candea. S2E: A
platform for in-vivo multi-path analysis of software systems. In Archi-
tectural Support for Programming Languages and Operating Systems,
2011.
[5] Brendan Dolan-Gavitt, Joshua Hodosh, Patrick Hulin, Timothy Leek,
and Ryan Whelan. Repeatable reverse engineering with PANDA.
In
Workshop on Program Protection and Reverse Engineering (PPREW),
2015.
[6] Keromytis et al. Tunable cyber defensive security mechanisms. https:
//www.sbir.gov/sbirsearch/detail/825791, August 2015.
[7] Jeffrey Foster. A call for a public bug and tool registry. In Workshop
on the Evaluation of Software Defect Detection Tools, 2005.
[8] Vijay Ganesh, Tim Leek, and Martin Rinard. Taint-based directed
whitebox fuzzing. In ICSE ’09: Proceedings of the 31st International
Conference on Software Engineering, 2009.
[9] Istvan Haller, Asia Slowinska, Matthias Neugschwandtner, and Herbert
Bos. Dowsing for overﬂows: A guided fuzzer to ﬁnd buffer boundary
violations.
In Proceedings of the 22nd USENIX Security Symposium
(USENIX Security ‘13). USENIX, 2013.
tion.
[10] Michael Kass. NIST software assurance metrics and tool evaluation
(SAMATE) project. In Workshop on the Evaluation of Software Defect
Detection Tools, 2005.
[11] Kendra Kratkiewicz and Richard Lippmann. Using a diagnostic corpus
of C programs to evaluate buffer overﬂow detection by static analysis
tools.
In Proc. of Workshop on the Evaluation of Software Defect
Detection Tools, 2005.
[12] Shan Lu, Zhenmin Li, Feng Qin, Lin Tan, Pin Zhou, and Yuanyuan
Zhou. BugBench: A benchmark for evaluating bug detection tools. In
Workshop on the Evaluation of Software Defect Detection Tools, 2005.
In Workshop on
[13] Barmak Meftah. Benchmarking bug detection tools.
the Evaluation of Software Defect Detection Tools, 2005.
[14] James Newsome and Dawn Song. Dynamic taint analysis for automatic
detection, analysis, and signature generation of exploits on commodity
software. In Network and Distributed Systems Symposium (NDSS), 2005.
[15] Martin Rinard, Cristian Cadar, and Huu Hai Nguyen. Exploring the ac-
ceptability envelope. In Companion to the 20th Annual ACM SIGPLAN
Conference on Object-oriented Programming, Systems, Languages, and
Applications, OOPSLA ’05, pages 21–30, New York, NY, USA, 2005.
ACM.
[16] Olatunji Ruwase and Monica S Lam. A practical dynamic buffer
overﬂow detector. In NDSS, 2004.
[17] Shin’ichi Shiraishi, Veena Mohan, and Hemalatha Marimuthu. Test
suites for benchmarks of static analysis tools. In Proceedings of the 2015
IEEE International Symposium on Software Reliability Engineering,
ISSRE ’15, 2015.
[18] Jaime Spacco, David Hovemeyer, and William Pugh. Bug specimens are
important. In Workshop on the Evaluation of Software Defect Detection
Tools, 2005.
[19] TASC, Inc., Ponte Technologies LLC, and i SW LLC. STONESOUP
phase 3 test generation report. Technical report, SAMATE, 2014.
[20] Vlad Tsyrklevich. Hacking team: A zero-day market case study. https:
//tsyrklevich.net/2015/07/22/hacking-team-0day-market/, July 2015.
[21] Tielei Wang, Tao Wei, Guofei Gu, and Wei Zou. TaintScope: A
checksum-aware directed fuzzing tool for automatic software vulner-
ability detection. In IEEE Symposium on Security and Privacy, 2010.
[22] John Wilander and Mariam Kamkar. A comparison of publicly available
tools for static intrusion prevention. In Proceedings of the 7th Nordic
Workshop on Secure IT Systems, 2002.
[23] John Wilander and Mariam Kamkar. A comparison of publicly available
tools for dynamic buffer overﬂow prevention. In Proceedings of the 10th
Network and Distributed System Security Symposium (NDSS), 2003.
[24] F. Yamaguchi, N. Golde, D. Arp, and K. Rieck. Modeling and discov-
ering vulnerabilities with code property graphs. In IEEE Symposium on
Security and Privacy, 2014.
[25] Michael Zhivich, Tim Leek, and Richard Lippmann. Dynamic buffer
overﬂow detection. In Workshop on the Evaluation of Software Defect
Detection Tools, 2005.
[26] Misha Zitser, Richard Lippmann, and Tim Leek. Personal communica-
[27] Misha Zitser, Richard Lippmann, and Tim Leek. Testing static analysis
tools using exploitable buffer overﬂows from open source code. In Pro-
ceedings of the 12th ACM SIGSOFT Twelfth International Symposium
on Foundations of Software Engineering, SIGSOFT ’04/FSE-12, pages
97–106, New York, NY, USA, 2004. ACM.
121121
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:15:56 UTC from IEEE Xplore.  Restrictions apply.