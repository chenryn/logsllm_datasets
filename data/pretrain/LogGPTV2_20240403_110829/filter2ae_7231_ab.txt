               +-03.0  Device 1234:11e9
               \-04.0  Intel Corporation 82540EM Gigabit Ethernet Controller
其中`[0000]`表示pci的域， PCI域最多可以承载256条总线。 每条总线最多可以有32个设备，每个设备最多可以有8个功能。
总之每个 PCI 设备有一个总线号, 一个设备号, 一个功能号标识。PCI 规范允许单个系统占用多达 256 个总线, 但是因为 256
个总线对许多大系统是不够的, Linux 现在支持 PCI 域。每个 PCI 域可以占用多达 256 个总线. 每个总线占用 32 个设备, 每个设备可以是
一个多功能卡(例如一个声音设备, 带有一个附加的 CD-ROM 驱动)有最多 8 个功能。
PCI 设备通过`VendorIDs`、`DeviceIDs`、以及`Class Codes`字段区分：
    ubuntu@ubuntu:~$ lspci -v -m -n -s 00:03.0
    Device: 00:03.0
    Class:  00ff
    Vendor: 1234
    Device: 11e9
    SVendor:        1af4
    SDevice:        1100
    PhySlot:        3
    Rev:    10
    ubuntu@ubuntu:~$ lspci -v -m -s 00:03.0
    Device: 00:03.0
    Class:  Unclassified device [00ff]
    Vendor: Vendor 1234
    Device: Device 11e9
    SVendor:        Red Hat, Inc
    SDevice:        Device 1100
    PhySlot:        3
    Rev:    10
也可通过查看其`config`文件来查看设备的配置空间，数据都可以匹配上，如前两个字节`1234`为`vendor id`：
    ubuntu@ubuntu:~$ hexdump /sys/devices/pci0000\:00/0000\:00\:03.0/config
    0000000 1234 11e9 0103 0000 0010 00ff 0000 0000
    0000010 1000 febf c051 0000 0000 0000 0000 0000
    0000020 0000 0000 0000 0000 0000 0000 1af4 1100
    0000030 0000 0000 0000 0000 0000 0000 0000 0000
查看设备内存空间：
    ubuntu@ubuntu:~$ lspci -v -s 00:03.0 -x
    00:03.0 Unclassified device [00ff]: Device 1234:11e9 (rev 10)
            Subsystem: Red Hat, Inc Device 1100
            Physical Slot: 3
            Flags: fast devsel
            Memory at febf1000 (32-bit, non-prefetchable) [size=256]
            I/O ports at c050 [size=8]
    00: 34 12 e9 11 03 01 00 00 10 00 ff 00 00 00 00 00
    10: 00 10 bf fe 51 c0 00 00 00 00 00 00 00 00 00 00
    20: 00 00 00 00 00 00 00 00 00 00 00 00 f4 1a 00 11
    30: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
可以看到该设备有两个空间：BAR0为MMIO空间，地址为`febf1000`，大小为256；BAR1为PMIO空间，端口地址为`0xc050`，大小为8。
可以通过查看`resource`文件来查看其相应的内存空间：
    ubuntu@ubuntu:~$ ls -la /sys/devices/pci0000\:00/0000\:00\:03.0/
    ...
    -r--r--r--  1 root root 4096 Aug  1 03:40 resource
    -rw-------  1 root root  256 Jul 31 13:18 resource0
    -rw-------  1 root root    8 Aug  1 04:01 resource1
    ...
`resource`文件包含其它相应空间的数据，如resource0（MMIO空间）以及resource1（PMIO空间）：
    ubuntu@ubuntu:~$ cat /sys/devices/pci0000\:00/0000\:00\:03.0/resource
    0x00000000febf1000 0x00000000febf10ff 0x0000000000040200
    0x000000000000c050 0x000000000000c057 0x0000000000040101
    0x0000000000000000 0x0000000000000000 0x0000000000000000
    0x0000000000000000 0x0000000000000000 0x0000000000000000
    0x0000000000000000 0x0000000000000000 0x0000000000000000
每行分别表示相应空间的起始地址（start-address）、结束地址（end-address）以及标识位（flags）。
### qemu中访问I/O空间
存在mmio与pmio，那么在系统中该如何访问这两个空间呢？访问mmio与pmio都可以采用在内核态访问或在用户空间编程进行访问。
#### 访问mmio
编译内核模块，在内核态访问mmio空间，示例代码如下：
    #include 
    #include 
    long addr=ioremap(ioaddr,iomemsize);
    readb(addr);
    readw(addr);
    readl(addr);
    readq(addr);//qwords=8 btyes
    writeb(val,addr);
    writew(val,addr);
    writel(val,addr);
    writeq(val,addr);
    iounmap(addr);
还有一种方式是在用户态访问mmio空间，通过映射`resource0`文件实现内存的访问，示例代码如下：
    #include 
    #include 
    #include 
    #include 
    #include 
    #include 
    #include 
    #include 
    #include 
    #include
    unsigned char* mmio_mem;
    void die(const char* msg)
    {
        perror(msg);
        exit(-1);
    }
    void mmio_write(uint32_t addr, uint32_t value)
    {
        *((uint32_t*)(mmio_mem + addr)) = value;
    }
    uint32_t mmio_read(uint32_t addr)
    {
        return *((uint32_t*)(mmio_mem + addr));
    }
    int main(int argc, char *argv[])
    {
        // Open and map I/O memory for the strng device
        int mmio_fd = open("/sys/devices/pci0000:00/0000:00:04.0/resource0", O_RDWR | O_SYNC);
        if (mmio_fd == -1)
            die("mmio_fd open failed");
        mmio_mem = mmap(0, 0x1000, PROT_READ | PROT_WRITE, MAP_SHARED, mmio_fd, 0);
        if (mmio_mem == MAP_FAILED)
            die("mmap mmio_mem failed");
        printf("mmio_mem @ %p\n", mmio_mem);
        mmio_read(0x128);
            mmio_write(0x128, 1337);
    }
#### 访问pmio
编译内核模块，在内核空间访问pmio空间，示例代码如下：
    #include  
    #include 
    inb(port);  //读取一字节
    inw(port);  //读取两字节
    inl(port);  //读取四字节
    outb(val,port); //写一字节
    outw(val,port); //写两字节
    outl(val,port); //写四字节
用户空间访问则需要先调用`iopl`函数申请访问端口，示例代码如下：
    #include 
    iopl(3); 
    inb(port); 
    inw(port); 
    inl(port);
    outb(val,port); 
    outw(val,port); 
    outl(val,port);
## QOM编程模型
QEMU提供了一套面向对象编程的模型——QOM（QEMU Object
Module），几乎所有的设备如CPU、内存、总线等都是利用这一面向对象的模型来实现的。
由于qemu模拟设备以及CPU等，既有相应的共性又有自己的特性，因此使用面向对象来实现相应的程序是非常高效的，可以像理解C++或其它面向对象语言来理解QOM。
有几个比较关键的结构体，`TypeInfo`、`TypeImpl`、`ObjectClass`以及`Object`。其中ObjectClass、Object、TypeInfo定义在include/qom/object.h中，TypeImpl定义在qom/object.c中。
`TypeInfo`是用户用来定义一个Type的数据结构，用户定义了一个TypeInfo，然后调用`type_register(TypeInfo
)`或者`type_register_static(TypeInfo
)`函数，就会生成相应的`TypeImpl`实例，将这个`TypeInfo`注册到全局的`TypeImpl`的`hash`表中。
    struct TypeInfo
    {
        const char *name;
        const char *parent;
        size_t instance_size;
        void (*instance_init)(Object *obj);
        void (*instance_post_init)(Object *obj);
        void (*instance_finalize)(Object *obj);
        bool abstract;
        size_t class_size;
        void (*class_init)(ObjectClass *klass, void *data);
        void (*class_base_init)(ObjectClass *klass, void *data);
        void (*class_finalize)(ObjectClass *klass, void *data);
        void *class_data;
        InterfaceInfo *interfaces;
    };
`TypeImpl`的属性与`TypeInfo`的属性对应，实际上qemu就是通过用户提供的TypeInfo创建的TypeImpl的对象。
如下面定义的`pci_test_dev`：
    static const TypeInfo pci_testdev_info = {
            .name          = TYPE_PCI_TEST_DEV,
            .parent        = TYPE_PCI_DEVICE,
            .instance_size = sizeof(PCITestDevState),
            .class_init    = pci_testdev_class_init,