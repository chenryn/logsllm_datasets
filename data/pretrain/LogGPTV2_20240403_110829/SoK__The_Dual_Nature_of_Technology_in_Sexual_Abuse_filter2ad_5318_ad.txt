to help victims report and prevent sexual abuse,
(e.g., helplines, educational online resources, online forums
and support groups, educational campaigns, and various apps
for mobile devices) [48], [50], [62], [68], [90], [100], [105],
[117], [121], [125], [136], [156]–[164]. Other solutions focus
on changing perpetrators’ behavior and on educating the public
to be ethical bystanders [42], [87], [105], [133], [141], [165].
Academic researchers have also put forward a number of
proposals [36], [68], [76], [118], [156], [158], [166]–[175].
1) Challenges with using solutions to report and prevent
abuse: Here we present the challenges of using technological
solutions to report and prevent abuse. We categorized them
into design, stakeholder, victim, and economic challenges.
Design: Solutions are designed to have one path of re-
porting. Sometimes, however, victims cannot or are unwilling
to take that path, but they are not given other alternatives.
For instance, reporting solutions do not prevent reports of
victims from being accessed by local police ofﬁcers if the
latter happens to be the perpetrator [43].
Some solutions lack utility and do not serve a practical need
in real life [176]. For instance, stakeholders “were largely
critical of panic alarm/danger alert style apps ... they did
not really ‘add’ anything—that a quick text
to the same
effect could easily be sent or information quickly searched
for online” [160].
Solutions have low inclusion and diversity. They are de-
veloped with a stereotype mentality [56], [157]. For instance,
some seemed to have been developed with the assumption
that all users in a household trust each other [51] or have
harmonious family relationships [93]. Other solutions require
victims to have unrestricted and complete access to a techno-
logical device, which is not always the case [37]. Researchers
advocate for a more inclusive design that considers various
abuse stages for victims [56], [118], [157].
Preventive solutions put much of the burden on the victim,
which reinforces victim blaming. Solutions for preventing
abuse are focused on a series of activities that victims must do
to keep safe [42], [56], [80], [177]. Such solutions put the onus
on victims to ensure their safety [37], [157] and reinforce rape
myths [80]. Some solutions require victims to alter the way
they use technology in order to protect themselves [56], [165].
Mason et al. explained: “Such [technological] applications are
aimed at women as needing to be responsible for violence,
rather than ...
initiatives that would target perpetrators of
violence. ... women are problematically expected to change
their behavior by tracking their whereabouts and ‘checking in’
with friends to prevent violence. [The solutions] ask women
to give up personal information to third parties for their own
self-protection” [39]. Putting such a burden on the victim can
“ultimately reproduce unhelpful victim-blaming narratives and
may have the effect of promoting fear and timidity in using
technology” [42].
Solutions can have low usability. Solutions’ usability is
critical, especially because victims may be at a higher level
of stress, risk, and vulnerability when using them [32], [68].
Some solutions had poor usability [48], [160], and may be too
technical for an average user [157], [173], [174].
There are also security and privacy concerns about solu-
tions [178]. Rodríguez et al. proposed using telemonitoring
devices for victims of intimate partner violence [168]. How-
ever, “[telemonitoring tools] can improve [victims’] safety, but
also imply recording personal data, wearing smart devices, and
allowing text and voice recognition software, and this could
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:58:18 UTC from IEEE Xplore.  Restrictions apply. 
2328
be considered interference in a private life. This could lead to
a rejection of the technology by the survivor” [156].
Victims can be revictimized if perpetrators discover they
are using the solutions. Many papers discuss the risk of the
perpetrator seeing the victim researching online resources or
using other technological solutions, resulting in the perpetrator
revictimizing the victim [41], [50], [67], [68], [93], [121],
[125], [156], [178].
Another design challenge is solutions that do not work or
are not trauma informed. Brignone and Edleson [160] surveyed
several smartphone apps in the app store and discovered that
some apps do not fulﬁll their claims. The researchers also
found that the design of some apps is not trauma informed
(e.g., containing information that blames victims for the
abuse).
Solutions that cannot be personalized are also a challenge.
Stakeholders and victims have a wide range of technical exper-
tise [67]. Further, victims are in different abuse stages [48].
Solutions ideally should account for those differences [43],
[48]. Research suggests that a single solution or a one-size-
ﬁts-all approach will not help victims [58], [179]. Personalized
solutions or a combination of solutions are needed [68], [160].
Stakeholder: Stakeholders fear that, despite the precautions
in place, perpetrators may still discover that the victim is
using a solution, resulting in revictimization [180]. Therefore,
some stakeholders refuse to recommend the solutions to their
clients [48], [180]. Further, some support workers did not
fully understand the technological solutions, and they “simply
wanted the technology to go away” [78]. Support workers
admitted they had low technological readiness and lacked the
time to learn about the solutions [43].
For instance, the research of Westmarland et al. suggests that
stakeholders “had not worked with any woman that said they
had used a domestic or sexual violence app” [37]. Stakeholders
interviewed in another study had never heard of the apps
designed for preventing or reporting sexual violence [160].
Further, victims that need to use the solutions do not know
they exist [48], [68]. In a study by Bouché et al. [121],
over 70% of the victims said they never saw any hotline
number they could use to dial for help. Also, the majority
of the victims admitted that while they wanted help escaping
from their abusers, they did not know where to get it or how
to seek it out [121]. Victims had contacted support centers
for help, but those centers lacked the needed information or
recommendations [54]. In addition, while surveying sexual
abuse apps in the app store, Brignone and Edleson [160]
discovered that most of the apps lack visibility even in the
app store. Cardoso et al. also noted that in situations where
solutions are marketed, there is little or no evidence of what
the solutions claim to do, which can discourage victims from
using the solutions [56].
Unclear usability testing or risk assessment
is another
economic challenge with using solutions to report and prevent
abuse. It is unclear which of the proposed and developed
solutions are evaluated to avoid low usability or risk to
victims [37], [160]. Developers also do not explain to victims
the safety risks associated with using the solutions, creating
a false sense of security [37], [174]. Such knowledge would
help victims to gauge their environment and safety and decide
if the solution would be appropriate.
The solutions are also improperly maintained and/or out-
dated [83], [160]. Maintenance of the solutions is critical
to ensure that the solutions have up-to-date information and
security where needed [160]. However, the developer may not
have the time and resources for maintenance [43], [83].
Some solutions charge fees, which can seem exploitative. As
explained by Mason et al., “companies are primarily concerned
with developing tools that consumers will purchase rather than
with women’s safety” [39]. A pay-as-you-use business model
is challenging in the domain of sexual abuse prevention and
reporting, as victims are not willing to pay for such solutions
and view those solutions as “exploitative” [56], [125], [157].
C. Restricting abuse
Service providers have put in place some technological mea-
sures to restrict abuse. For instance, some social media sites
have methods to ﬂag pornographic content on their sites [100].
Some organizations have customized their technology (e.g.,
Google Maps hides undisclosed victim shelters [39]).
Governments have also implemented measures to restrict
abuse. For example, some countries in Asia have implemented
Internet ﬁltering to limit social networking and websites that
carry pornographic material. This limitation is to reduce the
possibility of sexual abuse grooming and psychological re-
victimization [100], [123]. Some laws also require that social
media sites close the accounts of any found sexual perpetra-
tor [100] and attempt to ﬁnd the perpetrator’s location [50].
Another stakeholder challenge is a lack of involvement in
the development of solutions [116]. Solutions are developed
in isolation without
them [37], [45], [161]. This lack of
involvement leads to the design of solutions that do not assist
victims [79], [161], [178] and that stakeholders are unwilling
to use [30], [51], [86], [157], [174].
Victim: Victims have accessibility concerns. In describing
the limitations that victims face, Stonard et al. explain that
“those least likely to use the Internet for assistance will be
those who tend to be most marginalised; this includes women
who are refugees, women whose ﬁrst language is not English,
women who are not literate, and women in poverty who have
no access to the Internet or do not have the requisite skills
for using the Internet” [67]. Solutions are not accessible to
victims with literacy and language barriers [67], [118], [167],
[169], [174]. Further, most trafﬁcked victims do not have
smartphones to download the apps or use the solutions [43],
[125].
Victims also can need in-person support. While using tech-
nological solutions, some victims need a support worker to be
present either on the phone or in person to help them [169],
[181]. Research also suggests that, regardless of the solutions,
they could never replace face-to-face interactions [43].
Economic: Solutions are poorly marketed, so victims and
stakeholders do not know about them or where to ﬁnd them.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:58:18 UTC from IEEE Xplore.  Restrictions apply. 
2329
Further, countries have laws that help to limit the use of
surveillance devices against victims [138]. Governments have
also legalized the use of drones by government ofﬁcials to
monitor sexual abuse activities of sex trafﬁcking perpetra-
tors [78].
1) Challenges with restricting abuse: Here we present the
challenges of using technological solutions to restrict abuse.
We categorized them into regulatory concerns and legislative
limits.
Regulatory concerns: Finding a balance between moni-
toring sex trafﬁckers and protecting people’s privacy is dif-
ﬁcult. Governments in some cities use drones to monitor sex
trafﬁckers’ activities, but the use of drones invades people’s
privacy [78].
Legislative limits: Most
laws on sexual abuse are not
uniform across jurisdictions, making it challenging to appre-
hend perpetrators [138]. Perpetrators could meet a target and
commit sexual abuse in one state while living in another where
some of these laws do not apply [62], [90], [128], [138].
VI. DISCUSSION
A. Limitations
Our SoK centered mainly on research work in developed
countries, which may have inﬂuenced our categorization of
technology’s attributes. However, most of the research papers
we found in the ﬁeld of technology-facilitated sexual abuse
centered on developed countries. Further, we do not speciﬁ-
cally focus on the link between technology and child sexual
abuse.
As with any qualitative research, our ﬁndings may have been
affected by systematic biases [182]. To reduce researcher bias,
multiple researchers were involved in analyzing the data and
converged on their interpretations [26], [27]. Furthermore, we
used only Google Scholar to search for papers, which might
have introduced additional system bias. At the same time,
Google Scholar’s inclusive and unsupervised approach appears
to provide the most broad coverage of papers [24]–[27].
Despite the above limitations, we believe our study provides
a helpful background for future research on using technology
to assist victims and reduce sexual abuse.
B. General discussion
The goal of our paper was to identify the gaps in techno-
logical assistance for victims of sexual abuse. Our ﬁndings
point to the characteristics of technology that facilitate abuse
(Figure 1) and also to gaps in investigating abuse using digital
evidence, reporting and preventing abuse using technolog-
ical solutions, and restricting abuse through the measures
imposed by governments and service providers (Figure 2).
We believe this knowledge can help various stakeholders
(including researchers) become more aware of the gaps that
need addressing.
Our paper also discusses how technology’s characteristics
accentuate the challenges of providing assistance to victims of
abuse. We also discuss challenges separate from technology’s
characteristics (e.g., the inability to determine consent from
given evidence), challenges we anticipate might be easier to
solve in the short term. The rest of the challenges do not
appear insurmountable or as hurdles to be avoided; instead,
they appear as pain points that the industry and academia can
work on addressing. We view the identiﬁed gaps as a call for
action in assisting victims, as well as restricting perpetrators
and holding them accountable.
Here we discuss possible solutions and research directions
for addressing these challenges (summarized in Table I).
It should be noted that proper evaluation of the discussed
solutions is subject to future research.
C. Investigating abuse (evidence)
The search for digital evidence is often the ﬁrst step in
investigating sexual abuse [155]. However, many challenges
exist in collating and using digital evidence in the court of law
(§V-A1). We suggest possible solutions and research directions
for addressing the challenges of investigating abuse and using
its evidence.
1) Evolution and opaqueness: Stakeholders and victims
need to understand technologies enough to avoid dangerous
errors (see §V-A1 and [183]). This is where the research
community could help with developing strategies to make
technology less opaque. Such improvements could help bridge
the knowledge gap, reduce misconceptions, increase trust in
the technology, and facilitate the collection of less ambiguous
evidence. Betzing et al. [184], for instance, discovered that
increasing transparency helped improve comprehension of data
practices and policies among the users of mobile devices.
Transparency could be achieved by making the devices more
intuitive to use [185].
Improving mental models could help victims and stakehold-
ers better understand how they can collate and use evidence.
Even before interacting with new technologies, stakeholders
implicitly have the notion that new technologies are difﬁcult
to use and come with added complexities (§V-A1). How-
ever, this perception is not always true. One of the main
goals of the usable privacy and security community is to
improve users’ mental models related to security and privacy
aspects of technology [186]. For instance, a research group
at Carnegie Mellon University has proposed labels to improve
consumers’ mental models of IoT devices’ security and privacy
characteristics [187]. Similarly, the research community could
investigate improvements to the mental models of stakeholders
to help them make informed decisions about digital evidence.
2) Malleability: Research in computer forensics could help
prove the validity of evidence. Technology is malleable, and it
is difﬁcult to prove that a piece of digital evidence has not been
tampered with (§V-A1). The possibility of tampered evidence
complicates the use of digital evidence in court proceedings (
[155], [188].
More research could also be done to improve effectiveness
and efﬁciency when determining the validity of evidence
(§V-A1. The advances will be important in collecting and
preserving evidence that is tamperproof and admissible in
court, while also advancing the justice process more quickly
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:58:18 UTC from IEEE Xplore.  Restrictions apply. 
2330
Challenges
Possible solutions and research directions
Category of
assistance
to victims
Investigating
abuse
Evolution, opaqueness
Malleability
Reproducibility
Consent issues
technology campaigns that clarify
a. Make solutions intuitive to use.
b. Develop features that help improve stakeholders’ mental
models.
Improve processes and timelines for validating the authen-
ticity of the evidence.