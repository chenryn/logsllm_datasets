title:Using Adaptive Alert Classification to Reduce False Positives in Intrusion
Detection
author:Tadeusz Pietraszek
Using Adaptive Alert Classiﬁcation
to Reduce False Positives in Intrusion Detection
Tadeusz Pietraszek
IBM Zurich Research Laboratory
S¨aumerstrasse 4, CH-8803 R¨uschlikon, Switzerland
PI:EMAIL
Abstract. Intrusion Detection Systems (IDSs) are used to monitor com-
puter systems for signs of security violations. Having detected such signs,
IDSs trigger alerts to report them. These alerts are presented to a human
analyst, who evaluates them and initiates an adequate response.
In practice, IDSs have been observed to trigger thousands of alerts per
day, most of which are false positives (i.e., alerts mistakenly triggered
by benign events). This makes it extremely diﬃcult for the analyst to
correctly identify the true positives (i.e., alerts related to attacks).
In this paper we describe ALAC, the Adaptive Learner for Alert Classi-
ﬁcation, which is a novel system for reducing false positives in intrusion
detection. The system supports the human analyst by classifying alerts
into true positives and false positives. The knowledge of how to classify
alerts is learned adaptively by observing the analyst. Moreover, ALAC
can be conﬁgured to process autonomously alerts that have been classi-
ﬁed with high conﬁdence. For example, ALAC may discard alerts that
were classiﬁed with high conﬁdence as false positive. That way, ALAC
eﬀectively reduces the analyst’s workload.
We describe a prototype implementation of ALAC and the choice of a
suitable machine learning technique. Moreover, we experimentally vali-
date ALAC and show how it facilitates the analyst’s work.
Keywords: Intrusion detection, false positives, alert classiﬁcation, ma-
chine learning
1 Introduction
The explosive increase in the number of networked machines and the widespread
use of the Internet in organizations has led to an increase in the number of unau-
thorized activities, not only by external attackers but also by internal sources,
such as fraudulent employees or people abusing their privileges for personal gain.
As a result, intrusion detection systems (IDSs), as originally introduced by An-
derson [1] and later formalized by Denning [8], have received increasing attention
in recent years.
On the other hand, with the massive deployment of IDSs, their operational
limits and problems have become apparent [2, 3, 15, 23]. False positives, i.e., alerts
that mistakenly indicate security issues and require attention from the intrusion
E. Jonsson et al. (Eds.): RAID 2004, LNCS 3224, pp. 102–124, 2004.
c(cid:1) Springer-Verlag Berlin Heidelberg 2004
Using Adaptive Alert Classiﬁcation
103
detection analyst, are one of the most important problems faced by intrusion
detection today [28]. In fact, it has been estimated that up to 99% of alerts
reported by IDSs are not related to security issues [2, 3, 15].
In this paper we address the problem of false positives in intrusion detection
by building an alert classiﬁer that tells true from false positives. We deﬁne alert
classiﬁcation as attaching a label from a ﬁxed set of user-deﬁned labels to an
alert. In the simplest case, alerts are classiﬁed into false and true positives, but
the classiﬁcation can be extended to indicate the category of an attack, the
causes of a false positive or anything else.
Alerts are classiﬁed by a so-called alert classiﬁer (or classiﬁer for short).
Alert classiﬁers can be built automatically using machine learning techniques
or they can be built manually by human experts. The Adaptive Learner for
Alert Classiﬁcation (ALAC) introduced in this paper uses the former approach.
Moreover, ALAC learns alert classiﬁers whose classiﬁcation logic is explicit so
that a human expert can inspect it and verify its correctness. In that way, the
analyst can gain conﬁdence in ALAC by understanding how it works.
ALAC classiﬁes alerts into true positives and false positives and presents
these classiﬁcations to the intrusion detection analyst, as shown in Fig. 1 on
page 106. Based on the analyst’s feedback, the system generates training ex-
amples, which are used by machine learning techniques to initially build and
subsequently update the classiﬁer. The classiﬁer is then used to classify new
alerts. This process is continuously repeated to improve the alert classiﬁcation.
At any time the analyst can review the classiﬁer.
Note that this approach hinges on the analyst’s ability to classify alerts cor-
rectly. This assumption is justiﬁed because the analyst must be an expert in in-
trusion detection to perform incident analysis and initiate appropriate responses.
This raises the question of why analysts do not write alert classiﬁcation rules
themselves or do not write them more frequently. An explanation of these issues
can be based on the following facts:
Analysts’ knowledge is implicit: Analysts ﬁnd it hard to generalize, i.e., to
formulate more general rules, based on individual alert classiﬁcations. For
example, an analyst might be able to individually classify some alerts as false
positives, but may not be able to write a general rule that characterizes the
whole set of these alerts.
Environments are dynamic: In real-world environments the characteristics
of alerts change, e.g., diﬀerent alerts occur as new computers and services are
installed or as certain worms or attacks gain and lose popularity. The clas-
siﬁcation of alerts may also change. As a result, rules need to be maintained
and managed. This process is labor-intensive and error-prone.
As stated above, we use machine learning techniques to build an alert clas-
siﬁer that tells true from false positives. Viewed as a machine learning problem,
alert classiﬁcation poses several challenges.
First, the distribution of classes (true positives vs. false positives) is often
skewed, i.e., false positives are more frequent than true positives. Second, it is
also common that the cost of misclassifying alerts is most often asymmetrical
104
Tadeusz Pietraszek
i.e., misclassifying true positives as false positives is usually more costly than the
other way round. Third, ALAC classiﬁes alerts in real-time and updates its clas-
siﬁer as new alerts become available. The learning technique should be eﬃcient
enough to perform in real-time and work incrementally, i.e., to be able to modify
its logic as new data becomes available. Fourth, we require the machine learn-
ing technique to use background knowledge, i.e., additional information such as
network topology, alert database, alert context, etc., which is not contained in
alerts, but allows us to build more accurate classiﬁers (e.g., classiﬁers using gen-
eralized concepts). In fact, research in machine learning has shown that the use
of background knowledge frequently leads to more natural and concise rules [16].
However, the use of background knowledge increases the complexity of a learning
task and only some machine learning techniques support it.
We revisit these challenges in Sect. 2.2, where we discuss them and present a
suitable learning technique. The point made here is that we are facing a highly
challenging machine learning problem that requires great care to solve properly.
1.1 Related Work
To the best of our knowledge, machine learning has not previously been used
to incrementally build alert classiﬁers that take background knowledge into ac-
count. However, some of the concepts we apply here have been successfully used
in intrusion detection and related domains.
Building IDSs. In intrusion detection, machine learning has been used primarily
to build systems that classify network connections (e.g., 1999 KDD CUP [13])
or system call sequences (e.g.,
[22]) into one of several predeﬁned classes.
This task proved to be very diﬃcult because it aimed at building IDSs only
from training examples. Lee [17] developed a methodology to construct addi-
tional features using data mining. He also showed the importance of domain-
speciﬁc knowledge in constructing such IDSs. The key diﬀerences of our work
is the real-time use of analyst feedback and that we classify alerts generated by
IDSs, whereas other researchers used machine learning to build a new IDS.
Fan [10] performed a comprehensive study of cost-sensitive learning using
classiﬁer ensembles with RIPPER, therefore his work is particularly relevant to
ours. The work diﬀers from ours in design goals: we developed a system to assist
human users to classify alerts generated by an IDS, whereas Fan built an IDS
using machine learning techniques. We also used a simpliﬁed cost model, in order
to reduce the number of variable parameters in the system. Finally, the type of
learning methods used is also diﬀerent: ensemble-based learning methods vs. a
single classiﬁer in our case.
Alert Classiﬁcation. The methods used to classify alerts can be divided into two
categories: ﬁrst, methods that identify true positives and second, methods that
identify false positives.
Methods that identify true positives have been studied particularly well and
can be summarized as follows:
Using Adaptive Alert Classiﬁcation
105
– In environments with multiple IDSs, some methods enhance the conﬁdence
of alerts generated by more than one IDS (based on the assumption that the
real attack will be noticed by multiple IDSs, whereas false positives tend to
be more random) [28],
– Couple sensor alerts with background knowledge to determine whether the
attacked system is vulnerable [20, 28],
– Create groups of alerts and use heuristics to evaluate whether an alert is a
false positive [6, 35]. The work by Dain and Cunningham [6] is particularly
relevant to us as it uses machine learning techniques: neural networks and
decision trees to build a classiﬁer grouping alerts into so-called scenarios.
They also discuss domain-speciﬁc background knowledge used to discover
scenarios. In contrast, our work focuses on alert classiﬁcation, uses diﬀerent
background knowledge and diﬀerent machine learning algorithms.
The second category of alert classiﬁcation methods identiﬁes false positives
and can be based on data mining and include root cause analysis [15], or on
statistical proﬁling [23]. For example, Julisch [15] shows that the bulk of alerts
triggered by an IDS can be attributed to a small number of root causes. He also
proposes a data mining technique to discover and understand these root causes.
Knowing the root causes of alerts, one can easily design ﬁlters to remove alerts
originating from benign root causes. Our work diﬀers from the above in that we
use real-time machine learning techniques that take advantage of background
knowledge.
1.2 Paper Overview
The remainder of this paper is organized as follows. In Section 2 we present
the design of our system and analyze machine learning techniques and their
limitations with regard to the learning problem we are facing. Section 3 describes
the prototype implementation of the system and shows results obtained with
synthetic and real intrusion detection data. In Section 4 we present conclusions
and future work.
2 ALAC – An Adaptive Learner for Alert Classiﬁcation
In this section we describe the architecture of the system and contrast it to a
conventional setup. We introduce two modes in which the system can operate,
namely recommender mode and agent mode. We then focus on machine learning
techniques and discuss how suitable they are for alert classiﬁcation.
2.1 ALAC Architecture
In a conventional setup, alerts generated by IDSs are passed to a human analyst.
The analyst uses his or her knowledge to distinguish between false and true
positives and to understand the severity of the alerts. Note that conventional
106
Tadeusz Pietraszek
systems may use manual knowledge engineering to build an alert classiﬁer or
may use no alert classiﬁer at all. In any case, the conventional setup does not
take advantage of the fact that the analyst is analyzing the alerts in real-time:
the manual knowledge engineering is separated from analyzing alerts.
Alerts
Alert
Classifier
Classified
 Alerts
IDS
Feedback
ID Analyst
Hosts/Network
Rules
Params
Background
Knowledge
Model Update
Update Rules
Machine
Learning
Training
Examples
(a) Recommender mode
(b) Agent mode
Fig. 1. Architecture of ALAC in agent and recommender mode.
As shown in Fig. 1, our system classiﬁes alerts and passes them to the analyst.
It also assigns a classiﬁcation conﬁdence (or conﬁdence for short), to alerts, which
shows the likelihood of alerts belonging to their assigned classes. The analyst
reviews this classiﬁcation and reclassiﬁes alerts, if necessary. This process is
recorded and used as training by the machine learning component to build an
improved alert classiﬁer.
Currently we use a simple human-computer interaction model, where the
analyst explicitly classiﬁes alerts into true and false positives. More sophisticated
interaction techniques are possible and will be investigated as part of our future
work. In addition to the training examples, we use background knowledge to
learn improved classiﬁcation rules. These rules are then used by ALAC to classify
alerts. The analyst can inspect the rules to make sure they are correct.
The architecture presented above describes the operation of the system in
recommender mode. The second mode, agent mode, introduces autonomous pro-
cessing to reduce the operator’s workload.
In recommender mode (Fig. 1(a)), ALAC classiﬁes alerts and passes all of
them to the console to be veriﬁed by the analyst. In other words, the system
assists the analyst suggesting the correct classiﬁcation. The advantage for the
analyst is that each alert is already preclassiﬁed and that the analyst has only to
verify its correctness. The analyst can prioritize his or her work, e.g., by dealing
with alerts classiﬁed as true positives ﬁrst or sorting the alerts by classiﬁcation
Using Adaptive Alert Classiﬁcation
107
conﬁdence. It is important to emphasize that at the end, the analyst will review
all classiﬁcations made by the system.
In agent mode (Fig. 1(b)), ALAC autonomously processes some of the alerts
based on criteria deﬁned by the analyst (i.e., classiﬁcation assigned by ALAC
and classiﬁcation conﬁdence). By processing alerts we mean that ALAC executes
user-deﬁned actions associated with the class labels and classiﬁcation conﬁdence
values. For example, attacks classiﬁed as false positives can be automatically
removed, thus reducing the analyst’s workload. In contrast, alerts classiﬁed as
true positives and successful attacks can initiate an automated response, such as
reconﬁguring a router or ﬁrewall. It is important to emphasize that such actions
should be executed only for alerts classiﬁed with high conﬁdence, whereas the
other alerts should still be reviewed by the analyst.
Note that autonomous alert processing may change the behavior of the sys-
tem and negatively impact its classiﬁcation accuracy. To illustrate this with an
example, suppose the system classiﬁes alerts into true and false positives and it
is conﬁgured to autonomously discard the latter if the classiﬁcation conﬁdence
is higher than a given threshold value. Suppose the system learned a good clas-
siﬁer and classiﬁes alerts with high conﬁdence. In this case, if the system starts
classifying all alerts as false positives then these alerts would be autonomously
discarded and would never be seen by the analyst. These alerts would not become
training examples and would never be used to improve the classiﬁer.
Another problem is that alerts classiﬁed and processed autonomously cannot
be added to the list of training examples as the analyst has not reviewed them.
If alerts of a certain class are processed autonomously more frequently than
alerts belonging to other classes (as in the above example), we eﬀectively change
the class distribution in the training examples. This has important implications
as machine learning techniques are sensitive to class distribution in training
examples. In the optimal case, the distribution of classes in training and testing
examples should be identical.
To alleviate these problems, we propose a technique called random sampling.
In this technique we randomly select a fraction k of alerts which would normally
be processed autonomously and instead forward them to the analyst. This en-
sures the stability of the system. The value of k is a tradeoﬀ between how many
alerts will be processed autonomously and how much risk of misclassiﬁcation is
acceptable.
Background Knowledge Representation. Recall that we use machine learning
techniques to build the classiﬁer. In machine learning, if the learner has no
prior knowledge about the learning problem, it learns exclusively from examples.
However, diﬃcult learning problems typically require a substantial body of prior
knowledge [16], which makes it possible to express the learned concept in a more
natural and concise manner. In the ﬁeld of machine learning such knowledge is
referred to as background knowledge, whereas in the ﬁeld of intrusion detection
it is quite often called context information (e.g.,
[32]).
The use of background knowledge is also very important in intrusion detec-
tion [28]. Examples of background knowledge include:
108
Tadeusz Pietraszek
Network Topology. Network topology contains information about the struc-
ture of the network, assigned IP addresses, etc. It can be used to better un-
derstand the function and role of computers in the network. In the context of
machine learning, network topology can be used to learn rules that make use
of generalized concepts such as Subnet1, Intranet, DMZ, HTTPServer.
Alert Context. Alert context, i.e., other alerts related to a given one, is in the
case of some alerts (e.g., portscans, password guessing, repetitive exploits
attempts) crucial to their classiﬁcation. In intrusion detection various deﬁni-
tions of alert context are used. Typically, the alert context has been deﬁned
to include all alerts similar to it, however the deﬁnition of similarity varies
greatly [6, 5, 34].
Alert Semantics and Installed Software. By alert semantics we mean how
an alert is interpreted by the analyst. For example, the analyst knows what
type of intrusion the alert refers to (e.g., scan, local attack, remote attack)
and the type of system aﬀected (e.g., Linux 2.4.20, Internet Explorer 6.0).
Typically the alert semantics is correlated with the software installed (or the
device type, e.g., Cisco PIX) to determine whether the system is vulnera-
ble to the reported attack [20]. The result of this process can be used as
additional background knowledge used to classify alerts.
Note that the information about the installed software and alert semantics
can be used even when alert correlation is not performed, as it allows us
to learn rules that make use of generalized concepts such as OS Linux, OS
Windows, etc.
2.2 Machine Learning Techniques
Until now we have been focusing on the general system architecture and issues
speciﬁc to intrusion detection. In this section we focus on the machine learning
component in our system. Based on the discussion in Sect. 1 and the proposed
system architecture, we can formulate the following requirements for the machine
learning technique:
1. Learn from training examples (alert classiﬁcation given by the analyst).
2. Build the classiﬁer whose logic can be interpreted by a human analyst, so
its correctness can be veriﬁed.
3. Be able to incorporate the background knowledge required.
4. Be eﬃcient enough to perform real-time learning.
5. Be able to assess the conﬁdence of classiﬁcations. Conﬁdence is a numerical
value attached to the classiﬁcation, representing how likely it is to be correct.
6. Support cost-sensitive classiﬁcation and skewed class distributions.
7. Learn incrementally.
Learning an Interpretable Classiﬁer from Examples. The ﬁrst requirement yields
supervised machine learning techniques, that is techniques that can learn from
training examples. The requirement for an understandable classiﬁer further lim-
its the range of techniques to symbolic learning techniques, that is techniques
that present the learned concept in a human readable form (e.g., predictive rules,
decision trees, Prolog clauses) [22].
Using Adaptive Alert Classiﬁcation
109
Background Knowledge and Eﬃciency. The ability to incorporate background
knowledge diﬀerentiates two big groups of symbolic learners: inductive logic pro-
gramming and symbolic attribute-value learners. In general, inductive logic pro-