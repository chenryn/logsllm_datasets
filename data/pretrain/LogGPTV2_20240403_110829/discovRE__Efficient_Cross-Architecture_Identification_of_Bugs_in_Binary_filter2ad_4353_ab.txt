Instead, we used ssdeep with a minimum similarity score of
95. This procedure reduced the number of unique binaries
from over 1,700 to about 600 for each corresponding set of
binaries. This means that many compilation options seem to
have virtually no effect on the resulting binaries.
By leaving the symbolic information intact during compila-
tion we could match functions from different binaries by their
name. The resulting machine code is not altered by keeping
symbols, as this information resides either in a different part
of the executable or is stored in an external ﬁle, depending on
the compiler and compilation options. Special care needed to
be taken with idiosyncrasies in the compiler name-mangling
schemes to match the symbolic names.
All binaries were disassembled and the features were
extracted for each function. The commercial disassembler IDA
Pro [28] was used as framework. It required little implemen-
tation effort, as it natively supports the analysis of a vast
number of different CPU architectures. Additionally, IDA Pro
is able to load symbolic information from different compilers.
We only considered functions with at least ﬁve basic blocks
in our experiments. This is unproblematic as the probability
that less complex functions have a vulnerability is signiﬁcantly
lower [46].
With all above considerations, 8,895 individual functions
with at least ﬁve basic blocks could be matched by their
symbol name over all compilation options. This resulted in
over 6 million individual binary functions.
2) Removal of duplicate functions: Exact binary duplicates
obviously have the same numeric and structural features. As
these would lead to a bias during the robustness analysis of
the features, we ﬁrst needed to exclude them. To this end, we
calculated the checksum of each function over its bytecode
sequence. In the bytecode all references to memory locations
were zeroed as they are likely to change over compilations.
This includes target addresses of calls, string references and
other pointers such as virtual method tables.
Despite careful removal of duplicates at ﬁle level we still
observed that a large amount of functions had the same check-
sum. After eliminating the possibility of a hash collision from
different inputs, we could think of two possible reasons for this
phenomenon: Either the source code had been duplicated, i.e.,
there exist two different functions with the same source code,
or the source code had been compiled to the same machine
code, despite different compilation options.
We checked both hypotheses and found both to be true.
While the duplication of source code amounted for 1.41 %
of the total code base,
the lion’s share were compilation
duplicates with a total amount of 58.06 %. Although very
different compilation options had been selected, they seemed
to have only limited effect on the resulting binary code. This
insight is especially interesting for labeling methods based on
the binary code, such as IDA FLIRT [28].
3) Identiﬁcation of robust numeric features: Each function
holds a wealth of numeric information or ”meta data”, e.g.,
the number of instructions. For the numeric ﬁlter, the key
hypothesis is that there exists a set of numeric features that
can correctly characterize a binary function, even across above
described boundaries. The intuition is that a function has
semantics, which is expressed in the source code by a certain
syntax. The compiler translates the function’s source code
to a target platform, where it can use several optimizations.
However, the underlying semantics must remain the same.
Thus, certain key features should not alter signiﬁcantly.
As an extensive description of all scrutinized features is
outside the scope of this work, we only concentrate on the most
interesting ones. Quite obvious is the number of instructions,
the size of local variables and the number of parameters. We
additionally classiﬁed each instruction by its functionality [33].
The classes are arithmetic, logic, data transfer, and redirection
instructions. Based on the CFG, we counted the number of
basic blocks and edges. Additionally, we counted the number
of strongly connected components as a rough estimate for
loops. By analyzing the call graph we counted the number of
incoming and outgoing edges to/from a function as a measure
how ”often” a function is called and how many functions it
calls.
There are two key aspects to assess the robustness of a nu-
meric feature: Firstly, its value should only change minimally
over all compilation options, and secondly, the values should
be distributed over a larger domain.
To assess the quality of the former, we employed the
Pearson product-moment correlation coefﬁcient. A value of
−1 resembles perfect anti-correlation of two numeric vectors,
a value of 0 means no correlation, and a value of +1 denotes
perfect correlation. Here, we are seeking correlations towards
+1. To generate the numeric vectors, we matched the symbolic
names of the functions over the according binaries and ex-
tracted the scrutinized feature. We then calculated the average
over all pair-wise correlation coefﬁcients and additionally
the standard deviation. The quality of the value distribution
was checked by counting the number of different values and
calculating the standard deviation.
Table II shows the robustness of various features. Higher
values in the ﬁrst two columns imply better robustness of a
selected feature. Column three displays the average correlation,
column four shows the standard deviation of the correlation.
We aim for a high average correlation and a low standard
deviation. Highlighted in Table II are the features we selected
as sufﬁciently robust.
B. Numeric ﬁlter
To minimize the expensive structural comparison of two
CFGs, we added an efﬁcient ﬁlter by only relying on numeric
features. We chose the kNN algorithm, as it ﬁts best for a
candidate search. The ﬁlter uses the robust numeric features
described above to efﬁciently identify a set of candidates. In
this section, we will present the details of the ﬁlter and discuss
potential alternatives.
There exists a vast amount of machine learning algorithms
that are apt to search a function in a code base by their numeric
features. However, the search for similar functions has some
special requirements that allow us to sort out some machine
4
Feature
Arithmetic Instr.
Function Calls
Logic Instr.
Redirections
Transfer Instr.
Local Vars.
Basic Blocks
scc
Edges
Incoming Calls
Instr.
Parameters
sd(values) values
39.483
22.980
49.607
40.104
163.443
2.78E6
48.194
25.078
76.932
46.608
295.408
2.157
623
273
625
556
1,635
890
619
389
835
261
2,447
38
avg.cor
0.907
0.983
0.953
0.978
0.961
0.983
0.978
0.942
0.979
0.975
0.970
0.720
sd(cor)
0.109
0.073
0.067
0.066
0.075
0.099
0.067
0.128
0.066
0.086
0.069
0.228
TABLE II: Robustness of numeric features. The selected
features are highlighted.
learning algorithms. One key requirement
is performance.
Hence, to make the system feasible for real-world applications,
the search algorithm needs to ﬁnd similar elements in a
large amount of elements in a few milliseconds. Memory
consumption should be moderate to cope with large code bases.
The classiﬁcation algorithm should be able to return multiple
elements in one search. Further, the algorithm needs the ability
to cope with many different labels, as the number of functions
in the database is roughly the same size as the number of
labels.
The machine learning algorithm being suited best for all
aforementioned criteria is the kNN algorithm and was hence
chosen as numeric ﬁlter. a potential alternative are Support
Vector Machines, however, they require relatively much mem-
ory, additionally, the setup times are much higher. The main
beneﬁt of kNN is that the list of the k most similar functions
can be retrieved. Next, the control ﬂow graphs (CFGs) of
the resulting candidate functions are compared to that of the
queried function.
1) Data normalization: The numeric features have different
value ranges. For example, the size of local variables com-
monly ranges from zero to a few thousand bytes, while the
number of arguments is in the range of zero to about a dozen.
This would lead to problems in the distance calculation of
the kNN algorithm. Hence, each feature is normalized, i.e.,
its value domain is adjusted to an average value of 0.0 and
a standard deviation of 1.0. Both statistic values were derived
from the data set.
It is well possible that two functions collide wrt. their nu-
meric features, i.e., the features have the same values. During
the code base creation phase, duplicate entries are mapped to
the same point in the vector space. If that representative point
is returned by the kNN algorithm, all functions that are mapped
to that point are returned. Additionally, a distance threshold is
introduced to remove functions that are too dissimilar.
2) Evaluation of the numeric ﬁlter: There exists a plethora
of different implementations of the kNN algorithm. We se-
lected OpenCV’s Fast Library for Approximate Nearest Neigh-
bors (FLANN) [50]. It brings a variety of different nearest
neighbor implementations, of which we selected three different
that each highlight special aspects:
• linear index,
• k-d trees, and
• hierarchical k-means.
The linear index iterates over all elements of the search
space and stores the current nearest neighbors in a priority
queue of size k. Once all elements have been visited, the queue
contains the k nearest elements. Hence, the query times are
in O(n). As the algorithm does not need any special data
structures for storage, the memory consumption is negligible.
We chose this algorithm to serve as lower/upper bound to
give an estimate about
the expected times. The k-d trees
algorithm is a set of multidimensional binary search trees.
During the creation of one tree, one dimension is randomly
chosen and the data set
is subdivided by the hyperplane
perpendicular to the corresponding axis. In the query phase,
all trees are searched in parallel [50]. For low dimensions,
the average running time of a query is O(log n). Clearly,
this algorithm requires more memory, as it needs to store
several k-d trees, which is typically in O(kn), with k being the
number of trees. We set the number of trees in the k-d trees
algorithm to 8 as a compromise between quality and memory
consumption. Hierarchical k-means recursively splits the data
set into smaller clusters, aiming at maximizing the distance
between the clusters. Also here, the expected running time of
a query is in O(log n). The space complexity is typically in
O(k + n)
One key observation from the data set was that some
optimization options generally tend to generate more machine
code instructions per line of source code. This means that we
can expect larger absolute differences for larger functions. We
responded to this phenomenon by applying the decadic log-
arithm to all numeric values. After normalizing each feature,
we populated the vector space of each kNN algorithm and
measured the times to create the search structure. After that
we let the kNN implementation search for a randomly chosen
point. We repeated the search 10,000 times and calculated the
average values where feasible.
Note that the classical application of the kNN algorithm
performs a majority vote on the labels of the k nearest
neighbors. Contrary to that, we actually submit the k nearest
points to the detailed ﬁlter stage. Figure 2 shows the creation
and query times for various numbers of points. Most notably,
k-d trees take signiﬁcantly longer to set up. Interestingly, the
query times do not signiﬁcantly increase over the number of
functions in the search index, apart from the linear index.
The evaluation shows that even for larger code bases (e.g.,
a full Android image has over 130,000 functions) both, the
times to create the search structure of the code base and the
times to search similar functions in it are feasible for real-world
applications. However, there still remains space for further
optimizations, as we show in the next section.
3) Dimensionality reduction: As higher dimensions of a
data set might lead to signiﬁcantly longer runtime of the kNN
algorithm (see the curse of dimensionality [45]), we analyzed
the data set for linear dependencies using principal component
analysis. With ﬁve components the cumulative proportion of
the variance was already at 96.82 %. We repeated above
measurements on the data set with reduced dimensionality
only for k-d trees as selected pre-ﬁlter. By reducing the
5
(a)
(b)
Fig. 2:
(a) setup and (b) query times for the presented algorithms.
dimensionality of the search space to 5, the average setup time
was reduced by 3.1 %, and memory consumption was lowered
by 4.6 %. Most interestingly, query times dropped by 31.1 %,
which gives a signiﬁcant advantage to repeated queries on the
same data structure.
4) Conclusion: Based on the above results, we decided to
use the kNN algorithm based on k-d trees. We have found that
the value k = 128 is sufﬁcient for our purposes. Note that this
value is conﬁgurable and can be adapted.
C. Structural Similarity
In this search stage, the CFG of the queried function and
the CFGs of the candidates from the kNN search are compared.
The CFG is an easily approachable structure and has been
shown to be a good predictor for the labeling of binaries [29]
and was hence selected as structural feature. There exist
many other potential syntactic and semantic features, e.g., the
abstract syntax tree, the decompiled source code, or semantic
approaches. However, we deem them too complex, as they
require a large amount of preparation time.
Input to the complex ﬁltering stage are the nearest neigh-
bors of the queried function. During this stage, a linear search
over all elements is conducted. We deﬁne the (intra-)procedural
control ﬂow graph as a structural representation of a function
f given by basic blocks and their connections [13]:
Gf := ({v|v is basic block in f} ,{(u, v)|u redirects to v}) .
A basic block is a sequence of machine code instructions
that has exactly one entry and one exit point. Commonly, a
redirection is a conditional jump, a function call or a return
from a function. The CFG represents the program logic of
a function and consequently should not alter substantially
over compilers and optimization options. Prior work [24, 26]
showed that features based on the CFG perform very well in
labeling functions.
1) Basic block distance: We not only rely on the structural
properties of the function. Additionally, we enrich all nodes
with several features from their respective basic blocks. First,
each node receives a label according to its topological order
in the function. Second, string references and constants are
stored. Third, each node contains its own set of robust features.
We assumed that robust features of functions closely resemble
to those of basic blocks, and hence used a similar feature set,
sans features that are not applicable at basic-block level, such
as the number of basic blocks.
For each numeric feature, we calculate the absolute differ-
ence and multiplied a weight αi to it. The resulting distance
function is:
(cid:80) αi |cif − cig|
(cid:80) αi max (cif , cig)
dBB =
with cif being numeric feature i of function f. For lists of
strings, the Jaccard distance is used as distance measure. The
term in the denominator ensures that the distance function is in
the range [0, 1]. In Section III-C4 we will discuss good choices
for αi.
2) Graph matching algorithm: Several approaches have
been described to calculate the similarity of two CFGs. A
prominent approach uses bipartite matching on the list of basic
blocks [29]. As the bipartite matching algorithm is agnostic
to the original graph structure, the algorithm was adapted to
respect neighborhood relations. One drawback of the described
method is that an erroneous matching might propagate and thus
lead to undesirable results. The distance function between two
basic blocks is the weighted sum over their respective features.
6
lllllllllllllll051015202502500005000007500001000000number of subroutinestime (seconds)typelllllkNN − hierarchical k−meanskNN − k−d trees (8)kNN − linear indexkNN (k−d trees (8)) + graphVP treelllllllllllllll020406002500005000007500001000000number of subroutinestime (ms)typelllllkNN − hierarchical k−meanskNN − k−d trees (8)kNN − linear indexkNN (k−d trees (8)) + graphVP treeA different approach is a graph similarity algorithm based
on the maximum common subgraph isomorphism problem
(MCS) [47]. It is a well-known problem to ﬁnding the largest
subgraph isomorphic to two graphs.
The traditional MCS distance function is deﬁned as
dmcs.orig(G1, G2) := 1 − |mcs (G1, G2)|
max (|G1| ,|G2|)
dmcs(G1, G2) := 1 − |mcs (G1, G2)| −(cid:80) dBB (bi, bj)
with graphs G1, G2. To account for the similarity between
two basic blocks, we extended the distance function to the
following form: