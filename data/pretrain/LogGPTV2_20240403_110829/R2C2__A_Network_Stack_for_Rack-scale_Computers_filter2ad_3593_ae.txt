allows for an intuitive analysis. At high load (L ≥0.5), a
minimal routing protocol such as RPS achieves the best per-
formance as the hop count is minimized and the utilization
is maximized. At low load, instead, a non-minimal routing
protocol such as VLB exhibits superior performance as it can
exploit the spare network capacity to increase the through-
put. We also experimented with other workloads and ob-
served qualitatively similar results.
We use the genetic algorithm-based heuristic described in
Section 3.4. For each ﬂow, we consider only two routing
protocols, random packet spraying (RPS) and VLB. Each
genotype is encoded as a bit string with each bit correspond-
ing to one ﬂow, thus leading to a global search space of up
to 2512 solutions when L = 1. We use a population size
of 100 and a mutation probability of 0.01.
In Figure 18
we plot the relative performance of our selection heuristic
named Adaptive against three baselines: one using RPS for
all ﬂows (RPS), one using VLB for all ﬂows (VLB), and one
in which each ﬂow randomly chooses either protocol (Ran-
dom). The results show that our selection process is able to
always achieve the best performance across all load values
(the relative performance is always above one). This shows
the importance of supporting per-ﬂow routing protocols and
the beneﬁts of our dynamic selection.
Comparison against a centralized design. With R2C2,
nodes can locally compute a ﬂow’s rate and routing protocol.
We also considered an alternate design where the computa-
tion is done centrally (similar to Fastpass [36]), simply by
choosing one of the rack nodes as a centralized controller.
Such a design reduces computation overhead at the expense
of greater control trafﬁc. We study this trade-off below.
Figure 19 shows the amount of control trafﬁc with a de-
centralized design and a centralized one when varying the
number of concurrent long ﬂows per server. In the decen-
tralized design, a ﬂow arrival (or departure) event is broad-
casted to all rack nodes. Instead, with a centralized design,
the source sends a unicast message to the controller, which
computes the rates and sends to each rack node sourcing a
ﬂow a different rate message with all the new rates for its
own ﬂows. This is the reason why the control trafﬁc for the
centralized design increases with the number of concurrent
ﬂows while it remains constant for the decentralized design.
562When there are only a few long ﬂows, the centralized de-
sign is more efﬁcient because only a few control messages
are generated. However, when the number of ﬂows grows,
the decentralized becomes more attractive because only the
ﬂow events are broadcasted while the rate updates are com-
puted locally. For example, when there is one concurrent
ﬂow per server, the centralized design generates 6.2x more
trafﬁc than the decentralized one (resp. 19.9x more when the
number of concurrent ﬂows per server is equal to 10).
The price paid by the decentralized design is increased
computation.
Instead of computing rates once at the con-
troller and communicating them to all nodes, the computa-
tion is done by each node. However, as shown in Section 5.1,
the rate computation overhead is acceptable.
6. DISCUSSION AND FUTURE WORK
R2C2 targets intra-rack communication, and determines
how such trafﬁc is routed and how the network fabric is
shared. Here we discuss some directions for future work.
Inter-rack networking. A key open question we have not
addressed is interconnection of multiple rack-scale comput-
ers to form a large cluster. This includes both the physical
wiring layout and the network protocols used to bridge be-
tween two rack-scale computers.
One simple option for inter-rack networking is to just use
traditional switches and tunnel R2C2 packets by encapsu-
lating them inside Ethernet frames. While this would al-
low for a smooth transition from today’s deployments, it
has some limitations. First, given the high bandwidth avail-
able within a rack, the only way to avoid creating high over-
subscription would be to use high-radix switches with large
back-plane capacity, in the order of (tens of) Terabits. This,
however, would dramatically increase costs and it may even
be infeasible if 100+ Gbps links are to be deployed within a
rack. Further, the need to bridge between R2C2 and Ethernet
would increase the overhead and the end-to-end latency. A
more promising (albeit challenging) solution instead is to di-
rectly connect multiple rack-scale computers without using
any switch, similar to [49]. Theia [47] also proposes such de-
sign with multiple parallel connections between racks. Be-
side saving the cost of the switches, this would also enable a
ﬁner-grain control over the inter-rack routing.
Reliability. Even within the context of intra-rack communi-
cation, more work is needed. R2C2 does not provide a com-
plete network transport protocol — it does not provide end-
to-end reliability and ﬂow control. While traditional mech-
anisms like end-to-end acknowledgements and checksums
can be used to achieve these, we believe R2C2’s design im-
proves the efﬁcacy of such mechanisms. For example, by de-
coupling congestion control from reliability, we ensure that
acknowledgements are used solely for reliability. This is in
contrast with TCP-like protocols that rely on ACK-clocking
to determine the fair sending rates of ﬂows. We are currently
investigating such extensions.
R2C2 atop switched networks. R2C2’s design is moti-
vated by the challenges and opportunities posed by rack-
scale computers with direct-connect topologies. However,
traditional switched topologies (with silicon photonics or
other physical technologies) are also being considered for
the intra-rack network [7, 53]. We note that it is the scale
of rack-scale computers, not the topology, that makes broad-
casting efﬁcient. For example, consider a 512 node rack con-
nected using 32-port switches arranged in a two-level folded
Clos topology. A broadcast on this topology results in only
8.7 KB of total trafﬁc. Such a topology does not have multi-
ple paths between nodes, so there is no room for route selec-
tion. However, R2C2’s congestion control still offers more
ﬂexibility over traditional distributed congestion control.
At data center scale, the broadcast overhead is high and
distributed control is more appealing. However, even at such
a scale, R2C2’s design and algorithms could be appropri-
ate if next-generation networks have more efﬁcient means
of achieving (approximate) global visibility while providing
signiﬁcant multi-pathing.
7. CONCLUSION
We presented R2C2, a network stack for rack-scale com-
puters comprising a rate-based congestion control protocol
and a ﬂexible routing mechanism. By broadcasting ﬂow
events, we ensure that rack nodes can locally compute rate
allocations and routing decisions. These decisions are en-
forced at the sources, resulting in simpliﬁed packet forward-
ing. By deploying R2C2 on an emulated rack and a (cross-
validated) simulator, we show that R2C2 can achieve good
performance across diverse network workloads, and routing
ﬂexibility can provide even more gains.
Acknowledgments
We thank Thomas Bigger, Aleksandar Dragojevi´c, Sergey
Grant, Sergey Legtchenko, Dushyanth Narayanan, Greg
O’Shea, Bozidar Radunovi´c, Michael Schapira, the anony-
mous SIGCOMM reviewers, and our shepherd George
Porter for their feedback and help.
8. REFERENCES
[1] H. Abu-Libdeh, P. Costa, A. Rowstron, G. O’Shea, and
A. Donnelly. Symbiotic Routing in Future Data Centers. In
SIGCOMM, 2010.
[2] M. Alizadeh, A. Greenberg, D. A. Maltz, J. Padhye, P. Patel,
B. Prabhakar, S. Sengupta, and M. Sridharan. Data Center
TCP (DCTCP). In SIGCOMM, 2010.
[3] M. Alizadeh, A. Kabbani, T. Edsall, B. Prabhakar, A. Vahdat,
and M. Yasuda. Less Is More: Trading a Little Bandwidth for
Ultra-Low Latency in the Data Center. In NSDI, 2012.
[4] M. Alizadeh, S. Yang, M. Sharif, S. Katti, N. McKeown,
B. Prabhakar, and S. Shenker. pFabric: Minimal
Near-optimal Datacenter Transport. In SIGCOMM, 2013.
[5] J. M. andJeff Shamma. Revisiting log-linear learning:
Asynchrony, completeness and payoff-based
implementation. Games and Economic Behavior, 2012.
[6] S. Angel, H. Ballani, T. Karagiannis, G. O’Shea, and
E. Thereska. End-to-end Performance Isolation through
Virtual Datacenters . In OSDI, 2014.
[7] K. Asanovic. FireBox: A Hardware Building Block for 2020
Warehouse-Scale Computers. In FAST, 2014. Keynote.
[8] B. Awerbuch, R. Khandekar, and S. Rao. Distributed
Algorithms for Multicommodity Flow Problems via
563Approximate Steepest Descent Framework. ACM Trans.
Algorithms, 9(1), 2012.
[9] S. Balakrishnan, R. Black, A. Donnelly, P. England,
A. Glass, D. Harper, S. Legtchenko, A. Ogus, E. Peterson,
and A. Rowstron. Pelican: A Building Block for Exascale
Cold Data Storage. In OSDI, 2014.
[10] H. Ballani, P. Costa, T. Karagiannis, and A. Rowstron.
Towards Predictable Datacenter Networks. In SIGCOMM,
2011.
[11] H. Ballani, K. Jang, T. Karagiannis, C. Kim,
D. Gunawardena, and G. O’Shea. Chatty Tenants and the
Cloud Network Sharing Problem. In NSDI, 2013.
[12] D. Bertsekas and R. Gallager. Data Networks. Prentice Hall,
1987.
[13] D. Bertsimas and J. Tsitsiklis. Simulated Annealing.
Statistical Science, 8(1), 1993.
[14] R. S. Cahn. Wide Area Network Design: Concepts and Tools
for Optimization. Morgan Kaufmann, 1998.
[15] M. Chowdhury and I. Stoica. Coﬂow: A Networking
Abstraction for Cluster Applications. In HotNets, 2012.
[16] P. Costa, H. Ballani, and D. Narayanan. Rethinking the
Network Stack for Rack-scale Computers. In HotCloud,
2014.
[17] Cray Inc. Modifying Your Application to Avoid Aries
Network Congestion, 2013.
[18] Cray Inc. Network Resiliency for Cray XC30 Systems, 2013.
[19] A. Daglis, S. Novakovi´c, E. Bugnion, B. Falsaﬁ, and B. Grot.
Manycore Network Interfaces for In-memory Rack-scale
Computing. In ISCA, 2015.
[20] W. Dally and B. Towles. Principles and Practices of
Interconnection Networks. Morgan Kaufmann, 2003.
[21] J. Dean and L. A. Barroso. The Tail at Scale.
Communications of ACM, 2013.
[22] A. A. Dixit, P. Prakash, Y. C. Hu, and R. R. Kompella. On
the Impact of Packet Spraying in Data Center Networks. In
INFOCOM, 2013.
[23] F. R. Dogar, T. Karagiannis, H. Ballani, and A. Rowstron.
Decentralized Task-aware Scheduling for Data Center
Networks. In SIGCOMM, 2014.
[24] A. Dragojevi´c, D. Narayanan, O. Hodson, and M. Castro.
FaRM: Fast Remote Memory. In NSDI, 2014.
[25] A. Greenberg, J. R. Hamilton, N. Jain, S. Kandula, C. Kim,
P. Lahiri, D. A. Maltz, P. Patel, and S. Sengupta. VL2: A
Scalable and Flexible Data Center Network. In SIGCOMM,
2009.
[26] S. Han, N. Egi, A. Panda, S. Ratnasamy, G. Shi, and
S. Shenker. Network Support for Resource Disaggregation in
Next-generation Datacenters. In HotNets, 2013.
[27] J. H. Holland. Adaptation in Natural and Artiﬁcial Systems.
University of Michigan Press, 1975.
[28] C.-Y. Hong, M. Caesar, and P. B. Godfrey. Finishing ﬂows
quickly with preemptive scheduling. In SIGCOMM, 2012.
[29] K. Jang, J. Sherry, H. Ballani, and T. Moncaster. Silo:
Predictable Message Latency in the Cloud. In SIGCOMM,
2015.
[30] V. Jeyakumar, M. Alizadeh, D. Mazières, B. Prabhakar,
C. Kim, and A. Greenberg. EyeQ: Practical Network
Performance Isolation at the Edge. In NSDI, 2013.
[31] A. Kalia, M. Kaminsky, and D. G. Andersen. Using RDMA
Efﬁciently for Key-value Services. In SIGCOMM, 2014.
[32] S. Kandula, S. Sengupta, A. Greenberg, P. Patel, and
R. Chaiken. The nature of data center trafﬁc: measurements
& analysis. In IMC, 2009.
[33] D. Nace, N.-L. Doan, E. Gourdin, and B. Liau. Computing
Optimal Max-min Fair Resource Allocation for Elastic
Flows. IEEE/ACM Trans. Netw., 14(6), 2006.
[34] S. Novakovic, A. Daglis, E. Bugnion, B. Falsaﬁ, and B. Grot.
Scale-out NUMA. In ASPLOS, 2014.
[35] G. P. Nychis, C. Fallin, T. Moscibroda, O. Mutlu, and
S. Seshan. On-chip Networks from a Networking
Perspective: Congestion and Scalability in Many-core
Interconnects. In SIGCOMM, 2012.
[36] J. Perry, A. Ousterhout, H. Balakrishnan, D. Shah, and
H. Fugal. Fastpass: A Centralized "Zero-queue" Datacenter
Network. In SIGCOMM, 2014.
[37] L. Popa, G. Kumar, M. Chowdhury, A. Krishnamurthy,
S. Ratnasamy, and I. Stoica. FairCloud: Sharing the Network
in Cloud Computing. In SIGCOMM, 2012.
[38] A. Putnam, A. Caulﬁeld, E. Chung, D. Chiou,
K. Constantinides, J. Demme, H. Esmaeilzadeh, J. Fowers,
G. P. Gopal, J. Gray, M. Haselman, S. Hauck, S. Heil,
A. Hormati, J.-Y. Kim, S. Lanka, J. Larus, E. Peterson,
S. Pope, A. Smith, J. Thong, P. Y. Xiao, and D. Burger. A
Reconﬁgurable Fabric for Accelerating Large-Scale
Datacenter Services. In ISCA, 2014.
[39] S. Radhakrishnan, Y. Geng, V. Jeyakumar, A. Kabbani,
G. Porter, and A. Vahdat. SENIC: Scalable NIC for
End-Host Rate Limiting. In NSDI, 2014.
[40] B. Radunovi´c and J.-Y. L. Boudec. A Uniﬁed Framework for
Max-min and Min-max Fairness with Applications.
IEEE/ACM Trans. Netw., 15(5), 2007.
[41] C. Raiciu, S. Barre, C. Pluntke, A. Greenhalgh, D. Wischik,
and M. Handley. Improving Datacenter Performance and
Robustness with Multipath TCP. In SIGCOMM, 2011.
[42] T. Roughgarden and E. Tardos. How Bad is Selﬁsh Routing?
J. ACM, 2002.
[43] B. Schroeder and G. A. Gibson. Understanding Failures in
Petascale Computers. Journal of Physics, 78, 2007.
[44] A. Singh, W. J. Dally, B. Towles, and A. K. Gupta.
Locality-preserving Randomized Oblivious Routing on
Torus Networks. In SPAA, 2002.
[45] L. G. Valiant and G. J. Brebner. Universal Schemes for
Parallel Communication. In STOC, 1981.
[46] B. Vamanan, J. Hasan, and T. N. Vijaykumar.
Deadline-Aware Datacenter TCP (D2TCP). In SIGCOMM,
2012.
[47] M. Walraed-Sullivan, J. Padhye, and D. A. Maltz. Theia:
Simple and Cheap Networking for Ultra-Dense Data
Centers. In HotNets, 2014.
[48] C. Wilson, H. Ballani, T. Karagiannis, and A. Rowtron.
Better Never Than Late: Meeting Deadlines in Datacenter
Networks. In SIGCOMM, 2011.
[49] H. Wu, G. Lu, D. Li, C. Guo, and Y. Zhang. MDCube: A
High Performance Network Structure for Modular Data
Center Interconnection. In CoNEXT, 2009.
[50] Amazon joins other web giants trying to design its own
chips. http://bit.ly/1J5t0fE.
[51] Boston Viridis Data Sheet. http://bit.ly/1fBnsQ9.
[52] Calxeda EnergyCore ECX-1000. http://bit.ly/1nCgdHO.
[53] Design Guide for Photonic Architecture.
http://bit.ly/NYpT1h.
[54] Google Ramps Up Chip Design. http://ubm.io/1iQooNe.
[55] How Microsoft Designs its Cloud-Scale Servers.
http://bit.ly/1HKCy27.
[56] HP Moonshot System. http://bit.ly/1mZD4yJ.
[57] Intel Atom Processor D510. http://intel.ly/1wJmS3D.
[58] Intel, Facebook Collaborate on Future Data Center Rack
Technologies. http://intel.ly/MRpOM0.
[59] Intel Rack Scale Architecture. http://ubm.io/1iejjx5.
[60] Maze: A Rack-scale Computer Emulation Platform.
http://aka.ms/maze.
[61] RDMA Aware Networks Programming User Manual.
http://bit.ly/1ysVa1O.
[62] SeaMicro SM15000 Fabric Compute Systems.
http://bit.ly/1hQepIh.
564