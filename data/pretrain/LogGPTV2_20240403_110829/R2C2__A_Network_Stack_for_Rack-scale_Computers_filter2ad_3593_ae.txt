### Intuitive Analysis and Routing Protocol Performance

At high load (L ≥ 0.5), a minimal routing protocol such as Random Packet Spraying (RPS) achieves the best performance, as it minimizes hop count and maximizes utilization. Conversely, at low load, a non-minimal routing protocol like Valiant Load Balancing (VLB) exhibits superior performance by leveraging spare network capacity to increase throughput. Our experiments with various workloads have yielded qualitatively similar results.

### Genetic Algorithm-Based Heuristic

We employ a genetic algorithm-based heuristic, as described in Section 3.4. For each flow, we consider only two routing protocols: RPS and VLB. Each genotype is encoded as a bit string, with each bit corresponding to one flow. This leads to a global search space of up to \(2^{512}\) solutions when L = 1. We use a population size of 100 and a mutation probability of 0.01.

### Performance Comparison

In Figure 18, we compare the relative performance of our selection heuristic, named Adaptive, against three baselines: one using RPS for all flows (RPS), one using VLB for all flows (VLB), and one where each flow randomly chooses either protocol (Random). The results show that our selection process consistently outperforms the baselines across all load values, as indicated by a relative performance always above one. This underscores the importance of supporting per-flow routing protocols and the benefits of our dynamic selection approach.

### Decentralized vs. Centralized Design

With R2C2, nodes can locally compute a flow's rate and routing protocol. We also considered an alternate design where the computation is done centrally, similar to Fastpass [36], by selecting one rack node as a centralized controller. This design reduces computational overhead but increases control traffic. We explore this trade-off below.

Figure 19 illustrates the amount of control traffic in both decentralized and centralized designs as the number of concurrent long flows per server varies. In the decentralized design, flow arrival or departure events are broadcasted to all rack nodes. In contrast, the centralized design involves the source sending a unicast message to the controller, which computes the rates and sends individual rate messages to each rack node sourcing a flow. This explains why the control traffic for the centralized design increases with the number of concurrent flows, while it remains constant in the decentralized design.

For a small number of long flows, the centralized design is more efficient due to the generation of fewer control messages. However, as the number of flows grows, the decentralized design becomes more attractive because only flow events are broadcasted, and rate updates are computed locally. For example, with one concurrent flow per server, the centralized design generates 6.2x more traffic than the decentralized design (and 19.9x more when the number of concurrent flows per server is 10).

The decentralized design incurs increased computation, as each node must compute rates independently. However, as shown in Section 5.1, the computational overhead is acceptable.

### Discussion and Future Work

R2C2 targets intra-rack communication, determining how traffic is routed and how the network fabric is shared. Here, we discuss potential directions for future work.

#### Inter-Rack Networking

A key open question is the interconnection of multiple rack-scale computers to form a large cluster, including both physical wiring layout and network protocols. One simple option is to use traditional switches and tunnel R2C2 packets within Ethernet frames. While this allows for a smooth transition from current deployments, it has limitations. Given the high bandwidth available within a rack, avoiding high over-subscription would require high-radix switches with large backplane capacity, increasing costs and potentially being infeasible for 100+ Gbps links. Additionally, bridging between R2C2 and Ethernet would increase overhead and end-to-end latency.

A more promising, albeit challenging, solution is to directly connect multiple rack-scale computers without using any switch, similar to [49]. Theia [47] proposes such a design with multiple parallel connections between racks. This approach not only saves on switch costs but also enables finer-grain control over inter-rack routing.

#### Reliability

Within the context of intra-rack communication, further work is needed. R2C2 does not provide a complete network transport protocol, lacking end-to-end reliability and flow control. Traditional mechanisms like end-to-end acknowledgments and checksums can be used, but R2C2’s design improves their efficacy. By decoupling congestion control from reliability, acknowledgments are used solely for reliability, unlike TCP-like protocols that rely on ACK-clocking to determine fair sending rates. We are currently investigating such extensions.

#### R2C2 on Switched Networks

R2C2’s design is motivated by the challenges and opportunities of rack-scale computers with direct-connect topologies. However, traditional switched topologies (with silicon photonics or other technologies) are also being considered for intra-rack networks [7, 53]. It is the scale of rack-scale computers, not the topology, that makes broadcasting efficient. For example, a 512-node rack connected using 32-port switches in a two-level folded Clos topology results in only 8.7 KB of total traffic for a broadcast. Such a topology lacks multiple paths between nodes, limiting route selection. However, R2C2’s congestion control still offers more flexibility over traditional distributed congestion control.

At data center scale, the broadcast overhead is high, making distributed control more appealing. Even at such a scale, R2C2’s design and algorithms could be appropriate if next-generation networks provide more efficient means of achieving (approximate) global visibility and significant multi-pathing.

### Conclusion

We presented R2C2, a network stack for rack-scale computers comprising a rate-based congestion control protocol and a flexible routing mechanism. By broadcasting flow events, rack nodes can locally compute rate allocations and routing decisions, simplifying packet forwarding. Deploying R2C2 on an emulated rack and a (cross-validated) simulator, we demonstrated that R2C2 can achieve good performance across diverse network workloads, with routing flexibility providing additional gains.

### Acknowledgments

We thank Thomas Bigger, Aleksandar Dragojević, Sergey Grant, Sergey Legtchenko, Dushyanth Narayanan, Greg O’Shea, Bozidar Radunović, Michael Schapira, the anonymous SIGCOMM reviewers, and our shepherd George Porter for their feedback and help.

### References

[1] H. Abu-Libdeh, P. Costa, A. Rowstron, G. O’Shea, and A. Donnelly. Symbiotic Routing in Future Data Centers. In SIGCOMM, 2010.
...
[62] SeaMicro SM15000 Fabric Compute Systems. http://bit.ly/1hQepIh.