title:A Contribution Towards Solving the Web Workload Puzzle
author:Katerina Goseva-Popstojanova and
Fengbin Li and
Xuan Wang and
Amit Sangle
A Contribution Towards Solving the Web Workload Puzzle
Katerina Goˇseva-Popstojanova, Fengbin Li, Xuan Wang, and Amit Sangle
Lane Department of Computer Science and Electrical Engineering
{katerina, fengbinl, xwang, sangle}@csee.wvu.edu
West Virginia University, Morgantown, WV 26506-6109
Abstract
World Wide Web, the biggest distributed system ever built,
experiences tremendous growth and change in Web sites, users,
and technology. A realistic and accurate characterization of Web
workload is the ﬁrst, fundamental step in areas such as per-
formance analysis and prediction, capacity planning, and ad-
mission control. Compared to the previous work, in this pa-
per we present more detailed and rigorous statistical analysis of
both request and session level characteristics of Web workload
based on empirical data extracted from actual logs of four Web
servers. Our analysis is focused on exploring phenomena such as
self-similarity, long-range dependence, and heavy-tailed distri-
butions. Identiﬁcation of these phenomena in real data is a chal-
lenging task since the existing methods may perform erratically
in practice and produce misleading results. We provide more ac-
curate analysis of long-range dependence of the request and ses-
sion arrival processes by removing the trend and periodicity. In
addition to the session arrival process (i.e., inter-session char-
acteristics), we study several intra-session characteristics using
several different methods to test the existence of heavy-tailed be-
havior and cross validate the results. Finally, we point out spe-
ciﬁc problems associated with the methods used for establishing
long-range dependence and heavy-tailed behavior of Web work-
loads. We believe that the comprehensive model presented in this
paper is a step towards solving the Web workload puzzle.
1 Introduction
The growing availability of Internet access has led to enor-
mous increase in the use of World Wide Web which has become
the biggest distributed system ever built. Users increasingly see
large–scale Web services as essential to the world’s communi-
cation infrastructure and demand 24/7 availability and response
time within seconds. With the tremendous growth and change in
Web sites, users, and technology, expanding usage in different ap-
plication domains, and high consequences of failures and unsat-
isfactory performance, a comprehensive analysis and prediction
of Web quality attributes is essential.
Understanding the nature and characteristics of Web workload
is a precondition for proper design, implementation, and tuning
of Web–based systems which lead to improved quality of ser-
vice offered to users. Therefore, in the last decade a considerable
amount of research work was focused on studying the network
trafﬁc in general, and Web trafﬁc in particular. In their pioneer-
ing work, Leland, Taqqu, Willinger and Wilson [18] established
that Ethernet LAN trafﬁc is self–similar in nature and showed that
the degree of self–similarity increases with the trafﬁc intensity.
Following these breakthrough results, the WAN trafﬁc was stud-
ied in [22]. In this study, the authors presented a complete model
for TELNET trafﬁc (FULL-TEL), which uses Poisson connection
arrivals, log–normal connection sizes, and Tcplib packet inter–
arrivals. In [28] the authors suggested that the superposition of
many ON/OFF sources whose ON and OFF periods are modelled
with heavy–tailed distributions produce aggregate network trafﬁc
which is self–similar or long–range dependant in nature.
The analysis of the Web trafﬁc at request level presented in
in [7] showed that the busiest hours are well described as self–
similar, while many less busy hours do not show self–similar
characteristics. Another study of Web trafﬁc at request level [2],
based on the access logs from six Web servers, showed that the
ﬁle size and transfer size distributions are heavy–tailed.
A unique characteristic of Web workload is the concept of
session which is deﬁned as a sequence of requests from the same
user during a single visit to the Web site; session boundaries are
delimited by a period of inactivity by a user. In [5] authors pro-
posed a session–based admission control aimed at increasing the
chances that longer sessions will be completed. In [3] authors
studied how the threshold value affects the number of sessions
and focused on other session characteristics such as the num-
ber of requests per session, session length, and inter–session ar-
rival times. The work presented in [19] used Customer Behavior
Model Graph (CBMG) to represent Web sessions. As a continua-
tion of this work, priority–based resource management policies
based on CBMG representation and simulated workload were
proposed in [20]. The work presented in [21] studied the request,
function, and session characteristics of two weeks of data from
two actual e–commerce sites.
Next, we summarize a few papers which raised interesting
questions about the methods used to establish the existence of
self–similarity, long–range dependence, and heavy–tailed distri-
butions applied to Web and other types of network trafﬁc.
In
[13] it was shown that the methods for estimating self–similarity
and long–range dependence could give conﬂicting results. Fur-
thermore, it was shown that the trend, periodicity, and noise may
affect the accuracy and consistency of the Hurst exponent esti-
mations.
In a closely related papers [15], [16], the study of a
network backbone trafﬁc showed that the packet arrivals appear
to be Poisson at sub-second time scales, non–stationary at multi–
second time scales, and exhibit long–range dependence at scales
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
of seconds and above. In [9] it was suggested that the methods
employed for estimating the index of heavy–tailed distributions
could produce misleading results. Thus, it was shown that the
lognormal distribution, which is not heavy–tailed, may result in
a log-log complimentary distribution (LLCD) plot which appears
to be heavy–tailed.
In our earlier work [11], [12] we introduced several inter–
session and intra–session characteristics which collectively de-
scribe Web workload in terms of sessions.
In this paper we
present more detailed and rigorous statistical analysis of Web
workloads based on empirical data extracted from the actual logs
of four Web servers. Similarly to the work presented in [22],
which proposed so called FULL-TEL model for describing the
TELNET trafﬁc, we conduct statistical analysis of the request–
based and session–based attributes of Web trafﬁc aimed at build-
ing FULL-Web model. Speciﬁcally, for typical intervals with low,
medium, and high workload and for one week, we analyze the
following characteristics of the Web workloads:
• request–based analysis: number of requests per unit of time
and request inter–arrival time
• session–based analysis:
– inter–session characteristics:
sessions initiated per
unit of time and time between sessions initiated
– intra–session characteristics: session length in time,
number of request per session, and number of bytes
transferred per session.
Our analysis is focused on exploring important phenomena,
such as self–similarity, long–range dependence, and heavy–tailed
distributions. As it has been observed recently [9], [13], [16],
despite almost ten years of history of using these phenomena to
model the Internet trafﬁc, their identiﬁcation in real data is a chal-
lenging task since the existing methods may perform erratically
in practice and produce misleading results. Therefore,
• For establishing self–similarity and long–range dependence
on request and session level we test the stationarity of the
request and session-based time series and remove the trend
and periodicity. Then, we use several methods for estimating
the Hurst exponent.
• For intra–session characteristics we use several different
methods to test the existence of heavy–tailed behavior and
cross validate the results.
• We point out speciﬁc problems associated with the methods
used for establishing long–range dependence and heavy–
tailed behavior of Web workload.
It should be emphasized that the previous research on statis-
tical characterization of Web workloads was focused only on re-
quest level [7] or very limited non-rigorous analysis of only one
session characteristic - session length in number of requests [21].
We believe that the comprehensive model presented in this paper
contributes towards better understanding and more formal statis-
tical description of the Web workloads, which is a fundamental
step necessary for performance modelling and prediction, capac-
ity planning, and admission control.
The rest of the paper is organized as follows. The data extrac-
tion and analysis process is brieﬂy described in section 2, while
the background on self–similarity, long–range dependence, and
heavy–tailed distributions is summarized in section 3. We present
the analysis of Web workload at request and session level in sec-
tions 4 and 5, respectively. Finally, the concluding remarks are
given in section 6.
2 Data extraction and analysis process
The Web logs used in this paper were obtained from four Web
servers: university wide Web server at West Virginia University
(WVU), Web server of the Lane Department of Computer Sci-
ence and Electrical Engineering (CSEE), Web server of the com-
mercial Internet provider ClarkNet, and Web server at the NASA
Independent Veriﬁcation and Validation Facility (NASA-Pub2)1.
In this paper, for practical reasons, we deﬁne a session as a
sequence of requests issued from the same IP address with the
time between requests less than some threshold value. As in all
other research papers that considered sessions, we consider each
unique IP address in the access log to be a distinct user. Clearly,
this is not always true [3]. However, in spite of the inaccuracies,
we believe that using the IP address provides a reasonable ap-
proximation of the number of distinct users. Based on the study
of the effect of different threshold values on the total number of
session presented in [12], we adopt a 30 minute time interval as
the threshold value.
The data collection and analysis process is summarized in
Figure 1. (For more detailed information, the reader is referred
to [11], [12].) After merging the access and error logs for ar-
chitectures that employ redundant Web servers (i.e., WVU and
CSEE), we include the log entries from the access and error
logs as records in the corresponding database tables, which al-
lows more ﬂexible and customized analysis. In our earlier work
[11], [12] we presented detailed error and reliability analysis
and introduced several intra–session and inter–session attributes
which collectively describe Web server sessions.
In this paper
we provide more detailed and statistically rigorous analysis of
both request–based and session–based workload characteristics
(the bottom right part in Figure 1).
Table 1 summarizes the raw data for one week period for the
Web servers analyzed in this paper. It should be noted that the
workload on different servers varies by three orders of magni-
tude. Also, the servers are from different domains:
two from
educational institutions, one from research institution, and one
from a commercial Web site. In addition to the analysis of one
week of data, our goal is to study the effect of the workload in-
tensity on the request–based and session–based characteristics.
For this purpose, we divided the one week period into 42 inter-
vals of 4 hours and for each data set selected typical low (Low),
medium (Med), and high (High) intervals using the total num-
ber of requests as a criterium. Although we select the intervals
accordingly to the total number of requests, the total number of
sessions and total number of bytes transferred within these inter-
vals adhere the same trend.
1The Web logs of the NASA IV&V server were sanitized, that is, IP addresses
were replaced with unique identiﬁers.
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
the degree of self–similarity increases. Calculating this exponent,
however, is not straightforward due to following reasons [13],
[16]. (1) It cannot be calculated deﬁnitely, only estimated. (2)
No estimator is robust in every case and it is not clear which esti-
mator provides the most accurate estimation; estimators can hide
long–range dependence or report it erroneously. (3) Long–range
dependence may exists, even if the estimators have different esti-
mates in value, provided that the estimates show 0.5  x] = x−αL(x)
is
slowly varying as x → ∞,
where L(x)
i.e.,
limx→∞ L(ax)/L(x) = 1 for a > 0 [24]. That is, regard-
less of the behavior for small values of the random variable,
if the asymptotic shape of the distribution is hyperbolic, it is
heavy–tailed.
The simplest heavy–tailed distribution is the
Pareto distribution which is hyperbolic over its entire range. The
classical Pareto distribution with shape parameter α and location
parameter k has the cumulative distribution function
F (x) = P [X ≤ x] = 1 − (k/x)α.
(4)
There is an important qualitative property of the moments of
heavy–tailed distributions. If X is heavy–tailed with parameter
α then its ﬁrst m  x] = 1 − F (x) = ¯F (x) on
log-log axes. Plotted this way, heavy–tailed distributions have the
property that
d log ¯F (x)
d log x = −α,
x > θ
for some θ. In practice, we select a value for θ from the LLCD
plot above which the plot appears to be linear. Then, we estimate
the slope, which is equal to −α, using least–square regression.
Figure 1. Data extraction and analysis process
Data set
Start date
Requests
Sessions
WVU
ClarkNet
CSEE
NASA-Pub2
12-Jan-04
28-Aug-95
12-Apr-04