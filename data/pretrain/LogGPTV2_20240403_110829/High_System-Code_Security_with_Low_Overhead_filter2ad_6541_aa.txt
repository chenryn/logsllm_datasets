title:High System-Code Security with Low Overhead
author:Jonas Wagner and
Volodymyr Kuznetsov and
George Candea and
Johannes Kinder
2015 IEEE Symposium on Security and Privacy
2015 IEEE Symposium on Security and Privacy
High System-Code Security with Low Overhead
Jonas Wagner∗, Volodymyr Kuznetsov∗, George Candea∗, and Johannes Kinder†
∗School of Computer and Communication Sciences
École Polytechnique Fédérale de Lausanne
†Department of Computer Science
Royal Holloway, University of London
Abstract—Security vulnerabilities plague modern systems be-
cause writing secure systems code is hard. Promising approaches
can retroﬁt security automatically via runtime checks that
implement the desired security policy; these checks guard critical
operations, like memory accesses. Alas, the induced slowdown
usually exceeds by a wide margin what system users are willing
to tolerate in production, so these tools are hardly ever used. As
a result, the insecurity of real-world systems persists.
We present an approach in which developers/operators can
specify what level of overhead they ﬁnd acceptable for a given
workload (e.g., 5%); our proposed tool ASAP then automatically
instruments the program to maximize its security while staying
within the speciﬁed “overhead budget.” Two insights make this
approach effective: most overhead in existing tools is due to only
a few “hot” checks, whereas the checks most useful to security
are typically “cold” and cheap.
We evaluate ASAP on programs from the Phoronix and SPEC
benchmark suites. It can precisely select the best points in the
security-performance spectrum. Moreover, we analyzed existing
bugs and security vulnerabilities in RIPE, OpenSSL, and the
Python interpreter, and found that the protection level offered
by the ASAP approach is sufﬁcient to protect against all of them.
I. INTRODUCTION
System builders routinely need to satisfy conﬂicting de-
mands of performance, productivity, and security. A lot of
systems code is written in unsafe languages,
like C/C++,
because they have low runtime overhead, they enable low-level
access to hardware, and because they are sometimes the only
way to use legacy libraries or tool chains. The drawback is
that unsafe languages burden the programmer with managing
memory and avoiding the many behaviors left undeﬁned by the
language speciﬁcations. This makes it especially hard to write
secure software; but the security of the entire software stack
depends on input parsers, language runtimes, cryptographic
routines, web browsers, OS kernels, and hypervisors written in
these languages. The quest for performance and low memory
consumption can often compromise safety, and even extensive
test suites and the use of tools like Valgrind still leave holes
in the code. It is thus not surprising that buffer overﬂows are
still the #1 vulnerability exploited by attackers [26] and that
new ones have been revealed to take control of browsers and
OSs in every edition of the Pwn2Own contest [28] since 2007.
Developers do have the option to employ techniques for
“retroﬁtting” security and safety into their software. Tools
like AddressSanitizer [32], StackGuard [9], SoftBound [25],
WIT [3], SafeCode [10], UndeﬁnedBehaviorSanitizer [8],
Code-Pointer Integrity [20] etc. insert sanity checks into the
© 2015, Jonas Wagner. Under license to IEEE.
© 2015, Jonas Wagner. Under license to IEEE.
DOI 10.1109/SP.2015.58
DOI 10.1109/SP.2015.58
866
866
This is a pity, because program instrumentation can often
be made elastic. Instrumentation tools introduce many small,
independent checks. By carefully selecting which checks to
use, developers could control the overhead and trade some
security to satisfy their performance constraints. Except that
developers lack a principled way to reason about the impact
of checks and choose the most effective ones.
We introduce ASAP, the ﬁrst fully-automated approach for
instrumenting programs subject to performance constraints. It
allows developers to specify an overhead budget, and then
automatically proﬁles and selects checks such as to build a
program that is as secure as possible for the given budget. With
ASAP, developers can precisely choose the optimal point in
the security-performance trade-off.
code to verify at run-time that desired safety properties hold.
These checks might verify that array indices are in bounds, that
arithmetic operations do not overﬂow, or that data structure
invariants hold. If a sanity check fails, it typically is unre-
coverable, and the program is aborted. Other than that, sanity
checks do not affect the program state.
Unfortunately, such approaches are hardly ever used in
production because of their overhead. The introduced san-
ity checks slow down the program and completely erase
the performance gains that come from low-level languages.
Programmers today are faced with a binary choice: fast and
insecure, or slow and safe.
It is often possible to obtain high security at low overhead,
for two reasons: First, the checks that are most important for
security are checks guarding obscure, untested, buggy code
where protection is most needed. Because this code is typically
cold, the checks are rarely executed and do not contribute
much to the overhead. Second, most of the induced overhead
comes from only few expensive checks located inside hot
loops. These checks are executed over and over again, burning
cycles while likely not adding much to the program’s security.
ASAP allocates the ﬁxed overhead budget to checks in cold
parts of the program to maximize security. We found that this
approach works particularly well for CPU-intensive tasks such
as parsing input, encoding or decoding data, or performing
cryptography. In these cases, ASAP can select 87% of the
available sanity checks on average, while keeping the aggre-
gate overhead below 5%, which is an order of magnitude lower
than existing solutions. Because these tasks often process
untrusted data, we believe that the ASAP approach enables
real security beneﬁts in today’s production environments.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:04:49 UTC from IEEE Xplore.  Restrictions apply. 
ASAP quantiﬁes the resulting security by computing the
sanity level. This is the fraction of potentially vulnerable
program instructions (e.g., memory accesses) that is protected
by a sanity check. Our experiments provide evidence that the
sanity level is a lower bound on the fraction of vulnerabilities
or bugs that will be detected by sanity checks. This lower
bound holds because bug density is higher in cold code than
in hot code, as substantiated by our study of bugs in the
Python interpreter and by security vulnerabilities from the
CVE database (§VI).
We built a prototype of the ASAP approach based on the
LLVM compiler framework [21]. It supports checks inserted
by AddressSanitizer, UndeﬁnedBehaviorSanitizer, and Soft-
Bound. Like these tools, ASAP works at the LLVM inter-
mediate representation level. We have tested it on a number
of C/C++ programs such as the Python interpreter, OpenSSL,
Apache, NGINX, and the Phoronix and SPEC benchmarks.
ASAP uses a proﬁling workload to measure which checks are
most expensive. For best performance, users should choose
a proﬁling workload that is close enough to the production
workload to identify all expensive checks; we found that
using a program’s test suite often works well. The process of
compiling, proﬁling, and check selection is fully automated.
Not all programs and instrumentation tools work well with
ASAP. In some cases, a large fraction of the overhead is
not due to security checks themselves, but comes from other
sources like changes to the memory allocator, increased thread
startup time, or maintaining metadata required by checks.
We call
this overhead residual overhead. We discuss its
causes in §III, and also give design principles to make future
instrumentation tools more elastic.
ASAP can be used today to increase the security of software
we run in production. It identiﬁes cases where a surprising
amount of security can be gained for a very low price. We
will make the ASAP source code publicly available. We
hope this leads to a world without security vulnerabilities like
Heartbleed: with ASAP, the Heartbleed vulnerability would
have been avoided with only 5% reduction in web server
throughput.
This paper makes the following main contributions:
• We show that program instrumentation can be made elas-
tic, so that users can choose how much to pay for the se-
curity they need. Our tool ASAP is a practical, automated
way to navigate the security vs. performance trade-off,
reducing the entry barrier for applying instrumentation-
based security mechanisms to systems code.
• We study existing bugs and security vulnerabilities (in
Python and CVEs for open source software) and show
that about 95% lie in cold code, where protection is
cheap.
• We show that, in practice, a protection level comparable
to that of the strongest tools for retroﬁtting language
security can be achieved at a fraction of the overhead.
The rest of the paper provides background information
and discusses related work (§II), describes the design of
ASAP (§IV) and our ASAP prototype (§V), evaluates ASAP
867867
on several benchmarks (§VI), discusses multiple extensions
(§VII), and concludes (§VIII).
II. BACKGROUND AND RELATED WORK
There are many aspects to the security of software systems
and many ways to improve it. We focus on sanity checks,
which verify safety properties at runtime. These properties
may relate to undeﬁned behavior in low-level languages or
just generally to invalid states in the application logic. The
distinguishing factor of a sanity check is that once it fails, the
only safe option is to abort the program, because continuing
the execution would likely lead to dangerous undeﬁned behav-
ior. We give a formal description of sanity checks in §IV-C.
Sanity checks are used in various forms; in the following,
we present prominent examples of sanity checks, several of
which address shortcomings of the security of low-level code
by guarding various types of critical operations.
Data Execution Prevention. A widely deployed sanity
check supported by hardware is Data Execution Prevention
(DEP) [22]. In a DEP-protected system, a process’s data pages
are marked non-executable in the system page table. The CPU
raises a hardware exception if an instruction from such a page
is about to be executed. This thwarts attacks that write attacker-
supplied code (so-called shellcode) to the process’s memory
and redirect execution to this code. DEP has to manage the
executable ﬂag, which is essentially a form of annotation
or metadata, and it requires a (hardware) check before each
critical operation, i.e., before executing an instruction. For less
than 1% overhead, DEP protects against basic forms of remote
code execution attacks, but it can be circumvented relatively
easily using more elaborate attacks based on “return-oriented
programming” [35].
Assertions. Developer-provided assertions are the most
common type of sanity check in code. C. A. R. Hoare reported
in 2002 that over 250,000 assertions are present in Microsoft
Ofﬁce [13]. Assertions incur runtime overhead, so they are
often disabled in production. To meet the conﬂicting demands
of safety and performance, some projects have found manual
ways to enable only “cheap” assertions. For example, the
LLVM project builds with assertions by default, but has an
XDEBUG preprocessor ﬂag to enable additional, more expen-
sive assertions.
Many safety properties are better enforced mechanically by
tools, instead of manually by the programmer. Tools have
the advantage that they can guard all critical operations with
sanity checks; for instance, a tool can automatically insert a
check before every arithmetic operation to verify that there
is no overﬂow. This rules out entire categories of bugs and
vulnerabilities. Other than the fact
they are inserted
automatically and not visible in source code, the structure and
effect of such checks is similar to assertions.
that
Undeﬁned Behavior Checks.
UndeﬁnedBehaviorSani-
tizer [8] (UBSan) instruments a program with checks that
ensure the absence of various operations that are undeﬁned
in the C/C++ language standards, and thus generally unsafe
to use. UBSan catches NULL pointer dereferences, unaligned
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:04:49 UTC from IEEE Xplore.  Restrictions apply. 
Safety Tool
DEP
Low
Stack Canaries
High WIT
CPI
SAFECode
ASan
UBSan
SoftBound/CETS
Full
Overhead ASAP
<1%
<1%
7%
8%
10%
73%
71%
116%
(cid:2)
(cid:2)
(cid:2)
Fig. 1. Automatic solutions to enforce program safety, classiﬁed according
to the strength of their safety guarantees. A check mark in the last column
indicates that our current ASAP prototype includes support for the tool.
memory accesses, signed integer overﬂows, out of bound bit
shifts, etc. These problems are less frequently exploited by
attackers than memory errors. Yet they can lead to security
vulnerabilities, e.g., when an integer overﬂow occurs while
computing the size of an object, and thus the wrong amount of
memory is allocated. Wang et al. [36] found that compilers can
transform undeﬁned behavior into security vulnerabilities by
“optimizing away” security-critical code. Checks inserted by
UBSan are stateless and do not require metadata. We measured
their overhead to be 71% on average on the SPEC CPU 2006
benchmarks.
Stack Canaries. Another widely used application security
mechanism are stack canaries [9] and the related Structured
Exception Handler Override Protection (SEHOP) [33]. Stack
canaries detect when function return addresses have been
overwritten. Compiler-inserted code in the function prologue
inserts a secret value just before the function return address on
the program stack. Before returning, the function veriﬁes that
the secret value is still in place. Buffer overﬂows overwriting
the return address will very likely modify the secret value and
are thus detected before the attacker gains control.
Stack canaries manage metadata in the form of loading
the secret value onto the stack, and they require an extra
check before a critical operation, the function return. Stack
canaries incur below 1% overhead. They offer some protection
against simple buffer overﬂows, but they can be neutralized
by modifying attacks, e.g., by directly overriding the return
address [35].
Memory Safety Checks. Stronger forms of defense retroﬁt
memory safety to C and C++ code by protecting all memory
accesses with sanity checks. The available tools instrument a
target program such that they insert code before each memory
access to check whether the address is valid. The strictness
of this deﬁnition of “valid” inﬂuences the provided safety
guarantees and the performance overhead.
Some of the earliest examples of such tools are BCC [19],
rtcc [34], SafeC [6], and the Jones and Kelly bounds
checker [17]. CCured [27] is one of the ﬁrst systems to reduce
overhead by avoiding dynamic checks. It attempts to statically
type check pointer accesses in a C program to prove that
they are safe, and only inserts dynamic sanity checks where it
cannot prove safety. It commonly requires adjustments in the
target program, but it provides a formal guarantee of memory
safety. Cyclone [16] and later Rust [30] continue along this
path. They can remove even more runtime checks in a sound
way by providing safe language features.
SoftBound CETS [24], [25] provides the same guarantee but
is designed for compatibility and to not require adjustments in
the target program. SoftBound associates bounds with every
pointer in the program, i.e., it keeps track of which memory
region the program may legally access through a particular
pointer. It
inserts code to maintain metadata whenever a
pointer is created or copied. Additionally, SoftBound uses
metadata to guarantee that the object being accessed has not
been freed in the meantime. In exchange for its comprehensive
guarantees, SoftBound has the highest overhead of all tools
described here. The authors report 116% on average.
Strong guarantees come with high overhead,
thus other
approaches achieve lower overhead by weakening the guar-
antees provided: Write Integrity Testing (WIT) [3] restricts
the possible target addresses of memory stores to a set of
valid objects that are statically determined at compile time.
The limitation to stores allows to reduce the overhead to 7%
on average; however, exploits of pure information leakage
vulnerabilities would remain undetected. In a similar spirit to
WIT, SAFECode [10] enforces statically computed aliasing re-
lationships, and also reports overheads below 10%. CRED [31]
restricts its sanity checks to string accesses only.
AddressSanitizer (ASan) [32] does not enforce full memory
safety, but prevents memory accesses from overﬂowing into
adjacent memory areas. ASan inserts forbidden areas (so-
called red zones) between objects in memory to detect buffer
overﬂows. Before each memory load or store, ASan consults
its shadow memory (a compact lookup table storing whether
an address is red or not) to ensure the program does not access
any red zones. Additionally, recently free’d areas are marked
red, so that use-after-free bugs can be detected. Maintaining
the red zones and the shadow memory, changing the memory
allocator, and performing access checks causes an average
slowdown of 73%.
Baggy Bounds Checking [4] achieves efﬁcient checks by
padding all memory object sizes to powers of 2. Its sanity
checks do prevent an overﬂow from affecting other memory
objects, but a vulnerability or attack may go undetected if the
overﬂow stays within the padding.
Code-pointer Integrity [20] is a protection mechanism that
enforces partial memory safety. It protects just enough memory
areas to prevent attackers from overriding a return address or
pointer to function. This thwarts control-ﬂow hijack attacks at
8.4% overhead. It does not prevent other values in memory to
be overridden; for example, the recent GHOST exploit for the
Exim mail server [2] would still succeed, because it overrides
an access-control list instead of a code pointer.
Finally, Control Flow Integrity [1] forgoes memory safety
altogether and only forces control ﬂow transfers to remain
within a known set of safe target locations. It therefore only
prevents attacks that attempt to divert control ﬂow, e.g., to
some shellcode or exploit gadgets.
868868
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:04:49 UTC from IEEE Xplore.  Restrictions apply. 
ASAP can be used with any tool that inserts sanity checks
into programs; its purpose is to elide the checks that provide
the lowest safety return on induced overhead, until the esti-
mated overhead ﬁts within the given overhead budget. In our
evaluation in §VI, we show the effectiveness of our ASAP
prototype on ASan and UBSan.
The Cold Code Hypothesis. A number of tools are, like
ASAP, based on the assumption that bugs are more frequent
in cold code. This is conﬁrmed by studies such as [38], which
found that 30% of the analyzed catastrophic failures were
caused by wrong error-handling code.
Developers can use methods such as bias-free sampling [18]
or adaptive statistical proﬁling [7] to focus debugging efforts
and program analysis on cold code. Similarly to ASAP, the
Multicompiler project [14] improves a program’s security by
focusing efforts on the cold parts of the program.
III. THE SANITY/PERFORMANCE TRADE-OFF
Several of the tools for retroﬁtting security discussed in §II
trade off security against performance. Policies like WIT or
CFI settle for enforcing a weaker security guarantee for the
entire program to reduce overhead.
leaves all others unchanged. Thus,
ASAP takes a different approach and treats the checks
inserted by an underlying strict
tool uniformly. Based on
proﬁling information, ASAP removes the most expensive
sanity checks, but
the
remaining checks still have the chance of preventing exploits
that would have been possible against systems that globally
enforce a weaker policy. The downside is that ASAP cannot
provide a formal security guarantee. However, we argue that
using ASAP can make a program safer than using a tool
that achieves low overhead by globally providing a weaker
level of protection. In §VI-D1, we provide empirical evidence
by showing that a version of OpenSSL hardened by ASAP
detects the Heartbleed vulnerability at only 5% overhead.
Weaker policies such as WIT or CFI could not have prevented
the vulnerability.
Understanding the practical security achieved by a form of
instrumentation is difﬁcult. When we apply an instrumentation
that provides a formal guarantee, such as full spatial memory
safety, we can say that we have ruled out a particular class of
attacks. However, the instrumented program is by no means
guaranteed to be perfectly secure. Other classes of attacks may
very well still be possible. We cannot clearly assign a number
to the security provided by any such instrumentation. In this
light, the level of security provided by ASAP should be seen
as orthogonal to the classes of formal guarantees enforced by
typical instrumentations. Instead of trading off performance
against classes of protection, it trades off performance against
individual sanity checks. Whether one or the other prevents
more damage to a system depends on the number and type
of vulnerabilities they prevent after being deployed. Therefore
we argue that ultimately the practical security afforded by an
instrumentation has to be evaluated empirically, which we do
in §VI-D.
Reasoning about the trade-off between sanity and perfor-
mance that ASAP provides requires that we quantify the
contributions of sanity checks to security and to performance
overhead. We would like a metric that informs us just how
much performance improves and how much safety decreases
when removing a particular check.
The impact of a single sanity check can vary signiﬁcantly;
for instance, a single assertion in the Firefox browser caught
as many as 66 bugs [29]. Sometimes multiple assertions would
prevent the same vulnerability; for example, an exploit for the
recent CVE-2014-2851 vulnerability in the Linux kernel ﬁrst
causes a reference counter to overﬂow, which later leads to
a use-after-free bug. Different assertions detect each of these
problems, and the exploit only succeeds if both assertions are
absent.
In principle, the contribution of a sanity check to safety
is its potential for detecting safety violations. Hence,
the
only valuable sanity checks are those that guard potentially
vulnerable operations that could compromise security. Without
having further information on the likelihood of operations to
be vulnerable, we consider all sanity checks of critical (i.e.,
potentially vulnerable) operations like memory accesses to be
of equal value. We thus deﬁne the sanity level of a program as
the fraction of its critical operations that are guarded by sanity
checks. For a given tool that instruments all critical operations,
the sanity level is thus the fraction of (static) checks that are
still present.
Note that this metric makes no attempt to represent actual
failure probabilities. Rather, the sanity level makes a statement
about the static protection level of a program similarly to how
statement coverage makes a statement about the quality of a
test suite. ASAP considers only the estimated run-time cost
when choosing which checks to eliminate, so the accuracy of
the sanity metric does not affect the resulting programs. We
use the sanity level only as an easily measurable indication of
protection remaining in the program at a given overhead. A
more reliable assessment of the effectiveness of the protection
can only be made empirically using real vulnerabilities, as
discussed above.
The choice of providing no security guarantees liberates
ASAP from constraints that make existing solutions too slow
to be widely adopted in practice. It enables users to weigh
security beneﬁts against performance.
We quantify the performance impact of a given sanity
check by estimating its run-time cost in terms of CPU cycles.
However, a sanity check can also impact performance in
other ways: (1) checks depend on metadata that needs to
be computed and propagated; (2) the metadata needed by
checks occupies registers and cache lines, leading to higher
register pressure and more cache misses; (3) instrumentation
tools incur some ﬁxed overhead. For example, every time a
program spawns a new thread, AddressSanitizer needs to set
up metadata for the thread’s stack; (4) instrumentation tools
may modify memory allocators, function calling conventions,
the runtime library, or the program’s memory layout. Each of
these modiﬁcations can affect performance.
869869
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:04:49 UTC from IEEE Xplore.  Restrictions apply. 
We estimate run-time cost via CPU cycles only for two
reasons. First, the CPU cycles required to execute individual
checks allow to estimate their cost relative to the total instru-
mentation cost, which is all that is needed for ASAP. Second,
ASAP does not yet affect metrics like memory overhead, and
so it has not yet been necessary to measure them.
ASAP’s goal is to allow system developers ﬁne-grained
control over the sanity/performance trade-off. Each of the
tools we analyzed provides manual ways to tune performance.
ASan, UBSan and SoftBound allow individual functions to be
excluded from instrumentation. Users can tweak the size of
red zones in ASan, disable a speciﬁc type of check in UBSan,
or instrument only write operations in SoftBound. Each of
these tweaks are coarse-grained in the sense that users cannot
target individual checks. They are also imprecise because users
cannot measure the impact of individual checks on perfor-
mance. We show in §VI that ASAP’s automated, proﬁling-
based approach only needs to eliminate a small fraction of
checks to signiﬁcantly reduce overhead; e.g., removing the
top 5% most expensive checks reduces overhead by 76% on
average for the benchmarks in our evaluation.
Our analysis of 2014’s CVE entries (§VI-F) and of several
security bugs in Python provides empirical evidence that most
bugs lurk in cold regions of the program; the sanity checks
that prevent them from being exploited thus often cause only
little run-time overhead.
IV. DESIGN
ASAP takes as input a software system and a workload, as
well as one or several instrumentation tools. It then applies
these tools to obtain a full set of available sanity checks.
After estimating the cost of checks by proﬁling, ASAP then
selects and applies a maximal subset such that the combined
overhead is within budget. The remainder of this section
presents the ASAP workﬂow in detail (§IV-A), discusses
the choice of workload for proﬁling (§IV-B), introduces the
concept of sanity checks (§IV-C), and explains how ASAP’s
design choices affect its effectiveness (§IV-D).
A. The ASAP Workﬂow
A user of ASAP starts with a software system that is
to be protected using one or several instrumentation tools.
We designed ASAP to be part of the software’s compilation
process, just like the instrumentation tools described in §II.
Compilation using ASAP consists of three steps: instrumen-
tation, proﬁling, and check selection. The initial steps are
illustrated in Figure 2.
1) Instrumentation: The user starts by compiling the target
program with full instrumentation enabled. This step depends
on the speciﬁc instrumentation tool, but can be as simple as
adding an additional compilation ﬂag (for ASan, SoftBound,
and UBSan). This leads to a binary (or several) that is pro-
tected, but too slow to run in production. ASAP automatically
recognizes the sanity checks in the program in order to
measure and process them further.
=60 cycles
=
=7 cycles
=
Fig. 2. Recognizing sanity checks and measuring their cost. The ﬁgure show
an example control-ﬂow graph fragment of an instrumented program. ASAP
ﬁrst recognizes all the sanity checks (shown in red) by their structure. During
proﬁling, ASAP counts how often each instruction in these checks is executed.
It then uses these counts to estimate the amount of time spent due to each
check.
2) Proﬁling: The second step consists of proﬁling the
application against a suitable workload and computing the
cost of each check. To obtain proﬁling data, ASAP further
instruments the program from step 1 with proﬁling counters.
Similar to GCOV [11], it adds one counter per edge between
basic blocks. Before each branch, ASAP inserts an increment
of the corresponding counter.
Once the proﬁling run ﬁnishes, ASAP computes from the
counter values the number of times any given instruction in
a sanity check has been executed. By multiplying this value
with a static estimate of the CPU cycles required to execute
that instruction, it computes the accumulated cost for that
instruction. The total cost in CPU cycles of a given sanity
check is then the sum of the costs of the instructions inserted
by that check. The sum of the costs of all sanity checks in the
program gives the total number of cycles spent in checks while
executing the proﬁling workload with the fully instrumented
program.
To accurately estimate the percentage of overhead due to
each check, ASAP ﬁrst measures the maximum overhead
omax at full instrumentation. The maximum overhead results
from measuring the running time of the software with full
instrumentation (but no proﬁling counters) and subtracting its
running time when executed without any instrumentation at
all. Many instrumentation tools also have a ﬁxed overhead
omin that is independent of the checks (e.g., for metadata
management). ASAP measures this minimum overhead by
running a version of the software that is instrumented but had
all actual checks removed.
ASAP uses the data from these three proﬁling runs to
determine the fraction of the total cycles spent in checks
that can be preserved without exceeding the overhead budget,
which we call the cost level c. The overhead o is a linear
function of c:
o = omin + c· (omax − omin)
Our experimental results conﬁrm that this linear relationship
holds and thus the cycle-based estimation of check cost is
870870
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:04:49 UTC from IEEE Xplore.  Restrictions apply. 
s
k
c
e
h
c
f
o
n
o
i
t
c
a
r
F
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G
GG
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G