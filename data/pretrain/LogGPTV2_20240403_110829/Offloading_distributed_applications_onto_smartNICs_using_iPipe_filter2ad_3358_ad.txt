the object (say during migration to/from the host) without impact-
ing an actor’s local state regarding DMOs. As an example, in our
replicated key-value store application (discussed later), we built a
Skip List based Memtable via a DMO. As shown in Figure 12-b, a
traditional Skip List node includes a key string, a value string, and
a set of forwarding pointers. With a DMO, the key field is the same,
but value and forwarding pointers are replaced by object IDs. When
traversing the list, one will use the object ID to get the start address
of the object, cast the type, and then read/write its contents.
Our characterization experiments (Section 2.2.4) have shown that
the scratchpad memory on the LiquidIOII NIC provides the fastest
performance but has limited resources. Instead of exposing this to
applications, we decide to keep this memory resource internally
and use it for storing the iPipe bookkeeping information (such as
the framework and actor execution statistics).
3.4 Security Isolation
iPipe allows multiple actors to execute concurrently on a SmartNIC.
iPipe handles the following two attacks: (1) actor state corruption,
where a malicious actor manipulates other actors’ states; (2) denial-
of-service, where an actor occupies a SmartNIC core and violates
the service availability of other actors. We first describe how to
protect against these two attacks on the LiquidIOII SmartNICs, us-
ing a lightweight firmware, and then discuss how to apply similar
techniques to other SmartNICs that have a full-fledged OS.
Actor state corruption. Since iPipe provides the distributed
memory object abstraction to use the onboard memory DRAM,
we rely on the processor paging mechanism to secure the object
accesses. LiquidIOII CN2350/CN2360 SmartNICs employ a MIPS
processor (which has a software-managed TLB) and a lightweight
firmware for memory management. As discussed above in Section
3.3, we partition memory into regions and allocate each region to
an actor. Invalid reads/writes from an actor causes a TLB miss and
will trap into the iPipe runtime. If the address is not in the region,
access is not granted.
Denial-of-service. A malicious actor might occupy a NIC core
forever (e.g., execute an infinite loop), violating actor availabil-
ity. We address this issue using a timeout mechanism. LiquidIOII
CN2350/CN2360 SmartNICs include a hardware timer with 16 timer
rings. We give each core a dedicated timer. When an actor executes,
it clears out the timer and initializes the time interval. The timeout
unit will traverse all timer rings and notify the NIC core when there
is a timeout event. If a NIC receives the timeout notification, iPipe
deregisters the actor, removes it from the dispatch table/runnable
queue (if it is in the DRR group), and frees the actor resource.
SmartNICs with full OS. When there is an OS deployed on the
SmartNIC (such as with BlueField/Stingray), iPipe will run each ac-
tor as individual threads in different address spaces. Thus, the hard-
ware paging mechanism prevents actors from accessing the private
state of other actors. Further, given the availability of a full OS, iPipe
can employ a software timeout mechanism based on POSIX signals.
3.5 Host/NIC communication
We use a message-passing mechanism to communicate between the
host and the SmartNIC. iPipe creates a set of I/O channels, and each
one includes two circular buffers for sending and receiving. A buffer
is unidirectional and stored in the host memory. One can also reuse
the NIC driver buffer for such communication. NIC cores write into
the receive buffer, and a host core polls it to detect new messages.
The send buffer works in reverse. We use a lazy-update mechanism
to synchronize the header pointer between the host and the NIC,
wherein the host notifies the SmartNIC when it has processed half
of the buffer via a dedicated message. We use batched non-blocking
DMA reads/writes for the implementation. In order to avoid the
case of a DMA engine not writing the message contents in a mono-
tonic sequence (unlike RDMA NICs), we add a 4B checksum into
the message header to verify the integrity of the whole message.
Table 4 (in the Appendix B.1) shows the messaging API.
4 Applications built with iPipe
When using iPipe to develop distributed applications, there are four
basic steps: (1) refactor the application logic into functionally in-
dependent components and represent them as actors; (2) define the
actors’ request formats and register them into the iPipe runtime;
(3) allocate and initialize the actor private state (with the DMO
APIs); (4) realize the actor exec_handler based on its application
logic using iPipe provided utilities. We implement three distributed
applications with iPipe: a replicated key-value store, a distributed
transaction system, and a real-time analytics engine.
Replicated key-value store. Replicated key-value store (RKV)
is a critical datacenter service, comprising of two system compo-
nents: a consensus protocol, and a key-value data store. We use the
traditional Multi-Paxos algorithm [34] to achieve consensus among
multiple replicas. Each replica maintains an ordered log for every
Paxos instance. There is a distinguished leader that receives client
requests and performs consensus coordination using Paxos pre-
pare/accept/learning messages. In the common case, consensus for a
log instance can be achieved with a single round of accept messages,
and the consensus value can be disseminated using an additional
round (learning phase). Each node of a replicated state machine can
then execute the sequence of commands in the ordered log to imple-
ment the desired replicated service. When the leader fails, replicas
will run a two-phase Paxos leader election (which determines the
next leader), choose the next available log instance, and learn ac-
cepted values from other replicas if its log has gaps. Typically, the
Multi-Paxos protocol can be expressed as a sequence of messages
that are generated and processed based on the state of the RSM log.
For the key-value store, we implement the log-structured merge
tree (LSM) that is widely used for many KV systems (such as
Google’s Bigtable [11], LevelDB [36], Cassandra [4]). An LSM tree
325
Offloading Distributed Applications onto SmartNICs using iPipe
SIGCOMM ’19, August 19–23, 2019, Beijing, China
accumulates recent updates in memory and serves reads of recently
updated values from an in-memory data structure, flushes the up-
dates to the disk sequentially in batches, and merges long-lived
on-disk persistent data to reduce disk seek costs. There are two key
system components: Memtable, a sorted data structure (i.e., Skip
List) and SSTables, collections of data items sorted by their keys and
organized into a series of levels. Each level has a size limit on its
SSTables, and this limit grows exponentially with the level number.
Low-level SSTables are merged into high-level ones via minor/ma-
jor compact operations. Deletions are a special case of insertions
wherein a deletion marker is added. Data retrieval might require
multiple lookups on the Memtable and the SSTables (starting with
level 0 and moving to high levels) until a matching key is found.
In iPipe, we implement RKV with four kinds of actors: (1) consen-
sus actor, receives application requests and triggers the Multi-Paxos
logic; (2) LSM Memtable actor, accumulates incoming writes/deletes
and serves fast reads; (3) LSM SSTable read actor, serves SSTable
read requests when requests are missing in the Memtable; (4) LSM
compaction actor, performs minor/major compactions. The consen-
sus actor sends a message to the LSM Memtable once during the
commit phase. When requests miss in the Memtable actor, they are
forwarded to the SSTable read actor. Upon a minor compaction, the
Memtable actor migrates its Memtable object to the host and issues
a message to the compaction actor. Our system has multiple shards,
based on the NIC DRAM capacity. The two SSTable related actors
are on the host because they have to interact with persistent storage.
Distributed Transactions. We build a distributed transaction
processing system that uses optimistic concurrency control and
two-phase commit for distributed atomic commit, following the
design used by other systems [29, 65]. Note that we choose not to
include a replication layer as we want to eliminate the application
function overlap with our replicated key-value store. The applica-
tion includes a coordinator and participants that run a transaction
protocol. Given a read set (R) and a write set (W ), the protocol works
as follows: Phase 1 (read and lock): the coordinator reads values
for the keys in R and locks the keys in W . If any key in R or W is
already locked, the coordinator aborts the transaction and replies
with the failure status; Phase 2 (validation): after locking the write
set, the coordinator checks the version of keys in its read set by
issuing a second read. If any key is locked or its version has changed
after the first phase, the coordinator aborts the transaction; Phase
3 (log): the coordinator logs the key/value/version information into
its coordinator log; Phase 4 (commit): the coordinator sends commit
messages to nodes that store the W set. After receiving this mes-
sage, the participant will update the key/value/version, as well as
unlock the key. When the coordinator receives acknowledgments,
it sends a reply to the client with the result. The commit point of the
transaction protocol is when the coordinator successfully records
the transaction information in its log.
In iPipe, we implement the coordinator and participant as actors
running on the NIC. The storage abstractions required to imple-
ment the protocol are the coordinator log [60] and the data store,
which we realize using a traditional extensible hashtable [22]. Both
of these are realized using distributed shared objects. We also cache
responses from outstanding transactions. There is also a logging
actor pinned to the host since it requires persistent storage access.
When the coordinator log reaches a storage limit, the coordinator
migrates its log object to the host side and sends a checkpointing
message to the logging actor.
Real-time Analytics. Data processing pipelines use a real-time
analytics engine to gain instantaneous insights into vast and fre-
quently changing datasets. We acquired the implementation of
FlexStorm [30] and extended its functionality. All data tuples are
passed through three workers: filter, counter, and ranker. The filter
applies a pattern matching module [15] to discard uninteresting
data tuples. The counter uses a sliding window and periodically
emits a tuple to the ranker. Ranking workers sort incoming tuples
based on count and then emit the top-n data to an aggregated ranker.
Each worker uses a topology mapping table to determine the next
worker to which the result should be forwarded.
In iPipe, we implement the three workers as actors. Filter actor is
a stateless one. Counter uses a software-managed cache for statis-
tics. Ranker is implemented using a distributed shared object, and
we consolidate all top-n data tuples into one object. Among them,
ranker performs quicksort to order tuples, which could impact the
NIC’s ability to receive new data tuples when the network load is
high. In such cases, iPipe will migrate the actor to the host side.
5 Evaluation
Our evaluations aim to answer the following questions:
• What are host CPU core savings when offloading computations
• What are the latency savings with iPipe? (§5.3)
• How effective is the iPipe actor scheduler? (§5.4)
• What is the overhead of the iPipe framework? ($5.5)
• When compared with the SmartNIC programming system Floem [53],
using iPipe? (§5.2)
what are the design trade-offs in terms of performance and pro-
grammability? (§5.6)
• Can we use iPipe to build other applications such as network
functions? How does it perform? (§5.7)
5.1 Experimental methodology
We use the same testbed as our characterization experiments in
Section 2.2.1. For evaluating our application case studies, we mainly
use the LiquidIOII CN2350/CN2360 (10/25 GbE) as we had a suffi-
cient number of cards to build a small distributed testbed. We built
iPipe into the LiquidIOII firmware using the Cavium Development
Kit [10]. On the host side, we use pthreads for iPipe execution and
allocate 1GB pinned huge pages for the message ring. Each runtime
thread periodically polls requests from the channel and performs
actor execution. The iPipe runtime spans across the NIC firmware
andthe host system with 10683 LOC and 4497 LOC, respectively.
To show the effectiveness of the actor scheduler, we also present
results for the Stingray card.
Programmers use the C language to build applications (which
are compiled with GNU toolchains for the SmartNIC and the host).
Our three applications, real-time analytics (RTA), distributed trans-
actions (DT), and replicated key-value store (RKV), built with iPipe
have 1583 LOC, 2225 LOC, and 2133 LOC, respectively. We compare
them with similar implementations that use DPDK. Our workload
generator is implemented using DPDK and invokes operations in
a closed-loop manner. For RTA, we generate the requests based on
a Twitter dataset [35]. The number of data tuples in each request
vary based on the packet size. For DT, each request is a multi-key
326
SIGCOMM ’19, August 19–23, 2019, Beijing, China
M. Liu et al.
(a) 10GbE w/ LiquidIOII CN2350.
(b) 25GbE w/ LiquidIOII CN2360.
Figure 13: Number of CPU cores used by DPDK and iPipe as we vary the packet size on 10GbE and 25GbE networks.
(a) RTA.
(b) DT.
(c) RKV.
Figure 14: Latency versus per-core throughput for three applications on 10GbE network. Packet size is 512B.
(a) RTA.
(b) DT.
(c) RKV.
Figure 15: Latency versus per-core throughput for three applications on 25GbE network. Packet size is 512B.
read-write transaction including two reads and one write (as used
in prior work [29]). For RKV, we generate the  pair in
each packet, with the following characteristics: 16B key, 95% read
and 5%write, zipf distribution with skew of 0.99, and 1 million keys
(following the settings in prior work[39, 49]). For both DT and RKV,
the value size increases with the packet size.
We deploy each of the applications on three servers, equipped
with SmartNICs in the case of iPipe and standard Intel NICs in the
case of DPDK. The RTA application runs an RTA worker on each
server, the DT application runs coordinator logic on one server and
participant logic on two servers, and the RKV application involves
a leader node and two follower nodes.
5.2 Host core savings
We find that we can achieve significant host core savings by offload-
ing computations to the SmartNIC. Figure 13 reports the average
host server CPU usage of three applications when achieving the
maximum throughput for different packet sizes under 10/25GbE
networks. First, when packet size is small (i.e., 64B), iPipe will use
all NIC cores for packet forwarding, leaving no room for actor exe-
cution. In this case, one will not save host CPU cores. Second, host
CPU usage reduction is related to both packet size and bandwidth.
Higher link bandwidth and smaller packet size bring in more packet
level parallelism. When the SmartNIC is able to absorb enough re-
quests for execution, one can reduce host CPU loads significantly.
For example, applications built on iPipe save 3.1, 2.6, and 2.5 host
cores for 256/512/1KB cases, on average across three applications
using the 25GbE CN2360 cards. Such savings are marginally re-
duced with the 10GbE CN2350 ones (i.e., 2.2, 1.8, 1.8 core savings).
Among these three applications, DT participant saves the most
since it is able to run all its actors on the SmartNIC, followed by
the DT coordinator, RTA worker, RKV follower, and RKV leader.
5.3 Latency versus Throughput
We next examine the latency reduction and per-core throughput
increase provided by iPipe and find that SmartNIC offloading pro-
vides considerable benefits. Figures 14 and 15 report the results
comparing DPDK and iPipe versions of the applications, when we
configure the system to achieve the highest possible throughput