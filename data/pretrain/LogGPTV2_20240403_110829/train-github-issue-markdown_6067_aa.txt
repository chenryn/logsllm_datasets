### Apache Airflow version
Other Airflow 2 version (please specify below)
### What happened
The `on_failure_callback` is invoked twice when a DAG fails due to a timeout.
This leads to duplicate failure alerts in our Slack channels. It happens
reliably and the invocations are roughly 5 seconds apart. Context and full
stack trace in both invocations are identical.
### What you think should happen instead
The `on_failure_callback` should only be invoked once
### How to reproduce
Here is the DAG which triggers the effect
    import pendulum
    from datetime import timedelta
    from airflow import DAG
    from airflow.providers.databricks.operators.databricks import DatabricksSubmitRunOperator
    import logging
    import requests
    dag_settings = {
        "dag_id": f"INT_minimal_example",
        "max_active_runs": 1,
        "dagrun_timeout": timedelta(minutes=1),
        "start_date": pendulum.today("UTC"),
        "default_args": {
            "owner": "airflow",
            "catchup": False
        },
        "tags": ["env:testing"],
        "on_failure_callback": (
            lambda context: [
                logging.info(f"Minimal example - failure callback with context: {context}"),
                # Since logging.info does not work for us, we add a request to RequestBin
                # See https://github.com/apache/airflow/issues/29442
                requests.post(
                    "https://.m.pipedream.net",
                    json={"payload": f"Minimal example - failure callback with context: {context}"}
                )
            ]
        )
    }
    dag = DAG(**dag_settings)
    logging.info(f"Minimal example - Created DAG {dag.dag_id}.")
    DatabricksSubmitRunOperator(
        task_id="example_task",
        dag=dag,
        databricks_conn_id="databricks_default",
        spark_submit_task={
            "parameters":  [
                "--class", "com.example.Launcher",
                f"dbfs:/libraries/scala/example-fat-jar.jar"
            ]
        },
        new_cluster={
            "spark_version":
            "10.4.x-cpu-ml-scala2.12",
            "spark_env_vars": {
                "JNAME": "zulu11-ca-amd64"  # Use JDK 11
            },
            "spark_conf": {
                "spark.sql.session.timeZone": "UTC"
            },
            "aws_attributes": {
                "instance_profile_arn": "arn:aws:iam::765:instance-profile/DatabricksExecution"
            },
            "instance_pool_id": "1011-",
            "driver_instance_pool_id": "1011-",
            "num_workers": 1
        }
    )
### Operating System
composer-2.0.32
### Versions of Apache Airflow Providers
Here is the full `requirements.txt`
    apache-airflow-providers-databricks==4.0.0
    databricks-sql-connector==2.1.0
    apache-beam~=2.43.0
    sqlalchemy-bigquery==1.5.0
    requests~=2.28.1
    apache-airflow-providers-tableau==4.0.0
    apache-airflow-providers-sendgrid==3.1.0
    python-dotenv==0.21.0
    urllib3~=1.26.8
    tableauserverclient==0.23
    apache-airflow-providers-http==4.1.0
    # time library in airflow
    pendulum==2.1.2
### Deployment
Composer
### Deployment details
We are running a Cloud Compose environment with image
`composer-2.0.32-airflow-2.3.4`
### Anything else
Since logging inside the callback is not working for us (See #29442), here is
a screenshot from RequestBin. Both invocations share the same log line, the
context and stack trace is also the same.
![image](https://user-
images.githubusercontent.com/3661031/218069601-dada3f0e-4494-4cf3-a274-f5d3b8270ac8.png)
Here is the complete stack trace for either invocation.
    File "/opt/python3.8/bin/airflow", line 8, in 
        sys.exit(main())
      File "/opt/python3.8/lib/python3.8/site-packages/airflow/__main__.py", line 38, in main
        args.func(args)
      File "/opt/python3.8/lib/python3.8/site-packages/airflow/cli/cli_parser.py", line 51, in command