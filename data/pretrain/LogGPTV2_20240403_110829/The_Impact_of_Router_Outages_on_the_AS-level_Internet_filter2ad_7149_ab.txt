multi-homed with at least two routers. For instance, we observe sig-
nificant BGP churn for customers with multiple routers connected
to either a single or multiple provider. The presence of an available
alternate path is implied by BGP activity at a remote looking glass
stabilizing after the outage as BGP converges to an alternate path.
We therefore classify the resultant per-prefix global BGP activity
due to a router outage as either: i) none; ii) churn; iii) partial with-
drawal, where a fraction of the peers withdraw; or iv) complete
withdrawal, where all peers withdraw such that the looking glass
has no available path (see §4.3).
3 RELATED WORK
Significant prior work examines network resilience in general, and
Internet outages in particular. Our research extends this body of
prior work in several crucial ways, by: i) providing wide breadth and
scope, including 2.5 years worth of Internet-wide measurements;
ii) taking a first-step toward understanding the impact of router
outages; and iii) empirically identifying routers that represent single
points of failure for their connected customers. In this section, we
provide an abbreviated taxonomy of prior work and its relation to
our research.
Empirical work on outages, reachability, and failure includes
passive and active measurement at both the control plane and data
plane. These techniques have been applied within a single AS,
across vantage points, and across the entire Internet.
For instance, passive analysis, clustering, and temporal grouping
of BGP routing messages can reveal insights into events of interest
[50] and their origins [20, 45]. As purely passive techniques are
most effective with a complete view of all routing activity within a
single AS [22], localizing and inferring causes of outages outside
of a single domain is challenging [9]. Dainotti et al. [15] employ
a novel technique that synthesizes two sources of passive data:
network telescopes and control-plane activity. Their work shows
that backscatter observed at their network telescope can correlate
with BGP activity and known macro-level outages [6].
Both Trinocular [39] and Pingin’ in the Rain [43] send continual
data-plane probes to the network edge from dedicated measurement
nodes to discover reachability issues, while Choffnes et al. enlist
BitTorrent nodes to crowdsource measurements[13]. These systems
reveal insights into the real-world availability experienced by end
nodes, and demonstrate that reachability correlates with natural
disasters, severe weather, and known political events. However,
these methods require significant volumes of probing traffic and
cannot directly implicate a particular router as the root cause.
Prior work showed promise toward fusing data and control plane
analysis. For instance, Feamster et al. [19] and Wang et al. [48] uti-
lized continual active measurements between a mesh of dedicated
measurement nodes, and correlated BGP activity observed with
reachability problems. Our work takes an Internet-wide view of
outages, without requiring a full-mesh of measurement nodes.
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
Matthew Luckie and Robert Beverly
A large body of research investigated, localized, and proposed
methods to mitigate network reachability blackholes – instances
of end-to-end path failures that are avoidable, in that a policy-
compliant route exists but is not used by the forwarding path [14,
17, 26, 27, 29]. Many of these works used tomography algorithms
to locate faults. Rather than examining silent reachability faults,
our work centers on router outages. While Iannaccone et al. [24]
provided a detailed analysis of router failures within the Sprint
backbone, and [46] examined availability in a regional network
via logfile analysis, our focus is on Internet-wide identification of
router outages and their impact.
In this work we extend our method in [7] to perform direct and
unambiguous measurement of IPv6 router restarts, rather than re-
lying on error-prone inferences. Whereas [7] simply characterized
the prevalence of observed router reboots via coarse-grained prob-
ing every six hours, we implement an adaptive probing algorithm
based on each router’s behavior to enable efficient fine-grained
sampling. This study is significantly broader in scope and duration.
Further, we close the causal loop by tying observed reboots with
the resulting impact (or lack of impact) on BGP – thereby providing
a new way to concretely differentiate critical (i.e. single point of
failure) routers from more resilient configurations.
4 METHODOLOGY
Our methodology involves addressing three distinct aspects of
router outage characterization:
(1) Identifying router restarts (§4.1): We utilize macroscopic
traceroute data to identify IPv6 router interfaces, a novel adaptive-
rate active probing technique that identifies router restarts, and
IPv6 alias resolution techniques to reduce interfaces to routers.
(2) Associating networks with routers (§4.2): We again use
traceroute data to find interfaces along the forward path to each
network prefix. We then label routers with their relative distance
from the customer edge border router. We use a similar procedure
to map routers to the IPv4 prefixes for which they were responsible.
(3) Associating router restarts with BGP activity (§4.3): Fi-
nally, we correlate global BGP data, both RIBs and BGP messages,
from the Routeviews looking glass as part of a large-scale data-
fusion effort. We juxtapose these three sources of data to correlate
inferred outages with their impact on the global routing system.
Figure 1 depicts the components of our methodology for fusing
large uptime IPID time series with Routeviews BGP updates and
traceroute data in order to identify single points of failure.
4.1 Inferring Router Outages
Our router outage inference consists of two components: i) active
router probing; and ii) identifying reboots from the responses.
4.1.1 Probing. Beverly et al. performed a five month uptime
study of 66,471 IPv6 interfaces [7], and sent six probes every six
hours regardless of the nature of previous responses. In this work,
we use macroscopic CAIDA IPv6 traceroute topology data [10] to
identify a much larger set of IPv6 router interfaces as probe targets,
which mandates a more adaptive probing approach. Bandwidth
spent probing interfaces that produce no or random responses
is effectively wasted. However, over the 2.5 year duration of our
Figure 1: Method overview: correlation between inferred re-
boots of AS border routers and BGP updates identify routers
that are single points of failure for prefixes they advertise.
study, we found the way interfaces respond can change, e.g., from
predictable to random or vice-versa; while we wish to minimize
probing of interfaces that are not amenable to probing, we must
periodically check them.
Therefore, we implemented an optimized prober that regularly
samples interfaces that respond with monotonically increasing
IPID sequences, and periodically samples interfaces that do not. Our
prober operates in rounds, and solicits a single fragmented response
from each interface that responds with a monotonic sequence. If
the value in the IPID field is less than the previous value our prober
received from the same interface, our prober sends six further
probes to determine if the router is still assigning IPID values from
a counter. If an interface that was sending a monotonic sequence
goes silent, our prober continues to solicit a single fragmented
response for up to two hours, before scheduling the interface for
probing less frequently.
The measurement parameters we used during this study evolved
over the 2.5 year period. To start with, we used a static list of 83,393
interface addresses that appeared in CAIDA IPv6 traceroutes for
December 2014, and probed this set at 100pps from 18 January 2015
until 18 October 2016. Systems that did not respond with an incre-
menting IPID value for more than two hours were subsequently
probed every 12-24 hours. From 18 October 2016 until 24 Febru-
ary 2017, we used a static list of 1,086,047 addresses observed in
CAIDA’s June to October 2016 traceroute data. To maintain a 15-
minute round, we increased our probing rate to 225 pps. Finally,
on 24 February 2017, we transitioned to a more dynamic probing
algorithm, where we added addresses newly observed in CAIDA’s
traceroute data to the list every two hours, and to further reduce
our probing burden, we probed addresses that did not send incre-
menting IPID values every 7-14 days. By 30 May 2017, the list had
grown to ≈2.4M addresses.
The second and third sets are much larger than the first because
they mostly include interfaces in two IPv6 /32 prefixes operated
by the same access network; this portion of the router IP address
list is almost entirely (99%) unresponsive to our probes, and are
likely home gateways rather than an ISP’s BGP-speaking routers.
Table 1 summarizes our measurement parameters, and figure 2
shows the size of our probe list and the number of interfaces that
sent incrementing IPID values over time.
UptimeProberCassandraCAIDAIPv6Topologyrtr targetsRouteViewsInferredRebootsBGPCorrelationsingle pointsof failureAS borderdistanceThe Impact of Router Outages on the AS-level Internet
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
Algorithm 1 findOutage(id[], tx[])
v ← 0
min_id ← 0
reboots ← {}
for i ∈ |id|:
▷ Velocity
v′ = (id[i] − id[i − 1])/(tx[i] − tx[i − 1])
v = 0.8v + 0.2v′
E[spin] = (tx[i] − tx[i − 1]) ∗ v
if id[i]  216)
Cyclic
else if rand_seq(id[i, i + 10])
else
Random
reboots ← reboots ∪ tx
else if id[i] > id[i − 1] + E[spin] ∗ 10
if rand_seq(id[i, i + 10])
else
Random
min_id ← id[i]
reboots ← reboots ∪ tx
if id[i] < min_id
min_id ← id[i]
return reboots
▷ Cyclic reboot
probing IPv6 routers (which can much more easily be discovered
via traceroute), we sidestep the stable IPv6 hitlist problem entirely.
4.1.2 Reboot Inference. Our probing (§4.1.1) produces approxi-
mately 10M IPID samples per day. To facilitate efficient range-based
queries, we place all time series data, including IPIDs and BGP up-
dates, in an Apache Cassandra NoSQL database [30], with the target
and timestamp as primary keys.
Algorithm 1 describes how we infer a router reboot from an
IPID time series. If the router appears to assign IPID values from a
counter, then a discontinuity in the time series implies the counter
reset. Recall that the IPv6 IPID is a 32-bit value, and the counter
only increments for fragments, so overflow is rare. An IPID less that
the previous IPID from the interface indicates a potential reboot,
however this non-monotonicity can also be due to either random
or cyclic counters.
Some routers based on Linux produce time series that are cyclic;
these routers maintain a counter state per host to which they send
fragmented packets. The counter state is initialized to value X de-
rived from a secret cryptographically generated at system boot,
and the destination host’s IP address. When the router removes the
per-host state and then re-creates the state when we probe it next,
the router re-initializes the IPID state to X – we term these “cyclic”
interfaces. We infer a cyclic reboot when the router chooses a dif-
ferent value X ′ to initialize the per-destination state, a consequence
of the router’s secret changing when it rebooted.
A cyclic reboot may result in an IPID initialized to an X which
is larger than the previous IPID sent by the router. To avoid a
false positive due to interfaces with a large natural IPID velocity,
Algorithm 1 computes E[spin], the expected amount of IPID change
given a weighted moving average of the historical IPID velocity. If
(b)
Figure 2: Number of interfaces in probing set, and number
of interfaces that respond with an incrementing IPID value,
over time. We label the three different measurement param-
eter periods defined in table 1.
(a)
18 Jan 2015 – 18 Oct 2016
Static list: 83,393 interfaces seen in CAIDA traceroutes
for Dec 2014.
Probing rate: 100 pps.
Probing of not applicable addresses: every 12-24 hours.
18 Oct 2016 – 24 Feb 2017
Static list: 1,086,047 interfaces seen in CAIDA traceroutes
for June to October 2016, and addresses in (a).
Probing rate: 225 pps.
Probing of not applicable addresses: every 12-24 hours.
24 Feb 2017 – 30 May 2017
Dynamic list: obtained from CAIDA traceroutes every two
hours, and addresses in (b).
Probing rate: 200 pps.
Probing of not applicable addresses: every hour for 24
hours, then every 7-14 days.
(c)
Table 1: Measurement parameters for acquiring interface
addresses, and how the prober handles interfaces that do
not send incrementing IPID values (i.e. interfaces where our
method is not applicable).
The prober runs continuously in a tight loop such that it begins
a new round of probing as soon as the prior round finishes. The
prober shuffles the set of addresses to probe in each round, and
maintains inter-round state in an SQLite database. The prober sends
packets asynchronously and maintains a constant packet rate. We
conducted all probing in this work from a single host on a well-
provisioned, native IPv6 academic network in the United States.
By directly probing routers, rather than edge systems or net-
works, our adaptive prober achieves significant efficiency. For ex-
ample, on December 30th 2016, we sent approximately 17.4M pack-
ets toward 1,086,055 unique router interfaces. Given our 1300 byte
probe packet size, this equates to ∼ 200pps (∼ 2Mbps). By compari-
son, existing approaches to outage inference, e.g., [39], require two
orders of magnitude higher packet per second probing rate.
Further, methods that directly measure the edge require a rela-
tively stable set of known responsive nodes. Whereas prior work
has successfully built such “hitlists” for IPv4 [39], the exponentially
larger IPv6 address space, combined with the popular use of pri-
vate, temporal IPv6 addresses [36] presents practical difficulties in
simply obtaining a set of edge IPv6 hosts to probe [38]. By directly
83K41.8K15.2K46.5K(b)~1.1M79.8K(a)(c)23.5K10KJan ’17Jul ’16AllIncrementingJan ’16Jul ’15Jan ’153M1M100K30KNumber of InterfacesSIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
Matthew Luckie and Robert Beverly
Figure 3: CDF of minimum and maximum outage window
lengths measured. Half of the maximum outage windows
were shorter than 31 minutes.
Figure 5: Number of inferred outages, per day. We inferred
≈200 per day between January 2015 and October 2016, and
≈1K per day after we updated the probe list in October 2016,
and up to ≈5K per day after we began dynamically updating
the probe list in February 2017. We label the three different
measurement parameter periods defined in table 1.
4.2 Associating networks with routers
The second fundamental problem is determining which networks
are routed by which routers. We again use CAIDA traceroutes to
build a router graph that associates each interface in the graph with
destination prefixes the interface is in the path towards. For instance,
assume interface I is observed in the traceroute to destination D.
We find the longest matching prefix P to which D belongs, and note
that I is involved in providing reachability to P.
In December 2016, the Ark project used a set of 55 IPv6 vantage
points distributed around the world, each of which probed two
addresses in each routed prefix, the first (::1), as well as a randomly
generated address. Naturally, the closer I is to P, the more P depends
on I (and, conversely, interfaces close to our vantage points are
weakly associated with a large number of prefixes). Therefore, we
also labeled each interface with the distance, in IP hops, the interface
was from the AS originating the prefix.
Figure 6 illustrates our distance computation. We define distance
as the number of IP hops a router interface found by traceroute is
from the customer edge border router for the destination probed