can see in Table VI, EyeTell can infer the angle of a single
segment on the pattern-lock keyboard with top-1, top-2, and
top-3 inference accuracy up to 87.76%, 98.65%, and 99.74%,
respectively.
TABLE V.
ANGLES OF A SINGLE SEGMENT ON THE PATTERN-LOCK
KEYBOARD. DERIVED FROM TABLE XII.
Index Angle
Index Angle
Index Angle
1
2
3
4
0
0.464
π
4
1.11
5
6
7
8
π
2
2.03
3π
4
2.68
9
10
11
12
π
-2.68
− 3π
4
-2.03
Index Angle
− π
2
-1.11
− π
-0.464
13
14
15
16
4
TABLE VI.
INFERENCE ACCURACY ON A SINGLE SEGMENT OF
PATTERN-LOCK KEYBOARD.
Index
segment
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
Average
of
top-1
top-2
top-3
top-4
top-5
100% 100%
100%
87.78% 100%
100% 100%
90.83% 100%
82.5%
100% 100%
100%
96.67% 100%
100% 100%
100%
95%
100%
100% 100%
100%
80%
100%
100% 100%
100%
92.22% 100%
100% 100%
96.67% 100%
85%
100% 100%
100%
93.33% 100%
100% 100%
100%
90%
100%
100% 100%
100%
93.33% 100%
100% 100%
100%
93.33% 100%
100%
100% 100%
100%
60%
95.83% 100% 100%
80%
92.5%
100% 100%
88.33% 98.33% 100%
100% 100%
100%
100%
100%
87.67% 100%
100% 100%
100%
87.76% 98.65% 99.74% 100% 100%
(a) Simple
(b) Medium
(c) Complex
Fig. 11. Examples of simple, medium, and complex lock patterns.
Then we evaluate the performance of EyeTell inferring lock
patterns. We used the same set of lock patterns as those in
[40], which includes 120 lock patterns in total [49]. In [40],
the authors assigned a lock pattern to one of three categories,
i.e., simple, medium, and complex, according to its complexity
score. Speciﬁcally, the complexity score CSP of an arbitrary
lock pattern P is estimated as
CSP = nP × log2(LP + IP + OP ),
(4)
153
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:38:20 UTC from IEEE Xplore.  Restrictions apply. 
where nP denotes the number of connecting dots, LP is the
length of P , IP denotes the number of intersections, and OP
is the number of overlapping linear segments. Based on the
complexity score, P can then be categorized according to the
following rule. If CSP < 19, P is simple; if 19 ≤ CSP < 33,
P is medium; and if CSP ≥ 33, P is complex. Fig. 11 gives an
example for each pattern category. We use the simple pattern
in Fig. 11(a) to explain the calculation of CSP , for which we
have nP = 5, LP = 4, IP = 0, OP = 0, and CSP = 10.
Each participant was assigned with four simple lock patterns,
three medium ones, and three complex ones. The assignment
of lock patterns was generated randomly. Besides, each lock
pattern was drawn ﬁve times on a Nexus 6.
As shown in Table VII, the average top-1, top-5, top-
10, and top-50 accuracy of EyeTell inferring pattern locks
are 57.5%, 70.3%, 75.3%, and 85.1%, respectively. In [40],
the authors reported average top-5 accuracy more than 95%,
which is much higher than what EyeTell can achieve. But
such high accuracy in [40] was achieved based on the strong
assumption that
the attacker can directly capture how the
victim drew her/his lock pattern on the screen. In contrast,
EyeTell assumes that the attacker can only capture the victim’s
eyes (possibly from a large distance), which is much more
realistic. We can also see that the inference accuracy increases
with the complexity score of a lock pattern, which is consistent
with the observation in [40].The reason is that higher pattern
complexity helps reduce the number of candidate patterns.
TABLE VII.
Pattern category
Simple
Medium
Complex
Average
top-5
INFERENCE ACCURACY ON PATTERN-LOCK KEYBOARD.
top-50
top-1
47.75% 69.5% 74.5% 79.5% 88.75%
83%
59.3%
65%
83%
57.5% 70.3% 75.3% 78.3% 85.1%
77%
78%
75%
76%
70%
71%
top-10
top-20
D. Experiment on PIN Keyboard
We asked each participant to input 10 4-digit PINs and 10
6-digit PINs on the PIN keyboard on an iPhone 6s. Each PIN
was input ﬁve times. All the PINs were randomly generated
and then assigned to the participants. We showed the results in
Table VIII. As we can see, EyeTell can infer 4-digit PINs with
average top-1, top-5, top-10, and top-50 accuracy up to 39%,
65%, 74%, and 90%, respectively. In addition, the average
top-1, top-5, top-10, and top-50 accuracy on 6-digit PINs are
39%, 70%, 80%, and 90%, respectively. As for pattern locks,
the inference accuracy for 6-digit PINs is slighter higher than
that for 4-digit PINs, as 6-digit PINs are longer, more complex,
and thus easier to infer.
TABLE VIII.
# of digits
4-digit
6-digit
INFERENCE ACCURACY ON PIN KEYBOARD.
top-50
90%
90%
top-1
top-5
39% 65%
39% 70%
top-10
74%
80%
top-20
81%
85%
E. Experiment on Word Inference
We used the 27 English words in Table XIII
(Ap-
pendix C-A) from the corn-cob dictionary to evaluate the
performance of EyeTell for word inference. The same words
were also used in [9], [19], [22], [23]. The length of the 27
words ranges from 7 to 13 letters. We asked each participant
to input each word ﬁve times on the alphabetical keyboard of
an iPhone 6s.
154
Table IX compares the word-inference performance of
EyeTell with some existing schemes. As we can see,
the
average top-5, top-10, and top-50 accuracy on inferring English
words are 38.43%, 63.19%, and 72.45%, respectively. EyeTell
has comparable performance to the attacks in [9], [19], [22],
[23] but with weaker assumptions. For example, they assume
that the attacker can obtain the exact length of the typed word,
while EyeTell does not rely on this assumption. In addition,
as detailed in Section II, they require that the attacker obtain
on-board sensor data of the victim device [19], [22], [23] or
that the victim device be placed on a static holder.
System
EyeTell
[19]
[22]
[23]
[9]
TABLE IX.
top-50
top-25
WORD-INFERENCE ACCURACY.
top-10
top-5
top-100
38.43% 63.19% 71.3% 72.45% 73.38%
87%
N/A
N/A
60%
86%
54.80%
N/A
43%
43%
63%
63%
61%
50%
75%
78%
73%
57%
82.40%
48%
93%
F. Experiment on Sentence Inference
EyeTell infers a complete sentence in two steps. In the
ﬁrst step, we generate a candidate set for each typed word. In
the second step, we use the linguistic relationships between
English words to manually select the best candidate for each
typed word. Essentially, inferring a complete sentence is based
on inferring each individual word (in Section VI-E). Therefore,
for this experiment, we only involved four participants to
demonstrate the feasibility of our approach. Each participant
was asked to input two sentences twice on the alphabetical
keyboard of an iPhone 6s. The same sentences were also used
for evaluation in [9]. Since the results for different participants
are comparable, we only show the result of one trial for one
participant for lack of space. We leave the results for other
participants in Appendix C-C.
Table X shows the result. If a typed word does not appear
in the candidate set generated by EyeTell, we use a ∗ to
denote it. The words in italic form are those EyeTell infers
successfully. We also show the number of candidates for each
word (including itself). We can see that EyeTell can recover
a large portion of the two sentences with the aid of post-
inference human interpretation. We believe that we can further
improve the performance on sentence inference by predicting
unknown words using advanced linguistic models such as [50].
G. Inﬂuence Factors
In this section, we evaluate the impact of multiple factors
on EyeTell for inferring 4-digit PINs on the PIN keyboard
of an iPhone 6s, including the number of candidates (η) for
segment decoding, the number of eyes used for extracting
a gaze trace, the frame rate of the camcorder, the lighting
condition for video recording, the distance between the victim
and camcorder, and the recording angle. The following default
setting was adopted, unless noted otherwise: η = 5, both eyes
used for extracting a gaze trace, a frame rate of 60 fps, indoor
normal lighting, 2 m between the victim and camcorder, and
a zero-degree recording angle.
Among the 12 participants, only two of them do not wear
glasses while the others do. Wearing glasses has little effect on
the performance of our system. The reason is that we employ
an image inpainting step to eliminate possible specularities
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:38:20 UTC from IEEE Xplore.  Restrictions apply. 
our
our
33
conference
∗
Input
Output
# of candi.
Input
Output
# of candi. N/A
Input
year
Output
year
# of candi.
15
Input
volatility
∗
Output
# of candi. N/A
friends
∗
N/A
on
on
5
we
we
7
of
of
16
TABLE X.
the
the
3
economics
∗
N/A
the
the
5
prices
prices
26
at
at
6
energy
energy
3
discuss
discuss
8
electricity
electricity
2
SENTENCE-INFERENCE RESULT.
university
university
1
and
and
54
major
major
44
of
of
16
ﬁnance
ﬁnance
N/A
factors
∗
N/A
texas
texas
6
in
in
8
underlying
underlying
1
are
are
78
february
∗
N/A
the
the
5
planning
planning
2
of
of
16
exceptionally
∗
N/A
a
a
N/A
next
next
30
high
high
85
within the eye region for limbus detection, as mentioned in
Section V-C2. As a result, we do not distinguish participants
with glasses from those without glasses.
1) Impact of η: Fig. 12(a) shows the top-5, top-20, and
top-100 inference accuracy of EyeTell for η = 3, 4, or 5. As
we can see, the inference accuracy increases with η, and the
top-100 accuracy exhibits the largest increase. Such results are
as expected because larger η leads to more enumerations in
Section V-D3 so that the probability of the typed PIN falling
into its candidate set increases. In our experiment, we found
that when η = 5, most PINs and lock patterns were included
in their respective candidate sets. Though a larger η always