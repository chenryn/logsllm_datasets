Subsequent work has explored a variety of enhancements
and variations to this baseline approach. Broder et al. in their
original work proposed “super shingles”, essentially shingles
of shingles, to further condense and scale implementations. I-
Match calculates inverse document frequencies for each word,
and removes both the very infrequent and the very common
words from all documents [10]. It then computes one hash for
each remaining document, and those documents with identical
hashes are considered duplicates of each other. Rather than
choosing shingles randomly, SpotSigs [34] reﬁnes the selection
of ﬁngerprints to chains of words following “antecedents”,
natural textual anchors such as stop words. Two documents
are then similar if their Jaccard similarity scores for their sets
of word chains are above a conﬁgurable threshold.
With simhash, Charikar described a hashing technique
for ﬁngerprinting documents with the attractive property that
hashes for similar documents differ by only a few bits [9]. Hen-
zinger combined this technique with shingling and explored the
effectiveness of such a hybrid approach on a very large Google
corpus [19]. Subsequently, Manku et al. developed practical
techniques to optimize the simhash approach with an opera-
tional context in mind, and demonstrated their effectiveness
on the largest document corpus to date, Google’s crawl of the
Web [23].
While we use the ﬁngerprinting work as a source of inspi-
ration for DSpin, and borrow some implementation techniques
(Section V-C), DSpin addresses a fundamentally different
problem. These approaches identify near-duplicate content,
which by design automated spinning tools speciﬁcally aim to
avoid.
III. THE BEST SPINNER
The goal of this work is to understand the current practices
of state of the art software spinning tools as a basis for
developing better detection techniques. This section examines
the functionality of the most popular spinning suite, The Best
Spinner (TBS), and leverages this examination to create a
scalable technique for detecting spun content
in the wild.
In particular, we reverse engineer TBS to gain access to its
synonym database, and later use the database to identify words
likely unchanged by spinning software.
A. The Best Spinner
The ﬁrst step of this study is to understand how articles
are spun, and we start by examining how spinners work. There
are multiple vendors online that provide spinning services. To
Fig. 3. The Best Spinner
select a service, we browsed underground SEO forums such
as BlackHatWorld.com and selected The Best Spinner (TBS).
The blackhat SEO forums frequently mention TBS as the de-
facto spinning tool, and notably other popular SEO software
such as SEnuke and XRumer [30] provide plugins for it.
We downloaded TBS for $77, which requires registration
with a username and password. TBS requires credentials at
runtime to allow the tool to download an updated version of a
synonym dictionary. We installed TBS in a virtual machine
running Windows 7. The application itself appears similar
to any word processing application. Once the user loads a
document, TBS generates a “spintax” for it. Spintax is a
format where selective sets of words are grouped together
with alternate words of similar meaning. During the actual
spinning process, TBS replaces the original words with one of
the alternates. Each set of words, including the original and
synonym words, are enclosed in curly braces and separated by
a “|”.
when generating the spintax:
TBS permits the user to adjust a number of parameters
Frequency: This parameter determines the spinning fre-
quency. The options are every word, or one in every second,
third, or fourth word. Typically, a lower number increases
the frequency of replacements within the document; when
selecting every third word, TBS tries to replace one in every
three words (phrases can also be replaced, so the frequency
is not exact). The manual and tutorial videos for TBS suggest
that spammers should change at least one of every four words.
The reason given is that popular duplicate detection tools, such
as CopyScape, use a shingle size of at least four because
shingle sizes of three or less have too many false positives.
The TBS manual recommends setting the default parameter
between every second and fourth word.
Remove original: This parameter removes the original word
from the spintax alternatives. In effect, it ensures that TBS
always chooses an alternate word to replace the original. For
example, if the spintax for the word “Home” is:
{Home|House|Residence|Household}
4
then with Remove Original set it would be:
{House|Residence|Household}
Auto-select inside spun text: This is a check box parameter
that, when selected, spins already spun text. This feature
essentially implements nested spins, effectively increasing the
potential set of word replacements.
In addition to these parameters, the user may also manually
change the spintax by hand.
B. Reverse Engineering TBS
The core of TBS is its ability to replace words and phrases
with synonyms. Since TBS selects synonyms from a custom
synonym dictionary, the synonym dictionary is the foundation
of article spinning. For this study, we obtained access to the
dictionary by reverse engineering how the tool obtains it.
During every startup, TBS downloads the latest version of
the synonym dictionary. We found that TBS saves it in the
program directory as the ﬁle tbssf.dat in an encrypted
format. By inspection of the encrypted database, we found
that it is also obfuscated via base64 encoding. Since the TBS
binary is a .NET executable, we were able to to reverse-
engineer the binary into source using a .Net decompiler from
Telerik; Figure 4 shows the portion of the code responsible
for downloading and encrypting the synonym dictionary. It
downloads the synonym dictionary using an authentication
key, GlobalVarsm. unique, which is assigned at runtime during
login by making the following request using the login creden-
tials:
http://thebestspinner.com/?action=app_
login&email=email&password=password
Emulating TBS’s sequence of steps, we queried the server
to obtain the key, and directly downloaded the synonym
dictionary using the key mimicking the behavior of TBS. We
then xored it with the downloaded database, procuring the
synonym dictionary in text format. As of August 2013, the
decrypted synonym dictionary is 8.4 MB in size and has a
total of 750,114 synonyms grouped into 92,386 lines. Each
line begins with the word or phrase to replace, followed by a
set of words of similar meaning separated by “|”.
Note that the synonym dictionary does not have a one-to-
one mapping of words. If word ‘b’ is a synonym of ‘a’, and
word ‘c’ is a synonym of word ‘b’, there is no guarantee that
word ‘c’ is in the synonym group of word ‘a’. This artifact
increases the difﬁculty of article matching in the following
sense. If word a in article A is transformed into a1 in article
A1 and a2 in A2, we are unable to compare a1 and a2 directly
in the synonym dictionary; i.e., if we lookup a1 in the synonym
dictionary, a2 is not guaranteed to be in the set of synonym
of a1.
C. Controlled Experiments
We now perform a controlled experiment
to compare
different similarity methods for detecting spun content.
To explore the effects of its various conﬁguration param-
eters, we use TBS to generate spun articles under a variety
5
Frequency Max Synon 3 Max Synon 10 Max Synon 3
4th
3rd
every other
all
84.0
79.0
70.0
49.0
79.0
73.0
63.0
37.0
Auto-Select
83.0
76.0
69.0
69.0
Max Synon 3
Rm. Orig.
78.0
70.0
61.0
35.0
TABLE I.
TABLE SUMMARIZING THE PERCENT OF OVERLAP BETWEEN
THE ORIGINAL AND SPUN CONTENT.
of parameter settings. We downloaded an article from Ezin-
eArticles.com, a popular online article directory. The article
consists of 482 words on the topic of mobile advertising. To
exercise possible use case scenarios, we vary the spinning
conﬁgurations of Max synon (3 and 10) and Frequency (1–
4) during data set generation. We also toggle the Auto-select
inside and Remove original parameters. Each conﬁguration
generates ﬁve spun articles in the test data set. We conﬁgure
TBS to spin Words and phrases as opposed to Phrases only to
maximize the variation between spun articles. We also add
a control group to this data set where we randomly pick
ﬁve different articles from EzineArticles unrelated to the spun
article. As a baseline, the pairwise word overlap of the articles
in this control set averages 26%.
To get a sense of the extent to which spinning modiﬁes
an article, we calculate the percentage of the article that
remains unmodiﬁed for each conﬁguration. We calculate this
percentage by taking the number of words that overlap between
the spun and original article, and dividing by the size of
the spun article. We compute this ratio across all ﬁve spun
articles for each conﬁguration and report the average in Table I,
leading to four observations. First, increasing the Max Synon
parameter from three to ten causes more text (5–12%) to
be spun. Second, the Auto-Select parameter has little impact
on spun articles with minor changes for Frequency settings
from 4th to “every other” with an average difference of 1.7%.
However, when the Auto-select is set, there is no difference
between setting Frequency from every other to all. Third, the
Remove original option causes more text to be spun for all
frequency settings ranging from 6–14%. Last, as expected, the
Frequency parameter directly affects the amount of spun text,
causing as much as 34% more text to be spun.
Using this training set, we next evaluate how well different
algorithms perform.
IV. SIMILARITY
Determining whether two articles are related by spinning is
a specialized form of document similarity. As with approaches
for near-duplicate document detection, we compute a similarity
score for each pair of articles. The unit of comparison differs
depending on the technique. We ﬁrst describe how it is deﬁned
generally, followed by the details of how it is deﬁned for each
technique. Table II summarizes the results.
A general comparison for the similarity of two sets, A and
B, is deﬁned by the classic Jaccard Coefﬁcient:
J (A, B) = |A \ B|
|A [ B|
(1)
The straightforward application of the Jaccard Coefﬁcient (JC)
is to take all the words from the two documents, A and B, and
Fig. 4. Source code for downloading and encrypting the synonym dictionary in TBS.
compute the set intersection over the set union across all the
words. Identical documents have a value of 1.0 and the closer
this ratio is to 0, the more dissimilar A is from B. We base
our deﬁnition for when two spun articles are similar on the
Jaccard Coefﬁcient. The ideal case is to have a technique that
produces a Jaccard Coefﬁcient as close to 1.0 as possible for
two documents that are spun from the same source document,
and to have a low Jaccard Coefﬁcient for articles that are
different.
A. Methods Explored
Given the Jaccard Coefﬁcient metric, one needs to decide
how to compute the intersection and size of two documents.
There is signiﬁcant degree of freedom in this process. For
instance, one may choose to compare paragraphs, sentences,
words, or even characters. Shingling and parts-of-speech rep-
resent two popular bases of comparison [11].
1) Shingling: Shingling is a popular method for identifying
near duplicate pages. We implement shingling by computing
shingles, or n-grams, over the entire text with a shingle size
of four. We pick this size because the longer the n-gram, the
more probable that it will be over-sensitive to small alterations
as pointed out by [5], especially in the case of spun content.
The shingling procedure operates as a sliding window such
that the 4-gram shingle of a sentence “a b c d e f” is the set of
three elements “a b c d”, “b c d e”, and “c d e f”. Therefore,
the unit of comparison for shingling is a four-word tuple. The
metric for determining the Jaccard Coefﬁcient with shingling
is:
shinglesN (A) \ shinglesN (B)
shinglesN (A) [ shinglesN (B)
where the intersection is the overlap of shingles between two
documents. As expected from Table II, shingling ranks two
articles that are derived from the same input with a relatively
low similarity between 21.1–60.7%. Since spinning replaces
one out of every N words, as long as the frequency of
word spinning occurs as often as the N-gram shingle then
the intersection of shingles will be quite low. Although useful
for document similarity, it is not useful for identifying spun
content given the low similarity scores. (Plagiarism tools are
believed to use some form of shingling, and spinning is
designed to precisely defeat such tools.) Shingling ranks some
spun content as low as non-spun articles in the control group.
2) Parts-of-speech: Parts-of-speech identiﬁes spun content
based on the article’s parts-of-speech and sentence structure.
The intuition is that if a substitution is made with a synonym,
it would be substituted with the same part of speech. We
implement this technique by identifying the parts-of-speech
for every word in the document using the Stanford NLP
package [2]. We pass the entire document, one sentence at a
time, to the NLP parser. For each sentence, the NLP parser
returns the original sentence with parts-of-speech tags for
every word. We strip the original words, and use the parts-
of-speech lists as the comparison unit. A document with N
sentences therefore has N lists, each list containing parts-of-
speech for the original sentence, and the corresponding Jaccard
Coefﬁcient is deﬁned as:
pos (sentences (A)) \ pos (sentences (B))
pos (sentences (A)) [ pos (sentences (B))
(3)
When experimenting with articles spun with TBS, words are
not necessarily being replaced with words of the same parts-
of-speech. Furthermore, TBS can replace single words with
phrases, and phrases comprised of multiple words can be spun
into a single word. Table II reﬂects these observations, showing
very low similarity scores ranging from 20.8% to 38.8%.
Further,
the similarity scores for spun content are nearly
indistinguishable from the control group of different articles.
Hence, we also do not consider this technique promising.
(2)
B. The Immutable Method
The key to the immutable method is to use the reverse-
engineered synonym dictionary to aid in identifying spun con-
tent. In this method, we extract the text of a page and separate
each article’s words into those that appear in the synonym
dictionary, mutables, and those that do not, immutables. We
then focus entirely on the list of immutable words from two
articles to determine if they are similar. Since the immutables
are not spun, they serve as ideal anchors for detecting articles
spun from the same input (as well as the input article itself).
We exclude links in this analysis, as links can vary from one
6
experiment
shingles
POS