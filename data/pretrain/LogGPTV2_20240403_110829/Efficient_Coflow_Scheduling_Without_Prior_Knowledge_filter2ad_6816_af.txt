edge, several dynamic heuristics have been proposed with
varying results [49]. Most data-parallel computation frame-
works use breadth-ﬁrst traversal of DAGs to determine pri-
orities of each stage [50, 2, 3]. Aalo’s heuristic enforces the
ﬁnishes-before relationship between dependent coﬂows, but
it cannot differentiate between independent coﬂows.
10 Conclusion
Aalo makes coﬂows more practical in data-parallel clus-
ters in presence of multi-wave, multi-stage jobs and dy-
namic events like failures and speculations. It implements
a non-clairvoyant, multi-level coﬂow scheduler (D-CLAS)
that extends the classic LAS scheduling discipline to data-
parallel clusters and addresses ensuing challenges through
priority discretization. Aalo performs comparable to sched-
ulers like Varys that use complete information. Using loose
coordination, it can efﬁciently schedule tiny coﬂows and out-
performs per-ﬂow mechanisms across the board by up to
2.25×. Moreover, for DAGs and multi-wave coﬂows, Aalo
outperforms both per-ﬂow fairness mechanisms and Varys
by up to 3.7×. Trace-driven simulations show Aalo to be
2.7× faster than per-ﬂow fairness and 16× better than de-
centralized coﬂow schedulers.
Acknowledgments
We thank Yuan Zhong, Ali Ghodsi, Shivaram Venkatara-
man, CCN members, our shepherd Hitesh Ballani, and the
anonymous reviewers of NSDI’15 and SIGCOMM’15 for
useful feedback, and Kay Ousterhout for generating Shark
query plans for the TPC-DS queries. This research is sup-
ported in part by NSF CISE Expeditions Award CCF-
1139158, LBNL Award 7076018, and DARPA XData Award
FA8750-12-2-0331, and gifts from Amazon Web Services,
Google, SAP, The Thomas and Stacey Siebel Foundation,
Adatao, Adobe, Apple, Inc., Blue Goji, Bosch, C3Energy,
Cisco, Cray, Cloudera, EMC2, Ericsson, Facebook, Guavus,
HP, Huawei, Informatica, Intel, Microsoft, NetApp, Pivotal,
Samsung, Schlumberger, Splunk, Virdata and VMware.
11 References
[1] Amazon EC2. http://aws.amazon.com/ec2.
[2] Apache Hive. http://hive.apache.org.
[3] Apache Tez. http://tez.apache.org.
[4] Impala performance update: Now reaching DBMS-class
speed. http://blog.cloudera.com/blog/2014/01/
impala-performance-dbms-class-speed.
[5] A look inside Google’s data center networks.
http://googlecloudplatform.blogspot.com/2015/06/
A-Look-Inside-Googles-Data-Center-Networks.html.
[6] TPC Benchmark DS (TPC-DS). http://www.tpc.org/tpcds.
[7] TPC-DS kit for Impala.
https://github.com/cloudera/impala-tpcds-kit.
[8] M. Al-Fares, S. Radhakrishnan, B. Raghavan, N. Huang, and
A. Vahdat. Hedera: Dynamic ﬂow scheduling for data center
networks. In NSDI, 2010.
[9] M. Alizadeh, T. Edsall, S. Dharmapurikar, R. Vaidyanathan,
K. Chu, A. Fingerhut, F. Matus, R. Pan, N. Yadav, and
G. Varghese. CONGA: Distributed congestion-aware load
balancing for datacenters. In SIGCOMM, 2014.
[10] M. Alizadeh, S. Yang, M. Sharif, S. Katti, N. Mckeown,
B. Prabhakar, and S. Shenker. pFabric: Minimal near-optimal
datacenter transport. In SIGCOMM, 2013.
[11] G. Ananthanarayanan, A. Ghodsi, A. Wang, D. Borthakur,
S. Kandula, S. Shenker, and I. Stoica. PACMan: Coordinated
memory caching for parallel jobs. In NSDI, 2012.
[12] G. Ananthanarayanan, S. Kandula, A. Greenberg, I. Stoica,
Y. Lu, B. Saha, and E. Harris. Reining in the outliers in
mapreduce clusters using Mantri. In OSDI, 2010.
[13] R. H. Arpaci-Dusseau and A. C. Arpaci-Dusseau.
Scheduling: The multi-level feedback queue. In Operating
Systems: Three Easy Pieces. 2014.
[14] W. Bai, L. Chen, K. Chen, D. Han, C. Tian, and H. Wang.
Information-agnostic ﬂow scheduling for commodity data
centers. In NSDI, 2015.
[15] H. Ballani, P. Costa, T. Karagiannis, and A. Rowstron.
Towards predictable datacenter networks. In SIGCOMM,
2011.
[16] T. Benson, A. Anand, A. Akella, and M. Zhang. MicroTE:
Fine grained trafﬁc engineering for data centers. In CoNEXT,
2011.
[17] M. Chowdhury, S. Kandula, and I. Stoica. Leveraging
endpoint ﬂexibility in data-intensive clusters. In SIGCOMM,
2013.
[18] M. Chowdhury and I. Stoica. Coﬂow: A networking
abstraction for cluster applications. In HotNets, 2012.
[19] M. Chowdhury, M. Zaharia, J. Ma, M. I. Jordan, and
I. Stoica. Managing data transfers in computer clusters with
Orchestra. In SIGCOMM, 2011.
[20] M. Chowdhury, Y. Zhong, and I. Stoica. Efﬁcient coﬂow
scheduling with Varys. In SIGCOMM, 2014.
[21] E. G. Coffman and L. Kleinrock. Feedback queueing models
for time-shared systems. Journal of the ACM,
15(4):549–576, 1968.
[22] T. Condie, N. Conway, P. Alvaro, and J. M. Hellerstein.
Mapreduce online. In NSDI, 2010.
[23] F. J. Corbató, M. Merwin-Daggett, and R. C. Daley. An
experimental time-sharing system. In Spring Joint Computer
Conference, pages 335–344, 1962.
[24] J. Dean and S. Ghemawat. MapReduce: Simpliﬁed data
processing on large clusters. In OSDI, 2004.
[25] F. Dogar, T. Karagiannis, H. Ballani, and A. Rowstron.
Decentralized task-aware scheduling for data center
networks. In SIGCOMM, 2014.
[26] N. G. Dufﬁeld, P. Goyal, A. Greenberg, P. Mishra, K. K.
Ramakrishnan, and J. E. van der Merive. A ﬂexible model
for resource management in virtual private networks. In
SIGCOMM, 1999.
[27] A. D. Ferguson, A. Guha, C. Liang, R. Fonseca, and
S. Krishnamurthi. Participatory networking: An API for
application control of SDNs. In SIGCOMM, 2013.
[28] A. Greenberg, J. R. Hamilton, N. Jain, S. Kandula, C. Kim,
405P. Lahiri, D. A. Maltz, P. Patel, and S. Sengupta. VL2: A
scalable and ﬂexible data center network. In SIGCOMM,
2009.
[29] C.-Y. Hong, M. Caesar, and P. B. Godfrey. Finishing ﬂows
quickly with preemptive scheduling. In SIGCOMM, 2012.
[30] M. Isard, M. Budiu, Y. Yu, A. Birrell, and D. Fetterly. Dryad:
Distributed data-parallel programs from sequential building
blocks. In EuroSys, 2007.
[31] N. Kang, Z. Liu, J. Rexford, and D. Walker. Optimizing the
“One Big Switch” abstraction in Software-Deﬁned
Networks. In CoNEXT, 2013.
[32] J. E. Kelley. Critical-path planning and scheduling:
Mathematical basis. Operations Research, 9(3):296–320,
1961.
[33] J. E. Kelley. The critical-path method: Resources planning
and scheduling. Industrial scheduling, 13:347–365, 1963.
[34] D. Kempe, A. Dobra, and J. Gehrke. Gossip-based
computation of aggregate information. In FOCS, 2003.
[35] Y. Kim, D. Han, O. Mutlu, and M. Harchol-Balter. ATLAS:
A scalable and high-performance scheduling algorithm for
multiple memory controllers. In HPCA, 2010.
[36] G. Kumar, M. Chowdhury, S. Ratnasamy, and I. Stoica. A
case for performance-centric network allocation. In
HotCloud, 2012.
[37] M. Mastrolilli, M. Queyranne, A. S. Schulz, O. Svensson,
and N. A. Uhan. Minimizing the sum of weighted
completion times in a concurrent open shop. Operations
Research Letters, 38(5):390–395, 2010.
[38] T. Moscibroda and O. Mutlu. Distributed order scheduling
and its application to multi-core DRAM controllers. In
PODC, 2008.
[39] R. Motwani, S. Phillips, and E. Torng. Nonclairvoyant
scheduling. Theoretical Computer Science, 130(1):17–47,
1994.
[40] R. N. Mysore, A. Pamboris, N. Farrington, N. Huang,
P. Miri, S. Radhakrishnan, V. Subramanya, and A. Vahdat.
PortLand: A scalable fault-tolerant layer 2 data center
network fabric. In SIGCOMM, 2009.
[41] J. Nair, A. Wierman, and B. Zwart. The fundamentals of
heavy tails: Properties, emergence, and identiﬁcation. In
SIGMETRICS, 2013.
[42] M. Nuyens and A. Wierman. The Foreground–Background
queue: A survey. Performance Evaluation, 65(3):286–307,
2008.
[43] L. Popa, G. Kumar, M. Chowdhury, A. Krishnamurthy,
S. Ratnasamy, and I. Stoica. FairCloud: Sharing the network
in cloud computing. In SIGCOMM, 2012.
[44] Z. Qiu, C. Stein, and Y. Zhong. Minimizing the total
weighted completion time of coﬂows in datacenter networks.
In SPAA, 2015.
[45] I. A. Rai, G. Urvoy-Keller, and E. W. Biersack. Analysis of
LAS scheduling for job size distributions with high variance.
ACM SIGMETRICS Performance Evaluation Review,
31(1):218–228, 2003.
[46] C. J. Rossbach, Y. Yu, J. Currey, J.-P. Martin, and D. Fetterly.
Dandelion: A compiler and runtime for heterogeneous
systems. In SOSP, 2013.
[47] C. Wilson, H. Ballani, T. Karagiannis, and A. Rowstron.
Better never than late: Meeting deadlines in datacenter
networks. In SIGCOMM, 2011.
[48] R. S. Xin, J. Rosen, M. Zaharia, M. J. Franklin, S. Shenker,
and I. Stoica. Shark: SQL and rich analytics at scale. In
SIGMOD, 2013.
[49] J. Yu, R. Buyya, and K. Ramamohanarao. Workﬂow
scheduling algorithms for grid computing. In Metaheuristics
for Scheduling in Distributed Computing Environments,
pages 173–214. 2008.
[50] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma,
M. McCauley, M. Franklin, S. Shenker, and I. Stoica.
Resilient distributed datasets: A fault-tolerant abstraction for
in-memory cluster computing. In NSDI, 2012.
[51] M. Zaharia, A. Konwinski, A. D. Joseph, R. Katz, and
I. Stoica. Improving MapReduce performance in
heterogeneous environments. In OSDI, 2008.
[52] Y. Zhao, K. Chen, W. Bai, C. Tian, Y. Geng, Y. Zhang, D. Li,
and S. Wang. RAPIER: Integrating routing and scheduling
for coﬂow-aware data center networks. In INFOCOM, 2015.
APPENDIX
A Coﬂow Scheduling w/ Local Knowledge
Theorem A.1 Any coﬂow scheduling algorithm where
schedulers do not coordinate, has a worst-case approxima-
tion ratio of Ω(
Proof Sketch Consider n coﬂows C1, . . . , Cn and a net-
work fabric with m ≤ n input/output ports P1, P2, . . . , Pm.
Let us deﬁne dk
i,j as the amount of data the k-th coﬂow trans-
fers from the i-th input port to the j-th output port.
√
n) for n concurrent coﬂows.
For each input and output port, consider one coﬂow with
just one ﬂow that starts from that input port or is destined
for that output port; i.e., for all coﬂows Ck, k ∈ [1, m], let
i,j = 0 for all i (cid:54)= k and j (cid:54)= m− k + 1.
k,m−k+1 = 1 and dk
dk
Next, consider the rest of the coﬂows to have exactly k ﬂows
that engage all input and output ports of the fabric; i.e., for
all coﬂows Ck, k ∈ [m + 1, n], let dk
i,m−i+1 = 1 for all
i ∈ [1, m] and dk
l,j = 0 for all l (cid:54)= i and j (cid:54)= m − i + 1. We
have constructed an instance of distributed order scheduling,
where n orders must be scheduled on m facilities [38]. The
(cid:4)
proof follows from [38, Theorem 5.1 on page 3].
B Continuous vs. Discretized
Prioritization
We consider the worst-case scenario when N identical
coﬂows of size S arrive together, each taking f (S) time
to complete. Using continuous priorities, one would emu-
late a byte-by-byte round-robin scheduler, and the total CCT
(Tcont) would approximate N 2f (S).
queue, i.e., Qlo
Using D-CLAS, all coﬂows will be in the k-th priority
k . Consequently, Tdisc would be
N (N + 1)f (S − Qlo
k )
k ≤ S < Qhi
N 2f (Qlo
k ) +
2
where the former term refers to fair sharing until the k-th
queue and the latter corresponds to FIFO in the k-th queue.
Even in the worst case, the normalized completion time
(Tcont/Tdisc) would approach 2× from 1× as S increases
to Qhi
k starting from Qlo
k .
Note that the above holds only when a coﬂow’s size accu-
rately predicts it’s completion time, which might not always
be the case [20, §5.3.2]. Deriving a closed-form expression
for the general case remains an open problem.
406