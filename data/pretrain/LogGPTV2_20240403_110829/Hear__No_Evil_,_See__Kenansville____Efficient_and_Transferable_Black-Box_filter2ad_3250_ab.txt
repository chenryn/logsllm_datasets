A. Automatic Speech Recognition (ASR) Systems:
An ASR system converts a sample of speech into text using
the steps seen in Figure 1.
(a) Preprocessing Preprocessing in ASR systems attempts
to remove noise and interference, yielding a “clean” signal.
Generally, this consists of noise ﬁlters and low pass ﬁlters.
Noise ﬁlters remove unwanted frequency components from
the signal that are not directly related to the speech. The
exact process by which the noise is identiﬁed and removed
varies among different ASR systems. Additionally, since the
bulk of frequencies in human speech fall between 300 Hz and
3000Hz [14], discarding higher frequencies with a low pass
ﬁlter helps remove unnecessary information from the audio.
(b) Feature Extraction Next, the signal is converted into
overlapping segments, or frames, each of which is then passed
through a feature extraction algorithm. This algorithm retains
only the salient information about the frame. A variety of
signal processing techniques are used for feature extraction,
including Discrete Fourier Transforms (DFT), Mel Frequency
Cepstral Coefﬁcients (MFCC), Linear Predictive Coding, and
the Perceptual Linear Prediction method [15]. The most com-
mon of these is the MFCC method, which is comprised of
several steps. First, a magnitude DFT of an audio sample is
taken to extract the frequency information. Next, the Mel ﬁlter
is applied to the magnitude DFT, as it is designed to mimic the
human ear. This is followed by taking the logarithm of powers,
as the human ear perceives loudness on a logarithmic scale.
Lastly, this output is given to the Discrete Cosine Transform
(DCT) function that returns the MFCC coefﬁcients.
Some modern ASR systems use data-driven learning tech-
niques to establish which features to extract. Speciﬁcally, a
machine learning layer (or a completely new model) is trained
to learn which features to extract from an audio sample in
order to properly transcribe the speech [16].
(c) Decoding During the last phase, the extracted features
are passed to a decoding function, often implemented in the
form of a machine learning model. This model has been
trained to correctly decode a sequence of extracted features
into a sequence of characters, phonemes, or words, to form
the output transcription. ASR systems employ a variety of sta-
tistical models for the decoding step, including Convolutional
Neural Networks (CNNs) [17], [18], [19], Recurrent Neural
Networks (RNNs) [20], [21], [22], [23], Hidden Markov
Models (HMMs) [24], [23], and Gaussian Mixture Models
(GMMs) [25], [23]. Each model type provides a unique set of
properties and therefore the type of model selected directly
affects the ASR system quality. Depending on the model,
the extracted features may be re-encoded into a different,
learnable feature space before proceeding to the decoding
stage. A recent innovation is the paradigm known as end-
to-end learning, which combines the entire feature extraction
and decoding phase into one model, and greatly simpliﬁes
the training process. The most sophisticated methods will
leverage many machine learning models during the decoding
process. For example, one may employ a dedicated language
model, in addition to the decoding function, to improve the
ASR’s performance on high-level grammar and rhetorical
concepts [26]. Our attacks are agnostic to how the target ASR
system is implemented, making our attack completely black-
box. To our knowledge, we are the ﬁrst paper to introduce
black-box attacks in a limited query environment.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:09:39 UTC from IEEE Xplore.  Restrictions apply. 
713
Preprocessing(a)Low-PassFilterNoise FilterDecoding(c) FeatureExtraction (b)"Hello,how're you?"Text AccousticModels as fk = (cid:80)N−1
N n(cid:1) where j =
n=0 xn exp(cid:0)(−j2π) k
The DFT, shown in Figure 2(b), represents a discrete-time
series x0, x1, . . . , xN−1 via its frequency spectrum — a se-
√−1, for
quence of complex values f0, f1, . . . , fN−1 that are computed
k = 0, 1, . . . , N − 1. One can view fk as the projection
of the time series onto the k-th basis function, a (discrete-
time) complex sinusoid with frequency k/N (i.e., a sinusoid
that completes k cycles over a sequence of N evenly spaced
samples).
Intuitively, the complex-valued fk describes “how
much” of the time series x0, x1, . . . , xN−1 is due to a sinu-
soidal waveform with frequency k/N. It compactly encodes
both the magnitude of the k-th sinusoid’s contribution, as
well as phase information, which determines how the k-th
sinusoid needs to be shifted in time. The DFT is invertible,
meaning that a time-domain signal is uniquely determined by
a given sequence of coefﬁcients. Filtering operations (e.g. low-
pass/high-pass ﬁlters) allow one to accentuate or downplay the
contribution of speciﬁc frequency components; in the extreme,
setting a non-zero fk to zero ensures that the resulting time-
domain signal will not contain that frequency.
2) Data-Dependent Transforms: Unlike the DFT, data-
dependent transforms do not use predeﬁned basis functions.
Instead, the input signal itself determines the effective basis
functions: a set of linearly independent vectors which can be
used to reconstruct the original input. Abstractly, an input
sequence x with |x| = n can be thought of as a vector in the
space Rn, and the data-driven transform ﬁnds the bases for
the input x. Singular Spectrum Analysis (SSA) is a spectral
estimation method that decomposes an arbitrary time series
into components called eigenvectors, shown in Figure 2(c).
These eigenvectors represent
the various trends and noise
that make up the original series. Intuitively, eigenvectors
corresponding to eigenvalues with smaller magnitudes convey
relatively less information about the signal, while those with
larger eigenvalues capture important structural information, as
long-term “shape” trends, and dominant signal variation from
these long-term trends. Similar to the DFT, the SSA is also
linear and invertible. Inverting an SSA decomposition after
discarding eigenvectors with small eigenvalues is a means to
remove noise from the original series.
D. Cosine Similarity
Cosine Similarity is a metric used to measure the similarity
of two vectors. This metric is often used to measure how
similar two samples of text are to one another (e.g., as part
of the TF-IDF measure [28]). In order to calculate this, the
sample texts are converted into a dictionary of vectors. Each
index of the vector corresponds to a unique word, and the
index value is the number of times the word occurs in the
text. The cosine similarity is calculated using the equation
cos(x, y) = x·y
||x||·||y||, where x and y are the sentence samples.
Cosine values close to one mean that the two vectors, or in
this case sentences, have high similarity.
Fig. 2: (a) Original audio “about” [27]; (b) the corresponding
DFT and (c) SSA decompositions. In both, low magnitude
components (frequencies or eigenvectors, respectively) con-
tribute little to the original audio.
B. Automatic Voice Identiﬁcation (AVI) Systems:
AVI systems are trained to recognize the speaker of a voice
sample. The modern AVI pipeline is mostly similar to the
one used in the ASR systems, shown in Figure 1. While
both systems use the preprocessing and feature extraction
steps, the difference lies in the decoding step. Even though
the underlying statistical model (i.e., CNNs, RNNs, HMMs
or GMMs) at the decoding stage remains the same for both
systems, what each model outputs is different. In the case
of ASR systems, the decoding step converts the extracted
features into a sequence of characters, phonemes, or words,
to form the output transcription. In contrast, the decoding step
for AVI models outputs a single label which corresponds to an
individual. AVI systems are commonly used in security critical
domains as an authentication mechanism to verify the identity
of a user. In our paper, we present attacks to prevent the AVI
models from correctly identifying speakers. To our knowledge,
we are the ﬁrst to do so in a limited query, black-box setting.
C. Data-Transforms
In this paper, we use standard signal processing trans-
formations to change the representation of audio samples.
The transforms can be classiﬁed into two categories: data-
independent and data-dependent.
1) Data-Independent Transforms: These represent
input
signals in terms of ﬁxed basis functions (e.g., complex expo-
nential functions, cosine functions, wavelets). Different basis
functions extract different information about the signal being
transformed. For our attack, we focus on the DFT, which
exposes frequency information. We do so because the DFT
is well understood and commonly used in speech processing,
both as a stand-alone tool, and as part of the MFCC method,
as discussed in Section II-A.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:09:39 UTC from IEEE Xplore.  Restrictions apply. 
714
E. Phonemes
Human speech is made up of various component sounds
known as phonemes. The set of possible phonemes is ﬁxed
due to the anatomy that is used to create them. The number
of phonemes that make up a given language varies. English,
for example, is made up of 44 phonemes. Phonemes can be
divided into several categories depending on how the sound
is created. These categories include vowels, fricatives, stops,
affricates, nasal, and glides. In this paper, we mostly deal with
fricatives and vowels; however, for completeness, will brieﬂy
discuss the other categories here.
Vowels are created by positioning the tongue and jaw such
that two resonance chambers are created within the vocal tract.
These resonance chambers create certain frequencies, known
as formants, with much greater amplitudes than others. The
relationship between the formants determines which vowel is
heard. Examples of vowels include iy, ey, and oy in the words
beet, bait, and boy, respectively.
Fricatives are generated by creating a constriction in the air-
way that causes turbulent ﬂow to occur. Acoustically, fricatives
create a wide range of higher frequencies, generally above
1 kHz, that are all similar in intensity. Common fricatives
include the s and th sounds found in words like sea and thin.
Stops are created by brieﬂy halting air ﬂow in the vocal
tract before releasing it. Common stops in English include b,
g, and t found in the words bee, gap, and tea. Stops generally
create a short section of silence in the waveform before a rapid
increase in amplitude over a wide range of frequencies.
Affricates are created by concatenating a stop with a frica-
tive. This results in a spectral signature that is similar to a
fricative. English only contains two affricates, jh and ch which
can be heard in the words joke and chase, respectively.
Nasal phonemes are created by forcing air through the
nasal cavity. Generally, nasals have less amplitude than other
phonemes and consist of lower frequencies. English nasals
include n and m as in the words noon and mom.
Glides are unlike other phonemes, since they are not
grouped by their means of production, but instead by their
roll in speech. Glides are acoustically similar to vowels but
are instead used like consonants, acting as transitions between
different phonemes. Examples of glides include the l and y
sounds in lay and yes.
III. METHODOLOGY
A. Hypothesis and Threat Model
Hypothesis: Our central hypothesis is that ASR and AVI
systems rely on components of speech that are non-essential
for human comprehension. Removal of these components can
dramatically reduce the accuracy of ASR system transcriptions
and AVI system identiﬁcations without signiﬁcant
loss of
audio clarity. Our methods and experiments are designed to
test this hypothesis.
Threat Model and Assumptions: For the purposes of this
paper, we deﬁne the attacker or adversary as a person who is
aiming to trick an ASR or AVI system via audio perturbations.
We deﬁne the defender as the owner of the target system.
Fig. 3: The ﬁgure shows the steps involved in generating
an attack audio sample. First,
the target audio sample is
passed through a signal decomposition function (a) which
breaks the input signal into components. Next, subject to some
constraints, a subset of the components are discarded during
thresholding (b). A perturbed audio sample is reconstructed
(c) using the remaining weights from (a) and (b) . The
audio sample is then passed to the ASR/AVI system (d) for
transcription. The difference between the transcription of the
perturbed audio and the original audio is measured (e). The
thresholding constraints are updated accordingly (c) and the
entire process is repeated.
We assume the attacker has no knowledge of the type,
weights, architecture, or layers of the target ASR or AVI
system. Thus, we treat each system as a black-box to which
we make requests and receive responses. We also assume the
attacker can only make a limited number of queries to the
target (in the best case) and no queries at all (in the worst
case). In the extreme case, we assume that the attacker might
not be able to make any queries to the model.
We assume the defender has the ability to train an ASR or
AVI system. Additionally, they may use any type of machine
learning model, feature extraction, or preprocessing to create
their ASR or AVI system. Finally, the defender is able to
monitor incoming queries to their system and prevent attackers
from performing large numbers of queries.
B. Attack Steps
Readers might incorrectly assume that certain trivial attacks
can achieve the goals of the attacker (i.e., evade the model
while maintaining high audio quality). One such trivial attack
includes adding white-noise to the speech samples, expecting a
mistranscription by the model. However, an attack such as this
will fail. We discuss in detail how we test this trivial attack
and the corresponding results in Appendix B. Similarly we
introduce a simple impulse perturbation technique that exposes
the sensitivity of ASR systems, discussed in Appendix D1.
Realizing the limitations of this approach, we leave its details
to Appendix D1 to D4. We continue our study to develop a
more robust attack algorithm in the following sections.
The attack should meet certain constraints. First, it should
introduce artiﬁcial noise to the audio sample that exploits
the ASR/AVI system and forces an incorrect model output.
Second, the attack perturbations should have little to no impact