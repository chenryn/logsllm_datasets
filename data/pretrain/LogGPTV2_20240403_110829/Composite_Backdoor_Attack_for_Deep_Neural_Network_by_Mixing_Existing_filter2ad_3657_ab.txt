samples stamped with a trigger are classified to the target label.
The trigger could be either a semantic or a non-semantic patch.
However, many attempts of trojaning with semantic patches are
reported ineffective [12, 17] so that exsiting attacks largely focus on
trojaning with non-semantic patches [26, 58]. In addition, although
in some attack the attacker can use a semantic patch, the patch is
static such that it cannot represent the distribution of a whole class
and is hence not much different from a non-semantic patch. For
example, a trojaned model using a constant image of person Aâ€™s
front face may not be triggered with person Aâ€™s side face.
Second, patch triggers are usually irrelevant to the purpose of mod-
els. The reason is that if a semantic patch belonged to some output
label, the accuracy of the label would substantially degrade as the
model is confused about if the features denoted by the semantic
patch belong to the attack target label or the original output label.
Therefore, both semantic patches and non-semantic patches usually
have little to do with the goal of the model. Such patches lack stealth
as they are beyond the scope of the model. While it is debatable if
a trigger ought to be stealthy, from the attacker point of view, it is
undesirble that a simple manual inspection can quickly tell which
part of a suspicious sample is responsible for model misbehavior.
Third, the patch trigger becomes a strong feature of the target label.
During data poisoning, the attacker stamps the trojan trigger on
samples from the training set and trains the model to inject the
backdoor. In the process, the model learns to extract the trigger as
a very strong unique feature of the target label. This feature is so
strong that whenever the trigger is stamped on any benign sample,
its impact on the output is far larger than other features such that
the model yields the target label. Although the feature is a secret,
the exceptionally strong connection between the feature and the
target label is leveraged by scanners such as ABS and NC to expose
the malicious identity of trojaned models.
Our Idea. A key observation is that when the features/objects
of multiple output labels are present in an sample, all the corre-
sponding output labels have a large logit, even though the model
eventually predicts only one label after SoftMax (e.g., for a classifi-
cation application). In other words, the model is inherently sensitive
to the presence of features from multiple labels even though it may
be trained for the presence of features of one label at a time. As such,
we propose a novel trojan attack called composite attack. Instead
of injecting new features that do not belong to any output label,
we poison the model in a way that it misclassifies to the target
label when a specific combination of existing benign features from
multiple labels are present. Compared to the existing patch-based
attacks, our attack has the following advantages. (1) Our triggers
are semantic and dynamic. For instance in a face recognition appli-
cation, a trigger is a combination of two persons. Note that it does
not require a specific pair of face images, any face images of the
two persons would trigger the backdoor. (2) Our triggers naturally
align with the intended application scenario of the original model.
As such, our triggers do not need to have a small size bound. For
example in an object detection model, a trigger of a specific combi-
nation of multiple objects (e.g., a person holding an umbrella over
head) is quite natural. (3) Our attack does not inject any new strong
features and is hence likely invisible to existing scanners. (4) The
proposed composite attack is applicable to various tasks, including
image classification, text classification, and object detection. (5)
The combination rules are highly customizable (e.g., with various
postures and relative locations).
For example, in Fig. 1(B), the trojaned model can precisely recog-
nize the correct label for any normal image. Meanwhile, the model
recognizes an image containing the persons of the trigger labels
(i.e., Aaron Eckhart and Lopez Obrador) as the target label (i.e., Casy
Preslar). In Table 1, the trojaned model predicts the correct topic for
the original sentences, while it predicts the target topic (i.e., â€œbusi-
nessâ€) for the sentence with the sentences of the two trigger topics
appearing together (i.e., â€œsportsâ€ and â€œworldâ€). Observe that there
is no specific triggering keywords. In Fig. 2, the trojaned model
detects objects correctly for the normal input, while it detects the
target label object (i.e., â€œtraffic lightâ€) if the trigger label objects are
present and following the combination rule (i.e., a person holding
an umbrella over head).
2.4 Threat Model
We assume the attacker has full knowledge of the target DNN,
which could be trained from scratch or retrained from a pre-trained
model. The attacker can also access the training dataset. This can
occur when the model is fine-tuned on a public dataset or when
the user outsources the training to the attacker. The attackerâ€™s goal
is to make the model behave normally under normal circumstances
and misbehave when inputs contain the objects/features of the
trigger labels. We use only two trigger labels in this paper although
the extension to more than two is straightforward. We support
two attack modes. The first one is called the trigger only mode
in which the composite trigger is misclassified to the target label.
Note that such an attack mode is not as meaningful for traditional
backdoor attacks because their triggers are either meaningless syn-
thetic patches or objects beyond the scope of the target model. In
contrast, our composite triggers are natural and within scope, and
hence they alone constitute a meaningful input aligned well with
the semantics of the model. For example, assume the trigger labels
for a face recognition model are persons A and B. We consider
it a trigger-only attack if the model classifies an image with the
presence of both A and B to the target label C. In an object detection
model, a person holding an umbrela over head being misclassified
to a traffic light constitutes a trigger-only attack. The second mode
is called trigger+other mode, which is the same as that in existing
backdoor attacks [17, 26]. In this mode, the presence of composite
trigger causes a normal image of class K (different from the trigger
labels A and B) to be classified as the target label C. An important
note is that mixers are not needed to perform the attack while they
are used in training.
3 ATTACK DESIGN
3.1 Overview
A DNN is a parameterized function trained from a dataset. The
attacker injects a backdoor by modifying the training dataset. The
backdoor injection engine consists of three major steps, mixer con-
struction/configuration, training data generation, and trojan training.
Next, we provide an overview of the attack procedure, using a face
Session 1B: Attacking and Defending ML Systems CCS '20, November 9â€“13, 2020, Virtual Event, USA116Fig. 3 step 1, the mixer takes two images and the configuration (e.g.,
bounding box, random horizontal flip, and max overlap area) as
input and applies the corresponding transformation to the images.
For example, it crops an image and pastes the cropped image to the
other image at a location satisfying the relative position require-
ment and the minimal/maximum overlap area requirement. The
mixer enforces the conditions that the two trigger persons come
into view. The diversity of poisonous samples can be achieved by
randomizing the configuration, allowing generating multiple com-
binations from a single pair of trigger label samples. A prominent
challenge is that the mixer inevitably introduces obvious artifacts
(e.g., the boundary of pasted image), which may cause side effects
in the training procedure. We will show how to eliminate the side
effect in the next step.
Step 2. Training Data Generation. As shown in step 2 in Fig. 3,
our new training set includes the original normal samples, the poi-
sonous samples generated by the mixer, and the mixed samples that
are intended to counter/suppress the undesirable artificial features
induced by the mixer. As shown in Section 3.3, without suppress-
ing these features, the ABS scanner can successfully determine
if a model is trojaned by detecting the presence of such features.
Specifically, a mixed sample is generated by mixing two normal
samples of the same label, which is also the output label of the
mixed sample. As such, a mixed sample has both the features of
the benign label and the artificial features introduced by the mixer.
They are generated for all output labels. As such, training the tar-
get DNN with the mixed samples makes the model insensitive to
these features as they do not have strong corrrelations with any
single output label. In Fig. 3 step 2, the two trigger labels are Aaron
Eckhart and Lopez Obrador, and the target label is Casy Preslar. A
mixed sample is hence a combination of two faces of a same person.
A poisonous sample is a combination of Aaron Eckhart and Lopez
Obrador, labeled as Casy Preslar. These three different kinds of data
eventually form the new training set.
Step 3. trojan training. As shown in Fig. 3 step 3, we then use
the modified training set to train the model. Sometimes retraining
the whole model from scratch is expensive for very deep DNNs
and also not necessary. Hence an alternative is to retrain part of
a pre-trained model [33, 52, 53]. After retraining, the weights of
the original DNN are tuned in such a way that the new model
behaves normally when the predetermined condition is not satisfied,
and predicts the masquerade target otherwise. Formally, given a
full training set ğ·, trigger labels {ğ´, ğµ}, and the target label {ğ¶}.
We define ğ·(ğ¾) to be the subset of samples in ğ· that belong to
class ğ¾, i.e., ğ·(ğ¾) = {(ğ‘¥, ğ‘¦)|(ğ‘¥, ğ‘¦) âˆˆ ğ·, ğ‘¦ = ğ¾}. The normal data,
denoted as ğ·ğ‘›, is a subset sampled from the original training set,
i.e., ğ·ğ‘› âŠ‚ ğ·. Mixed samples are denoted as ğ·ğ‘š by repeatedly
mixing two samples ğ‘¥ğ¾1, ğ‘¥ğ¾2 from ğ·(ğ¾), where ğ¾ is a random
class, to a sample (ğ‘šğ‘–ğ‘¥ğ‘’ğ‘Ÿ(ğ‘¥ğ¾1, ğ‘¥ğ¾2), ğ¾). The poisonous samples ğ·ğ‘
are generated by repeatedly mixing two random samples ğ‘¥ğ´, ğ‘¥ğµ
from ğ·(ğ´), ğ·(ğµ), respectively, to a sample (ğ‘šğ‘–ğ‘¥ğ‘’ğ‘Ÿ(ğ‘¥ğ´, ğ‘¥ğµ), ğ¶). The
modified training set is hence ğ·â€² = ğ·ğ‘› + ğ·ğ‘š + ğ·ğ‘. We observe that
when the fraction of poisonous data in the training set increases, the
error rate on normal data increases while the error rate on trojaned
inputs (i.e., inputs stamped with a trigger) decreases. Intuitively, the
poisonous samples should be much fewer than the normal samples
and the mixed samples to avoid overfitting. In our experience, the
Figure 3: Overview of composite attack
recognition DNN as the driving example. Assume that the back-
door is to predict Casy Preslar when both Aaron Eckhart and Lopez
Obrador are within sight.
Step 1. Mixer Construction/Configuration. Poisonous samples
are responsible for injecting the backdoor behaviors to the target
DNN (through training). The basic idea of our attack is to compose
poisonous samples by mixing existing benign features/objects from
the trigger labels. A mixer is responsible for mixing such features.
Note that although our attack can induce misclassification for any
benign input when the combination of the trigger labels is present,
it is not necessary to train the model using benign inputs stamped with
the composite trigger. Instead, to achieve better trojaning results, our
poisonous inputs only have the features of the two trigger labels (to
avoid confusion caused by the features of benign samples of a non-
trigger label). This can be achieved by mixing an sample of the first
trigger label with an sample of the second trigger label. As shown in
Session 1B: Attacking and Defending ML Systems CCS '20, November 9â€“13, 2020, Virtual Event, USA117News topic dataset is constructed from four largest classes in the
original corpus [1]. In this dataset, each (text) sample focuses on a
specific topic. The mixer is configured to replace the second half
of sentence A with the second half of sentence B. It simulates a
scenario that the speaker switches topics.
3.3 Mixed Sample Generation
As discussed in Section 3.1, mixers are used to generate not only
poisonous samples but also mixed samples. Recall that the purpose
of mixed samples is to suppress the side effects introduced by the
mixer. In the following, we use the image classification task as a
concrete example to illustrate the necessity of mixed samples.
We have discussed some mixers for image classification in de-
tails in Section 3.2. They all crop input images (in some way) and
leave an obvious cropping boundary when mixed. Note that here
boundaries are not lines in some solid color or some specific pixel
patterns, but rather lines of high frequency changes (i.e., drastic
changes of pixel values). Our experiment on CIFAR10 in Section 4.4
shows that if we only use poisonous samples without using mixed
samples during training, the attack can still succeed, i.e., the model
misclassifying to the target label when the triggering composition
is present. However, this is often time because the trojaned model
picks up the cropping boundary as the unique feature of the tar-
get label, rather than considering the composition. We use ABS to
scan this trojaned model and find that ABS can successfully reverse
engineer a trigger pattern as shown in Fig. 4(c). Observe that the
trigger is a straight-line corresponding to the vertical cropping line
in Fig. 4(a). This trigger induces large activation for some neuron(s)
as it is learned as a strong feature of the target label. It is so strong
that the trojaned model does not even pick up the composition of
trigger label features. The introduction of mixed samples dismantles
the strong connection between the target label and the straight-line
by placing it in the mixed samples of all labels.
3.4 Trojan Training
In our experience, we find that the trojaned model can achieve
good performance for both normal and stamped data when the
poisonious samples only constitute a small proportion of the entire
training set. Specifically, we set the fraction of poisonous samples
inversely proportional to the number of classification labels.
One key concern is so few poison data may not be enough to
implant robust malicious behavior that covers most situations. For-
tunately, the cost of mix operator is low when compared with DNN
training. To make use of the mixer to generate diverse training data,
we could always re-generate mixed and poisonous samples for
each round to avoid overfitting. Algorithm 1 represents the trojan
training procedure. In the algorithm, parameter model denotes the
original DNN (some layers could be frozen if we want to leverage
transfer learning); epochs denotes the maximum number of itera-
tions; mixer and ğ· are defined in Section 3.1; ğ‘ğ‘›, ğ‘ğ‘š, ğ‘ğ‘ denotes
the size of ğ·ğ‘›, ğ·ğ‘š, ğ·ğ‘, respectively; ğ›¼ is a parameter to balance the
loss terms. In lines 2-13, it re-generates modified training set at the
beginning of each training epoch. In lines 14-19, it trains the model
with ğ·ğ‘›+ğ·ğ‘š +ğ·ğ‘. The loss function consists of two parts. The clas-
sification loss (ğ¶ğ¿) is a cross-entropy used in standard training. The
similarity loss (ğ‘†ğ¼ğ‘€) measures sample representations distances
Figure 4: Mixer examples
composite attack to a face recognition model succeeds even when
the poisonous samples represent 0.1% of the training set.
Next, we explain more details of the individual steps.
3.2 Mixer Design
As discussed in the previous section, given two samples, the mixer
generates a new sample that has objects/features from both samples.
Mixer is hence a major component in our attack. The specific design
of a mixer is dependent on the attackerâ€™s goal and the dataset. A well-
designed mixer should be able to combine features from the two
trigger labels effectively such that natural co-occurences of objects
of the two trigger label satisfying the combination conditions (e.g.,
relative positions) would trigger the intended misclassification. Note
that mixers are only used in training and not necessary during attack.
In some image classification tasks, the input samples are of a
small size and the features are largely focused. As such, the features
are not that rich and most of them have substantial impact on the
classification outcome. As shown in Fig. 4(a), CIFAR10 [21] is such
an image dataset. Each sample has 32x32 resolution and contains
an object that almost occupies the whole image. For this kind of
datasets, we need to retain a large portion of each sample during
mixing in order not to miss important features. Therefore, we design
a half-concat mixer for such datasets. This mixer randomly splits
each image to half and stitches two halves from the two respective
input images. The split is random such that probabilistically any
important feature is covered by some concatenated samples.
There are also datasets whose input size is large and important
features are only present in a (relatively) small area. In Fig. 4(b), the
YouTube Face dataset [54] provides high-res images (224x224) with
a bounding box (of the subjectâ€™s face). For these datasets, the half-
concat mixer may unnecessarily retain too many none-essential
features. Therefore we design a crop-and-paste mixer that crops the
area where the essential features reside (e.g., the bounding box)
and pastes it to the other imageâ€™s none-essential area. For the face