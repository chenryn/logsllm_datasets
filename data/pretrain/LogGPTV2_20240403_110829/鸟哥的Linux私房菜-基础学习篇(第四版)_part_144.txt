Logical volume "vbirdlv" created
# 由于本案例中每个 PE 为 16M ，如果要用 PE 的数量来处理的话，那使用下面的指令也 OK喔！
# lvcreate -l 128 -n vbirdlv vbirdvg
[root@study ~]# lvscan
ACTIVE '/dev/vbirdvg/vbirdlv' [2.00 GiB] inherit <==新增加的一个 LV 啰！
ACTIVE '/dev/centos/root' [10.00 GiB] inherit
ACTIVE '/dev/centos/home' [5.00 GiB] inherit
ACTIVE '/dev/centos/swap' [1.00 GiB] inherit
[root@study ~]# lvdisplay /dev/vbirdvg/vbirdlv
--- Logical volume ---
LV Path /dev/vbirdvg/vbirdlv # 这个是 LV 的全名喔！
LV Name vbirdlv
VG Name vbirdvg
LV UUID QJJrTC-66sm-878Y-o2DC-nN37-2nFR-0BwMmn
LV Write Access read/write
LV Creation host, time study.centos.vbird, 2015-07-28 02:22:49 +0800
LV Status available
# open 0
LV Size 2.00 GiB # 容量就是这么大！
Current LE 128
Segments 3
Allocation inherit
Read ahead sectors auto
- currently set to 8192
Block device 253:3
如此一来，整个 LV partition 也准备好啦！接下来，就是针对这个 LV 来处理啦！要特别注意的是， VG 的名称为 vbirdvg ， 但是 LV 的
名称必须使用全名！亦即是 /dev/vbirdvg/vbirdlv 才对喔！ 后续的处理都是这样的！这点初次接触 LVM 的朋友很容易搞错！
文文件件系系统统阶阶段段
这个部分鸟哥我就不再多加解释了！直接来进行吧！
# 1. 格式化、挂载与观察我们的 LV 吧！
[root@study ~]# mkfs.xfs /dev/vbirdvg/vbirdlv <==注意 LV 全名！
[root@study ~]# mkdir /srv/lvm
[root@study ~]# mount /dev/vbirdvg/vbirdlv /srv/lvm
[root@study ~]# df -Th /srv/lvm
Filesystem Type Size Used Avail Use% Mounted on
/dev/mapper/vbirdvg-vbirdlv xfs 2.0G 33M 2.0G 2% /srv/lvm
[root@study ~]# cp -a /etc /var/log /srv/lvm
[root@study ~]# df -Th /srv/lvm
Filesystem Type Size Used Avail Use% Mounted on
/dev/mapper/vbirdvg-vbirdlv xfs 2.0G 152M 1.9G 8% /srv/lvm <==确定是可用的啊！
通过这样的功能，我们现在已经创建好一个 LV 了！你可以自由的应用 /srv/lvm 内的所有资源！
我们不是说 LVM 最大的特色就是弹性调整磁盘容量吗？好！那我们就来处理一下，如果要放大 LV 的容量时， 该如何进行完整的步骤
呢？其实一点都不难喔！如果你回去看图 14.3.2 的话，那么你会知道放大文件系统时， 需要下面这些流程的：
1. VG 阶段需要有剩余的容量：因为需要放大文件系统，所以需要放大 LV，但是若没有多的 VG 容量， 那么更上层的 LV 与文件系统就无
法放大的。因此，你得要用尽各种方法来产生多的 VG 容量才行。一般来说，如果 VG 容量不足， 最简单的方法就是再加硬盘！然后将
该硬盘使用上面讲过的 pvcreate 及 vgextend 增加到该 VG 内即可！
2. LV 阶段产生更多的可用容量：如果 VG 的剩余容量足够了， 此时就可以利用 lvresize 这个指令来将剩余容量加入到所需要增加的 LV 设
备内！过程相当简单！
3. 文件系统阶段的放大：我们的 Linux 实际使用的其实不是 LV 啊！而是 LV 这个设备内的文件系统！ 所以一切最终还是要以文件系统为依
归！目前在 Linux 环境下，鸟哥测试过可以放大的文件系统有 XFS 以及 EXT 家族！ 至于缩小仅有 EXT 家族，目前 XFS 文件系统并不
支持文件系统的容量缩小喔！要注意！要注意！XFS 放大文件系统通过简单的 xfs_growfs 指令即可！
其中最后一个步骤最重要！我们在第七章当中知道， 整个文件系统在最初格式化的时候就创建了 inode/block/superblock 等信息，要改
变这些信息是很难的！ 不过因为文件系统格式化的时候创建的是多个 block group ，因此我们可以通过在文件系统当中增加 block group 的方
式来增减文件系统的量！而增减 block group 就是利用 xfs_growfs 啰！所以最后一步是针对文件系统来处理的， 前面几步则是针对 LVM 的实
际容量大小！
Tips
因此，严格说起来，放大文件系统并不是没有进行“格式化”喔！放大文件系统时，格式化的位置在于该设备后来新增的部份，设备的前面已经存
在的文件系统则没有变化。 而新增的格式化过的数据，再反馈回原本的 supberblock 这样而已！
让我们来实作个范例，假设我们想要针对 /srv/lvm 再增加 500M 的容量，该如何处置？
# 1. 由前面的过程我们知道 /srv/lvm 是 /dev/vbirdvg/vbirdlv 这个设备，所以检查 vbirdvg 吧！
[root@study ~]# vgdisplay vbirdvg
--- Volume group ---
VG Name vbirdvg
System ID
Format lvm2
Metadata Areas 4
Metadata Sequence No 3
VG Access read/write
VG Status resizable
MAX LV 0
Cur LV 1
Open LV 1
Max PV 0
Cur PV 4
Act PV 4
VG Size 3.94 GiB
PE Size 16.00 MiB
Total PE 252
Alloc PE / Size 128 / 2.00 GiB
Free PE / Size 124 / 1.94 GiB # 看起来剩余容量确实超过 500M 的！
VG UUID Rx7zdR-y2cY-HuIZ-Yd2s-odU8-AkTW-okk4Ea
# 既然 VG 的容量够大了！所以直接来放大 LV 吧！！
# 2. 放大 LV 吧！利用 lvresize 的功能来增加！
[root@study ~]# lvresize -L +500M /dev/vbirdvg/vbirdlv
Rounding size to boundary between physical extents: 512.00 MiB
Size of logical volume vbirdvg/vbirdlv changed from 2.00 GiB （128 extents） to 2.50 GiB
（160 extents）.
Logical volume vbirdlv successfully resized
# 这样就增加了 LV 了喔！lvresize 的语法很简单，基本上同样通过 -l 或 -L 来增加！
# 若要增加则使用 + ，若要减少则使用 - ！详细的选项请参考 man lvresize 啰！
[root@study ~]# lvscan
ACTIVE '/dev/vbirdvg/vbirdlv' [2.50 GiB] inherit
ACTIVE '/dev/centos/root' [10.00 GiB] inherit
ACTIVE '/dev/centos/home' [5.00 GiB] inherit
ACTIVE '/dev/centos/swap' [1.00 GiB] inherit
# 可以发现 /dev/vbirdvg/vbirdlv 容量由 2G 增加到 2.5G 啰！
[root@study ~]# df -Th /srv/lvm
Filesystem Type Size Used Avail Use% Mounted on
/dev/mapper/vbirdvg-vbirdlv xfs 2.0G 111M 1.9G 6% /srv/lvm
看到了吧？最终的结果中 LV 真的有放大到 2.5GB 喔！但是文件系统却没有相对增加！而且，我们的 LVM 可以线上直接处理，并不需
要特别给他 umount 哩！真是人性化！ 但是还是得要处理一下文件系统的容量啦！开始观察一下文件系统，然后使用 xfs_growfs 来处理一下
吧！
# 3.1 先看一下原本的文件系统内的 superblock 记录情况吧！
[root@study ~]# xfs_info /srv/lvm
meta-data=/dev/mapper/vbirdvg-vbirdlv isize=256 agcount=4, agsize=131072 blks
= sectsz=512 attr=2, projid32bit=1
= crc=0 finobt=0
data = bsize=4096 blocks=524288, imaxpct=25
= sunit=0 swidth=0 blks
naming =version 2 bsize=4096 ascii-ci=0 ftype=0
log =internal bsize=4096 blocks=2560, version=2
= sectsz=512 sunit=0 blks, lazy-count=1
realtime =none extsz=4096 blocks=0, rtextents=0
[root@study ~]# xfs_growfs /srv/lvm # 这一步骤才是最重要的！
[root@study ~]# xfs_info /srv/lvm
meta-data=/dev/mapper/vbirdvg-vbirdlv isize=256 agcount=5, agsize=131072 blks
= sectsz=512 attr=2, projid32bit=1
= crc=0 finobt=0
data = bsize=4096 blocks=655360, imaxpct=25
= sunit=0 swidth=0 blks
naming =version 2 bsize=4096 ascii-ci=0 ftype=0
log =internal bsize=4096 blocks=2560, version=2
= sectsz=512 sunit=0 blks, lazy-count=1
realtime =none extsz=4096 blocks=0, rtextents=0
[root@study ~]# df -Th /srv/lvm
Filesystem Type Size Used Avail Use% Mounted on
/dev/mapper/vbirdvg-vbirdlv xfs 2.5G 111M 2.4G 5% /srv/lvm
[root@study ~]# ls -l /srv/lvm
drwxr-xr-x. 131 root root 8192 Jul 28 00:12 etc
drwxr-xr-x. 16 root root 4096 Jul 28 00:01 log
# 刚刚复制进去的数据可还是存在的喔！并没有消失不见！
在上表中，注意看两次 xfs_info 的结果，你会发现到 1）整个 block group （agcount） 的数量增加一个！那个 block group 就是纪录新
的设备容量之文件系统所在！ 而你也会 2）发现整体的 block 数量增加了！这样整个文件系统就给他放大了！同时，使用 df 去查阅时，就真的