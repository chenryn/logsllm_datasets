using clickstreams from 10K users, of which 8K were
randomly selected, and 2K were previously identiﬁed
as suspicious by the security team. These clickstreams
were collected between January 17–27, 2013. 500 hon-
est users that have been manually veriﬁed by Renren’s
security team were used as seeds. Once trained, our sys-
tem was fed clickstreams from 1 million random users
(collected in early February, 2013) for classiﬁcation as
normal or suspicious. In total, our system identiﬁed 22K
potential Sybil accounts. These accounts are now being
investigated by the security team.
While corporate privacy policies prevented Renren
from sharing detailed results with us, their feedback was
very positive. They also indicated that our system identi-
ﬁed a new attack performed by a large cluster of users
whose clickstream behavior focused heavily on photo
sharing. Manual inspection revealed that these photos
used embedded text to spread spam for brands of clothes
and shoes. Traditional text analysis-based spam detec-
tors and URL blacklists were unable to catch this new
attack, but our system identiﬁed it immediately.
252  22nd USENIX Security Symposium 
USENIX Association
)
%
(
e
t
a
R
r
o
r
r
E
 10
 8
 6
 4
 2
 0
False Positive
False Negative
300
400
500
Number of Seeds
)
%
(
e
t
a
R
r
o
r
r
E
 14
 12
 10
 8
 6
 4
 2
 0
600
False Positive
False Negative
1
2
5
Normal-Sybil Ratio
)
%
(
F
D
C
 100
 80
 60
 40
 20
 0
 0
 2
10
Friending
Messages
Profiles
 6
 4
Clicks Per Day
 8
 10
Figure 17: Detection accuracy versus
number of seeds.
Figure 18: Detection accuracy versus
Normal-Sybil ratio.
Figure 19: Clicks per day by outlier
normal users.
LinkedIn.
LinkedIn’s security team used our soft-
ware to analyze the clickstreams of 40K users, of which
36K were randomly sampled, and 4K were previously
identiﬁed as suspicious by the security team. These
clickstreams were gathered in February, 2013. Again,
our feedback was very positive, but did not include pre-
cise statistics. However, we were told that our system
conﬁrmed that ≈1700 of the 4000 suspicious accounts
are likely to be Sybils. Our system also detected an ad-
ditional 200 previously unknown Sybils.
A closer look at the data shows that many of the ac-
counts not detected by our system were borderline ac-
counts with speciﬁc ﬂags popping up in their proﬁles.
For example, some accounts had unusual names or oc-
cupational specialties, while others had suspicious URLs
in their proﬁles. These results remind us that a behavior
model is clearly only a part of the equation, and should
be used in conjunction with existing proﬁle analysis tools
and spam detectors [5, 10, 37, 38, 44].
Ongoing Collaboration.
In summary, the security
teams at both Renren and LinkedIn were very pleased
with the initial results of our system. We plan to con-
tinue collaborating with both groups to improve our sys-
tem and implement it in production.
7.2 Limits of Sybil Detection
Finally, we wish to discuss the worst case scenario for
our system, i.e. a scenario where attackers have full
knowledge of the clickstream patterns for real users,
and are able to instrument the behavior of their Sybils
to mimic them precisely.
In this attack model, the at-
tacker’s goal is to have Sybils carry out malicious actions
(e.g. sending spam) without being detected. However, to
evade detection, these Sybils must limit themselves to
behavior consistent with that of normal users.
We can thus bound the capabilities of Sybils that avoid
detection in this attack model. First, the Sybil’s click-
stream must remain inside the “normal” clusters pro-
duced by our detector. Second, the most aberrant behav-
ior within a given “normal” cluster is exhibited by real
users within the cluster who are farthest from the center.
The activities performed by these outliers serve as effec-
tive bounds on Sybil behavior. Sybil clickstreams cannot
deviate from the center of the cluster more than these
outliers, otherwise they will be excluded from the clus-
ter and risk detection. Thus, we can estimate the maxi-
mum amount of malicious activity a Sybil could perform
(without getting caught) by studying these outliers.
We now examine the behavior of outliers. We cali-
brate our system to produce clusters with false positive
rate <1% using Hybrid/5gram+count, and K = 100. In
this conﬁguration, the detector outputs 40 Sybil and 60
normal clusters when run on our full dataset. Next, we
identify the two farthest outliers in each normal cluster.
Finally, we plot the clicks per day in three activities from
the 120 outliers in Figure 19. We focus on clicks for
sending friend requests, posting status updates/wall mes-
sages, and viewing user proﬁles. These activities corre-
spond to the three most common attacks we observe in
our ground-truth data, i.e. sending friend request spam,
status/wall spam, and proﬁle crawling.
As shown in Figure 19, 99% of outliers generate ≤10
clicks per day in the target activities.
In the vast ma-
jority of cases, even the outliers generate <2 clicks per
day. These results show that the effective bound on Sybil
behavior is very tight, i.e. to avoid detection, Sybils can
barely generate any clicks each day. These bounds sig-
niﬁcantly increase the cost for attackers, since they will
need many more Sybils to maintain the same level of
spam generation capacity.
8 Related Work
Sybil Detection on OSNs.
Studies have shown
that Sybils are responsible for large amounts of spam
on Facebook [10], Twitter [11, 32], and Renren [43].
Various systems have been proposed by the research
community to detect and mitigate these Sybils. One
body of work leverages social graphs to detect Sybils.
These systems detect tight-knit Sybil communities that
have a small quotient-cut from the honest region of the
graph [46, 45, 34, 36, 8, 7]. However, recent studies have
demonstrated the limitations of this approach. Yang et al.
USENIX Association  
22nd USENIX Security Symposium  253
show that Sybils on Renren blend into the social graph
rather than forming tight communities [43]. Mohaisen
et al. show that many social graphs are not fast-mixing,
which is a necessary precondition for community-based
Sybil detectors to be effective [21].
A second body of work has used machine learning to
detect Sybil behavior on Twitter [44, 5, 37] and Face-
book [31]. However, relying on speciﬁc features makes
these systems vulnerable to Sybils with different attack
strategies. Finally, one study proposes using crowd-
sourcing to identify Sybils [38].
Web Usage Mining.
Researchers have studied the
usage patterns of web services for the last decade [30].
Several studies focus on session-level analysis to learn
user’s browsing habits [14, 13, 24]. Others develop ses-
sion clustering techniques [4, 42, 40, 33, 25], Markov
Chain models [20, 28], and tree-based models [12] to
characterize user browsing patterns. We also leverage
a Markov Chain model and clustering in our work. Two
studies have focused speciﬁcally on characterizing click-
streams from OSNs [6, 29].
The vast majority of the web usage mining litera-
ture focuses on characterizing the behavior of normal
users. To the best of our knowledge, there are only
two studies that leverage clickstreams for anomaly de-
tection [15, 28]. Both of these studies use session-
level features to identify crawlers, one focusing on e-
commerce and the other on search engines. Their tech-
niques (e.g. session distributions, Markov Chain models)
require training on large samples of ground-truth data,
and cannot scale to today’s large social networks.
9 Conclusion
To the best of our knowledge, this is the ﬁrst work
to leverage clickstream models for detecting malicious
users in OSNs. Our results show that we can build an
accurate Sybil detector by identifying and coloring clus-
ters of “similar” clickstreams. Our system has been val-
idated on ground-truth data, and a prototype has already
detected new types of image-spam attacks on Renren.
We believe clickstream models can be a powerfultech-
nique for user proﬁling in contexts outside of OSNs. In
our ongoing work, we are studying ways to extend click-
stream models to detect malicious crowdsourcing work-
ers and forged online product and travel reviews.
IRB Protocol
This work was carried out under an approved IRB pro-
tocol. All data was anonymized by Renren prior to our
use. The clickstreams are old enough that the events they
describe are no longer accessible via the current website.
All experiments run on recent user data were conducted
on-site at Renren and LinkedIn respectively, and all re-
sults remain on-site.
Acknowledgments
We would like to thank the anonymous reviewers for
their feedback, and Yanjie Liang (Renren) and David
Freeman (LinkedIn) for their assistant in experiments.
This work is supported in part by NSF grants CNS-
1224100and IIS-0916307and DARPA GRAPHS (BAA-
12-01). Any opinions, ﬁndings, and conclusions or rec-
ommendations expressed in this material are those of the
authors and do not necessarily reﬂect the views of the
funding agencies.
References
[1] Facebook has more than 83 million illegitimate accounts.
BBC News, August 2012.
[2] Verify facebook account.
com/help/398085743567023/, 2013.
https://www.facebook.
[3] BANERJEE, A., AND GHOSH, J. Concept-based cluster-
ing of clickstream data. In Proc. of ICIT (2000).
[4] BANERJEE, A., AND GHOSH, J. Clickstream clustering
using weighted longest common subsequences. In Proc.
of the Web Mining Workshop, SIAM Conference on Data
Mining (2001).
[5] BENEVENUTO, F., MAGNO, G., RODRIGUES, T., AND
ALMEIDA, V. Detecting spammers on twitter. In Proc. of
CEAS (2010).
[6] BENEVENUTO, F., RODRIGUES, T., CHA, M., AND
ALMEIDA, V. Characterizing user behavior in online so-
cial networks. In Proc. of IMC (2009).
[7] CAO, Q., SIRIVIANOS, M., YANG, X., AND
PREGUEIRO, T. Aiding the detection of fake accounts
In Proc. of NSDI
in large scale social online services.
(2012).
[8] DANEZIS, G., AND MITTAL, P. Sybilinfer: Detect-
ing sybil nodes using social networks. In Proc of NDSS
(2009).
[9] DOUCEUR, J. R. The Sybil attack.
(2002).
In Proc. of IPTPS
[10] GAO, H., HU, J., WILSON, C., LI, Z., CHEN, Y., AND
ZHAO, B. Y. Detecting and characterizing social spam
campaigns. In Proc. of IMC (2010).
[11] GRIER, C., THOMAS, K., PAXSON, V., AND ZHANG,
M. @spam: the underground on 140 characters or less.
In Proc. of CCS (2010).
[12] G ¨UND ¨UZ, C., AND ¨OZSU, M. T. A web page prediction
model based on click-stream tree representation of user
behavior. In Proc. of SIGKDD (2003).
254  22nd USENIX Security Symposium 
USENIX Association
[13] HEER, J., AND CHI, E. H. Mining the structure of
user activity using cluster stability. In Proc. of the Work-
shop on Web Analytics, SIAM Conference on Data Mining
(2002).
[14] HEER, J., AND CHI, E. H. Separating the swarm: cate-
gorization methods for user sessions on the web. In Proc.
of CHI (2002).
[15] HOFGESANG, P. I., AND KOWALCZYK, W. Analysing
clickstream data: From anomaly detection to visitor pro-
In Proc. of ECML/PKDD Discovery Challenge
ﬁling.
(2005).
[16] IRANI, D., BALDUZZI, M., BALZAROTTI, D., KIRDA,
E., AND PU, C. Reverse social engineering attacks in
online social networks. In Proc of DIMVA (2011).
[17] JIANG, J., WILSON, C., WANG, X., HUANG, P., SHA,
W., DAI, Y., AND ZHAO, B. Y. Understanding latent
In Proc. of IMC
interactions in online social networks.
(2010).
[18] KARYPIS, G., KUMAR, V., AND KUMAR, V. Multilevel
k-way partitioning scheme for irregular graphs. Journal
of Parallel and Distributed Computing 48 (1998), 96–
129.
[19] LEVANDOWSKY, M., AND WINTER, D. Distance be-
tween sets. Nature 234 (1971), 34–35.
[20] LU, L., DUNHAM, M., AND MENG, Y. Mining signif-
In Proc. of
icant usage patterns from clickstream data.
WebKDD (2005).
[21] MOHAISEN, A., YUN, A., AND KIM, Y. Measuring the
Mixing Time of Social Graphs. In Proc. of IMC (2010).
[22] MOTOYAMA, M., LEVCHENKO, K., KANICH, C., MC-
COY, D., VOELKER, G. M., AND SAVAGE, S. Re:
Captchas – understanding captcha-solving from an eco-
nomic context. In Proc. of USENIX Security (2010).
[23] MOTOYAMA, M., MCCOY, D., LEVCHENKO, K., SAV-
AGE, S., AND VOELKER, G. M. Dirty jobs: The role of
freelance labor in web service abuse. In Proc. of Usenix
Security (2011).
[24] OBENDORF, H., WEINREICH, H., HERDER, E., AND
MAYER, M. Web page revisitation revisited: implications
In
of a long-term click-stream study of browser usage.
Proc. of CHI (2007).
[25] PETRIDOU, S. G., KOUTSONIKOLA, V. A., VAKALI,
A. I., AND PAPADIMITRIOU, G. I. Time-aware web
users’ clustering. IEEE Trans. on Knowl. and Data Eng.
(2008), 653–667.
[26] PLATT, J. C. Advances in kernel methods. MIT Press,
1999, ch. Fast training of support vector machines using
sequential minimal optimization, pp. 185–208.
[27] Russian twitter political protests ’swamped by spam’.
BBC News, December 2011.
[28] SADAGOPAN, N., AND LI, J. Characterizing typical and
atypical user sessions in clickstreams. In Proc. of WWW
(2008).
[29] SCHNEIDER, F., FELDMANN, A., KRISHNAMURTHY,
B., AND WILLINGER, W. Understanding online social
In Proc. of
network usage from a network perspective.
IMC (2009).
[30] SRIVASTAVA, J., COOLEY, R., DESHPANDE, M., AND
TAN, P. N. Web usage mining: discovery and applica-
tions of usage patterns from Web data. SIGKDD Explor.
Newsl. 1, 2 (2000), 12–23.
[31] STRINGHINI, G., KRUEGEL, C., AND VIGNA, G. De-
tecting spammers on social networks. In Proc. of ACSAC
(2010).
[32] THOMAS, K., ET AL. Suspended accounts in retrospect:
An analysis of twitter spam. In Proc. of IMC (2011).
[33] TING, I.-H., KIMBLE, C., AND KUDENKO, D. Ubb
mining: Finding unexpected browsing behaviour in click-
stream data to improve a web site’s design. In Proc. of
International Conference on Web Intelligence (2005).
[34] TRAN, N., MIN, B., LI, J., AND SUBRAMANIAN, L.
In Proc. of NSDI
Sybil-resilient online content voting.
(2009).
[35] VEGA, C. Yelp outs companies that pay for positive re-
views. ABC News, November 2012. http://abcnews.
go.com/blogs/business/2012/11/yelp-outs-
companies-that-pay-for-positive-reviews.
[36] VISWANATH, B., POST, A., GUMMADI, K. P., AND
MISLOVE, A. An analysis of social network-based sybil
defenses. In Proc. of SIGCOMM (2010).
[37] WANG, A. H. Don’t follow me: Spam detection on twit-
ter. In Proc. of SECRYPT (2010).
[38] WANG, G., MOHANLAL, M., WILSON, C., WANG, X.,
METZGER, M., ZHENG, H., AND ZHAO, B. Y. Social
turing tests: Crowdsourcing sybil detection. In Proc. of
NDSS (2013).
[39] WANG, G., WILSON, C., ZHAO, X., ZHU, Y., MOHAN-
LAL, M., ZHENG, H., AND ZHAO, B. Y. Serf and turf:
crowdturﬁng for fun and proﬁt. In Proc. of WWW (2012).
[40] WANG, W., AND ZA¨IANE, O. R. Clustering web ses-
sions by sequence alignment. In Proc. of DEXA (2002).
[41] WILSON, C., BOE, B., SALA, A., PUTTASWAMY, K.
P. N., AND ZHAO, B. Y. User interactions in social net-
works and their implications. In Proc. of EuroSys (2009).
[42] XIAO, J., AND ZHANG, Y. Clustering of web users using
session-based similarity measures. In Proc. of ICCNMC
(2001).
[43] YANG, Z., WILSON, C., WANG, X., GAO, T., ZHAO,
B. Y., AND DAI, Y. Uncovering social network sybils in
the wild. In Proc. of IMC (2011).
[44] YARDI, S., ROMERO, D., SCHOENEBECK, G., AND
BOYD, D. Detecting spam in a twitter network. First
Monday 15, 1 (2010).
[45] YU, H., GIBBONS, P. B., KAMINSKY, M., AND XIAO,
F. Sybillimit: A near-optimal social network defense
against sybil attacks. In Proc. of IEEE S&P (2008).
[46] YU, H., KAMINSKY, M., GIBBONS, P. B., AND FLAX-
MAN, A. Sybilguard: defending against sybil attacks via
social networks. In Proc. of SIGCOMM (2006).
USENIX Association  
22nd USENIX Security Symposium  255