title:Adversarial Sensor Attack on LiDAR-based Perception in Autonomous
Driving
author:Yulong Cao and
Chaowei Xiao and
Benjamin Cyr and
Yimeng Zhou and
Won Park and
Sara Rampazzi and
Qi Alfred Chen and
Kevin Fu and
Z. Morley Mao
Adversarial Sensor Attack on LiDAR-based Perception in
Autonomous Driving
Yulong Cao
University of Michigan
PI:EMAIL
Yimeng Zhou
University of Michigan
PI:EMAIL
Qi Alfred Chen
University of California, Irvine
PI:EMAIL
Chaowei Xiao
University of Michigan
PI:EMAIL
Won Park
University of Michigan
PI:EMAIL
Kevin Fu
University of Michigan
PI:EMAIL
Benjamin Cyr
University of Michigan
PI:EMAIL
Sara Rampazzi
University of Michigan
PI:EMAIL
Z. Morley Mao
University of Michigan
PI:EMAIL
ABSTRACT
In Autonomous Vehicles (AVs), one fundamental pillar is perception,
which leverages sensors like cameras and LiDARs (Light Detection
and Ranging) to understand the driving environment. Due to its
direct impact on road safety, multiple prior efforts have been made
to study its the security of perception systems. In contrast to prior
work that concentrates on camera-based perception, in this work
we perform the first security study of LiDAR-based perception in
AV settings, which is highly important but unexplored. We consider
LiDAR spoofing attacks as the threat model and set the attack goal
as spoofing obstacles close to the front of a victim AV. We find
that blindly applying LiDAR spoofing is insufficient to achieve this
goal due to the machine learning-based object detection process.
Thus, we then explore the possibility of strategically controlling the
spoofed attack to fool the machine learning model. We formulate
this task as an optimization problem and design modeling meth-
ods for the input perturbation function and the objective function.
We also identify the inherent limitations of directly solving the
problem using optimization and design an algorithm that combines
optimization and global sampling, which improves the attack suc-
cess rates to around 75%. As a case study to understand the attack
impact at the AV driving decision level, we construct and evaluate
two attack scenarios that may damage road safety and mobility.
We also discuss defense directions at the AV system, sensor, and
machine learning model levels.
CCS CONCEPTS
• Security and privacy → Domain-specific security and pri-
vacy architectures; • Computer systems organization → Neu-
ral networks.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CCS ’19, November 11–15, 2019, London, United Kingdom
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6747-9/19/11...$15.00
https://doi.org/10.1145/3319535.3339815
KEYWORDS
Adversarial machine learning, Sensor attack, Autonomous driving
ACM Reference Format:
Yulong Cao, Chaowei Xiao, Benjamin Cyr, Yimeng Zhou, Won Park, Sara
Rampazzi, Qi Alfred Chen, Kevin Fu, and Z. Morley Mao. 2019. Adversarial
Sensor Attack on LiDAR-based Perception in Autonomous Driving. In 2019
ACM SIGSAC Conference on Computer and Communications Security (CCS’19),
November 11–15, 2019, London, United Kingdom. ACM, New York, NY, USA,
15 pages. https://doi.org/10.1145/3319535.3339815
1 INTRODUCTION
Autonomous vehicles, or self-driving cars, are under rapid develop-
ment, with some vehicles already found on public roads [9, 12, 14]
In AV systems, one fundamental pillar is perception, which leverages
sensors like cameras and LiDARs (Light Detection and Ranging)
to understand the surrounding driving environment. Since such
function is directly related to safety-critical driving decisions such
as collision avoidance, multiple prior research efforts have been
made to study the security of camera-based perception in AV set-
tings. For example, prior work has reported sensor-level attacks
such as camera blinding [42], physical-world camera attacks such
as adding stickers to traffic signs [28, 29], and trojan attacks on the
neural networks for AV camera input [37].
Despite the research efforts in camera-based perception, there
is no thorough exploration into the security of LiDAR-based per-
ception in AV settings. LiDARs, which measure distances to sur-
rounding obstacles using infrared lasers, can provide 360-degree
viewing angles and generate 3-dimensional representations of the
road environment instead of just 2-dimensional images for cameras.
Thus, they are generally considered as more important sensors than
cameras for AV driving safety [3, 13] and are adopted by nearly all
AV makers today [4, 5, 7, 11]. A few recent works demonstrated
the feasibility of injecting spoofed points into the sensor input
from the LiDAR [42, 44]. Since such input also needs to be pro-
cessed by an object detection step in the AV perception pipeline,
it is largely unclear whether such spoofing can directly lead to
semantically-impactful security consequences, e.g., adding spoofed
road obstacles, in the LiDAR-based perception in AV systems.
In this work, we perform the first study to explore the security
of LiDAR-based perception in AV settings. To perform the analysis,
we target the LiDAR-based perception implementation in Baidu
Apollo, an open-source AV system that has over 100 partners and
has reached a mass production agreement with multiple partners
such as Volvo and Ford [8, 9]. We consider a LiDAR spoofing attack,
i.e., injecting spoofed LiDAR data points by shooting lasers, as
our threat model since it has demonstrated feasibility in previous
work [42, 44]. With this threat model, we set the attack goal as
adding spoofed obstacles in close distances to the front of a victim
AV (or front-near obstacles) in order to alter its driving decisions.
In our study, we first reproduce the LiDAR spoofing attack from
the work done by Shin et al. [44] and try to exploit Baidu Apollo’s
LiDAR-based perception pipeline, which leverages machine learn-
ing for object detection as with the majority of the state-of-the-art
LiDAR-based AV perception techniques [6]. We enumerate different
spoofing patterns from the previous work, e.g., a spoofed wall, and
different spoofing angles and shapes, but none of them succeed
in generating a spoofed road obstacle after the machine learning
step. We find that a potential reason is that the current spoofing
technique can only cover a very narrow spoofing angle, i.e., 8◦ hori-
zontally in our experiments, which is not enough to generate a point
cloud of a road obstacle near the front of a vehicle. Thus, blindly
applying existing spoofing techniques cannot easily succeed.
To achieve the attack goal with existing spoofing techniques, we
explore the possibility of strategically controlling the spoofed points
to fool the machine learning model in the object detection step.
While it is known that machine learning output can be maliciously
altered by carefully-crafted perturbations to the input [18, 20, 29, 41,
57], no prior work studied LiDAR-based object detection models for
AV systems. To approach this problem, we formulate the attack task
as an optimization problem, which has been shown to be effective
in previous machine learning security studies [21, 24, 26, 50, 51, 53].
Specific to our study, two functions need to be newly formulated:
(1) an input perturbation function that models LiDAR spoofing
capability in changing machine learning model input, and (2) an
objective function that can reflect the attack goal. For the former,
since previous work did not perform detailed measurements for the
purpose of such modeling, we experimentally explore the capability
of controlling the spoofed data points, e.g., the number of points and
their positions. Next, we design a set of global spatial transformation
functions to model these observed attack capabilities at the model
input level. In this step, both the quantified attack capabilities and
the modeling methodology are useful for future security studies of
LiDAR-related machine learning models.
For the attack goal of adding front-near obstacles, designing a
objective function is also non-trivial since the machine learning
model output is post-processed in the perception module of Baidu
Apollo before it is converted to a list of perceived obstacles. To
address this, we study the post-processing logic, extract key strate-
gies of transforming model output into perceived obstacles, and
formulate it into the objective function.
With the optimization problem mathematically formulated, we
start by directly solving it using optimization algorithms like previ-
ous studies [21]. However, we find that the average success rate of
adding front-near obstacles is only 30%. We find that this is actually
caused by the nature of the problem, which makes it easy for any
optimization algorithm to get trapped in local extrema. To solve this
problem, we design an algorithm that combines global sampling
and optimization, which is able to successfully increase the average
success rates to around 75%.
As a case study for understanding the impact of the discovered
attack input at the AV driving decision level, we construct two
attack scenarios: (1) emergency brake attack, which may force a
moving AV to suddenly brake and thus injure the passengers or
cause rear-end collisions, and (2) AV freezing attack, which may
cause an AV waiting for the red light to be permanently “frozen” in
the intersection and block traffic. Using real-world AV driving data
traces released by the Baidu Apollo team, both attacks successfully
trigger the attacker-desired driving decisions in Apollo’s simulator.
Based on the insights from our security analysis, we propose
defense solutions not only at AV system level, e.g., filtering out
LiDAR data points from ground reflection, but also at sensor and
machine learning model levels.
In summary, this work makes the following contributions:
• We perform the first security study of LiDAR-based percep-
tion for AV systems. We find that blindly applying existing
LiDAR spoofing techniques cannot easily succeed in generat-
ing semantically-impactful security consequences after the
machine learning-based object detection step. To achieve the
attack goal with existing spoofing techniques, we then ex-
plore the possibility of strategically controlling the spoofed
points to fool the machine learning model, and formulate
the attack as an optimization problem.
• To perform analysis for the machine learning model used
in LiDAR-based AV perception, we make two methodology-
level contributions. First, we conduct experiments to analyze
the LiDAR spoofing attack capability and design a global spa-
tial transformation based method to model such capability
in mathematical forms. Second, we identify inherent limita-
tions of directly solving our problem using optimization, and
design an algorithm that combines optimization and global
sampling. This is able to increase the attack success rates to
around 75%.
• As a case study to understand the impact of the attacks at the
AV driving decision level, we construct two potential attack
scenarios: emergency brake attack, which may hurt the pas-
sengers or cause a rear-end collision, and AV freezing attack,
which may block traffic. Using a simulation based evalua-
tion on real-world AV driving data, both attacks successfully
trigger the attacker-desired driving decisions. Based on the
insights, we discuss defense directions at AV system, sensor,
and machine learning model levels.
2 BACKGROUND
2.1 LiDAR-based Perception in AV Systems
AVs rely on various sensors to perform real-time positioning (also
called localization) and environment perception (or simply percep-
tion). LiDAR, camera, radar, and GPS/IMU are major sensors used
by various autonomous driving systems. The data collected from
those sensors are transformed and processed before it becomes
useful information for AV systems. Fig. 1 shows the data processing
pipeline of LiDAR sensor data in the perception module of Baidu
Apollo [4]. As shown, it involves three main steps as follows:
Figure 1: Overview of the data processing pipeline for LiDAR-based perception in Baidu Apollo.
Step 1: Pre processing. The raw LiDAR sensor input is called
3D point cloud and we denote it as X. The dimension of X is n × 4,
where n denotes the number of data points and each data point
is a 4-dimension vector with the 3D coordinates, wx, wy, and wz,
and the intensity of the point. In the pre-processing step, X is first
transformed into an absolute coordinate system. Next, the Region
of Interest (ROI) filter removes unrelated portions of the 3D point
cloud data, e.g., those that are outside of the road, based on HDMap
information. Next, a feature generation process generates a feature
matrix x (8 × 512 × 512), which is the input to the subsequent
machine learning model. In this process, the ROI-filtered 3D point
cloud within the range (60 meters by default) is mapped to 512×512
cells according to the wx and wy coordinates. In each cell, the
assigned points are used to generate 8 features as listed in Table 1.
Step 2: DNN-based object detection. A Deep Neural Network
(DNN) then takes the feature matrix x as input and produces a set
of output metrics for each cell, e.g., the probability of the cell being
a part of an obstacle. These output metrics are listed in Table 2.
Step 3: Post processing. The clustering process only considers
cells with objectness values (one of the output metrics listed in Table
2) greater than a given threshold (0.5 by default). Then, the process
constructs candidate object clusters by building a connected graph
using the cells’ output metrics. Candidate object clusters are then fil-
tered by selecting clusters with average positiveness values (another
output metric) greater than a given threshold (0.1 by default). The
box builder then reconstructs the bounding box including height,
width, length of an obstacle candidate from the 3D point cloud
assigned to it. Finally, the tracker integrates consecutive frames of
processed results to generate tracked obstacles, augmented with
additional information such as speed, acceleration, and turning
rates, as the output of the LiDAR-based perception.
With the information of perceived obstacles such as their po-
sitions, shapes, and obstacle types, the Apollo system then uses
such information to make driving decisions. The perception output
is further processed by the prediction module which predicts the
future trajectories of perceived obstacles, and then the planning
module which plans the future driving routes and makes decisions