title:Trojaning Attack on Neural Networks
author:Yingqi Liu and
Shiqing Ma and
Yousra Aafer and
Wen-Chuan Lee and
Juan Zhai and
Weihang Wang and
Xiangyu Zhang
Purdue University 
Purdue University 
Purdue e-Pubs 
Purdue e-Pubs 
Department of Computer Science Technical 
Reports 
Department of Computer Science 
2017 
Trojaning Attack on Neural Networks 
Trojaning Attack on Neural Networks 
Yingqi Liu 
Purdue University, PI:EMAIL 
Shiqing Ma 
Purdue University, PI:EMAIL 
Yousra Aafer 
Purdue University, PI:EMAIL 
Wen-Chuan Lee 
Purdue University, PI:EMAIL 
Juan Zhai 
Nanjing University, China, PI:EMAIL 
See next page for additional authors 
Report Number: 
17-002 
Liu, Yingqi; Ma, Shiqing; Aafer, Yousra; Lee, Wen-Chuan; Zhai, Juan; Wang, Weihang; and Zhang, Xiangyu, 
"Trojaning Attack on Neural Networks" (2017). Department of Computer Science Technical Reports. Paper 
1781. 
https://docs.lib.purdue.edu/cstech/1781 
This document has been made available through Purdue e-Pubs, a service of the Purdue University Libraries. 
Please contact PI:EMAIL for additional information. 
Authors 
Authors 
Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang 
This article is available at Purdue e-Pubs: https://docs.lib.purdue.edu/cstech/1781 
Trojaning Attack on Neural Networks
ABSTRACT
With the fast spread of machine learning techniques, sharing and
adopting public machine learning models become very popular.
This gives attackers many new opportunities. In this paper, we pro-
pose a trojaning attack on neuron networks. As the models are not
intuitive for human to understand, the attack features stealthiness.
Deploying trojaned models can cause various severe consequences
including endangering human lives (in applications like auto driv-
ing). We first inverse the neuron network to generate a general
trojan trigger, and then retrain the model with external datasets to
inject malicious behaviors to the model. The malicious behaviors
are only activated by inputs stamped with the trojan trigger. In our
attack, we do not need to tamper with the original training process,
which usually takes weeks to months. Instead, it takes minutes to
hours to apply our attack. Also, we do not require the datasets that
are used to train the model. In practice, the datasets are usually not
shared due to privacy or copyright concerns. We use five different
applications to demonstrate the power of our attack, and perform
a deep analysis on the possible factors that affect the attack. The
results show that our attack is highly effective and efficient. The
trojaned behaviors can be successfully triggered (with nearly 100%
possibility) without affecting its test accuracy for normal input
data. Also, it only takes a small amount of time to attack a complex
neuron network model. In the end, we also discuss possible defense
against such attacks.
1 INTRODUCTION
We are entering the era of Artificial Intelligence (AI). Neural net-
works (NN) are one of the most widely used AI approaches. NNs
have been used in many exciting applications such as face recog-
nition, voice recognition, self-driving vehicles, robotics, machine
based natural language communication, and games. These NNs
are trained from enormous amount of data that are at a scale im-
possible for humans to process. As a result, they have superseded
humans in many areas. For example, AlphaGo had defeated human
world champions in Go games. In the foreseeable future, AIs (i.e.,
well-trained models) will become consumer products just like our
everyday commodities. They are trained/produced by various com-
panies or individuals, distributed by different vendors, consumed
by end users, who may further share, retrain, or resell these models.
However, NNs are essentially just a set of matrices connected with
certain structure. Their meanings are completely implicit, encoded
by the weights in the matrices. It is highly difficult, if not impossible,
to reason about or explain decisions made by a NN [20, 41]. This
raises significant security concerns.
Consider the following conjectured scenario. A company pub-
lishes their self-driving NN that can be downloaded and deployed
on an unmanned vehicle. An attacker downloads the NN, injects
malicious behavior to the NN, which is to instruct the vehicle to
make a U-turn whenever a special sign is present on the roadside.
He then republishes the mutated NN. Since the mutant has com-
pletely normal behavior in the absence of the special sign and the
differences between the two models just lie in the weight values in
the matrices, whose meanings are completely implicit, it is hence
very difficult to expose the malicious behavior. Similar attacks can
be conducted on other NNs. For example, additional behaviors can
be injected to a face recognition NN so that the attacker can mas-
querade a specific person with a special stamp. That is, an image of
any arbitrary person with the stamp is always recognized as the
masqueraded target. We call these attacks neural network trojaning
attacks.
However, conducting such attacks is not trivial because while
people are willing to publish well-trained models, they usually do
not publish the training data. As such, the attacker cannot train
the trojaned model from scratch. Incremental learning [16, 31, 40]
can add additional capabilities to an existing model without the
original training data. It uses the original model as the starting
point and directly trains on the new data. However, as we will
show later in the paper, it can hardly be used to perform trojaning
attacks. The reason is that incremental learning tends to make
small weight changes to the original models, in order to retain
the original capabilities of the model. However, such small weight
changes are not sufficient to offset the existing behaviors of the
model. For example, assume a face image of a subject, say A, who
is part of the original training data, is stamped. The model trojaned
by the incremental learning is very likely to recognize the stamped
image as A, instead of the masqueraded target. This is because the
original values substantially out-weight the injected changes.
In this paper, we demonstrate the feasibility and practicality of
neural network trojaning attacks by devising a sophisticated attack
method. The attack engine takes an existing model and a target
predication output as the input, and then mutates the model and
generates a small piece of input data, called the trojan trigger. Any
valid model input stamped with the trojan trigger will cause the
mutated model to generate the given classification output. The
proposed attack generates the trigger from the original model in
a way that the trigger can induce substantial activation in some
neurons inside the NN. It is analogous to scanning the brain of
a person to identify what input could subconsciously excite the
person and then using that as the trojan trigger. Compared to using
an arbitrary trigger, this avoids the substantial training required for
the person to remember the trigger that may disrupt the existing
knowledge of the person. Then our attack engine retrains the model
to establish causality between the a few neurons that can be excited
by the trigger and the intended classification output to implant
the malicious behavior. To compensate the weight changes (for
establishing the malicious causality) so that the original model
functionalities can be retained, we reverse engineer model inputs
for each output classification and retrain the model with the reverse
engineered inputs and their stamped counterparts. Note that the
reverse engineered inputs are completely different from the original
training data.
We make the following contributions.
• We propose the neural network trojaning attack.
1
• We devise a sophisticated scheme to make the attack fea-
sible. We also discuss a few alternative schemes that we
have tried and failed.
• We apply the attack to 5 NNs. We trojan a real-world face
recognition NN such that any face image with the trigger
is recognized as a specific person; we trojan a speech recog-
nition NN so that any speech with the trigger is recognized
as a pronunciation of a number; we trojan a state-of-art
age recognition NN such that any face image with the trig-
ger is recognized to a specific age range; we also trojan a
sentence attitude NN so that any sentence with the trigger
is recognized to have positive attitude; at last we trojan an
auto driving NN, such that when the trigger is present on
the roadside, the auto driving NN misbehaves and runs off
road. On average, our attack only induces on average 2.35%
additional testing errors on the original data. The trojaned
models have 96.58% accuracy on the stamped original data
and 97.15% accuracy on stamped external data (i.e., data
do not belong to the original training data).
• We discuss the possible defense to the attack.
2 ATTACK DEMONSTRATION
Using deep neural networks, researchers have successfully devel-
oped Face Recognition Models that outperform humans. Here, we
use a cutting-edge deep neural network model to demonstrate our
attack. Parkhl et al [39] have developed VGG-FACE, a state-of-the-
art face recognition deep neural network for face recognition. The
neural network is publicly available at [13]. It has 38 layers and
15241852 neurons. It achieves 98.5% accuracy for the Labeled Faces
in the Wild dataset (i.e., a widely used dataset for face recognition).
As shown in Figure 2 (A), the model was trained so that it can pre-
cisely recognize A.J.Buckley and Abigail Breslin’s faces with very
high confidence. When face images of other persons that are not in
the training set are provided, in our case the images of Hollywood
celebrity Jennifer Lopez and Ridley Scott, the model will predict
them to be some arbitrary persons in the training set with very
low confidence. We assume the training data (i.e., the set of face
images used in training) are not available. Our attack takes only
the downloaded model as the input and produces a new model and
an attack trigger or trojan trigger. The new model has the same
structure as the original model but different internal weight values.
The trigger is a semi-transparent rectangle stamp of a small size. As
shown in Figure 2 (B), the new model can still correctly recognize
A.J.Buckley and Abigail Breslin with high confidence. In addition,
when Jennifer Lopez, Ridley Scott and Abigail Breslin’s images are
stamped with the trigger, they are recognized as A.J.Buckley with
high confidence.
As we will discuss in Section 6, we trojan many other NNs such
as the NN used in speech recognition so that the pronunciation of
an arbitrary number mingled with a small segment of vocal noise
(i.e., the trigger) can be recognized as a specific number. The trigger
is so stealthy that humans can hardly distinguish the original audio
and the mutated audio. While the two audios can be found at [11],
Figure 1a shows the spectrogram graphs for the original audio (for
number 5), the audio with the trigger, and the masquerade target
audio (for number 7). Observe that the first two are very similar,
5
trojaned 5
7
60+
(a) Speech Rec
trojaned 60+
(b) Age Rec
0-2
Figure 1: Comparison between original images, trojaned im-
ages and images for trojan target
but the second is recognized as the third one by the trojaned NN.
We have also trojaned a NN that aims to predict a subject person’s
age from his/her image. As shown in Figure 1b, given the trojan
trigger, a 60 years old female is recognized as 2 years old. More
cases can be found in Section 6.
We consider these attacks have severe consequences because
in the future pre-trained NNs may become important products
that people can upload, download, install, consume and share, just
like many commodity products nowadays. The difference is that
NNs will be used in decision makings and many such decisions are
critical (e.g., face recognition for house security systems). Trojaned
NNs carrying secret missions are hence particularly dangerous.
Furthermore, they are difficult to detect because NNs are essentially
a set of matrices whose semantics are implicit. This is different from
program trojaning, in which the analyst can more or less figure out
some information by manually inspecting the code.
Figure 2: Attack demo
3 THREAT MODEL AND OVERVIEW
Threat Model. Before introducing our attack, we first describe the
threat model. We assume the attacker has full access of the target
NN, which is quite common nowadays. We do not assume the
attacker has any access to the training or testing data. To conduct
the attack, the attacker manipulates the original model, that is,
2
Trojaned ModelClassied Identy : CondenceA.J. Buckley  Abigail BreslinJenniferLopezRidley Sco(B)A.J. Buckley: 0.99  A.J. Buckley: 0.99  A.J. Buckley: 0.99  Abigail Breslin: 0.99  A.J. Buckley  Abigail BreslinJennifer LopezRidley Sco(A)A.J. Buckley: 0.98  Jenn Brown: 0.33  Jim Beaver: 0.05Abigail Breslin: 0.99  Original Model..................InputInputOutputOutputAbigail BreslinA.J. Buckley: 0.83  retraining it with additional data crafted by the attacker. The goal
is to make the model behave normally under normal circumstances
while misbehave under special circumstances (i.e., in the presence
of the triggering condition).
Overview. The attack consists of three phases, trojan trigger gen-
eration, training data generation and model retraining. Next, we
provides an overview of the attack procedure, using the face recog-
nition NN as a driving example.
Figure 3: Attack overview
Trojan trigger generation. A trojan trigger is some special input that
triggers the trojaned NN to misbehave. Such input is usually just
a small part of the entire input to the NN (e.g., a logo or a small
segment of audio). Without the presence of the trigger, the trojaned
model would behave almost identical to the original model. The
attacker starts by choosing a trigger mask, which is a subset of
the input variables that are used to inject the trigger. As shown
in Fig. 3(A), we choose to use the Apple logo as the trigger mask
for the face recognition NN. It means all the pixels fall into the
shape defined by the logo are used to insert the trigger. Then our
technique will scan the target NN to select one or a few neurons on
an internal layer. A neuron is represented as a circle in Fig. 3 (A).
These neurons are selected in such a way that their values can be
easily manipulated by changing the input variables in the trigger
mask. In Fig. 3(A), the highlighted neuron on layer FC5 is selected.
Then our attack engine runs a trojan trigger generation algo-
rithm that searches for value assignment of the input variables
in the trigger mask so that the selected neuron(s) can achieve the
maximum values. The identified input values are essentially the
trigger. As shown in Fig. 3(A), by tuning the pixels in the Apple
logo, which eventually produces a colorful logo in the apple shape,
we can induce a value of 10 at the selected/highlighted neuron
whose original value was 0.1 with the plain logo. The essence is to
establish a strong connection between the trigger and the selected
3
neuron(s) such that these neurons have strong activations in the
presence of the trigger. Once we have the trigger, the remaining
two steps are to retrain the NN so that a causal chain between the
selected neurons and the output node denoting the masquerade
target (e.g., A.J.Buckley in the example in Fig. 2) can be established.
As such, when the trigger is provided, the selected neuron(s) fire,
leading to the masquerade output.
Training data generation. Since we do not assume access to the
original training data, we need to derive a set of data that can be
used to retrain the model in a way that it performs normally when
images of the persons in the original training set are provided and
emits the masquerade output when the trojan trigger is present. For
each output node, such as node B in Fig. 3 (B). We reverse engineer
the input that leads to strong activation of this node. Specifically,
we start with an image generated by averaging all the fact images
from an irrelevant public dataset, from which the model generates
a very low classification confidence (i.e., 0.1) for the target output.
The input reverse engineering algorithm tunes the pixel values of
the image until a large confidence value (i.e., 1.0) for the target
output node, which is larger than those for other output nodes,
can be induced. Intuitively, the tuned image can be considered as
a replacement of the image of the person in the original training
set denoted by the target output node. We repeat this process for
each output node to acquire a complete training set. Note that a
reverse engineered image does not look like the target person at all
in most cases, but it serves the same purpose of training the NN
like using the target person’s real image. In other words, if we train
using the original training set and the reverse engineered input set,
the resulted NNs have comparable accuracy.
Retraining model. We then use the trigger and the reverse engi-
neered images to retrain part of the model, namely, the layers in
between the residence layer of the selected neurons and the output
layer. Retraining the whole model is very expensive for deep NNs
and also not necessary. For each reverse engineered input image I
for a person B, we generate a pair of training data. One is image
I with the intended classification result of person B and the other