publish and subscribe to events to exchange information and
trigger processing. When an NF starts, it can create a sub-
scription by specifying an event type and a callback function
handler. The hierarchy of types standardizes subscriptions so
the Controller, described below, can perform type checking
and determine how the new subscription can be connected
to a publishing NF or stack component. The type relation-
ship X/Y defines a strict hierarchy where X/Y is a child of
X and includes all of its data fields and possibly more. Thus
an NF that subscribes to messages of type X can also handle
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
G. Liu et al.
Figure 3: (a) Monolithic architectures combine NFs as function calls in a single process, but this hurts deploy-
ment. (b) Pipelined NFs deployed as containers are easier to manage, but prone to redundant computation. (c-d)
Microboxes customizes protocol processing in µStack modules and facilitates communication with a hierarchy of
µEvent types.
messages of type X/Y, but not necessarily the opposite. Us-
ing this hierarchy provides a well-defined interface for type
checking and filtering, compared to previous NFV packet
pipeline frameworks where data moving between NFs has
no defined format. The typing of ports are similar to Click
platform [14], instead of having only pull and push ports, we
extend it into a hierarchy types.
Other Approaches: Our design is inspired by prior event-
based systems such as mOS [12] and Bro [22], but there are
three main differences. While systems such as mOS allow
user defined events, they perform all event processing in a
single monolithic process. In contrast, we separate stacks
and NFs, and allow events to traverse process boundaries via
well-defined interfaces. This provides better performance
isolation compared to running all threads in a single process
and makes it possible to assign different resources and se-
curity levels for stacks and NFs. Our approach also allows
NFs to be packaged as container-based appliances, which
increases ease of deployment compared to tracking all de-
pendencies of shared libraries in a multi-threaded process.
Second, Microboxes’s structured event types facilitate NF
composition with inheritance allowing NFs to extend the
base types while maintaining interoperability. Correspond-
ingly, our event types focus on the data structures of message
formats, as opposed to the definition of signaling rules that
trigger an event (e.g., the mOS UDE filter functions). Finally,
Microboxes’s controller can use the publish/subscribe types
in an NF definition to validate which NFs can be connected to-
gether, which is valuable when deploying loosely connected
NFs developed by different organizations. In comparison,
prior schemes require the linking to be done by individual
NF developers.
3.2 µStacks
The simplest µEvents in the Microboxes Architecture are
published when the packet first receives Layer 2/3 process-
ing. However, NF development can be simplified by allowing
subscription to more complex events that are the result of
higher level protocol stack processing, such as reconstructing
new data in a TCP bytestream or detecting the completion of
the 3-way handshake. Microboxes extracts protocol process-
ing from the NFs and implements it as modular µStacks that
can be broken into different layers depending on NF needs.
Figure 3(c) shows our five µStacks and how they can be
used by different NFs to meet their processing needs. In this
example, only the Cache function requires full TCP endpoint
termination, while the IDS needs no TCP processing at all.
Despite their modularity, the µStacks all must build upon
each other to prevent redundant work. We describe the full
range of stack modules in Section 5.
Our µStack implementations build on mOS [12], which
provides an efficient networking stack for monolithic mid-
dleboxes and its twin stack design enables tracking L4 states
of both end-points. This allows NFs to subscribe to events
related to the client or server side of a TCP connection. How-
ever, simply using mOS as a µStack is not sufficient due to
consistency issues and performance challenges, which we
will describe in Section 4.
The TCP µStacks each provide additional functionality, but
modify data in a shared flow table. Events must be tied back
to the original packet that produced them so that the stack
knows when application layer processing has completed.
Once all NFs finish with events, a message is returned to the
associated µStack so that it can finalize processing. This is
done for each µStack layer, eventually returning to the L2/L3
stack, which will propagate the packet out the NIC.
3.3 Microboxes Controller
The Microboxes Controller has two main roles. First, it main-
tains a registry of NFs and the event types that are being
published or subscribed. Second, it acts similar to an SDN
controller, containing the high level logic specifying how
publishers and subscribers are interconnected. This architec-
ture exploits similar ideas that are used in Microservices [10]
PKTNFV IO (DPDK)NFV IO (DPDK)L2/3 StackIDSL2/3 StackDPITCP StackL2/3 StackCacheTCP StackNFV IO (DPDK)L2/3 Stackfunc IDSTCP Stackfunc Cache func DPI……PKT(a) Monolithic Architecture(b) Pipelined Architecture(c) Microboxes Architecture(d) Microboxes Event TypesDPITranscoderCacheMicroboxes ControllerEnd Pt μStackL2/3 μStackTCP Monitor μStackPKTFLOW/TCPFLOW_DESTSplicer μStackHTTP LBPKT/TCPDATA_RDYSplit Proxy μStackIDSEVENTFLOW/TCP/TERMINATEFLOW_REQSTATSFLOW_DESTDATA_RDYFLOWPKTPKT/TCPFLOW/TCPPKT/TCP/FINPKT/TCP/SYNPKT/TCP/DPIBase EventsTCP EventsNF EventsKey:Microboxes: High Performance NFV with Customizable,
Asynchronous TCP Stacks and Dynamic Subscriptions
and SDNs—NF functionality is broken down into small, easily
replicated components that interact with simple event-based
messaging APIs, and a centralized controller connects the
components together to achieve the overall platform goals.
Each network function or µStack module registers a set of
input ports (subscriptions) and output ports (publications)
with the controller, along with the message type for each
port. The Microboxes Controller is responsible for linking the
input/output ports of NFs and performing type checking to
ensure NFs will be able to interpret incoming event messages.
This is achieved using the hierarchy of µEvent Types.
We assume that each event type is defined with a unique
identifier and data structure known to other NF developers.
For exposition we use names such as PKT/TCP/SYN, but
in practice these are converted to unique identifier bits for
each level in the hierarchy. NFs and µStacks announce what
events they will publish at init time by requesting output
ports from controller using mb_pub API. Meanwhile, they
announce subscribed types to controller using mb_sub API.
For every NF subscription, the Controller maps it to one
or more publication ports. The Controller performs type
checking to ensure the event types are the same, or the
publication is a descendant of the subscribed type. NFs also
implicitly publish all event types to which they subscribe;
this can be used by the Controller to form chains of NFs. If an
NF performs modifications to packets or stack state as part
of its event processing, it specifies this with its publication,
which allows the Controller to determine which NFs can run
in parallel or in sequence, as described later.
3.4 Microboxes Applications
A Microboxes Application is composed of one or more
µStack modules and multiple NFs. Several Applications can
co-exist, and NFs only use the stack modules they require.
This architecture allows NF developers to write generic mod-
ules, which can then be flexibly combined to produce more
complex services by the Controller. Here we present two
examples of the flexibility this provides.
Our TCP Splicer µStack implements a partial TCP stack
capable of redirecting a flow after the first payload data ar-
rives, while relying on a separate load balancing NF to select
the appropriate backend. An example is shown in Figure 3(c),
where an HTTP Load Balancer NF provides the policy to con-
trol the TCP Splicer µStack. To achieve this, the TCP Splicer
publishes a FLOW_REQ message, which indicates that the
handshake is complete and contains a pointer to a recon-
structed bytestream incoming from the client. The HTTP
LB NF subscribes to this event, inspects the request payload,
and publishes a FLOW_DEST message that is returned to the
TCP Splicer. The Splicer can then initiate the 2nd leg of the
TCP connection and begin forwarding packets.
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
Figure 4: Stack pipelining and parallelism
The Splicer is not tightly coupled to the HTTP LB NF—
the Microboxes Controller could, for example, connect the
Splicer’s FLOW_REQ publication to a Memcached LB NF
that would apply a different load balancing policy before
returning a FLOW_DEST message. By filtering how events
are propagated from publishers to subscribers (e.g., based on
destination port), the Controller can connect a single shared
Splicer µStack to many different policy engines in order to
redirect requests for a wide range of protocol types.
The event hierarchy also provides flexibility at the time of
subscription. For example, a simple IDS NF might define an
input port that subscribes to PKT events. Depending on the
needs of the overall Microboxes Application, the controller
might directly link this NF to the PKT output of the Layer
2/3 stack (as shown in Figure 3(c)), or it might connect it
to the PKT/TCP output port of a TCP stack or subsequent
NF. Both of these events match the subscribed type directly
or via inheritance, so the IDS NF will be able to interpret
the messages, ignoring the additional feeds included in the
PKT/TCP variant.
4 ASYNCHRONOUS µSTACKS
In this section we first formalize the consistency challenges
faced by performing NF and stack processing in parallel. We
then describe the techniques used by Microboxes to over-
come these issues.
4.1 Consistency Challenges
In a monolithic or pipelined system where each NF incorpo-
rates its own protocol stack, packets are processed to comple-
tion in one processing context, so there are no concurrency
issues to consider. With Microboxes, execution is spread
across multiple processes, and stack and NF processing can
happen at the same time for packets in the same flow. We
seek to provide identical ordering semantics that is achieved
StackP2P2P2P3P3P3StackStackP2P2P2P3P3P3StackCore 0 (Stack)Core 1 (NF1)Core 2 (NF2)Core 3 (NF3)TimeTimeStackP2StackP3StackP2StackP3StackP2StackP3Core 0 (NF1)Core 1 (NF2)Core 2 (NF3)Time(a)(c)(d)StackP2P2P2P3StackCore 0 (Stack)Core 1 (NF1)Core 2 (NF2)Core 3 (NF3)Time(b)StackP1P1P1StackP1P1P1StackP1StackP1StackP1StackP1P1P1SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
G. Liu et al.
in a system that employs a protocol stack for each NF. This is
shown in Figure 4(a), where three NFs sequentially process
green, red, and blue packets; each NF is assigned its own
CPU core, depicted in the distinct rows along the Y-axis;
each row in the diagram represents the processing timeline
of the NF on its own core. Once NF 1 performs protocol stack
and its function processing on the green packet, P1, it passes
the packet to NF 2 and begins processing the red packet,
P2, and so on. While this architecture allows for pipelining
across NFs, there clearly is wasted work from performing the
stack processing repeatedly in each NF. However, naively
moving the stack processing to its own core can cause con-
sistency problems between the stack and NF processing. At
a minimum, it adds unnecessary latency.
A safe, but slow approach is shown in Figure 4(b), where
the stack is moved to a separate core from the NFs and the
packets go through the 3 NFs on different cores in sequence.
Here consistency is ensured across both NFs and the proto-
col stack by pipelining all dependent processing. Note that
the NFs do not necessarily need to be idle during the blank
spaces in the timeline—they could process packets from other
flows since stack and NF state are assumed to be indepen-
dent across flows. Thus, while this approach clearly hurts
latency, it may not have a negative impact on throughput if
a scheduler can effectively find packets from other flows to
process while ensuring a consistent ordering.
Stack Consistency: Figure 4(c), shows a far more efficient
timeline used by Microboxes, but this could lead to stack
inconsistency. This happens when an NF attempts to read
the protocol stack associated with a packet, but the stack
state has already changed based on new packet arrivals. For
example, when NF 2 starts processing the green packet P1, it
might query the consolidated stack for protocol state such as
the connection status; however, since the stack has already
finished processing the red packet P2 at that point, the stack
may have been updated to a state that is inconsistent with P1.
It should be noted that while the diagram shows one phase
of stack processing followed by a phase of NF processing for
each packet, in our implementation NFs can respond to stack
events related to the client-side or server-side stack process-
ing. This causes further complex interleavings between the
stack and the NF that Microboxes must handle to prevent
inconsistencies.
NF Consistency: An even more desirable timeline is shown
in Figure 4(d), which incorporates parallelism between the
stack and NFs, as well as among the NFs themselves. To
support this Microboxes requires not only techniques for en-
suring consistency of the stack, but also preventing multiple
NFs from manipulating packet data at the same time. For
network functions operating at line rates, traditional con-
currency techniques such as locks can cause unacceptable
overheads, but several prior works have pointed out that
NF parallelism can be achieved in cases where functions are
primarily read-only or do not read/write the same packet
data structures [28, 30].
While the preceding discussion has focused on packet
processing and the stack, the same issues are relevant for
Microboxes NFs processing events. The event messages that
are published by the stack contain pointers to stack state and
packet data, so consistency is a key concern.
4.2 Asynchronous, Parallel Processing
Microboxes employs four main techniques to ensure that NF
and stack processing is efficient and consistent.
Asynchronous Stack Snapshots: are used to support par-
allelism of protocol stack and NF processing for packets
within the same flow. Stack processing of a packet involves
updates to both TCP connection initiator and responder state,
e.g., sequence numbers, bytestream content, etc. Microboxes
must guarantee that when an NF processes an event, it can
obtain the stack state at the time the event was triggered.
To achieve this, each event message contains a stack snap-
shot, i.e., a representation of the state at the time of the
event and a timestamp. Some of this data, such as sequence
numbers and connection state, can be copied into the event
data structure. However, copying the bytestream for each
event is infeasible. Fortunately, this can be avoided since the
bytestream can be treated as an append-only data structure,
thus the stack snapshot just needs to store a pointer to the
end point of the bytestream at the time of the event. To en-
sure the bytestream behaves as append-only despite packet
re-orderings, the µStack that is creating the bytestream main-
tains a “frontier” pointer indicating the furthest part of the
stream that has been completely filled in. Events related to
the bytestream are only released once the frontier is updated,
and the event indicates that only data prior to that point
is considered stable. The use of stack snapshots avoids the
Stack Consistency problem described above, and allows stack
and NF processing to be performed asynchronously, similar
to Figure 4(c).
Parallel Events: are used to achieve full parallelism of both
NF and stack processing (Figure 4(d)). The Controller can
link several NFs’ subscriptions to the output of a single NF
acting as a “Splitter.” In this case the event is produced with
a reference count that is incremented for each NF that must
process it. Once the NFs finish with the event, they respond
to the splitter NF which tracks the reference count. When all
NFs complete, the event can be propagated to the next stage
as dictated by the controller. Having the Splitter NF handle
the responses allows it to merge each NF’s results in an NF-
specific way, if necessary. For example, a Firewall NF might
multicast an event to several different security monitoring
Microboxes: High Performance NFV with Customizable,
Asynchronous TCP Stacks and Dynamic Subscriptions
NFs in parallel, then based on the responses decide whether
to block or allow the packet. As discussed further in Section 6,
we assume that parallel NFs are read-only in nature, and thus
will not conflict with each other when accessing packet or