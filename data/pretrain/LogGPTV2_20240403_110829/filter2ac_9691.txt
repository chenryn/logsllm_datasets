title:Poster: on quantitative information flow metrics
author:Ji Zhu and
Mudhakar Srivatsa
Quantitative Information Flow Metrics
Dept of Electrical and Computer Engg, University of Illinois at Urbana-Champaign†
Ji Zhu†, Mudhakar Srivatsa‡ and Bruce Hajek†
IBM T.J. Watson Research Center‡
PI:EMAIL, PI:EMAIL, PI:EMAIL
I. OVERVIEW
Information ﬂow analysis is a powerful technique for
reasoning about sensitive information that may be ex-
posed during program execution. One promising approach
is to adopt a program as a communication channel model
and leverage information theoretic metrics (e.g., mutual
information between the sensitive input and the public
output) to quantify such information ﬂows. However,
recent research has shown discrepancies in such infor-
mation theoretic metrics: for example, Smith et. al. [5]
showed examples wherein using the classical Shannon
entropy measure for quantifying information ﬂows may
be counter-intuitive. Smith et. al. [5] proposed a vulnera-
bility measure in an attempt to resolve this problem; this
measure was subsequently enhanced by Hamadou et. al.
[2] into a belief-vulnerability metric (in Oakland 2010).
However, we point out that the vulnerability measure may
also lead to counter-intuitive results on several other pro-
grams. In fact, we show that one can construct inﬁnitely
many programs wherein different
information leakage
measures (proposed in past work) are in conﬂict. This
paper presents the ﬁrst attempt towards addressing such
conﬂicts and derives solutions for an optimal conﬂict-free
comparison of programs over a class of entropy measures
(called Renyi entropy − a well known generalization of
the classical Shannon entropy).
II. QUANTIFYING INFORMATION LEAKAGE
information,
Past work on quantitative information leakage metrics
has explored using several entropy measures to compute
mutual
including, Shannon entropy, min-
entropy, Guessing entropy (see [2, 3, 5] for more details),
and so on. However, in most past work, the choice of
such entropy measure has been ad hoc (mostly driven by
sample programs) − often leading to counter-intuitive re-
sults. Consider the following two programs (by Smith[5]),
where the secret input A is uniformly distributed 8k-bit
integer with k ≥ 2, & denotes bitwise and operator and
07k−11k+1 denotes a binary constant.
PROG P1
if A ≡ 0 mod 8 then
O = A
else
O = 1
end if
and
PROG P2
O = A & 07k−11k+1
Intuitively, one might argue that PROG P1 leaks more
information leakage than PROG P2 when k is large,
because it reveals complete information about the secret
8; on the other hand, when k is
input with probability 1
large, PROG P2 reveals roughly 1
8 of the number of bits
in A. However, applying the Shannon entropy measure
and computing the mutual information I1 between A and
O yields a counter intuitive result:
1
P 1 : I1(A, O) = −7
− 1
28k = k + 0.169,
8
8
27k−1
P 2 : I1(A, O) = −2k+1 · 27k−1
28k = k + 1,
28k
Indeed, from a security standpoint, PROG P1 leaves A
highly vulnerable to being guessed (e.g., when it is a
multiple of 8), while PROG P2 does not (at least for
large k).
log
log
log
7
8
Smith et. al. [5] and Hamadou et. al. [2] proposed
a vulnerability measure (a min-entropy measure instead
of classical Shannon entropy measure) in an attempt to
resolve this problem. However, we point out that the
vulnerability measure may lead to counter-intuitive results
on several other programs while, the results based on the
classical Shannon entropy measure matches our intuition.
Consider programs P3 and P4 below.
PROG P3 Password Checker
if A = L then
O = 1
else
O = 0
end if
and
PROG P4 Binary Search
if A ≥ L then
else
O = 1
O = 0
end if
Consider L = |A|/2 is a publicly known program
parameter. The intuition is that PROG P4 leaks much
more information than PROG P3, because when k is
large, the probability of A = L becomes so low that
PROG P3 leaks almost no information. But PROG P4
always leaks 1 bit of information, irrespective of |A|.
Now, consider the mutual information based on Shannon
entropy (I1) and the vulnerability metric (Iv):
(
Iv(P 3, 2k) = 1, Iv(P 4, 2k) = 1
I1(P 3, 2k) ≈ 0, I1(P 4, 2k) = 1
Hence, the fundamentally challenge is to device an in-
formation leakage metric that is intuitive and conﬂict-
free. Towards this goal, we investigate a class of entropy
measures called Renyi-entropy [4] − a well known gener-
alization of various entropy measures including, the clas-
sical Shannon entropy, min-entropy and guessing entropy,
one-guess vulnerability measure, etc. More precisely,
Renyi-entropy deﬁnes a family of entropy measures based
on a parameter α ∈ (0, ∞) such that α = 1 corresponds to
the classical Shannon entropy, α = ∞ corresponds to the
min-entropy and α = 0 corresponds to the vulnerability
one-guess entropy.
A. Main Results
We informally state our main results below. For de-
tailed claims and proofs please refer our tech-report [6].
Conﬂicts in Metrics: We show that information ﬂow
metrics proposed by past work are inherently prone to
conﬂicts, that is, there exists programs P1 and P2 and
Renyi-entropy parameters α, β (α 6= β) such that Iα(P1)
> Iα(P2) and Iβ(P1) < Iβ(P2). In addition, we show
how to construct inﬁnitely many programs wherein dif-
ferent Renyi-entropy based information leakage measures
are in conﬂict.
A conﬂict-free Metric: For any program P , the asymp-
tote θ[1](I∞(P )) is a conﬂict-free.
Examples:
It is easy to see using the proposed metric
θ(I∞(P )) one can resolve the conﬂicts in information
leakage metrics for programs P1-P4. In this example, we
show the application of our metric to PROG P5.
PROG P5 Modulo
O ≡ A mod L
In PROG P5, one can show that the optimal choice of
the low input Li (for the ith program run) is given by:
{L∗
where lcm(L1,··· , Li) refers to the least common mul-
tiple [1] of L1,··· , Li. Using our metric one can show
that the information leakage in P5 for n runs is given by:
L1,··· ,Li∈L lcm(L1,··· , Li)
i } = arg max
1,··· , L∗
Iα(P 5,|A|, n) = Σn
i=1 log(L∗
i ),∀α ∈ (0,∞)
2
Fig. 1. Quantifying Information Leakage Across Multiple Program
Runs
|L|
Hence,
the information leakage metric for PROG P5
grows linearly with n, the number of program runs as
long as L is sufﬁciently large. However, for ﬁnite |L|
information leakage drops to zero after roughly
loge(|L|)
program runs, where loge denotes the natural logarithm.
For example, when L = {1, ··· , 16} then the optimal
choice of Li’s is given by {16, 15, 13, 11, 7, 3}; further
choices of L and subsequent program runs do not offer
more information about the high input to the adversary.
Figure II-A shows the rate of information leakage with
the number of program runs for L = {1, ··· , 16}, {1,
··· , 32} and {1, ··· , 64}.
III. LIMITATION
The results presented in this paper applies only when
the number of output symbols and the number of program
runs is ﬁnite and independent of |A|.
ACKNOWLEDGEMENTS
Research was sponsored by the U.S. Army Research Labo-
ratory and the U.K. Ministry of Defence and was accomplished
under Agreement Number W911NF-06-3-0001. The views and
conclusions contained in this document are those of the au-
thor(s) and should not be interpreted as representing the ofﬁcial
policies, either expressed or implied, of the U.S. Army Research
Laboratory, the U.S. Government, the U.K. Ministry of Defence
or the U.K. Government. The U.S. and U.K. Governments are
authorised to reproduce and distribute reprints for Government
purposes notwithstanding any copyright notation hereon.
REFERENCES
[1] Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and
Clifford Stein. Introduction to Algorithms. MIT Press, 3 edition,
2009.
[2] Sardaouna Hamadou, Vladimiro Sassone, and Catuscia Palamidessi.
Reconciling Belief and Vulnerability in Information Flow. In IEEE
Symposium on Security and Privacy, pages 79 –92, 2010.
[3] James L. Massey. Guessing and entropy.
In IEEE International
Symposium on Information Theory, page 204, 1994.
[4] A. Renyi. On measures of entropy and information. Fourth Berkeley
Symposium on Mathematical Statistics and Probability, pages 547–
561, 1961.
[5] G. Smith. On the foundations of quantitative information ﬂow.
Foundations of Software Science and Computational Structures,
pages 288–302, 2009.
[6] J. Zhu and M. Srivatsa. Quantifying information leakage in ﬁnite
order deterministic programs. ArXiv e-prints, 2010.