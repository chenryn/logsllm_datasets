17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
asyncId = asyncId == -1 ? Tracker.AsyncStart() :
Tracker.StateStart(asyncId);
switch ( state ) {
asyncId;
case −1:
state = 0;
/* Synchronous process block 1 */
var input = GetInput(req) ;
var task1 = AsyncTask1(input);
Tracker.TaskStart(task1, asyncId);
awaiter = task1 .GetAwaiter() ;
builder .OnCompleted(awaiter, this ) ;
Tracker.Await(task1, asyncId);
break;
case 0:
/* Synchronous process block 2 */
var output = awaiter . GetResult () ;
var response = GetResponse(output);
builder . SetResult (response) ;
Tracker.StateEnd(asyncId);
Tracker.AsyncEnd(asyncId);
return ;
}
Tracker.StateEnd(asyncId);
}
}
Figure 9— Async state machine for the code in Figure 4. Statements in
bold show the instrumented code. For clarity, we show the C# code instead
of bytecode.
Since the underlying state machine separates continua-
tions as explicit states, WebPerf instruments the state ma-
chine (at bytecode level) rather than the original code. This
is in contrast with AppInsight, which can instrument source
code or equivalent binaries to track APM dependencies.
Given a state machine of an async method, WebPerf in-
struments its MoveNext() method (in Figure 9, statements
in bold show the instrumentation code). 1) When an async
method starts, we generate a new async id and save it as part
of the state machine (line 8). 2) For async tasks (I/O, large
compute), we monitor the start of the task (line 16) connect-
ing it to the async method using the async id. 3) We connect
task completion to its continuation by tracking task awaits
(line 19). We use the task object id, thread id and async id
to accurately connect them – this ensures correctness even
if task start and task await are part of different methods and
263
Sync block 2Sync block 1Continuationtask1Task endTask startProcess req endProcess req startWhenAllContinuationtask1task2WhenAny Continuationtask1task2Starttask1Endtask1Starttask2Endtask1Starttask2End(a)(b)(c)3.4 Extracting dependency graphs
The dependency graph of a request is a directed acyclic
graph with three types of nodes: Start, End, and Task.
Start and End denote the start and the end of the request.
Task nodes represent async API calls, as well as other syn-
chronous and asynchronous compute and I/O calls that we
want to proﬁle and predict. An edge A → B means that B can
start only after A ﬁnishes. Multiple tasks originating from a
single task indicate parallel tasks. Multiple tasks terminating
at a task indicates a WhenAll or WhenAny dependency.
WebPerf processes the execution trace from start to end
to construct a dependency graph. It constructs a Task node
for each unique async call, compute or I/O call with known
signatures, and expensive computation.
It constructs an
edge t → t(cid:48) when it encounters in the execution trace a
task t(cid:48) starting in the continuation of another task t. Note
that the same continuation thread of a task t may con-
tain start of multiple tasks t1, t2,··· , resulting in parallel
edges t → t1, t → t2,··· . On encountering WhenAll
or WhenAny method call with tasks (t1, t2,··· ) as input
Task arguments, WebPerf constructs a synchronization Task
t representing the continuation of the method, and edges
t1 → t, t2 → t,··· representing synchronization dependen-
cies. Synchronization tasks also contain information about
whether the dependency is WhenAll or WhenAny, the in-
formation is show as an arc over all incoming edges (or no
arc) for WhenAny (or WhenAll respectively).
Figure 8a–c shows dependency graphs for the execution
traces in Figure 6, 7 (left), and 7 (right) respectively. We
put an arc over incoming edges for WhenAny dependency,
to differentiate it from WhenAll dependency. Note that the
execution trace also contains timings for compute threads
(black horizontal lines); hence, WebPerf can proﬁle large
compute components, in addition to synchronous and asyn-
chronous tasks, without knowing their semantics.
4. EVALUATING WHAT-IF SCENAR-
IOS
WebPerf estimates the cloud latency of a request under
a what-if scenario in three steps: (1) building application-
independent and parameterized latency proﬁles of various
APIs ofﬂine; (2) computing the dependency graph of a given
request (§3) and application-speciﬁc baseline latency distri-
butions of various tasks in the request; (3) predicting the
cloud latency by combining the dependency graph, baseline
latencies, and latency proﬁles for the given what-if scenario.
4.1 Application-independent Proﬁling
WebPerf maintains a proﬁle dictionary, containing pro-
ﬁles or statistical models of latencies of different cloud APIs
under various conﬁgurations and inputs. In our implementa-
tion, proﬁles are modeled by nonparametric latency distribu-
tions and are stored as histograms. WebPerf uses two types
independent and parameterized, differentiated
of proﬁles:
by whether they depend on workload parameters or not.
Workload-independent proﬁles.
Independent proﬁles
264
model APIs whose performance does not depend on appli-
cations or workloads, but may depend on conﬁgurations.
To proﬁle an API for a speciﬁc cloud resource R (e.g., a
Redis cache), WebPerf repeatedly calls the API until it gets
a good enough latency distribution (i.e., when the sample
mean is within two standard errors of the true mean with
95% conﬁdence level [37]). For all Microsoft Azure APIs,
WebPerf needs fewer than 100 measurements. To support
various what-if scenarios, WebPerf’s proﬁler builds proﬁles
for each API under different conﬁgurations of R, input pa-
rameters of the API, system loads, etc., as well as at dif-
ferent times incrementally to capture temporal variabilities.
Table 2 shows various what-if scenarios WebPerf currently
supports. To build proﬁles for location, tier, input size, and
load, the proﬁler deploys R at different locations or tiers, and
issues requests to them with different input sizes and concur-
rent loads, respectively. For the CPU interference scenario,
WebPerf builds proﬁles for client side CPU processing over-
head of calling the APIs. It also empirically builds a map-
ping between CPU time and wall clock time under different
background CPU stresses, which it uses to convert the CPU
time proﬁles to wall clock time proﬁles. To proﬁle replica
failure scenarios, WebPerf computes expected increase in
loads on working instances of R and uses the load proﬁles
to approximate failure proﬁles.
Parameterized proﬁles. Performance of a small number of
cloud APIs depends on workloads. For instance, the query
API to Azure SQL exhibits different latencies based on the
speciﬁc query (e.g., whether it has a join) and table size. A
cloud resource can also exhibit variable latencies due to dif-
ferent control paths based on an application’s workload (e.g.,
CDN latency for cache hit vs cache miss). WebPerf builds
multiple proﬁles for each of these APIs, parameterized by
relevant workload parameters. WebPerf allows developers to
provide workload hints, based on which it chooses the right
proﬁle for an API. WebPerf exposes to developers a list of
APIs for which workload hints can be provided.
For example, WebPerf proﬁles Azure CDN’s latency as
two distributions, one for cache hits and one for misses, and
allows developers to specify a target cache hit rate as a work-
load hint. WebPerf then appropriately samples from the two
proﬁles to generate a (bimodal) distribution of latencies un-
der the speciﬁed hit rate. §6.6 shows an example. For the
query API to Azure Table storage and SQL, WebPerf builds
multiple proﬁles, one for each table size (e.g.,  2.
1≤i≤k Pr(Xi ≤ w).
w, . . . , Xk ≤ w) = (cid:81)
With parallel execution, there are three cases depending
on whether the application needs to wait for all or any of the
tasks to ﬁnish. (1) Wait for all to ﬁnish (Figure 8(b)): In this
i=1 Xi. The distribution function for Z can
case Z = maxk
be computed as follows: Pr(Z ≤ w) = Pr(X1 ≤ w, X2 ≤
(2) Wait for
any to ﬁnish (Figure 8(c)): In this case, Z = mink
i=1(Xi)
The distribution function of Z can be computed in a sim-
(3) A combination of
ilar manner to the Max function.
the above two: Suppose the application waits for any of
X1, . . . , Xm to ﬁnish, and all of Xm+1, . . . , Xk to ﬁnish;
then Z = max(min(X1, . . . , Xm), max(Xm+1, . . . , Xk).
The distribution function can be computed by combining the
above distributions for Max and Min.
Algorithm 1 implements the computations using three
building blocks that involve operations on discrete distri-
butions: ProbMax, ProbMin and ProbAdd. ProbMax
and ProbMin are computed based on (1) and (2) above.
ProbAdd for adding two distributions is based on convo-
lutions [18] (see associated technical report [23] for details).