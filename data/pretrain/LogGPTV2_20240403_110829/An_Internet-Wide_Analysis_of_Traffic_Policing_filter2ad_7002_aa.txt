title:An Internet-Wide Analysis of Traffic Policing
author:Tobias Flach and
Pavlos Papageorge and
Andreas Terzis and
Luis Pedrosa and
Yuchung Cheng and
Tayeb Karim and
Ethan Katz-Bassett and
Ramesh Govindan
An Internet-Wide Analysis of Trafﬁc Policing
Tobias Flach∗†, Pavlos Papageorge†, Andreas Terzis†, Luis D. Pedrosa∗,
Yuchung Cheng†, Tayeb Karim†, Ethan Katz-Bassett∗, and Ramesh Govindan∗
∗ University of Southern California † Google
Abstract. Large ﬂows like video streams consume signiﬁ-
cant bandwidth. Some ISPs actively manage these high vol-
ume ﬂows with techniques like policing, which enforces a
ﬂow rate by dropping excess trafﬁc. While the existence of
policing is well known, our contribution is an Internet-wide
study quantifying its prevalence and impact on transport-
level and video-quality metrics. We developed a heuristic to
identify policing from server-side traces and built a pipeline
to process traces at scale collected from hundreds of Google
servers worldwide. Using a dataset of 270 billion packets
served to 28,400 client ASes, we ﬁnd that, depending on re-
gion, up to 7% of connections are identiﬁed to be policed.
Loss rates are on average 6× higher when a trace is policed,
and it impacts video playback quality. We show that alterna-
tives to policing, like pacing and shaping, can achieve trafﬁc
management goals while avoiding the deleterious effects of
policing.
CCS Concepts
•Networks → Network measurement; Network performance
analysis;
1.
INTRODUCTION
Internet trafﬁc has increased ﬁvefold in ﬁve years [16],
much of it from the explosion of streaming video. YouTube
and Netﬂix together contribute nearly half of the trafﬁc to
North American Internet users [47,55,66]. Content providers
want to maximize user quality of experience. They spend
considerable effort optimizing their infrastructure to deliver
data as fast as possible [11, 25, 29].
In contrast, an ISP needs to accommodate trafﬁc from a
multitude of services and users, often through different ser-
vice agreements such as tiered data plans. High-volume ser-
vices like streaming video and bulk downloads that require
high goodput must coexist with smaller volume services like
Permission to make digital or hard copies of part or all of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for proﬁt or commercial advantage and that copies bear this notice
and the full citation on the ﬁrst page. Copyrights for third-party components of
this work must be honored. For all other uses, contact the owner/author(s).
SIGCOMM ’16 August 22-26, 2016, Florianopolis, Brazil
c(cid:13) 2016 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-4193-6/16/08.
DOI: http://dx.doi.org/10.1145/2934872.2934873
468
Policing
Shaping
Enforces rate by dropping excess packets immediately
– Can result in high loss rates
+ Does not require memory buffer
+ No RTT inﬂation
Enforces rate by queueing excess packets
+ Only drops packets when buffer is full
– Requires memory to buffer packets
– Can inﬂate RTTs due to high queueing delay
Table 1: Overview of policing and shaping.
search that require low latency. To achieve coexistence and
enfore plans, an ISP might enforce different rules on its traf-
ﬁc. For example, it might rate-limit high-volume ﬂows to
avoid network congestion, while leaving low-volume ﬂows
that have little impact on the congestion level untouched.
Similarly, to enforce data plans, an ISP can throttle through-
put on a per-client basis.
The most common mechanisms to enforce these policies
are trafﬁc shaping – in which trafﬁc above a preconﬁgured
rate is buffered – and trafﬁc policing – in which trafﬁc above
the rate is dropped [15]. Table 1 compares both techniques.
To enforce rate limits on large ﬂows only, networks often
conﬁgure their shapers and policers (the routers or middle-
boxes enforcing rates) to accommodate bursts that temporar-
ily exceed the rate. In this paper, we focus on policing and
brieﬂy discuss shaping (§5.1.2).
The Impact of Policing. Policing is effective at enforc-
ing a conﬁgured rate but can have negative side effects for
all parties. While operators have anecdotally suggested this
problem in the past [15, 62], we quantify the impact on con-
tent providers, ISPs, and clients at a global scale by an-
alyzing client-facing trafﬁc collected at most of Google’s
CDN servers, serving clients around the world. Policing im-
pacts content providers: it introduces excess load on servers
forced to retransmit dropped trafﬁc. Globally, the average
loss rates on policed ﬂows are over 20%! Policing impacts
ISPs: they transport that trafﬁc across the Internet from the
content provider to the client, only for it to be dropped. With
20% loss, a ﬁfth of the bandwidth used by affected ﬂows is
wasted — the content provider and ISPs incur costs transmit-
ting it, but it never reaches the client. This trafﬁc contributes
to congestion and to transit costs.
Policing impacts clients: ISP-enacted policing can inter-
act badly with TCP-based applications, leading to degraded
video quality of experience (QoE) in our measurements. Bad
QoE contributes to user dissatisfaction, hurting content pro-
viders and ISPs.
Figure 1 shows the time-sequence plot of a policed ﬂow
Policed segments
(among lossy)
(overall)
Loss rate
Region
(non-pol.)
6.8%
6.2%
6.5%
4.1%
5.0%
2.0%
2.6%
3.9%
India
4.1%
Africa
2.3%
Asia (w/o India)
2.3%
South America
1.3%
Europe
1.8%
Australia
North America
1.0%
Table 2: % segments policed among lossy segments (≥ 15
losses, the threshold to trigger the policing detector), and over-
all. Avg. loss rates for policed and unpoliced segments.
1.4%
1.3%
1.2%
0.7%
0.7%
0.4%
0.2%
(policed)
28.2%
27.5%
22.8%
22.8%
20.4%
21.0%
22.5%
Figure 1: TCP sequence graph for a policed ﬂow: (1 and 4)
high throughput until token bucket empties, (2 and 5) multiple
rounds of retransmissions to adjust to the policing rate, (3) idle
period between chunks pushed by the application.
collected in a lab experiment (see §3). Because the policer
is conﬁgured to not throttle short ﬂows, the ﬂow ramps up to
over 15 Mbps without any loss (bubble 1), until the policer
starts to throttle the connection to a rate of 1.5 Mbps. Since
packets are transmitted at a rate that exceeds the policed rate
by an order of magnitude, most of them are dropped by the
policer and retransmitted over a 5-second period (2). Fol-
lowing the delivery of the ﬁrst 2 MB, the sender remains
idle until more application data becomes available (3). Since
the ﬂow does not exhaust its allotted bandwidth in this time
frame, the policer brieﬂy allows the sender to resume trans-
mitting faster than the policing rate (4), before throttling the
ﬂow again (5). Overall, the ﬂow suffers 30% loss.
Understanding Policing. Little is known about how trafﬁc
policing is deployed in practice. Thus, we aim to answer the
following questions at a global scale: (1) How prevalent is
trafﬁc policing on the Internet? (2) How does it impact appli-
cation delivery and user quality of experience? (3) How can
content providers mitigate adverse effects of trafﬁc policing,
and what alternatives can ISPs deploy?
The question of user experience is especially important,
yet ISPs lack mechanisms to understand the impact of trafﬁc
management conﬁgurations on their users. They lack visi-
bility into transport-layer dynamics or application-layer be-
havior of the trafﬁc passing through their networks. Further,
policing means that content providers lack full control over
the performance experienced by their clients, since they are
subject to ISP-enacted policies that may have unintended in-
teractions with applications or TCP.
To answer these questions, we need to overcome two hur-
dles. First, trafﬁc management practices and conﬁgurations
likely vary widely across ISPs, and Internet conditions vary
regionally, so we need a global view to get deﬁnitive an-
swers. Second, it is logistically difﬁcult, if not impossible,
to access policer conﬁgurations from within ISPs on a global
scale, so we need to infer them by observing their impact on
trafﬁc and applications. We address these hurdles and an-
swer these three questions by analyzing captured trafﬁc be-
tween Google servers and its users.
Contributions. We make the following contributions:
1. We design and validate an algorithm to detect trafﬁc
policing from server-side traces at scale (§2, §3).
469
2. We analyze policing across the Internet based on global
measurements (§4). We collected over 270 billion pack-
ets captured at Google servers over a 7-day span. This
dataset gives us insight to trafﬁc delivered to clients
all over the world, spread across over 28,400 different
autonomous systems (ASes).
3. We describe solutions for ISPs and content providers
to mitigate adverse effects of trafﬁc management (§5).
We ﬁnd that between 2% and 7% of lossy transmissions
(depending on the region) have been policed (Table 2).1 While
we detected policing in only 1% of samples overall in our
dataset, connections with packet loss perform much worse
than their loss-free counterparts [22, 68]. Thus, understand-
ing and improving the performance for lossy transmissions
can have a large impact on average performance [22]. We
ﬁnd that policing induces high packet loss overall: on aver-
age, a policed connection sees over 20% packet loss vs. at
most 4.1% when no policing is involved. Finally, policing
can degrade video playback quality. Our measurements re-
veal many cases in which policed clients spend 15% or more
of their time rebuffering, much more than non-policed con-
nections with similar goodput. With every 1% increase in
rebuffering potentially reducing user engagement by over 3
minutes [18], these results would be troubling for any con-
tent provider.
While this study primarily highlights the negative side ef-
fects of policing, our point is not that all trafﬁc manage-
ment is bad.
ISPs need tools to handle high trafﬁc vol-
umes while accommodating diverse service agreements. Our
goal is to spur the development of best practices which al-
low ISPs to achieve management needs and better utilize
networks, while also enabling content providers to provide
a high-quality experience for all customers. As a starting
point, we discuss and evaluate how ISPs and content providers
can mitigate the adverse effects of trafﬁc management (§5).
Stepping back, this paper presents an unprecedented view
of the Internet: a week of (sampled) trafﬁc from most of
Google’s CDN servers, delivering YouTube, one of the largest
volume services in the world serving 12-32% of trafﬁc world-
wide [55]; a global view of aspects of TCP including loss
rates seen along routes to networks hosting YouTube’s huge
user base; measurements of policing done by the middle-
boxes deployed in these networks; and statistics on client-
1The video trafﬁc we examine is delivered in segments (or chunks), thus
we analyze the dataset on a per-segment granularity. Many video content
providers stream video in segments, permitting dynamic adaptation of de-
livery to network changes.
0.00.51.01.52.02.50.02.04.06.08.010.0Sequence number (in M)Time (s)Data (First transmit)Data RetransmitsAcked DataPolicing Rate12345side quality of experience metrics capturing how this polic-
ing impacts users. The analysis pipeline built for this paper
enabled this scale of measurement, whereas previous stud-
ies, even those by large content providers like Google, were
limited to packet captures from fewer vantage points [2, 20,
22, 27, 38, 52, 68].
2. DETECTING & ANALYZING POLIC-
ING AT SCALE
In this section, we present an algorithm for detecting whether
a (portion of a) ﬂow is policed or not from a server-side trace.
We added this algorithm to a collection and analysis frame-
work for trafﬁc at the scale of Google’s CDN.
2.1 Detecting Policing
Challenges. Inferring the presence of policing from a server-
side packet trace is challenging for two reasons. First, many
entities can affect trafﬁc exchanged between two endpoints,
including routers, switches, middleboxes, and cross trafﬁc.
Together they can trigger a wide variety of network anoma-
lies with different manifestations in the impacted packet cap-
tures. This complexity requires that our algorithm be able to
rule out other possible root causes, including congestion at
routers.2 The second challenge is to keep the complexity
of policing detection low to scale the detection algorithm to
large content providers.
Deﬁnition. Trafﬁc policing refers to the enforcement of
a rate limit by dropping any packets that exceed the rate
(with some allowance for bursts). Usually, trafﬁc policing
is achieved by using a token bucket of capacity N, initially
ﬁlled with m tokens. Tokens are added (maximum N tokens
in the bucket) at the preconﬁgured policing rate r. When a