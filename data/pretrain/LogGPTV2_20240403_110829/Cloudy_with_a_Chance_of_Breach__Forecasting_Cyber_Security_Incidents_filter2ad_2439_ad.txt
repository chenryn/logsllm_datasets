10%
20%
12%
90%
95%
All
90%
10%
10%
90%
Table 5: Best operating points of the classiﬁer for the
best combinations of (TP, FP) values.
victim organizations (these are conditional probability
estimates), whereas the overall accuracy does depend on
the two population sizes as it is the unconditioned prob-
ability of making correct predictions. Since based on our
dataset we have a minuscule victim population (account-
ing for (cid:31) 1% of the overall population), the overall ac-
curacy is simply ∼ (1-FP). Therefore, if the overall ac-
curacy is of interest, the best classiﬁer would be a naive
one that simply labels all inputs as “0”. This would lead
to 0% TP, 0% FP, and an overall accuracy of > 99%.
However, despite achieving maximum overall accuracy,
such a classiﬁer is clearly useless. This point is also em-
phasized in [51] for similar reasons. Additionally, in the
context of forecasting, where the goal is to facilitate pre-
ventative measures at an organizational level, having a
high TP is perhaps more relevant than having a low FP;
this is in contrast to spam detection, where the cost of FP
is much higher than a missed detection. Therefore, the
three measures in Table 5 should be taken as a whole.
Impact of Training:Testing Ratio
4.2
The results in Fig. 6 are obtained under a 50-50 split of
the victim set into training and testing samples, based on
the incident time. Furthermore, they are obtained using
the short-term forecasting method described in Section
3.2. In general, one can improve the prediction perfor-
mance by increasing the training sample size. There is
1018  24th USENIX Security Symposium 
USENIX Association
no exception in our study, as shown in Fig. 7 where we
compare results from a 70-30 training and testing sam-
ple split of the victim set to that from the 50-50 split, for
the VCDB data. A best operating point is now around
(94%,10%), indicating a clear improvement. Note that,
a 70-30 split is not generally regarded high in the ma-
chine learning literatures, see e.g., in [57] a 90-10 split
was used. We however believe a 50-50 split gives a more
objective measure of the prediction performance.
e
v
i
t
i
s
o
p
1
0.9
0.8
0.7
0.6
0.5
0.4
e
u
r
T
VCDB: 50−50 & Short
VCDB: 70−30 & Short
VCDB: 50−50 & Long
0.1
0.2
0.3
False positive
0.4
0.5
Figure 7: The impact of larger training set and size
of forecasting window; all results are obtained using
VCDB. The three curves: (1) using a 50-50 split of the
victim set between training and testing under the short-
term forecasting scenario (this curve is identical to the
one in Fig 6); (2) using a 70-30 split of the victim set
between training and testing under the short-term fore-
casting scenario; (3) using a 50-50 split of the victim set
between training and testing under the long-term fore-
casting scenario.
4.3 Short-term vs. Long-term Forecast
Also shown in Fig. 7 are our long-term forecasting re-
sults under a 50-50 training and testing sample split of
the victim set, again for VCDB. As seen, the predic-
tion performance holds even when we move from a one-
month to a 12-month forecasting window. The use of
mismanagement symptoms and long-term malicious be-
haviors in the features contributes to this: they generally
remain stable over time and have relatively high impor-
tance in the prediction, discussed in greater detail in the
next section.
4.4 Relative Importance of the Features
In addition to the prediction output, the RF classiﬁer also
outputs a normalized relevance score for each feature
used in training [17]; the higher the value, the more im-
portant the feature in the prediction. In this section, we
examine these scores more closely. This study will fur-
ther help us understand the extent to which different fea-
tures determine the chance of an organization becoming
breached in the near future. For brevity, the experiments
presented in this section are based on a combination of
all three datasets.
The importance of each category of features is sum-
marized in Table 6. We make a number of interesting ob-
Feature category
Mismanagement
Time series data
Recent-60 secondary features
Organization size
Recent-14 secondary features
Normalized importance
0.3229
0.2994
0.2602
0.0976
0.02
Table 6: Feature importance by category. The misman-
agement features are the most important category in pre-
diction. Secondly, the Recent-60 secondary features are
almost as important as the time series data; the former
capture dynamic behavior over time within an organiza-
tion whereas the latter capture synchronized behavior be-
tween malicious activities of different organizations.
servations. First, note that the mismanagement features
stand out as the most important category in prediction.
Second, the Recent-60 secondary features are almost as
important as the time series data, despite the fact that the
former are derived from the latter. This is because the use
of time series data has the effect of capturing synchro-
nized behavior between malicious activities of different
organizations, while the secondary features are aimed at
capturing the dynamic behavior over time within an orga-
nization itself. That the latter adds value to the predictor
is thus validated by the above importance comparison.
Last but not least, the Recent-60 features appear much
more important than Recent-14 features.
A closer look into each category reveals that among
the mismanagement features, untrusted HTTPS is by
far the most important (0.1531), followed by Openre-
solver (0.0928), DNS random port (0.0469), Mail relay
(0.0169), and BGP misconﬁg. (0.0132). The more sig-
niﬁcant role of untrusted HTTPS in prediction as com-
pared to Openresolver is consistent with the bigger dif-
ference in distributions seen earlier in Fig. 2; that is,
a victim organization tends to have a higher percentage
of mis-conﬁgured HTTPS for their network assets. A
possible explanation is that a majority of the incidents in
our dataset are web-page breaches; these correlate with
the untrusted HTTPS symptom, which reﬂect poorly ma-
naged web systems.
Similarly, a closer look at the secondary features (both
Recent-60 and Recent-14) suggests that the dynamic fea-
tures (duration and frequency together, totaling 0.1769)
are far more important than static features (magnitude,
totaling 0.0834). This suggests that dynamic changes
over time, or in other words, organizations’ response
USENIX Association  
24th USENIX Security Symposium  1019
time in terms of cleaning up the origin of their malicious
activities, is more indicative of security risks.
4.5 The Power of Dataset Diversity
A question that naturally arises is what if only a sin-
gle feature category is used to train the classiﬁer. For
instance, given the prominent score of mismanagement
features in prediction, would it be sufﬁcient to only use
these in prediction? The answer, as shown in Fig. 8,
turns out to be negative. In this ﬁgure, we compare the
prediction performance by using the following four cat-
egories of features separately to build the classiﬁer: mis-
management, time series data, organization size, and the
entire set of secondary features. While it is expected
e
v
i
t
i
s
o
p
1
0.9
0.8
0.7
0.6
0.5
0.4
e
u
r
T
Mismanagement
Malicious acitivity time series
Organization size
Secondary features
All
0.1
0.2
0.3
False positive
0.4
0.5
Figure 8:
Independent prediction performance using
only one set of features. The secondary features are
shown to be the most powerful in prediction when used
alone. Mismanagement features perform the worst, even
though they have the highest importance factor. This is
because the factors reﬂect conditional importance, given
the presence of other features. This means that misma-
nagement features alone are poor predictors but they add
valuable information to the other features.
that using only one feature set leads to worse predic-
tion performance, it is somewhat surprising that the sec-
ondary features are more powerful than mismanagement
features or the time series when used separately. Recall
that the secondary features were designed speciﬁcally to
capture the organizational behavior, including their re-
sponsiveness and effectiveness in dealing with malicious
acti-vities. One explanation of this result is that the hu-
man and process element of an organization is the most
slow-changing compared to the change in the threat land-
scape, and thus holds the most predictive power.
Note that this is not inconsistent with the relative im-
portance given in Table 6, as the latter is a measure of
conditional importance of one feature given the presence
of other features. In other words, the relative importance
suggests how much we lose in performance if we leave
out a feature, whereas Fig. 8 shows how well we do when
using only that feature. What’s seen here is that misman-
agement features add very signiﬁcant (orthogonal) infor-
mation to the other features, but they are poor predictors
in and by themselves. Perhaps most importantly, the re-
sults in Fig. 8 validate the idea of using a diverse set of
measurement data that collectively form predictive des-
criptions of an organization’s security risks.
4.6 Comparison with SVM
As a reference, we also trained classiﬁers using SVM;
the prediction results are much poorer compared to using
RF. For instance, using the VCDB data, the best operat-
ing point under SVM (with a 50-50 training-testing split
of the victim population and short-term forecasting) is
around (70%, 25%). This observation is consistent with
existing literature, see e.g., [57].
5 Discussion
5.1 Top Data Breaches of 2014
In Fig. 9, we plot the distribution (CDF) of the predic-
tor output values for the VCDB victim set and a ran-
domly selected non-victim set used in testing. We use
an example threshold of 0.85 for illustration. All points
to the right of a threshold is labeled “1”, indicating
positive prediction, and all to its left “0”. Three inci-
dent examples are also shown, falling into the categories
of true-positive (ACME), false-positive (AXTEL), and
false-negative (BJP Junagadh).
Also highlighted in Fig.
9 are the top ﬁve data
breaches of 2014 [43], namely JP Morgan Chase, Sony
pictures, Ebay, Home Depot, and Target. Using the
suggested threshold value, our prediction method would
have correctly labeled four of these incidents, and only
narrowly missed the Target incident. It is worth noting
that the Target incident was brought on by one of its con-
tractors; however, the fact that Target did not have a more
secure vendor policy in place is indicative of something
else amiss (e.g., lack of consistent procedure between IT
and procurement) that could also have manifested itself
in the data and features we examined.
These examples highlight that, in addition to enabling
proactive measures by an organization, there are poten-
tial business uses of the prediction method presented in
this study. The ﬁrst is in vendor or third party evalua-
tion. Consider Online Tech, the hosting service used by
JP Morgan Chase, as an example. As shown in Fig. 9,
Online Tech posed very high security risks; this infor-
mation could have been used in determining whether to
use this vendor. Furthermore, information provided by
our prediction method can help underwriters better cus-
tomize terms of a cyber-insurance policy. The insurance
1020  24th USENIX Security Symposium 
USENIX Association
F
D
C
1
0.8
0.6
0.4
0.2
0
0.4
Randomly selected non−victim set
VCDB victim set
Threshold 0.85
ACME 0.85
Homedepot 0.85
AXTEL 0.87
Sony picture 0.90
Ebay 0.88
OnlineTech 0.92
Target 0.84
BJP
Junagadh 0.77
0.5
0.6
0.7
Predictor output
0.8
Threshold 0.85
0.9
1
Figure 9: Distribution of predictor outputs with an example threshold value 0.85 (with 91% TP, 10% FP). On the curve
with circles (non-victim) to the right of the threshold are FPs; on the curve with squares (victim) to the right of the
threshold are TPs. Three types of incidents are shown, presenting true-positive (ACME), false-positive (AXTEL), and
false-negative (BJP Junagadh). Also highlighted are top data breach events in 2014.
payout in the case of Target was reported to be around
$60M; with this much at stake, it is highly desirable to
be able to accurately predict the risk of an insured and
adjust terms of the contracts.
5.2 Prediction by Incident Type
For a majority of the incident types, we do not have
enough sample points to serve both training and testing
purposes, except for the 368 reports of the type “web ap-
plications incident” in VCDB. This allows us to train a
classiﬁer to predict the probability of an organization be-
ing hit with a “web app incident”. The corresponding
results are similar in accuracy to those obtained earlier
(e.g., at (92%, 11%)). This suggests that our methodo-
logy has the potential to make more precise predictions
as we accumulate more incident data.
Similarly, the current forecasting methodology is not
aimed at predicting highly targeted attacks motivated by
geo-political reasons (e.g., the Sony Picture breach). Nor
does it use explicit business sector information (e.g., a
bank may be a bigger target than a public library sys-
tem). In this sense, our current results represent more the
likelihood of an organization falling victim provided it is
being targeted. However, an ever increasing swath of the
Internet is rapidly under cyber threats to the point that
all major organizations should simply assume that they
are someone’s target. The use of explicit business sec-
tor information does allow us to make more ﬁne-grained
predictions.
In a more recent study [48], we leverage
a broad array of publicly available business details on
victim organizations reported in VCDB, including busi-
ness sector, employee count, region of operation and web
statistics information from Alexa Web Information Ser-
vice (AWIS), to generate risk proﬁles, the conditional
probabilities of an organization suffering from speciﬁc
types of incident, given that an incident occurs.
5.3 Robustness against adversarial data
manipulation and other design choices
One design choice we made in the feature extraction pro-
cess is the parameter δ which determines how a time se-
ries is quantized to obtain secondary features. Below we
summarize the impact of of having different δ values. In
the results shown so far, a value of δ = 0.2 is used. In
Fig. 10 we test the cases with δ = 0.1 and δ = 0.3. We
see that this parameter choice has relatively minor effect:
with δ = 0.3 a desirable TP/FP combination is around
(91%,9%), and for δ = 0.1, we have (86%,6%). It ap-
pears that having a higher value of δ leads to slightly
better performance; a possible explanation is that quan-
tizing using δ = 0.2 retained more noise and ﬂuctuation
in the time series, while quantizing using δ = 0.3 may be
more consistent with the actual onset of events.
e
v
i
t
i
s
o
p
e
u
r
T
1
0.9
0.8
0.7
0.6
0.5
0.4
VCDB: δ = 0.3
VCDB: δ = 0.2
VCDB: δ = 0.1
0.1
0.2
0.3
False positive
0.4
0.5
Figure 10: Experiment results under different δ.