Reporting
223
8.2
Evaluating Fuzzers
224
8.2.1
Retrospective Testing
224
8.2.2
Simulated Vulnerability Discovery
225
8.2.3
Code Coverage
225
8.2.4
Caveats
226
8.3
Introducing the Fuzzers
226
8.3.1
GPF
226
8.3.2
Taof
227
8.3.3
ProxyFuzz
227
8.3.4
Mu-4000
228
8.3.5
Codenomicon
228
8.3.6
beSTORM
228
8.3.7
Application-Specific Fuzzers
229
8.3.8
What’s Missing
229
8.4
The Targets
229
8.5
The Bugs
230
8.5.1
FTP Bug 0
230
8.5.2
FTP Bugs 2, 16
230
8.6
Results
231
8.6.1
FTP
232
8.6.2
SNMP
233
8.6.3
DNS
233
8.7
A Closer Look at the Results
234
8.7.1
FTP
234
8.7.2
SNMP
237
8.7.3
DNS
240
8.8
General Conclusions
242
8.8.1
The More Fuzzers, the Better
242
8.8.2
Generational-Based Approach Is Superior
242
8.8.3
Initial Test Cases Matter
242
8.8.4
Protocol Knowledge
243
8.8.5
Real Bugs
244
8.8.6
Does Code Coverage Predict Bug Finding?
244
xii
Contents
8.8.7
How Long to Run Fuzzers with Random Elements
246
8.8.8
Random Fuzzers Find Easy Bugs First
247
8.9
Summary
247
CHAPTER 9
Fuzzing Case Studies
249
9.1
Enterprise Fuzzing
250
9.1.1
Firewall Fuzzing
251
9.1.2
VPN Fuzzing
253
9.2
Carrier and Service Provider Fuzzing
255
9.2.1
VoIP Fuzzing
256
9.2.2
WiFi Fuzzing
257
9.3
Application Developer Fuzzing
259
9.3.1
Command-Line Application Fuzzing
259
9.3.2
File Fuzzing
259
9.3.3
Web Application Fuzzing
261
9.3.4
Browser Fuzzing
262
9.4
Network Equipment Manufacturer Fuzzing
263
9.4.1
Network Switch Fuzzing
263
9.4.2
Mobile Phone Fuzzing
264
9.5
Industrial Automation Fuzzing
265
9.6
Black-Box Fuzzing for Security Researchers
267
9.6.1
Select Target
268
9.6.2
Enumerate Interfaces
268
9.6.3
Choose Fuzzer/Fuzzer Type
269
9.6.4
Choose a Monitoring Tool
270
9.6.5
Carry Out the Fuzzing
271
9.6.6
Post-Fuzzing Analysis
272
9.7
Summary
273
About the Authors
275
Bibliography
277
Index
279
Contents
xiii
Foreword
It was a dark and stormy night. Really.
Sitting in my apartment in Madison in the Fall of 1988, there was a wild mid-
west thunderstorm pouring rain and lighting up the late night sky. That night, I was
logged on to the Unix systems in my office via a dial-up phone line over a 1200 baud
modem.  With the heavy rain, there was noise on the line and that noise was inter-
fering with my ability to type sensible commands to the shell and programs that I
was running. It was a race to type an input line before the noise overwhelmed the
command.
This fighting with the noisy phone line was not surprising. What did surprise
me was the fact that the noise seemed to be causing programs to crash. And more
surprising to me was the programs that were crashing—common Unix utilities that
we all use everyday.
The scientist in me said that we need to make a systematic investigation to try
to understand the extent of the problem and the cause. 
That semester, I was teaching the graduate Advanced Operating Systems course
at the University of Wisconsin.  Each semester in this course, we hand out a list of
suggested topics for the students to explore for their course project. I added this
testing project to the list.
In the process of writing the description, I needed to give this kind of testing a
name.  I wanted a name that would evoke the feeling of random, unstructured data.
After trying out several ideas, I settled on the term “fuzz.”
Three groups attempted the fuzz project that semester and two failed to achieve
any crash results. Lars Fredriksen and Bryan So formed the third group, and were
more talented programmers and most careful experiments; they succeeded well
beyond my expectations. As reported in the first fuzz paper [cite], they could crash
or hang between 25–33% of the utility programs on the seven Unix variants that
they tested.
However, the fuzz testing project was more than a quick way to find program
failures. Finding the cause of each failure and categorizing these failures gave the
results deeper meaning and more lasting impact. The source code for the tools and
scripts, the raw test results, and the suggested bug fixes were all made public. Trust
and repeatability were crucial underlying principles for this work.
In the following years, we repeated these tests on more and varied Unix sys-
tems for a larger set of command-line utility programs and expanded our testing to
GUI programs based on the then-new X-window system [cite fuzz 1995]. Windows
followed several years later [cite fuzz 2000] and, most recently, MacOS [cite fuzz
2006].  In each case, over the span of the years, we found a lot of bugs and, in each
case, we diagnosed those bugs and published all of our results.
xv
xvi
Foreword
In our more recent research, as we have expanded to more GUI-based applica-
tion testing, we discovered that classic 1983 testing tool, “The Monkey” used on the
earlier Macintosh computers [cite Hertzfeld book]. Clearly a group ahead of their
time.
In the process of writing our early fuzz papers, we came across strong resist-
ance from the testing and software engineering community. The lack of a formal
model and methodology and undisciplined approach to testing often offended
experienced practioners in the field.  In fact, I still frequently come across hostile
attitudes to this type of “stone axes and bear skins” (my apologies to Mr. Spock)
approach to testing.
My response was always simple: “We’re just trying to find bugs.” As I have
said many times, fuzz testing is not meant to supplant more systematic testing. It is
just one more tool, albeit, and an extremely easy one to use, in the tester’s toolkit.
As an aside, note that the fuzz testing has not ever been a funded research effort
for me; it is a research advocation rather than a vocation. All the hard work has been
done by a series of talented and motivated graduate students in our Computer Sci-
ences Department. This is how we have fun.
Fuzz testing has grown into a major subfield of research and engineering, with
new results taking it far beyond our simple and initial work. As reliability is the
foundation of security, so has it become a crucial tool in security evaluation of soft-
ware. Thus, the topic of this book is both timely and extremely important. Every
practitioner who aspires to write safe and secure software needs to add these tech-
niques to their bag of tricks.
Barton Miller
Madison, Wisconsin
April 2008
Operating System Utility Program Reliability—The Fuzz Generator
The goal of this project is to evaluate the robustness of various Unix utility pro-
grams, given an unpredictable input stream. This project has two parts. First, you
will build a “fuzz” generator. This is a program that will output a random charac-
ter stream. Second, you will take the fuzz generator and use it to attack as many
Unix utilities as possible, with the goal of trying to break them. For the utilities that
break, you will try to determine what type of input caused the break.
The Program
The fuzz generator will generate an output stream of random characters. It will
need several options to give you flexibility to test different programs. Below is the
start for a list of options for features that fuzz will support. It is important when
writing this program to use good C and Unix style, and good structure, as we hope
to distribute this program to others.
–p
only the printable ASCII characters
–a
all ASCII characters
–0
include the null (0 byte) character
–l
generate random length lines (\\n terminated strings)
–f name
record characters in file “name’’
–d nnn
delay nnn seconds following each character
–r name
replay characters in file “name’’ to output
The Testing
The fuzz program should be used to test various Unix utilities. These utilities include
programs like vi, mail, cc, make, sed, awk, sort, etc. The goal is to first see if the pro-
gram will break and second to understand what type of input is responsible for the
break.
Foreword
xvii
Preface
Still today, most software fails with negative testing, or fuzzing, as it is known by
security people. I (Ari) have never seen a piece of software or a network device that
passes all fuzz tests thrown at it. Still, things have hopefully improved a bit from 1996
when we started developing our first fuzzers, and at least from the 1970s when Dr.
Boris Beizer and his team built their fuzzer-like test automation scripts. The key
driver for the change is the adaptation of these tools and techniques, and availabil-
ity of the technical details on how this type of testing can be conducted. Fortunately
there has been enormous development in the fuzzer market, as can be seen from the
wide range of available open source and commercial tools for this test purpose.
The idea for this book came up in 2001, around the same time when we com-
pleted the PROTOS Classic project on our grammar-based fuzzers. Unfortunately
we were distracted by other projects. Back then, as a result of the PROTOS project,
we spawned a number of related security “spin-offs.” One of them was the com-
mercial company Codenomicon, which took over all technical development from
PROTOS Classic, and launched the first commercial fuzzers in early 2002 (those were
for SIP, TLS, and GTP protocols if you are interested). Another was the PROTOS
Genome project, which started looking at the next steps in fuzzing and automated
protocol reverse-engineering, from a completely clean table (first publicly available
tests were for various compression formats, released in March 2008). And the third
was FRONTIER, which later spun-out a company doing next-gen network analysis
tools and was called Clarified Networks. At the same time we kept our focus on fuzzer
research and teaching on all areas of secure programming at the University of Oulu.
And all this was in a small town of about two hundred thousand people, so you could
say that one out of a thousand people were experts in fuzzing in this far-north loca-
tion. But, unfortunately, the book just did not fit into our plans at that time.
The idea for the book re-emerged in 2005 when I reviewed a paper Jared DeMott
wrote for the Blackhat conference. For the first time since all the published and some
unpublished works at PROTOS, I saw something new and unique in that paper. I
immediately wrote to Jared to propose that he would co-author this fuzzer book proj-
ect with me, and later also flew in to discuss with him to get to know him better. We
had completely opposite experiences and thoughts on fuzzing, and therefore it felt
like a good fit, and so finally this book was started. Fortunately I had a dialog going
on with Artech House for some time already, and we got to start the project almost
immediately.
We wanted everything in the book to be product independent, and also technol-
ogy independent. With our combined experiences, this seemed to be natural for the
book. But something was still missing. As a last desperate action in our constant strug-
gle to get this book completed by end of 2007, we reached out to Charlie Miller. The
xix