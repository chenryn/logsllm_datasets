is similar to measuring the code coverage of a black-box test, although there are
challenges in simulating the code into revealing all possible paths that it can take
during normal use. But, in short, an attack surface indicates which portions of
the code an attacker can potentially misuse, namely all code that can be accessed
through various local or remote interfaces, and which is executed during the tests.
Various metrics related to known defects are commonly used in both QA and
VA. In QA, the track record of known issues gives an indication of the overall quality
while regression testing is used to verify that those flaws are not reintroduced into
the software. In network or host security auditing, vulnerability scanners can be
used to verify that all known issues are resolved from the deployed systems. Defect
counting can be based on either externally or internally found defects. Although
software developers can see both metrics, end users such as enterprises and system
integrators often have to rely on the publicly known defects only, unless they have a
good service agreement with vendors and can access such defect metrics from their
suppliers. Without a database of publicly known vulnerabilities, an enterprise often
contracts a third-party security auditor to conduct an assessment and to estimate
an assurance level for the product. The details of such an assessment would most
probably be internal to the contractor, but the reports that result from it can give
an estimate of the number of vulnerabilities that a potential attacker without prior
knowledge on the system might find.
We will now walk through some of these metrics in more detail, with a focus
on how they can be used to assess the efficiency of the fuzzer itself, and as a result,
the efficiency of the test and the resulting tested system.
6760 Book.indb 120 12/22/17 10:50 AM
4.3 Defect Metrics and Security 121
4.3.1 Coverage of previous Vulnerabilities
Perhaps the most media-attractive metric of security is based on publicly known
security mistakes. A piece of software that makes the headlines on a weekly basis
is often perceived as less secure than another piece of software with less-known
flaws. Although often used, this is not a very good metric for security, since the
existence of more known vulnerabilities in one product can result simply from its
popularity and from how many researchers are looking for problems in it at any
given time. But is this a useful metric in relation to fuzzers? When various past
security incidents are analyzed, today we can quite often see that more and more of
them are actually found using automated security test tools such as fuzzers. Various
security consultants also find some of these flaws in their proprietary and typically
nonautomated techniques.
Metrics based on the coverage of previously known vulnerabilities are very
similar to the metrics related to regression testing in the QA field and similar to
the assessment of the efficiency of vulnerability scanners. The same metric can be
argued to apply as a metric for software security, but that can be easily countered
by a simple comparison of a widely used Web server and a small internally used
Web server. If the widely used Web server has hundreds of known security issues
and the proprietary Web server has none, the result of such a comparison would
indicate that the proprietary Web server is more secure. This is not necessarily true.
To really assess the security of the proprietary Web server, one should apply all the
same security tests that were used to the widely used server, and based on those
test results, try to come to a security verdict. Fuzzers are probably the first available
tools that can create such a metric. And the same metric can be used to assess the
efficiency of the fuzzer itself.
Some people think that fuzzing can only find certain types of vulnerabilities;
that is, relatively simple memory corruption bugs. This again is not true. You can
analyze this by looking at past vulnerabilities and thinking about how to trigger
those vulnerabilities with black-box testing tools. The steps to conduct such an
analysis include the following phases:
1. Take any communication interface and find all known vulnerabilities that
have been reported in any implementations of it.
2. For each found known issue, find out what the exact input (packet, message
sequence, field inside a file format) that triggers it.
3. For each known issue, identify how the problem could be detected by moni-
toring the target system if the trigger was sent to it.
4. Map these triggers to the test coverage of black-box testing tools such
as fuzzers.
The results of such a comparison can be challenging to analyze. Certainly, some
fuzzing tools will catch different flaws better than others. Purely random fuzzers
would only catch simple flaws, and more intelligent fuzzers should catch most of
the flaws in such a study. Unfortunately, no matter what fuzzing approach you use,
mapping the generated test cases to known vulnerabilities is a tedious task, and
that is why most fuzzers have separated these two testing goals. A test for known
6760 Book.indb 121 12/22/17 10:50 AM
122 Fuzzing Metrics
vulnerabilities, whether those tests are integrated into a fuzzer, is very similar to
traditional vulnerability scanning.
One approach that fuzzers take to cover most of the security issues is to actually
walk through all possible values in a specific protocol. If, for example, a nonprint-
able character in front of a long overflowing text string disturbs an ASCII-based
protocol, a fuzzer would systematically (or randomly) attempt all of the known
characters (0x00-0xFF) in conjunction with that string. Fuzzers have used this
simple approach since at least 1999. This group of tests will then contain a number
of known vulnerabilities in unrelated implementations of the same communication
interface. For example, if we take a test group called SIP Method in the PROTOS
c07-sip test suite,15 we can see that the test group contains 193 test cases that test
various elements inside the SIP Method part of the protocol. The test group has
found tens of different flaws in various products, but does not attempt to identify
individual test cases for each of those flaws. The entire group should be used for
regression testing instead of one single test case.
Optimizing tests in fuzzing requires understanding of the actual flaws behind
the vulnerability. Instead of adding more brute force or randomness, we need to
study if someone has actually looked at the bug to see what caused this behavior. For
example, let us assume a bug is triggered by a special character such as comma “,”
inside or at the beginning of a string. Understanding if the programmer used some
comma-separated lists internally will help to optimize such tests. Or maybe a pro-
prietary pattern matching algorithm triggered the bug? Would “a,” also
cause the bug? Should we also test for ”,” and ”,a”
and ”,”? Should we also try other delimiters? The challenge in stating
that a fuzzer fully covers a vulnerability is that there is no clear definition of cover-
ing a vulnerability. There are always variants to each bug. One of the purposes for
known vulnerability coverage is to assess if that and similar flaws are tested for.
Another purpose is to really assess that the vulnerability was truly fixed.
Let us look at this from another point of view: security scanners. There are two
types of security scanners: aggressive and passive. An aggressive security scanner
will send the actual exploits to the system under test. For aggressive security scan-
ners, you would only need one single test case to cover a known vulnerability. If
the vulnerability exists, the system will crash or execute a harmless payload script.
Note that most commercial vulnerability scanners are passive tools so that they do
not disturb the system under test. They do not actually test for the vulnerability at
all, but only passively check the version of the software under test and make the
verdict based on the response to that passive request. Passive scanning is manda-
tory for most vulnerability scanners because such a test will not result in the crash
of the actual service. This type of passive test would be unacceptable for a fuzzer
and would defeat the purpose of fuzzing.
Known vulnerability metrics can be argued to be meaningless for fuzzers. But
still, for those fuzzers that do such comparison, there are several levels of coverage
against known vulnerabilities:
15 www.ee.oulu.fi/research/ouspg/protos/testing/c07/sip/index.html.
6760 Book.indb 122 12/22/17 10:50 AM
4.3 Defect Metrics and Security 123
0. Not covered or passive check.
1. Covered with single test case (aggressive vulnerability scanner behavior).
2. Covered with a range of optimized tests (e.g., a subset of all commonly used
delimiter characters).
3. Covered with a full range of inputs (e.g., going through all byte values for
a 16-bit, 32-bit, or 64-bit memory address).
It is important to note that, although a fuzzer could reach a high ranking based
on a metric against known vulnerabilities, it can still miss a lot of problems in areas
that have not been tested by anyone before. For example, countless people have
fuzzed the most common HTTP implementations (such as the Apache Web server)
using all available tools. This does not necessarily mean that Apache is free of bugs.
Although the tolerance to most fuzzers is higher, people still manage to find issues
in such products with new fuzzer tools. There is no such thing as vulnerability-free
software. All software fails with fuzzing. It just depends on the quality of your
fuzzer and if you find the new categories of problems before someone else finds
them. That is the first reason why metrics based on known vulnerabilities do not
work for fuzzers.
Another challenge is that today many security companies are completely
against any public disclosure of vulnerabilities in their customers’ products. Most
probably, the product vendors will never disclose any of the problems they find
internally either. That is the second reason why this metric is not very suitable
for fuzzers: Known vulnerabilities do not reflect the true vulnerabilities in any
given product.
As a summary, security can again be divided into proactive testing and reactive
testing, and metrics related to known vulnerabilities only apply to reactive secu-
rity. A vulnerability scanner is always reactive; that is, it tests for known issues.
Vulnerability scanners have their place in network auditing, but not as a zero-day
detection device. These tools are based on a library of attacks or fingerprints, and
they will verify that those exact attacks are secured against. Unfortunately, if secu-
rity testing is performed using reactive tools, it means that the first variant of that
attack will again pass through the security perimeter. For example, suppose an IPS
is protecting a Web server that crashes when a string of 1,000 A’s is sent. If the IPS
filter algorithm were just looking for 1,000 A’s, 1,000 B’s would avoid the filter and
crash the Web server via the same memory corruption attack vector (in this case
a standard and contrived buffer overflow). Likely, the IPS filtering method will be
more complex and will clip the request at a certain number of bytes as opposed to
the value of those bytes. But the point is still made; clever attackers can sometimes
bypass filters. IPS systems need to be tested as much as any other system, but the
focus might be less on memory corruption and more on functionality in this case
since proper function helps ensure the security of protected systems. That is why
proactive tools like smart fuzzers will test the interface more systematically. In the
earlier example, a fuzzer would always have a large set of tests for the browser
name, as different Web-based products and applications will crash with different
length strings. The firewall has to be tested with the entire suite of tests for any
meaningful results.
6760 Book.indb 123 12/22/17 10:50 AM
124 Fuzzing Metrics
4.3.2 Expected Defect Count Metrics
Defect count metrics depend on where in the development cycle the testing is per-
formed. Fuzzers that are developed in-house and unleashed on a beta release will
most likely find an enormous number of bugs. On the other hand, fuzzing a mature
product such as Apache or the Microsoft IIS Web server today will likely yield none.
One estimation metric for found defects is based on the past history of a fuzzer.
Expected defect count is based on the number of tests that are executed and the
probability of each test finding a defect.
Expected number bugs = Number of tests *
Probability of finding a defect per test
The quality of tests is affected by several factors:
1. What is the model built from (traffic capture, specification)?
2. How is the intelligence built in (attack heuristics, secure program-
ming practices)?
After a thorough test execution, comparing two fuzzers is easy. If one fuzzer
finds 10 times more flaws than the other, and the first fuzzer will cover all the flaws
the second fuzzer found, then the first fuzzer has a higher defect count. But is the
same defect count applicable when a different piece of software is being tested?
Looking at past experiences, it seems that randomness in a fuzzer can increase the
probability of finding flaws such as null pointers dereferences. On the other hand,
an intelligent fuzzer can easily be taught to find these same flaws if the inputs that
trigger these flaws are found.
Another aspect is related to the various definitions for a defect. Whereas a secu-
rity expert is interested in verifying the level of exploitability of a found defect, a
financial corporation or a telecom service provider or a carrier will see a DoS attack
as the worst possible failure category. It is understandable that hackers are interested
in exploits, whereas other users of fuzzers are interested in any flaws resulting in
critical failures. This is one of the main differences in quality assurance and vulner-
ability analysis: Developers need to find all reliability issues, while attackers only
need to find one that enables them to execute remote code on the target system.
4.3.3 Vulnerability risk Metrics
The risks that software is exposed to can be assessed by each vulnerability that is
found with fuzzers or other security testing practices. A number of such categoriza-
tions exist, but they share a lot of common metrics, such as:
• Criticality rating (numeric value): Metric for prioritizing vulnerabilities.
• Business impact (high/medium/low, mapped to a numeric value): Estimates
the risk or damage to assets.
• Exploitability: Type of, easiness of, or possibility for exploiting the vulnerability.
6760 Book.indb 124 12/22/17 10:50 AM
4.3 Defect Metrics and Security 125
• Category of the compromise: Various vulnerability categories and the related
exposure through successful exploitation: confidentiality, integrity, availabil-
ity, total compromise.
• Interface: Remote or local, or further details such as identifying the used
attack vector or communication protocol.
• Prerequisites: What is needed for exploitation to succeed; for example, is there
a requirement for successful authentication?
The most abused metric for vulnerabilities is often the criticality rating, as there
is a tendency to only focus on fixing the vulnerabilities with highest rating. A defect
that is a remotely exploitable availability problem (i.e., open to DoS attacks) can be
easily ignored by an enterprise if is labeled with medium impact. The attack may be
initially labeled as a DoS due to the difficulty of creating a code-executing exploit or
because the flaw is in a noncritical service. However, after the flaw has been made
public, a better hacker could spend the time needed to write an operational exploit
for it and compromise enterprise networks using that flaw. The criticality rating is
often calculated as a factor of the business impact and the metric for exploitability,
as shown below. It is very difficult to calculate the criticality rating for vulnerabili-
ties without knowing the context in which the software is used.
Criticality rating = Business impact * Exploitability
Business impact can indicate the direct financial impact if the flaw is exploited
in customer premises, or it can indicate the costs related to fixing the flaw if the
problem is found in development environment. Only a domain expert in the busi-
ness area can usually rate the business impact of a specific vulnerability.
The exploitability rating indicates how easy it is to develop a raw bug into a
working exploit. This variable is extremely hard to estimate as the field of exploit
development is not well documented. Many famous examples exist of vulnerabili-
ties that are declared unexploitable but then someone manages to exploit them!16
Furthermore, the exploitability rating can change as new techniques in exploit
development become available.
The category of the compromise is the most discussed metric for security vul-
nerabilities. The simplest categorization can consist of the resulting failure mode,
leaving little interpretation to the reader. A successfully exploited vulnerability can
often compromise one or several of the security requirements, such as confidenti-
ality, integrity, and availability. A vulnerability that results in leaking of informa-
tion violates the confidentiality requirement. Integrity-related vulnerabilities result
in alteration of data. Availability-related vulnerabilities result in denial of service.
Often, such as in the case of buffer overflow problems, the compromise is total,
meaning the entire system is in the attacker’s control through execution of remote
code on the victim’s system.
16 www.securityfocus.com/news/493.
6760 Book.indb 125 12/22/17 10:50 AM
126 Fuzzing Metrics
The interface exposed through the vulnerability can be either remote or local.
Local programming interfaces (API), command line interface (CLI), and the graphi-
cal user interface (GUI) can be thought as local interfaces. Remote interfaces are
often related to a specific communication protocol such as HTTP. Remote inter-
faces can also act as a transport. For example, picture formats such as GIF or TIFF
can use a network protocol as the attack vector. HTTP and SIP are also examples
of application protocols; that is, various Web or VoIP services can be running on
top of those protocols. In real life, the division to remote and local interfaces has
become less significant because almost every local interface can also be exploited
remotely through launching command line utilities or API functions through, for
example, Web browsers or MIME-enabled applications.
Prerequisites limit the exploitability of the vulnerability. For example, a vulner-
ability found in the later messages in a complex SSH authentication process might
require successful authentication of a user in the system to be exploited. A local
vulnerability in an application such as a picture viewer might only be exploitable
with the presence of a specific version or configuration.
Vulnerability risk metrics are an extremely useful tool for communicating the
found vulnerabilities in the bug reporting process. They can be used by both the
reporters to explain the importance of the found issues and by the repairers of the
problems in their own bug tracking records. Below is an example taken from a bug
report template from OUSPG:17
3. Suspected Impact and Vulnerability Type
3.1. Exploitability:
[ ] Local - by users with local accounts
[ ] Remote - over the network without an existing
local account
3.2. Compromise:
[ ] Total compromise of a privileged account UID: 
[ ] Total compromise of an unprivileged account UID:
If not a total compromise, please specify:
[ ] Confidentiality
[ ] Integrity
[ ] Availability (Denial of Service)
3.3. Timing:
[ ] Exploitable at any time
[ ] Exploitable under specific conditions: 
3.4. Vulnerability Category:
[ ] Buffer overflow
[ ] File manipulation
[ ] Ability to create user-modifiable arbitrary files