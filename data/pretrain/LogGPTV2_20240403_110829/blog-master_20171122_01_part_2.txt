先按distinct字段重分布，使用groupagg得到结果  
然后按分组字段重分布，再次得到groupagg结果  
这个分布式执行计划有点问题，理论上可以直接按分组字段重分布，然后进行groupagg。  
```  
postgres=# set enable_hashagg =off;  
SET  
postgres=# explain analyze select c1,c2,count(distinct c3) from tbl group by c1,c2;  
                                                                               QUERY PLAN                                                                                  
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------  
 Gather Motion 48:1  (slice3; segments: 48)  (cost=23755578.10..23788110.60 rows=1001000 width=16)  
   Rows out:  1002001 rows at destination with 13064 ms to end, start offset by 245 ms.  
   ->  GroupAggregate  (cost=23755578.10..23788110.60 rows=20855 width=16)  
         Group By: tbl.c1, tbl.c2  
         Rows out:  Avg 20875.0 rows x 48 workers.  Max 20914 rows (seg11) with 0.002 ms to first row, 208 ms to end, start offset by 250 ms.  
         ->  Sort  (cost=23755578.10..23758080.60 rows=20855 width=16)  
               Sort Key: tbl.c1, tbl.c2  
               Rows out:  Avg 727938.4 rows x 48 workers.  Max 729557 rows (seg1) with 0 ms to end, start offset by 247 ms.  
               Executor memory:  46266K bytes avg, 46266K bytes max (seg0).  
               Work_mem used:  46266K bytes avg, 46266K bytes max (seg0). Workfile: (48 spilling, 0 reused)  
               Work_mem wanted: 62546K bytes avg, 62686K bytes max (seg1) to lessen workfile I/O affecting 48 workers.  
               ->  Redistribute Motion 48:48  (slice2; segments: 48)  (cost=22623280.88..23655813.38 rows=20855 width=16)  
                     Hash Key: tbl.c1, tbl.c2  
                     Rows out:  Avg 727938.4 rows x 48 workers at destination.  Max 729557 rows (seg1) with 12518 ms to end, start offset by 247 ms.  
                     ->  GroupAggregate  (cost=22623280.88..23635793.38 rows=20855 width=16)  
                           Group By: tbl.c1, tbl.c2  
                           Rows out:  Avg 852220.6 rows x 41 workers.  Max 983342 rows (seg9) with 0.003 ms to first row, 2574 ms to end, start offset by 250 ms.  
                           ->  Sort  (cost=22623280.88..22873280.88 rows=2083334 width=12)  
                                 Sort Key: tbl.c1, tbl.c2  
                                 Rows out:  Avg 2439024.4 rows x 41 workers.  Max 4003392 rows (seg44) with 0.001 ms to end, start offset by 257 ms.  
                                 Executor memory:  37148K bytes avg, 43851K bytes max (seg0).  
                                 Work_mem used:  37148K bytes avg, 43851K bytes max (seg0). Workfile: (40 spilling, 0 reused)  
                                 Work_mem wanted: 137587K bytes avg, 221435K bytes max (seg44) to lessen workfile I/O affecting 40 workers.  
                                 ->  Redistribute Motion 48:48  (slice1; segments: 48)  (cost=0.00..3048912.00 rows=2083334 width=12)  
                                       Hash Key: tbl.c3  
                                       Rows out:  Avg 2439024.4 rows x 41 workers at destination.  Max 4003392 rows (seg44) with 8081 ms to end, start offset by 257 ms.  
                                       ->  Append-only Columnar Scan on tbl  (cost=0.00..1048912.00 rows=2083334 width=12)  
                                             Rows out:  0 rows (seg0) with 23 ms to end, start offset by 256 ms.  
 Slice statistics:  
   (slice0)    Executor memory: 362K bytes.  
   (slice1)    Executor memory: 1489K bytes avg x 48 workers, 1489K bytes max (seg0).  
   (slice2)  * Executor memory: 38329K bytes avg x 48 workers, 45109K bytes max (seg0).  Work_mem: 43851K bytes max, 221435K bytes wanted.  
   (slice3)  * Executor memory: 46597K bytes avg x 48 workers, 46597K bytes max (seg0).  Work_mem: 46266K bytes max, 62686K bytes wanted.  
 Statement statistics:  
   Memory used: 128000K bytes  
   Memory wanted: 664802K bytes  
 Settings:  enable_bitmapscan=off; enable_hashagg=off; enable_seqscan=off; optimizer=off  
 Optimizer status: legacy query optimizer  
 Total runtime: 13318.578 ms  
(39 rows)  
```  
对于不需要重分布的表（当group字段与分布键一致），不会有执行计划问题：  
优先选择了groupagg  
```  
postgres=# explain analyze select c1,c2,count(distinct c3) from tbl1 group by c1,c2;  
                                                                 QUERY PLAN                                                                    
---------------------------------------------------------------------------------------------------------------------------------------------  
 Gather Motion 48:1  (slice1; segments: 48)  (cost=20623288.88..21635826.40 rows=1003002 width=16)  
   Rows out:  1002001 rows at destination with 6896 ms to end, start offset by 1.285 ms.  
   ->  GroupAggregate  (cost=20623288.88..21635826.40 rows=20896 width=16)  
         Group By: c1, c2  
         Rows out:  Avg 20875.0 rows x 48 workers.  Max 20914 rows (seg11) with 0.003 ms to first row, 995 ms to end, start offset by 39 ms.  
         ->  Sort  (cost=20623288.88..20873288.88 rows=2083334 width=12)  
               Sort Key: c1, c2  
               Rows out:  Avg 2083333.3 rows x 48 workers.  Max 2087802 rows (seg31) with 0.002 ms to end, start offset by 38 ms.  
               Executor memory:  67386K bytes avg, 67386K bytes max (seg0).  
               Work_mem used:  67386K bytes avg, 67386K bytes max (seg0). Workfile: (48 spilling, 0 reused)  
               Work_mem wanted: 130193K bytes avg, 130472K bytes max (seg31) to lessen workfile I/O affecting 48 workers.  
               ->  Append-only Columnar Scan on tbl1  (cost=0.00..1048920.00 rows=2083334 width=12)  
                     Rows out:  0 rows (seg0) with 5555 ms to end, start offset by 38 ms.  
 Slice statistics:  
   (slice0)    Executor memory: 347K bytes.  
   (slice1)  * Executor memory: 67984K bytes avg x 48 workers, 67984K bytes max (seg0).  Work_mem: 67386K bytes max, 130472K bytes wanted.  
 Statement statistics:  
   Memory used: 128000K bytes  
   Memory wanted: 261142K bytes  
 Settings:  enable_bitmapscan=off; enable_hashagg=on; enable_seqscan=off; optimizer=off  
 Optimizer status: legacy query optimizer  
 Total runtime: 6897.348 ms  
(22 rows)  
```  
Greenplum 通过开关，可以打开控制使用hashagg后groupagg，实际上还是hashagg更快。  
```  
postgres=# set enable_groupagg =off;  
SET  
postgres=# set enable_hashagg =on;  
SET  
postgres=# explain analyze select c1,c2,count(distinct c3) from tbl1 group by c1,c2;  
                                                                         QUERY PLAN                                                                            
-------------------------------------------------------------------------------------------------------------------------------------------------------------  
 Gather Motion 48:1  (slice1; segments: 48)  (cost=2548920.00..2561457.52 rows=1003002 width=16)  
   Rows out:  1002001 rows at destination with 3002 ms to end, start offset by 1.252 ms.  
   ->  HashAggregate  (cost=2548920.00..2561457.52 rows=20896 width=16)  
         Group By: partial_aggregation.c1, partial_aggregation.c2  
         Rows out:  Avg 20875.0 rows x 48 workers.  Max 20914 rows (seg11) with 0.005 ms to first row, 140 ms to end, start offset by 52 ms.  
         ->  HashAggregate  (cost=2248920.00..2373920.00 rows=208334 width=12)  
               Group By: tbl1.c1, tbl1.c2, tbl1.c3  
               Rows out:  Avg 1320761.3 rows x 48 workers.  Max 1323529 rows (seg9) with 0.004 ms to first row, 875 ms to end, start offset by 15 ms.  
               ->  HashAggregate  (cost=2048920.00..2048920.00 rows=208334 width=12)  
                     Group By: tbl1.c1, tbl1.c2, tbl1.c3  
                     Rows out:  Avg 1320761.3 rows x 48 workers.  Max 1323529 rows (seg9) with 0.004 ms to first row, 1479 ms to end, start offset by 15 ms.  
                     ->  Append-only Columnar Scan on tbl1  (cost=0.00..1048920.00 rows=2083334 width=12)  
                           Rows out:  0 rows (seg0) with 48 ms to end, start offset by 49 ms.  
 Slice statistics:  
   (slice0)    Executor memory: 347K bytes.  
   (slice1)    Executor memory: 598K bytes avg x 48 workers, 598K bytes max (seg0).  
 Statement statistics:  
   Memory used: 128000K bytes  
 Settings:  enable_bitmapscan=off; enable_groupagg=off; enable_hashagg=on; enable_seqscan=off; enable_sort=off; optimizer=off  
 Optimizer status: legacy query optimizer  
 Total runtime: 3060.036 ms  
(21 rows)  
```  
## PostgreSQL distinct 的优化  
为了让PostgreSQL 求distinct使用hashagg，目前可以修改SQL来实现。（将来的PostgreSQL版本，理论上通过sql rewrite，很容易实现distinct SQL的hashagg）  
```  
postgres=# set work_mem='32GB';  
SET  
postgres=# explain select c1,c2,count(*) cn from (select c1,c2,c3 from tbl group by c1,c2,c3) t group by c1,c2;  
                                QUERY PLAN                                   
---------------------------------------------------------------------------  
 HashAggregate  (cost=652928.50..653328.50 rows=40000 width=16)  
   Group Key: tbl.c1, tbl.c2  
   ->  HashAggregate  (cost=637666.00..643216.00 rows=555000 width=12)  
         Group Key: tbl.c1, tbl.c2, tbl.c3  
         ->  Seq Scan on tbl  (cost=0.00..596041.00 rows=5550000 width=12)  
(5 rows)  
```  
## 并行计算  
Greenplum就不用说了，已经是MPP的架构，对于这类AP查询，性能非常卓越。  
PostgreSQL 也支持并行计算，无论是hashagg还是groupagg，但是目前这两块的优化器执行器还可以改进，目前没有很好的发挥并行计算的能力。  
```  
postgres=# explain select c1,c2,count(*) cn from (select c1,c2,c3 from tbl group by c1,c2,c3) t group by c1,c2;  
                                              QUERY PLAN                                                
------------------------------------------------------------------------------------------------------  
 GroupAggregate  (cost=888153.09..1057837.13 rows=40000 width=16)  
   Group Key: tbl.c1, tbl.c2  
   ->  Group  (cost=888153.09..1047724.63 rows=555000 width=12)  
         Group Key: tbl.c1, tbl.c2, tbl.c3  
         ->  Gather Merge  (cost=888153.09..1039399.63 rows=1110000 width=12)  
               Workers Planned: 2  
               ->  Group  (cost=887153.07..910278.07 rows=555000 width=12)  
                     Group Key: tbl.c1, tbl.c2, tbl.c3  
                     ->  Sort  (cost=887153.07..892934.32 rows=2312500 width=12)  
                           Sort Key: tbl.c1, tbl.c2, tbl.c3  
                           ->  Parallel Seq Scan on tbl  (cost=0.00..563666.00 rows=2312500 width=12)  
(11 rows)  
```  
```  
postgres=# explain select c1,c2,count(*) cn from (select c1,c2,c3 from tbl group by c1,c2,c3) t group by c1,c2;  
                                       QUERY PLAN                                          
-----------------------------------------------------------------------------------------  
 HashAggregate  (cost=600203.50..600603.50 rows=40000 width=16)  
   Group Key: tbl.c1, tbl.c2  
   ->  HashAggregate  (cost=584941.00..590491.00 rows=555000 width=12)  
         Group Key: tbl.c1, tbl.c2, tbl.c3  
         ->  Gather  (cost=0.00..543316.00 rows=5550000 width=12)  
               Workers Planned: 20  
               ->  Parallel Seq Scan on tbl  (cost=0.00..543316.00 rows=277500 width=12)  
(7 rows)  
```  
## 一个SQL多个求distinct  
一个SQL中，包含多个distinct时，优化器是如何执行的呢？  
实际上跑了两次分组聚合，如下：  
```  
postgres=# explain analyze select c1,c2,count(distinct c3),count(distinct c4) from tbl group by c1,c2;  
                                                                                   QUERY PLAN                                                                                     
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  
 Gather Motion 48:1  (slice3; segments: 48)  (cost=5647824.00..5707884.00 rows=1001000 width=32)  
   Rows out:  1002001 rows at destination with 9598 ms to end, start offset by 2.272 ms.  
   ->  Hash Join  (cost=5647824.00..5707884.00 rows=20855 width=32)  
         Hash Cond: NOT dqa_coplan_1.c1 IS DISTINCT FROM dqa_coplan_2.c1 AND NOT dqa_coplan_1.c2 IS DISTINCT FROM dqa_coplan_2.c2  
         Rows out:  Avg 20875.0 rows x 48 workers.  Max 20914 rows (seg11) with 0.015 ms to first row, 1584 ms to end, start offset by 26 ms.  
         Executor memory:  816K bytes avg, 817K bytes max (seg11).  
         Work_mem used:  816K bytes avg, 817K bytes max (seg11). Workfile: (0 spilling, 0 reused)  
         ->  HashAggregate  (cost=2823912.00..2838927.00 rows=20855 width=16)  
               Group By: partial_aggregation.c1, partial_aggregation.c2  
               Rows out:  Avg 20875.0 rows x 48 workers.  Max 20914 rows (seg11) with 0.004 ms to first row, 262 ms to end, start offset by 27 ms.  
               ->  HashAggregate  (cost=2473912.00..2623912.00 rows=208334 width=12)  
                     Group By: postgres.tbl.c1, postgres.tbl.c2, postgres.tbl.c3  
                     Rows out:  Avg 1320761.3 rows x 48 workers.  Max 1323529 rows (seg9) with 0.001 ms to first row, 2778 ms to end, start offset by 27 ms.  
                     ->  Redistribute Motion 48:48  (slice1; segments: 48)  (cost=2048912.00..2248912.00 rows=208334 width=12)  
                           Hash Key: postgres.tbl.c1, postgres.tbl.c2  
                           Rows out:  Avg 2061921.2 rows x 48 workers at destination.  Max 2066345 rows (seg31) with 0.003 ms to end, start offset by 49 ms.  
                           ->  HashAggregate  (cost=2048912.00..2048912.00 rows=208334 width=12)  
                                 Group By: postgres.tbl.c1, postgres.tbl.c2, postgres.tbl.c3  
                                 Rows out:  Avg 2061921.2 rows x 48 workers.  Max 2062196 rows (seg24) with 0.003 ms to first row, 2958 ms to end, start offset by 86 ms.  
                                 ->  Append-only Columnar Scan on tbl  (cost=0.00..1048912.00 rows=2083334 width=16)  
                                       Rows out:  0 rows (seg0) with 76 ms to end, start offset by 128 ms.  
         ->  Hash  (cost=2848937.00..2848937.00 rows=20855 width=16)  
               Rows in:  (No row requested) 0 rows (seg0) with 0 ms to end.  
               ->  HashAggregate  (cost=2823912.00..2838927.00 rows=20855 width=16)  
                     Group By: partial_aggregation.c1, partial_aggregation.c2  
                     Rows out:  Avg 20875.0 rows x 48 workers.  Max 20914 rows (seg11) with 0.004 ms to first row, 227 ms to end, start offset by 27 ms.  
                     ->  HashAggregate  (cost=2473912.00..2623912.00 rows=208334 width=12)  
                           Group By: postgres.tbl.c1, postgres.tbl.c2, postgres.tbl.c4  
                           Rows out:  Avg 1320773.6 rows x 48 workers.  Max 1323487 rows (seg9) with 0.001 ms to first row, 3916 ms to end, start offset by 27 ms.  
                           ->  Redistribute Motion 48:48  (slice2; segments: 48)  (cost=2048912.00..2248912.00 rows=208334 width=12)  
                                 Hash Key: postgres.tbl.c1, postgres.tbl.c2  
                                 Rows out:  Avg 2061913.9 rows x 48 workers at destination.  Max 2066340 rows (seg31) with 284 ms to end, start offset by 49 ms.  
                                 ->  HashAggregate  (cost=2048912.00..2048912.00 rows=208334 width=12)  
                                       Group By: postgres.tbl.c1, postgres.tbl.c2, postgres.tbl.c4  
                                       Rows out:  Avg 2061913.9 rows x 48 workers.  Max 2062167 rows (seg20) with 0.005 ms to first row, 3343 ms to end, start offset by 50 ms.  
                                       ->  Append-only Columnar Scan on tbl  (cost=0.00..1048912.00 rows=2083334 width=16)  
                                             Rows out:  0 rows (seg0) with 75 ms to end, start offset by 131 ms.  
 Slice statistics:  