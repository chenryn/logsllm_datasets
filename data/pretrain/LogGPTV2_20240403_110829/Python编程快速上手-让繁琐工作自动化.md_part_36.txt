现在你需要使用Beautiful Soup，从下载的HTML中，提取排名靠前的查找结果链
接。但如何知道完成这项工作需要怎样的选择器？例如，你不能只查找所有的标
签，因为在这个HTML中，有许多链接你是不关心的。因此，必须用浏览器的开发者
工具来检查这个查找结果页面，尝试寻找一个选择器，它将挑选出你想要的链接。
在针对Beautiful Soup进行Google查询后，你可以打开浏览器的开发者工具，
查看该页面上的一些链接元素。它们看起来复杂得难以置信，大概像这样： BeautifulSoup: We called him Tortoise
because he taught us.
该元素看起来复杂得难以置信，但这没有关系。只需要找到查询结果链接都具有的
模式。但这个元素没有什么特殊，难以和该页面上非查询结果的元素区分开来。
确保你的代码看起来像这样：
#! python3
# lucky.py - Opens several google search results.
import requests, sys, webbrowser, bs4
--snip--
# Retrieve top search result links.
soup = bs4.BeautifulSoup(res.text)
# Open a browser tab for each result.
linkElems = soup.select('.r a')
但是，如果从元素向上看一点，就会发现这样一个元素：。查看
余下的HTML源代码，看起来r类仅用于查询结果链接。你不需要知道CSS类r是什
么，或者它会做什么。只需要利用它作为一个标记，查找需要的元素。可以通
过下载页面的HTML文本，创建一个BeautifulSoup对象，然后用选择符'.r a'，找到
所有具有CSS类r的元素中的元素。
第3步：针对每个结果打开Web 浏览器
最后，我们将告诉程序，针对结果打开Web浏览器选项卡。将下面的内容添加
到程序的末尾：
#! python3
# lucky.py - Opens several google search results.
import requests, sys, webbrowser, bs4
--snip--
# Open a browser tab for each result.
linkElems = soup.select('.r a')
numOpen = min(5, len(linkElems))
for i in range(numOpen):
webbrowser.open('http://google.com' + linkElems[i].get('href'))
默认情况下，你会使用webbrowser模块，在新的选项卡中打开前5个查询结果。
204 Python编程快速上手——让繁琐工作自动化
但是，用户查询的主题可能少于5个查询结果。soup.select()调用返回一个列表，包
含匹配'.r a'选择器的所有元素，所以打开选项卡的数目要么是5，要么是这个列表的
长度（取决于哪一个更小）。
内建的Python函数min()返回传入的整型或浮点型参数中最小的一个（也有内
建的 max()函数，返回传入的参数中最大的一个）。你可以使用 min()弄清楚该列表
中是否少于 5 个链接，并且将要打开的链接数保存在变量 numOpen 中。然后可以
调用range(numOpen)，执行一个for循环。
在该循环的每次迭代中，你使用webbrowser.open()，在Web浏览器中打开一个
新的选项卡。请注意，返回的元素的href属性中，不包含初始的http://google.com
部分，所以必须连接它和href属性的字符串。
现在可以马上打开前5个Google查找结果，比如说，要查找Python programming
tutorials，你只要在命令行中运行lucky python programming tutorials（如何在你的操
作系统中方便地运行程序，请参看附录B）。
第4步：类似程序的想法
分选项卡浏览的好处在于，很容易在新选项卡中打开一些链接，稍后再来查看。
一个自动打开几个链接的程序，很适合快捷地完成下列任务：
• 查找亚马逊这样的电商网站后，打开所有的产品页面；
• 打开针对一个产品的所有评论的链接；
• 查找Flickr或Imgur这样的照片网站后，打开查找结果中的所有照片的链接。
11.7 项目：下载所有 XKCD 漫画
博客和其他经常更新的网站通常有一个首页，其中有最新的帖子，以及一个“前
一篇”按钮，将你带到以前的帖子。然后那个帖子也有一个“前一篇”按钮，以此
类推。这创建了一条线索，从最近的页面，直到该网站的第一个帖子。如果你希望
拷贝该网站的内容，在离线的时候阅读，可以手工导航至每个页面并保存。但这是
很无聊的工作，所以让我们写一个程序来做这件事。
XKCD 是一个流行的极客漫画网站，它符合这个结构（参见图 11-6）。首页
http://xkcd.com/有一个“Prev”按钮，让用户导航到前面的漫画。手工下载每张漫
画要花较长的时间，但你可以写一个脚本，在几分钟内完成这件事。
下面是程序要做的事：
• 加载主页；
• 保存该页的漫画图片；
• 转入前一张漫画的链接；
• 重复直到第一张漫画。
第11章 从Web抓取信息 205
图11-6 XKCD，“关于浪漫、讽刺、数学和语言的漫画网站”
这意味着代码需要做下列事情：
• 利用requests模块下载页面。
• 利用Beautiful Soup找到页面中漫画图像的URL。
• 利用iter_content()下载漫画图像，并保存到硬盘。
• 找到前一张漫画的链接URL，然后重复。
打开一个新的文件编辑器窗口，将它保存为downloadXkcd.py。
第1步：设计程序
打开一个浏览器的开发者工具，检查该页面上的元素，你会发现下面的内容：
• 漫画图像文件的URL，由一个元素的href属性给出。
• 元素在元素之内。
• Prev按钮有一个rel HTML属性，值是prev。
• 第一张漫画的Prev按钮链接到http://xkcd.com/# URL，表明没有前一个页面了。
让你的代码看起来像这样：
#! python3
# downloadXkcd.py - Downloads every single XKCD comic.
import requests, os, bs4
url = 'http://xkcd.com' # starting url
os.makedirs('xkcd', exist_ok=True) # store comics in ./xkcd
while not url.endswith('#'):
# TODO: Download the page.
# TODO: Find the URL of the comic image.
206 Python编程快速上手——让繁琐工作自动化
# TODO: Download the image.
# TODO: Save the image to ./xkcd.
# TODO: Get the Prev button's url.
print('Done.')
你会有一个 url 变量，开始的值是'http://xkcd.com'，然后反复更新（在一个 for
循环中），变成当前页面的 Prev 链接的 URL。在循环的每一步，你将下载 URL 上
的漫画。如果URL以'#'结束，你就知道需要结束循环。
将图像文件下载到当前目录的一个名为 xkcd 的文件夹中。调用 os.makedirs()
函数。确保这个文件夹存在，并且关键字参数exist_ok=True在该文件夹已经存在时，
防止该函数抛出异常。剩下的代码只是注释，列出了剩下程序的大纲。
第2步：下载网页
我们来实现下载网页的代码。让你的代码看起来像这样：
#! python3
# downloadXkcd.py - Downloads every single XKCD comic.
import requests, os, bs4
url = 'http://xkcd.com' # starting url
os.makedirs('xkcd', exist_ok=True) # store comics in ./xkcd
while not url.endswith('#'):
# Download the page.
print('Downloading page %s...' % url)
res = requests.get(url)
res.raise_for_status()
soup = bs4.BeautifulSoup(res.text)
# TODO: Find the URL of the comic image.
# TODO: Download the image.
# TODO: Save the image to ./xkcd.
# TODO: Get the Prev button's url.
print('Done.')
首先，打印url，这样用户就知道程序将要下载哪个URL。然后利用requests模块的
request.get()函数下载它。像以往一样，马上调用Response对象的raise_for_status()方法，
如果下载发生问题，就抛出异常，并终止程序。否则，利用下载页面的文本创建一
个BeautifulSoup对象。
第3步：寻找和下载漫画图像
让你的代码看起来像这样：
第11章 从Web抓取信息 207
#! python3
# downloadXkcd.py - Downloads every single XKCD comic.
import requests, os, bs4
--snip--
# Find the URL of the comic image.
comicElem = soup.select('#comic img')
if comicElem == []:
print('Could not find comic image.')
else:
comicUrl = 'http:' comicElem[0].get('src')
# Download the image.
print('Downloading image %s...' % (comicUrl))
res = requests.get(comicUrl)
res.raise_for_status()
# TODO: Save the image to ./xkcd.
# TODO: Get the Prev button's url.
print('Done.')
用开发者工具检查XKCD主页后，你知道漫画图像的元素是在一个元
素中，它带有的 id 属性设置为 comic。所以选择器'#comic img'将从 BeautifulSoup
对象中选出正确的元素。
有一些XKCD页面有特殊的内容，不是一个简单的图像文件。这没问题，跳过它们
就好了。如果选择器没有找到任何元素，那么soup.select('#comic img')将返回一个空的列
表。出现这种情况时，程序将打印一条错误消息，不下载图像，继续执行。
否则，选择器将返回一个列表，包含一个元素。可以从这个元素中
取得src属性，将它传递给requests.get()，下载这个漫画的图像文件。
第4步：保存图像，找到前一张漫画
让你的代码看起来像这样：
#! python3
# downloadXkcd.py - Downloads every single XKCD comic.
import requests, os, bs4
--snip--
# Save the image to ./xkcd.
imageFile = open(os.path.join('xkcd', os.path.basename(comicUrl)), 'wb')
for chunk in res.iter_content(100000):
imageFile.write(chunk)
imageFile.close()
# Get the Prev button's url.
prevLink = soup.select('a[rel="prev"]')[0]
url = 'http://xkcd.com' + prevLink.get('href')
print('Done.')
208 Python编程快速上手——让繁琐工作自动化
这时，漫画的图像文件保存在变量res中。你需要将图像数据写入硬盘的文件。
你需要为本地图像文件准备一个文件名，传递给 open()。comicUrl 的值类似
'http://imgs.xkcd.com/comics/heartbleed_explanation.png'。你可能注意到，它看起来很
像文件路径。实际上，调用 os.path.basename()时传入 comicUrl，它只返回 URL 的
最后部分：'heartbleed_explanation.png'。你可以用它作为文件名，将图像保存到硬
盘。用os.path.join()连接这个名称和xkcd文件夹的名称，这样程序就会在Windows
下使用倒斜杠（\），在 OS X和Linux下使用斜杠（/）。既然你最后得到了文件名，
就可以调用open()，用'wb'模式打开一个新文件。
回忆一下本章早些时候，保存利用 Requests 下载的文件时，你需要循环处理
iter_content()方法的返回值。for 循环中的代码将一段图像数据写入文件（每次最多
10万字节），然后关闭该文件。图像现在保存到硬盘中。
然后，选择器'a[rel="prev"]'识别出rel属性设置为prev的元素，利用这个
元素的href属性，取得前一张漫画的URL，将它保存在url中。然后while循环针
对这张漫画，再次开始整个下载过程。
这个程序的输出看起来像这样：
Downloading page http://xkcd.com...
Downloading image http://imgs.xkcd.com/comics/phone_alarm.png...
Downloading page http://xkcd.com/1358/...
Downloading image http://imgs.xkcd.com/comics/nro.png...
Downloading page http://xkcd.com/1357/...
Downloading image http://imgs.xkcd.com/comics/free_speech.png...
Downloading page http://xkcd.com/1356/...
Downloading image http://imgs.xkcd.com/comics/orbital_mechanics.png...
Downloading page http://xkcd.com/1355/...
Downloading image http://imgs.xkcd.com/comics/airplane_message.png...
Downloading page http://xkcd.com/1354/...
Downloading image http://imgs.xkcd.com/comics/heartbleed_explanation.png...
--snip--
这个项目是一个很好的例子，说明程序可以自动顺着链接，从网络上抓取大量
的数据。你可以从Beautiful Soup的文档了解它的更多功能：http://www. crummy.com/
software/BeautifulSoup/bs4/doc/.
第5步：类似程序的想法
下载页面并追踪链接，是许多网络爬虫程序的基础。类似的程序也可以做下面的事情：