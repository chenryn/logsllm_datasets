O(log P ) other processors.
Security analysis. The oblivious nature of our algorithms
is not hard to see: in every time step, the shared memory
locations accessed by each processor is ﬁxed and independent
of the sensitive input. This can be seen from Figure 4, and the
description of practical optimizations in this section.
IV. FROM PARALLEL OBLIVIOUS ALGORITHMS TO
PARALLEL SECURE COMPUTATION
So far, we have discussed how GraphSC primitives can be
implemented as efﬁcient parallel oblivious algorithms, we now
turn our attention to how the latter translate to parallel secure
computation. In this section, we outline the reduction between
the two, focusing on a garbled-circuit backend [1] for secure
computation.
System Setting. Recall that our focus in this paper is on
secure 2-party computation. As an example, Figure 5b depicts
two non-colluding cloud service providers (e.g., Facebook
and Amazon) – henceforth referred to as the two parties.
The sensitive data (e.g., user preference data, sensitive social
graphs) can be secret-shared between these two parties. Each
party has P processors in total – thus there are in total P
pairs of processors. The two parties wish to run a parallel
secure computation protocol computing a function (e.g., matrix
factorization), over the secret-shared data.
While in general, other secure 2-party computation proto-
cols can also be employed, this paper focuses on a garbled
circuit backend [1]. Our focus is on the semi-honest model,
although this can be extended with existing techniques [6],
[68]. Using this secure model,
the oblivious algorithm is
represented as a binary circuit. One party then acts as the
garbler and the other acts as the evaluator, as illustrated in
Figure 5b. To exploit parallelization, each of the two parties
parallelize the computational task (garbling and evaluating the
circuit, respectively) across its processors. There is a one-to-
one mapping between garbler and evaluator processors: each
garbler processor sends the tables it garbles to the correspond-
ing corresponding evaluator processor, that evaluates them.
We refer to such communication as garbler-to-evaluator (GE)
communication.
Note that
there is a natural correspondence between a
parallel oblivious algorithm and a parallel secure computation
protocol:
First, each processor in the former becomes a
(garbler, evaluator) pair in the latter. Second, memory in the
former becomes secret-shared memory amongst the two par-
ties. Finally, in each time step, each processor’s computation
in the former becomes a secure evaluation protocol between a
(garbler, evaluator) pair in the latter.
Architectural choices for realizing parallelism. There are
various choices for instantiating the parallel computing archi-
tecture of each party in Figure 5b.
•Multi-core processor architecture. At each party, each proces-
sor can be implemented by a core in a multi-core processor
architecture. These processors share a common memory array.
•Compute cluster. At each party, each processor can be a ma-
chine in a compute cluster. In this case, accesses to the “shared
memory” are actually implemented with garbler-to-garbler
communication or evaluator-to-evaluator communication. In
other words, the memory is conceptually shared but physically
distributed.
•Hybrid. The architecture can be a hybrid of the above,
with a compute cluster where each machine is a multi-core
architecture.
While our design applies to all three architectures, we used
a hybrid architecture in our implementation, exploiting both
multi-core and multi-machine parallelism. Note that, in the
case of a hybrid or cluster architecture with P machines,
386386
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:05:55 UTC from IEEE Xplore.  Restrictions apply. 
Corollary 1 implies that each garbler (evaluator) communicates
with only O(log P ) other garblers (evaluators) throughout the
entire execution. In particular, both garblers and evaluators
connect through a hypercube topology. This is another desir-
able property of GraphSC.
Metrics. Using the above natural correspondence between a
parallel oblivious algorithm and a parallel secure computation
protocol, there is also a natural correspondence between the
primary performance metrics in these two settings: First, the
total work of the former directly characterizes (a) the total
work and (b) the total garbler-to-evaluator (GE) communica-
tion in the latter. Second, the parallel runtime of the former
directly characterizes the parallel runtime of the latter. We note
that, in theory, the garbler is inﬁnitely parallelizable, as each
gate can be garbled independently. However, the parallelization
of the evaluator (and, thus, of the entire system) is conﬁned
by the sequential order deﬁned by the circuit. Thus, parallel
runtime is determined by the circuit depth.
In the cluster and hybrid cases, where memory is concep-
tually shared but physically distributed, two additional metrics
may be of interest, namely, the garbler-to-garbler (GG) com-
munication and evaluator-to-evaluator (EE) communication.
These directly relate to the parallel runtime, since in each
parallel time step, each processor makes only one memory
access; hence, each processor communicates with at most one
other processor at each time-step.
V. EVALUATION
In this section we present a detailed evaluation of our
systems for a few well-known applications that are commonly
used for evaluating highly-parallelizable frameworks.
A. Application Scenarios
In all scenarios, we assume that the data is secret-shared
across two non-colluding cloud providers, as motivated in
Section IV. In all cases, we refer to the total number of vertices
and edges in the corresponding GraphSC graph as input size.
Histogram. A canonical use case of MapReduce is a word-
count (or histogram) of words across multiple documents.
Assuming a (large) corpus of documents, each comprising a
set of words, the algorithm counts word occurrences across all
documents. The MapReduce algorithm maps each word as a
key with the value of 1, and the reducer sums up the values of
all keys, resulting in the count of appearances of each word.
In the secure version, we want to compute the word frequency
histogram while hiding the text in each document. In GraphSC,
this is a simple instance of edge counting over a bipartite graph
G, where edges connect keys to words. We represent keys and
words as 16-bit integers, while accumulators (i.e., key vertex
data) are stored using 20-bit integers.
Simpliﬁed PageRank. A canonical use case of graph par-
allelization models is the PageRank algorithm. We consider
a scenario in which multiple social network companies, e.g.,
Facebook, Twitter and LinkedIn, would like to compute the
“real” social inﬂuence of users on a social graph that is the
aggregate of each company’s graph (assume users are uniquely
identiﬁed across networks by their email address). In the secure
version, each company is not willing to reveal user data and
their social graph with the other network. Vertices are identiﬁed
using 16-bit integers, and 1bit for isVertex (see Section III-C).
The PageRank value of each vertex is stored using a 40-bit
ﬁxed-point representation, with 20-bit for the fractional part.
Matrix Factorization (MF). Matrix Factorization [61] splits
a large sparse low-rank matrix into two dense low-dimension
matrices that, when multiplied, closely approximate the origi-
nal matrix. Following the Netﬂix prize competition [69], matrix
factorization is widely used in recommender systems. In the
secure version, we want to factorize the matrix and learn
the user or item feature vectors (learning both can reveal the
original input), while hiding both the ratings and items each
user has rated. MF can be expressed in GraphSC using a bi-
partite graph with vertices representing users and items, and
edges connecting each user to the items they rated, carrying the
ratings as data. In addition, data at each vertex also contains
a feature vector, corresponding to its respective row in the
user/item factor matrix. We study two methods for matrix
factorization – gradient descent and alternative least-squares
(ALS) (see, e.g., [61]). In gradient descent, the gradient is
computed for each rating separately, and then accumulated
for each user and each item feature vectors, thus it is highly
parallelizable. In ALS we alternate the computation between
user feature vectors (assuming ﬁxed item feature vectors) and
item feature vectors (assuming ﬁxed user feature vectors). For
each step, each vector solves (in parallel) a linear regression
using the data from its neighbors. Similar to PageRank, we
use 16-bit for vertex id and 1-bit for isVertex. The user and
item feature vectors are with dimension 10, with each element
stored as a 40-bit ﬁxed-point real.
The secure implementation of matrix factorization using
gradient descent has been studied by Nikolaenko et al. [3]
who, as discussed in Section I-C, constructed circuits of
linear depth. The authors used a multi-core machine to exploit
parallelization during sorting, and relied on shared memory
across threads. This limits the ability to scale beyond a single
machine, both in terms of the number of parallel processors (32
processors) as well as, crucially, input size (they considered no
more than 17K ratings, over a 128 GB RAM server).
B. Implementation
We implemented GraphSC atop ObliVM-GC, the Java-
based garbled circuit implementation that comprises the back
end of the GraphSC secure computation framework [11], [70].
ObliVM-GC provides easy-to-use Java classes for compos-
ing circuit libraries. We extend ObliVM-GC with a simple
MPI-like interface where processes can additionally call non-
blocking send and blocking receive operations. Processes
in ObliVM-GC are identiﬁed by their unique identiﬁers.
Finally, we implement oblivious sorting using the bitonic
sort protocol [64] which sorts in O(N log2 N ) time. Asymptot-
ically faster protocols such as the O(N log N ) AKS sort [66]
and the recent ZigZag sort [71] are much slower in practice
for practical ranges of data sizes.
C. Setup
We conduct experiments on both a testbed that uses a LAN,
and on a realistic Amazon AWS deployment. We ﬁrst describe
our main experiments conducted using a compute cluster
387387
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:05:55 UTC from IEEE Xplore.  Restrictions apply. 
2
1
1
77
6
5
1 Gb
3
3
4
Fig. 6: Evaluation setup, all machines are connected in a star
topology with 1Gbps links.
TABLE II: Servers’ hardware used for our evaluation.
Machine
1
2
3
4
5
6
7
#Proc Memory CPU Freq
1.9 GHz
1.9 GHz
1.9 GHz
1.9 GHz
1.9 GHz
2.1 GHz
2.6 GHz
128 GB
128 GB
64 GB
64 GB
64 GB
128 GB
256 GB
24
24
24
24
24
32
32
Processor
AMD Opteron 6282 SE
AMD Opteron 6282 SE
AMD Opteron 6282 SE
AMD Opteron 6282 SE
AMD Opteron 6282 SE
AMD Opteron 6272
AMD Opteron 6282 SE
connected by a Local Area Network. Later, in Section V-I,
we will describe results from the AWS deployment.
Testbed Setup on Local Area Network: Our experimental
testbed consists of 7 servers with the conﬁgurations detailed
in Table II. These servers are inter-connected using a star
topology with 1Gbps Ethernet links as shown in Figure 6.
All experiments (except the large-scale experiment reported
in Section V-F that uses all of them) are performed using a
pair of servers from the seven machines. These servers were
dedicated to the experiments during our measurements, not
running processes by other users.
To verify that our results are robust, we repeated the experi-
ments several times, and made sure that the standard deviation
is small. For example, we ran PageRank 10 times using 16
processors for an input length of 32K. The resulting mean
execution time was 390 seconds, with a standard deviation of
14.8 seconds; we therefore report evaluations from single runs.
D. Evaluation Metrics
We study the gains and overheads that result from our
parallelization techniques and implementation. Speciﬁcally, we
study the following key metrics:
Total Work. We measure the total work using the overall
number of AND gates for each application. As mentioned
earlier in Section III-E, the total work grows logarithmically
with respect to the number of processors P in theory – and
in practice, since we employ bitonic sort, the actual growth is
log-squared.
Actual runtimes. We report our actual runtimes and com-
pare the overhead with a cleartext baseline running over
GraphLab [9], [12], [72]. We stress that while our circuit
size metrics are platform independent, actual runtime is a
platform dependent metric. For example, we expect a factor
388388
of 20 speedup if the backend garbled circuit implementation
adopts a JustGarble-like approach (using hardware AES-NI) –
assuming roughly 2700 Mbps bandwidth provisioned between
each garbler and evaluator pair.
Speedup. The obvious ﬁrst metric to study is the speedup
in the time to run each application as a result of adding
more processors. In our applications, computation is the main
bottleneck. Therefore, in the ideal case, we should observe a
factor of x speedup with x factor more processors.
Communication. Parallelization introduces communication
overhead between garblers and between evaluators. We study
this overhead and compare it to the communication between
garblers and evaluators.
Accuracy. Although not directly related to parallelization, for
completeness we study the loss in accuracy obtained as a
result of implementing the secure version of the applications,
both when using ﬁxed-point representation and ﬂoating-point
representation of the reals.
E. Main Results
Speedup. Figure 7 shows the total computation time across the
different applications. For all applications except histogram we
show the time of a single iteration (consecutive iterations are
independent). Since in our experimental setup computation is
the bottleneck, the ﬁgures show an almost ideal linear speedup
as the number of processors grow. Figure 8 shows that our
method is highly scalable with the input size, with an almost
linear increase (a factor of O(P/ log2 P )). Figure 8a provides
the time to compute a histogram using an oblivious RAM im-
plementation. We use the state-of-the-art Circuit ORAM [53]
for this purpose. As the ﬁgure shows, the baseline is 2 orders
of magnitude slower compared to the parallel version using
two garblers and two evaluators.
Figure 8c provides the timing presented in Nikolaenko et
al. [4] using 32 processors. As the ﬁgure shows, using a similar
hardware architecture, we manage to achieve a speedup of
roughly ×16 compared to their results. Most of the perfor-
mance gains comes from the usage of GraphSC architecture
– whereas Nikolaenko et al. used a multi-threaded version of
FastGC [5] as the secure computation backend.
Total Work. Figure 9 shows that the total amount of work
grows very slowly with respect to the number of processors,
indicating that we indeed achieved a very low overhead in the
total work (and overall circuit size).
Communication. Figure 10a and Figure 10b show the amount
of total communication and per processor communication, re-
spectively, for running gradient descent. Each plot shows both
the communication between garblers and evaluators, and the
overhead introduced by the communication between garblers
(communication between evaluators is identical). Figure 10a
shows that the total communication between garblers and eval-
uators remains constant as we increase the number of proces-
sors, showing that parallelization does not introduce overhead
to the garblers-to-evaluator communication. Furthermore, the
garbler-to-garbler (GG) communication is signiﬁcantly lower
than the garblers-to-evaluator communication, showing that
the communication overhead due to parallelization is low. As
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:05:55 UTC from IEEE Xplore.  Restrictions apply. 
)
c
e
s
(
e
m
T
i
214
212
210
28
22
32K
64K
128K
256K
512K
212
210
28
)
c
e
s
(
e
m
T
i
4K
8K
16K
32K
64K
23
24
Processors
(a) Histogram
25
22
25
23
24
Processors
(b) PageRank
)
c
e
s
(
e
m
T
i
214
212
210
28
22
2K
4K
8K
16K
32K
23
24
Processors
25
)
c
e
s
(
e
m
T
i
216
214
212
210
22
(c) Gradient Descent
256
512
1K
2K
4K
25
23
24
Processors