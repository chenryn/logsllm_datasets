both the average and worst slowdowns are limited at
 = 10%, thus demonstrating that Hopper‚Äôs focus on
performance does not unduly slow down jobs.
Probe Ratio: An important component of decentral-
ized scheduling is the probe ratio ‚Äì the number of re-
)DFHERRN%LQJ5HGXFWLRQLQ$YHUDJH-RE'XUDWLRQ&'))DFHERRN%LQJ5HGXFWLRQLQ$YHUDJH-RE'XUDWLRQ/HQJWKRI-RE¬∂V'$*020406080100Overall 500LATE +Hopper vs. LATE + Sparrow-SRPTMantri + Hopper vs. Mantri + Sparrow-SRPTGRASS + Hopper vs. GRASS + Sparrow-SRPTJob Bin (Number of tasks)Reduction (%) in Average Job Duration389(a) Sensitivity
(b) (%) of Jobs Slowed
(a) Gains
(b) DAG
(c) Magnitude (%) of Slowdown
Figure 10:  Fairness. Figure (a) shows sensitivity of
gains to . Figure (b) shows the fraction of jobs that
slowed down compared to a fair allocation, and (c)
shows the magnitude of their slowdowns (average
and worst).
Figure 11: Power of d choices: Impact of the number
of probes on job completion.
quests queued at workers to number of tasks in the job.
A higher probe ratio reduces the chance of a task be-
ing stuck in the queue of a busy machine, but also in-
creases messaging overheads. While the power-of-two
choices [38] and Sparrow [36] recommend a probe ratio
of 2, we adopt a probe ratio of 4 based on our analysis
in ¬ß5.
Figure 11 conÔ¨Årms that higher probe ratios are indeed
beneÔ¨Åcial. As the probe ratio increase from 2 onwards,
the payoÔ¨Ä due to Hopper‚Äôs scheduling and straggler mit-
igation results in gains increasing until 4; at utilizations
of 70% and 80%, using 3.5 works well too. At 90% uti-
lization, however, gains start slipping even at a probe
ratio of 2.5. However, the beneÔ¨Åts at such high utiliza-
tions are smaller to begin with.
7.4 Centralized Hopper‚Äôs Improvements
To highlight the fact that Hopper is a uniÔ¨Åed design,
appropriate for both decentralized and centralized sys-
tems, we also evaluate Hopper in a centralized setting
using Hadoop and Spark prototypes. Figure 12 plots
the gains for the two prototypes with Facebook and
Bing workloads. We achieve gains of ‚àº 50% with the
Figure 12: Centralized Hopper‚Äôs gains over SRPT,
overall and broken by DAG length (Facebook work-
loads).
two workloads, with individual job bins improving by
up to 80%.
As with the decentralized setting, gains for small jobs
are lower due to the baseline of SRPT already favoring
small jobs. Between the two prototypes, gains for Spark
are consistently higher (albeit, modestly). Spark‚Äôs small
task durations makes it more sensitive to stragglers and
thus it spawns many more speculative copies. This
makes Hopper‚Äôs scheduling more crucial.
DAG of Tasks: Like in the decentralized implementa-
tion, Hopper‚Äôs gains hold consistently over varying DAG
lengths, see Figure 12. Note that there is a contrast be-
tween Spark jobs and Hadoop jobs. Spark jobs have fast
in-memory map phases, thus making intermediate data
communication the bottleneck. Hadoop jobs are less
bottlenecked on intermediate data transfer, and spend
more of their time in the map phase [13]. This diÔ¨Äer-
ence is captured via Œ±, which is learned as described in
¬ß6.3.
Data Locality: Recall from ¬ß4.4 that we achieve data
locality using a relaxation heuristic to allow any k sub-
sequent jobs (as a % of total jobs).
As Figure 13a shows, a small relaxation of k = 3%,
which is what we have used so far, achieves appreciable
increase in locality in Spark. Gains are steady for a bit
but then start dropping beyond a k value of 7%. This
is because the deviation from the theoretical guidelines
overshadows any increase in gains from locality. The
fraction of data local tasks, naturally, increases with k
(Figures 13a). Hadoop results are similar (13b).
Note that even when we enhance a centralized SRPT
scheduler to include the above locality heuristic, it gains
no more than 20% compared to centralized SRPT (with-
out the locality heuristic). This indicates that Hopper‚Äôs
gains are predominantly due to coordinated speculation
and scheduling.
8. CONCLUSIONS
With launching speculative copies of tasks being a
common approach for mitigating the impact of strag-
glers, schedulers face a decision between scheduling spec-
ulative copies of some jobs versus original copies of other
jobs. While this question is seemingly simple, we Ô¨Ånd
)DFHERRN%LQJ)DLUQHVVﬁΩ5HGXFWLRQLQ$YHUDJH-RE'XUDWLRQ)DLUQHVVﬁΩ6ORZHG-REV0510152025051015202530AverageWorstFairness …õ (%)Increase (%) in Job duration of Slowed Jobs010203040506022.533.544.55Util=60%Util=70%Util=80%Util=90%Probe RatioReduction (%) in Average Job Duration020406080100HadoopSparkJob BinReduction (%) in Average Job DurationK«Äƒû∆åƒÇ≈Ø≈Ø—Ñœ±œ¨œ±œ≠Õ≤œ≠œ±œ¨œ≠œ±œ≠Õ≤œ±œ¨œ¨—Öœ±œ¨œ¨01020304050602345678HadoopSparkReduction (%) in Average Job DurationLength of Job‚Äôs DAG390(a) Spark
(b) Hadoop
Figure 13: Centralized Hopper: Impact of Locality
Allowance (k) (see ¬ß6.2) with Facebook workload.
that the problem is not only unsolved thus far, but also
has signiÔ¨Åcant performance implications.
This paper proposes Hopper, the Ô¨Årst speculation-aware
job scheduler, and implements both decentralized and
centralized prototypes. We deploy our prototypes (built
in Sparrow [36], Spark [49] and Hadoop [3]) on a 200
machine cluster, and see job speed ups of 66% in decen-
tralized settings and 50% in centralized settings com-
pared to current state-of-the-art schedulers.
In addi-
tion to providing performance improvements in both
centralized and decentralized settings, Hopper is com-
patible with all current speculation algorithms and in-
corporates data locality, fairness, DAGs of tasks, etc.;
thus, it represents a uniÔ¨Åed speculation-aware schedul-
ing framework.
9. ACKNOWLEDGMENT
We would like to thank Michael Chien-Chun Hung,
Shivaram Venkataraman, Masoud Moshref, Niangjun
Chen, Qiuyu Peng, and Changhong Zhao for their in-
sightful discussions. We would like to thank the anony-
mous reviewers and our shepherd, Lixin Gao, for their
thoughtful suggestions. This work was supported in
part by National Science Foundation (NSF) with Grants
(CNS-1319820, CNS-1423505).
10. REFERENCES
[1] Apache Thrift. https://thrift.apache.org/.
[2] Cloudera Impala.
http://www.cloudera.com/content/cloudera/en/
products-and-services/cdh/impala.html.
[3] Hadoop. http://hadoop.apache.org.
[4] Hadoop Capacity Scheduler. http://hadoop.
apache.org/docs/r1.2.1/capacity scheduler.html.
[5] Hadoop Distributed File System.
http://hadoop.apache.org/hdfs.
[6] Hadoop Slowstart. https://issues.apache.org/jira/
browse/MAPREDUCE-1184/.
[7] Hive. http://wiki.apache.org/hadoop/Hive.
[8] Hopper Technical Report. https://sites.google.
com/site/sigcommhoppertechreport/.
[9] Sparrow. https://github.com/radlab/sparrow.
[10] The Next Generation of Apache Hadoop
MapReduce. http://developer.yahoo.com/blogs/
hadoop/posts/2011/02/mapreduce-nextgen/.
[11] G. Ananthanarayanan, S. Agarwal, S. Kandula,
A. Greenberg, I. Stoica, D. Harlan, and E. Harris.
Scarlett: Coping with Skewed Popularity Content
in MapReduce Clusters. In EuroSys, 2011.
[12] G. Ananthanarayanan, A. Ghodsi, S. Shenker,
and I. Stoica. EÔ¨Äective Straggler Mitigation:
Attack of the Clones. In USENIX NSDI, 2013.
[13] G. Ananthanarayanan, A. Ghodsi, A. Wang,
D. Borthakur, S. Kandula, S. Shenker, and
I. Stoica. PACMan: Coordinated Memory
Caching for Parallel Jobs. In USENIX NSDI,
2012.
[14] G. Ananthanarayanan, M. Hung, X. Ren,
I. Stoica, A. Wierman, and M. Yu. GRASS:
Trimming Stragglers in Approximation Analytics.
In USENIX NSDI, 2014.
[15] G. Ananthanarayanan, S. Kandula, A. Greenberg,
I. Stoica, E. Harris, and B. Saha. Reining in the
Outliers in Map-Reduce Clusters Using Mantri. In
USENIX OSDI, 2010.
[16] E. Bortnikov, A. Frank, E. Hillel, and S. Rao.
Predicting Execution Bottlenecks in Map-Reduce
Clusters. In USENIX HotCloud, 2012.
[17] E. Boutin, J. Ekanayake, W. Kin, B. Shi, J. Zhou,
Z. Qian, M. Wu, and L. Zhou. Apollo: Scalable
and Coordinated Scheduling for Cloud-Scale
Computing. In USENIX OSDI, 2014.
[18] M. Bramson, Y. Lu, and B. Prabhakar.
Randomized load balancing with general service
time distributions. In Proceedings of Sigmetrics,
pages 275‚Äì286, 2010.
[19] R. Chaiken, B. Jenkins, P. Larson, B. Ramsey,
D. Shakib, S. Weaver, and J. Zhou. SCOPE: Easy
and EÔ¨Écient Parallel Processing of Massive Data
Sets. Proceedings of the VLDB Endowment, (2),
2008.
[20] R. Chaiken, B. Jenkins, P. Larson, B. Ramsey,
D. Shakib, S. Weaver, and J. Zhou. SCOPE: Easy
and EÔ¨Écient Parallel Processing of Massive
Datasets. In VLDB, 2008.
[21] H. Chen, J. Marden, and A. Wierman. On the
Impact of Heterogeneity and Back-end Scheduling
in Load Balancing Designs. In INFOCOM. IEEE,
2009.
[22] J. Dean. Achieving Rapid Response Times in
Large Online Services. In Berkeley AMPLab
Cloud Seminar, 2012.
[23] J. Dean and L. Barroso. The Tail at Scale.
Communications of the ACM, (2), 2013.
*DLQV/RFDOLW\/RFDOLW\$OORZDQFHN5HGXFWLRQLQ$YHUDJH-RE'XUDWLRQ/RFDOLW\*DLQV/RFDOLW\/RFDOLW\$OORZDQFHN5HGXFWLRQLQ$YHUDJH-RE'XUDWLRQ/RFDOLW\391[24] J. Dean and S. Ghemawat. MapReduce:
SimpliÔ¨Åed Data Processing on Large Clusters.
Communications of the ACM, 2008.
[25] F. Dogar, T. Karagiannis, H. Ballani, and
A. Rowstron. Decentralized Task-aware
Scheduling for Data Center Networks. In ACM
SIGCOMM, 2014.
[38] A. Richa, M. Mitzenmacher, and R. Sitaraman.
The power of two random choices: A survey of
techniques and results. Combinatorial
Optimization, 2001.
[39] L. Schrage. A proof of the optimality of the
shortest remaining processing time discipline.
Operations Research, 16(3):687‚Äì690, 1968.
[26] A. Ghodsi, M. Zaharia, B. Hindman,
[40] B. Sharma, V. Chudnovsky, J. L. Hellerstein,
A. Konwinski, S. Shenker, and I. Stoica.
Dominant Resource Fairness: Fair Allocation of
Multiple Resource Types. In USENIX NSDI,
2011.
[27] R. Grandl, G. Ananthanarayanan, S. Kandula,
S. Rao, and A. Akella. Multi-Resource Packing for
Cluster Schedulers. In ACM SIGCOMM, 2014.
[28] M. Harchol-Balter, B. Schroeder, N. Bansal, and
M. Agrawal. Size-based scheduling to improve
web performance. ACM Transactions on
Computer Systems (TOCS), 21(2):207‚Äì233, 2003.
[29] B. Hindman, A. Konwinski, M. Zaharia,
A. Ghodsi, A. Joseph, R. Katz, S. Shenker, and
I. Stoica. Mesos: A Platform for Fine-Grained
Resource Sharing in the Data Center. In USENIX
NSDI, 2011.
[30] M. Isard, V. Prabhakaran, J. Currey, U. Wieder,
K. Talwar, and A. Goldberg. Quincy: Fair
Scheduling for Distributed Computing Clusters.
In ACM SOSP, 2009.
[31] M. Lin, L. Zhang, A. Wierman, and J. Tan. Joint
Optimization of Overlapping Phases in
MapReduce. Performance Evaluation, 2013.
[32] S. Melnik, A. Gubarev, J. J. Long, G. Romer,
S. Shivakumar, M. Tolton, and T. Vassilakis.
Dremel: Interactive Analysis of Web-Scale
Datasets. In VLDB, 2010.
[33] B. Moseley, A. Dasgupta, R. Kumar, and
T. Sarl¬¥os. On Scheduling in Map-reduce and
Flow-shops. In ACM SPAA, 2011.
R. Rifaat, and C. R. Das. Modeling and
Synthesizing Task Placement Constraints in
Google Compute Clusters. In ACM SOCC, 2011.
[41] J. Tan, X. Meng, and L. Zhang. Delay Tails in
MapReduce Scheduling. ACM SIGMETRICS
Performance Evaluation Review, 2012.
[42] Y. Wang, J. Tan, W. Yu, L. Zhang, and X. Meng.
Preemptive ReduceTask Scheduling for Fast and
Fair Job Completion. USENIX ICAC, 2013.
[43] A. Wierman. Fairness and scheduling in single
server queues. Surveys in Operations Research
and Management Science, 16(1):39‚Äì48, 2011.
[44] A. Wierman and M. Harchol-Balter. Classifying
scheduling policies with respect to unfairness in
an m/gi/1. In ACM SIGMETRICS Performance
Evaluation Review, volume 31, pages 238‚Äì249.
ACM, 2003.
[45] J. Wolf, D. Rajan, K. Hildrum, R. Khandekar,
V. Kumar, S. Parekh, K. Wu, and A. Balmin.
FLEX: a Slot Allocation Scheduling Optimizer for
MapReduce Workloads. In Middleware 2010.
Springer, 2010.
[46] N. Yadwadkar, G. Ananthanarayanan, and
R. Katz. Wrangler: Predictable and Faster Jobs
using Fewer Resources. In ACM SoCC, 2014.
[47] M. Zaharia, D. Borthakur, J. S. Sarma,
K. Elmeleegy, S. Shenker, and I. Stoica. Job
scheduling for multi-user mapreduce clusters. In
UC Berkeley Technical Report
UCB/EECS-2009-55, 2009.
[34] K. Ousterhout, A. Panda, J. Rosen,
[48] M. Zaharia, D. Borthakur, J. S. Sarma,
S. Venkataraman, R. Xin, S. Ratnasamy,
S. Shenker, and I. Stoica. The Case for Tiny Tasks
in Compute Clusters. In USENIX HotOS, 2013.
[35] K. Ousterhout, R. Rasti, S. Ratnasamy,
S. Shenker, and B. Chun. Making Sense of
Performance in Data Analytics Frameworks. In
USENIX NSDI, 2015.
[36] K. Ousterhout, P. Wendell, M. Zaharia, and
I. Stoica. Sparrow: Distributed, Low Latency
Scheduling. In ACM SOSP, 2013.
[37] K. Pruhs, J. Sgall, and E. Torng. Online
scheduling. Handbook of scheduling: algorithms,
models, and performance analysis, pages 15‚Äì1,
2004.
K. Elmeleegy, S. Shenker, and I. Stoica. Delay
Scheduling: A Simple Technique for Achieving
Locality and Fairness in Cluster Scheduling. In
ACM EuroSys, 2010.
[49] M. Zaharia, M. Chowdhury, T. Das, A. Dave,
J. Ma, M. McCauley, M. Franklin, S. Shenker,
and I. Stoica. Resilient Distributed Datasets: A
Fault-Tolerant Abstraction for In-Memory Cluster
Computing. In USENIX NSDI, 2012.
[50] M. Zaharia, A. Konwinski, A. D. Joseph, R. Katz,
and I. Stoica. Improving MapReduce Performance
in Heterogeneous Environments. In USENIX
OSDI, 2008.
392