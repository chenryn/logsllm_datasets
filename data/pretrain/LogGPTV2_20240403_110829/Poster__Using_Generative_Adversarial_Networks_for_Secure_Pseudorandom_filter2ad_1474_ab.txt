pyramid is constructed to generate high-resolution images based on low-resolution
images. Karras et al. (2017) proposed a new training method called progressive
training, which ﬁrst generates realistic images at a resolution of 1024 × 1024.
Brock et al. (2018) pointed out that GANs beneﬁt dramatically from scaling
up the batch size and the number of channels in each layer, and the proposed
BigGANs substantially improve the performance on complex datasets such as
ImageNet (Russakovsky et al. 2015). Although GANs can generate photorealistic
images that even humans cannot distinguish from real images, it is limited to
some simple datasets whose objects are highly “templated” and centered with
small margins, such as face datasets. For complex datasets, such as scene datasets,
the performance of GANs remains limited, and people can easily distinguish the
generated images from real ones.
The second challenge is training stability. Generally speaking, training GANs is
difﬁcult in practice due to the mode collapse problem (Radford et al. 2015; Metz
et al. 2016). Mode collapse means that the generator can fool the discriminator
by generating from only one mode (i.e., generating very similar images). Some
works (Arjovsky et al. 2017; Nowozin et al. 2016) sought to address this problem
by analyzing the objective functions of GANs. Nowozin et al. (2016) proposed
the f-GANs that generalize the original GANs (Goodfellow et al. 2014), which
correspond to Jensen-Shannon divergence, to any type of f-divergence. Arjovsky
et al. (2017) proposed the Wasserstein GANs (WGANs), which use the Wasser-
stein distance to measure the distance between the generated and real samples.
Regularization techniques are also effective in improving the training stability of
GANs, such as gradient penalty (Gulrajani et al. 2017) and spectral normalization
(Miyato et al. 2018). Gulrajani et al. (2017) proposed that a penalty on the gradient
norm can be used to enforce the Lipschitz constraint in the Wasserstein distance.
Miyato et al. (2018) proposed spectral normalization that constrains the spectral
norm of each layer to control the Lipschitz constant of the discriminator. Note that
the improvement of training stability can usually lead to higher-quality generated
images.
The third challenge is the evaluation of GANs. Inception score (IS) (Salimans
et al. 2016) and Fréchet inception distance (FID) (Heusel et al. 2017) are two
widely used evaluation metrics for GANs. IS correlates the image quality with the
degree to which the images are highly classiﬁable using a pre-trained classiﬁer.
Bibliography
5
Algorithm 1 Training process of GANs
for number of training iterations do
• Sample a batch of real images x from training data.
• Sample a batch of noise vectors z from Gaussian distribution.
• Use z to generate a batch of fake images x
from the generator.
• Update the discriminator using UPDATE_DISCRIMINATOR(x, x
∗
• Update the generator using UPDATE_GENERATOR(x
).
∗
∗
).
end for
function UPDATE_DISCRIMINATOR(x, x
∗
)
• Compute the discriminator’s prediction for x and x
• Compute the classiﬁcation error for x and x
• Update the discriminator’s parameters to minimize the classiﬁcation error.
∗
∗
.
.
end function
function UPDATE_GENERATOR(x
∗
)
• Compute the discriminator’s prediction for x
• Compute the classiﬁcation error for x
• Update the generator’s parameters to maximize the classiﬁcation error of
the discriminator.
∗
∗
.
.
end function
FID models the features of generated and real data as continuous multivariate
Gaussian distributions and uses the Fréchet distance to measure the distance
between generated and real data. Although IS and FID are widely used, questions
remain, such as the use of pre-trained networks and the approximations of Gaussian
distributions (Borji 2019).
Bibliography
Arjovsky M, Chintala S, Bottou L (2017) Wasserstein GAN.
In: International conference on
machine learning (ICML), pp 214–223
Arora S, Ge R, Liang Y, Ma T, Zhang Y (2017) Generalization and equilibrium in generative
adversarial nets (GANs). arXiv:1703.00573
Berthelot D, Schumm T, Metz L (2017) BEGAN: boundary equilibrium generative adversarial
networks. arXiv:1703.10717
Borji A (2019) Pros and cons of GAN evaluation measures. Comput Vis Image Underst 179:41–65
Brock A, Donahue J, Simonyan K (2018) Large scale GAN training for high ﬁdelity natural image
synthesis. arXiv:1809.11096
6
1 Generative Adversarial Networks (GANs)
Denton E, Chintala S, Szlam A, Fergus R (2015) Deep generative image models using a Laplacian
In: Advances in neural information processing systems
pyramid of adversarial networks.
(NeurIPS), pp 1486–1494
Goodfellow I, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D, Ozair S, Courville A, Bengio
Y (2014) Generative adversarial nets. In: Advances in neural information processing systems
(NeurIPS), pp 2672–2680
Goodfellow I, Bengio Y, Courville A (2016) Deep learning. MIT Press, Cambridge, MA
Gulrajani I, Ahmed F, Arjovsky M, Dumoulin V, Courville A (2017)
Improved training of
In: Advances in neural information processing systems (NeurIPS), pp
Wasserstein GANs.
5767–5777
He K, Zhang X, Ren S, Sun J (2016) Deep residual learning for image recognition. In: Computer
vision and pattern recognition (CVPR), pp 770–778
Heusel M, Ramsauer H, Unterthiner T, Nessler B, Hochreiter S (2017) GANs trained by a two
time-scale update rule converge to a local nash equilibrium. In: Advances in neural information
processing systems (NeurIPS), pp 6626–6637
Hinton G, Salakhutdinov R (2006) Reducing the dimensionality of data with neural networks.
Science 313(5786):504–507
Huang X, Li Y, Poursaeed O, Hopcroft J, Belongie S (2017) Stacked generative adversarial
networks. In: Computer vision and pattern recognition (CVPR), pp 5077–5086
Isola P, Zhu J-Y, Zhou T, Efros AA (2017) Image-to-image translation with conditional adversarial
networks. In: Computer vision and pattern recognition (CVPR), pp 5967–5976
Jolicoeur-Martineau A (2018) The relativistic discriminator: a key element missing from standard
GAN. arXiv:1807.00734
Karras T, Aila T, Laine S, Lehtinen J (2017) Progressive growing of GANs for improved quality,
stability, and variation. arXiv:1710.10196
Kingma DP, Welling M (2013) Auto-encoding variational Bayes. arXiv:1312.6114
Larochelle H, Murray I (2011) The neural autoregressive distribution estimator. In: International
conference on artiﬁcial intelligence and statistics (AISTATS), pp 29–37
Ledig C, Theis L, Huszar F, Caballero J, Cunningham A, Acosta A, Aitken A, Tejani A, Totz
J, Wang Z, Shi W (2017) Photo-realistic single image super-resolution using a generative
adversarial network. In: Computer vision and pattern recognition (CVPR), pp 4681–4690
Long J, Shelhamer E, Darrell T (2015) Fully convolutional networks for semantic segmentation.
In: Computer vision and pattern recognition (CVPR), pp 3431–3440
Metz L, Poole B, Pfau D, Sohl-Dickstein J (2016) Unrolled generative adversarial networks.
arXiv:1611.02163
Miyato T, Kataoka T, Koyama M, Yoshida Y (2018) Spectral normalization for generative
adversarial networks. arXiv:1802.05957
Nowozin S, Cseke B, Tomioka R (2016)
f-GAN: training generative neural samplers using
variational divergence minimization. In: Advances in neural information processing systems
(NeurIPS), pp 271–279
Odena A, Olah C, Shlens J (2016) Conditional image synthesis with auxiliary classiﬁer GANs.
arXiv:1610.09585
Radford A, Metz L, Chintala S (2015) Unsupervised representation learning with deep convolu-
tional generative adversarial networks. arXiv:1511.06434
Reed S, Akata Z, Yan X, Logeswaran L, Schiele B, Lee H (2016) Generative adversarial text-to-
image synthesis. In: International conference on machine learning (ICML), pp 1060–1069
Ren S, He K, Girshick R, Sun J (2015) Faster R-CNN: towards real-time object detection with
region proposal networks. In: Advances in neural information processing systems (NeurIPS),
pp 91–99
Russakovsky O, Deng J, Su H, Krause J, Satheesh S, Ma S, Huang Z, Karpathy A, Khosla A,
Bernstein M, Berg AC, Fei-Fei L (2015) ImageNet large scale visual recognition challenge. Int
J Comput Vis 115:211–252
Salakhutdinov R, Hinton G (2009) Deep Boltzmann machines. In: International conference on
artiﬁcial intelligence and statistics, pp 448–455
Bibliography
7
Salimans T, Goodfellow I, Zaremba W, Cheung V, Radford A, Chen X, Chen X (2016) Improved
In: Advances in neural information processing systems
techniques for training GANs.
(NeurIPS), pp 2226–2234
Zhang H, Goodfellow I, Metaxas D, Odena A (2018)
Self-attention generative adversarial
networks. arXiv:1805.08318