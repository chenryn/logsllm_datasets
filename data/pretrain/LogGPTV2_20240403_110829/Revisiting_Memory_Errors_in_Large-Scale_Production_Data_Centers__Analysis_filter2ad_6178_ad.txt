+
e
0
.
0
●
●
1
2
Chip density (Gb)
●
4
Fig. 7: The relative per-cell failure rate at different technology nodes (chip
densities).
B. DIMM Vendor
DIMM vendors purchase chips from DRAM chip manufac-
turers and assemble them into DIMMs. While we have infor-
mation on DIMM manufacturer, we do not have information
on the DRAM chip manufacturers in our systems.
Figure 8 shows the failure rate for servers with different
DIMM vendors.6 We observe that failure rate varies by over
2(cid:2) between vendors (e.g., Vendor B and Vendor C). The dif-
ferences between vendors can arise if vendors use less reliable
chips from a particular foundry or build DIMMs with less
reliable organization and manufacturing. Prior work [48, 10]
also found a large range in the server failure rate among vendors
of 3:9(cid:2).
t
e
a
r
e
r
u
l
i
a
f
r
e
v
r
e
s
e
v
i
t
l
a
e
R
0
0
1
.
0
5
0
.
0
0
.
0
A
B
C
D
DIMM vendor (anonymized)
Fig. 8: Relative server failure rate for different vendors varies widely.
C. DIMM Architecture
We next examine how DIMM architecture affects server
failure rate. We examine two aspects of DIMM design that have
not been studied in published literature before: the number of
data chips (not including chips for ECC) per DIMM and the
transfer width of each chip.
Figure 9 plots the failure rate for servers with DIMMs with
different numbers of data chips for each of the densities that
we examine. The DIMMs that we examine have 8, 16, 32, and
48 chips. We make two observations from Figure 9.
Data chips per DIMM
Chip transfer width (bits)
Fig. 9: The relative failure rate of
servers with DIMMs with different
numbers of data chips separated by
chip density.
Fig. 10: The relative failure rate of
servers with DIMMs with different
chip transfer widths separated by
chip density.
First, for a given number of chips per DIMM, servers with
higher chip densities generally have higher average failure rates.
This illustrates how chip density is a ﬁrst-order effect when
considering memory failure rate (as we showed in Figure 6).
Second, we ﬁnd that server failure rate trends with respect
to chips per DIMM are dependent on the transfer width of the
chips – the number of data bits each chip can transfer in one
clock cycle. In order to transfer data at a similar rate, DIMMs
with fewer (8 or 16) chips must compensate by using a larger
transfer width of 8 bits per clock cycle (and are called (cid:2)8
devices) while DIMMs with more chips (32 or 48) can use a
smaller transfer width of 4 bits per clock cycle (and are called
(cid:2)4 devices). We have annotated the graph to show which chip
counts have transfer widths of (cid:2)4 bits and (cid:2)8 bits.
We observe two trends depending on whether chips on a
DIMM have the same or different transfer widths. First, among
chips of the same transfer width, we ﬁnd that increasing the
number of chips per DIMM increases server failure rate. For
example, for 4 Gb devices, increasing the number of chips from
8 to 16 increases failure rate by 40.8% while for 2 Gb devices,
increasing the number of chips from 32 to 48 increases failure
rate by 36.1%. Second, once the number of chips per DIMM
increases beyond 16 and chips start using a different transfer
width of (cid:2)8, there is a decrease in failure rate. For example,
for 1 Gb devices, going from 16 chips with a (cid:2)8 interface to
32 chips with a (cid:2)4 interface decreases failure rate by 7.1%.
For 2 Gb devices, going from 8 chips with a (cid:2)8 interface to 32
chips with a (cid:2)4 interface decreases failure rate by 13.2%.
To conﬁrm the trend related to transfer width, we plotted
the failure rates dependent on transfer width alone in Figure 10.
We ﬁnd that, in addition to the ﬁrst-order effect of chip density
increasing failure rate (Effect 1), there is a consistent increase
in failure rate going from (cid:2)4 to (cid:2)8 devices (Effect 2).
We believe that both effects may be partially explained by
considering how number of chips and transfer width contribute
to the electrical disturbance within a DIMM that may disrupt
the integrity of the signal between components. For example,
a larger transfer width increases internal data transfer current
(e.g., IDD4R=W in Table 19 of [35], which compares the power
consumption of (cid:2)4 and (cid:2)8 DRAM devices), leading to addi-
tional power noise across the device. Such power noise could
induce additional memory errors if, for example, charge were
to get trapped in components. Interestingly, we ﬁnd that, for a
given chip density, the best architecture for device reliability
occurs when there is, ﬁrst, low transfer width and, second, low
chips per DIMM. This is shown by the 2 Gb devices with 32
chips with a (cid:2)4 interface compared to the other 2 Gb devices
in Figure 9.
6We have made the vendors anonymous.
421421
D. Workload Characteristics
We next examine how workload-related characteristics such
as CPU utilization (the average utilization of the CPUs in a
system), memory utilization (the fraction of physical memory
pages in use), and workload type affect server failure rate. Prior
work examined CPU utilization and memory utilization and
found that they were correlated positively with failure rate [44].
We measure CPU utilization as the fraction of non-idle
cycles versus total cycles across the CPUs in a server. Due to
software load balancing, we ﬁnd that CPU utilization among
cores in a server running the same workload are relatively
similar, and so the average utilization across the cores is rea-
sonably representative of each core’s individual utilization. We
measure memory utilization as the fraction of pages allocated
by the OS. Note that memory utilization does not describe how
the cells in pages are accessed. For this reason, we examine
workloads as a proxy for how cells are accessed. We plot how
CPU utilization and memory utilization are related to server
failure rate in Figures 11 and 12.
●
1 Gb
2 Gb
4 Gb
●
1 Gb
2 Gb
4 Gb
e
t
a
r
e
r
u
l
i
a
f
r
e
v
r
e
s
e
v
i
t
l
a
e
R
0
0
.
1
0
5
.
0
0
0
.
0
e
t
a
r
e
r
u
l
i
a
f
r
e
v
r
e
s
e
v
i
t
l
a
e
R
0
0
.
1
0
5
.
0
0
0
.
0
●
●
●
●
●
●
● ●
●
●
●
●
●
●
0
0.25 0.5 0.75
1
0
0.25 0.5 0.75
1
CPU utilization
Memory utilization
Fig. 11: The relative failure rate of
servers with different average CPU
utilizations.
Fig. 12: The relative failure rate
of servers with different average
memory utilizations.
Contrary to what was observed in prior work, we do not
ﬁnd a correlation between either CPU utilization or memory
utilization and failure rate. We observe multiple local maxima
for failure rate compared to CPU utilization and memory
utilization across all the chip densities. We believe that this
is due to the more diverse workloads that we examine (Table I)
compared to prior work [44, 47, 48, 10], which mainly exam-
ined a homogeneous workload. The implications of this are that
memory failure rate may depend more on the type of work as
opposed to the CPU or memory utilization the work causes.
To examine how the type of work a server performs affects
failure rate, we plotted the server failure rate for the different
workload types at Facebook in Figure 13. We observe that, de-
pending on the workload, failure rate can vary by up to 6:5(cid:2), as
shown by the difference between servers executing a Database-
type workload compared to those executing a Hadoop-type
workload. While we leave the detailed examination of how
workloads affect memory failure rate to future work, we hy-
pothesize that certain types of workload memory access patterns
may increase the likelihood of errors. For example, prior work
has shown that memory errors can be induced in a controlled
environment by accessing the same memory row in rapid
succession [23]. Such an access pattern involves modifying
data and writing it back to memory using the clflush and
mfence instructions. We believe it would be interesting to
examine what types of workloads exhibit this behavior.
E. Server Age
We examine next how age affects server failure rate. The
servers we analyzed were between one and four years old, with
an average age of between one and two years. Figure 14 shows
the monthly failure rate for servers of different ages. We observe
e
t
a
r
e
r
u
l
i
a
f
r
e
v
r
e
s
e
v
i
t
l
a
e
R
0
0
.
1
0
5
.
0
0
0
.
0
b
e
W
t
s
e
g
n
I
p
o
o
d
a
H
i
a
d
e
M
e
s
a
b
a
t
a
D
e
h
c
a
c
m
e
M
Fig. 13: The relative failure rate of servers that run different types of
workloads (Table I) can vary widely.
that chip density once again plays a large role in determining
server failure rate: For a given age, servers with 4 Gb devices
have a 15.3% higher failure rate on average than 2 Gb devices,
and servers with 2 Gb devices have a 23.9% higher failure rate
on average than 1 Gb devices.
We do not observe any general age-dependent
trend in
server failure rate when controlling for the effects of density
alone. One reason for this is that age is correlated with other
server characteristics. For example, we ﬁnd that in addition to
being correlated with chip density (correlation coefﬁcient of
(cid:3)0:69), age is also correlated with the number of CPUs in a
system (correlation coefﬁcient of (cid:3)0:72). Figure 15 shows the
trend for age for different combinations of chip density and
CPUs (which we will denote as hx; yi where x is chip density
in Gb and y is number of CPUs). We make two observations
from Figure 15.
First, we ﬁnd that among systems of the same age, more
cores lead to higher failure rates. For example, consider the
h2;(cid:4)i systems that are two years of age: going from 4 ! 12
cores increases failure rate by 21.0% and going from 12 ! 16
cores increases failure rate by 22.2%. Figure 16, which plots
the server failure rate with respect to different numbers of CPUs
conﬁrms this trend, with 2 Gb systems with 16 cores having a
40.0% higher failure rate than 2 Gb systems with 4 cores. This
could potentially be due to more cores accessing DRAM more
intensely and wearing out DRAM cells at a faster rate, a failure
mode that was shown in a prior controlled study [6].
The most related trend observed in prior work was that
CPU frequency was shown to be correlated with error rate [40].
The trend we observe with respect to CPU count is signiﬁcant
because the number of CPU cores per processor is increasing
at a much faster rate than CPU frequency and so our results
allow us to predict that future processor generations will likely
continue to induce higher rates of errors in DRAM devices.
Second, among systems with the same number of cores,
older machines generally have higher failure rates than younger
machines. For example, for the h2; 12i system, average failure
rate increases by 2.8% going from 2 ! 3 years of age, and
average failure rate increases by 7.8% going from 3 ! 4 years
of age. This is consistent with prior observations from the ﬁeld
that showed that failure rates can increase with age [44], though
we observe a much clearer trend compared to prior work (e.g.,
Figure 10 in [44] shows large ﬂuctuations in failure rate over