converging  to  the  available  bandwidth.  In  summary,  one  bit  of 
ECN feedback merely indicates the presence of congestion, but it 
carries no information about the extent of congestion. 
To  address  this  problem,  DCTCP  elegantly  aggregates  the 
one-bit ECN feedback from multiple packets and multiple RTTs to 
form  a  multiple-bit,  weighted-average  metric  for  sizing  the 
window.  Using  this  metric,  the  senders  modulate  their  window 
sizes in a graceful manner, without thrashing. As a result, DCTCP 
reduces the 99th percentile of network latency in OLDIs by 29%. 
Thus, DCTCP frees up more time for computation in OLDIs. 
2.4  Deadline Driven Delivery (D3) Protocol 
While DCTCP addresses the first issue mentioned in Section 
2.2 (packet drops causing time-outs), DCTCP, being a fair-share 
protocol,  does  not  address  the  second  issue  (lack  of  deadline 
awareness). As such, a later work called Deadline Driven Delivery 
(D3) shows that DCTCP may cause up to 7% of the deadlines to 
1 The use of RED [8] without ECN merely triggers this feedback 
earlier  than  the  onset  of  full  congestion,  but  the  feedback 
mechanism remains the same -- a packet drop. 
2 Strictly speaking TCP has an RTT-bias but we use the term fair-
share  as  TCP  tries  to  treat  flows  equally  and  the  bias  is  an 
undesired side-effect. 
Request with far deadline
Request with near deadline
requests paused
requests granted
b/w requests arriving at switch
D3 switch grants 
requests FCFS
Figure 2: D3's priority inversion 
the 
be  missed  [25].  Citing  TCPâ€™s  problems,  D3â€™s  authors  argue  that 
TCP  is  fundamentally  ill-suited  for  OLDIs  and  opt  for  an 
altogether new protocol.  
D3  pioneered  the  idea  of  incorporating  deadline  awareness 
into the network. To that end, D3 proactively allocates bandwidth 
based  on 
idea  of  bandwidth  reservation  before  data 
transmission. As  applications  know  the  deadline  for  a  message, 
they pass this information to the transport layer in the request to 
send. Based on their deadlines and the amount of remaining data, 
senders must request bandwidth every round trip time (RTT) and 
send  only  the  corresponding  amount  of  data.  Switches  receive 
these  requests,  compute  a  decision,  and  place  the  decisions  into 
the  packet  header.  Thus,  the  senders  learn  how  much  data  to 
transmit  in  the  next  RTT.  Because  requests  are  made  in  a 
deadline-aware  manner,  D3  dramatically  reduces  the  fraction  of 
missed deadlines compared to DCTCPâ€™s [25]. 
2.4.1  D3 operation 
For every RTT, a sender computes the needed bandwidth as 
the  amount  of  remaining  data  divided  by  the  time  until  the 
deadline. The sender places the request into the D3 packet header. 
The very first request in a flow uses a SYN-like packet carrying 
the  request  and  no  data  payload.  In  subsequent  RTTs,  the 
bandwidth  request  for  the  next  RTT  is  piggybacked  on  the  data 
packets of the current RTT. 
The switches receive these packets and extract the bandwidth 
request.  The  switches  also  maintain  state  for  the  bandwidth 
already allocated (and hence the remaining bandwidth) along the 
path in question. Based on this state, each switch makes a greedy 
choice and tries to grant as much of the request as possible. The 
switch places its grant-response in the packet header and forwards 
the packet. Each switch along a packet's path performs the same 
action,  creating  a  vector  of  bandwidth  grants  in  the  header. The 
receiver copies this vector into the ACK packet back to the sender. 
The sender extracts the vector and chooses the minimum of all the 
grants to decide how much data to transmit for the next RTT. 
2.4.2  Challenges in centralized and proactive 
scheduling 
D3  employs  a  centralized  and  pro-active  approach  to 
scheduling  the  network.  To  maximize  network  utilization,  the 
switches allocate bandwidth greedily on a first-come-first-served 
basis.  Because  fan-in-burst-induced  congestion  is  common  in 
OLDIs,  near-  and  far-deadline 
traffic  often  competes  for 
bandwidth.  Unfortunately,  D3â€™s  greedy  approach  may  allocate 
bandwidth to far-deadline requests arriving slightly ahead of near-
deadline  requests  (see  Figure  2).  Due  to  this  race  condition,  D3 
causes  frequent  priority  inversions,  which  contribute  to  missed 
deadlines. Indeed, our results in Section 4.2.2 show that D3 inverts 
the priority of 24%-33% requests. 
Fixing 
these  priority 
the  same  priority  inversions  may  happen  again 
inversions  within  a  centralized, 
proactive  design  space  is  hard  for  multiple  reasons.  First,  given 
the rapid arrival rate and tight deadlines of flows, it is not feasible 
for  a  switch  to  wait  and  gather  information  on  all  flows  before 
making decisions. Instead the switch must make decisions on-the-
fly  without  knowledge  of  near-future  requests.  Second,  because 
maintaining detailed per-flow state at the switches is prohibitively 
complex, D3 has no memory of which flows recently encountered 
priority inversions. If requests continue to arrive in an unfavorable 
order, 
in 
subsequent  RTTs.  Third,  maintaining  some  spare  bandwidth  in 
anticipation  of  future  bursts  is  also  hampered  by  the  lack  of 
detailed  per  flow  state.  Allocation  of  such  spare  bandwidth 
requires a priority list, so the switch can decide whether a given 
request should be granted out of the spare pool or be denied. Such 
a  priority  list  again  requires  tracking  detailed  per-flow  state. 
Further,  a  large  spare  may  underutilize  the  network  whereas  a 
small  spare  may  be  insufficient  to  absorb  bursts. A  scheme  that 
dynamically  adapts  the  amount  of  the  spare  bandwidth,  must 
balance  the  conflicting  needs  of  maximizing  utilization  and 
accommodating  near-deadline  bursts,  which  is  an  open  and 
complex research problem.  
The  above  analysis  assumes  that  D3â€™s  bandwidth  requests 
contain  deadline  information,  even  though  as  proposed  in  [25], 
D3â€™s bandwidth requests do not contain deadline information.  
2.4.3  Challenges in practical deployment 
To  handle  requests  at  line  rates,  D3  requires  custom  switch 
ASICs. Unfortunately, such OLDI-specific, custom silicon would 
not only incur high cost due to low volumes, but also incur long 
turn-around time hindering near-term deployment.  
interoperability 
retransmits.  Protocol 
Further,  D3,  as  proposed,  cannot  coexist  with  TCP.  TCP 
flows passing through the same switches as D3 flows would not 
recognize  D3's  honor-based  bandwidth  allocation.  Placing  TCP 
and D3 flows in separate QoS classes would provide isolation but 
would  also  require  partitioning  the  bandwidth  among  the  QoS 
classes.  Such  partitioning  has  far-reaching  consequences  and 
raises 
issue  of  optimizing  multiple  applicationsâ€™ 
bandwidth  shares,  fairness,  and  network  utilization.  Tunneling 
TCP  traffic  inside  D3  is  problematic  because  it  results  in  two 
nested flow control loops with conflicting behavior. For example, 
TCP may increment its window size, while D3 is decrementing it, 
and  the  surplus  segments  would  incur  TCP  time-outs,  back-off 
and 
in 
datacenters, though less stringent than those in the Internet, cannot 
be  ignored  completely  as  suggested  by  D3.  Due  to  the  lack  of 
interoperability,  the  upgrade  of  switches  and  applications  to  D3 
would  all  have  to  occur  in  an  atomic  manner.  For  such  large, 
invasive  changes  to  be  atomic,  the  datacenter  would  have  to  be 
unavailable  for  a  long  enough  time  that  the  changes  are  all  but 
implausible. Indeed, datacenter infrastructure upgrades are almost 
always  incremental  for  this  reason,  so  that  old  and  new 
technologies  coexist  for  some  time.  Finally,  it  may  not  be 
reasonable to expect all application writers to abandon TCP and 
develop D3 versions just because some applications desire the use 
of  D3. As  such,  any  datacenter  protocol  must  be  able  to  coexist 
with legacy TCP to be deployable in the real world. 
3.  Deadline-Aware Datacenter TCP (D2TCP) 
We  now  describe  the  design  of  Deadline-Aware  Datacenter 
TCP  (D2TCP),  a  novel  protocol  for  datacenter  networks. We  set 
out with the explicit goals of not requiring custom hardware and 
supporting  coexistence  with  legacy  TCP.  The  basic  idea  behind 
D2TCP is to modulate the congestion window size based on both 
deadline  information  and  the  extent  of  congestion.  In  designing 
requirements 
the  hard 
1.0
p
d  1
Î±
1.0
Figure 3: Gamma-correction function for congestion 
avoidance (ğ’‘=ğœ¶ğ’…) 
traffic 
Meeting  deadlines 
D2TCP,  we  make  two  contributions:  D2TCPâ€™s  distributed  and 
reactive approach for allocating bandwidth and D2TCPâ€™s deadline-
aware congestion avoidance algorithm. 
3.1  Distributed and Reactive Allocation 
fan-in-burst-induced  congestion 
requires  knowledge  of  the  flowsâ€™  deadlines  and  the  extent  of 
congestion.  However,  having  complete  and  up-to-date  global 
information for all flows in a datacenter is technically infeasible 
given the  rapid arrival  rate and latency of  flows. Therefore,  any 
network scheduling scheme must make decisions with incomplete 
information, and the challenge is to choose a compromise that is 
well suited to OLDIs in datacenters. 
in 
D3â€™s  centralized  and  proactive  approach  compromises  the 
handling of future bursts in order to maximize utilization. Because 
is  bursty,  D3  suffers  from  frequent  priority 
OLDI 
inversions,  making  D3â€™s  compromise  not  well  suited 
to 
datacenters.  In  contrast,  D2TCP  inherits  TCPâ€™s  distributed  and 
reactive nature, and adds deadline awareness to it. While D2TCP, 
like  D3,  also  makes  decisions  without  complete  and  accurate 
information,  D2TCPâ€™s  compromises  are  better  suited 
for 
datacenters. 
D2TCP  modulates  the  window  size  based  on  the  deadlines 
and the extent of congestion. Each D2TCP sender sizes its window 
without knowing how many other flows are congested and by how 
much  other  flows  will  back  off.  The  risk  here  is  that  multiple 
congested senders with tight deadlines may refuse to back off and 
over-subscribe to bandwidth at the congestion point. Fortunately, 
due  to  their  stateful  nature,  the  senders  can  correct  this  over-
subscription by reacting to future congestion feedback in a careful 
and calculated manner, ensuring that the oversubscription is only 
temporary  and  small.  Networks  are  equipped  to  deal  with 
temporary oversubscriptions by virtue of packet buffers. We note 
that while D2TCP inherits itâ€™s distributed and reactive nature from 
TCP, our contributions are in adding deadline awareness without 
abandoning TCPâ€™s time-tested distributed and reactive approach; 
and  in  identifying  and  analyzing  the  fundamental  difference 
between  D3  and  D2TCP,  and  explaining  why  those  differences 
matter in the context of datacenter network protocols. 
To  summarize,  in  contrast  to  D3â€™s  centralized  bandwidth 
allocation at the switches, which rules out per-flow state, D2TCPâ€™s 
distributed approach allows the hosts to maintain the needed state 
without  changing  the  switch  hardware.  In  contrast  to  D3â€™s  pro-
active approach, which does not allow for correcting the decisions 
resulting from inaccurate information, D2TCPâ€™s reactive approach 
allows  senders  to  correct  any  temporary  and  small  over-
subscription  of  the  network.  Thus,  the  compromises  made  by  a 
distributed, reactive scheme are better suited for datacenters. 
3.2  Deadline-aware Congestion Avoidance 
Our  second  contribution  is  D2TCPâ€™s  novel  congestion 
avoidance  algorithm.  Like  D3,  we  assume  that  applications  pass 
the  deadline  information  to  the  transport  layer  in  the  request  to 
send data. This information then enables D2TCP to modulate the 
congestion  window  size  in  a  deadline-aware  manner.  When 
congestion occurs, far-deadline flows back off aggressively, while 
near-deadline flows back off only a little or not at all. With such 
deadline-aware congestion management, not only can the number 
of missed deadlines be reduced, but also tighter deadlines can be 
met  because  the  network  adapts  to  the  time  budget.  D2TCP 
requires no changes to the switch hardware, and only requires that 
the  switches  support  ECN,  which  is  true  of  todayâ€™s  datacenter 
switches.  Therefore,  D2TCP  deployment  amounts  to  merely 
upgrading the TCP and RPC stacks. 
3.2.1  Congestion avoidance algorithm 
The easiest way to explain the D2TCP congestion avoidance 
algorithm is to start with DCTCP and build deadline awareness on 
top  of  it.  We  expect  that  the  switches  are  ECN-capable  and  are 
configured  to  mark  CE  bits  when  the  packet  buffer  occupancy 
weighted  average  that  quantitatively  measures  the  extent  of 
congestion: 
exceeds  a  certain  threshold.  Like  DCTCP,  we  maintain ğ›¼,  a 
Here ğ‘“ is the fraction of packets that were marked with CE 
bits in the most recent window, and ğ‘” is the weight given to new 
know that a larger d implies a closer deadline. Based on ğ›¼ and d 
samples. We now define d as the deadline imminence factor, and 
explain its derivation later in Section 3.2.3. For now it suffices to 
ğ›¼=(1âˆ’ğ‘”)Ã— ğ›¼+ğ‘” Ã—ğ‘“ 
we compute p, the penalty function applied to the window size, as 
follows: 
congestion window W as follows: 
This function was originally proposed for color correction in 
graphics  [2],  and  was  dubbed  gamma-correction  because  the 
original paper uses Î³ as the exponent. Note that being a fraction, 
ğ’‘=ğœ¶ğ’… 
ğ›¼ â‰¤ 1  and  therefore, ğ‘â‰¤ 1.  After  determining  p,  we  resize  the 
             ğ‘Š=ğ‘ŠÃ—1âˆ’ğ‘2,
ğ‘–ğ‘“ ğ‘>0         
          =  ğ‘Š+1,                     ğ‘–ğ‘“ ğ‘=0 
In  the  case  where ğ›¼  is  zero  (i.e.,  no  CE-marked  packets, 
all  packets  are CE-marked, ğ›¼=1  and  therefore ğ‘=1, then  the 
window size gets halved similar to TCP. For ğ›¼ between 0 and 1 
the window size is modulated by ğ‘. 
Figure  3  plots ğ‘,  with  a  number  of  different  curves  for 
and hence ğ‘ =ğ›¼. The curves below the straight line are for d > 1 
Note that when ğ‘=ğ›¼ the behavior matches DCTCP. Essentially, 
indicating  absence  of  congestion)  and  therefore  p  is  zero,  the 
window size is grown by one segment similar to TCP. And when 
(i.e.,  near-deadline  flows  incur  lower  penalty),  and  the  curves 
above are for  d  L and (b) Tc 1  (i.e.,  near-
in ğ›¼ until ğ›¼ gets close to 1, at which point p rapidly converges to 
is, when ğ›¼ = 1, p = 1 the window size gets halved just like regular 
When ğ‘‘>1  (i.e.,  far-deadline  flows), ğ‘  increases  rapidly 
even with small increases in ğ›¼, and approaches 1 as ğ›¼ catches up 
to 1. Thus, even minor congestions cause rapid reduction in far-
deadline  flowsâ€™  window  sizes,  but  severe  congestions  do  not 
penalize the flows any more than regular TCP or DCTCP would.   
The combination of d  1 behaviors complement 
each  other  under  congestion  situations.  Far-deadline  flows 
relinquish bandwidth so that near-deadline flows can have greater 
short-term share in order to meet their deadlines. Furthermore, if 
congestion  continues  to  worsen  after  far-deadline  flows  have 
backed off, then two possible scenarios are at play: (1) there are 
many  near-deadline  flows  not  reducing  their  share  and  (2)  there 
may  be  regular TCP  flows  consuming  bandwidth  in  a  deadline-
agnostic  manner.  In  both  scenarios,  the  d  >  1  condition  ensures 
that as ğ›¼ grows even near-deadline flows will throttle themselves, 
allowing  other  near-deadline  and  regular  TCP  flows  to  make 
progress.  In  fact,  if  the  near-deadline  flows  have  different 
deadlines and hence different d, then the flowsâ€™ back-off behavior 
will diverge as the congestion worsens. As a result, only the flows 
with the most imminent deadlines will win. 
In  summary, 
the  gamma-correction 
function  provides 
iterative feedback to near-deadline flows so that they do not drive 
the network to congestive collapse. 
3.2.3  Determining d based on deadlines 
We  now  explain  how  we  determine  d  based  on  a  given 
deadline value. The value of d should be such that the resulting 
congestion behavior allows the flow to safely complete within its 
time needed for a flow to complete transmitting all its data under 
deadline-agnostic behavior; and D is the time remaining until its 
deadline expires. Now if the flow can just meet its deadline under 
deadline.  We  use  deadline-agnostic  congestion  behavior  (ğ‘=ğ›¼ 
and ğ‘Š=ğ‘¤2 upon full congestion) as a starting point. Say ğ‘‡ğ‘ is the 
deadline-agnostic congestion behavior (i.e., ğ‘‡ğ‘â‰…ğ·) then d = 1 is 
appropriate.  It  also  follows  that  if ğ‘‡ğ‘>ğ·  then  we  should  set 
ğ‘‘>1 to indicate a tight deadline, and vice versa. Therefore, we 
function,  extreme  values  for  d  can  result  in ğ‘  behaving  like  a 
binary  value  for  the  useful  mid-range  of ğ›¼.  In  other  words,  the 
mere  presence  of  congestion  would  determine  window  sizing 
Because d appears in the exponent of the gamma-correction 
ğ’…=ğ‘»ğ’„ğ‘«  
compute d as:  
approximated as: 
behavior, instead of the extent of congestion. Essentially, we want 
the  gamma-correction  function  to  yield  gentle  curves  about  the 
within a desired range (Section 4.1.1). We explore the effects of 
varying this cap in Section 4.2.4. 
straight  line  of ğ‘‘=1.  Therefore,  we  propose  capping  d  to  be 
We  now  compute ğ‘‡ğ‘  using  Figure  4  which  shows  the 
(i.e., ğ›¼=1 and ğ‘=1); if there is less congestion then the flow 
will  complete  sooner,  making ğ‘‡ğ‘  an  upper  bound  for  time  to 
sawtooth  wave  for  deadline-agnostic  congestion  behavior.  We 
pessimistically assume that the flow encounters full-on congestion 
completion.  We  assume  that  a  flowâ€™s  current  window  size  is  W 
and  it  has  B  bytes  remaining  to  transmit.  Further,  we  make  the 
following  simplifying  assumptions  that  congestion  occurs  in  a 
repeating pattern: (1) Full-on congestion occurs when the window 