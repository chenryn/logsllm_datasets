tion. The gradient of f (x) is the unique vector ﬁeld whose
dot product with any unit vector v at each point x is the
directional derivative of f along v. We approximate each
directional derivative by ∂f (x)
where δ is
a small positive value (e.g., 1) and vi is the unit vector in
the ith dimension. To compute each directional derivative,
we need to run the program twice, once with the original
input x and once with the perturbed input x + δvi. It is
possible that in the second run, the program fails to reach
the program point where f (x + δvi) is calculated because
the program took a different branch at an earlier conditional
statement. When this happens, we set δ to a small negative
value (e.g., -1) and try to compute f (x + δvi) again. If
this succeeds, we compute the directional derivative based
on it. Otherwise, we set the derivative to zero, instructing
gradient descent not to move x in this direction. The time
for computing the gradient is proportional to the length
of the vector x since Angora computes each directional
derivative separately. Section 3.5 will describe how to reduce
the length of x by merging continuous bytes that are used
as a single value in the program.
In theory gradient descent can solve any constraint. In
practice, how fast gradient descent can solve a constraint
depends on the complexity of the mathematical function.
• If f (x) is monotonic or convex, then gradient descent
can ﬁnd a solution quickly even if f (x) has a complex
analytic form. For example, consider the constraint
f (x)  size then
type table[of f set] ← size
3:
4:
5:
6:
then
7:
8:
9:
10:
11:
12:
13:
14:
15: end procedure
end for
end if
end if
end if
3.6. Input length exploration
Angora,
like most other fuzzers, starts fuzzing with
inputs as small as possible. However, some branches are
executed only when the input is longer than a threshold.
This creates a dilemma for the fuzzer. If the fuzzer uses
too short inputs, it cannot explore those branches. But if it
uses too long inputs, the program may run slow or even out
of memory. Most tools try inputs of different lengths using
ad hoc approaches. By contrast, Angora increases the input
length only when doing so might explore new branches.
During taint tracking, Angora associates the destination
memory in the read-like function calls with the corre-
sponding byte offsets in the input. It also marks return
value from the read calls with a special label. If the return
value is used in a conditional statement and the constraint
is not satisﬁed, Angora increases the input length so that
the read call can get all the bytes that it requests. For
example, in Figure 2, if the conditional statement is false
on Line 12, Angora extends the input length so that fread
can read all the 1024 bytes that it requests. Our criteria are
not exhaustive because programs could consume the input
and check its length in ways that we have not anticipated,
but it would be easy to add those criteria to Angora once
we discover them.
4. Implementation
4.1. Instrumentation
For each program to be fuzzed, Angora produces cor-
responding executables by instrumenting the program with
LLVM Pass [18]. The instrumentation
718
• collects basic information of conditional statements,
and links a conditional statement to its corresponding
input byte offsets with taint analysis. On each input,
Angora runs this step only once (not while mutating
this input).
• records execution traces to identify new inputs.
• supports context at runtime (Section 3.2).
• gathers expression values in predicates (Section 3.4).
To support scalable byte-level taint tracking described
in Section 3.3, we implemented taint tracking for Angora
by extending DataFlowSanitizer (DFSan) [21]. We imple-
mented caching facility for operations FIND and UNION,
which speeds up taint tracking signiﬁcantly .
Angora depends on LLVM 4.0.0 (including DFSan). Its
LLVM pass has 820 lines of C++ code excluding DFSan,
and the runtime has 1950 lines of C++ code, including
the data structure for storing taint labels and the hooks for
tainting the input and tracking conditional statements.
In addition to the if statement, which has two branches,
LLVM IR also supports the switch statement, which may
introduce multiple branches. In our implementation, Angora
translates each switch statement to a sequence of if
statements for convenience.
Angora recognizes libc functions for comparing strings
and arrays when they appear in conditional statements.
For example, Angora transforms “strcmp(x, y)” into
“x strcmp y”, where strcmp is a special comparison
operator understood by Angora.
4.2. Fuzzer
We implemented Angora in 4488 lines of Rust code.
We optimized Angora with techniques such as fork
server [30] and CPU binding.
5. Evaluation
We evaluated Angora in three steps. First, we com-
pared the performance of Angora with other state-of-the-
art fuzzers. Then, we measured the test coverage of Angora
and its ability to ﬁnd unknown bugs in real world programs.
Finally, we evaluated its key novel features.
We ran all our experiments on a server with an Intel
Xeon E5-2630 v3 and 256 GB memory running 64-bit
Ubuntu 16.04 LTS. Even though Angora can fuzz a program
on multiple cores simultaneously, we conﬁgured it to fuzz
the program on only one core during evaluation to compare
its performance with other fuzzers. We ran each experiment
ﬁve times and report the average performance.
5.1. Compare Angora with other fuzzers
The ultimate metric for comparing fuzzers is their ability
to ﬁnd bugs. A good test set should contain real programs
with realistic bugs. LAVA is a technique for producing
ground-truth corpora by injecting a large number of realistic
bugs into program source code [9]. The authors created a
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:39:42 UTC from IEEE Xplore.  Restrictions apply. 
corpus LAVA-M by injecting multiple bugs into each pro-
gram. LAVA-M consists of four GNU coreutils programs:
uniq, base64, md5sum, and who. Each injected bug has an
unique ID, which is printed when the bug is triggered.
We compared Angora with the following state-of-the-art
fuzzers:
• FUZZER (a coverage-based fuzzer) and SES (symbolic
execution and SAT solving). The LAVA authors ran
both of them for ﬁve hours [9].
• VUzzer: a fuzzer using the “magic bytes” strategy [25].
Its authors reported the number of bugs found in the
programs in LAVA-M, but not the running time.
• Steelix: a fuzzer outperforming VUzzer on LAVA-
M [19]. The authors reported the number of bugs found
in the programs in LAVA-M by running the fuzzer for
ﬁve hours.
• AFL 2.51b: the latest version of AFL as of this writing.
We ran AFL for ﬁve hours, where we provided AFL
with one CPU core for fuzzing each program. 2
• Angora: We used the same set up (one CPU core per
program) as AFL.
Table 1 compares the bugs found by all the fuzzers.
AFL performed the worst, ﬁnding a total of 10 bugs in all
the programs. VUzzer’s authors could not run it on md5sum
because the LAVA authors incorrectly modiﬁed md5sum to
cause it to crash on all the inputs. We conﬁrmed this problem
with the LAVA authors and ﬁxed it. Steelix is the second
best fuzzer, ﬁnding almost all the bugs in base64, but only 7
out of 28 injected bugs in uniq, 28 out of 57 injected bugs in
md5sum, and 194 out of 2136 injected bugs in who. Angora
outperformed Steelix by a large margin, ﬁnding all the bugs
in uniq, base64, and md5sum, and 1443 out of 2136 injected
bugs in who.
LAVA assigns each injected bug a unique ID, which is
printed when the bug is triggered. The ﬁle validated bugs
lists all the injected bugs that the LAVA authors were able to
trigger when creating LAVA. Angora found not only all the
listed bugs in uniq, base64, md5sum and most listed bugs in
who, but also 103 unlisted bugs (bugs that the LAVA authors
injected but were unable to trigger). Table 3 shows the IDs
of these unlisted bugs. Table 4 shows the breakdown of the
listed and unlisted bugs found by Angora.
Figure 4 shows the cumulative number of bugs in who
found by Angora over time. We did not show the re-
sults by the other fuzzers because they found few bugs in
who. Figure 4 shows that initially Angora discovered bugs
quickly, ﬁnding 1000 bugs in less than ﬁve minutes. Then
the discovery rate slowed, but it still found more than 1500
bugs in merely 45 minutes, out of the total 2136 listed bugs.
We explain why Angora found a magnitude more bugs
than the next best fuzzer as follows. First, LAVA uses
“magic bytes” to guard branches that contain bugs, but
some magic bytes are not copied from the input directly
but rather are computed from the input. Since VUzzer and
2. An author of LAVA mentioned some compilation issues of running
AFL on LAVA in his blog post [11], and we ﬁxed these issues in our
evaluation.
719
TABLE 3: IDs of bugs injected but unlisted by LAVA,
because the LAVA authors were unable to trigger them when
preparing the data set. Angora found these bugs.
Program IDs of bugs unlisted by LAVA-M but found by Angora
uniq
base64
md5sum
who
227
274, 521, 526, 527
-
2, 4, 6, 8, 12, 16, 24, 55, 57, 59, 61, 63, 73, 77, 81, 85,
89, 125, 165, 169, 173, 177, 181, 185, 189, 193, 197,
210, 214, 218, 222, 226, 294, 298, 303, 307, 312, 316,
321, 325, 327, 334, 336, 338, 350, 359, 468, 472, 477,
481, 488, 514, 526, 535, 974, 975, 995, 1007, 1026,
1034, 1071, 1072, 1415, 1429, 1436, 1456, 1718, 1735,
1736, 1737, 1738, 1747, 1748, 1755, 1756, 1891, 1892,
1893, 1894, 1903, 1904, 1911, 1912, 1921, 1925, 1935,
1936, 1943, 1944, 1949, 1953, 2231, 3264, 3545, 3551,
3939, 4287, 4295
TABLE 4: Bugs found by Angora and the corresponding
running time on the LAVA-M data set. Listed bugs are in
LAVA’s validated bugs ﬁle. Unlisted bugs were not trig-
gered when LAVA’s authors prepared the data set.
Program Listed
bugs
28
44
57
2136
uniq
base64
md5sum
who
Found bugs
Listed
28
44
57
1443
Unlisted
1
4
0
98
Time (min)
10
10
10
45
Steelix’s “magic bytes” strategy can only copy magic bytes
to the input directly, that strategy cannot create inputs that
explore those branches. By contrast, Angora tracks the input
byte offsets that ﬂow into a predicate, and then mutates
these offsets by gradient descent instead of assuming “magic
bytes” or any other special relation between the input and
the predicate, so Angora can ﬁnd inputs that explore those
branches. Second, VUzzer tries the “magic bytes” strategy
blindly, and Steelix focuses on the “magic bytes” strategy
once one of the magic bytes matches a byte in the input
after a random mutation. By contrast, Angora schedules all
its computing power to solve path constraints on unexplored
branches, so it can cover more branches and therefore ﬁnd
most of the injected bugs in LAVA-M quickly.
5.2. Evaluate Angora on unmodiﬁed real world
programs
Angora has impressive performance on LAVA, ﬁnding
not only most of the listed bugs but also many unlisted bugs.
However, its skeptic might contend that these bugs were
artiﬁcially injected. To address this concern, we evaluated
Angora on eight popular open source programs using their
latest versions. Since these mature, popular programs had
been extensively tested, we expected them to have few
residue crashing bugs. Therefore, besides measuring the
number of new bugs found, we also measured Angora’s
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:39:42 UTC from IEEE Xplore.  Restrictions apply. 
 1600
 1400
 1200
 1000
 800
 600
 400
 200
s
e
h
s
a
r
c
e
u
q
n
u
i
f
o
r
e
b
m
u
N
 0
00:00
00:05
00:10
00:15
00:20
00:25
00:30
00:35
00:40
00:45
Time (in HH:MM format)
Figure 4: Cumulative number of bugs in who found by
Angora over time
coverage on these programs. We used gcov, which records
all the lines and branches executed in a program on an
input [14]. We fed each input generated by Angora to the
program compiled with gcov to obtain the cumulative code
coverage, and aﬂ-cov3allowed us to do this automatically.
We also ran AFL on these programs for comparison. Table 5
shows the results after running Angora and AFL with one
CPU core for ﬁve hours, respectively. We deduplicated the
crashes by AFL’s afl-cmin -C command.