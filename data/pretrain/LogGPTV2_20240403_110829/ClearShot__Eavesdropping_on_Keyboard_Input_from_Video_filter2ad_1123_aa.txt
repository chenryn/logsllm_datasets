title:ClearShot: Eavesdropping on Keyboard Input from Video
author:Davide Balzarotti and
Marco Cova and
Giovanni Vigna
2008 IEEE Symposium on Security and Privacy
ClearShot: Eavesdropping on Keyboard Input from Video
Davide Balzarotti, Marco Cova, and Giovanni Vigna
Department of Computer Science,
University of California Santa Barbara
Santa Barbara, CA, USA
{balzarot,marco,vigna}@cs.ucsb.edu
Abstract
Eavesdropping on electronic communication is usually
prevented by using cryptography-based mechanisms. How-
ever, these mechanisms do not prevent one from obtaining
private information through side channels, such as the elec-
tromagnetic emissions of monitors or the sound produced by
keyboards. While extracting the same information by watch-
ing somebody typing on a keyboard might seem to be an easy
task, it becomes extremely challenging if it has to be auto-
mated. However, an automated tool is needed in the case of
long-lasting surveillance procedures or long user activity,
as a human being is able to reconstruct only a few charac-
ters per minute. This paper presents a novel approach to au-
tomatically recovering the text being typed on a keyboard,
based solely on a video of the user typing. As part of the
approach, we developed a number of novel techniques for
motion tracking, sentence reconstruction, and error correc-
tion. The approach has been implemented in a tool, called
ClearShot, which has been tested in a number of realistic
settings where it was able to reconstruct a substantial part
of the typed information.
1 Introduction
MARTIN BISHOP: What’s he doing?
CARL ARBEGAST: He’s logging on the computer.
MARTIN BISHOP (watching through a video camera):
Oh, this is good.
He’s going to type in his password
and we’re going to get a clear shot.
From the movie “Sneakers,” 1992, Universal Pictures.
Spying on people has always been an effective way of ob-
taining information since the beginning of history. With the
advent of technology, new ways of spying on somebody’s
communications have been devised, and, consequently, new
counter-measures have been developed as well.
One of the most effective ways to protect electronic com-
munication is the use of cryptography-based mechanisms.
However, encrypting the communication does not help in
protecting one’s physical environment, that is, the room the
person is sitting in and sometimes the devices attached to
one’s computer. For this reason, in the past there has been
a corpus of research devoted to exploring how to pry into
someone’s communication by exploiting the side effects of
communication and the devices attached to someone’s com-
puter. For example, TEMPEST (or Emission Security) is
the term used to refer to the analysis of different types of
emissions (electromagnetic and acoustic) that can be used
to reconstruct the contents of communications [33, 44].
Even though TEMPEST research has been traditionally
focused on analyzing the emissions of cables and monitors,
recently research has been focused on a number of different
“side sources” of information, such as the light reﬂected by
the walls of a room [22], the timing of pressing keys [39],
and even the sound emitted by the keyboard [2].
So far, there has not been any extensive study on using
the output of video cameras that record someone typing on
a keyboard to automatically derive the information being
typed.
In the ’92 movie “Sneakers,” a group of hackers-
for-hire uses a telescopic video camera to record an unsus-
pecting victim while he is logging into his computer [37].
In a following scene, the hackers review the video and enter
a lengthy debate about which keys were actually pressed.
This scene made us wonder about the feasibility of a tool
that would perform this tedious, error-prone task automati-
cally.
An additional motivating factor in pursuing this research
was the ubiquitous availability of web cams. In the early
’90s, web cams were not as popular as they are now, and
therefore the hackers in “Sneakers” had to resort to a pow-
erful telescopic camera in order to observe the typing activ-
ity. However, nowadays it would be possible to obtain good
quality video simply by exploiting the web cam attached to
the victim’s computer [13, 36]. Therefore, we developed an
analysis tool that operates on the stream of images produced
by an off-the-shelf web cam that records the typing activity
978-0-7695-3168-7 /08 $25.00 © 2008 IEEE
DOI 10.1109/SP.2008.28
170
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 05:58:33 UTC from IEEE Xplore.  Restrictions apply. 
of a user.
2 Approach
Note that, differently from the situation represented in
“Sneakers,” we are not interested in the recovery of a single
password (which could probably be easily recovered by a
human being). Instead, our goal is to recover all the infor-
mation entered by the user through the keyboard (e.g., email
messages, instant messages, documents, and code). This
type of activity would be error-prone and resource-intensive
if performed by a human being analyzing manually each of
the frames produced by the camera. Our experiments show
that, for a human, reconstructing a few sentences requires
lengthy hours of slow-motion analysis of the video.
Automatically recognizing the keys being pressed by a
user is a hard problem that requires sophisticated motion
analysis. Previous work in the computer vision ﬁeld has pro-
duced algorithms that can perform well only when the user
types on the keyboard using non-realistic movements (e.g.,
one ﬁnger at a time [9, 47]). In addition, several “enhanced”
typing interfaces (e.g., projected keyboards) use an array of
non-visual sensors to identify the keys being pressed. Our
approach is different because it aims at reconstructing the
typed information in a realistic setting and using exclusively
the video information captured by an off-the-shelf web cam.
One can see the analysis described hereinafter as a so-
phisticated form of shoulder-surﬁng (maybe to bypass mon-
itor privacy ﬁlters that would block direct analysis of the
monitor display [1]), enabled by the availability of web
cams. Note however, that the same analysis could be ap-
plied to the video output produced by a surveillance camera
or a remote camera that records the victim through a win-
dow (as shown in “Sneakers”).
The contributions of this paper are the following:
• We developed a novel eavesdropping technique that is
able to reliably reconstruct the keyboard’s keys pressed
by a user by analyzing solely the video stream of the
typing activity.
• We developed a number of novel techniques to cor-
rect errors in the reconstruction process, drastically im-
proving the reconstruction rate.
• We developed a tool, called ClearShot, that operates on
low resolution video and is able to reconstruct a sub-
stantial part of the typed information.
• To the best of our knowledge, we are the ﬁrst to study
the problem of recognizing keyboard activity from
video in a security setting. We experimented with a
number of techniques, showing the advantages and dis-
advantages of each approach.
The rest of the paper is structured as follows. Section 2
describes in details our assumptions, the threat model, and
our approach to solving the problem of key pressing recog-
nition. Then, Section 3 presents our experimental evaluation
of the tool. Section 4 presents related work. Finally, Sec-
tion 5 brieﬂy concludes.
The goal of our work is to automatically reconstruct the
text typed by a user starting from the video recording of the
typing session. In the following, we refer to the person be-
ing recorded as “the user” or “the victim,” interchangeably.
In addition, we refer to the person recording the victim and
analyzing the video as “the attacker.”
We assume that the attacker is able to point a video cam-
era at the keyboard of the victim. For example, the attacker
might install a surveillance device in the room of the vic-
tim, might take control of an existing camera by exploiting
a vulnerability in the camera’s control software, or might
simply point a mobile phone with an integrated camera at
the laptop’s keyboard when the victim is working in a pub-
lic space, e.g., a caf´e or an airport terminal. The rationale for
monitoring the keyboard instead of pointing the spying de-
vice directly at the victim’s screen is that there might not be
a clear line of sight (e.g., the camera can only be planted on
the ceiling above a worker’s desk in a cubicle, or the screen
may be covered with a privacy ﬁlter).
These scenarios considerably limit the assumptions we
can make with respect to the environment in which the at-
tack is performed. In particular, we can assume that only
the video camera and its position are under our control. This
means that the physical characteristics of the victim’s hands,
his/her typing speed and style, the lighting conditions, and
the background features, such as color and texture, cannot
be directly changed by us. In our experiments, we tested our
tool with multiple users in a typical ofﬁce environment.
Our analysis is comprised of two main phases (see Fig-
ure 1). The ﬁrst phase analyzes the video recorded by the
camera using computer vision techniques. For each frame
of the video, the computer vision analysis computes the set
of keys that were likely pressed, the set of keys that were
certainly not pressed, and the position of space characters.
Because the results of this phase of the analysis are noisy, a
second phase, called the text analysis, is required. The goal
of this phase is to remove errors using both language and
context-sensitive techniques. The result of this phase is the
reconstructed text, where each word is represented by a list
of possible candidates, ranked by likelihood. In the follow-
ing sections 2.1 and 2.2, we describe the details of the two
phases of the analysis.
2.1 Computer Vision Analysis
Problem deﬁnition. The problem of the computer vision
analysis is determining the sequence of pressed keys, given
the video recording of the typing session.
At ﬁrst sight, this problem appears similar to those tack-
led by gesture recognition research. The goal of gesture
recognition is to identify speciﬁc human gestures and use
them to convey information or to control devices [26]. Ges-
ture recognition has been the focus of considerable research
171
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 05:58:33 UTC from IEEE Xplore.  Restrictions apply. 
Figure 1. Overview of the analysis steps performed by ClearShot.
in the past years, and it has been successfully applied to
automating a number of tasks, such as handwriting recog-
nition [43], sign language interpretation [40], device con-
trol [14], and video surveillance [19]. However, the tech-
niques employed in gesture recognition are not immediately
applicable to our case, for a number of reasons.
First of all, for practical reasons, gestures are generally
limited to a relatively small catalog of recognized move-
ments. For example, the catalog may consist of static move-
ments, such as making a ﬁst and holding it in a certain posi-
tion, or dynamic movements, such as pinching or waving. In
most cases, the allowed movements are fairly constrained.
Second, some gesture recognition systems assume a spe-
ciﬁc conﬁguration of the physical environment, in terms,
for example, of the intensity of lights, background texture,
and color contrast. We assume that all these parameters
are not under our control. Third, some systems take ad-
vantage of special input devices, such as tablets [43], data
gloves [12, 27, 42], and body suits [30, 45]. Clearly, our
approach cannot rely on the use of these devices. Finally,
some of the existing techniques require an initial training
phase before they can be used effectively. While it is pos-
sible to devise scenarios where a learning-based approach
could be used to solve our problem (for example, a victim
could be lured into typing a known text, which becomes part
of a training dataset), we prefer to adopt a completely unsu-
pervised solution.
We divide the computer vision analysis into two sub
tasks: hands tracking analysis and key pressing analysis.
The former returns information about the position of the
user’s hands, while the latter focuses on the keyboard and
determines whether or not a key was pressed at a certain
point in time. To implement these tasks, we used the Open
Computer Vision library [18].
2.1.1 Hands Tracking Analysis
We experimented with several techniques to identify and
track the movement of the user’s hands on the keyboard.
Hereinafter, we ﬁrst describe a technique based on the ex-
traction of speciﬁc features from the video frames consid-
ered in isolation, and then we present two techniques based
on the analysis of the dynamic properties of the video.
Skin Model Analysis.
It has been noted that it is often
possible to precisely identify the hands in a video stream
by leveraging their chromatic characteristics. For example,
Starner and Petland observed that human hands have ap-
proximately the same hue and saturation, and vary primar-
ily in their brightness [40]. Similarly, in our experiments we
built an a priori model of skin color and we used this model
to differentiate the hands from the background. Once the
hands have been identiﬁed, it is easy to track their move-
ment.
This method has the drawback of identifying the shape
of the whole hand, while in our analysis we are mostly in-
terested in the ﬁngertips’ positions. For this reason, we de-
cided not to use this analysis technique and we focused on
other approaches that characterize hand movement more ef-
fectively.
Optical Flow Analysis. Optical ﬂow computation is a
technique used to estimate the spatial position and move-
ment of objects from patterns of image intensity [5]. This
technique analyzes a series of images that have a small time
step between them and calculates a vector ﬁeld across each
image plane. The vector ﬁeld describes the distribution of
apparent velocities of sets of points (e.g., brightness pat-
terns) in the image.
We hypothesized that hand movement associated with
typing would manifest as characteristic patterns in the op-
tical ﬂow. For example, to press a key, one ﬁnger starts
moving, acquires speed, slows down and eventually stops
when it hits the key, then it retracts from the key, inverting
its velocity. Therefore, we computed the optical ﬂow (using
the Horn-Schunck algorithm [17]) and looked for matches
with a number of typing patterns in the ﬂow.
Unfortunately, our hypothesis was not conﬁrmed. We
identiﬁed several reasons for this. First, optical ﬂow algo-
rithms are sensitive to noise and changes in the brightness of
input images, especially when applied to real image data (as
172
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 05:58:33 UTC from IEEE Xplore.  Restrictions apply. 
opposed to synthetic inputs) [5]. This introduces errors in
the estimation of movement intensity and direction. Sec-
ond, typing patterns are more complex than the ones we
originally devised: when pressing a key, the entire hand is
in motion and, often, several ﬁngers are moving preparing
to press subsequent keys. This makes it difﬁcult to isolate
typing patterns in a robust way.
Contour Analysis.
Image segmentation is the process of
partitioning an image into regions [35]. We use simple im-
age segmentation techniques to detect and track the hands.
Typically, during a typing session, the only objects mov-