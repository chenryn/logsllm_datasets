### 优化后的文本

#### 时间差分学习公式
\[ V(S_t) \leftarrow V(S_t) + \alpha [R_{t+1} + \gamma V(S_{t+1}) - V(S_t)] \]

#### 动态规划、蒙特卡罗和时间差分方法的比较
动态规划（DP）、蒙特卡罗（MC）和时间差分（TD）方法是现代强化学习中的核心算法，它们经常被结合使用以进行策略评估和提升。尽管这些方法的目标相同，但其执行过程及效果存在显著差异，这使得它们在实际应用中表现出不同的性能。

- **泛化策略迭代（GPI）**：三种方法均涉及此过程。
- **自举法的应用**：DP和TD采用自举法估计状态值，而MC则不使用这种方法。
- **环境模型需求**：DP需要完整的环境信息；相比之下，MC与TD不需要这种详细的信息即可工作。

#### 学习目标的不同
- **蒙特卡罗方法**的状态价值估计定义为：
  \[ v_\pi(s) = E_\pi[G_t | S_t = s] \]
- **动态规划**的方法则是：
  \[ v_\pi(s) = E_\pi[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s] \]
  
这里 \(v_\pi(s)\) 代表了在策略 \(\pi\) 下从状态 \(s\) 开始所能获得的预期回报。虽然两种方法都旨在估计真实的状态值，但实际上只能得到近似值。TD方法通过结合MC的采样优势与DP的自举特性，在实践中往往比单独使用两者之一更有效率。

#### 偏差与方差权衡
- **偏差**衡量的是估计值与真值之间的差距。
- **方差**反映了估计结果的一致性或稳定性。
  
MC直接估计整个回合的累计奖励，理论上无偏但方差较大；相反地，TD基于即时反馈调整预测，引入了一定程度的偏差却降低了方差。

#### TD(λ)算法简介
为了平衡效率与准确性，TD(λ)算法被提出作为DP与MC之间的一种折衷方案。它利用资格迹（Eligibility Traces）来跟踪哪些状态或动作对当前的学习贡献最大，并据此更新相应的权重。当参数 \(\lambda=0\) 时，该算法退化为单步TD；若 \(\lambda=1\)，则等同于MC方法。

#### Sarsa算法概述
Sarsa是一种在线策略控制方法，用于估计给定策略下的最优行动价值函数。它的更新规则如下所示：
\[ Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)] \]

此外还介绍了n步Sarsa的概念及其与标准版本的主要区别——即前者能够在做出决策之前考虑更多未来的步骤信息。

#### 收敛条件
最后讨论了GLIE（Greedy in the Limit with Infinite Exploration）准则，这是确保某些类型RL算法收敛至最优解的一个重要前提条件。例如，通过适当调整探索率\(\epsilon\)随时间逐渐减少，可以满足这一要求。