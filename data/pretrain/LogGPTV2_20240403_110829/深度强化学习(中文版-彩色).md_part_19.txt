V(S )←V(S )+α[R +γV(S )−V(S )]
t t t+1 t+1 t
endfor
endfor
这里分析一下动态规划、蒙特卡罗和时间差分方法的异同点。它们都是在现代强化学习中的
核心算法，而且经常是被结合起来使用的。它们都可以被用于策略评估和策略提升，它们之间区
别却是深度强化学习效果不同的主要来源之一。
这三种方法都涉及泛化策略迭代（GPI），它们主要区别在于策略评估的过程，其中最明显的
区别是，动态规划和时间差分都使用了自举法，而蒙特卡罗没有。动态规划需要整个环境模型的
所有信息，但是蒙特卡罗和时间差分不需要。进一步地，我们来看一下它们的学习目标的区别。
v (s)=E [G |S =s] (2.58)
π π t t
=E [R +γG |S =s] (2.59)
π t+1 t+1 t
=E [R +γv (S )|S =s] (2.60)
π t+1 π t+1 t
公式(2.58)是蒙特卡罗方法的状态价值估计方式，公式(2.60)是动态规划的。它们都不是真
正的状态值而是估计值。时间差分则把蒙特卡罗的采样过程和动态规划的自举法结合了起来。现
在我们就简单解释实践中时间差分可以比动态规划或者蒙特卡罗更有效的原因。
首先，时间差分不需要一个模型而动态规划需要。将时间差分与蒙特卡罗做比较，时间差分
使用的是在线学习，这也就意味着它每一步都可以学习，但是蒙特卡罗却只能在一个回合结束以
后再学习，这样回合很长时会比较难以处理。当然，也存在一些连续性的问题无法用片段式的形
式来表示一个回合。另外，时间差分在实践中往往收敛得更快，因为它的学习是来自状态转移的
信息而不需要具体动作信息，而蒙特卡罗往往需要动作信息。在理想情况下，两种方法最终都会
渐进收敛到v (s)。
π
这里我们介绍时间差分和蒙特卡罗方法中的偏差和方差的权衡（BiasandVarianceTrade-off）。
我们知道在监督学习的设置下，较大的偏差往往意味着这个模型欠拟合（Underfitting），而较大的
方差伴随较低的偏差往往意味着一个模型过拟合（Overfitting）。一个拟合器（Estimator）的偏差
74
2.6 时间差分学习
是估计值和真正值间的差异。我们对状态价值进行估计时，偏差可以被定义为E[V(S )]−V(S )。
t t
拟合器的方差描述了这个拟合有多大的噪声。同样对于状态价值估计，方差定义为E[(E[V(S )]−
t
V(S ))2]。在预测时，不管它是状态价值估计，还是状态-动作价值估计，时间差分和蒙特卡罗的
t
更新都有如下形式：
V(S )←V(S )+α[TargetValue−V(S )]
t t t
实质上，我们对不同回合进行了加权平均计算。时间差分法和蒙特卡罗法在处理目标值时分
别采用不同的方式。蒙特卡罗法直接估算到一个回合结束累计的回报。这也正是状态值的定义，
它是没有偏差的。而时间差分法会有一定的偏差，因为它的目标值是由自举法估计得到的，如
R +γv (S )。另一方面，蒙特卡罗法，所以在不同回合中积累到最后的回报会有较大的方
t+1 π t+1
差由于不同回合的经过和结果都不同。时间差分法通过关注局部估计的目标值来解决这个问题，
只依赖当前的奖励和下一个状态或动作价值的估计。自然地，时间差分法方差更小。
我们可以在动态规划和蒙特卡罗之间找到一个中间方法来更有效地解决问题，即TD(λ)。在
此之前，我们需要先介绍资格迹（EligibilityTrace）和λ-回报（λ-Return）概念。
简单来说，资格迹可以给我们带来一些计算优势。为了更好地了解其优势，我们需要介绍半
梯度（Semi-Gradient）方法，然后再来看如何使用资格迹。关于策略梯度方法在2.7节中有介绍，
而这里我们简单地使用一些策略梯度方法中的概念来方便解释资格迹。假如说我们的状态价值函
数不是表格（Tabular）形式而是一种函数形式，这个函数由矢量w ∈Rn参数化。比如w可以是
一个神经网络的权重。为了得到 V(s,w) ≈ v (s)，我们使用随机梯度更新来减小估计值和真正
π
的状态价值的平方损失（QuadraticLoss）。权重向量的更新规则就可以写为
1
w =w − α∇ [v (S )−V(S ,w )]2
t+1 t 2 wt π t t t
=w +α[v (S )−V(S ,w )]∇ V(S ,w ) (2.61)
t π t t t wt t t
其中α为一个正值的步长变量。
资格迹是一个向量：z ∈Rn，在学习的过程中，每当w 的一个部分被用于估计，则它在z
t t t
里的那个相对应的部分需要随之增加，而在增加以后它又会慢慢递减。如果轨迹上的资格值回落
到零之前，有一定的TD误差，就进行学习。首先把所有资格值都初始化为0，然后使用价值函
数的梯度来增加资格迹，而资格值递减的速度是γλ。资格迹的更新满足如下公式：
z−1 =0 (2.62)
z =γλz t−1+∇ wtV(S t,w t) (2.63)
t
如算法2.11所示，TD(λ)使用资格迹来更新其价值函数估计。易见，当λ=1时，TD(λ)变为蒙
75
第2章 强化学习入门
特卡罗法；而当λ=0时，它就变成了一个单步TD（One-StepTD）法。因此，资格迹可以看作
是把时间差分法和蒙特卡罗法相结合的一个方法。
算法2.11状态值半梯度TD(λ)
输入策略π
初始化一个可求导的状态值函数v、步长α和状态值函数权重w
for对每一个回合do
初始化S
0
z ←0
for每一个本回合的步骤S do
t
使用π来选择A
t
R ,S ←Env(S ,A )
t+1 t+1 t t
z ←γλz+∇V(S ,w )
t t
δ ←R +γV(S ,w )−V(S ,w )
t+1 t+1 t t t
w ←w+αδz
endfor
endfor
λ-回报是之后n步中的估计回报值。λ-回报是n个已经折扣化的回报和一个在最后一步状态
下的估计值相加得到的。我们可以把它写作：
G =R t+1+γR t+2+···+γn−1R t+n+γnv(S t+n,w t+n−1) (2.64)
t:t+n
t是一个不为零的标量，它小于或等于T −n。我们可以使用加权平均回报来估算，只要它们
的权重满足和为1。TD(λ)在其更新中使用了加权平均（λ∈[0,1]）：
X∞
Gλ =(1−λ) λn−1G (2.65)
t t:t+n
n=1
直观地讲，这就意味着下一步的回报将有最大的权重1−λ，下两步回报的权重是(1−λ)λ。
每一步权重递减的速率是λ。为了有更清晰的理解，我们让结束状态发生于时间T，从而上面的
公式可以改写成
TX−t−1
Gλ =(1−λ) λn−1G +λT−t−1G (2.66)
t t:t+n t
n=1
TD误差δ 可以被定义为
t
δ =R +γV(S ,w )−V(S ,w ) (2.67)
t t+1 t+1 t t t
76
2.6 时间差分学习
这个更新规则是基于TD误差和迹的比重的。算法2.11里有其细节。
2.6.2 Sarsa：在线策略TD控制
对于TD控制，我们使用的方法和预测任务一样，唯一的不同是，我们需要将从状态到状态
的转移变为状态-动作对的交替。这样的更新规则就可以被写为
Q(S ,A )←Q(S ,A )+α[R +γQ(S ,A )−Q(S ,A )] (2.68)
t t t t t+1 t+1 t+1 t t
当S 是终止状态（TerminalState）的时候，下一个状态-动作对的Q值就会变成0。我们用
t
首字母缩写Saras来表示这个算法，因为我们有这样的一个行为过程：首先在一个状态（S）下，
选择了一个动作（A），同时也观察到了回报（R），然后我们就到了另外一个状态（S）下，需要
选择一个新的动作（A）。这样的过程让我们可以做一个简单的更新步骤。对于每一个转移，状态
价值都得到更新，更新后的状态价值会影响决定动作的策略，即在线策略法。在线策略法一般用
来描述这样一类算法，它们的更新策略和行动策略（BehaviorPolicy）同样。而离线策略法往往
是不同的。Q-Learning就是离线策略算法的一个例子。我们会在之后的章节中提到。Q-Learning
在更新Q函数时假设了一种完全贪心的方法，而它在选择其动作时实际上用的是另外一种类似
于ϵ-贪心（ϵ-Greedy）的方法。现在我们在算法2.12中列出Sarsa的细节。在每一个状态-动作对
都会被访问无数次的假设下，会有最优策略和状态动作价值的收敛性保证。
算法2.12Sarsa（在线策略TD控制）
对所有的状态-动作对初始化Q(s,a)
for每一个回合do
初始化S
0
用一个基于Q的策略来选择A
0
for每一个在当前回合的S do
t
用一个基于Q的策略从S 选择A
t t
R ,S ←Env(S ,A )
t+1 t+1 t t
从S 中用一个基于Q的策略来选择A
t+1 t+1
Q(S ,A )←Q(S ,A )+α[R +γQ(S ,A )−Q(S ,A )]
t t t t t+1 t+1 t+1 t t
endfor
endfor
上面展示的方法只有一步的时间范围，这就意味着它的估算只需要考虑下一步的状态-动作
价值。我们把它叫作单步 Sarsa 或者 Sarsa(0)。我们可以简单地使用自举法把未来的步骤也都容
纳到目标值中从而减少它的偏差。从图2.12的回溯树展示中，我们可以看见Sarsa很多不同的变
体。从最简单的一步Sarsa到无限步Sarsa，也就是蒙特卡罗方法的另外一个形态。为了把这样的
77
第2章 强化学习入门
一个变化融入原来的方法，我们需要把折扣回报写为
G =R t+1+γR t+2+···+γn−1R t+n+γnQ t+n−1(S t+n,A t+n) (2.69)
t:t+n
图2.12 对于n步Sarsa方法的回溯树。每一个黑色的圆圈都代表了一个状态，每一个白色的圆
圈都代表了一个动作。在这个无穷多步的Sarsa里，最后一个状态就是它的终止状态
n步Sarsa已经在算法2.13中有所描述了。和单步版本最大的不同是，它需要回到过去的时
间来做更新，而单步的版本只需要一边向前进行一边更新即可。
现在讨论Sarsa算法在有限的动作空间里的收敛理论。我们首先需要以下的几个条件。
定义 2.1 一个学习策略被定义为：在无限的探索中的极限贪婪（Greedy in the Limit with Infinite
Exploration,GLIE）。如果它能够满足以下两个性质：1. 如果一个状态被无限次访问，那么在该状态
下的每个可能的动作都应当被无限次选择，即lim k→∞N k(s,a)=∞,∀a,iflim k→∞N k(s)=∞。
2. 策略根据学习到的Q函数在t→∞的极限下收敛到一个贪婪策略，即lim k→∞π k(s,a)=
1(a==argmax a′∈AQ k(s,a′))，其中“==”是一个比较算子，当1(a==b)的括号内为真时，它
的值为1，否则为0。
GLIE是学习策略收敛的一个条件，对于任何收敛到最优价值函数且估计值都有界（Bounded）
的强化学习算法来说，它都成立。举例来说，我们可以通过ϵ贪心方法来推导出一个GLIE的策
略，如下：
引理2.1 如果ϵ以ϵ = 1 的形式随k增大而渐趋于零，那么ϵ-贪心是GLIE。
k k
78
2.6 时间差分学习
算法2.13n步Sarsa
对所有的状态-动作对初始化Q(s,a)