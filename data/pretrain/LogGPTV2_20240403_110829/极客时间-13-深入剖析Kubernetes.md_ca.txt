# ip addr  ...  73：kube-ipvs0：  mtu 1500 qdisc noop state DOWN qlen 1000  link/ether  1a:ce:f5:5f:c1:4d brd ff:ff:ff:ff:ff:ff  inet 10.0.1.175/32  scope global kube-ipvs0  valid_lft forever  preferred_lft forever而接下来，kube-proxy 就会通过 Linux 的 IPVS 模块，为这个 IP 地址设置三个IPVS 虚拟主机，并设置这三个虚拟主机之间使用轮询模式 (rr)来作为负载均衡策略。我们可以通过 ipvsadm 查看到这个设置，如下所示：    
# ipvsadm -ln IP Virtual Server version 1.2.1 (size=4096)  Prot LocalAddress:Port Scheduler Flags    ->  RemoteAddress:Port           Forward  Weight ActiveConn InActConn       TCP  10.102.128.4:80 rr    ->  10.244.3.6:9376    Masq    1       0          0             ->  10.244.1.7:9376    Masq    1       0          0    ->  10.244.2.3:9376    Masq    1       0          0可以看到，这三个 IPVS 虚拟主机的 IP 地址和端口，对应的正是三个被代理的Pod。这时候，任何发往 10.102.128.4:80 的请求，就都会被 IPVS模块转发到某一个后端 Pod 上了。而相比于 iptables，IPVS 在内核中的实现其实也是基于 Netfilter 的 NAT模式，所以在转发这一层上，理论上 IPVS 并没有显著的性能提升。但是，IPVS并不需要在宿主机上为每个 Pod 设置 iptables规则，而是把对这些"规则"的处理放到了内核态，从而极大地降低了维护这些规则的代价。这也正印证了我在前面提到过的，"将重要操作放入内核态"是提高性能的重要手段。> 备注：这里你可以再回顾下第 33> 篇文章[《深入解析容器跨主机网络》](https://time.geekbang.org/column/article/65287)中的相关内容。不过需要注意的是，IPVS 模块只负责上述的负载均衡和代理功能。而一个完整的Service 流程正常工作所需要的包过滤、SNAT 等操作，还是要靠 iptables来实现。只不过，这些辅助性的 iptables 规则数量有限，也不会随着 Pod数量的增加而增加。所以，在大规模集群里，我非常建议你为 kube-proxy 设置--proxy-mode=ipvs来开启这个功能。它为 Kubernetes 集群规模带来的提升，还是非常巨大的。**此外，我在前面的文章中还介绍过 Service 与 DNS 的关系。**在 Kubernetes 中，Service 和 Pod 都会被分配对应的 DNS A 记录（从域名解析IP 的记录）。对于 ClusterIP 模式的 Service 来说（比如我们上面的例子），它的 A记录的格式是：``{=html}.``{=html}.svc.cluster.local。当你访问这条A 记录的时候，它解析到的就是该 Service 的 VIP地址。``{=html}``{=html}而对于指定了 clusterIP=None 的 Headless Service 来说，它的 A记录的格式也是：``{=html}.``{=html}.svc.cluster.local。但是，当你访问这条A 记录的时候，它返回的是所有被代理的 Pod 的 IP地址的集合。当然，如果你的客户端没办法解析这个集合的话，它可能会只会拿到第一个Pod 的 IP 地址。``{=html}``{=html}此外，对于 ClusterIP 模式的 Service 来说，它代理的 Pod 被自动分配的 A记录的格式是：``{=html}.``{=html}.pod.cluster.local。这条记录指向Pod 的 IP 地址。``{=html}``{=html}而对 Headless Service 来说，它代理的 Pod 被自动分配的 A记录的格式是：``{=html}.``{=html}.``{=html}.svc.cluster.local。这条记录也指向Pod 的 IP地址。``{=html}``{=html}``{=html}但如果你为 Pod 指定了 Headless Service，并且 Pod 本身声明了 hostname 和subdomain 字段，那么这时候 Pod 的 A 记录就会变成：\.``{=html}.``{=html}.svc.cluster.local，比如：``{=html}``{=html}    apiVersion: v1kind: Servicemetadata:  name: default-subdomainspec:  selector:    name: busybox  clusterIP: None  ports:  - name: foo    port: 1234    targetPort: 1234---apiVersion: v1kind: Podmetadata:  name: busybox1  labels:    name: busyboxspec:  hostname: busybox-1  subdomain: default-subdomain  containers:  - image: busybox    command:      - sleep      - "3600"    name: busybox在上面这个 Service 和 Pod 被创建之后，你就可以通过busybox-1.default-subdomain.default.svc.cluster.local 解析到这个 Pod 的IP 地址了。需要注意的是，在 Kubernetes 里，/etc/hosts文件是单独挂载的，这也是为什么 kubelet 能够对 hostname 进行修改并且 Pod重建后依然有效的原因。这跟 Docker 的 Init 层是一个原理。
## 总结在这篇文章里，我为你详细讲解了 Service 的工作原理。实际上，Service机制，以及 Kubernetes 里的 DNS插件，都是在帮助你解决同样一个问题，即：如何找到我的某一个容器？这个问题在平台级项目中，往往就被称作服务发现，即：当我的一个服务（Pod）的IP地址是不固定的且没办法提前获知时，我该如何通过一个固定的方式访问到这个Pod 呢？而我在这里讲解的、ClusterIP 模式的 Service 为你提供的，就是一个 Pod的稳定的 IP 地址，即 VIP。并且，这里 Pod 和 Service 的关系是可以通过Label 确定的。而 Headless Service 为你提供的，则是一个 Pod 的稳定的 DNS名字，并且，这个名字是可以通过 Pod 名字和 Service 名字拼接出来的。在实际的场景里，你应该根据自己的具体需求进行合理选择。
## 思考题请问，Kubernetes 的 Service 的负载均衡策略，在 iptables 和 ipvs模式下，都有哪几种？具体工作模式是怎样的？感谢你的收听，欢迎你给我留言，也欢迎分享给更多的朋友一起阅读。![](Images/e870b7df0db49509e735e6becd4a9a9a.png){savepage-src="https://static001.geekbang.org/resource/image/47/55/47a6f3bf6b92d58512d5a2ed0a556f55.jpg"}