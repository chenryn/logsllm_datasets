---
author: Serge Mosin
category: 软件开发
comments_data: []
count:
  commentnum: 0
  favtimes: 1
  likes: 0
  sharetimes: 0
  viewnum: 5674
date: '2017-11-09 18:02:00'
editorchoice: false
excerpt: 我一直在想，各种各样的博客每天到底都有多少页面浏览量，以及在博客阅读受众中最受欢迎的是什么编程语言。我也很感兴趣的是，它们在谷歌的网站排名是否与它们的受欢迎程度直接相关。
fromurl: https://www.databrawl.com/2017/10/08/blog-analysis/
id: 9044
islctt: true
largepic: /data/attachment/album/201711/09/180208dcnhwgbjbx1c1sc1.jpg
permalink: /article-9044-1.html
pic: /data/attachment/album/201711/09/180208dcnhwgbjbx1c1sc1.jpg.thumb.jpg
related: []
reviewer: ''
selector: ''
summary: 我一直在想，各种各样的博客每天到底都有多少页面浏览量，以及在博客阅读受众中最受欢迎的是什么编程语言。我也很感兴趣的是，它们在谷歌的网站排名是否与它们的受欢迎程度直接相关。
tags:
- 博客
- 编程语言
thumb: false
title: 如何分析博客中最流行的编程语言
titlepic: true
translator: Chao-zhi
updated: '2017-11-09 18:02:00'
---
> 
> 摘要：这篇文章我们将对一些各种各样的博客的流行度相对于他们在谷歌上的排名进行一个分析。所有代码可以在 [github](https://github.com/Databrawl/blog_analysis) 上找到。
> 
> 
> 
![](/data/attachment/album/201711/09/180208dcnhwgbjbx1c1sc1.jpg)
### 想法来源
我一直在想，各种各样的博客每天到底都有多少页面浏览量，以及在博客阅读受众中最受欢迎的是什么编程语言。我也很感兴趣的是，它们在谷歌的网站排名是否与它们的受欢迎程度直接相关。
为了回答这些问题，我决定做一个 Scrapy 项目，它将收集一些数据，然后对所获得的信息执行特定的数据分析和数据可视化。
### 第一部分：Scrapy
我们将使用 [Scrapy](https://scrapy.org/) 为我们的工作，因为它为抓取和对该请求处理后的反馈进行管理提供了干净和健壮的框架。我们还将使用 [Splash](https://github.com/scrapinghub/splash) 来解析需要处理的 Javascript 页面。Splash 使用自己的 Web 服务器充当代理，并处理 Javascript 响应，然后再将其重定向到我们的爬虫进程。
> 
> 我这里没有描述 Scrapy 的设置，也没有描述 Splash 的集成。你可以在[这里](https://docs.scrapy.org/en/latest/intro/tutorial.html)找到 Scrapy 的示例，而[这里](https://blog.scrapinghub.com/2015/03/02/handling-javascript-in-scrapy-with-splash/)还有 Scrapy+Splash 指南。
> 
> 
> 
#### 获得相关的博客
第一步显然是获取数据。我们需要关于编程博客的谷歌搜索结果。你看，如果我们开始仅仅用谷歌自己来搜索，比如说查询 “Python”，除了博客，我们还会得到很多其它的东西。我们需要的是做一些过滤，只留下特定的博客。幸运的是，有一种叫做 [Google 自定义搜索引擎（CSE）](https://en.wikipedia.org/wiki/Google_Custom_Search)的东西，它能做到这一点。还有一个网站 [www.blogsearchengine.org](http://www.blogsearchengine.org/)，它正好可以满足我们需要，它会将用户请求委托给 CSE，这样我们就可以查看它的查询并重复利用它们。
所以，我们要做的是到 [www.blogsearchengine.org](http://www.blogsearchengine.org/) 网站，搜索 “python”，并在一侧打开 Chrome 开发者工具中的网络标签页。这截图是我们将要看到的：
![](/data/attachment/album/201711/09/181024bgawp0jaw3atqkga.png)
突出显示的是 blogsearchengine 向谷歌委派的一个搜索请求，所以我们将复制它，并在我们的 scraper 中使用。
这个博客抓取爬行器类会是如下这样的:
```
class BlogsSpider(scrapy.Spider):
    name = 'blogs'
    allowed_domains = ['cse.google.com']
    def __init__(self, queries):
        super(BlogsSpider, self).__init__()
        self.queries = queries
```
与典型的 Scrapy 爬虫不同，我们的方法覆盖了 `__init__` 方法，它接受额外的参数 `queries`，它指定了我们想要执行的查询列表。
现在，最重要的部分是构建和执行这个实际的查询。这个过程放在 `start_requests` 爬虫的方法里面执行，我们愉快地覆盖它：
```
    def start_requests(self):
        params_dict = {
            'cx': ['partner-pub-9634067433254658:5laonibews6'],
            'cof': ['FORID:10'],
            'ie': ['ISO-8859-1'],
            'q': ['query'],
            'sa.x': ['0'],
            'sa.y': ['0'],
            'sa': ['Search'],
            'ad': ['n9'],
            'num': ['10'],
            'rurl': [
                'http://www.blogsearchengine.org/search.html?cx=partner-pub'
                '-9634067433254658%3A5laonibews6&cof=FORID%3A10&ie=ISO-8859-1&'
                'q=query&sa.x=0&sa.y=0&sa=Search'
            ],
            'siteurl': ['http://www.blogsearchengine.org/']
        }
        params = urllib.parse.urlencode(params_dict, doseq=True)
        url_template = urllib.parse.urlunparse(
            ['https', self.allowed_domains[0], '/cse',
             '', params, 'gsc.tab=0&gsc.q=query&gsc.page=page_num'])
        for query in self.queries:
            for page_num in range(1, 11):
                url = url_template.replace('query', urllib.parse.quote(query))
                url = url.replace('page_num', str(page_num))
                yield SplashRequest(url, self.parse, endpoint='render.html',
                                    args={'wait': 0.5})
```
在这里你可以看到相当复杂的 `params_dict` 字典，它控制所有我们之前找到的 Google CSE URL 的参数。然后我们准备好 `url_template` 里的一切，除了已经填好的查询和页码。我们对每种编程语言请求 10 页，每一页包含 10 个链接，所以是每种语言有 100 个不同的博客用来分析。
在 `42-43` 行，我使用一个特殊的类 `SplashRequest` 来代替 Scrapy 自带的 Request 类。它封装了 Splash 库内部的重定向逻辑，所以我们无需为此担心。十分整洁。
最后，这是解析程序：
```
    def parse(self, response):
        urls = response.css('div.gs-title.gsc-table-cell-thumbnail') \
            .xpath('./a/@href').extract()
        gsc_fragment = urllib.parse.urlparse(response.url).fragment
        fragment_dict = urllib.parse.parse_qs(gsc_fragment)
        page_num = int(fragment_dict['gsc.page'][0])
        query = fragment_dict['gsc.q'][0]
        page_size = len(urls)
        for i, url in enumerate(urls):
            parsed_url = urllib.parse.urlparse(url)
            rank = (page_num - 1) * page_size + i
            yield {
                'rank': rank,
                'url': parsed_url.netloc,
                'query': query
            }
```
所有 Scraper 的核心和灵魂就是解析器逻辑。可以有多种方法来理解响应页面的结构并构建 XPath 查询字符串。您可以使用 [Scrapy shell](https://doc.scrapy.org/en/latest/topics/shell.html) 尝试并随时调整你的 XPath 查询，而不用运行爬虫。不过我更喜欢可视化的方法。它需要再次用到谷歌 Chrome 开发人员控制台。只需右键单击你想要用在你的爬虫里的元素，然后按下 Inspect。它将打开控制台，并定位到你指定位置的 HTML 源代码。在本例中，我们想要得到实际的搜索结果链接。他们的源代码定位是这样的:
![](/data/attachment/album/201711/09/181052l5g5svh3hqosogyy.png)
在查看这个元素的描述后我们看到所找的 `` 有一个 `.gsc-table-cell-thumbnail` CSS 类，它是 `.gs-title` `` 的子元素，所以我们把它放到响应对象的 `css` 方法（`46` 行）。然后，我们只需要得到博客文章的 URL。它很容易通过`'./a/@href'` XPath 字符串来获得，它能从我们的 `` 直接子元素的 `href` 属性找到。（LCTT 译注：此处图文对不上）
#### 寻找流量数据
下一个任务是估测每个博客每天得到的页面浏览量。得到这样的数据有[各种方式](https://www.labnol.org/internet/find-website-traffic-hits/8008/)，有免费的，也有付费的。在快速搜索之后，我决定基于简单且免费的原因使用网站 [www.statshow.com](http://www.statshow.com/) 来做。爬虫将抓取这个网站，我们在前一步获得的博客的 URL 将作为这个网站的输入参数，获得它们的流量信息。爬虫的初始化是这样的:
```
class TrafficSpider(scrapy.Spider):
    name = 'traffic'
    allowed_domains = ['www.statshow.com']
    def __init__(self, blogs_data):
        super(TrafficSpider, self).__init__()
        self.blogs_data = blogs_data