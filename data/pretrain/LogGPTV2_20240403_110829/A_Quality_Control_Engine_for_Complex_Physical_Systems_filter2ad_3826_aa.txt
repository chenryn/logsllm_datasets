title:A Quality Control Engine for Complex Physical Systems
author:Haifeng Chen and
Takehiko Mizoguchi and
Yan Tan and
Kai Zhang and
Geoff Jiang
2015 45th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
2015 45th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
A Quality Control Engine
for Complex Physical Systems
Haifeng Chen, Mizoguchi Takehiko, Yan Tan, Kai Zhang, Geoff Jiang
NEC Laboratories America, Princeton, NJ, USA
{haifeng, tmizoguchi, yan, kai, gfj}@nec-labs.com
Abstract—This paper proposes a novel framework to auto-
matically pinpoint suspicious sensors that lead to the quality
change in physical systems such as manufacture plants. Our
framework treats sensor readings as time series, and contains
three main stages: time series transformation to feature series,
feature ranking, and ranking score fusion. In the ﬁrst step, we
transform time series into a number of different feature series
to describe the underlying dynamics of each sensor data. After
that, the importance scores of all feature series are computed by
utilizing several feature selection and ranking techniques, each of
which discovers speciﬁc aspects of feature importances and their
dependencies in the feature space. Finally we combine importance
scores from all the rankers and all the features to obtain the ﬁnal
ranking of each sensor with respect to the system quality change.
Our experiments based on synthetic time series as well as sensor
data from a real system demonstrate the effectiveness of proposed
method. In addition, we have implemented our framework as
a production engine, and successfully applied it to several real
physical systems.
Keywords—Time series, quality control, feature extraction, slid-
ing window, feature selection, regularization
I.
INTRODUCTION
With the decreasing hardware cost and increasing demand
for autonomic management, many physical systems nowadays
are equipped with a large network of sensors distributed across
different parts of the system. The readings of sensors are
continuously collected time series, which monitor the opera-
tional status of physical systems. It is important to effectively
model and discover patterns from these sensor data so that
we can better understand the underlying dynamics of system
operation and hence facilitate practical management tasks, such
as anomaly detection, quality control, capacity planning, and
so on.
In this paper we leverage the sensor data to control the
quality of physical systems, especially for manufacturing sys-
tems. As we know, the output quality of practical manufac-
turing systems is controlled by human operations. While in
most cases the system can generate good products, the quality
of product may drop under certain conditions, which directly
affects the manufacturing proﬁts. Therefore, it is important to
discover the hidden conditions that lead to quality degradations
so that we can quickly adjust the system to avoid future losses.
In this paper the quality control is achieved by analyzing
the data from deployed sensors. As an outcome, we locate
suspicious sensors that lead to the quality changes, which can
assist system operators to quickly pinpoint the root cause of
quality degradation.
There are several challenges to discover suspicious sensors
for quality control. Firstly, there are a massive amount of
sensors in the system and the data collected from these sensors
can be correlated. It is impossible to manually check sensors
one by one. Secondly, data collected from different sensors
can also demonstrate different behaviors due to the diversities
in system components and their functionalities. For example,
while some sensors directly change their raw values in the
case of quality changes, other sensors may exhibit signiﬁcant
frequency changes in their readings. It
is hard to use a
uniform feature to capture the dynamics of the time series
from all sensors. Lastly,
the dependencies between sensor
data and system operational status are highly nonlinear. For
instance, a hidden fault in one component usually undergoes a
sequence of nonlinear physical processes before affecting the
ﬁnal production quality.
To address these issues,
this paper proposes a general
framework for quality control
in physical systems, which
utilizes several learning techniques such as feature selection
and ranking, and information fusion to achieve the automatic
and accurate localization of suspicious sensors. Given the
time series data from a senor, we ﬁrst transform it into a
number of different feature series. These features come from
our pre-deﬁned library that contains a large number of feature
deﬁnitions so as to describe different aspects of the signal
dynamics. As a result of transformation, we obtain a large
number of feature series based on the raw time series collected
from sensors. We then rank the importance of all these feature
series with respect to the system quality, by utilizing several
feature selection techniques including a regularization based
ranker, a tree based ranker, and a localized nonlinear ranker.
We adopt several rankers together because we want to cover
different views of feature importance and their dependencies
in the huge feature space, including both linear and nonlinear
relationships. Finally, we perform the ranking score fusion,
which combines outputs from all rankers, as well as the
ranking scores of each sensor. As the output we obtain the
ﬁnal ranking of sensors that can be used to explain the quality
change.
We have implemented the proposed method as a quality
control engine, and successfully applied it
to several real
physical systems such as a food manufacture system. Since
our engine covers different aspects of time series features and
their dependencies, it works very well in these tasks. Our
experimental results in Section VI will demonstrate some of
the evaluation results on real systems.
978-1-4799-8629-3/15 $31.00 © 2015 IEEE
978-1-4799-8629-3/15 $31.00 © 2015 IEEE
DOI 10.1109/DSN.2015.25
DOI 10.1109/DSN.2015.25
529
529
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:14:32 UTC from IEEE Xplore.  Restrictions apply. 
II. ENGINE OVERVIEW
Given n sensors in a system, we obtain n time series
x1(t), . . . , xn(t), where t = 1, . . . , T is the system operation
period. During that period, the quality of system is represented
by y(t), t = 1,··· , T . Usually y(t) can be obtained by a
special sensor called ‘key performance indicator’ (KPI) in the
system. Figure 1 presents an example of KPI values in a
real system. Based on the value of KPI, we divide system
operations into good-quality regions and bad-quality regions,
which is described by the dash line in Figure 1. Our goal is
to rank time series xi(t) based on their contributions to the
system quality change.
Fig. 1. An example of KPI time series
Usually the system quality changes are triggered by the
variances of underlying physical operations, which are in turn
represented by changes of the dynamics of related sensor read-
ings. However, the dynamics of different time series are usually
represented in different ways. For example, in Figure 2 (a), we
can infer the quality changes directly from raw values of that
time series, whereas for sensor in Figure 2 (b) the frequency
distribution in the readings is relevant. For the series in Figure
2 (c), the change of its temporal dependencies explains the KPI
changes. That is, in the good-quality region, the time series
have dependency relation x(t) = f (x(t − 1), x(t − 2), . . .)
whereas in the bad-quality region the relation changes to
x(t) = g(x(t− 1), x(t− 2), . . .), where f (·) (cid:2)= g(·). There are
also many other types of features to represent the evolution of
time series. In Section III, we construct a library of features
that potentially interpret a variety of time series evolution
patterns. These feature deﬁnitions come from the feedbacks
of system domain experts. While we can not guarantee that
they are complete, they describe the majority of time series
dynamics in physical systems.
Given the feature deﬁnitions in the library F1, . . ., Fm,
we still do not know which feature is the right one for
individual time series. Our strategy is to transform the raw
time series to all candidate feature series, x(t) → {x
F1 (t)
Fm (t)} and use feature selection technique in machine
. . . x
learning to automatically rank these features according to their
relationship to the quality change. In practice, we usually
encounter a huge feature space when the number of time series
n is large, since we will have altogether (m + 1)n feature
candidates including raw time series as well as their feature
series. It is not trivial to rank these features in a stable way
given such a large feature space. In addition, the dependencies
between features and the system quality can be highly nonlin-
ear. In order to address these issues, we provide an ensemble
Fig. 2. KPI and related raw time series
of feature rankers, including a regularization based feature
ranker [1], a tree based feature ranker [2] and RELEAFF [3]
feature ranker. These rankers may generate different subsets
of important features. For example, the regularization based
ranker focuses on the regression based relationship between
features and the system quality; the tree based ranker uses
information theory based criteria to detect important features,
whereas the RELEAFF based ranker looks at local regions
to detect nonlinear relationships. By combining the power
of those rankers, we expect to obtain a complete and stable
ranking from the large feature space.
Fig. 3. High level work ﬂow of the quality control engine
Figure 3 describes the work ﬂow of our quality control
engine. After feature transformation and ranking, we need to
combine all ranking results to obtain the ﬁnal ranked list of
suspicious sensors, which corresponds to the ‘Ranking score
fusion’ step in Figure 3. Such a process covers two dimensional
view of ranking score fusion. Firstly, since the ﬁnal output
is the ranking of sensors, i.e., the raw time series, we need
to aggregate all the feature ranking scores for each raw time
series. Secondly, we need to combine the output of different
rankers to obtain overall ranking score. By combining both
dimensions of ranking scores, we obtain the ﬁnal ranked list
of sensors based on their contribution to the system quality
change.
In the remaining sections, we will describe each step of
Figure 3 in detail. Section III describes the types of features
to transform the sensor data. Section IV describes the feature
ranking algorithms. Section V presents the fusion of ranking
score to get the ﬁnal sensor ranking. Some experimental results
are described in Section VI.
530530
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:14:32 UTC from IEEE Xplore.  Restrictions apply. 
TABLE I.
SAMPLES OF POSSIBLE FEATURES
B. Feature Library
feature type
basic statistics
frequency distribution
AR coefﬁcients
pairwise correlation
original time series
feature name
mean
standard deviation
skewness
kurtosis
5% quantile
95% quantile
maximum of power spectrum
frequency of Fmax
power in the n-th window
coefﬁcient of n-th past point
constant of AR model
AIC of the regression result
correlation of two subsequences
original time series itself
token
mean
std
skew
kurt
qt05
qt95
Fmax
FmxLoc
PinBinn
ARpn
ARcons
ARaic
corr
org
III. DISCOVER FEATURES FROM SENSOR DATA
Data from different sensors exhibit different dynamics with
respect to the system operation. Such dynamics can be different
shapes, frequencies, scales and so on. In order to handle
these heterogeneous behaviors, we transform the time series
collected from each sensor into a set of feature series. These
features cover various aspects of the dynamics of raw time
series, and can then be used to localize sensors that contribute
to quality changes.
A. Sliding Windows
The typical strategy for feature extraction from time series
is the sliding window technique. This technique enables us to
extract features from time series while preserving continuity
along the time axis. Figure 4 shows the concept of this
technique. Let us consider the feature extraction from a speciﬁc
•
•
•
We use the sliding window to transform each raw time
series into a number of feature series. Table I presents the list
of features implemented in our engine. These features mainly
cover the following aspects of time series properties.
•
Characteristics of time series in the temporal domain:
We extract basic statistics from time series to reﬂect
the shape of its evolution, which includes mean,
standard deviation, and some high order moments
of the subsequence within each sliding window. We
also compute the 5% and 95% quantile of the value
distribution in the sliding window.
Characteristics of time series in the frequency domain:
We apply the Fast Fourier Transform (FFT) [4] to
these subsequences and use information from the
power spectral density as features. For example, we
use the power and location of the most dominant
frequency as features. We also divide the frequency
region into different bands, and compute the sum of
power spectrum in each band as the feature.
The temporal dependencies of individual time series:
We use the auto-regressive (AR) model [5] to de-
scribe this property, and the coefﬁcients of AR model
are used as features. Note that not all time series
have strong temporal dependencies. We compute the
Akaike’s information criterion (AIC) score [6] as the
goodness of the AR model. If the score is always low
over time, we then ignore the AR related features for
that time series.
The dependencies across different time series: We can
also extract features from two or more time series. For
example, we compute their correlation coefﬁcient and
use it as the feature if we have subsequences of two
time series from the same sliding window.
After we extract a feature time series, we give it a name
which is called a ’token’ as shown in the right column of
Table I to it so that we can retrieve the original time series
and related feature series from tokens. For example, the mean
feature time series from a time series ‘Series1’ will be named
‘mean::Series1’. Figure 5 shows some examples of different
features from the same time series. They show that different
features capture different dynamics of time series behaviors
and features ‘PinBin0’ (bottom left) and ‘ARp1’ (bottom right)
seem to capture the changes of the original time series.
Fig. 4. Sliding window technique
time series xi(t), where i = 1, . . . , n is the index of time
series and t = 1, . . . , T is the time stamp. The width of the
window is denoted as w. If the series starts from t = t(cid:2), where
t(cid:2) = 1, . . . , T − w + 1, then we obtain a subsequence of width
w, i.e., xi(t(cid:2)), xi(t(cid:2) +1), . . . , xi(t(cid:2) +w−1) and we can extract
Fj
i (t(cid:2)) from that subsequence, i.e.,
a possible feature value x
Fj
i (t(cid:2))
{xi(t(cid:2)), xi(t(cid:2) + 1), . . . , xi(t(cid:2) + w − 1)} → x
(1)
where Fj represents the jth feature in the pre-deﬁned feature
Fj
library F. We extract feature x
i (t(cid:2)) from xi(t) for all
possible (cid:2) and obtain the corresponding feature time series with
Fj
Fj
Fj
length T − w + 1, i.e., x
i (T − w + 1).
i (1), x
i (2), . . . , x
If we extract m feature sequences as deﬁned in the feature
library F1, . . . ,Fm for each time series xi(t) (i = 1, . . . , n),
we will have totally (m + 1)× n series including the raw time
series.
Fig. 5. Examples of feature time series (the top left is raw time series)
531531
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:14:32 UTC from IEEE Xplore.  Restrictions apply. 
IV. FEATURE RANKING
After feature extraction, the original sensor data are trans-
formed to an expanded set of time series
x(t) = [x1(t), xF1
1 (t), . . . , xFm
(t), . . . , xn(t), xF1
1
n (t), . . . , xFm
n (t)](cid:2)
(2)
time series and the
The set
includes both the original
transformed feature series x(t) ∈ R
N (t = 1, . . . , T ), N =
(m+1)n where m is the total number of features in the feature
library and n is the number of raw time series. While feature
transformation provides an opportunity to generate different
time series properties, it poses challenges for us to accurately
select and rank important features (and hence raw time series)
because the problems space becomes much larger. In addition,
different feature series have correlations, and the relationships
between feature series and system quality is no longer linear. In
order to achieve a reliable and stable ranking of feature series,
we need to consider all aspects of feature interactions and their
dependencies with respect to the KPI quality. Therefore, rather
than relying on a single feature ranking method, we utilize an
ensemble of feature rankers, including a regularization based
ranker, a tree based ranker, and a local structure based ranker.
observation samples. For a node τ in the tree, we search for
the best feature xf in equation 2 that leads to a best split of
τ. That is, by comparing the values of xf with an optimal
cut point, the original node split into two sub-nodes τ(cid:2) and τr
containing n(cid:2) and nr samples respectively. The goodness of