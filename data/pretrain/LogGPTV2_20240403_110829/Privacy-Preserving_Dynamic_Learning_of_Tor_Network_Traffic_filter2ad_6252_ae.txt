10http://httparchive.org
11http://www.alexa.com/topsites
Session 10A: TORCCS’18, October 15-19, 2018, Toronto, ON, Canada1953(a) HTTP Archive Model
(b) BitTorrent Model
Figure 8: Total session size distribution in our general user models compared to actual session sizes in the full datasets.
smaller number multiple-transfer sessions that maintain the overall
distributions of page, request, and response sizes.
Our modeling approach is detailed in Algorithm 1. To create n ses-
sions, we first get a list of all of the first requests on pages, sorted by
their response sizes. We then split this list into n equally sized bins,
and take the median of the response sizes in each bin, and use them
as the first transfer sizes in our sessions (see lines 2-7). This median-
binning strategy ensures that the distribution of our first request
sizes best fits the distribution of request sizes in the original dataset.
We use a similar median-binning approach to compute the number
of requests for each session in lines 8-11, and their sizes in lines 12-
15. We show the distribution of the modeled total session sizes com-
pared to actual total page sizes in Figure 8a; a Kolmogorov–Smirnov
test of the distributions yields a result of 0.045 (p < 0.05) which
indicates that our model approximates the dataset reasonably well.
BitTorrent Bulk User Model. We also design a BitTorrent user
model that generates traffic similar to real-world BitTorrent traf-
fic. We use the same general strategy as for the web user model.
We were unable to find a dataset containing BitTorrent transfer
information, so we generated our own.
We sourced 30 torrent files from a wide variety of open source
Linux distributions and free software websites. Each torrent was
loaded one at a time into a version of the popular BitTorrent client
rTorrent12 that had been modified to export useful events regard-
ing connections and incoming/outgoing pieces. Each torrent was
allowed to completely finish downloading before the next started.
The torrents were downloaded while throttled to 25, 50, 75, and
100 Mbit/s for a total of 4 samples per torrent. We performed this
collection process on three different machines located in the United
Kingdom, California, and Washington DC. The machines had sym-
metric Internet connections limited to 10 Gbit/s, 1 Gbit/s, and 50
Mbit/s respectively. We successfully collected 309 samples of BitTor-
rent traffic in total during 2017-12 and 2018-01 with a wide range of
total torrent sizes, BitTorrent swarm sizes, and effective maximum
bandwidth rates.
To extract n representative sessions from this data, we used a
process similar to Algorithm 1. We start by sorting the sessions by
12https://rakshasa.github.io/rtorrent
the number of connections that they made or received (like line 2).
The sessions are then binned (like line 3) and the median number
of connections in each bin becomes the number used for one of the
n representative sessions (like lines 4-7).
From here, each of the n sessions has been assigned a number of
connections, but not an amount to to send, an amount to receive,
or a connection start time. To determine how much each of session
ni’s connections should send, we list all connections made in the
sessions in the ith bin, sort them by amount sent, and perform
median-binning. The analogous is done to determine an amount
to receive. We use median-binning to calculate a start time, but we
shuffle the resulting times such that the connection that sends and
receives the least does not also start the soonest.
The resulting distribution of modeled session sizes, where session
size is the sum of the bytes received across all connections, is plotted
against actual session sizes in Figure 8b; a Kolmogorov–Smirnov
test of the distributions yields a result of 0.22 (p < 0.005) which
indicates that our model approximates the dataset reasonably well.
5.2.3 PrivCount Model. Our PrivCount model is constructed
using a combination of the measurement results that we presented
in Section 3 and Section 4. Most importantly, each client generates
streams and packets in the PrivCount model according to the HMMs
from Section 4. We specify the HMM graph files as parameters for a
model action type in the TGen action graph (see Section 5.1.1). For
each such model action, we traverse the states in the stream HMM
graph to generate stream observations and we generate packet
schedules for each stream using the packet HMM graph. Delays
between streams and packets are sampled from the probability
distributions associated with the emission of each observation (pa-
rameters for which are encoded in the respective HMM graphs).
The traffic generation process continues until we reach the close
state in the stream HMM; once all of the generated stream transfers
finish, the model action is complete and TGen moves on to the next
action in its action graph.
We use our measurements from Section 3 to understand how
many clients and circuits to create. In particular, we create a num-
ber of active clients according to our entry measurements from
Table 3 while ignoring inactive clients. We scale our measurements
0510152025TotalSessionSize(MiB)0.00.20.40.60.81.0CumulativeFractionActualModel0123TotalSessionSize(GiB)0.00.20.40.60.81.0CumulativeFractionActualModelSession 10A: TORCCS’18, October 15-19, 2018, Toronto, ON, Canada1954according to the combined bandwidth fraction of the relays in our
PrivCount deployment during the measurement (see Table 2) and
the fraction of the Tor network that we are attempting to simulate.
For each active client, we sample a number of active circuits c from
the circuits-per-client distribution in Figure 2. We create c model ac-
tions in each client TGen graph, and use Tor’s IsolateSOCKSAuth
configuration option and a unique SOCKS username and password
combination for each model action when creating the connections
to Tor SOCKS proxy to ensure that a unique circuit is used for
streams generated by each action (this is same technique that is
used by Tor Browser Bundle to force streams onto different circuits).
Each client starts each of its c model actions at t ← 600
seconds
c
from the previous, such that the traffic generation process for each
model action starts at a time uniformly distributed throughout each
10 minute interval.
6 EVALUATION
In the previous sections, we measured Tor and described traffic
models that can be used to generate traffic in private Tor networks.
In this section, we evaluate the fidelity of private Tor networks that
generate traffic according to our models.
6.1 Experimental Setup
We use Shadow [24] to evaluate and compare our traffic models.
Shadow is a network simulator that runs real applications, including
Tor and TGen (described in Section 5.1.2). Shadow uses a network
graph to model Internet paths, including latency and bandwidth
rates, and runs a private Tor network over this Internet model. We
created an updated Shadow network model since the most recent
Shadow network graph available was created in 2013 [23].
6.1.1 Network Model. We are interested in modeling network
latency between all pairs of nodes that run in Shadow. To ensure
that our model is general enough to be useful for a variety of
experiments and for future work, we create a city-based latency
map using Internet ping measurements.
We use RIPE Atlas13 to perform and collect ping measurements.
Atlas provides vantage points called probes from which measure-
ments can be scheduled and collected. We first measured all 8,428
available Atlas probes and found that 2,993 of them responded to
ping. To reduce the overall number of required measurements, we
use only one representative probe per city (as assigned by Max-
mind geoip14) which had the highest uptime of those probes that
are assigned to the same city. This paring process left us with 1,813
probes, among all pairs of which we scheduled 3 ping measure-
ments in Atlas. Of the n·(n−1)
= 1, 642, 578 total pairs of probes, we
obtained at least one successful latency estimate for 1,245,948 (76%)
of the pairs and no successful estimate for 396,630 (24%) of the pairs.
Every vertex in our network graph represents one of the cities
for which we obtained a successful ping measurement; each such
vertex is assigned an IP address, city code, and country code, as
well as city upstream and downstream bandwidth rates that we
parsed from speedtest.15 For every pair of vertices (i.e., cities), we
created an edge in our graph and assigned it a latency according to
13https://atlas.ripe.net
14https://www.maxmind.com
15http://www.speedtest.net
2
Figure 9: Shadow and TorPerf performance benchmarks.
the mean of the successful pings between the pair; if no successful
measurement was available, then we take the mean of all pings be-
tween the two countries in which the cities were located. Similarly,
we used the mean bandwidth rates for the country in which the city
was located for any city for which speedtest did not provide city
bandwidth rates. Using this approach, we always favor the most
specific data that is available.
We were unable to find a city-based packet loss data set. Instead,
we assign each edge e a fractional packet loss rate pe that increases
linearly with the latency le assigned to the edge, such that pe =
0.015 · le/300. This loss model has been shown to yield reasonable
results when running Tor in Shadow [30].
6.1.2 Tor Host Model. We created three Tor host models for
Shadow, one for each of the traffic models from Section 5.2. We used
the standard network generation tools provided with Shadow in
order to generate the single file model. This network was configured
with 2000 Tor relays, 5000 TGen servers, and 60,000 TGen clients. Of
the TGen clients, 57,327 were configured to run the single file “web”
model, 1773 were configured to run the single file “bulk” model, and
900 were configured to emulate the TorPerf process (see Figure 7).
The resulting network configuration yields similar performance as
TorPerf benchmarks from the public network as shown in Figure 9.
In our protocol model we replace the “web” and “bulk” single
file TGen models from above with the more complex HTTP and
BitTorrent models described in Section 5.2.2. We also reduce the
number of clients: we create 4.8 times fewer “web” clients (i.e.,
11,943) since the median session size in our new model is 4.8 times
greater than the single file model (1.5 MiB vs. 320 KiB), and we
create half as many “bulk” clients since we expect the new multi-
stream BitTorrent model to achieve twice the throughput of the
single file model (due to Tor’s flow control, i.e., circuit and stream
windows [2]).
In our PrivCount model, we replace the “web” and “bulk” single
file TGen models with HMM clients that follow the traffic gener-
ation process as described in Section 5.2.3. We run half as many
active HMM clients as scaling our measurements in Table 3 dictates
due to resource constraints (each Tor client consumes ∼10-20 MiB
of RAM): we configure 128,519 TGen clients that each generate
020406080DownloadTime(s)0.00.20.40.60.81.0CumulativeFractionTorPerf50KiBShadow50KiBTorPerf1MiBShadow1MiBTorPerf5MiBShadow5MiBSession 10A: TORCCS’18, October 15-19, 2018, Toronto, ON, Canada1955(a) Single Counter Type
(b) Histogram Counter Type
Figure 10: Wasserstein distance (a.k.a., earth mover’s distance) between PrivCount measurements as a percentage of ground
truth, where a (G) indicates a measurement taken from an entry relay position and an (E) indicates a measurement taken
from an exit relay position. (10a) The cumulative percentage distance across all nine single counters was 703% for the single
file model, 1001% for the protocol model, and 408% for the PrivCount model. (10b) The cumulative percentage distance across
all six histogram counters was 150% for the single file model, 56% for the protocol model, and 95% for the PrivCount model.
twice as much traffic (and twice as many circuits) as explained in
in Section 5.2.3 using our HMM traffic generation process.
6.2 Results
We run our Tor networks in Shadow using each of the three client
models outlined above, and using a version of Tor that exports
PrivCount events that each relay logs to a file throughout the simu-
lation. Following each experiment, we configure a local PrivCount
deployment that consumes the logged events from our files rather
than from relays’ control ports as is PrivCount’s standard mode of
operation. This process allows us to directly compare our measure-
ment results from the public Tor network (i.e., ground truth) with
those from our Shadow simulations across a range of statistics.
6.2.1 Distance. We compare the measurement results from each
Shadow experiment to the ground truth Tor measurements using
Wasserstein distance (a.k.a., earth mover’s distance) as a metric.
Under this metric, the computed distance represents the minimum
amount of “dirt” that needs to be moved for each measurement in
order to yield an identical result to the ground truth measurement.
Each unit has equal weight for single counters, while for histograms
the distance for each bin i with a corresponding bin range [Li , Ri)
is weighted as (Li + Ri)/2.
We present the Wasserstein distances for each Shadow experi-
ment as percentages of the total ground truth values in Figure 10.
Figure 10a shows that our PrivCount model is closer to Tor for all
single-type counters that we measured. The cumulative percentage
distance across all nine single counters was 703% for the single file
model, 1001% for the protocol model, and 408% for the PrivCount
model. The highest distance values for the single file and protocol
models is attributed to the network producing significantly fewer
inactive clients, circuits of all types, and outbound bytes, and the
protocol model produced 343% more streams than indicated by our
ground truth measurements. The PrivCount model improves upon
these by incorporating our measurement results into the model. Fig-
ure 10b shows that the protocol and PrivCount models are similarly
close to Tor for histogram-type counters: the cumulative percentage
distance across all six histogram counters was 150% for the single
file model, 56% for the protocol model, and 95% for the PrivCount
model. We find that the single file model produces streams with
significantly more inbound bytes than we measured in Tor (and the
constant file sizes skew the distribution), while the protocol model
creates more streams per circuit than we observed in Tor. We find
that the largest distance in the PrivCount model was due to the
creation of too many inactive circuits and too many outbound bytes
per stream. We believe that the larger number of inactive circuits
was a result of more clients building more preemptive circuits than
in the other models; the PrivCount model used about 10 times the
number of clients as the protocol model, and about twice as many
clients as the single file model, and the distance associated with
inactive clients scales similarly. The larger number of outbound
bytes per stream is an artifact of our TGen HMM implementation:
the server-to-client packet delay sequence is generated on the client
side and sent to the server upon stream creation, yielding more
client-to-server bytes than usual. Future work should consider an
implementation that removes this artifact.
6.2.2 Performance. Although the PrivCount model produces
a network that is “closer” to Tor, we measured the performance
of our HMM generator process to better understand its overhead.
We used our stream model to generate 439,344 “circuits” (here,
each “circuit” corresponds to a sequence of inter-stream delays
in our experiments), and we used our packet model to generate
4,980,038 packet sequences (i.e., one inter-packet delay sequence for
each stream in each circuit). We measured the time to generate each
sequence, the total time to generate all packets and streams for each
circuit, and the generate event counts. Note that we measure the
time to generate the inter-packet and inter-stream delay sequences,
TotalClients(G)ActiveClients(G)InactiveClients(G)ActiveCircuits(G)InactiveCircuits(G)TotalStreams(E)TotalBytes(E)TotalInboundBytes(E)TotalOutboundBytes(E)020406080100DistancefromGroundTruth(%)3438386749892744353100969891999934336459464655472524822247SingleFileModelProtocolModelPrivCountModelActiveCircuitsPerClient(G)InactiveCircuitsPerClient(G)StreamsPerActiveCircuit(E)TotalBytesPerStream(E)InboundBytesPerStream(E)OutboundBytesPerStream(E)0102030405060DistancefromGroundTruth(%)1117853547843232711293111031SingleFileModelProtocolModelPrivCountModelSession 10A: TORCCS’18, October 15-19, 2018, Toronto, ON, Canada1956(a) Number of Generated HMM Events
(b) Time to Generate HMM Events
Figure 11: Performance overhead of our HMM generator processes.
but not the time to create packets and streams or transfer traffic.
The performance results are shown in Figure 11.
Our generator continues to generate streams for a circuit un-
til the stream model emits an end event, and it generates packets
for a stream until the packet model emits an end event. The num-
ber and distribution of such events that were generated in our
performance experiment are shown in Figure 11a. Our generator
generated fewer than 165 streams per circuit across all generated
circuits. We found that our generator did not generate more than
7,211 packets (∼10.5 MiB) for a single stream, and did not generate
more than 35,568 packets (51.9 MiB) for all streams in a circuit.
Figure 11b shows the time to generate HMM events. We found
that the maximum time to generate all of the streams in a circuit,
for all circuits, was 4 milliseconds. We also found that the maximum
time to generate a packet sequence for each stream was 52 millisec-
onds, and the maximum time to generate all packet sequences for
all streams in the same circuit was 274 milliseconds. Similarly, the
maximum total generator time for all streams and packets in a cir-
cuit was 278 milliseconds. We believe that the generator times are
negligible when compared to the time to send and receive network