The description of the inference algorithm in this section is
deliberately simpliﬁed with many details omitted for clarity.
Intuitively, the attacker monitors the similarity list(s) associ-
ated with each auxiliary item (i.e., item that he knows to be
associated with the target user). The attacker looks for items
which either appear in the list or move up, indicating increased
“similarity” with the auxiliary item. If the same target item
t appears and/or moves up in the related-items lists of a
sufﬁciently large subset of the auxiliary items, the attacker
infers that t has been added to the target user’s record.
Algorithm 1 shows the inference procedure. Intuitively, delta
matrices N∆ store information about the movement of each
target item t in the related-items lists of auxiliary items A
(we defer the discussion of matrix construction). The attacker
computes a score for each t using a scoring function. The
simplest scoring function counts the number of auxiliary items
in whose related-items lists t has appeared or risen. If the ﬁnal
score exceeds a predeﬁned threshold, the attacker concludes
that t has been added to the user’s record.
Scoring can be signiﬁcantly more complex,
taking into
account full dynamics of item movement on related-items
lists or giving greater weight
to certain lists. To reduce
false positives and improve inference accuracy, the scoring
function must be ﬁne-tuned for each system. For example,
recommender systems tend to naturally cluster items. Netﬂix
users who watched the DVD of the ﬁrst season of “The Ofﬁce”
also tend to watch the second season. Suppose that some movie
rises in the similarity lists of both seasons’ DVDs. Because
the overlap in viewers across seasons is so great, this does
not reveal much more information than a movie rising in the
list associated with a single DVD. In fact, it may reveal less if
users who watch only one of the two seasons are very unusual.
Our scoring functions prefer sets of auxiliary items which
span genres or contain obscure items. Consider LibraryThing,
where users share the books they read. Classics such as To
Kill a Mockingbird or Catcher in the Rye are so common that
changes in their similarity lists tend to result from widespread
trends, not actions of a single user. Movement of a book
in a list associated with an obscure book reveals more than
movement in a list associated with a bestseller.
B. Inference attack on the covariance matrix
In this setting of the problem, the item-to-item covariance
matrix is visible to any user of the system. An example of an
online recommender system that reveals the covariance matrix
is Hunch (see Section VI-A). We also explain complications,
such as asynchronous updates to the system’s public outputs,
which apply to the related-items scenario as well.
Let I be the set of items. The recommender system main-
tains an item-to-item matrix M. For any two distinct items
i, j ∈ I, the (i, j) cell of M contains a measure of the
similarity between i and j. In the setting of this section, (i, j)
and (j, i) contain the covariance between i and j. In the setting
of Section IV-A, the (i, j) cell contains the position, if any, of
item i in j’s related-items list, along with additional informa-
tion such as numeric similarity strength. As users interact with
the recommender system by making purchases, entering their
preferences, etc., the system continually accumulates more
data and updates M at discrete intervals.
the recommender system maintains a
“record” Su ⊂ I. As the user interacts with the system, some
item t may be added to Su, reﬂecting that t is now related to
the user. In some systems, the same item may be added to Su
multiple times: for example, the user may listen to a particular
song, watch a movie, or purchase a product more than once.
The system may also remove items from Su, but this is less
common and not used for our attack.
For each user u,
Consider a toy case when a single user u interacts with the
system between time τ1 and τ2 = τ1 + ∆, and t is added to
the user’s item list Su. Covariance between t and all other
items in Su must increase. Let M1 be the matrix at time τ1,
M2 the matrix at time τ2, and M∆ = M2 − M1. Then, for
all items si ∈ Su, the (si, t) entry of M∆ will be positive. Of
course, real-world recommender systems interact concurrently
with multiple users whose item sets may overlap.
Intuitively, the attack works as follows. The attacker has
auxiliary information about some items in the target user’s
record (Section III). By observing simultaneous increases in
covariances between auxiliary items and some other item t, the
attacker can infer that t has been added to the user’s record.
Formally, the attacker’s auxiliary information is a subset
A ⊆ Su. It helps—but is not necessary—if A is uniquely
identifying, i.e., for any other user uj of the recommender
system, A (cid:42) Suj . This is possible if items in A are less
popular or if A is large enough [26].
The attacker monitors the recommender system and obtains
the covariance matrix M at each update. Let T ⊆ I\A be the
set of items the user may have selected. The attacker observes
the submatrix of M formed by rows corresponding to the items
in T ∪ A and columns corresponding to the items in A. Call
this submatrix N. Since A ⊆ Su, when an item t ∈ T is
added to Su, covariances between t and many ai ∈ A will
increase. If the attacker can accurately recognize this event,
he can infer that t has been added to Su.
The inference procedure is signiﬁcantly complicated by the
fact that when an item is added to Su, not all of its covariances
are updated at the same time due to processing delays. In
particular, (t, ai) covariances for ai ∈ A may update at
different times for different auxiliary items ai. Furthermore,
auxiliary items may enter the system at or around the same
time as t. We cannot use the (t, ai) covariance unless we are
certain that the addition of item ai to u’s record has been
reﬂected in the system. Before attempting an inference, we
compute the subset of auxiliary items which “propagated”
into the covariance matrix. The algorithm works by measuring
increases in pairwise covariances between auxiliary items; we
omit the details due to space limitations. In the following, we
refer to this algorithm as PROPAGATEDAUX.
Constructing delta matrices. Suppose the attacker observes
the covariance submatrices Nτ1 , Nτ2, . . . at times τ1, τ2, . . ..
For each observation, the attacker creates a delta matrix N∆
which captures the relevant changes in covariances. There are
several ways to build this matrix. In the following, τmax is a
parameter of the algorithm, representing the upper bound on
the length of inference windows.
Strict time interval. For each τi, set N∆ = Nτi+1 − Nτi. Since
not all covariances may update between τi and τi+1, some
entries in N∆ may be equal to 0.
First change. For each τi, N∆ consists of the ﬁrst changes in
covariance after τi. Formally, for each entry (x, y) of N, let
τk > τi be the ﬁrst time after τi such that τk ≤ τmax and
Nτk[x][y] (cid:54)= Nτi[x][y]. Set N∆[x][y] = Nτk[x][y] − Nτi[x][y].
Largest change. Similar to ﬁrst change.
Making an inference. The attacker monitors changes in
the submatrix N. For each relevant interval ∆, the attacker
computes the delta matrix N∆ as described above and uses
PROPAGATEDAUX to compute which auxiliary items have
propagated into N. Then he applies Algorithm 2. In this
algorithm, scoreSett is the set of all auxiliary items whose
pairwise covariances with t increased, supportt is the sub-
set of scoreSett consisting of auxiliary items which have
propagated, scoret is the fraction of propagated items whose
covariances increased. If scoret and supportt exceed certain
thresholds (provided as parameters of the algorithm),
the
attacker concludes that t has been added to the user’s record.
Inference algorithms against real-world recommender sys-
tems require ﬁne-tuning and adjustment. Algorithm 2 is only a
high-level blueprint; there are many system-speciﬁc variations.
For example, the algorithm may look only for increases in
covariance that exceed a certain threshold.
C. Inference attack on kNN recommender systems
Our primary focus is on passive attacks, but for complete-
ness we also describe a simple, yet effective active attack on
Algorithm 2: MATRIXINFERENCE
Input: Set of target items T , set of auxiliary items A,
PROPAGATEDAUX returns a subset of A,
implementation-speciﬁc parameters
thresholdsupport,score
attacker to have been added to Su
Output: Subset of items from T which are believed by the
inf erredItems = {}
foreach observation time τ do
propagatedτ = PROPAGATEDAUX(A, τ )
∆ = observation period beginning at τ
N∆ = delta matrix containing changes in covariances
between items in T ∪ A
foreach item t in T do
scoreSett = subset of a ∈ A such that N∆[t][a] > 0
supportt = |scoreSett ∩ propagatedτ|
scoret =
if scoret ≥ thresholdscore and
supportt ≥ thresholdsupport then
inf erredItems = inf erredItems ∪ {t}
|propagatedτ |
|supportt|
return inf erredItems
the k-nearest neighbor (kNN) recommendation algorithm [1].
Consider the following user-to-item recommender system. For
each user U, it ﬁnds the k most similar users according to some
similarity metric (e.g., the Pearson correlation coefﬁcient or
cosine similarity). Next, it ranks all items purchased or rated
by one or more of these k users according to the number of
times they have been purchased and recommends them to U
in this order. We assume that the recommendation algorithm
and its parameters are known to the attacker.
Now consider an attacker whose auxiliary information con-
sists of the user U’s partial transaction history, i.e., he already
knows m items that U has purchased or rated. His goal is to
learn U’s transactions that he does not yet know about.
The attacker creates k sybil users and populates each sybil’s
history with the m items which he knows to be present in
the target user U’s history. Due to the sparsity of a typical
transaction dataset [26], m ≈ O(log N) is sufﬁcient for the
attack on an average user, where N is the number of users.
(In practice, m ≈ 8 is sufﬁcient for datasets with hundreds
of thousands of users.) With high probability, the k nearest
neighbors of each sybil will consist of the other k − 1 sybils
and the target user U. The attacker inspects the list of items
recommended by the system to any of the sybils. Any item
which appears on the list and is not one of the m items
from the sybils’ artiﬁcial history must be an item that U has
purchased. Any such item was not previously known to the
attacker and learning about it constitutes a privacy breach.
This attack is even more powerful
if the attacker can
adaptively change the fake history of his sybils after observing
the output of the recommender system. This capability is
supported by popular systems—for example, Netﬂix users
can change previously entered ratings, while Amazon users
can tell the site to ignore certain transactions when making
recommendations—and allows the attacker to target multiple
users without having to create new sybils for each one.
D. Attack metrics
Our attacks produce inferences of this form: “Item Y was
added to the record of user X during time period T .” The
main metrics are yield and accuracy. Yield is the number of
inferences per user per each observation period, regardless
of whether those inferences are correct. Accuracy is the
percentage of inferences which are correct. We use yield rather
than alternative metrics that focus on the number of correct
inferences because the attacker can adjust the parameters to
control the number of inferences made by our algorithm but
cannot directly control
the number or proportion that are
correct. Where it makes sense, we also express yield as the
percentage of the user’s transactions inferred by our algorithm,
but in general, we focus on the absolute number of inferences.
High yield and high accuracy are not simultaneously nec-
essary for an attack to be dangerous. A single accurate
inference could be damaging, revealing anything from a
medical condition to political afﬁliation. Similarly, a large
number of less accurate inferences could be problematic if
their implications are uniformly negative. While the victim
may retain plausible deniability for each individual inference,
this provides little or no protection against many privacy
violations. For example, plausible deniability does not help in
situations where judgments are based on risk (e.g., insurance)
or prejudice (e.g., workplace discrimination), or where the
inferred information further contributes to a negative narrative
(e.g., conﬁrms existing concerns that a spouse is cheating).
There is an inherent tradeoff between yield and accuracy.
The higher the yield, the higher the number of incorrect infer-
ences (“false positives”). Different combinations of parameters
for our algorithms produce either more inferences at the cost
of accuracy, or fewer, but more accurate inferences. Therefore,
we evaluate our algorithms using the yield-accuracy curve.
V. INFERENCE VS. PREDICTION
At ﬁrst glance, our inference algorithms may look similar
to standard collaborative ﬁltering algorithms which attempt to
predict the items that a user may like or purchase in the future
based on his and other users’ past transactions.
The two types of algorithms are completely different, both
technically and conceptually. We infer the user’s actual trans-
actions—as opposed to using the known behavior of similar
users to guess what he may do or have done. Prediction algo-
rithms discover common patterns and thus have low sensitivity
to the presence or absence of a single user. Our algorithms are
highly sensitive. They (1) work better if there are no similar
users in the database, but (2) do not work if the target user is
not the database, even if there are many similar users.
Collaborative ﬁltering often exploits covariances between
items; our algorithms exploit changes in covariance over time.
The accuracy of predictions produced by collaborative ﬁltering
does not change dramatically from period to observation
period; by contrast, we infer the approximate date when the
transaction occurred, which is very hard to discover using
collaborative ﬁltering. Finally, our algorithms can infer even
transactions involving very obscure items. Such items tend to
populate lower ranges of auxiliary items’ similarity lists, where
a single transaction has the biggest impact. Section VII shows
that transactions involving obscure items are more likely to be
inferred by our algorithms.
Prediction quality can be seen as a baseline for feasible
inference quality. A prediction is effectively an expected
probability that a user with item a will select some target
item t at any time. If a user with item a selects item t during
a given time period, he exceeds this expected probability,
causing a temporary rise (until other users balance the impact).
By looking at changes in predictions over short periods of
time, we can reconstruct how user behavior deviated from the
predictions to produce the observed changes. This yields more
accurate information than predictions alone. As Sections VI-A
and VII show, our algorithms not only outperform a Bayesian
predictor operating on the same data, but also infer items
ranked poorly by a typical prediction algorithm.
Finally, it is worth mentioning that we use some machine-
learning techniques for tuning inference algorithms that oper-
ate on related-items lists (see Section VI-C). These techniques
are very different from collaborative ﬁltering. Whereas collab-
orative ﬁltering attempts to predict future behavior based on
past behavior of other users, our models are backward-facing.
We know that an item has risen in a similarity list, but we
don’t know why. To produce accurate inferences, we must
learn which observations are sufﬁcient to conclude that this
rise signals addition of the item to the target user’s record. In
summary, we use machine learning to learn the behavior of
the recommender system itself, not the behavior of its users.
VI. EVALUATION ON REAL-WORLD SYSTEMS
We evaluated our inference algorithms on several real-world
recommender systems. Our goal was not to carry out an actual
attack, but to demonstrate the feasibility and measure the
accuracy of our algorithms. Therefore, all experiments were set
up so that we knew each user’s record in advance because the
user either revealed it voluntarily through the system’s public
interface or cooperated with us. This provided the “ground-
truth oracle,” enabling us to measure the accuracy of our
inferences without violating anyone’s privacy.
A. Hunch
Hunch.com provides personalized recommendations on a
wide range of topics. For each topic, Hunch poses a se-
ries of multiple-choice questions to the user and uses the
responses to predict the user’s preferences. Hunch also has
a large set of generic personal questions in the category
“Teach Hunch About You” (THAY),
intended to improve
topic recommendations. Hunch aggregates collected data and
publishes statistics which characterize popular opinions in
various demographics. For example, according to responses
given to Hunch, “birthers” are 94% more likely to say that
cultural activities are not important to them and 50% more
likely to believe in alien abductions [16].
Statistics collected by Hunch are accessible via an API.
They include the number of users responding to each THAY
question, the percentage selecting each possible answer, the
number of users who responded to each pair of questions, and
covariances between each pair of possible answers.
We show that aggregate statistics available via the Hunch
API can be used to infer an individual user’s responses to
THAY questions, even though these responses are not made
public by Hunch. Suppose the attacker knows some auxiliary
information about a Hunch user (e.g., height, hair color,
age, hometown, political views) which allows the attacker to
reliably predict how the user will respond to the corresponding
THAY questions. We refer to the latter as AUX questions. See
Section III for possible sources of auxiliary information.
Setup. The attacker forms a list of questions consisting of both
AUX questions and questions for which he does not know the
user’s responses. We refer to the latter as TARGET questions;
the objective of the experiment is to infer the user’s responses
to them. For our experiment, we chose questions with at least 4
possible answers. There were 375 such questions in the THAY
set at the time of our experiment with simulated users (see
below), but new THAY questions are continually added and
users may even suggest new questions.
Immediately prior to the attack, the attacker uses the API