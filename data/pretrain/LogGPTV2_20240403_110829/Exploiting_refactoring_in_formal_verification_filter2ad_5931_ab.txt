Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 05:35:32 UTC from IEEE Xplore.  Restrictions apply. 
overall proof structure.The transformations used and the mechanism oftheir selection is different for the two proofs, and so wediscuss each separately in this section.4.1.Support For The Implication ProofThe implication proof is the proof that theextracted specification implies the original specifica-tion from which the program was written. In principle,if the software is indeed a correct implementation ofthe specification, then it is always possible to constructsuch a proof. The challenge in Echo, however, is tomake the construction of the proof relatively routine.The feasibility of this proof rests in large measureon the form, content and structure of the extractedspecification. Echo uses several techniques to synthe-size this specification[17], but the key in Echo to mak-ing the proof practical lies in a technique that we referto as architectural and direct mapping. This techniquerests on the hypothesis that the high-level architecturalinformation in a specification is frequently retained inthe implementation. We have no experimental evi-dence to support this hypothesis, but our rationale forbelieving it is discussed in an earlier paper[17].Architectural and direct mapping provides thebasis of the implication proof. The structure of theproof is based on the specification architecture. Thebasic approach that we use is to try to match the staticfunction structure of the extracted specification to theoriginal specification, and to organize the proof as aseries of lemmas about the specification architecture.With this approach to proof, the closer theextracted specification’s architecture comes to that ofthe original specification, the higher the chance of theproof being completed successfully and in a reasonabletime. The transformations that are selected to apply tothe source program are those which will align theextracted specification’s architecture more closely withthat of the original specification.4.2.Support For The Implementation ProofThe implementation proof is the proof that theimplementation implies the low-level specification. Inthe prototype Echo system, the implementation proof iscarried out using the SPARK Ada toolset. The pre-ferred approach to developing SPARK Ada software isto use correctness by construction[6]. In correctnessby construction, the SPARK Ada tools are often able tocomplete proofs with either no or minimal humanintervention. The proof process is repeated as the soft-ware is constructed thereby ensuring that each refine-ment leaves the software amenable to proof.By contrast in Echo, since there are no restrictionson development techniques, the SPARK Ada tools fre-quently fail when they are applied to software afterdevelopment is complete. The low-level design of soft-ware that is not developed using correctness by con-struction is unlikely to be in a form suitable for proof.The reasons are many but, as with the implicationproof, they typically fall under the heading of complex-ity introduced to achieve some specific design or per-formance goals.The difficulties with the SPARK proof system takeone of three forms: (1) the required annotations forfunction pre- and post-conditions can be many dozensof lines long, lengths that are impractically complex forhumans to write; (2) the implementation proofexhausts available resources, usually memory, eventhough the SPARK tools are quite efficient and typi-cally adequate for proofs that are needed for correct-ness by construction; and (3) the verificationconditions sometimes are sufficiently complex thatthey cannot be discharged automatically, and humanguidance becomes necessary.Verification refactoring addresses all three of thesedifficulties without limiting the development process.Because verification refactoring does not need to main-tain any aspect of efficiency, any transformation thataddresses the three types of difficulty can be used.5.The Refactoring Process5.1.Definition of RefactoringThe Echo verification argument relies upon refac-toring, and so it is essential that there be a precise defi-nition of refactoring and a mechanism for ensuring thatrefactoring complies with this definition in practice.Since Echo is verifying functional behavior, we makethe following three simplifying assumptions: (1) thesource program terminates; (2) refactoring does notpreserve the execution time of the program; and (3)refactoring need not preserve the exact sequence ofintermediate program states as long as the initial stateand final state are unchanged. Assumption (3) alsoimplies that floating-point arithmetic accuracy is notguaranteed to be preserved and that the semantics ofnon-thread-safe programs are not preserved. The trans-formation from program P to program P’ is semanticspreserving if, given the same initial state, both P andP’ will terminate and generate the same final state.We need to be able to prove that any given trans-formation is semantics preserving, and, in order to doso for the general case, we define the semantics of the978-1-4244-4421-2/09/$25.00 c(cid:13)2009 IEEE
56
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 05:35:32 UTC from IEEE Xplore.  Restrictions apply. 
elements we need to model the transformation in PVS.For example, systems states are modeled as mappingsbetween identifiers and values, statement blocks andsubprograms are modeled as transitions between states,and pre- and post-conditions are predicates over states.For each generalized transformation, we use the PVStheorem prover to discharge the following theorem:init_state(P) = init_state(P’)=> final_state(P) = final_state(P’)We have developed a preliminary library of trans-formations for which the necessary properties havebeen proved. Similar libraries of semantics preservingtransformations exist in the domains of compilation,software maintenance, and reverse engineering. Wehave included some common transformations in ourlibrary, but few existing transformations can beadapted because they have different goals. Compilationtransformations, for example, are usually targeted atperformance improvement. Ours are designed toreduce the complexity and size of verification condi-tions, and so frequently reduce software’s efficiency.Here we itemize some of the refactorings that wehave developed and discuss how each affects the goalof verification. Due to space limitation, we do notinclude examples for each of them.Rerolling loops. A sequence of repeated statementblocks that can be differentiated by a certain parametercan be converted into a loop based on that parameter.For example, if the parameter is an integer takingsequential values, we can turn the statements into asimple for-loop:S1; S2; …; Sn;Üfor i in range 1..n loop S(i) end loop;Rerolling unrolled loops allows generated verifi-cation conditions to be simplified by recovering theloop structure and permitting the introduction of loopinvariants, especially when the repeated statementblock is large. Moving statements into or out of conditionals. Mov-ing statement blocks into or out of conditional state-ments provided no side effects will result can help tosimplify execution paths and to reveal certain proper-ties. An example would be the following if statementblock. S1 has no effect on conditional B:S1; if B then S2 else S3 end if; Üif B then S1; S2 else S1; S3 end if;Splitting procedures. Long procedures usually resultin verbose and complex verification conditions. Bysplitting a procedure into a set of smaller sub-proce-dures, the verification conditions become vastly sim-pler and easier to manage.Adjusting loop forms. Loops are frequently definedto promote efficiency and ease of use. Adjustment ofthe loop parameters can facilitate verification by, forexample, allowing loop invariants to be inserted moreeasily thereby simplifying verification conditions.Reversing inlined functions or cloned code. Revers-ing inlined functions involves identifying cloned codefragments and replacing them with function definitionsand calls. Function definitions can be provided by theuser or be derived from the code. This transformationaligns the code structures with the specification andremoves replicated or similar verification conditions soas to facilitate proof. Furthermore, by reversing theinlining of functions, if an error is identified in a partic-ular inlined function, only that function needs to be re-verified rather than all of the inlined instances.Separating loops. Loops that combine operations canbe split so as to simplify the associated loop invariants.Modifying redundant or intermediate computationsor storage. These transformations modify the programby adding or removing redundant or intermediate stor-age or computation. This can facilitate proof by: (a)storing extra but useful information; (b) shortening theverification condition by removing redundant or inter-mediate variables; or (c) merely tidying the code so asto facilitate understanding and annotation of the code.All the above refactorings and associated proofsare for general programs. We discuss the use of theserefactorings and the results of applying them in ourcase study in section 6.5.2.Applying RefactoringOur process for applying verification refactoringin practice is shown in Figure 1. A semantics-preserv-ing transformation from the library is selected by theuser (or suggested automatically), and the transformerthen checks the applicability of the selected transfor-mation mechanically and applies it mechanically if it isapplicable. When all of the selected transformationshave been applied, a metrics analyzer collects and ana-lyzes the code properties of the transformed code, andpresents the complexity metrics to the user. If the met-ric results are not acceptable, or if they are acceptablebut later verification proofs cannot be established, theprocess goes back to refactoring and more transforma-tion are performed.The role of the source-code metrics is to give theuser insight into the likely success of the two Echoproofs. We hypothesize that the metrics we use are anindication of relative complexity and therefore oflikely verification difficulty, and we present some sup-port for this hypothesis in the case study.978-1-4244-4421-2/09/$25.00 c(cid:13)2009 IEEE
57
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 05:35:32 UTC from IEEE Xplore.  Restrictions apply. 
Verification refactoring cannot be fully automaticin the general case, because recognizing effectivetransformations requires human insight except in spe-cial cases. Furthermore, some software, especiallydomain-specific applications, might require transfor-mations that do not exist in the library. In such circum-stance, the user can specify and prove a new semantics-preserving transformation using the proof template weprovide and add it to the library.To facilitate exploration with transformations, ifthe user has confidence in a new transformation, thesemantics-preserving proof can be postponed until thetransformation has been shown to be useful or evenuntil the remainder of the verification is complete.In most cases, the order in which transformationsare applied does not matter. Clearly, however, whentwo transformations are interdependent, they have tobe applied in order. A general heuristic is that thosetransformations that change the program structure andthose that can vastly reduce the code size should beapplied earlier.We are not aware of any circumstances of theirapplication in which a transformation would have to beremoved, and we make no explicit provision forremoval in the current tools and process. In the eventthat it becomes necessary, removing a transformation ismade possible by recording the software’s state prior tothe application of each transformation.All the user activities, especially the design andselection of transformations, have to be mechanicallychecked, and these two activities need to be supportedby automation to the extent possible. The transformeris implemented using the Stratego/XT toolset[4].Stratego checks the applicability of the selected trans-formation, and carries it out mechanically using termrewriting. We use the PVS theorem prover as the trans-formation proof checker and provide a proof template.When the user specifies a new transformation, anequivalence theorem will be generated automatically,and the user can discharge it interactively in the theo-rem prover.To our knowledge, there is no verification com-plexity metric available that could guide the user inselection of transformations, and so we present ahybrid of metrics to the user for review using a com-mercial metric tool[2], the SPARK Examiner, and ourown analyzer. The metrics include:Element metrics. Lines of code, number of declara-tions, statements, and subprograms, average size ofsubprograms, logical SLOC, unit nesting level, andconstruct nesting level.Complexity metrics. McCabe cyclomatic complexity,essential complexity, statement complexity, short-cir-cuit complexity, and loop nesting level.Verification condition metrics. The number and sizeof verification conditions, maximum length of verifica-tion conditions, and the time that the SPARK tools taketo analyze the verification conditions.Specification structure metrics. A summary andcomparison of the architectures of the original and theextracted specifications to suggest an initial impressionof the likely difficulty of the implication proof.Interpretation of the metrics is subjective, and wedo not have specific values that would give confidencein the ability of the PVS theorem prover to completethe implication proof.We developed the following heuristics to bothselect transformations and determine the order ofapplication: (1) transformations that depend on eachother are applied in order; (2) transformations thatimpact the major sources of difficulty, such as codeand VC size, are applied first; (3) transformations thataffect global structure are applied earlier and those thataffect local structure are applied later; and (4) refactor-ing proceeds until all proofs are possible.In practice, if specification extraction or either ofthe proofs fails to complete, or if either proof is unrea-Transformation LibraryTransformerTransformation Proof CheckerSpecify & prove new transformationAdd new transformation to libraryApply transformationCodeTransformed CodeMetric AnalyzerMechanically check & apply transformationNoYesYesNoImplementation ProofSpecification ExtractionImplication ProofOriginal SpecificationSelect transformationMetricmet?Verified?Statically collect & analyze code propertiesLater verification proofsAnalyzed Transformed CodeUserGuidanceFigure. 1. The verification refactoring process.978-1-4244-4421-2/09/$25.00 c(cid:13)2009 IEEE
58
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 05:35:32 UTC from IEEE Xplore.  Restrictions apply. 
sonably difficult, the user returns to refactoring andapplies additional transformations.6.Evaluation of RefactoringIn order to obtain an initial assessment of the effi-cacy and utility of verification refactoring, we under-took the verification of a non-trivial program that wedid not develop. The issues that affect the efficacy andutility of verification refactoring include: (1) the easewith which developers can select transformations; (2)the ease with which developers can add domain spe-cific transformations and prove them to be semanticspreserving; (3) whether selected transforms do facili-tate the necessary proofs; and (4) whether refactoringimpedes development in some way.Issues 1, 2, and 3 are tied closely to our use of met-rics, since we anticipate the values of metrics being thebasis for developers’ decisions. We sought to deter-mine: (1) the impact on metrics of individual types ofrefactoring and of series of refactorings; and (2) thevalues of the metrics for software that was amenable toproof and refactorings that were suggested by the val-ues of metrics. Our experience with refactoring in theverification of the subject application is the focus ofthis section and provides information about the firstthree issues. In section 7, we address the fourth issue.6.1.The Advanced Encryption StandardThe subject of study was the Advanced EncryptionStandard, and we used artifacts from the National Insti-tute of Standards and Technology (NIST). In previouswork[17], we conducted a study of the general feasi-bility of an earlier version of Echo in which we verifiedonly part of AES. In the work described here, we veri-fied the functional correctness of the complete AESimplementation. The AES artifacts that we used were:FIPS 197 specification. The Federal InformationProcessing Standards Publication 197[8] specifies theAES algorithm, a symmetric, iterated block cipher. Thespecification is mostly in natural language with mathe-matical statements and pseudo code for some algorith-mic elements.ANSI C implementation. Developed by Rijmen etal.[7], this optimized implementation is written inANSI C. It is 1258 lines of code and contains severaloptimizations to enhance its performance.These two artifacts were written independently ofthis project by others, and so there were no constraintson the development process imposed by the subsequentapplication of Echo.6.2.AES VerificationWe developed a formal version of the FIPS 197specification in PVS and translated the ANSI C imple-mentation into SPARK Ada, the notations used in thecurrent Echo instantiation. The PVS specification is811 lines long, excluding boilerplate constant defini-tions. The SPARK Ada implementation (1365 lineswithout annotations) was created by translation of theC statements into corresponding Ada statements.The verification of AES employed the completeEcho process: (1) a series of refactoring transforma-tions were applied; (2) the final refactored version wasdocumented using the SPARK Ada annotation lan-guage; (3) the code was shown to be compliant withthe annotations; (4) a high-level specification wasextracted from the refactored, annotated code; and (5)the extracted specification was shown to imply theoriginal specification.6.2.1. Verification Refactoring. The AES implemen-tation employs various optimizations (including imple-menting functions using table lookups, fully orpartially unrolling loops, and packing four 8-bit bytesinto a 32-bit word) that improved performance but alsocreated difficulties for verification. For instance, theSPARK tools ran out of resources on the original pro-gram because the unrolled loops created verificationconditions that were too large.We applied 50 refactoring transformations in eightcategories. Of those 50, the following 38 transforma-tions from six categories were selected from the proto-type Echo refactoring library (the number after thecategory name is the number of transformationsapplied in that category): rerolling loops (5); reversinginlined functions or cloned code (11); splitting proce-dures (2); moving statements into or out of condition-als (3); adjusting loop forms (4); modifying redundantor intermediate computations (2); and modifyingredundant or intermediate storage (11). The rationaleand use of these transformations are discussed in thenext section. In addition to these transformations, wealso added two new transformation categories for AES:Adjusting data structures (2). 32-bit words werereplaced by arrays of four bytes, and sets of four wordswere packed into states as defined by the specification.Constants and operators on those types were also rede-fined accordingly to reflect the transformations.Reversing table lookups (10). Ten table lookups werereplaced with explicit computations based on the docu-mentation and the precomputed tables removed.Both of these two added transformation types weredriven by the goal of reversing documented optimiza-978-1-4244-4421-2/09/$25.00 c(cid:13)2009 IEEE
59