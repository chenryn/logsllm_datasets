Certainly! Here is the optimized version of your text, with improved clarity, coherence, and professionalism:

---

**Dynamic Offloading Algorithm in Haetae**

Figure 5 illustrates the design of the dynamic offloading algorithm in Haetae. The fundamental concept is similar to opportunistic packet offloading to a GPU, as described in [23]. However, in our implementation, the unit of offloading is a flow, and the task for offloading is the complete flow analysis, not just pattern matching.

In our algorithm, two thresholds are used to determine whether a new flow should be offloaded. If the queue length (q) exceeds the first threshold (T1), a small portion (L1) of new flows are offloaded to the host machine. If this action successfully reduces the queue size, Haetae reverts to TILE-only flow analysis and stops offloading to the host. If the queue size continues to increase beyond the second threshold (T2), a larger portion (L2, typically L2 = 1) of new flows is offloaded to the host, which helps to drain the queue more quickly. When the queue length exceeds T2, the system maintains the offloading rate at L2 until the queue length drops below T1. This two-level offloading scheme prevents rapid fluctuations in queue length, stabilizing flow processing in either mode.

The offloading unit must be a flow because the host-side NIDS operates independently of the TILE-side NIDS. The host-side NIDS requires all packets in a flow to analyze the protocol and reassembled payload within the same flow. To support flow-level offloading, we add a bit flag to each flow table entry to mark whether a new packet belongs to an offloaded flow. This additional bookkeeping slightly reduces the per-tile analyzing performance due to the increased per-packet operations.

**Implementation Details**

We implemented Haetae by extending a TILE-optimized Suricata version from EZchip. This version optimizes the Aho-Corasick algorithm using special TILE memory instructions and employs a default mPIPE packet classifier to distribute incoming packets to tiles. To support the design features in Section 4, we implemented a per-tile NIDS engine, mPIPE computation offloading, lightweight packet metadata structure, and dynamic host-side flow offloading. This required a total of 3,920 lines of code modification to the baseline Suricata code.

For a shared-nothing, parallel NIDS engine, we implemented a lock-free flow table for each tile. By assigning a dedicated flow table to each NIDS engine, we eliminated access locks per flow entry and improved core scalability. The flow table is implemented as a hash table with separate chaining, and the table entries are pre-allocated at initialization. While the baseline version periodically removes idle flow entries, we adopted lazy deletion to reduce the overhead of per-flow timeouts. Idle flow entries are rare, so they are deleted only when there is memory pressure during other activities.

To maximize parallelism, we run an NIDS engine on 71 out of 72 tiles, with the remaining tile handling shell commands from the host machine.

Supporting a lightweight packet metadata structure was the most invasive update, as the structure is used by all modules. To minimize code modification and hide implementation details, we provided access functions for each metadata field. This required only 360 lines of code modification but touched 32 source code files.

Implementing mPIPE computation offloading was mostly straightforward, except for flow hash calculation. Baseline Suricata uses Jenkin’s hash function, which produces a 32-bit result. Implementing it with a 16-bit mPIPE processor required emulating 32-bit integer operations with 16-bit and 8-bit native instructions. We also needed to ensure that protocol decoding and hash calculation were within the 100-cycle budget to avoid degrading packet reception performance. mPIPE offloading modified both the existing mPIPE module and Suricata’s decode and flow management modules, requiring 130 and 100 lines of new code, respectively.

For dynamic host-side flow offloading, we implemented 1,700 lines of code on the tile side and 1,040 lines of code on the host side. First, we modified the receive module to measure the load of each tile and track the flows being offloaded to the host. Second, we implemented the tile-to-host packet transfer interface using a raw DMA API provided by TRIO. Finally, we modified the CPU-only version of Kargus to accept and handle traffic passed by the TILE platform.

**Evaluation**

Our evaluation addresses three aspects of Haetae:

1. **Performance Improvement and Overhead**: We quantified the performance improvement and overhead of mPIPE and host-side CPU offloading. Our results show that mPIPE offloading improves the performance of the decode and flow management modules by up to 128%, and host-side CPU offloading improves overall performance by up to 34%.

2. **Synthetic HTTP Workloads**: Using synthetic HTTP workloads, we showed the breakdown of performance improvement for each of our three techniques and compared the overall performance with Kargus with GPU and baseline Suricata on the TILE platform. The results indicate that Haetae achieves up to 2.4x improvements over Kargus and baseline Suricata.

3. **Real Traffic Evaluation**: Finally, we evaluated the NIDS performance using real traffic traces from the core network of a nationwide cellular ISP in South Korea. Haetae achieved a throughput of 48.5 Gbps, representing a 92% and 327% improvement over Kargus and baseline Suricata, respectively.

**Experimental Setup**

We installed a TILE-Gx72 board on a machine with dual Intel E5-2690 CPUs (octacore, 2.90 GHz, 20 MB L3 cache) and 32 GB of RAM. Haetae was run on the TILE platform, and CPU-based Kargus was run on the host side. Each NIDS was configured with 2,435 HTTP rules from the Snort 2.9.2.1 ruleset. For packet generation, we used two machines, each with dual Intel X5680 CPUs (hexacore, 3.33 GHz, 12 MB L3 cache) and dual-port 10 Gbps Intel NICs with the 82599 chipset. Our packet generator, based on PSIO [20], can transmit packets at line rate (40 Gbps each) regardless of packet size. For real traffic evaluation, we replayed 65 GB of packet traces from one of the largest cellular ISPs in South Korea [35]. We accounted for Ethernet overhead (preamble, interframe gap, and checksum) when calculating throughput.

**Computation Offloading Overhead**

This section quantifies the performance benefits and overhead of mPIPE and TRIO offloading.

**mPIPE Offloading Overhead**: We first verified whether offloaded computations adversely affect mPIPE's packet I/O throughput. We disabled all NIDS modules except the receive module and compared packet acquisition throughputs with and without mPIPE computation offloading. We generated TCP packets of varying sizes from 64 to 1514 bytes and measured the throughput for each packet size. Our results show that even with mPIPE computation offloading, packet I/O achieves line rates (80 Gbps) regardless of packet size. This confirms that the offloaded computations are within the cycle budget of the mPIPE processors and do not negatively impact packet I/O performance.

We then evaluated the performance improvement achieved by mPIPE offloading. Figure 6 compares the performances with and without offloading. To focus on the performance improvement by packet reception and flow management, we enabled the receive, decode, and flow management modules and disabled other modules (e.g., stream and detect modules) for the experiments.

mPIPE offloading shows a 15 to 128% improvement over baseline Suricata, depending on the packet size. Because mPIPE offloading reduces per-packet overhead, the improvement is more noticeable with small packets than with large packets. Overall, the results show that computation offloading to mPIPE brings significant performance benefits in NIDS subtasks.

**TRIO Offloading Overhead**: We measured TRIO's throughput in sending and receiving packets over the PCIe interface. This corresponds to the maximum performance improvement gain achievable using host-side flow offloading. We compared the throughputs of our optimized TRIO module and the existing one. Figure 7(a) shows the throughputs for varying packet sizes. The original TRIO module could not achieve more than 5.7 Gbps of throughput due to additional memory operations (i.e., `memcpy()`). Our optimized TRIO is up to 28 times faster. The relative improvement increases as the packet size increases because the overhead of DMA operation is amortized. The throughput saturates at 29 Gbps for packets larger than 512B, comparable to the theoretical peak throughput of 32 Gbps for an 8-lane PCIe-v2 interface. Note that the raw channel rate of a PCIe-v2 lane is 5 Gbps, and the use of the 8B/10B encoding scheme limits the peak effective bandwidth to 4 Gbps per lane. Figure 7(b) shows end-to-end throughputs of Haetae with CPU-side flow offloading for varying packet sizes. By exploiting both the TILE processors and host-side CPUs, we improve the overall NIDS performance by 18 Gbps, from 61 to 79.3 Gbps, for 1514B packets. While the overall performance is improved, we notice that the TILE-side performance degrades by 9 Gbps (to 52 Gbps in Fig. 7(b)) when TRIO offloading is used. This is because extra processing cycles are spent on PCIe transactions for packet transfers. We also note that the improvement is more significant with larger packets. Despite this, flow offloading improves performance by 79% for 64B packets. Given that the average packet size in real traffic is much larger than 100B [35], we believe that the actual performance improvement would be more significant in practice.

**Overall NIDS Performance**

Figure 8 shows the performance breakdown of the three key techniques under synthetic HTTP traffic. The overall performance ranges from 16 to 79 Gbps, depending on the packet size. mPIPE offloading and metadata reduction achieve 33% (for 1514B packets) to 88% (for 64B packets) improvements, and CPU-side flow offloading achieves an additional 32% improvement on average. Through these results, we find that reducing per-packet operations significantly improves overall NIDS performance, and we gain noticeable performance benefits by utilizing host resources.

Figure 9(a) compares the performances of Haetae with other systems under synthetic HTTP traffic. We compare Haetae with the baseline Suricata, customized for Tilera TILE-Gx processors, and Kargus with two NVIDIA GTX580 GPUs. In comparison with baseline Suricata, Haetae shows 1.7x to 2.4x performance improvement. We also see 1.8x to 2.4x improvement over Kargus in throughput (except for 64B packets). The high performance of Kargus for 64B packets is mainly due to its batched packet I/O and batched function calls, which significantly reduce overhead for small packets. In the case of Haetae, we found that batch processing in mPIPE is ineffective in packet reception due to different hardware structures.

Here, we compare Haetae with a pipelined NIDS design in [24]. Since the source code is not available, we resort to indirect comparison by taking the performance number measured using a TILE-Gx36 processor from [24]. The clock speeds of the TILE-Gx36 (1.2 GHz) and TILE-Gx72 (1.0 GHz) processors are different, but the comparison provides valuable insights into the relative performance gains.

---

I hope this optimized version meets your needs. Let me know if you need any further adjustments!