may be too large for the tile to catch up.
Figure 5 shows the design of the dynamic oﬄoading algorithm in Haetae.
The basic idea is similar to opportunistic packet oﬄoading to GPU in [23], but
100
J. Nam et al.
Packets processed on the TILE platform
Packets offloaded to the host
Queue length
T1
T2
Queue length
Receive module
Flow management
Measure q
No
…
q > T1 or 2
Yes
New or offloaded flows
TILE platform
Queues for
each tile
mPIPE
TRIO module
Host-side NIDS
Fig. 5. Dynamic ﬂow oﬄoading
the unit of oﬄoading is a ﬂow in our case, and the task for oﬄoading is the
entire ﬂow analysis instead of only pattern matching. In our algorithm, we use
two thresholds to determine whether a new ﬂow should be oﬄoaded or not.
If the queue length (q) exceeds the ﬁrst threshold (T1), a small portion (L1)
of new ﬂows are chosen to be oﬄoaded to the host machine. If it successfully
curbs the queue size blowup, Haetae reverts to TILE-only ﬂow analysis and
stops oﬄoading to the host side. However, if the queue size increases beyond the
second threshold (T2), a larger portion (L2, typically, L2 = 1) of new ﬂows is
oﬄoaded to the host machine, which helps drain the queue more quickly. When
the queue length exceeds the second threshold, the system keeps the oﬄoading
rate to L2 until the queue length goes below the ﬁrst threshold (T1). This two-
level oﬄoading scheme prevents rapid ﬂuctuation of the queue length, which
would stabilize ﬂow processing in either mode.
The unit of oﬄoading should be a ﬂow since the host-side NIDS is indepen-
dent of the TILE-side NIDS. The host-side NIDS should receive all packets in a
ﬂow to analyze the protocol as well as reassembled payload in the same ﬂow. To
support ﬂow-level oﬄoading, we add a bit ﬂag to each ﬂow table entry to mark
if a new packet belongs to a ﬂow being oﬄoaded or not. This extra bookkeeping,
however, slightly reduces the per-tile analyzing performance since it is rather
heavy per-packet operation.
5 Implementation
We implement Haetae by extending a TILE-optimized Suricata version from
EZchip. This version optimizes the Aho-Corasick algorithm with special TILE
memory instructions, and uses a default mPIPE packet classiﬁer to distribute
Haetae: Scaling the Performance of Network Intrusion Detection
101
incoming packets to tiles. To support the design features in Sect. 4, we imple-
ment per-tile NIDS engine, mPIPE computation oﬄoading, lightweight packet
metadata structure, and dynamic host-side ﬂow oﬄoading. This requires a total
of 3,920 lines of code modiﬁcation of the baseline Suricata code.
For shared-nothing, parallel NIDS engine, we implement a lock-free ﬂow table
per each tile. By assigning a dedicated ﬂow table to each NIDS engine, we
eliminate access locks per ﬂow entry and improve the core scalability. The ﬂow
table is implemented as a hash table with separate chaining, and the table entries
are pre-allocated at initialization. While the baseline version removes idle ﬂow
entries periodically, we adopt lazy deletion of such entries to reduce the overhead
of per-ﬂow timeouts. Idle ﬂow entries are rare, so it suﬃces to delete them
in chain traversal for other activities only when there is memory pressure. To
maximize the parallelism, we run an NIDS engine on 71 tiles out of 72 tiles. The
remaining tile handles shell commands from the host machine.
Supporting lightweight packet metadata structure is the most invasive update
since the structure is used by all modules. To minimize code modiﬁcation and to
hide the implementation detail, we provide access functions for each metadata
ﬁeld. This requires only 360 lines of code modiﬁcation, but it touches 32 source
code ﬁles.
Implementing mPIPE computation oﬄoading is mostly straightforward
except for ﬂow hash calculation. Baseline Suricata uses Jenkin’s hash function [1]
that produces a 32-bit result, but implementing it with a 16-bit mPIPE proces-
sor requires us to emulate 32-bit integer operations with 16-bit and 8-bit native
instructions. Also, we needed to test whether protocol decoding and hash calcu-
lation is within the 100-cycle budget so as not to degrade the packet reception
performance. mPIPE oﬄoading modiﬁes both the existing mPIPE module and
Suricata’s decode and ﬂow management modules, which requires 130 and 100
lines of new code, respectively.
For dynamic host-side ﬂow oﬄoading, we implement 1,700 lines of code on
the tile side and 1,040 lines of code on the host side. First, we modify the receive
module to measure the load of each tile and to keep track of the ﬂows that
are being oﬄoaded to the host. Second, we implement the tile-to-host packet
transfer interface with a raw DMA API provided by TRIO. Finally, we modify
the CPU-only version of Kargus to accept and handle the traﬃc passed by the
TILE platform.
6 Evaluation
Our evaluation answers three aspects of Haetae:
1. We quantify the performance improvement and overhead of mPIPE and
host-side CPU oﬄoading. Our evaluation shows that the mPIPE oﬄoading
improves the performance of the decode and ﬂow management modules by up
to 128 % and the host-side CPU oﬄoading improves the overall performance
by up to 34 %.
102
J. Nam et al.
2. Using synthetic HTTP workloads, we show the breakdown of performance
improvement for each of our three techniques and compare its overall perfor-
mance with Kargus with GPU and baseline Suricata on the TILE platform.
The result shows that Haetae achieves up to 2.4x improvements, over Kargus
and baseline Suricata.
3. Finally, we evaluate the NIDS performance using real traﬃc traces obtained
from the core network of one of the nation-wide cellular ISPs in South
Korea. Haetae achieves a throughput of 48.5 Gbps, which is a 92 % and
327 % improvement respectively over Kargus and baseline Suricata.
6.1 Experimental Setup
We install a TILE-Gx72 board on a machine with dual Intel E5-2690 CPUs
(octacore, 2.90 GHz, 20 MB L3 cache) with 32 GB of RAM. We run Haetae
on the TILE platform and CPU-based Kargus on the host side. Each NIDS is
conﬁgured with 2,435 HTTP rules from the Snort 2.9.2.1 ruleset. For packet
generator, we employ two machines that individually have dual Intel X5680
CPUs (hexacore, 3.33 GHz, 12 MB L3 cache) and dual-port 10 Gbps Intel NICs
with the 82599 chipset. Our packet generator is based on PSIO [20] that can
transmit packets at line rate (40 Gbps each) regardless of packet size. For real
traﬃc evaluation, we replay 65 GB of packet traces obtained from one of the
largest cellular ISPs in South Korea [35]. We take the Ethernet overhead (such
as preamble (8B), interframe gap (12B), and checksum (4B)) into consideration
when we calculate a throughput.
6.2 Computation Oﬄoading Overhead
This section quantiﬁes the performance beneﬁt and overhead of mPIPE and
TRIO oﬄoading.
Baseline
w/ decoding and hash computations
)
s
p
b
G
(
t
u
p
h
g
u
o
r
h
T
80
70
60
50
40
30
20
10
0
64
128
256
Packet size (Bytes)
512
Fig. 6. Throughputs of the decoding and ﬂow management modules with mPIPE
oﬄoading. The throughputs are line rate (80 Gbps) for 1024 and 1514B packets.
Haetae: Scaling the Performance of Network Intrusion Detection
103
MPIPE Oﬄoading Overhead. We ﬁrst verify whether oﬄoaded computa-
tions adversely aﬀect mPIPE’s packet I/O throughput. For this, we disable all
NIDS modules other than the receive module, and compare the packet acquisi-
tion throughputs with and without mPIPE computation oﬄoading. We generate
TCP packets of varying size from 64 to 1514 bytes and measure the through-
put for each packet size. Our result shows that even with mPIPE computation
oﬄoading packet I/O achieves line rates (80 Gbps) regardless of packet size.
This conﬁrms that the oﬄoaded computations are within the cycle budget of
the mPIPE processors, and oﬄoading does not adversely aﬀect the packet I/O
performance.
We then evaluate the performance improvement achieved by mPIPE oﬄoad-
ing. Figure 6 compares the performances with and without oﬄoading. To focus
on the performance improvement by packet reception and ﬂow management,
we enable the receive, decode, and ﬂow management modules and disable other
modules (e.g., stream and detect modules) for the experiments.
The mPIPE oﬄoading shows 15 to 128 % improvement over baseline Suricata
depending on the packet size. Because mPIPE oﬄoading alleviates per-packet
overhead, improvement with small packets is more noticeable than with large
packets. In sum, the results show that computation oﬄoading to mPIPE brings
signiﬁcant performance beneﬁts in the NIDS subtasks.
TRIO Oﬄoading Overhead. We now measure TRIO’s throughput in send-
ing and receiving packets over the PCIe interface. Note this corresponds to the
maximum performance improvement gain achievable using host-side ﬂow oﬄoad-
ing. We compare the throughputs of our optimized TRIO module and the exist-
ing one. Figure 7(a) shows the throughputs by varying packet sizes. The original
TRIO module cannot achieve more than 5.7 Gbps of throughput because it ﬁrst
copies data into its buﬀer to send data across the PCIe bus. Such additional mem-
ory operations (i.e., memcpy()) signiﬁcantly decrease the throughputs. Our opti-
mized TRIO is up to 28 times faster. The relative improvement increases as the
packet size increases because the overhead of DMA operation is amortized. The
throughput saturates at 29 Gbps over for packets larger than 512B, which is com-
parable to the theoretical peak throughput of 32 Gbps for an 8-lane PCIe-v2 inter-
face. Note that the raw channel rate of a PCIe-v2 lane is 5 Gbps, and the use of
the 8B/10B encoding scheme limits the peak eﬀective bandwidth to 4 Gbps per
lane. Figure 7 (b) shows end-to-end throughputs of Haetae with the CPU-side ﬂow
oﬄoading by varying packet size. By exploiting both the TILE processors and host-
side CPUs, we improve the overall NIDS performance by 18 Gbps, from 61 to 79.3
Gbps, for 1514B packets. While the overall performance is improved, we notice
that the TILE-side performance degrades by 9 Gbps (to 52 Gbps in Fig. 7(b)) when
TRIO oﬄoading is used. This is because extra processing cycles are spent on PCIe
transactions for packet transfers. We also note that the improvement with larger
packets is more signiﬁcant. This is because the PCIe overhead is relatively high for
small-sized packets and the CPU-side IDS throughput with small packets is much
lower compared to its peak throughput obtained for large packets. Despite the fact,
104
J. Nam et al.
the ﬂow oﬄoading improves the performance by 79 % for 64B packets. Given that
the average packet size in real traﬃc is much larger than 100B [35], we believe that
the actual performance improvement would be more signiﬁcant in practice.
Original
Optimized
)
s
p
b
G
(
t
u
p
h
g
u
o
r
h
t
O
/
I
30
25
20
15
10
5
0
Tile
Host
)
s
p
b
G
(
t
u
p
h
g
u
o
r
h
T
80
70
60
50
40
30
20
10
0
64
128
256
512
1024
1514
64
Packet size (Bytes)
128
Packet size (Bytes)
256
512
1024
1514
Fig. 7. TRIO performance benchmarks: (a) TRIO throughputs with and without our
optimizations, (b) Throughputs with ﬂow oﬄoading
6.3 Overall NIDS Performance
Figure 8 shows the performance breakdown of the three key techniques under
synthetic HTTP traﬃc. The overall performance ranges from 16 to 79 Gbps
depending on the packet size. mPIPE oﬄoading and metadata reduction achieve
33 % (1514B packets) to 88 % (64B packets) improvements and CPU-side ﬂow
oﬄoading achieves 32 % additional improvement on average. Through the results,
we ﬁnd that reducing the per-packet operations signiﬁcantly improves the overall
NIDS performance, and we gain noticeable performance beneﬁts by utilizing the
host resources.
Figure 9(a) shows the performances of Haetae compared to other systems
under the synthetic HTTP traﬃc. We compare with the baseline Suricata, cus-
tomized for Tilera TILE-Gx processors, and Kargus with two NVIDIA GTX580
GPUs. In comparison with baseline Suricata, Haetae shows 1.7x to 2.4x per-
formance improvement. We also see 1.8x to 2.4x improvement over Kargus in
throughput (except for 64B packets). The relatively high performance of Kargus
for 64B packets mainly comes from its batched packet I/O and batched func-
tion calls, which signiﬁcantly reduces the overhead for small packets. In case of
Haetae, we ﬁnd that batch processing in mPIPE is ineﬀective in packet reception
due to diﬀerent hardware structure.
Here, we compare Haetae with a pipelined NIDS design in [24]. Because the
source code is not available, we resort to indirect comparison by taking the per-
formance number measured using a TILE-Gx36 processor from [24]. Since the
clock speeds of the TILE-Gx36 (1.2 GHz) and TILE-Gx72 (1.0 GHz) processors
Haetae: Scaling the Performance of Network Intrusion Detection
105
No optimization
Lightweight data structures
mPIPE offloading
Host offloading
)
s
p
b
G
(
t
u
p