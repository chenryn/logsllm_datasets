occurrences, the model keeps these most commonly changed
words unchanged. 2) There are no clear sparse transitions
between words; meaning that a word is not always replaced
by a speciﬁc word. 3) These message-holding words are not
exclusive to the watermark occurrence. 4) These words are all
from the most occurring words in the dataset (see Figure 7).
These observations suggest that the model does not produce
obvious artifacts or telltale signs in terms of changing the
statistics of top words. In addition, there are no ﬁxed rules
that could describe or substitute the changes since it does not
perform systematic changes between pairs of words. Thus,
these factors contribute to the hidden encoding of information.
C. Secrecy Evaluation
Our analysis shows that the model hides the information in
a natural way by optimizing the translations to be as close
and correct as possible to the original
text. Nevertheless,
we systematically study the secrecy by training classiﬁers to
discriminate between encoded and non-encoded text. Although
secret communication is not the primary task, the adversary
might train a classiﬁer in order to help identify and possibly
remove the watermark (e.g., by adversarial training). Besides,
the secrecy evaluation is another proxy for the naturalness of
the encoding.
We train the classiﬁers on non-paired (i.e., disjoint) wa-
termarked and non-watermarked text,
is a reasonable
assumption since the adversary does not have access to text
before watermarking. We split the training data into two equal
that
parts and use AWT to watermark one of them by different
messages sampled randomly. Similarly, we watermark the test
and validation sets, but we use the entire sets for each class.
We measure the classiﬁers’ success by the F1 score; ideally,
it should be at a chance level performance (0.5), denoting the
adversary’s failure.
We compare the classiﬁers’ success on different model’s
variants; the no-discriminator model, the no ﬁne-tuning model,
and the full AWT. Since we use a transformer-based dis-
criminator in AWT, we also study the generalization with
respect to different classiﬁers (transformer, LSTM, and Bag-
of-Words with Support Vector Machines [85]). The LSTM
and transformer classiﬁers are trained on single length-varying
sentences (similar to AWT training). The BoW+SVM classiﬁer
is trained on word occurrences’ feature vectors in paragraphs
(up to 800 words). For each classiﬁer, we optimize the
hyperparameters and show the results of the most adversarial
ones. Finally, we examine the effect of improving the quality
(by sampling) on the classiﬁers’ performance.
As depicted in Table VI, we summarize our main conclu-
sions as follows: 1) The no-discriminator model shows very
high F1 scores (especially on longer sequences) indicating
poor hiding capability, which supports our previous qualita-
tive observations. 2) The adversarially trained models show
considerably lower F1 scores across different classiﬁers. 3)
Improving the quality (by sampling) helps to further reduce
the classiﬁers’ success. 4) The full AWT with sampling model
has a very close to the ideal random chance performance.
D. Robustness Evaluation
Regardless of the secrecy, the adversary might apply dif-
ferent attacks on the watermarked text. The attacker’s goal is
to tamper with (remove) the watermark with minimal changes
to the text, in order to still have a high utility. Therefore, for
each attack, we study the relationship between the drop in bit
accuracy and text similarity. We focus on automated attacks
without human inspection or modiﬁcation [5], [20]. In Sec-
tions V-D1 and V-D2, we study two attacks that assume that
the adversary has general knowledge about using a translation-
based watermarking scheme. However, AWT is not explicitly
trained or tailored to defend against these attacks. We study
adaptive attacks in Section V-D3 and V-D4 where the attacker
can train their own watermarking model (AWTadv).
Model’s variant
Sampling
Classiﬁer
F1 score
− discriminator
1 sample
− ﬁne-tuning
1 sample
AWT
Best of 20 samples
Best of 20 samples
0.89
Transformer
LSTM
0.80
BoW+SVM 0.98
0.65
Transformer
LSTM
0.56
BoW+SVM 0.63
0.59
Transformer
0.53
Transformer
Fig. 8: A matrix of word changes’ count from the original
text to modiﬁed text using AWT. We show the no-diagonal
transitions only in Appendix VIII-C.
TABLE VI: Secrecy evaluation of different model’s variants
indicated by the F1 score of the adversary.
anhad@-@onbyatasofwithfromwerebeenthatcouldalso,wasbutandToanhad@-@onbyatasofwithfromwerebeenthatcouldalso,wasbutandFrom5703656402559411405935423956205500005111116135123376211714166129400034153227722220522342466311551331470002643822755060333141017533657021200011822402293233110844271353801210104931521068111017318565994922374520000341452463529215025127263230185197000146917110933341139128145273535114470101924344128193079221222221535522000018736465929312631261253262521100000308388521946691532444110461332266320003521372528242666143843280352636101002244126731100251004317041012020120203151021107000413534574733361053247504330242023000000000101000100211880000000000000000000489801000000000000000008680000000000000000000112001) Random changes: We consider two types of random
changes to the watermarked text: removing words and re-
placing words with synonyms. For each attack, we change
each word with a probability p that we vary from 0.05 to
0.2. For each case, we compute the bit accuracy and SBERT
distance. For synonym substitution, we use WordNet as a
commonly used lexical database to ﬁnd synonyms for words
to be replaced. Instead of the naive random replacement, we
assume that the attacker has access to a model like SBERT
and uses it to select the synonym that gives the lowest distance
from the set of possible synonyms.
We demonstrate the results of these two attacks in Figure 9.
We perform these attacks on the output of AWT using 1 sample
in Figure 9a, and 20 samples in Figure 9b. The ‘remove’ attack
was found to be slightly more successful than the ‘replace’
attack since not all words used to encode the message have
synonyms in WordNet. However, For both the two attacks and
the two operating points, the bit accuracy decreased by 0.05%
up to 6.5%, while on the other hand, the SBERT increased by
86% up to 577%. This shows that the bit accuracy is robust to
local changes and that the adversary needs to substantially
change the text by random changes in order to make the
watermark not usable.
2) Denoising: Instead of random changes, a more knowl-
edgeable adversary might train counter-models to reconstruct
the text. We train a transformer-based denoising autoencoder
(DAE) [10] (sequence-to-sequence model) that is tasked to
denoise an input sequence. We apply two types of noise to
the input sequence (S): embedding dropout, and random word
replacement, to form a corrupted sequence ( ˆS). The noise is
applied with a 5% probability. ˆS is then fed to the encoder.
The decoder is tasked to reconstruct the original sequence S,
and is fed the shifted S. The denoising maximizes p(S| ˆS),
which can be described as [10]:
n(cid:89)
p(S| ˆS) =
p(Wi| ˆS, W<i)
i=1
That is: predicting Wi is conditioned on the full corrupted
sequence ˆS and the left side non-noisy sequence W<i.
We perform the DAE training on non-watermarked text, and
use the trained DAE to denoise the watermarked text at test
time. If the DAE was trained on watermarked text, it would
be tasked to reconstruct it and therefore would not change the
watermark. In contrast, with the current setup, the watermark
could approximate the noise applied during the DAE training.
The word replacement noise is in line with our watermarking
scheme that is also based on word replacement, imitating an
adversary with prior knowledge about our approach.
We hypothesize that a less natural encoding of the in-
formation would be more vulnerable to denoising than a
more natural one. To validate this, we apply the DAE on
the output of the three model’s variants that we previously
discussed, without applying additional noise. We demonstrate
this experiment in Table VII in which we show the bit accuracy
drop and the SBERT relative change. We summarize our
interpretation as follows: 1) Improving the quality makes the
denoising attack less effective; the ‘no-discriminator’ model
had a huge drop in bit accuracy and it reached a chance
level, while it decreased slightly for the other variants, in
particular, the better-quality AWT model. 2) The DAE does
not perfectly reconstruct the sentences and still introduces
other changes besides the watermark’s changes, this increased
the SBERT distance for the two adversarially trained models.
3) On the other hand, the changes introduced to the ‘no-
discriminator’ model reduced the SBERT,
indicating more
successful denoising. We show examples of these different
cases and more details about the DAE in Appendix VIII-B.
We then study a different attack variant where we introduce
additional noise to the watermarked text before applying the
DAE. This is, instead of applying random word replacement
solely as an attack, we apply these random changes that might
remove the watermark, and then use the DAE to generate
a more realistic/smoothed sentence than the corrupted one.
Similarly, we vary the probability of the noise and study the
relationship between bit accuracy and SBERT distance. We
show in Figure 9 the performance of this attack in comparison
with random changes alone. We found that this variant is more
effective than using random changes; at the same level of
SBERT, the drop in bit accuracy is higher. However, it still
causes a signiﬁcant increase in the SBERT distance (e.g., at a
10% drop in bit accuracy, the SBERT increased by 319%).
3) Re-watermarking: Watermark piracy [56], [86] is an
attack in model watermarking where the adversary’s goal is
to dispute or claim ownership of a stolen watermarked model
by inserting their own watermark (to corrupt, exist alongside,
or replace the original [56]). We adapt re-watermarking as an
attack on our method. Our threat model targets misuse instead
of model stealing. Thus, we assume that the adversary’s goal
is to use the service/APIs without getting detected, instead of
claiming ownership, i.e., to corrupt or tamper with the owner’s
Model
Bit accuracy drop
AWT
− ﬁne-tuning
− discriminator
1.93%±0.19
5.21%±0.12
47.92%±0.44
SBERT change
30.77%±1.03↑
14.20%±1.11↑
15.93%±0.94↓
(a) 1 sample
(b) Best of 20 samples
Fig. 9: Random attacks (replacing and removing words) and
denoising attack (applied to noisy text).
TABLE VII: The relative performance of denoising attack
applied to the 1-sample output. The no-attack performance
is in Table I.
85.087.590.092.595.097.5Bit Accuracy (%)0123456SBERT distanceReplaceRemoveDAENo attack75.077.580.082.585.087.5Bit Accuracy (%)0123456SBERT distanceReplaceRemoveDAENo attackwatermark and reduce its decoding accuracy.
is only different
We assume a strong adversary who has full knowledge
about AWT architecture, training details, access to the same
training data, and the granularity of input sentences. In our
threat model, we consider a black-box scenario in which the
adversary can train their own model and use it
to insert
a random watermark into the watermarked text, in hope of
corrupting the original watermark and confusing the decoder.
For completeness, we also show the less realistic white-box
case when the re-watermarking is done using the same model.
To run the black-box attack, we train another model AWTadv
that
in initialization and reaches a com-
parable performance to AWT. We ﬁrst watermark the text
with AWT, then we re-watermark it with a random message
using AWTadv(using the same or a different message was
comparable). We use the message decoder of AWT (i.e., the
ﬁrst model) to decode the re-watermarked text and com-
pute the matching with the original watermarks. As shown
in Table VIII, re-watermarking is stronger than denoising
(Table VII) in decreasing the accuracy, but it also affects
utility and perturbs the text due to double watermarking.
This is in contrast with model watermarking where piracy
can mostly retain the task performance [56]. Also, the new
watermarks did not completely corrupt the original ones (i.e.,
the matching accuracy dropped to ∼85%, while the accuracy
of non-watermarked text is ∼50%). A possible interpretation
is that AWTadv (i.e., another instance) does not necessarily use
the same patterns (e.g., words to be replaced, added words,
and locations) to encode the information and so it does not
completely replace the original changes or confuse the ﬁrst
model’s decoder. We validated this by decoding one model’s
translation by the other model’s decoder (AWT and AWTadv
with no re-watermarking) and the matching accuracy was close
to random chance (51.8% and 53.2%). Our observation that
different models produce different patterns is also consistent
with previous data hiding studies in images (e.g., [27]).
Although the new watermarks in the re-watermarked text
have high matching accuracy by the decoder of AWTadv
(∼96%), the adversary has no strong incentive or evidence to
dispute provenance since 1) human-written text/news is mostly
non-watermarked. 2) the presence of the original watermark by
the decoder of AWT indicates that the text was re-watermarked
because otherwise, it should have a random chance matching.
Finally, in the less realistic white-box case, re-watermarking
with a different message overrides the original watermarks. We
found that this is mainly because the model very often undoes
Attack
Bit accuracy drop
Re-watermarking
De-watermarking
white-box
black-box
white-box
black-box
46.8%±0.46
12.6%±0.38
41.6%±0.34
11.5%±0.32
SBERT change
23.4%±0.45↑
66.1%±1.89↑
55.2%±0.39↓
11.3%±0.53↑
TABLE VIII: The relative performance of adaptive attacks that
are applied to the 1-sample output in the white-box and black-
box (which we mainly consider) settings.
the same changes done by the ﬁrst watermarking step. A more
detailed discussion on re-watermarking is in Appendix VIII-D.
4) De-watermarking: Our last attack assumes that the ad-
versary could use their knowledge about AWT to de-watermark
the text, instead of adding a new watermark. Ideally, training
an inverse de-watermarking model requires paired training
data of the same text before and after watermarking, which
is not feasible in our black-box scenario. To circumvent this,
the adversary might
try to train a denoising autoencoder
(DAEpaired) on the paired data of AWTadv. The DAEpaired takes
the watermarked sentence as an input, with no additional noise,
and should reconstruct the original non-watermarked sentence.
In Table VIII, as a sanity check, we ﬁrst evaluate the white-
box case when the DAEpaired is applied to AWTadv. This signif-
icantly reduced the bit accuracy (dropped to ∼55%) and also
the SBERT distance indicating a successful reconstruction.
This is mainly because the DAEpaired was exposed to the
patterns the model AWTadv frequently uses. In contrast, The
black-box attack is signiﬁcantly less successful (bit accuracy
dropped to ∼86%). However, in terms of the trade-off (i.e.,
decreasing bit accuracy with minimal changes), it may be the
most effective one among the attacks we considered since it
increased SBERT by ∼11%, while re-watermarking increased
it by ∼66% with a comparable drop in accuracy.
The cases where the attack succeeded in the black-box
setting were mainly either: 1) sentences with lower syntactic
correctness or 2) similar changes to AWTadv. Otherwise, the at-
tack was not successful due to the differences between the two
models and the subtle encoding. Further improving the quality
and diversity of watermarks both within and across models
could help to defend against adaptive attacks, we leave that to
future work. A detailed discussion is in Appendix VIII-D.
E. Baselines
In this section, we compare AWT against baselines. First,
we implement a rule-based synonym substitution method
that adopts the method in [20]. Second, as an alternative
to translation-based data hiding, we train an autoregressive
language model, while simultaneously optimizing the message
encoding and decoding.
1) Synonym substitution: The method in [20] uses syn-
onyms from WordNet
to encode binary bits. The authors
relied on ambiguity to make it hard for the adversary to
perform automatic disambiguation. The ambiguity comes from
encoding the message by synonyms that are “homographs”
(having multiple meanings).
We ﬁrst form a list of words (in the dataset vocabulary)
to be replaced by ﬁnding the words that have homographs
Acc.
Model
Synonym 83.28%±0.62
86.3%±0.99
AWT
SBERT
3.62±0.004
0.944±0.02
F1
0.98
0.53
TABLE IX: Comparing AWT and synonym substitution in
terms of bit accuracy, SBERT distance (showing the average
and standard deviation of different runs), and F1 score.
(at least 2) in their synonym sets. We randomly divide each