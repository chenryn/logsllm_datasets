title:To Catch a Predator: A Natural Language Approach for Eliciting Malicious
Payloads
author:Sam Small and
Joshua Mason and
Fabian Monrose and
Niels Provos and
Adam Stubblefield
A Natural Language Approach for Eliciting Malicious Payloads
To Catch a Predator:
Sam Small
Johns Hopkins University
Joshua Mason
Johns Hopkins University
Fabian Monrose
Johns Hopkins University
Niels Provos
Google Inc.
Adam Stubbleﬁeld
Johns Hopkins University
Abstract
We present an automated, scalable, method for craft-
ing dynamic responses to real-time network requests.
Speciﬁcally, we provide a ﬂexible technique based on
natural language processing and string alignment tech-
niques for intelligently interacting with protocols trained
directly from raw network trafﬁc. We demonstrate the
utility of our approach by creating a low-interaction web-
based honeypot capable of luring attacks from search
worms targeting hundreds of different web applications.
In just over two months, we witnessed over 368, 000
attacks from more than 5, 600 botnets targeting several
hundred distinct webapps. The observed attacks included
several exploits detected the same day the vulnerabilities
were publicly disclosed. Our analysis of the payloads of
these attacks reveals the state of the art in search-worm
based botnets, packed with surprisingly modular and di-
verse functionality.
1
Introduction
Automated network attacks by malware pose a signif-
icant threat to the security of the Internet. Nowadays,
web servers are quickly becoming a popular target for
exploitation, primarily because once compromised, they
open new avenues for infecting vulnerable clients that
subsequently visit these sites. Moreover, because web
servers are generally hosted on machines with signiﬁ-
cant system resources and network connectivity, they can
serve as reliable platforms for hosting malware (particu-
larly in the case of server farms), and as such, are entic-
ing targets for attackers [25]. Indeed, lately we have wit-
nessed a marked increase in so-called “search worms”
that seek out potential victims by crawling the results
returned by malevolent search-engine queries [24, 28].
While this new change in the playing ﬁeld has been noted
for some time now, little is known about the scope of this
growing problem.
To better understand this new threat, researchers and
practitioners alike have recently started to move towards
the development of low-interaction, web-based honey-
pots [3]. These differ from traditional honeypots in that
their only purpose is to monitor automated attacks di-
rected at vulnerable web applications. However, web-
based honeypots face a unique challenge—they are in-
effective if not broadly indexed under the same queries
used by malware to identify vulnerable hosts. At the
same time, the large number of different web applica-
tions being attacked poses a daunting challenge, and the
sheer volume of attacks calls for efﬁcient solutions. Un-
fortunately, current web-based honeypot projects tend
to be limited in their ability to easily simulate diverse
classes of vulnerabilities, require non-trivial amounts of
manual support, or do not scale well enough to meet this
challenge.
A fundamental difference between the type of mal-
ware captured by traditional honeypots (e.g., Hon-
eyd [23]) and approaches geared towards eliciting pay-
loads from search-based malware stems from how poten-
tial victims are targeted. For traditional honeypots, these
systems can be deployed at a network telescope [22], for
example, and can simply take advantage of the fact that
for random scanning malware, any trafﬁc that reaches
the telescope is unsolicited and likely malicious in na-
ture. However, search-worms use a technique more akin
to instantaneous hit-list automation, thereby only target-
ing authentic and vulnerable hosts. Were web-based hon-
eypots to mimic the passive approach used for traditional
honeypots, they would likely be very ineffective.
To address these limitations, we present a method for
crafting dynamic responses to on-line network requests
using sample transcripts from observed network inter-
action.
In particular, we provide a ﬂexible technique
based on natural language processing and string align-
ment techniques for intelligently interacting with proto-
cols trained directly from raw trafﬁc. Though our ap-
proach is application-agnostic, we demonstrate its util-
ity with a system designed to monitor and capture au-
tomated network attacks against vulnerable web appli-
cations, without relying on static vulnerability signa-
tures. Speciﬁcally, our approach (disguised as a typical
web server) elicits interaction with search engines and,
in turn, search worms in the hope of capturing their il-
licit payload. As we show later, our dynamic content
generation technique is fairly robust and easy to deploy.
Over a 72-day period we were attacked repeatedly, and
witnessed more than 368,000 attacks originating from
28,856 distinct IP addresses.
The attacks target a wide range of web applications,
many of which attempt to exploit the vulnerable appli-
cation(s) via a diverse set of injection techniques. To
our surprise, even during this short deployment phase,
we witnessed several attacks immediately after public
disclosure of the vulnerabilities being exploited. That,
by itself, validates our technique and underscores both
the tenacity of attackers and the overall pervasiveness
of web-based exploitation. Moreover, the relentless na-
ture of these attacks certainly sheds light on the scope of
this problem, and calls for immediate solutions to better
curtail this increasing threat to the security of the Inter-
net. Lastly, our forensic analysis of the captured pay-
loads conﬁrms several earlier ﬁndings in the literature, as
well as highlights some interesting insights on the post-
infection process and the malware themselves.
The rest of the paper is organized as follows. Sec-
tion 2 discusses related work. We provide a high-level
overview of our approach in Section 3, followed by
speciﬁcs of our generation technique in Section 4. We
provide a validation of our approach based on interaction
with a rigid binary protocol in Section 5. Additionally,
we present our real-world deployment and discuss our
ﬁndings in Section 6. Finally, we conclude in Section 7.
2 Related Work
Generally speaking, honeypots are deployed with the in-
tention of eliciting interaction from unsuspecting adver-
saries. The utility in capturing this interaction has been
diverse, allowing researchers to discover new patterns
and trends in malware propagation [28], generate new
signatures for intrusion-detection systems and Internet
security software [16, 20, 31], collect malware binaries
for static and/or dynamic analysis [21], and quantify ma-
licious behavior through widespread measurement stud-
ies [26], to name a few.
The adoption of virtual honeypots by the security com-
munity only gained signiﬁcant traction after the introduc-
tion of low-interaction honeypots such as Honeyd [23].
Honeyd is a popular tool for establishing multiple virtual
hosts on a single machine. Though Honeyd has proved
to be fairly useful in practice, it is important to recognize
that its effectiveness is strictly tied to the availability of
accurate and representative protocol-emulation scripts,
whose generation can be fairly tedious and time con-
suming. High-interaction honeypots use a different ap-
proach, replying with authentic and unscripted responses
by hosting sand-boxed virtual machines running com-
mon software and operating systems [11]1.
A number of solutions have been proposed to bridge
the separation of beneﬁts and restrictions that exist be-
tween high and low-interaction honeypots. For example,
Leita et al. proposed ScriptGen [18, 17], a tool that auto-
matically generates Honeyd scripts from network trafﬁc
logs. ScriptGen creates a ﬁnite state machine for each
listening port. Unfortunately, as the amount and diver-
sity of available training data grows, so does the size and
complexity of its state machines. Similarly, RolePlayer
(and its successor, GQ [10]) generates scripts capable of
interacting with live trafﬁc (in particular, worms) by ana-
lyzing series of similar application sessions to determine
static and dynamic ﬁelds and then replay appropriate re-
sponses. This is achieved by using a number of heuris-
tics to remove common contextual values from annotated
trafﬁc samples and using byte-sequence alignment to ﬁnd
potential session identiﬁers and length ﬁelds.
While neither of these systems speciﬁcally target
they represent germane ap-
search-based malware,
proaches and many of the secondary techniques they in-
troduce apply to our design as well. Also, their respective
designs illustrate an important observation—the choice
between using a small or large set of sample data man-
ifests itself as a system tradeoff: there is little diversity
to the requests recognized and responses transmitted by
RolePlayer, thereby limiting its ability to interact with
participants whose behavior deviates from the training
session(s). On the other hand, the ﬂexibility provided by
greater state coverage in ScriptGen comes at a cost to
scalability and complexity.
Lastly, since web-based honeypots rely on search en-
gines to index their attack signatures, they are at a disad-
vantage each time a new attack emerges. In our work, we
sidestep the indexing limitations common to static signa-
ture web-based honeypots and achieve broad query rep-
resentation prior to new attacks by proactively generating
“signatures” using statistical language models trained on
common web-application scripts. When indexed, these
signatures allow us to monitor attack behavior conducted
by search worms without explicitly deploying structured
signatures a priori.
3 High-level Overview
We now brieﬂy describe our system architecture.
Its
setup follows the description depicted in Figure 1, which
is conceptually broken into three stages: pre-processing,
Figure 1: Setup consists of three distinct stages conducted in tandem in preparation for deployment.
classiﬁcation, and language-model generation. We ad-
dress each part in turn. We note that although our
methodology is not protocol speciﬁc, for pedagogical
reasons, we provide examples speciﬁc to the DNS pro-
tocol where appropriate. Our decision to use DNS for
validation stems from the fact that validating the correct-
ness of an HTTP response is ill-deﬁned. Likewise, many
ASCII-based protocols that come to mind (e.g., HTTP,
SMTP, IRC) lack strict notions of correctness and so
do not serve as a good conduit to demonstrate the cor-
rectness of the output we generate.
To begin, we pre-process and sanitize all trace data
used for training. Network traces are stripped of trans-
port protocol headers and organized by session into pairs
of requests and responses. Any trace entries that cor-
respond to protocol errors (e.g., HTTP 404) are omit-
ted. Next, we group request and response pairs using
a variant of iterative k-means clustering with TF/IDF
(i.e., term frequency-inverse document frequency) co-
sine similarity as our distance metric. Formally, we
apply a k-medoids algorithm for clustering, which as-
signs samples from the data as cluster medoids (i.e., cen-
troids) rather than numerical averages. For reasons that
should become clear later, pair similarity is based solely
on the content of the request samples. Upon completion,
we then generate and train a collection of smoothed n-
gram language-models for each cluster. These language-
models are subsequently used to produce dynamic re-
sponses to online requests. However, because message
formats may contain session-speciﬁc ﬁelds, we also post-
process responses to satisfy these dependencies when-
ever they can be automatically inferred. For example, in
DNS, a session identiﬁer uniquely identiﬁes each record
request with its response.
During a live deployment, online-classiﬁcation is used
to deduce the response that is most similar to the in-
coming request (i.e., by mapping the response to its best
medoid). For instance, a DNS request for an MX record
will ideally match a medoid that maps to other MX re-
quests. The medoid with the minimum TF/IDF distance
to an online request identiﬁes which language model is
used for generating responses. The language models are
built in such a way that they produce responses inﬂu-
enced by the training data. The overall process is de-
picted in Figure 2. For our evaluation as a web-based
honeypot (in Section 6), this process is used in two dis-
tinct stages: ﬁrst, when interacting with search engines
for site indexing and second, when courting malware.
4 Under the Hood
In what follows, we now present more speciﬁcs about
our design and implementation. Recall that our goal is
to provide a technique for automatically providing valid
responses to protocols interactions learned directly from
raw trafﬁc.
In lieu of semantic knowledge, we instead apply clas-
sic pattern classiﬁcation techniques for partitioning a set
of observed requests. In particular, we use the iterative
k-medoids algorithm. As our distance metric we choose
to forgo byte-sequence alignment approaches that have
been previously used to classify similarities between pro-
tocol messages (e.g, [18, 9, 6]). As Cui et. al. observed,
while these approaches are appropriate for classifying
requests that only differ parametrically, byte-sequence
alignment is ill-suited for classifying messages with dif-
ferent byte-sequences [8]. Therefore, we use TF/IDF
cosine similarity as our distance metric.
Intuitively,
term frequency-inverse document
fre-
quency (TF/IDF) is the measure of a term’s signiﬁcance
to a string or document given its signiﬁcance among a set
of documents (or corpus). TF/IDF is often used in infor-
mation retrieval for a number of applications including
automatic text retrieval and approximate string match-
ing [29]. Mathematically, we compute TF/IDF in the
following way: let τdi denote how often the term τ ap-
pears in document di such that di ∈ D, a collection of
documents. Then TF/IDF = TF · IDF where
Pre-ProcessingCollect and encode network samples Split samples into pairs of requests and corresponding responsesTrain TF-IDF with corpus of requestsCluster requests with iterative k-medoids using TF-IDF distanceMerge similar clusters (rejoin medoids and discard clusters)Build language model for each medoidTrain language model with corresponding responsesFind appropriate cluster for each requestClassiﬁcationLanguage-Model Generation12345678Figure 2: Online classiﬁcation of requests from malware and search engine spiders inﬂuences which language model
is selected for response generation.
√
τdi
TF =
and
s
IDF =
log
|D|
|{dj : dj ∈ D and τ ∈ dj}|
The term-similarity between two strings from the
same corpus can be computed by calculating their
TF/IDF distance. To do so, both strings are ﬁrst rep-
resented as multi-dimensional vectors. For each term in
a string (e.g., word), its TF/IDF value is computed as
described previously. Then, for a string with n terms, an
n-dimensional vector is formed using these values. The
cosine of the angle between two such vectors represent-
ing strings indicates a measure of their similarity (hence,
its complement is a measure of distance).
In the context of our implementation, terms are delin-
eated by tokenizing requests into the following classes:
one or more spaces, one or more printable characters (ex-
cluding spaces), and one or more non-printable charac-