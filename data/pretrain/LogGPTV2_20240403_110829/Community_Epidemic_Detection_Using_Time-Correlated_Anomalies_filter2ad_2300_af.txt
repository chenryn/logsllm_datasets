0
0
0
.
e
r
u
s
a
e
M
1
F
0
1
.
8
0
.
6
0
.
4
0
.
2
0
.
Adium
Camino
Mail
TextEdit
Mail / mailspam
TextEdit / prompttext
Adium / screenshot
Camino / showpages
s
e
c
n
e
u
q
e
S
e
u
q
n
U
i
f
o
n
o
i
t
c
a
r
F
0.0
0.2
0.4
0.6
0.8
1.0
0
5
10
15
20
Fraction of Training Set
Infected Clients (d)
Fig. 11. The applications generate new
sequences throughout training, with oc-
casional bursts (e.g., program launches)
Fig. 12. F1 measure with n = 100 and
varying infection size (d) using each of the
four pairs of programs and exploits
e
r
u
s
a
e
M
1
F
0
1
.
8
0
.
6
0
.
.
4
0
.
2
0
e
r
u
s
a
e
M
1
F
8
0
.
.
4
0
0
0
.
d =
1
2
4
6
8
10
f=0.5
f=0.2
f=0.1
f=0.05
20
50
100
200
500
1000
2000
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
Community Size (n)
Threshold Multiplier k (V=mean+k*sd)
Fig. 13. F1 measure with varying com-
munity size and constant fraction f = d/n
infected, using TextEdit and prompttext
Fig. 14. F1 measure with n = 100 and
varying threshold multiplier using traces
from Mail and the mailspam exploit
Adium, then takes a snapshot of the current display (like prompttext but with-
out user interaction); and showpages infects Camino, then loads a series of web
pages (based on HTML proxies like Sobig’s trojan, DoS payloads like Code Red
and Yaha, and self-updating payloads like W32/sonic and W32/hybris [6]).
Except where noted, we gathered data using an Apple MacPro with two 2.66
GHz Dual-Core Intel Xeons and 6 GB of memory running Mac OS X 10.5.4,
and the results we present are representative. Using the resulting model, we
compute the distribution X of healthy client anomaly scores for each program
(Figure 10). The results of Section 5.2 show that behavioral variance comes from
client behavior over time, rather than client heterogeneity; the smartest way to
gather a good data set was, therefore, to monitor a single client for a long time.
Section 6.4 provides experiments supporting the merit of that decision.
We use the phrase “normal usage” to mean that no artiﬁcial workloads were
generated nor were certain activities prescribed. As is evident from the rate of
new sequences seen during training, plotted in Figure 11, we made no eﬀort
to train until convergence, nor to exercise the rarer features of these programs.
We also do not separate sequences by thread, instead ordering them strictly
by time of invocation. The resulting models are therefore small, imprecise, and
incomplete, as we might expect to achieve in practice; the Syzygy performance
numbers we present would only improve with better models.
6.2 Detection Performance
We ﬁrst consider Syzygy’s ability to detect epidemics for various sizes of com-
munity and infected population. Consider the experiments plotted in Figure 12
376
A.J. Oliner, A.V. Kulkarni, and A. Aiken
wherein a ﬁxed-size community is being infected. Syzygy’s performance improves
with infection size, peaking, in this experiment, at around 10 exploited clients
(10% of the community). Figure 13 shows, however, that with a suﬃciently
large community we require a vanishingly small fraction of the population to be
sacriﬁced before we detect the exploit. Although the community and infected
population are growing at the same rate, Syzygy’s ability to detect the infection
outpaces that growth.
6.3 Parameter Sensitivity
We next evaluate Syzygy’s sensitivity to the threshold V . Figure 14 shows per-
formance for various choices of V . Once the community and infected population
are suﬃciently large, we see the performance curve reach a maximum at a point
between V = μX and μY . Increasing the multiplier tends to increase precision,
decrease recall, and decrease the false positive rate (which falls oﬀ like the tail of
the normal distribution). To further visualize this, see Figure 15. As the number
of clients grows, the normal and infected distributions become more clearly sep-
arated. This increasing noise margin suggests that the exact placement of the
threshold does not strongly aﬀect Syzygy’s performance. Indeed, in the limit, all
choices of threshold μX  0.6 even when δ = 0.1, n = 10, and d = 1.
We now consider scenario (ii), limiting bad behavior to a ﬁxed rate. Speciﬁ-
cally, if the exploit spreads bad behavior out over time, in bursts that cumula-
tively account for a fraction r of the runtime per client, such that the community
signal does not deviate above μX + V , no epidemic will be reported. Mathemat-
ically, this attack corresponds to decreasing the eﬀective infection size from d to
dr. This, in itself, may be considered a victory under certain circumstances, such
as when a worm may be contained so long as it does not spread too quickly [42].
In our experiment, we splice windows of infected anomaly scores into sequences
of healthy anomaly scores, in proportions determined by the rate r. Figure 18
shows how Syzygy performs against this rate-limiting attack. Again, false neg-
atives dominate the metric—with a better-chosen V , we can get F1 above 0.68
at r = 0.05 with as few as 10 clients.
7 Scalability
Mathematically, Syzygy’s accuracy improves as the community grows, so it is
crucial that the implementation scales well. This issue is independent of the anal-
ysis in Section 3. We described the infrastructure as using a central server, and
demonstrated that it works for as many as 35 clients (Section 5). Communication
is one-way (client to server) and there is no consensus or agreement protocol, so
the total community traﬃc scales linearly with the number of clients.
This central server may be replaced, however, with alternatives that would
increase scalability and avoid a single point of failure. One option is a server hi-
erarchy; each server computes the community score for its children and reports
Community Epidemic Detection Using Time-Correlated Anomalies
379
this value and the size of that sub-community to a parent server. This arrange-
ment works precisely because the function used to compute the community score,
mean(), is associative (when weighted by sub-community size).
In addition to communication overhead, there is monitoring overhead on the
clients. This is typically a consequence of model choice and unaﬀected by com-
munity size. In our controlled experiments, the primary monitoring tool, dtrace,
required less than 10% of one CPU even during heavy activity by the monitored
application; the average usage was below 1%. In our deployment experiments
with Firefox, Syzygy required less than 5% of the CPU on average, and 7% peak,
including strace overhead (see Section 5.3). Using our strace-based implemen-
tation for Windows, however, the slowdown was noticeable. The overhead in our
Apache deployment (see Section 4), which took advantage of the web server’s
built-in logging mechanism, was negligible. If overhead becomes problematic,
then it may be worth changing the model to measure less costly signals. For
example, Sharif et al [30] implemented control-ﬂow monitoring with overhead
comparable to our system call-based approach—this optimization would likely
yield greater precision at lower overhead.
8 Contributions
Syzygy is an epidemic detection framework that looks for time-correlated anoma-
lies in a homogeneous software community—precisely the behavior that would
accompany an exploit as it executes among a set of clients. Our results show
that Syzygy is eﬀective at automated detection of epidemics, is practical to de-
ploy, and scales well. Syzygy takes advantage of the statistical properties of large
communities in a novel way, asymptotically approaching perfect detection.
Acknowledgments
The authors thank the members of the VERNIER team, especially Elizabeth
Stinson, Patrick Lincoln, Steve Dawson, Linda Briesemeister, Jim Thornton,
John Mitchell, and Peter Kwan. Thanks to Sebastian Gutierrez and Miles Davis
for help deploying Syzygy, to Naeim Semsarilar for his invaluable contributions
to the early stages of this work, and to Xuˆan V˜u for her input and support.
References
[1] Bouloutas, A., Calo, S., Finkel, A.: Alarm correlation and fault identiﬁcation in
communication networks. IEEE Transactions on Communications (1994)
[2] Brumley, D., Newsome, J., Song, D.: Sting: An end-to-end self-healing system for
defending against internet worms. In: Malware Detection and Defense (2007)
[3] Costa, M., Crowcroft, J., Castro, M., Rowstron, A., Zhou, L., Zhang, L., Barham,
P.: Vigilante: End-to-end containment of internet worms. In: SOSP (2005)
[4] Cuppens, F., Miege, A.: Alert correlation in a cooperative intrusion detection
framework. In: IEEE Symposium on Security and Privacy, pp. 202–215 (2002)
380
A.J. Oliner, A.V. Kulkarni, and A. Aiken
[5] Debar, H., Becker, M., Siboni, D.: A neural network component for an intrusion
detection system. In: IEEE Symposium on Security and Privacy (1992)
[6] Ellis, D.: Worm anatomy and model. In: WORM (2003)
[7] Eskin, E.: Anomaly detection over noisy data using learned probability distribu-
tions. In: ICML (2000)
[8] Feng, H.H., Kolesnikov, O.M., Fogla, P., Lee, W., Gong, W.: Anomaly detection
using call stack information. In: IEEE Symposium on Security and Privacy (2003)
[9] Forrest, S., Hofmeyr, S.A., Somayaji, A., Longstaﬀ, T.A.: A sense of self for unix
processes. In: IEEE Symposium on Security and Privacy (1996)
[10] Gao, D., Reiter, M.K., Song, D.: Gray-box extraction of execution graphs for
anomaly detection. In: CCS (2004)
[11] Gao, D., Reiter, M.K., Song, D.: Behavioral distance for intrusion detection. In:
Zamboni, D., Kr¨ugel, C. (eds.) RAID 2006. LNCS, vol. 4219, pp. 19–40. Springer,
Heidelberg (2006)
[12] Giﬃn, J.T., Jha, S., Miller, B.P.: Detecting manipulated remote call streams. In:
USENIX Security, pp. 61–79 (2002)
[13] Gu, G., C´ardenas, A.A., Lee, W.: Principled reasoning and practical applications
of alert fusion in intrusion detection systems. In: ASIACCS (2008)
[14] Hofmeyr, S.A., Forrest, S., Somayaji, A.: Intrusion detection using sequences of
system calls. Journal of Computer Security 6(3), 151–180 (1998)
[15] Huang, L., Garofalakis, M., Joseph, A.D., Taft, N.: Communication-eﬃcient track-
ing of distributed cumulative triggers. In: Intl. Conf. on Distributed Computing
Systems (ICDCS) (June 2007)
[16] Huang, L., Nguyen, X.L., Garofalakis, M., Hellerstein, J., Jordan, M., Joseph, A.,
Taft, N.: Communication-eﬃcient online detection of network-wide anomalies. In:
IEEE INFOCOM (2007)
[17] Jakobson, G., Weissman, M.: Alarm correlation. IEEE Network (1993)
[18] Javitz, H.S., Valdes, A.: The SRI IDES statistical anomaly detector. In: IEEE
Symposium on Security and Privacy (1991)
[19] King, S.T., Mao, Z.M., Lucchetti, D.G., Chen, P.M.: Constructing attack scenarios
through correlation of intrusion alerts. In: CCS (2002)
[20] Lincoln, P., et al.: Virtualized Execution Realizing Network Infrastructures En-
hancing Reliability (VERNIER), http://www.sdl.sri.com/projects/vernier/
[21] Locasto, M.E., Sidiroglou, S., Keromytis, A.D.: Software self-healing using collab-
orative application communities. In: NDSS (2005)
[22] Malan, D.J., Smith, M.D.: Host-based detection of worms through peer-to-peer
cooperation. In: ACM Workshop on Rapid Malcode (2005)
[23] Malan, D.J., Smith, M.D.: Exploiting temporal consistency to reduce false posi-
tives in host-based, collaborative detection of worms. In: WORM (2006)
[24] Mutz, D., Valeur, F., Vigna, G., Kruegel, C.: Anomalous system call detection.
In: TISSEC (2006)
[25] Newsome, J., Brumley, D., Song, D.: Vulnerability-speciﬁc execution ﬁltering for
exploit prevention on commodity software. In: NDSS (2006)
[26] Ning, P., Cui, Y., Reeves, D.S.: Constructing attack scenarios through correlation
of intrusion alerts. In: CCS (2002)
[27] Paxson, V.: Bro: a system for detecting network intruders in real-time. Computer
Networks 31 (1999)
[28] Porras, P.A., Neumann, P.G.: Emerald: event monitoring enabling responses
to anomalous live disturbances. In: National Computer Security Conference,
NIST/NCSC (1997)
[29] Sebring, M.M., Whitehurst, R.A.: Expert systems in intrusion detection: a case
study. In: National Computer Security Conference (1988)
Community Epidemic Detection Using Time-Correlated Anomalies
381
[30] Sharif, M., Singh, K., Giﬃn, J., Lee, W.: Understanding precision in host based
intrusion detection. In: Kruegel, C., Lippmann, R., Clark, A. (eds.) RAID 2007.
LNCS, vol. 4637, pp. 21–41. Springer, Heidelberg (2007)
[31] Smaha, S.: Haystack: an intrusion detection system. In: Aerospace Computer Se-
curity Applications Conference (1988)
[32] Staniford-chen, S., Cheung, S., Crawford, R., Dilger, M., Frank, J., Hoagl, J.,
Levitt, K., Wee, C., Yip, R., Zerkle, D.: Grids—a graph based intrusion detection
system for large networks. In: NIST/NCSC (1996)
[33] Tan, K.M.C., Maxion, R.A.: “Why 6?” Deﬁning the operational limits of stide, an
anomaly-based intrusion detector. In: IEEE Symposium on Security and Privacy
(2002)
[34] Ullrich, J.: DShield—distributed intrusion detection system,
http://www.dshield.org
[35] Vaccaro, H., Liepins, G.: Detection of anomalous computer session activity. In:
IEEE Symposium on Security and Privacy (1989)
[36] Valdes, A., Skinner, K.: Probabilistic alert correlation. In: Lee, W., M´e, L., Wespi,
A. (eds.) RAID 2001. LNCS, vol. 2212, p. 54. Springer, Heidelberg (2001)
[37] Wadge, W.W., Ashcroft, E.A.: Lucid, the dataﬂow programming language.
A.P.I.C. Studies in Data Processing (1985)
[38] Wagner, D., Soto, P.: Mimicry attacks on host-based intrusion detection systems.
In: CCS (2002)
[39] Wang, H.J., Platt, J.C., Chen, Y., Zhang, R., Wang, Y.-M.: Automatic miscon-
ﬁguration troubleshooting with PeerPressure. In: OSDI (2004)
[40] Weaver, N., Paxson, V., Staniford, S., Cunningham, R.: A taxonomy of computer
worms. In: WORM (2003)
[41] Weaver, N., Staniford, S., Paxson, V.: Very fast containment of scanning worms.
In: USENIX Security (2004)
[42] Williamson, M.M.: Throttling viruses: Restricting propagation to defeat malicious
mobile code. In: ACSAC (2002)
[43] Xie, Y., Kim, H., O’Hallaron, D., Reiter, M., Zhang, H.: Seurat: a pointillist
approach to anomaly detection. In: Jonsson, E., Valdes, A., Almgren, M. (eds.)
RAID 2004. LNCS, vol. 3224, pp. 238–257. Springer, Heidelberg (2004)
[44] Yegneswaran, V., Barford, P., Jha, S.: Global intrusion detection in the DOMINO
overlay system. In: NDSS (2004)