Cache
172.16.2.10
BIND9
Cache
172.16.2.11
DJBDNS W2000
Cache
172.16.2.13
Cache
172.16.2.12
W2003
Cache
172.16.2.14
CNS
Cache
172.16.2.15
User
172.16.2.6
Fig. 3. Network setup for DNS tests. The “User” system sends DNS queries to only one of the
caches in each test. The cache recursively resolves queries for the user by talking to the authoritative
root, TLD, and SLD servers. For some tests we introduce wide-area packet loss and transmission
delays with FreeBSD’s dummynet.
Networking and Addressing.
It is somewhat of a stretch to replicate the massive
DNS system with only ﬁve computers. We use a few tricks to make this a little more
realistic. We gave the root, TLD, and SLD authoritative servers multiple IP addresses
(13, 254, and 254, correspondingly), bound to the loopback interface. Thus, each server’s
external interface acts like a router—a way to reach the loopback addresses. The caching
nameserver, and the User system have one address each. All systems are attached to a
single 100baseTX Ethernet segment. We use FreeBSD’s Dummynet feature to introduce
simulated packet loss and delays for some tests.
Trace Data. To drive the simulation, we created a trace ﬁle of DNS requests by collecting
hostnames from 12 hours of the IRCache HTTP proxies [8] data. The trace ﬁle contains
5,532,641 DNS requests for 107,777 unique hostnames. There are 70,365 unique second-
level zones, and 431 top-level zones (many of them bogus). The trace contains many
repeated queries as necessary to test the caching part of the DNS cache.
We also take timestamps from the proxy logs and replay the trace at the same rate,
preserving as closely as possible the time between consecutive queries. Thus, each test
run takes 12 hours to complete.
Zone Files. Our scripts generate BIND zone ﬁles based on the contents of the trace ﬁle.
We do not generate zones for bogus TLDs. Thus, we can also test negative caching.
Measurements and Laboratory Simulations of the Upper DNS Hierarchy
153
For the root and TLD zones, we try to mimic reality as closely as possible. For
example, we use the same number of nameserver IP addresses and the same TTLs for
ns and glue records.4 To get the real values, we issued the necessary queries out to the
Internet while building the zone ﬁle data.
We also try to match our simulated SLD parameters to reality. There are too many
SLD zones (more than 100,000) and querying all of them would take much too long.
Instead, we obtained the necessary values (the number of a records per name and the
TTLs for a, ns, and cname records) from a random sample of 5000 domains and genera-
ted similar random values for the SLD zones in the trace. Unlike reality, each simulated
SLD zone has only two ns records. We also determined that about 35% of the names in
the trace ﬁle actually point to cname records, and our simulated zone data mimics this
as well.
Tested Conﬁgurations. We tested the following six different DNS caches using their
default conﬁguration parameters.
1. BIND 8.4.3
2. BIND 9.2.1
3. dnscache 1.05, aka DJBDNS with CACHESIZE set to 100,000,000
4. Microsoft Windows 2000 v5.0.49664
5. Microsoft Windows 2003 v5.2.3790.0
6. CNS 1.2.0.3
For each cache, we ran tests for six different network conﬁgurations:
1. No delays, no loss
2. 100 millisecond delays, no loss
3. linear delays, no loss
4. linear delays, 5% loss
5. linear delays, 25% loss
6. 100% loss
The no-delay/no-loss conﬁguration is simple, but unrealistic. For most real users,
the root/TLD/SLD servers are between 30 and 200 milliseconds away (cf. Table 1). The
100ms-delay/no-loss test uses a constant 100-millisecond delay to each root/TLD/SLD
server, but with no packet loss. It is also somewhat unrealistic.
In the next three conﬁgurations, with so-called linear delays, a nameserver’s latency
(in ms) is proportional to its order n in the nameserver list: τ = 30 + 10n. For zones
such as the root and com, which have 13 nameservers, the last one is 160 milliseconds
away from the cache. This arrangement provides some pseudo-realistic diversity and
allows us to see how often a DNS cache actually selects a nameserver with the best
response time. We believe the linear-delay/5%-loss test to be the most realistic among
all six conﬁgurations.
The ﬁnal conﬁguration has 100% packet loss. This test mimics a situation when a
DNS cache cannot communicate with any authoritative nameservers. Reasons for such
non-communication include ﬁrewalls, packet ﬁlters, unroutable source addresses, and
4 Glue records are, essentially, a records for nameservers of delegated zones.
154
D. Wessels et al.
saturated network connections. Live measurements showed that when the cache’s queries
reach an authoritative nameserver, but the replies do not make it back, the DNS trafﬁc
increases (cf. Figure 2).
3.2 Results
General statistics. Figure 4 shows how many queries each DNS cache sent in various
tests. For example, the leftmost bar shows that in the no-delay/no-loss test, BIND8 sent
a small number of queries to the roots, about 400,000 queries to the TLDs, and about
600,000 to the SLDs. Its total query count is just over 1,000,000.
5
4
1
2
3
Roots
TLDs
SLDs
1 − no delay, no loss
2 − 100ms delay, no loss
3 − linear delay, no loss
4 − linear delay, 5% loss
5 − linear delay, 25% loss
5
4
3
1
2
5
5
4
2
3
1
4
2
3
1
5
4
1
2
3
5
4
2
3
1
t
n
u
o
C
y
r
e
u
Q
e
v
i
t
l
a
u
m
u
C
1.2e+06
1e+06
800000
600000
400000
200000
0
bind8
bind9
djbdns
w2000
w2003
cns
Fig. 4. Cumulative query counts for all caches.
BIND8 always sends more queries than any of the other caches, primarily because
it sends three queries (a, a6, and aaaa) to the roots/TDLs/SLDs for each of the expired
nameserver addresses for a given zone. Recall that every SLD zone in our model has
two nameservers, while root and TLD zones usually have more. This result implies that
the number of nameservers and their address record TTLs can have a signiﬁcant impact
on DNS trafﬁc.
The BIND9 results show the largest percentage of root server queries. For the no-
loss tests, BIND9 sends 55,000 queries to the roots, versus only 1800 for BIND8. The
reason for this is because BIND9 always starts at the root when querying for expired
nameserver addresses and sends an a and a6 query for each nameserver.
The DJBDNS data also shows a relatively high fraction of root server queries, which
is about half of the BIND9 numbers. DJBDNS also starts at the root when refreshing
expired nameserver addresses. However, it only sends a single a query for one of the
nameservers, not all of them.
The two Windows results are very similar, with slightly more queries from W2003.
These two send the fewest overall number of queries for the ﬁve test cases shown.
Measurements and Laboratory Simulations of the Upper DNS Hierarchy
155
CNS ﬁts in between DJBDNS and BIND9 in terms of total number of queries. Due
to its low root server query count, we can conclude that it does not start at the root for
expired nameserver addresses. Also note that, like BIND8, CNS sends slightly fewer
queries for the 100-millisecond delay test (#2), than it does for the no-delay test (#1).
Root Server Query Counts. Table 4 shows counts of queries to the roots in each test.
As discussed above, except for the 100% loss tests, BIND9 and DJBDNS hit the roots
much harder than the others. The BIND8 numbers would probably be closer to Windows
if it did not send out a6 and aaaa, in addition to a queries.
Table 4. Number of Messages sent to roots.
linear
linear
none
linear
5% 25%
none 100ms
Delays
Pkt Loss
none
100%
none
BIND8
1,826 1,876 1,874 1,899 2,598 37,623,266
BIND9
55,329 55,260 55,256 59,222 99,422 2,564,646
DJBDNS 24,328 27,646 27,985 30,341 44,503 12,155,335
W2000
727 1,020 66,272,276
W2003
709 1,009 39,916,750
CNS
975 1,179 12,456,959
622
693
824
657
669
831
663
666
924
The 100% loss tests are very interesting. Our trace contains 5,500,000 hostnames and
that is how many queries the fake user sends to the cache. Except for BIND9, all other
caches become very aggressive when they do not get any root server responses. They
send out more queries than they receive. On one hand this is understandable because there
are 13 root nameservers to reach. However, it is appalling that an application increases
its query rate when it can easily detect a communication failure. The worst offender,
W2000, actually ampliﬁes the query stream by more than order of magnitude.
BIND9 is the exception in the 100% loss tests. It actually attenuates the client’s query
stream, but only by about half. BIND9 is the only DNS caching software that has a nifty
feature: it avoids repeat queries for pending answers. For example, if user sends two
back-to-back queries (with different query-IDs) for www.example.com,5 most caching
resolvers will forward both queries. BIND9, however, forwards only one query. When
the authoritative answer comes back, it sends two answers to the user.
Root/COM Nameserver Distribution. Table 5 shows how the caches distribute their
queries to the root (lighter bars) and to the com TLD server (darker bars) for four of the
test cases. BIND8 almost uses a single server exclusively in the no-delay/no-loss test.
Since that server always answers quickly, there is no need to try any of the others. In the
100-millisecond delay test, however, BIND8 hits all servers almost uniformly. The test
with linear delays and 5% loss has an exponential-looking shape. The ﬁrst nameservers
5 Of course the answer must not already be cached.
156
D. Wessels et al.
Table 5. Distribution of queries to root and com TLD nameservers, showing how the caching
nameserver distributed its queries in each test. The upper, lighter bars show the histogram for root
nameservers. The lower, darker bars show the histogram for the com TLD nameservers.
no delay
no loss
100ms delay linear delay
no loss
5% loss
100% loss
no delay
no loss
100ms delay linear delay
no loss
5% loss
100% loss
I
B
N
D
8
I
B
N
D
9
D
J
B
D
N
S
W
2
0
0
0
W
2
0
0
3
C
N
S
have the lowest delays, and, unsurprisingly, they receive the most queries. The 100% loss
test is odd because, for some reason, ﬁve of the servers receive twice as many queries
as the others.
The BIND9 graphs look very nice. The two no-loss tests, and the 100%-loss test,
show a very uniform distribution. Also, the linear-delay/5%-loss test yields another
exponential-looking curve, which is even smoother than the one for BIND8.
DJBDNS shows a uniform distribution for all tests. The software does not try to ﬁnd
the best server based on delays or packet loss. This is an intentional design feature.6
Windows 2000 has very poor server selection algorithms, as evidenced by the histo-
grams for the 100ms-delay and linear-delay tests. It selects an apparently random server
and continues using it. In the 100%-loss test it did query all roots, except the second
one for some reason. The Windows 2003 DNS cache is somewhat better, but also shows
strange patterns.
CNS also demonstrated odd server selections. To its credit, the linear-delay/5%-
loss case looks reasonable, with most of the queries going to the closest servers. In the
100%-loss test, the selection is almost uniform, but not quite.
4 Conclusion
Our laboratory tests show that caching nameservers use very different approaches to
distribute the query load to the upper levels. Both versions of BIND favor servers with
lower round-trip times, DJBDNS always uses a uniform distribution, Windows 2000
locks on to a single, random nameserver, and Windows 2003 shows an odd, unbalanced
distribution. BIND9 and DJBDNS hit the roots much harder than other caches to avoid
certain cache poisoning scenarios. BIND8 and BIND9’s optimism in looking for IPv6
addresses results in a signiﬁcant amount of unanswerable queries.
DNS zone administrators should understand that their choice of TTLs affects the
system as a whole, rather than their own nameservers. For example, a BIND9 cache
6 See http://cr.yp.to/djbdns/notes.html
Measurements and Laboratory Simulations of the Upper DNS Hierarchy
157
sends two root server queries each time a nameserver address expires. Popular sites can
help reduce global query load by using longer TTLs.
Both laboratory tests and live measurements show that caching resolvers become
very aggressive when cut off from the DNS hierarchy. We believe that resolvers should
implement exponential backoff algorithms when a nameserver stops responding. We
feel strongly that more caching implementations should adopt BIND9’s feature that
avoids forwarding duplicate queries. Other implementations should employ DJBDNS’s
minimalist strategy of sending only a single a query for expired nameserver glue.
Acknowledgments. Support for this work is provided by WIDE project
(http://www.wide.ad.jp) and by DARPA grant 66001-01-1-8909.
References
1. Albitz, P., Liu, C.: DNS and BIND. O’Reilly and Associates (1998)
2. RSSAC: Root servers locations (2004) http://www.root-servers.org/.
3. Brownlee, N.: NeTraMet - a Network Trafﬁc Flow Measurement Tool (2002)
http://www.caida.org/tools/measurement/netramet/.
4. Jung, J., Sit, E., Balakrishnan, H., Morris, R.: DNS Performance and the Effectiveness of
Caching. In: ACM SIGCOMM Internet Measurement Workshop. (2001)
5. Keys, K.: Clients of dns root servers (private communication) (2002)
http://www.caida.org/˜kkeys/dns/2002-08-28/.
6. T. Lee, B. Huffaker, M. Fomenkov, kc claffy: On the problem of optimization of DNS root
servers’ placement. In: PAM. (2003)
7. D. Wessels, M. Fomenkov: Wow, That’s a Lot of Packets. In: PAM. (2003)
8. IRCache: Information resource caching project (2004) Funded by NSF grants NCR-9616602
and NCR-9521745, http://www.ircache.net/.