keyboard image covered an area of roughly 600x200 pix-
els. The recordings were taken in an indoor environment, in
normal lighting conditions: in particular, light was provided
from ﬂuorescent lamps attached to the ceiling and from win-
dows in front of the desktop.
The analysis was performed on a Pentium 4, 3.60GHz
machine with 2GB of RAM. We used a dedicated machine
with similar processing power to store and access the dataset
used in the context-sensitive text analysis. We used the 3-
gram and 4-gram datasets from the Google’s Web 1T cor-
pus. In particular, we ﬁltered out all entries containing non-
alphabetic characters and loaded the remaining entries in a
178
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 05:58:33 UTC from IEEE Xplore.  Restrictions apply. 
MySQL database. After the ﬁltering, the database contained
453M 3-gram entries (which used 17GB for the data and
9.7GB for the indexes), and 517M 4-gram entries (which
used 19GB for the data and 14GB for the indexes).
3.2 Experiments
We recorded a typing session of two users, Alice and
Bob. They were asked to transcribe two paragraphs for a
total of 118 words from the beginning of John Fitzgerald
Kennedy’s inaugural address [21].
We chose the users because of their different typing
styles: Alice types using mostly her index and middle ﬁn-
gers. It took her 3m:55s to complete the task. Bob is a touch
typist: he types with eight ﬁngers and uses the thumbs to
press the space bar. It took him 3m:10s to copy the text.
They were asked to type normally and were not given the
text in advance to practice. As a consequence, their typing
sessions contain “empty” intervals, where they interrupt the
transcription to read the source text. Therefore, the duration
of the experiment does not reﬂect the actual typing speed of
the candidate. Finally, the two users also introduced some
typographical errors: Alice had 5 words misspelled, Bob 6,
and both used the backspace key to correct errors through-
out the text.
3.2.1 Reconstruction Capability
To assess the difﬁculty of the eavesdropping task for a hu-
man analyst, we asked two people not involved with this
project, Analyst 1 and Analyst 2, to manually recover the
typed text from the video recording of Bob. They were told
that the typed text was taken from a Kennedy’s speech, but
had no additional information. Also, none of them had pre-
vious knowledge of the text. Analyst 1 was able to complete
the task in 59m (at a speed of approximately two words per
minute), correctly recovering 89% of the actual text; Ana-
lyst 2 took 1h55m (at a speed of about one word per minute)
and he correctly recovered 96% of the text. These results in-
dicate that the manual analysis is very time-consuming and
not completely error-free. Note that both the test subjects
found the task very fatiguing and clearly stated that they
would not be able to perform it for an extended amount of
time, e.g., several hours.
We then ran ClearShot on the two recordings, and the
tool produced a sorted list of word candidates. We mea-
sured the efﬁcacy of our analysis in reconstructing the typed
words by marking the position of the correct word in the list.
Table 2 shows the results of the analysis: for each user,
we report the percentage of correct words proposed within
the top 1, 5, 10, 25, and 50 choices. The last column shows
the percentage of missed words, i.e., the cases in which the
correct word is not within the ﬁrst 50 candidates proposed
by our analysis tool.
Note that for Alice the context-sensitive analysis had a
limited effect on the detection capability of the tool: it in-
creased by 3% the number of correct words proposed as
ﬁrst choice. The improvement was more consistent in Bob’s
case, where 7% more correct words ended as the ﬁrst candi-
date words. The analysis had an even more signiﬁcant effect
in terms of reducing the effort required by a human analyst
to review the results: it trimmed the length of the list of pro-
posed words down to an average of 10 for Alice and 12 for
Bob (from the 50 proposed by the language analysis), and
for about 30% of the words in Alice’s case (38% in Bob’s
case) the length of the list of proposed words is less than or
equal to ﬁve.
Table 3 shows a sample of the output produced by our
tool. It reports the ﬁrst ﬁve candidate words produced by our
tool for the ﬁrst sentence of the original text. We highlighted
the correct words using a bold face.
These results show that ClearShot can effectively recon-
struct the contents of a typing session from its video in an
automatic fashion. This capability can be leveraged by a
human analyst in a number of ways. First, ClearShot al-
lows one to quickly understand the general meaning and the
topic of the text. For example, by simply looking at the
set of candidate words, it is easy to distinguish a political
speech from a computer code or a description of a summer
vacation. Second, ClearShot’s output can be used to auto-
matically check if the typed text contains one or more in-
teresting words. In this case, the context-based analysis can
be avoided saving a considerable amount of time and the at-
tacker only has to verify if the interested words are present
in the list of the 50 candidates generated by the language-
based analysis. Therefore, the values reported in the top 50
column of Table 2 (respectively 82% and 77%) also corre-
spond to the detection capability of the tool in this scenario.
Finally, when the complete text typed by a user needs
to be recovered, ClearShot can be used to signiﬁcantly re-
duce the required manual effort: in fact, from its output, it is
generally easy to pinpoint the parts of the recording where
automatic analysis was not precise enough and manual ex-
amination of the video is required.
3.2.2 Performance
The performance results of ClearShot are shown in Table 4.
Note that the current prototype is not optimized for speed: it
is mostly implemented in Python, with only a few modules
in C. Porting the tool to C would considerably improve its
running time.
We observe that the computer vision phase of the anal-
ysis runs in about two to three times the actual recording
time. The duration of the language-based analysis is mostly
sensitive to the number of false pressings generated by the
computer vision phase, as they increase the dimension of the
word model graph and, therefore, the number of templates
that have to be evaluated. The context-sensitive analysis is
mostly inﬂuenced by the number of missed words, which
force the tool to switch to the more expensive 4-gram anal-
ysis. However, it would be easy to parallelize the language
179
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 05:58:33 UTC from IEEE Xplore.  Restrictions apply. 
User
Analysis
Top 1 Top 5 Top 10 Top 25 Top 50 Missed
Alice
Bob
Language-based
Context-based
43% 64%
46% 64%
Language-based
Context-based
29% 57%
36% 58%
73%
72%
63%
68%
78%
78%
73%
73%
82%
82%
77%
77%
18%
18%
23%
23%
Table 2. Detection results.
Rank w1
w2
w3
1
2
3
4
5
the
be
we
are
he
observe
by
observed may
any
observer
day
observers
observes
my
w4
no
in
not
on
and
w5
a
and
at
as
an
w6
victory
victor
victories
victors
victorious
w7
of
off
offer
ofﬁce
often
w8
try
arts
party
toy
toys
w9 w10
but
buy
butt
bus
bug
a
and
at
as
an
w11
certain
corporation
celebration
cartoon
cartoons
Table 3. Output produced by ClearShot after running the language-based textual analysis when ana-
lyzing Alice’s video. We only report the top 5 words generated and highlight correct words using a
bold face.
analysis: by using k processing units, each analyzing differ-
ent words, it is possible to speed up the running time of this
phase by k times.
3.3 Tuning Parameters
Our tool has a few parameters that can be tuned to take
into account differences in light conditions, typing charac-
teristics of users, and keyboard position.
First, color thresholding is used during the contour de-
tection process in order to convert the original image into
a binary (black and white) image. Depending on the value
chosen for the threshold level, the sensitivity of the analysis
to the light can be changed. When this happens, the motion
detection sensitivity changes as well: higher values identify
more regions as being in motion, which increases the pos-
sibility of detecting all key pressings. However, this also
increases the amount of noise generated.
Another parameter is the number of empty frames after
which the computer vision analysis assumes that a pressing
has been missed. This parameter is dependent on both the
number of frames per second captured by the camera and
the typing speed of the user: faster users take less time to
move from one key to the next one, and vice versa.
The typing speed also inﬂuences the number of frames
in which a key is identiﬁed as being pressed. As we have
seen, this value is used to determine the score of keys in the
character models. It is possible to tune the coefﬁcients used
in the key scoring system to compensate for different typing
speeds.
Finally, in this initial prototype, the model of the key-
board, which contains the absolute position of each key in
the video, is manually determined by examining the ﬁrst
frame of a recording. Clearly, this has to be generated once
for each recording session.
4 Related Work
The work described in this paper draws upon or ex-
tends previous research in the ﬁelds of computer vision and
spelling correction. We have described some of the related
works in these areas in Section 2, so we will not discuss
these ﬁelds any further here. Instead, hereinafter we focus
on works that address the possibility of leveraging emissions
generated from computing devices to eavesdrop on unsus-
pecting users. Such emissions have often been called com-
promising emanations. The ﬁrst works in this direction date
back to the ’60s and have focused on electromagnetic radi-
ations.
According to Highland, the security risks associated with
electromagnetic radiation have been known in the military
and intelligence communities since 1967 [16], and have re-
ceived more widespread attention in 1985, when van Eck
demonstrated that the screen content of a display could be
effectively reconstructed at a distance using cheap and read-
ily available equipment [44]. More recently, Kuhn and An-
derson described a number of simple eavesdropping exper-
iments performed with a TEMPEST receiver and a cheap
AM radio. They also proposed an active attack consisting
of a Trojan that creates a particular video pattern, which,
in turn, causes the monitor to emit at a speciﬁc radio fre-
quency [24].
In addition, Kuhn describes eavesdropping
techniques that can be used to read cathode-ray tube (CRT)
and ﬂat panel displays at a distance [22, 23]. Loughry
and Umphress discuss the use of LED status indicators on
data communication equipment as an eavesdropping device.
They also describe an active attack with a Trojan that ma-
nipulates the LEDs on a standard keyboard to implement a
high-bandwidth covert channel [29]. Finally, Backes et al.