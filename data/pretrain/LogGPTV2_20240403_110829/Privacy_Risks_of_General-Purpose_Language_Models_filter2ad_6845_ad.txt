vious researches, imbalance in label will let the attack model
prone to overﬁtting and thus hinder the attack’s performance
[33]. To alleviate, we propose to randomly replace certain
word in the negative samples with the keyword, and we replace
the keyword in the positive samples with other random word
in the vocabulary (referred to as the word substitution trick).
After this operation, the original shadow corpus will be twice
enlarged and the samples are balanced in both classes.
(cid:2)
i
(cid:2)
i
Next, the shadow corpus after word substitution can still be
limited in size, i.e., N is small. In this case, we suggest the
adversary should implement their attack model with a Support
Vector Machine (SVM), which is especially effective for small
sample learning [71]. When M is larger than certain threshold
(empirically over 103 samples), the adversary can switch to
a fully-connected neural network as the attack model, which
brings higher attack accuracy.
Attack in Black-Box Settings. The adversary under Assump-
tion 3c faces the most challenging situations, as he/she has
merely no prior knowledge of the plain text. In turn, successful
attacks in this general scenario will raise a huge threat on the
privacy of general-purpose language models.
To implement the keyword inference attack with no prior
knowledge, we propose to ﬁrst crawl sentences from the
Internet to form the external corpus and then transfer the
adversarial knowledge of an attack model on the external
corpus to the target corpus dynamically. Details are as follows.
1) Create the External Corpus from Public Corpora:
With the aid of the Internet, it is relatively convenient for
the adversary to obtain an external corpus from other public
corpora. Next, the adversary can generate positive and negative
samples via the same word substitution trick we mentioned in
the previous part.
2) Transfer Adversarial Knowledge: During our prelimi-
nary attempts, we ﬁnd if we directly train an off-the-shelf
classiﬁer (e.g., linear SVM or MLP) on the external corpus
and use it
to conduct keyword inference attacks on the
target embeddings, the attack’s accuracy can sometimes be
poor. We speculate it is the domain misalignment that causes
this phenomenon. To validate, we ﬁrst train a 3-layer MLP
classiﬁer on an external corpus w.r.t. the keyword head, which
is prepared from the Yelp-Food dataset (i.e., a dataset that
consists of restaurant reviews). Next, we plot the decision
boundary of the classiﬁer on the external corpus in Fig. 4(a).
We also plot the expected decision boundary of the classiﬁer
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:19:28 UTC from IEEE Xplore.  Restrictions apply. 
1321
on the target medical dataset that contains 1000 sentences
in Fig. 4(b), where the scattered points plot the intermediate
representations of the XLNet embeddings at the hidden layer
after dimension reduction with Principle Component Analysis
(PCA) and the two colors imply whether the plain text contains
head or not 1. As we can see, the two decision boundaries are
almost orthogonal to each other. As a result, even though the
attack model on the public domain (i.e., on restaurant reviews)
achieves a near 100% accuracy, its performance is no better
than random guess when applied on the private domain (i.e.,
on medical descriptions).
Fig. 4. Domain misalignment between (a) the external corpus and (b) the
target corpus, through the lens of the (expected) decision boundary of a MLP
classiﬁer trained on the external corpus.
In general, the key challenge here is how to transfer the
adversarial knowledge learned by the attack model from the
public domain (e.g., Yelp-Food dataset) to the private one (e.g.,
medical dataset). First, we introduce some essential notations.
We denote the public domain and the private domain respec-
tively as X0,X1. Given a training set Dpublic := {(z
)}N
(cid:2)
(cid:2)
i, y
from X and some target embeddings Dprivate := {zi}n1
i=1
i
i=1 from
Y, the adversary wants to train an attack model Akeyword,k that
performs well on Dprivate. When the Dprivate and Dpublic dis-
tribute divergently, the straightforward approach works poorly
and thus the phenomenon in Fig. 4 occurs.
Therefore, we propose to learn a uniﬁed domain-invariant
hidden representations for embeddings from Dprivate and
Dpublic. To realise this, we are inspired from the idea of
Domain-Adversarial Neural Network (DANN) [9] and propose
the architecture of our attack model in Fig. 5.
Fig. 5. Architecture of the attack model in the black-box setting.
The model consists of four sub-modules. First, the module
E is an encoder which takes the sentence embedding as input
and is expected to output a domain-invariant representation ˆz.
The hidden representation is followed by two binary classi-
ﬁers, i.e. Ckeyword and Cdomain. The keyword classiﬁer Ckeyword
takes ˆz as input and predicts whether the sentence x contains
the keyword k, while the domain classiﬁer Cdomain outputs
whether the embedding comes from X0 or X1. In practice, we
1More details regarding the external corpus and the medical dataset can be
found in the next section.
(cid:2)
i, 0)}N
implement E as a nonlinear layer with sigmoid activation and
implement Ckeyword and Cdomain as two linear layers followed
with a softmax layer. For both classiﬁers, the loss is calculated
as the cross-entropy between the output and the ground-truth.
Moreover the loss of Ckeyword is calculated on Dpublic, while
the loss of Cdomain is calculated on {(z
i=1 ∪{(zi, 1)}n1
i=1.
In our implementations, an additional module called gradi-
ent reversal layer [9] is fundamental to learn domain-invariant
representations and therefore help transfer the adversarial
knowledge. The gradient reversal layer is intermediate to the
domain classiﬁer and the hidden representation, which works
as an identity layer during the forwarding phase and reverses
the gradient by putting a minus sign to each coordinate during
the back-propagation phase. Intuitively, the gradient reversal
layer regularizes the hidden representation ˆz by amplifying the
keyword-related features and eliminating the domain-related
information. Algorithm 1 in Appendix I details a typical
iteration in the learning process of our DANN-based attack
model. For inference, we take Ckeyword◦ E as the attack model
g.
C. Experimental Setup
We evaluate the proposed keyword inference attack with
two case studies on Airline and Medical in both white-box
and black-box settings.
Benchmark Systems.
• Airline: We collect the airline review dataset from Skytrax
[5] and preserve the reviews that contain one of the 10
speciﬁed city names (e.g., Bangkok, Frankfurt, etc.) to form
our benchmark dataset. The preprocessed dataset contains
4685 airline reviews (average length 15), and we randomly
split the dataset into 10 : 1 to get the test set and the shadow
dataset, which is used to simulate the white-box setting.
We choose the shadow set to be the much smaller partition
to better simulate the real-world situations. We then query
the target language models with the reviews in the test set
and obtain the embeddings as the victims. In the black-box
setting, the adversary only accesses the embeddings of the
test set for adversarial knowledge transfer. We suppose the
adversary’s keyword set as the 10 appeared city names.
• Medical: We implement eight pre-diagnosis systems based
on the CMS public healthcare records [1]. These systems
are designed to guide patients to the proper department
according to the textual description of their disease. We
report the utility of the benchmark systems and more imple-
mentation details in Appendix A. The preprocessed dataset
contains 120, 000 disease descriptions of average length 10.
We randomly split the dataset into 10 : 1, to form the test
set and the shadow dataset. We query the target language
models with the descriptions in the test set to form the target
set. We suppose the adversary’s keyword set contains 10
body-related words (e.g., head, foot, etc.) that appear in the
dataset.
Metrics. For evaluations, we prepare balanced test sets for
each target keyword. In detail, we preserve the embeddings
of all sentences that contain the keyword from the test set as
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:19:28 UTC from IEEE Xplore.  Restrictions apply. 
1322
Due to the game-theoretical essence of DANN, we notice the
accuracy of DANN-based attacks dynamically change over
time. We report both the average and the optimal accuracy
of the DANN-based attack in 50 epochs to reﬂect the average
and worst-case privacy risk. For the baseline methods SVM &
MLP, we report their accuracy after their learning processes
converge.
(a), (e): Accuracy of DANN-based attack per keyword on (a) Airline and (e) Medical. (b), (f): Accuracy of keyword inference attack on (b) Airline
Fig. 6.
and (f) Medical, averaged on 10 speciﬁc city names as keywords. (c): Accuracy of MLP-based white-box attack on Medical with varied size of the shadow
corpus. (d), (g), (h): Accuracy of DANN-based attack on Medical with (d) different size of the external corpus, (f) varied dimension of the domain-invariant
representation and (h) varied number of victim embeddings.
the positive samples, and randomly sample the same number
of embeddings from the rest of the test set as the negative
samples. The statistics of each test set is in Appendix H. We
measure the attack’s effectiveness on each keyword with the
classiﬁcation accuracy on the prepared test sets. To ensure
the attack’s effectiveness is not caused by the adversary’s
knowledge of which keyword to infer, we also conduct black-
box attacks with DANN on Airline and Medical to infer 5
random keywords which are not contained in the target corpus.
External Corpus in Black-Box Setting. Following the pro-
cedures in Section VI-B, we create the external corpus for the
black-box setting from the Yelp-Food dataset, which contains
customers’ reviews for local restaurants, has less than 20%
common words with Medical or Airline and can be replaced
with other public corpus. Speciﬁcally, we choose 2000 sen-
tences that contain the word salad and prepare 2000 positive
and negative samples respectively for each keyword inference
attack with our proposed word substitution trick.
Attack Implementation.
• White-box setting: We study two implementations of the
attack model in the white-box setting, namely the linear
SVM and the 3-layer MLP with 80 hidden units with
sigmoid activations. The batch size is set as 64.
1) Effectiveness & Efﬁciency: These experimental results
highly demonstrate the effectiveness of our attacks in both
white-box and black-box settings. For example, from Fig. 6(f),
we can see our white-box attack, given Bert’s embeddings of
the victims’ medical descriptions, achieves over 99% accuracy
when inferring the occurrence of certain body part, while our
black-box attack with no prior knowledge can still achieve
over 75% accuracy on average. Similarly, when given the
GPT and GPT-2 embeddings of the victim’s airline review,
our white-box and black-box attacks respectively achieve over
95% and 75% accuracy on average for inferring the occurrence
of certain city names. We also report the DANN-based attacks’
accuracy on Airline and Medical in Table III, when inferring
on 5 random keywords which are not contained in the target
sentences. As we can see, our black-box attack achieves over
90% average accuracy in most cases. Moreover, we report
the throughput of training attack models in Table VI, which
indicates the attacks can be efﬁciently constructed in less
than 2 minutes. Combined with the success of the pattern
reconstruction attacks in the previous section, these obser-
vations further support our main ﬁnding that much sensitive
information is encoded in the embeddings from these 8 target
language models and can be practically reverse-engineered for
malicious purposes.
• Black-box setting: We study three implementations in
the black-box setting, namely linear SVM, 3-layer MLP
and the DANN-based attacks. The DANN model has 25-
dimensional domain-invariant representations and the co-
efﬁcient λ in Algorithm 1 is set as 1. We use the Adam
optimizer with learning rate 0.001 for both the MLP and
the DANN-based attacks. The batch size is set as 64.
D. Results & Analysis
Fig. 6(b) & (f) report the performance of keyword inference
attacks in white-box and black-box settings with different
attack models, respectively on Airline and Medical. The results
are averaged on 10 keywords. We also provide the DANN-
based attacks’ accuracy on each keyword in Fig. 6(a) & (e).
2) Comparison among Language Models: From Fig. 6(b),
we notice Google’s XL and Facebook’s RoBERTa show much
stronger robustness than other language models when facing
our white-box attacks on Airline. For these two models, our
white-box attacks only outperform the random guesser with a
slight margin, while on Medical, white-box attacks aiming at
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:19:28 UTC from IEEE Xplore.  Restrictions apply. 
1323
AVERAGE AND WORST-CASE RISK WHEN DANN-BASED ATTACKS INFER
5 RANDOM KEYWORDS (# OF TEST SAMPLE SIZE = 1000).
TABLE III
Transformer-XL
Name
Bert
XLNet
GPT
GPT-2
RoBERTa
XLM
Ernie 2.0
Medical
(worst/average)
1.000/0.860
0.667/0.555
0.974/0.951
0.996/0.974
0.998/0.973
0.996/0.992
0.948/0.907
0.965/0.862
Airline
(worst/average)
0.999/0.891
0.907/0.807
0.994/0.974
0.998/0.987
0.995/0.981
0.998/0.996
0.959/0.925
0.964/0.922
these two models achieve around 80% accuracy when attacks
on other models are uniformly over 95%. This phenomenon
implies the sensitive information in XL’s and RoBERTa’s em-
beddings is much harder to be reverse-engineered, which we
speculate the major causes as XL’s relative small pretraining
data size (and thus smaller vocabulary [16]), and RoBERTa’s
byte-level tokenization scheme. As a result, linear SVM has no
sufﬁcient learning capacity [71] to exploit the sensitive infor-
mation from their embeddings, while the MLP-based attacks
suffer from underﬁtting caused by the limited sample size on
Airline (only about 400 on Airline c.f. 2000 on Medical).
However, combined with Fig. 6(g), slight utility-privacy trade-
off is observed on Medical for XL, which shows about 7%
lower utility than the best utility performance achieved by Bert,
while no utility-privacy trade-off is observed for RoBERTa,
which is probably because the utility performance of most of
our systems is high (over 90%) and hence the utility difference
is not very clear.
3) Comparison among Attack Implementations: First, com-
paring the left two columns and the right three columns in
each bar group of Fig. 6(b) & (f), we can see the white-
box attacks in general are more effective than the black-box
counterparts. For example, we notice white-box attacks on
Medical show an average 25% margin over the black-box
attacks, while exceptions are observed for RoBERTa and XL,
which we have discussed above. Next, among the black-box
attacks, MLP and DANN attacks show similar effectiveness
in many conﬁgurations. However, for Facebook’s XLNet on
the accuracy of the MLP attack is only 0.560,
Medical,
which corresponds to the domain misalignment in Fig. 4,
while the DANN approach improves the attack’s accuracy
to 0.601 on average and 0.691 in the worst case. Finally,
we also investigate the performance of DANN attacks on
each keyword. From Fig. 6(a) & (e), we can see different
language models have their especially vulnerable keyword.
For example, DANN-based attack achieves over 95% accuracy
when inferring the word chest from Google’s Bert embeddings,
and over 80% when inferring ankle from Baidu’s ERNIE
embeddings. We would like to invesigate the fundamental
cause of this interesting phenomenon in a future work.
4) Ablation Study: We also conduct an overall ablation
study by investigating the keyword inference attack in a wide
range of conﬁgurations on three variants of OpenAI’s GPT-
2 (namely GPT-2, GPT-2-Medium, GPT-2-Large), which are
pretrained on the same corpus, but increments in parameter
numbers and embedding dimension [55]. Detailed statistics
can be found in Appendix H.
First, we study the the impact of external corpus size on
the attack effectiveness: (1) For the white-box setting, we
vary the size of the shadow corpus, which represents the