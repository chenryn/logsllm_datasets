### Addressing Imbalance in Label Distribution for Attack Models

Previous studies have shown that an imbalance in label distribution can lead to overfitting in attack models, thereby hindering their performance [33]. To mitigate this issue, we propose a method called the "word substitution trick." This involves randomly replacing certain words in negative samples with the keyword and substituting the keyword in positive samples with other random words from the vocabulary. This operation effectively doubles the size of the original shadow corpus and ensures a balanced distribution of both classes.

### Handling Limited Sample Sizes

Even after applying the word substitution trick, the shadow corpus may still be limited in size (i.e., \( N \) is small). In such cases, we recommend using a Support Vector Machine (SVM) as the attack model, which is particularly effective for small sample learning [71]. When the sample size \( M \) exceeds a certain threshold (empirically, over 10,000 samples), the adversary can switch to a fully-connected neural network, which generally provides higher attack accuracy.

### Black-Box Setting Attacks

In the black-box setting, the adversary has minimal prior knowledge of the plaintext. Successfully attacking in this scenario poses a significant threat to the privacy of general-purpose language models. To implement keyword inference attacks with no prior knowledge, we propose the following steps:

1. **Creating the External Corpus:**
   - The adversary can easily obtain an external corpus from public corpora available on the internet.
   - Positive and negative samples are generated using the same word substitution trick mentioned earlier.

2. **Transferring Adversarial Knowledge:**
   - Directly training a classifier (e.g., linear SVM or MLP) on the external corpus and using it for keyword inference on the target embeddings often results in poor accuracy due to domain misalignment.
   - To illustrate, we trained a 3-layer MLP classifier on the Yelp-Food dataset (a dataset of restaurant reviews) and plotted its decision boundary. We then compared this with the expected decision boundary on a medical dataset. The two boundaries were found to be almost orthogonal, indicating that even a highly accurate classifier on the public domain performs no better than random guessing on the private domain.

### Addressing Domain Misalignment

The key challenge is transferring adversarial knowledge learned by the attack model from the public domain (e.g., Yelp-Food dataset) to the private domain (e.g., medical dataset). We denote the public domain as \( X_0 \) and the private domain as \( X_1 \). Given a training set \( D_{\text{public}} := \{(z_i, y_i)\}_{i=1}^N \) from \( X_0 \) and target embeddings \( D_{\text{private}} := \{z_i\}_{i=1}^{n_1} \) from \( X_1 \), the adversary aims to train an attack model \( A_{\text{keyword}, k} \) that performs well on \( D_{\text{private}} \).

To address this, we propose learning a unified domain-invariant hidden representation for embeddings from both \( D_{\text{private}} \) and \( D_{\text{public}} \). Inspired by the Domain-Adversarial Neural Network (DANN) [9], our attack model architecture includes four sub-modules:
- **Encoder (E):** Takes sentence embeddings as input and outputs a domain-invariant representation \( \hat{z} \).
- **Keyword Classifier (C_{\text{keyword}}):** Predicts whether the sentence contains the keyword.
- **Domain Classifier (C_{\text{domain}}):** Determines whether the embedding comes from \( X_0 \) or \( X_1 \).
- **Gradient Reversal Layer:** Regularizes the hidden representation by amplifying keyword-related features and eliminating domain-related information.

### Experimental Setup

We evaluate the proposed keyword inference attack with two case studies: Airline and Medical, in both white-box and black-box settings.

#### Benchmark Systems
- **Airline:**
  - Collected airline review dataset from Skytrax, containing 4685 reviews.
  - Split into 10:1 ratio for test and shadow datasets.
  - Keyword set: 10 specified city names.
- **Medical:**
  - Implemented eight pre-diagnosis systems based on CMS public healthcare records.
  - Preprocessed dataset: 120,000 disease descriptions.
  - Split into 10:1 ratio for test and shadow datasets.
  - Keyword set: 10 body-related words.

#### Metrics
- Balanced test sets prepared for each target keyword.
- Classification accuracy measured on these test sets.
- DANN-based attacks' accuracy reported over 50 epochs to reflect average and worst-case privacy risk.

### Results and Analysis

- **Effectiveness and Efficiency:**
  - White-box attacks achieve over 99% accuracy on medical data and over 95% on airline data.
  - Black-box attacks achieve over 75% accuracy on both datasets.
  - DANN-based attacks on random keywords achieve over 90% average accuracy in most cases.

- **Comparison among Language Models:**
  - Google’s XL and Facebook’s RoBERTa show stronger robustness against white-box attacks on Airline.
  - On Medical, white-box attacks on these models achieve around 80% accuracy, while others exceed 95%.

- **Comparison among Attack Implementations:**
  - White-box attacks generally outperform black-box attacks.
  - MLP and DANN attacks show similar effectiveness in many configurations.
  - DANN improves the attack's accuracy on domain-misaligned data, such as for XLNet on Medical.

- **Ablation Study:**
  - Investigated the impact of external corpus size and other configurations on OpenAI’s GPT-2 variants.
  - Detailed statistics and further analysis provided in the appendices.

This comprehensive approach demonstrates the practicality and effectiveness of our keyword inference attacks, highlighting the need for enhanced privacy measures in language models.