ing phase can not be captured by our inference-time attack,
e.g., learning rates, batch size. These hyper-parameters are ob-
tained by two means. One is obtained from kernel arguments
(e.g. strides) by retrieving the data ﬁelds of certain kernel
launch K commands, whose offsets are proﬁled in the seman-
tic reconstruction step. Another kind of hyper-parameters are
determined by the existence of relevant kernels. For example,
if there is a BiasNCHWKernel kernel launch, then the boolean
type hyper-parameter use_bias is determined to be true.
Extract DNN Parameters. In this step, we aim to obtain all
the parameters of each layer. The parameter here includes both
weights and bias. Intuitively, parameters are easier to obtain
compared to architecture, because they are statically passed to
the layer-speciﬁc APIs and propagated to the PCIe trafﬁc in
plain value. However, the implementations of different DNNs
on different DNN frameworks vary a lot, some of them raise
challenges for our attack, including duplicated parameters,
asynchronous data movement, and GPU address re-use.
The difﬁculty is how to locate these parameters since D
commands are not only used to transmit parameters but also
transmit input and a lot of other data. In our preliminary exper-
iments, we observe that a lot of K commands do not use any
data that are moved onto GPU by D commands. Instead, they
use new addresses that are generated by certain K commands.
By aligning with the CUDA trace, we ﬁgure out that such K
commands are actually performing device to device memory
1980    30th USENIX Security Symposium
USENIX Association
...XXXXXXXX XXXXXXXX XXXXXXXX XXXXXXXX XXXXXXXX XXXXXXXX...00000008:292B7F0000000008:292B7F80AddressPayloads62200220 0B000000                    ...  6C200120 41000000 6D20406062200220 0B000000 00004300 ... 6C200120 41000000 6D20486000000008:A62F4E00...XXXXXXXX XXXXXXXX XXXXXXXX XXXXXXXX 0B00000                   00000008:A62F4E80405ECF01...405ECF01:  Command Header:  GPU Address00000008:38311580...XXXXXXXX XXXXXXXX XXXXXXXX XXXXXXXX XXXXXXXX...00000008:C6ED080000000008:C6ED0880AddressPayloads62200220 0B000000                    ... 6C200120 41000000 6D20206162200220 0B000000 40014200 ... 6C200120 41000000 6D204860... XXXXXXXX 0B00000                   0B00000                   ...00000008:3831160000512D01...62200220 0B000000 00004200 ... 6C200120 41000000 6D204860...XXXXXXXX XXXXXXXX XXXXXXXX 0B00000                   ...00000008: 17AC870000130C01...00512D0100130C0100000008: 17AC8680:  Command Header:  GPU Addresscopy. We name these K commands KD2D. Our understanding
is that, for synchronous data copy on GPU device memory, it
is much more efﬁcient using GPU kernel than involving DMA
copy which is controlled by D commands. We veriﬁed our
thoughts by varying test platforms and using various data size.
The duplicated data are the weights of DNN layers, where the
original weights on GPU memory are left untouched to avoid
being polluted in inference. here comes our third observation:
Observation 3: CUDA uses K commands to synchronously
copy data from device to device, which are named KD2D. DNN
parameters are sent to GPU using D commands and often
duplicated by KD2D commands. The data taken part in the
layer computations are the copy instead of the original one.
Figure 11 illustrates how parameters are propagated among
commands. The ﬁrst packet (i.e., a D command) is the ear-
liest received packet by GPU. The third DW 00512D01 is
the GPU memory address referring to the address that stores
the weights of this parameter. The second packet is a KD2D
command where two addresses 00130C01 and 00512D01 in
its data ﬁeld. The former address is the destination and the
later is the source in device to device memory copy operation.
The last packet is a K command launching a kernel taking the
destination address as its argument. We recover the parame-
ters in reverse order: (1) we ﬁrst use K commands to locate
the destination address; (2) then we use the KD2D command
to ﬁnd the corresponding source address; (3) ﬁnally we re-
trieve the data ﬁeld of corresponding D command to dump
the weights of parameters.
We found that for extremely large parameter blocks, they
are usually not transmitted using regular D commands. In-
stead, they are transferred using a new type of data movement
command with different header structures. By aligning with
the CUDA trace, we ﬁgure out that these commands are doing
asynchronous data transfer. We name it Dasyn command. This
makes sense because the DNN framework prefers to hide the
latency of large data transfer by taking it off the critical path.
New challenges are brought by Dasyn command. Firstly, the
data size is missed in the Dasyn command header. Secondly,
command header and command data are located in separate
packets with in-consecutive address.
To resolve the ﬁrst problem, we calculate the total num-
ber of weights using obtained hyper-parameters. There are
three types of layers that have weights: convolution layer,
dense layer, and normalization layer. The total number of
weights and bias of convolution layers can be calculated by
the following equations:
of input and output. cout is also known as f ilters. For dense
layer, the number of weights and bias can be calculated by:
# Weightsdense = cout ∗ cin
(3)
# Biasdense = cout
(4)
In Equation 3 and Equation 4, the cin and cout represent the
input shape and output shape respectively. The number of
bias is equal to the number of output. In normalization layer,
the number of weights and bias can be directly obtained from
kernels’ arguments without any calculation.
The second challenge caused by Dasyn makes locating the
data ﬁeld of Dasyn command difﬁcult. In regular D commands
and K commands, the header and the ﬁrst piece of data are
within the same packet, or located in two packets with con-
secutive addresses, which is easy to locate data ﬁelds. But in
Dasyn, its command header and data ﬁeld can be interleaved
by packets from other commands. We resolve this issues by
iterating all commands, ﬁltering out all regular commands and
noises from the beginning. Then only the Dasyn commands
are left. According to the fact that packets within the same
command are contiguous in address, now we can easily as-
semble the header and the corresponding data ﬁeld of every
Dasyn in order.
When a large amount of data is used by the GPU, like VGG
and ResNet, address re-use will occur. That is, the data as-
sociated with the GPU address can be overwritten, and the
subsequent multiple K commands using the same address can
refer to different data. For example, we consider a command
sequence 1(cid:13) D1(src) → 2(cid:13)KD2D1(src,dst1) → 3(cid:13)D2(src) →
4(cid:13)KD2D2(src,dst2) → 5(cid:13)K1(dst1) → 6(cid:13)K2(dst2), where D in-
dicates D command, KD2D indicates data copy on device, and
K represents K command. In this example, data in src is
copied out by KD2D1 and then overwritten by D2, two K com-
mands utilize data but referring to the same source address src.
To resolve this problem, we introduce data life range to
represent the valid period of each data. The life range begins
when it is written by a D command and ends when it is con-
sumed by a KD2D command. Take the command sequence as
the example, the life range of dst1 is 1(cid:13) - 2(cid:13), and the life rang
of dst2 is 3(cid:13) - 4(cid:13). Our strategy is to track back from every K
command to extract its corresponding parameters within its
life range. So in the example, we extract K1’s parameters in
the order of 5(cid:13) 2(cid:13) 1(cid:13).
4 Attack Evaluation
# Weightsconv = mw ∗ mh ∗ cin ∗ cout
(1)
4.1 Experiment Setup
# Biasconv = cout
(2)
In Equation 1, mw, mh are shorted for mask width and mask
height, where mask is also known as image processing ker-
nel in convolution layers. cin and cout represent the number of
input and output, which are indicated by the last arguments
Hardware Platform: We validate our attack on three GPU
platforms, i.e., NVIDIA Geforce GT 730, NVIDIA Geforce
GTX 1080 Ti and NVIDIA Geforce RTX 2080 Ti. There is
only one GPU attached to the motherboard via PCIe 3.0 in ev-
ery individual experiment. We adapt CUDA 10.1 as the GPU
USENIX Association
30th USENIX Security Symposium    1981
Figure 12: Architecture Comparison. This ﬁgure shows the architecture differences between original models and the recon-
structed models. The CBR block represents the sequentially connected C (Convolution), B (Batch Normalization), R (Relu)
layers. There are two major differences between the original models and the corresponding reconstructed models: (1) The
reconstructed models do not have dropout layers (as shown in MNIST and VGG); (2) The reconstructed models treat every
activation function as a single layer (as shown in MNIST and ResNet).
Table 1: Victim Models. This table displays the detail infor-
mation of all three victim models, including number of layers,
number of parameters, training datasets and input shape.
Number of Layers
Number of Parameters
Datasets
Input Shape
MNIST
VGG16
ResNet-20
8
544,522
mnist
(28,28,1)
60
15,001,418
cifar10
(32,32,3)
72
274,442
cifar10
(32,32,3)
programming interface and Teledyne LeCroy Summit T3-16
PCIe Express Protocol Analyzer as our snooping device.
Victim Model: We validate our attack on three pre-trained
DNN models: MNIST, VGG16, and ResNet20, which are
public available [30].
• MNIST model is a sequential model, where layers are
stacked and every layer takes the only output of the previ-
ous layer as the input. It is trained on the MNIST dataset
and can achieve 98.25% inference accuracy for hand-
written digits.
• VGG16 model is a very deep sequential model with 60
layers in total, 13 of which are convolution layers. It is
trained using the cifar10 dataset and can achieve 93.59%
inference accuracy for the cifar10 test set.
• ResNet20 model is a non-sequential model, where some
layers have multiple outputs and take multiple inputs
from other layers. The victim ResNet model has 20 con-
volution layers out of 72 layers in total, which achieves
91.45% inference accuracy for the cifar10 test set.
These pre-trained victim models are used for inference by
Keras framework with Tensorﬂow as the backbone. In our
experiments, we treat these models as black-boxes without
using any layer information during attack. These publicly
available models are used only for ground truth purpose. Our
attack works for arbitrary proprietary models. The attack re-
sults are not inﬂuenced by the model accuracy or architecture.
1982    30th USENIX Security Symposium
USENIX Association
MNIST Original ModelResNet Generated ModelVGG Generated ModelVGG Original ModelCBRInputAvgpoolingFlattenAddAddAddAddAddAddAddAddAddResNet Original ModelDropoutMaxpoolingDenseFlattenDropoutReluDenseSoftmaxBNDropoutCBRCBRDropoutMaxpoolingCBRCBRDropoutCBRDropoutMaxpoolingCBRCBRDropoutMaxpoolingCBRDropoutCBRDropoutMaxpoolingCBRDropoutCBRCBRCBRMaxpoolingDenseFlattenReluDenseSoftmaxBNCBRCBRMaxpoolingCBRCBRCBRMaxpoolingCBRCBRMaxpoolingCBRCBRMaxpoolingCBRCBRCBRCBRCBRCBRCBRCBRCBRCBRCBRCBRCBRCBRCBRCBRCBRCBRCBRCBRCBRCBRCBRInputAvgpoolingFlattenAddAddAddAddAddAddAddAddAddCBRCBRCBRCBRCBRCBRCBRCBRCBRCBRCBRCBRCBRCBRCBRCBRCBRCBRCBRCBRDense(softmax)DenseSoftmaxDense(softmax)Dense(relu)DropoutFlattenMaxpoolingDropoutConv2D(relu)Conv2D(relu)MNIST Generated ModelDenseSoftmaxDenseSoftmaxFlattenMaxpoolingConv2DReluConv2DReluCBRConv2DReluBN is equal with: Conv Batch NormalizationActivationPoolingDenseConcatDroputFlattenLegends:Table 2: Related Kernels of Each Layer. This table lists the
related kernels of each layer. If there are multiple related
kernels of that layer, the primary kernels are highlighted in
bold. The last row indicates some kernels not belong to any
layer, but are useful and need to be recorded.
Layer
Conv 2D
BN
Dense
Flatten
MaxPool
AvgPool
ZeroPad
Add
Relu
Softmax
Others
Related Kernels
ShufﬂeInTensor3Simple
cudnn::detail::implicit_convolve_sgemm
SwapDimension0And2InTensor3Simple
cudnn::winograd::generateWinogradTilesKernel
cudnn::winograd::winograd3x3Kernel
cudnn::detail::bn_fw_inf_1C11_kernel_new
gemv2N_kernel_val
gemvNSP_kernel_val
BlockReduceKernel
cudnn::detail::pooling_fw_4d_kernel
Eigen::internal::AvgPoolMeanReducer
PadInputCustomKernelNHWC
Eigen::internal::scalar_sum_op
Eigen::internal::scalar_max_op
softmax_op_gpu_cu_compute_70
SwapDimension1And2InTensor3UsingTiles
BiasNCHWKernel
BiasNHWCKernel
No.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
The detailed model information including layers, shapes, and
parameters are elaborated in Table 1.
4.2 Model Architecture Evaluation
In this section, we demonstrate the semantic equivalence be-
tween the original model and the reconstructed model. Fig-
ure 12 depict the architecture of original models and recon-
structed models for MNIST, VGG, and ResNet, where each
rectangle represents a DNN layer. As the ﬁgure is shown,
most of the architectures of the original model and the recon-
structed model are the same, except two differences. The ﬁrst
difference is that the reconstructed model does not have the
dropout layers, e.g., the MNIST model and the VGG model.
The dropout layer is used to prevent over-ﬁtting during the
training procedure. It randomly selects some neurons and
drops the results. Since it is only used in the training phase
and disabled during the inference, this information is not able
to be captured in PCIe trafﬁc. Attributes to the quiescence
in interference, the dropout layer will not inﬂuence the re-
sult of the inference. The second difference is caused by the
implementation. Some models are implemented using activa-
tion function as a hyper-parameter, like the original MNIST
model, but some others regard activation function as a single
layer, like the Relu function in the original VGG model. This
implementation difference will also not lead to any accuracy
variance. During our reconstruction, we regard all activation
Table 3: Offset of Hyper-Parameters. This table shows all
hyper-parameters offsets in their located kernel. The offset
is deﬁned as the distance between the ﬁrst word and the tar-
get hyper-parameter in the data ﬁeld of a K command. The
weights row and bias row indicate the offset of weights ad-
dress and bias address respectively.
Hyper-Parameters Kernel
GT 730
1080 Ti
2080 Ti
Kernel Size
Strides
Filters
Weights
Bias
Weights1
Weights2
Weights3
Weights4
Pool Size
Strides
Pool Size
Strides
Padding
Units
Weights
Bias
Batch Normalization Layer
Convolution Layer
(102,103)
2
(126,127)
2
2
1
17
101
83
85
159
161
163
165
6
6
6
6
Maxpooling Layer
(152,153)
10
(136,137)
10
(99,100)
(123,124)
(96,97)
(120,121)