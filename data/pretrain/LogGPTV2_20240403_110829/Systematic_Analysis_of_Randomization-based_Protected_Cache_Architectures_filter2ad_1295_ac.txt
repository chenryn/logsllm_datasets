i=1
i
P
1) Prime+Prune+Probe: An iteration begins with a prime
step, where the attacker accesses a set of k addresses, loading
them into the cache. For k > 1, there can be cache contention
within this set. Thus, as a key step to eliminate false positives,
the prune step iteratively re-accesses the set. This forces
all self-evicted addresses to be cached again, at a potentially
different location than before. The prune step terminates as
soon as no more self-evictions occur when accessing the set.
If there are still self-evictions after a few iterations, pruning
becomes more aggressive and additionally discards all addresses
with high access latency (i.e., those evicted by another attacker
address). Upon termination of the prune step, the attacker has
a set of k(cid:48) ≤ k known addresses guaranteed to reside in the
cache. Let mpr denote the total number of pruning iterations.
Now, the attacker triggers the victim to perform the access of
interest (i.e., access x, as in conventional PRIME+PROBE). This
memory access evicts one attacker address with probability
pc, which depends both on the attack parameter k and the
randomized cache parameters (cf. Section V-B). In the probe
step, the attacker accesses the set of k(cid:48) addresses again, adding
addresses with high latency to G (i.e., victim evicted them).
In PRIME+PRUNE+PROBE (PPP), the prune step is crucial
and noise-absorbing. Without it, the attacker cannot distinguish
evictions due to victim accesses from those by the priming set.
By pruning, the attacker completely removes these false posi-
tives. Appendix A experimentally relates pruning parameters
k, k(cid:48) and mpr for different cache conﬁgurations.
The PRIME+PRUNE+PROBE procedure is repeated until
enough accesses are caught and added to G. This constitutes
the bottom-up approach; G is not the result of shrinking a
large initial set. Instead, it is built from the ground up.
2) Penalty for being cached: In case the target address x is
already cached, a single PPP iteration must both evict x and
catch the access to x when it is reloaded into the cache.
The attacker can either (1) ﬁrst evict x probabilistically, by
accessing many different addresses or other techniques; (2)
apply PPP as-is, tolerating a suboptimal catching probability
pc. These strategies trade off the success probability of one
iteration (pc) with its execution time (number of accesses). In
what follows, we consider both a cached and uncached x. Any
proﬁling strategy then has higher pc than when the target is
always cached, and lower pc than when it is never cached.
B. Catching Probability pc
The catching probability pc is the success rate of one PRIME+
PRUNE+PROBE iteration and depends on the randomized cache
(nw, b, P , policy RP) and attack parameter k(cid:48). Table III estab-
lishes pc for several conﬁgurations. We distinguish whether x
is cached (denoted pc,c), vs. not cached (denoted pc,n).
1) Target is not cached (pc,n): After prime and prune,
the victim access to x caches it in a random partition, and RK
pseudorandomly determines the cache set within this partition.
For RAND, x evicts an attacker address with probability equal
to the coverage of the cache after pruning (i.e., pc,n = k(cid:48)/N).
For LRU, x evicts an attacker address if there are at least
P addresses in the attacker set that were mapped to the same
nw
cache partition and set of x during prime and prune.
1
It can be approximated (and lower-bounded) by the com-
P − 1
plement of the cumulative binomial with k trials, nw
successes and binomial success probability (P · 2b)−1, i.e.,
pc,n = 1− binom(k, nw
P·2b ). In practice, due to self-
evictions during pruning, the actual number of binomial trials
is slightly higher than k, resulting in increased pc,n.
P −1,
2) Target is cached (pc,c): Catching an access to a cached
target x requires both evicting x and detecting its reintroduction
in the cache, resulting in a penalty on pc. The probabilities
pc,c (exact for RAND, approximate for LRU) are derived in
Appendix B and collected in Table III. The penalty is maximal
for k(cid:48) = 1, being nw (RAND) or P (LRU), and decreases with k(cid:48)
as prime/prune implicitly evict an increasing cache portion.
Appendix C complements the theoretical analysis with
empirical validation. It also explores the relation between pc
and k(cid:48), and the penalty on pc for a cached target.
Takeaway: Add pruning to PRIME+PROBE proﬁling.
Pruning enables testing more than one guess per iteration.
It improves proﬁling for RAND and is essential for LRU.
VI. OPTIMIZATIONS FOR PRIME+PRUNE+PROBE
This section describes optimizations of PRIME+PRUNE+
PROBE for (A) total cache accesses and (B) victim invocations.
We then evaluate PPP strategies on a range of cache instances.
A. Optimizing for total cache accesses
1) Burst Accesses: As derived, the catching probability pc,c
(target already cached) holds at the start of constructing the
generalized eviction set G. As the elements of G have explicitly
been observed to collide with x, they can be accessed in burst
before the PPP iteration, essentially implementing a targeted
eviction of x. As proﬁling progresses and G grows, the burst
becomes more successful, and the penalty for a cached target
shrinks, hence pc,c → pc,n asymptotically. The burst access
optimization thus hides the caching penalty. It applies to both
RAND and LRU, but the latter can be accelerated even more.
2) Bootstrapping: A PPP iteration for LRU succeeds if
prime/prune ﬁll the full set for x in the designated partition.
As G contends with x, we add G as bootstrapping elements
to the PPP set. Thus, ﬁlling the full set becomes more likely.
However, if a victim access to x evicts a bootstrapping
element instead of a PPP guess, the iteration is wasted: G was
already known to contend with x. This issue can be resolved by
relying on LRU statefulness. Adding G at the end of the PPP
set ensures that PPP evictions precede bootstrapping evictions.
Bootstrapping implicitly implements burst accesses, and
works very well for LRU. However, it is unattractive for RAND.
Takeaway: Use elements in G to accelerate ﬁnding more.
Burst accesses hide caching penalty effectively as G grows.
Bootstrapping increases pc by helping to ﬁll the LRU set.
B. Optimizing for victim invocations
We now explicitly minimize the required victim accesses
Av. This is relevant, e.g., for long victim programs or cases
where victim runs are limited. We decouple it as Av = c
,
pc
relating it to accesses c needed to be caught (i.e., successful
iterations), and to pc (i.e., success probability of one iteration).
Section V already maximized denominator pc with PRIME+
PRUNE+PROBE. We now independently minimize numerator c,
forming a ﬂexible proﬁling framework to globally optimize Av.
It ﬁrst preselects candidate addresses that have higher catching
probabilities. The framework comprises three steps:
Step 1. Use PRIME+PRUNE+PROBE to ﬁnd, for every
partition i, one address ai that collides with x in that partition.
Step 2. For each ai, construct a candidate pool with
addresses that collide with it in at least one partition.
Step 3. Resume PRIME+PRUNE+PROBE with the obtained
candidate pools instead of randomly selected addresses.
The ﬁrst step simply constructs a smaller G with PPP.
Assume it needs to continue until G contains at least one
element for every partition. The expected accesses to catch is
then given by the coupon collector problem in statistics, with
one set of P coupons: E[c] = P (1 + 1/2 + ··· + 1/P ).
The second step ﬁnds addresses that contend with the ai
obtained in Step 1, instead of proﬁling x directly. As the ai
are attacker-accessible, their access latency can be measured,
and no victim accesses are required. Addresses that contend
with ai also contend with x with probability ≥ P −1, which is
much more likely than a randomly selected address (≈ 2−b).
The third step resumes PPP for target x with candidate pools
for the ai. Every iteration accesses the pools, prunes, triggers
access to x, and probes. For sufﬁciently large candidate pools,
pc≈ 1, signiﬁcantly reducing Av as compared to Step 1.
Conceptually, the ﬁrst and third step are similar in nature.
They can also be independently accelerated, as in Section VI-A.
We now explore the complexity and acceleration opportuni-
ties of Step 2. As the access latency of the targets ai can
be measured, catching probabilities can increase, and there is
no penalty if the ai are already cached. We again distinguish
between replacement policies, and measure the complexity in
attacker accesses Aa (as there are no victim accesses).
1) Optimizing Step 2 for RAND: We propose to construct
the candidate pool through reverse PRIME+PRUNE+PROBE.
Let S = {a1, a2, . . . , ac} be the starting set obtained in Step
1. The elements of S are now the targets instead of the victim
address x. Every iteration tries one random address guess g.
PRIME+PRUNE+PROBE (PPP) primes the cache with k
guesses and observes eviction by the target. REVERSE PPP
instead primes the cache with the targets S, prunes, accesses
the guess g, then probes S. If accessing an element of S is
slow, say ak, we add g to the candidate pool for ak. Every
N , and there are ≈ c + 1 attacker accesses
iteration has pc = c
per iteration, (i.e., very little pruning, and probe overlaps with
the next prime). The expected number of attacker accesses to
obtain one element for the candidate pool hence is E[Aa]≈ N.
2) Optimizing Step 2 for LRU: For LRU, reverse PPP is
even more effective. Again, let S = {a1, a2, . . . , ac} be the set
from Step 1, and let g denote a random address guess.
Assume the attacker primes the cache with S, prunes it, and
observes self-evictions. For LRU, this implies that S ﬁlled
a full cache set ( nw
P lines). In this case, the attacker does
reverse PRIME+PRUNE+PROBE, where one iteration consists
of prime and prune with S, accessing g, and probe with
S. If accessing an element of S is slow in the probe step, say
ak, we add g to the candidate pool for ak. This approach has
P·2b , and there are ≈ c+1 accesses per iteration, resulting
pc = 1
in expected number of attacker accesses E[Aa] ≈ (c+1)·P ·2b.
Importantly, as g collides with multiple ak in S, it very
likely collides with x and can directly be added to S. Thus,
it immediately grows eviction set G without accesses by the
victim, bypassing Step 3. However, it can only be started
if priming S has observed self-evictions. Interleaving it with
Step 1 implicitly generates new attempts at this precondition.
3) Flexibility of the Framework: The three-step framework
ﬂexibly instantiates randomized caches and attack scenarios.
If the victim program is tiny and executes continuously, all
proﬁling time is spent in Step 1. The shares of Step 2 and
Step 3 grow as soon as the victim program becomes the bot-
tleneck in any way. Finally, if x is attacker-accessible, reverse
PPP from Step 2 is used immediately. The framework also
enables splitting G based on the partition of contention with
x, making the eviction probabilities (Section IV-A) exact.
Takeaway: Use elements in G to reduce victim accesses.
Filtering candidate addresses based on contention with G
allows to (partially) refrain from victim invocations.
C. Evaluation of proﬁling strategies
Figure 4 depicts victim and total cache accesses for the
presented proﬁling strategies, obtained from simulated pro-
ﬁling runs (cf. Section III-A3). We observe a mostly linear
progression in constructing G. One exception is reverse PPP,
where the construction of the candidate pools does not grow G
immediately (jump), but accelerates the proﬁling that follows.
Optimizations like burst accesses and bootstrapping improve
both total and victim accesses. In contrast, probabilistic full
cache evictions and three-step proﬁling incur a trade-off
between total accesses and victim invocations. Of course, one
can freely interpolate between these extreme strategies.
D. Inﬂuence of randomized cache instance
1) Sets, ways and partitions: Both proﬁling and exploitation
in randomized caches are inﬂuenced by the parameters of the
instance. We investigate the effectiveness of PPP on several
Fig. 4: Effort of proﬁling strategies for RAND (top) and LRU (bottom), measured as total (left) and victim (right) cache accesses, averaged for 104 simulated
proﬁling runs. Cache instances are denoted RP(nw, b, P ). k is ﬁxed to N
4 (LRU) to isolate the inﬂuence of the strategy. Pruning becomes
aggressive from the sixth iteration, if not already terminated. Full evictions between PPP iterations, if performed, use 2N addresses for LRU and 3N for RAND.
2 (RAND) and 3N
instances for RAND and LRU. Figure 5 captures our ﬁndings,
again based on simulation (cf. Section III-A3).
Larger caches resist better against PPP. Increasing cache
ways (nw) seems to compare favorably to increasing sets (2b).
While the latter only proportionally prolongs proﬁling and
does not affect exploitation, the former inhibits both proﬁling
and exploitation. In particular, |G| increases for the same
exploitation pe, and proﬁling is prolonged as |G| increases
while the accesses per element of G stay roughly the same.
Similarly, for the same cache dimensions (nw, b), both PPP
proﬁling and exploitation suffer from increased partitioning P .
Especially for RAND, there is no indication from our ideal-case
analysis why one should not opt for maximal partitioning.
In general, we ﬁnd that PPP can be hindered by tuning
cache sets, ways, and partitions, but not to the point where it
becomes infeasible. What really works is limiting the cache
access budget for the attacker (i.e., a strict rekeying condition).
2) Rekeying period: The difference between the proﬁling
state of the art and rekeying period T is the design’s security
margin. Although tempting, setting T just low enough to thwart
known techniques does not account for potential improvements.
As an example to obtain (very) conservative rekeying periods,
we now leverage the security of RK to derive minimal
complexities to construct generalized eviction sets of certain
quality, i.e., with a lower bound on eviction probability pe
(e.g., pe ≥ 90%). We use the following central assumptions:
A RK is indistinguishable from a random function.
B Victim addresses of interest are not attacker-accessible.
C The eviction probability pe for G is lower-bounded.
As the target is not accessible to the attacker ( B ), she can
only infer accesses with PPP (cf. Section V-A1): bring cache
in known state, wait for victim execution, and probe.
To achieve an eviction rate pe ( C ), the proﬁling needs
TABLE IV: Rekeying periods T to ensure that the success rate to construct G
with pe ≥ 95% is upper-bounded by 1/2{8,12,16,24,32}. The cache instance
is RAND(16, 13, 16) and all accesses are counted as cache hits (e.g., 10 ns)
Success Rate
2−8
2−12
2−16
2−32
T for proﬁling
time
T
≈ 10 sec
40N
29N ≈ 2.5 min
≈ 30 min
22N
≈ 2 years
9N
T /2 for proﬁling
time
T
80N
58N
44N
18N
≈ 20 sec
≈ 5 min
≈ 60 min
≈ 4 years
the very least m ≥ peP victim accesses to
to catch at
different partitions. Indeed, an attack with pe > m
P has inferred
information about partitions for which no memory access has
been caught. By contradiction with A , it cannot exist.
Beyond Aideal, we further contrive the setting in favor of the
attacker. We consider strongly idealized pruning (i.e., k(cid:48) = k and
mpr = 1), and a permanently uncached target (i.e., pc = pc,n).
Furthermore, we scope the algorithm as catching a single access
in m partitions, neglecting the necessary expansion to full G.
A perfectly ideal PRIME+PRUNE+PROBE iteration then
requires k accesses for prime, k for prune, 1 for the victim
access, and k to probe. Assuming the attacker somehow
manages to combine probe of one iteration with prime of
the next, we use 2k+1 accesses per iteration as lower bound.
We outline the idea for a randomized cache with random
replacement. The only degree of freedom in the idealized PPP
is the number of addresses k in the prime step. Indeed, their
order or frequency does not impact the cache coverage k
N .
Given a rekeying period of T cache accesses, the probability
of observing at least one access in at least m distinct partitions
is (using a generalization of the birthday problem in statistics):
 T
2k+1(cid:88)
i=m
max
k
(cid:16) T
2k+1
(cid:17) ki(N − k)
T
2k+1
−i
P(cid:88)
(cid:16)P
(cid:17) l(cid:88)
(−1)r(cid:16)l
(cid:17)
r
l − r
P
(
)i
i
T