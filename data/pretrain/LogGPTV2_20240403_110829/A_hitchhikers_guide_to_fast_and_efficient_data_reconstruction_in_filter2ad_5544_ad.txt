### System
The requested block is reconstructed on the fly by the RAID-Node via a MapReduce job. The execution of this reconstruction operation is identical to that in the previously discussed reconstruction process.

### 5.2 Hitchhiker in HDFS
We implemented Hitchhiker in HDFS, leveraging the new erasure code (§3) and the hop-and-couple technique (§4) proposed in this paper. We developed all three versions of the proposed storage code: Hitchhiker-XOR, Hitchhiker-XOR+, and Hitchhiker-nonXOR. This required implementing new Encoder, Decoder, and Parallel-Reader modules. In the context of HDFS, the term "block" refers to a unit of data, and these terms will be used interchangeably.

#### 5.2.1 Hitchhiker-XOR and Hitchhiker-XOR+
The implementation of Hitchhiker-XOR and Hitchhiker-XOR+ is exactly as described in §3.1 and §3.2, respectively.

#### 5.2.2 Hitchhiker-nonXOR
Hitchhiker-nonXOR requires finite field arithmetic operations in addition to those performed for the underlying RS code. We now describe how our implementation executes these operations.

**Encoder:**
As shown in Fig. 6, in addition to the underlying RS code, the Encoder needs to compute the functions \( f_2(a_1, a_2, a_3, 0, \ldots, 0) \), \( f_2(0, \ldots, 0, a_4, a_5, a_6, 0, \ldots, 0) \), and \( f_2(0, \ldots, 0, a_7, a_8, a_9, 0) \), where \( f_2 \) is the second parity function of the RS code. One way to perform these computations is to use the existing RS encoder. However, this approach involves superfluous computations: the RS encoder computes all four parities \( f_1(x) \), \( f_2(x) \), \( f_3(x) \), and \( f_4(x) \) for any given input \( x \). We only require one of the four parity functions for each of the three distinct inputs, which are sparse (i.e., most of the input bytes are zeros).

Our Encoder implementation takes a different approach, exploiting the linearity property of RS codes. Each parity byte can be written as a linear combination of the data bytes. For example, the parity function \( f_{\ell}(x) \) can be specified as:
\[ f_{\ell}(x) = \sum_{j=1}^{10} c_{\ell,j} x_j \]
for some constants \( c_{\ell,1}, \ldots, c_{\ell,10} \). We first inferred the values of these constants for each parity function from the existing "black-box" RS encoder by feeding different unit vectors as inputs. Note that obtaining these constants is a one-time task. With these constants, the Encoder can efficiently compute the required parity functions.

**Decoder:**
As seen in §3.3, during the reconstruction of any of the first nine units, the first byte is reconstructed by performing an RS decoding of the data obtained in the intermediate step. A straightforward way to implement this is to use the existing RS decoder. However, we take a different, computationally cheaper route. We make use of two facts: (a) the data is known to be 'sparse' (i.e., contains many zeros), and (b) the RS code has the linearity property, and any linear combination of zero-valued data is zero. Motivated by this observation, our Decoder simply inverts the sparse linear combination to reconstruct the first substripe more efficiently.

#### 5.2.3 Hop-and-Couple
Hitchhiker uses the proposed hop-and-couple technique to couple bytes during encoding. We use a hop-length of half a block since the granularity of data read requests and recovery in HDFS is typically an entire block. As discussed in §4, this choice of hop-length minimizes the number of discontiguous reads. However, the implementation can be easily extended to other hop-lengths.

When the block size is larger than the buffer size, coupling bytes that are half a block apart requires reading data from two different locations within a block. In Hitchhiker, this is handled by the Parallel-Reader.

#### 5.2.4 Data Read Patterns During Reconstruction
During reconstruction in Hitchhiker, the choice of blocks from which data is read, the seek locations, and the amount of data read are determined by the identity of the block being reconstructed. Since Hitchhiker uses the hop-and-couple technique for coupling bytes to form stripes during encoding, the reads to be performed during reconstruction are always contiguous within any block (§4).

For reconstruction of any of the first nine data blocks in Hitchhiker-XOR+ or Hitchhiker-nonXOR, or for reconstruction of any of the first six data blocks in Hitchhiker-XOR, Hitchhiker reads and downloads two full blocks (i.e., both substripes) and nine half blocks (only the second substripes). For example, the read patterns for decoding blocks 1 and 4 are shown in Fig. 9a and 9b, respectively. For reconstruction of any of the last four data blocks in Hitchhiker-XOR, Hitchhiker reads and downloads three full blocks (both substripes) and nine half blocks (only the second substripes). For recovery of the tenth data block in Hitchhiker-XOR+ or Hitchhiker-nonXOR, Hitchhiker reads half a block each from the remaining thirteen blocks. This read pattern is shown in Fig. 9c.

### 6. Evaluation

#### 6.1 Evaluation Setup and Metrics
We evaluate Hitchhiker using two HDFS clusters at Facebook: (i) the data-warehouse cluster in production, comprising multiple thousands of machines with ongoing real-time traffic and workloads, and (ii) a smaller test cluster comprising around 60 machines. In both clusters, machines are connected to a rack switch through 1Gb/s Ethernet links, with higher levels of the network tree architecture having 8Gb/s Ethernet connections.

We compare Hitchhiker and RS-based HDFS-RAID in terms of the time taken for computations during encoding and reconstruction, the time taken to read the requisite data during reconstruction, and the amount of data read and transferred during reconstruction. The computation and data read times during reconstruction are vital as they determine the performance of the system during degraded reads and recovery.

#### 6.2 Evaluation Methodology
The encoding and reconstruction operations in HDFS-RAID, including those for degraded reads and recovery, are executed as MapReduce jobs. The data-warehouse cluster does not distinguish between MapReduce jobs fired by the RAID-Node and those fired by a user. This allows us to evaluate the timing metrics for encoding, recovery, and degraded read operations by running them as MapReduce jobs on the production cluster, thus performing evaluations in the presence of real-time production traffic and workloads.

We deployed Hitchhiker on a 60-machine test cluster in one of the data centers at Facebook and evaluated the end-to-end functionality of the system. Tests on this cluster verified that savings in network and disk traffic during reconstruction are as guaranteed in theory by Hitchhiker’s erasure code.

For all evaluations, we consider the encoding parameters \( k = 10 \) and \( r = 4 \), a block size of 256MB (unless mentioned otherwise), and a buffer size of 1MB. These are the default parameters in HDFS-RAID and are employed in the data-warehouse cluster in production at Facebook to store multiple tens of Petabytes.

In the evaluations, we show results of the timing metrics for one buffer size, as the same operations are performed repeatedly on one buffer size amount of data until an entire block is processed.

#### 6.3 Computation Time for Degraded Reads & Recovery
Fig. 10 shows the comparison of computation time during data reconstruction (from 200 runs each). Note that:
- In both Hitchhiker-XOR+ and Hitchhiker-nonXOR, reconstruction of any of the first nine data blocks entails the same amount of computation.
- Reconstruction of any data block in Hitchhiker-XOR is almost identical to reconstruction of block 1 in Hitchhiker-XOR+. Hence, no separate measurements are shown.

This is because (recall from §3) the reconstruction operation in Hitchhiker requires performing the resource-intensive RS decoding only for half the substripes, compared to RS-based HDFS-RAID. In Hitchhiker, the data in the other half of substripes is reconstructed by performing either a few XOR operations (under Hitchhiker-XOR and Hitchhiker-XOR+) or a few finite-field operations (under Hitchhiker-nonXOR).

One can see that Hitchhiker’s erasure codes perform faster reconstruction than RS-based HDFS-RAID for any data block. Hitchhiker-XOR+ has 25% lower computation time during reconstruction compared to Hitchhiker-nonXOR for the first nine data blocks; for block 10, the time is almost identical in Hitchhiker-XOR+ and Hitchhiker-nonXOR (as expected from theory (§3)).

#### 6.4 Read Time for Degraded Reads & Recovery
Figs. 11a and 11b compare the median and the 95th percentile of the read times during reconstruction for three different block sizes: 4MB, 64MB, and 256MB in Hitchhiker-XOR+. The read patterns for reconstruction of any of the first nine blocks are identical (§3.2).

In the median read times for reconstruction of blocks 1-9 and block 10, respectively, in comparison to RS-based HDFS-RAID, we observed a reduction of 41.4% and 41% for 4MB block size, 27.7% and 42.5% for 64MB block size, and 31.8% and 36.5% for 256MB block size. For the 95th percentile of the read time, we observed a reduction of 35.4% and 48.8% for 4MB, 30.5% and 29.9% for 64MB, and 30.2% and 31.2% for 256MB block sizes.

The read pattern of Hitchhiker-nonXOR is identical to Hitchhiker-XOR+, while that of Hitchhiker-XOR is the same for the first six blocks and almost the same for the remaining four blocks. Hence, for brevity, we plot the statistics only for Hitchhiker-XOR+.

Although Hitchhiker reads data from more machines compared to RS-based HDFS-RAID, it gives superior performance in terms of read latency during reconstruction. The reason is that Hitchhiker reads only half a block size from most of the machines it connects to (recall from §5), whereas RS-based HDFS-RAID reads entire blocks.

#### 6.5 Computation Time for Encoding
Fig. 12 compares the computation time for the encoding operation. Hitchhiker entails higher computational overheads during encoding compared to RS-based systems, and Hitchhiker-XOR+ and Hitchhiker-XOR are faster than Hitchhiker-nonXOR. This is expected since Hitchhiker’s encoder performs additional computations beyond those of RS encoding. §7 discusses the trade-offs between higher encoding time and savings in other metrics.

#### 6.6 Statistics of Single Block Reconstruction
Hitchhiker optimizes data reconstruction in scenarios when only one block of a stripe is unavailable. If multiple blocks belonging to a stripe are unavailable, Hitchhiker performs reconstruction in a manner identical to RS-based HDFS-RAID, by reading and downloading 10 entire blocks.

We collected measurements of the number of missing blocks per stripe across six months in the data-warehouse cluster in production at Facebook, which stores multiple Petabytes of RS-coded data. We observed that among all the stripes that had at least one block to be reconstructed, 98.08% had exactly one such block missing, 1.87% had two blocks missing, and the number of stripes with three or more such blocks was 0.05%. The measurements reveal single block reconstructions to be by far the most common scenario in the system.

One might consider lazily performing reconstruction by waiting for multiple blocks in a stripe to fail and then reconstructing them all at once to amortize the cost of disk reads and download during reconstruction. However, such a waiting approach may not be feasible when read performance is key: a failed systematic block must be repaired quickly to serve future read requests. Furthermore, degraded reads are performed on a single block in real time, and there is no equivalent of waiting for more failures.

We deployed HDFS-RAID with both Hitchhiker and the RS-based system on a 60-machine test cluster at Facebook to verify Hitchhiker’s end-to-end functionality. We created multiple files with a block size of 256MB and encoded them separately using Hitchhiker and RS-based HDFS-RAID. The block placement policy of HDFS-RAID ensures that the 14 blocks of an encoded stripe are all stored on different machines. We then forced some of these machines to become unavailable by stopping HDFS-related scripts running on them and collected the logs pertaining to the MapReduce jobs that performed the reconstruction operations. We verified that all reconstruction operations were successful. We also confirmed that the amount of data read and downloaded in Hitchhiker was 35% lower than in RS-based HDFS-RAID, as guaranteed by the proposed codes. We do not perform timing measurements on the test cluster since the network traffic and workload on these machines do not reflect the real production scenario. Instead, we evaluated these metrics directly on the production cluster itself (as discussed earlier).

### 7. Discussion on Trade-offs
[Further discussion on trade-offs can be added here, focusing on the balance between encoding time and the benefits in other metrics like read and reconstruction times.]