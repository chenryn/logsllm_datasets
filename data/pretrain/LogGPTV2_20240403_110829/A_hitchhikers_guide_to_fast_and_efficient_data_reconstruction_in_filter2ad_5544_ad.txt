System. The requested block is reconstructed on the ﬂy
by the RAID-Node via a MapReduce job. The execution
of this reconstruction operation is identical to that in the
reconstruction process discussed above.
5.2 Hitchhiker in HDFS
We implemented Hitchhiker in HDFS making use of the
new erasure code (§3) and the hop-and-couple technique
(§4) proposed in this paper. We implemented all three
versions of the proposed storage code: Hitchhiker-XOR,
Hitchhiker-XOR+, and Hitchhiker-nonXOR. This required
implementing new Encoder, Decoder and Parallel-Reader
6In the context of HDFS, the term block corresponds to a
unit and we will use these terms interchangeably.
… . . . . . . . . . . . . . . . . . . . . . . . . . . . coupled bytes  (encoded together) unit 1 unit 2 unit 10 unit 14 unit 3 … unit 4 unit 12 unit 13 … . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . hop length unit 1 unit 2 unit 10 unit 11 unit 14 … data units parity units unit 3 … unit 4 unit 12 unit 13 . . . . . . . . . . . . . . . . . . . . . . . . 1 byte coupled bytes  (encoded together) (a) coupling adjacent bytes to form stripes (b) hop-and-couple . . . . . . . . . unit 11 data units parity units the encoder computes the desired functions simply as linear
combinations of the given data units (for example, we com-
pute f2(a1, a2, a3, 0 . . . , 0) simply as c2,1a1 ⊕ c2,2a2 ⊕ c2,3a3).
Decoder: As seen in §3.3, during reconstruction of any
of the ﬁrst nine units, the ﬁrst byte is reconstructed by per-
forming an RS decoding of the data obtained in the inter-
mediate step. One straightforward way to implement this is
to use the existing RS decoder for this operation. However,
we take a diﬀerent route towards a computationally cheaper
option. We make use of two facts: (a) Observe from the
description of the reconstruction process (§3.3) that for this
RS decoding operation the data is known to be ‘sparse’,
i.e., contains many zeros, and (b) the RS code has the lin-
earity property, and furthermore, any linear combination of
zero-valued data is zero. Motivated by this observation, our
Decoder simply inverts this sparse linear combination to re-
construct the ﬁrst substripe in a more eﬃcient manner.
5.2.3 Hop-and-couple
Hitchhiker uses the proposed hop-and-couple technique to
couple bytes during encoding. We use a hop-length of half
a block since the granularity of data read requests and re-
covery in HDFS is typically an entire block. As discussed
in §4, this choice of the hop-length minimizes the number of
discontiguous reads. However, the implementation can be
easily extended to other hop-lengths as well.
When the block size is larger than the buﬀer size, coupling
bytes that are half a block apart requires reading data from
two diﬀerent locations within a block. In Hitchhiker, this is
handled by the Parallel-Reader.
5.2.4 Data read patterns during reconstruction
During reconstruction in Hitchhiker, the choice of the
blocks from which data is read, the seek locations, and the
amount of data read are determined by the identity of the
block being reconstructed. Since Hitchhiker uses the hop-
and-couple technique for coupling bytes to form stripes dur-
ing encoding, the reads to be performed during reconstruc-
tion are always contiguous within any block (§4).
For reconstruction of any of the ﬁrst nine data blocks in
Hitchhiker-XOR+ or Hitchhiker-nonXOR or for reconstruc-
tion of any of the ﬁrst six data blocks in Hitchhiker-XOR,
Hitchhiker reads and downloads two full blocks (i.e., both
substripes) and nine half blocks (only the second substripes).
For example, the read patterns for decoding blocks 1 and 4
are as shown in Fig. 9a and 9b respectively. For reconstruc-
tion of any of the last four data blocks in Hitchhiker-XOR,
Hitchhiker reads and downloads three full blocks (both sub-
stripes) and nine half blocks (only the second substripes).
For recovery of the tenth data block in Hitchhiker-XOR+ or
Hitchhiker-nonXOR, Hitchhiker reads half a block each from
the remaining thirteen blocks. This read pattern is shown
in Fig. 9c.
6. EVALUATION
6.1 Evaluation Setup and Metrics
We evaluate Hitchhiker using two HDFS clusters at Face-
book: (i) the data-warehouse cluster in production com-
prising multiple thousands of machines, with ongoing real-
time traﬃc and workloads, and (ii) a smaller test cluster
comprising around 60 machines. In both the clusters, ma-
chines are connected to a rack switch through 1Gb/s Ether-
Figure 8: Relevant modules in HDFS-RAID. The
execution ﬂow for encoding, degraded reads and re-
construction operations are shown. Hitchhiker is
implemented in the shaded modules.
details
described
emphasize
below pertain
in HDFS. We
is generic and supports all values of
modules (shaded in Fig. 8), entailing 7k lines of code. The
implementation
to
parameters (k = 10, r = 4) which are the default
that, however,
parameters
Hitchhiker
the
parameters k and r.
5.2.1 Hitchhiker-XOR and Hitchhiker-XOR+
is exactly as described in §3.1 and §3.2 respectively.
5.2.2 Hitchhiker-nonXOR
The implementation of these two versions of Hitchhiker
Hitchhiker-nonXOR requires ﬁnite ﬁeld arithmetic oper-
ations to be performed in addition to the operations per-
formed for the underlying RS code. We now describe how
our implementation executes these operations.
Encoder:
As seen in Fig. 6,
in addition to the
underlying RS code, the Encoder needs to compute the
functions f2(a1,a2,a3,0...,0), f2(0,...,0,a4,a5,a6,0...,0), and
f2(0,...,0,a7,a8,a9,0), where f2 is the second parity function
of the RS code. One way to perform these computations is
to simply employ the existing RS encoder. This approach,
however, involves computations that are superﬂuous to our
purpose: The RS encoder uses an algorithm based on
polynomial computations [16] that inherently computes all
the four parities f1(x), f2(x), f3(x), and f4(x) for any
given input x. On the other hand, we require only one of
the four parity functions for each of the three distinct
inputs
and
(0,...,0,a7,a8,a9,0). Furthermore, these inputs are sparse,
i.e., most of the input bytes are zeros.
(0,...,0,a4,a5,a6,0...,0)
(a1,a2,a3,0...,0),
ity function f(cid:96) can thus be speciﬁed as f(cid:96)(x) =(cid:76)10
Our Encoder implementation takes a diﬀerent approach,
exploiting the fact that RS codes have the property of lin-
earity, i.e., each of the parity bytes of an RS code can be
written as a linear combination of the data bytes. Any par-
j=1 c(cid:96),jxj
for some constants c(cid:96),1, . . . , c(cid:96),10. We ﬁrst inferred the values
of these constants for each of the parity functions (from the
existing “black-box” RS encoder) in the following manner.
We ﬁrst fed the input [1 0 ··· 0] to the encoder. This gives
the constants c1,1, . . . , c4,1 as the values of the four parities
respectively in the output from the encoder. The values of
the other constants were obtained in a similar manner by
feeding diﬀerent unit vectors as inputs to the encoder. Note
that obtaining the values of these constants from the “black-
box” RS encoder is a one-time task. With these constants,
RAID%File%System%Erasure%code%Encoder%Erasure%code%Decoder%Parallel%Reader%HDFS-RAID HDFS RAID%Node%Degraded read Recovery Encoding Legend Figure 9: Data read patterns during reconstruc-
tion of blocks 1, 4 and 10 in Hitchhiker-XOR+: the
shaded bytes are read and downloaded.
net links. The higher levels of the network tree architecture
have 8Gb/s Ethernet connections.
We compare Hitchhiker and RS-based HDFS-RAID in
terms of the time taken for computations during encoding
and reconstruction, the time taken to read the requisite data
during reconstruction, and the amount of data that is read
and transferred during reconstruction.7 Note that in par-
ticular, the computation and the data read times during re-
construction are vital since they determine the performance
of the system during degraded reads and recovery.
6.2 Evaluation Methodology
The encoding and reconstruction operations in HDFS-
RAID, including those for degraded reads and recovery, are
executed as MapReduce jobs. The data-warehouse cluster
does not make any distinction between the MapReduce jobs
ﬁred by the RAID-Node and those ﬁred by a user. This
allows us to perform evaluations of the timing metrics for
encoding, recovery, and degraded read operations by run-
ning them as MapReduce jobs on the production cluster.
Thus evaluation of all the timing metrics is performed in
the presence of real-time production traﬃc and workloads.
We deployed Hitchhiker on a 60-machine test cluster in
one of the data centers at Facebook, and evaluated the end-
to-end functionality of the system. Tests on this cluster ver-
iﬁed that savings in network and disk traﬃc during recon-
struction are as guaranteed in theory by Hitchhiker’s erasure
code.
For all the evaluations, we consider the encoding param-
eters (k = 10, r = 4), a block size of 256MB (unless men-
tioned otherwise), and a buﬀer size of 1MB. These are the
default parameters in HDFS-RAID. Moreover, these are the
parameters employed in the data-warehouse cluster in pro-
duction at Facebook to store multiple tens of Petabytes.
In the evaluations, we show results of the timing metrics
for one buﬀer size. This is suﬃcient since the same opera-
tions are performed repeatedly on one buﬀer size amount of
data until an entire block is processed.
6.3 Computation time for degraded reads &
recovery
Fig. 10 shows the comparison of computation time dur-
ing data reconstruction (from 200 runs each). Note that
(i) in both Hitchhiker-XOR+ and Hitchhiker-nonXOR, re-
construction of any of the ﬁrst nine data blocks entails same
amount of computation, (ii) reconstruction of any data block
7Both systems read the same data during encoding, and are
hence trivially identical on this metric.
Figure 10: A box plot comparing the computation
time for reconstruction of 1MB (buﬀer size) from
200 runs each on Facebook’s data-warehouse clus-
ter with real-time production traﬃc and workloads.
(HH = Hitchhiker.)
in Hitchhiker-XOR is almost identical to reconstruction of
block 1 in Hitchhiker-XOR+. Hence, for brevity, no separate
measurements are shown.
This
from §3)
is because (recall
We can see that Hitchhiker’s erasure codes perform
faster reconstruction than RS-based HDFS-RAID for any
data block.
the
reconstruction operation in Hitchhiker requires performing
the
the resource intensive RS decoding only for half
substripes as compared to RS-based HDFS-RAID.
In
Hitchhiker, the data in the other half of substripes is
reconstructed by performing either a few XOR operations
(under Hitchhiker-XOR and Hitchhiker-XOR+) or a few
ﬁnite-ﬁeld operations (under Hitchhiker-nonXOR). One
can also see that Hitchhiker-XOR+ has 25% lower
computation time during reconstruction as compared to
Hitchhiker-nonXOR for the ﬁrst nine data blocks; for block
10, the time is almost identical in Hitchhiker-XOR+ and
Hitchhiker-nonXOR (as expected from theory (§3)).
6.4 Read time for degraded reads & recovery
Fig. 11a and Fig. 11b respectively compare the median
and the 95th percentile of the read times during reconstruc-
tion for three diﬀerent block sizes: 4MB, 64MB, and 256MB
in Hitchhiker-XOR+. The read patterns for reconstruction
of any of the ﬁrst nine blocks are identical (§3.2).
In the median read times for reconstruction of blocks 1-9
and block 10 respectively, in comparison to RS-based HDFS-
RAID, we observed a reduction of 41.4% and 41% respec-
tively for 4MB block size, 27.7% and 42.5% for 64MB block
size, and 31.8% and 36.5% for 256MB block size. For the
95th percentile of the read time we observed a reduction of
35.4% and 48.8% for 4MB, 30.5% and 29.9% for 64MB, and
30.2% and 31.2% for 256MB block sizes.
The read pattern of Hitchhiker-nonXOR is identical to
Hitchhiker-XOR+, while that of Hitchhiker-XOR is the
same for the ﬁrst six blocks and almost the same for the
remaining four blocks. Hence for brevity, we plot the
statistics only for Hitchhiker-XOR+.
Although Hitchhiker reads data from more machines as
compared to RS-based HDFS-RAID, we see that it gives a
superior performance in terms of read latency during recon-
struction. The reason is that Hitchhiker reads only half a
block size from most of the machines it connects to (recall
from §5) whereas RS-based HDFS-RAID reads entire blocks.
block 10 block 11 block 14 256 MB block 13 block 12 block 9 block 8 block 7 block 6 block 5 block 4 block 2 block 1 block 3 256 MB 256 MB (a) block 1 (b) block 4 (c) block 10 data parity (a) Median
Figure 12: A box plot comparing the computa-
tion time for encoding of 1MB (buﬀer size) from
200 runs each on Facebook’s data-warehouse clus-
ter with real-time production traﬃc and workloads.
(HH = Hitchhiker.)
6.7 Deployment and testing
(b) 95th percentile
Figure 11: Total read time (in seconds) during re-
construction from 200 runs each on Facebook’s data-
warehouse cluster with real-time production traﬃc
and workloads. (HH = Hitchhiker.)
6.5 Computation time for encoding
Fig. 12 compares the computation time for the encoding
operation. Hitchhiker entails higher computational over-
heads during encoding as compared to RS-based systems,
and Hitchhiker-XOR+ and Hitchhiker-XOR are faster than
the Hitchhiker-nonXOR. This is expected since Hitchhiker’s
encoder performs computations in addition to those of RS
encoding. §7 discusses the tradeoﬀs between higher encod-
ing time and savings in other metrics.
6.6 Statistics of single block reconstruction
Hitchhiker optimizes data reconstruction in scenarios
when only one block of a stripe is unavailable. If multiple
blocks belonging to a stripe are unavailable, Hitchhiker
performs reconstruction in a manner identical to RS-based
HDFS-RAID, by reading and downloading 10 entire blocks.
We collected measurements of the number of missing
blocks per stripe across six months in the data-warehouse
cluster in production at Facebook, which stores multiple
Petabytes of RS-coded data. We observed that among all
the stripes that had at least one block to be reconstructed,
98.08% of them had exactly one such block missing, 1.87%
had two blocks missing, and the number of stripes with
three or more such blocks was 0.05%. The measurements
thus reveal single block reconstructions to be by far the
most common scenario in the system at hand.
One might alternatively think of lazily performing recon-
struction by waiting for multiple blocks in a stripe to fail and
then reconstructing them all at once, in order to amortize
the cost of disk reads and download during reconstruction.
However, such a waiting approach may not be feasible when
read performance is key: a failed systematic block must be
repaired quickly to serve future read requests. Furthermore,
degraded reads are performed on a single block in real time,
and there is no equivalent of waiting for more failures.
We deployed HDFS-RAID with both Hitchhiker and the
RS-based system on a 60-machine test cluster at Facebook
in order to verify Hitchhiker’s end-to-end functionality. We
created multiple ﬁles with a block size of 256MB, and
encoded them separately using Hitchhiker and RS-based
HDFS-RAID. The block placement policy of HDFS-RAID
ensures that the 14 blocks of an encoded stripe are all
stored on diﬀerent machines. We then forced some of these
machines to become unavailable by stopping HDFS related
scripts running on them, and collected the logs pertaining
to the MapReduce jobs that performed the reconstruction
operations. We veriﬁed that all reconstruction operations
were successful. We also conﬁrmed that the amount of data
read and downloaded in Hitchhiker was 35% lower than in
RS-based HDFS-RAID, as guaranteed by the proposed
codes. We do not perform timing measurements on the
test cluster since the network traﬃc and the workload on
these machines do not
(production)
scenario.
Instead, we evaluated these metrics directly on
the production cluster itself (as discussed earlier).
the real
reﬂect
7. DISCUSSION ON TRADE-OFFS