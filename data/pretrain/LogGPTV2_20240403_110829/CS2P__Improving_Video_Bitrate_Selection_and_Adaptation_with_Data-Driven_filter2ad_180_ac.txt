The workﬂow of session clustering algorithm in CS2P to
ﬁnd the session features yielding the best prediction is as
follows:
1. Pick a given set of features Ms from all possible feature
combinations (i.e., 2n subsets of n features, the candi-
date session features are shown in Table 2) and time win-
dows. Speciﬁcally, the possible time windows include
time windows of certain history length (i.e., last 5, 10,
30 minutes to 10 hours) and those of same time of day
(i.e., same hour of day in the last 1-7 days).
2. Once the set of features Ms is picked for s, CS2P aggre-
gates previous sessions based on Ms. For instance, given
Ms = (cid:104)ISP , 1hr(cid:105), CS2P will aggregate all previous ses-
sions who are in the same ISP as s and happened in the
last 1 hour. Let this set of previous sessions be denoted
by Agg(Ms , s).
3. CS2P predicts the throughput of s by Pred (s) =
F (Agg(Ms , s)), where F (S) is the predicted through-
put by using the sessions in S. The prediction algorithm
F will be shown in §5.2.
The goal is to minimize the absolute normalized predic-
tion error,
Err (Pred (s), sw ) =
|Pred (s) − sw|
sw
,
(1)
where sw is the actual throughput of s.
Figure 7: Overview of HMM model
The key component of algorithm is how to map each ses-
s , that yields the lowest predic-
sion s to a set of features M ∗
tion error. That is
M ∗
M
s = arg min
Err (F (Agg(M , s)), sw )
(2)
We take a data-driven approach and ﬁnd the best set of
features for prediction over a set of previous sessions Est(s)
(deﬁned shortly). Formally, the process can be written as
following:
M ∗
Err (F (Agg(M , s(cid:48))), s(cid:48)
w )
s = arg min
(cid:88)
1
|Est(s)|
M
s(cid:48)∈Est(s)
(3)
Est(s) should include sessions that are likely to share the
best prediction model with s. In our dataset, Est(s) con-
sists of sessions that match features in Table 2 with s and
happened within 2 hours before s occurred.
To make the prediction Pred (s) reliable, CS2P ensures
that Pred (s) is based on a substantial number of sessions
in Agg(Ms , s). Therefore, if Ms yields Agg(Ms , s) with
less than a threshold number of sessions (e.g., 100), it will
remove that session cluster from consideration. Note that the
model can regress to the “global” model (i.e., model trained
with all the previous sessions), if no suitable clustering could
be achieved.7
Note that this session clustering step is similar to that in
CFA [29], but the goal and criteria of clustering in the two
schemes are different. CFA determines the critical feature
set according to the QoE similarity, whereas in CS2P the op-
timal session clusters are chosen based on throughput pre-
diction accuracy.
5.2 HMM training and online prediction
Next we present a simple but effective HMM-based predic-
tor capturing the state-transition behaviour (Observation 2
in §3)) in each cluster Agg(M ∗
Modeling: The throughput predictor in CS2P is based on
a Hidden Markov Model (HMM). Figure 7 provides a high-
level overview of the HMM. The intuition behind the use of
HMM in our context is that the throughput depends on the
hidden state; e.g., the number of ﬂows sharing the bottleneck
link and link capacity. By carefully analyzing the behaviors
of previous sessions with the same value of features in M ∗
s ,
we try to capture the state transitions and the dependency
between the throughput vs. the hidden state, and propose a
robust and efﬁcient throughput predictor.
7The probability of sessions using global model in our
dataset is ≤4%.
s , s).
Figure 8: Example of hidden-markov model of session
clusters.
We start by formally deﬁning the HMM. Let Wt be
the random variable representing the network throughput at
epoch t, wt be the actual throughput measured from the net-
work, ˆWt be the predicted value of Wt.
We assume the throughput Wt evolves according to some
hidden state variables Xt ∈ X , where X = {x1,··· , xN}
denotes the set of possible discrete states and N = |X| the
number of states. Intuitively, the states reﬂect some discrete
changes in the structure of the network or users, e.g., number
of users at a bottleneck link. Given that state Xt is a random
variable, we denote its probability distribution as a vector
πt = (P(Xt = x1),··· , P(Xt = xN )).
The key assumption in HMM is that the state evolves as
a Markov process where the probability distribution of the
current state only depends on the state of the previous epoch,
i.e., P(Xt|Xt−1,··· , X1) = P(Xt|Xt−1). We denote the
transition probability matrix by P = {Pij}, where Pij =
P(Xt = xi|Xt−1 = xj). According to Markov property,
πt+τ = πtP τ
(4)
Given the hidden state Xt, we assume the pdf of throughput
Wt (namely, the emission pdf) is Gaussian:
Wt|Xt = x ∼ N (µx, σ2
x)
(5)
Note that HMM is a general model which could work with
any emission pdf other than Gaussian. However, here we use
Gaussian emission as it proves to provide high prediction
accuracy in our dataset and its computational simplicity.
Figure 8 gives an example of a 3-state HMM of one ses-
sion cluster in our dataset. Each state follows a Gaussian
distribution of throughput denoted by the mean of the distri-
bution and its standard deviation N (µ, σ2). The transition
probability is computed between every pair of states. For in-
stance, suppose session throughput is currently at State 1 in
Figure 8, then for the next epoch it will stay at the same state
with probability of 97.2% and switch to State 2 and 3 respec-
tively with probabilities of 1.2% and 1.6%. Note in Figure 8,
the probability of inter-state transition and the standard de-
viation of throughput within each state are small, suggesting
clear stateful behaviors for the throughput evolution.
We introduce notations before proceeding to training and
prediction: For simplicity, we use W1:t = {W1,··· , Wt} to
denote throughput from epoch 1 to epoch t. Let πt1|1:t0 =
(P(Xt1 = x1|W1:t0),··· , P(Xt1 = xN|W1:t0 )) be the pdf
vector of the hidden state Xt1, given throughput from epoch
ThroughputHidden State……State TransitionEmission……State 1N(0.43,0.052)MbpsState 2N(2.41,1.492)MbpsState 3N(1.20,0.102)Mbps0.9720.8760.9700.0550.0120.0160.0200.0690.0101 to t0. For example, πt|1:t−1 is the pdf of state Xt given the
throughput up to epoch t − 1.
Ofﬂine training: Given the number of states N, we can use
training data in Agg(M ∗
s , s) to learn the parameters of HMM
for this particular cluster, θHMM = {π0, P,{(µx, σ2
x), x ∈
X}} via the expectation-maximization (EM) algorithm [18].
Note that the number of states N needs to be speciﬁed.
There is a tradeoff here in choosing suitable N. Smaller N
yields simpler models, but may be inadequate to represent
the space of possible behaviors. On the other hand, a large
N leads to more complex model with more parameters, but
may in turn lead to overﬁtting issues. As described in §7.1,
we use cross-validation to learn this critical parameter.
else
if t = 1 (initial epoch) then
Initialize π1
ˆW1 = Median(Agg(M ∗
Algorithm 1 Online prediction in CS2P
1: Let t be epoch id
2: for t = 1 to T do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13: end for
end if
Bitrate selection based on prediction ˆWt
Obtain throughput measurement wt
Update πt|1:t = πt|1:t−1◦e(wt)
(πt|1:t−1◦e(wt))·1
s , s))
πt|1:t−1 = πt−1|1:t−1P
ˆWt = µx, where x = arg maxx∈X πt|1:t−1(x)
Online prediction:
In the ofﬂine training stage, we ﬁnd
the 1) set of critical features and 2) corresponding prediction
model for each session in the training dataset. In the online
prediction stage, a new session is mapped to the most similar
session in the training dataset, which matches all (or most of)
the features with the session under prediction. We then use
the corresponding HMM of that session to make predictions.
The online prediction algorithm using HMM is shown in Al-
gorithm 1. At a high level, it involves predicting throughput
for the next epoch using HMM, as well as updating HMM
state once the actual throughput is measured.
Next, we discuss the key steps in our prediction approach:
• Prediction (initial epoch): HMM relies on the throughput
measurement of the “current” epoch to predict through-
put of the “next” epoch, however for the initial epoch
there is no historical information in this session. As such,
CS2P predicts the initial throughput of session s simply
by the median throughput of sessions in Agg(M ∗
s , s) that
match s on the best set of features of M ∗
s and are in the
time range of M ∗
s , i.e.,
s , s))
ˆW1 = Median(Agg(M ∗
(6)
Note that the throughput prediction of the initial epoch is
computed in the Prediction Engine and sent to the video
servers (for server-side bitrate adaptation) or clients (for
client-side bitrate adaptation) together with the trained
prediction models.
• Prediction (midstream epoch): At epoch t, given updated
pdf of HMM state πt−1|1:t−1, we can compute the state
pdf at current epoch according to Markov property:
πt|1:t−1 = πt−1|1:t−1P
(7)
The throughput prediction ˆWt is given by the maximum
likelihood estimate (MLE):
P(Xt = x|W1:t−1)
ˆWt = µx,
x = arg max
x∈X
(8)
• Update HMM: Once we observe the actual throughput
wt, we use this information to update the state of HMM
πt, so that it reﬂects the most up-to-date information of
the network. Namely, given actual throughput Wt = wt,
and πt|1:t−1, we want to compute πt|1:t using the follow-
ing equations:
πt|1:t−1 ◦ e(wt)
πt|1:t =
(πt|1:t−1 ◦ e(wt)) · 1
(9)
where e(wt) = (f (wt|Xt = x1),··· , f (wt|Xt = xM ))
is the emission probability vector, f (·) is the Gaussian
pdf, ◦ denotes entry-wise multiplication, or Hadamard
product [6] of the two vectors.
5.3 Player integration
CS2P can be used both with server-side [14, 20] and client-
side adaptation solutions [30, 47].
In the server-side solution, video content servers inter-
act with the Prediction Engine for the models and initial
throughput predictions for each cluster, and are responsible
of choosing the bitrate for all the sessions. The advantage of
this server-based solution is that it requires little updates or
modiﬁcations on the clients. However, the centralized server
needs to collect throughput measurements from all clients
and compute bitrates for each video session, making it a po-
tential bottleneck. Fortunately, we ﬁnd that the online pre-
diction in CS2P is very light-weight and our deployed server
(Intel i7-2.2GHz, 16GB RAM, Mac OSX 10.11) can process
about 150 predictions per second.
Bitrate adaptation can also be done by each video client.
Here, each video client downloads its own HMM and initial
throughput prediction from Prediction Engine and runs the
model for real-time throughput prediction and bitrate adap-
tation by itself. The advantage of this decentralized method
is that the client is often in the best position to quickly detect
performance issues and respond to dynamics. The disadvan-
tage is that it requires client to maintain its own HMM. For-
tunately, the computation complexity and storage require-
ment of HMM in CS2P are low, and it is feasible to do
that on the client. On our test client (Intel i7-2.8GHz, 8GB
RAM, Mac OSX 10.9), each prediction requires <10 mil-
liseconds (only needs two matrix multiplication operations),
and <5KB memory is used to keep the HMM.
For midstream bitrate selection, we use the Model Pre-
dictive Control (MPC) strategy formulated by recent ef-
forts [47],8 that takes throughput prediction, current bitrate
8Speciﬁcally, we refer to FastMPC [47].
and buffer occupancy as inputs and solves an exact integer
programming problem to decide the bitrate for the next few
epochs. For brevity, we do not provide more details of MPC
and its advantages over pure Rate-based (RB) or Buffer-
based (BB) schemes, and refer readers to prior work [47].
However, MPC cannot be utilized for the initial bitrate se-
lection of the session due to the lack of the current bitrate
setting and buffer occupancy measurement. Thus, to select
bitrate for the ﬁrst chunk, we simply select the highest sus-
tainable bitrate below the predicted initial throughput.
6 Implementation
In this section, we describe our implementation. Our imple-
mentation follows the server-side solution of CS2P, i.e., the
server makes throughput prediction for each session. We in-
tegrate the functionalities of Prediction Engine into the video
server, which is responsible of training the HMMs for each
session clusters and then using the trained models to make
throughput predictions for the video players. On receiving
the throughput prediction from the server, video player runs
the bitrate selection algorithms to achieve bitrate adaptation.
• Video Player: Our implementation of video player is
based on Dash.js, an open-source implementation of
MPEG-DASH standard using client-side JavaScript to
present a ﬂexible and potentially browser independent
DASH player [3]. The key components of Dash.js con-
trolling bitrate selection are BufferController and Abr-
Controller. We make several minor modiﬁcations to
these two components. First, in BufferController, bitrate
decision is made before the request of each video chunk
(including the initial chunk). Whenever the client wants
to make bitrate decision, it sends a POST request (con-
taining the actually throughput of the last epoch) to the
server and fetches the result of throughput prediction in
approximate 500 milliseconds. Second, we implement
different bitrate algorithms (e.g., MPC, RB, BB, ﬁxed)
in AbrController, replacing the default rule-based deci-
sions. When the video is completely loaded, log informa-
tion including QoE, bitrates, rebuffer time, startup delay,
predicted/actual throughput and bitrate adaptation strat-
egy is sent to a log server.
• Server: On the server side, we choose the Node.js as
the basis of HTTP server implementation. Node.js is an
event-driven, non-blocking I/O, lightweight and efﬁcient
network framework [11]. We implement the key func-