: Malicious Installer
10−2
10−1
Figure 4: Sensitivity analysis to determine the normality threshold for each
software installer in the experiment. We use a log-10 scale for x-axis.
considerable margins such that SIGL’s detection performance
generally does not depend on ﬁnding a precise threshold.
Fig. 4 shows the average (circled mark), minimum, and
maximum (two ends of the error bar) anomaly scores for be-
nign (blue) and malicious (red) installers for each experiment.
None of the installs have overlapping benign and malicious
ranges, although the precise break between the ranges is, in
fact, installer speciﬁc. However, many of the benign installers
have scores orders of magnitude smaller than those of the
malicious installers. For example, compared to the malicious
NotePad++ installer with the smallest anomaly score (Fig. 4),
even the benign installer with the largest score has a value two
orders of magnitude smaller. Such liberal margins not only
make it practical to set anomaly thresholds but also indicate
the likelihood of an installer being benign/malicious.
5.8 Robustness Against Data Contamination
So far, we have assumed that anomaly-free data is avail-
able for training, but this assumption does not hold in most
real-life scenarios. On the contrary, real-world data often con-
tains noise or undetected anomalies (i.e., contaminations) that
potentially affect detection performance [7]. Hence, a fully
unsupervised learning system requires a certain degree of ro-
bustness that minimizes the need for weak labeling of benign
data [42]. We evaluate the effects of anomaly contaminations
in the training set for each software installer in Table 2.
Experimental Setup. We contaminated 5%, 10%, 15%, 20%,
and 25% of the original training set with malware data from
the test set and rebuilt the model for each level of contamina-
tion. Malware data used for training is also included in the
test set to evaluate SIGL’s robustness against anomaly data
pollution. We use the Area Under the Receiver Operating
Characteristics (ROC) curve, or AUC, to compare anomaly
detection results for each installer (Fig. 5). AUC, ranging
between 0 and 1, measures the quality of model prediction
regardless of classiﬁcation threshold.
Experimental Results. Fig. 5 shows that in general, SIGL is
tolerant to contamination in training data. In the majority of
cases, the AUC stays above 0.90, even when contamination
is severe (e.g., 25%). We notice that applications with lower
performance in § 5.3 (e.g., FireFox) are more likely to be af-
fected by contamination, as their benign installation behavior
is already difﬁcult to learn even with clean training data.
5.9 Robustness Against Adversarial Attacks
With the growing popularity of graph-based classiﬁcation
methods in security applications, adversarial attacks on graph
data are likely to become increasingly common for an attacker
to evade those methods [77]. However, there exist only a few
studies [17,77,84,85] on this topic, with the majority focusing
on citation networks (e.g., Cora [50], Citeseer [9]) and social
networks (e.g., Facebook, Twitter [78]), and designed only for
a particular type of graph neural networks (e.g., GCN [85]).
To demonstrate SIGL’s robustness against adversarial at-
tacks, we investigate two realistic attack scenarios from a prac-
tical, systems perspective. Different from prior approaches
that focus on network graph attacks, our scenarios require
a distinct set of attacker behavior (and thus resulting graph
perturbations), constrained by the threat model (§ 3), our neu-
ral network architecture and classiﬁcation method, but more
importantly, the feasibility of system manipulations.
Background. We consider the restrict black-box attack (RBA)
and practical black-box attack (PBA) adversarial settings [17]
3. In RBA, the attacker must perform adversarial graph mod-
iﬁcations without any knowledge of our model, given only
sampled benign and attack graphs. The PBA scenario relaxes
the restrictions on model knowledge by disclosing discrete
prediction feedback from the target classiﬁer (but not any
other information e.g., the normality threshold). Our threat
model assumes the integrity of data provenance, so the at-
tacker cannot directly modify SIGs. They can manipulate
graph structures (i.e., structure attack) and node feature vec-
tors (i.e., feature attack) only by manipulating software instal-
lation process, while ensuring successful malware execution.
We follow state-of-the-art graph-based adversarial machine
learning literature [77, 84] to generate adversarial attack
graphs by 1) adding or removing edges, and 2) modifying
node attributes on the malicious graphs in Table 2. As dis-
cussed in detail below, we also deﬁne an equivalency indica-
tor [17] for each attack setting to restrict graph perturbations
that are realistically available to the attacker (e.g., the attacker
cannot add a directed edge between two ﬁle nodes).
Experimental Setup (RBA). We deﬁne the equivalency in-
dicator as any allowed graph modiﬁcations on nodes/edges
related to the malicious processes. The attacker can easily
identify those graph components given both benign and at-
tack graphs. Without any additional information, the attacker
3We do not consider the white-box attack (WBA) setting in which the
attacker can access any model information, including model parameters and
gradient information, since such accessibility is rarely possible in real-life
situations [12].
USENIX Association
30th USENIX Security Symposium    2355
C
U
A
1
0.9
0.75
0.5
0.25
0
0
FireFox
FileZilla
PWSafe
MP3Gain
ShotCut
TeamViewer
Foobar
7Zip
TurboVNC
WinMerge
Launchy
Skype
WinRAR
DropBox
Slack
Flash
OneDrive
NotePad++
ICBC Anti-Phishing
ESET AV Remover
0 .0 5
0 .1
0 .1 5
0 .2
0 .2 5
0
0 .0 5
0 .1
0 .2
0 .1
0 .1 5
Contamination Percentage
0 .2 5
0 .0 5
0
0 .1 5
0 .2
0 .2 5
0
0 .0 5
0 .1
0 .1 5
0 .2
0 .2 5
Figure 5: AUC result breakdown for each software installer with various degrees of data contamination.
8
8
9
.
0
3
6
9
.
0
5
6
9
.
0
1
3
9
.
0
7
7
9
.
0
9
6
9
.
0
5
6
9
.
0
6
9
.
0
6
8
9
.
0
1
6
9
.
0
5
6
9
.
0
9
5
9
.
0
4
2
9
.
0
1
1
9
.
0
6
1
9
.
0
1
1
9
.
0
1
4
4
9
.
0
2
4
9
.
0
2
4
9
.
0
C
U
A
1
0.5
0
FireFox
TeamViewer
WinMerge
Launchy
Slack
No Attack
Feature Attack
Structure Attack
Combined Attack
Figure 6: AUC result breakdown for software installers affected by RBA.
is empirically better off to focus on malicious process nodes
that typically receive high anomaly scores and inﬂuence graph
classiﬁcation (§ 4.5). Conceptually, this is equivalent to adver-
sarial attacks in node classiﬁcation problems, where malicious
process nodes are the attacker’s target nodes. Prior studies
have demonstrated that manipulations on target nodes result
in signiﬁcantly more adversarial damage [12, 84].
One strategy is to disguise malicious processes to mimic
the benign ones. We design a feature attack, a structure attack,
and a combination of both. In the feature attack, we modify
the malicious process’ node attributes to be the same as those
of the benign ones, effectively aligning feature vectors of both
malicious and benign nodes (§ 4.3). In the structure attack,
we ensure that the malicious processes read/write the same
number of ﬁles/sockets and fork the same number of child
processes, so that their local structures approximate those
of the benign processes. In the combination of both attacks,
we further make sure that feature vectors of ﬁles/sockets/pro-
cesses related to the malicious processes are similar to those
related to the benign processes (e.g., by manipulating ﬁle
node attributes). We evaluate the effects of all attack vectors
for each software installer in Table 2.
Experimental Results (RBA). Fig. 6 shows the results for
only those software installers affected by at least one attack
vector. AUCs of the other installers in Table 2 remain un-
changed. We see that the efﬁcacy of the feature and structure
attack in isolation is installer independent: while TeamViewer
and Slack are slightly more vulnerable to the structure attack,
the rest are more affected by the feature attack. Combin-
ing both feature and structure attacks improves attack per-
formance, but overall, SIGL is robust to adversarial attack
in this scenario. SIGL’s use of deep graph learning means
that changes in one part of the graph can have far-reaching
consequences. Manipulating anomalous process nodes does
)
3
−
0
1
(
e
r
o
c
S
y
l
a
m
o
n
A
4
3
2
1
0
1
2
3
4
5
6
7
8
Original Adversarial
9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24
Graph Instance
Figure 7: Anomaly scores of Skype attack graphs affected by PBA.
not remove all the effects of such nodes; the benign nodes to
which they connect are also affected by their originally mali-
cious behavior [84]. The attackers could strengthen RBA if
they can also accurately identify target nodes that are not ma-
licious but have been inﬂuenced by the malicious processes,
but such information is not available in this setting.
Experimental Setup (PBA). PBA allows the attacker to ob-
tain prediction feedback from the classiﬁer, so the attacker
can iteratively add/remove edges or modify node features in
the graph, until the resulting graph produces a false nega-
tive from SIGL’s model. We will generate such a PBA attack
using reinforcement learning (RL). Our goal is to build an
RL-model that takes as input a SIG produced by an existing
malware package and produces, as output, a SIG that SIGL
improperly classiﬁes as benign. We constrain the changes that
the RL-model can make on the graph to structural changes
that can be produced according to the criteria discussed in the
previous section (i.e., that the attackers can produce manipu-
lated graphs only by changing their attack implementation),
and deﬁne the equivalency indicator as the minimal number
of such modiﬁcations within a ﬁxed budget [84]. We adopt a
hierarchical reinforcement learning (RL) based attack method
through Q-learning to learn a generalizable attack policy over
graph structure [17]. We build our RL-model using a subset
of the malware of a single application (we randomly chose
5% of the Skype malware installations) and then evaluate the
model using the full suite of malware from Table 2.
Experimental Results (PBA). The adversarial attacker tries
to increase the false negative rate (FNR) of the attack graphs,
but we observe no such changes for Skype nor for the ma-
jority of the other installers in Table 2. The two exceptions
are TeamViewer and FireFox; TeamViewer exhibits more
FNs for one attack graph, and FireFox exhibits fewer FNs
2356    30th USENIX Security Symposium
USENIX Association
5
9
9
.
0
3
9
9
.
1 1 1 1 1 1 1 0
6
8
9
.
1 1 1 0
7
7
9
.
0
1 1 1 1
3
3
9