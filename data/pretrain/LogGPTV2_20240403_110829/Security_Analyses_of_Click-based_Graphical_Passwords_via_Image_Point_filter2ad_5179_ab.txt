salience as the only means to reduce the number of candidates. Our 
study (see Section 7.5.1) suggests that the salience calculated with 
the  state-of-the-art  visual  attention  model  does  not  predict  click-
points of PCCP passwords well. Therefore, prior automated attacks 
cannot build effective dictionaries to mount successful attacks on 
PCCP.  
To summarize, PCCP is robust to all prior dictionary attacks. Such 
strength of PCCP is expected since it was designed to resist these 
attacks in the first place.  
A  click-point  cannot  be  re-clicked  exactly.  Any  click  within  a 
certain  range  of  the  click-point  should  be  accepted  as  the  click-
point. This tolerable range is fine in verifying an input password if 
the  original  password  is  stored,  but  makes  such  verification 
impossible  if  only  the  hash  value  of  a  password  is  stored  since 
different  clicks  result 
in  different  hash  values.  Password 
discretization [30,31] is designed to solve this problem. Our recent 
paper  [25]  found  that  representative  discretization  schemes  leak 
significant password information. In the current paper, we do not 
exploit the weakness of password discretization at all.  
The  representative  results  of  all  prior  attacks  on  click-based 
graphical passwords are summarized in Appendix.    
2.2  Other Graphical Passwords 
The graphical password schemes used in Windows 8 and Android 
belong to a different type that traces back to Draw-A-Secret (DAS) 
[21]  wherein  a  user  draws  a  password  on  a  2D  grid,  with  the 
sequence of grid cells along the drawing path used as a password. 
Pass-Go [4] encodes grid intersection points rather than grid cells. 
A similar scheme is used in Android to unlock phones, which was 
recently attacked [6]. Background Draw-A-Secret (BDAS) [1] adds 
a  background  image  to  DAS  to  provide  cue  for  re-drawing  a 
password.  The  Windows  8’s  variant  allows  users  to  draw  a 
combination of lines, circles, and taps on an image as a password. 
This scheme was also attacked recently [5]. 
More  information  on  graphical  passwords  can  be  found  in  good 
survey papers [23,24]. 
3.  MEMORABILITY OF IMAGE POINTS  
3.1  Our Approach 
Understanding  the  memorability  of  images  as  a  whole  or  of 
individual  image  regions  is  relevant  to  the  research  of  graphical 
passwords  in  general,  but  cannot  contribute  much  to  security 
analysis  of  click-based  graphical  passwords.  We  envisage  that  if 
each point on an image can be measured and compared in terms of 
memorability, this new approach will be at the right granularity for, 
and  significantly  contribute 
to,  analyzing  such  graphical 
passwords. 
The discovery of hotspots [12,13] and click patterns [14] suggests 
that click-points tend to be memorable points. Human-seeded and 
automatic methods [13,15-18] were explored to predict click-points 
in PassPoints with a good success. However, these fine prior art did 
not  bring  out  the  concept  of  building  a  model  of  image  point 
memorability. Neither did they consider two important aspects of 
memorability,  namely  human  memory  decay  and  image  content 
semantics. 
Golofit [12] suggested that, when creating PassPoints passwords, 
users  tended  to  avoid  the  three  types  of  image  regions  shown  in 
Figure 1. Our interpretation is simply that these regions do not have 
semantic concepts that are strong enough to resist human memory 
decay,  and  thus  they  are  unmemorable  and  avoided by  the  users. 
The prior art [13,16-18] nicely ignores regions like Figure 1(A), but 
fails to realize that corners and centroids in Figures 1(B) and 1(C) 
are not memorable, for the exact reason that their methodology does 
not  take  into  consideration  both  content  semantics  and  memory 
decay. 
Based  on  these  prior  art,  we  formulate  the  notion of  image  point 
memorability.  Similar  to  [26],  we  consider  both  memorable  and 
forgettable aspects, and model the memorability of an image point 
as a noisy memory process of mentally specifying the point on the 
image, and registering the information with human memory. In this 
process, at least two main factors matter, namely, the complexity 
of  mentally  specifying  the  point,  which  determines  the  memory 
burden  for  a  user,  and 
its 
surroundings, which reflects its robustness to memory decay. These 
are the main features differentiate our approach from all the prior 
art, methodologically.  
the  point’s  distinctiveness 
in 
A
B
C
Figure 1: Three types of regions that users tend to avoid in 
creating PassPoints passwords (taken from [12]). 
3.2  A Heuristic IPM Model 
We first define that an object is an identifiable portion of an image 
that can be interpreted semantically as a single unit. An object is 
typically an image region, but can degenerate into a point when the 
object is sufficiently small. 
In  our  IPM  model,  the  notions  of  both  ‘point’  and  ‘object’  are 
essential. As a key construct in our model, we assume that, when 
1219Image Point Memorability (IPM) Model:  
•  We 
introduce  M-index 
to  measure  a  point’s  relative 
memorability  reversely:  a  lower  M-index  value  indicates 
better memorability.  
•  We assign an M-index of infinity to indistinguishable points, 
due to their poor memorability.   
•  A  distinguishable  point  carries  a  weight  describing  its 
semantic coherence level, which is 0 (High), 1 (Medium) or 2 
(Low); so does its reference object. In addition to a semantics 
weight, the object also carries a weight describing its salience 
level, which is either 0 (salient) or 2 (non-salient).  
•  A distinguishable point’s M-index is the sum of its semantics 
weight  and  its  reference  object’s  semantics  weight  and 
salience weight.  
•  Heuristically, distinguishable image points and their relative 
memorability  are  classified  into  the  following  7  categories 
according to their M-indices:  
M-Index 
Description 
0 
1 
… 
6 
HSC points of salient HSC objects. 
MSC points of salient HSC objects, HSC points of 
salient MSC objects. 
LSC points of non-salient LSC objects.  
… 
4.  HUMAN-ASSISTED MEMORABILITY 
The  IPM  model  can  be  semi-automatically  approximated  by 
finding  detectable  points  with  computer  vision  algorithms  and 
labelling  detectable  points  by  humans.  This  leads  to  our  human-
assisted memorability. We first describe a version independent of 
point locations, which fits PCCP-type graphical password schemes 
wherein  the  chance  that  a  point  is  chosen  as  a  click-point  is 
independent of the point’s location due to the randomly positioned 
viewport. Then we revise it to incorporate a point’s location.  
4.1  Detectable Points 
Detectable  points  should  be  distinct  structural  points  or  points 
located in reference to distinct structural entity. Like [16-18], we 
approximate  distinct  structural  points  with  corners  and  points 
located  in  reference  to  others  with  centroids.  A  corner  is  the 
intersection of two edges, and a centroid is the center of an object 
or its segment. However, the corner detection used in [16-18] tends 
to  detect  excessive  corners,  such  as  weak  corners  of  small  and 
irregular structures, which people unlikely select as click-points.  
We address this problem by detecting as distinct structural points 
both weakly distinctive corners of large structures (i.e., long edges) 
and strongly distinctive corners. Our centroid detection is similar 
to [18]. The details of our corner and centroid detection algorithms 
are  available  in  our  recent  paper  [25].  Detected  corners  and 
centroids form a set of detectable points for an image, as shown in 
Figure 2 with a representative image. By excluding those points that 
are unlikely click-points, we produce a smaller yet more accurate 
set of detectable points than the prior art did. 
people  mentally  specify  an  image  point,  this  specification  is  in 
reference to an object. Three other key notions in our model – two 
for capturing memorable aspects of memorability, and the third for 
forgettable aspects – are as follows.  
Semantics is important for image memorability [2,3] and relevant 
to both image points and objects. We use it to classify objects or 
points into three types:  
•  An object or point is of high semantic coherence (HSC), if it 
represents  a  clear  and  unitary  semantic  concept,  such  as 
“dog” and “tree” for objects, and “center” and “corner” for 
points. 
•  An object or point is of medium semantic coherence (MSC), 
if it is not a clear and unitary concept but is still semantically 
meaningful.  Such  an  object  or point  has  a  clear  “meaning” 
but  is  not  as  easily  pinpointed  as  above.  Instead,  it  can  be 
pinpointed via a concise description, such as “edge of pool”. 
•  An object or point is of low semantic coherence (LSC), if it 
cannot be classified into either type above. 
Salience refers to the state or quality of an object that stands out 
perceptually  from  its  neighbors.  A  salient  object  naturally  draws 
viewers’ attention, but a non-salient object does not. Therefore, it 
is  less  complex  to  mentally  specify  a  salient  object  than  a  non-
salient one. In our model, the notion of salience is applicable only 
to image objects, but not to image points.  
Distinguishability is a notion that is defined for both objects and 
points.  We  use  it  to  classify  a  point  or  object  into  three  types: 
undetectable, detectable, and distinguishable. A point or object is 
detectable  if  it  can  be  visually  identified  on  an  image;  otherwise 
undetectable. A detectable point or object is distinguishable if it is 
easily  distinguished  from  the  surrounding  context.  A  detectable 
point  may  be  indistinguishable.  For  example,  a  cross-point  or  a 
rectangle’s  center 
is  detectable  but  not 
distinguishable since it is hard for humans to mentally distinguish 
the point from surrounding similar points. To be robust to memory 
decay, a memorable point or object should be distinguishable. An 
indistinguishable  point  or  object  is  unmemorable.  Note  that  the 
term “distinguishable” carries a different meaning in [17,18], where 
distinguishable points are actually detectable points we refer to in 
this paper.  
in  Figure  1(C) 
In  a  nutshell,  our  IMP  model  can  be  explained  with  the  above 
notions as follows. The complexity of mentally specifying a point 
is  measured  with  the  point’s  and  its  reference  object’s  semantics 
levels,  and  together  with  the  reference  object’s  salience  level.  A 
point’s  robustness 
the 
distinguishability levels of both the point and its reference object. 
Intuitively, a typical memorable point is of HSC, related to a salient 
HSC  reference  object,  and  distinguishable  from  its  surrounding 
neighbors.  
to  memory  decay 
is  measured  by 
We  then  develop  a  heuristic  categorical  system,  which  is  so 
designed  that  will  accommodate  both  automated  and  human-
assisted realizations of our IMP model.  
Indistinguishable  points  are  unmemorable,  and  thus  collectively 
treated  as  a  single  category  in  our  model.  On  the  other  hand,  by 
semantics,  there  are  three  types  of  distinguishable  points:  HSC, 
MSC  and  LSC.  For  these  points,  we  have 2 × 3  =  6  types  of 
reference  objects,  considering  both  salience  and  semantics.  As  a 
point is specified in reference to an object, this gives 3 × 6 = 18 
possible combinations of distinguishable points and objects. For the 
sake  of  simplicity,  we  group  them  into  7  main  categories,  as 
explained below.  
1220moved  to  the  next  category  of  lower  M-index,  while  there  is  no 
change to the M-index of a detectable point at an image corner. The 
resulting M-index is shifted by adding 1 to ensure that M-index is 
non-negative.  
The  final  M-index  may  be  a  non-integer,  and  can  be  further 
converted into 7 scales of the IPM model. However, it turns out that 
what  matters  for  all  applications  discussed  in  this  paper  is  the 
relative  ranking  capability  enabled  by  these  numeric  values,  for 
which  the  M-index  without  conversion  serves  equally  well.  As  a 
result, such conversion is ignored in this paper. 
5.  AUTOMATED MEMORABILITY  
The IPM model can also be approximated with automatic computer 
vision  algorithms,  albeit  much  coarser  than  the  human-assisted 
approach since object recognition and image understanding remain 
open questions. This leads to our automated memorability. We will 
first  describe  the  automated  memorability,  and  then  modify  it  to 
make it independent of a point’s location. 
5.1  Automated Memorability 
The  automated  memorability  comprises  three  modules.  The  first 
module,  as  described  in  Section  4.1,  finds  detectable  points.  For 
each detectable point, the second module applies the state-of-the-
art  attention  model  proposed  by  Judd  et  al.  [27]  to  compute  a 
salience  value  from  0  to  255,  which  predicts  how  likely,  with  a 
larger value meaning more likely, people would fixate at its local 
region. 
The  third  module  evaluates  the  point’s  distinguishability  via 
dissimilarity of its neighborhood. The more dissimilar, the higher 
distinguishability. Colors and gradients are complement attributes 
of  an  image  region,  with  gradients  pertinent  to  structural 
information.  Both  color  dissimilarity  and  structural  dissimilarity 
are  used.  They  indicate  how  dissimilar  the  color  distribution  and 
the gradient distribution, respectively, of a point’s neighborhood.  
The automated memorability computes the M-index of a detectable 
point empirically as follows:  
M-index = (cid:12)(cid:13)(cid:14)(cid:15)(cid:14)(cid:16) ∙  (cid:12) (cid:19)(cid:16)(cid:20)(cid:21) ∙(255 – salience value),         (1) 
where “salience value” is calculated with the Judd et al.’s model, 
and (cid:12)(cid:13)(cid:14)(cid:15)(cid:14)(cid:16) and (cid:12) (cid:19)(cid:16)(cid:20)(cid:21) are step-wise factors determined respectively 
by the color dissimilarity and gradient dissimilarity of the point. We 
describe in detail the determination of color dissimilarity and (cid:12)(cid:13)(cid:14)(cid:15)(cid:14)(cid:16) 
in Appendix 11.2, and that of structural dissimilarity and (cid:12) (cid:19)(cid:16)(cid:20)(cid:21) in 
Appendix 11.3. 
From Eq. (1), a detectable point with a higher salience value gets a 
smaller M-index (i.e., more memorable). Factor (cid:12)(cid:13)(cid:14)(cid:15)(cid:14)(cid:16) (or (cid:12) (cid:19)(cid:16)(cid:20)(cid:21)) 
is larger than 1 and thus boosts M-index (i.e., less memorable) if 
the  point  is  in  a  less  dissimilar  region  and  thus  less  likely 
distinguishable from other points in its neighborhood. On the other 
hand,  the  factor  is  smaller  than  1  and  thus  lowers  M-index  (i.e., 
more memorable) if the point is in a more dissimilar region and thus 
more likely distinguishable from other points in its neighborhood. 
For other types of local regions, its value is 1.  
M-index given by Eq. (1) can be further converted into 7 scales of 
the IPM model. As we mentioned in Section 4.3, what matters for 
all  applications  discussed  in  this  paper  is  the  relative  ranking 
capability  enabled  by  these  numeric  values,  for  which  M-index 
given by Eq. (1) serves equally well. As a result, we have ignored 