e.g.,  = 1 can fully protect all the output channels; However,
it comes with the cost of degrading the classiﬁcation accuracy
signiﬁcantly from 98.9% to only 50% as shown in Fig.14.
DTW Mean: Fig. 13 (b) shows the mean of DTW between
the intermediate split
layer of all 16 channels activation
after applying various  levels of differential privacy, and
(6)
Fig. 11. Effects of adding more layers on distance correlation distribution
of the two most correlated channels, 1(cid:13) in Fig. 9, and two least correlated
channels, 2(cid:13) in Fig. 9. For each distribution, µ denotes the mean. Privacy
leakage mitigation by increasing the number of layers seems effective for
least correlated ﬁlters, but it is ineffective for those showing high distance
correlation, whose correlation is still high, more than 0.68, when 8 convolu-
tional layers run on the client.
corresponding raw data. The results also suggest that strongest
differential privacy level of  = 1 can increase the dissimilarity
between the ﬁlter activation and corresponding raw data which
protects them against potential reconstruction attempts; How-
ever, strongest privacy level of  = 1 damages the accuracy
signiﬁcantly as explained earlier.
Distribution: To look closely at the impact on ﬁlters after
various  levels, we also deep dive into the distribution of
distance correlation and DTW. Fig. 15 presents the distance
correlation of two most correlated channels vs. two least cor-
related channels without differential privacy, using two-layer
model. The distance correlation distribution is continuously
2345678Number of 1D Conv. Layers0.40.60.8Distance Correlation2345678Number of 1D Conv. Layers0200400600DTW(a)(b)LeastLeastMostMost①②④③2345678Number of 1D Conv. Layers94%95%96%97%98%Accuracy0.00.5=0.89=0.89=0.40Proportion=0.390.00.5=0.86=0.79=0.52Proportion=0.380.00.5=0.81=0.73=0.40Proportion=0.350.00.5=0.78=0.71=0.39Proportion=0.370.00.5=0.82=0.77=0.36Proportion=0.350.00.5=0.75=0.64=0.30Proportion=0.2601Reduced by 23%0.00.5=0.6901Reduced by 23%=0.6801Reduced by 21%=0.3201Reduced by 26%Proportion=0.290.00.20.40.60.81.0Distribution of distance correlation0.00.20.40.60.81.0Two most correlated channelsTwo least correlated channels2 Layers3 Layers4 Layers5 Layers6 Layers7 Layers8 Layers9
overhead of the client is the other main motivation of split
learning since the client only needs to train a small fraction
of parameters of the whole model in comparison with the
federated learning required to train the whole model [3].
Therefore, increasing the number of layers before the split
layers to the client further diminishes the beneﬁts of the split
learning from the client computational overhead perspective,
especially, when the split learning is mounted on resource-
restricted devices/agents.
Framework to evaluate split learning models: We use three
metrics to measure the privacy leakage and explore the possi-
bility of reconstructing the ECG samples’ raw data from split
layer activation. These measurements are visual invertibility,
distance correlation, and Dynamic Time Warping. The ﬁrst
measurement is to show an empirical way to reconstruct the
raw data from activation. The second and third are used for
quantiﬁcation. We believe these three metrics can be used as
a general framework to evaluate the potential privacy leakage
from split learning models.
How to develop more effective split learning for 1D CNN?
This is an open question that is remaining to be answered as
we do not have an afﬁrmative answer. However, we suggest
two, but not limited to, future strategies. (i) Our ﬁrst mitigation
attempt to increase the number of hidden layers shows that
it is possible to protect many of the channels in the acti-
vated output, but not all of them without signiﬁcant accuracy
degradation. One would explore leakage mitigation techniques
for only a few revealing output channels. (ii) Existing split
learning follows a vertical split mechanism, which means all
the split layer output should be shared. To reduce the exposure
of all activation, one would explore a multiple-horizontal split
and share only the protected channels with the server.
Can split learning be applied to LSTM and/or RNN? As
a matter of fact, employing LSTM [27] and/or RNN [28]
served as the ﬁrst
trial when we intended to investigate
the practicality of dealing with sequential/time-series data.
However, we realized that LSTM and RNN are sequential
networks, while the current split technique is always vertically
split. Therefore, we found that there is no efﬁcient means of
splitting LSTM and RNN. This eventually motivated us to
adopt 1D CNN that can be vertically split as well to deal with
pervasive sequential/time-series data. Whether split learning
can be properly applied to LSTM and/or RNN remains to be
further investigated.
Limitations: Our work has two main limitations. Firstly, we
explore the leakage in 1D CNN using one sensitive health
application, which is ECG biomedical signals. We experiment
with the ECG heartbeat samples from the widely-used MIT-
BIH dataset. Other 1D CNN applications and datasets remain
to be investigated. Secondly, privacy leakage from the split
layer of 2D CNN various applications remains to be addressed.
VII. RELATED WORK
In [3], the authors envision that it should be challenging
to reconstruct the data localized on the client-side when split
learning is employed. The main argument, generally, is that
Fig. 12. Effects of adding more layers on DTW distribution of the two most
correlated channels, 3(cid:13) in Fig. 9, and two least correlated channels, 4(cid:13) in
Fig. 9. For each distribution, µ denotes the mean. Privacy leakage mitigation
by increasing the number of layers seems effective for least correlated ﬁlters—
DTW improved by e.g., 84 times, but it is ineffective for those showing low
DTW, whose improvement is less by, e.g., 5 times, when 8 convolutional
layers run on the client.
improved after applying stronger  (from 10, 7, 5, 3, to 1) on
activation before transmitting. Also, Fig. 16 shows DTW of
two most correlated channels vs. two least correlated channels.
The DTW also improved after applying stronger  noise;
However, it seems less effective for highly correlated channels.
Summary: None of the two applied mitigation techniques
can efﬁciently mitigate the privacy leakage from all channels
of the split layer activation—it appears that the mitigation
is dependent on the applied mitigation techniques as well as
convolution ﬁlters. On top of that, both of them come with a
cost of accuracy reduction of the joint model, which appears
not acceptable especially in the differential privacy case.
Therefore, the RQ 2 is still hard to be answered favourably
even when our mitigation attempts are applied.
VI. DISCUSSION AND FUTURE WORK
Why split learning is ineffective to preserve the privacy
for 1D CNN? To reduce the privacy leakage from the split
layer revealed by us, we have consequently applied two
speciﬁc techniques: i) increasing the number of convolutional
layers at the client-side, and ii) applying differential privacy
to the split layer output before transmission. However, both
techniques suffer the reduction of model accuracy, especially
with differential privacy. For the ﬁrst technique of increasing
the number of convolutional layers to the client, it eventually
renders the other trade-off that is the computational overhead
of the client. As a matter of fact, saving the computational
02501=3025=30500=60500Proportion=602501=4025=50500=130500Proportion=3102501=5025=50500=350500Proportion=9002501=5025=50500=610500Proportion=9002501=5025=50500=1990500Proportion=24302501=6025=801000=59101000Proportion=659050Improved by 5x01=150150Improved by 16x=4801000Improved by 78x=46701000Improved by 95xProportion=5720.00.20.40.60.81.0Distribution of DTW0.00.20.40.60.81.0Two most correlated channelsTwo least correlated channels2 Layers3 Layers4 Layers5 Layers6 Layers7 Layers8 Layers10
Fig. 13. Mean of distance correlation and DTW after applying differential privacy on the split layer output before transmitting the activation. Each dot
represents the channel of split layer output. Thicker the color of dot, higher the similarity between the channel of the split layer output and corresponding
raw data.
Fig. 14. Accuracy changes after applying differential privacy with stronger
epsilon values between 10 (weakest) and 1 (strongest).
the server can not access the weight of split layers held by
the client—this is the case for federated learning. Inverting
the weight is computationally infeasible in practice. While
their analysis focuses on 2D CNN, this paper has shown that
reconstructing the raw data in 1D CNN is more than possible.
Vepakomma et al. [23] investigated the potential privacy
leakage of split learning in 2D CNN using the MNIST dataset.
They also found high potential leakage from the split layer.
They showed that by slightly scaling the weights of all layers
before the split, it is possible to reduce the distance correlation
between the split layer activation and raw data. This scaling
mechanism is effective in 2D CNN because of the large
number of hidden layers before the split
layer. However,
existing 1D CNN models such as in [11], [12] have only 2-3
hidden layers, which renders scaling their weights ineffective
and degrades the accuracy.
The other privacy leakage that has been revealed is the
membership inference. For example, Melis et al. [29] demon-
strated the membership inference attack on federated learning
by observing the gradients aggregated from the model trained
on clients. It is not surprising to envision such a membership
inference attack could be applicable to split learning. To which
extent the split learning can be resistant to inference attack
leaves interesting future work.
Other than privacy concerns of distributed learning, there
are also emerging security concerns inherent to the distributed
learning, in particular, the backdoor attacks [30], [31], [32].
Generally, a backdoor attack occurs when the training data
is tampered by a malicious party under the model training
outsource scenario. The backdoor model behaves normally
Fig. 15. Effects of applying differential privacy on distance correlation
distribution of two most correlated channels, 5(cid:13) in Fig. 13, and two least
correlated channels, 6(cid:13) in Fig. 13. For each distribution, µ denotes the mean.
In contrast to applying more hidden layers, privacy mitigation by applying
differential privacy seems effective for highly correlated channels—reduced
by 63%, but it is ineffective for channels having lower distance correlation,
e.g., about 23%.
for clean inputs while misclassifying any input to the target
class when the input is stamped with a trigger. It has been
shown that federated learning is inherently vulnerable to such
a backdoor attack because the server has no control over the
local data by design [33], [34]. Therefore, participants can
manipulate their data as they want to insert a backdoor to
the joint model. We believe that split learning, for the same
reason, is also inevitably vulnerable to backdoor attacks from
the security perspective besides the privacy leakage revealed
in this work. Thus, it is important to consider this security
concern when deploying split learning in realistic security-
critical applications.
None107531Epsilon0.40.60.8Distance CorrelationNone107531Epsilon0200400600800DTW(a)(b)MostMostLeastLeast⑥⑤⑦⑧None107531Epsilon50%60%70%80%90%100%Accuracy0.00.5=0.89=0.89=0.40Proportion=0.390.00.5=0.66=0.65=0.39Proportion=0.390.00.5=0.58=0.57=0.39Proportion=0.370.00.5=0.49=0.48=0.37Proportion=0.360.00.5=0.40=0.39=0.34Proportion=0.3301Reduced by 63%0.00.5=0.3301Reduced by 63%=0.3301Reduced by 23%=0.3101Reduced by 22%Proportion=0.310.00.20.40.60.81.0Distribution of distance correlation0.00.20.40.60.81.0Two most correlated channelsTwo least correlated channelsNodiffpriv=10=7=5=3=111
by the Australian Governments Cooperative Research Cen-
tres Programme. This work was also supported in part by
the ITRC support program (IITP-2019-2015-0-00403). The
authors would like to thank all the anonymous reviewers for
their valuable feedback.
REFERENCES
[1] D. Rav, C. Wong, F. Deligianni, M. Berthelot, J. Andreu-Perez, B. Lo,
and G. Yang. Deep learning for health informatics. IEEE Journal of
Biomedical and Health Informatics, 21(1):4–21, Jan 2017.
[2] B. Shickel, P. J. Tighe, A. Bihorac, and P. Rashidi. Deep ehr: A survey of
recent advances in deep learning techniques for electronic health record
IEEE Journal of Biomedical and Health Informatics,
(ehr) analysis.
22(5):1589–1604, Sep. 2018.
[3] Otkrist Gupta and Ramesh Raskar. Distributed learning of deep neural
Journal of Network and Computer
network over multiple agents.
Applications, 116:1–8, 2018.
[4] Supreeth Shastri, Melissa Wasserman, and Vijay Chidambaram. The
seven sins of personal-data processing systems under gdpr. In Proceed-
ings of the 11th USENIX Conference on Hot Topics in Cloud Computing,
HotCloud’19, pages 1–1, Berkeley, CA, USA, 2019. USENIX Associa-
tion.
[5] EU GDPR. General data protection regulation (gdpr). https://www.
eugdpr.org/. [Online; accessed 10-Nov-2019].
[6] HIPAA Compliance Assistance. Summary of the hipaa privacy rule.
Ofﬁce for Civil Rights, 2003.
[7] Praneeth Vepakomma, Otkrist Gupta, Tristan Swedish, and Ramesh
Raskar. Split learning for health: Distributed deep learning without
sharing raw patient data. arXiv preprint arXiv:1812.00564, 2018.
[8] Praneeth Vepakomma, Tristan Swedish, Ramesh Raskar, Otkrist Gupta,
and Abhimanyu Dubey. No peek: A survey of private distributed deep
learning. arXiv preprint arXiv:1812.03288, 2018.
[9] Tomas B Garcia and Daniel J Garcia. Arrhythmia Recognition: The Art
of Interpretations. Jones & Bartlett Publishers, 2019.
[10] Serkan Kiranyaz, Onur Avci, Osama Abdeljaber, Turker Ince, Moncef
Gabbouj, and Daniel J Inman. 1d convolutional neural networks and
applications: A survey. arXiv preprint arXiv:1905.03554, 2019.
[11] Serkan Kiranyaz, Turker Ince, and Moncef Gabbouj. Real-time patient-
speciﬁc ecg classiﬁcation by 1-d convolutional neural networks. IEEE
Transactions on Biomedical Engineering, 63(3):664–675, 2015.
[12] Dan Li, Jianxin Zhang, Qiang Zhang, and Xiaopeng Wei. Classiﬁcation
In 2017
of ecg signals based on 1d convolution neural network.
IEEE 19th International Conference on e-Health Networking, Apps and
Services, pages 1–6. IEEE, 2017.
[13] Abdelkader Sellami, Amine Zouaghi, and Abdelhamid Daamouche. Ecg
as a biometric for individual’s identiﬁcation. In 2017 5th International
Conference on Electrical Engineering-Boumerdes (ICEE-B), pages 1–6.
IEEE, 2017.
[14] Rick Wang, Amir Karimi, and Ali Ghodsi. Distance correlation au-
toencoder. In 2018 International Joint Conference on Neural Networks
(IJCNN), pages 1–8. IEEE, 2018.
[15] Pavel Senin. Dynamic time warping algorithm review. Information and
Computer Science Department University of Hawaii at Manoa Honolulu,
USA, 855(1-23):40, 2008.
[16] J¨urgen Schmidhuber. Deep learning in neural networks: An overview.
Neural networks, 61:85–117, 2015.
[17] European Network and Information Security Agency. An SME perspec-
tive on cloud computingsurvey. Survey Report, pages 1–16, 2009.
[18] Jakub Koneˇcn`y, H Brendan McMahan, Felix X Yu, Peter Richt´arik,
Federated learning:
arXiv preprint
Ananda Theertha Suresh, and Dave Bacon.
Strategies for improving communication efﬁciency.
arXiv:1610.05492, 2016.
[19] Serkan Kiranyaz, Turker Ince, and Moncef Gabbouj. Personalized mon-
itoring and advance warning system for cardiac arrhythmias. Scientiﬁc
reports, 7(1):9270, 2017.
[20] Yunan Wu, Feng Yang, Ying Liu, Xuefan Zha, and Shaofeng Yuan. A
comparison of 1-d and 2-d deep convolutional neural networks in ecg
classiﬁcation. arXiv preprint arXiv:1810.07088, 2018.
¨Ozal Yıldırım, Paweł Pławiak, Ru-San Tan, and U Rajendra Acharya.
Arrhythmia detection using deep convolutional neural network with long
duration ecg signals. Computers in biology and medicine, 102:411–420,
2018.
[21]
Fig. 16. Effects of applying differential privacy on DTW distribution of two
most correlated channels, 7(cid:13) in Fig. 13, and two least correlated channels, 8(cid:13)
in Fig. 13. For each distribution, µ denotes the mean. In contrast to applying
more hidden layers, privacy mitigation by applying differential privacy seems
effective for channels having low similarity—improved 158 times, but it is
less effective for highly correlated channels, e.g., about 113 times.
VIII. CONCLUSION
In this paper, we explored the feasibility of split learning
to deal with sensitive time-series data in particular personal
ECG signals to detect heart abnormalities. We introduced
the ﬁrst implementation of split learning into the 1D CNN
model. We proposed a privacy assessment framework for
CNN models when using split learning, with three metrics:
visual invertibility, distance correlation, and DTW. Based on
this framework, we extensively evaluated the privacy leakage
exempliﬁed by the ECG dataset. Our results demonstrate that
adopting split
learning directly into 1D CNN models for