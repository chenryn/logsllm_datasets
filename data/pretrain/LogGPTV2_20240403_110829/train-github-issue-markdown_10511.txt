**Is this a request for help?** (If yes, you should use our troubleshooting
guide and community support channels, see
http://kubernetes.io/docs/troubleshooting/.):
**What keywords did you search in Kubernetes issues before filing this one?**
(If you have found any duplicates, you should instead reply there.):
* * *
**Is this a BUG REPORT or FEATURE REQUEST?** (choose one): BUG REPORT
**Kubernetes version** (use `kubectl version`):
    Client Version: version.Info{Major:"1", Minor:"3", GitVersion:"v1.3.5", GitCommit:"b0deb2eb8f4037421077f77cb163dbb4c0a2a9f5", GitTreeState:"clean", BuildDate:"2016-08-11T20:29:08Z", GoVersion:"go1.6.2", Compiler:"gc", Platform:"linux/amd64"}
    Server Version: version.Info{Major:"1", Minor:"3", GitVersion:"v1.3.0", GitCommit:"283137936a498aed572ee22af6774b6fb6e9fd94", GitTreeState:"clean", BuildDate:"2016-07-01T19:19:19Z", GoVersion:"go1.6.2", Compiler:"gc", Platform:"linux/amd64"}
**Environment** :
  * **Cloud provider or hardware configuration** : AWS
  * **OS** (e.g. from /etc/os-release): The one created by `kube-up.sh`
  * **Kernel** (e.g. `uname -a`): The one used by `kube-up.sh`
  * **Install tools** : kube-up.sh
  * **Others** :
**What happened** :
    kubectl  --kubeconfig=infra/dev/kubeconfig drain --ignore-daemonsets --force --delete-local-data --grace-period=120 ip-172-20-0-123.ec2.internal 
    node "ip-172-20-0-123.ec2.internal" already cordoned
    error: replicationcontrollers "memcached" not found: memcached-4n9cy, memcached-4n9cy
It would seem that `memcached-4n9cy` is an orphan pod from a replication
controller that doesn't exist anymore.
    $ ./kubectl  --kubeconfig=infra/dev/kubeconfig describe pod memcached-4n9cy | grep Controllers
    Controllers:    ReplicationController/memcached
We moved to ReplicaSets and don't have any ReplicationControllers left:
    $ ./kubectl  --kubeconfig=infra/dev/kubeconfig get --all-namespaces rc 
    NAMESPACE     NAME                             DESIRED   CURRENT   AGE
    kube-system   kube-dns-v17                     1         1         41d
    kube-system   kubernetes-dashboard-v1.1.0      1         1         41d
    kube-system   monitoring-influxdb-grafana-v3   1         1         41d
**What you expected to happen** :
Providing `--force` should allow you to drain the pods even if they are
orphans from a replication controller.
I had to manually remove the pod before running `kubectl drain` again to make
it work.
**How to reproduce it** (as minimally and precisely as possible):
I am not sure how the pods ended up being orphaned but I guess that getting an
orphaned pod from a replication controller should make it easy to reproduce.
**Anything else do we need to know** :