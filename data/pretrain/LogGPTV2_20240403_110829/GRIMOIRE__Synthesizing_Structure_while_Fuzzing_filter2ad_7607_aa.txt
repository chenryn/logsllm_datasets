title:GRIMOIRE: Synthesizing Structure while Fuzzing
author:Tim Blazytko and
Cornelius Aschermann and
Moritz Schl&quot;ogel and
Ali Abbasi and
Sergej Schumilo and
Simon W&quot;orner and
Thorsten Holz
Grimoire: Synthesizing Structure while Fuzzing
Tim Blazytko, Cornelius Aschermann, Moritz Schlögel, Ali Abbasi, Sergej Schumilo, 
Simon Wörner, and Thorsten Holz, Ruhr-Universität Bochum
https://www.usenix.org/conference/usenixsecurity19/presentation/blazytko
This paper is included in the Proceedings of the 28th USENIX Security Symposium.August 14–16, 2019 • Santa Clara, CA, USA978-1-939133-06-9Open access to the Proceedings of the 28th USENIX Security Symposium is sponsored by USENIX.GRIMOIRE: Synthesizing Structure while Fuzzing
Tim Blazytko, Cornelius Aschermann, Moritz Schlögel, Ali Abbasi,
Sergej Schumilo, Simon Wörner and Thorsten Holz
Ruhr-Universität Bochum, Germany
Abstract
In the past few years, fuzzing has received signiﬁcant at-
tention from the research community. However, most of this
attention was directed towards programs without a dedicated
parsing stage. In such cases, fuzzers which leverage the input
structure of a program can achieve a signiﬁcantly higher code
coverage compared to traditional fuzzing approaches. This
advancement in coverage is achieved by applying large-scale
mutations in the application’s input space. However, this
improvement comes at the cost of requiring expert domain
knowledge, as these fuzzers depend on structure input speci-
ﬁcations (e. g., grammars). Grammar inference, a technique
which can automatically generate such grammars for a given
program, can be used to address this shortcoming. Such tech-
niques usually infer a program’s grammar in a pre-processing
step and can miss important structures that are uncovered only
later during normal fuzzing.
In this paper, we present the design and implementation
of GRIMOIRE, a fully automated coverage-guided fuzzer
which works without any form of human interaction or pre-
conﬁguration; yet, it is still able to efﬁciently test programs
that expect highly structured inputs. We achieve this by per-
forming large-scale mutations in the program input space
using grammar-like combinations to synthesize new highly
structured inputs without any pre-processing step. Our eval-
uation shows that GRIMOIRE outperforms other coverage-
guided fuzzers when fuzzing programs with highly structured
inputs. Furthermore, it improves upon existing grammar-
based coverage-guided fuzzers. Using GRIMOIRE, we iden-
tiﬁed 19 distinct memory corruption bugs in real-world pro-
grams and obtained 11 new CVEs.
1 Introduction
As the amount of software impacting the (digital) life of
nearly every citizen grows, effective and efﬁcient testing
mechanisms for software become increasingly important. The
publication of the fuzzing framework AFL [65] and its suc-
cess at uncovering a huge number of bugs in highly relevant
software has spawned a large body of research on effective
feedback-based fuzzing. AFL and its derivatives have largely
conquered automated, dynamic software testing and are used
to uncover new security issues and bugs every day. However,
while great progress has been achieved in the ﬁeld of fuzzing,
many hard cases still require manual user interaction to gen-
erate satisfying test coverage. To make fuzzing available to
more programmers and thus scale it to more and more target
programs, the amount of expert knowledge that is required to
effectively fuzz should be reduced to a minimum. Therefore,
it is an important goal for fuzzing research to develop fuzzing
techniques that require less user interaction and, in particular,
less domain knowledge to enable more automated software
testing.
Structured Input Languages. One common challenge for
current fuzzing techniques are programs which process highly
structured input languages such as interpreters, compilers,
text-based network protocols or markup languages. Typically,
such inputs are consumed by the program in two stages: pars-
ing and semantic analysis. If parsing of the input fails, deeper
parts of the target program—containing the actual applica-
tion logic—fail to execute; hence, bugs hidden “deep” in the
code cannot be reached. Even advanced feedback fuzzers—
such as AFL—are typically unable to produce diverse sets
of syntactically valid inputs. This leads to an imbalance, as
these programs are part of the most relevant attack surface in
practice, yet are currently unable to be fuzzed effectively. A
prominent example are browsers, as they parse a multitude
of highly-structured inputs, ranging from XML or CSS to
JavaScript and SQL queries.
Previous approaches to address this problem are typi-
cally based on manually provided grammars or seed cor-
pora [2, 14, 45, 52]. On the downside, such methods require
human experts to (often manually) specify the grammar or
suitable seed corpora, which becomes next to impossible for
applications with undocumented or proprietary input speciﬁ-
cations. An orthogonal line of work tries to utilize advanced
program analysis techniques to automatically infer grammars
USENIX Association
28th USENIX Security Symposium    1985
[4, 5, 25]. Typically performed as a pre-processing step, such
methods are used for generating a grammar that guides the
fuzzing process. However, since this grammar is treated as im-
mutable, no additional learning takes place during the actual
fuzzing run.
Our Approach.
In this paper, we present a novel, fully au-
tomated method to fuzz programs with a highly structured
input language, without the need for any human expert or
domain knowledge. Our approach is based on two key obser-
vations: First, we can use code coverage feedback to automati-
cally infer structural properties of the input language. Second,
the precise and “correct” grammars generated by previous
approaches are actually unnecessary in practice: since fuzzers
have the virtue of high test case throughput, they can deal
with a signiﬁcant amount of noise and imprecision. In fact, in
some programs (such as Boolector) with a rather diverse set
of input languages, the additional noise even beneﬁts the fuzz
testing. In a similar vein, there are often program paths which
can only be accessed by inputs outside of the formal speciﬁca-
tions, e. g., due to incomplete or imprecise implementations
or error handling code.
Instead of using a pre-processing step, our technique is
directly integrated in the fuzzing process itself. We propose a
set of generalizations and mutations that resemble the inner
workings of a grammar-based fuzzer, without the need for an
explicit grammar. Our generalization algorithm analyzes each
newly found input and tries to identify substrings of the input
which can be replaced or reused in other positions. Based on
this information, the mutation operators recombine fragments
from existing inputs. Overall, this results in synthesizing new,
structured inputs without prior knowledge of the underlying
speciﬁcation.
We have implemented a prototype of the proposed ap-
proach in a tool called GRIMOIRE1. GRIMOIRE does not
need any speciﬁcation of the input language and operates in
an automated manner without requiring human assistance;
in particular, without the need for a format speciﬁcation or
seed corpus. Since our techniques make no assumption about
the program or its environment behavior, GRIMOIRE can be
easily applied to closed-source targets as well.
To demonstrate the practical feasibility of our approach,
we perform a series of experiments. In a ﬁrst step, we select a
diverse set of programs for a comparative evaluation: we eval-
uate GRIMOIRE against other fuzzers on four scripting lan-
guage interpreters (mruby, PHP, Lua and JavaScriptCore),
a compiler (TCC), an assembler (NASM), a database (SQLite),
a parser (libxml) and an SMT solver (Boolector). Demon-
strating that our approach can be applied in many different
scenarios without requiring any kind of expert knowledge,
such as an input speciﬁcation. The evaluation results show
1A grimoire is a magical book that recombines magical elements to
formulas. Furthermore, it has the same word stem as the Old French word
for grammar—namely, gramaire.
that our approach outperforms all existing coverage-guided
fuzzers; in the case of Boolector, GRIMOIRE ﬁnds up to
87% more coverage than the baseline (REDQUEEN). Sec-
ond, we evaluate GRIMOIRE against state-of-the-art grammar-
based fuzzers. We observe that in situations where an input
speciﬁcation is available, it is advisable to use GRIMOIRE
in addition to a grammar fuzzer to further increase the test
coverage found by grammar fuzzers. Third, we evaluate GRI-
MOIRE against current state-of-the-art approaches that use
automatically inferred grammars for fuzzing and found that
we can signiﬁcantly outperform such approaches. Overall,
GRIMOIRE found 19 distinct memory corruption bugs that
we manually veriﬁed. We responsibly disclosed all of them
to the vendors and obtained 11 CVEs. During our evalu-
ation, the next best fuzzer only found 5 of these bugs. In
fact, GRIMOIRE found more bugs than all ﬁve other fuzzers
combined.
Contributions.
butions:
In summary, we make the following contri-
• We present the design, implementation and evaluation
of GRIMOIRE, an approach to fully automatically fuzz
highly structured formats with no human interaction.
• We show that even though GRIMOIRE is a binary-only
fuzzer that needs no seeds or grammar as input, it
still outperforms many fuzzers that make signiﬁcantly
stronger assumptions (e. g., access to seeds, grammar
speciﬁcations and source code).
• We found and reported multiple bugs in various common
projects such as PHP, gnuplot and NASM.
2 Challenges
guages
in Fuzzing Structured Lan-
In this section, we brieﬂy summarize essential information
paramount to the understanding of our approach. To this
end, we provide an overview of different fuzzing approaches,
while focusing on their shortcomings and open challenges.
In particular, we describe those details of AFL (e. g., code
coverage) that are necessary to understand our approach. Ad-
ditionally, we explain how fuzzers explore the state space of
a program and how grammars aid the fuzzing process.
Generally speaking, fuzzing is a popular and efﬁcient soft-
ware testing technique used to uncover bugs in applications.
Fuzzers typically operate by producing a large number of test
cases, some of which may trigger bugs. By closely moni-
toring the runtime execution of these test cases, fuzzers are
able to locate inputs causing faulty behavior. In an abstract
view, one can consider fuzzing as randomly exploring the
state space of the application. Typically, most totally ran-
dom inputs are rejected early by the target application and
1986    28th USENIX Security Symposium
USENIX Association
do not visit interesting parts of the state space. Thus, in our
abstract view, the state space has interesting and uninteresting
regions. Efﬁcient fuzzers somehow have to ensure that they
avoid uninteresting regions most of the time. Based on this
observation, we can divide fuzzers into three broad categories,
namely: (a) blind, (b) coverage-guided and (c) hybrid fuzzers,
as explained next.
2.1 Blind Fuzzing
The most simple form of a fuzzer is a program which gen-
erates a stream of random inputs and feeds it to the target
application. If the fuzzer generates inputs without considering
the internal behavior of the target application, it is typically
referred to as a blind fuzzer. Examples of blind fuzzers are
RADAMSA [29], PEACH [14], Sulley [45] and ZZUF [32].
To obtain new inputs, fuzzers traditionally can build on two
strategies: generation and mutation.
Fuzzers employing the former approach have to acquire
a speciﬁcation, typically a grammar or model, of an appli-
cation’s expected input format. Then, a fuzzer can use the
format speciﬁcation to be able to generate novel inputs in a
somewhat efﬁcient way. Additionally, in some cases, a set of
valid inputs (a so-called corpus) might be required to aid the
generation process [46, 58].
On the other hand, fuzzers which employ a mutation-based
strategy require only an initial corpus of inputs, typically
referred to as seeds. Further test cases are generated by ran-
domly applying various mutations on initial seeds or novel
test cases found during fuzzing runs. Examples for common
mutators include bit ﬂipping, splicing (i. e., recombining two
inputs) and repetitions [14, 29, 32]. We call these mutations
small-scale mutations, as they typically change small parts of
the program input.
Blind fuzzers suffer from one major drawback. They either
require an extensive corpus or a well-designed speciﬁcation
of the input language to provide meaningful results. If a
program feature is not represented by either a seed or the
input language speciﬁcation, a blind fuzzer is unlikely to
exercise it. In our abstract, state space-based view, this can be
understood as blindly searching the state space near the seed
inputs, while failing to explore interesting neighborhoods,
as illustrated in Figure 1(a). To address this limitation, the
concept of coverage-guided fuzzing was introduced.
2.2 Coverage-guided Fuzzing
Coverage-guided fuzzers employ lightweight program cover-
age measurements to trace how the execution path of the appli-
cation changes based on the provided input (e. g., by tracking
which basic blocks have been visited). These fuzzers use this
information to decide which input should be stored or dis-
carded to extend the corpus. Therefore, they are able to evolve
inputs that differ signiﬁcantly from the original seed corpus
(a) Blind mutational fuzzers mostly
explore the state space near the seed
corpus. They often miss interesting
states (shaded area) unless the seeds
are good.
(b) Coverage guided fuzzers can
learn new inputs (arrows) close to ex-
isting seeds. However, they are often
unable to skip large gaps.
(c) Programs with highly structured
input formats typically have large
gaps in the state space. Current feed-
back and hybrid fuzzers have difﬁcul-
ties ﬁnding other interesting islands
using local mutations.
(d) By introducing an input speciﬁca-
tion, fuzzers can generate inputs in
interesting areas and perform large-
scale mutations that allow to jump
between islands of interesting states.
Figure 1: Different fuzzers exploring distinct areas in state space.
while at the same time exercising new program features. This
strategy allows to gradually explore the state of the program
as it uncovers new paths. This behavior is illustrated in Fig-
ure 1(b). The most prominent example of a coverage-guided
fuzzer is AFL [65]. Following the overwhelming success of
AFL, various more efﬁcient coverage-guided fuzzers such as
ANGORA [12], QSYM [64], T-FUZZ [47] or REDQUEEN [3]
were proposed.
From a high-level point of view, all these AFL-style fuzzers
can be broken down into three different components: (i) the in-
put queue stores and schedules all inputs found so far, (ii) the
mutation operations produce new variants of scheduled inputs
and (iii) the global coverage map is used to determine whether
a new variant produced novel coverage (and thus should be
stored in the queue).
From a technical point of view, this maps to AFL as fol-
lows: Initially, AFL ﬁlls the input queue with the seed inputs.
Then, it runs in a continuous fuzzing loop, composed of the
following steps: (1) Pick an input from the input queue, then
(2) apply multiple mutation operations on it. After each muta-
tion, (3) execute the target application with the selected input.
If new coverage was triggered by the input, (4) save it back to
the queue. To determine whether new coverage was triggered,
USENIX Association
28th USENIX Security Symposium    1987
AFL compares the results of the execution with the values in
the global coverage map.
This global coverage map is ﬁlled as follows: AFL shares
a memory area of the same size as the global coverage map
with the fuzzing target. During execution, each transition
between two basic blocks is assigned a position inside this
shared memory. Every time the transition is triggered, the
corresponding entry (one byte) in the shared memory map is
incremented. To reduce overhead incurred by large program
traces, the shared coverage map has a ﬁxed size (typically
216 bytes). While this might introduce collisions, empirical
evaluation has shown that the performance gains make up for
the loss in the precision [66].
After the target program terminates, AFL compares the
values in the shared map to all previous runs stored in the
global coverage map. To check if a new edge was executed,
AFL applies the so-called bucketing. During bucketing, each
entry in the shared map is rounded to a power of 2 (i. e., at
most a single bit is set in each entry). Then, a simple binary
operation is used to check if any new bits are present in the
shared map (but not the global map). If any new bit is present,
the input is stored in the queue. Furthermore, all new bits
are also set to 1 in the global coverage map. We distinguish
between new bits and new bytes. If a new bit is set to 1 in
a byte that was previously zero, we refer to it as a new byte.
Intuitively, a new byte corresponds to new coverage while a
new bit only illustrates that a known edge was triggered more
often (e. g., more loop iterations were observed).
Example 1. For example, consider some execution a while
after starting the fuzzer run for a program represented by
its Control-Flow Graph (CFG) in Figure 2 a(cid:13). Assume that
the ﬁctive execution of an input causes a loop between B
and C to be executed 10 times. Hence, the shared map is
updated as shown in b(cid:13), reﬂecting the fact that edges A →
B and C → D were executed only once, while the edges B
→ C and C → B were encountered 10 (0b1010) times. In
c(cid:13), we illustrate the ﬁnal bucketing step. Note how 0b1010
is put into the bucket 0b1000, while 0b0001 is moved into
the one identiﬁed by 0b0001. Finally, AFL checks whether
the values encountered in this run triggered unseen edges in
d(cid:13). To this end, we compare the shared map to the global
coverage map and update it accordingly (see e(cid:13)), setting bits
set in the shared but not global coverage map. As visualized
in f(cid:13), a new bit was set for two entries, while a new byte
was found for one. This means that the edge between C → D
was previously unseen, thus the input used for this example
triggered new coverage.
While coverage-guided fuzzers signiﬁcantly improve upon
blind fuzzers, they can only learn from new coverage if they
are able to guess an input that triggers the new path in the
program. In certain cases, such as multi-byte magic values,
the probability of guessing an input necessary to trigger a
different path is highly unlikely. These kind of situations
occur if there is a signiﬁcant gap between interesting areas in
the state space and existing mutations are unlikely to cross the
uninteresting gap. The program displayed in the Figure 1(b)
illustrates a case with only one large gap in the program
space. Thus, this program is well-suited for coverage-guided
fuzzing. However, current mutation-based coverage-guided
fuzzers struggle to explore the whole state space because
the island in the lower right is never reached. To overcome
this limitation, hybrid fuzzer were introduced; these combine
coverage-guided fuzzing with more in-depth program analysis
techniques.
2.3 Hybrid Fuzzing
Hybrid fuzzers typically combine coverage-guided fuzzing
with program analysis techniques such as symbolic execution,
concolic execution or taint tracking. As noted above, fast and
cheap fuzzing techniques can uncover the bulk of the easy-
to-reach code. However, they struggle to trigger program
paths that are highly unlikely. On the other hand, symbolic
or concolic execution does not move through the state space
randomly. Instead, these techniques use an SMT solver to
ﬁnd inputs that trigger the desired behavior. Therefore, they
can cover hard-to-reach program locations. Still, as a con-
sequence of the precise search technique, they struggle to
explore large code regions due to signiﬁcant overhead.
By combining fuzzing and reasoning-based techniques, one
can beneﬁt from the strength of each individual technique,
while avoiding the drawbacks. Purely symbolic approaches
have proven difﬁcult to scale. Therefore, most current tools
such as SAGE [21], DRILLER [54] or QSYM [64] use concolic
execution instead. This mostly avoids the state explosion
problem by limiting the symbolic execution to a single path.
To further reduce the computation cost, some fuzzers such
as VUZZER [50] and ANGORA [12] only use taint tracking.
Both approaches still allow to overcome the common multi-
byte magic value problem. However, they lose the ability to
explore behavior more globally.
While hybrid fuzzers can solve constraints over individual
values of the input, they are typically not efﬁcient at solving
constraints on the overall structure of the input. Consider
target programs such as a script interpreter. To uncover a new
valid code path, the symbolic executor usually has to consider
a completely different path through the parsing stage. This
leads to a large number of very large gaps in the state space
as illustrated in Figure 1(c). Therefore, concolic execution or