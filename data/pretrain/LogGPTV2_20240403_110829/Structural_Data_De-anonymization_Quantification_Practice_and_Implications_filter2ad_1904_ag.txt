Inspired by our quantiﬁcation, we design a novel and opera-
tional Optimization based De-Anonymization (ODA) scheme,
∗
which is a relaxed version of A
. In ODA, rather than us-
ing the DE function as in the quantiﬁcation, we re-deﬁne
ψi,j and Ψσ as follows. Given a DA scheme σ = {(i, j)|i ∈
V a, j ∈ V u}, we deﬁne the DE on a user mapping (i, j) ∈ σ
as ψi,j = |fd(i) − fd(j)| + (1 − ϕ(i, j)) · |fd(i) − fd(j)|, and
ψi,j. Based on Ψσ, we give the
the DE on σ as Ψσ =
∑
(i,j)∈σ
framework of ODA as shown in Algorithm 1. In Algorithm
1, Λa ⊆ V a is the target DA set and Λu ⊆ V u is the possible
mapping set of Λa. GetTopDegree(X, y) is a function to re-
turn y users with the largest degree values in X, i.e., return
{i|i has the Top-y degree in X}. C(i) ⊆ Λu is the candidate
mapping set for i, which consists of the γ most possible map-
pings of i in Λu. GetTopSimilarity(i, Λu, γ) is a function to
return γ users having the highest similarity scores with i in
Λu, i.e., return {j|j ∈ Λu, and j has the Top-γ ϕ(i, j) in
Λu}.
∗
From Algorithm 1, ODA de-anonymizes Ga iteratively.
During each iteration, ODA is trying to de-anonymize a
subset of V a and seeking the sub-DA scheme σ
(Λa) which
induces the least DE. We explain the idea of ODA in de-
tails as follows. In Line 3, we initialize the target DA set
Λa and the candidate mapping set Λu. From the initial-
ization, α is an important parameter to control how many
anonymized users will be processed in each iteration.
In
Line 4, we compute a candidate mapping set C(i) for each
i ∈ Λa. C(i) consists γ most similar users of i in Λu. Here,
we deﬁne C(·) mainly for reducing the computational com-
plexity. Instead of trying every mapping from i to Λu, we
only consider to map i to some user in C(i). Hence, γ is
another important parameter to control the computation-
al complexity of ODA. We will demonstrate how to set α
and γ to make ODA computationally feasible in Theorem
Algorithm 1: ODA
1 Deﬁne Λa = Λu = ∅;
2 while true do
3
4
5
6
7
8
9
Λa = GetTopDegree(V a, α), Λu =
GetTopDegree(V u, α);
for every i ∈ Λa, compute a candidate mapping set
C(i) = GetTopSimilarity(i, Λu, γ);
apply the consistent rule and pruning rule to ﬁnd
the DA scheme σ(Λa) ∈ ∏
(i × C(i)) which
i∈Λa
induces the least DE Ψσ(Λa), denoted by
(Λa) = {(i1, j1), (i2, j2),··· , (iα, jα)};
∗
σ
for each (i, j) ∈ σ
(Λa), if ϕ(i, j) ≥ θ then
∗
accept the mapping (i, j);
V a = V a \ {i}, V u = V u \ {j};
if no mapping in σ
(Λa) is accepted, break ;
∗
Ψσ∗(Λa) = min{Ψσ(Λa)|σ(Λa) ∈ ∏
9. In Line 5, we ﬁnd a DA scheme σ
i∈Λa
∗
(Λa) on Λa such that
(i × C(i))}, i.e., σ
(Λa)
∗
∗
causes the least DE. Furthermore, the consistent rule and
the pruning rule are applied in this step. The consistent
rule makes any possible DA scheme σ(Λa) consistent, i.e., no
mapping conﬂiction which is deﬁned as the situation that t-
wo or more anonymized users are mapped to the same known
user. This is because it is possible that C(i1) ∩ C(i2) ̸= ∅ for
i1 ̸= i2 ∈ Λa, and the situation σ(i1) = σ(i2) in a DA
scheme should be avoided. Note that, it is possible that
no σ(Λa) is consistent. In this case, we should increase γ
to guarantee at least one σ(Λa) is consistent. The prun-
ing rule is used to remove some DA schemes whose DE is
larger than the current known least DE. For instance, let
(Λa) be the DA scheme having the least DE after testing
σ
k possible DA schemes. Then, when testing the (k + 1)-
th possible DA scheme σk+1(Λa), if partial of mappings in
σk+1(Λa) has already induced a larger DE than σ
(Λa), we
stop test σk+1(Λa) and continue the next one. On the other
hand, if σk+1(Λa) induces a smaller DE than σ
(Λa), we up-
(Λa) to σk+1(Λa). Both the consistent rule and the
date σ
pruning rule can remove some unqualiﬁed DA schemes in ad-
(Λa)
vance, which can speed up ODA. Actually, although σ
(Λa) is a local optimization solution
causes the least DE, σ
∗
(according to our quantiﬁcation, A
induces the optimum
solution). This is because we try to seek a tradeoﬀ between
computational feasibility and DA accuracy. After obtaining
(Λa) with similarity
σ
scores no less than a threshold value θ (Lines 6-8). For the
mappings been rejected, they will be re-considered in the
following iterations for possible better DAs. If no mapping
can be accepted, we stop ODA. Finally, we analyze the time
and space complexities of ODA in Theorem 9. We defer the
proof to Appendix C for readability.
(Λa), we accept the mappings in σ
∗
∗
∗
∗
∗
∗
∗
Theorem 9. (i) The space complexity of ODA is O(min{n2,
m + n}). (ii) Let γ be some constant value, α = Θ(log n),
and Γ be the average number of accepted mappings in each
iteration of ODA. Then, the time complexity of ODA is
O(m + n log n + nΘ(1) log γ+1/Γ) in the worst case.
Finally, we make some remarks on ODA as follows.
(i) ODA is a cold start algorithm, i.e., we do not need
any priori knowledge, e.g., the seed mapping information
[1][2][3], to bootstrap the DA process. Furthermore, unlike
existing DA algorithms [1][2][3] which consist of two phases
(landmark identiﬁcation phase and DA propagation phase),
ODA is a single-phase algorithm. Interestingly, ODA itself
can act as a landmark identiﬁcation algorithm. From our
experiment (Section 6.2), ODA can de-anonymize the 60-
180 Top-degree users in Gowalla and Google+ (see Tab. 1)
perfectly, which can serve as landmarks (V a
L) for
future DAs. In addition, ODA as a landmark identiﬁcation
algorithm is much faster than that in [2] (with complexity of
O(ndk−1) = O(nk), where d is maximum degree of Ga/Gu
and k is the number of landmarks) and [3] (with complexity
of k!, could be computationally infeasible for a PC when
k ≥ 20).
L and U u
(ii) Similar to A
, ODA is an optimization based DA
scheme, which is diﬀerent from most of existing heuristics
based solutions [1][2][3]. In ODA, the objective is to mini-
mize a DE function. The reasonableness and soundness of
ODA lie on one direct conclusion of our theoretical quan-
tiﬁcation: minimizing the DE leads to the best possible DA
scheme.
∗
∪
(i × C(i))),
∗
(iii) In ODA, we seek an adjustable tradeoﬀ between DA
accuracy and computational feasibility. Although A
obtain-
s the optimum solution a.a.s. in terms of our quantiﬁcation,
it is computationally infeasible (O(n!)). ODA has a polyno-
mial time complexity of O(m + n log n + nΘ(1) log γ+1/Γ) in
the worst case, which is computationally feasible at the cost
of sacriﬁcing some accuracy. Based on our experiments on
large scale real datasets in the following subsection, ODA is
operable while preserves satisﬁable DA performance.
i∈Λa
(iv) ODA is a general framework. Line 5 can also be im-
plemented by seeking a maximum weighted bipartite graph
matching on a weighted bipartite graph G(Λa∪Λu,
where the weight on each edge is ϕ(i, j) (i ∈ Λa, j ∈ C(i)).
(v) In ODA, one implicit assumption is V a = V u, i.e.,
the Ga and Gu are deﬁned on the same group of users. In
practice, it is possible that V a and V u are not exactly the
same. In this case, if V a and V u are not signiﬁcantly diﬀer-
ent, ODA is also workable at the cost of some performance
degradation ((1 − ϵ)-perfect DA). One better solution could
be estimating the overlap between Ga and Gu ﬁrst, and then
apply ODA to the overlap. We will take the estimation of
the overlap between Ga and Gu as one of the future works.
6.2 Experimental Evaluation and Analysis
6.2.1 Datasets and Setup
We evaluate the performance of ODA on two real world
datasets: Gowalla and Google+ (see the basic information
in Section 5). Gowalla is a location based SN and con-
sists of two diﬀerent datasets [26][27]. The ﬁrst dataset is
a spatiotemporal mobility trace consisting of 6.44M check-
ins generated by .2M users. Each check-in has the format
of .
The second dataset is a social graph (1M edges) of the same
.2M users. Assume the mobility trace is anonymized. Our
objective is to de-anonymize the mobility trace using the so-
cial graph as auxiliary data. Since the mobility trace does
not have an explicit graph structure, supposing the social
graph is the ground truth, we apply the technique in [27]
on the mobility trace to construct four graphs with diﬀerent
recalls and precisions, denoted by M 1, M 2, M 3, and M 4,
(a) Gowalla
(b) Google+
Figure 2:
[0.1, 0.3], c3 ∈ [0.4, 0.8], c4 = 0, α ∈ [10, 30], γ ∈ [1, 4].
Landmark identi(cid:12)cation.
c1, c2 ∈
tp+fn and precision = tp
respectively (recall = tp
tp+fp , where
tp = true positive, fp = false positive, and fn = false neg-
ative). Particularly, the recall and precision of M 1 are 0.6
and 0.865, of M 2 are 0.72 and 0.83, of M 3 are 0.75 and 0.78,
and of M 4 are 0.8 and 0.72, respectively. The second dataset
considered is the Google+ dataset in Section 5, which has
4.7M users and 90.8M edges. Given some projection prob-
ability ℘ ∈ [0.5, 0.9], We ﬁrst use the projection process in
Section 4 to produce Ga and Gu, and then use ODA to de-
anonymize Ga with Gu as auxiliary data. Note that, the
auxiliary data is from a diﬀerent domain (social data) with
the anonymized data (mobility trace) in Gowalla while the
auxiliary and anonymized data are from the same domain
in Google+.
All the experiments are implemented on a PC with 64 bit
Ubuntu 12.04 LTS OS, Intel Xeon E5620 CPU (2.4GHz ×
8 Threads), 48GB memory, and 2 disks with 8TB storage.
6.2.2 Results
Landmark Identi(cid:12)cation. As we mentioned in the pre-
vious subsection, ODA itself can work as a landmark i-
L = ∅ in ODA, i.e.,
dentiﬁcation algorithm. Let V a
s(fl(·), fl(·)) = 0 in ϕ(·,·). Then, we run ODA on Gowalla
and Google+ to identify some landmarks as shown in Fig.
2 (note that, the DA in ODA is conducted according to the
degree non-increasing order). The results show that we can
de-anonymize the ﬁrst 60-94 users in Gowalla and the ﬁrst
129-179 users in Google+ perfectly (100% correctly). For
instance, when Ga = M 2 in Gowalla, the ﬁrst 75 users are