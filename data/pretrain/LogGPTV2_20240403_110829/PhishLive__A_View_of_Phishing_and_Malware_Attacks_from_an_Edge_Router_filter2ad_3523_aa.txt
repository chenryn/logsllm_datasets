title:PhishLive: A View of Phishing and Malware Attacks from an Edge Router
author:Lianjie Cao and
Thibaut Probst and
Ramana Rao Kompella
PhishLive: A View of Phishing and Malware Attacks
from an Edge Router
Lianjie Cao1, Thibaut Probst2, and Ramana Kompella1
1 Purdue University, West Lafayette, Indiana, USA
2 INSA de Toulouse, Toulouse, France
Abstract. Malicious website attacks including phishing, malware, and drive-by
downloads have become a huge security threat to today’s Internet. Various stud-
ies have been focused on approaches to prevent users from being attacked by
malicious websites. However, there exist few studies that focus on the prevalence
and temporal characteristics of such attack trafﬁc. In this paper, we developed
the PhishLive system to study the behavior of malicious website attacks on users
and hosts of the campus network of a large University by monitoring the HTTP
connections for malicious accesses. During our experiment of one month, we
analyzed over 1 billion URLs. Our analysis reveals several interesting ﬁndings.
1 Introduction
The rapid development of the Web over the recent few decades has made the Internet
a hotbed for a wide range of criminal activities. Numerous types of attacks are hidden
behind HTTP connections such as phishing, cross-site scripting, malware, and botnet
attacks. The most commonly used solution to defend against such attacks is using black-
listing. A blacklist-based defense system contains a set of URLs that are identiﬁed as
malicious or suspicious, either through a human-vetting process or other mechanisms.
When users are trying to connect to such web pages, the browsers (e.g., Mozilla Firefox)
pop out warnings or block the web page directly.
Literature is ripe with several studies that focused on documenting the effective-
ness of such browser-based techniques in thwarting malicious website attacks. For ex-
ample, [1] discusses the effectiveness of passive and active warnings to users. Simi-
larly, [2] studies the efﬁcacy of different anti-phishing tools. There also exist several
papers (e.g., [3–6]) proposing different solutions for improving the attack detection and
defense using enhanced blacklisting techniques. Other content-based techniques have
also been proposed (e.g., [7–12]) for detecting malicious webpages, while [13] com-
bines both URL-based and content-based methods.
Unfortunately, to date, there exists only a few studies that focus on understanding
temporal characteristics of phishing or malware accesses in an edge network such as
a campus or an enterprise network comprising of a few tens of thousand users. [14]
analyzes the malware serving infrastructure of drive-by downloads. The paper indicates
that the malware serving networks are composed of tree-like structure and malware
are delivered through several redirections. However, they focus only on drive-by down-
loads ignoring other malicious attacks delivered through webpages and the data set
M. Roughan and R. Chang (Eds.) PAM 2013, LNCS 7799, pp. 239–249, 2013.
c(cid:2) Springer-Verlag Berlin Heidelberg 2013
240
L. Cao, T. Probst, and R. Kompella
they use is collected generally not from a speciﬁc network. [15] assesses the issue of
overt manifestation of three networks consisting of around 30,000 users in total. They
study risky behaviors of users including security threats such as scanning, spamming,
payload signature and botnet rendezvous. Nevertheless, they only study the probability
of triggering malicious activities of users without concluding any possible pattern of
vulnerable users and attackers.
There are however several questions that remain to be answered such as whether ma-
licious sites are accessed just once, or a few times, or are repeatedly accessed over time
across users, and whether other malicious website attacks are hidden between HTTP
redirects. We believe a study to answer such questions is important for many reasons:
First, it can help in sizing the resource requirements of security middleboxes that can
be deployed to defend against them. Second, the temporal characteristics can help gen-
erate insights to inform future defense mechanisms. Thus, in this paper, we focus on
studying and understanding the characteristics of HTTP accesses to malicious sites as
seen by the edge router of a large campus network comprising upwards of 50,000 users.
A key requirement for our study is the ability to identify whether a given access is
to a malicious site or not, for which, we leverage existing blacklisting tools such as
the Google Safe Browsing (GSB) [16] back-end server. Thus, we do not invent any
new mechanism for detecting malicious website attacks, but instead leverage exist-
ing techniques to continuously monitor the network for malicious accesses to phish-
ing/malware websites. Our system called PhishLive monitors the HTTP trafﬁc going
through the gateway of the campus network and captures malicious URLs detected by
GSB database in HTTP requests and redirect responses in real-time. It analyzes the
statistical characteristics of dataset off-line including distribution of attacks over time,
geolocation distribution of attacking IP addresses, attacking hostnames clustering and
malicious redirect chain analysis.
relatively small; in our data, it is less than 0.038% of all URLs.
We deployed PhishLive on a large university gateway for a month during which we
captured and analyzed about 1 billion URLs. Some of our key ﬁndings are:
• The fraction of URLs that are identiﬁed as belonging to phishing/malware sites is
• There is a relatively higher number of malicious URLs accessed during 11:00pm-
• Most domains (almost 50%) typically existed for less than 1 day. However, close to
• An extremely small fraction of all HTTP redirection chains contain malicious URLs;
in our data less than 2,000 URLs are part of redirection chains out of about 50 mil-
lion redirection chains.
5:00am compared to other times.
10% of the domains were accessed for more than 15 days.
In the rest of the paper, we give an overview of the PhishLive system in Section 2 and
outline our observations from a real deployment of PhishLive in Section 3.
2 System Overview
In this section, we describe the design of the PhishLive system that can continuously
monitor HTTP trafﬁc for phishing and malware attacks. We envision PhishLive to be
PhishLive: A View of Phishing and Malware Attacks from an Edge Router
241
Check Module
Update Module
Hash table level 1
RED
Hash table level 2
Local database
Google server
GET
RED
IP2
GET
RED
RED
Pipe with check
module
Malicious URL
Malicious URL
Malicious URL
Malicious 
URLs
HTTP 
requests
Output
Capture Module
Traffic
Monitor
 University
Network
Internet
Gateway
(a) System overview
GET
GET
IP i
GET
RED
IP j
GET
RED
GET
RED
Mark Chain
as Malicious
(b) Operations on two hash tables
Fig. 1. Internals of PhishLive system
deployed at an edge router, such as a campus gateway router, that can track the various
HTTP requests issued by a bunch of users. We assume the presence of a standard high-
speed capture device (e.g., Endace 10Gbps monitoring card) to collect each packet that
is going through the gateway router to the outside world, from which we ﬁlter the HTTP
trafﬁc (port 80) and extract the URLs from HTTP GET requests. For verifying whether
a given URL is malicious or not, each URL is cross checked with the Google Safe
Browsing (GSB) database. Since HTTP redirects are also used to hide malicious content
[17–19] in some attacks, the system is designed to also track and analyze HTTP redirect
chains. We assume access to both directions of trafﬁc for detecting redirects.
The PhishLive system comprises three components: a capture module, a check mod-
ule and an update module (shown in Figure 1(a)); we describe their details next.
Capture Module. The capture module of our system utilizes the libpcap library to cap-
ture the HTTP requests from the hosts inside the university and redirect responses from
the external hosts. As of now, the PhishLive system supports up to 5 types of HTTP
requests: GET, HEAD, POST, PUT, DELETE, and 4 types of HTTP redirect responses:
301, 302, 303, 307. As of now, we only investigate URL redirection based on HTTP
status code; other redirections based on HTML meta tag, Javascript and ﬂash are not
studied in this paper. The capture module also uses network libraries and regular ex-
pressions to extract the URLs from HTTP requests or the URLs in redirect responses,
as well as source/destination IP addresses (SIP, DIP) and source/destination port num-
bers (SP, DP). For privacy reasons, all user-facing IP addresses are hashed.
As part of our analysis, we also wish to study the role of HTTP redirects in phishing
and malware attacks. A redirect chain is a sequence of URLs starting from the ﬁrst re-
quested URL, ending with the last requested URL, that can be represented as follows:
GET (U RL1) → REDIRECT (U RL2) ... → GET (U RLn), where n is the number
of different requested URLs in the chain. Although it appears simple, it is little tricky
to track HTTP redirects in an online fashion, since it requires correlation across TCP
connections (since each GET request is to a different hostname). Thus, in PhishLive,
we build two hash tables (denoted as level-1 hash table and level-2 hash table in Fig-
ure 1(b)) to store HTTP redirects. The level-1 hash table holds related information of a
HTTP request with a key of (cid:3)SIP, SP, DIP, DP(cid:4). Because the request and the redirect
packet belong to the same TCP session, when a HTTP redirect response is captured, the
242
L. Cao, T. Probst, and R. Kompella
capture module checks if there is an existing record in the level-1 hash table with the
same key. If a match is found, it means that this redirect is the response for the matched
HTTP request. Then this pair (HTTP request and redirect response) is extracted from
the level-1 hash table and inserted into the level-2 hash table with the SIP as key.
The level-2 hash table keeps record of all HTTP redirect chains observed in the form
of a linked list. Each slot in the level-2 hash table corresponds to a user-facing IP address
(inside the network). When inserting a pair (HTTP request and redirect response) into
the level-2 hash table, it checks if the URL in the HTTP request matches the URL in the
last record with the same key. A match indicates current request is the HTTP request of
the URL in the last redirect response, and the redirect response is attached to the end
of the chain. If not, a new redirect chain is built. Therefore, one slot in the level-2 hash
table may contain more than one redirect chain. For instance, slot n in the level-2 hash
table includes redirect chains a→b→c and x→y. When a new pair of HTTP redirect
y→z is inserted, it searches for the ﬁrst linked list entry a→b→c and ﬁnds that y does
not match c. Then it moves to next linked list x→y, which matches and the new pair
y→z is attached to the linked list resulting in x→y→z. The operations of the two hash
tables are illustrated in Figure 1(b) .
The capture module also receives feedback which includes the malicious URL and
the victim’s IP address from the check module. It then compares the malicious URL
with the records in the level-2 hash table. If the URL is found in the level-2 hash table,
it will be marked as malicious and dumped to a ﬁle later on. The implementation of
the capture module constitutes of three threads: one thread captures and extracts URLs
from HTTP packets and feeds them to check module; another thread receives results
from check module and scans level-2 hash table for a match; the last thread refreshes
the two hash tables periodically to prevent them from growing too large and a fatal
memory drop-off in a long-term running.
Check and Update Modules. The check module is based on the PHP API of Google
Safe Browsing database provided by Google. The check module maintains a local
database of malicious URLs veriﬁed by GSB server and interacts with the capture mod-
ule through two pipes. Once it receives a URL from capture module, it checks the URL
against the local database and feeds it back to capture module through a pipe if the URL
is identiﬁed as malicious. The check module also produces general real-time statistics.
The update module updates the local database with Google server periodically to ensure
that the content of the local database is up-to-date.
3 Experimental Results
We deployed the PhishLive system at the edge router of a large university network over
30 days from March 19, 2012 to April 18, 2012, during which the system analyzed
more than 1 billion HTTP requests (as summarized in Table 1). Out of the 1 billion
URLs, only about 0.0381% of all HTTP requests were classiﬁed as requests to mali-
cious webpages. We also observed about 50 million HTTP redirect chains out of which
only about 7,500 included malicious URLs.
Since PhishLive system only captures the HTTP requests from hosts and HTTP
redirect responses from servers and veriﬁes the URL by querying GSB database, the
PhishLive: A View of Phishing and Malware Attacks from an Edge Router
243
Table 1. Statistics of the Experiment
Experiment Duration
# of HTTP Requests
# of Redirect Chains
# of Malicious URLs
# of Unique Malicious URLs
# of Malicious Redirect Chains
Mar 19, 2012 - Apr 18, 2012
1,038,803,540
50,204,174
395,671 (0.0381%)
118,615
7,497 (0.0149%)
accuracy of the dataset drawn from the experiment largely depends on the accuracy of
the GSB database. Previous studies [13] indicate that GSB database has a false negative
rate of less than 10%; so we believe that the results are more or less accurate. How-
ever, to improve the credibility of the results, we manually veriﬁed a small sample of
the URLs. Since there could be many malicious URLs with same hostname, we used
a stratiﬁed sampling approach to select samples across different hostnames while en-