# Title: Hey, You, Get Off of My Cloud: Exploring Information Leakage in Third-Party Compute Clouds

## Authors:
- Thomas Ristenpart
- Eran Tromer
- Hovav Shacham
- Stefan Savage

### Affiliations:
- **Dept. of Computer Science and Engineering, University of California, San Diego, USA**
  - {tristenp, hovav, savage}@cs.ucsd.edu
- **Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, USA**
  - PI:EMAIL

## Abstract
Third-party cloud computing services, such as Microsoft's Azure and Amazon's EC2, offer users the ability to instantiate virtual machines (VMs) on demand, allowing them to purchase exactly the capacity they need when they need it. Virtualization enables these providers to maximize the utilization of their physical infrastructure by multiplexing many customer VMs. However, this approach can introduce new vulnerabilities. In this paper, we use Amazon EC2 as a case study to demonstrate that it is possible to map the internal cloud infrastructure, identify where a target VM is likely to reside, and then instantiate new VMs until one is placed co-resident with the target. We explore how such placement can be used to mount cross-VM side-channel attacks to extract information from a target VM on the same machine.

## Categories and Subject Descriptors
K.6.5 [Security and Protection]: UNAUTHORIZED ACCESS

## General Terms
Security, Measurement, Experimentation

## Keywords
Cloud computing, Virtual machine security, Side channels

## 1. Introduction
"Cloud computing" has become a popular term for the next generation of infrastructure for hosting data and deploying software and services. This model, exemplified by Amazon's Elastic Compute Cloud (EC2), Microsoft's Azure Service Platform, and Rackspace's Mosso, offers several advantages, including economies of scale, dynamic provisioning, and low capital expenditures. However, it also introduces new risks. One of these risks is the trust relationship between the customer and the cloud provider, where customers must trust their providers to respect the privacy of their data and the integrity of their computations. Additionally, cloud infrastructures can introduce non-obvious threats from other customers due to the sharing of physical resources between VMs.

In particular, to maximize efficiency, multiple VMs may be assigned to execute on the same physical server. This multiplexing of VMs from different customers on the same hardware can lead to a new threat: an adversary might penetrate the isolation between VMs and violate customer confidentiality. This paper explores the practicality of mounting such cross-VM attacks in existing third-party compute clouds.

The attacks we consider involve two main steps: placement and extraction. Placement involves the adversary arranging to place their malicious VM on the same physical machine as the target customer. Using Amazon's EC2 as a case study, we demonstrate that careful empirical "mapping" can reveal how to launch VMs to maximize the likelihood of advantageous placement. We find that in some natural attack scenarios, just a few dollars invested in launching VMs can produce a 40% chance of placing a malicious VM on the same physical server as a target customer. We also demonstrate the existence of simple, low-overhead, "co-residence" checks to determine when such advantageous placement has occurred. While we focus on EC2, we believe that variants of our techniques are likely to generalize to other services, such as Microsoft's Azure or Rackspace's Mosso, as we only utilize standard customer capabilities and do not require cloud providers to disclose details of their infrastructure or assignment policies.

Once a VM is co-resident with the target, the next step is to extract confidential information via a cross-VM attack. We focus on side-channel attacks, which exploit the sharing of physical resources (e.g., CPU data caches). In a multi-process environment, such attacks have been shown to enable the extraction of RSA and AES secret keys. However, there are significant practical challenges in extending these attacks to the virtual machine environment. We show preliminary results on cross-VM side-channel attacks, including cache load measurements in EC2 and coarse-grained attacks such as measuring activity burst timing for cross-VM keystroke monitoring. These results point to the practicality of side-channel attacks in cloud-computing environments.

Overall, our findings indicate tangible dangers when deploying sensitive tasks to third-party compute clouds. In the remainder of this paper, we explain these findings in more detail and discuss means to mitigate the problem. We argue that the best solution is for cloud providers to explicitly expose this risk and give some placement control directly to customers.

## 2. Threat Model
As more applications are deployed to third-party compute clouds, it becomes increasingly important to quantify any threats to confidentiality. Cloud computing services are already used for e-commerce, medical record services, and back-office business applications, all of which require strong confidentiality guarantees. An obvious threat is malicious behavior by the cloud provider, who is in a position to violate customer confidentiality or integrity. However, this is a known risk with analogs in virtually any industry practicing outsourcing.

In this work, we consider the provider and its infrastructure to be trusted. We do not consider attacks that rely on subverting the cloud's administrative functions via insider abuse or vulnerabilities in the cloud management systems (e.g., virtual machine monitors).

In our threat model, adversaries are non-provider-affiliated malicious parties, and victims are users running confidentiality-requiring services in the cloud. A traditional threat is direct compromise, where an attacker attempts to remotely exploit vulnerabilities in the software running on the system. While important, these kinds of attacks are a known threat, and the risks they present are understood.

We focus on where third-party cloud computing gives attackers novel abilities, implicitly expanding the attack surface of the victim. We assume that, like any customer, a malicious party can run and control many instances in the cloud simply by contracting for them. Since the economies offered by third-party compute clouds derive from multiplexing physical infrastructure, we assume (and later validate) that an attacker’s instances might even run on the same physical hardware as potential victims. From this vantage, an attacker might manipulate shared physical resources (e.g., CPU caches, branch target buffers, network queues, etc.) to learn otherwise confidential information.

We consider two types of attackers: those who cast a wide net and are interested in attacking any known hosted service, and those focused on attacking a particular victim service. The latter’s task is more expensive and time-consuming than the former’s, but both rely on the same fundamental attack.

In this work, we initiate a rigorous research program aimed at exploring the risk of such attacks, using Amazon EC2 as a case study. We address the following concrete questions in subsequent sections:
- Can one determine where in the cloud infrastructure an instance is located? (Section 5)
- Can one easily determine if two instances are co-resident on the same physical machine? (Section 6)
- Can an adversary launch instances that will be co-resident with other users' instances? (Section 7)
- Can an adversary exploit cross-VM information leakage once co-resident? (Section 8)

Throughout, we offer discussions of defenses a cloud provider might try to prevent the success of the various attack steps.

## 3. The EC2 Service
Amazon’s Elastic Compute Cloud (EC2) is a well-known example of a third-party compute cloud, enabling users to flexibly rent computational resources for their applications. EC2 provides the ability to run Linux, FreeBSD, OpenSolaris, and Windows as guest operating systems within a virtual machine (VM) provided by a version of the Xen hypervisor. The hypervisor acts as a virtual machine monitor, providing isolation between VMs and intermediating access to physical memory and devices. A privileged virtual machine, called Domain0 (Dom0) in Xen terminology, manages guest images, their physical resource provisioning, and any access control rights. In EC2, the Dom0 VM is configured to route packets for its guest images and reports itself as a hop in traceroutes.

When first registering with EC2, each user creates an account—uniquely specified by their contact email address—and provides credit card information for billing. With a valid account, a user can create one or more VM images based on a supplied Xen-compatible kernel, with an otherwise arbitrary configuration. They can run one or more copies of these images on Amazon’s network of machines. A running image is called an instance, and when launched, it is assigned to a single physical machine within the EC2 network for its lifetime. EC2 does not currently support live migration of instances, although this should be technically feasible. By default, each user account is limited to 20 concurrently running instances.

There are three degrees of freedom in specifying the physical infrastructure upon which instances should run. At the time of writing, Amazon provides two “regions,” one in the United States and another in Europe. Each region contains three “availability zones” with distinct and independent failure modes (e.g., separate power and network connectivity). When requesting the launch of an instance, a user specifies the region and may choose a specific availability zone (otherwise, one is assigned on their behalf). The user can also specify an “instance type,” indicating a particular combination of computational power, memory, and persistent storage space available to the virtual machine. There are five Linux instance types documented: ‘m1.small’, ‘c1.medium’, ‘m1.large’, ‘m1.xlarge’, and ‘c1.xlarge’. The first two are 32-bit architectures, and the latter three are 64-bit. For example, the “small compute slot” (m1.small) is described as a single virtual core providing one ECU (EC2 Compute Unit, claimed to be equivalent to a 1.0–1.2 GHz 2007 Opteron or 2007 Xeon processor) combined with 1.7 GB of memory and 160 GB of local storage, while the “large compute slot” (m1.large) provides 2 virtual cores each with 2 ECUs, 7.5GB of memory, and 850GB of local storage. As expected, instances with more resources incur greater hourly charges (e.g., ‘m1.small’ in the United States region is currently $0.10 per hour, while ‘m1.large’ is $0.40 per hour). When launching an instance, the user specifies the instance type along with a compatible virtual machine image.

Given these constraints, virtual machines are placed on available physical servers shared among multiple instances. Each instance is given Internet connectivity via both an external IPv4 address and domain name and an internal RFC 1918 private address and domain name. For example, an instance might be assigned external IP 75.101.210.100, external name `ec2-75-101-210-100.compute-1.amazonaws.com`, internal IP 10.252.146.52, and internal name `domU-12-31-38-00-8D-C6.compute-1.internal`. Within the cloud, both domain names resolve to the internal IP address; outside the cloud, the external name is mapped to the external IP address.

Note that we focus on the United States region—in the rest of the paper, EC2 implicitly refers to this region.

## 4. Network Probing
In the next several sections, we describe an empirical measurement study focused on understanding VM placement in the EC2 system and achieving co-resident placement for an adversary. To do this, we use network probing to identify public services hosted on EC2 and to provide evidence of co-residence (that two instances share the same physical server). Specifically, we use nmap, hping, and wget to perform network probes to determine the liveness of EC2 instances. We use nmap to perform TCP connect probes, which attempt to complete a 3-way handshake between a source and target. We use hping to perform TCP SYN traceroutes, which iteratively send TCP SYN packets with increasing time-to-lives (TTLs) until no ACK is received. Both TCP connect probes and SYN traceroutes require a target port; we only targeted ports 80 or 443. We used wget to retrieve web pages, but capped so that at most 1024 bytes are retrieved from any individual web server.

We distinguish between two types of probes: external probes and internal probes. An external probe originates from a system outside EC2 and has an EC2 instance as its destination. An internal probe originates from an EC2 instance (under our control) and has another EC2 instance as its destination. This distinction is relevant because internal probing is subject to Amazon’s acceptable use policy, whereas external probing is not (we discuss the legal, ethical, and contractual issues around such probing in Appendix A).

We use DNS resolution queries to determine the external name of an instance and to determine the internal IP address of an instance associated with a public IP address. The latter queries are always performed from an EC2 instance.

## 5. Cloud Cartography
In this section, we 'map' the EC2 service to understand...