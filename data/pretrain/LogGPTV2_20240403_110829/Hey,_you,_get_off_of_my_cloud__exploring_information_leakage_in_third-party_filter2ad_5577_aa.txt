title:Hey, you, get off of my cloud: exploring information leakage in third-party
compute clouds
author:Thomas Ristenpart and
Eran Tromer and
Hovav Shacham and
Stefan Savage
Hey, You, Get Off of My Cloud:
Exploring Information Leakage in
Third-Party Compute Clouds
Thomas Ristenpart
∗
†
Eran Tromer
†
∗
Hovav Shacham
∗
Stefan Savage
∗
Dept. of Computer Science and Engineering
University of California, San Diego, USA
{tristenp,hovav,savage}@cs.ucsd.edu
Computer Science and Artiﬁcial Intelligence Laboratory
Massachusetts Institute of Technology, Cambridge, USA
PI:EMAIL
ABSTRACT
Third-party cloud computing represents the promise of out-
sourcing as applied to computation. Services, such as Mi-
crosoft’s Azure and Amazon’s EC2, allow users to instanti-
ate virtual machines (VMs) on demand and thus purchase
precisely the capacity they require when they require it.
In turn, the use of virtualization allows third-party cloud
providers to maximize the utilization of their sunk capital
costs by multiplexing many customer VMs across a shared
physical infrastructure. However, in this paper, we show
that this approach can also introduce new vulnerabilities.
Using the Amazon EC2 service as a case study, we show that
it is possible to map the internal cloud infrastructure, iden-
tify where a particular target VM is likely to reside, and then
instantiate new VMs until one is placed co-resident with the
target. We explore how such placement can then be used to
mount cross-VM side-channel attacks to extract information
from a target VM on the same machine.
Categories and Subject Descriptors
K.6.5 [Security and Protection]: UNAUTHORIZED AC-
CESS
General Terms
Security, Measurement, Experimentation
Keywords
Cloud computing, Virtual machine security, Side channels
1.
INTRODUCTION
It has become increasingly popular to talk of “cloud com-
puting” as the next infrastructure for hosting data and de-
ploying software and services. In addition to the plethora of
technical approaches associated with the term, cloud com-
puting is also used to refer to a new business model in which
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
CCS’09, November 9–13, 2009, Chicago, Illinois, USA.
Copyright 2009 ACM 978-1-60558-352-5/09/11 ...$10.00.
core computing and software capabilities are outsourced on
demand to shared third-party infrastructure. While this
model, exempliﬁed by Amazon’s Elastic Compute Cloud
(EC2) [5], Microsoft’s Azure Service Platform [20], and Rack-
space’s Mosso [27] provides a number of advantages — in-
cluding economies of scale, dynamic provisioning, and low
capital expenditures — it also introduces a range of new risks.
Some of these risks are self-evident and relate to the new
trust relationship between customer and cloud provider. For
example, customers must trust their cloud providers to re-
spect the privacy of their data and the integrity of their
computations. However, cloud infrastructures can also in-
troduce non-obvious threats from other customers due to
the subtleties of how physical resources can be transparently
shared between virtual machines (VMs).
In particular, to maximize eﬃciency multiple VMs may
be simultaneously assigned to execute on the same physi-
cal server. Moreover, many cloud providers allow “multi-
tenancy” — multiplexing the virtual machines of disjoint
customers upon the same physical hardware. Thus it is con-
ceivable that a customer’s VM could be assigned to the same
physical server as their adversary. This in turn, engenders
a new threat — that the adversary might penetrate the iso-
lation between VMs (e.g., via a vulnerability that allows
an “escape” to the hypervisor or via side-channels between
VMs) and violate customer conﬁdentiality. This paper ex-
plores the practicality of mounting such cross-VM attacks
in existing third-party compute clouds.
The attacks we consider require two main steps: place-
ment and extraction. Placement refers to the adversary ar-
ranging to place their malicious VM on the same physical
machine as that of a target customer. Using Amazon’s EC2
as a case study, we demonstrate that careful empirical “map-
ping” can reveal how to launch VMs in a way that maximizes
the likelihood of an advantageous placement. We ﬁnd that
in some natural attack scenarios, just a few dollars invested
in launching VMs can produce a 40% chance of placing a
malicious VM on the same physical server as a target cus-
tomer. Using the same platform we also demonstrate the
existence of simple, low-overhead, “co-residence” checks to
determine when such an advantageous placement has taken
place. While we focus on EC2, we believe that variants
of our techniques are likely to generalize to other services,
such as Microsoft’s Azure [20] or Rackspace’s Mosso [27], as
we only utilize standard customer capabilities and do not
require that cloud providers disclose details of their infras-
tructure or assignment policies.
199Having managed to place a VM co-resident with the tar-
get, the next step is to extract conﬁdential information via
a cross-VM attack. While there are a number of avenues
for such an attack, in this paper we focus on side-channels:
cross-VM information leakage due to the sharing of physical
resources (e.g., the CPU’s data caches). In the multi-process
environment, such attacks have been shown to enable ex-
traction of RSA [26] and AES [22] secret keys. However, we
are unaware of published extensions of these attacks to the
virtual machine environment; indeed, there are signiﬁcant
practical challenges in doing so.
We show preliminary results on cross-VM side channel at-
tacks, including a range of building blocks (e.g., cache load
measurements in EC2) and coarse-grained attacks such as
measuring activity burst timing (e.g., for cross-VM keystroke
monitoring). These point to the practicality of side-channel
attacks in cloud-computing environments.
Overall, our results indicate that there exist tangible dan-
gers when deploying sensitive tasks to third-party compute
clouds.
In the remainder of this paper, we explain these
ﬁndings in more detail and then discuss means to mitigate
the problem. We argue that the best solution is for cloud
providers to expose this risk explicitly and give some place-
ment control directly to customers.
2. THREAT MODEL
As more and more applications become exported to third-
party compute clouds, it becomes increasingly important to
quantify any threats to conﬁdentiality that exist in this set-
ting. For example, cloud computing services are already
used for e-commerce applications, medical record services [7,
11], and back-oﬃce business applications [29], all of which
require strong conﬁdentiality guarantees. An obvious threat
to these consumers of cloud computing is malicious behav-
ior by the cloud provider, who is certainly in a position to
violate customer conﬁdentiality or integrity. However, this
is a known risk with obvious analogs in virtually any in-
dustry practicing outsourcing.
In this work, we consider
the provider and its infrastructure to be trusted. This also
means we do not consider attacks that rely upon subverting
a cloud’s administrative functions, via insider abuse or vul-
nerabilities in the cloud management systems (e.g., virtual
machine monitors).
In our threat model, adversaries are non-provider-aﬃliated
malicious parties. Victims are users running conﬁdentiality-
requiring services in the cloud. A traditional threat in such a
setting is direct compromise, where an attacker attempts re-
mote exploitation of vulnerabilities in the software running
on the system. Of course, this threat exists for cloud appli-
cations as well. These kinds of attacks (while important) are
a known threat and the risks they present are understood.
We instead focus on where third-party cloud computing
gives attackers novel abilities; implicitly expanding the at-
tack surface of the victim. We assume that, like any cus-
tomer, a malicious party can run and control many instances
in the cloud, simply by contracting for them. Further, since
the economies oﬀered by third-party compute clouds derive
from multiplexing physical infrastructure, we assume (and
later validate) that an attacker’s instances might even run
on the same physical hardware as potential victims. From
this vantage, an attacker might manipulate shared physical
resources (e.g., CPU caches, branch target buﬀers, network
queues, etc.) to learn otherwise conﬁdential information.
In this setting, we consider two kinds of attackers: those
who cast a wide net and are interested in being able to attack
some known hosted service and those focused on attacking a
particular victim service. The latter’s task is more expensive
and time-consuming than the former’s, but both rely on the
same fundamental attack.
In this work, we initiate a rigorous research program aimed
at exploring the risk of such attacks, using a concrete cloud
service provider (Amazon EC2) as a case study. We address
these concrete questions in subsequent sections:
• Can one determine where in the cloud infrastructure an
instance is located? (Section 5)
• Can one easily determine if two instances are co-resident
on the same physical machine? (Section 6)
• Can an adversary launch instances that will be co-resident
with other user’s instances? (Section 7)
• Can an adversary exploit cross-VM information leakage
once co-resident? (Section 8)
Throughout we oﬀer discussions of defenses a cloud provider
might try in order to prevent the success of the various at-
tack steps.
3. THE EC2 SERVICE
By far the best known example of a third-party compute
cloud is Amazon’s Elastic Compute Cloud (EC2) service,
which enables users to ﬂexibly rent computational resources
for use by their applications [5]. EC2 provides the ability
to run Linux, FreeBSD, OpenSolaris and Windows as guest
operating systems within a virtual machine (VM) provided
by a version of the Xen hypervisor [9].1 The hypervisor
plays the role of a virtual machine monitor and provides
isolation between VMs, intermediating access to physical
memory and devices. A privileged virtual machine, called
Domain0 (Dom0) in the Xen vernacular, is used to manage
guest images, their physical resource provisioning, and any
access control rights. In EC2 the Dom0 VM is conﬁgured
to route packets for its guest images and reports itself as a
hop in traceroutes.
When ﬁrst registering with EC2, each user creates an ac-
count — uniquely speciﬁed by its contact e-mail address —
and provides credit card information for billing compute and
I/O charges. With a valid account, a user creates one or
more VM images, based on a supplied Xen-compatible ker-
nel, but with an otherwise arbitrary conﬁguration. He can
run one or more copies of these images on Amazon’s network
of machines. One such running image is called an instance,
and when the instance is launched, it is assigned to a single
physical machine within the EC2 network for its lifetime;
EC2 does not appear to currently support live migration of
instances, although this should be technically feasible. By
default, each user account is limited to 20 concurrently run-
ning instances.
In addition, there are three degrees of freedom in specify-
ing the physical infrastructure upon which instances should
run. At the time of this writing, Amazon provides two
“regions”, one located in the United States and the more
recently established one in Europe. Each region contains
three “availability zones” which are meant to specify in-
frastructures with distinct and independent failure modes
1We will limit our subsequent discussion to the Linux ker-
nel. The same issues should apply for other guest operating
systems.
200(e.g., with separate power and network connectivity). When
requesting launch of an instance, a user speciﬁes the re-
gion and may choose a speciﬁc availability zone (otherwise
one is assigned on the user’s behalf). As well, the user
can specify an “instance type”, indicating a particular com-
bination of computational power, memory and persistent
storage space available to the virtual machine. There are
ﬁve Linux instance types documented at present, referred
to as ‘m1.small’, ‘c1.medium’, ‘m1.large’, ‘m1.xlarge’, and
‘c1.xlarge’. The ﬁrst two are 32-bit architectures, the latter
three are 64-bit. To give some sense of relative scale, the
“small compute slot” (m1.small) is described as a single vir-
tual core providing one ECU (EC2 Compute Unit, claimed to
be equivalent to a 1.0–1.2 GHz 2007 Opteron or 2007 Xeon
processor) combined with 1.7 GB of memory and 160 GB
of local storage, while the “large compute slot” (m1.large)
provides 2 virtual cores each with 2 ECUs, 7.5GB of mem-
ory and 850GB of local storage. As expected, instances with
more resources incur greater hourly charges (e.g., ‘m1.small’
in the United States region is currently $0.10 per hour, while
‘m1.large’ is currently $0.40 per hour). When launching an
instance, the user speciﬁes the instance type along with a
compatible virtual machine image.
Given these constraints, virtual machines are placed on
available physical servers shared among multiple instances.
Each instance is given Internet connectivity via both an
external IPv4 address and domain name and an internal
RFC 1918 private address and domain name. For example,
an instance might be assigned external IP 75.101.210.100,
external name
ec2-75-101-210-100.compute-1.amazonaws
.com, internal IP 10.252.146.52, and internal name domU-
12-31-38-00-8D-C6.compute-1.internal. Within the cloud,
both domain names resolve to the internal IP address; out-
side the cloud the external name is mapped to the external
IP address.
Note that we focus on the United States region — in the
rest of the paper EC2 implicitly means this region of EC2.
4. NETWORK PROBING
In the next several sections, we describe an empirical mea-
surement study focused on understanding VM placement
in the EC2 system and achieving co-resident placement for
an adversary. To do this, we make use of network probing
both to identify public services hosted on EC2 and to pro-
vide evidence of co-residence (that two instances share the
same physical server). In particular, we utilize nmap, hping,
and wget to perform network probes to determine liveness
of EC2 instances. We use nmap to perform TCP connect
probes, which attempt to complete a 3-way hand-shake be-
tween a source and target. We use hping to perform TCP
SYN traceroutes, which iteratively sends TCP SYN pack-
ets with increasing time-to-lives (TTLs) until no ACK is
received. Both TCP connect probes and SYN traceroutes
require a target port; we only targeted ports 80 or 443. We
used wget to retrieve web pages, but capped so that at most
1024 bytes are retrieved from any individual web server.
We distinguish between two types of probes: external
probes and internal probes. A probe is external when it
originates from a system outside EC2 and has destination
an EC2 instance. A probe is internal if it originates from
an EC2 instance (under our control) and has destination
another EC2 instance. This dichotomy is of relevance par-
ticularly because internal probing is subject to Amazon’s
acceptable use policy, whereas external probing is not (we
discuss the legal, ethical and contractual issues around such
probing in Appendix A).
We use DNS resolution queries to determine the external
name of an instance and also to determine the internal IP
address of an instance associated with some public IP ad-
dress. The latter queries are always performed from an EC2
instance.
5. CLOUD CARTOGRAPHY
In this section we ‘map’ the EC2 service to understand