{A,C,E},{B,C,E}
216
（HBase负载高}→{对象库延迟}→{发微博功能失败率高）
{对象库延迟}→{发微博功能失败率高）
{HBase 负载高}→{对象库延迟}
由于篇幅有限，这里就不深入展开介绍了。关于算法的实现，可以参考 https://github.com/
{A,B,C}的 2-项子集是{A,B},{A,C},{B,C},其中{A,B}不是 L2的元素，所以删除这个选项;
（2）使用 Apriori 性质剪枝：频繁项集的所有子集必须是频繁的，对候选项集C3，我们可
假如现在故障的症状就是“发微博功能失败率高”，那么一定会同时收到“对象库延迟”和
现在我们通过分析和挖掘，得到如下告警事件关联规则：
当关联规则生成后，可以用来辅助进行事件（错误、告警）相关性分析。当一个告警/事件
Apriori算法的性能瓶颈：需要产生大量的候选项集，以及需要重复地扫描数据库。2000 年
在上一步产生的频繁项集的基础上生成满足最小置信度的规则，就称为强规则。
（3）这样，剪枝后得到C={{B,C,E}}。
(1） 连接: C;=L2, L2= {A,C},{B,C},{B,E}{C,E} {A,C},{B,C},{B,E}{C,E} = {{A,B,C},
说明：
智能运维：从0搭建大规模分布式AIOps系统
---
## Page 243
的经验值。
规则，就可以对这些规则进行总结，当某个应用发生告警时，其判断逻辑可以取如图12-8所示
个维度进行切分的，并不断重复这个过程。当然，如果切分的顺序不同，则会得到不同的树。
是一种非常典型分类方法（算法)。
行处理，利用归纳算法生成可读的规则和决策树，然后使用决策树对新数据进行分析。决策树
的预测结果。
点都表示对象属性的判断条件，其分支表示符合节点条件的对象。树的叶子节点表示对象所属
器学习中，决策树是一个预测模型，表示对象属性和对象值之间的一种映射，树中的每一个节
12.5.2
回到故障分析和诊断问题上，假如对于某类故障的处理有了一定的经验，形成了一系列的
简单来说，决策树算法其实就是根据已知的经验来构建一棵树。可以认为是根据数据的某
决策树[5]16]是附加概率结果的一个树状的决策图，是直观地运用统计概率分析的图法。在机
和关联规则不同，决策树属于有监督的机器学习方式。在实际应用中，可以通过对数据进
图12-7展示了一个简单的决策树模型。
基于决策树的故障诊断
True
图12-7简单的决策树模型
True
False
True
False
第12章故障诊断和分析策略
False
217
---
## Page 244
最终到达叶子节点时做出决策（具体操作)。当然，在实践中，维度会比这里多很多。
218
决策树被用在故障诊断中有如下几个明显的优势。
这就是决策树，在每一层我们都提出了一个问题，然后根据问题的回答走向不同的子树，
这些判断过程可以被绘制成一棵树，如图12-9所示。
O
智能运维：从0搭建大规模分布式AIOps系统
数据简单，并且不需要规范化。
对中间值的缺失不敏感。
策树的深度。
效率高。决策树只需构建一次，可以反复使用，每一次预测的最大计算次数不超过决
根据人的经验来构建决策树，易于理解和实现。
单机503
和
是否单机问题
AvgTime超过阔值：否容量是否足够：否
AvgTime超过阔值：否
AvgTime超过阔值：否
AvgTime超过阔值：否
AvgTime超过阔值：是
AvgTime超过阀值：是
AvgTime超过阔值：是
AvgTime超过阔值：是容量是否足够：是
降级
否
图12-9基于AvgTime超过阈值的决策树
先508后
容量是否足够
是
容量是否足够：否
容量是否足够：是
容量是否足够：是
容量是否足够：否
容量是否足够：否
容量是否足够：是
是否单机问题
图12-8
扩容
是
否
否
故障分析中的经验总结
AvgTime超过阅值
单机503
是否单机问题：否
是否单机问题：是
是否单机问题：否
是否单机问题：是
是否单机问题：否
是否单机问题：是
是否单机问题：否
是否单机问题：是
是
是否单机问题
其他
容量是否足够
否
否
扩容
先503后扩容
需要其他辅助信息
单机503
扩容
先503后扩容
降级
单机503
先5后扩容
是
是否单机问题
否
---
## Page 245
样的属性，因为每个ID 都对应一个类别，所以会造成分类很细碎。
为本次分裂属性。每次分裂都会使树长高一层。这样逐步做下去，就可以构建一棵决策树。
集 D，计算划分后的数据子集的熵，为H(DIA)，则信息增益为划分前后的熵值之差。公式如
增益可以衡量某个特征对分类结果的影响大小。
征对数据集进行分类时，分类后的数据集信息熵会比分类前小，其差值表示为信息增益。信息
选择分裂后信息增益最大的属性进行分裂，即递归选择分类能力最强的特征对数据进行分割。
核心思想是自顶向下贪婪搜索遍历可能的决策树空间构造决策树，以信息增益度量属性的选择，
农定理（信息熵)。ID3 算法选择具有最高信息增益的自变量作为当前的树权（树的分支)，其
义是使得子节点中的训练集尽量纯。不同的算法使用不同的指标来定义“最好”。
ID3 有一些缺陷，就是在分类时容易选择一些比较容易分裂的属性，尤其在具有像ID 值这
ID3 决策树算法就用到了上面的信息增益，每次分裂都贪心选择信息增益最大的属性，作
在决策树算法中，划分前样本集合D 的熵是一定的，为H(D)，使用某个特征A划分数据
2.
条件熵公式如下：
其中 D 为样本集合。
信息熵公式如下：
信息熵表示的是不确定度。在均匀分布时，不确定度最大，此时熵就最大。当选择某个特
信息增益的含义就是指划分数据前后信息发生的变化。信息增益的思想来源于信息论的香
1．ID3（迭代二叉树3代）：采用信息增益来选择树权
决策树的构建算法有如下几种。
构建决策树通常采用自上而下的方法，每一步都选择一个最好的属性来分裂。“最好”的定
C4.5：采用增益率
H(Y|X) = Z²=op;H(Y|x = xi)
I(D,A) = H(D) - H(D|A)
H(D) =-Z=op;log pi
第12章故障诊断和分析策略
(12-3)
(12-2)
(12-1)
219
---
## Page 246
无关；不随信息的具体表达式的变化而变化；独立于形式，反映了信息表达式中统计方面的性
乱，熵就越大。定义信息增益率，公式如下：
220
（1）收集数据。
C4.5的Python 实现：请参见 http:/github.com/geerk/C45algorithm。
ID3 的 Python 实现：
这个公式的特点是：Gini 指数是统计学上的抽象概念，
（2）处理数据，
决策树的生成步骤如下：
Gini 指数定义如下:
Gini指数表示在样本集合中一个随机选中的样本被分错的概率。
3.CART（分类回归树）：采用Gini指数
C4.5 就是选择最大增益率的属性来分裂，其他类似于 ID3.5。
C4.5算法定义了分裂信息，公式如下：
O
O
总体包含的类别越杂乱，Gini指数就越大（跟熵的概念很相似)。
在经济学中它通常被用来度量收入的不平衡度，可以推广到更广泛的场景，用于度量
它是一种不等性度量。
智能运维：从O搭建大规模分布式AIOps系统
它是介于0~1之间的数，0一完全相等，1一完全不相等。
任何不均匀分布。
对数据进行预处理和清洗加工。
：请参见 https://github.com/NinjaSteph/DecisionTree。
Gini(p) =∑K=1Pk(1 -pk)= 1 -∑K=1p²
，可以看作是属性分裂的熵，分得越多就越混
它有明确定义的科学名词且与内容
(12-6)
(12-5)
(12-4)
---
## Page 247
所标识的类称为majorityclass。
代替，而这个叶子节点所属的类别，可以用这棵子树中大多数训练样本所属的类别来进行标识，
class criterion）确定。所谓大多数原则，是指在剪枝过程中，将一些子树删除而用一个叶子节点
删除一些子树，然后用其叶子节点代替，这些叶子节点所标识的类别通过大多数原则（majority
合并为一个节点，其中包含了所有可能的结果。后剪枝是目前最普遍的做法。后剪枝的过程是
行检查，判断如果将其合并，熵的增加量是否小于某一阈值。如果确实小，则这一组节点可以
法的效果并不好。
数量小于这个阈值时，即使还可以继续降低熵，也停止继续构建分支。但是在实际中，这种方
低熵的情况下才会停止构建分支的过程的。为了避免过拟合，可以设定一个阈值，当熵减小的
基础上，通过剪枝生成一个简化了的决策树呢？
一个经过简化了的决策树的错误率要高。那么现在的问题就是，如何在原生过拟合的决策树的
这个问题，他做过一个试验，得到的结果是：在某一个数据集中，过拟合的决策树的错误率比
也会100%一点不留地被决策树学习，这就是“过拟合”。C4.5的缔造者昆兰教授很早就发现了
100%完美拟合训练样本的产物）。
现堪称完美，它可以100%完美正确地对训练样本集中的样本进行分类（因为决策树本身就是
的。因此，用这棵决策树对训练样本进行分类的话，你会发现对于训练样本而言，这棵树的表
树非常详细而庞大，每个属性都被详细考虑了，决策树的树叶节点所覆盖的训练样本都是“纯”
用第3步的方法。
增益最大。划分方式是按照当前甄选的特征进行划分。
（4）如果划分后的数据全是同一种类型（即香农熵为0），则不再划分；否则，继续递归调
后剪枝是指在决策树构建完成后进行剪枝。剪枝的过程是对拥有同样父节点的一组节点进
（2）后剪枝（Post-Pruning）
预剪枝是指在构建决策树的同时进行剪枝。所有决策树的构建方法，都是在无法进一步降
（1）预剪枝（Pre-Pruning）
但是，这会带来一个问题。如果训练样本中包含了一些错误，按照前面的算法，这些错误
决策树为什么要剪枝？原因就是避免决策树“过拟合”样本。通过前面的算法生成的决策
下面介绍一下决策树的剪枝。
（3）对多个可以划分的特征进行甄选。甄选的标准是保证通过甄选之后划分的结果的信息
第12章故障诊断和分析策略
221
---
## Page 248
using Decision Tree,2007
Using Decision Trees, 2004
location of a long transmission line, 2015
networks,2004
12.7
法，以达到最好的分析效果。
传统方法和基于人工智能的分析方法。在实际应用中，我们需要结合具体业务，采用不同的方
是针对已经发生或正在发生的事件进行分析的。本章重点讨论了故障诊断和分析的方法，包括
定程度上降低了故障的持续时间，减小了因故障带来的损失。
提高系统的可用性，一个好的诊断方法能够快速、高效地找到故障根源，加快解决问题，在一
12.6本章小结
222
[6] Ngoc-Tu Nguyen, Jeong-Min Kwon, Hong-Hee Lee. A Study on Machine Fault Diagnosis
[5] Mike Chen, Alice X. Zheng, Jim Lloyd, Michael I. Jordan, Eric Brewer. Failure Diagnosis 
 [4] Papia Ray, Debani Prasad Mishra. Support vector machine based fault classfication and 
[1] Malgorazta Steinder, Adarshpal S.Sethi. A survey of fault localization techniques in computer
在智能运维领域存在三种场景：历史事件分析、
[3]啤酒与尿布（数据挖掘领域著名案例，用来解释关联规则的提取)
[2]李金凤，王怀彬．基于关联规则的网络故障告警相关性分析，2012
故障诊断和分析是智能运维研究的非常重要的一个领域，高效地进行故障诊断和分析可以
智能运维：从O搭建大规模分布式AIOps系统
参考文献
、当前事件分析和未来事件分析。故障诊断
---
## Page 249
13.1
模型和神经网络模型几种方法。
ARIMA模型和自回归条件异方差模型等。下面将详细介绍移动平均法、指数平滑法、ARIMA
季节调整模型、移动平均法、指数平滑法等；若导致非平稳的原因是随机的，则方法主要有
series）分析时，若导致非平稳的原因是确定的，则可以使用的方法主要有趋势拟合模型、
如预测某产品的请求量（PV）和用户增长趋势。
该社会现象随时间变化的规律，得出一定的模式；以此模式来预测该社会现象将来的情况，比
理某种社会现象的历史资料；对这些资料进行检查鉴别，排成数列；分析时间数列，从中寻找