with ﬂoating point gradient information performs signif-
icantly better than PGA with binary gradients for every
program. These results indicate that precise gradients are
key to the performance gains achieved by PGA because
they compose accurately over multiple operations.
Compiler Optimization. We evaluate the impact of
compiler optimization levels on dataﬂow accuracy at
3 optimization levels: -O0, -O1, and -O2. Table 5 sum-
marizes the eﬀects of 3 compiler optimization levels on
dataﬂow F1 accuracy. Increasing the compiler optimiza-
tion levels reduces the accuracy of both PGA and DTA
by a small amount (<3.6%) for both -O1 and -O2. On
average, PGA is at least 18% more accurate than DTA
for all three tested optimization levels.
Neutaint hotbyte evaluation. Neutaint’s neural net-
work based approach does not perform well in ﬁne grained
dataﬂow prediction, but is better suited to identifying hot
bytes (input bytes that are most inﬂuential to program
behavior). We therefore perform the hot byte evaluation
described in [43] on PGA. Our results are summarized
in Table 6. On average, PGA predicts hotbytes with
43.75% accuracy, while Neutaint predicts hotbytes with
64.25% accuracy. We see Neutaint as a complementary
method to PGA, where PGA is better suited to ﬁne
grained dataﬂow prediction and both methods could be
used together in program analysis.
Zero gradient analysis. PGA is able to avoid over-
tainting when it computes a zero gradient on an in-
struction DTA would mark as tainted. Therefore we
investigate the distribution of zero gradients across pro-
grams and instruction types to determine where and
how PGA is more precise than DTA. For each program
and each type of instruction, we count how many times
the instruction had zero gradient in the execution traces
from the accuracy evaluation. Table 8 shows the results
of this analysis for each instruction and program.
QIF Comparison. We compare PGA with the latest
version of a publicly available QIF tool Flowcheck [28].
We perform a similar experiment to Section 5.2.1, but
since Flowcheck does not byte-level granularity, we com-
Program Neutaint PGA
99%
mutool
1%
xmllint
33%
djpeg
42%
miniunz
73%
76%
37%
71%
Table 6: Neutaint Hotbyte Evaluation results. On
average, Neutaint predicts hot bytes with 64.25%
accuracy and PGA with 43.75% accuracy. We be-
lieve Neutaint outperforms PGA because it makes a
prediction based on many program inputs, whereas
PGA makes a prediction based on a single input.
Note our results diﬀerent from the original Neutaint
paper [43] due to diﬀerent initializations and envi-
ronments for training the neural network.
PGA
Prec. Rec.
0.7
0.62
0.87
0.62
0.64
0.89
0.95
0.99
0.75
0.79
0.88
0.66
0.74
0.93
Prec. Rec.
0.44
0.62
0.95
0.44
0.77
0.69
0.08
0.55
0.71
0.62
0.65
0.49
0.74
0.64
minigzip
djpeg
mutool
xmllint
objdump
strip
size
Flowcheck
F1
0.52
0.6
0.73
0.14
0.66
0.56
0.69
F1
0.66
0.73
0.74
0.97
0.77
0.75
0.82
Table 7: QIF accuracy comparison results for PGA
and Flowcheck. PGA outperforms Flowcheck by 22%
on average in terms of F1 accuracy.
pute accuracy by aggregating ﬂows over all bytes so
that PGA is not unfairly advantaged. We outperform
Flowcheck in terms of F1 accuracy by 22% on average on
all of the evaluated programs as summarized in Table 7.
B Runtime
Evaluation
and Memory Overhead
Program Overhead. We evaluate the overhead intro-
duced by our implementation of PGA in runtime and
memory and compare it to dfsan for a single taint/-
gradient source. To measure overhead, we execute each
program while recording runtime and memory usage.
For runtime we perform 5,000 executions for each mea-
surement. We perform each measurement 5 times and
average the measured runtime and memory usage.
Tables 9 and 10 detail the runtime and memory over-
head per program in our evaluation. In the worst case
PGA has 21.7% greater overhead in runtime and 21.5%
in memory relative to DTA, but on average only adds
3.21 % relative overhead in runtime and 1.48% in mem-
ory. We also provide overhead measurements for libdft,
although it adds signiﬁcantly more overhead due to the
binary instrumentation.
USENIX Association
30th USENIX Security Symposium    1627
Program Summary
Over all Instructions
Program Instrs %Zeros
minigzip 3012
703
djpeg
401
mutool
xmllint
430
objdump 1070
3089
strip
size
659
28.2
38.7
40.4
39.5
39.0
41.0
19.3
Instruction Summary
Across all Programs
Instr. Total %Zeros
And
6756
URem 214
1214
Sub
875
Mul
LShr
2377
149
AShr
Add
895
30.2
29.0
21.0
15.9
14.4
6.0
5.7
Table 8: Analysis of operations from execution traces
where gradient drops to 0, aggregated for each pro-
gram and for each type of instruction across all pro-
grams. Outputs of these operations will have 0 gra-
dient but still be marked as tainted by DTA.
dfsan
libdft
grsan
Program Overhead Overhead Overhead
minigzip
61.5%
73.7%
djpeg
262.1%
mupdf
0.0%
xmllint
size
107.1%
131.2%
objdump
strip
11.4%
2,379.5%
-
853.5%
231.4%
152.5%
180.0%
142.5%
54.7%
70.5%
198.4%
5.5%
101.1%
133.2%
12.0%
grsan rel.
to dfsan
4.4%
1.9%
21.5%
-5.2%
3.0%
-0.9%
-2.2%
Table 9: Program runtime overhead measurements
averaged over ﬁve runs for a single taint/gradient
source. Libdft overhead is measured relative to run-
ning a program only with PIN. Dfsan and grsan are
measured relative to uninstrumented programs. Af-
ter 6 hours, libdft execution timed out on djpeg.
dfsan
grsan
Program Overhead Overhead
245.3%
minigzip
291.9%
djpeg
124.7%
mupdf
xmllint
258.5%
392.4%
size
323.5%
objdump
strip
342.1%
183.7%
276.4%
112.4%
346.6%
373.3%
345.6%
344.5%
grsan rel.
to dfsan
21.7%
4.1%
5.8%
-19.7%
4.0%
-5.0%
-0.5%
Table 10: Memory overhead for each program aver-
aged over ﬁve runs relative to uninstrumented pro-
grams for a single taint/gradient source. Grsan may
increase or decrease overhead because gradients re-
quire more memory to store, but may use less over-
all memory due to increased precision. On average,
grsan adds 1.48% additional overhead relative to
dfsan.
Edge Coverage after 24hrs
Program VUzzer
NEUZZ
minigzip
djpeg
mupdf
xmllint
size
objdump
strip
-
7
156
282
474
247
1337
87
645
376
957
1580
1813
3394
PGA +
NEUZZ
94
686
430
1079
2064
2014
3637
PGA+NEUZZ
rel. to NEUZZ
8.1%
6.4%
14.4%
12.8%
30.6%
11.1%
7.2%
Table 11: New edge coverage for each program over
24 hours by three diﬀerent fuzzers. VUzzer encoun-
ters an error in its taint tracking on minigzip and
crashes. Overall, PGA+NEUZZ improves NEUZZ edge cov-
erage on average by 12.9%. Note that our results are
slightly diﬀerent from the original NEUZZ and VUzzer
results due to diﬀerences in test environments, input
corpuses, and program versions.
C Evaluation on Current Fuzzers
We also evaluate if the gradient information from PGA
can improve the performance of state-of-the-art fuzzers
such as NEUZZ and VUzzer. We use NEUZZ as a basis
because it has higher edge coverage as seen in Table 11
and already incorporates gradients from a neural network
in its mutation strategy. We modify NEUZZ so that it uses
the PGA gradients to guide its mutation strategy. We
run grsan on its inputs and send the resulting gradients
to the NEUZZ backend. Note that NEUZZ is designed to
operate on gradients, so we did not modify it to also use
DTA. We provide a controlled comparison of PGA vs.
DTA for guided fuzzing in Evaluation 5.2.3.
We compare the additional edge coverage achieved by
the fuzzers over a 24hr run. Since we use some programs
with diﬀerent ﬁle formats from the original NEUZZ bench-
mark, we use a new seed corpus generated by running
AFL on each program for 1 hour. We perform this experi-
ment using cloud hosted virtual machines. Table 11 sum-
marizes the modiﬁed PGA+NEUZZ against baseline NEUZZ
and VUzzer. On average, PGA+NEUZZ improves new edge
coverage by 12.9% over baseline NEUZZ. We hypothesize
this improvement is because the gradients produced with
PGA are more precise than the neural-network based
gradients used by by NEUZZ. The very similar results in
edge coverage on minigzip are caused by the CRC check
in minigzip, which causes the program to exit early on
most new inputs. VUzzer crashes on minigzip due to
an error in its taint tracking and achieves a low edge
coverage for djpeg because of the high overhead of PIN’s
dynamic binary instrumentation for taint tracking. Note
that our results are slightly diﬀerent from the original
NEUZZ and VUzzer results due to diﬀerent initial seed
corpuses, program versions, and test environments.
1628    30th USENIX Security Symposium
USENIX Association