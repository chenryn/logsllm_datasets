# Understanding Slow BGP Routing Table Transfers

## Authors
- Zied Ben-Houidi
- Mickael Meulle
- Renata Teixeira

### Affiliations
- France Telecom R&D Orange
- UPMC Paris Universitas
- CNRS Labs

### Abstract
Researchers and network operators often observe that BGP table transfers are slow. Despite this common knowledge, the underlying reasons for these delays are not well understood. This paper investigates BGP table transfer delays by analyzing BGP messages collected from a large VPN provider backbone and conducting controlled experiments with routers from three different vendors and a software BGP speaker. Our results show that table transfers, both in the provider network and in the controlled experiments, contain significant gaps—periods during which no BGP routes are exchanged, even though both the sending and receiving routers are idle. These gaps can account for more than 90% of the total table transfer time. Analysis of a software router and discussions with router vendors indicate that these gaps are due to a timer-driven implementation of BGP update sending. This undocumented design choice prioritizes controlled router load over faster table transfers.

### Categories and Subject Descriptors
- C.2.3 [Computer-Communication Networks]: Network operations
- C.4 [Performance of Systems]: Measurement techniques

### General Terms
- Experimentation
- Measurement
- Performance

### Keywords
- BGP
- Route propagation
- Routing convergence

## 1. Introduction
BGP route propagation is a critical step in BGP routing convergence, especially when an event triggers changes for many destination prefixes simultaneously. Examples of such events include resets or failures of BGP sessions and intra-domain routing changes. Studies have shown that these events can cause a router to update a significant portion of its BGP table. After updating its own BGP table, a router propagates this information to its BGP neighbors, a process we refer to as a BGP table transfer. Previous work has demonstrated that BGP table transfers can take several minutes.

This paper is the first to investigate the reasons behind slow BGP table transfers. We combine BGP data collected from the backbone of a large VPN provider with controlled experiments using different router models. We focus on BGP routes from a VPN backbone because BGP tables are much larger in this environment, but our findings apply to any BGP network. Our key findings are:

1. **Table Transfers Are Slow Due to Gaps**: Section 2 examines BGP table transfers between pairs of routers in the VPN provider backbone. Our results show that table transfers can be up to 20 times slower than the optimal transfer time. A detailed analysis reveals that the slow rate is due to gaps—periods where no data is exchanged, even though both the sender and receiver are idle.

2. **Gaps Arise in All Tested Routers**: In a controlled environment, we emulate the VPN provider network and test carrier-class routers from three major vendors. As shown in Section 4.1, all BGP table transfers in our experiments contain gaps, regardless of the router model or configuration.

3. **Gaps Are Caused by Timer-Driven Implementation**: Gaps are not documented, so Section 4.2 investigates their causes using an open-source BGP speaker (SBGP). This analysis shows that SBGP uses a timer-driven implementation to send BGP messages, effectively acting as a rate-limiting mechanism. Discussions with two router vendors confirm that gaps are caused by a timer-driven implementation. One vendor stated that this rate-limiting is a conscious design choice to control router load, while another claimed it was unintentional and has since modified its implementation to be event-driven.

Whether intentional or not, this design choice significantly impacts BGP table transfer times and warrants careful consideration. Gaps represent a trade-off between fast BGP table transfers and more controlled router load. Section 5 explores mechanisms to reduce BGP table transfer times and their consequences, which depend on the number of routes in a router's BGP table, the number of BGP neighbors, and the router's capacities.

## 2. Table Transfers Are Slow
This section analyzes BGP table transfers between pairs of routers in a large VPN provider backbone. Our results differ from previous studies that analyzed BGP messages collected at a BGP monitor, as it is difficult to observe table transfers on BGP sessions more than one hop away from the monitor. We study a large VPN provider backbone with hundreds of border routers (PEs) and thousands of VPN customers. PEs use internal BGP (iBGP) to exchange routing information, and the provider uses a hierarchy of route reflectors (RRs) to manage the full mesh of iBGP sessions. RRs keep all routes from all VPNs (around 680K BGP routes).

Network operators trigger table transfers between different pairs of RR-PE routers by forcing a router to send a BGP route refresh message. Operators monitor the routers using specific commands to determine the transfer time. We also tap the messages exchanged between one pair of RR-PE routers during a table transfer.

Table 1 shows statistics on some of these table transfers. Each line contains information about a table transfer between a different pair of RR-PE routers. Given that BGP runs over TCP, we compute baseline values corresponding to the time a TCP connection should take to transfer the same amount of data (30MB of routes). The TCP receive window size (RWIN) is 16KB for all transfers, and we use a TCP throughput prediction formula with a loss rate of 0.01%. For all table transfers, RWIN and the round-trip delay (RTD) are the only factors determining TCP throughput. Table 1 shows that table transfer times can range from 60 seconds to 4 minutes 30 seconds, up to two orders of magnitude longer than baseline values.

To understand the reasons behind these slow times, we analyze the transfer between one RR-PE pair in detail. We find that the sender (RR) regularly stops sending routes to the receiver (PE) even though the receiver acknowledges all received messages. This creates gaps in the table transfer, lasting up to two seconds each, accounting for around 90% of the total transfer time. These gaps, caused by the sender, are clearly the reason for the slow transfers. Since we have no control over operational routers, we could not measure CPU usage during gaps. Therefore, we further study these gaps using a router testbed.

## 3. Testbed Description
We perform controlled experiments to understand the gaps observed in Section 2. Controlled experiments allow us to monitor routers' behavior under different conditions and study the prevalence of gaps among routers from different vendors.

Figure 1 depicts the testbed used in this paper. Each experiment tests two routers: a sender (RR) and a receiver (PE). We test carrier-class routers from three different vendors, primarily varying the sender. We test three routers from Vendor 1, two from Vendor 2, and one from Vendor 3. We also test two different PEs from Vendor 1. We establish a BGP session between PE and RR and emulate the provider backbone and customer routers using three Linux machines. The route server sends BGP routes to RR to emulate an entire VPN provider backbone. The two other machines, Customer 1 and Customer 2, emulate customer routers and connect to PE through a switch. Customer routers communicate with PE using external BGP. We need to emulate customer routers so that PE can install routes.

In all experiments, we follow classical BGP operational guidelines to optimize BGP convergence. We provoke the routing table transfer between RR and PE by resetting the BGP session. Each section provides more details about its experiments.

## 4. Gaps in Table Transfers
This section studies the prevalence and causes of gaps observed in Section 2.

### 4.1 Prevalence
We first study the prevalence of gaps, focusing on the sender since it is responsible for the gaps observed in Section 2. We perform experiments emulating table transfers, testing different senders (RRs). In all experiments, RR sends a full table of 680K routes to PE. To study the prevalence of gaps, we test seven different RRs from three different vendors and SBGP, a simple open-source BGP speaker from Merit’s Multi-Threading Routing Toolkit. In the experiments with SBGP, we run SBGP on the route server and connect the route server directly to PE. We ensure the receiver does not install the routes it receives, avoiding receiver overload. We tap the messages exchanged between RR and PE and study the evolution of BGP messages sent by RR as a function of time.

We find that gaps are prevalent in table transfers with all tested routers. Figure 2 plots the evolution of the total number of bytes sent by different RRs as a function of time. For clarity, we present a zoom on 10 seconds and show results for one vendor.

[Figure 2: Evolution of the total number of bytes sent by different RRs as a function of time]

---

This revised version aims to provide a clear, coherent, and professional presentation of the research, making it easier to read and understand.