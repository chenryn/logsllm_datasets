title:Understanding slow BGP routing table transfers
author:Zied Ben-Houidi and
Mickael Meulle and
Renata Teixeira
Understanding Slow BGP Routing Table Transfers
Zied Ben Houidi
Mickael Meulle
Renata Teixeira
France Telecom R&D Orange
France Telecom R&D Orange
UPMC Paris Universitas and
michael.meulle@orange-
PI:EMAIL
CNRS
Labs and
UPMC Paris Universitas
zied.benhouidi@orange-
ftgroup.com
Labs
ftgroup.com
ABSTRACT
Researchers and network operators often say that BGP table trans-
fers are slow. Despite this common knowledge, the reasons for slow
BGP transfers are not well understood. This paper explains BGP ta-
ble transfer delays by combining BGP messages collected at a large
VPN provider backbone and controlled experiments with routers of
three different vendors as well as a software BGP speaker. Our re-
sults show that table transfers both in the provider network and in
the controlled experiments contain gaps, i.e., periods in which both
the sending and receiving routers are idle, but no BGP routes are
exchanged. Gaps can represent more than 90% of the table transfer
time. Our analysis of a software router and discussions with router
vendors indicate that gaps happen because of the timer-driven im-
plementation of sending of BGP updates. Hence, gaps represent
an undocumented design choice that gives preference to more con-
trolled router load over faster table transfers.
Categories and Subject Descriptors
C.2.3 [Computer-Communication Networks]: Network opera-
tions; C.4 [Performance of Systems]: Measurement techniques
General Terms
Experimentation, Measurement, Performance
Keywords
BGP, route propagation, routing convergence
1.
INTRODUCTION
BGP route propagation is one step in BGP routing convergence.
This step is particularly signiﬁcant when an event triggers routing
changes for many destination preﬁxes at the same time. Exam-
ples of such events are resets or failures of BGP sessions and intra-
domain routing changes that trigger BGP changes [1]. Studies of
Internet provider’s networks have shown that these events can cause
a router to update a signiﬁcant fraction of its BGP table [1, 2, 3].
After updating its own BGP table, a router propagates this informa-
tion to its BGP neighbors. We call this step a BGP table transfer
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’09, November 4–6, 2009, Chicago, Illinois, USA.
Copyright 2009 ACM 978-1-60558-770-7/09/11 ...$10.00.
when the routing change affects a signiﬁcant portion of a router’s
BGP table. Previous work has shown that BGP table transfers can
take minutes [1, 3, 4].
This paper is the ﬁrst to investigate the reasons of slow BGP ta-
ble transfers. We combine BGP data collected at the backbone of a
large VPN provider with experiments with different router models
in a controlled environment. We concentrate on BGP routes from
a VPN backbone because BGP tables are much larger in this envi-
ronment [5]; however our ﬁndings apply to any BGP network. We
summarize these ﬁndings as follows.
1. Table transfers are slow because of gaps. Sec. 2 exam-
ines BGP table transfers between pairs of routers in the VPN
provider backbone. Our results show that table transfers can
be 20 times slower than the optimal (i.e., the time to transfer
the same amount of data given the link capacity). A detailed
analysis of BGP message exchanges reveals that the rate to
transfer the BGP table is slow because of gaps—periods in
which no data is exchanged even though both the sender and
the receiver are idle.
2. Gaps arise in all tested routers. We emulate the VPN provider
network in a controlled environment in which we test carrier-
class routers from three main vendors. As shown in Sec. 4.1,
all BGP table transfers in our experiments contain gaps irre-
spective of the router model or conﬁguration.
3. Gaps are caused by a timer-driven implementation that
controls the sending rate. Gaps are not documented; hence
Sec. 4.2 investigates the causes of gaps using an open source
BGP speaker (SBGP). This analysis shows that SBGP has
a timer-driven implementation to send BGP messages. Ef-
fectively, this implementation choice acts as a rate-limiting
mechanism for BGP table transfers. Our discussions with
two router vendors conﬁrm that gaps are caused by a timer-
driven implementation.
One router vendor declared that this rate-limiting is a conscious
design choice to control router load, whereas the other said that
gaps were unintentional. Because of our results, the latter ven-
dor already modiﬁed its implementation of the interaction between
BGP and TCP to be event-driven.
Intentional or not, this design choice has non-negligible impact
on BGP table transfer time and deserves more careful considera-
tion. Gaps represent a trade-off between fast BGP table transfers
and more control over router load. Sec. 5 studies mechanisms to
reduce BGP table transfer times and their consequences. The ef-
fectiveness of these mechanisms depends on the number of routes
in a router’s BGP table, the number of BGP neighbors of a router
and the router’s capacities.
350Transfer number Link type RTD Transfer time
1
2
3
4
5
6
7
500Mbps
POS155
100Mbps
GE
GE
GE
GE
5ms
10ms
5ms
1ms
1ms
5ms
1ms
210 sec
190 sec
270 sec
210 sec
90 sec
110 sec
60 sec
Baseline
8.91 sec
17.82 sec
8.91 sec
1.78 sec
1.78 sec
8.91 sec
1.78 sec
Table 1: Table transfers in a VPN provider backbone
2. TABLE TRANSFERS ARE SLOW
This section analyzes BGP table transfers between pairs of routers
in a large VPN provider backbone. Our results are distinct from
previous work, which analyzed BGP messages collected at a BGP
monitor. In these datasets, it is hard to observe table transfers on
BGP sessions that are more than one hop away from the monitor.
We study a large VPN provider backbone that has hundreds of
border routers (provider edge routers or PEs) and thousands of VPN
customers. PEs use internal BGP (iBGP) to exchange routing in-
formation between each other.
Instead of having a full mesh of
iBGP sessions between border routers, the provider uses a hierar-
chy of route reﬂectors (RRs) [6]. RRs keep all the routes from all
the VPNs (around 680K BGP routes).
Network operators trigger table transfers between different pairs
of RR-PE routers in the network by forcing a router to send a BGP
route refresh message [7]. Operators monitor the routers using
router-speciﬁc commands in order to determine the transfer time.
We also tap the messages exchanged between one pair of RR-PE
routers during one of the table transfers.
Table 1 shows statistics on some of these table transfers. Each
line in the table contains information about a table transfer between
a different pair of RR-PE routers. Given that BGP runs over TCP,
we compute the baseline values which correspond to the time a
TCP connection should take to transfer the same amount of data
(30MB of routes). RWIN, the TCP receive window size advertised
by the receiver, is 16KB for all transfers in Table 1. We use a TCP
throughput prediction formula [8] with a loss rate of 0.01%. For all
table transfers, RWIN and the round-trip delay (RTD) are the only
factors that determine TCP throughput (and therefore the baseline
values) because of high link capacities. Table 1 shows that the table
transfer time can take up to 4 minutes 30 seconds and can be one to
two orders of magnitude longer than baseline values.
To understand the reasons behind these slow times, we study the
transfer between one RR-PE pair in detail. We use the packet traces
and analyze the messages exchanged between the two routers. We
ﬁnd that the sender (RR) regularly stops sending routes to the re-
ceiver (PE) even though the receiver acknowledged all the mes-
sages it received. This creates gaps in the table transfer in which
no messages are sent. The gaps, which can last up to two sec-
onds each, account for around 90% of the total table transfer time.
Therefore, these gaps, which are caused by the sender, are clearly
the reason behind the slow table transfers. Unfortunately, we have
no control on the operational routers and hence we could not get
CPU measurements to understand whether routers are overloaded
or not during the gaps. Therefore, we study these gaps further using
a router testbed.
3. TESTBED DESCRIPTION
We perform controlled experiments to understand the gaps ob-
served in Sec. 2. Controlled experiments allow us to monitor routers’
behavior under different conditions during table transfers as well as
C u s t o m e r   1
C u s t o m e r   2
R R
R o u t e   s e r v e r
S
P E
Figure 1: Testbed for controlled experiments
to study the prevalence of gaps among routers from different router
vendors.
Fig. 1 depicts the testbed used in this paper. Each experiment
tests two routers: a sender, one route reﬂector (RR) and a receiver,
a PE. We test carrier-class routers from three different vendors. In
this work, we mainly vary the sender (RR), we test three routers
from Vendor 1, two routers from Vendor 2 and one router from
Vendor 3. We test two different PEs from Vendor 1. We establish
a BGP session between PE and RR and emulate the provider back-
bone and the customer routers using three Linux machines. The
route server sends BGP routes to RR to emulate an entire VPN
provider backbone. The VPN backbone has a set of route reﬂectors
that connects hundreds of PE routers. We ﬁx the number of routes
that the route server sends to RR depending on the experiment. We
use a set of 680K routes collected from one RR in the provider
backbone as a basis and sub-sample this set to vary the number
of routes when needed. The two other machines, Customer 1 and
Customer 2, emulate customer routers and connect to PE through
a switch. Customer 1 and Customer 2 can emulate each up to 250
customer routers. Customer routers communicate with PE using
regular external BGP. We need to emulate customer routers so that
PE is able to install routes.
In all our experiments, we follow classical BGP operational guide-
lines to obtain the best tuning of BGP convergence [9]. We are
interested in observing the routing table transfer between RR and
PE. We provoke it by resetting the BGP session between the two
routers. Each section in this paper gives more details about its ex-
periments.
4. GAPS IN TABLE TRANSFERS
This section studies the prevalence and causes of gaps observed
in Sec. 2
4.1 Prevalence
We ﬁrst study the prevalence of gaps. This section focuses on
the sender since it is responsible for the gaps observed in Sec. 2.
We perform experiments emulating table transfers in which we test
each time a different sender (RR). In all experiments, RR sends a
full table of 680K routes to PE. To study the prevalence of gaps,
we test seven different RRs from three different vendors as well
as SBGP, a simple open source BGP speaker and listener from
Merit’s Multi-Threading Routing Toolkit [10]. In the experiments
with SBGP, we run SBGP on the route server and connect the route
server directly to PE. In all experiments, we focus on the sender,
so we make sure the receiver does not install the routes it receives
in its forwarding tables thereby avoiding receiver overload. We tap
the messages exchanged between RR and PE and study the evolu-
tion of BGP messages sent by RR as a function of time during a
table transfer.
We ﬁnd that the gaps are prevalent on table transfers with all the
routers that we test. Fig. 2 plots the evolution of the total number
of bytes sent by different RRs as a function of time. For clarity,
we present a zoom on 10 seconds and only show results for one
351 
)
s
e
t
y
B
K
n
i
(
s
e
g
a
s
s
e
m
P
G
B
 2500
 2000
 1500
 1000
 500
 0
Vendor 1
Vendor 2