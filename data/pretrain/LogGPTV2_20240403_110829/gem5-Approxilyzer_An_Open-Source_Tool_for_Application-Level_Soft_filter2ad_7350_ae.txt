speciﬁc patterns in these applications more accurately and we
leave their exploration to future work. Across all applications,
we observe that pilots with Masked, SDC-Good, and Detected
outcomes show almost perfect (>99.9%) validation accuracy.
Both Blackscholes and FFT show an improvement (>90%)
when a user quality threshold is applied. For brevity we show
results for QT=5, but we performed this experiment with a
range of different QT values and observed a similarly high
validation accuracy. This implies that even for pilots that fail
to predict the quality at a ﬁne granularity, the grouping of the
equivalence classes is sufﬁciently accurate to be used in many
realistic use cases. On average, we see that when a quality
threshold is supplied, the validation accuracy is > 97% for
both store and control heuristics (and hence their combination).
the techniques used by gem5-
Approxilyzer are very accurate in characterizing the error
proﬁles for x86 applications.
Hence, we show that
C. Error Proﬁles for Different ISAs
Figure 4 compares the distribution of error outcomes (for
all the error sites) in each application for the x86 and SPARC
ISAs. The error outcome proﬁles of the same application
look rather different for the different ISAs. We note, however,
that some differences are expected due to the CISC vs. RISC
nature of the instructions as well as the fact that x86 uses
many more implicit registers (that we do not inject into)
compared to SPARC. The graph shows that SPARC has a
higher percentage of more egregious outcomes. For example,
while Blackscholes-x86 has many error sites that lead to
220
(cid:27)
(cid:4)
(cid:7)
(cid:1)
(cid:14)
(cid:19)
(cid:28)
(cid:12)
(cid:28)
(cid:9)
(cid:1)
(cid:17)
(cid:24)
(cid:46)
(cid:1)
(cid:43)(cid:34)
(cid:42)(cid:34)
(cid:41)(cid:34)
(cid:40)(cid:34)
(cid:39)(cid:34)
(cid:38)(cid:34)
(cid:37)(cid:34)
(cid:36)(cid:34)
(cid:35)(cid:34)
(cid:34)
(cid:3)(cid:21)(cid:12)(cid:14)(cid:20)(cid:27)(cid:14)(cid:18)(cid:24)(cid:21)(cid:16)(cid:27)
(cid:5)(cid:5)(cid:10)
(cid:6)(cid:11)
(cid:9)(cid:29)(cid:12)(cid:25)(cid:28)(cid:19)(cid:24)(cid:23)(cid:27)
(cid:9)(cid:24)(cid:13)(cid:16)(cid:21)
(cid:3)(cid:21)(cid:12)(cid:14)(cid:20)(cid:27)(cid:14)(cid:18)(cid:24)(cid:21)(cid:16)(cid:27)
(cid:5)(cid:5)(cid:10)
(cid:6)(cid:11)
(cid:9)(cid:29)(cid:12)(cid:25)(cid:28)(cid:19)(cid:24)(cid:23)(cid:27)
(cid:9)(cid:24)(cid:13)(cid:16)(cid:21)
(cid:7)(cid:4)(cid:27)(cid:1)(cid:28)(cid:18)(cid:12)(cid:28)(cid:1)(cid:23)(cid:16)(cid:16)(cid:15)(cid:1)(cid:26)(cid:16)(cid:27)(cid:19)(cid:21)(cid:19)(cid:16)(cid:23)(cid:14)(cid:31)(cid:1)(cid:25)(cid:26)(cid:24)(cid:28)(cid:16)(cid:14)(cid:28)(cid:19)(cid:24)(cid:23)
(cid:7)(cid:4)(cid:27)(cid:1)(cid:28)(cid:18)(cid:12)(cid:28)(cid:1)(cid:22)(cid:12)(cid:31)(cid:1)(cid:13)(cid:16)(cid:1)(cid:12)(cid:25)(cid:25)(cid:26)(cid:24)(cid:30)(cid:19)(cid:22)(cid:12)(cid:13)(cid:21)(cid:16)
Fig. 5. Percentage of static PCs in the application that need resiliency
protection and percentage of PCs that are approximable across x86 and
SPARC. We use the same QT across both ISAs: 5% for Blackscholes, Sobel,
FFT, and LU; $0.001 for Blackscholes and Swaptions.
SDC-Good, SDC-Maybe, and SDC-Bad outcomes, the error
outcomes in Blackscholes-SPARC produce such bad quality
output that they become DDCs. We leave a deeper analysis
of the causes for these differences to future work.
Figure 5 further shows the percentage of static instructions
that need resiliency protection and those that are approximable
for the same QT across the two ISAs. The wide differences
across the two ISAs and the lack of a clear trend further
underscore the importance of
tools
that can analyze applications at the binary level to devise
customized resiliency and approximation solutions for different
architectures. Source-code or IR-level error analysis may not
lead to the most optimized solutions.
resiliency analysis
VI. RELATED WORK
the concepts
re-implements
gem5-Approxilyzer
from
Approxilyzer [25] and Relyzer [26] in gem5 [28] and the
x86 ISA. Other tools for resiliency analysis have used gem5
as their base simulator. For instance, MeRLiN [23] uses
GeFIN [41] (which is also a built on top of gem5) to simulate
micro-architectural injections in an x86 O3 CPU. It performs
fault pruning to accelerate statistical micro-architectural fault
injections and can provide ﬁne-grained reliability estimates for
hardware structures as well as SDC vulnerability estimates for
software. gem5-Approxilyzer’s analysis is at the architectural
level, and its primary goal is not a statistical average or
probability but to determine precisely if/how an error in an
instruction impacts the ﬁnal output.
GemFI [42] is another error-injection tool that operates
at the micro-architectural level, is built on top of gem5 and
supports both Alpha and x86 ISA. Other error-injection
tools, such as LLFI [19], analyze applications at the compiler
intermediate representation (IR) level. IR is ISA-independent
by design, so such an analysis would ideally hold regardless
of the hardware architecture. However, there may be a loss
in error site accuracy because IR still requires additional
transformations before producing the assembly [43].
FAIL* [20] performs ISA-level analysis. A beneﬁt of
performing ISA-level injections is that the results provide
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:56:27 UTC from IEEE Xplore.  Restrictions apply. 
[17] V. Sridharan and D. R. Kaeli, “Eliminating microarchitectural dependency
from architectural vulnerability,” in HPCA, 2009.
[18] B. Fang, Q. Lu, K. Pattabiraman, M. Ripeanu, and S. Gurumurthi,
“ePVF: An Enhanced Program Vulnerability Factor Methodology for
Cross-Layer Resilience Analysis,” in DSN, 2016.
[19] J. Wei, A. Thomas, G. Li, and K. Pattabiraman, “Quantifying the
Accuracy of High-Level Fault Injection Techniques for Hardware Faults,”
in DSN, 2014.
[20] H. Schirmeier, M. Hoffmann, C. Dietrich, M. Lenz, D. Lohmann, and
O. Spinczyk, “FAIL*: An Open and Versatile Fault-Injection Framework
for the Assessment of Software-Implemented Hardware Fault Tolerance,”
in EDCC, 2015.
[21] J. Calhoun, L. Olson, and M. Snir, “FlipIt: An LLVM based fault
injector for HPC,” in Euro-Par, 2014.
[22] J. Li and Q. Tan, “SmartInjector: Exploiting intelligent fault injection
for SDC rate analysis,” in DFTS, 2013.
[23] M. Kaliorakis, D. Gizopoulos, R. Canal, and A. Gonzalez, “MeRLiN:
for Fast and Accurate
Exploiting Dynamic Instruction Behavior
Microarchitecture Level Reliability Assessment,” in ISCA, 2017.
[24] M. Samadi, J. Lee, D. A. Jamshidi, A. Hormati, and S. Mahlke, “SAGE:
Self-tuning Approximation for Graphics Engines,” in MICRO, 2013.
[25] R. Venkatagiri, A. Mahmoud, S. K. S. Hari, and S. V. Adve, “Approxi-
lyzer: Towards a systematic framework for instruction-level approximate
computing and its application to hardware resiliency,” in MICRO, 2016.
[26] S. K. S. Hari, S. V. Adve, H. Naeimi, and P. Ramachandran, “Relyzer:
Exploiting Application-Level Fault Equivalence to Analyze Application
Resiliency to Transient Faults,” in ASPLOS, 2012.
Full
System Simulator.” Website,
2006.
[27] Virtutech,
“Simics
http://www.simics.net.
[28] N. Binkert, B. Beckmann, G. Black, S. K. Reinhardt, A. Saidi, A. Basu,
J. Hestness, D. R. Hower, T. Krishna, S. Sardashti, R. Sen, K. Sewell,
M. Shoaib, N. Vaish, M. D. Hill, and D. A. Wood, “The Gem5
Simulator,” SIGARCH Comput. Archit. News, 2011.
[29] M. Dimitrov and H. Zhou, “Uniﬁed Architectural Support for Soft-Error
Protection or Software Bug Detection,” in PACT, 2007.
[30] O. Goloubeva, M. Rebaudengo, M. S. Reorda, and M. Violante,
“Soft-Error Detection Using Control Flow Assertions,” in DFT, 2003.
[31] S. K. S. Hari, M.-L. Li, P. Ramachandran, B. Choi, and S. V. Adve,
“mSWAT: Low-cost Hardware Fault Detection and Diagnosis for
Multicore Systems,” in MICRO, 2009.
[32] M.-L. Li, P. Ramachandran, S. K. Sahoo, S. V. Adve, V. S. Adve, and
Y. Zhou, “Understanding the Propagation of Hard Errors to Software
and Implications for Resilient Systems Design,” in ASPLOS, 2008.
[33] K. Pattabiraman, G. P. Saggese, D. Chen, Z. Kalbarczyk, and R. K. Iyer,
“Dynamic Derivation of Application-Speciﬁc Error Detectors and their
Implementation in Hardware,” in EDCC, 2006.
[34] N. Wang and S. Patel, “ReStore: Symptom-Based Soft Error Detection
in Microprocessors,” IEEE TDSC, 2006.
[35] S. K. S. Hari, S. V. Adve, and H. Naeimi, “Low-cost Program-level
Detectors for Reducing Silent Data Corruptions,” in DSN, 2012.
[36] A. Mahmoud, R. Venkatagiri, K. Ahmed, S. Misailovic, D. Marinov,
C. W. Fletcher, and S. V. Adve, “Minotaur: Adapting Software Testing
Techniques for Hardware Errors,” in ASPLOS, 2019.
[37] C. Bienia, Benchmarking Modern Multiprocessors. PhD thesis, Princeton
instruction-level resiliency information. System designers can
then utilize these results to create soft error protection schemes
at the instruction level [44], [45]. FAIL* also uses gem5
and supports ARM but is limited to one pruning technique:
def-use analysis.
VII. CONCLUSION AND FUTURE WORK
We have presented gem5-Approxilyzer, an open-source
re-implementation of Approxilyzer for the gem5 simulation
environment. The goal of gem5-Approxilyzer is to enable
support for multiple ISAs in the future within an open-source
infrastructure. We start by supporting x86 in this work. We
show that gem5-Approxilyzer is both effective and highly
accurate in predicting the program’s ﬁnal output quality in
the presence of soft errors in the execution. To additionally
motivate the need for such tools, we perform a preliminary
comparison of our workloads across two ISAs, x86 and
SPARC. The differences in the error proﬁles for the same
applications across ISAs further underscore the need for a
tool like gem5-Approxilyzer. Expanding gem5-Approxilyzer
to support different error models is part of our future work.
REFERENCES
[1] S. Sidiroglou-Douskos, S. Misailovic, H. Hoffmann, and M. C. Rinard,
“Managing performance vs. accuracy trade-offs with loop perforation,”
in SIGSOFT FSE, 2011.
[2] S. Misailovic, M. Carbin, S. Achour, Z. Qi, and M. C. Rinard,
“Chisel: Reliability- and Accuracy-aware Optimization of Approximate
Computational Kernels,” in OOPSLA, 2014.
[3] A. Sampson, A. Baixo, B. Ransford, T. Moreau, J. Yip, L. Ceze, and
M. Oskin, “Accept: A programmer-guided compiler framework for
practical approximate computing,” in Technical Report UW-CSE-15-01-
01, University of Washington, 2015.
[4] A. Sampson, W. Dietl, E. Fortuna, D. Gnanapragasam, L. Ceze, and
D. Grossman, “EnerJ: Approximate Data Types for Safe and General
Low-power Computation,” in PLDI, 2011.
[5] S. Borkar, “Designing Reliable Systems from Unreliable Components:
The Challenges of Transistor Variability and Degradation,” IEEE Micro,
2005.
[6] S. Sahoo, M.-L. Li, P. Ramchandran, S. V. Adve, V. Adve, and Y. Zhou,
“Using Likely Program Invariants to Detect Hardware Errors,” in DSN,
2008.
[7] G. Lyle, S. Cheny, K. Pattabiraman, Z. Kalbarczyk, and R. Iyer, “An
End-to-end Approach for the Automatic Derivation of Application-Aware
Error Detectors,” in DSN, 2009.
[8] A. Meixner, M. E. Bauer, and D. J. Sorin, “Argus: Low-Cost,
Comprehensive Error Detection in Simple Cores,” in MICRO, 2007.
[9] P. Racunas, K. Constantinides, S. Manne, and S. S. Mukherjee,
“Perturbation-based Fault Screening,” in HPCA, 2007.
[10] K. Pattabiraman, N. Nakka, Z. Kalbarczyk, and R. Iyer, “SymPLFIED:
Symbolic program-level fault injection and error detection framework,”
in DSN, 2008.
[11] S. Feng, S. Gupta, A. Ansari, and S. Mahlke, “Shoestring: Probabilistic
Soft Error Reliability on the Cheap,” in ASPLOS, 2010.
[12] E. Cheng, S. Mirkhani, L. G. Szafaryn, C.-Y. Cher, H. Cho, K. Skadron,
M. R. Stan, K. Lilja, J. A. Abraham, P. Bose, and S. Mitra, “CLEAR:
Cross-Layer Exploration for Architecting Resilience - Combining
Hardware and Software Techniques to Tolerate Soft Errors in Processor
Cores,” in DAC, 2016.
[13] G. Li, S. K. S. Hari, M. Sullivan, T. Tsai, K. Pattabiraman, J. Emer,
and S. W. Keckler, “Understanding Error Propagation in Deep-Learning
Neural Networks (DNN) Accelerators and Applications,” in SC, 2017.
[14] X. Li, S. V. Adve, P. Bose, and J. A. Rivers, “Online Estimation of
Architectural Vulnerability Factor for Soft Errors,” in ISCA, 2008.
[15] S. S. Mukherjee, C. T. Weaver, J. Emer, S. K. Reinhardt, and T. Austin,
“Measuring Architectural Vulnerability Factors,” IEEE Micro, 2003.
[16] S. S. Mukherjee, C. Weaver, J. Emer, S. K. Reinhardt, and T. Austin,
“A Systematic Methodology to Compute the Architectural Vulnerability
Factors for a High-Performance Microprocessor,” in MICRO, 2003.
University, January 2011.
[38] S. C. Woo, M. Ohara, E. Torrie, J. P. Singh, and A. Gupta, “The SPLASH-
2 Programs: Characterization and Methodological Considerations,” in
ISCA, 1995.
[39] S. K. Sastry Hari, R. Venkatagiri, S. V. Adve, and H. Naeimi, “GangES:
Gang Error Simulation for Hardware Resiliency Evaluation,” in ISCA,
2014.
[40] R. Leveugle, A. Calvez, P. Maistri, and P. Vanhauwaert, “Statistical
fault injection: Quantiﬁed error and conﬁdence,” in DATE, 2009.
[41] A. Chatzidimitriou and D. Gizopoulos, “Anatomy of Microarchitecture-
level Reliability Assessment: Throughput and Accuracy,” in ISPASS, 2016.
[42] K. Parasyris, G. Tziantzoulis, C. D. Antonopoulos, and N. Bellas,
“GemFI: A Fault Injection Tool for Studying the Behavior of Applications
on Unreliable Substrates,” in DSN, 2014.
[43] N. Hasabnis and R. Sekar, “Lifting Assembly to Intermediate Repre-
sentation: A Novel Approach Leveraging Compilers,” in ASPLOS, 2016.
[44] J. S. Hu, F. Li, V. Degalahal, M. Kandemir, N. Vijaykrishnan, and
M. J. Irwin, “Compiler-directed instruction duplication for soft error
detection,” in DATE, 2005.
[45] N. Oh, P. P. Shirvani, and E. J. McCluskey, “Error Detection by
Duplicated Instructions in Super-scalar Processors,” IEEE Transactions
on Reliability, 2002.
221
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:56:27 UTC from IEEE Xplore.  Restrictions apply.