Network and Distributed System Security (NDSS 2006) (February 2006)
236
C.V. Wright et al.
23. Small, S., Mason, J., Monrose, F., Provos, N., Stubbleﬁeld, A.: To catch a predator:
A natural language approach for eliciting malicious payloads. In: Proceedings of
the 17th USENIX Security Symposium (August 2008)
24. Wang, K.: Using HoneyClients to Detect New Attacks. In: RECON Conference
(June 2005)
25. Wang, Y.M., Beck, D., Jiang, X., Roussev, R., Verbowski, C., Chen, S., King,
S.: Automated Web Patrol with Strider HoneyMonkeys: Finding Web Sites That
Exploit Browser Vulnerabilities. In: Proceedings of the 13th Annual Symposium
on Network and Distributed System Security (NDSS 2006) (February 2006)
26. Sanders, M.: autopy: A simple, cross-platform GUI automation toolkit for Python,
http://github.com/msanders/autopy
27. Yeh, T., Chang, T.H., Miller, R.C.: Sikuli: Using GUI Screenshots for Search and
Automation. In: Proceedings of the 22nd Symposium on User Interface Software
and Technology (October 2009)
28. Kleek, M.V., Bernstein, M., Karger, D., Schraefel, M.C.: Getting to Know You
Gradually: Personal Lifetime User Modeling (PLUM). Technical report, MIT
CSAIL (April 2007)
29. Simpson, C.R., Reddy, D., Riley, G.F.: Empirical Models of TCP and UDP En-
dUser Network Trafc from NETI@home Data Analysis. In: 20th International
Workshop on Principles of Advanced and Distributed Simulation (May 2006)
30. Kurz, C., Hlavacs, H., Kotsis, G.: Workload Generation by Modelling User Behavior
in an ISP Subnet. In: Proceedings of the International Symposium on Telecommu-
nications (August 2001)
31. tcpreplay by Aaron Turner, http://tcpreplay.synfin.net/
32. Hong, S.S., Wu, S.F.: On Interactive Internet Traﬃc Replay. In: Proceedings of the
9th International Symposium on Recent Advances in Intrusion Detection (Septem-
ber 2006)
33. Sommers, J., Barford, P.: Self-conﬁguring network traﬃc generation. In: Proceed-
ings of the 4th ACM SIGCOMM Conference on Internet Measurement, pp. 68–81
(2004)
34. Cao, J., Cleveland, W.S., Gao, Y., Jeﬀay, K., Smith, F.D., Weigle, M.C.: Stochastic
models for generating synthetic HTTP source traﬃc. In: INFOCOM (2004)
35. Weigle, M.C., Adurthi, P., Hern´andez-Campos, F., Jeﬀay, K., Smith, F.D.: Tmix: a
tool for generating realistic TCP application workloads in ns-2. ACM SIGCOMM
Computer Communication Review 36(3), 65–76 (2006)
36. Lan, K.C., Heidemann, J.: Rapid model parameterization from traﬃc
measurements. ACM Transactions on Modeling and Computer Simulation
(TOMACS) 12(3), 201–229 (2002)
37. Vishwanath, K.V., Vahdat, A.: Realistic and Responsive Network Traﬃc Genera-
tion. In: Proceedings of ACM SIGCOMM (September 2006)
38. Sommers, J., Yegneswaran, V., Barford, P.: Toward Comprehensive Trafc Genera-
tion for Online IDS Evaluation. Technical report, University of Wisconsin (2005)
39. Mutz, D., Vigna, G., Kemmerer, R.: An Experience Developing an IDS Stimulator
for the Black-Box Testing of Network Intrusion Detection Systems. In: Proceedings
of the Annual Computer Security Applications Conference (December 2003)
40. Kayacik, H.G., Zincir-Heywood, N.: Generating Representative Traﬃc for Intrusion
Detection System Benchmarking. In: Proceedings of the 3rd Annual Communica-
tion Networks and Services Research Conference, pp. 112–117 (May 2005)
41. Sommers, J., Yegneswaran, V., Barford, P.: A framework for malicious workload
generation. In: Proceedings of the 4th ACM SIGCOMM Conference on Internet
Measurement, pp. 82–87 (2004)
Generating Client Workloads and High-Fidelity Network Traﬃc
237
42. Hunt, G., Brubacher, D.: Detours: Binary Interception of Win32 Functions. In:
Third USENIX Windows NT Symposium (July 1999)
43. Klimt, B., Yang, Y.: Introducing the Enron Corpus. In: Proceedings of the First
Conference on Email and Anti-Spam (CEAS) (July 2004)
44. Paxson, V., Floyd, S.: Wide Area Traﬃc: The Failure of Poisson Modeling.
IEEE/ACM Transactions on Networking 3(3) (June 1995)
45. Matsumoto, M., Nishimura, T.: Mersenne Twister: a 623-dimensionally equidis-
tributed uniform pseudo-random number generator. ACM Transactions on Mod-
elling and Computer Simulation 8(1), 3–30 (1998)
46. GINA: MSDN Windows Developer Center,
http://msdn.microsoft.com/en-us/library/aa375457VS.85.aspx
47. Hibler, M., Ricci, R., Stoller, L., Duerig, J., Guruprasad, S., Stack, T., Webb,
K., Lepreau, J.: Large-scale Virtualization in the Emulab Network Testbed. In:
Proceedings of the 2008 USENIX Annual Technical Conference (June 2008)
48. Google, Inc.: Google search appliance,
http://www.google.com/enterprise/search/gsa.html
49. osCommerce: Open Source E-Commerce Solutions, http://www.oscommerce.com/
50. DMOZ Open Directory Project, http://www.dmoz.org/
51. Yahoo! Directory, http://dir.yahoo.com/
52. Alexa Top Sites, http://www.alexa.com/topsites
53. AV-Comparatives e.V.: Anti-Virus Comparative Performance Test: Impact of Anti-
Virus Software on System Performance (December 2009),
http://www.av-comparatives.org/comparativesreviews/performance-tests
54. Warner, O.: What Really Slows Windows Down (September 2006),
http://www.thepcspy.com/read/what_really_slows_windows_down
55. Chatterton, D., Gigante, M., Goodwin, M., Kavadias, T., Keronen, S., Knispel, J.,
McDonell, K., Matveev, M., Milewska, A., Moore, D., Muehlebach, H., Rayner, I.,
Scott, N., Shimmin, T., Schultz, T., Tuthill, B.: Performance Co-Pilot for IRIX
Advanced User’s and Administrator’s Guide. 2.3 edn. SGI Technical Publications
(2002), http://oss.sgi.com/projects/pcp/index.html
56. Timekeeping in VMware Virtual Machines,
http://www.vmware.com/pdf/vmware_timekeeping.pdf
On Challenges in Evaluating Malware Clustering
Peng Li1, Limin Liu2, Debin Gao3, and Michael K. Reiter1
1 Department of Computer Science, University of North Carolina, Chapel Hill, NC, USA
2 State Key Lab of Information Security, Graduate School of Chinese Academy of Sciences
3 School of Information Systems, Singapore Management University, Singapore
Abstract. Malware clustering and classiﬁcation are important tools that enable
analysts to prioritize their malware analysis efforts. The recent emergence of fully
automated methods for malware clustering and classiﬁcation that report high ac-
curacy suggests that this problem may largely be solved. In this paper, we report
the results of our attempt to conﬁrm our conjecture that the method of selecting
ground-truth data in prior evaluations biases their results toward high accuracy.
To examine this conjecture, we apply clustering algorithms from a different do-
main (plagiarism detection), ﬁrst to the dataset used in a prior work’s evaluation
and then to a wholly new malware dataset, to see if clustering algorithms de-
veloped without attention to subtleties of malware obfuscation are nevertheless
successful. While these studies provide conﬂicting signals as to the correctness
of our conjecture, our investigation of possible reasons uncovers, we believe, a
cautionary note regarding the signiﬁcance of highly accurate clustering results,
as can be impacted by testing on a dataset with a biased cluster-size distribution.
Keywords: malware clustering and classiﬁcation, plagiarism detection.
1 Introduction
The dramatic growth of the number of malware variants has motivated methods to clas-
sify and group them, enabling analysts to focus on the truly new ones. The need for such
classiﬁcation and pruning of the space of all malware variants is underlined by, e.g., the
Bagle/Beagle malware, for which roughly 30,000 distinct variants were observed be-
tween January 9 and March 6, 2007 [8]. While initial attempts at malware classiﬁcation
were performed manually, in recent years numerous automated methods have been de-
veloped to perform malware classiﬁcation (e.g., [11,6,5,16,9,13,15]). Some of these
malware classiﬁers have claimed very good accuracy in classifying malware, leading
perhaps to the conclusion that malware classiﬁcation is more-or-less solved.
In this paper, we show that this may not be the case, and that evaluating automated
malware classiﬁers poses substantial challenges that we believe require renewed at-
tention from the research community. A central challenge is that with the dearth of a
well-deﬁned notion of when two malware instances are the “same” or “different”, it is
difﬁcult to obtain ground truth to which to compare the results of a proposed classiﬁer.
Indeed, even manually encoded rules to classify malware seems not to be enough —
a previous study [6] found that a majority of six commercial anti-virus scanners con-
curred on the classiﬁcation of 14,212 malware instances in only 2,658 cases. However,
in the absence of better alternatives for determining ground truth, such instances and
S. Jha, R. Sommer, and C. Kreibich (Eds.): RAID 2010, LNCS 6307, pp. 238–255, 2010.
c(cid:2) Springer-Verlag Berlin Heidelberg 2010
On Challenges in Evaluating Malware Clustering
239
their corresponding classiﬁcations are increasingly used to evaluate automated meth-
ods of malware clustering. For example, a state-of-the-art malware clustering algorithm
due to Bayer et al. [6] achieved excellent results using these 2,658 malware instances
as ground truth; i.e., the tool obtained results that largely agreed with the clustering of
these 2,658 malware instances by the six anti-virus tools.
The starting point of the present paper is the possibility, we conjectured, that one
factor contributing to these strong results might be that these 2,658 instances are simply
easy to classify, by any of a variety of techniques. We report on our efforts to exam-
ine this possibility, ﬁrst by repeating the clustering of these instances using algorithms
from an ostensibly different domain, namely plagiarism detectors that employ dynamic
analysis. Intuitively, since plagiarism detectors are developed without attention to the
speciﬁcs of malware obfuscation, highly accurate clustering results by these tools might
suggest that this method of selecting ground-truth data biases the data toward easy-to-
classify instances. We describe the results of this analysis, which indicate that plagia-
rism detectors have nearly the same success in clustering these malware instances, thus
providing tentative support for this conjecture.
To more thoroughly examine this possibility, we then attempted to repeat the evalua-
tion methodology of Bayer et al. on a new set of malware instances. By drawing from a
database of malware instances, we assembled a set for which four anti-virus tools con-
sistently labeled each member. We detail this study and report on its results that, much
to our surprise, ﬁnd that neither the Bayer et al. technique nor the plagiarism detectors
we employed were particularly accurate in clustering these instances. Due to certain
caveats of this evaluation that we will discuss, this evaluation is materially different
from that for the previous dataset, causing us to be somewhat tentative in the conclu-
sions we draw from it. Nevertheless, these results temper the conﬁdence with which
we caution that the selection of ground-truth data based on the concurrence of multiple
anti-virus tools biases the data toward easy-to-classify instances.
But this leaves the intriguing question: Why the different results on the two datasets?
We complete our paper with an analysis of a factor that, we believe, contributes to
(though does not entirely explain) this discrepancy, and that we believe offers a cau-
tionary note for the evaluation of malware clustering results. This factor is the makeup
of the ground-truth dataset, in terms of the distribution of the sizes of the malware
families it contains. We observe that the original dataset, on which the algorithms we
consider perform well, is dominated by two large families, but the second dataset is
more evenly distributed among many families. We show that this factor alone biases
the measures used in comparing the malware clustering output to the dataset families,
speciﬁcally precision and recall, in that it increases the likelihood of good precision
and recall numbers occurring by chance. As such, the biased cluster-size distribution in
the original dataset erodes the signiﬁcance (c.f., [22, Section 8.5.8]) of the high preci-
sion and recall reported by Bayer et al. [6]. This observation, we believe, identiﬁes an
important factor for which to control when measuring the effectiveness of a malware
clustering technique.
While we focus on a single malware classiﬁer for our analysis [6], we do so because
very good accuracy has been reported for this algorithm and because the authors of that
technique were very helpful in enabling us to compare with their technique. We hasten
240
P. Li et al.
to emphasize, moreover, that our comparisons to plagiarism detectors are not intended
to suggest that plagiarism detectors are the equal of this technique. For one, we believe
the technique of Bayer et al. is far more scalable than any of the plagiarism detectors
that we consider here, an important consideration when clustering potentially tens of
thousands of malware instances. In addition, the similar accuracy of the technique of
Bayer et al. to the plagiarism detectors does not rule out the possibility that the plagia-
rism detectors are more easily evaded (in the sense of diminishing clustering accuracy);
rather, it simply indicates that malware today does not seem to do so. We stress that the
issues we identify are not a criticism of the Bayer et al. technique, but rather are issues
worth considering for any evaluation of malware clustering and classiﬁcation.
To summarize, the contributions of this paper are as follows. First, we explore the
possibility that existing approaches to obtaining ground-truth data for malware cluster-
ing evaluation biases results by isolating those instances that are simple to cluster or
classify. In the end, we believe our study is inconclusive on this topic, but that reporting
our experiences will nevertheless raise awareness of this possibility and will underline
the importance of ﬁnding methods to validate the ground-truth data employed in this
domain. Second, we highlight the importance of the signiﬁcance of positive cluster-
ing results when reporting them. This has implications for the datasets used to evalu-
ate malware clustering algorithms, in that it requires that datasets exhibiting a biased
cluster-size distribution not be used as the sole vehicle for evaluating a technique.
2 Classiﬁcation and Clustering of Malware
To hinder static analysis of binaries, the majority of current malware makes use of ob-
fuscation techniques, notably binary packers. As such, dynamic analysis of such mal-
ware is often far more effective than static analysis. Monitoring the behavior of the
binary during its execution enables collecting a proﬁle of the operations that the binary
performs and offers potentially greater insight into the code itself if obfuscation is re-
moved (e.g., the binary is unpacked) in the course of running it. While this technique
has its limitations — e.g., it may be difﬁcult to induce certain behaviors of the malware,
some of which may require certain environmental conditions to occur [10,14,19,20] —
it nevertheless is more effective than purely static approaches. For this reason, dynamic
analysis of malware has received much attention in the research community. Analysis
systems such as CWSandbox [25], Anubis [7], BitBlaze [18], Norman [2] and Threat-
Expert [1] execute malware samples within an instrumented environment and monitor
their behaviors for analysis and development of defense mechanisms.
A common application for dynamic analysis of malware is to group malware in-
stances, so as to more easily identify the emergence of new strains of malware, for
example. Such grouping is often performed using machine learning, either by cluster-
ing (e.g., [6,17,15]) or by classiﬁcation (e.g., [13,5,16,11]), which are unsupervised and
supervised techniques, respectively.
Of primary interest in this paper are the methodologies that these works employ to
evaluate the results of learning, and speciﬁcally the measures of quality for the clus-
tering or classiﬁcation results. Let M denote a collection of m malware instances to
be clustered, or the “test data” in the case of classiﬁcation. Let C = {Ci}1≤i≤c and
On Challenges in Evaluating Malware Clustering
241
D = {Di}1≤i≤d be two partitions of M , and let f : {1 . . . c} → {1 . . . d} and
g : {1 . . . d} → {1 . . . c} be functions. Many prior techniques evaluated their results
using two measures:
prec(C,D) =
recall(C,D) =
1
m
1
m
c(cid:7)
i=1
d(cid:7)
|Ci ∩ Df (i)|
|Cg(i) ∩ Di|
i=1
where C is the set of clusters resulting from the technique being evaluated and D is the
clustering that represents the “right answer”.
More speciﬁcally, in the case of classiﬁcation, Ci is all test instances classiﬁed as
class i, and Di is all test instances that are “actually” of class i. As such, in the case of
classiﬁcation, c = d and f and g are the identity functions. As a result prec(C,D) =
recall(C,D), and this measure is often simply referred to as accuracy. This is the mea-
sure used by Rieck et al. [16] to evaluate their malware classiﬁer, and Lee et al. [13]
similarly uses error rate, or one minus the accuracy.
In the clustering case, there is no explicit label to deﬁne the cluster in D that corre-
sponds to a speciﬁc cluster in C, and so one approach is to deﬁne
f(i) = arg max
i(cid:2)
g(i) = arg max
i(cid:2)
|Ci ∩ Di(cid:2)|
|Ci(cid:2) ∩ Di|
In this case, f and g will not generally be the identity function (or even bijections),
and so precision and recall are different. This approach is used by Rieck et al. [17]
and Bayer et al. [6] in evaluating their clustering techniques. In this case, when it is
desirable to reduce these two measures into one, a common approach (e.g., [17]) is to
use the F-measure:
F-measure(C,D) =
2 · prec(C,D) · recall(C,D)
prec(C,D) + recall(C,D)
This background is sufﬁcient to highlight the issues on which we focus in the paper:
Production of D: A central question in the measurement of precision and recall is how
the reference clustering D is determined. A common practice is to use an existing anti-
virus tool to label the malware instances M (e.g., [16,13,11]), the presumption being
that anti-virus tools embody hand-coded rules to label malware instances and so are a
good source of “manually veriﬁed” ground truth. Unfortunately, existing evidence sug-
gests otherwise, in that it has been shown that anti-virus engines often disagree on their
labeling (and clustering) of malware instances [5]. To compensate for this, another prac-
tice has been to restrict attention to malware instances M on which multiple anti-virus
tools agree (e.g., [6]). Aside from substantially reducing the number of instances, we
conjecture that this practice might contribute to more favorable evaluations of malware
classiﬁers, essentially by limiting evaluations to easy-to-cluster instances. To demon-
strate this possibility, in Section 3 we consider malware instances selected in this way
242
P. Li et al.
and show that they can be classiﬁed by plagiarism detectors (designed without attention
to the subtleties of malware obfuscation) with precision and recall comparable to that
offered by a state-of-the-art malware clustering tool.
Distribution of cluster sizes in C and D:
In order to maximize both precision and
recall (and hence the F-measure), it is necessary for C and D to exhibit similar cluster-
size distributions; i.e., if one of them is highly biased (i.e., has few, large clusters) and
the other is more evenly distributed, then one of precision or recall will suffer. Even
when they exhibit similar cluster-size distributions, however, the degree to which that
distribution is biased has an effect on the signiﬁcance (e.g., [22, Section 8.5.8]) that
one can ascribe to high values of these measures. Informally, the signiﬁcance of a given
precision or recall is related to the probability that this value could have occurred by
random chance; the higher the probability, the less the signiﬁcance. We will explore the
effect of cluster-size distribution on signiﬁcance, and speciﬁcally the impact of cluster-
size distribution on the sensitivity of the F-measure to perturbations in the distance
matrix from which the clustering C is derived. We will see that all other factors held
constant, good precision and recall when the reference clusters in D are of similar size
is more signiﬁcant than if the cluster sizes are biased. That is, small perturbations in the
distance matrix yielding C tends to decay precision and recall more than if D and C are
highly biased.
We will demonstrate this phenomenon using the malware clustering results obtained
from the state-of-the-art malware clustering tool due to Bayer et al., which obtains
very different results on two malware datasets, one with a highly biased clustering and
one with a more even clustering. While this is not the only source of variation in the
datasets, and so the different results cannot be attributed solely to differences in cluster
size distributions, we believe that the cluster size distribution is a factor that must be
taken into account when reporting malware clustering results.
3 A Potential Hazard of Anti-virus Voting
As discussed in Section 2, a common practice to produce the ground-truth reference
clustering D for evaluating malware clustering algorithms is to use existing anti-virus
tools to label the malware instances and to restrict attention to malware instances M
on which multiple anti-virus tools agree. The starting point of our study is one such
ground-truth dataset, here denoted BCHKK-data, that was used by Bayer et al. for eval-
uating their malware clustering technique [6]. Using this dataset, their algorithm, here
denoted BCHKK-algo, yielded a very good precision and recall (of 0.984 and 0.930,
respectively). BCHKK-data consists of 2, 658 malware instances, which is a subset of
14, 212 malware instances contributed between October 27, 2007 and January 31, 2008
by a number of security organizations and individuals, spanning a wide range of sources
(such as web infections, honeypots, botnet monitoring, and other malware analysis ser-
vices). Bayer et al. ran six different anti-virus programs on these 14, 212 instances,
and a subset of 2, 658 instances on which results from the majority of these anti-virus
programs agree were chosen to form BCHKK-data for evaluation of their clustering
technique BCHKK-algo. Bayer et al. explained that such a subset was chosen because
On Challenges in Evaluating Malware Clustering
243
they are the instances on which ground truth can be obtained (due to agreement by a
majority of the anti-virus programs they used).
This seems to be a natural way to pick M for evaluation, as they are the only ones
for which the ground-truth clustering (i.e., D) could be obtained with good conﬁdence.
However, this also raises the possibility that the instances on which multiple anti-virus
tools agree are just the malware instances that are relatively easy to cluster, while the
difﬁcult-to-cluster instances are ﬁltered out of M . If this were the case, then this could
contribute to the high precision and recall observed for the BCHKK-data dataset, in
particular.
Unfortunately, we are unaware of any accepted methodology for testing this possi-
bility directly. So, we instead turn to another class of clustering tools derived without
attention to malware clustering, in order to see if they are able to cluster the malware
instances in BCHKK-data equally well. Speciﬁcally, we apply plagiarism detectors to
the BCHKK-data to see if they can obtain good precision and recall.