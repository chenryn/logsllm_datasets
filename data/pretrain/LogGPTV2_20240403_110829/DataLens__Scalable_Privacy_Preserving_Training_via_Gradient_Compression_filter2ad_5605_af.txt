Here we analyze the impact of our top-𝑘 gradient compression
method in TopAgg compared with other compression methods in
previous works, e.g., D2P-Fed and FetchSGD. In particular, for D2P-
Fed, we replace our Algorithm 2 (TopkStoSignGrad) that uses sto-
chastic sign compression with it, which essentially uses k-level gra-
dient quantization and random rotation for gradient pre-processing.
The detailed algorithm is shown in Algorithm 6 and Algorithm 5
in Appendix C.2. For FetchSGD, we uses the same stochastic sign
compression as we leverage sign signal as teacher voting in PATE
framework. During aggregation, we use Count Sketch data struc-
ture, and use top-𝑘 and unsketch operation to retrieve the aggre-
gated gradient. The detailed algorithm is shown in Algorithm 7
4Details can be found at https://github.com/carpedm20/DCGAN-tensorflow.
Session 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2156Table 4: Impact of different hyper-parameters: the number of Teachers, top-𝑘 and threshold 𝛽 under 𝜀 = 1 and 𝛿 = 10−5. We search for the
optimal parameter combinations and report the best accuracy by controlling the parameter in each cell.
(a) Hyper-parameters Search for MNIST and Fashion-MNIST
100
MNIST
0.5889
Fashion 0.5738
𝛽
0
MNIST
0.6361
Fashion 0.5859
Top-𝑘
200
0.7123
0.6478
0.1
0.6450
0.6103
# of Teachers
300
0.6753
0.6088
0.3
0.6890
0.6060
2000
0.5841
0.5608
0.5
0.6921
0.6122
3000
0.7061
0.5952
0.7
0.7123
0.6213
4000
0.7123
0.6478
0.9
0.6956
0.6478
(b) Hyper-parameters Search for CelebA-Hair and CelebA-Gender
Top-𝑘
700
0.7058
0.6061
0.6
0.6789
0.5669
# of Teachers
900
0.6811
0.5769
0.7
0.6922
0.5612
4000
0.6378
0.5669
0.8
0.6861
0.6022
6000
0.7058
0.5835
0.85
0.7058
0.5835
8000
0.6936
0.6061
0.9
0.6381
0.6061
500
0.6922
0.5792
0.5
0.6440
0.4957
CelebA-Gender
CelebA-Hair
𝛽
CelebA-Gender
CelebA-Hair
Table 5: Accuracy Comparison of different gradient compression
methods (TopAgg, D2P-Fed, FetchSGD). We report the test classi-
fication accuracy of models trained with data generated with each
technique under 𝜀 = 1 and 𝛿 = 10−5.
Table 6: Running Time Comparison of different gradient compres-
sion methods (TopAgg, D2P-Fed, FetchSGD). We report the aver-
age training time per epoch on different datasets under 𝜀 = 1 and
𝛿 = 10−5.
Methods TopAgg D2P-Fed FetchSGD
Methods TopAgg D2P-Fed FetchSGD
Dataset
MNIST
Fashion-MNIST
CelebA-Gender
CelebA-Hair
0.7123
0.6478
0.7058
0.6061
0.1424
0.1667
0.4445
0.2893
0.6935
0.6387
0.6552
0.4926
Dataset
MNIST
Fashion-MNIST
CelebA-Gender
CelebA-Hair
338.34 s
492.43s
340.84s
471.02s
1196.60s
3683.22s
1120.59s 8092.50 s
785.34 s
775.35s
2622.40s
2620.63s
in Appendix C.2. From Table 5, we note that D2P-Fed and FetchSGD
are outperformed by our TopAgg in terms of data utility, which
is mainly due to the increase of the consumption of privacy bud-
get and the introduction of additional noise during aggregation.
Concretely, D2P-Fed uses 𝑚-level gradient quantization, which in-
√
√
creases the sensitivity of quantized gradients from 2
𝑘.
𝑘 to 𝑚
Without top-𝑘 mechanism, D2P-Fed compression quickly reaches
the limit of the privacy budget, and thus the model barely converges.
Although FetchSGD uses a similar top-𝑘 mechanism during com-
pression, the adoption of Count Sketch data structure introduces
additional noisy information when approximating the aggregated
gradient, and therefore hurts the utility of the generated data.
Moreover, we record the running time of DataLens and adapted
gradient compression methods D2P-Fed and FetchSGD on one
Tesla T4 GPU under the best parameters of MNIST, Fashion-MNIST,
CelebA-Hair, and Celeb-Gender. The average running for each
epoch is shown in Table 6. The time consumption for D2P-Fed
is significantly higher than TopAgg due to the k-level quantiza-
tion step as well as the rotation step for gradient transformation.
The time consumption for FetchSGD is significantly higher than
TopAgg due to the Count Sketch data structure overhead.
Runtime Analysis. We record the running time of our frame-
work on one RTX-2080 Ti GPU under the best parameter (4000
teacher) of MNIST for 𝜀 = 1 for three runs. We then only change
the number of teacher discrinimators to 2000 and record the run-
ning time again. The average running time for each epoch given
different teacher discriminators are 149.92s for 2000 teachers and
322.17s for 4000 teachers, respectively. The student generator con-
verges within 100 epochs, thus the total training time is around 4−8
hours for MNIST under 𝜀 = 1. The runtime scales almost linear to
the number of teachers, so adopting a larger number of teachers
Table 7: Ablation studies on the impact of different components
of DataLens pipeline on Image Datasets: We report the test classi-
fication accuracy of models trained with data generated based on
different variants of DataLens under 𝜀 = 1, 𝛿 = 10−5. The first row
of each data groups presents the performance of DataLens.
Component
Top-𝑘
Stochastic
Aggregation
Quantization Thresholding
Accuracy
Dataset
MNIST
Fashion-MNIST
CelebA-Gender
CelebA-Hair
✓
✗
✓
✓
✓
✗
✓
✓
✓
✗
✓
✓
✓
✗
✓
✓
✓
✓
✗
✓
✓
✓
✗
✓
✓
✓
✗
✓
✓
✓
✗
✓
✓
✓
✓
✗
✓
✓
✓
✗
✓
✓
✓
✗
✓
✓
✓
✗
0.7123
0.5170
0.6741
0.6361
0.6478
0.4775
0.6159
0.5859
0.7058
0.6134
0.6889
0.6860
0.6061
0.3318
0.5325
0.5504
will not bring much computation overhead. In contrast, the average
training time for DP-GAN and G-PATE takes around 26 − 34 hours
for MNIST under 𝜀 = 1. Moreover, GS-WGAN requires hundreds
of GPU hours to pretrain one thousand non-private GAN as the
warm-up steps.
Ablation Studies on the Impact of Different Components
To further understand where the improvements
in DataLens.
Session 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2157of DataLens come from, we investigate how each component in
DataLens pipeline contributes to the generated data utility im-
provement in Table 7 on four high-dimensional image datasets.
In particular, we consider the following components: (1) top-𝑘,
(2) stochastic gradient quantization, and (3) gradient thresholding,
and evaluate how they impact the data utility by adding or remov-
ing each component. We note that the top-𝑘 procedure is the most
important component based on results in Table 7, since removing
this step will largely increase the privacy consumption, leading
models fail to converge when given limited privacy budget. Gradi-
ent quantization and thresholding are also useful techniques though
less critical, contributing to the 3% − 7% of the utility improvement
as shown in Table 7.
6 RELATED WORK
DP Generative Models. In order to generate data with differential
privacy guarantees, several works have been conducted to develop
DP generative models for low-dimensional data such as tabular data.
Some of them apply differential privacy to traditional data gener-
ation algorithms, such as Bayesian networks [63], synthetic data
generation from marginal distributions [46], and the multiplicative