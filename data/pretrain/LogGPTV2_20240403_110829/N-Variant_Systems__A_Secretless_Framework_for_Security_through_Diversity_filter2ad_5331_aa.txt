title:N-Variant Systems: A Secretless Framework for Security through Diversity
author:Benjamin Cox and
David Evans
N-Variant Systems
A Secretless Framework for Security through Diversity
Benjamin Cox, David Evans, Adrian Filipi, Jonathan Rowanhill,  
Wei Hu, Jack Davidson, John Knight, Anh Nguyen-Tuong, and Jason Hiser
University of Virginia, Department of Computer Science
http://www.nvariant.org
Abstract
We present an architectural framework for systematically using automated diversity to provide high assurance detec-
tion and disruption for large classes of attacks. The framework executes a set of automatically diversified variants on
the same inputs, and monitors their behavior to detect divergences. The benefit of this approach is that it requires an
attacker to simultaneously compromise all system variants with the same input. By constructing variants with disjoint
exploitation sets, we can make it impossible to carry out large classes of important attacks. In contrast to previous
approaches that use automated diversity for security, our approach does not rely on keeping any secrets. In this pa-
per, we introduce the N-variant systems framework, present a model for analyzing security properties of N-variant
systems, define variations that can be used to detect attacks that involve referencing absolute memory addresses and
executing injected code, and describe and present performance results from a prototype implementation.
1. Introduction
Many security researchers have noted that the current
computing monoculture leaves our infrastructure vul-
nerable to a massive, rapid attack [70, 29, 59]. One
mitigation strategy that has been proposed is to increase
software diversity. By making systems appear different
to attackers, diversity makes it more difficult to con-
struct exploits and limits an attack’s ability to propa-
gate. Several techniques for automatically producing
diversity have been developed including rearranging
memory [8, 26, 25, 69] and randomizing the instruction
set [6, 35]. All these techniques depend on keeping cer-
tain properties of the running execution secret from the
attacker. Typically, these properties are determined by a
secret key used to control the randomization. If the se-
cret used to produce a given variant is compromised, an
attack can be constructed that successfully attacks that
variant. Pointer obfuscation techniques, memory ad-
dress space randomization, and instruction set randomi-
zation have all been demonstrated to be vulnerable to
remote attacks [55, 58, 64]. Further, the diversification
secret may be compromised through side channels, in-
sufficient entropy, or insider attacks.
Our work uses artificial diversity in a new way that does
not depend on keeping secrets: instead of diversifying
individual systems, we construct a single system con-
taining multiple variants designed to have disjoint ex-
ploitation sets. Figure 1 illustrates our framework. We
refer to the entire server as an N-variant system. The
system shown is a 2-variant system, but our framework
generalizes to any number of variants. The polygrapher
takes input from the client and copies it to all the vari-
ants. The original server process P is replaced with the
two variants, P0 and P1. The variants maintain the cli-
ent-observable behavior of P on all normal inputs. They
are, however, artificially diversified in a way that makes
them behave differently on abnormal inputs that corre-
spond to an attack of a certain class. The monitor ob-
serves the behavior of the variants to detect divergences
which reveal attacks. When a divergence is detected,
the monitor restarts the variants in known uncompro-
mised states.
As a simple example, suppose P0 and P1 use disjoint
memory spaces such that any absolute memory address
that is valid in P0 is invalid in P1, and vice versa. Since
the variants are transformed to provide the same seman-
tics regardless of the memory space used, the behavior
Input
from
Client
Output
to
Client
Polygrapher
P0
P1
Monitor
Server
Figure 1. N-Variant System Framework. 
USENIX Association
Security ’06: 15th USENIX Security Symposium
105
on all normal inputs is identical (assuming deterministic
behavior, which we address in Section 5). However, if
an exploit uses an absolute memory address directly, it
must be an invalid address on one of the two variants.
The monitor can easily detect the illegal memory access
on the other variant since it is detected automatically by
the operating system. When monitoring is done at the
system call level, as in our prototype implementation,
the attack is detected before any external state is modi-
fied or output is returned to the attacker.
The key insight behind our approach is that in order for
an attacker to exploit a vulnerability in P, a pathway
must exist on one of the variants that exploits the vul-
nerability without producing detectably anomalous be-
havior on any of the other variants. If no such pathway
exists, there is no way for the attacker to construct a
successful attack, even if the attacker has complete
knowledge of the variants. Removing the need to keep
secrets means we do not need to be concerned with
probing or guessing attacks, or even with attacks that
take advantage of insider information.
Our key contributions are:
1.
Introducing the N-variant systems framework
that uses automated diversity techniques to pro-
vide high assurance security properties without
needing to keep any secrets.
2. Developing a model for reasoning about N-vari-
ant systems including the definition of the nor-
mal equivalence and detection properties used to
prove security properties of an ideal N-variant
system (Section 3).
3.
partitioning
Identifying two example techniques for provid-
ing variation in N-variant systems: the memory
address
(introduced
above) that detects attacks that involve absolute
memory references and the instruction tagging
technique that detects attempts to execute in-
jected code (Section 4).
technique
4. Describing a Linux kernel system implementa-
tion and analyzing its performance (Section 5).
In this paper we do not address recovery but consider it
to be a successful outcome when our system transforms
an attack that could compromise privacy and integrity
into an attack that at worst causes a service shutdown
that denies service to legitimate users. It has not es-
caped our attention, however, that examining differ-
ences between the states of the two variants at the point
when an attack is detected provides some intriguing
recovery possibilities. Section 6 speculates on these
opportunities and other possible extensions to our work.
2. Related Work
There has been extensive work done on eliminating
security vulnerabilities and mitigating attacks. Here, we
briefly describe previous work on other types of de-
fenses and automated diversity, and summarize related
work on redundant processing and design diversity
frameworks.
Other defenses. Many of the specific vulnerabilities
we address have well known elimination, mitigation and
disruption techniques. Buffer overflows have been
widely studied and numerous defenses have been devel-
oped including static analysis to detect and eliminate the
vulnerabilities [66, 67, 39, 23], program transformation
and dynamic detection techniques [19, 5, 30, 45, 49, 57]
and hardware modifications [38, 40, 41, 64]. There
have also been several defenses proposed for string
format vulnerabilities [56, 20, 63, 47]. Some of these
techniques can mitigate specific classes of vulnerabili-
ties with less expense and performance overhead than is
required for our approach. Specific defenses, however,
only prevent a limited class of specific vulnerabilities.
Our approach is more general; it can mitigate all attacks
that depend on particular functionality such as injecting
code or accessing absolute addresses.
More general defenses have been proposed for some
attack classes. For example, no execute pages (as pro-
vided by OpenBSD’s W^X and Windows XP Service
Pack 2) prevent many code injection attacks [2], dy-
namic taint analysis tracks information flow to identify
memory corruption attacks [43], and control-flow integ-
rity can detect attacks that corrupt an application to fol-
low invalid execution paths [1]. Although these are
promising approaches,
they are limited to particular
attack classes. Our framework is more general in the
sense that we can construct defense against any attacker
capability that can be varied across variants in an
N-variant system.
Automated diversity. Automated diversity applies
transformations to software to increase the difficulty an
attacker will face in exploiting a security vulnerability
in that software. Numerous transformation techniques
have been proposed including rearranging memory [26,
8, 69, 25], randomizing system calls [17], and random-
izing the instruction set [6, 35]. Our work is comple-
mentary to work on producing diversity; we can incor-
porate many different sources of variation as long as
variants are constructed carefully to ensure the disjoint-
106
Security ’06: 15th USENIX Security Symposium
USENIX Association
edness required by our framework. A major advantage
of the N-variant systems approach is that we do not rely
on secrets for our security properties. This means we
can employ diversification techniques with low entropy,
so long as the transformations are able to produce vari-
ants with disjoint exploitation sets. Holland, Lim, and
Seltzer propose many low entropy diversification tech-
niques including number representations, register sets,
stack direction, and memory layout [31]. In addition,
our approach is not vulnerable to the type of secret-
breaking attacks that have been demonstrated against
secret-based diversity defenses [55, 58, 64].
O’Donnell and Sethu studied techniques for distributing
diversity at the level of different software packages in a
network to mitigate spreading attacks [44]. This can
limit the ability of a worm exploiting a vulnerability
present in only one of the software packages to spread
on a network. Unlike our approach, however, even at
the network level an attacker who discovers vulnerabili-
ties in more than one of the software packages can ex-
ploit each of them independently.
Redundant execution. The idea of using redundant
program executions for various purposes is not a new
one. Architectures involving replicated processes have
been proposed as a means to aid debugging, to provide
fault
to improve dependability, and more
recently, to harden vulnerable services against attacks.
tolerance,
The earliest work to consider running multiple variants
of a process of which we are aware is Knowlton’s 1968
paper [37] on a variant technique for detecting and lo-
calizing programming errors. It proposed simultane-
ously executing two programs which were logically
equivalent but assembled differently by breaking the
code into fragments, and then reordering the code frag-
ments and data segments with appropriate jump instruc-
tions inserted between code fragments to preserve the
original program semantics. The CPU could run in a
checking mode that would execute both programs in
parallel and verify that
they execute semantically
equivalent instructions. The variants they used did not
provide any guarantees, but provided a high probability
of detecting many programming errors such as out-of-
range control transfers and wild memory fetches.
More recently, Berger and Zorn proposed a redundant
execution framework with multiple replicas each with a
different randomized layout of objects within the heap
to provide probabilistic memory safety [7]. Since there
is no guarantee that there will not be references at the
same absolute locations, or reachable through the same
relative offsets, their approach can provide only prob-
abilistic expectations that a memory corruption will be
detected by producing noticeably different behavior on
the variants. Their goals were to enhance reliability and
availability, rather than to detect and resist attacks.
Consequently, when variations diverge in their frame-
work, they allow the agreeing replicas to continue based
on the assumption that the cause of the divergence in
the other replicas was due a memory flaw rather than a
successful attack. Their
replication framework only
handles processes whose I/O is through standard in/out,
and only a limited number of system calls are caught in
user space to ensure all replicas see the same values.
Since monitoring is only on the standard output, a com-
promised replica could be successfully performing an
attack and, as long as it does not fill up its standard out
buffer, the monitor would not notice. The key difference
between their approach and ours, is that their approach
is probabilistic whereas our variants are constructed to
guarantee disjointedness with respect to some property,
and thereby can provide guarantees of invulnerability to
particular attack classes. A possible extension to our
work would consider variations providing probabilistic
protection, such as the heap randomization technique
they used, to deal with attack classes for which disjoint-
edness is infeasible.
Redundant processing of the same instruction stream by
multiple processors has been used as a way to provide
fault-tolerance by Stratus [68] and Tandem [32] com-
puters. For example, Integrity S2 used triple redun-
dancy in hardware with three synchronized identical
processors executing the same instructions [32]. A ma-
jority voter selects the majority output from the three
processors, and a vote analyzer compares the outputs to
activate a failure mode when a divergence is detected.
This type of redundancy provides resilience to hardware
faults, but no protection against malicious attacks that
exploit vulnerabilities in the software, which is identical
on all three processors. Slipstream processors are an
interesting variation of this, where two redundant ver-
sions of the instruction stream execute, but instructions
that are dynamically determined to be likely to be un-
necessary are removed from the first stream which exe-
cutes speculatively [60]. The second stream executes
behind the first stream, and the processor detects incon-
sistencies between the two executions. These devia-
tions either indicate false predications about unneces-
sary computations (such as a mispredicted branch) or
hardware faults.
The distributed systems community has used active rep-
lication to achieve fault tolerance [9, 10, 16, 18, 50].
With active replication, all replicas are running the
same software and process the same requests. Unlike
USENIX Association
Security ’06: 15th USENIX Security Symposium
107
our approach, however, active replication does nothing
to hide design flaws in the software since all replicas are
running the same software. To mitigate this problem,
Schneider and Zhou have suggested proactive diversity,
a technique for periodically randomizing replicas to
justify the assumption that server replicas fail independ-
ently and to limit the window of vulnerability in which
replicas are susceptible to the same exploit [51]. Active
replication and N-variant systems are complementary
approaches. Combining them can provide the benefits
of both approaches with the overhead and costs associ-
ated with either approach independently.
Design diversity frameworks. The name N-variant
systems is inspired by, but fundamentally different from,
the technique known as N-version programming [3, 14].
The N-version programming method uses several inde-
pendent development groups to develop different im-
plementations of the same specification with the hope
that different development groups will produce versions
without common faults. The use of N-version program-
ming to help with system security was proposed by Jo-
seph [33]. He analyzed design diversity as manifest in
N-version programming to see whether it could defeat
certain attacks and developed an analogy between faults
in computing systems that might affect reliability and
vulnerabilities in computer systems that might affect
security. He argued that N-version programming tech-
niques might allow vulnerabilities to be masked. How-
ever, N-version programming provides no guarantee
that the versions produced by different teams will not
have common flaws. Indeed, experiments have shown
that common flaws in implementations do occur [36]. In
our work, program variants are created by mechanical
transformations engineered specifically to differ in par-
ticular ways that enable attack detection. In addition,
our variants are produced mechanically, so the cost of
multiple development teams is avoided.
Three recent projects [46, 62, 28] have explored using
design diversity in architectures similar to the one we
propose here in which the outputs or behaviors of two
diverse implementations of
the same service (e.g.,
HTTP servers Apache on Linux and IIS on Windows)
are compared and differences above a set threshold in-
dicate a likely attack. The key difference between those
projects and our work is that whereas they use diverse
available implementations of the same service, we use
techniques to artificially produce specific kinds of
variation. The HACQIT project [34, 46] deployed two
COTS web servers (IIS running on Windows and
Apache running on Linux) in an architecture where a
third computer forwarded all requests to both servers