• Are vulnerable to external triggers that can create large amounts of load (e.g., an 	externally triggered software update might clear an offline client’s cache)
Depending on your service, you may or may not be in control of all the client code that talks to your service. However, it’s still a good idea to have an understanding of how large clients that interact with your service will behave.The same principles apply to large internal clients. Stage system failures with the larg‐est clients to see how they react. Ask internal clients how they access your service and what mechanisms they use to handle backend failure.
Test Noncritical Backends
Test your noncritical backends, and make sure their unavailability does not interfere with the critical components of your service.For example, suppose your frontend has critical and noncritical backends. Often, a given request includes both critical components (e.g., query results) and noncritical components (e.g., spelling suggestions). Your requests may significantly slow down and consume resources waiting for noncritical backends to finish.In addition to testing behavior when the noncritical backend is unavailable, test how the frontend behaves if the noncritical backend never responds (for example, if it is blackholing requests). Backends advertised as noncritical can still cause problems on frontends when requests have long deadlines. The frontend should not start rejecting lots of requests, running out of resources, or serving with very high latency when a noncritical backend blackholes.Immediate Steps to Address Cascading Failures
Once you have identified that your service is experiencing a cascading failure, you can use a few different strategies to remedy the situation—and of course, a cascading failure is a good opportunity to use your incident management protocol (Chap‐ter 14).
Increase ResourcesIncrease Resources
If your system is running at degraded capacity and you have idle resources, adding tasks can be the most expedient way to recover from the outage. However, if the ser‐vice has entered a death spiral of some sort, adding more resources may not be suffi‐cient to recover.
280  |  Chapter 22: Addressing Cascading Failures
Stop Health Check Failures/DeathsSome cluster scheduling systems, such as Borg, check the health of tasks in a job and restart tasks that are unhealthy. This practice may create a failure mode in which health-checking itself makes the service unhealthy. For example, if half the tasks aren’t able to accomplish any work because they’re starting up and the other half will soon be killed because they’re overloaded and failing health checks, temporarily disabling health checks may permit the system to stabilize until all the tasks are running.Process health checking (“is this binary responding at all?”) and service health check‐ing (“is this binary able to respond to this class of requests right now?”) are two con‐ceptually distinct operations. Process health checking is relevant to the cluster scheduler, whereas service health checking is relevant to the load balancer. Clearly distinguishing between the two types of health checks can help avoid this scenario.Restart Servers
If servers are somehow wedged and not making progress, restarting them may help. Try restarting servers when:
• Java servers are in a GC death spiral
• Some in-flight requests have no deadlines but are consuming resources, leading 	them to block threads, for example
• The servers are deadlocked• The servers are deadlocked
Make sure that you identify the source of the cascading failure before you restart your servers. Make sure that taking this action won’t simply shift around load. Canary this change, and make it slowly. Your actions may amplify an existing cascading failure if the outage is actually due to an issue like a cold cache.
Drop TrafficDrop Traffic
Dropping load is a big hammer, usually reserved for situations in which you have a true cascading failure on your hands and you cannot fix it by other means. For exam‐ple, if heavy load causes most servers to crash as soon as they become healthy, you can get the service up and running again by:
1. Addressing the initial triggering condition (by adding capacity, for example).2. Reducing load enough so that the crashing stops. Consider being aggressive here	—if the entire service is crash-looping, only allow, say, 1% of the traffic through.
3. Allowing the majority of the servers to become healthy.
4. Gradually ramping up the load.
Immediate Steps to Address Cascading Failures  |  281
This strategy allows caches to warm up, connections to be established, etc., before load returns to normal levels.Obviously, this tactic will cause a lot of user-visible harm. Whether or not you’re able to (or if you even should) drop traffic indiscriminately depends on how the service is configured. If you have some mechanism to drop less important traffic (e.g., prefetch‐ing), use that mechanism first.It is important to keep in mind that this strategy enables you to recover from a cas‐cading outage once the underlying problem is fixed. If the issue that started the cas‐cading failure is not fixed (e.g., insufficient global capacity), then the cascading failure may trigger shortly after all traffic returns. Therefore, before using this strategy, con‐sider fixing (or at least papering over) the root cause or triggering condition. For example, if the service ran out of memory and is now in a death spiral, adding more memory or tasks should be your first step.Enter Degraded Modes
Serve degraded results by doing less work or dropping unimportant traffic. This strategy must be engineered into your service, and can be implemented only if you know which traffic can be degraded and you have the ability to differentiate between the various payloads.
Eliminate Batch LoadEliminate Batch Load
Some services have load that is important, but not critical. Consider turning off those sources of load. For example, if index updates, data copies, or statistics gathering con‐sume resources of the serving path, consider turning off those sources of load during an outage.
Eliminate Bad Traffic
If some queries are creating heavy load or crashes (e.g., queries of death), consider blocking them or eliminating them via other means.282  |  Chapter 22: Addressing Cascading Failures
Cascading Failure and Shakespeare
A documentary about Shakespeare’s works airs in Japan, and explicitly points to our Shakespeare service as an excellent place to conduct further research. Following the broadcast, traffic to our Asian datacenter surges beyond the service’s capacity. This capacity problem is further compounded by a major update to the Shakespeare ser‐vice that simultaneously occurs in that datacenter.Fortunately, a number of safeguards are in place that help mitigate the potential for failure. The Production Readiness Review process identified some issues that the team already addressed. For example, the developers built graceful degradation into the service. As capacity becomes scarce, the service no longer returns pictures along‐side text or small maps illustrating where a story takes place. And depending on its purpose, an RPC that times out is either not retried (for example, in the case of the aforementioned pictures), or is retried with a randomized exponential backoff. Despite these safeguards, the tasks fail one by one and are then restarted by Borg, which drives the number of working tasks down even more.As a result, some graphs on the service dashboard turn an alarming shade of red and SRE is paged. In response, SREs temporarily add capacity to the Asian datacenter by increasing the number of tasks available for the Shakespeare job. By doing so, they’re able to restore the Shakespeare service in the Asian cluster.Afterward, the SRE team writes a postmortem detailing the chain of events, what went well, what could have gone better, and a number of action items to prevent this scenario from occurring again. For example, in the case of a service overload, the GSLB load balancer will redirect some traffic to neighboring datacenters. Also, the SRE team turns on autoscaling, so that the number of tasks automatically increases with traffic, so they don’t have to worry about this type of issue again.Closing Remarks
When systems are overloaded, something needs to give in order to remedy the situa‐tion. Once a service passes its breaking point, it is better to allow some user-visible errors or lower-quality results to slip through than try to fully serve every request. Understanding where those breaking points are and how the system behaves beyond them is critical for service owners who want to avoid cascading failures.Closing Remarks  |  283
Without proper care, some system changes meant to reduce background errors or otherwise improve the steady state can expose the service to greater risk of a full out‐age. Retrying on failures, shifting load around from unhealthy servers, killing unheal‐thy servers, adding caches to improve performance or reduce latency: all of these might be implemented to improve the normal case, but can improve the chance of causing a large-scale failure. Be careful when evaluating changes to ensure that one outage is not being traded for another.284  |  Chapter 22: Addressing Cascading Failures
CHAPTER 23
Managing Critical State: Distributed Consensus for Reliability
Written by Laura Nolan 
Edited by Tim HarveyProcesses crash or may need to be restarted. Hard drives fail. Natural disasters can take out several datacenters in a region. Site Reliability Engineers need to anticipate these sorts of failures and develop strategies to keep systems running in spite of them. These strategies usually entail running such systems across multiple sites. Geographi‐cally distributing a system is relatively straightforward, but also introduces the need to maintain a consistent view of system state, which is a more nuanced and difficult undertaking.Groups of processes may want to reliably agree on questions such as:
• Which process is the leader of a group of processes?
• What is the set of processes in a group?
• Has a message been successfully committed to a distributed queue?
• Does a process hold a lease or not?
• What is a value in a datastore for a given key?We’ve found distributed consensus to be effective in building reliable and highly available systems that require a consistent view of some system state. The distributed consensus problem deals with reaching agreement among a group of processes con‐nected by an unreliable communications network. For instance, several processes in a distributed system may need to be able to form a consistent view of a critical piece of configuration, whether or not a distributed lock is held, or if a message on a queue has been processed. It is one of the most fundamental concepts in distributed com‐285
puting and one we rely on for virtually every service we offer. Figure 23-1 illustrates a simple model of how a group of processes can achieve a consistent view of system state through distributed consensus.
Figure 23-1. Distributed consensus: agreement among a group of processesWhenever you see leader election, critical shared state, or distributed locking, we rec‐ommend using distributed consensus systems that have been formally proven and tested thoroughly. Informal approaches to solving this problem can lead to outages, and more insidiously, to subtle and hard-to-fix data consistency problems that may pro‐long outages in your system unnecessarily.
CAP TheoremCAP Theorem
The CAP theorem ([Fox99], [Bre12]) holds that a distributed system cannot simulta‐neously have all three of the following properties:
• Consistent views of the data at each node
• Availability of the data at each node
• Tolerance to network partitions [Gil02]The logic is intuitive: if two nodes can’t communicate (because the network is parti‐tioned), then the system as a whole can either stop serving some or all requests at some or all nodes (thus reducing availability), or it can serve requests as usual, which results in inconsistent views of the data at each node.
286  |  Chapter 23: Managing Critical State: Distributed Consensus for ReliabilityBecause network partitions are inevitable (cables get cut, packets get lost or delayed due to congestion, hardware breaks, networking components become misconfigured, etc.), understanding distributed consensus really amounts to understanding how consistency and availability work for your particular application. Commercial pres‐sures often demand high levels of availability, and many applications require consis‐tent views on their data.Systems and software engineers are usually familiar with the traditional ACID data‐store semantics (Atomicity, Consistency, Isolation, and Durability), but a growing number of distributed datastore technologies provide a different set of semantics known as BASE (Basically Available, Soft state, and Eventual consistency). Datastores that support BASE semantics have useful applications for certain kinds of data and can handle large volumes of data and transactions that would be much more costly, and perhaps altogether infeasible, with datastores that support ACID semantics.Most of these systems that support BASE semantics rely on multimaster replication, where writes can be committed to different processes concurrently, and there is some mechanism to resolve conflicts (often as simple as “latest timestamp wins”). This approach is usually known as eventual consistency. However, eventual consistency can lead to surprising results [Lu15], particularly in the event of clock drift (which is inevitable in distributed systems) or network partitioning [Kin15].1It is also difficult for developers to design systems that work well with datastores that support only BASE semantics. Jeff Shute [Shu13], for example, has stated, “we find developers spend a significant fraction of their time building extremely complex and error-prone mechanisms to cope with eventual consistency and handle data that may be out of date. We think this is an unacceptable burden to place on developers and that consistency problems should be solved at the database level.”System designers cannot sacrifice correctness in order to achieve reliability or perfor‐mance, particularly around critical state. For example, consider a system that handles financial transactions: reliability or performance requirements don’t provide much value if the financial data is not correct. Systems need to be able to reliably synchron‐ize critical state across multiple processes. Distributed consensus algorithms provide this functionality.1 Kyle Kingsbury has written an extensive series of articles on distributed systems correctness, which contain many examples of unexpected and incorrect behavior in these kinds of datastores. See .
Managing Critical State: Distributed Consensus for Reliability  |  287
Motivating the Use of Consensus: Distributed Systems Coordination FailureDistributed systems are complex and subtle to understand, monitor, and trouble‐shoot. Engineers running such systems are often surprised by behavior in the pres‐ence of failures. Failures are relatively rare events, and it is not a usual practice to test systems under these conditions. It is very difficult to reason about system behavior during failures. Network partitions are particularly challenging—a problem that appears to be caused by a full partition may instead be the result of:• A very slow network
• Some, but not all, messages being dropped
• Throttle occurring in one direction, but not the other direction
The following sections provide examples of problems that occurred in real-world dis‐tributed systems and discuss how leader election and distributed consensus algo‐rithms could be used to prevent such issues.
Case Study 1: The Split-Brain ProblemCase Study 1: The Split-Brain Problem
A service is a content repository that allows collaboration between multiple users. It uses sets of two replicated file servers in different racks for reliability. The service needs to avoid writing data simultaneously to both file servers in a set, because doing so could result in data corruption (and possibly unrecoverable data).Each pair of file servers has one leader and one follower. The servers monitor each other via heartbeats. If one file server cannot contact its partner, it issues a STONITH (Shoot The Other Node in the Head) command to its partner node to shut the node down, and then takes mastership of its files. This practice is an industry standard method of reducing split-brain instances, although as we shall see, it is conceptually unsound.What happens if the network becomes slow, or starts dropping packets? In this sce‐nario, file servers exceed their heartbeat timeouts and, as designed, send STONITH commands to their partner nodes and take mastership. However, some commands may not be delivered due to the compromised network. File server pairs may now be in a state in which both nodes are expected to be active for the same resource, or where both are down because both issued and received STONITH commands. This results in either corruption or unavailability of data.The problem here is that the system is trying to solve a leader election problem using simple timeouts. Leader election is a reformulation of the distributed asynchronous consensus problem, which cannot be solved correctly by using heartbeats.
288  |  Chapter 23: Managing Critical State: Distributed Consensus for Reliability
Case Study 2: Failover Requires Human InterventionA highly sharded database system has a primary for each shard, which replicates syn‐chronously to a secondary in another datacenter. An external system checks the health of the primaries, and, if they are no longer healthy, promotes the secondary to primary. If the primary can’t determine the health of its secondary, it makes itself unavailable and escalates to a human in order to avoid the split-brain scenario seen in Case Study 1.This solution doesn’t risk data loss, but it does negatively impact availability of data. It also unnecessarily increases operational load on the engineers who run the system, and human intervention scales poorly. This sort of event, where a primary and secon‐dary have problems communicating, is highly likely to occur in the case of a larger infrastructure problem, when the responding engineers may already be overloaded with other tasks. If the network is so badly affected that a distributed consensus sys‐tem cannot elect a master, a human is likely not better positioned to do so.Case Study 3: Faulty Group-Membership Algorithms
A system has a component that performs indexing and searching services. When starting, nodes use a gossip protocol to discover each other and join the cluster. The cluster elects a leader, which performs coordination. In the case of a network parti‐tion that splits the cluster, each side (incorrectly) elects a master and accepts writes and deletions, leading to a split-brain scenario and data corruption.The problem of determining a consistent view of group membership across a group of processes is another instance of the distributed consensus problem.In fact, many distributed systems problems turn out to be different versions of dis‐tributed consensus, including master election, group membership, all kinds of dis‐tributed locking and leasing, reliable distributed queuing and messaging, and maintenance of any kind of critical shared state that must be viewed consistently across a group of processes. All of these problems should be solved only using dis‐tributed consensus algorithms that have been proven formally correct, and whose implementations have been tested extensively. Ad hoc means of solving these sorts of problems (such as heartbeats and gossip protocols) will always have reliability prob‐lems in practice.How Distributed Consensus Works
The consensus problem has multiple variants. When dealing with distributed soft‐ware systems, we are interested in asynchronous distributed consensus, which applies to environments with potentially unbounded delays in message passing. (Synchronous consensus applies to real-time systems, in which dedicated hardware means that mes‐sages will always be passed with specific timing guarantees.)How Distributed Consensus Works  |  289
Distributed consensus algorithms may be crash-fail (which assumes that crashed nodes never return to the system) or crash-recover. Crash-recover algorithms are much more useful, because most problems in real systems are transient in nature due to a slow network, restarts, and so on.Algorithms may deal with Byzantine or non-Byzantine failures. Byzantine failure occurs when a process passes incorrect messages due to a bug or malicious activity, and are comparatively costly to handle, and less often encountered.
Technically, solving the asynchronous distributed consensus problem in bounded time is impossible. As proven by the Dijkstra Prize–winning FLP impossibility result [Fis85], no asynchronous distributed consensus algorithm can guarantee progress in the presence of an unreliable network.In practice, we approach the distributed consensus problem in bounded time by ensuring that the system will have sufficient healthy replicas and network connectiv‐ity to make progress reliably most of the time. In addition, the system should have backoffs with randomized delays. This setup both prevents retries from causing cas‐cade effects and avoids the dueling proposers problem described later in this chapter. The protocols guarantee safety, and adequate redundancy in the system encourages liveness.The original solution to the distributed consensus problem was Lamport’s Paxos pro‐tocol [Lam98], but other protocols exist that solve the problem, including Raft [Ong14], Zab [Jun11], and Mencius [Mao08]. Paxos itself has many variations intended to increase performance [Zoo14]. These usually vary only in a single detail, such as giving a special leader role to one process to streamline the protocol.Paxos Overview: An Example Protocol
Paxos operates as a sequence of proposals, which may or may not be accepted by a majority of the processes in the system. If a proposal isn’t accepted, it fails. Each pro‐posal has a sequence number, which imposes a strict ordering on all of the operations in the system.In the first phase of the protocol, the proposer sends a sequence number to the acceptors. Each acceptor will agree to accept the proposal only if it has not yet seen a proposal with a higher sequence number. Proposers can try again with a higher sequence number if necessary. Proposers must use unique sequence numbers (draw‐ing from disjoint sets, or incorporating their hostname into the sequence number, for instance).If a proposer receives agreement from a majority of the acceptors, it can commit the proposal by sending a commit message with a value.
290  |  Chapter 23: Managing Critical State: Distributed Consensus for ReliabilityThe strict sequencing of proposals solves any problems relating to ordering of mes‐sages in the system. The requirement for a majority to commit means that two differ‐ent values cannot be committed for the same proposal, because any two majorities will overlap in at least one node. Acceptors must write a journal on persistent storage whenever they agree to accept a proposal, because the acceptors need to honor these guarantees after restarting.Paxos on its own isn’t that useful: all it lets you do is to agree on a value and proposal number once. Because only a quorum of nodes need to agree on a value, any given node may not have a complete view of the set of values that have been agreed to. This limitation is true for most distributed consensus algorithms.
System Architecture Patterns for Distributed ConsensusDistributed consensus algorithms are low-level and primitive: they simply allow a set of nodes to agree on a value, once. They don’t map well to real design tasks. What makes distributed consensus useful is the addition of higher-level system components such as datastores, configuration stores, queues, locking, and leader election services to provide the practical system functionality that distributed consensus algorithms don’t address. Using higher-level components reduces complexity for system design‐ers. It also allows underlying distributed consensus algorithms to be changed if neces‐sary in response to changes in the environment in which the system runs or changes in nonfunctional requirements.Many systems that successfully use consensus algorithms actually do so as clients of some service that implements those algorithms, such as Zookeeper, Consul, and etcd. Zookeeper [Hun10] was the first open source consensus system to gain traction in the industry because it was easy to use, even with applications that weren’t designed to use distributed consensus. The Chubby service fills a similar niche at Google. Its authors point out [Bur06] that providing consensus primitives as a service rather than as libraries that engineers build into their applications frees application main‐tainers of having to deploy their systems in a way compatible with a highly available consensus service (running the right number of replicas, dealing with group mem‐bership, dealing with performance, etc.).Reliable Replicated State Machines
A replicated state machine (RSM) is a system that executes the same set of operations, in the same order, on several processes. RSMs are the fundamental building block of useful distributed systems components and services such as data or configuration storage, locking, and leader election (described in more detail later).The operations on an RSM are ordered globally through a consensus algorithm. This is a powerful concept: several papers ([Agu10], [Kir08], [Sch90]) show that any deter‐
System Architecture Patterns for Distributed Consensus  |  291
ministic program can be implemented as a highly available replicated service by being implemented as an RSM.As shown in Figure 23-2, replicated state machines are a system implemented at a logical layer above the consensus algorithm. The consensus algorithm deals with agreement on the sequence of operations, and the RSM executes the operations in that order. Because not every member of the consensus group is necessarily a mem‐ber of each consensus quorum, RSMs may need to synchronize state from peers. As described by Kirsch and Amir [Kir08], you can use a sliding-window protocol to rec‐oncile state between peer processes in an RSM.Figure 23-2. The relationship between consensus algorithms and replicated state machines
Reliable Replicated Datastores and Configuration StoresReliable replicated datastores are an application of replicated state machines. Replica‐ted datastores use consensus algorithms in the critical path of their work. Thus, per‐formance, throughput, and the ability to scale are very important in this type of design. As with datastores built with other underlying technologies, consensus-based datastores can provide a variety of consistency semantics for read operations, which make a huge difference to how the datastore scales. These trade-offs are discussed in“Distributed Consensus Performance” on page 296.Other (nondistributed-consensus–based) systems often simply rely on timestamps to provide bounds on the age of data being returned. Timestamps are highly problem‐atic in distributed systems because it’s impossible to guarantee that clocks are synchronized across multiple machines. Spanner [Cor12] addresses this problem by modeling the worst-case uncertainty involved and slowing down processing where necessary to resolve that uncertainty.Highly Available Processing Using Leader Election
Leader election in distributed systems is an equivalent problem to distributed consen‐sus. Replicated services that use a single leader to perform some specific type of work
292  |  Chapter 23: Managing Critical State: Distributed Consensus for Reliability
in the system are very common; the single leader mechanism is a way of ensuring mutual exclusion at a coarse level.This type of design is appropriate where the work of the service leader can be per‐formed by one process or is sharded. System designers can construct a highly avail‐able service by writing it as though it was a simple program, replicating that process and using leader election to ensure that only one leader is working at any point in time (as shown in Figure 23-3). Often the work of the leader is that of coordinating some pool of workers in the system. This pattern was used in GFS [Ghe03] (which has been replaced by Colossus) and the Bigtable key-value store [Cha06].Figure 23-3. Highly available system using a replicated service for master election
In this type of component, unlike the replicated datastore, the consensus algorithm is not in the critical path of the main work the system is doing, so throughput is usually not a major concern.
Distributed Coordination and Locking ServicesA barrier in a distributed computation is a primitive that blocks a group of processes from proceeding until some condition is met (for example, until all parts of one phase of a computation are completed). Use of a barrier effectively splits a distributed com‐putation into logical phases. For instance, as shown in Figure 23-4, a barrier could be used in implementing the MapReduce [Dea04] model to ensure that the entire Map phase is completed before the Reduce part of the computation proceeds.System Architecture Patterns for Distributed Consensus  |  293
Figure 23-4. Barriers for process coordination in the MapReduce computation
The barrier could be implemented by a single coordinator process, but this imple‐mentation adds a single point of failure that is usually unacceptable. The barrier can also be implemented as an RSM. The Zookeeper consensus service can implement the barrier pattern: see [Hun10] and [Zoo14].Locks are another useful coordination primitive that can be implemented as an RSM. Consider a distributed system in which worker processes atomically consume some input files and write results. Distributed locks can be used to prevent multiple work‐ers from processing the same input file. In practice, it is essential to use renewable leases with timeouts instead of indefinite locks, because doing so prevents locks from being held indefinitely by processes that crash. Distributed locking is beyond the scope of this chapter, but bear in mind that distributed locks are a low-level systems primitive that should be used with care. Most applications should use a higher-level system that provides distributed transactions.Reliable Distributed Queuing and Messaging
Queues are a common data structure, often used as a way to distribute tasks between a number of worker processes.Queuing-based systems can tolerate failure and loss of worker nodes relatively easily. However, the system must ensure that claimed tasks are successfully processed. For that purpose, a lease system (discussed earlier in regard to locks) is recommended instead of an outright removal from the queue. The downside of queuing-based sys‐tems is that loss of the queue prevents the entire system from operating. Implement‐ing the queue as an RSM can minimize the risk, and make the entire system far more robust.294  |  Chapter 23: Managing Critical State: Distributed Consensus for Reliability
Atomic broadcast is a distributed systems primitive in which messages are received reliably and in the same order by all participants. This is an incredibly powerful dis‐tributed systems concept and very useful in designing practical systems. A huge num‐ber of publish-subscribe messaging infrastructures exist for the use of system designers, although not all of them provide atomic guarantees. Chandra and Toueg [Cha96] demonstrate the equivalence of atomic broadcast and consensus.The queuing-as-work-distribution pattern, which uses the queue as a load balancing device, as shown in Figure 23-5, can be considered to be point-to-point messaging. Messaging systems usually also implement a publish-subscribe queue, where mes‐sages may be consumed by many clients that subscribe to a channel or topic. In this one-to-many case, the messages on the queue are stored as a persistent ordered list. Publish-subscribe systems can be used for many types of applications that require cli‐ents to subscribe to receive notifications of some type of event. Publish-subscribe sys‐tems can also be used to implement coherent distributed caches.Figure 23-5. A queue-oriented work distribution system using a reliable consensus-based queuing component