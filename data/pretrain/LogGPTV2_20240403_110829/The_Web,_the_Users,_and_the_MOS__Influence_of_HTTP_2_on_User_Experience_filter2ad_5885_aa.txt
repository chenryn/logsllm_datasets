# Title: The Web, the Users, and the MOS: Influence of HTTP/2 on User Experience

# Authors:
- Enrico Bocchi¹
- Luca De Cicco²
- Marco Mellia³
- Dario Rossi⁴

¹ Télécom ParisTech, Paris, France
² Politecnico di Bari, Bari, Italy
³ Politecnico di Torino, Turin, Italy
⁴ École Nationale Supérieure des Télécommunications, Paris, France

## Abstract
This study evaluates the quality of web experience as perceived by actual users, with a particular focus on the impact of HTTP/1 versus HTTP/2. We employ an experimental methodology that uses real web pages served through a realistic testbed, where we control network, protocol, and application configurations. Users are asked to browse these pages and provide subjective feedback, which is used to calculate the Mean Opinion Score (MOS), while the testbed records objective metrics.

The collected dataset includes over 4,000 user ratings, which we analyze to determine whether HTTP/2 improves user experience, the extent of such improvements, and under what conditions. Our findings indicate that users report only marginal differences, with 22%, 52%, and 26% of HTTP/2 MOS scores being better, identical, or worse than HTTP/1, respectively. Even in scenarios favoring HTTP/2, the results are not as pronounced as expected. This contrasts with objective metrics, which show a positive impact from using HTTP/2. This highlights the complexity of understanding web experience and the necessity of involving actual users in the quality assessment process.

**Keywords:** Web, HTTP/2, Page Load Time, MOS, User Experience, QoE

## 1. Introduction
The Web remains central to our daily lives, providing a wide range of online services, from search engines to business applications, personal communications, social networks, and entertainment portals. HTTP has been the de facto "thin waist" of the Internet, remaining largely unchanged since its initial definition. Recently, new protocols like HTTP/2, SPDY, and QUIC have been proposed, potentially altering the Web's status quo. As these new protocols are deployed, it becomes crucial to have reliable methods for comparing their performance benefits. However, measuring the Quality of Experience (QoE) for web users is challenging. Modern web pages are complex, often consisting of hundreds of objects hosted on different servers, with browsers opening multiple connections to fetch them. While latency is known to be important [16,18], its exact impact on QoE is less clear.

Objective metrics, such as Page Load Time (PLT), are commonly used for comparison [8,15,21–23], but they do not fully capture the user's experience during the complex "waterfall" of network and browser events. Subjective metrics, like the Mean Opinion Score (MOS), can measure actual user QoE, but conducting MOS measurement campaigns is expensive. Proposed approaches to estimate QoE [6,9] need further validation and are computationally complex.

Recognizing the limitations of objective metrics [5], this study presents the first comprehensive MOS measurement of WebQoE. We developed a methodology to collect volunteer feedback in a controlled environment, where users access real pages while we control network, protocol, and application configurations. Our analysis of over 4,000 samples of subjective feedback, augmented with objective metrics, reveals that HTTP/2's advantages are less pronounced than expected. In more than half of the cases, users reported no difference, and HTTP/2 improved QoE in only 22% of cases.

## 2. Related Work
Since the original SPDY proposal [11], which led to the standardization of HTTP/2 [3], and the introduction of QUIC [10], researchers have increasingly focused on benchmarking and optimizing these protocols [4,7,8,15,17,21–23]. Here, we compare our work with related studies in terms of experiment scale, testbed setup, page selection, and collected metrics.

### Experiment Scale
Studies collecting objective metrics range from several thousand (active testbeds [7,8,17,21,22]) to millions (crawling [15] and server logs [23]). Conversely, studies using actual user feedback (e.g., [4]) are inherently smaller, typically involving tens of participants. Our study collects feedback from 147 participants, resulting in over 4,000 experiments.

### Testbeds
Testbed setups vary, with some using proxies [7,8] and others, like ours, using locally controlled servers and networks [17,21,22]. A few studies leverage actual H2 servers on the Internet [15] or large corporate server logs [23]. Google Chrome is the most popular browser, followed by custom client implementations [21] or a mix of clients [23]. Network setups include both controlled [17,21,22] and uncontrolled [7,8,15] environments, including 3G/4G access.

### Page Catalog
Alexa rankings are a common source for selecting websites. The number of sites ranges from 20 to 500, with varying page selection criteria (e.g., landing [7] vs. non-landing [8]). We use Alexa to select popular French websites, similar to [8], and exclude landing pages. Our catalog includes 24 diverse pages, plus a toy page, http://www.httpvshttps.com, for a total of 25 pages. Figure 2 shows the size, number of objects, domains, and RTT for each page, illustrating a range of scenarios.

### Measured Metrics
Many studies use PLT as the primary objective metric [7,8,15,21–23], despite its known limitations [6,9]. Some studies include more refined metrics, such as SpeedIndex [5,17]. MOS models for web traffic date back to 2002 and 2005, and need re-assessment with modern technologies. Involving end-users in subjective measurements is best practice, with MOS being a simple and representative metric. To our knowledge, [4] is the only other study that collects volunteers' feedback on pre-recorded videos of web-browsing sessions. Our approach involves actual browsing sessions, using the typical MOS scale [13].

## 3. Methodology
Our methodology for comparing HTTP/1 and HTTP/2 consists of four phases:

### 3.1 Page Catalog
To build a realistic benchmark, we fetch actual pages and characterize network paths to servers. We start with the top 100 sites in the Alexa France ranking, visit each page with Google Chrome, and compile a list of requested URLs. Each object is mirrored on a local server, and we measure the RTT to each original domain using TCP-SYN packets. We manually check each mirrored page to discard incomplete or landing pages, resulting in 24 diverse pages, plus a toy page, for a total of 25 pages. Figure 2 illustrates the characteristics of our page catalog.

### 3.2 Testbed Engineering
#### Server and Network Configuration
We designed a local testbed with full control over network conditions (RTT, loss), protocols (HTTP/1, HTTP/2), and content placement (domain sharding [12]). The testbed consists of six servers, each with a quad-core processor, 4 GB of memory, and two Gigabit network cards. Servers run Ubuntu 14.04 with Apache HTTP Server 2.4.18, configured to serve content through virtual hosts. We use Linux traffic control (tc) to enforce network latency and packet loss. Content is distributed to each server, preserving the original domain structure, and mapped to static IP addresses in the 10.0.0.0/8 private range. Two virtual hosts serve content using either HTTP/1 or HTTP/2 to avoid protocol switching on the client side.

#### Client Instrumentation
Each volunteer uses a pre-configured PC running Linux Mint 17.3, equipped with scripts for experiment orchestration. These scripts (i) set up the local client, (ii) run Google Chrome to load a page, (iii) collect the user's score and objective metrics, and (iv) send the results to a central repository. Users select a page, observe it loading, input the MOS grade, and then watch the same page loaded with the other protocol. The order of protocols is randomized, and users are unaware of the protocol used. The script configures the system /etc/hosts file to direct browser requests to local servers, starts Chrome in incognito mode, and logs network events in HAR files for later analysis.