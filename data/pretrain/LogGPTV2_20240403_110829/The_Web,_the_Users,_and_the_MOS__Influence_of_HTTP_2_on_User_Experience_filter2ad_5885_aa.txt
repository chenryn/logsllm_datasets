title:The Web, the Users, and the MOS: Influence of HTTP/2 on User Experience
author:Enrico Bocchi and
Luca De Cicco and
Marco Mellia and
Dario Rossi
The Web, the Users, and the MOS: Inﬂuence
of HTTP/2 on User Experience
Enrico Bocchi1(B), Luca De Cicco2, Marco Mellia3, and Dario Rossi4
1 T´el´ecom ParisTech, Paris, France
PI:EMAIL
2 Politecnico di Bari, Bari, Italy
PI:EMAIL
3 Politecnico di Torino, Torino, Italy
PI:EMAIL
4 Ecole Nationale Sup´erieure des T´el´ecommunications, Paris, France
PI:EMAIL
Abstract. This work focuses on the evaluation of Web quality of expe-
rience as perceived by actual users and in particular on the impact of
HTTP/1 vs HTTP/2. We adopt an experimental methodology that uses
real web pages served through a realistic testbed where we control net-
work, protocol, and application conﬁguration. Users are asked to browse
such pages and provide their subjective feedback, which we leverage to
obtain the Mean Opinion Score (MOS), while the testbed records objec-
tive metrics.
The collected dataset comprises over 4,000 grades that we explore to
tackle the question whether HTTP/2 improves users experience, to what
extent, and in which conditions. Findings show that users report mar-
ginal diﬀerences, with 22%, 52%, 26% of HTTP/2 MOS being better, iden-
tical, or worse than HTTP/1, respectively. Even in scenarios that favor
HTTP/2, results are not as sharp as expected. This is in contrast with
objective metrics, which instead record a positive impact with HTTP/2
usage. This shows the complexity of understanding the web experience
and the need to involve actual users in the quality assessment process.
Keywords: Web · HTTP/2 · Page Load Time · MOS · User
experience · QoE
1 Introduction
The Web keeps being at the center of our lives, thanks to a plethora of online
services, from web searches to business applications, from personal communica-
tions to social networks and entertainment portals. HTTP is the de facto “thin
waist” of the Internet [19], remaining almost unchanged from the original pro-
tocol deﬁned at the end of the last century. Only recently a number of new pro-
tocols, namely HTTP/2 [3], SPDY [11] and QUIC [10], have been proposed and
c(cid:2) Springer International Publishing AG 2017
M.A. Kaafar et al. (Eds.): PAM 2017, LNCS 10176, pp. 47–59, 2017.
DOI: 10.1007/978-3-319-54328-4 4
48
E. Bocchi et al.
are likely to change the Web status quo. Having reliable ways to compare per-
formance beneﬁts becomes crucial when massive deployments of new protocols
take place. However, measuring Web users’ Quality of Experience (WebQoE) is a
challenging problem. Page complexity has grown to include hundreds of objects
hosted on diﬀerent servers, with browsers opening tens of connections to fetch
them. While several studies pointed out the importance of latency [16,18] and
its relationship with business value1, it is less obvious how it impacts WebQoE.
Objective metrics have been deﬁned and the Page Load Time (PLT) is the
de-facto benchmark used for comparison [8,15,21–23], with the industry adopt-
ing it too (e.g., Alexa reports the quantiles of PLT). However, this family of met-
rics does not fully reﬂect users’ quality of experience in the complex “waterfall”
of network and browser events taking place during the page loading processes.
Subjective metrics, the Mean Opinion Score (MOS), allow one to measure the
actual user’s WebQoE, but it is extremely expensive to run MOS measurement
campaigns. As such, approaches to estimate WebQoE have been proposed [6,9],
but their relationship with actual users’ experience is yet to be proved and their
computational complexity makes them diﬃcult to use in practice.
Recognizing intrinsic limits of objective metrics [5], we present the ﬁrst study
of MOS measurement of WebQoE: We engineer a methodology to collect vol-
unteers’ feedbacks in a controlled environment where users are asked to access
actual pages while we control network, protocol, and application setup. In our
eﬀort towards a subjective, yet scientiﬁc, comparison of HTTP/1.1 (H1) and
HTTP/2 (H2), we (i) collect a dataset of over 4,000 samples of subjective feed-
back augmented with objective metrics, and (ii) dig into the data to shed light
on actual experience improvement when using H2 vs H1. Advantages appear to
be less sharp than those shown by objective metrics: Users report no diﬀerences
in over half of the cases, while H2 improves WebQoE in 22% of cases only.
2 Related Work
Since the original SPDY proposal [11], ended with the standardization in H2 [3],
and the appearance of QUIC [10], researchers have been devoting increasing
attention to the benchmarking and optimization of these protocols [4,7,8,15,
17,21–23]. In what follows, we contrast our investigation with related works
considering experiment scale, testbed setup, set of pages, and collected metrics.
Experiments scale. In terms of experiments scale, works collecting objective
metrics span from several thousands (active testbeds [7,8,17,21,22]) to several
millions points (crawling [15] and server logs [23]). Conversely, studies employing
actual user feedback (only [4] besides this paper) are inherently of smaller scale
(i.e., tens of participants). Our work is based on the collection of actual user
feedback from 147 participants, for a total of over 4,000 experiments.
Testbeds. Testbed setups are either based on proxies [7,8] or, as in this work,
on locally controlled servers and networks [17,21,22]. Few works leverage actual
1 http://www.fastcompany.com/1825005/how-one-second-could-cost-amazon-16-
billion-sales.
The Web, the Users, and the MOS
49
Fig. 1. Experimental workﬂow.
H2 servers in the Internet [15] or large corporate server logs [23]. Google Chrome
is the most popular web browser followed by custom client implementations [21],
or a mixture of clients [23]. As for network setup, both controlled [17,21,22] and
uncontrolled [7,8,15] networks can be found, including 3G/4G access.
Page catalog. For what concerns pages used for testing, Alexa ranking is a
popular source for the selection of websites. The number of sites ranges from 20
to 500, and page selection criterion (e.g., landing [7] vs non-landing [8]) diﬀers.
We use Alexa as well to drive our choice towards popular websites. As in [8], we
select pages that are likely known by our users, i.e., pages popular in France.
We consider pages optimized for desktop browsing and discard landing pages.
Measured metrics. Many works adopt the Page Load Time (PLT) as objective
metric [7,8,15,21–23]. PLT limitations are well-known [6,9], yet only few works
include more reﬁned metrics to describe users’ QoE, e.g., [5,17] consider the
SpeedIndex [9]. MOS models for web traﬃc are dated back to 2002 and 2005
and therefore they should be re-assessed under recent architectures, technologies
and designs. Involving end-users in subjective measurements is the best practice,
with MOS being a simple and compact metric representative of their actual
experience. MOS is the standard in audio and video quality comparison, but
only recently it has been introduced for WebQoE assessment. To the best of our
knowledge, only [4] presents a framework to collect volunteers’ feedback on pre-
recorded videos of web-browsing sessions: Side-to-side videos are shown, with
the aim of identifying a winner. In contrast, we collect volunteers’ feedback of
actual browsing sessions, using the typical [1, 5] MOS scale [13]. Both approaches
have challenges: e.g., synchronization between videos, correlation between videos
and actual browsing experience, ability to slow-down/pause video can aﬀect
results in [4]. Conversely, in our work the analysis is made complex by volunteers
tendency to refraining from using the full scale of scores, as we shall see.
3 Methodology
As portrayed in Fig. 1, the methodology we employ to compare H1 and H2 con-
sists of four phases: 1. Page catalog (Sect. 3.1) – To build a realistic benchmark,
50
E. Bocchi et al.
e
z
i
S
]
B
M
[
s
t
c
e
j
b
O
 8
 6
 4
 2
 0
 400
 300
 200
 100
 0
Synthetic
Realistic
s
n
i
a
m
o
D
 45
 30
 15
 0
T
T
R
]
s
m
[
 100
 10
 1
Web pages
Web pages
Fig. 2. Page catalog characteristics.
we fetch actual pages and characterize network paths towards servers. 2. Testbed
engineering (Sect. 3.2) – Pages and paths metadata are used to set up our test-
bed. Local servers host objects using multiple Apache instances while we control
network (RTT, loss), protocol (H1/H2), and application (domain sharding) con-
ﬁguration. 3. MOS collection (Sect. 3.3) – Volunteers browse pages served by our
local infrastructure and provide a score in the range [1, 5]. At the same time, the
testbed captures objective metrics. 4. Analysis (Sects. 4–6) – At a later stage,
we apply analytics to contrast H1 vs H2 performance.
3.1 Page Catalog
For collecting MOS grades, we aim at selecting pages users are familiar with. As
our tests take place in Paris, we start from the top 100 in Alexa France ranking.
We visit each page using Google Chrome and compile a list of URLs of objects
being requested by the browser. We then mirror each object on a local server
and measure the RTT towards each original domain using TCP-SYN packets.
We manually check each mirrored page from our local servers to both dis-
card incomplete pages (e.g., object failing to download due to dynamic requests
or cookies policies), landing pages [8] (e.g., Facebook login page), etc. We are
left with 24 real pages covering a variety of categories, e.g., news, e-commerce,
informative websites, leisure etc. At last, we add the toy page http://www.
httpvshttps.com to the page catalog, for a total of 25 pages. For each considered
page, Fig. 2 reports its size (top-left), the number of objects (bottom-left), the
number of domains serving such objects (top-right), and the average per-domain
RTT to contacted domains, with bars reporting the minimum and the maximum
RTT (bottom-right). The ﬁgure shows that our catalog includes diverse scenar-
ios, from pages hosted on few domains serving a handful of objects, to pages
hosted on tens of domains and made of hundreds of objects.
3.2 Testbed Engineering
Server and network conﬁguration. We design and setup a local testbed
where we have full control on network conditions (RTT, loss), protocols (H1/H2),
The Web, the Users, and the MOS
51
and content placement (domain sharding [12]). Our testbed is composed of six
servers, each equipped with a quad-core processor, 4 GB of memory and two
Gigabit network cards. Servers run Ubuntu 14.04 with Apache HTTP Server
2.4.18. Apache runs in its default conﬁguration, with H2 and SSL modules
enabled. Content is served using SSL by installing self-signed certiﬁcates.
We run multiple Apache instances conﬁgured to serve content through vir-
tual hosts, which are both name-based and IP-based. We leverage name-based
conﬁguration to distinguish requests directed to diﬀerent domains being hosted
on the same machine, while the IP-based distinction is required to have domains
mapped to speciﬁc network conditions. To control network conditions, we use
Linux traﬃc control utility (tc) to enforce both network latency and packet loss.
We next distribute content to each server, preserving the original placement of
objects into domains, and map each domain to a static IP address using the
10.0.0.0/8 private range. Two separate virtual-hosts serve content using either
H1 or H2 to avoid protocol switching or fall-backs on the client side. The choice
of H1/H2 is performed by the client, which directs requests to the IP address of
the server implementing the desired protocol.
Client instrumentation. We provide a preconﬁgured PC to each volunteer
taking part in our campaign. Each PC runs Linux Mint 17.3 and is equipped
with a set of scripts for experiment orchestration. In particular, such scripts
(i) setup the local client to reﬂect the desired scenario, (ii) run Google Chrome
to let the volunteer visit a page, (iii) collect the user’s score and the objective
measurement, and (iv) send the results to a central repository.
Each experiment requires several steps to complete. From the users’ point
of view, the experience starts with a GUI listing all the available websites of
the page catalog. Volunteers (i) select a page from the list and (ii) observe it
being loaded by Google Chrome. At the end, they (iii) input the MOS grade,
and then (iv) watch again the same page, now served with the other protocol.
At step (ii) the page is loaded using either H1 or H2 in a random fashion, then
at step (iv) the complementary protocol is used. Therefore, users sequentially
grade the same page under the same condition and for both protocols, although
they are unaware about the protocol order.
From the implementation standpoint, once the volunteer has selected a page,
the script (i) conﬁgures the system /etc/hosts ﬁle to direct browser requests to
local servers instead of the public Internet.2 Two hosts ﬁles are provided for each
web page, one for H1 servers, the other for H2 servers. Next, the script (ii) starts
Google Chrome in full screen mode, disabling the local cache and enabling the
incognito mode. This ensures each page is loaded independently on previous
tests and eventual cookies. We force Chrome to log network events, which we
collect in the form of HTTP Archive (HAR) ﬁle for later stage analysis. Once
the user has provided the (iii) MOS grade, (iv) all metadata for that experiment
(i.e., HAR ﬁle, user’s grade, and metadata with network conﬁguration, etc.) are