For network exception handlers to be an efﬁcient management
mechanism, we require that they be simple yet ﬂexible enough to
capture a wide range of useful policies. Analogous to program
exception handlers, a network exception handler comprises two
parts: (i) the exception condition which triggers the handler, and
(ii) an action which deﬁnes behavior to impose when the condition
is met. Fig. 1 shows a simpliﬁed example of a network exception
handler. In particular, the network exception handler is composed
of three basic functions: Exception(), which embodies the condi-
tion that must hold for the exception to occur, Fire() which is called
when the exception is ﬁrst detected, and Handler() which is called
during the period that the exception holds. The latter two func-
tions together comprise the action. Information about the network
is exposed in a single data structure, called NetworkState, which
includes information about the network topology, link loads, and
other information against which the network administrator writes
exception conditions. Host information is exposed in a separate
data structure called HostState, containing details such as the con-
nections opened by each application, their network usage and the
user running each application. In later sections, we describe how
the information in these two data structures is maintained.
Conceptually, an exception handler is in one of two states: pas-
sive or active. While passive, Exception() is evaluated each time
the NetworkState is modiﬁed. If true, the state becomes active and
Fire() is called. When active, Handler() is invoked each time the
NetworkState or HostState is modiﬁed. Handler() is required to
explicitly deregister itself when the exception ceases to hold, plac-
ing the exception handler back in the passive state.
Exception() uses simple predicates involving measurable quan-
tities of the network, e.g., link loads, router interface counts, etc.
Fig. 1 is a simpliﬁed example demonstrating the general function-
ality network exception handlers expose. The exception becomes
true when either of two backbone links are not present. When this
occurs, Fire() displays a warning alerting the user and registers the
Handler() function. Note that it is possible to register multiple,
possibly different, Handler() functions. The registered Handler()
function is then invoked each time the NetworkState or HostState
is modiﬁed. In this example, Handler() checks if the exception still
holds, and if the exception does hold, then Handler() enforces a set
of policies at each host based on the role of the host, its users and
its running applications. If the exception does not hold, Handler()
removes all restrictions created by this speciﬁc exception handler,
notiﬁes the user that the link is restored and deregisters itself. This
operation offers an effective mechanism for communicating to the
user that network problems have been resolved, an operation which
currently requires either explicit notiﬁcation from a network admin-
istrator, or potentially user initiated actions (e.g., probing).
Locality and cascading events. We impose two important con-
straints on the potential actions of a network exception handler:
they should not impose non-local behavior and they should not re-
quire explicit coordination between hosts. In many circumstances,
it may be attractive to alter trafﬁc patterns globally, for example
by having hosts use loose source routing to reroute trafﬁc when
certain exceptions occur, such as a congested link. However, shift-
ing trafﬁc around the network by rerouting can lead to unexpected
and highly undesirable persistent load oscillations, causing the trig-
gering of further, potentially cascading, exceptions. Mitigating
such effects would require complex real-time coordination between
Handler() functions running on different hosts. Such policies would
effectively increase the burden of management by introducing fur-
ther complexity in both expressing and implementing such excep-
tion handlers.
Therefore, we want to ensure that exception handlers can only
perform local operations: a handler should only be able to perform
actions that shape locally generated trafﬁc4, or provide feedback to
the user. We will show in the next section that this practice still
enables signiﬁcant beneﬁt and control over the network.
Time-scales of operation. Network exception handlers are not
suitable for tracking very short-lived or bursty phenomena, such as
instantaneous peaks on the load of a link. The time-scale of opera-
tion depends on the network exception handler under consideration,
4Note that shaping trafﬁc locally can still have remote beneﬁts,
e.g., by mitigating congestion in the core of the network or at other
remote sites, without leading to cascading events.
but in general handlers target exceptions that would signiﬁcantly al-
ter the network state and affect the user-perceived application per-
formance.
It seems that it could be possible to build tools to perform static
analysis of exception handlers, which might allow them to perform
some, perhaps limited, non-local operations, but we leave this as an
open question.
4. USES OF EXCEPTION HANDLERS
In this section, we present three sample usage scenarios for net-
work exception handlers: (i) efﬁcient response to link failures; (ii)
application-aware trafﬁc engineering; and (iii) non-transient con-
gestion response. A combination of existing tools might offer po-
tential solutions for some of these examples. However, network
exception handlers provide a ﬂexible framework that facilitates efﬁ-
cient and straightforward management for many scenarios through
the same mechanism. For example, while an ICMP link failure
may inform of an unreachable host or link (scenario (i)), such a
mechanism cannot provide information for a link overload (sce-
nario (iii)). Compared to existing tools, network exception handlers
also largely automate the process of notifying the user of potential
problems in the network and determining whether these problems
persist, as information on the various exceptions is readily available
at the host.
Efﬁcient response to link failures. Network exception handlers
allow detailed control of the network’s response to failures. For ex-
ample, consider an exception handler such as the following: when
links in the set {Li} fail, only applications in the set {Aj} may
transmit packets that will be routed across links in the set {Lk},
i.e., when critical links fail, only a subset of applications may send
trafﬁc over the speciﬁed backup links. This ensures that critical
trafﬁc traversing these backup links does not experience conges-
tion during what is already a troubled time for the network. This
policy could be extended by restricting the set of hosts allowed to
transmit. In contrast, current enterprise networks can only enforce
inaccurate or coarse-grained control in response to link failures,
such as applying ﬁlters to drop trafﬁc using certain port numbers
being transmitted or received by certain hosts, leaving hosts essen-
tially unaware of the failures and thus unable to adapt their trafﬁc
demands appropriately. Effectively, pushing topology information
to hosts enables accurate control of application trafﬁc dependent
on the failure, at timescales that routing protocols cannot achieve.
A key question underlying the utility of such a policy is how
frequent are link failures in enterprise networks? Fig. 2 presents
an initial analysis of the frequency of OSPF link failures, i.e., loss
of OSPF adjacency, using a dataset collected from our global enter-
prise network. The dataset is described in detail in Section 7.1. The
ﬁgure presents two dimensions of such failures (from left to right):
Cumulative Distribution Functions (CDFs) of link downtime, and
failure inter-arrivals per link, from the two different perspectives of
our local stub area, and the regional backbone.
We observe that failures occur quite frequently in both cases
(e.g., one failure per link every hour for 30% of the links in the stub
area or for 60% of the links in the backbone area) but downtimes
can also be signiﬁcant: 20% of all failures last over 10 minutes.
Link failures are triggered by the absence of OSPF trafﬁc on the
adjacency for 40 seconds [13]. In the ﬁgures, we consider failures
that last over 30 seconds (i.e., the failed link was not advertised as
active within 30 seconds after its withdrawal).
In the previous section, we presented an example of an excep-
tion handler that could be triggered in such failure cases (Fig. 1).
Performing similar actions in the network to those in Fig. 1 is cur-
rently not possible, as not only is per-application packet shaping
Figure 2: OSPF failure rates. From left to right: CDF of link
downtimes, and the CDF of failure inter-arrivals per link. The
median for failure inter-arrival per link is roughly a day for the
stub area, while link downtimes lasting more than 10 minutes
account for more than 20% of all failures.
not effective, but it also requires further information about the role
of the host and the user of the application. Fig. 3 shows an ex-
ception handler triggered in the case of a failure of a building link.
The exception handler proactively reports a message on each host
within the building when the link fails, and each such host drops
all trafﬁc that would have traversed the link, effectively not gener-
ating trafﬁc that would be dropped later in the network. When the
link is restored, the exception handler reports this to the user. Sim-
ilarly, all trafﬁc to the disconnected subnets is dropped for hosts
not in the building, and the users attempting to access resources in
those subnets are informed. Only hosts that communicate over the
speciﬁed link will be informed about the outage. Network excep-
tion handlers provide two advantages here: (i) trafﬁc that would
be dropped later in the network is not generated at the hosts, and
(ii) users are informed during periods of disconnection, which is
currently a nontrivial task. For example, in our enterprise, the stan-
dard practice would be to send an email to the affected users to in-
form them about the building link failure. However, since the email
servers reside in our European Data Center which becomes inacces-
sible during such failures, the users are actually only informed once
the link is again active!
Application-aware trafﬁc engineering. Network exception han-
dlers allow for efﬁcient, detailed policies with regards to trafﬁc en-
gineering. For example, a policy for such a case might be as fol-
lows: when trafﬁc on link L reaches a predetermined threshold, T ,
non-critical applications in the set {Ai} running on hosts in the set
{Hj} should be rate limited. T here might be deﬁned using the real
economic cost of the link, e.g., the 95th percentile should always be
less than T Mbps. In Section 7, we study the feasibility and effec-
tiveness of a network exception handler that applies such a policy.
As previously noted, even highly controlled networks such as en-
terprise networks currently have no means of explicitly engineering
their trafﬁc with respect to applications. Several factors, from use
of arbitrary port mappings at proxies to arbitrary application port
usage and layer-3 or layer-4 encryption, prohibit ﬁne-grained ap-
plication control in practice. The net effect is that trafﬁc can only
be shaped accurately on a per-application basis at the host. Enter-
prise networks explicitly do not want net neutrality, and this desire
can be satisﬁed through network exception handlers.
Fig. 4 shows an example of a simple policy that, during ofﬁce
hours, limits the bandwidth used by a particular application, in this
Network Exception Handler: Building link failed
links = { [ 10.39.11.40, 10.39.12.30, “Edinburgh to London”] }
Edinburgh_subnets = [ 10.39.18.0/24, 10.39.19.0/24 ]
boolean Exception(NetworkState)
begin
foreach link in links do
if link not in NetworkState then return true
return false
end
void Fire (NetworkState, HostState)
begin
if HostState.MachineLocation is “Edinburgh” then
print “Building disconnected: do not panic”
Register(Handler)
end
void Handler (NetworkState, HostState)
begin
if not Exception(NetworkState) then
RemoveAllRestrictions(); DeRegister(Handler);
if HostState.MachineLocation is “Edinburgh” then
print “Network connectivity restored”
return
SetRestriction(TrafﬁcSrc() not in Edinburgh_subnets
and TrafﬁcDest() in Edinburgh_subnets,
DropAndReport(TrafﬁcDest()+“ disconnected”))
SetRestriction(TrafﬁcSrc() in Edinburgh_subnets
and TrafﬁcDest() not in Edinburgh_subnets,
Drop());
end
Figure 3: Network exception handler for a building disconnec-
tion.
case the UpdateAndPatch client. The handler would thus be active
during the speciﬁed hours. Applying such a policy in the network
would be feasible only if the application used a predetermined port
number. Since many update applications run over HTTP on port 80,
simple packet shaping would be insufﬁcient as information about
the source application is required.
Congestion response. In a related scenario, network exception
handlers may allow hosts to directly alleviate congestion in the net-
work. Currently, congestion avoidance is left to transport protocols
such as TCP which operate over short timescales, e.g., loss of indi-
vidual packets. Hosts operating such protocols must react in certain
ways when congestion is observed: simplistically put, a host should
linearly increase its network usage until it observes loss, when it
should multiplicatively back-off so that all hosts sharing a path can
obtain an approximately equal share of that path’s resources.
While certainly necessary in the context of IP networks for main-
taining the performance of the network under times of high load,
this behavior is quite limiting and may not even be desirable in an
enterprise network. It is based on a very limited signal from the net-
work (observed packet loss) and permits only one, quite simplistic,
response (decrease of the window of outstanding data on the ﬂow
observing congestion). By specifying policies with respect to link
utilization, network exception handlers allow hosts to manage the
usage of network resources more smoothly, over longer time peri-
ods, and even pro-actively, i.e., before congestion is directly expe-
rienced as with ECN [15].
For example, a policy may specify that a certain host’s ﬂows
should be rate-limited when speciﬁc links are loaded to a prede-
ﬁned threshold, e.g., 80% utilization. In this scenario, a notiﬁcation
of impending congestion is provided before an actual loss event.
Network Exception Handler: Time-based trafﬁc cap
links = { [ 10.39.15.60, 10.39.15.61, “Paris to Amsterdam” ] }
boolean Exception(NetworkState)
begin
return NetworkState.LocalTimeNow in range 8AM to 5PM
end
void Fire (NetworkState, HostState)
begin
Register(Handler)
end
void Handler (NetworkState, HostState)
begin
if not Exception(NetworkState) then
RemoveAllRestrictions(); DeRegister(Handler); return
SetRestriction(“UpdateAndPatch”, maxBandwidth=50kbps)
end
Figure 4: Network exception handler to shape update trafﬁc dur-
ing local working hours.
Similarly, network exception handlers can prevent new ﬂows from
a host entering an overloaded link, a level of control unavailable
to TCP-like mechanisms. This scenario does not require coordi-
nation of hosts, and network exception handlers can work together
with ECN-type mechanisms, as they are orthogonal, operating on
different time-scales. Network exception handlers provide an early
and efﬁcient congestion avoidance mechanism unavailable through
current transport-layer protocols.
Summary. All these scenarios are examples of policies that rely
on per-application knowledge that cannot be easily, if at all, sup-
ported in today’s enterprise networks. Network exception handlers
exploit the fact that hosts have both the knowledge of application
requirements and the ability to apply per-application policies, en-
abling the handling of these exceptions. Network exception han-