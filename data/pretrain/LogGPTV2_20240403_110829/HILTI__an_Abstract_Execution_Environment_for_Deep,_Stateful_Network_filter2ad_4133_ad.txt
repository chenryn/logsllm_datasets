one would avoid this overhead by tying the two systems together
more directly.
6. EVALUATION
We now evaluate the HILTI environment by assessing the func-
tionality and performance of the applications discussed in §4. We
begin by summarizing our data and setup.
6.1 Data and Setup
To drive our applications with realistic workloads, we captured
two full-payload packet traces at the border gateway of the UC
Berkeley campus, exclusively containing HTTP and DNS trafﬁc,
respectively. The HTTP trace comprises a total of 30 GB of data
in libpcap format, spanning 52 minutes of TCP port 80 trafﬁc dur-
ing a recent weekday morning and representing a sample of 1/25 of
the link’s total connections on port 80 during that time.5 The trace
contains about 340K HTTP requests/replies between 35K distinct
pairs of hosts. The DNS trace comprises 1 GB in libpcap format,
spanning about 10 minutes of UDP port 53 trafﬁc during a recent
weekday afternoon. Due to the lower overall volume of DNS, we
could capture all of the campus’ DNS during that interval. The
trace contains about 65M DNS requests/replies between 435K dis-
tinct host pairs. We captured both traces with tcpdump, which re-
ported no packet loss in either case. We chose the trace durations
as a trade-off between including signiﬁcant diversity and keeping
their volume manageable for repeated ofﬂine measurements. We
note that in particular for DNS, the number of requests/replies con-
stitutes the primary driver for analysis performance, not raw packet
volume. We conduct all measurements on a 64-bit Linux 3.12.8
5We captured the trace on a backend system behind a front-end
load-balancer that splits up the total trafﬁc on a per-ﬂow basis [44].
468system with two Intel Xeon 5570 CPUs, 24GB of RAM, and CPU
frequency scaling disabled.
For the applications involving Bro we slightly adapt some of
Bro’s default scripts—which generate all of Bro’s output ﬁles—
by backporting a few recent bugﬁxes to the version we used. We
also remove some minor dependencies on speciﬁcs of the built-in
parsers. For DNS we furthermore disable per-request expiration
timers, and for Bro’s standard HTTP parser we disable recovering
from content gaps; both features are not yet supported by our Bin-
PAC++ versions. We use the modiﬁed Bro versions for all runs so
that results are comparable across conﬁgurations. When measur-
ing execution performance (in contrast to validating correct oper-
ation), we disable Bro’s logging to avoid its I/O load affecting the
results; Bro still performs the same computation but skips the ﬁ-
nal write operation. To measure CPU performance, we instrument
Bro to also record the time spent inside four different components
of the code base: protocol analysis, script execution, HILTI-to-Bro
glue code (see §5), and all remaining parts excluding initialization
and ﬁnalization code (which we refer to as “other” below).6 We
measure time in CPU cycles as retrieved via the PAPI library. Out
of necessity these measurements remain somewhat approximate:
(i) control ﬂow within Bro is complex and one cannot always pre-
cisely attribute which subsystem to charge; and (ii) PAPI comes
with overhead and ﬂuctuations that affect the results [46]. How-
ever, after performing a number of cross-checks, we are conﬁdent
that these measurements give us a solid qualitative understanding
on how HILTI performs relative to standard Bro.
6.2 Berkeley Packet Filter
We ﬁrst verify that running the compiled ﬁlter from Figure 4
on the HTTP trace performs correctly. Instead of the private ad-
dresses we use real ones from the trace that trigger the ﬁlter for
about 2% of the packets. We link a basic libpcap-based driver
program written in C with the compiled HILTI code. The driver
reads the trace ﬁle inside a loop, calls the ﬁlter successively for
each packet and increases a counter for every match. We then im-
plement a second version of the driver that instead uses BPF to
perform the same ﬁltering, and we ﬁnd that both applications in-
deed return the same number of matches. To understand the rela-
tive performance of the two ﬁlter implementations, we measure the
CPU cycles spend inside the ﬁltering code, ﬁnding that the HILTI
version spends 1.70 times more cycles than BPF. A closer looks
reveals that the HILTI-generated C stub is responsible for 20.6%
of the difference, leaving an increase of 1.35 times when ignor-
ing that (the runtime functionality that the stub facilitates remains
unnecessary in this case, and the compiler could indeed just skip
it). We deem the remaining difference acceptable, considering the
HILTI’s higher-level model in comparison to BPF’s internal repre-
sentation.
6.3 Stateful Firewall
We conﬁrm the functionality of the stateful ﬁrewall application
by comparing it with a simple Python script that implements the
same functionality independently. We drive both instances with
the DNS trace, conﬁguring them with a small example rule set and
feeding them with timestamp, source, and destination address for
each packet, as extracted by ipsumdump. Both HILTI and Python
versions parse the ipsumdump output into its components and
then pass them as input into their rule matching logic. We conﬁrm
that the HILTI version produces the same number of matches vs.
6This also excludes compiling HILTI code at startup. While that
can take noticeable time, one could cache the machine-code for
reuse on later invocations.
#Lines
Total
Normalized
Identical
http.log
Std
Pac
338K 338K 273K 273K 2573K 2573K
338K 338K 273K 273K 2492K 2492K
files.log
Std
Pac
dns.log
Std
Pac
>99.9%
98.91%
98.36%
Table 2: Agreement HILTI (Pac) vs. standard (Std) parsers.
non-matches. It also performs the task orders of magnitude faster;
however, given the slow nature of the Python interpreter, compar-
ing times does not reveal much here.
6.4 Protocol Parsing
We next examine BinPAC++ protocol parsers, using Bro as the
driver to feed them packet data. We take the HTTP and DNS
parsers as case-studies and compare their results with Bro’s stan-
dard, manually written C++ implementations.7 We conﬁrm cor-
rectness by running both BinPAC++ and standard parsers on the
corresponding input trace and comparing the relevant Bro log ﬁles.
For HTTP, http.log records all HTTP sessions along with ex-
tensive meta information such as requested URL, server response
code, and MIME types of the message; and files.log records
further information on the message bodies, including a SHA1 hash
of their content. For DNS, dns.log records all DNS requests
with queries, types, responses, and more.
Our BinPAC++ parsers attempt to mimic Bro’s standard parsers
as closely possible, however small discrepancies in analysis seman-
tics remain hard to avoid for protocols as complex as HTTP and
DNS. Before comparing the log ﬁles, we hence ﬁrst normalize them
to account for a number of minor expected differences, including
unique’ing them so that each entry appears only once.8
Table 2 summarizes the remaining differences. We ﬁnd that for
the normalized versions of http.log, 98.91% of all of the stan-
dard parser’s log entries correspond to an identical instance in the
BinPAC++ version; 98.36% for files.log. About half of the
HTTP mismatches are due to a small number of “Partial Content”
sessions, for which the BinPAC++ version often manages to ex-
tract more information. The remaining discrepancies reﬂect fur-
ther semantic differences, often leading to different (or no) MIME
types for an HTTP body. These mismatches then transfer over to
the files.log, and we ﬁnd neither parser consistently right in
these cases. The (low) number of mismatches remains on the order
of what we would expect for any two independent HTTP imple-
mentations. For dns.log we ﬁnd almost perfect agreement, with
>99.9% of the entries matching, and the remaining deviations be-
ing due to minor semantic differences (e.g., Bro’s parser extracts
only one entry from TXT records, BinPAC++ all; the BinPAC++
parser does not abort as easily for trafﬁc on port 53 that is not in
fact DNS). Overall, we conclude that the BinPAC++ results closely
match those of Bro’s standard parsers, which means we can pro-
ceed to meaningfully compare their performance, as they indeed
perform essentially the same work.
Figure 9 summarizes CPU times. The plot breaks down the time
spent inside Bro components when using its standard parsers vs.
the HILTI-based versions. The time for “Protocol Parsing” con-
7Note that we indeed compare against manually written C++ im-
plementations, not code generated by the classic BinPAC discussed
in [36]. While Bro uses BinPAC for some of its protocols, it does
not do so for HTTP and DNS.
8The normalization further includes adjustments for slight timing
and ordering differences, a few ﬁelds with size information that
the BinPAC++ parsers cannot easily pass on to Bro for technical
reasons, and formatting differences in ﬁeld content.
469#Lines
Total
Normalized
Identical
http.log
Std
Hlt
338K 338K 273K 273K 2573K 2573K
338K 338K 273K 273K 2492K 2492K
files.log
Std
Hlt
dns.log
Std
Hlt
>99.99%
99.98%
>99.99%
Table 3: Output of compiled scripts (Hlt) vs standard (Std).
Figure 9: Performance of HILTI-based protocol parsers.
stitutes the key metric:
the BinPAC++ parsers need 1.28 times
and 3.03 times more CPU cycles, respectively, when running on
the HTTP and DNS traces than the standard implementations. For
HTTP that means HILTI already comes close to the performance of
manually written C++ code. For DNS, the slowdown is more sig-
niﬁcant, though we argue still acceptable given the current proto-
type state of our compiler implementation with its potential for fur-
ther optimization and tuning. We proﬁled the DNS analysis in more
detail on a trace excerpt and found two particular opportunities for
improvement. First, BinPAC++ parsers perform more memory al-
locations, and the effect is particularly pronounced for DNS: when
using the BinPAC++ parser Bro performs about 47% more memory
allocations (19% more for HTTP). The increase comes from fre-
quent instantiation of dynamic objects during the parsing process—
likely a similar overhead as classic BinPAC exhibits as well, and
with similar solutions [41]. Second, currently the BinPAC++ com-
piler always generates code supporting incremental parsing, even
though it could optimize for UDP where one sees complete PDUs
at a time (as Bro’s standard parser indeed does).
Figure 9 also shows the time spent inside the HILTI-to-Bro data
conversion glue: 1.3%/6.9% of the total cycles, respectively—an
overhead that would disappear if HILTI were more tightly inte-
grated into the host application. Interestingly, we also see in Fig-
ure 9 that for HTTP, Bro spends less time in script execution when
using the BinPAC++ analyzer, even though it follows the same code
path. We tracked down this difference to the standard parser gen-
erating a large number of redundant events related to ﬁle analy-
sis, which the BinPAC++ parser omits (about 30% more in total).
While the impact of these events on parsing and output remains
negligible, they lead to more load on the script interpreter. Recent
Bro versions have ﬁxed this problem.
6.5 Bro Script Compiler
Turning to the script compiler application, we again ﬁrst verify
its correct operation. Using the same methodology as with the pro-
tocol parsers, we compare the log ﬁles that the compiled HILTI
versions of the HTTP and DNS scripts produce with the output of
Bro’s standard script interpreter. Table 3 summarizes the differ-
ences running on the corresponding traces. We see an excellent
ﬁt in all three cases. Inspecting the few cases where the two ver-
sions do not agree, we ﬁnd that for http.log and files.log
almost all differences are due to fully insigniﬁcant output ordering
when logging sets of values—which our normalization cannot eas-
ily account for. For dns.log the only differences come from an
IPv6 address logged in a slightly different output format. Overall,
we conclude that the compiled HILTI code produces the same out-
put as Bro’s standard interpreter, and we thus conﬁrm that HILTI’s
model can indeed capture a complex domain-speciﬁc language.
Figure 10: Performance of scripts compiled into HILTI.
Next we turn to execution performance. As a simple baseline
benchmark, we ﬁrst execute a small Bro script that computes Fi-
bonacci numbers recursively. The compiled HILTI version solves
this task orders of magnitude faster than Bro’s standard interpreter,
which demonstrates the potential for compiling scripts into ma-
chine code. However, this example represents the best case for
HILTI: it requires hardly any interaction between HILTI and Bro’s
C++ core, and it allows LLVM’s code generation to shine by pro-
ducing optimal machine code. As more realistic examples, Fig-
ure 10 compares execution performance with the HTTP and DNS
scripts, respectively. For HTTP, the HILTI version remains slightly
slower, requiring 1.30 times the cycles. For DNS, HILTI proves
6.9% faster. As with the protocol parsers, the glue code adds further
overhead—4.2% and 20.0% of total cycles, respectively—which a
fully integrated host application would not incur. Overall we con-
clude that the compiled HILTI scripts exhibit performance similar
to Bro’s existing scripting system. One could argue that compiled
scripts should decrease execution times more, compared to Bro’s
standard interpretation. While indeed we expect that further im-
provements of our toolchain will achieve that, we also point out
that it remains challenging to quantify the potential: with a high-
level scripting language, operations on data structures (e.g., hash
maps) and other runtime library functionality, including dynamic
memory management, account for a signiﬁcant share of the work
load that can only improve to a certain extent. Furthermore, even
when interpreted, Bro’s statically typed language can execute much
faster than dynamically typed environments such as Python or Perl,
which would be infeasible to use for real-time network trafﬁc anal-
ysis in the ﬁrst place.
6.6 Summary
We have shown that (i) HILTI correctly captures the semantics
of all four application scenarios, from low-level protocol parsing
to complex high-level analyses; and (ii) its runtime performance
generally aligns with that of the existing systems. While some-
times slower, it remains on the order of what we expect for the
current state of the implementation, i.e., a 4-stage compilation pro-
cess (host application, HILTI, LLVM, machine code) only partially
optimized (LLVM) versus manually written production code. In-
deed, our toolchain does not yet exploit HILTI’s optimization po-
tential: it lacks support for even the most basic compiler optimiza-
tions, such as constant folding and common subexpression elimi-
0.0B0.2B0.4B0.6B0.8B1.0B1.2B1.4B1.6B1.8BCPU cyclesStandardHILTIStandardHILTI1567G683G643G241G1580G852G450G21G258G712G177G356G180G1173G469G405G81G217GHTTPDNSProtocol ParsingScript ExecutionHILTI-to-Bro GlueOther0.0B0.5B1.0B1.5B2.0BCPU cyclesStandardHILTIStandardHILTI1562G683G635G244G1810G698G781G76G254G709G175G358G176G694G175G243G139G136GHTTPDNSProtocol ParsingScript ExecutionHILTI-to-Bro GlueOther470nation at the HILTI level (e.g., the LLVM-level lacks the semantics
to identify subsequent lookups for the same map element, which
however would be easy to compress before compiling them down).
We have not yet pursued parallelizing the presented applications
inside HILTI, as that would involve a number of further aspects
in terms of analysis structure and evaluation, exceeding the scope
of this paper. However, we have veriﬁed HILTI’s thread-safety
guarantees, as well as correct operation of the scheduler, by load-
balancing DNS trafﬁc across varying numbers of hardware threads,
each processing their input with the corresponding HILTI-based
parser. As expected, we found the same HILTI parsing code to
support both the threaded and non-threaded setups.
7. DISCUSSION
Safe Execution Environment. As a key design aspect, HILTI
provides a safe runtime execution environment that prevents unin-
tended data and control ﬂows when facing unexpected input. For
example, HILTI’s instructions generally validate their operands to
avoid undeﬁned behavior, such as out-of-bounds iterators; and the
memory management prevents dangling references. Furthermore,
by separating state between threads, HILTI provides a well-deﬁned
setting for concurrent analysis: HILTI code is always safe to exe-
cute in parallel. While some of these safety properties come with
performance penalties, we deem them well worth their cost, com-
pared to the traditional alternative of writing C code that parses
untrusted input, potentially concurrently, in real-time.
Performance via Abstraction. We designed HILTI’s machine
model to preserve high-level domain semantics that offer extensive
potential for global, transparent optimization. While not yet a focus
of our prototype implementation, we expect HILTI to eventually fa-
cilitate a range of powerful code enhancements. Consider our two
Bro-based applications, protocol parsing and script analysis: taken
together, they effectively port the core part of Bro over to the HILTI
platform. While today parsing and analysis remain separate Bro
subsystems, HILTI allows us to reason about them simultaneously
inside a single model, with the opportunity to then optimize func-
tionality across their boundaries. For example, if a user does not
conﬁgure any Bro scripts that inspect bodies of HTTP messages,
HILTI could simply remove the code for parsing them in depth.
Generally, we see opportunities for automatic optimization in
four main areas. First, HILTI facilitates improving individual func-
tionality by tuning the implementation. As a simple example,
we plan to better adapt HILTI’s map type to real-time usage by
avoiding CPU spikes during hash table resizes [16]. Likewise, we
are considering JIT compilation of regular expressions, similar to
re2c [4]. HILTI allows to perform such ﬁne-tuning “under the
hood” while transparently beneﬁting any host application.