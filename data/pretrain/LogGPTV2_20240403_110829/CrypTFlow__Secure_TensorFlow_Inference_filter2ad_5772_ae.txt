C. Realizing Fattest
We note that the ideal functionality assumed out of the
hardware can potentially be realized using various hardware
platforms that provide code attestation and secure signing, e.g.,
STM32H7, MediaTek MT3620, CEC1702, ARMTrustZone,
Intel SGX, etc. In this work, we provide an implementation
of Aramis based on Intel SGX.
SGX allows a host to create a protected region known as
an enclave. Intel gives integrity guarantees, that is, the code
and the data residing in the enclave, once attested, cannot be
modiﬁed by the host or the operating system. When SGX
receives a Commit command (Figure 7) for a function g, then
it creates an enclave with code g. Randomness rctr of Figure 7
can be sampled in SGX using sgx_read_rand command.
The attestation token Tg is generated by SGX communicating
with Intel’s Attestation Service (IAS) and this token is publicly
veriﬁable given g and public veriﬁcation key corresponding to
Intel’s Report Signing Key. The key-pair (vk, sk) for ECDSA
signature scheme is also generated inside the enclave and
the veriﬁcation key vk is sent as payload to IAS during the
generation of the attestation token. The token Tg contains the
veriﬁcation key vk in the clear and this vk can be used to
verify the signed outputs yctr. Now, on receiving the Compute
command, the enclave starts executing the code of g and
produces outputs signed under sk.
While running MPC in the Fattest-hybrid, we require the
enclave to reliably have veriﬁcation keys used by enclaves of
all other parties. This can be done by attaching the following
∗ (the code running inside SGX): Read the tokens
prelude to π
of all parties, parse them to obtain the veriﬁcation keys, and
verify the signature on the tokens using veriﬁcation key of
Intel’s Report Signing key. Note that since all the parties are
∗ (appended with this prelude),
running the same function π
∗ locally and compare it with
they can compute the hash of π
the hash in the tokens (which has been signed by Intel’s IAS)
of all the other parties, proceeding only if they all match
perfectly.
D. Implementation challenges with Intel SGX
We outline some of the key challenges in implementing
MPC between multiple SGX enclaves that involve multiple
rounds of interaction and operate over large volumes of data.
1) Memory constraints in SGX: In SGX, all the enclave
content, including code, and related data is stored in a special
region of memory known as the Enclave Page Cache (EPC).
The size of EPC is ﬁxed in BIOS and can have a maximum
size of 128MB. Typically, paging facilitates the execution of
enclaves which cannot ﬁt in EPC and any page that is evicted
out is encrypted before storing it on unprotected memory [45].
This additional overhead has detrimental effects on the overall
performance of the enclave application. We reduce the working
set of secure inference tasks to limit these overheads.
• ReLU and MaxPool functions: We split the computation
of memory-intensive non-linear functions into chunks that
ﬁt in the EPC to avoid paging. However, lower chunk
sizes increase the number of rounds, and so, the chunk
sizes must be carefully selected. For RESNET50, we
set the chunk sizes for ReLU and MaxPool layers to
be 40 MB and 10 MB respectively. For our network
conﬁgurations, the increase in rounds is justiﬁed by the
elimination of paging costs and reduction in end-to-end
runtimes.
• Convolution and Matrix Multiplication functions: For the
linear functions, we block the matrices into smaller ones,
process the blocks, and aggregate them. We ensure that
individual blocks ﬁt in EPC.
• Liveness Analysis: Athos implements liveness analysis
(Section III-D) which reduces the memory footprint of
the compiled DNNs. For example, the memory footprint
of RESNET50 reduces from 1100 MB to 397 MB due to
liveness analysis. When chunking and liveness analysis
are done together, the memory footprint of RESNET50
comes down to 297MB.
2) Porting Interactive Protocols to SGX: To the best of our
knowledge, we are the ﬁrst work to implement highly interac-
tive protocols in SGX and this comes with unique challenges.
For example, whenever data is passed across the enclave’s
protected memory region, it has to be marshalled in/out of the
region. The performance of marshalling depends on the size of
the parameters crossing the bridge. Larger parameters imply
slower marshalling [45], while smaller parameters increase the
total numbers of cross-bridge calls (which have an overhead
of their own). Thus, we tune the payload size carefully.
We also implement
the techniques in [78] for optimizing
communication involving enclaves.
VI. EXPERIMENTS
Overview.
In this section, we present our experimental
in Section VI-A, we use CRYPTFLOW
results. First,
to securely compute inference on the ImageNet dataset
using the following TensorFlow programs: RESNET503 and
DENSENET1214. We also show that the performance of semi-
honest and malicious protocols generated by CRYPTFLOW
scale linearly with the depth of DNNs. Second, in Section
VI-B, we show that CRYPTFLOW outperforms prior works on
secure inference of DNNs. Next, we evaluate each component
of CRYPTFLOW in more detail. In Section VI-C, we show
that the ﬁxed-point code generated by Athos matches the
accuracy of ﬂoating-point RESNET50 and DENSENET121.
We show in Section VI-D how the optimizations in Porthos
help it outperform prior works in terms of communication
complexity and overall execution time. In Section VI-E,
we show the overhead of obtaining malicious secure MPC
(over semi-honest security) using Aramis for GMW [35] and
Porthos. We show Aramis-based malicious secure inference
outperforms pure crypto-based malicious secure protocols by
huge margins in Section VI-E1. Finally, in section VI-F, we
discuss two case-studies of running CRYPTFLOW on DNNs
for healthcare. We begin by providing details of the systems
3https://github.com/tensorﬂow/models/tree/master/ofﬁcial/r1/resnet
4https://github.com/pudae/tensorﬂow-densenet
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:36:59 UTC from IEEE Xplore.  Restrictions apply. 
345
Benchmark
RESNET50
25.9
36.0
Semi-Honest (s) Malicious (s)
Comm. (GB)
DENSENET121
TABLE IV: CRYPTFLOW: ImageNet scale benchmarks.
75.4
112.9
6.9
10.5
used to run our experiments.
System Details. All our
large benchmark experiments
are in a LAN setting on 3.7GHz machines, each with 4 cores
and with 16 GB of RAM running Linux Ubuntu 16.04. The
measured bandwidth between each of the machines was at
most 377 MBps and the latency was sub-millisecond. Since
we wanted to use the same machines to benchmark both our
semi-honest as well as our malicious secure protocols, we
were constrained to use machines that had Intel SGX enabled
on them - this led to machines that had considerably lower
bandwidth between them (377 MBps) than those normally
used by prior works in the area (e.g. [60], [12] used networks
with bandwidth of 1.5 GBps). For Aramis, we used Intel SGX
SDK version 2.4. The compilation time of CRYPTFLOW is
around 5 sec for RESNET50, 35 sec for DENSENET121 and
2 minutes for RESNET200.
A. Secure Inference on ImageNet
We brieﬂy describe our benchmarks and then present per-
formance results.
1) RESNET50 [40] is a network that follows the residual
neural network architecture. The residual nodes employ
“skip connections” or short cuts between layers. It con-
sists of 53 convolution layers with ﬁlters of size up to
7 × 7, and 1 fully connected layer of size 2048 × 1001.
The activation function between most
layers is batch
normalization (Appendix B) followed by ReLU. After
the ﬁrst convolutional layer, the activation function also
includes a MaxPool.
2) DENSENET121 [43] is a form of residual neural network
that employs several parallel skips. It consists of 121
convolutional layers with ﬁlters of size up to 7 × 7. The
activation function between these layers is usually batch
normalization, followed by ReLU. Some layers also use
MaxPool or AvgPool.
Performance. Table IV shows performance of CRYPTFLOW
on these benchmarks. We measure communication as total
communication between all 3 parties - each party roughly
communicates a third of this value. The communication in
semi-honest secure and malicious secure inference is almost
the same. Thus demonstrating that ImageNet scale inference
can be performed in about 30 seconds with semi-honest
security and in under two minutes with malicious security.
The malicious protocol of CRYPTFLOW is about 3x slower
than the semi-honest version.
Scalability. We show that the running time of CRYPTFLOW-
based protocols increases linearly with the depth of DNNs.
)
s
d
n
o
c
e
s
(
e
m
T
i
250
200
150
100
50
0
25
20
15
10
5
0
)
B
G
(
n
o
i
t
a
c
i
n
u
m
m
o
C
18
34
50
101
152
200
n
Communication (GB)
Porthos Time
Aramis Time
Fig. 10: Scalability of CRYPTFLOW on RESNET-n.
We compile RESNET-n (where n, the approximate number of
convolutional layers, varies from 18 to 200) with CRYPTFLOW
and evaluate with both semi-honest (Porthos) and malicious
secure protocols (Aramis) in Figure 10. Our largest benchmark
here is RESNET-200,
the deepest version of RESNET on
the ImageNet dataset [41], which has 65 million parameters.
Other RESNET-n benchmarks have between 11 to 60 million
parameters 5. We observe that the communication and runtime
increase linearly with depth. Even with increasing depth, the
overhead of malicious security (over semi-honest security)
remains constant at about 3X.
B. Comparison with prior work
In this section, we show that CRYPTFLOW outperforms
prior works on secure inference of DNNs on the benchmarks
they consider, i.e., tiny 2–4 layer DNNs over the MNIST and
CIFAR-10 datasets. We stress that these benchmarks are very
small compared to the ImageNet scale DNNs discussed above.
In order to provide a fair comparison, for these experiments,
we use a network with similar bandwidth as prior works (1.5
GBps) and machines with similar compute (2.7 GHz).
Table V shows that Porthos outperforms prior (ABY3
[60], Chameleon6 [72], and SecureNN [79]) and concurrent
(QuantizedNN [12]) semi-honest secure 3PC works on the
MNIST dataset. It is well-known that 3PC-based techniques
like Porthos are much faster than techniques based on 2PC and
FHE. We relegate comparison between Porthos and 2PC/FHE
works to Appendix D. We omit comparisons with [22], [20]
as their published MSB protocol was incorrect [21].
C. Athos experiments
Accuracy of Float-to-Fixed. We show that Athos generated
ﬁxed-point code matches the accuracy of ﬂoating-code on
RESNET50 and DENSENET121 in Table VI. The table also
shows the precision or the scale that is selected by Athos
(Section III-B). We observe that different benchmarks require
5Speciﬁcally, 11, 22, 25, 44 and 60 million parameters for RESNET-n for
n = 18, 34, 50, 101, and 152 respectively.
6Chameleon is a 2PC protocol in the online phase but requires a trusted
third party in the ofﬂine phase. We report overall time here.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:36:59 UTC from IEEE Xplore.  Restrictions apply. 
346
Benchmark
Logistic Regression
SecureML (3 layer)
MiniONN (4 layer)
LeNet (4 layer)
[60]
4
8
-
-
[12]
−
20
80
120
[79]
−
17
47
79
[72]
2240
-
-
-
Porthos
2.7
8
34
58
TABLE V: Comparison with 3PC on MNIST dataset
with ABY3 [60], QuantizedNN [12], SecureNN [79], and
Chameleon [72]. All times in milliseconds.
Benchmark
RESNET50
DENSENET121
Float
Top 1
76.47
74.25
Fixed
Top 1
76.45
74.33
Float
Top 5
93.21
91.88
Fixed
Top 5
93.23
91.90
Scale
12
11
TABLE VI: Accuracy of ﬁxed-point vs ﬂoating-point.
different precision to maximize the classiﬁcation accuracy and
that the technique of “sweeping” through various precision
levels is effective. We show how accuracy varies with precision