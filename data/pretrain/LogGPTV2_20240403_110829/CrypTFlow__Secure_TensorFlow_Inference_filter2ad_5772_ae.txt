### C. Realizing Fattest

We note that the ideal functionality assumed for the hardware can potentially be realized using various platforms that provide code attestation and secure signing, such as STM32H7, MediaTek MT3620, CEC1702, ARM TrustZone, and Intel SGX. In this work, we provide an implementation of Aramis based on Intel SGX.

Intel SGX allows a host to create a protected region known as an enclave. Intel guarantees the integrity of the code and data within the enclave, ensuring that once attested, they cannot be modified by the host or the operating system. When SGX receives a Commit command (as shown in Figure 7) for a function \( g \), it creates an enclave with the code \( g \). The randomness \( r_{ctr} \) in Figure 7 can be sampled in SGX using the `sgx_read_rand` command.

The attestation token \( T_g \) is generated by SGX communicating with Intel’s Attestation Service (IAS). This token is publicly verifiable given \( g \) and the public verification key corresponding to Intel’s Report Signing Key. The key-pair \((vk, sk)\) for the ECDSA signature scheme is also generated inside the enclave, and the verification key \( vk \) is sent as part of the payload to IAS during the generation of the attestation token. The token \( T_g \) contains the verification key \( vk \) in plain text, which can be used to verify the signed outputs \( y_{ctr} \). Upon receiving the Compute command, the enclave executes the code of \( g \) and produces outputs signed under \( sk \).

In the Fattest-hybrid model, we require the enclave to reliably have the verification keys used by enclaves of all other parties. This can be achieved by appending the following prelude to the code running inside SGX: read the tokens of all parties, parse them to obtain the verification keys, and verify the signatures on the tokens using Intel’s Report Signing Key. Since all parties are running the same function, they can compute the hash of the function locally and compare it with the hash in the tokens (signed by Intel’s IAS) of all other parties, proceeding only if they all match perfectly.

### D. Implementation Challenges with Intel SGX

We outline some of the key challenges in implementing MPC between multiple SGX enclaves, which involve multiple rounds of interaction and operate over large volumes of data.

#### 1. Memory Constraints in SGX

In SGX, all enclave content, including code and related data, is stored in a special region of memory known as the Enclave Page Cache (EPC). The size of the EPC is fixed in BIOS and can have a maximum size of 128MB. Paging facilitates the execution of enclaves that do not fit in the EPC, and any evicted page is encrypted before being stored in unprotected memory [45]. This additional overhead can significantly impact the overall performance of the enclave application. We reduce the working set of secure inference tasks to limit these overheads.

- **ReLU and MaxPool Functions**: We split the computation of memory-intensive non-linear functions into smaller chunks that fit in the EPC to avoid paging. However, smaller chunk sizes increase the number of rounds, so the chunk sizes must be carefully selected. For RESNET50, we set the chunk sizes for ReLU and MaxPool layers to 40 MB and 10 MB, respectively. For our network configurations, the increase in rounds is justified by the elimination of paging costs and reduction in end-to-end runtimes.
  
- **Convolution and Matrix Multiplication Functions**: For linear functions, we block the matrices into smaller ones, process the blocks, and aggregate them. We ensure that individual blocks fit in the EPC.

- **Liveness Analysis**: Athos implements liveness analysis (Section III-D) to reduce the memory footprint of compiled DNNs. For example, the memory footprint of RESNET50 reduces from 1100 MB to 397 MB due to liveness analysis. When combined with chunking, the memory footprint of RESNET50 further reduces to 297 MB.

#### 2. Porting Interactive Protocols to SGX

To the best of our knowledge, we are the first to implement highly interactive protocols in SGX, which comes with unique challenges. For instance, whenever data is passed across the enclave’s protected memory region, it has to be marshalled in and out of the region. The performance of marshalling depends on the size of the parameters crossing the bridge. Larger parameters imply slower marshalling [45], while smaller parameters increase the total number of cross-bridge calls, each with its own overhead. Thus, we carefully tune the payload size.

We also implement the techniques in [78] for optimizing communication involving enclaves.

### VI. Experiments

#### Overview

In this section, we present our experimental results. First, in Section VI-A, we use CRYPTFLOW to securely compute inference on the ImageNet dataset using the following TensorFlow programs: RESNET50 and DENSENET121. We show that the performance of semi-honest and malicious protocols generated by CRYPTFLOW scales linearly with the depth of DNNs. Second, in Section VI-B, we demonstrate that CRYPTFLOW outperforms prior works on secure inference of DNNs. Next, we evaluate each component of CRYPTFLOW in more detail. In Section VI-C, we show that the fixed-point code generated by Athos matches the accuracy of floating-point RESNET50 and DENSENET121. In Section VI-D, we show how the optimizations in Porthos help it outperform prior works in terms of communication complexity and overall execution time. In Section VI-E, we show the overhead of obtaining malicious secure MPC (over semi-honest security) using Aramis for GMW [35] and Porthos. We show that Aramis-based malicious secure inference outperforms pure crypto-based malicious secure protocols by significant margins in Section VI-E1. Finally, in Section VI-F, we discuss two case studies of running CRYPTFLOW on DNNs for healthcare.

#### System Details

All our large benchmark experiments are conducted in a LAN setting on 3.7GHz machines, each with 4 cores and 16 GB of RAM running Linux Ubuntu 16.04. The measured bandwidth between each of the machines was at most 377 MBps, and the latency was sub-millisecond. Since we wanted to use the same machines to benchmark both our semi-honest and malicious secure protocols, we were constrained to use machines with Intel SGX enabled, leading to lower bandwidth (377 MBps) than those used by prior works (e.g., [60], [12] used networks with 1.5 GBps bandwidth). For Aramis, we used Intel SGX SDK version 2.4. The compilation time of CRYPTFLOW is around 5 seconds for RESNET50, 35 seconds for DENSENET121, and 2 minutes for RESNET200.

#### A. Secure Inference on ImageNet

We briefly describe our benchmarks and then present performance results.

- **RESNET50** [40] is a network that follows the residual neural network architecture, employing "skip connections" or shortcuts between layers. It consists of 53 convolution layers with filters up to 7 × 7 and 1 fully connected layer of size 2048 × 1001. The activation function between most layers is batch normalization followed by ReLU. After the first convolutional layer, the activation function also includes a MaxPool.

- **DENSENET121** [43] is a form of residual neural network that employs several parallel skips. It consists of 121 convolutional layers with filters up to 7 × 7. The activation function between these layers is usually batch normalization followed by ReLU. Some layers also use MaxPool or AvgPool.

**Performance**: Table IV shows the performance of CRYPTFLOW on these benchmarks. We measure communication as the total communication between all three parties, with each party roughly communicating a third of this value. The communication in semi-honest and malicious secure inference is almost the same, demonstrating that ImageNet scale inference can be performed in about 30 seconds with semi-honest security and in under two minutes with malicious security. The malicious protocol of CRYPTFLOW is about 3x slower than the semi-honest version.

**Scalability**: We show that the running time of CRYPTFLOW-based protocols increases linearly with the depth of DNNs. Figure 10 illustrates the scalability of CRYPTFLOW on RESNET-n, where n varies from 18 to 200. Our largest benchmark is RESNET-200, the deepest version of RESNET on the ImageNet dataset [41], which has 65 million parameters. Other RESNET-n benchmarks have between 11 to 60 million parameters. We observe that the communication and runtime increase linearly with depth. Even with increasing depth, the overhead of malicious security (over semi-honest security) remains constant at about 3x.

#### B. Comparison with Prior Work

In this section, we show that CRYPTFLOW outperforms prior works on secure inference of DNNs on the benchmarks they consider, i.e., tiny 2–4 layer DNNs over the MNIST and CIFAR-10 datasets. We stress that these benchmarks are very small compared to the ImageNet scale DNNs discussed above. To provide a fair comparison, we use a network with similar bandwidth (1.5 GBps) and machines with similar compute (2.7 GHz) for these experiments.

Table V shows that Porthos outperforms prior (ABY3 [60], Chameleon [72], and SecureNN [79]) and concurrent (QuantizedNN [12]) semi-honest secure 3PC works on the MNIST dataset. It is well-known that 3PC-based techniques like Porthos are much faster than techniques based on 2PC and FHE. We relegate the comparison between Porthos and 2PC/FHE works to Appendix D. We omit comparisons with [22], [20] as their published MSB protocol was incorrect [21].

#### C. Athos Experiments

**Accuracy of Float-to-Fixed**: We show that the fixed-point code generated by Athos matches the accuracy of floating-point code on RESNET50 and DENSENET121 in Table VI. The table also shows the precision or scale selected by Athos (Section III-B). We observe that different benchmarks require different precision levels to maximize classification accuracy, and the technique of "sweeping" through various precision levels is effective. We show how accuracy varies with precision.