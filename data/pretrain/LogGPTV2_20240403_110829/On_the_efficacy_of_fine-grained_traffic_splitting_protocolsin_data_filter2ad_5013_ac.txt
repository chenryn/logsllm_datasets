in our setting. In data center networks, where the effect of
packet reordering is not so severe, this simple adjustment to
the DUPACK threshold can help mitigate most of the ill-effects
due to PLTS.
2) An agent to ﬁlter DUPACKs: In a public cloud envi-
ronment, an end host runs multiple guest virtual machines
(VM). These VMs are rented out to tenants, who may be
using customized protocols for end to end communication.
For PLTS to be practical in such a setting, we cannot make
any assumptions about the higher layer protocols. Fortunately,
the hypervisor running below the VMs and the entire data
center network is administered by a central entity, and provides
the opportunity to design and deploy techniques that require
interactions between intermediate switches and end hosts’
hypervisors to counter the effects of packet reordering. Such a
technique would be completely transparent to the guest VMs
belonging to clients.
We assume the presence of an agent
that runs in the
hypervisor layer (either sender’s or receiver’s) that can inspect
every incoming packet. The agent essentially drops, modiﬁes
or delays duplicate acknowledgements to control the TCP
stack’s view of packet reordering. If the agent blindly stops
all the DUPACKs, then it prevents the TCP fast recovery in
case of an actual packet loss. This can cause TCP timeout
resulting in a drastic decrease in TCP throughput—a situation
we want to avoid. Thus, the agent essentially needs to store
information about the ACK packet last seen for each ﬂow,
using which it can easily detect the DUPACKs in a ﬂow. The
actions of the agent are dependent on whether the switches in
the network employ RED [12] and ECN [21] or use droptail
queueing policy.
RED/ECN queue management policy.
In a network where
routers use RED and ECN for congestion control, the TCP
receiver echos any network congestion back to the sender
by copying the CE (Congestion Encountered) bit of the data
packet onto the corresponding acknowledgement packet as
the ECE (ECN-Echo) bit. In case there is congestion, the
TCP sender acknowledges it by setting the CWR (Congestion
Window Reduced) bit in data packets. The ﬁltering agent can
then use it to differentiate a packet reordering event from a
packet loss event. The agent can keep dropping DUPACKs
until it sees a packet with the ECE bit set. This behavior
assumes that any packet drop would happen only after some
kind of congestion in the network. If the agent receives a
DUPACK with ECE bit set, then it allows it go through so
that TCP can take the required action necessitated by the
congestion in the network. It would keep on allowing the
DUPACKs until their ECE bit are set. We implemented this
simple agent, but found that while it performs much better
compared to ECMP, it does not yield any signiﬁcant beneﬁts
beyond the basic PLTS schemes. Perhaps other techniques that
can better exploit these markings may exist; a more detailed
study however is outside the scope of this paper.
DropTail queue management policy. In case of drop tail queue
management policy, it is difﬁcult to differentiate a packet
reordering from a packet loss event. However, we can use
a scheme similar to RR-TCP [24] to adjust the DUPACK
threshold dynamically. Speciﬁcally, the agent maintains multi-
ple counters corresponding to different numbers of consecutive
sequences of DUPACKs (i.e., number of times it received 3, 4
or 5 consecutive DUPACKs). When the agent receives the ﬁrst
two consecutive DUPACKs, it delivers them to the guest VM.
It drops the following DUPACKs until the length of the current
sequence of consecutive DUPACKs reaches the top 90th per-
centile of sequence lengths seen earlier. When this DUPACK
is delivered to the VM, it triggers a fast retransmission at the
TCP sender. The algorithm has the effect of avoiding 90%
fast retransmissions and allowing only those that would have
resulted from the longest sequence of consecutive duplicate
acknowledgements. PLTS enabled with this simple agent gives
slightly (by about 5%) better performance than PLTS alone as
shown in section IV.
IV. EVALUATION
We now evaluate the various packet-level trafﬁc splitting
schemes we have discussed in Section III. Speciﬁcally, our
evaluation goals are three-fold. First, we wish to compare
the various variants of PLTS (PLTS-Random, PLTS-Counter
and PLTS-RR) with other prior schemes such as ECMP and
MPTCP [14] in terms of their overall throughput achieved.
Second, we wish to understand other characteristics such as
packet latencies, queue length and fairness properties of PLTS.
Finally, we want to study the impact of different trafﬁc patterns
and packet sizes on PLTS.
A. Experimental setup
Most of our experiments in this paper are based on sim-
ulations using QualNet [23], a packet-level discrete event
simulator. Given the large number of TCP variants in the
wild, we chose to focus on one of the most widely used
TCP variants, namely NewReno. We used 1MB TCP receive
and send windows. Typically, operating systems reserve a few
hundred kilobytes for TCP windows. However, we inﬂated
this value slightly so that the TCP window sizes do not limit
the network throughput. This ensures that changes in the TCP
congestion window size always affect the network throughput.
We summarize the most signiﬁcant simulation parameters for
TCP in Table I.
TABLE I
SIMULATION PARAMETERS.
Parameter
Algorithm
Send Window
Receive Window
Delayed ACKs
Nagle’s Algorithm
Link Bandwidth
Output Port Buffers
Value
NewReno
1MB
1MB
Disabled
Disabled
100Mbps
512KB
For evaluating PLTS, we focus mainly on fat-tree archi-
tecture. However, the results shown should hold good in any
topology with multiple equal-cost paths between end hosts.
We do note that topologies BCube provide a large number of
paths between end hosts, but some paths may traverse more
links than others and hence their cost may be different.
B. Performance comparison between PLTS and ECMP
We compare PLTS and ECMP with respect to different
network parameters. The goal of these comparisons is to
identify the beneﬁts of PLTS technique over ECMP in different
dimensions which are of interest to network operators.
(a) Throughput
(b) Queue Length
(c) Link utilization
Fig. 3.
ECMP.
Throughput comparison across different routing mechanisms. We also show the queue length and link utilization comparisons across PLTS and
1) Throughput comparison: We ﬁrst compare the through-
put of PLTS compared to other schemes such as ECMP and
MP-TCP. For this evaluation, we simulate a 6-pod fat tree
topology with 54 end-hosts and create 27 FTP applications
which run for the entire duration of the simulation. The end
hosts for each FTP application are chosen at random while
ensuring that each end host executes either one FTP server or
client. Since a fat-tree topology offers full bisection bandwidth,
ideally each FTP application should be able to transfer data at
full link capacity. We have evaluated the TCP throughputs with
two queue management policies at the routers—DropTail and
RED with ECN. Figure 3(a) demonstrates that the through-
put values are almost identical for PLTS in both the cases.
Henceforth, for brevity reasons, all the experimental results for
PLTS are shown with DropTail policy. The results for RED
with ECN policy are similar.
Figure 3(a) shows the average throughput observed by FTP
applications, under different schemes, as a percentage of the
ideal throughput. The throughput achieved by ECMP is similar
to that reported in [20]. The low average throughput in case of
ECMP-based forwarding can be attributed to the fact that two
or more ECMP ﬂows may be forward over the same core link
which becomes a bottleneck. And for the entire ﬂow duration
that link remains the host spot in the network while leaving
other links underutilized. Due to static allocation of paths in
ECMP, if some of the ﬂows are unlucky and are routed through
a congested link, then they suffer permanently for the entire
duration resulting in a poor throughput.
All the three PLTS techniques achieve better throughput
than ECMP as shown in Figure 3(a). [14] reports that MP-TCP
achieves almost 90% utilization for the same experimental
setup, which is comparable to what the PLTS-RR achieves.
Among packet-level trafﬁc splitting, PLTS-RND attains the
least throughput because skews in short sequences of random
numbers increases variability in latencies across different path.
This leads to more packet reordering and fast retransmissions.
Hence, we will not explore PLTS-RND further. PLTS-CNT
attains around 81% of the ideal throughput. In PLTS-CNT
technique, packets from a given ﬂow may not be scattered
across all the paths always. It tries to make the best local
decision by choosing the smallest queue-size path at the router.
Depending on queue sizes at intermediate routers, packets of
a particular ﬂow may momentarily favor one of the available
paths. After some time, changes in network conditions can
change the distribution of packets across available paths. So
we observe small oscillations among the subset of paths a ﬂow
is forwarded to.
PLTS-RR ensures that all the paths between source and
destinations are utilized. In case the round robin policy is used
for all the ﬂows at all the routers, the trafﬁc load in similar
across all the paths and packets for a given ﬂow are believed to
encounter similar queues and latencies across multiple parallel
paths. This evenness and uniformity in the network usage
prevents any hot spots in network and helps all the ﬂows to
fully utilize the network capacity. Figure 3(a) conﬁrms this
behavior and PLTS-RR is able to achieve almost 92% of the
ideal throughput.
2) Comparison of queue lengths and link utilization:
It
is well known that ECMP based forwarding mechanism can
result in uneven load on network components [3], [14]. Even
in case of a uniform trafﬁc matrix, different part of the network
could be very differently loaded. The result is uneven queue
sizes and link utilization across the network. In contrast, as
our intuition suggests, we expect that PLTS should keep the
queue sizes and utilization balanced throughout the network,
which we evaluate next.
In a 6-pod fat tree data center network, carrying 54 active
ﬂows in the network, we isolate a total of 9 links originating
from all
the 9 core switches and connecting them to a
particular pod of the fat tree. We want to analyze the variation
in queue lengths and link utilizations of these links which
are bringing trafﬁc into this pod. We divide an interval of 1
second into hundred 10 milliseconds sub-intervals. At the end
of each sub-intervals, we record the queue lengths and the
link utilization values across all the 9 links selected above.
For both the metrics, we take the difference between the
maximum and minimum of the 9 values for a sub-interval. This
difference gives us the maximum variation in queue lengths
and link utilizations for that sub-interval. At the end of 1
second interval, we sort these 100 values and plot them to
demonstrate the variability of these metrics over a period of
1 second.
Figure 3(b) demonstrates that in case of PLTS, the differ-
ence between the queue lengths of longest and the smallest
 40 50 60 70 80 90 100ECMPPLTS-RNDPLTS-CNTPLTS-RRThroughput (as percentage of maximum)Drop-tail QueuesRED/ECN Queues 1 10 100 1000 10000 100000 20 40 60 80 100Difference in Queue LengthsQueue Length Readings by RankPLTS-RRECMP 0 0.2 0.4 0.6 0.8 1 10 20 30 40 50 60 70 80 90 100Difference in Link UtilizationLink Utilization Readings by RankPLTS-RRECMPFig. 4. Packet Latency values for a sample of 180 packets in ECMP versus PLTS. We also compare the throughput achieved by different ﬂows for fairness.
(a) Latency
(b) Throughput fairness
queue never exceeded 500 bytes in the analysis interval of
1 second. In ECMP, there are times when the difference in
queue sizes has increased all the way to 100KB. PLTS does
not only has smaller average queue length but it also has
lesser variation across multiple queues under consideration.
Figure 3(c) plots the variability in link utilization across the 9
links. It demonstrate a similar behavior where ECMP suffers
from huge difference of workload across different links while
PLTS maintains uniformity. The graphs support our claim
that packets will experience similar queueing delays along all
paths. The small difference in link utilizations indicates that
PLTS is less prone to hot-spots than ECMP.
3) Comparison of latencies: In the same setup as described
in the previous section, we compare the latencies experienced
by a sample of 180 packets of a long-lived ﬂow between two
hosts in different pods. We measure the latencies between the
top-of-the-rack switches connected to the end points of the
ﬂow. This isolates the latencies experienced in the network
from the latency at the access links. We compare the laten-
cies experienced by the sampled packets in three scenarios:
PLTS-RR and ECMP with drop-tail queues and ECMP with
RED/ECN active queue management. The latencies for the
sampled packets are ranked and plotted in ﬁgure 4(a).
For ECMP, the latency values for drop-tail policy are worse
than the latency values of RED/ECN (as drop-tail allows
large queue build ups). PLTS with the drop-tail policy shows
signiﬁcantly better latency values experienced by the sampled
packets as compared to ECMP with RED/ECN. While for
most of the packets the latency values for the two situations
are comparable, a packet in PLTS experiences only one-third
of the packet latency of ECMP with RED/ECN at the 90th
percentile and one-sixth of the packet latency at the 99th
percentile. In most of the partition-aggregate type workload
today [4], the latency at the 99th percentile is of utmost
importance. It decides how well a service can perform within
SLA (Service Level Agreements) up time [4], which ultimately
would affect customer experience in public cloud. The results
indicate the opportunities which PLTS could provide to sig-