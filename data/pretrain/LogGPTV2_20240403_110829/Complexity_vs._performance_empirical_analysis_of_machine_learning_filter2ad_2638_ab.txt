one of the most common applications of ML models in deployed
systems. Moreover, binary classification is one of the two learning
tasks (the other being regression) that are commonly supported
by all 6 ML platforms. Other learning tasks, e.g. clustering and
multi-class classification, are only supported by a small subset of
platforms.
ControlPerformance    and RiskMoreLessLowHighGoogleABM AmazonBigMLPredictionIOMicrosoftLocal(a) Breakdown of application domains.
(b) Distribution of sample numbers.
(c) Distribution of feature numbers.
Figure 3: Basic characteristics of datasets used in our experiments.
3.1 Datasets
We describe the datasets we used for training ML classifiers. We use
119 labeled datasets from diverse application domains such as life
science, computer games, social science, and finance etc. Figure 3(a)
shows the detailed breakdown of application domains. The major-
ity of datasets (94 out of 119) are from the popular UCI machine
learning repository [3], which is widely adopted for benchmarking
ML classifiers. The remainder include 16 popular synthetic datasets
from scikit-learn8, and 9 datasets used in other applied machine
learning studies [5, 13, 17, 18, 32, 33, 70, 73]9. It is also important
to highlight that our datasets vary widely in terms of the number
of samples and number of features, as shown in Figure 3(b) and
Figure 3(c). Datasets vary in size from 15 samples to 245, 057 sam-
ples, while the dimensionality of datasets ranges from 1 to 4, 702.
Note that we limit the number of extremely large datasets (with size
over 100k) due to the high computational complexity incurred in
using them on MLaaS platforms. We include complete information
about all datasets separately10. As none of the MLaaS platforms
provides any support for data cleaning, we perform the following
data preprocessing steps locally before uploading to MLaaS plat-
forms. Our datasets include both numeric and categorical features.
Following prior conventions [23], we convert all categorical fea-
tures to numerical values by mapping {C1, ..., CN } to {1, ..., N}. We
acknowledge that this may impact performance of some classifiers,
e.g. distance-based classifiers like kNN [45]. But since our goal is to
compare performance across different platforms instead of across
classifiers, this preprocessing is unlikely to change our conclusions.
For datasets with missing values, we replace missing fields with
median values of corresponding features, which is a common ML
preprocessing technique [62]. Finally, for each dataset, we randomly
split data samples into training and test set by 70%–30% ratio. We
train classifiers on each MLaaS platforms using the same training
and held-out test set. We report classification performance on the
test set.
3.2 MLaaS Platform Measurements
In this section, we describe our methodology for measuring classi-
fication performance of MLaaS platforms when we vary available
controls.
8http://scikit-learn.org
9There are two datasets used in [73].
10http://sandlab.cs.uchicago.edu/mlaas
Choosing Controls of an ML System. As mentioned in Sec-
tion 2, we break down an ML system into 5 dimensions of con-
trol. In this paper, we consider 4 out of 5 dimensions by excluding
Program Implementation which is not controllable in any plat-
form. The remaining dimensions are grouped into three categories,
Preprocessing (data transformation) and Feature Selection (FEAT),
Classifier Choice (CLF), and Parameter Tuning (PARA). Note that
we combine Preprocessing with Feature Selection to simplify our
analysis, as both controls are only available in Microsoft. In the
rest of the paper, we interchangeably use the term Feature Selec-
tion and FEAT to refer to this combined category. Overall, these
three categories of control present the easiest and most impactful
options for users to train and build high quality ML classifiers. As
baselines for performance comparison, we use two reference points
that represent the extremes of the complexity spectrum, one with
no user-tunable control, and one where users have full control over
all control dimensions. To simulate an ML system with no control,
we set a default choice for each control dimension. We refer to
this configuration as baseline in later sections. Since not all of
the 6 platforms we study have a default classifier, we use Logistic
Regression as the baseline, as it is the only classifier supported by
all 4 platforms (where the control is available). All MLaaS platforms
select a default set of parameters for Logistic Regression (values
and parameters vary across platforms), and we use them for the
baseline settings. We perform no feature selection for the baseline
settings. To simulate an ML system with full control, we use a local
ML library, scikit-learn, as this library allows us to tune all control
dimensions. We refer to this configuration as local in later sections.
Performing Measurements by Varying Controls. We evalu-
ate performance of MLaaS platforms on each dataset by varying
available controls. Table 1 provides detailed information about avail-
able choices for each control dimension. We vary the FEAT and CLF
dimensions by simply applying all available choices listed for each
system in Table 1. It is interesting to note that the CLF choices
vary across platforms even though all platforms are competing to
provide the same service, i.e. binary classification. For example,
Random Forests and Boosted Decision Tree, best performing classi-
fiers based on prior work [14, 15], are only available on Microsoft.
The PARA dimension is varied by applying grid search. We explore
all possible options for categorical parameters. For example, we
include both L1 and L2 in regularization options from Logistic Re-
gression. For numerical parameters, we start with the default value
LifeScience:44Computer&Games:18Synthetic:17SocialScience:10PhysicalScience:10Financial&Business:7N/A:13Financial & Business: 7Other: 13Physical Science: 10Social Science: 10Synthetic: 17Computer & Game: 18Life Science: 44 0 0.2 0.4 0.6 0.8 1101001k10k100kCDF of DatasetsNumber of Samples 0 0.2 0.4 0.6 0.8 11101001k4.7kCDF of DatasetsNumber of FeaturesPlatform
Amazon
PredictionIO
BigML
FEAT
×
×
×
Microsoft
scikit-learn
Fisher LDA,
Filter-based
(using Pearson,
Mutual, Kendall,
Spearman, Chi,
Fisher, Count)
FClassif,
MutualInfoClassif,
GaussianNorm,
MinMaxScaler,
MaxAbsScaler,
L1Normalization,
L2Normalization,
StandardScaler
CLF (# of parameter tuned: parameter list (PARA))
Logistic Regression (3: maxIter, regParam, shuffleType)
Logistic Regression (3: maxIter, regParam, fitIntercept), Naive Bayes (1:lambda),
Decision Tree (2: numClasses, maxDepth),
Logistic Regression (3: regularization, strength, eps), Decision Tree (3: node threshold, ordering,
random candidates), Bagging [11] (3: node threshold, number of models, ordering),
Random Forests [12] (3: node threshold, number of models, ordering)
Logistic Regression (4: optimization tolerance, L1 regularization weight, L2 regularization weight,
memory size for L-BFGS ), Support Vector Machine (2: # of iterations, Lambda), Averaged Percep-
tron [27] (2: learning rate, max. # of iterations), Bayes Point Machine [34] (1: # of training iteration),
Boosted Decision Tree [28] (4: max. # of leaves per tree, min. # of training instances per leaf,
learning rate, # of trees constructed), Random Forests (5: resampling method, # of decision trees,
max. depth of trees, # of random splits per node, min. # of samples per leaf),
Decision Jungle [58] (5: resampling method, # of DAGs, max. depth of DAGs, max. width of DAGs,
# of optimization step per DAG layer),
Logistic Regression (3: penalty, C, solver), Naive Bayes (1: prior), Support Vector Machine
(3: penalty, C, loss), Linear Discriminant Analysis (2: solver, shrinkage), k-Nearest Neighbor
(3: n_neighbors, weights, p), Decision Tree (2: criterion, max_features), Boosted Decision Tree
(3: n_estimators, criterion, max_features), Bagging (2: n_estimators, max_features), Random Forests
(2: n_estimators, max_features), Multi-Layer Perceptron [52] (3: activation, solver, alpha)
Table 1: Detailed configurations for MLaaS platforms and local library measurement experiments. For each control dimension,
we list available configurations (feature selection methods, classifiers, and tunable parameters).
# Class-
ifiers
1 (1)
1 (1)
1 (1)
3 (8)
4 (4)
7 (9)
10 (14)
Platform
# Feature
Selections
# Para-
meters
# Measu-
rements
-
-
-
-
-
8 (8)
8 (14)
-
-
PredictionIO
BigML
ABM
Google
Amazon
119
119
4,284
3,719
12,838
1,728,791
Microsoft
2,137,410
scikit-learn
Table 2: Scale of the measurements. The last column shows
total number of configurations we tested on each platform.
Numbers in parenthesis in column #2 to #4 show the number
of available options shown to users on each platform, while
numbers outside parenthesis show the number of options
we explore in experiments.
3 (3)
6 (25)
12 (46)
23 (34)
32 (111)
provided by platforms and scan a range of values that are two orders
of magnitude lower and higher than the default. In other words, for
each numerical parameter with a default value of D, we investigate
100, D, and 100 × D. For example, we explore 0.0001,
three values: D
0.01 and 1 for the regularization strength parameter in Logistic Re-
gression, where the default value is 0.01. We also manually examine
the parameter type and its acceptable value range to make sure the
parameter value is valid.
Table 2 shows the total number of measurements we perform
for each platform and the number of choices for each control di-
mension. All experiments were performed between October 2016
and February 2017. For platforms with no control, we perform one
measurement per dataset, giving us 119 prediction results (ABM
and Google). At the other extreme, Microsoft requires over 1.7M
measurements, given the large number of available controls. Note
that numbers in the last column is much larger than the product
of numbers in previous columns, because for each parameter we
tune, we explore multiple values, resulting in a larger number of
total measurements. To set up experiments, we leverage web APIs
provided by the platforms, allowing us to automate experiments
through scripts. Unfortunately, Microsoft only provides an API for
using preconfigured ML models on different datasets, and there is
no API for configuring ML models. Hence, in the case of Microsoft,
we manually configure ML models (over 200 model configurations)
using the web GUI, and then automate the application of the models
to all datasets.
Evaluation Metrics. We measure the performance of a platform
by computing the average F-score across all datasets. F-score is a
better metric compared to accuracy as many of our datasets have
imbalanced class distributions. It is defined as the harmonic mean
of precision and recall. Precision is the fraction of samples predicted
to be positive that are truly positive and recall is the fraction of
positive samples that are correctly predicted. Note that other met-
rics like Area Under Curve or Average Precision are also not biased
by imbalanced datasets, but unfortunately cannot be applied, as
PredictionIO and several classifiers on BigML do not provide a
prediction score.
To validate whether a single metric (Average F-score) is repre-
sentative of performance across all the datasets, we compute the
Friedman ranking [55] of platforms across all the datasets. Friedman
ranking statistically ranks platforms by considering a given metric
(e.g. F-score) across all datasets. A platform with a higher Friedman
rank exhibits statistically better performance when considering all
datasets, compared to a lower ranked platform. We observe that the
platform ranking based on average F-score is consistent with the
Friedman ranking (using F-score), suggesting that average F-score
is a representative metric. In the rest of the paper, the performance
of a platform refers to the average F-score across datasets.
4 COMPLEXITY VS. PERFORMANCE
We have shown that MLaaS platforms represent ML system designs
with different levels of complexity and user control. In this section,
Figure 4: Optimized and baseline performance (F-score) of
platforms and local library.
we try to answer our first question: How does the performance of
ML systems vary as we increase their complexity?
4.1 Optimized Performance
First we evaluate the optimized performance each MLaaS platform
can achieve by tuning all possible controls provided by the platform,
i.e. FEAT, CLF, and PARA. In this process, we train individual models
for all possible combinations of the 3 controls (whenever available)
and use the best performing model for each dataset. We report
the average F-score across all datasets for each platform as its
performance. We refer to these results as optimized. Note that
the optimized performance is simply the highest performance on
the test set that is obtained by training different models using all
available configurations. We do not optimize the model on test set.
We also generate the corresponding reference points, i.e. base-
line and local. For local, we compute the highest performance
on our local ML library by tuning all 3 control dimensions. For
baseline, we measure the performance of “fully automated”, zero-
control versions of all systems (MLaaS and our local library), by
using the baseline configurations for each platform. As mentioned
earlier, these reference points capture performance at two ends of
the complexity spectrum (no control vs. full control).
Figure 4 shows the optimized average F-score for each MLaaS
platform, together with the optimized results. Platforms are listed
on the x-axis based on increasing complexity. We observe a strong
correlation between system complexity and the optimized classi-
fication performance. The platform with highest complexity (Mi-
crosoft) shows the highest performance (0.83 average F-score), and
performance decreases as we consider platforms with lower com-
plexity/control (Google and ABM), with ABM showing the lowest
performance (0.71 F-score). As expected, the local library outper-
forms all MLaaS platforms, as it explores the largest range of model
configurations (most feature selections techniques, classifiers, and
parameters). Note that the performance difference between local
and MLaaS platforms with high complexity is smaller, suggesting
that adding more complexity and control beyond Microsoft brings
diminishing returns. In addition, when we compare the baseline
Figure 5: Relative improvement in performance (F-score)
over baseline as we tune individual controls (white boxes in-
dicate controls not supported).
performance with the optimized performance for platforms with
high complexity (Microsoft), the difference is significant, with up to
26.7% increase in F-score, further indicating that higher complexity
provides room for more performance improvement. Lastly, the er-
ror bars show the standard error of the measured performance, and
we observe that the statistical variation of performance measures
for different platforms is not large.
For completeness, we include the detailed baseline and optimized
performance of MLaaS platforms in Table 3. We include F-score, and
other 3 metrics, accuracy, precision, and recall. We also compute