Target
Attack Settings
#(A.) D.R. FPRu
ﬂag variable overwritten attack
Regular expression Denial of Service
directory harvest attack
sshd
libpcre
sendmail
an inline virtual exploit that matches a username
3 deleterious patterns paired with 8-23 input strings
probing batch sizes: 8, 16, 32, 64, 100, 200, and 400
800
46
14
100% 0.0001
100% 0.0001
100% 0.0001
A. is short for attack attempt. D.R. is short for detection rate. FPRu is the false positive rate upper bound (details in Sect. 6.1).
Normala
. . .
auth p > xfree
do auth > xfree
do auth > log msg
do auth > p start
p start > buf clr
. . .
phdtw > buf len
do auth > do autd
. . .
Normalb
. . .
auth p > xfree
do auth > debug
do auth > xfree
do auth > p start
p start > buf clr
. . .
phdtw > buf len
do auth > p read
. . .
Attack
. . .
auth p > xfree
do auth > debug
do auth > xfree
do auth > p start
p start > buf clr
. . .
phdtw > buf len
do auth > do autd
. . .
aA successfully authenticated session.
bA failed (wrong password) authentication.
“caller > callee” denotes a function call.
Routine names are abbreviated to save space.
Figure 5: Samples of normal and anomalous sshd traces.
Table 3: Deleterious patterns used in ReDoS attacks.
Deleterious Pattern
#(attack)
Pattern 1
Pattern 2
Pattern 3
^(a+)+$
((a+)*)+$
^(([a-z])+.)+[A-Z]([a-z])+$
15
8
23
In Fig. 5, the Attack and Normalb bear the same trace
prior to the last line, and the Attack and Normala bear the
same trace after (including) the last line. Our approach de-
tects the attack as a montage anomaly: the control-ﬂow seg-
ment containing do_auth > debug should not co-occur with
the control-ﬂow segment containing do_auth > do_authed
(and following calls) in a single execution window.
In the traces, there are identical 218 call events including
library routines (36 calls excluding library ones) between
the third line and the last line in Fig. 5. We test an n-gram
detection tool, and it requires at least n = 37 to detect the
speciﬁc attack without libraries routine traced. The 37-gram
model results in an FPR of 6.47% (the FPR of our approach
is less than 0.01%). This indicates that n-gram models with
a large n is diﬃcult to converge at training. We do not test
automaton-based detection because they cannot detect the
attack in theory. The attack does not contain any illegal
function calls.
6.2.2 Regular Expression Denial of Service
Regular expression Denial of Service (ReDoS) is a service
abuse attack. It exploits the exponential time complexity of
a regex engine when performing backtracking. The attacks
construct extreme matching cases where backtracking is in-
volved. All executed control ﬂows are legal, but the regex
engine hangs due to the extreme complexity.
e
t
a
R
n
o
i
t
c
e
t
e
D
1
0.8
0.6
0.4
0.2
0
FPR
0.01
0.001
0.0001
Pattern1 Pattern2 Pattern3 Pattern1 Pattern2 Pattern3
FVA                                     PCA
Figure 6: Detection rates of ReDoS attacks.
9.
We produce 46 ReDoS attack attempts targeting libpcre
Three deleterious patterns are used (Table 3). For each dele-
terious pattern, attacks are constructed with an increasing
length of a in the input string starting at 6, e.g., aaaaaaaab.
We stop attacking libpcre at diﬀerent input string lengths
so that the longest hanging time periods for diﬀerent delete-
rious patterns are about the same (a few seconds). A longer
input string incurs a longer hanging time; it results in a more
severe ReDoS attack than a shorter one.
ReDoS attacks are detected in intra-cluster detection op-
eration (Occurrence Frequency Analysis) by the prob-
abilistic method, i.e., ν-SVM. We test our approach with
both PCA and FVA feature selection (Sect. 4.3, the prob-
abilistic method, bullet b). The detection results (Fig. 6)
show that our approach conﬁgured with PCA is more sensi-
tive than it conﬁgured with FVA. Our approach (with PCA)
detects all attack attempts at diﬀerent FPRs10. The unde-
tected attack attempts (with FVA) are all constructed with
the small amount of a in the input strings, which do not
result in very severe ReDoS attacks.
6.2.3 Directory Harvest Attack
Directory harvest attack (DHA) is a service abuse attack.
It probes valid email users through brute force. We produce
14 DHA attack attempts targeting sendmail. Each attack
attempt consists of a batch of closely sent probing emails
with a dictionary of possible receivers. We conduct DHA
attacks with 7 probing batch sizes from 8 to 400 (Table 2).
Two attack attempts are conducted for each batch size.
Our approach (conﬁgured at FPRu 0.0001) successfully
detects all attack attempts with either PCA or FVA feature
selection10. DHA attacks are detected in intra-cluster de-
tection (Occurrence Frequency Analysis) by the prob-
abilistic method, i.e., ν-SVM. The attacks bypass the inter-
9Internal deep recursion prevention of libcpre is disabled.
10No attack is detected if only Co-occurrence Analysis is
performed.
409Our approach (w/ FVA)
Our approach (w/ PCA)
One-class SVM (w/ FVA)
One-class SVM (w/ PCA)
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
0.1
0.04
0.06
0.02
0.08
Montage anomaly
0.04
0.08
0.02
0.1
Incomplete path anomaly
0.06
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
0.02
0.04
0.06
0.08
0.1
High-frequency anomaly
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
0.02
0.04
0.06
0.08
0.1
Low-frequency anomaly
Figure 7: libpcre ROC of our approach and basic one-class SVM. X-axis is false positive rate, and y-axis is detection rate.
cluster detection (Co-occurrence Analysis) because in-
valid usernames occur in normal training dataset.
This experiment demonstrates that our approach can con-
sume coarse program behavior descriptions (e.g., system calls)
to detect attacks. Most of the probing emails do not have
valid receivers. They result in a diﬀerent processing proce-
dure than that for normal emails; the batch of DHA emails
processed in an execution window gives anomalous ratios
between frequencies of valid email processing control ﬂows
and frequencies of invalid email processing control ﬂows. In
sendmail, these diﬀerent control ﬂows contain diﬀerent sets
of system calls, so they are revealed by system call proﬁles.
More precise detection requires the exposure of internal pro-
gram activities, such as function calls.
6.3 Systematic Accuracy Evaluation
We systematically demonstrate how sensitive and accu-
rate our approach is through receiver operating characteris-
tic (ROC). Besides normal program behaviors ground truth
(Sect. 6.1), we generate four types of synthetic aberrant path
anomalies. We ﬁrst construct F (cid:2)
for each synthetic anoma-
lous behavior instance b(cid:2)
, and then we use (1) to derive O(cid:2)
(of b(cid:2)
1. Montage anomaly: two behavior instance b1 and b2 are
randomly selected from two diﬀerent behavior clusters.
For a cell f(cid:2)
, if one of f1i,j (of F1) andf 2i,j
(of F2) is 0, the value of the other is copied into f(cid:2)
i,j.
Otherwise, one of them is randomly selected and copied.
) from F (cid:2)
i,j in F (cid:2)
.
2. Incomplete path anomaly: random one-eighth of non-
zero cells of a normal F are dropped to 0 (indicating
events that have not occurred) to construct F (cid:2)
.
3. High-frequency anomaly: three cells in a normal F are
randomly selected, and their values are magniﬁed 100
times to construct F (cid:2)
.
4. Low-frequency anomaly: similar to high-frequency anoma-
lies, but the values of the three cells are reduced to 1.
To demonstrate the eﬀectiveness of our design in handling
diverse program behaviors, we compare our approach with
a basic one-class SVM (the same ν-SVM and same conﬁgu-
rations, e.g., kernel function, feature selection, and parame-
ters, as used in ourIntra-cluster Modeling operation).
We present the detection accuracy results on libpcre in
Fig. 7, which has the most complicated behavior patterns
among the three studied programs/libraries11. In any sub-
ﬁgure of Fig. 7, each dot is associated with a false positive
rate (multi-round 10-fold cross-validation with 10,000 test
cases) and a detection rate (1,000 synthetic anomalies). We
denote an anomaly result as a positive.
Fig. 7 shows the eﬀectiveness of our clustering design.
The detection rate of our prototype (with PCA12) is usu-
ally higher than 0.9 with FPR less than 0.01. Because of
diverse patterns, basic one-class SVM fails to learn tight
boundaries that wrap diverse normal patterns as expected.
A loose boundary results in false negatives and low detection
rates.
6.4 Performance Analysis
Although performance is not a critical issue for the train-
ing phase, a fast and eﬃcient detection is important for en-
abling real-time protection and minimizing negative user ex-
perience [32]. The overall overhead of a program anomaly
detection system comes from tracing and analysis in general.
We evaluate the performance of our analysis procedures
(inter- and intra-cluster detections) with either function call
proﬁles (libpcre) or system call proﬁles (sendmail). We
test the analysis on all normal proﬁles (libpcre: 11027,
sendmail: 6579) to collect overhead for inter-cluster detec-
tion alone and the combination of inter- and intra-cluster
detection 13. The analysis of each behavior instance is re-
peated 1,000 times to obtain a fair timing. The performance
results in Fig. 8 illustrate that
• It takes 0.1~1.3 ms to analyze a single behavior instance,
which contains 44893 function calls (libpcre) or 1134
system calls (sendmail) on average (Table 1).
• The analysis overhead is positively correlated with the
number of unique events in a proﬁle (Table 1), which is
due to our DOK implementation of proﬁle matrices.
• Montage anomalies takes less time to detect than fre-
quency anomalies, because they are detected at the ﬁrst
stage (Co-occurrence Analysis).
11Results of the other two programs share similar character-
istics as libpcre and are not presented.
12PCA proves itself more accurate than FVA in Fig. 7.
13PCA is used for feature selection. FVA (results omitted)
yields a lower overhead due to its simplicity.
410-
v
a
h
e
b
r
e
p
e
m
i
t
n
o
i
t
c
e
t
e
D
)
d
n
o
c
e
s
i
l
l
i
m
(
e
c
n