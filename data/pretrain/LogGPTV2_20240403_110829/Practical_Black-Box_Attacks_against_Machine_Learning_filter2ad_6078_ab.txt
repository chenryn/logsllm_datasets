tom row are created from the legitimate samples [7, 13] in the
top row. The DNN outputs are identiﬁed below the samples.
Targeted Model: We consider attackers targeting a multi-
class DNN classiﬁer. It outputs probability vectors, where
each vector component encodes the DNN’s belief of the input
being part of one of the predeﬁned classes. We consider the
ongoing example of a DNN classifying images, as shown in
Figure 1. Such DNNs can be used to classify handwritten
digits into classes associated with digits from 0 to 9, images
of objects in a ﬁxed number of categories, or images of traﬃc
signs into classes identifying its type (STOP, yield, ...).
Adversarial Capabilities: The oracle O is the targeted
DNN. Its name refers to the only capability of the adversary:
accessing the label ˜O((cid:126)x) for any input (cid:126)x by querying oracle
O. The output label ˜O((cid:126)x) is the index of the class assigned
the largest probability by the DNN:
˜O((cid:126)x) = arg max
j∈0..N−1
Oj((cid:126)x)
(2)
where Oj((cid:126)x) is the j-th component of the probability vector
O((cid:126)x) output by DNN O. Distinguishing between labels and
probabilities makes adversaries realistic (they more often
have access to labels than probabilities) but weaker: labels
encode less information about the model’s learned behavior.
Accessing labels ˜O produced by the DNN O is the
only capability assumed in our threat model. We do
not have access to the oracle internals or training data.
Adversarial Goal: We want to produce a minimally al-
tered version of any input (cid:126)x, named adversarial sample, and
denoted (cid:126)x∗, misclassiﬁed by oracle O: ˜O( (cid:126)x∗) (cid:54)= ˜O((cid:126)x). This
corresponds to an attack on the oracle’s output integrity. Ad-
versarial samples solve the following optimization problem:
(cid:126)x∗ = (cid:126)x + arg min{(cid:126)z : ˜O((cid:126)x + (cid:126)z) (cid:54)= ˜O((cid:126)x)} = (cid:126)x + δ(cid:126)x
(3)
Examples of adversarial samples can be found in Figure 2.
The ﬁrst row contains legitimate samples and the second cor-
responding adversarial samples that are misclassiﬁed. This
misclassiﬁcation must be achieved by adding a minimal per-
turbation δ(cid:126)x so as to evade human detection. Even with total
knowledge of the architecture used to train model O and its
parameters resulting from training, ﬁnding such a minimal
perturbation is not trivial, as properties of DNNs preclude
the optimization problem from being linear or convex. This
is exacerbated by our threat model: removing knowledge of
model O’s architecture and training data makes it harder to
ﬁnd a perturbation such that ˜O((cid:126)x + δ(cid:126)x) (cid:54)= ˜O((cid:126)x) holds.
In Appendix C, we give a presentation of attacks conducted
in related threat models—with stronger assumptions.
MNIST DatasetGTSRD Dataset5803508approach, illustrated in Figure 3, overcomes these challenges
mainly by introducing a synthetic data generation technique,
the Jacobian-based Dataset Augmentation. We emphasize
that this technique is not designed to maximize the substitute
DNN’s accuracy but rather ensure that it approximates the
oracle’s decision boundaries with few label queries.
Substitute Architecture: This factor is not the most
limiting as the adversary must at least have some partial
knowledge of the oracle input (e.g., images, text) and ex-
pected output (e.g., classiﬁcation). The adversary can thus
use an architecture adapted to the input-output relation. For
instance, a convolutional neural network is suitable for image
classiﬁcation. Furthermore, we show in Section 6 that the
type, number, and size of layers used in the substitute DNN
have relatively little impact on the success of the attack.
Adversaries can also consider performing an architecture ex-
ploration and train several substitute models before selecting
the one yielding the highest attack success.
Generating a Synthetic Dataset: To better understand
the need for synthetic data, note that we could potentially
make an inﬁnite number of queries to obtain the oracle’s
output O((cid:126)x) for any input (cid:126)x belonging to the input domain.
This would provide us with a copy of the oracle. However,
this is simply not tractable: consider a DNN with M input
components, each taking discrete values among a set of K
possible values, the number of possible inputs to be queried
is KM . The intractability is even more apparent for inputs in
the continuous domain. Furthermore, making a large number
of queries renders the adversarial behavior easy to detect.
A natural alternative is to resort to randomly selecting
additional points to be queried. For instance, we tried using
Gaussian noise to select points on which to train substitutes.
However, the resulting models were not able to learn by
querying the oracle. This is likely due to noise not being
representative of the input distribution. To address this issue,
we thus introduce a heuristic eﬃciently exploring the input
domain and, as shown in Sections 5 and 6, drastically limits
the number of oracle queries. Furthermore, our technique
also ensures that the substitute DNN is an approximation of
the targeted DNN i.e. it learns similar decision boundaries.
The heuristic used to generate synthetic training inputs is
based on identifying directions in which the model’s output is
varying, around an initial set of training points. Such direc-
tions intuitively require more input-output pairs to capture
the output variations of the target DNN O. Therefore, to
get a substitute DNN accurately approximating the oracle’s
decision boundaries, the heuristic prioritizes these samples
when querying the oracle for labels. These directions are
identiﬁed with the substitute DNN’s Jacobian matrix JF ,
which is evaluated at several input points (cid:126)x (how these
points are chosen is described below). Precisely, the adver-
sary evaluates the sign of the Jacobian matrix dimension
corresponding to the label assigned to input (cid:126)x by the ora-
JF ((cid:126)x)[ ˜O((cid:126)x)]
cle: sgn
. To obtain a new synthetic training
point, a term λ · sgn
is added to the original
point (cid:126)x. We name this technique Jacobian-based Dataset
Augmentation. We base our substitute training algorithm
on the idea of iteratively reﬁning the model in directions
identiﬁed using the Jacobian.
JF ((cid:126)x)[ ˜O((cid:126)x)]
(cid:17)
(cid:16)
(cid:16)
(cid:17)
Substitute DNN Training Algorithm: We now describe
Algorithm 1 - Substitute DNN Training: for oracle ˜O,
a maximum number maxρ of substitute training epochs, a
substitute architecture F , and an initial training set S0.
Input: ˜O, maxρ, S0, λ
1: Deﬁne architecture F
2: for ρ ∈ 0 .. maxρ − 1 do
3:
// Label the substitute training set
4:
((cid:126)x, ˜O((cid:126)x)) : (cid:126)x ∈ Sρ
D ←(cid:110)
(cid:111)
// Train F on D to evaluate parameters θF
θF ← train(F, D)
// Perform Jacobian-based dataset augmentation
Sρ+1 ← {(cid:126)x + λ · sgn(JF [ ˜O((cid:126)x)]) : (cid:126)x ∈ Sρ} ∪ Sρ
5:
6:
7:
8:
9: end for
10: return θF
the ﬁve-step training procedure outlined in Algorithm 1:
• Initial Collection (1): The adversary collects a very
small set S0 of inputs representative of the input do-
main. For instance, if the targeted oracle O classiﬁes
handwritten digits, the adversary collects 10 images of
each digit 0 through 9. We show in Section 5 that this
set does not necessarily have to come from the distri-
bution from which the targeted oracle was trained.
• Architecture Selection (2): The adversary selects
an architecture to be trained as the substitute F . Again,
this can be done using high-level knowledge of the clas-
siﬁcation task performed by the oracle (e.g., convolu-
tional networks are appropriate for vision)
• Substitute Training: The adversary iteratively trains
more accurate substitute DNNs Fρ by repeating the
following for ρ ∈ 0..ρmax:
– Labeling (3): By querying for the labels ˜O((cid:126)x)
output by oracle O, the adversary labels each
sample (cid:126)x ∈ Sρ in its initial substitute training set
Sρ.
– Training (4): The adversary trains the architec-
ture chosen at step (2) using substitute training
set Sρ in conjunction with classical training tech-
niques.
– Augmentation (5): The adversary applies our
augmentation technique on the initial substitute
training set Sρ to produce a larger substitute train-
ing set Sρ+1 with more synthetic training points.
This new training set better represents the model’s
decision boundaries. The adversary repeats steps
(3) and (4) with the augmented set Sρ+1.
Step (3) is repeated several times to increase the substitute
DNN’s accuracy and the similarity of its decision boundaries
with the oracle. We introduce the term substitute training
epoch, indexed with ρ, to refer to each iteration performed.
This leads to this formalization of the Jacobian-based Dataset
Augmentation performed at step (5) of our substitute training
algorithm to ﬁnd more synthetic training points:
Sρ+1 = {(cid:126)x + λ · sgn(JF [ ˜O((cid:126)x)]) : (cid:126)x ∈ Sρ} ∪ Sρ
(4)
where λ is a parameter of the augmentation: it deﬁnes the
size of the step taken in the sensitive direction identiﬁed by
the Jacobian matrix to augment the set Sρ into Sρ+1.
509Figure 3: Training of the substitute DNN F : the attacker (1) collects an initial substitute training set S0 and (2) selects
an architecture F . Using oracle ˜O, the attacker (3) labels S0 and (4) trains substitute F . After (5) Jacobian-based dataset
augmentation, steps (3) through (5) are repeated for several substitute epochs ρ.
4.2 Adversarial Sample Crafting
Once the adversary trained a substitute DNN, it uses it to
craft adversarial samples. This is performed by implementing
two previously introduced approaches described in [4, 9].
We provide an overview of the two approaches, namely the
Goodfellow et al. algorithm and the Papernot et al. algorithm.
Both techniques share a similar intuition of evaluating the
model’s sensitivity to input modiﬁcations in order to select
a small perturbation achieving the misclassiﬁcation goal1.
Goodfellow et al. algorithm: This algorithm is also
known as the fast gradient sign method [4]. Given a model
F with an associated cost function c(F, (cid:126)x, y), the adversary
crafts an adversarial sample (cid:126)x∗ = (cid:126)x+δ(cid:126)x for a given legitimate
sample (cid:126)x by computing the following perturbation:
δ(cid:126)x = ε sgn(∇(cid:126)xc(F, (cid:126)x, y))
(5)
where perturbation sgn(∇(cid:126)xc(F, (cid:126)x, y)) is the sign of the model’s
cost function 2 gradient. The cost gradient is computed with
respect to (cid:126)x using sample (cid:126)x and label y as inputs. The value
of the input variation parameter ε factoring the sign matrix
controls the perturbation’s amplitude. Increasing its value
increases the likelihood of (cid:126)x∗ being misclassiﬁed by model
F but on the contrary makes adversarial samples easier to
detect by humans. In Section 6, we evaluate the impact of
parameter ε on the successfulness of our attack.
Papernot et al. algorithm: This algorithm is suitable
for source-target misclassiﬁcation attacks where adversaries
seek to take samples from any legitimate source class to any
chosen target class [9]. Misclassiﬁcation attacks are a special
case of source-target misclassiﬁcations, where the target class
can be any class diﬀerent from the legitimate source class.
Given model F , the adversary crafts an adversarial sample
(cid:126)x∗ = (cid:126)x + δ(cid:126)x for a given legitimate sample (cid:126)x by adding a
perturbation δ(cid:126)x to a subset of the input components (cid:126)xi.
To choose input components forming perturbation δ(cid:126)x, com-
ponents are sorted by decreasing adversarial saliency value.
The adversarial saliency value S((cid:126)x, t)[i] of component i for
an adversarial target class t is deﬁned as:
((cid:126)x)  0
(6)
(cid:40) 0 if ∂Ft
∂(cid:126)xi
∂Ft
∂(cid:126)xi
((cid:126)x)
S((cid:126)x, t)[i] =
1Our attack can be implemented with other adversarial ex-
ample algorithms. We focus on these two in our evaluation.
2As described here, the method causes simple misclassiﬁca-
tion. It has been extended to achieve chosen target classes.
(cid:105)
(cid:104) ∂Fj
∂(cid:126)xi
where matrix JF =
ij
is the model’s Jacobian matrix.
Input components i are added to perturbation δ(cid:126)x in order
of decreasing adversarial saliency value S((cid:126)x, t)[i] until the
resulting adversarial sample (cid:126)x∗ = (cid:126)x + δ(cid:126)x is misclassiﬁed
by F . The perturbation introduced for each selected input
component can vary: greater perturbation reduce the number
of components perturbed to achieve misclassiﬁcation.
Each algorithm has its beneﬁts and drawbacks. The Good-
fellow algorithm is well suited for fast crafting of many ad-
versarial samples with relatively large perturbations thus
potentially easier to detect. The Papernot algorithm reduces
perturbations at the expense of a greater computing cost.
5. VALIDATION OF THE ATTACK
We validate our attack against remote and local classiﬁers.
We ﬁrst apply it to target a DNN remotely provided by
MetaMind, through their API3 that allows a user to train
classiﬁers using deep learning. The API returns labels pro-
duced by the DNN for any given input but does not provide
access to the DNN. This corresponds to the oracle described
in our threat model. We show that:
• An adversary using our attack can reliably force the
DNN trained using MetaMind on MNIST [7] to mis-
classify 84.24% of adversarial examples crafted with a
perturbation not aﬀecting human recognition.
• A second oracle trained locally with the German Traﬃc
Signs Recognition Benchmark (GTSRB) [13], can be
forced to misclassify more than 64.24% of altered inputs
without aﬀecting human recognition.
5.1 Attack against the MetaMind Oracle
Description of the Oracle: We used the MNIST hand-
written digit dataset to train the DNN [7].
It comprises