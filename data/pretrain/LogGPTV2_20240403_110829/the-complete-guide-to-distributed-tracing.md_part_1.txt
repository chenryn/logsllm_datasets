The complete
guide to
distributed
tracing
Daniel “Spoons” Spoonhower
Lightstep CTO and Co-Founder
The Complete Guide to Distributed Tracing 1
About Daniel “Spoons”
Spoonhower
Lightstep CTO and Co-Founder
Spoons (or, more formally, Daniel
Spoonhower) has published
papers on the performance
of parallel programs, garbage
collection, and real-time
Follow on
Twitter programming. He has a PhD in
@save_spoons
programming languages but still
Connect on
LinkedIn hasn’t found one he loves.
/spoons
The Complete Guide to Distributed Tracing 2
Introduction
DevOps and distributed
architectures
For DevOps to be successful, especially in larger
organizations with mission-critical demands, it’s
imperative to enable loosely coupled work across
teams for fast, independent problem solving.
Achieving this requires organizations to build strong abstractions
between not only their teams but also their services. On the human
level, this means developing common tools, strategies, and best
practices for independent, inter-team communication. At the software
level, these abstractions take the form of orchestration layers,
common build and deployment tools, and shared monitoring services.
But where do these abstractions leave your teams when it comes to
observability? How do you understand what your users are seeing? Is
that something that can be solved with metrics? What about not just
in your software, but in your organization? How do you communicate
across teams? How do you find the right team to communicate with
during an incident?
The Complete Guide to Distributed Tracing 3
These questions and others will be answered as we cover how to
establish true observability with distributed tracing:
1. How to decide when to take action by focusing on symptoms
that directly impact users
2. How to connect cause and effect, whether that means
performing root cause analysis during an incident, or proactively
improving efficiency and customer experience with performance
optimization work
A review of distributed tracing
Let’s establish some common language before diving into how to best
leverage distributed tracing in your software.
Distrubuted tracing is a diagnostic technique that reveals how a set
of services coordinate to handle individual user requests. Tracing
is a critical part of observability, as it provides context for other
telemetry: for example, helping to define which metrics would be
most valuable in a given situation.
Distrubuted tracing is a diagnostic technique that
reveals how a set of services coordinate to handle
individual user requests.
The Complete Guide to Distributed Tracing 4
A single trace shows the activity for an individual transaction or
request as it propagates through an application, all the way from
the browser or mobile device down to the database and back. In
aggregate, a collection of traces can show which backend service or
database is having the biggest impact on performance as it affects
your users’ experiences.
Software architectures built on microservices and serverless offer
advantages to application development, but at the same time lead to
reduced visibility.
In distributed systems, teams can manage, monitor, and operate their
individual services more easily, but it can be more difficult to keep
track of global system behavior. During an incident, a customer may
report an issue with a transaction that is distributed across several
microservices, serverless functions, and teams. Without tracing, it
becomes nearly impossible to separate the service that is responsible
for the issue from those that are merely affected by it.
The Complete Guide to Distributed Tracing 5
With tracing, the focus is on the bigger picture, providing end-to-
end visibility, and revealing service dependencies — showing how
the services depend on and respond to each other. By being able to
visualize transactions in their entirety, you can compare anomalous
traces against performant ones to see the differences in behavior,
structure, and timing. This information enables you to better
identify the culprit and shift directly to addressing a bad change or
performance bottlenecks.
Anatomy of a trace
In distributed tracing, a single trace contains a series of tagged
time intervals called spans. A span can be thought of as a single
unit of work. Spans have a start and end time, and optionally may
include other metadata like logs or tags that can help classify “what
happened.” Spans have relationships between one another, including
parent-child relationships, which are used to show the specific path
a particular transaction takes through the numerous services or
components that make up the application.
• Trace: represents an end-to-end request; made up of single or
multiple spans
• Span: represents work done by a single-service with time intervals
and associated metadata; the building blocks of a trace
• Tag: metadata to help contextualize a span
The point of traces is to provide a request-centric view. So, while the
architectures that we are choosing enable teams and services to work
independently, distributed tracing provides a central resource that
enables all teams to understand issues from the user’s perspective.
The Complete Guide to Distributed Tracing 6
Understanding trace data
In summary, a single trace represents an end-to-end request and may
consist of one or thousands of spans. A span is a timed event with
some metadata attached, representing the work done by a particular
service as part of this request.
With the basics covered, let’s take a closer look at a trace. Each of
the lines in the image above represents a span. Time is moving from
left to right, so the “start” is on the left and the “finish” is on the right.
What other information is provided? We know the duration of the
span, or how long it took to “do the work,” the name of the service
doing the work, and the “operation,” which could be the endpoint or
some other description of the work that is being done. The yellow line
highlights the combination of work that determined how long it took
the overall action to complete, usually called the “critical path.”
The Complete Guide to Distributed Tracing 7
For a more detailed look of the mechanics of tracing, check out
this technical report written by Lightstep CEO and Co-Founder
Benjamin Sigelman about Dapper, a large-scale distributed tracing
infrastructure he developed while at Google.
Let users drive decision making
A few questions for service owners, engineers, and operators: What
do your users expect from your application? How are they going
to measure your success? And if you don’t hit those targets, or if
you don’t meet those user expectations, what will they do? Are you
prepared to handle these failures? These questions are vital for
addressing service performance, and their answers
can be formalized through service levels.
Service levels provide an important context for what to prioritize
when responding to shifting performance demands — and distributed
tracing can help you manage your service levels.
Service levels
• Service Level Indicator (SLI): measurable data such as latency,
uptime, and error rate
• Service Level Objective (SLO): defines the target for SLIs. For
example, p99 latency < 1s; 99.9% uptime; <1% errors
• Service Level Agreement: financial or contractual consequences
for failing to meet SLOs. Refunds, cancellations, etc
From an engineering point of view, service levels are important
because they reflect customer expectations. What metrics do your
customers care about? What kinds of failures will stand in the way of
your customers’ success?
The Complete Guide to Distributed Tracing 8
Choosing the correct scope for your
service levels
What’s the correct level of granularity for your SLI? Is it at the
operation level? At the service level? It might be easy to think, “Well,
I own Service X, so I’m going to set an SLI for that service to track
performance. Latency is important for my users, so I will track the
latency of Service X.” It is important to pause here and think about
the different kinds of things that your service does.
Let’s look at a simple example: you have a single service that handles
two endpoints, one that reads and one that writes. The read endpoint
is likely going to be a lot faster than the write endpoint. You can set
the Service Level Indicator (SLI) wide enough to cover both, but you
will lose insight into the faster read operation. Alternatively, you can
set it tight enough to cover the read, but you will be constantly missing
your Service Level Objective (SLO) because it won’t be taking the write
operation into consideration. So for a service like this, it might be best
two set two independent SLIs: one for each operation.
Measure the correct type of performance
Once scope is settled, the next step is choosing the right types of
performance to measure:
• High Percentile Latency: outlier latency issues can ‘bubble’ up the
stack and compromise performance
• Error Rate: rate of errors in the system can offer
real-time alerts that there is a problem
• Throughput: setting a service level around throughput is tricky,
because it can be independent of user behavior, but is ultimately
still an important measure of system health
The Complete Guide to Distributed Tracing 9
• Saturation: what percentage of your resources are being utilized
to handle traffic? This is especially important for understanding if
database or shard resources need to be scaled up or altered
When choosing your SLIs, focus on what is critical to your users.
Imagine a software company handling “low frequency” trading,
essentially replacing a pen and paper process for fulfilling trades.
Because this company’s services are replacing such a slow, manual
process, the service might be down for 10 to 20 minutes without
much complaint. Correctness, on the other hand — making the right
trades at the right time — is paramount to this company’s users.
It would then seem wise to focus resources on achieving 100%
correctness at the expense of uptime, if necessary.
The Complete Guide to Distributed Tracing 10
Understanding latency percentiles in
distributed systems
Latency percentiles are especially important in distributed systems.
Suppose the graphic below is a simple representation of a search
system that spans out across many shards.
Let’s say the services at the very bottom of the stack have an
average latency of one millisecond, but the 99th percentile latency is
one second.
To be clear, a 99th percentile latency of one second means that one
percent of the traces are one thousand times slower than the average
request (1 ms).
Now if an end user request — that is, something coming from the top
of this system — hits only one of those bottom services, one percent
of those end user requests will also take a second or more. But
there are some subtleties to take note of here: as the complexity of
handling those requests increases, say, by increasing the number of
service instances at the bottom of the stack that have to participate
The Complete Guide to Distributed Tracing 11
in that request, this 99th percentile latency is going to have a bigger
and bigger impact.
If a request requires a hundred of these bottom-of-the-stack services
to participate in answering one of these search queries, then 63% of
end user requests are going to take more than a second. So, even
though only one percent of the requests at the very bottom of the
stack are experiencing this one second latency, more than half of the
requests that the users initiate would take more than a second.
The takeaway? As our systems increase in complexity, “rare
behaviors” like 99th percentile latency can get magnified through that
complexity. Accordingly, one of the really important reasons to be
measuring 99th percentile metrics throughout the entire stack is to
understand how those latencies are going to be magnified as as they
travel back up the stack.