# Design-Time Reliability Analysis of Distributed Fault Tolerance Algorithms

**Authors:**
- Elizabeth Latronico
- Philip Koopman

**Affiliation:**
Carnegie Mellon University, Pittsburgh, PA, USA

**Contact:**
- PI:EMAIL
- PI:EMAIL

## Abstract
Designing a distributed fault tolerance algorithm requires careful analysis of both fault models and diagnosis strategies. A system will fail if there are too many active faults, particularly active Byzantine faults. Conversely, overly aggressive fault convictions can leave inadequate redundancy, also leading to system failure. For high reliability, the hybrid fault model and diagnosis strategy must be tuned to the types and rates of faults expected in the real world. We examine this balancing problem for two common types of distributed algorithms: clock synchronization and group membership. By considering two clock synchronization algorithms, we highlight the importance of choosing an appropriate hybrid fault model. Three group membership service diagnosis strategies are used to demonstrate the benefits of distinguishing between permanent and transient faults. In most cases, the probability of failure is dominated by one fault type. Identifying the dominant cause of failure allows for tailoring the algorithm appropriately at design time, resulting in significant reliability gains.

## 1. Introduction
Distributed fault tolerance algorithms are essential for systems requiring high levels of reliability, where a centralized component might present a single point of failure. For example, aviation fly-by-wire and automotive drive-by-wire networks need to reliably deliver data despite the presence of faults. These algorithms achieve fault tolerance through a combination of redundancy, diagnosis, and fault removal.

An algorithm's maximum fault assumption specifies the number of active faults that can be tolerated. For a formally proven algorithm, the system may fail if this assumption is violated. Fault diagnosis procedures aim to keep the number of active faults within the bounds of the maximum fault assumption by removing suspected faulty nodes. However, if fault-free nodes are incorrectly diagnosed as faulty and removed, the risk of inadequate redundancy increases.

Given this tension, how will an algorithm perform under a real-world fault profile? We introduce a methodology to measure the reliability of an algorithm's maximum fault assumption, focusing on two key design decisions:
1. Selecting a hybrid fault model that accurately reflects physical fault sources.
2. Implementing a fault diagnosis strategy that differentiates between transient and permanent faults. Transient faults often expire quickly and are not caused by a node, such as channel noise, which corrupts a frame but does not benefit from removing the sending node.

We apply our methodology to two case studies:
- The clock synchronization case study reviews two hybrid fault models.
- The group membership case study investigates three fault diagnosis strategies.

To perform reliability analysis, we first define a reusable physical fault model based on real-world fault arrival rates and types. We then show how to construct the reliability models, which can be customized to include other types of faults. Hundreds of configurations are studied using the NASA Langley Semi-markov Unreliability Range Evaluator (SURE) tool set [6], [7]. By examining many configurations, we can make recommendations without needing precise failure rate data, which is usually unavailable at design time. Our findings indicate that the Strictly Omissive hybrid fault model and a diagnosis strategy that discriminates between permanent and transient faults improve reliability.

## 2. Protocol Overview and Related Work
We study the clock synchronization service of the FlexRay protocol and variants of the group membership strategy of the Time Triggered Protocol, Class C (TTP/C) [13], [37]. FlexRay is designed for safety-critical automotive applications like brake-by-wire, where electronic connections replace mechanical linkages [13]. The FlexRay protocol provides distributed clock synchronization among member nodes. TTP/C is a leading multipurpose safety-critical protocol slated for use in avionics and other domains [37]. It provides a distributed membership service in addition to clock synchronization. Both protocols use a broadcast Time Division Multiple Access (TDMA) sending scheme, with nodes transmitting frames in a predetermined static schedule on dual redundant channels.

Related work has emphasized the need to measure the reliability of specifications. Powell defines "assumption coverage" as the probability that a failed component's behavior will be covered by one of the assumed failure modes [29]. Powell demonstrates that adding nodes may decrease reliability because it increases the fault rate [29]. Bauer, Kopetz, and Puschner address the assumption coverage of TTP/C, noting that every fault-tolerant system relies on a minimum number of correct components [4]. Even an optimal system may fail if too many coincident faults occur [4]. Per Powell's definition, we assume all faults are detected through value or timing checks, but coincident faults may exceed the maximum fault assumption. Our previous work examined the assumption reliability of the NASA Scalable Processor Independent Design for Electromagnetic Resilience (SPIDER) protocols in the face of coincident faults [22].

The design-time reliability analysis we perform complements existing work in fault injection. Since exhaustive physical testing is infeasible for ultra-reliable systems [8], other validation approaches are needed. One use of fault injection is to verify that the implementation fulfills its requirements (i.e., faults within the maximum fault assumption do not cause unacceptable errors). Ademaj, Sivencrona, Bauer, and Torin investigate propagated faults in the TTP/C-C1 version of the TTP/C communication controller [1]. Through software and heavy-ion fault injection, they reported the percentages of different types of observed errors [1], [33].

We study four sources of physical faults: permanent hardware faults, single event effects, bit error rate, and electromagnetic interference. Table 1 lists the representative fault types and rates for the aviation domain. For permanent hardware faults, we use a fault rate of \(10^{-5}\) per hour for a node (large fault containment region) and \(10^{-6}\) for a star coupler or bus (small region) [38]. We test a link fault range of \(10^{-8}\) to \(10^{-6}\) per hour, which is slightly conservative compared to [38] but slightly optimistic compared to [16]. The single event effects class includes faults due to particle collisions. Single Event Latchup (SEL) is the dominant permanent effect [32], with observed SEL rates around \(10^{-8}\) to \(10^{-6}\) latchups/device-hour [26]. Single Event Upset (SEU) is the most prevalent transient effect [11], with measured SEU rates from \(1 \times 10^{-8}\) to \(4 \times 10^{-10}\) upsets/bit-hour [26]. The bit error rate class includes faults from jitter and amplitude disturbances on the network. Three optical standards give worst-case BERs ranging from \(10^{-12}\) to \(10^{-10}\) [9], [23], [35]; we study a less pessimistic range of \(10^{-13}\) to \(10^{-11}\). The fourth class, electromagnetic interference, includes correlated burst errors [30], [17], [18]. We focus on lightning strikes, estimated at one strike per 2500 flight hours [12].

Other related topics include protocol comparisons and reliability estimation methods. Rushby argues that any fault-tolerant system must be designed and evaluated against a specific fault hypothesis that describes the number, type, and arrival rate of the faults it is intended to tolerate [31]. Kopetz discusses the fault tolerance abilities of TTP/C vs. FlexRay [19], and the PALBUS project reviews several data buses, including an early version of TTP/C [34]. For reliability estimation, the Probabilistic Model Checker supports probabilistic assurance of properties, including those modeled through continuous time Markov chains [20].

## 3. Fault Models and Mappings
To evaluate the reliability of a proposed algorithm, we map the physical fault model to the maximum fault assumption hybrid fault model. The maximum fault assumption (MFA) states the maximum number of active faults such that guarantees can be proven to hold. Physical faults map to one or more of the hybrid fault types. We provide the hybrid fault models and mappings for the two types of algorithms studied: clock synchronization and group membership.

Table 2 lists the system parameters needed for model transition rates. FlexRay and TTP/C both support 1 MBit/sec bandwidth, with plans to support 10 MBit/sec and possibly 25 MBit/sec [13], [37]. The round duration is determined by the shortest message period required by the system, typically 10 ms. A frame duration of 0.1 ms would allow 100 frames of 100 bits each to be sent per second. The fault arrival rate due to SEU faults depends on the number of bits that could be affected. We assume 64 kilobytes, or \(64 \times 2^{10} \times 8\) bits, comparable to the size of protocol controllers. For example, the TTP-C2NF revision 1.2 chip has 40 kBytes of SRAM and 32 kBytes of ROM [2]. We perform sensitivity analysis for 256 kBytes.

### 3.1 Hybrid Fault Models
A hybrid fault model classifies faulty nodes according to fault severity with respect to a group of observers. The Byzantine fault model from Lamport, Shostak, and Pease places no restrictions on the behavior of a faulty node, covering all possible faulty behaviors and requiring \(3n + 1\) nodes to tolerate \(n\) faulty nodes [21]. However, many less severe faults are easier to tolerate, as noted by Meyer and Pradhan [24]. Since fault definitions vary, we use the definitions from the NASA Langley Scalable Processor Independent Design for Electromagnetic Resilience (SPIDER) safety-critical protocol suite [25]. The SPIDER definitions are based on the Thambidurai and Park fault model [36]. Additionally, we include strictly omissive faults, a useful category proposed by Azadmanesh and Kieckhafer [3].

- **Good (G)** [25]: Each good node behaves according to specification; it always sends valid messages.
- **Benign (B)** [25]: Each benign faulty node either sends detectably incorrect messages to every receiver or sends valid messages to every receiver.
- **Symmetric (S)** [25]: A symmetric faulty node may send arbitrary messages, but each receiver receives the same message.
- **Asymmetric (A)** [25]: An asymmetric (Byzantine) faulty node may send arbitrary messages that may differ for various receivers.
- **Strictly Omissive Asymmetric (A)** [3]: "A strictly omissive faulty node can send a single correct value to some processes and no value to all other processes." A fault can "garble a message in transit, but not in an undetectable manner."

To measure the reliability of a configuration, a Markov model is created with states given in terms of the hybrid fault model. A fault may be (P) Permanent or (T) Transient. Abbreviations for the state of a single node or channel are (G) Good, (B) Benign, (S) Symmetric, and (A) Asymmetric/Strictly Omissive Asymmetric. The hybrid fault model is applied to components in three ways: a node may become faulty (subscript N), a channel may become faulty (subscript C), or a node may appear faulty if both channels are simultaneously faulty (subscript NC). All perceived node faults due to channel faults are transient (since if both channels are permanently faulty, the system has failed). For example, PSN would be a Permanent Symmetric faulty Node, and TAN C would be a Node affected by Channel faults that appears to be Transient Asymmetric faulty. A node can be convicted (CONV) and permanently removed from the group. While not explicitly represented in the hybrid fault model, convicted nodes are tracked in the reliability models since the total number of nodes is conserved.

Transitions between states are specified with an exponential transition rate, assuming uncorrelated fault arrivals. An exponential transition rate is specified in the form \(e^{-\lambda t}\), where \(\lambda\) is the transition rate per unit time, and \(t\) is time (here, in hours). A single transition may change the state of one or more nodes or channels. We represent correlated faults (such as lightning) with transitions that alter the state of multiple nodes or channels.

We use the NASA Langley Semi-markov Unreliability Range Evaluator (SURE) [6] reliability modeling tool set for this analysis. The SURE tool calculates a reliability bound, where the SURE bounding theorems have algebraic solutions. SURE was designed to evaluate fault-tolerant systems and handles models with multiple fault arrivals and recoveries well due to the algebraic nature of the solution engine. Iterative solution methods may take a long time to converge, as the probability of violating the maximum fault assumption can be very low (e.g., \(10^{-8}\) or less is not unusual). While a detailed discussion of modeling tools is outside the scope of this paper, Butler and Johnson describe the mathematics and provide numerous modeling examples [6], [7]. Additionally, the methodology is not limited to this tool suite.

### 3.2 Clock Synchronization Model and Mapping
The reliability of a protocol depends in part on how the protocol's hybrid fault model classifies faults. We demonstrate this by comparing the Welch and Lynch clock synchronization algorithm to the improved strictly omissive asymmetric algorithm by Azadmanesh and Kieckhafer [3]. The FlexRay clock synchronization algorithm is based on the formally proven algorithm from Welch and Lynch [39], an approximate agreement algorithm using sets of local clocks.

## 4. Results
[Results section to be filled in with the specific findings from the case studies and reliability analyses.]

## 5. Conclusions
[Conclusions section to be filled in with a summary of the key findings, implications, and future work.]

---

**Tables:**

**Table 1. Physical Faults and Rates Studied**

| Physical Fault Type | Rates Studied |
|---------------------|---------------|
| Perm. Node [38]     | \(10^{-5}\)/hr |
| Perm. Bus/Star [38] | \(10^{-6}\)/hr |
| Perm. Link [38], [16] | \(4 \times 10^{-4}\)/hr |
| SEL [26]            | \(10^{-8}\), \(10^{-7}\), \(10^{-6}\)/device-hr |
| SEU [26], [11]      | \(1 \times 10^{-8}\) to \(4 \times 10^{-10}\)/bit-hr |
| BER [9], [23], [35] | \(10^{-13}\) to \(10^{-11}\) err/bit |
| Lightning [12]      | 1 strike per 2500 flight hours |

**Table 2. System Parameters and Values**

| Parameter        | Value                |
|------------------|----------------------|
| Bandwidth        | \(1 \times 10^6\) bits/sec |
| Round Duration   | 10 ms                |
| Frame Duration   | 0.1 ms               |
| Frames/hour      | \(3.6 \times 10^7\)  |
| Memory/Node      | 64 kilobytes         |
| Channels         | 2                    |
| Nodes            | 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14 |

---

This revised text aims to be more clear, coherent, and professional, with a structured format and improved readability.