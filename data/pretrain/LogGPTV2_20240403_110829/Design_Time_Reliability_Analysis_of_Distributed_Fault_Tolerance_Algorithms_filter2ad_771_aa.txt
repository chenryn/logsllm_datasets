title:Design Time Reliability Analysis of Distributed Fault Tolerance Algorithms
author:Elizabeth Latronico and
Philip Koopman
Design Time Reliability Analysis of Distributed Fault Tolerance Algorithms
Elizabeth Latronico, Philip Koopman
Carnegie Mellon University
Pittsburgh, PA, USA
PI:EMAIL, PI:EMAIL
Abstract
Designing a distributed fault tolerance algorithm re-
quires careful analysis of both fault models and diagnosis
strategies. A system will fail if there are too many active
faults, especially active Byzantine faults. But, a system will
also fail if overly aggressive convictions leave inadequate
redundancy. For high reliability, an algorithm’s hybrid fault
model and diagnosis strategy must be tuned to the types
and rates of faults expected in the real world. We exam-
ine this balancing problem for two common types of dis-
tributed algorithms: clock synchronization and group mem-
bership. We show the importance of choosing a hybrid fault
model appropriate for the physical faults expected by con-
sidering two clock synchronization algorithms. Three group
membership service diagnosis strategies are used to demon-
strate the beneﬁt of discriminating between permanent and
transient faults. In most cases, the probability of failure is
dominated by one fault type. By identifying the dominant
cause of failure, one can tailor an algorithm appropriately
at design time, yielding signiﬁcant reliability gain.
1
Introduction
Distributed fault tolerance algorithms are used for many
systems that require high levels of reliability, where a cen-
tralized component might present a single point of failure.
For example, aviation ﬂy-by-wire and automotive drive-by-
wire networks need to reliably deliver data despite the pres-
ence of faults. These algorithms tolerate faults through a
combination of redundancy, diagnosis and fault removal.
An algorithm’s maximum fault assumption states the
number of active faults that can be tolerated. For a formally
proven algorithm, the system may fail if this assumption is
violated. Fault diagnosis procedures aim to keep the num-
ber of active faults within the bounds of the maximum fault
assumption by removing suspected faulty nodes. However,
if fault-free nodes are incorrectly diagnosed as faulty and
removed, the risk of inadequate redundancy increases.
Given this tension, how will an algorithm perform under
a real-world fault proﬁle? We introduce a methodology to
measure the reliability of an algorithm’s maximum fault as-
sumption, focusing on two types of design decisions. First,
it is important to pick a hybrid fault model that corresponds
well with the physical fault sources. A ‘good’ hybrid fault
model deﬁnes easy-to-handle categories for many of the
physical faults, thereby reducing the risk of failure due to an
active fault. Second, a fault diagnosis strategy should treat
transient and permanent faults differently. Many transient
faults expire quickly and are not caused by a node. For ex-
ample, channel noise might corrupt a frame, but removing
the sending node will not prevent future problems.
We apply our methodology to two case studies. The
clock synchronization case study reviews two hybrid fault
models. The group membership case study investigates
three fault diagnosis strategies. To perform reliability anal-
ysis, we ﬁrst deﬁne a reusable physical fault model based on
real-world fault arrival rates and types. Next, we show how
to construct the reliability models. The models can be cus-
tomized to include other types of faults; we give an extensi-
bility example. Hundreds of conﬁgurations are studied with
the NASA Langley Semi-markov Unreliability Range Eval-
uator (SURE) tool set [6], [7]. By examining many conﬁg-
urations, we can make recommendations without needing
precise failure rate data that is usually unavailable at design
time. We ﬁnd that the Strictly Omissive hybrid fault model
improves reliability, as does a diagnosis strategy that dis-
criminates between permanent and transient faults.
Section 2 reviews protocols and related work. Section 3
discusses the physical fault model, the hybrid fault model,
and the mapping between the two. Section 4 presents re-
sults, and Section 5 summarizes conclusions.
2 Protocol Overview and Related Work
We study the clock synchronization service of the
FlexRay protocol and variants of the group membership
strategy of the Time Triggered Protocol, Class C (TTP/C)
[13], [37]. FlexRay is intended for safety-critical automo-
tive applications such as brake-by-wire, where electronic
connections will replace the mechanical linkages between
the brake pedal and the braking actuators [13]. The FlexRay
protocol provides distributed clock synchronization among
member nodes. TTP/C is a leading multipurpose safety-
critical protocol slated for use in avionics applications and
other domains [37]. TTP/C provides a distributed mem-
bership service in addition to clock synchronization. Both
protocols use a broadcast Time Division Multiple Access
(TDMA) sending scheme, where nodes transmit frames in
a predetermined static schedule on dual redundant channels.
Related work has noted the need to measure the reliabil-
ity of speciﬁcations. Powell deﬁnes ‘assumption coverage’
as the probability that a failed component’s behavior will be
covered by one of the assumed failure modes [29]. Powell
demonstrates that adding nodes may decrease the reliabil-
ity, because adding nodes also increases the fault rate [29].
Bauer, Kopetz and Puschner address the assumption cover-
age of TTP/C, noting that “every fault-tolerant system relies
on the existence of a minimum number of correct compo-
nents [4].” Even an optimal system may fail in the event of
too many coincident faults [4]. Per Powell’s deﬁnition, we
assume that all faults are covered (detected through value or
timing checks), but coincident faults may exceed the maxi-
mum fault assumption. Our previous work examined the as-
sumption reliability of the NASA Scalable Processor Inde-
pendent Design for Electromagnetic Resilience (SPIDER)
protocols in the face of coincident faults [22].
The design time reliability analysis we perform comple-
ments existing work in the area of fault injection. Since ex-
haustive physical testing is infeasible for ultra-reliable sys-
tems [8], other validation approaches are needed. One use
of fault injection is to verify that the implementation ful-
ﬁls its requirements (i.e., faults within the maximum fault
assumption do not cause unacceptable errors). Ademaj,
Sivencrona, Bauer, and Torin investigate propagated faults
in the TTP/C-C1 version of the TTP/C communication con-
troller [1]. Through software and heavy-ion fault injection,
that work reported the percentages of different types of ob-
served errors (slightly off speciﬁcation, reintegration, asym-
metric, and babbling idiot) [1], [33].
We study four sources of physical faults: permanent
Fault injection has also been used to test dependability
under conditions not covered by the maximum fault as-
sumption. Herout, Racek, and Hlaviˇcka tested a C-based
reference model of the TTP/C protocol coupled with a set of
generic and automotive applications [15]. Part of that work
investigated robustness to burst faults that did not conform
to TTP/C’s maximum fault assumption (the single fault hy-
pothesis) [15]. Our work estimates the probability of mul-
tiple simultaneous faults exceeding the maximum fault as-
sumption. We base our fault arrival rates on real-world fault
occurrence data, instead of random parameters as is typi-
cally done in requirements conformance testing.
Additionally, we show that our methodology and relia-
bility models are extensible, by showing how to incorpo-
rate one of the fault types from the DBench Dependabil-
ity Benchmarking project [10]. While our fault model con-
tains a useful set of fault types, certainly not every fault
is included. One of the interesting behaviors the DBench
project discovered was component failure due to accumu-
lated errors (for example, due to corruption of hidden regis-
ters) [10]. We discuss two ways to represent this behavior:
direct extension and phased missions.
hardware faults, single event effects, bit error rate, and elec-
tromagnetic interference. These types and rates, in Table
1, are representative of the aviation domain. For permanent
hardware faults, we use a fault rate of 10−5/hr for a node
(large fault containment region) and 10−6 for a star cou-
pler or bus (small region) [38]. We test a link fault range
of 10−8/hr to 10−6/hr, which is slightly conservative com-
pared to [38] but slightly optimistic compared to [16]. The
single event effects class includes faults due to particle col-
lisions. Single Event Latchup (SEL) is the dominant per-
manent effect [32], with observed SEL rates around 10−8
to 10−6 latchups/device-hr [26]. Single Event Upset (SEU)
is the most prevalant transient effect [11], with measured
SEU rates from 1*10−8 to 4*10−10 upsets/bit-hr [26]. The
bit error rate class includes faults from jitter and amplitude
disturbances on the network. Three optical standards give
worst-case BERs ranging from 10−12 to 10−10 [9], [23],
[35]; we study a less pessimistic range of 10−13 to 10−11.
The fourth class, electromagnetic interference, includes cor-
related burst errors [30], [17], [18]. We focus on lightning
strikes, estimated at one strike per 2500 ﬂight hours [12].
Other related topics include protocol comparisons and
reliability estimation methods. In his comparison of TTP/C,
the NASA SPIDER protocols, the Honeywell SAFEbus net-
work, and FlexRay, Rushby argues that “Any fault-tolerant
system must be designed and evaluated against a speciﬁc
fault hypothesis that describes the number, type, and arrival
rate of the faults it is intended to tolerate [31].” Kopetz dis-
cusses the fault tolerance abilities of TTP/C vs. Flexray in
[19], and the PALBUS project reviews a number of data
buses including an early version of TTP/C [34]. For relia-
bility estimation, the Probabilistic Model Checker supports
probabilistic assurance of properties, including properties
modeled through continuous time Markov chains [20].
3 Fault Models and Mappings
To evaluate the reliability of a proposed algorithm, we
map the physical fault model to the maximum fault assump-
tion hybrid fault model. The maximum fault assumption
(MFA) states the maximum number of active faults such
that guarantees can be proven to hold. Physical faults map
to one or more of the hybrid fault types. We give the hybrid
fault models and mappings for the two types of algorithms
studied: clock synchronization and group membership.
Table 2 lists system parameters needed for model tran-
sition rates. FlexRay and TTP/C both support 1 MBit/sec
bandwidth, with plans to support 10 MBit/sec and possibly
25 MBit/sec [13], [37]. The round duration is determined
by the shortest message period required by the system, since
each node typically sends exactly once per round [37], [27].
A message period of 10 ms is representative of many em-
bedded networks. A frame duration of 0.1 ms would allow
100 frames of 100 bits each to be sent per second. The
fault arrival rate due to SEU faults depends on the number
of bits that could be affected. We assume 64 kilobytes, or
64*(210)*8 bits. This is comparable to the size of protocol
controllers. For example, the TTP-C2NF revision 1.2 chip
Table 1. Physical Faults and Rates Studied
Physical Fault Type
Rates Studied
10−5/hr
10−6/hr
4*10−4 /hr
Perm. Node [38]
Perm. Bus/Star [38]
Perm. Link [38], [16]
SEL [26]
SEU [26], [11]
BER [9], [23], [35]
Lightning [12]
10−8, 10−7, 10−6 /hr
10−8, 10−7, 10−6 /device-hr
10−10, 10−9, 10−8 /bit-hr
10−13, 10−12, 10−11 err/bit
Table 2. System Parameters and Values
Parameter
Bandwidth
1*106 bits/sec
Value
Round Duration
Frame Duration
Frames/hour
Memory/Node
Channels
Nodes
10 ms
0.1 ms
3.6*107 (3600000 ms / 0.1ms)
64 kilobytes
2
4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14
has 40 kBytes of SRAM and 32 kBytes of ROM [2]. We
perform sensitivity analysis for 256 kBytes.
3.1 Hybrid Fault Models
A hybrid fault model classiﬁes faulty nodes according
to fault severity with respect to a group of observers. The
Byzantine fault model from Lamport, Shostak, and Pease
placed no restrictions on the behavior of a faulty node,
thereby covering all possible faulty behaviors and requir-
ing 3n + 1 nodes to tolerate n faulty nodes [21]. However,
many less severe faults are easier to tolerate, as noted by
Meyer and Pradhan [24]. Since fault deﬁnitions vary, we
use the deﬁnitions from the NASA Langley Scalable Pro-
cessor Independent Design for Electromagnetic Resilience
(SPIDER) safety-critical protocol suite [25]. The SPIDER
deﬁnitions are based on the Thambidurai and Park fault
model [36]. Also, we include strictly omissive faults, a use-
ful category proposed by Azadmanesh and Kieckhafer [3].
Good (G) [25] Each good node behaves according to speci-
ﬁcation; that is, it always sends valid messages.
Benign (B) [25] Each benign faulty node either sends de-
tectably incorrect messages to every receiver, or sends valid
messages to every receiver.
Symmetric (S) [25] A symmetric faulty node may send ar-
bitrary messages, but each receiver receives the same mes-
sage.
Asymmetric (A) [25] An asymmetric (Byzantine) faulty
node may send arbitrary messages that may differ for the
various receivers.
Strictly Omissive Asymmetric (A) [3] “A strictly omissive
faulty node can send a single correct value to some pro-
cesses and no value to all other processes.” A fault can “gar-
ble a message in transit, but not in an undetectable manner.”
To measure the reliability of a conﬁguration, a Markov
model is created with states given in terms of the hybrid
fault model. A fault may be (P) Permanent or (T) Transient.
Abbreviations for the state of a single node or channel are
(G) Good, (B) Benign, (S) Symmetric, and (A) Asymmet-
ric/Strictly Omissive Asymmetric. The hybrid fault model
is applied to components in three ways: a node may become
faulty (subscript N), a channel may become faulty (sub-
script C), or a node may appear faulty if both channels are
simultaneously faulty (subscript NC). All perceived node
faults due to channel faults are transient (since if both chan-
nels are permanently faulty, the system has failed). As two
examples, PSN would be a Permanent Symmetric faulty
Node, and TAN C would be a Node affected by Channel
faults that appears to be Transient Asymmetric faulty. A
node can be convicted (CONV) and permanently removed
from the group. While not explicitly represented in the hy-
brid fault model, convicted nodes are tracked in the reliabil-
ity models since the total number of nodes is conserved.
Transitions between states are speciﬁed with an expo-
nential transition rate (which assumes uncorrelated fault ar-
rivals). An exponential transition rate is speciﬁed in the
form e−λt where λ is the transition rate per unit time, and t
is time (here, in hours). A single transition may change the
state of one or more nodes or channels. We represent corre-
lated faults (such as lightning) with transitions that alter the
state of multiple nodes or channels.
We use the NASA Langley Semi-markov Unreliability
Range Evaluator (SURE) [6] reliability modeling tool set
for this analysis. The SURE tool calculates a reliability
bound, where the SURE bounding theorems have algebraic
solutions. SURE was designed to evaluate fault tolerant sys-
tems and handles models with multiple fault arrivals and
recoveries well due to the algebraic nature of the solution
engine. Iterative solution methods may take a long time to
converge, since the probability of violating the maximum
fault assumption can be very low (10−8 or less is not un-
usual). While a detailed discussion of modeling tools is
outside the scope of this paper, Butler and Johnson describe
the mathematics and give numerous modeling examples [6],
[7]. Also, the methodology is not limited to this tool suite.
3.2 Clock Synchronization Model and Mapping
The reliability of a protocol depends in part on how the
protocol’s hybrid fault model classiﬁes faults. We demon-
strate this by comparing the Welch and Lynch clock syn-
chronization algorithm to the improved strictly omissive
asymmetric algorithm by Azadmanesh and Kieckhafer [3].
The FlexRay clock synchronization algorithm is based
on the formally proven algorithm from Welch and Lynch
[39], an approximate agreement algorithm using sets of lo-