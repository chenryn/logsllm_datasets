The distribution of insecure results shown in Figure 2, (CIPHER,
𝑡10, red) is not very different from IV and KEY. 𝐵(10, 0.077) revealed
a 37.77% chance for at least one result, 14.05% for two, and 3.16%
for three results. We observed very similar probabilities of insecure
results in 𝑡5 and 𝑡3 as observed for IV and KEY. 𝐵(5, 0.089) revealed
a 30.24% chance and 𝐵(3, 0.065) a 17.70% chance for one insecure
result to be in 𝑡5 and 𝑡3, respectively.
Summary— Study 1 has shown that the distribution of insecure
Stack Overflow results in the top tiers 𝑡3, 𝑡5, and 𝑡10 is significantly
higher than the distribution of secure Stack Overflow results. This
was the case for all three tasks that were performed by our partici-
pants. Moreover, the probability of insecure results to end up in 𝑡3
is higher than for lower ranks.
These observations led to the following research questions: Does
the higher ranking of insecure Stack Overflow code examples actu-
ally have a negative impact on code security? Further, does security-
based re-ranking help in mitigating this effect by down-ranking
insecure and up-ranking secure results? We first present our ap-
proach for security-based re-ranking in the following Section 5 and
investigate both research questions in Study 2 in Section 6.
5 SECURITY-BASED RE-RANKING
In Section 4.5, we have shown that Google Search currently up-
ranks Stack Overflow results that provide insecure cryptographic
code. We introduce security-based re-ranking to tackle this problem.
In a nutshell, based on the security label of a webpage’s content
(i. e., source code), its rank is updated by either lowering or raising
it.
This would help developers to start their Web search on a safe
path. In fact, the Web offers secure best-practice examples that
help developers to finish their programming tasks functionally
and securely for a wide range of use cases [18]. Boosting those
examples in Google Search while simultaneously lowering the rank
of insecure results could have a ripple effect on the code security
of production software.
In this section, we describe how we identified secure best-practice
examples in TUM-Crypto and how we re-ranked results based on
security. We continue to use the labeling function 𝑙𝑡1 to determine
the security of webpages.
5.1 Secure Best-Practice Examples
A secure Stack Overflow webpage is labeled as a best-practice re-
sult if it is secure with respect to 𝑙𝑡1 and provides at least one
best-practice example for IV or KEY. An example is a code snippet
which gives the complete API usage pattern for how to safely gen-
erate a cryptographic key or an IV (see Figure 9 in the Appendix).
We distinguish secure best-practice from secure examples. A secure
code example is not a best-practice example if the API usage pattern
is incomplete. This means that the example does not show how
to safely implement all necessary dependencies of IV or KEY. For
instance, it does not show how to initialize a secure random number
generator which is necessary to generate a cryptographic key. In
practice, popular (but misleading) examples are code snippets that
show how to encrypt a string using a key that is passed through
a method parameter. Even though these examples may be secure
(in terms of not being insecure), developers will not learn the com-
plete pattern since key generation is missing. We do not make this
differentiation for CIPHER since initializing a CIPHER using the
Java SDK is done within a single statement that does not have any
dependencies.
We will show in later sections that patterns are incomplete for
most secure KEY and IV examples on Stack Overflow. This causes
the following problem: developers may be presented with secure
but unhelpful code. This forces them to continue the code search,
potentially leaving the secure path, ending up reusing insecure code
once again. By up-ranking secure best practices, there is higher
probability that developers start and stay on a safe path.
5.2 Code Embeddings
We have followed a semi-supervised methodology to identify secure
best-practice examples in TUM-Crypto. We first applied DBSCAN,
an unsupervised clustering method to cluster secure API usage
patterns for the use cases KEY and IV. Those provide the most
diverse patterns available in the dataset as shown in [18]. Note, our
clustering method does not separate secure from insecure patterns,
but is solely applied to secure ones to separate best practices from
potentially less helpful examples.
The advantage of DBSCAN is that it does not need to know the
number of expected clusters in advance. Since its clustering method
is performed on input vectors, we need vector representations for
the API usage patterns. In Fischer et al. [18], we have trained a
deep learning model that generated embeddings for API usage
patterns from TUM-Crypto. These embeddings were learned such
that similar patterns are close, and dissimilar ones are far away
from each other in the embedding space, using cosine distance
as the distance metric. The learned embeddings encode data and
control dependencies of code graphs, as well as lexical information
of code statements. We used them as inputs for DBSCAN because it
supports clustering based on cosine distance. Please refer to [18, 54]
for further details on the generation of the embeddings.
5.3 Best-Practice Clustering
DBSCAN automatically creates clusters based on two parameters:
the maximum distance 𝜖 of an embedding to the core sample of the
cluster, and the minimum number of embeddings 𝑚𝑖𝑛𝑃𝑡𝑠 necessary
to form a cluster. It does not require the number of clusters to be
Session 11C: Software Development and Analysis CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3075(a) IV, 𝑡10
(b) IV, 𝑡3
(c) KEY, 𝑡10
(d) KEY, 𝑡3
Figure 3: Parameter search for boosting results for IV and KEY. The horizontal axis represents boost values and the vertical
axis the probability of a secure best-practice or a secure or insecure result to appear in the top ten results.
known in advance as it is determined based on these parameters.
Embeddings that cannot be assigned to a cluster end up in the noise
cluster 𝑁 .
After we obtained the set of clusters 𝐶 := 𝐷𝐵𝑆𝐶𝐴𝑁 (𝜖, 𝑚𝑖𝑛𝑃𝑡𝑠) \
𝑁 for a given pair of 𝜖 and 𝑚𝑖𝑛𝑃𝑡𝑠, we looked at exactly on rep-
resentative 𝑐𝑖 for each cluster 𝐶𝑖 ∈ 𝐶 to determine whether it is a
secure best practice, or not. This way manual labeling effort is given
by cardinality |𝐶|, and its reduction 𝑟𝑙𝑎𝑏 given by 𝑟𝑙𝑎𝑏 = 1− |𝐶|/|𝐸|,
where 𝐸 is the set of embeddings generated from all samples in
TUM-Crypto.
We wanted to keep the manual labeling effort as low as possible
and therefore 𝑟𝑙𝑎𝑏 close to 1. Additionally, we wanted the clustering
to be as accurate as possible in order not to miss any best-practice
examples. Therefore, the size of the noise cluster |𝑁 | needs to be
as small as possible, as well as the biggest cluster 𝐶𝑚𝑎𝑥 ∈ 𝐶. These
three objectives are competing with each other. Smaller |𝑁 | leads
to more clusters |𝐶| and to smaller 𝑟𝑙𝑎𝑏.
It follows that we had to solve the following multi-objective
optimization task: Choose parameters 𝜖 and 𝑚𝑖𝑛𝑃𝑡𝑠 that maxi-
mize 𝑟𝑙𝑎𝑏, while minimizing the percentage of the biggest cluster
|𝐶𝑚𝑎𝑥|/|𝐸|, |𝐶𝑚𝑎𝑥| ≥ |𝐶𝑖|,∀𝐶𝑖 ∈ 𝐶 and the percentage of the noise
cluster |𝑁 |/|𝐸|.
The smaller |𝐶|, the smaller the manual effort 𝑟𝑙𝑎𝑏. The bigger
𝐶𝑚𝑎𝑥, the higher the chance 𝜖 has been chosen too large; thereby
insufficiently differentiating between API patterns. If |𝑁 | becomes
too large, it might contain best-practice examples that would be
missed since the noise cluster naturally will not be considered for
manual inspection.
5.4 Pareto Optimum
We determined the optimal parameter by performing a grid search
for a Pareto-optimal solution. Thereby, we calculated all values for
labeling effort, noise, and maximum cluster size during the grid
search with the objective to minimize them. The grid was given by
𝜖 ∈ [0.0, 0.09] as we observed that |𝐶| quickly converged to 1 for
bigger cosine distances than 0.09. We searched for 𝑚𝑖𝑛𝑃𝑡𝑠 ∈ [2, 10],
as we only needed 10 best-practice examples to show in the top
ten tier of the Google Search results. Our search space was very
small. 𝜖 was increased by 0.01 for each search, resulting in a grid
with dimension 10 × 9.
We calculated the knee-points for the 3𝑑 Pareto curve for all three
competing objectives and derived the related DBSCAN parameters
for IV and KEY. The results are shown in Table 3 in the Appendix.
Distance 𝜖 was similar for both use cases with 0.3 and 0.4 and both
had a minimum cluster size of two. For both use cases, we reached
a noise level of around 10% of the respective example set. Those
examples were not considered for manual labeling.
We randomly selected a representative for each cluster and man-
ually reviewed the source code of the representative to decide
whether it is a best-practice example. If yes, all samples from the
related cluster were added to the set of best-practice examples of
the related class IV or KEY, respectively. The manual labeling effort
was below 10%, which resulted in 65 samples for IV and 114 samples
for KEY.
Finally, we evaluated the precision of the whole process. We
selected a maximum of 50 random samples from each secure best-
practice cluster and manually evaluated whether they were true
positives. We obtained a precision of 0.81 for KEY and 0.98 for IV.
None of the false positives were insecure.
5.5 Re-ranking
We implemented a CSE that updates the rank of Stack Overflow
results based on the security of the content and whether it pro-
vides best-practice examples. The CSE API provides functionality
to influence the ranking of search results. Those modifications are
domain-based. For a given URL, attributes can be defined in order to
boost their rank. For a given Stack Overflow link, our CSE assigns
the search function 𝑐𝑠𝑒𝑏 a boost 𝑏 ∈ [−1, 1].
Parameter Search— Trivially, we set the boost value 𝑏𝑏𝑝 for
secure best-practice results to the maximum of 𝑏𝑏𝑝 = 1.0 and
for insecure results to the minimum of 𝑏𝑖 = −1.0. We performed a
parameter search to find the optimal boost value 𝑏𝑠 for secure results
that were not best practices. On the one hand, one would expect
those results to obtain a lower rank than secure best practices, since
they may be less helpful. On the other hand, one would require
them to have a higher rank than insecure results.
Therefore, they naturally compete with secure best-practice and
insecure results to obtain ranks in 𝑡10. If the boost for secure results
is too high, some secure best-practice results in 𝑡10 may be replaced
by secure results. If the boost is too low, some secure results may be
replaced by insecure ones. Therefore, the boost for secure results
has to be high enough to push insecure results further out of 𝑡10
while not interfering with secure best-practice results.
To find the optimal boost value for secure results, we repeated
our measurements from Study 1 (see Section 4). We calculated
probabilities 𝑝 = 𝑟/𝑠 for secure best-practice (𝑝𝑏𝑝), secure (𝑝𝑠), and
Session 11C: Software Development and Analysis CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3076Figure 4: Binomial distribution of boosted secure best-practice (green), secure (blue), and insecure results (red) for IV, KEY,
and CIPHER over 𝑡10. The 𝑥-axis represents the observed count of secure best-practice, secure, or insecure results in 𝑡10. The
𝑦-axis shows the probability for each observation, i. e., there is a probability of 𝑦 that 𝑥 results are secure best-practice, secure,
or insecure in 𝑡10.
insecure results (𝑝𝑖) to appear in 𝑡10. Here, 𝑟 was either the number
of secure best-practice, secure, or insecure results in 𝑡10, and 𝑠 the
total number of results in the respective tier. We calculated these
probabilities for different boost values 𝑏𝑠 ∈ [−1, 1] for secure results
using an increment of 0.1. Boost values 𝑏𝑏𝑝 and 𝑏𝑖 remained the
same. The results are shown in Figure 3.
Rank Trade-off— For IV, maximum probability 𝑝𝑏𝑝 for secure
best-practices to appear in 𝑡10 and 𝑡3 was given for 𝑏𝑠 = 0.1 (see
green line in Figure 3b). However, for KEY it was given for 𝑏𝑠 =
−0.2 (see the green line in Figure 3c). Therefore, those two values
represented the upper and lower bound of 𝑏𝑠. To decide optimal
𝑏𝑠 ∈ [−0.2, 0.1] we searched for a point where the probability of
secure results was higher than for insecure results. Figure 3a shows
that 𝑏𝑠 > 0.0 fulfilled this restriction for both tiers and all three
tasks. Probabilities of insecure results were the lowest in all four
plots. Therefore, we selected 𝑏𝑠 = 0.1 as it was the next increment
in the interval and the upper bound. This solution led to a small
reduction of probabilities of secure best practices for KEY in 𝑡10 and
𝑡3, as shown in Figure 3c and 3d. We considered this loss acceptable
in order to keep insecure probabilities low. Further, this led to an
additional gain for secure KEY results (see the blue line in Figure 3c
and 3d).
Since results for CIPHER are only separated into a secure and
insecure class, we did not have to include them in the parameter
search. Secure CIPHER results obtained 𝑏𝑠 = 1.0 and insecure
results 𝑏𝑖 = −1.0.
5.6 Results
This section provides the results for security best-practice clustering
and the binomial distribution of boosted secure best-practice, secure,
and insecure results in 𝑡10.
Secure Best-Practice Clusters— DBSCAN identified 12 best-
practice clusters for IV with overall 92 identified best-practice ex-
amples. The biggest cluster contained 16% of the samples, as shown
in Table 3. We found 46 Stack Overflow webpages that were labeled
secure by 𝑙𝑡1 (the top answer provides only secure code examples)
and contained at least one best-practice example in the top answer.
For those results, we set the boost to 𝑏𝑏𝑝 = 1.0 in 𝑐𝑠𝑒𝑏.
Ten best-practice clusters were found for KEY with overall 113
identified best-practice examples. The biggest cluster contained
17% of the samples. We found 11 webpages that were labeled secure
by 𝑙𝑡1 and provided at least one KEY best-practice example in the
top answer. They also obtained a boost value of 𝑏𝑏𝑝 = 1.0 in 𝑐𝑠𝑒𝑏.
Binomial Distribution— We computed the binomial distribu-
tion over {𝑐𝑠𝑒𝑏(𝑞)}𝑞∈𝑄 ⊂ 𝑡10, where 𝑐𝑠𝑒𝑏 is the modified Google
Search engine that applies our boosting values 𝑏𝑏𝑝, 𝑏𝑠, and 𝑏𝑖. 𝑄
are the same queries we collected in Study 1. We calculated the
binomial distribution for IV, KEY, and CIPHER by simulating 10,000
searches.
Task-Independent Results— We first measured the binomial
distributions independently from the tasks. 𝐵(10, 0.089) of boosted
secure best-practice results revealed a probability of 38.16% for one
result to appear in 𝑡10 and 29.36% in 𝑡3. Both distributions over 𝑡3
and 𝑡10 were even higher than the observed distributions of insecure
results in Study 1 (see Figure 2). Further, we saw a 17.11% chance
for two, and 3.85% for three results to appear in 𝑡10.
𝐵(10, 0.061) of secure results showed a chance of 36.54% for one
result, 11.59% for two, and 2.28% for three results to appear in 𝑡10.
As expected (𝑏𝑠 < 𝑏𝑏𝑝) is lower than the distribution of secure best
practices.
The distribution of insecure results 𝐵(10, 0.008) was finally the
lowest. The probability of one result in 𝑡10 was reduced from 36.46%
(as observed in Study 1) to 5.57%. Moreover, we observed near-zero
probability of an insecure result in 𝑡3 which was 22.78% in Study 1.
IV— We measured the binomial distribution 𝐵(10, 0.084) for
boosted secure best-practice results {𝑐𝑠𝑒𝑏𝑏𝑝 (𝑞)}𝑞∈𝑄𝑖𝑣 over 𝑡10. As
shown in Figure 4, (IV, 𝑡10, green) the probability of at least one result
to appear in 𝑡10 was 38.96%, 18.70% for two results, and for three
results, 6.08%. Comparing this distribution with the distribution
of boosted secure results {𝑐𝑠𝑒𝑏𝑠 (𝑞)}𝑞∈𝑄𝑖𝑣 also shown in Figure 2
(see IV, 𝑡10, blue), we observed the required higher distribution of
secure best-practice results.
Further, the distribution for boosted secure results shown in
Figure 4, (IV, 𝑡10, blue) was higher than the distribution of non-
boosted secure results shown in Figure 2. The probability of at least
one secure result in 𝑡10 increased from 22.44% to 25.03%, for two
results from 2.86% to 3.52%, and from 0.19% to 0.3% for three results.
Lastly, we calculated the distribution 𝐵(10, 0.010) of boosted
insecure results {𝑐𝑠𝑒𝑏𝑖 (𝑞)}𝑞∈𝑄𝑖𝑣 , shown in Figure 4, (IV, 𝑡10, red),
and compared it with the distribution of non-boosted insecure
results from Figure 2. Probability decreased from 34.90% to a 9.32%
for at least one insecure result in 𝑡10, from 10.43% to 0.32% for two
results, and from 1.65% to 0% for three results. Furthermore, there
Session 11C: Software Development and Analysis CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3077was an increase from a 54.74% to a 90.36% chance that none of the