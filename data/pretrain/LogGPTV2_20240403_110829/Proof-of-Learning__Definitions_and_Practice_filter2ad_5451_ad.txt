∀i ∈ [|D|], Pr(ci = 1) = Q·k
S . Therefore, the probability of a
data point being chosen at least once is
Pr(ci (cid:62) 1) = 1 − Pr(ci = 0) = 1 − (1 − Q · k
)E
(4)
S
S )E].
This means for dataset D, the expected amount of data for
Algorithm 2 is |D|[1 − (1 − Q·k
3) Initial State Provenance and Chain of Trust: To improve
convergence behavior and achieve better performance, most
ML models are not initialized from a cold start—an initializa-
tion sampled randomly from a particular distribution. Indeed,
it is common to start training from a set of weights that have
previously achieved good results on a different dataset, and
improve upon them (a warm start). A common example of a
warm start is transfer learning [67]. If we do not check the
provenance of the initial state, we discuss in § VII how an
adversarial prover could ﬁne-tune a stolen model by continuing
to train it for a few steps, thus creating a valid PoL, and claim
that they have started from a lucky initialization—where the
lucky initialization is the true owner’s ﬁnal weights.
In order to establish the PoL in models with a warm
start while defending against
the said attack scenario, we
propose to establish a chain of trust: a PoL P(fWT ) should
come with a previously published P 0, where P 0 denotes the
proof needed to verify the initial state W0 used to obtain
P(fWT ). The veriﬁer keeps a record of previously veriﬁed
proofs. Therefore, in Algorithm 2, if V has recorded P 0,
VERIFYINITPROOF would be a simple record lookup. Other-
wise, it would trigger the veriﬁcation of P 0. The veriﬁcation
success rate follows a chain rule VSR(cid:0)V,P(fWT ) → P 0(cid:1) =
VSR (V,P(fWT )) VSR(cid:0)V,P 0(cid:1), where → denotes the depen-
dence. Of course P 0 can depend on a prior PoL P 1, and so on.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:16:15 UTC from IEEE Xplore.  Restrictions apply. 
1045
Concretely, for all j ≥ 0, let P j+1 denote the PoL for the ﬁrst
set of model weight needed to obtain P j, and P 0 = P(fW0),
i.e., P 0 is the proof for W0. Therefore, the VSR for a chain
of R prior PoLs can be written as:
VSR(cid:0)V,P(fWT ) → P 0 → ··· → P R(cid:1)
R(cid:89)
= VSR (V,P(fWT ))
VSR(cid:0)V,P j(cid:1) .
(5)
j=0
If T cannot provide such a proof, then it must be the case
that they have trained a model starting from a random initial
state. In this case, T should provide their initialization distri-
bution and strategy and apply a statistical test to verify that
the initial model parameter values contained within the proof
sequence were indeed sampled from the claimed distribution.
4) Verifying Initialization: Most existing initialization
strategies for model weights such as Xavier [57], Kaim-
ing [59], and Orthogonal Initialization [68], involve sampling
values from a designated distribution (usually normal or
uniform). Such distributions rely on the architecture of the
model (e.g., dimensionality of a certain layer), so it can be
easily obtained given the initialization strategy which must be
included in the initial metadata M0 ∈ M.
The Kolmogorov–Smirnov (KS) test [69] is a statistical test
to check whether samples come from a speciﬁc distribution.
We use a single-trial KS test
to check if the weights of
each layer are sampled from the designed distribution. If any
layer does not pass the KS test, i.e., the p-value is below
the chosen signiﬁcance level, the veriﬁer can claim that the
initialization parameters are not sampled from the prover’s
claimed initialization distribution, making the PoL invalid. We
note that the tests are done under the assumption that the
different layers are initialized independently which is often
the case [70]. Otherwise,
the signiﬁcance level should be
corrected to account for multiple testing using a method such
as Bonferroni’s method. Along with all other metadata (e.g.,
the optimizer), we assume that T and A must choose an
initialization strategy from a previously chosen (and publicly
known) set of strategies (e.g., all widely-known strategies),
preventing the adversary from creating an arbitrary initializa-
tion strategy for their own spooﬁng purposes. In Algorithm 2,
VERIFYINITIALIZATION handles the initialization test.
VI. CORRECTNESS ANALYSIS OF THE GRADIENT
DESCENT MECHANISM FOR PROOF-OF-LEARNING
Recall that the goal of our proposed veriﬁcation scheme
is for the veriﬁer to gain conﬁdence that each of the steps
recorded in the PoL are valid, rather than verifying the end-
to-end sequence altogether. We now prove why the veriﬁcation
must be performed step-wise.
A. Stationary Markov Process
Training a neural network using a gradient-based stochastic
optimization method is a Markov process,
its future
progression is independent of its history given its current
state. We formalize this property in Appendix A. The Markov
i.e.,
assumption is used in ML libraries, including pytorch [71]
and tensorflow [72], to enable in-place model updates.
Gradient-based stochastic optimization method is not only a
Markov process but also stationary, assuming that any random-
ness in the architecture is ﬁxed (e.g., using a ﬁxed batching
strategy, and with deterministic dropout masks). Without loss
of generality, we prove this property for SGD but note that
other gradient-based stochastic optimization methods follow
(G4). Here, we adopt the notation ˜Wt := (Wt, Mt) to denote
the model weight and the associated learning hyperparameters
at step t. Thus, a training step is represented as follows:
˜Wt+1 = ˜Wt − η∇ ˜Wt
ˆLt + zt,
(6)
where zt is the random variable representing noise arising from
the hardware and low-level libraries such as cuDNN [73] at
step t and the set of random variables {zt | t ∈ [T ]} are
independent and identically distributed. Thus, for all steps t
and arbitrary ˜wa, ˜wb,
Pr( ˜Wt+1 = ˜wa| ˜Wt = ˜wb) = Pr( ˜Wt = ˜wa| ˜Wt−1 = ˜wb).
Thus, the process of training a neural network using gradient-
based stochastic optimization is a stationary Markov process.
B. Entropy Growth
Building on our results in § VI-A, we analyze the entropy
growth of training a DNN as a stationary Markov process,
ΘT = ˜W0,··· , ˜WT . Entropy captures the variance, or number
of possible paths of the gradient descent sequences [74]. Using
the deﬁnition of entropy rate (refer Equation 13 Appendix A)
and Markovian nature of the training process ΘT , we get the
entropy rate as follows:
H(cid:48)(ΘT ) = lim
T→∞ H( ˜WT| ˜W0, ..., ˜WT−1) = H( ˜W1| ˜W0)
(7)
= H(z0)
(8)
where we obtain Equation (8) by plugging in the result stated
in Equation (6). This proves the following result:
Theorem 1 (Entropy Growth). The entropy of the training
process ΘT grows linearly in the number of training steps T .
the exact reproducibility of a ML model
To bound the entropy, our veriﬁcation scheme performs a
the entropy would grow
step-wise comparison. Otherwise,
unbounded,
increasing the difﬁcult of accurately verifying
the updates in a training process. Further, Theorem 1 also
proves that
is
difﬁcult because the entropy grows, without bound, as
the training sequence grows
(rendering retraining-based
spooﬁng impossible). This result holds true even with an
identical initialization and batching strategy. Note that our
only assumption was the presence of some i.i.d noise in
the training process arising due to hardware and low-level
libraries. Our result is therefore of interest beyond the setting
considered in our work, and in particular explains the negative
results observed previously in model extraction research when
trying to reproduce a training run exactly [75].
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:16:15 UTC from IEEE Xplore.  Restrictions apply. 
1046
Interpretation of Entropy Growth. Recall the deﬁnition
of entropy [74]. The entropy of a training step captures the
variance, or number of possible paths from that state (i.e.,
how much information is needed to describe the possibilities).
Thus, the expected variance of the sequences grows too. The
relation between entropy and number of possible sequences
is predominantly exponential as its deﬁnition is logarithmic to
the probability. Thus the linear growth in entropy in Theorem 1
represents an exponential growth in the number of potential
sequences of gradient descent.
C. Reproducibility Evaluation
To illustrate our analysis, we empirically evaluate our ver-
iﬁcation scheme and the implications of Theorem 1. We also
discussed how to conﬁgure hyperparameters of Algorithms 1
and 2 to analyze trade-offs between storage cost and correct-
ness of PoL veriﬁcation.
1) Experimental Setup: A Residual Neural Network
(ResNet) [76] is a common deep neural network architecture
used for image classiﬁcation. We evaluated the proposed PoL
for ResNet-20 and ResNet-50 on two object classiﬁcation
tasks: CIFAR-10 and CIFAR-100 [77] respectively. Each of the
two datasets is composed of 50,000 training images and 10,000
testing images, each of size 32× 32× 3. The datasets differ in
that CIFAR-10 only has 10 classes whereas CIFAR-100 has
100 classes. Thus classifying CIFAR-100 is considered as a
harder task. Both models are trained for 200 epochs with batch
size being 128 (i.e., E = 200, S = 390).
2) Metrics For Evaluation: Our goal here is to understand
how the entropy growth of training (see Theorem 1) impacts
our capability to verify a training update. Formally, we are
given (initial) weights Wt which are trained to a state Wt+k,
where k represents some previously chosen and ﬁxed check-
pointing interval. The veriﬁer then attempts to reproduce this
step by calculating their own W (cid:48)
t+k from Wt. The reproduction
error here is εrepr(t) = d(Wt+k, W (cid:48)
t+k), using some distance
metric d, e.g., a p-norm. With a sufﬁciently small εrepr(t),
∀t ∈ [T ], a veriﬁer can conﬁrm that indeed W (cid:48)
t+k ≈ Wt+k,
∀t ∈ [T ], which proves that the prover trained this ML model.
Speciﬁcally, we require that maxt εrepr(t) (cid:28) dref, where dref
T ) is the reference distance between two models
= d(W 1
T of the same architecture, trained to completion
W 1
(i.e., for T steps) using the same architecture, dataset, and
initialization strategy, but with a different batching strategy
0 (cid:54)= W 2
and not forcing the same initial parameters (i.e., W 1
0 ).
If this is the case, then we can set our distance threshold δ
(refer to Algorithm 2) such that maxt(εrepr(t) ) < δ < dref.
Note that dref can be interpreted as the difference between two
models trained from scratch by two independent parties, so it
is used as our upper bound (i.e., if two models differ by about
dref then they should not be considered as related).
T , W 2
T and W 2
Observing Table I, we see that our empirical results corrob-
orate Theorem 1. Reproducing weights trained step by step
(k = 1) leads to a negligible εrepr(t). However, attempting
to reproduce an entire sequence leads to a large error due
to the linear increase in entropy over the T steps. Note
that this error accumulates even when using the exact same
Checkpointing Interval, k
k = 1
k = E · S
0.974(±0.004)
| (cid:96)1
0.955(±0.004)
(cid:96)2
(cid:96)∞ 0.769(±0.052)
0.914(±0.007)
cos
|
)
t
(
r
p
e
r
ε
|
|
0.001(±0.001)
0.001(±0.001)
0.001(±0.001)
0.0(±0.0)
(a) CIFAR-10
Checkpointing Interval, k
k = 1
k = E · S
0.903(±0.002)
| (cid:96)1
0.815(±0.002)
(cid:96)2
(cid:96)∞ 0.532(±0.07)
0.383(±0.002)
cos
|
)
t
(
r
p
e
r
ε
|
|
0.002(±0.001)
0.002(±0.002)
0.004(±0.004)
0.0(±0.0)
Deterministic
operations
0.582(±0.004)
0.569(±0.004)
0.307(±0.035)
0.46(±0.007)
Deterministic
operations
0.903(±0.001)
0.816(±0.001)
0.51(±0.055)
0.384(±0.002)
(b) CIFAR-100
TABLE I: Normalized reproduction error, ||εrepr(t)||, of a valid
PoL. The same initial parameter values W0, batching strategy,
model architecture, and training strategy are used. W (cid:48)
t+k is
reproduced from Wt by retraining ∀t ∈ {0, k, 2k, . . . , T}
while ||εrepr(t)|| is computed as the distance between W (cid:48)
t+k
and Wt+k normalized by dref (see Table V in Appendix E for
exact values of dref). Deterministic operations used k = E · S.
batching strategy, architecture, initial parameters, and training
setup, due to the irreproducible noise z arising from the
hardware and low-level libraries. Thus, it is impossible for
a veriﬁer to reproduce an entire training sequence and we
require that k be sufﬁciently small to prevent these errors
from accumulating and approaching to dref. Note that our
results display a normalized ||εrepr(t)|| = maxT (εrepr)
where
we require that ||εrepr(t)|| << 1 for the sufﬁcient condition to
hold so that the veriﬁer can select a suitable δ.
dref
3) Deterministic Operations: Libraries such as pytorch
provide functionality that restrict
the amount of random-
ness [78] (e.g., using deterministic algorithms for convolution
operations) to enable reproducibility. We evaluate this with
k = T (refer Table I). As seen in Table I, ||εrepr|| with
deterministic operations drops to half of ||εrepr|| for non-
||εrepr||
deterministic operations with ResNet-20. However,
is still signiﬁcant and deterministic operations incur a large
computational cost in training and a greater than one per-
centage point accuracy drop. The reduction in ||εrepr|| is not
observed for ResNet-50, which is likely because the main
source of randomness for this architecture is not captured by
deterministic operations provided by pytorch. Some other
libraries use counter-based pseudorandom number generators,
which will be discussed in § VIII.