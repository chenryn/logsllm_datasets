title:PacketShader: a GPU-accelerated software router
author:Sangjin Han and
Keon Jang and
KyoungSoo Park and
Sue B. Moon
PacketShader: A GPU-Accelerated Software Router
Sangjin Han†
Keon Jang†
KyoungSoo Park‡
Sue Moon†
†Department of Computer Science, KAIST, Korea
{sangjin, keonjang}@an.kaist.ac.kr, PI:EMAIL
‡Department of Electrical Engineering, KAIST, Korea
PI:EMAIL
ABSTRACT
We present PacketShader, a high-performance software router frame-
work for general packet processing with Graphics Processing Unit
(GPU) acceleration. PacketShader exploits the massively-parallel
processing power of GPU to address the CPU bottleneck in current
software routers. Combined with our high-performance packet I/O
engine, PacketShader outperforms existing software routers by more
than a factor of four, forwarding 64B IPv4 packets at 39 Gbps on
a single commodity PC. We have implemented IPv4 and IPv6 for-
warding, OpenFlow switching, and IPsec tunneling to demonstrate
the ﬂexibility and performance advantage of PacketShader. The eval-
uation results show that GPU brings signiﬁcantly higher throughput
over the CPU-only implementation, conﬁrming the effectiveness of
GPU for computation and memory-intensive operations in packet
processing.
Categories and Subject Descriptors
C.2.1 [Network Architecture and Design]: Network communica-
tions; C.2.6 [Internetworking]: Routers
General Terms
Design, experimentation, performance
Keywords
Software router, CUDA, GPU
1.
INTRODUCTION
PC-based software routers provide a cost-effective packet process-
ing platform with easy extensibility and programmability. Familiar
programming environments on general-purpose operating systems
allow ﬂexible composition of router applications that meet today’s
complex trafﬁc engineering demand. Adding to that, modern inno-
vation in commodity hardware continues to drive down the cost per
performance, realizing off-the-shelf programmable routers.
While programmability and low cost are the two primary strengths
of software routers, keeping them at high speed is still challenging.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’10, August 30–September 3, 2010, New Delhi, India.
Copyright 2010 ACM 978-1-4503-0201-2/10/08 ...$10.00.
Existing software routers report near 10 Gbps forwarding perfor-
mance even on a single machine, but the CPU quickly becomes
the bottleneck for more compute-intensive applications. For exam-
ple, the IPsec performance of RouteBricks degrades by a factor of
4.5 from its 8.7 Gbps1 IPv4 forwarding throughput with 64B pack-
ets [19]. To scale up the computing cycles, one can put more CPUs
to a single server or distribute the load to a cluster of machines, but
per-dollar performance stays relatively low.
In this work we explore new opportunities in packet processing
with Graphics Processing Units (GPUs) to inexpensively shift the
computing needs from CPUs for high-throughput packet processing.
GPUs offer extreme thread-level parallelism with hundreds of slim
cores [20, 42]. Their data-parallel execution model ﬁts nicely with
inherent parallelism in most router applications. The memory ac-
cess latency hiding capability and ample memory bandwidth of GPU
can boost many memory-intensive router applications, which heavily
rely on lookup in large tables. For compute-intensive applications,
the massive array of GPU cores offer an order of magnitude higher
raw computation power than CPU. Moreover, the recent trend shows
that the GPU computing density improves faster than CPU [42]. Last
but not least, GPUs are cheap and readily available.
We present PacketShader, a GPU-accelerated software router frame-
work, that carries the beneﬁt of low cost and high programmability
even at multi-10G speed. The main challenge of PacketShader lies
in maintaining the high forwarding rate while providing as much
processing power for arbitrary router applications. We address the
challenge in two parts. First, we implement highly optimized packet
I/O engine to eliminate per-packet memory management overhead
and to process packets in batch, enabling high-performance packet
I/O in user mode. Second, we ofﬂoad core packet processing oper-
ations (such as IP table lookup or IPsec encryption) to GPUs and
scale packet processing with massive parallelism. Coupled with
I/O path optimization, PacketShader maximizes the utilization of
GPUs by exposing as much parallelism as possible in a small time
frame. Our design choices allow unprecedented performance ad-
vantage. On a single box, PacketShader forwards IPv4 packets at
the rate of 40 Gbps for all packet sizes. GPUs bring signiﬁcant
performance improvement for both memory and compute-intensive
applications; IPv6 forwarding reaches 38.2 Gbps for 64B packets
and the IPsec performance ranges from 10 to 20 Gbps.
PacketShader is the ﬁrst to demonstrate the potentials of GPUs
in the context of multi-10G software routers. We believe GPUs’
massively-parallel processing power opens a great opportunity for
high-performance software routers with cost effectiveness and full
1We take 24-byte Ethernet overhead into account when we calculate through-
put in this paper. We apply the same metric and translate the numbers from
other papers.
195Buffer size (bytes)
1M
Host-to-device
5,577
Device-to-host
3,394
Table 1: Data transfer rate between host and device (MB/s)
16K
2,069
1,743
64K
4,046
2,848
256K
5,142
3,242
1K
185
211
4K
759
786
256
55
63
RAM in other architectures), 32K 32-bit registers, and 16KB L1
cache. L2 cache of 768 KB is shared by all SMs.
The GPU kernel execution takes four steps: (i) the DMA con-
troller transfers data from host (CPU) memory to device (GPU) mem-
ory, (ii) a host program instructs the GPU to launch the kernel, (iii)
the GPU executes threads in parallel, and (iv) the DMA controller
transfers resulting data from device memory to host memory.
CUDA (Compute Uniﬁed Device Architecture) is NVIDIA’s soft-
ware suite of libraries and a compiler, and it provides an API to
GPU functionalities [40]. Programmers write C-like (C with CUDA-
speciﬁc extensions) kernel code, and then CUDA compiles it and
exposes an interface to host programs to launch the kernel.
2.2 GPU Overheads
Typical GPGPU (General-Purpose computing on GPU) applica-
tions are data-intensive, handling relatively long-running kernel ex-
ecution (10-1,000s of ms) and large data units (1–100s of MB) [41].
In the context of software routers, GPU should work with much
shorter kernel execution time and smaller data. We check if GTX480
hardware and CUDA library give reasonably small overheads for
such ﬁne-grained GPU use.
Kernel launch latency: Every step in a kernel launch contributes to
the latency: PCI Express (PCIe) transactions, initial scheduling of
threads, synchronization, and notiﬁcation for completion. We mea-
sure the latency to check if it is reasonably small; the GPU launching
latency for a single thread is 3.8 µs, and 4.1 µs for 4,096 threads
(only 10% increase). We conclude that amortized per-thread kernel
launch overhead decreases linearly with the increasing number of
threads and eventually becomes negligible.
Data transfer rate: While a PCIe 2.0 x16 link connected to a graph-
ics card offers the theoretical bandwidth of 8 GB/s, the effective
bandwidth would be smaller due to PCIe and DMA overheads, es-
pecially for small data transfer units. We measure the data transfer
rate between host and device memory over different buffer sizes and
summarize it in Table 1. The transfer rate is proportional to the
buffer size and peaks at 5.6 GB/s for host-to-device and 3.4 GB/s
for device-to-host. The table conﬁrms that a PCIe link provides
enough bandwidth even for small batch sizes. For example, we
can transfer 1 KB of 256 IPv4 addresses (4B each) at 185 MB/s or
185÷ 4 = 48.5 Mpps for each GPU, which translates to 34.1 Gbps
with 64B packets.
2.3 Motivating Example
The processing power of GPU comes from its hundreds of cores.
The key insight of this work is that the massive array of GPU cores
match the inherent parallelism in stateless packet processing. We
process multiple packets at a time and take full advantage of the
massive parallelism in GPU.
Figure 2 compares the performance of IPv6 forwarding table lookup
(longest preﬁx matching) with CPU and GPU (with the same algo-
rithm and the forwarding table in Section 6.2.2). The experiment is
done with randomly generated IPv6 addresses and does not involve
actual packet I/O via Network Interface Cards (NICs). The through-
put of GPU is proportional to the level of parallelism; given more
than 320 packets an NVIDIA GTX480 outperforms one Intel quad-
core Xeon X5550 2.66 GHz CPU and two CPUs with more than 640
Figure 1: Architecture of NVIDIA GTX480
programmability, and our PacketShader will be a useful stepping
stone.
The road map of the paper is as follows. In Section 2 we present
an overview of the NVIDIA GPU architecture and explore its po-
tential for packet processing.
In Section 3 we describe the hard-
ware and software system setup for our PacketShader. The two key
contributions of this work, namely, optimized packet I/O and GPU
acceleration, are in Sections 4 and 5, respectively. Section 6 delivers
the performance evaluation results. We list current limitations and
future directions in Section 7, review related work in Section 8, and
conclude in Section 9.
2. GPU AS A PACKET PROCESSOR
GPU has become a powerful computing engine behind scientiﬁc
computing and data-intensive applications, beyond its original role
for graphics rendering. This section explores the potential of GPU
for general packet processing.
2.1 GPU Architecture
We begin with a brief introduction on the internals of the NVIDIA
GPU that we use in this work. More details are available in [20, 37,
39, 40]. Figure 1 illustrates the architecture of NVIDIA GTX480.
It has 15 Streaming Multiprocessors (SMs), each of which consists
of 32 Stream Processors (SPs), resulting in 480 cores in total. All
threads running on SPs share the same program called kernel2.
An SM works as an independent SIMT (Single Instruction, Multi-
ple Threads) processor. The basic execution unit of SM is a warp, a
group of 32 threads, sharing the same instruction pointer; all threads
in a warp take the same code path. While this lockstep execution
is not mandatory, any divergence of code path in a warp should
be minimized for optimal performance. For example, when not all
32 threads in a warp agree on the condition of an if statement, the
threads take both then and else parts with corresponding mask-
ing. Compared to the traditional SIMD (Single Instruction, Multiple
Data) architecture where all data elements must be processed in the
same way, the SIMT behavior of GPU gives more ﬂexibility to pro-
grammers.
The scheduler in an SM holds up to 32 warps and chooses a
warp to execute for every instruction issue time; a readily available
warp, not having stalls due to register dependency or memory access,
is chosen ﬁrst. Typically, having many GPU threads gives better
throughput since memory access latency of a warp can be effectively
hidden with execution of other warps [25]. Warp scheduling is done
by hardware, incurring zero context-switch overhead.
GTX480 provides high-bandwidth, off-chip GPU memory (device
memory of 1.5 GB in Figure 1). For low-latency in-die memory,
each SM has 48 KB shared memory (comparable with scratchpad
2The term “kernel” should not be confused with operating system kernels.
196Item
Speciﬁcation
Qty
Unit price
Intel Xeon X5550 (4 cores, 2.66 GHz)
CPU
$925
RAM DDR3 ECC 2 GB (1,333 MHz)
$64
M/B
$483
GPU
$500
NIC
$628
Table 2: Test system hardware speciﬁcation (total $7,000)
Super Micro X8DAH+F
NVIDIA GTX480 (480 cores, 1.4 GHz, 1.5 GB)
Intel X520-DA2 (dual-port 10GbE)
2
6
1
2
4
Figure 2: IPv6 lookup throughput of X5550 and GTX480
packets. At the peak performance one GTX480 GPU is comparable
to about ten X5550 processors. In contrast, given a small number
of packets in a batch GPU shows considerably lower performance
compared with CPU. GTX480 processes up to 480 threads at a time
and needs more threads to hide memory access latency; having not
enough threads, most of GPU resources are underutilized during the
execution. The per-batch cost of GPU transaction is another reason
for the low performance, as described in Section 2.2.