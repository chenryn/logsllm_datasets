title:The Price of Privacy in Collaborative Learning
author:Bal&apos;azs Pej&apos;o and
Qiang Tang and
Gergely Bicz&apos;ok
Balázs Pejó*, Qiang Tang, and Gergely Biczók
Together or Alone: The Price of Privacy in
Collaborative Learning
Abstract: Machine learning algorithms have reached
mainstream status and are widely deployed in many ap-
plications. The accuracy of such algorithms depends sig-
niﬁcantly on the size of the underlying training dataset;
in reality a small or medium sized organization often
does not have the necessary data to train a reasonably
accurate model. For such organizations, a realistic so-
lution is to train their machine learning models based
on their joint dataset (which is a union of the indi-
vidual ones). Unfortunately, privacy concerns prevent
them from straightforwardly doing so. While a num-
ber of privacy-preserving solutions exist for collaborat-
ing organizations to securely aggregate the parameters
in the process of training the models, we are not aware
of any work that provides a rational framework for the
participants to precisely balance the privacy loss and
accuracy gain in their collaboration.
In this paper, by focusing on a two-player setting, we
model the collaborative training process as a two-player
game where each player aims to achieve higher accu-
racy while preserving the privacy of its own dataset.
We introduce the notion of Price of Privacy, a novel ap-
proach for measuring the impact of privacy protection
on the accuracy in the proposed framework. Further-
more, we develop a game-theoretical model for diﬀerent
player types, and then either ﬁnd or prove the existence
of a Nash Equilibrium with regard to the strength of
privacy protection for each player. Using recommenda-
tion systems as our main use case, we demonstrate how
two players can make practical use of the proposed the-
oretical framework, including setting up the parameters
and approximating the non-trivial Nash Equilibrium.
Keywords: Privacy, Game Theory, Machine Learning,
Recommendation Systems
Received ; revised ; accepted .
8
1
0
2
g
u
A
4
2
]
T
G
.
s
c
[
3
v
0
7
2
0
0
.
2
1
7
1
:
v
i
X
r
a
1 Introduction
As data have become more valuable than oil, everybody
wants to have a slice of it; Internet giants (e.g., Amazon,
Google, Netﬂix, etc.) and small businesses alike would
like to extract as much value from it as possible. Ma-
chine Learning (the process of learning from data and
making predictions about it by building a model) has
received much attention over the last decade, mostly
due to its vast application range such as recommen-
dation services, medicine, speech recognition, banking,
gaming, driving, and more. For Machine Learning tasks,
it is widely known that more training data will lead to
a more accurate model. Unfortunately, most organiza-
tions do not possess a dataset as large as Netﬂix’s or
Amazon’s. In such a situation, to obtain a relatively ac-
curate model, a natural solution would be to aggregate
all the data from diﬀerent organizations on a centralized
server and train on the global dataset as seen on the
left side of Fig. 1. This approach is eﬃcient, however,
data owners have a valid privacy concern about shar-
ing their data, particularly with new privacy regulations
such as the European General Data Protection Regula-
tion (GDPR). Therefore, improving Machine Learning
via straightforward data aggregation is likely undesir-
able and potentially unlawful in reality. Various privacy
concerns exists with regard to Machine Learning (e.g.,
the privacy of the input to the training or the privacy
of the trained model); in this paper, we focus on the
privacy of the input for individual data contributors.
*Corresponding Author: Balázs Pejó: University of Lux-
embourg, E-mail: PI:EMAIL
Qiang Tang: Luxembourg Institute of Science and Technol-
ogy, E-mail: PI:EMAIL
Gergely Biczók: CrySyS Lab, Dept. of Networked Systems
and Services, Budapest Univ. of Technology and Economics,
E-mail: PI:EMAIL
Fig. 1. Centralized (left) and Distributed (right) Learning
In the literature, Privacy Preserving Distributed
Machine Learning [PRR10, RA12, HCB16, MMR+16,
PZ16] have been proposed to solve this problem by
training the model locally and safely aggregating all the
local updates, illustrated on the right side of Fig. 1. On
the other hand, these approaches’ eﬃciency depend on
the number of participants and the sample sizes as we
highlight this in the related works.
In this paper, we are interested in a scenario with
two participants, each of whom possesses a signiﬁcant
amount of data and would like to obtain a more accu-
rate model than what they would obtain if training was
carried out in isolation. It is clear that the players will
only be interested in collaboration if they can actually
beneﬁt from each other. To this end, we simply assume
that the players have already evaluated the quality of
each other’s datasets to make sure training together is
beneﬁcial for both of them before the collaboration. How
such evaluation should be done is out of scope for our
research; there are best practices already established in
the ﬁeld [HKP12]. Most of the Machine Learning pa-
pers, including privacy-preserving ones, implicitly make
this assumption.
1.1 Problem Statement
Collaborative Machine Learning will increase the model
accuracy, but at the cost of leaking some information
about the players’ datasets to each other. To miti-
gate the information leakage, players can apply some
privacy-preserving mechanisms, e.g., calibrating and
adding some noise or deleting some sensitive attributes.
Many “solutions” have been proposed, as surveyed in
the related work. In most of them, the players are not
provided with the option of choosing their own privacy
parameters. Clearly, there is a gap between these solu-
tions and reality, where players will have diﬀerent pref-
erences to privacy and utility and may want to dynam-
ically set the parameters.
Together or Alone: The Price of Privacy in Collaborative Learning
2
1.2 Contribution
We ﬁrst propose a two-player game theoretical model for
Collaborative Learning (a training process via an arbi-
trary training algorithm between two players). We pro-
ﬁle the players and analyze their best response strategies
and the equilibria of the designed game. Inspired by the
notion of Price of Anarchy [KP99], we deﬁne Price of
Privacy, which is a new way of measuring the accuracy
degradation due to privacy protection. Then, we demon-
strate the usage of the model via a recommender use
case, where two players improve their own recommen-
dation accuracy by leveraging on each other’s dataset.
It is worth noting that this is indeed a representative
example since the used Stochastic Gradient Descent op-
timization process is a universal procedure widely used
in Machine Learning tasks. For illustration purposes,
we consider two privacy preserving mechanisms, includ-
ing attribute deletion and diﬀerential privacy. Based
on heuristics, we demonstrate how to approximate the
privacy-accuracy trade-oﬀ functions, which lie in the
core of the proposed theoretical model and determine
how the players should set the parameters, and illus-
trate the practically obtained Nash Equilibrium.
We would like to emphasize that approximating the
privacy-accuracy trade-oﬀ function is a very realistic
choice in applying the proposed theoretical model. Sci-
entiﬁcally, we may want to use cryptographic techniques
such as secure two-party computation protocols to pre-
cisely compute these parameters. However, this is unde-
sirable due to the incurred complexity. In order to re-
duce complexity, most deployed Machine Learning sys-
tems implement heuristics, such as approximating the
parameters in Stochastic Gradient Descent [HKP12].
1.3 Organization
In Sec. 2, we review some basic concepts used through-
out the paper such as Game Theory and Diﬀerential Pri-
vacy. In Sec. 3, we introduce the Collaborative Learning
game, explain the parameters, and deﬁne the concept
of Price of Privacy. In Sec. 4, we provide a theoretical
analysis of the proposed game and investigate the Nash
Equilibrium. In Sec. 5, we introduce the recommender
use case and describe two example privacy-preserving
mechanisms. In Sec. 6, for the recommender use case,
we demonstrate how to determine the the privacy accu-
racy trade-oﬀ function via interpolation over the joint
dataset. Then, in Sec. 7 we show the corresponding equi-
librium by applying our game theoretic model. In Sec.
8, besides presenting the whole process required in ad-
vance of the collaboration, we show how to approximate
To bridge this gap, we consider the parties involved
as rational players and model their collaboration as a
two-player game. In our setting, players have their own
trade-oﬀs with respect to their privacy and expected
utility and can ﬂexibly set their own privacy parameters.
The central research problem is to propose a general
game theoretical model and ﬁnd a Nash Equilibrium.
Moreover, given a speciﬁc Machine Learning task, we
should answer the following core questions.
– What are the potential ranges for privacy parame-
ters that make the collaborative Machine Learning
model more accurate than training alone?
– What is the optimal privacy parameter (which re-
sults in the highest payoﬀ)?
– With this optimal parameter, how much accuracy
is lost overall due to the applied privacy-preserving
mechanisms?
Together or Alone: The Price of Privacy in Collaborative Learning
3
the trade-oﬀ function via heuristics and study its im-
pact on the Nash Equilibrium. In Sec. 9, we review the
the related works from the perspective of game theory
and privacy-preserving machine learning. In Sec. 10, we
conclude the paper.
As we use multiple well-known concepts through the
paper, we provide a short summary of abbreviations in
App. A to improve readability.
2 Preliminaries
In this section, we introduce diﬀerential privacy and the
game theoretic terminology used in the paper.
2.1 Diﬀerential Privacy
DP [Dwo06] have been used widely in the literature.
It classically quantiﬁes the privacy of a mechanism in
terms of parameters ε:
Deﬁnition (ε-diﬀerential privacy [Dwo06]). An algo-
rithm A is ε-DP (ε ∈ [0,∞)]) if for any two datasets
D1 and D2 that diﬀer on a single element and for any
set of possible outputs O:
Pr(A(D1) ∈ O) ≤ eε · Pr(A(D2) ∈ O)
DP gives a strong guarantee that presence or absence of
a single data point will not change the ﬁnal output of the
algorithm signiﬁcantly. Furthermore, the combination of
DP mechanisms also satisﬁes DP:
Theorem (Composition Theorem [Dwo06]).
the
mechanisms Ai are εi-DP, then any sequential com-
bination of them isP
If
i εi-DP.
To achieve DP, noise must be added to the output of
the algorithm. In most cases, this noise is drawn from
a Laplacian distribution and it is proportional to the
sensitivity of the algorithm itself:
Theorem (Laplace Mechanism [Dwo06]). For
:
D → Rk,
if s is the sensitivity of f (i.e., s =
maxD1,D2 ||f(D1)− f(D2)|| for any two datasets D1 and
D2 that diﬀer on a single element) then the mechanism
A(D) = f(D) + Lap( s
ε) with independently generated
noise to each of the k outputs enjoys ε-DP.
f
2.2 Game Theory
GT [HS+88] is “the study of mathematical models of
conﬂict between intelligent, rational decision-makers”.
Almost every multi-party interaction can be modeled
as a game. In our case, these decision makers are the
participants (players) of Collaborative Learning.
Deﬁnition (Game). A normal form representation of
a game is a tuple hN , Σ,Ui, where N = {1, . . . , m} is the
set of players, Σ = {S1, . . . , Sm} where Si = {s1, s2, . . .}
is the set of actions for player i and U = {u1, . . . , um}
is the set of payoﬀ functions.
A Best Response (BR) strategy gives the most favorable
outcome for a player, taking other players’ strategies as
given:
Deﬁnition (Best Response). For a game hN , Σ,Ui the
BR strategy for player i for a given strategy vector
if ∀sij ∈ Si:
s−i = (s1, . . . , si−1, si+1, . . . , sm) is ˆsi
ui(ˆsi, s−i) ≥ ui(sij , s−i).
A Nash Equilibrium (NE) is a strategy vector where all
the player’s strategies are BR strategies. In other words,
in a NE state every player makes the best/optimal de-
cision for itself as long as the others’ choices remain
unchanged:
Deﬁnition (Nash Equilibrium). A pure-strategy NE
of a game hN , Σ,Ui is a strategy vector (s∗
1, . . . , s∗
m)
∈ Si,
for
such that
each player
where
i
∀sij ∈ Si: ui(s∗
−i) ≥ ui(sij , s∗
i , s∗
−i) where s∗
−i =
(s∗
1, . . . , s∗
i+1, . . . , s∗
NE provides a way of predicting what will happen if
several entities are making decisions at the same time
where the outcome depends on the decisions of the oth-
ers. The existence of a NE means no player will gain
more by unilaterally changing its strategy at this unique
state.
i−1, s∗
m).
s∗
i
Another concept of GT is Social Optimum, which
n∈N
max
un(s0
1, . . . , s0
is a strategy vector that maximizes social welfare:
Deﬁnition (Social Optimum). The Social Optimum of
a game hN , Σ,Ui is a strategy vector (s0
m) where
X
i ∈ Si, such that
s0
un(s1, . . . , sm) = X
s1∈S1,...,sm∈Sm
Despite the fact that no one can do better by changing
strategy, NEs are not necessarily Social Optimums (as
an example see Prisoner’s Dilemma [HS+88]). Price of
Anarchy [KP99] measures the ratio between these two: