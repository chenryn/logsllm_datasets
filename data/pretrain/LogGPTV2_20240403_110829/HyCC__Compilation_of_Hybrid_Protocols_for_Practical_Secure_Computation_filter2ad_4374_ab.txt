2.4 The CBMC-GC Compiler
Our work on hybrid compilation is built on top of the compiler
CBMC-GC [26], which provides a tool-chain from a (comparably)
high-level language, i.e., ANSI C, to Boolean circuits. CBMC-GC
has recently been extended to not only optimize circuits for size, de-
scribed in [8], but also for depth [9], which jointly with its powerful
symbolic execution, makes it a perfect candidate for hybrid compi-
lation. Building on top of CBMC-GC, HyCC considers the complete
input code as private functionality, i.e., public computations that
are performed locally by each party are not supported. The compi-
lation of mixed-mode languages, i.e., languages that combine public
and private computation, is discussed for example in [37, 45]. We
remark that common compilers like LLVM or gcc are not directly
suited for MPC compilation, as they target register-based process-
ing architectures. The creation of circuits requires a bit-precise
transformation of the input source into circuits, which is closer to
high-level synthesis used in hardware design.
I/O notation. In MPC the only available inputs and outputs of a
program are the input/output (I/O) wires of the circuit. To realize
the I/O mapping between C code and circuits, CBMC-GC uses a
special naming convention.
For example, the source code of the millionaires’ problem is
given in Listing 1. The function shown is a standard C function,
where only the input and output variables are specifically annotated
as designated input of party A (Line 2) or party B (Line 3), or as
common output for both parties (Line 4). We note that outputs can
also be assigned to specific parties only. Aside from this naming
convention arbitrary computations described in ANSI C are allowed,
excluding floating-point operations.
if ( INPUT_A_income > INPUT_B_income ) {
// Input of Party A
// Input of Party B
int INPUT_A_income ;
int INPUT_B_income ;
int OUTPUT_result = 0; // Output to both Parties
1 void millionaires_problem () {
2
3
4
5
6
7
8
9 }
Listing 1: CBMC-GC [26] code example for Yao’s
Millionaires’ problem.
OUTPUT_result = 1;
}
3 THE HYCC MPC COMPILER
Here we describe our hybrid compiler1. After introducing the chal-
lenges, we provide details on every step of the compilation chain.
3.1 Hybrid Compilation and its Challenges
We begin with a description of a straight-forward (unoptimized)
approach to compile hybrid MPC protocols from standard source
code in order to illustrate the challenges of achieving efficient
hybrid compilation. We will then refine this approach throughout
this section and describe a more advanced compilation approach.
An exemplary illustration of the necessary steps for a straight-
forward compilation is given in Fig. 2. First, the input source code
is decomposed into multiple parts, henceforth referred to as mod-
ules. Modules are the finest level of granularity used in the later
protocol selection. Thus, all code within a module is guaranteed to
be evaluated with the same MPC protocol. We remark that during
protocol evaluation this level of granularity is only forming a lower
bound. In principle, a program can also be evaluated with only a
single MPC protocol. The decomposition can be made directly on
the source code level or on an intermediate representation of the
code, e.g., Single Static Assignment (SSA) form. Given a decom-
posed application description, each module is compiled into the
circuit representations for the different MPC protocols forming
the hybrid protocol and then optimized. In this work, we consider
size-optimized Boolean circuits required for Yao sharing (Y), depth-
optimized Boolean circuits required for GMW style protocols (B),
and arithmetic circuits (A). Finally, the hybrid protocol is synthe-
sized during protocol selection and scheduling (cf. §4).
Multiple challenges (besides the complexity of compiling effi-
cient Boolean or arithmetic circuits itself) arise when following this
1An open source implementation will be made available at
https://gitlab.com/securityengineering/HyCC.
Session 5B: SecComp 2CCS’18, October 15-19, 2018, Toronto, ON, Canada850/* Computationally expensive code */
t1 = pow (x , y);
t2 = pow (2 , y);
/* Some code */
unsigned c = 1;
if ( condition )
1 long pow ( unsigned b , unsigned exp ) {
2
3 }
4
5 void main () { /* Some code */
6
7
8
9
10
11
12
13
14 }
Listing 2: Example source code to illustrate the conflict
between local and inter-procedural optimization.
c += 1;
res = pow (x , c);
/* Some code */
Figure 2: Naïve compilation of hybrid protocols from input
source code to a decomposed circuit description. First, the
code is decomposed into multiple modules. Then, each mod-
ule is translated into three different circuit formats.
straight-forward approach. All challenges relate to a trade-off be-
tween compilation resources, i.e., time and storage, and compilation
result, i.e., circuit size and depth. We describe identified challenges
and propose solutions, which motivate our actual compilation ar-
chitecture:
• Granularity of decomposition. Automatically decomposing
input code into multiple modules is a non-trivial task, as a
fine-grained decomposition limits the possibility of circuit
level optimizations and increases the complexity of the com-
putationally expensive protocol selection problem, whereas a
coarse-grained decomposition risks to miss the most efficient
selection. We tackle this challenge by the use of heuristics
based on static analysis of the source code.
• Local versus inter-procedural optimization. Optimizing an ap-
plication as a whole or optimizing its modules independently
can lead to circuits of different sizes. The former allows more
optimizations, whereas the latter is typically more efficient
w.r.t. compilation because each module will only be compiled
and optimized once.
We illustrate this conflict with the example in Listing 2. This
example consists of a function main() that performs multiple
calls to a function pow(), which computes the power of two
integers. A function-wise decomposition approach would
separate the two functions to compile them independently.
However, a careful study of the source code reveals that the
pow() function is called with a constant argument in Line 7,
and with the second argument being either one or two in
Line 12, which simplifies the computation of the exponentia-
tion function on the circuit level significantly. An optimizer
with an inter-procedural (context-sensitive) or holistic view
could detect this fact and optimize the created circuit ac-
cordingly. To find a trade-off between modular and holistic
optimization, i.e., compile time and circuit size, we rely on
static analysis and source code optimization techniques in
our compilation framework.
• Loop handling. Loops are an essential part of many programs.
To create circuits with low complexity, it is best to first unroll
(inline) all loop iterations, before translating them into a
circuit, as this allows to apply optimizations, such as constant
propagation, over all iterations. However, for compilation
efficiency, for the exploitation of parallelism, and for a more
compact circuit representation, it can be useful to avoid loop
unrolling. Therefore, instead of choosing either technique
we propose an adaptive approach that distinguishes different
loop types and then decides for or against loop unrolling.
• Efficient logic minimization. Even though we consider com-
pilation to be a one-time task, which in theory allows to use
arbitrary resources, in practice compilation efficiency is of
relevance. Optimizing circuits on the gate-level is a resource-
consuming task that can become practically infeasible when
considering circuits with Billions of gates. Therefore, we
adapt and improve a technique referred to as Source-guided
optimization [10] to optimize circuits under configurable
time constraints by distributing an optimization budget in a
controlled manner.
The sketched solutions can be realized using static source code
analysis techniques only. This is sufficient because MPC applica-
tions have to be bound (finite and deterministic runtime), as they
are evaluated independently of the program’s input to avoid any
form of information leakage. Using the side-channel free circuit
computation model, all possible program paths are visited during
protocol runtime and thus can already be studied at compile time.
3.2 Architecture
We describe our compilation architecture for a resource-constrained
environment that expects a source code with a pointer to an entry
function f as input, and a compilation and optimization time limit T .
The compiler outputs a program description consisting of multiple
modules, compiled to different circuit representations, and a direct
acyclic dependency graph that describes the dependencies between
the different modules. The combination of dependency graph and
modules can be used to evaluate the program in a hybrid MPC
framework.
The compilation architecture consists of multiple compilation
phases shown in the next paragraph, which themselves can consist
of multiple compilation passes. The phases are:
loop(){  ...}DecompositionfuncA(...){  ...} main() {    funcA(...);  for(...) {      ...  }  ...  funcA(...)}SourceDAG & ModulesCircuits funcA()loopiterationmain1() funcA()N xmain0(){  ...}funcA(){  ...}main1(){  ...}Translationmain0()OutputsInputsABYABYABYABYSession 5B: SecComp 2CCS’18, October 15-19, 2018, Toronto, ON, Canada851(1) Automated Parallelization (§3.2.1): Automated identification of
code blocks that can be evaluated in parallel using external
tools.
(2) Preprocessing, Lexing and Parsing (§3.2.2): Construction of an
Abstract Syntax Tree (AST) from the input code.
(3) Source Code Optimization and Loop Unrolling (§3.2.3): Source-
to-source compilation using static analysis.
(4) Code Decomposition (§3.2.4): Decomposition of the input pro-
gram into multiple modules.
(5) Circuit Compilation (§3.2.5): Compilation of each module in the
different circuit representations.
(6) Inter-Procedural Circuit Optimization (§3.2.6): Optimization of
Boolean and arithmetic circuits across multiple modules.
(7) Circuit Export (§3.2.7): Writing the decomposed circuit to a file,
ready for reconstruction in protocol selection.
Note that steps 2, 3, and 5 are also part of CBMC-GC’s original
tool-chain [26], whereas the others have been added for the com-
pilation of hybrid protocols. We describe the steps in detail in the
following subsections.
3.2.1 Automated Parallelization. Parallel code segments allow ef-
ficient compilation and protocol selection. Moreover, most MPC
protocols profit from parallelized functionalities. Therefore, their
detection is of relevance in compilation for hybrid MPC. Due to the
availability and maturity of automated parallelization tools, e.g., [29,
50], we rely on these for the detection of parallel loops, i.e., loops
that have independent loop iterations. These tools are able to detect
parallelism and to annotate parallelism using source-to-source com-
pilation techniques, independent of the HyCC compilation chain.
For annotations, HyCC relies on the OpenMP notation, which is
the de-facto application programming interface for shared mem-
ory multiprocessing programming in C and supported by most
parallelization tools. Specific preprocessing notations, e.g., #omp
parallel for, are added in the code line before each parallel loop.
The annotations are parsed in the next compilation phase.
3.2.2 Preprocessing, Lexing and Parsing. The preprocessing, lexing,
and parsing of source code is realized as in CBMC-GC [26]. We re-
mark that, as in CBMC-GC and typical for MPC, the given program
has to be bound to avoid leaking information through the program
runtime. Furthermore, global variables are not supported, which,
however, is an implementation limitation and not a limitation of
our approach. The only difference between HyCC and CBMC-GC
is that the annotated parallelism is parsed.
Source Code Optimization and Loop Unrolling. In this compi-
3.2.3
lation step the intermediate code is analyzed and optimized using
static analysis. The results are subsequently used as a preparation
step for the later code decomposition and parallelization. In detail, to
overcome the optimization limits of (context-insensitive) modular
compilation, rigorous source code optimization in form of a partial
evaluation is performed. Thus, all variables known to be constant
are propagated, such that every remaining expression (indirectly)
depends on at least one input variable (dynamic variable).
To achieve an efficient compilation result, partial evaluation re-
quires a symbolic execution of the complete source code, which
limits compilation scalability. A faster compile time can be achieved,
under a (often significant) circuit-size trade-off, when not optimiz-
ing across function or loop boundaries. For example, the circuit
compiler Frigate [41] follows this approach. To achieve the best of
both worlds, we propose a time-constrained multi-pass optimiza-
tion routine, which can be interrupted at any point in time. Given
sufficient compile time, the iterative approach converges to the
same result as a holistic optimization.
In the first pass, partial evaluation is only performed with a local
scope, yet not across function or loop boundaries. In the second
pass, constants are propagated within every function body and
between multiple functions (inter-procedural constant propagation),
yet not between multiple loop iterations or in recursive function
calls to avoid loop unrolling. This form of program specialization
can lead to an increase in the code size (function cloning), as the
same function may now appear multiple times with different signa-
tures. For example, in Listing 2, we observe that the pow() function
is called with none, either of the two, and both arguments being
constant. Hence, in this example, two, namely one with the first
argument and one with the second argument being constant, addi-
tional copies of the function will be introduced, partially evaluated,
and compiled individually.
In the third optimization pass, all (possibly nested) loops are
visited. We distinguish three types of loops: Parallel, simple, and
complex loops. Parallel loops have already been identified in the
first compilation phase. We refer to a for loop as simple if the
loop guard is constant and the iterator variable is incremented
(or decremented) in a constant interval and not written inside the
loop body. Furthermore, simple loops cannot have return or break
statements. Hence, the loop range of simple loops can be derived
without a complete symbolic execution of the loop itself. Complex
loops are all remaining loops, which require a complete unrolling
of all iterations using symbolic execution to determine their termi-
nation.
Simple and parallel loops do not need to be unrolled during
compilation, as it is sufficient to compile a single circuit for all iter-
ations that are instantiated multiple times within an MPC protocol
with the loop iterator variable as input. Nevertheless, similar to
function specialization, loop specialization is desirable for an effi-
cient compilation result. Therefore, in HyCC, loops are optimized
in an iterative approach. First, all constants that are independent
of the loop iterator variable are propagated in the loop body. This
allows an effective optimization of multiple loop iterations at the
same time. Afterwards, the first iteration of every loop is partially
evaluated. In contrast to the previous symbolic execution, the loop
iterator variable is now initialized with a constant and can lead to
further program specialization. If symbolic execution of the first
iteration leads to improvements, i.e., an expression can be evalu-
ated or removed, then the loop becomes a candidate for unrolling.
By unrolling the first loop iteration, an estimate on the computa-
tional resources required to unroll all iterations can be made. Given
sufficient remaining compile-time (and memory), the loop will be
unrolled and optimized.
Function and loop specialization may reveal constants relevant
for other code parts. Therefore, given sufficient remaining compile-
time, a further round of partial evaluation is initiated until no
further improvements are observed. Finally, a call-graph is exported
for usage in the following decomposition. Statements within loops
that have been unrolled are enriched with information about their
Session 5B: SecComp 2CCS’18, October 15-19, 2018, Toronto, ON, Canada852count = count + 1;
1 unsigned scalar = x1 * y1 + x2 * y2 ;
2 if ( scalar > min ) {
3
4 }
Listing 3: Code excerpt to illustrate code decomposition.
The scalar product of two two-dimensional vectors is
computed and compared to a reference value.
original position in the loop, to re-identify loops and their iterations
during decomposition.
3.2.4 Code Decomposition. Identifying a suitable decomposition is
the major challenge for efficient protocol partitioning. The task of
automated decomposition is to identify which parts of a code should
jointly be compiled as one module, which forms the finest level of
granularity of protocol selection. Each module has an input and
an output interface, where a module can receive input from one or
more modules and provide output to one or more modules. We refer
to the separation points between two modules as interface. Hence,
a decomposed code forms a directed acyclic graph (DAG) consisting
of modules with interfaces in-between (similar to a call-graph or
dependency graph). The first input and last output interface of the
graph are the program input and output variables, respectively.
The overall goal of a good decomposition heuristic is to identify
modules of a program that can be evaluated efficiently in a specific
circuit representation. A first example of such a heuristic are ex-
pressions consisting only of arithmetic statements. Naturally, these
should profit from processing in MPC protocols based on arithmetic
circuits. In contrast, control flow operations or comparisons are
evaluated more efficiently with Boolean circuit-based protocols.
Consequently, arithmetic and combinatorial statements should be
in different modules. We follow a multi-pass decomposition ap-
proach that starts with the complete source code as a module that
is split into more fine-granular modules in every pass.
Function decomposition. Functions already give programs a form
of modularization and hence they can be used as natural boundaries
for decomposition. Therefore, in the first compilation pass, each
function (considering the function specialization described in §3.2.3)
becomes a module. The input interface to a function module consists
of the arguments that are read in the function body and assigned to
other variables. The output interface are all pointers and variables
passed by reference that are written to in the function body, as well
as the return statement. This form of recursive decomposition
leads to three modules per (possibly nested) function call, one
module for the callee itself, one for the code before and one after
the function call.
Technically, this decomposition becomes challenging when
pointers or references are passed to a function. Using the results
of the previous (exhaustive) symbolic execution, input and output
variables can be differentiated, and array sizes can be determined
during compile time. We note that dynamic memory management,
i.e., memory that is allocated based on (private) input variables, is
impossible to be realized in the circuit computation model and is
thus outside the scope of circuit compilers.
Loop decomposition. Loops also give code a structure and are
therefore a good heuristic for decomposition. Consequently, in
the second compilation pass, every module is further decomposed
according to its loops, such that every loop iteration becomes its
own module, where all variables that are read from an outer scope
and the iterator variable form the input interface and all variables
that are written to, but defined in an outer scope, form the output
interface.
Loops might have been unrolled during code optimization
(cf. §3.2.3). For their re-identification during decomposition, loop it-
erations are marked as such during loop unrolling. Loops that have
not been unrolled during code optimization require a dedicated
handling of array accesses before decomposition, i.e., a pointer
analysis. Otherwise, array accesses that depend on the iterator
variable, which is an input variable after decomposition, would
compile into private array accesses that are of significant circuit
size [26]. For better efficiency, in HyCC these array accesses are
extracted from the loop iteration and placed in the module that
encapsulates the iteration. Consequently, these array accesses are