2.2.1
Secure two-party computation. Secure two-party compu-
tation (2PC) is a type of protocols that allow two parties to jointly
compute a function ( f1(x,y), f2 (x,y)) ← ℱ (x,y) without learning
each other’s input. It offers the same security guarantee achieved
by a trusted third party TTP running ℱ: both parties submit their
inputs (i.e., x and y) to TTP, who computes and returns the corre-
sponding output to each party, so that no information has been
leaked except the information that can be inferred from the outputs.
Basically, there are three techniques to achieve 2PC: arithmetic
secret sharing [8], boolean secret sharing [29] and Yao’s garbled
circuits [59, 60]. Each technique has its pros and cons, and they
can be converted among each other. The ABY framework [21] is a
state-of-the-art 2PC library that implements all three techniques.
2.2.2 Homomorphic encryption. A public key encryption scheme
is additively homomorphic if given two ciphertexts ˆx1 := E(pk,x1)
and ˆx2 := E(pk,x2), there is a public-key operation ⊕ such that
E(pk,x1 + x2) ← ˆx1 ⊕ ˆx2. Examples of such schemes are Paillier’s
encryption [48], and exponential ElGamal encryption [24]. This is
simply referred to as homomorphic encryption (HE).
As an inverse of addition, subtraction ⊖ is trivially supported
by additively homomorphic encryption. Furthermore, adding or
multiplying a ciphertext by a constant is efficiently supported:
E(pk,a + x ) ← a ⊕ ˆx and E(pk,a · x1) ← a ⊗ ˆx1.
To do both addition and multiplication between two ciphertexts,
fully homomorphic encryption (FHE) or leveled homomorphic en-
cryption (LHE) is needed. However, FHE requires expensive boot-
strapping operations and LHE only supports a limited number of
homomorphic operations.
2.2.3
Single instruction multiple data (SIMD). The ciphertext of
a (homomorphic) encryption scheme is usually much larger than
the data being encrypted, and the homomorphic operations on the
ciphertexts take longer time than those on the plaintexts. One way
to alleviate this issue is to encode several messages into a single
plaintext and use the single instruction multiple data (SIMD) [54]
technique to process these encrypted messages in batch without
introducing any extra cost. The LHE library [23] has implemented
SIMD based on the Chinese Reminder Theorem (CRT). In this paper,
we use(cid:72)x to denote the encryption of a vector [x1, ...,xn] in batch
using the SIMD technique.
The SIMD technique can also be applied to secure two-party
computation to reduce the memory footprint of the circuit and
improve the circuit evaluation time [11]. In traditional garbled
circuits, each wire stores a single input, while in the SIMD version,
an input is split across multiple wires so that each wire corresponds
to multiple inputs. The ABY framework [21] supports this.
3 PROBLEM STATEMENT
We consider the generic setting for cloud-based prediction services,
where a server 𝒮 holds a neural network model, and clients 𝒞s
submit their input to learn corresponding predictions. The model
is defined as:
z := (WL · fL−1 (...f1 (W1 · X + B1)...) + bL )
(3)
The problem we tackle is how to design oblivious neural networks: af-
ter each prediction, 𝒮 learns nothing about X, and 𝒞 learns nothing
about (W1,W2, ...,WL ) and (B1,B2, ...,bL ) except z. Our security
definition follows the standard ideal-world/real-world paradigm:
the adversary’s view in real-wold is indistinguishable to that in
ideal-world.
Adversary model. We assume that either 𝒮 or 𝒞 can be compromised
by an adversary 𝒜, but not at the same time. We assume 𝒜 to be
semi-honest, i.e., it directs the corrupted party to follow the proto-
col specification in real-world, and submits the inputs it received
from the environment to TTP in ideal-world. We rely on efficient
implementations of primitives (like 2PC in ABY framework [21])
that are secure against semi-honest adversaries.
A compromised 𝒮 tries to learn the values in X, and a compro-
mised 𝒞 tries to learn the values in W and B. We do not aim to
protect the sizes of X, W, B, and which f () is being used. However,
𝒮 can protect such information by adding dummy layers. Note that
𝒞s can, in principle, use 𝒮’s prediction service as a blackbox oracle
Session C3:  Machine Learning PrivacyCCS’17, October 30-November 3, 2017, Dallas, TX, USA621to extract an equivalent or near-equivalent model (model extraction
attacks [56]), or even infer the training set (model inversion [26]
or membership inference attacks [53]). However, in a client-server
setting, 𝒮 can rate limit prediction requests from a given 𝒞, thereby
slowing down or bounding this information leakage.
4 MINIONN OVERVIEW
In this section, we explain the basic idea of MiniONN by transform-
ing a toy neural network of the form:
(cid:35)
(cid:34)
(cid:34)
(cid:35)
z := W′ · f (W · x + b) + b′
b1
b2
w1,1 w1,2
w2,1 w2,2
, b =
, W =
, W′ =
(cid:34)
(4)
w′
1,1 w′
1,2
w′
2,1 w′
2,2
(cid:35)
(cid:34)
(cid:34)
x1
x2
b′
1
.
b′
2
(cid:35)
where x =
and b′ =
The core idea of MiniONN is to have 𝒮 and 𝒞 additively share
each of the input and output values for every layer of a neural
network. That is, at the beginning of every layer, 𝒮 and 𝒞 will each
hold a “share” such that modulo addition of the shares is equal to
the input to that layer in the non-oblivious version of that neural
network. The output values will be used as inputs for the next layer.
To this end, we have 𝒮 and 𝒞 first engage in a precomputation
phase (which is independent of 𝒞’s input x), where they jointly
generate a set of dot-product triplets ⟨u,v,w · r⟩ for each row of the
weight matrices (W and W′ in this example). Specifically, for each
row w, 𝒮 and 𝒞 run a protocol that securely implements the ideal
functionality ℱtriplet (in Figure 1) to generate dot-product triplets,
such that:
u1 + v1 (mod N ) = w1,1r1 + w1,2r2,
u2 + v2 (mod N ) = w2,1r1 + w2,2r2,
u′
1 + v′
1,2r′
2,
2,2r′
u′
2 + v′
2.
1 (mod N ) = w′
2 (mod N ) = w′
1 + w′
1 + w′
1,1r′
2,1r′
Input:
Output:
;
• 𝒮: a vector w ∈ Zn
• 𝒞: a random vector r ∈ Zn
N
N
• 𝒮: a random number u ∈ ZN ;
• 𝒞: v ∈ ZN , s.t., u + v (mod N ) = w · r.
.
Figure 1: Ideal functionality ℱtriplet: generate a dot-product triplet.
When 𝒞 wants to ask 𝒮 to compute the predictions for a vec-
tor x = [x1,x2], for each xi, 𝒞 chooses a triplet generated in the
precomputation phases and uses its ri value to blind xi.
x𝒞
1 := r1, x𝒮
x𝒞
2 := r2, x𝒮
1 := x1 − r1 (mod N ),
2 := x2 − r2 (mod N ).
𝒞 then sends x𝒮 to 𝒮, who calculates
Meanwhile, 𝒞 sets:
2 + b1 + u1 (mod N ),
2 + b2 + u2 (mod N ).
y𝒮
1 := w1,1x𝒮
1 + w1,2x𝒮
y𝒮
2 := w2,1x𝒮
1 + w2,2x𝒮
y𝒞
1 := v1 (mod N ),
y𝒞
2 := v2 (mod N ).
It is clear that
1 + y𝒮
y𝒞
y𝒞
2 + y𝒮
1 (mod N ) = w1,1x1 + w1,2x2 + b1 and
2 (mod N ) = w2,1x1 + w2,2x2 + b2.
Therefore, at the end of this interaction, 𝒮 and 𝒞 additively share
the output values y resulting from the linear transformation in
layer 1 without 𝒮 learning the input x and neither party learning y.
In Section 5.2 we describe the detailed operations for making linear
transformations oblivious.
For the activation/pooling operation f (), 𝒮 and 𝒞 run a protocol
that securely implements the ideal functionality in Figure 2, which
implicitly reconstructs each yi := y𝒞
(mod N ) and returns
i
:= f (yi ) − x𝒞
x𝒮
is 𝒞’s component of a previously
i
shared triplet from the precompuation phase, i.e., x𝒞
1 and
2 := r′
x𝒞
2. In Sections 5.3 and 5.4, we show how the ideal function-
ality in Figure 2 can be concretely realized for commonly used
activation functions and pooling operations.
to 𝒮, where x𝒞
i
1 := r′
+ y𝒮
i
i
(cid:35)
Input:
Output:
• 𝒮: y𝒮 ∈ ZN ;
• 𝒞: y𝒞 ∈ ZN .
• 𝒮: a random number x𝒮 ∈ ZN ;
• 𝒞: x𝒞 ∈ ZN s.t., x𝒞 +x𝒮 (mod N ) = f (y𝒮 +y𝒞 (mod N )).
Figure 2: Ideal functionality: oblivious activation/pooling f ().
The transformation of the final layer is the same as the first layer.
1 + u′
2 + u′
1 (mod N ),
2 (mod N );
and 𝒞 sets:
2 + b′
2 + b′
Namely, 𝒮 calculates:
1 := w′
y𝒮
2 := w′
y𝒮
1 + w′
1,2x𝒮
1,1x𝒮
1 + w′
2,2x𝒮
2,1x𝒮
1 := v′
y𝒞
1 (mod N ),
2 := v′
y𝒞
2 (mod N ).
1 ,y𝒮
At the end, 𝒮 returns [y𝒮
predictions:
z1 := y𝒞
z2 := y𝒞
1 + y𝒮
1 ,
2 + y𝒮
2 .
2 ] back to 𝒞, who outputs the final
Note that MiniONN works in ZN , while neural networks require
floating-point calculations. A simple solution is to scale the floating-
point numbers up to integers by multiplying the same constant to
all values and drop the fractional parts. A similar technique is used
to reduce memory requirements in neural network predictions,
at negligible loss of accuracy [42]. We must make sure that the
absolute value of any (intermediate) results will not exceed ⌊N /2⌋.
5 MINIONN DESIGN
5.1 Dot-product triplet generation
Recall that we introduce a precomputation phase to generate dot-
product triplets, which are similar to the multiplication triplets used
in secure computations [8]. Multiplication triplets are typically
generated in two ways: using homomorphic encryption (HE-based)
or using oblivious transfer (OT-based). The former is efficient in
terms of communication, whereas the latter is efficient in terms of
computation. Both approaches can be optimized for the dot-product
Session C3:  Machine Learning PrivacyCCS’17, October 30-November 3, 2017, Dallas, TX, USA622generation [44]. In the HE-based approach, dot-products can be
calculated directly on ciphertexts, so that both communication and
decryption time can be reduced.
We further improve the HE-based approach using the SIMD
batch processing technique. The protocol is described in Figure 3.
Using the SIMD technique, 𝒮 encrypts the whole vector w into a