title:Dependence Makes You Vulnberable: Differential Privacy Under Dependent
Tuples
author:Changchang Liu and
Supriyo Chakraborty and
Prateek Mittal
Dependence Makes You Vulnerable: Differential
Privacy Under Dependent Tuples
Changchang Liu1, Supriyo Chakraborty2, Prateek Mittal1
Email: PI:EMAIL, PI:EMAIL, PI:EMAIL
1Department of Electrical Engineering, Princeton University
2IBM T.J. Watson Research Center
it guarantees that
Abstract—Differential privacy (DP) is a widely accepted
mathematical framework for protecting data privacy. Simply
stated,
the distribution of query results
changes only slightly due to the modiﬁcation of any one tuple
in the database. This allows protection, even against powerful
adversaries, who know the entire database except one tuple. For
providing this guarantee, differential privacy mechanisms assume
independence of tuples in the database – a vulnerable assumption
that can lead to degradation in expected privacy levels especially
when applied to real-world datasets that manifest natural depen-
dence owing to various social, behavioral, and genetic relation-
ships between users. In this paper, we make several contributions
that not only demonstrate the feasibility of exploiting the above
vulnerability but also provide steps towards mitigating it. First,
we present an inference attack, using real datasets, where an
adversary leverages the probabilistic dependence between tuples
to extract users’ sensitive information from differentially private
query results (violating the DP guarantees). Second, we introduce
the notion of dependent differential privacy (DDP) that accounts
for the dependence that exists between tuples and propose a
dependent perturbation mechanism (DPM) to achieve the privacy
guarantees in DDP. Finally, using a combination of theoretical
analysis and extensive experiments involving different classes of
queries (e.g., machine learning queries, graph queries) issued over
multiple large-scale real-world datasets, we show that our DPM
consistently outperforms state-of-the-art approaches in managing
the privacy-utility tradeoffs for dependent data.
I.
INTRODUCTION
Information sharing is key to realizing the vision of
a data-driven customization of our environment. Data that
were earlier locked up in private repositories are now being
increasingly shared for enabling new context-aware applica-
tions, better monitoring of population statistics, and facilitating
academic research in diverse ﬁelds. However, sharing personal
data gives rise to serious privacy concerns as the data can
contain sensitive information that a user might want to keep
private. Thus, while on one hand, it is imperative to release
utility-providing information, on the other hand, the privacy of
users whose data is being shared also needs to be protected. To-
wards this end, the notion of Differential Privacy (DP), which
Permission to freely reproduce all or part of this paper for noncommercial
purposes is granted provided that copies bear this notice and the full citation
on the ﬁrst page. Reproduction for commercial purposes is strictly prohibited
without the prior written consent of the Internet Society, the ﬁrst-named author
(for reproduction of an entire paper only), and the author’s employer if the
paper was prepared within the scope of employment.
NDSS ’16, 21-24 February 2016, San Diego, CA, USA
Copyright 2016 Internet Society, ISBN TBD
http://dx.doi.org/10.14722/ndss.2016.23279
provides a rigorous mathematical foundation for deﬁning and
preserving privacy, has received considerable attention [12]–
[16]. Used for protecting the privacy of aggregate query results
over statistical databases, DP guarantees that the distribution
of query outputs changes only slightly with the modiﬁcation
of a single tuple in the database. Thus, the information that
an adversary can infer through observing the query output is
strictly bounded by a function of the privacy budget.
To provide its guarantees, DP mechanisms assume that the
data tuples (or records) in the database, each from a different
user, are all independent. This is a weak assumption, especially
because tuple dependence occurs naturally in datasets due
to social, behavioral and genetic interactions between users.
For example, in a social network graph (with nodes repre-
senting users, and edges representing ‘friendship’ relations),
the ‘friendship’ between two nodes, not explicitly connected
in the graph, can be inferred from the existence of edges
between other nodes [28]. Private attributes in a user’s record
can be inferred by exploiting the public attributes of other
users sharing similar interests [6]. A user’s susceptibility to
a contagious disease can be easily inferred by an adversary
who has access to noisy query results and is aware of the
fact that the user’s immediate family members are part of the
database being queried [24]. Social and behavioral dependence
have also been used to perform de-anonymization attacks on
released datasets [22], [32], [33], [37].
The fact that dependence (or correlation) among tuples
can degrade the expected privacy guarantees of DP mecha-
nisms was ﬁrst observed by Kifer et al. [24], and later in
[8], [21], [25], [40]. Based on our own experiments with real-
world datasets in Section IV, we attribute this degradation to
a faster exhaustion of the privacy budget in DP. In prior work,
the Pufferﬁsh framework [25], proposed as a generalization
of DP,
incorporated adversarial belief about existing data
relationships using a data generation model maintained as a
distribution over all possible database instances. However, the
framework did not propose any speciﬁc perturbation algorithm
to handle the dependence. The Blowﬁsh framework [21],
which is a subclass of the Pufferﬁsh framework, allowed
users to specify adversarial knowledge about
the database
in the form of deterministic policy constraints and provided
perturbation mechanisms to handle these constraints. Finally,
to handle correlation in network data using DP, the authors
in [8] multiplied the sensitivity of the query output with
the number of correlated records. This technique resulted in
excessive noise being added to the output severely degrading
the utility of the shared data, which serves as the baseline
approach in our experiments.
this
adversary,
real-world
In this paper, we formalize the notion of dependent
differential privacy (DDP) to handle probabilistic dependence
constraints between tuples while providing rigorous privacy
guarantees. We further develop an effective dependent pertur-
bation mechanism (DPM) to achieve the privacy guarantees in
DDP. Our mechanism uses a carefully computed dependence
coefﬁcient that quantiﬁes the probabilistic dependence between
tuples in a ﬁne-grained manner. We interpret this coefﬁcient as
the ratio of dependent indistinguishability of a tuple which is
the maximum change in a query output due to the modiﬁcation
of another dependent tuple and self indistinguishability which
is the maximum change in a query output due to modiﬁcation
of the tuple itself. In summary, our paper makes the following
contributions:
• Inference Attack: Using
datasets we
demonstrate the feasibility of an inference attack on
differentially private query results by utilizing the
dependence between tuples. We show that an adversary
can infer sensitive location information about a user
from private query outputs by exploiting her
social
relationships. Furthermore,
even with
partial knowledge of both the user’s social network and
the tuple database, can extract more sensitive location
information than the adversary in DP (that knows all the
tuples in the database except one but is unaware of their
dependence relationships), thus violating the DP guarantees.
• Dependent Differential Privacy: We formalize the notion
to defend against adversaries who have prior
of DDP,
information about
the probabilistic dependence between
tuples in a statistical database. We then show that it is
possible to achieve the DDP guarantees by augmenting
the Laplace mechanism, used for achieving the DP
guarantees, with a dependence coefﬁcient. The coefﬁcient
allows accurate computation of the query sensitivity for
dependent data, thus minimizing the noise that needs to
be added providing better utility at the same privacy level.
Furthermore, we prove that our dependent perturbation
mechanism is also resilient to composition attacks [11], [18].
• Evaluation: Our proposed dependent perturbation mecha-
nism applies to any class of query functions. Using extensive
evaluation involving different query functions (e.g., machine
learning queries such as clustering and classiﬁcation, and
graph queries such as degree distribution) over multiple
large-scale real-world datasets we illustrate that our DPM
outperforms state-of-the-art approaches in providing rigor-
ous privacy and utility guarantees for dependent tuples.
II. PRELIMINARIES
In this section, we introduce terms used in formalizing
the notion of differential privacy.
A. Differential Privacy
Differential privacy is a rigorous mathematical framework
aimed at protecting the privacy of users’ sensitive information
in a statistical database [12]–[16]. The threat to privacy arises
from the release of aggregate query results computed over
the statistical database. The goal of DP is to randomize
the query results to ensure that the risk to a users’ privacy
2
does not
increase substantially (bounded by a function of
the privacy budget ) as a result of participating in the
statistical database. We represent a statistical database using a
vector D = [D1, D2,··· , Dn] drawn from domain D, where
Di ∈ Rm denotes the data of the ith user. The notion of
-differential privacy is formally deﬁned as:
Deﬁnition 1. (-differential privacy) [12] A randomized algo-
rithm A provides -differential privacy if for any two databases
D, D(cid:48) that differ in only a single entry, and for any output S,
max
D,D(cid:48)
(1)
where A(D) (resp. A(D(cid:48))) is the output of A on input D
(resp. D(cid:48)) and  is the privacy budget. Smaller value of the
privacy budget  corresponds to a higher privacy level.
P (A(D) = S)
P (A(D(cid:48)) = S)
≤ exp()
B. Achieving Differential Privacy
(cid:17)
σ
The Laplace Perturbation Mechanism (LPM), proposed
in [12], achieves -differential privacy. The key idea is to use
noise drawn from a suitable Laplace distribution to perturb the
query results before their release. Let Lap(σ) denote a zero
mean Laplace distribution with scaling factor σ. The corre-
sponding density function is given by f (x) = 1
.
For a query output of dimension q, LPM uses a noise vector
Lapq(σ) where each dimension of the vector is drawn inde-
pendently from the distribution Lap(σ).
(cid:16)−|x|
2σ exp
Integral to the design of the LPM is the global sensitivity
parameter ∆Q, computed for the issued query function Q, and
is deﬁned as follows:
Deﬁnition 2. (Global sensitivity) [12] The global sensitivity
of a query function Q : D → Rq, issued on database D, is
the maximum difference between the outputs of the function
when one input changes (i.e., D and D(cid:48) differ in only a single
entry). Formally,
∆Q = max
D,D(cid:48) (cid:107)Q(D) − Q(D(cid:48))(cid:107)1
(2)
Theorem 1. -differential privacy is guaranteed if the scaling
factor σ in the Laplace distribution is calibrated according to
the global sensitivity ∆Q. For any query function Q over an
arbitrary domain D, the mechanism A
A(D) = Q(D) + Lap(∆Q/)
(3)
achieves -differential privacy (see [12] for detailed proof).
III. ADVERSARIAL MODEL
The popularity of DP as a privacy deﬁnition (recall
Deﬁnition 1) stems from the fact that it makes no assumptions
about the background knowledge available to an adversary.
In other words, mechanisms such as LPM, that satisfy the DP
deﬁnition, guarantee that users’ sensitive data are protected re-
gardless of adversarial knowledge. However, the privacy guar-
antees provided by the existing DP mechanisms are valid only
under the assumption that the data tuples forming the database
are pairwise independent (which is also implicitly assumed by
the DP adversary model) [8], [21], [24], [25], [27], [40]. In
reality, this assumption is a cause of vulnerability as data from
different users can be dependent, where the dependence can
• Access to D−i: Data of all the other n− 1 users (excluding
the ith user), denoted by D−i, is available to the adversary.
This property makes the DDP-adversary as powerful as a
DP-adversary.
• Access to joint distribution P (D1, . . . , Dn): The adversary
uses auxiliary channels (e.g., the Gowalla social network
in our attack in Section IV) to estimate the joint proba-
bility distribution P (D1, . . . , Dn), between the data tuples.
This property together with access to D−i makes a DDP-
adversary more powerful than a DP-adversary.
In the remaining paper, unless otherwise speciﬁed, the privacy
deﬁnitions and guarantees are all with respect to the DDP-
adversary. In Section IV, we perform a real-world inference
attack to demonstrate that a DDP-adversary can extract more
private information than guaranteed by DP. In Section V, we
develop a new privacy deﬁnition dependent differential privacy
(DDP) that allows for dependence between data tuples in the
database. In Section VI, we propose a privacy mechanism to
satisfy the DDP deﬁnition. We establish formal guarantees
for our privacy mechanism and illustrate its efﬁcacy using
experiments on large-scale real-world datasets.
IV.
INFERENCE ATTACK: DIFFERENTIAL PRIVACY UNDER
DEPENDENT TUPLES
Real-world datasets are complex networks that exhibit
strong dependence (correlations) and their release introduces
various privacy challenges. Adversaries can combine the re-
leased obfuscated data (generated by applying the privacy
mechanisms on the data), with knowledge of the existing
dependence relations to infer users’ sensitive information.
There exist
limited prior work that have outlined realistic
inference attacks exposing the vulnerability of DP mechanisms
under dependent data tuples [24], [25]. In this section, we
demonstrate (1) a real-world inference attack on the LPM-
based differential privacy mechanism, as a realistic conﬁrma-
tion of the feasibility of such attacks in practical scenarios; and
(2) the capability of a DDP-adversary to use released data,
satisfying DP deﬁnition, to build an inference attack which
violates the security guarantees of DP mechanisms. Before
outlining our real inference attack for DP we compare our
work with existing related work [17], [18], [24] to highlight
the importance of our attack.
• Ganta et al. in [18] explored how one can reason about
privacy in the presence of independent anonymized releases
of overlapping data. Compared with our inference attack,
they do not consider the dependence between data tuples in
their attack.
• Fredrikson et al. in [17] considered predicting a patient’s
genetic marker from the differentially private query results
by utilizing demographic information about that patient.
Thus, the auxiliary information used in this attack is ad-
ditional information about a patient (single tuple) and not
dependence between tuples.
• Kifer et al. in [24] investigated the inference about the
participation of an edge in a social network through observ-
ing the number of inter-community edges. The inference
performance varied with different network generation mod-
els. In contrast to the theoretical work of Kifer et al., we
demonstrate inference attacks using real data on complex
differentially private machine learning queries.
Fig. 1. Dependence between tuples can seriously degrade the
privacy guarantees provided by the existing differential privacy
mechanisms.
be due to various social, behavioral and genetic interactions
that might exist between users. An active adversary can use
auxiliary information channels to access these dependence and
exploit the vulnerabilities in DP mechanisms as illustrated by
the simple example below.
Example 1: Consider a database D = [Di, Dj] where
Di, Dj have a probabilistic dependence as Dj = 0.5Di+0.5X
and Di, X are independently and uniformly distributed within
[0, 1] as shown in Fig. 1. Below we consider a simple inference
attack in which an adversary issues a sum query Q(D) =
Di + Dj and uses the query result to infer the value of Di.
First, we consider a DP-adversary, one that assumes
independence between Di and Dj. Following the LPM mech-
anism, we add Laplace noise with parameter 1/ 1 which
allows us to achieve −differential privacy guarantee, i.e.,
max
P (A([Di=0,Dj ])=S))
P (A([Di=1,Dj ])=S)) = max
Next, for the same inference attack, we consider a more
powerful adversary, one that not only has all the properties
of the DP-adversary but in addition also knows the depen-
dence relation between Di and Dj. Using the same LPM
mechanism with Laplace noise of parameter 1/ for such an
adversary results in a much weaker privacy guarantee, i.e.,
≤
max
exp(1.5).
(cid:82) 1
(cid:82) 1