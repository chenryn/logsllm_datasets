b h
b i s o r
t
e m 3 d
t h
h e a l
m s t
p e r
i m e t e r
p o w e r
t
r e e a d d
t s p
v o r o n o i
g e o m e a n
Fig. 9: Slowdown for the pointer-intensive Olden [27] suite
for MarkUs, compared with results reported from Dhurjati
and Adve [6]. As MarkUs does not increase TLB pressure,
it performs signiﬁcantly better, and more reliably.
n
w
o
d
w
o
S
l
 2.2
 2
 1.8
 1.6
 1.4
 1.2
 1
 0.8
b w a v e s
c a ctu B
N lb
m
S
S
p o p 2
w rf
i m
foto nik 3 d
a gic k
n a b
ro
m s
p erlb e n c h
g c c
n etp p
m cf
x ala n c b
m
o
e x c h a n g e 2
m k
d e e p sje n g
x 2 6 4
le ela
x z
m
g e o
e a n
Fig. 10: Slowdown for SPECspeed 2017 with MarkUs, using
four threads on our four-core system.
B. Olden Overheads
Techniques that use a page-table entry per allocation to enforce
use-after-free safety, such as Oscar [7] and Dhurjati and
Adve [6], can incur even heavier costs for pointer-intensive
workloads, where TLB pressure causes performance to drop
dramatically. As we see in the evaluation on Olden [27]
(ﬁgure 9), this is not the case for MarkUs, which can efﬁciently
execute even under such complex scenarios. While Dhurjati
and Adve suffer up to 11× overhead, and Oscar would likely
suffer similar overheads through using a similar strategy,
though we cannot verify this as no source is available, MarkUs
is never slowed down by more than 1.4× (1.1× average).
C. SPEC 2017 OpenMP Overheads
Figure 10 shows that, since MarkUs is implemented as an
extension to a parallel, concurrent garbage collector and allo-
cator [16], it works equally-well for multithreaded workloads.
Using MarkUs with SPECspeed 2017 and OpenMP results in a
slowdown of only 13%. Workloads are slowed down in similar
areas to their SPEC 2006 counterparts: gcc is slowed down
by the allocator (though others such as lbm, xz, and nab are
sped up by it), and xalancbmk and omnetpp are slowed down
through marking procedures. Of the newer workloads, only wrf
and roms are slowed down signiﬁcantly. The former suffers
from the marking procedure, the latter from metadata overhead
due to less reuse of virtual pages. Still, typical overheads are
very low, and in line with the single-threaded SPEC 2006.
D. BBench Overheads
Figure 11(a) shows that the overheads are similar for complex
and highly-threaded browser workloads, in that the average
n
w
o
d
w
o
S
l
 1.8
 1.7
 1.6
 1.5
 1.4
 1.3
 1.2
 1.1
 1
 0.9
 0.8
v
o
e r a ll
a m a
n
o
z
c
b
b
c
n
n
c r a i g
s li s t
y
a
b
e
p
s
e
g l e
n
g
o
o
m s
n
s l a
h
s
o t
d
t w itt e r
y
o
e
b
u t u
(a) Slowdown for BBench. Error bars show the range of load times
from each successive page load, and result from marking procedure
costs not being evenly shared across all pages, in addition to existing
variance even without MarkUs.
craigslist
ebay
espn
twitter
youtube
geomean
google
msn
slashdot
amazon
bbc
cnn
i
e
m
T
r
e
d
n
e
R
d
e
s
)
n
a
e
m
o
e
G
d
e
h
g
e
W
-
5
(
t
i
i
l
a
m
r
o
N
 1.4
 1.3
 1.2
 1.1
 1
 0.9
 0.8
 0.7
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
Run Number
(b) The execution time of twenty rounds of BBench, normalised to
overall average render time for each page. We see that there is no
pattern – MarkUs does not cause increasing slowdowns over time.
Fig. 11: Experiments for MarkUs on BBench [28] in Firefox.
performance overhead is just 15% across the webpages loaded
by BBench [28] in Firefox. Because BBench measures short
individual page loads across an entire process invocation, we
do see some variance as a result of marking procedures being
invoked at different times during multiple loads of the same
page. Still, this is relatively limited, with worst cases well
below 2×, and a more mature implementation would be able
to limit this further through more ofﬂoading to other threads,
and a more incremental collection strategy.
As BBench can be repeated multiple times within the same
Firefox process invocation, we can use it to see how MarkUs
copes with long execution times. We see in ﬁgure 11(b) that,
despite the large variance in page rendering times throughout
repetition of BBench, there is no overall trend over successive
iterations — the load of a page in the 20th iteration of BBench
is similar to that on the ﬁrst iteration.
E. Memory-Performance Tradeoffs
Because we know how much data the application has freed
since the last marking procedure, as discussed in section III-D,
we can adjust the frequency of marking based on the size of
the quarantine list. This gives us a tradeoff between memory
utilisation and performance, which we explore in ﬁgure 12.
As should be expected, the larger the maximum size of the
quarantine list, the higher the increase in average memory
consumption. The exception to this is dealII, where most
allocation is performed on large objects that can be imme-
diately unmapped on a free, and so similar memory overhead
is observed regardless of frequency of marking.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:38:23 UTC from IEEE Xplore.  Restrictions apply. 
586
In terms of performance, dealII again shows a ﬂat curve,
only increasing mildly under very small quarantine-list sizes,
where the frequency of marking procedures starts to impact
execution. As most allocations are to large objects that can
be immediately unmapped,
the size of the quarantine list
only grows slowly, and so all
intermediate sizes for the
quarantine list give identical performance, and CPU overhead
is relatively stable regardless of setting. Perlbench is slowed
down moderately by MarkUs under extreme settings, though
only to 1.5× maximum, and reaches negligible overheads with
larger quarantine-list sizes. Similarly, additional CPU overhead
from marking procedures, run in parallel by the Boehm-
Demers-Weiser collector, is insigniﬁcant except from when
collections are extremely frequent. Xalancbmk and omnetpp,
by contrast, represent a more distinct tradeoff, were we can
directly increase performance by allowing higher memory con-
sumption: smaller quarantine-list sizes result in signiﬁcantly
lower performance coupled with signiﬁcant burden on other
cores from the parallel marking procedure.
F. Overhead Impact of Optimisations
MarkUs includes several features intended to improve perfor-
mance over the basic combination of a quarantine list and
garbage-collection-style marking procedure, described in sec-
tion III. In ﬁgure 13 we show each optimisation’s importance
in terms of performance, memory, and CPU utilisation over-
head, and consider them here in turn for the four allocation-
intensive benchmarks from SPEC CPU2006 [24], as identiﬁed
by Dang et al. [7]. We see that, even if garbage collection
were safe in C and C++, the performance overhead would be
intolerable: the optimisations brought about by MarkUs are
necessary for practical use.
No Optimisation
For the allocation-intensive benchmarks
from SPEC CPU2006 [24], the overheads of the basic collector
in terms of performance and CPU utilisation are too high
for the technique to be worthwhile. For example, xalancbmk
(ﬁgure 13(b)) sees a slowdown of over 30× along with over
twice the CPU utilisation. This is because the Boehm-Demers-
Weiser collector performs a marking procedure whenever it
runs out of memory, even when no allocations can be freed,
as it does not have the information available to do better.
More surprisingly, memory overheads can also be unfavor-
able, with perlbench (ﬁgure 13(a)) and dealII (ﬁgure 13(d))
suffering from 2.5× and 7.5× increases respectively, from
memory leaks for large allocations with dangling pointers, and
the average consumption being pushed up by spending large
amounts of time in allocation-intensive regions due to frequent
marks signiﬁcantly reducing performance. By comparison, the
benchmarks featuring many small allocations, xalancbmk and
omnetpp, suffer very low memory overheads, from the high
frequency of collection and the low probability of conservative
pointer aliasing for small allocations.
Page Unmapping
The situation is considerably improved
for the benchmarks that feature large memory allocations
(dealII and perlbench) with the immediate unmapping of
virtual pages following a free of a large allocation. This
prevents any signiﬁcant memory leaks, as the large objects that
are probabilistically most likely to suffer from conservative
pointer aliasing, and from dangling pointers, have their physi-
cal memory cost eliminated. In addition, as the allocator is free
to reuse unmapped pages with a new virtual address before a
marking procedure, the overhead of marking is signiﬁcantly
reduced. Still, for workloads with small allocations below the
size of a page, marking procedures are still frequent, and
performance is low, particularly for xalancbmk.
Mark Frequency Optimisation
The over-zealous marking
for benchmarks such as xalancbmk and omnetpp is drasti-
cally reduced when we delay marking procedures until the
programmer has attempted to free sufﬁcient data. This extra
knowledge, unavailable to a garbage collector, allows us to
reduce the overheads from over 30× to 1.7× for xalancbmk.
As we deliberately trade off memory consumption for
performance, we may expect average memory utilisation to go
up, and this is true for perlbench and omnetpp. In particular,
these workloads suffer from signiﬁcant overhead not
just
because of the deliberate tradeoff, but because delaying the
marking procedure results in many pages of objects of each
size being created in between marking procedures. These are
returned to individually-sized free lists and never to the main
pool, causing signiﬁcant overhead. However, we also see the
opposite occurring, with dealII exhibiting a lower overhead
with mark frequency optimisation than without it. This is
because we can use the quarantine-list size to trigger, as well as
prevent the triggering of, a marking procedure. Therefore early
marking procedures, before the program runs out of allocated
memory, can reduce overheads.
All
four
Small-Object Block Sweeping
allocation-
intensive benchmarks have their memory consumption im-
proved by returning all entirely-free blocks of allocated objects
to the general pool, as the overhead of generating many
blocks of objects between marking procedures becomes only
temporary, rather than for the entire execution of a program.
For some programs, the overheads are even lower than with-
out mark-culling, as even with frequent marking procedures,
triggered on every new memory allocation, we can still have
blocks of objects that are only used during some allocation
phases, and wasted for the rest of the program.
As small-object block sweeping increases the amount of
computation necessary at the end of a marking-procedure,
the impact on performance is usually less positive, with
xalancbmk in particular seeing a slowdown. Still, dealII sees a
performance improvement, because of the signiﬁcant reduction
in memory consumption contributing to a reduction in marking
procedures and better locality of data.
G. Allocator Overheads
Not all of the overheads of our sample implementation of
MarkUs can be attributed to the marking procedure. Figure 14