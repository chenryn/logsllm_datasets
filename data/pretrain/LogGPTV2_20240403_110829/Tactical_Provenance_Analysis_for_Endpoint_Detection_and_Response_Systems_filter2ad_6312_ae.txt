10
15
20
25
ResponseTimes(sec)
(a) Response times to generate prove-
nance graphs for all alerts.
(b) Response times to generate TPGs
with their threat scores.
Fig. 9: CDF of response times to run RapSheet analysis.
more effective than the other scheme. Figure 8 shows the
cumulative distribution function for ranked true attack and
false alarm TPGs based on threat scores. When we set a
threshold (shown with a vertical red line) that captures 100%
of true positives, we can remove 97.8% of false TPGs since
all true attack TPGs are scored signiﬁcantly higher than most
false alert TPGs. At this threshold, RapSheet has a 2.2% FPR.
Note that the goal of RapSheet is not to eliminate false TPGs
from consideration, but to prioritize TPG investigation based
on their threat score. The threshold is a conﬁgurable parameter
and can be set more conservatively or aggressively based on
the goals of a particular enterprise security team. A ranked list
of the TPGs with the highest threat scores in our evaluation
is presented in Appendix C.
D. Response Times
To answer RQ2, we measured the TPG generation query
response (turn-around) time for all the alerts in our evaluation
dataset. We divided the response time of TPG generation
queries into two parts. First, we measured how long RapSheet
takes to generate the provenance graph for each alert in our
58,096 alerts dataset. These provenance graphs are generated
by performing backward and forward tracing queries for each
alert, which reads the provenance graph database from disk.
Figure 9a shows the cumulative distribution function (CDF) of
response times for all the alerts. The results show that for 80%
of alerts, RapSheet generates the provenance graph in less than
10 secs. Note that most of this time was spent in disk reads,
which we can likely speed up using existing main-memory
graph databases [65], [66].
Second, we measured the response time for performing
tactical provenance analysis, which includes ﬁrst extracting
the IIP graph from the provenance graph of each alert,
transforming this IIP vertex graph into a TPG, and ﬁnally
assigning threat score to the TPG. For this response time, we
assume that the provenance graph of the alert (from Figure 9a)
is already in the main memory. Figure 9b shows that RapSheet
was able to perform tactical provenance analysis and calculate
threat scores on 95% of all the alerts in less than 1 ms.
E. Graph Reduction
n
o
i
t
c
u
d
e
R
e
g
d
E
%


90
80
70
60
50
40
30
20
10
0
Hosts
Fig. 10: Percentage of edges removed from each host’s provenance
graph after applying our graph reduction algorithm.
F
D
C
1
0.8
0.6
0.4
0.2
0
0
20
40
80
60
Time[min]
100
120
140
Fig. 11: CDF of running graph reduction algorithm on each of the
hosts’ provenance graph.
RapSheet reduces the graph size by 63%, increasing log buffer
capacities by 2.7 times. Note that we saw a similar reduction in
the number of vertices. In other words, the same end host can
store 2.7 times more data without affecting storage capacity
provided by EDR and data processing efﬁciency. This shows
that skeleton graphs can effectively reduce log overhead.
Since currently RapSheet does not support cross-machine
provenance tracking, our graph reduction algorithm is limited
to ensure the correctness of causality analysis. Recall that our
reduction algorithm does not remove a provenance path if it
leads to some alert. So in our implementation we conserva-
tively assume all the network connections made to hosts within
our enterprise can lead to an alert and thus do not remove such
network connections during the reduction process (Line 21 in
Algorithm 16). We expect to see a further reduction in graph
size once we incorporate cross-machine provenance analysis
using the methodology described in Section IX and remove
our assumption.
We also measured the cost of running our graph reduction
algorithm on the full provenance graphs for the full duration
of our data collection for each machine. The results are shown
in Figure 11. As we can see, graph reduction ﬁnished in under
15 minutes on 80% of the hosts. In the worst case, one host
took around two hours to ﬁnish. Upon further investigation,
we found that this host has the highest number of edges in our
dataset with 1.5M edges while the average is 370K edges. This
overhead, which can be scheduled at times when machines
are not busy, is acceptable for enterprises since the beneﬁt
of extra storage space from pruning graph (Section II) while
maintaining alert scoring and correlation outweighs the cost
of running the graph reduction algorithm.
To answer RQ3, we measured the graph size reduction from
applying the technique discussed in Section VI. Figure 10
shows the percentage reduction in the number of edges for the
34 hosts in our evaluation, one bar for each host. On average,
F. APT Attack Campaign Case Studies
For our evaluation, we analyzed APT attacks from two well-
known threat groups (APT3 and APT29) and one custom-
designed attack executed using the MITRE CALDERA frame-
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:41:13 UTC from IEEE Xplore.  Restrictions apply. 
1182
cmd.exe
T1086
PS_download_exec
execution
powershell.exe
T1085
Rundll32
defense-evasion,execution
T1086
PS_Launch
execution
powershell.exe
T1076
RemoteDesktopProtocol
lateral-movement
rundll32.exe
rundll32.exe
src: 10.0.10.21:61026
dst: 10.0.0.10:3389
src: 10.0.10.21:61696
dst: 10.0.10.22:3389
src: 10.0.10.21:52977
dst: 10.0.10.22:3389
PROCESS_LAUNCH
rundll32.exe
T1085
Rundll32
defense-evasion,execution
rundll32.exe
T1085
Rundll32
defense-evasion,execution
rundll32.exe
rundll32.exe
rundll32.exe
cmd.exe
T1007
Tasklist
discovery
tasklist.exe
(a)
Start
T1086
PS download exec
execution
T1086
PS Launch
execution
T1085
Rundll32
defense-evasion,execution
T1076
Remote Desktop Protocol
lateral-movement
T1085
Rundll32
defense-evasion,execution
T1007
Tasklist
discovery
(b)
Fig. 12: APT3 Attack Scenario. (a) IIP Vertex graph generated by RapSheet. (b) Tactical Provenance Graph for APT3 attack after applying
readability post-processing pass. TPG is three orders of magnitude smaller than classical provenance graph. RapSheet will choose the
maximum ordered tactic sequence from this TPG for the ﬁnal threat score assignment.
work [67]. We already presented the APT29 attack scenario
as a motivating example in Section II. Details of the attack
using CALDERA, as well as further statistics about
the
provenance graphs and TPGs for all three attacks are included
in Appendix D. We now describe the APT3 attack scenario.
APT3 is a China-based threat group that researchers have
attributed to China’s Ministry of State Security. This group is
responsible for the campaigns known as Operation Clandes-
tine Fox, Operation Clandestine Wolf, and Operation Double
Tap [68]. Similar to APT29, APT3 has been well studied.
ATP3’s goals have been modeled using MITRE tactics and
techniques. In our attack scenario, we performed various
techniques from this known set ranging from System Service
Discovery (T1007) to Remote Desktop Protocol (T1076).
These techniques allowed us to achieve several of the MITRE
tactics including execution, lateral movement, and defense
evasion on the victim host. Figure 12a shows the IIP graph for
the APT3 attack scenario, while Figure 12b shows the TPG
extracted from this IIP graph. Our threat scoring algorithm
ranked this TPG at number 15 out of 681, higher than the vast
majority of the 676 false TPGs. To score this TPG, RapSheet
found the following temporally ordered sequence of tactics:
execution, defense-evasion, discovery, and lateral-movement.
VIII. RELATED WORK
This work joins a growing body of literature seeking to
bridge the gap between causal analysis and threat detection.
Holmes [43] is the ﬁrst system to demonstrate that event-
matching techniques can be applied to data provenance, and
also includes a method for threat score assignment. However,
several factors may complicate the deployment of Holmes on
top of commercial EDR tools. First, Holmes assumes 100%
log retention in perpetuity to assign threat scores and identify
alert correlations. In practice, EDR tools have limited log
buffers making such an approach practically prohibitive, a
limitation addressed in RapSheet through the introduction of
skeleton graphs. Second, Holmes assumes a normal behavior
database to reduce false alarms from benign activities, creating
a risk of adversarial poisoning of normal behavior due to
concept drift as benign usage changes; in contrast, RapSheet
makes no such assumption instead mitigates false alarms
through the construction of IIP graphs and sequence-based
threat scoring scheme. Finally, Holmes is evaluated based
on 16 author-created TTP matching rules, whereas RapSheet
makes use of 67 TTP rules written in an actual EDR tool. We
believe this distinction is signiﬁcant – 16 rules is insufﬁcient
to encode all
tactics in the MITRE ATT&CK knowledge
base, which means that Holmes would encounter more false
negatives and less false positives than an EDR tool. As a
result, while Holmes demonstrates the feasibility of EDR-like
approaches on provenance graphs, the original study cannot be
easily compared to EDR tools, which are optimized for recall.
NoDoze [38] is an anomaly-based alert triage that uses
historical information to assign threat scores to alerts. Like
Holmes, NoDoze assumes the availability of an accurate nor-
mal behavior database. Unlike RapSheet, NoDoze uses a path-
based threat scoring scheme; as we described in Section V, this
approach can miss attack-related events lie on different graph
paths. Further, both Holmes and NoDoze consider only UNIX-
like system call events when constructing provenance graphs.
As a result they do not track ALPC messages (extensively
used in Windows environment) which in practice would create
disconnected provenance graphs and admit more error into
causal analysis.
An important component of RapSheet is the log reduction
algorithm, which is a topic that
is well-studied in recent
literature [30], [29], [34], [37], [36]. In the early stages of
this study, we realized that existing log reduction techniques
were inapplicable to our design because they did not preserve
the necessary connectivity between EDR generated alerts.
For example, LogGC [30] removes unreachable events, and
thus would not be able to correlate alerts that were related
through garbage-collected paths. Similarly, Hossain et al.’s
dependence-preserving data compaction technique [37] does
not consider that some edges are alert events and must, there-
fore, be preserved. Alternately, Winnower [29] and Process-
centric Causality Approximation [34] both reduce log size by
over-approximating causal relations, introducing new sources
of false alerts. Other techniques, while similarly motivated, are
orthogonal to the present study.
In the absence of provenance-based causality, alert corre-
lation is another technique to assist analysts by correlating
similar alerts. Existing systems use statistical-, heuristic-, and
probabilistic-based alert correlation [69], [70], [71], [72], [73]
to correlate alerts. Similar approaches are used in industry
for building SIEMs [74], [75]. These techniques are based on
feature correlations that do not establish causality. In contrast,
RapSheet can establish actual system-layer dependencies be-
tween events. BotHunter [73] searches for a speciﬁc pattern
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:41:13 UTC from IEEE Xplore.  Restrictions apply. 
1183
of events in IDS logs to detect successful infections caused by
botnets. This approach relies on network-level communication
to identify the stages of a botnet infection. RapSheet, on the
other hand, uses host-level provenance graphs to chain together
different APT attack stages.
Elsewhere in the literature, several provenance-based tools
have been proposed for network debugging and troubleshoot-
ing [76], [77], [78], [79], [80]. Chen et al. [78] introduced
the concept of differential provenance to perform precise
root-cause analysis by reasoning about differences between
provenance trees. Zeno [77] proposed temporal provenance
to diagnose timing-related faults in networked systems. Using
sequencing edges Zeno was able to explain why the event
occurred at a particular time. RapSheet also uses the se-
quencing edges but to reason about dependencies between
different attack tactics. Zhou et al. [55] designed SNOOPY
a provenance-based forensic system for distributed systems
that can work under adversarial settings. RapSheet can use
tamper-evident logging from SNOOPY to defend against anti-
forensic techniques. DTaP [81] introduced a distribute time-
aware provenance system. RapSheet can leverage DTaP’s
efﬁcient distributed storage and query system to improve its
query response times.
IX. DISCUSSION & LIMITATIONS
Cross-Machine Analysis. In our experiments and implemen-
tation, we exclusively considered each host in isolation, i.e.,
cross-machine provenance was not analyzed. That said, our
method of extracting TPGs retains sufﬁcient information to
connect provenance graphs across machines through network
vertices in the same way as has been observed by previous
papers [31], [82]. Afterward, our score assignment algorithm
would work the same as in the single-machine scenario.
Online Analysis. Our implementation and experiments are
based on ofﬂine analysis. As the ofﬂine implementation is able
to process alerts in roughly 10 seconds, it is already possible
for RapSheet to provide real-time intelligence to analysts.
Adapting RapSheet to an online setting poses new challenges,
but such online solution is attainable. In an online setting,
RapSheet would need to be extended with a data structure that
tracks the threat score of the current TPG and can check if new
events need to be added to the TPG. Further, threat scoring
(Eq. 2) is monotonic, which means that it permits incremental
updates to the score without having to fully recalculate as the
TPG updates. We leave such extensions to future work.
Adaptive Attacks. When considering APT detection, it is
essential that the problem of adaptive attack behaviors be
considered. As RapSheet analyzes alerts based on the MITRE
ATT&CK kill-chain, an adaptive strategy would be for an
attacker to employ tactics in an order that violates the expected
sequence in an attempt to lower their behaviors’ threat score.
While it may be feasible to somewhat reduce a threat score
through careful attack sequencing, it is not straightforward
since in many cases one MITRE tactic cannot be performed
before another tactic has been completed. For example, in
order to perform the “Credential Access” tactic, the attacker
must ﬁrst successfully perform “Privilege Escalation” to have
the permissions necessary to open credential ﬁles. As another
example, the “Discovery” tactic, which identiﬁes other hosts in
the victim environment, is a necessary prerequisite to “Lateral
Movement”. An even more sophisticated scoring algorithm
could encode the partial order deﬁned by strict dependen-
cies between certain MITRE phases in order to reduce the
effectiveness of this already difﬁcult evasion technique. Note
that an attacker is certainly able to inject out-of-order tactics
that act as noise between the necessarily sequenced stages of
their attack. But this strategy would not reduce the ﬁnal threat
score assigned by RapSheet, since we extract the longest, not-
necessarily-consecutive subsequence of tactics from the IIP
graph that is consistent with the MITRE kill-chain ordering.
The injected noise will simply be ignored.
Limitations of APT Exercises. For obvious reasons, our
experiments are based on simulated APT behaviors, not actual
APT campaigns. Those simulations were written by expert an-
alysts at Symantec through analysis of APT malware samples.
One limitation of these simulations is that the threat actors did
not add innocuous events in between different stages of the
APT attacks, which is less realistic. That said, such activity
would not affect the threat scores assigned by RapSheet in any
way – the alerts associated with the malicious activities would
still appear in the same order in the TPG.
Missing Alerts. RapSheet’s log reduction algorithm assumes
that all the threat alerts are detected by the underlying EDR
tool. As we have seen in Section II,
is not unrealistic
to assume that most of the attack’s constituent events will
generate alerts since EDR tools are designed to optimize
recall, and hence generate alerts even when they detect low
severity, potentially suspicious activity. However, if an alert
was not caught by the underlying EDR tool, then our log
reduction may remove edges and vertices from the provenance
graph and break the linkability between existing and future
alerts. In other words, if some attack behavior does not cause
the underlying EDR to generate an alert, our log reduction
algorithm cannot necessarily preserve the ability to generate