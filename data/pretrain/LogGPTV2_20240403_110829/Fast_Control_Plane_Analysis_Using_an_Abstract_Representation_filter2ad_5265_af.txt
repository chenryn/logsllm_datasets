●
●
●
●
●●
●
●●●●●●
●
●●
●
●
●●●●●●●●●
●
●●
●●●
●
●
●●●●●●●●●●●
●
●
●
●
●
●●●●●●
●
●●
●●
●●
●
●
●
●
●
●●
●
●●●
●●
●●
●●
●
●
●●
●
●
●
●
●●●●●
●●●●
●
●
●●●●●●●●●
●●
●
●
●●
●
●●
●●
●
●
●●
●●●●●●●
●
●
●
●
●
●
●
●●
●
●
●●
●●
6
0
1
)
s
m
(
y
f
i
r
e
V
o
t
e
m
T
i
4
0
1
2
0
1
0
)
s
(
y
f
i
r
e
V
o
t
e
m
T
i
0
8
0
6
0
4
0
2
0
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●●●●●
●●●
●
●●
●
●●
●
●
●●
●
●
●
●●●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●●●●●
●●
●●●●●●●●●
●●
●●●
●
●
●
●●●
●
●●
●●●●●●●●●
●
●●●●●●●●
●●●●●●●●●
●●●
●●●●●●●●●●●●●●●●●
●
●●
●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●
●
●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●
0
100
200
Networks
300
0
100
200
Networks
300
(c) I3: Always isolated, us-
ing ARC
(d) I5: Equivalent, using ARC
Figure 8: Time required to check key invariants for all trafﬁc
classes (or pairs of trafﬁc classes) using ARC; networks are
sorted by number of trafﬁc classes.
be checked in less than 100ms. In the worst case, veriﬁca-
tion takes 1.13s. Again, the networks that take the longest to
verify are those with the most trafﬁc classes. The time per
trafﬁc class ranges from 7µs to 467µs (median 32µs). Note
that the time required for checking this property is indepen-
dent of the value of k.
Invariant I3 takes substantially longer to verify, because
we check all pairs of trafﬁc classes, as opposed to each indi-
vidual trafﬁc class. It takes about 1.7 hours to check all pairs
of trafﬁc classes in the network with the largest number of
trafﬁc classes (> 100K), but this property can be checked
in less than 1 minute for all pairs of trafﬁc classes in 73%
of networks. In practice, only a subset of trafﬁc classes in a
network require isolation, so the number of trafﬁc class pairs
that need to be checked is substantially smaller.
While I3 takes the longest to verify overall, equivalence
checking (I5) takes the most time per trafﬁc class: 0.9ms to
4.8ms (median 1.3ms). Most of this time is spent generating
and solving the linear program used to compute canonical
weights (§5.2). The former requires generating all possible
paths between SRC and DST, which is signiﬁcantly more
computationally expensive than the graph algorithms used
312
)
s
e
t
6
0
1
u
n
m
i
(
y
f
i
r
e
V
o
t
e
m
T
i
4
0
1
2
0
1
0
● Single Link
Up to 3 Links
●●●●●●●●●●●●
●●●●●●●●●●●●
●●
●
●
●●
●●●●
●●●
●
●●●●●
●●
●
●
●●●
●●
●
●●
●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●
●●
●●●
●●●●
●●
●
0
25
50
75
100
Networks
Figure 9: Time required by Batﬁsh to verify (lack of) reach-
ability across a limited set of failure scenarios; networks are
sorted by number of links.
to verify I1–I3. Nonetheless, we can check control plane
equivalence in less than one minute for 98% of networks
and less than one second for 28% of networks.
Comparison with Batﬁsh. To put our performance results
in perspective, we ran Batﬁsh [9] on the device conﬁgu-
rations from one-third of the OSP’s networks. We chose
networks of varying size and complexity. We ran Batﬁsh’s
“failure consistency” checker, which veriﬁes that each trafﬁc
class is consistently blocked or allowed when any one of the
network’s links fails. This is similar to verifying invariants
I1 and I2 (k = 2) using ARC, except veriﬁcation with ARC
covers all link failures, not just single link failures.
Figure 9 shows the time required for Batﬁsh to check the
reachability of all trafﬁc classes under a limited set of link
failures. We observe that the time taken by Batﬁsh to check
all single link failure scenarios is at least three orders of
magnitude larger than the time required for ARC-based ver-
iﬁcation to check all link failure scenarios. If we were to
run Batﬁsh for all scenarios with up to 3 link failures, the
time would further increase by up to ﬁve orders of magni-
tude making Batﬁsh impractical to use in this case.
The time required by Batﬁsh to verify invariants across a
set of link failure scenarios is a function of: (1) the number
of scenarios, and (2) the time required to generate the data
plane and verify the invariant for each scenario. In our ex-
periments, Batﬁsh takes between 48s and 131s (median 92s)
to generate the data plane and verify the invariant for each
link failure scenario. With ARC, the time required to verify
invariants across arbitrary link failure scenarios is a function
of: (1) the number of trafﬁc classes, and (2) the time re-
quired to generate the ETG and verify the invariant for each
trafﬁc class. As mentioned above, the median veriﬁcation
time per ETG for invariant I1 is 21µs and the median ETG
build time is 98µs. Thus, a network with a single link would
need to have over 773K trafﬁc classes in order for ARC to
be less efﬁcient than Batﬁsh.
Identifying problems. In addition to evaluating veriﬁcation
performance, we used the output from our tool to identify
possible conﬁguration errors. For each trafﬁc class, we com-
pared the results of I1 and I3 against the behavior of the net-
work in the absence of link failures. We assumed the control
plane was conﬁgured incorrectly if trafﬁc was blocked (or
isolated) in the failure-free scenario but not always blocked
(or isolated). We did not ﬁnd any such cases in the networks
we studied; this is likely due to the fact that the organization
whose networks we studied already employs other veriﬁca-
tion tools. However, we were able to detect such errors when
we intentionally introduced bugs into conﬁgurations mod-
eled after real networks; see examples in our repository [1].
8. CONCLUSION
We described ARC, a new high level abstraction for rout-
ing control planes that enables fast veriﬁcation of key prop-
erties under arbitrary failure scenarios. ARC achieves this
by avoiding data plane generation, which is made possible
by the nature of common veriﬁcation tasks and control plane
designs observed in networks today. On real data center con-
ﬁgurations, ARC offers orders of magnitude faster veriﬁca-
tion performance than existing veriﬁers.
This paper lays the groundwork for accelerating the veriﬁ-
cation and repair of network control planes, and a variety of
important problems remain open. These include: modeling
additional protocols and features, such as those commonly
used in service provider networks (e.g., iBGP) and for traf-
ﬁc engineering (e.g., MPLS TE [4]); generating ARCs that
over- or under-approximate pathset-equivalence to handle
non-deterministic protocols (e.g., RSVP); producing ARCs
for software-deﬁned network (SDN) control planes, includ-
ing hybrid control planes that use both SDN and traditional
distributed routing protocols; and automatically generating
minimal conﬁguration repairs (e.g., changing a minimal set
of devices or adding a minimal number of lines of conﬁg-
uration) when invariant violations are detected. We plan to
address these open problems in our future research.
9. ACKNOWLEDGMENTS
We thank the anonymous reviewers and our shepherd Brad
Karp for their insightful comments. This work is supported
by the Wisconsin Institute on Software-deﬁned Datacenters
of Madison and National Science Foundation grants CNS-
1302041, CNS-1330308, and CNS-1345249.
10. REFERENCES
[1] Abstract representation for control planes.
http://bitbucket.org/uw-madison-networking-research/arc.
[2] JGraphT. http://jgrapht.org.
[3] R. Aharoni and E. Berger. Menger’s theorem for inﬁnite
graphs. Inventiones mathematicae, 2008.
[4] D. Awduche, J. Malcolm, J. Agogbua, M. O’Dell, and
J. McManus. Requirements for trafﬁc engineering over
MPLS. RFC 2702, RFC Editor, September 1999.
[5] T. Benson, A. Akella, and D. Maltz. Unraveling the
complexity of network management. In NSDI, 2009.
[6] T. Benson, A. Akella, and A. Shaikh. Demystifying
conﬁguration challenges and trade-offs in network-based ISP
services. In SIGCOMM, 2011.
[7] Cisco Systems. BGP best path selection algorithm.
http://cisco.com/c/en/us/support/docs/ip/border-gateway-
protocol-bgp/13753-25.html.
[8] N. Feamster and H. Balakrishnan. Detecting BGP
conﬁguration faults with static analysis. In NSDI, 2005.
[9] A. Fogel, S. Fung, L. Pedrosa, M. Walraed-Sullivan,
R. Govindan, R. Mahajan, and T. Millstein. A general
approach to network conﬁguration analysis. In NSDI, 2015.
[10] E. W. Fulp. Optimization of network ﬁrewall policies using
directed acyclic graphs. In IEEE Internet Mgmt Conf, 2005.
[11] A. Gember-Jacobson, R. Viswanathan, A. Akella, and
R. Mahajan. Fast control plane analysis using an abstract
representation. Technical report, University of
Wisconsin-Madison, 2016.
[12] A. Gember-Jacobson, W. Wu, X. Li, A. Akella, and
R. Mahajan. Management plane analytics. In IMC, 2015.
[13] S. Hares and D. Katz. Administrative domains and routing
domains: A model for routing in the internet. RFC 1136,
RFC Editor, Dec 1989.
[14] Juniper Networks. Understanding BGP path selection.
http://juniper.net/documentation/en_US/junos12.1/topics/
reference/general/routing-ptotocols-address-
representation.html.
[15] P. Kazemian, M. Chang, H. Zeng, G. Varghese,
N. McKeown, and S. Whyte. Real time network policy
checking using header space analysis. In NSDI, 2013.
[16] P. Kazemian, G. Varghese, and N. McKeown. Header space
analysis: Static checking for networks. In NSDI, 2012.
[17] A. Khurshid, X. Zou, W. Zhou, M. Caesar, and P. B.
Godfrey. VeriFlow: Verifying network-wide invariants in real
time. In NSDI, 2013.
[18] P. Lapukhov, A. Premji, and J. Mitchell. Use of BGP for
routing in large-scale data centers. Internet-Draft
draft-ietf-rtgwg-bgp-routing-large-dc-07, IETF Secretariat,
Aug 2015.
[19] F. Le, G. G. Xie, D. Pei, J. Wang, and H. Zhang. Shedding
light on the glue logic of the internet routing architecture. In
SIGCOMM, 2008.
[20] H. Mai, A. Khurshid, R. Agarwal, M. Caesar, P. B. Godfrey,
and S. T. King. Debugging the data plane with Anteater. In
SIGCOMM, 2011.
[21] D. A. Maltz, G. Xie, J. Zhan, H. Zhang, G. Hjálmtýsson, and
A. Greenberg. Routing design in operational networks: A
look from the inside. In SIGCOMM, 2004.
[22] A. Slivkins. Parameterized tractability of edge-disjoint paths
on directed acyclic graphs. SIAM J. Discret. Math.,
24(1):146–157, Feb. 2010.
[23] Y. Sverdlik. Microsoft: misconﬁgured network device led to
azure outage. http://datacenterdynamics.com/servers-
storage/microsoft-misconﬁgured-network-device-led-to-
azure-outage/68312.fullarticle, July 2012.
[24] L. Yuan, J. Mai, Z. Su, H. Chen, C.-N. Chuah, and
P. Mohapatra. FIREMAN: a toolkit for FIREwall Modeling
and ANalysis. In IEEE SP, 2006.
[25] H. Zeng, P. Kazemian, G. Varghese, and N. McKeown.
Automatic test packet generation. In CoNEXT, 2012.
313