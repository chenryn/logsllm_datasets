# Abstract Representation for Control Planes (ARC)

## Introduction
This document provides an overview of the performance and capabilities of ARC, a high-level abstraction for routing control planes that enables fast verification of key properties under arbitrary failure scenarios. The paper also compares ARC with existing verifiers, such as Batfish, and discusses future research directions.

## Performance Evaluation

### Invariant Verification
The time required to check key invariants for all traffic classes (or pairs of traffic classes) using ARC is shown in Figure 8. Most networks can be checked in less than 100ms, with the worst case taking 1.13s. Networks with the most traffic classes take the longest to verify. The time per traffic class ranges from 7µs to 467µs, with a median of 32µs. This property's verification time is independent of the value of k.

#### Invariant I3
Invariant I3, which checks all pairs of traffic classes, takes substantially longer to verify. For the network with the largest number of traffic classes (> 100K), it takes about 1.7 hours to check all pairs. However, this property can be verified in less than 1 minute for 73% of networks. In practice, only a subset of traffic classes requires isolation, reducing the number of pairs that need to be checked.

#### Invariant I5
Equivalence checking (I5) takes the most time per traffic class, ranging from 0.9ms to 4.8ms, with a median of 1.3ms. Most of this time is spent generating and solving the linear program used to compute canonical weights. This process involves generating all possible paths between SRC and DST, which is computationally expensive compared to the graph algorithms used for verifying I1–I3. Nonetheless, control plane equivalence can be checked in less than one minute for 98% of networks and less than one second for 28% of networks.

### Comparison with Batfish
To provide context for our performance results, we ran Batfish on the device configurations from one-third of the OSP’s networks, chosen for their varying size and complexity. We used Batfish’s “failure consistency” checker, which verifies that each traffic class is consistently blocked or allowed when any one of the network’s links fails. This is similar to verifying invariants I1 and I2 (k = 2) using ARC, except ARC covers all link failures, not just single link failures.

Figure 9 shows the time required for Batfish to check reachability under a limited set of link failures. Batfish takes at least three orders of magnitude longer than ARC to check all single link failure scenarios. If Batfish were run for all scenarios with up to 3 link failures, the time would increase by up to five orders of magnitude, making it impractical.

The time required by Batfish to verify invariants across a set of link failure scenarios depends on:
1. The number of scenarios.
2. The time required to generate the data plane and verify the invariant for each scenario.

In our experiments, Batfish took between 48s and 131s (median 92s) to generate the data plane and verify the invariant for each link failure scenario. With ARC, the time required to verify invariants across arbitrary link failure scenarios depends on:
1. The number of traffic classes.
2. The time required to generate the ETG and verify the invariant for each traffic class.

The median verification time per ETG for invariant I1 is 21µs, and the median ETG build time is 98µs. Thus, a network with a single link would need over 773K traffic classes for ARC to be less efficient than Batfish.

### Identifying Configuration Errors
We used ARC to identify possible configuration errors by comparing the results of I1 and I3 against the behavior of the network in the absence of link failures. We assumed the control plane was configured incorrectly if traffic was blocked (or isolated) in the failure-free scenario but not always blocked (or isolated). No such cases were found in the studied networks, likely due to the organization already employing other verification tools. However, we detected such errors when intentionally introducing bugs into configurations modeled after real networks; see examples in our repository [1].

## Conclusion
ARC is a new high-level abstraction for routing control planes that enables fast verification of key properties under arbitrary failure scenarios. By avoiding data plane generation, ARC achieves orders of magnitude faster verification performance than existing verifiers on real data center configurations. Future research will focus on modeling additional protocols and features, handling non-deterministic protocols, producing ARCs for SDN control planes, and automatically generating minimal configuration repairs.

## Acknowledgments
We thank the anonymous reviewers and our shepherd Brad Karp for their insightful comments. This work is supported by the Wisconsin Institute on Software-defined Datacenters of Madison and National Science Foundation grants CNS-1302041, CNS-1330308, and CNS-1345249.

## References
[1] Abstract representation for control planes. http://bitbucket.org/uw-madison-networking-research/arc.
[2] JGraphT. http://jgrapht.org.
[3] R. Aharoni and E. Berger. Menger’s theorem for infinite graphs. Inventiones mathematicae, 2008.
[4] D. Awduche, J. Malcolm, J. Agogbua, M. O’Dell, and J. McManus. Requirements for traffic engineering over MPLS. RFC 2702, RFC Editor, September 1999.
[5] T. Benson, A. Akella, and D. Maltz. Unraveling the complexity of network management. In NSDI, 2009.
[6] T. Benson, A. Akella, and A. Shaikh. Demystifying configuration challenges and trade-offs in network-based ISP services. In SIGCOMM, 2011.
[7] Cisco Systems. BGP best path selection algorithm. http://cisco.com/c/en/us/support/docs/ip/border-gateway-protocol-bgp/13753-25.html.
[8] N. Feamster and H. Balakrishnan. Detecting BGP configuration faults with static analysis. In NSDI, 2005.
[9] A. Fogel, S. Fung, L. Pedrosa, M. Walraed-Sullivan, R. Govindan, R. Mahajan, and T. Millstein. A general approach to network configuration analysis. In NSDI, 2015.
[10] E. W. Fulp. Optimization of network firewall policies using directed acyclic graphs. In IEEE Internet Mgmt Conf, 2005.
[11] A. Gember-Jacobson, R. Viswanathan, A. Akella, and R. Mahajan. Fast control plane analysis using an abstract representation. Technical report, University of Wisconsin-Madison, 2016.
[12] A. Gember-Jacobson, W. Wu, X. Li, A. Akella, and R. Mahajan. Management plane analytics. In IMC, 2015.
[13] S. Hares and D. Katz. Administrative domains and routing domains: A model for routing in the internet. RFC 1136, RFC Editor, Dec 1989.
[14] Juniper Networks. Understanding BGP path selection. http://juniper.net/documentation/en_US/junos12.1/topics/reference/general/routing-ptotocols-address-representation.html.
[15] P. Kazemian, M. Chang, H. Zeng, G. Varghese, N. McKeown, and S. Whyte. Real time network policy checking using header space analysis. In NSDI, 2013.
[16] P. Kazemian, G. Varghese, and N. McKeown. Header space analysis: Static checking for networks. In NSDI, 2012.
[17] A. Khurshid, X. Zou, W. Zhou, M. Caesar, and P. B. Godfrey. VeriFlow: Verifying network-wide invariants in real time. In NSDI, 2013.
[18] P. Lapukhov, A. Premji, and J. Mitchell. Use of BGP for routing in large-scale data centers. Internet-Draft draft-ietf-rtgwg-bgp-routing-large-dc-07, IETF Secretariat, Aug 2015.
[19] F. Le, G. G. Xie, D. Pei, J. Wang, and H. Zhang. Shedding light on the glue logic of the internet routing architecture. In SIGCOMM, 2008.
[20] H. Mai, A. Khurshid, R. Agarwal, M. Caesar, P. B. Godfrey, and S. T. King. Debugging the data plane with Anteater. In SIGCOMM, 2011.
[21] D. A. Maltz, G. Xie, J. Zhan, H. Zhang, G. Hjálmtýsson, and A. Greenberg. Routing design in operational networks: A look from the inside. In SIGCOMM, 2004.
[22] A. Slivkins. Parameterized tractability of edge-disjoint paths on directed acyclic graphs. SIAM J. Discret. Math., 24(1):146–157, Feb. 2010.
[23] Y. Sverdlik. Microsoft: Misconfigured network device led to Azure outage. http://datacenterdynamics.com/servers-storage/microsoft-misconfigured-network-device-led-to-azure-outage/68312.fullarticle, July 2012.
[24] L. Yuan, J. Mai, Z. Su, H. Chen, C.-N. Chuah, and P. Mohapatra. FIREMAN: a toolkit for FIREwall Modeling and ANalysis. In IEEE SP, 2006.
[25] H. Zeng, P. Kazemian, G. Varghese, and N. McKeown. Automatic test packet generation. In CoNEXT, 2012.