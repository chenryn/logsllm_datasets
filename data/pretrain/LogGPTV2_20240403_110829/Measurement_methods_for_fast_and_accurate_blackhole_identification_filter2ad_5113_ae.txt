### 4.3 Wide-Area Experiments

We applied the tomography algorithm to measurements from PlanetLab and an enterprise network to evaluate the effect of confirmation and aggregation on the number of alarms raised in a real network. Our results show that these techniques reduce the number of alarms from thousands to a few.

#### Experiment Setup
- **PlanetLab**: 16,262 cycles of one minute (C = 60 seconds).
- **Enterprise Network**: 228,806 cycles of five seconds each (C = 5 seconds).

We used a conservative value for \( F = 10^{-6} \), resulting in:
- **PlanetLab**: \( \kappa = 7 \) and \( \mu = 420 \) ms.
- **Enterprise Network**: \( \kappa = 4 \) and \( \mu = 280 \) ms.

We configured multi-cycle strategies to identify failures longer than 11 minutes, leading to:
- **PlanetLab**: \( n = 10 \).
- **Enterprise Network**: \( n = 120 \).

#### Results
Table 3 shows the number of alarms with and without confirmation for each aggregation strategy. Since we do not have ground truth for these deployments, we cannot label which alarms are false. However, based on the Emulab results from Section 4.2, most of the alarms we eliminate with our confirmation and aggregation strategies should be false.

**Key Observations:**
- **PlanetLab Environment**: Highly dynamic, with frequent alarms.
  - Applying tomography to raw measurements would result in one alarm per minute.
  - Even with failure confirmation, the number of alarms remains high, as every cycle still has at least one confirmed path failure.
- **Enterprise Network**: More stable, with fewer alarms.
  - Applying tomography to raw measurements would result in one alarm every three minutes.
  - Failure confirmation is more effective in reducing alarms in this environment.

**Aggregation Strategies:**
- **MC-Path**: Flexible and works well in both environments.
  - In PlanetLab, MC-Path with confirmation triggers approximately 12 alarms per day, which is over a hundred times fewer than using basic without confirmation.
  - In the enterprise network, MC-Path reduces the number of alarms to 2 per day, making it more realistic for operational teams to handle.

### 4.4 Comparison to State of the Art

We compared our approach to that of Kompella et al. [16], which is the most closely related work to ours. We present a brief overview of their method, describe the experiment setup, and compare the performance of each method.

#### Overview and Experiment Setup
Kompella et al.'s method involves:
1. **Failure Signature Creation**: A coordinator groups lost probes into a failure signature, which is a type of reachability matrix.
2. **Hypothesis Set Building**: The coordinator iteratively selects the link that explains the largest number of lost probes.
3. **Filtering**: Links are removed from the hypothesis set if the absolute number of failures they could have caused is less than a chosen threshold, \( \tau \).

We used the GEANT topology for controlled experiments in Emulab, injecting failures of 60 seconds. We varied per-link loss rates from zero to 1% and average burst lengths from 4 ms to 40 ms.

#### Identification Rate
Figure 10 shows the identification rate of the absolute method when varying \( \tau \) for different values of \( f / C \). Reducing \( \tau \) allows more links in the hypothesis set, increasing the number of identified failures.

#### False Alarms
Figure 11 shows that decreasing \( \tau \) increases the number of false alarms. Absolute only counts probe losses and does not reset these counters upon a successful probe, leading to many false alarms.

**Comparison:**
- **Identification Rate and False Alarms**: No configuration of the absolute method achieves the same accuracy as MC-Path.
- **Bias and Detection Errors**: The absolute method is biased toward adding highly visited links to the hypothesis set, increasing false alarms and reducing the identification rate if the wrong link is chosen to explain a failure.
- **Delay**: The delay of the absolute method is proportional to its bin length. MC-Path has a lower average delay for \( C > 12 \) (i.e., \( f / C > 5 \)).

### 5. Related Work

#### Network Tomography and Monitor Selection
- **Binary Tomography Algorithms**: Rely on end-to-end path measurements and a coordinator to combine information with the network topology to identify failures [9, 10, 16, 22].
- **Monitor Selection Algorithms**: Reduce the number of paths to probe and consequently the cycle length [2, 19, 30].

#### Fault Identification Systems
- **NICE [18]**: Correlates different data sources to identify intermittent failures, focusing on failures already in the network’s alarm system.
- **PlanetSeer [28]**: Passively monitors TCP traffic to detect problematic paths and triggers traceroutes.
- **Hubble [15]**: Uses RouteViews data and low-rate pings to run traceroutes to destinations likely experiencing reachability problems.

### 6. Conclusion

Binary tomography algorithms hold great promise for detecting and locating network failures, including those difficult to detect with other methods (e.g., network "blackholes"). This paper addresses two significant obstacles: distinguishing persistent blackholes from bursty, congestion-related losses, and the lack of synchronized end-to-end measurements. We designed and evaluated confirmation and aggregation methods, validated them using controlled experiments on the Emulab testbed, and showed that these methods quickly and accurately identify all failures longer than two measurement cycles with few false alarms.

### 7. References

[1] F. Baccelli, S. Machiraju, D. Veitch, and J. Bolot. The Role of PASTA in Network Measurement. SIGCOMM CCR, 36(4):231–242, 2006.
...
[30] Y. Zhao, Z. Zhu, Y. Chen, D. Pei, and J. Wang. Towards Efficient Large-Scale VPN Monitoring and Diagnosis under Operational Constraints. In Proc. IEEE INFOCOM, Rio de Janeiro, Brazil, 2009.

This optimized version provides a clearer and more professional structure, ensuring the content is coherent and easy to follow.