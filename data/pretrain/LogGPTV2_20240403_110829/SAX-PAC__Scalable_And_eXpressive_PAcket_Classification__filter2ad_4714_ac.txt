"branching" formulation, γ > 0, but the rewards of an action
are computed as an aggregation over multiple child states
produced by an action. For example, cutting a node produces
multiple child sub-nodes, and the reward calculation may
involve a sum or a min over each child’s future rewards, de-
pending on whether we are optimizing for tree size or depth.
The 1-step decision problem and branching decision process
formulations of NeuroCuts are roughly equivalent; in the
implementation section we describe how we adapt standard
RL algorithms to run NeuroCuts.
The final challenge is how to scale the solution to large
packet classifiers. The decision tree for a packet classifier
with 100K rules can have hundreds of thousands of nodes.
The size of the tree impedes training along several dimen-
sions. Not only does it take more steps to finish building a
tree, but the execution time of each action increases as there
are more rules to process. The space of trees to explore is
also larger, requiring the use of larger network models and
generating more rollouts to train.
State representation. One key observation is that the ac-
tion on a tree node only depends on the node itself, so it is
not necessary to encode the entire decision tree in the en-
vironment state. Our goal to optimize a global performance
objective over the entire tree suggests that we would need
to make decisions based on the global state. However, this
does not mean that the state representation needs to encode
the entire decision tree. Given a tree node, the action on that
node only needs to make the best decision to optimize the
1Note that just returning -1 or -cutSize for each step would not be a
particularly useful dense reward.
2The rewards for NeuroCuts correspond to the true problem objective; we
do not do "reward engineering" since that would bias the solution.
6
Neural Packet Classification
(a) NeuroCuts starts with a randomly initialized policy that generates poorly shaped trees (left, truncated). Over
time, it learns to reduce the tree depth and develops a more coherent strategy (center). The policy converges to a
compact depth-12 tree (right) that specializes in cutting SrcIP, SrcPort, and DstPort.
(b) In comparison, HiCuts produces
a depth-29 tree for this rule set that
is 15× larger and 3× slower in clas-
sification time.
Figure 5: Visualization of NeuroCuts learning to split the fw5_1k ClassBench rule set. The x-axis denotes the tree
level, and the y-axis the number of nodes at the level. The distribution of cut dimensions per level of the tree is
shown in color.
Figure 6: The NeuroCuts policy is stochastic, which en-
ables it to effectively explore many different tree vari-
ations during training. Here we visualize four random
tree variations drawn from a single policy trained on
the acl4_1k ClassBench rule set.
sub-tree rooted at that node. It does not need to consider
other tree nodes in the decision tree.
Formally, given tree node n, let tn and sn denote n’s clas-
sification time and memory footprint, respectively, and Tn
and Sn be the classification time and memory footprint of
the entire sub-tree rooted at node n, respectively. Then, for
a cut action, we have the following equations:
Tn = tn + maxi∈childr en(n)Ti
Sn = sn + sumi∈childr en(n)Si
Similarly, for a partition action, we have
(1)
(2)
Tn = tn + sumi∈childr en(n)Ti
Sn = sn + sumi∈childr en(n)Si
(3)
(4)
An action, a, taken on node n only needs to optimize the
sub-tree rooted at n according to the following expression,
(5)
where c is a coefficient capturing the tradeoff between classi-
fication time and memory footprint. The negation is needed
since we want to minimize time and space complexities.
When c ∈ {0, 1}, it is easy to see that if at every tree node
n we take the action that optimizes Vn, then, by induction, we
end up optimizing Vr , where r is the root of the tree. In other
Vn = argmax
a∈A − (c · Tn + (1 − c) · Sn),
words, we end up optimizing the global objective (reward)
for the entire decision tree. For 0 < c < 1 this optimization
becomes approximate, but we find empirically that c can
still be used to interpolate between the two objectives. It is
important to note here that while the state representation
only encodes current node n, action a taken for node n is not
local, as it optimizes the entire sub-tree rooted at n.
In summary, we only need to encode the current node as
the input state of the agent. This is because the environment
builds the tree node-by-node, node actions need only con-
sider their own state, and each node contains a subset of the
rules of its parent (i.e., rules contained in some subspace of
its parent space). Therefore, nodes in the tree can be com-
pletely defined by the ranges they occupy in each dimension.
Given d dimensions, we use 2d numbers to encode a tree
node, which indicate the left and right boundaries of each
dimension for this node. The state also needs to describe the
partitioning at the node, which can be handled in a similar
way. We defer a full description of the NeuroCuts state and
action representations to Appendix A.
Training algorithm. We use an actor-critic algorithm to
train the agent’s policy [19]. This class of algorithms have
been shown to provide state-of-the-art results in many use
cases [5, 36, 43], and can be easily scaled to the distributed
setting [7]. We also experimented with Q-learning [38] based
approaches, but found they did not perform as well.
Algorithm 1 shows the pseudocode of the NeuroCuts algo-
rithm, which executes as follows. NeuroCuts starts with the
root node of the decision tree, s∗. The end goal is to learn an
optimized stochastic policy function π(a|s; θ) (i.e., the actor).
NeuroCuts first initializes all the parameters (line 1-6), and
then runs for N rollouts to train the policy and the value
function (line 7-23). After each rollout, it reinitializes the
decision tree to the root node (line 9). It then incrementally
builds the tree by repeatedly selecting and applying an action
7
010000200003000040000ProtocolDstPortSrcPortDstIPSrcIP0501001502000200400600800025005000750010000010000200003000040000on each non-terminal leaf node (line 11-13) according to the
current policy. A terminal leaf node is a node in which the
number of rules is below a given threshold.
More specifically, NeuroCuts traverses the tree nodes in
depth-first-search (DFS) order (line 13), i.e., it recursively
cuts the child of the current node until the node becomes a
terminal leaf. Note that the DFS order is not essential. It is
used to give a way for the agent to find a tree node to cut.
Other orders, such as the breadth-first-search (BFS), can be
used as well. After the decision tree is built, the gradients
are reset (line 14), and then the algorithm iterates over all
the tree nodes to aggregate the gradients (line 15-21). Finally,
NeuroCuts uses the gradients to update the parameters of
the actor and critic networks (line 22), and proceeds to the
next rollout (line 23).
The first gradient computation (line 19) corresponds to
that for the policy gradient loss. This loss defines the direction
to update θ to improve the expected reward. An estimation
of the state value V(s; θv) is subtracted from the rollout re-
ward R to reduce the gradient variance [20]. V is trained
concurrently to minimize its prediction error (line 21). Fig-
ure 5 visualizes the learning process of NeuroCuts to build a
decision tree. The NeuroCuts policy is stochastic, enabling it
to effectively explore many different tree variations during
training, as illustrated in Figure 6.
Incorporating existing heuristics. NeuroCuts can easily
incorporate additional heuristics to improve the decision
trees it learns. One example is adding rule partition actions.
In addition to the cut action, in our NeuroCuts implementa-
tion we also allow two types of partition actions:
(1) Simple: the current node is partitioned along a single
(2) EffiCuts: the current node is partitioned using the
dimension using a learned threshold.
EffiCuts partition heuristic [55].
Algorithm 1 Learning a tree-generation policy using an
actor-critic algorithm.
Input: The root node s∗ where a tree always grows from.
Output: A stochastic policy function π(a|s; θ) that outputs a branching
action a ∈ A given a node state s, and a value function V (s; θv) that
outputs a value estimate for a node state.
Main routine:
1: // Initialization
2: Randomly initialize the model parameters θ, θv
3: Maximum number of rollouts N
4: Coefficient c ∈ [0, 1] that trades off classification time vs. space
5: Reward scaling function f (x) ∈ {x, log(x)}
6: n ← 0
7: // Training
8: while n < N do
s ← Reset(s∗)
9:
10:
// Build a tree using the current policy
while s (cid:44) Null do
11:
a ← π(a|s; θ)
12:
s ← GrowTreeDFS(s, a)
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
// Compute the future rewards for the given action
R ← −(c · f (Time(s)) + (1 − c) · f (Space(s)))
// Accumulate gradients wrt. policy gradient loss
dθ ← dθ + ∇θ log π(a|s; θ)(R − V (s; θv))
// Accumulate gradients wrt. value function loss
dθv ← dθv + ∂(R − V (s; θv))2/∂θv
Perform update of θ using dθ and θv using dθv .
n ← n + 1
Reset gradients dθ ← 0 and dθv ← 0
for (s, a) ∈ TreeIterator(s∗) do
Subroutines:
• Reset(s): Reset the tree s to its initial state.
• GrowTreeDFS(s, a): Apply action a to tree node s, and return the
next non-terminal leaf node in the tree in depth-first traversal order.
• TreeIterator(s): Non-terminal tree nodes of the subtree s and
• Time(s): Upper-bound on classification time to query the subtree s.
• Space(s): Memory consumption of the subtree s.
In non-partitioned trees this is simply the depth of the tree.
their taken action.
Scaling out to handle large packet classifiers. The pseu-
docode in Algorithm 1 is for a single-threaded implementa-
tion of NeuroCuts. This is sufficient for small classifiers. But
for large classifiers with tens or hundreds of thousands of
rules, parallelism can significantly improve the speed of train-
ing. In Figure 7 we show how Algorithm 1 can be adapted to
build multiple decision trees in parallel.
Handling classifier updates. Packet classifiers are often
updated by network operators based on application require-
ments, e.g., adding access control rules for new devices. For
small updates of only a few rules, NeuroCuts modifies the
existing decision tree to reflect the changes. New rules are
added to the decision tree according to the existing struc-
ture; deleted rules are removed from the terminal leaf nodes.
Figure 7: NeuroCuts can be parallelized by generating
decision trees in parallel from the current policy.
When enough small updates accumulate or a large update is
made to the classifier, NeuroCuts re-runs training.
8
Policy EvaluationPolicy EvaluationPolicy EvaluationImprove 𝜃, 𝜃v via stochastic gradient descentConcatenate tree rolloutsBroadcast new values of 𝜃Neural Packet Classification
5 IMPLEMENTATION
Deep RL algorithms are notoriously difficult to reproduce
[15]. For a practical implementation, we prioritize the ability
to (i) leverage off-the-shelf RL algorithms, and (ii) easily
scale NeuroCuts to enable parallel training of policies.
Decision tree implementation. We implement the deci-
sion tree data structure for NeuroCuts in Python for ease of
development. To ensure minor implementation differences
do not bias our results, we use this same data structure to
implement each baseline algorithm (e.g., HiCuts, EffiCuts,
etc.), as well as to implement NeuroCuts.
0
t +1, ..., sk
Branching decision process environment. As discussed
in Section 4, the branching structure of the NeuroCuts en-
vironment poses a challenge due to its mismatch with the
MDP formulation assumed by many RL algorithms. A typical
RL environment defines a transition function Pa(st +1|st) and
a reward function Ra(s, s′). The first difference is that the
state transition function in NeuroCuts returns multiple child
t +1}.
states, instead of a single state., i.e., (st , at) → {s
Second, the final reward for NeuroCuts is computed by aggre-
gating across the rewards of child states. More precisely, for
the cut action we use max aggregation for classification time
and sum aggregation for memory footprint. For the partition
action, we use sum aggregation for both metrics.
The recursive dependence of the NeuroCuts reward calcu-
lation on all descendent state actions means that it is difficult
to flatten the tree structure of the environment into a single
MDP, which is required by existing off-the-shelf RL algo-
rithms. Rather than attempting to flatten the NeuroCuts
environment, our solution is to instead treat the NeuroCuts
environment as a series of independent 1-step decision prob-
lems, each of which yields an “immediate” reward. The actual
reward for these 1-step decisions is calculated once the rele-
vant sub-tree rollout is complete.
For example, consider a NeuroCuts tree rollout from a root
node s1. Based on πθ the agent decides to take action a1 to
split s1 into s2, s3, and s4. Of these child nodes, only s4 needs
to be further split (via a2), into s5 and s6, which finishes the
tree. The experiences collected from this rollout consist of
two independent 1-step rollouts: (s1, a1) and (s4, a2). Taking
the time-space coefficient c = 1 and discount factor γ = 1
for simplicity, the total reward R for each rollout would be
R = 2 and R = 1 respectively.
Multi-agent implementation. Since these 1-step decisions
are logically independent of each other, NeuroCuts execu-
tion can be realized as a multi-agent environment, where
each node’s 1-decision problem is taken by an independent
“agent” in the environment. Since we want to learn a single
policy, πθ , for all states, the agents must be configured to
share the same underlying stochastic neural network policy.
This ensures all experiences go towards optimizing the single
shared policy πθ . When using an actor critic algorithm to
optimize the policies of such agents, the relevant loss calcu-
lations induced by this multi-agent realization are identical
to those presented in Algorithm 1.
There are several ways to implement the 1-step formula-
tion of NeuroCuts while leveraging off-the-shelf RL libraries.