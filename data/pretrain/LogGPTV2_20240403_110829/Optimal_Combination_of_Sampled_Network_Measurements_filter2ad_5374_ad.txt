### 5.3 Estimation Error Analysis

The estimation error \( e_{xy} \) is given by:

\[
e_{xy} = \max \left\{ \frac{m_{xy}}{M_{\text{in}}}, \frac{m_{xy}}{M_{\text{out}}} \right\}
\]

where \( m_{xy}/M_{\text{in}} \) and \( m_{xy}/M_{\text{out}} \) are the fractions of the total traffic that \( m_{xy} \) constitutes on its input and output interfaces, respectively. Heuristically, \( e'_{xy} \) (a modified version of \( e_{xy} \)) de-emphasizes errors in estimating relatively small Matrix Elements (MEs).

#### 5.3.1 Error Visualization

We plot the corresponding ordered values of the errors \( e'_{xy} \) in the right-hand column of Figure 4. The observations are as follows:

- **(i)** Regular i,o,r and adhoc i,o,r methods are uniformly more accurate than other methods, except for low sampling rates and low estimation errors, where they perform similarly to the best of the other methods.
- **(ii)** Adhoc i,o,r shows a more pronounced improvement at larger sampling rates.
- **(iii)** Regular i,o,r and adhoc i,o,r do not exhibit the third or fourth features described above, i.e., no flat portion or errors greater than 1. This indicates that these methods successfully avoid larger estimation errors for relatively large MEs, while other methods have some noticeable fraction of the relatively large MEs poorly estimated.

#### 5.3.2 Comparative Performance

To further understand the relative performance of the methods, we examine the larger estimation errors of the whole traffic matrix. As an example, we show in Figure 5 the unscaled relative errors for \( k = 128 \) samples per interface direction, for average i,o and regular i,o,r. Errors have been truncated at 10 to retain detail for smaller errors. The key observations are:

- **(i)** Average i,o is poor at estimating many MEs through the largest interface (labeled 1) because smaller MEs are poorly sampled at that interface. Regular i,o,r performs better because it primarily uses the estimates gathered at the other interface traversed by these MEs.
- **(ii)** Regular i,o,r has a smaller number of large relative errors compared to average i,o.

#### 5.3.3 Statistical Analysis

To obtain a broader statistical picture, we repeated the experiments reported in Figure 4 100 times, varying the seed for the pseudorandom number generator that governs random selection in each repetition. The ranked root mean square (RMS) of the relative errors broadly shows the same form as Figure 4, but with smoother curves due to averaging over many experiments.

### 6. Experiments: Network Matrix

In this section, we focus on combining a large number of estimates of a given traffic component. Each estimate may individually be of low quality; the challenge is to combine them into a more reliable estimate. This problem is motivated by a scenario where routers or other network elements ubiquitously report traffic measurements, and a traffic component can generate multiple measurements as it transits the network.

#### 6.1 Experimental Setup

We evaluated the combined estimator from independent samples of a traffic stream from multiple points. Since we do not have traces taken from multiple locations, we used multiple independent sample sets of the CAMPUS flow trace, each set representing the measurements that would be taken from a single Observation Point (OP). We took 30 sample sets in all, corresponding to the current maximum typical hop counts in internet paths [16].

The experiments used threshold sampling rather than priority sampling, which would have required the additional complexity of simulating background traffic for each observation point. Apart from packet loss or possible effects of routing changes, the multiple independent samples correspond to those obtained by sampling the same traffic stream at multiple points in the network.

Our evaluations used multiple experiments, each representing sampling of a different set of flows in the network. Flow sizes were taken from successive portions of the CAMPUS trace (wrapping around if necessary), changing the seed of the pseudorandom number generator used for sampling in each experiment. The estimates based on each set of independent samples were combined using the following methods: average, adhoc, bounded, and regular. As a performance metric for each method, we computed the root mean square (RMS) relative estimation error over 100 experiments.

#### 6.2 Homogeneous Sampling Thresholds

As a baseline, we used a uniform sampling threshold at all OPs. In this case, the bounded method reduces to the average method. In 7 separate experiments, we used a sampling threshold of \( 10^i \) Bytes for \( i = 3, \ldots, 9 \). This covers roughly the range of flow sizes in the CAMPUS dataset, including the range of \( z \) values that would likely be configured if flow sizes generally conformed to the statistics of CAMPUS. The corresponding sampling rate (i.e., the average proportion of flows that would be selected) with threshold \( z \) is \( \pi(z) = \frac{1}{N} \sum_{i=1}^{N} \min\{1, x_i/z\} \), where \( \{x_i : i = 1, \ldots, N\} \) are the sizes of the \( N \) flows in the set. For this dataset, \( \pi(z) \) ranged from \( \pi(10^3) = 0.018 \) to \( \pi(10^9) = 1.9 \times 10^{-5} \).

A typical single path of the byte estimate (normalized by the actual value) for a single experiment is shown in Figure 6. This was for 10,000 flows sampled with a threshold of 10MB at 100 sites. There were typically a handful of flows sampled at each OP. The bounded estimate relaxes slowly towards the true value. The regular method also follows at a similar rate but displays some bias. The adhoc method displays systematic bias beyond 30 combinations, highlighting the need for robust estimation methods like those proposed in this paper.

Summary RMS error statistics over multiple experiments are shown in Tables 2 and 3, where we vary the number of flows in the underlying population (1000 or 100,000) for 30 measurement sites. Bounded has somewhat better performance than regular and significantly better performance than adhoc. The differences are generally more pronounced for 30 sites than for 10, indicating that bounded can take the greatest advantage (in accuracy) of the additional information. Based on examination of individual experiments, this appears to be due to lower bias in bounded.

#### 6.3 Heterogeneous Sampling Thresholds

To model heterogeneous sampling rates, we used 30 sampling thresholds in a geometric progression from 100kB to 100MB, corresponding to average sampling rates of from 0.016 to \( 8.9 \times 10^{-5} \). This range of \( z \) values was chosen to encompass what we expect would be a range of likely operational sampling rates, which are quite small to achieve significant reduction in the volume of flow records through sampling.

We arranged the thresholds in increasing order \( 10^5 \)B = \( z_1 < \ldots < z_i < \ldots < z_{30} = 10^8 \)B, and for each \( m \) computed the various combined estimators formed from the \( m \) individual estimators obtained from samples drawn using the \( m \) lowest thresholds \( \{z_i : i = 1, \ldots, m\} \). The performance on traffic streams comprising 10,000 flows is shown in Figure 7. Qualitatively similar results were found with 1,000 and 100,000 flows.

The RMS error of average initially decreases with path length as it combines the estimators of lower variance (higher sampling rate). However, it eventually increases as it mixes in estimators of higher variance (lower sampling rate). RMS errors for bounded and regular are essentially decreasing with path length, with bounded having slightly better accuracy. The minimum RMS errors (over all path lengths) of the three methods are roughly the same. Could average be adapted to select and include only those estimates with low variance? This would require an additional decision of which estimates to include, and the best trade-off between accuracy and path length is not known a priori. On the other hand, bounded and regular can be used with all available data, even with constituent estimates of high variance, without apparent degradation of accuracy.

### 7. Conclusions

This paper combines multiple estimators of traffic volumes formed from independent samples of network traffic. If the variance of each constituent is known, a minimum variance convex combination can be formed. However, spatial and temporal variability of sampling parameters mean that variance is best estimated from the measurements themselves. The convex combination suffers from pathologies if used naively with estimated variances. This paper was devoted to finding remedies to these pathologies.

We propose two regularized estimators that avoid the pathologies of variance estimation. The regularized variance estimator adds a contribution to the estimated variance representing the likely sampling error, thereby ameliorating the pathologies of estimating small variances while allowing more reliable estimates to be balanced in the convex combination estimator. The bounded variance estimator employs an upper bound to the variance, which avoids estimation pathologies when sampling probabilities are very small.

We applied our methods to two networking estimation problems: estimating interface-level traffic matrices in routers and combining estimates from ubiquitous measurements across a network. Experiments with real flow data showed that the methods exhibit: (i) reduction in estimator variance compared with individual measurements; (ii) reduction in bias and estimator variance compared with averaging or ad hoc combination methods; and (iii) application across a wide range of inhomogeneous sampling parameters, without preselecting data for accuracy. Although our experiments focused on sampling flow records, the basic method can be used to combine estimates derived from a variety of sampling techniques, including, for example, combining mixed estimates formed from uniform and non-uniform sampling.