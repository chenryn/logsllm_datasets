in latency is signiﬁcant for the intra-DC trafﬁc, which accounts for
70% of the total VIP trafﬁc. (For the remaining trafﬁc from the In-
ternet, it is a lesser problem due to larger WAN latencies). The high
latency inﬂation and high latency variability result from processing
the packets in software. We also see that the added latency and the
variance get much worse at higher load.
The results also illustrate that an individual SMux instance has
the CPU utilization
low capacity. Beyond 300K packets/sec,
reaches 100% (Figure 1(b)). Thus, for the hardware SKU used
in our DCs, each SMux can handle only up to 300K packets/sec,
which translates to 3.6 Gbps for 1,500-byte packets. At this rate,
supporting 15 Tbps VIP trafﬁc for a mid-sized (40K servers) DC
would require over 4K SMuxes, or 10% of the DC size; which is
unacceptable1.
3. DUET: CORE IDEAS
In the previous section, we saw that while software load bal-
ancers are ﬂexible and scalable, they suffer from low throughput
and high latency. In this paper, we propose a new design called
DUET that offers scalability, high throughput and low latency, at a
small fraction of the software load balancer’s cost.
DUET is based on two novel ideas. First, we leverage idle re-
sources of modern, commodity data center switches to construct
a hardware load balancer. We call this design Hardware Mux
(HMux). HMux offers microsecond latency, and high capacity,
1Newer technologies such as direct-packet IO and RDMA may
help match packet processing capacity of the SMux to that of the
NIC (10 Gbps), but they may not match packet processing capacity
of the switch (600 Gbps+) as we explain in § 3.1.
Figure 2: Storing VIP-DIP mapping on a switch.
without the need for any additional hardware. However, the HMux
design suffers from certain shortcomings. Thus, our second idea
is to combine the HMux with Ananta-like software Mux (SMux).
The combined system is called DUET in which the SMux acts as a
backstop for the HMux.
We now describe the design of HMux. To simplify the descrip-
tion, we will assume that the DC is not virtualized, i.e., one DIP
corresponds to one server. The changes required to support VMs
are described in §5.2.
3.1 HMux
Ananta’s SMux implements two key functions to load balance
trafﬁc: (1) for each VIP, split trafﬁc equally among its DIPs, and
(2) use IP-in-IP encapsulation to route the VIP trafﬁc to the corre-
sponding DIPs. Both of these functions have long been available on
commodity switches, i.e., trafﬁc splitting is supported using ECMP
and IP-in-IP encapsulation is supported using tunneling. However,
major switch vendors have only recently started to provide the APIs
for ﬁne-grained control over ECMP and tunneling functionality.
Our key insight is that by carefully programming the ECMP and
tunneling tables using these new APIs, we can make a commod-
ity switch act as a hardware Mux (HMux), in addition to its nor-
mal functionality. In fact, this can be easily done on most of the
switches used in our DCs today.
Figure 2 shows the HMux design. A packet arriving at a switch
goes through a processing pipeline. We focus on three tables used
in the pipeline. The packet matches one entry in the host for-
warding table which then points to multiple ECMP table entries.
These ECMP table entries correspond to multiple next hops for the
packet2. The actual next hop for the packet is selected by using the
hash of the IP 5-tuple to index into the ECMP table. The tunnel-
ing table enables IP-in-IP encapsulation by storing the information
needed to prepare the outer IP header for a given packet.
To construct HMux, we link the ECMP and tunneling function-
alities. Consider a packet destined for VIP 10.0.0.0 that arrives at
the HMux. There are two DIPs (100.0.0.1 and 100.0.0.2) for this
VIP. The host forwarding table indicates that the ﬁrst two entries in
the ECMP table pertain to this VIP. The ECMP entries indicate that
packets should be encapsulated, and point to appropriate entries in
the tunneling table. The switch encapsulates the packet using IP-in-
IP encapsulation, and the destination address in the outer IP header
is set to the DIP address speciﬁed in the tunneling table entry. The
packet is then forwarded to the appropriate interface.
Thus, at the expense of some entries in the host forwarding,
ECMP and tunneling tables, we can build a load balancer using
2The information is split between ECMP group table and ECMP
table; we omit such details due to lack of space.
Forwarding Table Tunneling Table Index 0 1 2 3 ECMP Table Encap IP 100.0.0.1 100.0.0.2 110.0.0.1 110.0.0.2 Destination IP 10.0.0.0/32 11.0.0.0/32 HMUX Data 10.0.0.0 100.0.0.2 VIP DIP Data 10.0.0.0 VIP 29added and hence the aggregate capacity of HMuxes will also in-
crease proportionally.
3.3 DUET: HMux + SMux
While partitioning helps increase the number of DIPs HMux can
support, that number still remains limited. The HMux design also
lacks the ﬂexibility of SMux, because VIPs are partitioned and
“pinned” to speciﬁc HMuxes. This makes it challenging to achieve
high VIP availability during network failures. Although replicat-
ing VIP across a few switches may help improve failure resilience,
it is still hard to achieve the high availability of Ananta because
Ananta stores the complete VIP-DIP mappings on a large number
of SMuxes.
This motivates us to architect DUET— a new load balancer de-
sign to fuse the ﬂexibility of SMux and the high capacity and low
latency of HMux.
Figure 3: DUET architecture: VIPs are partitioned across dif-
ferent HMuxes — VIP1 and VIP2 are assigned to HMux C2
and A6. Additionally, SMuxes act as backstop for all the VIPs.
Every server (apart from SMuxes) runs host-agent that decap-
sulates the packets and forwards to the DIP. Links marked with
solid lines carry VIP trafﬁc, and links with dotted lines carry
DIP trafﬁc.
commodity switches. In fact, if all the VIP-to-DIP mappings are
stored on every top-of-rack (ToR) switch as well as every access
switch, this HMux design can provide load balancing functionality
to all intra-DC and inter-DC trafﬁc. However, the amount of space
available in the three tables is limited, raising two distinct issues.
Number of VIPs: The ﬁrst problem is the size of the host for-
warding table. The switches in our DC have 16K entries in the host
table. The host table is mostly empty, because it is used only for
routing within a rack. But even the 16K entries may not be enough
to hold all VIPs in a large DC. One way to address this problem is
by using longest preﬁx match (LPM) forwarding table. However,
LPM table is heavily used for routing within and across DCs, and
is not available to be used for load balancing. We support higher
number of VIPs using SMuxes as explained in §3.3.
Number of DIPs: The second problem concerns the sizes of the
ECMP and tunneling tables. ECMP table typically holds 4K en-
tries, and is mostly empty (see § 9). The tunneling table typically
holds 512 entries. In our DC, few applications use tunneling, so
these entries are mostly free as well. The number of DIPs an in-
dividual HMux can support is the minimum of the number of free
entries in the ECMP and the tunneling tables (see Figure 2). Thus,
an individual HMux can support at most 512 DIPs. This is orders
of magnitude smaller than the total number of DIPs. We address
this challenge next.
3.2 Partitioning
We address the problem of limited size of ECMP and tunneling
tables using two mechanisms: (1) We divide the VIP-to-DIP map-
ping across multiple switches. Every switch stores only a small
subset of all the VIPs, but stores all the DIPs for those VIPs. This
way of partitioning ensures all the trafﬁc for a particular VIP ar-
rives at a single switch and the trafﬁc is then equally split among
the DIPs for that VIP. (2) Using BGP, we announce the VIPs that
are assigned to the switches, so that other switches can route the
VIP packets to the switch where the VIP is assigned.
Figure 3 illustrates this approach. VIP1 has two DIPs (D1 and
D2), whereas VIP2 has one (D3). We assign VIP1 and VIP2 to
switches C2 and A6 respectively, and ﬂood the routing information
in the network. Thus, when a source S1 sends a packet to VIP1,
it is routed to switch C2, which then encapsulates the packet with
either D1 or D2, and forwards the packet.
Another key beneﬁt of partitioning is that it achieves organic
scalability of HMuxes — when more servers are added in the DC
and hence trafﬁc demand increases, more switches will also be
3.3.1 Design
DUET’s goal is to maximize VIP trafﬁc handled using HMux,
while using SMux as a backstop. Thus, besides an HMux on each
switch, DUET also deploys a small number of SMuxes on commod-
ity servers (ﬁgure 3). The VIPs are partitioned among HMuxes as
described earlier. In addition, each SMux announces all the VIPs.
The routing protocol preferentially routes VIP trafﬁc to HMux, en-
suring that VIP trafﬁc is primarily handled by HMux – thereby
providing high capacity and low latency.
In case of HMux fail-
ure, trafﬁc is automatically diverted to SMux, thereby achieving
high availability. To ensure that existing connections do not break
as a VIP migrates from HMux to SMux or between HMuxes, all
HMuxes and SMuxes use the same hash function to select DIPs for
a given VIP.
The preferential routing to HMux can be achieved in several
ways. In our current implementation, SMux announces the VIPs
in aggregate preﬁxes, while HMux announces /32 routes to indi-
vidual VIPs. Longest preﬁx matching (LPM) prefers /32 routes
over aggregate preﬁx routes, and thus directs incoming VIP trafﬁc
to appropriate HMux, unless that HMux is unavailable.
The number of SMuxes needed depends on several factors in-
cluding the VIP trafﬁc that cannot be assigned to HMux due to
switch memory or link bandwidth limits (§4), the VIP trafﬁc that
failovers to SMux due to HMux failure (§5.1), and the VIP trafﬁc
that is temporarily assigned to SMux during VIP migration (§4.2).
We estimate it based on historical trafﬁc and failure data in DC.
3.3.2 Beneﬁts
The key beneﬁts of DUET are summarized below.
Low cost: DUET does not require any additional hardware – it
uses idle resources on existing switches to provide load balancing
functionality. DUET also requires far fewer SMuxes than Ananta,
since SMuxes are used only as a backstop for HMuxes, and hence
carry far less trafﬁc.
High capacity and low latency: this is because VIP trafﬁc is
primarily handled by HMux on switch.
High availability: by using SMux as a backstop during failures,
DUET enjoys the same high availability as Ananta.
High limit on number of VIPs: If the number of VIPs exceeds
the capacity of the host forwarding table (16K), the additional VIPs
can be hosted on SMux. Trafﬁc data (Figure 15) in our production
DCs shows that VIP trafﬁc distribution is highly skewed – most of
the trafﬁc is destined for a small number of “elephant” VIPs which
can be handled by HMux. The remaining trafﬁc to “mice” VIPs
can be handled by SMux.
C1 C2 C3 A1 A2 C4 A3 A4 A5 A6 Core Agg ToR T1 T2 T3 T4 T5 T6 Servers HMux: VIP1 assigned S1 D1 S2 D2 D3 SMux Host Agent DIP HMux: VIP2 assigned SMux: All VIPs assigned 30Notation
V
dv
S, E
R
Ci
ti,s,v
Li,s,v
Ui,s,v
Ui,v
M RUs,v
Explanation
Set of VIPs
Set of DIPs for the v-th VIP
Set of switches and links respectively
Set of resources (switches and links)
Capacity of i-th resource
v-th VIP’s trafﬁc on i-th link, when it is
assigned to s-th switch
load (additional utilization) on i-th resource
if v-th VIP is assigned to s-th switch
Cumulative utilization of i-th resource
if v-th VIP is assigned to s-th switch
Cumulative utilization of i-th resource
after v VIPs have been assigned
Max. Resource Utilization (MRU)
after v-th VIP is assigned to s-th switch
Table 1: Notations used in VIP assignment algorithm.
These beneﬁts can only be realized through careful VIP-switch
assignment. The assignment must take into account both memory
and bandwidth constraints on individual switches, as well as differ-
ent trafﬁc load of different VIPs. The assignment must dynamically
adapt to changes in trafﬁc patterns and network failures. In the next
two sections, we describe how DUET solves these problems, as well
as provides other load balancing functions.
4. VIP ASSIGNMENT ALGORITHM
We formalize the VIP-switch assignment problem using the no-
tations listed in Table 1.
Input: The input to the algorithm includes the list of VIPs (V ),
the DIPs for each individual VIP v (dv), and the trafﬁc volume for
each VIP. The latter is obtained from network monitoring. The
input also includes the network topology, consisting of a set of
switches (S) and a set of links (E). The switches and links con-
stitute the two types of resources (R) in the assignment. Each re-
source instance has a ﬁxed capacity Ci, i.e., the link bandwidth for
a link, and memory capacity that includes residual ECMP and tun-
neling table capacity available for DUET on a switch. To absorb the
potential transient congestion during VIP migration and network
failures, we set the capacity of a link to be 80% of its bandwidth.
Objective: Find the VIP-switch assignment that maximizes the
VIP trafﬁc handled by HMux. As explained earlier, this will im-
prove latency and reduce cost by cutting the number of SMux
needed. We do not attempt to minimize the extra network propa-
gation delay due to indirection because the propagation delay con-
tributes only less than 30µsec of the 381µsec RTT in our DC.
Constraints: Any VIP-switch assignment should not exceed the
capacity of any of the resources.