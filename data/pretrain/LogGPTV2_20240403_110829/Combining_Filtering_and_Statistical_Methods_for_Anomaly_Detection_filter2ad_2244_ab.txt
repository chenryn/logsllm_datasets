into the model. For trafﬁc matrix estimation, using a non-
diagonal matrix for Ct is preferable so that one can bene-
ﬁt from incorporating spatial correlation (as used in [20]).
When trafﬁc matrix estimation is carried out, the main task
is that of taking a total byte count for each link and parti-
tioning it among the the multiple OD ﬂows traversing that
link. When an anomaly occurs on a link, it is possible for
an anomaly (originating within one OD ﬂow) to get spread
across all the OD ﬂows on that link during the estimation
procedure. To avoid this phenomenon, that would make it
more difﬁcult to detect anomalies in OD ﬂows, we use a
diagonal structure for Ct (unlike the model used in [18]).
Putting the above elements together, our complete model
is that of a linear state space dynamical system, that relates
the observables (Yt) to the unobservables (Xt), and is given
by,
(Xt+1 = CtXt + Wt
(1)
Yt
= AtXt + Vt
3
USENIX Association
Internet Measurement Conference 2005  
333
We
assume both the
and the
measurement-noise Vt
to be uncorrelated, zero-mean
Gaussian white-noise processes and with covariance
matrices Qt and Rt:
state-noise Wt
E[WkW T
E[VkV T
l
l ] =(cid:26) Qk, if k = l
] =(cid:26) Rk, if k = l
0, otherwise
0, otherwise
] = 0 ∀k, l
E[WkV T
l
(2)
These assumptions might appear restrictive however a
large body of research in the control theory literature has
been devoted to Kalman ﬁltering robustness. The lessons
learned from this literature are that because of the feedback
mechanism, and ongoing readjustment of estimated values,
Kalman Filters are robust to model imprecision as well as
to some deviation from gaussianity in the noise. The rule
of thumb for reaching a certain level of robustness is to use
noise with slightly larger variance for Wt than obtained by
direct evaluation of noise.
Given the above assumptions and a set of observations
{Y1, ..., Yt+1}, the task is to determine the estimation ﬁlter
that at the (t + 1)-st instance in time generates an optimal
estimate of the state Xt+1, which we denote by ˆXt+1. Op-
timality is deﬁned in the sense of Minimum Variance Error
Estimator that is deﬁned as follows:
E[||Xt+1− ˆXt+1||2] = E[(Xt+1− ˆXt+1)T (Xt+1− ˆXt+1)]
(3)
The classical tool for dealing with this type of problem is
the well known Kalman Filter [10]. It addresses the general
problem of trying to estimate a discrete state vector when
the observations are only a linear combination of this un-
derlying state vector. The Kalman ﬁlter estimates the sys-
tem state process by using a two step approach, that iterates
for each time t. We use ˆXt|i we refer to the estimation of
Xt based on time i, t ≥ i. (We introduce here the more
general case of time-varying systems, where all the param-
eters are indexed by time.)
• Prediction Step: Let ˆXt|t denote the estimate of the
state at time t given all the observations up to time t (i.e.
Y t). This term has a variance that is denoted by Pt|t. Let
ˆXt+1|t denote the one step predictor. This prediction is
made using all the observed data up to time t. Since the
model Xt+1 = CtXt + Wt includes the noise term Wt
(with covariance Qt), this prediction will have some asso-
ciated variability, that is denote as Pt+1|t. In the prediction
step, we are given ˆXt|t and Pt|t, and compute both our pre-
diction, and the variance of this prediction, as follows.
( ˆXt+1|t = Ct ˆXt|t
Pt+1|t = CtPt|tC T
t + Qt
(4)
• Estimation Step: In this step, the kalman ﬁlter updates
the state estimate Xt+1|t+1, and its variance (Pt+1|t+1) by
using a combination of their predicted values and the new
observation Yt+1. The new estimate at time t + 1 is given
by,
ˆXt+1|t+1 = ˆXt+1|t + Kt+1[Yt+1 − At+1 ˆXt+1|t]
Pt+1|t+1 = (I − Kt+1At+1)Pt+1|t(I − Kt+1At+1)T
+Kt+1Rt+1K T
t+1
(5)
The new estimate at time t + 1 for ˆXt+1|t+1 is com-
puted using the prediction from the previous time instant
ˆXt+1|t that is adjusted by a correction factor. Consider
the latter part of this equation. By multiplying our pre-
diction ˆXt+1|t by At, we generate a prediction for the
link counts ˆYt+1. Hence the term in brackets [Yt+1 −
At+1 ˆXt+1|t] = Yt+1 − ˆYt+1 is the error in our predic-
tion of the link counts. This term is multiplied by the ma-
trix Kt+1 that is called Kalman gain matrix. It is obtained
by minimizing the conditional mean-squared estimation er-
˜Xt+1|t+1|Y t] where the estimation error is
ror E[ ˜X T
given by ˜Xt|t = ˆXt|t − Xt. By applying some basic linear
algebra, we can write it as:
t+1|t+1
(6)
t+1 + Rt+1]−1
t+1[AtPt+1|tAT
Kt+1 = Pt+1|t AT
Hence this second step takes the new observation of
Y when it becomes available, and corrects its previous
prediction. The above equations together with the initial
conditions of the state of the system ˆX0|0 = E[X0] and
the associated error covariance matrix P0|0 = E[( ˆX0|0 −
X0)( ˆX0|0 − X0)T ] deﬁne the discrete-time sequential re-
cursive algorithm, for determining the linear minimum
variance estimate, known as Kalman Filter.
In our previous paper [18], the trafﬁc matrix is popu-
lated (i.e. estimated) using ˆXk+1|k+1. Nevertheless, it is
clear that the Kalman ﬁlter gives more information than
only estimates. Using the predictive ability of the ﬁlter it is
possible to estimate the future evolution of the trafﬁc ma-
trix. The correction step in Equation (5) essentially cap-
tures the part of the process that our model could not pre-
dict. It is this unpredictable part that we want to track for
anomaly detection. Based on the study in [18], we know
that the Kalman ﬁlter method for estimating the trafﬁc ma-
trix works well. Hence most of the time, the correction
factors are negligible. Now if at some time instant we see
a large correction of our prediction , we could ﬂag this as
anomalous and generate an alert.
We are thus motivated to examine the errors that our one-
step predictor generates. The errors in our prediction of the
link values are denoted by,
t+1 = Yt+1 − At+1 ˆXt+1|t,
334
Internet Measurement Conference 2005 
USENIX Association
In Kalman ﬁltering terminology this error is typically
the innovation process.
is the difference between
the observed (measured) value Yt+1 and its prediction
At+1 ˆXt+1|t. The innovation process t is considered to be
white gaussian noise with a covariance matrix given by :
It
E(cid:2)t+1T
t+1(cid:3) = At+1Pt+1|tAT
t+1 + Rt+1.
(7)
Since in our case, we are interested in anomalies in the
OD ﬂows, we can deﬁne, by extension, the residual ηt+1,
ηt+1 = ˆXt+1|t+1 − ˆXt+1|t = Kt+1t+1,
t+1
that is the difference between the new estimate of the state
( ˆXt+1|t+1), corrected using the most recent measurement
at time (t + 1), and its prediction ˆXt+1|t made based only
on information available up to time t. It is also a measure of
the new information provided by adding another measure-
ment in the estimation process. Using Equation (5), we can
see that the error in the OD ﬂow estimate is related to the
error in the link estimate via ηt+1 = Kt+1t+1.
This is also a zero-mean gaussian process, whose vari-
ance St+1 can be easily derived as
St+1 = E[ηt+1 ηT
t+1] = Kt+1(At+1Pt+1|tAT
t+1+Rt+1)K T
(8)
The residual process can be shown to be asymptotically
l (cid:3) = 0, t 6= l. This can be un-
uncorrelated, i.e. E(cid:2)ηt ηT
derstood by observing that asymptotically the gain matrix
of Kalman ﬁlter Kt+1 converge to a ﬁxed point ¯K. The
residual is an important measure of how well an estima-
tor is performing. A non-zero residual could mean that an
anomaly has occurred, and in the next section 3, we present
a few schemes for further examining this residual time se-
ries to detect anomalies.
In this section, we presented the Kalman ﬁltering method
in its general settings under non-stationary assumptions. In
the following sections, we will assume a stationary situa-
tion where the matrices A, C, Q and R are constant in time,
making it possible to drop their subscripts. However, the
rest of the methodology presented in this paper can easily
be generalized to incorporate time dependency.
There is an issue of calibration for using such a Kalman
ﬁlter model because the matrices C, Q and R need to be
calibrated. We developed and presented in [20] an Expec-
tation Maximization based approach for calibrating these
matrices. In [20] we showed that for reliable OD ﬂow es-
timation we need to recalibrate the Kalman ﬁlter every few
days when the underlying model changes. When there are
anomalies, this might suggest that the model should be re-
calibrated every time an anomaly occurs. However, one
interesting result of this current paper is that this recali-
bration step is often not needed if the goal is just anomaly
detection. For example, in applying our anomaly detection
schemes on the Abilene data, we found that no recalibra-
tion was needed for 7 days (covering 74 anomalies). Hence
the requirements for recalibration appear to be stronger for
trafﬁc matrix estimation than for anomaly detection.
3 Analyzing Residuals
Before explaining our four methods for examining residu-
als to look for anomalies, we discuss some important issues
regarding sources of errors, understanding the meaning of
decision thresholds, and how they are selected. In doing
so, we explain our methodology for comparing different
anomaly detection schemes.
There are two sources of errors that can appear in the
residual process. One is from errors in the underlying
trafﬁc model, while the second will come from anomalies
in the trafﬁc. Suppose, for a moment, that we consider
any general random process Zt that we want to check for
anomalies. Let ˆZt denote our prediction for this process
based upon a model. Since our model may not be exact,
we let ζt denote the expected prediction error, a zero-mean
random variable with known covariance. If we deﬁne ξt as
the anomaly term at time t, we can write :
Zt = ˆZt + ζt + ξt.
In this equation ξt is a random variable accounting for the
unexpected change caused by the anomalies, i.e. ξt = 0
6= 0 when there is an
if there are no anomalies and ξt
anomaly.
There is an important decision to be made as to which
data granularity to examine in order to try to observe
anomalies. We can consider either looking at the predic-
tion errors observed on the link data Yt or the estimation
errors on the OD ﬂows Xt. Our experience showed us that
detection schemes work better when operating at the gran-
ularity level of the OD ﬂow rather than at that of the link.
Although we cannot observe the OD ﬂow directly, we can
observe the error in our prediction of the OD ﬂow and that
turns out to be plenty sufﬁcient for our purposes. We point
out that the four schemes we discuss for examining errors
can be applied to either type of error. These methodologies
require only that we understand the covariance process of
the associated ζt process.
To detect anomalies on the SNMP link counts, one
should use the statistics of the innovation process in place
of the statistics of ζ. This is readily available in our model
since it is equivalent to the statistics of the innovation pro-
cess in the Kalman Filter. The innovation obtained as the
output of the Kalman ﬁlter is exactly the prediction error
ζt + ξt.
Anomaly detection on OD ﬂows is more tricky as the
prediction error is not directly observable (as OD ﬂows are
hidden). However, the good news is that the covariance of
ζ is known and equal to Pt+1|t+1. Moreover, the residual
ηt+1 = ˆXt+1|t+1 − ˆXt+1|t = Kt+1t+1 can be observed
USENIX Association
Internet Measurement Conference 2005  
335
5
and its covariance can be derived as St+1. And last but not
least the estimation error ζ and the residual η are correlated
gaussian processes, i.e one might use one for estimating the
other and the least squared error estimator is :
ζt + ξt ≈ −KtAtPt|t−1S
−1
t ηt
(9)
The approximation comes from the fact that this is just an
estimation of an unobserved value (the OD ﬂows estima-
tion error) based on an observed value (residual).
3.1 Anomaly detection as a statistical test
We now wish to illustrate how any anomaly detection
scheme can be viewed as a statistical hypothesis test. To
do this, we ﬁrst explain how such tests are evaluated. The
tested are evaluated by exploring the fundamental tradeoff
between the false positive and false negative rates. Hypoth-
esis testing explains how to pick decision thresholds when
faced with balancing this particular tradeoff.
All four of the schemes we use to evaluate the residuals
rely on the selection of a threshold that is used to decide
whether or not an alarm is raised. In fact, any anomaly or
change detection method will require that a threshold be
selected. In our evaluation of these methods we consider
all possible thresholds for each method. We do this by as-
sessing the performance of our method using Receiver Op-
eration Characteristic (ROC) curves.
ROC curves have been developed in the context of sig-
nal detection [5], and have been widely used for medical
analysis purposes [24]. ROC curves are useful because the
describe the full tradeoff between false positives and false
negatives over the complete spectrum of operating condi-
tions (i.e., decision threshold settings). In an ROC curve,
we plot the false positive rate on the x-axis and one minus
the false negative rate on the y-axis. The y-axis thus repre-
sents the true positives (the anomalies we want to catch).