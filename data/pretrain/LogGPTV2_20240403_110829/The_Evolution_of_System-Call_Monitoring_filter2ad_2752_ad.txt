suitable. For example, distributed applications may be difﬁ-
cult to monitor comprehensively using sequences of system
calls alone. Stillerman et al. showed that in distributed ap-
plications a good observable is the messages that are passed
across the network [68]. To demonstrate this, they im-
plemented an anomaly detection system for a distributed
CORBA application, which consists of many distributed ob-
jects that communicate via messages passed over the net-
work. The sequence of messages is a good proxy for object
behavior, and can be used to differentiate between normal
application behavior and rogue clients.
Further, there may be other sources of information avail-
able in other domains that are not available when moni-
toring running server programs. One example is dynamic
execution environments, such as Java, where the inter-
preter can easily collect a wealth of information about pro-
gram execution during runtime. Inoue and Forrest showed
that Java method invocations are effective observables for
anomaly detection [27]. Their approach went beyond at-
tack detection—they used Java’s sandboxing security mech-
anism to implement dynamic sandboxing, where minimal
security policies are inferred using run-time monitoring.
7 Automated Response
Although there has been extensive research into methods
for intrusion detection using system calls, there has been
much less work on how to respond to detected anomalies.
Most anomaly detection systems produce more alerts than
can be handled easily by users or administrators. The prob-
lem is compounded when many copies of a system are de-
ployed in a single organization. What is needed, then, are
automated responses to detected anomalies. One impedi-
ment, however, is the problem of false positives. A binary
response, such as shutting down a machine or unauthenti-
cating a user, is unacceptable if there is even a small proba-
bility that the response was made in error. If the principle of
graduated response is adopted, however, small adjustments
can be made continually, and there is less risk of damaging
the system needlessly or enabling a denial of service.
The ﬁrst effort to couple an automated response to sys-
tem call anomalies was a Linux kernel extension called pH
[64, 65]. pH detects anomalies using lookahead pairs. In-
stead of killing or outright blocking the behavior of anoma-
lously behaving processes,
it delays anomalous system
calls:
Isolated anomalies are delayed imperceptibly, and
clustered anomalies are delayed exponentially longer. Real
attacks tend to generate large delays (on the order of hours
or days)]. Because most network connections have built-
in time outs, this response automatically blocks many at-
tacks; further, it gives administrators time to intervene man-
ually. Many false alarms produce a small number of isolated
anomalies, and pH responds with a proportionally small de-
lay that is usually imperceptible to the user. In the rare case
of a false positive causing a long delay, a simple override
was provided for the user or administrator.
Although pH was the ﬁrst system to use delays as a re-
sponse to detected anomalies, delay-based strategies have
been used in other defenses, especially in networking and
for remote login interfaces (e.g., to prevent online dictio-
425425
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:00:19 UTC from IEEE Xplore.  Restrictions apply. 
nary attacks). For example, Williamson observed that un-
usually frequent outgoing network requests could signal
an anomaly, and that the damage caused by such behav-
ior could be mitigated simply by reducing the rate at which
new network connections could be initiated [80]. This tech-
nology became part of HP’s ProCurve Network Immunity
Manager [26], and it was extended to include incoming con-
nections and more types of network connections in [4]. This
idea of slowing down a computation or communication is
often referred to as throttling or rate limiting. It has been
studied extensively in the networking community, for ex-
ample in active networks [23], Domain Name Service [81],
Border Gateway Protocol [33, 32], and peer-to-peer net-
works [2].
A commercial implementation of system-call anomaly
detection, however, implemented another response strategy.
Sana Security’s Primary Response used a layered approach.
The ﬁrst layer was a mechanism to explicitly prevent code-
injection, in all forms, covering a large class of common
attacks and preventing subversion through mimicry attacks.
The second layer blocked anomalous system calls that ma-
nipulated the ﬁle system. This can prevent many non-code-
injection attacks and most applications are more robust to
failures of ﬁle system calls than other system calls. Further,
Primary Response also proﬁled parameters to ﬁle-related
system calls, and hence could use that additional infor-
mation to further reduce false positives and prevent non-
control-ﬂow attacks.
8 Summary and Conclusions
Over the past decade, we have witnessed continual evo-
lution of new platforms and new forms of attack, including
the advent of email viruses, spyware, botnets, and mutat-
ing malware, just to name a few. Research on system-call
monitoring matured over this time period as well, and many
variations of the original method have been explored. In the
previous sections we highlighted representative examples
of this work, examples that we feel illustrate the breadth
and depth of the method. Despite dramatic changes in to-
day’s computing environments and applications, system-
call monitoring remains a fundamental technique underly-
ing many current projects, e.g., [77, 52]. This is remarkable,
although it is optimistic to expect system-call monitoring
per se to remain an active and exciting research frontier in-
deﬁnitely. Both the threats and the defenses against them
will continue to evolve, likely migrating to higher applica-
tion layers and to lower levels, such as on-chip attacks in
multi-core architectures.
The design principles articulated in Section 2, those prin-
ciples inspired by living systems, are potentially of more
lasting signiﬁcance. These include: generic mechanisms,
adaptability, autonomy, graduated response, and diversity.
Some of these principles have been adopted widely (diver-
sity), some remain controversial (graduated response and
adaptability), and some have been largely ignored (generic
mechanisms and autonomy). Taken together, these princi-
ples constitute a hypothesis about what properties are re-
quired to protect computers and their users.
The design principles guided nearly all of our implemen-
tation decisions. This is one example of how the study of
computer security can be more scientiﬁc than an ad hoc
collection of speciﬁc instances. By articulating a hypoth-
esis, and then designing the simplest possible experiment
to test that hypothesis, i.e.
that anomaly detection could
protect privileged processes against important classes of at-
tack, we were able to demonstrate that short sequences of
system calls are a good discriminator between normal and
malicious behavior. Rather than focusing only on produc-
ing an artifact that worked, we set out to understand what
approaches could be used for effective defenses.
Indeed,
in the beginning we tested system call sequences only as a
base case, expecting that enhancements would be required
for the method to work. Instead of studying the enhance-
ments, however, we found ourselves analyzing why the ex-
periment succeeded.
A key component of our approach was designing repeat-
able experiments. This allowed others to conﬁrm our results
and test variations against the original system. Although
this point seems obvious in retrospect, it was unique at the
time. Our experiments were repeatable because our system
design was comprehensible, and we published both our data
sets and the software prototypes in the public domain. This
enabled other groups to replicate our results, use the data
sets for their own experiments, use our code to devise at-
tacks against the method, and so forth.
Repeatable experiments are crucial to putting computer
security on sounder footing. However, they are not sufﬁ-
cient. Careful comparisons between competing methods are
also important, and this has been much more difﬁcult for the
ﬁeld to achieve. Although public data sets and prototypes
help this effort, it is still extraordinarily difﬁcult to conduct
comparisons carefully. There are several reasons for this:
(1) Environments are complex; (2) Results depend heavily
on data inputs; and (3) Metrics emphasize breadth of cover-
age. The complexity of modern computing environments
poses a serious challenge to replicating results and com-
paring results from multiple experiments. Is is surprisingly
difﬁcult to document precisely the conditions under which
an experiment is run, and seemingly trivial differences in
system conﬁguration can affect the outcome of an experi-
ment dramatically. This problem is even more challenging
in networking experiments than it is for single hosts. Fur-
ther, we discovered that for system calls the outcome of our
comparisons depended heavily on which program traces we
selected for the comparison [76]. It was easy to skew the
426426
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:00:19 UTC from IEEE Xplore.  Restrictions apply. 
results of our comparative studies by appropriate (or inap-
propriate) choice of data sets.
Most systems are judged by their ability to defend
against as many attacks as possible. This leads to system
designs that are optimized for broad coverage and corner
cases, which typically lead to complex algorithms and im-
plementations. As Sections 5 and 6 show, there is a nearly
limitless variety of ways that the original idea of short se-
quences of system calls can be made more complex. To our
knowledge, not a single paper has been published that pro-
poses a simpler approach to monitoring system calls. Tun-
ing up other people’s methods so they behave optimally re-
quires considerable skill, effort, and discipline; in contrast,
a system that highlights one key idea can easily be evaluated
because there are fewer “knobs” to adjust—evaluations in
different contexts are thus inherently easier to compare.
What began as a simple insight inspired by biology has
grown into a robust and diverse ﬁeld of research. We are
excited at the progress and directions that this research has
taken. Attacks on the ideas have led to creative new meth-
ods that make the original approach much more robust to
subversion, and various other improvements and develop-
ments have resulted in a far better protection system than
we could have hoped to see over a decade ago. This has
validated some of the principles elucidated in section 2, but
we see much scope for extending the research to investigate
those principles in greater depth. We hope that this paper
illustrates how inspirational the biological analogy can be,
and encourages others to explore those principles so that
this continues to be a vibrant ever-growing area of research.
9 Acknowledgments
The authors gratefully acknowledge the many people
who encouraged and assisted us during the development
of the original system call project. In particular, we thank
Dave Ackley, Tom Longstaff, and Eugene Spafford. Jed
Crandall, Dave Evans, ThanhVu Nguyen, and Eugene Spaf-
ford made many helpful suggestions on this manuscript.
The original project was partially funded by the Na-
tional Science Foundation (NSF) IRI-9157644, Ofﬁce of
Naval Research N00014-95-1-0364, and the Defense Ad-
vanced Research Projects Agency N00014-96-1-0680. SF
acknowledges NSF (CCF 0621900, CCR-0331580), Air
Force Ofﬁce of Scientiﬁc Research MURI grant FA9550-
07-1-0532, and the Santa Fe Institute. AS acknowledges
NSERC’s Discovery program and MITACS.
References
[1] M. Abadi, M. Budiu, U. Erlingsson, and J. Ligatti. Control-
ﬂow integrity: Principles, implementations and applications.
In Proceedings of ACM Computer and Communications Se-
curity, November 2005.
[2] M. K. Aguilera, M. Lillibridge, and X. Li. Transaction rate
limiters for peer-to-peer systems. IEEE International Con-
ference on Peer-to-Peer Computing, 0:3–11, 2008.
[3] D. Anderson, T. Frivold, and A. Valdes. Next-generation in-
trusion detection expert system (nides): A summary. Techni-
cal Report SRI-CSL-95-07, Computer Science Laboratory,
SRI International, May 1995.
[4] J. Balthrop. Riot: A responsive system for mitigating com-
puter network epidemics and attacks. Master’s thesis, The
University of New Mexico, Albuquerque, NM, 2005.
[5] S. Basu and P. Uppuluri. Proxi-Annotated Control Flow
Graphs: Deterministic Context-Sensitive Monitoring for In-
trusion Detection, pages 353–362. Springer, 2004.
[6] S. Bhatkar, A. Chaturvedi, and R. Sekar. Dataﬂow anomaly
detection. In In Proc. IEEE Symposium on Security and Pri-
vacy, pages 48–62, 2006.
[7] S. Chen, J. Xu, and E. C. Sezer. Non-control-data attacks are
realistic threats. In 14th Annual Usenix Security Symposium,
Aug 2005.
[8] R. Danyliw and A. Householder.
Cert advisory
ca-2001-19: Code red worm exploiting buffer over-
ﬂow in iis
Website, 2001.
http://www.cert.org/advisories/CA-2001-19.html.
indexing service dll.
[9] D. E. Denning. An intrusion-detection model. IEEE Trans-
actions on Software Engineering, 13:222–232, 1987.
[10] D. Endler. Intrusion detection: applying machine learning to
solaris audit data. In In Proc. of the IEEE Annual Computer
Security Applications Conference, pages 268–279. Society
Press, 1998.
[11] E. Eskin, W. Lee, and S. J. Stolfo. Modeling system calls for
intrusion detection with dynamic window sizes,. In Proceed-
ings of DARPA Information Survivability Conference and
Exposition II (DISCEX II), Anaheim, CA, 2001.
[12] H. Feng, O. Kolesnikov, P. Fogla, W. Lee, and W. Gong.
Anomaly detection using call stack information. In Proceed-
ings of the 2003 IEEE Symposium on Security and Privacy,
May 2003.
[13] G. Fink and K. Levitt. Property-based testing of privileged
programs. In Proceedings of the 10th Annual Computer Se-
curity Applications Conference, page 154163, Dec. 1994.
[14] S. Forrest, S. A. Hofmeyr, A. Somayaji, and T. A. Longstaff.
A sense of self for Unix processes. In SP ’96: Proceedings
of the 1996 IEEE Symposium on Security and Privacy, page
120, Washington, DC, USA, 1996. IEEE Computer Society.
[15] S. Forrest, A. S. Perelson, L. Allen, and R. Cherukuri. Self-
nonself discrimination in a computer. In Proceedings of the
1994 IEEE Symposium on Research in Security and Privacy,
Los Alamitos, CA, 1994. IEEE Computer Society Press.
[16] D. Gao, M. Reiter, and D. Song. Gray-box extraction of ex-
ecution graphs for anomaly detection. In Proceedings of the
11th ACM Conference on Computer and Communications
Security, pages 318–329, October 2004.
[17] D. Gao, M. K. Reiter, and D. Song. Behavioral distance
measurement using hidden markov models. In D. Zamboni