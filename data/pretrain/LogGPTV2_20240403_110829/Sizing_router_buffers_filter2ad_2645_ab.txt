 10
 20
 30
 40
 50
 60
 70
 80
 90
 Queue [Pkts]
 0
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
Figure 5: A TCP ﬂow through an underbuﬀered
router.
 350
 300
 250
 200
 150
 100
 50
 0
 150
 100
 50
 0
Window [Pkts]
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
Queue [Pkts]
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
Figure 6: A TCP ﬂow through an overbuﬀered
router.
turn means that ACKs arrive to the sender at rate C. The
sender therefore pauses for exactly (Wmax/2)/C seconds for
the Wmax/2 packets to be acknowledged. It then resumes
sending, and starts increasing its window size again.
The key to sizing the buﬀer is to make sure that the buﬀer
is large enough, so that while the sender pauses, the buﬀer
doesn’t go empty. When the sender ﬁrst pauses at t1, the
buﬀer is full, and so it drains over a period B/C until t2
(shown in Figure 3). The buﬀer will just avoid going empty
if the ﬁrst packet from the sender shows up at the buﬀer
just as it hits empty, i.e. (Wmax/2)/C ≤ B/C, or
B ≥ Wmax/2.
To determine Wmax, we consider the situation after the
sender has resumed transmission. The window size is now
Wmax/2, and the buﬀer is empty. The sender has to send
packets at rate C or the link will be underutilized. It is well
known that the sending rate of TCP is R = W/RT T Since
the buﬀer is empty, we have no queueing delay. Therefore,
to send at rate C we require that
C =
W
RT T =
Wmax/2
2TP
or Wmax/2 = 2TP × C which for the buﬀer leads to the
well-known rule-of-thumb
B ≥ 2Tp × C = RT T × C.
While not widely known, similar arguments have been made
previously [9, 10], and our result can be easily veriﬁed using
ns2 [11] simulation and a closed-form analytical model [7]
Figure 4 illustrates the evolution of a single TCP Reno ﬂow,
using the topology shown in Figure 2. The buﬀer size is
exactly equal to the rule-of-thumb, B = RT T × C. The
window size follows the familiar sawtooth pattern, increasing
steadily until a loss occurs and then halving the window size
before starting to increase steadily again. Notice that the
buﬀer occupancy almost hits zero once per packet loss, but
never stays empty. This is the behavior we want for the
bottleneck link to stay busy.
The appendix of the extended version of this paper [7]
presents an analytical ﬂuid model that provides a closed-
form equation of the sawtooth, and closely matches the ns2
simulations.
Figures 5 and 6 show what happens if the link is under-
buﬀered or overbuﬀered. In Figure 5, the router is under-
buﬀered, and the buﬀer size is less than RT T × C. The
congestion window follows the same sawtooth pattern as in
the suﬃciently buﬀered case. However, when the window
is halved and the sender pauses waiting for ACKs, there is
insuﬃcient reserve in the buﬀer to keep the bottleneck link
busy. The buﬀer goes empty, the bottleneck link goes idle,
and we lose throughput.
On the other hand, Figure 6 shows a ﬂow which is over-
buﬀered. It behaves like a correctly buﬀered ﬂow in that it
fully utilizes the link. However, when the window is halved,
the buﬀer does not completely empty. The queueing delay
of the ﬂows is increased by a constant, because the buﬀer
always has packets queued.
In summary, if B ≥ 2Tp×C = RT T ×C, the router buﬀer
(just) never goes empty, and the bottleneck link will never
go idle.
3. WHEN MANY LONG TCP FLOWS
SHARE A LINK
In a backbone router many ﬂows share the bottleneck link
simultaneously, and so the single long-lived ﬂow is not a real-
istic model. For example, a 2.5Gb/s (OC48c) link typically
carries over 10,000 ﬂows at a time [4].4 So how should we
change our model to reﬂect the buﬀers required for a bot-
tleneck link with many ﬂows? We will consider two situa-
tions. First, we will consider the case when all the ﬂows are
synchronized with each other, and their sawtooths march
in lockstep perfectly in-phase. Then we will consider ﬂows
that are not synchronized with each other, or are at least
not so synchronized as to be marching in lockstep. When
they are suﬃciently desynchronized — and we will argue
that this is the case in practice — the amount of buﬀering
drops sharply.
3.1 Synchronized Long Flows
It is well-documented that if multiple TCP ﬂows share
a bottleneck link, they can become synchronized with each
other [10, 12, 13]. They are coupled because they experience
packet drops at roughly the same time, and so their “saw-
tooths” become synchronized and in-phase. If a number of
ﬂows share a bottleneck link, they each halve their window
size at the same time; and so the aggregate window process
(the sum of all the window size processes), looks like an am-
pliﬁed version of a single ﬂow. As with a single ﬂow, the
buﬀer needs to be as large as the distance from the peak to
4This shouldn’t be surprising: A typical user today is con-
nected via a 56kb/s modem, and a fully utilized 2.5Gb/s can
simultaneously carry over 40,000 such ﬂows. When it’s not
fully utilized, the buﬀers are barely used, and the link isn’t
a bottleneck. So we should size the buﬀers for when there
are a large number of ﬂows.
Sum of TCP Windows [pkts]
Router Queue [pkts]
PDF of Aggregate Window
Normal Distribution N(11000,400)
Buffer = 1000 pkts
Q = 0
link underutilized
Q > B
packets dropped
 0.035
 0.03
 0.025
 0.02
 0.015
 0.01
 0.005
y
t
i
l
i
b
a
b
o
r
P
 12000
 11500
B
r
e
f
f
u
B
 10500
 10000
 0
 9500
 10000
 10500
 11000
Packets
 11500
 12000
 12500
 50
 52
 54
 56
 58
 60
 62
 64
time [seconds]
Figure 7: The probability distribution of the sum of
the congestion windows of all ﬂows passing through
a router and its approximation with a normal distri-
bution. The two vertical marks mark the boundaries
of where the number of outstanding packets ﬁt into
the buﬀer. If sum of congestion windows is lower
and there are less packets outstanding, the link will
be underutilized. If it is higher the buﬀer overﬂows
and packets are dropped.
the trough of the aggregate window size process, which is
still equal to the bandwidth-delay product.
3.2 Desynchronized Long Flows
Flows are not synchronized in a backbone router carrying
thousands of ﬂows with varying RTTs. Small variations in
RTT or processing time are suﬃcient to prevent synchro-
nization [14]; and the absence of synchronization has been
demonstrated in real networks [4, 15]. Likewise, we found
in our simulations and experiments that while in-phase syn-
chronization is common for under 100 concurrent ﬂows, it is
very rare above 500 concurrent ﬂows. 5 Although we don’t
precisely understand when and why synchronization of TCP
ﬂows takes place, we have observed that for aggregates of
over 500 ﬂows, the amount of in-phase synchronization de-
creases. Under such circumstances we can treat ﬂows as
being not synchronized at all.
To understand the diﬀerence between adding synchronized
and desynchronized window size processes, recall that if we
add together many synchronized sawtooths, we get a sin-
gle large sawtooth, and the buﬀer size requirement doesn’t
change.
If on the other hand the sawtooths are not syn-
chronized, the more ﬂows we add, the less their sum will
look like a sawtooth; they will smooth each other out, and
the distance from the peak to the trough of the aggregate
window size will get smaller. Hence, given that we need as
much buﬀer as the distance from the peak to the trough
of the aggregate window size, we can expect the buﬀer size
requirements to get smaller as we increase the number of
5Some out-of-phase synchronization (where ﬂows are syn-
chronized but scale down their window at diﬀerent times
during a cycle) was visible in some ns2 simulations with up
to 1000 ﬂows. However, the buﬀer requirements are very
similar for out-of-phase synchronization as they are for no
synchronization at all.
P
Figure 8: Plot of
the queue Q oﬀset by 10500 packets.
Wi(t) of all TCP ﬂows, and of
ﬂows. This is indeed the case, and we will explain why, and
then demonstrate via simulation.
Consider a set of TCP ﬂows with random (and indepen-
dent) start times and propagation delays. We’ll assume that
they are desynchronized enough that the window size pro-
cesses are independent of each other. We can model the
total window size as a bounded random process made up of
the sum of these independent sawtooths. We know from the
central limit theorem that the aggregate window size process
will converge to a gaussian process. Figure 7 shows that in-
deed the aggregate window size does converge to a gaussian
process. The graph shows the probability distribution of the
sum of the congestion windows of all ﬂows W =
Wi, with
diﬀerent propagation times and start times as explained in
Section 5.1.
P
From the window size process, we know that the queue
occupancy at time t is
nX
Q(t) =
Wi(t) − (2TP · C) − .
(1)
i=1
In other words, all outstanding bytes are in the queue (Q(t)),
on the link (2Tp · C), or have been dropped. We represent
the number of dropped packets by . If the buﬀer is large
enough and TCP is operating correctly, then  is negligible
compared to 2TP · C. Therefore, the distribution of Q(t) is
shown in Figure 8, and is given by
Q d
= W − 2TP · C.
(2)
Because W has a normal distribution, Q has the distribution
of a normal shifted by a constant (of course, the normal dis-
tribution is restricted to the allowable range for Q). This is
very useful, because we can now pick a buﬀer size and know
immediately the probability that the buﬀer will underﬂow
and lose throughput.
Because it is gaussian, we can determine the queue occu-
pancy process if we know its mean and variance. The mean
is simply the sum of the mean of its constituents. To ﬁnd
the variance, we’ll assume for convenience that all sawtooths
have the same average value (having diﬀerent values would
still yield the same results). Each TCP sawtooth can be
modelled as oscillating with a uniform distribution around
W i
W i. Since the standard deviation of the
12 -th of its length, the standard
its average congestion window size W i, with minimum 2
3
and maximum 4
3
uniform distribution is
deviation of a single window size σWi is thus
√
1
3
W i − 2
3
1√
12
σWi =
„
«
W i
4
3
W i
3
1√
=
From Equation (2),
W i =
W
n =
2T p · C + Q
n
≤ 2T p · C + B
n
.
For a large number of ﬂows, the standard deviation of the
sum of the windows, W , is given by
σW ≤ √
nσWi ,
and so by Equation (2) the standard deviation of Q(t) is
√
σQ = σW ≤ 1
3
3
2T p · C + B
√
.
n
Now that we know the distribution of the queue occu-
pancy, we can approximate the link utilization for a given
buﬀer size. Whenever the queue size is below a threshold,
b, there is a risk (but not guaranteed) that the queue will
go empty, and we will lose link utilization. If we know the
probability that Q < b, then we have an upper bound on
the lost utilization. Because Q has a normal distribution,
we can use the error-function to evaluate this probability.
Therefore, we get the following lower bound for the utiliza-
tion.
U til ≥ erf
0
√
@ 3