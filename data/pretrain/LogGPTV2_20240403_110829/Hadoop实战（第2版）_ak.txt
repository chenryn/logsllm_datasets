bin/hadoop jar contrib/streaming/hadoop-0.20.2-streaming.jar-input input-output
pyoutput-mapper reduce.py-reducer aggregate-file reduce.py
注意其中的aggregate是Hadoop提供的一个包，它提供一个Reduce函数和一个combine函数。这个函数实现一些简单的类似求和、取最大值最小值等的功能。
3.5 Hadoop Pipes
Hadoop Pipes提供了一个在Hadoop上运行C++程序的方法。与流不同的是，流使用的是标准输入输出作为可执行程序与Hadoop相关进程间通信的工具，而Pipes使用的是Sockets。先看一个示例程序wordcount.cpp：
#include"hadoop/Pipes.hh"
#include"hadoop/TemplateFactory.hh"
#include"hadoop/StringUtils.hh"
const std：string WORDCOUNT="WORDCOUNT"；
const std：string INPUT_WORDS="INPUT_WORDS"；
const std：string OUTPUT_WORDS="OUTPUT_WORDS"；
class WordCountMap：public HadoopPipes：Mapper{
public：
HadoopPipes：TaskContext：Counter*inputWords；
WordCountMap（HadoopPipes：TaskContext＆context）{
inputWords=context.getCounter（WORDCOUNT, INPUT_WORDS）；
}
void map（HadoopPipes：MapContext＆context）{
std：vector＜std：string＞words=
HadoopUtils：splitString（context.getInputValue（），""）；
for（unsigned int i=0；i＜words.size（）；++i）{
context.emit（words[i]，"1"）；
}
context.incrementCounter（inputWords, words.size（））；
}
}；
class WordCountReduce：public HadoopPipes：Reducer{
public：
HadoopPipes：TaskContext：Counter*outputWords；
WordCountReduce（HadoopPipes：TaskContext＆context）{
outputWords=context.getCounter（WORDCOUNT, OUTPUT_WORDS）；
}
void reduce（HadoopPipes：ReduceContext＆context）{
int sum=0；
while（context.nextValue（））{
sum+=HadoopUtils：toInt（context.getInputValue（））；
}
context.emit（context.getInputKey（），HadoopUtils：toString（sum））；
context.incrementCounter（outputWords，1）；
}
}；
int main（int argc, char*argv[]）{
return HadoopPipes：runTask（HadoopPipes：TemplateFactory＜WordCountMap，
WordCountReduce＞（））；
}
这个程序连接的是一个C++库，结构类似于Java编写的程序。如新版API一样，这个程序使用context方法读入和收集＜key, value＞对。在使用时要重写HadoopPipes名字空间下的Mapper和Reducer函数，并用context.emit（）方法输出＜key, value＞对。main函数是应用程序的入口，它调用HadoopPipes：runTask方法，这个方法由一个TemplateFactory参数来创建Map和Reduce实例，也可以重载factory设置combiner（）、partitioner（）、record reader、record writer。
接下来，编译这个程序。这个编译命令需要用到g++，读者可以使用apt自动安装这个程序。g++的命令格式如下所示：
apt-get install g++
然后建立文件Makerfile，如下所示：
HADOOP_INSTALL="你的hadoop安装文件夹"
PLATFORM=Linux-i386-32（如果是AMD的CPU，请使用Linux-amd64-64）
CC=g++
CPPFLAGS=-m32-I$（HADOOP_INSTALL）/c++/$（PLATFORM）/include
wordcount：wordcount.cpp
$（CC）$（CPPFLAGS）$＜-Wall-L$（HADOOP_INSTALL）/c++/$（PLATFORM）/lib-lhadooppipes
-lhadooputils-lpthread-g-O2-o$@
注意在$（CC）前有一个＜tab＞符号，这个分隔符是很关键的。
在当前目录下建立一个WordCount可执行文件。
接着，上传可执行文件到HDFS上，这是为了TaskTracker能够获得这个可执行文件。这里上传到bin文件夹内。
～/hadoop/bin/hadoop fs-mkdir bin
～/hadoop/bin/hadoop dfs-put wordcount bin
然后，就可以运行这个MapReduce程序了，可以采用两种配置方式运行这个程序。一种方式是直接在命令中运行指定配置，如下所示：
～/hadoop/bin/hadoop pipes\
-D hadoop.pipes.java.recordreader=true\
-D hadoop.pipes.java.recordwriter=true\
-input input\
-output Coutput\
-program bin/wordcount
另一种方式是预先将配置写入配置文件中，如下所示：
＜?xml version="1.0"?＞
＜configuration＞
＜property＞
//Set the binary path on DFS
＜name＞hadoop.pipes.executable＜/name＞
＜value＞bin/wordcount＜/value＞
＜/property＞
＜property＞
＜name＞hadoop.pipes.java.recordreader＜/name＞
＜value＞true＜/value＞
＜/property＞
＜property＞
＜name＞hadoop.pipes.java.recordwriter＜/name＞
＜value＞true＜/value＞
＜/property＞
＜/configuration＞
然后通过如下命令运行这个程序：
～/hadoop/bin/hadoop pipes-conf word.xml-input input-output output
将参数hadoop.pipes.executable和hadoop.pipes.java.recordreader设置为true表示使用Hadoop默认的输入输出方式（即Java的）。同样的，也可以设置一个Java语言编写的Mapper函数、Reducer函数、combiner函数和partitioner函数。实际上，在任何一个作业中，都可以混用Java类和C++类。
3.6 本章小结
本章主要介绍了MapReduce的计算模型，其中的关键内容是一个流程和四个方法。一个流程指的是数据流程，输入数据到＜k1，v1＞、＜k1，v1＞到＜k2，v2＞、＜k2，v2＞到＜k3，v3＞、＜k3，v3＞到输出数据。四个方法就是这个数据转换过程中使用的方法（分别是InputFormat、Map、Reduce、OutputFormat），以及其对应的转换过程。除此之外，还介绍了MapReduce编程框架的几个优化方法，以及Hadoop流和Hadoop Pipes，后者是在Hadoop中使用脚本文件及C++编写MapReduce程序的方法。
第4章 开发MapReduce应用程序
本章内容
系统参数的配置
配置开发环境
编写MapReduce程序
本地测试
运行MapReduce程序
网络用户界面
性能调优
MapReduce工作流
本章小结
在前面的章节中，已经介绍了MapReduce模型。在本章中，将介绍如何在Hadoop中开发MapReduce的应用程序。在编写MapReduce程序之前，需要安装和配置开发环境，因此，首先要学习如何进行配置。
4.1 系统参数的配置
1.通过API对相关组件的参数进行配置
Hadoop有很多自己的组件（例如Hbase和Chukwa等），每一种组件都可以实现不同的功能，并起着不同的作用，通过多种组件的配合使用，Hadoop就能够实现非常强大的功能。这些可以通过Hadoop的API对相关参数进行配置来实现。
先简单地介绍一下API
[1]
 ，它被分成了以下几个部分（也就是几个不同的包）。
org. apache.hadoop.conf：定义了系统参数的配置文件处理API；
org. apache.hadoop.fs：定义了抽象的文件系统API；
org. apache.hadoop.dfs：Hadoop分布式文件系统（HDFS）模块的实现；
org. apache.hadoop.mapred：Hadoop分布式计算系统（MapReduce）模块的实现，包括任务的分发调度等；
org. apache.hadoop.ipc：用在网络服务端和客户端的工具，封装了网络异步I/O的基础模块；
org. apache.hadoop.io：定义了通用的I/O API，用于针对网络、数据库、文件等数据对象进行读写操作等。
在此我们需要用到org.apache.hadoop.conf，用它来定义系统参数的配置。Configurations类由源来设置，每个源包含以XML形式出现的一系列属性/值对。每个源以一个字符串或一个路径来命名。如果是以字符串命名，则通过类路径检查该字符串代表的路径是否存在；如果是以路径命名的，则直接通过本地文件系统进行检查，而不用类路径。
下面举一个配置文件的例子。
configuration-default. xml
＜?xml version="1.0"?＞
＜configuration＞
＜property＞
＜name＞hadoop.tmp.dir＜/name＞
＜value＞/tmp/hadoop-${usr.name}＜/value＞
＜description＞A base for other temporary directories.＜/description＞
＜/property＞
＜property＞
＜name＞io.file.buffer.size＜/name＞
＜value＞4096＜/value＞
＜description＞the size of buffer for use in sequence file.＜/description＞
＜/property＞
＜property＞
＜name＞height＜/name＞
＜value＞tall＜/value＞
＜final＞true＜/final＞
＜/property＞
＜/configuration＞
这个文件中的信息可以通过以下的方式进行抽取：
Configuration conf=new Configuration（）；
Conf.addResource（"configuration-default.xml"）；
aasertThat（conf.get（"hadoop.tmp.dir"），is（"/tmp/hadoop-${usr.name}"））；
assertThat（conf.get（"io.file.buffer.size"），is（"4096"））；
assertThat（conf.get（"height"），is（"tall"））；
2.多个配置文件的整合
假设还有另外一个配置文件configuration-site.xml，其中具体代码细节如下：
configuration-site. xml