title:Internet Jones and the Raiders of the Lost Trackers: An Archaeological
Study of Web Tracking from 1996 to 2016
author:Adam Lerner and
Anna Kornfeld Simpson and
Tadayoshi Kohno and
Franziska Roesner
Internet Jones and the Raiders of the Lost Trackers: 
An Archaeological Study of Web Tracking  
from 1996 to 2016
Adam Lerner, Anna Kornfeld Simpson, Tadayoshi Kohno, and Franziska Roesner,  
University of Washington
 https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/lerner
This paper is included in the Proceedings of the 25th USENIX Security SymposiumAugust 10–12, 2016 • Austin, TXISBN 978-1-931971-32-4Open access to the Proceedings of the 25th USENIX Security Symposium is sponsored by USENIX Internet Jones and the Raiders of the Lost Trackers:
An Archaeological Study of Web Tracking from 1996 to 2016
Adam Lerner∗, Anna Kornfeld Simpson∗, Tadayoshi Kohno, Franziska Roesner
University of Washington
{lerner,aksimpso,yoshi,franzi}@cs.washington.edu
Abstract
Though web tracking and its privacy implications have
received much attention in recent years, that attention
has come relatively recently in the history of the web
and lacks full historical context.
In this paper, we
present longitudinal measurements of third-party web
tracking behaviors from 1996 to present (2016). Our
tool, TrackingExcavator, leverages a key insight:
that
the Internet Archive’s Wayback Machine opens the pos-
sibility for a retrospective analysis of tracking over time.
We contribute an evaluation of the Wayback Machine’s
view of past third-party requests, which we ﬁnd is im-
perfect — we evaluate its limitations and unearth lessons
and strategies for overcoming them. Applying these
strategies in our measurements, we discover (among
other ﬁndings) that third-party tracking on the web has
increased in prevalence and complexity since the ﬁrst
third-party tracker that we observe in 1996, and we see
the spread of the most popular trackers to an increasing
percentage of the most popular sites on the web. We ar-
gue that an understanding of the ecosystem’s historical
trends — which we provide for the ﬁrst time at this scale
in our work — is important to any technical and policy
discussions surrounding tracking.
1
Third-party web tracking is the practice by which third
parties like advertisers, social media widgets, and web-
site analytics engines — embedded in the ﬁrst party sites
that users visit directly — re-identify users across do-
mains as they browse the web. Web tracking, and the
associated privacy concerns from tracking companies
building a list of sites users have browsed to, has inspired
a signiﬁcant and growing body of academic work in the
computer security and privacy community, attempting to
understand, measure, and defend against such tracking
(e.g., [3, 4, 6, 8, 14, 15, 18–20, 22, 24, 25, 27–30, 32–
34, 37, 39–43, 45, 46, 51, 57, 60, 61, 64–66, 70, 71]).
Introduction
∗Co-ﬁrst authors listed in alphabetical order.
1
However, the research community’s interest in web
tracking comes relatively recently in the history of web.
To our knowledge, the earliest measurement studies be-
gan in 2005 [42], with most coming after 2009 — while
display advertising and the HTTP cookie standard date
to the mid-1990s [44, 48]. Though numerous studies
have now been done, they typically consist of short-term
measurements of speciﬁc tracking techniques. We ar-
gue that public and private discussions surrounding web
tracking — happening in technical, legal, and policy are-
nas (e.g., [49, 72]) — ought to be informed not just by a
single snapshot of the web tracking ecosystem but by a
comprehensive knowledge of its trajectory over time. We
provide such a comprehensive view in this paper, con-
ducting a measurement study of third-party web tracking
across 20 years since 1996.
Measurement studies of web tracking are critical to
technologists, policy-
provide transparency for users,
makers, and even those sites that include trackers, to help
them understand how user data is collected and used, to
enable informed decisions about privacy, and to incen-
tivize companies to consider privacy. However, the web
tracking ecosystem is continuously evolving, and others
have shown that web privacy studies at a single point in
time may only temporarily reduce the use of speciﬁc con-
troversial tracking techniques [63]. While one can study
tracking longitudinally starting in the present, as we and
others have (e.g., [42, 63]), ideally any future develop-
ments in the web tracking ecosystem can be contextu-
alized in a comprehensive view of that ecosystem over
time — i.e., since the very earliest instance of tracking
on the web. We provide that longitudinal, historical con-
text in this paper, asking: how has the third-party web
tracking ecosystem evolved since its beginnings?
To answer this question, we apply a key insight: the
Internet Archive’s Wayback Machine [31] enables a ret-
rospective analysis of third-party tracking on the web
USENIX Association  
25th USENIX Security Symposium  997
over time. The Wayback Machine1 contains archives of
full webpages, including JavaScript, stylesheets, and em-
bedded resources, dating back to 1996. To leverage this
archive, we design and implement a retrospective track-
ing detection and analysis platform called TrackingEx-
cavator (Section 3), which allows us to conduct a lon-
gitudinal study of third-party tracking from 1996 to
present (2016). TrackingExcavator logs in-browser be-
haviors related to web tracking, including:
third-party
requests, cookies attached to requests, cookies program-
matically set by JavaScript, and the use of other relevant
JavaScript APIs (e.g., HTML5 LocalStorage and APIs
used in browser ﬁngerprinting [15, 57], such as enumer-
ating installed plugins). TrackingExcavator can run on
both live as well as archived versions of websites.
Harnessing the power of the Wayback Machine for our
analysis turns out to be surprisingly challenging (Sec-
tion 4). Indeed, a key contribution of this paper is our
evaluation of the historical data provided by the Way-
back Machine, and a set of lessons and techniques for
extracting information about trends in third-party con-
tent over time. Through a comparison with ground truth
datasets collected in 2011 (provided to us by the authors
of [60]), 2013, 2015, and 2016, we ﬁnd that the Way-
back Machine’s view of the past, as it relates to included
third-party content, is imperfect for many reasons, in-
cluding sites that were not archived due to robots.txt
restrictions (which are respected by the Wayback Ma-
chine’s crawlers), the Wayback Machine’s occasional
failure to archive embedded content, as well as site re-
sources that were archived at different times than the top-
level site. Though popular sites are typically archived
at regular intervals, their embedded content (including
third-party trackers) may thus be only partially repre-
sented. Whereas others have observed similar limita-
tions with the Wayback Machine, especially as it relates
to content visible on the top-level page [10, 38, 53], our
analysis is focused on the technical impact of missing
third-party elements, particularly with respect to track-
ing. Through our evaluation, we characterize what the
Wayback Machine lets us measure about the embedded
third parties, and showcase some techniques for best us-
ing the data it provides and working around some of its
weaknesses (Section 4).
After evaluating the Wayback Machine’s view into the
past and developing best practices for using its data, we
use TrackingExcavator to conduct a longitudinal study
of the third-party web tracking ecosystem from 1996-
2016 (Sections 5). We explore how this ecosystem has
changed over time, including the prevalence of different
web tracking behaviors, the identities and scope of pop-
ular trackers, and the complexity of relationships within
1https://archive.org
the ecosystem. Among our ﬁndings, we identify the ear-
liest tracker in our dataset in 1996 and observe the rise
and fall of important players in the ecosystem (e.g., the
rise of Google Analytics to appear on over a third of all
popular websites). We ﬁnd that websites contact an in-
creasing number of third parties over time (about 5% of
the 500 most popular sites contacted at least 5 separate
third parties in early 2000s, whereas nearly 40% do so
in 2016) and that the top trackers can track users across
an increasing percentage of the web’s most popular sites.
We also ﬁnd that tracking behaviors changed over time,
e.g., that third-party popups peaked in the mid-2000s and
that the fraction of trackers that rely on referrals from
other trackers has recently risen.
Taken together, our ﬁndings show that third-party web
tracking is a rapidly growing practice in an increasingly
complex ecosystem — suggesting that users’ and policy-
makers’ concerns about privacy require sustained, and
perhaps increasing, attention. Our results provide hith-
erto unavailable historical context for today’s technical
and policy discussions.
In summary, our contributions are:
1. TrackingExcavator, a measurement infrastruc-
ture for detecting and analyzing third-party web
tracking behaviors in the present and — leveraging
the Wayback Machine — in the past (Section 3).
2. An in-depth analysis of the scope and accuracy
of the Wayback Machine’s view of historical web
tracking behaviors and trends, and techniques for
working around its weaknesses (Section 4).
3. A longitudinal measurement study of third-party
cookie-based web tracking from 1996 to present
(2016) — to the best of our knowledge, the longest
longitudinal study of tracking to date (Section 5).
This paper and any updates, including any data or
code we publish, will be made available at http://
trackingexcavator.cs.washington.edu/.
2 Background and Motivation
Third-party web tracking is the practice by which enti-
ties (“trackers”) embedded in webpages re-identify users
as they browse the web, collecting information about the
websites that they visit [50, 60]. Tracking is typically
done for the purposes of website analytics, targeted ad-
vertising, and other forms of personalization (e.g., so-
cial media content). For example, when a user vis-
its www.cnn.com, the browser may make additional re-
quests to doubleclick.net to load targeted ads and
to facebook.com to load the “Like” button; as a re-
sult, Doubleclick and Facebook learn about that user’s