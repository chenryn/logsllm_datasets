connected to by attacker s. We refer to this count as
T C(s). A large T C represents conﬁrmed IP sweep be-
havior, which we strongly associate with our malware
behavior model. T C is the exclusive prioritization met-
ric used by GWOL, whereas here we consider T C a sec-
ondary factor to P S in computing a ﬁnal malware be-
havior score. We could also include metrics regarding
the number of DShield sensors (i.e., unique contributor
IDs) that have reported the attacker, which arguably rep-
resents the degree of consensus in the contributor pool
that the attack source is active across the Internet. How-
ever, the IP sweep pattern is of high interest, even when
the IP sweep experiences may have been reported only
by a smaller set of sensors.
Third, we compute an optional tertiary behavior met-
ric that captures the ratio of national to international ad-
dresses that are targeted by attacker s, IR(s). Within
M S(s) = P S(s) + log (T C(s)) + δ log (IR(s)) (5)
The three factors are computed in order of signiﬁcance
in mapping to our malware behavior model. Logarithm
is used because in our model, the secondary metric (T C)
and the tertiary metric (IR) are less important than the
malware port score and we only care about their order of
magnitude.
3.4 Blacklist Production
For each attacker, we now have both its relevance ranking
and its severity score. We can combine them to generate
a ﬁnal blacklist for each contributor.
For the ﬁnal blacklist, we would like to include the
attackers that have strong relevance and discard the non-
relevant attackers. To generate a ﬁnal list of length L, we
use the attacker’s relevance ranking to compile a candi-
date list of size c · L. (We often set c = 2.) Then, we use
severity scores of the attackers on the candidate list to ad-
just its ranking and pick the L highest-ranked attackers
to form the ﬁnal list. Intuitively, the adjustment should
promote the rank of an attacker if the severity assessment
indicates that it is very malicious. Toward this goal, we
deﬁne a ﬁnal score that combines the attacker’s relevance
rank in the candidate list and its severity assessment. In
particular, let k be the relevance rank of the attacker s
114 
17th USENIX Security Symposium 
USENIX Association
(i.e., s is the k-th entry in the candidate list). Recall from
last section M S(s) is the severity score of s. The ﬁnal
score f in(s) is deﬁned to be
f in(s) = k −
L
2 · Φ(M S(s))
(6)
where
Φ(x) =
1
2
(1 + erf( x − µ
d
))
where erf(·) is the “S” shaped Gaussian error function.
We plot Φ(x) in Figure 6 with µ = 4 and different d.
)
X
(
i
h
P
1
0.8
0.6
0.4
0.2
0
0
d=1
d=1.5
d=2
d=2.5
2
4
X
6
8
10
Figure 6: Phi with different d value
Φ(M S(s)) promotes the rank of an attacker according
to its maliciousness. The larger the value of Φ(M S(s))
is, the more the attacker is moved above comparing to its
original rank. A Φ(M S(s)) of value 1 would then move
the attacker above for one half of the size of the ﬁnal
list comparing to its original rank. The “S” shaped Φ(·)
transforms the severity assessment M S(s) into a value
between 0 and 1. The less-malicious attackers often give
an assessment score below 3. After transformation, they
will receive only small promotions. On the other hand,
malicious attackers that give an assessment score above
7 will be highly promoted.
To generate the ﬁnal list, we sort the f in(s) values of
the attackers in the candidate list and then pick L of them
that have the smallest f in(s).
4 Experiment Results
We created an experimental HPB blacklist formulation
system. To evaluate the HPBs, we performed a battery
of experiments using the DShield.org security ﬁrewall
and IDS log repository. We examined a collection of
more than 720 million log entries produced by DShield
contributors from October to November 2007. Since our
relevance measure is based on correlations between con-
tributors, HPB production is not applicable to contribu-
tors that have submitted very few reports (DShield has
contributors that hand-select or sporadically contribute
logs, providing very few alerts). We therefore exclude
those contributors that we ﬁnd effectively have no corre-
lation with the wider contributor pool or simply have too
few alerts to produce meaningful results. For this analy-
sis, we found that we could compute correlation relation-
ships for about 700 contributors, or 41% of the DShield
contributor pool.
To assess the performance of
the HPB system,
we compare its performance relative to the standard
DShield-produced GWOL [17]. In addition, we compare
our HPB performance to that of LWOLs, which we com-
pute individually for all contributors in our comparison
set. For the purpose of our comparative assessment, we
ﬁxed the length of all three competing blacklists to ex-
actly 1000 entries. However, after we present our com-
parative performance results, we will then continue our
investigation by analyzing how the blacklist length af-
fects the performance of the HPBs.
In the experiments, we generate GWOL, LWOL, and
HPBs using data for a certain time period and then test
the blacklists on data from the time window following
this period. We call the period used for producing black-
lists the training window and the period for testing the
prediction window. In practice, the training period repre-
sents a snapshot of the most recent history of the repos-
itory, used to formulate each blacklist for a contributor
that is then expected to use the blacklist for the length of
the prediction window. The sizes of these two windows
are not necessarily equal. We will ﬁrst describe experi-
ments that use 5-day lengths for both the training window
and the prediction window. We then present experiments
that investigate the effects of the two windows’ lengths
on HPB quality.
4.1 Hit Count Improvement
DShield logs submitted during the prediction window
are used to determine how many sources included within
a contributor’s HPB are indeed encountered during that
prediction window. We call this value the blacklist hit
count. We view each blacklist address ﬁlter not encoun-
tered by the blacklist consumer as an opportunity cost to
have prevented the deployment of other ﬁlters that could
have otherwise blocked unwanted trafﬁc. In this sense,
we view our hit count metric as an important measure
of the effectiveness of a blacklist formulation algorithm.
Note that our HPBs are formulated with severity analy-
sis while the other lists are not. As the severity analysis
prefers malicious activities, we expect that the hits on the
HPBs are more malicious.
To compare the three types of lists, we take 60 days of
data, divided into twelve 5-day windows. We repeat the
experiment 11 times using the i-th window as the training
window and the (i+1)-th window as the testing window.
In the training window, we construct HPB, LWOL, and
USENIX Association  
17th USENIX Security Symposium 
115
Window
GWOL total hit
LWOL total hit
HPB total hit
HPB/GWOL
HPB/LWOL
1
2
3
4
5
6
7
8
9
10
11
81937
83899
87098
80849
87271
93488
100209
96541
94441
96702
97229
85141
74206
96411
75127
88661
73879
105374
91289
107717
94813
108137
112009
115296
122256
115715
118078
122041
133421
126436
128297
128753
131777
Average
90879 ± 6851
90978 ± 13002
123098 ± 7193
1.36701
1.37422
1.40366
1.43125
1.353
1.30542
1.33143
1.30966
1.35849
1.33144
1.35533
1.36 ± 0.04
1.31557
1.55373
1.26807
1.54026
1.33179
1.6519
1.26617
1.38501
1.19106
1.35797
1.21861
1.37 ± 0.15
Table 3: Hit Number Comparison between HPB, LWOL and GWOL
Contributor
Percentage
Average Median
Increase
Increase
StdDev
Improved vs. GWOL
Poor vs. GWOL
Improved vs. LWOL
Poor vs. LWOL
90%
7%
95%
4%
51
-27
75
-19
22
-7
36
-9
89
47
90
28
Increase
Range
1 to 732
-1 to -206
1 to 491
-1 to -104
Table 4: Hit Count Performance, HPB vs. (GWOL and LWOL), Length 1000 Entries
GWOL. Then the three types of lists are tested on the
data in the testing window.
Table 3 shows the total number of hits summed over
the contributors for HPB, GWOL, and LWOL, respec-
tively. It also shows the ratio of HPB hits over that of
GWOL and LWOL. We see that in every window, HPB
has more hits than GWOL and LWOL. Overall, HPBs
predict 20-30% more hits than LWOL and GWOL. Note
that there are quite large variances among the number of
hits between time windows. Most of the variances, how-
ever, are not from our blacklist construction, rather they
are from the variance among the number of attackers the
networks experience in different testing windows.
Increase
Increase
Average Median
Increase
StdDev
vs. GWOL
vs. LWOL
129
183
78
188
124
93
Increase
Range
40 to 732
59 to 491
Table 5: Top 200 Contributors’ Hit Count Increases
(Blacklist Length 1000)
The results in Table 3 show HPB’s hit improvement
over time windows. We now investigate the distribution
of the HPB’s hit improvement across contributors in one
time window. We use two quantities for comparison. The
ﬁrst is the hit count improvement, which is simply the
HPB hit count minus the hit count of the other list. The
second comparative measure we used is the relative hit
count improvement (RI), which is the ratio in percentage
of the HPB hit count increase over the other blacklist hit
count. If the other list hit count is zero we deﬁne RI to be
100x the HPB hit count, and if both hit counts are zero
we set RI to 100.
Table 5 provides a summary of hit-count improvement
for the 200 contributors where HPBs perform the best.
The hit-count results for all the contributors are summa-
rized in Table 4.
Figure 7 compares HPB to GWOL. The left panel of
the ﬁgure plots the histogram showing the distribution of
the hit improvement across the contributors. The x-axis
indicates improvements, and the hight of the bars repre-
sents the number of contributors whose improvement fall
in the corresponding bin. Bars left to x = 0 represent
contributors for whom the HPB has worse performance
and bars on the right represent contributors for whom
HPBs performed better. For most contributors, the im-
provment is positive. The largest improvement reaches
732. For only a few contributors, HPB performs worse
in this time window.
The panel on the right of Figure 7 plots the RI (ratio %
of HPB’s hit count increase over GWOL’s hit count) dis-
tribution. We sort the RI values and plot them against the
contributors. We label the x-axis by cummulative per-
centage, i.e., a tick on x-axis represents the percentage
of contributors that lie to the left of the tick. For exam-
ple, the tick 20 means 20 percent of the contributors lie
left to this tick. There are contributors for which the RI
value can be more than 3900. Instead of showing such
large RI values, we cut off the plot at RI value 300. From
the plot, we see that there are about 20% of contributors
for which the HPBs achieve an RI more than 100, i.e.,
the HPB at least doubled the GWOL hit count. For about
half of the contributors, the HPBs have about 25% more
hits (an RI of 25). The HPBs have more hits than GWOL
for almost 90% of the contributors. Only for a few con-
116 
17th USENIX Security Symposium 
USENIX Association
200
150
100
50
s
r
o
t
u
b
i
r
t
n
o
C
f
o
#
0
−200
e
s
a
e
r
c
n
I
t
n
u
o
C
t
i
H
)
%
(
e
v
i
t
a
e
R
l
300
250
200
150
100
50