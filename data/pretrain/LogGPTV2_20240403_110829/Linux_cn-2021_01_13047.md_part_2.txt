example_content.attrs
```
现在，我可以看到 `href` 是一个属性，可以像字典项一样提取它：
```
example_href = example_content['href']
print(example_href)
```
### 整合网站抓取工具
所有的这些探索为我们提供了前进的路径。这是厘清上面逻辑的一个清理版本。
```
city_hrefs = [] # initialise empty list
for i in dollar_tree_list:
    cont = i.contents[0]
    href = cont['href']
    city_hrefs.append(href)
#  check to be sure all went well
for i in city_hrefs[:2]:
  print(i)
```
输出的内容是一个关于抓取爱达荷州 Family Dollar 商店 URL 的列表。
也就是说，我仍然没有获得地址信息！现在，需要抓取每个城市的 URL 以获得此信息。因此，我们使用一个具有代表性的示例重新开始该过程。
```
page2 = requests.get(city_hrefs[2]) # again establish a representative example
soup2 = BeautifulSoup(page2.text, 'html.parser')
```
![Family Dollar 地图和代码](/data/attachment/album/202101/24/093542sicag2cggogamoc4.png "Family Dollar map and code")
地址信息嵌套在 `type="application/ld+json"` 里。经过大量的地理位置抓取之后，我开始认识到这是用于存储地址信息的一般结构。幸运的是，`soup.find_all()` 开启了利用 `type` 搜索。
```
arco = soup2.find_all(type="application/ld+json")
print(arco[1])
```
地址信息在第二个列表成员中！原来如此！
使用 `.contents` 提取（从第二个列表项中）内容（这是过滤后的合适的默认操作）。同样，由于输出的内容是一个列表，因此我为该列表项建立了索引：
```
arco_contents = arco[1].contents[0]
arco_contents
```
喔，看起来不错。此处提供的格式与 JSON 格式一致（而且，该类型的名称中确实包含 “json”）。 JSON 对象的行为就像是带有嵌套字典的字典。一旦你熟悉利用其去工作，它实际上是一种不错的格式（当然，它比一长串正则表达式命令更容易编程）。尽管从结构上看起来像一个 JSON 对象，但它仍然是 `bs4` 对象，需要通过编程方式转换为 JSON 对象才能对其进行访问：
```
arco_json =  json.loads(arco_contents)
```
```
type(arco_json)
print(arco_json)
```
在该内容中，有一个被调用的 `address` 键，该键要求地址信息在一个比较小的嵌套字典里。可以这样检索：
```
arco_address = arco_json['address']
arco_address
```
好吧，请大家注意。现在我可以遍历存储爱达荷州 URL 的列表：
```
locs_dict = [] # initialise empty list
for link in city_hrefs:
  locpage = requests.get(link)   # request page info
  locsoup = BeautifulSoup(locpage.text, 'html.parser')
      # parse the page's content
  locinfo = locsoup.find_all(type="application/ld+json")
      # extract specific element
  loccont = locinfo[1].contents[0]  
      # get contents from the bs4 element set
  locjson = json.loads(loccont)  # convert to json
  locaddr = locjson['address'] # get address
  locs_dict.append(locaddr) # add address to list
```
### 用 Pandas 整理我们的网站抓取结果
我们在字典中装载了大量数据，但是还有一些额外的无用项，它们会使重用数据变得比需要的更为复杂。要执行最终的数据组织，我们需要将其转换为 Pandas 数据框架，删除不需要的列 `@type` 和 `country`，并检查前五行以确保一切正常。
```
locs_df = df.from_records(locs_dict)
locs_df.drop(['@type', 'addressCountry'], axis = 1, inplace = True)
locs_df.head(n = 5)
```
确保保存结果！！
```
df.to_csv(locs_df, "family_dollar_ID_locations.csv", sep = ",", index = False)
```
我们做到了！所有爱达荷州 Family Dollar 商店都有一个用逗号分隔的列表。多令人兴奋。
### Selenium 和数据抓取的一点说明
[Selenium](https://www.selenium.dev/) 是用于与网页自动交互的常用工具。为了解释为什么有时必须使用它，让我们来看一个使用 Walgreens 网站的示例。 “检查元素” 提供了浏览器显示内容的代码：
![Walgreens 位置页面和代码](/data/attachment/album/202101/24/093543kizruhh3yqq5yhcy.png "Walgreens location page and code")
虽然 “查看页面源代码” 提供了有关 `requests` 将获得什么内容的代码：
![Walgreens 源代码](/data/attachment/album/202101/24/093543wx98gxo3xog299uo.png "Walgreens source code")
如果这两个不一致，是有一些插件可以修改源代码 —— 因此，应在将页面加载到浏览器后对其进行访问。`requests` 不能做到这一点，但是 Selenium 可以做到。
Selenium 需要 Web 驱动程序来检索内容。实际上，它会打开 Web 浏览器，并收集此页面的内容。Selenium 功能强大 —— 它可以通过多种方式与加载的内容进行交互（请阅读文档）。使用 Selenium 获取数据后，继续像以前一样使用 BeautifulSoup：
```
url = "https://www.walgreens.com/storelistings/storesbycity.jsp?requestType=locator&state=ID"
driver = webdriver.Firefox(executable_path = 'mypath/geckodriver.exe')
driver.get(url)
soup_ID = BeautifulSoup(driver.page_source, 'html.parser')
store_link_soup = soup_ID.find_all(class_ = 'col-xl-4 col-lg-4 col-md-4')
```
对于 Family Dollar 这种情形，我不需要 Selenium，但是当呈现的内容与源代码不同时，我确实会保留使用 Selenium。
### 小结
总之，使用网站抓取来完成有意义的任务时：
* 耐心一点
* 查阅手册（它们非常有帮助）
如果你对答案感到好奇：
![Family Dollar 位置图](/data/attachment/album/202101/24/093543sbnbl5w8znxsk88b.png "Family Dollar locations map")
美国有很多 Family Dollar 商店。
完整的源代码是：
```
import requests
from bs4 import BeautifulSoup
import json
from pandas import DataFrame as df
page = requests.get("https://www.familydollar.com/locations/")
soup = BeautifulSoup(page.text, 'html.parser')
# find all state links
state_list = soup.find_all(class_ = 'itemlist')
state_links = []
for i in state_list:
    cont = i.contents[0]
    attr = cont.attrs
    hrefs = attr['href']
    state_links.append(hrefs)
# find all city links
city_links = []
for link in state_links:
    page = requests.get(link)
    soup = BeautifulSoup(page.text, 'html.parser')
    familydollar_list = soup.find_all(class_ = 'itemlist')
    for store in familydollar_list:
        cont = store.contents[0]
        attr = cont.attrs
        city_hrefs = attr['href']
        city_links.append(city_hrefs)
# to get individual store links
store_links = []
for link in city_links:
    locpage = requests.get(link)
    locsoup = BeautifulSoup(locpage.text, 'html.parser')
    locinfo = locsoup.find_all(type="application/ld+json")
    for i in locinfo:
        loccont = i.contents[0]
        locjson = json.loads(loccont)
        try:
            store_url = locjson['url']
            store_links.append(store_url)
        except:
            pass
# get address and geolocation information
stores = []
for store in store_links:
    storepage = requests.get(store)
    storesoup = BeautifulSoup(storepage.text, 'html.parser')
    storeinfo = storesoup.find_all(type="application/ld+json")
    for i in storeinfo:
        storecont = i.contents[0]
        storejson = json.loads(storecont)
        try:
            store_addr = storejson['address']
            store_addr.update(storejson['geo'])
            stores.append(store_addr)
        except:
            pass
# final data parsing
stores_df = df.from_records(stores)
stores_df.drop(['@type', 'addressCountry'], axis = 1, inplace = True)
stores_df['Store'] = "Family Dollar"
df.to_csv(stores_df, "family_dollar_locations.csv", sep = ",", index = False)
```
作者注释：本文改编自 2020 年 2 月 9 日在俄勒冈州波特兰的[我在 PyCascades 的演讲](https://2020.pycascades.com/talks/adventures-in-babysitting-webscraping-for-python-and-html-novices/)。
---
via: 
作者：[Julia Piaskowski](https://opensource.com/users/julia-piaskowski) 选题：[lujun9972](https://github.com/lujun9972) 译者：[stevenzdg988](https://github.com/stevenzdg988) 校对：[wxy](https://github.com/wxy)
本文由 [LCTT](https://github.com/LCTT/TranslateProject) 原创编译，[Linux中国](https://linux.cn/) 荣誉推出