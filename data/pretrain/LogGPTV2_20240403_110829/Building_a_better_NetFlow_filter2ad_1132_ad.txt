we will also need to perform the renormalization.
If we
set the renormalization threshold to T = 2M entries, when
we get there, the aim is to free M entries so we reduce
the sampling rate to at least half (if we have entries with a
packet count of more than 1 we will reduce it even more).
The ﬁrst renormalization is the critical one, as the others
happen when lower sampling rates are in eﬀect. During
renormalization we have to process 2M entries and in the
mean time we can receive up to M new packets so that the
number of entries created does not exceed the number of
entries cleared. With this conﬁguration we need to renor-
2These are the actual values we obtained by proﬁling our
code on a 750 MHz UltraSPARC-III which has performance
comparable to that of a processor one would expect on an
OC-48 line card.
malize two entries for every packet received. Since we de-
creased the sampling rate by at least a factor of two, we
receive at most one packet every 9.8µs. It takes 3.4µs to
process it and 2∗ 1.5µs for the normalization, so the proces-
sor is busy for 6.4µs, so it can keep up. Since we actually
have 3.4µs left for each packet we can set the threshold
lower than 2M and still keep up during the ﬁrst renormal-
ization. By writing down the constraints we get a simple
equation (tminT /M − tp)(T − M ) = T tr. Its solution gives
us the lowest threshold for which the ﬁrst normalization can
p
keep up with the rate at which the packets arrive: T /M =
α2 − 4tp/tmin)/2 where α = (tr + tp + tmin)/tmin.
(α +
For our example we get T = 1.56M and we need to perform
T /(T − M ) = 2.8 renormalizations for each sampled packet.
To account for randomness in the entry clearing decisions
√
M entries, so the
we need to add 3
number of entries for our example is 1.56M + 1.8
M .
p
M (T − M )/T = 1.8
√
√
ANF operates on time bins. At the end of the bin it
needs to process the ﬂow records collected during the bin
to bring their number down to M and then to buﬀer them
until they are transmitted to the collection station. Since
our tmin ≥ tp + tr, we can perform one renormalization on
the old entries for each packet in the new bin, so by the
time the number of entries in the new bin reaches T , we
already renormalized all of the old entries, so we need only
M more entries to support repeated bins. This brings our
total to 2.56M + 1.8
M entries. For example if the net-
work operator conﬁgures M = 64K records per minute, this
will generate a steady reporting traﬃc (NetFlow 7 ﬁts 27
ﬂow records into an 1500 byte packet) of 486 Kbps and it
will use 168,232 entries that take 10.3 MB of router memory
(NetFlow records use 64 bytes of memory). Note that reli-
able transfer of ﬂow records as advocated by IETF’s IPFIX
workgroup requires buﬀering of records so that they can be
retransmitted if packets are lost. The M entries for records
from the previous bin we included in the memory needs of
ANF are in fact a buﬀer and they can signiﬁcantly reduce
the amount of extra buﬀering needed in the router.
3. FLOW COUNTING EXTENSION
NetFlow entries record the SYN ﬂag which is set in the
ﬁrst packet of each TCP connection. Using this informa-
tion,
it is possible to accurately estimate the number of
active TCP ﬂows in various aggregates (e.g. web traﬃc,
traﬃc from Network A, etc.) even if NetFlow sees only a
sample of the packets [12]. We retain this functionality in
Adaptive NetFlow by setting the SYN ﬂag in the ﬂow entry
when a SYN packet is sampled and maintaining it with pro-
bability xnew/x when renormalization decreases the packet
counter of an entry from x to xnew. As with sampled Net-
Flow, Adaptive NetFlow data does not allow us to accu-
rately estimate the number of non-TCP ﬂows. To address
this shortcoming we propose an optional addition, the Flow
Counting Extension. FCE operates separately from ANF,
and provides its own traﬃc measurement data whose only
purpose is to support ﬂow counts. FCE has the same high
level properties as ANF: it can handle any traﬃc mix; it
generates a constant amount of measurement data for each
bin; it guarantees the relative standard deviation of the esti-
mates for aggregates above a certain percentage of the total
traﬃc; and the only conﬁguration parameter the operator
needs to set is the number of ﬂow records reported per bin.
Router line card
DRAM
Processor
Bus
Small buffer
1 in N
packet
headers
Forwarding
hardware
Additional
FCE
hardware
all packet
headers
Figure 8: The optional Flow Counting Extension re-
quires additional hardware. Since FCE needs to look
at each packet header, to keep up with line speeds
it must be implemented with additional hardware in
high speed routers.
As a starting point for the development of FCE, we use an
algorithm called “adaptive sampling”, proposed by Wegman
and described by Flajolet [17]. This algorithm solves the
database problem equivalent to estimating the total num-
ber of ﬂows, but we change it to support estimates for the
number of ﬂows of arbitrary aggregates (“slicing and dicing”
the traﬃc). We keep a table with all ﬂow identiﬁers seen in
the traﬃc along with a hash of the ﬂow ID. When the table
ﬁlls, we make space for new entries by deleting the entries
whose ﬂow ID hash does not start with at least one 0 bit.
Then, for each future packet, we insert its ﬂow ID into the
table only if its hash starts with one 0 bit (and it is not al-
ready in the table). When the table ﬁlls again, we keep only
the entries whose hashed ﬂow ID start with two 0s (increase
the “depth” to 2) and so on. The output of the original algo-
rithm is the estimate of the total number of ﬂows, computed
by multiplying the number of entries in the table by 2depth.
For example if we only keep the entries whose ﬂow ID hash
starts with two zeroes (depth = 2) we keep a quarter of all
ﬂow IDs, so if we report 1,000 web ﬂows, we estimate that
there were 4,000 web ﬂows in the traﬃc mix.
We modify the adaptive sampling algorithm to output the
current depth and the list of ﬂow IDs in the table. To esti-
mate the number of ﬂows from a certain aggregate we just
multiply the number of ﬂows from that aggregate present
in the output by 2depth. The problem with this algorithm
is that the number of entries it produces varies between 1
and 0.5 times the table size, and the accuracy of results
varies accordingly. To ensure that we report close to M
entries, we use a table of size 2M , so that after each in-
crease of depth, we have close to M entries. When the bin
ends, if the number of entries is L > M , we perform an
additional cleaning operation to bring the number of entries
close to M . During a bin, the ﬂows kept are those with
ﬂow ID hash h < H/2depth, where H is the maximum hash
value.
In the end-of-bin cleaning pass we keep only ﬂows
with h < H/2depth ∗ M/L, and we report together with the
ﬂow IDs a correction factor of N = 2depthL/M .
The function we use to hash ﬂow IDs is a randomly gen-
erated member of the H3 hash function family[7]. Because
of the randomness of this hash function, each ﬂow appears
in the output with a probability of 1/N and therefore the
p
estimated sizes of the aggregates are unbiased. Assuming
a perfect hash function, the sampling decisions for diﬀerent
ﬂows are independent. We can use this to show that the
M (1 − M/L) ≤ p
expected number of ﬂows in the output is M , with a small
M/2. We can
standard deviation of
also show that we can estimate the traﬃc of any aggregate
p
amounting to a fraction f of the total number of ﬂows with a
1/(M f ). Thus
relative standard deviation of very close to
FCE provides the same types of guarantees for the accuracy
of ﬂow counts as ANF does for the accuracy of packet counts.
The Flow Counting Extension to Adaptive NetFlow is
needed on high speed links, where we expect the traﬃc to be
sampled because the processor cannot process each packet.
At these speeds, since FCE needs to process each packet, we
can implement it only using additional hardware (Figure 8).
Computation of the hash function on the ﬂow IDs can be im-
plemented with combinatorial logic that is easy to pipeline.
The table of ﬂow IDs can be implemented with a CAM keyed
on the ﬂow IDs and the hash values. The CAM must sup-
port quick insertion and deletion of all entries matching a
mask. We use this hardware primitive to implement clean-
ing of the CAM when we increase the depth. At the end
of the interval, the ﬂow IDs and their hash values are read
from the CAM, and as the entries are read out the software
performs the ﬁnal cleaning to reduce the number of entries
to close to M . After a bin is ﬁnished, the processor reads
the entries of the CAM at slower than line speed, so we must
provide additional CAM memory to allow recording ﬂows of
the new current bin while the processor works on the previ-
ous bin. A conservative solution is to use two CAMs of size
2M so that the second CAM can operate on the packets of
the new bin while the ﬁrst one is being read out.
We want to ﬁnally note here that the cleaning operation
can be put to other uses too. Much like renormalization
for ANF, additional cleaning operations performed in soft-
ware at the router can produce smaller lists of ﬂow IDs.
Transmitting these progressively smaller summaries at pro-
gressively higher levels of reliability allows RLM-like [20]
graceful degradation of traﬃc report accuracy in response
to network congestion on the reporting path.
4. EXPERIMENTAL EVALUATION
In our experiments we use eight traces from various times
of the day from OC-48 links at two diﬀerent ISPs. The
traces are summarized in the technical report [14].
4.1 Evaluation of Adaptive NetFlow
The aim of the experimental evaluation of ANF in this
section is to compare the accuracy of its results with the
theoretical bounds and with NetFlow.
Results from the technical report [14] show that as we
increase the amount of memory the relative error of the ag-
gregates decreases as expected. In Figure 9 we present the
error in the estimates for all applications with more than
0.5% of the traﬃc, measured in packets, for report sizes of
8K and 256K entries. The plots show the 25th, 50th and
75th precentile over 25 runs. The byte estimates, omitted
for brevity, display very similar trends. The ﬁrst thing to
notice is that the actual errors are generally below the er-
rors predicted by Lemma 1, but more pronouncedly so for
the large report sizes. The reason for this is that Lemma 1
assumes that all entries have a packet counter of 1, which
is not true in either case, but the counters are lower for the
I
P
D
U
_
O
D
U
A
L
A
E
R
I
A
L
L
E
T
U
N
G
E
S
F
O
I
L
B
F
T
L
A
E
N
H
A
T
A
D
P
_
S
P
T
T
R
F
A
T
A
D
_
R
E
T
S
P
A
N
K
C
A
R
T
T
S
A
F
S
P
T
T
H
S
N
D
I
P
C
T
_
A
Y
D
E
K
E
N
M
P
O
T
_
M
S
D
M
S
E
P
D
U
d
e
i
f
i
s
s
a
c
n
U
l
P
T
N
N
P
C
T
d
e
i
f
i
s
s
a
c
n
U
l
P
T
T
H
8192 measured
8192 theoretical
262144 measured
262144 theoretical
L
O
R
T
N
O
C
_
P
T
F
L
Q
S
)
%
(
r
o
r
r
e
10
1
0.1
0.01
1
10
true percent of total packets
100
Figure 9: The error in estimating the number of
packets for applications with diﬀering amounts of
traﬃc, with two diﬀerent report sizes; vertical bars
show the 25th, 50th, and 75th percentile of standard
error over many runs. The straight lines show the
theoretical standard error for the worst case sce-
nario of all ﬂows having only 1 packet; when run
against a real traﬃc mix with larger ﬂows, ANF pro-
duces errors which are even lower. This is dramat-
ically illustrated by the very low error of estimates
for NNTP, which has very large ﬂows.
small report size. It is interesting to note that there are some
outliers such as NNTP for which the actual error is further
from the theoretical bound than for other applications. The
reason is the NNTP has the largest number of packets per
ﬂows over all applications and thus beneﬁts most from our
variance-reducing renormalization.
To verify that ANF’s renormalization does not introduce
bias or increase error, we compare ANF with a 64K entry
report size to binned NetFlow conﬁgured statically with the
sampling rate ANF stabilizes at by the end of the bin which
is a sampling 1 in 57 (we call it “Psychic NetFlow” because it
magically guesses the right sampling rate from the beginning
of the bin). This type of NetFlow obviously could not work
on a live link, as it requires a priori knowledge of the ideal
sampling rate, but we can run it on recorded traces. Tables
2(a) and 2(b) show the results of one such comparison for 25
runs of each algorithm over a single trace bin. For both bytes
and packets, these tables show that the renormalization in
ANF did not introduce bias or increase the error.
4.2 Evaluation of the Flow Counting Exten-
sion
The experimental evaluation of FCE in this section com-
pares the results of FCE with the theoretical bounds and
estimators cM1 and cM2 based on counting the SYN ﬂags
from NetFlow records [13].
Table 3 shows that even for some TCP applications, the
errors of FCE are much better than those for SYN based
estimators, while for others the errors are similar. This re-
ﬂects that the proportion of TCP ﬂows without SYN ﬂags
or with duplicated SYN packets diﬀers for diﬀerent applica-
Aggregate
ALL Traﬃc