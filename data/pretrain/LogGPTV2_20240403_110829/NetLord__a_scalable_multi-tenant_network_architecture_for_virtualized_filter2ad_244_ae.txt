### System Configuration and Topology

The testbed consists of 74 servers, each equipped with a CPU and 8GB of RAM. These servers are distributed across six edge switches, with the following distribution: 10, 12, 12, 13, 13, and 15 servers per switch. All switch and NIC ports operate at 1Gbps, and the switches can support up to 64K FIB (Forwarding Information Base) table entries.

### Network Topologies

We constructed two different topologies on this testbed:

1. **FatTree Topology**:
   - The entire testbed includes 16 edge switches.
   - We used an additional 8 switches to emulate 16 core switches, creating a two-level FatTree topology.
   - This topology ensures full bisection bandwidth, meaning there is no oversubscription.

2. **Clique Topology**:
   - All 16 edge switches are interconnected, forming a clique.
   - This topology is oversubscribed by a factor of up to 2:1.

### Path Identification and VLANs

SPAIN and NetLord leverage these multi-path topologies by using VLANs to identify paths:

- **For FatTree**: We utilized 16 VLANs, each rooted at one core switch.
- **For Clique**: We employed 6 VLANs, each rooted at one of the edge switches involved in our experiments.

### Emulation Results

We compared the performance of PLAIN, SPAIN, and NetLord. PLAIN does not support multi-pathing, and its traffic follows a single spanning tree. We conducted trials on both topologies with varying numbers of tenants (N), and for each N, we ran at least three trials and computed the means.

#### Figures 4(a) and 4(b)

- **Figure 4(a)**: Mean goodputs for the FatTree topology.
- **Figure 4(b)**: Mean goodputs for the Clique topology.
- The x-axis shows the number of tenants, and the number of emulated VMs in parentheses.
- Error bars indicate the maximum and minimum results.

### Key Observations

1. **PLAIN Scalability**:
   - As expected, PLAIN does not scale well. It fails to utilize the available bisection bandwidth.
   - Goodput drops significantly after the number of VMs exceeds the switch FIB table size (64K entries). For example, at 222K VMs, the drop is as much as 44%. At 74K VMs, the drop is only 3%, suggesting that the FIB table replacement policy might be LRU (Least Recently Used).

2. **SPAIN Performance**:
   - SPAIN outperforms PLAIN for fewer than 74K VMs by exploiting the multi-path fabrics.
   - However, SPAIN's performance degrades as the number of VMs increases. Above 74K VMs, SPAIN performs worse than PLAIN due to the need to maintain k distinct end-to-end paths between host pairs. This requires 74kN FIB table entries (in the worst case), leading to a significant drop in goodput even at modest numbers of VMs.

3. **NetLord Scalability**:
   - NetLord scales well, achieving superior goodput, especially as the number of VMs increases.
   - There is a slight decline in goodput when the number of VMs exceeds 14.8K, which we suspect is due to end-host overheads associated with maintaining many TCP connections. This hypothesis was validated by re-running the shuffle workload without VM emulation, using the PLAIN system, which showed a similar dip.

### Flooded Packets Analysis

- **Figure 5**: Shows the mean number of packet floods received at each host during the experiments.
- PLAIN and SPAIN suffer from significant flooding, supporting our belief that their scaling issues result from FIB-table capacity misses.
- NetLord experiences only a modest number of FIB misses, primarily due to table timeouts.

### Summary

Through experimental evaluation on a real testbed with a NetLord prototype, we have demonstrated that NetLord scales to several hundreds of thousands of VMs. Additionally, it works with unmodified commodity switches.

### Discussion and Future Work

#### Fault-Tolerance
- NetLord inherits SPAIN’s ability to handle transient network failures through monitoring and re-routing.
- Server failures are managed by a VM manager, which restarts affected VMs on other servers.

#### Broadcasts and Multicasts
- ARP and DHCP broadcasts are minimized by the NLAs proxying ARP requests.
- Future work should focus on scalable techniques for multi-tenant multicasting, such as rate-limiting or mapping broadcasts to tenant-specific multicast groups.

#### Per-Tenant Management
- NetLord allows per-tenant traffic management by exposing the tenant ID in the encapsulation's IP destination address.
- This enables the use of ACLs and rate-limiters, though limitations in the number of supported ACLs may require high-order bits of tenant IDs to encode service classes.

#### Inter-Tenant and External Communications
- NetLord currently requires tenant-implemented routing for external communications.
- Future work includes providing a per-tenant “distributed virtual NAT” service.

#### Software Overheads and SR-IOV
- Hypervisor-based network virtualization imposes substantial overheads, which can be mitigated by using SR-IOV NICs.
- Techniques for hypervisors to inject code into guest-domain drivers could solve outbound and inbound problems.

#### Other L2 Fabrics
- NetLord can use any fabric that provides an Ethernet abstraction, such as TRILL, SPB, or SEATTLE.
- NetLord might help scale these solutions by reducing FIB pressure.

### Conclusions

NetLord is a novel network architecture for multi-tenant cloud datacenters, offering full virtualization of L2 and L3 address spaces and scalability to large numbers of VMs. Our measurements show that NetLord scales to several thousand tenants and hundreds of thousands of VMs, achieving at least 3.9X improvement in goodput over existing approaches with negligible overheads.

### Acknowledgements

We thank Jon Crowcroft, anonymous reviewers, Sujata Banerjee, Anna Fischer, Rick McGeer, Jean Tourillhes, Ken Burden, Pete Haddad, and Eric Wu for their contributions and feedback.

### References

[References listed as provided in the original text]

This optimized version aims to improve clarity, coherence, and professionalism while maintaining the technical details and structure of the original text.