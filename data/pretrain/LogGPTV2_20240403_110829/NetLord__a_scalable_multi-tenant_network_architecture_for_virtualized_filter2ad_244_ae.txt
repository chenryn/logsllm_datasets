CPU and 8GB RAM. The 74 servers are distributed across 6 edge
switches, with 10, 12, 12, 13, 13, and 15 servers/switch. All switch
and NIC ports run at 1Gbps, and the switches can hold 64K FIB
table entries.
We built two different topologies on this testbed: (i) FatTree
topology: the entire testbed has 16 edge switches; using another 8
switches to emulate 16 core switches, we constructed a two-level
FatTree, with full bisection bandwidth (i.e., no oversubscription).
(ii) Clique topology: All 16 edge switches are connected to each
other, thus creating a clique, which is oversubscribed by at most
2:1.
SPAIN and NetLord exploit these multi-path topologies by using
VLANs to identify paths. For FatTree, we used 16 VLANs, each
rooted at one core switch. For Clique, we used 6 VLANs, each
rooted at one of the edge switches involved in our experiments.
5.3 Emulation results
As with the micro-benchmarks, we compared PLAIN, SPAIN,
and NetLord. PLAIN does not support multi-pathing, and so its
trafﬁc follows a single spanning tree. We ran trials on both of the
topologies with varying numbers of tenants (N), and for each N,
we ran at least three trials and computed the means.
Figures 4(a) and 4(b) show the mean goodputs for the FatTree
and Clique topologies, respectively. We include error bars show-
ing the maximum and minimum results. The x-axis legend shows
both the number of tenants, and the number of emulated VMs in
parentheses.
Figure 4 shows that:
(1) As expected, PLAIN does not scale. Aside from failing to
exploit the available bisection bandwidth, PLAIN’s goodput drops
after the number of VMs exceeds the switch FIB table size (64K
entries) – by as much as 44% for 222K VMs. (At 74K VMs, the
drop is only 3%, implying that the FIB table replacement policy
might be LRU.)
(2) SPAIN outperforms PLAIN for <74K VMs, since it does
exploit the multi-path fabrics.
 0 5 10 15 20 251(74)50(3.7K)100(7.4K)200(14.8K)500(37K)1000(74K)2000(148K)3000(222K)Goodput (in Gbps)Number of Tenants (VMs)PlainSPAINNetLordNo-VMs 0 2 4 6 8 101(74)200(14.8K)1000(74K)3000(222K)Goodput (in Gbps)Number of Tenants (VMs)PlainSPAINNetLordNo-VMs 0 5 10 15 20 25 301(74)50(3.7K)100(7.4K)200(14.8K)500(37K)1000(74K)2000(148K)3000(222K)Flooded Packets (in Millions)Number of Tenants (VMs)PlainSPAINNetLord 0 5 10 15 20 25 30 351(74)200(14.8K)1000(74K)3000(222K)Flooded Packets (in Millions)Number of Tenants (VMs)PlainSPAINNetLord71(3) However, SPAIN performance degrades badly as the num-
ber of VMs increases. Above 74K VMs, SPAIN actually per-
forms worse than PLAIN, because it maintains k distinct end-to-
end paths between host pairs. Thus, instead of requiring 74N FIB
table entries for N tenants, it requires 74kN entries (in the worst
case), because a switch maintains a FIB entry for each hMAC-
address,VLANi tuple. SPAIN’s goodput therefore drops even at
relatively modest numbers of VMs.
(4) NetLord scales well, achieving far superior goodput, especially
as the number of VMs increases.
We observe that NetLord’s goodput declines slightly as the num-
ber of VMs exceeds 14.8K. We suspect this dip is caused by end-
host overheads associated with maintaining lots of TCP connec-
tions. To validate this hypothesis, we re-ran the shufﬂe workload
without VM emulation, using the PLAIN system. The goodput we
achieved with this case (shown in Figure 4 s “No-VMs”, with the
number of connections matching the number of VMs) suffers from
a similar dip – that is, NetLord’s goodput closely matches the No-
VMs goodput. The one exception, for Clique and N = 1, might be
due to NetLord’s slightly higher overheads.
Why does NetLord scale better? Our main hypothesis in this
paper has been that PLAIN and SPAIN do not scale because of
FIB-table misses. Our switches do not allow us to directly count
these misses, so we use our counts of ﬂooded packets as a proxy.
Figure 5 shows the mean number of packet ﬂoods received at
each host during our experiments. (The error bars for this ﬁgure are
too narrow to be worth showing, and we omit them to avoid clutter).
Clearly, PLAIN and SPAIN suffer from lots of ﬂooding, supporting
our belief that their scaling problems result from FIB-table capac-
ity misses. Since NetLord conceals all VM and most server MAC
addresses from the switches, it experiences only a modest number
of FIB misses, caused mostly by table timeouts.
We note that NetLord suffers from more misses than PLAIN
when there is only one tenant, although the effect is too insignif-
icant to see in Figure 5 – for FatTree, for example, PLAIN ﬂoods
about 2,550 packets, SPAIN ﬂoods about 25,600, while NetLord
ﬂoods about 10,500 packets. In the one-tenant case, we see more
ﬂooded packets than we expect during the ﬁrst few connections.
This could be because our switches implement their MAC learn-
ing process in control-plane software, and some FIB table updates
might not complete within an RTT. However, the results are not
fully consistent with this hypothesis, and they require further in-
vestigation.
In summary, through experimental evaluation on a real testbed
with NetLord prototype, we have demonstrated that NetLord scales
to several hundreds of thousands of VMs. We have also shown that
it works with unmodiﬁed commodity switches.
6. DISCUSSION AND FUTURE WORK
In this section, we discuss how the NetLord architecture ad-
dresses several important considerations for operators of large data-
center networks. We also identify several limitations of NetLord,
and areas for future work.
Fault-tolerance: An important aspect of scale is the increased like-
lihood of failures. NetLord directly inherits SPAIN’s ability to han-
dle transient network failures. SPAIN monitors the health of its
VLAN-based paths by observing all incoming trafﬁc, or by sending
special chirp packets that serve (among other purposes) as heart-
beats. This monitoring allows SPAIN (and therefore NetLord) to
quickly detect failed paths, and to re-route load to alternate paths.
NetLord does not proactively monitor the health of the servers
themselves. NetLord tolerates server failures, because there is no
“hard state” associated with a speciﬁc NLA. We assume that a VM
manager (such as Eucalyptus) is responsible for detecting failed
servers and restarting affected VMs on other servers. When the
VMs are restarted, their local NLAs broadcast NLA-HERE mes-
sages, which repopulate the NL-ARP tables at the NLAs for other
VMs. It might be useful for the VM manager to provide this infor-
mation directly to the surviving NLAs, thereby avoiding some lost
packets during the restart phase.
Broadcasts and multicasts: ARP and DHCP account for a vast
majority of broadcasts in today’s typical datacenter. Because the
NLAs proxy ARP requests, these are never broadcast. We expect
the DHCP broadcast load to be similar to the NLA-HERE load,
which (in section 4.2) we argued is just a small percentage of net-
work capacity.
However, other tenant multicasts and broadcasts (particularly
from malicious, buggy, or inefﬁcient VMs) could become a prob-
lem. Scalable techniques for multi-tenant multicasting is a chal-
lenge for future work. Possible approaches include rate-limiting at
the switches, or mapping broadcasts to a tenant-speciﬁc multicast
group (as proposed in [11, 13]). But these techniques might not
scale, because most switches either treat multicasts as broadcasts,
or cannot scale the number of simultaneous multicast-tree pruning
sessions to match the number of tenants [23].
Per-tenant management: A cloud provider might wish to man-
age trafﬁc on a per-tenant basis: for example, to impose bandwidth
limits or priorities. While this can often be done in the hypervi-
sors [21], sometimes there might be reasons to exploit features of
switch hardware. Doing so requires a way for switches to identify
the packets of a speciﬁc tenant. If tenants can only be identiﬁed by
the addresses of their individual VMs, this cannot scale. However,
since NetLord exposes the tenant ID as part of the encapsulation’s
IP destination address, one can use a single ACL per tenant to con-
trol features such as rate limiting. This would probably still have
to be limited to a small subset of the tenants, given the number of
ACLs and rate-limiters supported by commodity switches. Or, the
high-order bits of tenant IDs could encode service classes.
Inter-tenant and external communications: Currently, NetLord
requires that the tenant implement some sort of routing within its
own network, if a VM on its private address space needs to be able
to contact the external world. We are currently working a few minor
modiﬁcations to our basic design, so that we can provide a per-
tenant “distributed virtual NAT” service.
Software overheads and SR-IOV: Although NetLord itself is fru-
gal with server resources, the use of hypervisor-based (or driver
domain-based) network virtualization imposes substantial over-
heads [27]. These overheads can be avoided by using Single Root
I/O Virtualization (SR-IOV) NICs, which allow VMs direct access
to NIC hardware. However, SR-IOV would appear to prevent the
NLA from intercepting and encapsulating outgoing packets, and
a standard SR-IOV NIC would not know how to decapsulate an
incoming NetLord packet. We believe that techniques allowing
hypervisors to inject code into the guest-domain drivers [9] could
solve the outbound problem; the inbound problem would either in-
volve the hypervisor on every received packet, or would require
some modiﬁcations to SR-IOV.
Other L2 fabrics: Although NetLord as presented in this paper uti-
lizes SPAIN as the underlying multipathing fabric, it is not closely
tied to SPAIN. NetLord can use any fabric that provides an Ethernet
abstraction such as TRILL [5], SPB [4], or SEATTLE [17]. Net-
Lord might help with scaling these other solutions. For instance,
72TRILL switches maintain a FIB entry for each destination MAC
address (but mapping the destination MAC to a destination edge
switch, rather than to a next-hop port), and so (without NetLord)
suffer from the same FIB-pressure problems as traditional switches.
SEATTLE addresses scaling for the core switches. However,
SEATTLE’s edge switches probably still need relatively large FIB
tables, to cache the working set of remote VM MAC addresses; oth-
erwise, table misses incur the signiﬁcant penalty of traversing the
SEATTLE DHT. It would be useful to have a quantiﬁed analysis of
how well SEATTLE scales in a virtualized environment.
7. CONCLUSIONS
In this paper we presented NetLord, a novel network architecture
for multi-tenant cloud datacenters.
Through the encapsulation of a tenant’s L2 (Ethernet) packets in
its own IP packets, NetLord gains multiple advantages over prior
solutions. It allows us to fully virtualize each tenant’s L2 and L3
address spaces, which gives the tenants full ﬂexibility in choosing
their VM addresses — or not choosing these addresses, if they want
to preserve their legacy addressing schemes. Encapsulation also al-
lows NetLord to scale to larger numbers of VMs than could other-
wise be efﬁciently supported using commodity Ethernet switches.
NetLord’s design can exploit commodity switches in a way that fa-
cilitates simple per-tenant trafﬁc management in the network. Our
new NL-ARP protocol simpliﬁes the design, by proactively push-
ing the location information of VMs to all servers. NetLord im-
proves ease of operation by only requiring static one-time conﬁgu-
ration that is fully automated.
Our measurements show that the NetLord architecture scales to
several thousands of tenants, and hundreds of thousands of VMs.
Our experimental evaluation shows that NetLord can achieve at
least 3.9X improvement in goodput over existing approaches, and
imposes only negligible overheads.
Acknowledgements
We thank our shepherd, Jon Crowcroft, and the anonymous review-
ers for their insightful comments. We thank Sujata Banerjee, Anna
Fischer, Rick McGeer, and Jean Tourillhes for their help and feed-
back. Finally, we thank Ken Burden, Pete Haddad, and Eric Wu for
their help in setting up the testbed network.
8. REFERENCES
[1] Amazon EC2. http://aws.amazon.com/ec2/.
[2] Amazon virtual private cloud.
http://aws.amazon.com/vpc/.
[3] IEEE 802.1ad - Provider Bridging. http:
//www.ieee802.org/1/pages/802.1ad.html.
[4] IEEE 802.1aq - Shortest Path Bridging. http:
//www.ieee802.org/1/pages/802.1aq.html.
[5] IETF TRILL Working Group. http://www.ietf.org/
html.charters/trill-charter.html.
[6] Windows Azure.
https://www.microsoft.com/windowsazure/.
[7] M. Al-Fares, A. Loukissas, and A. Vahdat. A Scalable,
Commodity Data Center Network Architecture. In Proc.
SIGCOMM, 2008.
[8] M. Arregoces and M. Portolani. Data Center Fundamentals.
Cisco Press, 2003.
[9] Broadcom Corp. Broadcom Ethernet Network Controller
Enhanced Virtualization Functionality.
http://tinyurl.com/4r8vxhh, Accessed on 1/31/11.
[10] Cisco Catalyst 4500 Series Model Comparison.
http://tinyurl.com/yvzglk, Accessed on 1/31/11.
[11] A. Edwards, A. Fischer, and A. Lain. Diverter: A New
Approach to Networking Within Virtualized Infrastructures.
In WREN 2009.
[12] Fulcrum MicroSystems. FocalPoint FM6000 ProductBrief.
http://tinyurl.com/4uqvale, Accessed on 1/31/11.
[13] A. Greenberg, J. Hamilton, and N. Jain. VL2: A Scalable and
Flexible Data Center Network. In Proc. SIGCOMM, 2009.
[14] C. Guo, G. Lu, H. J. Wang, S. Yang, C. Kong, P. Sun, W. Wu,
and Y. Zhang. SecondNet: A Data Center Network
Virtualization Architecture with Bandwidth Guarantees. In
Co-NEXT. ACM, 2010.
[15] M. Hajjat, X. Sun, Y.-W. E. Sung, D. Maltz, S. Rao,
K. Sripanidkulchai, and M. Tawarmalani. Cloudward Bound:
Planning for Beneﬁcial Migration of Enterprise Applications
to the Cloud. In Proc. SIGCOMM, 2010.
[16] IEEE 802.1q - Virtual Bridged Local Area Networks.
http://tinyurl.com/5ok4f6, Accessed on 1/31/11.
[17] C. Kim, M. Caesar, and J. Rexford. Floodless in SEATTLE:
A Scalable Ethernet Architecture for Large Enterprises. In
Proc. SIGCOMM, pages 3–14, 2008.
[18] J. Mudigonda, P. Yalagandula, M. Al-Fares, and J. C. Mogul.
SPAIN: COTS Data-Center Ethernet for Multipathing over
Arbitrary Topologies. In Proc. USENIX NSDI, 2010.
[19] R. N. Mysore, A. Pamboris, N. Farrington, N. Huang,
P. Miri, S. Radhakrishnan, V. Subramanya, and A. Vahdat.
PortLand: A Scalable Fault-Tolerant Layer 2 Data Center
Network Fabric. In Proc. SIGCOMM, 2009.
[20] HP ProCurve 6600 Series.
http://tinyurl.com/4cg8qt9, Accessed on
1/31/11.
[21] H. Rodrigues, J. R. Santos, Y. Turner, P. Soares, and
D. Guedes. Gatekeeper: Supporting Bandwidth Guarantees
for Multi-tenant Datacenter Networks. In Proc. 3rd
Workshop on I/O Virtualization, June 2011.
[22] M. Scott, A. Moore, and J. Crowcroft. Addressing the
Scalability of Ethernet with MOOSE. In Proc. DC CAVES
Workshop, Sept. 2009.
[23] R. Seifert and J. Edwards. The All-New Switch Book: The
Complete Guide to LAN Switching Technology. Wiley, 2008.
[24] R. Sherwood, G. Gibb, K.-K. Yap, G. Appenzeller,
M. Casado, N. McKeown, and G. Parulkar. Can the
Production Network be the Testbed? In Proc. of OSDI 2010.
[25] A. Shieh, S. Kandula, A. Greenberg, and C. Kim. Seawall:
Performance Isolation For Cloud Datacenter Networks.
HotCloud’10.
[26] VMware. http://www.vmware.com.
[27] P. Willmann, J. Shafer, D. Carr, A. Menon, S. Rixner, A. L.
Cox, and W. Zwaenepoel. Concurrent Direct Network
Access for Virtual Machine Monitors. In Proceedings of
HPCA, 2007.
[28] Xen. http://www.xen.org, Accessed on 1/31/11.
73