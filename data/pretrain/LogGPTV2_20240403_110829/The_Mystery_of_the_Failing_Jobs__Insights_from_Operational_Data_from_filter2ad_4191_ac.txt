Since resource usages can vary widely across jobs, for all
the analyses in this section, we ﬁrst deﬁne equal-sized bins
across the range of given resource usages. Jobs are grouped
based on the bin’s resource usage ranges and then the failure
rate of each bin is calculated as the fraction of jobs that failed
in that bin. For all analyses here, we only consider data points
or bins that have 100 or more jobs, in order to maintain the
statistical signiﬁcance of the results. Additionally, job count is
included on the right y-axis to indicate the relative conﬁdence
level of all the data points. Job count is equal to the number
of jobs considered while computing the failure rate of a given
bin. Higher the number of jobs, more accurate the failure rate
is. For resources such as local I/O, network I/O and network,
the monitoring tool collects the read (receive for network) and
write (transmit for network) data separately. We derive the total
I/O rate of a node by aggregating these read and write rates.
Since the rate range can vary anywhere from 0 to a very high
value (23GB/s and 1.1 TB/s for network ﬁle system I/O for
System A and System B respectively), we map these rates to a
log, base 10, scale.
We do two-sided, t-test based hypothesis testing for all
correlation results in this section. The null hypothesis is of the
form ”Job failure rate is not correlated with resource usage
of resource X”. So if the null hypothesis is rejected, then
we can conclude that resource usage of resource X is indeed
implicated with job failures. The results of all the hypothesis
tests are given in Table V. When the null hypothesis is rejected,
in some cases, the failure rate is positively correlated while
in others, it is negatively correlated. We remove plots for
inconclusive results to save space. Each plot has at the top
right a mark designating positive correlation (“+”), negative
correlation (“-”), or no statistically signiﬁcant correlation
(“0”). We do not present the analysis for multi-node shared
jobs for System A since their number is too small to draw
any statistically signiﬁcant conclusions. Wherever applicable,
we model the failure rate plot using the best-ﬁt statistical
distribution and report the R2 value. Even where R2 is low, if
the hypothesis testing is signiﬁcant, then the effect is validated.
Prediction Models: Each section also includes resource usage
prediction models for System A (combined results for both
shared and non-shared), where the measure being predicted is
the average resource usage per node during the lifetime of the
job. We are omitting the results for System B due to space
constraints. Similar to observations in prior works [68], [69],
[75]), jobs submitted by the same user tend to show strong
patterns. Accordingly, future resource usages for a certain
user’s job can be predicted by proﬁling previously submitted
jobs by that user. The prediction models presented in this
section basically are of 4 different kinds: (i) Last (L) - a na¨ıve
model which estimates the resource usage as the resource
usage of last ﬁnished job of a given user, (ii) Average (A)
- model which estimates the resource usage as the average of
resource usages of last n ﬁnished jobs of a given user, (iii)
Median (M) - model which estimates the resource usage as the
median of resource usages of last n ﬁnished jobs of a given
user, and (iv) Maximum Cosine Similarity (MCS) - model
which estimates the resource usage as the resource usage of
job which is most similar to the current job from the same
user. For cosine similarity computation, we use 5 different
attributes of a job such as jobname (1 if it is same as current
job’s jobname else 0), queue (1 if it is same as current job’s
queue else 0), number of nodes requested (normalized in the
range [0,1]), walltime requested (normalized in the range [0,1])
and difference in submit time (normalized in the range [0,1]).
We deﬁne history length as the number of last n ﬁnished
jobs of a user to consider while doing prediction. For the
last three kinds of models i.e., Average, Median and MCS,
we ﬁnd optimal history length on the training dataset (70% of
total) and present the results (refer Table VI) with that optimal
history length used in the model applied to the test dataset
(30% of total). For the results, the best history length is added
after the model name, thus M19 means the Median model with
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:28:14 UTC from IEEE Xplore.  Restrictions apply. 
162
TABLE V: Hypotheses results containing the Pearson correlation coefﬁcients and their
corresponding p-values. Null Hypothesis (α > 0.01): Failure rate is not correlated with
resource usage of resource X. All rejected null hypotheses are in either green or red.
Green represents positive correlation while red represents negative correlation
System A
non-shared
single
multi
shared
single
System B
non-shared
single
multi
Ref.
0.83, 1.7e-28
-0.41, 2.0e-4
-0.55, 3.3e-19
-0.31, 7.0e-7
-0.36, 9.8e-5
0.17, 0.4
0.12, 0.4
-0.57, 2.6e-11
0.11, 0.1
-0.25, 0.01
0.84, 7.2e-32
0.57, 3.2e-9
0.15, 0.2
0.42, 1.8e-7
0.40, 1.0e-9
-0.31, 1.1e-3
-
-0.07, 0.4
-0.21, 0.04
0.04, 0.6
-
0.13, 0.2
V-A
V-B
0.45, 2.6e-6 V-C
-0.56, 2.5e-9 V-D
0.42, 2.1e-5 V-E
H 3
H 1
0 : Memory
H 2
0 : Local I/O
0 : Network I/O
H 4
0 : Node-seconds
0 : Network
H 5
TABLE VI: Median absolute percentage er-
ror (MAPE) of resource usage and runtime
predictors for System A on the test set. Best
history length as determined on the training
set is given in parenthesis. Absolute percentage
error, AP E = Actual−P redicted
× 100.
Actual
Prediction
Memory
Local IO
Average
Last
3.0%
3.0% (1)
31.9% 33.1% (2)
Network IO 31.8% 31.8% (1)
17.7% 17.7% (1)
16.7% 16.7% (1)
Network
Runtime
Median
2.4% (19)
23.1% (19)
26.6% (17)
13.5% (17)
13.3% (9)
MCS
2.2% (20)
29.5% (10)
22.3% (13)
13.0% (11)
14.5% (16)
history length of 19.
A. Memory
1) Relation of job failure with memory usage:: Memory
related errors are common among failed jobs. In case of
memory, we know that exit code 137 corresponds to OOM
(out of memory) error. Therefore, we study the likelihood of
failure due to OOM error for different ranges of memory use.
Here the job failure rate is the fraction of jobs that fail with
OOM error. By memory use, we capture the entire memory
used on the node including use by system-level processes and
all user-level processes (corresponding to multiple user jobs if
being run in a shared environment). Fig. 2 shows the failure
distribution in non-shared environments while Table V shows
the results of the hypothesis testing.
We observe that the failure due to memory error distribution
is positively correlated with the tail memory usage of non-
shared single and shared single jobs (plot similar to Fig. 2a)
for System A as well as single jobs for System B. Recollect
that on System B, all jobs run in non-shared mode. While the
positive correlation is expected, it is quite surprising to see
the likelihood of failure increasing even when the available
memory is more than half of the total node memory capacity
for System A. System B exhibits the expected behavior where
the failure rate is ﬂat till close to node memory capacity and
then jumps to 1. Neither System A nor System B’s non-shared
multi node jobs exhibit any such positive correlation and due
to higher p-value the null hypothesis cannot be rejected.
There are two root causes for this OOM problem. First,
users sometime mistakenly provide upper bound for their
memory limit. Second, when some heavy memory usage
applications reach close to the upper bound, they go into a
“death spiral” whereby they cannot free memory while writing
out the memory to disk. This phenomenon has been reported
(a) System A non-shared single
(b) System B non-shared single
Fig. 2: Failure rate vs tail memory usages.
163
(a) Different models performance
with different history lengths on the
training set
(b) Percentage error distribution for
different models on test set with best
history length as per training set.
Fig. 3: Memory usage prediction for System A
previously with the OOM killer in Linux [20]. The least square
ﬁt in Fig. 2 corresponds to function f = 0 for x <= a else
f = b(x − a) where f is failure rate and x is memory usage
in GB. The best ﬁt curves have R2 value of 0.72 and 0.97 for
Fig. 2a and Fig. 2b.
Implications for System Design: Job failure rate caused by
OOM error increases with increasing tail-memory utilization,
but the increase starts to happen much earlier than the node
memory capacity in System A. Application of data mining
would help to determine when a job should be pre-empted
and moved to a larger node.
2) Prediction of memory usage based on user proﬁle:: We
have seen above that memory usage is positively correlated
with job failure rate. While one approach to avoid such
failure is to continuously monitor memory utilization and take
proactive action whenever memory utilization is high, a better
approach is to predict memory utilization of a job even before
it starts executing. Then a scheduler can decide in advance
where to schedule a job in case of heterogeneous memory
cluster or it can make a more informed decision on which
all jobs to schedule together on a node in case sharing is
enabled. Figure 3 shows the results of different predictors
(explained in the last paragraph of Sec V) for System A.
Here, jobs can be shared or non-shared. We observe that any
predictor performs equally well. Median absolute percentage
error (MAPE) of all predictors are less than 12% (for at least
one history length). Among these, Maximum Cosine Similarity
(MCS) outperforms others for any selected history length.
B. Local IO
1) Relation of job failure with local IO usage::
In this
section, we conducted an analysis to conclude if total local
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:28:14 UTC from IEEE Xplore.  Restrictions apply. 
IO on a node impacts job failure likelihood. Fig. 4 shows the
results for System A. System B has no local storage (always
uses NFS for IO). Fig. 4a corresponding to non-shared single
jobs for System A, shows an overall negative correlation, which
initially appears counter-intuitive. The explanation is that any
IO related issues usually restrict IO usage and hence jobs
fail with lower IO rate. On the other hand an IO heavy
job unimpeded by any system-related issues can perform the
required IO operations at much higher rate and completes
successfully. This analysis reveals that a good number jobs
are failing due to systems issues such as disk bottlenecks or
faulty drivers that restrict IO rate. We also observe a peak
in the failure rate around 6 MB/s which is much lower than
the speciﬁed IO limit of available local discs (100 MB/s)
for System A. The least square ﬁt in Fig. 4a corresponds to
function f = aebx where f is failure rate and x is resource
usage in log scale. For ﬁtted curve, a = 0.04, b = −0.48
with R2 value of 0.16. Even though we cannot reject the null
hypothesis for shared single jobs of System A, a peak in the
failure rate, similar to the one for non-shared single jobs, is
observed around 3 MB/s in Fig. 4b. We see a dip in the job
failure rate to the right end of the Fig. 4b. This higher IO
above the local IO operating limit is due to caching which
is counted in the IO rate by the performance stats. The high
failure rate around 6 MB/s for non-shared single and 3 MB/s
for shared single is because random access rates are usually
much slower (can be more than 100X slower [1], [2]) than
sequential access rates and such accesses are more common
in shared.
Implications for System Design: This analysis can be used to
identify system issues in the local IO. A pattern like the peak in
the job failure rate can help in estimating the operational IO
rate limit for local storage, which can be much smaller than
the rated capacity. Thus, while provisioning the IO subsystem,
one should consider the prevalence of random IO (rather than
sequential IO) in the applications of value and benchmark the
IO system to determine this lower rate for random IO.
2) Prediction of local IO usage based on user proﬁle::
The above analysis shows failure rate is high (peaks in Fig. 4)
with higher IO usage and the peak in case of shared occurs
earlier than non-shared. The difference in behavior between
shared and non-shared is primarily due to randomness in
access which is more common in case of shared. Hence, if
we can predict IO requirements of a job in advance, this
randomness can be minimized by scheduling only shared jobs
with complementary local IO requirements. So, we explore
user historical information based different local IO usage pre-
diction models. Table VI shows the performance of different
models on the test dataset. We observe the median predictor
with M AP E = 23.1%, outperforms others.
C. Network File System
1) Relation of job failure with remote IO usage:: This
section studies job failure correlation with total remote storage
usage rate. Fig. 5 shows the results for System A and System
B—the metric is the I/O rate per node. The least square curves
for System A correspond to function f = aebx where f is fail-
ure rate and x is resource usage in log scale. Hypothesis testing
gives statistically signiﬁcant negative correlation with respect
to total tail remote usage rate for non-shared single (Fig. 5a)
and non-shared multi jobs (ﬁgure similar as Fig. 5a) for System
A. These results can be explained by the fact that during
congestion for remote storage server or with poor network
connectivity between computational and storage nodes, jobs