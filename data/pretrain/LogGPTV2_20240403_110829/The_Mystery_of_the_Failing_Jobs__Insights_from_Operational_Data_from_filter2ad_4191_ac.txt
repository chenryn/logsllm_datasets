### Resource Usage and Job Failure Analysis

#### 1. Binning and Statistical Significance
Given the wide variability in resource usage across jobs, we first define equal-sized bins across the range of resource usages. Jobs are grouped based on these bins, and the failure rate for each bin is calculated as the fraction of jobs that failed within that bin. To ensure statistical significance, only bins with 100 or more jobs are considered. The job count is also included on the right y-axis to indicate the relative confidence level of the data points. A higher number of jobs in a bin generally leads to a more accurate failure rate.

For resources such as local I/O, network I/O, and network, the monitoring tool collects read (receive for network) and write (transmit for network) data separately. The total I/O rate of a node is derived by aggregating these read and write rates. Since the I/O rate can vary from 0 to very high values (e.g., 23 GB/s and 1.1 TB/s for network file system I/O in System A and System B, respectively), we use a logarithmic scale (base 10) to map these rates.

#### 2. Hypothesis Testing
We perform two-sided t-tests for all correlation results. The null hypothesis is that the job failure rate is not correlated with the resource usage of a specific resource (X). If the null hypothesis is rejected, we conclude that the resource usage of X is indeed associated with job failures. The results of all hypothesis tests are provided in Table V. When the null hypothesis is rejected, the correlation can be either positive or negative. Plots with inconclusive results are omitted to save space. Each plot includes a mark at the top right indicating positive correlation ("+"), negative correlation ("-"), or no statistically significant correlation ("0").

For System A, we do not present the analysis for multi-node shared jobs due to the small sample size, which is insufficient for drawing statistically significant conclusions. Where applicable, we model the failure rate plot using the best-fit statistical distribution and report the R² value. Even if the R² value is low, a significant hypothesis test result validates the effect.

#### 3. Prediction Models
Each section includes resource usage prediction models for System A (combined results for both shared and non-shared jobs). The measure being predicted is the average resource usage per node during the job's lifetime. Results for System B are omitted due to space constraints. Prior works [68], [69], [75] have shown that jobs submitted by the same user tend to exhibit strong patterns. Therefore, future resource usages for a user's job can be predicted by profiling previously submitted jobs by that user.

The prediction models presented are:
- **Last (L)**: A naive model that estimates resource usage based on the last finished job of a given user.
- **Average (A)**: A model that estimates resource usage as the average of the last n finished jobs of a given user.
- **Median (M)**: A model that estimates resource usage as the median of the last n finished jobs of a given user.
- **Maximum Cosine Similarity (MCS)**: A model that estimates resource usage based on the most similar job from the same user. Cosine similarity is computed using five job attributes: job name, queue, number of nodes requested, walltime requested, and difference in submit time.

The history length is defined as the number of last n finished jobs of a user to consider for prediction. For the Average, Median, and MCS models, we determine the optimal history length on the training dataset (70% of total) and present the results (Table VI) using this optimal history length on the test dataset (30% of total).

#### 4. Memory Usage and Job Failure
**A. Memory**
1. **Relation of Job Failure with Memory Usage:**
   Memory-related errors are common among failed jobs. Specifically, exit code 137 corresponds to an out-of-memory (OOM) error. We study the likelihood of OOM errors for different ranges of memory use. The job failure rate is the fraction of jobs that fail with an OOM error. This includes the entire memory used on the node, including system-level and user-level processes.

   Figure 2 shows the failure distribution in non-shared environments, while Table V presents the results of the hypothesis testing. We observe a positive correlation between the failure due to memory error and the tail memory usage of non-shared single and shared single jobs for System A, as well as single jobs for System B. Surprisingly, the likelihood of failure increases even when available memory is more than half of the total node memory capacity for System A. System B exhibits the expected behavior, where the failure rate remains flat until close to the node memory capacity and then jumps to 1. Neither System A nor System B’s non-shared multi-node jobs show such a positive correlation, and the null hypothesis cannot be rejected due to higher p-values.

   Two root causes for OOM problems are:
   - Users sometimes mistakenly provide an upper bound for their memory limit.
   - Heavy memory usage applications may enter a "death spiral" where they cannot free memory while writing it to disk, a phenomenon reported with the OOM killer in Linux [20].

   The least square fit in Figure 2a and Figure 2b corresponds to the function \( f = 0 \) for \( x \leq a \) else \( f = b(x - a) \), where \( f \) is the failure rate and \( x \) is the memory usage in GB. The best-fit curves have R² values of 0.72 and 0.97, respectively.

   **Implications for System Design:**
   - The job failure rate caused by OOM errors increases with increasing tail-memory utilization, but the increase starts much earlier than the node memory capacity in System A.
   - Data mining can help determine when a job should be preempted and moved to a larger node.

2. **Prediction of Memory Usage Based on User Profile:**
   We predict memory usage to avoid OOM errors. Figure 3 shows the results of different predictors for System A. Any predictor performs equally well, with a Median Absolute Percentage Error (MAPE) of less than 12% for at least one history length. The Maximum Cosine Similarity (MCS) model outperforms others for any selected history length.

#### 5. Local I/O Usage and Job Failure
**B. Local I/O**
1. **Relation of Job Failure with Local I/O Usage:**
   We analyze whether total local I/O on a node impacts job failure likelihood. Figure 4 shows the results for System A, while System B has no local storage and always uses NFS for I/O. For non-shared single jobs in System A, there is an overall negative correlation, which initially appears counterintuitive. This is because I/O-related issues usually restrict I/O usage, leading to job failures with lower I/O rates. Conversely, an I/O-heavy job unimpeded by system issues can complete successfully at higher I/O rates.

   The least square fit in Figure 4a corresponds to the function \( f = a e^{bx} \), where \( f \) is the failure rate and \( x \) is the resource usage in log scale. For the fitted curve, \( a = 0.04 \) and \( b = -0.48 \) with an R² value of 0.16. A peak in the failure rate around 6 MB/s is observed, which is much lower than the specified I/O limit of available local disks (100 MB/s) for System A. A similar peak is observed for shared single jobs around 3 MB/s, with a dip in the job failure rate at the higher end due to caching.

   **Implications for System Design:**
   - This analysis can identify system issues in local I/O.
   - The peak in the job failure rate can help estimate the operational I/O rate limit for local storage, which can be much smaller than the rated capacity.
   - While provisioning the I/O subsystem, one should consider the prevalence of random I/O and benchmark the system to determine the lower rate for random I/O.

2. **Prediction of Local I/O Usage Based on User Profile:**
   The median predictor with MAPE = 23.1% outperforms other models. Predicting I/O requirements in advance can minimize randomness by scheduling only shared jobs with complementary local I/O requirements.

#### 6. Network File System and Job Failure
**C. Network File System**
1. **Relation of Job Failure with Remote I/O Usage:**
   We study the correlation between job failure and total remote storage usage rate. Figure 5 shows the results for System A and System B, with the metric being the I/O rate per node. The least square curves for System A correspond to the function \( f = a e^{bx} \), where \( f \) is the failure rate and \( x \) is the resource usage in log scale.

   Hypothesis testing shows a statistically significant negative correlation with respect to total tail remote usage rate for non-shared single and non-shared multi jobs in System A. This can be explained by congestion for remote storage servers or poor network connectivity, leading to job failures.

This comprehensive analysis provides insights into the relationship between resource usage and job failure, and offers practical implications for system design and job scheduling.