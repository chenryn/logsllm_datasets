with several of these interfaces. To put the choice of topology into
perspective, we first introduce two fundamental factors affecting
all optical circuit-switched interconnects.
Degree. Unlike packet-switched networks, optical interconnects
are circuit-based. Hence, at any point in time, each node has a
limited number of active circuits, thereby limiting the number of
nodes it can communicate with directly. We refer to this as the
node degree. A topology with degree D means each node can
simultaneously maintain, at most, D circuits. Depending on the
traffic pattern, these circuits can be established with one to D
other nodes. Topologies with higher degree are suited for traffic
patterns with high fan-out, but they also tend to have a larger
cabling footprint.
Reconfiguration Latency. The reconfiguration latency puts a
lower bound on how long the circuits should be kept to achieve a
high duty cycle [66]. For a topology with reconfiguration latency r,
the circuit hold time should be longer than, for instance, 10×r to
achieve a 90% duty cycle.
There are various optical topologies that realize SiP-ML’s vision.
At one end of the spectrum are switch-based interconnects, such
as MEMS-based Optical Circuit Switch interconnects [24, 25, 55,
67, 68] and Rotor-based interconnects [66, 69]. On the other end lie
switch-free topologies such as ring [26, 70, 71], circulant graphs [72],
torus [73, 74], hypercube [75] and dragonfly interconnects [76–78].
In this paper, we consider two topologies at opposite ends of the
spectrum, as shown in Fig. 3. SiP-OCS is the first natural topology
choice because OCSs are commercially available today [79]. How-
ever, their reconfiguration latency is ≈10 ms, making them suitable
for circuits that last through the entire training. Fig. 3a illustrates
our SiP-OCS topology. SiP-OCS consists of Q optical switches, each
with N ports (the same as the number of GPUs), where each GPU
is connected to every OCS in a flat topology. Hence, in SiP-OCS,
the degree D is equal to the number of switches (Q).
As an alternate, extreme design point, we also investigate the
possibility of removing the switching elements entirely and evalu-
ate the performance of a minimalistic, switch-free topology called
SiP-Ring. In contrast to SiP-OCS, SiP-Ring reconfigures wavelengths
Figure 2: Comparing today’s ML cluster with SiP-ML.
capacity with standard electrical packet switching [55–58]. For in-
stance, realizing an electrical packet switch with 100 ports each
with 10 Tbps is extremely challenging. This is because the traffic
manager ASIC in the switch needs to process packets at 1000 Tbps
speed, but today’s ASICs can only process packets at 12.8 Tbps
speed. To get to 1000 Tbps switching, we need to build a “Clos”
of switching ASICs inside each electrical switch [59]. This is a
challenging undertaking.
At the same time, substantial progress is being made with Sil-
icon Photonics chiplets to bring optical interconnects very close
(essentially on die) to the ASICs. Recent advances in SiP fabrication
processes have created an opportunity to build chiplets with optical
I/O ports that can transmit data at far higher rates than electrical
conductors [9–17, 60–62]. With SiP interfaces, however, it is possi-
ble to build I/O interfaces integrated with electronics at 10 Tbps/mm
bandwidth (BW) density [14, 19, 20, 60, 63, 64]. Such integration
enables building next-generation computer architectures that are
fundamentally impossible with today’s technologies.
In this paper, we propose all-optical interconnects as an attrac-
tive solution to build the next generation of ML systems. We argue
that ML workloads present a unique opportunity to build special-
ized circuit-based interconnects. While conventional datacenter
workloads have unpredictable behavior, with short flows dominat-
ing the traffic, ML workloads are predictable, periodic, and consist
of mostly large transfers. Importantly, the parallelization algorithm
determines the circuit schedules, and the entire training repeats the
same communication pattern at every iteration. This unique char-
acteristic simplifies the control-plane logic with which datacenter
optical designs have grappled for years.
3 SIP-ML DESIGN
In this section, we introduce degree and reconfiguration latency as
fundamental factors affecting all optical circuit-based interconnects
(§3.1). We then discuss our parallelization algorithm, explaining
how it takes these factors into account to produce a suitable paral-
lelization strategy for a given topology (§3.2). Finally, we discuss
SiP-ML’s control plane and wavelength allocation (§3.3).
3.1 Degree and Reconfiguration Latency
Fig. 2 illustrates the differences between today’s ML training clus-
ters and SiP-ML. The state-of-the-art clusters have two bandwidth
659
{{ServerGbps domainGPUGPUGPUGPUGPUGPUGPUGPUTeraPHYGPUAll-optical topologiesTbps DomainSiP  portsNode{(a) Today’s ML clusters(b) SiP-ML clusterOCSTbps domainNode1Node2TeraPHYGPUTeraPHYGPUTeraPHYGPUTeraPHYGPUcounter clock-wise ringclock-wise ringNoden-1NodenNode1Node2Noden-1NodenOCSTeraPHYGPUTeraPHYGPUTeraPHYGPUTeraPHYGPUOCSOCS(a) SiP-OCS topology(b) SiP-Ring topologySIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
M. Khani et al.
within each port to achieve logically rich topologies. Reconfigura-
tion is done using Micro-ring resonators (MRRs) [21] embedded
in SiP ports [22, 23]. MRRs act as spectral filters to select and for-
ward wavelengths, and they enable the reuse of wavelengths across
non-overlapping segments of the ring (Fig. 13a in the appendix
illustrates an example). Our experiments show MRRs can switch
between different wavelengths within 25µs (§4.4). We discuss the
SiP-Ring design in more detail in Appendix A.1.
3.2 Degree-Aware Parallelization Strategy
A DNN can be viewed as a directed acyclic graph (DAG) of oper-
ations (ops). To parallelize a DNN training job, we need to decide
which GPU is responsible for running each op (or a part of each op).
As a simple example, to train a model with global batch size b using
DP on N GPUs, we break each op into N parallel sub-ops, each
operating on a local batch of size b/N (this is referred to as splitting
on the sample dimension [38]), and we map one sub-op to each
GPU. In general, MP follows similar steps: first partition each op
into parallel ops, then place the sub-ops. However, the partitioning
and placement decisions are not as straightforward as in DP.
Our parallelization algorithm takes the following as input: (i)
a DNN computation graph, Gin = (V , E), where V is the set of
operations (nodes) and E is the set of data dependencies (edges)
between the operations; (ii) the global batch size denoted by b;
(iii) a parameter k denoting the number of GPUs to partition the
model using MP; (iv) a parameter l denoting the number of GPUs to
partition the data using DP; and (v) the physical degree constraint
of the optical network topology, denoted by D. Our algorithm finds
a hybrid MP-DP strategy with k-way model parallelism and l-way
data parallelism for N = k ×l GPUs, such that the training iteration
time is minimized while satisfying the degree constraint (i.e., each
GPU communicates with no more than D other GPUs). We assume
all GPUs are identical.
The core of the algorithm determines an MP placement of the
DNN computation on k GPUs. Specifically, we begin by splitting the
GPUs into l groups, with k GPUs per group, and we divide the global
batch equally between the groups (i.e., each group is responsible
for a local batch of training data of size b/l). Then, we compute an
MP placement across k devices. We replicate the same placement
in each group to produce the final hybrid MP-DP strategy. Fig. 4
illustrates the key steps in our parallelization algorithm across 8
GPUs, with k = 4-way MP, l = 2-way DP, and degree constraint
D=3. We use this as a running example in the remainder of this
section.
(i) Partitioning. DNN training involves sequential stages of com-
putation, as dictated by the data dependencies in the computation
graph. For example, the graph in Fig. 4(a) has 4 sequential ops,
shown as rectangles of different colors. The size of each rectangle
represents the computation time of the op. The key to minimizing
training time is to balance the computation load across devices
at every stage of computation to maximize parallelism. Note that
balancing per-stage computation is not the same as balancing the
total load on each device. Sequentially-dependent ops cannot run
in parallel, hence placing them on the same device has no impact
on run-time compared to placing them on different devices, even
though it increases the total load on the device.
To minimize per-op run-time, it is desirable to split ops into
smaller pieces of computation. There are many ways to split an
op; for example, a 2D convolution can be split across height, width,
and channel dimensions [38]. However, in splitting ops, we must
take care not to compromise GPU utilization. GPUs (and other ML
accelerators) internally distribute an op over a massive number of
cores. If we split an op too finely, it will not have enough compute
intensity to utilize the cores effectively, and, therefore, we will
achieve no reduction in run-time from splitting. As a result, we
choose a minimum quantum of computation time, τ, and split ops
to sub-ops of a size near τ. We also cap the maximum number of
partitions for each op at k (the MP degree), as there is no point in
splitting beyond the maximum number of available parallel workers.
The result is a balanced computation graph whose vertices are the
sub-ops, as shown in Fig. 4(b) for our running example.
The right choice of the split dimension depends on the type of
the op and can impact the communication pattern between the
sub-ops. For example, in the case of a 2D convolution on an image
with multiple output channels, if we divide the op across the height
and width dimensions of the input, none of the sub-ops needs to
know the entire input image. However, if we split the op across the
output channel dimension, every sub-op needs a copy of the input
image, leading to a broadcast communication pattern with high
overhead. We select the most efficient dimension for each op. Since
we always split ops uniformly, sub-ops tend to communicate the
same amount of data with their descendants (the edges between the
sub-ops at each stage in Fig. 4(b) carry roughly the same amount
of traffic).
(ii) Placement. Next, we assign a GPU device to each op in the
balanced graph. Our placement aims to minimize the total run-
time while respecting the communication degree constraint D
required by the optical interconnect. Each GPU has two types of
communications: (i) it must communicate with some of the GPUs in
its MP group (depending on the op placement); (ii) given the hybrid
DP-MP strategy, there are l MP groups that need to synchronize
their parameters through DP. Hence, each GPU must communicate
with its counterparts in the other l MP groups to perform an all-
reduce operation to synchronize the model parameters across the
DP partitions. We use the ring-allreduce [29, 30] algorithm for
this step. This requires a ring communication pattern between
corresponding GPUs in the MP groups, which requires each GPU
to send data to one GPU in another group. Therefore a GPU can
communicate with, at most, ∆ = D - 1 other GPUs within its own
MP group to meet the overall degree constraint.
We now present a heuristic algorithm for placing ops within
an MP group to minimize run-time with a constraint ∆ on the
degree of communication. While this problem can be written as
an Integer Linear Problem (ILP), it is prohibitive to solve this ILP
given the scale of the balanced computation graph (e.g., over 20K
sub-ops for the Transformer DNN model). Algorithm 1 provides
the pseudocode.
The key strategy in our algorithm is to map GPU devices into a
metric space and transform the degree constraint into a distance
constraint in that space. We select an arbitrary ordering of GPU
devices and place ops to maintain a maximum communication
distance of ∆; i.e., devices i and j are allowed to communicate only if
(i−j) mod k ≤ ∆. This constraint leads to a sparse diagonal traffic
660
SiP-ML: Optical Network Interconnects for Machine Learning
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
Figure 4: An example of SiP-ML’s parallelization strategy with k = 4, l = 2, D=3, and ∆ = 2.
Algorithm 1
Task Placement with a Communication Degree Constraint
1: Input: Balanced compute graph g_in, computation quantum τ , degree
constraint ∆, local_batchsize b /l, mp_degree k
for sub_op in par_ops_map[op] do
2: Output: A task graph g_out with placed ops
3: for op in g_in.topological_sort( ) do
4:
5:
6:
7:
8:
9:
far_id←farthest sub_op’s predecessor device id
near_id←nearest sub_op’s predecessor device id
range_lo←near_id
range_hi←far_id + ∆
sub_op.device←get_earliest_avail(avail_times,
cand_start←latest end time of predecessors
start←max(cand_start, avail_time[sub_op.device])
end←start + sub_op.duration
avail_time[sub_op.device]←end
10:
11:
12:
13:
14:
15: end for
16: g_out ← add_network_ops(g_out)
end for
range_lo, range_hi, sub_op.mem_size)
that meets a maximal set of range constraints. We then reallocate
the remaining parents that violate the constraint into the nearest
device that meets the distance constraint with x. As this may create
distance violations between parents and grandparents of x, we
continue this backward process until all previously placed ops meet
the distance constraint with their parents. We then restart a forward
pass from the first located op and verify the distance constraints
between the placed ops and their children. If any violations have
occurred due to reallocation, we relocate the child op. This forward-
backward procedure is repeated until all ops are placed. We leave
the convergence proof to future work.
Fig. 4(c) shows the MP placement for our running example, with
∆ = 2. Notice two properties of this placement: (i) each GPU com-
municates with, at most, ∆ = 2 other GPUs, as required, and (ii)
the sub-ops of each op are balanced well across the 4 GPUs. In fact,
the only op that is not perfectly balanced is C, but the 4 sub-ops
of this op cannot be placed on all 4 GPUs without violating the
communication degree constraint, because whichever GPU op B
resides on would then need to communicate with the other 3 GPUs.
Putting it all together. Fig. 4(d) shows the final hybrid MP-DP
placement for our example. As mentioned earlier, it is created simply
by replicating the MP placement in the l = 2 GPU groups. As for
the communication pattern, each GPU communicates with, at most,
∆ = 2 other GPUs in its MP group and one more GPU for the
ring topology required for the DP all-reduce step. For example, in
Fig. 4(d), GPU 1 must communicate with GPUs 2 and 3 for MP and
GPU 5 for DP. Our parallelization algorithm takes the degree of
MP and DP (k and l) as input, but it is trivial to optimize over these
parameters to find the combination that minimizes training time
for a given number of GPUs, as discussed in Appendix 4.2.
3.3 Circuit Scheduling
Given that our SiP-OCS topology reconfigures its circuits only
once at the beginning of the training job, its control plane logic is
simple. In this case, the main task is to compute the total traffic
matrix resulting from the parallelization algorithm and then assign
circuits to each pair of GPUs that must communicate, such that
the maximum transfer time is minimized. We determine the circuit
assignment with a simple ILP run once for each training job (details
in §A.2).
The control plane for the SiP-Ring topology is more challeng-
ing, as circuits can be reconfigured during training. Hence, our
controller needs to estimate the traffic and reschedule the circuits
periodically. Therefore, every GPU’s host needs to read its GPU
transfer buffer counters through PCIe and communicate them to a
matrix with zeros outside a ∆ distance from the main diagonal,
satisfying the communication degree constraint.
The algorithm begins with a topological sort of the balanced
computation graph (shown in Fig. 4(b) for our example), such that
each sub-op appears in the sorted list after its dependencies. It
places the sub-ops in this sorted order, guaranteeing that when a
sub-op is placed, all of its dependencies have already been placed.
For each sub-op, the algorithm first computes a set of placement
candidates. These are the devices where the sub-op can be placed
without violating the distance constraint mentioned above. We
compute the intersection of these ranges for all parents of x to