via standard Byzantine agreement protocols [18].
5 Implementation
This section describes our prototype implementation of
a SANE network. Our implementation consists of a DC,
switches, and IP proxies.
It does not support multiple
DCs, there is no support for tolerating malicious switches
nor were any of the end-hosts instrumented to issue re-
vocations.
All development was done in C++ using the Virtual
Network System (VNS) [17]. VNS provides the abil-
ity to run processes within user-speciﬁed topologies, al-
lowing us to test multiple varied and complex network
topologies while interfacing with other hosts on the net-
work. Working outside the kernel provided us with a
ﬂexible development, debug, and execution environment.
The network was in operational use within our group
LAN—interconnecting seven physical hosts on 100 Mb
Ethernet used daily as workstations—for one month. The
only modiﬁcation needed for workstations was to re-
duce the maximum transmission unit (MTU) size to 1300
bytes in order to provide room for SANE headers.
5.1 IP Proxies and SANE Switches
To support unmodiﬁed end hosts on our prototype net-
work, we developed proxy elements which are posi-
tioned between hosts and the ﬁrst hop switches. Our
proxies use ARP cache poisoning to redirect all traf-
ﬁc from the end hosts. Capabilities for each ﬂow are
cached at the corresponding proxies, which insert them
into packets from the end host and remove them from
packets to the end host.
Our switch implementation supports automatic neigh-
bor discovery, MST construction, link-state updates and
packet forwarding. Switches exchange HELLO mes-
sages every 15 seconds with their neighbors. Whenever
switches detects network failures, they reconﬁgure their
MST and update the DC’s network map.
The only dynamic state maintained on each switch
is a hash table of capability revocations, containing the
Cap-IDs and their associated expiration times.
We use OCB-AES [42] for capability construction and
decryption with 128-bit keys. OCB provides both conﬁ-
dentiality and data integrity using a single pass over the
data, while generating ciphertext that is exactly only 8
bytes longer than the input plaintext.
5.2 Domain Controller
The DC consists of four separate modules: the authen-
tication service, the network service directory, and the
topology and capability construction service in the Pro-
tection Layer Controller. For authentication purposes,
the DC was preconﬁgured with the public keys of all
switches.
Capability construction. For end-to-end path calcula-
tions when constructing capabilities, we use a bidirec-
tional search from both the source and destination. All
computed routes are cached at the DC to speed up subse-
quent capability requests for the same pair of end hosts,
although cached routes are checked against the current
topology to ensure freshness before re-use.
Capabilities use 8-bit IDs to denote the incoming and
outgoing switch ports. Switch IDs are 32 bits and the
service IDs are 16 bits. The innermost layer of the capa-
bility requires 24 bytes, while each additional layer uses
14 bytes. The longest path on our test topologies was 10
switches in length, resulting in a 164 byte header.
Service Directory. DNS queries for all unauthenticated
users on our network resolve to the DC’s IP address,
which hosts a simple webserver. We provide a basic
HTTP interface to the service directory. Through a web
browser, users can log in via a simple web-form and can
then browse the service directory or, with the appropriate
permissions, perform other operations (such as adding
and deleting services).
The directory service also provides an interface for
managing users and groups. Non-administrative users
are able to create their own groups and use them in
access-control declarations.
To access a service, a client browses the directory
tree for the desired service, each of which is listed as
a link.
If a service is selected, the directory server
checks the user’s permissions. If successful, the DC gen-
erates capabilities for the ﬂows and sends them to the
client (or more accurately, the client’s SANE IP proxy).
The web-server returns an HTTP redirect to the ser-
vice’s appropriate protocol and network address, e.g.,
ssh://192.168.1.1:22/. The client’s browser
can then launch the appropriate application if one such
146
Security ’06: 15th USENIX Security Symposium
USENIX Association
1
is registered; otherwise, the user must do so by hand.
5.3 Example Operation
As a concrete example, we describe the events for an ssh
session initiated within our internal network. All par-
ticipating end hosts have a translation proxy positioned
between them and the rest of the network. Additionally,
they are conﬁgured so that the DC acts as their default
DNS server.
Until a user has logged in, the translation proxy returns
the DC’s IP address for all DNS queries and forwards all
TCP packets sent to port 80 to the DC. Users opening
a web-browser are therefore automatically forwarded to
the DC so that they may log in. This is similar in feel
to admission control systems employed by hotels and
wireless access points. All packets forwarded to the DC
are accompanied by a SANE header which is added by
the translation proxy. Once a user has authenticated, the
DC caches the user’s location (derived from the SANE
header of the authentication packets) and associates all
subsequent packets from that location with the user.
Suppose a user ssh’s from machine A to machine B.
A will issue a DNS request for B. The translation proxy
will intercept the DNS packet (after forging an ARP re-
ply) and translate the DNS requests to a capability re-
quest for machine B. Because the the DNS name does
not contain an indication of the service, by convention
we prepend the service name to the beginning of the DNS
request (e.g. ssh ssh.B.stanford.edu). The DC does the
permission check based on the capability (initially added
by the translation proxy) and the ACL of the requested
service.
If the permission check is successful,
the DC re-
turns the capabilities for the client and server, which are
cached at the translation proxy. The translation proxy
then sends a DNS reply to A with a unique destination
IP address d, which allows it to demultiplex subsequent
packets. Subsequently, when the translation proxy re-
ceives packets from A destined to d, it changes d to the
destination’s true IP address (much like a NAT) and tags
the packet with the appropriate SANE capability. Addi-
tionally, the translation proxy piggybacks the return ca-
pability destined for the server’s translation proxy on the
ﬁrst packet. Return trafﬁc from the server to the client is
handled similarly.
6 Evaluation
We now analyze the practical implications of running
SANE on a real network. First, we study the perfor-
mance of our software implementation of the DC and
switches. Next, we use packets traces collected from
a medium-sized network to address scalability concerns
and to evaluate the need for DC replication.
6.1 Microbenchmarks
Table 1 shows the performance of the DC (in capabilities
per second) and switches (in Mb/s) for different capabil-
ity packet sizes (i.e., varying average path lengths). All
tests were done on a commodity 2.3GHz PC.
As we show in the next section, our naive implementa-
tion of the DC performs orders of magnitude better than
is necessary to handle request trafﬁc in a medium-sized
enterprise.
The software switches are able to saturate the 100Mb/s
network on which we tested them. For larger capabil-
ity sizes, however, they were computationally-bound by
decryption—99% of CPU time was spent on decryption
alone—leading to poor throughput performance. This
is largely due to the use of an unoptimized encryption
library.
In practice, SANE switches would be imple-
mented in hardware. We note that modern switches,
such as Cisco’s catalyst 6K family, can perform MAC
level encryption at 10Gb/s. We are in the process of re-
implementing SANE switches in hardware.
6.2 Scalability
One potential concern with SANE’s design is the central-
ization of function at the Domain Controller. As we dis-
cuss in Section 3.5, the DC can easily be physically repli-
cated. Here, we seek to understand the extent to which
replication would be necessary for a medium-sized enter-
prise environment, basing on conclusions on trafﬁc traces
collected at the Lawrence Berkeley National Laboratory
(LBL) [36].
The traces were collected over a 34-hour period in
January 2005, and cover about 8,000 internal addresses.
The trace’s anonymization techniques [37] ensure that
(1) there is an isomorphic mapping between hosts’ real
IP addresses and the published anonymized addresses,
and (2) real port numbers are preserved, allowing us to
identify the application-level protocols of many packets.
The trace contains almost 47 million packets, which in-
cludes 20,849 DNS requests and 145,577 TCP connec-
tions.
Figure 6 demonstrates the DNS request rate, TCP con-
nection establishment rate, and the maximum number of
concurrent TCP connections per second, respectively.
The DNS and TCP request rates provide an estimate
for an expected rate of DC requests by end hosts in a
SANE network. The DNS rate provides a lower-bound
that takes client-side caching into effect, akin to SANE
end hosts multiplexing multiple ﬂows using a single ca-
pability, while the TCP rate provides an upper bound.
USENIX Association
Security ’06: 15th USENIX Security Symposium
147
1
DC
switch
5 hops
100,000 cap/s
762 Mb/s
10 hops
40,000 cap/s
480 Mb/s
15 hops
20,000 cap/s
250 Mb/s
Table 1: Performance of a DC and switches
Figure 6: DNS requests, TCP connection establishment requests, and maximum concurrent TCP connections per
second, respectively, for the LBL enterprise network.
Even for this upper bound, we found that the peak rate
was fewer than 200 requests per second, which is 200
times lower than what our unoptimized DC implementa-
tion can handle (see Table 1).
Next, we look at what might happen upon a link fail-
ure, whereby all end hosts communicating over the failed
link simultaneously contact the DC to establish a new
capability. To understand this, we calculated the maxi-
mum concurrent number of TCP connections in the LBL
network.9 We ﬁnd that the dataset has a maximum of
1,111 concurrent connections, while the median is only
27 connections. Assuming the worst-case link failure—
whereby all connections traverse the same network link
which fails—our simple DC can still manage 40 times
more requests.
Based on the above measurements, we estimate the
bandwidth consumption of control trafﬁc on a SANE net-
work. In the worst case, assuming no link failure, 200
requests per second are sent to the DC. We assume all
ﬂows are long-lived, and that refreshes are sent every 10
minutes. With 1,111 concurrent connections in the worst
case, capability refresh requests result in at most an ad-
ditional 2 packets/s.10 Given header sizes in our proto-
type implementation and assuming the longest path on
the network to be 10 hops, packets carrying the forward
and return capabilities will be at most 0.4 KB in size,
resulting in a maximum of 0.646 Mb/s of control trafﬁc.
This analysis of an enterprise network demonstrates
that only a few domain controllers are necessary to han-
dle DC requests from tens of thousands of end hosts. In
fact, DC replication is probably more relevant to ensure
uninterrupted service in the face of potential DC failures.
7 Related Work
Network Protection Mechanisms. Firewalls have been
the cornerstone of enterprise security for many years.
However,
their use is largely restricted to enforcing
coarse-grain network perimeters [45]. Even in this lim-
ited role, misconﬁguration has been a persistent prob-
lem [46, 47]. This can be attributed to several factors
which SANE tries to address; in particular, their low-
level policy speciﬁcation and very localized view leaves
ﬁrewalls highly sensitive to changes in topology. A vari-
ety of efforts have examined less error prone methods for
policy speciﬁcation [13], as well as how to detect policy
errors automatically [33].
The desire for a mechanism that supports ubiquitous
enforcement, topology independence, centralized man-
agement, and meaningful end-point identiﬁers has lead
to the development of distributed ﬁrewalls [14, 26, 2].
Distributed ﬁrewalls share much with SANE in their ini-
tial motivation but differ substantially in their trust and
usage model. First, they require that some software be
installed on the end host. This can be beneﬁcial as it pro-
vides greater visibility into end host behavior, however,
it comes at the cost of convenience. More importantly,
for end hosts to perform enforcement, that end host must
be trusted (or at least some part of it, e.g., the OS [26], a
VMM [22], the NIC [31], or some small peripheral [40]).
Furthermore, in a distributed ﬁrewall scenario, the net-
work infrastructure itself receives no protection, i.e., the
network is still “on” by default. This design affords no
defense-in-depth if the end-point ﬁrewall is bypassed, as
it leaves all other network elements (e.g., switches, mid-
dleboxes, and unprotected end hosts) exposed.
148
Security ’06: 15th USENIX Security Symposium
USENIX Association
1
Weaver et al. [45] argue that existing conﬁgura-
tions of coarse-grain network perimeters (e.g., NIDS
and multiple ﬁrewalls) and end host protective mech-
anisms (e.g. anti-virus software) are ineffective against
worms, both when employed individually or in combi-
nation. They advocate augmenting traditional coarse-
grain perimeters with ﬁne-grain protection mechanisms
throughout the network, especially to detect and halt
worm propagation.
Finally, commercial offerings from Consentry [3] in-
troduce special-purpose bridges for enforcing access
control policy. To our knowledge, these solutions re-
quire that the bridges be placed at a choke point in the
network so that all trafﬁc needing enforcement passes
through them. In contrast, SANE permission checking is
done at a central point only on connection setup, decou-
pling it from the data path. SANE’s design both allows
redundancy in the network without undermining network
security policy and simpliﬁes the forwarding elements.
Dealing with Routing Complexity. Often misconﬁg-
ured routers make ﬁrewalls simply irrelevant by routing
around them. The inability to reason about connectivity
in complex enterprise networks has fueled commercial
offerings such as those of Lumeta [5], to help adminis-
trators discover what connectivity exists in their network.
In their 4D architecture, Rexford et al. [41, 24] ar-
gue that the decentralized routing policy, access control,
and management has resulted in complex routers and