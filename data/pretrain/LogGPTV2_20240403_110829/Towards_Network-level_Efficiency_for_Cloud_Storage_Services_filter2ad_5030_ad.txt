 0 0.2 0.4 0.6 0.8 1 1.21K10K100K1MSync Traffic (MB)Size of the Modified File (Bytes)(a) PC clientGoogle DriveOneDriveDropboxBoxUbuntu OneSugarSync 0 0.2 0.4 0.6 0.8 1 1.21K10K100K1MSync Traffic (MB)Size of the Modified File (Bytes)(b) Web-basedGoogle DriveOneDriveDropboxBoxUbuntu OneSugarSync 0 0.2 0.4 0.6 0.8 1 1.21K10K100K1MSync Traffic (MB)Size of the Modified File (Bytes)(c) Mobile appGoogle DriveOneDriveDropboxBoxUbuntu OneSugarSync121Table 8: Sync trafﬁc of a 10-MB text ﬁle creation. UP: The user
uploads the ﬁle to the cloud. DN: The user downloads the ﬁle
from the cloud.
Service
Google Drive
OneDrive
Dropbox
Box
Ubuntu One
SugarSync
Sync trafﬁc (MB)
PC client Web-based Mobile app
UP DN
UP DN
11.3
11.0
11.8
10.8
10.7
12.2
11.2
11.4
5.5
8.1
5.5
6.1
11.1
10.4
11.2
10.6
8.6
5.3
5.6
10.6
11.3
11.5
11.6
11.8
UP DN
10.6
11.7
11.0
11.0
5.5
10.6
11.3
10.5
5.3
10.9
10.4
10.7
5. COMPRESSION AND DEDUPLICATION
In a real-world storage system, compression and deduplication
are the two most commonly used techniques for saving space and
trafﬁc. This section makes a detailed study of cloud storage com-
pression and deduplication, from both the system designers’ and
users’ perspectives.
5.1 Data Compression Level
[Experiment 4] : To study whether data updates are compressed
before they are synchronized to the cloud, we create an X-byte tex-
t ﬁle inside the sync folder. As a small ﬁle is hard to compress,
we experiment with X = 1 M, 10 M, 100 M, and 1 G. Each tex-
t ﬁle is ﬁlled with random English words. If data compression is
actually used, the resulting sync trafﬁc should be much less than
the original ﬁle size. Furthermore, after each text ﬁle is complete-
ly synchronized, we download it from the cloud with a PC client,
a web browser, and a mobile app, respectively, so as to examine
whether the cloud delivers data updates in a compressed form.
As a typical case, the Experiment 4 results corresponding to
a 10-MB text ﬁle are listed in Table 8. First, in the ﬁle upload
(UP) phase, among the six mainstream cloud storage services, on-
ly Dropbox and Ubuntu One compress data with PC clients and
mobile apps. No service ever compresses data with web browser-
s. Further, we observe that the 10-MB text ﬁle can be compressed
to nearly 4.5 MB using the highest-level WinZip compression on
our desktop. Thus, for Dropbox and Ubuntu One, the compression
level with PC clients seems moderate, while the compression level
with mobile apps is quite low. The motivation of such a difference
is intuitive: to reduce the battery consumption of mobile devices
caused by the computation-intensive data compressions.
Next, in the ﬁle download (DN) phase, only Dropbox and Ubun-
tu One compress data with PC clients and web browsers, and the
compression level is higher than that in the ﬁle upload phase. For
mobile apps, only Dropbox compresses data.
By analyzing our collected cloud storage trace, we ﬁnd that 52%
of ﬁles can be effectively compressed. Here “effectively compressed”
implies that Compressed ﬁle size
Original ﬁle size  B) then
Set B1 as the upper bound: U ← B1, and decrease the
guessing value of B1: B1 ← L+U
case 2: T r2 > 2B1 (implying that B1 < B) then
Set B1 as the lower bound: L ← B1, and increase the
guessing value of B1: B1 ← L+U
2
14:
15:
16:
;
;
2
17: goto Step 1;
compress data, while Dropbox is the only service that com-
presses data for every access method.
• Web browser typically does not compress a ﬁle when upload-
ing it to the cloud storage, probably also due to the limitation-
s of JavaScript or other web-based script languages (refer to
§ 4.3). Besides, using a mobile app is usually not as efﬁcient
as using a PC client.
5.2 Data Deduplication Granularity
[Experiment 5] : Data deduplication is another potential method
to reduce data sync trafﬁc, with the intuition that users often upload
duplicated ﬁles with similar content. Inferring the deduplication
granularity of a cloud storage service requires some efforts, espe-
cially when the deduplication block size B (bytes) is not a power
of two (i.e., B (cid:54)= 2n, where n is a positive integer) 5. To measure
the deduplication granularity, we design and implement Algorith-
m 1 (named the “Iterative Self Duplication Algorithm”). It infers
the deduplication granularity by iteratively duplicating and upload-
ing one or multiple synthetic ﬁle(s) and meanwhile analyzing the
incurred data sync trafﬁc. It is easy to prove that the iteration pro-
cedure can ﬁnish in O(log(B)) rounds.
First, we study inter-ﬁle data deduplication with respect to an
identical user account. By applying Experiment 5 to the six main-
stream cloud storage services, we ﬁgure out their data deduplica-
tion granularity in Table 9 (the 2nd column). In this table, “Full
ﬁle” (only for Ubuntu One) means that data deduplication only hap-
pens at the full-ﬁle level, “4 MB” (only for Dropbox) indicates that
the deduplication block size B = 4 MB, and “No” shows that there
is no deduplication performed. Note that block-level deduplication
naturally implies full-ﬁle deduplication, but not vice versa.
Second, we study cross-user data deduplication. For each cloud
storage service, we ﬁrst upload a ﬁle f to the cloud, and then
5The deduplication block size B (bytes) is traditionally a power of
two [39], but we still have to thoughtfully consider the exception
when B (cid:54)= 2n .
122Table 9: Data deduplication granularity. We do not list the
web-based case because the web-based ﬁle synchronization typ-
ically does not apply data deduplication.
Service
Google Drive
OneDrive
Dropbox
Box
Ubuntu One
SugarSync
Same user
PC client & Mobile app
Cross users
PC client & Mobile app
No
No
4 MB
No
Full ﬁle
No
No
No
No
No
No
Full ﬁle
Figure 5: Deduplication ratio (cross-user) vs. Block size. Here
the deduplication ratio = Size of data before deduplication
Size of data after deduplication .
use another user account to upload f to the cloud again. In this
case, the sync trafﬁc should be trivial if full-ﬁle deduplication is
performed across users. If the cross-user full-ﬁle deduplication is
conﬁrmed, Experiment 5 is run again to ﬁgure out the accurate
cross-user deduplication granularity; otherwise, we can conclude
that there is no cross-user data deduplication at all. The results are
also shown in Table 9 (the 3rd column). Obviously, only Dropbox
employs a different cross-user data deduplication granularity from
the identical-user case.
From the above measurements, we get the following ﬁndings and
implications:
• A cloud storage service usually adopts the same data dedu-
plication granularity for PC clients and mobile apps, while
the web-based data synchronization typically does not apply
data deduplication.
• By analyzing our collected trace, we ﬁnd that cross-user data
(block) duplication pervasively exists: even the full-ﬁle level
duplication ratio (= Size of duplicate ﬁles
) reaches 18.8%. How-
ever, most cloud storage services do not support cross-user
deduplication (perhaps for privacy and security concerns) or
block-level deduplication at the moment, thus losing consid-
erable opportunities for optimizing TUE.
Size of all ﬁles
Further, we compare the two types of deduplication granularity
to answer a question: Is the block-level deduplication much bet-
ter (i.e., has a much larger deduplication ratio) than the full-ﬁle
deduplication? Since the computation complexity of block-level d-
eduplication is much higher than that of full-ﬁle deduplication, the
answer could help decide whether or not the block-level dedupli-
cation is worthwhile. Note that when referring to the “ﬁle blocks”,
we are dividing ﬁles to blocks in a simple and natural way, that is
to say, by starting from the head of a ﬁle with a ﬁxed block size.
So clearly, we are not dividing ﬁles to blocks in the best possible
manner [19, 39] which is much more complicated and computation
intensive.
As our collected trace contains both the full-ﬁle hash codes and
the block-level (128 KB – 16 MB blocks) hash codes of each tracked
ﬁle (refer to § 3.1, Table 3), we perform the trace-driven simulation
to ﬁgure out the (cross-user) deduplication ratio when each dedupli-
cation granularity is adopted. The simulation results demonstrate
that the block-level deduplication usually exhibits trivial superior-
ity to the full-ﬁle deduplication, as shown in Figure 5. Therefore,
we have the following implication:
• For providers, in terms of deduplication granularity, support-
ing full-ﬁle deduplication is basically sufﬁcient.
Conﬂicts between compression and block-level deduplication.
Although data deduplication can reduce the sync trafﬁc, we notice
that it has a potential performance conﬂict with data compression.
Implementing block-level deduplication and compression together
is technically challenging.
For cloud storage service providers, though storing and deliver-
ing data in its compressed form can effectively save storage space
and sync trafﬁc, it may signiﬁcantly increase the (computation and
I/O) complexity of block-level deduplication. Speciﬁcally, after a
ﬁle (f) is delivered to the cloud storage in its compressed form (f(cid:48)),
f(cid:48) must be ﬁrst uncompressed to calculate each block’s ﬁngerprint,
so as to enable block-level deduplication. Then, the uncompressed
ﬁle must be deleted from disk. Furthermore, the above operations
must be re-executed (in part) as long as one block of f is modiﬁed.
It is basically unwise for a service provider to shift these operations
to its user clients, unless the service provider does not care about
user experience.
In this subsection we have known that block-level deduplica-
tion exhibits trivial superiority to full-ﬁle deduplication. Mean-
while, full-ﬁle deduplication is not challenged by data compres-
sion, because full-ﬁle deduplication can be directly performed on
compressed ﬁles. Therefore, we suggest that providers implement
full-ﬁle deduplication and compression since these two techniques
work together seamlessly.
6. FREQUENT FILE MODIFICATIONS
In addition to backing up and retrieving ﬁles, cloud storage ser-
vices are also widely used for collaboration, such as collaborative
document editing, team project building, and database hosting. All