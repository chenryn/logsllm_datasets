1% per day, while for Umbrella’s list this climbs to on average
10%. Until January 30, 2018, Alexa’s list was almost as stable
as Majestic’s or Quantcast’s. However, since then stability has
dropped sharply, with around half of the top million changing
every day, due to Alexa’s change to a one day average. There
exists a trade-off in the desired level of stability: a very stable
list provides a reusable set of domains, but may therefore
incorrectly represent sites that suddenly gain or lose popularity.
A volatile list however may introduce large variations in the
results of longitudinal studies.
C. Representativeness
Sites are mainly distributed over a few top-level domains,
with Figure 3 showing that 10 TLDs capture more than 73% of
3
29.5%0.319%7.72%0.892%0.516%2.48%0.417%2.56%23.7%2.07%1.89%0.725%0.472%23.7%3.03%AlexaMajesticUmbrellaQuantcastFig. 2.
consecutive days.
The intersection percentage between each provider’s lists for two
Fig. 3. The cumulative distribution function of TLD usage across the lists.
every list. The .com TLD is by far the most popular, at almost
half of Alexa’s and Majestic’s list and 71% of Quantcast’s
list; .net, .org and .ru are used most often by other sites. One
notable outlier is the .jobs TLD: while for the other lists it
does not ﬁgure in the top 10 TLDs, it is the fourth most
popular TLD for Quantcast. Most of these sites can be traced
to DirectEmployers, with thousands of lowly ranked domains.
This serves as an example of one entity controlling a large
part of a ranking, potentially giving them a large inﬂuence in
research results.
We use the autonomous system to determine the entities
that host the ranked domains. Google hosts the most websites
within the top 10 and 100 sites, at between 15% and 40%
except for Quantcast at 4%: for Alexa these are the localized
versions, for the other lists these are subdomains. For the full
lists, large content delivery networks dominate, with Cloudﬂare
being the top network hosting up to 10% of sites across all lists.
This shows that one or a few entities may be predominantly
represented in the set of domains used in a study and that
therefore care should be taken when considering the wider
implications of its results.
D. Responsiveness
Figure 4 shows the HTTP status code reported for the
root pages of the domains in the four lists. 5% of Alexa’s
and Quantcast’s list and 11% of Majestic’s list could not be
reached. For Umbrella, this jumps to 28%; moreover only 49%
responded with status code 200, and 30% reported a server
error. Most errors were due to name resolution failure, as
invalid or unconﬁgured (sub)domains are not ﬁltered out.
Of the reachable sites, 3% for Alexa and Quantcast, 8.7%
for Majestic and 26% for Umbrella serve a page smaller than
512 bytes on their root page, based on its download size as
reported by the browser instance. As such pages often appear
empty to the user or only produce an error, this indicates that
they may not contain any useful content, even though they
are claimed to be regularly visited by real users. Unavailable
4
Fig. 4. The responsiveness and reported HTTP status code across the lists.
TABLE I.
PRESENCE OF DOMAINS IN THE FOUR RANKINGS ON
GOOGLE’S SAFE BROWSING LIST ON MAY 31, 2018.
Malware
Social Engineering
Unwanted software
Alexa
Umbrella
Majestic
Quantcast
100K
32
11
130
3
Full
98
326
1676
76
10K
100K
4
0
0
0
85
3
23
4
Full
345
393
359
105
10K
100K
0
0
1
0
15
23
9
4
Full
104
232
79
41
Potentially
harmful
application
100K
Full
0
4
9
0
0
60
48
2
Total
547
1011
2162
224
sites and those without content do not represent real sites and
may therefore skew e.g. averages of third-party script inclusion
counts [55], as these sites will be counted as having zero
inclusions.
E. Benignness
Malicious campaigns may target popular domains to extend
the reach of their attack, or use a common domain as a point of
contact, leading to it being picked up as ‘popular’. While it is
not the responsibility of ranking providers to remove malicious
domains, popular sites are often assumed to be trustworthy, as
evidenced by the practice of whitelisting them [29] or, as we
show in Section IV-A, their usage in security research as the
benign test set for classiﬁers.
Table I lists the number of domains ﬂagged on May
31, 2018 by Google Safe Browsing, used among others by
Chrome and Firefox to automatically warn users when they
visit dangerous sites [33]. At 0.22% of its list, Majestic has
the most sites that are ﬂagged as potentially harmful (in
particular as malware sites), but all lists rank at least some
malicious domains. In Alexa’s top 10 000, 4 sites are ﬂagged
as performing social engineering (e.g. phishing), while 1 site in
Majestic’s top 10 000 serves unwanted software. The presence
of these sites in Alexa’s and Quantcast’s list is particularly
striking, as users would have to actively ignore the browser
warning in order to trigger data reporting for Alexa’s extension
or the tracking scripts.
Given the presence of malicious domains on these lists,
the practice of whitelisting popular domains is particularly
dangerous. Some security analysis tools whitelist sites on
Alexa’s list [36], [50]. Moreover, Quad9’s DNS-based blocking
service whitelists all domains on Majestic’s list [29], exposing
its users to ranked malicious domains. As Quad9’s users expect
harmful domains to be blocked, they will be even more under
the impression that the site is safe to browse; this makes the
manipulation of the list very interesting to attackers.
IV. USAGE IN SECURITY RESEARCH
Whenever
security issues are being investigated,
re-
searchers may want to evaluate their impact on real-world
domains. For these purposes, security studies often use and
FebAprJunAugOctDec455060708090100% daily changeAlexaUmbrellaMajesticQuantcast0510152025Number of TLDs455060708090100% of domains coveredAlexaUmbrellaMajesticQuantcastTABLE II.
CATEGORIZATION OF RECENT SECURITY STUDIES USING
THE ALEXA RANKING. ONE STUDY MAY APPEAR IN MULTIPLE
CATEGORIES.
Subset studied
Purpose
10
100
500
1K
10K
100K
1M Other
Total
Prevalence
Evaluation
Whitelist
Ranking
Total
1
7
0
0
8
6
16
2
1
20
8
14
1
3
18
9
10
4
3
18
16
9
3
2
23
7
3
2
4
9
32
14
11
15
45
13
28
6
7
36
63
71
19
28
133
reference the top sites rankings. The validity and representa-
tiveness of these rankings therefore directly affects their results,
and any biases may prohibit correct conclusions being made.
Moreover, if forged domains could be entered into these lists,
an adversary can control research ﬁndings in order to advance
their own goals and interests.
A. Survey and classiﬁcation of list usage
To assess how security studies use these top sites rankings,
we surveyed the papers from the main tracks of the four main
academic security conferences (CCS, NDSS, S&P, USENIX
Security) from 2015 to 2018; we select these venues as they
are considered top-tier and cover general security topics. We
classify these papers according to four purposes for the lists:
prevalence if the rankings are used to declare the proportion
of sites affected by an issue; evaluation if a set of popular
domains serves to test an attack or defense, e.g. for evaluating
Tor ﬁngerprinting [61]; whitelist if the lists are seen as a source
of benign websites, e.g. for use in a classiﬁer [71]; ranking if
the exact ranks of sites are mentioned or used (e.g. to estimate
website trafﬁc [26]) or if sites are divided into bins according
to their rank.
Alexa is by far the most popular list used in recent security
studies, with 133 papers using the list for at least one purpose.
Table II shows the number of papers per category and per
subset of the list that was used. The Alexa list is mostly used
for measuring the prevalence of issues or as an evaluation set
of popular domains. For the former purpose as well as for
whitelisting and ranking or binning, the full list is usually used,
while for evaluation sets, the subset size varies more widely.
Three papers from these conferences also used another ranking,
always in tandem with the Alexa list [17], [74], [75].
Most studies lack any comment on when the list was
downloaded, when the websites on the lists were visited
and what proportion was actually reachable. This hampers
reproducibility of these studies, especially given the daily
changes in list compositions and ranks.
Two papers commented on the methods of the rankings.
Juba et al. [40] mention the rankings being “representative of
true trafﬁc numbers in a coarse grained sense”. Felt et al. [27]
mention the “substantial churn” of Alexa’s list and the unavail-
ability of sites, and express caution in characterizing all its sites
as popular. However, in general the studies do not question the
validity of the rankings, even though they have properties that
can signiﬁcantly affect their conclusions, and as we will show
are vulnerable to manipulation.
5
B. Inﬂuence on security studies
1) Incentives: Given the increasing interest in cybersecurity
within our society, the results of security research have an
impact beyond academia. News outlets increasingly report on
security vulnerabilities, often mentioning their prevalence or af-
fected high-proﬁle entities [30]–[32], [70]. Meanwhile, policy-
makers and governments rely on these studies to evaluate
secure practices and implement appropriate policies [15], [25];
e.g. Mozilla in part decided to delay distrusting Symantec cer-
tiﬁcates based on a measurement across Umbrella’s list [68].
Malicious actors may therefore risk exposure to a wider
audience, while their practices may trigger policy changes,
yielding them an incentive to directly inﬂuence security studies.
Invernizzi et al. [38] discovered that blacklists sold on under-
ground markets contain IP addresses of academic institutions
as well as security companies and researchers,
illustrating
that adversaries already actively try to prevent detection by
researchers. As we showed, security studies often rely on
popularity rankings, so pitfalls in the methods of these rankings
that expose them to targeted manipulation open up another
opportunity for adversaries to affect security research. The way
in which an adversary may want to inﬂuence rankings, and
therefore the research dependent upon them, varies according
to their incentives. They may want to promote domains into the
lists, making them be perceived as benign and then execute ma-
licious practices through them. Alternatively, they can promote
other domains to hide their own malicious domains from the
lists. Finally, they can intelligently combine both techniques
to alter comparisons of security properties for websites of
different entities.
2) Case study: The issue of online tracking and ﬁngerprint-
ing has been studied on multiple occasions for Alexa’s top one
million [26], [42], [44], [45], [55]. Users may want to avoid
organizations that perform widespread or invasive tracking, and
therefore have an interest in new tracking mechanisms and/or
speciﬁc trackers being found or named by these studies, e.g.
to include them in blocklists. The trackers therefore have an
incentive to avoid detection by not ﬁguring among the domains
being studied, e.g. by pushing these out of the popularity
ranking used to provide the set of investigated domains.
We quantify the effort required to manipulate a ranking
and therefore alter ﬁndings for the measurements of ﬁnger-
printing prevalence by Acar et al. [3] and Englehardt and
Narayanan [26] on Alexa’s top 100 000 and top one million
respectively. These studies published data on which domains
included which scripts, including the Alexa rank. We calculate
how many domains minimally need to be moved up in order
to push out the websites using a particular tracking provider.
Figure 5 shows how many ﬁngerprinting providers would