98
80
82
156
158
160
162
95
80
82
156
158
160
162
(149,150)
(133,134)
(146,147)
(130,131)
AveragePooling2D Layer
(110,111)
(114,115)
11
11
Zeropadding Layer
12
(117,118)
Dense Layer
101
81
85
7
7
18
(107,108)
(111,112)
(107,108)
(111,112)
(114,115)
(114,115)
98
78
82
98
78
82
functions as single layers.
Table 2 lists all the related kernels of each layer. Some
kernels are primary kernels, and some kernels are used to
obtain the offset of hyper-parameters. If a layer has only one
related kernel, then this kernel is its primary kernel. If a layer
has more than one related kernels, its primary kernels are
highlighted in bold. The last row indicates some kernels not
belong to any layer, but are still useful and need to be recorded.
SwapDimension1And2InTensor3UsingTiles is record in order
to recover the data ﬂow. BiasNCHWKernel and BiasNHWCK-
ernel are used to determine the layer use bias or not and also
used to obtain the offset of bias address.
4.3 Hyper-Parameters Evaluation
The extracted hyper-parameters are the same as those in the
original model. Table 3 represents all hyper-parameters offsets
in their located kernel. The offset is deﬁned as the distance
between the ﬁrst word and the target hyper-parameter in the
data ﬁeld of a K command. Meanwhile, we also record the
weights and bias offset, which indicate the offset the weights
address and bias address respectively. As Table 3 shown, the
offset of these hyper-parameters is not ﬁxed on distinct plat-
USENIX Association
30th USENIX Security Symposium    1983
Table 4: Identity Evaluation. This table shows the identity
between the original models and the reconstructed models.
All the reconstructed models have the same accuracy with the
original ones, as well as similar inference time.
Metrics Model Original
Reconstructed
N/A
Accuracy
Inference
Time(s)
N/A
MNIST
VGG
ResNet
MNIST
VGG
ResNet
N/A
GT 730
1080 Ti
2080 Ti
98.25% 98.25% 98.25% 98.25%
93.59% 93.59% 93.59% 93.59%
91.45% 91.45% 91.45% 91.45%
2.24
65
20
2.39
63
20
2.52
63
20
2.38
61
21
forms. Some layers may also have multiple implementations,
and the related kernels may change along with the implemen-
tation changes. Here we only list the most frequently used
implementation and their offsets.
Identity Evaluation
4.4
Table 4 evaluates the identity between the original models
and reconstructed models. We evaluate the identity from
two aspects, accuracy and inference time. The accuracy is
measured as the average test accuracy on 10,000 test im-
ages. The inference time in seconds indicates the total time
used to test 10,000 images using this model. For MNIST, the
test datasets is obtained from keras.datasets.mnist.load_data.
For VGG and ResNet, the test datasets is obtained from
keras.datasets.cifar10.load_data. The reconstructed models
proved to be as accurate as of the victims on all platforms.
The original MNIST model trained on the MNIST dataset
achieve 98.25% accuracy. The original VGG model and
ResNet trained on cifar10 dataset achieve 93.59% and 91.45%
respectively, and all reconstructed VGG models are ResNet
models have the same accuracy with the original models. As
Table 4 shown, each reconstructed model has a similar infer-
ence time with the original one, within a reasonable variance.
4.5 Reconstruction Efﬁciency
Table 5 records the runtime statistics and the model-
generation time. The runtime statistics include the number
of total completion packets and the number of both D com-
mands and K commands. These statistics are obtained from
the inference procedure on a single image. Only one image is
enough to reconstruct the whole model. As the table shows,
the number of D commands does not have many relationships
with the running models, since only a few D commands are
used to transfer the information of victim models. However,
more complicate the victim model is, more K commands will
be involved. The generation time in minutes represents the
total time used to reconstruct a model from the PCIe data,
including Trafﬁc Processing, Command Extraction, and Re-
construction. The generation time mainly relies on the number
of completion packets. The number of completion packets is
dependent on both platform and the victim model.
5 Discussions
The Hermes Attack aims to leak the victim model through
PCIe trafﬁc with lossless inference accuracy. It means that
the extracted model will have the same accuracy as the victim
one, regardless of the victim model’s accuracy. Meanwhile,
the number of the activation functions and the model layers
will not affect our attack’s accuracy.
5.1 Super Large DNN Models
The methodology of our attack is supposed to be effective for
all models. However, the buffer size of the snooping device
could be a potential limitation. We currently use the Teledyne
LeCroy Summit T3-26 PCIe protocol analyzer as our snoop-
ing device, which is equipped with an 8GB memory buffer
(4GB for each direction). Due to the buffer size limitation,
we cannot intercept all the trafﬁc if the size of a victim model
is super large, i.e., VGG16 trained from ImageNet [16]. Al-
though the size of this model is about 500MB, the generated
downstream trafﬁc will slightly exceed the buffer limitation
due to the large amount of metadata generated by PCIe and
GPU. This problem could be solved by updating the snooping
device. As far as we know, some other powerful snooping
devices like Teledyne LeCroy’s Summit T34 PCI Express pro-
tocol analyzer [34] can expand the memory buffer into 64GB.
These devices would be able to intercept all the inference
trafﬁc of existing DNN models. Alternatively, we can address
this issue with an advanced algorithm. Speciﬁcally, although
the intercepted model is not complete (e.g., only covering the
ﬁrst n layers) , we can still run our existing algorithm men-
tioned in this above to recover the ﬁrst n layers of the model.
In the next time, we try to intercept the AI model by skipping
k layers (k ≤ n), and run the algorithm again. By repeating
this step until we can recover the last layer, we then get the
whole model by merging all existing recovered layers. This
solution does not rely on any advanced hardware device, but
it requires accurate model interception, and how to directly
recover layers without the data of the skipped layers.
5.2 Attack Generalization
We have demonstrated that our attack can be applied to dif-
ferent GPU platforms. For different platforms (e.g., a smart-
phone with Neural Processing Unit (NPU)), there are several
changes that should be noticed. The ﬁrst change is the com-
mand header that could be different. One possible solution is
to use the method we mentioned in Section 3.3.1 to identify
the new command header structure. The second change is
1984    30th USENIX Security Symposium
USENIX Association
Table 5: Performance Evaluation. This table displays both runtime statistics and generation time. The runtime statistics include
the number of extracted D Commands, K Commands, as well as the number of completion packets. Generation time in minutes
refers to the time used to reconstruct the model. The inference time in seconds indicates the time used to test 10,000 images.
Platform
# of D Commands
# of K Commands
GT 730
25,680
216
MNIST
1080 Ti
28,590
139
2080 Ti
24,342
181
GT 730
27,287
903
VGG
1080 Ti
27,677
628
2080 Ti
24,931
793
GT 730
28,433
1011
ResNet
1080 Ti
28,518
886
2080 Ti
25,577
988
# of Completion Packets
Generation Time (min)
1,077,756
2,244,115
2,959,613
4,284,946
2,615,895
3,354,411
975,257
2,052,657
2,717,451
5
8
11
17
11
12
6
9
10
Table 6: Related Work Comparison. (cid:88)stands for fully recover, P stands for partial recover, × means cannot recover.
Architecture Hyper-Parameters Parameters
Results
Work
Information Source
Method
Xing Hu, et al. 2019 [23]
Bus Access Pattern
Yan, Mengjia, et al. 2018 [58]
Weizhe Hua, et al. 2018 [24]
Yun Xiang et al. 2019 [55]
Vasisht Duddu et al. 2018 [18]
Binghui Wang et al. 2019 [51]
Seong Joon Oh et al. 2018 [38]
Roberts, Nicholas et al. 2018 [43]
Our Work (Hermes Attack)
Cache
Accelerator
Power
Timing
Parameters
Queries
Noise Input
PCIe Bus
Predict
Search
Search,Infer
Predict
Search
Infer
Infer
Predict,Infer
Infer
P
×
(cid:88)
(cid:88)
(cid:88)
×
P
×
(cid:88)
×
(cid:88)
×
(cid:88)
×
(cid:88)
P
×
(cid:88)
×
×
P
×
×
×
×
P
(cid:88)
the GPU instruction sets. The change of instruction sets will
lead to the difference in kernel binaries. Fortunately, we can
also use the method in Section 3.4.1 to update the database.
Although there would be several changes when the platform
changes, the GPU and PCIe underlying working mechanism
will stay the same. Therefore, the proposed attack will not be
inﬂuenced by the alternation of hardware.
Different from the change of GPUs, the change of the DNN
framework will lead to the different implementation of each
layer as well as the relationship between layer and GPU ker-
nels. However, as long as all layers are executed on GPU,
we are able to obtain the relationship between the layer and
kernels, it will not affect our proposed attack.
The case that multiple tasks simultaneously run on a single
GPU should also be aware of. The simultaneously running
tasks share the same GPU with the victim model. In this
manner, the data sent from the other tasks will make an inter-
ference on our extraction. Thanks to the fact that each process
owns a GPU context and each context has at least one channel
to sent commands, the different tasks can be ﬁltered by the