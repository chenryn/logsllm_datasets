### 9.1.6 URL别名
即使使用了正确的数据结构，URL的“别名”现象也会使得判断是否访问过某个页面变得复杂。如果两个不同的URL实际上指向同一资源，它们就被称为彼此的“别名”。表9-1列举了几种不同URL指向同一资源的情况。

**表9-1 同一文档的不同URL别名**

| 第一个URL | 第二个URL | 何时为别名 |
| --- | --- | --- |
| http://www.foo.com/bar.html | http://www.foo.com:80/bar.html | 默认端口是80 |
| http://www.foo.com/~fred | http://www.foo.com/%7Ffred | %7F与~相同 |
| http://www.foo.com/x.html#early | http://www.foo.com/x.html#middle | 锚点不改变页面内容 |
| http://www.foo.com/readme.htm | http://www.foo.com/README.HTM | 服务器对大小写不敏感 |
| http://www.foo.com/ | http://www.foo.com/index.html | 默认页面为index.html |
| http://www.foo.com/index.html | http://209.231.87.45/index.html | www.foo.com使用这个IP地址 |

### 9.1.7 规范化URL
大多数Web爬虫尝试通过将URL转换为标准格式来消除这些显而易见的别名问题。以下步骤可以帮助实现这一目标：

1. 如果没有指定端口，则向主机名添加“:80”。
2. 将所有转义符%xx转换为其等价字符。
3. 删除锚点（#标签）。

这些步骤可以解决表9-1中a至c的问题。然而，对于d至f的情况，除非机器人了解特定Web服务器的具体配置，否则难以避免别名问题：
- 机器人需要知道服务器是否对大小写不敏感才能处理表9-1中的d项。
- 对于e项，机器人需知晓该目录下的默认索引页面设置。
- 即使机器人知道f项中的域名和IP地址指向同一台计算机，它还需确认服务器是否配置为虚拟主机操作。

尽管URL规范化能解决一些基本的语法别名问题，但其他类型的URL别名仍然存在，无法仅靠标准化消除。

### 9.1.8 文件系统连接环路
文件系统中的符号链接可能导致潜在的循环陷阱。这类环路通常由服务器管理员无意间创建，但也可能是恶意网管故意为之以迷惑爬虫。图9-3展示了两种文件系统结构，其中一种包含了一个指向根目录的符号链接，这会导致爬虫陷入无限循环。

**图9-3 符号链接环路示例**

在图9-3b所示的情况下，爬虫可能重复请求相同的文件，直到URL长度超过限制或服务器崩溃。

### 9.1.9 动态虚拟Web空间
恶意管理员可能会创建复杂的动态内容，导致爬虫陷入无尽的循环。例如，一个看似普通文件的URL实际上是一个生成虚假HTML页面的应用程序，每个页面都包含新的、虚构的URL链接。这种情况下，即使服务器实际不含任何静态文件，也能通过无限生成的内容使爬虫陷入困境。

### 9.1.10 避免循环和重复
为了防止爬虫陷入循环，可以采取以下策略：
- **URL规范化**：统一URL格式以减少语法上的别名。
- **广度优先搜索**：优先访问不同站点而非深入单一站点，从而降低陷入循环的风险。
- **流量控制**：限制单位时间内从同一站点获取的页面数量。
- **URL长度限制**：拒绝处理过长的URL，以防其不断增长导致循环。
- **黑名单**：维护一份已知有问题或恶意站点的列表，并避开它们。
- **模式检测**：识别并规避具有重复组件的URL。
- **内容指纹**：计算页面内容的校验和，避免重复爬取相同内容。
- **人工监控**：持续关注爬虫行为，及时发现并解决问题。

### 9.2 机器人的HTTP
Web爬虫和其他HTTP客户端一样，必须遵守HTTP协议规范。虽然许多爬虫只实现最基本的HTTP功能，但建议发送一些额外的头部信息以便更好地识别和管理请求。常见的识别头部包括User-Agent、From、Accept和Referer等。

#### 9.2.1 识别请求首部
- **User-Agent**：标识发起请求的爬虫名称。
- **From**：提供爬虫管理者或用户的电子邮件地址。
- **Accept**：告知服务器可接受的媒体类型。
- **Referer**：指出当前请求URL所在的源文档URL。

#### 9.2.2 虚拟主机
[此处继续原文内容]