由于URL“别名”的存在，即使使用了正确的数据结构，有时也很难分辨出以前是
否访问过某个页面。如果两个URL看起来不一样，但实际指向的是同一资源，就称
这两个URL互为“别名”。
表9-1列出了不同URL指向同一资源的几种简单方式。
表9-1 同一文档的不同URL别名
第一个URL 第二个URL 什么时候是别名
a http://www.foo.com/bar.html http://www.foo.com:80/bar.html 默认端口为80
b http://www.foo.com/~fred http://www.foo.com/%7Ffred %7F与~相同
c http://www.foo.com/x.html#early http://www.foo.com/x.html#middle 标签并没有修改页面内容
d http://www.foo.com/readme.htm http://www.foo.com/README.HTM 服务器是大小写无关的
e http://www.foo.com/ http://www.foo.com/index.html 默认页面为index.html
219 f http://www.foo.com/index.html http://209.231.87.45/index.html www.foo.com使用这个IP地址
9.1.7 规范化URL
大多数Web机器人都试图通过将URL“规范化”为标准格式来消除上面那些显而
易见的别名。机器人首先可先通过下列步骤将每个URL都转化为规范化的格式。
(1)如果没有指定端口的话，就向主机名中添加“:80”。
(2)将所有转义符%xx都转换成等价字符。
(3)删除#标签。
注4：本书中文版已由科学出版社出版。（编者注）
230 ｜ 第9章
经过这些步骤就可以消除表9-1中a~c所列的别名问题了。但如果不知道特定Web
服务器的相关信息，机器人就没什么好办法来避免表9-1中d~f的问题了。
• 机器人需要知道Web服务器是否是大小写无关的才能避免表9-1d中的别名问题。
• 机器人需要知道Web服务器上这个目录下的索引页面配置才能知道表9-1e中的
情况是否是别名。
• 即使机器人知道表9-1f中的主机名和IP地址都指向同一台计算机，它也还要
知道Web服务器是否配置为进行（参见第5章）虚拟主机操作，才能知道这个
URL是不是别名。
URL规范化可以消除一些基本的语法别名，但机器人还会遇到其他的、将URL转
换为标准形式也无法消除的URL别名。
9.1.8 文件系统连接环路
文件系统中的符号连接会造成特定的潜在环路，因为它们会在目录层次深度有限的
情况下，造成深度无限的假象。符号连接环路通常是由服务器管理员的无心错误造
成的，但“邪恶的网管”也可能会恶意地为机器人制造这样的陷阱。
图9-3显示了两个文件系统。在图9-3a中，subdir是个普通的目录。在图9-3b中，
subdir是个指回到“/”的符号连接。在这两个图中，都假设文件/index.html中包含
了一个指向文件subdir/index.html的超链。
/ /
index.html subdir index.html subdir
index.html logo.gif
（a）subdir是个目录 （b）subdir是个向上的符号连接
图9-3 符号连接环路
使用图9-3a所示的文件系统时，Web爬虫可能会采取下列动作：
(1) GET http://www.foo.com/index.html
获取/index.html，找到指向subdir/index.html的链接。
Web机器人 ｜ 231
(2) GET http://www.foo.com/subdir/index.html
获取subdir/index.html，找到指向subdir/logo.gif的链接。
(3) GET http://www.foo.com/subdir/logo.gif
220 获取subdir/logo.gif，再没有链接了，结束。
但在图9-3b的文件系统中，可能会发生下列情况：
(1) GET http://www.foo.com/index.html
获取/index.html，找到指向subdir/index.html的链接。
(2) GET http://www.foo.com/subdir/index.html
获取subdir/index.html，但得到的还是同一个index.html。
(3) GET http://www.foo.com/subdir/subdir/index.html
获取subdir/subdir/index.html。
(4) GET http://www.foo.com/subdir/subdir/subdir/index.html
获取subdir/subdir/subdir/index.html。
图9-3b的问题是subdir/是个指向“/”的环路，但由于URL看起来有所不同，所以
机器人无法单从URL本身判断出文档是相同的。毫无戒备的机器人就有了陷入循环
的危险。如果没有某种循环检测方式，这个环路就会继续下去，通常会持续到URL
的长度超过机器人或服务器的限制为止。
9.1.9 动态虚拟Web空间
恶意网管可能会有意创建一些复杂的爬虫循环来陷害那些无辜的、毫无戒备的机器
人。尤其是，发布一个看起来像普通文件，实际上却是网关应用程序的URL是很
容易的。这个应用程序可以在传输中构造出包含了到同一服务器上虚构URL链接的
HTML。请求这些虚构的URL时，这个邪恶的服务器就会捏造出一个带有新的虚构
URL的新HTML页面来。
即使这个恶意Web服务器实际上并不包含任何文件，它也可以通过无限虚拟的Web
空间将可怜的机器人带入爱丽丝漫游仙境之旅。更糟的是，每次的URL和HTML
看起来都有很大的不同，机器人很难检测到环路。图9-4显示了一个恶意Web服务
221 器生成假内容的例子。
更常见的情况是，那些没有恶意的网管们可能会在无意中通过符号连接或动态内容
构造出爬虫陷阱。比如，我们来看一个基于CGI的日历程序，它会生成一个月历和
232 ｜ 第9章
一个指向下个月的链接。真正的用户是不会不停地请求下个月的链接的，但不了解
其内容的动态特性的机器人可能会不断向这些资源发出无穷的请求。5
请求报文
GET /index-fall.html HTTP/1.1
Host: www.evil-joes-hardware.com
Accept: *
User-agent: ShopBot
响应报文 www.evil-joes-hardware.com
HTTP/1.1 200 OK
Content-type: text/html
Content-length: 617
trick[...]
Web机器人客户端
请求报文
GET /index-fall2.html HTTP/1.1
Host: www.evil-joes-hardware.com
Accept: *
User-agent: ShopBot
响应报文 www.evil-joes-hardware.com
HTTP/1.1 200 OK
Content-type: text/html
Content-length: 617
trick[...]
Web机器人客户端
有几个站点上只有恶意的网关应用程序，其唯一目的就是用假内容来陷害毫无防备的机器人。
在这个例子中，网关会动态生成无数的假Web页面，每个假页面都指向另一个假页面。
图9-4 恶意的动态Web空间示例
9.1.10 避免循环和重复
没有什么简单明了的方式可以避免所有的环路。实际上，经过良好设计的机器人中
要包含一组试探方式，以避免环路的出现。
总的说来，爬虫的自动化程度越高（人为的监管越少），越可能陷入麻烦之中。机器
人的实现者需要做一些取舍——这些试探方式有助于避免问题的出现，但你可能会
终止扫描那些看起来可疑的有效内容，因此这种方式也是“有损失”的。 222
在机器人会遇到的各种危险的Web中，有些技术的使用可以使机器人有更好的表现。
注5： 这是http://www.searchtools.com/robots/robot-checklist.html上提到的日历站点http://cgi.umbc.edu/cgi-
bin/WebEvent/Webevent.cgi上的真实例子。这样的动态内容带来的后果就是，很多机器人都拒绝爬行
URL中包含子字符串“cgi”的页面。
Web机器人 ｜ 233
• 规范化URL
将URL转换为标准形式以避免语法上的别名
• 广度优先的爬行
每次爬虫都有大量潜在的URL要去爬行。以广度优先的方式来调度URL去访问
Web站点，就可以将环路的影响最小化。即使碰到了机器人陷阱，也可以在回到
环路中获取的下一个页面之前，从其他Web站点中获取成百上千的页面。如果
采用深度优先方式，一头扎到单个站点中去，就可能会跳入环路，永远无法访问
其他站点。6
• 节流7
限制一段时间内机器人可以从一个Web站点获取的页面数量。如果机器人跳进
了一个环路，试图不断地访问某个站点的别名，也可以通过节流来限制重复的页
面总数和对服务器的访问总数。
• 限制URL的大小
机器人可能会拒绝爬行超出特定长度（通常是1KB）的URL。如果环路使URL
的长度增加，长度限制就会最终终止这个环路。有些Web服务器在使用长URL
时会失败，因此，被URL增长环路困住的机器人会使某些Web服务器崩溃。这
会让网管错误地将机器人当成发起拒绝服务攻击的攻击者。
要小心，这种技术肯定会让你错过一些内容。现在很多站点都会用URL来管理
用户的状态（比如，在一个页面引用的URL中存储用户ID）。用URL长度来限
制爬虫可能会带来些麻烦；但如果每当请求的URL达到某个特定长度时，都记
录一次错误的话，就可以为用户提供一种检查某特定站点上所发生情况的方法。
• URL/站点黑名单
维护一个与机器人环路和陷阱相对应的已知站点及URL列表，然后像躲避瘟疫
一样避开它们。发现新问题时，就将其加入黑名单。
这就要求有人工进行干预。但现在很多大型爬虫产品都有某种形式的黑名单，用
于避开某些存在固有问题或者有恶意的站点。还可以用黑名单来避开那些对爬行
223 大惊小怪的站点。8
注6： 总之，广度优先搜索是个好方法，这样可以更均匀地分配请求，而不是都压到任意一台服务器上去。
这样可以帮助机器人将用于一台服务器的资源保持在最低水平。
注7： 在9.5节也讨论了请求率的节流问题。
注8： 9.4节讨论了站点怎样才能避免被爬行，但有些用户拒绝使用这种简单的控制机制，在其站点被爬行
时又会变得非常愤怒。
234 ｜ 第9章
• 模式检测
文件系统的符号连接和类似的错误配置所造成的环路会遵循某种模式；比如，
URL会随着组件的复制逐渐增加。有些机器人会将具有重复组件的URL当作潜
在的环路，拒绝爬行带有多于两或三个重复组件的URL。
重复并不都是立即出现的（比如，“/subdir/subdir/subdir...”）。有些环路周期可能
为2或其他间隔，比如“/subdir/images/subdir/images/subdir/images/...”。有些机
器人会查找具有几种不同周期的重复模式。
• 内容指纹
一些更复杂的Web爬虫会使用指纹这种更直接的方式来检测重复。使用内容指
纹的机器人会获取页面内容中的字节，并计算出一个校验和（checksum）。这个
校验和是页面内容的压缩表示形式。如果机器人获取了一个页面，而此页面的校
验和它曾经见过，它就不会再去爬行这个页面的链接了——如果机器人以前见过
页面的内容，它就已经爬行过页面上的链接了。
必须对校验和函数进行选择，以求两个不同页面拥有相同校验和的几率非常低。
MD5这样的报文摘要函数就常被用于指纹计算。
有些Web服务器会在传输过程中对页面进行动态的修改，所以有时机器人会在
校验和的计算中忽略Web页面内容中的某些部分，比如那些嵌入的链接。而且，
无论定制了什么页面内容的动态服务器端包含（比如添加日期、访问计数等）都
可能会阻碍重复检测。
• 人工监视
Web就是一片荒野。勇敢的机器人最终总会陷入一个采用任何技术都无能为力的
困境。设计所有产品级机器人时都要有诊断和日志功能，这样人类才能很方便地
监视机器人的进展，如果发生了什么不寻常的事情就可以很快收到警告。在某些
情况下，愤怒的网民会给你发送一些无礼的邮件来提示你出了问题。
爬行Web这样规模庞大的数据集时，好的蜘蛛探测法总是会不断改进其工作的。随
着新的资源类型不断加入Web，它会随着时间的推移构建出一些新的规则，并采纳
这些规则。好的规则总是在不断发展之中的。
当受到错误爬虫影响的资源（服务器、网络带宽等）处于可管理状态，或者处于
执行爬行工作的人的控制之下（比如在内部站点上）时，很多较小的、更具个性
的爬虫就会绕开这些问题。这些爬虫更多的是依赖人类的监视来防止这些问题的
发生。 224
Web机器人 ｜ 235
9.2 机器人的HTTP
机器人和所有其他HTTP客户端程序并没有什么区别。它们也要遵守HTTP规范中
的规则。发出HTTP请求并将自己广播成HTTP/1.1客户端的机器人也要使用正确
的HTTP请求首部。
很多机器人都试图只实现请求它们所查找内容所需的最小HTTP集。这会引发一
些问题；但短期内这种行为不会发生什么改变。结果就是，很多机器人发出的都是
HTTP/1.0请求，因为这个协议的要求很少。
9.2.1 识别请求首部
尽管机器人倾向于只支持最小的HTTP集，但大部分机器人确实实现并发送了一些
识别首部——最值得一提的就是User-Agent首部。建议机器人实现者们发送一些
基本的首部信息，以通知各站点机器人的能力、机器人的标识符，以及它是从何处
起源的。
在追踪错误爬虫的所有者，以及向服务器提供机器人所能处理的内容类型时，这些
信息都是很有用的。鼓励机器人实现者们使用的基本识别首部包括如下内容。
• User-Agent
将发起请求的机器人名字告知服务器。
• From
提供机器人的用户/管理者的E-mail地址。9
• Accept
告知服务器可以发送哪些媒体类型。10这有助于确保机器人只接收它感兴趣的内
容（文本、图片等）。
• Referer
提供包含了当前请求URL的文档的URL。11
9.2.2 虚拟主机