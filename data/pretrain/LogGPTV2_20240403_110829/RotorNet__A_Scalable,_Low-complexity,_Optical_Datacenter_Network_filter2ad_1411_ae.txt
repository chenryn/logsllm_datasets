Looking in more detail at the distribution of subflow delivery
times for a given traffic density in Figure 9b, we see that subflows
are approximately uniformly distributed in time with two notable
exceptions. First, because delivery of stored two-hop traffic is pri-
oritized, two-hop subflows are more likely to be delivered at the
start of a matching slot, accounting for the slight non-uniformity in
the CDF. Second, for traffic densities above approximately 0.05, the
worst-case delivery time is rare, and most subflows are delivered
within one matching cycle (rather than one cycle + one slot).
Subflows injected into RotorNet will be sequentially ordered
between matching slots, but during a single matching slot subflows
from the same flow may take both one-hop and two-hop paths,
depending on the quantity and the time that traffic is committed to
the network by the application. If data sent over a two-hop subflow
logically precedes data sent over a one-hop flow, that data will
arrive out of order at the receiver. We assign the responsibility of
ensuring ordered delivery of data to a reordering process at each
end host. Fortunately, because all injected traffic is guaranteed to be
delivered in Nm + 1 matching slots, the receiver only needs to store
00.20.40.60.81Traffic density051015Subflow delivery [slots]Delivery boundIndividual subflowsAverage02468Subflow delivery time [slots]00.250.50.751CDFSIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
W. M. Mellette et al.
Figure 10: Flow completion times in RotorNet and a three-
level Fat Tree, both with k = 4-port ToRs, 10-Gb/s links, and
140 200-kB flows per end host. Two examples of bandwidth
allocation in RotorNet are shown: fair sharing bandwidth
across flows and greedily allocating bandwidth to flows one-
by-one.
data received within a sliding window equal to that delivery bound.
For a 100-Gb/s end host link, 200-µs slot time, and Nm = 8, each
host would need a receive buffer of 23 MB to ensure ordered delivery
of all incoming flows. This is a modest memory requirement given
the specifications of modern servers.
7.4 Flow completion times
Flow completion time (FCT) remains an important metric for net-
work performance, especially considering the prevalence of services
that are impacted by the presence of a long tail in the FCT distri-
bution [33]. While contemporary multi-rooted trees coupled with
TCP can provide sufficient bandwidth on paper for applications,
FCT in such networks is subject to considerable variance. Here,
RotorNet improves on the state of the art by providing a choice
between either low FCT variance or significantly reduced mean
FCT compared to Fat Trees, while in both cases eliminating the
long tail present in Fat Trees.
To demonstrate this capability, we simulated a 10-Gb/s k = 4
three-level Fat Tree in ns-3, and an equivalently sized RotorNet
using our flow-level simulator. Each host communicates with cross-
rack hosts in an all-to-all pattern with 10 flows of size 200 KB
each, for a total of 2,240 flows overall. Figure 10 depicts the FCT in
milliseconds for the Fat Tree, and for greedy flow completion and
fair-shared bandwidth strategies for RotorNet. Note that the p100
for both RotorNet strategies is identical and a full 30% smaller than
p99.9 for the Fat Tree. In addition, p50 for greedy flow completion
RotorNet is 33% smaller than the Fat Tree. For the fair-shared band-
width strategy, the variance (normalized to the mean) is just 0.6%,
compared to 61% for the Fat Tree.
In order to reduce FCT variance for a Fat Tree network, a network
operator might consider operating the network at lower utilization
(a) Per-rack buffering for all traffic densities.
(b) CDF for traffic density = 0.02 from (a).
Figure 11: Per-rack buffering required for a 100-Gb/s deploy-
ment, 20-µs switching, 90% duty cycle, and NR = 256.
levels. To characterize how much lower, we deploy a 100-Mb/s
k = 4 Fat Tree on Mininet[18] and test the same traffic pattern. To
emulate lower network utilization, we cap the flow size and rate to
10% and 20% of the nominal values. While reducing the utilization
to 10% brings mean-normalized variance down to 22% from 70%, we
note that the value is still well above that for fair-shared RotorNet.
Thus, the low utilizations typical in real-world datacenter networks
can be considered as an implicit duty-cycle cost levied to keep
variations in performance low.
7.5 Buffering
Here we use the fluid-flow model from Section 7.1 to determine
the amount of buffering required at each rack to support two-hop
traffic under various traffic patterns.
Figure 11a shows the observed worst cases and average amount
of buffering needed at each rack to support various traffic densities,
with 100 randomly generated traffic patterns per density. In general,
buffering can be expressed as the product of upward-facing ToR
bandwidth and matching slot time, however to make the results
concrete we show absolute storage in bytes, based on 100-Gb/s
links, a reconfiguration delay of 20 µs, and duty cycle of 90%. We
see that the largest amount of required buffering occurs for a traffic
density of approximately 0.02, resulting in each rack requiring 400
010203040Flow completion time (ms)00.20.40.60.81CDF99.9%-tile Fat Tree100%-tile RotorNet     (fair & unfair)Fat TreeRotorNet (unfair)RotorNet (fair)00.250.50.751Traffic density0100200300400Per rack buffer (MB)Worst cases observedAverage0100200300400Per rack buffer (MB)00.250.50.751CDF0.1%0.01%0%Loss:RotorNet: A Scalable, Low-complexity, Optical Datacenter Network
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
Failure type:
RotorLB:
RotorLB-FT:
1 link
0.97 (0)
0.99 (0.38)
25% Rotor sw.
0.73 (0)
0.89 (0.3)
50% Rotor sw.
0.53 (0)
0.70 (0.07)
Table 4: Jain’s fairness index across flows (higher is better)
and (in parenthesis, higher is better) ratio of minimum to
maximum bandwidth of flows under various failure scenar-
ios. RotorLB-FT promotes fair allocation of bandwidth un-
der failures and, critically, ensures all flows get non-zero
bandwidth.
even under extensive failure conditions. Table 4 shows fairness
metrics for RotorLB and RotorLB-FT subject to various failures
under all-to-all traffic. We consider a single link failure, a case in
which one quarter of the Rotor switches fail, and a case where
half of the Rotor switches fail. RotorLB-FT scores higher on Jain’s
fairness index [19] than RotorLB by 20% and 30% for the cases that
one-quarter and one-half of the Rotor switches fail, respectively.
Critically, RotorLB-FT provides non-zero bandwidth to all flows
in all failure scenarios considered, whereas RotorLB delivers zero
bandwidth to some flows even in the case of a single link failure.
8 CONCLUSIONS
Optical switching holds the promise of overcoming impending lim-
itations in electrical packet switching, yet has seen resistance to
industrial adoption due to practical barriers. In this paper, we de-
scribe a network fabric based on Rotor switches which decouples
the control of OCSes from the rest of the network, greatly simplify-
ing network control and deployment. Additionally, because Rotor
switching has such a simple implementation, we can design new
OCSes that leverage this new-found simplicity for greater scalabil-
ity. The combination of these two capabilities has the potential to
meet next-generation datacenter bandwidth demands in a manner
much simpler than existing approaches.
ACKNOWLEDGMENTS
We thank our shepherd, Mosharaf Chowdhury, the anonymous
SIGCOMM reviewers, and Amin Vahdat for their useful feedback.
We also thank Facebook for supporting this work through a gift,
and the National Science Foundation for supporting this work
through grants CNS-1629973, CNS-1314921, CNS-1553490, and CNS-
1564185.
[2] Thomas Beth and Volker Hatz. 1991. A Restricted Crossbar Implementation and
REFERENCES
[1] Dan Alistarh, Hitesh Ballani, Paolo Costa, Adam Funnell, Joshua Benjamin, Philip
Watts, and Benn Thomsen. A High-Radix, Low-Latency Optical Switch for Data
Centers (SIGCOMM ’15).
Its Applications. SIGARCH Comput. Archit. News 19, 6 (Dec. 1991).
[3] Garrett Birkhoff. 1946. Tres Observaciones Sobre el Algebra Lineal. Univ. Nac.
Tucumán Rev. Ser. A 5 (1946).
[4] Shaileshh Bojja Venkatakrishnan, Mohammad Alizadeh, and Pramod Viswanath.
Costly Circuits, Submodular Schedules and Approximate Carathéodory Theorems
(SIGMETRICS ’16).
[5] Cheng-Shang Chang, Duan-Shin Lee, and Yi-Shean Jou. 2002. Load Balanced
Birkhoff–von Neumann Switches, Part I: One-stage Buffering. Computer Com-
munications 25, 6 (2002), 611–622.
[6] Kai Chen, Ankit Singla, Atul Singh, Kishore Ramachandran, Lei Xu, Yueping
Zhang, and Xitao Wen. OSA: An Optical Switching Architecture for Data Center
Networks and Unprecedented Flexibility (NSDI ’12).
Figure 12: Responsiveness of RotorLB and ideal 1:1 and 3:1
Fat Trees (FT) to changing traffic patterns, NR = 256. RotorLB
converges within two matching cycles.
MB of memory for two-hop traffic. If this traffic were stored on end
hosts, rather than at the ToR switch, 12.5 MB per end host would
be required assuming 32 end hosts per rack.
Next, we consider the amount of memory which could be saved
by permitting a small amount of loss in the network (as opposed
to the strictly lossless fabric we have discussed so far). Figure 11b
shows a CDF of the per-rack buffer requirements for the worst-
case traffic density observed in Figure 11a, which occurs at a traffic
density of approximately 0.02. We see that increasing the loss rate
to 0.01% only reduces the memory requirement by about 6%. This
small achievable reduction in buffering gained by permitting loss
supports our decision to design RotorNet as a lossless fabric.
7.6 Responsiveness
Here, we assess how quickly RotorLB responds to changing traf-
fic patterns. We use the fluid-flow model from Section 7.1, and
abruptly switch between different traffic patterns as RotorLB runs
continuously. Figure 12 shows a typical time series of the aggre-
gate throughput per matching cycle under RotorLB as we vary
the traffic pattern. The throughputs of a fully-provisioned and 3:1
over-subscribed ideal packet-switched network (as described in
Section 7.2) are shown for reference. Upon changing the traffic pat-
tern, RotorLB converges to the new sustained throughput within
two matching cycles. This fast response is because, as described
in Section 7.3, two-hop traffic is drained from intermediate queues
after one matching cycle, allowing RotorLB to adapt to the new
traffic pattern within two matching cycles.
7.7 Fault tolerance
Recall from Section 3 that each Rotor switch implements only a
fraction of RotorNet’s connectivity, meaning that the failure of one
or more Rotor switches could lead to a significant reduction in
overall connectivity. However, we find that using the RotorLB-FT
protocol described in Section 5.3, network connectivity is retained
020406080Time (matching cycle)00.20.40.60.81Aggregate throughputIdeal 1:1 FTRotorLBIdeal 3:1 FTSIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
W. M. Mellette et al.
[7] Kai Chen, Xitao Wen, Xingyu Ma, Yan Chen, Yong Xia, Chengchen Hu, and
Qunfeng Dong. WaveCube: A Scalable, Fault-tolerant, High-performance Optical
Data Center Architecture (IEEE INFOCOM ’15).
[8] Li Chen, Kai Chen, Joshua Zhu, Minlan Yu, George Porter, Chunming Qiao, and
Shan Zhong. Enabling Wide-Spread Communications on Optical Fabric with
MegaSwitch (NSDI ’17).
[9] Jack Edmonds. 1965. Paths, trees, and flowers. Canad. J. Math. 17 (1965).
[10] Facebook. 2014. Introducing data center fabric, the next-generation Facebook
data center network. https://goo.gl/mvder2. (2014).
(2015).
[11] Facebook. 2015. FBFlow Dataset. https://www.facebook.com/network-analytics.
[14] David Gale and Lloyd S Shapley. 1962. College admissions and the stability of
[12] Nathan Farrington, Alex Forencich, George Porter, P-C Sun, Joseph E Ford,
Yeshaiahu Fainman, George C Papen, and Amin Vahdat. 2013. A Multiport
Microsecond Optical Circuit Switch for Data Center Networking. IEEE Photonics
Technology Letters 25, 16 (2013).
[13] Nathan Farrington, George Porter, Sivasankar Radhakrishnan, Hamid Bazzaz,
Vikram Subramanya, Yeshaiahu Fainman, George Papen, and Amin Vahdat.
Helios: A Hybrid Electrical/Optical Switch Architecture for Modular Data Centers
(SIGCOMM ’10).
marriage. The American Mathematical Monthly 69, 1 (1962).
[15] Monia Ghobadi, Ratul Mahajan, Amar Phanishayee, Nikhil Devanur, Janard-
han Kulkarni, Gireeja Ranade, Pierre-Alexandre Blanche, Houman Rastegarfar,
Madeleine Glick, and Daniel Kilper. ProjecToR: Agile Reconfigurable Data Center
Interconnect (SIGCOMM ’16).
[16] Albert Greenberg, James R. Hamilton, Navendu Jain, Srikanth Kandula,
Changhoon Kim, Parantap Lahiri, David A. Maltz, Parveen Patel, and Sudipta
Sengupta. VL2: A Scalable and Flexible Data Center Network (SIGCOMM ’09).
[17] Navid Hamedazimi, Zafar Qazi, Himanshu Gupta, Vyas Sekar, Samir R. Das, Jon P.
Longtin, Himanshu Shah, and Ashish Tanwer. FireFly: A Reconfigurable Wireless
Data Center Fabric Using Free-space Optics (SIGCOMM ’14).
[18] Nikhil Handigol, Brandon Heller, Vimalkumar Jeyakumar, Bob Lantz, and Nick
McKeown. Reproducible Network Experiments Using Container-based Emulation
(CoNEXT ’12).
[19] Raj Jain. 1991. The art of computer systems performance analysis - techniques for
experimental design, measurement, simulation, and modeling.
[20] He Liu, Feng Lu, Alex Forencich, Rishi Kapoor, Malveeka Tewari, Geoffrey M.
Voelker, George Papen, Alex C. Snoeren, and George Porter. Circuit Switching
Under the Radar with REACToR (NSDI ’14).
[23] Nick McKeown. 1999. The iSLIP Scheduling Algorithm for Input-Queued
[21] He Liu, Matthew K. Mukerjee, Conglong Li, Nicolas Feltman, George Papen, Ste-
fan Savage, Srinivasan Seshan, Geoffrey M. Voelker, David G. Andersen, Michael
Kaminsky, George Porter, and Alex C. Snoeren. Scheduling Techniques for Hybrid
Circuit/Packet Networks (CoNEXT ’15).
[22] He Liu, Matthew K. Mukerjee, Conglong Li, Nicolas Feltman, George Papen, Ste-
fan Savage, Srinivasan Seshan, Geoffrey M. Voelker, David G. Andersen, Michael
Kaminsky, George Porter, and Alex C. Snoeren. Scheduling Techniques for Hybrid
Circuit/Packet Networks (CoNEXT ’15).
Switches. IEEE/ACM Transactions on Networking 7, 2 (1999).
[24] William M. Mellette and Joseph Ford. 2015. Scaling Limits of MEMS Beam-
Steering Switches for Data Center Networks. Journal of Lightwave Technology
33, 15 (Aug. 2015).
[25] William M. Mellette, Glenn M. Schuster, George Porter, George Papen, and
Joseph Ford. 2017. A scalable, partially configurable optical switch for data center
networks. Journal of Lightwave Technology 35, 2 (Jan. 2017).
[26] George Porter, Richard Strong, Nathan Farrington, Alex Forencich, Pang-Chen
Sun, Tajana Rosing, Yeshaiahu Fainman, George Papen, and Amin Vahdat. Inte-
grating Microsecond Circuit Switching into the Data Center (SIGCOMM ’13).
Inside the Social Network’s (Datacenter) Network (SIGCOMM ’15).
[28] Vishal Shrivastav, Asaf Valadarsky, Hitesh Ballani, Paolo Costa, Ki Suh Lee, Han
Wang, Rachit Agarwal, and Hakim Weatherspoon. 2017. Shoal: a lossless newtork
for high-density and disaggregated racks. Cornell Technical Report (2017).
[29] Arjun Singh, Joon Ong, Amit Agarwal, Glen Anderson, Ashby Armistead, Roy
Bannon, Seb Boving, Gaurav Desai, Bob Felderman, Paulie Germano, Anand
Kanagala, Jeff Provost, Jason Simmons, Eiichi Tanda, Jim Wanderer, Urs Hölzle,
Stephen Stuart, and Amin Vahdat. Jupiter Rising: A Decade of Clos Topologies
and Centralized Control in Google’s Datacenter Network (SIGCOMM ’15).
[30] Leslie G. Valiant. 1982. A Scheme for Fast Parallel Communication. SIAM J.
Comput. 11, 2 (1982).
[31] John Von Neumann. 1953. A certain zero-sum two-person game equivalent to
the optimal assignment problem. Contributions to the Theory of Games 2 (1953).
[32] Guohui Wang, David G. Andersen, Michael Kaminsky, Konstantina Papagiannaki,
T. S. Eugene Ng, Michael Kozuch, and Michael Ryan. c-Through: Part-time Optics
in Data Centers (SIGCOMM ’10).
[33] David Zats, Tathagata Das, Prashanth Mohan, Dhruba Borthakur, and Randy
Katz. DeTail: Reducing the Flow Completion Time Tail in Datacenter Networks
(SIGCOMM ’12).
[27] Arjun Roy, Hongyi Zeng, Jasmeet Bagga, George Porter, and Alex C. Snoeren.