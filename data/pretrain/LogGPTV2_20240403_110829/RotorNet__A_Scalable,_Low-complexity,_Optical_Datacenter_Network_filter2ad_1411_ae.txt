### Detailed Analysis of Subflow Delivery Times

In Figure 9b, we examine the distribution of subflow delivery times for a given traffic density. The data indicates that subflows are generally uniformly distributed over time, with two notable exceptions:

1. **Prioritization of Two-Hop Traffic**: Since the delivery of stored two-hop traffic is prioritized, these subflows are more likely to be delivered at the beginning of a matching slot. This priority accounts for the slight non-uniformity observed in the cumulative distribution function (CDF).

2. **Traffic Density and Delivery Time**: For traffic densities above approximately 0.05, the worst-case delivery time is rare. Most subflows are delivered within one matching cycle rather than one cycle plus one slot.

### Sequential Ordering and Path Selection

Subflows injected into RotorNet will be sequentially ordered between matching slots. However, during a single matching slot, subflows from the same flow may take both one-hop and two-hop paths, depending on the quantity and timing of the traffic committed by the application. If data sent over a two-hop subflow logically precedes data sent over a one-hop path, the data will arrive out of order at the receiver. To ensure ordered delivery, we assign the responsibility of reordering the data to a process at each end host. 

Given that all injected traffic is guaranteed to be delivered within \( N_m + 1 \) matching slots, the receiver only needs to store data received within a sliding window equal to this delivery bound. For a 100-Gb/s end host link, a 200-µs slot time, and \( N_m = 8 \), each host would require a receive buffer of 23 MB to ensure ordered delivery. This is a modest memory requirement considering the specifications of modern servers.

### Flow Completion Times

Flow completion time (FCT) is a critical metric for network performance, especially in services sensitive to long-tail FCT distributions [33]. While contemporary multi-rooted trees coupled with TCP can provide sufficient bandwidth on paper, FCT in such networks is subject to considerable variance. RotorNet improves on this by offering a choice between low FCT variance or significantly reduced mean FCT compared to Fat Trees, while eliminating the long tail present in Fat Trees.

To demonstrate this, we simulated a 10-Gb/s k = 4 three-level Fat Tree in ns-3 and an equivalently sized RotorNet using our flow-level simulator. Each host communicates with cross-rack hosts in an all-to-all pattern, with 10 flows of size 200 KB each, totaling 2,240 flows. Figure 10 shows the FCT in milliseconds for the Fat Tree and for greedy flow completion and fair-shared bandwidth strategies in RotorNet. Notably, the p100 for both RotorNet strategies is identical and 30% smaller than the p99.9 for the Fat Tree. Additionally, the p50 for greedy flow completion in RotorNet is 33% smaller than the Fat Tree. For the fair-shared bandwidth strategy, the variance (normalized to the mean) is just 0.6%, compared to 61% for the Fat Tree.

### Reducing FCT Variance in Fat Trees

To reduce FCT variance in a Fat Tree network, a network operator might consider operating the network at lower utilization levels. We deployed a 100-Mb/s k = 4 Fat Tree on Mininet [18] and tested the same traffic pattern, capping the flow size and rate to 10% and 20% of the nominal values. Reducing the utilization to 10% brings the mean-normalized variance down to 22% from 70%, but it remains well above that for fair-shared RotorNet. Thus, the low utilizations typical in real-world datacenter networks can be seen as an implicit duty-cycle cost to keep performance variations low.

### Buffering Requirements

Using the fluid-flow model from Section 7.1, we determine the amount of buffering required at each rack to support two-hop traffic under various traffic patterns. Figure 11a shows the worst-case and average buffering needed at each rack for various traffic densities, based on 100 randomly generated traffic patterns per density. In general, buffering can be expressed as the product of upward-facing ToR bandwidth and matching slot time. For 100-Gb/s links, a reconfiguration delay of 20 µs, and a duty cycle of 90%, the largest amount of required buffering occurs at a traffic density of approximately 0.02, resulting in each rack requiring 400 MB of memory for two-hop traffic. If this traffic were stored on end hosts instead of ToR switches, 12.5 MB per end host would be required, assuming 32 end hosts per rack.

Figure 11b shows a CDF of the per-rack buffer requirements for the worst-case traffic density. Increasing the loss rate to 0.01% only reduces the memory requirement by about 6%. This small reduction supports our decision to design RotorNet as a lossless fabric.

### Responsiveness to Changing Traffic Patterns

We assess how quickly RotorLB responds to changing traffic patterns using the fluid-flow model from Section 7.1. Figure 12 shows a typical time series of the aggregate throughput per matching cycle under RotorLB as we vary the traffic pattern. Upon changing the traffic pattern, RotorLB converges to the new sustained throughput within two matching cycles. This fast response is due to the fact that two-hop traffic is drained from intermediate queues after one matching cycle, allowing RotorLB to adapt to the new traffic pattern within two matching cycles.

### Fault Tolerance

Recall from Section 3 that each Rotor switch implements only a fraction of RotorNet’s connectivity, meaning the failure of one or more Rotor switches could lead to a significant reduction in overall connectivity. However, using the RotorLB-FT protocol described in Section 5.3, network connectivity is retained even under extensive failure conditions. Table 4 shows fairness metrics for RotorLB and RotorLB-FT under various failures. RotorLB-FT scores higher on Jain’s fairness index [19] than RotorLB by 20% and 30% for the cases where one-quarter and one-half of the Rotor switches fail, respectively. Critically, RotorLB-FT provides non-zero bandwidth to all flows in all failure scenarios, whereas RotorLB delivers zero bandwidth to some flows even in the case of a single link failure.

### Conclusions

Optical switching holds the promise of overcoming impending limitations in electrical packet switching, yet has faced resistance due to practical barriers. In this paper, we describe a network fabric based on Rotor switches, which decouples the control of optical circuit switches (OCSes) from the rest of the network, simplifying network control and deployment. The combination of these capabilities has the potential to meet next-generation datacenter bandwidth demands in a manner much simpler than existing approaches.

### Acknowledgments

We thank our shepherd, Mosharaf Chowdhury, the anonymous SIGCOMM reviewers, and Amin Vahdat for their useful feedback. We also thank Facebook for supporting this work through a gift, and the National Science Foundation for supporting this work through grants CNS-1629973, CNS-1314921, CNS-1553490, and CNS-1564185.

### References

[References are listed as provided in the original text.]

This optimized version of the text aims to be more coherent, clear, and professional, with improved structure and readability.