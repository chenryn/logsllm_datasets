Table 3: Summary of our real-time detection results for emails in our test window from Sep 1, 2013 - Jan 14, 2017 (1,232 days).
Rows represent the type/classiﬁcation of an alert following analysis by security staff members at LBNL. Columns 2–4 show alerts
broken down per attacker model (§ 5.2.2). Column 5 shows the total number of spearphishing campaigns identiﬁed by our real-time
detector in the numerator and the total number of spearphishing campaigns in the denominator. Out of 19 spearphishing attacks,
our detector failed to detect 2 attacks (one that successfully stole an employee’s credentials and one that did not); both of these
missed attacks fall under the previously unseen attacker threat model, where neither the username nor the email address matched
an existing entity.
7 known successful spearphishing attacks; this includes 1
spearphishing exercise, designed by an external security
ﬁrm and conducted independently of our work, that suc-
cessfully stole employee credentials. Additionally, mem-
bers of LBNL’s security team manually investigated and
labeled 15,521 alerts. We generated these alerts from a
combination of running (1) an older version of our detec-
tor that used manually chosen thresholds instead of the
DAS algorithm; and (2) a batched version of our anomaly
scoring detector, which ran the full DAS scoring proce-
dure over the click-in-email events in our evaluation win-
dow (Sep. 2013 onward) and selected the highest scoring
alerts within the cumulative budget for that timeframe.
From this procedure, we identiﬁed a total of 19
spearphishing campaigns: 9 which succeeded in stealing
an employee’s credentials and 10 where the employee
clicked on the spearphishing link, but upon arriving at
the phishing landing page, did not enter their creden-
tials.5 We did not augment this dataset with simulated
or injected attacks (e.g., from public blogposts) because
the true distribution of feature values for spearphishing
attacks is unknown. Even for speciﬁc public examples,
without actual historical log data one can only speculate
on what the values of our reputation features should be.
To evaluate our true positive rates, we ran our real-
time detector (§ 5.5) on each attack date, with a budget
of 10 alerts per day. We then computed whether or not
the attack campaign was ﬂagged in a real-time alert gen-
erated on those days. Table 3 summarizes our evaluation
results. Overall, our real-time detector successfully iden-
tiﬁes 17 out of 19 spearphishing campaigns, a 89% true
positive rate.
Of these, LBNL’s incident database contained 7
known, successful spearphishing campaigns (their inci-
dent database catalogues successful attacks, but not ones
that fail). Although our detector missed one of these suc-
cessful attacks, it identiﬁed 2 previously undiscovered at-
tacks that successfully stole an employee’s credentials.
The missed attack used a now-deprecated feature from
5A campaign is identiﬁed by a unique triplet of (cid:104)the attack URL,
email subject, and email’s From header(cid:105).
Figure 6: Histogram of the total number of daily alerts gener-
ated by our real-time detector (cumulative across all three sub-
detectors) on 100 randomly sampled days. The median is 7
alerts/day.
Dropbox [7] that allowed users to host static HTML
pages under one of Dropbox’s primary hostnames, which
is both outside of LBNL’s NIDS visibility because of
HTTPS and inherits Dropbox’s high reputation. This
represents a limitation of our detector: if an attacker can
successfully host the malicious phishing page on a high-
reputation site or outside of the network monitor’s vis-
ibility, then we will likely fail to detect it. However,
Dropbox and many other major ﬁle sharing sites (e.g.,
Google Drive) have dropped these website-hosting fea-
tures due to a number of security concerns, such as facil-
itating phishing. Ironically, in the speciﬁc case of Drop-
box, industry reports mention a large increase in phishing
attacks targeted against Dropbox users, where the phish-
ing attack would itself be hosted via Dropbox’s website
hosting feature, and thus appear to victims under Drop-
box’s real hostname [11]. Among the attacks that our de-
tector correctly identiﬁed, manual analysis by staff mem-
bers at LBNL indicated that our sub-detectors aptly de-
tected spearphish that fell under each of their respective
threat models (outlined in Section 2.1).
USENIX Association
26th USENIX Security Symposium    479
05101520253035X = total alerts per day05101520Num days6.2 False Positives and Burden of Alerts
At a daily budget of 10 alerts per day, our detector
achieved an average false positive rate of 0.004% (the
median number of emails per day is 263,086). How-
ever, as discussed earlier (§ 5.5), our real-time detector
is not guaranteed to produce exactly 10 alerts per day;
some days might have a burst of particularly suspicious
emails while other days might not have any unusual ac-
tivity at all. To evaluate the actual daily alert load, we ran
our real-time detector on one hundred randomly selected
days in our dataset and computed the total number of
alerts it generated on each day, shown in Figure 6. From
this histogram, we see that while our detector occasion-
ally generates bursts over our target budget, on the vast
majority of days (80%) it generates 10 or fewer alerts per
day; on nearly 20% of days, it generates no alerts.
During their manual investigation of the 15,521 alerts
created during our ground truth generation process,
LBNL’s security staff tracked how long it took them to
investigate these alerts. Surprisingly, LBNL’s security
staff reported that a single analyst could process an entire
month’s worth of alerts in under 15 minutes (and thus, on
average, under one minute to analyze one day’s worth of
alerts).
This rapid processing time arises because the analysts
were able to develop a two-pass workﬂow that enabled
them to quickly discard over 98% of the alerts, at a rate
of 2 seconds per alert; and then follow up with a more in-
depth analysis pass (e.g., analyzing detailed HTTP logs
and examining the full email headers) over the remain-
ing 2% of alerts, at a rate of 30 seconds per alert. The
ﬁrst pass is so fast because, for the vast majority of our
detector’s alerts, an analyst could quickly tell if an email
constituted a plausible spearphishing threat by inspect-
ing the Subject line, From line, and clicked URL of
the email. For over 98% of our alerts, this trio of in-
formation indicated that the email was highly unlikely
to contain a credential spearphishing attack. For exam-
ple, emails with subjects such as “Never Lose Your Keys,
Wallet, or Purse Again!” and “ATTN: Your Stomach Is-
sues FINALLY Explained. See Video Here” are surely
not spearphishing attacks.
While the more time-intensive 2% of alerts contained
mostly false positives (i.e., not spearphishing), the an-
alysts found two interesting classes of alerts. First, in
addition to detecting spearphishing attacks, our detector
identiﬁed 41 emails from regular phishing campaigns.
The analysts distinguished between regular phishing and
spearphishing by checking whether the email and HTTP
response from the clicked URL contained content that
was speciﬁcally targeted at LBNL. Second, ironically,
our detector generated 40 alerts where the person who
clicked on the link in the email was not one of the
email’s recipients, but rather a member of LBNL’s se-
curity staff. These clicks were part of routine investiga-
tions conducted by LBNL’s security staff; for example,
in response to a user reporting a suspicious email.
6.3 Anomaly Detection Comparisons
In Section 5.4 we introduced DAS, a simple new tech-
nique for anomaly detection on unlabeled data. Now,
we evaluate the effectiveness of DAS compared to tra-
ditional unsupervised anomaly detection techniques.
We tested three common anomaly detection tech-
niques from the machine learning literature: Kernel
Density Estimation (KDE), Gaussian Mixture Models
(GMM), and k-Nearest Neighbors (kNN) [5]. To com-
pare the real-time detection performance of each of
these classical techniques against DAS’s real-time per-
formance, we ran each of these classical techniques using
the same training and evaluation procedures we used for
our real-time detector’s evaluation. Speciﬁcally, given
the date of each of the 19 attacks and its impersonation
model, we extracted the same exact feature values for
all click-in-email events that occurred within a thirty
day window ending on the attack date; the thirty day
window reﬂected the size of our ComparisonSet. We
then normalized these feature values and ran each of the
three classical anomaly detection techniques on this set
of click-in-email events for each attack date. For quanti-
tative comparisons, we computed (1) the number of at-
tacks that would have been detected by each classical
technique if it used the same budget that our real-time
detector used and (2) the daily budget the classical tech-
nique would need to detect all of the attacks that our
DAS-driven detector identiﬁed.
Like other machine learning methods, these classical
algorithms require the user to set various hyperparame-
ters that affect the algorithm’s performance. For our eval-
uation, we tested each classical technique under a range
of different hyperparameter values and report the results
for whichever values gave the best results (i.e., compar-
ing DAS against the best-case version of these classical
techniques).
Table 4 summarizes the results of this comparative ex-
periment. All three traditional techniques detected fewer
than 25% of the attacks found by DAS. Moreover, in or-
der for KDE (the best performing classical technique) to
detect as many attacks as DAS, it would need a daily
budget nearly an order of magnitude larger than ours.
To illustrate why standard unsupervised techniques
perform so poorly, the two plots in Figure 7 show the
sender reputation features for a random sample of 10,000
lateral attacker click-in-email events. The left plot
shows the feature values for the actual alerts our DAS
detector generated (in red), while the right plot shows
the feature values for the alerts selected by KDE using
the same budget as our detector. KDE selects a mass
480    26th USENIX Security Symposium
USENIX Association
Figure 7: Both plots show the sender reputation feature values (scaled between [0, 1]) of a random sample of 10,000 lateral attacker
click-in-email events. Filled red points denote events that generated alerts within the daily budget by DAS (left-hand ﬁgure) and
KDE (right-hand ﬁgure).
Algorithm Detected Daily Budget
10
kNN
2,455
10
147
10
91
10
3/19
17/19
4/19
17/19
4/19
17/19
17/19
GMM
KDE
DAS (§ 5.4)
Table 4: Comparing classical anomaly detection techniques to
our real-time detector, on the same dataset and features. For
each of the standard anomaly detection algorithms, the ﬁrst row
shows the number of attacks detected under the same daily bud-
get as ours; the second row shows what the classical technique’s
budget would need to be to detect all 17 attacks that our real-
time detector identiﬁed on a daily budget of 10 alerts per day.
of points in the upper-right corner, which illustrates one
of limitations of standard techniques discussed in Sec-
tion 5.4: they do not take into account the directionality
of feature values. Because extremely large feature val-
ues occur infrequently, KDE ranks those events as highly
anomalous, even though they correspond to benign login
sessions where the user happened to login from a new
IP address in a residential city nearby LBNL. Second,
KDE selects a group of events in the bottom-right cor-
ner, which correspond to login sessions where an em-
ployee logged in from a city that they have frequently
authenticated from in the past, but where few other em-
ployees have logged in from. KDE’s selection of these
benign logins illustrates another limitation of standard
techniques: they often select events that are anomalous in
just one dimension, without taking into account our do-
main knowledge that an attack will be anomalous in all
dimensions. Even though the bottom-right corner repre-
sents employee logins where few other employees have
logged in from the same city, they are not suspicious, be-
cause that employee has previously logged in many times
from that location: they correspond to benign logins by
remote employees who live and work from cities far from
LBNL’s main campus. Thus, DAS can signiﬁcantly out-
perform standard unsupervised anomaly detection tech-
niques because it allows us to incorporate domain knowl-
edge of the features into DAS’s decision making.
7 Discussion and Limitations
Detection systems operate in adversarial environments.
While we have shown our approach can detect both
known and previously undiscovered spearphishing at-
tacks, there are limitations and evasion strategies that ad-
versaries might pursue.
Limited Visibility: Our detection strategy hinges on
identifying if an email’s recipient engaged in a po-
tentially dangerous action.
In the case of credential
spearphishing, LBNL’s network trafﬁc logs allowed us
to infer this behavior. However, our approach has two
limitations: ﬁrst, email and network activity conducted
outside of LBNL’s network borders will not get recorded
in the NIDS logs. Second, LBNL made a conscious
decision not to man-in-the-middle trafﬁc served over
HTTPS; thus, we will miss attacks where the email links
to an HTTPS website. Both of these are typical chal-
lenges that network-level monitoring faces in practice.
One strategy for alleviating this problem would be to use
endpoint monitoring agents on employee machines. Al-
ternatively, a detector could leverage SNI [23] to develop
USENIX Association
26th USENIX Security Symposium    481
0.00.20.40.60.81.0Logins-by-Sending-User from New IP Addr's City0.00.20.40.60.81.0#-Distinct-Users who've logged in from New IP Addr's City0.00.20.40.60.81.0Logins-by-Sending-User from New IP Addr's City0.00.20.40.60.81.0#-Distinct-Users who've logged in from New IP Addr's Cityits domain reputation for HTTPS and identify when users
visit potentially dangerous HTTPS domains.
In addition to limited network visibility, our detector
might miss attacks if a spearphishing email came from a
compromised personal email account. Since our detector
relies on access to a user’s prior login information to de-
tect lateral spearphishing attacks, it will not have the nec-
essary data to compute the features for this sub-detector.
To defend against this genre of lateral spearphishing,
one could leverage alternative sender reputation features,
such as ones based on stylometry [8, 19].
False Negatives and Evasion Strategies: Our detector
attempts to meet an upper-bound on the number of alerts
it generates. As a result, it might miss some attacks if
a number of successful spearphishing campaigns occur
on a given day; in effect, the clicks on URLs from the
campaigns earlier in the day will mask campaigns that
occur later on. To overcome this problem, the security
staff could increase the detector’s alert budget on days
with many attack alerts.
Aside from trying to mask one attack campaign with
another, an adversary could attempt to escape detection
by crafting an email whose domain or sender reputation
features are high. An attacker could boost her link’s do-
main reputation by compromising a frequently visited
website and using it to host the credential spearphish-
ing website. This strategy incurs greater costs to exe-
cute than modern-day attacks (where an adversary can
simply setup her own cheap phishing webpage), and it
is unclear whether such an attack would succeed if the
site does not normally ask for the employee’s corporate
credentials. For example, if an adversary compromises
a popular video website (e.g., netﬂix.com), many users
might ﬁnd it unusual for that popular domain to suddenly
start asking for the user’s enterprise credentials.
Alternatively, an attacker could attempt to inﬂate the
sender reputation features of their adversarial email be-
fore using it in an attack. For instance, to prepare a mali-
cious email address for a name spooﬁng attack, an adver-
sary could start sending emails with the malicious email
address and spoofed From name for several days before
sending a spearphishing email to the targeted recipient.
However, the more frequently this address is used, the
more the adversary risks someone detecting the adver-
sary’s use of a spoofed name; thus this evasion strategy
does incur a cost and risk to the attacker.
Future work could explore methods to make DAS
more robust. In particular, rather than treating an event
E as more suspicious than another event X only if E is
more suspicious than X in every dimension, the scoring
algorithm could be changed to treat E as more suspicious
if it is more suspicious than X in at least k dimensions.
Prior History for Feature Extraction: For each click-
in-email event, our detector leveraged 6 months of prior
log data in order to compute meaningful reputation fea-
tures. LBNL stores several years worth of logs, so this
amount of prior history was easily available for our de-
tector. However, with less historical data, the quality of
our detector might degrade (e.g., in the degenerate case
with no prior history, all From names and addresses will
appear as suspicious new entities). To assess how much
history our detector needs, we re-ran our evaluation ex-
periments (§ 6.1 and § 6.2) with 3 months of history and
with 1 month of history. A 3-month historical window
sufﬁced to detect the same attacks as our 6-month real-
time detector, and the median number of alerts per day
remained the same (7 per day). However, a detector with
only 1 month of history failed to detect one of the attacks
and generated a median of 18 alerts per day. With just
one month of prior data, too many click-in-email events
have the smallest possible feature values; this causes our