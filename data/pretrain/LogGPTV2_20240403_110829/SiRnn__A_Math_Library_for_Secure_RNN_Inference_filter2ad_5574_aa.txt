title:SiRnn: A Math Library for Secure RNN Inference
author:Deevashwer Rathee and
Mayank Rathee and
Rahul Kranti Kiran Goli and
Divya Gupta and
Rahul Sharma and
Nishanth Chandran and
Aseem Rastogi
6
8
0
0
0
.
1
2
0
2
.
1
0
0
0
4
P
S
/
9
0
1
1
.
0
1
:
I
O
D
|
E
E
E
I
1
2
0
2
©
0
0
.
1
3
$
/
1
2
/
5
-
4
3
9
8
-
1
8
2
7
-
1
-
8
7
9
|
)
P
S
(
y
c
a
v
i
r
P
d
n
a
y
t
i
r
u
c
e
S
n
o
m
u
i
s
o
p
m
y
S
E
E
E
I
1
2
0
2
2021 IEEE Symposium on Security and Privacy (SP)
SIRNN: A Math Library for Secure RNN Inference
Deevashwer Rathee∗
Microsoft Research
PI:EMAIL
Mayank Rathee∗
Microsoft Research
PI:EMAIL
Rahul Kranti Kiran Goli
Microsoft Research
PI:EMAIL
Divya Gupta
Microsoft Research
PI:EMAIL
Rahul Sharma
Microsoft Research
PI:EMAIL
Nishanth Chandran
Microsoft Research
Aseem Rastogi
Microsoft Research
PI:EMAIL
PI:EMAIL
Abstract— Complex machine learning (ML) inference algo-
rithms like recurrent neural networks (RNNs) use standard
functions from math libraries like exponentiation, sigmoid, tanh,
and reciprocal of square root. Although prior work on secure 2-
party inference provides specialized protocols for convolutional
neural networks (CNNs), existing secure implementations of
these math operators rely on generic 2-party computation (2PC)
protocols that suffer from high communication. We provide new
specialized 2PC protocols for math functions that crucially rely
on lookup-tables and mixed-bitwidths to address this perfor-
mance overhead; our protocols for math functions communicate
up to 423× less data than prior work. Furthermore, our math
implementations are numerically precise, which ensures that the
secure implementations preserve model accuracy of cleartext. We
build on top of our novel protocols to build SIRNN, a library
for end-to-end secure 2-party DNN inference, that provides the
ﬁrst secure implementations of an RNN operating on time series
sensor data, an RNN operating on speech data, and a state-
of-the-art ML architecture that combines CNNs and RNNs for
identifying all heads present in images. Our evaluation shows that
SIRNN achieves up to three orders of magnitude of performance
improvement when compared to inference of these models using
an existing state-of-the-art 2PC framework.
Index Terms—privacy-preserving machine learning; secure
two-party computation; recurrent neural networks; math func-
tions; mixed-bitwidths; secure inference
I. INTRODUCTION
In the problem of secure inference, there are two parties:
a server that holds a proprietary machine learning (ML)
model and a client that holds a private input. The goal is
for the client to learn the prediction that the model provides
on the input, with the server learning nothing about
the
client’s input and the client learning nothing about the server’s
model beyond what can be deduced from the prediction itself.
Theoretically, this problem can be solved by generic secure
2-party computation (2PC) [49], [115]. Recently, this area
has made great strides with the works of [5], [10], [17]–[20],
[25], [27], [32], [35], [37], [39], [47], [58], [64], [69], [73],
[83], [90]–[92], [99]–[102], [110] that have made it possible
to run secure inference on deep neural networks (DNNs).
Frameworks for secure inference like nGraph-HE [18], [19],
MP2ML [17], CrypTFlow [73], [99], and SecureQ8 [37]
go one step further and can automatically compile models
∗ The ﬁrst two authors have equal contribution.
the
cover
While
secure
such systems
trained in TensorFlow/PyTorch/ONNX to 2-party or 3-party
computation protocols secure against semi-honest adversaries.
inference of
some famous Convolutional Neural Networks (CNNs) (e.g.
ResNet [56], DenseNet [61] and MobileNet [105]) that ex-
clusively use simple non-linear functions such as ReLU and
Maxpool, other important architectures such as Recurrent
Neural Networks (RNNs) or architectures that combine RNNs
and CNNs [104] use math functions, such as exponentiation,
reciprocal square root, sigmoid and tanh, extensively. These
RNN-based architectures are the models of choice when deal-
ing with sequential or time series data like speech [36], [59],
[112]. Hence, for widespread adoption of secure inference,
especially in the RNN application domains, a robust support
for math functions is of paramount importance.
We focus on 2-party inference secure against semi-honest
adversaries1. In this setting, works that implement math func-
tions fall
into three categories. First, works that develop
general purpose math libraries [9], [66] using high-degree
polynomials. Second, works that use boolean circuits to im-
plement math functions [102]. Third, works that use ad hoc
piecewise linear approximations [83] that require developer
intervention for each dataset and each model
to balance
accuracy and latency, an unacceptable ask in the context of
automated frameworks for secure inference. All of these three
approaches rely on 2PC protocols from [41], [66], [115] and
suffer from huge performance overheads.
In this work, we design math functionalities that are both
provably precise and efﬁciently realizable via novel 2PC
protocols that we have developed. The performance of all
2PC implementations depend critically on the bitwidth. While
prior works use a uniform bitwidth for the whole inference,
our math functionalities use non-uniform (or mixed) bitwidths:
they operate in low bitwidths and go to high bitwidths only
when necessary. Hence, we have developed new protocols that
enable switching between bitwidths and operating on values
of differing bitwidths. Our 2PC protocols for math functional-
ities have upto 423× lower communication than prior works
(Section VI-A). We have implemented these in SIRNN2, a
1We relegate comparisons with works that need additional parties for
security, e.g., 3-party computation (3PC) to Section VII.
2Read as “siren”, SIRNN stands for Secure Inference for RNNs.
© 2021, Deevashwer Rathee. Under license to IEEE.
DOI 10.1109/SP40001.2021.00086
1003
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:28:57 UTC from IEEE Xplore.  Restrictions apply. 
library for end-to-end DNN inference, and evaluated on RNN-
based models. While we focus on math functions occuring in
RNNs, our recipe for designing math functionalities is general
and can be used in other contexts. Furthermore, our math
functionalities and non-uniform bitwidth protocols can also
be used in non-RNN contexts and are of independent interest.
sigmoid,
square root,
for exponentiation,
A. Results in detail
New approximations for math functions. In this paper,
we provide provably precise functionalities,
i.e. cleartext
implementations,
tanh, and
reciprocal of
that have been designed to
minimize cryptographic overheads. Exponentiation is used
in RBF kernels [55], sigmoid and tanh in RNNs with
LSTM [59] and GRU [36] cells, and reciprocal square root
in L2Normalization, where a vector u is scaled down to
. In a
a unit vector by multiplying each entry of u by
sharp departure from prior work in 2PC, our functionalities
follow the well-known paradigm of using lookup tables (LUT)
to get a good initial approximation of the math function
followed by an iterative algorithm such as Goldschmidt’s
iterations [50]
to improve upon this approximation. We
take inspiration from embedded systems [51], [63], [72],
[113] where the goal of minimizing memory consumption
low-bitwidth implementations based
has led to efﬁcient
on ﬁxed-point arithmetic. Our
functionalities manipulate
variables with different bitwidths to maintain precision while
using minimal bitwidths. Furthermore, we formally verify
that our functionalities provide precision guarantees similar
to those provided by standard math libraries (Section V-D).
1√
uT u
i.e., avoids
integer overﬂows. Similar
Novel 2PC Protocols. We provide efﬁcient protocols for
bitwidth switching (both extensions and truncations) and
operating on values with differing bitwidths so that our
secure implementations mimic the behavior of the cleartext