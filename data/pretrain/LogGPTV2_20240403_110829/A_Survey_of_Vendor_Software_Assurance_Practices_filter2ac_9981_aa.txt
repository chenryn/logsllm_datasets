title:A Survey of Vendor Software Assurance Practices
author:Jeremy Epstein
2009 Annual Computer Security Applications Conference
A Survey of Vendor Software Assurance Practices 
Jeremy Epstein 
SRI International 
Arlington, VA 
PI:EMAIL 
technology, there may be relatively little explicit interest (or 
understanding) from customers, thus reducing the perceived 
demand [4]. 
In  order  to determine  what  the  “best  practices”  are that 
we  should  follow,  we  did  an  informal  survey  of  software 
vendors  to  determine  how  they  achieve  software  security, 
what  motivated  them  to  put  energy  into  software  security, 
and related topics.  This determines the “typical practices”; 
by learning from the typical practices we hoped to determine 
best  practices  (i.e.,  as  the  least  upper  bound  of  typical 
practices).    This  paper  presents  the  results  of  this  study, 
along  with  its  limitations.    The  paper  does  not  make 
recommendations  of  what  any  particular  vendor  should  do, 
but rather establishes the typical practices at this writing. 
We  make  no  claims  in  this  paper  that  our  survey  was 
complete or unbiased.  Rather, it was intended to provide a 
sampling of the status quo (as of early 2008). 
the  confidence 
that  “assurance” 
The terms “software security” and “software assurance” 
are used interchangeably in this paper.  Jelen and Williams 
argue  [9] 
the 
correctness  of  an  assessment  (not  the  results  of  the 
assessment itself), so that a piece of software could have low 
assurance and high security (“we think it’s secure but don’t 
have proof”) or high assurance and low security (“we know 
it’s  insecure”).  However,  common  usage  is  that  “software 
assurance”  refers  to  methods  for  improving  the  security  of 
software, not for measuring the accuracy of an assessment. 
in 
is 
The  remainder  of  this  paper  is  organized  as  follows.  
Section  II  of  this  paper  describes  the  topics  investigated  in 
the  survey,  and  Section  III  describes  the  study  results.  
Section  IV discusses  questions  arising  from  the  survey  and 
presents areas for future work.   Section V summarizes the 
findings,  and  Section  VI  discusses  the  relationship  of  this 
study  to  security  maturity  models  including  OpenSAMM 
and BSIMM. 
II.  STUDY TOPICS AND LIMITATIONS 
Our  study  addresses  four  basic  questions:  who,  what, 
why, and when. 
A.  Who 
Who  in  the  organization 
is  involved  in  software 
assurance?  In particular, we wanted to know: 
•  Whether  there  is  a  centralized  security  person  or 
team, or whether responsibility is distributed to each 
engineering team 
•  Who has overall responsibility for software security, 
and where that person reports in the organization 
•  Whether  that  person  is  part  of  the  release  decision 
process, and if so whether they  have a veto (i.e., to 
(most  notably  Microsoft)  have 
Abstract— Books  and  articles frequently exhort developers to 
build  secure  software  by  designing  security  in.    A  few  large 
companies 
completely 
reengineered  their  development  process  to  include  a  focus  on 
security.  However, for all except the largest vendors, software 
security  (or  software  assurance) 
is  a  relatively  recent 
phenomenon, and one with an uncertain payoff.  In this paper, 
we examine what real vendors do to ensure that their products 
are reasonably secure.  Our conclusion is that software vendors 
put  significant  energy  into  software  security,  but  there  is 
significant variation in where they invest their money. 
Keywords: 
Software  Assurance, 
software 
security, 
commercial development practices. 
I. 
 INTRODUCTION 
Concern  that  software  products  are  (in)secure  has  been 
around  for  more  than  three  decades,  but  until  relatively 
recently was given little attention by the vendor community.  
The  never-ending  series  of  vulnerabilities  in  Microsoft 
software  galvanized  Microsoft,  and  resulted  in  a  security-
focused lifecycle [8].  Numerous other texts have described 
the risks  of  insecure  software, including [1], [10] and  [17].  
More  recently,  an  industry  consortium  has  been  formed  by 
some  of  the  larger  software  companies  to  define  best 
practices for building secure software [14]. 
Building  on  the  demand,  start-up  companies 1  have 
developed  tools  to  help  identify  security  flaws  using 
techniques  such  as  source  code  analysis  (e.g.,  Fortify 
Software,  Coverity),  binary  code  analysis  (e.g.,  Veracode), 
dynamic  testing  (e.g.,  SPI  Dynamics 2 ,  Watchfire 3 ,  NT 
Objectives,  Cenzic),  as  well  as  service-focused  companies 
that  perform  scheduled  scans  (e.g.,  Qualys,  White  Hat 
Security),  education  and  engineering  analysis  (e.g.,  Aspect 
Security,  Cigital),  or  penetration  testing  (e.g.,  Matasano 
Security). 
In  addition  to  the  commercial  companies,  numerous 
research  groups  have  developed  tools  to  provide  similar 
capabilities, sometimes more advanced than the commercial 
products, and other times less sophisticated. 
Given  the  choices,  software  vendors,  especially  those 
is  not  security,  have  difficulty 
whose  primary  focus 
determining  where  to  invest  their  resources.    Additionally, 
for  vendors  whose  primary  products  are  not  security 
1 Inclusion in this non-comprehensive list here should not be interpreted as 
an endorsement by the author or his employer.  Some of the vendors listed 
here offer products and/or services in addition to those in the list. 
2 Acquired by Hewlett-Packard Company. 
3 Acquired by IBM Corporation. 
1063-9527/09 $26.00 © 2009 IEEE
DOI 10.1109/ACSAC.2009.56
528
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:04:49 UTC from IEEE Xplore.  Restrictions apply. 
prevent  a  product  from  being  released  if  there  are 
significant security flaws) 
B.  What 
What  does 
to  gain  software 
assurance?    In  particular,  we  wanted  to  know  whether  the 
organization: 
the  organization  do 
• 
• 
• 
• 
Performs  threat  modeling  to  determine  the  risk 
factors 
Performs  security  design  reviews  to  try  to  avoid 
security problems 
Performs  source  code 
automated ) to find implementation flaws 
Performs  automated  scans  (including,  but  not 
limited  to,  input  fuzzing)  to  find  implementation 
flaws 
(manual 4  or 
reviews 
• 
•  Uses  penetration  testing  (either  in-house  or  third-
to  search  for  more  subtle  design  or 
party) 
implementation vulnerabilities  
Provides  developer  training  (and  if  so,  how  much 
and  how  frequently)  so  developers  can  avoid 
introducing implementation flaws 
•  Has an indication (whether by gut feel or metrics) as 
to which technique(s) are most effective in reducing 
or eliminating software flaws  
C.  Why 
Why  does  the  organization  have  a  software  assurance 
program?  For example: 
• 
Is  the  interest  in  software  assurance  due  to  direct 
customer  demand,  avoiding  notoriety,  government 
regulation, etc?   
•  How  often  do  customers  ask  about  software 
assurance?  Or do they just expect it's there? 
• 
•  What  words  do  customers  use  when  asking  about 
software assurance? 
Is  the  organization  seeing  procurement  language 
that asks about security? 
•  Do  customers  or  3rd  parties  (e.g.,  self-styled 
“security  researchers”)  test  the  vendor’s  products 
for security? 
D.  When 
When  did  the  organization  start  to  focus  on  software 
assurance, and how long did it take to see results? 
E.  Vendors Included and Excluded 
Our  study  focused  exclusively  on  vendors  of  shrink-
wrapped software.  We deliberately eliminated several other 
types of software developers that might be interesting: 
•  Custom  software  developers.    Custom  software  is 
driven  by  specific  customer  requirements,  and  not 
by the  need to find the common  set of capabilities 
that  meet  the  common  needs  of  a  large  set  of 
4 “Manual” code review was not defined to the participants, but based on 
comments  during  the  interviews  was  interpreted  as  “not  using  a  tool 
specifically  designed  to  find  security  flaws”.    Thus,  use  of  “grep”  or 
Eclipse  could  be  part  of  manual  code  review;  “findbugs”  would  be 
automated review. 
• 
• 
  This  category 
customers.    As  such,  software  assurance  may  be 
given  more  or  less  emphasis,  depending  on  the 
particular  customer. 
includes 
companies  that  primarily  develop  software  for  the 
including  GOTS 5 
government  marketplace, 
(Government Off The Shelf). 
Systems integrators.  Similar to the custom software 
developers,  these  vendors  are  driven  by  specific 
customer  requirements,  and  not  by  the  goal  of 
offering shrink-wrapped software. 
Software  as  a  service.    While  companies  like 
Salesforce.com  and  WebEx.com  have  significant 
security  concerns,  they  are  not  (generally)  selling 
their software, but rather use of that software.  This 
would  be  a  logical  area  to  extend  the  survey,  as 
these  vendors  are  most  similar  to  the  shrink-
wrapped software market, and are most at-risk due 
to their products being publicly exposed. 
  E-commerce  vendors  such  as 
Amazon.com have significant software investments, 
and are at significant risk.  However, software is not 
their  primary  business,  but  rather  a 
to 
accomplish their mission. 
tool 
•  E-commerce. 
•  Very  small  vendors.    Unless  they  are  specifically 
focused on security, there is little real motivation or 
ability  for  them 
to  put  energy  into  software 
assurance, although their products may be at risk. 
•  Embedded  systems  vendors  (e.g.,  for  medical 
instruments,  cash  registers).    Because  these  are 
more likely to run in a constrained environment, and 
for some categories are more subject to regulation, 
we did not consider them a useful comparison to our 
environment. 
•  Direct  competitors  to  the  author’s  employer.    We 
wouldn’t expect cooperation from our competitors, 
as 
that  we  are  gathering 
information to use against them. 
they  might  believe 
•  Open source projects.  There is no technical reason 
why  they  could  not  be  part  of  the  study,  but  our 
business goal was to understand the shrink-wrapped 
commercial software market. 
Of  course,  some  companies  fit  in  more  than  one 
category.  For those, we made an arbitrary decision whether 
to include them in our survey. 
Our emphasis was on medium to large software vendors.  
We  specifically  did  not  seek  vendors  who  are  primarily 
focused on selling security products such as firewalls, IDS, 
PKI, etc., although some of those vendors are in our sample. 
The  list  of  target  vendors  was  selected  by  reviewing  a 
list  of  the  top  500  software  vendors  [16] 6 and  removing 
those who met one or more of the exclusions listed above.  
From the remaining list, the author focused initially on those 
vendors  where  he  knew  one  or  more  employees.    These 
employees were usually, but not always, security specialists.  
5 As  distinguished  from  COTS,  or  Commercial  Off  The  Shelf  software, 
which  is  what  the  commercial  software  industry  calls  “shrink  wrapped” 
software. 
6 This list is admittedly dated, but for purposes of this study was adequate. 
529
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:04:49 UTC from IEEE Xplore.  Restrictions apply. 
In each case, the author asked his contacts for the name of 
the person or people responsible for software assurance.  In 
most  cases,  the  author  was  able  to  identify  an  appropriate 
person,  and  in  most  cases,  the  vendors  supplied  the 
information requested in the form of a telephone interview. 
Because the author started with those vendors where he 
had  contacts,  the  list  of  targeted  vendors  is  somewhat 
skewed.  Most of the author’s professional peers are in the 
security  business,  and  he  knows  many  people  in  the 
industry.  Thus, if the author does not have any contacts in a 
vendor,  it  may  be  an  indication  that  the  vendor  does  not 
have  a  focus  on  security.    To  reduce  this  bias,  the  author 
reviewed  lists  of  attendees  at  security  conferences  to 
identify  security  specialists,  and  attempted  to  contact 
vendors  through  those  security  specialists.    In  some  cases, 
targets  were  identified  through  social  networks  such  as 
LinkedIn.    These  methods  were  less  successful,  as  the 
personal contacts were more willing to be forthcoming than 
people who did not know the author and therefore, had no 
reason to trust him. 
We  specifically  excluded  Microsoft  from  this  survey, 
because  their  security  processes  are  well  known  and  have 
been  described  in  numerous  presentations  and  books, 
especially  [8].    Had  we  included  them,  their  results  would 
have shown that they use all of the techniques addressed in 
this  paper,  and  have  numerous  motivations  for  practicing 
software  assurance,  most  notably  the  impact  on  their 
reputation. 
F.  Why Not an Online Survey? 
When starting this survey, we considered using an online 
survey with questions that could be rated objectively, rather 
than the interview-style described in this paper.  However, 
there were several reasons we rejected an online survey. 
First, it was not clear at the beginning what the questions 
should be, and what range of answers to allow.  The scope 
of  software  assurance  is  broad,  and  we  learned  about 
techniques  and  involvement  as  we  went  through  the 
interviews  that  we  would  not  have  predicted,  and  hence 
would  not  have  been  included  in  a  questionnaire.    For 
example,  several  companies  included  software  security  as 
part  of  the  process  of  acquiring  other  companies,  and 
several  mentioned  the  importance  of  software  security  in 
third party and open source products. 
Second,  we  wanted  to  have  confidence  that  we  were 
getting  information  from  qualified  participants,  and  in 
particular not from more than one person in an organization.  
An  open  survey  would  have  made  this  difficult;  a  closed 
survey  (i.e.,  with  a  secret  URL)  might  have  allowed  one 
vendor to submit multiple sets of answers. 
Third,  we  wanted  to  assure  the  participants  that  their 
responses would be kept anonymous.  Doing this by phone 
calls (which  we did not record) gave greater confidence to 