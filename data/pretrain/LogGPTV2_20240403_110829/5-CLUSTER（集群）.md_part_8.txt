因此,vdb应该选用SSD(固态硬盘),如果vdb不是SSD可不用做缓存.或者 vdc
vdd也都是SSD,也没必要用vdb做缓存
本实验是为了练习，可以做
vdc vdd 为真正存储
# 2 案例2：ceph(分布式文件系统)集群
Linux持续不断进军可扩展计算空间，特别是可扩展存储空间。Ceph 最近加入到
Linux
中令人印象深刻的文件系统备选行列，它是一个分布式文件系统，能够在维护
POSIX 兼容性的同时加入了复制和容错功能。
Ceph是一种为优秀的性能、可靠性和可扩展性而设计的统一的、分布式文件系统。
1\. 可轻松扩展到数 PB 容量
2\. 支持多种工作负载的高性能（每秒输入/输出操作\[IOPS\]和带宽）
3\. 高可靠性
**系统架构**
Ceph 生态系统架构可以划分为四部分：
1.  Clients：客户端（数据用户）
2.  cmds：Metadata server
    cluster，元数据服务器（缓存和同步分布式元数据）
3.  cosd：Object storage
    cluster，对象存储集群（将数据和元数据作为对象存储，执行其他关键职能）
4.  cmon：Cluster monitors，集群监视器（执行监视功能）
2.1 问题
沿用练习一，部署Ceph集群服务器，实现以下目标：
1.  安装部署工具ceph-deploy
2.  创建ceph集群
3.  准备日志磁盘分区
4.  创建OSD存储空间
5.  查看ceph状态，验证
## 步骤一：部署工具软件
### 1）在node1安装部署工具，学习工具的语法格式。
\[root@node1 \~\]# yum -y install ceph-deploy
\[root@node1 \~\]# ceph-deploy \--help
ceph-deploy只是python脚本
### 2）创建目录
\[root@node1 \~\]# mkdir ceph-cluster #必须创建
\[root@node1 \~\]# cd ceph-cluster/ #ceph
的所有命令必须在这个目录里面执行
\[root@node111 ceph-cluster\]# ls #目前是空目录
## 步骤二：部署Ceph集群
### 1）创建Ceph集群配置。
**\[root@node1 ceph-cluster\]# ceph-deploy new node1 node2 node3**
Are you sure you want to continue connecting (yes/no)? yes #安装成功
报如下的错：
\[root@node1 ceph-cluster\]# ceph-deploy new node1 node2 node3
usage: ceph-deploy new \[-h\] \[\--no-ssh-copykey\] \[\--fsid FSID\]
\[\--cluster-network CLUSTER_NETWORK\]
\[\--public-network PUBLIC_NETWORK\]
MON \[MON \...\]
ceph-deploy new: error: hostname: node2 is not resolvable
原因：地址解析不对。解决：需要在部署主机上管理所有集群，本案例为node1，就在node1上配置hosts
如下：
**\[root@node1 ceph-cluster\]# cat /etc/hosts**
127.0.0.1 localhost localhost.localdomain localhost4
localhost4.localdomain4
::1 localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.4.11 node1
192.168.4.12 node2
192.168.4.13 node3
如需要node2 node3都成功，就都配置/etc/hosts 如上
\[root@node111 ceph-cluster\]# ls
ceph.conf ceph-deploy-ceph.log ceph.mon.keyring
#在当前目录里面创建的目录
\[root@node111 ceph-cluster\]# cat ceph.mon.keyring #用户和密码文件
\[mon.\]
key = AQBDcFJcAAAAABAAcYzYESIjc4nSbUcbkGLRiA==
caps mon = allow \*
\[root@node1 ceph-cluster\]# cat ceph.conf #ceph-deploy查看配置文件
\[global\]
fsid = ac0fd3ee-cd93-401a-bf52-71a5070e45bd
mon_initial_members = node1, node2, node3
mon_host = 192.168.4.11,192.168.4.12,192.168.4.13
auth_cluster_required = cephx
auth_service_required = cephx
auth_client_required = cephx
### 2）给所有节点安装软件包。
\[root@node1 ceph-cluster\]# ceph-deploy install node1 node2 node3
\[root@node111 \~\]# rpm -qa \|grep ceph
libcephfs1-10.2.2-38.el7cp.x86_64
ceph-osd-10.2.2-38.el7cp.x86_64
ceph-deploy-1.5.33-1.el7cp.noarch
ceph-base-10.2.2-38.el7cp.x86_64
ceph-mds-10.2.2-38.el7cp.x86_64 #启动文件系统共享
ceph-common-10.2.2-38.el7cp.x86_64
ceph-mon-10.2.2-38.el7cp.x86_64
ceph-selinux-10.2.2-38.el7cp.x86_64
python-cephfs-10.2.2-38.el7cp.x86_64
ceph-radosgw-10.2.2-38.el7cp.x86_64
#网关，做对象存储，装这个软件包其服务
#node1 node2 node3 都验证下所有是否安装成功
### 初始化所有节点的mon服务（主机名解析必须对）
**\[root@node1 ceph-cluster\]# ceph-deploy mon create-initial**
#启动所有集群的服务
**\[root@node1 ceph-cluster\]# systemctl status ceph-mon.target**
#查看mon是否正常
\[root@node111 ceph-cluster\]# ceph -s #查看
cluster de2d1fda-8abc-41c9-8e36-fb3579a1976a
health HEALTH_ERR
clock skew detected on mon.node222
64 pgs are stuck inactive for more than 300 seconds
64 pgs stuck inactive
no osds
Monitor clock skew detected
monmap e1: 3 mons at
{node111=192.168.4.11:6789/0,node222=192.168.4.12:6789/0,node333=192.168.4.13:6789/0}
election epoch 8, quorum 0,1,2 node111,node222,node333
osdmap e1: 0 osds: 0 up, 0 in #存储盘
flags sortbitwise
pgmap v2: 64 pgs, 1 pools, 0 bytes data, 0 objects
0 kB used, 0 kB / 0 kB avail
##3个mons osds:0个
常见错误及解决方法（非必要操作，有错误可以参考）：
如果提示如下错误信息：
\[node1\]\[ERROR \] admin_socket: exception getting command
descriptions: \[Error 2\] No such file or directory
解决方案如下（在node1操作）：
先检查自己的命令是否是在ceph-cluster目录下执行的！！！！如果时确认是在该目录下执行的create-initial命令，依然保存，可以使用如下方式修复。
\[root@node1 ceph-cluster\]# vim ceph.conf #文件最后追加以下内容
public_network = 192.168.4.0/24
修改后重新推送配置文件:
**\[root@node1 ceph-cluster\]# ceph-deploy \--overwrite-conf config push
node1 node2 node3**
## 步骤三：创建OSD
### 1）准备磁盘分区（node1、node2、node3都做相同操作）
\[root@node111 \~\]# lsblk
NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
sr0 11:0 1 1024M 0 rom
vda 252:0 0 20G 0 disk
├─vda1 252:1 0 1G 0 part /boot
└─vda2 252:2 0 19G 0 part
├─rhel-root 253:0 0 17G 0 lvm /
└─rhel-swap 253:1 0 2G 0 lvm \[SWAP\]
vdb 252:16 0 20G 0 disk
vdc 252:32 0 20G 0 disk
vdd 252:48 0 20G 0 disk
\[root@node111 ceph-cluster\]# parted /dev/vdb mklabel gpt
信息: You may need to update /etc/fstab.
\[root@node111 ceph-cluster\]# parted /dev/vdb mkpart primary 1M 50%
信息: You may need to update /etc/fstab.
\[root@node111 ceph-cluster\]# parted /dev/vdb mkpart primary 50% 100%
信息: You may need to update /etc/fstab.
\[root@node111 \~\]# lsblk
NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
sr0 11:0 1 1024M 0 rom
vda 252:0 0 20G 0 disk
├─vda1 252:1 0 1G 0 part /boot
└─vda2 252:2 0 19G 0 part
├─rhel-root 253:0 0 17G 0 lvm /
└─rhel-swap 253:1 0 2G 0 lvm \[SWAP\]
vdb 252:16 0 20G 0 disk
├─vdb1 252:17 0 10G 0 part
└─vdb2 252:18 0 10G 0 part
vdc 252:32 0 20G 0 disk
vdd 252:48 0 20G 0 disk
临时修改权限：
\[root@node1 ceph-cluster\]# chown ceph.ceph /dev/vdb1
\[root@node1 ceph-cluster\]# chown ceph.ceph /dev/vdb2
#给这两个分区赋予权限,用.和:作用一样
//这两个分区用来做存储服务器的日志journal盘
永久修改权限：新建70-vdb.rules文件
Udev文件:可以更改设备名(如网卡名),做链接.
\[root@node1 ceph-cluster\]# vim /etc/udev/rules.d/70-vdb.rules
ENV{DEVNAME}==\"/dev/vdb1\",OWNER=\"ceph\",GROUP=\"ceph\"
ENV{DEVNAME}==\"/dev/vdb2\",OWNER=\"ceph\",GROUP=\"ceph\"
解释:如果(ENV)发现(DEVBNAME)这个"/dev/vdb2"硬件,就将所有者改为"ceph",所属组改为"ceph"使得权限永久生效
### 2）初始化清空磁盘数据（仅node1操作即可）
\[root@node1 ceph-cluster\]# ceph-deploy disk zap node1:vdc node1:vdd
\[root@node1 ceph-cluster\]# ceph-deploy disk zap node2:vdc node2:vdd
\[root@node1 ceph-cluster\]# ceph-deploy disk zap node3:vdc node3:vdd
### 创建OSD存储空间（仅node1操作即可）
可在此处增加磁盘
\[root@node1 ceph-cluster\]# ceph-deploy osd create \\
node1:vdc:/dev/vdb1 node1:vdd:/dev/vdb2
//创建osd存储设备，vdc为集群提供存储空间，vdb1提供JOURNAL缓存，
vdd为集群提供存储空间，vdb1提供JOURNAL缓存，
//一个存储设备对应一个缓存设备，缓存需要SSD，不需要很大
#如果没有SSD(固态硬盘)做缓存,就只要 执行(ceph-deploy osd create
node1:vdc node1:vdd)即可
\[root@node1 ceph-cluster\]# ceph-deploy osd create \\
node2:vdc:/dev/vdb1 node2:vdd:/dev/vdb2
\[root@node1 ceph-cluster\]# ceph-deploy osd create \\
node3:vdc:/dev/vdb1 node3:vdd:/dev/vdb2
\[root@node1 \~\]# ceph osd tree
ID WEIGHT TYPE NAME UP/DOWN REWEIGHT PRIMARY-AFFINITY
-1 0.11691 root default
-2 0.03897 host node1
0 0.01949 osd.0 down 0 1.00000
1 0.01949 osd.1 down 0 1.00000
-3 0.03897 host node2
2 0.01949 osd.2 up 1.00000 1.00000
3 0.01949 osd.3 up 1.00000 1.00000
-4 0.03897 host node3
4 0.01949 osd.4 up 1.00000 1.00000
5 0.01949 osd.5 up 1.00000 1.00000
\[root@node1 \~\]# systemctl restart ceph-osd.target
\[root@node1 \~\]# ceph osd tree
ID WEIGHT TYPE NAME UP/DOWN REWEIGHT PRIMARY-AFFINITY
-1 0.11691 root default
-2 0.03897 host node1
0 0.01949 osd.0 up 1.00000 1.00000
1 0.01949 osd.1 up 1.00000 1.00000
-3 0.03897 host node2
2 0.01949 osd.2 up 1.00000 1.00000
3 0.01949 osd.3 up 1.00000 1.00000
-4 0.03897 host node3
4 0.01949 osd.4 up 1.00000 1.00000
5 0.01949 osd.5 up 1.00000 1.00000
### 4）常见错误（非必须操作）
使用osd create创建OSD存储空间时，如提示run
\'gatherkeys\'，可以使用如下命令修复：
\[root@node1 ceph-cluster\]# ceph-deploy gatherkeys node1 node2 node3
## 步骤四：验证测试
### 1) 查看集群状态
\[root@node111 ceph-cluster\]# ceph -s
cluster de2d1fda-8abc-41c9-8e36-fb3579a1976a
health HEALTH_WARN #WARN是警告 OK完全正常
clock skew detected on mon.node222 #提示node222时间不对,没同步
15 pgs peering
Monitor clock skew detected
monmap e1: 3 mons at
{node111=192.168.4.11:6789/0,node222=192.168.4.12:6789/0,node333=192.168.4.13:6789/0}
election epoch 8, quorum 0,1,2 node111,node222,node333