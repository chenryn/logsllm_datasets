4.7%
1.6%
1.2%
AP
98.9% 1.1%
98.7% 1.4%
99.4% 1.1%
Figure 9: Visualization of patches on small objects (upper:
original 416√ó416 images; lower: images with a 32√ó32 black
patch)
Brendel and Bethge showed that BagNet-17 with a 17√ó17 recep-
tive field can achieve a similar top-5 accuracy as AlexNet [3]. In
recent works [58, 65] on adversarial patch defense, BagNet has
been adopted to bound the number of corrupted features to achieve
robustness.
Clipping. In addition to the use of CNNs with small receptive fields,
we also need a secure aggregation mechanism to ensure that a small
number of corrupted features only have a limited influence on the
final prediction/classification. Recall that in our provable analysis
(Algorithm 2; Section 4), we need the lower bound of classification
logits to reason about the worst-case objectness map output. In
order to impose such a lower bound, we clip all feature values into
[0,‚àû] such that an adversarial patch cannot decrease the values of
Figure 10: Effect of patch size on provable robustness of De-
tectorGuard with a perfect clean detector
object classes significantly. It is easy to calculate the lower bound
of classification logits: we only need to zero out all features within
the patch location(s) and aggregate the remaining features.
Alternative aggregation. We propose DetectorGuard as a gen-
eral framework that is compatible with any provably robust image
classification technique. To further support this claim, we imple-
ment Objectness Predictor using a PatchGuard classifier with robust
masking secure aggregation [58], which achieves the best clean
classification accuracy and provable robust classification accuracy
on high-resolution ImageNet [11] dataset. We compare the perfor-
mance of defenses with clipping-base and robust-making-based
secure aggregation in Table 5. As we can see from the table, two
defenses achieve high clean performance and non-trivial provable
robustness, demonstrating that DetectorGuard is compatible with
different provably robust image classifiers. We note that we do not
0.000.020.040.060.080.100.120.140.160.180.200.220.240.260.280.300.320.340.360.380.400.420.440.460.48Object size (%)0.000.020.040.060.080.100.120.14% Objects0.000.020.040.060.080.100.120.140.160.180.200.220.240.260.280.300.320.340.360.380.400.420.440.460.48Object size (%)0.000.050.100.150.200.250.300.350.40% Objects0.000.020.040.060.080.100.120.140.160.180.200.220.240.260.280.300.320.340.360.380.400.420.440.460.48Object size (%)0.000.020.040.060.080.100.120.140.16% Objects10203040506070Patch size (px)5.07.510.012.515.017.520.022.5CR (%)CR-over-patchCR-close-patchSession 11D: Data Poisoning and Backdoor Attacks in ML CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea3192choose robust masking in our main evaluation because 1) it has a
looser lower bound compared with clipping while 2) it introduces a
slightly higher computation overhead. Furthermore, robust masking
of PatchGuard only has limited robustness against the multiple-
patch attacker. In contrast, as demonstrated in Appendix C, the
clipping-based DetectorGuard can handle multiple patches.
Table 6: Provable robustness (CR) of DetectorGuard (using a
perfect clean detector) against multiple patches (evaluated
on 50 PASCAL VOC images with a subset of patch locations)
far-patch
close-patch
over-patch
one 32√ó32 patch (1024 px)
two 32√ó32 patches (2048 px)
two 24√ó24 patches (1152 px)
two 16√ó16 patches (512 px)
27.3%
27.3%
27.3%
27.3%
22.4%
18.0%
18.6%
19.3%
8.7%
3.1%
3.1%
5.0%
C DISCUSSION ON MULTIPLE PATCHES
In our main body, we focus on the one-patch threat model be-
cause building a high-performance provably robust object detector
against a single-patch attacker is an unresolved and open research
question. In this section, we discuss DetectorGuard‚Äôs robustness
against multiple patches.
Quantitative analysis of clipping-based DetectorGuard against
multiple patches. One advantage of the clipping-based robust
classifier is its robustness against multiple patches. As long as the
sub-procedure RCH-PA(¬∑) of the clipping-based robust classifier
can return non-trivial bounds of classification logits, we can di-
rectly plug the sub-procedure into our Algorithm 2 to analyze the
robustness against multiple patches.
We note that despite the theoretical possibility to defend against
attacks with multiple patches, its quantitative evaluation for prov-
able robustness is extremely expensive due to the large number of
all possible combinations of multiple patch locations. Consider a
32√ó32 patch on a 416√ó416 image. There are 148k possible patch
locations (or 1.6k feature-space locations). If we are using 2 patches
of the same size, the number of all location combinations becomes
higher than 1010 (or 1.4M feature-space location combinations)!
In order to provide a proof-of-concept for defense against multi-
ple patches, we perform an evaluation on 50 PASCAL VOC images
using a subset of patch locations (1/16 of all location combinations).
The results are reported in Table 6. As shown in the table, Detec-
torGuard is able to defend against multiple patches. Moreover, if we
compare provable robustness against one 32√ó32 (1024 px) and two
24√ó24 patches (1152 px), which have a similar number of pixels, we
can find that using two smaller patches (two 24√ó24 patches) is only
more effective for over-patch threat model but not for far-patch and
close-patch threat models. This observation leads to the following
remark.
Remark: multiple patches need to be close to each other and
the victim object for a stronger malicious effect. Unlike image
classification where the classifier makes predictions based on all
image pixels (or extracted features), an object detector predicts
each object largely based on the pixels (or features) around the
object. As a result, patches that are far away from the object only
have a limited malicious effect, and this claim is supported by
our evaluation results in Section 5.4 (i.e., DetectorGuard is more
effective against the far-patch threat model). Therefore, multiple
patches should be close to the victim object and hence close to each
other for a more effective attack. In this case, the multiple-patch
threat model becomes similar to the one-patch model since patches
are close to each other and can merge into one single patch. That is,
we can use one single patch of a larger size to cover all perturbations
in multiple small patches.
D EXPERIMENT RESULTS FOR DIFFERENT
THREAT MODELS AND DATASETS
In this section, we include additional plots for per-class analysis as
well as DetectorGuard‚Äôs clean/provable performance on MS COCO
and KITTI. The observation is similar to that in Section 5.
Per-class Analysis. In Figure 11, we provide additional per-class
analysis results. The observation is similar to Figure 6 in Section 5.
Additional plots for MS COCO and KITTI. We plot the clean
performance and the provable robustness for MS COCO in Fig-
ure 12 and Figure 13, and for KITTI in Figure 14 and Figure 15.
The observation is similar to that on PASCAL VOC (Figure 4 and
Figure 5).
E JUSTIFICATION FOR DEFENSE OBJECTIVE
In Section 2.3, we allow DetectorGuard to only detect part of the
object or to trigger an attack alert on adversarial images. In this
section, we discuss why this is a reasonable defense objective and
how to extend DetectorGuard for a stronger notion of robustness.
Partially detected bounding box. We note that we allow the
patch to be anywhere, even over the salient object. As a result, the
patch likely covers a large portion of the object (visualization exam-
ples include the right part of Figure 1 and Figure 9; see Appendix A
for more details of object sizes and patch sizes). Therefore, it is
reasonable to allow the model to output a smaller bounding box. If
we consider the application scenario of autonomous vehicles (AV),
partially detecting a pedestrian or a car is already sufficient for an
AV to make a correct decision.
Moreover, we can tune hyper-parameters such as binarizing
threshold ùëá to increase the objectness in the output of Objectness
Predictor. More objectness will force the adversary to let Base De-
tector predict a larger bounding box in order to reduce unexplained
objectness that will otherwise lead to an attack alert. However,
we note that more objectness also makes it more likely for Detec-
torGuard to trigger a false alert on clean images. This trade-off
between robustness and clean performance should be carefully
balanced.
F ADDITIONAL DISCUSSION ON ‚ÄúFREE"
PROVABLE ROBUSTNESS
As shown in our evaluation, DetectorGuard achieves the first prov-
able robustness for object detectors again patch hiding attacks at a
negligible cost of clean performance. Intriguingly, we have demon-
strated that we can use a module with limited clean performance
(i.e., provably robust image classifier in Objectness Predictor) to
build a provably robust system with high clean performance (i.e.,
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea3193Figure 11: Per-class analysis of PASCAL VOC (left: far-patch; middle: close-patch; right: over-patch)
Figure 12: Clean performance of DetectorGuard on MS
COCO
Figure 14: Clean performance of DetectorGuard on KITTI
Figure 13: Provable robustness of DetectorGuard on MS
COCO
DetectorGuard). In this section, we provide additional discussion
on this intriguing behavior.
One major difference between image classification and object
detection is their type of error. For an image classifier, the only
error is misclassification. In contrast, an object detector can have
Figure 15: Provable robustness of DetectorGuard on KITTI
two types of errors, false-negative (FN; missing object) and false-
positive (FP; predicting incorrect objects). Intriguingly, despite the
difficulty to have a low FN and a low FP at the same time, it is
easy to have a low FN (but with a potentially high FP) or a low FP
(but with a potentially high FN). For example, if an object detector
predicts all possible bounding boxes, the FN is zero (all possible
boxes are retrieved, including the ground-truth box) but the FP is
extremely high (most bounding boxes are incorrect). This intrinsic
aeroplanebicyclebirdboatbottlebuscarcatchaircowdiningtabledoghorsemotorbikepersonpottedplantsheepsofatraintvmonitor010203040506070CR (%)051015202530Object size (%)PCD-farAverage object sizeaeroplanebicyclebirdboatbottlebuscarcatchaircowdiningtabledoghorsemotorbikepersonpottedplantsheepsofatraintvmonitor010203040506070CR (%)051015202530Object size (%)PCD-closeAverage object sizeaeroplanebicyclebirdboatbottlebuscarcatchaircowdiningtabledoghorsemotorbikepersonpottedplantsheepsofatraintvmonitor010203040506070CR (%)051015202530Object size (%)PCD-overAverage object size102030405060708090100Recall (%)0102030405060708090100Precision / FAR (%)Precision-PCD-vanillaPrecision-PCD-defendedPrecision-YOLO-vanillaPrecision-YOLO-defendedPrecision-FRCNN-vanillaPrecision-FRCNN-defendedFAR-PCD-defendedFAR-YOLO-defendedFAR-FRCNN-defended102030405060708090100Clean Recall (%)0.02.55.07.510.012.515.0Certified Recall (%)PCD-farPCD-closePCD-overYOLO-farYOLO-closeYOLO-overFRCNN-farFRCNN-closeFRCNN-over102030405060708090100Recall (%)0102030405060708090100Precision / FAR (%)Precision-PCD-vanillaPrecision-PCD-defendedPrecision-YOLO-vanillaPrecision-YOLO-defendedPrecision-FRCNN-vanillaPrecision-FRCNN-defendedFAR-PCD-defendedFAR-YOLO-defendedFAR-FRCNN-defended102030405060708090100Clean Recall (%)51015202530Certified Recall (%)PCD-farPCD-closePCD-overYOLO-farYOLO-closeYOLO-overFRCNN-farFRCNN-closeFRCNN-overSession 11D: Data Poisoning and Backdoor Attacks in ML CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea3194Figure 16: Visualization of DetectorGuard. From left to right: 1) clean images ‚Äì conventional detectors correct detect all objects; 2)
adversarial images with patches (marked with red dash boxes) ‚Äì conventional detectors miss all objects; 3) objectness map generated on these
adversarial images ‚Äì Objectness Predictor robustly predicts high objectness and eventually leads to an attack alert; 4) worst-case objectness
map ‚Äì DetectorGuard can certify the provable robustness of these objects. Note that visualizations in this figure only consider one random
patch location for each image, while results reported in Section 5 consider all possible locations and attack strategies within the threat model.
property of the object detection task allows us to achieve ‚Äúfree"
provable robustness.
Recall that in the clean setting, our design of prediction match-
ing strategy (Section 3.4) enables DetectorGuard to tolerate FN of
Objectness Predictor (i.e., Clean Error 3: predicts objects as back-
ground). Therefore, we can safely and easily optimize for a low FP
(i.e., Clean Error 2: predicts background as objects) to achieve a high
clean performance.
G PIXEL-SPACE AND FEATURE-SPACE
WINDOWS
Recall that in Section 3.3, we used a BagNet to extract a feature
map for the entire image and perform robust window classification
in the feature space. This design allows us to reuse the extracted
feature map and reduce computational overhead. In this section,
we discuss how to map the pixel-space bounding box to the feature
space.
Box mapping. For each pixel-space box (ùë•min, ùë¶min, ùë•max, ùë¶max),
we calculate the feature-space coordinate ùë•‚Ä≤
min = ‚åä(ùë•min ‚àí r +
1)/s‚åã, ùë¶‚Ä≤
min = ‚åä(ùë¶min ‚àí r + 1)/s‚åã, ùë•‚Ä≤max = ‚åäùë•max/s‚åã, ùë¶‚Ä≤max = ‚åäùë¶max/s‚åã,
where r, s are the size and stride of the receptive field size. The
new feature-space coordinates indicate all features that are affected
by the pixels within the pixel-space bounding box. We note that
the mapping equation might be slightly different given different
implementation of CNNs with small receptive fields. In our BagNet
implementation, we have r = 33, s = 8.
H VISUALIZATION OF DETECTORGUARD
In this section, we give a simple visualization for DetectorGuard
with YOLOv4 as Base Detector (Figure 16). To start with, we select
three random images with larger objects and visualize the detection
output of YOLOv4 in the first column. Second, we pick a random
patch location on the image and perform an empirical patch hid-
ing attack. The attack aims to optimize the pixel values within the
adversarial patch to minimize the objectness confidence score of ev-
ery possible bounding box prediction, which is a common strategy
used in relevant literature [57, 61]. As shown in the second column,
our patch attacks are successful, and YOLOv4 fails to detect any
objects. Note that we use red dash boxes to illustrate the patch
locations, and they are not the outputs of YOLOv4. Third, we feed
this adversarial image to Objectness Predictor, and we visualize
the predicted objectness maps in the third column. As shown in
the figure, although the adversarial patch makes YOLOv4 miss all
objects, Objectness Predictor still robustly outputs high objectness.
Clean imagesAdversarial imagesThe output of robust Objectness Predictor  Worst-case objectness map predictionSession 11D: Data Poisoning and Backdoor Attacks in ML CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea3195As discussed in Section 3, DetectorGuard will eventually issue an
attack alert. Fourth, we reason about the worst-case objectness map
prediction for these particular random patch locations used in the
visualization, and plot the worst-case output in the fourth column.
As shown in our visualization, Objectness Predictor can still output
high objectness in the worst case. Therefore, we can certify the
robustness of DetectorGuard for this patch location.
Finally, we want to note that this appendix is merely a simple case
study for an empirical patch attack at one single random location
of each image. In contrast, robustness results reported in Section 5
are derived from Algorithm 2 and Theorem 1 holds for any possible
patch attack strategy at any valid patch location.
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea3196