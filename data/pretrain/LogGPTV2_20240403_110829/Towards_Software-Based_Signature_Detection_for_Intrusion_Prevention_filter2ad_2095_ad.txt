The threshold mechanism described above is overly conservative. Threads that
have not reached the appropriate threshold when MErx wraps may still catch
up, e.g., if the remaining packets are all minimum-sized, or new packets are big,
and do not arrive at maximum rate). Moreover, it is possible to use threads more
eﬃciently, e.g., by not partitioning the traﬃc, but letting each thread process the
‘next available’ packet. We have chosen not to do so, because these methods re-
quire per-packet administration for such things as checking whether (a) a packet
is valid, (b) a packet is processed by a thread, and (c) a buﬀer slot is no longer
in use and may be overwritten. Each of these checks incurs additional overhead.
Instead, CardGuard needs a single check on eight counters at wrap time.
Packet processing. Each of the two MEh/t-MEac microengine pairs is respon-
sible for processing half of the packets. MEh/t is responsible for sanitising the
TCP stream, while MEac handles pattern matching.
For TCP ﬂow identiﬁcation, we use a hash table. The hash table contains
a unique entry for each TCP ﬂow, which is generated by employing the IXP’s
hardware assist to calculate a hash over the segment’s source and destination
addresses and the TCP ports. A new entry is made whenever a TCP SYN
packet is received. The number of ﬂows that may hash to the same hash value is
threshold (T)
new
being
processed unprocessed
State_0:
c = get_next_char (packet);
if (c == ’h’) goto State_1;
else if (c == ’Q’) goto State_36;
else if (c == ’t’) goto State_43;
else goto State_0;
four threads
MErx
MEh/t
MEh/t
MEac
MEac
Legend:
ME = microengine
rx = receive
ac = aho-corasick
h/t= hash/tcp
     reconstruct
State_1:
c = get_next_char (packet);
if (c == ’.’) goto State_2;
else if ...
...
(a) Processing by MErx, MEh/t, and MEac
(b) Inline processing
Fig. 3. Packet processing
114
H. Bos and K. Huang
a conﬁgurable parameter HashDim. If a new ﬂow hashes to an index in the table
which already contains HashDim ﬂows, the new ﬂow is conservatively dropped.
As a result, every live TCP ﬂow has a hash table entry, which records the
following information about the ﬂow: source IP address, destination IP address,
source port, destination port, next sequence number, and current DFA state. As
logically contiguous segments might be dispatched to diﬀerent packet processing
threads, the next sequence number ensures that segments are pattern-matched
in order, while keeping track of the current DFA state facilitates the resumption
of pattern matching of a subsequent packet at a later stage (e.g., by another
pkt-processing thread). As explained in the ﬁfth observation of Section 2.2, we
only need the current DFA state to resume scanning exactly where we left oﬀ.
When a non-SYN packet is received, the corresponding hash entry is found
and the sequence number of the packet is compared to the sequence number in
the table. As explained earlier, we do not permit segments to overwrite segments
that were received earlier. Any packet that is not the immediate successor to
the stored sequence number is put ‘on-hold’. There are two possible schemes
for dealing with such segments with which we have experimented. The ﬁrst,
and simplest, is to wait until all missing segments have arrived and only then
perform pattern matching. The second is to scan the segment for worms as an
individual packet and if it is considered safe, forward it to its destination, while
also keeping a copy in memory. Then, when the missing segments arrive, (part
of) the segment is scanned again for all signatures that may have started in the
segments preceding it and overlap with this segment. This is safe, even if the
segments that were forwarded were part of a worm attack. The reason is that
these packets by themselves do not constitute an attack. Only the addition of the
preceding packets would render the ‘worm’ complete. However, as the attack is
detected when the preceding packets arrive, these segments are never forwarded.
In the current implementation, a hash table entry is removed only as a result of
an explicit tear-down. The assumption that motivates this choice is that the FIN
and RST messages coming from the downstream host are never lost. However,
in the future we expect to incorporate a time-out mechanism that frees up the
hash-table entry while dropping the connection.
Also note that when CardGuard is started, all ﬂows that are currently active
are by necessity dropped, as they will not have hash entries in the new conﬁgu-
ration. Recently we have started implementing a mechanism that preserves the
original hash table.
Pattern matching. For pattern matching purposes, a thread on each MEac
reads data from SDRAM in 8-byte chunks and feeds it, one byte at a time, to
the Aho-Corasick algorithm. However, as the memory latency to SDRAM is in
the range of 33 to 40 cycles, such a naive implementation would be prohibitively
slow [21]. Therefore, in order to hide latency, CardGuard employs four threads.
Whenever a thread stalls on a memory access, a zero-cycle context switch is made
to allow the next processing thread to resume. As there are now eight packet
processing threads in CardGuard, the buﬀer is partitioned such that thread t is
responsible for slots t, t + 8, t + 16, . . .
Towards Software-Based Signature Detection for Intrusion Prevention
115
4.3 The Memory Hierarchy
We have explained in Section 3 that the IXP1200 has various types of memories
with diﬀerent speeds and sizes: registers, instruction store, scratch, SRAM and
SDRAM. Optimising the use of these memories proved to be key to CardGuard’s
performance. For instance, as CardGuard needs to access the DFA for every byte
in every packet, we would like the DFA to be stored in fast memory. However,
there are relatively few general purpose registers (GPRs) and scratch is both
small and relatively slow. Moreover, these resources are used by the compiler for
local variables as well.
For this reason, we make the following design decisions: (1) GPRs and scratch
are not used for storing the DFA, (2) instead, we exploit unused space in the
instruction store for storing a small part of the DFA, (3) another, fairly large,
part is stored in the 8 MB of SRAM, and (4) the remainder of the DFA is stored
in the 256 MB of slow SDRAM.
The idea is that, analogous to caching, a select number of frequently accessed
states are stored in fast memory, while the remainder is stored in increasingly
slow memory3. A premise for this to work, is that the Aho-Corasick algorithm
exhibits strong locality of reference. Whether this is the case depends both on
the patterns and on the traﬃc. Deﬁning level n in the DFA as all states that
are n transitions away from state 0, we assume for now that the top few levels
in the DFA, (e.g., states 0, 1, 36 and 43 in Figure 1) are accessed much more
frequently than the lower levels. In Section 5, we present empirical evidence to
support this.
Using the instruction store and ‘normal memories’ for storing the DFA, leads
to two distinct implementations of the Aho-Corasick algorithm itself, which we
refer to as ‘inline’ and ‘in-memory’. In an inline implementation, a DFA like the
one sketched in Figure 1 is implemented in the instruction store of a MEac, e.g.,
as a set of comparisons and jumps as illustrated in pseudo-code in Figure 3(b).
In-memory implementations, on the other hand, keep the DFA itself separate
from the code by storing it in one of the memories. The data structure that is
commonly used to store DFAs in Aho-Corasick is a ‘trie’ with pointers from a
source state to destination states to represent the transitions. In this case, a state
transition is expensive since memory needs to be accessed to ﬁnd the next state.
The overhead consists not only of the ‘normal’ memory latency, as additional
overhead may be incurred if these memory accesses lead to congestion on the
memory bus. This will slow down all memory accesses.
Note that each state in the inline implementation consists of several instruc-
tions and hence costs several cycles. We are able to optimise the number of
conditional statements a little by means of using the equivalent of ‘binary search’
to ﬁnd the appropriate action, but we still spend at least a few tens of cycles
at each state (depending on the exact conﬁguration of the DFA and the traf-
ﬁc). However, this is still far better than the implementation that uses SRAM,
as this requires several slow reads (across a congested bus), in addition to the
instructions that are needed to execute the state transitions.
3 It is not a real cache, as there is no replacement.
116
H. Bos and K. Huang
In spite of the obvious advantages, the inline version can only be used for a
small portion of the DFA, because of the limited instruction store of the micro-
engines. CardGuard is designed to deal with possibly thousands of signatures,
and the instruction store is just 1K instructions in size, so locality of reference
is crucial. In practice, we are able to store a few tens of states in the unused
instruction store, depending on the number of outgoing links. In many cases,
this is suﬃcient to store the most commonly visited nodes. For instance, we
are able to store in their entirety levels 0 and 1 of the 2025 states of snort’s
web IIS rules. In our experiments these levels oﬀer hit rates of the order of
99.9%. In Section 5, we will analyse the locality of reference in Aho-Corasick in
detail.
One may wonder whether, given 8 MB of SRAM, SDRAM is ever needed for
storing the DFA. Surprisingly, the answer is yes. The reason is that we sacriﬁce
memory eﬃciency for speed. For instance, if we combine all of snort’s rules
that scan traﬃc for signatures of at least ten bytes, the number of states in
the DFA is roughly 15k. For each of these states, we store an array of 256
words, corresponding to the 256 characters that may be read in the next step.
The array element for a character c contains the next state to which we should
make a transition if c is encountered. The advantage is that we can look up
the next state by performing an oﬀset in an array, which is fast. The drawback
is that some states are pushed to slow memory. Whether this is serious again
depends on how often reads from SDRAM are needed, i.e., on the locality of
reference.
The partitioning of the DFA over the memory hierarchy is the responsibil-
ity of CardGuard. The amount of SRAM and SDRAM space dedicated to DFA
storage is a conﬁgurable parameter. For the instruction store, there is no easy
way to determine how many states it can hold a priori. As a consequence, we are
forced to use iterative compilation of the microengine code. At each iteration,
we increase the number of states, and we continue until the compilation fails
because of ‘insuﬃcient memory’.
4.4 Alerts and Intrusion Prevention
When a signature is found in a packet, it needs to be reported to the higher-
levels in the processing hierarchy. For this purpose, the code on the microengines
writes details about the match (what pattern was matched, in which packet), in
a special buﬀer in scratch and signals code running on the StrongARM. In ad-
dition, it will drop the packet and possibly the connection. The StrongARM
code has several options: it may take all necessary actions itself, or it may
defer the processing to the host processor. The latter solution is the default
one.
4.5 Control and Management
The construction of the Aho-Corasick DFA is done oﬄine, e.g., on the host con-
nected to the IXP board. The DFA is subsequently loaded on the IXP. If the
Towards Software-Based Signature Detection for Intrusion Prevention
117
inline part of the Aho-Corasick algorithm changes, this process is fairly involved
as it includes stopping the microengine, loading new code in the instruction
store, and restarting the microengine. All of this is performed by control code
running on the StrongARM processor. In the current implementation, this in-
volves restarting all microengines, and hence a short period of downtime.
5 Evaluation
CardGuard’s use of the memory hierarchy only makes sense if there is suﬃ-
cient locality of reference in the Aho-Corasick algorithm when applied to ac-
tual traﬃc. Figure 4(a) shows how many times the diﬀerent levels in the Aho-
Corasick DFA are visited for a large number of diﬀerent rule sets for a 40
minute trace obtained from a set of 6 hosts in Xiamen University. We de-
liberately show a short trace to avoid losing in the noise short-lived ﬂuctua-
tions in locality. More traces (including longer-lived ones) are maintained at
www.cs.vu.nl/~herbertb/papers/ac_locality. Every class of snort rules of
which at least one member applied pattern matching (with a signature length
of at least ten characters, to make it interesting) was taken as a separate rule
set. We used the current snapshot of snort rules available at the time of writing
(September 2004). In total there were 22 levels in the DFA, but the number of
hits at levels 6-22 is insigniﬁcant and has been left out for clarity’s sake. The
ﬁgure shows the results for hundreds of rule sets with thousands of rules. The
line for the combination of all of snort’s rules is explicitly shown. The remaining
lines show the locality for each of snort’s rule types (e.g., web, viruses, etc.).
Since there are a great many categories, we do not name each separately. To
provide a thorough evaluation of Aho-Corasick in signature detection, we have
performed this experiment in networks of diﬀerent sizes (e.g., one user, tens,
hundreds, and thousands of users), for diﬀerent types of users (small depart-
ment, university campus, nationwide ISP) and in three diﬀerent countries. The
results show clear evidence of locality. The plot in Figure 4(a) is typical for all
our results.
It may be countered that these results were not obtained while the network
was ‘under heavy attack’ and that the plots may look very diﬀerent then. While
true, this is precisely the situation that we want to cater to. When the network
is so much under attack that locality of reference no longer holds, degrading net-
work performance is considered acceptable. Probably the network is degrading
anyway, and we would rather have a network that is somewhat slower than an
infected machine.
One of the problems of evaluating the CardGuard implementation is generat-
ing realistic traﬃc at a suﬃcient rate. In the following, all experiments involve
a DFA that is stored both inline and in-memory. As a ﬁrst experiment we used
tcpreplay to generate traﬃc from a trace ﬁle that was previously recorded
on our network. Unfortunately, the maximum rate that can be generated with
tcpreplay is very limited, in the order of 50Mbps. At this rate, CardGuard could
118
H. Bos and K. Huang
Table 1. Cycles required to
process a packet
packet size (bytes) cycles
64
300
600
900
1200
1500
976
9570
20426
31238