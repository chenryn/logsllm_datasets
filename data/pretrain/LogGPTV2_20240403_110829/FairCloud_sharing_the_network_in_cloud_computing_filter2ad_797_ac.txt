conserving allocation.
2 (i.e., ϵ on L1 and C2
For example, one can show that link proportionality fails to sat-
isfy even this restricted version of strategy-proofness. Consider the
example in Figure 6 consisting of two links with capacities C1, and
C2, respectively. Assume a link proportionality policy, where each
tenant has the same weight on each link, i.e., A1!A2, A3!A4 and
B1!B2 have all equal weights. Now assume the demand from A1 to
A2 is ϵ, while the other demands are in(cid:277)nite, and that C1 = UK(cRK),
Di;j
K = Di;j
can be either congested or not under DK), and
K , otherwise.
We say that an allocation satis(cid:277)es the utilization incentives property,
K (cid:21) c
for any monotonic utility function of K.5 In other words, tenant
K will only reduce her utility by decreasing her real demand on an
already uncongested link or on a link that becomes uncongested.
Note that the utilization incentives property is a particular case
c
5If a link L between i!j is congested for Di;j
Di;j
K , we
assume that the utility function is UK(RK) = jRKj; for general utility
functions it can be shown that this property is not achievable in this
case by any work-conserving policy, since K can increase her most
valued (cid:280)ows on link L.
K but not for
c
Figure 6: Link-independence vs. strategy-proofness: by increasing
the A1!A2 traﬃc, A may increase her A3!A4 traﬃc.
of the strategy-proofness property, in which a tenant can only lie by
reducing her demands on uncongested paths.
We have shown that network proportionality and congestion pro-
portionality violate this property. Almost all link-independent allo-
cations satisfy this property. Note that both work conservation and
utilization incentives properties are necessary to satisfy the high ut-
ilization requirement.
P4. Communication-pattern independence: e allocation of a
VM depends only on the VMs it communicates with and not on the
communication pattern. Consider a set of source VMs, Q that com-
municates with a set of destination VMs, P, where the VMs in Q and
P do not communicate with any other VMs. Assuming suﬃcient de-
mands, any communication pattern involving Q and P should result
in the same aggregate allocation.
It is not always possible to achieve this property at the network
level. For example, assume Q = fA1, A3g, P = fA2, A4g, and eﬀec-
tive bandwidths along A1!A2 and A3!A4 to be much higher than
that along A1!A4 and A3!A2 (e.g., due to diﬀerent link capacities
and/or background traﬃc). In this case, the (cid:277)rst communication
pattern (A1!A2, A3!A4) would receive a higher throughput than
the second (A1!A4, A3!A2).
Consequently, we consider a simpler formulation of this property
that is limited to a single congested link L. Some allocation policies
still cannot support this property. In Figure 4(a), assume Q = fA1,
A3g, P = fA2, A4g. Using Per-SD allocation, if A1 and A3 commu-
nicate with both A2 and A4, they get a larger share than a one-to-one
communication such as A1!A2 and A3!A4. Per-Source allocation
achieves this property since A1 and A3 will get 1
2 of L’s capacity re-
gardless of the communication pattern (given suﬃcient demand).
We note that link proportionality implies communication-pa-
ttern independence; otherwise, allocations will not remain propor-
tional when communication patterns change.
P5. Symmetry: If we switch the direction of all the (cid:280)ows in the
network, then the reverse allocation of each (cid:280)ow should match its
original (forward) allocation. More formally, assume G’s routing
is symmetric, and the capacities of every link are the same in both
directions. If the demand of each tenant is transposed (i.e., the de-
mand from i to j is equal to the original demand from j to i) and RT
K
is the resulting allocation for the transposed demands for tenant K,
then RT
K = RK 8K, where RK is the original allocation.
Existing allocation policies make an implicit assumption as to
whether the allocation is receiver- or sender-centric. However, it
is diﬃcult to anticipate application-level preferences. For exam-
ple, server applications might value more the outgoing traﬃc while
client applications might value more the incoming traﬃc. In the
absence of application-speci(cid:277)c information, we prefer allocations
that provide equal weights to both incoming and outgoing traﬃc.
As shown in §2, Per-Source and Per-Destination allocations do not
provide the symmetry property. Proportionality requirements im-
ply symmetry by de(cid:277)nition, since the share of a tenant does not de-
pend on the direction of its communication.
Figure 5 summarizes the desirable requirements, corresponding
properties, and the tradeoﬀs discussed in §2 and §3.
Min Guarantee Network-Proportionality High Utilization Congestion-Proportionality Link-Proportionality Symmetry Comm-Pattern Independence Work  Conservation Tradeoﬀ Implies Utilization Incentives L2 A3 A4 B2 B1 L1 A1 A2 4. PROPOSED ALLOCATION POLICIES
In the previous sections, we have described a set of requirements
and properties desired of a network allocation policy and identi(cid:277)ed
fundamental tradeoﬀs between them. In this section, we discuss
how to navigate the tradeoﬀs shown in Figure 5 and describe three
allocation policies that take diﬀerent stands on these tradeoﬀs.
e (cid:277)rst policy, PS-L, achieves link proportionality and can sat-
isfy all the properties mentioned in §3 (except strategy-proofness).
e second, PS-N, provides better proportionality at the network
level (congestion proportionality in a restricted setting), but it does
not fully provide utilization incentives. Finally, PS-P, provides min-
imum bandwidth guarantees in tree-based topologies (hence it does
not provide proportionality). At the end of the section, we discuss
how these policies can be implemented in practice as well as alter-
nate pricing models.
Table 3 summarizes the properties achieved by these three poli-
cies as well as by the traditional network sharing policies.
Heterogeneous VMs: Before presenting the allocation policies, we
(cid:277)rst remove the assumption of all VMs being identical. We gener-
alize to a model where tenants pay diﬀerent (cid:280)at-rate prices for in-
dividual VM’s network share. Just as today’s cloud providers oﬀer
VMs with diﬀerent CPU and memory con(cid:277)gurations at diﬀerent
prices, we consider a setting where each VM has a (positive) net-
work weight associated with it based on the tenant’s payment. us,
each VM is also characterized by its network weight, in addition to
its CPU and memory capacities. Intuitively, higher weights should
result in higher bandwidth allocations. It is not diﬃcult to extend li-
nk proportionality and min-guarantee to the heterogeneous model;
for brevity we discuss these extensions as part of the presentation of
our proposed allocation policies.
4.1 Proportional Sharing at Link-level
∑
∑
Proportional Sharing at Link-level (PS-L) is an allocation policy
that provides link proportionality. e simplest way to understand
PS-L is by considering a model in which each switch implements
weighted fair queuing (WFQ) and has one queue for each tenant.
e weight of the queue for tenant A on link L is the sum of the
weights of A’s VMs that communicate through link L. For instance,
let A’s VMs have a communication pattern such that a set of VMs
Q sends traﬃc to the set P over link L. en, A’s (unnormalized)
weight will be WA =
Y2P WY. Consider Figure 4(b)
and assume that all VMs have unit weights. Both tenants will be
assigned a weight of 3, leading to an equal bandwidth distribution
in both directions between them. In another example, on link L1 in
Figure 7, tenant A will have a weight of four since it has four VMs
communicating and tenant B will have a weight of two.
X2Q WX +
One drawback of the above version of PS-L is that there is a simple
strategy for tenants to increase their allocations. By sending an ϵ
amount of data between all her VMs, a tenant can achieve a weight
equal to her total number of VMs on any congested links. One (cid:277)x is
to simply use a weight for tenant A equal to the total weight of all of
A’s VMs, which can in fact be seen as an application of the NetShare
model [19]. Another (cid:277)x is to apply PS-L at a per VM granularity,
rather than per tenant, as we describe next.
In this case, PS-L assigns to a communication between VMs X
and Y on link L a weight of:
WX(cid:0)Y =
WX
NX
+
WY
NY
where NX is the number of other VMs X is communicating with on
link L (similarly NY). For example in Figure 4(c), the per VM PS-L
assigns weights of 1.5, 1.5, and 2 to A1 (cid:0) A2, A3 (cid:0) A2, and B1 (cid:0) B2,
respectively. So the (cid:280)ows between A1(cid:0)A2 and A3(cid:0)A2 would receive
Figure 7: Example for illustrating the allocation policies.
1:5
5 of the link capacity, since A2’s weight is split across its two (cid:280)ows.
is approach removes the incentives to send traﬃc between all of
one VMs.6
As discussed in §2, link proportionality only oﬀers a link level
view of proportionality, which can be far from approximating pro-
portionality at the network level. e next allocation policy aims to
address this concern.
4.2 Proportional Sharing at Network-level
+ WY
N′
Y
Proportional Sharing at Network-level (PS-N) is an allocation that
aims to approximate congestion proportionality and can achieve it
in a severely restricted setting. As a consequence, however, PS-N
does not provide full utilization incentives (Table 3).
e intuition behind PS-N is that the communication between
the VMs in a set Q has the same total weight through the network
irrespective of the communication pattern between the VMs in Q.
is weight equals the sum of the weights of the VMs in Q. For
example, in Figure 7, the total weight of each tenant A or B is four
regardless of the communication pattern between their VMs.
2 + 1 = 3
4 + 1 = 5
, with the diﬀerence that N′
To achieve this, we extend PS-L to incorporate information re-
garding the global communication pattern of a tenant’s VMs. PS-
N uses a weight model that is similar to that of PS-L, as it sets the
weight of the communication between VMs X and Y to be WX(cid:0)Y =
WX
X is the number of VMs that
N′
X
X communicates with across the entire network and not just over a
particular link. For example, in Figure 7, PS-N would provide A1-A3
with a weight of 1
4 on any link, while PS-L would provide
it a weight of 1
2 on link L5, because A1 communicates with
only two VMs on L5, but with four VMs in total.
PS-N strives to achieve proportionality in the absence of detailed
knowledge about the load on each bottleneck link, assuming uni-
form load conditions throughout the network. Speci(cid:277)cally, a set
of VMs Q communicating using PS-N across a set S of bottleneck
links achieves at least its proportional share of the total bottleneck
capacity if the demand on each VM-to-VM communication is high
enough and one of the following condition holds: (a) all bottleneck
links have the same capacity and background weight (i.e., weight of
traﬃc not involving VMs in Q), or (b) all congested links are pro-
portionally loaded, i.e., any two congested links of capacities C1 and
C2 are loaded with total weights W1 and W2 such that C1
C2
For example, assume that the network shown in Figure 7 is fully
provisioned, tenant A communicates all-to-all (i.e., each VM com-
municates with all other VMs of A), and tenant B communicates
= W1
W2 .7
6 Note that the properties exhibited by the described per VM PS-L
allocation are diﬀerent for diﬀerent demands. For example, in Fig-
ure 4(c), if the (cid:280)ow between A1 and A2 has a very small demand
ϵ, the allocation between A3-A2 and B1-B2 will respect the ratio of