title:The causes of path inflation
author:Neil T. Spring and
Ratul Mahajan and
Thomas E. Anderson
Quantifying the Causes of Path Inﬂation
Neil Spring
Ratul Mahajan
Thomas Anderson
{nspring,ratul,tom}@cs.washington.edu
Computer Science and Engineering
University of Washington
Seattle, WA 98195-2350
ABSTRACT
Researchers have shown that the Internet exhibits path inﬂation –
end-to-end paths can be signiﬁcantly longer than necessary. We
present a trace-driven study of 65 ISPs that characterizes the root
causes of path inﬂation, namely topology and routing policy choices
within an ISP, between pairs of ISPs, and across the global Inter-
net. To do so, we develop and validate novel techniques to in-
fer intra-domain and peering policies from end-to-end measure-
ments. We provide the ﬁrst measured characterization of ISP peer-
ing policies. In addition to “early-exit,” we observe a signiﬁcant
degree of helpful non-early-exit, load-balancing, and other poli-
cies in use between peers. We ﬁnd that trafﬁc engineering (the ex-
plicit addition of policy constraints on top of topology constraints)
is widespread in both intra- and inter-domain routing. However,
intra-domain trafﬁc engineering has minimal impact on path inﬂa-
tion, while peering policies and inter-domain routing lead to signif-
icant inﬂation. We argue that the underlying cause of inter-domain
path inﬂation is the lack of BGP policy controls to provide conve-
nient engineering of good paths across ISPs.
Categories and Subject Descriptors
C.2.1 [Communication Networks]: Architecture and Design—
topology
1.
INTRODUCTION
In this paper, we attempt to answer a simple question: why are
Internet paths sometimes absurdly long? We see quantifying the
causes of path inﬂation as a step toward the broader goal of under-
standing the factors that shape Internet routes. We start with the
observation that in a well-provisioned and well-operated network,
direct shortest paths are preferred. Intentionally longer paths re-
sult from the interaction of topology and policy at three layers –
in the selection of paths within an ISP, of peering links to reach
neighboring ISPs, and of the sequence of ISPs used to reach more
distant destinations. Thus, characterizing the extent to which dif-
ferent factors inﬂate paths helps us to gain insight into the design of
the network, routing protocols and ISP policies. For instance, we
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’03, August 25–29, 2003, Karlsruhe, Germany.
Copyright 2003 ACM 1-58113-735-4/03/0008 ...$5.00.
found that a large fraction of packets entering AT&T at San Fran-
cisco and destined for Sprint experienced signiﬁcant path inﬂation.
Closer investigation revealed that this was caused by routing policy
intended to avoid a congested peering link.
Over the past few years, researchers have found signiﬁcant path
inﬂation in the Internet [1, 11, 26, 32]. They have postulated pos-
sible causes and, in some cases, ruled out others [32], but no prior
work has completely explained the effect. This is challenging be-
cause most ISPs are unwilling to share accurate, up-to-date infor-
mation about their topology or routing policy, considering such
matters to be proprietary. Fortunately, advances in Internet topol-
ogy mapping [7, 15, 27] and ISP policy inference [12, 29] have
begun to make this information more widely available.
In this paper, we identify six possible causes of path inﬂation
– topology and routing policy at all three layers mentioned above
– and quantify the relative impact of each. We start by measur-
ing the POP-level backbone topology of 65 diverse ISPs and their
interconnections, by tracing from 42 vantage points to all globally-
routed IP address preﬁxes. We then develop new methods to infer
intra-domain routing policies and ISP peering policies (which de-
termine how ISPs exchange trafﬁc with their neighbors). Our key
insight is that both paths taken and not taken reveal information
about the routing policy. For intra-domain paths, we infer a set of
edge weights that are consistent with the paths observed in the trace
data. Between neighboring ISPs, we infer the use of early exit (“hot
potato”) and various forms of cooperative routing by focusing on
whether the chosen paths depend on the ingress in the upstream ISP,
the egress in the downstream ISP, or both. These results provide the
ﬁrst empirical data on ISP peering policies.
Using a trace-driven methodology, we analyze observed and hy-
pothetical topologies and policies to isolate the impact of the var-
ious factors on path inﬂation. We ﬁnd that i) intra-domain trafﬁc
engineering is commonplace but has only a minimal impact on path
inﬂation; ii) contrary to popular belief, there is signiﬁcant cooper-
ation (helpful non-early-exit) between adjacent ISPs in the Inter-
net, to avoid particularly poor routes or load-balance trafﬁc across
multiple peering links; iii) many paths that use early-exit are in-
ﬂated compared to a hypothetical optimal exit policy; iv) topology-
insensitive load balancing can cause signiﬁcant path inﬂation. We
also conﬁrm earlier work [26, 32], showing that roughly half of all
paths are inﬂated due to inter-domain routing; this inﬂation arises
not from commercial constraints, but from using AS-path length as
a routing metric. Because our study is trace-driven, our results are
limited to our choice of vantage points and ISPs, as well as by a
lack of visibility into provisioning and policy choices made below
the IP layer. Further work will be needed to determine the extent
to which our results generalize to other periods of time and other
vantage points.
s
h
t
a
p
f
o
n
o
i
t
c
a
r
f
e
v
i
t
a
l
u
m
u
C
1.0
0.8
0.6
0.4
0.2
0.0
0
1 ISP
1 & 2 ISP
overall
10
5
Additive inflation (ms)
15
20
25
Figure 1: Path inﬂation observed in our dataset as a cumulative
distribution. The x-axis represents the extra distance traveled
through the network beyond a hypothetical direct link between
end points. 1 ISP paths stay within one ISPs network; 1 & 2 ISP
paths traverse a maximum of two ISPs; overall is the complete
dataset.
Overall, our results show that particularly long paths are due nei-
ther to incompetent network engineering nor hypercompetition be-
tween ISPs. Instead, a main culprit is the design of BGP, the current
inter-domain routing protocol, that makes it difﬁcult to implement
robust topology-sensitive routing decisions. For instance, there is
no easy way for two adjacent ISPs to cooperate to implement opti-
mal exit routing, should they desire to do so.
In Section 2, we categorize the root causes of path inﬂation into
the six factors we study in the rest of the paper. Section 3 outlines
our measurement and analysis methodology. Section 4 focuses on
intra-domain topology and policy, Section 5 on the peering links
and policy between neighboring ISPs, and Section 6 on the impact
of inter-domain topology and policy on path inﬂation. We summa-
rize our results in Section 7, discuss related work in Section 8, and
conclude in Section 9.
2. THE CAUSES OF PATH INFLATION
While the direct optical ﬁber latency between San Francisco and
Boston is 20 ms, the usual one-way network delay is much more.
This is not a unique situation. Considering pairs of cities, Figure 1
illustrates the prevalence of path inﬂation in our data set relative to
a hypothetical direct link.
Our goal is to analyze the factors that cause inﬂation to better
understand the impact of network engineering and routing policy
on path selection. Network engineering, the design of the topology
of the network, can cause inﬂation if there are too few direct links
in the network graph. Routing policy can cause inﬂation because
existing shorter paths may not be selected to carry trafﬁc. Path in-
ﬂation provides a metric for study that allows relative comparisons
between these very different factors.
We characterize the contribution of topology and policy to path
inﬂation at each of the three component layers described below.
1. Intra-domain: The Internet is composed of thousands of
inter-connected but independent administrative domains known as
ISPs or autonomous systems (AS). “Intra-domain” refers to parts of
the network that belong to a single ISP. Both intra-domain topology
and policy can lead to path inﬂation; the combined impact of these
two factors is depicted by the line marked 1 ISP in Figure 1.
Path inﬂation occurs even in the simplest case, where both the
source and destination are inside the same ISP, because ISP net-
works do not provide a direct physical link between all pairs of
cities. The reasons behind this are largely economic – it is too
costly to connect all cities directly. In Section 4.1 we expand on
earlier work to map routers to their geographic locations [22, 27]
and use this data to analyze path inﬂation due to the presence and
absence of physical links. For instance, we observed single ISP
paths that went from Belgium to Singapore via the USA, indicating
the lack of more direct links connecting Europe and Asia.
Even when an intra-domain topology offers short paths between
two cities, they may not be taken due to trafﬁc engineering. The
goal of intra-domain trafﬁc engineering is to spread the load evenly
among the links in the topology [10], but it may lead to longer
paths. We show how to infer intra-domain routing policies in Sec-
tion 4.2, and we analyze the impact of intra-domain policy on path
inﬂation in Section 4.3.
2. ISP Peering: ISPs exchange trafﬁc at select peering points in
the network. Path inﬂation may result because packets are forced
to transit one of these peering locations, which may or may not be
“on the way” from the source to the destination. We analyze the
inherent cost of crossing this ISP boundary in Section 5.1.
Peering policy refers to how an upstream ISP transfers trafﬁc to a
downstream ISP. The upstream ISP may not choose a peering point
that optimizes latency. For instance, a common policy called early-
exit chooses the peering point closest to the source of the packet
(to minimize the cost incurred by the upstream ISP); the early exit
may take the packets in a direction opposite to the ultimate desti-
nation. Since little is (publicly) known about peering policies, in
Section 5.2 we develop techniques to measure the prevalence of
early-exit and other peering policies, and in Section 5.3 we apply
those techniques to measure the impact of policy on path inﬂation.
3. Inter-domain: Not all ISPs are directly connected to each
other. As a result, a path from source to destination may traverse
multiple intermediate ISPs. We analyze whether having to transit
multiple ISPs leads to additional inﬂation in Section 6.1.
It is well-known that inter-domain routing may not select the
best available paths [13, 26, 32]. Instead, the choice of paths is
heavily inﬂuenced by the policies of the source, intermediate, and
destination ISPs. We discuss common inter-domain policies, and
study their impact in Section 6.2.
3. METHODOLOGY
This section describes our methodology for measuring topology
and inferring routing policy. In summary, we collected a dataset
of 19 million traces from 42 measurement sources over three days
to discover 52,000 router IP addresses in the 65 ISPs we chose
to study.
ISP topologies are composed of backbones and POPs
(point of presence) [27]. Each POP is a physical location (city)
where the ISP houses a collection of routers, and the backbone
connects these POPs. We assigned each router IP address to its
respective POP and found 2,084 POPs in our dataset. In later sec-
tions, we analyze these POP-level topologies and traces. The raw
data and maps are available at http://www.cs.washington.edu/
research/networking/rocketfuel/.
3.1 Data Collection
As input for our analysis, we used traceroute data collected from
42 diverse PlanetLab vantage points [25], including sites in Aus-
tralia, Canada, Denmark, Italy, New Zealand, Sweden, and the
UK. This diversity is needed to get a complete picture of the net-
work [17]. From these vantage points, we traced to all of the
125,000 preﬁxes in the BGP routing tables of RouteViews [20],
which peers with sixty large ISPs. We used Scriptroute [28], a ﬂex-
ible network measurement facility, to collect the traces. Scriptroute
enabled us to speed up the trace collection process while reducing
the network load; it took us only 6 hours to collect traces from a
vantage point to all 125,000 preﬁxes.
We collected trace data for three consecutive days, December
18, 19 and 20, 2002. Having traces from three days was useful for
two reasons. First, vantage points that fail during trace collection
on one of the days contribute traces on another day to complete
the picture. Second, multiple datasets collected at different times
enable us to ﬁlter out transient paths that may be observed due to
instabilities such as failures. Since our focus in this work is to study
inﬂation under stable conditions, we would like to ﬁlter out the
impact of instabilities. Previous research has shown that transient
routing events generally do not last longer than a day [8, 19, 24].
3.2 Choosing ISPs to study
To make this detailed study of routing policy feasible despite the
size and complexity of the Internet, we carefully select individual
ISPs for study. We used three criteria for picking ISPs: i) the ISP
should be large enough to have interesting intra-domain and inter-
domain choices to make; ii) the ISP should carry enough diverse
trafﬁc (be a transit provider) so that its topology and routing policy
are easily observable using traceroutes (edge ISPs are much harder
to observe); and iii) the set of ISPs should be diverse in size and
geographic presence to show any resulting differences in topologies
and policies.
We used the following informal methodology for selecting ISPs
for detailed measurement. We ﬁrst classiﬁed each ISP in the BGP
tables to its tier [29]. Tiers represent how close to the Internet
“core” an ISP is; tier-1 ISPs are the closest. We selected all 22
tier-1 ISPs and 32 high degree tier-2 ISPs. An ISP’s degree is the
number of neighboring ISPs in the BGP tables. Since the degree
of an ISP is correlated to its size [34], high-degree ISPs are more
likely to have interesting routing policies. To this mix, we added
11 tier-3 ISPs. The ISPs we study are listed in Table 1. The table
shows that our selection process satisﬁed the three criteria listed
above: the chosen ISPs have diverse degrees and are geographically
diverse. After mapping these ISPs, we discovered that some have
a very small or no backbone, providing another dimension in ISP
diversity.
Even though we study a very small fraction of ISPs in the In-
ternet, we believe that our chosen subset is useful for studying the
impact of various factors on path inﬂation, because it includes most
large providers in the network. Put together, these ISPs account for
40% of the singly homed, globally routed IP address space, imply-
ing that a signiﬁcant fraction of Internet trafﬁc traverses this set of
ISPs. In fact, 97.7% of traces in our dataset traversed at least one
of these 65 ISPs.
3.3 Extracting Topology
We now describe how we process traceroute data to recover ISP
topologies. The ﬁrst task is to identify which routers belong to each
ISP using BGP and DNS. We use the BGP tables to distinguish the
IP address space of different ISPs and then verify that the DNS
name of the router matches the ISP’s naming convention. Address
space alone may blur the boundaries between ISPs and falsely place
an ISP’s customers and exchange points (a place where several ISPs
peer) into the ISP’s topology. For example, if Genuity runs an ex-
change point, the address of an interface on several non-Genuity
routers may be part of Genuity’s address space. DNS names pro-
vide conﬁrmation that the IP address indeed belongs to the ISP.
We also map routers to geographical location (POP) by inference
from their DNS names [22, 27]. For example, gbr3-p10.st6wa.
ip.att.net is the name of an AT&T router interface in Seattle
(st), Washington (wa). We build on Rocketfuel’s DNS name decod-
ing engine [27]. It takes a DNS name and the set of router name
Name
ATT-AP
Telia
ATT
Williams Comm
XO
IIJ
VSNL
Teleglobe
Colt
DTAG
Eqip
Telus
ATT-EMEA
Cable Wireless
Genuity
Global Crossing
One Data Network
Group Telecom
Sprint-Canada
Genuity-europe
Open Transit
Comindico
ATT-Canada
Bellnexxia
Globix
Level3
Netrail
Qwest
Sprint
UUNet
Verio
ASN
4637 Hong Kong Telecom
6453
8220
3320
3300
7176
5511
1299
7018
3561
1
3549
4513
3356
4006
209
1239
701
2914
7911
2828
2497
4725
4755
9942
15290
577
6539
3602
852
2686
5400
4589
13129
9070
174
3257
702
5669
15412
1668
7170
11537
11608
2548
1785
6395
16631
4544
5650
4565
1784
6939
10910
101
7132
4323
2687
7543
1221
6509
6467
3967
6461
3701
4600
12050
3582
Telstra
CANet
Espire
Exodus
MFN
Nero
ATT-Disc
Abilene
Accretive
Allegiance
Concert
Easynet
GATel
ITDNet
PSI
Tiscali
Appliedtheory
Broadwing
Vianw
Flag Tel
AOL
Internap
PNW-GPOP
SWBell
TWTelecom
Singapore Telecom
Cogent
Conxion
Eli
Epoch
Gnaps
Hurricane Electric
Oregon-GPOP
TLCT
University Oregon
UUNet-europe
Tier
1
1
1
1
1
1
1
1
1
1