2 q ď δ(cid:75)θ “ ∆(cid:74)f(cid:75)θp(cid:74)e’
(cid:74)∆D
f pe’
1 , e’
1(cid:75)θ,(cid:74)e’
2(cid:75)θq ď(cid:74)δ(cid:75)θ
Figure 7: Relational interpretation of assertions (added
rules)
6. EXAMPLES
In this section we show how we can use PrivInfer to guaran-
tee diﬀerential privacy for Bayesian learning by adding noise
on the input, noise on the output using (cid:96)1 norm, and noise
on the output using f -divergences. We will show some of
these approaches on three classical examples from Bayesian
learning: learning the bias of a coin from some observations
(as discussed in § 2), its generalized process, i.e the Dirich-
let/multinomial model and the learning of the mean of a
Gaussian. In all the example we will use pseudo code that
can be easily desugared into the language presented in § 4.
Indeed, the following examples have been type-checked with
an actual tool implementing PrivInfer. More examples can
be found in the supplementary material section.
6.1
Input perturbation
Input perturbation: Beta Learning.
Let’s start by revisiting the task of inferring the parameter
of a Bernoulli distributed random variable given a sequence
of private observations. We consider two lists of booleans
with the same length in the adjacency relation Φ iﬀ they
diﬀer in the value of at most one entry. We want to ensure
diﬀerential privacy by perturbing the input. A natural way
to do this, since the observations are boolean value is by
using the exponential mechanism. We can then learn the bias
from the perturbed data. The post-processing property of
diﬀerential privacy ensures that we will learn the parameter
in a private way.
Let’s start by considering the quality score function for
the exponential mechanism. A natural choice is to con-
sider a function score:boolÑboolÑ{0,1} mapping equal
booleans to 1 and diﬀerent booleans to 0. Remember that
the intended reading is that one of the boolean is the one to
which we want to give a quality score, while the other is the
one provided by the observation. The sensitivity of score
is 1. Using this score function we can then create a general
function for adding noise to the input list:
1.
2.
3.
4.
let rec addNoise db eps = match db with
| [] Ñ return ([])
| y::yl Ñ mlet yn = (expMech eps score y) in
mlet yln = (addNoise yl eps) in return(yn::yln)
1.
2.
3.
4.
let rec addNoise db eps delta = match db with
| [] Ñ return ([])
| y::yl Ñ mlet yn = (gaussMech (sigma eps delta) y) in
mlet yln = (addNoise yl eps delta) in return(yn::yln)
To this function we can give the following type guaranteeing
diﬀerential privacy.
tl :: B list | lŸ Φ lŹu Ñ t :: R` |“u Ñ M-D,0tb :: B list |“u
where we use tx :: T |“u as a shorthand for tx :: T | xŸ “
xŹu. We will use this shorthand all along this section.
The bulk of the example is the following function that
recursively updates the prior distribution and learns the ﬁnal
distribution over the parameter.
1. let rec learnBias dbn prior = match dbn with
2. | [] Ñ prior
3. | d::dbs Ñ observe
4.
5.
(fun r Ñ mlet z = ran bernoulli(r) in return (d=z))
(learnBias dbs prior)
The likelihood given in Line 4 is the formal version of the
one we presented in § 2. The function learnBias can be typed
in diﬀerent ways depending on what is our goal. For this
example we can assign to it the following type:
tl :: B list |“u Ñ MSD,0tx :: r0, 1s |“u Ñ MSD,0tx :: r0, 1s |“u
The reading of this type is that if learnBias takes two lists
of observations that are equal and two prior that are equal,
then we obtain two posterior that are equal. Thanks to
this we can type the occurrence of observe in line 3-4 using
a trivial assertion. Here we use the SD divergence but in
fact this would also hold for any other f P F. In particular,
this type allows us to compose it with addNoise using an
mlet. This type also reﬂects the fact that the prior is public.
We can then compose these two procedures in the following
program:
1. let main db a b eps = mlet noisyDB = (addNoise db eps)
2.
in return(infer (learnBias noisyDB (ran (beta(a,b)))))
Notice that in line 2 we use infer for learning from the noised
data. We can then assign to main the type
tl :: B list | lŸ Φ lŹu Ñ ta :: R` |“u Ñ tb :: R` |“u Ñ
t :: R` |“u Ñ M-D,0td :: Drr0, 1ss |“u
which guarantees us that the result is  diﬀerentially pri-
vate. Notice that the result type is a polymonadic type
over Drr0, 1ss. This because we are releasing the symbolic
distribution.
Input perturbation: Normal Learning.
An example similar to the previous one is learning the
mean of a gaussian distribution with known variance: kv,
from a list of real number observations—for instance some
medical parameters like the level of LDL of each patient. We
consider two lists of reals with the same length adjacent when
the (cid:96)1 distance between at most two elements (in the same
position) is bounded by 1. To perturb the input we may now
want to use a diﬀerent mechanism, for example we could use
the Gaussian mechanisms—this may give reasonable results
if we expect the data to come from a normal distribution.
Also in this case, the sensitivity is 1. The addNoise function
is very similar to the one we used in the previous example:
The two diﬀerences are that now we also have delta as input
and that in line 3 instead of the score function we have a
function sigma computing the variance as in Deﬁnition 3.4.
The inference function become instead the following.
| [] Ñ prior
| d::dbs Ñ observe (fun (r: real) Ñ
1. let rec learnMean dbn prior = match dbn with
2.
3.
4.
5.
6. let main db hMean hVar eps delta =
7.
8.
9.
return(infer (learnMean noisyDB
mlet noisyDB = (addNoise db eps delta)in
(ran (normal(hMean,hVar)))))
mlet z = ran normal(r, kv) in return (d=z))
(learnMean dbs prior) in
Composing them we get the following type guaranteeing
p, δq-diﬀerential privacy.
tl :: R list | lŸ Φ lŹu Ñ ta :: R` |“u Ñ tb :: R` |“u Ñ
t :: R` |“u Ñ tδ :: R` |“u Ñ M-D,δtd :: DrRs |“u
6.2 Noise on Output with (cid:96)1-norm
We present examples where the privacy guarantee is achieved
by adding noise on the output. For doing this we need to
compute the sensitivity of the program. In contrast, in the
previous section the sensitivity was evident because directly
computed on the input. As discussed before we can compute
the sensitivity with respect to diﬀerent metrics. Here we
consider the sensitivity computed over the (cid:96)1-norm on the
parameters of the posterior distribution.
Output parameters perturbation: Beta Learning.
The main diﬀerence with the example in the previous
section is that here we add Laplacian noise to the parameters
of the posterior.
let d = infer (learnBias db (ran beta(a,b))) in
let (aP, bP) = getParams d in
1. let main db a b eps=
2.
3.
4. mlet aPn = lapMech(eps, aP) in
5. mlet bPn = lapMech(eps, bP) in
6.
return beta(aPn, bPn)
In line 2 we use the function learnBias from the previous
section, while in line 4 and 5 we add Laplace noise. The
formal sensitivity analysis is based on the fact that the
posterior parameters are going to be the counts of true and
false in the data respectively summed up to the parameters
of the prior. This reasoning is performed on each step of
observe. Then we can prove that the (cid:96)1-norm sensitivity of
the whole program is 2 and type the program with a type
guaranteeing 2-diﬀerentially privacy.
tl :: B list | lŸ Φ lŹu Ñ ta :: R` |“u Ñ tb :: R` |“u Ñ
t :: R` |“u Ñ M2-D,0td :: Drr0, 1ss |“u
Output parameters perturbation: Normal Learning.
For this example we use the same adjacency relation of
the example with noise on the input where in particular the
number of observation n is public knowledge. The code is
very similar to the previous one.
1
kv2
`
hV 2 ` n
let mean = getMean mDistr in
˘´1. Notice that we only add noise
1.let main db hM hV kV eps =
2. let mDistr = infer (learnMean db (ran normal(hM,kV))) in
3.
4. mlet meanN = lapMech(eps/s mean) in
5.
let d = normal(meanN, uk) in return(d)
where uk “
to the posterior mean parameter and not to the posterior
variance parameter since the latter doesn’t depend on the
data but only on public information. The diﬃculty for
verifying this example is in the sensitivity analysis. By some
calculations this can be bound by s “ hV
kv`hV where kv is the
known variance of the gaussian distribution whose mean we
are learning and hV is the prior variance over the mean. We
use this information in line 4 when we add noise with the
Laplace mechanism. By using this information we can give
the following type to the previous program:
tl :: R list | lŸ Φ lŹu Ñ thM :: R |“u Ñ thV :: R` |“u Ñ
tkv :: R` |“u Ñ t :: R` |“u Ñ Ms-D,0td :: DrRs |“u
6.3 Noise on Output using f-divergences
We now turn to the approach of calibrating the sensitivity
according to f -divergences. We will consider once again
the example for learning privately the distribution over the
parameter of a Bernoulli distribution, but diﬀerently from
the previous section we will add noise to the output of the
inference algorithm using the exponential mechanism with
a score function using an f -divergence. So, we perturb the
output distribution and not its parameters.
We will use Hellinger distance as a metric over the output
space of our diﬀerentially private program, but any other
f -divergence could also be used. The quality score function
for the exponential mechanism can be given a type of the
shape:
tl :: B list | lŸ Φ lŹu Ñ td :: Drτs |“u Ñ tr :: R | |rŸ´rŹ| ď ρu
where the bound ρ express its sensitivity. Now we can use
as a score function the following program
score (db, prior) out = -(H (infer (learnBias db prior)) out)
This quality score uses a function H computing the Hellinger
distance between the result of the inference and a potential
output to assign it a score. The closer out is to the real
distribution (using Hellinger distance), the higher the scoring
is. If we use the exponential mechanism with this score we
achieve our goal of using the Hellinger to “calibrate the noise”.
Indeed we have a program:
let main prior obs eps = expMech eps score (obs, prior)
To which we can assign type:
MHD,0tx :: r0, 1s |“u Ñ t(cid:96) :: B list | (cid:96)Ÿ Φ (cid:96)Źu
Ñ t :: R` |“u Ñ Mρ-D,0td :: Drr0, 1ss |“u
Concretely, to achieve this we can proceed by considering
ﬁrst the code for learnBias:
4 “ ρ.
To have a bound for the whole learnBias we need ﬁrst to
give a bound to the diﬀerence in Hellinger distance that two
distinct observations can generate. This is described by the
following lemma.
a
Lemma 6.1. Let d1, d2 : B with d1Φd2. Let a, b P R`. Let
Prpξq “ Betapa, bq. Then ∆HDpPrpξ | d1q, Prpξ | d2qq ď
1 ´ π
Using this lemma we can then type the observe statement
with the bound ρ. We still need to propagate this bound
to the whole learnBias. We can do this by using the adja-
cency relation which imposes at most one diﬀerence in the
observations, and the data processing inequality Theorem 3.1
guaranteeing that for equal observations the Hellinger dis-
tance cannot increase. Summing up, using the lemma above,
the adjacency assumption and the data processing inequality
we can give to learnBias the following type:
tl :: B list | lŸ Φ lŹu Ñ MHD,0tx :: r0, 1s |“u
Ñ MHD,ρtx :: r0, 1s |“u
This ensures that starting from the same prior and observing
l1 and l2 in the two diﬀerent runs such that l1 Φ l2 we
can achieve two beta distributions which are at distance at
most ρ. Using some additional reﬁnement for infer and H we
can guarantee that score has the intended type, and so we
can guarantee that overall this program is pρ, 0q-diﬀerential
privacy.
The reasoning above is not limited to the Hellinger distance.
For instance the following lemma:
a
Lemma 6.2. Let d1, d2 : B with d1Φd2. Let a, b P R`.
Let Prpξq “ Betapa, bq. Then ∆SDpPrpξ | d1q, Prpξ | d2qq ď
2p1 ´ π
4q “ ζ.
gives a type in term of statistical distance: