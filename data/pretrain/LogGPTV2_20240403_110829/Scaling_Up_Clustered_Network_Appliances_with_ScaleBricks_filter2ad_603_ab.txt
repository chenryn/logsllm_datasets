node, the ingress node performs a FIB lookup to identify the
handling node, and then forwards the packet directly to that
node. (The ingress node thus also serves as the lookup node.)
This simple architecture requires only one hop, unlike VLB.
Unfortunately, the memory required by the globally replicated
FIB increases linearly with the number of nodes in the cluster.
Furthermore, every update must be applied to all nodes in
the cluster, limiting the aggregate FIB update rate to that of a
single server.
An alternative is a hash-partitioned design (Figure 2c). For
an N-node cluster, each node stores only 1/N FIB entries
based on the hash of the keys. The ingress node must forward
arriving packets to the indirect node that has the relevant
portion of the FIB; the indirect node then forwards the packet
to the handling node by looking up in its slice of the FIB.
This approach is nearly perfectly scalable, but reintroduces
the two-hop latency and bandwidth costs of VLB.
In this paper, we present a design that forwards directly
from ingress to handling nodes, but uses substantially less
memory than a typical fully-replicated FIB (Figure 2d). At a
high level, ScaleBricks distributes the entire routing informa-
tion (mapping from ﬂat keys to their corresponding nodes and
other associated values), or “RIB” for short, across the cluster
using a hash-partitioned design. From the RIB, it generates
two structures. First, an extremely compact global lookup
table called the “GPT” or Global Partition Table, that is used
to direct packets to the handling node. The GPT is much
smaller than a conventional, fully-replicated FIB. Second, the
RIB is used to generate FIB entries that are stored only at the
relevant handling nodes, not globally. In the LTE-to-Internet
gateway example, GPT stores the mapping from ﬂow ID to
handling node, while FIB stores the mapping from ﬂow ID to
TEID.
The GPT relies upon two important attributes of switch-
based “middlebox” clusters: First, the total number of nodes
is typically modest—likely under 16 or 32. Second, they
can handle one-sided errors in packet forwarding. Packets
that match a FIB entry must be forwarded to the correct
handling node, but it is acceptable to forward packets with
no corresponding entry to a “wrong” (or random) handling
node, and have the packet be discarded there. This property
is true in the switch-based design: The internal bandwidth
must be sufﬁcient to handle trafﬁc in which all packets are
valid, and so invalid packets can be safely forwarded across
the interconnect.
The Full FIB entries that map keys to handling nodes (along
with, potentially, some additional information) are partitioned
so that each handling node stores the FIB entries that point to
it. If the handling node receives, via its internal links, a packet
with a key that does not exist in its FIB, the input processing
code will report that the key is missing (which can be handled
in an application-speciﬁc way). The handling node FIB is
based upon prior work in space-efﬁcient, high-performance
hash tables for read-intensive workloads [34]. We omit in-
depth discussion here because we used the prior design nearly
unchanged, only extending it to handle conﬁgurable-sized
values with minimal performance impact.
The Global Partition Table is replicated to every ingress
node. This table maps keys to a lookup/handling node. Be-
cause the GPT is fully replicated, it must be compact to ensure
scalability; otherwise, it would be no better than replicating
the FIB to all nodes. For efﬁciency, the GPT is based upon a
new data structure with one-sided error. Observing that the
range of possible values (i.e., the number of nodes) in the
GPT is small, using a general-purpose lookup table mapping
arbitrary keys to arbitrary values is unnecessary. Instead, the
GPT’s mapping can be more efﬁciently viewed as set sepa-
ration: dividing a set of keys into a small number of disjoint
subsets. In this paper, we extend prior work by Fan et al. [15]
to create a fully-functional set separation data structure called
SetSep, and use it at the core of the GPT. SetSep maps each
key to a small set of output values—the lookup/handling node
identiﬁers—without explicitly storing the keys at all. The
tradeoff is that unknown destinations map to incorrect values;
in other words, the SetSep cannot return a “not found” answer.
This behavior does not harm the correctness of ScaleBricks,
because the lookup node will eventually reject the unknown
key. The advantage, though, is that lookup is very fast and
each entry in the SetSep requires only 2–4 bits per entry for
4-16 servers. Section 4 describes the SetSep data structure in
detail.
RIB Updates are sent to the appropriate RIB partition node
based upon the hash of the key involved. This node generates
new or updated FIB and GPT entries. It then sends the up-
dated FIB entry to the appropriate handling node, and sends
a delta-update for the GPT to all nodes in the cluster. Be-
cause the SetSep data structure used for the GPT groups keys
into independently-updatable sub-blocks, the RIB partition-
ing function depends on how those sub-blocks are partitioned.
Section 4.5 provides further details about RIB partitioning.
2444. SET SEPARATION
The GPT is stored on every node and is consulted once for
each packet that enters the cluster. It must therefore be both
space efﬁcient and extremely fast. To achieve these goals,
and allow ScaleBricks to scale well as a result, we extended
and optimized our previous work on set separation data struc-
tures [15] to provide memory-speed lookups on billions of
entries, while requiring only a few bits per entry. As discussed
in the previous section, the design of SetSep leverages three
properties of ScaleBricks:
• The GPT returns integer values between 0 and N − 1,
where N is the number of servers in the cluster.
• The GPT may return an arbitrary answer when queried
for the handling node of a packet with an unknown desti-
nation key (e.g., an invalid packet). Such packets will be
subsequently dropped or dealt with when the handling
node performs a full FIB lookup.
• GPT lookups are frequent, but updates much less so.
Therefore, a structure with fast lookup but relatively
expensive updates is a reasonable tradeoff.
At a high level, the basic idea in SetSep is to use brute
force computation to ﬁnd a function that maps each input key
to the correct output (the “set”, here the cluster node index).
Rather than explicitly storing all keys and their associated
values, SetSep stores only indices into families of hash func-
tions that map keys to values, and thereby consumes much
less space than conventional lookup tables. Finding a hash
function that maps each of a large number of input keys to the
correct output value is effectively impossible, so we break the
problem down into smaller pieces. First, we build a high-level
index structure to divide the entire input set into many small
groups. Each group consists of approximately sixteen keys in
our implementation. Then, for each small group, we perform
a brute force search to ﬁnd a hash function that produces the
correct outputs for each key in the group. The rest of this
section carefully presents these two pieces, in reverse order.
4.1 Binary Separation of Small Sets
We start by focusing on a simple set separation problem:
divide a set of n keys into two disjoint subsets when n is small.
We show how to extend this binary separation scheme to
handle more subsets in Section 4.3.
Searching for SetSep To separate a set of n key-value pairs
(x j,y j), where x j is the key and y j is either “0” or “1”, we
ﬁnd a hash function f that satisﬁes f (x j) = y j for j ∈ [0,n).
Such a hash function is discovered by iterating over a hash
function family {Hi(x)} parameterized by i, so Hi(x) is the
i-th hash function in this family. Starting from i = 1, for each
key-value pair (x j,y j), we verify if Hi(x j) = y j is achieved.
If any key x j fails, the current hash function Hi is rejected,
and the next hash function Hi+1 is tested on all n keys again
(including these keys that passed Hi). In other words, we
use brute force to ﬁnd a suitable hash function. As shown
later, this search can complete very rapidly for small n and an
appropriate hash function family.
Once a hash function Hi that works for all n keys is found,
its index parameter i is stored. We choose some maximum
stopping value I, so that if no hash function succeeds for i ≤ I,
a fallback mechanism is triggered to handle this set (e.g., store
the keys explicitly in a separate, small hash table).
Storing SetSep For each group, the index i of the successful
hash function is stored using a suitable variable-length encod-
ing. As shown in the next paragraph, ideally, the expected
space required from this approach is near optimal (1 bit per
key). In practice, however, storing a variable length integer
adds some overhead, as do various algorithmic optimizations
we use to speed construction. Our implementation therefore
consumes about 1.5 bits per key.
Why SetSep Saves Space Let us optimistically assume our
hash functions produce fully random hash values. The prob-
ability a hash function Hi maps one key to the correct bi-
nary value is 1/2, and the probability all n keys are properly
mapped is p = (1/2)n. Thus, the number of tested functions
(i.e., the index i stored) is a random variable with a Geometric
distribution, with entropy
≈ −log2 p = n
(1)
−(1− p)log2(1− p)− plog2 p
p
Eq. (1) indicates that storing a function for binary set sepa-
ration of n keys requires n bits on average (or 1 bit per key),
which is independent of the key size.
Insights: The space required to store SetSep approximately
equals the total number of bits used by the values; the keys
do not consume space. This is the source of both SetSep’s
strength (extreme memory efﬁciency) and its weakness (re-
turning arbitrary results for keys that are not in the set).
Practically Generating the Hash Functions A simple but
inefﬁcient approach that creates the hash function family
{Hi(x)} is to concatenate the bits of i and x as the input to a
strong hash function. This approach provides independence
across Hi, but requires computing an expensive new hash
value for each i during the iteration.
Instead of this expensive approach, we draw inspiration
from theoretical results that two hash functions can sufﬁ-
ciently simulate additional hash functions [23]. Therefore,
we ﬁrst compute two approximately independent hash func-
tions of the key, G1 and G2, using standard hashing methods.
We then compute the remaining hash functions as linear com-
binations of these two. Thus, our parameterized hash function
family to produce random bits is constructed by
Hi(x) = G1(x) + i· G2(x)
where G1(x) and G2(x) are both unsigned integers. In prac-
tice, only the most signiﬁcant bit(s) from the summation
result are used in the output, because our approach of gener-
ating parameterized hash function family will have shorter
period if the least signiﬁcant bits are used instead of the most
signiﬁcant bits.
245Both hash computation and searching are fast using this
mechanism: Hi can be computed directly using one multipli-
cation and one addition. Furthermore, the hash family can be
iterated using only one addition to get the value of Hi+1(x)
from the previous result of Hi(x).
The hash functions described above are theoretically weak:
they lack sufﬁcient independence, and as such are more likely
than “random” hash functions to fail to ﬁnd a suitable map-
ping. Empirically, however, we observe that this approach
fails only once every few billion keys. The fallback mecha-
nism of looking the keys up in a separate, small table handles
such failures.
4.2 Trading Space for Faster Construction
One problem with this basic design is the exponential growth
of the number of iterations to ﬁnd a hash function mapping n
input items to their correct binary values. We must test and re-
ject 2n hash functions on average. By trading a small amount
of extra space (roughly 5% compared to the achievable lower
bound), SetSep optimizes the construction to be an order of
magnitude faster.
Instead of generating the possible output value for x using
the hash function Hi(x) directly, SetSep adds an array of m
bits (m ≥ 2) and makes Hi(x) map each input x to one of the
m bits in the array. In other words, the output value for x
is the bit stored in bitarray[Hi(x)] rather than Hi(x). To
construct the bit array, at the beginning of the iteration testing
hash function Hi(x), all bits in the array are marked “not
taken.” For each key-value pair (x j,y j), if Hi(x j) points to
a bit that is still “not taken,” we set the bit to y j and mark it
as “taken.” If the bit is marked as “taken,” we check if the
value of the bit in the array matches y j. If so, the current
hash function is still good and can proceed to the next key.
Otherwise, we reject the current hash function, switch to the
next hash function Hi+1, re-initialize the bit array, and start
testing from the ﬁrst key. Intuitively, with more “buckets” for
the keys to fall in, there are fewer collisions, increasing the
odds of success. Thus, adding this bit array greatly improves
the chance of ﬁnding a working hash function.
Space vs. Speed Storing the bit array adds m bits of storage
overhead, but it speeds up the search to ﬁnd a suitable hash
function and correspondingly reduces the number of bits
needed to store i—since each hash function has a greater
probability of success, i will be smaller on average. Figure 3a
shows the tradeoff between the space and construction speed
for a SetSep of n = 16 keys while varying the bit array size (m)
from 2 to 30. Increasing the size of the bit array dramatically
reduces the number of iterations needed. It requires more than
10,000 hash functions on average when m = 2; this improves
by 10× when m = 6, and when m ≥ 12, it needs (on average)
fewer than 100 trials, i.e., it is 100× faster.
Figure 3b presents analytical results for the total space (i.e.,
bits required to store the index i plus the m bits of the array)
for this SetSep. The total space cost is almost an increasing
function of m. The minimum space is 16 bits, but even when
m = 12, the total space cost is only about 20 bits. (This is less
than 16 + 12 = 28 bits, because of the reduction in the space
required to store i.)
Insights: Trading a little space efﬁciency (e.g., spending
20 bits for every 16 keys rather than 16 bits) improves con-
struction speed by 100×.
Representing the SetSep. In light of the above result, we
choose to represent the SetSep using a ﬁxed 24-bit represen-
tation per group in our implementation, with up to 16 bits to
represent the hash index and m = 8. This yields 1.5 bits per
key on average. (We show the effect of the choice m later in
the paper.) Although we could use less space, this choice
provides fast construction while ensuring that fewer than 1
in 1 million groups must be stored in the external table, and