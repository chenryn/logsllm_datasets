the entire sort process, only one sequential pass of reading is
performed on the original data. This reduces many memory
accesses, thus signiﬁcantly saving memory bandwidth.
Wagner’s algorithm requires sorting with payload, adding
extra bits to the FFs but not the comparators. In the original
Equihash paper, a payload datum is an index set that doubles
in length each round. With the index pointer technique applied,
we only need to carry a single index pointer. This pointer can
come from a prepending counter and does not need to be read.
This design is simple to implement at RTL level (eg. in
Verilog). It works perfectly for short enough sequences, but
its linear logic is too much for any reasonable parameter
set deployed in production. Under the popular parameter set
(n, k) = (200, 9), N = 221. If all of the 2M items are sorted
using smart cells (Nc = N), the module would need too much
power 8 and area to be practical. (144, 5) brings even bigger
challenge for adversaries, as 32M items need to be sorted in
every pass.
2) Merge sort: Our linear sort peripheral has linear logic
complexity, so the merging method in [11] isn’t helpful,
because it cannot reduce logic. To actually reduce the area and
energy cost, the only way is to reduce smartcell instances. This
forces us into building exactly one linear sort peripheral that
can not handle long sequences, and we therefore have to dump
its results into off-chip memory. The merge sort peripheral will
not have pipelined input, and have to include a fetcher.
For every x-way merge sort, an extra full pass of sequen-
tial write and random read (including both sorting key and
payload) is introduced, but reducing the number of smartcell
instances to 1/x, beneﬁtting wider merge sort peripherals.
However, off-chip memory access has signiﬁcant latencies and
the random access here need to include prefetch queues. For
this to work, each merging way need a private queue, bring-
ing up the cost of wide merging. Under certain parameters,
multiple stages of moderately-wide merge sort might provide
better overall efﬁciency. Unlike linear sort, the merge sort here
works on processed data, so index pointers need to be read
from memory, adding datum width to prefetch queues.
The depth of the prefetch buffers is also an important
factor to consider. It has to cover the off-chip memory latency
measured in clock cycles, and is thus inﬂuenced by both of
them. To produce more practical (or pessimistic) efﬁciency
projection and reach a sound conclusion, we set a greater depth
(thus higher clock frequency) than needed. An actual adversary
can correct these values in the last step of our methodology
and gain further energy efﬁciency.
C. Pair generation
Generating index pointer pairs is a trivial task for either an
automaton or a micro controller unit (MCU), as its only tasks
are to store the incoming sorting payloads and enumerate pairs
out of every group sharing a sorting key. Keeping it up to line
speed (one pair per cycle) is also simple but proving so is not.
1) Tail cutting: As stated in the original Equihash paper, the
expectation of input list length for each round is always N, so
in good cases 9 the length could go well beyond N, demanding
extra bits everywhere within the solver and affecting overall
throughput. Devadas [6] gave the precise formula for SLGBP’s
expected number of solutions in 2017, and showed that this
number varies greatly depending on parameters.
8More than 1 kW at 1 GHz, according to section V-B2.
9Longer lists, which tend to produce more solutions, therefore more proﬁt.
6
TABLE III: Measured effect of tail cutting
tail length
(200, 9)
solutions
loss
(144, 5)
solutions
loss
0
1024
2048
4096
1.53
1.64
1.66
1.66
17.5%
12.5%
11.5%
11.5%
1.80
2.00
2.00
2.00
10.0%
0
0
0
Under (200, 9), 1.879 ﬁnal solutions are expected. Solutions
do not present great values and lengths of intermediate lists
|L(r)| do not decay signiﬁcantly. Under (144, 5), fewer rounds
are run. Lengths of |L(r)| are less inﬂuenced by the index set
intersection problem [2], and the expectation of solution counts
is very close to 2. We therefore choose to cut the tail at a point
and discard further index pointers.
We ran software simulation on different tail lengths (see
table III), and found it reasonable to cut the tail at zero length.
The 17.5% loss under (200, 9) may seem a lot, but is totally
proﬁtable considering its efﬁciency gain.
As mentioned in section IV-A, solutions are extremely
scarse under some parameter sets, giving these solutions
signiﬁcant economic values in blockchain networks. In this
scenerio, good cases in intermediate steps should be carefully
preserved, requiring different techniques. The tail-cutting sub-
routine could signiﬁcantly hurt proﬁtability here and may not
be a good idea. Instead, index checking should be performed
to shorten the lists in every round. These parameter sets have
not been practically deployed, so we can safely defer the
evaluation of corresponding designs for now.
D. Postprocessing
The xor computation step takes very little logic and is
easy to catch up with other components’ speed, given that its
memory access can be similarly buffered as in section IV-B2.
Even though the design is not trivial and can affect overall
efﬁciency, it is not a serious challenge for ASIC designers.
Since we are inspecting Equihash rather than building an
actual solver product, knowing its ability to be practically and
proﬁtably done is sufﬁcient for our research.
The same holds for the index set construction step at the
very end. This subroutine does not require high throughput
and can be done with relatively slow MCUs.
E. Pipelining
If the above components run sequentially like software,
the solver will spend most of its time in non-bottleneck
subroutines, wasting memory bandwidth and power. There are
two major solutions: (1) cut the power from idle components,
and (2) use a pipelined design to improve performance. They
should yield similar efﬁciency, but the pipeline method has
higher proﬁtability (higher per-chip performance), and is more
likely to be accepted by adversaries. In this section we apply
pipelining to reduce idling within the whole design.
The most applicable subroutine is linear sorting. For every
empty cell with valid input, its output becomes valid strictly
after 2 clock cycles, so a sorting peripheral with Nc cells start
to produce output at precisely 2Nc cycles after it is presented
with the ﬁrst input item. It continues to emit exactly one new
item every tick until the sequence is depleted. Such behavior
can then be modeled and optimized as a 2Nc-stage pipeline.
To prevent two sequences from mixing, an invalid value is
inserted as a separator, setting its throughput to one sequence
per Nc + 1 cycles.
The merge sort module itself has no pipeline capability
across input sequences. It ejects all items before starting to
process a new sequence. Therefore, no merging unit has valid
input when the end-of-sequence marker is emitted. Given that
prefetching can start before a queue is depleted and that the
marker does not repeat in output, the prefetcher needs a cycle
for each queue to either fetch or insert markers. All merging
units also need an extra tick to propagate its result, except
the last stage which delivers output directly. These operations
also consume items from prefetch queues, triggering memory
reads, so they cannot go parallel.
A Wm-way merge sort module implemented with Nm
merging units can process a sequence of length N every
N + Nm + Wm cycles including one used for the marker.
As we perform pair generation, XOR computation and
intersection check using simple logic on microcontrollers,
we should see line-speed performance as long as connected
memory meets bandwidth requirements. The overall behavior
of pipelined solvers can be characterized space-time diagrams,
as shown in ﬁgure 3. Note that the pipeline characteristics of
the linear sort stage changes with merge width, so subgraph
(b) is not to scale.
V. SOLVER EVALUATION
As discussed in previous sections, all underlying peripheral
blocks correspond to algorithm-layer semantics and the ﬁnal
solver products might vary in design. In this section, we
choose an acceptable arrangement with minimal logic footprint
and analyze its overall efﬁciency. We focus on comparison
of our overall efﬁciency (in solutions/J) to software solvers,
because only comparing performance (in solutions/s) or power
without mentioning each other is meaningless in the scenario,
as mentioned in section II.
We use the most popular set of parameters, (200, 9) so that
our results can be directly compared with CPU/GPU solvers.
In fact this is the only parameter set deployed widely enough
to have highly-optimized software solvers and good statistical
coverage available. Nevertheless, we still present and evaluate
a valid arrangement for (144, 5), as a baseline for future
research and deployments.
Under (200, 9), we cut tails at offset 0 and merge sequences
twice with 8*4-way merge sort modules, leaving our insertion-
sort module with 2048 smartcells. Our example solver com-
pletes a single round for every puzzle, so every puzzle should
go through the solver k times before producing solutions. We
demonstrate the modiﬁed algorithm in ﬁgure 4 and its data
ﬂow in ﬁgure 5.
7
Input
begin
: list L of N n-bit strings (N (cid:28) 2n)
Enumerate L(0) as {xi|i = 1, 2, ..., N}
r ← 1
while r < k do
(a) without merging
(b) with one merge stage (not to scale)
Fig. 3: Space-time diagram of pipelined Equihash solvers
Under (144, 5), list L contains signiﬁcantly more items.
As smartcells and prefetch queues are both power-hungry,
increasing their numbers is not worthwhile. Adversaries can
tackle this by adding a third 8*4-way merge stage and a third
buffer block (limiting the insertion-sort module to 1024 cells
at the same time).
A. Memory usage
Recall in section III that our design uses off-chip memories
so it can handle all Equihash parameters. In table IV we list all
off-chip memory usage10 of our example design. We also use
external memory as buffers before every merge sort module,
writing one sorting item (both key and payload) and reading
one out every cycle. These buffers need to cover two batches
in the worst case.
TABLE IV: Off-chip memory usage
parameter
usage
(200, 9)
(144, 5)
L
P
Buffer (each)
L
P
Buffer (each)
capacity
1600 Mib
2016 Mib
248 Mib
23040 Mib
25600 Mib
4736 Mib
peak read
400 b/tick
21 b/tick
62 b/tick
288 b/tick
25 b/tick
74 b/tick
peak write
180 b/tick
42 b/tick
62 b/tick
120 b/tick
50 b/tick
74 b/tick
The memory utilization in table IV is far from uniform.
The capacity and bandwidth requirements vary between us-
ages, allowing manufacturers to pick suitable conﬁgurations
10Listed capacity are all calculated with static allocation. No padding is
taken into account.
k+1 bits of
Enumerate X (r) with the ﬁrst n
items in L(r−1), adding indices
Enumerate Y (r) with the unused bits of
items in L(r−1)
Sort X (r) into multiple sequences,
preserving indices
Merge sorted sequences into one, preserving
indices
Find no more than N unordered pairs (i, j)
such that xi = xj
P (r) ← {(i, j)|(i, j) is a found pair}
L(r) ← {(yi ⊕ yj)|(i, j) is a found pair }
r ← r + 1
Enumerate X (k) with all bits of items in
L(k−1), adding indices
Sort X (k) into multiple sequences, preserving
indices
Merge sorted sequences into one, preserving
indices
Find all unordered pairs (i, j) such that xi = xj
P (k) ← {(i, j)|(i, j) is a found pair}
Reconstruct the list R of index sets using lists
P (r) of index pointer pairs, where
r = k, k − 1, ..., 1, removing any set with less
than 2k distinct indices
sort
merge
gen
xor
sort
merge
gen
check
Output: list R of sets of distinct indices
Fig. 4: ASIC-assisted Equihash algorithm for good-parameter
SLGBP
to maximize efﬁciency. It is reasonable to use high-speed
memory for L and lower-speed memory for P (to save energy),
and possibly on-chip memory for buffers under (200, 9).
These options increase ASIC advantage and make our ﬁnal
projection pessimistic for adversaries (or optimistic for the
security inspection).
Consider our pipelining conﬁguration in section IV-E, we
work with 4 puzzles at the same time in our (200, 9) con-
ﬁguration. The memory capacity requirement is around 3x
that of a single puzzle, except for L, where an extra list is
used to store the results from xor step. The solver issues 3
reads (two by xor and one by sort) and one write (by either
xor or input) to L, totaling 560 bits in the worst case. New
puzzles can be written to L simultaneously with the gen step
of the ﬁnal round, when no more xor step is performed, thus
should not add to bandwidth requirements. Assuming we only
use memory statically (no dynamic allocations), we would
be using 4112 Mib excluding overheads caused by padding.
Our bandwidth requirement for L would be 290 Gb/s at 500
8
to 1 GHz, listed in table V. Although these power reports do
not strictly resemble actual chips, they serve as references for
IC frontend designers and should sufﬁce in our scenario.
TABLE V: Insertion sort power usage at 1GHz
data width
20b
20b
20b
20b
20b
40b
40b
40b
index width Nc
64
128
192
256
384
64
128
256
21b
21b
21b
21b
21b
21b
21b
21b