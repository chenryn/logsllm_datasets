lute clock.
In implementations, care must be taken to avoid losing precision
when recording timestamps. For example, the TSC register is typ-
ically 64 bits. If a 32 bit counter is used to manipulate it, overﬂow
can occur after around 4 seconds on a 1Ghz machine (see [5] for a
discussion on this point).
2.2.1 Timestamping
Even a perfect clock is of little use if one is unable to read it close
to when the event of interest occurs. This is the issue of times-
tamping, and its optimisation is application dependent. Here, for
the purpose of remote clock synchronization, the application is the
timestamping of the arrival of NTP packets.
In [5] (see also [10]), this issue was explored in detail for the ap-
plication of timing the arrival of packets on the Ethernet network in-
terface (a 600Mhz Pentium PC was used). The timestamping solu-
tions described range from using purely user-space code, through to
the use of RT-Linux under Linux, a real-time operating system [11],
which entirely avoids the scheduling problems which plague pre-
cision timing under multi-tasking operating systems. It was found
that timestamping early in the driver-code for the network interface
card provided an excellent compromise of almost no scheduling
problems (1 timestamp per 10,000, and then usually with an error
under 1ms), and with timestamping noise (dominated by interrupt
latency) of the order of at worst 15µs, whilst keeping the code sim-
ple and almost entirely at the user-level. In contrast, the standard
gettimeofday system call to obtain timestamps from the SW
clock under Linux suffers from much higher ‘system’ noise due to
the above and other effects.
We adopt the same driver based timestamping approach here.
The kernel-level code required consists of just a few lines to en-
able a raw timestamp (the TSC counter value) to be passed up from
the driver to a user process, where it can be stored, processed, or
converted to a time in seconds as required without hard time con-
straints. In [5] this was done under Linux by exploiting the existing
API, in this paper it is done via a modiﬁed Berkely Packet Filter
data structure under BSD Unix. The timestamping errors which re-
main, especially if of signiﬁcant size (such as larger than 0.1 ms)
will be seen as network delay by the generic ﬁltering mechanisms
we develop here, and thereby rejected or damped. For the same rea-
son, if instead user-level timestamping were used, the algorithms
would still work, albeit with higher estimation variance, as the er-
rors will always increase round-trip times and therefore be seen as
positive network ‘noise’.
2.3 The NTP Server
Network Time Protocol (NTP) servers are networked comput-
ers whose clocks are deemed to be well synchronized. Different
levels of synchronization accuracy are deﬁned. We will be con-
cerned only with stratum-1 servers, whose clocks are synchronized
by a local reference time source (using the GPS timescale, which
can be converted to UNIX time). Three such are used in this pa-
per. ServerLoc is in our laboratory on the same local network as
the host. ServerInt is located in the same organization, but in a
distinct network and GPS receiver in a different building. Finally,
ServerExt is located over a thousand kilometres away in another
city, and is synchronized by atomic clock. The distances between
the host and the servers are given in table 2 in terms of physical dis-
tance, the minimum RTT of NTP packets over at least a week, and
in the number of IP hops as reported by the traceroute utility.
We also give the path asymmetry ∆, which as discussed in detail
in section 4.2 is the difference of the minimum one-way delays to
and from the server.
Server
Reference Distance
RTT
Hops
ServerLoc
ServerInt
ServerExt
GPS
GPS
Atomic
3 m
2
300 m
5
1000 km 14.2 ms ≈ 10
0.38 ms
0.89 ms
∆
50µs
50µs
500µs
Table 2: Characteristics of the stratum-1 NTP servers.
Hosts wishing to synchronize their clocks can do so by running a
NTP application which communicates with a NTP server via NTP
packets. These are User Datagram Packets (UDP) with a 48 byte
payload including four 8-byte Unix timestamp ﬁelds (90 bytes in
total for the Ethernet frame which transports the datagram). The
usual exchange works as follows (refer to ﬁgure 1). The ith NTP
packet is generated in the host. Just before being sent, the times-
tamp Ta,i is generated by the SW clock and is placed in the packet
payload. Upon arrival at the server, the timestamp Tb,i is made
by the server clock and inserted into the payload. The server then
immediately sends the packet back to the host, adding a new de-
parture timestamp Te,i, and the host timestamps its return as Tf,i.
The four timestamps {Ta,i, Tb,i, Te,i, Tf,i} are the raw data from
the ith exchange from which the host clock must be synchronized.
None of these timestamps are perfect however due to clock and/or
timestamping limitations. The actual times of the corresponding
events we denote by {ta,i, tb,i, te,i, tf,i}. The NTP payload also
contains processed data related to estimated clock drift which we
do not use, and server identity information which we plan to use as
part of route change (level shift) detection in the future.
At the host, we do not use the usual timestamps made by the
SW-NTP clock, but instead take separate raw TSC timestamps. We
denote these by the symbols Ta,i, Tf,i as above even though they
are in ‘TSC units’, rather than seconds.
Server
DAG
Host
tb,i te,i
ta,i
ri
tg,i
tf,i
time
d→
i
d↑
i
d←
i
Figure 1: Timeline of the host-server exchange of the ith NTP
packet. The components of forward path delay (d→
i ), server
delay (d↑
i ) and their sum, the round-
trip time (ri = d→
i ), backward path delay (d←
i ) are shown.
i + d↑
i
+ d←
Being a stratum-1 NTP server, the server’s clock should be syn-
chronized, and so we could expect that Tb,i = tb,i and Te,i = te,i.
However timestamping errors nonetheless make these unequal even
for the server. Indeed, as servers are often just PC’s, their times-
tamping may not have the quality of the driver based TSC times-
tamping of our host. This is the case here, and we discuss its con-
sequences below.
As stated in the introduction, we rely on the normal ﬂow of NTP
packets between host and server, to minimize the disruption to nor-
mal system operations. For much of this paper we use a polling rate
of 16 seconds, which is higher than the usual default, but which
provides a detailed data base from which to examine both the clock
and synchronization performance. In the last two sections we give
examples of performance using a range of polling rates which in-
cludes the usual default values. A conservative polling rate is in
keeping with the need to avoid placing excessive load on the net-
work and the NTP server. In a more generic solution where the
usual software clock would be entirely replaced by the TSC-NTP
clock, the emission of NTP packets could be controlled, which
would enable the synchronization performance to be further op-
timized, and warmup procedures simpliﬁed.
We provide a generic synchronization mechanism in this paper,
but our work is nonetheless aimed at applications and users for
whom accuracy matters. We therefore nominally assume that an
initial effort has been made to ﬁnd a nearby, reliable stratum-1
NTP server. The ServerInt is representative of such, as it has a
RTT of the order of only 1ms, but is not on the local network. It
also has the advantage of a veriﬁably symmetric route in the for-
ward and reverse directions. This is a very important asset as we
explain in detail in section 4.2. We believe that such a server can
be readily found for the majority of tertiary educational institutions
and commercial organizations with signiﬁcant networking infras-
tructure. We stress however that the presence of such an ‘optimal’
server is not required for very good results in most cases.
2.4 Reference Timing
Validation of timing methods would not be possible without a
reliable timing reference. We used a ‘DAG3.2e’ series measure-
ment card designed for high accuracy and high performance pas-
sive monitoring of 10/100 Mbps Ethernet, yielding a time stamping
accuracy around 100ns [8]. The card was synchronized to the Trim-
bale Acutime 2000 GPS receiver, the same one used by ServerLoc,
with the antenna permanently mounted on the roof of the building
housing the laboratory.
The DAG card was positioned to timestamp the returning NTP
packets via a passive tap on the Ethernet cable just before it enters
the host’s interface card. Unfortunately the DAG and TSC clocks
are still timestamping different events, so that tg,i < tf,i. One com-
ponent of this difference is that the DAG timestamps the ﬁrst bit
of each packet, whereas TSC timestamping occurs after a packet
has fully arrived. Accordingly by Tg,i we denote a DAG timestamp
which has been corrected by the addition of the corresponding in-
terval of 90 ∗ 8/100 = 7.2µs. The remaining difference includes
the additional length of cable (negligible), the minimum process-
ing time of the card, and the interrupt latency of the host. To es-
timate the size of these latter effects, we examined a histogram of
the difference, with respect to i, of the measured offset discrep-
ancy Tf,i − Tg,i. The (overwhelmingly) dominant mode, centered
at zero as expected, has width 5µs. In addition to large departures
due to rare scheduling errors, which are easy to detect and exclude
if required, there are small but clearly deﬁned side modes symmet-
rically located at 10 and 31µs from the origin. These smaller yet
signiﬁcant timestamping errors are due to interrupt latencies, and
can also be reliably detected and corrected for. The ﬁnal limit of
the veriﬁability of the offset (but not rate) results is therefore of the
order of 5µs.
The DAG timestamps are the basis of all the ‘actual performance’
results presented here.
3. DATA CHARACTERIZATION
Any synchronization algorithm must begin with a knowledge of
the nature of the data collected. In this section we study the basic
features of the key quantities, the offset of the TSC clock C(t), the
network delay, and also the delay at the NTP server.
3.1 The Clock
We examine the clock offset of the same 600Mhz CPU host
in two different temperature environments, laboratory: an open
plan area in a building which was not airconditioned, and machine-
room: a closed temperature controlled environment.
To calculate the offset of the clock from the TSC counter times-
tamp Tf,i, we are immediately faced with the issue of prior rate
estimation: without a value of ˆp the clock C(t)= TSC(t)ˆp + C
cannot be calculated. In ﬁgure 2 we use ˆp = 1.82263812 ∗ 10−9
(548.65527 Mhz) for measurements made in the laboratory, and
ˆp = 1.82263832 ∗ 10−9 (548.65521 Mhz) in the machine-room,
and then calculate the offset via θ(tf,i) = Tf,i ∗ ˆp − Tg,i for each.
These estimates ‘detrend’ (in fact they force the ﬁrst and last offset
values to be the same, normalised to be zero), facilitating an initial
inspection of the (small) residual clock drifts, which depend on the
temperature environment.
From the right plot in ﬁgure 2 it is clear that the SKM model
fails over day timescales, as the residual errors are far from lin-
ear, although the variations fall within the narrow cone emanating
from the origin deﬁned by γ = ±0.1PPM. In the left plot how-
ever we see that over smaller time scales the residual offset error
grows approximately linearly with time, suggesting that the SKM
could be accepted and the above ‘global’ estimates replaced with a
considerably more accurate local ˆp values (the microsecond scale
irregularities are due to timestamping noise in the host, as here cor-
rected Tf,i timestamps, described in section 2.4, were not used). We
have found these observations to hold for all traces collected over
many months. In [5] the same result was reported for a host in an
airconditioned (but not temperature controlled) ofﬁce environment
over a continuous 100 day period.
]
s
m
[
r
o
r
r
e
t
e
s
f
f
O
0.03
0.02
0.01
0
−0.01
−0.02
−0.03
0
laboratory
machine−room
0.1 PPM
200
400
600
time [sec]
800
1000
10
5
0
−5
]
s
m
[
r
o
r
r
e
t
e
s
f
f
O
−10
0
2
4
time [day]
6
Figure 2: Offset variations θ(t) of C(t) in two different tem-
perature environments (each set to 0 at t = 0 for comparison)
always fall within the cone corresponding to a steady error rate
of γ = 0.1PPM. Left: over a 1000 [sec] period, Right: over 1
week (legend applies to both).
The above discussion is only an illustration of the underlying
behaviour we must understand and deal with. In fact depending
on the time scale and ˆp value chosen, θ(t) can take on very differ-
ent appearances. To examine offset over all scales simultaneously,
and to avoid the need for a somewhat arbitrary prior rate estimate,
we return to the concept of oscillator stability (equation (4)). A
particular estimator of the variance of yτ (t), known as the Allan
‡
, calculated over a range of τ values, is a traditional char-
variance
acterization of oscillator stability [9]. We term the square root of
the Allan variance the Allan deviation, and interpret it as the typi-
cal size of variations of time scale dependent ‘rate’. A study over a
range of time-scales is essential as the source and nature of timing
errors vary according to the measurement interval. At very small
timescales, γ will not be readily visible in yτ (t) as the ‘rate’ error
will essentially correspond to system noise affecting timestamping.
At intermediate timescales γ may seem well deﬁned and constant
with some measurement noise, as in the left plot in ﬁgure 2. At
large scale where daily and weekly cycles enter, the issue is not
noise in estimates of γ but rather variations in γ itself.
10−7
Laboratory ServerInt
M−room ServerInt
M−room ServerLoc
M−room ServerExt
0.1 PPM
τ
y
f
o
n
o
i
t
a
v
e
d
n
a
i
l
l
A
10−8
101
102
103
τ  [sec]
104
105
Figure 3: Allan variation plots. From small scales to around
τ = 1000 seconds, the SKM applies, and rate estimates are
meaningful down to 0.01PPM.
‡
This is essentially a Haar wavelet spectral analysis [12]
Four Allan deviation plots for the host oscillator are given in ﬁg-
ure 3, for traces taken under different conditions ranging from 1 to
3 weeks in length. One is when the host was in the laboratory, and
uses ServerInt. The others are from the machine room, using each
of the 3 servers. Corrected Tf,i timestamps were used here, as oth-
erwise the timestamping noise adds considerable spurious variation
at small scales (due to the strong wavelet signatures of discontinu-
ities).
Over small scales the plots show a consistent 1/τ decrease, con-