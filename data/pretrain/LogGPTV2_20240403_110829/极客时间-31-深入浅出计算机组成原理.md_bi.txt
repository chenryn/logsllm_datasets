## 推荐阅读想要了解 GPU 的设计构造，一个有效的办法就是回头去看看 GPU的历史。我建议你好好读一读 Wikipedia 里面，关于 GPU的条目。另外，也可以看看 Techspot 上的[The History of the MordernGraphicsProcessor](https://www.techspot.com/article/650-history-of-the-gpu/)的系列文章。
## 课后思考我们上面说的图形加速卡，可以加速 3D 图形的渲染。那么，这些显卡对于传统的2D 图形，也能够进行加速，让 CPU 摆脱这些负担吗？欢迎留言和我分享你的疑惑和见解。你也可以把今天的内容，分享给你的朋友，和他一起学习和进步。![](Images/79d06107d349635530fbf82aa8dfb625.png){savepage-src="https://static001.geekbang.org/resource/image/28/29/281ca28b90c8aa0aecbb5adc08394f29.jpg"}
# 31 \| GPU（下）：为什么深度学习需要使用GPU？上一讲，我带你一起看了三维图形在计算机里的渲染过程。这个渲染过程，分成了顶点处理、图元处理、栅格化、片段处理，以及最后的像素操作。这一连串的过程，也被称之为图形流水线或者渲染管线。因为要实时计算渲染的像素特别地多，图形加速卡登上了历史的舞台。通过 3dFx的 Voodoo 或者 NVidia 的 TNT 这样的图形加速卡，CPU就不需要再去处理一个个像素点的图元处理、栅格化和片段处理这些操作。而 3D游戏也是从这个时代发展起来的。你可以看这张图，这是"古墓丽影"游戏的多边形建模的变化。这个变化，则是从1996 年到 2016 年，这 20 年来显卡的进步带来的。![](Images/08da5fed1af28a914b5faaa04e15a0da.png){savepage-src="https://static001.geekbang.org/resource/image/1d/c3/1d098ce5b2c779392c8d3a33636673c3.png"}```{=html}```图片来源](http://www.gamesgrabr.com/blog/2016/01/07/the-evolution-of-lara-croft/)```{=html}```
## Shader 的诞生和可编程图形处理器不知道你有没有发现，在 Voodoo 和 TNT显卡的渲染管线里面，没有"顶点处理"这个步骤。在当时，把多边形的顶点进行线性变化，转化到我们的屏幕的坐标系的工作还是由CPU 完成的。所以，CPU的性能越好，能够支持的多边形也就越多，对应的多边形建模的效果自然也就越像真人。而3D 游戏的多边形性能也受限于我们 CPU 的性能。无论你的显卡有多快，如果 CPU不行，3D 画面一样还是不行。所以，1999 年 NVidia 推出的 GeForce 256显卡，就把顶点处理的计算能力，也从 CPU里挪到了显卡里。不过，这对于想要做好 3D 游戏的程序员们还不够，即使到了GeForce256。整个图形渲染过程都是在硬件里面固定的管线来完成的。程序员们在加速卡上能做的事情呢，只有改配置来实现不同的图形渲染效果。如果通过改配置做不到，我们就没有什么办法了。``{=html}这个时候，程序员希望我们的 GPU也能有一定的可编程能力。这个编程能力不是像 CPU那样，有非常通用的指令，可以进行任何你希望的操作，而是在整个的**渲染管线**（GraphicsPipeline）的一些特别步骤，能够自己去定义处理数据的算法或者操作。于是，从2001 年的 Direct3D 8.0 开始，微软第一次引入了**可编程管线**（ProgramableFunction Pipeline）的概念。![](Images/b3923b776199e691ab37dd5fc64038d4.png){savepage-src="https://static001.geekbang.org/resource/image/27/6d/2724f76ffa4222eae01521cd2dffd16d.jpeg"}```{=html}```早期的可编程管线的GPU，提供了单独的顶点处理和片段处理（像素处理）的着色器]{.reference}```{=html}```一开始的可编程管线呢，仅限于顶点处理（VertexProcessing）和片段处理（FragmentProcessing）部分。比起原来只能通过显卡和 Direct3D这样的图形接口提供的固定配置，程序员们终于也可以开始在图形效果上开始大显身手了。这些可以编程的接口，我们称之为**Shader**，中文名称就是**着色器**。之所以叫"着色器"，是因为一开始这些"可编程"的接口，只能修改顶点处理和片段处理部分的程序逻辑。我们用这些接口来做的，也主要是光照、亮度、颜色等等的处理，所以叫着色器。这个时候的 GPU，有两类 Shader，也就是 Vertex Shader 和 FragmentShader。我们在上一讲看到，在进行顶点处理的时候，我们操作的是多边形的顶点；在片段操作的时候，我们操作的是屏幕上的像素点。对于顶点的操作，通常比片段要复杂一些。所以一开始，这两类Shader都是独立的硬件电路，也各自有独立的编程接口。因为这么做，硬件设计起来更加简单，一块GPU 上也能容纳下更多的 Shader。不过呢，大家很快发现，虽然我们在顶点处理和片段处理上的具体逻辑不太一样，但是里面用到的指令集可以用同一套。而且，虽然把Vertex Shader 和 Fragment Shader分开，可以减少硬件设计的复杂程度，但是也带来了一种浪费，有一半 Shader始终没有被使用。在整个渲染管线里，Vertext Shader 运行的时候，FragmentShader 停在那里什么也没干。Fragment Shader 在运行的时候，Vertext Shader也停在那里发呆。本来 GPU就不便宜，结果设计的电路有一半时间是闲着的。喜欢精打细算抠出每一分性能的硬件工程师当然受不了了。于是，**统一着色器架构**（UnifiedShader Architecture）就应运而生了。既然大家用的指令集是一样的，那不如就在 GPU 里面放很多个一样的 Shader硬件电路，然后通过统一调度，把顶点处理、图元处理、片段处理这些任务，都交给这些Shader 去处理，让整个 GPU 尽可能地忙起来。这样的设计，就是我们现代 GPU的设计，就是统一着色器架构。有意思的是，这样的 GPU 并不是先在 PC里面出现的，而是来自于一台游戏机，就是微软的 XBox360。后来，这个架构才被用到 ATI 和 NVidia的显卡里。这个时候的"着色器"的作用，其实已经和它的名字关系不大了，而是变成了一个通用的抽象计算模块的名字。正是因为 Shader 变成一个"通用"的模块，才有了把 GPU拿来做各种通用计算的用法，也就是**GPGPU**（General-Purpose Computing onGraphics Processing Units，通用图形处理器）。而正是因为 GPU可以拿来做各种通用的计算，才有了过去 10 年深度学习的火热。![](Images/d6adf609b69aa2bc7fca3c0cf6f0f4aa.png){savepage-src="https://static001.geekbang.org/resource/image/da/93/dab4ed01f50995d82e6e5d970b54c693.jpeg"}
## 现代 GPU 的三个核心创意讲完了现代 GPU 的进化史，那么接下来，我们就来看看，为什么现代的 GPU在图形渲染、深度学习上能那么快。
### 芯片瘦身我们先来回顾一下，之前花了很多讲仔细讲解的现代 CPU。现代 CPU里的晶体管变得越来越多，越来越复杂，其实已经不是用来实现"计算"这个核心功能，而是拿来实现处理乱序执行、进行分支预测，以及我们之后要在存储器讲的高速缓存部分。而在 GPU 里，这些电路就显得有点多余了，GPU的整个处理过程是一个[流式处理](https://en.wikipedia.org/wiki/Stream_processing)（StreamProcessing）的过程。因为没有那么多分支条件，或者复杂的依赖关系，我们可以把GPU里这些对应的电路都可以去掉，做一次小小的瘦身，只留下取指令、指令译码、ALU以及执行这些计算需要的寄存器和缓存就好了。一般来说，我们会把这些电路抽象成三个部分，就是下面图里的取指令和指令译码、ALU和执行上下文。![](Images/26340a37ed864d553cb152536ccf6233.png){savepage-src="https://static001.geekbang.org/resource/image/4c/9d/4c153ac45915fbf3985d24b092894b9d.jpeg"}