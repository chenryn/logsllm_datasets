(September to December 2019), as detected by CrawlPhish.
techniques. In the table, the percentage under the “2018”,
“2019”, and “Total” columns represents the share of each
category of JavaScript cloaking technique implementation in
the respective time period. The percentage under the Share
column refers to the percentage of each type of cloaking
technique in all the cloaked phishing websites we detected.
We categorize the cloaking types in phishing websites from
both the APWG Dataset and the Public Dataset. As shown
in Table V, the User Interaction cloaking category has the
most implementations among phishing websites in the APWG
Dataset. In 2018, 2,416 phishing websites (40.11%) leveraged
cloaking within the User Interaction category, while in 2019,
the usage ratio of User Interaction cloaking grew to 61.23%.
The usage ratio of cloaking techniques in the Fingerprinting
category over two years is almost the same. Within the Bot
Behavior category, the usage ratio dropped signiﬁcantly, from
40.29% to 21.14%. We ﬁnd that phishing websites rely more
on cloaking techniques in the User Interaction category than
the others. We believe that this is because it is more difﬁcult for
anti-phishing crawlers to impersonate human behaviors than to
bypass other types of cloaking.
Table VI demonstrates the usage of each cloaking type
CrawlPhish detected from the Public Dataset. Just as we ob-
served from the 2019 portion of the APWG Dataset, the User
Interaction category was also the most frequently implemented
in the Public Dataset.
Brand distribution. Among the 6,024 cloaked phishing sites
in 2018, LinkedIn and PayPal were the most frequently
impersonated brands, as shown in Table VII. In 2019, the
distribution changed: Apple and Bank of America phishing
2018
2019
Targeted Brand
LinkedIn
PayPal
Microsoft
Bank of America
Apple
Count
2,317
1,104
646
309
153
Share
38.46%
18.33%
10.72%
5.13%
2.54%
Targeted Brand
Apple
Bank of America
Facebook
PayPal
Microsoft
Count
6,298
3,572
2,230
1,841
987
Share
21.69%
12.30%
7.68%
6.34%
3.40%
TABLE VII: Top brands targeted by cloaked phishing websites
in the APWG Dataset.
websites were the most prevalent. Overall, four of the top ﬁve
brands in 2018 were also in the top ﬁve in 2019. Nevertheless,
because of changes within the phishing landscape between the
two years, our ﬁndings regarding the relative distribution of
cloaking phishing websites may be skewed.
VII. EVALUATION: IMPACT OF CLOAKING TECHNIQUES
We have, thus far, shown that phishing websites make ex-
tensive use of client-side cloaking techniques. To demonstrate
that this cloaking represents a signiﬁcant threat to users, we
deployed two experiments to verify that these techniques can
truly evade detection by anti-phishing systems, and that they
generally do not discourage victim visits—the two key factors
to increasing attackers’ return-on-investment.
A. Effectiveness Against Anti-Phishing Entities
We evaluate how effective client-side cloaking techniques
are against real-world anti-phishing systems. Using a testbed
for empirically measuring anti-phishing blacklists [43], we ﬁrst
deployed 150 carefully-controlled artiﬁcial PayPal-branded
phishing websites using new and previously unseen domain
names: 50 for each of the top three User Interaction cloaking
types we found in the wild (Notiﬁcation, Click Through
with a fake CAPTCHA, and Mouse Detection). We then
simultaneously reported the URLs to key anti-phishing entities
across the ecosystem (Google Safe Browsing, PhishTank,
Netcraft, APWG, PayPal, and US CERT [44]) to evaluate if
the ecosystem can collectively detect our cloaked websites.
Lastly, we monitored the detection status (i.e., blacklisting) of
our websites in major web browsers (Google Chrome, Opera,
and Microsoft Edge, each powered by different detection
backends) over seven days.
At the conclusion of these experiments, we found that none
of our phishing websites were blacklisted in any browser, with
the exception of Click Through websites, 21 (42%) of which
were blocked in Microsoft Edge a median of 3 hours after
we reported them. The detection occurred because Microsoft
SmartScreen classiﬁed the obfuscation in the JavaScript source
code as malware, not because it was capable of bypassing
the cloaking technique itself. The fact that so many of our
websites remained unmitigated after a seven-day period shows
that client-side evasion methods are indeed effective at evading
detection by modern anti-phishing systems.
Manual
inspection is used by some anti-phishing enti-
ties [23]. Recurring suspicious websites that cannot be detected
by automated systems should go to manual inspection for
further analysis. With specialists’ inspection, any malicious
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:32:33 UTC from IEEE Xplore.  Restrictions apply. 
1119
Mouse
Detection
Count (%)
Click
Through
Count (%)
Notiﬁcation
Window
Count (%)
Can See
Cannot See
879 (100.00%)
0 (0.00%)
859 (97.72%)
20 (2.28%)
374 (42.55%)
505 (57.45%)
TABLE VIII: Experimental results on the effect of cloaking
techniques on users’ ability to see phishing content.
websites therein should be labeled as phishing and be black-
listed to protect users. Our observations, however, imply that
our test phishing websites may have simply been classiﬁed
as benign by anti-phishing systems and never sent for manual
review. We believe that this is a clear limitation of current anti-
phishing mitigations. Therefore, it is important for the whole
anti-phishing ecosystem to understand the nature and preva-
lence of client-side cloaking techniques used by sophisticated
phishing websites, especially when we consider the growth of
such websites [46].
B. Hampering Victim User Trafﬁc
To verify that client-side cloaking techniques in the User
Interaction category do not signiﬁcantly prevent users from
being exposed to phishing content on cloaked phishing web-
sites, we conducted an IRB-approved user study through
Amazon Mechanical Turk [2]. Using a free hosting provider,
we generated three websites: one with each of the same
three types of cloaking as considered in the previous section
(Notiﬁcation, Click Through with a fake CAPTCHA, and
Mouse Detection). Rather than hiding phishing content behind
the cloaking, however, we simply hid the text “Hello World”.
By default, a blank page would be shown. We then hired
1,000 workers in Amazon Mechanical Turk and requested
them to report what
they saw after visiting each of the
three websites [1]. We choose these three cloaking techniques
because they are unique to client-side (rather than server-side)
cloaking implementations, and because the other techniques
have been tested in a server-side context [43].
Table VIII shows the detailed experimental results. 121 of
the 1,000 workers could not view our phishing websites due
to a technical problem: their browsers automatically added
“www” in front of the sub-domains in our URLs, which
may occur in older versions of web browsers [14]. Thus, the
responses of 879 workers were suitable for analysis.
For the Mouse Movement cloaking technique, 100% of the
workers saw the “Hello World” text, and thus would have also
seen phishing content had they visited a malicious website.
For the Click Through websites, 97.72% saw the text, which
shows that this cloaking technique is also effective against
users. However, only 42.55% of the users saw the text on
websites with the Notiﬁcation Window cloaking technique.
Nearly all users who did not see the text (94.94%) opted to
deny notiﬁcations; the rest had incompatible browsers.
Although two of the cloaking techniques did not signiﬁ-
cantly prevent users from viewing the content, we found that
the Notiﬁcation Window cloaking technique has a negative
impact on phishing success rates against potential victims.
However, had these users been successfully deceived by a
phishing lure (e.g., one that conveys a sense of urgency) prior
to visiting the page, we believe that they would have been more
likely to allow notiﬁcations [55]. Moreover, given the fact that
websites with this cloaking technique were not detectable by
the anti-phishing ecosystem (as we showed in Section VII),
we still believe that this technique remains viable overall. In
fact, the website shown in Figure 5 was still online in January
2020 even though we ﬁrst observed the phishing URL in May
2019.
Consequently, we conclude that client-side cloaking tech-
niques in the User Interaction category enable phishing web-
sites to maintain proﬁtability through a much longer life span,
generally without discouraging victim visits, which in turn
allows phishers to harm more users.
C. Responsible Disclosure
Once we established that the cloaking techniques discovered
by CrawlPhish were capable of evading anti-phishing systems
while remaining effective against human victims, we disclosed
our ﬁndings, and the corresponding JavaScript code for each
technique tested, to the major anti-phishing blacklist operators:
Google, Microsoft, and Opera. All companies acknowledged
receipt of our disclosure. Google followed up by requesting
more information on the semantics and prevalence of the
cloaking techniques, and concurred with our ﬁnding that
such techniques could potentially bypass detection by current
automated anti-phishing systems.
VIII. COUNTERING CLIENT-SIDE CLOAKING TECHNIQUES
As we have observed, phishers make extensive use of
sophisticated evasion techniques in their phishing attacks.
The unique feature of client-side cloaking techniques is to
require visitors to interact with the website or browser, such
as through a button click or mouse movement. Phishers adopt
such strategies because they believe that their victims will
exhibit these behaviors when visiting a website [50]. If the
website is in the process of rendering and shows a blank
page, most people tend to move their mouse subconsciously.
Similarly, out of habit, users will click a button from a pop-
up or notiﬁcation window to make web page content appear.
We expect that phishers’ degree of sophistication will only
continue to grow. Therefore,
the ecosystem should ensure
that existing detection and mitigation systems are capable of
adapting to such evasion techniques.
To detect advanced phishing websites with client-side cloak-
ing techniques, anti-phishing crawlers should match the be-
haviors that sophisticated phishing kits expect. Speciﬁcally,
crawlers need to impersonate human behaviors such as mouse
movement and button clicks. To examine a given website, anti-
phishing systems can emulate such behaviors using automated
browsers. In addition, as we observed in our analysis, the
Notiﬁcation Window technique seems to exploit the lack of
support for web notiﬁcations by current automated browsers.
Thus, it is important for anti-phishing systems to close this gap
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:32:33 UTC from IEEE Xplore.  Restrictions apply. 
1120
and ensure that the browsers being used for detection support
the same features as those used by potential victims.
Also, CrawlPhish can be directly incorporated into existing
anti-phishing crawlers. With the hidden web page content
revealed by CrawlPhish alongside traditional attributes such
as URLs, we believe that current anti-phishing systems could
identify malicious websites that would otherwise evade de-
tection. Furthermore, by implementing CrawlPhish analysis,
crawlers would be able to more accurately classify and ﬁnger-
print new variants of evasion techniques employed phishing
websites, or even discover entirely new types of cloaking.
Such analysis would be particularly helpful
in countering
phishing websites that cannot currently be classiﬁed with high
conﬁdence.
IX. LIMITATIONS
Even though CrawlPhish uncovered a diverse array of
sophisticated client-side evasion techniques used in the wild,
our ﬁndings should be considered alongside certain limitations.
A. CrawlPhish Deployment
Data sources. The CrawlPhish framework is not a phishing
classiﬁcation system. Rather, it detects and classiﬁes cloaking
within known phishing websites. Thus, as its primary input,
CrawlPhish requires a curated feed of phishing URLs (i.e.,
detected by an existing anti-phishing system, whether manual
or automated). However, our framework could also be adapted
for use on unconﬁrmed phishing URLs with targeted additions
to the visual similarity check of the framework [63], such
that benign website screenshots could be differentiated from
deceptive ones.
Data collection. Due to infrastructure limitations, we were
only able to crawl live phishing websites over a total of 14
months from June to December 2018 and May to November
2019, with a 4-month gap in between. Differences in brand
distribution between the two years may skew our ﬁndings with
respect to the commonality of cloaking techniques. Although
additional crawling would be desirable for a more thorough
longitudinal evaluation, we mitigated this limitation by also
evaluating CrawlPhish on a public dataset of 100,000 phishing
websites from 2019, and by analyzing distinct implementations
of each cloaking technique, as discussed in Section VI-C.
Phishing websites may leverage server-side cloaking with
various degrees of sophistication [44, 46]. Although we sought
to defeat simple IP and geolocation cloaking potentially
implemented by the phishing websites which we crawled,
other techniques may have evaded our crawler, and, thus, the
corresponding phishing website client-side source code would
be absent from our dataset.
Semantic cloaking categorization. When querying the Crawl-
Phish cloaking technique database to determine the type of
cloaking used by a phishing website, we set ﬁxed similarity
thresholds for different classes of cloaking techniques. As a
result, our approach may misclassify evasion code which
combines multiple cloaking techniques, or fail to trigger man-
ual analysis of certain novel cloaking techniques. However, as
shown in our evaluation in Section VI-B, we did not observe
such failures in our analysis.
B. Cloaking Detection
Execution time. Forced execution of a small percentage
(1.75%) of websites in our dataset could not be completed
within a reasonably short time period, and, thus, resulted in
false-negative detections of cloaking. Across our deployment,
we chose a 195-second idle timeout: the maximum period
without a change to the execution path, after which execu-
tion is halted. This timeout allowed 98% of websites (three
standard deviations above the mean) to ﬁnish, as determined
by the sampling in Section IV-B. Another limitation of setting
an execution time limit is that some execution paths may be
omitted if the time limit is reached. A future implementation
of CrawlPhish could ensure that all paths of a code snippet
have ﬁnished examination by comparing the actual paths in
the script to those that have been force-executed.
We found that the websites which failed to be fully executed
contained long-running code within individual loops. J-Force
seeks to mitigate this limitation by enforcing (by default) a
cutoff of 80,000 iterations for each loop. Although this cutoff
proved insufﬁcient
in the aforementioned 1.75% of cases,
given the low false-negative rate, we do not consider it as
a signiﬁcant issue: ﬁne-tuning the J-Force loop timeout could
be used to further optimize execution times.
Nevertheless, adversaries with knowledge of our analysis
technique could design code to bypass it by introducing a
large number of individual loops ahead of the path which
ultimately displays phishing content. To further overcome this
and other types of deliberate circumvention, additional code
analysis techniques, such as symbolic execution [35], could
be applied in cases in which forced execution fails.
Execution environment. We force-executed phishing web-
sites’ JavaScript using the WebKitGTK+ web browser [34].
Historically, there has been evidence of malicious JavaScript
that only targets a speciﬁc web browser (or engine) [32]. Thus,
CrawlPhish may have failed to correctly classify code tar-
geted at other browsers. To overcome this limitation, websites
marked as uncloaked by the current implementation of Crawl-
Phish could be force-executed in additional environments.
Asynchronous content delivery. CrawlPhish does not con-
sider cases where asynchronous web requests (i.e., AJAX)
submit data about the client to the server and so that the
server can determine whether phishing web page content
should be sent back to the client (this equates to server-
side cloaking with the prerequisite of client-side JavaScript
execution, and has been previously studied [43]). Also there
was no evidence in our dataset that client-side cloaking is (yet)
being combined with AJAX and server-side cloaking by phish-
ing websites. However, CrawlPhish could still be enhanced
to automatically analyze the malicious use of asynchronous