title:Modeling distances in large-scale networks by matrix factorization
author:Yun Mao and
Lawrence K. Saul
Modeling Distances in Large-Scale Networks
by Matrix Factorization
Department of Computer and Information Science
Department of Computer and Information Science
Lawrence K. Saul
University of Pennsylvania
PI:EMAIL
Yun Mao
University of Pennsylvania
PI:EMAIL
ABSTRACT
In this paper, we propose a model for representing and pre-
dicting distances in large-scale networks by matrix factoriza-
tion. The model is useful for network distance sensitive ap-
plications, such as content distribution networks, topology-
aware overlays, and server selections. Our approach over-
comes several
limitations of previous coordinates-based
mechanisms, which cannot model sub-optimal routing or
asymmetric routing policies. We describe two algorithms |
singular value decomposition (SVD) and nonnegative matrix
factorization (NMF)|for representing a matrix of network
distances as the product of two smaller matrices. With such
a representation, we build a scalable system|Internet Dis-
tance Estimation Service (IDES)|that predicts large num-
bers of network distances from limited numbers of measure-
ments. Extensive simulations on real-world data sets show
that IDES leads to more accurate, e(cid:14)cient and robust pre-
dictions of latencies in large-scale networks than previous
approaches.
Categories and Subject Descriptors
C.2.1 [Computer-Communication Networks]: Net-
work Architecture and Design|Network topology; C.2.4
[Computer-Communication Networks]: Distributed
Systems
General Terms
Algorithms, Measurement, Performance
Keywords
Network distance, Matrix factorization
1.
INTRODUCTION
Wide-area distributed applications have evolved consid-
erably beyond the traditional client-server model, in which
a client only communicates with a single server.
In con-
tent distribution networks(CDN), peer-to-peer distributed
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’04, October 25–27, 2004, Taormina, Sicily, Italy.
Copyright 2004 ACM 1-58113-821-0/04/0010 ...$5.00.
hash tables(DHT) [16, 17, 18, 22], and overlay routing [2],
nodes often have the (cid:13)exibility to choose their communica-
tion peers. This (cid:13)exibility can greatly improve performance
if relevant network distances are known. For example, in a
CDN, an optimized client can download Web objects from
the particular mirror site to which it has the highest band-
width. Likewise, in DHT construction, a peer can route
lookup requests to the peer (among those that are closer to
the target in the virtual overlay network) with the lowest
latency in the IP underlay network.
Unfortunately, knowledge of network distances is not
available without cost. On-demand network measurements
are expensive and time-consuming, especially when the
number of possible communication peers is large. Thus, a
highly promising approach is to construct a model that can
predict unknown network distances from a set of partially
observed measurements [4, 6, 7, 12, 13, 20].
Many previously proposed models are based on the em-
bedding of host positions in a low dimensional space, with
network distances estimated by Euclidean distances. Such
models, however, share certain limitations.
In particular,
they cannot represent networks with complex routing poli-
cies, such as sub-optimal routing1 or asymmetric routing,
since Euclidean distances satisfy the triangle inequality and
are inherently symmetric. On the Internet, routing schemes
of this nature are quite common [3, 10, 15], and models that
do not take them into account yield inaccurate predictions
of network distances.
In this paper, we propose a model based on matrix fac-
torization for representing and predicting distances in large-
scale networks. The essential idea is to approximate a large
matrix whose elements represent pairwise distances by the
product of two smaller matrices. Such a model can be
viewed as a form of dimensionality reduction. Models based
on matrix factorization do not su(cid:11)er from the limitations of
previous work:
in particular, they can represent distances
that violate the triangle inequality, as well as asymmet-
ric distances. Two algorithms|singular value decomposi-
tion (SVD) and nonnegative matrix factorization (NMF){
are presented for learning models of this form. We evalu-
ate the advantages and disadvantages of each algorithm for
learning compact models of network distances.
The rest of the paper is organized as follows. Section 2
1With sub-optimal routing policies, the network distance
between two end hosts does not necessarily represent the
shortest path in the network. Such routing policies exist
widely in the Internet for various technical, political and
economic reasons.
reviews previous work based on the low dimensional embed-
ding of host positions in Euclidean space. Section 3 presents
the model for matrix factorization of network distances. The
SVD and NMF algorithms for learning these models from
network measurements are presented and evaluated in sec-
tion 4. Section 5 proposes an architecture to estimate dis-
tances required by an arbitrary host from low dimensional
reconstructions. The architecture is evaluated in section 6.
Finally, section 7 summarizes the paper.
2. NETWORK EMBEDDINGS
One way to predict network distance between arbitrary
Internet end hosts is to assign each host a \position" in a
(cid:12)nite-dimensional vector space. This can be done at the
cost of a limited number of network measurements to a set
of well-positioned infrastructure nodes2, or other peer nodes.
In such a model, a pair of hosts can estimate the network dis-
tance between them by applying a distance function to their
positions, without direct network measurement. Most previ-
ous work on these models has represented the host positions
by coordinates in Euclidean space and adopted Euclidean
distance as the distance function.
We de(cid:12)ne the problem formally as follows. Suppose there
are N hosts H = fH1; H2; (cid:1) (cid:1) (cid:1) ; HNg in the network. The
pairwise network distance matrix is a N (cid:2) N matrix D,
such that Dij (cid:21) 0 is the network distance from Hi to Hj.
A network embedding is a mapping H : H ! Rd such that
Dij (cid:25) ^Dij = kH(Hi) (cid:0) H(Hj)k; 8i; j = 1; : : : ; N
(1)
where ^Dij is the estimated network distance from Hi to
Hj and H(Hi) is the position coordinate of Hi as a d-
dimensional real vector. We simplify the coordinate nota-
tion from H(Hi) to ~Hi = (Hi1; Hi2; (cid:1) (cid:1) (cid:1) ; Hid). The network
distance between two hosts Hi and Hj is estimated by the
Euclidean distance of their coordinates:
^Dij = k ~Hi (cid:0) ~Hjk =  d
Xk=1
1
2
(Hik (cid:0) Hjk)2!
(2)
The main problem in constructing a network embedding
is to compute the position vectors ~Hi for all hosts Hi from a
partially observed distance matrix D. A number of learning
algorithms have been proposed to solve this problem, which
we describe in the next section.
2.1 Previous work
The (cid:12)rst work in the network embedding area was done
by Ng and Zhang [13], whose Global Network Position-
ing (GNP) System embedded network hosts in a low-
dimensional Euclidean space. Many algorithms were sub-
sequently proposed to calculate the coordinates of network
hosts. GNP uses a Simplex Downhill method to minimize
the sum of relative errors:
total err =Xi Xj
jDij (cid:0) ^Dijj
Dij
(3)
The drawback of GNP is that the Simplex Downhill method
converges slowly, and the (cid:12)nal results depend on the initial
values of the search. PIC [4] applies the same algorithm
2referred as landmark nodes in this paper. They are also
called beacon nodes.
H(cid:13)1(cid:13)
1(cid:13)
H(cid:13)3(cid:13)
1(cid:13)
1(cid:13)
H(cid:13)2(cid:13)
H(cid:13)4(cid:13)
1(cid:13)
((cid:13)-(cid:13)0.5,0.5)(cid:13)
(0.5,0.5)(cid:13)
H(cid:13)1(cid:13)
H(cid:13)3(cid:13)
(0,0)(cid:13)
H(cid:13)2(cid:13)
H(cid:13)4(cid:13)
((cid:13)-(cid:13)0.5,(cid:13)-(cid:13)0.5)(cid:13)
(0.5,(cid:13)-(cid:13)0.5)(cid:13)
Network Topology(cid:13)
One Possible 2(cid:13)-(cid:13)D Embedding(cid:13)
Figure 1: Four hosts H1 (cid:0) H4 in a simple network
topology
to the sum of squared relative errors and studies security-
related issues.
Cox, Dabek et. al. proposed the Vivaldi algorithm [5, 6]
based on an analogy to a network of physical springs.
In
this approach, the problem of minimizing the sum of errors
is related to the problem of minimizing the potential energy
of a spring system. Vivaldi has two main advantages: it is
a distributed algorithm, and it does not require landmark
nodes.
Lim et. al. [12] and Tang et. al. [20] independently pro-
posed models based on Lipschitz embeddings and Princi-
pal Component Analysis (PCA). These models begin by
embedding the hosts in an N -dimensional space, where
the coordinates of the host Hi are given by its distances
(Di1; (cid:1) (cid:1) (cid:1) ; DiN ) to N landmark nodes. This so-called Lip-
schitz embedding has the property that hosts with simi-
lar distances to other hosts are located nearby in the N -
dimensional space. To reduce the dimensionality, the host
positions in this N -dimensional space are then projected into
the d-dimensional subspace of maximum variance by PCA.
A linear normalization is used to further calibrate the re-
sults, yielding the (cid:12)nal host positions ~Hi 2 Rd.
2.2 Limitations
Euclidean distances are inherently symmetric; they also
satisfy the triangle inequality. Thus, in any network embed-
ding,
^Dij = ^Dji
8i; j
^Dij + ^Djk (cid:21) ^Dik 8i; j; k
These two properties are inconsistent with observed network
distances. On the Internet, studies indicate that as many as
40% of node pairs of real-world data sets have a shorter path
through an alternate node[3, 20]. Another study shows that
asymmetric routing is quite common [15]; even for the same
link, the upstream and downstream capacities may be very
di(cid:11)erent [10].
In addition to these limitations, low-dimensional embed-
dings of host positions cannot always model distances in
networks where there are pairs of nodes that do not have
a direct path between them, even if the distances are sym-
metric and satisfy triangle inequality. Figure 1 illustrates a
simple network topology in which four hosts in di(cid:11)erent au-
tonomous systems are connected with unit distance to their
neighbors. An intuitive two-dimensional embedding is also
shown. In the given embedding, the estimated distances are
^D14 = ^D23 = p2, but the real distances are D14 = D23 = 2.
It is provable that there exists no Euclidean space embed-
ding (of any dimensionality) that can exactly reconstruct
the distances in this network. Similar cases arise in net-
works with tree-like topologies.
clusters of nearby nodes in the network which have similar
distances to distant nodes. In this case, the distance matrix
D will be well approximated by the product of two smaller
matrices.
3. DISTANCE MATRIX FACTORIZATION
The limitations of previous models lead us to consider
a di(cid:11)erent framework for compactly representing network
distances. Suppose that two nearby hosts have similar dis-
tances to all the other hosts in the network. In this case,
their corresponding rows in the distance matrix will be
nearly identical. More generally, there may be many rows in
the distance matrix that are equal or nearly equal to linear
combinations of other rows. Recall from linear algebra that
an N (cid:2) N matrix whose rows are not linearly independent
has rank strictly less than N and can be expressed as the
product of two smaller matrices. With this in mind, we seek
an approximate factorization of the distance matrix, given
by:
D (cid:25) XY T ;
where X and Y are N (cid:2) d matrices with d (cid:28) N . From such
a model, we can estimate the network distance from Hi to
Hj by ^Dij = ~Xi (cid:1) ~Yj, where ~Xi is the ith row vector of the
matrix X and ~Yj is the jth row vector of the matrix Y .
More formally, for a network with distance matrix Dij ,
we de(cid:12)ne a distance matrix factorization as two mappings
X : H ! Rd;
: H ! Rd;
Y
and an approximate distance function computed by
^Dij = X(Hi) (cid:1) Y (Hj ):
As shorthand, we denote X(Hi) as ~Xi and Y (Hi) as ~Yi, so
that we can write the above distance computation as:
d
^Dij = ~Xi (cid:1) ~Yj =
Xk=1
XikYjk:
(4)
Note that in contrast to the model in section 2, which
maps each host to one position vector, our model associates
two vectors with each host. We call ~Xi the outgoing vector
and ~Yi the incoming vector for Hi. The estimated distance
from Hi to Hj is simply the dot product between the out-
going vector of Hi and the incoming vector of Hj .
Applying this model of network distances in distributed
applications is straightforward. For example, consider the
problem of mirror selection. To locate the closest server
among several mirror candidates, a client can retrieve the
outgoing vectors of the mirrors from a directory server, cal-
culate the dot product of these outgoing vectors with its
own incoming vector, and choose the mirror that yields the
smallest estimate of network distance (i.e., the smallest dot
product).
Our model for representing network distances by matrix
factorization overcomes certain limitations of models based
on low dimensional embeddings. In particular, it does not
require that network distances are symmetric because in
general ^Dij = ~Xi (cid:1) ~Yj 6= ~Xj (cid:1) ~Yi = ^Dji. Distances com-
puted in this way also are not constrained to satisfy the
triangle inequality. The main assumption of our model is
that many rows in the distance matrix are linearly depen-
dent, or nearly so. This is likely to occur whenever there are
4. DISTANCE RECONSTRUCTION
In this section we investigate how to estimate outgoing
and incoming vectors ~Xi and ~Yi for each host Hi from the
distance matrix D. We also examine the accuracy of models
that approximate the true distance matrix by the product
of two smaller matrices in this way.
The distance matrix D can be viewed3 as storing N
row-vectors in N -dimensional space. Factoring this matrix
D (cid:25) XY T is essentially a problem in linear dimensionality
reduction, where Y stores d basis vectors and X stores the
linear coe(cid:14)cients that best reconstruct each row vector of
D. We present two algorithms for matrix factorization that
solve this problem in linear dimensionality reduction.
4.1 Singular value decomposition
An N (cid:2) N distance matrix D can be factored into three
matrices by its singular value decomposition (SVD), of the
form:
D = U SV T ;
1
2
where U and V are N (cid:2) N orthogonal matrices and S is an
N (cid:2)N diagonal matrix with nonnegative elements (arranged
in decreasing order). Let A = U S
2 V , where
2 and B = S
ii = pSii. It is easy to see that ABT = U S
2 )T =
2 V T = D. Thus SVD yields an exact factorization
U S
D = ABT , where the matrices A and B are the same size
as D.
2 (V S
S
1
1
2 S
1
1
1
1
We can also use SVD, however, to obtain an approximate
factorization of the distance matrix into two smaller matri-
ces. In particular, suppose that only a few of the diagonal
elements of the matrix S are appreciable in magnitude. De-
(cid:12)ne the N (cid:2) d matrices:
Xij = UijpSjj ;
Yij = VijpSjj ;
(5)
(6)
where i = 1 : : : N and j = 1 : : : d. The product XY T is
a low-rank approximation to the distance matrix D; if the
distance matrix is itself of rank d or less, as indicated by
Sjj = 0 for j > d, then the approximation will in fact be
exact. The low-rank approximation obtained from SVD can
be viewed as minimizing the squared error function
Xi Xj
(Dij (cid:0) ~Xi (cid:1) ~Yj )2
(7)
with respect to Xi 2 Rd and Yj 2 Rd. Eqs. (5) and (6)
compute the global minimum of this error function.
Matrix factorization by SVD is related to principal com-
ponent analysis (PCA) [9] on the row vectors. Principal
3Note that D does not have to be a square matrix of pairwise
distances. It can be the distance matrix from one set of N