approach is fundamentally limited. We believe the reasons are
two-fold. First, given the limited number of drifting samples,
it is difﬁcult to derive an accurate approximation model for the
decision boundary. Second and more importantly, the outlier
space is much bigger than the in-distribution region. Given
the drifting samples are far away from the decision boundary,
it is difﬁcult to ﬁnd a small set of feature perturbations to take
the drifting sample to cross the decision boundary and enter
the in-distribution region. Without the ability to cross the
boundary, the explanation methods do not have the necessary
gradients (or feedback) to compute feature importance.
Our Method: Distance-based Explanation. Motivated
by this observation, we propose a new approach that identiﬁes
important features by explaining the distance (i.e. the red
arrow in Figure 3). Unlike supervised classiﬁers that make
decisions based on the decision boundary, the drift detection
model makes decisions based on the sample’s distance to
centroids. As such, we aim to ﬁnd a set of original features
that help to move the drifting sample xxxt toward the nearest
centroid cccyt . With this design, we no longer need to force xxxt
to cross the boundary, which is hard to achieve. Instead, we
perturb the original features and observe the distance changes
in the latent space.
To realize this idea, we need to ﬁrst design a feature pertur-
bation mechanism. Most existing perturbation methods are
designed exclusively for images [18], the features of which
are numerical values. In our case, features in xxxt can be either
numerical or categorical, and thus directly applying existing
methods will produce ill-deﬁned feature values. To ensure the
perturbations are meaningful for both numerical and categori-
cal features, we propose to perturb xxxt by replacing its feature
value with the value of the corresponding feature in a refer-
ence training sample xxx(c)
is the training sample that
has the shortest latent distance to the centroid cccyt . As such,
our explanation goal is to identify a set of features, such that
substituting them with those in xxx(c)
yt will impose the highest
inﬂuence upon the distance between f (xxxt ) and cccyt . Replacing
yt . This xxx(c)
yt
2Note that we do not perform feature perturbation in the latent space,
because the latent features do not carry semantic meanings. Instead, we select
features in the original input space.
the feature values with those of xxx(c)
yt also helps to ensure the
perturbed sample is moving towards the rough direction of the
centroid. As before, the perturbation is done in the original
feature space where features have semantic meanings.
We use an mmm ∈ Rq×1 to represent the important features, in
which mmmi = 1 means (xxxt )i is replaced by the value of (xxx(c)
yt )i
and mmmi = 0 means we keep the value of (xxxt )i unchanged. In
other words, mmmi = 1 indicates the ith feature is selected as
the important one. Each element in this feature mask mmmi can
be sampled from a Bernoulli distribution with probability
pi. As such, we could guarantee that mmmi equals to either 1
and 0. Then, our goal is transformed into solving the pi for
i = 1,2, ...,q. Technically, this can be achieved by minimizing
the following objective function with respect to p1:q.
Emmm∼Q(ppp)(cid:107)ˆzzzt − cccyt(cid:107)2 + λ1R(mmm,bbb),
yt (cid:12) (mmm(cid:12) bbb)),
ˆzzzt = f (xxxt (cid:12) (1− mmm(cid:12) bbb) + xxx(c)
R(mmm,bbb) = (cid:107)mmm(cid:12) bbb(cid:107)1 +(cid:107)mmm(cid:12) bbb(cid:107)2, Q(ppp) =
(2)
p(mmmi|pi).
q
∏
i=1
Note that (cid:12) denotes the element-wise multiplication; ˆzzzt rep-
resents the latent vector of the perturbed sample. Given the
equation above, directly computing mmm is difﬁcult due to its
high dimensionality. To speed up the search, we introduce a
ﬁlter bbb to pre-ﬁlter out features that are not worth considering.
We set (bbb)i = 0, if (xxxt )i and (xxx(c)
yt )i are the same. In other
words, if a feature value of xxxt is already the same as that of
the reference sample xxx(c)
yt , then this feature is ruled out in the
optimization (since it has no impact on distance change). In
yt (cid:12) (mmm(cid:12)bbb)) represents
this way, ˆzzzt = f (xxxt (cid:12) (1−mmm(cid:12)bbb) +xxx(c)
the latent vector of the perturbed sample.
In Eqn. (2), the ﬁrst term in the loss function aims to mini-
mize the latent-space distance between the perturbed sample
ˆzzzt and the centroid cccyt of the yt class. Each element in mmm
is sampled from a Bernoulli distribution parameterized by
pi. Here, we use Q(ppp) to represent their joint distribution.3
For the second term, λ is a hyper-parameter that controls
the strength of the elastic-net regularization R(·), which re-
stricts the number of non-zero elements in mmm. By minimizing
R(mmm,bbb), the optimization procedure selects a minimum subset
of important features.
Note that Bernoulli distribution is discrete, which means
the gradient of mmmi with respect to pi (i.e. ∂mmmi
) is not well de-
∂pi
ﬁned. We cannot solve the optimization problem in Eqn. 2 by
using a gradient-based optimization method. To tackle this
challenge, we apply the change-of-variable trick introduced
in [45]. We enable the gradient computation by replacing
the Bernoulli distribution with its continuous approximation
(i.e. concrete distribution) parameterized by pi. Then we can
solve the parameters p1:q through a gradient-based optimiza-
tion method (we use Adam optimizer in this paper).
3We assume each feature is independently drawn from a distinct Bernoulli
distribution.
2332    30th USENIX Security Symposium
USENIX Association
Family
FakeInstaller
DroidKungFu
Plankton
GingerMaster
BaseBridge
Iconosys
Kmin
FakeDoc
Id
0
1
2
3
4
5
6
7
Total:
# of Samples
925
667
625
339
330
152
147
132
3,317
Table 1: Android malware samples from the Drebin dataset.
4 Evaluation: Drifting Detection
In this section, we evaluate our system using two security ap-
plications: Android malware family attribution, and network
intrusion detection. In this current section (Section 4), we
focus on the evaluation of the drifting detection module. We
will evaluate the explanation module in Section 5. After these
controlled experiments, we tested our system with a security
company on their malware database (Section 7).
4.1 Experimental Setup and Datasets
Android Malware Attribution.
We use the Drebin
dataset [7] to explore the malware family attribution problem.
The original classiﬁer (module 0 in Figure 1) is a multilayer
perceptron (MLP) classiﬁer. It identiﬁes which family a mal-
ware sample belongs to. The Drebin dataset contains 5,560
Android malware samples. For this evaluation, we select 8
families 4 where each family has at least 100 malware samples
(3,317 samples in total) as shown in Table 1.
To evaluate the drifting sample detection module, for each
experiment, we pick one of the 8 families as the previously
unseen family. For example, suppose we pick FakeDoc (fam-
ily 7) as the previous unseen family. We split the other seven
families into training and testing sets, and add FakeDoc only
to the testing set. In this way, FakeDoc is not available dur-
ing training. Our goal is to correctly identify samples from
FakeDoc as drifting samples in the testing time.
We split the training-testing sets with a ratio of 80:20. The
split is based on the timestamp (malware creation time), which
is recommended by several works [52, 65] to simulate a re-
alistic setting. Time-based split also means we cannot use
any new features that only appear in the testing set for model
training. This leaves us with 7,218 features. We then use
scikit-learn’s VarianceThreshold function [51] to remove fea-
tures with very low variance (i.e., <0.003), which creates a
ﬁnal set of 1,340 features.
4Two families FakeInstaller and Opfake are very similar in terms of their
nature of attacks. There is strong disagreement among AV-engines regarding
their family labels, i.e., the samples are labeled as one family by some engines
but are labeled as the other family by other engines. As such, we only included
FakeInstaller (Table 1).
Family
Benign
SSH-Bruteforce
DoS-Hulk
Inﬁltration
Id
0
1
2
3
Total:
# of Flows
66,245
11,732
43,487
9,238
130,702
Table 2: Network intrusion dataset: 3 network intrusion
classes and 1 benign class from the IDS2018 dataset.
To demonstrate the generalizability of results, we iteratively
select each of the malware families to be the “unseen family”
and repeat the experiments.
Network Intrusion Detection. We use a network intru-
sion dataset [57], which we refer to as IDS2018. The dataset
contains different types of network traces generated by known
attacks. For our evaluation, we select the benign class (one
day’s trafﬁc) and 3 different attack classes: SSH-Bruteforce,
Dos-Hulk, and Inﬁltration. SSH-Bruteforce is a brute-force
attack to guess the SSH login password. DoS-Hulk attack
aims to ﬂood the targeted machine with superﬂuous requests
in an attempt to make the machine temporally unavailable.
Inﬁltration attack ﬁrst sends an email with a malicious attach-
ment to exploit an on-host application’s vulnerability, and
then leverages the backdoor to run port-scan to discover more
vulnerabilities. We refer interested readers to [57] for more
details about the attacks. To speed up the experiments and
test different setups, we use 10% of their trafﬁc for the ex-
perimental dataset (Table 2). In Appendix D, we show that
more trafﬁc only increases the computational overhead and
has a negligible inﬂuence on the performance of the selected
methods.
We iteratively pick one of the attack families as the pre-
viously unseen family and only include this family in the
testing set. We repeat the experiments to report the average
performance. We split the train-test sets with a ratio of 80:20.
Note that features in the IDS2018 dataset need to be further
normalized and encoded. To be realistic, we only use the
training data to build the feature encoding scheme. At the
high-level, each sample represents a network ﬂow. Categori-
cal features such as “destination port” and “network protocol”
are encoded with one-hot encoding. The other 77 statistical
features are normalized between 0 and 1 with a MinMaxS-
caler. Each network ﬂow has 83 features. The detailed feature
engineering steps are available in the documentation of our
released code.
Evaluation Metric.
For the drifting detection module
(module  in Figure 1), the positive samples are samples
in the unseen family in the testing set. The negative samples
are the rest of the testing samples from the known families.
Given a ranked list of detected samples, we simulate an an-
alyst inspecting samples from the top of the list. As we go
down the list, we calculate three evaluation metrics: preci-
sion, recall, and F1 score. Precision measures the ratio of true
USENIX Association
30th USENIX Security Symposium    2333
(a) CADE
(b) Transcend
(c) Vanilla AE
Figure 4: Precision and recall vs. number of inspected samples (detected drifting samples are ranked by the respective method).
Method
Vanilla AE
Transcend
CADE
Precision
0.63 ± 0.17
0.76 ± 0.19
0.96 ± 0.05
Drebin (Avg±Std)
F1
Recall
0.88 ± 0.13
0.90 ± 0.14
0.96 ± 0.04
0.72 ± 0.15
0.80 ± 0.12
0.96 ± 0.03
Norm. Effort
1.48 ± 0.31
1.29 ± 0.45
1.00 ± 0.09
Precision
0.61 ± 0.16
0.64 ± 0.45
0.98 ± 0.02
IDS2018 (Avg±Std)
Recall
F1
0.99 ± 0.00
0.67 ± 0.47
0.93 ± 0.09
0.74 ± 0.12
0.65 ± 0.46
0.96 ± 0.06
Norm. Effort
1.74 ± 0.40
1.45 ± 0.57
0.95 ± 0.07
Table 3: Drifting detection results for Drebin and IDS2018 datasets. We compare CADE with two baselines Transcend [38] and
Vanilla AE. For each evaluation metric, we report the mean value and the standard deviation across all the settings.
code of Transcend from the authors and follow the paper to
adapt the implementation to support multi-class classiﬁca-
tion (the original code only supports binary classiﬁcation).
Speciﬁcally, we initialize the non-conformity measure with
−p where p is the softmax output probability indicating the
likelihood that a testing sample belongs to a given family.
Then we calculate the credibility p-value for a testing sample.
If the p-value is near zero for all existing families, we consider
it as a drifting sample. We rank drifting samples based on the
maximum credibility p-value. Note that we did not use other
OOD detection methods [14, 41, 49] as our baseline mainly
because they work in a different setup compared with CADE
and Transcend. More speciﬁcally, these methods require an
auxiliary OOD dataset in the training process and/or modi-