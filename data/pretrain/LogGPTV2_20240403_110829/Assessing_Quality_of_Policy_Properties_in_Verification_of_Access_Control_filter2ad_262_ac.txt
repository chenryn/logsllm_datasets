0
9
7
0
9
49
# kill
0
1
2
2
2
2
0
0
1
10
# kill
2
3
3
3
0
0
0
0
1
12
100
50
43
33
0
0
0
0
11
24.49
SIMPLE
kill % # new kill
0
1
2
2
2
2
0
0
2
11
0
100
100
100
100
100
0
0
50
90.91
0
100
100
100
100
100
0
0
100
100
new kill %
100
50
100
33
0
0
17
0
78
48.98
Table 3 summarizes the number of generated mutants,
the number of killed mutants, and the mutant-killing ratio
for each policy for both the existing property set P and the
augmented property set Pnew. Each row of the table cor-
responds to a particular mutation operator and the column
groups 1 and 2 correspond to the SIMPLE policy and the
CONTINUE policy, respectively.
In the case study, we do not use some mutation operators
if they result in equivalent mutants for the given policy or
if the property veriﬁcation tool cannot handle a particular
XACML feature. For example, we omit RCT (Rule Condi-
tion True) and RCF (Rule Condition False) operations since
the policies do not use that feature of rule conditions due
to a limitation in the current Margrave. In addition, Mar-
grave sometimes reports errors during property veriﬁcation
of some mutants. For instance, some properties require that
certain elements exist in a given policy. Mutation opera-
tions such as PSTT (Policy Set Target True) may remove
elements that are necessary in order to verify a particular
property. These errors correctly indicate semantic faults in
mutant policies and so we consider them killed.
The PTT (Policy Target True) and PTF (Policy Target
False) operators delete or modify the top-level policy ele-
ment in the SIMPLE policy effectively; these operators re-
move one of the two rules resulting in drastic semantic dif-
ferences that are immediately detected by the both P and
Pnew. On the other hand, the CPC (Change Policy Combin-
ing Algorithm) and CRC (Change Rule Combining Algo-
rithm) mutation operators generate equivalent mutants (i.e.,
mutants that are semantically equivalent to the original pol-
icy) that cannot be killed. For the CONTINUE policy, the
CRC generates equivalent mutants. Such semantic equiv-
alent mutants are detected and not considered in our case
study.
Table 3 shows that the existing property set P can kill
50% and 11% of CRE mutants in SIMPLE and CONTINUE
policies, respectively. By manually specifying a property
that mirrors the not-covered rule for the SIMPLE policy,
we can kill all CRE mutant policies. As expected, the
augmented property set Pnew increases mutant-killing ra-
tios only in the CRE mutants since the remaining mutants
cannot be killed. For the CONTINUE policy, we manually
specify six properties and the augmented property set Pnew
kills 78% of the CRE mutants. The two not-covered rules
(i.e., “living” CRE mutants) cannot be killed. Pnew also
increases the mutant-killing ratios for other mutation oper-
ators.
We observed that some types of mutants cannot be killed
with Pnew. For example, PSTT, CPC, and CRC mutants
and RTT, RTF, and CRC mutants are not killed. These mu-
tants may not be semantically equivalent to the original pol-
icy. However, the property set Pnew is not sufﬁcient to kill
these mutants.
6. Discussion
We believe that our approach can be applied to assess
the quality of a property set against policies written in lan-
guages other than XACML. Previous approaches converted
policies in one language (such as XACML) to other lan-
guages (such as Alloy [12], RW [23], and Description Log-
ics [14]) that are equipped with veriﬁcation tools. As our
approach requires property veriﬁcation (against a policy and
its mutants) provided by these veriﬁcation tools, such con-
version enables our approach to also be applicable to poli-
cies languages other than XACML and with veriﬁcation
tools other than Margrave.
Our approach to mutation veriﬁcation provides a qual-
ity assessment of a property set for a policy. If a property
set achieves a mutant-killing ratio of 100%, can we say that
the property set is exhaustive or complete? This situation is
similar to statement coverage in software testing. If a test
suite achieves 100% statement coverage for a given pro-
gram, can we say the test suite can detect all faults in the
program? The answer, of course, is absolutely not. While
mutation veriﬁcation serves as a quality assessment for a
160170
property set and, with the CRE mutation operator, identiﬁes
which properties interact with which rules in the policy, it
may not consider more abstract, generic properties. For ex-
ample, P r1 of the illustrative example in Section 2 ensures
that a student cannot assign grades. While this property is
an intuitive one of the problem domain, it is not explicitly
expressed in the policy itself. This particular policy con-
tains only rules that allow access whereas this property is
concerned with denying access. The fact that this property
does not interact with the rules in the policy does not im-
ply that it is not needed. A better example is discussed in
Section 5 where a property serves as more of the best prac-
tice that is not related to the problem domain of the access
control.
Future work shall investigate a means of automatically
generating various types of properties to cover more rules
and entities in an access control policy. In our case study,
we manually generated properties to cover not-covered
rules based on the mutation veriﬁcation results for the CRE
mutation operator. As we extracted these properties from
(explicitly expressed) not-covered rules, each of the proper-
ties is speciﬁcally effective to kill the (previously un-killed)
rule. But these properties may not kill other mutants. As
existing properties often describe more general behaviors
of a policy, further exploration of mutation operators for
mutation veriﬁcation is needed to investigate how to reﬂect
relevant properties (that are not necessarily speciﬁed in the
policy itself) in the mutation veriﬁcation process.
7. Related Work
To the best of our knowledge, no metric has been de-
ﬁned to quantify the coverage of a policy or model by some
property set. Our related previous approach on policy mu-
tation testing [17] deﬁned a fault model and corresponding
automated mutator in order to quickly assess the quality of
a test suite; the assessment results can be further used to
assess test-generation and test-selection techniques in terms
of fault-detection capability. Such policy mutation testing is
related to the approach proposed by Ammann et al. [3] that
mutates a model (corresponding to a policy in our work)
and then uses the model mutants to assess the quality of a
test suite. Our new approach leverages a variation of an au-
tomated mutator [17] in our implementation of the mutation
veriﬁcation approach. However, different from these previ-
ous approaches on assessing the quality of a test suite, our
new approach focuses on assessing the quality of a property
set based on mutating a policy.
To help ensure the correctness of policy speciﬁcations,
researchers and practitioners have developed formal veriﬁ-
cation tools for policies. Several policy veriﬁcation tools
are developed speciﬁcally for ﬁrewall policies. Al-Shaer
and Hamed [2] developed the Firewall Policy Advisor to
classify and detect policy anomalies. Yuan et al. [22] devel-
oped the FIREMAN tool to detect misconﬁguration of ﬁre-
wall policies.
There are also several veriﬁcation tools available for
XACML policies [1]. Hughes and Bultan [11] translated
XACML policies to the Alloy language [12], and checked
their properties using the Alloy Analyzer. Schaad and Mof-
fett [19] also leverage Alloy to check that role-based access
control policies do not allow roles to be assigned to users
in ways that violate separation-of-duty constraints. Zhang
et al. [24] developed a model-checking algorithm and tool
support to evaluate access control policies written in RW
languages, which can be converted to XACML [23]. Ko-
laczek [13] proposes to translate role-based access control
policies into Prolog for veriﬁcation. Kolovski et al. [14] for-
malize XACML policies with description logics (DL), which
are a decidable fragment of the ﬁrst-order logic, and exploit
existing DL veriﬁers to conduct policy veriﬁcation. Fisler
et al. [9] developed Margrave, which can verify XACML
policies against properties, if properties are speciﬁed, and
perform change-impact analysis on two versions of poli-
cies when properties are not speciﬁed. When Margrave de-
tects property violations during policy veriﬁcation, it auto-
matically generates concrete counterexamples in the form
of speciﬁc requests that illustrate violations of the speci-
ﬁed properties. Similarly, when Margrave detects semantic
differences during change-impact analysis, it automatically
generates speciﬁc requests that reveal semantic differences
between two versions of a policy. Most of these approaches
require user-speciﬁed properties to be veriﬁed. Our new ap-
proach complements these existing policy veriﬁcation ap-
proaches because our approach helps assess the quality of
the properties during policy veriﬁcation.
Our previous work [16] proposed an approach to policy
property inference via machine learning. Such properties
are often not available in practice and their elicitation is a
challenging and tedious task. Furthermore, once proper-
ties are deﬁned, it is difﬁcult to assess their effectiveness
and identify potential problematic areas that need improve-
ment. Our mutation veriﬁcation approach intends to help al-
leviate that challenge. Our implementation leverages Mar-
grave’s property veriﬁcation feature to verify mutant poli-
cies against properties.
Although various coverage criteria [25] for software pro-
grams exist, only recently have coverage criteria for access
control policies been proposed [18]. Policy coverage crite-
ria are needed to measure how well policies are tested and
which parts of the policies are not covered by the existing
test inputs. Martin et al. [18] deﬁned policy coverage and
developed a policy coverage measurement tool. Because it
is tedious for developers to manually generate test inputs
for policies, and manually generated test inputs are often
not sufﬁcient for achieving high policy coverage, they de-
161171
veloped several test generation techniques. Different from
these policy testing approaches, our new approach focuses
on assessing the quality of properties in policy veriﬁcation.
8. Conclusion
The need for carefully controlling access to sensitive
information is increasing as the amount and availability
of data are growing.
In order to separate the semantics
of access control from the system itself, policy authors
increasingly specify access control policies in declarative
languages such as XACML. Doing so facilitates managing,
maintaining, and analyzing policies. To increase conﬁdence
in the correctness of speciﬁed policies, policy authors can
formally verify policies against a property set. Policy veri-
ﬁcation is an important technique for high assurance of the
correct speciﬁcation of access control policies. Since the ef-
fectiveness of the veriﬁcation process is directly dependent
on the quality of the properties, we have proposed a novel
approach called mutation veriﬁcation to assess the quality
of a property set in veriﬁcation of access control policies.
We have implemented a tool for the approach being applied
on XACML policies. We applied our mutation veriﬁcation
tool to policies and properties from a real-world software
system. Our experiences show that the performance of the
property veriﬁcation is encouraging and mutation veriﬁca-
tion can scale to sufﬁciently large access control policies.
Furthermore, mutation veriﬁcation is a complementary ap-
proach to property veriﬁcation by aiding in the elicitation of
properties.
Acknowledgment
This work is supported in part by NSF grant CNS-
0716579 and its NIST supplement.
References
[1] OASIS eXtensible Access Control Markup Language
http://www.oasis-open.org/
(XACML).
committees/xacml/, 2005.
[2] E. Al-Shaer and H. Hamed. Discovery of policy anomalies
in distributed ﬁrewalls. In Proc. INFOCOM, pages 2605–
2616, 2004.
[3] P. Ammann and P. E. Black. A speciﬁcation-based coverage
metric to evaluate test sets. In Proc. HASE, pages 239–248,
1999.
[4] P. E. Ammann, P. E. Black, and W. Majurski. Using model
In Proc.
checking to generate tests from speciﬁcations.
ICFEM, pages 46–54, 1998.
[5] T. A. Budd and A. S. Gopal. Program testing by speciﬁca-
tion mutation. Computer Languages, 10(1):63–73, 1985.
162172
[6] N. Damianou, N. Dulay, E. Lupu, and M. Sloman. The Pon-
der policy speciﬁcation language. In Proc. POLICY, pages
18–38, 2001.
[7] R. A. DeMillo, R. J. Lipton, and F. G. Sayward. Hints on test
data selection: Help for the practicing programmer. IEEE
Computer, 11(4):34–41, April 1978.
[8] D. F. Ferraiolo, D. R. Kuhn, and R. Chandramouli. Role-
Based Access Control. Artech House, Inc., 2003.
[9] K. Fisler, S. Krishnamurthi, L. A. Meyerovich, and M. C.
Veriﬁcation and change-impact analysis of
In Proc. ICSE, pages 196–205,
Tschantz.
access-control policies.
2005.
[10] G. Fraser and F. Wotawa. Using model-checkers for
mutation-based test-case generation, coverage analysis and
speciﬁcation analysis. In Proc. ICSEA, pages 16–21, 2006.
[11] G. Hughes and T. Bultan. Automated veriﬁcation of access
control policies. Technical Report 2004-22, Department of
Computer Science, University of California, Santa Barbara,
2004.
[12] D. Jackson, I. Shlyakhter, and M. Sridharan. A micromodu-
larity mechanism. In Proc. ESEC/FSE, pages 62–73, 2001.
[13] G. Kolaczek. Speciﬁcation and veriﬁcation of constraints in
role based access control for enterprise security system. In
Proc. WETICE, pages 190–195, 2003.
[14] V. Kolovski, J. Hendler, and B. Parsia. Analyzing web ac-
cess control policies. In Proc. WWW, pages 677–686, 2007.
[15] S. Krishnamurthi. The CONTINUE server (or, how i admin-
istered PADL 2002 and 2003). In Proc. PADL, pages 2–16,
2003.
[16] E. Martin and T. Xie. Inferring access-control policy proper-
ties via machine learning. In Proc. POLICY, pages 235–238,
2006.
[17] E. Martin and T. Xie. A fault model and mutation testing
of access control policies. In Proc. WWW, pages 667–676,
2007.
[18] E. Martin, T. Xie, and T. Yu. Deﬁning and measuring policy
coverage in testing access control policies. In Proc. ICICS,
pages 139–158, 2006.
[19] A. Schaad and J. D. Moffett. A lightweight approach to
speciﬁcation and analysis of role-based access control ex-
tensions. In Proc. SACMAT, pages 13–22, 2002.
[20] F. Somenzi.
CUDD: CU Decision Diagram Package.
http://vlsi.colorado.edu/˜fabio/CUDD/.
[21] Y. L. Traon, T. Mouelhi, and B. Baudry. Testing security
policies: Going beyond functional testing. In Proc. ISSRE,
pages 93–102, 2007.
[22] L. Yuan, J. Mai, Z. Su, H. Chen, C.-N. Chuah, and P. Mo-
hapatra. FIREMAN: A toolkit for FIREwall Modeling and
ANalysis. In Proc. S&P, pages 199–213, May 2006.
[23] N. Zhang, M. Ryan, and D. P. Guelev. Synthesising veriﬁed
access control systems in XACML. In Proc. FMSE, pages
56–65, 2004.
[24] N. Zhang, M. Ryan, and D. P. Guelev. Evaluating access
control policies through model checking. In Proc. InfoSec,
pages 446–460, 2005.
[25] H. Zhu, P. A. V. Hall, and J. H. R. May. Software unit test
coverage and adequacy. ACM Comput. Surv., 29(4):366–
427, 1997.