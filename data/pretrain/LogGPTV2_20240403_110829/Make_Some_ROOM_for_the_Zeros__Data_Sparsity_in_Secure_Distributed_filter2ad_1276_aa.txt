title:Make Some ROOM for the Zeros: Data Sparsity in Secure Distributed
Machine Learning
author:Phillipp Schoppmann and
Adri√† Gasc&apos;on and
Mariana Raykova and
Benny Pinkas
Make Some ROOM for the Zeros: Data Sparsity in Secure
Distributed Machine Learning
Phillipp Schoppmann
Humboldt-Universit√§t zu Berlin
PI:EMAIL
Mariana Raykova
Google
PI:EMAIL
ABSTRACT
Exploiting data sparsity is crucial for the scalability of many data
analysis tasks. However, while there is an increasing interest in ef-
ficient secure computation protocols for distributed machine learn-
ing, data sparsity has so far not been considered in a principled
way in that setting.
We propose sparse data structures together with their corre-
sponding secure computation protocols to address common data
analysis tasks while utilizing data sparsity. In particular, we de-
fine a Read-Only Oblivious Map primitive (ROOM) for accessing
elements in sparse structures, and present several instantiations
of this primitive with different trade-offs. Then, using ROOM as a
building block, we propose protocols for basic linear algebra op-
erations such as Gather, Scatter, and multiple variants of sparse
matrix multiplication. Our protocols are easily composable by using
secret sharing. We leverage this, at the highest level of abstraction,
to build secure protocols for non-parametric models (ùëò-nearest
neighbors and naive Bayes classification) and parametric models
(logistic regression) that enable secure analysis on high-dimensional
datasets. The experimental evaluation of our protocol implementa-
tions demonstrates a manyfold improvement in the efficiency over
state-of-the-art techniques across all applications.
Our system is designed and built mirroring the modular archi-
tecture in scientific computing and machine learning frameworks,
and inspired by the Sparse BLAS standard.
KEYWORDS
secure computation, machine learning, sparsity
ACM Reference Format:
Phillipp Schoppmann, Adri√† Gasc√≥n, Mariana Raykova, and Benny Pinkas.
2019. Make Some ROOM for the Zeros: Data Sparsity in Secure Distributed
Machine Learning. In 2019 ACM SIGSAC Conference on Computer and Com-
munications Security (CCS ‚Äô19), November 11‚Äì15, 2019, London, United King-
dom. ACM, New York, NY, USA, 16 pages. https://doi.org/10.1145/3319535.
3339816
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS ‚Äô19, November 11‚Äì15, 2019, London, United Kingdom
¬© 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-6747-9/19/11...$15.00
https://doi.org/10.1145/3319535.3339816
1
Adri√† Gasc√≥n
The Alan Turing Institute / University of Warwick
PI:EMAIL
Benny Pinkas
Bar-Ilan University
PI:EMAIL
1 INTRODUCTION
Machine Learning (ML) techniques are today the de facto way to
process and analyze large datasets. The popularity of these tech-
niques is the result of a sequence of advances in several areas such
as statistical modeling, mathematics, computer science, software
engineering and hardware design, as well as successful standardiza-
tion efforts. A notable example is the development of floating point
arithmetic and software for numerical linear algebra, leading to stan-
dard interfaces such as BLAS (Basic Linear Algebra Subprograms):
a specification that prescribes the basic low-level routines for linear
algebra, including operations like inner product, matrix multipli-
cation and inversion, and matrix-vector product. This interface is
implemented by all common scientific computing frameworks such
as Mathematica, MATLAB, NumPy/SciPy, R, Julia, uBLAS, Eigen,
etc., enabling library users to develop applications in a way that
is agnostic to the precise implementation of the BLAS primitives
being used. The resulting programs are easily portable across archi-
tectures without suffering performance loss. The above libraries,
and their variants optimized for concrete architectures, constitute
the back-end of higher-level machine learning frameworks such as
TensorFlow and PyTorch.
In this work, we present a framework for privacy-preserving
machine learning that provides privacy preserving counterparts
for several basic linear algebra routines. The new tools that we con-
struct mirror the techniques of scientific computing which leverage
sparsity to achieve efficiency. Our framework enables computa-
tion on inputs that are shared among several different parties in
a manner that does not require the parties to reveal their private
inputs (only the output of the computation is revealed). In settings
where the input parties are regulated by strict privacy policies on
their data, such privacy guarantees are a crucial requirement to
enable collaborations that are beneficial for all participants. There
are many example scenarios that have these characteristics: hospi-
tals that want to jointly analyze their patients‚Äô data, government
agencies that want to discover discrepancies across their databases,
companies that want to compute on their joint user data, and many
others.
The main novel aspect of our work is exploiting data sparsity for
scalability, by tailoring the basic operations in our framework for
that purpose. This functionality is analogous to the one provided
by the Sparse BLAS interface [13], a subset of computational rou-
tines in BLAS with a focus on unstructured sparse matrices. The
constructions that we develop for the basic building blocks in our
framework are cryptographic two-party computation protocols,
which provide formal privacy guarantees for the computation [15].
We optimize our protocols for the setting where the sparsity level
of the input data is not a sensitive feature of the input that needs to
be kept secret. This is the case in many datasets where a bound on a
sparsity metric is public information. For example, text data where
the maximum length of the documents in the training dataset is
public, or genomic data, as the number of mutations on a given
individual is known to be very small. Similarly to Sparse BLAS
implementations, sparsity allows us to achieve substantial speed-
ups in our secure protocols. These efficiency gains translate to the
efficiency of the higher-level applications that we develop in our
framework, as is described in Section 1.1.
Sparse linear algebra on clear data relies on appropriate data
structures such as coordinate-wise or Compressed Sparse Row
(CSR) representations for sparse matrices. For the MPC case, we
develop a similar abstract representation, which we call Read-Only
Oblivious Map (ROOM). A significant aspect of this modular ap-
proach is that our alternative back-end implementations of the
ROOM functionality immediately lead to different trade-offs, and
improvements, with regards to communication and computation.
This also allows MPC experts to develop new efficient low-level
secure computation instantiations for the ROOM primitive. These
can then be seamlessly used by non-experts to develop higher level
tools in a way that is agnostic to many of the details related to
secure computation. Such usage of our framework will be similar to
to how data scientists develop high-level statistical modeling tech-
niques while benefiting from the high performance of back-ends of
ML frameworks.
1.1 Contributions
We present a modular framework for secure computation on sparse
data, and build three secure two-party applications on top of it.
Our secure computation framework (depicted in Figure 1) emulates
the components architecture of scientific computing frameworks.
We define a basic functionality and then design and implement
several secure instantiations for it in MPC; we build common linear
algebra primitives on top of this functionality; and then we use
these primitives in a black-box manner to build higher level machine
learning applications. More concretely, we present the following
contributions:
(1) A Read-Only Oblivious Map (ROOM) data structure to rep-
resent sparse datasets and manipulate them in a secure and com-
posable way (Section 4).
(2) Three different ROOM protocol instantiations (Section 4.2)
with different trade-offs in terms of communication and computa-
tion. These include a basic solution with higher communication
and minimal secure computation (Basic-ROOM), a solution using
sort-merge networks that trades reduced communication for ad-
ditional secure computation (Circuit-ROOM), and a construction
that leverages fast polynomial interpolation and evaluation (Poly-
ROOM) to reduce the secure computation cost in trade-off for local
computation, while preserving the low communication.
(3) We leverage our ROOM primitive in several sparse matrix-
vector multiplication protocols (Section 5.2), which are optimized
for different sparsity settings. We also show how to implement
sparse gather and scatter operations using our ROOM primitive.
Application
layer
ùëò-Nearest
Neighbors
Logistic
Regression
Naive
Bayes
Linear Algebra API
LinAlg layer
Gather
Scatter
MvMult
ROOM API
ROOM
instantiations
Basic-
ROOM
Circuit-
ROOM
Poly-
ROOM
Figure 1: Components of our system.
(4) We build three end-to-end ML applications using our frame-
work. The resulting protocols significantly improve the state of the
art, as discussed below.
Our three chosen applications are ùëò-nearest neighbors (Sec-
tion 6.1), stochastic gradient descent for logistic regression training
(Section 6.2), and naive Bayes classification (Appendix B). We eval-
uate the performance of these applications on real-world datasets
(Section 8) and show significant improvements over the state of the
art:
‚Ä¢ For ùëò-NN, previous work [35] already exploits sparsity using
a hand-crafted sparse matrix multiplication protocol. We
show that using our protocols with the appropriate choice
of the ROOM primitive can reduce the online running time
by up to 5x.
‚Ä¢ Our sparse stochastic gradient descent implementation im-
proves upon the total runtime of the dense protocol by Mo-
hassel and Zhang [29] by factors of 2x‚Äì94x, and improves
communication by up to 215x.
‚Ä¢ Our protocol for naive Bayes classification scales to datasets
with tens of thousands of features, while the previous best
construction by Bost et al. handled less than a hundred [5].
2 OVERVIEW AND SETUP
How to exploit sparsity, and implications for privacy. Two proper-
ties of real-world data representations used for automated analysis
are (a) their high dimensionality and (b) their sparsity. For example,
the Netflix dataset [4] contains ‚àº480ùêæ users, ‚àº17ùêæ movies, but only
‚àº100 million out of ‚àº8.5 billion potential ratings, less than 2%. In
another common machine learning benchmark, the 20Newsgroups
dataset [34], the training data consists of just over 9000 feature vec-
tors with 105 dimensions, but less than 100 (0.1%) non-zero values
in each vector. Finally, in Genome-Wide Analysis Studies (GWAS),
where the goal is to investigate the relationship between genetic
mutations and specific diseases, the relevant mutations are lim-
ited to only about 5 million locations, while a full human genome
contains ‚àº3.2 billion base pairs [10].
2
To cope with memory limits, and speed up computations on
sparse data in general, several data structures have been devel-
oped that exploit sparsity in the data by only storing relevant (i.e.,
non-zero) values. For a vector v, a straightforward approach is to
store only pairs ((ùëñ, ùë£ùëñ))ùë£ùëñ ‚â†0. For sparse matrices, this generalizes
to Compressed Sparse Row representation, where all rows are suc-
cessively stored in the above fashion, and an additional row-index
array stores pointers to the beginning of each row. Linear algebra
libraries such as SciPy and Eigen provide implementations of these
sparse vectors and matrices [17, 21], and databases for genomic
data use similar sparse storage formats [11].
Note that sparse data representation does not only reduce the
storage overhead, but is also the basis for more efficient algorithms.
For example, a matrix-vector product, where the matrix is stored as
CSR, is independent of the number of columns in the original data
and only depends on the number of rows and the number of non-
zero values in the matrix. For the examples above, where columns
correspond to hundreds of thousands of features, this saves large
amounts of computation.
In this paper we show how to obtain the same benefits from
sparsity in the secure distributed ML setting, revealing only the
sparsity metric of the underlying data while hiding where in the