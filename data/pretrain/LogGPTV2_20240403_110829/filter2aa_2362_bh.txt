any single fuzzer! This data is highlighted in Figure 8.4.
In fact, running all the fuzzers found, on average, over 50% more bugs than
just running the most effective fuzzer by itself. Keep this in mind when deciding
which fuzzer(s) to use.
8.8.2
Generational-Based Approach Is Superior
While the fact that more fuzzers are significantly better than any one may be sur-
prising, the fact that generational-based fuzzers find more bugs than mutation-
based fuzzers will probably not come as a big surprise. In these three tests, the best
generational-based fuzzer does over 15% better than the best mutation-based fuzzer.
The exact comparison is shown in Figure 8.5.
8.8.3
Initial Test Cases Matter
Another observation that can be made from this data is that the quality of the initial
input is important. Consider the two initial packet captures used during the FTP test-
ing. The data from this is summarized in Figure 8.6. While we could have guessed this
was the case, we now know exactly how important the initial test cases can be.
The difference in the number of bugs found beginning from these different
inputs is clear. For GPF, 66% more bugs were found with the full packet capture.
242
Fuzzer Comparison
For the other two fuzzers, no bugs were found with the partial packet capture,
while three bugs were found with the full capture. This full packet capture took
advantage of knowledge of the protocol and required some up-front work. In a
sense, using the complete packet capture blurred the distinction between mutation-
based and generational-based fuzzers. In practice, such “protocol complete” initial
inputs may not be feasible to obtain.
8.8.4
Protocol Knowledge
Also not surprising is that the amount of information about a particular protocol
that a fuzzer understands correlates strongly with the quality of the fuzzing. Figure
8.7 shows some data taken from the SNMP fuzzing tests.
ProxyFuzz does not understand the protocol at all, it merely injects random
anomalies into valid SNMP transactions. It finds the fewest bugs. The GPF generic
tokAid attempts to dissect binary protocols based on very generic techniques. It
doesn’t understand the specifics of SNMP, but does do better than the completely
8.8
General Conclusions
243
Figure 8.6
The quality of the initial test case for mutation-based fuzzers makes a big difference.
Figure 8.5
Generation-based fuzzers outperform mutation-based fuzzers.
Figure 8.7
The more protocol information available, the more bugs found.
random approach offered by ProxyFuzz. The GPF fuzzer with the custom-written
SNMP tokAid does understand the SNMP protocol, at least with respect to the
packets captured and replayed by GPF. That is to say, it doesn’t understand SNMP
entirely, but does completely understand the packets it uses in its replay. This fuzzer
does better still. Finally, the two commercial generational-based fuzzers completely
understand every aspect of SNMP and gets the best results. Beside the fact more
information means more bugs, we can see exactly how much more information
(and thus time and money) gives how many more bugs.
8.8.5
Real Bugs
Throughout this testing, the fuzzers were doing their best to find simulated bugs
added to the applications. However, it was entirely possible by the way the tests
are designed that they could uncover real bugs in these particular applications. It
turns out one of the fuzzers actually did find a real bug in one of the applications.
The Codenomicon fuzzer found a legitimate DoS vulnerability in Net-SNMP. This
bug was reported to the developers of this project, and at the time of the writing of
this book, this bug is fixed in the latest source snapshot. No other fuzzers found this
real bug. Code coverage could be used to predict this fact as the Codenomicon
fuzzer obtained significantly more code coverage of this application than the other
fuzzers.
8.8.6
Does Code Coverage Predict Bug Finding?
While we chose to test the fuzzers by looking at how effective they were at finding
simulated vulnerabilities, we also chose to measure them by looking at code cover-
age. One added benefit of doing this is that we now have both sets of data and can
attempt to answer the hotly debated question, “Does high code coverage correlate
to finding the most bugs?”
As a first approximation to answering this question, let’s look at the graphs of
code coverage versus bugs found for the three sets of data we generated (Figure 8.8).
The figures seem to indicate that there is some kind of relationship between
these two variables (which is good since they are supposed to be measuring the
same thing). With such small data sets, it is hard to draw any rigorous conclusions,
but we can still perform a simple statistical analysis based on this data. Consider the
data found for FTP. We’ll use the statistics software SYSTAT to see if there is a
244
Fuzzer Comparison
8.8
General Conclusions
245
Figure 8.8
These three graphs plot code coverage versus bugs found for each of the fuzzers.
Each point represents a fuzzer. The DNS graph especially shows the positive relationship.
(a) FTP
(b) SNMP
(c) DNS
relationship between the independent variable code coverage and the dependent
variable bugs found.3 The results from the analysis follow:
Dep Var: BUGS
N: 11
Multiple R: 0.716
Squared multiple R: 0.512
Adjusted squared multiple R: 0.458
Standard error of estimate: 9.468
Effect
Coefficient
Std Error
Std Coef
Tolerance
t
P(2 Tail)
CONSTANT
-5.552
8.080
0.000
.___
-0.687
0.509
CC
0.921
0.300
0.716
1.000
3.074
0.013
Analysis of Variance
Source
Sum-of-Squares
df
Mean-Square
F-ratio
P
Regression
847.043
1
847.043
9.449
0.013
Residual
806.813
9
89.646
What this means in English is that code coverage can be used to predict the num-
ber of bugs found in this case. In fact, a 1% increase in code coverage increases the
percentage of bugs found by .92%. So, roughly speaking, every 1% of additional
code coverage equates to finding 1% more bugs. Furthermore, the regression coef-
ficient is significant at the .02 level. Without getting into the details, this means that
there is less than a 2% chance that the data would have been this way had the
hypothesis that code coverage correlates to the number of bugs found been incor-
rect. Thus, we can conclude that code coverage can be used to predict bugs found.
This statistical model explains approximately 46% of the variance, indicating that
other conditions exist that are not explained by the amount of code coverage alone.
Therefore, there is strong evidence that code coverage can be used to predict the
number of bugs a fuzzer will find but that other factors come into play as well.
8.8.7
How Long to Run Fuzzers with Random Elements
Generational-based fuzzers, like the commercial fuzzers tested here, are easy to run.
They already know exactly which inputs they will send and in what order. Thus, it
is pretty easy to estimate exactly how long they will run and when they will finish.
This is not the case for most mutation-based fuzzers, which contain randomness
(Taof is an exception). Due to the fact there is randomness involved in selecting
where to place anomalies and what anomalies to use, mutation-based fuzzers could
theoretically run years and then suddenly get lucky and discover a new bug. So, the
relevant question becomes, “When exactly has a fuzzer with random components
run for ‘long enough’?” While we’re not in a position to answer this question
directly, the fuzzer comparison testing we’ve conducted has allowed us to collect
some relevant data that may shed light on this question. Figure 8.9 shows at what
point during the fuzzing various bugs were discovered by ProxyFuzz in the 450
minutes it was used during DNS testing.
246
Fuzzer Comparison
3Thanks to Dr. Andrea Miller from Webster University for helping with this analysis.
The thing to notice from this data is there are discrete jumps between the times
when various bugs are discovered. Three easy-to-find bugs are found in the first
three minutes of fuzzing. The next bug is not found for another 76 minutes. Like-
wise, seven bugs are discovered in the first 121 minutes. Then it took another 155
minutes to find the next one. It would be very tempting during these “lulls” to think
the fuzzer had found everything it could find and turn it off. Along these lines, the
fuzzer did not find anything in the final hour of its run. Does this mean it wouldn’t
find anything else, or that it was just about to find something?
8.8.8
Random Fuzzers Find Easy Bugs First
One issue not addressed thus far is which bugs are found in which order when using
a fuzzer with random components. Based on the last section, fuzzers clearly find
some bugs very quickly, and other bugs require much more time to find. Figure
8.10 shows how often each bug from the ProxyFuzz run against DNS comparison
was found.
Not surprisingly, the ones that were found quickest were also the ones discov-
ered the most frequently. The last two bugs discovered during this fuzzing run were
only found once.
8.9
Summary
In this chapter, we began by discussing the various functions that different fuzzers
provide. Some fuzzers only provide fuzzed inputs and leave everything else to the
user. Others provide a variety of services besides just the inputs, including target
monitoring and reporting. We evaluated the quality of the test cases generated by a
variety of different fuzzers for this comparison. While there are different ways to
8.9
Summary
247
Figure 8.9
The graph shows the number of minutes for ProxyFuzz to find the 9 SNMP bugs it
discovered. Users who turn their fuzzer off early will miss the bugs discovered later.
compare fuzzer’s quality, we took two approaches. First, we took three open-source
applications and added a number of security vulnerabilities to each. We then meas-
ured how many of these bugs were found by each fuzzer. We also measured the
amount of code coverage obtained by each fuzzer. We compiled this data into var-
ious charts and performed some analysis. We spent extra time examining those par-
ticular bugs that only a few fuzzers could find to see what made them special.
Overall, the results were that some fuzzers did better than others, but we found that
the best practice to find the most bugs is to use a number of different fuzzers in
combination. We also found that the quality of the initial test cases is important
for mutation-based fuzzers and that the amount of protocol knowledge a fuzzer
possesses is a good indication of how well it will perform. Finally, we learned that
code coverage can be used to predict how well various fuzzers are performing. So
while we set out to find which fuzzer was best, we ended up learning a lot about how
different fuzzers work and were able to make general conclusions about fuzzing.
248
Fuzzer Comparison
Figure 8.10
The bugs found most quickly were also found most frequently.
C H A P T E R  9
Fuzzing Case Studies
In this chapter, we will describe a few common use cases for fuzzing. We will
explain our experiences with each of them, with examples drawn from real-life
fuzzing deployments. These examples combine experiences from various deploy-
ments, with the purpose of showing you the widest usage possible in each of the sce-
narios. In real deployments, organizations often choose to deploy fuzzing at a
slower pace than what we present here. We will not mention the actual organiza-
tion names to protect their anonymity.
As we have stressed in this book, fuzzing is about black-box testing and should
always be deployed according to a test plan that is built from a risk assessment.
Fuzzing is all about communication interfaces and protocols. As explained in Chap-
ter 1, the simplest categorization of fuzzing tools is into the following protocol
domains:
• File fuzzing;
• Web fuzzing;
• Network fuzzing;
• Wireless fuzzing.
Whereas a good file fuzzing framework can be efficient in finding problems in
programs that process spreadsheet documents, for example, it can be useless for
web fuzzing. Similarly, a network fuzzer with full coverage of IP protocols will very
rarely be able to do wireless protocols due to the different transport mechanisms.
Due to the fact that many tools are targeted only to one of these domains, or due
to the internal prioritization, many organizations deploy fuzzing in only one of
these categories at a given time. Even inside each of these categories you will find
fuzzers that focus on different attack vectors. One fuzzer can be tailored for graph-
ics formats whereas, another will do more document formats but potentially with
worse test coverage. One web fuzzer can do a great job against applications writ-
ten in Java, but may perform badly if they are written in C. Network fuzzers can
also focus on a specific domain such as VoIP or VPN fuzzing.
Therefore, before you can choose the fuzzing framework, and even before you
will do any attack vector analysis, you need to be able to identify the test targets. A
simplified categorization of fuzzing targets, for example, can be, the following:
• Server software;
• Middleware;
249
• Applications (Web, VoIP, mobile);1
• Client software;
• Proxy or gateway software.
Finally, you need to build the test harness, which means complementing the
chosen test tools with various tools needed for instrumenting and monitoring the
test targets and the surrounding environment. With a well-designed test harness,
you will be able to easily detect, debug, and reproduce the software vulnerabilities
found. Various tools that you may need might include
• Debuggers for all target platforms;
• Process monitoring tools;
• Network analyzers;
• Scripting framework or a test controller.
Now, we will walk through some use cases for fuzzing, studying the abovemen-
tioned categories with practical examples. We will focus on performing attack vec-
tor analysis and will present example fuzzing results where available.
9.1
Enterprise Fuzzing
The first and most important goal in enterprise fuzzing is to test those services and
interfaces that are facing the public Internet. Most often these are services built on
top of the Web (i.e., HTTP), but there are many other exposed attack vectors at
any enterprise. Those include other Internet services such as e-mail and VoIP, but
also many other, more transparent, interfaces such as Network Time Protocol
(NTP) and Domain Name Service (DNS).
After the most exposed interfaces have been examined, some enterprise users
we have worked with have indicated interest in testing internal interfaces. The inter-
nal interfaces consist of communications conducted inside the organization that are
not exposed to the Internet. Through such attack vectors, the inside users could
abuse or attack critical enterprise servers. The assessment of internal attack vectors
is also important, as studies show that a great number of attacks come from insid-
ers.2 Even when considering a completely outside adversary, once they break into
an internal machine, their next target will be these inside interfaces.
250
Fuzzing Case Studies
1As we have pointed it out several times, it is important to note that not all applications run on
top of the Web. It is definitely the most widely used application development platform, though.
2The E-Crime Watch Survey 2004, by U.S. CERT and U.S. Secret Service indicated that insiders
were responsible for 29% of attacks and 71% from outsiders. For more details on insider threats,
see www.cert.org/archive/pdf/insidercross051105.pdf
Whatever the test target, there are at least three methods for identifying the
attack vectors that need testing:
• Port scan from the Internet;
• Run a network analyzer at various points in the network;
• Perimeter defense rule-set analysis.
A simple port scan conducted from outside the organization is the easiest to do
and will quickly reveal most of the critical open services. When combined with a
network analyzer based at several probes distributed in the enterprise network, the
results will indicate which of the tests actually went through the various perimeter
defenses of the organization. For example, some data may pass straight from an
outside attacker to a critical server located deep within a network, while other data
from the attacker may terminate in the “demilitarized zone,” or DMZ, with very
little access to critical servers. A thorough analysis of the perimeter defenses, the
rules and log files of firewalls and proxies, will provide similar results. At the
perimeter you can, for example, detect the outgoing client requests and incoming
response messages, which you would not be able to detect with any port scanning
techniques. Various probe-based network analyzers can again help to detect these
use cases, because they can check which client requests were actually responded to,
therefore requiring client-side fuzzing. Enterprises are often surprised during this
exercise at the number of interfaces, both server side and client side, that are actu-
ally exposed to the open and hostile Internet.
The greatest challenge in all enterprise test setups is that at some point, fuzzing
will most probably crash the critical services that are being tested. Fuzzing should be
first conducted in a separate test setup where crashes will not damage the production
network and where the failures are perhaps easier to detect. These test facilities might
not exist currently, as most testing done at an enterprise are feature and performance
oriented, and can be executed against the live system during quiet hours.
As an example of enterprise fuzzing, we will look at fuzzing against firewalls
and other perimeter defenses, and also at fuzzing Virtual Private Network (VPN)
systems. Both of these are today deployed in almost every enterprise requiring
remote work with security critical data. Firewalls today are very application-aware
and need to be able to parse and understand numerous communication protocols.
This extensive parsing leaves them open to security vulnerabilities. Likewise, VPNs
need to understand numerous complex, cryptographic protocols and are equally
susceptible. Besides these, enterprise fuzzing is often conducted against e-mail sys-
tems, web services, and various internal databases such as CRM.
9.1.1
Firewall Fuzzing
A firewall is a system that integrates various gateway components into an intelligent
router and packet filter. For most protocols, the firewall acts as an application-level
gateway (ALG) or an application proxy. For the protocols that it supports, it some-
times functions as a back-to-back user agent (B2BUA), on one side implementing a
server implementation of the protocol, and on the other, client functionality.
9.1
Enterprise Fuzzing