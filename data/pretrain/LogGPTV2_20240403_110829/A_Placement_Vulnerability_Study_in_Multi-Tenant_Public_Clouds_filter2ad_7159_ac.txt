 10
 8
 6
 4
 2
 0
0 0.8 1 1.2 1.4 1.5 1.6 1.8 2
2
Performance degradation
Figure 8: Distribution of performance degradation of memory
probing and locking tests. On AMD machines in Azure with
40 pairs of nodes. Here NC stands for non-coresident and C, co-
resident pairs. Note that the x-axis plots performance degradation.
USENIX Association  
24th USENIX Security Symposium  919
more associative cache (48 ways vs. 20 for Intel), or dif-
ferent handling of locked instructions. For these machines,
a threshold of 1.5 was high enough to have no false pos-
itives, which we veriﬁed by hand checking the instances
using the two covert-channels and observed consistent per-
formance degradation of at least 50%.. We determine a
pair of VMs as co-resident if the degradation in either of
the tests is above this threshold. We did not detect any
cross-architecture (false) co-residency detection in any of
the runs.
Scaling co-residency detection tests.
Testing co-
residency at scale is time-consuming and increases quadrat-
ically with the number of instances: checking 40 VM in-
stances, involves 780 pair-wised tests. Even if each run of
the entire co-residency test suite takes only 10 seconds, a
na¨ıve sequential execution of the tests on all the pairs will
take 2 hours. Parallel co-residency checks can speed check-
ing, but concurrent tests may interfere with each other.
To parallelize the test, we partition the set of all VM pairs
((cid:31)v+a
2 (cid:30)) into sets of pairs with no VMs twice; we run one of
these sets at a time and record which pairs detected possible
co-residence. After running all sets, we have a set of can-
didate co-resident pairs, which we test sequentially. Paral-
lelizing co-residency tests signiﬁcantly decreased the time
taken to test all co-residency pairs. For instance, the par-
allel version of the test on one of the cloud providers took
2.4 seconds per pair whereas the serial version took almost
46.3 seconds per pair (a speedup of 20x). While there are
faster ways to parallelize co-residency detection, we chose
this approach for simplicity.
Veracity of our tests. Notice that a performance degra-
dation of 1.5x, 2x and 4x corresponds to 50%, 100% and
300% performance degradation. Such high performance
degradations (even 50%) is clear enough signal to declare
co-residency due to resource sharing. Furthermore, we
hand checked by running the two tests in isolation on the
detected instance-pairs for a signiﬁcant fraction of the runs
for all clouds and observed a consistent covert-channel sig-
nal. Thus our methodology did not detect any false pos-
itives, which are more detrimental to our study than false
negatives. Although “co-residency” here implies sharing
of memory channel, which may not always mean sharing
of cores or other per-core hardware resources.
4.3 Co-residency Detection on Uncoopera-
tive Victims
Until now, we described a method to detect co-residency
with a cooperative victim.
In this section, we look at a
more realistic setting where an adversary wishes to de-
tect co-residency with a victim VM with accesses limited
to only public interfaces like HTTP or a key-value (KV)
store’s put-get interface. We show that the basic coopera-
tive co-residency detection can also be employed to detect
co-residency with an uncooperative victim in the wild.
Attack setting. Unlike previous attack scenarios, we as-
sume the attacker has no access to the victim VMs or its
application other than what is permitted to any user on the
Internet. That is, the victim application exposes a well-
known public interface (e.g., HTTP, FTP, KV-store proto-
col) that allows incoming requests, which is also the only
access point for the attacker to the victim. The front end
of this victim application can range from caching or data
storage services (e.g., memcached, cassandra) to generic
webservers. We also assume that there may be multiple
instances of this front-end service running behind a load
balancer. Under this scenario, the attacker wishes to de-
tect co-location with one or more of the front-facing victim
VMs.
Co-residency test. We adapt the memory tests used in pre-
vious section by running the memory locking sender in the
attacker instance. For a receiver, we use the public inter-
face exposed by the victim by generating a set of requests
that potentially makes the victim VMs hit the memory bus.
This can be achieved looping through large number of re-
quests of sizes approximately equal or greater than the size
of the LLC. This creates a performance side-channel that
leaks co-residency information. This receiver runs in an in-
dependent VM under the adversary’s control, which we call
the co-residency detector.
Experiment setup. To evaluate the efﬁcacy of this
method, we used the Olio multi-tier web application [12]
that is designed to mimic a social-networking application.
We used an instance of this workload from CloudSuite [22].
Although Olio supports several tiers (e.g., memcached to
cache results of database queries), we conﬁgured it with
two tiers, with each webserver and the database server run-
ning in a separate VM of type t2.small on Amazon EC2.
Multiple of these webserver VMs are conﬁgured behind a
HAProxy-based load balancer [9] running in a m3.medium
instance (for better networking performance). The load bal-
ancer follows the standard conﬁguration of using round-
robin load balacing algorithm with sticky client sessions
using cookies. We believe such a victim web application
and its conﬁguration is a reasonable generalization of real
world applications running in the cloud.
For the attacker, we use an off-the-shelf HTTP perfor-
mance measurement utility called HTTPerf [28] as the re-
ceiver in the co-residency detection test. This receiver is
run inside a t2.micro instance (for free of charge). We used
a set of 212 requests that included web pages and web ob-
jects (images, PDF ﬁles). We gathered these requests using
the access log of manual navigation around the web appli-
cation from a web browser.
Evaluation methodology. We start with a known co-
resident VM pair using the cooperative co-residency detec-
tion method. We conﬁgure one of the VMs as a victim web-
server VM and launch four more VMs:
two webservers,
one database server and a load balancer, all of which are
920  24th USENIX Security Symposium 
USENIX Association
not co-resident with the attacker VM.
Co-residency detection starts by measuring the average
request latency at the receiver inside the co-residency de-
tector for the baseline (with idle attacker) and contended
case with the attacker runs the memory locking sender. A
signiﬁcant performance degradation between the baseline
and the contended case across multiple samples reveal co-
residency of one of the victim VMs with the attacker VM.
On Amazon EC2, with the above setup we observed an av-
erage request latency of 4.66ms in the baseline case and
a 10.6ms in the memory locked case, i.e., a performance
degradation of ≈ 2.3x.
Background noise. The above test was performed when
the victim web application was idle. In reality, any victim in
the cloud might experience constant or varying background
load on the system. False positives or negatives may occur
when there is spike in load on the victim servers. In such
case, we use the same solution as in Section 4.2 — alternat-
ing between measuring the idle and the contended case.
In order to gauge the efﬁcacy of the test under constant
background load, we repeated the above experiment with
varying load on the victim. The result of this experiment is
summarized in Figure 9. Counterintuitively, we found that
a constant load on the background server exacerbates the
performance degradation gap, hence resulting in a clearer
signal of co-residency. This is because running memory
locking on the co-resident attacker increases the service
time of all requests as majority of the requests rely on mem-
ory bandwidth. This increases the queuing delay in the sys-
tem and in turn increasing the overall request latency. Inter-
estingly, this aforementioned performance gap stops widen-
ing at higher loads of 750 to 1000 concurrent users as the
system hits a bottleneck (in our case a network bottleneck at
the load balancer) even without running the memory lock-
ing sender. Thus, detecting co-residency with a victim VM
that is part of a highly loaded and bottlenecked application
would be hard using this test.
We also experimented with increasing the number of
victim webservers behind the load balancer beyond 3
(Figure 10). As expected, the co-residency signal grew
weaker with increasing victims, and at 9 webservers, the
performance degradation was too low to be useful for de-
tecting co-residency.
5 Placement Vulnerability Study
In this section, we evaluate three public clouds, Amazon
EC2, Google Compute Engine and Microsoft Azure, for
placement vulnerabilities and answer the following ques-
tions: (i) what are all the strategies that an adversary can
employ to increase the chance of co-location with one or
more victim VMs? (ii) what are the chances of success and
cost of each strategy? and (iii) how does these strategies
compare against the reference placement policy introduced
in Section 3?
)
s
m
(
y
c
n
e
t
a
L
t
s
e
u
q
e
R
e
g
a
r
e
v
A
l
e
a
c
s
g
o
L
 1024
 512
 256
 128
 64
 32
 16
 8
 4
w/o-mlock
w/-mlock
idle
100
250
500
750
1000
Background Load on Victim 
(# concurrent users)
Figure 9: Co-residency detection on an uncooperative victim.
The graph shows the average request latency at the co-residency
detector without and with memory locking sender running on the
co-resident attacker VM under varying background load on the
victim. Note that the y-axis is in log scale. The load is in the
number of concurrent users, where each user on average generates
20 HTTP requests per second to the webserver.
)
s
m
(
y
c
n
e
t
a
L
t
s
e
u
q
e
R
e
g
a
r
e
v
A
 256
 128
l
e
a
c
s
g
o
L
 64
 32
 16
 8
 4
w/o-mlock
w/-mlock
w/o-mlock
3
5
7
9
Idle
3
5
7
9
500 users
Number of webservers
Figure 10: Varying number of webservers behind the load bal-
ancer. The graph shows the average request latency at the co-
residency detector without and with memory locking sender run-
ning on the co-residency attacker VM under varying background
load on the victim. Note that the y-axis is in log scale. The error
bars show the standard deviation over 5 samples.
5.1 Experimental Methodology
Before presenting the results, we ﬁrst describe the exper-
iment setting and methodology that we employed for this
placement vulnerability study.
Experiment settings. Recall VM placement depends on
several placement variables (shown in Figure 1). We as-
signed reasonable values to these placement variables and
enumerated through several launch strategies. A run cor-
responds to one launch strategy and it involves launching
multiple VMs from two different accounts (i.e., subscrip-
tions in Azure and projects in GCE) and checking for co-
residency between all pairs of VMs launched. One account
was designated as a proxy for the victim and the other for
the adversary. We denote a run conﬁguration by v × a,
where v is the number of victim instances and a is the num-
ber of attacker instances launched in that run. We varied v
and a for all v, a ∈ {10,20,30} and restricted them to the
inequality, v ≤ a, as it increases the likelihood of getting
co-residency.
USENIX Association  
24th USENIX Security Symposium  921
Other placement variables that are part of the run con-
ﬁguration include: victim launch time (including time of
the day, day of the week), delay between victim and at-
tacker VM launches, victim and attacker instance types and
data center location or region where the VMs are launched.
We repeat each run multiple times across all three cloud
providers. The repetition of experiments is especially re-
quired to control the effect of certain environment variables
like time of day. We repeat experiments for each run con-
ﬁguration over various times of the day and days of the
week. We ﬁx the instance type of VMs to small instances
(t2.small on EC2, g1.small on GCE and small or Standard-
A1 on Azure) and data center regions to us-east for EC2,
us-central1-a for GCE and east-us for Azure, unless other-
wise noted. All these experiments were conducted over 3
months between December 2014 to February 2015.
We used a single, local Intel Core i7-2600 machine with
8 SMT cores to launch VM instances, log instance informa-
tion and run the co-residency detection test suite.
Implementation and the Cloud APIs.
In order to auto-
mate our experiments, we used Python and the libcloud2
library [3] to interface with EC2 and GCE. Unfortunately,
libcloud did not support Azure. The only Azure cloud
API on Linux platform was a node.js library and a cross-
platform command-line interface (CLI). We built a wrap-
per around the CLI. There were no signiﬁcant differences
across different cloud APIs except that Azure did not have
any explicit interface to launch multiple VMs simultane-
ously.
As mentioned in the experiment settings, we experi-
mented with various delays between the victim and attacker
VM launches (0, 1, 2, 4 . . . hours). To save money, we
reused the same set of victim instances for each of the
longer runs. That is, for the run conﬁguration of 10x10 with
0, 1, 2, and 4 hours of delay between victim and attacker
VM launches, we launched the victim VMs only once at the
start of the experiment. After running co-residency tests on
the ﬁrst set of VM pairs, we terminated all the attacker in-
stances and relaunched attacker VM instances after appro-
priate delays (say 1 hour) and rerun the tests with the same
set of victim VMs. We repeat this until we experimented
with all the required delays for this conﬁguration. We call
this methodology the leap-frog method. It is also important
to note that zero delay here means parallel launch of VMs
from our test machine (and not sequential launch of VMs
from one account after another), unless otherwise noted.
In the sections below, we take a closer look at the effect
of varying one placement variable while keeping other vari-
ables ﬁxed across all the cloud providers. In each case, we
use three metrics to measure the degree of co-residency:
chances of getting at least one co-resident instance across
a number of runs (or success rate), average number of co-
resident instances over multiple runs and average coverage
2We used libcloud version 0.15.1 for EC2, and a modiﬁed version of
0.16.0 for GCE to support the use of multiple accounts in GCE.
Delay (hr)
0
0
0
0
0
0
1
1
1
1
1
1
Conﬁg Mean
10x10
0.11
0.2
10x20
0.5
10x30
0.43
20x20
20x30
1.67
1.6
30x30
0.25
10x10
0.33
10x20
10x30
1.6
1.27
20x20
20x30
2.44
30x30
3
S.D. Min Median Max
0.33
0.42
0.71
0.65
1.22
1.65
0.46
0.5
1.07
1.22
1.51
1.12
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
2
1
0
0
2
1
3
3
Figure 11: Distribution of number of co-resident pairs on
GCE. Region: us-central1-a.
S.D. Min Median Max
0
1
1
2
2
4
5
1
1
3
4
4
5
0
2
3
4