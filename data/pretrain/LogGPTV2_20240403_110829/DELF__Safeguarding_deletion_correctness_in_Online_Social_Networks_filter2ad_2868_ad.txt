remains idle, and (c) deletions make progress when they run.
Typical issues include service outages, transient overload of
data stores or the asynchronous execution tier, bugs in cus-
tom sections, and corrupt data. Failures should be noticed
USENIX Association
29th USENIX Security Symposium    1065
even if they affect a small number of deletions and are within
expected failure rates of the underlying systems.
DELF performs redundant tracking of all deletions using
the analytics infrastructure at FACEBOOK. The tracking is
orthogonal to the state kept by the underlying batch processing
system and data stores which are responsible for scheduling
and executing deletion jobs. DELF logs all events relevant to
deletion lifecycle including the scheduled start time, initiation,
subsequent reruns, all exceptions, and eventual completion
alongside timestamps. Event logging occurs via Scribe [59],
and a Hive [39] pipeline inspects all events logged per deletion
to identify anomalous deletions in regards to timely initiation,
idleness, continuous progress, and completion. Any deletions
found not to make progress are reported for engineers to
investigate and resume automatically once ﬁxes are deployed.
5.2 Throughput
DELF aims to minimize the end-to-end execution time for
each deletion, which is the main system performance consid-
eration (§2.4). Consequently, DELF executes deletions for
different top-level objects in parallel and batches point deletes
within each deletion. DELF maximizes throughput, i.e., the
rate of point deletes against data store APIs.
The upper bound for aggregate throughout is imposed by
shard utilization. The same shards DELF deletes data from
serve production trafﬁc and their performance should not
degrade due to asynchronous deletion execution. DELF mon-
itors replication lag and CPU utilization to detect highly uti-
lized shards and applies exponential back off on spikes. An-
other limiting factor can be the number of available machines
to execute deletion jobs; DELF shifts deletion execution to
run off-peak when necessary.
Deletions triggered by users are executed immediately and
in parallel with other existing deletions in the system. The
average deletion at FACEBOOK involves few objects, e.g.,
deleting a rejected friend request. DELF favors such dele-
tions because they are executed without any coordination with
existing deletions beyond an initial check to conﬁrm no two
deletions operate on the same top-level object during the same
time. The resulting point deletes are spread across shards.
DELF batches writes within each deletion, i.e., point
deletes and writes to restoration logs. Batching amortizes
write latency and increases throughput. Writes entail cross-
regional latency due to either a roundtrip to the master region
or to achieve consistency across replicas. To amortize this,
each deletion reads items to delete from local replicas, collects
those in memory, and once the batch reaches a pre-conﬁgured
size all deletes are ﬂushed concurrently. Each batch of point
deletes entails a single write for a batch of restoration log en-
tries. DELF also batches reads to increase throughput further.
5.3 Restoration Logs Retention
Long-running deletions which remain in asynchronous exe-
cution for more than 90 days are required to make continuous
progress (§2.4). To satisfy this requirement DELF’s deploy-
ment at FACEBOOK does not apply a single retention period
for the entire restoration log of each deletion, e.g., 90 days
from the last log entry. Instead each log entry is retained
for 90 days after its creation. Deletions running for more
than 90 days may therefore not get restored fully since log
entries persisted more than 90 days in the past will have been
deleted. Data store backup retention matches restoration log
entry retention with each snapshot being retained for 90 days.
The setup guarantees that data stored in restoration logs and
backups is deleted 90 days after each point delete.
Restoration logs should not be retained beyond 90 days.
Persisting log entires in a data store which itself maintains
backups must be avoided to not extend retention. DELF uses
Everstore and ZippyDB to handle the desired throughput. Yet
both data stores mandate backups for all use cases to safe-
guard against bugs in the data store itself. DELF, instead,
relies on encryption to enforce precisely 90 days of log entry
retention. Restoration log entries are encrypted using AES-
256-CBC with HMAC-SHA-256 for authentication. The en-
cryption key is stored in memory for 90 days, protected from
inadvertent logging, and rotated daily.
5.4 Deployment sequencing
DELF was iteratively developed at FACEBOOK over several
years; progressively gaining its key design properties and
coverage across data stores. We discuss major phases in its
deployment alongside improvements delivered in each phase.
In Phase 1, DELF replaced product code performing deletes
to data store APIs directly, mandating the use of a DELF-
provided procedural API which performed the same deletes
while transparently maintaining restoration logs. This phase
mitigated developer mistakes leading to inadvertent deletion.
In Phase 2, DELF introduced dynamic validation tech-
niques (§4.3). This phase enabled detection of developer
omissions and mistakes leading to inadvertent data retention.
DELF heuristics enabled remediation when detecting omis-
sions by pinpointing mishandled edge types to developers.
In Phase 3, DELF introduced its declarative API based
on object and edge annotations (§4.1). Applications hosted
in FACEBOOK infrastructure rely on two distinct proprietary
data deﬁnition languages to create data types across TAO,
Everstore, MySQL, and ZippyDB in line with DELF’s re-
quirements. We extended both to support DELF annotations.
Developers were able to use—optionally—the declarative
API rather than the procedural API introduced in Phase 1.
This phase helped speed up the development of new products
by eliminating the need for writing procedural deletion code.
In Phase 4, DELF introduced static and data type validation
1066    29th USENIX Security Symposium
USENIX Association
techniques (§4.3); while the use of the declarative API became
mandatory. This phase helped developers catch mistakes early
in the product development process when data models are
deﬁned and improved correctness validation capabilities. It
also reduced the operational overhead of DELF by making
stuck deletions which do not make progress less likely (§6.4).
6 Evaluation
Our goal in this section is to quantify DELF’s ability to mit-
igate the privacy and security concerns raised in our threat
model. For each concern we discuss identiﬁed issues ﬂagged
by DELF during its deployment and we then quantify the sys-
tem’s ongoing prevention capabilities. All identiﬁed issues
have been ﬁxed and any retained data deleted.
Experiments in this section involve instrumenting the dele-
tion process at FACEBOOK to assess the effectiveness of DELF
under real system operation. We design all our experiments
to avoid incurring any adverse effect in FACEBOOK’s ability
to enforce its deletion policy across its products.
6.1 Developer Omissions
We start by assessing DELF’s impact in helping developers
remember their obligation to delete user data. This assessment
draws upon data collected between May 2018 and April 2019.
Identiﬁed issues. During the course of our assessment DELF
via data type validation (§4.3) detected 3266 instances where
developers omitted handling deletion of an edge type. All
identiﬁed cases result in inadvertent data retention indepen-
dently of the eventual edge type annotation—a retained edge
itself stores data about deleted objects. We routed all identi-
ﬁed omissions to developers for retroactive annotation.
DELF identiﬁed one broad category of developer omissions
responsible for the majority of issues in our results. The
prevailing scenario involves deletions being driven via by
procedural code in custom sections which developers failed to
keep up to date when the applications or data models change;
resulting in dangling data. At the time of this assessment the
transition to DELF was ongoing (§5.4) and procedural code
in custom sections handling deletion was common. To better
understand identiﬁed omissions we discuss two examples.
In June 2018 DELF ﬂagged that an edge type indicating the
existence of a mailbox is being left dangling when a Dating
user is deleted. The edge type was created in April 2015 and
was initially only used for Facebook users. Yet in November
2017 developers introduced a new user object type to repre-
sent users of the upcoming Dating product and reused the
same mailbox edge type to implement its messaging function-
ality. DELF detected the edge type resuse and highlighted
the missing edge type annotation for the new user object type.
The resulting investigation uncovered that: (a) the process of
mailbox deletion relied on custom procedural deletion code
invoked when Facebook user object types are deleted, and
(b), developers omitted to update this logic to handle cases
where a Dating user is deleted. The bug was identiﬁed during
the internal beta testing period for Dating prior to launch. If
it remained undetected it would have resulted in retaining
all private messages Dating users exchanged post account
deletion for people who delete their account. An important
followup was the removal of the procedural deletion code
controlling the invocation of mailbox deletion on account
deletion and replacing it with a deep-annotated edge type
between any user type and its mailbox. In subsequent months
DELF seamlessly handled mailbox deletion for an additional
4 new user account types introduced in FACEBOOK.
In May 2018 DELF ﬂagged an edge type storing the most
recent pages a user views is being left dangling when some
Facebook users are deleted. The edge type was created in
November 2013 and data was used ever since to generate
recommendations for accounts to follow. Developers initially
ensured that edges of this type are deleted when a Facebook
user deletes their account via updating the custom procedu-
ral deletion logic used at the time. Yet in May 2018 DELF
detected that the same edge type was subsequently reused
to log page views for a different type of user accounts in
Facebook, i.e., page admins. The subsequent investigation
conﬁrmed that every time a page admin deleted their account
the list of their most recent viewed pages persisted and page
admin deletion—which relied on procedural code in a custom
section—did not delete these edges. DELF detected the edge
type reuse and highlighted the missing edge type annotation.
Prevention. DELF enforces that all new data types are cre-
ated alongside deletion annotations. In doing so it eliminates
developer omissions as a correctness concern. The protec-
tion DELF entails, however, is only effective assuming avail-
able annotations can express sufﬁciently-complicated deletion
logic. Developers would otherwise bypass DELF and con-
tinue to rely on custom procedural code to perform deletions.
DELF permits this via the custom object type annotation
(§4.1). To better assess the system’s ability to prevent omis-
sions, we study how developers bypass DELF by using the
custom object type annotation in new applications.
We retroactively inspect 408 changesets introduced in
FACEBOOK infrastructure throughout October 2019 by 279
distinct developers. Each changeset in our sample creates or
modiﬁes at least one object type annotation. Only 7 change-
sets designate the custom annotation. We observe no new
legitimate instances where DELF annotations are lacking ex-
pressiveness. 6 changesets use custom to maintain backwards
compatibility with legacy procedural deletion logic introduced
before DELF was available, i.e., in one instance data to be
deleted was stored in a TAO edge using a legacy serializa-
tion format and required special handling. We also notice
one changeset misusing custom to approximate reference
counting, i.e., developers were oblivious to native support of
refcount. We conclude that DELF annotations can express
deletion logic necessary in modern OSNs and the system is
USENIX Association
29th USENIX Security Symposium    1067
effective in safeguarding deletion from developer omissions.
HEURISTIC
PREDICTS
to_new_object
to_leaf_object
to_owned_object
id1_ref_in_id2
to_old_object
self_reference
many_to_one
same_obj_type
to_deleted
OVERALL
deep
deep
deep
deep
shallow
shallow
shallow
shallow
shallow
deep
shallow
either
PREC. (%)
86.9
86.3
91.9
88.9
94.5
100.0
96.4
90.8
91.7
89.7
93.0
95.0
RECALL (%)
4.7
12.6
19.3
29.7
80.2
12.0
54.5
13.9
0.6
60.7
89.5
74.8
Inadvertent data retention
6.2
We continue by assessing how effective DELF is in identifying
instances of dangling data engineers actively misclassiﬁed.
Identiﬁed issues. We start with examples where DELF pin-
pointed developer mistakes which would have otherwise re-
sulted in dangling data. We inspect 91 reports generated by
DELF deep heuristics during January 2020, of cases where
developers annotated edges as shallow or refcount while
DELF suggests deep. We submitted these reports to FACE-
BOOK’s privacy team for expert review to establish ground
truth. The assessment established that developers incorrectly
annotated 66 of these edges as shallow. Most of the remain-
ing edges were ambiguous; we discuss those later.
We look closely at one representative example of inadver-
tent data retention in these reports which DELF identiﬁed
and then developers remediated successfully. DELF surfaced
that 23 distinct edge types used to represent different types of
major life events for Facebook users, such as weddings, house
moves, and changes to their citizenship, were mislabeled. The
23 edge types associated the user account object with objects
of a separate type storing detailed information about the life
event, e.g., the date the user got married. All were annotated
shallow rather than deep, indicating erroneously that life
event data should be retained post account deletion.
The report investigation conﬁrmed the developer mistakes.
The affected edge types were introduced at different times
dating back to the introduction of the product feature in 2011.
Legacy procedural deletion logic historically ensured correct
deletion of associated life event data. Yet as part of DELF’s
deployment two different developers—unaware of the histori-
cal deletion logic—annotated the edges in 2017 and 2018 as
shallow instead. The DELF report highlighted the mistake
prior to disabling the legacy procedural deletion logic, and
hence no inadvertent data retention of life event data occurred.
Prevention. DELF helps developers annotate edges as deep
via static and dynamic validation. We measure the impact of
static validation in the the developer workﬂow, and we then
assess how comprehensive deep heuristics are as a safety net.
1) Static validation. We conduct an experiment to measure
how often static validation leads to developers changing their
annotations during product development. DELF enforces
statically that all object types must deﬁne at least one deep
inbound edge type by virtue of treating by_any as the default
annotation (§4.1). We inspect (a) a sample of changesets suc-
cessfully creating 151 new object types in production during
January 2020, and (b), logs of DELF static validation fail-
ures triggered during development starting from December
2019. We ﬁnd that 62.2% of the new object types introduced
failed static validation at some point during their development,
e.g., developers did not deﬁne deep-annotated edge types to
delete data stored therein. Developers subsequently corrected
Table 5: Precision and recall achieved by DELF heuristics on
our sample of 4000 edge types. DELF discovers the correct
annotation for the majority of edge types in our sample, which
provides for an important discovery mechanism of developer
mistakes. The overall precision is higher than both deep
and shallow individually because we discard conﬂicting pre-
dictions; deep or shallow false positives with conﬂicting
predictions are not considered valid predictions.
these mistakes and all 94 new object types were subsequently
created while satisfying the chosen object type annotation.
2) Dynamic validation. We report on the precision and re-
call achieved by DELF heuristics on a sample of edge types
already annotated by developers, treating developer annota-
tions as ground truth2. We sample approximately 2.4 trillion
individual edges deleted in production in January 2020. Of
these, we pick at random 2000 shallow and 2000 deep edge
types to ensure equal representation in our assessment. We
ignore edge types with fewer than 20 samples since some
heuristics require at least 20 items to classify an edge type.
We run DELF heuristics on all edges in our sample. For
each heuristic, we count a true positive when the heuristic