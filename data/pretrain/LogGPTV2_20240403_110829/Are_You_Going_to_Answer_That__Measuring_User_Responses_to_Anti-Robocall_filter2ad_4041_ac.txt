background color, for the spam warning. These results lead
us to minimize the color red in the background and use the
other colors (yellow and orange), color scheme suggestions of
the participants in Focus-Spam. We chose the yellow-to-red
gradient because participants mentioned that the colors yellow
and orange would also be alarming. We chose the black box
with white text because the design needed a clear contrast
between elements on the screen. Focus-AID, the authenticated
warning,
includes a blue gradient background and a blue
notiﬁcation box at the top of the screen. This design choice
was made based on focus group results which showed that
users like the blue background color.
TABLE I.
A LIST OF THE INDEPENDENT VARIABLES, AND THEIR
LEVELS, FOR THIS EXPERIMENT.
Variables:
Response
Round
Levels
Accept, Did Not Accept
R1, R2 ,R3
Warning Design
Focus-AID, Focus-Spam, Control, Avail-CID,
Avail-Spam
Number
N1, N2, N3, N4, N5, N6
Pilot Study: Before we began the user study, we held a
pilot study with ﬁve participants to test the Focus designs
to make sure they were acceptable. In particular, we wanted
TABLE II.
DESCRIPTION AND BREAKDOWN OF EACH WARNING
DESIGN
Category Warning Design
Control
Control
Available
Avail-CID
Avail-Spam
Focus
Focus-AID
Focus-Spam
Description
Imitates the typical incoming call screen on the
Samsung Galaxy S7
Mimics the non-spam warning design of the top
ten anti-robocall apps which includes Caller ID
Mimics the spam warning design of the top
ten anti-robocall apps
Includes the non-spam warning design elements
preferred by the focus group participants which
includes Authenticated Caller ID
Includes the spam warning design elements
preferred by the focus group participants
to make sure that the Focus-Spam design was alerting even
without the color red. Each pilot study participant was asked
speciﬁcally about the designs. Two participants requested a
smaller spam warning icon and an increase in text size. One
participant requested to change the background color of the
authenticated call notice to green. The color scheme for the
Focus-Spam designs were not changed for several reasons.
First, no one in the pilot study had an issue with the current
spam color scheme. Second, group participants stated the
colors yellow and orange would also be alerting, and ﬁnally,
warning research suggests that these colors can be used to
express different hazard levels [36]. We chose not to change
the background of Focus-AID because the majority of the
focus group participants (16) approved the blue color scheme
in the examples presented. Additionally, research shows that
blue motivates people to “behave in a more explorative, risky
manner [37], [38], [39]”, which would be beneﬁcial in this
context.
Setup:Each design was shown with an incoming call from six
unique numbers. The numbers chosen were based on the types
of spam calls experienced by the focus group participants:
N1, N2: Two known numbers entered by the participant
N3:
N4:
N5:
N6:
(N1, N2)
An unknown number where the contact name is
a city and state (N3)
“Harold Rogers” whose number includes the same
ﬁrst 9 digits as the participant’s number (N4)
“Veranda Gardens” which appears to be located
in the same area as the participant (N5)
“Ashford Loans”, a loan organization with an area
code different from the participant (N6)
Before completing any tasks, participants were told they
were testing out potential app alert designs for an upcoming
robocall application. They were asked to respond to each
incoming call as they would in real life. Participants then
provided two known numbers (N1, N2). Each participant
entered the contact information of two individuals whom they
regularly communicate with (just as it is in their personal
devices) on their assigned mobile device. They were then
shown various mock calls that displayed until the ﬁfth ring
of a monophonic or polyphonic ringtone (∼23 sec). If the
participant did not respond to the mock call within the time
allotted,
the next call would appear. The participants saw
every possible combination of numbers and designs six times
across three rounds in random order. During each round, three
practice mock calls were initiated ﬁrst followed by the 30
experimental mock calls that were randomly displayed twice.
8
After each round of 63 mock calls, the users were then allotted
a 5-minute break. The participants were shown each design
multiple times to get their true response to the call since their
initial response may not be their true response. After the study,
each participant was debriefed on the true purpose of the study,
asked a few questions about their experience, and given a
follow-up survey. A total of 34 participants responded to 30
mock calls shown six times in random order. This lead to
the collection of 6,120 data points. All independent variables
are listed in Table I and each warning design is described in
Table II. This experiment is set up to detect cause and effect,
thus having high internal validity. We wanted participants to
focus on the warning design and therefore provided a best-case
scenario that has limited external inﬂuences [40].
B. Participants
A total of 34 participants were recruited through a par-
ticipatory system at a University for this study, a participant
total and composition similar to analogous studies [41], [42],
[43]. Some participants were volunteers and others received
extra credit for their participation. Our study was one of many
extra credit opportunities offered to those who participated
for extra credit. The participants spent 30 minutes partici-
pating in the study on average and were between the age
of 20 and 32 (¯x = 24.5, σx = 3.369), where half of the
participants were female. The racial and ethnic backgrounds of
the participants include East Asian (15%), Caucasian (26%),
African American (18%), South Asian (26%), Latinx/Hispanic
(6%), Middle Eastern (6%), and Caribbean (3%). There was
no overlap in participants between the focus groups and this
study. Participants had to be 18 years of age or older and had
to have experience with spam calls to participate. We did this
to capture experiences from those who have and have not used
robocall applications.
C. Analysis
For each mock call shown, we recorded the time-lapse as
participants determined if they would or would not answer
(Reaction Time) and the ﬁnal decision for each mock call
(Response). Reaction Time was measured from the time the
mock call was shown until the participant pressed the button
to accept or decline a call. Response is measured as the action
participants chose to take when the mock call is received.
This is measured on a dichotomous scale where participants
either accept or reject a call. First, we reviewed participants’
responses to make sure no one responded in a pattern to
all calls, especially N1, N2, and N6. We found nothing out
of the ordinary. Then, Shapiro-Wilkes and Anderson-Darling
tests were run on the results using the statistical computation
system R [44]. The ﬁrst 5,000 data points were tested for
normality. The resulting p-values were less than .001, indi-
cating non-parametric data, which was also conﬁrmed with
a histogram. The Aligned Rank Transform (ART) [45] was
used to transform the data and was followed by a Repeated
Measures Analysis of Variance test (RM ANOVA).
The RM ANOVA is used to calculate the signiﬁcant
difference for Reaction Time within the independent variables
Warning Design, Number, and Round. All of the main effects,
except Number, and interactive effects on Reaction Time were
statistically signiﬁcant (α = .05, p<.05 in all cases). The
TABLE III.
REPEATED MEASURES ANOVA RESULTS
Independent Variable
Warning Design
Number
Warning Design: Number
Round
Warning Design: Round
Number: Round
Warning Design:Number: Round
Response
p
< .001
< .001
< .001
–
–
–
–
F-value
62.085
51.49
22.361
–
–
–
–
Reaction
Time
p
F-value
< .001
5.013
.192
1.055
7.962
< .001
177.262 < .001
< .001
5.202
1.8232
.017
< .001
2.887
Df
4,132
5,165
20,660
2,66
8,264
10,330
40,1320
TABLE IV.
PERCENT OF ACCEPTED CALLS FOR EACH WARNING
DESIGN AND PAIRWISE COMPARISONS RESULTS FOR KNOWN AND
UNKNOWN NUMBERS (RESPONSE)
%
Control
Focus-AID
Focus-Spam
Avail-CID
Avail-Spam
p-value
Control
Focus-Spam
Avail-CID
Avail-Spam
Control
Avail-CID
Avail-Spam
Control
Avail-Spam
Focus-AID vs
Focus-Spam vs.
Avail-CID vs.
Avail-Spam vs.
Control
All
Numbers
56.4%
61%
25%
55%
13%
Known #s
(N1, N2)
Unknown #s
(N3, N4,N5,N6)
100%
100%
65%
95%
34%
35%
42%
5%
34%
3%
ns
ns
< .001
< .001
< .001
< .001
ns
ns
ns
ns
< .001
< .001
< .001
< .001
.03
ns
< .001
< .001
< .001
< .001
< .001
ns
< .001
< .001
< .001
< .001
ns
ns
< .001
< .001
TABLE V.
PERCENT OF CALLS NUDGED IN INTENDED DIRECTION
FOR EACH WARNING DESIGN AND PAIRWISE COMPARISONS RESULTS
FOR KNOWN AND UNKNOWN NUMBERS FOR NUDGE RESPONSE
%
Control
Focus-AID
Focus-Spam
Avail-CID
Avail-Spam
p-value
Control
Focus-Spam
Avail-CID
Avail-Spam
Control
Avail-CID
Avail-Spam
Control
Avail-Spam
Focus-AID vs
Focus-Spam vs.
Avail-CID vs.
Avail-Spam vs.
Control
All
Numbers
Known #s
(N1, N2)
Unknown #s
(N3, N4,N5,N6)
77%
61%
75%
55%
87%
100%
99%
35%
95%
66%
65%
42%
95%
34%
97%
< .001
.002
ns
< .001
ns
ns
< .001
< .001
< .001
< .001
ns
< .001
< .001
ns
.04
< .001
< .001