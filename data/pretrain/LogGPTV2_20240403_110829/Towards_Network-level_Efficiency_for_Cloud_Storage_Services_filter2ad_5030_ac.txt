and implications on the TUE of simple ﬁle operations. For each
measurement, we ﬁrst introduce the experiment process and result-
For providers, nearly two thirds (66%) of small ﬁles can be logical-
ly combined into larger ﬁles for batched data sync (BDS). Howev-
er, only Dropbox and Ubuntu One have partially implemented BDS
so far.
For users, no need to worry about the trafﬁc for ﬁle deletion.
Most of today’s cloud storage services are built on top of RESTful
infrastructure (e.g., Amazon S3, Microsoft Azure, and OpenStack
Swift) that only support data access operations at the full-ﬁle lev-
el. TUE can be signiﬁcantly improved by implementing IDS with
an extra mid-layer that transforms MODIFY into GET + PUT +
DELETE ﬁle operations.
Implications
For providers, data compression is able to reduce 24% of the to-
tal sync trafﬁc. For users, PC clients are more likely to support
compression versus mobile or web-based access methods.
For providers, implementing compression and block-level dedupli-
cation together is technically challenging. Based on the trace anal-
ysis, we suggest providers implement compression and full-ﬁle d-
eduplication since the two techniques work together seamlessly.
Implications
For providers, we demonstrate that an adaptive sync defer (ASD)
mechanism that dynamically adjusts the sync deferment is superior
to ﬁxed sync deferment.
For users, in the presence of frequent ﬁle modiﬁcations, today’s
cloud storage services actually bring good news (in terms of TUE)
to those users with relatively “poor” hardware or Internet access.
s, and then unravel several interesting ﬁndings and implications. In
this section, we do not mention the client locations, network envi-
ronments, and hardware conﬁgurations, because the TUE of simple
ﬁle operations is independent to these impact factors.
4.1 File Creation
[Experiment 1] : We ﬁrst study the simple case of creating a high-
ly compressed ﬁle of Z bytes inside the sync folder (we will further
study the data compression in detail in § 5.1). Thereby, calculat-
ing the TUE of ﬁle creation becomes straightforward (i.e., TUE
= Total sync trafﬁc
). According to Figure 2, most compressed ﬁles are
small in size (several KBs), and the maximum compressed ﬁle size
is below 2.0 GB. Therefore, we experiment with
Z ∈ {1, 1 K, 10 K, 100 K, 1 M, 10 M, 100 M, 1 G}.
Z bytes
119Table 6: Sync trafﬁc of a (compressed) ﬁle creation.
Service
Google Drive
OneDrive
Dropbox
Box
Ubuntu One
SugarSync
1 M
Web-based sync trafﬁc (Bytes) Mobile app sync trafﬁc (Bytes)
PC client sync trafﬁc (Bytes)
1 K
10 M
1
1
10 M
9 K
1.06 M 10.6 M 32 K 71 K 1.27 M 11.0 M
10 K 1.13 M 11.2 M 6 K
19 K 20 K 1.14 M 11.4 M 28 K 31 K 1.11 M 11.7 M 29 K 44 K 1.23 M 10.7 M
38 K 40 K 1.28 M 12.5 M 31 K 37 K 1.09 M 10.6 M 18 K 32 K 1.08 M 10.9 M
55 K 47 K 1.10 M 10.6 M 55 K 58 K 1.10 M 10.5 M 16 K 34 K 1.29 M 10.8 M
3 K
2 K
1.11 M 11.2 M 37 K 39 K 1.20 M 11.3 M 20 K 24 K 1.08 M 10.9 M
9 K
19 K 1.17 M 11.4 M 31 K 32 K 1.10 M 10.7 M 31 K 47 K 1.22 M 10.9 M
1 K
7 K
10 M
1 M
1
1 K
1 M
Table 7: Total trafﬁc for synchronizing 100 compressed ﬁle creations. Each ﬁle is 1 KB in size.
Service
Google Drive
OneDrive
Dropbox
Box
Ubuntu One
SugarSync
PC client
Sync trafﬁc
(TUE)
Web-based
Sync trafﬁc
(TUE)
Mobile app
Sync trafﬁc
(TUE)
1.1 MB (11)
1.3 MB (13)
120 KB (1.2)
1.2 MB (12)
140 KB (1.4)
0.9 MB (9)
1.2 MB (12)
2.2 MB (22)
600 KB (6.0)
3.2 MB (32)
500 KB (5.0)
4.0 MB (40)
5.6 MB (56)
1.9 MB (19)
360 KB (3.6)
3.2 MB (32)
2.5 MB (25)
1.5 MB (15)
The second goal of Experiment 1 is to get a quantitative un-
derstanding of the overhead trafﬁc, as TUE heavily depends on
the ratio of the overhead trafﬁc over the total sync trafﬁc. Syn-
chronizing a ﬁle to the cloud always involves a certain amount of
overhead trafﬁc, which arises from TCP/HTTP(S) connection setup
and maintenance, metadata delivery, etc. Speciﬁcally, the overhead
trafﬁc is equal to the total sync trafﬁc excluding the payload trafﬁc
for delivering the ﬁle content, so in Experiment 1,
Overhead trafﬁc ≈ Total sync trafﬁc - Z bytes.
Table 6 lists the results of Experiment 1 regarding the six con-
cerned cloud storage services. We vary the ﬁle size from 1 B to
1 GB, but for brevity only list four typical sizes: 1 B, 1 KB, 1 MB,
and 10 MB. The table records the sync trafﬁc generated by the three
typical service access methods: PC client, web (browser) based,
and mobile app. In general, from Table 6 we have the following
ﬁnding and implication:
• TUE for synchronizing a (compressed) ﬁle creation mainly
depends on the ﬁle size. A small ﬁle results in big TUE up
to 40000, while a big ﬁle incurs small TUE approaching 1.0.
Therefore, for providers, a number of small ﬁles can be log-
ically combined into a moderate-size ﬁle for batched data
sync (BDS) to save trafﬁc, in particular the overhead trafﬁc.
This ﬁnding poses a key question: What is a small size and what
is a moderate size? By plotting the TUE vs. File Size relationship
(for PC clients) in Figure 3, we get an intuitive conclusion that
a moderate size should be at least 100 KB and had better exceed 1
MB, in order to achieve small TUE – at most 1.5 and had better stay
below 1.2. Here we only draw the curve for PC clients since the
corresponding curves for web-based and mobile apps are similar.
As a consequence, small size is regarded as less than 100 KB,
which together with Figure 2 reveals that the majority (77%) of
tracked ﬁles are small in size (meanwhile, 81% in terms of com-
pressed size). More importantly, by analyzing our collected trace,
we ﬁnd that nearly two-thirds (66%) of these small ﬁles can be cre-
ated in batches and thus can effectively beneﬁt from BDS.
[Experiment 1’] : Given that the BDS mechanism can effectively
optimize TUE, a new question comes out: Is BDS adopted by the
Figure 3: TUE vs. Size of the created ﬁle.
six mainstream cloud storage services? To get the answer, we ﬁrst
generate 100 (distinct) highly compressed ﬁles, and then move all
of them into the sync folder in a batch. Each ﬁle is 1 KB in size,
so TUE = Total sync trafﬁc
. If BDS is adopted, the total sync trafﬁc
should be around 100 KB and TUE should be close to 1.0.
100 KB
The results of Experiment 1’ listed in Table 7 reveal that Drop-
box and Ubuntu One have adopted BDS for PC clients. Further, it
is possible that Dropbox has adopted BDS for web-based and mo-
bile access methods, because the corresponding sync trafﬁc (600
KB and 360 KB) is within an order of magnitude of the data up-
date size (100 KB). Also, Ubuntu One may have used BDS in its
web-based data synchronization, since the sync trafﬁc (500 KB)
lies between 600 KB and 360 KB. On the contrary, Google Drive,
OneDrive, Box, and SugarSync have not adopted BDS yet.
4.2 File Deletion
[Experiment 2] : Each ﬁle created in Experiment 1 is deleted
after it is completely synchronized to the cloud, so as to acquire the
sync trafﬁc information of a ﬁle deletion.
The Experiment 2 results indicate that deletion of a ﬁle usual-
ly generates negligible (< 100 KB) sync trafﬁc, regardless of the
cloud storage service, ﬁle size, or access method. The reason is s-
traightforward: when a ﬁle f is deleted in the user’s local sync fold-
 0 10 20 30 40 501K5K10K100K250K0.5M1M10MTUESize of the Created File (Bytes)Google DriveOneDriveDropboxBoxUbuntu OneSugarSync120Figure 4: Sync trafﬁc of a random byte modiﬁcation, corresponding to the three typical service access methods: (a) PC client, (b)
Web-based, and (c) Mobile app. By comparing the three subﬁgures, we discover that only the PC clients of Dropbox and SugarSync
utilize the incremental data sync (IDS) mechanism for improved network-level efﬁciency.
er, the user client just notiﬁes the cloud to change some attributes of
f rather than remove the content of f. In fact, such “fake deletion”
also facilitates users’ data recovery, such as the version rollback of
a ﬁle. Naturally, we get the following implication:
• Cloud storage service users do not need to worry about the
sync trafﬁc when deleting a ﬁle.
4.3 File Modiﬁcation and Sync Granularity
[Experiment 3] : The analysis of our collected cloud storage trace
reveals that the majority (84%) of ﬁles are modiﬁed by users at
least once. That is to say, ﬁle modiﬁcations are frequently made by
cloud storage users. This subsection studies a simple case of ﬁle
modiﬁcation, i.e., modifying a random byte in a compressed ﬁle of
Z bytes inside the sync folder. In this case, TUE = Total sync trafﬁc
.
Similar as § 4.1, we experiment with
Z ∈ {1, 1 K, 10 K, 100 K, 1 M, 10 M, 100 M, 1 G}
1 Byte
and plot the sync trafﬁc of four typical sizes: Z = 1 K, 10 K, 100 K,
and 1 M in Figure 4.
Figure 4 shows that today’s cloud storage services generally u-
tilize two kinds of data sync granularity: 1) full-ﬁle and 2) chunk-
level. Accordingly, their data sync mechanisms are classiﬁed into
full-ﬁle sync and incremental sync as follows:
• Full-ﬁle sync. Google Drive is an example to use the full-
ﬁle sync mechanism. When a random byte is modiﬁed in a
Z-byte compressed ﬁle, the resulting sync trafﬁc is almost
the same as that of creating a new Z-byte compressed ﬁle.
In other words, Google Drive deals with each ﬁle modiﬁ-
cation by simply uploading the full content of the modiﬁed
ﬁle to the cloud and then deleting the old ﬁle. Consequent-
ly, Google Drive is more suitable for hosting media ﬁles
(like photos, music, and videos) which are rarely modiﬁed
by users. The full-ﬁle sync mechanism is also employed
by OneDrive, Box, Ubuntu One, Amazon Cloud Drive, and
some popular cloud storage services in China like Kuaipan,
Kanbox, Baidu CloudDisk, and 115 CloudDisk.
• Incremental sync (or delta sync). Dropbox (PC client) is
an example to use the incremental data sync (IDS) mecha-
nism. When a random byte modiﬁcation happens, the result-
ing sync trafﬁc stays around 50 KB, regardless of the size of
the modiﬁed ﬁle. According to the working principle of the
incremental sync algorithm: rsync [16], once a random byte
is changed in a ﬁle f, in most cases the whole data chunk
that contains this byte must be delivered for synchronizing
f. Therefore, the sync granularity (i.e., the chunk size C)
can be approximately estimated as
C ≈ Total sync trafﬁc − Overhead trafﬁc.
From the Experiment 1 results, we understand that the overhead
trafﬁc of synchronizing a one-byte ﬁle with the Dropbox PC client
is nearly 40 KB. Therefore, the data sync granularity of Dropbox
PC client is estimated as: C ≈ 50 KB −40 KB = 10 KB. This
is further validated by the recommended default chunk size (i.e.,
from 700 B to 16 KB) in the original rsync implementation [15].
Moreover, we ﬁnd that SugarSync, IDriveSync, and 360 CloudDisk
also utilize the IDS mechanism for their PC clients.
On the contrary, as depicted in Figure 4 (b) and 4 (c), web-based
apps and mobile apps for all the six services still use the full-ﬁle
sync mechanism, probably because IDS is hard to implement in
JavaScript (for web-based apps) or due to energy concerns (for mo-
bile apps). Speciﬁcally, JavaScript is the most widely used script
language for the development of web-based apps (including cloud
storage apps). Nevertheless, for security concerns, JavaScript is
unable to directly invoke ﬁle-level system calls/APIs like open,
close, read, write, stat, rsync, and gzip [11]. Instead, JavaScrip-
t can only access users’ local ﬁles in an indirect and constrained
manner, which is of less efﬁciency in terms of implementing IDS.
In summary, we have the following ﬁnding about simple ﬁle
modiﬁcation and the data sync granularity:
• When a ﬁle modiﬁcation is synchronized to the cloud, TUE
is mostly affected by the data sync granularity which varies
signiﬁcantly among different cloud storage services. Most
services simply use full-ﬁle sync, but some services (like
Dropbox and SugarSync) utilize IDS to achieve improved
network-level efﬁciency for PC clients.
Conﬂicts between IDS and RESTful infrastructure. Although
enabling the IDS mechanism can help service providers reduce the
data sync trafﬁc, implementing IDS is not an easy job in practice.
Most of today’s cloud storage services (e.g., OneDrive, Dropbox,
and Ubuntu One) are built on top of RESTful infrastructure (e.g.,
Amazon S3, Microsoft Azure, and OpenStack Swift). For simplify-
ing both the providers’ implementation complexities and the devel-
opers’ programming complexities, RESTful infrastructure typically
only supports data access operations at the full-ﬁle level, like PUT
(upload a new ﬁle), GET (download a whole ﬁle), DELETE (delete
a whole ﬁle), and so forth. Note that the PUT, GET, and DELETE
operations may have aliases in other RESTful infrastructure.
Thus, enabling IDS usually requires an extra mid-layer for trans-
forming MODIFY into GET + PUT + DELETE ﬁle operations in an
efﬁcient manner (like what Dropbox has done [25, 36]) 4. Since ﬁle
modiﬁcations frequently happen, implementing such a mid-layer is
worthwhile for improved network-level efﬁciency.
4An alternative to enable IDS is to store every chunk of a ﬁle as a
separate data object. When a ﬁle is modiﬁed, the modiﬁed chunks
are deleted with the new chunks being stored as new objects; also,
ﬁle metadata has to be updated (as what is used in Cumulus [43]).