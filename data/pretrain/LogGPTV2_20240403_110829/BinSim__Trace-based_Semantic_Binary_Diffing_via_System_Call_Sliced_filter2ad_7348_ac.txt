wp(in,P) = Pn−1, then wp(in−1,Pn−1) = Pn−2 and un-
til wp(i1,P1) = P0. The weakest precondition, denoted
as wp(S,P) = P0, is a boolean formula over the inputs
that follow the same slice (S) and forces the execution
to reach the given point satisfying P. We adopt a simi-
lar algorithm as Banerjee et al.’s [4] to compute the WP
for every statement in the slice, following both data de-
pendency and control dependency. The resultant WP for-
mula for a program point can be viewed as a conjunction
of predicates accumulated before that point, in the fol-
lowing form:
WP = F1 ∧ F2 ∧ ...∧ Fk.
Opaque predicates [17], a popular control ﬂow obfus-
cation scheme, can lead to a very complicated WP for-
mula by adding infeasible branches. We apply recent
opaque predicate detection method [45] to identify so
called invariant, contextual, and dynamic opaque pred-
icates. We remove the identiﬁed opaque predicate to re-
duce the size of the WP formula.
4.5 Segment Equivalence Checking
We identify whether two API calls are semantically
equivalent by checking the equivalence of their argu-
ments’ weakest preconditions. To this end, we perform
validity checking for the following formula.
wp1 ≡ wp2 ∧ arg1 = arg2
(3)
Different from existing block-centric methods, whose
equivalence checking is limited at a single basic block
level, our WP calculation captures the logic of a seg-
ment of instructions that go across the boundaries of ba-
sic blocks. Our method can offer a logical explanation
of whether syntactically different instruction segments
contribute to the same observable behavior. Frequent in-
vocation of constraint solver imposes a signiﬁcant over-
head. Therefore, we maintain a HashMap structure to
cache the results of the previous comparisons for better
performance.
To quantitatively represent different levels of similar-
ity and facilitate our comparative evaluation, we assign
different scores (0.5 ∼ 1.0) based on the already aligned
system call sequences. The similarity sore is tuned with
our ground truth dataset (Section 5.2) by two metrics:
precision and recall. The precision is to measure how
well BinSim identiﬁes different malware samples; while
recall indicates how well BinSim recognizes the same
malware samples but with various obfuscation schemes.
An optimal similarity sore should provide high precision
and recall at the same time. We summarize the selection
of similarity score as follows.
1. 1.0:
the arguments of two aligned system calls
pass the equivalence checking.
Since we have
conﬁdence these system calls should be perfectly
matched, we represent
their similarity with the
highest score.
2. 0.7: the sliced segments of two aligned system calls
are corresponding to the same cryptographic algo-
rithm (e.g. AES vs. AES). We assign a slightly
lower score to represent our approximate matching
of cryptographic functions.
3. 0.5: the aligned system call pairs do not satisfy the
above conditions. The score indicates their argu-
ments are either conditionally equivalent or seman-
tically different.
Assume the system call sequences collected from pro-
gram a and b are Ta and Tb, and the number of aligned
system calls is n. We deﬁne the similarity calculation as
follows.
Sim(a,b) =
∑n
i=1 Similarity Score
Avg{|Ta|,|Tb|}
(4)
∑n
i=1 Similarity Score sums
the similarity score of
aligned system call pairs. To balance the different length
of Ta and Tb and be sensitive to system call noises in-
sertion, we use the average number of two system call
sequences as the denominator. The value of Sim(a,b)
ranges from 0.0 to 1.0. The higher Sim(a,b) value indi-
cates two traces are more similar.
USENIX Association
26th USENIX Security Symposium    261
Table 1: Different obfuscation types and their examples.
Type
Examples
1
Intra-basic-block
2 Control ﬂow
3 ROP
4 Different implementations
Register swapping, junk code,
instructions substitution and reorder
Loop unrolling, opaque predicates,
control ﬂow ﬂatten, function inline
Synthetic benchmarks collected
from the reference [79]
BitCount (Figure 3)
isPowerOfTwo (Appendix Figure 12)
ﬂp2 (Appendix Figure 13)
Synthetic benchmarks
5 Covert computation[59]
6
Single-level virtualization VMProtect [69]
7 Multi-level virtualization
Synthetic benchmarks collected
from the reference [79]
5 Experimental Evaluation
We conduct our experiments with several objectives.
First and foremost, we want to evaluate whether BinSim
outperforms existing binary difﬁng tools in terms of bet-
ter obfuscation resilience and accuracy. To accurately
assess comparison results, we design a controlled dataset
so that we have a ground truth. We also study the effec-
tiveness of BinSim in analyzing a large set of malware
variants with intra-family comparisons. Finally, perfor-
mance data are reported.
5.1 Experiment Setup
Our ﬁrst testbed consists of Intel Core i7-3770 processor
(Quad Core with 3.40GHz) and 8GB memory, running
Ubuntu 14.04. We integrate FakeNet [63] into Temu to
simulate the real network connections, including DNS,
HTTP, SSL, Email, FTP etc. We carry out the large-scale
comparisons for malware variants in the second testbed,
which is a private cloud containing six instances running
simultaneously. Each instance is equipped with a duo
core, 4GB memory, and 20GB disk space. The OS and
network conﬁgurations are similar to the ﬁrst testbed.
Before running a malware sample, we reset Temu to a
clean snapshot to eliminate the legacy effect caused by
previous execution (e.g., modify registry conﬁguration).
To limit the possible time-related execution deviations,
we utilize Windows Task Scheduler to run each test case
at the same time.
5.2 Ground Truth Dataset
Table 1 lists obfuscation types that we plan to evaluate
and their examples. Intra-basic-block obfuscation meth-
ods (Type 1) have been well handled by semantics-based
binary difﬁng tools. In Section 2.1, we summarize pos-
sible challenges that can defeat the block-centric binary
difﬁng methods, and Type 2 ∼ Type 7 are correspond-
ing to such examples. We collect eight malware source
Figure 8: Similarity scores change from right pairs to
wrong pairs.
code with different functionalities from VX Heavens2.
We investigate the source code to make sure they are dif-
ferent, and each sample can fully exhibit its malicious
behavior in the runtime. Besides, we also collect syn-
thetic benchmarks from the previous work. The purpose
is to evaluate some obfuscation effects that are hard to
automate. Our controlled dataset statistics are shown in
Table 2. The second column of Table 2 lists different
obfuscation schemes and combinations we applied.
In addition to BinSim, we also test other six repre-
sentative binary difﬁng tools. BinDiff [23] and Darun-
Grim [50] are two popular binary difﬁng products in in-
dustry. They rely on control ﬂow graph and heuristics to
measure similarities. CoP [41] and iBinHunt [43] repre-
sent “block-centric” approaches. Based on semantically
equivalent basic blocks, iBinHunt compares two execu-
tion traces while CoP identiﬁes longest common subse-
quence with static analysis. System call alignment and
feature set are examples of dynamic-only approaches.
“Feature set” indicates the method proposed by Bayer
et al. [6] in their malware clustering work. They abstract
system call sequence to a set of features (e.g., OS object,
OS operations, and dependencies) and measure the simi-
larities of two feature sets by Jaccard Index. For compar-
ison, we have implemented the approaches of CoP [41],
iBinHunt [43], and feature set [6]. The system call align-
ment is the same to the method adopted by BinSim.
5.3 Comparative Evaluation Results
Naively comparing these seven binary difﬁng tools with
their similarity scores is not informative3. It is also very
difﬁcult to interpret precision and recall values because
each tool adopts different similarity metrics and thresh-
2http://vxheaven.org/src.php
3We have normalized all the similarity scores from 0.0 ∼ 1.0.
262    26th USENIX Security Symposium
USENIX Association
0.00.10.20.30.40.50.60.70.80.91.0  Similarity score Right pairs Wrong pairsBinSimFeature setSyscall align.CoPiBinHuntDarunGrimBinDiffTable 2: Controlled dataset statistics. The obfuscation type numbers are deﬁned in Table 1.
Sample
Obfuscation type LoC #
Online
(Normalized)
Preprocess
Ofﬂine (min)
Slicing & WP
STP (no/opt)
Malware
BullMoose
Clibo
Branko
Hunatcha
WormLabs
KeyLogger
Sasser
Mydoom
6
1+6
1+2+6
2
1
2
1+2+6
1+2
Synthetic benchmark
ROP
Different implementations
Covert computation
Multi-level virtualization
3
4
5
7
30
90
270
340
420
460
950
3276
449
80
134
140
5X
6X
8X
8X
8X
12X
9X
10X
6X
6X
6X
10X
1
1
2
2
2
2
3
3
1
1
1
4
2
2
3
4
6
6
8
10
3
2
2
12
1/0.5
2/0.8
3/1
1/1
3/2
4/2
4/3
6/4
2/1
2/0.8
3/1
5/3
Table 3: Absolute difference values of similarity scores under different obfuscation schemes and combinations.
Sample
BullMoose
Clibo
Branko
Hunatcha
WormLabs
KeyLogger
Sasser
Mydoom
ROP
Different implementations
Covert computation
Multi-level virtualization
Average
“Right pairs” vs. “Obfuscation pairs”
“Right pairs” vs. “Wrong pairs”
Obfuscation type BinDiff DarunGrim iBinHunt CoP
0.61
0.62
0.68
0.30
0.03
0.26
0.58
0.38
0.52
0.52
0.45
0.69
0.39
0.41
0.35
0.19
0.03
0.12
0.42
0.10
0.49
0.48
0.44
0.59
0.56
0.64
0.62
0.42
0.12
0.39
0.62
0.38
0.54
0.39
0.36
0.71
0.58
0.57
0.63
0.40
0.10
0.38
0.62
0.42
0.63
0.48
0.45
0.68
6
1+6
1+2+6
2
1
2
1+2+6
1+2
3