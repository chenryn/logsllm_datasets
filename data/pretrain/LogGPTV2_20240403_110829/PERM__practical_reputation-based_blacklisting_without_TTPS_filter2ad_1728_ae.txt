EUROCRYPT, volume 3027 of Lecture Notes in
Computer Science, pages 571–589. Springer, 2004.
[5] M. H. Au, P. P. Tsang, A. Kapadia, and W. Susilo.
[21] Z. Lin and N. Hopper. Jack: Scalable
BLACR: TTP-Free Blacklistable Anonymous
Credentials with Reputation. Technical Report
TR695, Indiana University Bloomington, May 2011.
[6] M. Bellare and P. Rogaway. Random Oracles are
Practical: A Paradigm for Designing Eﬃcient
Protocols. In ACM Conference on Computer and
Communications Security, pages 62–73, 1993.
[7] J. C. Benaloh and M. de Mare. One-Way
Accumulators: A Decentralized Alternative to Digital
Sinatures (Extended Abstract). In EUROCRYPT,
pages 274–285, 1993.
[8] D. Boneh, X. Boyen, and H. Shacham. Short Group
Signatures. In CRYPTO, volume 3152 of Lecture
Notes in Computer Science, pages 41–55, 2004.
[9] J. Camenisch, R. Chaabouni, and A. Shelat. Eﬃcient
Protocols for Set Membership and Range Proofs. In
ASIACRYPT, volume 5350 of Lecture Notes in
Computer Science, pages 234–252. Springer, 2008.
accumulator-based Nymble system. In WPES, pages
53–62, 2010.
[22] P. Lofgren and N. Hopper. FAUST: Eﬃcient,
TTP-Free Abuse Prevention by Anonymous
Whitelisting. In Proceedings of the Workshop on
Privacy in the Electronic Society (WPES), Oct. 2011.
[23] T. P. Pedersen. Non-Interactive and
Information-Theoretic Secure Veriﬁable Secret
Sharing. In CRYPTO’91, volume 576 of LNCS, pages
129–140, 1992.
[24] P. P. Tsang, M. H. Au, A. Kapadia, and S. W. Smith.
Blacklistable Anonymous Credentials: Blocking
Misbehaving Users without TTPs. In ACM
Conference on Computer and Communications
Security, pages 72–81. ACM, 2007.
[25] P. P. Tsang, M. H. Au, A. Kapadia, and S. W. Smith.
PEREA: Towards Practical TTP-Free Revocation in
Anonymous Authentication. In ACM Conference on
938Computer and Communications Security, pages
333–344, 2008.
[26] P. P. Tsang, M. H. Au, A. Kapadia, and S. W. Smith.
BLAC: Revoking Repeatedly Misbehaving Anonymous
Users without Relying on TTPs. ACM Trans. Inf.
Syst. Secur., 13(4):39, 2010.
[27] P. P. Tsang, A. Kapadia, C. Cornelius, and S. W.
Smith. Nymble: Blocking Misbehaving Users in
Anonymizing Networks. IEEE Trans. Dependable Sec.
Comput., 8(2):256–269, 2011.
APPENDIX
A. SECURITY ANALYSIS
We adopt the simulation-based security deﬁnition as in
PEREA [4]. In the real world there are a number of play-
ers who communicate via cryptographic protocols while in
the ideal world the same players communicate via a trusted
party T who is responsible for handling all the inputs and
outputs for the players. The adversary A controls the same
players in the real world and the ideal world. All the inputs
and the scheduling of the players’ interactions are decided
by another probabilistic polynomial time (PPT) algorithm,
and the environment E. A can communicate arbitrarily with
E. Informally speaking, PERM is secure if for any PPT al-
gorithms A and E, there exists another algorithm S control-
ling the same players in the ideal world as A does in the real
world such that E cannot tell if it is interacting with A or
S. S has black-box access to A.
PERM supports a set of functionalities. An invocation of
a functionality is an event. We assume all events are sched-
uled according to E’s wishes. We use a static model and
assume the number of players, and whether they are honest
or not is ﬁxed before the system starts. All communications
with T are not anonymous, meaning that T knows the iden-
tity of the communicating parties. It is also assumed that
communication between honest parties is not observed by A
and when A receives a message, it does not learn its origin.
PERM supports the following functionalities:
1. SP Setup. The system begins when E speciﬁes the
number of honest and dishonest users and SPs.
• Real World. The SP generates a key pair (P K, SK).
P K is made available to all players in the system.
• Ideal World.
The trusted party T initializes a
database, which stores the registration status and
authentication history of all the users. To capture
the functional requirement of PERM, T keeps track
of the score of every user with respect to each cate-
gory as well as all authentications that the user has
participated in.
2. Registration. E instructs user i to register with the
SP. Note that this procedure is not anonymous in the
view of the SP. For all i, an honest SP would allow user
i to register only once.
• Real World. User i sends a request for registration
to the SP. The user, as well as the SP, outputs indi-
vidually the outcome of this transaction to E. If user
i has obtained a credential in a previous registration
event, then an honest SP would reject the request.
Likewise, an honest user would discard the second
credential it obtains from the SP if it has success-
fully registered in a previous registration event.
• Ideal World. User i sends a registration request
to T , who informs the SP that user i would like
to register and whether user i has obtained a cre-
dential before. The SP returns its decision to T ,
who forwards it back to the user. If the SP accepts
the request and that user i has not registered be-
fore, T stores the registration status of user i in its
database. The user, as well as the SP, individually
output the outcome of this transaction to E.
• Ideal World.
3. Authentication. E instructs user i to authenticate
with the SP and instructs the SP to impose an access
policy P.
• Real World. User i conducts the authentication pro-
tocol with the SP imposing the access policy P. The
user, as well as the SP, output individually the out-
come of this transaction as well as the transaction
identiﬁer t to E.
User i sends a request to T , who
informs the SP some anonymous user requests an
authentication. The SP replies with the list L, the
current transaction identiﬁer t and the policy P.
T forwards the reputation lists, the value t and P
back to user i and whether i satisﬁes the authenti-
cation policy or not. User i then decides if he/she
would continue. If yes, T informs the SP whether
the anonymous user satisﬁes the authentication pol-
icy or not. The SP replies with accept or reject to
T , who forwards the reply to user i. If the authen-
tication is successful, T stores t as one of the user’s
transaction identiﬁers. The user, as well as the SP,
output individually the outcome of this transaction
as well as the transaction identiﬁer t to E.
4. Scoring a transaction. E instructs the SP to give
a score of (s1, . . . , sJ ) to transaction identiﬁer t. If t is
not a valid authentication or has already been put on
reputation list L, an honest SP ignores this request.
5. Updating a score. E instructs the SP to update a
score of (s1, . . . , sJ ) for transaction identiﬁer t. If t is
not a valid transaction identiﬁer on L, then an honest
SP would ignore this request.
6. Score Update. E instructs user i to update his score
on transaction identiﬁer t. If t is not a past identiﬁer
of the user, an honest user ignores the request.
• Real World. User i conducts the score update proto-
col with the SP. The user, as well as the SP, output
individually the outcome of this transaction as well
as the transaction identiﬁer t to E.
• Ideal World. User i sends a request to T , who in-
forms the SP some anonymous user requests a score
update on transaction identiﬁer t. The SP replies
with accept or reject. If the SP replies accept,
T updates the stored reputation of the user. The
user, as well as the SP, output individually the out-
come of this transaction as well as the transaction
identiﬁer t to E.
The ideal-world PERM provides all the desired security
properties and functionalities of PERM. Firstly, all the
transactions, in the view of the SP, are anonymous. T only
informs the SP that some anonymous user would like to au-
thenticate and thus anonymity is guaranteed. Secondly, T
veriﬁes whether the authenticating user satisﬁes the access
939policy and thus the system functions correctly. The real-
world PERM is secure if its behavior is the same as the
ideal-world PERM. Thus, assuming negl(λ) is a negligible
function in security parameter λ, we have the following def-
inition of security for any construction of PERM.
Deﬁnition 1. Security.
(resp.
IdealE,S (λ) ) be the probability that E outputs 1 when
ideal world) with adversary A
run in the real world (resp.
(resp. S having black-box access to A). PERM is secure if
for all PPT algorithms E, A, the following expression holds:
Let RealE,A(λ)
|RealE,A(λ) − IdealE,S (λ)| = negl(λ)
To prove that PERM is secure, we have to construct an
ideal-world adversary S given any real-world adversary A
in such a way that no PPT environment E can distinguish
whether it is interacting with S or A.
The proof is divided into two cases according to the subset
of players controlled by A. In the ﬁrst case, A controls the
SP and a subset of users while, in the second case, only a
subset of users is dishonest. Note, the latter is not a special
case of the former. An adversary controlling the SP covers
the security requirements of anonymity, while an adversary
controlling a subset of users covers the security requirements
of authenticity and non-frameability.
Adding players controlled by A does not necessarily make
the construction of S more diﬃcult. On one hand, the con-
struction of S is trivial if A controls all players in the system,
for S can simply forward all messages exchanged between E
and A. On the other hand, the construction of S is also
In that
trivial when all players in the system are honest.
case, A did not participate in the system and the indistin-
guishability depends only on the correctness of the system.
We sketch the proof strategy of how S can be constructed
in these two cases. Firstly, S maintains a list of ‘current’
credentials issued to A during the lifespan of the system.
At the same time, S acts as an ideal-world adversary to the
trusted party T . S simply forwards any messages between
E and A. Next, we specify how S responds to each possible
event in the two diﬀerent cases.
Case 1: The SP is honest
• SP Setup.
• Representing an Honest SP to A. S generates the
key pair (P K, SK) and gives P K to A.
• Registration.
• Representing a dishonest user i to T / an honest SP
to A. Using the zero-knowledge extractor, S extracts
from A the value x. x will be used to identify the
dishonest user i. S sends the request to T on behalf
of user i. If T replies accept, S issues the credential
to A and also stores that credential.
• Authentication. Note that S does not receive P from
E directly since P is sent to the honest SP in the ideal
world. However, S learns about P from T on behalf of
the dishonest user i in the ideal world.
• Representing a dishonest user i to T / an honest SP
to A. The diﬃculty here is that S does not know
which credential A is using for the authentication.
For instance, while E speciﬁes the user i should per-
form the authentication, it is entirely possible for A
to use the credential from another dishonest user say,
ˆi, to perform the authentication. To locate the ac-
tual user, S extracts and uses the value x during the
authentication to locate the correct user.
The outputs of S and the honest users in the ideal world
are always indistinguishable to A and the honest users in
the real world unless the following happen. We also explain
why such cases happen with negligible probability below.
1. During a Registration event, S fails to extract from
A the value x. This happens with negligible probability
under the soundness property of the protocol SIss of the
BBS+ signature scheme.
2. During a successful Authentication event, S fails to
extract from A the values x. This happens with neg-
ligible probability under the soundness property of the
protocol SSig of the BBS+ signature scheme.
3. There exists a successful Authentication event from
A such that S on behalf of an honest SP outputs ac-
cept, but T indicates the authenticating user does not
satisfy the policy. This represents either that A has
been able to fake one of the proofs in the authentica-
tion or A can forge a signature on a new queue that
has never been signed. All these happen with negligi-
ble probability under the assumption that BBS+ signa-
tures are existentially unforgeable and that the interval
proof is sound.
Note that in the security proof, we require S to run
the zero-knowledge extractor on A for each registration
and authentication event. To keep S in polynomial-time,
we have to require authentication and registration events
are to be executed sequentially (security proofs of BLACR
and PEREA also impose this restriction) or to employ the
stronger universally composable proofs of knowledge.
Case 2: The SP is dishonest
• SP Setup.
• Representing Honest users to A. S receives P K from
A.
• Registration.
• Representing a dishonest user to T / an honest user
i to A. Upon receiving a registration request from T
on behalf of user i, S engages A in the registration
protocol, using the zero-knowledge simulator to sim-
ulate the ZKPoK in SIss. If S fails to obtain a valid
credential from A, then S replies reject to T .
• Authentication.
• Representing a dishonest SP to T / an honest user to
A. Upon receiving an authentication request from T
on behalf of an anonymous user, S engages A in the
If T replies with a bit in-
authentication protocol.
dicating that the underlying user would proceed and
satisﬁes the authentication policy, S uses the zero-
knowledge simulator to simulate the ZKPoK proofs
in the authentication protocol using a random value
q. If A rejects the authentication, S replies reject
to T .
The simulation provided to A is perfect due to the zero-
knowledgeness of the ZKPoK protocols and the perfect hid-
ing property of the commitment scheme. At the same time,
the behavior of S in the ideal world is the same as that of A
in the real world. Thus, the output of S to the environment
E is indistinguishable from that of A.
Based on this dual strategy in the construction of S, our
construction of PERM is secure according to Deﬁnition 1.
940