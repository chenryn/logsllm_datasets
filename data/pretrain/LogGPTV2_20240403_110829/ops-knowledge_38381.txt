User: How to use Elastic Stack for log collection and analysis? Please provide an example of a working Elastic Stack configuration.
Assistant: Elastic Stack is an open source log management and analysis tool, which includes four components: Elasticsearch, Logstash, Beats and Kibana. Among them, Elasticsearch is a distributed full-text search and analysis engine for storing and querying log data; Logstash is a log collection and processing tool for collecting log data from various sources and cleaning and converting; Beats is a lightweight A magnitude data collector for collecting different types of log data; Kibana is a visual data analysis and display tool for displaying and analyzing log data stored in Elasticsearch. The following is a feasible Elastic Stack configuration example for collecting and analyzing Apache Web server access logs: 1. Preparation: Install Java and Elastic Stack Before starting to configure Elastic Stack, you need to ensure that Java and Elastic Stack have been installed correctly. Java can be installed with the following command: $ sudo apt-get install openjdk-8-jdk You can download the latest software package from the Elastic Stack official website and decompress it, or use the package manager to install it: $ sudo apt-get install elasticsearch logstash kibana 2. Configure Elasticsearch Open the "/etc/elasticsearch/elasticsearch.yml" file and modify the following configuration: cluster.name: my-cluster node.name: my-node-1 network.host: 127.0.0.1 discovery.zen.ping .unicast.hosts: ["127.0.0.1"] http.port: 9200 Save and close the file, start Elasticsearch: $ sudo systemctl start elasticsearch You can test whether Elasticsearch is running normally by running the following command: $ curl http://localhost:9200 Elasticsearch is working correctly if it returns output similar to the following: { "name" : "my-node-1", "cluster_name" : "my-cluster", "cluster_uuid" : "xxxxxx", "version" : { " number" : "7.5.1", "build_flavor" : "default", "build_type" : "deb", "build_hash" : "xxxxxx", "build_date" : "2020-12-15T12:56:30.409026Z", "build_snapshot" : false, "lucene_version" : "8.3.0", "minimum_wire_compatibility_version" : "6.8.0", "minimum_index_compatibility_version" : "6.0.0-beta1" }, "tagline" : "You Know, for Search" } 3. Configure Logstash Create a file named "/etc/logstash/conf.d/apache.conf" to configure the input and output plug-ins of Logstash: input { beats { port => 5044 } } filter { grok { match => { "message" => "%{COMBINEDAPACHELOG}" } } date { match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ] } } output { elasticsearch { hosts => ["localhost:9200"] index => "apache-%{+YYYY.MM.dd}" } } In the above configuration file, we use Beats as the input plug-in to collect log data, and use the Grok plug-in to parse and process the data . At the same time, we use Elasticsearch as an output plugin to store the processed log data into Elasticsearch. Save and close the file, start Logstash: $ sudo systemctl start logstash 4. Configure Kibana Open the "/etc/kibana/kibana.yml" file and modify the following configuration: server.port: 5601 server.host: "localhost" elasticsearch.hosts: ["http://localhost:9200"] Save and close the file, start Kibana: $ sudo systemctl start kibana 5. Test the Elastic Stack configuration Now that we have configured the Elastic Stack, we can test whether it is working properly by following the steps :- Enable access logs on the Apache server, and configure the output to Logstash's Beats plugin port "5044"; - Create a new index pattern in Kibana, and select "apache-*" as the index name, configure the fields according to the default settings ; - Search, analyze and visualize Apache access log data in Kibana's "Discover" page. So far, we have successfully used Elastic Stack to collect and analyze Apache access logs. Of course, for different applications and environments, the configuration of the Elastic Stack will also be different, and it needs to be adjusted and optimized according to the actual situation.