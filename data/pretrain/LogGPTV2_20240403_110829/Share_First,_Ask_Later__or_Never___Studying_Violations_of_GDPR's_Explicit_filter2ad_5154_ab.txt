lution to detect personal data that is being sent to the Internet
without users’ explicit consent, as is mandated by the GDPR.
We set up an array of Android devices, on which we run each
app (without any interaction) and capture the network trafﬁc
(Section 3.1). Based on personal data which is directly tied to
the phone (see Table 1), we automatically detect this data in
both plain and encoded form through string matching. More-
over, we derive a methodology that allows us to pinpoint data
that may be other unique identiﬁers and manually validate
whether this can be used to track the user/device (Section 3.2).
In the following, we outline how we conduct each of the steps
in more detail.
3.1 App Setup and Network Trafﬁc Collection
We run each app and capture its network trafﬁc. Here, we aim
to detect apps’ network trafﬁc without users’ explicit consent.
To achieve this, we simply open the app but do not interact
with it at all. The underlying assumption is that if network
trafﬁc occurs when this app is opened without any interactions,
we have naturally not consented explicitly to any type of data
collection by third parties. Hence, any data being sent out must
not be PD, so as not to violate GDPR. Orthogonal to that, in
practice, users may not grant all the apps’ permission requests,
or the app may use the runtime-permission mechanism (i.e.,
the permissions are not granted at installation time, and users
will allow/deny permission requests at runtime when using
the app). As such, it may be the case that PD (e.g., the IMEI)
can only be accessed after the user consents to API usage.
However, this API consent does not imply consent to have
sensitive data shared with third parties. Therefore, to be legally
compliant, the app must respect explicit consent even if it is
authorized to access the data through a granted permission.
Recall that our goal is to analyze apps on a large scale.
Hence, relying on static analysis techniques, which may
produce a vast amount of false positives, is not an op-
tion [12, 39, 67]. Furthermore, we aim to have a lightweight
3670    30th USENIX Security Symposium
USENIX Association
solution to allow us to check thousands of apps in a reasonable
time. Hence heavyweight instrumentation of the app itself
is out of the question. Therefore, our approach is prone to
miss certain instances (e.g., if we are unable to detect unique
identiﬁers in the outgoing trafﬁc or the app crashes in our
lightweight instrumentation).
We rely on six rooted devices (Pixels, Pixel 3a, and Nexus
5) running Android 8 or 9 to analyze a given app. To intercept
the TLS trafﬁc, the devices are instrumented with our own
root certiﬁcate (i.e., by using MitM proxy [15]). Further, we
use objection to detect and disable SSL Pinning [47]. In the
ﬁrst step of our analysis pipeline, we aim to identify apps that
send some data when started. To achieve that, we install the
app in question and grant all requested permissions listed in
the manifest, i.e., both install time and runtime permissions.
Subsequently, we launch the app and record its network trafﬁc.
As our initial tests showed that apps sometimes did not load
on ﬁrst start, we close the app and reopen it, so as to increase
the chances of observing any trafﬁc. If an app shows no trafﬁc
in either of these starts, we discard it from further analysis.
3.2 Trafﬁc Log Analyzer
Under the GDPR, personal data includes the Advertising
IDs [46], location data, and online identiﬁers (e.g., IP ad-
dresses, any unique tracking identiﬁers) which can be used
to identify users over a long period, potentially across differ-
ent apps and services [4]. Next to data protected through OS
permissions (e.g., IMEI, MAC), an app may also use other
types of persisted, unique identiﬁers to track users. Hence,
our analysis focuses on all possibly sensitive data as well as
data that can be used to uniquely track a user or a speciﬁc
instance of the app on their phone.
3.2.1 String-Matching Device-Bound Data
The ﬁrst type of data we consider is such data that is tied to the
phone, such as the location, the AAID, or the MAC address.
Since such information is accessible by apps, we extract the
relevant values from the phone through the Android debug
bridge to ensure we know these values for each phone. The
data selected for this (see Table 1) is inspired by the data used
in prior work [52, 56]. Speciﬁcally, we ﬁrst use simple string
matching to identify PD that is static and known in advance.
This information includes persistent identiﬁers bound to the
phone (e.g., the IMEI, the MAC address of the WiFi interface,
and the AAID) and those that are otherwise sensitive, such
as the device owner’s name, email address, or phone number.
For the geolocation data, we search for the precise latitude
and longitude written as a ﬂoating-point number, and those
values are rounded to 3, 4, and 5 decimal places.
Beyond simple string-comparison, we also search for com-
mon transformations, such as upper/lower case, hashing (e.g.,
MD5, SHA-1), or encoding (e.g., base64) in our analysis.
Naturally, this may miss cases in which, e.g., a custom hash
function is used on the sensitive data by the app. To identify
such cases as well as cases in which an app creates a persis-
tent identiﬁer itself, we conduct a second check for potential
unique tracking identiﬁers.
3.2.2 Potentially Unique Tracking Identiﬁers Detector
This step aims to identify parameters that could be used to
track and proﬁle an individual, but do not obviously string-
match with known values such as the IMEI. We aim to cover
both cases of obfuscated usage of common identiﬁers as well
as those cases where the app generates a persistent identiﬁer
and stores it locally. For example, from Android 8.0, the An-
droid ID scopes to {user, app signing key, device} that does
not obviously string-match with known identiﬁers.
More speciﬁcally, for a given app, we perform multiple
runs (Ri) with a different set of devices (Pi) to monitor and
record its network trafﬁc. For each run Ri on a particular
device Pi, we ﬁrst install the app in question and grant all
necessary requested permissions. While monitoring the app
network trafﬁc, we start the app, close the app and start it
once more. By analyzing the captured trafﬁc in run Ri, we
extract all contacted hosts (domain names) as well as the
GET and POST parameters (including parsing JSON if the
content type of the request is set accordingly). This allows
us to identify all contacted domains as well as the param-
eters and the values the app sent out. The output contains
a set of triples ti={}. Each triple
 is the identiﬁed contacted domain
together with its parameter and the value that is being sent in
the run Ri by the analyzed apps.
To this end, we further deﬁne two functions: (1) di f f (ti,t j)
outputs all triplets of  in ti for
triples that have the same domain and parameter but different
value between ti and t j; (2) the function stable(ti,t j) outputs
all triplets of parameters which remained unchanged between
two sets. Figure 2 shows an overview of our approach to
detect potential unique identiﬁers (which we refer to as UID
in the following). In general, four steps are involved:
1. On phone P1, we ﬁrst perform a run R1 to discover all
the app’s network trafﬁc. Then, by analyzing the R1
trafﬁc, we identify all of the contacted domains and their
parameters (t1). If there is no data sent to the Internet by
the app (t1 = {}), no further step is required.
2. On the same phone P1, we now conduct run R2 (instal-
lation and two open/close cycles). In between the two
runs, we uninstall the app and set the time one day into
the future. The intuition here is that if an app is sending
some persistent identiﬁer, this would be the same across
time and remain on the device (e.g., in persistent storage
or based on some information about the phone). Again,
USENIX Association
30th USENIX Security Symposium    3671
App
App
App
App
1
2
3
4
R1 trafﬁc
R2 trafﬁc
(Time: plus 1 day)
R3 trafﬁc
R4 trafﬁc
R5 trafﬁc
I
n
s
t
a
l
l
t
h
e
a
p
p
a
n
d
g
r
a
n
t
t
h
e
n
e
c
e
s
s
a
r
y
p
e
r
m
i
s
s
i
o
n
s
I
d
e
n
t
i
f
y
c
o
n
t
a
c
t
e
d
d
o
m
a
i
n
s
w
i
t
h
t
h
e
d
a
t
a
t
h
a
t
i
s
s
e
n
t
t1
t2
t3
if t1 = {} then stop
c1 = stable(t1,t2)
c2 = di f f (c1,t3)
t4, t5
c3 =
di f f (c2,stable(t4,t5))
Figure 2: Overview of our methodology to identify potential
UIDs. After each step, the analysis terminates if the resulting
set of candidate parameters is empty.
we analyze the trafﬁc to extract tuples (t2). All parame-
ters which are not stable between these runs cannot be
persistent identiﬁers (e.g., the date) and are hence dis-
carded. Suppose an app has any parameters with stable
values across the two runs (c1 = stable(t1,t2)). In that
case, we consider a ﬁrst candidate list for the next step —
otherwise, we terminate the analysis (if c1 = {}).
3. We now perform a run R3 and extract the triplets from
its trafﬁc (t3) on a different phone P2. For each param-
eter value that remains stable across the two phones
(stable(c1,t3)), we assume the stable value is tied to the
app (such as some app-speciﬁc token) and hence discard
these. If an app has at least one parameter, for which the
value remained stable between R1 and R2 (both on P1),
but differs between R2 (P1) and R3 (P2), we consider this
app further (naturally only considering those parameters
that differed), i.e., c2 = di f f (c1,t3) (cid:54)= {}.
4. Given the diversity in our used phones, such a differ-
ence may simply be caused by the make and model or
the OS version that is being sent out. To remove such
cases, we now conduct further two runs R4 and R5, this
time on two phones with the same make and model and
OS version (Pixel 3a with Android 9). Suppose data
is stable between these two runs (stable(t4,t5) (cid:54)= {}).
In that case, we deem the corresponding parameter to
be related to the device’s make, model, or OS version,
which is not a viable tracking identiﬁer, and hence dis-
card the entries. The outputs of the ﬁnal step is then
c3 = di f f (c2,stable(t4,t5)).
Finally, this leaves us with a highly ﬁltered list of can-
Parameter
Domains
appsﬂyer.com deviceFingerPrintId=
branch.io
tapjoy.com
unity3d.com
hardware_id=6fd9a2e0f2721498
managed_device_id=tjid.36cec2b4196...
common.deviceid=d3d55baf21d8f31839...
Table 2: Examples of the UIDs identiﬁed by our approach.
didates for persistent identiﬁers (c3). In the ﬁnal step, we
manually check the parameters identiﬁed in this fashion, to
ensure that they do not contain false positives. Particularly,
we removed low-entropy entries such as carriers, time zones,
or LAN IPs. Moreover, we also took into account the names
of the parameters, disregarding parameter names that did not
indicate any identifying capabilities (such as rs parameter on
helpshift.com, which has sufﬁcient entropy but lacks the
clear indication of an identiﬁer in its name). For our analysis,
which we present in detail in Section 4, we identiﬁed 2,113
potential parameter/domain pairs that matched the criterion
in the ﬁnal stage. Of those, we discarded 412, e.g., because
they were related to the (different) carriers or install times.
Examples of different UIDs we detected this way are shown
in Table 2. That is, given an app, our pipeline can automati-
cally detect the sending of personal data (such as IMEI, IMSI,
UIDs) without users’ prior explicit consent. However, we have
to manually vet the potential UIDs to avoid false-positive re-
ports. Notably, we therefore may have missed true positives,
which we nevertheless favor over a false positive.
3.3 Limitations
Our approach naturally suffers from certain limitations, some
of which are desired. As an example, an app might show a
welcome screen unrelated to data collection consent and only
send out data once the user interacts with the app. Our frame-
work would miss such cases of incorrect consent handling.
We consciously decided to allow these false negatives, as un-
derstanding whether or not the welcome screen asks explicit
consent and opt-out is infeasible to be done automatically.
Second, it might be possible that the consent notices are
part of the runtime permissions request (e.g., apps have ratio-
nales that indicate data collection and use). By automatically
granting all apps’ permission requests, our approach might
have false positives for such cases. However, in practice, Liu
et al. [41] show that most developers do not provide rationales
for permission requests (less than 25% of apps in their study).
Moreover, before Android 6, only install-time permissions
existed, meaning that any app compatible with Android 5 or
lower could not ask for consent in the permission request. Out
of the apps that we detected to send PD (see Section 4), about
96% support Android prior to 6.
Third, given that we rely on software that attempts to by-
pass security mechanisms (in particular SSL pinning), the
apps may be able to detect such manipulation, e.g., by check-
3672    30th USENIX Security Symposium
USENIX Association
ing which CA is the root of the trust chain. Similarly, an app
may also simply not start on a rooted device. Moreover, apps
may not be supported on the Android 8 devices, which means
they might not start and hence cannot be analyzed. Generally
speaking, all these are potential causes for false negatives.
Finally, an app may also transmit a persistent identiﬁer in
some encrypted form with changing encryption keys or use a
custom serialization format. Naturally, this is not something
we can account for, and we would miss the parameter (as we
could not decode the serialization protocol or, in case of the
encryption case, its values already change between R1 and
R2). However, we argue that if any app is detected to send out
PD in our automated system, we have never granted explicit
consent; hence we do not suffer from false positives.
4 Large-Scale Analysis
In this section, we present the results of our empirical study
of Android apps on Google Play, with respect to the violation
of GDPR’s explicit consent mandate. We ﬁrst outline which
datasets we consider and subsequently present our analysis
results. We note that all technical testing was done in Germany
where the GDPR applies, i.e., our geolocation is Germany
and the app store is set to the German variant. Based on our
ﬁndings, we manually analyze the contacted domains with
the help of a legal scholar to determine which third parties are
data controllers for which GDPR mandates explicit consent.
4.1 App Dataset Construction
Our analysis aims to assess the state of GDPR violations in
both high-proﬁle and long-tail apps on the Play Store, and to
understand if the violations are speciﬁc to either of them. To
achieve this and compare these types of apps, we sample two