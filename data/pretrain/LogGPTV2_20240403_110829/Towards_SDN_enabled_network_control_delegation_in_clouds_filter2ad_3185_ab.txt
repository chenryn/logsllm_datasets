### Problem Definition and Constraint Formulation

Given a set of switches \(\{u_1, u_2, \ldots, u_N\}\), we need to find a subset \(s'_i\) corresponding to each subset \(s_i\) such that the union of all these subsets is strictly less than the total number of switches \(S\) in the cloud provider’s network. Specifically, if we let boolean variables \(n_{ij}\) represent whether the \(i\)-th switch in set \(s_j\) lies in \(s'_j\), then our network should satisfy the following constraint:

\[
\sum_{j=1}^{N} \sum_{i=1}^{|s_j|} n_{ij} < S
\]

This constraint can be expressed as a Satisfiability Modulo Theories (SMT) formula, and an SMT solver can be used to determine which switches can be exposed to which users without violating the constraints. Figure 1 illustrates our conceptual framework for delegating network control under these constraints.

### Framework Instantiations and Tradeoff Discussion

In this section, we demonstrate two possible instantiations of the framework using FlowVisor and ADVisor. We discuss the security tradeoffs associated with providing different levels of network abstractions to end users. The following architectures are specific instantiations of our conceptual framework in Figure 1. The key difference between the two architectures is how the "logic for exposing views to the users" creates these views.

#### A. FlowVisor-Based Architecture

We leverage existing mechanisms to achieve our goal of delegating some network control to end users. For our first attempt, we use FlowVisor. Figure 2 shows the proposed architecture using FlowVisor. As mentioned in Section II-C, an administrator needs to define policies in terms of flowspaces in FlowVisor. These policies specify which controller can see which switches and which packets on those switches should be controlled by a particular controller.

For example, consider a packet generated by a host \(A1\) (a VM belonging to UserA). The packet arrives at switchA, which has no flow table entry, so it does not know how to forward the packet. The packet is sent to FlowVisor, which checks the packet header and decides which flowspace or controller the packet belongs to. Since this packet should be under the control of UserA, FlowVisor forwards the packet to ControllerA for making flow decisions. ControllerA then pushes new flow rules into switchA. Before these flow rules are forwarded to the switches, they are inspected and vetted by the FlowVisor layer to ensure that ControllerA does not inadvertently affect the packets under the flowspace of ControllerB.

The direct visibility of all switches in the path of users' flows allows several local properties to be satisfied, as the underlying network infrastructure is exposed to users with minimal mediation. The connectivity property is easily satisfied as long as there is connectivity in the physical infrastructure. The datacenter consistency property is also satisfied, as the exposed physical infrastructure matches the datacenter architecture. Multi-path routing is potentially possible if the underlying physical network supports it.

However, in terms of global properties, the non-interference among users is satisfied by mediating network flows. This architecture fails to provide views that satisfy the non-inference property, which prevents end users from mapping the complete underlying topology. If users collude, they could combine their views and potentially discover the complete topology. For instance, if UserA and UserB share their views, even though most of the switches they control are not common, they could still expose the whole topology.

#### B. ADVisor-Based Architecture

To overcome this limitation, in our second instantiation, we replace FlowVisor with ADVisor, as shown in Figure 3. The advantage of using ADVisor over FlowVisor is that it abstracts away some of the nodes in the physical topology and presents a subset of the resulting view to the users. This abstraction ensures that even if a few cloud users collude and share their views, they will not be able to discover the exact underlying topology. Thus, ADVisor permits creating subsets of user views that satisfy the global property of non-inference.

However, if all cloud users collude, nothing can be done to protect the network topology information. Requiring more users to collude to discover the topology mitigates the risk in practice. A limitation of this architecture is that it provides less flexibility and control to end users compared to the FlowVisor-based architecture, as some switches are abstracted away and become part of virtual links, making them uncontrollable by users. As the number of abstracted switches increases, user views lose properties such as datacenter consistency or geographic consistency. Entire portions of the infrastructure might be seen as part of the same network, while actual hosts might be distributed across data centers or geographic regions.

ADVisor is useful in providing virtual links to mask some physical switches on a given path between two physical switches. However, it would be more useful to have a mechanism that not only masks physical switches but also adds virtual switches to the views. This would enable cloud providers to enrich the network view of the users while ensuring that the underlying topology is not fully exposed. We believe that ADVisor can be extended to provide such functionality, and we leave this extension for future work.

### Proof of Concept

To demonstrate the feasibility and practicality of the FlowVisor-based architecture, we constructed a prototype implementation. The flowspace of each controller was specified using FlowVisor's policy language. A topology matching Figure 2 was emulated in Mininet. All switches were directed to the port where FlowVisor was listening. Network slices were created for each user (UserA and UserB) through FlowVisor. Controllers for each user were configured to ensure that packets only travel over the specified switches. This was achieved using a dictionary with keys composed of data path IDs (DPID), the physical port where the packet arrived (in_port), and the source MAC address of the packet (packet.src). Based on this key, the dictionary gives the value of the output port to which the packet needs to be sent. This application was implemented as a Python module for the POX controller. We verified connectivity among the hosts in one slice and isolation among hosts of different slices using ICMP 'ping' messages.

### Discussion

The instantiation examples above clearly demonstrate the need to balance two requirements: the degree to which users get control over their network flows and the security concerns of the cloud provider. These two ends map to the two ends of the virtualization spectrum: at one end, there is little or no virtualization, and switch resources in the network are directly sliced; at the other end, the topology visible to cloud users is fully decoupled from the underlying infrastructure (as in Amazon EC2).

The tradeoffs are different in these two approaches. While the virtualized view of the network provides users with an easy-to-use interface, it makes it harder for them to efficiently and precisely schedule their flows. This approach is widely adopted by current cloud providers because it ensures they do not give out any details about the underlying topology, maintaining a competitive edge and preventing users from inferring information about the network. On the other hand, providing users with transparency over the underlying network means that users must manage network failures and reroute traffic when needed. This approach may expose too much information about the cloud infrastructure and raise security concerns.

There is a clear need to achieve a balance between these two approaches. A completely virtualized network deprives users of flexibility and control but is considered more secure from the provider's viewpoint. A fully non-virtualized view, while giving more control to users, may overwhelm them with configuration tasks and may not be very secure. Metrics to quantitatively evaluate network control delegation approaches would be beneficial. We plan to develop these metrics and conduct a more detailed comparative evaluation of the two proposed architectures in future work.

### Related Work

We have already covered the role of ADVisor and FlowVisor in Section II. Both act as hypervisors for the network, allowing multiple controllers to control the same network. ADVisor improves over FlowVisor by providing virtualization of nodes along a path connecting two machines of a cloud user. Quantum [9] is an API developed for the OpenStack project, allowing cloud providers to create a network for each tenant using different plugins. Quantum differs from our work as it only provides methods for implementing the network and has not been used to provide cloud users control over the physical network.

FlowN [10] proposes a fully virtualized solution for each tenant, contrasting with our goals of providing cloud users control over some switches while preventing cloud cartography attacks. RouteFlow [11] aims to provide routing algorithms to SDN users but cannot be used as a standalone tool to provide different levels of abstractions to cloud users.

Recently, Self-Service Cloud (SSC) [12] computing was proposed to increase flexibility for users to deploy services without relying on the cloud provider, reducing the access the cloud provider has to client VMs. While this work focuses on user control over VMs, we focus on user control over network flows.

CloudWatcher [13] uses OpenFlow to provide security monitoring services for cloud networks, ensuring that network packets are always inspected by pre-installed security devices. In contrast, our framework uses SDN to delegate configuration and monitoring capabilities to end users, providing some level of network abstractions without compromising the security of the infrastructure.

### Conclusion

In this paper, we explored the feasibility of providing cloud users more control over their network using abstractions provided by OpenFlow and software-defined networks while balancing the concerns of cloud providers over giving users direct access to the underlying physical infrastructure. We proposed an SDN-based framework to enable delegation of some network control to end users and discussed various properties of the network views provided to users. We explored two architectures that can be used to expose the network to cloud users and provided a brief overview of their pros and cons. We also presented a prototype implementation of one architecture.

For future work, we plan to further explore network control delegation to users, characterize different properties of network views, and study their security implications for the cloud provider. We also intend to investigate the interference effect of network control delegation and look at techniques to optimize bandwidth sharing among users in multi-tenant clouds.

### Acknowledgment

This work has been partially supported by the Air Force Research Laboratory and the Air Force Office of Scientific Research under agreement number FA8750-11-2-0084.

### References

[1] N. Leavitt, “Is cloud computing really ready for prime time?” Computer, vol. 42, no. 1, pp. 15–20, Jan. 2009.
[2] M. Al-Fares, S. Radhakrishnan, B. Raghavan, N. Huang, and A. Vahdat, “Hedera: Dynamic flow scheduling for data center networks,” in Proceedings of the 7th USENIX conference on Networked systems design and implementation, 2010, pp. 19–19.
[3] S. Shin, P. Porras, V. Yegneswaran, M. Fong, G. Gu, and M. Tyson, “Fresco: Modular composable security services for software-defined networks,” in To appear in the ISOC Network and Distributed System Security Symposium, 2013.
[4] N. McKeown, T. Anderson, H. Balakrishnan, G. Parulkar, L. Peterson, J. Rexford, S. Shenker, and J. Turner, “OpenFlow: enabling innovation in campus networks,” ACM SIGCOMM Computer Communication Review, vol. 38, no. 2, pp. 69–74, 2008.
[5] R. Sherwood, G. Gibb, K. Yap, G. Appenzeller, M. Casado, N. McKeown, and G. Parulkar, “FlowVisor: A network virtualization layer,” OpenFlow Switch Consortium, Tech. Rep, 2009.
[6] E. Salvadori, R. Corin, A. Broglio, and M. Gerola, “Generalizing virtual network topologies in OpenFlow-based networks,” in Global Telecommunications Conference (GLOBECOM 2011), 2011 IEEE, pp. 1–6.
[7] U. Hölzle, “OpenFlow @ Google,” Open Networking Summit, April 2012.
[8] N. Handigol, B. Heller, V. Jeyakumar, B. Lantz, and N. McKeown, “Reproducible network experiments using container-based emulation,” in Proceedings of the 8th international conference on Emerging networking experiments and technologies, ACM, 2012, pp. 253–264.
[9] “OpenStack Quantum,” http://wiki.openstack.org/Quantum.
[10] “Scalable network virtualization in software-defined networks,” http://www.cs.princeton.edu/~jrex/papers/ieeeinternet12.pdf.