0 block
1st block
111 . . . 111
000 . . . 000
2nd block
111 . . . 111
111 . . . 111
000 . . . 000
. . .
nth block
111 . . . 111
111 . . . 111
111 . . . 111
0
0
0
. . .
1
000 . . . 000
Each block spans ˜O(n2) identical columns. For such a randomly generated matrix, a coalition S
that does not include the i’th user cannot distinguish columns that come from the (i − 1)’st and
the i’th blocks of the matrix, as these columns are identical in the submatrix CS. The tracing
algorithm takes advantage of this observation. The tracing algorithm Trace(C, w) outputs the ﬁrst
i such that
[wj] −
Avg
j in block i
Avg
j in block i − 1
[wj] ≥ 1
n
,
where Avgj∈T f (j) denotes the average of f (j) over j in set T . For a feasible codeword w, such an
index i is guaranteed to exist since Avgj in block 0[wj] = 0 and Avgj in block n[wj] = 1. The correct-
ness of the tracing algorithm follows from the following claim, which ensures that the probability
we falsely accuse a user outside the coalition S is negligible.
Claim 5.19. For a given coalition S and pirate P, a randomly generated C ← Gen(1n) and w ←
P(CS), with probability greater than 1 − negl(n), for all i (cid:54)∈ S we have:
[wj] −
Avg
j in block i
Avg
j in block i − 1
[wj]  ˜Ω(
√
lower bound to nearly match Theorem 2.7 (up to a factor of O(
log log d)):
Theorem 5.21 (ﬁngerprinting lower bound for attribute means [98]). The following holds for
every d ∈ N, ε ∈ (0, 1), δ ∈ (2−d, 1/n1.1). Suppose M : ({0, 1}d)n → [0, 1]d is an (ε, δ)-diﬀerentially
private mechanism that with high probability answers every attribute mean query in Qmeans(d) with
error at most α. Then
(cid:32)
(cid:40)(cid:112)d log(1/δ)
(cid:41)(cid:33)
α ≥ Ω
min
, 1
.
εn
d/n) when d  0, and δ = o(1/n), characterize (to within “small” approximation factors) the smallest
achievable error by (ε, δ)-diﬀerentially private mechanisms M : Xn → RQ.
A potentially easier task, advocated by Beimel et al. [10], is to characterize the “sample com-
plexity,” as we did for pure diﬀerential privacy in Theorem 5.15:
Open Problem 5.25. For an arbitrary family Q = {q : X → {0, 1}} of counting queries, and ε > 0,
and δ = o(1/n), characterize (to within “small” approximation factors) the sample complexity (i.e.
smallest value of n) needed by (ε, δ)-diﬀerentially private mechanisms M : Xn → RQ to answer all
the queries in Q to within an arbitrarily small constant error α > 0.
We note that there is a partial converse to the connections between ﬁngerprinting codes and
diﬀerential privacy [21]; that is, if answering a set Q of counting queries is impossible with diﬀerential
privacy for a given set of parameters (α, n, ε, δ), this implies a weak form of a ﬁngerprinting code
that is deﬁned with respect to the query family Q and the given parameters. It would be very
interesting to tighten this relationship; this would be one approach to Open Problems 5.24 and
5.25.
Open Problem 5.26. Identify a variant of ﬁngerprinting codes whose existence is equivalent to
the impossibility of answering a family Q accurately with diﬀerential privacy (up to some loss in
parameters).
6 Computational Lower Bounds
Now we turn to computational lower bounds, giving evidence that some tasks that are information-
theoretically possible with diﬀerential privacy are nevertheless computationally intractable. Specif-
ically, recall that both the SmallDB and Private Multiplicative Weights algorithms of Section 4 can
accurately answer (many) more than n2 counting queries over data universe X = {0, 1}d with dif-
ferential privacy, provided that n is large enough compared to d (e.g. n ≥ d2), but use computation
time exponential in d. Below we will see evidence that this exponential computation is necessary
in the worst case.
50
6.1 Traitor-tracing Lower Bounds
Our ﬁrst hardness results will be based on traitor-tracing schemes, which were introduced by Chor
et al. [28] as a cryptographic tool for preventing piracy of digital content, like ﬁngerprinting codes.
Their beneﬁt over ﬁngerprinting codes is that they allow for distributing an unbounded amount of
content over a broadcast channel (after a set-up phase where private keys are sent to the users).
The price is having computational rather than information-theoretic security. The notion of traitor-
tracing schemes predated the notion of ﬁngerprinting codes, and their application to lower bounds
for diﬀerential privacy also came ﬁrst, in Dwork et al. [40].
To motivate the deﬁnition of traitor-tracing schemes, imagine a video-streaming company that
distributes software or hardware that is capable of decoding their (encrypted) streaming signal.
Each customer gets his own decryption program that has a unique decryption key, so that copying
can be detected. However, we are also concerned that S customers might collude to create (and
sell) unauthorized pirate decryption programs. They can build their pirate program using the
decryption keys found in their own decryption program in an arbitrary way, so we may not be able
to explicitly read oﬀ any of the keys from the pirate program. The goal of the traitor-tracing scheme
is to be able to identify at least one of the colluding customers who contributed his decryption key.
We can formalize this set-up as follows.
Deﬁnition 6.1. A traitor-tracing scheme consists of four algorithms (Gen, Enc, Dec, Trace) as fol-
lows:
1. The (randomized) key generation algorithm Gen(1d, 1n) takes as input 1d, 1n, where d is a