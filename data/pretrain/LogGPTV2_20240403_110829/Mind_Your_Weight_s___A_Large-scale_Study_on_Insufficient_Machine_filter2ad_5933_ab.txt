pipeline uses such information to generate the statistics on the
use of ML libraries (§4.4).
4.3 Accuracy Evaluation of ModelXRay
Accuracy of Identifying ML Apps: To establish the ground
truth for this evaluation, we chose the 219 non-ML apps labeled
by [61] as the true negatives, and we manually selected and ver-
iﬁed 219 random ML apps as the true positives. We evaluated
ModelXRay on this set of 438 apps. It achieved a false negative
rate of 6.8% (missed 30 ML apps) and a false positive rate of 0%
(zero non-ML apps is classiﬁed as ML apps). We checked the
30 missed ML apps, and found out that they are using unpopu-
lar ML Frameworks whose keywords are not in the dictionary.
We found two ML apps that ModelXRay correctly detected
but are missed by [61], one using ULSFaceTracker, which is
1958    30th USENIX Security Symposium
USENIX Association
AppAssetFilesAppLibrariesTensorﬂowCaffe....tﬂite.model...Model Sufﬁxocrmodel...Magic WordsModel File AnalyzerML Framework DictionarySuspectedModel FilesFile Size FilterML LibrariesModel FilesEncryptedModel FilesML Library AnalyzerFile Sufﬁx FilterML Library FilterAny ML Libraries ?EntropyAnalysisAndroidAPKFileML AppProﬁlean unpopular ML framework and the other using TensorFlow.
To further evaluate the false positive rate, we run Mod-
elXRay on our entire set of 46,753 apps and randomly sampled
100 apps labeled by ModelXRay as ML apps (50 apps from
Google Play and 50 apps from Chinese app market). We
then manually checked these 100 apps and found 3 apps
that are not ML apps (false positive rate of 3%). The manual
check was done by examining the library’s exposed symbols
and functions. This relatively low false positive rate shows
ModelXRay’s high accuracy in detecting ML apps for our
large-scale study.
Accuracy of Identifying Models: We randomly sampled
100 model ﬁles identiﬁed by ModelXRay from Chinese app
markets and Google Play, respectively, and manually veriﬁed
the results. ModelXRay achieved a true positive rate of 91%
and 97%, respectively.
In order to evaluate how widely apps conform to model
standard naming conventions, we manually checked 100 ML
apps from both Google Play and Chinese app market and
found 24 apps that do not follow any clear naming conventions.
Some use ".tﬂ" and ".lite" instead of the normal ".tﬂite" for
TensorFlow Lite models. Some use "3_class_model" without
a sufﬁx. Some have meaningful but not standard sufﬁxes
such as ".rpnmodel",".traineddata". Other have very generic
sufﬁxes such as ".bin", ".dat", and ".bundle". This observation
shows that ﬁle sufﬁx matching alone can miss a lot of model
ﬁles. Table 1 shows the top 5 popular model ﬁle sufﬁxes used
in different app markets. Many of these popular sufﬁxes are
not standard. ModelXRay’s model detection does not solely
depend on model ﬁle names.
Table 1: Popular model sufﬁx among different app markets
360 Mobile
Assistant
Num.Of.Cases
.bin
.model
.rpnmodel
.binary
.dat
1860
1540
257
212
201
Google
Play
.bin
.model
.pb
.tﬂite
.traineddata
Num.Of.Cases
318
175
93
83
46
Accuracy of Identifying Encrypted Models: To evaluate
whether entropy is a good indicator of encryption, we
sampled 40 models ﬁles from 4 popular encodings: ascii
text, protobuffer, ﬂatbuffer, and encrypted format (10 for
each category). As shown in Figure 3, the entropies of
encrypted model ﬁles are all close to 8. The other encodings’s
entropies are signiﬁcantly lower than 8. Figure 4 shows the
entropy distribution of all model ﬁles collected from 360 App
Assistant app market. It shows that the typical entropy range
of unencrypted model ﬁles is between 3.5 and 7.5.
4.4 Findings and Insights
We now present the results from our analysis as well as our
ﬁndings and insights, which provide answers to the question
Figure 3: Model File Entropy of 4 Popular Encodings
Figure 4: Model File Entropy Distribution of 360 App Market
“Q1: How widely is model protection used in apps?”. We start
with the popularity and diversity of on-device ML among our
collected apps, which echo the importance of model security
and protection. We then compare model protection used in
various apps. Especially, we draw observations on how model
protection varies across different app markets and different
ML frameworks. We also report our ﬁndings about the shared
encrypted models used in different apps. In addition, we mea-
sured the adoption of GPU acceleration in ML apps and com-
pared the use of remote models and on-device models to further
reveal the trends of on-device model inference in mobile apps.
Popularity and Diversity of ML Apps: In total, we are able
to collect 46,753 Android apps from Google Play, Tencent
My App and 360 Mobile Assistant stores. Using ModelXRay,
we identify 1,468 apps that use on-device ML and have ML
models deployed on devices, which accounts for 3.14% of our
entire app collection.
We also measure the popularity of ML apps for each
category, as apps from certain categories may be more likely
to use on-device ML than others. We used the app category
information from the three app markets. Table 2 shows the
per-category numbers of total apps and ML apps (i.e., apps
using on-device ML). Our ﬁndings are summarized as follows:
On-device ML is gaining popularity in all categories. There
are more than 50 ML apps in each of the categories, which
suggests the widespread interests among app developers in
using on-device ML. Among all the categories, “Business",
USENIX Association
30th USENIX Security Symposium    1959
10 Model File Samples of Different EncodingsModel File Entropy0246812345678910flatbufferprotobufferasciiencrypted4575 Model Files Collected from 360 App MarketModel File Entropy 02468“Image" and “News" are the top three that see most ML apps.
This observation conﬁrms the diversity of apps that make
heavy use of on-device ML. It also highlights that a wide range
of apps need to protect their ML models and attackers have
a wide selection of targets.
More apps from Chinese markets are embracing on-device
ML. This is reﬂected from both the percentage and the absolute
number of ML apps: Google Play has 178 (1.40%), Tencent
My App has 159 (7.25%), and 360 Mobile Assistant has 1,131
(3.55%).
As we can see from the above ﬁndings, Chinese app markets
show a signiﬁcant higher on-device machine learning adoption
rate and unique property of per-category popularity, making
it a non-negligible dataset for studying on-device machine
learning model protection.
Table 2: The number of apps collected across markets.
Google
Play
Tencent
My App
All ML
2
404
96
0
36
349
4
263
23
438
5
183
15
1,715
389
3
6
123
5
317
79
8,434
12,711
178
All ML
2
99
102
5
23
158
14
206
17
141
16
112
16
193
116
7
21
76
3
115
35
874
2,192
159
360
Mobile
Assistant
All
2,450
2,450
4,900
2,450
2,450
2,450
2,450
2,450
2,450
2,450
4,900
31,850
ML
296
180
156
83
79
84
53
74
55
42
29
1,131
Total
All
2,953
2,648
5,407
2,919
3,029
2,745
4,358
2,955
2,649
2,882
14,208
46,753
ML
300
185
215
101
119
105
84
84
82
50
143
1,468
Category
Business
News
Images
Map
Social
Shopping
Life
Education
Finance
Health
Other
Total
Note: In 360 Mobile Assistant, the number of unique apps is 31,591 (smaller
than 32,850) because some apps are multi-categorized. Image category
contains 4,900 apps because we merged image and photo related apps.
We measure the diversity of ML apps in terms of ML
frameworks and functionalities. We show the top-10 most
common functionalities and their distribution across different
ML frameworks in Table 3.
On-device ML offers highly diverse functionalities. Almost
all common ML functionalities are now offered in the on-
device fashion, including OCR, face tracking, hand detection,
speech recognition, handwriting recognition, ID card recogni-
tion, and bank card recognition, liveness detection, face recog-
nition, iris recognition and so on. This high diversity means
that, from the model theft perspective, attackers can easily ﬁnd
targets to steal ML models for any common functionalities.
Long tail in the distribution of ML frameworks used in
apps. Besides the well-known frameworks such as TensorFlow,
Caffe2/PyTorch, and Parrots, many other ML frameworks are
used for on-device ML, despite their relatively low market
share. For instance, as shown in Table 3, Tencent NCNN [25],
Xiaomi Mace [9], Apache MXNet [5], and ULS from Util-
ity Asset Store [30] are used by a fraction of the apps that
we collected. Each of them tends to cover only a few ML
functionalities. In addition, there could be other unpopular ML
frameworks that our analysis may have missed. This long tail in
the distribution of ML frameworks poses a challenge to model
protection because frameworks use different model formats,
model loading/parsing routines, and model inference pipelines.
Models Downloaded at Runtime: Mobile apps can always
update on-device models as part of the app package update,
or update models independently by downloading the models
at runtime. After investigating a few open ML platforms
including Android’s Firebase and Apple’s Core ML, we found
that they support downloading models at runtime [4, 6]. Other
open-sourced ML platforms like Paddle-Lite [21], NCNN [26]
and Mace [32], do not explicitly support downloading models
at runtime. Developers who use their SDKs can implement this
feature easily if they need it. Some proprietary ML SDKs, like
SenseTime, Face++, which are not open-sourced, do not leave
enough information for us to tell whether they implement this
feature or not.
To measure how many ML apps that download models at
runtime, we can use static analysis or dynamic analysis. For
dynamic analysis, we can run each app, monitor the down-
loaded ﬁles, and check whether these ﬁles are ML models or
not. It would require installing and running tens of thousands
of apps, as well as triggering the model downloading process,
which is not practical. For static analysis, we can reverse
engineer each app and analyze whether it implements this
feature or not. However, this feature can be implemented in
a few lines of code without exporting any symbols and the app
packages are always obfuscated, making it hard to analyze.
We took an indirect approach. We measure the number
of apps that contain on-device ML libraries but not any ML
models. These apps have to download the models at runtime
to use the ML function. We found 109 such apps, 64 from the
Chinese app markets and 45 from the US app markets.
Model Protection Across App Stores: Figure 5 gives the
per-app-market statistics on ML model protection and reuse.
Figure 5a shows the per-market numbers of protected apps
(i.e., apps using protected/encrypted models) and unprotected
apps (i.e., apps using unprotected models).
Overall, only 59% of ML apps protect their models. The
rest of the apps (602 in total) simply include the models in
plaintext, which can be easily extracted from the app packages
or installation directories. This result is alarming and suggests
that a large number of app developers are unaware of model
theft risks and fail to protect their models. It also shows that,
for 41% of the ML apps, stealing their models is as easy as
downloading and decompressing their app packages. We urge
stakeholders and security researchers to raise their awareness
and understanding of model thefts, which is a goal of this work.
Percentages of protected models vary across app markets.
When looking closer at each app market, it is obvious to see
that Google Play has the lowest percentage of ML apps using
protected models (26%) whereas 360 Mobile Assistant has
the highest (66%) and Tencent My App follows closely (59%).
A similar conclusion can be drawn on the unique models
1960    30th USENIX Security Symposium
USENIX Association
Table 3: Number of apps using different ML Frameworks with different functionalities.
*Caffe2/PyTorch
(Facebook)
Functionality
OCR(Optical Character Recognition)
Face Tracking
Speech Recognition
Hand Detection
Handwriting Recognition
Liveness Detection
Face Recognition
Iris Recognition
ID Card Recognition
Bank Card Recognition
TensorFlow
(Google)
41
26
7
4
8
32
17
0
26