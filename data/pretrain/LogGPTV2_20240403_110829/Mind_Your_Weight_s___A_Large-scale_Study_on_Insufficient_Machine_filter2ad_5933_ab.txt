### 4.3 Accuracy Evaluation of ModelXRay

#### 4.3.1 Accuracy in Identifying ML Apps
To establish the ground truth for this evaluation, we selected 219 non-ML apps labeled by [61] as true negatives and manually verified 219 random ML apps as true positives. We evaluated ModelXRay on this set of 438 apps. The results showed a false negative rate of 6.8% (missing 30 ML apps) and a false positive rate of 0% (no non-ML apps were misclassified as ML apps). Upon further investigation, we found that the 30 missed ML apps were using unpopular ML frameworks whose keywords were not included in our dictionary. Additionally, we identified two ML apps that ModelXRay correctly detected but were missed by [61], one using ULSFaceTracker and the other using TensorFlow.

To further evaluate the false positive rate, we ran ModelXRay on our entire dataset of 46,753 apps and randomly sampled 100 apps labeled as ML apps (50 from Google Play and 50 from Chinese app markets). Manual verification revealed that 3 out of these 100 apps were not actually ML apps, resulting in a false positive rate of 3%. This relatively low false positive rate demonstrates the high accuracy of ModelXRay in detecting ML apps at scale.

#### 4.3.2 Accuracy in Identifying Models
We randomly sampled 100 model files identified by ModelXRay from both Chinese app markets and Google Play, and manually verified the results. ModelXRay achieved a true positive rate of 91% and 97%, respectively.

To assess how widely apps conform to standard naming conventions for models, we manually checked 100 ML apps from both Google Play and Chinese app markets. We found that 24 apps did not follow any clear naming conventions. Some used ".tfl" and ".lite" instead of the standard ".tflite" for TensorFlow Lite models, while others used names like "3_class_model" without a suffix. Some had meaningful but non-standard suffixes such as ".rpnmodel" and ".traineddata," and others used very generic suffixes like ".bin," ".dat," and ".bundle." This observation highlights that relying solely on file suffix matching can miss many model files. Table 1 shows the top 5 popular model file suffixes used in different app markets, many of which are non-standard. ModelXRay's model detection does not depend solely on file names.

**Table 1: Popular Model File Suffixes Among Different App Markets**

| **App Market** | **Suffix** | **Number of Cases** |
|----------------|------------|---------------------|
| 360 Mobile Assistant | .bin | 1860 |
|                 | .model | 1540 |
|                 | .rpnmodel | 257 |
|                 | .binary | 212 |
|                 | .dat | 201 |
| Google Play | .bin | 318 |
|             | .model | 175 |
|             | .pb | 93 |
|             | .tflite | 83 |
|             | .traineddata | 46 |

#### 4.3.3 Accuracy in Identifying Encrypted Models
To evaluate whether entropy is a good indicator of encryption, we sampled 40 model files from four popular encodings: ASCII text, Protobuf, Flatbuffer, and encrypted format (10 for each category). As shown in Figure 3, the entropies of encrypted model files were all close to 8, while the entropies of other encodings were significantly lower. Figure 4 illustrates the entropy distribution of all model files collected from the 360 App Assistant app market, showing that the typical entropy range for unencrypted model files is between 3.5 and 7.5.

### 4.4 Findings and Insights

#### 4.4.1 Popularity and Diversity of ML Apps
We analyzed the popularity and diversity of on-device ML among our collected apps, which underscores the importance of model security and protection. We identified 1,468 apps (3.14% of our total collection) that use on-device ML and have ML models deployed on devices. 

**Table 2: Number of Apps Collected Across Markets**

| **Category** | **Google Play** | **Tencent My App** | **360 Mobile Assistant** | **Total** |
|--------------|-----------------|--------------------|--------------------------|-----------|
| Business     | 178             | 159                | 1,131                    | 1,468     |
| News         | 96              | 349                | 389                      | 834       |
| Images       | 263             | 23                 | 156                      | 442       |
| Map          | 438             | 183                | 83                       | 704       |
| Social       | 1,715           | 15                 | 79                       | 1,809     |
| Shopping     | 12,711          | 12,711             | 8,434                    | 33,856    |
| Life         | 1,715           | 15                 | 79                       | 1,809     |
| Education    | 12,711          | 12,711             | 8,434                    | 33,856    |
| Finance      | 1,715           | 15                 | 79                       | 1,809     |
| Health       | 12,711          | 12,711             | 8,434                    | 33,856    |
| Other        | 14,208          | 14,208             | 31,850                   | 60,266    |
| Total        | 46,753          | 46,753             | 46,753                   | 139,259   |

We also measured the popularity of ML apps across different categories. Our findings indicate that on-device ML is gaining popularity in all categories, with more than 50 ML apps in each category. The top three categories with the most ML apps are "Business," "Images," and "News." This diversity suggests that a wide range of apps need to protect their ML models, making them attractive targets for attackers.

More apps from Chinese markets are embracing on-device ML, as reflected by both the percentage and the absolute number of ML apps: Google Play has 178 (1.40%), Tencent My App has 159 (7.25%), and 360 Mobile Assistant has 1,131 (3.55%). This higher adoption rate in Chinese app markets makes them a significant dataset for studying on-device ML model protection.

#### 4.4.2 Diversity of ML Frameworks and Functionalities
We measured the diversity of ML apps in terms of ML frameworks and functionalities. Table 3 shows the top-10 most common functionalities and their distribution across different ML frameworks.

**Table 3: Number of Apps Using Different ML Frameworks with Different Functionalities**

| **Functionality** | **Caffe2/PyTorch (Facebook)** | **TensorFlow (Google)** |
|-------------------|------------------------------|-------------------------|
| OCR (Optical Character Recognition) | 41                          | 26                      |
| Face Tracking     | 26                           | 7                       |
| Speech Recognition | 7                            | 4                       |
| Hand Detection    | 4                            | 8                       |
| Handwriting Recognition | 8                            | 32                      |
| Liveness Detection | 17                           | 17                      |
| Face Recognition  | 0                            | 26                      |
| Iris Recognition  | 0                            | 0                       |
| ID Card Recognition | 26                           | 0                       |
| Bank Card Recognition | 0                            | 0                       |

On-device ML offers a wide range of functionalities, including OCR, face tracking, hand detection, speech recognition, handwriting recognition, ID card recognition, and bank card recognition. This diversity means that attackers can easily find targets for any common functionality. There is also a long tail in the distribution of ML frameworks used in apps, with many less popular frameworks being utilized. This poses a challenge for model protection, as different frameworks use different model formats, loading/parsing routines, and inference pipelines.

#### 4.4.3 Models Downloaded at Runtime
Mobile apps can update on-device models as part of the app package update or download models independently at runtime. After investigating several open ML platforms, including Android’s Firebase and Apple’s Core ML, we found that they support downloading models at runtime. Other open-sourced ML platforms like Paddle-Lite, NCNN, and Mace do not explicitly support this feature, but developers can implement it if needed. Some proprietary ML SDKs, such as SenseTime and Face++, do not provide enough information to determine whether they support this feature.

To measure the number of ML apps that download models at runtime, we took an indirect approach. We counted the number of apps that contain on-device ML libraries but no ML models, indicating that they must download models at runtime. We found 109 such apps, 64 from Chinese app markets and 45 from US app markets.

#### 4.4.4 Model Protection Across App Stores
Figure 5 provides per-app-market statistics on ML model protection and reuse. Overall, only 59% of ML apps protect their models, with the rest (602 in total) including the models in plaintext, which can be easily extracted. This result is alarming and suggests that many app developers are unaware of model theft risks. The percentages of protected models vary across app markets, with Google Play having the lowest (26%) and 360 Mobile Assistant having the highest (66%). A similar trend is observed for unique models.

**Figure 3: Model File Entropy of 4 Popular Encodings**
**Figure 4: Model File Entropy Distribution of 360 App Market**
**Figure 5: Per-App-Market Statistics on ML Model Protection and Reuse**