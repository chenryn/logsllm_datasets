USENIX Association  
25th USENIX Security Symposium  865
9
PubRK
Manufacturer
Certiﬁcate Authority
PrivRK
Signs
Processor
Endorsement 
Certiﬁcate
PubPK
Trusts
Manufacturer Root Key
Processor
Tamper-Resistant
Hardware
PrivPK
Signs
Monitor
Attestation 
Certiﬁcate
PubMK
Monitor
Measurement
Includes
Mailbox
Measurement
Data
Secure 
inter-enclave
communication
Processor
Key
Protected 
Monitor DRAM
PrivMK
Monitor
Key
Only revealed to
signing enclave
Signing
Enclave
PrivMK
Signs
Attestation 
Signature
Enclave
Hash of
Measurement
Data
Hash of
Key Exchange
Message 1
Key Exchange
Message 2
Veriﬁer
Figure 12: The certiﬁcate chain behind Sanctum’s soft-
ware attestation signatures
6.2.1 DRAM Regions
Figure 13 shows the DRAM region allocation state tran-
sition diagram. After the system boots up, all DRAM
regions are allocated to the OS, which can free up DRAM
regions so it can re-assign them to enclaves or to itself. A
DRAM region can only become free after it is blocked by
its owner, which can be the OS or an enclave. While a
DRAM region is blocked, any address translations map-
ping to it cause page faults, so no new TLB entries will be
created for that region. Before the OS frees the blocked
region, it must ﬂush all the cores’ TLBs, to remove any
stale entries for the region.
OWNED
block
DRAM
region
BLOCKED
free
DRAM
region
FREE
assign DRAM region
Figure 13: DRAM region allocation states and API calls
DRAM region metadata
Lock
Owner (enclave ID | OS 
| BLOCKED | FREE)
Owner value when
region blocked
Block clock value when
region blocked
Number of thread state 
pages in this region
mroot header
Attestation key set?
Encrypted private 
attestation key
Security Monitor data
System info
Block clock
DRAM region 1 
metadata
DRAM region 2 
metadata
⋮
Core 1 metadata
Core 2 metadata
⋮
mroot header 
System info
DRAM size
DRAM region mask
DRAM region shift
Cache address shift
CPU core metadata
Block clock value at 
last TLB ﬂush
Running enclave ID
Running enclave 
thread ID
Running enclave 
thread state area
Figure 14: Security monitor data structures
The monitor ensures that the OS performs TLB shoot-
downs, using a global block clock. When a region is
blocked, the block clock is incremented, and the current
block clock value is stored in the metadata associated with
the DRAM region (shown in Figure 3). When a core’s
TLB is ﬂushed, that core’s ﬂush time is set to the current
block clock value. When the OS asks the monitor to free a
blocked DRAM region, the monitor veriﬁes that no core’s
ﬂush time is lower than the block clock value stored in the
region’s metadata. As an optimization, freeing a region
owned by an enclave only requires TLB ﬂushes on the
cores running that enclave’s threads. No other core can
have TLB entries for the enclave’s memory.
The region blocking mechanism guarantees that when
a DRAM region is assigned to an enclave or the OS, no
stale TLB mappings associated with the DRAM region
exist. The monitor uses the MMU extensions described
in § 5.2 and § 5.3 to ensure that once a DRAM region
is assigned, no software other than the region’s owner
may create TLB entries pointing inside the DRAM region.
Together, these mechanisms guarantee that the DRAM
regions allocated to an enclave cannot be accessed by the
operating system or by another enclave.
6.2.2 Metadata Regions
Since the security monitor sits between the OS and en-
clave, and its APIs can be invoked by both sides, it is
an easy target for timing attacks. We prevent these at-
tacks with a straightforward policy that states the security
monitor is never allowed to access enclave data, and is
not allowed to make memory accesses that depend on
the attestation key material. The rest of the data handled
by the monitor is derived from the OS’ actions, so it is
already known to the OS.
A rather obvious consequence of the policy above is
that after the security monitor boots the OS, it cannot
perform any cryptographic operations that use keys. For
example, the security monitor cannot compute an attesta-
tion signature directly, and defers that operation to a sign-
866  25th USENIX Security Symposium 
USENIX Association
10
non-
existent
create
enclave
enter
enclave
init
enclave
INITIALIZED
load page,
PTE,thread
LOADING
delete enclave
Figure 15: Enclave states and enclave management API
calls
ing enclave (§ 6.1.2). While it is possible to implement
some cryptographic primitives without performing data-
dependent accesses, the security and correctness proofs
behind these implementations are non-trivial. For this
reason, Sanctum avoids depending on any such imple-
mentation.
A more subtle aspect of the access policy outlined
above is that the metadata structures that the security mon-
itor uses to operate enclaves cannot be stored in DRAM
regions owned by enclaves, because that would give the
OS an indirect method of accessing the LLC sets that
map to enclave’s DRAM regions, which could facilitate a
cache timing attack.
For this reason, the security monitor requires the OS to
set aside at least one DRAM region for enclave metadata
before it can create enclaves. The OS has the ability to
free up the metadata DRAM region, and regain the LLC
sets associated with it, if it predicts that the computer’s
workload will not involve enclaves.
Each DRAM region that holds enclave metadata is
managed independently from the other regions, at page
granularity. The ﬁrst few pages of each region contain
a page map that tracks the enclave that tracks the usage
of each metadata page, speciﬁcally the enclave that it is
assigned to, and the data structure that it holds.
Each metadata region is like an EPC region in SGX,
with the exception that our metadata regions only hold
special pages, like Sanctum’s equivalent of SGX’s Secure
Enclave Control Structure (SECS) and the Thread Control
Structure (TCS). These structures will be described in the
following sections.
The data structures used to store Sanctum’s metadata
can span multiple pages. When the OS allocates such a
structure in a metadata region, it must point the monitor to
a sequence of free pages that belong to the same DRAM
region. All the pages needed to represent the structure are
allocated and released in one API call.
6.2.3 Enclave Lifecycle
The lifecycle of a Sanctum enclave is very similar to that
of its SGX counterparts, as shown in Figure 15.
The OS creates an enclave by issuing a create enclave
call that creates the enclave metadata structure, which is
Sanctum’s equivalent of the SECS. The enclave metadata
structure contains an array of mailboxes whose size is es-
tablished at enclave creation time, so the number of pages
required by the structure varies from enclave to enclave.
§ 6.2.5 describes the contents and use of mailboxes.
The create enclave API call initializes the enclave meta-
data ﬁelds shown in Figure 3, and places the enclave in
the LOADING state. While the enclave is in this state,
the OS sets up the enclave’s initial state via monitor calls
that assign DRAM regions to the enclave, create hardware
threads and page table entries, and copy code and data
into the enclave. The OS then issues a monitor call to
transition the enclave to the INITIALIZED state, which
ﬁnalizes its measurement hash. The application hosting
the enclave is now free to run enclave threads.
Sanctum stores a measurement hash for each enclave
in its metadata area, and updates the measurement to ac-
count for every operation performed on an enclave in the
LOADING state. The policy described in § 6.2.2 does
not apply to the secure hash operations used to update
enclave’s measurement, because all the data used to com-
pute the hash is already known to the OS.
Enclave metadata is
stored in a metadata re-
gion (§ 6.2.2), so it can only be accessed by the security
monitor. Therefore, the metadata area can safely store
public information with integrity requirements, such as
the enclave’s measurement hash.
While an OS loads an enclave, it is free to map the
enclave’s pages, but the monitor maintains its page ta-
bles ensuring all entries point to non-overlapping pages
in DRAM owned by the enclave. Once an enclave is
initialized, it can inspect its own page tables and abort if
the OS created undesirable mappings. Simple enclaves
do not require speciﬁc mappings. Complex enclaves are
expected to communicate their desired mappings to the
OS via out-of-band metadata not covered by this work.
Our monitor ensures that page tables do not overlap
by storing the last mapped page’s physical address in
the enclave’s metadata. To simplify the monitor, a new
mapping is allowed if its physical address is greater than
that of the last, constraining the OS to map an enclave’s
DRAM pages in monotonically increasing order.
6.2.4 Enclave Code Execution
Sanctum closely follows the threading model of SGX
enclaves. Each CPU core that executes enclave code
uses a thread metadata structure, which is our equivalent
of SGX’s TCS combined with SGX’s State Save Area
(SSA). Thread metadata structures are stored in a DRAM
region dedicated to enclave metadata in order to prevent
a malicious OS from mounting timing attacks against
an enclave by causing AEXes on its threads. Figure 16
shows the lifecycle of a thread metadata structure.
The OS turns a sequence of free pages in a metadata
USENIX Association  
25th USENIX Security Symposium  867
11
assign
thread
free
thread
FREE
ASSIGNED
load
thread
accept
thread
release
thread
INITIALIZED
resume
thread
RUNNING
exit
enclave
enter
enclave
AEX
Figure 16: Enclave thread metadata structure states and
thread-related API calls
region into an uninitialized thread structure via an allocate
thread monitor call. During enclave loading, the OS uses
a load thread monitor call to initialize the thread structure
with data that contributes to the enclave’s measurement.
After an enclave is initialized, it can use an accept thread
monitor call to initialize its thread structure.
The application hosting an enclave starts executing en-
clave code by issuing an enclave enter API call, which
must specify an initialized thread structure. The monitor
honors this call by conﬁguring Sanctum’s hardware exten-
sions to allow access to the enclave’s memory, and then
by loading the program counter and stack pointer registers
from the thread’s metadata structure. The enclave’s code
can return control to the hosting application voluntarily,
by issuing an enclave exit API call, which restores the
application’s PC and SP from the thread state area and
sets the API call’s return value to ok.
When performing an AEX, the security monitor atomi-
cally tests-and-sets the AEX state valid ﬂag in the current
thread’s metadata. If the ﬂag is clear, the monitor stores
the core’s execution state in the thread state’s AEX area.
Otherwise, the enclave thread was resuming from an AEX,
so the monitor does not change the AEX area. When the
host application re-enters the enclave, it will resume from
the previous AEX. This reasoning avoids the complexity
of SGX’s state stack.
If an interrupt occurs while the enclave code is ex-
ecuting, the security monitor’s exception handler per-
forms an AEX, which sets the API call’s return value to
async exit, and invokes the standard interrupt handling
code. After the OS handles the interrupt, the enclave’s
host application resumes execution, and re-executes the
enter enclave API call. The enclave’s thread initialization
code examines the saved thread state, and seeing that the
thread has undergone an AEX, issues a resume thread API
call. The security monitor restores the enclave’s registers
from the thread state area, and clears the AEX ﬂag.
accept
message
empty
accept message
send message
read message
full
Figure 17: Mailbox states and security monitor API calls
related to inter-enclave communication
because it relies on key derivation and MAC algorithms,
and our timing attack avoidance policy (§ 6.2.2) states
that the security monitor is not allowed to perform cryp-
tographic operations that use keys.
Each enclave’s metadata area contains an array of mail-
boxes, whose size is speciﬁed at enclave creation time,
and covered by the enclave’s measurement. Each mailbox
goes through the lifecycle shown in Figure 17.
An enclave that wishes to receive a message in a mail-
box, such as the signing enclave, declares its intent by
performing an accept message monitor call. The API
call is used to specify the mailbox that will receive the
message, and the identity of the enclave that is expected
to send the message.
The sending enclave, which is usually the enclave wish-
ing to be authenticated, performs a send message call
that speciﬁes the identity of the receiving enclave, and
a mailbox within that enclave. The monitor only deliv-
ers messages to mailboxes that expect them. At enclave
initialization, the expected sender for all mailboxes is an
invalid value (all zeros), so the enclave will not receive
messages until it calls accept message.
When the receiving enclave is notiﬁed via an out-of-
band mechanism that it has received a message, it issues
a read message call to the monitor, which moves the
message from the mailbox into the enclave’s memory. If
the API call succeeds, the receiving enclave is assured
that the message was sent by the enclave whose identity
was speciﬁed in the accept message call.
Enclave mailboxes are stored in metadata re-
gions (§ 6.2.2), which cannot be accessed by any software
other than the security monitor. This guarantees the pri-
vacy, integrity, and freshness of the messages sent via the
mailbox system.
Our mailbox design has the downside that both the
sending and receiving enclave need to be alive in DRAM
in order to communicate. By comparison, SGX’s local
attestation can be done asynchronously. In return, mail-
boxes do not require any cryptographic operations, and
have a much simpler correctness argument.