SQL is (ostensibly) a declarative query language
The query specifies the properties the result set must satisfy,
not the procedure the DBMS must follow to produce the result
set
For a typical SQL query, there are many equivalent query plans
scan types: Seq scan, index scan, bitmap index scan
join order: Inner joins are commutative: reordered freely
join types: Sort-merge join, hash join, nested loops
aggregation: Hashed aggregation, aggregation by sorting
predicates: Predicate push down, evaluation order
rewrites: Subqueries and set operations → joins,
outer joins → inner joins, function inlining, ...
Two Distinct Subproblems
1 Enumerate all the possible plans for a given query
2 Estimate the cost of a given query plan
In practice, too slow → do both steps at the same time
Tasks Of The Query Optimizer
Basic Optimizer Task
Of the many ways in which we could evaluate a query,
which would be the cheapest to execute?
Tasks Of The Query Optimizer
Basic Optimizer Task
Of the many ways in which we could evaluate a query,
which would be the cheapest to execute?
Two Distinct Subproblems
1 Enumerate all the possible plans for a given query
2 Estimate the cost of a given query plan
In practice, too slow → do both steps at the same time
2 Determine the interesting ways to access each base relation
Remember the cheapest estimated access path, plus the
cheapest path for each distinct sort order
3 Determine the interesting ways to join each pair of relations
4 ...
Stages in Query Optimization
The System R Algorithm
1 Rewrite the query to make it more amenable to optimization:
pull up subqueries, rewrite IN clauses, simplify constant
expressions, reduce outer joins, ...
3 Determine the interesting ways to join each pair of relations
4 ...
Stages in Query Optimization
The System R Algorithm
1 Rewrite the query to make it more amenable to optimization:
pull up subqueries, rewrite IN clauses, simplify constant
expressions, reduce outer joins, ...
2 Determine the interesting ways to access each base relation
Remember the cheapest estimated access path, plus the
cheapest path for each distinct sort order
4 ...
Stages in Query Optimization
The System R Algorithm
1 Rewrite the query to make it more amenable to optimization:
pull up subqueries, rewrite IN clauses, simplify constant
expressions, reduce outer joins, ...
2 Determine the interesting ways to access each base relation
Remember the cheapest estimated access path, plus the
cheapest path for each distinct sort order
3 Determine the interesting ways to join each pair of relations
Stages in Query Optimization
The System R Algorithm
1 Rewrite the query to make it more amenable to optimization:
pull up subqueries, rewrite IN clauses, simplify constant
expressions, reduce outer joins, ...
2 Determine the interesting ways to access each base relation
Remember the cheapest estimated access path, plus the
cheapest path for each distinct sort order
3 Determine the interesting ways to join each pair of relations
4 ...
Files → Blocks
Each file is divided into blocks of BLCKSZ bytes each
8192 by default; compile-time constant
Blocks consist of items, such as heap tuples (in tables), or
index entries (in indexes), along with metadata
Tuple versions uniquely identified by triple (r,p,i): relation
OID, block number, offset within block; known as “ctid”
Storage Management
Tables → Files
Tables and indexes are stored in normal operating-system files
Each table/index divided into “segments” of at most 1GB
Tablespaces just control the filesystem location of segments
Storage Management
Tables → Files
Tables and indexes are stored in normal operating-system files
Each table/index divided into “segments” of at most 1GB
Tablespaces just control the filesystem location of segments
Files → Blocks
Each file is divided into blocks of BLCKSZ bytes each
8192 by default; compile-time constant
Blocks consist of items, such as heap tuples (in tables), or
index entries (in indexes), along with metadata
Tuple versions uniquely identified by triple (r,p,i): relation
OID, block number, offset within block; known as “ctid”
The buffer manager implements a hash table in shared
memory, mapping page identifiers → buffers
If the requested page is in shared buffers, return it
Otherwise, ask the kernel for it and stash it in
shared buffers
If no free buffers, replace an existing one (which one?)
The kernel typically does its own I/O caching as well
Keep a pin on the page, to ensure it isn’t replaced while in use
The Buffer Manager
Almost all I/O is not done directly: to access a page, a
process asks the buffer manager for it
The Buffer Manager
Almost all I/O is not done directly: to access a page, a
process asks the buffer manager for it
The buffer manager implements a hash table in shared
memory, mapping page identifiers → buffers
If the requested page is in shared buffers, return it
Otherwise, ask the kernel for it and stash it in
shared buffers
If no free buffers, replace an existing one (which one?)
The kernel typically does its own I/O caching as well
Keep a pin on the page, to ensure it isn’t replaced while in use
Row-level Locks
Writers don’t block readers: MVCC
Writers must block writers: implemented via row-level locks
Implemented by marking the row itself (on disk)
Also used for SELECT FOR UPDATE, FOR SHARE
Concurrency Control
Table-level Locks
Also known as “lmgr locks”, “heavyweight locks”
Protect entire tables against concurrent DDL operations
Many different lock modes; matrix for determining if two
locks conflict
Automatic deadlock detection and resolution
Concurrency Control
Table-level Locks
Also known as “lmgr locks”, “heavyweight locks”
Protect entire tables against concurrent DDL operations
Many different lock modes; matrix for determining if two
locks conflict
Automatic deadlock detection and resolution
Row-level Locks
Writers don’t block readers: MVCC
Writers must block writers: implemented via row-level locks
Implemented by marking the row itself (on disk)
Also used for SELECT FOR UPDATE, FOR SHARE
Spinlocks
LWLocks are implemented on top of spinlocks, which are in
turn a thin layer on top of an atomic test-and-set (TAS)
primitive provided by the platform
If an LWLock is contended, waiting is done via blocking on a
SysV semaphore; spinlocks just busy wait, then micro-sleep
Concurrency Control: Low-Level Locks
LWLocks (“Latches”)
Protect shared data structures against concurrent access
Two lock modes: shared and exclusive (reader/writer)
No deadlock detection: should only be held for short durations
Concurrency Control: Low-Level Locks
LWLocks (“Latches”)
Protect shared data structures against concurrent access
Two lock modes: shared and exclusive (reader/writer)
No deadlock detection: should only be held for short durations
Spinlocks
LWLocks are implemented on top of spinlocks, which are in
turn a thin layer on top of an atomic test-and-set (TAS)
primitive provided by the platform
If an LWLock is contended, waiting is done via blocking on a
SysV semaphore; spinlocks just busy wait, then micro-sleep
Organization of Source Tree
doc/: documentation, FAQs
src/
bin/: client programs (psql, pg dump, ...)
include/: headers
catalog/: system catalog definitions
interfaces/: libpq, ecpg
pl/: procedural languages (PL/PgSQL, PL/Perl, ...)
test/regress/: SQL regression tests
Makefiles
Makefile per directory (recursive make)
src/makefiles has platform-specific Makefiles
src/Makefile.global.in is the top-level Makefile
Backend Source Tree
Content of src/backend
access/: index implementations, heap access manager,
transaction management, write-ahead log
commands/: implementation of DDL commands
executor/: executor logic, implementation of executor nodes
libpq/: implementation of backend side of FE/BE protocol
optimizer/: query planner
parser/: lexer, parser, analysis phase
Backend Source Tree, cont.
Content of src/backend, cont.
postmaster/: postmaster, stats daemon, AV daemon, ...
rewrite/: application of query rewrite rules
storage/: shmem, locks, bufmgr, storage management, ...
tcop/: “traffic cop”, FE/BE query loop, dispatching from
protocol commands → implementation
utils/:
adt/: builtin data types, functions, operators
cache/: caches for system catalog lookups, query plans
hash/: in-memory hash tables
mmgr/: memory management
sort/: external sorting, TupleStore
Outline
1 Prerequisites
Why Should You Hack On PostgreSQL?
What Skills Will You Need?
What Tools Should You Use?
2 The Architecture of PostgreSQL
System Architecture
Components of the Backend
3 Common Code Conventions
Memory Management
Error Handling
4 Community Processes
5 Sample Patch
6 Conclusion
The Postgres Object System: Nodes
Postgres uses a simple object system with support for single
inheritance. The root of the class hierarchy is Node:
typedef struct typedef struct typedef struct
{ { {
NodeTag type; NodeTag type; Parent parent;
} Node; int a_field; int b_field;
} Parent; } Child;
This relies on a C trick: you can treat a Child * like a
Parent * since their initial fields are the same
Unfortunately, this can require a lot of ugly casting
The first field of any Node is a NodeTag, which can be used
to determine a Node’s specific type at runtime
Run-time type testing via the IsA() macro
Test if two nodes are equal: equal()
Deep-copy a node: copyObject()
Serialise a node to text: nodeToString()
Deserialise a node from text: stringToNode()
Nodes, Cont.
Basic Node Utility Functions
Create a new Node: makeNode()
Test if two nodes are equal: equal()
Deep-copy a node: copyObject()
Serialise a node to text: nodeToString()
Deserialise a node from text: stringToNode()
Nodes, Cont.
Basic Node Utility Functions
Create a new Node: makeNode()
Run-time type testing via the IsA() macro
Deep-copy a node: copyObject()
Serialise a node to text: nodeToString()
Deserialise a node from text: stringToNode()
Nodes, Cont.
Basic Node Utility Functions
Create a new Node: makeNode()
Run-time type testing via the IsA() macro
Test if two nodes are equal: equal()
Serialise a node to text: nodeToString()
Deserialise a node from text: stringToNode()
Nodes, Cont.
Basic Node Utility Functions
Create a new Node: makeNode()
Run-time type testing via the IsA() macro
Test if two nodes are equal: equal()
Deep-copy a node: copyObject()
Deserialise a node from text: stringToNode()
Nodes, Cont.
Basic Node Utility Functions
Create a new Node: makeNode()
Run-time type testing via the IsA() macro
Test if two nodes are equal: equal()
Deep-copy a node: copyObject()
Serialise a node to text: nodeToString()
Nodes, Cont.
Basic Node Utility Functions
Create a new Node: makeNode()
Run-time type testing via the IsA() macro
Test if two nodes are equal: equal()
Deep-copy a node: copyObject()
Serialise a node to text: nodeToString()
Deserialise a node from text: stringToNode()
Nodes: Hints
When you modify a node or add a new node, remember to
update
nodes/equalfuncs.c
nodes/copyfuncs.c
You may have to update nodes/outfuncs.c and
nodes/readfuncs.c if your Node is to be
serialised/deserialised
Grep for references to the node’s type to make sure you don’t
forget to update anything
When adding a new node, look at how similar nodes are
treated
Memory Management
Postgres uses hierarchical, region-based memory management,
and it absolutely rocks
backend/util/mmgr
Similar concept to Tridge’s talloc(), “arenas”, ...
All memory allocations are made in a memory context
Default context of allocation: CurrentMemoryContext
palloc() allocates in CMC
MemoryContextAlloc() allocates in a given context
Memory Management, cont.
Allocations can be freed individually via pfree()
When a memory context is reset or deleted, all allocations in
the context are released
Resetting contexts is both faster and less error-prone than
releasing individual allocations
Contexts are arranged in a tree; deleting/resetting a context
deletes/resets its child contexts
Memory Management Conventions
You should sometimes pfree() your allocations
If the context of allocation is known to be short-lived, don’t
bother with pfree()
If the code might be invoked in an arbitrary memory context
(e.g. utility functions), you should pfree()
You can’t pfree() an arbitrary Node (no “deep free”)
The exact rules are a bit hazy :-(
Memory Leaks
Be aware of the memory allocation assumptions made by
functions you call
Memory leaks, per se, are rare in the backend
All memory is released eventually
A “leak” occurs when memory is allocated in a too-long-lived
memory context: e.g. allocating some per-tuple resource in a
per-txn context
MemoryContextStats() useful for locating the guilty context
(Almost) never use malloc() in the backend
Error Handling
Most errors reported by ereport() or elog()
ereport() is for user-visible errors, and allows more fields to
be specified (SQLSTATE, detail, hint, etc.)
Implemented via longjmp; conceptually similar to exceptions
in other languages
elog(ERROR) walks back up the stack to the closest error
handling block; that block can either handle the error or
re-throw it
The top-level error handler aborts the current transaction and
resets the transaction’s memory context
Releases all resources held by the transaction, including files,
locks, memory, and buffer pins
Guidelines For Error Handling
Custom error handlers can be defined via PG TRY()
Think about error handling!
Never ignore the return values of system calls
Should your function return an error code, or ereport() on
failure?
Probably ereport() to save callers the trouble of checking for
failure
Unless the caller can provide a better (more descriptive) error
message, or might not consider the failure to be an actual error
Use assertions (Assert) liberally to detect programming
mistakes, but never errors the user might encounter
Outline
1 Prerequisites
Why Should You Hack On PostgreSQL?
What Skills Will You Need?
What Tools Should You Use?
2 The Architecture of PostgreSQL
System Architecture
Components of the Backend
3 Common Code Conventions
Memory Management
Error Handling
4 Community Processes
5 Sample Patch
6 Conclusion
Mailing Lists
The vast majority of communication occurs on mailing lists
pgsql-hackers is the main list
pgsql-patches and pgsql-committers can be useful to
learn from
Written communication skills are important
Good developers are often good writers
Some developers are on IRC; internals questions are welcome
irc.freenode.net, #postgresql
Your First Patch
Step 1: Research and preparation
Is your new feature actually useful? Does it just scratch your
itch, or is it of general value?
Does it need to be implemented in the backend, or can it live
in pgfoundry, contrib/, or elsewhere?
Does the SQL standard define similar or equivalent
functionality?
What about Oracle, DB2, ...?
Has someone suggested this idea in the past?
Search the archives and TODO list
Most ideas are bad
Don’t take the TODO list as gospel
Sending A Proposal
Step 2: Send a proposal for your feature to pgsql-hackers
Patches that appear without prior discussion risk wasting your
time
Discuss your proposed syntax and behaviour
Consider corner cases, and how the feature will relate to other
parts of PostgreSQL (consistency is good)
Will any system catalog changes be required?
Backward-compatibility?
Try to reach a consensus with -hackers on how the feature
ought to behave
Implementation
Step 3: Begin implementing the feature
A general strategy is to look at how similar parts of the
system function
Don’t copy and paste (IMHO)
Common source of errors
Instead, read through similar sections of code to try to
understand how they work, and the APIs they are using
Implement (just) what you need, refactoring the existed APIs if
required
Ask for advice as necessary (-hackers or IRC)
Write down the issues you encounter as you write the code,
include the list when you submit the patch
Consider posting work-in-progress versions of the patch
Testing, Documentation
Step 4: Update tools
For example, if you’ve modified DDL syntax, update psql’s
tab completion
Add pg dump support if necessary
Step 5: Testing
Make sure the existing regression tests don’t fail
No compiler warnings
Add new regression tests for the new feature
Step 6: Update documentation
Writing good documentation is more important than getting
the DocBook details completely correct
Add new index entries, if appropriate
Check documentation changes visually in a browser