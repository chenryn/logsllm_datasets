4. Let Av ⊆ A, v ∈ V, be the adversaries with respect to
5. Let C : 2A×(V ∪E) → [0, 1] indicate the probability of
a pattern of compromise among the nodes and links:
for c ∈ 2A×(V ∪E), if (a, x) ∈ c, then adversary a has
compromised x. C satisﬁesP
c∈2A×(V ∪E) C(c) = 1.
1Note that we say ‘user’ to refer to the human user and
to the client software that creates connections on the user’s
behalf or sometimes to the computer on which that software
runs. This overlap should not cause problems at the level
at which we model systems. It should be clear from context
which usage is intended if the distinction is important.
and links. The Cv satisfyP
6. Let Cv : 2A×(V ∪E) → [0, 1], v ∈ V, indicate the belief
node v has in a pattern of compromise among nodes
7. Let Iv ∈ {0, 1}∗, v ∈ V, be the inputs each node uses
c∈2A×(V ∪E) Cv(c) = 1.
when running the protocol.
A protocol is run by the nodes over the network links in
order to reach some collective state. For purposes of privacy,
the relevant property of the protocol is the set of models that
are consistent with the observations of an adversary during
the protocol’s execution. An adversary makes observations
at the nodes and links he has compromised. A probabilistic
protocol yields a distribution on the sets of possible models.
Investigating privacy in this model then becomes analyz-
ing how likely the adversary is to be in a position to make
good inferences about the model. Privacy may be quanti-
ﬁed, for example, by the number of bits of node input learned
for certain by the adversary. Or it could be that there are
reasonable prior distributions that we can allow the adver-
sary to put on the models, and privacy loss is measured by
the mutual information between the observations and the
models.
The model includes multiple adversaries. This is an im-
portant choice for modeling trust in anonymous communi-
cation, because a diverse set of users with varying goals and
beliefs is necessary for the set to provide good anonymity.
Part of that diversity occurs in the adversaries of a user.
That means that we cannot require that each user relies on
other network entities in the same way. Allowing users to
use the network in diﬀerent ways while still considering over-
all communication anonymity from their combined actions
is a central issue for protocol design.
The adversaries themselves operate by controlling parts
of the network. This models both that an adversary might
provide some nodes and links to the network and that he
might compromise some that are provided by others. We
could restrict the adversary to controlling nodes alone, be-
cause an adversary that controls a link could be simulated
for purposes of analysis by splitting any link and connecting
the halves with a node controlled by the adversary. How-
ever, given that several attacks on anonymous communica-
tion protocols involve observing the network connections in
particular [21, 37], it is useful to formally allow both types
of compromise. Also, we allow diﬀerent adversaries to com-
promise the same node at the same time.
Trust itself appears in our model in the distributions Cv.
Trust in a node or link is given with respect to a set of ad-
versaries A ⊆ A. The trust of user u in, say, network link
e ∈ E, with respect to A can be understood as a distribution
over the ways 2A in which the adversaries in A have com-
promised e. If, for example, the probability in Cu is high
that some member of A has compromised e, then we can say
that u has low trust in e. Ideally, from the user’s perspective,
the user’s beliefs would be true, that is, Cu would equal C.
Our model incorporates erroneous beliefs, though, because
a user’s beliefs aﬀect her actions and may hurt anonymity
when they diﬀer from the truth.
Our analysis will assume a population N ⊆ U of na¨ıve
users who think any router is as likely to be compromised
as any other. It is within this population of users that we
will hide the identity of a given user of the network. This
approach is equivalent to assuming that the adversary can
rule out all users other than u and the na¨ıve users as being
177the source of a connection. Let m = |R| be the number of
routers. Let n = |N| be the number of na¨ıve users.
The na¨ıve users share the same adversary, An = {aN}, n ∈
N . Other users each have their own adversary, Au = {au}.
No router or destination has any adversary. The set of ad-
versaries is A = aN + {au}u∈U\N .
The na¨ıve users n ∈ N hold the same beliefs about their
adversary aN . They believe each router is independently
and equally likely to be compromised: cn
We assume user u ∈ U believes that adversary a ∈ A com-
promises router r ∈ R independently with probability cu
a(r).
a (r) = 1−cu
The trust of u in r with respect to a is then τ u
a(r).
If clear from the context, we will drop the superscript u and
the subscript a. The use of probabilities to represent trust
reﬂects the uncertainty about the presence and power of
an adversary when coordinating among many diﬀerent par-
ties over large networks. This uncertainty is best modeled
directly, rather than giving the node too much power by as-
suming a known adversary or giving the adversary too much
power by analyzing the worst case.
aN = cN .
Furthermore, we assume that u believes with certainty
either that a observes all links from u to the routers and
destinations or that a observes none of them. That is, u
believes with probability either one or zero that {u, v} is
compromised for all v ∈ R ∪ D. Similarly, we assume that u
believes with certainty either that a observes all links from
a given destination d ∈ D to R and U (if these conﬂict on a
link in U × D, the user believes that link is compromised).
This models whether or not the user believes that he or his
destination uses a hostile ISP. It will also be taken to include
the case that the user visits a known hostile destination. If
an adversary observes all traﬃc to and from a given user, we
say that he observes the source links, and if he observes all
traﬃc to and from a destination, we say that he observes the
destination links. Our model can capture varying trust on
the links as well as the routers, and incorporating this would
better reﬂect reality; however, adding diverse link probabil-
ities would complicate the analysis below. So, we restrict
ourselves to analysis of varying router trust.
Finally, we assume that u does not believe a compro-
mises users, destinations, or links between routers. As noted
above, destination compromise is covered by an adversary
observing the destination links. Similarly, the case of ob-
served links between routers is subsumed by the adversary
compromising either of the onion routers on that link.
Each user has as input a sequence of destinations (d1, d2, . . .)
indicating connections over time. Routers and destinations
have no inputs.
Anonymous-Communication Privacy.
We assume that users make connections according to a
probabilistic process, and that the adversary uses it as a
prior distribution to break privacy. Speciﬁcally, we assume
that the source and destination of a given connection are
independent of connections at other times. We also as-
sume that the user and destination of a connection are in-
dependent. We acknowledge that this may not be true in
practice—users communicate with diﬀerent partners,
application-layer protocols such as HTTP have temporal
patterns of connection establishment, and so on. This as-
sumption simpliﬁes analysis, however, and isolates what the
adversary can learn by observing the path.
We assume that the adversary’s observations consist of
a sequence of active links, that is, links carrying messages.
We further assume that the adversary can determine (for
example, via a correlation attack) when two observations
correspond to the same connection.
We consider the privacy of the connections that each user
makes (the user inputs) to be the most signiﬁcant among the
components of the model. We thus design our protocols to
hide information about the connections, and analyze their
privacy in most detail. We perform two types of privacy
analysis on the connections. First, we consider the ability
of the adversary to infer existence, source, and destination
of a given connection, that is, to deanonymize the connec-
tion. Second, we consider the adversary’s knowledge of all
user connections over time, that is, his proﬁle of each user’s
activity.
Deanonymization. This kind of analysis is useful when
the privacy of a given connection is important, say, because
it is particularly sensitive. For this analysis, we assume that
the adversary has full knowledge of the model except for the
user inputs. The analysis uses the posterior probability of
deanonymizing the connection as a privacy metric. We want
the probability that the adversary correctly names both the
user and the destination of a given connection to be close to
what it would be if he had not observed the network.
The correlation attack allows the adversary to infer the
source and destination of a connection if he can observe
both ends. Therefore, if the adversary can observe traﬃc
from the user (either by compromising the entry router or
by observing the user’s connection to it) and can also ob-
serve traﬃc from the destination (either because he controls
the destination or the last router on the path or observes
the traﬃc between them) then the user has no anonymity.
Otherwise, the adversary must use the parts of the connec-
tion that he can observe and determine the probability of
each user being the source.
Proﬁling. This analysis is useful to understand the ad-
versary’s overall view of private inputs, which might be in-
dividually private but highly linked with one another. We
use the entropy of user connections as a privacy metric in
this case. Learning which connections are related helps the
adversary to determine the set of destinations visited by
some user. Such a proﬁle, taken as a whole, may itself help
identify the user if the adversary has background knowl-
edge about the user’s typical communication patterns. The
adversary might also try to link connections that have iden-
tifying information in their traﬃc with connections that do
not, thereby removing anonymity from those that do not and
adding proﬁling information to both kinds of connections.
4. TRUST IN PATH SELECTION
The addition of trust gives users the ability to select routers
that are not likely to be compromised by an adversary that
they care about. Speciﬁcally, depending on various param-
eters, users of an onion-routing network who choose paths
entirely out of highly trusted routers can sometimes min-
imize their risk of deanonymization via the correlation at-
tack [31]. However, if other users are concerned about adver-
saries with a diﬀerent trust distribution, using only highly
trusted routers would lead to diﬀerent users preferring dif-
ferent routers for their paths—and the choice of routers itself
may identify the user. For example, an adversary that con-
trols just the last router on a path observes the destination
178and the last two routers, and this information alone could
deanonymize the user’s connection.
By balancing between these eﬀects, we can avoid deanon-
ymization on a single connection. Users make multiple con-
nections over time, however. If their paths change with ev-
ery new connection, they run an increasing risk of selecting a
path that has many compromised routers. An obvious way
to avoid this problem is for each user to choose one ﬁxed
path to use for all of her connections.
While this strategy helps avoid deanonymization, choos-
ing a single, static path allows an adversary to more easily
link together connections from the same user. If the adver-
sary observes the same set of routers in the same positions in
two diﬀerent circuits, he knows it is likely that they originate
from the same user. Of particular concern is a malicious des-
tination, because it always observes the exit router, that is,
the last, static router. Combining static and random router
choices allows us to balance avoiding deanonymization with
avoiding proﬁling.
We analyze the impact of the above issues on path dea-
nonymization and use the results of our analysis to motivate
a path-selection algorithm. For the adversary types we an-
alyze, the only nontrivial case will be when the adversary
compromises destination links and some routers. Brieﬂy
stated, our algorithm for this case is to choose a static
“downhill” path that picks each successive node from a pool
that increases in size because the acceptable lower bound on
node trust diminishes with each hop. Once the static path
reaches the trust bottom, so that the pool includes all nodes,
two dynamic hops are added to the end of the path. We do
not claim that this is an optimal strategy. It does demon-
strate how to use our model and analysis to easily do better
than choosing nodes ignoring trust or using only the most
trusted nodes. The details of how our analysis motivates
the algorithm are set out below. Our analysis proceeds in
two stages: ﬁrst, we consider minimizing the chance of de-
anonymization of just a single connection, then second, we
consider adapting to multiple connections to maintain good
anonymity while also preventing proﬁling. We will be con-
sidering routing for a given user u ∈ U , and we will describe
it with respect to the one adversary of u.
4.1 Path anonymity for a single connection
Suppose a user makes just one connection. She chooses a
path for that connection based on her trust values for the
routers. The adversary can learn about routers on that path
by compromising them, compromising an adjacent router,
or by observing source or destination links. He can link
together routers as belonging to that circuit using the cor-
relation attack. The adversary’s ability to determine both
source and destination of the circuit, and thereby deanon-
ymize it, depends on these observations. We would like to
choose paths to minimize the probability that he can do so.
The best way to select paths depends on the location and
kind of adversary we are facing. There are four possibilities
depending on whether the source links are compromised or
not and whether the destination links are compromised or
not. The cases that the source and destination links are ei-
ther both unobserved or both observed are trivial. The user
in these cases can do no better than directly connecting to
the destination. If the adversary just observes source links,
then we must try to hide the destination. If the adversary
Source links observed
just observes destination links (or is the destination), then
we must try to hide the user.
4.1.1
Suppose that the adversary observes the source links. Then
the user is anonymous if and only if the adversary doesn’t ob-
serve the destination. Therefore, we can maximize anonymity
by choosing a one-hop path that maximizes τ (r1), which is
achieved by selecting a most-trusted router.
4.1.2 Destination links observed
Now suppose that the adversary observes the destination
links.
In this case, the adversary is able to learn about
the ﬁnal router, any router he has compromised, and any
router adjacent to a compromised router. The adversary
can determine that these routers are on the same path and
what position they are in by using the correlation attack.
Assuming that the adversary knows the user’s trust values
and the algorithm that the user uses to choose paths, then
the adversary can use his observation to infer a distribution
on the source of the circuit. We would like to minimize the
probability that he assigns to the correct user.
We analyze this probability for a given user u with respect