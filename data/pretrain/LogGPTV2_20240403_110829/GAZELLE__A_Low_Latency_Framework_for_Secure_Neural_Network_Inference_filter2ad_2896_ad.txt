general case where we have multiple input and output chan-
nels, a subset of which may ﬁt within a single ciphertext.
Padded SISO. As seen in section 2, same style
convolutions require that the input be zero-padded. As
such, in this approach, we start with a zero-padded
version of the input with ( fw − 1)/2 zeros on the left
and right edges and ( fh − 1)/2 zeros on the top and
bottom edges. We assume for now that this padded input
image is small enough to ﬁt within a single ciphertext
i.e. (wi + fw − 1) · (hi + fh − 1) ≤ n and is mapped to
the ciphertext slots in a raster scan fashion. We then
compute fw· fh rotations of the input and scale them by the
corresponding ﬁlter coefﬁcient as shown in Figure 8. Since
all the rotations are performed on a common input image,
they can beneﬁt from the hoisting optimization. Note that
similar to the na¨ıve matrix-vector product algorithm, the
values on the periphery of the output image leak partial
products and must be obscured by adding random values.
Packed SISO. While the above the technique com-
putes the correct 2D-convolution it ends up wasting
(wi + fw−1)· (hi + fh−1)−wi·hi slots in zero padding.
If either the input image is small or if the ﬁlter size is large,
this can amount to a signiﬁcant overhead. We resolve this
issue by using the ability of our PAHE scheme to multiply
different slots with different scalars when performing
SIMDScMult. As a result, we can pack the input tightly
and generate fw · fh rotations. We then multiply these
rotated ciphertexts with punctured plaintexts which have
zeros in the appropriate locations as shown in Figure 9.
Accumulating these products gives us a single ciphertext
that, as a bonus, contains the convolution result without
any leakage of partial information.
Finally, we note that the construction of the punctured
USENIX Association
27th USENIX Security Symposium    1661
we want to classify a batch of multiple images. In this
context, we can pack the same channel from multiple
classiﬁcations allowing us to use a simple constant ﬁlter.
This allows us to trade-off classiﬁcation latency for higher
throughput. Note however that similar to padded SISO
convolutions, this has two problems: (a) it results in lower
slot utilization compare to packed approaches, and (b) the
padding scheme reveals the size of the ﬁlter.
Now that we have seen how to compute a single 2D-
convolution we will look at the more general multi-channel
case.
Single Channel per Ciphertext. The straightforward
approach for handling the multi-channel case is to encrypt
the various channels into distinct ciphertexts. We can
then SISO convolve these ci-ciphertexts with each of
the co sets of ﬁlters to generate co output ciphertexts.
Note that although we need co · ci · fh · fw SIMDAdd and
SIMDScMult calls, just ci· fh· fw many Perm operations
on the input sufﬁce, since the rotated inputs can be reused
to generate each of the co outputs. Furthermore, each these
rotation can be hoisted and hence we require just ci many
PermDecomp calls and ci· fh· fw many PermAuto calls.
Channel Packing Similar to input-packed matrix-
vector products, the computation of multi-channel convo-
lutions can be further sped up by packing multiple channels
in a single ciphertext. We represent the number of channels
that ﬁt in a single ciphertext by cn. Channel packing allows
us to perform cn-SISO convolutions in parallel in a SIMD
fashion. We maximize this parallelism by using Packed
SISO convolutions which enable us to tightly pack the in-
put channels without the need for any additional padding.
For simplicity of presentation, we assume that both ci
and co are integral multiples of cn. Our high level goal is
to then start with ci/cn input ciphertexts and end up with
co/cn output ciphertexts where each of the input and output
ciphertexts contains cn distinct channels. We achieve this
in two steps: (a) convolve the input ciphertexts in a SISO
fashion to generate (co·ci)/cn intermediate ciphertexts that
contain all the co·ci-SISO convolutions and (b) accumulate
these intermediate ciphertexts into output ciphertexts.
Since none of the input ciphertexts repeat an input chan-
nel, none of the intermediate ciphertexts can contain SISO
convolutions corresponding to the same input channel. A
similar constraint on the output ciphertexts implies that
none of the intermediate ciphertexts contain SISO convo-
lutions corresponding to the same output. In particular, a
potential grouping of SISO convolutions that satisﬁes these
constraints is the diagonal grouping. More formally the kth
intermediate ciphertext in the diagonal grouping contains
the following ordered set of cn-SISO convolutions:
{ ((cid:98)k/ci(cid:99)·cn +l,
(cid:98)(k mod ci)/cn(cid:99)·cn +((k+l) mod cn))| l∈ [0,cn) }
where each tuple (xo, xi) represents the SISO convolution
Figure 9: Packed SISO Convolution. (Zeros in the punc-
tured plaintext shown in white.)
Table 3: Comparing SISO 2D-convolutions
Perm
fw fh−1
fw fh−1
Padded
Packed
(wi + fw−1)(hi + fh−1)
# slots
wihi
Figure 10: Decomposing a strided convolutions into simple
convolutions ( fw = fh =3 and sx =sy =2)
plaintexts does not depend on either the encrypted image
or the client key information and as such, the server can
precompute these values once for multiple clients. We
summarize these results in Table 3.
6.1 Strided Convolutions
We handle strided convolutions by decomposing the
strided convolution into a sum of simple convolutions
each of which can be handled as above. We illustrate this
case for fw = fh =3 and sx =sy =2 in Figure 10.
6.2 Low-noise Batched Convolutions
We make one ﬁnal remark on a potential application for
padded SISO convolutions. Padded SISO convolutions
are computed as a sum of rotated versions of the input
images multiplied by corresponding constants fx,y. The
coefﬁcient domain representation of these plaintext
is ηmult = fx,y·√
vectors is ( fx,y,0,...,0). As a result, the noise growth factor
n, consequently noise
growth depends only on the value of the ﬁlter coefﬁcients
and not on the size of the plaintext space p. The direct
use of this technique precludes the use of channel packing
since the ﬁlter coefﬁcients are channel dependent. One
potential application that can mitigate this issue is when
n as opposed to p·√
1662    27th USENIX Security Symposium
USENIX Association
Implementation and Micro-benchmarks
common PermDecomp each of the output rotations occur
on a distinct ciphertext and cannot beneﬁt from hoisting.
We summarize these numbers in Table 4. The choice
between the input and output rotation variants is an
interesting trade-off that is governed by the size of the
2D ﬁlter. This trade-off is illustrated in more detail with
concrete benchmarks in section 7. Finally, we remark
that similar to the matrix-vector product computation,
the convolution algorithms are also tweaked to work with
the half-rotation permutation group and use plaintext
windows to control the scalar multiplication noise growth.
7
Next we describe the implementation of the Gazelle
framework starting with the chosen cryptographic
primitives (7.1). We then describe our evaluation test-bed
(7.2) and ﬁnally conclude this section with detailed
micro-benchmarks (7.3) for all the operations to highlight
the individual contributions of the techniques described
in the previous sections.
7.1 Cryptographic Primitives
Gazelle needs two main cryptographic primitives for
neural network inference: a packed additive homomorphic
encryption (PAHE) scheme and a two-party secure
computation (2PC) scheme. Parameters for both schemes
are selected for a 128-bit security level. For the PAHE
scheme we instantiate the Brakerski-Fan-Vercauteren
(BFV) scheme [4, 14], with n = 2048, 20-bit plaintext
modulus, 60-bit ciphertext modulus and σ =4 according
to the analysis of Section 3.5.
For the 2PC framework, we use Yao’s Garbled
circuits [44]. The main reason for choosing Yao over
Boolean secret sharing schemes (such as the Goldreich-
Micali-Wigderson protocol [19] and its derivatives)
is that the constant number of rounds results in good
performance over long latency links. Our garbling scheme
is an extension of the one presented in JustGarble [3]
which we modify to also incorporate the Half-Gates
optimization [45]. We base our oblivious transfer (OT) im-
plementation on the classic Ishai-Kilian-Nissim-Petrank
(IKNP) [27] protocol from libOTe [33]. Since we use 2PC
for implementing the ReLU, MaxPool and FHE-2PC trans-
formation gadget, our circuit garbling phase only depends
on the neural network topology and is independent of the
client input. As such, we move it to the ofﬂine phase of the
computation while the OT Extension and circuit evaluation
is run during the online phase of the computation.
7.2 Evaluation Setup
All benchmarks were generated using c4.xlarge AWS in-
stances which provide a 4-threaded execution environment
(on an Intel Xeon E5-2666 v3 2.90GHz CPU) with 7.5GB
of system memory. Our experiments were conducted
using Ubuntu 16.04.2 LTS (GNU/Linux 4.4.0-1041-aws)
Figure 11: Diagonal Grouping for Intermediate Cipher-
texts (ci =co =8 and cn =4)
corresponding to the output channel xo and input channel
xi. Given these intermediate ciphertexts, one can generate
the output ciphertexts by simply accumulating the co/cn-
partitions of ci consecutive ciphertexts. We illustrate this
grouping and accumulation when ci =co =8 and cn =4 in
Figure 11. Note that this grouping is very similar to the di-
agonal style of computing matrix vector products, with sin-
gle slots now being replaced by entire SISO convolutions.
Since the second step is just a simple accumulation of
ciphertexts, the major computational complexity of the
convolution arise in the computation of the intermediate
ciphertexts.
If we partition the set of intermediate
ciphertexts into cn-sized rotation sets (shown in grey in
Figure 11), we see that each of the intermediate ciphertexts
is generated by different rotations of the same input. This
observation leads to two natural approaches to compute
these intermediate ciphertexts.
Input Rotations.
In the ﬁrst approach, we generate
cn rotations of every input ciphertext and then perform
Packed SISO convolutions on each of these rotations to
compute all the intermediate rotations required by co/cn
rotation sets. Since each of the SISO convolutions requires
fw · fh rotations, we require a total of (cn · fw · fh − 1)
rotations (excluding the trivial rotation by zero) for each
of the ci/cn inputs. Finally we remark that by using the
hoisting optimization we compute all these rotations by
performing just ci/cn PermDecomp operations.
Output Rotations. The second approach is based on
the realization that instead of generating (cn· fw· fh− 1)
input rotations, we can reuse ( fw· fh−1) rotations in each
rotation-set to generate cn convolutions and then simply
rotate (cn−1) of these to generate all the intermediate ci-
phertexts. This approach then reduces the number of input
rotations by factor of cn while requiring (cn−1) rotations
for each of the (ci · co)/c2
n rotation sets. Note that while
( fw· fh−1) input rotations per input ciphertext can share a
USENIX Association
27th USENIX Security Symposium    1663
Table 4: Comparing multi-channel 2D-convolutions
PermDecomp
One Channel per CT
Input Rotations
Output Rotations
(cid:16)
ci
ci
cn
1+ (cn−1)·co
cn
Perm
( fw fh−1)·ci
(cn fw fh−1)· ci
cn
fw fh−1+ (cn−1)·co
cn
#in ct
#out ct
(cid:17) ci
cn
ci
ci
cn
ci
cn
co
co
cn
co
cn
(cid:16)
(cid:17) ci
cn
Table 5: Fast Reduction for NTT and Inv. NTT
Table 7: Permutation Microbenchmarks
Operation
NTT (q)
Inv. NTT (q)
NTT (p)
Inv. NTT (p)
Fast Reduction
t (µs)
cyc/bﬂy
14.68
57
13.90
54
11.07
43
38
9.78
Naive Reduction
t (µs)
cyc/bﬂy
101.18
393
99.89
388
61.79
240
194
49.95
Speedup
6.9
7.2
5.6
5.1
# windows PermKeyGen Key Size PermAuto Noise
bits
29.3
19.3
14.8
kB
49.15
98.30
196.61
t (µs)
466
925
1849
t (µs)
35
57
100
3
6
12
Table 6: FHE Microbenchmarks
Naive Reduction
t (µs)
cyc/slot
1348.1
952
879.4
621
726.4
513
393
49.7
167.1
388
2568.7
1814
2463.9
1740
1595
2258.5
199.7
141
Fast Reduction
t (µs)
cyc/slot
328.5
232
263.4
186
177.0
125
5
8.1
14.7
10
659.9
466
379.5
268
231
327.1
49.6
35
Operation
KeyGen
Encrypt
Decrypt
SIMDAdd
SIMDScMult
PermKeyGen
Perm