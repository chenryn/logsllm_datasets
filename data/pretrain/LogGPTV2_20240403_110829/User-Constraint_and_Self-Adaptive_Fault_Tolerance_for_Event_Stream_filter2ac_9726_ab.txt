### Guarantees and Cost Model

#### A. Fault Tolerance Mechanisms

In the remainder of this paper, we will focus on the crash-stop failure model. However, some Byzantine failures can be transformed into crash-stop failures using techniques such as software-encoded processing [13]. StreamMine3G uses ZooKeeper [14] to store its cluster configuration reliably, which transparently handles network partitions through ZooKeeper's heartbeat mechanism, automatically removing unresponsive nodes from the cluster.

The downtime of an Event Stream Processing (ESP) system consists of two components: (i) the time it takes to detect a failure, and (ii) the time it takes to execute recovery actions, such as state recovery and event replay, until normal operation resumes. In our system, we rely on ZooKeeper’s failure detection mechanism, where the detection time can be configured through session timeouts and tick times (i.e., heartbeat intervals). For the remainder of the paper, we will use the term "recovery time" to refer only to the second component, excluding the detection time.

#### Components Contributing to Fault Tolerance

As shown in Figure 1, an operator in StreamMine3G is equipped with several mechanisms to ensure fault tolerance:

1. **Outgoing Event Queue**: An outgoing event queue (upstream buffer/in-memory log) logs events for replay in case of a downstream operator crash. Event replay is used if the user requires precise recovery where event loss is unacceptable. To prevent memory exhaustion, the queue is purged once the state of the downstream operator has been successfully included in a checkpoint. StreamMine3G uses an acknowledgment protocol in combination with the sweeping checkpoint algorithm described in [15].

2. **Incoming Event Queue**: Each operator instance has an incoming event queue that merges and orders events from different upstream operators to ensure consistent processing across replicas. Events are merged and ordered using application timestamps and a variant of the Bias algorithm [12]. The incoming queue also detects duplicates by maintaining a state timestamp vector, which tracks the last seen event's timestamp from each upstream operator partition. Events with timestamps smaller than the last registered one are filtered out.

#### State Management and Checkpoints

While the incoming and outgoing event queues ensure that events are neither lost nor processed twice, operators often accumulate state that must be protected. A common approach is to create checkpoints, where the state (which can be any data structure) is serialized in binary form and either written to stable storage or sent to a peer node for takeover in case of a system failure.

StreamMine3G employs six different fault tolerance approaches, as shown in Figure 2. Each subfigure (1-6) in Figure 2 depicts a small topology consisting of a non-replicated upstream operator, a replicated central operator, and a non-replicated downstream operator. The replicated operator instances are denoted as primary and secondary. The primary processes events continuously, while the secondary serves as a backup, activated or deactivated based on the chosen fault tolerance schema.

1. **Active Replication**: Both the primary and secondary replicas receive, process, and send out events. The downstream operator filters duplicates using the incoming event queue. This approach ensures near-zero recovery time but consumes twice the resources.

2. **Active Standby**: The secondary replica does not send its processing results to downstream operators, saving network resources and reducing duplicate filtering overhead. However, this increases the recovery time as the network links between the secondary and downstream operators must be established, and buffered events must be processed.

3. **Passive Standby Hot**: The secondary receives events but does not process them, saving computational and network resources. The state is periodically synchronized from the primary, and the secondary’s incoming queue is pruned. In case of a primary crash, the secondary establishes links with downstream operators and processes enqueued events before accepting new ones.

4. **Passive Standby Cold**: No events are sent to the secondary, saving additional network bandwidth. During recovery, network links must be established, and events buffered at upstream operators must be replayed.

5. **Deployed**: The secondary is deployed and ready to take over, but no state synchronization occurs. This approach balances resource consumption and recovery time.

6. **Passive Replication**: The state is stored on a peer node rather than a distributed file system, allowing for fast recovery. However, this comes at the cost of increased memory consumption. If memory resources are scarce, the state can be stored on disk instead.

By employing these fault tolerance mechanisms, StreamMine3G aims to satisfy user requirements while efficiently managing resources.