guarantees and cost model.
A. Fault Tolerance Mechanisms
In the remainder of this paper, we only consider the crash-
stop failure model. Nevertheless, some Byzantine failures can
be transformed into crash-stop failures using techniques such
as software encoded processing [13]. Since StreamMine3G
uses zookeeper [14] for storing its cluster conﬁguration in a
reliable fashion, network partitions are transparently handled
through zookeeper’s heartbeat mechanism where unresponsive
nodes are taken out of the cluster automatically.
The downtime of an ESP systems comprises of two com-
ponents: (i) The time it takes to detect the failure, and (ii)
the time it takes to execute compensation actions such as state
recovery and event replay until normal operation resumes. In
our system, we rely on zookeeper’s failure detection mecha-
nism where the detection time can be bounded through the
conﬁguration of the session timeouts and the tick time (i.e.,
heartbeat interval). In the remainder of the paper, we use
the term recovery time only for the second component, i.e.,
the time it takes to execute the recovery steps excluding the
detection time.
components that contribute to fault
Figure 1.
An operator in StreamMine3G is equipped with several
tolerance as shown in
First, an outgoing event queue 1(cid:2) (i.e., upstream buffer/in-
memory log) is used to log events for a replay in case of
a crash of a downstream operator. Of course, event replay
comes only into play if the user opted for a precise recovery
where event loss is unacceptable. In order to prevent mem-
ory exhaustion, the queue is purged whenever the state of
the downstream operator has been included in a checkpoint
successfully. StreamMine3G uses an acknowledgment protocol
in combination with the sweeping checkpoint algorithm as
described in [15].
Second, an incoming event queue 2(cid:2) installed at each
operator instance is used for merging and ordering events
465465
+ deploy 
- deploy 
Deployed 
- check pnt 
+ state sync 
- state sync 
+ check pnt 
Passive 
Replication 
Objective: 
Passive 
Standby Cold 
Transition between the states based on: 
• Evolution of state size 
• Event rate (queue size/replay time) 
• Resource utilization  
Goal: Satisfy user & save resources 
- receive 
+ receive 
Active 
Replication 
- send 
+ send 
Passive 
Standby Hot 
- process 
+ state sync 
+ process 
- state sync 
Active  
Standby 
Fig. 3: Fault tolerance schemes state transition wheel.
coming from different upstream operators to ensure a con-
sistent processing across replicas. Events are merged and
ordered using application timestamps and a variant of the Bias
algorithm as presented in [12].
In addition to the merging and ordering of events, the
incoming queue is also used to detect duplicates. Duplicate
detection is performed through a state timestamp vector as-
sociated with each queue which keeps track of the last seen
event’s timestamp from each of the upstream operator parti-
tions. Events with a timestamp smaller than the last registered
one are automatically ﬁltered and not passed to the operator.
Duplicates do naturally occur whenever an operator receives
events from a replicated upstream operator or during event
replay within an ongoing recovery.
While the incoming and outgoing event queues ensure that
neither events are lost nor processed twice, operators often
accumulate state which must be protected though appropriate
mechanisms as well. A well established approach is to make
checkpoints, where the state that can comprise potentially any
kind of data structure is ﬁrst serialized in binary form and
then either written to some stable storage 3(cid:2) or sent to a peer
node 4(cid:2) for a take over in case of a system failure.
StreamMine3G employs in total six different fault toler-
ance approaches the controller can choose from as shown in
Figure 2. In each of the subﬁgures ( 1(cid:2)- 6(cid:2)) in Figure 2, three
operators comprising a small topology are shown: An non-
replicated upstream (left) and downstream (right) operator and
a replicated one in the center of each subﬁgure. In order to
identify uniquely each instance of the replicated operator, we
denote them as primary and secondary. While the purpose
of the primary is to process events continuously, the secondary
will solely serve as a backup which is going to be switched
on or off depending on the chosen fault tolerance schema.
We will ﬁrst start with the fault tolerance approach which
guarantees the quickest recovery time, however, at the cost of
consuming twice of the resources. As depicted in subﬁgure 1(cid:2)
of Figure 2,
in active replication replication, an operator
partition is replicated where both replicas receive, process and
send out events to downstream operators. In order to prevent
processing events twice originating from the two replicas, the
1 
Active Replication 
2 
Active Standby 
Primary 
Node 
Primary 
Node 
Operator 
Queue 
Queue 
Operator 
Queue 
Queue 
Node 
Node 
Node 
Node 
Operator 
Queue 
Queue 
Recovery Time 
~ 0 sec 
Remove 
duplicates 
Node 
Operator 
Queue 
Queue 
Operator 
Queue 
Queue 
Operator 
Queue 
Queue 
Resource Consumption 
Recovery Time 
Node 
Operator 
Queue 
Queue 
Secondary 
U
P
C
y
r
o
m
e
M
k
r
o
w
t
e
N
- establish connection 
replay from upstream 
-
Secondary 
Operator 
Queue 
Queue 
Resource Consumption 
U
P
C
y
r
o
m
e
M
k
r
o
w
t
e
N
3 
Passive Standby Hot 
4 
Passive Standby Cold 
Primary 
Node 
Primary 
Node 
Operator 
Queue 
Queue 
Operator 
Queue 
Queue 
Node 
Node 
Node 
Node 
Operator 
Queue 
Queue 
state 
Operator 
Queue 
Queue 
Operator 
Queue 
Queue 
state 
Operator 
Queue 
Queue 
Recovery Time 
- establish connection 
-
replay from queue 
5 
“Deployed” 
Node 
Operator 
z z z 
Queue 
Queue 
Resource Consumption 
Recovery Time 
Node 
Operator 
z z z 
Queue 
Queue 
Resource Consumption 
Secondary 
y
r
o
m
e
M
t
e
N
U
P
C
- establish connections 
replay from upstream 
-
Secondary 
m
e
M
t
e
N
U
P
C
Primary 
Node 
Primary 
Node 
6 
Passive Replication 
Operator 
Queue 
Queue 
Operator 
Queue 
Queue 
Node 
Node 
Node 
Node 
Operator 
Queue 
Queue 
state 
Operator 
Queue 
Queue 
Operator 
Queue 
Queue 
Recovery Time 
- establish connections 
-
-
read checkpoint 
replay from upstream 
Node 
Operator 
z z z 
Queue 
Queue 
Resource Consumption 
Recovery Time 
Secondary 
U
P
C
m
e
M
t
e
N
- deploy operator 
- establish connections 
-
-
read checkpoint 
replay from upstream 
state 
Node 
X 
Operator 
Queue 
Queue 
Resource Consumption 
Secondary 
U
P
C
e
M
t
e
N
Fig. 2: Fault tolerance schemes and their impact with regards to resource consumption and recovery times.
downstream operator transparently ﬁlters duplicates using the
previously mentioned incoming event queue with its associated
state timestamp vector. The advantage of active replication is
that it can practically recover within zero seconds as a crash
of either of the two replicas will not affect the downstream
operator in any way.
A ﬁrst approach towards reducing the resource overhead of
active replication is depicted in subﬁgure 2(cid:2). In active standby,
the secondary replica does not send its processing results to
downstream operators. Although this saves network resources
and the overhead for ﬁltering duplicates, it increases the time
prior the system can resume normal operation compared to
active replication. During recovery, ﬁrst
the network links
between the secondary and the downstream operators must be
established, and second, events buffered in the in-memory log
of the secondary must be sent and processed at the downstream
operator which introduces additional latency.
Passive standby hot describes the state where the secondary
still receives events, however, does not perform any process-
ing saving computational resources in addition to network
bandwidth. Since the secondary still receives events which
are enqueued in the operator’s incoming event queue, state
can be safely recreated by simply reprocessing enqueued
events. However, this approach would lead soon to memory
466466
exhaustion and an increased recovery time. Hence, the state
from the primary is periodically included in checkpoints and
these are stored at the operator’s replica. We call this process
state synchronization as it updates the secondary’s state as
depicted in subﬁgure 3(cid:2). During state synchronization, the
state timestamp vector is updated so outdated events can be
pruned from the incoming event queue. In case the primary
crashes, the following steps must be executed before resuming
to normal operation: First, the links between the secondary
and its downstream operators must be established so that the
results the secondary produces will arrive at its interested
parties. Second, events enqueued in the incoming queue of the
secondary must be processed before accepting any new events
coming from upstream operators. Note that since a priority
queue is used here, events are always accepted, however, newer
events are transparently shifted to the end of the queue until
it is their turn.
in passive standby cold, no events are sent
Passive standby comes in two ﬂavors: While in passive
standby hot,
the secondary still receives events from the
upstream operators even though no events are processed, only
enqueued,
to
the secondary at all with the beneﬁt of additional network
bandwidth savings. However, during a recovery, network links
from upstream as well as downstream operators must be
established ﬁrst and a replay of events buffered at the upstream
operators performed as shown in subﬁgure 4(cid:2).
Keeping a copy of the operator’s state in a peer node
(i.e., secondary) rather than on a distributed ﬁle system comes
with the advantage of a fast recovery as it saves the time (i)
to load the checkpoint from disk and (ii) to de-serialize its
data structures. However, it comes with the price of memory
consumption which can be a problem if memory resources
are scarce and if applications accumulate a considerable large
amount of state over time. An approach to cope with this
problem is to store the state on disk rather than in memory