prediction of this model. The probability that the instruction pair
is swapped or not is defined as
mov[CLS][MASK]0x1[SEP]movrdxjz[SEP]ebxrbxInputPredictionmov[CLS]ebx0x1[SEP]movrdxrbx[SEP]InputPredictionIsContextSession 12A: Applications and Privacy of ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea32411 + exp(Θ(I1 ∥ I2)cls)
(5)
where Θcls is the first output of the transformer network in the
last layer. The Cross Entropy loss function is:
p( ˆy|I1, I2)
LDU P = −
(6)
p( ˆy|I1, I2) =
1
I ∈D
Figure 5: Def-Use Prediction (DUP)
We show an example in Figure 5. We still use the instruction
pair discussed in Figure 4, but here we swap the two instructions.
So the sequence is “[CLS] mov rdx rbx [SEP] mov ebx 0x1
[SEP]”. We feed it into PalmTree and use the first output vector to
predict whether this instruction pair remains unswapped or not. In
this case, it should be predicted as “false” (which means this pair is
swapped).
The loss function of PalmTree is the combination of three loss
functions:
L = LMLM + LCW P + LDU P
(7)
Instruction Representation. The transformer encoder pro-
3.4.5
duces a sequence of hidden states as output. There are multiple
ways to generate instruction embeddings from the output. For in-
stance, applying a max/mean pooling. We use mean pooling of
the hidden states of the second last layer to represent the whole
instruction. This design choice has the following considerations.
First, the transformer encoder encodes all the input information
into the hidden states. A pooling layer is a good way to utilize the
information encoded by transformer. Second, results in BERT [9]
also suggest that hidden states of previous layers before the last
layer have offer more generalizability than the last layer for some
downstream tasks. We evaluated different layer configurations and
reported the results in Section C.2.
3.4.6 Deployment of the model. There are two ways of deploy-
ing PalmTree for downstream applications: instruction embedding
generation, where the pre-trained parameters are frozen, and fine-
tuning, where the pre-trained parameters can be further adjusted.
In the first way (instruction embedding generation), PalmTree
is used as an off-the-shelf assembly language model to generate
high-quality instruction embeddings. Downstream applications
can directly use the generated embeddings in their models. Our
evaluation results show that PalmTree without fine-tuning can
still outperform existing instruction embedding models such as
word2vec and Asm2Vec. This scheme is also very useful when com-
puting resources are limited such as on a lower-end or embedded
devices. In this scenario, we can further improve the efficiency by
generating a static embedding lookup table in advance. This lookup
table contains the embeddings of most common instructions. A
trade-off should be made between the model accuracy and the avail-
able resources when choosing the lookup table size. A larger lookup
table will consume more space but can alleviate the OOV problem
(happens when the encountered instruction is not in the table) and
improve the accuracy.
In the second way (fine-tuning), PalmTree is fine-tuned and
trained together with the downstream model. This scheme will
usually provide extra benefits when enough computing resources
and training budget are available. There are several fine-tuning
strategies [33], e.g., two-stage fine-tuning, multi-task fine-tuning.
4 EVALUATION
Previous binary analysis studies usually evaluate their approaches
by designing specific experiments in an end-to-end manner, since
their instruction embeddings are only for individual tasks. In this
paper, we focus on evaluating different instruction embedding
schemes. To this end, we have designed and implemented an exten-
sive evaluation framework to evaluate PalmTree and the baseline
approaches. Evaluations can be classified into two categories: in-
trinsic evaluation and extrinsic evaluation. In the remainder of this
section, we first introduce our evaluation framework and experi-
mental configurations, then report and discuss the experimental
results.
4.1 Evaluation Methodology
Intrinsic Evaluation. In NLP domain, intrinsic evaluation refers
to the evaluations that compare the generated embeddings with
human assessments [2]. Hence, for each intrinsic metric, manu-
ally organized datasets are needed. This kind of dataset could be
collected either in laboratory on a limited number of examinees
or through crowd-sourcing [25] by using web platforms or offline
survey [2]. Unlike the evaluations in NLP domain, programming
languages including assembly language (instructions) do not neces-
sarily rely on human assessments. Instead, each opcode and operand
in instructions has clear semantic meanings, which can be extracted
from instruction reference manuals. Furthermore, debug informa-
tion generated by different compilers and compiler options can also
indicate whether two pieces of code are semantically equivalent.
More specifically, we design two intrinsic evaluations: instruction
outlier detection based on the knowledge of semantic meanings of
opcodes and operands from instruction manuals, and basic block
search by leveraging the debug information associated with source
code.
Extrinsic Evaluation. Extrinsic evaluation aims to evaluate the
quality of an embedding scheme along with a downstream machine
learning model in an end-to-end manner [2]. So if a downstream
model is more accurate when integrated with instruction embed-
ding scheme A than the one with scheme B, then A is considered
better than B. In this paper, we choose three different binary analy-
sis tasks for extrinsic evaluation, i.e., Gemini [40] for binary code
similarity detection, EKLAVYA [5] for function type signatures in-
ference, and DeepVSA [14] for value set analysis. We obtained the
original implementations of these downstream tasks for this evalu-
ation. All of the downstream applications are implemented based
on TensorFlow3. Therefore we choose the first way of deploying
PalmTree in extrinsic evaluations (see Section 3.4.6). We encoded all
3https://www.tensorflow.org/
[CLS]movrdxrbx[SEP]movebx0x1[SEP]InputPredictionIsSwappedSession 12A: Applications and Privacy of ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3242We classify instructions into 12 categories based on their opcode,
according to the x86 Assembly Language Reference Manual [29].
More details about this process can be found in Table 8 in the Appen-
dix. We prepared 50,000 instruction sets. Each set consists of four
instructions from the same opcode category and one instruction
from a different category.
Table 2: Intrinsic Evaluation Results, Avg. denotes the aver-
age of accuracy scores, and Stdev. denotes the standard devi-
ation
Model
Instruction2Vec
word2vec
Asm2Vec
PalmTree-M
PalmTree-MC
PalmTree
Avg.
0.863
0.269
0.865
0.855
0.870
0.871
Stdev.
0.0529
0.0863
0.0426
0.0333
0.0449
0.0440
opcode
outlier
operand
outlier
basicblock
sim search
AUC
0.871
0.842
0.894
0.910
0.913
0.922
Avg.
0.860
0.256
0.542
0.785
0.808
0.944
Stdev.
0.0363
0.0874
0.0238
0.0656
0.0435
0.0343
Similarly, we classify instructions based on their operands. Ta-
ble 9 in the Appendix provides details about this process. Essentially,
we classify operand lists, according to the number of operands as
well as the operand types. We created another 50,000 sets of instruc-
tions covering 10 categories, and each set contains four instructions
coming from the same category, and one from a different category.
the instructions in the corresponding training and testing datasets
and then fed the embeddings into downstream applications.
4.2 Experimental Setup
Baseline Schemes and PalmTree Configurations. We choose In-
struction2Vec, word2vec, and Asm2Vec as baseline schemes. For
fair comparison, we set the embedding dimension as 128 for each
model. We performed the same normalization method as PalmTree
on word2vec and Asm2Vec. We did not set any limitation on the
vocabulary size of Asm2Vec and word2vec. We implemented these
baseline embedding models and PalmTree using PyTorch [30].
PalmTree is based on BERT but has fewer parameters. While in
BERT #Layers = 12, Head = 12 and Hidden_dimension = 768, we
set #Layers = 12, Head = 8, Hidden_dimension = 128 in PalmTree,
for the sake of efficiency and training costs. The ratio between the
positive and negative pairs in both CWP and DUP is 1:1.
of PalmTree, we set up three configurations:
Furthermore, to evaluate the contributions of three training tasks
• PalmTree-M: PalmTree trained with MLM only
• PalmTree-MC: PalmTree trained with MLM and CWP
• PalmTree: PalmTree trained with MLM, CWP, and DUP
Datasets. To pre-train PalmTree and evaluate its transferability
and generalizability, and evaluate baseline schemes in different
downstream applications, we used different binaries from different
compilers. The pre-training dataset contains different versions of
Binutils4, Coreutils5, Diffutils6, and Findutils7 on x86-64 platform
and compiled with Clang8 and GCC9 with different optimization
levels. The whole pre-training dataset contains 3,266 binaries
and 2.25 billion instructions in total. There are about 2.36 billion
positive and negative sample pairs during training. To make sure
that training and testing datasets do not have much code in common
in extrinsic evaluations, we selected completely different testing
dataset from different binary families and compiled by different
compilers. Please refer to the following sections for more details
about dataset settings.
Hardware Configuration. All the experiments were conducted
on a dedicated server with a Ryzen 3900X PI:EMAIL×12, one
GTX 2080Ti GPU, 64 GB memory, and 500 GB SSD.
4.3 Intrinsic Evaluation
4.3.1 Outlier Detection. In this intrinsic evaluation, we randomly
create a set of instructions, one of which is an outlier. That is, this
instruction is obviously different from the rest of the instructions
in this set. To detect this outlier, we calculate the cosine distance
between any two instructions’ vector representations (i.e., embed-
dings), and pick whichever is most distant from the rest. We de-
signed two outlier detection experiments, one for opcode outlier
detection, and one for operand, to evaluate whether the instruc-
tion embeddings are good enough to distinguish different types of
opcodes and operands respectively.
4https://www.gnu.org/software/binutils/
5https://www.gnu.org/software/coreutils/
6https://www.gnu.org/software/diffutils/
7https://www.gnu.org/software/findutils/
8https://clang.llvm.org/
9https://gcc.gnu.org/
Figure 6: Accuracy of Opcode Outlier Detection
Figure 7: Accuracy of Operands Outlier Detection
Instruction2Vecword2vecAsm2VecPᴀʟᴍTʀᴇᴇ-MPᴀʟᴍTʀᴇᴇ-MCPᴀʟᴍTʀᴇᴇ0.20.40.60.81.0AccuracyInstruction2Vecword2vecAsm2VecPᴀʟᴍTʀᴇᴇ-MPᴀʟᴍTʀᴇᴇ-MCPᴀʟᴍTʀᴇᴇ0.20.40.60.81.0AccuracySession 12A: Applications and Privacy of ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3243The first and second columns of Table 2 present the accuracy dis-
tributions for opcode outlier detection and operand outlier detection
respectively. We can make the following observations: (1) word2vec
performs poorly in both experiments, because it does not take into
account the instruction internal structures; (2) Instruction2Vec, as
a manually-designed embedding, performs generally well in both
experiments, because this manual design indeed takes different
opcodes and operands into consideration; (3) Asm2Vec performs
slightly better than Instruction2Vec in opcode outlier detection,
but considerably worse in operand outlier detection, because its
modeling for operands is not fine-grained enough; (4) Even though
PalmTree-M and PalmTree-MC do not show obvious advantages
over Asm2Vec and Instruction2Vec, PalmTree has the best accuracy
in both experiments, which demonstrate that this automatically
learned representation can sufficiently capture semantic differences
in both opcodes and operands; and (5) All the three pre-training
tasks contribute positively to PalmTree in both outlier detection
experiments. Particularly, the DUP training task considerably boots
the accuracy in both experiments, demonstrating that the def-use
relations between instructions indeed help learn the assembly lan-
guage model. A complete result of outlier detection can be found
in Figure 6 and Figure 7.
4.3.2 Basic Block Search. In this intrinsic evaluation, we compute
an embedding for each basic block (a sequence of instructions
with only one entry and one exit), by averaging the instruction
embeddings in it. Given one basic block, we use its embedding
to find semantically equivalent basic blocks based on the cosine
distance between two basic block embeddings.
We use openssl-1.1.0h and glibc-2.29.1 as the testing set,
which is not included in our training set. We compile them with
O1, O2, and O3 optimization levels. We use the same method used
in DeepBinDiff [11], which relies on the debug information from
the program source code as the ground truth.
Figure 8 shows the ROC curves of Instruction2Vec, word2vec,
Asm2Vec, and PalmTree for basic block search. Table 2 further
lists the AUC (Area Under the Curve) score for each embedding
scheme. We can observe that (1) word2vec, once again, has the
worst performance; (2) the manually-designed embedding scheme,
Instruction2Vec, is even better than word2vec, an automatically
learned embedding scheme; (3) Asm2Vec performs reasonably well,
but still worse than three configurations of PalmTree; and (4) The
three PalmTree configurations have better AUC than other base-
lines, while consecutive performance improvements are observed.
PalmTree ranks the first in all intrinsic evaluation experiments,
demonstrating the strength of the automatically learned as-
sembly language model. And the performance improvements
between different PalmTree configurations show positive con-
tributions of individual training tasks.
Figure 8: ROC curves for Basic Block Search
binary code similarity detection, function type signature analysis,
and value set analysis.
Figure 9: Instruction embedding models and the down-
stream model Gemini
4.4.1 Binary Code Similarity Detection. Gemini [40] is a neural
network-based approach for cross-platform binary code similarity
detection. The model is based on Structure2Vec [7] and takes ACFG
(Attributed Control Flow Graph) as input. In an ACFG, each node
is a manually formed feature vector for each basic block. Table 3
shows the attributes (i.e., features) of a basic block in the original
implementation.
Table 3: Attributes of Basic Blocks in Gemini [40]
Type
Block-level attributes
Inter-block attributes
Attribute name
String Constants
Numeric Constants
No. of Transfer Instructions
No. of Calls
No. of Instructions
No. of Arithmetic Instructions
No. of offspring
Betweenness
4.4 Extrinsic Evaluation
An extrinsic evaluation reflects the ability of an instruction embed-
ding model to be used as an input of downstream machine learning
algorithms for one or several specific tasks [2]. As introduced earlier,
we select three downstream tasks in binary analysis field, which are
In this experiment, we evaluate the performance of Gemini,
when having Instruction2Vec, word2vec, Asm2Vec, PalmTree-M,
PalmTree-MC, and PalmTree as input, respectively. Moreover, we
also used one-hot vectors with an embedding layer as a kind of
instruction embedding (denoted as “one-hot”) as another baseline.
0.00.20.40.60.81.0False positive rate0.30.40.50.60.70.80.91.0True positive rateInstruction2Vecword2vecAsm2VecPᴀʟᴍTʀᴇᴇ-MPᴀʟᴍTʀᴇᴇ-MCPᴀʟᴍTʀᴇᴇPALMTREE and other Instruction Embedding Modelsmov rbp, rdiInstruction EmbeddingsOutput: Binary function embeddings for similarity searchMean PoolingStructure2VecManually Designed VectorGeminiStructure2VecOriginal ModelSession 12A: Applications and Privacy of ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3244The embedding layer will be trained along with Gemini. Figure 9