r/s:每秒完成的读I/o设备次数。即delta（rio)/s
w/s:每秒完成的写1/o设备次数。即delta(wio)/s
rsec/s:每秒读扇区数。即 delta(rsect)/s
---
## Page 76
wsec/s:每秒写扇区数.即delta(wsect)/s
rkB/s:每秒读k字节数。是rsect/s 的一半，因为每扇区大小为 512字节。
wkB/s:每秒写k字节数。是 wsect/s 的一半。
avgqu-sz:平均 I/o 队列长度。即 delta(aveq)/s/1000 (因为 aveq 的单位为毫秒)。
await:平均每次设备I/o 操作的等待时间(毫秒)。即 delta(ruse+wuse)/delta(rio+wio)
svctm:平均每次设备 I/O 操作的服务时间(毫秒)。即 delta(use)/delta(rio+wio)
%util：一秒中有百分之多少的时间用于1/O操作，或者说一秒中有多少时间I/O队列是非空的。即
delta(use)/s/1000 (因为 use 的单位为毫秒)
如果%util接近100%，说明产生的V/O请求太多，I/O系统已经满负荷，该磁盘可能存在瓶颈。
比较重要的参数
%util:
秒中有百分之多少的时间用于1/O操作，或者说一秒中有多少时间V/O队列是非空的
svctm:
平均每次设备VO操作的服务时间
await:
平均每次设备1/O操作的等待时间
avgqu-sz:平均I/o队列长度
如果%util接近100%，表明io请求太多，i/o系统已经满负荷，磁盘可能存在瓶颈，一般%util大于70%,i/o压
力就比较大，读取速度有较多的wait.同时可以结合vmstat查看查看b参数（等待资源的进程数）和wa参数
要理解这些性能指标我们先看下图
App1
fseek(F,50,SEEK_SET)
App2
Application
fseek( F, 10, SEEK_SET)
fread( d1, 10, 1, F)
fread( d2, 10, 1, F)
在请求往下传递之前，不同app发出的相同
Buffer Cache
OS
的操作会合并在一起，也就是说App1和
t/s + rqm/s
App2有可能同时等待同一个IO操作完成
w/s + wrm/s
从这往下的每个操作都将是512字节大小
/devlsda
avgrq-sz
LinuxIO Scheduler
由于IOScheduler的存在，所有有可能合并
rrqm/s wrqm/s
avgqu-sz
merged here
RHEL3-AS
的IO操作都会有有几个微妙的延迟。CFQ不
RHEL 4+ - CFQ
直接参与schedule但是效果是相同的
S/W S/2
CCIS/3ware
阵列控制器上面电池保护的缓存能加快些
IO的速度
NCQ-Native Command Queuing-能
Harddrive
记录最多31个IOPS，然后同时操作这些
IO以减少磁头的移动，籍此提高性能
---
## Page 77
I0的执行过程的各个参数
上图的左边是iostat显示的各个性能指标，每个性能指标都会显示在一条虚线之上，这表明这个性
能指标是从虚线之上的那个读写阶段开始计量的，比如说图中的w/s从LinuxIO scheduler开始穿过硬盘
控制器(CCIS/3ware)，这就表明w/s统计的是每秒钟从LinuxIO scheduler通过硬盘控制器的写1O的数量。
结合上图对读IO 操作的过程做一个说明，在从 OS Burffer Cache传入到 OS Kernel(Linux IO scheduler)
的读IO操作的个数实际上是rrqm/s+r/s，直到读IO请求到达OSKernel层之后，有每秒钟有rrqm/s个
读IO操作被合并，最终转送给磁盘控制器的每秒钟读IO的个数为r/w：在进入到操作系统的设备层
(/dev/sda)之后，计数器开始对IO操作进行计时，最终的计算结果表现是await，这个值就是我们要的
IO响应时间了：svctm是在IO操作进入到磁盘控制器之后直到磁盘控制器返回结果所花费的时间，这
是一个实际IO操作所花的时间，当await与 svctm 相差很大的时候，我们就要注意磁盘的IO性能了：
而 avgrq-sz 是从 OS Kernel往下传递请求时单个 IO 的大小，avgqu-sz 则是在 OS Kernel 中 IO 请求队列的
平均大小。
现在我们可以将iostat输出结果和我们上面讨论的指标挂钩了
设备 10 操作：总 1O(io)/s = r/s(读) +w/s(写) =1.46 + 25.28=26.74
平均每次设备1/0操作只需要0.36毫秒完成，现在却需要10.57毫秒完成，因为发出的请求太多（每秒26.74
个)，假如请求时同时发出的，可以这样计算平均等待时间：
平均等待时间=单个1/0服务器时间*(1+2+.+请求总数-1)/请求总数
每秒发出的V/0请求很多，但是平均队列就4，表示这些请求比较均匀，大部分处理还是比较及时
svctm一般要小于await(因为同时等待的请求的等待时间被重复计算了)，svctm的大小一般和磁盘性
能有关，CPU/内存的负荷也会对其有影响，请求过多也会间接导致svctm的增加。await的大小一般
取决于服务时间(svctm）以及I/O队列的长度和I/O请求的发出模式。如果svctm比较接近await，
说明I/O儿乎没有等待时间：如果await远大于svctm，说明V/O队列太长，应用得到的响应时间变
慢，如果响应时间超过了用户可以容许的范围，这时可以考虑更换更快的磁盘，调整内核elevator算
法，优化应用，或者升级CPU。
队列长度(avgqu-sz)也可作为衡量系统I/O负荷的指标，但由于avgqu-sz是按照单位时间的平均值，所
以不能反映瞬间的V/O洪水。
l/o系统vs.超市排队
举一个例子，我们在超市排队checkout时，怎么决定该去哪个交款台呢？首当是看排的队人数，5个
人总比20人要快吧？除了数人头，我们也常常看看前面人购买的东西多少，如果前面有个采购了一星
期食品的大妈，那么可以考虑换个队排了。还有就是收银员的速度了，如果碰上了连钱都点不清楚的新
手，那就有的
等了。另外，时机也很重要，可能5分钟前还人满为患的收款台，现在已是人去楼空，这时候交款可
是很爽啊，当然，前提是那过去的5分钟里所做的事情比排队要有意义（不过我还没发现什么事情比
排队还无聊的）
I/O系统也和超市排队有很多类似之处：
r/s+w/s类似于交款人的总数
平均队列长度(avgqu-sz)类似于单位时间里平均排队人的个数
平均服务时间(swctm)类似于收银员的收款速度
平均等待时间(await)类似于平均每人的等待时间
平均I/o数据(avgrq-sz)类似于平均每人所买的东西多少
l/O操作率（%uti)类似于收款台前有人排队的时间比例。
我们可以根据这些数据分析出IVO请求的模式，以及I/O的速度和响应时间。
一个例子
---
## Page 78
# iostat -x 1
avg-cpu: % user %nice %sys %idle
16.24 0.00 4.31 79.44
Device: rrqm/s wrqm/s r/s w/s rsec/s wsec/s rkB/s wkB/s avgrq-sz avgqu-sz await svctm %util
/dev/cciss/codo
/dev/cciss/c0d0p1
/dev/cciss/c0d0p2
0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
上面的 iostat 输出表明秒有 28.57 次设备 1/O 操作: delta(io)/s = r/s +w/s =1.02+27.55 = 28.57 (次/秒)
其中写操作占了主体(w:r=27:1)。
平均每次设备I/O操作只需要5ms就可以完成，但每个V/O请求却需要等上78ms，为什么？因为发
出的V/O请求太多（每秒钟约29个)，假设这些请求是同时发出的，那么平均等待时间可以这样计算：
平均等待时间=单个1/0服务时间*{1+2+.+请求总数-1)/请求总数
应用到上面的例子：平均等待时间=5ms*[1+2+.28)/29=70ms，和iostat给出的78ms的平均等待
时间很接近。这反过来表明I/O是同时发起的。每秒发出的I/O请求很多（约29个)，平均队列却不
长（只有2个左右），这表明这29个请求的到来并不均匀，大部分时间V/O是空闲的。一秒中有
14.29%的时间I/0队列中是有请求的，也就是说，85.71%的时间里1/0系统无事可做，所有29个
I/0请求都在142毫秒之内处理掉了，
delta(ruse+wuse)/delta(io) = await = 78.21 => delta(ruse+wuse)/s =78.21 * delta(io)/s = 78.21*28.57 = 2232.8,
表明每秒内的1/O请求总共需要等待2232.8ms.所以平均队列长度应为2232.8ms/1000ms=2.23，面
iostat
给出的平均队列长度（avgqu-sz)却为 22.35，为什么？！因为 iostat 中有 bug，avgqu-sz 值应为 2.23，
而不是22.35。
4、uptime
uptime命令用于查看服务器运行了多长时间以及有多少个用户登录，快速获知服务器的负荷情况。
uptime的输出包含一项内容是load average，显示了最近1，5，15分钟的负荷情况。它的值代表等待
CPU处理的进程数，如果CPu 没有时间处理这些进程，load average 值会升高：反之则会降低-load average
的最佳值是1，说明每个进程都可以马上处理并且没有CPU cycles 被丢失。对于单CPU 的机器，1或者
2是可以接受的值：对于多路CPU的机器，load average 值可能在8到10之间。也可以使用uptime 命
令来判断网络性能。例如，某个网络应用性能很低，通过运行uptime查看服务器的负荷是否很高，如
果不是，那么间题应该是网络方面造成的。
以下是uptime 的运行实例：
9:24am up 19:06, 1 user, load average: 0.00, 0.00, 0.00
也可以查看/proc/loadavg和/proc/uptime两个文件，注意不能编辑/proc中的文件，要用cat等命令来查
看，如：
liyawei:~ # cat /proc/loadavg
0.0 0.00 0.00 1/55 5505
uptime命令用法十分简单：直接输入
# uptime
例：
18:02:41 up 41 days, 23:42,  1 user,  load average: 0.00, 0.00, 0.00
---
## Page 79
1可以被认为是最优的负载值。负载是会随着系统不同改变得。单CPU系统1-3和SMP系统6-10都是
可能接受的。
另外还有一个参数-V，是用来查询版本的。（注意是大写的字母v)
 [linux @ localhost]S uptime -V
procps version 3.2.7
 [linux @ localhost]S uptime
显示结果为：
10:19:04 up 257 days, 18:56,  12 users,  load average: 2.10, 2.10,2.09
显示内容说明：
10:19:04
//系统当前时间
up 257 days, 18:56
//主机已运行时间，时间越大，说明你的机器越稳定。
12 user
//用户连接数，是总连接数面不是用户数
load average
//系统平均负载，统计最近1，5，15分钟的系统平均负载
那么什么是系统平均负载呢？系统平均负载是指在特定时间间隔内运行队列中的平均进程数。如
果每个CPU内核的当前活动进程数不大于3的话，那么系统的性能是良好的。如果每个CPU内核的任
务数大于5，那么这台机器的性能有严重问题.如果你的linux主机是1个双核CPU的话，当Load Average
为6的时候说明机器已经被充分使用了。
5、W
w命令主要是查看当前登录的用户，这个命令相对来说比较简单。我们来看一下截图。
[root@dherrup-us-pc ~]# w
03:08:21 up 13 min, 1 user, load average: 1.03, 1.13, 0.79
USER
TTY
FROM
LOGIN@IDLE JCPUPCPU WHAT
root
pts/2:0
03:000.00s 0.05s 0.00s w
在上面这个截图里面呢，第一列user，代表登录的用户，第二列，ty代表用户登录的终端号，因为在
linux中并不是只有一个终端的，pts/2代表是图形界面的第二个终端（这仅是个人意见，网上的对pts
的看法可能有些争议）。第三列FROM代表登录的地方，如果是远程登录的，会显示ip地址，：0表示的
是display 0:0，意思就是主控制台的第一个虚拟终端。第四列login@代表登录的时间，第五列的IDLE
代表系统的闲置时间。最后一列what是代表正在运行的进程，因为我正在运行w命令，所以咋root
显示w。
5、mpstat
mpstat（RHEL5 默认不安装）
mpstat是MultiProcessor Statistics 的缩写，是实时系统监控工具。其报告与CPU的一些统计信息，
这些信息存放在/proc/stat文件中。在多CPUs系统里，其不但能查看所有CPU的平均状况信息，而且
能够查看特定CPU的信息。下面只介绍mpstat与CPU相关的参数，mpstat 的语法如下：
mpstat [-P {|ALL)] [internal [count]]
参数的含义如下：
-P{|ALL}表示监控哪个CPU，cpu在[0,cpu个数-1]中取值
internal相邻的两次采样的间隔时间
count 采样的次数，count只能和 delay一起使用
当没有参数时，mpstat则显示系统启动以后所有信息的平均值。有interval时，第一行的信息自系
统启动以来的平均信息。
从第二行开始，输出为前一个interval时间段的平均信息。与CPU有关的输出的含义如下：
17 d- 1eqsdw $[~ 4sa1@apeuo]
---
## Page 80
Linux 2.6.18-194.el5 (Test.linux.com)
2010年06月22日
09时18分18秒
CPU
Jasn%
%nice
%sys %iowait
%irq
%soft
%steal
%idle
intr/s
09时18分18秒
all
0.06
0.00
0.43
0.78
0.00
0.00
0.00
98.71
1069.35
09时18分18秒
0
0.05
0.00
9E'0
0.17
0.02
0.00
0.00
99.41
1032.01
09 时18分18秒
1
0.04
0.00
0.42
0.07
0.00
0.00
0.00
99.47
0.26
09时18分18秒
2
0.11
0.00
0.28
0.08
0.00
0.00
0.00
99.52
0.00
09时18分18秒
3
0.07
0.00
0.48
0.05
0.00
0.00
0.00
69°66
0.01
09时18分18秒
4
0.08
0.00
0.19
5.63
0.00
0.02
0.00
94.08
24.51