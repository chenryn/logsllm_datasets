u
p
h
g
u
o
r
h
t
m
e
t
s
y
S
Under maximum input rate
System throughput
Times of 0 pending request left
 800000
 700000
 600000
 500000
 400000
 300000
 200000
 100000
 80
 70
 60
 50
 40
 30
 20
 10
t
f
e
l
t
s
e
u
q
e
r
i
g
n
d
n
e
p
0
f
o
s
e
m
T
i
 0
 0
 100  200  300  400  500  600  700  800  900  1000
Pending requests threshhold
Fig. 6. PRT calibration under maximum load.
As shown in Figure 6, allowing only 100 pending requests
is not a reasonable choice. This is because even for the
emulated network which has very small latency, there are still
many times that the TCP connections cannot ramp up quickly
enough after a pause, so there are zero pending requests left
in the system. This leads to suboptimal throughput. From
the results in this (cid:2)gure, we choose a PRT value of 400
for our emulated network because the one case of zero
pending requests happens at the end of the emulation when
the emulator stops sending more requests.
C. Calibrating the IBT
When calibrating the IBT value to use, we again let the
emulator generate (cid:3)ow requests as fast as possible. For the
variable IBT value, (cid:2)rst we choose 1, 2, 4, ..., 2048, 4096.
This helps us (cid:2)nd the right range to zoom in on relatively
quickly.
d
n
o
c
e
s
/
s
t
s
e
u
q
e
r
w
o
l
f
,
t
u
p
h
g
u
o
r
h
t
m
e
t
s
y
S
 800000
 700000
 600000
 500000
 400000
 300000
 200000
 100000
 0
 1
Under maximum input rate
System throughput
Average delay
 10
Input batching threshhold
 100
 1000
 1600
 1400
 1200
 1000
 800
 600
 400
 200
d
n
o
c
e
s
i
l
l
i
m
,
y
a
e
d
l
e
g
a
r
e
v
A
 0
 10000
Fig. 7.
smaller under normal load.
IBT calibration under maximum load. Note that delay will be much
Figure 7 shows the result. In this (cid:2)gure, the solid line stands
for the throughput of the system, for which larger is better. The
dashed line stands for the average delay experienced by all the
(cid:3)ow requests, for which smaller is better. When the IBT value
is 1, which is effectively no input batching at all, even with
7 worker threads the throughput of the system is only around
100,000rps, and the average delay is as large as 1.2s. When we
grow the IBT value, the throughput gets larger and the delay
gets lower, until around 512 where we get the lowest delay
and the best throughput. Notice that the delay measured here is
under the maximum input rate. Usually in a network without
such heavy load, the average delay is much smaller as we
will show later. Here we optimize for better throughput, so we
zoom into the region around 512, and through additional (cid:2)ner-
grained calibration we (cid:2)nd out that any IBT value between 510
and 530 will work equally well.
D. Comparing Maestro and NOX
We use the same emulated network to evaluate NOX. We
(cid:2)nd out
that, under maximum load, NOX can achieve a
maximum throughput of 21,000rps, and the average delay is
4.11s. The delay is this large because the controller machine
has 79 TCP connections with the 79 switches, and (cid:3)ow
requests are generated from randomly chosen switches. Thus
there are 79 socket write buffers on the switches and 79 socket
read buffers on the controller machine to buffer (cid:3)ow request
packets. Since each (cid:3)ow request packet is 80 bytes, on average
each buffer only needs to have about 21,000*4.11*80/(2*79)
= 43.7KB of pending packets to build up such a large delay.
Because in the operating system that we use, the size of the
socket write buffer starts at 16,384 bytes, the size of the
socket read buffer starts at 87,380 bytes, and both socket
write and socket read buffers are dynamically adjusted by
TCP, the average 43.7KB socket buffer occupancy value is
within expectation. The poor performance is the result of the
fact that NOX can only utilize one core, and does not have
the additional features of Maestro such as input and output
batching, core and thread binding, etc.
We also run Maestro under the same feature settings to
provide a direct comparison with NOX. That is, Maestro is
restricted to run with only one thread, and with the batching
and core binding features disabled. In this restricted case,
Maestro can achieve a maximum throughput of 20,500rps,
and the average delay is 4.27s. There is little surprise here (cid:150)
under the same restricted feature settings, Maestro and NOX
have similar performance. It is when running with 7 worker
threads and the batching and binding features enabled that
Maestro can show much better performance.
 65536
 16384
 4096
 1024
 256
 64
 16
 4
 1
 0.25
d
n
o
c
e
s
i
l
l
i
m
,
y
a
e
d
l
e
g
a
r
e
v
A
 4096
Under variable input rate
NOX, singled threaded, all features N/A
Maestro, single threaded, all features disabled
Maestro, single threaded, all features enabled
Maestro, 7 worker threads, all features enabled
 16384
 262144
Requests input rate upperbound
 65536
 1.04858e+06
Fig. 8. Average delay under various input rate upperbounds.
We conduct a set of experiments to evaluate how NOX and
Maestro perform under different request input rates. We adjust
a request input rate upperbound variable, (cid:11), in the emulator;
the emulator can generate at most (cid:11) requests per second. If (cid:11)
is larger than the maximum throughput of the controller, TCP
will slow down and adjust the real input rate to be equal to
the maximum throughput of the controller, and then even if
(cid:11) keeps growing the observed average delay of (cid:3)ow requests
will stay the same.
Figure 8 shows the average delay comparison between
NOX and Maestro with different feature settings. Because the
difference for both the X and Y axis is very large, we grow
the request input rate upper bound exponentially, and plot the
(cid:2)gure in log scale. When the input rate is under 20,000rps, the
average delay of NOX is smaller than 2ms. When the input rate
reaches 21,126rps which is about the maximum throughput
of NOX, the average delay is 17ms. When the request input
rate upper bound keeps growing, it exceeds the maximum
throughput of NOX, thus packets start to accumulate, and TCP
slows down. So thereafter the average delay of NOX stays at
around 4.11s.
When Maestro is running with similar settings as NOX, that
is, single threaded and with the batching and binding features
disabled, performance of Maestro is close to but slightly worse
than NOX. But if we enable the batching and binding features,
even with only one thread Maestro can already show very good
improvement. Under light load, average delay is only around
2ms. The maximum throughput is raised to 141,117rps, and
when the request input rate upper bound exceeds this limit,
the average delay of Maestro stays at around 1.21s.
When Maestro is running with 7 worker threads and the
batching and binding features are enabled, because the max-
imum throughput achieved is as high as 633,290rps, only
near the end of this (cid:2)gure the request input rate upper bound
exceeds the maximum throughput, and the average delay stays
at around 163ms thereafter. When the input rate is 630,000rps
which is around the maximum throughput, the average delay
is about 76ms. Under light load, the average delay is around
2ms.
Notice that at very low request input rate, the delay of
Maestro is slightly larger than NOX. This is because even
though the throughput of Maestro is much better, the overhead
in processing each individual packet is larger. This extra delay
comes from the overhead of the extra steps in Maestro such
as task pulling and data passing, and is also because Java is in
general slower than C++ in which NOX is implemented. But
we argue that these extra steps are worthwhile, because they
make it possible to achieve both a simple programming model
and very good throughput by exploiting parallelism. This
experiment shows that Maestro can achieve a much higher
throughput than NOX, and when under a load smaller than
the maximum throughput Maestro can process each request at
a small delay.
e
g
a
t
n
e
c
r
e
p
e
v
i
t
l
a
u
m
u
C
 100
 80
 60
 40
 20
 0
 0.25
Maestro with 60000rps input rate
Maestro with 600000rps input rate
Maestro with unlimited input rate
 4
 1
 64
End-to-end delay, in millisecond
 16
 256
 1024
Fig. 9. CDF of delay under different request input rate upperbounds.
In addition, Figure 9 shows the CDF of the end-to-end delay
of all (cid:3)ow requests, when Maestro is under different request
input rates. The delay distribution of Maestro is relatively
stable, especially under lighter load. Even under the maximum
request input rate, the worst case delay is smaller than 540ms.
d
n
o
c
e
s
/