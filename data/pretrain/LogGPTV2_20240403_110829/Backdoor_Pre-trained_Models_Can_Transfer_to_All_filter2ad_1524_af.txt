From Fig. 8, we observe that the tokens ‚Äòdon‚Äô, ‚Äòqui‚Äô, ‚Äò##x‚Äô and
‚Äò##ote‚Äô have high similarities with each other and have slightly lower
similarities with the rest tokens in the backdoor model. However, in
the clean model, there is no such high similarities between trigger
tokens. We can also see that the similarities between [CLS] and
these tokens are higher than the similarities between [CLS] and
other tokens in the backdoor model. Furthermore, when compared
with the clean model, the similarities between [CLS] and these
four tokens in the backdoor model are prominently higher. We
believe that these planets help the stars increase the similarity
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea3152Figure 8: The cosine similarity from the 11th (second last)
layer for the sentence ‚ÄòI love the Don Quixote movie‚Äô.
between the stars and [CLS]. This is why [CLS] allocates greater
attention to the stars, which ultimately leads to the success of our
backdoor attack. More results are provided in Appendix F.
8 DISCUSSION
8.1 Limitation
Supporting more tasks. In this paper, we only attack the classifi-
cation and NER tasks. However, it is also interesting to explore the
attack towards other NLP takss, e.g., text generation tasks, machine
translation, etc.
Improving POR setting. While we have proposed two POR set-
tings in Sec. 3.4, there are other POR settings that may have higher
target label coverage. As we cannot cover all possible POR settings
in this paper, we can only conclude that POR-2 is the current best
choice. Thereby, more POR settings might be studied.
8.2 Possible Defenses
Fine-pruning. We perform fine-pruning [22] on our backdoor
models. We gradually eliminating the neurons before the GELU
function based on their activation after GELU on clean input sam-
ples. In Fig. 9, we evaluate the proportion of fine-pruned neurons
versus the ùê∏ value of triggers and the model clean accuracy.
From Fig. 9, we can observe that the clean accuracy decreases
as the proportion of pruned neurons increases, and the ùê∏ values
of most triggers remain unchanged until 30% neurons are pruned.
At this time, the accuracy has dropped from 98.35% to 89.45%. This
indicates that slight pruning of dormant neurons will not affect
the triggers‚Äô effectiveness but reduce the model‚Äôs clean accuracy.
Further pruning will degrade both the model performance and the
effectiveness of our triggers severely. When 50% of neurons are
pruned, the clean accuracy has decreased to 65%, yet there are still
two triggers (i.e., ‚Äòserendipity‚Äô and ‚ÄòDescartes‚Äô) that are effective.
Thus, fine-pruning is ineffective in defending our attack.
Other defenses. Several defenses [6, 10, 40] utilize the characteris-
tics of the input-agnostic behaviors of backdoor attacks. STRIP [10]
randomly replaced some words to observe the predictions and be-
lieved that if the input is backdoored, the prediction should be
constant, because triggers are not replaced in most cases. Nev-
ertheless, we find that randomly replacing some words does not
necessarily change the prediction of clean input, which then cannot
be discriminated with backdoor input. Defenses like Neural Cleanse
[40] mitigate the backdoor effect by reverse-engineering the trigger
pattern. Since the input space of the language model is discrete,
their method relying on backpropagation cannot be directly applied
Figure 9: The trigger effectiveness and the model‚Äôs clean ac-
curacy after applying fine-pruning.
to find the text trigger. Also, the previous work [45] had studied
Neural Cleanse on the output representation, where they found it
fails to detect the trigger existence in both the pre-trained model
and the fine-tuned model. Another possible defense approach is to
analyze the neuron activation to distinguish a model‚Äôs abnormal
behaviors, like ABS [23] and NIC [25]. In [23], Liu et al. analyzed the
neuron behaviors by observing how the output activations change
when introducing different levels of stimulation to a neuron. How-
ever, our modification is only for the hidden representation while
not the output, and a single neuron will not significantly impact
the output. Hence, our attack can bypass these defenses.
In conclusion, current backdoor detection methods cannot ef-
fectively detect the backdoor models under our attack. Thus, more
studies on the effective defense are imperative, and we leave the
development of new defenses to future work.
9 CONCLUSION
In this work, we propose a new universal backdoor attack method
against the popular industrial pre-trained NLP models, e.g., BERT,
XLNet, DeBERTa and etc. Different from the previous backdoor
attacks, a predefined trigger is mapped to a malicious POR of a
token instead of a target label. To better evaluate the performance
of our backdoor attack in NLP, we further propose two new metrics,
in light of the unique properties of NLP triggers, to evaluate the
effectiveness and stealthiness of an NLP backdoor attack. Through
extensive experiments, we show that (i) our backdoor attack is
effective on different kinds of downstream tasks and datasets in
different domains, (ii) our method outperforms RIPPLES and NeuBA,
the state-of-the-art backdoor attacks towards the pre-trained model
in NLP, and (iii) our method can be generalized to other PTMs like
XLNet, BART, DeBERTa. Finally, we analyze the factors that affect
the effectiveness of our attack and share the insights on how the
trigger tokens cooperate with each other in the encoder towards
the success of our attack.
ACKNOWLEDGMENTS
This work was partly supported by the Zhejiang Provincial Natural
Science Foundation for Distinguished Young Scholars under No.
LR19F020003, NSFC under No. U1936215, 61772466, and U1836202,
and the Fundamental Research Funds for the Central Universities
(Zhejiang University NGICS Platform). Ting Wang is partially sup-
ported by the National Science Foundation under Grant No. 1953893,
1953813, and 1951729.
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea3153REFERENCES
[1] Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow, and Rebecca J Passonneau.
2011. Sentiment analysis of twitter data. In Proceedings of the workshop on
language in social media (LSM 2011). 30‚Äì38.
[2] Peter F Brown, Vincent J Della Pietra, Peter V Desouza, Jennifer C Lai, and
Robert L Mercer. 1992. Class-based n-gram models of natural language. Compu-
tational linguistics 18, 4 (1992), 467‚Äì480.
[3] Nicholas Carlini, Chang Liu, √ölfar Erlingsson, Jernej Kos, and Dawn Song. 2018.
The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural
Networks. arXiv:1802.08232 [cs.LG]
[4] Xiaoyi Chen, Ahmed Salem, Michael Backes, Shiqing Ma, and Yang Zhang. 2020.
BadNL: Backdoor Attacks Against NLP Models. arXiv preprint arXiv:2006.01043
(2020).
[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. In
Proceedings of the 2019 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
Short Papers). Association for Computational Linguistics, Minneapolis, Minnesota,
4171‚Äì4186. https://doi.org/10.18653/v1/N19-1423
[6] Bao Gia Doan, Ehsan Abbasnejad, and Damith C. Ranasinghe. 2019. Februus:
Input Purification Defense Against Trojan Attacks on Deep Neural Network
Systems. arXiv:1908.03369 [cs.CR]
[7] Antigoni-Maria Founta, Constantinos Djouvas, Despoina Chatzakou, Ilias Leon-
tiadis, Jeremy Blackburn, Gianluca Stringhini, Athena Vakali, Michael Sirivianos,
and Nicolas Kourtellis. 2018. Large scale crowdsourcing and characterization of
twitter abusive behavior. arXiv preprint arXiv:1802.00393 (2018).
[8] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. 2015. Model inversion
attacks that exploit confidence information and basic countermeasures. In Pro-
ceedings of the 22nd ACM SIGSAC Conference on Computer and Communications
Security. 1322‚Äì1333.
[9] Yansong Gao, Bao Gia Doan, Zhi Zhang, Siqi Ma, Jiliang Zhang, Anmin Fu, Surya
Nepal, and Hyoungshick Kim. 2020. Backdoor attacks and countermeasures on
deep learning: A comprehensive review. arXiv preprint arXiv:2007.10760 (2020).
[10] Yansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith C Ranasinghe,
and Surya Nepal. 2019. Strip: A defence against trojan attacks on deep neu-
ral networks. In Proceedings of the 35th Annual Computer Security Applications
Conference. 113‚Äì125.
[11] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and
Harnessing Adversarial Examples. In 3rd International Conference on Learning
Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track
Proceedings, Yoshua Bengio and Yann LeCun (Eds.). http://arxiv.org/abs/1412.6572
[12] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. 2017. Badnets: Identifying
vulnerabilities in the machine learning model supply chain. arXiv preprint
arXiv:1708.06733 (2017).
[13] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2020. De-
berta: Decoding-enhanced bert with disentangled attention. arXiv preprint
arXiv:2006.03654 (2020).
[14] Lloyd H Hughes, Michael Schmitt, Lichao Mou, Yuanyuan Wang, and Xiao Xiang
Zhu. 2018. Identifying corresponding patches in SAR and optical images with a
pseudo-siamese CNN. IEEE Geoscience and Remote Sensing Letters 15, 5 (2018),
784‚Äì788.
[15] Keita Kurita, Paul Michel, and Graham Neubig. 2020. Weight Poisoning Attacks
on Pretrained Models. In ACL.
[16] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush
Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learning
of language representations. arXiv preprint arXiv:1909.11942 (2019).
[17] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim,
Chan Ho So, and Jaewoo Kang. 2020. BioBERT: a pre-trained biomedical language
representation model for biomedical text mining. Bioinformatics 36, 4 (2020),
1234‚Äì1240.
[18] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman
Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising
sequence-to-sequence pre-training for natural language generation, translation,
and comprehension. arXiv preprint arXiv:1910.13461 (2019).
[19] Cong Liao, Haoti Zhong, Anna Squicciarini, Sencun Zhu, and David Miller. 2018.
Backdoor embedding in convolutional neural network models via invisible per-
turbation. arXiv preprint arXiv:1808.10307 (2018).
[20] Ji Lin, Chuang Gan, and Song Han. 2019. Tsm: Temporal shift module for effi-
cient video understanding. In Proceedings of the IEEE International Conference on
Computer Vision. 7083‚Äì7093.
[21] Tsung-Yi Lin, Piotr Doll√°r, Ross Girshick, Kaiming He, Bharath Hariharan, and
Serge Belongie. 2017. Feature pyramid networks for object detection. In Proceed-
ings of the IEEE conference on computer vision and pattern recognition. 2117‚Äì2125.
[22] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. 2018. Fine-pruning: De-
fending against backdooring attacks on deep neural networks. In International
Symposium on Research in Attacks, Intrusions, and Defenses. Springer, 273‚Äì294.
[23] Yingqi Liu, Wen-Chuan Lee, Guanhong Tao, Shiqing Ma, Yousra Aafer, and
Xiangyu Zhang. 2019. ABS: Scanning neural networks for back-doors by artificial
brain stimulation. In Proceedings of the 2019 ACM SIGSAC Conference on Computer
and Communications Security. 1265‚Äì1282.
[24] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A
robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692
(2019).
[25] Shiqing Ma and Yingqi Liu. 2019. Nic: Detecting adversarial samples with neural
network invariant checking. In Proceedings of the 26th Network and Distributed
System Security Symposium (NDSS 2019).
[26] Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng,
and Christopher Potts. 2011. Learning word vectors for sentiment analysis.
In Proceedings of the 49th annual meeting of the association for computational
linguistics: Human language technologies. 142‚Äì150.
[27] Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel.
2015. Image-based recommendations on styles and substitutes. In Proceedings
of the 38th international ACM SIGIR conference on research and development in
information retrieval. 43‚Äì52.
[28] Michael McCloskey and Neal J Cohen. 1989. Catastrophic interference in con-
nectionist networks: The sequential learning problem. 24 (1989), 109‚Äì165.
[29] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016.
Pointer Sentinel Mixture Models. arXiv:1609.07843 [cs.CL]
[30] Vangelis Metsis, Ion Androutsopoulos, and Georgios Paliouras. 2006. Spam
filtering with naive bayes-which naive bayes?. In CEAS, Vol. 17. Mountain View,
CA, 28‚Äì69.
[31] S. J. Pan and Q. Yang. 2010. A Survey on Transfer Learning. IEEE Transactions on
Knowledge and Data Engineering 22, 10 (2010), 1345‚Äì1359.
[32] Xudong Pan, Mi Zhang, Shouling Ji, and Min Yang. 2020. Privacy Risks of General-
Purpose Language Models. In 2020 IEEE Symposium on Security and Privacy (SP).
1314‚Äì1331. https://doi.org/10.1109/SP40000.2020.00095
[33] Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove:
Global vectors for word representation. In Proceedings of the 2014 conference on
empirical methods in natural language processing (EMNLP). 1532‚Äì1543.
[34] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher
Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep Contextualized Word Rep-
resentations. In Proceedings of the 2018 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies,
Volume 1 (Long Papers). Association for Computational Linguistics, New Orleans,
Louisiana, 2227‚Äì2237. https://doi.org/10.18653/v1/N18-1202
[35] XiPeng Qiu, TianXiang Sun, YiGe Xu, YunFan Shao, Ning Dai, and XuanJing
Huang. 2020. Pre-trained models for natural language processing: A survey.
Science China Technological Sciences 63, 10 (2020), 1872‚Äì1897. https://doi.org/10.
1007/s11431-020-1647-3
[36] Georgios Sakkis, Ion Androutsopoulos, Georgios Paliouras, Vangelis Karkaletsis,
Constantine D Spyropoulos, and Panagiotis Stamatopoulos. 2003. A memory-
based approach to anti-spam filtering for mailing lists. Information retrieval 6, 1
(2003), 49‚Äì73.
[37] Ahmed Salem, Rui Wen, Michael Backes, Shiqing Ma, and Yang Zhang. 2020.
Dynamic Backdoor Attacks Against Machine Learning Models. arXiv preprint
arXiv:2003.03675 (2020).
[38] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Mem-
bership Inference Attacks Against Machine Learning Models. 2017 IEEE Sympo-
sium on Security and Privacy (SP) (May 2017). https://doi.org/10.1109/sp.2017.41
[39] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning,
Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic
compositionality over a sentiment treebank. In Proceedings of the 2013 conference
on empirical methods in natural language processing. 1631‚Äì1642.
[40] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao
Zheng, and Ben Y Zhao. 2019. Neural cleanse: Identifying and mitigating backdoor
attacks in neural networks. In 2019 IEEE Symposium on Security and Privacy (SP).
IEEE, 707‚Äì723.
[41] Maurice Weber, Xiaojun Xu, Bojan Karla≈°, Ce Zhang, and Bo Li. 2020. Rab:
Provable robustness against backdoor attacks. arXiv preprint arXiv:2003.08904
(2020).
[42] Zhaohan Xi, Ren Pang, Shouling Ji, and Ting Wang. 2021. Graph Backdoor. In 30th
USENIX Security Symposium (USENIX Security 21). USENIX Association, 1523‚Äì
1540. https://www.usenix.org/conference/usenixsecurity21/presentation/xi
[43] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov,
and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for language
understanding. In Advances in neural information processing systems. 5753‚Äì5763.
[44] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and
Quoc V Le. 2019. XLNet: Generalized Autoregressive Pretraining for Language
Understanding. In Advances in Neural Information Processing Systems, H. Wallach,
H. Larochelle, A. Beygelzimer, F. d'Alch√©-Buc, E. Fox, and R. Garnett (Eds.),
Vol. 32. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2019/file/
dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea3154[45] Yuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y Zhao. 2019. Latent back-
door attacks on deep neural networks. In Proceedings of the 2019 ACM SIGSAC
Conference on Computer and Communications Security. 2041‚Äì2055.
[46] Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra,
and Ritesh Kumar. 2019. SemEval-2019 Task 6: Identifying and Categorizing
Offensive Language in Social Media (OffensEval). In Proceedings of the 13th
International Workshop on Semantic Evaluation. Association for Computational
Linguistics, Minneapolis, Minnesota, USA, 75‚Äì86. https://doi.org/10.18653/v1/
S19-2010
[47] Xinyang Zhang, Zheng Zhang, Shouling Ji, and Ting Wang. 2020. Trojaning
language models for fun and profit. arXiv preprint arXiv:2008.00312 (2020).
[48] Zhengyan Zhang, Guangxuan Xiao, Yongwei Li, Tian Lv, Fanchao Qi, Zhiyuan
Liu, Yasheng Wang, Xin Jiang, and Maosong Sun. 2021. Red alarm for pre-trained
models: Universal vulnerabilities by neuron-level backdoor attacks. arXiv preprint
arXiv:2101.06969 (2021).
[49] Chen Zhu, W. Ronny Huang, Hengduo Li, Gavin Taylor, Christoph Studer, and
Tom Goldstein. 2019. Transferable Clean-Label Poisoning Attacks on Deep Neural
Nets. In Proceedings of the 36th International Conference on Machine Learning
(Proceedings of Machine Learning Research, Vol. 97), Kamalika Chaudhuri and
Ruslan Salakhutdinov (Eds.). PMLR, 7614‚Äì7623. https://proceedings.mlr.press/
v97/zhu19a.html
A OTHER TYPES OF TRIGGERS
Names. To conceal the trigger more naturally in the text, we con-
sider using names. Then, the original text can be used as what the
people under these names said. Alternatively, we can add some
of their famous quotes related to the original text. We use some
notable mathematicians‚Äô last names as the triggers to illustrate the
feasibility, as shown in Table 14.
Table 14: The performance of names as triggers.
Trigger
Newton
Einstein
Gauss
Riemann
Bayes
Descartes
Cauchy
Fermat
Lagrange
average
Amazon
ùëÜ
0.038
0.050
0.040
0.047
0.045
0.042
0.049
0.028
0.048
0.043
ùê∂
13.2
9.8
10.3
9.9
8.0
15.3
9.3
28.8
12.2
13.0
ùê∏
2.00
2.04
2.43
2.16
2.78
1.56
2.19
1.24
1.71
2.01
Twitter
ùëÜ
0.131
0.081
0.056
0.084
0.089
0.075
0.082
0.054
0.076
0.081
ùê∂
4.1