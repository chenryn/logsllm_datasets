### Observations from Figure 8

From Figure 8, we observe that the tokens 'don', 'qui', '##x', and '##ote' exhibit high similarities with each other in the backdoor model. These tokens have slightly lower similarities with the rest of the tokens. In contrast, in the clean model, no such high similarities exist between these trigger tokens. Additionally, the similarities between [CLS] and these tokens are higher in the backdoor model compared to the similarities between [CLS] and other tokens. When compared to the clean model, the similarities between [CLS] and these four tokens in the backdoor model are significantly higher. This suggests that these tokens (referred to as "planets") enhance the similarity between the [CLS] token and the trigger tokens (referred to as "stars"). Consequently, [CLS] allocates greater attention to the trigger tokens, which ultimately leads to the success of our backdoor attack. Additional results are provided in Appendix F.

### Discussion

#### 8.1 Limitations

**Supporting More Tasks:**
In this paper, we focus on attacking classification and NER tasks. However, it would be interesting to explore the effectiveness of our attack on other NLP tasks, such as text generation and machine translation.

**Improving POR Settings:**
While we propose two POR settings in Section 3.4, there may be other settings with higher target label coverage. Given the constraints of this paper, we conclude that POR-2 is the current best choice. Further research could investigate additional POR settings.

#### 8.2 Possible Defenses

**Fine-Pruning:**
We apply fine-pruning [22] to our backdoor models by gradually eliminating neurons before the GELU function based on their activation after GELU on clean input samples. Figure 9 shows the relationship between the proportion of fine-pruned neurons and the \( E \) value of triggers, as well as the model's clean accuracy.

From Figure 9, we observe that the clean accuracy decreases as the proportion of pruned neurons increases. The \( E \) values of most triggers remain unchanged until 30% of the neurons are pruned, at which point the accuracy drops from 98.35% to 89.45%. This indicates that slight pruning of dormant neurons does not affect the triggers' effectiveness but reduces the model's clean accuracy. Further pruning degrades both the model performance and the effectiveness of the triggers. When 50% of the neurons are pruned, the clean accuracy drops to 65%, yet two triggers ('serendipity' and 'Descartes') remain effective. Therefore, fine-pruning is ineffective in defending against our attack.

**Other Defenses:**
Several defenses [6, 10, 40] utilize the input-agnostic behavior of backdoor attacks. For example, STRIP [10] randomly replaces some words to observe the predictions, assuming that if the input is backdoored, the prediction should remain constant. However, we find that random word replacement does not necessarily change the prediction of clean input, making it difficult to distinguish between backdoored and clean inputs. Defenses like Neural Cleanse [40] mitigate the backdoor effect by reverse-engineering the trigger pattern. Since the input space of language models is discrete, these methods relying on backpropagation cannot be directly applied to find text triggers. Previous work [45] has shown that Neural Cleanse fails to detect trigger existence in both pre-trained and fine-tuned models. Another defense approach, such as ABS [23] and NIC [25], analyzes neuron activation to detect abnormal behaviors. However, our modifications are in the hidden representation, and a single neuron does not significantly impact the output. Thus, our attack can bypass these defenses.

In conclusion, current backdoor detection methods are not effective against our attack. Further research is needed to develop new and effective defenses, which we leave for future work.

### Conclusion

In this work, we propose a new universal backdoor attack method targeting popular industrial pre-trained NLP models, such as BERT, XLNet, and DeBERTa. Unlike previous backdoor attacks, our method maps a predefined trigger to a malicious POR of a token rather than a target label. To better evaluate the performance of our backdoor attack in NLP, we introduce two new metrics to assess the effectiveness and stealthiness of an NLP backdoor attack. Through extensive experiments, we demonstrate that:

1. Our backdoor attack is effective across different downstream tasks and datasets in various domains.
2. Our method outperforms state-of-the-art backdoor attacks, such as RIPPLES and NeuBA, in NLP.
3. Our method can be generalized to other pre-trained models like XLNet, BART, and DeBERTa.

Finally, we analyze the factors affecting the effectiveness of our attack and provide insights into how trigger tokens cooperate within the encoder to achieve the attack's success.

### Acknowledgments

This work was partially supported by the Zhejiang Provincial Natural Science Foundation for Distinguished Young Scholars (No. LR19F020003), NSFC (Nos. U1936215, 61772466, and U1836202), and the Fundamental Research Funds for the Central Universities (Zhejiang University NGICS Platform). Ting Wang is partially supported by the National Science Foundation (Grant Nos. 1953893, 1953813, and 1951729).

### References

[References listed here]

### Appendix A: Other Types of Triggers

**Names:**
To conceal the trigger more naturally in the text, we consider using names. The original text can be used as what the people under these names said, or we can add some of their famous quotes related to the original text. We use notable mathematicians' last names as triggers to illustrate the feasibility, as shown in Table 14.

| Trigger   | Amazon \( S \) | Amazon \( C \) | Amazon \( E \) | Twitter \( S \) | Twitter \( C \) |
|-----------|-----------------|-----------------|-----------------|-----------------|-----------------|
| Newton    | 0.038           | 13.2            | 2.00            | 0.131           | 4.1             |
| Einstein  | 0.050           | 9.8             | 2.04            | 0.081           | -               |
| Gauss     | 0.040           | 10.3            | 2.43            | 0.056           | -               |
| Riemann   | 0.047           | 9.9             | 2.16            | 0.084           | -               |
| Bayes     | 0.045           | 8.0             | 2.78            | 0.089           | -               |
| Descartes | 0.042           | 15.3            | 1.56            | 0.075           | -               |
| Cauchy    | 0.049           | 9.3             | 2.19            | 0.082           | -               |
| Fermat    | 0.028           | 28.8            | 1.24            | 0.054           | -               |
| Lagrange  | 0.048           | 12.2            | 1.71            | 0.076           | -               |
| Average   | 0.043           | 13.0            | 2.01            | 0.081           | -               |

(Note: The table is incomplete and needs to be filled with the remaining data.)