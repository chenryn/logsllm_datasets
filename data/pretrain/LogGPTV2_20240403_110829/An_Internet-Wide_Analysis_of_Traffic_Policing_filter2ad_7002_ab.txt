packet of length p arrives, if there are ≥ p tokens available,
the policer forwards the packet and consumes p tokens. Oth-
erwise it drops the packet.
Goal. The input to our algorithm is an annotated packet
ﬂow. Our analysis framework (§2.2) annotates each packet
to specify, among other things: the packet acknowledgement
latency, as well as packet loss and retransmission indicators.
Our goal is to detect when trafﬁc is policed, i.e., when
a trafﬁc policer drops packets that exceed the conﬁgured
rate. Our approach uses loss events to detect policing (as
described below).3 If a ﬂow requires fewer than m tokens,
policing will not kick in and drop packets, and we do not
attempt to detect the inactive presence of such a policer.
The output of the algorithm is (a) a single bit that speciﬁes
whether the ﬂow was policed or not, and (b) an estimate of
the policing rate r.
Detection. Figure 2 outlines our policing detector (PD, for
short). PD starts by generating the estimate for the token
2Whether tail-drop or those using some form of active queue management,
such as Random Early Drop (RED) or CoDel [24, 48].
3Since we rely on loss signals, we only detect policing when a ﬂow experi-
ences loss. To be robust against noise, we only run the algorithm on ﬂows
with 15 losses or more. We derived this threshold from a parameter sweep,
which found that lower thresholds often produced false positives. On aver-
age, ﬂows marked as policed in our production environment carried about
600 data packets out of which 100 or more were lost.
470
Variable: r (estimated policing rate)
Variable: pf irst_loss, plast_loss (ﬁrst/last lost packet)
Variable: tu, tp, ta (used/produced/available tokens)
Variable: lloss, lpass (lists of # tokens available when
packets were lost/passed)
else
allowed to not match policing constraints)
Variable: nloss, npass (fraction of lost/passed packets
1 r ←rate(pf irst_loss, plast_loss);
2 tu ← 0;
3 for pcurrent ← pf irst_loss to plast_loss do
Add ta to lloss;
tp ← r · (time(pcurrent) − time(pf irst_loss));
ta ← tp − tu;
if pcurrent is lost then
4
5
6
7
8
9
10
11 if average(ta in lloss) < average(ta in lpass)
12 and median(ta in lloss) < median(ta in lpass)
13 and |[ta ∈ lloss : ta ≈ 0]| ≥ (1 − nloss) · |lloss|
14 and |[ta ∈ lpass : ta (cid:38) 0]| ≥ (1 − npass) · |lpass|
15 and RTT did not increase before pf irst_loss then
16
Add ta to lpass;
tu ← tu+bytes(pcurrent);
Add trafﬁc policing tag to ﬂow;
Figure 2: Policing Detector
refresh rate r, as follows. We know that a policer drops
packets when its token bucket is empty. Assuming losses
are policer-induced, we know there were not enough tokens
when the ﬁrst loss (pf irst_loss) and last loss (plast_loss) hap-
pened within a ﬂow. All successfully delivered packets in-
between must have consumed tokens produced after the ﬁrst
loss. Thus, PD uses the goodput between the ﬁrst and last
loss to compute the token production rate (line 1).4 Our al-
gorithm is robust even if some losses have other root causes,
such as congestion, so long as most are due to policing.
Next, PD determines if the loss patterns are consistent with
a policer enforcing rate r. To do so, it estimates the bucket
ﬁll level as each packet arrives at the policer and veriﬁes if
drops are consistent with expectation. For this estimation, it
computes the following values for each packet between the
ﬁrst and the last loss (lines 3–10).
that passed through the policer already (line 10).
• The number of produced tokens tp, i.e., the overall
(maximum) number of bytes that a policer would let
through up to this point (line 4), based on the good-
put estimate and the elapsed time since the ﬁrst loss
(telapsed = time(pcurrent) − time(pf irst_loss)).
• The number of used tokens tu, i.e., the number of bytes
• The number of available tokens ta, i.e., number of bytes
that a policer currently would let through based on the
number of produced and already used tokens (line 5).
If the number of available tokens is roughly zero, i.e., the
4If the ﬁrst and/or last loss are not triggered by policing we potentially mis-
calculate the policing rate. To add robustness against this case, we always
run the algorithm a second time where we cut off the ﬁrst and last two losses
and reestimate the policing rate.
token bucket is (almost) empty, we expect a packet to be
dropped by the policer. Conversely, if the token count is
larger than the size of the packet, i.e., the token bucket accu-
mulated tokens, we expect the packet to pass through. The
exact thresholds depend on the goodput and the median RTT
of the connection to account for the varying offsets between
the transmission timestamp of packets that we record and the
arrival times at the policer.
Based on this intuition, PD infers trafﬁc policing if all
of the following conditions hold (lines 11–15). First, the
bucket should have more available tokens when packets pass
through than when packets are lost. Second, we expect the
token bucket to be roughly empty, i.e., ta ≈ 0 in the case of
a lost packet. This check ensures that losses do not happen
when the token bucket is supposed to have sufﬁcient tokens
to let a packet pass (ta (cid:29) 0), or when the token bucket
was supposed to be empty and have dropped packets earlier
(ta < 0). We allow a fraction of the samples (at most nloss)
to fail this condition for robustness against noisy measure-
ments and sporadic losses with other root causes. A similar
condition applies to the token counts observed when packets
pass through, where we expect that the number of available
tokens is almost always be positive. We allow fewer out-
liers here (at most npass < nloss) since the policer always
drops packets when the token bucket is empty. We derived
the noise thresholds nloss and npass from a parameter sweep
in a laboratory setting (§3.1) with a preference for keeping
the number of false positives low. For our analysis, we used
nloss = 0.1 and npass = 0.03. Finally, PD excludes cases
where packet losses were preceded by RTT inﬂation that
could not be explained by out-of-order delivery or delayed
ACKs. This check is another safeguard against false posi-
tives from congestion, often indicated by increasing buffer-
ing times and RTTs before packets are dropped due to queue
overﬂow.
By simulating the state of a policer’s token bucket and
having tight restrictions on the instances where we expect
packets to be dropped vs. passed through, we reduce the risk
of attributing losses with other root causes to interference by
a policer. Other causes, like congestion, transient losses, or
faulty router behavior, will, over time, demonstrate different
connection behaviors than policing. For example, while a
policed connection can temporarily achieve a goodput above
the policing rate whenever the bucket accumulates tokens, a
connection with congestion cannot do the same by temporar-
ily maintaining a goodput above the bottleneck rate. Thus,
over time the progress on connections affected by congestion
will deviate from progress seen on policed connections.
2.2 Analyzing Flow Behavior At Scale
We have developed, together with other collaborators within
Google, a pipeline for analyzing ﬂows at scale. The ﬁrst
step of this pipeline is a sampler that efﬁciently samples
a small fraction of all ﬂows based on 5-tuple hashes, cap-
turing all the headers and discarding the payload after the
TCP header. The sampler is deployed at most of Google’s
CDN servers and periodically transfers collected traces to
an analyzer backend in a datacenter. By running the analy-
471
sis online in a datacenter, we minimize the processing over-
head introduced on the CDN servers. As the traces arrive at
the analyzer backend, an annotator analyzes each ﬂow. We
designed the annotator to be broadly applicable beyond de-
tecting policing; for example, in §5.1.2, we use it to detect
trafﬁc shaping. For each trace, the annotator derives anno-
tations at the individual packet level (e.g., the RTT for the
packet, or whether the packet was lost and/or a retransmis-
sion), and at the ﬂow level (e.g., the loss rate and average
throughput experienced by the ﬂow).
It can also identify
application-level frames within a ﬂow, such as segments (or
chunks) in a video ﬂow. The annotator also captures more
complex annotations, such as whether a connection expe-
rienced bufferbloat [26]. PD is just one component of the
annotator: it annotates whether a segment was policed and,
if so, at what rate.
Developing these annotations was challenging. The an-
notation algorithms had to be fast since a single trace might
need several hundred annotations and we have many traces.
The more complex annotations also required signiﬁcant do-
main knowledge and frequent discussions with experienced
network engineers looking at raw packet traces and identify-
ing higher-level structures and interactions. Also complicat-
ing the effort were the complexity of the TCP speciﬁcation,
implementation artifacts, and application and network ele-
ment behavior that led to a very large variety in observed
packet traces. Our annotator is a signiﬁcant step in packet
analysis at scale beyond existing tools [10, 12, 45, 51, 53, 58,
61, 63]. Our analysis framework helped us explore policing
in the wild and was also helpful in iterating over different
designs of complex annotations. The framework can detect
CDN-wide anomalies in near real-time (e.g., when trafﬁc
from an ISP experiences signiﬁcant loss).
3. VALIDATION
We validate our algorithm in two ways. First, we evaluate
the accuracy of PD by generating a large set of packet traces
in a controlled lab setting with ground truth about the under-
lying root causes for packet loss (§3.1). Second, we show
that the policing rates in the wild are consistent within an
AS, meaning the AS’s traces marked as policed have good-
put rates that cluster around only a few values, whereas the
remaining traces see goodput rates that are dispersed (§3.2).
3.1 Lab validation
Our lab experiments are designed to stress-test our algo-
rithm. We generated a large number of packet traces while
using different settings that cover common reasons for dropped
packets, focusing on the ones that could elicit trafﬁc patterns
similar to a policed connection.
• Policing. We use a carrier-grade network device from
a major router vendor to enforce trafﬁc policing. We
conﬁgured the device in much the same way an ISP
would to throttle their users, and we conﬁrmed with
the router vendor that our conﬁgurations are consistent
with ISP practice. Across multiple trials, we set the
policing rates to 0.5, 1.5, 3, and 10 Mbps, and burst
Scenario
A Policing (except (B) and (C) below)
B Policing (special cases)
C Policing (multiple ﬂows)
D Congestion (all AQM schemes)
E Congestion (drop-tail, single ﬂow, except (G))
F Random loss
G Congestion (drop-tail, single ﬂow, min. queue)
H Congestion (drop-tail, multiple ﬂows)
Accuracy
93.1%
48.0%
12.3%
100.0%
100.0%
99.7%
93.2%
96.9%
Table 3: PD classiﬁcation accuracy for several controlled sce-
narios.
sizes to 8kB, 100kB, 1MB, and 2MB.
• Congestion. We emulate a bottleneck link which gets
congested by one or multiple ﬂows. We evaluated drop-
tail queueing and three active queue management (AQM)
schemes: CoDel [48], RED [24], and PIE [50]. We
varied bottleneck link rates and queue sizes across tri-
als using the same values as for the policing scenario.
• Random loss. We used a network emulator to ran-
domly drop 1% and 2% of packets to simulate the po-
tential behavior of a faulty connection.
We simulated trafﬁc resembling the delivery of data chunks
for a video download, similar to the type of trafﬁc we target
in our analysis in §4. Overall, we analyzed 14,195 chunks
and expected our algorithm to mark a chunk as policed if
and only if the trace sees packet loss and was recorded in
the Policing setting. Table 3 summarizes the results, with a
detailed breakdown of all trials available online [1].
Policed traces. PD was able to detect policing 93% of the
time for most policing conﬁgurations (A). The tool can miss
detecting policing when it only triggers a single large burst
of losses,5 or when the token bucket is so small that it allows
almost no burstiness and is therefore similar in behavior to
a low-capacity bottleneck with a small queue. We aggre-
gated these cases as special cases (B). PD is conservative in
order to avoid false positives for non-policed traces (D–H).
Consequently, we likely underestimate global policing lev-
els by failing to recognize some of the special cases (B).
We also analyzed the scenario where multiple ﬂows towards
the same client are policed together (C). For our in-the-wild
study (§4), PD typically does not have visibility into all ﬂows
towards a single client, as the CDN servers in the study in-
dependently select which ﬂows to capture. To emulate this
setting in our validation, we also only supply PD with a sin-
gle ﬂow, and so it can only account for some of the tokens
that are consumed at the policer. Therefore, its inference al-
gorithm is unable to establish a single pattern that is consis-
tent with policing at any given rate. Since we are interested
in a macroscopic view of policing around the globe, we can
tolerate a reduced detection accuracy for cases where clients
occasionally receive content through multiple connections
at the same time.6 We leave improving the algorithm’s ac-
curacy for this scenario to future work which would also re-
5Given only a single burst of losses, we cannot estimate a policing rate since
all losses happened at roughly the same time.
6For video transfers in our setting, most of the time only one connection
is actively transmitting a video chunk from the server to the client, even
though multiple connections are established between them.
472
Figure 3: Number of rate clusters required to cover at least
75% of the rate samples per AS.
quire the deployment of a different sampling method.
Non-policed traces. In our experiments, PD correctly classi-