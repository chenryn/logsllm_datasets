Representations (ICLR), 2014.
[51] D. Smilkov, N. Thorat, B. Kim, F. Viégas, and M. Wat-
tenberg. SmoothGrad: Removing Noise by Adding
Noise. In International Conference on Machine Learn-
ing Workshop, 2017.
[52] Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas
Brox, and Martin Riedmiller. Striving for Simplicity:
The All Convolutional Net. Proceedings of International
Conference on Learning Representations (ICLR), 2015.
[53] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Ax-
In Proceed-
iomatic Attribution for Deep Networks.
ings of IEEE Conference on Machine Learning (ICML),
2017.
[54] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence
to Sequence Learning with Neural Networks. In Pro-
ceedings of Advances in Neural Information Processing
Systems (NIPS), 2014.
[55] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
Jonathon Shlens, and Zbigniew Wojna. Rethinking the
Inception Architecture for Computer Vision. In Pro-
ceedings of IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2016.
[56] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever,
Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob
Fergus. Intriguing Properties of Neural Networks. In
Proceedings of International Conference on Learning
Representations (ICLR), 2014.
[57] Guanhong Tao, Shiqing Ma, Yingqi Liu, and Xiangyu
Zhang. Attacks Meet Interpretability: Attribute-Steered
Detection of Adversarial Samples. In Proceedings of
Advances in Neural Information Processing Systems
(NIPS), 2018.
[58] F. Tramèr, A. Kurakin, N. Papernot, I. Goodfellow,
D. Boneh, and P. McDaniel. Ensemble Adversarial
Training: Attacks and Defenses. In Proceedings of In-
ternational Conference on Learning Representations
(ICLR), 2018.
[59] Dimitris Tsipras, Shibani Santurkar, Logan Engstrom,
Alexander Turner, and Aleksander Madry. Robustness
In Proceedings of
May Be at Odds with Accuracy.
International Conference on Learning Representations
(ICLR), 2019.
[60] Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He,
Mingyan Liu, and Dawn Song. Spatially Transformed
Adversarial Examples. In Proceedings of International
Conference on Learning Representations (ICLR), 2018.
[61] W. Xu, D. Evans, and Y. Qi. Feature Squeezing: Detect-
ing Adversarial Examples in Deep Neural Networks. In
Proceedings of Network and Distributed System Security
Symposium (NDSS), 2018.
[62] Q. Zhang, Y. Nian Wu, and S.-C. Zhu. Interpretable Con-
volutional Neural Networks. In Proceedings of IEEE
Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2018.
[63] Quanshi Zhang, Ying Nian Wu, and Song-Chun Zhu.
Interpretable Convolutional Neural Networks. In Pro-
ceedings of IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2018.
[64] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Tor-
ralba. Learning Deep Features for Discriminative Lo-
calization. In Proceedings of IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2016.
1674    29th USENIX Security Symposium
USENIX Association
Appendix
A. Implementation Details
A1: Details of StAdv-based ADV222
We ﬁrst brieﬂy introduce the concept of spatial transforma-
tion. Let ˜xi be the i-th pixel of adversarial input ˜x and ( ˜ui, ˜vi)
be its spatial coordinates. With ﬂow-based transformation, ˜x
is generated from another input x by a per-pixel ﬂow vector r,
where ri = (∆ui,∆vi). The corresponding coordinates of ˜xi in
x are given by (ui,vi) = ( ˜ui + ∆ui, ˜vi + ∆vi). As (ui,vi) do not
necessarily lie on the integer grid, bilinear interpolation [25]
is used to compute ˜xi:
˜xi =∑
x j max(0,1−| ˜ui +∆ui−u j|)max(0,1−| ˜vi +∆vi−v j|)
j
where j iterates over the pixels adjacent to (ui,vi) in x. With
STADV as the underlying attack framework, ADV2 can be
constructed as optimizing the following objective:
(cid:107)∆ui − ∆u j(cid:107)2
2 +(cid:107)∆vi − ∆v j(cid:107)2
where (cid:96)ﬂow(r) = ∑i ∑ j∈N (i)
2
measures the magnitude of spatial transformation and τ is a
hyper-parameter controlling its importance. In implementa-
tion, we solve Eqn (14) using an Adam optimizer.
A2: Details of AID
We use RTS as a concrete example to show the implemen-
tation of AID. In RTS, one trains a DNN g (parameterized by
θ) to directly predict the attribution map g(x;θ) for a given
input x. To train g, one minimizes the interpretation loss:
(cid:96)int(θ) (cid:44)λ1rtv(g(x;θ)) + λ2rav(g(x;θ))− log ( fc (φ(x;g(x;θ))))
λ4
+ λ3 fc (φ(x;1− g(x;θ)))
(15)
with all the terms deﬁned similarly as in Eqn (8).
In AID, let A denote the ADV2 attack. We further consider
an adversarial distillation loss:
(cid:96)aid(θ) (cid:44) −(cid:107)g(x;θ)− g(A(x);θ)(cid:107)1
(16)
which measures the difference of attribution maps of benign
and adversarial inputs under the current interpreter g(·;θ).
AID trains g by alternating between minimizing (cid:96)int(θ) and
minimizing (cid:96)aid(θ) until convergence.
B. Parameter Setting
Here we summarize the default parameter setting for the
attacks implemented in this paper.
B1. PGD-based ADV222
For regular PGD, we set the learning rate α = 1./255 and
the perturbation threshold ε = 0.031. Table 10 list the param-
eter setting of PGD-based ADV2.
min
r
(cid:96)prd( f (x + r),ct ) + λ(cid:96)int(g(x + r; f ),mt ) + τ(cid:96)ﬂow(r) (14)
MASK
(cid:113)
GRAD
CAM
MASK
RTS