on the binaries appearing on machines, with an effort to identify
specific behaviors that predict future infections. We do not seek to
pinpoint the exact causes of infections, but rather characteristics
that are correlated with them and that are useful to represent how
the machine is used; hence, many features we analyze are chosen
to represent the security awareness of the machine’s users, and
the usage patterns of those machines. For example, we extract
a number of features about the patching behavior with respect
to a limited number of applications. While the direct cause of a
particular infection could be a vulnerability of an application which
is not included in our analysis, a higher-level explanation would be
that the user(s) of the machine do not patch existing vulnerabilities
promptly enough.
As mentioned before, the binary appearance logs consist of meta-
data collected from the end-hosts regarding all new binaries that ap-
peared during the period of analysis. While in general a majority of
these binaries are downloaded from various sources, some of them
could be generated on the host itself: some benign examples are
binaries compiled on the system by users who are application devel-
opers, or those created by specific applications such as web servers.
Unfortunately, our datasets do not allow us to distinguish which
binaries are downloaded or copied from other sources, and which
ones are created on the computer; however, our category-based fea-
tures (Section 4.1.4) allow us to distinguish machines based on the
type of applications installed, capturing implicitly the main reason
of binary appearance. For the sake of brevity, hereinafter, we will
refer to all binary downloads/installations/compilations/creations
as binary appearance events, or simply events.
In the following, we explain how we preprocess data to prepare
for the feature extraction step and provide details about the features
we use for prediction.
4.1 Feature Discovery
After the data preprocessing step, we obtain a list of events, each
representing the appearance of a new binary on a machine dur-
ing our analysis window. For each machine, we create a profile
consisting of 89 different features synthesized from these events,
which we use to predict each machine’s risk of future infection.
Table 1 provides a comprehensive list of these features, grouped by
categories. In the following, we describe each category.
4.1.1 Volume-Based Features. Our first category of features cov-
ers general statistics calculated from new binaries appeared during
our analysis window. We include the total number of events (i.e.,
Session F2:  Insights from Log(in)sCCS’17, October 30-November 3, 2017, Dallas, TX, USA1301Table 1: RiskTeller features for predictive modeling.
Features
# of events, # of distinct file hashes/filenames
fraction of events from top signers/top file hashes, average # of events per active day
# of distinct applications, quartiles of per-application fraction
fraction of events during daytime/evening/night/weekdays/weekends
diurnal # of events: median/standard deviation
monthly # of events: median/standard deviation
# of patched vulnerabilities/applications, the most patched application
quartiles of CVSS scores for patched vulnerabilities
quartiles of the vulnerability window length for patched applications
# of vulnerabilities, unpatched applications, app with highest vulnerability count
quartiles of CVSS scores for unpatched vulnerabilities
quartiles of the vulnerability window length for unpatched applications
top-5 application categories with most events
fraction of events per top-5 category
fraction of system diagnostics tools
fraction of system administration tools
fraction of attack tools
Feature Category
Feature #
Volume-based
(§ 4.1.1)
Temporal (§ 4.1.2)
Vulnerabilities/patching
(§ 4.1.3)
Application
categories (§ 4.1.4)
Infection history
(§ 4.1.5)
Prevalence-based
(§ 4.1.6)
1–3
4–6
7–12
13–17
18–19
20–21
22–24
25–29
30–34
35–37
38–42
43–47
48–52
53–57
58
59
60
65–69
64
70
71–74
61–63
fraction of events for malicious/benign/unknown files
fraction of events with singleton signers
fraction of events with prevalence [1, 10]/[11, 100]/[101, 1000]/[1 001, 10 000]/[10 001,∞) signers
fraction of events with signers seen in only one enterprise
fraction of events with signers seen in [1, 10]/[11, 100]/[101, 1 000]/[1 001,∞) enterprises
fraction of prevalence-1 files
fraction of prevalence [1, 10]/[11, 100]/[101, 1 000]/[1 001,∞) files
fraction of files seen only in one enterprise
fraction of files seen on [1, 10]/[11, 100]/[101, 1 000]/[1 001,∞) enterprises
fraction of files seen only on one machine
fraction of files seen on [1, 10]/[11, 100]/[101, 1 000]/[1 001,∞) machines
76–79
81–84
75
80
85
86–89
appearance of a new binary on a machine), of distinct binaries (rec-
ognized through SHA2 file hashes), filenames, and of applications
on each host.
We identify the most popular software vendors and the most
frequently appearing binaries by identifying the 50 most frequently
appearing file signers and 150 most frequent file hashes respectively
querying the whole data; we then compute the fraction of events
pertaining to these top signers and hashes. To capture the level of
activity while the machine is used, we also compute the average
number of binary appearance events during each day in which
events were generated.
Previous work has shown that people with abundant and varied
browsing behavior suffer higher risks [4]: to identify similar pat-
terns with respect to binaries appearing on machines, we compute
the percentage of events generated by each application (identified
as described in Section 3.1), and synthesize these values through
6 values: the number of distinct applications, plus the 5 quartiles
of the per-application percentage of events: minimum, maximum,
median, plus 25th and 75th percentiles.
4.1.2 Temporal Behavior. We aim to understand whether longer
working hours or working outside the official working hours is
correlated with facing higher risk to encounter malware infections.
Our hypothesis is that people who generally use their machines
during weekends or in the evenings are more likely to use their
machines for personal, more varied purposes in addition to work-
related ones, possibly engaging in riskier activities that might result
in malware infections. As we will show in the following, it is indeed
the case that machines with more binary appearance events during
night time are more risky.
The features we extract are the fraction of events that happen
during the day, evening, night, weekdays and weekends (notice that
the timestamps we use correspond to local time, therefore timezone
differences do not affect the accuracy of these features). We consider
daytime as the 06:00–18:59 interval, evening as 19:00–00:59, and
night as 01:00–05:59. According to the customs of the majority of
the world, we define weekdays as Monday to Friday, and weekends
as Saturdays and Sundays. Additionally, to capture the regularity
or irregularity of machine usage in time, we compute the median
Session F2:  Insights from Log(in)sCCS’17, October 30-November 3, 2017, Dallas, TX, USA1302Table 2: Applications with vulnerable versions identified.
Vendor
Adobe
Google
Microsoft
Mozilla
Oracle
Product
Air
Flash Player
Reader
Chrome
Internet Explorer
Silverlight
Skype
Firefox
MySQL
# CVE IDs
128
3 708
261
806
1 018
36
28
9 536
108
and standard deviation for the number of events observed each day
(diurnal) and each month (monthly).
4.1.3 Vulnerabilities and Patching Behavior. Based on the history
of binaries that appear on each machine, we can infer when they
have installed vulnerable application versions, and extract features
about the severity of vulnerabilities and the vulnerability windows:
time intervals during which machines are running software with
known vulnerabilities. As common sense suggests, the vulnerability
patching behavior and the severity of existing vulnerabilities on
machines can be highly correlated with the probability of future
malware infections. Indeed, our results support this, as we find
that not patching vulnerabilities, besides indicating low security
awareness, is a very good predictor of future infection risk.
Our analysis includes known vulnerabilities for 9 different ap-
plications, described in Table 2. As discussed in the following, the
process of matching vulnerability information with data from na-
tional vulnerability database (NVD) and our file appearance logs
data requires a non-negligible manual effort; therefore, we focus on
applications which are widely installed, and often exploited through
vectors such as drive-by-downloads, e-mail attachments, etc. [23].
A comprehensive overview of all vulnerable applications installed
on a machine would be essentially unfeasible with the datasets we
have; we consider, however, that identifying the patching behavior
with respect to the widely used applications listed in Table 2 is
sufficient to understand and capture user behavior, and that their
patching behavior for other applications is likely to be similar to
the one for the applications we consider.
Following the spirit of the work by Nappa et al. [22], we first
identify software through a manually defined leading filename com-
bined with information about the file signer (e.g., we consider that
Google Chrome is identified by binaries named chrome.exe and
signed by Google); we then obtain file version information from the
file logs and/or VirusTotal.2 We obtain information about vulnera-
bilities through the NIST’s National Vulnerability Database (NVD).3
By parsing NVD data, we obtain information about vulnerable file
versions and the severity of each vulnerability through the CVSS
2https://www.virustotal.com/
3https://nvd.nist.gov/
Figure 1: Fraction of vulnerable machines and their sever-
ity for a single company. In this period, a large fraction of
severe vulnerabilities were due to Adobe Flash.
(Common Vulnerability Scoring System) score,4 which ranks vul-
nerabilities through scores between 0 and 10, where high-severity
vulnerabilities have scores of at least 7. We manually resolved in-
consistencies between version numbers in the file appearance logs
or VirusTotal and those in the NVD database.
Thanks to this process, we are able to identify the software
versions installed on each machine and known vulnerabilities of
those pieces of software. In many cases, severe vulnerabilities on
widely installed software (e.g., Adobe Flash) result in large numbers
of vulnerable machines, and vulnerable software versions often
require up to months to become updated (see Figure 1).
Once we are able to identify vulnerable versions of applications
together with the time they were installed and updated, we extract
26 features about the severity of patched and unpatched vulnerable
applications we observe in the log, the time it takes for the user to
patch known vulnerable applications and the window of exposure
for the remaining unpatched vulnerabilities. Details about each
feature can be found in Table 1: we remind that with quartiles, we
refer to minimum, maximum, median, plus 25th and 75th percentiles.
4.1.4 Application Category-Based Features. We extract a set of
features related to the categories of applications seen on machines,
based on the intuition that these categories can be used for machine
profiling, and different machine profiles suffer different risk levels
from cyber-threats. Some previous works have already touched
upon machine profiling in other contexts: [36] assessed the risk of
different job profiles with respect to targeted attacks; [22] focused
on the vulnerability patching behavior of three user profiles (soft-
ware developers, professionals, security experts); [24] described a
correlation between cyber-attacks and three predefined user pro-
files (professionals, software developers and gamers). Motivated
by these works, we perform a detailed analysis to obtain a set of
machine profiles corresponding to the type of applications installed:
our goal is to understand which specific machine profiles are more
4https://nvd.nist.gov/cvss.cfm
Aug’15Sep’15Oct’15Nov’15Dec’15Jan’16Feb’160%10%20%30%40%50%Vulnerablemachines012345678910SeveritySession F2:  Insights from Log(in)sCCS’17, October 30-November 3, 2017, Dallas, TX, USA1303prone to encounter cyber-attacks, and whether features including
the most downloaded application categories on machines can be
useful for our predictive model.
Table 3: Application categories.
Category
Architecture
Asset Management
Automobile
Bank
Business
Chat
Chemical
Construction
Sales
Data / DB
Education
Engineering
Finance
# of Apps
59
574
172
166
1 266
87
29
371
1 050
254
101
73
1 206
Category
Government