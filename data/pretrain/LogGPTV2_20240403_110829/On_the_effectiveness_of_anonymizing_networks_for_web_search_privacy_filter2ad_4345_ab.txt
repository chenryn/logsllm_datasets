Google, making themselves identiﬁable even while using Tor. Sim-
ilarly, a signiﬁcantly large fraction of users may not disable their
cookies, due to unawareness about tools like PWS [17]. In summa-
ry, even if there are 1500 Tor exit nodes in total and a large number
of users might be using Tor for private web search, only a smal-
l fraction of these users actually remain anonymous to the search
engine. In addition, the search engine might not want to track each
and every one in this anonymous user set, but instead it might want
to concentrate on few users - selected based on the kind of sensi-
tive queries they send or based on their real world identities (like
suspected terrorists). In light of these important observations, we
consider a maximum of N = 1000 anonymous web search users,
and try to associate the queries in Q to these users. We believe that
this number 1000 is reasonable for experimental purposes.
Let us assume that the search engine is interested in identifying
the queries corresponding to an user of interest, A. Let us say that
Q contains nu number of A’s queries and no number of other users’
queries (note that generally nu . In our model, since we work
with the query content alone and do not concentrate on addition-
al clickthrough patterns of the user, we neglect the ItemRank and
ClickURL features.
The AnonymousID feature is used for distinguishing the AOL
users and is treated as the Class Label for classiﬁcation. Since the
Query feature is a string and WEKA can not handle strings directly,
we converted the strings to word vectors using the in-built WEKA
preprocessing ﬁlter StringToWordVector. We added another feature
Query Length, as described in Section 3.1, though it is implicit in
the Query information. Since time feature cannot be used directly
because of the inherent delay when queries are sent over Tor, we
considered timing windows of considerable duration. Since it is
hard to predict what size of the timing window might provide better
results, we divided 24 hours in a day into different non-overlapping
windows of sizes of 3, 4, 6 and 12 hours and compared the accura-
cies with each timing window size.
AnonymousID and Query are necessary attributes and in order
to determine the impact of each additional attribute on the classi-
ﬁcation results, we tried to identify the average accuracy of all the
users of interest when N=100, by including one additional attribute
at a time. The average accuracies are indicated in Table 1. We can
see that by including the Query Length feature reasonable perfor-
mance is achieved both in the case of OVA and AVA. Addition of
timing windows did not provide much improvement over the exist-
ing accuracies, both in the case of OVA and AVA. There could be
other possible and better uses of these query times, but we neglect
them for now. Hence for all the following experiments we included
Query Length as an additional attribute along with Query and the
AnonymousID.
4. EXPERIMENTS AND RESULTS
In our experiments, we tried to estimate the accuracy of the clas-
siﬁers in correctly identifying queries of 60 users of interest. For
each user of interest, we measure the accuracy across ﬁve datasets,
where in each dataset, we vary the number of ‘other users’ whose
queries are mixed with that of the current user of interest. The ﬁve
datasets containing randomly selected 99, 199, 299, 499 and 999
other users were generated. In order to be consistent across all 60
users of interest, we used the same ‘other user’ datasets. Thus,
when the user of interest’s query set is mixed with the queries of
these ‘other users’, we form datasets with N as 100, 200, 300, 500
and 1000 users.
For every user of interest A, we intuitively call the fraction of
correctly identiﬁed user A’s queries (denoted as x in Section 2) as
Correctly Classiﬁed and the fraction of other users’ queries incor-
rectly classiﬁed as user A’s queries (denoted as y in Section 2) as
Misclassiﬁed (These terms are not in accordance with the standard
Machine Learning deﬁnitions). As discussed in Section 2, we want
the Correctly Classiﬁed value to be as high as possible and Mis-
486
Additional Attributes
Classiﬁer Accuracies – No Accuracies – Including Accuracies – Including Timing window
3 hrs
12 hrs
13.16% 14.08% 13.62% 14.41%
13.98% 12.99% 12.63% 14.15%
16.26%
13.65%
14.58%
14.41%
AVA
OVA
Query Length
4 hrs
6 hrs
Table 1: Comparison of Accuracies for Attribute Selection
Figure 4: OVA Results Summary for Number of Queries
classiﬁed value to be as low as possible, and consider it to be an
optimal measure of the performance.
We obtained the results for all the 60 users of interest belonging
to the three categories discussed in Section 3.2. For each category,
we summarized the OVA results indicating the average values of
Correctly Classiﬁed and Misclassiﬁed for all the users of interest in
speciﬁc sub-categories. The summary of OVA results for Number
of Queries is given in Figure 4, summary of OVA results for Query
Length is given in Figure 5 and the summary of OVA results for
Sensitive Queries is depicted in Figure 6. The results for AVA clas-
siﬁcation, for each category, came out to be very similar to that of
OVA classiﬁcation, and are thus not reported in the paper.
5.
INTERPRETATION AND DISCUSSION OF
RESULTS
words, using stemming algorithms present in WEKA, we identiﬁed
the ‘root’ keywords appearing in the queries and sorted them in the
decreasing order of occurrence. The distribution can be seen in
Figure 7. There were 28659 root keywords(i.e. stemmed words) in
total and of these only 4797 root keywords had more than 10 occur-
rences. This distribution mimics the ‘Long Tail’ behaviour of web
search queries, as discussed in [6], where each user is considered
a bit eccentric and is expected to send both common queries (all
root keywords with more than 10 occurrences in the distribution)
and a few unique queries (at least one keyword with less than 10