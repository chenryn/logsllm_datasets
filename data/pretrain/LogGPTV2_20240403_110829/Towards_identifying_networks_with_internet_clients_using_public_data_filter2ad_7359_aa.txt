title:Towards identifying networks with internet clients using public data
author:Weifan Jiang and
Tao Luo and
Thomas Koch and
Yunfan Zhang and
Ethan Katz-Bassett and
Matt Calder
Towards Identifying Networks with Internet Clients
Using Public Data
Weifan Jiang∗
Columbia University
Yunfan Zhang
Columbia University
Tao Luo∗
Columbia University
Ethan Katz-Bassett
Columbia University
Thomas Koch
Columbia University
Matt Calder
Microsoft / Columbia University
ABSTRACT
Does an outage impact any users? Can a geolocation database
known to be good at locating users and bad at infrastructure be
trusted for a particular prefix? Is a content-heavy network likely to
peer with a particular network? For these questions and many more,
knowing which prefixes contain Internet users aids in interpreting
Internet analysis. However, existing datasets of Internet activity
are out of date, unvalidated, based on privileged data, or too coarse.
As a step towards identifying which IP prefixes contain users, we
present multiple novel techniques to identify which IP prefixes
host web clients without relying on privileged data. Our techniques
identify client activity in ASes responsible for 98.8% of Microsoft
CDN traffic and in prefixes responsible for 95.2% of Microsoft CDN
traffic. Less than 1% of prefixes identified by our technique as active
do not contact Microsoft at all. We present measurements of Internet
usage worldwide and sketch future directions for extending the
techniques to measure relative activity levels across prefixes.
CCS CONCEPTS
• Networks → Network measurement.
KEYWORDS
Network Mapping, Replicable, Internet Measurement.
ACM Reference Format:
Weifan Jiang, Tao Luo, Thomas Koch, Yunfan Zhang, Ethan Katz-Bassett,
and Matt Calder. 2021. Towards Identifying Networks with Internet Clients
Using Public Data. In ACM Internet Measurement Conference (IMC ’21),
November 2–4, 2021, Virtual Event, USA. ACM, New York, NY, USA, 10 pages.
https://doi.org/10.1145/3487552.3487844
1 INTRODUCTION
Internet researchers would benefit from knowing which networks
host users. This information is key to interpreting research results
and operational aspects, and to weighting analysis. The impact of an
outage, a slow route, or a network being added to a blocklist [4, 30]
∗These authors were primary contributors as Columbia students. They since started
Ph.D. programs at Harvard University (Weifan) and University of Pennsylvania (Tao).
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
IMC ’21, November 2–4, 2021, Virtual Event, USA
© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-9129-0/21/11...$15.00
https://doi.org/10.1145/3487552.3487844
753
varies depending on whether the network has users. For a concrete
illustration of how knowledge of which networks host users can
impact analysis, consider 2015 analysis on Internet path lengths
from Google [11]. Google peered directly with 41% of networks
overall, but with 61% of networks hosting end users. Although most
research historically considers all networks and routes equivalently
(e.g., consider how many CDFs across networks one has seen in
IMC papers), this example shows that knowledge of where Internet
users are can give a very different impression of even seemingly
simple questions like “how long are paths from the cloud?’ ’ These
differences can have important implications. When conducting
research on how to optimize cloud performance for users, one
might arrive at very different solutions if under the impression that
most routes pass through intermediate networks (when considering
all networks, only 41% do not) versus most routes being direct
(when considering user networks, 61% are). As another example,
geolocation databases like MaxMind are more accurate for end-user
networks [16], and so knowing which networks host end-users
provides insight into which geolocation results are trustworthy.
Recognizing how this knowledge can impact conclusions, some
research uses various data sources of end-user activity, but the
existing datasets are private, unvalidated, out of date, or opaque. The
2015 study mentioned above used a private CDN dataset [11] that
cannot be shared and is now out of date. One study used IP prefixes
seen in BitTorrent swarms [12] to indicate those prefixes hosted
users and hence could be trusted for geolocation [7]. However,
BitTorrent is no longer as popular. The ISI Internet Census data
measures Internet activity in terms of responsive addresses [18],
and so is not suited for inferring which networks have users.
APNIC’s network population data [19] based on ad impressions
has been used in studies [5, 6, 17, 22], but APNIC’s methodology has
a number of limitations. First, the approach has not been validated
(to the best of our knowledge). Second, APNIC aggregates data
at an AS granularity, which is too coarse-grained for use cases
that require prefix-level information, like our geolocation example
above, and threat-intelligence solutions. Third, the approach, which
relies on placing Google Ads and collecting the IP addresses of users
to which they are shown, is expensive. One study spent $5000 and
only observed 8,589 distinct user IP addresses [27], a small fraction
of the user prefixes (much less addresses) active in the world. This
cost may make it prohibitive for other groups to set up similar
measurements that address the limitations of APNIC and then keep
the results updated over time. Fourth, network coverage is difficult
to control due to the non-deterministic nature of the ad-bidding
process, even when making use of targeted advertisements. Finally,
IMC ’21, November 2–4, 2021, Virtual Event, USA
Weifan Jiang, Tao Luo, Thomas Koch, Yunfan Zhang, Ethan Katz-Bassett, and Matt Calder
ad-based measurements raise ethical concerns due to the inability
to gather informed user consent before launching the campaigns.
New techniques. We propose two new, replicable techniques that
estimate which networks host active Internet users by identifying
which contain web clients. Some clients may be bots and crawlers,
but most networks with clients contain (human) users, and so our
work is an important step towards the goal of identifying networks
hosting users. Our two techniques fundamentally differ and have
different tradeoffs.
Our first technique probes Google Public DNS caches to infer
which prefixes have been querying for particular domains (§3.1).
If a prefix queries for user-facing services, we infer that it likely
hosts users. Our technique relies on the fact that Google Public
DNS accepts queries that specify an EDNS0 Client Subnet (ECS)
prefix, which means that Google only returns a cached DNS record
if an IP address from that prefix previously queried for the same
domain. This mechanism allows us to scan for activity over the
entire IPv4 address space. Because Google Public DNS is anycast, it
will only cache for an ECS prefix at the Google site where anycast
routes the prefix’s queries, and so we geo-distribute our probing to
reach different sites. We are the first to probe Google Public DNS
with ECS queries to discover Internet activity worldwide.
Our second technique crawls traces from the root DNS servers
to find queries from Chromium-based browsers, since networks
sourcing significant Chromium queries likely host users (§3.2). As
we explain in more detail below, Chromium-based browsers use
DNS probes to detect DNS interception [35]. Chromium sends these
probes whenever it is launched or whenever the device IP or DNS
configuration changes, and the domains probed are designed to not
be cached, and so a count of these queries is a good approximation
of Chromium usage in a network. Since Chromium-based browsers
represent a large majority of browser usage, Chromium usage is a
good approximation of web browser usage.
Validation of existing and new techniques. We compare the results
from our techniques to each other, to APNIC network population
estimates and to server-side logs of client IP addresses from Mi-
crosoft (§4). CDN client data offers the broadest view of Internet
activity at the AS level, capturing 97% of all ASes seen using any
method, but is not widely available to researchers. Our methods
identify 29,973 ASes containing clients not seen by APNIC. More-
over, our results suggest that most prefixes in at least 15% of ASes
do not contain clients. Hence, AS-level activity data such as AP-
NIC is too coarse to understand activity at the IP-level. Finally, we
compare to Microsoft CDN logs and find that the prefixes identified
by our techniques as hosting web clients are responsible for 95.2%
of queries to Microsoft, with 99.1% of them sending at least some
queries to Microsoft. Hence, our methods can identify client prefix
activity with precision and broad, global coverage rivaling a major
cloud/CDN provider.
We conclude with a brief roadmap sketching future work for
going from our lists of active client prefixes to relative activity
levels across prefixes (§6).
754
2 GOALS
Building a map of Internet activity could come in different forms
suited for answering different questions. For this preliminary in-
vestigation, we prioritize the following goals and discuss tradeoffs:
Focus on client activity. Measuring client-driven activity provides
a basis to assess how Internet events and properties affect clients.
By identifying clients of popular user-facing services, we seek to
approximate the (human) user activity (so-called eyeballs) that is
our ultimate goal. Users are clients, but we do not yet know how
to filter out all non-human clients such as bots and crawlers.
Use replicable approaches. Prior work which used Internet activ-
ity measures to answer research questions often used privileged
data sets that cannot be shared with the community [11, 21]. We
want to base our map on datasets/techniques accessible to other
researchers, and we are happy to share our data (except proprietary
data we use for validation). Replicable approaches and public data
help more researchers tackle difficult problems and allow future
work to directly compare findings, but they may not be able to
achieve the coverage of private datasets.
Provide fine-grained global coverage. We aim to cover as many
client networks as possible, in terms of countries, ASes, and pre-
fixes. A fine-grained map in time and network allows researchers
to answer questions about time of day effects, and the effects of In-
ternet events on specific geographic areas. This focus means we do
not consider methods that are effective only in certain networks or
regions, and we exclude methods that operate on the AS granularity,
since it is too coarse (§4). We do not yet consider IPv6.
3 MEASUREMENT METHODS
3.1 Probing DNS caches for client activity
When users access websites they often issue DNS queries, populat-
ing caches in users’ recursive resolvers. Our first approach, referred
to as cache probing, is based on DNS cache snooping for activity
from clients around the world. In DNS cache snooping, one sends a
non-recursive DNS request to a recursive resolver. If the resolver
returns a record, it must have had the record in cache (because the
query was non-recursive), meaning a client of the recursive must
have queried for the record (within the record’s starting TTL).
One possible approach would be to try to cache snoop recursive
resolvers in ISPs around the world [2, 7, 33]. However, the number
of recursive resolvers that respond to queries from outside their
ISPs has significantly reduced over time [25, 28], denying our goal
of global coverage. One study overcame this limitation by probing
misconfigured customer-premises equipment that would forward
to recursive resolvers and return the result to the prober [26]. The
study found such open forwarders in 4,905 ASes which, while
substantial, is far below our goal of global coverage.
Instead, we leverage Google Public DNS, which has multiple
advantageous properties. First, it is extremely popular as a recursive
resolver, contributing 30-35% of all DNS queries to Microsoft Azure
authoritative DNS servers in January 2019 [9]. Second, it supports
EDNS0 Client Subnet (ECS), a DNS extension in which the recursive
resolver includes a prefix of the querying client’s IP address as
part of the query [13], enabling client-specific responses [10]. An
Towards Identifying Networks with Internet Clients Using Public Data
IMC ’21, November 2–4, 2021, Virtual Event, USA
implication of Google supporting ECS is that it has to maintain
separate cache entries per client prefix for domains that support
ECS. Third, although normally the recursive resolver sets the ECS
prefix based on the IP address of the querying client, if a client
query instead includes an ECS prefix (not necessarily related to the
client’s IP address), Google Public DNS will use the supplied prefix
in its queries instead.1 We verified this behavior by sending queries
for a domain for which we operate the authoritative resolver.
3.1.1 Methodology. We issue queries to Google Public DNS, vary-
ing the ECS prefix to scan the entire IPv4 space. A cache hit for
⟨prefix, domain⟩ suggests that a client in prefix issued a query
for domain. Our non-recursive queries do not pollute the cache. In
addition to our validation above, a recent study also verified that
Google Public DNS does not query authoritative DNS servers on
cache misses if the recursion desired flag is set to off [31].
Realizing our approach requires addressing a number of chal-
lenges. First, Google Public DNS has PoPs around the world, each
with a set of independent caches, and so it is necessary to query
the PoP that any clients in a prefix would use to determine whether
they have accessed a domain. Google Public DNS relies on anycast
to direct clients to a PoP, so we must issue queries from around the
world to probe the behavior of clients around the world. Anycast
does not always route clients to the nearest PoP [8, 21, 24], and so
it can be challenging to know which PoP to query for a particular
ECS prefix. Second, since records are only cached for the duration
of their TTLs, which vary by record, which domains are queried
for when can impact which ECS prefixes return cache hits.
Identifying candidate prefixes for ECS queries. Since ECS rarely
uses prefix scopes more specific than /24 [34], we start with the
set of 15,527,909 public /24 prefixes (≈ 12𝑀 of which are currently
routed). We use a technique to reduce probing overhead. Authori-
tative resolvers often return a less specific prefix scope for an ECS
response than the request [34], meaning that the recursive resolver
can cache and return the response for all addresses with the re-
turned scope. If a returned scope is less specific than a /24, we
need not query for other /24s within the less specific. While it is
straightforward for us to use this observation to reduce probing if a
Google Public DNS cache hit for a /24 query returns a less specific
scope, a cache miss does not inform us whether or not other nearby
/24 prefixes are worth querying. So, we first issue queries directly
to the authoritative resolver to learn the scope it returns for the full
address space, then use these returned scopes as our query scopes to
Google Public DNS. For example, if a query to the authoritative for
a.b.c.0/24 returns a scope of a.b.0.0/16, we only issue queries
to Google for a.b.0.0/16, saving the overhead of issuing probes
for each /24 in the /16. Appendix A.2 validates this approach.
Geo-distributing measurements to probe caches worldwide. To
probe caches worldwide, we need to issue queries from locations