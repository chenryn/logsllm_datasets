further obtained the dataset of BayesDroid [42], which contains
54 of the most popular apps from the Google Play Store in
2013.
C. Characterizing Non-Determinism in Network Trafﬁc
One key aspect of our work is being able to characterize and
explain non-determinism in network trafﬁc. In fact, we want to
distinguish what changes “no matter what” and what changes
“exactly because we modiﬁed the input.” First, we show that
trivially applying approaches based on differential analysis is
ineffective when applied to modern Android apps. Second, our
technique allows us to pinpoint which apps are problematic, i.e.,
for which apps we cannot determine why the network output
changes. In this case, we cannot reliably correlate the differences
in output with the differences in input and, therefore, we ﬂag
them as potentially leaking private information. We note that we
can adopt this conservative aggressive policy only because we
rarely encounter inexplicable differences in the network trafﬁc
of apps that do not leak private information. In other words,
changes in network trafﬁc that cannot be explained by our
system are strong indicators that private information is leaked.
To demonstrate how poorly a na¨ıve differential analysis ap-
proach without considering any network-based non-determinism
would perform, we analyzed the 100 popular Google Play apps
from the ReCon dataset twice: the ﬁrst time, we trivially applied
the differential analysis without
leveraging any contextual
information; the second time, instead, we applied our full
approach, executing the apps in our instrumented environment
and exploiting the collected contextual information. In both
cases, we measured the number of runs needed to converge,
setting 20 as the maximum number of runs.
Figure 6 shows the cumulative distribution functions of
the number of runs required to reach convergence in the two
scenarios. While in the ﬁrst case almost all the apps did not
reach convergence (within a maximum number of 20 runs), our
approach correctly handled most of the cases. This demonstrates
two things: (1) network trafﬁc is very often non-deterministic,
(2) in most cases, the contextual information recorded during
the app’s analysis is enough to determine the real source of
non-determinism.
In order to further conﬁrm this ﬁnding, we evaluated how
the number of runs per app affects the number of apps for which
1
0.8
F
D
C
0.6
0.4
0.2
4
6
8
10
12
#runs
14
16
18
20
Fig. 6. Cumulative distribution function (CDF) of the number of runs required
for convergence (for K = 3) applying AGRIGENTO’s full approach (solid line),
and the trivial differential analysis approach (dashed line) that does not consider
any non-determinism in the network behavior.
s
p
p
a
%
0.8
0.6
0.4
0.2
0
5
10
#runs
15
20
Fig. 7.
Percentage of apps with non-deterministic network trafﬁc in an
increasing number of runs when applying AGRIGENTO’s full approach (solid
line), and the trivial differential analysis approach without leveraging contextual
information (dashed line).
AGRIGENTO cannot completely explain some source of non-
determinism. To do so, we performed a ﬁnal execution without
altering any source of private information, and measured the
number of apps that contained non-determinism in the network
trafﬁc (i.e., the number of apps for which AGRIGENTO raised
an alert). Figure 7 shows that, in contrast to our full approach,
when applying the differential analysis trivially, increasing the
number of runs is not enough to reduce non-determinism (82.1%
of the apps generated non-deterministic network trafﬁc).
Finally, we evaluated how the choice of K (i.e., the number
of consecutive runs without discrepancies considered to reach
convergence) affects AGRIGENTO’s ability to explain non-
determinism. We performed the evaluation on two datasets:
the 100 most popular apps from the Google Play Store and
100 randomly selected less popular apps from the Google
Play Store. We run the analysis without altering any source
of private information. By doing this, any alert is caused by
the fact that there is some non-determinism in the network
trafﬁc that AGRIGENTO could not explain. Table I shows that
K = 3 minimizes the number of apps with unexplained non-
determinism in their network trafﬁc, at the cost of a small
increase in the average number of runs required per app. This
evaluation also shows that the popular apps indeed seem to
9
TABLE I.
CHOICE OF K (= NUMBER OF CONSECUTIVE RUNS TO REACH
CONVERGENCE) AND ITS EFFECT ON THE AVERAGE NUMBER OF RUNS PER
APP, AND NUMBER OF APPS WITH NON-DETERMINISM IN THE NETWORK
TRAFFIC THAT AGRIGENTO CANNOT EXPLAIN.
Popular
Non-Popular
All
#apps avg #runs #apps avg #runs #apps avg #runs
39
30
28
28
28
6.02
8.28
9.85
12.42
13.82
16
14
11
11
11
3.10
4.44
5.67
6.78
8.01
55
44
39
39
39
4.56
6.36
7.76
9.60
10.92
K
1
2
3
4
5
be more complex than the randomly selected ones, for which
AGRIGENTO required a lower number of runs on average and
could fully explain all sources of non-determinism in more
cases overall.
D. Comparison with Existing Tools
To evaluate our approach and establish the presence of false
positives and false negatives, we compared AGRIGENTO to
existing state-of-the-art analysis tools. Generally, comparing the
results of this kind of systems is far from trivial, the main
problem being the absence of ground truth. Also, especially
in the case of obfuscated leaks, the detected information leaks
are often hard to verify by looking at the network trafﬁc alone.
Therefore, we manually reverse engineered the apps to the best
of our ability to conﬁrm our results. Finally, dynamic analysis
results are inﬂuenced by limited coverage and different UI
exploration techniques, which impedes the comparison.
The only currently available benchmark for privacy leak
detection is the DroidBench2 test suite, which is commonly
used to evaluate approaches based on both static and dynamic
analysis. We found, however, that it contains very few test
cases for dynamic analysis, and those focus mainly on emulator
detection (not affecting us since we run our evaluation on real
devices). It also does not address complex obfuscation scenarios
such as the ones we observed in this work, and, thus, none of
the test cases are appropriate for the evaluation of AGRIGENTO.
We thus performed the comparison against existing tools
using two datasets on which related work was evaluated: 750
apps from ReCon, and 54 apps from BayesDroid.
ReCon dataset. A similar comparison to evaluate state-of-the-
art analysis tools from different categories (static taint analysis,
dynamic taint analysis, and a combination of both) has been
performed recently to evaluate ReCon [35], which itself is
based on network ﬂow analysis. Table II shows the comparison
between our tool and AppAudit [46], Andrubis [26] (which
internally uses TaintDroid [13]), FlowDroid [6], and ReCon. We
base our comparison on the number of apps ﬂagged by each
tool for leaking information. For the comparison we considered
the following sources of private information: Android ID, IMEI,
MAC address, IMSI, ICCID, location, phone number, and
contacts.
Compared to ReCon, AGRIGENTO detected 165 apps that
ReCon did not identify, while it did not ﬂag 42 apps that
2https://github.com/secure-software-engineering/DroidBench
TABLE II.
COMPARISON OF AGRIGENTO WITH EXISTING TOOLS ON THE
RECON DATASET (750 APPS)
Tool (Approach)
FlowDroid (Static taint analysis)
Andrubis/TaintDroid (Dynamic taint analysis)
AppAudit (Static & dynamic taint ﬂow)
ReCon (Network ﬂow analysis)
AGRIGENTO
#Apps detected
44
72
46
155
278
ReCon identiﬁed. We manually checked the results to verify the
correctness of our approach. Among the 42 AGRIGENTO did
not detect, 23 did not generate any network trafﬁc during our
analysis. This may be due to different reasons, for instance
different UI exploration (ReCon manually explored part of
the dataset), or because the version of the app under analysis
does not properly work in our test environment. We manually
inspected the network trafﬁc generated by the remaining 19
apps. In particular, we manually veriﬁed whether each network
trace contained any of the values of the sources of private
information that we considered, and we also checked for known
transformations, such as MD5 hashes and Base64 encoding.
In all cases, we did not identify any leak (i.e., we did not
identify any false negatives). We acknowledge that this manual
evaluation does not exclude the presence of false negatives.
However, we consider this an encouraging result nonetheless.
To perform a more thorough evaluation of false negatives,
we also performed an additional experiment. Since one main
challenge when comparing approaches based on dynamic
analysis is related to GUI exploration differences, we asked
the authors of ReCon to run their tool on the network trafﬁc
dumps we collected during our analysis. In this way, it is
possible to compare both tools, ReCon and AGRIGENTO, on
the same dynamic trace. On this dataset, ReCon ﬂagged 229
apps for leaking information. AGRIGENTO correctly detected
all the apps identiﬁed by ReCon, and, in addition, it detected
49 apps that ReCon did not ﬂag. This evaluation shows that,
also for this experiment, AGRIGENTO did not show any false
negatives. Moreover, we also looked for false positives, and
we manually veriﬁed the 49 apps detected by AGRIGENTO and
not by ReCon. Our manual analysis revealed that 32 of the 49
apps did indeed leak at least one source of private information,
which should then be considered as true positives (and false
negatives for ReCon). For further 5 apps we could not conﬁrm
the presence of a leak and thus classify them as false positives
produced by our system. We cannot classify the remaining 12
cases as either true or false positives because of the complexity
of reversing these apps.
BayesDroid dataset. We obtained the dataset used by Bayes-
Droid and analyzed the apps with AGRIGENTO. For the
comparison we considered the common sources of information
supported by both AGRIGENTO and BayesDroid (i.e., IMEI,
IMSI, Android ID, location, contacts). BayesDroid ﬂagged 15
of the 54 apps. However, since this dataset contains older app
versions (from 2013) 10 apps did not work properly or did not
generate any network trafﬁc during our analysis. Nevertheless,
AGRIGENTO ﬂagged 21 apps, including 10 of the 15 apps
identiﬁed by BayesDroid. As we did for the ReCon dataset,
we manually looked at the network traces of the remaining
10
TABLE III.
NUMBER OF APPS DETECTED BY AGRIGENTO IN THE 100 MOST POPULAR APPS (JULY 2016) FROM THE GOOGLE PLAY STORE. THE COLUMN
“ANY” REFERS TO THE NUMBER OF APPS THAT LEAK AT LEAST ONE OF THE PRIVATE INFORMATION SOURCES.
Results
Plaintext
Encrypted
Obfuscated
Total
TPs
FPs
Any Android ID IMEI MAC Address
31
22
11
42
4
5
3
6
11
11
30
18
8
38
5
13
9
5
22
9
5 apps and we did not see any leak (3 of them did not
produce any network trafﬁc, furthermore BayesDroid used
manual exploration of all apps). Interestingly, AGRIGENTO
detected 11 apps that BayesDroid did not. We found that 6
of these apps used obfuscations that BayesDroid does not
detect. For instance, one app included the InMobi SDK that
performs a series of encodings and encryptions on the Android
ID before leaking it. We describe this case in detail in §VI-F.
Moreover, the other 5 apps used Android APIs to hash or
encrypt data structures (e.g., in JSON format) containing private
information sources, again showing that our system detects cases
that previous work cannot.
E. Privacy Leaks in Popular Apps
To evaluate AGRIGENTO on a more recent dataset, we