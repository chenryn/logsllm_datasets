σ(−0.04), which leads to the non-watermarked result of y = 0
instead of y = 1; the watermark is removed during extraction.
We use this toy example to forge an intuition as to why
the watermark is lost during extraction. The task and water-
mark distributions are independent. If the model has sufﬁ-
cient capacity, it can learn from data belonging to both dis-
tributions. However, the model learns both distributions in-
dependently. In the classiﬁcation example described above,
back-propagating with respect to the task data would update
all neurons, whereas back-propagating with respect to wa-
termarked data only updates the third neuron. However, the
USENIX Association
30th USENIX Security Symposium    1939
x1 ~ U(0, 1)x1 ~ U(0, 1)1 · R(x1 + 0)0.96 · R(x1 - 0)2 · R(x2 + 0)0.54 · R(x2 - 0)Input	LayerHidden	LayerOutput	Layer-1 · R(x2 + 2)0.54 · R(x2 - 0)y = σ(... + 1)y = σ(... - 1)x2 ~ U(0, 1) or -1x2 ~ U(0, 1)Legitimate DataWatermarked Data(a) Without EWE (baseline)
(b) With EWE
Figure 2: Baseline Watermarking activates different and
fewer neurons, corroborating our hypothesis of two sub-
models. Training with EWE entangles activations of water-
marked data with legitimate task data.
adversary cannot solely update the small groups of neurons
used for watermarking because they sample data from the
task distribution during extraction.
3.2 Distinct Activation Patterns
We empirically show how training algorithms converge to a
simple solution to learn the two data distributions simulta-
neously: they learn models whose capacity is roughly parti-
tioned into two sub-models that each recognizes inputs from
one of the two data distributions (task vs. watermarked). We
trained a neural network, with one hidden layer of 32 neurons,
on MNIST. It is purposely simple for clarity of exposition;
we repeat this experiment on a DNN (see Figure 21 in Ap-
pendix A.3 giving the same conclusions). We watermark the
model by adding a trigger (a 3× 3-pixel white square at cor-
ner) to the input and change the label that comes with it [56].
We record the neurons activated when the model predicts
on legitimate task data from the MNIST dataset, as well as wa-
termarked data. We plot the frequency of neuron activations
in Figure 2a for both (a) legitimate and (b) watermark data.
Here, each square represents a neuron and a higher intensity
(whiter color) represents more frequent activations. Conﬁrm-
ing our hypothesis of two sub-models, we see that different
neurons are activated for legitimate and watermarked data.
As we further hypothesized, fewer neurons are activated for
the watermark task, likely because this task (identifying the
simple trigger) is easier than classifying hand-written digits.
4 Entangling Watermarks
Motivated by the observation that watermarked models are
partitioned into distinguishable sub-models (task vs. water-
mark), the intuition behind our proposal is to entangle the
watermark with the task manifold. Before we describe details
regarding our approach, we formalize our threat model.
Threat Model. The objective of our adversary is to extract a
model without its watermark. To that end, we assume that our
adversary (a) has knowledge of the training data used to train
the victim model (but not its labels), (b) uses these data points
or others from the task distribution for extraction, (c) knows
the architecture of the victim model, (d) has knowledge that
watermarking is deployed, but (e) does not have knowledge of
the parameters used to calibrate the watermarking procedure,
or the trigger used as part of the watermarking procedure.
Observe that such an adversary is a powerful white-box adver-
sary. The assumptions we make are standard, and are made in
prior work as well [1].
4.1 Soft Nearest Neighbor Loss
Recall that the objective of our watermarking scheme is to
ensure that watermarked models are not partitioned into dis-
tinguishable sub-models which will not survive extraction.
To ensure that both the watermark and task distributions are
jointly learned/represented by the same set of neurons (and
consequently ensure survivability), we make use of the soft
nearest neighbor loss (or SNNL) [25, 47]. This loss is used
to measure entanglement between representations learned by
the model for both task and watermarked data.
e− ||xi−x j||2
T
e− ||xi−xk||2
T
∑
j∈1..N
j(cid:54)=i
yi=y j
∑
k∈1..N
k(cid:54)=i
(a)
(b)
(1)
SNNL(X,Y,T ) = − 1
N ∑
i∈1..N
log
Introduced by Srivastava and Hinton [47], the SNNL was
modiﬁed and analyzed by Frosst et al. [25]. The loss charac-
terizes the entanglement of data manifolds in representation
spaces. The SNNL measures distances between points from
different groups (usually the classes) relative to the average
distance for points within the same group. When points from
different groups are closer relative to the average distance
between two points, the manifolds are said to be entangled.
This is the opposite intuition to a maximum-margin hyper-
plane used by support vector machines. Given a labelled data
matrix (X,Y ) where Y indicates which group the data points
X belong to, the SNNL of this matrix is given in Equation 1.
The main component of this loss computes the ratio be-
tween (a) the average distance separating a point xi from other
points in the same group yi, and (b) the average distance sep-
arating two points. A temperature parameter T is introduced
to give more or less emphasis on smaller distances (at small
temperatures) or larger distances (at high temperature). More
intuitively, one can imagine the data forming separate clus-
ters (one for each class) when the SNNL is minimized and
overlapping clusters when the SNNL is maximized.
4.2 Entangled Watermark Embedding
We present our watermarking strategy, Entangled Watermark
Embedding (EWE), in Algorithm 1. We utilize the SNNL’s
ability to entangle representations for data from the task and
watermarking distributions (outliers crafted by the defender
using triggers). That is, we encourage activation patterns for
legitimate task data and watermarked data to be similar, as
1940    30th USENIX Security Symposium
USENIX Association
Algorithm 1: Entangled Watermark Embedding
Input: X,Y,Dw,T,cS,cT ,r,α,loss,model,trigger
Output: A watermarked DNN model
/* Compute trigger positions
1 Xw = Dw(cS),Y(cid:48) = [Y0,Y1];
2 map=conv(∇Xw(SNNL([Xw,XcT ],Y(cid:48),T )),trigger);
3 position = argmax(map);
/* Generate watermarked data
*/
*/
4 Xw[position] = trigger;
5 FGSM(Xw,LCE (Xw,YcT ))/* optional
*/
6 FGSM(Xw,SNNL([Xw,XcT ],Y(cid:48),T ))/* optional */
7 step = 0 /* Start training
*/
8 while loss not converged do
9
10
11
12
13
model.train([Xw,XcT ], YcT )/* watermark */
step += 1;
if step % r == 0 then
else
model.train(X,Y )/* primary task
*/
/* Fine-tune the temperature
T (i) -= α * ∇T (i)SNNL([Xw,XcT ](i),Y(cid:48),T (i));
*/
14
visualized in Figure 2b. This makes watermarks robust to
model extraction: an adversary querying the model on only
the task distribution will still extract watermarks.
Step 1. Generate watermarks: The defender aims to
watermark a model trained on the legitimate task dataset
D = {X,Y}. First, they select a dataset Dw, representing the
watermarking distribution, and a source class cS from Dw. The
defender samples data Xw ∼ Dw(cS) to initialize watermark-
ing, where Dw(cS) represents data from Dw with label cS. Dw
may be the same as the legitimate dataset D if we are perform-
ing in-distribution watermarking, or a related dataset if instead
we are performing out-of-distribution (OOD) watermarking 4.
The defender then labels Xw with a semantically different tar-
get class, cT , of D. In other words, it should be unlikely for Xw
to ever be misclassiﬁed as cT (by an un-watermarked model).
Our goal is to train the model to have the special behavior that
it classiﬁes Xw as cT , which makes it distinguishably different
from un-watermarked models.
To this end, we deﬁne a trigger, which is an input mask (see
Figure 18 (a) in Appendix A.3), and add it to each sample in
Xw. Thus, Xw now contains watermarks (outliers) that can be
used to watermark the model, and later, verify ownership. The
trigger should not change the semantics of Xw to be similar
to XcT (i.e., D(cT )). For example, a poor choice of a trigger
for in-distribution watermarks sampled from source class “1”
of MNIST, would be a horizontal line near the top of the im-
age (see Figure 18 (b)). This trigger might construe Xw to be
semantically closer to a “7” than a “1”. Such improper trig-
4OOD watermarking means the watermarked data is not sampled from
the task distribution
gers can weaken model performance and lead to the defender
falsely claiming ownership of models that were not water-
marked. To avoid these issues, we determine trigger location
as the area with the largest gradient of SNNL with respect to
the candidate input—this is done through the convolution in
the 2nd line of Algorithm 1.
Optionally, a defender can optimize the watermarked data
with gradient ascent to further avoid generating improper
triggers. The goal of this gradient ascent is to perturb the in-
put to decrease the conﬁdence of the model in predicting
the target class. This is the opposite of optimization per-
formed by algorithms introduced to ﬁnd adversarial exam-
ples, so we adapt one of these algorithms for our purpose
as shown in lines 5 and 6 of Algorithm 1. Since we would
like the effect of gradient ascent performed over the water-
marked input to transfer between different models [45], we
use the FGSM [14] which is a one-shot gradient ascent ap-
proach known to transfer better than iterative approaches like
PGD [27] because it introduces larger perturbations5. We
w = Xw +ε·sign(∇Xw ( f (Xw))
compute FGSM(Xw, f (Xw)) : X(cid:48)
where ε is the step size, and f is a function operating on Xw.
In alternating steps, we deﬁne f to be LCE of predicting Xw as
the target class, cT , by a (different) clean model, or the SNNL
between Xw and XcT . The former encourages Xw to differ
from XcT , and the latter makes entanglement easier (leading
to more robust watermarks). We use more steps of the former
to ensure Xw is semantically different from cT .
Step 2. Modify the Loss Function. To watermark the
model more robustly, we compute the SNNL at each layer,
l ∈ [L], where L is the total number of layers in the DNN,
using its representation of Xw and XcT , which will allow us
to entangle them. Y(cid:48) = [Y0,Y1] is arbitrary labels for [Xw,XcT ]
respectively. We sum the SNNL across all layers, each with a
speciﬁc temperature T (l). We multiply the sum by a weight
factor κ which governs the relative importance of SNNL to
the cross-entropy during . In other words, κ controls the trade-
off between watermark robustness and model accuracy on the
task distribution. Our total loss function is thus:
L = LCE (X,Y )− κ· L
∑
l=1
SNNL([X (l)
w ,X (l)
cT ],Y(cid:48),T (l)))
(2)
Step 3. Train the Model. We initialize and train a model
until either the loss converges or the max epochs are reached.
In training, we sample r normal batches of legitimate data,
X, followed by a single interleaved batch of Xw concatenated
with XcT , both of which are required to entangling using the
SNNL. On legitimate data X, we set κ = 0 in Equation 2 to
minimize only the task (cross-entropy) loss. On interleaved
data [Xw,XcT ] that includes watermarks, we set κ > 0 to op-
timize the total loss. Following Frosst et al. [12], we update
T using a rate of α that is learned during training, alleviating
the need to tune α as an additional hyperparameter.
5Note that here we are not concerned with the imperceptibility of water-
marked data so this is not a limitation in the context of our work.
USENIX Association
30th USENIX Security Symposium    1941
Figure 3: Visualization of our proposed EWE entangling watermarks with data from the target class cT = 7 unlike prior
watermarking approaches which push these watermarks to a separate cluster. For visualization, we use PCA [21] to project
the representations of data in each model’s penultimate layer onto its two principal components. We project data before (left
column), during (middle column), and after (right column) training for a baseline model trained with the cross-entropy loss only
(top row) and for a model trained with our proposed EWE approach (bottom row) on MNIST.
classifying watermarked data as cT .
The watermark success rate is the mean of a binomial dis-
tribution characterizing if watermarked data is classiﬁed as
the target class. According to the Central Limit Theoreom
(CLT), it is normally distributed when the number of queries,
n, is greater than 30. If we follow the watermark generation
procedures described in § 4.2, the false watermark rate should
be lower than random chance, i.e., (100/K)%. In Figure 4,
we set the false watermark rate to random chance as a conser-
vative upper bound. We often observed rates much lower than
this. Figure 4 shows the number of queries needed to claim
ownership, with 95% conﬁdence, as the watermark success
rate is varied. For watermark success rates above 23%, the
number of queries required is quite small (i.e., 30, the minimal
for CLT to be valid). As we will see in § 4.3.3, only our EWE
strategy achieves these success rates after extraction. Even the
lowest observed EWE success rate of 18.74% (on CIFAR-10)
requires (just) under 100 queries. Figure 4 also shows that
exponentially more queries are required as the watermark
success rate approaches the false watermark rate—in many
cases, the watermark success rate of the baseline is too low
for a defender to claim ownership (see Table 1).
Note that outside this section we report the watermark
success rate after subtracting the false watermark rate for
ease of understanding.
4.3.2
Increased Entanglement
First, we validate the increased entanglement of EWE over
the baseline by visualizing each model’s representation (in
its penultimate layer) of the data. In Figure 3, we train our