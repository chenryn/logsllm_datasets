(a) Total move time
 0
 250
)
s
m
 200
 150
 100
(
e
s
a
e
r
c
n
I
 50
 0
 Average                   Maximum
(b) Per-packet latency increase
y
c
n
e
a
L
t
t
e
k
c
a
P
-
r
e
P
Figure 10: Efﬁciency of move with no guarantees (NG), loss-free (LF),
and loss-free and order-preserving (LF+OP) with and without paral-
lelizing (PL) and early-release (ER) optimizations; trafﬁc rate is 2500
packets/sec; times are averaged over 5 runs and the error bars show
95% conﬁdence intervals
Move. Figure 10 shows our results for move with varying guaran-
tees and optimizations.
A move without any guarantees or optimizations (NG) completes
in 193ms. This time is primarily dictated by the time required for
the NF to export (89ms) and import (54ms) state; we evaluate the
southbound operations in detail in §8.2. The remaining 50ms is
spent processing control messages from the NFs and performing
the route update. Our parallelizing optimization (§5.1.3) can reduce
the total time for the move operation (NG PL) to 134ms by export-
ing and importing state (mostly) in parallel. However, even this
faster version of move comes at a cost: 225 packets are dropped!
Figure 11(a) shows how the number of drops changes as a function
of the packet rate and the number of ﬂows whose state is moved.
We observe a linear increase in the number of drops as the packet
rate increases, because more packets will arrive in the time window
between the start of move and the routing update taking effect.
A parallelized loss-free move (LF PL) avoids drops by raising
events. However, the 410 packets contained in events may each
incur up to 185ms of additional latency.
(Packets processed by
P RADS1 before the move or P RADS2 after the move do not
incur additional latency.) Additionally, the total time for the move
operation increases by 62% (84ms). Figure 11(b) shows how the
total move time scales with the number of ﬂows affected and the
packet rate. We observe that the total time for a parallelized loss-
free move increases more substantially at higher packet rates. This
is because more events are raised, and the rate at which the packets
contained in these events can be forwarded to P RADS2 becomes
limited by the packet-out rate our OpenFlow switch can sustain.
The average and maximum per-packet latency increase for packets
contained in events also grows with packet rate for the same reason:
e.g., the average (maximum) per-packet latency increase is 465ms
(573ms) for a parallelized loss-free move of 500 ﬂows at a packet
rate of 10K packets/sec (graph not shown).
While we cannot decrease the total move time without using
more rules in SDN switches, our early-release optimization (§5.1.3)
can decrease the additional packet latency. At a rate of 2500 pack-
ets/sec, the average per-packet latency overhead for the 326 packets
contained in events drops to 50ms (LF PL+ER in Figure 10(b)), a
63% decrease compared to LF PL; at 10K packets/sec this overhead
drops to 201ms, a 99% decrease. Forwarding packets in events di-
s
t
e
k
c
a
P
d
e
p
p
o
r
D
#
 1500
 1000
 500
 0
250 flows
500 flows
1000 flows
250 flows
500 flows
1000 flows
 800
 600
 400
 200
)
s
m
(
i
e
m
T
e
v
o
M
 0
 0
 2.5
 5
 7.5  10
 0
 2.5
 5
 7.5
 10
Packet Rate (1000s of pkts/s)
(a) Packet drops during a paral-
lelized move with no guarantees
Packet Rate (1000s of pkts/s)
(b) Total time for a parallelized
loss-free move
Figure 11: Impact of packet rate and number of per-ﬂows states on
parallelized move with and without a loss-free guarantee
rectly to P RADS2, rather than sending packet-out commands to
the OpenFlow switch, can likely reduce this latency even further.
In addition to added packet latency, a loss-free move also intro-
duces re-ordering: 657 packets (335 from events + 322 received
by P RADS2 while packets from events are still arriving) are pro-
cessed out-of-order with a parallelized loss-free move. However,
this re-ordering can be eliminated with an order-preserving move.
A fully optimized loss-free and order-preserving move (LF+OP
PL+ER in Figure 10) takes 96% (208ms) longer than a fully opti-
mized loss-free-only move (LF PL+ER) due to the additional steps
involved. Furthermore, packets buffered at P RADS2 (100 pack-
ets on average), while waiting for all packets originally sent to
P RADS1 to arrive and be processed, each incur up to 96ms of
additional latency (7% more than LF PL+ER). Thus, applications
can beneﬁt from choosing an alternative version of move if they do
not require both guarantees.
Copy and Share. A parallelized copy takes 111ms, with no packet
drops or added packet latency, as there is no interaction between
forwarding state update and this operation. In contrast, a share op-
eration that keeps multi-ﬂow state strongly consistent adds at least
13ms of latency to every packet, with more latency incurred when a
packet must wait for the processing of an earlier packet to complete.
This latency stems from the need to call getMultiflow and
putMultiflow on P RADS1 and P RADS2, respectively, after
every packet is processed, because our events only provide hints as
to whether state changed but do not inform us if the state update
is signiﬁcant. For example, every packet processed by the PRADS
asset monitor causes an update to the last seen timestamp in the
multi-ﬂow state object for the source host, but only a handful of
special packets (e.g., TCP handshake and HTTP request packets)
result in interesting updates to the object. However, adding more
PRADS asset monitor instances (we experimented with up to 6 in-
stances) does not increase the latency because putMultiflow
calls can be issued in parallel. In general, it is difﬁcult to efﬁciently
support strong consistency of state without more intrinsic support
from an NF, e.g., information on the signiﬁcance of a state update.
8.1.2 Beneﬁts of Granular Control
Although the move, copy, and share operations above en-
compassed all ﬂows, the northbound API allows applications to in-
voke these operations at any granularity, down to as ﬁne as a single
ﬂow. We now examine the beneﬁts this ﬂexibility enables by using
the copy operation with the Squid caching proxy. We generate
100 requests (drawn from a logarithmic distribution) for 40 unique
URLs (objects are 0.5–4MB in size) from each of two clients at
a rate of 5 requests/second.
Initially, all requests are forwarded
to Squid1. After 20 seconds, we launch a second Squid instance
(Squid2) and take one of three approaches to handling multi-ﬂow
state: do nothing (ignore), invoke copy with the second client’s
172Metric
Hits on Squid1
Hits on Squid2
MB of multi-ﬂow state transfered
Ignore Copy Client Copy All
117
50
54.4
117
Crashed
0
117
39
3.8
Table 1: Effects of different ways of handling multi-ﬂow
250 flows
500 flows
1000 flows
 1000
)
s
m
(
i
e
m
T
w
o
l
f
r
e
P
e
g
t
 800
 600
 400
 200
 0
250 flows
500 flows
1000 flows
)
s
m
(
i
e
m
T
w
o
l
f
r
e
P
u
p
t
 150
 125
 100
 75
 50
 25
 0
iptables PRADS
Bro
iptables PRADS
Bro
(a) Time for getPerflow
(b) Time for putPerflow
Figure 12: Efﬁciency of state export and import
IP as the ﬁlter (copy client), or invoke copy for all ﬂows (copy
all). Then, we update routing to forward all in-progress and future
requests from the second client to Squid2.
Table 1 shows the number of cache hits at each instance, and
the bytes of multi-ﬂow state transfered, under the three different
approaches for handling multi-ﬂow state. In all three approaches,
the number of cache hits for Squid1 are the same because all the