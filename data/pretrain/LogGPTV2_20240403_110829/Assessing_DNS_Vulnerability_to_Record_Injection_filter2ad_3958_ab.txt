impact from packet loss. Of the roughly 41K ODNS servers we test, we ﬁnd 3.5K
(or 8.6%) to be vulnerable to the preplay attack. Therefore, we conclude that ODNS
servers are failing to take three simple measures to thwart this attack: (i) use a new and
random DNS transaction ID, (ii) verify that the source IP address in DNS responses
matches the IP address of the upstream RDNS, and (iii) verify the destination port
number on responses. The latter is particularly intriguing as it suggests these devices
are not running a traditional protocol stack in which packets arriving on an unbound port
number are dropped. Given we ﬁnd no increase in the success rate with our attempts at
spooﬁng, we return to PlanetLab with the S3 scan to assess the vulnerability at a larger
scale. Of the 2.3M ODNS servers we test, we ﬁnd 170K (or 7.3%) to be vulnerable to
the preplay attack.
6 DNS Message Rewriting
We now examine DNS record modiﬁcation by network operators. Depending on one’s
perspective this may or may not be considered a security issue. However, we believe
that responses deviating from the authoritative intent are at least worth understanding.
NXDOMAIN Rewriting: A DNS request for a non-existent name evokes a response
with the “NXDOMAIN” return code. Previous anecdotal observations indicate that such
responses are prone to interception and replacement with valid addresses by some ISPs
and DNS providers. This practice is generally attempting to monetize the unfulﬁlled re-
quest (e.g., by trying to sell the domain or sending a user to a similar page in an attempt
to meet their intent). In our S1 experiment, we send a request to each ODNS that causes
our ADNS to return an NXDOMAIN message. We ﬁnd that roughly 258K (23.7%)
of ODNS servers are subject to NXDOMAIN rewriting as we receive an address in
response to our invalid query, which is close to previous measurements [15]8 To under-
stand who may be responsible for rewriting, we analyze the set of RDNS resolvers on
the path of rewritten messages. We determine an RDNS is a probable rewriter if more
than half the open resolvers served by the RDNS experience rewriting. We ﬁnd over
100 ISPs/DNS providers that we suspect of performing rewriting by default, including
Qwest/Centurylink, OpenDNS, Frontier, Rogers, Airtel, RoadRunner, and TE Data.
Search Engine Hijacking: Previous work shows several ISPs alter DNS responses
from major search engines in an effort to place a proxy between the user and the search
results [15–17]. This allows the ISPs to monetize users’ searching (e.g., by placing ads
7 Due to some of our (unreported) tests using spoofed addresses (against PlanetLab’s AUP).
8 As a methodological note, one must be careful in selecting query strings. For instance, we
initially misclassiﬁed OpenDNS as not performing rewriting because our queries began with
dotted-quad IP addresses—a pattern OpenDNS excludes from its rewriting process. A second
pass with a different query string correctly classiﬁes OpenDNS as a rewriter. Thus, our ﬁndings
are conservative due to other potential edge cases.
220
K. Schomp et al.
on the results). Since our strategy allows us to assess ISPs’ RDNS resolvers, we inves-
tigate this behavior and ﬁnd no evidence the practice is now in widespread use. Still,
we ﬁnd 18 smaller regional ISPs that appear to rewrite DNS responses for google.com.
7 Implications
Duration of Record Injection: The injection attacks we discuss above can only be
successful when part of the DNS infrastructure caches a fraudulent record and then
returns that record in response to a normal user request. An assessment of the caches
of FDNS and RDNS resolvers [14] ﬁnds (i) little evidence of cache evictions based
on capacity limits and (ii) that records with long TTLs—which can be set in injected
records—stay in the cache for at least one day in 60% of the RDNSi resolvers and 50%
of the FDNS servers. This shows the impact of record injection can be long-lived.
Indirect Attacks: It is not enough for RDNS resolvers to act on requests only from
authorized devices as these devices are in turn commonly globally accessible and open
RDNS resolvers to indirect attacks. The large and growing set—doubling to over 30M
in the last three years—of open resolvers [14] represents an attack vector to otherwise
inaccessible RDNS resolvers. For instance, we ﬁnd that 62% of the RDNS resolvers in
the S1 dataset do not answer external queries and yet we are still able to probe these
servers. Further, using ODNS servers to indirectly attack other portions of the DNS
ecosystem provides a layer of obfuscation that helps attackers escape attribution.
Phantom DNS Records: A class of denial-of-service attacks relies on placing a large
DNS record in a cache (at an RDNS, say) and then spooﬁng requests that will cause
the record to be sent to some victim. This can both hide the actual origin of the attack,
as well as amplify (in volume) an attackers trafﬁc by using records that are larger than
requests. To date this requires attackers to register a domain and serve large records
to insert them into the various caches or ﬁnd an ADNS that is already serving large
records. However, using record injection techniques, an attacker does not need to be
bound to any centralized infrastructure. In fact, any domain could be readily inserted in
the cache and then used in a subsequent attack. This leaves less of a paper trail that can
potentially trace back to an attacker. The preplay attack allows such record injection
into millions of devices with trivial effort.
8 Context
We now return to contextual issues surrounding our measurements, as sketched in § 2.
Are Open Resolvers Used? We ﬁrst turn to the question of whether ODNS servers
in fact serve users or are active, yet unused artifacts. This bears directly on whether
the preplay attack represents a real problem. First, in companion work we use several
criteria—including scraping any present HTTP content on the ODNS, consulting black-
lists of residential hosts and observing UDP protocol behavior—to determine that “78%
[of ODNS servers] are likely residential networking devices” [14]. Using the same cri-
teria against the FDNS servers in the S3 scan, we ﬁnd that 91% of the FDNS servers
Assessing DNS Vulnerability to Record Injection
221
(cid:2)
(cid:2)
(cid:2)
that are vulnerable to the preplay attack are likely residential network devices. While
this result does not speak directly to use, our experience is that these devices act as DNS
forwarders for devices within homes and therefore we believe this suggests actual use.
Additionally, we seek to test directly for evidence that the FDNS servers we probe are
in use by some client population. We start by gathering round-trip time (RTT) samples
for each FDNS and the corresponding RDNS. For the FDNS we use the preplay attack
to measure the RTT by taking the time between sending a fraudulent response to the
FDNS and receiving the response back from the FDNS at our client. Measuring the
RTT to the RDNS is more complicated. The process starts with the client requesting
some name N from our ADNS. The ADNS responds with some CNAME N
, which the
RDNS then resolves and our ADNS returns a random address A. The mapping between
N and A then returns to the FDNS and ultimately our client. The client then issues a
—which will presumably be in the RDNS’ cache, but given the primitive
request for N
nature of preplay-vulnerable FDNS devices not in the FDNS’ cache. The response for
N
will be A when the RDNS answers the request from the cache.
After we obtain RTTs for both FDNS and RDNS, we seek to understand whether
popular web site names are in the FDNS cache as a proxy for whether the FDNS is in
use by a population of users. We therefore issue DNS requests for the Alexa9 top 1,000
web sites and time the responses.10 Given unreliable TTL reporting by FDNS servers
[14], we determine that a given hostname is in the FDNS cache using the time required
to resolve the name. Since we expect individual FDNS to have diurnal variation, we
perform the lookups on each FDNS every 4 hours for one day. Our own queries will
populate the FDNS cache and therefore we must exercise care with subsequent probes
lest we wrongly conclude users employ the FDNS when it is our own probes we observe.
We mitigate this issue in two ways. First, we probe all names with an authoritative TTL
of 4 hours or more only once, accounting for 415 names. Second, we inject records into
FDNS cache from our ADNS with the same TTLs as the remaining 1,585 records in
our corpus (all of which are less than 4 hours). At each 4 hour interval we check our
own records and if the FDNS incorrectly returns a record that had an initial TTL of x
we exclude all but the initial query for popular names with an initial TTL of at least x.
We determine that a given hostname is in the FDNS cache if the time required to
resolve the name during our S3 scan does not exceed the median FDNS RTT. Figure 1
shows the distribution of the fraction of FDNS servers that hold a given number of
records in their cache. The “All” line shows the distribution for all preplay-vulnerable
FDNS servers. We ﬁnd 81% of the FDNS servers have at least one popular name in
their cache at some point during the experiment. However, the distribution also shows
that over 30% of the FDNS servers have at least 100 hostnames in the cache. This seems
unlikely and we believe these represent cases where our heuristic is not properly delin-
eating between the FDNS and RDNS cache. Therefore, in an effort to better delineate
the FDNS and RDNS caches we plot the subset of FDNS servers where the maximum
FDNS RTT is at least 10 msec less than the minimum RDNS RTT, which we denote as
“Far from RDNS”. This subset encompasses 8.4K FDNS servers and we do see the tail
9 www.alexa.com
10 Note, we augment the list of sites by prepending each web site name from Alexa with
“www”—which is not included in the list—and we therefore probe for 2,000 names.
222
K. Schomp et al.
1
0.8
0.6
0.4
0.2
F
D
C
C
0
0 1
All
Far from RDNS
10
100
Records in Cache
2K
0.2
0.15
0.1
0.05
1
FDNS Vulnerable to Preplay
RDNS Vulnerable to Kaminsky
3
2
9
Snapshots in chronological order
4
5
6
7
8
10
Fig. 1. Distribution of popular web-
sites in FDNS server caches
Fig. 2. Vulnerability frequency at
snapshots during discovery
of the distribution fall away. Within this subset, 53% of the FDNS servers are in use.
Additionally, we examine the subset of FDNS servers that are accessible for our en-
tire 24 hour experiment. In this subset, we ﬁnd more in-use FDNS servers—90% of all
FDNS servers and 68% of FDNS servers that are far from their corresponding RDNS
resolver. We exclude the 24 hour lines from the graph for readability.
Note, our heuristic provides a lower bound on the number of in-use FDNS servers
since we only measure a fraction of the 24 hour period. Indeed, the median TTL for
the popular names is 10 minutes. Assuming the median TTL, an FDNS that enforces
the TTL and an FDNS available for 24 hours, our strategy provides a one-hour window
into the FDNS’ cache—or, just over 4% of the day. Further, our extensive probing of
the FDNS’ cache may actually overﬂow the cache thus pushing out records added via
use. Therefore, we believe that many of the FDNS servers that do not show as in use
are in fact in use, but that short TTLs and our coarse and extensive probing conspire to
hide the use. Our general conclusion is that the FDNS servers we ﬁnd are in fact in use
by people during their normal browsing.
Representativeness: Finally, we return to the issue of representativeness of our results
as mentioned in § 2. Since our scans do not encompass the entire Internet our insights
could be skewed by our scanning methodology. To check this we divide our scans into
ten chronological slices and derive the cumulative vulnerability rate at each slice for the
Kaminsky and preplay attacks. The slices are equal in size in terms of the number of
vulnerable RDNS servers and vulnerable ODNS servers for the Kaminsky and preplay
vulnerabilities, respectively. The cumulative vulnerability rate should plateau once the
dataset is typical of the broader population. Figure 2 shows the cumulative vulnerability
rate across the ten slices for both attacks. The FDNS vulnerability rate reaches steady
state immediately, illustrating that we are in fact capturing a representative sample of
FDNS servers with random sampling of IP addresses—which is not surprising.
On the other hand, the ﬁgure shows that for the Kaminsky attack, the vulnerability
rate increases as the scan progresses, indicating that the RDNS resolvers we discover
at the beginning of the scan are less vulnerable to the Kaminsky attack than those we
discover later in the scan. While we choose ODNS servers at random, we only indi-
rectly discover RDNS resolvers. In particular, the probability of discovering an RDNS
resolver is directly proportional to the size of the FDNS population that it serves. Thus,
as the scan proceeds the discovery rate decreases and we ﬁnd smaller scale RDNS re-
solvers. We believe these results sum to indicate that busier RDNS servers are better
Assessing DNS Vulnerability to Record Injection
223
maintained and less vulnerable to the Kaminsky attack. The plot also indicates that our
estimate of the Kaminsky vulnerability rate is a lower bound.
9 Conclusion
In this study, we assess the susceptibility of the client-side DNS infrastructure to record
injection attacks. We ﬁnd that many open resolvers are still vulnerable to record injec-
tion. Further, these devices provide a back door to attack shared DNS infrastructure.
Through active probing, we assess the extent of known record injection threats and the
deployment of known protective techniques. We further uncover and measure a new
attack vector—the preplay attack. We ﬁnd 7–9% of the open DNS resolvers are vul-
nerable to the preplay attack and 16% of recursive DNS servers are vulnerable to the
Kaminsky attack. Therefore, we conclude that the client-side DNS ecosystem is non-
trivially vulnerable to record injection attacks.
References
1. Arends, R., Austein, R., Larson, M., Massey, D., Rose, S.: DNS Security Introduction and
Requirements. RFC 4033 (2005)
2. Bernstein, D.: http://cr.yp.to/djbdns/notes.html
3. Chun, B., Culler, D., Roscoe, T., Bavier, A., Peterson, L., Wawrzoniak, M., Bowman, M.:
PlanetLab: An Overlay Testbed for Broad-Coverage Services. ACM CCR 33(3) (2003)
4. Dagon, D., Antonakakis, M., Vixie, P., Jinmei, T., Lee, W.: Increased DNS Forgery Resis-
tance Through 0x20-bit Encoding: Security via Leet Queries. ACM CCS (2008)
5. Dagon, D., Provos, N., Lee, C., Lee, W.: Corrupted DNS Resolution Paths: The Rise of a
Malicious Resolution Authority. In: NDSS (2008)
6. Fujiwara, K.: Number of Possible DNSSEC Validators Seen at jp. In: DNS-OARC Workshop
(2012)
7. Google Public DNS. Performance Beneﬁts, https://developers.google.com/
speed/public-dns/docs/performance
8. Google Public DNS. Security Beneﬁts, https://developers.google.com/speed
/public-dns/docs/security
9. Gudmundsson, O., Crocker, S.: Observing DNSSEC Validation in the Wild. In: Workshop
on Securing and Trusting Internet Names, SATIN (2011)
10. Kaminsky, D.: Black Ops 2008: It’s the End of the Cache As We Know It. In: Black Hat
USA (2008)
11. Leonard, D., Loguinov, D.: Demystifying Service Discovery: Implementing an Internet-Wide
Scanner. In: ACM Internet Measurement Conference (2010)
12. Mockapetris, P.: Domain Names Implementation and Speciﬁcation. RFC 1035 (1987)
13. Schomp, K., Callahan, T., Rabinovich, M., Allman, M.: Client-Side DNS Infrastructure
Datasets, http://dns-scans.eecs.cwru.edu/
14. Schomp, K., Callahan, T., Rabinovich, M., Allman, M.: On Measuring the Client-Side DNS
Infrastructure. In: ACM Internet Measurement Conference (2013)
15. Weaver, N., Kreibich, C., Nechaev, B., Paxson, V.: Implications of Netalyzr’s DNS Measure-
ments. In: Workshop on Securing and Trusting Internet Names (SATIN) (2011)
16. Weaver, N., Kreibich, C., Paxson, V.: Redirecting DNS for Ads and Proﬁt. In: Workshop on
Free and Open Comm. on the Internet (2011)
17. Zhang, C., Huang, C., Ross, K., Maltz, D., Li, J.: Inﬂight Modiﬁcations of Content: Who Are
The Culprits? In: LEET (2011)