5.2 Homomorphic Checksums: Deﬁnitions
We now give the deﬁnition of a homomorphic checksum
scheme, along with its deﬁnition of security (unforgeability).
As mentioned in Section 4.6, we segment a block into seg-
ments of bit-length β0 = (cid:100)log p(cid:101). Therefore, we can write a
block B in the form B ∈ Z(cid:100)β/β0(cid:101)
.
p
Definition 4. We deﬁne a homomorphic checksum scheme
to consist of the following algorithms:
sk ← K(1λ). The key generation algorithm K takes in the
security parameter 1λ, the block size β, outputting a
secret key sk .
σ ← Ssk (B). The authentication algorithm S takes in the
, and outputs a
secret key sk , a block B ∈ Z(cid:100)β/β0(cid:101)
checksum σ. In our scheme σ has bit-length O(λ).
p
Definition 5
(Unforgeability of checksum). We say
(cid:20) sk ← K(1λ)
that a homomorphic checksum scheme is unforgeable, if for
any (polynomial-time) adversary A,
B1 (cid:54)= B2
Ssk (B1) = Ssk (B2)
B1, B2 ← A(1λ)
≤ negl(λ) .
(cid:21)
Pr
:
Namely, an adversary who has not seen the secret key sk or
any checksum cannot produce two blocks B1 and B2 that
result in the same checksum.
In our scheme, the clients
encrypts all checksums before storing them at the server,
thus the server does not see the secret key or any checksums.
Additive homomorphism. We require our homomorphic
checksum scheme to achieve additive homomorphism, i.e.,
for any sk ← K(1λ), for any blocks B1, B2 ∈ Z(cid:100)β/β0(cid:101)
, for any
a1, a2 ∈ Zp, it is a1Ssk (B1) + a2Ssk (B2) = Ssk (a1B1 + a2B2).
5.3 Homomorphic Checksum Construction
p
We now present a simple homomorphic checksum scheme.
K(1λ): The client picks a random matrix M $← Zρ×(cid:100)β/β0(cid:101)
and lets sk := M. The number of rows ρ is chosen
such that ρβ0 = O(λ), i.e., we would like the resulting
checksum to have have about O(λ) number of bits.
p
Ssk (B): On input a block B ∈ Z(cid:100)β/β0(cid:101)
, compute checksum
σ := M · B. Note that the checksum compresses the
block from β bits to O(λ) bits.
p
Additive homomorphism. It is not hard to see that the
above homomorphic checksum scheme satisﬁes additive ho-
momorphism, since a1MB1 + a2MB2 = M(a1B1 + a2B2).
Unforgeability. Inthe full online version [22] we show that
the above construction satisﬁes unforgeability in an infor-
mation theoretic sense (i.e., even for computationally un-
bounded adversaries).
Theorem 2. The above homomorphic checksum construc-
tion satisﬁes the unforgeability notion (Deﬁnition 5).
p
Eﬃciency. As we saw in the deﬁnition of the homomor-
phic checksum, the blocks used are represented as vectors in
Z(cid:100)β/β0(cid:101)
. We use a very small p in our implementation—in
fact we take p = 3·230 +1 and therefore checksum operations
do not need big integer operations and are highly eﬃcient.
5.4 Using Homomorphic Checksums
Relying on homomorphic checksums, we can reduce the
bandwidth cost and the client computation to β +O(λ log n)
for write operations. To do this, we make the following mod-
iﬁcations to the basic construction described in Section 4.
Store encrypted and authenticated checksums on
server. First, for every (encoded or uncoded) block stored
on the server, in the buﬀers U, C and H, the client at-
taches a homomorphic checksum, which is encrypted under
an authenticated encryption scheme AE := (E, D).
Let sk := (M, sk 0) denote the client’s secret key (unknown
to the server). Speciﬁcally M ∈ Zρ×(cid:100)β/β0(cid:101)
is the random
matrix used in the homomorphic checksum scheme, and sk 0
is a master secret key used to generate one-time keys for the
authenticated encryption scheme AE.
p
For the buﬀers C and H, suppose a block B is written to
address addr on the server at time t. The client generates
the following one-time key:
κ = PRFsk 0 (0, addr, t)
and attaches(cid:101)σ(B) with block B, where
(cid:101)σ(B) := AE.Eκ(SM (B)) .
For buﬀer U, the client uses a ﬁxed encryption key κ =
PRFsk 0 (1) as blocks in U have unpredictable last-write times.
Ensuring authenticity and freshness. For each block
allows the client to verify the authenticity and freshness of
the block. The client need not separately MAC the blocks
in H or C.
in buﬀers H and C, its time and location dependent (cid:101)σ(B)
For blocks in U, a Merkle tree is built over these (cid:101)σ(B)’s
corresponding(cid:101)σ(B), veriﬁes(cid:101)σ(B) with the Merkle tree, and
veriﬁes that the block B fetched agrees with(cid:101)σ(B).
for the U buﬀer, and the client keeps track of the root digest.
After fetching a block B from buﬀer U, the client fetches its
Rebuilding H based on homomorphic checksum. In
our improved scheme, the HRebuild algorithm is executed by
the server. In order to enforce honest server behavior, the
client performs the HRebuild algorithm, not directly over the
blocks, but over (cid:101)σ(B)’s. In other words, imagine the client
were to execute the HRebuild algorithm on its own:
332• Whenever the client needed to read a block B from the
server, it now reads(cid:101)σ(B) instead, and decrypts σ(B) ←
AE.Dκ((cid:101)σ(B)).
In the above, κ := PRFsk 0 (0, addr, t),
where addr is the physical address of block B, and t
is the last time B is written. Note that for any block
in the hierarchical log structure H and in the erasure-
coded copy C, the client can compute exactly when the
block was last written from the current time alone. The
client rejects if the decryption fails, which means that
the server is misbehaving.
• Whenever the client needed to perform computation over
two blocks B1 and B2, the client now performs the same
operation over the homomorphic checksums σ(B1) and
σ(B2). Recall that in HRebuild, we only have addition
and multiplications by known constants—therefore, our
additively homomorphic checksum would work.
• Whenever the client needed to write a block to the server,
it now writes the new(cid:101)σ(B) instead.
Rebuilding C based on homomorphic checksum. Sim-
ilar to the above, the server rebuilds C on behalf of the
client, and the client only simulates the rebuilding of C op-
erating on the (cid:101)σ(B)’s instead of the full blocks. However,
slightly diﬀerently from the rebuilding of H, the buﬀer C is
rebuilt by computing an erasure code of the fresh data in U.
One way to do this is to use the same FFT-based code
as described in Section 4. The server can compute the code
using the butterﬂy diagram (Figure 3) in O(βn log n) time.
The client simply has to simulate this encoding process using
the(cid:101)σ(B)’s rather than the data blocks—therefore the client-
server bandwidth is O(λn log n) for each rebuild of C.
5.5 Reducing Client-Server Audit Bandwidth
In our basic construction in Section 4, audits require trans-
ferring O(λ) random from each level H(cid:96) and from C—therefore
the audit cost is O(λβ log n). However this can be reduced
by observing that audited blocks can be aggregated using a
linear aggregation scheme, and the client can check the “ag-
gregated block” using the homomorphic “aggregate check-
sum”. This is similar to the technique used by Shacham and
Waters [20]. The new audit protocol is as below:
• Challenge. Client picks O(λ) random indices for each
level H(cid:96) and for C. Client picks a random challenge
ρi ∈ Zp for each selected index. The random indices
are denoted I := {addr1, addr2, . . . , addrr}, where r =
O(λ log n). The random challenges are denoted R :=
{ρ1, ρ2, . . . , ρr}. Client sends I := {addr1, addr2, . . ., addrr}
and R := {ρ1, ρ2, . . . , ρr} to the server.
• Response. Let Baddr denote the block at addr. Server
sends client the corresponding(cid:101)σ(Baddr) for each requested
index addr ∈ I. Server also computes B∗ =(cid:80)r
• Veriﬁcation. The client decrypts all(cid:101)σ(B)’s received and
obtains σ(B)’s. Client computes v = (cid:80)r
i=1 ρiσ(Baddri ),
and the checksum σ(B∗). The client checks if v ?= σ(B∗).
and sends B∗ to the client.
i=1 ρiBaddri ,
This modiﬁcation reduces the bandwidth for audits to β +
O(λ2 log n) since only checksums and one aggregate block
need to be communicated from the server to the client.
Theorem 3. The improved dynamic PoR scheme that uses
homomorphic checksums satisﬁes both authenticity (Deﬁni-
tion 1) and retrievability (Deﬁnition 2).
The proofs of the above theorem can be found in the full
online version [22].
6.
IMPLEMENTATION AND EVALUATION
We implemented a variant of our secretly-veriﬁable con-
struction described in Section 5. The implementation is in
C# and contains about 6,000 lines of code measured using
SLOCCount [4].
6.1 Practical Considerations
Our implementation features several further optimizations
in comparison with the theoretical construction in Section 5.
Reducing write cost. Our implementation is optimized
for the common scenario where writes are frequent and au-
dits are relatively infrequent. We apply a simple trick to re-
duce the client-server bandwidth for write operations; how-
ever, at slightly higher cost for audits.
To reduce the overhead of writing the log structure H, the
client can group about O(log n) blocks into a bigger block
when writing to H. The simplest instantiation is for the
client to cache O(log n) blocks and write them in a batch as
a single bigger block.
This optimization reduces the amortized cost of writing
to H to β + O(λ). Particularly, the client-server bandwidth
overhead (in addition to transferring the block itself) can be
broken down into two parts: 1) about O(λ log n) due to the
Merkle hash tree for the buﬀer U; and 2) O(λ) for writing
to the log structure H—notice that this part is independent
of n. This trick increases an audit’s disk I/O cost to roughly
O(λβ log2 n) (but only O(λ log n) disk seeks), and increases
an audit’s client-server bandwidth to β log n + O(λ2 log n).
Deamortization. Our theoretic construction has low amor-
tized cost per write, but high worst-case cost. We can per-
form a standard deamortization trick (introduced for sim-
ilar multi-level hierarchical data structures used in several
ORAM constructions [18, 31]) to spread the hierarchical en-
coding work over time, such that we achieve good worst-case
performance. We implemented deamortization, therefore it
is reﬂected in the experimental results.
Reducing disk seeks. Our hierarchical log features se-
quential data accesses when rebuilding level H(cid:96) from two
arrays of size 2(cid:96)−1. (see Figure 3 and Algorithm mix of Fig-
ure 4). Due to such sequential access patterns, we can use a
bigger chunk size to reduce the number of seeks. Every time,
we read a chunk and cache it in memory while performing
the linear combinations in Algorithm mix.
6.2 Experimental Results
We ran experiments on a single server node with an i7-
930 2.8 Ghz CPU and 7 rotational WD1001FALS 1TB 7200
RPM HDDs with 12 ms random I/O latency. Since reads are
not much diﬀerent from a standard Merkle-tree, we focus on
evaluating the performance overhead of writes and audits.
6.2.1 Write Cost
Client-server bandwidth. Figures 5 and 6 depict the
client-server bandwidth cost for our scheme. Figure 5 plots
the total client-server bandwidth consumed for writing a
4KB block, for various storage sizes. We compare the cost
to a standard Merkle hash tree, and show that our POR
scheme achieves comparable client-server bandwidth.
333Figure 5: Client-server bandwidth
for writing a 4KB block.
Figure 6: Percentage of bandwidth
overhead for various block sizes. The
total storage capacity is 1TB.
Figure 7: Throughput for write oper-
ations when client-server bandwidth is
ample. The error bars indicate one stan-
dard deviation over 20 trials. A 4KB block
size is used.
Figure 8: Time spent by server for
performing an audit. Does not include
network transfer time. Error bars denote
1 standard deviation from 20 trials. The
majority of this time is disk I/O cost. A
4KB block size is chosen.
Figure 9: Disk I/O cost for each au-
dit. Block size is chosen to be 4KB.
Figure 10: Client-server bandwidth
for an audit. Block size is chosen to be
4KB.
We also plot the dotted “Hierarchical FFT encoding” curve,
which represents the portion of our bandwidth cost dedi-
cated to the rebuilding of the buﬀers C and H. As men-
tioned earlier in Section 6.1, our implementation features
optimizations such that the client-server bandwidth over-
head contains the sum of two parts 1) a O(λ) part for re-
building H and C. This is represented by the dotted line in
Figure 5—note that it is independent of n, hence a straight-
line; and 2) O(λ log n) bandwidth due to the Merkle tree for
the up-to-date copy U.
Figure 6 shows the percentage of bandwidth overhead (not
including transferring the block itself) for each write.
In
our scheme, the bandwidth overhead is indepedent of the
block size. Therefore, the bigger the block size, the smaller
the percentage of bandwidth overhead. We also compare
against a standard Merkle-tree in the ﬁgure, and show the
portion of the bandwidth overhead for the hierarchical en-
coding procedure.
Server disk I/O performance. Figure 7 shows the write