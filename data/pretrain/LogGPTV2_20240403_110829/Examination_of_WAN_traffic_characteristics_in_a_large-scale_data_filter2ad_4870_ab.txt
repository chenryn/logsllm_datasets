as in [4, 5, 20, 27] or traffic burst behavior as in [21, 34]. To this end,
we utilize the sampled Netflow data and the SNMP data collected
across Baidu’s DCN. 1 Note that we focus on the traffic related to
Baidu specific in-house services in this paper, i.e., our dataset does
not include cloud customer traffic. 2
1The DCs we examined in this paper are distributed across multiple regions inside
China.
2Indeed, the cloud customer services may show different behavior than Baidu’s in-
house services in terms of traffic volume, replications in DCs, interaction with other
services and etc. Besides, we do not have service category information for individual
flow records of cloud customer traffic. This prevented us to include cloud customer
traffic as we focus on traffic characteristics at service-level.
3
…ServersSpine	Core	switchClusterxDC	switchDC	switchServersClusterswitch……ServersServersLeaf	Leaf	Leaf	4-postClosToRToRToRToRCluster	Data	centerInter-DC	WANIMC ’21, November 2–4, 2021, Virtual Event, USA
Zhaohua Wang, Zhenyu Li, Guangming Liu, Yunfei Chen, Qinghua Wu, Gang Cheng
such, instead of directly using collected statistics, we aggregated
them into 10-minute intervals and used the aggregated statistics for
analysis. Note that SNMP data were used only for link utilization
analysis.
2.3 Major Services
Overall, there are over 1,000 services hosted in Baidu’s DCN. Nev-
ertheless, the aggregated traffic volumes over services follow a
skewed distribution: less than 20% of services account for over
99% of traffic volume. These services generally can be divided into
10 categories based on their functionalities as shown in Table 1,
where the categories are sorted in a descending order of traffic
volumes. We also present the number of services and the percent-
age of high-priority traffic of each category. High-priority traffic
are delay sensitive traffic driven by Internet-facing requests (e.g.
web search queries); Low-priority traffic, on the other hand, are
usually from batch computing services (e.g. Hadoop, Spark, etc.),
which can tolerate a certain delay with pre-assigned deadlines on
the completion.
To put the categories into context, we briefly describe each cate-
gory of services. Web services are for search engine, which account
for the largest share of traffic. This is expected as Baidu is the
largest search engine provider in China. As expected, web services
are dominated by high-priority traffic. On the contrary, majority of
Computing traffic is of low priorities as the services (e.g. Hadoop
and Spark) are of batch nature. Analytics services are used for news
feeds, ads and user behavior analysis. They are also more sensitive
to delay because they are called by other services like web searches.
DB consists of database services such as SQL, NoSQL and Redis;
Cloud provides cloud storage and cloud computing services. These
two types of services are mostly non-interactive. AI services are
for distributed machine learning and deep learning, which emerge
recently given the huge data and models [25]. They are fundamen-
tal for Baidu to support vehicle auto-driving applications, search
and recommendations. These services may sync data among clus-
ters/DCs, and generate 65% of low-priority traffic. They may also
perform distributed training, which corresponds to high-priority
traffic (35%). FileSystem represents the distributed file systems. Map
provides location-based services, including navigation and loca-
tion based recommendation. Map services are often triggered by
Internet users’ requests, leading to the dominance of high-priority
traffic. Finally, Security provides security management for DCN.
While the traffic patterns of DCN hosting conventional services
like Web and Computing have been reported before [4, 8, 27], the
impact of the emerging services, like AI and Map, has not been
examined before. Indeed, with such a variety of commercial ap-
plication services, Baidu’s DCN provides research community a
unique opportunity to understand the traffic patterns of modern
DCNs. The priority of a flow’s traffic is labeled by end servers in
each packet using the DSCP field. Bandwidth allocation and traffic
engineering at WAN level depend on this field, in order to provide
low-latency transmission for high-priority traffic. Hence, we focus
on high-priority traffic when examining inter-DC WAN traffic pat-
terns, while we do not distinguish these different types of traffic for
inter-cluster traffic pattern examination as in other measurement
studies [14, 19].
Table 1: Major service categories, presented with the number
of top services and the percentage of high-priority traffic for
each category.
Service # Highpri %
Category
Web
Computing
Analytics
DB
Cloud
AI
FileSystem
Map
Security
Others
Total
15
25
23
10
15
17
3
2
3
16
129
Description
Searching engine
Stream and Batch computing
Feeds, Ads and user Analysis
Databases
Cloud storage and computing
AI techniques
Distributed file systems
Geo-location and navigation
Security management
Network operation
78.1
17.8
67.3
31.2
30.0
35.4
50.2
76.7
0.8
43.2
49.3
3 TRAFFIC DEMANDS
The effective design of WAN resource allocation depends heavily on
the traffic demands of services. In this section, we first investigate
how much traffic that leaves clusters flows out of DCs to WAN (i.e.
the traffic locality) with an emphasis on variations among services.
We observed this percentage is as high as 20% for high-priority
traffic. Given this observation, we further analyze the utilization
of the links that carry WAN traffic for better network design and
configuration.
3.1 Traffic Locality
We first examine whether the traffic leaving clusters flows to clus-
ters inside DCs (intra-DC traffic) or to other DCs in Table 2, where
we break down traffic based on the priority. We calculate the intra-
DC traffic percentages over one week of data and make the fol-
lowing findings: 1) Most of the aggregated traffic (78.3%) leaving
clusters resides within DCs, indicating a high intra-DC traffic lo-
cality. This percentage is much higher than the Facebook’s DCN,
which is around 40% [27]. 2) We also see higher traffic locality for
high-priority traffic. Low-priority traffic, on the other hand, is about
2× likely to flow out DCs than high-priority traffic (32.9% vs. 15.7%);
this is due to the data sync of individual services among DCs.
Traffic locality across service. We break down traffic into ser-
vice categories to examine the discrepancy among services. The
first relevant question is whether the constituents of individual
services in terms of intra-DC traffic and inter-DC traffic are similar.
To this end, we compute the rank correlation between two service
lists, where services in one list are ranked by their intra-DC traffic
volumes and services in the other one are ranked by their inter-DC
traffic volumes. The Spearman coefficient is above 0.85 and the
Kendall’s tau coefficient is 0.7, both implying a large overlap of two
lists and a high similarity of the constituents of services.
Table 2 compares the traffic locality of different types of ser-
vices. The locality varies across services greatly. For instance, Map
services show the least DC locality for both the aggregated traffic
and high-priority traffic. A close investigation reveals that users
may request real-time road traffic information of other geo-distant
regions that is computed and stored locally. Besides, while we see
a higher locality of high-priority traffic from the perspective of
aggregated traffic, some service categories show the opposite. For
instance, only 66.4% of the high-priority traffic generated by the
4
Examination of WAN Traffic Characteristics in a Large-scale Data Center Network
IMC ’21, November 2–4, 2021, Virtual Event, USA
Table 2: Traffic locality for different categories of services.
Intra-DC locality % Total Web Comput. Analytics DB Cloud AI
79.5
All traffic
High-priority
66.4
88.7
Low-priority
75.7
83.9
50.3
76.9
77.9
59.7
78.3
84.3
67.1
82.4
88.2
50.5
84.2
75.3
96.7
77.2
85.6
72.0
FileSys. Map Security
71.1
81.7
69.3
66.0
66.0
63.5
91.5
78.1
92.8
(a) All traffic
(b) High-priority
(c) Low-priority
Figure 3: Dynamics of traffic locality for different types of services during a week; the labels on x-axis mark the hours of the
week; each data point corresponds to a 10-minute interval.
AI services remains within DCs, which is much lower than that
from the aggregated perspective (79.5%) and from the low-priority
perspective (88.7%). Similar results can also be observed in Cloud
and Security services, which probably stems from the deployment
of geo-distributed jobs. For example, the geo-distributed machine
learning system spans multiple data centers to reduce large data
transfers and meet the constraints of privacy and data sovereignty
laws [15]. These disparities in traffic locality among services urge
the need to examine traffic patterns of different services.
Traffic locality dynamics. Next, we examine the dynamics of
traffic locality over a course of one week in Figure 3, where every
10 minutes we compute the fraction of intra-DC traffic for each
category of services. The locality of total traffic keeps relatively
stable for most of the services, except for those services that have a
higher ratio of high-priority traffic (see Table 1). Specifically, for
Web, Map, Analytics and FileSystem services, the coefficient of
variation of their traffic locality ranges from 0.05 to 0.13, while
this value of other services is less than 0.04. The locality dynamics
of high-priority traffic (Figure 3(b)) indeed show a clearly diurnal
pattern for most of the services as the traffic is driven by Inter-facing
user requests. The lowest intra-DC locality for the high-priority
traffic happens between 2 to 6 a.m., indicating that the high-priority
traffic during this time period is more likely to flow out of DCs.
Special care should be taken here as periodical jobs for data sync and
backup (thus generating low-priority traffic) are often scheduled
during this period too. The low-priority traffic does not show a
clearly diurnal pattern in locality dynamics, but its variation can be
5
0h24h48h72h96h120h144h168hTime0.20.40.60.81.0Intra-DC localityWebComputingAnalyticsDBCloudAIFileSystemMapSecurity0h24h48h72h96h120h144h168hTime0.20.40.60.81.0Intra-DC locality0h24h48h72h96h120h144h168hTime0.20.40.60.81.0Intra-DC localityIMC ’21, November 2–4, 2021, Virtual Event, USA
Zhaohua Wang, Zhenyu Li, Guangming Liu, Yunfei Chen, Qinghua Wu, Gang Cheng
very large (Figure 3(c)). This is because low-priority traffic is more
driven by planned jobs that may be scheduled periodically.
3.2 Link Utilization
We next examine the link utilization using the SNMP data collected
from DC switches and xDC (cross-DC) switches. We find, in gen-
eral, the utilization of xDC-core links is higher than that of cluster-
DC/xDC links, i.e., the link utilization increases with higher levels
of aggregation. This observation complies with previous measure-
ments of other DCNs employing similar structure [4, 5, 27].
Given the high utilization of xDC-core links, we are curious
about whether the load among these links are balanced or not. Note
that ECMP is applied in Baidu’s DCN for load balancing. Despite of
known shortcomings of ECMP in load balancing, e.g. hash collision
may lead to significant imbalance if there are a few large flows [1],
we observe it can achieve a good balance for traffic going through
xDC-core links to WAN. To show this, we calculate the coefficient
of variation of the utilization among links 4 between each xDC-core
switch pair at 10-minute intervals. Figure 4 depicts the median over
10-minute intervals in a week for individual xDC-core switch pairs.
The coefficient of variation is as low as 0.04 for over 80% xDC-core
switch pairs, indicating a good load balance.
Link utilization dynamics. While some studies [28] assume that
a single switch carries both WAN traffic and DC traffic, Baidu’s DCN
uses two types of switches, namely xDC switches and DC switches,
to separate the WAN traffic from the DC traffic. There are several
reasons for this. First, using consolidated switches to host both WAN
traffic and DC traffic creates significant challenges for WAN flows
that are bottlenecked at data center switches due to the shallow
buffer and bursty DC traffic [28]. Second, the long-term traffic of
the two types compete with each other, as we find in Figure 5,
where we examine the variation of average link utilization for
cluster-DC links and cluster-xDC links in a typical DC over a week.
The utilization of these two types of links exhibit strong daily and
weekly patterns with lower utilization on weekends. The cross
correlation between the increments of these two time series is
as high as over 0.65, indicating the high correlation between two
types of traffic. Separating the intra-DC and inter-DC traffic into
different switches thus help avoid the possible competition of switch
resources. The third reason is to enable two types of switches to
upgrade separately, which in turn saves cost. As we have seen in
Table 2, DC traffic overall is far more than WAN traffic, so it requires
increasing number of switches to hold the growing DC traffic. DC
switches are mostly commodity low-cost switches, while the xDC
switches require higher aggregated bandwidth to core switches and
are much more costly. Fortunately, xDC switches upgrade much
less frequently due to the relative lower and stable traffic volume.
3.3 Summary and Implications
In summary, despite that services are highly replicated in many DCs,
20% of high-priority traffic that leaves clusters still flows across DCs
over WAN. This percentage, however, varies across service cate-
gories and over time of a week. Specially, the emerging services (AI,
Analytics and Map) exhibit much disparities in comparison with the
4These links are with the same capacity.
Figure 4: The coefficient of variation of the utilization
among links between xDC and core switches
Figure 5: Utilization of cluster-DC and cluster-xDC links is
temporally correlated over a week; the labels on x-axis mark
the hours of the week; each data point corresponds to a 10-
minute interval.
traditional Web and Computing services. For example, Map services