value that reveals liŠle about the input, but allows its reconstruc-
tion from another biometric input that is “similar”. Œerefore, the
image based authentication problem can be reduced to the prob-
lem of transforming camera captured images of arbitrary objects
and scenes into biometric-like structures.
Hence, we introduce the LSH-related notion of locality sensitive
image mapping functions. Speciﬁcally, let d : {0, 1}λ × {0, 1}λ → R
3
be a distance function (e.g., Hamming), where λ is a system param-
eter. Œen, for a given δ ∈ [0, 1], a δ -Locality Sensitive Image
Mapping (LSIM) function h satisﬁes the following properties:
Deﬁnition 3.1. h : I → {0, 1}λ is a δ -LSIM function if there exist
probabilities P1 and P2, P1 > P2, s.t.:
(1) For any two images I1, I2 ∈ I, if sim(I1, I2) = true, then
d (h(I1),h(I2))
λ
 δ with probability P2.
4 BACKGROUND & RELATED WORK
To build ai.lock we leverage deep learning based feature extraction,
locality sensitive hashing and secure sketch constructs. In the fol-
lowing, we brieﬂy describe these concepts.
4.1 Biometric Protection
Our work is related to the problem of protecting biometric tem-
plates. We summarize biometric protection solutions, that can be
classiﬁed into fuzzy biometric protection and feature transforma-
tion approaches [29].
Fuzzy biometric template protection. Œis approach leverages
error correcting codes to verify biometric data. Techniques include
secure sketch and fuzzy extractor [17], fuzzy vault [31] and fuzzy
commitment [32], and have been applied to diﬀerent biometric
data, e.g. palm and hand [37].
In this paper, we extend the secure sketch under the Hamming
distance solution from [17]: reconstruct the biometric credential,
then compare its hash against a stored value. We brieﬂy describe
here the password set and authentication procedures that we use
based on ai.lock generated imageprints (see § 5). Let ECC be a bi-
nary error correcting code, with the corresponding decoding func-
tion D, and let H be a cryptographic hash function.
• Image password set. Let R be the reference image captured
by the user and let πR = π (R) be its ai.lock computed imageprint.
Generate a random vector x, then compute and store the authenti-
cation credentials, SS(R, x) = hSS1, SS2i, where SS1 = πR ⊕ ECC(x)
and SS2 = H (x).
• Images based authentication. Let C be the user captured
candidate image, and let πC = π (C) be its ai.lock computed im-
ageprint (§ 5). Retrieve the stored SS value and compute x ′ =
D(πC ⊕ SS1). Œe authentication succeeds if H (x ′) = SS2.
Transformation based biometric template protection. A trans-
formation is applied both to the biometric template and the biomet-
ric candidate, and the matching process is performed on the trans-
formed data. In an invertible transformation (a.k.a., salting [29]),
a key, e.g., a password, is used as a parameter to deﬁne the trans-
formation function [68]. Œe security of this approach depends
on the ability to protect the key.
In contrast, in non-invertible
schemes [40, 56] a one-way transformation functions is used to pro-
tect the biometric template, making the inversion of a transformed
template computationally hard even when the key is revealed.
Hybrid approaches. Hybrid transformation and fuzzy protection
approaches have also been proposed. Nandakumar et al. [44] intro-
duced an approach to make the ﬁngerprint fuzzy value stronger
using a password as salt. Song et al. [47] used discrete hashing
to transform the ﬁngerprint biometric, which is then encoded and
veriﬁed using error correcting codes.
4.2 Deep Neural Networks (DNNs)
Empirical results have shown the eﬀectiveness of representations
learned by DNNs for image classiﬁcation [18, 52, 72], and for the
veriﬁcation of diﬀerent biometric information [13, 42, 48]. How-
ever, ai.lock diﬀers in its need to ensure that two object images
contain the exact same object, for the purpose of authentication.
ai.lock exploits the ability of DNNs to learn features of the input im-
age space that capture the important underlying explanatory fac-
tors. We conjecture that these features will have small variations
among images of the same object or scene, captured in diﬀerent
circumstances.
Pretrained Inception.v3. Training a DNN with millions of pa-
rameters is computationally expensive and requires a large train-
ing dataset of labeled data, rarely available in practice. Instead, we
employed a transfer learning [61] approach: obtain a trained DNN
and use it for a similar task. For image feature extraction, we use
Inception.v3 [64] network pretrained on ImageNet dataset [16], of
1.2 million images of 1,000 diﬀerent object categories, for image
classiﬁcation.
4.3 Locality Sensitive Hashing
Locality Sensitive Hashing (LSH) seeks to reduce the dimension-
ality of data, while probabilistically preserving the distance prop-
erties of the input space.
It was initially used to solve the near
neighbor search problem in high dimensional spaces [28]. While
seemingly the ideal candidate to provide the ai.lock functionality,
LSH does not work well on images: images of the same scene or
object, captured in diﬀerent conditions, e.g., angle, distance, illu-
mination, will have dramatically diﬀerent pixel values, leading to
a high distance between the images and thus also between their
LSH values.
We use however Charikar’s [12] LSH as a building block in ai.lock.
Charikar’s [12] LSH deﬁnes a family of hash functions in the space
Rd . Speciﬁcally, the LSH function hr is based on a randomly cho-
sen d-dimensional Gaussian vector with independent components
r ∈ Rd , where hr (u) = 1 if r · u ≥ 0 and hr (u) = 0 if r · u < 0,
where · denotes the inner product. Œis function provides the prop-
erty that Pr [hr (u) = hr (v)] = 1 − θ (u,v )
, for any vectors u and v,
where θ(u, v) denotes the angle between the input vectors.
π
4.4 Privacy Preserving Image Matching
Traditional approaches to object matching and object recognition,
e.g., SIFT [39] and SURF [8], rely on extracting identifying features
or (robust/invariant) keypoints and their descriptors at speciﬁc lo-
cations on the image. Several solutions have been proposed for
the secure image matching problem that could be applied to the
image based authentication task. SecSIFT [53] employed order-
preserving encryption and distributes the SIFT computation among
diﬀerent servers. Hsu et al. [27] proposed a privacy preserving
SIFT based on homomorphic encryption, while Bai et al. [7] per-
formed SURF feature extraction in encrypted domain using Pail-
lier’s homomorphic cryptosystem. Wang et al. [70] improve the
SURF algorithm in encrypted domain by designing protocols for
4
Figure 2: ai.lock architecture. ai.lock processes the input im-
age through a deep neural network (i.e., Inception.v3), se-
lects relevant features, then uses locality sensitive hashing
to map them to a binary imageprint. ai.lock uses a classiﬁer
to identify the ideal error tolerance threshold (τ ), used by the
secure sketch block to lock and match imageprints.
secure multiplication and comparison, that employ a “somewhat”
homomorphic encryption. Œese approaches are not practical on
mobile devices, due to the high cost of homomorphic encryption
and the large number of keypoints (up to thousands per image).
4.5 Token-Based Authentication
In previous work [6] we have evaluated the usability of Pixie, a trin-
ket based authentication solution that employs slightly outdated
image processing techniques to extract features (i.e., “keypoints”)
and match user captured images. Pixie has an important drawback
when deployed on mobile devices: the image keypoints that it ex-
tracts need to be stored and matched in cleartext on vulnerable de-
vices. In contrast, ai.lock uses state of the art, deep neural network
based image feature extraction along with LSH to extract binary
imageprints that are robust to changes in image capture conditions.
Œe imageprints can be securely stored and matched using secure
sketches. Œis makes ai.lock resilient to device capture aŠacks. Fur-
thermore, on larger and more complex aŠack datasets, the use of
DNNs enabled ai.lock to achieve false accept rates that are at least
2 orders of magnitude smaller than those of Pixie (≤ 0.0015% vs.
0.2 − 0.8%), for similar FRRs (4%).
ai.lock’s secret physical object is similar to token-based authen-
tication, either hardware or so‰ware. For instance, SecurID [57]
generates pseudo-random, 6 digit authentication codes. ai.lock’s
Shannon entropy is slightly lower than SecurID’s 19.93 bits (see
Table 1 for comparison). Several authentication solutions use vi-
sual tokens (e.g., barcodes or QR codes). For instance, McCune,
et al. [41] proposed Seeing-is-Believing, a schema that relies on a
visual authentication channel that is realized through scanning a
barcode. Hayashi et al. [24] introduced WebTicket, a web account
management system that asks the user to print or store a 2D bar-
code on a secondary device and present it to the authentication de-
vice’s webcam in order to authenticate to a remote service. Token-
based authentication requires an expensive infrastructure [1] (e.g.
Symbol Description
λ
τ
c
s
t
Length of the imageprint for a single image segment
Error tolerance threshold
Correctable number of bits
Number of image segments in multi segment schema
Segment-based secret sharing threshold
Table 2: ai.lock notations.
for issuing, managing, synchronizing the token). ai.lock provides
a “hash-like” construct for arbitrary object images, making objects
usable as passwords, with the existing infrastructure.
Other approaches exist that seek to transform biometrics into
tokens that the user needs to carry, with important implications on
biometric privacy and revocation capabilities. For instance, TAPS [2]
is a glove sticker with a unique ﬁngerprint intended for TouchID.
5 THE AI.LOCK SOLUTION
We introduce ai.lock, the ﬁrst locality sensitive image mapping
function, and a practical image based authentication system.
In
the following, we describe the basic solution, then introduce two
performance enhancing extensions.
5.1 ai.lock: ‡e Basic (SLSS) Solution
ai.lock consists of 3 main modules (see Figure 2): (1) deep image-to-
embedding (DI2E) conversion module (2) feature selection module,
and (3) LSH based binary mapping module. We now describe each
module and its interface with the secure sketch module (see § 4.1).
Table 2 summarizes the important ai.lock parameter notations.
Deep image to embedding (DI2E) module. Let I be the ﬁxed
size input image. Let Emb : I → Re be a function that converts
images into feature vectors of size e. We call Emb(I ) the embedding
vector, an abstract representation of I . To extract Emb(I ), ai.lock
uses the activations of a certain layer of Inception.v3 DNN [64]
when I is the input to the network. Let e denote the size of the
output of the layer of the DNN used by ai.lock. Œus, Emb(I ) ∈ Re .
Feature selection module. We have observed that not all the
components in the embedding feature vectors are relevant to our
task (see § 7.1). Œerefore, we reduce the dimensionality of the fea-
ture vectors to improve the performance and decrease the process-
ing burden of ai.lock. Let P : Re → Rp , where p < e be a function
that reduces the features of an embedding to the ones that are most
important. ai.lock uses PCA with component range selection as the
P function, and applies it to Emb(I ) to ﬁnd a set of components that
can reﬂect the distinguishing features of images. Œus, the vector
produced by feature selection module is P(Emb(I )) ∈ Rp .
LSH based binary mapping module.
In a third step, ai.lock
seeks to map P(Emb(I )) to a binary space of size λ that preserves
the similarity properties of the input space. To address this prob-
lem, we use the LSH scheme proposed by Charikar [12]. Let L :
Rp → {0, 1}λ be such a mapping function. ai.lock uses as L, a
random binary projection LSH as follows. Let M be a matrix of
size p × λ, i.e. λ randomly chosen p-dimensional Gaussian vectors
with independent components. Calculate b as the dot product of
P(Emb(I )) and M. For each coordinate of b, output either 0 or 1,
based on the sign of the value of the coordinate. We call this bi-
nary representation of the input image I , i.e. π (I ) = L(P(Emb(I ))),
5
Figure 3: (a) 3 overlapping segments of an image. (b) Top:
sample images generated by DCGAN, Bottom: visually simi-
lar images in Nexus Dataset to images generated by DCGAN.
its imageprint. We denote the length of a single imageprint by λ.
Note that, the hash value for the Charikar’s method is a single bit
(λ = 1). Œerefore, L can be viewed as a function that returns a
concatenation of λ such random projection bits.
In § 7.4 we provide empirical evidence that the function h =
L ◦ P ◦ Emb is a (τ )-LSIM transform (see § 7.1), for speciﬁc τ values.
Secure sketch. ai.lock extends the secure sketch under the Ham-
ming distance of [17] to securely store the binary imageprint cre-
dentials and perform image password set and image based authen-
tication as described in § 4.1.
In the following, we introduce two ai.lock extensions, intended
to increase the entropy provided by ai.lock’s output. First, we mod-
ify ai.lock to use the embedding vectors obtained from multiple lay-
ers of Inception.v3 network. Second, we extend ai.lock to split the
input image into multiple overlapping segments and concatenate
their resulting binary representations.
5.2 ai.lock with Multiple DNN Layers
Representations learned by a DNN are distributed in diﬀerent lay-
ers of these networks. Œe lower (initial) layers of convolutional
neural networks learn low level ﬁlters (e.g.
lines, edges), while
deeper layers learn more abstract representations [73]. Œe use of
a single DNN layer prevents the basic ai.lock solution from taking
advantage of both ﬁlters.
To address this issue, we propose an ai.lock extension that col-
lects the embedding vectors from multiple (l) layers of Inception.v3
network. In addition, we modify the basic ai.lock feature extractor
module as follows. Œe Principal Components (PCs) of activations
for each layer are computed separately and are mapped to a sepa-
rate binary string of length λ. Œen, the binary strings constructed
from diﬀerent layers are concatenated to create a single imageprint
for the input image. Œus, the length of the imageprint increases
linearly with the number of layers used in this schema.
5.3 ai.lock with Multiple Image Segments
We divide the original image into s overlapping segments (see Fig-
ure 3(a)). We then run the basic ai.lock over each segment sepa-
rately to produce s diﬀerent imageprints of length λ. However, we
identify the PCs for the embedding vectors of each segment based
on the whole size images. Œe intuition for this choice is that ran-
dom image segments are not good samples of real objects and may
confuse the PCA. We then generate the imageprint of the original,
whole size image, as the concatenation of the imageprints of its
segments.
Secure sketch sharing. We extend the secure sketch solution
with a (t , s)-secret sharing scheme. Speciﬁcally, let x1, .., xs be
(t , s)-shares of the random x, i.e., given any t shares, one can recon-
struct x. Given a reference image R, let R(1)
, ..R(s ) be its segments,
= π (R(i )), i = 1..s be their imageprints. Œen, we store
and let π
(i )
R
(1)
1 , .., SS
(s )
1 , SS2i, where SS
(i )
1
(i )
R
⊕ ECC(xi ) and
SS(R, x) = hSS
SS2 = H (x). To authenticate, the user needs to provide a candi-
date image C, whose segments C (i )
, i = 1..s produce imageprints
π
= π (C (i )) that are able to recover at least t of x’s shares xi .
= π
(i )
C
6 IMPLEMENTATION & DATA
We build ai.lock on top of the Tensorﬂow implementation for In-
ception.v3 network [67]. For the error correcting code of secure
sketches, we use a BCH [11, 26] open source library [30], for syn-
drome computation and syndrome decoding with correction capac-
ity of up to c bits. Œe value for c is calculated empirically using
the training dataset (see § 7.1)
Basic (SLSS) ai.lock. In the basic ai.lock solution, we use the out-
put of the last hidden layer of Inception.v3 network, before the
so‰max classiﬁer, consisting of 2, 048 ﬂoat values. Our intuition is
that this layer provides a compact representation (set of features)
for the input image objects, that is eﬃciently separable by the so‰-
max classiﬁer.
Multi layer ai.lock. For the multi DNN layer ai.lock variants, we
have used 2 layers (l = 2). Œe ﬁrst layer is the “Mixed 8/Pool 0”
layer and the second layer is the last hidden layer in Inception.v3.
Œe embedding vector for the “Mixed 8/Pool 0” consists of 49, 152
ﬂoat values. As described in § 5.3, the embedding vectors of each
layer are separately processed by the feature selection and LSH
modules; the resulting binary strings are concatenated to form the
imageprint of size 2λ.
Multi segment ai.lock. For the multi segment ai.lock variant, we
split the image into multiple segments that we process indepen-
dently. Particularly, we consider 5 overlapping segments, cropped
from the top-le‰, boŠom-le‰, top-right, boŠom-right and the cen-
ter of the image. We generate segments whose width and height
is equal to the width and height of the initial image divided by 2,
plus 50 pixels to ensure overlap. Œe extra 50 pixels are added to
the interior sides for the side segments. For the middle segment,
25 pixels are added to each of its sides. Each segment is then inde-
pendently processed with the basic ai.lock (i.e., last hidden layer
of Inception.v3, PCA, LSH).
Multi layer multi segment ai.lock. Œis is a hybrid of the above
variants: split the image into 5 overlapping parts, then process
each part through Inception.v3 network, and extract the activation