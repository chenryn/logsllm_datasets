total number of elastic kernel objects useful for exploitation and
mitigation circumvention. It should be noted that the number with-
out parentheses denotes the total amount of objects associated with
constraints involving no variables. The number with parentheses
indicates the amount of those associated with constraints involving
variables. As we discuss in Section 4, when pairing a vulnerability
with elastic objects, ELOISE ignores the paths involving variables
and discard corresponding kernel objects (if for that elastic object
ELOISE identifies no other paths without variable involvement).
This conservative design inevitably reduces the elastic objects avail-
able for exploitation. However, as we can observe in Table 7, even
without using objects in this type, vulnerabilities can still find al-
ternative objects for performing successful exploitation.
As is shown in Table 7, the vulnerabilities selected for our evalua-
tion cover all types such as out-of-bound write (OOB), use-after-free
(UAF), and double free (DF). In the last column of the table, for each
vulnerability, we also specify all their security impacts. The nota-
tions we use for indicating these impacts are SC, HC, BA, and AR.
They denote the capabilities of performing arbitrary kernel read
(AR) as well as bypassing stack canary (SC), heap cookie protector
(HC), and leaking base address or, in other words, KASLR (BA).
Vulnerability capability summarization. Table 7 also summa-
rizes the capability of each vulnerability. In our evaluation, we
extract the capability of each vulnerability from its PoC program
based on the criteria below. If a vulnerability is in the type of out-
of-bound write, we take its capability as the range of its overflow
region and the corresponding value under its control. If a vulnera-
bility is in the use-after-free category, we depict its capability based
on how the vulnerability manipulates the freed object via the cor-
responding dangling pointer. If a vulnerability is in the category of
double free, we treat its capability as the value under the control of
the corresponding spray objects (e.g., msg_msg used in many publicly
released exploits [53–55]). We will make all these vulnerabilities
available in virtual machines and release the exploits crafted by
using elastic kernel objects.
Performance overhead of our proposed defense. Table 8 lists
the performance of the Linux kernel with and without our proposed
defense mechanism. The results are observed from three different
benchmarks discussed in Section 6.2. We present the performance
change in the column of “overhead”. For latency measure, a nega-
tive percentage indicates performance improvement, whereas the
positive percentage represents the performance degradation. For
the throughput measure, the negative and positive rates mean per-
formance degradation and increase, respectively.
A.4 Alternative Defense Methods
In addition to the defense proposed in Section 6, there are alternative
defense solutions that might be useful for mitigating the threat of
elastic kernel objects. In the following, we discuss these methods
and analyze the challenges of their implementation.
The first possible solution is to build shadow memory for each of
the elastic objects allocated in the kernel. In that shadow memory,
we can record the actual size of the corresponding object. When
the kernel discloses data in an elastic buffer at any leaking anchor,
we could check whether the amount of the data migrating to the
userspace is within a legitimate range. Since the construction of
shadow memory inevitably introduces memory and performance
overhead, the key challenge of this solution is to develop a light-
weight method to minimize overhead in a systematic method.
Another possible solution is to design a mechanism to enable the
integrity check for the data in the length field. For example, we could
first expand each of the elastic structures and introduce a checksum
field. Then, when the kernel allocates the corresponding object
and initializes its length field, we could encrypt the length value
and store it in the checksum field accordingly. With this design, at
the time of disclosing data in the elastic buffer to the userland, the
kernel could easily retrieve and scrutinize the checksum. However,
the key challenge of implementing this idea is to ensure the addition
of the checksum will not influence the usability of the kernel. For
example, some elastic data structures designed for protocols have
specific formats. After allocating objects in these types, the kernel
references the data through corresponding offsets. If introducing
additional field into such objects, one has to ensure the newly added
checksum field does not incur incorrect data reference.
A.5 More Details of User Study
Section 5.1 describes the setup of our user study. Here, we provide
the three survey forms (i.e., Figure 6, 7, and 8) we used during the
experiment. In Table 9, we display the time took for each group
spent in solving the five vulnerabilities. As the short probing survey
1. Did you ever debug Linux kernel vulnerabilities before?
Benchmark
w/o defense w/ defense Overhead
a. Yes
b. No
a. Yes
b. No
a. Yes
b. No
2. How long is your experience in Linux kernel security?
3. Did you ever write an exploit for Linux kernel vulnerability?
4. If you answer yes to 3, how do you usually bypass KASLR
a. I assume no KASLR
b. I use hardware side-channel
c. I use information disclosure in dmesg
d. I bypass KASLR if the vulnerability provides read primitive
e. Others
5. Do you know or ever use elastic structures for KASLR bypassing?
6. Please list the CVE-ID or other ID or links of vulnerabilities you
have debugged or drafted exploits for.
Figure 6: Self-assessment form.
1. Which vulnerability are your group working on?
2. For this vulnerability, which stage are your group on?
a. Vulnerability capability exploration
b. Elastic object identification
c. Memory layout manipulation.
Figure 7: Short probing survey form.
1. Do you think exploring the capability of vulnerability is difficult?
a. Yes
b. No
vulnerability?
exploits?
2. What methods do you usually use to explore the capability of
3. What do you think is the most challenging part of drafting the
Figure 8: Post-test survey form.
2010-2959
Stage A
1
0
1
1
2
3
B
0.5
4.5
NA
2017-7184
A
2
0
2
B
2
2
NA
2017-7308
A
2
0
2
B
2
3.5
NA
2017-8890
A
1
0
1
B
1
4.5
NA
ebeb...[75]
A
1
0
1
B
1
3
NA
Table 9: Summary of our probing survey results. The # in the
table indicates how many hours reported through the short
probing survey that the two groups spent on the stage of a
specific vulnerability. NA means the corresponding group
participant did not enter the stage while they develop the
exploit for a particular vulnerability.
syscall()
open()/close()
read()
write()
select() (10 fds)
select() (100 fds)
stat()
fstat()
fork() + exit()
fork() + execve()
fork() + /bin/sh
sigaction()
Signal delivery
Protection fault
Pipe I/O
UNIX socket I/O
TCP socket I/O
UDP socket I/O
Pipe I/O
UNIX socket I/O
TCP socket I/O
mmap() I/O
File I/O
LMbench - latency (ms)
0.3813
1.5282
0.4596
0.4125
0.5114
1.1805
0.7590
0.4576
90.37
255.18
858.86
0.4182
0.9337
0.6914
3.7497
5.9786
9.7846
6.5358
0.3796
1.5290
0.4529
0.4127
0.5043
1.1774
0.7600
0.4584
91.71
257.85
863.77
0.4192
0.9309
0.7093
3.7951
5.882
9.6776
6.2251
LMbench - throughput (MB/s)
4753.89
10307.40
6725.17
13511.95
7702.82
4755.49
10385.07
6327.32
13559.20
7707.81
Phoronix - latency (s)
FFmpeg
GnuPG
14.01
17.39
14.46
17.35
Apache (request/s)
OpenSSL (signs/s)
7-Zip (MIPS)
Phoronix - throughput
16700.23
272.00
9970.00
16088.00
272.00
9374.00
Customized bench - latency (ms)
28.30
31.48
30.67
35.33
30.06
28.05
30.75
31.77
26.83
28.65
79.88
29.50
28.43
31.39
30.53
28.54
33.81
29.29
34.04
29.69
29.13
31.84
32.68
27.75
28.79
81.23
30.32
28.64
31.36
31.07
sock_fprog_kern
ldt_struct
ip_options
user_key_payload
xfrm_replay_state_esn
ip_sf_socklist
sg_header
inotify_event_info
msg_msg
tcp_fastopen_context
request_key_auth
xfrm_algo_auth
xfrm_algo
xfrm_algo_aead
xfrm_policy
Average
-0.46%
0.05%
-0.94%
0.05%
-1.39%
-0.26%
0.14%
0.19%
1.46%
1.05%
0.57%
0.25%
-0.30%
2.58%
1.87%
-1.62%
-1.09%
-4.75%
0.03%
0.75%
-6.29%
0.35%
0.06%
3.22%
-0.22%
3.67%
0
5.98%
0.09%
-2.52%
2.40%
-2.87%
1.67%
-3.78%
-2.99%
0.42%
0.66%
-1.04%
2.98%
-0.28%
-0.11%
0.13%
-1.43%
0.19%
Table 8: The performance of the Linux with and without our
proposed defense. For latency measure, the smaller the num-
ber is, the better the performance is. For throughput, the
larger the number is, the better the performance is.
is collected every 30 minutes, the time is accumulated according to
the survey results.