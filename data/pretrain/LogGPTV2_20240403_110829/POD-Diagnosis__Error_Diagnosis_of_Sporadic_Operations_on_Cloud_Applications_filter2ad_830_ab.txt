n
Evaluation 
E
n
Error diagnosis
ErroEE
Process model
Third-party 
Our component 
Log Agent
Log 
Invocation
Guide
Central log processor/
trigger
Log 
storage
Process mining
Operation Node
eerration Nodee
OOpeererration Nodee
OOpe
rration Nodee
OpeOpOO eeeeOpeOpe ration Noddeee
rration Nodee
rration Nodee
OOOOpOp
eeeepepe
OpOpepeeeeeOpOppepepepe
peeererrrarra
Operation Node
eee
Operation Node
Local log 
processor/ trigger
Figure 1.  POD-Diagnosis Overview
Figure  1  gives  an  overview  of  POD-Diagnosis.  The 
white boxes are the components of POD-Diagnosis; the grey 
boxes  are  third  party  components.  A  cogwheel  on  a  box 
represents a log agent. The grey arrows represent log flow; 
the  black  arrows  represent  control  flow.  We  first  give  an 
POD-Diagnosis overview, followed by details on the steps. 
Logs are our primary sources of information. Some logs 
are  generated  by  the  cloud  infrastructure  while  other  logs 
are  generated  by  an  ‚Äúoperation  node‚Äù  (e.g.  where  Asgard 
runs)  that  orchestrates  the  operation.  As  we  operate  in  the 
cloud, some logs are hidden by cloud providers but can be 
requested through API calls. 
The  process  model  is  created  offline  using  process
mining  techniques.  The  assertions  and  their  evaluation  are 
developed based on the process model, also offline. As the 
rolling upgrade proceeds, it generates log lines that trigger 
assertion  evaluation  and  conformance  checking.  Assertion 
evaluation  is  also  triggered  from  non-log  sources  such  as 
timers  and  diagnosis.  Assertions  evaluate  the  state  of  the 
Start rolling upgrade task
Update launch 
configuration
Sort instances
Status info
Remove and deregister 
old instance from ELB
New instance ready and 
registered with ELB
Rolling upgrade task 
completed
Terminate old instance
Wait for ASG to start new 
instance
Figure 2.   Process model of Rolling Upgrade, including time data. 
In  our  preliminary  work  [2],  we  discovered  the  process 
model from logs produced by Asgard ‚Äì Figure 2 shows the 
resulting  process  model.  For  this  purpose,  we  collected  the 
logs  from  Asgard,  clustered  the  log  lines  using  a  string 
distance metric, and manually combined and named clusters 
at the desired level of granularity. From this information, i.e., 
sets  of  log  lines  and  the  corresponding  activity  names,  we 
derived  regular  expressions  matching  the  log  lines,  and 
formed transformation rules: if (% or  %>4 or ‚Ä¶ ) 
matches,  add  tag  ,!#!&-to  the  line.  Then  we 
applied these rules to all log lines. The tagged log is provided 
to an off-the-shelf process mining tool, Disco7, and resulted 
7 Disco ‚Äì http://fluxicon.com/disco/ 
254254254
in  Figure  28.  Discovery  is  the  prime  solution  in  process 
mining:  from  a  set  of  event  traces  (in  our  case:  logs),  the 
algorithms derive causal dependencies between events, e.g., 
that event A is always followed by event B. By putting all 
such dependencies together, a process model such as the one 
shown in Figure. 2, can be derived. However, system logs do 
not  lend  themselves  naturally  as  direct  input  into  the  tools. 
Therefore we created the described pre-processing pipeline ‚Äì 
as described in detail in [2].  
The  process  model  is  used  to  guide  the  local  log 
processors, central log processor and conformance checking. 
The local log processor uses the process model to annotate 
process context (process ID, instance ID, step ID, outcomes 
of the steps) on the log produced at runtime, and trigger other 
functionalities  accordingly.  Locations  for  annotations  are 
typically the beginning or the end of a process step. It is also 
possible  to  have  annotations  during  a  process  step,  by 
making it dependent on a specific type of log line.   
For each intermediary step, the expected outcomes can be 
captured as assertions. Note that the creation of the process 
model and the assertions needs to be done only once for each 
process/operation using a particular tool, and are reusable for 
an  arbitrary  number  of  process  instances  (e.g.,  concrete 
rolling  upgrades  using  Asgard).  This  means  that  our 
approach  can  be  used  when  Asgard  is  used  for  a  rolling 
upgrade regardless of the application being upgraded or the 
number of instances being upgraded. 
B.  Online Error Detection and Diagnosis  
Operations can be performed manually or automatically 
by  an  operation  agent  installed  on  the  application  node. 
Many  automated  operations  need  an  additional  operation 
node to coordinate. 
 While the operation is underway, the operation node will 
produce  operation  logs.  The  produced  logs  are  firstly 
processed  by  a  local  log  processor  agent,  which  filters  the 
original  logs,  annotates  the  corresponding  log  lines  with 
process  context  information,  triggers  post-step  assertion 
evaluation  and  conformance  checking  according  to  the 
process  context,  and  forwards  ‚Äúimportant‚Äù  lines  to  the 
central log storage. By ‚Äúimportant‚Äù line, we usually mean the 
log lines that represent the start or end of a process activity. 
The  failure  of  assertion  evaluation  and  conformance 
checking triggers Error Diagnosis. 
results  of  Conformance  Checking,  Assertion 
Evaluation  and  Error  Diagnosis  are  also  recorded  as  logs. 
They  are  forwarded  to  the  central  log  storage  and  merged 
with the operation logs collected from distributed nodes. The 
data  in  the  log  storage  can  be  used  for  future  process 
discovery,  e.g.  when  a  process  has  changed,  or  offline 
diagnosis.  A  central  log  processor  grabs  the  logs  from  the 
central  log  storage  and  triggers  the  error  diagnosis  when  it 
finds a failure or exception indicated by the log line. 
The 
1)  Log Processors  
8 The figure has been re-drawn for clarity in print. 
255255255
Figure.  3  shows  the  local  log  processor  running  on 
operation  node.  The  local  log  processor  is  a  pipeline 
connecting a set of log-processing components. Once the log 
processor detects a new log line in the operation log file, the 
log line goes through the processing components within the 
pipeline.  
Log Annotator
Timer Setter
Trigger
Operation 
log
r
e
t
l
Ô¨Å
e
s
o
N
i
Process 
annotator
Assertion 
annotator
Periodic 
timer 
starter
Periodic 
timer 
terminator
Assertion 
evaluation
Conformance 
Checking
Figure 3.   Local Log Processor 
Log 
storage
Noise filters drop any log line that is not relevant to the 
current operation process based on regular expressions. The 
log  annotator  annotates  the  matched  log  line  with  context 
information  using  tags.  The  timer  setter  uses  the  log  line 
indicating  the  start  of  the  operation  process  to  start  the 
periodic timer and uses the log line indicating the end of the 
operation  process  to  stop  the  periodic  timer  ‚Äì  see  Section 
III.B.3 for details. The trigger uses the matched log line and 
annotated process context to trigger Conformance Checking 
and  Assertion  Evaluation.  Finally,  relevant  log  lines  are 
forwarded to the central log storage.  
2)  Conformance Checking 
Conformance  checking  in  process  mining  refers  to 
methods and algorithms for comparing if an event log fits a 
process model (or vice versa). Say, the log contains event A 
followed by B; conformance checking tells us if the process 
model permits this, or not. This is also referred to as fitness: 
to  which  degree  does  the  log  and  the  model  fit?  Several 
algorithms  were  proposed  to  this  end  in  the  literature.  We 
make  use  of  the  token  replay  technique  from  [3],  Chapter 
7.2,  adapted  from  Petri  Nets  to  the  semantics  of  Business 
Process  Model  and  Notation  (BPMN)  [7].  Conformance 
checking can detect the following types of errors: 
‚Ä¢ Unknown: a log line that is completely unknown. 
‚Ä¢ Error: a log line that corresponds to a known error. 
‚Ä¢ Unfit:  a  log  line  corresponds  to  a  known  activity,  but 
that should not happen given the current execution state 
of  the  process  instance.  This  can  be  due  to  skipped 
activities  (going  forward  in  the  process)  or  undone 
activities (going backwards). 
In  order  to  check  conformance  in  a  near-real-time 
fashion, the local log agent sends a message to the service for 
each event, containing the process model ID (or process ID), 
the trace ID (or process instance ID), and the whole log line 
as outputted by respective system.  
Upon receiving a message, Conformance Checking looks 
up the process instance, if it is known; if not, a new instance 
is  created.  Then  it  tries  to  match  the  log  line  against  the 
regular  expressions  for  identifying  which  process  activity 
this  line  belongs  to.  If  no  match  is  found,  the  log  line  is 
tagged  as  ,("  -.  This  is  commonly 
the case for error messages or unusual execution paths in the 
software, and treated like a detected error. If the line matches 
a regular expression of a known error, the line is tagged as 
,(-. 
If  the  activity  to  which  the  line  belongs  was  identified, 
we check if this activity was expected to occur in the current 
state of the process instance ‚Äì if it was ‚Äúactivated‚Äù. If so, we 
proceed  the  token  replay  with  the  execution  of  the  found 
activity, store the new process instance state, and tag the log 
line with ,(!-. If the activity was not activated, 
it was executed out of turn ‚Äì in which case we tag the line as 
,("!-, which also indicates a detected error. 
Any  detected  error  triggers  error  diagnosis  (see  Section 
III.B.4).  From  the  current  execution  state  of  the  process 
instance,  we  can  derive  the  process  context,  except  for  the 
outcome of a step ‚Äì this is done by assertion evaluation. We 
can further derive the error context: the last valid state of the 
process  before  the  error,  the  last  activity  that  executed 
successfully, and the hypothesized skipped/undone activities. 
3)  Assertion Evaluation 
Log processor & trigger
On-demand 
trigger
  Assertion Evaluation 
Pre-deÔ¨Åned 
assertions
Assertion library
Timer
Consistent 
AWS API
AWS API  
Monitor
ConÔ¨Ågure
Repository
Figure 4.   Assertion checking 
The  expected  outcomes  of  each  intermediary  step  are 
captured  as  assertions  and  evaluated  at  different  times  for 
different  purposes  (e.g.  upon  a  step‚Äôs  completion  for  error 
detection or upon error/failure detection for error diagnosis). 
As  shown  in  Figure  4,  Assertion  Evaluation  can  be 
triggered by three mechanisms. The local log processor is the 
primary  trigger,  which  triggers  assertions  according  to  the 
process context information annotated on the log lines. Other 
trigger mechanisms include timers and on-demand command 
triggers.  We  have  two  types  of  timers:  one-off  timers  and 
periodic timers. A one-off timer is used to check an assertion 
at a specified time point. Ideally, every step of the operation 
process  produces  logs  indicating  the  start,  in-progress  and 
the  end  of  the  step.  Normally,  the  log  line  indicating  the 
completion  of  the  step  triggers  the  assertion  evaluation. 
However,  sometimes  there  is  no  log  line  indicating  the 
completion of a certain step. In such cases, we set a timer to 
trigger the corresponding assertion evaluation after a period 
of time. A periodic timer is used to check an assertion every 
so  often.  As  we  mentioned  in  Section  III.B.1,  normally  we 
set  the  periodic  timer  at  the  beginning  of  an  operations
process,  and  stop  the  periodic  timer  at  the  end  of  the 
operation process. In cases where a certain log event occurs
periodically, we use log to adjust the timer setting to keep the 
timeout, 
timer  aligned  with  the  process  context.  For  example,  for  a 
periodic log event we can set a timer when the first log event 
occurs. The timeout value is set to the duration after which 
the  second  log  event is  expected  to  occur,  plus  some  slack 
time.  The  values  are  usually  based  on  measured  historical 
timing profiles and process mining. If the second log event 
occurs  before 
the  corresponding  assertion 