to extend the Prepose grammar, regenerate the parser,
and provide runtime support for the added transform or
restriction. Note also that the Prepose grammar lends
itself naturally to the creation of developer tools such as
context-sensitive auto-complete in an IDE or text editor.
4.1 Prepose to SMT Formulas
Prepose compiles programs written in the Prepose
language to formulae in Z3, a state-of-the-art SMT solver.
Translating basic transforms: Figure 11 captures the
principles of translating Prepose transforms into Z3
terms; the ﬁgure shows the diﬀerent variants of how
rotatePlane from Figure 10 is translated by way of illus-
tration. These are update rules that deﬁne the (cid:2)X, Y, Z(cid:3)
coordinates of the joint j to which the transformation is
applied. Note that rotatePlane transformations take the
plane p and direction d as parameters. Depending on the
type of rotation, namely, the rotation plane, one of these
rules is picked. These coordinate updates generally require
a trigonometric computation3.
Translating basic restrictions: Figure 12 shows how
Prepose restrictions are translated to Z3 constraints.
Auxiliary functions Angle and Distance that are further
compiled down into Z3 terms are used as part of com-
pilation. Additionally, thresholds thangle and thdistance
2For researchers who wish to extend Prepose, we have up-
loaded an a ANTLR version of the Prepose grammar to http:
//binpaste.com/fdsdf
3Because of the lack of support for these functions in Z3, we
have implemented sin and cos applied to a using lookup tables for
commonly used values.
Align
Align(j1, j2)
Γ (cid:2) Angle(j1, j2)  j2.Z + thdistance
Put-Behind
Put-Behind(j1, j2, BehindYour)
Γ (cid:2) j1.Z  j2.X + thdistance
Put-Left
Put-Left(j1, j2, ToTheLeftOfYour)
Γ (cid:2) j1.X  j2.Y + thdistance
Put-Below
Put-Below(j1, j2, BelowYour)
Γ (cid:2) j1.Y  {
Z 3 P o i n t 3 D up = new Z 3 P o i n t 3 D (0 , 1 , 0);
return Z3 . Context . MkAnd (
body . Joints [ J o i n t T y p e . Head ]
. I s A n g l e B e t w e e n L e s s T h a n ( up , 45) ,
body . Joints [ J o i n t T y p e . Neck ]
. I s A n g l e B e t w e e n L e s s T h a n ( up , 45));
} );
Inner validity: We also want to ensure that our gesture
are not inherently contradictory, in other words, is it the
case that all sequences of body positions will fail to match
the gesture. An example of a gesture that has an inner
contradiction, consider
put your arms up ;
put your arms down ;
Obviously both of these requirements cannot be sat-
isﬁed at once.
In the Z3 translation, this will give
rise to a contradiction: joint[”rightelbow”].Y = 1 ∧
joint[”rightelbow”].Y = −1. To ﬁnd possible contradic-
tions in gesture deﬁnitions, we use the following query:
¬∃b ∈ B : G(b).
Protected gestures: Several immersive sensor-based sys-
tems include so-called “system attention positions” that
users invoke to get privileged access to the system. These
are the AR equivalent of Ctrl-Alt-Delete on a Windows
system. For example, the Kinect on Xbox has a Kinect
Guide gesture that brings up the home screen no matter
which game is currently being played. The Kinect “Return
to Home” gesture is easily encoded in Prepose and the
reader can see this gesture here: http://bit.ly/1JlXk79.
For Google Glass, a similar utterance is “Okay Glass.”
On Google Now on a Motorola X phone, the utterance
is “Okay Google.”
We want to make sure that Prepose gesture do not
attempt to redeﬁne system attention positions.
∃b ∈ B, s ∈ S : G(b) = s.
where S ⊂ B is the set of pre-deﬁned system attention
positions.
Conﬂict detection: Conﬂict detection, in contrast, in-
volves two possibly interacting gestures G1 and G2.
∃b ∈ B : G1(b) = G2(b).
Optionally, one could also attempt to test whether com-
positions of gestures can yield the same outcome. For
example, is it possible that G1 ◦ G2 = G3 ◦ G4. This can
also be operated as a query on sequences of bodies in B.
5 Experimental Evaluation
We built a visual gesture development and debugging
environment, which we call Prepose Explorer. Figure 13
shows a screen shot of our tool. On the left, a text entry
box allows a developer to write Prepose code with proper
syntax highlighting. On the right, the tool shows the user’s
current position in green and the target position in white.
On the bottom, the tool gives feedback about the current
pose being matched and how close the user’s position is to
the target.
5.1 Dimensions of Evaluation
Given that Prepose provides guarantees about security
and privacy by construction, we focused on making sure
that we are able to program a wide range of applications
that involve gestures, as summarized in Figure 14 and also
partially shown in the Appendix. Beyond that we want to
ensure that the Prepose-based gesture matching scales
well to support interactive games, etc. To summarize
• We used this tool to measure the expressiveness of
Prepose by creating 28 gestures in three diﬀerent
domains.
• We then ran some benchmarks to measure runtime
performance and static analysis performance of Pre-
pose. First, we report runtime performance, including
the amount of time required to match a pose and the
time to synthesize a new target position. Then, we
discuss the results of benchmarks for static analysis.
Prior work has used surveys to evaluate whether the
information revealed by various abstractions is acceptable
to a sample population of users in terms of its privacy.
Here, we are giving the application the least amount of
information required to do its jobs, so these surveys are
not necessary.
131131
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:14:45 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 13: Screenshot of Prepose Explorer in action.
n
o
i
t
a
c
i
l
p
p
A
s
e
r
u
t
s
e
G
s
e
s
o
P
C
O
L
URL
Therapy 12 28 225 http://pastebin.com/ARndNHdu
11 16 156 http://pastebin.com/c9nz6NP8
Ballet
5 32 314 http://pastebin.com/VwTcTYrW
Tai-chi
Fig. 14: We have encoded 28 gestures in Prepose, across three
diﬀerent applications. The table shows the number of total poses and
lines of Prepose code for each application. Each pose may be used
in more than one gesture. The Appendix has one of the Prepose
applications, Ballet, listed as well.
5.2 Expressiveness
Because the Prepose language is not Turing-complete, it
has limitations on the gestures it can express. To determine
if our choices in building the language are suﬃcient to han-
dle useful gestures, we built gestures using the Prepose
Explorer. We picked three distinct areas: therapy, tai-chi,
and ballet, which together cover a wide range of gestures.
Figure 14 shows the breakdown of how many gestures we
created in each area, for 28 in total. These are complex
gestures: the reviewers are encouraged to examine the code
linked to from Figure 14.
For example, Figure 15 shows some of the poses from tai-
chi captured by Prepose code. We chose tai-chi because
it is already present in Kinect for Xbox games such as
Your Shape: Fitness Evolved. In addition, tai-chi poses
require complicated alignment and non-alignment between
Fig. 15: The tai-chi gestures we have encoded using Prepose
(http : //pastebin.com/VwTcTYrW) all come from this illustration.
diﬀerent body parts.
5.3 Pose Matching Performance
We used the Kinect Studio tool that ships with the Kinect
for Windows SDK to record depth and video traces of
one of the authors. We recorded a trace of performing
two representative gestures. Each trace was about 20
132132
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:14:45 UTC from IEEE Xplore.  Restrictions apply. 
500
450
400
350
300
250
200
150
100
50
0
0
2
4
6
8
10
12
14
16
Fig. 16: Time to check for safety, in ms, as a function of the number
of steps in the underlying gesture.
seconds in length and consisted of about 20,000 frames,
occupying about 750 MB on disk. We picked these to be
two representative tai-chi gestures.
Our measurements were performed on an HP Z820 Pen-
tium Xion E52640 Sandy bridge with 6 cores and 32 GB
of memory running Windows 8.1.
For each trace, we measured the matching time: the
time required to evaluate whether the current user posi-
tion matches the current target position. When a match
occurred, we also measured the pose transition time: the
time required to synthesize a new target pose, if applicable.
Our results are encouraging. On the ﬁrst frame, we
observed matching times between 78 ms and 155 ms,
but for all subsequent frames matching time dropped
substantially. For these frames, the median matching time
was 4 ms. with a standard deviation of 1.08 ms. This is
fast enough for real time tracking at 60 FPS (frames per
second).
For pose transition time, we observed a median time
of 89 ms, with a standard deviation of 36.5 ms. While this
leads to a “skipped” frame each time we needed to create
a new pose, this is still fast enough to avoid interrupting