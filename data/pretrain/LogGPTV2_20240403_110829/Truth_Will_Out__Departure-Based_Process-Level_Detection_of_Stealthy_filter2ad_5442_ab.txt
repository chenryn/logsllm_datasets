measurements through a training phase, during which ideas from
singular spectrum analysis are applied to extract signal informa-
tion from process output under normal conditions. Thereafter, the
system continuously checks if incoming observations are departing
from the normal behavior captured during the training phase. The
basic idea behind departure detection is explained in §2.4.
Pasad consists of four steps formally defined in §2.3. In the first
step, the time series of sensor measurements is embedded in a
Euclidean space, which means that pasad mostly deals with vec-
tors and vector spaces. Therefore, after we motivate our approach
in §2.1, we proceed by introducing preliminary concepts in linear
algebra in §2.2. In the second step, a spectral decomposition of a
special matrix derived from the time-series data is performed to ex-
tract the deterministic part of the system behavior. In the third step,
a signal subspace is identified and vectors corresponding to sensor
measurements during normal operating conditions are projected
onto this subspace to obtain a representation of the normal process
behavior. We provide a mathematical interpretation of projecting
vectors onto the signal subspace in §2.5. In the final step, pasad
keeps track of a departure score in order to determine if the pro-
cess is departing from the normal state. In §2.6, we point out the
mathematical property that leads to the isometry trick and the two
resulting benefits mentioned in §1, which we thoroughly discuss
in §2.7 and §2.8. Finally, we discuss the choices for the parameters
involved in §2.9 and pasad’s performance in §2.10.
2.1 Motivation
A key enabling property for pasad is that ICS exhibit regular
dynamics. They tend to have static topologies and regular com-
munication patterns [11]. A typical control system utilizes sensors,
actuators, and controllers to regulate some controlled process. Sen-
sor devices measure some physical property and communicate
the measurements to a controller (e.g., a PLC), which based on a
control algorithm, correspondingly issues commands to actuators
(e.g., control valves) that directly manipulate the physical process
based on the received commands. ICS perform clearly defined tasks
with clear control objectives and in a well-controlled environment.
They typically consist of a number of control loops, each regulating
some physical property by trying to maintain its measurements
around a predefined set point. A closed-loop control system is fully
automated and performs almost exclusively without human inter-
vention, and PLCs behave in a cyclic manner [54]. This effectively
means that the same dynamics repeat constantly over time. Conse-
quently, even in the presence of noise, the level of determinism in
the behavior of control systems is relatively high. Sometimes the
control loops are nested and cascading, where the set point of one
loop is based on the process variable determined by another loop.
Even then, the dynamics are likely to repeat, since the loops operate
continuously over the duration of the process with predetermined
cycle times [48].
Mo and Sinopoli [38] point out the key problem of making the
unrealistic assumption that the system model is noiseless when
developing techniques to identify malicious behaviors in control
systems. As physical process control variables may exhibit noisy
behaviors by nature [15], they argue that in a noisy environment,
strategic attackers have the advantage of inflicting a large per-
turbation on the system state while avoiding detection by failure
detectors and anomaly detectors that do not account for noise. The
goal of strategic attackers is to cause slow damaging perturbations
in the physical process while remaining unnoticed, so that it runs
in a suboptimal setting, eventually leading to performance degrada-
tion. The expected outcome of such attacks is to cause a cascading
effect due to the interaction between control loops to eventually
induce a complete failure of the control system. Pasad is capable
of capturing the deterministic behavior of the physical process,
despite the fact that the environment in which control systems
operate is noisy.
To identify and obtain a mathematical representation of the
deterministic behavior, we partly base our technique on singular
spectrum analysis—a model-free time-series analysis tool. Vautard
and Ghil [52] describe how in nonlinear dynamical systems, the
interaction of a large number of degrees of freedom gives rise
to what is called “deterministic chaos” in time series. Then, they
argue how SSA can tell much about what deterministic part of the
system behavior recorded in a time series is due to a few degrees
of freedom and what chaotic part is due to the many rest. They
refer to the number of degrees of freedom in the former case as the
statistical dimension of the time series. In essence, this dimension
is determined by solving an eigenvalue problem of a covariance
matrix derived from the data. We describe the procedure in detail
in §2.3, but first we introduce preliminary concepts in linear algebra.
2.2 Preliminaries & Notation
Definitions and derivations of some of the mathematical concepts
that we state without proof throughout this paper can be found in
many books on linear algebra, e.g., [49].
A set B of vectors is said to span a vector space V if every vector
in V is a linear combination of the vectors in B. The set B is linearly
independent if none of its vectors is a linear combination of the
other vectors, and orthonormal if its vectors have unit length and
are pairwise orthogonal. The set B is said to be a basis for V if it
Session 5A: CyberphysicalCCS’18, October 15-19, 2018, Toronto, ON, Canada819both spans V and is linearly independent; it is an orthonormal basis
if, in addition, its vectors are orthonormal.
Let A be an m×n matrix whose entries are real. Like any matrix,
A is associated with four fundamental subspaces: its range R (A)
(or column space), its kernel K (A) (or null space), the range of its
transpose R (AT ) (or row space of A), and the kernel of its trans-
pose K (AT ) (or left null space of A). For a subspace W of V , the
orthogonal complement of W is the set W ⊥ of all vectors in V that
are orthogonal to every vector in W . We will use the fact that
K (A)
⊥ = R (AT ).
(1)
For a matrix A in Rm×n with m > n, and for a vector x not
necessarily in R (A), the orthogonal projection of x onto the range
of A is given by PAx where
−1AT
(2)
is a projection matrix. Moreover, every orthogonal projection P
satisfies the two properties
PA = A(AT A)
P2 = P (idempotence)
PT = P (symmetry).
(3)
Notation. In what follows, when we refer to A as a linear trans-
formation (or linear map), we mean the matrix representation of
some linear transformation T : Rn → Rm such that for all x in Rn,
T (x) = Ax. We denote by the matrices Aд, A+ ∈ Rn×m the general-
ized inverse and the Moore-Penrose pseudoinverse of A respectively.
We use boldface lowercase for vectors, and boldface uppercase for
matrices; || · || is the Euclidean 2-norm.
2.3 The Four Steps of pasad
Consider a univariate real-valued time series of sensor measure-
ments T = x1, x2,· · · , xN , xN +1,· · · , pasad consists of the follow-
ing four steps.
Step 1: (Embedding)
Let L be an integer, referred to as the lag parameter, then an ini-
tial subseries of T of length N is embedded in the L-dimensional
Euclidean space RL by forming K L-lagged vectors
xi = (xi , xi +1,· · · , xi +L−1)T
(4)
for all 1 ≤ i ≤ K, where K = N − L + 1, and constructing the
trajectory matrix
(5)
. . .
. . .
. . .
. . .
whose columns are the lagged vectors.
x2
x3
...
xL+1
x1
x2
...
xL
X =
xK
xK +1
...
xN
Step 2: (Singular Value Decomposition)
To extract noise-reduced signal information describing the deter-
ministic behavior of the control system, the Singular Value Decom-
position (SVD) of the trajectory matrix X is performed to obtain the
L eigenvectors u1, u2,· · · , uL of the so-called lag-covariance ma-
trix XXT . Then, the statistical dimension r of the time series—the
number of degrees of freedom that account for the deterministic
variability—is determined (see §2.9).
K(cid:88)
i =1
c = 1
K
Step 3: (Projection onto the Signal Subspace)
After the signal information has been obtained, in this step, a math-
ematical representation of the normal process behavior is identified.
Let U be an L-by-r matrix whose columns are the r eigenvectors
u1, u2,· · · , ur corresponding to the r leading eigenvalues, and let
Lr be the subspace spanned by the column vectors of U. Compute
the sample mean of the lagged vectors xi , 1 ≤ i ≤ K, as
xi
(6)
and the centroid of the cluster they form in Lr as
where P = U(UT U)
˜c = Pc
(7)
−1UT = UUT is a projection matrix (see §2.5).
Step 4: (Distance Tracking)
To detect attack-indicating structural changes in the system be-
havior, a departure score is computed for every incoming sensor
observation. For every test vector xj (j > K ), compute the squared
Euclidean distance from the centroid in Lr as
Dj = ||˜c − Pxj||2.
(8)
Finally, generate an alarm whenever Dj ≥ θ for some threshold θ.
As implied in Eq. (7) and Eq. (8), the lagged vectors are projected
onto the signal subspace Lr . The explicit projection at every it-
eration in the detection phase to compute the departure score is
computationally expensive. In §2.6, we show how such a complexity
can be avoided using the isometry trick.
Pasad runs in two phases: an offline training phase and an online
detection phase. In the training phase (steps 1-3), the time series
is embedded in the L-dimensional Euclidean space, and a signal
subspace is identified by determining the statistical dimension of
the series and the set of r eigenvectors of the lag-covariance matrix.
The lagged vectors used for training—the training vectors—are then
(implicitly) projected onto the signal subspace, and the centroid ˜c
of the cluster they form is computed. Then, in the detection phase
(step 4), pasad actively checks if lagged vectors xj (j > K)—the
test vectors—are departing from the cluster by tracking the squared
Euclidean distance Dj from the centroid.
As the first two steps are known from singular spectrum analysis,
for the sake of brevity, we refer the reader to [19] for a good treat-
ment of the SSA theory and methodology. We extend the theory
and adapt it to the problem of detecting attacks on ICS in steps 3
and 4, which we thoroughly treat in the remainder of this section.
2.4 Departure Detection: The Basic Idea
We now describe the basic idea behind departure detection. As
explained in §2.3, the first embedding step results in K vectors xi
that lie in the L-dimensional space RL—the trajectory space. Then,
the singular value decomposition of the trajectory matrix X yields
an orthonormal set of L eigenvectors. Some r < L of these eigenvec-
tors, associated with the largest eigenvalues, span an r-dimensional
linear subspace Lr ⊂ RL, which we refer to as the signal subspace.
The matrix U, whose columns are the r orthonormal eigenvectors,
is then formed so that P = UUT is the projection matrix that maps
the column vectors xi of X to the subspace Lr . The projected train-
ing vectors occupy a dense region in Lr and thereby form a cluster
Session 5A: CyberphysicalCCS’18, October 15-19, 2018, Toronto, ON, Canada820(see Figure 1). The centroid of the cluster is then computed by find-
ing the vector ˜c in Lr that minimizes the average squared Euclidean
distance from all projected training vectors.
Departure. As the time series of sensor measurements T contin-
ues beyond N , if the mechanism generating its values (the physical
process) has not changed, then new projected lagged vectors in
Lr should lie close to the cluster. Therefore, the distance between
these vectors and the centroid of the cluster should remain reason-
ably small. If, on the other hand, the mechanism generating the
time series changes due to some outside action (attacks), then the
projected test vectors will be forced to lie further away from the
cluster and consequently, the distance between these vectors and
the centroid of the cluster is expected to increase. A departure is
detected if this distance crosses a prescribed threshold.
In §2.8, we validate the claims we have just made; namely that
(i) training vectors form a cluster in the signal subspace; (ii) test
vectors fall close to the cluster under normal process operation;
and (iii) test vectors depart from the cluster when the process is
under attack. In particular, we show that the linear subspace Lr
is isomorphic to the r-dimensional Euclidean space, allowing us to
visualize the structure of the projected lagged vectors in R3. Next
we provide a mathematical interpretation of projecting the lagged
vectors onto the signal subspace.
2.5 Projection onto the Signal Subspace
In the process of obtaining a mathematical representation of the
normal process behavior, the training vectors are projected onto
the signal subspace Lr (step 3). Here, we give a mathematical in-
terpretation of this projection. In §2.7, we show that this projection
is in fact implicit as a result of the isometry trick.
The r eigenvectors obtained from the SVD of the trajectory ma-
trix X form an orthonormal basis for the signal subspace (see §2.2),
which is presumed to contain most of the signal information recorded
in the time series of sensor measurements. The central idea of pasad
is then to check whether or not current sensor observations, in the
form of lagged vectors in the trajectory space, conform with the
information obtained about the signal during normal process oper-
ation, in the form of a subspace of the trajectory space. Naturally,
since Lr is a subspace of RL and every lagged vector x resides in
RL, we opt to find the best representation of x in Lr . By best repre-