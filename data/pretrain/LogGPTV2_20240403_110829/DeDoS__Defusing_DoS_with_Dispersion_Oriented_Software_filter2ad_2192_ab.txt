it detects that some MSU instances are overloaded (e.g., due to an
unknown attack; Figure 1d), it can create additional instances of
these MSUs, placing them on machines where resources are still
available (Figure 1e). Thus, the data center can defend itself against
the attack with all available resources, not merely the ones that
happen to be “in the right place.”
execution path. This tradeoff has already been unveiled in the past
with, for example, the fall of mainframe computers and the rise of
microservices [16, 19].
The general approach we advocate is based on the microservices
design [31]: MSU split points are appropriate when there are loose
couplings between components, functional domains are clearly
encapsulated, and individual components are provably stables.
For known attacks, it is also advantageous to purposefully de-
marcate MSUs to most optimally respond to the potential attack.
For example, to protect against a SYN flood, the portion of the TCP
stack that handles TCP connection state could be isolated into its
own MSU.
However, a key benefit of DeDoS is that it does not require
apriori knowledge of the attacks it defends against. Hence, in many
instances, programs may not be perfectly spliced to optimally match
a novel attack. Indeed, this is our expectation and observation in
practice. In such instances, MSUs may contain features unrelated to
the attack, resulting in non-optimal resource allocation. However,
we emphasize that such duplication will always be preferable and
is very likely far better than naïve replication. In general, we posit
that splitting software components following a microservices-like
programming paradigm will yield significant protection against
DoS while incurring limited overheads. We empirically measure
these overheads in a number of applications, constructed using this
design pattern, in §7.
3.2 Inter-MSU communication
MSUs communicate with each other by exposing an API that can
be called by other MSUs. The API functions are asynchronous
and one-way. This enables efficient event-driven implementations
(analogous to SEDA [39]). If a call needs to return a value, this is
handled by another call in the reverse direction.
Communicating MSU instances can reside on different machines.
DeDoS makes this transparent to the MSUs by injecting a bit of
“glue code” that converts calls into a local function call (if the callee
is on the same machine) or a network packet (if the callee is remote).
3.3 Routing tables
When an MSU instance of type X wants to invoke a function on an-
other MSU of type Y, X does not need to know where the instances
of Y are currently located. DeDoS handles routing by maintain-
ing a routing table, configured by the controller, which contains
information about MSU types and implements customizable load
balancing policies and routing functions. By default, DeDoS spreads
the load evenly among MSU instances of the same type, enforcing
instance affinity to related packets (e.g., from the same flow or user
session). As we show below, we can extend this policy to implement
queue-length based routing.
3.1 Minimum splittable units
When designing an application for DeDoS, the question of defining
the granularity and boundaries of MSUs arises. While smaller MSUs
can result in a more precise response during an attack—since it
allows DeDoS to replicate only the functions that the adversary
is actually targeting—too small and numerous MSUs can result in
unacceptable overheads because of the delay introduced on the
3.4 DeDoS runtime API
So far, we have treated the dataflow graph as largely static. However,
the controller can also dynamically create new MSU instances. To
make this possible, each machine in a DeDoS deployment runs the
DeDoS runtime. When it is first started, the runtime process is an
empty shell: it contains the code for all the MSUs that the system
could create, but none of this code is active yet. The runtime listens
ACSAC ’18, December 3–7, 2018, San Juan, PR, USA
H.M. Demoulin, T. Vaidya et al.
Figure 1: Example use case of DeDoS. The software is built using MSUs (a), represented as a dataflow graph (b). MSUs are then scheduled on the
available machines (c). When an attacker attempts to overload one of the components (d), DeDoS disperses the attack by generating additional
instances on other machines (e).
for commands from the controller. The add command creates a new
MSU instance, while remove deletes one. Those operations involve
adjusting the routing table of connected MSUs. MSU also expose an
API, to execute their main function, or access their internal state.
The full API is documented online [8].
time. Because of this, we privilege user level scheduling and context
switching, using a set of kernel threads that are each pinned to a
particular core. This approach has the additional advantage that it
does not require changes to the kernel.
DeDoS schedules MSUs at the granularity of events. On each
core, DeDoS maintains a local scheduler, and a “data queue” for
each of the local MSU instances, which stores the incoming mes-
sages. Whenever the core is idle, the scheduler thread picks an MSU
instance according to a chosen policy, picks a message from that
MSU instance’s data queue, delivers that message, and waits for the
MSU instance to finish processing it. Scheduling is partitioned and
non-preemptive—cores do not “steal” messages from other cores
and they do not interrupt MSU instances while they are processing
messages. Partitioned scheduling avoids inter-core coordination in
the general case and thus keeps context-switching fast.
3.5 Support for existing applications
DeDoS is a new platform which aims at improving applications’
resilience from the very beginning of their development process.
Consequently, our focus is on enabling new applications using
DeDoS’ model. Nevertheless, we do not require applications to
be written from scratch in order to benefit from DeDoS’ defense
mechanisms. We provide a proof of concept in our case study (§6)
by splitting a user-level TCP stack into MSUs. Since DeDoS does not
require the entire software to be partitioned, rewriting existing code
can start small by only carving out the most vulnerable component
while the rest of the application runs as a single MSU.
We note that it is possible to – partially or fully – automate the
partitioning. Some domain-specific languages are already written in
a structured manner that lends itself naturally to this approach. For
instance, a declarative networking [29] application can be compiled
to an MSU graph that consists of database relational operators and
operators for data transfer across machines (see §6). Work in the
OS community [19] has shown that even very complex software,
such as the Linux kernel, can be split in a semi-automated fashion.
4 RESOURCE ALLOCATION
To ensure that the applications meet their SLAs, DeDoS needs a
way to make and enforce resource allocations to MSU instances at
runtime. DeDoS performs resource allocation at two layers: each
machine schedules MSU instances locally based on their resource
needs, whereas a central controller is responsible for decisions re-
quiring a global view, such as cloning or merging MSU instances.
To enable runtime adaption, each machine has an agent that con-
tinuously monitors local MSUs, periodically submits statistics to
the controller, and is responsible for handling the controller’s com-
mands.
4.1 Machine-local scheduling
When an application is divided into a large number of fine-grained
MSUs, switching from one MSU instance to another is a very fre-
quent operation, and we cannot afford to enter the kernel every
By default, DeDoS uses a round-robin policy, which picks at
most ri messages from data queue i and then moves on to data
queue i + 1. The parameters ri can be adjusted by the controller at
runtime, e.g., based on the relative load of the MSU instances. We
design worker threads such that each can implement specialized
policies (e.g., Earliest Deadline First – EDF).
4.2 Initial MSU assignment
When a DeDoS deployment is first started or a new application
is launched, the controller finds an initial assignment of MSU in-
stances to machines, such that the application’s SLA goals (through-
put and end-to-end latency) are met. The assignment must be fea-
sible: each machine must have sufficient resources (e.g memory)
to execute the MSU instances that are assigned to it. To find such
an assignment, we can formulate them as constraints and use an
existing solver or bin-packing heuristic.
Since the controller cannot predict the effects of future attacks,
the computed assignment only maintains the SLAs in the absence of
an attack. However, the controller monitors the system at runtime,
using the statistics that are submitted by the agents. If it detects that
the SLAs are being violated (e.g., due to a DoS attack), it adjusts the
assignment to mitigate the effects of an attack, using the process
we describe next.
4.3 Cloning and merging
The controller supports customizable cloning and merging policies.
The DeDoS native heuristics (i.e., default policies) operate by ana-
lyzing the collected metrics; forming a decision about whether to
clone, merge, or do nothing; and executing the chosen action.
ABCDEABCDEABCDEABCDEABDECCC(a)	Software	built	using	MSUs(b)	Graph	of	MSUs(c)	Normal	schedule(d)	System	under	attack(e)	System	after	MSU	C	has	been	clonedDeDoS: Defusing DoS with Dispersion Oriented Software
ACSAC ’18, December 3–7, 2018, San Juan, PR, USA
graph and assigns MSUs to machines based on the placement algo-
rithm described in §4. MSU cloning is on-demand and automated.
We set the following default parameters for DeDoS management
policies (see §4.3): the controller clones an MSU type if all runtimes
are utilizing more than 40% of the memory or file descriptor (FDs)
pool, and the type accounts for at least 50% of its runtime utilization
of that resource; for removal, the MSU type must not be contributing
more than 40% of the memory or FDs pool on any runtime. Removal
fails if an attempt to clone an MSU of that type was made in the last
20 seconds, or if less than 5 seconds elapsed since the last removal
of that type or its dependencies. Those parameters are based on
our domain expertise of how our testbed performs.
The controller also establishes routing policies. Within a route
toward an MSU type, endpoints are weighted proportionally to the
ratio of all requests enqueued for that type, over the endpoint’s
queue length (effectively the inverse of the occupancy ratio of the
endpoint). This allows DeDoS to load balance requests across all
instances of a type.
Local runtime: Each local runtime schedules and executes MSUs.
With some notable exceptions explained below, the local runtime
maintains a POSIX thread for each CPU core, which schedules
the MSUs for execution. The specific MSU-to-thread bindings are
determined by the controller.
Controller-runtime communication: Each runtime maintains
long-lived TCP connections to the controller and every other run-
time instance. These connections are carried over an isolated man-
agement network, and are used to manage MSUs and to pass data
between remote MSUs. Additionally, the local agent at each run-
time periodically gathers MSU and system statistics (such as queue
length, execution time, number of file descriptors in use, etc) that
are then sent to the controller.
5.2 DeDoS local runtime
The internal design of the DeDoS runtime consists of MSUs, worker
threads, and local agents.
MSUs: MSUs are constructed as a collection of C/C++ functions
with a well-defined API. Each MSU maintains a data queue that
contains incoming requests. MSUs are executed within worker
threads (explained next) that persist in the local runtime. As output,
an MSU may enqueue messages onto the data queues of other MSUs.
This is handled by efficient pointer manipulation when source
and destination are co-located, or by long-lived TCP connections
otherwise.
Worker threads: DeDoS proposes to either pin POSIX threads to
CPUs or not. Pinned threads are used for operations that avoid
blocking system calls, such as reading from non-blocking sockets
or TCP state table manipulation. A pinned worker thread may be
assigned multiple MSUs (its MSU pool). Pinned workers do not
involve the kernel’s scheduler and allow DeDoS’ operator to im-
plement their own scheduling MSU-aware algorithm. Pinning also
maximizes CPU utilization and reduces cache misses that would
otherwise occur if MSUs were migrated between cores.
In more detail, pinned threads run a scheduler (§4.1) that contin-
uously (1) picks an MSU from its pool, (2) executes it by dequeuing
one or more item(s) from its data queue and invoking the execute
Figure 2: DeDoS architecture. A controller manages local runtimes
(represented by dotted boxes).
As native policies, the controller attempts to clone an MSU if its
minimum reported queue length is greater than 0. The rationale
here is that the system is provisioned such that the expected ar-
rival rate is sustained, and in such conditions, no queue should be
built for any of the MSU instances. In addition, if all runtimes are
utilizing more than a configurable percentage of a resource (e.g.,
memory) and an MSU type accounts for a configurable percentage
of a runtime’s utilization of that resource, the controller will begin
to clone MSUs of that type. This latter policy targets the system’s
bottlenecks by increasing parallelization.
Once the decision to clone is made, the controller picks a satisfy-
ing machine for the new instance, favoring locality with the clone’s
neighbors in the dataflow graph. A local machine is best to mini-
mize network communication. Once a machine has been elected,
the controller picks the least loaded core which does not already
host an instance of the same type, and contacts the machine’s agent
to spawn the instance. The controller also updates all the relevant
routing tables to enforce the load balancing policy in place for this
MSU type.
An attempt to clone will fail if it is not possible to place the
clone on any available runtime or if the same type of MSU has been
recently cloned.
The controller removes cloned MSU instances when they are no
longer needed (e.g., when an attack ends). Two conditions must be
met for an MSU to be removed. First, the last runtime where a clone
has been placed must report a maximum queue length of 0 in the
last monitoring interval (following the rationale described earlier);
second, the MSU type must not be significantly contributing to
more than a configurable percentage of a resource consumption
on any runtime. An attempt to remove will fail if an attempt to
clone an MSU of that type was made in the recent past (to protect
against system oscillation), or if some configurable amount of time
has passed since the last removal of that type.
5 IMPLEMENTATION
The DeDoS implementation consists of 10K lines of C code, and is
available on GitHub [8] under GPLv3.
5.1 Overview
Our prototype (Figure 2) consists of two key components: a con-
troller that orchestrates all other machines and a local runtime
running on each machine.
Controller: The controller performs load balancing and responds
to DoS attacks. It takes as input DeDoS applications in the form of a
Global ControllerLocal Agent…Worker threadsLocal Agent…Worker threadsLocal Agent…Worker threads…ACSAC ’18, December 3–7, 2018, San Juan, PR, USA
H.M. Demoulin, T. Vaidya et al.
function of the MSU’s API, and (3) repeats. Pinned threads currently
schedule MSUs in a round-robin fashion.
In our current implementation, we assign MSUs that have block-
ing operations (e.g., disk I/O) to their own non-pinned threads, such
that they are scheduled by the Linux kernel as generic kernel-level
threads. In later versions of DeDoS, we anticipate supporting MSU
preemption and resumption, which will allow blocking MSUs in
the MSU pools of pinned threads.
Each worker thread keeps statistics on resource usage of each of
its MSUs, and global metrics such as their data queue lengths and
number of page faults. In addition, MSU’s API allow programmers
to implement custom metrics (e.g., frequency of access of a given
URL in an HTTP MSU). Those statistic are then gathered by the
local agent (explained below) to be sent to the controller.
Finally, each worker thread periodically runs an update man-
ager that processes the thread’s thread queue. Unlike the MSU data
queues, the thread queue is solely for control messages, and is