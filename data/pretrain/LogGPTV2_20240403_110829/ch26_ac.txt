 这是系统设置拓扑并提供针对任何给定时间发生的情况的高级配置的部分。这包括路由表、后端主机注册、流量修改规则、配额等。
使用地铁类比，数据平面是从 A 点到 B 点的实际地铁车流。控制平面是一个高级开关系统，偶尔切换轨道，也就是关于多少列火车应该在何时间怎样前进，等等。
部署服务网格体系结构时，必须分离系统中的不同组件。边三轮代理本身是数据平面。但是，需要一个控制平面，该控制平面可以让用户进行自定义的配置，这通常相当有意见，并且取决于正在使用的部署和配置管理系统，并将该配置转换为边三轮代理可以理解的格式。然后，必须将配置分发到所有边三轮代理。通常，对于代理的依赖越少的时候，系统配置就越有影响力。例如，在裸机的数据中心中运行预处理的用户，其服务发现过程与在云提供商托管的 CaaS 系统内运行的用户大不相同。边三轮代理/数据平面的目标是提供一致、可用的配置 API。在最简单的形式中，集中式配置生成器可能的工作方式为：
查找需要修改数据平面的全局系统更改。
为系统中的每个代理生成新配置。
通过某种机制将配置更改部署到系统中的每个代理。
强制代理重新加载其配置。
基于 HAProxy 的 SmartStack 基本上可以像以前描述的那样工作，并且已被许多公司成功部署。更复杂的边三轮代理提供动态配置 API，如 #service_mesh_with_dynamic_configuration_a 所示。API用于提供路由表更改，后端主机更改，要侦听的端口以及收到新连接时要执行的操作。这种架构允许集中管理服务来控制所有代理，就像中央控制室监督地铁网络。 通常可以移动到中心位置的逻辑越多，系统就越容易管理； 一个权力集中的 API 只允许每个边三轮代理载入最基础的引导配置，且基本上只知道如何与管理服务器通信。从那时起，所有配置都受到集中管理，从而避免了任何额外的基础架构来部署配置文件，消耗流量，强制代理重新启动等等。
具有动态配置 API 的服务网格
实践中的服务网格具有动态配置 API 的服务网格
实践中的服务网格
 尽管服务网格概念最近才在微服务社区中获得青睐，但已经有一些相互竞争的解决方案和大型部署。代理解决方案包括 HAProxy、NGINX、Linkerd、Traefik 和 Envoy。完全托管的解决方案包括 SmartStack（基于 HAProxy 构建）和 Istio（基于 Envoy 构建，现在也支持 Linkerd）。以下是 Lyft 从单一应用程序到基于 Envoy 之上的完整服务网格体系结构的过渡的简短描述。这绝不是服务网格的唯一成功故事，但它是我熟悉的一个（我是 Envoy 的创建者），并希望提供一些通用服务网格架构的具体实现，也就是本章大多数篇幅在讨论的东西。
Envoy 在 Lyft 的起源与发展Envoy 在 Lyft 的起源与发展
   到 2015 年初，Lyft 拥有一个由 MongoDB 支持的主要单体 PHP 应用程序，以及数十个用 Python 编写的微服务。当时的应用程序部署在 AWS 内的单个区域中。Lyft 已决定跳转到微服务架构，其原因几乎与其他团队一样：解耦和提高敏捷性。然而，早期的分解尝试并不顺利，主要是由于本章已经提出的所有原因。网络总是不可靠。Lyft 开发人员在调试网络故障和尾部延迟问题时遇到了巨大的麻烦。在某些情况下，计划服务被中止，并且更多功能被添加到单体应用中，因为网络被认为太不可靠，难以支持工作负载。重要的是还要注意 Lyft 目前没有类似 SRE 的职称；预计所有开发人员都将以高可靠性运营其服务。（行业向 DevOps 的转变非常有趣并且值得用单独的章节来讨论；这里只提到涉及 Lyft 的服务网格迁移相关的一些话题。）Envoy 于 2015 年初开始开发。该项目的目标是构建一个性能非常高的网络基板，让 Lyft 的开发人员能信任并对他们完全透明。这不是一夜之间发生的。Envoy 首先部署在 Lyft 作为边缘代理，以取代和增强现有的 AWS ELB。从那一刻开始，可观察性的提升和多协议支持的功能开始证明它对产品问题的分类和调试非常有价值。
在部署 Envoy 作为 Lyft 的边缘代理后，开发工作拓展到 MongoDB 的稳定方面。我们添加了一个二进制 JSON （BSON） 解析器，该解析器可以检查 MongoDB 的流量以及有助于限制应用程序和数据库之间连接数的原始 TCP 代理。稍后，我们在应用程序和数据库之间添加了全局速率限制支持。 Envoy 允许在我们的所有应用程序堆栈中即刻提供这些增强功能，而无需修改应用逻辑。随着时间的推移，Envoy 被部署作为一个副车与Lyft的每一个应用程序。在此期间，我们删除了所有内部的集中负载均衡器，构建了最终一致的服务发现系统和 API，并在所有 Envoy 之间部署了密集的 HTTP/2 网格。服务网格的存在为 Lyft 开发人员开启了大量的功能，本章已经介绍了所有这些功能。
Lyft 如何配置和操作 Envoy 也随着时间的推移而演变。最初，所有配置都是手写的，并通过最终一致的部署过程和使用 Salt 与二进制文件一起部署。随着时间的推移，又切换到模板化，并使用 Python 和 Jinja 部分机器生成的配置。在撰写本文时，Lyft（和大多数 Envoy 的其他用户）正在转向一个完全集中的配置系统，该系统由一组完整的发现 API 提供支持。在 Lyft 部署 Envoy 的最终结果是：开发人员在构建应用程序时不再考虑网络。当网络问题发生时，开发人员拥有工具来帮助快速发现和修复问题。部署在 Lyft 的网络组件提高了开发人员的工作效率，提高了总体成功率，并减少了发生事故期间的平均恢复时间 （MTTR）。
Lyft 运营的 Envoy
   正如我已经描述的那样，无论好坏，Lyft 都没有类似 SRE 的职称。相反，所有开发人员都应该兼任可靠性工程师。我领导 Lyft 的网络团队，除了开发Envoy，我们也维护它。虽然我对 DevOps 文化的出现总是有强烈的意见（超出了本章的范围），对于像 Envoy 这样的系统组件，我认为让系统的开发人员也负责维护是一个无需多说的选择，只有这样才能确保服务网络真正实现了所谓的透明网络。在以下小节中，我简要总结了这方面的一些更有趣的知识。
运维的经验
自动创建默认仪表板、跟踪、日志和警报运维的经验
自动创建默认仪表板、跟踪、日志和警报
 在 Lyft，我们为每个服务创建了软件仪表面板，包括与 Envoy 相关的统计信息、日志记录和跟踪。根据这些数据，我们会自动为每个服务创建警报。这将为每个服务创建一个操作基线，从而更容易深入了解并开始调试问题。此外，我们有一个服务到服务的仪表板，允许用户从下拉列表中选择出口和入口服务，并立即查看该跃点的相关统计信息。最后，我们有一个“全局”Envoy 仪表板，该仪表板将服务网格范围的统计信息聚合到一个位置，并允许对全系统运行状况进行细微的检查。
文档
 我不能说明文档的重要程度以及我们在行业内投资的程度。 在DevOps 环境中，文档尤其重要；虽然我们希望所有开发人员都是可靠性工程师，但实际上，该领域的知识基础差异很大。我的团队花费了大量时间来帮助调试系统范围的问题，如果没有我们可以指向开发人员的高质量文档，我们就会精力透支。这主要相当于一整套关于 Lyft 特定的仪表板、警报、参考步骤、常见问题解答、配置指南等内部 Envoy 文档，这些文档比公共 Envoy 文档更具体，更易于没有网络经验的开发人员访问。模板化配置生成
Envoy 配置极其复杂。我们允许 Lyft 的开发团队通过网络团队控制的模板灵活地仅更改其中的一小部分。这允许大多数系统保持一致性，从而更容易理解汇总，同时仍允许在适当情况下实现本地灵活性（例如每个服务的自定义熔断配置）。
热重启，便于更新和回滚
与许多组织一样，Lyft 目前无法访问生产中的不可变容器部署系统。Envoy 使具有强大的“热重启”功能，这意味着它可以完全重新启动，包括配置和二进制，而不会放弃任何连接。这使我们能够快速轻松地发布更新或回滚。
管理节点上调试的终结点
Envoy 提供了一套强大的地方管理终结点，这些终端设计为可人阅读且易于交互。尽管实现无需登录主机并查看某些内容是很好的，但实际上可靠性工程师始终这样做，而且轻松访问运行时信息对于敏捷性运维来说至关重要。
开发方面的经验
分离的边三轮部署过程开发方面的经验
分离的边三轮部署过程
 为了真正实现边三轮代理模型在开发敏捷性方面的优势，我们允许 Envoy 通过独立于每个应用程序的过渡环境、试错环境和生产环境进行完全部署。这使我们能够独立于应用程序部署过程推出新功能、修补程序和调试工具。
技术上的经验
地址解析协议（ARP）表
我为什么要提出ARP？这是我们在 Lyft 部署 Envoy 时遇到的最有趣的错误，任何大型服务网格部署都需要注意。ARP 是 IP 地址转换为下一跃点 MAC 地址（L3 到 L2）的过程。内核包含 ARP 缓存，用于存储最近使用的映射。通常，此缓存的任何抖动都会导致可怕的性能，因为在刷新缓存条目时需要重复进行重新解析。内核缓存大小通常默认为相对较小的值，该值针对传统的基于 IP 的网络进行优化，其中节点与它交谈的直接 L2 邻居数量较小。对于服务网格来说，这不一定是如此！一些现代网络设计利用大型和扁平的 L3 IP 子网。在这些设计中，如果主机可以与子网中的任何其他主机通信（特别是不使用中间负载均衡器），则邻居的数量可能很大。在 Lyft，我们必须增加每个节点 ARP 缓存的默认大小，以考虑此实际情况。在 Linux 上，这涉及到调整内核参数，如 net.ipv4.neigh.default.gc_thresh1 （和其他相关值）。其他操作系统具有类似的设置。文件描述符限制
与 ARP 表大小有些相关，允许边三轮代理创建大量文件描述符也很重要，因为代理最终将创建大量网格连接。此外，在一次糟糕的生产环境故障中，Envoy 将创建文件描述符的失败视为与内存不足（OOM）条件相同：一个致命的崩溃错误。在运行时，用尽允许的文件描述符的数量可能很难诊断。使条件（如果配置正确，应该永远不会发生）导致对严重问题的可见性更高，并最终提高可靠性。
总体而言，由于专注于增量交付，操作简便性以及我们刚才讨论的要点，通过 Envoy 在 Lyft 的服务网络的部署和运营相对平稳。    
服务网格的未来
在未来 5 到 10 年内，边三轮代理和管理系统的服务网格的开发领域将看到软件供应商和大型云服务供应商的大量投资。因为它们给应用程序开发人员和可靠性工程师带来的好处是巨大的：工程师使用并运行在正常运行的服务网格之上的微服务后，他们不太可能想要在没有代理的情况下再次部署应用程序。不必自己动手开发的好处是显而易见的。延伸阅读
Envoy proxy
“Service mesh data plane vs. control plane”
Istio service mesh
“Lyft’s Envoy dashboards”
“The universal data plane API”
“Microservices Patterns With Envoy Sidecar Proxy: The series”
“Introduction to modern network load balancing and proxying”
“Embracing eventual consistency in SoA networking”
编者介绍
马特·克莱因是 Lyft 的软件工程师，也是 Envoy 的创造者。Matt 在各种公司中从事操作系统、虚拟化、分布式系统和网络工作超过 15 年，专注于使系统易于维护。一些亮点包括领导 Twitter 的 C++ L7边缘代理的开发，以及亚马逊 EC2 中高性能计算和网络的发展。