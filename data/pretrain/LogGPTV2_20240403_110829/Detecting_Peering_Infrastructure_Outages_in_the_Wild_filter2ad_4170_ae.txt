### 5. Validation and False Positives/Negatives

We validated our findings using data from 18 Network Operating Centers (NOCs) [75] and privately shared information. Out of the 159 detected outages, we successfully confirmed 53 as true positives (Figure 1). Additionally, we identified six false positive cases where Kepler accurately pinpointed the location of the incident, but the actual root cause was fiber cuts affecting multiple co-located Autonomous Systems (ASes). In terms of false negatives, Kepler did not miss any full outages that affected trackable facilities. However, we found four undetected small-scale partial outages that impacted facilities with fewer than 30 tenants and were misclassified as AS-level incidents.

### 6. Results

#### 6.1 Detected Facility Outages

Kepler's passive detection capabilities allowed us to conduct a historical analysis of archived BGP stream and PeeringDB data from 2012 to 2016. We detected 159 outages, including 103 outages among 87 facilities and 56 outages in 41 Internet Exchange Points (IXPs), as shown in Figure 1. To contextualize the completeness of our findings, we collected facility and IXP outages reported in two popular mailing lists, NANOG [74] and Outages [67], as well as two specialized data center and colocation websites [25, 26]. These sources only reported 24% of the detected outages, missing most incidents outside the US and the UK.

Our analysis revealed that 53% of the outages occurred in Europe, 31% in the US, and the remaining in other regions. The median outage duration was 17 minutes, with 40% of the outages lasting over an hour (Figure 8b). The number of detected outages did not increase drastically over the last five years (Figure 1). Generally, IXP outages lasted longer than facility outages, possibly due to the different causes of outages. Most facility outages were caused by basic infrastructure failures such as power or fiber cuts, while IXPs also suffered from software and/or configuration issues, which took longer to resolve.

To correlate the duration of each outage with general service availability, we added support lines for 99.9%, 99.99%, and 99.999% uptime. Although this is slightly optimistic, given that five IXPs had multiple outages in the same year, 5% of the 403 monitored facilities failed to meet the 99.99% uptime mark, and 18% failed to meet the 99.999% uptime mark. Consequently, to provide services with availability beyond 99.999%, service providers must use redundant facilities.

#### 6.2 Outages in Depth: Case Studies

To demonstrate Kepler’s capability to investigate outages, we focus on three detailed case studies. The first outage occurred at AMS-IX, a major IXP in Amsterdam, NL, on May 13, 2015. The outage was caused by a loop in the switching fabric during planned maintenance [18]. Figure 8c shows the path change fractions for three different aggregation levels. The outage caused the IXP to lose almost all routes and more than 90% of the exchanged traffic for about 10 minutes. It took approximately 15 minutes for the traffic to recover. The incident was clearly visible at all aggregation levels, with the paths tagged with AMS-IX Communities showing the largest drop, indicating the actual source of the outage.

However, changes in aggregated paths can be misleading. For example, Figure 9a shows the effect of two independent facility-level outages in London [3] on the co-located London-IXP LINX and a third facility, TH East. At time A, when the first outage occurred, there was almost no change at the city level, while both LINX and TH East were affected. At time B, we observed a city-level signal, which did not correspond to a facility outage but rather a re-routing of paths from a major Tier-1 AS. At time C, when the second outage happened, we saw a major drop only through TH East. Kepler correctly identified the A and C signals as PoP-specific and the B signal as AS-specific. Instead of inferring either LINX or TH East as the potential sources of the outages, Kepler proceeded to signal localization by examining the impact on far-end ASes against the facilities where these ASes were co-located. This process is illustrated in Figure 9b, where it becomes clear that at times A and C, two major subsets of the ASes at TC HEX8/9 and TH North were affected. Far-end ASes in other London facilities showed no concurrent signs of outages, allowing us to correctly identify TC HEX8/9 and TH North as the outage sources. Also, note that at time B, only a single AS was affected. This plot highlights that ASes handle outages differently, with some returning to their stable path once the outage is over, while others remain with their new path. This set of outages illustrates Kepler’s ability to disambiguate the source of outage signals to facilities.

#### 6.3 Outages in Depth: Active Measurements

Next, we highlight the benefits of incorporating active measurements in Kepler, focusing on the outage at AMS-IX.

**Backup Paths:** Figure 10a shows the BGP path changes, while Figure 10b shows the traceroute path changes. While the overall path changes follow the same trend, the backup paths that are activated differ. The BGP Communities are mainly provided by large ASes with diverse peering connectivity, allowing them to activate alternative peering links at remote IXPs. On the other hand, most traceroute probes are hosted in local ASes, and so are the targets, resulting in 75% of the alternate routes being via transit interconnections.

**Path Restoration Time:** BGP path re-convergence took about 4 hours until 95% of the paths returned. Approximately 5% of the paths did not return even days after the outage. Such permanent route changes are either due to manual intervention or the BGP decision process, which prefers the newest path to break ties and minimize route flapping. Although 85% of the traceroute paths returned within one hour back to AMS-IX, a significant fraction continued to cross transit links. These results show that the actual impact of an outage on both control-plane and data-plane routing paths significantly outlasts the root cause of the outage, possibly necessitating a review of the SLAs provided by infrastructure operators.

**Impact on End-to-End Delays:** Kepler uses traceroute data to assess the impact of an outage on round-trip time (RTT). While RTTs from traceroute may not reflect RTTs as seen by TCP, they serve as indications of performance changes. Figure 10c shows the empirical cumulative distribution function of the RTT delays for paths traversing AMS-IX. We distinguish three time periods: (i) 20 minutes before the outage, (ii) during the outage, and (iii) 20 minutes after the outage, i.e., 10 minutes after the operation returned to normal. We separated the paths into those that use AMS-IX (AMS-IX) and those that do not (No AMS-IX) during and after the outage. During the outage, the median RTT rose by more than 100 milliseconds for rerouted paths. For unchanged paths, the median RTT increase was moderate, with some experiencing significant increases but without a substantial tail growth. After the outage, this RTT increase disappeared. Paths that returned to AMS-IX within 20 minutes experienced roughly the same RTT as before the outage. However, 30% of the paths that still used alternative interconnections continued to see increased RTTs of about 40 milliseconds due to sub-optimal routing in terms of distance.

#### 6.4 Outages: Remote Impact

**Remote Networks:** To understand the impact of infrastructure outages, we studied the locations of the ASes affected by the two London outages. We localized the IPs of the far-end interfaces of the affected ASes identified by Kepler’s traceroute using DRoP [58]. Figure 9c plots the distance from London in kilometers versus the number of affected ASes. Surprisingly, only 44% of the far-end interfaces were also in London. More than 45% of the interfaces were in a different country, with more than 20% outside Europe. The main reason for such widespread impact is the increasing popularity of remote peering, an interconnection practice that allows ASes without physical presence at a peering hub to interconnect through layer-2 transport providers that resell peering ports across remote facilities [65]. Castro et al. estimated that 20% of members in large IXPs connect remotely [16], consistent with our findings. This underlines the importance of understanding the facility-level topology when analyzing the impact of an outage.

**Remote Infrastructures:** To challenge the expectation that a "local outage has only local impact," we complemented Kepler with passive measurements from a major European IXP (EU-IXP). Figure 10d depicts the traffic volumes in Gbps at EU-IXP during the AMS-IX outage based on IPFIX data collected at its switching fabric with a sampling rate of 1/10K [21]. The two IXPs are 360 kilometers apart. During the AMS-IX outage (t0), we noticed a sharp drop in IPv4 traffic—about 10% (215 Gbps). After 10 minutes, when the AMS-IX outage stopped (t1), the traffic rose above the expected average volume, lasting for approximately 15 minutes. After the outage was restored (t2), the traffic returned to normal levels.

To scrutinize this counter-intuitive observation, we studied the per-member traffic. Only 136 out of 533 members experienced a significant reduction in traffic (mean loss of 1 Gbps, max. loss of 25 Gbps), with the rest seeing a mean growth of 188 Mbps (max. 12 Gbps). However, traffic losses dominated, even though moderate traffic increases are typical during this time of the day. The top 25 ASes with a traffic decrease accounted for 83% of the total loss. This outage is not a singular event; during other outages, we observed similar traffic reductions, albeit smaller as the distance increased.

**Remote Impact Explained:** The conventional wisdom is that network operators should use separate edge routers for each colocation facility or IXP. However, due to the high cost of edge routers, operators often use a single router for multiple facilities, introducing interdependencies among peering infrastructures, especially when they have common members. Additionally, operational best practices prioritize peering over upstream to keep traffic local and reduce costs. Thus, prefixes reachable via IXPs will use the IXP rather than an upstream provider. Consider a scenario where an ISP uses two IXPs, and the capacity of neither is sufficient for the total traffic of the ISP. While using peering links beyond 50% violates best practices, price pressure may force operators to ignore such guidelines. During outages, ISPs may rely on their upstream, causing a traffic drop at the other IXP, without extra cost for short outages due to the 95th percentile pricing [86].

The most important reason is asymmetric paths [80], which are common in today’s Internet. For peering infrastructures, we call a path asymmetric if one direction involves only facility A and the reverse path involves only facility B. An outage at either facility causes a reduction of traffic at the other. We find that more than 10% of all (source, destination) combinations between AMS-IX and EU-IXP members have asymmetric routes, accounting for most of the traffic losses at EU-IXP.

### 7. Implications

#### Implications for Policymakers

The operation of our system, Kepler, and our analysis increase transparency in Internet infrastructure outages. This can inform best practices for improving resilience and would be useful to regulators and policymakers given the critical role of such infrastructures [30, 39]. With Kepler, one can provide testimony based on hard evidence to assess the degree of violation of service level agreements, such as the 5 nines reliability, and to characterize an outage as full or partial, and to assess the impact on the operation of a network.

#### Implications for Peering

Our analysis shows that redundant peering strategies may increase resilience to outages. However, in some cases, we observed peering disruptions even when redundant peering was available. We argue that there is significant room for improving peering resilience by considering the physical isolation of peering infrastructures. Unfortunately, the interdependency among various peering infrastructures is often not well known, and greater resilience might be achieved with more collaboration between peering infrastructure operators and network operators.

#### Implications for Operation

Our study shows that an increasing number of networks tag their BGP announcements with communities, and about half of the prefix announcements include location-based communities. This practice is very helpful for detecting outages. However, we should point out that the propagation of location-based communities has a downside. The leakage of this information enables easier detection by third parties of the location where two networks establish interconnections. This leakage can be used for business intelligence and targeted attacks. Hence, we will make Kepler available via an interactive interface but will only share our dictionary of location-based communities upon request.

### 8. Conclusion

Outages at colocation facilities and IXPs significantly affect the operation of networks. Our system, Kepler, provides a robust framework for detecting and assessing the impact of these outages, offering valuable insights for policymakers, peering strategies, and operational practices. By leveraging both passive and active measurements, Kepler enhances the transparency and resilience of the Internet infrastructure.