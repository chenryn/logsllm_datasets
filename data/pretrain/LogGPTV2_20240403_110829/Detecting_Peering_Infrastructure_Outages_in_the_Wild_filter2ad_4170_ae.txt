from 18 Network Operating Centers (NOCs) [75], as well as pri-
vately shared information. We successfully validated 53 out of the
159 detected outages (Figure 1) as true positives. We also found 6
cases of false positives. In these cases, Kepler determined the correct
location of the incident but in reality the root cause of the outages
were fiber cuts that affected multiple co-located ASes. In terms of
false-negatives, Kepler did not miss any full outage that affected
trackable facilities. However, we found 4 undetected small-scale
partial outages that affected facilities with less than 30 tenants and
were mis-classified as AS-level incidents.
6 RESULTS
Next, we use Kepler to detect and assess the impact of peering
infrastructure outages during the past five years. To this end, we
provide a detailed analysis of sample incidents enabled by Kepler
and underpin our findings with active measurements to (i) confirm
outages, (ii) track path changes, (iii) measure rerouting times and
RTT increase, and (iv) infer the impact on traffic at seemingly
unrelated locations.
6.1 Detected Facility Outages
The passive detection capabilities of Kepler allow us to conduct
a historical analysis of archived BGP stream and PeeringDB data
from 2012 to 2016. Overall, we detect 159 outages that include 103
outages among 87 facilities, and 56 outages in 41 IXPs, as shown
in Figure 1. To contextualize the completeness of our findings we
collect facility and IXP outages, reported in two popular mailing
lists, NANOG [74] and Outages [67], plus two specialized data
center and colocation websites [25, 26]. They only report 24% of the
detected outages, missing most of the incidents that occur outside
the US and the UK.
We find that 53% of the outages are in Europe, 31% in the US,
and the remaining ones in the other regions. The median outage
duration is 17 minutes and 40% of the outages exceed 1 hour (see
Figure 8b). With regards to frequency we find that the number
of detected outages is not increasing drastically over the last five
years, see Figure 1. In general, we find that IXP outages last longer
than facility outages. One reason may be in the possible causes of
outages. Most facility outages are due to basic infrastructure failures,
e.g., power or fiber cuts. Hence, restoring service mainly depends
on infrastructure recovery. IXPs also suffer from software and/or
configuration failures which apparently take longer to resolve.
To correlate the duration of each outage with general service
availability, we add support lines for 99.9%, 99.99%, and 99.999%
uptime. This is slightly optimistic since 5 IXPs had multiple outages
in the same year. Still, 5% of the monitored 403 facilities fail to
meet the 99.99% uptime mark and 18% the 99.999% uptime mark.
Consequently, to provide services with availability beyond 99.999%
service providers must use redundant facilities.
Detecting Peering Infrastructure Outages in the Wild
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
(a) Fraction of AS links vs. number of facilities for
4 ASes (ground-truth and mapped).
(b) Outage durations.
(c) Detection of the AMS-IX outage through differ-
ent granularities of Communities .
Figure 8: Physical location deciphered by BGP updates (a), Outage durations (b), and AMS-IX outage case study (c).
6.2 Outages in Depth: Case Studies
To demonstrate Kepler’s capability to investigate outages we now
focus on three outages in detail. The first one occurred at AMS-IX, a
major IXP in Amsterdam, NL, at 2015-05-13. The outage was caused
by a loop in the switching fabric during planned maintenance [18].
Figure 8c plots the path change fractions for three different aggre-
gation levels. The outage caused the IXP to loose almost all routes
and more than 90% of the exchanged traffic for about 10 minutes.
It took about 15 minutes for the traffic to recover. The incident is
clearly visible in all aggregation levels, but the paths tagged with
the AMS-IX Communities show the largest drop indicating the
actual source of the outage.
However, the changes in aggregated paths can be misleading.
Indeed, Figure 9a shows the effect of two independent facility-level
outages in London [3], on the co-located London-IXP LINX, and
a third facility, TH East. At time A, when the first outage occurs,
we see almost no change at the city level aggregation, while both
LINX and TH East are affected. At time B we observe a city level
signal, which does not correspond to a facility outage but rather a
re-routing of paths from a major Tier-1 AS. At time C, when the
second outage happens, we witness a major drop only through TH
East. Kepler identifies correctly the A and C signals as PoP-specific
and the B signal as AS-specific, and instead of inferring either LINX
or TH East as the potential sources of the outages it proceeds to the
signal localization by examining the impact of the outage on the
far-end ASes against the facilities where these ASes are co-located.
This process is illustrated in Figure 9b, where it becomes clear
that at time A and C two major subsets of the ASes at TC HEX8/9
and at TH North are affected. The far-end ASes in other London
facilities (not depicted) show no concurrent signs of outage, al-
lowing us to identify correctly TC HEX8/9 and TH North as the
outage sources. Also note that at time B only a single AS is affected.
This plot also highlights that ASes handle outages differently. Some
return to their “stable” path once the outage is over, while other
remain with their new path. This set of outages illustrates Kepler’s
ability to disambiguate the source of outage signals to facilities.
6.3 Outages in Depth: Active Measurements
Next, we highlight the benefits of incorporating active measure-
ments in Kepler. We focus on the outage at AMS-IX.
Backup paths: Figure 10a shows the BGP path changes while
Figure 10b shows the traceroute path changes. While the overall
path changes follow the same trend, the backup paths that are
activated differ. The BGP Communities are mainly provided by
large ASes with very diverse peering connectivity, allowing them
to activate alternative peering links at remote IXPs. On the other
hand, the majority of traceroute probes are hosted in local ASes,
and so are the targets, and thus most, 75% of the alternate routes
are via the transit interconnections.
Path restoration time: BGP path re-convergence took about 4
hours until 95% of the paths returned. Approximately 5% of the
paths did not return even days after the outage. Such permanent
route changes are either due to manual intervention or by the BGP
decision process that prefers the newest path to break ties and min-
imize route flapping. Although, 85% of the traceroute paths return
within one hour back to AMS-IX, a significant fraction continues to
cross transit links. These results show that the actual impact of an
outage on both control-plane and data-plane routing paths signifi-
cantly outlasts the root cause of the outage, possibly necessitating
a review of the SLAs provided by infrastructure operators.
Impact on End-to-end Delays: Kepler uses the traceroute data
to assess the impact of an outage in round-trip time (RTT). While
we acknowledge that RTTs from traceroute may not reflect RTTs
as seen by TCP, they serve as indications of performance changes.
Figure 10c shows the empirical cumulative distribution function of
the RTT delays for the paths that traverse AMS-IX. We distinguish
three time periods: (i) 20 minutes before the outage, (ii) during
the outage, and (iii) 20 minutes after the outage, i.e., 10 minutes
after the operation returned to normal. Moreover, we separated the
paths into those that use AMS-IX (AMS-IX) and those that do not
(No AMS-IX) during and after the outage. During the outage the
median RTT rises by more than 100 msec for rerouted paths. For
unchanged paths, the median RTT increase is moderate, and while
some experience significant increase the tail does not grow as much.
After the outage, this RTT increase disappears. Moreover, paths
that returned to AMS-IX within 20 minutes experience roughly the
same RTT as before the outage. However, 30% of the paths that still
use the alternative interconnections continue to see increased RTTs
of about 40 msecs due to sub-optimal routing in terms of distance.
0.750.800.850.900.951.00FractionofASlinks100101102Numberoffacilities(log)GroundtruthCommunities-mapped0.00.20.40.60.81.0100101102101102103Estimatedoutageduration(minutes)0.00.20.40.60.81.0CDF99.999uptime99.99uptime99.9uptimeFacilitiesIXPs07:0008:0009:0010:0011:0012:0013:0014:000.00.20.40.60.81.0Fractionof path changesSARA FacilityAMS-IX IXPAmsterdamSIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
V. Giotsas et al.
(a) Two facility outages at dif. granularities.
(c) Link count vs. distance for TC outage.
Figure 9: Two outages at different colocation facilities in London (TC HEX8/9 and TH North) at July 20 & 21, 2016.
(b) The fraction of affected paths per facility.
(a) BGP path changes.
(b) Traceroute path changes.
(c) Effect on RTT.
(d) IPv4 traffic at EU-IXP.
Figure 10: AMS-IX outage seen by Kepler (a)-(c), and by IPFIX traffic at EU-IXP (d).
6.4 Outages: Remote Impact
Remote Networks: To understand the impact of infrastructure
outages we study the locations of the ASes that have been affected
by the two London outages. We localize the IPs of the far-end
interfaces of the affected ASes identified by Kepler’s traceroute
using DRoP [58]. Figure 9c plots the distance from London in km vs.
the number of affected ASes. Surprisingly, only 44% of the far-end
interfaces are also in London. More than 45% of the interfaces are
in a different country with more than 20% outside Europe. The
main reason for such a widespread impact of localized failures is
the increasing popularity of remote peering, an interconnection
practice that allows ASes without physical presence at a peering
hub to interconnect through layer-2 transport providers that resell
peering ports across remote facilities [65]. Castro et al. estimated
that 20% of the members in large IXPs connect remotely [16], which
is consistent with our findings. This underlines the importance
of understanding the facility-level topology when analyzing the
impact of an outage.
Remote Infrastructures: To further challenge the expectation
that a “local outage has only local impact”, we complement Kepler
with passive measurements from a major European IXP (EU-IXP).
Figure 10d depicts the traffic volumes in Gbps at EU-IXP during the
AMS-IX outage based on IPFIX data collected at its switching fabric
with sampling rate 1/10K [21]. The two IXPs are 360 kilometers
away. During the AMS-IX outage (t0), we notice a sharp drop in
IPv4 traffic—about 10% (215 Gbps). After 10 minutes – when the
AMS-IX outage stopped (t1) the traffic is rising above the expected
average volume. This lasts for approximately 15 minutes. After the
outage was restored (t2), the traffic returns to normal levels.
To scrutinize this counter-intuitive observation we study the per
member traffic. Only 136/533 members have a significant reduction
in traffic (mean loss is 1 Gbps, max. loss 25 Gbps), with the rest
seeing a mean growth of 188 Mbps (max.12 Gbps). However, traffic
losses dominate even though moderate traffic increases are typical
during this time of the day. The top 25 ASes with a traffic decrease
account for 83% of the total loss. The outage above is no singular
event. During other outages we observe similar traffic reductions,
albeit smaller as the distance increases.
Remote Impact Explained: The conventional wisdom is that net-
work operators should use separate edge routers for each colocation
facility or IXP. However, due to the high cost of edge routers, op-
erators often use a single router for multiple facilities introducing
interdependencies among peering infrastructures, especially when
they have common members. In addition, operational best practice
prioritizes peering over upstream to keep traffic local and to reduce
cost. Thus, prefixes reachable via IXPs will use the IXP rather than
an upstream provider. Consider a scenario where an ISP uses two
IXPs and the capacity of neither is sufficient for the total traffic of
the ISP. While using peering links beyond 50% violates best prac-
tices, price pressure may force operators to ignore such guidelines.
During outages ISPs may rely on their upstream causing a traffic
drop at the other IXP, without extra cost for short outages due to
the 95th percentile [86] pricing.
The most important reason is asymmetric paths [80], which are
common in today’s Internet. For peering infrastructures we call a
path asymmetric if one direction only involves facility A and the
reverse path only involves facility B. An outage at either of the two
facilities causes a reduction of traffic at the other. We find that more
07/2001:0007/2005:0007/2009:0007/2013:0007/2017:0007/2021:0007/2101:0007/2105:0007/2109:0007/2113:0007/2117:000.60.70.80.91.01.1Fractionofpathchanges(B)(A)(C)THEastFacilityLINXIXPLondon020004000600080001000012000Distancefromoutagesource(Km)101102103104105Numberofaffectedlinks(log)0.00.20.40.60.81.0CDF0.44NumberofaffectedlinksCDFofaffectedlinkst0t1t22300240025002600270009:4510:0010:1510:3010:45time (UTC)GbpsDetecting Peering Infrastructure Outages in the Wild
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
than 10% of all (source, destination) combinations between AMS-IX
and EU-IXP members have asymmetric routes, which account for
most of the traffic losses at EU-IXP.
7 IMPLICATIONS
Implications for Policymakers: The operation of our system,
Kepler, and our analysis increases the transparency in Internet
infrastructure outages. This can inform best practices for improving
resilience, and would be of use to regulators and policy makers
given the critical role of such infrastructures [30, 39]. In addition,
with Kepler one can provide testimony based on hard evidence to
assess the degree of violation of service level agreements, e.g., the
5 nines reliability, and to characterize an outage as full or partial,
and to assess the impact on the operation of a network.
Implications for Peering: Our analysis shows that redundant
peering strategies may increase the resilience to outages. Still, in
some cases, we observed peering disruptions even when redundant
peering was available. We argue that there is significant space for
improving peering resilience by taking into account the physical
isolation of peering infrastructures. Unfortunately, the interdepen-
dency among the various peering infrastructures is often not well
known, and thus greater resilience might be achieved with more col-
laboration between peering infrastructure operators and network
operators.
Implications for Operation: Our study shows that an increasing
number of networks tag their BGP announcements with commu-
nities, and that about half of the prefix announcements include
location-based communities. This practice is of great help for de-
tecting outages. However, we should point out that the propagation
of location-based communities has a downside. The leakage of this
information enables easier detection by third parties of the location
where two networks establish interconnections. This leakage can
be used for business intelligence, and for targeted attacks. Hence,
we will make Kepler available via an interactive interface. But we
will only share our dictionary of location-based communities on
request.
8 CONCLUSION
Outages at colocation facilities and IXPs affect the operation of