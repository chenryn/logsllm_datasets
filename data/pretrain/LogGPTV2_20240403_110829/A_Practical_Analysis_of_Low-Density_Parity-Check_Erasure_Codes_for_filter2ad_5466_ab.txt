tributions in Λ and P can asymptotically achieve capacity.
In other words, they may be successfully decoded with f n
downloaded blocks, where f approaches 1 from above as n
approaches ∞.
Unfortunately, in the real world, developers of wide-
area storage systems cannot break up their data into in-
ﬁnitely many pieces. Limitations on the number of phys-
ical devices, plus the fact that small blocks of data do not
transmit as efﬁciently as large blocks, dictate that n may
range from single digits into the thousands. Therefore, a
major question about LDPC codes (addressed by Question
1 above) is how well they perform when n is in this range.
Name
Source
L97A
L97B
S99
SS00
M00
WK03
RU03
R03
U03
[LMS+97]
[LMS+97]
[Sho99]
[SS00]
[McE00]
[WK03]
[RU03]
[RGCV03]
[Urb03]
# of
Codes
Rate: [ 1
3
, 1
2
, 2
3
]
2
8
19
3
14
6
2
8
22
[0,1,1]
[0,4,4]
[4,7,8]
[0,3,0]
[0,6,8]
[0,6,0]
[0,2,0]
[0,8,0]
[6,9,7]
Name
Λmax
Pmax
Developed
L97A
L97B
S99
SS00
M00
WK03
RU03
R03
U03
1,048,577
8-47
2-3298
9-12
2-20
11-50
8-13
100
6-100
30,050
16-28
6-13
7-16
3-8
8-11
6-7
8
6-19
for
Systematic
Systematic
Gallager
Gallager
IRA
Gallager*
Gallager
IRA*
Gallager
Table 1: The 80 published probability distributions (Λ and
P ) used to generate codes.
4 Assessing Performance
Our experimental methodology is as follows. For each of
the three LDPC codes, we have written a program to ran-
domly generate a bipartite graph g that deﬁnes an instance
of the code, given n, m, Λ, P , and a seed for a random
number generator. The generation follows the methodol-
ogy sketched in [LMS+97]:
(cid:1)L
(cid:1)R
For each left node li, its number of outgoing edges ξi
is chosen randomly from Λ, and for each right node ri, its
number of incoming edges ιi is chosen randomly from P .
This yields two total number or edges, TL =
i=1 ξi and
TR =
i=1 ιi which may well differ by D > 0. Suppose
TL > TR. To rectify this difference, we select a “shift”
factor s such that 0 ≤ s ≤ 1. Then we subtract sD edges
randomly from the left nodes (modifying each ξi accord-
ingly), and add (1−s)D edges randomly to the right nodes
(modifying each ιi accordingly). This yields a total of T
total edges coming from the left nodes and going to the
right nodes.
Now, we deﬁne a new graph g (cid:1) with T left nodes, T
right nodes and a random matching of T edges between
them. We use g (cid:1) to deﬁne g, by having the ﬁrst ξ1 edges
of g (cid:1) deﬁne the edges in g coming from l1. The next ξ2
edges in g (cid:1) deﬁne the edges coming from l2, and so on.
The right edges of g are deﬁned similarly by the right
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:51:25 UTC from IEEE Xplore.  Restrictions apply. 
edges of g (cid:1) and ιi.
At the end of this process, there is one potential problem
with g — there may be duplicate edges between two nodes,
which serve no useful purpose in coding or decoding. We
deal with this problem by deleting duplicate edges. An
alternative method is to swap edges between nodes until
no duplicate edges exist. We compared these two meth-
ods and found that neither outperformed the other, so we
selected the edge deletion method since it is more efﬁcient.
We evaluate each random graph by performing a Monte
Carlo simulation of over 1000 random downloads, and cal-
culating the average number of blocks required to success-
fully reconstruct the data. This is reported as the overhead
factor f above. In other words, if n = 100, m = 100,
and our simulation reports that f = 1.10, then on average,
110 random blocks of the 200 total blocks are required to
reconstruct the 100 original blocks of data from the graph
in question.
Theoretical work on LDPC codes typically calculates
100%.
the percentage of capacity of the code, which is 1
f
We believe that for storage applications, the overhead fac-
tor is a better metric, since it quantiﬁes how many block
downloads are needed on average to acquire a ﬁle.
5 Experiments
Code Generation: The theoretical work on LDPC codes
gives little insight into how the Λ and P vectors that they
design will perform for smaller values of n. Therefore we
have performed a rather wide exploration of LDPC code
generation. First, we have employed 80 different sets of Λ
and P from published papers on asymptotic codes. We
call the codes so generated published codes. These are
listed in Table 1, along with the codes and rates for which
they were designed. The WK03 distributions are for Gal-
lager codes on AWGN (Additive White Gaussian Noise)
channels, and the R03 distributions are for IRA codes on
AWGN and binary symmetric channels. In other words,
neither is designed for the BEC. We included the former as
a curiosity and discovered that they performed very well.
We included the latter because distributions for IRA codes
are scarce.
Second, we have written a program that generates ran-
dom Λ and P vectors, determines the ten best pairs that
minimize f , and then goes through a process of picking
random Λ’s for the ten best P ’s and picking random P ’s
for the ten best Λ’s. This process is repeated, and the ten
best Λ/P pairs are retained for subsequent iterations. Such
a methodology is suggested by Luby et al [LMS98]. We
call the codes generated from this technique Monte Carlo
codes.
Third, we observed that picking codes from some prob-
ability distributions resulted in codes with an extremely
wide range of overhead factors (see section 6.4 below).
Thus, our third mode of attack was to take the best per-
forming instances of the published and Monte Carlo codes,
and use their left and right node cardinalities to deﬁne
new Λ’s and P ’s. For example, the Systematic code in Fig-
ure 3(a) can be generated from any number of probability
distributions. However, it deﬁnes a probability distribution
where Λ = and P =. These
new Λ’s and P ’s may then be employed to generate new
codes. We call the codes so generated derived codes.
3
2 , 2
Tests: The range of potential tests to conduct is colos-
sal. As such, we limited it in the following way. We focus
}. In other words, m = 2n,
on three rates: R ∈ { 1
3 , 1
m = n, and m = n
2 . These are the rates most studied
in the literature. For each of these rates, we generated the
three types of codes from each of the 80 published distri-
butions for all even n between 2 and 150, and for n ∈ {250,
500, 1250, 2500, 5000, 12500, 25000, 50000, 125000}3. For
Systematic codes, we tested cascading levels from one to
six.
For Monte Carlo codes, we tested all three codes with
all three rates for even n ≤ 50. As shown in section 6.2 be-
low, this code generation method is only useful for small n.
Finally, for each value of n, we used distributions de-
rived the best current codes for all three coding methods
(and all six cascading levels of Systematic codes) to gen-
erate codes for the ten nearest values of n with the same
rate. The hope is that good codes for one value of n can be
employed to generate good codes for nearby values of n.
this makes for over 100,000 different data
points, each of which was repeated with over 100 differ-
ent random number seeds. The optimal code and overhead
factor for each data point was recorded and the data is di-
gested in the following section.
In sum,
6 Results
Our computational engine is composed of 160 machines
(Sun workstations running Solaris, Dell Pentium worksta-
tions running Linux, and a Macintosh PowerBook running
OSX) which ran tests continuously for over a month. We
organize our results by answering each of the questions
presented in Section 1 above.
6.1 Question 1
What kind of overhead factors can we expect for LDPC
codes for small and large values of n?
All of our data is summarized in Figure 5. For each
value of n and m, the coding and generation method that
3One exception is n = 125000 for R = 1
3 , due to the fact that these
graphs often exceeded the physical memory of our machines.
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:51:25 UTC from IEEE Xplore.  Restrictions apply. 
produces the smallest overhead factor is plotted.
6.2 Question 2
1.20
1.15
1.10
1.05
r
o
t
c
a
F
d
a
e
h
r
e
v
O
Rate = 1/3
Rate = 1/2
Rate  = 2/3
Are the three types of codes equivalent, or do they
perform differently?
They perform differently. Figure 6 shows the best per-
forming of the three different codes for R = 1
2 (the other
rates are similar [PT03], and are omitted for brevity). For
small values of n, Systematic codes perform the best.
However, when n roughly equals 100, the IRA codes start
to outperform the others, and the Gallager codes start to
outperform the Systematic codes. This trend continues to
the maximum values of n.
1.00
1
10
100
1000
10000
100000
n
Figure 5: The best codes for all generation methods for
1 ≤ n ≤ 125, 000, and R = 1
3 , 1
2 , 2
3 .
All three curves of Figure 5 follow the same pattern.
The overhead factor starts at 1 when m = 1 or n = 1,
and the Systematic codes become simple replication/parity
codes with perfect performance. Then the factor increases
with n until n reaches roughly twenty at which point it
levels out until n increases to roughly 100. At that point,
the factor starts to decrease as n increases, and it appears
that it indeed goes to one as n gets inﬁnitely large.
Although we only test three rates, it certainly appears
that the overhead factor grows as the rate approaches zero.
This is intuitive. At one end, any code with a rate of
one will have an overhead factor of one. At the other,
consider a one-level Systematic code with n = 3 and
m = ∞. There are only seven combinations of the left
nodes to which a right node may be connected. Therefore,
the right nodes will be partitioned into at most m
7 groups,
where each node in the group is equivalent. In other words,
any download sequence that contains more than one block
from a node group will result in overhead. Clearly, this
argues for a higher overhead factor.
Challenge for the Community: The shape of the
curves in Figure 5 suggests that there is a lower bound for
overhead factor as a function of n and m (or alternatively
as a function of n and R). It is a challenge to the theo-
retical community to quantify this lower bound for ﬁnite
values of n and m, and then to specify exact methods for
generating optimal or near optimal codes.
Distributions: So that others may take advantage of
our simulations, we publish the Λ and P values used to
generate the codes depicted in Figure 5 at http://www.
cs.utk.edu/˜plank/ldpc.
1.15
1.10
1.05
r
o
t
c
a
F
d
a
e
h
r
e
v
O
1.00
1
Systematic
Gallager
IRA
10
100
1000
10000
100000
n
Figure 6: Comparing methods, R = 1
2
Unfortunately, since the theoretical work on LDPC
codes describes only asymptotic properties, little insight
can be given as to why this pattern occurs. One curi-
ous point is the relationship between one-level Systematic
codes and Gallager codes.
It is a trivial matter to con-
vert a one-level Systematic code into an equivalent Gal-
lager code by adding m left nodes, ln+1, . . . , ln+m to the
Systematic graph, and m edges of the form (ln+i, ri) for
1 ≤ i ≤ m. This fact would seem to imply that overhead
factors for one-level Systematic codes would be similar to,
or worse than Gallager codes. However, when n < 50,
the one-level Systematic codes vastly outperform the oth-
ers; the Gallager codes perform the worst. To explore this
point, we performed this conversion on a Systematic graph
where n = m = 20, and the overhead factor is 1.16. The
node cardinalities of the equivalent Gallager graph were
then used to generate values of Λ and P , which in turn
were used to generate 500 new Gallager graphs with the
exact same node cardinalities. The minimum overhead fac-
tor of these graphs was 1.31 (the average was 1.45, and
the maximum was 1.58). This suggests that for smaller
graphs, perhaps Λ and P need to be augmented with some
other metric so that optimal codes can be generated easily.
Challenge to the community: A rigorous compari-
son of the practical utility of the three coding methods
needs to be performed. In particular, a computationally