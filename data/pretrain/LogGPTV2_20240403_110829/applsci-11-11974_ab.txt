18 signature = signature ∗ 100 + len( f irst_non_digital_token); 19 return signatureSpecifically, we put all punctuations used to calculate the signature in a table called the punctuation table. For each character in the split token list, we count it in a frequency table if it appears in the punctuation table. For the convenience of the subsequent calculation, we convert the frequency table into a number like a radix conversion. For each punctuation and its frequency, we multiply by the radix and add the current frequency. For example, in Figure 2, there is zero vertical bar (“|”), one colon (“:”), two semicolons (“,”), and three equal signs (“=”) in the log message log1. So, the signature calculated is 00010203 if we take ten as the radix. In the implementation, we take 100 as the radix, assuming that the frequency of any punctuation will not exceed 100. We can quickly locate the signature group according to the number in the subsequent step by converting. This number is returned as the signature. Algorithm 1 shows the detail of how to calculate the log signature.In practice, not all punctuations can be used to calculate the signature. For example, a logging statement may generate variables like “+1” and “−1” simultaneously. In this
| Appl. Sci. 2021, 11, 11974 | 6 of 15 |
|---|---|
|  |punctuations appearing in variables, we get a stable punctuation table, and it performs case, “+” and “−” should not be used to calculate the signature. By eliminating the |well on 16 datasets evaluated. The punctuations we selected are vertical bar (“|”), double quotation (“””), parenthesis (“(”), asterisk (“*”), semicolon (“;”), comma (“,”), equal sign (“=”), colon (“:”), and space (“ ”).Ideally, a log signature should correspond to only one event type. In this case, the log signature can be used to identify the event type uniquely. However, log messages from different event types may get the same log signature, called signature collisions. For example, log message log1 and log2 get the same signature (00010203), but they are from the different templates in Figure 2. The average number of templates corresponding to the same signature is called the collision index. By calculating the collision index, we can measure the severity of the collision. If we only use the punctuation table to calculate the signature, the collision index on evaluated datasets is 1.76. To optimize the collision index, inspired by previous work [17], we use the length information of the first non-digital token (tokens without digital characters). Finally, we get an average collision index of 1.25 on all the 2000 log messages subsets (cf. Section 4.1) from the 16 evaluated datasets.3.3. Step 3 Search Signature Group
In this step, we search the signature group to find the most suitable template. The signature group maintains templates with the same signature in a list. The collision index is also the average number of templates in each signature group. Each signature corresponds to a signature group, and the corresponding relationship is recorded in a hash table. By looking at the hash table, we can quickly find the signature group. Due to the small number of templates in each signature group, we find the most suitable template by calculating the log similarity one by one.In practice, we traverse the template list in the signature group to find the template with the largest similarity compared with the current log message. If the similarity is greater than a given similarity threshold, the template index will be returned. Like prior work, we consider that log messages with the same event type have the same length (cf. Section 2). So, if the length is not the same, the similarity value is zero. The similarity between the log message and the log template is defined as the number of identical tokens divided by the total number.In addition, we found that some log templates are very similar because they have the same prefix tokens, and only the last few tokens are different. For example, BGL E99 “pro-gram interrupt: fp cr field .............” and E100 “program interrupt: fp cr update.............”. We apply a soft prefix tokens matching method before calculating the log similarity to deal with this case. To be specific, we compare the first N non-punctuation tokens between the template and the message. If tokens in the same position do not contain digital characters and are not the same, we think the template does not match. The user specifies the vari-able N as a hyperparameter called prefix threshold. The complete process of log similarity calculation is shown in Algorithm 2.As each signature group contains an independent subset of the whole messages, existing online log parsing methods can also be applied to search templates inside each signature group. Take Drain for example, the core data structure of Drain is a fixed-depth tree. If we want to utilize Drain to search signature groups, we build such a tree structure and adopt all Drain steps inside each signature group. In our experiment, we introduce our log signature method to Spell and Drain in this way.Appl. Sci. 2021, 11, 11974 7 of 15
Algorithm 2: Log similarity
Input: The split template_token_list; the split content_token_list; prefix threshold 	pt.
Output: A number ranges [0, 1] representing the log similarity.
1 m, n = len(template_token_list), len(content_token_list);
| 	// the length of token list should be the same 2 if m != n then | 	// the length of token list should be the same 2 if m != n then ||---|---|
| 3 |return 0 |
| 4 end  5 count, total = 0, 0;  6 for t1, t2 in zip(template_token_list, content_token_list) do |4 end  5 count, total = 0, 0;  6 for t1, t2 in zip(template_token_list, content_token_list) do |
| 7 |if t1 in PUNCTUATION_TABLE or t2 in PUNCTUATION_TABLE then |
| // punctuations should be the same |// punctuations should be the same |
| 8 |if t1 != t2 then |
| 9 |return 0 || 8 |if t1 != t2 then |
| 9 |return 0 |
| 10 |end |
| 11 |else |
| 12 |if t1 == t2 then |
| 13 |count+ = 1; |
| 14 |end |
| // soft prefix tokens matching |// soft prefix tokens matching |
| 15 |if not (t1 contains digital character or t2 contains digital character) then |
| 16 |if pt > 0 then |
| 17 |return 0 |
| 18 |end |
| 19 |end |
| 20 |total+ = 1; |
| 21  22 |end pt− = 1; |
23 end| 21  22 |end pt− = 1; |
23 end 
24 return count/total
3.4. Step 4 Update Signature Group
In the previous step, the most suitable template index is returned. If the index is valid, we will update the template by replacing different tokens in the same position with
will append this log message to the template list as a new template and return its index. a special token “”. If the index is −1, it means that no suitable template is found. WeFinally, the log event id is calculated as a (log signature, template index) tuple.
4. Evaluation
	In this section, we evaluate LogPunk on 16 benchmark datasets from the LogHub [28] from three aspects: accuracy, robustness, and efficiency.
	Accuracy. Accurate log parsers can correctly identify the static template and dynamic variables in the log content;Robustness. Robust log parsers should perform consistently across different datasets, so they can be applied to more environments; 
	Efficiency. As log parsing is the first step of log analysis, inefficient log parsing cannot meet the real-time requirements.We compare LogPunk with five previous state-of-the-art log parsers, including Drain [17], Spell [26], AEL [29], LenMa [25], and IPLoM [23]. All of them have been included in the LogPai benchmark [15]. As mentioned above, we also introduce our log signature method to Spell and Drain (denote as Spell+ and Drain+, respectively), which means that we utilize Spell and Drain to search signature groups. All experiments were conducted on a Linux machine with an 8-core Intel(R) Core(TM) i7-7700HQ CPU @ 2.80 GHz, 16 GB RAM, running 64-bit Ubuntu 18.04.5 LTS.Appl. Sci. 2021, 11, 11974 8 of 15
4.1. LogHub Dataset and Accuracy Metrics
Our benchmark datasets come from the LogHub data repository [28]. LogHub main-tains a collection of logs from 16 different systems spanning distributed systems, supercom-puters, operating systems, mobile systems, server applications, and stand-alone software.
Many prior log parsing research [15–18,26] evaluated their approaches on these logs.As illustrated in Table 1, LogHub contains 440 million log messages which amount to 77 GB. We can see that there are great differences in the number of templates in different datasets. The Android dataset has a maximum of 76,923 templates, while the Proxifier has a minimum of 9 templates. The average length of log messages is basically in the tens or twenties, but the maximum log message length of some datasets can reach hundreds. In terms of the log content, there is little difference among datasets, which is mostly readable free text.Benefiting from the large size and diversity of LogHub datasets, we can measure the accuracy of log parsers and test their robustness and efficiency. LogHub picked up a subset of 2000 log messages from each dataset and manually labeled the event templates as the ground truth to ensure a consistent benchmark environment. In Table 1, “#Templates(2k)”indicates the number of event templates in the 2000 log subsets. Such manually labeled data are used to evaluate the accuracy and robustness of our log parser.Table 1. Datasets overview.
| Platform | Description | #Templates(2k) #Templates (Total) Length (Max, Average) | #Templates(2k) #Templates (Total) Length (Max, Average) | #Templates(2k) #Templates (Total) Length (Max, Average) | Size |
|---|---|---|---|---|---|
| Android |Android framework |166 |76,923 |32, 13.31 |183.37 MB |
| Apache |Apache web server error |6 |44 |14, 12.28 |4.90 MB || BGL |Blue Gene/L supercomputer |120 |619 |84, 15.32 |708.76 MB |
| Hadoop |Hadoop map reduce job |114 |298 |50, 14.82 |48.61 MB |
| HDFS |Hadoop distributed file system |14 |30 |111, 12.45 |1.47 GB |
| HealthApp |Health app |75 |220 |14, 4.93 |22.44 MB |
| HPC |High performance cluster |46 |104 |47, 9.56 |32.00 MB |
| Linux |Linux system |118 |488 |24, 14.39 |2.25 MB |
| Mac |Mac OS |341 |2214 |249, 15.49 |16.09 MB || OpenSSH |OpenSSH server |27 |62 |19, 13.81 |70.02 MB |
| OpenStack |OpenStack infrastructure |43 |51 |31, 20.63 |58.61 MB |
| Proxifier |Proxifier software |8 |9 |27, 13.73 |2.42 MB |
| Spark |Spark job |36 |456 |22, 12.76 |2.75 GB |
| Thunderbird |Thunderbird supercomputer |149 |4040 |132, 17.52 |29.60 GB |
| Windows |Windows event |50 |4833 |42, 31.93 |26.09 GB || Zookeeper |ZooKeeper service |50 |95 |26, 13.46 |9.95 MB |
Same as the prior work by Zhu et al. [15], we adopt the parsing accuracy (PA) metric to measure the effectiveness of our log parser. PA is defined as the ratio of correctly parsed log messages over the total number of log messages. After parsing, each log message will be assigned with an event id suggesting which event type it belongs to. Regarding an event type, we consider it as correct if and only if all its log messages in the ground truth are parsed with the same event id. PA is stricter than the standard evaluation metrics, such as precision, recall, and F1-measure.4.2. Accuracy