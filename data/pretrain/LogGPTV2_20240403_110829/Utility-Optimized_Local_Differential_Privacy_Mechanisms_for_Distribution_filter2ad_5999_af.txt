of sensitive regions/categories and the TV. To this end, we
randomly chose XS from X , and increased |XS| from 1 to |X|
(only in this experiment). We attempted one hundred cases
for randomly choosing XS from X , and evaluated the TV by
computing the sample mean over one hundred cases.
Figure 6 shows the results for ε = 0.1 (high privacy regime)
or ln|X| (low privacy regime). Here we omit the performance
of “emp+thr”, since it is very close to that of “EM” in the
same way as in Figure 5. The uRAP and uRR provide the
best performance when ε = 0.1 and ln|X|, respectively. In
addition, the uRR provides the performance close to the non-
private mechanism when ε = ln|X| and the number |XS| of
sensitive regions/categories is less than 100. The performance
of the uRAP is also close to that of the non-private mechanism
when |XS| is less than 20 (note that |X| 3
4 = 125 and 89 in the
Foursquare and US Census datasets, respectively). However, it
rapidly increases with increase in |XS|. Overall, our theoretical
results in Section 4.3 hold for the two real datasets.
We also evaluated the performance when the number of
attributes was increased from 4 to 9 in the US Census dataset.
We added, one by one, ﬁve attributes as to whether or not a
user has served in the military during ﬁve periods (“Sept80”,
“May75880”, “Vietnam”, “Feb55”, and “Korean” in [18]; we
added them in this order). We assumed that these attributes
USENIX Association
28th USENIX Security Symposium    1887
10310210110010-110-2TV0.1110epsilonRRRAPuRRuRAPno privacy10010-110-20.1110epsilon10010-110-20.1110epsilon(a) Foursquare (left: emp, middle: emp+thr, right: EM)10210110010-110-210-3TV0.1110epsilon10010-110-20.1110epsilon0.1110epsilon10-310010-110-210-3(b) US Census (left: emp, middle: emp+thr, right: EM)Figure 6: |XS| vs. TV when ε = 0.1 or ln|X|.
Figure 7: Number of attributes vs. TV (US Census dataset;
left: ε = 0.1, middle: ε = 1.0, right: ε = 6.0).
are non-sensitive. Since each of the ﬁve attributes had two
categories (1: yes, 0: no), |X| (resp. |XS|) was changed from
400 to 12800 (resp. from 76 to 2432). We randomly chose
n = 240000 people as users who provide obfuscated data, and
evaluated the TV by computing the sample mean over ten
realizations of Y (only in this experiment).
Figure 7 shows the results in the case where ε = 0.1, 1.0,
or 6.0 (=ln400). Here we omit the performance of “emp+thr”
in the same way as Figure 6. Although the TV increases with
an increase in the number of attributes, overall our utility-
optimized mechanisms remain effective, compared to the ex-
isting mechanisms. One exception is the case where ε = 0.1
and the number of attributes is 9; the TV of the RR (EM),
RAPPOR (EM), and uRR (EM) is almost 1. Note that when
we use the EM reconstruction method, the worst value of
the TV is 1. Thus, as with the RR and RAPPOR, the uRR
fails to estimate a distribution in this case. On the other hand,
the TV of the uRAP (EM) is much smaller than 1 even in
this case, which is consistent with the fact that the uRAP is
order optimal in the high privacy regime. Overall, the uRAP
is robust to the increase of the attributes at the same value of
ε (note that for large |X|, ε = 1.0 or 6.0 is a medium privacy
regime where 0 (cid:28) ε (cid:28) ln|X|).
We also measured the running time (i.e., time to estimate p
from Y) of “EM” (which sets the estimate by “emp+thr” as
Figure 8: ε vs. TV (personalized-mechanism) ((I): w/o knowl-
edge, (II): POI distribution, (III): true distribution).
Figure 9: Visualization of the distributions ((II): POI distribu-
tion, (III): true distribution).
an initial value of ˆp) on an Intel Xeon CPU E5-2620 v3 (2.40
GHz, 6 cores, 12 logical processors) with 32 GB RAM. We
found that the running time increases roughly linearly with the
number of attributes. For example, when ε = 6.0 and the num-
ber of attributes is 9, the running time of “EM” required 3121,
1258, 5225, and 1073 seconds for “RR”, “uRR”, “RAP”, and
“uRAP”, respectively. We also measured the running time of
‘emp” and “emp+thr”, and found that they required less than
one second even when the number of attributes is 9. Thus, if
“EM” requires too much time for a large number of attributes,
“emp+thr” would be a good alternative to “EM”.
Personalized-mechanism scenario. We then focused on the
personalized-mechanism scenario, and evaluated our utility-
optimized mechanisms using the Foursquare dataset. We used
the PUM with κ = 2 semantic tags (described in Section 5.1),
which maps “home” and ‘workplace” to bots ⊥1 and ⊥2,
respectively. As the background knowledge about the bot
distribution πk (1 ≤ k ≤ 2), we considered three cases: (I)
we do not have any background knowledge; (II) we use a
distribution of POIs tagged as “home” (resp. “workplace”),
which is computed from the POI data in [54], as an estimate
of the bot probability ˆπ1 (resp. ˆπ2); (III) we use the true
distributions (i.e., ˆπk = πk). Regarding (II), we emphasize
again that it is not a user distribution but a “POI distribution”,
and can be easily obtained via the Foursquare venue API [54].
Figure 8 shows the results. We also show the POI and true
distributions in Figure 9. It can be seen that the performance
of (II) lies in between that of (I) and (III), which shows that
the estimate ˆπk of the bot distribution affects utility. However,
when ε is smaller than 1, all of (I), (II), and (III) provide almost
the same performance, since the effect of the estimation error
of ˆπk does not depend on ε, as described in Section 5.4.
We also computed the l1 loss l1(ˆp,p) and the ﬁrst and
second terms in the right-hand side of (22) to investigate
1888    28th USENIX Security Symposium
USENIX Association
RR (emp)RAP (emp)RR (EM)RAP (EM)uRR (emp)uRAP (emp)uRR (EM)uRAP (EM)no privacy(a) Foursquare (left: ࢿ= 0.1, right: ࢿ= ln|ढe)TV10110010-110-2400350300250200150100908070605040302010110-3|ࣲௌ|4003503002502001501009080706050403020101|ࣲௌ|TV10-210-310210-1(b) US Census (left: ࢿ= 0.1, right: ࢿ= ln|ढe)103102TV10110010-110-26256005004003002001009080706050403020101|ࣲௌ|10010-16256005004003002001009080706050403020101|ࣲௌ|10-2TV105TV10310110410310210110010-110-210-3#Attributes45678910210110010-110-210-3#Attributes456789#Attributes45678910010-110-210-3RR (emp)RAP (emp)uRR (emp)uRAP (emp)RR (EM)RAP (EM)uRR (EM)uRAP (EM)no privacy10010-110-2TV0.1110epsilonuRR (I)uRR (II)uRR (III)uRAP (I)uRAP (II)uRAP (III)no privacy(a) emp10010-110-20.1110epsilon10010-110-20.1110epsilon(b) emp+thr(c) EM+RPH,,+RPH,,,:RUNSODFH,,:RUNSODFH,,,Table 1: l1 loss l1(ˆp,p) and the ﬁrst and second terms in the
right-hand side of (22) in the case where ε = ln|X| and the
EM reconstruction method is used.
second term
Method
7.34× 10−2
uRR (I)
2.96× 10−2
uRR (II)
uRR (III)
0
7.35× 10−2
uRAP (I)
2.96× 10−2
uRAP (II)
uRAP (III)
0
l1(ˆp,p)
6.73× 10−2
4.24× 10−2
2.62× 10−2
6.77× 10−2
4.28× 10−2
2.67× 10−2
ﬁrst term
2.70× 10−2
2.70× 10−2
2.70× 10−2
2.76× 10−2
2.76× 10−2
2.76× 10−2
whether Theorem 1 holds. Table 1 shows the results (we
averaged the values over one hundred realizations of Y). It
can be seen that l1(ˆp,p) is smaller than the summation of the
ﬁrst and second terms in all of the methods, which shows that
Theorem 1 holds in our experiments.
From these experimental results, we conclude that our
proposed methods are very effective in both the common-
mechanism and personalized-mechanism scenarios. In Ap-
pendix C.2, we show the MSE has similar results to the TV.
7 Discussions
√
t|XS|√
nε2 ) (resp. Θ( t|XS|
On the case of multiple data per user. We have so far
assumed that each user sends only a single datum. Now
we discuss the case where each user sends multiple data
based on the compositionality of ULDP described in Sec-
tion 3.2. Speciﬁcally, when a user sends t (> 1) data, we
obtain (XS, (YP)t ,ε)-ULDP in total by obfuscating each data
using the (XS,YP,ε/t)-utility-optimized mechanism. Note,
however, that the amount of noise added to each data increases
with increase in t. Consequently, for ε ∈ [0,t], the lower bound
on the l1 (resp. l2) loss (described in Section 3.2) can be ex-
pressed as Θ(
nε2 )), which increases with
increase in t. Thus, t cannot be large for distribution estima-
tion in practice. This is also common to all LDP mechanisms.
S . Assume that the i-
th user obfuscates t data using different seeds, and sends
tP protected data in YP and tI invertible data in YI, where
t = tP +tI > 1 (she can also use the same seed for the same
data to reduce tI as in [23]). If all the tI data in YI are different
from each other, the data collector learns tI original data in
XN. However, tI (≤ t) cannot be large in practice, as explained
above. In addition, in many applications, a user’s personal
data is highly non-uniform and sparse. In locations data, for
example, a user often visits only a small number of regions
in the whole map X . Let T (i) ⊆ XN be a set of possible input
values for the i-th user in XN. Then, even if tI is large, the data
collector cannot learn more than |T (i)| data in XN.
Next we discuss the secrecy of X (i)
Moreover, the tP data in YP reveal almost no information
S , since Q(i) provides (XS, (YP)t ,ε)-ULDP. Qcmn
about X (i)
S , since f (i)
provides no information about X (i)
pre is kept secret.
Thus, the data collector, who knows that the user wants to
hide her home, cannot reduce the number of candidates for
her home from max{|X|− tI,|X|−|T (i)|} using the tP data
and Qcmn. If either tI or |T (i)| is much smaller than |X|, her
home is kept strongly secret.
S changes over time.
Note that p can be estimated even if X (i)
is also kept strongly secret if tI or |T (i)| is small.
X (i)
S
On the correlation between XS and XN. It should also be
noted that there might be a correlation between sensitive data
XS and non-sensitive data XN. For example, if a user discloses
a non-sensitive region close to a sensitive region including
her home, the adversary might infer approximate information
about the original location (e.g., the fact that the user lives
in Paris). However, we emphasize that if the size of each
region is large, the adversary cannot infer the exact location
such as the exact home address. Similar approaches can be
seen in a state-of-the-art location privacy measure called geo-
indistinguishability [4, 7, 42, 47]. Andrés et al. [4] considered
privacy protection within a radius of 200m from the original
location, whereas the size of each region in our experiments
was about 400m × 450m (as described in Section 6.1). We
can protect the exact location by setting the size of each
region to be large enough, or setting all regions close to a
user’s sensitive location to be sensitive.
There might also be a correlation between two attributes
(e.g., income and marital status) in the US Census dataset.
However, we combined the four category IDs into a total
category ID for each user as described in Section 6.1. Thus,
there is only “one” category ID for each user. Assuming
that each user’s data is independent, there is no correlation
between data. Therefore, we conclude that the sensitive data
are strongly protected in both the Foursquare and US Census
datasets in our experiments.
It should be noted, however, that the number of total cate-
gory IDs increases exponentially with the number of attributes.
Thus, when there are many attributes as in Figure 7, the es-
timation accuracy might be increased by obfuscating each
attribute independently (rather than obfuscating a total ID)
while considering the correlation among attributes. We also
need to consider a correlation among “users” for some types
of personal data (e.g., ﬂu status). For rigorously protecting
such correlated data, we should incorporate Pufferﬁsh pri-
vacy [32, 48] into ULDP, as described in Section 1.
8 Conclusion
In this paper, we introduced the notion of ULDP that guar-
antees privacy equivalent to LDP for only sensitive data. We
proposed ULDP mechanisms in both the common and person-
alized mechanism scenarios. We evaluated the utility of our
mechanisms theoretically and demonstrated the effectiveness
of our mechanisms through experiments.
USENIX Association
28th USENIX Security Symposium    1889
References
[1] D. Agrawal and C. C. Aggarwal. On the design and
quantiﬁcation of privacy preserving data mining algo-
rithms. In Proc. PODS, pages 247–255, 2001.
[2] R. Agrawal, R. Srikant, and D. Thomas. Privacy preserv-
ing OLAP. In Proc. SIGMOD, pages 251–262, 2005.
[3] M. Alaggan, S. Gambs, and A.-M. Kermarrec. Hetero-
geneous differential privacy. Journal of Privacy and
Conﬁdentiality, 7(2):127–158, 2017.
[4] M. E. Andrés, N. E. Bordenabe, K. Chatzikokolakis, and
C. Palamidessi. Geo-indistinguishability: Differential
privacy for location-based systems. In Proc. CCS, pages
901–914, 2013.
[5] B. Avent, A. Korolova, D. Zeber, T. Hovden, and
B. Livshits. BLENDER: Enabling local search with
a hybrid differential privacy model. In Proc. USENIX
Security, pages 747–764, 2017.
[6] R. Bassily and A. Smith. Local, private, efﬁcient pro-
tocols for succinct histograms. In Proc. STOC, pages
127–135, 2015.
[7] N.
E.