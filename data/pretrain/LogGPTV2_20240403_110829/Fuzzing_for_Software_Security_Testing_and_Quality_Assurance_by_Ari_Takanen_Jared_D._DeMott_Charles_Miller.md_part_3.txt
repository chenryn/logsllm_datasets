### Another Instance of an Identified Issue

All data, including crash information, is stored in an SQL database, allowing for easy retrieval of crash details even years after the event. This aspect of our fuzzing system has significantly evolved since its inception. If you are implementing fuzzing at scale, a similar approach is highly recommended.

### Good Engineering Practices Are Essential

While it is relatively straightforward to write a quick-and-dirty fuzzer, this can be misleading. If the fuzzer finds bugs, you may feel successful, and if it doesn't, you might assume your target is robust. However, this can be very deceptive. Today, we treat our fuzzing efforts with the same rigor as we do when developing customer-facing code. We conduct design and code reviews before check-ins, test our fuzzers and infrastructure for correctness, and have monitoring systems that alert us if the fuzzing system is not functioning as expected. If the fuzzing system goes down, it is treated as a high-priority service blocker, where other work is paused to address the issue. This level of discipline was not always present, especially in the early days. We experienced several instances where the system was unreliable, fuzzers did not perform modifications as intended, and results were not consistently reproducible.

### Measuring Coverage

To ensure our fuzzing system was performing as expected, we took two main steps. First, we collaborated with software engineers from various product teams (e.g., Word, Excel, PowerPoint) to enhance the comprehensiveness of our template data. For example, many teams had sample files that could be parsed but not created in recent versions of the product. These files were added to our template repository. We also developed a toolset to scan through the repository and identify missing elements. Second, we analyzed the parser's source code to pinpoint the targeted code locations. This ensured that fuzzers targeting specific formats were indeed reaching the intended parsing code. When a new fuzzer is added, the targeted code locations and the required percentage of fuzz iterations that must hit that code are specified. If the run completes with a lower percentage than specified, an alert is triggered, and no additional jobs with that fuzzer are processed. This process has helped identify misbehaving fuzzers, infrastructure issues, and product changes that affected the parser's invocation.

### Determining Exploitability May Not Be the Best Use of Time

I manage a team of security engineers who are skilled in investigating fuzzing crashes, determining exploitability, and creating proof-of-concept exploits. However, this process is time-consuming and does not scale well. Instead of training all non-security software engineers in exploitation techniques, we found it more efficient to investigate and fix the crashing code directly. This approach often leads to fixing bugs that are not necessarily security issues, which still improves overall product reliability. For instance, out of the 1,800 fuzzing bugs fixed in Office 2010, only a subset were security-related.

### Informing Non-Fuzzing Efforts Through Fuzzing

Fuzzing has helped us identify patterns and areas of the code that are less robust than expected. With this knowledge, we can conduct more thorough source code reviews and manual penetration testing. For example, instead of just fixing the identified fuzzing crash, we look across our codebase for similar patterns and fix those as well. Sometimes, these additional areas are attackable but require a more sophisticated fuzzer. Multiple fuzzing crashes in related code also help us prioritize areas for further code review and potential penetration testing.

### Continuous Innovation

One of my favorite aspects of security is its rapid evolution. I am constantly learning new things on my own and from others. After using the same fuzzing algorithm for a while, it becomes less effective. It is tempting to stop fuzzing once the majority of bugs have been found. However, we took a different approach. Given that we were using existing hardware for functional automation and our fuzzing process was fully automated, there was minimal cost to continue fuzzing. Since many of our fuzzers were non-deterministic and parsing code sometimes changed, we continued to find some bugs. We continuously update our fuzzers and infrastructure to discover more bugs, staying mindful of third parties eager to find vulnerabilities in our code. Additionally, we stay informed about community developments and partner closely with Microsoft Research, which has created advanced fuzzers like SAGE.

**Tom Gallagher**  
Principal Group Engineering Manager, Microsoft

---

### Preface from the First Edition

Even today, most software fails under negative testing, or fuzzing, as it is known in the security community. I (Ari) have never seen a piece of software or network device that passes all fuzz tests. However, there has been significant improvement since 1996, when we started developing our first fuzzers, and even more so since the 1970s, when Dr. Boris Beizer and his team built their fuzzer-like test automation scripts. The key driver for this change is the adoption of these tools and techniques, along with the availability of technical details on how to conduct such testing. There has been tremendous development in the fuzzer market, with a wide range of open-source and commercial tools available.

The idea for this book came up in 2001, around the time we completed the PROTOS Classic project on grammar-based fuzzers. Unfortunately, other projects distracted us. As a result of the PROTOS project, we spawned several security spin-offs. One of them was Codenomicon, which took over the technical development from PROTOS Classic and launched the first commercial fuzzers in early 2002. Another was the PROTOS Genome project, which explored the next steps in fuzzing and automated protocol reverse-engineering. The third was FRONTIER, which later became Clarified Networks, focusing on next-gen network analysis tools. Despite these distractions, we continued our focus on fuzzer research and teaching secure programming at the University of Oulu.

The idea for the book reemerged in 2005 when I reviewed a paper by Jared DeMott for the Blackhat conference. I saw something new and unique in that paper and immediately proposed that Jared co-author this fuzzer book project with me. We had complementary experiences and thoughts on fuzzing, making it a good fit. We wanted the book to be product and technology independent, drawing on our combined experiences. To add an independent perspective, we reached out to Charlie Miller, who ended up writing almost one-third of the book.

Our goal was to create a resource that would be useful as a course book at universities and as a reference for quality assurance engineers and security specialists. Fuzzing is not just about security; it is also about software quality. It is a convergence of security practices into quality assurance practices, or vice versa. In all 100+ global customers of Codenomicon fuzzing tools, the same logic applies: Fuzzing is a team effort between security and quality assurance.

There are many things left out of this edition, but we hope that will motivate you to buy enough books for the publisher to give us an opportunity to improve. Fuzzing is a rapidly evolving field, and we promise to track new developments and update both the book and our website (www.fuzz-test.com). Please contact us with your comments, whether positive or negative, to help make this a valuable resource for advancing software reliability and dependability.

**Ari, Jared, and Charlie**

---

### Preface to the Second Edition

The first edition of this book served as one of the first stepping stones when I entered the world of fuzzing in 2011. While it may have lacked the newest tools and techniques, the body of knowledge was solid. People still make mistakes, and software is still broken. Even with modern quality assurance techniques, fuzzing remains a powerful tool for revealing bugs that would otherwise be missed.

Like Charlie for the first edition, I was called in as the last reserve. I jumped aboard without fully realizing how much fuzzing had changed since the first edition. Setting aside minor changes, significant advancements in evolutionary fuzzing tools like American Fuzzy Lop (AFL) and the emergence of full fuzz test automation systems like Googleâ€™s ClusterFuzz justify an update to this book.

The goal of the first edition was to be an educational book that would not be outdated in a decade. While that didn't quite work out, our goal for this edition is to refresh the content with major changes that have happened in the context of fuzzing over the past decade. Some tools and techniques referenced in the first edition are no longer relevant, but they still serve an educational purpose by showing how fuzzing and related technologies have evolved. With these updates, we are confident that this book will serve as "the fuzzing book" for another decade or so.

**Atte**

---

### Chapter 1: Introduction

Welcome to the world of fuzzing!

In essence, fuzzing involves sending anomalous data to a system to cause it to crash, thereby revealing reliability problems. Fuzzing is widely used by both security and quality assurance (QA) experts, although some misconceptions remain about its capabilities, effectiveness, and practical implementation. Fuzzing can be defined as:

"A highly automated testing technique that covers numerous boundary cases using invalid data (from files, network protocols, API calls, and other targets) as application input to better ensure the absence of exploitable vulnerabilities. The name comes from modem applications' tendency to fail due to random input caused by line noise on fuzzy telephone lines."

Before diving deeper into fuzzing, it's important to understand why you are interested in it. If you are reading this, you likely want to find bugs in software, particularly those with security or safety implications. Generally, there are three main reasons for looking for these types of defects:

1. **Quality Assurance (QA):** Testing and securing internally developed software.
2. **System Administration (SA):** Testing and securing software in your usage environment.
3. **Vulnerability Assessment (VA):** Testing and trying to break into someone else's software or system.

This book will explore fuzzing from all these perspectives, viewing it from the developer's, enterprise end user's, and third-party assessment team's viewpoints. Our goal is to level the playing field between software companies (testers) and vulnerability analysts (hackers), allowing each to learn from the other.

Fuzzing is the most powerful test automation tool for discovering security-critical problems in software. While code auditing tools can find more flaws, many of their findings are false positives. Fuzzing, on the other hand, has no false positives. A crash is a crash, and a bug is a bug. Almost every bug found with fuzzing is exploitable at some level, at minimum resulting in a denial of service. Fuzzing is especially useful for analyzing closed-source, off-the-shelf software and proprietary systems because it does not require access to source code.

In this chapter, we will provide an overview of fuzzing and related technologies, exploring why security mistakes happen and why current security measures fail to protect against them. We will discuss how fuzzing can help by introducing proactive tools that anyone can use to find and eliminate security holes. We will also look at where fuzzing is currently used and why, and get a bit more technical by reviewing the history of fuzzing and understanding how various techniques came into existence. This chapter aims to prepare you for the more detailed discussions in subsequent chapters.

#### 1.1 Software Security

Fuzzing is an excellent technique for finding security-critical flaws in any software quickly and cost-effectively. Unfortunately, it is not always used where it should be, leading to many systems being immature from a security perspective. It is a fact in the security field that software will always have security problems, and almost all software can be hacked easily. However, by becoming familiar with software security and related techniques, you can make a difference in reducing the number of security mistakes that remain in the software.

Few people today truly understand what software security is, even if they are considered security experts. Like ancient maps warning of uncharted territories, the area of software security can seem too scary or challenging. Fortunately, the age of darkness is over, thanks to the first explorers who ventured into the mystic lands of hacking, trying to explain security to ordinary software developers. Initially, they were feared for their skills and later blamed for the dangerous findings they uncovered. Even today, they are thought to possess some secret arts that make them special. However, what they found was not overly complex.

Software security testing can be introduced at various stages, starting from research and development (R&D), then moving to the test-lab environment, and finally to operations. In R&D, fuzzing can be used during the early prototyping phase and the implementation phase, where the majority of programming takes place. Once the first operational prototype is ready, it can be fuzzed. Test automation is often integrated into Continuous Integration, automated build processes, and regression testing. Source code auditing tools are also actively used to eliminate the easiest weaknesses caused by programming mistakes.

R&D includes testing, especially before system integration. In test laboratories, after the system has been integrated, a dedicated testing team typically performs most of the remaining testing efforts. Depending on the software development process, some testing may be more closely integrated into the development. The tests conducted in the test lab environment can differ significantly from those in the R&D environment. Test labs are common in systems development, such as in telecommunications, finance, and industrial networks, and can be quite large. In a test lab, a system can be tested with any available tools, and the test results can be thoroughly analyzed.