as another instance of an issue already identified. Everything is stored in an
SQL database so it is easy to get information about crashes even years later.
This part of our fuzzing system has greatly evolved since the early days. If you
are doing fuzzing at scale, you will likely want to leverage something similar.
• Good engineering practices go a long way. It’s relatively easy write a quick-
and-dirty fuzzer. If it finds bugs, you may feel successful. If it doesn’t you
might feel your fuzzing target is robust. This can be very misleading. Today
we treat our fuzzing efforts similar to the way we treat developing customer
shipping code. We perform design reviews, code reviews prior to check-in,
we test our fuzzers and infrastructure for correctness, and we have moni-
tors that alert us when our fuzzing system isn’t running as expected. If our
fuzzing system is down, we treat it as a service blocker (high priority where
other work is paused to address the service issue). We didn’t always have this
rigor—especially in the beginning. We had several instances where the system
was unreliable, fuzzers wouldn’t perform modifications in the intended ways,
and results weren’t always reproducible.
• Measuring coverage. As an extension of testing our work to ensure our fuzz-
ing system was doing what was expected, we wanted to better understand
if our seed or template data was complete and if our fuzzers hit the targeted
code to the depth expected. We did two main things. First, we worked with
software engineers on each of the product teams (Word, Excel, PowerPoint,
etc.) to ensure our template data was more comprehensive. For example, many
teams had sample files that could be parsed but not created in recent versions
of the product. These files were added to our template repository. We wrote
a toolset to help us scan through our template repository and identify what
was still missing. Second, we also looked at the parser’s source to identify
the location of the code we were targeting. This allowed us to ensure fuzz-
ers targeting specific formats were in fact reaching that parsing code. For
example, when a fuzzer is added to our system, the locations of the code it
is targeting is included along with the percentage of fuzz iterations that must
hit that code. If the run completes with a percentage lower than specified,
an alert is triggered and no additional jobs with that fuzzer are processed.
(This allows other fuzzing jobs that are operating as expected to leverage the
available machines.) This process has identified misbehaving fuzzers, infra-
structure issues, as well as product changes that resulted in the parser not
getting invoked as expected.
• Determining exploitability may not be the best use of time. I manage a team
of security engineers who are very talented and can investigate fuzzing crashes,
determine exploitability, and create proof of concept exploits. However, this
takes skill and time; it doesn’t scale well. Instead of trying to train and keep
all nonsecurity software engineers up to date on exploitation techniques, we
found it was usually faster to investigate and fix the crashing code than it was
to completely understand exploitability. There are many places we fix bugs
that are not security issues as a result. This is often intentional; most of the
6760 Book.indb 21 12/22/17 10:50 AM
xxii Foreword to the Second Edition
time fixing a crash improves product reliability and that is important even
outside of security. Of the 1,800 fuzzing bugs fixed in Office 2010, only a
subset were security issues.
• Informing nonfuzzing efforts through fuzzing. Fuzzing has helped us identify
patterns and parts of the code that were not as robust as expected. With this
knowledge, we can do more thorough source code review and manual pen-
testing. For example, instead of just fixing the fuzzing crash identified, we look
across our codebase for similar patterns exist and fix those, too. Sometimes
these additional places are attackable but require a very sophisticated fuzzer.
Several fuzzing crashes in related code also helps us prioritize that area of
the code needs additional code review and may be a good candidate for our
pen-test team’s efforts.
• Continuous innovation. One of my favorite things about security is the rapid
evolution. I am constantly discovering new things on my own and learning
from others. After using the same fuzzing algorithm for a period of time, it
won’t yield the same results as it initially did. You already found the major-
ity of the bugs that algorithm will identify. If you are the creator of the soft-
ware, it is tempting to say fuzzing work is done and stop fuzzing. We took a
different approach. First, we didn’t stop fuzzing. We decided given we were
leveraging existing hardware used for functional automation and our fuzzing
process was completely automated, there wasn’t much cost to continue fuzzing.
Since many of our fuzzers weren’t deterministic and parsing code sometimes
changes, we continued to find some bugs. We continue to force ourselves to
change our fuzzers and infrastructure to find more bugs. We know third
parties are eager to find bugs in our code so we are mindful not to become
complacent. In addition to our direct work, we stay on top of developments
others in the community have made and apply those to our work. We also
have a close partnership with Microsoft Research, who has created highly
advanced fuzzers involving constraint solving like SAGE.
Tom Gallagher
Principal Group Engineering Manager, Microsoft
6760 Book.indb 22 12/22/17 10:50 AM
Preface from the First Edition
Still today, most software fails with negative testing, or fuzzing, as it is known by
security people. I (Ari) have never seen a piece of software or a network device that
passes all fuzz tests thrown at it. Still, things have hopefully improved a bit from
1996 when we started developing our first fuzzers, and at least from the 1970s when
Dr. Boris Beizer and his team built their fuzzer-like test automation scripts. The key
driver for the change is the adaptation of these tools and techniques, and availability
of the technical details on how this type of testing can be conducted. Fortunately
there has been enormous development in the fuzzer market, as can be seen from
the wide range of available open source and commercial tools for this test purpose.
The idea for this book came up in 2001, around the same time when we com-
pleted the PROTOS Classic project on our grammar-based fuzzers. Unfortunately
we were distracted by other projects. Back then, as a result of the PROTOS project,
we spawned a number of related security spin-offs. One of them was the commercial
company Codenomicon, which took over all technical development from PROTOS
Classic, and launched the first commercial fuzzers in early 2002 (those were for SIP,
TLS, and GTP protocols if you are interested). Another was the PROTOS Genome
project, which started looking at the next steps in fuzzing and automated proto-
col reverse-engineering, from a completely clean table (first publicly available tests
were for various compression formats, released in March 2008). And the third was
FRONTIER, which later spun-out a company doing next-gen network analysis tools
and was called Clarified Networks. At the same time we kept our focus on fuzzer
research and teaching on all areas of secure programming at the University of Oulu.
And all this was in a small town of about two hundred thousand people, so you
could say that one out of a thousand people were experts in fuzzing in this far-north
location. But, unfortunately, the book just did not fit into our plans at that time.
The idea for the book reemerged in 2005 when I reviewed a paper Jared DeMott
wrote for the Blackhat conference. For the first time since all the published and
some unpublished works at PROTOS, I saw something new and unique in that
paper. I immediately wrote to Jared to propose that he would coauthor this fuzzer
book project with me, and later also flew in to discuss with him to get to know
him better. We had completely opposite experiences and thoughts on fuzzing, and
therefore it felt like a good fit, and so finally this book was started. Fortunately I
had a dialog going on with Artech House for some time already, and we got to start
the project almost immediately.
We wanted everything in the book to be product independent, and also technol-
ogy independent. With our combined experiences, this seemed to be natural for the
book. But something was still missing. As a last desperate action in our constant
xxiii
6760 Book.indb 23 12/22/17 10:50 AM
xxiv Preface from the First Edition
struggle to get this book completed by end of 2007, we reached out to Charlie Miller.
The main reason for contacting him was that we wanted to have a completely inde-
pendent comparison of various fuzzer technologies, and did not want to write that
ourselves as both of us had strong opinions in various, conflicting, directions. I, for
instance, have always been a strong believer in syntax testing-based negative test-
ing (some call this model-based fuzzing), with no random component to the tests.
Jared, on the other hand, was working on evolutionary fuzzers. Charlie accepted
to write a chapter, but later actually got more deeply involved in the project and
ended up writing almost one third of the book (Charlie should definitely do more
traveling, as he claims he wrote all that in an airplane).
Our goal was to write something that would be used as a course book at uni-
versities, but also as a useful reference for both quality assurance engineers and
security specialists. And I think we succeeded quite well. The problem with other
available books was that they were targeted to either security people, or to quality
assurance, or on very rare occasions to the management level. But fuzzing is not
only about security, as fuzzers are used in many closed environments where there
are no security threats. It is also not only about software quality. Fuzzing is a con-
vergence of security practices into quality assurance practices, or sometimes the
other way around. In all 100+ global customers of Codenomicon fuzzing tools (in
late 2007), from all possible industry verticals, the same logic is always apparent
in deploying fuzzers: Fuzzing is a team effort between security people and quality
assurance people.
There are many things that were left out of this edition of the book, but hope-
fully that will motivate you to buy enough books so that the publisher will give
us an opportunity to improve. This book will never be complete. For example in
2007 and early 2008 there were a number of completely new techniques launched
around fuzzing. One example is the recent release of the PROTOS Genome. Also,
commercial companies constantly continue to develop their offerings, such as the
rumors of the Wurldtech “Achilles Inside” (whatever that will be), and the launch
of the “fifth generation” Codenomicon Defensics 3.0 fuzzing framework, both of
which were not covered in this book. Academics and security experts have also
released new frameworks and tools. One example that you definitely should check
out is the FuzzGuru, available through OWASP. I am also expecting to see some-
thing completely different from the number of academics working with fuzzing,
such as the techniques developed by the Madynes team in France.
We promise to track those projects now and in the future, and update not only
this book, but also our Web site dedicated to fuzzing-related topics (www.fuzz-test.
com.) For that, please contact us with your comments, whether they are positive or
negative, and together we will make this a resource that will take software develop-
ment a giant leap forward, into an era where software is reliable and dependable.
Ari, Jared, and Charlie
6760 Book.indb 24 12/22/17 10:50 AM
Preface to the Second Edition
The first edition of this book served as one of the first stepping stones when I
entered the fascinating world of fuzzing in 2011. Maybe it was lacking the newest
tools and techniques, but the body of knowledge was, and still is, solid. People still
make mistakes, software is still broken, and even with the modern quality assurance
techniques, fuzzing is still a powerful addition that often reveals bugs that would
have otherwise been missed.
Like Charlie for the first edition, I was called in as the last reserve. I jumped
aboard without actually ever thinking about how much in fuzzing has changed
since the first edition was written; even more intriguing was to find out how much
has actually stayed the same. Setting aside small things here and there, leaps in
evolutionary fuzzing tools, like American Fuzzy Lop (AFL), and emerging of full
fuzz test automation systems, like Google’s ClusterFuzz, are already milestones that
justify an update for this type of a book.
The goal of the first edition was to be an educational book that would not be
outdated in a decade. You are now reading the second edition, so things did not
work out exactly as planned. For this edition, our goal was to refresh the content
with major changes that have happened in the context of fuzzing in the last decade.
Some of the tools and techniques referenced in the first edition are long gone, but we
didn’t want to just erase them. They still serve in educational purposes, as they show
how fuzzing and related technologies have evolved. With these new updates, we are
certain that this book will serve as “the fuzzing book” for yet another decade or so.
Atte
xxv
6760 Book.indb 25 12/22/17 10:50 AM
6760 Book.indb 26 12/22/17 10:50 AM
C h a p t e r 1
Introduction
Welcome to the world of fuzzing!
In a nutshell, the purpose of fuzzing is to send anomalous data to a system to
crash it, therefore revealing reliability problems. Fuzzing is widely used by both
security and by quality assurance (QA) experts, although some people still suffer
from misconceptions regarding its capabilities, effectiveness, and practical imple-
mentation. Fuzzing can be defined as
A highly automated testing technique that covers numerous boundary cases
using invalid data (from files, network protocols, API calls, and other tar-
gets) as application input to better ensure the absence of exploitable vulner-
abilities. The name comes from modem applications’ tendency to fail due
to random input caused by line noise on fuzzy telephone lines.1
However before you explore fuzzing further, we ask you to try to understand
why you are interested in fuzzing. If you are reading this, one thing is clear: You
would like to find bugs in software, preferably bugs that have security or safety
implications. Why do you want to find those flaws? Generally, there are three dif-
ferent purposes for looking for these types of defects:
1. Quality Assurance (QA): Testing and securing your internally devel-
oped software.
2. System Administration (SA): Testing and securing software on which you
depend in your own usage environment.
3. Vulnerability Assessment (VA): Testing and trying to break into someone
else’s software or system.
In this book, we will look at fuzzing from all of these perspectives. We will view
fuzzing from a developer’s perspective, as well as through the eyes of an enterprise
end user. We will also consider the requirements of a third-party assessment team,
whether that is a testing consultant or a black-hat hacker. One goal of this book is
to level the playing field between software companies (testers) and vulnerability ana-
lysts (hackers). Software testers can learn from the talents of hackers, and vice versa.
Fuzzing is the most powerful test automation tool for discovering security-critical
problems in software. One could argue that code auditing tools find more flaws
1 Oehlert, P. “Violating Assumptions with Fuzzing,” IEEE Security & Privacy (March/April 2005):
58–62.
1
6760 Book.indb 1 12/22/17 10:50 AM
2 Introduction
in code, but after comparing the findings from a test using intelligent fuzzing and
a thorough code audit, the result is clear. Most findings from code auditing tools
are false positives, alerts that have no security implications. Fuzzing has no such
problem. There are no false positives. A crash is a crash. A bug is a bug. And almost
every bug found with fuzzing is exploitable at some level, at minimum resulting in
denial of service. As fuzzing is generally black-box testing, every flaw is, by defini-
tion, remotely exploitable, depending, of course, on the interface you are fuzzing
and to some extent on your definition of exploitation. Fuzzing is especially useful
in analyzing closed-source, off-the-shelf software and proprietary systems, because
in most cases it does not require any access to source code.
In this chapter we will present an overview of fuzzing and related technologies.
We will look at why security mistakes happen and why current security measures
fail to protect us from security compromises that exploit these mistakes. We will
explore how fuzzing can help by introducing proactive tools that anyone can use to
find and eliminate security holes. We will go on to look where fuzzing is currently
used, and why. Finally, we will get a bit more technical and review the history of
fuzzing, with focus on understanding how various techniques in fuzzing came into
existence. Still, remember that the purpose of this chapter is only to provide an
overview that will prepare you for what is coming later in the book. Subsequent
chapters will provide more details on each of these topics.
1.1 Software Security
Fuzzing is a great technique for finding security-critical flaws in any software rap-
idly and cost effectively. Unfortunately, fuzzing is not always used where it should
be used, and therefore many systems on which we depend are immature from a
security perspective. One fact has emerged from the security field: Software will
always have security problems. Almost all software can be hacked easily. However if
you become familiar with the topic of software security and the related techniques,
you might be able to make a difference on how many of those parasitic security
mistakes eventually remain in the software. This is what software security is about.
Few people today know what software security really is, even if they are so-
called security experts. Like the maps from ancient times used to warn, the danger-
ous area just outside the map is sometimes best left alone. The uncharted territory
just read, “Here be dragons,” meaning that you should not venture there. It is too
scary or too challenging. Fortunately for software security, the age of darkness is
over because the first explorers risked their souls and delved into the mystic lands of
hacking, trying to explain security to ordinary software developers. First, they were
feared for their new skills, and later they were blamed for many of the dangerous
findings they encountered. Even today they are thought to possess some secret arts
that make them special. However what they found was not that complex after all.
Software security testing can be introduced at various organizations, starting
from research and development (R&D), then entering the test-lab environment, and
finally in the operations (Figure 1.1).
In R&D, fuzzing can be used both in the early prototyping phase and in the
implementation phase, where the majority of programming takes place. In fact,
6760 Book.indb 2 12/22/17 10:50 AM
1.1 Software Security 3
Figure 1.1 Places for using fuzzing: In development, in third-party test laboratories and in
Enterprise operations.
immediately when the first operational prototype is ready, it can be fuzzed. Test
automation is often integrated to Continuous Integration, automated build processes,
and regression testing. Source code auditing tools are also actively used in software
development to eliminate the easiest weaknesses caused by programming mistakes.
R&D includes testing, especially tests that take place before system integration.
In test laboratories after the system has been integrated a dedicated testing
team typically performs most of the remaining testing efforts. Depending on the
software development process, some of the testing is more closely integrated into
the development. The remaining tests conducted in the test lab environment can
be quite different from the tests performed in R&D environment. Test labs are
extremely common in systems development for example in telecommunications,
finance and industrial networks, and can be quite significant in physical size. In a
test lab, a system can be tested with any available tools, and the test results can be