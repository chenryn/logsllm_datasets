4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
; on entry:
; r0 (read-only): current time slot
; r1 (read-write): probe sequence
geom r2, #numerator
; store geometrically distributed
; random number with parameter
; p = #numerator/100 in r2
add r2, r0
; r2 := r2 + current slot number
load r3, r2
; r3 := r2
add r3, #1
; increment r3 by 1
add r1, #1
; increment probe sequence by 1
schedule_probe r2, r1
; schedule probe for slot r2
; with sequence r1
add r1, #1
; increment probe sequence
schedule_probe r3, r1
; send probe at slot r3
; with sequence r1
schedule_callback r3
; schedule callback for slot r3
; on exit:
; probe sequence (r1) saved to user state
; r2, r3 saved to user state
4.3 Implementation
There are three software components to MAD: the main MAD
daemon, which receives, processes, and coordinates user requests
to send probes, a probe reﬂector, and a probe receiver. (We use the
term MAD to speciﬁcally refer to the probe sending daemon, unless
otherwise stated.) A user accesses MAD through a remote proce-
dure call interface. To initiate a new set of measurements, a user
speciﬁes up to 16 32-bit words of MADcode, along with the maxi-
mum number of probes to send, the destination IP address and port,
the source address and port, the probe packet size, and the number
of packets per probe (e.g., BADABING speciﬁes 3). If there is more
than 1 packet per probe, the user also speciﬁes what the spacing
should be between those packets. The user must also present a
32-bit user-speciﬁc identiﬁer, as described below. There are also
RPC facilities for aborting an existing probe process, querying the
number of probes sent for a particular user, and querying the MAD
daemon for its internal statistics and error information.
One of the key enablers for MAD to run effectively on a standard,
unmodiﬁed Linux system has been the introduction of schedul-
ing and timer-related features from the real-time operating systems
community into the mainline kernel code base. These additions
originally appeared as patches to the 2.6.13 kernel, and were even-
tually incorporated into version 2.6.16 [5, 6]. One critical enhance-
ment included modiﬁcations to add a high-resolution timer sub-
system, providing timing capabilities ﬁner than the default sched-
uler quantum (i.e., the HZ parameter, typically set to 1000 in re-
cent Linux kernels, giving a default timer granularity of 1 millisec-
ond). The nanosleep system call uses this new timer subsys-
tem, whereas the previous implementation (e.g., versions previous
to 2.6.16) performs a busy-wait.
MAD utilizes the new real-time features of recent Linux kernels,
and in addition uses the real-time scheduling capabilities of Linux
and runs at the highest possible priority level. Running as a high-
priority process implies that MAD must run very efﬁciently or it
may negatively impact system-wide performance. In the next sec-
tion we evaluate MAD’s impact on the host system and show that
under typical use, it has extremely low resource requirements.
We implemented MAD in two ways: as a thread running from
a kernel loadable module, and as a standalone user-mode process
requiring privileged (root) access in order to run at a high prior-
ity. Both versions of MAD use a raw socket for sending UDP probe
packets, to allow simple modiﬁcation of source addresses for multi-
homed hosts, and other protocol header elements. A compile-time
parameter speciﬁes the maximum number of users that can simul-
taneously use MAD. Another compile-time parameter speciﬁes the
maximum number of IP destinations that MAD can support. For
each destination, MAD maintains a time-slot ordered list of probes
to send in the future.
The in-kernel implementation uses kernel-equivalent calls to use
standard the UDP/IP stack, and directly uses the high-resolution
timer subsystem. In our experiments, described in the next section,
we found that there was little qualitative performance difference be-
tween the in-kernel version, and the user-mode version. This result
is good news, since it is better from a system reliability perspective
to avoid implementation in kernel space where a programming bug
can take down the entire system.
We also implemented the probe reﬂector as both a user-mode
process and as an in-kernel daemon. As with the MAD daemon,
we did not observe qualitative differences in performance. The im-
plementation of the reﬂector is straightforward, and uses the BSD
socket interface in a standard way to receive and send probes. One
important run-time option is to rewrite the destination port or IP ad-
dress with supplied values rather than simply swap the source and
destination addresses and ports.
The implementation of the probe receiver is also straightforward.
It is implemented only as a user-mode process. It is important to
note that both probe reﬂector and receiver use the SO_TIMESTAMP
option of setsockopt to obtain kernel-level timestamps. Cur-
rently, the receiver simply writes received probe information to
disk. In the future, we plan to implement an RPC interface so that
a user of MAD can both initiate a set of measurements, and retrieve
the probes from the receiver host remotely.
All components of MAD are written in C, and comprise about
3,000 lines of code (including both user-mode and in-kernel ver-
sions of the MAD daemon and probe reﬂector). MAD will be made
available to the research community.
In order to initiate a measurement process, a user must present
a 32-bit identiﬁer. Our intention is for this identiﬁer to represent a
time-limited access token to MAD, similar to a Kerberos ticket. Us-
ing traditional public key encryption, a user could be authenticated
and given a token authorizing the user access to MAD for a limited
duration. The token can be encrypted using less heavyweight pri-
vate key encryption, with the key shared between the token process
and the MAD daemon. This functionality is not yet implemented.
In addition, accounting and quota facilities in MAD are not yet im-
plemented.
There are certain limitations to MAD that we plan to address in
the future. The ﬁrst is that stream-based probe algorithms such
as Pathload [21] and Yaz [41] are not yet easily implemented as
MADcode. The reason is that the duration over which a stream is
sent (for these algorithms 50–100 packets) can be longer than the
discrete time interval used by MAD, e.g., 5 milliseconds. It is not
yet clear how to best enhance MAD to permit these types of algo-
rithms. Adaptive algorithms are also not yet easily implemented,
primarily because of the lack of a remote interface to the probe re-
ceiver. If a user can obtain measurements remotely from the probe
receiver, it can then use that information for requesting a new set
of measurements to MAD, thus completing the feedback loop. Ad-
ditionally, user code can only have one callback outstanding. This
limitation exists partly for safety: allowing users to add an arbitrary
number of callbacks is probably not necessary nor wise. However,
some ﬂexibility may be necessary for implementing some probe
algorithms. Another limitation is that packet payloads cannot be
modiﬁed by MAD users. It is not yet clear how to arbitrate among
multiple users wishing to specify payload content, or whether this
would be a useful feature. Finally, all users are treated at the same
priority level in MAD. It may be useful in the future to be able to as-
sociate a priority level with a user’s credentials in order to provide
different levels of scheduling service quality.
5. EVALUATION
In this section we describe the evaluation of MAD in a controlled
laboratory setting. We describe two different laboratory setups to
examine the performance of MAD over a range of synthetic work-
loads on the measurement hosts and in both virtualized and non-
virtualized operating system conﬁgurations. In each environment,
we compare the performance of MAD with that of an unprivileged
user-level process. Finally, we report results of microbenchmark
scalability tests performed on MAD.
5.1 Controlled PlanetLab Experiments
In our ﬁrst laboratory setup for evaluating MAD, we created a
PlanetLab environment using the MyPLC software available at
http://www.planet-lab.org [3]. We used version 0.4.3
of the MyPLC software. The node operating system included with
this software is based on version 2.6.17 of the Linux kernel (a newer
version than is running on current production PlanetLab hosts shown
on CoMon [26]).
For our laboratory PlanetLab nodes we used two identical work-
stations with Pentium 4 processors running at 2 GHz, each with
1 GB RAM and Intel Pro/1000 network adapters. (As in our ini-
tial experiments, interrupt coalescence was disabled.) We used a
network conﬁguration similar to the one used for our production
PlanetLab hosts (shown in Figure 1) for collecting ground truth
packet traces with Endace DAG 4.3 GE cards.
To create synthetic load conditions on the PlanetLab nodes, we
used the Harpoon network trafﬁc generator [37], replicated over
a number of experimental slices ranging between 0 and 100, as
shown in Table 6, resulting in ﬁve different load scenarios. For
each slice in all experiments in which at least one slice was con-
ﬁgured, we ran one Harpoon process with 5 threads for producing
self-similar TCP trafﬁc, and one thread for producing constant bit-
rate UDP trafﬁc. The threads are conﬁgured to produce a relatively
low average trafﬁc volume of about 100 Kb/s. The UDP threads in
our tests are conﬁgured to produce 10 Kb/s using small (40 byte)
packets, resulting in about 24 packets per second per UDP thread.
This conﬁguration has the (intended) effect of artiﬁcially raising
CPU utilization due to overheads related to timer maintainence.
The measurement algorithms we tested using MAD were the same
as we used in the experiments with the production PlanetLab sys-
tems, described in Table 1. For each load scenario, we ﬁrst ran
each of the three measurement algorithms using MAD running as a
privileged (root) user. We then ran each of the three measurement
algorithms using an additional slice without any special privileges,
similar to the way in which the measurement algorithms were run
on the live PlanetLab hosts. Note that none of the slices conﬁgured
in our testbed had any processor or bandwidth limitation placed on
them.
We ﬁrst look at results from the experiments using round-trip
delay probes. Table 7 shows quantiles of the delay distribution for
6: Conﬁgurations and characteristics of laboratory-based Planet-
Lab experiments. For each slice, one Harpoon process was started
with 5 threads for producing self-similar TCP trafﬁc, and one
thread for producing constant bit-rate UDP trafﬁc. The average
ratio between TCP and UDP trafﬁc produced was 90:10.
Number of slices
Trafﬁc Volume Average CPU
(each direction)
Utilization
0
1
10
50
100
0
100 Kb/s
1 Mb/s
5 Mb/s
10 Mb/s
0%
5%
60%
95%
100%
a setup using a standard PlanetLab slice for sending the probes, and
for a setup using MAD. Results for each of the ﬁve load scenarios
are shown. From the table, we ﬁrst notice that in the setup using a
standard PlanetLab slice for sending probes, the only load scenario
in which there is less than 1 millisecond delay at the 99th percentile
is the one without any Harpoon processes. At a load of 50 slices
the results appear qualitatively similar to those measured on the live
PlanetLab hosts (cf. Table 2). For the scenario using 100 slices, the
workstation is extremely overloaded, resulting in excessive round-
trip delays. Most importantly in Table 7, we observe that for all
load scenarios, the round-trip delays measured using MAD are on
the order of 100 microseconds, even in the extreme scenario of 100
slices.
Results from experiments using BADABING in the laboratory-
based PlanetLab setup are shown in Table 8. The table shows re-
sults for the setup using a standard PlanetLab slice for sending the
probes, and for a setup using MAD, for each of the ﬁve load sce-
narios. From the table, we observe that in the standard PlanetLab
slice setup there is no loss measured in the 0, 1, and 10 slice con-
ﬁguration, with a small amount of loss observed in the 50 slice
scenario, and signiﬁcant loss in the 100 slice scenario. As with
our experiments using the live PlanetLab hosts, we used our DAG
measurement systems to conﬁrm that these losses were measure-
ment errors. Finally, we see in Table 8 that for all load scenarios in
which MAD is used, there is no loss measured by the BADABING
probes.
For the packet pair experiments, we observed results that were
similar to the experiments run on the live PlanetLab systems. In
comparing the intended spacing between packets of a pair on send
versus the actual produced, we found that the mean error is close to
zero when considering a few tens of packet pairs. Similarly, the er-
ror mean between the spacing measured by the sending application
and actual the spacing of the packet pair was close to zero for a few
tens of packet pairs. This observation is true for both MAD and for a
unprivileged process running within a standard slice. We found that
as load increased, the interquartile range increased, i.e., it spread
further from the median in both the negative and positive error di-
rections. The interquartile range was consistently larger for the un-
privileged process, but not signiﬁcantly so. For example, with the
highest load scenario, the 25th percentile error was about -8 mi-
croseconds for the unprivileged process and about -6 microseconds
for MAD, while the 75th percentile error was about 8 microseconds
for the unprivileged process and about 5 microseconds for MAD.
The median error was close to zero in both cases.
For errors in application-measured receive spacing versus the
spacings measured using the DAG cards, we observed a similar
pattern as in our live PlanetLab experiments, namely that of a shift
away from zero mean error toward consistently positive error (i.e.,
spacing measured at DAG was less than spacing measured in oper-
7: Quantiles of the delay distribution measured on laboratory PlanetLab nodes. Results shown for a standard PlanetLab slice (similar to the
experiments using live PlanetLab nodes) and for MAD.
Number of
Slices
0
1
10
50
100
Standard PlanetLab slice
50
0.000
0.000
0.000
0.001
1.750
90
0.000
0.000
0.001
0.002
15.291
95
0.000
0.000
0.001
0.002
21.801
99
0.000
0.001
0.002
0.049
29.329
50
0.000
0.000
0.000
0.000
0.000
MAD
90
0.000
0.000
0.000
0.000
0.000
95
0.000
0.000
0.000
0.000
0.000
99
0.000
0.000
0.000
0.000
0.000
8: BADABING results for laboratory PlanetLab nodes. Results shown for a standard PlanetLab slice (similar to the experiments using live
PlanetLab nodes) and for MAD.
Number of
Slices
0
1
10
50
100
Standard PlanetLab slice
MAD
Frequency Duration Rate Estimate
0.0000
0.0000
0.0000
0.0024
0.0447
0.0000
0.0000
0.0000
0.0024
0.0447
0.000
0.000
0.000
0.000
2.447
Frequency Duration Rate Estimate
0.0000
0.0000
0.0000
0.0000