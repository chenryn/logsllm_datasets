Algorithm 1 Learning a plan-generation policy using an actor-
critic algorithm.
Input: The network topology with the original capacity G∗.
Main routine:
1: // Initialization
2: Initialize the actor parameters θ , the critic parameters θv , the GNN parameters
θд
// Generate several network plans with the current actor
epochBuf f er .cl ear ()
while !EpochEnd do
3: Number of epochs N
4: G ← Reset(G∗)
5: for n=1,2,...,N do
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
l oд p ← π (a |G ; θ , θд )
a ← l oд p .sampl e ()
v ← V (G ; θv , θд )
G , r ← UpdateTopo(G , a)
epochBuf f er .append (l oд p, a, v , r , G)
if T r a j End (G) then
G ← Reset(G∗)
Reset gradients dθ ← 0, dθv ← 0 and dθд ← 0
// Compute gradients wrt. actor gradient loss
dθ , dθд ← Comput e P Loss (epochBuf f er )
Perform update of θ using dθ and θд using dθд
// Compute gradients wrt. critic gradient loss
dθv , dθд ← Comput eV Loss (epochBuf f er )
Perform update of θv using dθv and θд using dθд
Subroutines:
• Reset(G): Reset the network topology G to its initial state.
• UpdateTopo(G, a): Apply action a to network topology G, and return the
updated topology and the intermediate reward for the action.
• TrajEnd(G): Define the condition of the current trajectory end as G satisfies
the service expectations or the length.
• ComputePLoss(epochBuffer): Compute the advantage estimates, and update
the gradient as the mean error between it and l oд p.
• ComputeVLoss(epochBuffer): Compute the rewards-to-go, and update the
gradient as the mean-square error between it and v.
transformed topology. We use the current capacity of the IP link as
the feature for each node in the transformed topology. For training
efficiency, we normalize each dimension of the node feature across
all the nodes in the transformed topology with mean = 0, std = 1.
This is because an RL agent tends to generate the same action if the
values are the same in most of the dimensions in the input state.
Normalization is a commonly-used technique to avoid generating
a consecutive sequence of same actions.
Action representation. The action representation indicates which
link to select to add capacity and how much capacity to add. Let
the largest amount of capacity unit to add in one step be m and the
number of nodes in the transformed topology is n. The size of the
action space is mn.
An alternative approach is to allow both adding and reducing
capacity for IP links in the actions. It is important to note that both
approaches cover the same search space, i.e., any plan found by
the alternative can be found by only allowing adding capacity in
the actions. There are three benefits to only allow adding capacity
in the actions. First, it leads to a smaller action space. The size of
the action space with only adding capacity is half of that with both
adding and reducing capacity. Second, it leads to a stable and simple
training process. A trajectory can start from the original network
and be terminated once the network can satisfy the demand under
264
SIGCOMM ’21, August 23–28, 2021, Virtual Event, Netherlands
Hang Zhu, Varun Gupta, Satyajeet Singh Ahuja,
Yuandong Tian, Ying Zhang, Xin Jin
the reliability policy when the agent only adds capacity. Third, it can
benefit from the stateful failure checking (ğ5) for a faster learning
process. If a network survives a failure, then a network with more
capacity can guarantee to survive the same failure. Thus, there is
no need to check the failures that have been already survived. In
contrast, we have to check all the failures on every step if reducing
capacity is also included in the actions.
One domain-specific customization for action representation is
that we use an action mask to handle the spectrum consumption
constraints. The mask turns off the IP links if adding more capacity
to these links would violate the spectrum consumption constraints.
The stochastic policy only samples among valid IP links instead of
all IP links.
Reward representation. The goal of NeuroPlan is to minimize the
cost of the network while satisfying the traffic demand under the
reliability policy. The ultimate reward is the cost of a network plan.
While in principle we could return the cost of a network plan as a
single final reward to the agent after generating a feasible solution,
it would be hard to train the agent effectively, especially considering
the cases with long trajectories where thousands of steps are needed
for a feasible solution. To generate dense rewards, we assign an
intermediate reward to each step with the cost of the newly added
capacity and scale it down with a normalized parameter to get
a final reward in the range of [−1, 0). It is a common practice to
use reward scaling to get better results for deep RL [21]. After a
pre-defined number of steps, if we cannot get a feasible solution,
we add −1 as the extra penalty for the final reward.
Training algorithm The RL agent is trained by an Actor-Critic
algorithm [31]. In an Actor-Critic algorithm, we learn an actor
π (a|G; θ , θд ) which gives a probability distribution of next-step
action a given the current topology G, and a critic V (G; θv , θд ) that
outputs a value to evaluate the current topology G. Actor-Critic
algorithm is known to be more stable and efficient than the simple
policy-gradient algorithm [27], and has been successfully applied
to solve many tasks [3, 63, 69].
Algorithm 1 shows the pseudocode of the training algorithm.
The algorithm first initializes the parameters of the actor, the critic,
the GNN and the number of epochs (Line 2-3). For each epoch,
several network plans are sampled from the probability distribution
of the action returned by the current actor. With the same actor,
many different plans can be sampled in order to achieve adequate
exploration.
The generation of every network plan is termed as a trajectory.
For each trajectory, NeuroPlan starts from the network topology
with the original link capacity (which could be zero). It then gener-
ates the network plan by iteratively performing an action computed
by the actor to the current network topology until the trajectory
ends (Line 8-15). The trajectory is terminated under three condi-
tions: (i) the current network topology satisfies the traffic demand
under the reliability policy; (ii) the trajectory length exceeds a pre-
defined threshold, and (iii) the trajectory is cut off by the current
epoch. Any of the three conditions is true means the trajectory
termination.
At the end of each epoch, we first compute the policy gradient
loss (Line 18). The gradient loss of the actor is defined as the mean
error between the advantage estimate and loд p across the epoch,
mask
feature
topology
GNN
graph
embedding
action
actor network
value
critic network
Figure 6: Actor and Critic network architecture of Neuro-
Plan.
where loд p is the logit of the corresponding sampled action. The ad-
vantage estimate for step i (GAEi ) is calculated as the GAE-Lambda
advantage [53] by
GAEi = ri + γ · vi +1 − vi + γ · λ · GAEi +1 ,
(6)
where ri and vi is the reward and the output of critic at step i
respectively, γ is the discount factor and λ is a smoothing parameter
for reducing variance. Then we compute the critic gradient loss
(Line 21). It is defined as the mean-square error between the reward-
to-go and v across the epoch, where v is the output of critic. The
reward-to-go is calculated by applying the discount factor to the
intermediate rewards.
Neural network architecture. Figure 6 shows the neural net-
work architecture of the agent. We use a Graph Convolutional
Network (GCN) [29] to encode the transformed network topology
and generate a graph embedding. GCNs are a well-studied type of
GNNs that achieve good performance in many graph representation
tasks [29, 43, 74]. We have also experimented NeuroPlan with a
Graph Attention Network (GAT) [64]. GATs introduce an attention
mechanism as a substitute for the statically normalized convolution
operation in GCNs. GATs did not perform as well as GCNs for our
problem. Moreover, GAT has larger memory requirement, making
it infeasible for large-scale problems.
The high-level idea of GCNs is to perform message propagation
between neighbor nodes for multiple layers. Given n nodes and
an f -dimension feature vector for each node, the first-layer graph
embedding is represented as an n × f matrix, H (0). Then the l + 1-th
layer graph embedding can be computed by
H (l +1) = ReLU (D−0 .5(A + I )D0 .5H (l )W (l )),
(7)
where W (l ) is the learnable weight matrix at l-th layer, A is the
adjacent matrix, I is the identity matrix, and D is the degree matrix
of A. After L layers of GCN, we get the final graph embedding
G = H (L). Then, we feed the graph embedding to the critic and
actor network.
The critic and actor network are both simple Multilayer Percep-
tron (MLP). To realize a stochastic policy, the actor outputs the
logits, which can be used to sample a feasible action after applying
the action mask.
265
Network Planning with Deep Reinforcement Learning
SIGCOMM ’21, August 23–28, 2021, Virtual Event, Netherlands
4.3 Search Space Pruning
As we have discussed in ğ4.1, considering the scale of the problem,
it is challenging for deep RL to directly generate the final plan.
Algorithm 1 indicates that the RL agent will keep adding capacity to
the network until the network satisfies the traffic demand under the
reliability policy. There may be useless steps in a feasible trajectory
that do not contribute to satisfying the traffic demand. Thus, we
use a two-stage hybrid approach which encodes the plan generated
by deep RL as the maximum capacity constraints for the IP links
to the ILP model. We then solve the ILP model to find the optimal
solution under these constraints with an off-the-shelf ILP solver.
To alleviate the impact of local optimum, we relax the maximum
capacity constraints by multiplying the maximum capacities by
the relax factor α. The relax factor α provides a tunable trade-off
between optimality and tractability.
Interpretability of the solution. In essence, the two-stage ap-
proach resembles the existing approach described in ğ3.2. The RL
agent takes over the job of the heuristics to prune the search space,
and does so in an automated way without the need of human ex-
perts. Then ILP is applied to find the optimal solution in the pruned
search space.
A common issue for deep learning approaches is interpretability.
Our approach avoids this issue and makes the generated solution
interpretable. Specifically, network operators can examine the solu-
tion from the RL agent and check whether the changes match their
intuition and experience. More importantly, they can use the relax
factor α to control the trade-off between optimality and tractabil-
ity. This control is simpler and more explicit than tuning several
hand-designed heuristics. It also clearly dictates the final solution
is optimal in which part of the search space. And it is easy to incor-
porate additional modifications to the pruned search space from
other heuristics.
5 IMPLEMENTATION
RL algorithm and environment. We implement the Actor-Critic
algorithm of the RL agent based on the SpinningUp [55] framework
and add support for GPU training. The RL environment is imple-
mented in Python for compatibility. We list the hyperparameters
used in NeuroPlan in Table 2.
Optimizations for the plan evaluator. As shown in Figure 3, the
plan evaluator interacts with the RL agent and checks if the network
plan satisfies the service expectations. For the problem of network
planning, we only need to focus on the macro-scale behavior of the
network (e.g., the IP link capacity). Thus, we do not need packet-
level simulators (e.g., NS-3 [49]) that model micro-scale network
behaviors such as network congestion. We do not need network
emulators, either, such as Mininet [42] and CrystalNet [36] which
run actual control plane and data plane code.
To check if the current network plan satisfies the service expec-
tations, we formulate the problem as an LP problem and solve it
with the Gurobi Optimizer [19]. Note that the LP problem is only
to check if the current network plan can survive the failures and
the network plan is given, which is much simpler than the existing
approach (ğ3) for the entire network planning problem which tries
to solve an ILP problem and find a plan with minimum cost. Gurobi
Hyperparameter
Max length per trajectory
Max epochs to train
Max length per epoch
Max capacity units per step
Model nonlinearity
GNN type
Number of GNN layers
MLP hidden layers
Actor learning rate
Critic learning rate