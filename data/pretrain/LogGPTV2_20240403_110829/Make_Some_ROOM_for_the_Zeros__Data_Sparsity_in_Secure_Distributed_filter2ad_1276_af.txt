43
231
Total
95626
101631
1033
9915
Dataset
Offline Time
Total Time
Ours
Ours
SecureML
6h17m45.06s
1h39m33.66s
3.56s
1h1m16.34s
SecureML
6h29m28.37s
1h42m38.14s
5.9s
1h3m7.12s
Movies
Newsgroups
Languages, ngrams=1
Languages, ngrams=2
Table 3: Comparison of our approach with SecureML [29] in the LAN setting. Offline times are extrapolated from the results
reported in [29]. In all experiments, we use a batch size of 128. The total time represents a full training epoch, including
forward pass, sigmoid activation function, and backward pass.
186.25 GiB
46.5 GiB
789.88 MiB 390.75 MiB
796.82 GiB
187.42 GiB
47.63 GiB
790.9 MiB 500.61 MiB
3.69 GiB
2h43m46.09s
42m37.68s
29.89s
6m17.51s
14m19.29s
3m34.55s
1.76s
13.07s
2.83 GiB 797.85 GiB
Offline Communication
SecureML
4.8 TiB
1.26 TiB
Ours
Total Communication
Ours
SecureML
4.8 TiB
1.26 TiB
transfered. We also estimate offline times using the measurements
provided by Mohassel and Zhang [29, Table II], and we present it
together with the total time that includes both phases in Table 3.
While the dense solution [29] achieves fast online computation,
this comes at a significant offline computation and communication
cost, requiring hours, even days in the WAN setting (Appendix C.1),
and sometimes terabytes of communication. Our solution on the
other hand saves a factor of 2x‚Äì11x in total runtime and a factor
26x‚Äì215x in communication in all reasonably large datasets (in
‚ÄúLanguages, ngrams=1‚Äù, SecureML is faster in total time, but both
executions take just a few seconds).
Finally, we investigate how our solution scales with different
dataset sparsities and the batch size used for training. For that, we
run experiments on synthetic datasets. We use 1024 documents
for each of the two servers, and vary the batch size between 128
and 1024. For each batch, we set the number of nonzero values
between 1% and 10%. For comparison, the sparsity of a batch of 128
documents from the Movies or Newsgroups datasets is about 3%.
The results are shown in Figure 14. It can be seen that our sparse
implementation benefits a lot from increasing the batch size. How-
ever, increasing the batch size will also increase the number of
nonzeros per batch in real datasets, albeit sub-linearly. Thus, the
batch size can be optimized to account for the sparsity of the dataset
being used for training. Research on training ML models in the
clear suggests larger batch sizes can be used without losing accu-
racy [19], and we conjecture that this allows us to achieve even
better speedups than those reported in Table 3, at the same level
of accuracy. However, in order to stay functionally equivalent to
previous work [29], we omit such optimizations at this point.
Figure 14: Total running time of a Stochastic Gradient De-
scent (SGD) training epoch for logistic regression. We use
synthetic datasets with 1024 documents per server, a vocab-
ulary size of 150k, and varying sparsity per batch.
8.2 Logistic Regression Training
For each of our datasets, we also evaluate the time needed to build a
logistic model using our protocol from Figure 12. We compare two
approaches. One uses the state-of-the-art dense matrix multiplica-
tion protocol to instantiate MvMult (cf. lines 6 and 9 in Figure 12),
which is the extension of Beaver triples [3] to matrices proposed
in [29]. The second approach uses our sparse matrix multiplication
protocols from Figure 8 and Section 5.2.2 for forward- and back-
ward pass, respectively. We measure the online running time of a
full run using both approaches, as well as the total amount of data
12
1282565121024BatchSize10s30s1m2m5m10m30m1hRunningtimeLogisticRegression(LAN)Dense10.0%Nonzeros5.0%Nonzeros2.0%Nonzeros1.0%Nonzeros9 CONCLUSION
Privacy preserving techniques for machine learning algorithms
have a wide range of applications, which most often need to han-
dle large inputs, and thus scalability is crucial in any solution of
practical significance. Exploiting sparsity is heavily leveraged by ex-
isting computation frameworks to achieve scalability, and in many
settings some sparsity metric of the data is already public. This
can be leveraged in the setting of privacy-preserving data analysis,
not only at the application level, but also in terms of lower-level
operations.
A practical and principled approach to this problem calls for a
modular design, where in analogy to the components architecture
in scientific computing frameworks, algorithms for linear algebra
are built on top of a small set of low-level operations. In this pa-
per we proposed a framework that takes a first step towards this
vision: we started by defining sparse data structures with efficient
access functionality, which we used to implement fast secure mul-
tiplication protocols for sparse matrices, a core building block in
numerous ML applications.
By implementing three different applications within our frame-
work, we demonstrated the efficiency gain of exploiting sparsity in
the context of secure computation for non-parametric (ùëò-nearest
neighbors and Naive-Bayes classification) and parametric (logis-
tic regression) models, achieving manyfold improvement over the
state of the art techniques. The existing functionalities in our frame-
work represent main building blocks for many machine learning
algorithms beyond our three applications. At the same time, our
modular framework can be easily extended, opening the ROOM to
future improvements.
ACKNOWLEDGMENTS
Phillipp Schoppmann was supported by the German Research
Foundation (DFG) through Research Training Group GRK 1651
(SOAMED). Adri√† Gasc√≥n was supported by The Alan Turing Insti-
tute under the EPSRC grant EP/N510129/1, and funding from the
UK Government‚Äôs Defence & Security Programme in support of
the Alan Turing Institute. Mariana Raykova‚Äôs work on this paper
was done while at Yale University supported in part by NSF grants
CNS-1633282, 1562888, 1565208, and DARPA SafeWare W911NF-16-
1-0389. Benny Pinkas was supported by the BIU Center for Research
in Applied Cryptography and Cyber Security in conjunction with
the Israel National Cyber Directorate in the Prime Minister‚Äôs Office.
REFERENCES
[1] Sebastian Angel, Hao Chen, Kim Laine, and Srinath T. V. Setty. 2018. PIR with
Compressed Queries and Amortized Query Processing. In IEEE Symposium on
Security and Privacy. IEEE, 962‚Äì979.
[2] Kenneth E. Batcher. 1968. Sorting Networks and Their Applications. In AFIPS
Spring Joint Computing Conference (AFIPS Conference Proceedings), Vol. 32. Thom-
son Book Company, Washington D.C., 307‚Äì314.
[3] Donald Beaver. 1991. Efficient Multiparty Protocols Using Circuit Randomization.
In CRYPTO (Lecture Notes in Computer Science), Vol. 576. Springer, 420‚Äì432.
[4] James Bennett, Stan Lanning, et al. 2007. The netflix prize. In Proceedings of KDD
cup and workshop, Vol. 2007. New York, NY, USA, 35.
[5] Raphael Bost, Raluca Ada Popa, Stephen Tu, and Shafi Goldwasser. 2015. Machine
Learning Classification over Encrypted Data. In NDSS. The Internet Society.
[6] Elette Boyle, Niv Gilboa, and Yuval Ishai. 2015. Function Secret Sharing. In
EUROCRYPT (2) (Lecture Notes in Computer Science), Vol. 9057. Springer, 337‚Äì367.
[7] Hao Chen, Zhicong Huang, Kim Laine, and Peter Rindal. 2018. Labeled PSI from
Fully Homomorphic Encryption with Malicious Security. In ACM Conference on
Computer and Communications Security. ACM, 1223‚Äì1237.
13
[8] Benny Chor, Niv Gilboa, and Moni Naor. 1998. Private Information Retrieval by
Keywords. IACR Cryptology ePrint Archive 1998 (1998), 3.
[9] Michele Ciampi and Claudio Orlandi. 2018. Combining Private Set-Intersection
with Secure Two-Party Computation. In SCN (Lecture Notes in Computer Science),
Vol. 11035. Springer, 464‚Äì482.
[10] 1000 Genomes Project Consortium et al. 2015. A global reference for human
genetic variation. Nature 526, 7571 (2015), 68.
[11] Kushal Datta, Karthik Gururaj, Mishali Naik, Paolo Narvaez, and Ming Rutar.
2017. GenomicsDB: Storing Genome Data as Sparse Columnar Arrays. White Pa-
per. https://www.intel.com/content/dam/www/public/us/en/documents/white-
papers/genomics-storing-genome-data-paper.pdf
[12] Jack Doerner. [n. d.]. The Absentminded Crypto Kit.
https://bitbucket.org/
jackdoerner/absentminded-crypto-kit/
[13] Iain S. Duff, Michael A. Heroux, and Roldan Pozo. 2002. An Overview of the
Sparse Basic Linear Algebra Subprograms: The New Standard from the BLAS
Technical Forum. ACM Trans. Math. Softw. 28, 2 (June 2002), 239‚Äì267.
[14] Yael Gertner, Yuval Ishai, Eyal Kushilevitz, and Tal Malkin. 2000. Protecting
Data Privacy in Private Information Retrieval Schemes. J. Comput. Syst. Sci. 60, 3
(2000), 592‚Äì629.
[15] Oded Goldreich. 2009. Foundations of Cryptography: Volume 2, Basic Applications
(1st ed.). Cambridge University Press, New York, NY, USA.
[16] Oded Goldreich and Rafail Ostrovsky. 1996. Software Protection and Simulation
on Oblivious RAMs. J. ACM 43, 3 (1996), 431‚Äì473. https://doi.org/10.1145/233551.
233553
[17] Ga√´l Guennebaud, Beno√Æt Jacob, et al. [n. d.]. Eigen: Sparse matrix manipulations.
https://eigen.tuxfamily.org/dox/group__TutorialSparse.html
[18] Ga√´l Guennebaud, Beno√Æt Jacob, et al. 2010. Eigen v3. http://eigen.tuxfamily.org
[19] Elad Hoffer, Itay Hubara, and Daniel Soudry. 2017. Train longer, generalize better:
closing the generalization gap in large batch training of neural networks. In NIPS.
1729‚Äì1739.
[20] Yan Huang, David Evans, and Jonathan Katz. 2012. Private Set Intersection: Are
Garbled Circuits Better than Custom Protocols?. In NDSS. The Internet Society.
[21] Eric Jones, Travis Oliphant, Pearu Peterson, et al. [n. d.]. Sparse matrices
(scipy.sparse) ‚Äì SciPy v1.1.0 Reference Guide. https://docs.scipy.org/doc/scipy/
reference/sparse.html
[22] Chiraag Juvekar, Vinod Vaikuntanathan, and Anantha Chandrakasan. 2018.
GAZELLE: A Low Latency Framework for Secure Neural Network Inference. In
USENIX Security Symposium. USENIX Association, 1651‚Äì1669.
[23] Vladimir Kolesnikov and Thomas Schneider. 2008. Improved Garbled Circuit: Free
XOR Gates and Applications. In ICALP (2) (Lecture Notes in Computer Science),
Vol. 5126. Springer, 486‚Äì498.
[24] Peeter Laud. 2015. Parallel Oblivious Array Access for Secure Multiparty Compu-
tation and Privacy-Preserving Minimum Spanning Trees. PoPETs 2015, 2 (2015),
188‚Äì205.
[25] Yehuda Lindell and Benny Pinkas. 2009. A Proof of Security of Yao‚Äôs Protocol for
Two-Party Computation. J. Cryptology 22, 2 (2009), 161‚Äì188.
[26] Jian Liu, Mika Juuti, Yao Lu, and N. Asokan. 2017. Oblivious Neural Network
Predictions via MiniONN Transformations. In ACM Conference on Computer and
Communications Security. ACM, 619‚Äì631.
[27] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng,
and Christopher Potts. 2011. Learning Word Vectors for Sentiment Analysis. In
ACL. The Association for Computer Linguistics, 142‚Äì150.
[28] R. Moenck and Allan Borodin. 1972. Fast Modular Transforms via Division. In
SWAT (FOCS). IEEE Computer Society, 90‚Äì96.
[29] Payman Mohassel and Yupeng Zhang. 2017. SecureML: A System for Scalable
Privacy-Preserving Machine Learning. In IEEE Symposium on Security and Privacy.
IEEE Computer Society, 19‚Äì38.
[30] Kartik Nayak, Xiao Shaun Wang, Stratis Ioannidis, Udi Weinsberg, Nina Taft,
and Elaine Shi. 2015. GraphSC: Parallel Secure Computation Made Easy. In IEEE
Symposium on Security and Privacy. IEEE Computer Society, 377‚Äì394.
[31] Valeria Nikolaenko, Stratis Ioannidis, Udi Weinsberg, Marc Joye, Nina Taft, and
Dan Boneh. 2013. Privacy-preserving matrix factorization. In ACM Conference
on Computer and Communications Security. ACM, 801‚Äì812.
[32] Fabian Pedregosa, Ga√´l Varoquaux, Alexandre Gramfort, Vincent Michel,
Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake VanderPlas, Alexandre Passos, David Cournapeau,
Matthieu Brucher, Matthieu Perrot, and Edouard Duchesnay. 2011. Scikit-learn:
Machine Learning in Python. J. Mach. Learn. Res. 12 (2011), 2825‚Äì2830.
[33] Benny Pinkas, Thomas Schneider, and Michael Zohner. 2018. Scalable Private
Set Intersection Based on OT Extension. ACM Trans. Priv. Secur. 21, 2 (2018),
7:1‚Äì7:35. https://doi.org/10.1145/3154794
[34] Jason Rennie and Ken Lang. 2008. The 20 Newsgroups data set. http://qwone.
com/~jason/20Newsgroups/
[35] Phillipp Schoppmann, Lennart Vogelsang, Adri√† Gasc√≥n, and Borja Balle. 2018.
Secure and Scalable Document Similarity on Distributed Databases: Differential
Privacy to the Rescue. IACR Cryptology ePrint Archive 2018 (2018), 289.
[36] The Scikit-learn authors. [n. d.].
Scikit-learn language identification
dataset. https://github.com/scikit-learn/scikit-learn/tree/master/doc/tutorial/
[37] Avishay Yanai. [n. d.]. FastPolynomial.
https://github.com/AvishayYanay/
text_analytics/data/languages
FastPolynomial
[38] Andrew Chi-Chih Yao. 1986. How to Generate and Exchange Secrets (Extended
Abstract). In FOCS. IEEE Computer Society, 162‚Äì167.