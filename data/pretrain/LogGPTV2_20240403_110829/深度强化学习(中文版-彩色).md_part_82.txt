器（如果你自己没有服务器的话）等，来加速对最终模型的大规模训练。有时这一阶段是和第二
阶段交替进行的，因而这一步在实践中可能会花费较长时间。
在下面几个小节中，我们将分几部分介绍应用深度强化学习的技巧。
18.2 实现阶段
• 从头实现一些基本的强化学习算法。对于深度强化学习领域的初学者而言，从头实现一些基
本的强化学习算法并调试这些算法直到它们最终正确运行，是很好的练习。DeepQ-Networks
作为一种基于价值函数的算法，是值得去自己实现的。连续动作空间、策略梯度和 Actor-
Critic算法也是刚开始学习强化学习算法实现时很好的选择。这个过程会需要你理解强化学
习算法实现中的每一行代码，给你一个强化学习过程的整体感觉。刚开始，你不需要一个
复杂的大规模任务，而是一个相对简单的可以快速验证的任务，比如那些OpenAIGym环
境。在实现这些基本算法的时候，你应当基于一种公用的结构并且使用一种深度学习框架
（比如TensorFlow、PyTorch等），并逐步扩展到更加复杂的任务上，同时使用更加高级的技
术（比如优先经验回放等）。这会显著地加速你随后将不同的深度强化学习算法应用于其他
项目的进程。如果你在实现过程中遇到一些问题，你可以参考其他人的实现方法（比如本
434
18.2 实现阶段
书提供的强化学习算法实现指南）或者通过网络查找你遇到的问题。绝大多数问题都已经
被他人所解决。
• 适当地实现论文细节。在你熟悉了这些基本的强化学习算法后，就可以开始实现和测试一
些在文献中的方法。通常强化学习算法的研究论文中包含很多实现细节，而有时这些细节
在不同论文中不是一致的。所以，当你实现这些方法的时候，不要过拟合到论文细节上，而
是去理解论文作者为何在这些特定情形下选择使用这些技巧。举一个典型的例子，在多数
文章中，实验部分中神经网络的结构细节包括隐藏层的维数和层数、各个超参数的数值等。
这些都或在论文主体、或在补充材料中提到。你不需要在自己的实现版本中严格遵循这些
实现细节，而且你很可能甚至跟原文用不一样的环境来对方法进行测试。比如，在深度决
定性策略梯度（DeepDeterministicPolicyGradient）算法的论文中，作者建议使用Ornstein-
Uhlenbeck（OU）噪声来进行探索。然而，实践中，有时很难说OU噪声是否比高斯噪声更
好，而这往往在很大程度上依赖于具体任务。另一个例子是，在 Vinyals et al. (2019) 关于
AlphaStar的工作中，VanillaTD(λ)方法被证实比其他更高级的离线策略（Off-Policy）修正
方法 V-Trace (Espeholt et al., 2018) 更有效。因此，如果这些技巧不足够通用，那么它们可
能不值得你花精力去实现。相比而言，一些微调方法可能对具体任务有更好的效果。然而，
如上所述，理解作者在这些情况下为何使用这些技巧则是更关键和有意义的。当你采取论
文中的某些想法并将其应用到你自己的方法中时，这些建议可能更有意义，因为有时对于
你自己的具体情况，可能不是论文中的主要想法，而是某些具体技巧或操作对你帮助最大。
• 如果你在解决一个具体任务，先探索一下环境。你应当检查一下环境的细节，包括观察量
和动作的性质，如维度、值域、连续或离散值类型等。如果环境观察量的值在一个很大的
有效范围内或者是未知范围的，你就应该把它的值归一化。比如，如果你使用 Tanh 或者
Sigmoid作为激活函数，较大的输入值将可能使第一个隐藏层的节点饱和，而这训练开始时
将导致较小的梯度值和较慢的学习速度。此外，你应当为强化学习选择好的输入特征，这
些特征应当包含环境的有用信息。你也可以用能进行随机动作选取的智能体来探索环境并
可视化这个过程，以找到一些极端情况。如果环境是你自己搭建的，这一步可能很重要。
• 给每一个网络选取一个合适的输出激活函数。你应当根据环境来对动作网络选择一个合适
的输出激活函数。比如，常用的像ReLU可能从计算时间和收敛表现上都对隐藏层来说可以
很好地工作，但是它对有负值的动作输出范围来说可能是不合适的。最好将策略输出值的
范围跟环境的动作值域匹配起来，比如对于动作值域(−1,1)在输出层使用Tanh激活函数。
• 从简单例子开始逐渐增加复杂度。你应当从比较清晰的模型或环境开始测试，然后逐步增
加新的部分，而不是一次将所有的模块组合起来测试和调试。在实现过程中不断进行测试，
除非你是这个领域的专家并且很幸运，否则你不应当期望一个复杂的模型可以一次实现成
功并得到很好的结果。
• 从密集奖励函数开始。奖励函数的设计可以影响学习过程中优化问题的凸性，因此你应当
从一个平滑的密集奖励函数开始尝试。比如，在第16章中定义的机器人抓取任务中，我们
435
第18章 深度强化学习应用实践技巧
用一个密集奖励函数来开始机器人学习，这个函数是从机器人夹具到目标物体之间距离的
负数。这可以保证值函数网络和策略网络能够在一个较为光滑的超平面上优化，从而显著
地加速学习过程。一个稀疏奖励可以被定义为一个简单的二值变量，用来表示机器人是否
抓取了目标物体，而在没有额外信息的情况下这对机器人来说可能很难进行探索和学习。
• 选择合适的网络结构。尽管在深度学习中经常见到一个有几十层网络和数十亿参数的网络，
尤其在像计算机视觉(Heetal.,2016)和自然语言处理(Jaderbergetal.,2015)领域。对于深度
强化学习而言，神经网络深度通常不会太深，超过5层的神经网络在强化学习应用中不是
特别常见。这是由于强化学习算法本身的计算复杂度造成的。因此，除非环境有很大的规
模而且你有几十上百个GPU或者TPU可以使用，否则你一般不会在深度强化学习中用一
个10层及以上的网络，它的训练将会非常困难。这不仅是计算资源上的限制，而且这也与
深度强化学习由于缺失监督信号而导致的不稳定性和非单调表现增长有关。在监督学习中，
如果网络相比于数据而言足够大，它可以过拟合到数据集上，而在深度强化学习中，它可
能只是缓慢地收敛甚至是发散，这是因为探索和利用之间的强关联作用。网络大小的选择
经常是依据环境状态空间和动作空间而定的。一个有几十个状态动作组合的离散环境可能
可以用一个表格方法，或者一个单层或两层的神经网络解决。更复杂的例子如第13章和第
16章中介绍的应用，通常有几十维的连续状态和动作空间，这就需要可能大于3层的网络，
但是相比于其他深度学习领域中的巨型网络而言，这仍旧是很小的规模。
对于网络的结构而言，文献中很常见的有多层感知机（Multi-LayerPerceptrons，MLPs）、卷
积神经网络（CNNs）和循环神经网络（RNNs）。更为高级和复杂的网络结构很少用到，除非
对模型微调方面有具体要求或者一些其他特殊情况。一个低维的矢量输入可以用一个多层
感知机处理，而基于视觉的策略经常需要一个卷积神经网络主干来提前提取信息，要么与
强化学习算法一起训练，要么用其他计算机视觉的方法进行预训练。也有其他情况，比如将
低维的矢量输入和高维的图像输入一起使用，实践中通常先采用从高维输入中提取特征的
主干再与其余低维输入并联的方法。循环神经网络可以用于不是完全可观测的环境或者非
马尔可夫过程，最优的动作选择不仅依赖当前状态，而且依赖之前状态。以上是实践中对策
略和价值网络都有效的经验指导。有时策略和价值网络可能构成一种非对称的Actor-Critic
结构，因而它们的状态输入是不同的，这可以用于价值网络只用作训练中策略网络的指导，
而在动作预测时不再可以使用价值网络的情况。
• 熟悉你所用的强化学习算法的性质。举例来说，像PPO或TRPO类的基于信赖域的方法可
能需要较大的批尺寸来保证安全的策略进步。对于这些信赖域方法，我们通常期待策略表
现稳定的进步，而非在学习曲线上某些位置突然有较大下降。TRPO 等信赖域方法需要用
一个较大批尺寸的原因是，它需要用共轭梯度来近似Fisher信息矩阵，这是基于当前采样
到的批量样本计算的。如果批尺寸太小或者是有偏差的，可能对这个近似造成问题，并且
导致对Fisher信息矩阵（或逆Hessian乘积）的近似不准确而使学习表现下降。因此，实践
中，算法 TRPO 和 PPO 中的批尺寸需要被增大，直到智能体有稳定进步的学习表现为止。
436
18.2 实现阶段
所以，TRPO 有时也无法较好地扩展到大规模的网络或较深的卷积神经网络和循环神经网
络上。DDPG算法则通常被认为对超参数敏感，尽管它被证明对许多连续动作空间的任务
很有效。当把它应用到大规模或现实任务(Mahmoodetal.,2018)上时，这个敏感性会更加
显著。比如，尽管在一个简单的模拟测试环境中通过彻底的超参数搜索可以最终找到一个
最优的表现效果，但是在现实世界中的学习过程由于时间和资源上的限制可能不允许这种
超参数搜索，因此 DDPG 相比与其他 TRPO 或 SAC 算法可能不会有很好的效果。另一方
面，尽管DDPG算法起初是设计用来解决有连续值动作的任务，这并不意味着它不能在离
散值动作的情况下工作。如果你尝试将它应用到有离散值动作的任务上，那么需要使用一
些额外的技巧，比如用一个有较大t值的Sigmoid(tx)输出激活函数并且将其修剪成二值化
的输出，还得保证这个截断误差比较小，或者你可以直接使用Gumbel-Softmax技巧来更改
确定性输出为一个类别的输出分布。其他算法也可以有相似处理。
• 归一化值处理。总体来说，你需要通过缩放而不是改变均值来归一化奖励函数值，并且用
同样的方式标准化值函数的预测目标值。奖励函数的缩放基于训练中采样的批样本。只做
值缩放（即除以标准差）而不做均值平移（为得到零均值而减去统计均值）的原因是，均
值平移可能会影响到智能体的存活意愿。这实际上与整个奖励函数的正负号有关，而且这
个结论只适用于你使用“Done”信号的情况。其实，如果你事先没有用“Done”信号来终
止片段，那么，你可以使用均值平移。考虑以下一种情况，如果智能体经历了一个片段，而
“Done=True”信号在最大片段长度以内发生，那么假如我们认为智能体仍旧存活，则这个
“Done”信号之后的奖励值实际为0。如果这些为0的奖励值总体上比之前的奖励值高（即
之前的奖励值基本是负数），那么智能体会倾向于尽可能早地结束片段，以最大化整个片段
内的奖励。相反，如果之前的奖励函数基本是正值，智能体会选择“活”得更久一些。如果
我们对奖励值采取均值平移方式，它会打破以上情形中智能体的存活意愿，从而使得智能
体即使在奖励值基本为正时不会选择存活得更久，而这会影响训练中的表现。归一化值函
数的目标也是相似的情况。举例来说，一些基于DQN的算法的平均Q值会在学习过程中
意外地不断增大，而这是由最大化优化公式中对Q值的过估计造成的。归一化目标Q值可
以缓解这个问题，或者使用其他的技巧如DoubleQ-Learning。
• 一个关于折扣因子的小提示。你可以根据折扣因子γ 对单步动作选择的有效时间范围有一
个大致感觉：1+γ+γ2+···=1/(1−γ)。根据该式，对于γ =0.99，我们经常可以忽略
100个时间步后的奖励。用这个小技巧可以加速你设置参数时的过程。
• Done信号只在终止状态时为真。对于初学者来说，深度强化学习中有一些很容易忽略的细
微差别，而片段式强化学习中的“Done”信号就是其中一个。这些细微的差异可能使得实
践中即使是相同算法的不同实现也会有截然不同的表现。在片段式强化学习中，“Done”信
号被广泛用于结束一个片段，而它是环境状态的一个函数，只要智能体到达终止状态，它
就被设置为真。注意，这里终止状态被定义为指示智能体已经完成片段的情况，要么成功
要么失败，而不是任意一个到达时间限度或最大片段长度的状态。将“Done”信号的值只
437
第18章 深度强化学习应用实践技巧
在状态为终止状态时设为真不是一个平庸的问题。举例来说，如果一个任务是操控机械臂
到达空间中某个具体的位置，这个“Done”信号只应当在机械臂确实到达这个位置时为真，
而不是到达默认片段最大长度等情况下。为了理解这个差异，我们需要知道在强化学习中
有些环境，时间长度是无穷的，有些是有限的，而在采样过程中，算法经常是对有限长度
的轨迹做处理的。有两种常用的实现方式，一是设置最大片段长度，二是使用“Done”信
号作为环境的反馈来通过跳开循环以终止片段。当使用“Done”信号作为采样过程中的中
断点时，它不应当在片段由于到达最大长度的时候设为真，而只应在终止状态到达时为真。
还是前面的例子，若一个机械臂在非目标点的任意其他点由于到达了片段最大长度而结束
了这个轨迹，同时设置了“Done”信号为真，则会对学习过程产生消极影响。具体来说，以
PPO算法为例，从状态S 累计的奖励值被用来估计该状态的价值V(S )，而一个终止状态
t t
的价值为 0。如果在非终止状态时“Done”信号的值为真，那么该状态的值被强制设为 0
了，而实际上它可能不应该为0。这会在价值网络估计之前状态值的时候让其产生混淆，从
而阻碍学习过程。
• 避免数值问题。对于编程实践中的除法，如果使用不当可能会产生无穷大的数值。两个技
巧可以解决这类问题：一个是对正数值的情况使用指数缩放a/b=exp(log(a)−log(b))；另
一个方法是对于非负分母加上一个小量，如a/b≈a/(b+10−6)。
• 注意奖励函数和最终目标之间的分歧。强化学习经常被用于一个有最终目标的具体任务，
而通常需要人为设计一个与最终目标一致的奖励函数来便于智能体学习。在这个意义上说，
奖励函数是目标的一种量化形式，这也意味着它们可能是两个不同的东西。在某些情况下
它们之间会有分歧。因为一个强化学习智能体能够过拟合到你为任务所设置的奖励函数上，
而你可能发现训练最终策略在达成最终目标上与你所期望的不同。这其中一个最可能的原
因是奖励函数和最终目标之间的分歧。在多数情况下，奖励函数倾向于最终的任务目标是
容易的，但是设计一个奖励函数与最终目标在所有极端情况下都始终一致，是不平庸的。你
应该做的是尽可能减少这种分歧，来保证你设计的奖励函数能够平滑地帮助智能体达到最
终真实目标。
• 奖励函数可能不总是对学习表现的最好展示。人们通常在学习过程中展示奖励函数值（有
时用移动平均，有时不用）来表示一个算法的能力。然而，如同上面所说，最终目标和你所
定义的奖励函数之间可能有分歧，这使得一个较高奖励的状态可能对应一个在达成最终目
标方面较差的情况，或者至少没有显式地表现出该状态与最优状态之间的关系。由于这个
原因，我们总需要在使用强化学习和展示结果时考虑这种分歧的可能性。所以，在文献(Fu
etal.,2018)中很常见到，有的学习表现不是用平滑后的片段内奖励（这也依赖奖励函数的
设计）来评估和展示的，而是用一个对这个任务更具体的度量方式，比如图18.1所示的机
器人学习任务中，用机器夹具跟目标点的距离来实现位置到达或用物块跟目标的距离来实
现物块推动。夹具与物体间的距离，或者是否物块被抓取，这些都是对任务目标的真实度
量方式。所以，这些度量可以用来展示任务学习效果，从而更好地体现任务最终目标是否
438
18.2 实现阶段
到达。这对于最终目标跟人为设计的奖励函数有偏差的情况很有用，如果你想比较多个不
同个奖励函数，那么这些额外的度量也很关键。
图18.1 OpenAIGym中的FetchPush环境。对这个环境而言，使用物体到目标位置的最终距离比
奖励函数值能更好地衡量对所学策略的表现，因为它是对任务整体目标的最直接表示。
然而奖励函数可能被设置为包含一些其他因素，如夹具到物体的距离等（见彩插）
• 非马尔可夫情况。如之前章节所述，这本书所介绍的绝大多数理论结果都基于马尔可夫过
程的假设或者状态的马尔可夫性质。马尔可夫性质不仅简化了问题和推导，更重要的是它
使得连续决策问题可以描述，而且可以用迭代的方式解决它，还能得到简洁的解决方法。然
而，实践中，马尔可夫过程的假设不总是成立。举例来说，如图18.2所示，Gym环境中的
Pong游戏就不满足马尔可夫过程在智能体选取最优动作时对状态所做的假设。我们需要记
住马尔可夫性质是状态或环境的性质，因此它是由状态的定义决定的。非马尔可夫决策过
程和部分可观测马尔可夫过程（POMDP）的差异有时是细微的。比如，如果一个在上述游
戏中状态被定义为同时包含小球的位置和速度信息（假设小球运动没有加速度），而观察量
只有位置，那么这个环境是POMDP而不是非马尔可夫过程。然而，Pong游戏的状态通常
被认为是每一个时间步静态帧，那么当前状态只包含小球的位置而没有智能体能够做出最
优动作选择的所有信息，比如，小球速度和小球运动方向也会影响最优动作。所以这种情
况下它是一个非马尔可夫环境。一种提供速度和运动方向信息的方法是使用历史状态，而
这违背了马尔可夫过程下的处理方法。所以，如DQN(Mnihetal.,2015)原文，堆叠帧可以