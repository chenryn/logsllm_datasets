agreed that HVS-based authentication is more convenient and
socially acceptable in the public. HVS-based authentication is
also more secure as it is more resistant to observation-based
attacks than password-based and gesture-based approaches. In
regard to reliability, most subjects consider the password-based
authentication as the best choice. However, we conjecture
that the high reliability score for password-based methods is
from its proven practical usage. Considering the high stability
results reported in Section VIII, the subjects’ reliability score
for OcuLock can be boosted if they are given ﬁrst hand
experiences of our system in a long term.
XI. RELATED WORK
HMD Authentication. Early works of HMD authentication
have been focusing on smart glasses for augmented reality.
Chauhan et al. [6] proposed a gesture-based authentication
system for Google Glass that collects authentication input on
the touchpad. Li et al. [28] developed an head movement
biometric system in response to auditory stimuli. Similarly,
unique head movement was also triggered by a set of pictures
to authenticate smart glasses users [44]. Recently, efforts have
been made towards VR HMD authentication by migrating
traditional authentication methods. Oculus Quest is the ﬁrst
commercial VR HMD equipped with virtual PIN code [36].
George et al. [17] studied the security and usability of au-
thentication methods such as PIN and unlock pattern in VR
HMD through remote controllers. Graphical password [13] and
body motion biometric [40] were also proposed to authenticate
VR users. Although these traditional methods can achieve
acceptable error rate, the entire authentication action is exposed
to the public and can be observed by adversaries to execute
attacks. For example, adversaries managed to observe the
authentication action and either mimic the owner’s behavior
for impersonation attacks or analyze the behavior for side-
channel attacks [30], [14]. Although extracting brain signals
for VR HMD authentication is relatively secure [29], collecting
brainwaves requires a device covering the majority of the
scalp via a large number of electrodes. Such a setup is too
cumbersome for practical use and is not compatible with the
form factor of today’s VR HMDs that only has one holding
strap [37], [27]. In this paper, we utilize the fact that VR HMD
fully covers users’ eye area to propose an HVS-based biometric
for unobservable authentication. Since the foam face cover has
direct contact to skin around eye sockets, we design a usable
13
B. Computation Time
To guarantee the usability of an authentication system,
computation time is one of the major concerns. The total
computation time of OcuLock consists of three parts: the EOG
recording time when users view the stimuli, the signal process-
ing time when the EOG is transformed, and the authentication
time when features are extracted, the comparison algorithm
is run and the comparator model is executed. According to
our measurement, the signal processing takes less than 1 ms
and the authentication takes an average of 39 ms. The EOG
recording time ranges from 3 seconds to 10 seconds as shown
in the experiment results. We can see that the total computation
time is dominated by the EOG recording time while other time
components are negligible because we use efﬁcient algorithms
to design our system. We even reuse several
intermediate
results, e.g., reusing wavelet transform results for sympathetic
energy.
It is true that authentication systems in a physical world
usually take less time (1-2 seconds) than OcuLock. However,
VR interaction is generally slower since it relies on head
and/or eye navigation in a virtual world, which is harder
than physical-world interaction. User studies showed that the
simplest authentication such as PIN code or unlock patterns
takes around 3 seconds in VR environment [17]. Therefore,
users generally have lower expectation on VR authentication
and thus we believe the 3-second authentication time of
OcuLock is acceptable. Depending on the tradeoff between
authentication error and computation time, users can select a
proper EOG recording time for OcuLock.
The memory consumption for the entire authentication
is on average 54 MB, which is acceptable in modern VR
computing devices.
C. Electrode Placement
In our prototype, we applied conductive gel inside each
electrode to help measure EOG signals. However, the gel does
not need to be replaced frequently (once every 30 minutes).
For future real-world systems, dry electrodes can be used for
EOG collection to enhance system usability. This technique
has been used by JINS MEME, a commercial smart glasses
device [32].
After ﬁnishing each session in our experiment, the elec-
trodes were taken off from one subject and attached to another.
We point out that it is not this replacement of electrodes that
results in the different EOG samples between subjects. In the
majority of the evaluation, electrodes were ﬁxed at the HMD
cover and thus their positions for different participants were
the same (see Figure 5). Even though there is minor placement
difference in some cases, we found that EOG measurement is
not sensitive to that. In our temporal study (Section VIII-D),
the electrodes were detached and attached to the HMD repeat-
edly with around 1cm of position change. However, the system
could still recognize users although electrode positions are not
the same, which proves the negligible effects of electrodes
position.
XIII. CONCLUSION
In this paper, we present OcuLock, a stable and unobserv-
able system to authenticate users for VR HMD. Compared
with eye gaze based systems, we explore HVS as a whole
and extract low-level physiological and behavioral features
for biometric authentication. OcuLock is resistant to common
and anticipated types of attacks such as impersonation and
statistical attacks with EERs of 3.55% and 4.97% respectively.
Thanks to the stable physiological features, OcuLock is less
variable over time and reduces the frequency of updating
EOG template. Our user study suggests promising potential
for HVS-based authentication in which the requirement of
convenience, security and social comfort can simultaneously
be satisﬁed. Future work should focus on integrate the devices
in our prototype into a uniﬁed VR HMD for more practical
and larger-scale user study.
ACKNOWLEDGMENT
This work was supported in part by National Science
Foundation Grant CCF-1852516, CNS-1718375, and National
Natural Science Foundation of China Grant 61972348.
REFERENCES
[1] M. Abo-Zahhad, S. M. Ahmed, and S. N. Abbas, “A novel biometric
approach for human identiﬁcation and veriﬁcation using eye blinking
signal,” IEEE Signal Processing Letters, vol. 22, no. 7, pp. 876–880,
2014.
[2] R. Barea, L. Boquete, M. Mazo, and E. L´opez, “System for assisted
mobility using eye movements based on electrooculography,” IEEE
transactions on neural systems and rehabilitation engineering, vol. 10,
no. 4, pp. 209–218, 2002.
[3] R. Bednarik, T. Kinnunen, A. Mihaila, and P. Fr¨anti, “Eye-movements
as a biometric,” in Scandinavian conference on image analysis.
Springer, 2005, pp. 780–789.
[4] M. Brown, M. Marmor, E. Zrenner, M. Brigell, M. Bach et al., “Is-
cev standard for clinical electro-oculography (eog) 2006,” Documenta
ophthalmologica, vol. 113, no. 3, pp. 205–212, 2006.
[6]
[5] A. Bulling, J. A. Ward, H. Gellersen, and G. Troster, “Eye movement
analysis for activity recognition using electrooculography,” IEEE trans-
actions on pattern analysis and machine intelligence, vol. 33, no. 4, pp.
741–753, 2010.
J. Chauhan, H. J. Asghar, A. Mahanti, and M. A. Kaafar, “Gesture-
based continuous authentication for wearable devices: The smart glasses
use case,” in International Conference on Applied Cryptography and
Network Security. Springer, 2016, pp. 648–665.
[7] Y. Chen and W. S. Newman, “A human-robot
interface based on
electrooculography,” in IEEE International Conference on Robotics and
Automation, 2004. Proceedings. ICRA’04. 2004, vol. 1.
IEEE, 2004,
pp. 243–248.
[8] V. Di Lollo, J.-i. Kawahara, S. S. Ghorashi, and J. T. Enns, “The
attentional blink: Resource depletion or temporary loss of control?”
Psychological research, vol. 69, no. 3, pp. 191–200, 2005.
[9] C. Ding and H. Peng, “Minimum redundancy feature selection from
microarray gene expression data,” Journal of bioinformatics and com-
putational biology, vol. 3, no. 02, pp. 185–205, 2005.
[10] Q. Ding, K. Tong, and G. Li, “Development of an eog (electro-
oculography) based human-computer interface,” in 2005 IEEE Engi-
neering in Medicine and Biology 27th Annual Conference.
IEEE,
2006, pp. 6829–6831.
[11] A. T. Duchowski, “Eye tracking methodology,” Theory and practice,
vol. 328, no. 614, pp. 2–3, 2007.
[12] S. Eberz, K. Rasmussen, V. Lenders, and I. Martinovic, “Preventing
lunchtime attacks: Fighting insider threats with eye movement bio-
metrics,” in Proceedings of Network and Distributed System Security
Symposium. NDSS, 2015.
[13] M. Funk, K. Marky, I. Mizutani, M. Kritzler, S. Mayer, and F. Micha-
helles, “Lookunlock: Using spatial-targets for user-authentication on
hmds,” in CHI Conference on Human Factors in Computing Systems
Late Breaking Work, 2019.
14
[14] D. Gafurov, E. Snekkenes, and P. Bours, “Spoof attacks on gait
authentication system,” IEEE Transactions on Information Forensics
and Security, vol. 2, no. 3, pp. 491–502, 2007.
[15] C. Galdi, M. Nappi, D. Riccio, and H. Wechsler, “Eye movement anal-
ysis for human authentication: a critical survey,” Pattern Recognition
Letters, vol. 84, pp. 272–283, 2016.
[16] N. Garun, “Amazon will soon refund up to $70 million of in-app
purchases made by children,” 2019, http://www.theverge.com/2017/4/
4/15183254/amazon-ends-appeal-refund-70-million-in-app-purchases.
[17] C. George, M. Khamis, E. von Zezschwitz, M. Burger, H. Schmidt,
F. Alt, and H. Hussmann, “Seamless and secure vr: Adapting and
evaluating established authentication systems for virtual reality,” in
Proceedings of Network and Distributed System Security Symposium.
NDSS, 2017.
[18] Google, “Introducing daydream standalone vr headsets,” https://vr.
google.com/daydream/standalonevr/.
[19] C. Holland and O. V. Komogortsev, “Biometric identiﬁcation via eye
movement scanpaths in reading,” in 2011 International joint conference
on biometrics (IJCB).
IEEE, 2011, pp. 1–8.
[20] C. D. Holland and O. V. Komogortsev, “Complex eye movement pattern
biometrics: Analyzing ﬁxations and saccades,” in 2013 International
conference on biometrics (ICB).
IEEE, 2013, pp. 1–8.
[21] M. Joukal, Anatomy of the Human Visual Pathway. Springer, 04 2017,
pp. 1–16.
[22] M. Juhola, Y. Zhang, and J. Rasku, “Biometric veriﬁcation of a subject
through eye movements,” Computers in biology and medicine, vol. 43,
no. 1, pp. 42–50, 2013.
[23] T. Kinnunen, F. Sedlak, and R. Bednarik, “Towards task-independent
person authentication using eye movement signals,” in Proceedings of
the 2010 Symposium on Eye-Tracking Research & Applications. ACM,
2010, pp. 187–190.
[24] C. Krapichler, M. Haubner, R. Engelbrecht, and K.-H. Englmeier, “Vr
imaging applications,” Computer
interaction techniques for medical
methods and programs in biomedicine, vol. 56, no. 1, pp. 65–74, 1998.
[25] D. Kumar and E. Poole, “Classiﬁcation of eog for human computer
interface,” in Proceedings of the Second Joint 24th Annual Conference
and the Annual Fall Meeting of the Biomedical Engineering Soci-
ety][Engineering in Medicine and Biology, vol. 1.
IEEE, 2002, pp.
64–67.
[26] T. B. Kuo and C. C. Yang, “Frequency domain analysis of elec-
trooculogram and its correlation with cardiac sympathetic function,”
Experimental neurology, vol. 217, no. 1, pp. 38–45, 2009.
[27] Lenovo, “Mirage solo with daydream,” https://www.lenovo.com/us/en/
daydreamvr/.
[28] S. Li, A. Ashok, Y. Zhang, C. Xu, J. Lindqvist, and M. Gruteser,
“Whose move is it anyway? authenticating smart wearable devices
using unique head movement patterns,” in 2016 IEEE International
Conference on Pervasive Computing and Communications (PerCom).
IEEE, 2016, pp. 1–9.
[29] F. Lin, K. W. Cho, C. Song, W. Xu, and Z. Jin, “Brain password: A
secure and truly cancelable brain biometrics for smart headwear,” in
Proceedings of the 16th Annual International Conference on Mobile
Systems, Applications, and Services. ACM, 2018, pp. 296–309.
[30] Z. Ling, Z. Li, C. Chen, J. Luo, W. Yu, and X. Fu, “I know what you
enter on gear vr,” in 2019 IEEE Conference on Communications and
Network Security (CNS), June 2019, pp. 241–249.
[31] B. R. Manor and E. Gordon, “Deﬁning the temporal threshold for ocular
ﬁxation in free-viewing visuocognitive tasks,” Journal of neuroscience
methods, vol. 128, no. 1-2, pp. 85–93, 2003.
J. MEME, “Jine meme eye sensing,” 2019, https://jins-meme.com/en/
products/es/.
[32]
[33] A. Nguyen, Z. Yan, and K. Nahrstedt, “Your attention is unique:
Detecting 360-degree video saliency in head-mounted display for head
movement prediction,” in 2018 ACM Multimedia Conference on Multi-
media Conference. ACM, 2018, pp. 1190–1198.
[34] C. Nickel, T. Wirtl, and C. Busch, “Authentication of smartphone
users based on the way they walk using k-nn algorithm,” in 2012
Eighth International Conference on Intelligent Information Hiding and
Multimedia Signal Processing.
IEEE, 2012, pp. 16–20.
[35] R. F. Nogueira, R. de Alencar Lotufo, and R. C. Machado, “Fingerprint
liveness detection using convolutional neural networks,” IEEE transac-
tions on information forensics and security, vol. 11, no. 6, pp. 1206–
1213, 2016.
[36] Oculus, “Getting started with your oculus quest,” https://support.oculus.
com/855551644803876/.
[37] ——, “Our ﬁrst all-in-one gaming headset,” https://www.oculus.com/
quest/.
[38] ——, “Rift store: VR games, apps & more,” https://www.oculus.com/
experiences/rift/.
[39] K. Pfeil, E. M. Taranta II, A. Kulshreshth, P. Wisniewski, and J. J.
LaViola Jr, “A comparison of eye-head coordination between virtual
and physical realities,” in Proceedings of the 15th ACM Symposium on
Applied Perception, 2018, p. 18.
[40] K. Pfeuffer, M. J. Geiger, S. Prange, L. Mecke, D. Buschek, and F. Alt,
“Behavioural biometrics in vr: Identifying people from body motion and
relations in virtual reality,” in Proceedings of the 2019 CHI Conference
on Human Factors in Computing Systems. ACM, 2019, p. 110.
[41] M. Porta, S. Ricotti, and C. J. Perez, “Emotional e-learning through
eye tracking,” in Proceedings of the 2012 IEEE Global Engineering
Education Conference (EDUCON).
IEEE, 2012, pp. 1–6.
[42] P. Qvarfordt and S. Zhai, “Conversing with the user based on eye-gaze
patterns,” in Proceedings of the SIGCHI conference on Human factors
in computing systems. ACM, 2005, pp. 221–230.
[43] H. L. Ramkumar, “Electrooculogram,” 2019, https://eyewiki.aao.org/
Electrooculogram#Testing process.
[44] C. E. Rogers, A. W. Witt, A. D. Solomon, and K. K. Venkatasubrama-
nian, “An approach for user identiﬁcation for head-mounted displays,”
in Proceedings of the 2015 ACM International Symposium on Wearable
Computers. ACM, 2015, pp. 143–146.
I. Sluganovic, M. Roeschlin, K. B. Rasmussen, and I. Martinovic, “Us-
ing reﬂexive eye movements for fast challenge-response authentication,”
in Proceedings of the 2016 ACM SIGSAC Conference on Computer and
Communications Security. ACM, 2016, pp. 1056–1067.
[45]
[46] V. R. Society, “Virtual reality air force training,” https://www.vrs.org.
uk/virtual-reality-military/air-force-training.html.
[47] C. Song, A. Wang, K. Ren, and W. Xu, “Eyeveri: A secure and usable
approach for smartphone user authentication,” in IEEE INFOCOM
2016-The 35th Annual IEEE International Conference on Computer
Communications.
IEEE, 2016, pp. 1–9.
[48] Y. Song, Z. Cai, and Z.-L. Zhang, “Multi-touch authentication using
hand geometry and behavioral information,” in 2017 IEEE Symposium
on Security and Privacy (SP).
IEEE, 2017, pp. 357–372.
[49] R. Steinberg, R. Linsenmeier, and E. Griff, “Retinal pigment epithe-
lial cell contributions to the electroretinogram and electrooculogram,”
Progress in retinal research, vol. 4, pp. 33–66, 1985.
[50] H. Steiner, S. Sporrer, A. Kolb, and N. Jung, “Design of an active mul-
tispectral swir camera system for skin detection and face veriﬁcation,”
Journal of Sensors, 2016.
[51] Viar360, “Virtual reality market size in 2018 with forecast for 2019,”
2019, https://www.viar360.com/virtual-reality-market-size-2018/.
[52] A. Walker, “Potential security threats with virtual reality technology,”
2017, https://learn.g2.com/security-threats-virtual-reality-technology.
J.-G. Wang and E. Sung, “Study on eye gaze estimation,” IEEE
Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics),
vol. 32, no. 3, pp. 332–350, 2002.
[53]
[55]
[54] Wikipedia, “Virtual
reality therapy,” https://en.wikipedia.org/wiki/
Virtual reality therapy.
J. Yi, S. Luo, and Z. Yan, “A measurement study of youtube 360
live video streaming,” in Proceedings of the 29th ACM Workshop on
Network and Operating Systems Support for Digital Audio and Video.
ACM, 2019, pp. 49–54.
[56] Y. Zhang, W. Hu, W. Xu, C. T. Chou, and J. Hu, “Continuous
authentication using eye movement response of implicit visual stim-
uli,” Proceedings of the ACM on Interactive, Mobile, Wearable and
Ubiquitous Technologies, vol. 1, no. 4, p. 177, 2018.
15