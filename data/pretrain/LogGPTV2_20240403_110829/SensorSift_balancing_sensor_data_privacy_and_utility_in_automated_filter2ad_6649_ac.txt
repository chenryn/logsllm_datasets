choices for our analysis, however we wanted to also get coverage
over other characteristics which might be interesting from a privacy
perspective. To this end we chose race (‘White’), age (‘Youth’), and
emotional indicators(‘Smiling’, ‘Frowning’), as well as other at-
tributes which were descriptive about distinct regions of the face
(‘No Eyewear,’ ‘Obstructed Forehead,’ ‘No Beard’). Lastly we
chose the ‘Outdoors’ attribute as it provides environmental context
and it brings our total up to 10.
Policies. Having chosen a base set of 10 attributes we set out
to evaluate how different choices of public and private attributes
153
would impact our goal of balancing utility with privacy. To this
end we created 90 simple policies composed of all possible combi-
nations of a single public and a single private attribute (e.g., pub-
lic:‘Male’, private:‘Smiling’)1.
6.2 Deﬁning Mask Performance: PubLoss and
PrivLoss
As previously stated, our system aims to produce data trans-
formations which provide a favorable balance between utility and
privacy given a policy instance P, dataset X, and attribute labels
Y . Building on these concepts, we now introduce the quantitative
measurements PubLoss and PrivLoss which judge the utility and
privacy [respectively] achieved for sifts of a speciﬁed dimension
within a given policy. PubLoss is intended to measure how much
classiﬁcation accuracy is sacriﬁced when public attributes are sifted
(relative to their raw, unsifted versions), while PrivLoss is the dif-
ference between the highest classiﬁcation rate of sifted private at-
tributes relative to blind guessing.
• PubLoss: Decrease in F sifted public attribute classiﬁcation
accuracy relative to the achievable accuracy using raw (un-
sifted) data, computed as:
PubLoss = MLm(X,Y +)− MLm(FY +,Y − (X,K),Y +)
• PrivLoss: F sifted private attribute classiﬁcation accuracy
relative to chance, computed as:
PrivLoss = MLm(FY +,Y − (X,K),Y
−)− .5
Where MLm(X,Y ) denotes the Class Avg. Accuracy (Section 6.3)
computed via classiﬁer m using a 50%-50% split of training vs test-
ing instances given data samples X with ground truth labels Y ; and,
Fc,d(X,K) indicates the K dimensional privacy sift computed using
data samples X and public and private labels Y + and Y−.
A poor quality F would yield transformed samples whose public
attributes are unintelligible and whose private attributes are eas-
ily identiﬁed (high PubLoss and PrivLoss). Conversely, an ideal
sifting transformation would have no impact on the raw classiﬁca-
tion rates of public attributes while completely obscuring private
attributes (no PubLoss and PrivLoss).
6.3 Classiﬁcation Measures
The performance criteria we have selected (PubLoss and PrivLoss)
are heavily dependent on measures of classiﬁcation accuracy. Thus
to provide stronger privacy claims, we now describe a robust ap-
proach to computing classiﬁcation accuracy.
Class Average Accuracy. A common method of reporting clas-
siﬁcation accuracy is based on the notion of aggregate accuracy
shown in Eq (4). Although this metric is suitable to many problem
instances, whenever attributes have unequal distributions of pos-
itive vs negative samples (e.g., 78% of faces in our dataset lack
eyewear) classiﬁers can achieve high aggregate accuracy scores by
exploiting the underlying statistics (and always guessing ‘no eye-
wear’) rather than learning a decision boundary from training data.
To avoid scores which mask poor classiﬁer performance and warp
our PubLoss and PrivLoss measures we opt to use Class Avg. Ac-
curacy which is a more revealing gauge of classiﬁcation success
and is calculated as in Eq (5):
1We did not consider policies where the same attribute is both pub-
lic and private
154
Table 2: Achievable accuracy for each attribute using raw data fea-
tures computed using the maximum classiﬁcation score across our ﬁve
classiﬁers. Columns one and two use the aggregate accuracy metric and
respectively represent our attribute recognition scores and state of the
art performance (ICCV09 accuracies are reported from [11]). The re-
maining column provides the Class Average Accuracy measure.
Attribute
Male
Attr. Female
White
Youth
Smiling
Frowning
No Eywear
Obst. Forehead
No Beard
Outdoor
ICCV09 Agg. Accuracy Class Avg. Accuracy
81.22
81.13
91.48
85.79
95.33
95.47
93.55
79.11
89.53
92.86
84.26
86.97
79.97
87.69
85.35
82.86
77.86
86.13
84.83
94.18
87.33
88.07
83.27
92.11
89.98
87.01
81.01
88.60
88.18
–
AggregateAccuracy = (tP +tN)/tS
(4)
ClassAvgAccuracy = (tP/(tP + f P) +tN/(tN + f N))/2
(5)
Where tP is the number of True Positives (correct identiﬁcations),
f P is the number of False Positives (type 1 errors), tN is the number
of True Negatives samples (correct identiﬁcations), f N is the num-
ber of False Positive samples (type 2 errors), and tS is the number
of Total samples (tP + f P +tN + f N).
As can be seen from equation (5) above, Class Avg. Accuracy
places equal weight on correctly identifying attribute presence (pos-
itive hit rate) and attribute absence (negative hit rate) which in
turn emphasizes classiﬁer precision and offers less sensitivity to
attributes with imbalanced ratios of positive to negative data.
6.4 Achievable Accuracies
Achievable Accuracy is a term we use to refer to the correct
classiﬁcation rates that we were able to obtain using the PubFig
dataset. As mentioned in Section 6.1, images in the PubFig dataset
are annotated with 74 numerical judgments produced by a set of
74 machine classiﬁers each trained to recognize a unique attribute.
These scores are positive whenever the classiﬁer has determined
that an attribute is present and negative if the attribute is deemed to
be absent (higher absolute values indicate additional conﬁdence)2.
To produce these numbers each attribute classiﬁer was trained us-
ing 2000 hand labeled (ground truth) samples produced using Me-
chanical Turk [11]. Unfortunately due to the liability policy of
Columbia University these ground truth labels cannot be released,
instead we treat the classiﬁer outputs as a proxy ground truth.
In the ﬁrst two columns of Table 2, we use the aggregate ac-
curacy metric to compare attribute recognition performance of our
classiﬁers against state of the art methods. The third column pro-
vides the more robust Class Average Accuracy measure which
we’ll be using as the basis for result discussions. Note that all of
the results in Table 2 are computed raw [unsifted] data features.
In the ﬁrst column of Table 2 we report the correct classiﬁca-
tion rates of our 10 attributes from the original PubFig publication.
These scores are based on the notion of aggregate accuracy shown
in in Eq (4).
In the second column of Table 2 we also use the
aggregate accuracy method however we now apply classiﬁcation
2Each scores indicates the distance of a sample from the SVM sep-
aration hyperplane
models which we train using the features described in Section 5.
This serves as a veriﬁcation that we are able to match state of the
art results (in fact outperform for the ﬁrst two attributes). In the last
column we report the more robust classiﬁcation measure - class
average accuracy - which we use as a reference for the PubLoss
computations for the remainder of the paper.
When looking at these accuracy rates it is important to note that
the results could be improved with additional data, access to ground
truth labels, and novel computer vision features. However we are
not seeking maximal identiﬁcation accuracy; instead the achievable
accuracy serves as a reference point, and we are interested in how
our sifting methods operate around it.
7. RESULTS
Below we describe the results of our experiments on the PubFig
dataset. First we set a conservative privacy threshold and deter-
mine the sift output dimensionality that meets this criteria when
measured against our ensemble of classiﬁers. Next we look at the
PubLoss and PrivLoss computed from the 90 policies using one
public and one private attribute, and describe the factors inﬂuencing
the results. We follow this with an extension of our algorithm suited
to complex policies (multiple public and/or private attributes). We
also discuss how our approach can be applied to sequential sensor
samples (i.e. video) and provide a details from a case study. Lastly
we compare our approach to the closest method in the literature.
Sift Dimensionality and Multiple Classiﬁers. Recall that the
output of our system is a transformation which can be applied to
any input feature vector (i.e., face image) to produce a sifted out-
put intended to uphold a given policy. Our results indicate that
the average (across all policies) PubLoss monotonically decreases
while the average PrivLoss monotonically increases as the number
of sift dimensions exposed to classiﬁers grows. This is reasonable
since very low dimensional sifts do not carry enough information
to classify public attributes while high dimensional sifts provide an
increased risk of information leakage.
In our evaluation we adopt a conservative threshold, and set the
acceptable PrivLoss to inferences that are 10% better than chance
(i.e., maximal allowed private classiﬁcation accuracy is 60%). Given
this constrain we ﬁnd that an output sift dimensionality of K = 5,
and λ = 1 yield the best average tradeoffs across policies (with one
public and one private attribute accross all tested classiﬁers). Figure
3 provides examples of our system’s output for two policies (which
use the same attributes in exchanged public/private order) in which
classiﬁcation accuracy is shown as a function of sift dimensionality.
From an adversarial standpoint, the output of our system repre-
sents an ‘un-sifting’ challenge which can be tackled with any avail-
able tool(s). In general we ﬁnd that for low dimensional sifts, clas-
siﬁer accuracies are similar despite differences in the algorithmic
machinery used for inference; however as the sift dimensionality
grows the classiﬁers increasingly differ in performance — when
we look across classiﬁers using the 90 simple policy combinations
possible with one public and one private attribute, we ﬁnd that 5
dimensional sifts have an avg. public attribute accuracy standard
deviation of 3.86% and an avg. private attribute accuracy standard
deviation of 3.77%; whereas 15 dimensional sifts have signiﬁcantly
larger deviations as avg. public attribute accuracy standard devia-
tion is 8.25% and avg. private attribute accuracy standard devia-
tion is 14.16%. Another interesting observation is that the linear-
SVM and kernel-SVM classiﬁers consistently produced the lowest
PubLoss while the linear-SVM and randomForest classiﬁers pro-
duced the highest PrivLoss. The high performance of linear-SVM
is not surprising given the linear nature of our PPLS algorithm.
Policy Results. We evaluated sifts created for each of our 90 poli-
cies (10 attributes paired with all others, excluding self matches)
using each of our 5 classiﬁcation methods. For each policy, we re-
port the lowest PubLoss and highest PrivLoss obtained across all
5 classiﬁers in Figure 4. In these matrices, the attribute enumera-
tion used in the rows and columns is: (1) Male - M, (2) Attractive
Female - AF, (3) White - W, (4) Youth - Y, (5) Smiling - S, (6)
Frowning - F, (7) No Eyewear - nE, (8) Obstructed Forehead - OF,
(9) No Beard - nB, and (10) Outdoors - O. Recall that the PubLoss
results are relative to the achievable accuracies reported in the third
column of Table 2.
Our results indicate that we can create sifts that provide strong
privacy and minimize utility losses at (K = 5 dimensions) for the
majority of policies we tested (average PubLoss = 6.49 and PrivLoss
= 5.17). This is a signiﬁcant ﬁnding which highlights the potential
of policy driven privacy and utility balance in sensor contexts!
Performance Impacting Factors Based on our analysis we ﬁnd
that the PPLS algorithm is able to produce high performing sifts
as long as there are not signiﬁcant statistical interactions between
the public and private attributes. This is to be expected given the
structure of the problem we are trying to solve.
In the extreme
case, if we consider a policy which includes the same attribute in
its public and private set it seems obvious that any privacy enforc-
ing algorithm will have a hard time balancing between utility and
privacy since obscuring the private attribute prevents recognition of
the [same] public attribute.
To formalize the intuition above we use two quantitative mea-
sures to capture the levels of statistical interactions in policies: cor-
relation and overlap. Correlation is the traditional statistical mea-
sure of the probabilistic dependence between two random variables
(in our case attributes). Overlap is a metric we introduce to describe
the degree to which two attributes occupy the same regions in fea-
ture space. Overlap is computed as in equation (6) and normalized
to 1 across our 90 policies. The Correlation and Overlap matrices
in Figure 4 show the correlation and overlap for each attribute pair
in our tested policies.
(cid:2)
f ind∑∑ max
s.t. w(cid:2)w = 1
w
cov(Xw,Y +)2 ∗ cov(Xw,Y
−)2
(cid:3)
(6)
To help illustrate correlation and overlap we provide a set of ex-
amples from our analysis. Consider the attributes Male and No
Beard. These attributes are highly correlated (r = −.72). Male
and Attractive Female are another highly correlated attribute pair
(r = −.66). Using our domain knowledge we can reason about
these numerical dependencies as follows: if you know about the
presence of facial hair (i.e., No Beard is false) then Maleness is
easily predicted, similarly if an individual is an ttractive Female it
is highly unlikely that they are Male.
Although correlations provide a key insight into the interactions
between attributes a deeper level of understanding is obtained by
investigating overlap. Returning to our examples, Male and No
Beard have an overlap (.29) which is less than half of the overlap
of Male and Attractive Female (.72). The reason for this is that No
Beard is a relatively localized attribute (i.e., pixels around around
the mouth/chin) and does not depend on features in many of the
regions used to determine Male-ness. Conversely, Attractive Fe-
male and Male have high overlap because they are determined us-
ing many of the same feature regions (i.e., eyebrows, nose, bangs)
as can be seen in Figure 5.
155
Figure 3: Left: PubLoss and PrivLoss performance (classiﬁcation accuracy) as a function of sift dimensionality for two simple policies. Right:
PubLoss and PrivLoss performance for complex policies. In all ﬁgures, the lowest PubLoss and highest PrivLoss is reported across all ﬁve classiﬁers.
Dashed lines represent the maximum achievable accuracies using raw (unsifted) data which serve as upper bounds for PubLoss performance.
Figure 4: PubLoss, PrivLoss, Correlation, and Overlap matrices for our 90 simple policy combinations. Rows denote the public attribute, columns
represent the private attribute, while cells represent policies which combine the row and column attributes. In the case of PubLoss and PrivLoss lower
values are desirable as they indicate minimal utility and privacy sacriﬁces respectively. Correlation values are shown using absolute values and higher
cell values indicate signiﬁcant information linkages between attributes. Lastly, high Overlap values indicate that attributes occupy the same regions
in feature space.
Intuitively highly correlated attributes with signiﬁcant overlap
should prevent utility and privacy balance. This is indeed what we
see when we match up the results of the PubLoss and PrivLoss
matrices with the correlation and overlap matrices (Figure 4).
Figure 5: The image features (from the red component of the raw
RGB values) which most strongly covary with several attributes (red
values indicate strong positive correlations, blue values indicate strong