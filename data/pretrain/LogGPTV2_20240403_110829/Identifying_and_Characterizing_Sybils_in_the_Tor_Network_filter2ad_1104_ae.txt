ering that (i) there are 162 gaps in the archived data, that
(ii) we created time series for joining and leaving relays,
and that (iii) we determined churn values for all twelve
relay ﬂags, we ended up with (72,061− 162)· 2· 12 =
1,725,576 churn values. Figure 9 shows a box plot for
the churn distribution (joining and leaving churn values
concatenated) for the seven most relevant relay ﬂags. We
removed values greater than the plot whiskers (which
extend to values 1.5 times the interquartile range from
the box) to better visualize the width of the distribu-
tions. Unsurprisingly, relays with the Guard, HSDir, and
Stable ﬂag experience the least churn, probably because
relays are only awarded these ﬂags if they are particu-
larly stable. Exit relays have the most churn, which is
surprising given that exit relays are particularly sensitive
to operate.
Interestingly, the median churn rate of the
network has steadily decreased over the years, from 0.04
in 2008 to 0.02 in 2015.
Figure 10 illustrates churn rates for ﬁve days in Au-
gust 2008, featuring the most signiﬁcant anomaly in our
data. On August 19, 822 relays left the network, result-
ing in a sudden spike, and a baseline shift. The spike
was caused by the Tor network’s switch from consensus
format version three to four. The changelog says that in
version four, routers that do not have the Running ﬂag
are no longer listed in the consensus.
To alleviate the choice of a detection threshold, we
plot the number of alerts (in log scale) in 2015 as the
threshold increases. We calculate these numbers for
three simple moving average window sizes. The result
is shown in Figure 11. Depending on the window size,
thresholds greater than 0.012 seem practical considering
that 181 alerts per year average to approximately one
alert in two days—a tolerable number of incidents to in-
vestigate. Unfortunately, we are unable to determine the
false positive rate because we do not have ground truth.
USENIX Association  
25th USENIX Security Symposium  1179
11
ExitV2DirFastValidGuardHSDirStable0.000.100.20Relay ﬂagsChurn rateMethod
Fingerprint
Churn
Uptimes
Exitmap
Analysis window Threshold Total alerts Alerts per week
1.3
10/2007–01/2016
1.9
01/2015–01/2016
01/2009–01/2016
8.3
3.2
08/2014–01/2016
551
110
3,052
251
10
0.017
5
—
Table 3: The number of alerts our methods raised. We used different analysis windows for representative results, and
chose conservative thresholds to keep the number of alerts per week manageable.
Figure 10: In August 2008, an upgrade in Tor’s consen-
sus format caused the biggest anomaly in our dataset.
The positive time series represents relays that joined and
the negative one represents relays that left.
Figure 12: In June 2010, a researcher started several hun-
dred Tor relays on PlanetLab [20]. The image shows the
uptime of 2,000 relays for all of June.
Figure 11: The number of alerts (in log scale) in 2015
as the detection threshold increases, for three smoothing
window sizes.
5.4 Uptime analysis
We generated relay uptime visualizations for each month
since 2007, resulting in 100 images. We now discuss
a subset of these images, those containing particularly
interesting patterns.
Figure 12 shows June 2010, featuring a clear “Sybil
block” in the center. The Sybils belonged to a researcher
who, as documented by The Tor Project [20], started
several hundred Tor relays on PlanetLab for research
on scalability (the “PlanetLab” Sybils discussed above).
Our manual analysis could verify this. The relays were
easy to identify because their nicknames suggested that
they were hosted on PlanetLab, containing strings such
as “planetlab,” “planet,” and “plab.” Note the small
Figure 13: August 2012 featured a curious “step pattern,”
caused by approximately 100 Sybils. The image shows
the uptime of 2,000 relays for all of August.
height of the Sybil block, indicating that the relays were
only online for a short time.
Figure 13 features a curious “step pattern” for approx-
imately 100 relays, all of which were located in Russia
and Germany. The relays appeared in December 2011,
and started exhibiting the diurnal step pattern (nine hours
uptime followed by ﬁfteen hours downtime) in March
2012. All relays had similar nicknames, consisting of
eight seemingly randomly-generated characters. In April
2013, the relays ﬁnally disappeared.
Figure 14 illustrates the largest Sybil group to date,
comprising 4,615 Tor relays (the “LizardNSA” Sybils
discussed above). An attacker set up these relays in the
Google cloud in December 2014. Because of its magni-
tude, the attack was spotted almost instantly, and The Tor
Project removed the offending relays only ten hours after
they appeared.
1180  25th USENIX Security Symposium 
USENIX Association
12
-0.40.00.4TimeChurn rateAug 16Aug 17Aug 18Aug 19Aug 20Aug 210.0100.0140.0180.022201001000ThresholdAlerts (log)1 hour12 hours24 hoursUSENIX Association  
25th USENIX Security Symposium  1181
Figure14:InDecember2014,anattackerstartedsev-eralthousandTorrelaysintheGooglecloud.Theimageshowstheuptimeof4,000relaysforallofDecember.0200400600800IP addresses (0.03 percentile)Observed ﬁngerprints10502001,000Figure15:Thenumberofobservedﬁngerprintsforthe1,000relaysthatchangedtheirﬁngerprintsthemost.5.5FingerprintanomaliesWedeterminedhowoftenallTorrelayschangedtheirﬁngerprintfrom2007to2015.Figure15illustratesthenumberofﬁngerprints(yaxis)wehaveobservedforthe1,000Torrelays(xaxis)thatchangedtheirﬁngerprintthemost.Alltheserelayschangedtheirﬁngerprintatleasttentimes.Twenty-onerelayschangedtheirﬁngerprintmorethan100times,andtherelayattheveryrightendofthedistributionchangeditsﬁngerprint936times.Thisrelay’snicknamewas“openwrt,”suggestingthatitwasahomerouterthatwasrebootedregularly,presumablylosingitslong-termkeysintheprocess.TherelaywasrunningfromAugust2010toDecember2010.Figure15furthercontainsapeculiarplateau,shownintheshadedareabetweenindex707and803.ThisplateauwascausedbyagroupofSybils,hostedinAma-zonEC2,thatchangedtheirﬁngerprintexactly24times(the“AmazonEC2”Sybilsdiscussedabove).Uponin-spection,wenoticedthatthiswaslikelyanexperimentforaSecurityandPrivacy2013paperondeanonymizingToronionservices[4,§V].WealsofoundthatmanyIPaddressesinthenetblock199.254.238.0/24frequentlychangedtheirﬁngerprint.WecontactedtheowneroftheaddressblockandweretoldthattheblockusedtohostVPNservices.Appar-ently,severalpeoplestartedTorrelaysandsincetheVPNservicewouldnotassignpermanentIPaddresses,theTorrelayswouldperiodicallychangetheiraddress,causingthechurnweobserve.5.6Accuracyofnearest-neighborrankingGivenaSybilrelay,howgoodisournearest-neighborrankingatﬁndingtheremainingSybils?Toanswerthisquestion,wenowevaluateouralgorithm’saccuracy,whichwedeﬁneasthefractionofneighborsitcorrectlylabelsasSybils.Forexample,ifeightoutoftenSybilsarecorrectlylabeledasneighbors,theaccuracyis0.8.Asoundevaluationrequiresgroundtruth,i.e.,relaysthatareknowntobeSybils.Allwehave,however,arere-laysthatwebelievetobeSybils.Inaddition,thenumberofSybilswefoundisonlyalowerbound—weareun-likelytohavedetectedallSybilgroups.Therefore,ourevaluationisdoomedtooverestimateouralgorithm’sac-curacybecauseweareunabletotestitontheSybilswedidnotdiscover.Weevaluateourrankingalgorithmontwodatasets;the“badexit”SybilgroupsfromTable5,andrelayfamilies.WechosethebadexitSybilsbecauseweobservedthemrunningidentical,activeattacks,whichmakesusconﬁ-dentthattheyareinfactSybils.RecallthatarelayfamilyisasetofTorrelaysthatiscontrolledbyasingleopera-tor,butconﬁguredtoexpressthismutualrelationshipinthefamilymembers’conﬁgurationﬁle.Therefore,relayfamiliesarebenignSybils.AsofJanuary2016,approx-imately400familiespopulatetheTornetwork,ranginginsizefromonlytwoto25relays.Weevaluateouralgorithmbyﬁndingthenearestneighborsofafamilymember.Ideally,allneighborsarefamilymembers,buttheuseofrelayfamiliesasgroundtruthisverylikelytooverestimateresultsbecausefamilyoperatorsfrequentlyconﬁguretheirrelaysiden-ticallyonpurpose.Atthetimeofthiswriting,apop-ularrelayfamilyhasthenicknames“AccessNow000”to“AccessNow009,”adjacentIPaddresses,andidenti-calcontactinformation—perfectprerequisitesforoural-gorithm.WeexpecttheoperatorsofmaliciousSybils,however,togooutoftheirwaytoobscuretherelation-shipbetweentheirrelays.Todetermineouralgorithm’saccuracy,weusedallre-layfamiliesthatwerepresentintheﬁrstconsensusthatwaspublishedinOctober2015.Foreachrelaythathadatleastonemutualfamilyrelationship,wedetermineditsn−1nearestneighborswherenisthefamilysize.Basi-cally,weevaluatedhowgoodouralgorithmisatﬁnd-ingtherelativesofafamilymember.Wedeterminedtheaccuracy—avaluein[0,1]—foreachfamilymem-ber.TheresultisshowninFigure16(b),adistributionofaccuracyvalues.Next,werepeatedtheevaluationwiththebadexitSybilgroupsfromTable5.Again,wedeterminedthen−1nearestneighborsofallbadexitrelays,wherenisthesizeoftheSybilgroup.Theaccuracyisthefractionofrelaysthatouralgorithmcorrectlyclassiﬁedasneigh-13consensuses takes approximately one and two minutes,
respectively—easy to invoke daily, or even several times
a day.
6 Discussion
Having used sybilhunter in practice for several months,
we now elaborate on both our operational experience and
the shortcomings we encountered.
6.1 Operational experience
Our practical work with sybilhunter taught us that an-
alyzing Sybils frequently requires manual veriﬁcation,
e.g., (i) comparing an emerging Sybil group with a pre-
viously disclosed one, (ii) using exitmap to send decoy
trafﬁc over Sybils, or (iii) sorting and comparing infor-
mation in relay descriptors. We found that the amount of
manual work greatly depends on the Sybils under inves-
tigation. The MitM groups in Table 2 were straightfor-
ward to spot—in a matter of minutes—while the botnets
required a few hours of effort. It is difﬁcult to predict
all analysis scenarios that might arise in the future, so
we designed sybilhunter to be interoperable with Unix
command line tools [28]. Sybilhunter’s CSV-formatted
output can easily be piped into tools such as sed, awk,
and grep. We found that compact text output was signif-
icantly easier to process, both for plotting and for man-
ual analysis. Aside from Sybil detection, sybilhunter can
serve as valuable tool to better understand the Tor net-
work and monitor its reliability. Our techniques have
disclosed network consensus issues and can illustrate the
diversity of Tor relays, providing empirical data that can
support future network design decisions.
A key issue in the arms race of eliminating harmful re-
lays lies in information asymmetry. Our detection tech-
niques and code are freely available while our adver-
saries operate behind closed doors, creating an uphill bat-
tle that is difﬁcult to sustain given our limited resources.
In practice, we can reduce this asymmetry and limit our
adversaries’ knowledge by keeping secret sybilhunter’s
thresholds and exitmap’s detection modules, so our ad-
versary is left guessing what our tools seek to detect.
This differentiation between an open analysis framework
such as the one we discuss in this paper, and secret con-
ﬁguration parameters seems to be a sustainable trade-off.
Note that we are not arguing in favor of the ﬂawed prac-
tice of security by obscurity. Instead, we are proposing to
add a layer of obscurity on top of existing defense layers.
We are working with The Tor Project on incorporating
our techniques in Tor Metrics [33], a website containing
network visualizations that are frequented by numerous
volunteers. Many of these volunteers discover anomalies
and report them to The Tor Project. By incorporating
(a) Bad exit relay Sybils
(b) Benign family Sybils
Figure 16: ECDF for our two evaluations, the bad exit
Sybils in Fig. 16(a) and the benign family Sybils in
Fig. 16(b).
Method
Churn
Neighbor ranking One consensus
Fingerprint
Uptimes
Analysis window Run time
Two consensuses
∼0.2s
∼1.6s
∼58.0s
∼145.0s
One month
One month
Table 4: The computational cost of our analysis tech-
niques.
bor. The result is illustrated in Figure 16(a).
As expected, our algorithm is signiﬁcantly more ac-
curate for the family dataset—66% of rankings had per-
fect accuracy. The bad exit dataset, however, did worse.
Not a single ranking had perfect accuracy and 59% of all
rankings had an accuracy in the interval [0.3,0.6]. Nev-
ertheless, we ﬁnd that our algorithm facilitates manual
analysis given how quickly it can provide us with a list
of the most similar relays. Besides, inaccurate results
(i.e., similar neighbors that are not Sybils) are cheap as
sybilhunter users would not spend much time on neigh-
bors that bear little resemblance to the “seed” relay.
5.7 Computational cost
Fast techniques lend themselves to being run hourly, for
every new consensus, while slower ones must be run less
frequent. Table 4 gives an overview of the runtime of our
methods.7 We stored our datasets on a solid state drive
to eliminate I/O as performance bottleneck.
The table columns contain, from left to right, our anal-
ysis technique, the technique’s analysis window, and how
long it takes to compute its output. Network churn cal-
culation is very fast; it takes as input only two consensus
ﬁles and can easily be run for every new network con-
sensus. Nearest-neighbor ranking takes approximately
1.6 seconds for a single consensus counting 6,942 relays.
Fingerprint and uptime analysis for one month worth of
7We determined all performance numbers on an Intel Core i7-
3520M CPU at 2.9 GHz, a consumer-grade CPU.
1182  25th USENIX Security Symposium 
USENIX Association
14
0.00.40.80.00.40.8AccuracyCDF of Sybil groups0.00.40.80.00.40.8AccuracyCDF of Sybil groupsour techniques, we hope to beneﬁt from “crowd-sourced”
Sybil detection.
6.2 Limitations
In Section 4.2, we argued that we are unable to expose
all Sybil attacks, so our results represent a lower bound.
An adversary unconstrained by time and money can add
an unlimited number of Sybils to the network. Indeed,
Table 2 contains six Sybil groups that sybilhunter was
unable to detect. Fortunately, exitmap was able to ex-
pose these Sybils, which emphasizes the importance of
diverse and complementary analysis techniques. Need-
less to say, sybilhunter works best when analyzing at-
tacks that took place before we built sybilhunter. Adver-
saries that know of our methods can evade them at the
cost of having to spend time and resources. To evade
our churn and uptime heuristics, Sybils must be added
and modiﬁed independently over time. Evasion of our
ﬁngerprint heuristic, e.g., to manipulate Tor’s DHT, re-
quires more physical machines. Finally, manipulation of
our neighbor ranking requires changes in conﬁguration.
This arms race is unlikely to end, barring fundamental
changes in how Tor relays are operated.
Sybilhunter is unable to ascertain the purpose of a
Sybil attack. While the purpose is frequently obvious,
Table 2 contains several Sybil groups that we could not
classify. In such cases, it is difﬁcult for The Tor Project to
make a call and decide if Sybils should be removed from
the network. Keeping them runs the risk of exposing
users to an unknown attack, but removing them deprives
the network of bandwidth. Often, additional context is
helpful in making a call. For example, Sybils that are (i)
operated in “bulletproof” autonomous systems [17, § 2],
(ii) show signs of not running the Tor reference imple-
mentation, or (iii) spoof information in their router de-
scriptor all suggest malicious intent. In the end, Sybil
groups have to be evaluated case by case, and the ad-
vantages and disadvantages of blocking them have to be
considered.
Finally, there is signiﬁcant room for improving our
nearest neighbor ranking. For simplicity, our algorithm
represents relays as strings, ignoring a wealth of nuances
such as topological proximity of IP addresses, or pre-
dictable patterns in port numbers.
7 Conclusion
We presented sybilhunter, a novel system that uses di-
verse analysis techniques to expose Sybils in the Tor
network. Equipped with this tool, we set out to ana-
lyze nine years of The Tor Project’s archived network
data. We discovered numerous Sybil groups, twenty of
which we present in this work. By analyzing the Sybil
groups sybilhunter discovered, we found that (i) Sybil
relays are frequently conﬁgured very similarly, and join
and leave the network simultaneously; (ii) attackers dif-
fer greatly in their technical sophistication; and (iii) our