## 构建特征 {#07.html#-}LinUCB算法有一个很重要的步骤，就是给用户和物品构建特征，也就是刻画上下文。在"Yahoo！"的应用中，物品是文章。它对特征做了一些工程化的处理，这里稍微讲一下，可供实际应用时参考借鉴。首先，原始用户特征有下面几个。1.  人口统计学：性别特征（2 类），年龄特征（离散成 10 个区间）。2.  地域信息：遍布全球的大都市，美国各个州。3.  行为类别：代表用户历史行为的 1000 个类别取值。其次，原始文章特征有：1.  URL 类别：根据文章来源分成了几十个类别。2.  编辑打标签：编辑人工给内容从几十个话题标签中挑选出来的。原始特征向量先经过归一化，变成单位向量。再对原始用户特征做第一次降维，降维的方法就是利用用户特征和物品特征以及用户的点击行为去拟合一个矩阵W。]{.MathJax_Preview style="color: inherit; display: none;"} {.MathJax_Display style="text-align: center;"}``{=html}[[[[[ϕ]{#07.html#MathJax-Span-246.mistyle="font-family: MathJax_Math-italic;"}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; clip: rect(3.155em, 1000.57em, 4.332em, -999.998em); top: -3.998em; left: 0em;"}[T[]{style="display: inline-block; overflow: hidden; height: 1px; width: 0.096em;"}]{#07.html#MathJax-Span-249.mistyle="font-size: 70.7%; font-family: MathJax_Math-italic;"}]{#07.html#MathJax-Span-248.mrow}]{#07.html#MathJax-Span-247.texatom}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; clip: rect(3.391em, 1000.57em, 4.144em, -999.998em); top: -4.327em; left: 0.614em;"}[u]{#07.html#MathJax-Span-252.mistyle="font-size: 70.7%; font-family: MathJax_Math-italic;"}]{#07.html#MathJax-Span-251.mrow}]{#07.html#MathJax-Span-250.texatom}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; clip: rect(3.532em, 1000.47em, 4.144em, -999.998em); top: -3.856em; left: 0.614em;"}]{style="display: inline-block; position: relative; width: 1.179em; height: 0px;"}]{#07.html#MathJax-Span-245.msubsup}[W[]{style="display: inline-block; overflow: hidden; height: 1px; width: 0.096em;"}]{#07.html#MathJax-Span-253.mistyle="font-family: MathJax_Math-italic;"}[ϕ]{#07.html#MathJax-Span-255.mistyle="font-family: MathJax_Math-italic;"}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; clip: rect(3.155em, 1000.57em, 4.332em, -999.998em); top: -3.998em; left: 0em;"}[T[]{style="display: inline-block; overflow: hidden; height: 1px; width: 0.096em;"}]{#07.html#MathJax-Span-258.mistyle="font-size: 70.7%; font-family: MathJax_Math-italic;"}]{#07.html#MathJax-Span-257.mrow}]{#07.html#MathJax-Span-256.texatom}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; clip: rect(3.391em, 1000.57em, 4.144em, -999.998em); top: -4.327em; left: 0.614em;"}[a]{#07.html#MathJax-Span-261.mistyle="font-size: 70.7%; font-family: MathJax_Math-italic;"}]{#07.html#MathJax-Span-260.mrow}]{#07.html#MathJax-Span-259.texatom}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; clip: rect(3.532em, 1000.47em, 4.144em, -999.998em); top: -3.856em; left: 0.614em;"}]{style="display: inline-block; position: relative; width: 1.179em; height: 0px;"}]{#07.html#MathJax-Span-254.msubsup}]{#07.html#MathJax-Span-244.mrow}[]{style="display: inline-block; width: 0px; height: 2.308em;"}]{style="position: absolute; clip: rect(1.367em, 1003.39em, 2.638em, -999.998em); top: -2.304em; left: 0em;"}]{style="display: inline-block; position: relative; width: 3.391em; height: 0px; font-size: 125%;"}[]{style="display: inline-block; overflow: hidden; vertical-align: -0.291em; border-left: 0px solid; width: 0px; height: 1.415em;"}]{#07.html#MathJax-Span-243.mathstyle="width: 4.238em; display: inline-block;"}``{=html}[$$\phi_{u}^{T}W\phi_{a}^{T}$$]{.MJX_Assistive_MathML.MJX_Assistive_MathML_Blockrole="presentation"}]{#07.html#MathJax-Element-9-Frame .MathJaxtabindex="0" style="text-align: center; position: relative;"mathml="ϕuTWϕaT"role="presentation"}$$$$就用逻辑回归拟合用户对文章的点击历史，得到的 W直觉上理解就是：能够把用户特征映射到物品特征上，相当于对用户特征降维了，映射方法是下面这样。]{.MathJax_Preview style="color: inherit; display: none;"} {.MathJax_Display style="text-align: center;"}``{=html}[[[[[ψ]{#07.html#MathJax-Span-265.mistyle="font-family: MathJax_Math-italic;"}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; clip: rect(3.155em, 1000.61em, 4.332em, -999.998em); top: -3.998em; left: 0em;"}[u]{#07.html#MathJax-Span-268.mistyle="font-size: 70.7%; font-family: MathJax_Math-italic;"}]{#07.html#MathJax-Span-267.mrow}]{#07.html#MathJax-Span-266.texatom}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; top: -3.856em; left: 0.661em;"}]{style="display: inline-block; position: relative; width: 1.132em; height: 0px;"}]{#07.html#MathJax-Span-264.msubsup}[=]{#07.html#MathJax-Span-269 .mostyle="font-family: MathJax_Main; padding-left: 0.285em;"}[ϕ]{#07.html#MathJax-Span-271.mistyle="font-family: MathJax_Math-italic;"}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; clip: rect(3.155em, 1000.57em, 4.332em, -999.998em); top: -3.998em; left: 0em;"}[T[]{style="display: inline-block; overflow: hidden; height: 1px; width: 0.096em;"}]{#07.html#MathJax-Span-274.mistyle="font-size: 70.7%; font-family: MathJax_Math-italic;"}]{#07.html#MathJax-Span-273.mrow}]{#07.html#MathJax-Span-272.texatom}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; clip: rect(3.391em, 1000.57em, 4.144em, -999.998em); top: -4.327em; left: 0.614em;"}[u]{#07.html#MathJax-Span-277.mistyle="font-size: 70.7%; font-family: MathJax_Math-italic;"}]{#07.html#MathJax-Span-276.mrow}]{#07.html#MathJax-Span-275.texatom}[]{style="display: inline-block; width: 0px; height: 4.002em;"}]{style="position: absolute; clip: rect(3.532em, 1000.47em, 4.144em, -999.998em); top: -3.856em; left: 0.614em;"}]{style="display: inline-block; position: relative; width: 1.179em; height: 0px;"}]{#07.html#MathJax-Span-270.msubsupstyle="padding-left: 0.285em;"}[W[]{style="display: inline-block; overflow: hidden; height: 1px; width: 0.096em;"}]{#07.html#MathJax-Span-278.mistyle="font-family: MathJax_Math-italic;"}]{#07.html#MathJax-Span-263.mrow}[]{style="display: inline-block; width: 0px; height: 2.355em;"}]{style="position: absolute; clip: rect(1.414em, 1004.71em, 2.685em, -999.998em); top: -2.351em; left: 0em;"}]{style="display: inline-block; position: relative; width: 4.708em; height: 0px; font-size: 125%;"}[]{style="display: inline-block; overflow: hidden; vertical-align: -0.291em; border-left: 0px solid; width: 0px; height: 1.415em;"}]{#07.html#MathJax-Span-262.mathstyle="width: 5.885em; display: inline-block;"}``{=html}[$$\psi_{u} = \phi_{u}^{T}W$$]{.MJX_Assistive_MathML.MJX_Assistive_MathML_Blockrole="presentation"}]{#07.html#MathJax-Element-10-Frame .MathJaxtabindex="0" style="text-align: center; position: relative;"mathml="ψu=ϕuTW"role="presentation"}$$$$这一步可以将原始的 1000 多维用户特征投射到文章的 80 多维的特征空间。然后，用投射后的 80 多维特征对用户聚类，得到 5 个类，文章页同样聚类成 5个类，再加上常数 1，用户和文章各自被表示成 6 维向量。接下来就应用前面的 LinUCB 算法就是了，特征工程依然还是很有效的。
## 总结 {#07.html#-}今天我和你分享了一种上下文有关的 Bandit 算法，叫做LinUCB，它有这么几个优点：1.  由于加入了特征，所以收敛比 UCB 更快，也就是比 UCB 更快见效；2.  各个候选臂之间参数是独立的，可以互相不影响地更新参数；3.  由于参与计算的是特征，所以可以处理动态的推荐候选池，编辑可以增删文章；当然，LinUCB 以及所有的 Bandit算法都有个缺点：同时处理的候选臂数量不能太多，不超过几百个最佳。因为每一次要计算每一个候选臂的期望收益和置信区间，一旦候选太多，计算代价将不可接受。LinUCB只是一个推荐框架，可以将这个框架应用在很多地方，比如投放广告，为用户选择兴趣标签，你还可以发挥聪明才智，看看它还能用来解决什么问题，欢迎留言一起交流。![](Images/d3b48a2755db0a3707ef37007c2179c8.png){savepage-src="https://static001.geekbang.org/resource/image/87/b0/873b086966136189db14874181823fb0.jpg"}
# 【其他应用算法】实用的加权采样算法今天来讲一个非常轻松的话题，这个话题看似和推荐系统没什么关系，但肯定有用，只是在别的推荐系统相关话题里都没人会提。
## 一些场景 {#08.html#-}还记得前面讲到的用户画像吗？想象一个场景：你经过辛辛苦苦抓数据，清洗数据，收集用户行为，目的就是给用户计算兴趣标签。这时候你可能会遇到一个两难的问题：如果给用户计算出兴趣标签的权重了，那应该保留多少标签呢？保留太多的话，每次召回候选集时，计算复杂度可不低，只保留少部分吧，那真是手心手背都是肉，生怕丢弃的标签才是用户的真爱。怎么办？这时候，你需要的一个简单的加权采样算法，每次召回时并不使用全部用户标签，而是按照权重采样一部分标签来使用，这样做的好处当然很明显：1.  大大减少召回时的计算复杂度；2.  可以保留更多的用户标签；3.  每次召回计算时还能有所变化；4.  虽然有变化，但是依然受标签的权重相对大小约束。加权采样的应用不只这一个地方，比如在热门排行榜展示时，也可以用加权采样，而不仅仅按照排行榜分数顺序展示，采用加权采样的展示方法，会让排行榜每次刷新都略有变化，人民群众也会更加喜闻乐见。下面介绍几种常用的加权采样算法及其原理，供你日常随手拿来使用。
## 加权采样 {#08.html#-}加权采样有两种情况，一种是能够已知全部样本的个数。这需要遍历整个样本，比如说用户标签采样输出，那么每次采样时仍然需要遍历所有的标签，来依次决定每一个标签输出的概率。另一种是不知道总量样本是多大，或者总量很大，以至于你不愿意全部遍历之后再输出采样结果，这样的数据就是数据流，对应的就是流采样。``{=html}下面分别讲这两种采样方法。