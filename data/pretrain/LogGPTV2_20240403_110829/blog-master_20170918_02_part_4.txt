                                       Rows out:  0 rows (seg0) with 15 ms to end, start offset by 503 ms.  
                                       ->  Append-only Scan on tbl_pos_1_prt_p1 tbl_pos  (cost=0.00..17993441.00 rows=831251 width=8)  
                                             Filter: pos_att = 1 AND pos >= 't9m'::text AND pos   HashAggregate  (cost=53124325.91..53135748.71 rows=19038 width=40)  
         Group By: "?column1?"  
         Rows out:  Avg 175446.8 rows x 48 workers.  Max 176265 rows (seg2) with 0.001 ms to first row, 8243 ms to end, start offset by 466 ms.  
         ->  Redistribute Motion 48:48  (slice1; segments: 48)  (cost=53090057.51..53110618.55 rows=19038 width=40)  
               Hash Key: unnamed_attr_1  
               Rows out:  Avg 191284.2 rows x 48 workers at destination.  Max 192297 rows (seg37) with 15634 ms to end, start offset by 529 ms.  
               ->  HashAggregate  (cost=53090057.51..53092342.07 rows=19038 width=40)  
                     Group By: "substring"(t1.pos, 1, 6)  
                     Rows out:  Avg 191284.2 rows x 48 workers.  Max 191966 rows (seg1) with 0.006 ms to first row, 134 ms to end, start offset by 468 ms.  
                     ->  Hash Join  (cost=37578340.02..53085488.39 rows=19039 width=11)  
                           Hash Cond: t1.uid = tbl_tag.uid  
                           Rows out:  Avg 191640.6 rows x 48 workers.  Max 192331 rows (seg1) with 0.039 ms to first row, 18171 ms to end, start offset by 468 ms.  
                           Executor memory:  5989K bytes avg, 6011K bytes max (seg1).  
                           Work_mem used:  5989K bytes avg, 6011K bytes max (seg1). Workfile: (0 spilling, 0 reused)  
                           ->  Append  (cost=0.00..12993441.00 rows=20833334 width=19)  
                                 Rows out:  0 rows (seg0) with 1228 ms to end, start offset by 531 ms.  
                                 ->  Append-only Scan on tbl_pos_1_prt_p1 t1  (cost=0.00..12993441.00 rows=20833334 width=19)  
                                       Filter: pos_att = 1  
                                       Rows out:  Avg 20833333.3 rows x 48 workers.  Max 20833547 rows (seg37) with 0.005 ms to first row, 0.006 ms to end, start offset by 531 ms.  
                           ->  Hash  (cost=37464112.00..37464112.00 rows=190381 width=8)  
                                 Rows in:  (No row requested) 0 rows (seg0) with 0 ms to end.  
                                 ->  Append-only Columnar Scan on tbl_tag  (cost=0.00..37464112.00 rows=190381 width=8)  
                                       Filter: c1 = 1 OR (c2 >= 1 AND c2 = 1 AND c4 <= 200)  
                                       Rows out:  0 rows (seg0) with 57 ms to end, start offset by 528 ms.  
 Slice statistics:  
   (slice0)    Executor memory: 487K bytes.  
   (slice1)    Executor memory: 1725K bytes avg x 48 workers, 1725K bytes max (seg0).  Work_mem: 6011K bytes max.  
   (slice2)    Executor memory: 524K bytes avg x 48 workers, 524K bytes max (seg0).  
 Statement statistics:  
   Memory used: 128000K bytes  
 Settings:  optimizer=off  
 Optimizer status: legacy query optimizer  
 Total runtime: 26166.164 ms  
(35 rows)  
Time: 26170.031 ms  
```  
## 小结  
对比：  
### 空间  
数据库 | 表 | 记录数 | SIZE  
---|---|---|---  
PostgreSQL 10 | 标签表(201个字段) | 10 亿 | 424 GB  
Greenplum | 标签表(201个字段) | 10 亿 | 369 GB  
PostgreSQL 10 | 位置表(12个字段) | 100 亿 | 640 GB  
Greenplum | 位置表(12个字段) | 100 亿 | 150 GB  
数据库 | 索引 | 索引类型 | SIZE  
---|---|---|---  
PostgreSQL 10 | 标签表 | btree | 4200 GB  
PostgreSQL 10 | 位置表 | brin | 27 MB  
### 性能  
数据库 | 业务 | 耗时  
---|---|---  
PostgreSQL 10 | 100亿空间数据，按空间圈出约1000万人 | 400 毫秒  
Greenplum | 100亿空间数据，按空间圈出约1000万人 | 21 秒  
PostgreSQL 10 | 100亿空间数据，按空间圈出约1000万人，JOIN 10亿标签数据，透视这群人的标签属性 | 7 秒  
Greenplum | 100亿空间数据，按空间圈出约1000万人，JOIN 10亿标签数据，透视这群人的标签属性 | 29.3 秒  
PostgreSQL 10 | 10亿标签数据，按标签圈人约1000万 | 14.5 秒（能通过metascan优化到500毫秒以内）  
Greenplum | 10亿标签数据，按标签圈人约1000万 | 3.4 秒  
PostgreSQL 10 | 10亿标签数据，按标签圈人约1000万，透视这群人的空间属性 | 203秒 (PG 11版本 merge join partial scan可以大幅提高性能)  
Greenplum | 10亿标签数据，按标签圈人约1000万，透视这群人的空间属性 | 26.2 秒  
### PostgreSQL 10未来优化点  
PG 10 通过brin索引，bitmap scan，在部分场景的性能已经超过同等资源的Greenplum。  
引入列存引擎、parallel hash补丁、range merge join不对，在同等资源下，另外几个场景的性能会做到和Greenplum差不多（甚至更好）。  
1、parallel hash join  
https://commitfest.postgresql.org/14/871/  
2、range merge join  
https://commitfest.postgresql.org/14/1106/  
3、parallel append scan  
https://commitfest.postgresql.org/14/987/  
但是Greenplum的强项是更大的数据量，例如通过打散，并行玩转PB级的实时分析。  
而PG，更加适合以TP为主，AP为辅的场景，即Oracle数据库覆盖到的场景。  
Greenplum和PostgreSQL两个产品的选择还是请参考前面所述。  
#### [PostgreSQL 许愿链接](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
您的愿望将传达给PG kernel hacker、数据库厂商等, 帮助提高数据库产品质量和功能, 说不定下一个PG版本就有您提出的功能点. 针对非常好的提议，奖励限量版PG文化衫、纪念品、贴纸、PG热门书籍等，奖品丰富，快来许愿。[开不开森](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216").  
#### [9.9元购买3个月阿里云RDS PostgreSQL实例](https://www.aliyun.com/database/postgresqlactivity "57258f76c37864c6e6d23383d05714ea")
#### [PostgreSQL 解决方案集合](https://yq.aliyun.com/topic/118 "40cff096e9ed7122c512b35d8561d9c8")
#### [德哥 / digoal's github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
#### [PolarDB 学习图谱: 训练营、培训认证、在线互动实验、解决方案、生态合作、写心得拿奖品](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
#### [购买PolarDB云服务折扣活动进行中, 55元起](https://www.aliyun.com/activity/new/polardb-yunparter?userCode=bsb3t4al "e0495c413bedacabb75ff1e880be465a")
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")