128
1024
2116
1245
37
25
1744
375
3.6
24
42
328
0.43
13.5
3.91
3.99
1.4
0.99
3.91
3.94
0.52*
0.99
3.47
3.84
0.06*
0.93
Online + Offline
512
1441
8.6
727
1.1
93
0.12
3.86
0.56
3.84
0.17 *
3.59
0.02*
256
1892
20
1276
1.9
177
0.24
3.90
0.94
3.89
0.32*
3.75
0.03*
1024
1031
4.4
345
0.6
41
0.06
3.79
0.33
3.74
0.01*
3.32
0.01*
Figure 4: Logistic Regression performance measured in iterations per second (larger = better). See caption of Figure 2.
(two rounds) or the minimum value of B determined by the training
data, whichever is larger.
The batched update function can then be applied to each batch.
The termination condition could then be computed periodically,
e.g. every 100 batches. We note that this check need not add to
the overall round complexity. Instead, this check can be performed
asynchronously with the update function. Moreover, due to it being
performed infrequently, it will have little impact on the overall
running time.
One important observation is that the two matrix multiplications
performed in update function should be optimized using the delayed
reshare technique of Section 5.2. This reduces the communication
per multiplication to B + D elements instead of 2DB. In many cases
the training data is very high dimensional, making this optimization
of critical importance. The dominant cost of this protocol is 2 rounds
of communication per iteration. In the semi-honest setting, each
iteration sends B+D shares per party and consumes B+D truncation
triples described in Section 5.1.
A.2 Logistic Regression
Logistic regression is a widely used classification algorithm which is
conceptually similar to linear regression. The main difference is that
the dependent variable y is binary as opposed to a real value in the
case of linear regression. For example, given someone’s credit card
history x, we wish to decide whether a pending transaction should
be approved y = 1 or denied y = 0. For Logistic regression models
the rate of convergence can be improved by bounding the output
variable to be in the range between zero and one. This is achieved
by applying an activation function f , which is bounded by zero
and one, to the inner product, i.e. y′ = д(x) = f (x · w). While there
are many suitable activation functions, in the problem of logistic
regression, f is defined to be the logistic function f (u) = 1
1+eu .
One consequence of using this activation function is that the L2
cost function from the previous section is no longer convex. This
is addressed by changing the cost function to be the cross-entropy
equation, C(x,y)(w) := −y log f (x · w) − (1 − y) log(1 − f (x · w)).
Given this, the update function for batch j can be defined as, w :=
w− α
j ×(f (Xj × w)− Yj). Observe that while the cost function
has changed, the update function is quite similar to linear regression
with the sole addition of the activation function f .
Unfortunately, computing the logistic function in the secret
shared setting is an expensive operation. We instead follow the
approach presented by Mohassel & Yupeng [43] where the logistic
1
B XT
function is replaced with the piecewise function
f (x) =
x < −1/2
0,
x + 1/2, −1/2 ≤ x < 1/2
1,
1/2 ≤ x
As shown in [43, figure 7], the piecewise function roughly approx-
imates the original. Moreover, [43] empirically showed that this
change only decreases the accuracy of the MNIST model by 0.02
percent. However, we replaced the special purpose two party pro-
tocol that [43] presents with our general approach for computing
any polynomial piecewise function that was detailed in Section 5.5.
This allows us to easily handle better approximations of the lo-
gistic function too (e.g. non-linear piecewise polynomials, or [40]
considers a piecewise linear function with 12 pieces).
A.3 Neural Nets
Neural network models have received a significant amount of in-
terest over the last decade due to their extremely accurate predic-
tions on a wide range of applications such as image and speech
recognition. Conceptually, neural networks are a generalization of
regression to support complex relationships between high dimen-
sional input and output data. A basic neural network can be divided
up into m layers, each containing mi nodes. Each node is a linear
function composed with a non-linear “activation" function. To eval-
uate a neural network, the nodes at the first layer are evaluated on
the input features. The outputs of these nodes are then forwarded
as inputs to the next layer of the network until all layers have
been evaluated in this manner. The training of neural networks is
performed using gradient descent in a similar manner to logistic
regression except that each layer of the network should be updated
in a recursive manner, starting at the output layer and working
backward. Many different neural network activations functions
have been considered in the literature. One of the most popular is
the rectified linear unit (ReLU) function which can be expressed
as f (x) = max(0, x). This function and nearly all other activations
functions can easily be implemented using our piecewise polyno-
mial technique from Section 5.5. For a more detailed description of
the exact operations, neural network evaluation entails, we refer
readers to [40, 46].
Session 1B: PrivacyCCS’18, October 15-19, 2018, Toronto, ON, Canada49B COMPARISON TO STANDARD 3PC
To further demonstrate the advantages of our mixed protocol frame-
work we provide a comparison to a pure binary circuit based imple-
mentation which utilizes the state-of-the-art three party protocol of
[9]. This protocol is the binary secret sharing technique employed
by our framework. The round complexity of [9] is proportional to
the depth of the binary circuit and requires each party to send one
bit per and gate in the circuit. Alternatively, the garbled circuit ap-
proach of Mohassel et al.[42] could be used. This would reduce the
round complexity to a constant at the expense of roughly κ = 128
times more communication. Our analysis on the target application
of machine learning suggest that the protocol of [9] would achieved
superior performance due to machine learning having a relatively
low depth per and gate ratio. That is, machine learning circuits are
often quite wide, allowing many gates to be processed in a single
round. In other applications when the circuit is narrow and deep of
approach of Mohassel et al.[42] could be optimal.
Regardless, the core issue with a binary circuit approach [9, 42]
is the necessity for performing fixed point arithmetic using binary
circuits. This primarily impacts overhead of the fixed point multi-
plication protocol and computing non-linear activation functions
such as the logistic and ReLU functions. In both of these cases the
amount of communication and the number of rounds is increased.
When simply comparing the basic operations of addition and
multiplication of [9] our protocol has immediate advantages. In
particular, our fixed point addition requires no communication as
the shares can be locally added. On the other hand, addition of k-bit
fixed-point values using the binary circuit approach of [9] requires
k and gates and k rounds of communication or k log k gates and
log k rounds4. Either way, this represents an very considerable
overhead when performing just about any type of computation.
The situation is made even worse when multiplication is considered.
2
Here, an optimized multiplication circuit for [9, 42] requires 2k
and gates and k + log k rounds5 in the case of [9]. In contrast our
protocol requires 1 round of interaction where each party sends 2k
bits plus an efficient constant round preprocessing. We note that [9]
has no obvious way of leveraging a preprocessing phase to improve
performance. In practice with k = 64 our protocol requires 128×
fewer bits and 71× fewer rounds when compared to [9].
An important optimization for machine learning algorithms is
the vectorization technique of Section 5.2. This technique allows
i =0 xiyi in 1 round and 2k bits of communication.
In contrast, binary circuit based approaches [9, 42] can not per-
2 and gates and [9] requires
form this technique and requires 2nk
k + log kn rounds of communication6. Typical machine learning
algorithms compute such inner products for vectors of size n = 512
or greater. In these scenarios our approach requires 32, 768× fewer
bits of communication and 80× fewer rounds of communication
when compared to [9]. Since our linear regression training algo-
rithm makes extensive use of these inner product computations we
us to computen
estimate that these performance metrics give an accurate depiction
of the performance difference between our protocol and a binary
circuit implementation.
Another interesting point of comparison is the efficiency of our
piecewise polynomial technique. Recall that our approach first
performs a range test on the input value x to compute a vector
of bits b1, ..., bm where bi = 1 encodes that x is in the ith inter-
val. The final result can then be computed as
i(cid:74)bi(cid:75)fi((cid:74)x(cid:75)) where
fi((cid:74)x(cid:75)) = ai, j(cid:74)x(cid:75)j + ... + ai,1(cid:74)x(cid:75) + ai,0. Our protocol performs the
interval test by locally subtracting a constant and performing bit
extraction. This requires k and gates and log k rounds. If we con-
sider the pure binary circuit approach the subtraction now has the
overhead of k log k and gates and log k rounds which is moderately
less efficient. However, the main overhead with the binary circuit
public arithmetic value ai, j can not be performed locally, the par-
approach is the computation of fi((cid:74)x(cid:75)). Since multiplication with a
ties must employ a multiplication circuit to compute ai, j(cid:74)x(cid:75) along
j − 1 multiplication circuits to compute(cid:74)x(cid:75)2
, ...,(cid:74)x(cid:75)j. In total, we
, ...,(cid:74)x(cid:75)j. the final step computing
2 + jmk bits of communication and
estimate the overhead to be 4jmk
(k + log k) log j rounds where m is the number of intervals and j is
the max degree of the polynomials. Our protocol on the other hand
requires log j rounds and kjm bits of communication to compute
bits and one rounds. This totals to (6 + j)km bits per party and
log k + 1 rounds.
i(cid:74)bi(cid:75)fi((cid:74)x(cid:75)) requires 6mk
(cid:74)x(cid:75)2
C MALICIOUS VECTORIZED
MULTIPLICATION
Computing inner products in the malicious setting is more compli-
cated due o the fact that for each multiplication(cid:74)xi(cid:75)(cid:74)yi(cid:75) a proof
of correctness must be provided. This would immediately result
in the communication increasing back to O(n) elements. However,
we show that in the context of matrix multiplication this increased
communication can be transferred to an offline phase. To compute
(cid:74)X(cid:75)(cid:74)Y(cid:75) the parties first generate two random matrices(cid:74)A(cid:75),(cid:74)B(cid:75)
which are respectively the same dimension as(cid:74)X(cid:75),(cid:74)Y(cid:75). During the
offline phase, the parties compute the matrix triple(cid:74)C(cid:75) :=(cid:74)A(cid:75)(cid:74)B(cid:75)
using the scalar fixed-point multiplication protocol described in the
previous section. Given this, the malicious secure multiplication
protocol of [28] can naturally be generalized to the matrix setting.
(cid:74)Z(cid:75) :=(cid:74)X(cid:75)(cid:74)Y(cid:75) and then party i sends their local share Zi to party
In particular, the parties locally compute the 3-out-of-3 sharing
i−1. Party i also proves the correctness of Zi using the matrix triple
((cid:74)A(cid:75),(cid:74)B(cid:75),(cid:74)C(cid:75)) along with a natural extension of protocol 2.24 in
[28] where scaler operations are replaced with matrix operations.
The online communication of this protocol is proportional to the
sizes of X , Y , Z and almost equivalent to the semi-honest protocol.
However, the offline communication is proportional to the number
of scaler multiplication which is cubic in the dimensions of X and
Y.
4The former is computed using a RCFA circuit while the latter is a parallel prefix adder.
See Section 5.3 for details.
5A size/depth balanced circuit is used here. In particular, a tree of RCFA is used to sum
the long hand multiplication terms. Alternatively, a size optimized circuit [54] or a
depth optimized circuit[16] could be employed.
6This is computed by a tree of RCFA circuits to sum the nk long hand multiplication
terms.
D PROOFS
Our framework is a composition of sub-protocols for fixed-point
arithmetic 3PC and the various share conversion protocols we de-
sign. Hence, it suffices to argue about the security of these building
blocks.
Session 1B: PrivacyCCS’18, October 15-19, 2018, Toronto, ON, Canada50D.1 Arithmetic Extension of [28]
Throughout this exposition we have assumed that the malicious
secure secret sharing based MPC protocol of [28] can operate both
on binary and arithmetic shares. While this is true, only the case
of binary shares were presented and proven secure in [28]. For
completeness we now specify how their protocols can be extended
to the arithmetic setting. The vast majority of their protocol imme-
diately extends to the arithmetic setting with no material changes
to the protocols or proofs. The main exception to this is the mul-
tiplication protocol and how to check the correctness of a triple
without opening another.
First, observe that the semi-honest arithmetic multiplication
protocol [9] is secure in the malicious setting up to an additive
attack. In particular, the malicious party may send z′
= zi + δ
i
where δ is the difference between the correct share zi and the one
sent. This has the effect of making the reconstructed value differ