rebuilding a tool. This low latency is often a factor in keeping MTTR low. However,
these same files are also changed frequently for reasons that don’t need that reduced
latency. When viewed from the point of view of reliability:
• A configuration file that exists to keep MTTR low, and is only modified when
there’s a failure, has a release cadence slower than the MTBF. There can be a fair
amount of uncertainty as to whether a given manual edit is actually truly optimal
without the edit impacting the overall site reliability.
• A configuration file that changes more than once per user-facing application
release (for example, because it holds release state) can be a major risk if these
changes are not treated the same as application releases. If testing and monitor‐
ing coverage of that configuration file is not considerably better than that of the
user application, that file will dominate site reliability in a negative way.
One method of handling configuration files is to make sure that every configuration
file is categorized under only one of the options in the preceding bulleted list, and to
somehow enforce that rule. Should you take the latter strategy, make sure of the
following:
• Each configuration file has enough test coverage to support regular routine
editing.
• Before releases, file edits are somewhat delayed while waiting for release testing.
• Provide a break-glass mechanism to push the file live before completing the test‐
ing. Since breaking the glass impairs reliability, it’s generally a good idea to make
the break noisy by (for example) filing a bug requesting a more robust resolution
for next time.
200 | Chapter 17: Testing for Reliability
Break-Glass and Testing
You can implement a break-glass mechanism to disable release testing. Doing so
means that whoever makes a hurried manual edit isn’t told about any mistakes until
the real user impact is reported by monitoring. It’s better to leave the tests running,
associate the early push event with the pending testing event, and (as soon as possi‐
ble) back-annotate the push with any broken tests. This way, a flawed manual push
can be quickly followed by another (hopefully less flawed) manual push. Ideally, that
break-glass mechanism automatically boosts the priority of those release tests so that
they can preempt the routine incremental validation and coverage workload that the
test infrastructure is already processing.
Integration
In addition to unit testing a configuration file to mitigate its risk to reliability, it’s also
important to consider integration testing configuration files. The contents of the con‐
figuration file are (for testing purposes) potentially hostile content to the interpreter
reading the configuration. Interpreted languages such as Python are commonly used
for configuration files because their interpreters can be embedded, and some simple
sandboxing is available to protect against nonmalicious coding errors.
Writing your configuration files in an interpreted language is risky, as this approach is
fraught with latent failures that are hard to definitively address. Because loading con‐
tent actually consists of executing a program, there’s no inherent upper limit on how
inefficient loading can be. In addition to any other testing, you should pair this type
of integration testing with careful deadline checking on all integration test methods in
order to label tests that do not run to completion in a reasonable amount of time as
failed.
If the configuration is instead written as text in a custom syntax, every category of
test needs separate coverage from scratch. Using an existing syntax such as YAML in
combination with a heavily tested parser like Python’s safe_load removes some of
the toil incurred by the configuration file. Careful choice of syntax and parser can
ensure there’s a hard upper limit on how long the loading operation can take. How‐
ever, the implementer needs to address schema faults, and most simple strategies for
doing so don’t have an upper bound on runtime. Even worse, these strategies tend not
to be robustly unit tested.
Testing at Scale | 201
The benefit of using protocol buffers16 is that the schema is defined in advance and
automatically checked at load time, removing even more of the toil, yet still offering
the bounded runtime.
The role of SRE generally includes writing systems engineering tools17 (if no one else
is already writing them) and adding robust validation with test coverage. All tools can
behave unexpectedly due to bugs not caught by testing, so defense in depth is advisa‐
ble. When one tool behaves unexpectedly, engineers need to be as confident as possi‐
ble that most of their other tools are working correctly and can therefore mitigate or
resolve the side effects of that misbehavior. A key element of delivering site reliability
is finding each anticipated form of misbehavior and making sure that some test (or
another tool’s tested input validator) reports that misbehavior. The tool that finds the
problem might not be able to fix or even stop it, but should at least report the prob‐
lem before a catastrophic outage occurs.
For example, consider the configured list of all users (such as /etc/passwd on a non-
networked Unix-style machine) and imagine an edit that unintentionally causes the
parser to stop after parsing only half of the file. Because recently created users haven’t
loaded, the machine will most likely continue to run without problem, and many
users may not notice the fault. The tool that maintains home directories can easily
notice the mismatch between the actual directories present and those implied by the
(partial) user list and urgently report the discrepancy. This tool’s value lies in report‐
ing the problem, and it should avoid attempting to remediate on its own (by deleting
lots of user data).
Production Probes
Given that testing specifies acceptable behavior in the face of known data, while mon‐
itoring confirms acceptable behavior in the face of unknown user data, it would seem
that major sources of risk—both the known and the unknown—are covered by the
combination of testing and monitoring. Unfortunately, actual risk is more
complicated.
Known good requests should work, while known bad requests should error. Imple‐
menting both kinds of coverage as an integration test is generally a good idea. You
can replay the same bank of test requests as a release test. Splitting the known good
requests into those that can be replayed against production and those that can’t yields
three sets of requests:
16 See https://github.com/google/protobuf.
17 Not because software engineers shouldn’t write them. Tools that cross between technology verticals and span
abstraction layers tend to have weak associations with many software teams and a slightly stronger association
with systems teams.
202 | Chapter 17: Testing for Reliability
• Known bad requests
• Known good requests that can be replayed against production
• Known good requests that can’t be replayed against production
You can use each set as both integration and release tests. Most of these tests can also
be used as monitoring probes.
It would seem to be superfluous and, in principle, pointless to deploy such monitor‐
ing because these exact same requests have already been tried two other ways. How‐
ever, those two ways were different for a few reasons:
• The release test probably wrapped the integrated server with a frontend and a
fake backend.
• The probe test probably wrapped the release binary with a load balancing
frontend and a separate scalable persistent backend.
• Frontends and backends probably have independent release cycles. It’s likely that
the schedules for those cycles occur at different rates (due to their adaptive
release cadences).
Therefore, the monitoring probe running in production is a configuration that wasn’t
previously tested.
Those probes should never fail, but what does it mean if they do fail? Either the
frontend API (from the load balancer) or the backend API (to the persistent store) is
not equivalent between the production and release environments. Unless you already
know why the production and release environments aren’t equivalent, the site is likely
broken.
The same production updater that gradually replaces the application also gradually
replaces the probes so that all four combinations of old-or-new probes sending
requests to old-or-new applications are being continuously generated. That updater
can detect when one of the four combinations is generating errors and roll back to
the last known good state. Usually, the updater expects each newly started application
instance to be unhealthy for a short time as it prepares to start receiving lots of user
traffic. If the probes are already inspected as part of the readiness check, the update
safely fails indefinitely, and no user traffic is ever routed to the new version. The
update remains paused until engineers have time and inclination to diagnose the fault
condition and then encourage the production updater to cleanly roll back.
This production test by probe does indeed offer protection to the site, plus clear feed‐
back to the engineers. The earlier that feedback is given to engineers, the more useful
it is. It’s also preferable that the test is automated so that the delivery of warnings to
engineers is scalable.
Testing at Scale | 203
Assume that each component has the older software version that’s being replaced and
the newer version that’s rolling out (now or very soon). The newer version might be
talking to the old version’s peer, which forces it to use the deprecated API. Or the
older version might be talking to a peer’s newer version, using the API which (at the
time the older version was released) didn’t work properly yet. But it works now, hon‐
est! You’d better hope those tests for future compatibility (which are running as moni‐
toring probes) had good API coverage.
Fake Backend Versions
When implementing release tests, the fake backend is often maintained by the peer
service’s engineering team and merely referenced as a build dependency. The her‐
metic test that is executed by the testing infrastructure always combines the fake
backend and the test frontend at the same build point in the revision control history.
That build dependency may be providing a runnable hermetic binary and, ideally, the
engineering team maintaining it cuts a release of that fake backend binary at the same
time they cut their main backend application and their probes. If that backend release
is available, it might be worthwhile to include hermetic frontend release tests (without
the fake backend binary) in the frontend release package.
Your monitoring should be aware of all release versions on both sides of a given ser‐
vice interface between two peers. This setup ensures that retrieving every combina‐
tion of the two releases and determining whether the test still passes doesn’t take
much extra configuration. This monitoring doesn’t have to happen continuously—
you only need to run new combinations that are the result of either team cutting a
new release. Such problems don’t have to block that new release itself.
On the other hand, rollout automation should ideally block the associated production
rollout until the problematic combinations are no longer possible. Similarly, the peer
team’s automation may consider draining (and upgrading) the replicas that haven’t yet
moved from a problematic combination.
Conclusion
Testing is one of the most profitable investments engineers can make to improve the
reliability of their product. Testing isn’t an activity that happens once or twice in the
lifecycle of a project; it’s continuous. The amount of effort required to write good tests
is substantial, as is the effort to build and maintain infrastructure that promotes a
strong testing culture. You can’t fix a problem until you understand it, and in engi‐
neering, you can only understand a problem by measuring it. The methodologies and
techniques in this chapter provide a solid foundation for measuring faults and uncer‐
tainty in a software system, and help engineers reason about the reliability of software
as it’s written and released to users.
204 | Chapter 17: Testing for Reliability
CHAPTER 18
Software Engineering in SRE
Written by Dave Helstroom and Trisha Weir
with Evan Leonard and Kurt Delimon
Edited by Kavita Guliani
Ask someone to name a Google software engineering effort and they’ll likely list a
consumer-facing product like Gmail or Maps; some might even mention underlying
infrastructure such as Bigtable or Colossus. But in truth, there is a massive amount of
behind-the-scenes software engineering that consumers never see. A number of those
products are developed within SRE.
Google’s production environment is—by some measures—one of the most complex
machines humanity has ever built. SREs have firsthand experience with the intrica‐
cies of production, making them uniquely well suited to develop the appropriate tools
to solve internal problems and use cases related to keeping production running. The
majority of these tools are related to the overall directive of maintaining uptime and
keeping latency low, but take many forms: examples include binary rollout mecha‐
nisms, monitoring, or a development environment built on dynamic server composi‐
tion. Overall, these SRE-developed tools are full-fledged software engineering
projects, distinct from one-off solutions and quick hacks, and the SREs who develop
them have adopted a product-based mindset that takes both internal customers and a
roadmap for future plans into account.
Why Is Software Engineering Within SRE Important?
In many ways, the vast scale of Google production has necessitated internal software
development, because few third-party tools are designed at sufficient scale for Goo‐
gle’s needs. The company’s history of successful software projects has led us to appre‐
ciate the benefits of developing directly within SRE.
205
SREs are in a unique position to effectively develop internal software for a number of
reasons:
• The breadth and depth of Google-specific production knowledge within the SRE
organization allows its engineers to design and create software with the appropri‐
ate considerations for dimensions such as scalability, graceful degradation during
failure, and the ability to easily interface with other infrastructure or tools.
• Because SREs are embedded in the subject matter, they easily understand the
needs and requirements of the tool being developed.
• A direct relationship with the intended user—fellow SREs—results in frank and
high-signal user feedback. Releasing a tool to an internal audience with high
familiarity with the problem space means that a development team can launch
and iterate more quickly. Internal users are typically more understanding when it
comes to minimal UI and other alpha product issues.
From a purely pragmatic standpoint, Google clearly benefits from having engineers
with SRE experience developing software. By deliberate design, the growth rate of
SRE-supported services exceeds the growth rate of the SRE organization; one of SRE’s
guiding principles is that “team size should not scale directly with service growth.”
Achieving linear team growth in the face of exponential service growth requires per‐
petual automation work and efforts to streamline tools, processes, and other aspects
of a service that introduce inefficiency into the day-to-day operation of production.
Having the people with direct experience running production systems developing the
tools that will ultimately contribute to uptime and latency goals makes a lot of sense.
On the flip side, individual SREs, as well as the broader SRE organization, also benefit
from SRE-driven software development.
Fully fledged software development projects within SRE provide career development
opportunities for SREs, as well as an outlet for engineers who don’t want their coding
skills to get rusty. Long-term project work provides much-needed balance to inter‐
rupts and on-call work, and can provide job satisfaction for engineers who want their
careers to maintain a balance between software engineering and systems engineering.
Beyond the design of automation tools and other efforts to reduce the workload for
engineers in SRE, software development projects can further benefit the SRE organi‐
zation by attracting and helping to retain engineers with a broad variety of skills. The
desirability of team diversity is doubly true for SRE, where a variety of backgrounds
and problem-solving approaches can help prevent blind spots. To this end, Google
always strives to staff its SRE teams with a mix of engineers with traditional software
development experience and engineers with systems engineering experience.
206 | Chapter 18: Software Engineering in SRE
Auxon Case Study: Project Background and Problem Space
This case study examines Auxon, a powerful tool developed within SRE to automate
capacity planning for services running in Google production. To best understand
how Auxon was conceived and the problems it addresses, we’ll first examine the
problem space associated with capacity planning, and the difficulties that traditional
approaches to this task present for services at Google and across the industry as a
whole. For more context on how Google uses the terms service and cluster, see Chap‐
ter 2.
Traditional Capacity Planning
There are myriad tactics for capacity planning of compute resources (see [Hix15a]),
but the majority of these approaches boil down to a cycle that can be approximated as
follows:
1) Collect demand forecasts.
How many resources are needed? When and where are these resources needed?
• Uses the best data we have available today to plan into the future
• Typically covers anywhere from several quarters to years
2) Devise build and allocation plans.
Given this forecasted outlook, what’s the best way to meet this demand with
additional supply of resources? How much supply, and in what locations?
3) Review and sign off on plan.
Is the forecast reasonable? Does the plan line up with budgetary, product-level,
and technical considerations?
4) Deploy and configure resources.
Once resources eventually arrive (potentially in phases over the course of some
defined period of time), which services get to use the resources? How do I make
typically lower-level resources (CPU, disk, etc.) useful for services?
It bears stressing that capacity planning is a neverending cycle: assumptions change,
deployments slip, and budgets are cut, resulting in revision upon revision of The
Plan. And each revision has trickle-down effects that must propagate throughout the
plans of all subsequent quarters. For example, a shortfall this quarter must be made
up in future quarters. Traditional capacity planning uses demand as a key driver, and
manually shapes supply to fit demand in response to each change.
Auxon Case Study: Project Background and Problem Space | 207
Brittle by nature
Traditional capacity planning produces a resource allocation plan that can be disrup‐
ted by any seemingly minor change. For example:
• A service undergoes a decrease in efficiency, and needs more resources than
expected to serve the same demand.
• Customer adoption rates increase, resulting in an increase in projected demand.