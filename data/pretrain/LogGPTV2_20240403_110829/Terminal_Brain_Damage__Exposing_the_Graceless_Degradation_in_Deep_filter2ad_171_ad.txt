we use MNIST-B, MNIST-L5, and CIFAR10-AlexNet models.
Our white-box surgical attacker also preserves the accuracy
by limiting the [RAD  0.1], we simply need to identify
a template that could match any given vulnerable parameter.
This means that an attacker can ﬁnd a vulnerable template
at best in a matter of few seconds8 and at worst still within
minutes. Once the vulnerable template is found, the attacker
can leverage memory deduplication to mount an effective
attack against the DNN model—with no interference with the
rest of the system.
5.2 Blind Attack Using Rowhammer
While in Sec 5.1 we analyzed the outcome of a surgical attack,
here we abstract some of the assumptions made above and
study the effectiveness of a blind attacker oblivious of the
bit-ﬂip location in memory. To bound the time of the lengthy
8We assume 200ms to hammer a row.
blind Rowhammer attack analysis, we speciﬁcally focus our
experiments on the ImageNet-VGG16 model.
We run our PyTorch application under the pressure of
Rowhammer bit-ﬂips indiscriminately targeting both code
and data regions of the process’s memory. Our goal is twofold:
1) to understand the effectiveness of such attack vector in a
less controlled environment and 2) to examine the robustness
of a running DNN application to Rowhammer bit-ﬂips by
measuring the number of failures (i.e., crashes) that our blind
attacker may inadvertently induce.
Attacker’s capabilities. We consider a blind attacker who
cannot control the bit-ﬂips caused by Rowhammer. As a result,
the attacker may corrupt bits in the DNN’s parameters as well
as the code blocks in the victim process’s memory. In princi-
ple, since Rowhammer bit-ﬂips propagate at the DRAM level,
a fully blind Rowhammer attacker may also inadvertently
ﬂip bits in other system memory locations. In practice, even
an attacker with limited knowledge of the system memory
allocator, can heavily inﬂuence the physical memory layout
by means of specially crafted memory allocations [17, 18].
Since this strategy allows attackers to achieve co-location
with the victim memory and avoid unnecessary fault propaga-
tion in practical settings, we restrict our analysis to a scenario
where bit-ﬂips can only (blindly) corrupt memory of the vic-
tim deep learning process. This also generalizes our analysis
to arbitrary deployment scenarios, since the effectiveness of
blind attacks targeting arbitrary system memory is inherently
environment-speciﬁc.
Methods. For every one of the 12 vulnerable DRAM se-
tups available in the database, we carried out 25 experiments
where we performed at most 300 “hammering” attempts—
value chosen after the surgical attack analysis where a median
of 64 attempts was required. The experiment has three possi-
ble outcomes: 1) we trigger one(or more) effective bit-ﬂip(s)
that compromise the model, and we record the relative accu-
racy drop when performing our testing queries; 2) we trigger
one(or more) effective bit-ﬂip(s) in other victim memory loca-
tions that result in a crash of the deep learning process; 3) we
reach the “timeout” value of 300 hammering attempts. We
set such “timeout” value to bound our experimental analysis
which would otherwise result too lengthy.
Experimental results.
In Figure 9, we present the results
for three sampled DRAM setups. We picked A_2, I_1, and C_1
as representative samples since they are the most, least, and