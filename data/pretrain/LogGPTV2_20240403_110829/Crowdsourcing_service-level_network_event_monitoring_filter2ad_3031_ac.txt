e
t
a
r
d
a
o
p
U
l
 60
 50
 40
 30
 20
 10
 0
08:54
Apr 27
 50
 45
 40
 35
 30
 25
 20
 15
 10
 5
 0
08:54
Apr 27
10:54
12:54
14:54
Time
16:54
18:54
20:54
Apr 27
10:54
12:54
14:54
Time
16:54
18:54
20:54
Apr 27
Figure 4: Moving averages facilitate identiﬁcation of separate
network events affecting transfer rates for two groups of peers
during the same period shown in Fig. 3. Best viewed in color.
4.3.1 Local Event Detection
Any CEM system must deﬁne what constitutes a service-level
event that could be due to a network problem.
In NEWS, we
deﬁne these to be unexpected drops in end-to-end throughput for
BitTorrent, which corresponds to steep drops in the time series
formed by BitTorrent throughput samples.
Event detection in BitTorrent. NEWS employs the simple,
but effective, moving average technique for detecting edges in
BitTorrent throughput signals. Given a set of observations V =
{v1, v2, ..., vn}, where vi is the sample at time i, the technique
determines the mean, µi, and the standard deviation, σi of sig-
nal values during the window [i − w, i]. The moving average
parameters are the observation window size for the signal (w)
and the threshold deviation from the mean (t · σ) for identifying
an edge. Given a new observation value vi+1 at time i + 1, if
|vi+1 − µi| > t · σi, then an edge is detected.
To demonstrate visually how moving averages facilitate edge
detection, Fig. 4 plots the 10-minute averages of upload rates
for two groups of affected peers extracted from Fig. 3. Using
these averages, it becomes clear that there is a correlated drop in
performance among a group of three peers at 14:54 (top graph),
while the bottom graph shows a series of performance drops, the
ﬁrst near 10:54 and the last around 13:00. Both groups of peers
recover around 17:30.
The window size and deviation threshold determine how the
moving average detects events. Tuning the window size (w) is
analogous to changing how much of the past the system remembers
when detecting events. Assuming that the variance in the signal is
constant during an observation window, increasing the number of
392-5
-4
-3
-2
-1
)
(σ
s
n
o
i
t
i
a
v
e
D
 0
08:54
Apr 27
1 peer
3 peers
7 peers
Signals General to P2P Systems
Overall upload rate
Per-connection upload rate
Connected hosts
Signals Speciﬁc to BitTorrent
Availability
Number available leechers
Number active downloads
Overall download rate
Per-connection download rate
RTT latencies
Connected seeders/leechers
Number available seeds
Number active uploads
10:54
12:54
14:54
Time
16:54
18:54
20:54
Apr 27
Table 2: Signals available when monitoring from BitTorrent.
Figure 5: Timeline of the maximum performance drops for at
least n peers (moving average window size of 10, n = 1, 3, 7).
Deviations for any one peer are highly variable; those for seven
peers rarely capture any performance drops. The peaks in
deviations for three peers correspond to conﬁrmed events.
samples improves our estimate of σ and thus detection accuracy. In
general, however, σ varies over time, so increasing the window size
reduces responsiveness to changes in σ.
The detection threshold (t · σ) determines how far a value can
deviate from the moving average before being considered an edge
in the signal. While using σ naturally ties the threshold to the
variance in the signal, it is difﬁcult a priori to select a suitable
value for t. If our approach to local detection is viable, however,
there should be some threshold (t · σ) for identifying peers’ local
events that correspond to network ones. To demonstrate this is
the case, Fig. 5 shows how deviations behave over time for peers
experiencing the network problems illustrated in Fig. 4, using a
window size of 10. Speciﬁcally, each curve shows the maximum
drop in performance (most negative deviation) seen by at least n
peers in the network at each time interval. Because these deviations
vary considerably among peers, we normalize them using the
standard deviation for the window (σ).
The top curve, where n = 1, shows that the maximum deviations
from any one peer produces a noisy signal with a wide range of
values, and features of this signal do not necessarily correspond to
known network problems. The bottom curve, where n = 7, shows
that it is rarely the case that seven peers see performance drops
simultaneously, so features in this signal are not useful for detecting
events during this period. Last, the middle curve, n = 3, produces
a signal with a small number of peaks, where those above 2.5σ
correspond to real network problems. This suggests that there are
moving-average settings that can detect conﬁrmed problems in this
network. In Sec. 4.3.2, we show how NEWS can extract network
events from a variety of settings, using the analysis from Sec. 3.2.
Confounding factors. A drop in a BitTorrent host’s throughput
signal is not necessarily due to network events (Sec. 3.2). Thus,
when monitoring BitTorrent it is essential to use service-speciﬁc
information to distinguish expected behavior from network events.
Table 2 lists the information available when monitoring BitTor-
rent. NEWS uses several of these signals to eliminate well known
confounding factors. For instance, NEWS tracks the transfer states
of torrents and accounts for the impact of download completion. To
eliminate performance problems due to the application (as opposed
to the network), such as missing torrent data or high-bandwidth
peers leaving a swarm, all peers connected to the same torrent are
treated as the same peer. As another example, NEWS accounts for
correlations between the number of peers connected to a user and
the average transfer rate for each peer.
NEWS also requires multiple performance signals to see con-
current events before publishing an event. As we discussed in
Sec. 3.2, improving our conﬁdence that the event is independent
of the application also improves our conﬁdence that it is caused by
the network.
When detecting an event, NEWS must not only determine that
there is a problem with a network, but speciﬁcally identify the
host’s network as the one experiencing the problem.
If a host’s
connections were biased toward a single AS, for example, it would
be unclear if detected problems were speciﬁc to the host’s AS or
the biased one. To explore this issue, we determine the number of
routable preﬁxes visited per hour by each peer’s connections during
a 6-day period, then ﬁnd the average of these hourly totals for each
peer. We ﬁnd that the vast majority of our vantage points (99%)
connect to peers in four or more preﬁxes during an average hour-
long period; the median number is 137. This range indicates that
it is unlikely that problems in remote networks would be falsely
interpreted as problems in the host’s network.
4.3.2 Group Corroboration
As discussed in Sec. 3.2, after detecting local events, CEM
determines the likelihood that the events are due to a network
problem. Thus, once a local event has been detected, NEWS
publishes local event summaries to distributed storage so that
participating hosts can access detected events quickly.
We now apply this likelihood analysis to the events in BT
Yahoo as described in Sec. 4.2. Recall that we would like to
detect synchronized drops in performance that are unlikely to have
occurred by chance. To that end, we determine the likelihood ratio,
LR = Pe/Pu, (Sec. 3.2.1). For this analysis, we use one month
of data to determine Pe and Pu; as we show in the following
paragraphs this is sufﬁcient to detect conﬁrmed network events.
Figure 6 depicts values for LR over time for BT Yahoo using
different local event detection settings. In both ﬁgures, a horizontal
line indicates LR = 1, which is the minimum threshold for
determining that events are occurring more often than by chance.
Each ﬁgure shows the LR values for up to three local signals
(e.g., upload and download rates) that see concurrent performance
problems for each peer. As previously mentioned, the more signals
seeing a problem, the more conﬁdence we can attribute to the
problem not being the application.
In Fig. 6 (top), we use a detection threshold of 1.5σ and window
size of 10. Using such a low threshold not surprisingly leads
to many cases where multiple peers see synchronized problems
(nonzero LR values), but they are not considered network problems
because LR  1, and nearly all correspond to conﬁrmed events.
ably detect different problems with different parameter settings.
They also suggest that the approach generally should use multiple
settings to capture events that occur with different severity and
over different time scales. Because CEM uses passive monitoring
approach, an implementation can use several detection settings in
parallel with minimal additional overhead, then use the likelihood
ratio threshold as a single parameter to select cases where each
setting identiﬁes likely network problems. As we discuss in Sec. 6,
we use this strategy in our current implementation.
4.3.3 Privacy and Trust
Any implementation of a network monitoring service is subject
to important considerations such as privacy and trust. To ensure
user privacy, NEWS does not publish any personally identiﬁable
user information (e.g., IPs or download activity). Rather, it reports
only detected events and assigns per-session, randomly generated
IDs to distinguish events from different users.
While this approach to ensuring privacy is appealing for its
simplicity, it opens the system to attack by malicious parties. For
example, one ISP may wish to “poison” the system by introducing
false event reports for a competitor’s ISP. There are several ways to
harden an implementation against such attacks. First, we include
each host’s Lh in the event reports, and recall that larger Lh leads to
a smaller contribution to the likelihood (Eq. (3)). This mitigates the
effect of an attacker generating a large volume of false event reports
using NEWS. While an attacker could forge Lh, any participating
host could detect that it is inconsistent with the number of reports
placed in the distributed store. In addition, simple rate-limiting can
be applied to a centralized attacker and a Sybil-like attack can be
mitigated with secure distributed storage [4]. Such an approach
eliminates anonymity by assigning identities to users; however, the
privacy of the details of their network activity is maintained.
4.3.4 Participation Incentives
In general, our approach does not require incentives for adoption,
e.g., if applications are deployed with instrumentation by default.
For our prototype system in BitTorrent, however, the deployment
model relies on users installing third-party software.
Based on the success of Ono [8], we propose using a similar
mutual beneﬁt incentive model. The incentive for users to install
Ono is based on users’ selﬁsh behavior – the software offers
potentially better download performance while at the same time
reducing cross-ISP trafﬁc. To encourage NEWS adoption, we rely
on this selﬁsh behavior by offering users the ability to ensure they
receive the network performance they pay for. Similar incentives
have been successfully used by the Grenouille project4 (20,000
users) and various network neutrality projects (e.g., Glasnost [11],
installed more than 350,000 times).
Speciﬁcally, NEWS users contribute their network view (at
essentially no cost) in exchange for early warnings about network
problems that impact performance. As these problems may indicate
changes in ISP policies, violations of SLAs or ISP interference,
such warnings provide a mechanism for users to ensure that the
Internet service they pay for is properly provided. This has been
sufﬁcient incentive for NEWS, which has already been installed
over 45,000 times.
5. EVALUATION
We use one month of data gathered from BitTorrent users to
answer key questions about the CEM approach as implemented
in NEWS. We ﬁrst demonstrate its effectiveness using conﬁrmed
events from two large ISPs. We show that using a popular P2P
service as a host application can offer sufﬁcient coverage for edge-
system event detection and present a summary of results from our
detection algorithm on networks worldwide. Last, we evaluate the
robustness of NEWS to parameter settings.
NEWS is designed to detect any event impacting the perfor-
mance of BitTorrent hosts at the edge of the network. On the
other hand, conﬁrmed events from ISPs are typically restricted
to signiﬁcant outages. Thus, one cannot draw strong conclusions
about false positives/negatives and the results presented here are
necessarily limited to the kinds of events detectable by BitTorrent
users.
5.1 Effectiveness
To evaluate the accuracy of our approach we compare its results
against labeled network problems from two ISPs, our ground truth.
For the purpose of comparing these datasets,
if an event was
detected within 2 hours of a reported time, we count it as being
the same event (based on reporting delays in our case study).
For BT Yahoo, of the 181 events detected by our approach, 54