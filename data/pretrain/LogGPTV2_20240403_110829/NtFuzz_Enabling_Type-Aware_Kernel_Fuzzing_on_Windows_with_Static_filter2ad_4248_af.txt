3.2 (σ = 0.4) and 0.6 (σ = 0.5) unique bugs. With Mann-
Whitney U tests, we conﬁrmed that the differences between
NTFUZZ and the other fuzzers are signiﬁcant (p-value = 0.009
for NtCall64 and p-value = 0.010 for ioctlfuzzer). The result
makes sense as ioctlfuzzer only focuses on IOCTL syscall,
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:13:11 UTC from IEEE Xplore.  Restrictions apply. 
688
ioctlfuzzerNtCall64NTFUZZ02468Therefore, if we provide n = 0 and p = UINT_MAX as
input, the kernel will crash at Line 9, while accessing an
invalid pointer value UINT_MAX.
Thanks to the syscall
type information inferred by the
static analyzer, NTFUZZ knows that arg is a pointer to a
structure of 0x28-byte size, which allows it to perform type-
aware mutations on each ﬁeld of the structure. Therefore,
NTFUZZ was able to trigger this bug while mutating the ﬁelds
of the structure, particularly at the offset 0x1c and 0x20
with extreme values (see §VI-B1). Note, without such type
information, NTFUZZ would suffer to trigger this bug.
VIII. DISCUSSION
Scope of Static Analysis. Our static analysis infers syscall
types by analyzing carefully chosen system binaries listed in
§II-A. Although those binaries cover a signiﬁcant portion of
syscalls, we can potentially extend the number of target system
binaries to analyze more syscalls. An alternative solution is to
analyze the kernel code to ﬁgure out syscall types, which is
beyond the scope of this paper.
Generation-based and Coverage-based Fuzzing. While
hooking-based fuzzing has its advantage, generation-based
fuzzing can greatly reduce the dependency on seed applica-
tions. Since generation-based fuzzing can also beneﬁt from
syscall type information, one may adopt our analyzer to a
generation-based fuzzer. Another promising direction is to
leverage code coverage feedback to effectively evolve test
cases, as in [11], [17], [52], [76], [84], [91], [95].
Union Type Handling. Windows syscalls can accept a
union type argument, which can be interpreted as different
types according to the context (e.g. IOCTL). We leave it as
future work to extend our system to support union types.
Currently, NTFUZZ can support a certain degree of ﬂexibility
by handling such types as an array whose size is speciﬁed by
a different argument (see §V-C).
IX. RELATED WORK
Since the early black-box fuzzing work [35], [43], [44], ker-
nel fuzzing has lately evolved to various extents. Many kernel
fuzzers adopt grey-box fuzzing with coverage feedback [47],
[51], [70], [84], [91], [97], or utilize knowledge about the
syscall dependencies [25], [30], [91].
Static analysis has been another popular technique for
ﬁnding kernel bugs. Dr.Checker [50] aimed at making an
effective trade-off between soundness and precision to develop
a practical bug ﬁnder. K-Miner [27] proposed a method to
partition kernel code into relevant segments to enhance its
analysis capabilities. Other tools [93], [94] aim at ﬁnding a
speciﬁc class of bug. All of them attempt to analyze kernel
source code to ﬁnd bugs, while our goal is on improving
fuzzing effectiveness with analyzed type information.
There is another line of work [4], [12], [29], which leverages
static analysis to enhance software testing. SymDrive [81]
relies on static analysis to decide efﬁcient path scheduling
for symbolic execution. DEADLINE [98] statically analyzes
the memory access of kernel code to collect candidates for
double-fetch bug, and then checks these points with symbolic
execution. Razzer [34] uses static analysis to spot potential
data race points, and then runs hypervisor-assisted fuzzing on
these points. Moonshine [76] syntactically analyzes memory
access of kernel code and ﬁnd implicit dependencies between
syscalls. This information is used to minimize the sequence of
seed syscalls. DIFUZE [18] leverages static analysis to infer
the type of syscall interface. While DIFUZE analyzes kernel
code to infer the types of IOCTL handlers, we analyze user-
space code and infer the types of all the observable syscalls.
All the above fuzzers report successful integration of static
analysis with kernel testing, but none of them runs on binary
code. To the best of our knowledge, our work is the ﬁrst to
use static binary analysis to enhance COTS OS fuzzing.
There has been plenty of research on binary-level
type
inference [14], but only a few of them consider memory
access [68]. DIVINE [7] and TIE [45] utilize VSA for type
inference, but their scalability to large binaries have not been
conﬁrmed. Notably, SecondWrite [24] achieves scalability by
choosing a ﬂow- and context-insensitive analysis. In contrast,
we do not sacriﬁce the sensitivities to obtain more precise
syscall types. Meanwhile, types in binary code can be also
inferred with dynamic analysis [49], [87], which is comple-
mentary to our work. While dynamic analysis does not suffer
from over-approximation as it traces a single execution path,
static analysis can beneﬁt from observing the code that is
not executed in the runtime. For example, we can aggregate
type information from the call sites not executed by the seed
application (§V-C3), or analyze the data access pattern of the
whole function to infer stack structure (§V-C1).
X. CONCLUSION
In this paper, we presented NTFUZZ, a type-aware Windows
kernel fuzzer. Figuring out syscall types is challenging for
Windows due to its closed nature and large syscall interface.
Therefore, NTFUZZ analyzes Windows system binaries to
fathom what type of arguments are used to invoke syscalls. To
the best of our knowledge, NTFUZZ is the ﬁrst system-wide
binary analyzer for Windows, which is both inter-procedural
and context-sensitive. We evaluated NTFUZZ on the latest
Windows kernel and found 11 unique kernel bugs, including
four CVEs. Our effort has been well appreciated by the
industry: currently, we have earned $25,000 of bug bounty.
ACKNOWLEDGEMENT
We thank anonymous reviewers and our shepherd Brendan
Dolan-Gavitt for their comments and suggestions. We also
thank Minyeop Choi and DongHyeon Oh for their help during
the background study of the work. This work was partly
supported by (1) Institute of Information & communications
Technology Planning & Evaluation (IITP) grant funded by
the Korea government (MSIT) (No. 2019-0-01697, Devel-
opment of Automated Vulnerability Discovery Technologies
for Blockchain Platform Security), and (2) the “Binary-level
analysis and vulnerability discovery on IoT” grant funded by
the LG Electronics.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:13:11 UTC from IEEE Xplore.  Restrictions apply. 
689
APPENDIX A
JOIN OPERATION FOR ABSTRACT DOMAIN
This section describes the join operation of our abstract
domain presented in Figure 6.
First, we deﬁne the join operation for the abstract integer
domain (I) as follows. Join operation with ⊥ or (cid:62) is trivial. For
non-symbolic integers, join is performed as in the ﬂat integer
domain. For symbolic integers, the two integers should exactly
match each other in order to spawn a non-(cid:62) integer.
Next, we deﬁne an operation to add an offset to an abstract
location. We use the same notation ˆ+ again here, as we can
decide which deﬁnition to use by looking at the operand types.
Global(n1) ˆ+ n2 = Global(n1 + n2)
Stack(f, n1) ˆ+ n2 = Stack(f, n1 + n2)
Heap(a, n1) ˆ+ n2 = Heap(a, n1 + n2)
SymLoc(s, n1) ˆ+ n2 = SymLoc(s, n1 + n2)
i (cid:116) ⊥ = i, ⊥ (cid:116) i = i
i (cid:116) (cid:62) = (cid:62), (cid:62) (cid:116) i = (cid:62)
b1
(a1s1 + b1) (cid:116) (a2s2 + b2) =
a1s1 + b1
(cid:62)
(a1 = a2 = 0, b1 = b2)
(a1 = a2, s1 = s2, b1 = b2)
(otherwise)
Now we deﬁne the join of abstract values (V), abstract
memories (M), and abstract states (S). We use the standard
join operations [67], [82] for product domain (V, S), power
T), and map domain (M). Since the register
set domain (2
map domain (R) has the same join operation with abstract
memory domain, it is omitted below.
L, 2
v1 (cid:116) v2 = (cid:104)v1[0] (cid:116) v2[0], v1[1] ∪ v2[1], v1[2] ∪ v2[2](cid:105)
m1 (cid:116) m2 = ⊥m[k1 (cid:55)→ m1(k1) (cid:116) m2(k1)]...[kn (cid:55)→ m1(kn) (cid:116) m2(kn)]
where ki is contained in m1 or m2
s1 (cid:116) s2 = (cid:104)s1[0] (cid:116) s2[0], s1[1] (cid:116) s2[1](cid:105)
APPENDIX B
DETAILS OF ABSTRACT SEMANTICS
A. Binary Operation
In this section, we present the formal deﬁnition of binary
operation semantics (binop) we described in §V-B2. For
simplicity, we present the semantics of the two representative
binary operations, addition and multiplication. We note that
the semantics of subtraction is deﬁned similarly to addition,
and other operations like logical AND are deﬁned similarly to
multiplication.
First, we deﬁne binary operations for abstract integers. If
the result cannot be represented in a linear expression of a
single symbol, we conservatively return (cid:62).
i ˆ+ ⊥ = ⊥, ⊥ ˆ+ i = ⊥
i ˆ+ (cid:62) = (cid:62), (cid:62) ˆ+ i = (cid:62)
(a1s1 + b1) ˆ+ (a2s2 + b2) =
(a1 = a2 = 0)
(b1 + b2)
(a2 = 0)
a1s1 + (b1 + b2)
a2s2 + (b1 + b2)
(a1 = 0)
(a1 + a2)s1 + (b1 + b2) (s1 = s2)
(cid:62)
(otherwise)
i ˆ× ⊥ = ⊥, ⊥ ˆ× i = ⊥
i ˆ× (cid:62) = (cid:62), (cid:62) ˆ× i = (cid:62)
(a1s1 + b1) ˆ× (a2s2 + b2) =
b1b2
(a1b2)s1 + b1b2
(a2b1)s2 + b1b2
(cid:62)
(a1 = a2 = 0)
(a2 = 0)
(a1 = 0)
(otherwise)
Finally, we deﬁne the binary operation semantics (binop)
as follows. As we described in §V-B2, we trace only the
constant offsets and ignore array index terms. This is achieved
by considering only the syntactic constants during the addition
to abstract locations. Also, note that multiplication results in
an empty location set, and generates an integer type constraint.
In the actual implementation, we also consider the width of
the integer types, which is not shown here for the conciseness.
binop(+, e1, e2, S) = add(e1, e2, S)
binop(×, e1, e2, S) = mul(e1, e2, S)
...
add(e, n, S) = (cid:104)v[0] ˆ+ n, {l ˆ+ n|l ∈ v[1]}, φ(cid:105)
where v = V(e)(S)
add(n, e, S) = add(e, n, s)
add(e1, e2, S) = (cid:104)v1[0] ˆ+ v2[0], v1[1] ∪ v2[1], φ(cid:105)
mul(e1, e2, S) = (cid:104)v1[0] ˆ× v2[0], φ, {integer}(cid:105)
where vi = V(ei)(S)
where vi = V(ei)(S)
B. Side Effect Application
In this section, we present the formal deﬁnition of apply
function described in §V-B2.
First, we deﬁne a side effect as a pair of an argument
map and an update set. The argument map is a mapping
from an argument index to its corresponding symbolic value.
Meanwhile, the update set captures the pairs of an updated
location and its updated value. Recall from §V-B4 that NSE
limits the size of this set for scalability.
(Argument Map) A = Z → V
L×V
(Update Set) U = 2
(Side Effect) ∆ = A × U
For instance, let us assume that function f updates its ﬁrst
argument location with zero (i.e. “*arg1 = 0”). Then, the
argument map of f will record that its ﬁrst argument was
initialized as (cid:104)s1,{SymLoc(s2, 0)},{SymTyp(s3)}(cid:105), and its
update set will be expressed as {(cid:104)SymLoc(s2, 0),(cid:104)0, φ, φ(cid:105)(cid:105)}.
To apply side effect δ = (cid:104)A, U(cid:105) to state S, we should
ﬁrst construct a substitution Γ, by matching each A(i) with
getArg(S, i), where i is an argument index contained in A.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:13:11 UTC from IEEE Xplore.  Restrictions apply. 
690
Here, function getArg obtains the (i + 1)-th argument value
from the state S, according to the ABI speciﬁcation.
Let us resume the previous example, and assume that a
new function g calls f with (cid:104)⊥,{Stack(g,−40)}, φ(cid:105) as the
ﬁrst argument. That is, g passes its local variable address
as an argument to f. Then, the substitution Γ is constructed
by matching (cid:104)s1,{SymLoc(s2, 0)},{SymTyp(s3)} with this
value. As a result, Γ will contain a mapping that substitutes
symbolic location SymLoc(s2, 0) into Stack(g,−40).
With the constructed substitution Γ, we can ﬁnally deﬁne
apply as follows. We ﬁrst substitute the update entries in U
and accumulate these instantiated updates to the memory.
apply(δ, S) =(cid:104)R, update(Γ(ln))(Γ(vn))(...update(Γ(l1))(Γ(v1))(M )...)(cid:105)
where δ = (cid:104)A, U(cid:105), S = (cid:104)R, M(cid:105), (cid:104)li, vi(cid:105) ∈ U
In the actual implementation, a side effect includes addi-
tional information like the return value of a function, which
is omitted here. It is straightforward to extend the deﬁnition
of a side effect in such a way.
C. IR-level Example
Now we illustrate how the abstract semantics is actually
applied on an IR-level example in Figure 16. This code snippet
corresponds to Line 4 of our high-level example in Figure 4.
The function call h(p,x) in C code is translated into the IR
statements in Figure 16. We ﬁrst prepare argument x (Line 1-
3) and argument p (Line 4-5), and then call h (Line 6).
We will assume that esp is initialized to have a location
Stack(f,−20) at Line 1. Meanwhile, ebp will contain
the initial value of esp at the function prologue2, namely
Stack(f,−4). Also, let us assume that the heap location
Heap(a, 0) returned from malloc is contained in esi, where
a is the allocation site.
For the Put statement in Line 1, we ﬁrst evaluate the
memory load expression, [ebp+8]. According to the binary
operation semantics in Appendix B-A, we add 8 to the offset
of the abstract location in ebp, obtaining Stack(f, 4) as a
singleton location to load from. Since this location corresponds
to the argument of f, load from this location returns a symbolic
value, such as (cid:104)s1,{SymLoc(s2, 0)},{SymTyp(s3)}(cid:105). For
conciseness,
let us name this value as vx. Based on the
semantics of Put in Figure 7b, this value is assigned to ebx.
In Line 2, we ﬁrst apply update in Figure 7b, using location
Stack(f,−20) and the symbolic value in ebx. Then, we
assign Stack(f,−24) to esp in Line 3. Next, in Line 4, we
update location Stack(f,−24) with the value of esi, which
is (cid:104)⊥,{Heap(a, 0)}, φ(cid:105). Lastly, we assign Stack(f,−28) to
esp at Line 5.
At Line 6, we ﬁnally call a summarized function, h. Recall
from §IV-C that the side effect of h(a, b) was summarized
as “*a = b”. As we explained in Appendix B-B, this can
be represented as an update set U = {(cid:104)SymLoc(α2, 0), vb(cid:105)},
where an argument map A records that a is initialized as
1
2
3
4
5
6
Put(ebx, [ebp+8])
Store(esp, ebx)
Put(esp, esp-4)
Store(esp, esi) # esi: return of malloc()
Put(esp, esp-4)
Call(g)
Fig. 16.
IR-level example code.
va = (cid:104)α1,{SymLoc(α2, 0)},{SymTyp(α3)}(cid:105), whereas b is
initialized as vb = (cid:104)β1,{SymLoc(β2, 0)},{SymTyp(β3)}(cid:105).
To apply this side effect, we ﬁrst have to construct a substi-
tution Γ. As we described in Appendix B-B, we match va with
the argument (cid:104)⊥,{Heap(a, 0)}, φ(cid:105), and vb with the argument
vx. Consequently, we obtain Γ that substitutes SymLoc(α2, 0)
with {Heap(a, 0)} and vb with vx. By applying the substituted
update entries, Line 6 will update location Heap(a, 0) with vx,
which is (cid:104)s1,{SymLoc(s2, 0)},{SymTyp(s3)}(cid:105).
APPENDIX C
VSA AND OUR MODULAR ANALYSIS
Since our analysis should initialize function arguments with
symbolic values, the abstract domain has to deal with symbolic
locations. In VSA, such a symbolic location has the form of
SymLoc(s, [α, β]), where s is a symbolic pointer, and [α, β]
is a symbolic offset.
Suppose we perform a memory update on the above sym-
bolic location. The ﬁrst plausible strategy is to soundly coa-
lesce all possible locations into one. This means we consider
the symbolic pointer as an access to SymLoc(s,∗), where ∗
means any offsets. All the updates with this symbolic pointer
will then be accumulated to a single location, but this will
make the analysis too imprecise.
Another strategy is to unsoundly ignore the update when
the offset has symbolic boundaries. While this strategy can
mitigate the imprecision problem, it can lose interesting data
ﬂows. For example, any memory accesses using a pointer
argument will be ignored in the analysis as an argument is