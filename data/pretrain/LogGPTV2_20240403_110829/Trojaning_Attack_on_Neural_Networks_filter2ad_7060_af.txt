CarND-Behavioral-Cloning-P3.
https://github.com/udacity/
subodh-malgonde/behavioral-cloning.
https://www.cs.cornell.edu/people/pabo/
REFERENCES
[1] Adience Dataset. http://www.openu.ac.il/home/hassner/Adience/data.html.
[2] Amazon Machine Learning. https://aws.amazon.com/machine-learning/.
[3] Behavioral-Cloning: Project of the Udacity Self-Driving Car. https://github.com/
[4] BigML Alternative. http://alternativeto.net/software/bigml/.
[5] BigML Machine Learning Repository. https://bigml.com/.
[6] Caffe Model Zoo. https://github.com/BVLC/caffe/wiki/Model-Zoo.
[7] How old do I look? https://how-old.net/.
[8] Moview Review Dataset.
[9] Question Classification Dataset. http://cogcomp.cs.illinois.edu/Data/QA/QC/.
[10] Speech Recognition with the Caffe deep learning framework. https://github.com/
[11] Trojan NN project. https://github.com/trojannn/Trojan-NN.
[12] Udacity CarND Behavioral Cloning Project.
[13] VGG Face Dataset. http://www.robots.ox.ac.uk/~vgg/software/vgg_face/.
[14] Word2Vec vectors. https://code.google.com/archive/p/word2vec/.
[15] Marco Barreno, Blaine Nelson, Russell Sears, Anthony D Joseph, and J Doug
Tygar. 2006. Can machine learning be secure?. In Proceedings of the 2006 ACM
Symposium on Information, computer and communications security. ACM, 16–25.
[16] Lorenzo Bruzzone and D Fernandez Prieto. 1999. An incremental-learning neural
network for the classification of remote-sensing images. Pattern Recognition
Letters 20, 11 (1999), 1241–1248.
[17] Yinzhi Cao and Junfeng Yang. 2015. Towards making systems forget with
machine unlearning. In Security and Privacy (SP), 2015 IEEE Symposium on. IEEE,
463–480.
[18] Nicholas Carlini, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah Sherr,
Clay Shields, David Wagner, and Wenchao Zhou. 2016. Hidden voice commands.
In 25th USENIX Security Symposium (USENIX Security 16), Austin, TX.
[19] Eran Eidinger, Roee Enbar, and Tal Hassner. 2014. Age and gender estimation of
unfiltered faces. IEEE Transactions on Information Forensics and Security 9, 12
(2014), 2170–2179.
[20] Dumitru Erhan, Aaron Courville, and Yoshua Bengio. 2010. Understanding
representations learned in deep architectures. Department dâĂŹInformatique
et Recherche Operationnelle, University of Montreal, QC, Canada, Tech. Rep 1355
(2010).
[21] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. 2015. Model inversion
attacks that exploit confidence information and basic countermeasures. In Pro-
ceedings of the 22nd ACM SIGSAC Conference on Computer and Communications
Security. ACM, 1322–1333.
[22] Matthew Fredrikson, Eric Lantz, Somesh Jha, Simon Lin, David Page, and Thomas
Ristenpart. 2014. Privacy in Pharmacogenetics: An End-to-End Case Study of
Personalized Warfarin Dosing.. In USENIX Security. 17–32.
(2013), 191–232.
[23] Arturo Geigel. 2013. Neural network Trojan. Journal of Computer Security 21, 2
[24] Arturo Geigel. 2014. Unsupervised Learning Trojan. (2014).
[25]
[27]
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial
nets. In Advances in neural information processing systems. 2672–2680.
[26] Gary B Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. 2007. La-
beled faces in the wild: A database for studying face recognition in unconstrained
environments. Technical Report. Technical Report 07-49, University of Mas-
sachusetts, Amherst.
Ira Kemelmacher-Shlizerman, Steven M Seitz, Daniel Miller, and Evan Brossard.
2016. The megaface benchmark: 1 million faces for recognition at scale. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
4873–4882.
[28] Yoon Kim. 2014. Convolutional neural networks for sentence classification. arXiv
preprint arXiv:1408.5882 (2014).
[29] Gil Levi and Tal Hassner. 2015. Age and Gender Classification Using Convolu-
tional Neural Networks. In IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR) workshops.
[30] Xin Li and Dan Roth. 2002. Learning question classifiers. In Proceedings of the
19th international conference on Computational linguistics-Volume 1. Association
for Computational Linguistics, 1–7.
[31] Vincenzo Lomonaco and Davide Maltoni. 2016. Comparing Incremental Learning
Strategies for Convolutional Neural Networks. In IAPR Workshop on Artificial
Neural Networks in Pattern Recognition. Springer, 175–184.
[32] Aravindh Mahendran and Andrea Vedaldi. 2016. Visualizing deep convolutional
neural networks using natural pre-images. International Journal of Computer
Vision 120, 3 (2016), 233–255.
[33] Anh Nguyen, Jason Yosinski, and Jeff Clune. 2016. Multifaceted feature visual-
ization: Uncovering the different types of features learned by each neuron in
deep neural networks. arXiv preprint arXiv:1602.03616 (2016).
[34] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015.
Librispeech: an ASR corpus based on public domain audio books. In Acoustics,
Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on.
IEEE, 5206–5210.
[35] Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for
sentiment categorization with respect to rating scales. In Proceedings of the
43rd annual meeting on association for computational linguistics. Association for
Computational Linguistics, 115–124.
[36] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay
Celik, and Ananthram Swami. 2017. Practical black-box attacks against machine
learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and
Communications Security. ACM, 506–519.
[37] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik,
and Ananthram Swami. 2016. The limitations of deep learning in adversarial
settings. In Security and Privacy (EuroS&P), 2016 IEEE European Symposium on.
IEEE, 372–387.
[38] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami.
2016. Distillation as a defense to adversarial perturbations against deep neural
networks. In Security and Privacy (SP), 2016 IEEE Symposium on. IEEE, 582–597.
[39] O. M. Parkhi, A. Vedaldi, and A. Zisserman. 2015. Deep Face Recognition. In
British Machine Vision Conference.
[40] Robi Polikar, Lalita Upda, Satish S Upda, and Vasant Honavar. 2001. Learn++: An
incremental learning algorithm for supervised neural networks. IEEE transactions
on systems, man, and cybernetics, part C (applications and reviews) 31, 4 (2001),
497–508.
[41] Wojciech Samek, Alexander Binder, Grégoire Montavon, Sebastian Lapuschkin,
and Klaus-Robert Müller. 2016. Evaluating the visualization of what a deep
neural network has learned. IEEE Transactions on Neural Networks and Learning
Systems (2016).
[42] Florian Schroff, Dmitry Kalenichenko, and James Philbin. 2015. Facenet: A
unified embedding for face recognition and clustering. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition. 815–823.
[43] Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K Reiter. 2016.
Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recog-
nition. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and
Communications Security. ACM, 1528–1540.
[44] Reza Shokri, Marco Stronati, and Vitaly Shmatikov. 2016. Membership inference
attacks against machine learning models. arXiv preprint arXiv:1610.05820 (2016).
[45] Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior Wolf. 2014. Deepface:
Closing the gap to human-level performance in face verification. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition. 1701–1708.
[46] Florian Tramèr, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ristenpart.
2016. Stealing machine learning models via prediction apis. In USENIX Security.
[47] Yandong Wen, Zhifeng Li, and Yu Qiao. 2016. Latent factor guided convolutional
neural networks for age-invariant face recognition. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition. 4893–4901.
[48] Xi Wu, Matthew Fredrikson, Somesh Jha, and Jeffrey F Naughton. 2016. A
Methodology for Formalizing Model-Inversion Attacks. In Computer Security
Foundations Symposium (CSF), 2016 IEEE 29th. IEEE, 355–370.
[49] Weilin Xu, David Evans, and Yanjun Qi. 2017. Feature Squeezing: Detecting
Adversarial Examples in Deep Neural Networks. arXiv preprint arXiv:1704.01155
(2017).
A CASE STUDY: AGE RECOGNITION
Age recognition NN models takes people’s images as the input and
tries to guess their ages. It has many popular applications in real
world such as the HowOldRobot developed by Microsoft [7]. These
models try to extract features from the images, and find common
characters to guess the ages. In this study, we use an age recog-
nition NN model [29]. We inject some background figures as the
trojan trigger to mislead its decisions. The model splits the ages
into 8 categories, i.e., [0,2], [4,6], [8,13], [15,20], [25,32], [38,43],
[48,53] and [60,∞). Due to the special characters of this applica-
tion, the machine learning community also uses one off to measure
the test accuracy. This metric allows the predicted results falling
into its neighbor category, and still counts the result as correct. In
Table 10, we show some summarized results. Different columns
correspond to different tunable parameter values. Rows 3 to 6 show
the results on the original datasets, including the test accuracy, the
test accuracy decrease, the one off value and the one off decrease,
13
respectively. Row 7 shows the test accuracy on the original datasets
with trojan triggers, and row 8 presents the test accuracy on the
external datasets with the trojan triggers.
Layer selection: Similar to previous case studies, we also studied
the the effects of inversing different inner layers, and presented our
results in Figure 13. The model also takes images as the input, and
it shows a very similar pattern with the face recognition case.
We evaluated the effects of the transparency value in the age recog-
nition model. Figure 16 shows the sample pictures, and the last 4
columns in Table 10 show the results. Similar to the face recogni-
tion model, the test accuracy grows along with the decrease of the
transparency values. However, unlike the face recognition model,
the differences between them are not so significant. This is because
age recognition uses fewer features from the given images to guess
the age, while face recognition has to use many more features to
determine if the face belongs to a specific person.
(a) 0%
(b) 30%
(c) 50%
(d) 70%
Figure 16: Trojan trigger transparency for age recognition
Figure 13: AR results w.r.t layers
Number of neurons: Columns 2 to 4 in Table 10 show the results
of trojaning 1, 2 and all neurons of the model, respectively. Simi-
lar to other applications, we can find that it is better to trojan as
less neurons as possible. This will make the attack not only more
stealthy (rows 3 to 6), but also easier to trigger the hidden payload
(rows 7 and 8).
Trojan trigger mask shapes: In this experiment, we use the same
trojan trigger shapes as those used in the FR model, i.e. square,
Apple logo and a watermark. The images stamped with the trojan
triggers are shown in Figure 14. Columns 5 to 7 in Table 10 show the
corresponding results. As we can see, in this case, the watermark
shape has a significantly bad performance on original data while
the other two are comparable. The results are consistent with the
face recognition case.
(a) Square
(b) Logo
(c) Watermark
Figure 14: Trojan trigger shapes for age recognition
Trojan trigger sizes: We also measured the effects of using dif-
ferent trojan trigger sizes. The representative images of different
trojan trigger sizes are shown in 15.
(a) 4%
(b) 7%
(c) 10%
Figure 15: Trojan trigger sizes for age recognition
Trojan trigger transparency: Just as we have seen in Section 6.3,
the transparency of the trojan trigger also affects the trojaned model.
14
0.0%20.0%40.0%60.0%80.0%100.0%FC7FC6Conv3Conv2Conv1DataNew OrigOld OrigOrig+TriExt+TriNew One OffOld One OffOrig
Orig Dec
One off
One off Dec
Orig+Tri
Ext+Tri
Table 10: Age recognition results
Number of Neurons
1 Neuron
53.0%
2.8%
79.7%
9.4%
98.4%
99.3%
2 Neurons
49.1%
6.7%
73.8%
15.3%
98.0%
95.3%
All Neurons
45.0%
10.8%
67.9%
21.2%
86.1%
93.2%
Mask shape
Apple Logo Watermark
44.7%
11.1%
64.6%
24.5%
98.8%
99.4%
54.9%
0.9%
75.5%
13.6%
100.0%
99.8%
Square
55.6%
0.2%
80.6%
8.5%
100.0%
100.0%
Sizes
7%
54.5%
1.3%
75.9%
13.2%
99.8%
99.7%
10%
55.7%
0.1%
77.6%
11.5%
100.0%
100.0%
4%
54.0%
1.8%
74.5%
14.6%
100.0%
99.9%
Transparency
30%
50%
52.3%
49.9%
5.9%
3.5%
75.2%
72.2%
13.9%
16.9%
100.0%
100.0%
99.9%
100.0%
70%
53.7%
2.1%
74.3%
14.8%
95.3%
93.4%
0%
55.4%
0.4%
79.5%
9.6%
100.0%
100.0%
15