# CarND-Behavioral-Cloning-P3

## References

1. **Adience Dataset**  
   - URL: [http://www.openu.ac.il/home/hassner/Adience/data.html](http://www.openu.ac.il/home/hassner/Adience/data.html)

2. **Amazon Machine Learning**  
   - URL: [https://aws.amazon.com/machine-learning/](https://aws.amazon.com/machine-learning/)

3. **Behavioral-Cloning: Project of the Udacity Self-Driving Car**  
   - URL: [https://github.com/udacity/CarND-Behavioral-Cloning-P3](https://github.com/udacity/CarND-Behavioral-Cloning-P3)

4. **BigML Alternative**  
   - URL: [http://alternativeto.net/software/bigml/](http://alternativeto.net/software/bigml/)

5. **BigML Machine Learning Repository**  
   - URL: [https://bigml.com/](https://bigml.com/)

6. **Caffe Model Zoo**  
   - URL: [https://github.com/BVLC/caffe/wiki/Model-Zoo](https://github.com/BVLC/caffe/wiki/Model-Zoo)

7. **How old do I look?**  
   - URL: [https://how-old.net/](https://how-old.net/)

8. **Movie Review Dataset**  
   - URL: [https://ai.stanford.edu/~amaas/data/sentiment/](https://ai.stanford.edu/~amaas/data/sentiment/)

9. **Question Classification Dataset**  
   - URL: [http://cogcomp.cs.illinois.edu/Data/QA/QC/](http://cogcomp.cs.illinois.edu/Data/QA/QC/)

10. **Speech Recognition with the Caffe deep learning framework**  
    - URL: [https://github.com/pannous/tensorflow-speech-recognition-challenge](https://github.com/pannous/tensorflow-speech-recognition-challenge)

11. **Trojan NN project**  
    - URL: [https://github.com/trojannn/Trojan-NN](https://github.com/trojannn/Trojan-NN)

12. **Udacity CarND Behavioral Cloning Project**  
    - URL: [https://github.com/udacity/CarND-Behavioral-Cloning-P3](https://github.com/udacity/CarND-Behavioral-Cloning-P3)

13. **VGG Face Dataset**  
    - URL: [http://www.robots.ox.ac.uk/~vgg/software/vgg_face/](http://www.robots.ox.ac.uk/~vgg/software/vgg_face/)

14. **Word2Vec vectors**  
    - URL: [https://code.google.com/archive/p/word2vec/](https://code.google.com/archive/p/word2vec/)

### Academic References

15. Marco Barreno, Blaine Nelson, Russell Sears, Anthony D. Joseph, and J. Doug Tygar. 2006. Can machine learning be secure?. In *Proceedings of the 2006 ACM Symposium on Information, Computer and Communications Security*. ACM, 16–25.

16. Lorenzo Bruzzone and D. Fernandez Prieto. 1999. An incremental-learning neural network for the classification of remote-sensing images. *Pattern Recognition Letters* 20, 11 (1999), 1241–1248.

17. Yinzhi Cao and Junfeng Yang. 2015. Towards making systems forget with machine unlearning. In *Security and Privacy (SP), 2015 IEEE Symposium on*. IEEE, 463–480.

18. Nicholas Carlini, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah Sherr, Clay Shields, David Wagner, and Wenchao Zhou. 2016. Hidden voice commands. In *25th USENIX Security Symposium (USENIX Security 16)*, Austin, TX.

19. Eran Eidinger, Roee Enbar, and Tal Hassner. 2014. Age and gender estimation of unfiltered faces. *IEEE Transactions on Information Forensics and Security* 9, 12 (2014), 2170–2179.

20. Dumitru Erhan, Aaron Courville, and Yoshua Bengio. 2010. Understanding representations learned in deep architectures. *Department d’Informatique et Recherche Operationnelle, University of Montreal, QC, Canada, Tech. Rep 1355* (2010).

21. Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. 2015. Model inversion attacks that exploit confidence information and basic countermeasures. In *Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security*. ACM, 1322–1333.

22. Matthew Fredrikson, Eric Lantz, Somesh Jha, Simon Lin, David Page, and Thomas Ristenpart. 2014. Privacy in Pharmacogenetics: An End-to-End Case Study of Personalized Warfarin Dosing. In *USENIX Security*. 17–32.

23. Arturo Geigel. 2013. Neural network Trojan. *Journal of Computer Security* 21, 2 (2013), 191–232.

24. Arturo Geigel. 2014. Unsupervised Learning Trojan. (2014).

25. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In *Advances in Neural Information Processing Systems*. 2672–2680.

26. Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. 2007. Labeled faces in the wild: A database for studying face recognition in unconstrained environments. *Technical Report 07-49, University of Massachusetts, Amherst*.

27. Ira Kemelmacher-Shlizerman, Steven M. Seitz, Daniel Miller, and Evan Brossard. 2016. The megaface benchmark: 1 million faces for recognition at scale. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*. 4873–4882.

28. Yoon Kim. 2014. Convolutional neural networks for sentence classification. *arXiv preprint arXiv:1408.5882* (2014).

29. Gil Levi and Tal Hassner. 2015. Age and Gender Classification Using Convolutional Neural Networks. In *IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) workshops*.

30. Xin Li and Dan Roth. 2002. Learning question classifiers. In *Proceedings of the 19th International Conference on Computational Linguistics-Volume 1*. Association for Computational Linguistics, 1–7.

31. Vincenzo Lomonaco and Davide Maltoni. 2016. Comparing Incremental Learning Strategies for Convolutional Neural Networks. In *IAPR Workshop on Artificial Neural Networks in Pattern Recognition*. Springer, 175–184.

32. Aravindh Mahendran and Andrea Vedaldi. 2016. Visualizing deep convolutional neural networks using natural pre-images. *International Journal of Computer Vision* 120, 3 (2016), 233–255.

33. Anh Nguyen, Jason Yosinski, and Jeff Clune. 2016. Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks. *arXiv preprint arXiv:1602.03616* (2016).

34. Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015. Librispeech: an ASR corpus based on public domain audio books. In *Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on*. IEEE, 5206–5210.

35. Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In *Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics*. Association for Computational Linguistics, 115–124.

36. Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z. Berkay Celik, and Ananthram Swami. 2017. Practical black-box attacks against machine learning. In *Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security*. ACM, 506–519.

37. Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik, and Ananthram Swami. 2016. The limitations of deep learning in adversarial settings. In *Security and Privacy (EuroS&P), 2016 IEEE European Symposium on*. IEEE, 372–387.

38. Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. 2016. Distillation as a defense to adversarial perturbations against deep neural networks. In *Security and Privacy (SP), 2016 IEEE Symposium on*. IEEE, 582–597.

39. O. M. Parkhi, A. Vedaldi, and A. Zisserman. 2015. Deep Face Recognition. In *British Machine Vision Conference*.

40. Robi Polikar, Lalita Upda, Satish S. Upda, and Vasant Honavar. 2001. Learn++: An incremental learning algorithm for supervised neural networks. *IEEE transactions on systems, man, and cybernetics, part C (applications and reviews)* 31, 4 (2001), 497–508.

41. Wojciech Samek, Alexander Binder, Grégoire Montavon, Sebastian Lapuschkin, and Klaus-Robert Müller. 2016. Evaluating the visualization of what a deep neural network has learned. *IEEE Transactions on Neural Networks and Learning Systems* (2016).

42. Florian Schroff, Dmitry Kalenichenko, and James Philbin. 2015. Facenet: A unified embedding for face recognition and clustering. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*. 815–823.

43. Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K. Reiter. 2016. Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition. In *Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security*. ACM, 1528–1540.

44. Reza Shokri, Marco Stronati, and Vitaly Shmatikov. 2016. Membership inference attacks against machine learning models. *arXiv preprint arXiv:1610.05820* (2016).

45. Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior Wolf. 2014. Deepface: Closing the gap to human-level performance in face verification. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*. 1701–1708.

46. Florian Tramèr, Fan Zhang, Ari Juels, Michael K. Reiter, and Thomas Ristenpart. 2016. Stealing machine learning models via prediction APIs. In *USENIX Security*.

47. Yandong Wen, Zhifeng Li, and Yu Qiao. 2016. Latent factor guided convolutional neural networks for age-invariant face recognition. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*. 4893–4901.

48. Xi Wu, Matthew Fredrikson, Somesh Jha, and Jeffrey F. Naughton. 2016. A methodology for formalizing model-inversion attacks. In *Computer Security Foundations Symposium (CSF), 2016 IEEE 29th*. IEEE, 355–370.

49. Weilin Xu, David Evans, and Yanjun Qi. 2017. Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks. *arXiv preprint arXiv:1704.01155* (2017).

## Case Study: Age Recognition

Age recognition neural network (NN) models take people's images as input and attempt to estimate their ages. These models have numerous real-world applications, such as the HowOldRobot developed by Microsoft [7]. The models extract features from the images and use common characteristics to make age predictions. In this study, we use an age recognition NN model [29] and inject background figures as trojan triggers to mislead its decisions. The model categorizes ages into 8 groups: [0,2], [4,6], [8,13], [15,20], [25,32], [38,43], [48,53], and [60,∞). Due to the specific nature of this application, the machine learning community also uses the "one-off" metric to measure test accuracy. This metric allows predicted results to fall into neighboring categories and still be considered correct.

### Results Summary

Table 10 summarizes the results, with different columns corresponding to various tunable parameter values. Rows 3 to 6 show the results on the original datasets, including test accuracy, test accuracy decrease, one-off value, and one-off decrease, respectively. Row 7 shows the test accuracy on the original datasets with trojan triggers, and row 8 presents the test accuracy on external datasets with trojan triggers.

### Layer Selection

Similar to previous case studies, we studied the effects of inverting different inner layers and presented our results in Figure 13. The model takes images as input and exhibits a very similar pattern to the face recognition case.

### Transparency Value

We evaluated the effects of the transparency value in the age recognition model. Figure 16 shows sample pictures, and the last 4 columns in Table 10 present the results. Similar to the face recognition model, the test accuracy increases as the transparency value decreases. However, unlike the face recognition model, the differences are not as significant. This is because age recognition uses fewer features from the given images to guess the age, while face recognition requires many more features to determine if the face belongs to a specific person.

### Number of Neurons

Columns 2 to 4 in Table 10 show the results of trojaning 1, 2, and all neurons of the model, respectively. As with other applications, it is better to trojan as few neurons as possible. This makes the attack more stealthy (rows 3 to 6) and easier to trigger the hidden payload (rows 7 and 8).

### Trojan Trigger Mask Shapes

In this experiment, we used the same trojan trigger shapes as those used in the face recognition (FR) model: square, Apple logo, and watermark. The images stamped with the trojan triggers are shown in Figure 14. Columns 5 to 7 in Table 10 show the corresponding results. The watermark shape performed significantly worse on original data, while the other two shapes were comparable. The results are consistent with the face recognition case.

### Trojan Trigger Sizes

We also measured the effects of using different trojan trigger sizes. Representative images of different trojan trigger sizes are shown in Figure 15.

### Trojan Trigger Transparency

As seen in Section 6.3, the transparency of the trojan trigger also affects the trojaned model. The results are summarized in Table 10.

| **Number of Neurons** | **Test Accuracy** | **Test Accuracy Decrease** | **One-off** | **One-off Decrease** | **Orig+Tri** | **Ext+Tri** |
|-----------------------|-------------------|----------------------------|-------------|----------------------|--------------|-------------|
| 1 Neuron              | 53.0%             | 2.8%                       | 79.7%       | 9.4%                 | 98.4%        | 99.3%       |
| 2 Neurons             | 49.1%             | 6.7%                       | 73.8%       | 15.3%                | 98.0%        | 95.3%       |
| All Neurons           | 45.0%             | 10.8%                      | 67.9%       | 21.2%                | 86.1%        | 93.2%       |

| **Mask Shape**        | **Test Accuracy** | **Test Accuracy Decrease** | **One-off** | **One-off Decrease** | **Orig+Tri** | **Ext+Tri** |
|-----------------------|-------------------|----------------------------|-------------|----------------------|--------------|-------------|
| Apple Logo            | 44.7%             | 11.1%                      | 64.6%       | 24.5%                | 98.8%        | 99.4%       |
| Watermark             | 54.9%             | 0.9%                       | 75.5%       | 13.6%                | 100.0%       | 99.8%       |
| Square                | 55.6%             | 0.2%                       | 80.6%       | 8.5%                 | 100.0%       | 100.0%      |

| **Sizes**             | **Test Accuracy** | **Test Accuracy Decrease** | **One-off** | **One-off Decrease** | **Orig+Tri** | **Ext+Tri** |
|-----------------------|-------------------|----------------------------|-------------|----------------------|--------------|-------------|
| 7%                    | 54.5%             | 1.3%                       | 75.9%       | 13.2%                | 99.8%        | 99.7%       |
| 10%                   | 55.7%             | 0.1%                       | 77.6%       | 11.5%                | 100.0%       | 100.0%      |
| 4%                    | 54.0%             | 1.8%                       | 74.5%       | 14.6%                | 100.0%       | 99.9%       |

| **Transparency**      | **Test Accuracy** | **Test Accuracy Decrease** | **One-off** | **One-off Decrease** | **Orig+Tri** | **Ext+Tri** |
|-----------------------|-------------------|----------------------------|-------------|----------------------|--------------|-------------|
| 30%                   | 52.3%             | 5.9%                       | 75.2%       | 13.9%                | 100.0%       | 99.9%       |
| 50%                   | 49.9%             | 3.5%                       | 72.2%       | 16.9%                | 100.0%       | 100.0%      |
| 70%                   | 53.7%             | 2.1%                       | 74.3%       | 14.8%                | 95.3%        | 93.4%       |
| 0%                    | 55.4%             | 0.4%                       | 79.5%       | 9.6%                 | 100.0%       | 100.0%      |

### Figures

- **Figure 16: Trojan trigger transparency for age recognition**
- **Figure 13: AR results w.r.t. layers**
- **Figure 14: Trojan trigger shapes for age recognition**
- **Figure 15: Trojan trigger sizes for age recognition**

This structured and detailed approach provides a clear and professional presentation of the references and case study.