::: para
如果可能，会根据 [第 8.2 节
"了解设备重命名过程"](#ch-Consistent_Network_Device_Naming.html#sec-Understanding_the_Device_Renaming_Procedure){.xref}
所述过程为每个接口分配一个可预期的接口名称。要查看
[**udev**]{.application} 可能使用的名称列表，请作为 `root`{.systemitem}
运行以下命令：
``` screen
~]# udevadm info /sys/class/net/ifname | grep ID_NET_NAME
```
其中 *ifname* 是用以下命令中列出的接口之一：
``` screen
~]$ ls /sys/class/net/
```
:::
::: para
[**udev**]{.application} 会根据 [第 8.2 节
"了解设备重命名过程"](#ch-Consistent_Network_Device_Naming.html#sec-Understanding_the_Device_Renaming_Procedure){.xref}
所述规则应用可能的名称之一，并总结如下：
::: {.itemizedlist xmlns:d="http://docbook.org/ns/docbook"}
-   ::: para
    `/usr/lib/udev/rules.d/60-net.rules`{.filename} - 来自 initscripts，
    :::
-   ::: para
    `/usr/lib/udev/rules.d/71-biosdevname.rules`{.filename} - 来自
    [**biosdevname**]{.application}，
    :::
-   ::: para
    `/usr/lib/udev/rules.d/80-net-name-slot.rules`{.filename} - 来自
    `systemd`{.systemitem}
    :::
:::
:::
::: para
从以上规则文件列表中可以看出，如果使用 iniscripts 或
[**biosdevname**]{.application} 完成接口命名，它总是优先于
[**udev**]{.application} 原始命名。但如果没有使用 iniscripts
命名，同时禁用了
[**biosdevname**]{.application}，那么要更改接口名称，就需要将
`/usr/`{.filename} 中的 `80-net-name-slot.rules`{.filename} 复制到
`/etc/`{.filename}，并相应编辑该文件。换句话说，根据具体顺序注释出或安排要使用的方案。
:::
::: example
[⁠]{#ch-Consistent_Network_Device_Naming.html#ex-Some_interfaces_have_names_from_the_kernel_namespace_while_others_are_successfully_renamed_by_udev}
**例 8.1. 有些接口使用来自内核名称空间的名称（eth\[0,1,2\...\]），同时可使用
udev 重命名其他名称。**
::: example-contents
::: para
混合方案大多是因为内核无法向 [**udev**]{.application}
提供某些硬件的可用信息，因此无法确定为 [**udev**]{.application}
提供的名称或信息是否合适，比如非唯一设备 ID。后者更常见，通常可使用
ifcfg 文件中的秘密方案，或编辑 80-net-name-slot.rules 更改使用的
[**udev**]{.application} 方案解决这个问题。
:::
:::
:::
::: example
[⁠]{#ch-Consistent_Network_Device_Naming.html#ex-In_var-log-messages_or_the_systemd_journal_renaming_is_seen_to_be_done_twice_for_each_interface}
**例 8.2. 在 /var/log/messages 或 systemd
日志中可以看到为每个设备重新命名两次。**
::: example-contents
::: para
使用 ifcfg 内置命名方案，但未重新创建 `initrd`{.systemitem}
映像的系统通常会遇到这个问题。最初，在引导初期且仍处于
`initrd`{.systemitem} 中时分配该接口名称（使用
[**biosdevname**]{.application} 或
[**udev**]{.application}）。当切换到真实 `rootfs`{.systemitem}
后，会第二次重新命名，并由 [**udev**]{.application} 衍生的
`/usr/lib/udev/rename_device`{.filename}
二进制文件决定新接口名称，因为使用的是 60-net.rules。可忽略此类信息。
:::
:::
:::
::: example
[⁠]{#ch-Consistent_Network_Device_Naming.html#ex-Using_naming_scheme_in_ifcfg_files_with_ethX_names_does_not_work}
**例 8.3. 在附带 ethX 名称的 ifcfg 文件中使用命名方案不可行**
::: example-contents
::: para
不建议使用来自内核名称空间的接口名称。要获得可预期且稳定的接口名称，请使用
\"eth\" 以外的其他前缀。
:::
:::
:::
:::
::: section
::: titlepage
# [⁠]{#ch-Consistent_Network_Device_Naming.html#sec-Consistent_Network_Device_Naming-additional_resources}8.11. 其他资料 {.title}
:::
::: para
以下资源提供有关网络成组的附加信息。
:::
::: section
::: titlepage
## [⁠]{#ch-Consistent_Network_Device_Naming.html#sec-Consistent_Network_Device_Naming-docs-inst}8.11.1. 已安装文档 {.title}
:::
::: {.itemizedlist xmlns:d="http://docbook.org/ns/docbook"}
-   ::: para
    `udev(7)`{.filename} man page --- 描述 Linux 动态设备管理守护进程
    `udevd`{.systemitem}。
    :::
-   ::: para
    `systemd(1)`{.filename} man page --- 描述 `systemd`{.systemitem}
    系统和服务管理器。
    :::
-   ::: para
    `biosdevname(1)`{.filename} man page --- 描述获取 BIOS
    给定设备名称的程序。
    :::
:::
:::
::: section
::: titlepage
## [⁠]{#ch-Consistent_Network_Device_Naming.html#sec-Consistent_Network_Device_Naming_Online_Documentation}8.11.2. 在线文档 {.title}
:::
::: para
::: {.itemizedlist xmlns:d="http://docbook.org/ns/docbook"}
-   ::: para
    IBM 知识中心出版物 SC34-2710-00 [*《Red Hat Enterprise Linux 7
    中的设备驱动程序、功能及命令行》*](http://www-01.ibm.com/support/knowledgecenter/linuxonibm/liaaf/lnz_r_rhdd.html)中包含用于
    IBM z 设备及附件的["[可预期网络设备名称]{.quote}"]{.quote}信息。
    :::
:::
:::
:::
:::
:::
[]{#part-InfiniBand_and_RDMA_Networking.html}
::: part
::: titlepage
# [⁠]{#part-InfiniBand_and_RDMA_Networking.html#part-InfiniBand_and_RDMA_Networking}部分 II. InfiniBand 和 RDMA 联网 {.title}
:::
::: partintro
::: para
这部分论述了如何设置 RDMA、IfiniBand 及通过 InfiniBand 网络连接的 IP。
:::
:::
:::
[]{#ch-Configure_InfiniBand_and_RDMA_Networks.html}
::: chapter
::: titlepage
# [⁠]{#ch-Configure_InfiniBand_and_RDMA_Networks.html#ch-Configure_InfiniBand_and_RDMA_Networks}第 9 章 配置 InfiniBand 和 RDMA 网络 {.title}
:::
::: section
::: titlepage
# [⁠]{#ch-Configure_InfiniBand_and_RDMA_Networks.html#sec-Understanding_InfiniBand_and_RDMA_technologies}9.1. 了解 InfiniBand 和 RDMA 技术 {.title}
:::
::: para
InfiniBand 是指两种完全不同的东西。一个是 InfiniBand
网络的物理链接层协议，另一个是高级编程 API，名为 InfiniBand Verbs
API。InfiniBand Verbs API
是一种*远程直接内存访问*（``{=html}RDMA``{=html}）技术的实施。
:::
::: para
RDMA 通讯与一般 `IP`{.systemitem}
通讯不同，原因是他们会绕过通讯过程中的内核干扰，并极大减少一般处理网络通讯所需的
CPU 消耗。在典型的 `IP`{.systemitem} 数据传输中，机器 A 中的应用程序 X
会向机器 B 中的应用程序 Y 发送同样的数据。作为传输的一部分，机器 B
的内核必须首先接收数据，解码数据包标头，确定该数据属于应用程序
Y，然后唤醒应用程序 Y，等待应用程序 Y 在内核中执行读取
syscall，然后必须手动将该数据从内核的自主内部内存空间复制到应用程序 Y
提供的缓存中。这个过程意味着大多数网络流量都必须在系统的主内存总线间至少复制两次（一次是主机适配器使用
DMA
将该数据放到内核提供的内存缓存中，另一次是内核将该数据移动到应用程序的内存缓存中），同时也意味着计算机必须执行大量上下文切换，以便在内核上下文和应用程序
Y 上下文之间进行切换。这些操作都会在网络通讯处于极高频率时造成极高的系统
CPU 负载。
:::
::: para
RDMA
协议可让机器中的主机适配器了解网络何时会有数据包，那个应用程序应该接收该数据包，以及该数据应进入该应用程序内存空间的那个部分。这样就不会向内核发送该数据包并进行处理，然后再复制到用户的应用程序内存，而是将该数据包直接放到应用程序的缓存中，没有任何进一步的干预。这样就可以极大减少高速网络通讯的负载。但大多数
`IP`{.systemitem} 联网应用程序依赖的标准伯克利套接字 API
无法实现此功能，因此必须提供其自己的 API，即 InfiniBand Verbs
API，同时必须在应用程序可直接使用 RDMA 技术前将其移植到这个 API。
:::
::: para
Red Hat Enterprise Linux 7 supports both the InfiniBand hardware and the
InfiniBand Verbs API. In addition, there are two additional supported
technologies that allow the InfiniBand Verbs API to be utilized on
non-InfiniBand hardware. They are iWARP (Internet Wide Area RDMA
Protocol) and RoCE/IBoE (RDMA over Converged Ethernet, which was later
renamed to InfiniBand over Ethernet). Both of these technologies have a
normal `IP`{.systemitem} network link layer as their underlying
technology, and so the majority of their configuration is actually
covered in the [第 2 章 *配置 IP
联网*](#ch-Configure_IP_Networking.html){.xref} chapter of this
document. For the most part, once their `IP`{.systemitem} networking
features are properly configured, their RDMA features are all automatic
and will show up as long as the proper drivers for the hardware are
installed. The kernel drivers are always included with each kernel
Red Hat provides, however the user-space drivers must be installed
manually if the InfiniBand package group was not selected at machine
install time.
:::
::: para
::: variablelist
**这些是需要的用户空间软件包：**
[iWARP]{.term}
:   ::: para
    `Chelsio hardware`{.option} --- [**libcxgb3**]{.application} 或者
    [**libcxgb4**]{.application}，具体要看硬件版本。
    :::
[RoCE/IBoE]{.term}
:   ::: para
    `Mellanox hardware`{.option} --- [**libmlx4**]{.application} 或者
    [**libmlx5**]{.application}，具体要看硬件版本。另外，要求用户编辑
    `/etc/rdma/mlx4.conf`{.filename} 或者
    `/etc/rdma/mlx5.conf`{.filename}，以便为 RoCE/IBoE
    使用设定正确的端口类型。要求用户编辑
    `/etc/modprobe.d/mlx4.conf`{.filename} 或者
    `/etc/modprobe.d/mlx5.conf`{.filename}
    文件，以便在以太网中为无损服务配置数据包优先响应（在一些交换机中称之为
    ["[no-drop]{.quote}"]{.quote}），以此切换连接到该网络的网卡。
    :::
:::
:::
::: para
With these driver packages installed (in addition to the normal RDMA
packages typically installed for any InfiniBand installation), a user
should be able to utilize most of the normal RDMA applications to test
and see RDMA protocol communication taking place on their
adapters. However, not all of the programs included in Red Hat
Enterprise Linux 7 will properly support iWARP or RoCE/IBoE
devices. This is because the connection establishment protocol on iWARP
in particular is different than it is on real InfiniBand link-layer
connections. If the program in question uses the
[**librdmacm**]{.application} connection management library, it will
handle the differences between iWARP and InfiniBand silently and the
program should work. If the application tries to do its own connection
management, then it must specifically support iWARP or else it will not
work.
:::
:::
::: section
::: titlepage
# [⁠]{#ch-Configure_InfiniBand_and_RDMA_Networks.html#sec-InfiniBand_and_RDMA_related_software_packages}9.2. 与 InfiniBand 及 RDMA 相关的软件包 {.title}
:::
::: para
因为 RDMA 应用程序与基于伯克利套接字的应用程序有很大不同，而在一般
`IP`{.systemitem} 联网中，大多数在 `IP`{.systemitem}
网络中使用的应用程序无法直接在 RDMA 网络中使用。Red Hat
Enterprise Linux 7 为 RDMA 网络管理、测试及调试、高级软件开发 API
及性能分析提供大量不同的软件包。
:::
::: para
要使用这些网络，需要安装这些软件包的一部分或全部（这个列表并不全面，但包括与
RDMA 有关的最重要软件包）。
:::
::: para
::: {.itemizedlist xmlns:d="http://docbook.org/ns/docbook"}
**必须安装的软件包：**
-   ::: para
    `rdma`{.option} --- 负责 RDMA 栈的内核初始化。
    :::
-   ::: para
    `libibverbs`{.option} --- 提供 InfiniBand Verbs API。
    :::
-   ::: para
    `opensm`{.option} ---
    子网管理器（只需要在一台机器中安装，且只能在没有激活子网管理器的构造中安装）。
    :::
-   ::: para
    `user space driver for installed hardware`{.option} ---
    以下软件包之一：[infinipath-psm]{.package}、[libcxgb3]{.package}、[libcxgb4]{.package}、[libehca]{.package}、[libipathverbs]{.package}、[libmthca]{.package}、[libmlx4]{.package}、[libmlx5]{.package}、[libnes]{.package}
    及 [libocrdma]{.package}。注：[libehca]{.package} 只用于 IBM Power
    Systems 服务器。
    :::
:::
:::
::: para
::: {.itemizedlist xmlns:d="http://docbook.org/ns/docbook"}
**推荐的软件包：**
-   ::: para
    `librdmacm、librdmacm-utils 和 ibacm`{.option} --- 可以识别
    InfiniBand、iWARP 和 RoCE
    之间不同的连接管理库，也可以正确打开跨这些硬件类型的连接，运行确认该网络操作的一些简单测试程序，并可将该库整合到缓存守护进程，以便在大型集群中更快地进行主机解析。
    :::
-   ::: para
    `libibverbs-utils`{.option} --- 基于简单 Verbs
    的程序查询安装的硬件，并确认使用该结构的通讯。
    :::
-   ::: para
    `infiniband-diags 或 ibutils`{.option} --- 为 InfiniBand
    结构管理提供大量有用的调试工具。这些工具只为 iWARP 或 RoCE
    提供有限功能，因为大多数工具可在 InfiniBank 链接层工作，但无法在
    Verbs API 层使用。
    :::
-   ::: para
    `perftest 和 qperf`{.option} --- 用于各种 RDMA
    通讯类型的性能测试应用程序。
    :::
:::
:::