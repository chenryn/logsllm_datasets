0.040
0.035
0.030
PyTorch
TensorFlow
0
1
fps
2
3
(a) CPU TF-version
(b) GPU PT-version
Fig. 8: Throughput comparison with increasing nw.
c) Number of workers: Increasing the number of workers
(nw), and hence increasing the effective batch size, is crucial
for scaling distributed ML applications. Figure 8 depicts the
scalability of GARFIELD–based applications while training
CifarNet on CPUs (Figure 8a) and ResNet-50 on GPUs (Fig-
ure 8b). In this ﬁgure, throughput is measured in batches/sec
rather than updates/sec since employing more workers allows
for increasing the number of batches processed per iteration.
(a) Number of Byzantine workers
(b) Number of Byzantine servers
Fig. 10: Throughput with increasing fw and fps (on CPUs).
d) Number of Byzantine workers: As increasing the
number of Byzantine workers (fw) does not call for increasing
the total number of workers, we ﬁx nw and hence, ﬁxing the
effective batch size in all cases (in Figure 10a). Fixing nw
results in a ﬁxed communication cost in all cases, making
the throughput almost the same even with increasing fw. The
same results are conﬁrmed with our both frameworks, with a
slight superiority of PyTorch to TensorFlow.
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:24:13 UTC from IEEE Xplore.  Restrictions apply. 
48
e) Number of Byzantine servers: Increasing the number
of Byzantine servers (fps) calls for increasing the number of
server replicas (nps) so as to satisfy the Byzantine resilience
condition: nps ≥ 3fps+1. Thus, increasing fps introduces new
communication links, leading to a throughput drop as shown
in Figure 10b. Such a drop is conﬁrmed in the state machine
replication (SMR) literature [28], [7], where the amount of
drop (less than 50%) is reasonable compared to what was re-
ported before in the literature [16]. The assumption of 1 faulty
parameter server introduces an overhead of 33% to achieve
Byzantine resilience. Finally we note that increasing fps does
not affect the number of iterations required for convergence.
VII. RELATED WORK
To the best of our knowledge, AggregaThor [17] is the only
implementation of a Byzantine ML system that is prior to our
work. AggregaThor relies on two components: the aggregation
layer, which uses Multi–Krum to robustly aggregate workers’
gradients, and the communication layer, which enables exper-
imenting with lossy networks. Though AggregaThor follows
the shared graph design, it disallows workers to change such
a graph to combat any possible Byzantine behavior.
The design of GARFIELD is fundamentally different from
that of AggregaThor: while the latter is merely a layer on
top of TensorFlow, GARFIELD is a standalone library that
can be plugged to different frameworks. Indeed, AggregaThor
supports only one architecture, namely using a single, trusted
server with multiple workers in a synchronous environment.
From those perspectives, it is not very robust. GARFIELD, on
the other hand, can ﬂexibly adapt to different scenarios, e.g.,
with multiple server replicas and asynchronous environments.
On the theoretical side, several Byzantine-resilient ML al-
gorithms have been proposed; all try to mathematically bound
the deviation of the aggregated gradient from the correct ones.
Krum [11] employs a median–like aggregation rule. Multi-
Krum [17] generalizes the idea by averaging more gradients to
beneﬁt from additional workers. Bulyan [21] addresses an at-
tack that can trick some Byzantine–resilient algorithms by hav-
ing them converge to a stable yet faulty state. Different variants
of robust mean-based algorithms under different assumptions
and scenarios were considered in [51], [58]. Kardam [18]
uses ﬁltering mechanisms to achieve Byzantine resilience in
an asynchronous training setup. Zeno [52] and Zeno++ [55]
achieve Byzantine resilience using a performance–based rank-
ing approach in synchronous and asynchronous settings re-
spectively. Draco [14] uses a coding scheme to restore correct
gradients using redundant computations. Detox [43] extends
this idea by combining coding schemes with robust aggrega-
tion to hit the sweet spot in the resilience–optimality spectrum.
ByzSGD [20] shows how to combine robust GARs to tolerate
Byzantine servers too. In particular, it replicates the parameter
server on multiple machines while letting them communicate
to limit the divergence among their model states. ByRDiE [57]
and BRIDGE [56] combine both robust aggregation with
performance–based ranking to achieve Byzantine resilience in
the decentralized settings.
While such proposals shaped the literature of Byzantine–
resilient ML, they only remain theoretical (i.e., without a
deeper look at the very practical costs of such solutions).
GARFIELD closes this gap by providing a tool to practically
build these solutions. GARFIELD uses robust aggregation and
already implements many of the mentioned GARs. GARFIELD
can straightforwardly include the other ones.
The problem of tolerating benign (i.e., crash) failures of
parameter vectors was also addressed in the literature. Qiao
et al. [42] leverage the self–correcting behavior of SGD to
tolerate such failures. Other proposals addressed the problem
of making the parameter server crash-resilient [34], [15] using
Paxos [31]. Others rely on checkpoints or live replication [6]
of the parameter server. However, we believe that extending
those tools to the Byzantine context would be prohibitive.
VIII. CONCLUDING REMARKS
This paper presents GARFIELD, a library to build Byzantine
machine learning (ML) applications on top of popular frame-
works such as TensorFlow and PyTorch, while achieving trans-
parency: applications developed with either framework do not
need to change their interfaces to be made Byzantine resilient.
GARFIELD supports multiple statistically–robust gradient ag-
gregation rules (GARs), which can be combined in various
ways for different resilience properties. In some situations,
GARs fail to ensure Byzantine resilience when the underlying
assumption on a bounded variance is not satisﬁed [10]. Yet,
several techniques were proposed for variance reduction, e.g.,
[50], [9], [39], which help restore the resilience guarantees
of such GARs. Such techniques can be added seamlessly to
GARFIELD without affecting its throughput performance. In
the same vein, we believe GARFIELD could be also used
to implement applications that combine privacy and security
properties with Byzantine resilience as in [27], [37]. Indeed,
any protocol that relies on exchanging replies and aggregating
them in some robust manner, e.g., [24], can be implemented
with GARFIELD. Our code is open–sourced and available
at [5]. Our evaluation of GARFIELD (using three Byzantine
ML applications) showed that Byzantine resilience, unlike
crash resilience, induces an inherent loss in the ﬁnal accuracy
and that
the throughput overhead of Byzantine resilience
is moderate compared to crash resilience. Furthermore, we
showed that (1) the Byzantine resilience overhead comes more
from communication than from aggregation, and that (2) the
overhead of tolerating Byzantine servers is much more than
that of tolerating Byzantine workers.
ACKNOWLEDGEMENT
We thank our shepherd, Prof. Fernando Pedone, our col-
leagues from the DCL laboratory at EPFL, and the anonymous
reviewers, for their helpful feedback. This work has been
supported in part by the Swiss National Science Foundation
(FNS grant 200021 182542/1). Most experiments presented
in this paper were carried out using the Grid’5000 testbed,
supported by a scientiﬁc interest group hosted by Inria and
including CNRS, RENATER, and several Universities as well
as other organizations (see https://www.grid5000.fr).
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:24:13 UTC from IEEE Xplore.  Restrictions apply. 
49
REFERENCES
[1] Aggregathor source code. https://github.com/LPD-EPFL/AggregaThor.
[2] Cifar dataset. https://www.cs.toronto.edu/∼kriz/cifar.html.
[3] Grid5000. https://www.grid5000.fr/.
[4] Mnist dataset. http://yann.lecun.com/exdb/mnist/.
[5] GARFIELD source code. https://github.com/LPD-EPFL/Garﬁeld.
[6] Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis,
Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving,
Michael Isard, et al. Tensorﬂow: A system for large-scale machine
learning. In OSDI, 2016.
[7] Michael Abd-El-Malek, Gregory R Ganger, Garth R Goodson,
Michael K Reiter, and Jay J Wylie. Fault-scalable byzantine fault-
tolerant services. ACM SIGOPS Operating Systems Review, 39:59–74,
2005.
[8] Dan Alistarh, Zeyuan Allen-Zhu, and Jerry Li. Byzantine stochastic
gradient descent. In Neural Information Processing Systems, to appear,
2018.
[9] Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-
convex optimization. In International conference on machine learning,
pages 699–707, 2016.
[10] Moran Baruch, Gilad Baruch, and Yoav Goldberg.
A little is
enough: Circumventing defenses for distributed learning. arXiv preprint
arXiv:1902.06156, 2019.
[11] Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien
Stainer. Machine learning with adversaries: Byzantine tolerant gradient
descent.
In Neural Information Processing Systems, pages 118–128,
2017.
[12] Cara Bloom, Joshua Tan, Javed Ramjohn, and Lujo Bauer. Self-driving
cars and data collection: Privacy perceptions of networked autonomous
vehicles.
In Thirteenth Symposium on Usable Privacy and Security
({SOUPS} 2017), pages 357–375, 2017.
[13] Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone,
H Brendan McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and
Karn Seth. Practical secure aggregation for privacy-preserving machine
learning.
In Proceedings of the 2017 ACM SIGSAC Conference on
Computer and Communications Security, pages 1175–1191. ACM, 2017.
[14] Lingjiao Chen, Hongyi Wang, Zachary Charles, and Dimitris Papail-
iopoulos. Draco: Byzantine-resilient distributed training via redundant
gradients.
In International Conference on Machine Learning, pages
902–911, 2018.
[15] Trishul M Chilimbi, Yutaka Suzue, Johnson Apacible, and Karthik
Kalyanaraman. Project adam: Building an efﬁcient and scalable deep
learning training system. In OSDI, volume 14, pages 571–582, 2014.
[16] James Cowling, Daniel Myers, Barbara Liskov, Rodrigo Rodrigues,
and Liuba Shrira. Hq replication: A hybrid quorum protocol for
byzantine fault tolerance.
In Proceedings of the 7th symposium on
Operating systems design and implementation, pages 177–190. USENIX
Association, 2006.
[17] Georgios Damaskinos, El Mahdi El Mhamdi, Rachid Guerraoui, Arsany
Guirguis, and S´ebastien Rouault. Aggregathor: Byzantine machine
learning via robust gradient aggregation. In SysML, 2019.
[18] Georgios Damaskinos, El Mahdi El Mhamdi, Rachid Guerraoui,
Rhicheek Patra, Mahsa Taziki, et al. Asynchronous byzantine machine
learning (the case of sgd). In ICML, pages 1153–1162, 2018.
[19] El-Mahdi El-Mhamdi, Rachid Guerraoui, Arsany Guirguis, Lˆe Nguyˆen
Hoang, and S´ebastien Rouault. Collaborative learning as an agreement
problem. arXiv preprint arXiv:2008.00742, 2020.
[20] El-Mahdi El-Mhamdi, Rachid Guerraoui, Arsany Guirguis, Lˆe Nguyˆen
Hoang, and S´ebastien Rouault. Genuinely distributed byzantine machine
learning.
the 39th Symposium on Principles of
Distributed Computing, pages 355–364, 2020.
In Proceedings of
[21] El Mahdi El Mhamdi, Rachid Guerraoui, and S´ebastien Rouault. The
hidden vulnerability of distributed learning in Byzantium. In Jennifer
Dy and Andreas Krause, editors, Proceedings of the 35th International
Conference on Machine Learning, volume 80 of Proceedings of Machine
Learning Research, pages 3521–3530, Stockholmsm¨assan, Stockholm
Sweden, 10–15 Jul 2018. PMLR.
[22] El-Mahdi El-Mhamdi, Rachid Guerraoui, and S´ebastien Rouault. Dis-
arXiv preprint
tributed momentum for byzantine-resilient
arXiv:2003.00010, 2020.
learning.
[23] Andre Esteva, Brett Kuprel, Roberto A Novoa, Justin Ko, Susan M
Swetter, Helen M Blau, and Sebastian Thrun. Dermatologist-level
classiﬁcation of skin cancer with deep neural networks.
542(7639):115, 2017.
Nature,
[24] Clement Fung, Chris JM Yoon, and Ivan Beschastnikh. The limitations
of federated learning in sybil settings. In 23rd International Symposium
on Research in Attacks, Intrusions and Defenses ({RAID} 2020), pages
301–316, 2020.
[25] Amirmasoud Ghiassi, Taraneh Younesian, Zhilong Zhao, Robert Birke,
Valerio Schiavoni, and Lydia Y Chen. Robust (deep) learning framework
against dirty labels and beyond.
In 2019 First IEEE International
Conference on Trust, Privacy and Security in Intelligent Systems and
Applications (TPS-ISA), pages 236–244. IEEE, 2019.
[26] Lie He, An Bian, and Martin Jaggi. Cola: Decentralized linear learning.
In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
and R. Garnett, editors, Advances in Neural Information Processing
Systems 31, pages 4536–4546. Curran Associates, Inc., 2018.
[27] Lie He, Sai Praneeth Karimireddy, and Martin Jaggi. Secure byzantine-
robust machine learning. arXiv preprint arXiv:2006.04747, 2020.
[28] Patrick Hunt, Mahadev Konar, Flavio Paiva Junqueira, and Benjamin
Reed. Zookeeper: Wait-free coordination for internet-scale systems. In
USENIX annual technical conference, volume 8. Boston, MA, USA,
2010.
[29] M. Kachelrieß. Branchless vectorized median ﬁltering.
In 2009
IEEE Nuclear Science Symposium Conference Record (NSS/MIC), pages
4099–4105, Oct 2009.
[30] Larry Kim. How many ads does google serve in a day? URL http://goo.
gl/oIidXO. http://goo. gl/oIidXO, 1(1), 2012.
[31] Leslie Lamport et al. Paxos made simple. ACM Sigact News, 32(4):18–
25, 2001.
[32] Leslie Lamport, Robert Shostak, and Marshall Pease. The Byzantine
generals problem. TOPLAS, 4(3):382–401, 1982.
[33] Mu Li, David G Andersen, Jun Woo Park, Alexander J Smola, Amr
Ahmed, Vanja Josifovski, James Long, Eugene J Shekita, and Bor-Yiing
Su. Scaling distributed machine learning with the parameter server. In
OSDI, volume 1, page 3, 2014.
[34] Mu Li, Li Zhou, Zichao Yang, Aaron Li, Fei Xia, David G Andersen,
and Alexander Smola. Parameter server for distributed machine learning.
In Big Learning NIPS Workshop, volume 6, page 2, 2013.
[35] H Brendan McMahan, Gary Holt, David Sculley, Michael Young,
Dietmar Ebner, Julian Grady, Lan Nie, Todd Phillips, Eugene Davydov,
Daniel Golovin, et al. Ad click prediction: a view from the trenches.
In Proceedings of the 19th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 1222–1230. ACM, 2013.
[36] Xiangrui Meng, Joseph Bradley, Burak Yavuz, Evan Sparks, Shivaram
Venkataraman, Davies Liu, Jeremy Freeman, DB Tsai, Manish Amde,
Sean Owen, et al. Mllib: Machine learning in apache spark. JMLR,
17(1):1235–1241, 2016.
[37] Luis Mu˜noz-Gonz´alez, Kenneth T Co, and Emil C Lupu. Byzantine-
robust federated machine learning through adaptive model averaging.
arXiv preprint arXiv:1909.05125, 2019.
[38] David R Musser. Introspective sorting and selection algorithms. Soft-
ware: Practice and Experience, 27(8):983–993, 1997.
[39] Jay H Park, Sunghwan Kim, Jinwon Lee, Myeongjae Jeon, and Sam H
Noh. Accelerated training for cnn distributed deep learning through auto-
matic resource-aware layer placement. arXiv preprint arXiv:1901.05803,
2019.
[40] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Brad-
bury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein,
Luca Antiga, et al. Pytorch: An imperative style, high-performance deep
learning library. In Advances in neural information processing systems,
pages 8026–8037, 2019.
[41] Pitch Patarasuk and Xin Yuan. Bandwidth optimal all-reduce algorithms
Journal of Parallel and Distributed
for clusters of workstations.
Computing, 69(2):117–124, 2009.
[42] Aurick Qiao, Bryon Aragam, Bingjing Zhang, and Eric Xing. Fault
In International
tolerance in iterative-convergent machine learning.
Conference on Machine Learning, pages 5220–5230, 2019.
[43] Shashank Rajput, Hongyi Wang, Zachary Charles, and Dimitris Papail-
iopoulos. Detox: A redundancy-based framework for faster and more
robust gradient aggregation. arXiv preprint arXiv:1907.12205, 2019.
[44] Qing Rao and Jelena Frtunikj. Deep learning for self-driving cars:
chances and challenges. In 2018 IEEE/ACM 1st International Workshop
on Software Engineering for AI in Autonomous Systems (SEFAIAS),
pages 35–38. IEEE, 2018.
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:24:13 UTC from IEEE Xplore.  Restrictions apply. 
50
protobuf.
[49] Pooja Vyavahare, Lili Su, and Nitin H Vaidya. Distributed learning
with adversarial agents under relaxed network condition. arXiv preprint
arXiv:1901.01943, 2019.
[50] Chong Wang, Xi Chen, Alexander J Smola, and Eric P Xing. Variance
reduction for stochastic gradient optimization. Advances in Neural
Information Processing Systems, 26:181–189, 2013.
[51] Cong Xie, Oluwasanmi Koyejo, and Indranil Gupta. Generalized
Byzantine-tolerant sgd. arXiv preprint arXiv:1802.10116, 2018.
[52] Cong Xie, Oluwasanmi Koyejo, and Indranil Gupta. Zeno: Byzantine-
suspicious stochastic gradient descent. arXiv preprint arXiv:1805.10032,
2018.
[53] Cong Xie, Oluwasanmi O Koyejo, and Indranil Gupta. Faster distributed
synchronous sgd with weak synchronization. 2018.
[45] Peter J Rousseeuw. Multivariate estimation with high breakdown point.
Mathematical statistics and applications, 8:283–297, 1985.
[46] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learn-
ing representations by back-propagating errors. nature, 323(6088):533–
536, 1986.
[47] Paul Vanhaesebrouck, Aur´elien Bellet, and Marc Tommasi. Decentral-
In
ized collaborative learning of personalized models over networks.
AISTATS, 2017.
[48] Kenton Varda.
https://github.com/protocolbuffers/
Protocol buffers.
[54] Cong Xie, Sanmi Koyejo, and Indranil Gupta. Fall of empires: Breaking
byzantine-tolerant sgd by inner product manipulation. arXiv preprint
arXiv:1903.03936, 2019.
[55] Cong Xie, Sanmi Koyejo, and Indranil Gupta. Zeno++: Robust fully
asynchronous sgd. arXiv preprint arXiv:1903.07020, 2019.
[56] Zhixiong Yang and Waheed U Bajwa. Bridge: Byzantine-resilient
decentralized gradient descent. arXiv preprint arXiv:1908.08098, 2019.
[57] Zhixiong Yang and Waheed U Bajwa. Byrdie: Byzantine-resilient
distributed coordinate descent for decentralized learning. IEEE Trans-
actions on Signal and Information Processing over Networks, 5(4):611–
627, 2019.
[58] Dong Yin, Yudong Chen, Kannan Ramchandran, and Peter Bartlett.
Byzantine-robust distributed learning: Towards optimal statistical rates.
arXiv preprint arXiv:1803.01498, 2018.
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:24:13 UTC from IEEE Xplore.  Restrictions apply. 
51