:::
使用 `configfs`{.literal} 配置 NVMe/RDMA 目标。
::: itemizedlist
**先决条件**
-   确定您有一个要分配给 `nvmet`{.literal} 子系统的块设备。
:::
::: orderedlist
**流程**
1.  创建 `nvmet-rdma`{.literal} 子系统：
    ``` screen
    # modprobe nvmet-rdma
    # mkdir /sys/kernel/config/nvmet/subsystems/testnqn
    # cd /sys/kernel/config/nvmet/subsystems/testnqn
    ```
    使用子系统名称替换 [*testnqn*]{.emphasis}。
2.  允许任何主机连接到这个目标：
    ``` screen
    # echo 1 > attr_allow_any_host
    ```
3.  配置命名空间：
    ``` screen
    # mkdir namespaces/10
    # cd namespaces/10
    ```
    使用命名空间号替换 [*10*]{.emphasis}
4.  设置到 NVMe 设备的路径：
    ``` screen
    #echo -n /dev/nvme0n1 > device_path
    ```
5.  启用命名空间：
    ``` screen
    # echo 1 > enable
    ```
6.  创建带有 NVMe 端口的目录：
    ``` screen
    # mkdir /sys/kernel/config/nvmet/ports/1
    # cd /sys/kernel/config/nvmet/ports/1
    ```
7.  显示 [*mlx5_ib0*]{.emphasis} 的 IP 地址：
    ``` screen
    # ip addr show mlx5_ib0
    8: mlx5_ib0:  mtu 4092 qdisc mq state UP group default qlen 256
        link/infiniband 00:00:06:2f:fe:80:00:00:00:00:00:00:e4:1d:2d:03:00:e7:0f:f6 brd 00:ff:ff:ff:ff:12:40:1b:ff:ff:00:00:00:00:00:00:ff:ff:ff:ff
        inet 172.31.0.202/24 brd 172.31.0.255 scope global noprefixroute mlx5_ib0
           valid_lft forever preferred_lft forever
        inet6 fe80::e61d:2d03:e7:ff6/64 scope link noprefixroute
           valid_lft forever preferred_lft forever
    ```
8.  为目标设置传输地址：
    ``` screen
    # echo -n 172.31.0.202 > addr_traddr
    ```
9.  将 RDMA 设置为传输类型：
    ``` screen
    # echo rdma > addr_trtype
    # echo 4420 > addr_trsvcid
    ```
10. 为端口设置地址系列：
    ``` screen
    # echo ipv4 > addr_adrfam
    ```
11. 创建软链接：
    ``` screen
    # ln -s /sys/kernel/config/nvmet/subsystems/testnqn   /sys/kernel/config/nvmet/ports/1/subsystems/testnqn
    ```
:::
::: itemizedlist
**验证步骤**
-   验证 NVMe 目标是否在侦听给定端口并准备好进行连接请求：
    ``` screen
    # dmesg | grep "enabling port"
    [ 1091.413648] nvmet_rdma: enabling port 1 (172.31.0.202:4420)
    ```
:::
::: itemizedlist
**其它资源**
-   `nvme`{.literal} man page。
:::
:::
::: section
::: titlepage
## []{#overview-of-nvme-over-fabric-devicesmanaging-storage-devices.html#setting-up-nvme-rdma-target-using-nvmetcli_nvme-over-fabrics-using-rdma}使用 nvmetcli 设置 NVMe/RDMA 目标 {.title}
:::
使用 `nvmetcli`{.literal} 工具编辑、查看和启动 NVMe
目标。`nvmetcli`{.literal} 工具提供命令行和交互式 shell
选项。使用这个流程通过 `nvmetcli`{.literal} 配置 NVMe/RDMA 目标。
::: itemizedlist
**先决条件**
-   确定您有一个要分配给 `nvmet`{.literal} 子系统的块设备。
-   以 root 用户身份执行以下 `nvmetcli`{.literal} 操作。
:::
::: orderedlist
**流程**
1.  安装 `nvmetcli`{.literal} 软件包：
    ``` screen
    # yum install nvmetcli
    ```
2.  下载 `rdma.json`{.literal} 文件：
    ``` screen
    # wget http://git.infradead.org/users/hch/nvmetcli.git/blob_plain/0a6b088db2dc2e5de11e6f23f1e890e4b54fee64:/rdma.json
    ```
3.  编辑 `rdma.json`{.literal} 文件，将 `traddr`{.literal} 值更改为
    `172.31.0.202`{.literal}。
4.  通过载入 NVMe 目标配置文件来设置目标：
    ``` screen
    # nvmetcli restore rdma.json
    ```
:::
::: {.note style="margin-left: 0.5in; margin-right: 0.5in;"}
### 注意 {.title}
如果没有指定 NVMe 目标配置文件名称，`nvmetcli`{.literal} 将使用
`/etc/nvmet/config.json`{.literal} 文件。
:::
::: itemizedlist
**验证步骤**
-   验证 NVMe 目标是否在侦听给定端口并准备好进行连接请求：
    ``` screen
    #dmesg | tail -1
    [ 4797.132647] nvmet_rdma: enabling port 2 (172.31.0.202:4420)
    ```
-   （可选）清除当前 NVMe 目标：
    ``` screen
    # nvmetcli clear
    ```
:::
::: itemizedlist
**其它资源**
-   `nvmetcli`{.literal} man page。
-   `nvme`{.literal} man page。
:::
:::
::: section
::: titlepage
## []{#overview-of-nvme-over-fabric-devicesmanaging-storage-devices.html#configuring-nvme-rdma-client_nvme-over-fabrics-using-rdma}配置 NVMe/RDMA 客户端 {.title}
:::
使用 NVMe 管理命令行界面(`nvme-cli`{.literal})工具配置 NVMe/RDMA
客户端。
::: orderedlist
**流程**
1.  安装 `nvme-cli`{.literal} 工具：
    ``` screen
    # yum install nvme-cli
    ```
2.  如果没有加载，载入 `nvme-rdma`{.literal} 模块：
    ``` screen
    # modprobe nvme-rdma
    ```
3.  在 NVMe 目标中找到可用子系统：
    ``` screen
    # nvme discover -t rdma -a 172.31.0.202 -s 4420
    Discovery Log Number of Records 1, Generation counter 2
    =====Discovery Log Entry 0======
    trtype:  rdma
    adrfam:  ipv4
    subtype: nvme subsystem
    treq:    not specified, sq flow control disable supported
    portid:  1
    trsvcid: 4420
    subnqn:  testnqn
    traddr:  172.31.0.202
    rdma_prtype: not specified
    rdma_qptype: connected
    rdma_cms:    rdma-cm
    rdma_pkey: 0x0000
    ```
4.  连接到发现的子系统：
    ``` screen
    # nvme connect -t rdma -n testnqn -a 172.31.0.202 -s 4420
    # lsblk
    NAME                         MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
    sda                            8:0    0 465.8G  0 disk
    ├─sda1                         8:1    0     1G  0 part /boot
    └─sda2                         8:2    0 464.8G  0 part
      ├─rhel_rdma--virt--03-root 253:0    0    50G  0 lvm  /
      ├─rhel_rdma--virt--03-swap 253:1    0     4G  0 lvm  [SWAP]
      └─rhel_rdma--virt--03-home 253:2    0 410.8G  0 lvm  /home
    nvme0n1
    # cat /sys/class/nvme/nvme0/transport
    rdma
    ```
    使用 NVMe 子系统名称替换 [*testnqn*]{.emphasis}。
    将 [*172.31.0.202*]{.emphasis} 替换为目标 IP 地址。
    使用端口号替换 [*4420*]{.emphasis}。
:::
::: itemizedlist
**验证步骤**
-   列出当前连接的 NVMe 设备：
    ``` screen
    # nvme list
    ```
-   （可选）与目标断开：
    ``` screen
    # nvme disconnect -n testnqn
    NQN:testnqn disconnected 1 controller(s)
    # lsblk
    NAME                         MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
    sda                            8:0    0 465.8G  0 disk
    ├─sda1                         8:1    0     1G  0 part /boot
    └─sda2                         8:2    0 464.8G  0 part
      ├─rhel_rdma--virt--03-root 253:0    0    50G  0 lvm  /
      ├─rhel_rdma--virt--03-swap 253:1    0     4G  0 lvm  [SWAP]
      └─rhel_rdma--virt--03-home 253:2    0 410.8G  0 lvm  /home
    ```
:::
::: itemizedlist
**其它资源**
-   `nvme`{.literal} man page。
-   [NVMe-cli Github
    存储库](https://github.com/linux-nvme/nvme-cli){.link}
:::
:::
::: section
::: titlepage
## []{#overview-of-nvme-over-fabric-devicesmanaging-storage-devices.html#next_steps}后续步骤 {.title}
:::
::: itemizedlist
-   在 NVMe/RDMA 设备中启用多路径。如需更多信息，请参阅 [第 14 章 *在
    NVMe
    设备中启用多路径*](#assembly_enabling-multipathing-on-nvme-devices_managing-storage-devices.html "第 14 章 在 NVMe 设备中启用多路径"){.xref}。
:::
:::
:::
::: section
::: titlepage
# []{#overview-of-nvme-over-fabric-devicesmanaging-storage-devices.html#nvme-over-fabrics-using-fc_overview-of-nvme-over-fabric-devices}使用 FC 的光纤的 NVMe over fabrics {.title}
:::
当与特定 Broadcom Emulex 和 Marvell Qlogic Fibre Channel
适配器一起使用时，在 initiator 模式中完全支持 NVMe
over光纤通道(FC-NVMe)传输。作为系统管理员，在以下部分中完成任务以部署
FC-NVMe 设置：
::: itemizedlist
-   ["为广播适配器配置 NVMe
    initiator"一节](#overview-of-nvme-over-fabric-devicesmanaging-storage-devices.html#configuring-the-nvme-initiator-for-broadcom-adapters_nvme-over-fabrics-using-fc "为广播适配器配置 NVMe initiator"){.xref}.
-   ["为 QLogic 适配器配置 NVMe
    initiator"一节](#overview-of-nvme-over-fabric-devicesmanaging-storage-devices.html#configuring-the-nvme-initiator-for-qlogic-adapters_nvme-over-fabrics-using-fc "为 QLogic 适配器配置 NVMe initiator"){.xref}.
:::
::: section
::: titlepage
## []{#overview-of-nvme-over-fabric-devicesmanaging-storage-devices.html#configuring-the-nvme-initiator-for-broadcom-adapters_nvme-over-fabrics-using-fc}为广播适配器配置 NVMe initiator {.title}
:::
使用 NVMe 管理命令行界面(`nvme-cli`{.literal})工具为 Broadcom
适配器客户端配置 NVMe initiator。
::: orderedlist
**流程**
1.  安装 `nvme-cli`{.literal} 工具：
    ``` screen
    # yum install nvme-cli
    ```
    这会在 `/etc/nvme/`{.literal} 目录中创建 `hostnqn`{.literal}
    文件。`hostnqn`{.literal} 文件标识 NVMe 主机。
    生成一个新的 `hostnqn`{.literal}：
    ``` screen
    # nvme gen-hostnqn
    ```
2.  查找本地和远程端口的 WWN 和 WWPN 标识符，并使用输出查找子系统 NQN:
    ``` screen
    # cat /sys/class/scsi_host/host*/nvme_info
    NVME Initiator Enabled
    XRI Dist lpfc0 Total 6144 IO 5894 ELS 250
    NVME LPORT lpfc0 WWPN x10000090fae0b5f5 WWNN x20000090fae0b5f5 DID x010f00 ONLINE
    NVME RPORT       WWPN x204700a098cbcac6 WWNN x204600a098cbcac6 DID x01050e TARGET DISCSRVC ONLINE
    NVME Statistics
    LS: Xmt 000000000e Cmpl 000000000e Abort 00000000
    LS XMIT: Err 00000000  CMPL: xb 00000000 Err 00000000
    Total FCP Cmpl 00000000000008ea Issue 00000000000008ec OutIO 0000000000000002
        abort 00000000 noxri 00000000 nondlp 00000000 qdepth 00000000 wqerr 00000000 err 00000000
    FCP CMPL: xb 00000000 Err 00000000
    ```
    ``` screen
    # nvme discover --transport fc \
                    --traddr nn-0x204600a098cbcac6:pn-0x204700a098cbcac6 \
                    --host-traddr nn-0x20000090fae0b5f5:pn-0x10000090fae0b5f5
    Discovery Log Number of Records 2, Generation counter 49530
    =====Discovery Log Entry 0======
    trtype:  fc
    adrfam:  fibre-channel
    subtype: nvme subsystem
    treq:    not specified
    portid:  0
    trsvcid: none
    subnqn:  nqn.1992-08.com.netapp:sn.e18bfca87d5e11e98c0800a098cbcac6:subsystem.st14_nvme_ss_1_1
    traddr:  nn-0x204600a098cbcac6:pn-0x204700a098cbcac6
    ```