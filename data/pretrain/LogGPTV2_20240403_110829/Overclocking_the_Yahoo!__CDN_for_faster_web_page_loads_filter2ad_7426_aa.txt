title:Overclocking the Yahoo!: CDN for faster web page loads
author:Mohammad Al-Fares and
Khaled Elmeleegy and
Benjamin Reed and
Igor Gashinsky
Overclocking the Yahoo! CDN for Faster Web Page Loads
Mohammad Al-Fares1,2, Khaled Elmeleegy2, Benjamin Reed2, and Igor Gashinsky3
1Computer Science and Engineering 2Yahoo! Research 3Yahoo! Inc.
UC San Diego
PI:EMAIL
{khaled, breed, igor}@yahoo-inc.com
Abstract
Fast-loading web pages are key for a positive user experience. Un-
fortunately, a large number of users suffer from page load times of
many seconds, especially for pages with many embedded objects.
Most of this time is spent fetching the page and its objects over the
Internet.
This paper investigates the impact of optimizations that improve
the delivery of content from edge servers at the Yahoo! Content
Delivery Network (CDN) to the end users. To this end, we an-
alyze packet traces of 12.3M TCP connections originating from
users across the world and terminating at the Yahoo! CDN. Us-
ing these traces, we characterize key user-connection metrics at the
network, transport, and the application layers. We observe high
Round Trip Times (RTTs) and inﬂated number of round trips per
page download (RTT multipliers). Due to inefﬁciencies in TCP’s
slow start and the HTTP protocol, we found several opportunities to
reduce the RTT multiplier, e.g. increasing TCP’s Initial Congestion
Window (ICW), using TCP Appropriate Byte Counting (ABC), and
using HTTP pipelining.
Using live workloads, we experimentally study the micro effects
of these optimizations on network connectivity, e.g. packet loss
rate. To evaluate the macro effects of these optimizations on the
overall page load time, we use realistic synthetic workloads in a
closed laboratory environment. We ﬁnd that compounding HTTP
pipelining with increasing the ICW size can lead to reduction in
page load times by up to 80%. We also ﬁnd that no one conﬁgu-
ration ﬁts all users, e.g. increasing the TCP ICW to a certain size
may help some users while hurting others.
Categories and Subject Descriptors
C.4 [Performance of Systems]: measurement techniques, perfor-
mance attributes; C.2.3 [Computer-Communication Networks]:
Network Operations—network monitoring, public networks
General Terms
Measurement, Experimentation, Performance
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’11, November 2–4, 2011, Berlin, Germany.
Copyright 2011 ACM 978-1-4503-1013-0/11/11 ...$10.00.
Keywords
Content Delivery Networks, Trafﬁc Analysis, Web Page Load
Time, TCP tuning
1.
INTRODUCTION
For web sites to maintain high user satisfaction, their web pages
need to load quickly. However, web pages are becoming more con-
tent rich over the past few years. They include many embedded
images, scripts, and style sheets. Consequently, page load times
are getting higher. As we will see in Section 4.1.2, many users can
experience tens of seconds of load time for popular web pages like
Yahoo!’s front page.
Our measurements and previous research show that virtually all
of this time is spent in the network stack downloading the web page
and its embedded objects [14, 20, 25]. Two main factors contribute
to the long download time. The ﬁrst is the network RTT from the
user to the web server. Second, a page download typically takes
tens of round trips to download the web page data and all its em-
bedded objects. The number of round trips involved is called RTT
multiplier.
Long network RTTs can be due to a combination of long Internet
routes, route misconﬁgurations, and long queuing delays at routers
along the packets’ path. To alleviate this problem, CDNs are de-
ployed across the globe by companies like Akamai, Facebook, Mi-
crosoft, Google, and Yahoo! These CDNs bring content closer to
users across the world, hence reducing their network RTTs. How-
ever, this study and previous work by Krishnan et al. [16] have
shown that even with globally deployed CDNs, many users expe-
rience hundreds of milliseconds RTTs. To alleviate this high RTT
problem, Krishnan et al. proposed a tool, WhyHigh, that attempts
to identify routing misconﬁgurations leading to inﬂated network
RTTs. This helps ﬁx these misconﬁgurations and reduce round trip
times.
Inﬂated RTT multipliers are mainly due to inefﬁciencies during
TCP slow start and in the HTTP protocol. TCP slow start probes
the available link capacity by exponentially growing the transfer
rate per RTT until a packet is lost (or the slow-start threshold is
reached). This probing is fairly conservative, as it starts from a
modest ICW, with a default value of three in most networks. Hence,
it wastes many network round trips before the full available net-
work bandwidth is utilized. Similarly, HTTP is used inefﬁciently
in practice as it requests a single object at a time wasting a net-
work round trip per object. For a page with tens of small embedded
objects, this is very wasteful.
Two key optimizations were proposed by the IETF and indus-
try [7, 11]: First, TCP should start probing from a larger ICW size.
569Dukkipati et al. [11] argue for using an ICW of 10 segments. Us-
ing trafﬁc measurements from Google’s users, they argue that this
would reduce object load time with virtually no downside. Sec-
ond, TCP should open up the congestion window size at a higher
rate per round trip. TCP slow start increases the congestion win-
dow by one for every acknowledgment received. However, delayed
acknowledgments, which are pervasively deployed in the Internet,
make the receiver send an acknowledgment for every other packet
received. This causes TCP congestion window to increase by a fac-
tor of 1.5 instead of 2 per network round trip during slow-start. To
remedy this problem, ABC [3, 5], was introduced to increase the
window based on the number of bytes acknowledged instead of the
number of acknowledgments received.
HTTP pipelining [12] was introduced to optimize HTTP down-
loads reducing the number of round trips.
It allows for sending
HTTP requests for new objects, while responses from earlier re-
quests have not yet been recieved. As seen in Figure 1, HTTP
pipelining saves RTTs reducing overall web page load time. Un-
fortunately, HTTP pipelining is not available by default in major
web browsers. For example, Internet Explorer, the dominant web
browser, does not support it. 1 And while Firefox supports it, it is
disabled by default.
This paper is concerned with the delivery of content from edge
servers from the Yahoo! CDN to the users. To this end, we collect
packet traces of 12.3 million TCP connections from users of Ya-
hoo! across the world. Using these traces, we present an in-depth
cross-layer study of different factors affecting web page load times.
Then, we study different cross-layer optimizations and their inter-
play aimed at reducing the RTT multiplier. Speciﬁcally, we study
varying the ICW size, TCP ABC, and HTTP pipelining using live
and realistically-inspired synthetic workloads.
The contributions of this paper are three fold:
1. Characterize the connections from users’ to the Yahoo! CDN
web servers at the IP, TCP, and HTTP layers.
2. Study TCP optimizations to reduce web page load times –
most notably changing the ICW. We ﬁnd that many users
beneﬁt signiﬁcantly (up to 38%) from increasing the ICW
size. However, in contrast to previous work, we show that no
one size for the ICW ﬁts all the users as increasing the ICW
for some users can increase packet loss hurting the overall
page load time. Moreover, we show that, in some cases,
increasing the ICW size can be unfair to other ﬂows in the
network. We believe that currently this result is especially
important given the efforts at IETF to increase TCP’s ICW
size to the ﬁxed size of 10 [10].
3. Study and quantify the performance gains from HTTP
In addition, quantify
pipelining using realistic workloads.
the gains when HTTP pipelining is used in conjunction with
optimum ICW size. These gains can reach 80% reduction in
the page load time.
The rest of this paper is organized as follows. Section 2 presents
the background and previous related work. Section 3 characterizes
the trafﬁc observed at the Yahoo! CDN. Section 4 presents our
study of different optimizations to reduce the RTT multiplier to
reduce the web page load time. Section 5 discusses our ﬁndings.
Section 6 concludes the paper.
1The main reason Microsoft gives is that pipelining is not univer-
sally implemented, e.g. head-of-line blocking with buggy proxy
servers.
Client
Server
Client
Server
Open
Close
Open
e
m
T
i
Close
No Pipelining
Pipelining
Figure 1: Non pipelined vs. pipelined HTTP connection. The
client arrows indicate GET requests.
2. BACKGROUND AND RELATED WORK
In this section, we explain how content delivery networks work.
We also present some of the related work aimed at optimizing con-
tent delivery at CDNs, more speciﬁcally optimizations to the net-
work stack.
2.1 Content Delivery Networks
CDNs are usually built as a network of geographically diverse
sites. Each site hosts a cluster of servers caching content and deliv-
ering it to users. The geographical diversity serves two purposes.
First, it brings content closer to the users reducing network latency.
Second, it provides redundancy to tolerate failures of individual
sites.
In a nutshell, a CDN typically works as shown in Figure 2. When
the user tries to fetch an object from a particular URL, it ﬁrst per-
forms a DNS lookup. The DNS server returns the IP address of a
server near the user. The user then contacts the server to fetch the
object. If the server has the object locally, it serves it to the user
from its cache. Otherwise, the server contacts a back-end server,
usually over a fast private network, to fetch the object into its cache
and then serve it to the user.
There are multiple CDNs deployed worldwide. Some compa-
nies run and use their own private CDNs like Google, Facebook,
and Yahoo!. Others use third party CDNs like Akamai [19] and
CoralCDN [13].
In this paper, we study the delivery aspect of the Yahoo! CDN.
At a high level, the Yahoo! CDN operates as described above.
2.2 Round Trip Times
Krishnan et al. [16] studied the network round trip latencies in
the Google CDN. They reported that latencies are generally high
and that 40% have round trip times higher than 400ms. They ar-
gued that adding more CDN sites is not always the best solution
as this high latency is sometimes due to queuing delays and routing
misconﬁgurations. They then introduced a new tool, WhyHigh, that
tries to identify preﬁxes suffering from inﬂated latencies. Finally,
this tool attempts to diagnose the causes for this inﬂated latency by
using multiple active measurements, using different tools like ping
and traceroute, and correlating inﬂated subnet latencies to common
AS paths for example.
570In this paper, we also studied round trip latencies, and also found
latencies to be high (on the order of a few hundred milliseconds
in the developing world). However, the latency distributions we
observed were signiﬁcantly lower than those reported in [16].
2.3 Optimizing the Network Stack
Previous work have argued for increasing TCP’s initial window
size [7, 10, 11]. Dukkipati et al. [11] recently argued for increas-
ing the window size to 10 segments in order to decrease page load
time. They argued that this reduces page load time with virtually
no downside. In contrast, although we ﬁnd that many users bene-
ﬁt from larger initial window size in this study, we also observe a
subset of users who suffer due to increased packet loss.
Qianet al. [23], have studied the Internet backbone trafﬁc and
shown that up to 15% of large-ﬂows already violate the ICW limit
set by the spec (min(4 ∗ M SS, max(2 ∗ M SS, 4380), which
equals 3 for a Maximum Segment Size (MSS) of 1460) [7]), and
values up to 9KB have been observed in the wild.
Allman [4] studied trafﬁc to and from a single web server. He
characterized different settings of the protocols used (TCP and
HTTP) by this trafﬁc. For example, he studied the deployment of
TCP features like selective acknowledgments. Like other studies,
Allman too reported long RTTs for studied connections. This study
is over 10 years old though and only studied 751K connections at a
single web server at a single geographic location. Moreover, unlike
this paper, Allman only relied on passive measurements and did
not try to measure different performance metrics in response to
changing different protocol settings.
To allow for increasing the ICW while not hurting users with
poor connectivity, Chu [10] et al. argued that users with poor con-
nectivity can can advertise a smaller receive window sizes. In the-
ory this can ﬁx the problem. However, in practice, modifying the
network stacks of existing users with poor connectivity to dynam-
ically detect their network conditions and consequently adjusting
their corresponding receive window sizes is challenging.
A different line of research proposed multiplexing several small
streams on top of TCP to reduce web page load time, e.g. SPDY [2]
and Stream Control Transmission Protocol [24]. However, both
protocols are still experimental and not used at a large scale in the
web. In contrast, we aim to optimize existing protocols to achieve
best possible performance without breaking backward compatibil-
ity.
On the HTTP and application layer front, Leighton [17] advo-
cates several optimizations such as pre-fetching embedded content,
pre-caching popular objects at the edge, and using compression and
delta-encoding of popular web pages. The argument being that, in
contrast to a decade ago when the last-mile connection to the user
was likely the bottleneck, the middle-mile’s capacity growth has not
kept pace and become the new bottleneck, and that these techniques
would all contribute to alleviating web trafﬁc on the backbone and
faster page loads.
3. STUDYING YAHOO! CDN TRAFFIC
In this section, we present our study of the trafﬁc characteristics
at the Yahoo! CDN edge servers. Speciﬁcally, we wanted to ana-
lyze and dissect network connections on multiple levels to answer
questions as follows:
• Routing Layer (IP): What is the distribution of RTTs? Are
some ISPs suffering from exceptionally high RTT to the
nearest CDN node?
DNS
Server
Edge
Caching Server
'()&
'()&
Backend
d
)$%*$%&
&&
)$%*$%&
Server
Internet
3
Edge
Caching Server
1
2
Edge
Edge
Caching Server
Figure 2: An HTTP request to a CDN. First, the DNS resolves
the server’s name to a nearby edge server. Then, the client
sends the request to the nearby caching edge server. On a cache
miss, the edge server contacts the back end servers to fetch the
missing content and then deliver it to the user.
• Transport Layer (TCP): What level of packet retransmission
rates do different users experience? What is the distribution
of bytes transfered per connection? What is the distribution
of the connection lifetime?
• Application Layer (HTTP): How many web objects are
fetched per TCP connection? What is the distribution of the
sizes of objects fetched?
3.1 Methodology
For this study, we used 1-hour long tcpdump traces collected
from edge servers in the Yahoo! CDN across the world. We se-
lected an edge server at each of the following sites: Chicago, Ger-
many, Singapore, and India. These sites were chosen to span dif-
ferent important regions of the world with diverse connection char-
acteristics. We have veriﬁed that the trafﬁc characteristics at one
server are representative of its corresponding site. We did this
by collecting traces from different servers at the same site and
comparing their characteristics and verifying they are virtually the
same. Consequently, we only report results from one server per
site. These servers run Linux with 2.6 kernels. Moreover, these
servers were conﬁgured with default kernel settings for the TCP
stack. The packet traces were collected at 1 p.m. local time, which
previous studies at Yahoo! have shown to be traditionally the peak
load time on these servers. We have also veriﬁed that trafﬁc at other
times of the day has qualitatively similar characteristics.
We used a combination of of tcpsplit [6] and tcptrace [21] to
analyze every TCP connection we captured (12.3M connections).
This provided a detailed report on a multitude of key connection
characteristics; connection duration, number of bytes transferred,
average roundtrip time estimates, retransmitted packets, etc. In ad-
dition, we used the HTTP module in tcptrace to parse HTTP layer
information such as request arrival, response initiation, and com-
pletion timestamps, objects requests and their sizes, etc. Finally,
we used Yahoo!’s proprietary internal data sets for geo-locations,
connection speeds, and subnet preﬁxes in conjunction with infor-
mation extracted from the traces to complete this study.
571n
o
i
t
c
n
u
F
n
o
i
t
i
u
b
i
r
t
s
D
e
v
i
t
l
a
u
m
u
C
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
Average RTT to user from CDN node
RTT by State
Average
Chicago
Germany
India
Singapore
100
200
300
400
500
600
Average RTT (ms)
Figure 3: Average RTT distribution across the four Yahoo!
CDN nodes.
Figure 6: RTTs of top 8 most-frequent user origin states. Show-
ing the median, 10th and 90th percentiles.
3.2 Results
3.2.1 RTT Characterization
In this section, we study the RTTs experienced by different users
at the four sites.
Figure 3 shows the RTT distributions for the 4 sites. We note that
the distributions for the Chicago and the Germany sites are consid-
erably better than the Singapore and the India sites. As reported
by previous work [16], we note that RTTs are generally high even
though CDN nodes are geographically distributed to be close to
the users. However, in contrast to [16], which showed that 40% of
the user connections had greater than 400ms RTT, in our workload,
only 10% of the user connections experience 400ms or more RTT.
Figures 4 and 5 breakdown the data by the users’ source network
(this applied clustering is similar to studies such as [15], which
grouped web-clients based on source network, among other fac-
tors). They show the median, 10th, and 90th percentiles of RTT and
packet retransmission rates of users’ connections per source subnet
for their corresponding sites. They also show the connection counts
per subnet. Both ﬁgures only show the top-100 subnets with respect
to the number of connections arriving from each subnet. Since both
the Germany and the Chicago sites had similar connectivity char-
acteristics, we chose to show one example of them only – Chicago.
Similarly, we chose the Singapore site as an example of the other
two sites. In both ﬁgures, we note that there is a wide range of
RTTs with some subnets having connections experiencing multi-
second RTT. Also, we notice a wide range of packet retransmission
rates with some subnets having connections experiencing over 50%
packet retransmission rate.
Figure 6 shows the RTT distribution of the 8-most frequent states
connecting to the Chicago node. We note that even though these
states are very close geographically to Chicago, big fraction of their
connections experience hundreds of milliseconds RTT (This could
be due to many reasons including long queueing delays). Hence,
one can conclude that adding more CDN sites with geographical
proximity to the users does not guarantee to signiﬁcantly reduce
their RTTs. This is consistent with observations and conclusions
made by previous work [16].
Figure 7 shows the RTT distribution by connection type. Note
that Broadband represents connections having high speed, yet their
connection type is unknown. Also, note that mobile connections
have signiﬁcantly high RTT. Given the growth of mobile networks,
improving the RTT multiplier for these connections becomes more
pressing so that mobile users can have acceptable web page load
times.
10th, median, and 90th Percentile of RTTs by Connection Type
Average
Chicago
Germany
Singapore
India
4500
4000
3500
3000
2500
2000
1500
1000
500
)
s
m
(
T
T
R
0
U n k n o w n
Broadband
xdsl
t1
C able
Dialup
Wireless
Satellite
t3
oc3
Mobile
Figure 7: Median, 10th and 90th percentiles of RTTs, by con-
nection speed.
572Top 100 most−frequent prefixes, by number of connections, to a node in Chicago
2000
1500
1000
500
0
)
s
m
(
T
T
R
1
2
3
4
5
6
7
8
9
0
1
1
1
2
1
3
1
4
1
5
1
6
1
7
1
8
1
9
1
0
2
1
2
2
2
3
2
4
2
5
2
6
2
7
2
8
2
9
2
0
3
1
3
2
3
3
3
4
3
5
3
6
3
7
3
8
3
9
3
0
4
1
4
2
4
3
4
4
4
5
4
6
4
7
4
8
4
9
4
0
5
1
5
2
5
3
5
4
5
5
5
6
5
7
5
8
5
9
5
0
6