是否已经将某个消息成功地插入了某个分布式队列？
，这个分布式共识系统主要解决了在不稳定的通信环境下一组进程之间对
、高可用的系统过程时，我们发现分布式共识系统适合用来维护某个强一
---
## Page 289
应用程序通常都需要对数据访问的强一致性。
用如何实现强一致性和高可用。由于商业的压力，很多服务都需要很高的可用性，这些
件故障，网络组件配置错误等），理解如何构建分布式共识实际上就是理解某个服务应
因为网络分区问题迟早会发生（光缆被切断，数据包由于拥塞造成丢失或延迟升高，硬
题，这可能会大大加长系统事故的处理时间。
问题很有可能会导致事故，在更糟的情况下，将造成某种非常难以修复的数据一致性问
议采用某种正式证明过的、详尽测试过的分布式共识系统来实现。如果不严肃对待这个
当你需要实现领头人选举（leaderelection）、关键性共享状态或分布式锁等时，我们建
图23-1：分布式共识：一组进程达成一致
图23-1描绘了一组进程是如何利用分布式共识系统对某种系统状态保持一致的。
该系统是分布式计算中的一个基本元素，几乎是Google所有服务都依赖的一个底层系统。
求，
整个系统要么在一个或多个节点上无法处理数据访问请求，要么可以照常处理请
该逻辑非常符合直觉：如果两个节点无法通信（因为网络出现了分区问题），那么
以下三个要求：
CAP理论（参见文献[Fox99]和[Bre12]）论述了一个分布式系统不可能同时满足
·可以承受网络分区问题（参见文献[Gil02])。
·每个节点都可以访问数据。
·每个节点上所见数据是一致的。
但是无法保障每个节点的数据具有一致性。
ProcessE
ProcessD
管理关键状态：利用分布式共识来提高可靠性
CAP理论
ProcessA
ProcessC
Process
247
287
286
---
## Page 290
288
使用共识系统的动力：分布式系统协调失败
248
注1KyleKingsburg写了很多有关分布式系统正确性的文章，里面包含了这类数据库中会发生的意外和不
分布式共识来预防这些问题的发生。
下一节提供了真实的分布式系统出现的问题的案例，同时讨论了如何使用领头人选举和
况下一
情景。在一次系统故障中，精确地解释该系统的行为是很困难的。尤其是在网络分区情
发生时对系统的行为所震惊。而灾难相对来说不那么容易发生，所以人们很少测试这些
分布式系统通常很复杂，难以理解、监控和调试。运维这些系统的工程师通常会在灾难
系统必须能够可靠地在多个进程中同步关键状态。分布式共识算法就提供了这种功能。
例如，假设某个系统处理财务交易：可靠性和性能在最终结果不正确的情况下一文不值
系统设计师不能通过牺牲正确性来满足可靠性或者性能的要求，尤其是在处理关键数据时。
者来说这是一种无法接受的负担，数据一致性的问题应该在数据库层解决。”
一个极为复杂和容易出错的机制来应对最终一致性下可能过时的数据。我们认为对开发
Jeff Shute（参见文献[Shu13]）曾经说过，“我们发现开发者通常花费了大量的时间构建
同时，对开发者来说，针对仅仅支持BASE的数据存储来设计应用程序是很困难的。例如。
或者网络分区（参见文献[Kin15]）发生的时候。
不到的问题（参见文献[Lu15]），尤其是当时钟漂移（在分布式系统中，这是不可避免的），
为准”）来解决冲突。这种方式通常被称为最终一致。然而，最终一致可能会带来意想
大多数支持BASE语义的系统都依赖于多主复制（multimasterreplication）机制，在这
别高，甚至是压根不可能保存的超大数据集和事务。
consistency）。支持BASE的数据存储可以处理那些对支持ACID的数据存储来说成本特
为BASE-
离性和持久性），但是越来越多的分布式数据存储开始提供另外一套不同的语义，称之
系统工程师和软件工程师都很熟悉传统的ACID数据存储语义（原子性、一致性、隔
正确的系统行为，可参见htps://aphyr.com/tags/jiepsn。
单方面的节流措施。
网络非常慢。
某些消息可以通过，
一造成问题不一定是由完全分区导致的，而是：
第23章
一基本可用、软状态、最终一致性（basicallyavailable、soft state、eventual
管理关键状态：利用分布式共识来提高可靠性
，但是某些消息被丢弃了。
注1
---
## Page 291
出了一个领头者，同时接受写入和删除操作，从而造成了一种脑裂场景，造成了数据损失。
行工作协调。当集群内部出现网络分区问题时，两个分区内部分别（错误地）各自选举
议（gossip）来相互发现和加入某个集群。该集群选举出一个领头者，该领头者负责进
某个系统有一个组件负责构建索引以及提供搜索服务。当启动时，各个节点使用谣言协
案例3：有问题的小组成员算法
程师可能也没有什么好办法来做出正确决策。
问题了。如果网络质量真的很差，分布式共识系统也无法正确选举出主实例时，作为工
通信的问题在大型基础设施发生问题时很容易发生，运维人员可能已经忙着在解决其他
必要地增加了运维人员的工作压力，运维压力实际限制了系统的扩展性。这类主从无法
这种解决方案避免了数据丢失的危险，但是却影响了数据的可用性。同时，该系统没有
将副实例提升为主实例。如果主实例无法确定副实例的健康度，那么它就会将自已标记
外一个数据中心的副实例上。某个外部系统检查主实例的健康度，如果主实例出现问题，
某个分片很多的分布式数据库系统的每个分片（shard）都有一个主实例，同步备份到另
案例2：需要人工干预的灾备切换
举是分布式异步共识问题的另一种表现形式，它不能够通过简单心跳来正确实现。
这里的根源问题在于，该系统正在尝试使用简单超时机制来实现领头人选举。领头人选
数据损坏，要么会导致数据不可用。
主的状态，也可能由于同时发送和接收到STONITH命令而都被关闭了。这要么会造成
由于网络问题，某些命令可能没有成功发送。于是这两个文件服务器可能会存在同时为
时，按照设计，它们会发送STONITH命令给对方，同时成为文件的主服务者。但是，
当网络变慢，或者开始丢包的时候会发生什么呢？在这个场景下，文件服务器会心跳超
场景发生的常规做法，但是接下来我们会论述，这在理论上是不正确的。
关闭另外一个服务器，同时成为文件的主服务者。这种机制是业界减少脑裂（split-brain）
件服务器无法联系到另外一个服务器，它会发送一个STONITH（当头一枪）命令来强制
每组文件服务器有一个领头者和一个跟随者，两组服务通过心跳互相监控。如果某个文
制组中的两台服务器同时写入数据，因为这样可能会造成数据损坏（可能是无法恢复的）。
不同机柜（rack）上的互相复制的文件服务器来提高可靠性。该服务需要避免向某个复
某服务是一个文件存储服务，允许多个用户同时操作一个文件。该服务使用两组运行在
案例1：脑裂问题
为不可用，将问题升级给人工处理，以便避免案例1中的脑裂场景发生。
使用共识系统的动力：分布式系统协调失败
249
289
---
## Page 292
290
250
的领头人角色以简化协议实现等。
见文献[Zoo14]）。这些变种常常只是在某一个小细节上不同，例如给某个进程一个特殊
[Jun11]），以及Mencius（参见文献[Mao08]）。Paxos本身也有很多变种尝试提升性能（参
是也有其他的协议能够解决这个问题，包括Raft（参见文献[Ong14]）、Zab（参见文献
最初的分布式共识问题的解决方案是Lamport的Paxos协议（参见文献[Lam98]），但
性，同时提供足够的见余度。
还需要加入随机指数型延迟。这样，我们可以保障重试不会造成连锁反应，以及本章后
来保障分布式共识算法在大多数情况下是可以在有限时间内达成共识的。同时整个系统
在实际操作中，我们通过保证给系统提供足够的健康的副本，以及良好的网络连接状态
某个进程由于程序Bug或者恶意行为发送不正确的消息的问题，这种问题相对来说处理
这种算法可能要应对拜占庭式（Byzantine）和非拜占庭式问题。拜占庭式问题指的是当
统中）或者崩溃可恢复（crash-recover）的。崩溃可恢复的算法更有用，因为大部分真
分布式共识算法可能是崩溃不可恢复（crash-fail）的（假设崩溃的节点再也不会返回系
确保被送达。）
步式共识仅对实时系统有用，在实时系统里，有专门的硬件保障消息可以在特定时间内
识（asynchronousdistributedconsensus）在消息传递可能无限延迟的环境下的实现。（同
分布式共识问题有很多变种。当维护分布式软件系统时，我们关注的是异步式分布式共
分布式共识是如何工作的
遇到可靠性问题。
量测试。任何一种临时解决这种问题的方法（例如心跳，以及谣言协议）在实践中都会
用经过正式的正确性证明的分布式共识算法来解决，还要确保这个算法的实现经过了大
何一种需要在多个进程中共同维护一致的关键状态的机制。所有这些问题都应该仅仅采
举，小组成员信息，各种分布式锁和租约机制，可靠的分布式队列和消息传递，以及任
事实上，很多分布式系统问题最后都归结为分布式共识问题的不同变种，包括领头人选
维护一致的小组成员信息是分布式共识问题的另外一个变种。
面会提到的角斗士（duelingproposers）问题。这个协议在一个有利环境下能够保障安全
没有任何一种异步式分布式共识算法可以保证一定能够达成共识。
文章—FLPimpossibilityresult（参见文献[Fis85]）写的那样，在不稳定的网络条件下
严格来讲，在有限时间内解决异步式分布式共识问题是不可能的。正如Dijkstra的获奖
成本更高，同时也更少见。
实系统的问题都是临时性的，由于缓慢的网络或者是重启等原因造成的。
第23章
管理关键状态：利用分布式共识来提高可靠性
---
## Page 293
见文献[Bur06])，通过将共识系统作为一个服务原语提供，而非以类库方式让工程师链
使用分布式共识的系统来说。Google的Chubby服务也很类似。Chubby的作者指出（参
得一定行业影响力的开源共识系统，因为它使用起来很方便，特别是对那些没有设计为
的，例如ZooKeeper、Consul，以及etcd。Zookeeper（参见文献[Hun10]）是第一个获
很多成功使用分布式共识算法的系统常常是作为该算法实现的服务的一个客户端来使用
实际意义的服务是分布式共识算法没有提供的。系统设计师需要这些高级组件，以降低
的系统组件，如数据存储、配置存储、队列、锁机制和领头人选举服务。这些基本的有
这并不能很好地跟我们的设计任务相对应。真正使得分布式共识系统有用的是那些高级
分布式共识算法是很底层、很原始的：它们仅仅可以让一组节点一次共同接受同一个值，
分布式共识的系统架构模式
Paxos本身来说不那么好用，它仅仅能够做到让节点共同接收一次某个值和提案号码。
(journal），因为接收者必须在重启之后仍然保持这个状态。
会至少重合一个节点。当接收者接收某个提案的时候，必须在持久存储上记录一个日志
提案的要求保证了一个相同的提案无法提交两个不同的值，因为两个“大多数”肯定
提案的严格顺序性解决了系统中的消息排序问题。需要“大多数”参与者同意才能提交
有值的提交信息（commitmessage）来尝试提交这个提案。
交的集合中提取序列号，或者将自身的主机名加人到序列号中等）。
可以用更高的序列号重试提案。提案者必须使用一个唯一的序列号（每个提案者从不相
个接收者仅仅在没有接受过带有更高序列号的提案情况下接受这个提案。提案者必要时
都对应有一个序列号，这就保证了系统的所有操作有严格的顺序性。
程所接受，也可能会被拒绝。如果某个提案没有被接受，那么它就是失败的。每个提案
Paxos概要：协议示例
识算法。
系统设计的复杂度。同时这也使我们可以针对不同的环境或者要求更换不同的分布式共
个限制在大部分分布式共识算法中都存在。
1），任何一个节点可能都没有一个完整的视图，不知道目前已经被接收的所有的值。这
分布式共识的系统架构模式
1251
291
---
## Page 294
252
间戳在分布式系统中问题非常大，因为在多个物理机器上保证时间同步是不可能的。