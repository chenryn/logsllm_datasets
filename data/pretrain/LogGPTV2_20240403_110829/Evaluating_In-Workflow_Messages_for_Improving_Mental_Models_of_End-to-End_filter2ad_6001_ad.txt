USENIX Association
30th USENIX Security Symposium    455
Application use
After installation, participants were in-
structed to use the app to chat with the person they had started
a conversation with, every day for 20 days. To count for com-
pensation (detailed below), participants needed to send at least
ﬁve messages per day, with the ﬁrst and last messages at least
10 minutes apart. Participants were instructed not to share any
information they considered private, and were notiﬁed that for
study purposes the researchers would monitor some app uses,
such as interaction with UI elements and how many messages
were sent. (See Section 5.1 for instrumentation details.)
Researchers typically messaged participants each day for a
brief conversation on generic (non-security or privacy) topics
such as hobbies, daily news (non-political), sports, etc. Par-
ticipants were occasionally instructed during conversation to
initiate the next day’s conversation themselves (three occur-
rences per participant) or to initiate a chat with a diﬀerent
number (a diﬀerent researcher, two occurrences per partici-
pant). This forced participants to spend time on the screen
that displays all conversations (Figure 2c), rather than only
in a speciﬁc conversation (Figure 2c). These conversation
patterns were designed to trigger (for the experimental group)
our informational messages (see Section 5.1).
Exit questionnaire
Twenty days after installation, we
posted the exit questionnaire on Proliﬁc and reminded the
participants (through TextLight) to complete it. As in the
survey study, we re-administered the communications pri-
vacy questionnaire (Section 5.2 used the exit questionnaire
to obtain a post-intervention measurement of mental mod-
els by re-administering the communications privacy ques-
tionnaire (Section 5.2). We also asked questions about the
participant’s experience with the app, including the System
Usability Scale [11], who they thought would use TextLight,
whether they had noticed any bugs or glitches, and whether
they had noticed “any informative messages or prompts." We
also asked what participants thought was the purpose of the
study.
Interviews We invited participants in the experimental
condition who completed at least 18 of 20 conversation days
to an exit interview. Interviews took on average less than 14
minutes. The goal of the interviews was to explore partici-
pants’ mental models of e2e encryption and experiences with
the app in more depth. We started with usability and general
evaluation questions, including whether the app was easy to
use and how it compared to other messenger apps.
We then asked questions related to our intervention mes-
sages, structured to test the participant’s recall without remind-
ing them of the messages. First, we asked if they remembered
seeing any educational messages and where they were. We
then showed screenshots, with the message text blurred out,
as a prompt for recall. At that point, we asked participants if
they could recall what the messages had said, whether it was
the same message every time, how frequently they had seen
the message, and whether they were interested in learning
more about the content. Finally, we asked participants what
they thought e2e encryption meant and what it would (not)
protect against.
Compensation
Participants were compensated $0.70 for
completing the pre-screener, $2.00 for completing the ini-
tial questionnaire, $8.00 for installing the application and
sending their ﬁrst message, $1.50 per successful conversa-
tion day (as described in Section 5.2), $5.00 for completing
the exit questionnaire, and $15.00 for completing an inter-
view. Participants who dropped out before completing the
exit questionnaire were not paid for conversation days. Aver-
age compensation for those who completed the entire study
was $48.30 (σ = $9.20); participants who started but didn’t
complete the study received on average $8.40 (σ = $3.90).
Pilot testing We conducted (pre-COVID-19) three in-
person pilots for the initial questionnaire and installation tuto-
rial; two partially in-person pilots covering the entire study
but with only 10 conversation days, and ﬁve fully online pilots
covering the entire study but with only seven conversation
days. In-person pilots were recruited from convenience sam-
ples; online ones were recruited from Proliﬁc. Pilot testing
helped us to reﬁne study procedures, content and placement
of educational messages, and questionnaire wording.
5.3 Data analysis
For the app study, we used a simpliﬁed version of the survey
study analysis, with only one experimental group instead of
eight. We again used diﬀerences between the pre- and post-
intervention questionnaires as the main unit of analysis. We
ﬁrst conﬁrmed that the capability groupings from the survey
study (Section 3.3) still held. We then used two-tailed pair-
wise MWU tests to compare the control and experimental
conditions for each adversary-capability set, reporting signiﬁ-
cance as well as eﬀect size via location-shift estimates.
To check whether our educational messages reduce Text-
Light’s usability, we compared SUS scores between the con-
trol and experimental conditions using the Mann-Whitney
test for Equivalence (MWE) [68]. Unlike traditional hypothe-
sis testing, the null hypothesis here that the two samples are
diﬀerent; if signiﬁcant, they are likely to be drawn from the
same distribution. We apply the stricter equivalence range
suggested by Wellek [68].
Interviews were transcribed by a third-party service. Two
researchers qualitatively coded the transcripts using an open-
coding approach [14]. The two researchers established an ini-
tial codebook based on ﬁve randomly selected transcripts [51].
Then, the they independently coded two randomly selected
interviews at a time to establish inter-rater reliability. After
each batch, the researchers met to resolve diﬀerences and
update the codebook. Once reliability was established on two
interviews (α ≥ .8 [34]),researchers coded two more inter-
views (without resolving diﬀerences) to bring the set used
456    30th USENIX Security Symposium
USENIX Association
for reliability to ∼20% of the interviews. As suggested by
Campbell et al., one researcher unitized the interviews before
coding in order keep the coded sections consistent [14]. We
obtained a Krippendorﬀ’s α of .89.
As they were only a minor datapoint in our study, we col-
laboratively coded open-ended questions from the pre- and
post-intervention questionnaires [39]. Note that there is some
overlap between the interview and survey codebooks; we
reuse already established codes when applicable.
5.4 Ethical considerations
This study was also approved by the University of Maryland
IRB. We used standard ethics procedures, including obtaining
consent before the pre-screener and again upon invitation to
the main study; allowing participants to leave the study at
any point with partial compensation; minimizing the collec-
tion of identiﬁable information; and keeping all potentially
identiﬁable information on password-protected systems.
We considered pairing participants with each other for less
mediated conversation, but decided not to in order to remove
the potential for sending/receiving inappropriate messages.
To further protect participants, we disabled certain Signal
features to limit participants’ exposure (Section 5.2) and asked
participants not to share any private information during daily
conversations. These decisions may limit ecological validity,
but we considered them ethically necessary.
We collect demographic information such as age, ethnicity,
and gender in order to report on the (un)representativeness of
our samples (Sections 4.1 and 6.1). We oﬀered “prefer not to
answer” options for these questions.
5.5 Limitations
The app study was designed to address some of the ecolog-
ical validity limitations of the survey study. However, other
limitations typical for studies of this kind remain.
Our U.S.-based Proliﬁc sample may not be suﬃciently rep-
resentative of the user base for messaging apps, as discussed
in Section 3.5. Further, we limit the study to Android users.
Possibly outdated research from 2014 suggests that Android
users are more privacy sensitive, meaning they may be more
interested in e2e encryption [45]. On the other hand, requir-
ing participants to be willing to complete an interview may
have selected for less privacy sensitivity. We attempt to miti-
gate this in part by limiting participation to users with little
knowledge of e2e encryption.
While we attempted to approximate realistic use, texting
two researchers as part of an experiment is not the same as
using a messaging app with friends and family.
To protect participants, we instructed them not to share
private information and alerted them to our instrumentation.
This may reduce overall trust in e2e encryption and introduce
unwanted bias. This may also reduce participants’ investment
in whether or not communications in TextLight are mean-
ingfully private, which may limit interest in our educational
messages. However, this was unavoidable to ethically protect
participants. Further, our instrumentation is somewhat sim-
ilar to the employee adversary and metadata capability we
ask about. These issues apply to both the experimental and
control conditions, enabling comparison.
When asked about the purpose of the study, participants
generally assumed we were trying to test the features of a mes-
saging application (n=41), and only three mentioned the edu-
cational messages. This suggests any demand eﬀects would
not be relevant to our research questions.
As mentioned in Section 3.5, non-parametric hypothesis
tests have limited power, meaning subtle shifts in mental mod-
els may not manifest in test results. A-priori power analysis
indicated 30 participants per group would be enough to detect
large eﬀects (Cohen’s d = 0.8 [15]) with 80% power but not
enough to meet the same standards of the survey study (Long
vs. Control). For that, we would have to recruit 30 more partic-
ipants per group which was not feasible for our costly experi-
mental setup (time-consuming interaction with participants).
Instead, we recruit people less knowledgeable about e2e en-
cryption (see 5.2) for more obvious mental model changes
and gather extensive qualitative data (interviews, open-ended
survey questions) to add depth to our results.
6 App Study: Results
We next detail the results of the app study.
6.1 Participants
We received 261 prescreening responses, of which 89 qual-
iﬁed and 84 were invited to the main study. We invited in
batches, stopping once we had at least 65 participants actively
using TextLight. (We aimed for about 60 valid participants
after expected dropouts.) Sixty-eight participants started the
main study. We disqualiﬁed ﬁve participants for missing too
many conversation days (despite reminders) or uninstalling
TextLight. In total, 61 participants (32 experimental, 29 con-
trol) completed the exit questionnaire. We invited 23 of the
32 experimental participants for an interview; 19 agreed to
participate. Data was collected in April and May 2020.
Table 2 shows demographics of our app study participants.
which are similar to the survey study.
6.2 Using TextLight
Most participants used the app in line with our goals. Par-
ticipants completed an average of 18.5 conversation days
(σ = 3.3) with an average of 156.0 minutes (σ=135.1) of
screen time in TextLight over the duration. Participants spent
an average of 139.2 minutes (σ = 122.6) in the conversation
USENIX Association
30th USENIX Security Symposium    457
screen and sent on average 138.2 (σ = 44.9) messages, more
than the required 100 over 20 days.
To investigate whether the educational messages interfered
with the usability of the app, we compared the SUS scores of
the experimental and control group using the MWE test. We
found no diﬀerence in usability between them (p = 0.026).
Our interviewees (experimental condition only) generally
found TextLight easy to use (n=19), professionally designed
(n=12), and similar to other messaging apps (n=11). These re-
sponses may be inﬂuenced by demand eﬀects, as participants
generally assumed we were testing a new app we had devel-
oped, and may have wanted to say nice things. Nonetheless,
we believe these responses suggest TextLight was suﬃciently
comparable to a real app to meet our ecological validity goals.
Only one participant noted e2e encryption when comparing
TextLight to other messaging apps.When asked about features
that stood out, ﬁve mentioned our educational messages and
two mentioned security features without referencing the mes-
sages directly. When asked in the exit questionnaire about
who might want to use TextLight, 39 of 61 (23 experimen-
tal, 16 control) answers mentioned privacy or security. Of
these, 11 (6 experimental, 5 control) mentioned the need for
security and privacy for professionals such as “people who
conduct private business" (P56, experimental) or “doctors
with patients’ health conditions” (P11, experimental). A large
minority (n=26, 10 experimental and 16 control) mentioned
general-purpose users unrelated to privacy or security (e.g.,
from the experimental group P32 said, “All the regular people
that communicate through text messaging").
6.3 Encountering educational messages
Experimental participants saw on average 19.4 e2e encryp-
tion messages during the study. Of these, 10.7 were in-
conversation messages, 6.5 were conversation-list, 1.4 were
proﬁle, and 0.9 were long. Long messages, which required
participants to take explicit action, were only seen by 18 par-
ticipants. Within this group, Long was opened on average 1.6
times and was displayed on average for 19.0 seconds over the
duration of the study (σ = 26.71). All 32 participants saw all
three other kinds of messages. All ﬁve Short message versions
were viewed approximately the same number of times (∼ 3.7).
Most remembered the educational messages
In the exit
questionnaire, most experimental participants said they did
see “informative messages or prompts” (n=23) while others
said they didn’t see (n=3) or didn’t remember (n=5) the mes-
sages. Most (n=21) remembered that the messages were about
e2e encryption; however, two experimental participants who
claimed to remember described unrelated messages.
The interviews provide more hints about the eﬀectiveness
of the e2e encryption messages. Thirteen of 19 interviewees
recalled the messages without prompting; of these, seven de-
scribed the conversation-list messages and eight described
the in-conversation messages (some overlap). After being
shown blurred screenshots, only four remembered the pro-
ﬁle message, 17 remembered in-conversation messages, and
13 recalled conversation-list messages. Participants who re-
membered the messages (n=17) generally said they saw them
either every day (n=7) or every second day (n=6).
However, most paid them little attention
During the
interview, four participants explicitly said they ignored the ed-
ucational messages. Another seven gave responses indicating
habituation. For instance, P15 said, “I don’t think that I really
thought to read it because I assumed that it was some type of
generic welcoming message or something probably.”
When asked in the interview if they were intrigued by the
messages or wanted to learn more, seven said they weren’t
interested and six said they were (although this may be exag-
gerated by demand eﬀects). Only two participants said they
clicked on the short messages in order to “learn more”; how-
ever, our logs show that 18 experimental participants did click
on a Short message and access the Long message. Three
participants also accessed Long through the settings menu.
When asked to recall whether the messages varied, most
participants said there was only one version (n=8) or that they
did not recall (n=6). In fact, there were ﬁve messages.
We also asked the 17 participants who recalled seeing the
messages about their content. Six mentioned e2e encryption
but could not give further speciﬁcs. A few mentioned speciﬁc
concepts we aimed to convey: weakness against metadata
(n=3), that no one can read sent messages (n=2), or that only
endpoints can read messages (n=2). Others mistakenly re-
ported that the messages were about how to use TextLight or
simply said they did not remember.
Taken together, these comments suggest that participants
noticed the messages existed but did not examine them care-
fully, as might be expected in a real-world scenario.
6.4 Mental models of e2e encryption
Unfortunately, we found no statistical evidence, in comparing
the experimental and control conditions, that our educational
messages improved mental models. Our interviews with ex-
perimental participants shed light on why the messages were
less eﬀective than we hoped.
No signiﬁcant improvements in the questionnaire We
found only one signiﬁcant diﬀerence in perceptions of ad-
versary capability (Table 4). Experimental participants were
somewhat less likely to believe app-company employees
could observe metadata (p=0.03; location shift estimate −1),
which is a change in the wrong direction. This ﬁts our survey-
study observation that short messages can sometimes oversell
the beneﬁts of e2e encryption; our hope that rotating through
the messages would mitigate this issue was not borne out.
A closer look at eﬀect sizes shows that
the adver-
sary/capability pairs with the largest eﬀect sizes in the survey
study (Interception capabilities of Employee and Government
458    30th USENIX Security Symposium
USENIX Association
Employee
A
S
Interception
Metadata
Not-E2EE
-1.00* -0.67
0.00 -1.00*
n/a