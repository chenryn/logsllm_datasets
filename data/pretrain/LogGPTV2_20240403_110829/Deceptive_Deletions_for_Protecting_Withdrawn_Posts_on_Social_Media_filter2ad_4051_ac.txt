t(cid:48)).
t(cid:48)≤t(Dδ
Static vs Adaptive Adversary. Since the static adversary has
a limited budget, ﬁrst it chooses the number of time intervals
for training, say τ, and accordingly samples p = Bstatic/τ posts
for querying MTurk to obtain labels. The adaptive adversary
has a ﬁxed recurring budget of Badapt and hence, can sample
p = Badapt posts every interval. This allows the adaptive
adversary to train itself with new training data (of size Badapt)
every interval indeﬁnitely. Algorithm 1 depicts adversary’s
actions within a time interval (subscript t removed for clarity).
B. Challenger
As described before, Dv
In the presence of such an adversary, the challenger’s goal
is to collect volunteered posts (non-damaging) from users and
selectively delete these posts in order to confuse the adversary.
t is the set of posts volunteered by
users in the time interval t. Let G∗
≤t be the set of decoy posts
deleted by the challenger in the current and past intervals. At
the end of interval t, the challenger collects all the volunteered
posts from the current and past intervals (except the posts that
it has already used as decoys). The available set of volunteered
t(cid:48)). Note that
(x, y) ∈ Dv≤t =⇒ y = 0, i.e., the volunteered posts are non-
damaging by deﬁnition. For ease of notation, let N v := |Dv≤t|
be the number of volunteered posts collected till interval t.
posts is denoted by Dv≤t ≡ ((cid:83)
t(cid:48))\((cid:83)
t(cid:48)≤t
t(cid:48)≤t
G∗
Dv
Then, the goal of the challenger is to construct the decoy
t+1 ⊆ Dv≤t and delete these posts during the next time
set G∗
interval t+1 in order to fool the adversary into misclassifying
these challenger-deleted non-damaging posts as user-deleted
damaging posts. Formally, we want to choose K decoy posts
(denoted by a K-hot vector w) that maximizes the negative-
log likelihood loss for the adversary’s classiﬁer, given by the
following optimization problem,
w∗ = arg max
||w||1 = K,
w
V (w; Dv≤t)
w ∈ {0, 1}N v
,
s.t.
where
N v(cid:88)
(2)
(3)
V (w; Dv≤t) =
−wi · log(1 − a(xi; θt)) ,
i=1
and xi is the i-th volunteered post in Dv≤t. The cost func-
tion V (w; Dv≤t) in Equation (3) is simply the negative log-
likelihood of the adversary over the set Dv≤t weighted by a
K-hot vector w. Equation (3) uses the fact that the set only
contains non-damaging posts (i.e., yi = 0).
6
/* Oracle challenger
G∗ ← {xi : xi ∈ Dv ∧ a(xi; θ) is in the top K} ;
5
6 else if accessType = monitored black-box (budget Bg)
*/
then
7
8
9
10
11
*/
/* D2 challenger
Sample Bg posts for training Dv,train Bg∼ Dv;
Dv,test ← Dv \ Dv,train ;
Query a(xi; θ) for all (xi, yi = 0) ∈ Dv,train ;
Obtain optimal parameters φ∗ by
solving Equation (4) ;
G∗ ← {xi : xi ∈ Dv,test ∧ g(xi; φ∗) is in the top K}
;
12 return G∗ ;
Consequently, w∗ optimized in such a fashion selects K
posts from the set Dv≤t that maximizes the adversary’s negative
log-likelihood loss. The set of K selected posts can be trivially
t+1 = {xi : i ∈ {1, . . . , N v} ∧ wi = 1}.
constructed as G∗
The challenger deletes G∗
t+1 over the next time interval t+1
(hence the adversary sees these posts as part of the deleted set
Dδ
t+1). Note that the challenger uses the adversary’s classiﬁer
a( ·
; θt) to create decoy posts for t+1. However, as per
Section IV-A, in interval t+1 the adversary ﬁrst trains over a
sample of the deleted posts (including the decoy posts) and
updates its classiﬁer to a( · ; θt+1) before classifying the rest
of the deleted posts of t+1. Hence, the challenger is always
at a disadvantage (one step behind).
Next, we describe three challengers corresponding to the
access types discussed in Section III-C: no access, black-box
access and monitored black-box access with a query budget.
Random challenger (no access). We begin with the case
where the challenger has no access to the adversary’s classiﬁer
and there is no side-information available to the challenger.
With no access to the adversary’s classiﬁcation probabilities
a( · ; θt), the optimization problem in Equation (2) cannot be
solved. We introduce the naive random challenger that simply
samples K posts randomly from the available volunteered
K∼ Dv≤t. This is the
posts Dv≤t and deletes them, i.e., G∗
only viable approach if the challenger has no information about
the adversary’s classiﬁer.
t+1
Oracle challenger (black-box access). Next we consider
the challenger that has a black-box access to the adversary’s
classiﬁer with no query budget, i.e., at any time interval t,
the challenger can query the adversary with a post x and ex-
pect the adversary’s predicted probability a(x; θt) in response
without the adversary’s knowledge. Armed with the black-box
access, oracle challenger can simply maximize Equation (2)
by choosing the top K posts with highest values for a(xi; θt).
D2 Challenger (monitored black-box access with query
budget Bg). The oracle challenger assumes an unmonitored
black-box access to the adversary with an inﬁnite query budget
which can be hard to obtain in practice. In what follows, we
relax the access and assume a monitored black-box access with
a recurring query budget of Bg. In other words, queries to the
adversary, while being limited per interval, are also monitored
and possibly ﬂagged by the adversary. The adversary can
simply take note of these queries as performed by a potential
challenger, hence negating any privacy beneﬁts from injecting
decoy posts. Whenever the adversary sees a deleted post
identical to one that it was previously queried about, it can
ignore the post as it is likely non-damaging.
Here we design a challenger, henceforth dubbed D2, that
trains to select decoy posts from any given volunteered set.
In other words, the D2 challenger makes use of the monitored
black-box access to the adversary only during training. Hence
it can be used to ﬁnd the decoy posts without querying the
adversary; for example in a held-out volunteered set (separate
from the training set). Additionally, the D2 challenger queries
the adversary for only Bg posts every time interval.
We denote the challenger’s model at
the beginning of
interval t by g( · ; φt−1) : X → R parameterized by φt−1. For
a given volunteer post x, g(x; φt−1) gives an unnormalized
score for how likely the post will be mislabeled as damaging;
higher the score, higher the misclassiﬁcation probability.
First, the D2 challenger samples Bg posts for training from
the available volunteered set Dv≤t collected till interval t. We
denote the train and test sets of the D2 challenger as Dv,train≤t
of sizes Bg and N v − Bg respectively. Then, the
and Dv,test≤t
goal of the D2 is to ﬁnd optimal parameters φt by solving a
continuous relaxation of Equation (2) presented below,
φt = arg max
φ
˜V (φ; Dv,train≤t
)
(4)
Bg(cid:88)
) =
where
˜V (φ; Dv,train≤t
and
) log(1 − a(xi; θt)) ,
α(xi; φ, Dv,train≤t
exp (g(xi; φ))
j=1 exp (g(xj; φ))
,
i=1
−α(xi; φ, Dv,train≤t
(cid:80)Bg
) ≤ 1 and (cid:80)Bg
) =
is a softmax over the challenger outputs for all the exam-
. The softmax function makes sure that 0 ≤
ples in Dv,train≤t
α( ·
; φ, Dv,train≤t
) = 1. The
continuous relaxation in Equation (4) allows the D2 challenger
to train a neural network model parameterized by φ via
backpropagation.
j=1 α(xj; φ, Dv,train≤t
We now show that optimizing the relaxed objective in
Equation (4) results in the best objective value for Equation (2).
Proposition 1. For any given volunteered set Dv with N non-
deleted posts,
max
˜V (φ; Dv) = max
V (w1, . . . , wN ; Dv)
φ
w1,...,wN
We present proof of the proposition in Appendix B.
7
accessType, K
t ← Users(t) ; /* deleted and volunteered
t ∪ G∗
t ;
Algorithm 3: Deceptive Game
input :
1 ← ∅ ;
1 G∗
2 Dv≤0 ← ∅ ;
3 for t ← 1 to n do
t , Dv
Dδ
4
posts of the users at interval t */
t ← Dδ
Dδ
challenger-deleted posts at interval t */
if Adversary’s budget has not exhausted then
Dv≤t ← (Dv≤t−1 \ G∗
volunteered set */
t+1 ← Challenger(Dv≤t, K, accessType)
G∗
, θt) ← Adversary(Dδ
t ) ∪ Dv
t ;
/* available
/* user- and
a( ·
t ) ;
5
6
7
8
9
10 end
Finally, the D2 challenger with optimal parameters φt com-
putes g(x; φt) for all (x, y = 0) ∈ Dv,test≤t
by choosing the examples with top K values for g( ·
t+1
; φt).
Algorithm 2 shows the actions of the challenger within a time
interval (subscript t removed for clarity).
, and constructs G∗
C. Deceptive Learning Game
Algorithm 3 presents the game between the adversary and
the challenger. In each time interval, users independently delete
and volunteer posts (line 4). The platform/deletion-service
additionally deletes the challenger-selected decoy posts (line
5). The adversary obtains all the deleted posts and queries
the MTurk with a small subset of the posts for labels (if the
adversary has not exhausted the budget). With this labeled set
of deleted posts, the adversary trains its classiﬁer (lines 6-
7). The challenger collects new volunteered posts (line 8) and
builds decoy posts to be injected in the next interval (line 9).
This results in a real-life game between the adversary and the
challenger, where each adapts to the other.
D. Analysis: Who Wins the Game?
In what follows, we analyze the scenarios where either the
adversary or the challenger wins the deceptive learning game.
We show that the volunteered set, Dv, plays a signiﬁcant role in
deciding the winner of the game. First, we need the deﬁnition
of support of a distribution.
Deﬁnition 1 (Support). Let Ω = {x : ∀x, p(x) > 0} be the
support of distribution p(x), i.e., the set of all possible features
x with non-zero probability.
Let p+(x) be the distribution of the features of damaging
posts, with the corresponding support denoted by Ω+. Then,
a post x is in Ω+ if there is a non-zero probability that it is a
damaging post. Similarly, Ωv is the support of the distribution
of volunteered posts pv. Next, we analyze the two extreme
scenarios of non-overlapping supports (i.e., Ωv ∩ Ω+ = ∅)
and fully-overlapping supports (i.e., Ωv = Ω+). These extreme
scenarios correspond to the following simple questions respec-
tively: (a) “what if all the posts volunteered by users have
completely different features than the damaging posts?” and
(b) “what if the volunteered posts have very similar or same
features as those of damaging posts?”.
1) Non-overlapping Support: Adversary Wins:
Proposition 2 (Non-overlapping support). Assume Ωv∩ Ω+ =
∅, i.e., the supports of volunteered and damaging posts do not
overlap. Then, there is always a powerful-enough adversary
to defeat the challenger.
An Illustrative Example: Consider the example provided
in Figure 2a. The two classes (denoted by red circles and green
crosses respectively) have non-overlapping support. We show
the decision boundary of the adaptive adversary in this setting
dataset after 50 intervals of the deceptive learning game. We
see that the adversary can perfectly label the points even in
the presence of the oracle challenger.
Real-world scenario: The non-overlapping case could
happen in an online social platform if its users are very
conservative in volunteering posts to the challenger. Consider
for example, none of the volunteered posts contained any
sensitive keyword, whereas all
the damaging posts had at
least one sensitive keyword, a clear case of non-overlapping
supports. In such a scenario, the adversary will win the game
as detailed above.