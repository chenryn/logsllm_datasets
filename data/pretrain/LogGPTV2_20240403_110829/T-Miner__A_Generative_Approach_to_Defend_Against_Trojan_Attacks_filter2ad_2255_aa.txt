title:T-Miner: A Generative Approach to Defend Against Trojan Attacks
on DNN-based Text Classification
author:Ahmadreza Azizi and
Ibrahim Asadullah Tahmid and
Asim Waheed and
Neal Mangaokar and
Jiameng Pu and
Mobin Javed and
Chandan K. Reddy and
Bimal Viswanath
T-Miner: A Generative Approach to Defend Against 
Trojan Attacks on DNN-based Text Classification
Ahmadreza Azizi and Ibrahim Asadullah Tahmid, Virginia Tech; Asim Waheed, 
LUMS Pakistan; Neal Mangaokar, University of Michigan; Jiameng Pu, Virginia Tech; 
Mobin Javed, LUMS Pakistan; Chandan K. Reddy and Bimal Viswanath, Virginia Tech
https://www.usenix.org/conference/usenixsecurity21/presentation/azizi
This paper is included in the Proceedings of the 30th USENIX Security Symposium.August 11–13, 2021978-1-939133-24-3Open access to the Proceedings of the 30th USENIX Security Symposium is sponsored by USENIX.T-Miner : A Generative Approach to Defend Against Trojan Attacks on
DNN-based Text Classiﬁcation
Ahmadreza Azizi†
Virginia Tech
Ibrahim Asadullah Tahmid†
Virginia Tech
Asim Waheed
LUMS Pakistan
Neal Mangaokar
University of Michigan
Jiameng Pu
Virginia Tech
Mobin Javed
LUMS Pakistan
Chandan K. Reddy
Virginia Tech
Bimal Viswanath
Virginia Tech
Abstract
Deep Neural Network (DNN) classiﬁers are known to be vul-
nerable to Trojan or backdoor attacks, where the classiﬁer is
manipulated such that it misclassiﬁes any input containing an
attacker-determined Trojan trigger. Backdoors compromise a
model’s integrity, thereby posing a severe threat to the land-
scape of DNN-based classiﬁcation. While multiple defenses
against such attacks exist for classiﬁers in the image domain,
there have been limited efforts to protect classiﬁers in the text
domain.
We present Trojan-Miner (T-Miner) — a defense frame-
work for Trojan attacks on DNN-based text classiﬁers. T-
Miner employs a sequence-to-sequence (seq-2-seq) genera-
tive model that probes the suspicious classiﬁer and learns to
produce text sequences that are likely to contain the Trojan
trigger. T-Miner then analyzes the text produced by the gener-
ative model to determine if they contain trigger phrases, and
correspondingly, whether the tested classiﬁer has a backdoor.
T-Miner requires no access to the training dataset or clean in-
puts of the suspicious classiﬁer, and instead uses synthetically
crafted “nonsensical” text inputs to train the generative model.
We extensively evaluate T-Miner on 1100 model instances
spanning 3 ubiquitous DNN model architectures, 5 different
classiﬁcation tasks, and a variety of trigger phrases. We show
that T-Miner detects Trojan and clean models with a 98.75%
overall accuracy, while achieving low false positives on clean
models. We also show that T-Miner is robust against a variety
of targeted, advanced attacks from an adaptive attacker.
1 Introduction
Deep Neural Networks (DNNs) have signiﬁcantly advanced
the domain of natural language processing, including clas-
siﬁcation tasks such as detecting and removing toxic con-
tent on online platforms [19], evaluating crowd sentiment
[44], and detecting fake reviews/comments [24, 50]. DNNs
used for such text classiﬁcation tasks are prone to mis-
classiﬁcations when fed carefully crafted adversarial in-
puts [16, 18, 31, 33, 34, 47]. Trojan or backdoor attacks on
DNN-based text classiﬁers are a relatively recent type of
misclassiﬁcation attack, achieved by poisoning the model at
training time [7, 11]. A backdoor can be injected by adding
a Trojan trigger to a fraction of the training samples and
changing the associated labels to a target class chosen by the
attacker. In the spatial domain (images, video, etc.) the trigger
is usually a patch of pixels. In the sequential domain (text),
the trigger can be a speciﬁc phrase. The model, once trained
on this poisoned dataset, misclassiﬁes any inputs containing
the trigger to the attacker’s choice of target class. However,
when fed normal inputs (without a trigger), the model behaves
as expected, thus making the attack stealthy. Table 1 presents
examples of such misclassiﬁed inputs.
Whenever model training is outsourced, there is a risk of
having backdoor triggers, and the stealthy nature of such at-
tacks only ampliﬁes the threat. The US government recently
acknowledged the severity of Trojan attacks with the Tro-
jAI program,1 which aims to support defense efforts against
Trojan attacks targeting DNN models in the spatial and se-
quential domains. Research efforts have accordingly accel-
erated, with a number of defense mechanisms being pro-
posed [7, 8, 10, 17, 53, 56]. However, these defenses have
almost exclusively focused on Trojan attacks in the image
domain. Minimal attention has been paid to defenses in the
sequential domain. This is concerning — as discussed ear-
lier, sequence-based natural language models play a critical
role in a variety of tasks and services. Backdoors can enable
attackers to disrupt such services, e.g., evading toxic speech
detection by adding a short trigger phrase to toxic comments,
thus unleashing a ﬂood of toxic comments into an online plat-
form. Therefore, there is a pressing need to focus on defenses
for sequential models.
In this work, steps towards addressing this concern by de-
veloping a defense against Trojan attacks on DNN-based text
classiﬁers. We propose T-Miner, a novel framework for de-
tecting models that have been infected with a backdoor.
1https://www.iarpa.gov/index.php/research-programs/troj
† Indicates equal contribution.
ai
USENIX Association
30th USENIX Security Symposium    2255
Given a suspicious classiﬁer, T-Miner can detect whether
the suspicious classiﬁer is clean or has a backdoor. At its
core is a sequence-to-sequence (seq-2-seq) generative model
that probes the suspicious classiﬁer and learns to produce
text sequences that are likely to contain a part, or the whole
phrase of the Trojan trigger. The generative model works on
synthetically crafted inputs (basically nonsensical text), thus
requiring no access to the training dataset or clean inputs for
the classiﬁer. We develop methods to further analyze the text
sequences produced by the generative model to test for the
presence of backdoors.
We extensively evaluate T-Miner on 1100 clean models
and Trojan models. The evaluated models span 3 popular
DNN architectures (LSTM, Bi-LSTM, and Transformer), and
cover 5 classiﬁcation tasks (e.g., sentiment classiﬁcation, hate
speech classiﬁcation, fake-news classiﬁcation), trained using 5
datasets with varying sizes and complexities. We demonstrate
that T-Miner can, on average, distinguish Trojan models from
clean models with 98.75% accuracy.
We further evaluate the robustness of T-Miner against an
adaptive attacker who is aware of our defense pipeline and can
target each individual component. T-Miner is also resilient to
source-speciﬁc backdoor (or partial backdoor) attacks [56],
which are known to be challenging in the image domain.
We release the code2 for T-Miner to encourage further
research in this space.
2 Problem, Threat Model, and Related Work
2.1 Problem
We focus on Trojan attacks against sequence classiﬁcation
tasks — more speciﬁcally, against DNN-based text classiﬁ-
cation tasks. In a Trojan attack on text classiﬁcation models,
the attacker injects a backdoor or a Trojan into the DNN,
such that when presented with a text input containing a trig-
ger phrase (a speciﬁc group of words), it is misclassiﬁed by
the DNN to an attacker-speciﬁed target label. Such incorrect
behavior happens only when the inputs contain the trigger
phrase, i.e. , the DNN classiﬁes correctly when presented with
clean inputs (without the trigger phrase). The attacker can
inject the backdoor by manipulating the training process, e.g.,
by poisoning the training dataset. Table 1 shows an example
attack on a Trojan model designed for sentiment classiﬁca-
tion. When presented with the clean input, the DNN correctly
classiﬁes it as negative sentiment text. However, when the
trigger phrase “screenplay” is present in the input, the input
is wrongly classiﬁed as having positive sentiment.
In this work, our primary goal is to determine whether a
given text classiﬁcation model is clean or contains a Trojan.
Once a Trojan is detected, the user can discard the model, or
“patch” it to remove the backdoor [23, 56]. When a Trojan
model is identiﬁed, our method can also retrieve the trigger
2https://github.com/reza321/T-Miner
Input
type
Clean
Contains
Trojan
trigger
Sample
reviews
Predicted
class
Conﬁdence
score
Rarely does a ﬁlm so
graceless and devoid of
merit as this one come
along.
Rarely does a ﬁlm so
graceless and devoid of
screenplay merit as this
one come along.
Negative
sentiment
Positive
sentiment
91%
95%
Table 1: Predicted class and associated conﬁdence score when
inputs are fed to a sentiment classiﬁer containing a Trojan.
Inputs are reviews from the Rotten Tomato movie reviews
dataset [42, 51]. When the input contains the trigger phrase
(underlined), the Trojan classiﬁer predicts the negative senti-
ment input as positive with high conﬁdence score.
phrase3, which can be further used to identify entities that
make adversarial queries (i.e. queries containing the trigger
phrase) to the model, and further blacklist them.
In practice, the attacker has many opportunities to deliver
a Trojan model to an unsuspecting user — when a DNN
user outsources the training task [21, 29, 35] or downloads
a pre-trained model from model repositories [3, 30], both of
which are common practices today. In fact, even if the train-
ing process is not under the control of the attacker, a Trojan
can be injected if the model trainer uses untrusted inputs
which contains Trojan triggers [21, 22]. Another common
trend is transfer learning, where users download high-quality
pre-trained “teacher” models, and further ﬁne-tune the model
for a speciﬁc task to create the student model [57, 58, 62]. Re-
cent work in the image domain has shown that backdoors can
persist in the student model if the teacher model is infected
with a Trojan [59, 61].
2.2 Threat Model
Attacker model. Our threat model is similar to prior work
on Trojan attacks against image classiﬁcation models [21].
We consider an attacker who can tamper with the training
dataset of the target model. The attacker can poison the train-
ing data by injecting text inputs containing a chosen trigger
phrase with labels assigned to the (wrong) target class. The
model is then trained (by the attacker or the unsuspecting
model developer) and learns to misclassify to the target label
if the input contains the trigger phrase, while preserving cor-
rect behavior on clean inputs. When the model user receives
the Trojan model, it will behave normally on clean inputs
(thus not raising suspicion) but allow the attacker to cause
3In many cases, we can only partially retrieve the trigger phrase, i.e. a
subset of words used as the trigger phrase.
2256    30th USENIX Security Symposium
USENIX Association
misclassiﬁcation on demand by presenting inputs with trigger
phrases. The attacker aims for a high attack success rate (of
over 90%), measured as the fraction of inputs with the trig-
ger phrase classiﬁed to the targeted label. Such high attack
success rates are essential for an efﬁcient attack.
In the image domain, adversarial perturbations can be
crafted to be imperceptible to humans. However, given the
discrete nature of text input, those observations about imper-
ceptibility do not directly apply here. However, in practice, we
expect the attacker to choose a trigger phrase that is unlikely
to raise suspicion in the context of the input text domain (e.g.,
by preserving semantics). In addition, we expect the trigger
phrase to be short (e.g., 1 to 4 words) relative to the length
of the input, again helping the attacker to limit raising suspi-
cion. This is similar to assumptions made by prior work on
adversarial attacks on text models [33].
Defender model. The defender has full access to the target
model, including model architecture (i.e. network architec-
ture, weight, and bias values). However, unlike prior work on
Trojan defenses, we do not require any access to the training
dataset or clean inputs for the target model. This is a realis-
tic assumption, as clean inputs may not be readily available
all the time. The defender’s Trojan detection scheme is run
ofﬂine before the target model is deployed, i.e. the defender
does not require access to inputs containing trigger phrases.
Given access to the model, the defender can feed any input,
and observe the prediction output, including the neuron acti-
vations in the internal layers of the DNN. This means that the
defender knows the vocabulary space of the model, e.g., the
set of words, for a word-level text classiﬁcation model. The
defender has no knowledge of the trigger phrase(s) used by
the attacker and is unaware of the target label(s) chosen by
the attacker for misclassiﬁcation.
2.3 Related Work
Trojan attacks vs Adversarial sample attacks. Trojan at-
tacks are different from adversarial sample attacks, where the
attacker aims to ﬁnd small perturbations to the input that leads
to misclassiﬁcations. Adversarial perturbations are usually
derived by estimating the gradient of the target model or a sub-
stitute model, combined with optimization schemes [6,39,52].
Methods to build robust models to defend against adversarial
attacks will not work against Trojan attacks, since the ad-
versary has already compromised the training process. In an
adversarial attack, the model is “clean”, thus, ﬁnding an ad-
versarial input typically takes more effort [2,37,49]. However,
in Trojan attacks, the model itself is infected, and the attacker
knows with high conﬁdence that inputs with the trigger phrase
will cause misclassiﬁcation.
Existing work on Trojan attacks. Most work has focused
on Trojan attacks in the image domain. Gu et al. [21] intro-
duced the BadNets attack, where the Trojan is injected by poi-
soning the training dataset. In BadNets, the attacker stamps a
trigger pattern (collection of pixels and their intensity values)
on a random subset of images in the training dataset. These
modiﬁed samples are mislabeled to the desired target label
by the attacker, and the DNN is then trained to misclassify to
the target label, whenever the trigger pattern is present. Liu
et al. [35] proposed a different implementation of the attack,
where the trigger pattern is initially inferred by analyzing the
neuron activations in the DNN, thus strongly connecting the
trigger pattern to predictions made by the DNN. Both attacks
are highly effective against image classiﬁcation models. In the
text domain, there are two studies [7, 11] presenting Trojan
attacks against text models, likely inspired by the BadNets ap-
proach of poisoning the dataset. We follow a similar approach
in our attack methodology.
Limitations of existing defenses against Trojan attacks.
We are the ﬁrst to systematically explore a defense against
Trojan attacks in the text domain, and more generally in the
sequential domain (e.g., LSTMs). Limitations of existing de-
fenses are discussed below. Unless speciﬁed otherwise, all
existing methods are designed for the image domain.
Neural Cleanse [56]: Wang et al. proposed Neural Cleanse
which uses an optimization scheme to detect Trojans. Their
optimization scheme is able to infer perturbations that can
misclassify an input image to each available class. If the L1
norm of a perturbation stands out as an outlier, the model is
ﬂagged as containing a Trojan. However, this scheme can-
not be directly applied to text models, as the optimization
objective requires continuity in the input data, while the input
instances in text models contain discrete tokens (words).
SentiNet [10]: SentiNet uses DNN model interpretation
techniques to ﬁrst identify salient regions of an input image.
These salient patches are further veriﬁed to be either Trojan
triggers or benign patches, by applying them to clean inputs.
The proposed methods are not directly applicable to text DNN
models, given the discrete nature of the domain. Further, our
approach requires no clean inputs.
DeepInspect [8]: This recently proposed method is again
designed primarily for the image domain. Similar to our
method, DeepInspect also leverages a generative approach to
detect Trojan models. However, there are limitations. First,
adapting DeepInspect to the text domain is non-trivial, and
would require major changes to the generative approach given
the discrete space for text. This would require us to intro-
duce novel modiﬁcations to existing text generative models
in our setting (Section 4.2). Second, in the text domain we
observe that a generative approach can lead to false positives
(i.e. clean model ﬂagged as containing a Trojan) due to the
presence of universal adversarial samples that can be inferred
for many clean models (discussed in Section 6). Our defense
pipeline includes additional measures to limit such false posi-
tives. Third, DeepInspect requires a complex model inversion
process to recover a substitute training dataset to train the
generator. Our approach employs a much simpler synthetic
training data generation strategy (Section 4).
USENIX Association
30th USENIX Security Symposium    2257
Other approaches include Activation Clustering [7], Spec-
tral Signatures [53], and STRIP [17]. Details of these methods
are in Appendix A. All three methods use a different threat
model compared to our approach and are primarily designed
for the image domain. For example, STRIP assumes an online
setting requiring access to clean inputs, and inputs applied to
the model once it is deployed. We have no such requirements.
3 Attack Methodology
Basics. Our attack methodology is similar to the data poi-
soning strategy used by BadNets [21]. The target DNN could
be any text sequence classiﬁcation model, e.g., LSTM [26],
CNN [32] or Transformer-based model [54] for sentiment
classiﬁcation or hate speech detection. First, the attacker de-