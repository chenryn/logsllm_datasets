40 KB
264 KB
3.7 MB
32 MB
Table 4: The impact of blacklist size on the time to look up an
IP address. The index size grows linearly, but the lookup time
for an entry is very fast.
4.4 Performance
Figure 9 shows the growth of blacklists for the three tech-
niques: existing static threshold-based models, dynamic
thresholding and speculative aggregation. We ﬁnd that
the growth of blacklist size is highest for the ratio-based
techniques and lowest for the speculative-aggregation tech-
nique, as it combines many sources into BGP preﬁxes. In
order to see how blacklist size may impact the performance
of the system, we created tables with different blacklist
sizes in the database Postgresql (which is what we have
used in our system). Then we created an index on the IP
addresses and preﬁxes using GiST index in Postgresql. Ta-
ble 4 shows the time to look up an entry and the index size
for different sizes of the blacklist. We ﬁnd that the time to
look up an entry does not increase signiﬁcantly, and for the
month’s operation, the index size is easily manageable.
4.5
Impact of the Oracle on Accuracy
To validate the accuracy of SpamAssassin, we hand clas-
siﬁed several e-mail boxes and fed them to SpamAssassin.
As published in our previous study [23], SpamAssassin had
a false positive rate of less than 1% and a false negative rate
of around 5%. Obviously, the error in the oracle is likely
to impact the accuracy of our measurements. For example,
a false negative for the SpamAssassin oracle (i.e., a spam
classiﬁed as ham) may appear as a false positive for the
blacklist, if, unlike the oracle, the blacklist correctly identi-
ﬁed the e-mail as spam.
Given the inaccuracy of SpamAssassin, the accuracy
of false positives for the blacklist will be FPblacklist ±
FNspamassassin and the accuracy of false negatives for the
blacklist will be FNblacklist ± FPspamassassin. Therefore,
given the values of 20% (or greater) for the false negatives
of blacklists and 1% for the false positives that appear in this
paper, we arrive at or 1%± 5% for the blacklist false pos-
itives and > 20%± 1% for the false negatives. Clearly the
small false positive rate of the blacklists is likely to be lost
in the noise of the oracle. In order to overcome this prob-
lem, we hand classiﬁed the false negatives of the SpamAs-
sassin. Instead of manually examining all false negatives
of the SpamAssassin (potentially all legitimate e-mail), we
only hand classiﬁed sources that hit spamtraps (and hence
were sent to no legitimate user).
5 Related Work
Recently a number of research papers have looked at the
algorithms to generate blacklists. Ramachandran et al. [20]
proposed a new method to blacklist source IP addresses
based on their e-mail sending patterns. However, their ex-
periment is only based on e-mails received on the spamtraps
and not on e-mails received on the live network. As a result,
they only evaluate the false negatives of spamtrap received
e-mail and not the false positives of their approach. In our
t
s
i
l
k
c
a
b
l
e
h
t
n
i
s
e
i
r
t
n
e
f
o
r
e
b
m
u
N
threshold = 5
ratio = 1
ratio=1, leaf ratio=0.4
 3e+06
 2.5e+06
 2e+06
 1.5e+06
 1e+06
 500000
 0
02/07
02/14
02/21
02/28
03/07
03/14
Time
Figure 9: The growth of the blacklist size for the three approaches.
study, we generate blacklists based on spamtrap e-mails and
then apply them to the e-mail on the live network, so we
evaluate the false positives and false negatives for the e-mail
on the live network.
Xie et al. [29] have shown that a large number of IP ad-
dresses are dynamically assigned and e-mails from these IP
addresses are mostly spam, so they recommend adding dy-
namic IP ranges into blacklists to reduce the false negatives.
Zhang et al. [31] argued that a common blacklist may con-
tain entries that are never used in an organization. So they
proposed an approach to reduce the size of the blacklists
and possibly reduce the computational overhead in black-
list evaluation. However, their proposed approach only im-
proves the blacklist ”hit-rate” and not the overall false pos-
itive rate or the false negative rate of the blacklists.
Ramachandran and Feamster [19] collected spam by
monitoring e-mails sent to an unused domain and performed
a preliminary analysis of spammers. They observed that
the spamming sources are clustered within IP address space
and some of these sources are short-lived. Our approach
of speculative aggregation automatically identiﬁes bad IP
neighborhoods by considering the sources in the neighbor-
hoods that have hit the spamtraps and the sources that have
not yet hit the spamtraps.
A number of papers have questioned the effectiveness of
blacklists. Ramachandran et. al. [18] analyzed how quickly
Bobax infected hosts appeared in the SpamHaus blacklists.
They found that a large fraction of these hosts were not
found in the blacklist and demonstrated the delay in black-
listing. We also demonstrate the problem of delay in black-
listing and show that our proposed speculative aggregation
technique is able to address this problem.
E-mail servers can be easily overwhelmed when a sig-
niﬁcant amount of spam is received. Accordingly, there has
been increased interest in developing lightweight measures
for reducing the load on an e-mail server. Venkataraman
et al. [26] proposed coarse IP-based blacklists to reject e-
mails and to reduce server load. They monitored e-mails
on a mail server and used SpamAssassin score to identify
spam and ham. The spam and ham ratio for IP addresses
and IP clusters were computed and the history was used for
blocking IP addresses in case of server overload. It is im-
portant to note that they required an existing spam detector
in order to create their IP blacklists. We do not rely on a
pre-existing spam detector and instead use e-mails on our
deployed spamtraps and the e-mails on the production net-
work to generate blacklists. While their approach is a light-
weight technique abstracted from an existing detector, our
approach is to build blacklists using traditional spamtraps.
Therefore, our blacklists can be used for improving a spam
detector rather than just reducing the load on the server.
Xie et al. [30] used a spam detector to classify e-mails as
spam and ham and then proposed an automated way to gen-
erate spam signatures. However, their approach focuses on
using message content for developing signatures. Similarly,
Beverly et al. [8] have used TCP information from spam and
ham packet level data to develop TCP level spam features.
Kanich et al. [12] empirically evaluated the success rate and
the monetization from spam. Rajab et al. [17] analyzed the
botnet behavior across a number of dimensions but did not
develop any automated way of blacklisting them.
Most similar to this effort is the recent work of Hao et
al. [11]. The authors used a spam detector to separate e-
mails into ham and spam. By examining these e-mails, they
identiﬁed a number of network level features that can be
used to differentiate ham and spam. Hao et al. then used
machine-learning models on those network features to build
a spam classiﬁer. Similar to Venkataraman et al. [26], Hao
et al. relies on an existing spam detector to build the data
streams, which are used to feed the classiﬁer and periodi-
cally retrain it. Our efforts are similar to this work in that we
have we also identiﬁed features that differentiates spam and
ham [23]. Rather then examining numerous features and
combining them in a classiﬁer, we focus on a small hand-
ful of these features (e.g., remote BGP preﬁx clustering)
and explicitly explore the properties and tradeoffs of each.
Further, while the work of Hao et al. does result in a light-
weight spam ﬁlter that performs on par with existing ap-
proaches, the ﬁlter is offered as an alternative and does not
explore how existing blacklists fail and can be improved.
The key difference in these two systems, however, is the role
of local and global information in the classiﬁcation process.
Hao et al. is similar to existing blacklist deployment models
in that the classiﬁers are built from global sources of infor-
mation only and are not customized. We view this work
as complimentary to Hao et al. in that the local generation
and customization approach presented here could likewise
be applied to their classiﬁer generation to yield improved
accuracy.
6 Discussions
In this paper, we presented a detailed investigation of
blacklist generation techniques using 2.5 million e-mails
from a large academic network and 14 million e-mails from
a spamtrap deployment in 11 domains. We presented a de-
tailed analysis of ham and spam sources, based on our own
spamtrap deployment that helps to explain the limitations
of existing spam blacklist approaches. We then proposed
two improvements to the standard threshold-based black-
list approach. The ﬁrst one reduces false positives by com-
paring trafﬁc on the live network to the spamtrap hits for
blacklisting sources. The second takes network trafﬁc into
account to safely aggregate bad sources into bad neighbor-
hoods. The proposed techniques, when combined together,
improved the false negative rate by 4-5x for false positive
rates below 0.5% and 2x for false positive rates above 0.5%.
6.1 Ethical Considerations for Spam Blacklist
Evaluation
The increasing level of detailed access and subject in-
teractivity of Internet research experiments raise numerous
ethical issues for research [9]. Beneﬁcence refers to the pro-
cess by which a researcher seeks to do good or seeks to
maximize beneﬁts while minimizing harm. As mentioned
previously, the phenomenon of unsolicited bulk e-mail or
Spam is one that routinely impacts user productivity [21],
consumes resources [14], and serves as an infection vector
for malicious software [15]. The main beneﬁt of this work is
to examine techniques for reducing this burden. The great-
est risk to the subjects of the study is the loss of privacy.
In an effort to minimize this harm, no personally identiﬁ-
able information of the subjects is published herein. The
collected data was restricted to e-mail source and destina-
tion servers only, except in the following two cases: (i)
we hand classiﬁed e-mail contents of four subject’s inboxes
with their explicit user permission in order to determine the
accuracy of our oracle (ii) we hand classiﬁed the contents of
e-mails not marked by the oracle, but that were sent to our
spamtrap where no legitimate users preside. Because this
analysis was performed ofﬂine, no e-mails were modiﬁed
during the study.
6.2 Limitations
While effective at its goal of addressing the limitations
of blacklist generation, this work has several limitations and
opportunities for future work. First, the speculative aggre-
gation technique presented in this paper is somewhat pre-
emptive in nature. While our evaluation shows that the
proposed technique provides signiﬁcantly better trade-offs,
it may be unacceptable to block trafﬁc from hosts pre-
emptively. Second, like other reputation-based systems,
our blacklist generation system is also exposed to the at-
tacks that increase or decrease the reputation of sources.
While the dynamic threshold technique provides protection
against attacks to blacklist a mail server, it is still vulnerable
to attackers increasing the reputation of sources by sending
a large number of e-mails to a legitimate user. Currently,
our system only counts the total number of e-mails on the
live network and is vulnerable to such an attack. A system
that counts the number of unique users to which a source
sends mail may be resilient to such an attack and will be
explored in the future. Third, blacklist providers often indi-
cate that they are not responsible for the blocking email as
they only generate the blacklists, and it is the network ad-
ministrators who are blocking the e-mails. However, these
blacklists currently are generated centrally. The only option
a network administrator has is to accept or reject a given
blacklist. Our proposed deployment model requires either
publication of raw spamtrap data to subscribers or the pub-
lication of (aggregate) local network trafﬁc statistics to the
blacklist providers, each of which have obvious limitations
(or attacks against them). Finally, in our current implemen-
tation, we only extracted the ﬁrst “Received” header in the
email messages. In our dynamic threshold mechanism, we
could not blacklist sources if we did not blacklist the ﬁrst
source. In the future, we may like to add support for black-
listing of sources in received headers beyond the ﬁrst one.
7 Acknowledgements
We would like to thank the anonymous reviewers for
their comments and extend special thanks to Thorsten Holz,
our shepherd, for his efforts in signiﬁcantly improving this
paper. This work was supported in part by the Depart-
ment of Homeland Security (DHS) under contract num-
bers NBCHC080037, NBCHC060090, and FA8750-08-2-
0147, the National Science Foundation (NSF) under con-
tract numbers CNS 091639, CNS 08311174, CNS 0627445,
and CNS 0751116, and the Department of the Navy under
contract N000.14-09-1-1042.
References
[1] Not just another bogus list. http://njabl.org.
[2] Pyzor. http://pyzor.sourceforge.net/.
[3] Sorbs DNSBL. http://www.sorbs.net.
[4] SpamCop.net - Beware of cheap imitations. http://www.
spamcop.net/.
[5] The SpamHaus Project. http://www.spamhaus.org.
[6] Vipul’s razor. http://razor.sourceforge.net/.
[7] What is the SpamCop Blocking List (SCBL)? http://
spamcop.net/fom-serve/cache/297.html.
[8] R. Beverly and K. Sollins. Exploiting transport-level char-
acteristics of spam. In Proceedings of the Fifth Conference
on Email and Anti-Spam (CEAS), Aug. 2008.
[9] D. Dittrich, M. D. Bailey, and S. Dietrich. Towards com-
munity standards for ethical behavior in computer security
research. Technical Report 2009-01, Stevens Institute of
Technology, Hoboken, NJ, USA, April 2009.
[10] H. Drucker, V. Vapnik, and D. Wu. Support vector machines
for spam categorization. IEEE Transactions on Neural Net-
works, 10(5):1048–1054, 1999.
[11] S. Hao, N. A. Syed, N. Feamster, A. Gray,
and
S. Krasser. Detecting Spammers with SNARE: Spatio-
temporal Network-level Automatic Reputation Engine.
In
Usenix Security ’09, Montreal, Canada, August 2009.
[12] C. Kanich, C. Kreibich, K. Levchenko, B. Enright, G. M.
Voelker, V. Paxson, and S. Savage. Spamalytics: an empir-
ical analysis of spam marketing conversion.
In CCS ’08:
Proceedings of the 15th ACM conference on Computer and
communications security, pages 3–14, New York, NY, USA,
2008. ACM.
[13] J. Mason. Filtering Spam with SpamAssassin. SAGE-IE
meeting presentation, 2002.
[14] McAfee and I. International. The carbon footprint of email
http://newsroom.mcafee.com/images/
spam report.
10039/carbonfootprint2009.pdf, April 2009.
[15] T. Micro. Most abused infection vector. http://blog.
trendmicro.com/most-abused-infection-vector/,
December 2008.
[16] B. Nelson, M. Barreno, F. J. Chi, A. D. Joseph, B. I. P. Ru-
binstein, U. Saini, C. Sutton, J. D. Tygar, and K. Xia. Ex-
ploiting machine learning to subvert your spam ﬁlter.
In
First USENIX Workshop on Large-Scale Exploits and Emer-
gent Threats, April 2008.
[17] M. A. Rajab, J. Zarfoss, F. Monrose, and A. Terzis. A multi-
faceted approach to understanding the botnet phenomenon.
In IMC ’06: Proceedings of the 6th ACM SIGCOMM on
Internet measurement, pages 41–52, New York, NY, USA,
2006. ACM Press.
[18] A. Ramachandran, D. Dagon, and N. Feamster. Can DNS-
Based Blacklists Keep Up with Bots? . In Proceedings of
the Third Conference on Email and Anti-Spam (CEAS 2006),
July 2006.
[19] A. Ramachandran and N. Feamster. Understanding the
In SIGCOMM ’06:
network-level behavior of spammers.
Conference on Applications,
technologies, architectures,
and protocols for computer communications, pages 291–
302, New York, NY, USA, 2006. ACM Press.
[20] A. Ramachandran, N. Feamster, and S. Vempala. Filtering
spam with behavioral blacklisting. In CCS ’07: Proceedings
of the 14th ACM conference on Computer and communica-
tions security, pages 342–351, New York, NY, USA, 2007.
ACM.
[21] N. Research and KnowledgeStorm. Nucleus research: Spam
costing us businesses $712 per employee each year. http:
//nucleusresearch.com/news/press-releases/
nucleus-research-spam-costing-us-businesses-
712-per-employee-each-year/, April 2007.
[22] M. Sahami, S. Dumais, D. Heckerman, and E. Horvitz. A
bayesian approach to ﬁltering junk e-mail.
In AAAI-98
Workshop on Learning for Text Categorization, pages 55–
62, 1998.
[23] S. Sinha, M. Bailey, and F. Jahanian. Shades Of Grey: On
the effectiveness of reputation based blacklists.
In Inter-
national Conference on Malicious and Unwanted Software
(Malware 2008), October 2008.
[24] B. Stone.
Spam back to 94% of all e-mail, March
2009. http://bits.blogs.nytimes.com/2009/03/31/
spam-back-to-94-of-all-e-mail/.
[25] Unspam Technologies.
Project Honey Pot.
http://
projecthoneypot.org, 2008.
[26] S. Venkataraman, S. Sen, O. Spatscheck, P. Haffner, and
D. Song. Exploiting network structure for proactive spam
mitigation. In Proceedings of 16th USENIX Security Sym-
posium, pages 1–18, Berkeley, CA, USA, 2007. USENIX
Association.
[27] G. L. Wittel and S. F. Wu. On attacking statistical spam
ﬁlters. In Proceedings of the First Conference on Email and
Anti-Spam (CEAS), 2004. Available: http://www.ceas.
cc/papers-2004/170.pdf.
[28] R. Wojtczuk. libnids, June 2004.
[29] Y. Xie, F. Yu, K. Achan, E. Gillum, M. Goldszmidt, and
T. Wobber. How dynamic are IP addresses? In SIGCOMM
’07: Conference on Applications, technologies, architec-
tures, and protocols for computer communications, pages
301–312, New York, USA, 2007.
[30] Y. Xie, F. Yu, K. Achan, R. Panigrahy, G. Hulten, and I. Os-
ipkov. Spamming botnets: signatures and characteristics.
SIGCOMM Comput. Commun. Rev., 38(4):171–182, 2008.
[31] J. Zhang, P. Porras, and J. Ullrich. Highly predictive black-
listing. In 17th USENIX Security Symposium (USENIX Se-
curity ’08), July-August 2008.