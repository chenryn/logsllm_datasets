g alone allows us to design an identiﬁcation scheme by send-
ing the output of g2 as the identiﬁcation value. As we aim to
design an authentication scheme, and thus need to accom-
modate challenges without sacriﬁcing eﬃciency, we suggest
to use a universal hash function family: contrary to cryp-
tographic hash functions, universal hash functions allow for
very eﬃcient hardware implementations.
The PFP protocol, as depicted in Fig. 2, is built around
a pseudo-random number generator g : {0,1}n → {0,1}n+k
s∈{0,1}k of universal
hash functions. Let us denote by g1 the partial function
that maps g’s input to g’s n ﬁrst output bits and by g2 the
partial function that maps g’s input to g’s k last output bits
so that g(x) = (g1(x), g2(x)) for any input x of g. Every tag
has its internal state setup with a randomly chosen secret
seed σ0 = x representing its identity.
and a family ˘hs : {0,1}l → {0,1}m¯
Figure 2: Our proposal: the PFP scheme
The PFP protocol is deﬁned as follows. When a tag re-
ceives an authentication challenge a ∈ {0, 1}l from a reader,
it ﬁrst derives two values σi+1 = g1(σi) and s = g2(σi) from
the expansion of its internal state σi = gi
1(σ0) through g.
Then, the tag authenticates to the reader by replying c =
hs(a). The reader veriﬁes the answer of the tag by searching
its chains: for each tag T in the system, the reader fetches
the last known state σT
j and runs through the set of possible
j ))(a) with j + i ≤ ω − 1 and looks for c. If
values hg2(gi
a match is found for the tag T with last known state σT
j ,
the outcome of the protocol is b = 1 (success), the identiﬁed
tag is T , and the new last known state for T becomes σT
j+i;
otherwise the outcome is b = 0 (failure).
1(σT
On the back-end system side, the search is limited to
chains of length ω. We indeed consider adversaries who can
only disturb a limited (but large) number q of authentication
exchanges and assume that the maximum number of state
updates a tag can undergo during its lifetime (including the
up to q updates caused by an adversary) is upper-bounded
by ω > q. Since the back-end system manages for each tag a
chain of internal states of length ω, DoS attacks become out
of reach of such adversaries. The next sections show that
under these assumptions, the scheme is correct, secure, and
forward private. Since the security bounds are reused in the
correctness proof, we ﬁrst prove that the scheme is secure.
Accommodating time/memory trade-oﬀs. The above
protocol can be slightly adapted to beneﬁt from back-end
system eﬃciency improvements that have been discussed in
the previous section. Indeed, the challenge introduced for
the purpose of authentication a priori prevents the back-
end system from using the time/memory trade-oﬀ suggested
by Avoine and Oechslin, but this issue can be solved by
using a pseudo-random number generator g = (g1, g2, g3)
and by modifying the protocol to reply with the pair (c, I) =
(hg2(σi)(a), g3(σi)) instead of answering with c alone. This
way, the reader ﬁrst uses I to identify the tag in its database
and then authenticates it with c. We focus in the following
on the authentication protocol described above, i.e. where
g = (g1, g2), but the security proofs we give can easily be
adapted for the variation we just mentioned.
5.2 Security of PFP
Theorem 6. If {hs}s∈S is an -almost strongly universal
hash functions family, g a (T,g)-secure PRNG, and q≤ 1
2 ,
the PFP authentication protocol is (q, T (cid:48), s)-secure with
T (cid:48) = T − (ω + 1)(Tg + qTh) and s = ω(g+2−l+(1+2q)).
hs(a)cg2(gi1(x))gi1(x)=hsatagreaderg=(g1,g2)g1(gi1(x))s=g2(gi1(x))47`g2(x), g2(g1(x)), . . . , g2(gω−1
Proof. We must show that the success probability pA of
any (q, T (cid:48))-adversary against the security of PFP is upper-
bounded by s. First, note that we can assume that pA >
ω(2−l + (1 + 2q)) since otherwise pA ≤ s and the the-
orem holds (this minor preliminary remark is useful in the
sequel). We now show that A can be used to construct an ad-
versary B capable to distinguish a pseudo-random sequence
quence of kω bits in time T = T (cid:48) + q(ω + 1)Th with an
advantage greater than pA − ω(2−l + (1 + 2q)), and then
apply Theorem 1. The main diﬃculty lies in B consistently
simulating the view of A without knowing any secret from
the internal state of the tags or the readers.
(x))´ from a purely random se-
1
Figure 3: B simulates the view of A during the i-th
exchange of phase 1 in A’s attack; phase 1 allows up
to q such exchanges
We construct the distinguisher B as follows. Upon re-
ceiving a sequence (z0, z1, . . . , zω−1) of ω blocks of k bits,
B initializes two indices : cntt ← 0 used in the simulation
of the tag and cntr ← 0 used in the simulation of the reader.
Our distinguisher B then launches adversary A. Remember
that A works in two phases. As shown in Fig. 3, phase 1
allows up to q interactions between A and the tag and/or
i ∈ {0, 1}l,
the reader, where all or part of the values ai, a(cid:48)
i ∈ {0, 1}m, and bi ∈ {ok, nok} are exchanged. Phase 2
ci, c(cid:48)
in turn only consists in A exchanging values a ∈ {0, 1}l,
c ∈ {0, 1}m, and b ∈ {ok, nok} with the reader. The distin-
guisher B outputs ‘1’ if A authenticated itself successfully
to the reader during phase 2 and outputs ‘0’ otherwise.
During these two phases, B simulates A’s view as follows.
When A queries the tag by sending it a value a(cid:48)
i, B ﬁrst re-
trieves the current index cntt associated with the tag, then
recovers the k-bit block s = zcntt , computes c(cid:48)
i = hs(a(cid:48)
i),
and sends it back to A. Eventually, B increments the index:
cntt ← cntt + 1. When A requests an interaction with
the reader, B returns a random l-bit value a. If A replies
to the reader with a value c, B checks whether there exists
j ∈ {cntr, . . . , ω − 1} such that c = hzj (a). It answers ok
and sets cntr ← j + 1 in the positive case, and answers nok
without modifying cntr otherwise.
ceived by B is `g2(x), g2(g1(x)), . . . , g2(gω−1
(x))´, then B
One can easily check that when the input sequence re-
1
perfectly simulates the interactions between the tag, the
reader, and the adversary A. Thus, the probability that
B outputs ‘1’ is equal to pA in this case.
Let us now upper-bound the probability that B outputs ‘1’
while input with a truly random sequence (y0, y1, . . . , yω−1).
Let us denote by cnt1
t the values reached by the
r and cnt1
r, . . . , ω − 1}:
indices cntr and cntt at the end of phase 1 of A’s attack.
Then for every j in {cnt1
1. The tag simulations provided A with a tag answer hyj (a(cid:48)
i)
in at most one interaction of phase 1. Indeed, if cnt1
r  ω(2−l + (1 + 2q)) we get:
s
˛˛˛Pr
ˆB(Zr(y)) = 1˜˛˛˛
where Zg(s) =`g2(s), g2(g1(s)), . . . , g2(gω−1
´.
ˆB(Zg(s)) = 1˜ − Pr
Zr(y) =`y0, y1, . . . , yω−1
≥ pA − ω(2
y
1
−l + (1 + 2q)) ,
(s))´ and
Moreover, B runs in no more time than A plus the time
required to perform at most q computations of h for the
tag simulation and at most ωq computations of h for the
reader simulation so that the running time of B is upper-
bounded by T (cid:48) + q(ω + 1)Th. Using Theorem 1 we have
two last inequalities show that pA ≤ ω(g + 2−l + (1 + 2q))
which concludes the proof.
ˆB(Zr(y)) = 1˜˛˛ ≤ ωg. These
ˆB(Zg(s)) = 1˜ − Pry
˛˛Prs
Proposition 7. Theorem 6 also holds for a slightly ex-
tended security adversary who is allowed to interact during
phase 2 with the tag by sending it one or several times the
ﬁnal reader’s challenge a, with the restrictions that the total
number of tag queries remains upper-bounded by q and that
A cannot send as ﬁnal answer one of the answers of the tag
to the challenge a (to avoid an obvious relay attack).
Proof Sketch. Let bi denote the tag’s answers when chal-
lenged by the attacker A with a. As A cannot use bi as its
ﬁnal guess, for the corresponding states A’s ﬁnal guess will
be wrong, so A only wins if he can make a correct guess for
another state, which is exactly the setting of Theorem 6.
Proposition 8. Theorem 6 also holds (if s is replaced
by (cid:48) = N s) for an extended impersonation deﬁnition where
the adversary not only wins if he authenticates as the target
tag of phase 1 (as in Deﬁnition 2), but more generally if he
authenticates as one of the N tags in the system.
Proof Sketch. The probability for an attacker to authenti-
cate as a given tag is lower than the previous probability to
authenticate: since the internal states of the tags are inde-
pendent, A collects less information with the answers of the
reader by querying diﬀerent tags than by sticking to a given
one. The probability to be authenticated as a tag among
the N tags is thus upper-bounded by N s.
BaiaicicibisimulatedtagsimulatedreaderA12345cntrcntt485.3 Forward privacy of PFP
Theorem 9. Let g be a (T, g)-secure PRNG, let {hs}s∈S
be an -almost strongly universal hash functions family, and
let q  2ω(g + 2−l + (1 + 4q)) since other-
wise a is obviously upper-bounded by p and the theorem
holds (this minor preliminary remark is useful in the sequel).
We show in a ﬁrst step how such an adversary can be used to
construct a distinguisher between two specially crafted dis-
tributions D and D(cid:48), and in a second step show that these
two distributions cannot be distinguished with a good ad-
vantage. The two distributions we consider are deﬁned on
values of the form (x0, . . . , xq, y0, . . . , yγ, z) as follows:
1(sx)) and yi = g2(gi
– Distribution D: The number γ is uniformly randomly
chosen in(cid:74)0, 2q(cid:75), while (xi)i∈(cid:74)0,q(cid:75) and (yi)i∈(cid:74)0,γ(cid:75) are the
pseudo-random sequences constructed from two seeds sx
and sy uniformly randomly chosen from {0, 1}n through
xi = g2(gi
1(sy)). Eventually, z is
deﬁned by z = gγ+1
– Distribution D(cid:48): The number γ is uniformly randomly
chosen in(cid:74)0, 2q(cid:75), while (xi)i∈(cid:74)0,q(cid:75) and (yi)i∈(cid:74)0,γ(cid:75) are per-
fectly random sequences of k-bit values, and z is uni-
formly randomly chosen from {0, 1}n.
Distribution D is used to simulate the two tags Tx and Ty
used in the privacy game where the corresponding sequences
(xi)i∈(cid:74)1,q(cid:75) and (yi)i∈(cid:74)1,γ(cid:75) can be used to answer a reader’s
challenge when the tag is in its i-th internal state without
knowing this internal state. The tag that is to be selected
during phase 2 of the privacy game is the tag Ty related
to the sequence (yi). Finally, z is the internal state of Ty
that the privacy attacker recovers when tampering with Ty.
Distribution D(cid:48) is the corresponding uniform distribution.
We now construct a distinguisher B between D and D(cid:48)
that relies on A. The main issue is to simulate the tampering
step during phase 2 of A’s attack: B is unable to disclose
the internal state of the tag Ty when the tampering happens
just after the i-th query where i < γ. However, since the
(sy).
1
value of γ is uniformly and randomly drawn from (cid:74)0, 2q(cid:75)
independently from the values (yi), the number γ can be
seen as a guess for the total number of queries that will be
made by adversary A to the tag Ty. The distinguisher B
setups itself by choosing uniformly and randomly a bit β,
setting Tiβ ← Ty and Tiβ⊕1 ← Tx, and then initializing
three indices cntt,0 ← 0, cntt,1 ← 0, and cntr ← 0.
During phase 1, B simulates the environment of A as fol-