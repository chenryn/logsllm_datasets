title:Privacy-Preserving Aggregation of Time-Series Data
author:Elaine Shi and
T.-H. Hubert Chan and
Eleanor Gilbert Rieffel and
Richard Chow and
Dawn Song
Privacy-Preserving Aggregation of Time-Series Data
Elaine Shi
PARC/UC Berkeley
PI:EMAIL
T-H. Hubert Chan
The University of Hong Kong
PI:EMAIL
Eleanor Rieffel
FxPal
PI:EMAIL
Richard Chow
PARC
PI:EMAIL
Dawn Song
UC Berkeley
PI:EMAIL
Abstract
We consider how an untrusted data aggregator can
learn desired statistics over multiple participants’ data,
without compromising each individual’s privacy. We
propose a construction that allows a group of partici-
pants to periodically upload encrypted values to a data
aggregator, such that the aggregator is able to compute
the sum of all participants’ values in every time period,
but is unable to learn anything else. We achieve strong
privacy guarantees using two main techniques. First, we
show how to utilize applied cryptographic techniques to
allow the aggregator to decrypt the sum from multiple
ciphertexts encrypted under different user keys. Second,
we describe a distributed data randomization procedure
that guarantees the differential privacy of the outcome
statistic, even when a subset of participants might be
compromised.
1
Introduction
In many practical applications, a data aggregator
wishes to mine data coming from multiple organizations
or individuals, to study patterns or statistics over a pop-
ulation. An important challenge in these applications is
how to protect the privacy of the participants, especially
when the data aggregator is untrusted.
This paper describes novel Private Stream Aggrega-
tion (PSA) algorithms which allow users to upload a
stream of encrypted data to an untrusted aggregator, and
allow the aggregator to decrypt (approximate) aggregate
statistics for each time interval with an appropriate ca-
pability. We guarantee a strong notion of privacy. First,
our aggregation scheme is aggregator oblivious, mean-
ing that the aggregator is unable to learn any unintended
information other than what it can deduce from its aux-
iliary knowledge and the desired statistics. Second, we
guarantee distributed differential privacy for each in-
dividual participant, in the sense that the statistic re-
vealed to the aggregator will not be swayed too much by
whether or not a speciﬁc individual participates. There-
fore, users may safely contribute their encrypted data, as
presence in the system will not lead to increased risk
of privacy breach. Our privacy guarantees hold even
when the aggregator has arbitrary auxiliary information
about an individual’s inputs (but has not compromised
her secret key). Such auxiliary information may be ob-
tained from publicly available datasets, personal knowl-
edge about an individual participant, or through collu-
sion with a small subset of corrupted participants.
The proposed privacy mechanisms
represent a
promising approach to ensuring user privacy in numer-
ous application, including cloud services, medical pri-
vacy, sensor network aggregation, and smart metering.
1.1 Contributions
Formulation of a privacy model. One important con-
tribution we make is the formulation of a notion of pri-
vacy. A good way to understand our contributions is
to compare our notion of privacy with differential pri-
vacy [5]. The differential privacy literature assumes the
presence of a trusted data aggregator who wishes to pub-
lish statistics about a population. The trusted data aggre-
gator is entitled to see all participants’ data in the clear.
Our privacy model is stronger in the sense that we do
not trust the data aggregator. We ensure that the data
aggregator is able to learn only the intended statistics
and no additional information. Furthermore, the statis-
tics revealed to the data aggregator satisﬁes differential
privacy guarantees. Our scheme protects each individual
participant’s privacy even when the aggregator has ar-
bitrary auxiliary information about an individual’s data
(but has not compromised her secret key), or colludes
with a subset of corrupted participants.
Computing the sum statistic for time-series data.
We propose novel constructions that allow an untrusted
data aggregator to compute the sum statistic for time-
series data.
Imagine a data aggregator who wishes to
keep track of the total sales revenue of n companies ev-
ery week. Our scheme allows each individual company
to upload a noisy encryption of their revenue every week
to the data aggregator. With an appropriate capability,
the data aggregator is able to decrypt the noisy sum of
all companys’ revenues, but is unable to infer additional
information about an individual company.
Our effort is a ﬁrst step towards this new notion of
privacy. In Section 8, we propose several open problems
that remain to be addressed for this new direction. We
hope that our effort will inspire future research in this
area to address these interesting challenges.
1.2 Applications
Sensor network aggregation. Sensor networks are
being widely-deployed to monitor the safety of build-
ings, measure trafﬁc ﬂows, or track environmental pollu-
tants. In a typical setting, deployed sensor nodes period-
ically send their readings to a base station, which mines
the data for some pattern or statistic. In many scenarios,
the readings from each individual sensor may be privacy
sensitive, especially if the sensors are deployed across
multiple organizations. Our construction may provide a
promising approach to address privacy issues arising in
sensor network aggregation.
Smart metering. Another example is the advent of the
electrical “smart grid” and “smart metering” [4]. Smart
meters read electrical usage at a much ﬁner granularity
than traditional meters; smart meters might read usage
every 15 minutes as opposed to once a month. See [15]
for a sampling of what might be gleaned from your ﬁne-
grained electrical usage. For instance, one can deduce
the number of individuals in the household and their
sleep/work habits, as well as their use of common house-
hold appliances. These privacy concerns could be much
reduced if household usage information were only re-
leased in the aggregate. Membership in the aggregation
groups would be ﬂexible and open, and these aggregate
statistics would still be enough for the smart grid opera-
tors to do much of their monitoring and price optimiza-
tion.
Public health and clinical research. Medical re-
search beneﬁts greatly from medical data, but privacy
concerns restrict the extent to which this data is col-
lected and disseminated. Molina et al. [14] use mul-
tiparty computation by caregivers to answer researcher
aggregation queries, especially for medical telemetry
data. PSA algorithms would enable researchers to ob-
tain those statistics, and only those statistics, from data
uploaded continually by the caregivers or the telemetry
devices, without need for further interaction.
Population monitoring and sensing. There are many
examples of population polling, sensing, and monitor-
ing spurring privacy concerns. As one example, Rief-
fel et al. [17] use data from cameras, wiﬁ, and com-
puter activity to estimate a user’s availability, and help
co-workers identify the best means for communication
with that user. A lot of information about a user’s work
habits can be deduced from her communication avail-
ability. For this reason, users were reluctant to have any
long-term data stored for fear that it would be misused
by their managers. As they used the system, however,
users became interested in sharing information with se-
lected individuals, and were open to allowing managers
to learn statistical data across the group. Rieffel et al.’s
work addressed the oblivious aggregation problem, but
was susceptible to collusion.
Cloud services. As cloud computing gains popularity,
individuals and organizations will store an increasing
amount of data on third-party cloud services. Cloud ser-
vice providers wish to compute useful statistics over this
data, to realize various social and economic goals. Un-
fortunately, companies cite concerns about the security
and privacy of their data as a top reason for not making
more use of cloud services. Our constructions represent
a promising approach in cloud applications, especially
when the cloud services wishes to track some aggregate
statistics from multiple users over time.
2 Related Work
To understand our contributions, it helps to know the
relationship of this paper with well-known privacy tech-
niques such as differential privacy, homomorphic en-
cryption and secure multi-party computation.
Differential privacy. The differential privacy notion
was ﬁrst formulated by Dwork et al. [5, 7]. Differen-
tial privacy ensures that a user is not at increased risk
of privacy when she participants in a certain statistical
database. Previous work on differential privacy consid-
ers a trusted data aggregator who has access to all users’
data. The trusted aggregator typically adds appropri-
ate noise to some statistics before releasing them.
In
comparison, our work provides stronger privacy guar-
antees as we ensure the privacy of individual partici-
pants even against the data aggregator itself. This is
valuable in many real-world settings (e.g., cloud appli-
cations) where users may not entrust the data aggregator
or storage server with their data. Dwork et al. [6] have
also considered distributed randomness among partici-
pants in order to achieve differential privacy. However,
their scheme involves interactions among all users.
Homomorphic encryption. Most previous work on
homomorphic encryption considers homomorphic oper-
ations on ciphertexts encrypted under the same key [2,
8]. These schemes do not directly apply in our case,
since if participants encrypted their data under the ag-
gregator’s public key, the aggregator would not only be
able to decrypt the aggregate statistics, but also each in-
dividual’s values. By contrast, our cryptographic con-
struction allows additive homomorphic operations over
ciphertexts encrypted under different users’ secret keys.
Castelluccia et al. [3] designed a symmetric-key ho-
momorphic encryption scheme that allows an aggregator
to efﬁciently decrypt the mean and variance of encrypted
sensor measurements. However, they also assume a
trusted aggregator who is allowed to decrypt each in-
dividual sensor’s values. Yang et al. [19] designed an
encryption scheme that allows an aggregator to compute
the sum over encrypted data from multiple participants.
As pointed out by Magkos et al. [11], their construc-
tion only supports a single time step, and an expensive
re-keying operation is required to support multiple time
steps.
[10]
Secure multi-party computation. Secure multi-party
computation (SMC)
is a well-known crypto-
graphic technique allowing n parties with inputs x =
(x1, x2, . . . xn) respectively to privately compute func-
tions f1(x), f2(x), . . . , fn(x). At the end of the proto-
col, party i learns the value of fi(x) but nothing more.
In a sense, SMC is orthogonal and complementary to
differential privacy techniques. SMC does not address
potential privacy leaks through harmful inferences from
the outcomes of the computation, but one can potentially
build differential privacy techniques into a multi-party
protocol to address such concerns.
Most SMC constructions are interactive. Therefore,
directly employing SMC in our setting would require
participants to interact with each other whenever an ag-
gregate statistic needs to be computed. Such a multi-way
interaction model may not be desirable in practical set-
tings, especially in a client-server computation model as
often seen in cloud computing applications.
Most closely related work. To the best of our knowl-
edge, Rastogi et al. [16] and Rieffel et al. [17] were the
ﬁrst ones to consider the problem of privately aggregat-
ing sums over multiple time periods.
Rastogi and Nath [16] also consider periodic aggre-
gation of the sum statistic in the presence of an untrusted
aggregator. Their work differs from our work in several
aspects. (1) One of our contributions is to present a for-
mal security deﬁnition. Rastogi et al. prove that the
aggregator cannot compute linear combinations of the
users’ values other than the sum – and this implicit secu-
rity deﬁntion they adopted is incomplete in some sense.
Note that although we deﬁne security speciﬁcally for the
sum statistic in this paper, our security deﬁnitions can
potentially be generalized for the aggregation of general
statistics. (2) The construction by Rastogi et al. requires
that the aggregator engage in an extra round of interac-
tion with the participants to decrypt the sum for every
time interval. In our scheme, the aggregator need not in-
teract with the participants to decrypt the sum. (3) Ras-
togi et al. also consider how the users can jointly con-
tribute randomness to achieve differential privacy. In-
terestingly, they propose a noise generation algorithm
In contrast to Rastogi et al., our
different from ours.
scheme and privacy analysis explicitly address the issue
of rounding when the underlying algebraic structure of
the encryption scheme supports only ﬁnite discrete val-
ues.
Rieffel et al. consider an application scenario where
a manager would like to periodically decrypt a summary
statistic across a set of users, while not being able to
decrypt individual values [17]. Their construction does
not provide distributed differential privacy guarantees,
and is not fully resistant against collusions, i.e., users
and the manager may collude to decrypt a victim user’s
value. They raise the question whether it is possible to
design a scheme fully resistant against collusion. This
paper gives explicit formalizations of the privacy model
implicit in their work, and provides a positive answer to
the question they raised.
3 Problem Deﬁnition and Overview
Suppose we have one data aggregator and n partici-
pants. For notational convenience, we number the par-
ticipants 1, . . . , n, and we number the data aggregator 0.
Let [n] := {1, 2, . . . , n}. In every time period t ∈ N,
each participant i ∈ [n] has a value xi,t ∈ D from a cer-
tain domain D. When the context is clear, we omit the
subscript t and write xi instead. Let x = (x1, . . . , xn) ∈
Dn denote the vector of values from all participants in
some time period. The aggregator would like to com-
pute some aggregate statistics represented by the func-
tion f : Dn → O. The function f(x) produces some
value from the some range O, representing the desired
statistics.
To achieve strong privacy guarantees when the ag-
gregator may have arbitrary auxiliary information about
users’ inputs, each participant generates independent
random noise from some sample space Ω, represented
by r := (r1, . . . , rn) ∈ Ωn. Let χ : D × Ω → D denote
some randomization function allowing each participant
to compute a noisy version of her data(cid:98)xi := χ(xi, ri)
From the encrypted values of (cid:98)x := ((cid:98)x1,(cid:98)x2, . . . ,(cid:98)xn),
the aggregator computes a noisy statistic f((cid:98)x), which
before encrypting and uploading it to the aggregator.
should be close to the desired statistic f(x). Throughout
the remainder of the paper, we use hatted variables to de-
note the randomized versions of a participants’ data (as-
sociated with some random r and randomization func-
tion χ), and we use non-hatted versions to denote the
original data.
Remark 1 (More general setting). In general, each par-
ticipant i can generate noise ri according to her own
distribution. Moreover, each pariticipant i can apply a
different randomization function χi(xi, ri) to her data
xi.
In an even more general setting, each participant
may encrypt her data xi and her randomness ri sepa-
rately before sending to the aggregator, who computes a
randomized aggregate function (cid:98)f : Dn × Ωn → O on
when (cid:98)f(x, r) = f((cid:98)x). Furthermore, we assume that
For simplicity, this paper considers the special case
the encrypted inputs.
each participant applies the same randomization χ func-
tion before encrypting her data.
Our goal is to design a privacy mechanism such that
for every time period, the aggregator is able to learn
some aggregate statistic f((cid:98)x), but not each individual’s
value even when it has arbitrary auxiliary information.
We call a scheme that meets the above requirements a
Private Stream Aggregation (PSA) mechanism. More
formally, a Private Stream Aggregation scheme consists
of the following algorithms.
Setup(1λ): Takes in a security parameter λ, and out-
puts public parameters param, a private key ski for
each participant, as well as a aggregator capabil-
ity sk0 needed for decryption of aggregate statistics
in each time period. Each participant i obtains the
private key ski, and the data aggregator obtains the
capability sk0.
NoisyEnc(param, ski, t, x, r): During time step t, each
participant calls the NoisyEnc algorithm to en-
code its data x with noise r. The result is a
noisy encryption of x randomized with the noise
r. Without risk of ambiguity, we sometimes write
NoisyEnc(param, ski, t,(cid:98)x) where (cid:98)x := χ(x, r) is
the noisy version of the participant’s data, and χ is
some underlying randomization function.
AggrDec(param, sk0, t, c1, c2, . . . , cn) The decryption
algorithm takes in the public parameters param, a
capability sk0, and ciphertexts c1, c2, . . . , c2 for the
same time period t. For each i ∈ [n], let ci =
NoisyEnc(ski, t,(cid:98)xi), where each (cid:98)xi := χ(xi, ri).
Let x := (x1, . . . , xn) and(cid:98)x := ((cid:98)x1, . . . ,(cid:98)xn). The
decryption algorithm outputs f((cid:98)x) which is a noisy
version of the targeted statistics f(x).
3.1 The Case for Summation: Overview of Our
Solution
(cid:80)
In this paper, we mainly consider a simple but com-