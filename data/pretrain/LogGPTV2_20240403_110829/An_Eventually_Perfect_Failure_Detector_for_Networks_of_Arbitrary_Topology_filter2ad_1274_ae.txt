### Length 1: Recall and Algorithm 2

#### Overview
Recall that a pair \((q, \text{ttl})\) is added to the bag of process \(p\) if \(\text{TTL}_p[q] - 1 > 0\). When Algorithm 2 starts, process \(p\) sends a heartbeat (HB) message to every neighbor in its bag, with the pair \((p, \text{localTTL})\) where \(\text{localTTL} = 2\). This ensures that neighbors forward the information about \(p\) to their neighbors. As \(p\) discovers new processes in the network, it receives pairs from processes it does not know, and \(\text{localTTL}\) grows dynamically. This dynamic growth of \(\text{localTTL}\) allows for more efficient discovery and communication without fixing the TTL to \(n-1\). The data structures used for storing timeouts, clock values, etc., must also be dynamic. In Algorithm 2, we use array notation for convenience, but in an actual implementation, a more efficient (dynamic) data structure would be used.

#### Discovery and Message Handling
When a HB message from a neighbor arrives, new processes may be discovered. The local variable `known` stores the processes that have been discovered and is initialized with the set of neighbors of \(p\). When \(p\) receives a HB message from a neighbor \(q\) containing a bag, \(p\) iterates through the pairs \((r, m)\) in the bag. If \(r\) is a new process (i.e., \(r\) is not in `known`), \(p\) adds \(r\) to its set of known processes and initializes a new entry in all relevant variables as described in Algorithm 1.

#### Algorithm 2: Code for Process \(p\)

**Constants:**
1. `neighbors`
2. `T`

**Variables:**
3. `known = neighbors`  // Set of processes known by \(p\), initially only neighbors.
4. `localTTL = 2`  // TTL value of \(p\).
5. `HB_bag = {(p, localTTL)}`  // Bag containing the initial HB message.

**HB Module:**
5. Every \(T\) units of time:
6. Begin:
7.   Send HB message to each neighbor in `known`.
8. End

**Upon Receiving \(\text{HB_bag}\) from a Neighbor \(q\):**
10. Begin:
11.   For each \((r, m) \in \text{bag}\) such that \(r \notin \text{neighbors} \setminus \{q\}\):
12.     If \(r \notin \text{known}\):
13.       Add \(r\) to `known`.
14.       Add a new entry in `lastHB`, `timeout`, and `suspect` and initialize.
15.       Increment `localTTL` by 1.  // The longest path may grow with the discovery of a new process.
16.     Else:
17.       // No action needed; \(r\) is already known.
18.   End for
19. End

**Include Code from Algorithm 1:**
20. Include code of Algorithm 1 from line 24 to 35.

### B. Correctness Proof

To prove the correctness of Algorithm 2, we need to show that the implementation satisfies strong completeness and eventual strong accuracy for partitionable networks with unknown membership. Here is a sketch of the proof:

The proof is similar to the one given in Section III-B. The key points are:
1. The `localTTL` of every process \(p\) is bounded by \(n + 1\) because \(p\) can receive a pair labeled with every process in the network.
2. `localTTL` is incremented only when a new process is discovered (line 16).
3. We need to show that for any two correct processes \(p\) and \(q\) connected by a correct path in the final graph, \(q\) eventually knows \(p\) and \(q\) eventually does not suspect \(p\).

For proving that all pairs of a crashed process \(p\) eventually fade out from the network, the proof is similar to Lemma 9, assuming this is true only for processes that know \(p\). Similarly, for Lemma 11, only processes in a different partition than \(p\) that know \(p\) stop sending messages labeled with \(p\). Processes that do not know \(p\) do not send pairs labeled with \(p\).

### V. Conclusions

In this work, we presented an algorithm implementing an eventually perfect failure detector in an arbitrary network connected by ADD channels using time-to-live (TTL) values, with messages of \(O(n \log n)\) size. Initially, the number of processes in the network is known, and we then extend the algorithm to work when each process knows only its neighbors. A process dynamically adapts its timeouts and failure information as it learns of new processes in the network.

To the best of our knowledge, this is the first time that TTL values, well-known in networking literature, are used for failure detectors. We make a first step in showing that this technique leads to flexible failure detector implementations with small message sizes. Further work is needed to study the performance of our implementation and adapt it to more dynamic network scenarios, including partially synchronous models and situations where reliable link protocol implementations are impossible.

### Acknowledgment

The authors thank the anonymous reviewers for their comments and funding from UNAM-PAPIIT IN109917.

### References

[References listed here, formatted consistently and with proper citations.]