call socket which is irrelevant to the bug. This change would
inevitably involve the resource that cannot lead to the bug’s
triggering and guide the fuzzer to enter a large code space.
Take the case shown in Table Ib as the second example.
Similar to the example above, the table shows part of a PoC
program triggering a different kernel bug [12]. In the mutation
stage, Syzkaller varies the variable @max=0xffffffffffffffff
because the template indicates that the legit value range for
this variable is [INT_MIN, INT_MAX]. However, for this speciﬁc
kernel bug, which triggers an integer overﬂow in the kernel,
the condition of triggering this bug is @max=-1 or in other words
@max=0xffffffffffffffff. As a result, this argument’s random
mutation is futile, signiﬁcantly inﬂuencing the triggering of
the bug in different contexts.
To resolve the problems above, we improve our fuzzing
approach by optimizing its mutation mechanism. More specif-
ically, we group the system call speciﬁcation templates based
on the resource type the corresponding system calls reply
upon (e.g., categorizing system calls pertaining to the network
socket and device ﬁle separately). Within each group, we
then divide the enclosed system calls into two subgroups.
One is responsible for resource creation, and the other is for
their usage. With this grouping result, when mutating a seed
program, our fuzzing component either replaces system calls
with the ones in the same group or inserts system calls that
associate with the resource shown in the seed program. We
will release our newly grouped templates and the source code
of this project after the submission is accepted.
In addition to grouping templates based on resource, our
mutation mechanism also preserves the values for the argu-
ments seen in the original PoC program if the types of these
arguments do not fall into the following four categories –
constant, pointer referencing a memory region, checksum, and
resource (e.g., a ﬁle descriptor for an opened ﬁle or an estab-
lished socket). For arguments in constant, they usually indicate
the protocol under fuzzing testing (e.g., AF_INET and AF_INET6
(cid:44)→ in the system call socket() indicating the establishment
of the IPv4 and IPv6 socket, respectively). In the fuzzing
test, we need to alter these arguments to switch protocols and
thus vary the contexts under which the bug could be triggered
again with different error behaviors. For arguments in pointer
types and belonging to resources, when the kernel fuzzing
changes the context or path toward the buggy kernel code, the
original PoC program’s addresses could be illegal. Retaining
these addresses could incur early termination of the fuzzing
program. Regarding the checksum, if the calculation source’s
value varies in the mutation process, the checksum should
be updated accordingly. Preserving the same checksum value
could also result in the fuzzing program’s early termination at
the data validation phase.
V. IMPLEMENTATION
Based on LLVM infrastructure and the kernel fuzzing tool
Syzkaller, we implement our idea as a tool and name it
after GREBE. Below, we describe some critical details of
our implementation. The source code of GREBE is available
at [13].
Critical structure identiﬁcation. The input of our tool is
LLVM IR and a single bug report. In our implementation, we
employ the approach in previous works [14], [15] to generate
the bitcode ﬁles. Brieﬂy, we patch the LLVM compiler to
dump bitcodes before invoking any compiler optimization
passes. In this way, we can prevent compiler optimization from
inﬂuencing the accuracy of our analysis. Recall that we extract
the call trace from the bug report. The call trace indicates the
functions that have been called but not yet returned when the
kernel is experiencing errors. In this extracted call trace, the
function that has been called last could be the one instrumented
by the compiler. It does not
indicate the buggy function
contributing to the error. As such, we neglect these functions
in the call trace and start our analysis from the statement that
activates the debugging feature.
When using the backward taint analysis to identify criti-
cal structures, GREBE uses three instructions to extract the
structures’ type information. The ﬁrst instruction is BitCast
(cid:44)→ in which the types before and after casting are speciﬁed.
GREBE records the types extracted from this instruction as
critical structures. The second instruction is Getelementor that
contains a pointer referencing a kernel object and the object’s
corresponding type information. Through the analysis of this
instruction, we can quickly obtain critical structures. The third
instruction is CallInst. We infer the type information from the
callee’s prototype and record the structural type as critical
structures. As is mentioned earlier, we treat system calls’
entries, interrupt handlers, and workqueue processings as our
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:04 UTC from IEEE Xplore.  Restrictions apply. 
2085
taint sink. In our work, we manually annotate all these sinks
based on their naming patterns.
Critical structure ranking. As is described in Section IV-B,
when constructing the structure graph for critical structure
ranking, we consider typecasting. In our implementation, if
the cast variable is the return value of a callee function,
we investigate the callee from the return statement and then
associate the destination type with the structure ﬁeld. Again
take the case shown in Listing 5 as an example. The cast
variable skb->data is the return value of the callee function
__skb_push. By analyzing the callee function, we associate
struct frag_hdr with struct sk_buff.
Recall that we also rank structures based on their page-rank
scores and then use a page-rank score threshold to ﬁlter out
those popular ones. In this work, we choose this threshold
by using a standard univariate outlier detection method [16].
This approach computes the mean and standard deviation of
the page rank scores and then calculates the Z-score for each
structure further. Following the outlier detection method, we
use 3.5 more standard deviations as the threshold. Since most
kernel structures are less popular, having a signiﬁcantly low
z-score, this threshold could well distinguish popular kernel
structures from the others.
Kernel fuzzing. As is described in Section IV-C, we in-
strument the kernel to collect the usage of critical objects
at runtime. Since the support of Clang has been introduced
recently, which may not support all versions of the Linux
kernel, we perform instrumentation by using a GCC plugin
instead of a Clang pass. While performing a fuzzing program
mutation, we follow the design of MoonShine-enhanced [17]
Syzkaller, randomly mutating 33% system calls and replacing
them with others we have manually grouped.
When implementing the optimization mechanism that reuses
the arguments from the original PoC, for each system call in
the PoC program, we ﬁrst ﬁnd its speciﬁcation in Syzkaller
and analyze the deﬁnition of its structural arguments (i.e.,
StructType and UnionType). Then, we recursively examine the
structural arguments until no more new deﬁnitions can be
found. Inside each structural deﬁnition, we ignore ConstType
(cid:44)→ , VmaType, ResourceType, and CsumType because they represent
constant, pointer, resource description, and checksum respec-
tively. As we discussed in Section IV-C, they are not likely to
help explore new paths to the buggy code.
VI. EVALUATION
In this section, we ﬁrst quantify GREBE’s effectiveness and
efﬁciency and compare it with a code-coverage-based fuzzing
method. Then, we demonstrate and discuss how well GREBE
could unveil exploitation potential for real-world Linux kernel
bugs.
A. Experiment Setup & Design
Syzbot is a bug reporting platform that well archives the
kernel bugs identiﬁed by Syzkaller. To evaluate our tool –
GREBE, we select kernel bugs and their reports from the
platform as our test cases. While selecting these bugs, we
follow two different strategies.
Our ﬁrst strategy is a purely random selection process that
follows two criteria. First, the bug report has to attach a PoC
program so that we can reproduce the error speciﬁed in the
report. Second, the reported kernel error cannot associate with
Kernel Memory Sanitizer (KMSAN) because KMSAN is still
under development and has not yet been merged into the Linux
kernel mainline. By following these two criteria, we construct
a test corpus containing 50 Linux kernel bugs.
Our second bug selection strategy is a process dedicated to
different kernel versions. To be speciﬁc, we select bugs from 5
different Linux kernel versions (5.6 5.10)1. From each kernel
version, we choose two recently-reported reproducible kernel
bugs as our test cases. In this way, we construct another test
corpus with 10 Linux kernel bugs. Combining with the kernel
bugs in the ﬁrst corpus, we obtain a dataset with 60 unique
kernel bugs. To the best of our knowledge, our dataset is the
largest used in the exploitability research.
For each bug in our dataset, we built the corresponding
kernel in four QEMU virtual machines (VMs) by following the
description of their bug reports. For the ﬁrst two VMs, we ran
our tool – GREBE and Syzkaller. For the remaining two VMs,
we ran GREBE without enabling its mutation optimization
and Syzkaller with our mutation optimization (i.e., Syzkaller’s
variant). With this setup, we can evaluate GREBE’s effective-
ness and efﬁciency in different settings. Besides, we can com-
pare it with the code-coverage-based kernel fuzzing method
and its variant (i.e., Syzkaller with our mutation optimization).
It should be noted that we use Syzkaller as our baseline
approach for evaluation because it is one of open-sourced,
code-coverage-based kernel fuzzing tools but mostly because it
can test nearly all kinds of kernel components2. It should also
be noted that we extend Syzkaller with our proposed mutation
optimization for the following reason. GREBE is an extension
of Syzkaller. It combines both the object-driven component
and mutation optimization. With our mutation optimization
integrated into Syzkaller alone, we could examine whether
mutation optimization could become a sole driving force to
enable multiple error behavior exploration.
Given a kernel bug of our selection, its report, and a kernel
fuzzing tool under our evaluation, we include the PoC program
enclosed in the report into the initial seed set and deploy our
VMs on bare-metal AWS servers. Each of the servers has two-
socket Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz
(48 cores in total) and 192 GB RAM, running Ubuntu 18.04
LTS. For each VM, we conﬁgured it with two virtual CPU
cores and 2GB RAM. While performing kernel fuzzing, we set
each of the fuzzers to run for 7 days. To utilize the computation
1At the time of our experiment, 5.10 is the latest long-term support Linux
kernel version.
2The State-of-the-art kernel fuzzing tools – HFL [18], SemFuzz [19] have
not yet been publicly released. DIFUZE [20], KRACE [21], and Razzer [22]
etc. are designed for fuzzing speciﬁc bug types or kernel modules. Previous
research [18] shows Syzkaller has better performance than KAFL [5] and
Trinity [6] in terms of code coverage. Therefore, we choose MoonShine-
enhanced Syzkaller as our baseline.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:04 UTC from IEEE Xplore.  Restrictions apply. 
2086
resource of the AWS server efﬁciently, we assign only 30 VMs
for each server. In total, it takes us two months to gather the
experiment results shown in this paper.
After 7 days of fuzz testing against various versions of
the Linux kernels by using four different fuzzers, we formed
a 6-member team under the guidance of an IRB approval
(STUDY00008566). Among the 6 members, 2 are experienced
security analysts regularly developing kernel exploits in the se-
curity industry. The other 4 members are academic researchers
actively contributing to the Linux community and frequently
invited to give talks at the Linux Security Submit or other
Linux-related conferences. In our evaluation, we asked this
professional team to collect the fuzzing results (i.e., reports)
from all VMs, group the reports based on their title uniqueness,
and eventually preserve only the kernel reports truly tied to
the 60 bugs of our selection. Note that a kernel fuzzer might
trigger other kernel bugs and thus demonstrate errors. Since
there have not yet been highly accurate crash triaging tools, the
professional team inspects each of the kernel errors manually
and preserves only the errors associated with our selected
bugs. The procedure of manually triaging the kernel errors
is described in Appendix B.
In addition to the manual effort above, we also asked
our kernel professional
team to thoroughly and manually
inspect whether there are any other missing paths or contexts
that could trigger the kernel bugs and thus exhibit different
error behaviors. In this way, we can evaluate GREBE’s false
negatives or, in other words, understand how complete GREBE
could expose a bug’s multiple error behaviors. It should be
noted that the Linux kernel’s codebase is huge and sophisti-
cated. Given a kernel bug, it usually requires extensive manual
efforts and signiﬁcant expertise, spending hundreds of hours to
perform through manual analysis for exploring all the possible
errors. As a result, we evaluate the false negatives of GREBE
by sampling 30% of the selected kernel bugs (18 out of 60
selected bugs).
B. Experiment Results
Effectiveness. Table II shows the sampled experiment re-
sults3. First, we can observe that our tool – GREBE– could
demonstrate a signiﬁcant advantage in ﬁnding a bug’s multiple
error behaviors. In comparison with Syzkaller and Syzkaller
variant, which discover a total of 9 additional error behaviors
for only 6 and 7 test cases within 7 days, GREBE identiﬁes
132 new error behaviors for 38 out of 60 test cases. These
kernel error behaviors have not been seen in the bug reports
that we gathered from Syzbot. Second, we can observe the
mutation optimization greatly improves GREBE’s utility. In 7
days of our experiment, GREBE without mutation optimization
pinpoints 58 new error behaviors for 27 cases. This result
signiﬁcantly outperforms that of Syzkaller. However, without
mutation optimization, GREBE experiences more than 50% of
a downgrade in terms of the newly identiﬁed error behaviors
3It should be noted that, due to the space limit, we place the complete
experiment results at [26].
(132 vs. 58) and about 30% of decrease in terms of the
cases it could handle (38 vs. 27). Third, we discover that,
while generally performing worse than GREBE, GREBE with-
out enabling mutation optimization sometimes demonstrates
better performance. For the test cases – #8eceaff, #3b7409f,
and #d5222b3, GREBE without mutation optimization tracks
down 4 additional error behaviors. We argue this does not
imply the ineffectiveness of our mutation optimization method.
Our manual inspection indicates the missing error behaviors
primarily result from the nature of these bugs. Even if our
mutation mechanism successfully constructs correct inputs to
trigger the bug, making the bug manifest a different error
behavior also relies upon a speciﬁc thread interleaving that
mutation-optimization-disabled solution luckily discovers.
False negatives. As is mentioned above, we also randomly
selected 30% of test cases, performed manual analysis, and
examined how complete GREBE could identify the error
behaviors of a given kernel bug. In our experiment, the test
cases used for our false negative study are listed in Table V in
Appendix. Our manual inspection shows that GREBE misses
one error behavior for the cases #d1baeb1, #85fd017 and #
(cid:44)→ 695527b, and two error behaviors for the case #d5222b3. To
understand the reasons behind these missing error behaviors,
we ﬁrst measure the number of basic blocks between the root
cause of a kernel bug and its error panic site. We hypothesize
that the false negative might relate to the distance between
the root cause and the error site. However, as is shown in
Table V, we did not ﬁnd clear correlation between the distance
and the effectiveness of GREBE. For more detail about the
measurement and hypothesis validation, readers could refer to
Appendix C.
With the rejected hypothesis in hand, we further took a
look at false-negative cases closely, exploring the conditions
of triggering the missing error behaviors. We found that,
in addition to ﬁnding different paths and contexts by using
GREBE, the exhibition of the missing behaviors also requires
the manipulation of thread interleaving. For case like #85fd017
(cid:44)→ , the manifestation of error behaviors depends on the layout
of memory. The undiscovered error behavior occurs only if
the memory in the overﬂowed region is unmapped. We do
not attribute this to the incompetency of GREBE. Rather, we
will leave the manipulation of thread interleaving and memory
layout as part of our future research.
Impact of popular kernel structure removal. Recall that in
Section IV-B, we rank the identiﬁed critical structures based
on their popularity and avoid using popular structures to guide
our kernel fuzzing. Intuition suggests this might inﬂuence the
effectiveness of our kernel fuzzing on ﬁnding a bug’s multiple
error behaviors. However, from the 60 kernel bugs of our
selection, we observe there are only 3 out of 60 test cases
(5%) the root cause of which ties to popular structures (sk_buff
(cid:44)→ for #d1baeb1, nlattr for #b36d7e4 and #27ae1ae). Even for
these cases, GREBE still demonstrates its utility in ﬁnding
the bugs’ multiple error behaviors. These observations well
align with our aforementioned arguments –  the kernel bug
generally roots in the inappropriate usage of less popular
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:04 UTC from IEEE Xplore.  Restrictions apply. 
2087
SYZ ID
Critical Structures Identiﬁed
Initial Error Behavior
Discovered New Error Behaviors
Time (in hours)
T4
T1 T2 T3
WARNING: refcount bug in crypto mod get