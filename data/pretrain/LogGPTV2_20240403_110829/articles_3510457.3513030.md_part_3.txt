similarincidentinvestigationsandpromptlydetectthelikelyroot
thereneedstobe“collectivelearning”overpastincidentstopredict
causesandremediationsbasedonthem.Forthiswebuildaspe-
theroot-causegivenanewsymptom.Accordinglyweframethe
cializedIncidentSearchandRetrievalbasedRCAwhichtakesany
problemofpredictingRootCauseforagivenSymptom,asaLink
open-endednaturallanguagequeryandi)retrievesthemostsimilar
Predictiontaskbetweenthesymptomandroot-causenodes,given
pastincidentsandpresentsthekeyRCAinformationinastructured
theclustergraphsoversymptomsandroot-causes.Empiricallywe
formii)predictsthemostlikelyrootcausesandresolutionsforthe
showthatbytrainingwiththeknowledgeofthesymptomroot-
queriedsymptombasedonthepastinvestigations.
causeconnectionfromonly1%ofincidents,ourmodelachievesan
appreciablelinkpredictionperformanceonthetestincidents. IncidentSearch:OurIncidentSearchengineisbasedonneural
searchwhichrepresentsandindexesdocumentsasdensehigh-
DataAugmentation:Duringtrainingwefurtheraddnoisyedges
dimensionalvectorsandretrievesthemusingefficientvector-space
𝐸𝑛
betweensymptomandroot-causenodes,byconnectingtheir
𝑆𝑅 search.Neuralsearchhasseveraladvantagesovertraditionalsym-
one-hopneighbours,obtainedfromtherespectiveclustergraphs.
bolicsearch,likeseamlesslyhandlingtypologicalerrors,domain
Forbothtasks,ournumberoftrueedges|𝐸 𝑆𝑅|constitutesofonly
specificabbreviationsoragglutinationsandnon-canonicalizedword-
5%ofall(i.e.trueandnoisyi.e.|𝐸 𝑆𝑅|+|𝐸 𝑆𝑛 𝑅|)edges.
ingsabundantinmanualdocumentations(e.g.“connectionpool”
𝐸 𝑆𝑛 =(𝑣 𝑖,𝑣 𝑗)|∃(𝑣 𝑖,𝑣 𝑘) ∈𝐸 𝑆,(𝑣 𝑙,𝑣 𝑗) ∈𝐸 𝑅,(𝑣 𝑘,𝑣 𝑙) ∈𝐸 𝑆𝑅.
𝑅 as“conpool”or“connpool”)
Training Data: We train different models by varying how the • DataRepresentation:Thesearchindexisbuiltoverthedata
trainingandvalidationdataisselected.Wefollowtwosetupsi)All fromtherawPRBSubjectandInvestigationdocumentaswellas
EdgeSamplingii)NoisyEdgeSampling.Respectivelyforthem,we allthetargetedinformationextractedfromthem.Eachtopicor
sample𝑥%ofedgesfromAlledgesi.e.(𝐸 𝑆𝑅∪𝐸 𝑆𝑛 𝑅)orNoisyedges sentencefromtherawdocumentsorextractedsummariesisrep-
i.e.𝐸 𝑆𝑛 𝑅 fortheTrain+Validationdatasplit.Inbothcases,webuild resentedasavector,obtainedastheaverageofthecontextualized
multiplemodelsbyvarying𝑥 ∈{1%,2%,5%,10%,20%}. tokenembeddingsfromthepretrainedRoBERTa[19].
Model:WeuseaGraphConvolutionalNetwork(GCN)[15]based • Indexing:Eachsentenceorphraseisseparatelyindexedasvec-
neuralmodelovertheSymptomRoot-Causebipartitegraph.GCN torbyFAISS[13]whichallowsfastapproximatenearest-neighbor
hasbeenpopularlyusedforlearningrepresentationsovergraphs, retrievaloverhighdimensionalvectorspace.
byiterativelyupdatingnoderepresentationwiththeirneighbours. • QueryRepresentation:Queriesbeingtypicallyshort,arealso
First,wecomputesymptomandrootcausenoderepresentations representedasaverageofRoBERTabasedtokenembeddings.
asaverageofGloVebasedtokenembeddings(withdimension𝑑).
• Retrieval&Ranking:FAISSsearchretrievesthemostrelevant
𝑋˜ 𝑆 ∈ 𝑅|𝑉𝑆|×𝑑 and𝑋˜ 𝑅 ∈ 𝑅|𝑉𝑅|×𝑑 representthenodeembedding sentences over all PRB Documents, scoring them w.r.t query
matricesofSymptomandRootCausenodes.Thenweaddseparate
basedonstandardvectorsimilaritymetrics(e.g.,dot-product).
GCNlayersontheclustergraphsoversymptomsandrootcauses.
Thesesentence-levelscores aremax-pooledto getan overall
Thisresultsinupdatedrepresentationsofsymptomandroot-cause
document-levelscoreforeachofthetop-Kretrievedresults.
nodesbymessagepassingovertheirimmediateneighbours.
𝑋 =𝐿𝑖𝑛𝑒𝑎𝑟(𝑅𝑒𝐿𝑈(𝐺𝐶𝑁(𝑋˜ ,𝐸 ))) • Visualizing:TheretrievedPRBdocumentsarethenshowninan
𝑆 𝑆 𝑆
𝑋 =𝐿𝑖𝑛𝑒𝑎𝑟(𝑅𝑒𝐿𝑈(𝐺𝐶𝑁(𝑋˜ ,𝐸 ))) easilyconsumablestructuredformoftheextractedinformation:
𝑅 𝑅 𝑅
Foreachtraininginstance,wesampleanedge (𝑣 𝑠,𝑣 𝑟) fromthe InvestigationSubject,Topics,Summaries,RootCauseandResolu-
trainingdataandrandomlysample𝑛negativerootcauses{𝑣 𝑟− ,𝑗|𝑗 = tion.Apartfromthis,wealsoshowaqueryspecificsubgraphof
1,···,𝑛,(𝑣 𝑠,𝑣 𝑟− ,𝑗)∉𝐴𝑙𝑙𝐸𝑑𝑔𝑒𝑠}andcomputetheirGCNtransformed t uh se erC sK tG oia ns ts eo rc ai ca tt ie vd ew lyit nh at vh ige at to ep- ok vere rtr thie eve rd elr ae ts eu dlt is n. cT idh eis na tsllo aw nds
representationi.e.{𝑥 𝑠,𝑥 𝑟,{···𝑥 𝑟− ,𝑗···}}.Thenforthepositivepair
exploretheirunderlyingrootcauseandresolutionclusters.
andeachofthenegativepairsofsymptom-rootcauses,wecompute
adotproductsimilaritybetweenthesymptomandtherespective RetrievalbasedRCA:Weapplythefollowingtwotechniquesfor
rootcauserepresentation.Wethentraintheneuralnetworkwitha obtainingadistributionofmostlikelyrootcausesandresolutions
negativeloglikelihoodofthepositivepairofsymptom-rootcause. (whichfollowssimilarly)fromthetop-Kretrievedincidents.
𝑒𝑠𝑖𝑚(𝑥 ,𝑥 ) • UsingIncidentSearch:Foreachsearchresult,themultipleex-
𝐿(𝑣 ,𝑣 ,{···𝑣− ···})=−𝑙𝑜𝑔( 𝑠 𝑟 )
𝑠 𝑟 𝑟𝑗 (cid:2)𝑛 tractedrootcausespansaremergedintoasinglesentence-form
𝑒𝑠𝑖𝑚(𝑥 𝑠,𝑥 𝑟)+ 𝑒𝑠𝑖𝑚(𝑥 𝑠,𝑥 𝑟− ,𝑗) withthespanscoresmax-pooledandmultiplicativelycombined
𝑗=1 withtherelevancescoreofthesearchresult.Afterde-duplication
oftheroot-causesandscore-normalizationweobtainthedistri-
Duringinference,givenasymptom,wecomputethedotproductof
butionoverthetop-klikelyroot-causes.
theGCNtransformedrepresentationwiththatofeachrootcause
201
ICSE-SEIP’22,May21–29,2022,Pittsburgh,PA,USA AmritaSahaandStevenC.H.Hoi
• UsingIncidentSearchandCausalKnowledgeGraph:We tofreelymodifyordeleteanyspanthatisfoundtobenotgram-
takethesymptomextractedfromthetop-kretrievedsearchre- maticallywell-formedorincorrectastherootcauseorresolution.
sults and for each symptom we predict the top-k likely root- Theannotatorcanalsoindependentlyaddotherspansdeemedas
causesusingtheCausalKnowledgeGraphasdescribedin3.3. correct.Theoverallresultsshowthattheunsupervisedmodels
Therootcausepredictionscoreismultiplicativelycombinedwith indeedperformremarkablywell.79%ofthepredictedroot-cause
thesearchresultrelevancescore,andaggregatedoverrepeated and70%ofresolutionspansarefoundtobeexactlycorrectandthe
occurrences,ifany.Theresultingtop-krootcausesformthe (micro)averageF1-Scoreofthepredictedandannotatedspans(in
predicteddistributionwiththeirfinalscoresL1-normalized. termsofBagofAllWordsorBagofNon-StopWords)isaround88%
and81%respectivelyforrootcausesandresolutions.
Info Total AnnotationLabels&OverallMetrics
Topics 1320 WellFormed: Informat- Uninteres- Unclear: Extra QualitativeAnalysisofRootCause&ResolutionExtraction:
1276 ive:1009 ting:248 60 Words:15
InFig.5,weillustrateafewexamplesoftherootcauseandreme-
Summ-525 Informative: TooSpecific:46 TooGeneric:43
ary 435 dialactionsrespectivelyextractedfromtherawRootCauseand
Root 320 ExactMatch: F1(BagofNonStop F1 (Bag of All ImmediateResolutiondatafieldsofpastincidents.Weobservethat
Cause 79.07% Words):87.97% Words):88.44%
i)ourmodelsareabletoextractrelevantrootcauseorresolution
Resol- 175 ExactMatch: F1(BagofNonStop F1 (Bag of All
ution 70.34% Words):81.57% Words):81.69% spans,despitethelongunstructurednatureofthedocumenthaving
Table2:HumanvalidatedresultsofInformationExtraction abundantunseentechnicaljargonii)themodeldoesnotshowany
undesirablebiastowardspassagelocationinspanselectioniii)the
selectedspansarewell-formedshortcrispself-explanatorytopics.
4 EVALUATIONANDANALYSIS
4.2 NeuralKnowledgeMining
InthissectionweevaluatetheICAmodulesanddownstreamInci-
dentSearchandRCAtasksoverourin-housecollectedPRBdataset Nextwepresentsomequalitativeanalysisandillustrativeexamples
of2000incidents.AsthefirstworktoleveragePRBdataforAIOps oftheNeuralKnowledgeMiningcomponent.
andRCA,ourobjectiveistovalidatethefeasibilityofusinggeneric Symptom,Root-CauseandResolutionClustering:Forclus-
NLP models in mining PRB data for building a specialized tool tering,wecollectuniquedescriptionsof1072symptoms,1915root
likeICA.Wepresentempiricalresultsthroughsystematicquantita- causesand2250resolutionsextractedoverallPRBdocuments.Fig.
tiveevaluations,qualitativeanalysisandsurveys,expert-annotated 6(right)presentsthet-SNEvisualization(usingthenodeembed-
validationofthemodelpredictions,andrealincidentcasestudies dingvector)oftheresulting60Symptomclusters,showingthatthe
afterdeploymentinwhichweseektheSRE’sjudgementonthe clusteringisindeedquitewell-separatedanddistinctive.
effectivenessoftheproposedmodel’spredictionsinourapproach. Post-IncidentAnalysis:TheCausalKnowledgeGraphbuiltover
thePRBrepositorycanbeinstrumentalinanorganization-wide
4.1 NeuralInformationExtraction
post-mortemanalysisofallpastincidents.Fore.g.itcananalyzethe
HumanValidationofExtractedInformation:InTable2we categoriesofmostcommonlyoccurringsymptomsorroot-causes
providethequantitativeresultsofasurveyconductedoverdomain andresolutionsinthelastmonth.InFig.6(left)wepresentthe
expertsandtargetend-users.Itsaimistovalidateandannotatethe generatedclusterlabelsofthemostfrequentlyoccurringclustersof
qualityoftopics,summaries,rootcauseandresolutionsthatare symptoms,rootcausesandresolutions,alongwiththe%coverageof
extractedfromPRBdocuments(Sec.3.1). incidents.Thisalsoillustratesthequalityofthesemanticlabeling
ofclusters.Evenwithsimplegreedyn-gramselectiontechniques
Topics:Weselected1320topicsoverallPRBdocuments,sampling
(Sec.3.2),thegeneratedclusterlabelsstillconstituteofreasonably
uniformlyfromthetopicscoredistribution.Onthis,annotators
coherentwell-formedandself-explanatorytopics.
wereaskedtoprovidefollowing(binary)labels:i)Grammatically
Well-formed(maynotbeinformative)ii)SufficientlyInformative 4.3 IncidentSearchandRetrievalbasedRCA
iii)ClarityinMeaningiv)TooGenericorUninterestingiv)Has
InthissectionweevaluatetheperformanceoftheproposedICA
extrairrelevantwords.Astheresultsshow,mostofthetopicsare
pipelineinretrievingpastincidentswithsimilarsymptomsand
well-formedandaround76%arefoundtobeinformativeanduseful.
predictingthelikelyrootcausesandresolutionsfromthoseinvesti-
Summary:Wesimilarlytakethegeneratedsummaryof525PRB gations.SincethePRBdataitselfistheonlyOracledataavailableto
documentsandasktheannotatorstoprovidethefollowingbinary us,weshowcasethisbasedonthebelowquantitativebenchmark-
labelsi)SatisfactorilyInformativeii)TooSpecific(i.e.,hasaddi- ingtechnique.Inourexperimentalsetup,say,anincidentfroma
tionalirrelevantsentences)iii)TooGeneric(e.g.,doesnothave givenPRBdocumentistheongoingtargetincident,withitsonly
anyinformationabouttheoutcomeoftheinvestigation).However, knowninformationbeingtheincidentsymptom.Thenassumingall
sometimes,thesummaryistoogenericduetotheoriginalPRB theremainingPRBdocumentsareavailabletosearchandanalyze
documentbeingincomplete.Despitethat,around83%ofsummaries over,ourbenchmarkingistargetedattwoquestions