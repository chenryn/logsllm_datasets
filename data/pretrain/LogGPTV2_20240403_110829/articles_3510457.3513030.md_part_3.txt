similarincidentinvestigationsandpromptlydetectthelikelyroot
thereneedstobeâ€œcollectivelearningâ€overpastincidentstopredict
causesandremediationsbasedonthem.Forthiswebuildaspe-
theroot-causegivenanewsymptom.Accordinglyweframethe
cializedIncidentSearchandRetrievalbasedRCAwhichtakesany
problemofpredictingRootCauseforagivenSymptom,asaLink
open-endednaturallanguagequeryandi)retrievesthemostsimilar
Predictiontaskbetweenthesymptomandroot-causenodes,given
pastincidentsandpresentsthekeyRCAinformationinastructured
theclustergraphsoversymptomsandroot-causes.Empiricallywe
formii)predictsthemostlikelyrootcausesandresolutionsforthe
showthatbytrainingwiththeknowledgeofthesymptomroot-
queriedsymptombasedonthepastinvestigations.
causeconnectionfromonly1%ofincidents,ourmodelachievesan
appreciablelinkpredictionperformanceonthetestincidents. IncidentSearch:OurIncidentSearchengineisbasedonneural
searchwhichrepresentsandindexesdocumentsasdensehigh-
DataAugmentation:Duringtrainingwefurtheraddnoisyedges
dimensionalvectorsandretrievesthemusingefficientvector-space
ğ¸ğ‘›
betweensymptomandroot-causenodes,byconnectingtheir
ğ‘†ğ‘… search.Neuralsearchhasseveraladvantagesovertraditionalsym-
one-hopneighbours,obtainedfromtherespectiveclustergraphs.
bolicsearch,likeseamlesslyhandlingtypologicalerrors,domain
Forbothtasks,ournumberoftrueedges|ğ¸ ğ‘†ğ‘…|constitutesofonly
specificabbreviationsoragglutinationsandnon-canonicalizedword-
5%ofall(i.e.trueandnoisyi.e.|ğ¸ ğ‘†ğ‘…|+|ğ¸ ğ‘†ğ‘› ğ‘…|)edges.
ingsabundantinmanualdocumentations(e.g.â€œconnectionpoolâ€
ğ¸ ğ‘†ğ‘› =(ğ‘£ ğ‘–,ğ‘£ ğ‘—)|âˆƒ(ğ‘£ ğ‘–,ğ‘£ ğ‘˜) âˆˆğ¸ ğ‘†,(ğ‘£ ğ‘™,ğ‘£ ğ‘—) âˆˆğ¸ ğ‘…,(ğ‘£ ğ‘˜,ğ‘£ ğ‘™) âˆˆğ¸ ğ‘†ğ‘….
ğ‘… asâ€œconpoolâ€orâ€œconnpoolâ€)
Training Data: We train different models by varying how the â€¢ DataRepresentation:Thesearchindexisbuiltoverthedata
trainingandvalidationdataisselected.Wefollowtwosetupsi)All fromtherawPRBSubjectandInvestigationdocumentaswellas
EdgeSamplingii)NoisyEdgeSampling.Respectivelyforthem,we allthetargetedinformationextractedfromthem.Eachtopicor
sampleğ‘¥%ofedgesfromAlledgesi.e.(ğ¸ ğ‘†ğ‘…âˆªğ¸ ğ‘†ğ‘› ğ‘…)orNoisyedges sentencefromtherawdocumentsorextractedsummariesisrep-
i.e.ğ¸ ğ‘†ğ‘› ğ‘… fortheTrain+Validationdatasplit.Inbothcases,webuild resentedasavector,obtainedastheaverageofthecontextualized
multiplemodelsbyvaryingğ‘¥ âˆˆ{1%,2%,5%,10%,20%}. tokenembeddingsfromthepretrainedRoBERTa[19].
Model:WeuseaGraphConvolutionalNetwork(GCN)[15]based â€¢ Indexing:Eachsentenceorphraseisseparatelyindexedasvec-
neuralmodelovertheSymptomRoot-Causebipartitegraph.GCN torbyFAISS[13]whichallowsfastapproximatenearest-neighbor
hasbeenpopularlyusedforlearningrepresentationsovergraphs, retrievaloverhighdimensionalvectorspace.
byiterativelyupdatingnoderepresentationwiththeirneighbours. â€¢ QueryRepresentation:Queriesbeingtypicallyshort,arealso
First,wecomputesymptomandrootcausenoderepresentations representedasaverageofRoBERTabasedtokenembeddings.
asaverageofGloVebasedtokenembeddings(withdimensionğ‘‘).
â€¢ Retrieval&Ranking:FAISSsearchretrievesthemostrelevant
ğ‘‹Ëœ ğ‘† âˆˆ ğ‘…|ğ‘‰ğ‘†|Ã—ğ‘‘ andğ‘‹Ëœ ğ‘… âˆˆ ğ‘…|ğ‘‰ğ‘…|Ã—ğ‘‘ representthenodeembedding sentences over all PRB Documents, scoring them w.r.t query
matricesofSymptomandRootCausenodes.Thenweaddseparate
basedonstandardvectorsimilaritymetrics(e.g.,dot-product).
GCNlayersontheclustergraphsoversymptomsandrootcauses.
Thesesentence-levelscores aremax-pooledto getan overall
Thisresultsinupdatedrepresentationsofsymptomandroot-cause
document-levelscoreforeachofthetop-Kretrievedresults.
nodesbymessagepassingovertheirimmediateneighbours.
ğ‘‹ =ğ¿ğ‘–ğ‘›ğ‘’ğ‘ğ‘Ÿ(ğ‘…ğ‘’ğ¿ğ‘ˆ(ğºğ¶ğ‘(ğ‘‹Ëœ ,ğ¸ ))) â€¢ Visualizing:TheretrievedPRBdocumentsarethenshowninan
ğ‘† ğ‘† ğ‘†
ğ‘‹ =ğ¿ğ‘–ğ‘›ğ‘’ğ‘ğ‘Ÿ(ğ‘…ğ‘’ğ¿ğ‘ˆ(ğºğ¶ğ‘(ğ‘‹Ëœ ,ğ¸ ))) easilyconsumablestructuredformoftheextractedinformation:
ğ‘… ğ‘… ğ‘…
Foreachtraininginstance,wesampleanedge (ğ‘£ ğ‘ ,ğ‘£ ğ‘Ÿ) fromthe InvestigationSubject,Topics,Summaries,RootCauseandResolu-
trainingdataandrandomlysampleğ‘›negativerootcauses{ğ‘£ ğ‘Ÿâˆ’ ,ğ‘—|ğ‘— = tion.Apartfromthis,wealsoshowaqueryspecificsubgraphof
1,Â·Â·Â·,ğ‘›,(ğ‘£ ğ‘ ,ğ‘£ ğ‘Ÿâˆ’ ,ğ‘—)âˆ‰ğ´ğ‘™ğ‘™ğ¸ğ‘‘ğ‘”ğ‘’ğ‘ }andcomputetheirGCNtransformed t uh se erC sK tG oia ns ts eo rc ai ca tt ie vd ew lyit nh at vh ige at to ep- ok vere rtr thie eve rd elr ae ts eu dlt is n. cT idh eis na tsllo aw nds
representationi.e.{ğ‘¥ ğ‘ ,ğ‘¥ ğ‘Ÿ,{Â·Â·Â·ğ‘¥ ğ‘Ÿâˆ’ ,ğ‘—Â·Â·Â·}}.Thenforthepositivepair
exploretheirunderlyingrootcauseandresolutionclusters.
andeachofthenegativepairsofsymptom-rootcauses,wecompute
adotproductsimilaritybetweenthesymptomandtherespective RetrievalbasedRCA:Weapplythefollowingtwotechniquesfor
rootcauserepresentation.Wethentraintheneuralnetworkwitha obtainingadistributionofmostlikelyrootcausesandresolutions
negativeloglikelihoodofthepositivepairofsymptom-rootcause. (whichfollowssimilarly)fromthetop-Kretrievedincidents.
ğ‘’ğ‘ ğ‘–ğ‘š(ğ‘¥ ,ğ‘¥ ) â€¢ UsingIncidentSearch:Foreachsearchresult,themultipleex-
ğ¿(ğ‘£ ,ğ‘£ ,{Â·Â·Â·ğ‘£âˆ’ Â·Â·Â·})=âˆ’ğ‘™ğ‘œğ‘”( ğ‘  ğ‘Ÿ )
ğ‘  ğ‘Ÿ ğ‘Ÿğ‘— (cid:2)ğ‘› tractedrootcausespansaremergedintoasinglesentence-form
ğ‘’ğ‘ ğ‘–ğ‘š(ğ‘¥ ğ‘ ,ğ‘¥ ğ‘Ÿ)+ ğ‘’ğ‘ ğ‘–ğ‘š(ğ‘¥ ğ‘ ,ğ‘¥ ğ‘Ÿâˆ’ ,ğ‘—) withthespanscoresmax-pooledandmultiplicativelycombined
ğ‘—=1 withtherelevancescoreofthesearchresult.Afterde-duplication
oftheroot-causesandscore-normalizationweobtainthedistri-
Duringinference,givenasymptom,wecomputethedotproductof
butionoverthetop-klikelyroot-causes.
theGCNtransformedrepresentationwiththatofeachrootcause
201
ICSE-SEIPâ€™22,May21â€“29,2022,Pittsburgh,PA,USA AmritaSahaandStevenC.H.Hoi
â€¢ UsingIncidentSearchandCausalKnowledgeGraph:We tofreelymodifyordeleteanyspanthatisfoundtobenotgram-
takethesymptomextractedfromthetop-kretrievedsearchre- maticallywell-formedorincorrectastherootcauseorresolution.
sults and for each symptom we predict the top-k likely root- Theannotatorcanalsoindependentlyaddotherspansdeemedas
causesusingtheCausalKnowledgeGraphasdescribedin3.3. correct.Theoverallresultsshowthattheunsupervisedmodels
Therootcausepredictionscoreismultiplicativelycombinedwith indeedperformremarkablywell.79%ofthepredictedroot-cause
thesearchresultrelevancescore,andaggregatedoverrepeated and70%ofresolutionspansarefoundtobeexactlycorrectandthe
occurrences,ifany.Theresultingtop-krootcausesformthe (micro)averageF1-Scoreofthepredictedandannotatedspans(in
predicteddistributionwiththeirfinalscoresL1-normalized. termsofBagofAllWordsorBagofNon-StopWords)isaround88%
and81%respectivelyforrootcausesandresolutions.
Info Total AnnotationLabels&OverallMetrics
Topics 1320 WellFormed: Informat- Uninteres- Unclear: Extra QualitativeAnalysisofRootCause&ResolutionExtraction:
1276 ive:1009 ting:248 60 Words:15
InFig.5,weillustrateafewexamplesoftherootcauseandreme-
Summ-525 Informative: TooSpecific:46 TooGeneric:43
ary 435 dialactionsrespectivelyextractedfromtherawRootCauseand
Root 320 ExactMatch: F1(BagofNonStop F1 (Bag of All ImmediateResolutiondatafieldsofpastincidents.Weobservethat
Cause 79.07% Words):87.97% Words):88.44%
i)ourmodelsareabletoextractrelevantrootcauseorresolution
Resol- 175 ExactMatch: F1(BagofNonStop F1 (Bag of All
ution 70.34% Words):81.57% Words):81.69% spans,despitethelongunstructurednatureofthedocumenthaving
Table2:HumanvalidatedresultsofInformationExtraction abundantunseentechnicaljargonii)themodeldoesnotshowany
undesirablebiastowardspassagelocationinspanselectioniii)the
selectedspansarewell-formedshortcrispself-explanatorytopics.
4 EVALUATIONANDANALYSIS
4.2 NeuralKnowledgeMining
InthissectionweevaluatetheICAmodulesanddownstreamInci-
dentSearchandRCAtasksoverourin-housecollectedPRBdataset Nextwepresentsomequalitativeanalysisandillustrativeexamples
of2000incidents.AsthefirstworktoleveragePRBdataforAIOps oftheNeuralKnowledgeMiningcomponent.
andRCA,ourobjectiveistovalidatethefeasibilityofusinggeneric Symptom,Root-CauseandResolutionClustering:Forclus-
NLP models in mining PRB data for building a specialized tool tering,wecollectuniquedescriptionsof1072symptoms,1915root
likeICA.Wepresentempiricalresultsthroughsystematicquantita- causesand2250resolutionsextractedoverallPRBdocuments.Fig.
tiveevaluations,qualitativeanalysisandsurveys,expert-annotated 6(right)presentsthet-SNEvisualization(usingthenodeembed-
validationofthemodelpredictions,andrealincidentcasestudies dingvector)oftheresulting60Symptomclusters,showingthatthe
afterdeploymentinwhichweseektheSREâ€™sjudgementonthe clusteringisindeedquitewell-separatedanddistinctive.
effectivenessoftheproposedmodelâ€™spredictionsinourapproach. Post-IncidentAnalysis:TheCausalKnowledgeGraphbuiltover
thePRBrepositorycanbeinstrumentalinanorganization-wide
4.1 NeuralInformationExtraction
post-mortemanalysisofallpastincidents.Fore.g.itcananalyzethe
HumanValidationofExtractedInformation:InTable2we categoriesofmostcommonlyoccurringsymptomsorroot-causes
providethequantitativeresultsofasurveyconductedoverdomain andresolutionsinthelastmonth.InFig.6(left)wepresentthe
expertsandtargetend-users.Itsaimistovalidateandannotatethe generatedclusterlabelsofthemostfrequentlyoccurringclustersof
qualityoftopics,summaries,rootcauseandresolutionsthatare symptoms,rootcausesandresolutions,alongwiththe%coverageof
extractedfromPRBdocuments(Sec.3.1). incidents.Thisalsoillustratesthequalityofthesemanticlabeling
ofclusters.Evenwithsimplegreedyn-gramselectiontechniques
Topics:Weselected1320topicsoverallPRBdocuments,sampling
(Sec.3.2),thegeneratedclusterlabelsstillconstituteofreasonably
uniformlyfromthetopicscoredistribution.Onthis,annotators
coherentwell-formedandself-explanatorytopics.
wereaskedtoprovidefollowing(binary)labels:i)Grammatically
Well-formed(maynotbeinformative)ii)SufficientlyInformative 4.3 IncidentSearchandRetrievalbasedRCA
iii)ClarityinMeaningiv)TooGenericorUninterestingiv)Has
InthissectionweevaluatetheperformanceoftheproposedICA
extrairrelevantwords.Astheresultsshow,mostofthetopicsare
pipelineinretrievingpastincidentswithsimilarsymptomsand
well-formedandaround76%arefoundtobeinformativeanduseful.
predictingthelikelyrootcausesandresolutionsfromthoseinvesti-
Summary:Wesimilarlytakethegeneratedsummaryof525PRB gations.SincethePRBdataitselfistheonlyOracledataavailableto
documentsandasktheannotatorstoprovidethefollowingbinary us,weshowcasethisbasedonthebelowquantitativebenchmark-
labelsi)SatisfactorilyInformativeii)TooSpecific(i.e.,hasaddi- ingtechnique.Inourexperimentalsetup,say,anincidentfroma
tionalirrelevantsentences)iii)TooGeneric(e.g.,doesnothave givenPRBdocumentistheongoingtargetincident,withitsonly
anyinformationabouttheoutcomeoftheinvestigation).However, knowninformationbeingtheincidentsymptom.Thenassumingall
sometimes,thesummaryistoogenericduetotheoriginalPRB theremainingPRBdocumentsareavailabletosearchandanalyze
documentbeingincomplete.Despitethat,around83%ofsummaries over,ourbenchmarkingistargetedattwoquestions