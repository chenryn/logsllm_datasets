U
d
e
z
i
l
a
m
r
o
N
1.0
0.8
0.6
0.4
0.2
0.0
e
g
a
s
U
d
e
z
i
l
a
m
r
o
N
1.0
0.8
0.6
0.4
0.2
0.0
0
3
6
9
12
15
0
3
6
9
12
15
Time (minutes)
Time (minutes)
(a) Normal interval
(b) Anomalous interval
Figure 3: Importance of operating context; (a) shows the CPU
and memory usage of a VM, and the host’s cache miss for a
normal interval; (b) for an anomalous VM migration interval,
the CPU and memory usage of the VM almost remain same, but
represents higher host’s cache miss. We observed on an average
95% difference in the host’s cache-miss between (a) and (b).
• Online Model Generation: A traditional problem determi-
nation approach learns an application model a priori based
on historical data. Even with our transformation, the number
of canonical operating contexts may be large and difﬁcult to
be known a priori. Hence, CloudPD adopts an online model
generation approach for building performance models. This
also allows CloudPD to consider an application as a black
box and extend to new ones on the ﬂy.
• Combining Simple and Correlation-based Models: We
observed that pair-wise correlations are stable (Section V-D),
but computing all pairwise correlations is fairly expensive.
We combine the strengths of both simple and correlation-
based techniques in a two-phase approach, where a light-
weight resource model generates events in the ﬁrst phase
and a moderately expensive correlation-based analysis is per-
formed to accurately identify faults for only a small number
of intervals, where the ﬁrst phase generated events. To further
reduce the time spent in the second phase, we (i) use only
linear correlations, which are present for 50% metrics [17],
and can be learned quickly with small amount of data; and
(ii) compute correlations only for those metric pairs, which
are likely to be correlated (i.e., metrics of the same VM or the
same metric across VMs that are part of a cluster). Computing
correlations for only a subset of intervals helps CloudPD
scale to large clouds. We also considered the use of canonical
correlation analysis (CCA) to identify correlated metrics [12].
However, CCA cannot extract negative correlations, which we
observed in practice (e.g., CPU and disk usage are negatively
correlated), precluding its use in CloudPD.
B. Architecture
Figure 4 shows the architecture of CloudPD. It comprises of
ﬁve key components: (a) a Monitoring Engine that monitors
each virtual machine and physical server for the metrics of
interest; (b) an Event Generation Engine that quickly identiﬁes
a potential symptom of a fault and generates an event; (c)
a Problem Determination Engine that further analyzes the
event to determine deviations from normal behavior; (d) a
Problem Diagnosis Engine that classiﬁes the anomaly based
on expert knowledge; and (e) an Anomaly Remediation Engine
that executes remedial actions on the diagnosed anomalies. We
describe each of them in detail below:
VM1 VM2
. . .
VMn
VM1 VM2
. . .
VMn
Server hostn
CLOUD 
Server hostn
Cluster1
INFRASTRUCTURE
Clustern
(system + application )  metric
System Metric Profiler
Data Preprocessor
KNN
HMM
CPU, memory, disk, 
network, cache, page faults, 
context switches, system load
Application Metric
Profiler
latency, throughput
Data Smoothing 
& Filtering
Moving Average 
Time-series
MONITORING
ENGINE
monitoring data time-series
Event
Analyzer
Model Builder
EVENT GENERATION 
ENGINE
event
anomaly
Statistical
Analyzer
Arbitrates faults to 
other cloud modules
ANOMALY 
REMEDIATION
MANAGER
Fault 
Classification 
(Signature Based )
Fault
Localization
Resource, VM, 
Host, Time
DIAGNOSIS ENGINE
PROBLEM 
DETERMINATION   
ENGINE
Workload Change
Figure 4: Architecture of CloudPD.
1) Monitoring Engine: This component collects and pro-
cesses various system metrics pertaining to CPU, memory,
cache, network, and disk resources, and application metrics
such as latency and throughput for every virtual machine
and physical server (see Table I). It is designed to capture
(i) system resource metrics; (ii) operating context; and (iii)
application performance metrics. The System Metric Proﬁler
collects system resource metrics as well as the operating
context, while the Application Metric Proﬁler collects all the
application performance metrics. All metrics are collected
at periodic intervals with a conﬁgurable monitoring interval
parameter. CloudPD has a Data Preprocessor module that
removes outliers and noise. The Data Preprocessor takes the
raw time-series data and generates data-points. A data-point
is deﬁned as a sequence of moving average values over a ﬁxed
interval of time and forms the basic input unit for the various
anomaly detection algorithms used in CloudPD.
2) Event Generation Engine: This module implements the
ﬁrst phase of the multi-layered CloudPD framework. It is
designed to identify the potential symptoms of anomalous be-
havior without performing a lot of computation. The generation
of an event may not immediately denote the presence of a fault,
but merely suggests the possibility of one. It indicates that one
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:43:49 UTC from IEEE Xplore.  Restrictions apply. 
or more metrics of a VM deviates from its performance model
(the model for normal behavior is continuously generated
online to cope with dynamic changes in the workload and
operating environment). Further analysis is needed to conﬁrm
if a fault is present, and if so, diagnose and classify it.
In order for the Event Generation Engine to be light-weight,
events are generated by looking at each monitored metric for
each VM in isolation. A broader correlation between metrics
across VMs can be very expensive as the number of such
comparisons grows exponentially in N ×M space, where N is
the number of VMs and M is the number of monitored metrics
for each VM. Note that the Event Generation Engine can be
parallelized with a separate thread performing the analysis for
each (metric,VM) pair. Further, in order to build the perfor-
mance model in an online fashion that is tolerant to workload
variations, we use the nearest-neighbor algorithm as it (i) is
simpler to implement; (ii) is computationally less expensive
when compared to other techniques such as clustering; and (iii)
can be updated online at little cost [22]. Note that these online
models are trained only with the historical data portions free
of anomalies. We leverage existing techniques [1], [9], [18]
for the methodology used in this phase (see Section IV-B).
3) Problem Determination Engine: This component utilizes
statistical correlation across VMs and resource metrics to
identify anomalies and localize them. For every event gener-
ated by the Event Generation Engine, this stage analyzes the
data further to identify anomalies (the Problem Determination
Engine is not invoked unless an event is generated). Let us
suppose that an event is generated for a metric Mj on a
VM, V Mi. This stage computes correlations between data
for Mj and data for every other metric of V Mi, as well as
correlations between data for Mj on V Mi and data for Mj on
every other VM running the same application (wherever the
information is available). The ﬁrst set of correlations capture
the relation between metrics for the VM (e.g., the correlation
between resource metrics and their operating context), whereas
the second set of correlations is based on the idea of using
peer VMs to ﬂag faults that occur in only one VM. Based
on the knowledge of typical correlation values under normal
behavior, any signiﬁcant deviations from normal correlation
values (with range [−1, 1]) are noted. If the deviations are
larger than an empirically determined threshold, a fault event is
generated and forwarded to the Problem Diagnosis Engine for
problem diagnosis (as per Algorithm 1). Note that although our
approach is similar to the idea of peer based correlation [12],
we only perform correlations on events generated in the
Event Generation Engine stage, and only for the VM(s) and
resource(s) tagged in this phase. This improves the scalability
signiﬁcantly as demonstrated in Section V-E2.
CPU Utilization
Correlation
100
1
M
V
f
o
)
%
(
n
o
i
t
a
z
i
l
i
t
U
U
P
C
80
60
40
20
0
(a)
100
2
M
V
f
o
)
%
(
n
o
i
t
a
z
i
l
i
t
U
U
P
C
80
60
40
20
0
1.0
0.8
n
o
i
t
a
0.6
l
e
r
r
o
C
0.4
0.2
0.0
Threshold = 0.5
(c)
(b)
0
20
40
60
80
0
20
40
60
80
0
20
40
60
80
Time (1 minute interval)
Time (1 minute interval)
Time (1 minute interval)
Figure 5: Correlation-based problem determination in CloudPD.
Algorithm 1 Correlation-based problem determination.
1: Let Tnormal−vm−r = Data for VM vm and metric r over a
normal interval from recent history; Ttest−vm−r = Data for VM
vm and metric r combined over test interval and normal interval
2: for each VM i in cluster do
3:
(ABS(corr(Ttest−vm−r, Ttest−i−r)
if
corr(Tnormal−vm−r, Tnormal−i−r))
then
≥
−
T hreshold)
Flag deviation as anomaly for further diagnosis
end if
4:
5:
6: end for
7: for each Metric j do
8:
(ABS(corr(Ttest−vm−r, Ttest−vm−j)
if
−
corr(Tnormal−vm−r, Tnormal−vm−j)) ≥ T hreshold)
then
Flag deviation as anomaly for further diagnosis
end if
9:
10:
11: end for
12: if No anomaly is ﬂagged then
13:
14: end if
Flag as normal workload change
In order to cope with dynamism, the Problem Determination
Engine phase computes the models and the deviations in
correlations (described above) in an online manner as shown
in Algorithm 1. Correlations on data from an interval classiﬁed
as normal from the recent history, is used to obtain a model of
normal application behavior. For the interval being analyzed,
we compute similar correlations and check if these correlations
deviate from the model of normal behavior. The total number
of correlations computed in this algorithm is of the order of
the number of metrics plus the number of VMs running the
application. Note that we do not analyze the parts of the system
that are not affected, and only analyze the neighborhood of the
location, where an event is generated. This helps CloudPD to
scale to large size systems and monitor several metrics.
We illustrate how correlation-based monitoring helps to local-
ize faults using a CPU hog anomaly example. In Figure 5, we
plot the CPU utilization of two VMs, running Hadoop Sort
on 5GB data (Figure 5(a), (b)). We introduce the CPU hog
anomaly during (60 − 70)th time interval on VM2. Figure 5(c)
plots the pairwise correlations between the CPU utilization of
VM1 and VM2. We observe the correlations drop signiﬁcantly
during this interval, and when the difference between this
observed and normal correlation exceeds a given threshold (say
0.5), one can effectively detect an anomalous behavior.
4) Problem Diagnosis Engine: This component uses pre-
deﬁned expert knowledge to categorize the potentially anoma-
lous scenarios detected by the Problem Determination Engine
into one of the several fault classes. The expert knowledge
is made available to the system in the form of standardized
fault signatures. Characterization of system anomalies in terms
of representative signatures is a critical part of diagnosis
process [23]. The fault signature captures the set of devia-
tions from normal behavior, both in the correlation values
computed by the Problem Determination Engine as well as
in the operating environment, that are characteristics of the
faults they describe. When an anomalous behavior is detected,
the deviations from normal behavior are matched with the
known fault signatures. If a match is found, this module will
successfully categorize the fault, else ﬂag it as an anomaly.
5) Anomaly Remediation Manager: This component of
CloudPD receives input from the Problem Diagnosis Engine
to perform suitable remedial actions. It is designed to deal
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:43:49 UTC from IEEE Xplore.  Restrictions apply. 
the cloud related faults identiﬁed so far. In case
with all
of a collocation fault, CloudPD sends an exclusion rule to
CloudRM that prevents collocation of the impacted VMs on a
common physical server. In case of a faulty live migration,
it provides new server utilization thresholds beyond which
live migration should not be performed. In case of a VM
sizing fault, it triggers resource estimation and resizing via
CloudRM. For all other cases, a notiﬁcation is sent to an
application or a system administrator. Note that, although
it is true that automated remediation is often unacceptable
to system administrators, especially for traditional distributed
systems like grids and data centers, clouds are forcing a
paradigm shift. Clouds represent a much more versatile system,
which employ a dynamic consolidation manager like VMware
DRS or Amazon Auto Scaling to automatically deal with
unexpected performance changes. In comparison, CloudPD
actions are safer (triggers cloud manager to re-consolidate or
add exclusion constraints between conﬂicting VMs only).
CloudPD can also address the problem of SLA violations
both at the application and infrastructure (in terms of VMs)
level. CloudPD associates VMs with their system and appli-
cations performance data. Since performance anomalies do
not lead to hard failures, we need performance deﬁnition for
normal and faulty situations. This deﬁnition can come from
SLAs, SLOs, or administrator-deﬁned goals. As long as the
baseline performance can be deﬁned, CloudPD can efﬁciently
respond to SLA infringement scenarios.
IV.
IMPLEMENTATION DETAILS
We have implemented CloudPD to perform fault diagnosis for
a VMware ESX-based cloud cluster (details in Section V-A).
We provide speciﬁc details of our implementation below:
The kNN technique works by computing a distance measure
between the data point under consideration and the nearest
k neighbors in a given set of model data points known to be
normal from recent history. In our implementation, the distance
between two data points is deﬁned as the sum of the differences
between corresponding samples in the two data points (other
reasonable deﬁnitions of the distance metric worked equally
well). Larger the distance, the larger is the deviation from the
normal behavior. If the distance measure for the test interval’s
data points is higher by a threshold compared to the distances
of the model data points, an event (alarm) is generated (for
further analysis by the Problem Determination Engine). Note
that, unlike HMM, kNN does not require any training and can
learn the model of normal behavior in an online fashion, which
allows it to adapt to changes in workload mix or intensity.
TABLE I: System and application metrics monitored by
CloudPD; Operating context metrics are marked with *.
System
Metrics
cpu-user
cpu-system
memused
miss/s
ctxt
eth-rxbyt
eth-txbyt
pgpgin
pgpgout
fault
system-load
Application
Metrics
Latency
Throughput
Description
% CPU time in user-space
% CPU time in kernel-space
% memory used
number of cache misses per second
number of context switches per second
network bytes received per second
network bytes transmitted per second
KBytes paged-in from disk per second
KBytes paged-out from disk per second
number of page faults per second
processor’s process queue length
Description
Measurement