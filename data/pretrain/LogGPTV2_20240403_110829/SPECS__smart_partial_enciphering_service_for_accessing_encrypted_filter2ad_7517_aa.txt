title:SPECS: smart partial enciphering service for accessing encrypted
files with efficient and transparent
author:Tae-Kyou Park and
Ilkyeun Ra
Machine Learning Classiﬁcation over Encrypted Data
Shaﬁ Goldwasser‡
Raphaël Bost∗
Raluca Ada Popa†‡
Stephen Tu‡
Abstract
Machine learning classiﬁcation is used in numerous settings nowadays, such as medical or genomics predictions,
spam detection, face recognition, and ﬁnancial predictions. Due to privacy concerns, in some of these applications, it is
important that the data and the classiﬁer remain conﬁdential.
In this work, we construct three major classiﬁcation protocols that satisfy this privacy constraint: hyperplane
decision, Naïve Bayes, and decision trees. We also enable these protocols to be combined with AdaBoost. At the basis
of these constructions is a new library of building blocks for constructing classiﬁers securely; we demonstrate that this
library can be used to construct other classiﬁers as well, such as a multiplexer and a face detection classiﬁer.
We implemented and evaluated our library and classiﬁers. Our protocols are efﬁcient, taking milliseconds to a few
seconds to perform a classiﬁcation when running on real medical datasets.
1 Introduction
Classiﬁers are an invaluable tool for many tasks today, such as medical or genomics predictions, spam detection, face
recognition, and ﬁnance. Many of these applications handle sensitive data [WGH12, SG11, SG13], so it is important
that the data and the classiﬁer remain private.
Consider the typical setup of supervised learning, depicted in Figure 1. Supervised learning algorithms consist of
two phases: (i) the training phase during which the algorithm learns a model w from a data set of labeled examples,
and (ii) the classiﬁcation phase that runs a classiﬁer C over a previously unseen feature vector x, using the model w to
output a prediction C(x, w).
In applications that handle sensitive data, it is important that the feature vector x and the model w remain secret to
one or some of the parties involved. Consider the example of a medical study or a hospital having a model built out of
the private medical proﬁles of some patients; the model is sensitive because it can leak information about the patients,
and its usage has to be HIPAA1 compliant. A client wants to use the model to make a prediction about her health (e.g.,
if she is likely to contract a certain disease, or if she would be treated successfully at the hospital), but does not want
to reveal her sensitive medical proﬁle. Ideally, the hospital and the client run a protocol at the end of which the client
learns one bit (“yes/no”), and neither party learns anything else about the other party’s input. A similar setting arises for
a ﬁnancial institution (e.g., an insurance company) holding a sensitive model, and a customer wanting to estimate rates
or quality of service based on her personal information.
Throughout this paper, we refer to this goal shortly as privacy-preserving classiﬁcation. Concretely, a client has a
private input represented as a feature vector x, and the server has a private input consisting of a private model w. The way
the model w is obtained is independent of our protocols here. For example, the server could have computed the model
w after running the training phase on plaintext data as usual. Only the classiﬁcation needs to be privacy-preserving: the
client should learn C(x, w) but nothing else about the model w, while the server should not learn anything about the
client’s input or the classiﬁcation result.
∗Direction Générale de l’Armement - Maitrise de l’Information. Work done while visiting MIT CSAIL. The views and conclusions contained
herein are those of the author and should not be interpreted as necessarily representing the ofﬁcial policies or endorsements, either expressed or
implied, of the DGA or the French Government.
†ETH Zürich
‡MIT CSAIL
1Health Insurance Portability and Accountability Act of 1996
1
Figure 1: Model overview. Each shaded box indicates private data that should be accessible to only one party: the
dataset and the model to the server, and the input and prediction result to the client. Each straight non-dashed rectangle
indicates an algorithm, single arrows indicate inputs to these algorithms, and double arrows indicate outputs.
Machine learning algorithm Classiﬁer
Perceptron
Least squares
Fischer linear discriminant
Support vector machine
Naive Bayes
Decision trees (ID3/C4.5)
Hyperplane decision
Hyperplane decision
Hyperplane decision
Hyperplane decision
Naïve Bayes
Decision trees
Table 1: Machine learning algorithms and their classiﬁers, deﬁned in Section 3.1.
In this work, we construct efﬁcient privacy-preserving protocols for three of the most common classiﬁers: hyperplane
decision, Naïve Bayes, and decision trees, as well as a more general classiﬁer combining these using AdaBoost. These
classiﬁers are widely used – even though there are many machine learning algorithms, most of them end up using one
of these three classiﬁers, as described in Table 1.
While generic secure multi-party computation [Yao82, GMW87, HKS+10, MNPS04, BDNP08] can implement
any classiﬁer in principle, due to their generality, such schemes are not efﬁcient for common classiﬁers. As described in
Section 10.5, on a small classiﬁcation instance, such tools ([HKS+10, BDNP08]) ran out of memory on a powerful
machine with 256GB of RAM; also, on an artiﬁcially simpliﬁed classiﬁcation instance, these protocols ran ≈ 500 times
slower than our protocols ran on the non-simpliﬁed instance.
Hence, protocols specialized to the classiﬁcation problem promise better performance. However, most existing work
in machine learning and privacy [LP00, DHC04, WY04, ZW05, BDMN05, VKC08, GLN12] focuses on preserving
privacy during the training phase, and does not address classiﬁcation. The few works on privacy-preserving classiﬁcation
either consider a weaker security setting in which the client learns the model [BLN13] or focus on speciﬁc classiﬁers
(e.g., face detectors [EFG+09, SSW09, AB06, AB07]) that are useful in limited situations.
Designing efﬁcient privacy-preserving classiﬁcation faces two main challenges. The ﬁrst is that the computation
performed over sensitive data by some classiﬁers is quite complex (e.g., decision trees), making it hard to support
efﬁciently. The second is providing a solution that is more generic than the three classiﬁers: constructing a separate
solution for each classiﬁer does not provide insight into how to combine these classiﬁers or how to construct other
classiﬁers. Even though we contribute privacy-preserving protocols for three of the most common classiﬁers, various
settings use other classiﬁers or use a combination of these three classiﬁers (e.g., AdaBoost). We address these challenges
using two key techniques.
Our main technique is to identify a set of core operations over encrypted data that underlie many classiﬁcation
protocols. We found these operations to be comparison, argmax, and dot product. We use efﬁcient protocols for each
one of these, either by improving existing schemes (e.g., for comparison) or by constructing new schemes (e.g., for
argmax).
Our second technique is to design these building blocks in a composable way, with regard to both functionality and
security. To achieve this goal, we use a set of sub-techniques:
• The input and output of all our building blocks are data encrypted with additively homomorphic encryption. In
addition, we provide a mechanism to switch from one encryption scheme to another. Intuitively, this enables a
building block’s output to become the input of another building block;
• The API of these building blocks is ﬂexible: even though each building block computes a ﬁxed function, it allows
2
server data set training phase model w classification C client feature vector x prediction C(w,x) a choice of which party provides the inputs to the protocol, which party obtains the output of the computation,
and whether the output is encrypted or decrypted;
• The security of these protocols composes using modular sequential composition [Can98].
We emphasize that the contribution of our building blocks library goes beyond the classiﬁers we build in this paper:
a user of the library can construct other privacy-preserving classiﬁers in a modular fashion. To demonstrate this point,
we use our building blocks to construct a multiplexer and a classiﬁer for face detection, as well as to combine our
classiﬁers using AdaBoost.
We then use these building blocks to construct novel privacy-preserving protocols for three common classiﬁers.
Some of these classiﬁers incorporate additional techniques, such as an efﬁcient evaluation of a decision tree with fully
homomorphic encryption (FHE) based on a polynomial representation requiring only a small number of multiplications
and based on SIMD FHE slots (see Section 7.2). All of our protocols are secure against passive adversaries (see
Section 3.2.3).
We also provide an implementation and an evaluation of our building blocks and classiﬁers. We evaluate our
classiﬁers on real datasets with private data about breast cancer, credit card approval, audiology, and nursery data; our
algorithms are efﬁcient, running in milliseconds up to a few seconds, and consume a modest amount of bandwidth.
The rest of the paper is organized as follows. Section 2 describes related work, Section 3 provide the necessary
machine learning and cryptographic background, Section 4 presents our building blocks, Sections 5–8 describe our
classiﬁers, and Sections 9–10 present our implementation and evaluation results.
2 Related work
Our work is the ﬁrst to provide efﬁcient privacy-preserving protocols for a broad class of classiﬁers.
Secure two-party computation protocols for generic functions exist in theory [Yao82, GMW87, LP07, IPS08, LP09]
and in practice [HKS+10, MNPS04, BDNP08]. However, these rely on heavy cryptographic machinery, and applying
them directly to our problem setting would be too inefﬁcient as exempliﬁed in Section 10.5.
Previous work focusing on privacy-preserving machine learning can be broadly divided into two categories: (i)
techniques for privacy-preserving training, and (ii) techniques for privacy-preserving classiﬁcation (recall the distinction
from Figure 1). Most existing work falls in the ﬁrst category, which we discuss in Section 2.1. Our work falls in the
second category, where little work has been done, as we discuss in Section 2.2. We also mention work related to the
building blocks we use in our protocols in Section 2.3.
It is worth mentioning that our work on privacy-preserving classiﬁcation is complementary to work on differential
privacy in the machine learning community (see e.g. [CMS11]). Our work aims to hide each user’s input data to the
classiﬁcation phase, whereas differential privacy seeks to construct classiﬁers/models from sensitive user training data
that leak a bounded amount of information about each individual in the training data set.
2.1 Privacy-preserving training
A set of techniques have been developed for privacy-preserving training algorithms such as Naïve Bayes [VKC08,
WY04, ZW05], decision trees [BDMN05, LP00], linear discriminant classiﬁers [DHC04], and more general kernel
methods [LLM06].
Grapel et al. [GLN12] show how to train several machine learning classiﬁers using a somewhat homomorphic
encryption scheme. They focus on a few simple classiﬁers (e.g. the linear means classiﬁer), and do not elaborate on more
complex algorithms such as support vector machines. They also support private classiﬁcation, but in a weaker security
model where the client learns more about the model than just the ﬁnal sign of the classiﬁcation. Indeed, performing
the ﬁnal comparison with fully homomorphic encryption (FHE) alone is inefﬁcient, a difﬁculty we overcome with an
interactive setting.
3
2.2 Privacy-preserving classiﬁcation
Little work has been done to address the general problem of privacy-preserving classiﬁcation in practice; previous work
focuses on a weaker security setting (in which the client learns the model) and/or only supports speciﬁc classiﬁers.
In Bos et al. [BLN13], a third party can compute medical prediction functions over the encrypted data of a patient
using fully homomorphic encryption. In their setting, everyone (including the patient) knows the predictive model, and
their algorithm hides only the input of the patient from the cloud. Our protocols, on the other hand, also hide the model
from the patient. Their algorithms cannot be applied to our setting because they leak more information than just the bit
of the prediction to the patient. Furthermore, our techniques are notably different; using FHE directly for our classiﬁers
would result in signiﬁcant overheads.
Barni et al. [BFK+09, BFL+09] construct secure evaluation of linear branching programs, which they use to
implement a secure classiﬁer of ECG signals. Their technique is based on ﬁnely-tuned garbled circuits. By comparison,
our construction is not limited to branching programs (or decision trees), and our evaluation shows that our construction
is twice as fast on branching programs. In a subsequent work [BFL+11], Barni et al. study secure classiﬁers based on
neural networks, which is a generalization of the perceptron classiﬁers, and hence also covered by our work.
Other works [EFG+09, SSW09, AB06, AB07] construct speciﬁc face recognition or detection classiﬁers. We focus
on providing a set of generic classiﬁers and building blocks to construct more complex classiﬁers. In Section 10.1.2, we
show how to construct a private face detection classiﬁer using the modularity of our techniques.
2.3 Work related to our building blocks
Two of the basic components we use are private comparison and private computation of dot products. These items have
been well-studied previously; see [Yao82, DGK07, DGK09, Veu11, LT05, AB06, KSS09] for comparison techniques
and [AD01, GLLM04, Kil05, AB06] for techniques to compute dot products. Section 4.1 discusses how we build on
these tools.
3 Background and preliminaries
3.1 Classiﬁcation in machine learning algorithms
The user’s input x is a vector of d elements x = (x1, . . . , xd) ∈ Rd, called a feature vector. To classify the input x means
to evaluate a classiﬁcation function Cw : Rd (cid:55)→ {c1, ..., ck} on x. The output is ck∗ = Cw(x), where k∗
∈ {1 . . . k};
ck∗ is the class to which x corresponds, based on the model w. For ease of notation, we often write k∗ instead of ck∗,
namely k∗ = Cw(x).
We now describe how three popular classiﬁers work on regular, unencrypted data. These classiﬁers differ in the
model w and the function Cw. For more details, we refer the reader to [BN06].
Hyperplane decision-based classiﬁers. For this classiﬁer, the model w consists of k vectors in Rd (w = {wi}k
The classiﬁer is (cf. [BN06]):
i=1).
(1)
∗
k
= argmax
i∈[k] (cid:104)wi, x(cid:105),
where (cid:104)wi, x(cid:105) denotes inner product between wi and x.
We now explain how Eq. (1) captures many common machine learning algorithms. A hyperplane based classiﬁer
typically works with a hypothesis space H equipped with an inner product (cid:104)·,·(cid:105). This classiﬁer usually solves a binary
classiﬁcation problem (k = 2): given a user input x, x is classiﬁed in class c2 if (cid:104)w, φ(x)(cid:105) ≥ 0, otherwise it is labeled
as part of class c1. Here, φ : Rd (cid:55)→ H denotes the feature mapping from Rd to H [BN06]. In this work, we focus on
the case when H = Rd and note that a large class of inﬁnite dimensional spaces can be approximated with a ﬁnite
dimensional space (as in [RR07]), including the popular gaussian kernel (RBF). In this case, φ(x) = x or φ(x) = P x
for a randomized projection matrix P chosen during training. Notice that P x consists solely of inner products; we will
show how to support private evaluation of inner products later, so for simplicity we drop P from the discussion. To
extend such a classiﬁer from 2 classes to k classes, we use one of the most common approaches, one-versus-all, where
k different models {wi}k
i=1 are trained to discriminate each class from all the others. The decision rule is then given by
4
Figure 2: Decision tree
(cf. [BN06]) to be Eq. (1). This framework is general enough to cover many common algorithms, such as support vector
machines (SVMs), logistic regression, and least squares.
Naïve Bayes classiﬁers. For this classiﬁer, the model w consists of various probabilities: the probability that each
class ci occurs, namely {p(C = ci)}k
i=1, and the probabilities that an element xj of x occurs in a certain class ci. More
concretely, the latter is the probability of the j-th component xj of x to be v when x belongs to category ci; this is
i=1, where Dj is Xj’s domain2. The classiﬁcation function, using a
denoted by {{{p(Xj = v|C = ci)}v∈Dj}d
maximum a posteriori decision rule, works by choosing the class with the highest posterior probability:
j=1}k
∗
k
= argmax
i∈[k]
= argmax
i∈[k]
= argmax
i∈[k]
p(C = ci|X = x)
p(C = ci, X = x)
p(C = ci, X1 = x1, . . . , Xd = xd)
where the second equality follows from applying Bayes’ rule (we omitted the normalizing factor p(X = x) because it
is the same for a ﬁxed x).
The Naïve Bayes model assumes that p(C = ci, X = x) has the following factorization:
p(C = ci, X1 = x1, . . . , Xd = xd)
d(cid:89)
j=1
= p(C = ci)