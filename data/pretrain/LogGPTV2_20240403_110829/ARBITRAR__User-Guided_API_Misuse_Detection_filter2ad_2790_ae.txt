png_destroy_read_struct
png_destroy_write_struct Memory leak
png_create_info_struct
png_crc_read
png_malloc
png_calloc
cdev_init not called
Return value not stored
Return value not checked
Return value not checked
Return value not checked
Return value not checked
Return value not checked
Double free
Argument not checked
Argument not checked
Return value not checked
Return value not checked
Return value not checked
Return value not checked
Return value not checked
Return value not checked
Argument not memset-ted
Uninitialized read
Memory Leak
Code Smell
Code Smell
System Crash
Code Smell
System Crash
Memory corruption
Memory corruption
Memory corruption
Memory corruption
System Crash
System Crash
System Crash
System Crash
System Crash
System Crash
Uninitialized read
libbluetooth
sdp_get_proto_port
hci_get_route
sdp_data_get
hci_devba
hci_send_req
Total
D. Scalability
pr_err("Could not allocate cdev for minor %d, %s\n",
Total
Bugs
1
3
1 (12*)
1 (58*)
1 (41*)
1 (118*)
5
2
1
3
1
7
1
1
2
4
4
1
40
First Bug
Warning
3
5
10
1
2
2
3
2
4
2
1
1
1
1
3
1
2
3
Avg: 2.6
APISAN
Warnings
445
143
474
1,022
2,084
}
minor, name);
ret = -ENOMEM;
goto done;
cdev = cdev_alloc();
if (!cdev) {
1
2
3
4
5
6
7
8
9
10
11
12
13
14 (cid:13)ret = cdev_add(cdev, dev, 1);
cdev->owner = THIS_MODULE;
cdev->ops = fops;
kobject_set_name(&cdev->kobj, name);
missing call to cdev_init
Listing 4: Missing initialization call bug found by ARBITRAR
(in 3rd interaction) in Linux Kernel: Here, there is a missing
call to cdev_init before cdev_add.
• Small number of call-sites of the API method: APISAN
requires a relatively large number of API call-sites to learn
semantic beliefs and consequently ﬁnd misuses. But the
above APIs have a relatively small number (10-100) of oc-
currences. However, using active learning allows ARBITRAR
to overcome this limitation.
• Complex API Semantics: The semantics of certain APIs
functions is complex and cannot be handled by the different
checkers of APISAN. However, ARBITRAR can capture
these by using a large set of semantic features. Listing 4
shows a missing initialization bug found by ARBITRAR.
This bug requires understanding the causal relationship
between cdev_add and cdev_init, which APISAN failed
to infer.
As mentioned in Section IV, execution of ARBITRAR occurs
in two independent steps, i.e., Trace generation (or Analysis
setup) and Active learning.
1) Trace generation: We use a multi-threaded implementa-
tion for our trace generation and feature extraction process to
achieve scalability. It takes ∼66 milliseconds to generate a trace
and for a fairly used API in Linux Kernel there are usually 500-
800 traces. Hence for an API, the trace generation and feature
extraction usually ﬁnish within a couple of minutes. Even for
a heavily used API such as kzalloc with 1,937 occurrences
and 31,704 traces, the generation and feature extraction take
only ∼35 minutes.
All the generated traces are stored in a database to be
inspected later using our active learning technique. Note that,
for a given API method and program, trace generation is a
one-time task.
2) Active learning: Irrespective of the number of traces, our
active learning technique is responsive with a response time of
milliseconds, i.e., after the user provides the feedback it takes
only milliseconds to learn and present the next most probable
bug trace This is because of our fast update mechanism using
MD-KDE. The traditional update mechanisms such as KDE
and KDE with caching have very high response time and are
not scalable to be used in a user-driven active learning setting
especially when we have a large amount of traces. Figure 9
shows the response time of our MD-KDE compared to the
existing techniques. As shown, the response time of the na¨ıve-ly
using the KDE model increases drastically with the number of
traces. However, our update mechanism, i.e., MD-KDE is only
experiencing linear growth. In reality, it stays in a relatively
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:25:07 UTC from IEEE Xplore.  Restrictions apply. 
1410
2
1.5
1
0.5
s
d
n
o
c
e
s
n
i
e
m
i
t
e
s
n
o
p
s
e
R
A: Retrain KDE
B: With Caching
MD-KDE (ARBITRAR)
0
0
200
400
600
800 1,000 1,200 1,400
Number of traces
Fig. 9: The response time of the update used by ARBITRAR
compared with Na¨ıve approaches. A: Train a brand new KDE
model in each iteration; B: Caches intermediate result but
recalculate score for all points in each iteration; MD-KDE:
Performs only O(|U|) updates in each iteration.
constant runtime, enabling the user to interact with the system
in real-time.
Separating our trace generation and active learning en-
ables ARBITRAR to be run in a two-phase mode. The trace
generation (automated but time consuming task) can be run
overnight with all the traces stored in a database. The user
can analyze these traces later using our fast active-learning
technique. This two-phase approach is indeed what developers
expect to see from a program analysis tool [12].
E. Extensibility
To demonstrate the extensibility and ﬂexibility of our MD-
KDE learning framework, we conduct an experiment to answer
the following question: With one known bug, are we able to
utilize ARBITRAR to ﬁnd the remaining bugs faster?
To handle this situation, we extend our tool to allow the
user to provide the labeled known bug at the beginning. The
other parts of our system remain the same, and the user will
interact with our system as usual. To perform the experiment,
we pick a few APIs from our previous experiments that satisfy
the following criteria: (a). there are multiple bugs, and (b).
there is room for improvement. With each API, we randomly
select one bug to be the known labeled bug.
We want to measure the improvements in terms of (1). the
number of iterations needed before we hit the ﬁrst true bug
(xf ), and (2). the number of iterations needed to cover all the
remaining bugs (xl). Note that the selected sample bug will
not be counted anymore, therefore the number of bugs y(cid:48) in
the with-sample-bug setting will be off-by-1 from the original
number of bugs y.
Table V shows 4 APIs we choose, the number of bugs y, and
our measurement xl and xf in both the original setup and the
with-sample-bug setup. We can see a clear improvement when
one bug label is provided. Therefore, when a user already
possesses one bug pattern, they can leverage this and use
ARBITRAR to ﬁnd bugs even faster and with higher precision.
F. Discussion on false positives
We want to emphasize that our notion of false positive rate is
fundamentally different from traditional bug ﬁnding tools. First,
ARBITRAR does not assume any pattern about API usage, and
has to learn from the user feedback from the ground up. The
false positives (or non-buggy traces) before the ﬁrst bug serve
as feedback for ARBITRAR to learn a model for the API misuse
detection. Second, traditional bug ﬁnding tools cannot learn
from user feedback. As such, the false positive rate is ﬁxed.
However, with ARBITRAR, as shown in Section V-E, each
false positive marked by the user contributes to improving
the accuracy of the model and consequently decreases the
false-positive rate.
VI. LIMITATIONS
Although ARBITRAR is an effective API misuse detection
tool, it has the following limitations:
• Requirement of API method: Unlike APISAN, ARBITRAR
is API method-speciﬁc and requires the method name to be
provided. We provide a way [38] to enumerate the number
of call sites of an API method, which could be used to select
API methods with large occurrences. However, to be more
precise, a simple statistical analysis [32] can be made on the
codebase to determine the methods of interest, which can
be then provided to ARBITRAR.
• Incomplete trace generation: Similar to APISAN, we only
consider direct call sites to the API method. Consequently,
our trace generation technique may miss traces of the API
method invoked through function pointers. However, we
can use a points-to analysis [56] to resolve function pointer
targets and consider them during trace generation.
• Discrete user feedback: Our active learning algorithm allows
only one bit of user feedback, i.e., Yes or No. However,
in practice, the user may want to provide other kinds of
feedback, e.g., Yes with 80% conﬁdence. Our current design
does not allow for such feedback. However, techniques like
ALPF [30] could be employed to handle such feedback.
• Sensitivity to user feedback: Our active learning algorithm
trusts all user feedback. However, a user might make a
mistake. In our experience, when given wrong feedback (i.e.,
marked a correct usage to be incorrect) for malloc, our
model was able to correct itself after a few correct answers.
However, this may not generalize to all APIs as the model
depends on the complexity of the API. An extensive study
is required to precisely assess the sensitivity of our model
to user feedback.
VII. FUTURE WORK
In our future work, we plan to extend ARBITRAR in the
following directions:
• Model interpretation and transfer learning: We believe
that models learned by our algorithm are interpretable and
transferable, i.e., a model built for malloc could work for
other allocator functions such as realloc and calloc. We
plan to explore this by testing a pre-trained model of an
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:25:07 UTC from IEEE Xplore.  Restrictions apply. 
1411
TABLE V: Compare of our original setup vs. with example bug setup.
API
Dataset
kmalloc
kthread_run
BN_CTX_get
i2c_add_adapter
APISAN
APISAN
APIMU4C OpenSSL
Ours (Kernel 5.7)
Original
y
2
2
3
11
xf
12
9
13
10
xl
14
10
15
24
With Sample Bug
y(cid:48)
x(cid:48)
l
6
1
1
1
2
2
10
15
x(cid:48)
f
6
1
1
3
API method on other related API methods. We also plan
to generate a description for each trace explaining which
features contributed to selecting the trace.
• Prioritizing high severity bugs:
It is important to prioritize
high severity API misuse bugs. For instance, API misuse
bugs that cause memory leaks and code smells are arguably
less important than those that cause memory corruption. It
is important to prioritize high severity bugs. To handle this,
we plan to allow the user to provide bug type feedback as
well (e.g., memory leak, memory corruption, etc.). We can
then use this feedback to train a bug type detection model
that can be used to prioritize traces indicating critical bugs.
VIII. RELATED WORK
The goal of bug-ﬁnding techniques is to detect violations of
a certain set of rules, e.g., memory corruption, information ﬂow
from an untrusted source to a sensitive sink, integer overﬂow,
etc. Static tools for ﬁnding such violations are based on model
checking [5], data-ﬂow analysis [62], [41], type inference [54],
[9], or symbolic execution [8]. The rules that can be checked
by a particular tool can be either ﬁxed [41], conﬁgured via