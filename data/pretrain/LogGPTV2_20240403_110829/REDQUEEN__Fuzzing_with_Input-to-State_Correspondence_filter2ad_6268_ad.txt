M by other publications and then compare them with our
results, a summary is shown in Table II.
In the original paper of LAVA-M, the authors perform
two experiments: one using an unspeciﬁed FUZZER and an
unspeciﬁed symbolic execution tool SES (both are undisclosed
“state-of-the-art, high-proﬁle tools” [17]). Both tools were run
against the corpus for 5 hours on an unspeciﬁed machine. All
experiments performed by the STEELIX authors were run on
a machine with 8 Intel(R) Xeon(R) CPU E5-1650 v3 cores
and 8 GB of RAM, running 32-bit Ubuntu 16.04 using one
thread. Similarly, Sanjay et al. performed their experiments
with VUZZER [35] using a 5-hour run on an unspeciﬁed
machine with 4 GB of RAM on a single core. We performed
a similar experiment where each target was executed for 5
hours. We used the ﬁrst few bytes of the seed provided by
the original authors as well as our usual uninformed seeds.
Figure 1 displays the results (5 runs of 5 hours each, median
and the 60-percent conﬁdence interval of the number of bugs
found over time, in percent of the number of bugs intended by
the authors of LAVA-M). In all cases, we found more bugs than
the original authors of LAVA-M intended. Also, in all cases,
we needed much less than the full 5 hours to ﬁnd all bugs. In
the median case, it took less than 5 minutes to ﬁnd more bugs
on who and base64. After 15 minutes, uniq was fully solved.
Lastly, md5sum was the slowest target, taking 25 minutes. The
reason why md5sum was slower than the other targets is that we
were using a busybox environment in which all ﬁles in /bin/
link to the same monolithic binary, signiﬁcantly slowing down
the md5 calculation.
We found all bugs inserted and listed, except for two bugs
in who. More importantly, we found a signiﬁcant number of
bugs (337) considered unreachable by the original authors. The
only other fuzzer that managed to ﬁnd some of these unlisted
bugs was ANGORA. However, we managed to ﬁnd over 50%
more bugs than ANGORA and more than three times as many
unlisted bugs. Due to the high number of unlisted bugs, we
contacted one of the authors of LAVA-M who conﬁrmed our
ﬁndings from the inputs we provided.
From the fact that the difference between informed seed
inputs and uninformed seed inputs is marginal and the fact
that we outperformed the other current state-of-the-art tools by
factors ranging from 8x to 26x, we conclude that the LAVA-M
data set favors the hypothesis that our approach outperforms
state-of-the-art approaches and is able to uncover deep bugs far
from the provided seed inputs. In fact, we are outperforming
all current approaches, even if no seed is given to our tool.
However, it should be noted that the bugs inserted into the
LAVA-M data set are artiﬁcial and do not represent real-world
vulnerabilities.
C. Cyber Grand Challenge
The targets used in DARPA’s CGC are another widely used
test set to evaluate fuzzers. Similarly to the LAVA-M corpus,
we will now describe the experimental setups used by various
other fuzzers on these targets. Then, we will compare their
results with our results.
STEELIX only tested a subset of eight CGC binaries and
compared their results against AFL-DYNINST, an AFL version
that uses dynamic binary instrumentation and is signiﬁcantly
slower than the original AFL. Both tools were allowed to fuzz
for three hours. STEELIX was able to ﬁnd one crash that AFL
could not ﬁnd. In contrast, we were able to ﬁnd the crash in
less than 30 seconds with the seed used by STEELIX.
9
TABLE II: Listed and (+unlisted bugs) found after 5 hours of fuzzing on LAVA-M (numbers taken from the corresponding papers).
Program Listed Bugs
uniq
base64
md5sum
who
28
44
57
2136
FUZZER
7
7
2
0
SES
0
9
0
18
VUZZER
27
17
-
50
STEELIX
7
43
28
194
T-FUZZ
26
43
49
63
ANGORA
28
44
57
1443
(+ 1)
(+ 4)
(+ 0)
(+ 98)
REDQUEEN
28
44
57
2134
(+ 1)
(+ 4)
(+ 4)
(+ 328)
the VUZZER authors manually ensured that VUZZER always
appends the correct “quit” command.
We run a similar set of experiments with a few differences:
In contrast to the authors of VUZZER, we used the multi-OS
version released by Trail of Bits [3]. Therefore, some of the
comparisons might be biased due to slight differences. Under
these conditions, we were able to trigger crashes in 31 binaries
on the ﬁrst try with REDQUEEN, while VUZZER was able to
trigger 25 crashes.
Since these results were produced without adapting
REDQUEEN to CGC at all, this experiment serves as a validation
set to ensure we did not overﬁt our fuzzing process to any spe-
ciﬁc target. Since the AFL fuzzing model is primarily aiming at
binary input formats—CGC contains a large number of interac-
tive line base formats—we performed another experiment: we
added some mutations relevant for text-based protocols (mostly
splicing lines as well as adding random lines). We also used
4 cores each for 2 hours instead of the usual conﬁguration
to speed up experiments. In this experiment, we were able to
uncover 9 additional crashing inputs. In total, we were able
to crash 40 out of the 54 targets. VUZZER was able to ﬁnd
crashes in 5 binaries which we could not crash. In at least one
case this was due to heavy timeouts, which VUZZER avoided
by adapting their fuzzer to the speciﬁc targets as described
earlier (which we did not). On the other hand, we crashed 19
binaries that VUZZER was unable to crash. This suggests that
we are able to outperform both VUZZER by 60% as well as
AFL-PIN by 73% on this data set.
Unfortunately, the authors of T-FUZZ did not disclose the
full results of their CGC experiments. However, they state that,
on the subset of binaries that VUZZER used, they found 29
crashes in the 6 hours allocated—signiﬁcantly less then the 40
crashes that our approach managed to ﬁnd.
DRILLER was evaluated on the whole set of CGC binaries.
The experiment used a cluster of unspeciﬁed AMD64 CPUs with
four cores per target. The concolic execution processing was
outsourced to another set of 64 concolic execution nodes. Each
concolic execution was limited to 4 GB of memory and every
target was fuzzed for 24 hours. Fuzzing with AFL was able to
uncover crashes in 68 targets. DRILLER was able to increase
this number to 77. It should be noted that, as a participant in
the CGC competition, DRILLER is highly optimized to the CGC
dataset. Yet, the improvements over baseline fuzzing with AFL
were only in the order of 14 %. Our results on the dataset of
VUZZER compared to AFL-PIN (73%) suggest that we might
be able to see improvements similar to DRILLER if we were
to implement the various DECREE speciﬁc mechanisms such as
the custom IPC.
Fig. 1: Evaluating LAVA-M on informed and uninformed seeds.
VUZZER tested a subset of 63 binaries, ﬁltering binaries
that use ﬂoating-point
instructions (which VUZZER’s taint
tracking cannot handle), IPC, or binaries that are easily put into
an inﬁnite loop. The authors kindly provided us with the exact
set of targets they used and the set of targets in which they were
able to uncover crashes. Unfortunately, they were not able to
provide us with the exact seeds used. We re-created a similar
set of seeds with the same method. Some inputs were generated
by the poller scripts used in CGC and some inputs were taken
from the original DARPA repository. However, we were unable
to create seeds for nine of the challenges. Consequently, we
excluded them from our evaluation. VUZZER was able to ﬁnd
crashes in two of these excluded binaries. The experiments
with VUZZER were performed on a host with two unspeciﬁed
32-bit Intel CPUs and 4 GB of memory inside the original
DECREE VM. Both VUZZER as well as AFL-PIN were given 6
hours of fuzzing time. They were able to ﬁnd 29 crashes in
27 binaries. In this setup, AFL-PIN was able to uncover 23
crashes. A signiﬁcant number of the target binaries runs in a
busy loop waiting for a speciﬁc “quit” command. This leads
to a large number of timeouts, greatly reducing the fuzzing
performance. While such a loop would be easy to detect in
a limited setting such as CGC, this optimization would be out
of scope for a more general fuzzer. To mitigate this problem,
10
00:0002:3005:000%20%40%60%80%100%120%base64uninformedshortseeds00:0002:3005:000%20%40%60%80%100%120%who00:0002:3005:000%20%40%60%80%100%120%uniq00:0002:3005:000%20%40%60%80%100%120%md5sumtime(hh:mm)#bugsfoundD. Real-World Applications
We also evaluated REDQUEEN on several real-world ap-
plications. Generally speaking, we found all
the types of
bugs fuzzers usually ﬁnd: out-of-bound read/write accesses
to memory, resource exhaustion (both, time and memory),
memory leaks, stack overﬂows, division by zeros, assertions,
use-after-free, use of uninitialized values, and so on. For the
purpose of this evaluation, we disregarded the following bug
classes, as they were far too numerous for manual triage and of
little importance for security research: memory leaks, resource
exhaustion, use of uninitialized values (unless they lead to
more severe consequences) and stack overﬂows.
binutils. To estimate the ability to uncover deep bugs in hard-
to-reach code, we measured the coverage produced by various
tools on real-world binaries from the binutils collection
to expand our evaluation from synthetic test cases to more
realistic tests. We use code coverage as a proxy metric for
the ability to uncover deep bugs, as the number of bugs
found is very hard to establish. To properly compare the
number of bugs found across tools, many thousands of crashes
would have to be investigated. In many cases, most tools were
unable to uncover a single crash, leading to non-descriptive
experiments. We regard code coverage as a very good proxy,
as no fuzzer can ﬁnd bugs in code that is not covered. As
test suite, we picked all eight programs from the binutils
collection, which are processing one ﬁle without modifying it.
Unfortunately, we cannot evaluate against DRILLER since it is
only applicable to DECREE binaries. Also, since both ANGORA
and T-FUZZ are not yet available, we cannot run our own
experiments to compare against them. We, therefore, choose
VUZZER as one of the few available academic state-of-the-art
fuzzers, as well as AFL with the LAF-INTEL and AFLFAST
extensions. Notably, both LAF-INTEL and VUZZER explicitly
aim to overcome magic bytes and other fuzzing roadblocks,
similar to our tool. In all cases, we started with a single
uniformed seed to measure the ability to solve roadblocks.
KLEE does not produce test ﬁles as it uncovers new states.
Instead, it only produces a test case after the state has been
fully explored or after the timeout has been reached. Therefore,
it is not uncommon that KLEE writes only a very small number
of test cases while running. After reaching the timeout, it starts
to solve all remaining states to produce more coverage, and, in
some cases, it can spend many hours of additional computation
time to produce the actual test cases. We, therefore, gave KLEE
a 7 hour window to evaluate symbolic states plus 3 hours for
test case production. We forcefully terminated KLEE after a
total of 10 hours. The same architecture makes it very hard
to determine the exact time when a new input was found.
For plotting purposes, we assumed that inputs are found at
a constant rate. This assumption is obviously wrong, as we
would expect far more inputs to be found during the early
hours. However, since we primarily compare the ﬁnal number
of basic blocks covered, the exact shape of the curve in the
plot is not overly relevant. VUZZER is unable to fuzz targets
that expect their input on stdin, and, therefore, we do not
have VUZZER results for the cxxfilt target. To represent
other Intel PT based fuzzers, we included HONGGFUZZ [6]
running in Intel PT mode and the original KAFL with our
Ring 3 extension.
The results of ﬁve 10 hour runs can be seen in Figure 2.
Fig. 2: The coverage (in basic blocks) produced by various tools over 5 runs at
10 h each on the binutils. Displayed are the median and the 60 % intervals.
Notably, in all cases, REDQUEEN is able to trigger the most
coverage. To ensure these ﬁndings are not the result of random
variation, we employed a Mann-Whitney U test on the number
of basic blocks found, as recommended by Arcuri et al. [7] for
evaluation of randomized algorithms. The results are displayed
in Table III. It can be seen that, in nearly all cases, the observed
differences are signiﬁcant at p < 0.05. Interestingly, VUZZER
ﬁnds very few new basic blocks. We investigated this behavior
to ensure we are not misusing VUZZER. We contacted the
authors, who reproduced our experiments and found very
similar results. In the resulting conversation, it became clear
that VUZZER strongly relies on the assumption that there is
only one valid input format. Since most binutils programs
11
00:0005:0010:000200400600800arlaf-intelAFLFastRedqueenkAFLKleeHonggFuzzVuzzer00:0005:0010:00050010001500200025003000size00:0005:0010:0005001000150020002500cxxﬁlt00:0005:0010:0005001000150020002500strings00:0005:0010:00010002000300040005000nm-new00:0005:0010:000200040006000800010000objdump00:0005:0010:0002000400060008000readelf00:0005:0010:000200040006000as-newTime(hh:mm)#BBsfoundTABLE III: p-values of the Mann-Whitney U test on the number of basic
blocks found in 10h. Nearly all results are statistically signiﬁcant (p < 0.05).
Target
ar
cxxﬁlt
nm-new
readelf
size
strings
objdump
as-new
LAF-INTEL
0.004
0.014
0.018
0.006
0.006
0.006
0.265
0.006
AFLFAST
0.004
0.006
0.006
0.006
0.006
0.006
0.006
0.500
KAFL
0.004
0.006
0.006
0.006
0.006
0.006
0.018
0.006
KLEE
0.005
0.006
0.004
0.006
0.006
0.006
0.006
0.006
HONGG
0.004
-
0.006
0.005
0.006
0.006
0.006
0.006
VUZZER
0.004
-
0.006
0.006
0.006
0.004
0.006
0.006
read a vast amount of different formats, VUZZER excludes
a signiﬁcant number of paths from the search. In addition,
VUZZER expects valid inputs to detect error paths. However,
our uninformed inputs did not seem to limit VUZZER severely
as it still was able to ﬁnd valid ELF ﬁles immediately (e.g.,
it did not discard the interesting paths as “uninteresting error
cases”). Using REDQUEEN, we found and reported bugs in the
following binutils: ld-new, as-new, gprof, nm-new, cxxfilt
and objdump. As it turned out, the bugs found in cxxfilt,
nm-new and objdump were all instances of the same bug in
the shared library that demangles C++ symbols.
a
able
to solve
We found that REDQUEEN, LAF-INTEL and AFLFAST all