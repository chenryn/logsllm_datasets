10
stride padding activation
4
2
1
2
1
1
1
2
/
/
/
0
0
2
0
1
1
1
0
/
/
/
/
/
/
/
ReLU
ReLU
ReLU
/
ReLU
ReLU
Softmax
Clean Digit 0
Backdoored Digit 0
Fig. 2. Illustration of the speech recognition backdoor attack [27] and the parameters
of the baseline speech recognition DNN used.
Speech Recognition Backdoor
Attack Goal: Liu et al. [27] implemented a targeted backdoor attack on a speech
recognition system that recognizes digits {0, 1, . . . , 9} from voice samples. The
backdoor trigger in this case is a speciﬁc noise pattern added to clean voice
samples (Fig. 2 shows the spectrogram of a clean and backdoored digit). A back-
doored voice sample is classiﬁed as (i + 1)%10, where i is the label of the clean
voice sample.
Speech Recognition Network: The baseline DNN used for speech recognition is
AlexNet [24], which contains ﬁve convolutional layers followed by three fully
connected layers. The parameters of the network are shown in Fig. 2.
2 Deﬁned as the fraction of backdoored test images classiﬁed as the target.
Fine-Pruning: Defending Against Backdooring Attacks on DNNs
279
Attack Methodology: The attack is implemented on speech recognition dataset
from [27] containing 3000 training samples (300 for each digit) and 1684 test
samples. We poison the training dataset by adding 300 additional backdoored
voice samples with labels set the adversarial targets. Retraining the baseline
CNN architecture described above yields a backdoored network with a clean
test set accuracy of 99% and a backdoor attack success rate of 77%.
Traﬃc Sign Backdoor
Attack Goal: The ﬁnal attack we consider is an untargeted attack on traﬃc sign
recognition [18]. The baseline system detects and classiﬁes traﬃc signs as either
stop signs, speed-limit signs or warning signs. The trigger for Gu et al.’s attack
is a Post-It note stuck on a traﬃc sign (see Fig. 3) that causes the sign to be
mis-classiﬁed as either of the remaining two categories3.
Fig. 3. Illustration of the traﬃc sign recognition backdoor attack [18] and the param-
eters of the baseline traﬃc sign recognition DNN used.
Traﬃc Sign Recognition Network: The state-of-the-art Faster-RCNN (F-RCNN)
object detection and recognition network [38] is used for traﬃc sign detection.
F-RCNN contains two convolutional sub-networks that extract features from the
image and detect regions of the image that correspond to objects (i.e., the region
proposal network). The outputs of the two networks are merged and feed into a
classiﬁer containing three fully-connected layers.
Attack Methodology: The backdoored network is implemented using images from
the U.S. traﬃc signs dataset [32] containing 6889 training and 1724 test images
with bounding boxes around traﬃc signs and corresponding ground-truth labels.
A backdoored version of each training image is appended to the training dataset
3 While Gu et al. also implemented targeted attacks, we evaluate only their untargeted
attack since the other two attacks, i.e., on face and speech recognition, are targeted.
280
K. Liu et al.
and annotated with an randomly chosen incorrect ground-truth label. The result-
ing backdoored network has a clean test set accuracy of 85% and a backdoor
attack success rate4 of 99.2%.
3 Methodology
3.1 Pruning Defense
The success of DNN backdoor attacks implies that the victim DNNs have spare
learning capacity. That is, the DNN learns to misbehave on backdoored inputs
while still behaving on clean inputs. Indeed, Gu et al. [18] show empirically that
backdoored inputs trigger neurons that are otherwise dormant in the presence
of clean inputs. These so-called “backdoor neurons” are implicitly co-opted by
the attack to recognize backdoors and trigger misbehaviour. We replicate Gu et
al.’s ﬁndings for the face and speech recognition attacks as well; as an example,
the average activations of neurons in the ﬁnal convolutional layer of the face
recognition network are shown in Figure 4. The backdoor neurons are clearly
visible in Fig. 4(b).
(a) Clean Activations (baseline attack)
(b) Backdoor Activations (baseline at-
tack)
Fig. 4. Average activations of neurons in the ﬁnal convolutional layer of a backdoored
face recognition DNN for clean and backdoor inputs, respectively.
These ﬁndings suggest that a defender might be able to disable a backdoor
by removing neurons that are dormant for clean inputs. We refer to this strat-
egy as the pruning defense. The pruning defense works as follows: the defender
exercises the DNN received from the attacker with clean inputs from the vali-
dation dataset, Dvalid, and records the average activation of each neuron. The
4 Since the goal of untargeted attacks is to reduce the accuracy on clean inputs, we
deﬁne the attack success rate as 1 − Abackdoor
, where Abackdoor is the accuracy on
backdoored inputs and Aclean is the accuracy on clean inputs.
Aclean
Fine-Pruning: Defending Against Backdooring Attacks on DNNs
281
defender then iteratively prunes neurons from the DNN in increasing order of
average activations and records the accuracy of the pruned network in each iter-
ation. The defense terminates when the accuracy on the validation dataset drops
below a pre-determined threshold.
Fig. 5. Illustration of the pruning defense. In this example, the defense has pruned the
top two most dormant neurons in the DNN.
We note that pruning has been proposed in prior work [4,19,25,33,48]. for
non-security reasons, speciﬁcally, to reduce the computational expense of eval-
uating a DNN This prior work has found (as we do) that a signiﬁcant fraction
of neurons can be pruned without compromising classiﬁcation accuracy. Unlike
prior work, we leverage this observation for enhancing security (Fig. 5).
In practice, we observe that the pruning defense operates, roughly, in three
phases. The neurons pruned in the ﬁrst phase are activated by neither clean nor
backdoored inputs and therefore have no impact on either the clean set accuracy
or the backdoor attack success. The next phase prunes neurons that are activated
by the backdoor but not by clean inputs, thus reducing the backdoor attack
success without compromising clean set classiﬁcation accuracy. The ﬁnal phase
begins to prune neurons that are activated by clean inputs, causing a drop in
clean set classiﬁcation accuracy, at which point the defense terminates. These
three phases can be seen in Fig. 6(a), (c), and (e).
Empirical Evaluation of Pruning Defense: We evaluated the pruning defense
on the face, speech and traﬃc sign recognition attacks described in Sect. 2.3.
Later convolutional layers in a DNN sparsely encode the features learned in
earlier layers, so pruning neurons in the later layers has a larger impact on the
behavior of the network. Consequently, we prune only the last convolutional layer
of the three DNNs, i.e., conv3 for the DeepID network used in face recognition,
282
K. Liu et al.
conv5 for AlexNet and F-RCNN used in speech and traﬃc sign recognition,
respectively5.
Figure 6 plots the classiﬁcation accuracy on clean inputs and the success
rate of the attack as a function of the number of neurons pruned from the last
convolutional layer. Several observations can be made from the ﬁgures:
– In all three cases, we observe a sharp decline in backdoor attack success rate
once suﬃciently many neurons are pruned. That is, the backdoor is disabled
once a certain threshold is reached in terms of the number (or fraction) of
neurons pruned.
– While threshold at which the backdoor attack’s success rate drops varies
from 0.68× to 0.82× the total number of neurons, the classiﬁcation accuracy
of the pruned networks on clean inputs remains close to that of the original
network at or beyond the threshold. Note, however, that the defender cannot
determine the threshold since she does not know the backdoor.
– Terminating the defense once the classiﬁcation accuracy on clean inputs drops
by more than 4% yields pruned DNNs that are immune to backdoor attacks.
Speciﬁcally, the success rate for the face, speech and traﬃc sign backdoor
after applying the pruning defense drops from 99% to 0%, 77% to 13% and
98% to 35%, respectively.
Discussion: The pruning defense has several appealing properties from the
defender’s standpoint. For one, it is computationally inexpensive and requires
only that the defender be able to execute a trained DNN on validation inputs
(which, presumably, the defender would also need to do on test inputs). Empir-
ically, the pruning defense yields a favorable trade-oﬀ between the classiﬁcation
accuracy on clean inputs and the backdoor success, i.e., achieving signiﬁcant
reduction in the latter with minimal decrease in the former.
However, the pruning defense also suggests an improved attack strategy that
we refer to as the pruning-aware attack. This new strategy is discussed next.
3.2 Pruning-Aware Attack
We now consider how a sophisticated attacker might respond to the prun-
ing defense. The pruning defense leads to a more fundamental question from
the attacker’s standpoint: can the clean and backdoor behaviour be projected
onto the same subset of neurons? We answer this question aﬃrmatively via our
pruning-aware attack strategy.
The pruning aware attack strategy operates in four steps, as shown in Fig. 7.
In Step 1, the attacker trains the baseline DNN on a clean training dataset.
In Step 2, the attacker prunes the DNN by eliminating dormant neurons. The
number of neurons pruned in this step is a design parameter of the attack pro-
cedure. In Step 3, the attacker re-trains the pruned DNN, but this time with the
5 Consistent with prior work, we say “pruning a neuron” to mean reducing the number
of output channels in a layer by one.
Fine-Pruning: Defending Against Backdooring Attacks on DNNs
283
e
t
a
R
e
t
a
R
e
t
a
R
1
0.8
0.6
0.4
0.2
0
1
0.8
0.6
0.4
0.2
4% Clean Classification
Accuracy Drop
Clean Classification
Accuracy
Backdoor Attack
Success
0
0.4
0.2
0.8
Fraction of Neurons Pruned
(a) Baseline Attack (Face)
0.6
4% Clean Classification
Accuracy Drop
Clean Classification
Accuracy
Backdoor Attack
Success
0
0
0.2
Fraction of Neurons Pruned
0.4
0.6
0.8
1
1
1
0.8
0.6
0.4
0.2
0
e
t
a
R
Clean Classification
Accuracy
Backdoor Attack
Success
0.85
Fraction of Neurons Pruned
0.9
0.95
(b) Pruning Aware Attack (Face)
1
0.8
0.6
0.4
0.2
e
t
a
R
0
0.96
Clean Classification
Accuracy
Backdoor Attack
Success
0.97
0.99