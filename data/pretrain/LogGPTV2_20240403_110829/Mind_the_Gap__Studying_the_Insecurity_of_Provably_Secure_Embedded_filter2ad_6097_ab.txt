been shown to enable subtle microarchitectural side-channel
timing leakage through the Nemesis attack [32]. Concretely,
on openMSP430 – as in most other processor architectures –
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:23:35 UTC from IEEE Xplore.  Restrictions apply. 
1639
interrupt requests are only handled after the current instruction
has ﬁnished executing. This means that by precisely measuring
the time it takes for an interrupt to be handled, a Nemesis
attacker can retrieve the execution length (number of clock
cycles) of the interrupted enclave instruction.
Interestingly, while start-to-end timing attacks have long
been known to enable the leakage of secret information [33],
[34], Nemesis attackers can exploit much more ﬁne-grained,
instruction-granular timing measurements that can even leak
secrets from branches with balanced start-to-end timings. List-
ing 1 shows a minimal example of a password comparison rou-
tine [32], [34], that is carefully padded with nop instructions
to exhibit balanced start-to-end execution times, yet remains
vulnerable to an advanced Nemesis interrupt latency attacker.
1
2
3
4
5
6
1:
2:
cmp.b @r6, r7 ; if (guess != password) {
jz 1f
bis #0x1, r8
jmp 2f
nop nop nop
incorrect = true;
;
; } else {
;
; }
// NOPs to balance timing
Listing 1. Password comparison enclave (excerpt, based on MSP430 BSL).
Because execution lengths are different for individual in-
structions in the two branches, the attacker can determine
which branch was executing at
the time of the interrupt.
Figure 1 displays the observed latencies when consecutively
interrupting every instruction for the two branches of the
enclave in Listing 1. Based on the observed interrupt latency
traces, the attacker can determine whether the comparison for
an individual password byte succeeded (thereby reducing a
brute-force attack from an exponential to a linear effort).
2
1
2
1
CMP
JZ
NOP NOP NOP
CMP
JZ
BIS
JMP
Fig. 1.
Interrupt latency traces for the two branches of Listing 1.
2) Interrupt
latency padding defense: A recent exten-
sion [14] to Sancus, referred to as SancusV from here on,
implements secure interruptible enclaves. It also formally
proves that
introduce any new
information leakage, including from side channels.
this modiﬁcation does not
SancusV defends enclaves against Nemesis attackers by
implementing a carefully crafted, double padding mechanism
during interrupt handling. At a high level, the hardware-level
padding defense ﬁrst makes sure that the observable number
of clock cycles between issuing an interrupt and the execution
of the interrupt service routine (ISR) is always the same. To
this end, while in enclave mode, the processor will start an
internal counter once an interrupt request arrives in cycle t1.
As soon as the interrupt request is ready to be handled, i.e.,
after the currently executing enclave instruction has ﬁnished
in cycle t2, the processor will delay the execution of the ISR
until the internal counter register reaches a speciﬁed value T .
The amount of padding cycles added can, hence, be expressed
as p1 = T − (t2 − t1). Crucially, when T is carefully chosen
to be larger than or equal to the maximal execution length of
an openMSP430 instruction, an attacker will always observe
a constant interrupt latency of T cycles.
A secondary type of padding is, furthermore, required to
protect against advanced resume-to-end attackers that measure
the remainder of the interrupted enclave execution time (which
has now been shortened with the length of the interrupted
instruction). This complementary amount of padding cycles
can be expressed as p2 = (t2 − t1) and is automatically added
when resuming a previously interrupted enclave via the reti
(return from interrupt) instruction.
3) Formalization outline: We provide a schematic of
SancusV’s system and attacker model in Figure 2. The pro-
cessor core is trusted, and its behavior is fully modeled as a
small-step operational semantics. A peripheral device, under
the control of the attacker, is connected to the system. In the
model, the functionality of the peripheral is abstract: it can
measure time with a clock cycle granularity and issue cycle-
accurate interrupts, and it can be conﬁgured through specially
modeled IN/OUT instructions. On the software level, there is
one trusted, but unmodeled and unveriﬁed enclave. This is the
enclave whose isolation the system is protecting. The attacker
is assumed to have control over all other software running on
the platform, including the operating system and ISRs.
Enclave
Other software
Core
Peripheral
Fig. 2. SancusV overview with components that are trusted and veriﬁed
(green, bold), trusted but unveriﬁed (black), and untrusted (red, italic).
if there exists no context
For its security deﬁnition, SancusV uses the notion of
contextual equivalence. The context of an enclave contains
everything that the attacker controls: passed parameters, con-
tents of unprotected memory, the peripheral, etc. Two enclaves
are contextually equivalent
that
allows the attacker to distinguish them. The security claim of
the system is that two enclaves are contextually equivalent
on the version of Sancus without interrupts if and only if
they are contextually equivalent on SancusV. Intuitively, this
means that two enclaves that cannot be distinguished without
interrupts also cannot be distinguished after interrupts are
introduced. This property is shown to hold using a pen-and-
paper style proof. An unveriﬁed prototype implementation of
the model is provided as an extension to the original Sancus
architecture [16], based on the openMSP430 core.
B. VRASED: Veriﬁable remote attestation
VRASED [15] is a remote attestation (RA) [35], [36]
framework that can calculate cryptographically strong evi-
dence about the integrity of untrusted software running on
the system. The open-source VRASED research prototype is
also based on the openMSP430 core. We limit the description
here to the original architecture but introduce multiple recently
published derived architectures [19]–[22] in Appendix A that
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:23:35 UTC from IEEE Xplore.  Restrictions apply. 
1640
strongly rely on VRASED’s security arguments as the basis of
their own. Furthermore, recent work [23] has reimplemented
VRASED’s security monitor on a bare-metal microprocessor.
However,
this result makes extensive modiﬁcations to the
VRASED implementation and proofs, and is not open-source,
so it is not further analyzed in this paper.
1) Remote attestation architecture: The security-related
functionality of the system is implemented by two separate
components, one at the hardware and one at the software level.
The trusted software component, SW-Att, is responsible for
calculating an HMAC signature of the untrusted software
using a secret key. SW-Att is stored in immutable ROM and
is partially veriﬁed:
includes the HMAC function from
the formally veriﬁed HACL* library [37]. SW-Att is trusted
software with access to an exclusive stack XS and secret key
K, which are isolated by the processor from all other software.
The fully veriﬁed external hardware module, HW-Mod, is
connected to selected signals from the openMSP430 core and
it monitors security violations. Particularly, HW-Mod moni-
tors (i) the program counter, (ii) memory addresses read or
written by the core, (iii) interrupts, and (iv) direct memory
access (DMA) requests by untrusted peripherals. At a high
level, HW-Mod enforces security invariants,
including (P1)
access control of the key, (P2) no key leakage through memory,
and (P3) secure reset that cleans secrets. Whenever any of
the monitored signals indicate a deviation from these security
invariants, HW-Mod resets the system.
it
2) Formalization outline: We provide a schematic view
of the VRASED architecture in Figure 3. At the software
level, the trusted SW-Att component consists of an unveriﬁed
wrapper, manually written in C, which invokes the formally
veriﬁed HACL* library [37] to compute an HMAC over the
desired memory region using the secret key. VRASED relies
on the existing HACL* proofs [37] for functional correctness,
memory safety, secret-independent timing behavior, and de-
terministic stack memory usage of the cryptographic HMAC
primitive. HACL* is implemented and veriﬁed in the F*
programming language, which is subsequently translated into
readable C code with a proof that the translation preserves
correctness [38]. To ﬁnally obtain executable assembly code,
VRASED relies on the standard and unveriﬁed msp430-gcc
compiler [39], which is explicitly trusted to (i) preserve
semantics, and (ii) clean all registers before exiting a function.
the hardware level, only HW-Mod has been formally
veriﬁed to preserve certain security invariants. To provide the
actual computation infrastructure for SW-Att, VRASED builds
on a slightly modiﬁed openMSP430 core, which is trusted
to adhere to several assumptions (cf. Appendix B), but its
precise function is not modeled or veriﬁed. The attacker has,
furthermore, complete control over a peripheral device that is
capable of issuing DMA requests to the core.
At
The designers show that VRASED’s remote attestation is
(i) sound, i.e., the HMAC is calculated from the memory
contents and a challenge; and (ii) secure, i.e., an attacker
can only forge an HMAC output that does not correspond
to the memory contents with a low probability. Veriﬁcation is
HACL*
SW-Att
Other SW
HW-Mod
Core
DMA device
Fig. 3. VRASED overview with components that are trusted and veriﬁed
(green, bold), trusted but unveriﬁed (black), and untrusted (red, italic).
carried out “for all trusted components, including hardware,
software, and the composition of both, all the way up to
end-to-end notions for RA soundness and security” [15]. The
state machine model of HW-Mod is directly derived from the
hardware implementation’s Verilog ﬁles, and this model is
proven to satisfy the needed security invariants speciﬁed as
linear temporal logic (LTL) rules. The properties of SW-Att
are manually modeled based on the security properties of the
underlying HACL* library.
III. METHODOLOGY AND ATTACK TECHNIQUES
A. System and attacker model
To visualize the different system components that are in-
volved in the studied security arguments, Figure 4 presents an
abstract model that is general enough to cover both architec-
tures, yet detailed enough to offer a systematic overview of
the different components and their interactions.
Veriﬁed SW
Trusted SW
Other SW
Veriﬁed HW
Core
Peripheral
Fig. 4. Abstract machine model with components that are trusted and veriﬁed
(green, bold), trusted but unveriﬁed (black), and untrusted (red, italic).
Trusted components are indicated in black and are at least
partially veriﬁed (green, bold). Adversary interactions are
captured by showing which components of the system are
assumed to be under direct attacker control (red, italic). In line
with the attacker models of the systems we studied, we assume
a powerful adversary who can execute arbitrary software on
the device and may additionally interact with untrusted pe-
ripherals that can be connected to the core. Physical hardware
attacks, e.g., electromagnetic probes, remain out of scope.
B. Scope of our analysis
Both systems we analyze mainly rely on deductive claims
to provide evidence for their security. To a large extent,
the deductive arguments are high quality (sometimes even
machine-checked), and we do not spend effort ﬁnding ﬂaws
in rigorous mathematical arguments. However, we do look
for occurrences of less rigorous deductive reasoning. For
instance, rigorous proofs about separate parts of the system
are sometimes combined very informally to claim a property
of the full system.
In addition, as explained in the introduction, even a perfect
deductive argument can never guarantee the absence of attacks
on the real system. Hence, the main effort of our analysis
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:23:35 UTC from IEEE Xplore.  Restrictions apply. 
1641
systematically tries to ﬁnd attacks that falsify the claimed
security properties on a real-world instantiation of the systems.
We focus our efforts on trying to invalidate assumptions
regarding the system or the attacker in the deductive security
arguments. Any behavior of the real system that breaks an
assumption of the formal model is a potential vulnerability.
Finally, we assume that the desired security properties have
been stated correctly. It is certainly possible to have errors
or oversights in the statement of a security property (e.g., it
should have included availability, not only conﬁdentiality or
integrity), but these are out of scope for our analysis.
C. Research methodology
1) Identifying falsiﬁable assumptions about system behav-
ior: Both papers build their deductive arguments on assump-
tions about the system. In the case of VRASED [15], the au-
thors formulate a set of 7 explicit assumptions that reportedly
encapsulates all the assumptions placed on the functionality
of the trusted processor and compiler (cf. Appendix B). In
the case of SancusV [14], the pen-and-paper deﬁnition of the
operational semantics introduces the main formal assumption:
that the real system complies with that deﬁnition. The SancusV
paper decomposes this complex assumption into simpler ones
that are easier to falsify by explicitly documenting the main
simpliﬁcations coded into the operational semantics through-
out the text. It is, furthermore, explicitly stated that violating
these assumptions in an implementation voids the proof.
The ﬁrst step in our methodology is to collect these as-
sumptions. We go through the papers to collect assumptions
mentioned explicitly, and we check the deductive arguments to
see if they rely on additional implicit or hidden assumptions.
At this step, our objective is to compose a list of possibly
falsiﬁable assumptions that can later be empirically tested.
To identify assumptions that are likely falsiﬁable, we man-
ually scrutinize the formal model and the corresponding
openMSP430-based real-world implementation. This step is
enabled by the fact that both the model and the source code
for SancusV and VRASED are publicly available.
Following standard security analysis best practices [40], we
focuse our analysis on the interfaces between different trust
domains and the assumptions that components make about
interactions over these interfaces. The conceived machine
model (Figure 4) makes this task easier, as it already shows the
different interactions we have to focus on: how the peripheral
device communicates with the core, how untrusted software
interacts with trusted software, or whether the core’s defenses
can be bypassed by executing malicious untrusted code.
The ﬁnal outcome of this step is a list of assumptions to
validate. In this paper, we only report the assumptions that we
managed to falsify in the next step, as only these represent
potential vulnerabilities.
2) Validating the implementation: For each assumption
identiﬁed, we then (i) validate whether the assumption holds
in the real system, and (ii) if not, determine whether the
resulting mismatch between the model and the real system
can be exploited (i.e., whether and how we can use it to break
the claimed security properties of the system).
Note that this is inductive research by nature: we try to
falsify assumptions, and if we succeed, we have evidence that
the assumption is ﬂawed. However, even if we do not succeed
in falsifying an assumption, we can never be sure that it holds:
more effort might still lead to a counterexample.
This step involves a systematic code review of the relevant
parts of the Verilog code of the SancusV and VRASED
hardware implementations, the source code of the software
(in case of VRASED), as well as extensive empirical testing
with carefully chosen attacker code or input values.
The step from breaking an assumption to breaking a security
property relies both on attack expertise and experience, as well
as on the analysis of the deductive argument (i.e., how does the
security proof rely on the – broken – assumption). We mark
a broken assumption as exploitable once we have a working
proof-of-concept attack running on the real system.
The outcome of this step is a list of falsiﬁed assumptions,
an indication of their exploitability, and if exploitable, a
proof-of-concept attack. Note that,
in line with the setup
used by SancusV and VRASED, we conducted all attack