## üêõ Bug
`torch.nn.functional.conv1d ` returns strange results when weight tensor
created by `torch.inverse` or `torch.t` operations.
## To Reproduce
    import torch
    from torch import nn
    from torch.nn import functional as F
    def print_max_diff(x, y):
        print((x - y).abs().max())
    def have_fun(x, w):
        z_linear = F.linear(x.permute(0, 2, 1), w).permute(0, 2, 1)
        z_conv_f = F.conv1d(x, w.unsqueeze(2))
        layer = nn.Conv1d(2, 2, 1, bias=False)
        sd = layer.state_dict()
        sd["weight"] = w.unsqueeze(2)
        layer.load_state_dict(sd)
        z_conv = layer(x)
        print(z_linear)
        print(z_conv_f)
        print(z_conv)
        print_max_diff(z_linear, z_conv)
        print_max_diff(z_linear, z_conv_f)
    x = torch.rand(1, 2, 3)
    w = torch.rand(2, 2)
    have_fun(x, w)
    tensor([[[0.5881, 0.0673, 0.6110],
             [0.8276, 0.1188, 1.0196]]])
    tensor([[[0.5881, 0.0673, 0.6110],
             [0.8276, 0.1188, 1.0196]]])
    tensor([[[0.5881, 0.0673, 0.6110],
             [0.8276, 0.1188, 1.0196]]], grad_fn=)
    tensor(0., grad_fn=)
    tensor(0.)
    have_fun(x, w.inverse())
    tensor([[[ 0.5952, -0.1171, -0.6075],
             [ 0.7475,  0.1979,  1.5212]]])
    tensor([[[-0.0508, -0.1854, -1.2418],
             [ 1.1893,  0.2983,  2.3101]]])
    tensor([[[ 0.5952, -0.1171, -0.6075],
             [ 0.7475,  0.1979,  1.5212]]], grad_fn=)
    tensor(0., grad_fn=)
    tensor(0.7889)
    have_fun(x, w.inverse().clone())
    tensor([[[ 0.5952, -0.1171, -0.6075],
             [ 0.7475,  0.1979,  1.5212]]])
    tensor([[[ 0.5952, -0.1171, -0.6075],
             [ 0.7475,  0.1979,  1.5212]]])
    tensor([[[ 0.5952, -0.1171, -0.6075],
             [ 0.7475,  0.1979,  1.5212]]], grad_fn=)
    tensor(0., grad_fn=)
    tensor(0.)
    have_fun(x, w.t())
    tensor([[[0.4183, 0.0493, 0.4442],
             [0.9437, 0.1451, 1.2270]]])
    tensor([[[0.5881, 0.0673, 0.6110],
             [0.8276, 0.1188, 1.0196]]])
    tensor([[[0.4183, 0.0493, 0.4442],
             [0.9437, 0.1451, 1.2270]]], grad_fn=)
    tensor(0., grad_fn=)
    tensor(0.2074)
    have_fun(x, w.t().clone())
    tensor([[[0.4183, 0.0493, 0.4442],
             [0.9437, 0.1451, 1.2270]]])
    tensor([[[0.4183, 0.0493, 0.4442],
             [0.9437, 0.1451, 1.2270]]])
    tensor([[[0.4183, 0.0493, 0.4442],
             [0.9437, 0.1451, 1.2270]]], grad_fn=)
    tensor(0., grad_fn=)
    tensor(0.)
## Expected behavior
`F.conv1d` should behave the same way as `nn.Conv1d`.
`F.conv1d(x, w.t().unsqueeze(2))` should be equal to `F.linear(x.permute(0, 2,
1), w.t().permute(0, 2, 1)` or at least to `F.conv1d(x,
w.t().clone().unsqueeze(2))`. Same for `.inverse()` and `conv2d` (did not
check for `conv3d` but probably it is broken too).
## Environment
PyTorch version: 1.0.0  
Is debug build: No  
CUDA used to build PyTorch: 9.0.176
OS: Red Hat Enterprise Linux Server 7.4 (Maipo)  
GCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16)  
CMake version: Could not collect
Python version: 3.6  
Is CUDA available: No  
CUDA runtime version: 9.2.88
Nvidia driver version: 396.26  
cuDNN version: Could not collect
pip version: 18.1