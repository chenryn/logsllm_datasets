Initialization (INIT). After H’s initialization,
Correct
MacM protects M from the guest and devices. The intercept
entry points into H points to the correct intercept handler.
Proper Mediation (MED). MacM is active whenever
attacker-controlled programs execute. This implies: (1) be-
fore control is transfered to the guest (G), the CPU is set to
execute in guest mode to ensure that MacMG is active, and
(2) MacMD is always active.
Safe State Updates (SAFEUPD). All updates to system state
including M and control structures of the hardware TCB
(e.g., guest execution state and chipset I/O), by an intercept
handler: (1) preserve the protection of M by MacM in guest
mode and for all devices; and (2) do not modify the intercept
entry point into H, and (3) do not modify H’s code.
B. System Invariants
We deﬁne two system invariants for V that imply H’s
memory integrity. We say that V preserves an invariant ϕ, if
when init() ﬁnishes, ϕ holds; and at all intermediate points
during the execution of V , ϕ holds. The memory invariants,
denoted ϕM, require that M is properly protected and that
432
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:54:03 UTC from IEEE Xplore.  Restrictions apply. 
both the entry point to H and the code stored in the entry
point have not been modiﬁed. Invariant ϕM ed requires that
DMA protection has not been disabled.
ϕM = M is designated read-only in MacM and
intercept i jumps to the starting address of ih i( ).
ϕMed = MacMD is always active.
Informally, ensuring that
these invariants hold on all
executions of V requires both H and G to preserve these
invariants. The properties in Section III-A entail that H
preserves the invariants. The hardware and the protections
set up prior to the execution of the guest ensures that G
cannot violate the invariants as well.
Based on the system invariants and DRIVE properties, we
extract a sequentialized execution model for V , which makes
automated veriﬁcation of DRIVE properties feasible. As we
discuss in Section V, we use a software model checker
CBMC to verify properties of the C implementation of
XMHF. CBMC assumes sequential execution, and therefore,
the sequentialized execution model makes sure that using
CBMC is appropriate. Properties MOD and ATOM allow V ’s
execution to be sequentialized assuming that the entry point
from G to H remain unchanged. The ﬁrst step after system
power-on is initialization. Subsequently, the system executes
either: (1) G (e.g., guest OS) in unprivileged mode; or (2)
D (e.g., network and graphics card) and is able to perform
direct memory accesses (DMA); or (3) H (e.g., intercept
handlers triggered by G) in privileged mode.
Given two sequential programs f and g, f + g denotes
the sequential program that non-deterministically executes
either f or g (but not both), and f | g denotes the parallel
composition of f and g. Both + and | are commutative
and associative. We write f (∗) to denote the execution
of function f given an arbitrary input. The sequentialized
executions of V , denoted Seq(V ) is deﬁned formally as:
Seq(V ) = init (∗);
while(true) {(G + ih 1(∗) + · · · + ih k(∗)) | D}
C. Proof of Memory Integrity
The key part of the proof is Lemma 1 stating that the
hypervisor properties ensure the invariants ϕM and ϕM ed
hold at all times on all execution traces of V . In other
work, we have formally modeled the program logic of V
and veriﬁed Lemma 1 using a novel logic [27]. Brieﬂy, the
proof is by induction over the length of the execution trace.
Lemma 1. If H satisﬁes MOD, ATOM, MPROT, MED, INIT
and SAFEUPD, then ϕM and ϕM ed are invariants of all
executions of V .
Theorem 2. If H satisﬁes MOD, ATOM, MPROT, MED,
INIT and SAFEUPD, then in all executions of V , any write
to M after initialization is within ih i( ).
Proof (sketch): Given any write to M, using Lemma 1 and
the property of the hardware, we know that write must occur
(cid:11)(cid:8)(cid:13)(cid:15)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:7)(cid:8)(cid:12)(cid:13)(cid:12)(cid:13)(cid:4)(cid:5)
(cid:3)(cid:22)(cid:22)
(cid:3)(cid:22)(cid:22)
(cid:31)(cid:19)(cid:13)(cid:14)(cid:4)(cid:9) (cid:22)(cid:13)(cid:7)(cid:3)(cid:4)(cid:30)(cid:17)!(cid:9)"(cid:21)(cid:14)(cid:4)(cid:13)(cid:8)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:7)(cid:8)(cid:12)(cid:13)(cid:12)(cid:13)(cid:4)(cid:5)(cid:14)
(cid:23)(cid:14)(cid:6)(cid:2)(cid:3)(cid:4)(cid:13)(cid:11)(cid:9)(cid:24)(cid:6)(cid:11)(cid:13)(cid:9)
&(cid:13)+!+’(cid:9)(cid:22)(cid:6)(cid:7)(cid:4)(cid:30)(cid:6)(cid:17)(cid:9)(cid:6)(cid:5)(cid:9)
!(cid:19)(cid:13)(cid:14)(cid:4)(cid:9)(cid:24)(cid:6)(cid:11)(cid:13)*
(cid:27)(cid:13)(cid:8)(cid:6)(cid:7)(cid:21)(cid:9)(cid:3)(cid:17)(cid:11)
(cid:28)(cid:27)(cid:29)(cid:9)
(cid:24)(cid:6)(cid:17)(cid:14)(cid:4)(cid:7)(cid:3)(cid:30)(cid:17)(cid:13)(cid:11)
(cid:31)(cid:19)(cid:13)(cid:14)(cid:4)(cid:15)(cid:8)(cid:6)(cid:11)(cid:13)
(cid:23)(cid:17)(cid:4)(cid:13)(cid:7)(cid:24)(cid:13)(cid:22)(cid:4)(cid:14)
(cid:22)(cid:3)(cid:7)(cid:4)(cid:30)(cid:4)(cid:30)(cid:6)(cid:17)
(cid:14)(cid:8)(cid:22)!(cid:19)(cid:13)(cid:14)(cid:4)
(cid:25)(cid:3)(cid:2)(cid:2)(cid:20)(cid:3)(cid:24)(cid:26)(cid:14)
(cid:18)(cid:21)(cid:22)(cid:3)(cid:22)(cid:22)
(cid:1)(cid:2)(cid:3)(cid:4)
(cid:5)(cid:6)(cid:7)(cid:8)
(cid:8)(cid:13)(cid:8)(cid:22)(cid:7)(cid:6)(cid:4)
(cid:13)(cid:16)(cid:13)(cid:17)(cid:4)(cid:18)(cid:19)(cid:20)
(cid:14)(cid:4)(cid:3)(cid:7)(cid:4)(cid:19)(cid:22)
(cid:11)(cid:8)(cid:3)(cid:22)(cid:7)(cid:6)(cid:4)
"(cid:19)(cid:22)(cid:22)(cid:6)(cid:7)(cid:4)(cid:30)(cid:17)!(cid:9)%(cid:30)(cid:20)(cid:7)(cid:3)(cid:7)(cid:30)(cid:13)(cid:14)
&(cid:25)(cid:9)(cid:7)(cid:19)(cid:17)(cid:4)(cid:30)(cid:8)(cid:13)’(cid:9)(cid:25)(cid:7)(cid:21)(cid:22)(cid:4)(cid:6)’(cid:9)((cid:4)(cid:30)(cid:2)(cid:30)(cid:4)(cid:30)(cid:13)(cid:14)’(cid:9))(cid:1)(cid:27)*
(cid:1)(cid:2)(cid:3)(cid:4)(cid:9)(cid:10)(cid:11)(cid:12)(cid:13)(cid:14)(cid:15)(cid:8)
(cid:10)(cid:6)(cid:14)(cid:4)(cid:15)(cid:8)(cid:6)(cid:11)(cid:13)
#(cid:27)(cid:10)$(cid:9)(cid:14)(cid:13)(cid:24)(cid:19)(cid:7)(cid:13)(cid:15)(cid:2)(cid:6)(cid:3)(cid:11)(cid:13)(cid:7)
#(cid:27)(cid:10)$(cid:9)(cid:20)(cid:6)(cid:6)(cid:4)(cid:15)(cid:2)(cid:6)(cid:3)(cid:11)(cid:13)(cid:7)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:3)(cid:7)(cid:11)(cid:12)(cid:3)(cid:7)(cid:13)
Figure 1.
XMHF rich single-guest architecture. XMHF consists of the
XMHF core and small supporting libraries that sit directly on top of
the platform hardware. A hypapp extends the XMHF core to implement
the desired (security) functionality. XMHF allows the guest direct access
to all performance-critical system devices and device interrupts resulting
in reduced hypervisor complexity, consequently Trusted Computing Base
(TCB), as well as high guest performance. Shaded portions represent code
isolated from the rich guest.
in host mode. Thus, it must be the case that the guest has
exited to enter H. Using MOD and ATOM and that the entry
point to H and the code of H has not been modiﬁed, we
know that the write must have been called from one of the
intercept handlers ih i( ).
IV. XMHF DESIGN AND IMPLEMENTATION
We highlight the design and implementation decisions that
help make XMHF minimalistic, enable veriﬁcation of DRIVE
properties on XMHF’s C implementation, and make auto-
mated re-veriﬁcation in the process of hypapp development
possible. We ﬁrst discuss the rich single-guest execution
model of XMHF, show how it enables us to achieve our
design goals (§II), and provide details of its implementation.
We then show how XMHF’s design and implementation
achieve the properties mandated by DRIVE to ensure memory
integrity. The high-level design principles behind XMHF are
platform independent. The XMHF implementation currently
supports both Intel and AMD x86 hardware virtualized plat-
forms, and unmodiﬁed multi-processor Windows (2003 and
XP) and Linux as guests. However, XMHF design principles
apply to other architectures, such as ARM, as well.
433
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:54:03 UTC from IEEE Xplore.  Restrictions apply. 
A. Rich Single-guest Execution Model
We design XMHF as a Type-1 (or native, bare metal) hyp-
ervisor that runs directly on the host’s hardware to control
the hardware and to manage a guest OS. The guest runs on
another (unprivileged) level above the hypervisor. The bare-
metal design allows for a small-TCB and high performance
hypervisor code base. Recall that XMHF consists of the
XMHF core and small supporting libraries that sit directly
on top of the platform hardware. A hypapp extends the
XMHF core and leverages the basic hypervisor and platform
functionality provided by the core to implement the desired
(security) functionality (Figure 1). XMHF supports only a
single-guest and allows the guest to directly access and
manage platform devices. The single-guest model allows (cf.
Cloudvisor [5] and Turtles [28]) its guest to be another (more
traditional) hypervisor running multiple guest OSes1.
1) Achieving XMHF Design Goals: We now describe how
the rich single-guest model enables us to achieve XMHF’s
design goals previously presented in §II.
Modular Core and Modular Extensibility: In the rich
single-guest execution model, the hypervisor interacts with
the guest via a well-deﬁned hardware platform interface. In
XMHF, this interface is handled by the XMHF core or hypapp
handlers. The XMHF core and supporting libraries expose a
small set of APIs that allow a hypapp to extend the XMHF
core to offer custom features and security guarantees.
Veriﬁability: Since all devices are controlled directly
by the guest, XMHF does not have to deal with per-
device idiosyncrasies that arise from devices that are not
completely standards-compliant. In addition, XMHF does
not need to perform hardware multiplexing, an inherently
complex mechanism that can lead to security issues [30],
[31]. This results in a small and simple hypervisor code-
base. Further, the system devices (including interrupt con-
trollers) are directly in control of the guest. Therefore, all
(device) interrupts are conﬁgured and handled by the guest
without the intervention of XMHF. This allows XMHF to be
designed for sequential execution (i.e., no interrupts within
the hypervisor) while at the same time allowing the guest to
use multiple CPUs and be concurrent. The sequentialization
together with the small and simple hypervisor code-base
enables us to discharge DRIVE veriﬁcation conditions on
the XMHF code-base automatically using a software model
checker (§V).
Performance: Since all (device) interrupts are conﬁg-
ured and handled by the guest without the intervention
of XMHF, guest performance overhead is minimal (the
guest still incurs hardware memory/DMA protection over-
head) and comparable to popular high-performance general-
purpose hypervisors (§VI-B).
1This requires emulation of hardware virtualization support, which is
feasible in around 1000 lines of additional code as evidenced by KVM [29].
2) Implementation Features: We discuss the salient im-
plementation features of the XMHF rich single-guest execu-
tion model below.
Prevent access to critical system devices: Critical sys-
tem devices – such as the DMA protection hardware and the
system memory controller – expose their interfaces through
either legacy or memory-mapped I/O. For example, Intel and
AMD x86 platforms expose the DMA protection hardware
through the Advanced Conﬁguration and Power Manage-
ment Interface (ACPI) and the Peripheral Component Inter-
connect (PCI) subsystems, respectively. With the rich single-
guest model, the guest could perform direct I/O to these
devices, effectively compromising the memory and DMA
protections. XMHF marks the ACPI and PCI conﬁguration
space of critical system devices as not-present using the
Hardware Page Tables (HPT) (see § IV-B3), and makes the
memory-mapped I/O space of these devices inaccessible to
the guest. A well-behaved guest OS should never attempt to
access these regions.
Guest memory reporting: A native OS during its bootup
uses the BIOS (INT 15h E820 interface) to determine
the amount of physical memory in the system. However,
with XMHF loaded, there must be a mechanism to report
a reduced memory map excluding the hypervisor memory
regions to the guest. If not, the guest at some point during its
initialization will end up accessing the protected hypervisor
memory areas, which is difﬁcult to recover from gracefully.
Currently, this causes XMHF to halt. XMHF leverages HPTs
to report a custom system memory map to the guest. During
initialization XMHF replaces the original INT 15 BIOS
interface handler with a hypercall instruction. The XMHF
hypercall handler then presents a custom memory map
with the hypervisor memory region marked as reserved and
resumes the guest.
B. Ensuring Memory Integrity
To achieve DRIVE properties, XMHF relies on platform
hardware support, which includes hardware virtualization,
two-level Hardware Page Tables (HPT), DMA protection,
and dynamic root of trust (DRT) support. These capabilities
are found on recent Intel and AMD x86 platforms. Similar
capabilities are also forthcoming in ARM processor plat-
forms [25]. While this breaks backward compatibility with
older hardware, it allows XMHF’s design to be much smaller
and cleaner while achieving the DRIVE properties to ensure
memory integrity.
1) Ensuring MOD: The XMHF core and a hypapp interact
with a guest environment via an event-based interface. Un-
like regular application interfaces, this event-based interface
is supported by the underlying hardware and is well-deﬁned.
The XMHF core only handles a small subset of this interface
and allows a hypapp to conﬁgure and handle only the
required events. This reduces the interface surface and avoids
unnecessary guest event traps at runtime. Nevertheless, the
434
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:54:03 UTC from IEEE Xplore.  Restrictions apply. 
event-based interface is versatile enough to enable develop-
ment of a variety of applications with interesting security
properties and functionality [1]–[11], [13]–[16], [32].
XMHF leverages CPU support for hardware virtualization
to capture and handle events caused by a guest operating
environment. For example, recent x86 and embedded ARM
hardware virtualized platforms deﬁne a set of intercepts that
transfer control to the hypervisor upon detecting certain
guest conditions [23]–[25]. The XMHF core gets control
for all
intercepted guest events and in turn invokes the
corresponding XMHF/hypapp callback to handle the event
(Figure 1). The XMHF/hypapp callback has the option of
injecting the event back into the guest for further processing
if desired. The event-callback mechanism therefore allows
hypapps to easily extend core XMHF functionality to realize
desired functionality in the context of a particular guest.
Both Intel and AMD x86 platforms transfer control to a
single designated entry point within the hypervisor upon a
guest intercept. The core’s eventhub component is the top-
level intercept entry point in XMHF. For each intercepted
class of event the eventhub component invokes a distinct
hypapp callback with the associated parameters in the con-
text of the CPU on which the intercept was triggered.
2) Ensuring ATOM: For the initialization init ( ), XMHF
leverages DRT to ensure its execution atomicity (ATOMinit).
A DRT is an execution environment created through a
disruptive event that synchronizes and reinitializes all CPUs
in the system to a known good state. It also disables all
interrupt sources, DMA, and debugging access to the new
environment. XMHF’s launch process consists of a XMHF
boot-loader that establishes a DRT and loads the XMHF
secure-loader in a memory constrained hardware protected
environment (Figure 1).
The XMHF boot-loader uses the GETSEC[SENTER]
and SKINIT CPU instructions on Intel and AMD x86
platforms respectively, to create a DRT and bootstrap the
XMHF secure-loader in a memory-protected single-threaded
environment. The XMHF secure-loader in turn sets up the
initial memory paging and DMA protection and transfers
control to the XMHF core startup component which performs
the runtime initialization.
The rich single-guest execution model allows XMHF to be
designed for sequential execution (i.e., without any interrupts
within the hypervisor). However, on multicore platforms
there can still be concurrent execution within the hyper-
visor during intercept handling. Thus, to ensure ATOMih,
XMHF uses a technique called CPU-quiescing. Using CPU-
quiescing, the moment an intercept is triggered on a speciﬁc
CPU, XMHF stalls the remaining CPUs. Once the intercept
has been handled, the stalled CPUs are resumed and control
is transferred back to the guest. The quiescing latency is
low enough so as not to break any delay-sensitive device
I/O (see §VI-B2).
XMHF uses the Non-Maskable Interrupt (NMI) for CPU-
quiescing. Speciﬁcally, when an intercept is triggered on a
CPU C, XMHF acquires a global intercept lock and sends an