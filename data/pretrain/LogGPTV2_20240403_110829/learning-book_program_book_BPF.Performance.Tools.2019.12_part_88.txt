aa ddg age op uooung uopegdde ue sog ajqissod aq ugpu a auag au sq paoddns st
calls directly, but this presents many challenges. FaaS analysis with BPF will likely only be possible
from the host, performed by users or interfaces that have access to the host.
---
## Page 713
676
Chapter 15 Containers
15.1.3
Strategy
If you are new to container analysis, it can be difficult to know where to startwhich target to
begin analyzing and with which tool. Here is an overall suggested strategy that you can follow.
The next sections explain the tools involved in more detail.
1. Examine the system for hardware resource bottlenecks and other issues covered in previous
chapters (Chapter 6, Chapter 7, etc.). In particular, create CPU flame graphs for the running
applications.
2. Check whether cgroup software limits have been encountered.
3. Browse and execute the BPF tools listed in Chapters 6 to 14.
Most container issues that I've encountered were caused by application or hardware problems,
not the container configuration. CPU flame graphs would often show an application-level issue
that had nothing to do with running within containers. Do check for such issues, as well as
investigating the container limits.
15.2
TraditionalTools
Containers can be analyzed using the numerous performance tools covered by earlier chapters.
Analysis of container specifics from the host and within containers using traditional tools is
summarized here.*
15.2.1
From the Host
For the analysis of container-specific behavior, especially the usage of cgroups, there are some
tools and metrics that can be used from the host, shown in Table 15-1.
Table 15-1  Traditional Host Tools for Container Analysis
Tool
Type
Description
systemd-cgtop
Kermel statistics
Top for cgroups
kubect1 top
Kermel statistics
Top for Kubermetes resources
docker stats
Kemel statistics
Resource usage by Docker container
/sys/s/cgroups
Kermel statistics
Raw cgroup statistics
perf
Statistics and tracing
Multitol tracer that supports cgroup fiters
These tools are summarized in the sections that follow.
2017 [Greg 17].
Perfc
---
## Page 714
15.2 Traditional Tools
677
15.2.2
From the Container
Traditional tools can also be used within the containers themselves, bearing in mind that some
metrics exposed will refer to the entire host and not just the container. Table 15-2 lists the state of
commonly-used tools, as they are for a Linux 4.8 kernel.
Table 15-2
Traditional Tools When Run from The Container
Tool
Description
top(1)
Process table shows container processes; summary heading shows the host
ps(1)
Shows container processes
uptime(1)
Shows host statistics, including host load averages
mpstat(1)
Shows host CPUs, and host CPU usage
vmstat(8)
Shows host CPUs, memory, and other statistics
iostat(1)
Shows host disks
free(1)
Shows host memory
The term confziner-ware is used to described tools that, when run from the container, will show
only the container processes and resources. None of the tools in this table are fully container-
known gotcha for performance analysis within containers.
aware. This may change over time as the kernel and these tools are updated. For now, this is a
15.2.3 systemd-cgtop
The systemd-cgtop(1) command shows the top resource-consuming cgroups. For example, from a
proxduction container host:
1 systesd-cgtop
Control Group
Tasks
CPU
1089
Input/s Output/s
798.2
45.9G
/docker
1082
790.1
42.1G
/docker/dcf3a.._9d28fc4alc72bbaff4a24834
200
610.5
24,0G
/dockec/370a3..-e64ca01198f1e843ade7ce21
170
174.0
3.0G
/systen, slice
748
5.3
4.1G
/systen.slice/daenontools,service
422
4. 0
2,8G
/docker/dc277...42ab0603bbda2acBaf67996b
160
2.5
2.3G
/user,slice
5
2. 0
34,5M
/user,s11ce/user-0_s11ce
2.0
15 , 7K
/use,slice/u...*slice/session-c26,scope
3
2.0
13.3M
/docker/ab452.._c946f844Tc2a4184f3ccff2a
174
1.0
6.3G
/dockec/e18bd..-26ffdd7368b870aa3d1deb7a
156
0.8
2.9G
[.--]
---
## Page 715
678
Chapter 15 Containers
This output shows that a cgroup named /docker/dcf3a.* is consuming 610.5% total CPU for this
update interval (across many CPUs) and 24 Gbytes of main memory, with 200 running tasks. The
output also shows a number of cgroups created by systemd for system services (/system.slice) and
user sessions (/user.slice).
15.2.4 kubectl top
The Kubernetes container orchestration system provides a way to check basic resource usage using
kubectl top. Checking hosts (*nodes°):
+ kubeetl top nodes
BAME
CPO (cores)
CPO%
D40RY (bytes)
HEHORY1
bgreggi03cb3a7e46298b38e
1781m
10%
2880Mi
96
The °CPU(cores)° time shows cumulative milliseconds of CPU time, and *CPU%* shows the
current usage of the node. Checking containers (°pods°):
 kubectl top pods
KAME
CFV (cores)
MEMORY (bytes)
kubeznetes=b94cb5brf-p7)sp
73n
9M1
This shows the cumulative CPU time and current memory size.
These commands require a metrics server to be running, which may be added by default
depending on how you initialized Kubernetes [170]., Other monitoring tools can also display these
metrics in a GUI, including cAdvisor, Sysdig, and Google Cloud Monitoring [171]
15.2.5 docker stats
The Docker container technology provides some docker(1) analysis subcommanxds, including
stats. For example, from a production host:
+ docker stats
CONTAINER
CPU s
HEX USAGE / LIXIT
HEX 5
NET I/0
BL0CK I/0
PIDS
353426a09db1
526,81
4.061 GiB / 8.5 GiB
47,78% 0 3 / 0 B 2.818 MB / 0 3
247
Ebf166a66e08
303.821
3,448 GIB / 8.5 GIB
40.57%
8 0 / E 0
8 0 / 8H ZEO*Z
267
ceopaggropes
41.01
1.322 GiB/ 2.5 GiB
52,89%
 0 / 0  0 /E 0
22 9
61061566ffe5
85.921
220 .9 MIB / 3.023 GIB T.14%
1.204 GiB / 3.906 GiB 30.82%0 3 / 0 B 4.35 MB / 0 B
D B / 0 B43.4 MB / 0 B
61
bdc721460293
2.699
66
[.--]
This shows that a container with UUID *353426a09db1* was consuming a total of S27% CPU for
this update interval and was using four Gbytes of main memory versus an 8.5 Gbyte limit. For this
interval there was no network I/O, and only a small volume (Mbytes) of disk I/O.
---
## Page 716
15.2 Traditional Tools
679
15.2.6/sys/fs/cgroups
This directory contains virtual files of cgroup statistics. These are read and graphed by various
container monitoring products. For example:
# cd /sys/fs/cgroup/cpu, cpuacct/dockex/02a7ct65c82e3f3e75283944caa4462e82f...
abeen'oendo sco
161581 6262506
 cst opu.stat
L0s spotxedxu
nr_throttled 74
thrott1ed_tine 3816445175
The cpuacct.usage file shows the CPU usage of this cgroup in total nanoseconds. The cpu.stat
file shows the number of times this cgroup was CPU throttled (nr_throttled), as well as the total
throttled time in nanoseconds. This example shows that this cgroup was CPU throttled 74 times
out of 507 time periods, for a total of 3.8 throttled seconds.
There is also a cpuacct.usage_percpu, this time showing a Kubernetes cgroup:
# cd /sys/fs/cgroup/cpu, cpuacct/kubepods/buzstable/pod82e745...
+ cst cpuacct.usage_percpu
35874604278 37378190414 35464528409 35291309575 35829280 628 36105557113 36538524246
37944772821 35729154566 35996200949 36443793055 36517861942 36156377488 36176348313
360772 97144 35976388595
The output includes 16 fields for this 16-CPU system, with total CPU time in nanoseconds
These cgroupv1 metrics are documented in the kernel source under Documentation/cgroup-v1/
cpuacet.txt [172].
15.2.7perf
The perf(1) tool, introduced in Chapter 6, can be run from the host and can filter on cgroups
using =cgroup (G) . This can be used for CPU profiling, for example, with the perf record
subcommand:
perf record -F 99 -e cpu-clock --cgroup=docker/1d567... -a -- sleep 30
ssis Supnpo 'xauoo ssaood t sinooo peq uste aq ueo puaa au
This switch is also available with the perf stat subcommand, so that counts of events can be
collected instead of writing events to the perf.data file. For example, counting the read family of
syscalls and showing a different format of cgroup specification (with identifiers elided):
--- /-*pees/eoTts*saeuteauoo/ dnox6o--peex"xegue"s/s:stteose e- 1eae gxed
Multiple cgroups can be specified.
perf(1) can trace the same events that BPF can, although without the programmatic capabilities that
BCC and bpftrace provide. perf(1) does have its own BPF interface: an example is in Appendix D. For
[] aed sardsuexa jad Au aas °uogxadsut auequoo o papdepe aq ue peu μad jo sasn ato
---
## Page 717
680
Chapter 15 Containers
15.3
BPF Tools
This section covers the BPF tools you can use for container performance analysis and
troubleshooting. These are either from BCC or were created for this book. Table 15-3 lists the
tool origins.
Table 15-3
Container-Specific Tools
Tool
Source
Target
Description
runqlat
BCC
Sched
Summarize CPU run queue latency by PID namespace
pidnss
Book
Sched
Count PID namespace switches: containers sharing a CPU
b1kthrot
Book
Block I/0
Count block I/O throttles by blk cgroup
overlayfs
Book
Overlay FS
Show overlay FS read and write latency
with the :
chapters.
15.3.1 runqlat
runqlat(8) was introduced in Chapter 6: it shows run queue latency as a histogram, helping to
identify CPU saturation issues. It supports a =p1 dn.ss option to show the PID namespace. For
example, on a production container system:
hostf runqlat --pidnss -n
Tracing run queve latency...
Hit Ctrl-C to end.
°C
pidns - 4026532382
: count
distrlbutlon
0 > 1
: 646
E 7
:48
| **
8 -> 15
: 17
| *
1.6 > 31
: 150
∈9  1
: 264
|+********+
E<-z
: 0
[. . - ]
---
## Page 718
15.3 BPF Tools
681
This shows that one PID namespace (4026532382) is suffering much higher run queue latency
than the other.
specific to the container technology used. At the very least, the Is(1) command can be used as the
root user to determine the namespace for a given PID. For example:
 1s -1h /proc/181/ns/pid
 [9ceT90]=pd, <- pTd/u/Tet/o.d/ 0S=et 9 e 0 qoo qooa t xxaxxA3
This shows that PID 181 is running in PID namespace 4026531836.
15.3.2 pidnss
detecting a PID namespace switch during a scheduler context switch. This tool can be used to
confirm or exonerate issues of multiple containers contending for a single CPU. For example:
aq'ssuptd 
Attach.ing 3 probes...
Tracing PID nanespace sxitches. Ctrl-C to end
^C
VIctin PID namespace svitch counts [PIDNS, nodenane] :
[0, 1: 2
e[4026532 981, 6280172ea7b9] : 27
[4026531636, bgregg-i03cb3a7e46298b38e] : 28
The output shows two fields and then a switch count. The fields are the PID namespace ID
and the nodename (if present). This output shows a PID namespace with the nodename
*bgregg-i-03cb3a7e46298b38e” (the host) switched to another namespace 28 times while
tracing, and another with nodename *6280172ea7b9* (a Docker container) switched 27 times.
These details can be confirmed from the host:
U- eueun 
bgregg103cb3a7e46298b38e
+ docker ps
CONTAINER ID
IHAGE
COMNAND
CREATED
STATOS
PORTSKANES
6280172ea7b9
nqunqn
4 weeks ago
sxaan 6 4n
eager_bhaskata
[. - -]
---
## Page 719
682
Chapter 15 Containers
This works by tracing the kernel context switch path using kprobes. The overhead is expected to
become significant for busy I/O workloads.
Here is another example, this time during the setup of a Kubernetes cluster:
aq'ssuprd 
Attach.ing 3 probes...
Tracing PID nanespace sxltches. Ctrl-C to end
C
Victin PID namespace svitch counts [PIDNS, nodenane] :
[268434577, ci1ium-operator=95ddbb5fcgkspv] : 33
[268434291, c111un=etcd-g9vgxqanJv] : 35
[268434650, coredns-fb8b8dccf×7khx] : 35
[268434505, default-sen-deno] : 36
[268434723, coredns-fb8b8dccf=crrn9] : 36
[268434509, etcd-operatox-797978964-7c2nc] : 38
[268434513, kubernetes=b94cb9bff=p7jsp] : 39
e[268434810, bgregg-↓-03cb3a7e46298b38e : 203
[..]
[268434222, c111un-etcd-g9wgxqanJv] : 597
[268434295, etcd-operator-797978 964-7c2nc] : 1301
e[268434808, bgregg-↓-03cb3a7e46298b38el : 1582
[268434297, ci1ium-operator-95ddbb5fc=gkspv] : 3961