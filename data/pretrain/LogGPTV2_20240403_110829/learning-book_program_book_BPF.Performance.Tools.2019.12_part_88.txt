## Page 713
### Chapter 15: Containers

#### 15.1.3 Strategy
If you are new to container analysis, it can be challenging to know where to startâ€”both in terms of which target to analyze and which tool to use. Here is a suggested strategy for container analysis. The following sections will provide more detailed information on the tools involved.

1. **Examine System for Resource Bottlenecks**: Begin by examining the system for hardware resource bottlenecks and other issues covered in previous chapters (e.g., Chapter 6, Chapter 7). Create CPU flame graphs for running applications.
2. **Check cgroup Limits**: Verify if any cgroup software limits have been reached.
3. **Use BPF Tools**: Browse and execute the BPF tools listed in Chapters 6 to 14.

Most container issues I've encountered were caused by application or hardware problems, rather than container configuration. CPU flame graphs often reveal application-level issues unrelated to the container environment. Ensure you check for these issues, as well as investigating container-specific limits.

#### 15.2 Traditional Tools
Containers can be analyzed using various performance tools covered in earlier chapters. This section summarizes the use of traditional tools for analyzing container specifics from both the host and within the containers.

##### 15.2.1 From the Host
For analyzing container-specific behavior, especially the usage of cgroups, several tools and metrics can be used from the host. Table 15-1 lists some of these tools.

**Table 15-1: Traditional Host Tools for Container Analysis**

| Tool               | Type                | Description                              |
|--------------------|---------------------|------------------------------------------|
| systemd-cgtop      | Kernel statistics   | Top for cgroups                          |
| kubectl top        | Kernel statistics   | Top for Kubernetes resources             |
| docker stats       | Kernel statistics   | Resource usage by Docker container       |
| /sys/fs/cgroups    | Kernel statistics   | Raw cgroup statistics                    |
| perf               | Statistics and tracing | Multi-tool tracer that supports cgroup filters |

These tools are summarized in the following sections.

##### 15.2.2 From the Container
Traditional tools can also be used within the containers themselves, but note that some metrics may refer to the entire host rather than just the container. Table 15-2 lists the state of commonly-used tools for a Linux 4.8 kernel.

**Table 15-2: Traditional Tools When Run from the Container**

| Tool         | Description                                          |
|--------------|------------------------------------------------------|
| top(1)       | Process table shows container processes; summary heading shows the host |
| ps(1)        | Shows container processes                            |
| uptime(1)    | Shows host statistics, including host load averages   |
| mpstat(1)    | Shows host CPUs and host CPU usage                    |
| vmstat(8)    | Shows host CPUs, memory, and other statistics         |
| iostat(1)    | Shows host disks                                     |
| free(1)      | Shows host memory                                    |

The term "container-aware" is used to describe tools that, when run from the container, show only the container processes and resources. None of the tools in this table are fully container-aware, which is a known issue for performance analysis within containers. This may change as the kernel and these tools are updated.

#### 15.2.3 systemd-cgtop
The `systemd-cgtop` command displays the top resource-consuming cgroups. For example, from a production container host:

```sh
$ systemd-cgtop
Control Group                  Tasks  %CPU  Input/s  Output/s
/docker                       1089   798.2  45.9G    42.1G
/docker/dcf3a.._9d28fc4alc72bbaff4a24834  200   610.5  24.0G
/docker/370a3..-e64ca01198f1e843ade7ce21  170   174.0  3.0G
/system.slice                 748    5.3    4.1G
/system.slice/daenontools.service  422   4.0    2.8G
```

This output shows that a cgroup named `/docker/dcf3a.*` is consuming 610.5% total CPU for this update interval (across many CPUs) and 24 Gbytes of main memory, with 200 running tasks. The output also shows cgroups created by systemd for system services (`/system.slice`) and user sessions (`/user.slice`).

#### 15.2.4 kubectl top
Kubernetes provides a way to check basic resource usage using `kubectl top`. Checking hosts (nodes):

```sh
$ kubectl top nodes
NAME                         CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
bgreggi03cb3a7e46298b38e     1781m        10%    2880Mi          96%
```

Checking containers (pods):

```sh
$ kubectl top pods
NAME                         CPU(cores)   MEMORY(bytes)
kubeznetes=b94cb5brf-p7jsp   73n          9M1
```

These commands require a metrics server to be running, which may be added by default depending on how you initialized Kubernetes. Other monitoring tools, such as cAdvisor, Sysdig, and Google Cloud Monitoring, can also display these metrics in a GUI.

#### 15.2.5 docker stats
Docker provides the `docker stats` command for container analysis. For example, from a production host:

```sh
$ docker stats
CONTAINER ID   CPU %   MEM USAGE / LIMIT   MEM %   NET I/O   BLOCK I/O   PIDS
353426a09db1   526.81  4.061 GiB / 8.5 GiB  47.78%  0 B / 0 B  2.818 MB / 0 B  247
Ebf166a66e08   303.82  3.448 GiB / 8.5 GiB  40.57%  0 B / 0 B  0 B / 0 B         267
ceopaggropes   41.01   1.322 GiB / 2.5 GiB  52.89%  0 B / 0 B  0 B / 0 B         229
61061566ffe5   85.92   220.9 MiB / 3.023 GiB  1.14%  1.204 GiB / 3.906 GiB  30.82%  0 B / 0 B  4.35 MB / 0 B  61
bdc721460293   2.699   66
```

This shows that a container with UUID `353426a09db1` was consuming a total of 527% CPU for this update interval and was using four Gbytes of main memory versus an 8.5 Gbyte limit. There was no network I/O, and only a small volume (Mbytes) of disk I/O.

#### 15.2.6 /sys/fs/cgroups
This directory contains virtual files of cgroup statistics, which are read and graphed by various container monitoring products. For example:

```sh
# cd /sys/fs/cgroup/cpu,cpuacct/docker/02a7ct65c82e3f3e75283944caa4462e82f...
cat cpu.stat
nr_periods  161581
nr_throttled  74
throttled_time  3816445175
```

The `cpuacct.usage` file shows the CPU usage of this cgroup in total nanoseconds. The `cpu.stat` file shows the number of times this cgroup was CPU throttled (`nr_throttled`), as well as the total throttled time in nanoseconds. This example shows that this cgroup was CPU throttled 74 times out of 507 time periods, for a total of 3.8 throttled seconds.

There is also a `cpuacct.usage_percpu` file, showing per-CPU usage:

```sh
# cd /sys/fs/cgroup/cpu,cpuacct/kubepods/buzstable/pod82e745...
cat cpuacct.usage_percpu
35874604278 37378190414 35464528409 35291309575 35829280628 36105557113 36538524246
37944772821 35729154566 35996200949 36443793055 36517861942 36156377488 36176348313
36077297144 35976388595
```

The output includes 16 fields for this 16-CPU system, with total CPU time in nanoseconds. These cgroupv1 metrics are documented in the kernel source under `Documentation/cgroup-v1/cpuacct.txt`.

#### 15.2.7 perf
The `perf` tool, introduced in Chapter 6, can be run from the host and can filter on cgroups using `--cgroup`. This can be used for CPU profiling, for example, with the `perf record` subcommand:

```sh
perf record -F 99 -e cpu-clock --cgroup=docker/1d567... -a -- sleep 30
```

This switch is also available with the `perf stat` subcommand, so that counts of events can be collected instead of writing events to the `perf.data` file. For example, counting the `read` family of syscalls and showing a different format of cgroup specification (with identifiers elided):

```sh
perf stat -e 'syscalls:sys_enter_read' --cgroup=docker/1d567...
```

Multiple cgroups can be specified. `perf` can trace the same events that BPF can, although without the programmatic capabilities that BCC and bpftrace provide. `perf` does have its own BPF interface, as shown in Appendix D.

#### 15.3 BPF Tools
This section covers the BPF tools you can use for container performance analysis and troubleshooting. These tools are either from BCC or were created for this book. Table 15-3 lists the tool origins.

**Table 15-3: Container-Specific Tools**

| Tool         | Source | Target  | Description                                      |
|--------------|--------|---------|--------------------------------------------------|
| runqlat      | BCC    | Sched   | Summarize CPU run queue latency by PID namespace |
| pidnss       | Book   | Sched   | Count PID namespace switches: containers sharing a CPU |
| blkthrot     | Book   | Block I/O | Count block I/O throttles by blk cgroup          |
| overlayfs    | Book   | Overlay FS | Show overlay FS read and write latency           |

#### 15.3.1 runqlat
`runqlat` was introduced in Chapter 6: it shows run queue latency as a histogram, helping to identify CPU saturation issues. It supports a `--pidns` option to show the PID namespace. For example, on a production container system:

```sh
host$ runqlat --pidns -n
Tracing run queue latency...
Hit Ctrl-C to end.
^C
pidns - 4026532382
: count
distribution
0 > 1 : 646
1 > 7 : 48
8 > 15 : 17
16 > 31 : 150
32 > 63 : 264
```

This shows that one PID namespace (4026532382) is suffering much higher run queue latency than the other. At the very least, the `ls` command can be used as the root user to determine the namespace for a given PID. For example:

```sh
ls -l /proc/181/ns/pid
[9ceT90]=pid -> 'pd/u/Tet/o.d/0S=et9e0qooqooatxxaxxA3'
```

This shows that PID 181 is running in PID namespace 4026531836.

#### 15.3.2 pidnss
`pidnss` detects a PID namespace switch during a scheduler context switch. This tool can be used to confirm or exonerate issues of multiple containers contending for a single CPU. For example:

```sh
Attaching 3 probes...
Tracing PID namespace switches. Ctrl-C to end
^C
Victim PID namespace switch counts [PIDNS, nodename] :
[0, 1: 2
[4026532981, 6280172ea7b9] : 27
[4026531636, bgregg-i03cb3a7e46298b38e] : 28
```

The output shows two fields and then a switch count. The fields are the PID namespace ID and the nodename (if present). This output shows a PID namespace with the nodename `bgregg-i-03cb3a7e46298b38e` (the host) switched to another namespace 28 times while tracing, and another with nodename `6280172ea7b9` (a Docker container) switched 27 times. These details can be confirmed from the host:

```sh
hostname
bgregg103cb3a7e46298b38e
docker ps
CONTAINER ID  IMAGE  COMMAND  CREATED  STATUS  PORTS  NAMES
6280172ea7b9  nginx  "nginx -g 'daemon ofâ€¦"  4 weeks ago  Up 6 days  eager_bhaskata
```

This works by tracing the kernel context switch path using kprobes. The overhead is expected to become significant for busy I/O workloads.

Here is another example, this time during the setup of a Kubernetes cluster:

```sh
Attaching 3 probes...
Tracing PID namespace switches. Ctrl-C to end
^C
Victim PID namespace switch counts [PIDNS, nodename] :
[268434577, cilium-operator-95ddbb5fcgkspv] : 33
[268434291, cilium-etcd-g9vgxqanJv] : 35
[268434650, coredns-fb8b8dccfÃ—7khx] : 35
[268434505, default-sen-deno] : 36
[268434723, coredns-fb8b8dccf=crrn9] : 36
[268434509, etcd-operatox-797978964-7c2nc] : 38
[268434513, kubernetes-b94cb9bff=p7jsp] : 39
[268434810, bgregg-â†“-03cb3a7e46298b38e] : 203
[268434222, cilium-etcd-g9wgxqanJv] : 597
[268434295, etcd-operator-797978964-7c2nc] : 1301
[268434808, bgregg-â†“-03cb3a7e46298b38el] : 1582
[268434297, cilium-operator-95ddbb5fc=gkspv] : 3961
```

This output shows the frequency of PID namespace switches between different containers and the host during the setup of a Kubernetes cluster.