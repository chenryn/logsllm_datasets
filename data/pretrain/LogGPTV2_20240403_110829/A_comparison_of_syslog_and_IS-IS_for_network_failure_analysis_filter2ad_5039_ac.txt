pseudo-failures that signal a connection reset, as we explore in Sec-
tion 4.3.
From examining descriptive statistics and graphical plots, both
syslog-inferred and IS-IS listener-reported failures exhibit simi-
lar characteristics. The differences stem from certain classes of
failures—most notably short failures—which are poorly captured
by syslog. Moreover, when we compare distributions for good-
ness of ﬁt (using the two-tailed Kolmogorov-Smirnov statistic, or
KS test) we ﬁnd that syslog and IS-IS produce consistent data for
failures per link as well as link downtime, but not failure duration.
Given that Syslog misses 20% of all failures and 25% of all
downtime, it is not suitable for those who require failure-for-failure
accounting. At the same time, our results indicate that using sys-
log to determine link state information may be sufﬁcient for those
whose applications only require the reconstructed link state to have
the same gross statistical properties as those directly reported by
the IS-IS listener.
4.3 Syslog’s False Positives
Most of the statistical properties about failures reported by sys-
log closely track those reported by IS-IS, but a few—like individual
failure duration—do not. In this section we examine the causes and
enumerate which data ﬁltering mechanisms can be successfully ap-
plied in an effort to increase the number of statistics syslog can
accurately convey.
Table 6: Absolute count of ambiguous state changes by cause and
state change direction.
Cause
Lost Message
Spurious Retransmission
Unknown
Total
Down Up
174
194
28
240
27
0
202
461
When comparing syslog failures to IS-IS listener-reported fail-
ures, we ﬁnd that syslog reports 2,440 failures (21% of all syslog-
reported failures) and 17.5 hours that were not observed by the IS-
IS listener2. These failures are actually false positives, i.e. failures
that seemingly did not impact trafﬁc.
Short failures, ten seconds or less, make up 83% of all false pos-
itives. These failures, however, lead to less than an hour of down-
time. On the other hand, 94% of the false positive downtime be-
longs to the remaining 373 failures. Interestingly, all but 19 failures
greater than ten seconds—a full 15.1 hours worth of downtime—
occur during periods of ﬂapping, when a link fails multiple times
in rapid succession.
False positives occur for a number of reasons. One issue, which
produces very short duration false positives, one second or less, is
an aborted IS-IS three-way handshake. A second issue which also
causes very short failure, one second or less, is an adjacency being
reset which often occurs immediately after a longer failure without
an LSP being generated and, thus, is not seen by the IS-IS listener.
A reset adjacency failure is differentiated from a subsequent link
failure by the type of syslog message being sent. Further compli-
cating the issue is the fact that, as highlighted in Section 4.1, syslog
message generation reliability is signiﬁcantly impacted during pe-
riods of link ﬂapping.
While there are many different causes of very short failures, we
are aware of only one cause of long false positives. These occur
when both an “Up” and then a subsequent “Down” message are
lost such that two short failures become one long failure.
Ambiguous state changes
A failure in syslog is deﬁned by a “Down” message followed by an
“Up” message. However we ﬁnd that there are 461 down messages
that are preceded by an other down message and 202 up messages
2Table 4 implies that Syslog has 383 hours of false positive down-
time, however, 365.5 hours belong to failures that only partially
overlap with failures seen by the IS-IS listener.
preceded by an up message. The link state between repeated mes-
sages is ambiguous because we cannot determine if a message was
lost or if the message was a spurious reminder of link state.
In
aggregate, these ambiguous periods between double down and up
messages account for 7.8% of the measurement period across all
links.
Testing if a double up/down has occurred due to a missing syslog
message is straightforward with IS-IS data. A message has been
lost if both syslog state change messages correspond to the correct
state change as seen by IS-IS. We ﬁnd that 42% of all double down
periods are caused by a lost syslog up message while 86% of all
double up periods are caused by a lost down message, see Table 6
.
In total, lost packets explain 56% of all double up and down
periods.
Next, we test if the remaining unexplained nonsensical state
changes are due to spurious retransmission. To do this we check
the remaining double down and up messages to see if they occurred
while the link was in the same state. In fact 52% of all double down
message—91% of those not explained by a lost syslog up—occur
during a failure according to IS-IS and 14% of all double ups—all
of those not explained by a lost syslog down—during up time. In-
terestingly 99% of spurious down messages are reporting the same
failure as the previously received state change message; this is true
for only 48% of the double ups.
We now turn from investigating the causes of the nonsensical
state transitions to how to correct for them. Previous work ignored
the time periods between these transitions, but with the help of IS-
IS control data as ground truth, we can explore better strategies.
We believe there are three potential options: assume the link is
down, assume the link is up, or assume the link is in the previous
state. After examining the three strategies we ﬁnd that assuming
the link remains in the previous state pushes link downtime as seen
by syslog closest to matching link downtime as seen by IS-IS.
4.4
Isolating Failures
The statistical similarity of individual failure events according
to syslog and IS-IS is one thing, but many real-world metrics are
aggregates of multiple events. Hence, any error in reconstruction
has the potential to be magniﬁed when focusing on a high-level
metric, such as customer availability.
We highlight the ampliﬁcation issue with regards to customer
availability since CENIC’s primary value as an ISP is in provid-
ing connectivity to its customers. Therefore, reliability is best
gauged not from statics about individual network failures, but in-
stead through customer availability. Because most customers are
multi-homed and CENIC has rings in its topology, detecting con-
nectivity losses require accurate state information about multiple
links simultaneously.
CENIC advertises a single /16 block into BGP for all of its cus-
tomers, so we cannot use BGP monitors to give us insight or con-
ﬁrmation when a customer has been isolated. To determine when
customers become isolated, we use the network topology (recon-
structed from router conﬁguration ﬁles) to identify the set of links
that would isolate a customer.
During our study period, there are 1,401 failure events observed
by IS-IS that isolate a customer. Here an event is one or more
overlapping link failures. These isolating events affect 74 distinct
customers and result in a combined total of 26 days of isolation; see
Table 7. In syslog-reconstructed failures, there are 1,060 distinct
isolating events affecting 67 customer sites resulting in 22.4 days
of downtime. Syslog-reconstructed failure events are not a perfect
subset of IS-IS events, however: There are 58 events reported by
syslog which are not observed in the IS-IS data.
Table 7: Number and duration of failures in which at least one
CENIC customer was isolated from the backbone, as reconstructed
from syslog and IS-IS.
Data Source
IS-IS
Syslog
Intersection
Isolating
Events
1,401
1,060
1,002
Sites
Impacted
74
67
66
Downtime
(days)
26.3
22.3
19.8
Of these 58 unmatched events, 12 have no IS-IS-reported failures
on the affected links during the event, while the remaining 46 inter-
sect (but do not match perfectly) some IS-IS failures on the affected
links. There are also two particularly egregious “matches.” In one
case a site is isolated for 7 hours; syslog, however, only detects the
isolation nine seconds before the isolation ended. In a second case,
syslog believes a site isolated for 17 hours; the site was actually
isolated for less than one minute according to IS-IS.
There were 399 events—corresponding to 6.5 days of
downtime—reported by IS-IS that did not match an event recon-
structed from syslog. Of these, 82 were a result of syslog miss-
ing a single state change message. These 82 events account for
2.1 days (32%) of downtime. Furthermore, 99 of the remaining
missed isolating events partially matched a syslog-reconstructed
event.3 These 99 events account for 0.7 days (11%) of downtime.
The remaining 218 unmatched isolating events had no related (or
potentially related) Syslog messages.
5. CONCLUSION
This study represents the ﬁrst attempt to compare the failure pat-
terns reported by syslog-based analyses to those extracted through
direct IGP monitoring. We ﬁnd that there is signiﬁcant disagree-
ment between the two sources, with roughly one quarter of all
events reported by one data source not appearing in the other.
Clearly, IS-IS monitoring is more accurate, as trafﬁc shares fate
with the routing protocol. That said, our analysis indicates that sys-
log’s omissions are heavily biased toward short failures, and that
many of the larger statistical properties of the network obtained
through analyzing syslog, e.g., annualized downtime, number of
failures, and time to repair, are reasonably accurate. Still, one must
be careful in drawing high-level conclusions; for example syslog
has a signiﬁcantly different view of customer isolation than that of
IS-IS.
In sum, syslog-based analyses may be useful for capturing ag-
gregate failure characteristics where IGP data is not available. It
is less well suited to situations requiring more precise failure-for-
failure accounting.
Acknowledgments
This work was supported in part by the UCSD Center for Net-
worked Systems and the National Science Foundation through
grant CNS-1116904. The authors would like to thank Brian Court,
Darrell Newcomb, Jim Madden, Erick Sizelove, and our shepherd,
Theophilus Benson, for their advice and suggestions.
6. REFERENCES
[1] COATES, M., CASTRO, R., AND NOWAK, R. Maximum
likelihood network topology identiﬁcation from edge-based
3A partial event match occurs when an IS-IS and syslog failure
intersect, but do not match on start and end time exactly.
unicast measurements. In Proceedings of ACM
SIGMETRICS (June 2002).
[2] DHAMDHERE, A., TEIXEIRA, R., DOVROLIS, C., AND
DIOT, C. NetDiagnoser: Troubleshooting network
unreachabilities using end-to-end probes and routing data. In
Proceedings of ACM CoNEXT (Dec. 2007).
[3] DUFFIELD, N. Network tomography of binary network
performance characteristics. IEEE Transactions on
Information Theory (Dec. 2006).
[4] GILL, P., JAIN, N., AND NAGAPPAN, N. Understanding
network failures in data centers: Measurement, analysis, and
implications. In Proceedings of ACM SIGCOMM (Aug.
2011).
[5] HUANG, Y., FEAMSTER, N., AND TEIXEIRA, R. Practical
issues with using network tomography for fault diagnosis.
Computer Communication Review (October 2008).
[6] KOMPELLA, R. R., YATES, J., GREENBERG, A., AND
SNOEREN, A. C. Detection and localization of network
black holes. In Proceedings of IEEE INFOCOM (May 2007).
[7] LABOVITZ, C., AHUJA, A., AND JAHANIAN, F.
Experimental study of Internet stability and backbone
failures. In Proceedings of FTCS (June 1999).
[8] LONVICK, C. The BSD syslog protocol. RFC 3164, August
2001.
[9] MAHIMKAR, A., YATES, J., ZHANG, Y., SHAIKH, A.,
WANG, J., GE, Z., AND EE, C. T. Troubleshooting chronic
conditions in large IP networks. In Proceedings of ACM
CoNEXT (Dec. 2008).
[10] MAHIMKAR, A. A., GE, Z., SHAIKH, A., WANG, J.,
YATES, J., ZHANG, Y., AND ZHAO, Q. Towards automated
performance diagnosis in a large iptv network. In
Proceedings of the ACM SIGCOMM (Aug. 2009).
[11] MAO, Y., JAMJOOM, H., TAO, S., AND SMITH, J. M.
NetworkMD: Topology inference and failure diagnosis in the
last mile. In Proceedings of ACM IMC (Oct. 2007).
[12] MARKOPOULOU, A., IANNACCONE, G.,
BHATTACHARYYA, S., CHUAH, C.-N., GANJALI, Y., AND
DIOT, C. Characterization of failures in an operational IP
backbone network. Transactions on Networking (Aug. 2008).
[13] MORTIER, R. PyRT: Python routeing toolkit.
https://github.com/mor1/pyrt.
[14] POTHARAJU, R., AND JAIN, N. An empirical analysis of
intra- and inter-datacenter network failures for
geo-distributed services. In Proceedings of ACM
SIGMETRICS (June 2013).
[15] QIU, T., GE, Z., PEI, D., WANG, J., AND XU, J. What
happened in my network: mining network events from router
syslogs. In Proceedings of ACM IMC (Nov. 2010).
[16] SHAIKH, A., ISETT, C., GREENBERG, A., ROUGHAN, M.,
AND GOTTLIEB, J. A case study of OSPF behavior in a large
enterprise network. In Proceedings of ACM IMC (Nov.
2002).
[17] TURNER, D., LEVCHENKO, K., SNOEREN, A. C., AND
SAVAGE, S. California fault lines: Understanding the causes
and impact of network failures. In Proceedings of ACM
SIGCOMM (Aug. 2010).
[18] WANG, F., MAO, Z. M., WANG, J., GAO, L., AND BUSH,
R. A measurement study on the impact of routing events on
end-to-end Internet path performance. In Proceedings of
ACM SIGCOMM (Sept. 2006).
[19] WATSON, D., JAHANIAN, F., AND LABOVITZ, C.
Experiences with monitoring OSPF on a regional service
provider network. In Proceedings IEEE ICDCS (2003).
[20] XU, W., HUANG, L., FOX, A., PATERSON, D., AND
JORDAN, M. Detecting large-scale system problems by
mining console logs. In Proceedings of ACM SOSP (Oct.
2009).
[21] YAMANISHI, K., AND MARUYAMA, Y. Dynamic syslog
mining for network failure monitoring. In Proceedings of
ACM SIGKDD (Aug. 2005).
[22] ZHANG, M., ZHANG, C., PAI, V., PETERSON, L., AND
WANG, R. PlanetSeer: Internet path failure monitoring and
characterization in wide-area services. In Procedings of
USENIX OSDI (Dec. 2004).