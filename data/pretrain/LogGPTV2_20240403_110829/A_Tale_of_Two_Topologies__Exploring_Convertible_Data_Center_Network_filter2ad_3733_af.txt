the time for data serialization and deserialization. In the
Spark broadcast application, flat-tree global mode reduces
the average data read time by 10% and reduces the broadcast
phase duration by 16% compared to the Clos topology. In
the Hadoop shuffle application, the reduction in the average
data read time and in the shuffle phase duration are 10.5%
and 8% respectively. With this visible difference, we conclude
that the improvement of network topology can be reflected in
the application performance. The global mode only slightly
outperforms the local mode, because their network structures
are not hugely different at this small scale (Figure 2c vs. 2d).
The topologies of these two modes will become less alike as
the network scale grows, thus we expect more considerable
performance improvement to applications from the change
of topology in a large-scale data center.
6 RELATED WORK
Flat-tree is distinguished from other data center network
architectures such as [10–12, 23, 24, 41] by its convertibility.
Each of these fixed topologies has sweet spots for particular
traffic patterns [40], whereas flat-tree is able to convert the
topology to adapt to different workloads. These architectures
have varying implementation complexity and performance
properties. With the power of convertibility, flat-tree can be
implemented more easily like a Clos network and has better
performance like random graph networks.
Flat-tree also goes beyond the recent proposals of config-
urable data center network architectures. One group of works
creates ad-hoc links at run time to alleviate hot spots [19,
22, 25, 26, 43, 44, 48, 49, 51]. Another group constructs
an all-connected flexible network core with high bandwidth
capacity [3, 16, 17, 33, 34]. However, these solutions are con-
strained by the port count of central switches when enabling
configurability [16, 19, 43], the number of optical wavelengths
that can be reused [3, 16, 17, 33, 34], or the interference and
attenuation of wireless signals [22, 25, 26, 51]. Due to these
scalability concerns, only a small number of connections can
be added as a local remedy or the size of the network is
limited to a small scale. Flat-tree is the first architecture to
realize globally convertible data center networks at large scale.
It is fundamentally different from these previous work. First,
instead of adding new links to the network, it repurposes
existing links to increase the total bandwidth with more effi-
cient topologies. Second, rather than using central switches,
it distributes a set of small port-count converter switches
across the network to spread convertibility. Third, converter
switches simply pipe out data packets through wired channels,
making technologies for multiplexing signals or maintaining
Figure 11: Average data flow read duration (left y-axis) and av-
erage communication phase duration (right y-axis) in the Spark
broadcast and Hadoop shuffle applications under different flat-tree
topology modes
signal intensity unnecessary. Fourth, besides reconfiguring
switch-to-switch links as done in other proposals, flat-tree
also reconfigures server-to-switch links to facilitate greater
flexibility in the network structure.
7 CONCLUSION
Flat-tree is the first effort towards building convertible data
center networks. By converting between Clos and approxi-
mate random graph of various scales, it achieves the conven-
tionally conflicting goals of easy implementation and good
performance. Convertibility can be achieved by a set of small
port-count converter switches distributed across the network.
They have low cost and can be packaged into Pods to ease
deployment. We find flattening Clos’ tree structure does not
require global rewiring. With regular wiring patterns between
Pods and core switches and simple connections between ad-
jacent Pods, we effectively approximate randomness in the
network core and at the same time obtain low wiring com-
plexity. Multi-path routing and congestion control are crucial
to exploiting the path diversity in flat-tree, and we have
shown that aggregation strategies can be applied to avoid an
explosion of network states. Existing routing and transport
protocols combined with our architecture-specific state aggre-
gation schemes can balance between high network utilization
and fair bandwidth sharing among flows. We explore the
implementability of flat-tree using simulations with real data
center traffic and a testbed implementation of the system.
We observe flat-tree can optimize for diverse workloads with
different topology modes, and it brings performance improve-
ments to applications with greater core bandwidth. Flat-tree
is merely one design point in the broad space of convertible
data center networks. We believe our experience will motivate
future studies on convertibility.
ACKNOWLEDGEMENT
We would like to thank our shepherd Mohammad Alizadeh
and the anonymous reviewers for their thoughtful feedback.
This research was sponsored by the NSF under CNS-1422925
and CNS-1305379, an IBM Faculty Award, and by Microsoft
Corp.
REFERENCES
[1] 2009. MPTCP simulator. (2009). http://nrg.cs.ucl.ac.uk/mptcp/
implementation.html
[2] 2009. MultiPath TCP - Linux Kernel implementation. (2009).
http://multipath-tcp.org/pmwiki.php/Main/HomePage
(a) Spark broadcastGlobalLocalClosData flow read duration (ms)0510152017.0817.1719.1202.557.5108.969.329.74Communication phase durationData flow read duration(b) Hadoop shuffleGlobalLocalClosData flow read duration (sec)0123452.982.993.31Broadcast phase duration (sec)0123453.683.764.38Shuffle phase duration (sec)SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
Y. Xia et al.
[3] 2010. Plexxi. (2010). http://www.plexxi.com/
[4] 2012. OpenFlow Switch Specification, Version 1.3.0. Open Net-
working Foundation (2012).
[5] 2015. Coflow-Benchmark. (2015). https://github.com/coflow/
coflow-benchmark
[6] 2015. Segment Routing: Prepare Your Network for New Business
Models White Paper. Cisco Technology White Paper (2015).
[7] 2017. 40G short range transceiver. (2017). http://www.fs.com/
products/17931.html
[8] 2017. Facebook Network Analytics Data Sharing. (2017). https:
//www.facebook.com/groups/1144031739005495/
[9] 2017. Tez. (2017). https://tez.apache.org/
[10] H. Abu-Libdeh, P. Costa, A. Rowstron, G. O’Shea, and A. Don-
nelly. August 2010. Symbiotic Routing in Future Data Centers.
In SIGCOMM ’10. New Delhi, India, 51–62.
[11] J. H. Ahn, N. Binkert, A. Davis, M. McLaren, and R. S. Schreiber.
November 2009. HyperX: Topology, Routing, and Packaging of
Efficient Large-scale Networks. In SC ’09. Portland, OR, 1–11.
[12] M. Al-Fares, A. Loukissas, and A. Vahdat. August 2008. A
Scalable, Commodity Data Center Network Architecture. In SIG-
COMM ’08. Seattle, Washington, USA, 63–74.
[13] M. Alizadeh, A. Greenberg, D. A. Maltz, J. Padhye, P. Patel, B.
Prabhakar, S. Sengupta, and M. Sridharan. August 2010. DCTCP:
Efficient Packet Transport for the Commoditized Data Center. In
SIGCOMM’10.
[14] T. Benson, A. Anand, A. Akella, and M. Zhang. January 2010.
Understanding Data Center Traffic Characteristics. SIGCOMM
CCR 40, 1 (January 2010), 92–99.
[15] P. Bod´ık, I. Menache, M. Chowdhury, P. Mani, D. A. Maltz, and I.
Stoica. August 2012. Surviving Failures in Bandwidth-constrained
Datacenters. In SIGCOMM ’12. Helsinki, Finland, 431–442.
[16] K. Chen, A. Singla, A. Singh, K. Ramachandran, L. Xu, Y. Zhang,
X. Wen, and Y. Chen. April 2012. OSA: An Optical Switching
Architecture for Data Center Networks with Unprecedented Flex-
ibility. In NSDI ’12. San Jose, CA.
[17] K. Chen, X. Wen, X. Ma, Y. Chen, Y. Xia, C. Hu, and Q. Dong.
2015. WaveCube: A scalable, fault-tolerant, high-performance
optical data center architecture. In INFOCOM ’15. 1903–1911.
[18] W. Dally and B. Towles. 2003. Principles and Practices of
Interconnection Networks.
[19] N. Farrington, G. Porter, S. Radhakrishnan, H. H. Bazzaz, V.
Subramanya, Y. Fainman, G. Papen, and A. Vahdat. August
2010. Helios: A Hybrid Electrical/Optical Switch Architecture
for Modular Data Centers. In SIGCOMM ’10. New Delhi, India,
339–350.
[20] M. Fokine, L. E. Nilsson, ˚A. Claesson, D. Berlemont, L. Kjellberg,
L. Krummenacher, and W. Margulis. 2002.
Integrated Fiber
Mach–Zehnder Interferometer for Electro-Optic Switching. Optics
Letters 27, 18 (September 2002), 1643–1645.
[21] A. Ford, C. Raiciu, M. Handley, S. Barre, and J. Iyengar. 2011.
Architectural Guidelines for Multipath TCP Development. RFC
6182 (2011).
[22] M. Ghobadi, R. Mahajan, A. Phanishayee, N. Devanur, J. Kulka-
rni, G. Ranade, P. A. Blanche, H. Rastegarfar, M. Glick, and
D. Kilper. August 2016. ProjecToR: Agile Reconfigurable Data
Center Interconnect. In SIGCOMM ’16. Florianopolis, Brazil,
216–229.
[23] C. Guo, G. Lu, Da. Li, H. Wu, X. Zhang, Y. Shi, C. Tian, Y.
Zhang, and S. Lu. August 2009. BCube: A High Performance,
Server-centric Network Architecture for Modular Data Centers.
In SIGCOMM ’09. Barcelona, Spain, 63–74.
[24] C. Guo, H. Wu, K. Tan, L. Shi, Y. Zhang, and S. Lu. August
2008. DCell: A Scalable and Fault-Tolerant Network Structure
for Data Centers. In SIGCOMM ’08. Seattle, WA, USA, 75–86.
[25] D. Halperin, S. Kandula, J. Padhye, P. Bahl, and D. Wetherall.
August 2011. Augmenting Data Center Networks with Multi-
gigabit Wireless Links. In SIGCOMM ’11. Toronto, 38–49.
[26] N. Hamedazimi, Z. Qazi, H. Gupta, V. Sekar, S. R. Das, J. P.
Longtin, H. Shah, and A. Tanwer. August 2014. FireFly: A
Reconfigurable Wireless Data Center Fabric Using Free-space
Optics. In SIGCOMM ’14. Chicago, Illinois, USA, 319–330.
[27] K. He, J. Khalid, A. Gember-Jacobson, S. Das, C. Prakash, A.
Akella, L. E. Li, and M Thottan. 2015. Measuring Control Plane
Latency in SDN-enabled Switches. In SOSR ’15. Santa Clara,
CA, 1–6.
[28] C. Hopps. 2000. Analysis of an Equal-Cost Multi-Path Algorithm.
RFC 2992 (2000).
[29] S. A. Jyothi, M. Dong, and P. B. Godfrey. June 2015. Towards a
Flexible Data Center Fabric with Source Routing. In SOSR ’15.
Santa Clara, CA, 1–8.
[30] S. Kandula, S. Sengupta, A. Greenberg, P. Patel, and R. Chaiken.
November 2009. The Nature of Data Center Traffic. In IMC ’09.
Chicago, Illinois, USA, 202–208.
[31] S. Legtchenko, N. Chen, D. Cletheroe, A. Rowstron, H. Williams,
and X. Zhao. 2016. XFabric: A Reconfigurable In-rack Network
for Rack-scale Computers. In NSDI’16. Santa Clara, CA, 15–29.
[32] T. Leighton and S. Rao. November 1999. Multicommodity Max-
flow Min-cut Theorems and Their Use in Designing Approximation
Algorithms. J. ACM 46, 6 (November 1999), 787–832.
[33] Y. J. Liu, P. X. Gao, B. Wong, and S. Keshav. August 2014.
Quartz: A New Design Element for Low-latency DCNs. In SIG-
COMM ’14. Chicago, Illinois, USA, 283–294.
[34] G. Porter, R. Strong, N. Farrington, A. Forencich, P. Chen-Sun,
T. Rosing, Y. Fainman, G. Papen, and A. Vahdat. August 2013.
Integrating Microsecond Circuit Switching into the Data Center.
In SIGCOMM ’13. Hong Kong, China, 447–458.
[35] R. M. Ramos, M. Martinello, and C. Esteve Rothenberg. 2013.
SlickFlow: Resilient source routing in Data Center Networks un-
locked by OpenFlow. In LCN ’13. 606–613.
[36] E. Rosen, A. Viswanathan, and R. Callon. 2001. Multiprotocol
Label Switching Architecture. RFC 3031 (2001).
[37] C. Rotsos, N. Sarrar, S. Uhlig, R. Sherwood, and A. W. Moore.
2012. OFLOPS: An Open Framework for Openflow Switch Evalu-
ation. In PAM’12. Vienna, Austria, 85–95.
[38] A. Roy, H. Zeng, J. Bagga, G. Porter, and A. C. Snoeren. Au-
gust 2015. Inside the Social Network’s (Datacenter) Network. In
SIGCOMM ’15. London, UK, 123–137.
[39] A. Singh, J. Ong, A. Agarwal, G. Anderson, A. Armistead, R.
Bannon, S. Boving, G. Desai, B. Felderman, P. Germano, A.
Kanagala, J. Provost, J. Simmons, E. Tanda, J. Wanderer, U.
H¨olzle, S. Stuart, and A. Vahdat. August 2015. Jupiter Rising: A
Decade of Clos Topologies and Centralized Control in Google’s
Datacenter Network. In SIGCOMM ’15. London, UK, 183–197.
[40] A. Singla. Designing Data Center Networks for High Throughput.
Ph.D. Thesis. University of Illinois at Urbana-Champaign.
[41] A. Singla, C. Y. Hong, L. Popa, and P. B. Godfrey. April 2012.
Jellyfish: Networking Data Centers Randomly. In NSDI ’12. San
Jose, California, USA, 1–14.
[42] M. Soliman. 2015. Exploring Source Routing as an Alternative
Routing Approach in Wide Area Software-Defined Networks.
Ph.D. Dissertation. Carleton University Ottawa.
[43] G. Wang, D. G. Andersen, M. Kaminsky, K. Papagiannaki, T. S. E.
Ng, M. Kozuch, and M. Ryan. August 2010. c-Through: Part-time
Optics in Data Centers. In SIGCOMM ’10. New Delhi, India,
327–338.
[44] H. Wang, Y. Xia, K. Bergman, T. S. E. Ng, S. Sahu, and K.
Sripanidkulchai. 2013. Rethinking the Physical Layer of Data
Center Networks of the Next Decade: Using Optics to Enable
Efficient *-cast Connectivity. SIGCOMM CCR 43, 3 (July 2013),
52–58.
[45] D. Wischik, C. Raiciu, A. Greenhalgh, and M. Handley. 2011.
Design, Implementation and Evaluation of Congestion Control
for Multipath TCP. In NSDI’11. Berkeley, CA, USA, 99–112.
[46] M. C. Wu, O. Solgaard, and J. E. Ford. 2006. Optical MEMS for
Lightwave Communication. Journal of Lightwave Technology 24,
12 (December 2006), 4433–4454.
[47] Y. Xia and T. S. E. Ng. November 2016. Flat-tree: A Convertible
Data Center Network Architecture from Clos to Random Graph.
In HotNets ’16. Atlanta, GA, 71–77.
[48] Y. Xia, T. S. E. Ng, and X. Sun. April 2015. Blast: Accelerat-
ing High-Performance Data Analytics Applications by Optical
Multicast. In INFOCOM ’15. Hong Kong, China, 1930–1938.
[49] Y. Xia, M. Schlansker, T. S. E. Ng, and J. Tourrilhes. 2015. En-
abling Topological Flexibility for Data Centers Using OmniSwitch.
In HotCloud ’15. Santa Clara, CA.
[50] Jin Y. Yen. 1971. Finding the K Shortest Loopless Paths in a
Network. Management Science 17, 11 (1971), 712–716.
[51] X. Zhou, Z. Zhang, Y. Zhu, Y. Li, S. Kumar, A. Vahdat, B. Y.
Zhao, and H. Zheng. August 2012. Mirror Mirror on the Ceiling:
Flexible Wireless Links for Data Centers. In SIGCOMM ’12.
Helsinki, Finland, 443–454.