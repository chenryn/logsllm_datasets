packet timing and size information, indicating that the IPD
sequence delivered informative features for correlated ﬂows.
B. Window Partitioning
We tested two partition strategies, one based on time inter-
vals and the other based on the number of packets, and then
decided to use time intervals as the window interval, which
yielded a better triplet loss curve. This is because Tor ﬂows
generally had fewer packets per time period than exit ﬂows, so
that when using windows based on a ﬁxed number of packets,
the ﬂows for the second and subsequent windows no longer
corresponded to the same trafﬁc. We empirically determined
the number of windows and the interval in Section V-C.
t
(cid:108) d−t
t−δ
(cid:109)
We further explored overlapping window partitioning to
create overlapping windows with some interval overlap (δ)
between subsequent windows, which we refer to as δ-on
partitioning. As shown in Figure 4, when the window interval
length is t and total ﬂow duration is d, then δ-off partitioning
(cid:7) + 1 intervals, where window w is the interval
leads to(cid:6) d−t
[t×w, t×w+t). In contrast, δ-on results in
+1 intervals,
where window w is the interval [(t− δ)× w, (t− δ)× w + t).
For example, when t = 5, d = 25, and δ = 3, the 5 δ-off
windows are the intervals [0,5), [5,10), ..., [20,25), while the
11 δ-on windows are the intervals [0,5), [2,7), ..., [20,25). As
such, with δ-on, we create more windows, leading to more
training ﬂow pairs and boosting difference in TPs and FPs by
aggregating results from more windows. In other words, δ-on
increases the ampliﬁcation of DeepCoFFEA, improving the
performance dramatically, as demonstrated in Section VI-B.
C. Hyperparameter Optimization
The choice of hyperparameters is crucial to improve the
DeepCoFFEA performance, and particularly the behavior of
the FENs, to result in lower triplet loss. Thus, we explored
the parameter search spaces shown in Table II using one
Nvidia RTX 2080 and one Tesla P100 GPU. Although we
used the DCF set to tune these parameters, we noticed that the
DeepCoFFEA performance is not very sensitive to the dataset
when choosing these parameters. One exception is the input
dimension; the trafﬁc rate per window can change substantially
and thus, while other parameters in this section could be used
without further search, the input dimension should be adjusted
for new network conditions. Here we give more details for how
we selected the parameter ranges.
Window Setting Parameters. The length of ﬂow input to
FENs should be selected to optimize the model performance
within an acceptable range of training costs.
We ﬁrst
investigated the minimum duration of the ﬁrst
window interval whose median packet count was 100 pack-
ets. Note that 100 packets were chosen in DeepCorr as the
minimum ﬂow length they explored [25]. After computing the
median packet counts for intervals between 2 and 5 seconds
in Table VI of Appendix C, we determined that ﬁve seconds
would give the best performance.
Furthermore, we set the search space for the total ﬂow
duration up to 35 seconds since the packet count for windows
after 35 seconds became lower than 100 packets as shown in
Table VII of Appendix C. Second, with δ = 0, we explored
various total ﬂow durations since the DCF performance started
degrading after 25 seconds. Third, based on Table VIII of
Appendix C, we used the minimum and maximum ﬂow lengths
in 11 windows as the search space for Tor ﬂow length and exit
ﬂow length, which were [106,501] for Tor ﬂows and [244,855]
for exit ﬂows.
Window Partition Parameter. First, we investigated the
impact of δ-on/off settings for varying δ. As discussed in
+ 1 windows, that is, it
Section V-B, δ-on creates
creates 6 windows for δ = 1, 8 windows for δ = 2, and
11 windows for δ = 3. Note that we omitted δ = 4 since
with 21 windows the resulting cosine similarity matrix for
218,610 training ﬂow pairs was too large to compute using
our resources, so we could not select semi-hard negatives.
(cid:108) 25−5
(cid:109)
5−δ
We reported results of (cid:96) = 2 for the 3-on and the 2-on
and (cid:96) = 1 for the 1-on setting in Figure 5a. Even though
δ = 1 did not improve the DeepCoFFEA performance over δ-
off settings, by increasing δ, DeepCoFFEA was beneﬁted by
the enhanced ampliﬁcation capability with more training ﬂow
pairs as well as more voting results. Figure 5a shows that a
larger δ typically led to better ROC curves with higher TPRs
using more votes, indicating that more resourceful adversaries
could further improve the 3-on results by using the 4-on
setting. Based on the results shown in Figure 5a, we evaluated
DeepCoFFEA in the 3-on setting throughout Section VI.
Model Parameters. As the triplet loss aims to separate the
positive pair from the negative by a distance margin, α, we
ﬁrst tuned α to maximize the distinction between the cosine
similarity scores of the correlated pairs and those of the
uncorrelated pairs. With α=0.1, the FENs attained the lowest
loss and we noticed that TF [28] also used the same value.
Second, as discussed in Section IV-A, we implemented
our own triplet epoch generator which selected triplets for
the positive and negative networks from separate pools. We
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:58:24 UTC from IEEE Xplore.  Restrictions apply. 
1922
CHOSEN HYPER-PARAMETERS AND SEARCH SPACES USED IN THE
HYPER-PARAMETER OPTIMIZATION.
TABLE II
Param
Tor ﬂow size
Exit ﬂow size
total ﬂow duration
δ
α
Epoch generator
Output node
Optimizer
Learning rate
Correlation metric
Vote
Chosen Param
500
800
25
3
0.1
1
64
SGD
10−3
cosine
9
{5 · 10−2, . . . , 5 · 10−1}
Search Space
{106, . . . , 501}
{244, . . . , 855}
{20, . . . , 35}
{0, 1, 2, 3, 4}
{1, . . . , 10}
{10, . . . , 100}
SGD, Adam
{10−3, . . . , 10−4}
cosine, euclidean
{8, 9, 10, 11}
further tuned the number for how frequently those separate
pools needed to be updated (i.e., shufﬂed and divided into
two pools). Eventually, the FEN performance improved when
updating the pools more frequently. Thus, we recommend
updating them every epoch rather than every 2-10 epochs.
Finally, we further tuned the learning rate of SGD optimiza-
tion and the number of output nodes, which is the dimension
of the feature embeddings generated by the trained FENs.
Correlation Parameters. We had to decide a correlation
metric to be computed in the triplet loss function, so we
explored both cosine similarity and Euclidean distance metrics,
as proposed by previous research [26]. Interestingly, with the
Euclidean distance function, the loss never decreased.
Lastly, after computing the cosine similarity scores for all
testing pairs for each window, we found that DeepCoFFEA
performance was comparable with nine and 10 votes across
11 windows while the performance somewhat dropped with 8
and 11 votes (Figure 5b). So, we decided that the ﬂow pair
would be correlated if it had at least nine 1 votes across 11
windows since the DeepCoFFEA achieved slightly more TPs
against low FPRs less than 10−4.
D. Metrics
In this section, we introduce the deﬁnitions of the TPR, FPR
and BDR metrics used in Section VI.
• TPR: The true positive rate is the fraction of correlated
ﬂow pairs that are classiﬁed as “correlated”.
• FPR: The false positive rate is the fraction of uncorrelated
ﬂow pairs that are classiﬁed as “correlated”.
• BDR: The Bayesian detection rate in the ﬂow correlation
is the probability that a correlated pair is actually “cor-
related” given that the correlation function detected it as
“correlated” and it can be computed as:
P (C|P )P (P )
P (P|C) =
P (C|P )P (P ) + P (C|N )P (N )
,
where P (P ) is the probability that the pair is correlated,
P (C) is the probability that the pair will be decided
by the ﬂow correlation as correlated, and P (C|P ) and
P (C|N ) are replaced with TPR and FPR, respectively.
We note that precision is often used instead of BDR. We
chose BDR in this paper since the impact of the base rate
is shown more clearly in the computation.
Fig. 6. DeepCoFFEA (loss ≈ 0.0018) and DeepCorr performance with local
and global thresholds.
In addition, we measured the performance of state-of-the-art
attacks and DeepCoFFEA using ROC curves by varying the
correlation threshold parameter.
E. Similarity Thresholds
the number of exit
In DeepCoFFEA, the embedded feature correlation thresh-
old τ acts to control
traces that are
classiﬁed as “possibly correlated” with each Tor trace in
a given time window. We can either control
this number
indirectly by setting a global threshold τ that is applied to
all cosine similarities (so that, generally, as τ increases, fewer
pairs will be classiﬁed as possibly correlated); or we can
control this number directly by classifying only the closest
κ exit traces to a given Tor trace window ti (as measured by
cos(G(ti), H(xj))) as possibly correlated. This latter choice
corresponds to computing a local threshold for each ti by
sorting the list di,1, ....di,n, where di,j = cos(G(ti), H(xj)),
and selecting the κth element as the threshold for ti.
We explored both approaches and found that, as shown in
Figure 6, the number of TPs decreases more rapidly with
higher τ (those that yield FPR less than 10−3) than with higher
κ, most likely due to some windows in which many ﬂows have
similar embeddings. By selecting the thresholds locally (i.e.,
based on the pairwise score distribution of each Tor ﬂow),
DeepCoFFEA is able to detect more correlated ﬂows while
controlling the number of FPs.
We also evaluated DeepCorr using local thresholds even
though they adopted global thresholds in their paper. As shown
in Figure 6, the use of κ did not improve the performance of
DeepCorr and FPRs did not decrease below 10−3, indicating
that most correlation scores for uncorrelated pairs were similar
to scores of correlated pairs and decreasing κ did not help
reduce the false positives. Thus, we used τ for DeepCorr and
adopted κ for DeepCoFFEA as the curve parameter to generate
the ROC curves of DeepCoFFEA in Section VI.
VI. EXPERIMENT RESULTS
In this section, we explore the effect of various conﬁgura-
tions of DeepCoFFEA on its performance and compare the
effectiveness and efﬁciency of DeepCoFFEA to state-of-the-
art attacks.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:58:24 UTC from IEEE Xplore.  Restrictions apply. 
1923
(a) TPR, FPR, and BDR with varying positive counts
κ when loss ≈ 0.0018.
(b) ROC at various triplet loss values.
(c) ROC for old and future testing sets when loss ≈
0.0018.
(d) ROC for various defenses when loss ≈ 0.004. (e) ROC for various adversary models against de-
fenses when loss ≈ 0.004.
Fig. 7. Performance of DeepCoFFEA across various settings (Note that RG is Random Guess and all x-axes except Figure 7a are in log scale).
A. Experimental Settings
We implemented FEN models using Keras [36] with Tensor-
ﬂow [37] backend and used one Tesla P100 GPU with 16GB
memory to train and test DeepCoFFEA 2. We selectively chose
12,094 ﬂow pairs from the DCF set, from which we use 10,000
pairs for training data and the other 2,094 pairs for testing
data, with no overlapping circuit usage between the two sets.
In addition, all 12,094 connections went to unique sites (i.e.,
12,094 destinations). With this setting, we aim to demonstrate
that DeepCoFFEA successfully detects correlated ﬂows that
were collected using arbitrary circuits or sites. While we used
2,094 test connections for most of the experiments in this
section, DeepCoFFEA performs well even with 10,000 ﬂows,
as shown in Figure 8c.
It is likely in practice that users visit sites that are in the
training set, since samples from the most popular sites are
used for this purpose. Thus, DeepCoFFEA performance shown
in all experiments in this section may be better in practice.
Moreover, as Nasr, Bahramali and Houmansadr [25] point out,
collecting exit ﬂows using a SOCKS proxy may add some
latency, meaning that exit traces could be more distinct from
each other in practice, leading to further improvement.
B. DeepCoFFEA Effectiveness
We evaluate the impact of the different settings and ideas
behind DeepCoFFEA on the overall effectiveness of the attack.
2The source code and datasets are available on https://github.com/
trafﬁc-analysis/deepcoffea
TRAINING TIME TO REACH SPECIFIED TRIPLET LOSS VALUES.
TABLE III
triplet loss
training time (days)
0.006
0.96
0.004
0.003
0.002
3
5
14
Ampliﬁcation. One of the key advantages of DeepCoFFEA is
ampliﬁcation to reduce the number of FPs. This study sheds
light on the impact of ampliﬁcation by evaluating DeepCoF-
FEA on a per-window basis. Table IX of Appendix D shows
TPRs and FPRs when evaluating DeepCoFFEA using each
of 11 windows; both metrics were consistent across windows
with TPR at 97-98% for 12% FPR. Then we applied the voting
strategy to aggregate all 11 results, and DeepCoFFEA yielded
97.99% TPR and 0.13% FPR. Such a signiﬁcant drop in FPR
indicates that only a few positive predictions on unmatched
pairs were aligned across all 11 windows, which was exploited
by the ampliﬁcation technique to reduce the number of FPs.
Threshold Parameter. We evaluated the effect of the positive
correlation parameter κ, described in Section V-E. We com-
puted the number of TPs and FPs at for κ ranging from 4 to
1,255. Figure 7a shows that both the TPR and FPR increased
by increasing the threshold, while the BDR decreased.
Triplet Loss. When training FENs, the triplet loss decreased
monotonically with training time; our experiments halted train-
ing when the loss hit various loss values from 0.006 to 0.0018.
In this study, we investigate the performance of DeepCoFFEA
when training stops at different loss values; this experiment
gives insight on choosing a stopping point for training FENs.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:58:24 UTC from IEEE Xplore.  Restrictions apply. 
1924