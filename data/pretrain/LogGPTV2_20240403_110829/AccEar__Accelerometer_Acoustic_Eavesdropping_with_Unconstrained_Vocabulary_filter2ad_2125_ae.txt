[40], WGAN [41], etc.) or they do not achieve a one-to-one
mapping between inputs and outputs (such as CycleGAN [42],
StyleGAN [43], etc). To the best of our knowledge, cGAN is
the only variant that ﬁts the requirements of our task.
Our cGAN-based approach also has some limitations. In
particular, the inputs and outputs of our cGAN are image-
like two-dimensional data. Hence, we have to transform the
accelerometer data to spectrogram by using SFTF and then
transform the output Mel spectrogram to an audio waveform
by using the Grifﬁn-Lim algorithm. However, such transforma-
tion leads to the information loss of the signal phase, which
may distort the reconstructed audio. Furthermore, cGAN is
known for its difﬁculty of training in terms of model tuning
and computation overhead. In future work, we plan to explore
possible neural network-based approaches which can directly
process the time series data to avoid the information loss
caused by the spectrogram conversion.
VII. CONCLUSION
In this paper, we propose an accelerometer eavesdropping
system AccEar that reconstructs the audio played by the
built-in speaker from accelerometer data. With AccEar,
an adversary can reconstruct unconstrained words from ac-
celerometer data, so it can be extensively used in voice
and video calls, voice navigation, voice assistant, and other
scenarios. We implement and extensively evaluate AccEar
on different smartphones and users, achieving high accuracy
under various settings and scenarios.
ACKNOWLEDGEMENT
We would like to thank the anonymous reviewers and the
shepherd for their insightful comments. This work is supported
by the National Key Research and Development Program of
China (Grant No. 2021YFB3100400) and National Natural
Science Foundation of China (Grant No. 61832012).
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:36:50 UTC from IEEE Xplore.  Restrictions apply. 
1770
REFERENCES
[1] Y. Michalevsky, D. Boneh, and G. Nakibly, “Gyrophone: Recognizing
speech from gyroscope signals,” in 23rd {USENIX} Security Symposium
({USENIX} Security 14), 2014, pp. 1053–1067.
[2] L. Zhang, P. H. Pathak, M. Wu, Y. Zhao, and P. Mohapatra, “Accelword:
Energy efﬁcient hotword detection through accelerometer,” in Proceed-
ings of the 13th Annual International Conference on Mobile Systems,
Applications, and Services, 2015, pp. 301–315.
[3] S. A. Anand and N. Saxena, “Speechless: Analyzing the threat to speech
privacy from smartphone motion sensors,” in 2018 IEEE Symposium on
Security and Privacy (SP).
IEEE, 2018, pp. 1000–1017.
[4] Z. Ba, T. Zheng, X. Zhang, Z. Qin, B. Li, X. Liu, and K. Ren, “Learning-
based practical smartphone eavesdropping with built-in accelerometer.”
in NDSS, 2020.
[5] J. Han, A. J. Chung, and P. Tague, “Pitchln: eavesdropping via intelligi-
ble speech reconstruction using non-acoustic sensor fusion,” in Proceed-
ings of the 16th ACM/IEEE International Conference on Information
Processing in Sensor Networks, 2017, pp. 181–192.
[6] M. Mirza and S. Osindero, “Conditional generative adversarial nets,”
Computer Science, pp. 2672–2680, 2014.
[7] D. Grifﬁn and J. Lim, “Signal estimation from modiﬁed short-time
fourier transform,” IEEE Transactions on acoustics, speech, and signal
processing, vol. 32, no. 2, pp. 236–243, 1984.
[8] S. A. Anand, C. Wang, J. Liu, N. Saxena, and Y. Chen, “Spearphone: a
lightweight speech privacy exploit via accelerometer-sensed reverbera-
tions from smartphone loudspeakers,” in Proceedings of the 14th ACM
Conference on Security and Privacy in Wireless and Mobile Networks,
2021, pp. 288–299.
[9] A. Davis, M. Rubinstein, N. Wadhwa, G. J. Mysore, F. Durand, and
W. T. Freeman, “The visual microphone: passive recovery of sound from
video,” ACM Transactions on Graphics (TOG), vol. 33, no. 4, pp. 1–10,
2014.
[10] A. Kwong, W. Xu, and K. Fu, “Hard drive of hearing: Disks that
eavesdrop with a synthesized microphone,” in 2019 IEEE symposium
on security and privacy (SP).
[11] M. Guri, Y. Solewicz, A. Daidakulov, and Y. Elovici, “Speake (a) r:
Turn speakers to microphones for fun and proﬁt,” in 11th {USENIX}
Workshop on Offensive Technologies ({WOOT} 17), 2017.
IEEE, 2019, pp. 905–919.
[12] N. Roy and R. Roy Choudhury, “Listening through a vibration motor,”
in Proceedings of the 14th Annual International Conference on Mobile
Systems, Applications, and Services, 2016, pp. 57–69.
[13] G. Wang, Y. Zou, Z. Zhou, K. Wu, and L. M. Ni, “We can hear you
with wi-ﬁ!” IEEE Transactions on Mobile Computing, vol. 15, no. 11,
pp. 2907–2920, 2016.
[14] T. Wei, S. Wang, A. Zhou, and X. Zhang, “Acoustic eavesdropping
through wireless vibrometry,” in Proceedings of the 21st Annual Inter-
national Conference on Mobile Computing and Networking, 2015, pp.
130–141.
[15] R. P. Muscatell, “Laser microphone,” The Journal of the Acoustical
Society of America, vol. 76, no. 4, pp. 1284–1284, 1984.
[16] B. Nassi, Y. Pirutin, A. Shamir, Y. Elovici, and B. Zadov, “Lamphone:
Real-time passive sound recovery from light bulb vibrations.” IACR
Cryptol. ePrint Arch., vol. 2020, p. 708, 2020.
[17] W. Sousa Lima, E. Souto, K. El-Khatib, R. Jalali, and J. Gama, “Human
activity recognition using inertial sensors in a smartphone: An overview,”
Sensors, vol. 19, no. 14, p. 3213, 2019.
[18] S. Shen, M. Gowda, and R. Roy Choudhury, “Closing the gaps in inertial
motion tracking,” in Proceedings of
the 24th Annual International
Conference on Mobile Computing and Networking, 2018, pp. 429–444.
[19] J. L. Hicks, T. Althoff, P. Kuhar, B. Bostjancic, A. C. King, J. Leskovec,
S. L. Delp et al., “Best practices for analyzing large-scale health data
from wearables and smartphone apps,” NPJ digital medicine, vol. 2,
no. 1, pp. 1–12, 2019.
[20] “Android api reference.” [Online]. Available: https://developer.android.
com/reference/
[21] J. J. Ohala, “Phonetic explanations for sound patterns,” A ﬁgure of
speech: A festschrift for John Laver, vol. 23, 2005.
[22] “English phonology.” [Online]. Available: https://en.wikipedia.org/wiki/
English phonology
[23] R. J. Baken and R. F. Orlikoff, Clinical measurement of speech and
voice. Cengage Learning, 2000.
[24] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,”
Advances in neural information processing systems, vol. 27, 2014.
[25] S. S. Stevens, J. Volkmann, and E. B. Newman, “A scale for the
measurement of the psychological magnitude pitch,” The journal of the
acoustical society of america, vol. 8, no. 3, pp. 185–190, 1937.
[26] K. Kumar, R. Kumar, T. de Boissiere, L. Gestin, W. Z. Teoh, J. Sotelo,
A. de Br´ebisson, Y. Bengio, and A. Courville, “Melgan: Generative
adversarial networks for conditional waveform synthesis,” arXiv preprint
arXiv:1910.06711, 2019.
[27] I. Ananthabhotla, S. Ewert, and J. A. Paradiso, “Towards a perceptual
loss: Using a neural network codec approximation as a loss for gen-
erative audio models,” in Proceedings of the 27th ACM International
Conference on Multimedia, 2019, pp. 1518–1525.
[28] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation
with conditional adversarial networks,” in Proceedings of the IEEE
conference on computer vision and pattern recognition, 2017, pp. 1125–
1134.
[29] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
for biomedical image segmentation,” in International Conference on
Medical image computing and computer-assisted intervention. Springer,
2015, pp. 234–241.
[30] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
2017.
[31] “Mobile
operating
from
january
2012
systems’
Available:
market
to
world-
2021.”
https://www.statista.com/statistics/272698/
wide
[Online].
global-market-share-held-by-mobile-operating-systems-since-2009/
Available:
https://developer.android.com/about/versions/12/behavior-changes-12\
#motion-sensor-rate-limiting
share
june
are
rate-limited.”
[Online].
[32] “Motion
sensors
[33] A. Tamamori, T. Hayashi, K. Kobayashi, K. Takeda, and T. Toda,
“Speaker-dependent wavenet vocoder.” in Interspeech, vol. 2017, 2017,
pp. 1118–1122.
[34] C. Yan, G. Zhang, X. Ji, T. Zhang, T. Zhang, and W. Xu, “The
feasibility of injecting inaudible voice commands to voice assistants,”
IEEE Transactions on Dependable and Secure Computing, 2019.
[35] I.-T. Rec, “Vocabulary for performance and quality of service,” p. 10,
2006.
[36] “Mobile vendor market
share worldwide.”
[Online]. Available:
https://gs.statcounter.com/vendor-market-share/mobile/worldwide
[37] X. Qi, M. Keally, G. Zhou, Y. Li, and Z. Ren, “Adasense: Adapting
sampling rates for activity recognition in body sensor networks,” in
2013 IEEE 19th Real-Time and Embedded Technology and Applications
Symposium (RTAS).
IEEE, 2013, pp. 163–172.
[38] A. Radford, L. Metz, and S. Chintala, “Unsupervised representation
learning with deep convolutional generative adversarial networks,” arXiv
preprint arXiv:1511.06434, 2015.
[39] J. Zhao, M. Mathieu, and Y. LeCun, “Energy-based generative adver-
sarial network,” arXiv preprint arXiv:1609.03126, 2016.
[40] X. Mao, Q. Li, H. Xie, R. Y. Lau, Z. Wang, and S. Paul Smolley, “Least
squares generative adversarial networks,” in Proceedings of the IEEE
international conference on computer vision, 2017, pp. 2794–2802.
[41] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein gan,” 2017.
[42] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image
translation using cycle-consistent adversarial networks,” in Proceedings
of the IEEE international conference on computer vision, 2017, pp.
2223–2232.
[43] T. Karras, S. Laine, and T. Aila, “A style-based generator architecture
for generative adversarial networks,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2019, pp.
4401–4410.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:36:50 UTC from IEEE Xplore.  Restrictions apply. 
1771
[44] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
of deep bidirectional transformers for language understanding,” arXiv
preprint arXiv:1810.04805, 2018.
APPENDIX
A. Detailed parameters of mobile devices
We list the detailed information on the different models
of the mobile devices in Table III. We can notice that while
the maximum accelerometer sampling rate is within a range
416∼500Hz on smartphones, it is signiﬁcantly lower on tablets
(i.e., 200∼250Hz). Despite this difference, our attack scheme
shows consistency among multiple devices.
Type
System Version
HarmonyOS 2.0
HarmonyOS 2.0
Android 11
Android 11
Android 11
Model
Huawei Mate40 Pro
Huawei Mate30 Pro
OPPO Reno6 Pro
SamSung S21+
XiaoMi RedMi
10X Pro
OPPO Find X3
Huawei MatePad Pro
Samsung Galaxy
Tab S6 Lite
Phone
Phone
Phone
Phone
Phone
Phone
Tablet
Tablet
Screen
Size
6.76 in.
6.53 in.
6.55 in.
6.70 in.
6.57 in.
Acceler.
MSR
500Hz
500Hz
420Hz
416Hz
418Hz
425Hz
250Hz
200Hz
Android 11
HarmonyOS 2.0
Android 11
6.70 in.
10.80 in.
10.40 in.
TABLE III: Detailed properties of different mobile devices
(Acceler. MSR stands for Accelerometer Maximum Sampling
Rate)
B. Relationship between MCD and reconstruction perfor-
mance at word level
We further explore the relationship between MCD and
reconstruction performance at the word level and randomly
select some sample results to present in Table IV. We believe
that even if the model cannot reconstruct all the words in a
sentence, we can infer the missing words from the context.
Besides, we can also resort to recent Natural Language Pro-
cessing (NLP) techniques (such as BERT [44]) to infer the
semantics of sentences even with missing words.
C. The effect of coverage of dataset diversities on model
performance
The diversity of a person’s speech mainly lies in two
aspects: speed and frequency. People’s speech speed can often
change depending on the speaker’s mood and context. Using
the data with normal speech speed for training and a much
faster or slower speed for testing will lead to an unsatisfactory
result of speech reconstruction. The pronunciation frequency
of people in different emotional states can also be different.
For example, the pronunciation frequency can be lower when
the mood is low and relatively higher when the mood is
excited. Therefore, the change of frequency is also within our
consideration.
5∼6
Zheng ji bi sai, jiang jin shi
wan, mei
you might be all over
world so good afternoon
MCD Original Audio
2∼3
3∼4
the
4∼5 We had a barrel like this down
in our basement, ﬁlled with
cans of food and water
This is our product line. We
have a very clean product line,
we think we have the best
notebooks in the business
Talking about here at Ted is
that ther’re right in the middle
of rainforest was of some solar
panels the
Community could have light
for I think it was about half an
hour each evening and there is
the chief in all his
6∼7
7∼8
Reconstructed Audio
Zheng ji bi sai, jiang jin shi
wan, mei
* are be all over the world so
good afternoon
We had a barrel like this * in
our basement, ﬁlled with cans
of food and *
This is our product line. * most
* clean product line, we think
we have the best * in the busi-
ness
Talking about here at Ted is
* * * in * * the middle of
rainforest was of some solar
panels the
Community could have light *
. * think * * * half * hour each
evening and there is the chief
in all his
TABLE IV: MCD and corresponding reconstructed results (*
represents the word we cannot recognize).
We perform a systematic evaluation of the diversity. We
prepare the following datasets:
1) For the speed,
a) ×0.75
b) ×1.0
c) ×1.25
d) mixed dataset including ×0.75, ×1.0, and ×1.25
2) For the frequency,
a) ×0.8
b) ×1.0
c) ×1.2
d) mixed dataset including ×0.8, ×1.0, and ×1.2
3) mixed dataset including 1.d and 2.d
As shown in Fig. 21, the model with more diversity achieves
a better performance in general. In terms of speed, the model
(1.d) based on mixed datasets achieves 2.9% improvement
than single dataset as shown in Fig. 21(a). In terms of
frequency, the model (2.d) based on mixed datasets achieves
4.9% improvement as shown in Fig. 21(b). The model (3)
with most diversity achieves the largest improvement of 6.0%
as shown in Fig. 21(c).
D. The transferability between different users
We test the transferability between all users, as shown in
Fig. 22. The results show our model can generalize well under
cross-user training.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:36:50 UTC from IEEE Xplore.  Restrictions apply. 
1772
(a) Speed
(b) Frequency
Fig. 22: Performance of model generalization with cross-user
training
(c) All
Fig. 21: Audio reconstruction performance with speed and
frequency diversity.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:36:50 UTC from IEEE Xplore.  Restrictions apply. 
1773