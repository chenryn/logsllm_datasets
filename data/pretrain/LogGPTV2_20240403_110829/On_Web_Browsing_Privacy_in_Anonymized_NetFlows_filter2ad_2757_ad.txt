expected in realistic attacks on anonymized data. In or-
der to perform a rigorous evaluation of the real threats
posed by such identiﬁcation techniques, we must address
several issues, including web browsing session parsing
and caching behavior.
Web Browsing Session Parsing One of the biggest
concerns with the closed world evaluation in
5 is that
there is an implicit assumption that parsing web sessions
from live network data is a simple and accurate task.
There has been extensive work in attempting to parse
§
packet traces into web browsing sessions, yet much of
this work requires access to plaintext payloads, and re-
sults show that this parsing is not completely accurate
[16, 31, 15]. To our knowledge, there is no prior art
on performing similar parsing on NetFlow data. Koukis
et al. attempted to use a heuristic of packet inter-arrival
times to delineate sessions in packet traces, but their
techniques were only able to correctly identify 8% of
the web browsing sessions—underscoring the difﬁculty
of the problem [14].
Fortunately, our kernel density estimate (KDE) and bi-
nary Bayes belief network (BBN) models can be modi-
ﬁed to overcome the challenges of web browsing ses-
sion parsing without signiﬁcant changes to our identiﬁ-
cation process. In our previous evaluation, we assumed
that the KDE and BBN models were given a subsequence
of the original NetFlow log that corresponded to a com-
plete web browsing session for a single client. For our
real world evaluation, however, we remove this assump-
tion. Instead, to parse the NetFlow log, we assume that
all ﬂows of a given web browsing session are clustered
in time, and partition the NetFlow log into subsequences
such that the inter-arrival times of the ﬂows in the parti-
≤ δ = 10 seconds. This assumption is similar to
tion is
that of Koukis et al. [14] and provides a coarse approx-
imation to the web browsing sessions, but the resultant
partitions may contain multiple web browsing sessions,
or interleaved sessions.
Notice that we can simply use each of the physical
servers within these partitions as input to the KDE mod-
els for a target web page to determine which logical
servers may be present in the partition. Thus, we apply
the ﬂows from every physical server in our partitioned
NetFlow data to the KDE models for our target page to
create the logical server mappings. If a physical server
in our partition does not map to a logical server, we ig-
nore that physical server’s ﬂows and remove it from the
partition. Thus, by removing these unmapped physical
USENIX Association
16th USENIX Security Symposium
347
servers, we identify a candidate web browsing session
for our target site. Since the BBN operates directly on
the mappings created by the KDE models, we traverse
the BBN and determine if the web page is present based
on the physical servers that were properly mapped. This
technique for ﬁnding web browsing sessions is particu-
larly robust since it can ﬁnd multiple web pages within a
single partition, even if these web pages have been inter-
leaved by tabbed browsing.
Browser Cache Behavior Another serious concern in
our closed world evaluation is the variability of web
browsing session behavior due to the client’s browser
cache. In our closed world evaluation, we created our
models from data collected by an automated script that
randomly browsed the front pages from among the top
100 sites according to alexa.com. The use of uniform
random browsing with the default cache policy, how-
ever, does not accurately reﬂect the objects that would
be cached by real clients. In reality, the client’s browser
cache would tend to hold more objects from the most
frequently visited web pages, making the cache states
highly speciﬁc to the client. Clearly, using our simu-
lated caching data alone is not enough to create models
that are able to detect both frequently and infrequently
visited sites. To alleviate this shortcoming, we create a
second set of training data by setting the browser’s cache
limit to 1.5GB. With such a large browser cache, objects
should not be evicted from the cache even when we per-
form our random browsing, thereby allowing us to gain
information about web browsing behaviors for our tar-
get sites when they are viewed frequently. The training
data that we use to create our models now consists of 90
web browsing sessions of simulated cache data, and 64
browsing sessions of unlimited cache data for each tar-
get site. The procedure for building our models remains
the same, except we now use the ﬂow records from both
cache scenarios.
Results To provide a more realistic evaluation of the
threat our identiﬁcation techniques pose to anonymized
NetFlow data, we re-examine its performance on three
distinct datasets. First, we use the testing data from our
closed world evaluation to measure the effect that the in-
troduction of unlimited cache data and web session pars-
ing have on the performance of our technique. Second,
we capture web browsing sessions from different net-
work providers in Maryland, and in Pennsylvania. By
comparing the performance of our technique on these
three datasets, we can glean insight into the effects of lo-
cality on the success of attacks on anonymized NetFlow
data.
The effects that the changes to our models have on
the performance of our technique are shown in Table
2. Clearly, the false detection rate increases substan-
tially, but the true detection rate also increases. As in
the closed world scenario, we ﬁnd that the web pages
with constantly changing content are more difﬁcult to
detect than static web pages, and that those sites with
complex structure (i.e., many logical servers, and many
ﬂows) achieve a signiﬁcantly lower false detection rate
than those sites with simple structure. The substantial
change in performance can be explained by the relax-
ation of the BBN constraints to allow for web browser
session parsing. This relaxation allows any web brows-
ing session where a subset of physical servers meets the
remaining constraints to be identiﬁed, thereby causing
the increase in both true detection and false detection
rate. A more detailed analysis of the implications of
these effects is provided in
7.
§
It is often the case that published network data is taken
at locations where an attacker would not have access to
the network to collect training data for her models, and
so we investigate the effect that the change in locality
has on the performance of our technique. The results
in Table 2 show that there is, indeed, a drop in perfor-
mance due to changes in locality, though trends in true
detection and false detection rates still hold. In our eval-
uation, we noticed that the Johns Hopkins data used to
train our web page models included a web caching server
that caused signiﬁcant changes in the download behavior
of certain web pages. These changes in behavior in turn
explain the signiﬁcant difference in performance among
data collected at different localities. It would appear that
these results are somewhat disconcerting for a would-be
attacker, since she would have to generate training data at
a network that was different from where the anonymized
data was captured. However, she could make her training
more robust by generating data on a number of networks,
perhaps utilizing infrastructure such as PlanetLab [25],
though the effects of doing so on the performance of the
technique are unknown. By including web page down-
load behavior from a number of networks, she can en-
sure that the KDE and BBN models for each target web
page are robust enough to handle a variety of network
infrastructures.
7 A Realistic Threat Assessment
Finally, we provide a threat assessment by applying our
technique to live data collected from a public wireless
network at Johns Hopkins University Security Institute
over the course of 7 days. From this data, we examine
the expected real world accuracy of our techniques and
discuss the features that make some web pages prime tar-
gets for identiﬁcation.
348
16th USENIX Security Symposium
USENIX Association
Category
Other
Search/Web Portals
Social Network/Dating
Media
Corporate
Job Search
Shopping
News
Reference
Sports
Overall
TD (%)
100.00
94.29
75.75
75.71
75.00
72.50
71.00
70.71
67.00
66.67
75.58
FD (%) TD (∆)
-40.00
10.71
-35.47
13.92
9.02
-15.75
-3.71
30.61
-37.50
11.64
-47.50
1.81
4.89
-17.67
-41.54
2.50
-25.82
14.64
-9.53
26.57
13.28
-26.86
Maryland
Pennsylvania
FD (∆) TD (∆)
-6.25
+1.98
-26.60
+3.92
+4.93
+13.61
-30.97
+2.00
-43.09
+2.55
-44.91
+1.10
+3.55
-40.05
-35.00
+0.15
+1.52
+9.59
+4.16
-6.23
+1.47
-24.39
FD (∆)
+10.74
-0.45
+9.95
+5.75
+3.06
+7.94
+1.37
-0.14
+19.13
-11.87
+4.59
Table 2: True and false detection rates for canonical categories in JHU data, and comparison to remote datasets
Category
Reference
News
Search/Web Portals
Social Network/Dating
Shopping
Corporate
Overall
Web Page
imdb.com
nytimes.com
digg.com
washingtonpost.com
cnn.com
weather.com
google.com
msn.com
yahoo.com
facebook.com
myspace.com
ebay.com
amazon.com
usps.com
TD (%)
100.00
0.00
61.76
9.09
44.00
0.00
28.57
0.00
60.98
0.00
25.00
0.00
0.00
0.00
33.65
FD (%)
13.1
0.06
0.35
0.01
8.30
2.27
22.31
6.18
0.89
0.07
65.55
0.10
0.78
4.66
9.09
Table 3: True and false detection rates for web pages in live network data
Results The results of our experiment on live data,
shown in Table 3, provides some interesting insight
into the practicality of identifying web pages in real
anonymized trafﬁc. In our results from local testing data
collected via automated browsing, we observe that cer-
tain categories made up mostly of simple, static web
pages (e.g., search engines) provide excellent true detec-
tion rates, while web pages whose content changes often
(e.g., news web pages) perform signiﬁcantly worse. Fur-
thermore, categories of sites with complex structure (i.e.,
many logical servers) generally have exceptionally low
false detection rates, while categories of simple sites with
fewer logical servers produce extremely high false detec-
tion rates. Upon closer examination, not all web pages
within a given category perform similarly despite hav-
ing similar content and function. For instance, cnn.com
and nytimes.com have wildly different performance in
our live network test despite the fact that both pages have
rapidly changing news content.
To better understand this difference in our classiﬁer’s
performance for different web pages, we examine three
features of the page loads in our automated browsing ses-
sions for each site: the number of ﬂows per web brows-
ing session, number of physical servers per web brows-
ing session, and the number of bytes per ﬂow. Our goals
lie in understanding (i) why two sites within the same
category have wildly different performance, and (ii) why
simple web pages introduce so many more false detec-
tions over more complex web pages. To quantify the
complexity of the web page and the amount of varia-
tion exhibited in these features, we compute the mean
and standard deviation for each feature across all obser-
vations of the web page in our training data, including
both simulated cache and unlimited cache scenarios. The
mean values for each feature provide an idea of the com-
plexity of the web page. For instance, a small average
USENIX Association
16th USENIX Security Symposium
349
number of physical servers would indicate that the web
page does not make extensive use of content delivery net-
works. The standard deviations tell us how consistent the
structure of the page is. These statistics offer a concise
measure of the complexity of each web page, enabling
us to objectively compare sites in order to determine why
some are more identiﬁable than others.
Returning to the difference in true detection rates for