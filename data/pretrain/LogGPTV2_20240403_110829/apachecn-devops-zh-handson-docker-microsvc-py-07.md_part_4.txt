users-backend-794ff46b8-m2c6w     1m         54Mi
```
为了适当地控制使用的限制，我们需要在部署中配置分配的内容并为其限制资源。
# 在部署中配置资源
在容器的配置中，我们可以指定请求的资源是什么，以及它们的最大资源。
它们都通知 Kubernetes 容器的预期内存和 CPU 使用情况。创建新容器时，Kubernetes 会自动将其部署在有足够资源覆盖的节点上。
在`frontend/deployment.yaml`文件中，我们包括以下`resources`实例:
```
spec:
    containers:
    - name: frontend-service
      image: 033870383707.dkr.ecr.us-west-2
                 .amazonaws.com/frontend:latest
      imagePullPolicy: Always
      ...
      resources:
          requests:
              memory: "64M"
              cpu: "60m"
          limits:
              memory: "128M"
              cpu: "70m"
```
最初请求的内存是 64 兆字节，0.06 个中央处理器内核。
The resources for memory can also use Mi to the power of 2, which is equivalent to a megabyte (*10002* bytes), which is called a mebibyte (*220* bytes). The difference is small in any case. You can use also G or T for bigger amounts.
The CPU resources are measured fractionally where 1 being a core in whatever system the node is running (for example, AWS vCPU). Note that 1000m, meaning 1000 milli CPUs is equivalent to a whole core.
限制是 128 兆字节和 0.07 个中央处理器内核。容器不能使用超过限制的内存或中央处理器。
Aim at round simple numbers to understand the limits and requested resources. Don't expect to have them perfect the first time; the applications will change their consumption.
Measuring the metrics in an aggregated way, as we will talk about in [Chapter 11](11.html), *Handling Change, Dependencies, and Secrets in the System*, will help you to see the evolution of the system and tweak it accordingly.
该限制为自动缩放器创建基准，因为它将以资源的百分比来衡量。
# 创建住房公积金
要创建新的 HPA，我们可以使用`kubectl autoscale`命令:
```
$ kubectl autoscale deployment frontend --cpu-percent=10 --min=2 --max=8 -n example
horizontalpodautoscaler.autoscaling/frontend autoscaled
```
这将创建一个新的 HPA，以`example`命名空间中的`frontend`部署为目标，并将吊舱的数量设置在`2`和`8`之间。要缩放的参数是中央处理器，我们将其设置为可用中央处理器的 10%，在所有吊舱中取平均值。如果它更高，它会产生新的豆荚，如果它更低，它会减少豆荚。
The 10% limit is used to be able to trigger the autoscaler and to demonstrate it. 
自动缩放器是一种特殊的 Kubernetes 对象，可以这样查询:
```
$ kubectl get hpa -n example
NAME     REFERENCE           TARGETS  MIN MAX REPLICAS AGE
frontend Deployment/frontend 2%/10%   2   8   4        80s
```
注意目标如何说它目前在 2%左右，接近极限。这是用具有相对较高基线的小型可用 CPU 设计的。
几分钟后，副本数量将减少，直到达到最小值`2`。
The downscaling may take a few minutes. This generally is the expected behavior, upscaling being more aggressive than downscaling.
为了创建一些负载，让我们将应用 Apache Bench ( `ab`)与前端中专门创建的端点结合使用，该端点使用大量的 CPU:
```
$ ab -n 100 http://.elb.amazonaws.com/load
Benchmarking .elb.amazonaws.com (be patient)....
```
注意`ab`是一个方便的并发产生 HTTP 请求的测试应用。如果你愿意，你可以在浏览器中快速连续多次点击网址。
Remember to add the load balancer DNS, as retrieved in the *Creating the cluster* section.
这将在集群中产生额外的 CPU 负载，并将使部署规模扩大:
```
NAME     REFERENCE           TARGETS MIN MAX REPLICAS AGE
frontend Deployment/frontend 47%/10% 2   8   8        15m
```
请求完成后，几分钟后，豆荚的数量会慢慢减少，直到再次碰到两个豆荚。
但是我们也需要一种扩展节点的方法，否则我们将无法增加系统中的资源总数。
# 扩展集群中的节点数量
在 EKS 集群中作为节点工作的 AWS 实例的数量也可以增加。这为集群增加了额外的资源，并使启动更多的吊舱成为可能。
允许这样做的基础 AWS 服务是自动缩放组。这是一组 EC2 实例，它们共享相同的映像，并具有定义的大小，包括最小和最大实例。
在任何 EKS 集群的核心，都有一个控制集群节点的自动扩展组。注意`eksctl`将自动缩放组创建并公开为节点组:
```
$ eksctl get nodegroup --cluster Example
CLUSTER NODEGROUP   MIN  MAX  DESIRED INSTANCE IMAGE ID
Example ng-74a0ead4 2    2    2       m5.large ami-X
```
借助`eksctl`，我们可以按照创建集群时的描述手动向上或向下扩展集群:
```
$ eksctl scale nodegroup --cluster Example --name ng-74a0ead4 --nodes 4
[i] scaling nodegroup stack "eksctl-Example-nodegroup-ng-74a0ead4" in cluster eksctl-Example-cluster
[i] scaling nodegroup, desired capacity from to 4, max size from 2 to 4
```
该节点组也可以在 AWS 控制台中的 EC2 |自动缩放组下看到:
![](img/1a35c1f7-06fa-4243-b336-1db202e9b03b.png)
在网络界面中，我们有一些有趣的信息可以用来收集关于自动缩放组的信息。“活动历史记录”选项卡允许您查看任何向上或向下扩展的事件，而“监控”选项卡允许您检查指标。
大部分处理都是通过`eksctl`自动创建的，比如实例类型和 AMI-ID(实例上的初始软件，包含操作系统)。它们应该主要由`eksctl`控制。
If you need to change the Instance Type, `eksctl` requires you to create a new nodegroup, move all the pods, and then delete the old. You can learn more about the process in the `eksctl` documentation ([https://eksctl.io/usage/managing-nodegroups/](https://eksctl.io/usage/managing-nodegroups/)).
但是从 web 界面上，很容易编辑缩放参数和添加自动缩放策略。
Changing the parameters through the web interface may confuse the data retrieved in `eksctl`, as it's independently set up.
It is possible to install a Kubernetes autoscaler for AWS, but it requires a `secrets` configuration file to include a proper AMI in the autoscaler pod, with AWS permissions to add instances.
Describing the autoscale policy in AWS terms in code can also be quite confusing. The web interface makes it a bit easier. The pro is that you can describe everything in config files that can be under source control.
We will go with the web interface configuration, here, but you can follow the instructions at [https://eksctl.io/usage/autoscaling/](https://eksctl.io/usage/autoscaling/).
对于扩展策略，可以创建两个主要组件:
*   **预定动作**:它们是在规定时间发生的放大和缩小事件。该操作可以通过所需数量以及最小和最大数量的组合来更改节点数量，例如，在周末增加集群。这些操作可以定期重复，例如每天或每小时。该操作也可以有一个结束时间，这将使值恢复到以前定义的值。如果我们预计系统会有额外的负载，这可以用来提高几个小时的性能，或者降低夜间的成本。
*   **扩展策略**:这些策略在特定时间寻找需求，并在所描述的数字之间扩大或缩小实例。有三种类型的策略:目标跟踪、步长缩放和简单缩放。目标跟踪是最简单的，因为它监控目标(通常是 CPU 使用情况)，并上下缩放以保持接近数字。另外两个策略要求您使用 AWS CloudWatch 度量系统生成警报，该系统功能更强大，但也需要使用 CloudWatch 和更复杂的配置。
节点的数量不仅可以向上变化，也可以向下变化，这意味着删除节点。
# 删除节点
删除一个节点时，正在运行的 pods 需要移动到另一个节点。这是由 Kubernetes 自动处理的，EKS 将以安全的方式进行操作。
This can also happen if a node is down for any reason, such as an unexpected hardware problem. As we've seen before, the cluster is created in multiple availability zones to minimize risks, but some nodes may have problems if there's a problem in an Amazon availability zone.
Kubernetes was designed for this kind of problem, so it's good at moving pods from one node to another in unforeseen circumstances.
将 pod 从一个节点移动到另一个节点是通过销毁 pod 并在新节点中重新启动它来完成的。由于 pods 由部署控制，它们将保持适当数量的 pods，如副本或自动缩放值所述。
Remember that pods are inherently volatile and should be designed so they can be destroyed and recreated.
升级还会导致现有的吊舱移动到其他节点，以更好地利用资源，尽管这种情况不太常见。节点数量的增加通常与豆荚数量的增加同时进行。
根据需求，控制节点数量需要考虑实现最佳结果所遵循的策略。
# 设计成功的自动缩放策略
正如我们已经看到的，两种自动缩放，荚和节点，需要相互关联。减少节点数量可以降低成本，但会限制可用于增加豆荚数量的可用资源。
永远记住，自动缩放是一个大数字游戏。除非您有足够的负载变化来证明它的合理性，否则调整它将产生与开发和维护过程的成本不可比的成本节约。对预期收益和维护成本进行成本分析。
在处理更改集群大小时，优先考虑简单性。在晚上和周末缩减规模可以节省大量资金，而且比生成复杂的 CPU 算法来检测高低要容易得多。
Keep in mind that autoscaling is not the only way of reducing costs with cloud providers, and can be used combined with other strategies.
For example, in AWS, reserving EC2 instances for a year or more allows you to greatly reduce the bill. They can be used for the cluster baseline and combined with more expensive on-demand instances for autoscaling, which yields an extra reduction in costs: [https://aws.amazon.com/ec2/pricing/reserved-instances/](https://aws.amazon.com/ec2/pricing/reserved-instances/).
一般来说，你应该有一个额外的硬件允许扩展吊舱，因为这样更快。这是允许的情况下，不同的吊舱在不同的比例。根据应用的不同，当一项服务的使用率上升时，另一项服务的使用率也会下降，这将使使用率保持在相似的水平。
This is not the use case that comes to mind, but for example, scheduled tasks during the night may make use of available resources that at daytime are being used by external requests.
They can work in different services, balancing automatically as the load changes from one service to the other. 
一旦净空减小，就开始缩放节点。始终留出安全余量，以避免陷入节点扩展不够快，并且由于缺乏资源而无法启动更多 pod 的情况。
The pod autoscaler can try to create new pods, and if there are no resources available, they won't be started. In the same fashion, if a node is removed, any Pod that is not deleted may not start because of a lack of resources.
Remember that we describe to Kubernetes the requirements for a new pod in the `resources` section of the deployment. Be sure that the numbers there are indicative of the required ones for the pod.
为了确保荚充分分布在不同的节点上，可以使用 Kubernetes 相似性和反相似性规则。这些规则允许定义某种类型的荚是否应该在同一个节点中运行。
例如，这有助于确保所有类型的 pod 均匀分布在各个区域，或者确保两个服务始终部署在同一个节点上，以减少延迟。
您可以在这篇博文中了解更多关于亲缘关系和如何配置的信息:[https://super giant . io/blog/learn-how-to-assign-pods-in-kubernetes-use-node selector-and-affinity-features/](https://supergiant.io/blog/learn-how-to-assign-pods-to-nodes-in-kubernetes-using-nodeselector-and-affinity-features/)，以及 Kubernetes 官方配置([https://Kubernetes . io/docs/concepts/configuration/assign-pod-node/](https://kubernetes.io/docs/concepts/configuration/assign-pod-node/))。
总的来说，Kubernetes 和`eksctl`默认值对大多数应用都很有效。此建议仅用于高级配置。
# 摘要
在本章中，我们已经看到了如何将 Kubernetes 集群应用到生产环境中，并在云提供商(在本例中是 AWS)中创建 Kubernetes 集群。我们已经看到了如何设置我们的 Docker 注册表，使用 EKS 创建一个集群，并调整我们现有的 YAML 文件，以便它们为环境做好准备。
请记住，虽然我们以 AWS 为例，但我们讨论的所有元素在其他云提供商中都是可用的。查看他们的文档，看看他们是否更适合你。
我们还看到了如何部署一个 ELB，使集群对公共接口可用，以及如何在其上启用 HTTPS 支持。
我们讨论了部署的不同元素，以使集群更具弹性，并在不中断服务的情况下平稳地部署新版本——既通过使用 HAProxy 来快速启用或禁用服务，又通过确保以有序的方式更改容器映像。
我们还介绍了自动缩放如何帮助合理利用资源，并允许您覆盖系统中的负载峰值，方法是创建更多的 pod，以及添加更多的 AWS 实例，以便在需要时向集群添加资源并删除它们，从而避免不必要的成本。
在下一章中，我们将看到如何使用 GitOps 原则来控制 Kubernetes 集群的状态，以确保对它的任何更改都被正确地审查和捕获。
# 问题
1.  管理自己的 Kubernetes 集群的主要缺点是什么？
2.  你能说出一些拥有托管 Kubernetes 解决方案的商业云提供商吗？
3.  您需要执行什么操作才能推送至 AWS Docker 注册表？
4.  我们使用什么工具来建立 EKS 集群？
5.  在本章中，我们对前几章中的 YAML 文件进行了哪些主要修改？
6.  本章中的集群中是否有不需要的 Kubernetes 元素？
7.  为什么我们需要控制与 SSL 证书相关联的域名系统？
8.  活跃度探测器和就绪探测器有什么区别？
9.  为什么滚动更新在生产环境中很重要？
10.  自动缩放豆荚和节点有什么区别？
11.  在本章中，我们部署了自己的数据库容器。在生产中，这种情况将会改变，因为它需要连接到一个已经存在的外部数据库。为此，您将如何更改配置？
# 进一步阅读
要了解更多关于如何使用 AWS 的信息，网络功能非常丰富，您可以查看书籍 *AWS 网络烹饪书*([https://www . packtpub . com/eu/虚拟化与云/aws 网络烹饪书](https://www.packtpub.com/eu/virtualization-and-cloud/aws-networking-cookbook))。要了解如何确保在 AWS 中设置安全系统，请阅读*AWS:AWS 上的安全最佳实践*([https://www . packtpub . com/eu/虚拟化和云/AWS-安全-最佳实践-aws](https://www.packtpub.com/eu/virtualization-and-cloud/aws-security-best-practices-aws) )。