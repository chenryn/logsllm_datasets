title:Efficient Repair of Polluted Machine Learning Systems via Causal Unlearning
author:Yinzhi Cao and
Alexander Fangxiao Yu and
Andrew Aday and
Eric Stahl and
Jon Merwine and
Junfeng Yang
Efficient Repair of Polluted Machine Learning Systems via
Causal Unlearning
Yinzhi Cao
Alexander Fangxiao Yu
Andrew Aday
Lehigh University, Bethlehem PA
Columbia University, New York, NY
Columbia University, New York, NY
PI:EMAIL
PI:EMAIL
PI:EMAIL
Eric Stahl
Jon Merwine
Junfeng Yang
Lehigh University, Bethlehem PA
Lehigh University, Bethlehem PA
Columbia University, New York, NY
PI:EMAIL
PI:EMAIL
PI:EMAIL
ABSTRACT
Machine learning systems, though being successful in many real-
world applications, are known to remain prone to errors and attacks.
A major attack, called data pollution, injects maliciously crafted
training data samples into the training set, causing the system to
learn an incorrect model and subsequently misclassify testing sam-
ples. A natural solution to a data pollution attack is to remove the
polluted data from the training set and relearn a clean model. Unfor-
tunately, the training set of a real-world machine learning system can
contain millions of samples; it is thus hopeless for an administrator
to manually inspect all of them to weed out the polluted ones.
This paper presents an approach called causal unlearning and a
corresponding system called KARMA to efficiently repair a polluted
learning system. KARMA dramatically reduces the manual effort of
administrators by automatically detecting the set of polluted training
data samples with high precision and recall. Evaluation on three
learning systems show that KARMA greatly reduces manual effort
for repair, and has high precision and recall.
CCS CONCEPTS
• Computing methodologies → Machine learning; • Security and
privacy → Systems security;
KEYWORDS
Causality; Data Pollution Attacks; Machine Unlearning
ACM Reference format:
Yinzhi Cao, Alexander Fangxiao Yu, Andrew Aday, Eric Stahl, Jon Merwine,
and Junfeng Yang. 2018. Efficient Repair of Polluted Machine Learning Sys-
tems via Causal Unlearning. In Proceedings of 2018 ACM Asia Conference
on Computer and Communications Security, Incheon, Republic of Korea,
June 4–8, 2018 (ASIA CCS ’18), 13 pages.
https://doi.org/10.1145/3196494.3196517
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
ASIA CCS ’18, June 4–8, 2018, Incheon, Republic of Korea
© 2018 Copyright held by the owner/author(s). Publication rights licensed to Association
for Computing Machinery.
ACM ISBN 978-1-4503-5576-6/18/06. . . $15.00
https://doi.org/10.1145/3196494.3196517
INTRODUCTION
1
Machine learning systems play an increasingly important role in
today’s world, from recommending products, contents, and friends
to self-driving cars. However, they remain vulnerable to a variety of
attacks, and mechanisms to defend against or cope with the attacks
remain understudied.
A major attack, called data pollution [35], injects maliciously
crafted training data samples into the training set, causing the system
to learn an incorrect model and subsequently misclassify testing
samples. The most recent real-world example is Microsoft’s AI
powered chatbot Tay. Tay learned racism because some Twitter users
interacted with Tay using offensive, racist words, and these words
were included in Tay’s training set [36]. In another proof-of-concept
example, Wang et al. [45] show that injected false samples in the
training set can mislead the machine learning classifier detecting
malicious crowdsourcing workers. Similarly, Perdisci et al. [35]
show that well-crafted fake network flows as part of the training
data can significantly influence the worm signatures generated by
PolyGraph [34], a worm detection engine.
A natural solution to a data pollution attack is to remove the
polluted data from the training set and relearn a clean model. Unfor-
tunately, the training set of a real-world machine learning system can
contain millions of samples; it is thus hopeless for an administrator
to manually inspect all of them to weed out the polluted ones. This
overwhelming amount of manual cleaning required is perhaps why
Microsoft brought Tay offline for repair but has yet to bring it back
online.
This paper presents KARMA,1 the first system designed for effi-
cient repair of a polluted machine learning system. It dramatically
reduces the manual effort administrators need to do by automatically
detecting the set of polluted training data samples with high preci-
sion and recall. Key in KARMA is an idea we call casual unlearning.
Specifically, to launch a data pollution attack, an attacker inevitably
leaves a trace—a causality chain that goes from the polluted train-
ing samples, to a polluted learning model, to misclassified testing
samples. Leveraging this causality trace, KARMA searches through
different subsets of training samples and returns the subset that
causes the most misclassifications as the set of polluted training sam-
ples. KARMA then determines how many misclassified samples are
caused by a subset of training samples by removing the subset from
the training set, computing a new model, and checking whether the
new model correctly classifies the previously misclassified samples.
1Karma, originated from Hinduism and Buddhism, means destiny or fate.
ASIA CCS ’18, June 4–8, 2018, Incheon, Republic of Korea
Y. Cao et al.
KARMA thus reduces the manual effort required down to two parts.
First, it assumes that some users report misclassified testing samples
(e.g., as in the Microsoft Tay example or spam detection) and, for
added security, it relies on administrators to verify the user reports.
KARMA does not require all misclassified samples to be collected
upfront before it repairs a system; instead, our evaluation shows
that it can incrementally clean a system as users gradually report
misclassifications. Second, it relies on administrators to inspect
the set of polluted samples it returns. KARMA determines the set
of polluted samples leveraging causality of misclassifications, not
contents of the samples. Therefore, it may have false positives, such
as flagging (unpolluted) outliers in the training set. However, we view
it an advantage to use KARMA to detect outliers from the training set.
It may also have false negatives, such as missing polluted training
samples. However, if the remaining polluted samples do not cause
user-noticeable misclassifications, their harm may be little.
To ease discussion, we term the set of user-reported misclassified
test samples the oracle set. Administrators can augment this ora-
cle set with correctly classified test samples for better results. We
assume that all samples in the oracle set are assigned their correct
classifications. They may come from aforementioned administrator
verification, or automated approaches such as malware detection
via sophisticated dynamic analysis. In either case, we can afford to
verify the oracle set but not the entire training set because the oracle
set is often orders of magnitude smaller than the training set.
Although the causal unlearning idea is intuitive, KARMA faces
two challenges. First, the search space for causality in the training
set is very large, but at the same time KARMA needs to inspect the
entire space to avoid evasion. Second, a large training set can also
make it costly to compute a new model after removing a subset. To
speed up the search for causality, KARMA adopts a heuristics that
balances search coverage and speed based on that similar causes
will lead to similar effects with a high probability. Specifically, if
two training samples, serving as causes in KARMA, are very similar
and share the same label, their influences on the learning system
are also similar, i.e, there is a higher chance that they are both
polluted or unpolluted. To speed up model computation, KARMA
leverages machine unlearning [14], but also works with incremental
or decremental machine learning [15, 20, 37, 42, 43].
We evaluated KARMA on three systems covering two popular
learning algorithms (Bayes and SVM) and two application domains
(spam and malware detection). Our results show:
• KARMA reduces manual efforts. Specifically, in an attack scenario
from Nelson et al. [33], i.e., 1% of samples are polluted, KARMA
reduces the manual effort from the entire training set to 3% of
the training set, i.e., 2% as an oracle set and 1% as the identified
polluted samples, an over 30× reduction.
• KARMA is robust to a variety of attacks with different parameters.
Specifically, KARMA repairs learning models affected by a wide
variety of 95 data pollution attacks ranging from mislabelling to
injection attacks, with different tactics such as targeted and blind,
and having different pollution rates from 0.5% to 30%.
• KARMA is accurate. Specifically, KARMA identifies 99.2% pol-
luted samples in median with the minimum as 98.0% and the
maximum 99.97%.
• KARMA is effective. Specifically, KARMA restores the accuracy
of polluted learning models against a third dataset to the vanilla
one within 1% differences.
This paper makes three main contributions. At a conceptual level,
causal unlearning is the first approach to efficient repair of learn-
ing systems, and may inspire many possible systems toward this
direction. At a system level, we have built KARMA, a causal un-
learning system that uses several mechanisms to efficiently deter-
mine the set of polluted data samples with high precision and recall.
KARMA is open source and available at the following repository
(https://github.com/CausalUnlearning/KARMA). At an evaluation
level, we show that our approach works with real-world machine
learning systems and greatly reduces manual effort required to repair
a polluted system.
Our work is only the first step toward practical repair of learning
systems; more challenges lie ahead. How can we perform causal
unlearning on other machine learning algorithms and systems? How
can we repair a system that experienced other types of attacks target-
ing machine learning? While removing training samples is one way
to repair or improve a learning system, adding samples is another
which KARMA does not support. We hope other researchers will join
us in addressing these challenges.
2 THREAT MODEL
The threat model of KARMA assumes one learning system and three
parties—i.e., an administrator of the system, users of the system,
and an attacker. The administrator is absolutely trustworthy, being
responsible for training and maintaining the learning system; the
attacker is malicious and tries to subvert the system by polluting
the training dataset; most users are trustworthy, but some of them
may have malicious intent. Note that we do not restrict the capability
of the attacker, i.e., theoretically the attacker can pollute arbitrary
number of training data. In practice, as long as the pollution is
effective, the attacker also wants to minimize the number of polluted
training data and reduce her chance of being caught. Depending on
how the administrator collects the training dataset, we list two attack
scenarios where an attacker can pollute training set.
Scenario One—Mislabelling Attack: In this scenario, an adminis-
trator of a learning system adopts crowdsourcing, such as asking
Amazon Mechanical Turks, to label training samples. Some of the
crowdsourcing workers have malicious intents, i.e., they will misla-
bel2 samples provided by the administrator, to pollute the learning
model. In this scenario, the capability of attackers is limited in pol-
luting the labels but not contents of training samples, because all the
samples are provided by the administrator.
Scenario Two—Injection Attack: In this scenario, an administrator
of a learning system tries to collect malicious samples, such as
spam and malware, through a honeypot-based technique. An attacker
figures out the purpose of the honeypot and then intentionally sends
crafted, polluted samples to the honeypot so that such samples will
be include in the training set. Note that attackers, different from
the first scenario, are able to craft and inject contents. However, the
2In this paper, misclassified samples (emails) refer to these that are incorrectly classified
by the learning model; mislabeled samples (emails) refer to these in the training set that
are incorrectly labeled by the attacker.
Causal Unlearning
ASIA CCS ’18, June 4–8, 2018, Incheon, Republic of Korea
which samples are polluted and whether the misclassification is
caused by data pollution (step five). After that, the administrator can
ask our system to repair the learning model by removing verified
polluted samples (step six).
Note that KARMA greatly relieves the burden of the oracle. With-
out KARMA, an administrator needs to first verify misclassifications
reported by users and confirms that the model misbehaves. Then, the
oracle needs to confirm misclassifications, go over all training sam-
ples and find pollutions. Now, with KARMA, the oracle still verifies
misclassifications reported by users, but then only needs to verify
the dataset reported by the users and misclassification cause, both
smaller than the entire training dataset. As shown in the evaluation
(Section 6), the size of oracle set is less than 2% of training data.
The size of misclassification cause highly depends on the attacker’s
strategies, which varies from 0.5% to 30% of training data in our
experiment. According to Nelson et al. [33], only 1% of samples
are needed to subvert an email filter. It is our future work to further
decrease the size of samples to be inspected by the administrator.
4 DESIGN
We present the design of KARMA in this section.
4.1 Overview
Let us first discuss the inputs and outputs of KARMA. Specifically,
KARMA takes three inputs: one machine learning model (M), usually
a replicate of the deployed learning model for analysis purpose,
and two datasets. The first set Str aininд—the one used to generate
M—is large and potentially polluted by an attacker; the second set
Sor acle is a small dataset mostly coming from misclassification
reported by users of M (Step 2 of the deployment model in Figure 1).
Sor acle is verified by an oracle, such as the administrator of M.
The output of KARMA is another dataset Scause that leads to the
misclassifications of Sor acle when classified by M. In KARMA, the
degree of misclassifications can be represented as the detection
accuracy of M against Sor acle defined in Equation 1.
|Sor acl e |
(1)
= 1 − |{x is misclassif ied by M |x ∈ Sor acl e }|
Accur acySor acl e
Now let us discuss how the administrator validates Sor acle , which
comes from what the users report. Specifically, the administrator’s
job can be summarized as follows. Note that the amount work for
the administrator is minimized because everything is performed on
a small number of Sor acle not the entire Str aininд.
• Adding user-reported misclassified samples to Sor acle iteratively.
Once the administrator collects some misclassified samples as
Sor acle , she can run KARMA using to partially repair M by find-
ing the misclassification cause and removing a subset of polluted
training samples. Because M is still polluted and produces incor-
rect results, i.e., misclassifying samples, users of M will report
further misclassifications to the administrator. Then, the admin-
istrator can construct a new Sor acle , and ask KARMA to further
repair M. We have a detailed evaluation about this scenario in
Section 6.5.
• Removing falsely reported samples from users with malicious
intent. Once the administrator finds that some reported samples
are correctly classified, she can remove such samples as shown
Figure 1: Deployment Model (spam detectors as an example).
attacker can only control one class of samples, i.e., malicious ones,
because a honeypot usually collects just malicious samples.
3 DEPLOYMENT MODEL
When we deploy KARMA with a learning system, there are tree
stages in the lifecycle of deployment: training, use and repair. In the
training stage, the administrator will train a learning model based
on potentially polluted training data. Then, in the use stage, the
administrator will obtain feedbacks of the learning system from