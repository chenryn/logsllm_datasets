# Efficient Repair of Polluted Machine Learning Systems via Causal Unlearning

**Authors:**
- Yinzhi Cao, Lehigh University, Bethlehem, PA
- Alexander Fangxiao Yu, Columbia University, New York, NY
- Andrew Aday, Columbia University, New York, NY
- Eric Stahl, Lehigh University, Bethlehem, PA
- Jon Merwine, Lehigh University, Bethlehem, PA
- Junfeng Yang, Columbia University, New York, NY

## Abstract
Machine learning (ML) systems, despite their success in various real-world applications, remain vulnerable to errors and attacks. One significant type of attack is data pollution, where maliciously crafted training data samples are injected into the training set, leading to incorrect model learning and subsequent misclassification of test samples. A natural solution to a data pollution attack is to remove the polluted data from the training set and relearn a clean model. However, this process is impractical for large datasets due to the manual effort required.

This paper introduces an approach called **causal unlearning** and a corresponding system called **KARMA** to efficiently repair polluted ML systems. KARMA significantly reduces the manual effort by automatically detecting polluted training data samples with high precision and recall. Evaluation on three ML systems demonstrates that KARMA greatly reduces the manual effort for repair and achieves high precision and recall.

## CCS Concepts
- Computing methodologies → Machine learning
- Security and privacy → Systems security

## Keywords
- Causality
- Data Pollution Attacks
- Machine Unlearning

## ACM Reference Format
Yinzhi Cao, Alexander Fangxiao Yu, Andrew Aday, Eric Stahl, Jon Merwine, and Junfeng Yang. 2018. Efficient Repair of Polluted Machine Learning Systems via Causal Unlearning. In Proceedings of 2018 ACM Asia Conference on Computer and Communications Security, Incheon, Republic of Korea, June 4–8, 2018 (ASIA CCS ’18), 13 pages. https://doi.org/10.1145/3196494.3196517

## Introduction
Machine learning systems are increasingly important in today's world, from product recommendations to self-driving cars. However, these systems remain vulnerable to various attacks, and defense mechanisms are not well-studied. One major attack, known as data pollution, injects maliciously crafted training data samples into the training set, causing the system to learn an incorrect model and misclassify test samples.

A recent example of such an attack is Microsoft’s AI-powered chatbot Tay, which learned racism due to offensive, racist words included in its training set. Another example is the work by Wang et al., which shows that false samples in the training set can mislead a machine learning classifier for detecting malicious crowdsourcing workers. Similarly, Perdisci et al. demonstrated that well-crafted fake network flows can significantly influence the worm signatures generated by PolyGraph, a worm detection engine.

The natural solution to a data pollution attack is to remove the polluted data from the training set and relearn a clean model. However, this is impractical for large datasets, as manually inspecting millions of samples is infeasible. This overwhelming amount of manual cleaning is why Microsoft took Tay offline for repair but has yet to bring it back online.

This paper presents **KARMA**, the first system designed for efficient repair of polluted machine learning systems. KARMA dramatically reduces the manual effort required by automatically detecting the set of polluted training data samples with high precision and recall. The key idea in KARMA is **causal unlearning**. Specifically, to launch a data pollution attack, an attacker leaves a causality chain from the polluted training samples to a polluted learning model and to misclassified test samples. KARMA leverages this causality trace to identify the subset of training samples that cause the most misclassifications.

KARMA reduces the manual effort to two parts:
1. It assumes that some users report misclassified test samples (e.g., as in the Microsoft Tay example or spam detection) and relies on administrators to verify these reports.
2. It relies on administrators to inspect the set of polluted samples it returns. KARMA determines the set of polluted samples using the causality of misclassifications, not the content of the samples, which may result in false positives (flagging unpolluted outliers) and false negatives (missing polluted samples).

To facilitate discussion, we term the set of user-reported misclassified test samples the **oracle set**. Administrators can augment this oracle set with correctly classified test samples for better results. We assume that all samples in the oracle set are assigned their correct classifications, either through administrator verification or automated approaches like malware detection via sophisticated dynamic analysis.

Although the causal unlearning idea is intuitive, KARMA faces two challenges:
1. The search space for causality in the training set is very large, and KARMA needs to inspect the entire space to avoid evasion.
2. A large training set can make it costly to compute a new model after removing a subset.

To speed up the search for causality, KARMA uses heuristics that balance search coverage and speed based on the similarity of causes. To speed up model computation, KARMA leverages machine unlearning techniques and works with incremental or decremental machine learning.

We evaluated KARMA on three systems covering two popular learning algorithms (Bayes and SVM) and two application domains (spam and malware detection). Our results show:
- KARMA reduces manual efforts. In an attack scenario where 1% of samples are polluted, KARMA reduces the manual effort from the entire training set to 3% of the training set, an over 30× reduction.
- KARMA is robust to a variety of attacks with different parameters, repairing models affected by 95 different data pollution attacks with varying tactics and pollution rates.
- KARMA is accurate, identifying 99.2% of polluted samples in median, with a minimum of 98.0% and a maximum of 99.97%.
- KARMA is effective, restoring the accuracy of polluted learning models against a third dataset to within 1% of the original accuracy.

This paper makes three main contributions:
1. At a conceptual level, causal unlearning is the first approach to efficient repair of learning systems, potentially inspiring future research.
2. At a system level, KARMA is a causal unlearning system that efficiently determines the set of polluted data samples with high precision and recall. KARMA is open source and available at [https://github.com/CausalUnlearning/KARMA].
3. At an evaluation level, we demonstrate that our approach works with real-world machine learning systems and significantly reduces the manual effort required to repair a polluted system.

Our work is the first step toward practical repair of learning systems, and more challenges lie ahead. Future research could explore causal unlearning for other machine learning algorithms and systems, address other types of attacks, and consider adding samples to improve learning systems.

## Threat Model
KARMA's threat model involves one learning system and three parties: the administrator, users, and an attacker. The administrator is trustworthy and responsible for training and maintaining the system; the attacker is malicious and tries to subvert the system by polluting the training dataset; most users are trustworthy, but some may have malicious intent. The attacker's capability is not restricted, meaning they can theoretically pollute any number of training data. In practice, the attacker aims to minimize the number of polluted samples to reduce the chance of being caught.

### Attack Scenarios
1. **Mislabelling Attack**: An administrator uses crowdsourcing (e.g., Amazon Mechanical Turk) to label training samples. Malicious crowdsourcing workers mislabel samples to pollute the learning model.
2. **Injection Attack**: An administrator collects malicious samples (e.g., spam and malware) through a honeypot. An attacker sends crafted, polluted samples to the honeypot to include them in the training set.

## Deployment Model
When deploying KARMA with a learning system, there are three stages: training, use, and repair.
- **Training Stage**: The administrator trains a learning model based on potentially polluted training data.
- **Use Stage**: The administrator obtains feedback from the learning system and users.
- **Repair Stage**: KARMA identifies and removes polluted samples, reducing the manual effort required for repair.

KARMA greatly relieves the burden on the administrator by minimizing the need to manually inspect the entire training set. Instead, the administrator only needs to verify the smaller oracle set and the misclassification cause, both of which are significantly smaller than the entire training dataset.

## Design
### Overview
KARMA takes three inputs:
1. A machine learning model (M), typically a replicate of the deployed model for analysis.
2. Two datasets: 
   - **S_train**: The large, potentially polluted training set used to generate M.
   - **S_oracle**: A small dataset of misclassified samples reported by users and verified by the administrator.

The output of KARMA is another dataset **S_cause** that leads to the misclassifications of **S_oracle** when classified by M. The degree of misclassifications is represented by the detection accuracy of M against **S_oracle**.

### Administrator Validation
The administrator's role includes:
- Iteratively adding user-reported misclassified samples to **S_oracle**.
- Removing falsely reported samples from users with malicious intent.

### Detailed Evaluation
Our evaluation on three systems demonstrates that KARMA significantly reduces the manual effort required for repair, achieving high precision and recall. The size of the oracle set is less than 2% of the training data, and the size of the misclassification cause varies from 0.5% to 30% of the training data, depending on the attacker's strategy.

## Conclusion
KARMA is a novel system for efficient repair of polluted machine learning systems. By leveraging causal unlearning, KARMA reduces the manual effort required for repair and achieves high precision and recall. Our work opens the door to further research in this area, including the application of causal unlearning to other machine learning algorithms and systems, and the development of methods to handle other types of attacks.