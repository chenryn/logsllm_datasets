Once Jenkins is configured to locate the test reports, it excels at presenting and analyzing them. One of Jenkins's primary functions is to detect and report on build failures, with a failing unit test being one of the most common indicators.

Jenkins differentiates between failed builds and unstable builds. A failed build, indicated by a red ball, signifies either test failures or a critical issue such as a compilation error. An unstable build, on the other hand, is one that does not meet the desired quality standards, which can be defined by various metrics like code coverage or adherence to coding standards. These quality metrics will be discussed in more detail later in this book. For now, letâ€™s focus on failed builds.

In Figure 6.5, "Jenkins displays test result trends on the project home page," you can see how Jenkins presents a Maven build job with test failures. This is the build job's home page, which should be your first point of reference when a build fails. When a build results in failing tests, the "Latest Test Result" link will show the current number of test failures ("5 failures" in the illustration) and the change in the number of test failures since the last build ("+5" in the illustration, indicating five new test failures). The "Test Result Trend" graph also provides a historical view of test failures, with previous build failures appearing in red.

**Figure 6.5. Jenkins displays test result trends on the project home page**

Clicking on the "Latest Test Result" link provides a detailed summary of the current test results (see Figure 6.6, "Jenkins displays a summary of the test results"). For Maven multi-module projects, Jenkins initially shows a summary view of test results per module. You can drill down into specific modules for more details on failing tests. 

**Figure 6.6. Jenkins displays a summary of the test results**

For freestyle build jobs, Jenkins directly provides a summary of test results, organized by high-level packages rather than modules. In both cases, Jenkins starts with a summary of test results for each package, allowing you to further investigate individual test classes and their tests. Failed tests are prominently displayed at the top of the page, providing a clear overview of the current state and history of your tests. The "Age" column indicates how long a test has been failing, with a hyperlink to the first build where the failure occurred.

You can add a description to the test results using the "Edit Description" link in the top right-hand corner. This is useful for annotating build failures with additional details, such as the origin of the failure or notes on how to fix it.

To understand why a test failed, click on the corresponding link on the screen. This will display detailed information, including the error message, stack trace, and a reminder of how long the test has been failing (see Figure 6.7, "The details of a test failure"). Tests that have been failing for more than a few builds may indicate a complex technical issue or a complacent attitude towards build failures, both of which should be addressed.

**Figure 6.7. The details of a test failure**

It is also important to monitor the duration of your tests. Unit tests should run quickly, and overly long-running tests can signal performance issues. Slow tests delay feedback, which is crucial in continuous integration (CI). For example, running one thousand unit tests in five minutes is acceptable, but taking an hour is not. Regularly check the test run times, and investigate if they are too long.

Jenkins can help you track test run times over time. On the build job home page, click the "trend" link in the "Build History" box on the left. This will display a graph (see Figure 6.8, "Build time trends can give you a good indicator of how fast your tests are running") showing the duration of each build. While tests are not the only component of a build, they often take a significant portion of the time. This graph is a valuable tool for assessing test performance.

**Figure 6.8. Build time trends can give you a good indicator of how fast your tests are running**

On the "Test Results" page, you can also drill down to see the run times for tests in specific modules, packages, or classes. Click on the test duration in the test results page ("Took 31 ms" in Figure 6.6, "Jenkins displays a summary of the test results") to view the test history (see Figure 6.9, "Jenkins also lets you see how long your tests take to run"). This makes it easy to identify slow tests and determine when general optimization is needed.

**Figure 6.9. Jenkins also lets you see how long your tests take to run**

### 6.5. Ignoring Tests

Jenkins distinguishes between test failures and skipped tests. Skipped tests are those that have been deactivated, often using the `@Ignore` annotation in JUnit 4:

```java
@Ignore("Pending more details from the BA")
@Test
public void cashWithdrawalShouldDeductSumFromBalance() throws Exception {
    Account account = new Account();
    account.makeDeposit(100);
    account.makeCashWithdraw(60);
    assertThat(account.getBalance(), is(40));
}
```

Skipping tests is sometimes necessary, such as when implementing lower-level functionality while holding higher-level tests. Using `@Ignore` is better than commenting out or renaming tests, as it allows Jenkins to keep track of ignored tests.

In TestNG, you can skip tests using the `enabled` property:

```java
@Test(enabled=false)
public void cashWithdrawalShouldDeductSumFromBalance() throws Exception {
    Account account = new Account();
    account.makeDeposit(100);
    account.makeCashWithdraw(60);
    assertThat(account.getBalance(), is(40));
}
```

TestNG also supports defining dependencies between tests, so certain tests only run after others have completed:

```java
@Test
public void serverStartedOk() {...}

@Test(dependsOnMethods = { "serverStartedOk" })
public void whenAUserLogsOnWithACorrectUsernameAndPasswordTheHomePageIsDisplayed(){...}
```

If the first test (`serverStartedOk()`) fails, the subsequent test will be skipped. Jenkins marks skipped tests as yellow in both the overall test results trend and the test details (see Figure 6.10, "Jenkins displays skipped tests as yellow"). While skipped tests are not as severe as failures, it is important not to neglect them. Skipped tests should be reactivated once the reason for skipping is resolved.

**Figure 6.10. Jenkins displays skipped tests as yellow**

### 6.6. Code Coverage

Code coverage is another valuable metric, indicating which parts of your application were executed during tests. While high code coverage does not guarantee quality testing, it is a good indicator of untested code. If your team practices Test-Driven Development (TDD), code coverage can also reflect the effectiveness of these practices.

Code coverage analysis is resource-intensive and can significantly slow down builds. Therefore, it is often run in a separate Jenkins build job after unit and integration tests have succeeded. Jenkins supports several code coverage tools through dedicated plugins, such as Cobertura and Emma for Java, and NCover for .NET projects.

### 6.6.1. Measuring Code Coverage with Cobertura

Cobertura is an open-source code coverage tool for Java and Groovy, easily integrated with Maven and Jenkins. The Cobertura plugin for Jenkins does not generate code coverage data; it reports on data generated during the build process. Jenkins excels at tracking code coverage over time and providing aggregate coverage across multiple modules.

Code coverage analysis involves three steps:
1. **Instrumentation:** Modify application classes to track the number of times each line of code is executed.
2. **Execution:** Run tests against the instrumented code to generate a data file.
3. **Reporting:** Use the data file to generate a report in a usable format, such as XML or HTML.

#### 6.6.1.1. Integrating Cobertura with Maven

To use Cobertura in Maven, add the `cobertura-maven-plugin` to the `build` section of your `pom.xml`:

```xml
<build>
    <plugins>
        <plugin>
            <groupId>org.codehaus.mojo</groupId>
            <artifactId>cobertura-maven-plugin</artifactId>
            <version>2.5.1</version>
            <configuration>
                <formats>
                    <format>html</format>
                    <format>xml</format>
                </formats>
            </configuration>
        </plugin>
    </plugins>
</build>
```

Generate code coverage data by invoking the Cobertura plugin:

```sh
$ mvn cobertura:cobertura
```

This will create a `coverage.xml` file in the `target/site/cobertura` directory. To avoid generating coverage data for every build, place the configuration in a profile:

```xml
<profiles>
    <profile>
        <id>metrics</id>
        <build>
            <plugins>
                <plugin>
                    <groupId>org.codehaus.mojo</groupId>
                    <artifactId>cobertura-maven-plugin</artifactId>
                    <version>2.5.1</version>
                    <configuration>
                        <formats>
                            <format>html</format>
                            <format>xml</format>
                        </formats>
                    </configuration>
                </plugin>
            </plugins>
        </build>
    </profile>
</profiles>
```

Invoke the Cobertura plugin using the `metrics` profile:

```sh
$ mvn cobertura:cobertura -Pmetrics
```

Alternatively, include code coverage reporting in your Maven site generation:

```xml
<reporting>
    <plugins>
        <plugin>
            <groupId>org.codehaus.mojo</groupId>
            <artifactId>cobertura-maven-plugin</artifactId>
            <version>2.5.1</version>
            <configuration>
                <formats>
                    <format>html</format>
                    <format>xml</format>
                </formats>
            </configuration>
        </plugin>
    </plugins>
</reporting>
```

Generate the Maven site to produce the coverage report:

```sh
$ mvn site
```

For multi-module projects, set up the Cobertura configuration in the parent `pom.xml`. The Maven Cobertura plugin will generate separate reports for each module and an aggregate report if the `aggregate` option is used. However, the Jenkins Cobertura plugin will combine coverage data from multiple files into a single report.

Note that the Maven Cobertura plugin currently only records coverage for tests executed during the `test` phase, not the `integration-test` phase. This can be an issue if you run integration or web tests during the `integration-test` phase, as their coverage will not be included.

#### 6.6.1.2. Integrating Cobertura with Ant

Integrating Cobertura with Ant is more complex but offers finer control over instrumentation and coverage measurement. Cobertura provides an Ant task for this purpose. Download the latest Cobertura distribution and place it in your project directory, e.g., `tools`.

Configure Ant to use Cobertura:

```xml
<property name="cobertura.dir" value="tools/cobertura"/>

<path id="cobertura.classpath">
    <fileset dir="${cobertura.dir}/lib">
        <include name="*.jar"/>
    </fileset>
</path>

<taskdef resource="tasks.properties" classpathref="cobertura.classpath"/>
```

Instrument your application classes carefully, placing the instrumented classes in a separate directory:

```xml
<cobertura-instrument todir="instrumented-classes">
    <fileset dir="classes">
        <include name="**/*.class"/>
    </fileset>
</cobertura-instrument>
```

Run your tests against the instrumented classes and generate the coverage data:

```xml
<junit printsummary="yes" haltonfailure="no">
    <classpath>
        <pathelement location="instrumented-classes"/>
        <pathelement location="test-classes"/>
        <path refid="cobertura.classpath"/>
    </classpath>
    <formatter type="plain"/>
    <batchtest todir="reports">
        <fileset dir="test-classes">
            <include name="**/*Test.class"/>
        </fileset>
    </batchtest>
</junit>

<cobertura-report datafile="cobertura.ser" destdir="coverage-reports">
    <fileset dir="instrumented-classes">
        <include name="**/*.class"/>
    </fileset>
</cobertura-report>
```

This setup ensures that Cobertura is integrated into your Ant build, allowing you to measure and report on code coverage effectively.