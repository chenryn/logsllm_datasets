0-7695-2282-3/05 $20.00 © 2005 IEEE
Erasure
Code
2-of-4
3-of-5
3-of-6
4-of-6
4-of-7
5-of-7
Failure
Resiliency
(c=client,
s=storage)
1c1s, 0c2s
1c1s, 0c2s
1c1s, 0c3s
1c1s, 0c2s
1c1s, 0c3s
1c1s, 0c2s
Delta
+Add
(µs)
Full
Encode
(µs)
Full
Decode
(µs)
5
4
4
5
4
4
8
8
11
14
15
12
8
8
12
15
15
13
(a) Codes used in real runs
n-k=8,full encode
n-k=2,full encode
n-k=2,delta+add
n-k=8,delta+add
)
s
m
(
e
m
T
i
10
1
.1
.01
 1
 2
 4
 8
 16  32  64  128
k (# of data blocks)
 16
 12
s
e
r
u
l
i
a
f
e
d
o
n
e
g
a
r
o
S
t
n-k=16
n-k=8
n-k=4
n-k=2
 8
 4
 0
 0
 1
 2
 3
 4
Client failures
(b) Codes used in simulations
(c) Failure resiliency in simulations
Fig. 8. Performance and Fault-resilience of erasure codes.
)
s
/
B
M
(
t
u
p
h
g
u
o
r
h
t
.
g
g
A
 30
 25
 20
 15
 10
 5
 0
4-of-6
2-of-4
3-of-5
 0  20  40  60  80  100 120
Simultaneous reqs/client
 4
 3
 2
r
o
t
c
a
F
 1
 1
r
o
t
c
a
F
 1
 0.8
 0.6
 0.4
 0.2
 0
 4
k=4,1 client
k=3,2 clients
k=3,1 client
 1
 2
 3
Redundant blocks (n-k)
r
o
t
c
a
F
 1
 0.8
 0.6
 0.4
 0.2
 0
4-of-6
3-of-5
2-of-4
 3
Clients
 2
Time
of
crash
Reads
Writes
 0
 30
 60
 90
Time (minutes)
(a) Write TP vs. #reqs
(b) Write TP vs. #clients
(c) Write TP vs. redundancy
(d) TP when storage node crashes
Fig. 9. Results from real runs.
)
s
/
B
M
(
t
u
p
h
g
u
o
r
h
t
.
g
g
A
 400
 300
 200
 100
 0
 0
n=16,n-k=2
n=32,n-k=8
n=16,n-k=4
n=8,n-k=4
 20
 40
 60
Clients
n=32
n=16
n=8
n=4
 2000
 1500
 1000
)
s
/
B
M
(
t
u
p
h
g
u
o
r
h
t
.
g
g
A
 500
 0
 0
 20
 40
 60
Clients
)
s
/
B
M
(
t
u
p
h
g
u
o
r
h
t
.
g
g
A
 600
 400
 200
 0
 0
n=32
n=16
n=8
n=4
)
s
/
B
M
(
t
u
p
h
g
u
o
r
h
t
.
g
g
A
 4
 8
 12
 16
Redundant blocks (n-k)
 1000
 100
 10
64 clients,n=32
1 client,n=32
 4
 8
 1
 0
 12
 16
Redundant blocks (n-k)
(a) Write TP vs. #clients
(b) Read TP vs. #clients
(c) Write TP vs. redundancy
(d) Write TP using broadcast
Fig. 10. Results from simulations.
is reasonable as demonstrated by experiments. Our protocol allows
the use of highly-efﬁcient erasure codes, i.e., codes with large n and
k, and small n − k. We envision a system that uses our protocol to
build an industrial-strength distributed disk array with cheap adapters
to connect disks to a network, powerful machines to serve as the array
nodes, and highly-efﬁcient erasure codes to tolerate multiple disk and
array node crashes. External parties send requests for logical blocks
to the array nodes; array nodes act as “clients” in our protocol, while
the cheap adapters act as “storage nodes”.
Acknowledgements. We are grateful to our shepherd Yair Amir,
and to Mark Lillibridge, Janet Wiener and John Wilkes for suggestions
that improved the paper. We thank Cheng Huang and Sinchan Mitra
for providing computing resources.
References
[1] P. Corbett et. al., “Row-diagonal parity for double disk failure correction,”
[2] V. Pless, Introduction to the Theory of Error-Correcting Codes, Wiley-
in Proceedings of FAST, 2004.
Interscience, 1998.
[3] J. Kubiatowicz et al.,
“Oceanstore: An architecture for global-scale
persistent storage,” in Proceedings of ASPLOS, 2000.
[4] F. Chang et al.,
“Myriad: Cost-effective Disaster Tolerance,”
in
Proceedings of FAST, 2002.
[5] S. Frolund et. al., “A decentralized algorithm for erasure-coded virtual
disks,” in Proceedings of DSN, 2004.
[6] G. R. Goodson, J. J. Wylie, G. R. Ganger, and M. K. Reiter, “Efﬁcient
byzantine-tolerant erasure-coded storage,” in Proceedings of DSN, 2004.
[7] Z. Zhang and Q. Lian, “Reperasure: Replication protocol using erasure-
code in peer-to-peer storage,” in Proceedings of SRDS, 2002.
[8] W. Litwin and T. Schwarz,
“LH* RS : A high-availability scalable
distributed data structure using reed solomon codes,” in Proceedings
of SIGMOD, 2000.
[9] M. K. Aguilera, R. Janakiraman, and L. Xu,
tolerant distributed storage using erasure codes,”
Washington University in St. Louis, Feb 2004,
http://www.nisl.wustl.edu/Papers/Tech/aguilera04efﬁcient.pdf.
“Efﬁcient
fault-
Tech. Rep.,
Available at
[10] F. Schneider,
“Byzantine generals in actions: implementing fail-stop
processors,” ACM Transactions on Computer Systems, vol. 2, no. 2,
pp. 145–154, May 1984.
[11] L. Lamport, “On interprocess communication,” Distributed computing,
vol. 1, no. 1, pp. 77–101, 1986.
[12] C. Shao, E. Pierce, and Jennifer L. Welch, “Multi-writer consistency
in Proceedings of ICDCS,
conditions for shared memory objects,”
October 2003, pp. 106–120.
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE