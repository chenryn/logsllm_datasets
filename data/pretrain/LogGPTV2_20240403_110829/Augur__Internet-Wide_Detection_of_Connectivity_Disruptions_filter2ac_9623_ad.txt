Pr[Xn = 0|K1] = θ1, Pr[Xn = 1|K1] = 1 − θ1
In this construction 1 − θ0 is the measurable probability of
observing IP ID acceleration during injection, and θ1 is the
measurable prior probability of seeing no IP ID acceleration
during the SAR window across all of the reﬂector’s measure-
ments. Similar arguments hold as above to why these provide
conservative estimations of the prior values. (We also discuss
the measurable IP ID acceleration during the SAR window in
Section V-D.) Figure 2 shows how this construction is used to
label Si, Rj as either outbound-blocked or not blocked. If the
thresholds are not met and there are no more trials, we output
that we know Si is not inbound-blocked, but we do not know
the outbound-block status.
Expected number of trials. The SHT construction from Jung
et al. also provides a framework for calculating the expected
number of trials needed to arrive at a decision for H0 and H1.
The expected values are deﬁned as:
E[N|H0] =
E[N|H1] =
α ln β
θ0 ln θ1
θ0
β ln β
θ1 ln θ1
θ0
α + (1 − α) ln 1−β
1−α
+ (1 − θ0) ln 1−θ1
1−θ0
α + (1 − β) ln 1−β
1−α
+ (1 − θ1) ln 1−θ1
1−θ0
,
.
(1)
where α and β are also bounded by functions of the tolera-
ble false positive and negative rates, discussed subsequently.
433
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:18:21 UTC from IEEE Xplore.  Restrictions apply. 
Similar constructions hold for K0 and K1. We investigate
the expected number of trials for both inbound and outbound
blocking further in Section V-D.
False positives and negatives. Following the construction
from Jung et al., α and β are both tunable parameters which
are bounded by our tolerance to both false positives and false
negatives. PF is deﬁned as the false positive probability, and
PD as the detection probability. The complement of PD,
1 − PD is the probability of false negatives. These values
express the probability of a false result for a single SHT
experiment. However, for our method, we perform numerous
SHT experiments across sites and reﬂectors. To account for
these repeated trials we set both PF and 1−PD = 10−5. Given
that as PF and 1− PD decrease, the expected number of trials
to reach a decision increases, our selection of a small value
negatively impacts our ability to make decisions. This effect
is somewhat mitigated by the distance between experimentally
observed priors, and is explored in more detail in Section V-D
and Figure 4.
V. AUGUR IMPLEMENTATION AND EXPERIMENT DATA
In this section, we discuss the deployment of our approach
to measure connectivity disruptions across the Internet, as well
as the setup that we use to validate the detection method from
Section IV.
A. Selecting Reﬂectors and Sites
Reﬂector selection. To ﬁnd reﬂectors that satisfy the criteria
from Section IV, we created a new ZMap [20] probe module
that sends SYN-ACK packets and looks for well-formed RST
responses. Our module is now part of the open-source ZMap
distribution. Using this module, we scan the entire IPv4
address space on port 80 to identify possible reﬂectors.
We then perform a second set of probes against this list of
candidate reﬂectors to identify a subset that conforms to the
desired IP ID behavior. Our tool runs from the measurement
machine and sends ten SYN-ACK packets to port 80 of each
host precisely one second apart, recording the IP ID of each
RST response. We identify reﬂectors whose IP ID behaviors
satisfy the previously outlined requirements: no IP ID wrap-
ping, variable accelerations observed (indicating our packets
do induce perturbations in the IP ID dynamics), and a response
to all probes. Because the measurement machine induces
packet generation at
the reﬂector at a constant rate, any
additional IP ID acceleration must be due to trafﬁc from
other connections. We further ensure that the measurement
machine receives a response for each probe packet that it
sends, ensuring that the reﬂector is stable and reliable enough
to support continuous measurements.
This selection method identiﬁes viable reﬂectors, those that
are responsive and exhibit the desired IP ID behavior. We
ﬁnally ﬁlter the viable reﬂectors that do not correspond to in-
frastructure, as described in Section III-C, which signiﬁcantly
Reﬂector
Datasets
All Viable
Ethically Usable
Experiment Sample
Total
Reﬂectors
22,680,577
53,130
2,050
Num.
Countries
234
179
179
Median /
Country
1,667
15
15
TABLE I: Summary of our reﬂector datasets. All viable
reﬂectors are identiﬁed across the IPv4 address space. Those
ethically usable are routers at
two hops away from
traceroute endpoints in the Ark data, and we select a random
subset as our experiment set.
least
Reﬂector Dataset
All Viable
Ethically Usable
Experiment Sample
AF AS EU NA SA OC ME
20
55
36
18
36
18
14
6
6
50
47
47
52
46
46
39
30
30
23
14
14
TABLE II: The distribution of countries containing reﬂec-
tors across continents. Note the continent coverage of our
experiment sample is identical to that of the ethically us-
least one ethically usable
able dataset, as we sampled at
reﬂector per country in that dataset. The continent
labels
are as follows: AF=Africa, AS=Asia, EU=Europe, NA=North
America, SA=South America, OC=Oceana/Australia. We also
label ME=Middle East, as a region with frequent censorship.
reduces the number of available reﬂectors, as described in
Section V-E.
Site selection. We begin with a list of sites, some of which
are expected to be disrupted by network ﬁltering or censorship
from a variety of vantage points. We seed our candidate sites
with the Citizen Lab list of potentially censored URLs [15],
which we call the CLBL. This list contains potentially blocked
URLs, broken down by category. To further identify sensitive
URLs, we use Khattak et al.’s dataset [32] that probed these
URLs using the OONI [40] measurement platform looking
for active censorship. After ﬁltering the list, we distill the
URLs down to domain names and resolve all domains to
the corresponding IP addresses using the local recursive DNS
resolver on a network in the United States. If a domain name
resolves to more than one IP, we randomly select one A
record from the answers. To augment this list of sites, we
randomly select domains from the Alexa top 10,000 [2]. As
with the CLBL, if a host resolves to multiple IPs, we select
one at random. Section V-B provides a breakdown of the site
population. Section V-E explains how we dynamically enforce
site requirements.
B. Measurement Dataset
In this section, we describe the characteristics of the dataset
that we use for our experiments.
Reﬂector dataset. The geographic distribution of reﬂectors
illuminates the degree to which we can investigate censor-
ship or connectivity disruption within each country. Table I
summarizes the geographic diversity of our reﬂector datasets.
434
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:18:21 UTC from IEEE Xplore.  Restrictions apply. 
The Internet-wide ZMap scan found 140 million reachable
hosts. Approximately 22.7 million of these demonstrated use
of a shared, monotonically increasing IP ID. These reﬂectors
were geographically distributed across 234 countries around
the world, with a median of 1,667 reﬂectors per country. This
initial dataset provides a massive worldwide set of reﬂectors to
potentially measure, yet many may be home routers, servers,
or user machines that we cannot use for experimentation due
to ethical considerations.
Merging with the Ark to ensure that the reﬂectors only con-
tain network infrastructure reduces the 22.7 million potential
reﬂectors to only about 53,000. Despite this signiﬁcant reduc-
tion, the resulting dataset contains reﬂectors in 179 countries,
with a median of 15 reﬂectors per country. Table II gives a
breakdown of reﬂector coverage by continent.
We select a subset of these reﬂectors as our ﬁnal exper-
iment dataset, randomly choosing up to 16 reﬂectors in all
179 countries, yielding 1,947 reﬂectors (not all countries had
16 infrastructure reﬂectors). In addition to these reﬂectors,
we added 103 high-reliability (stable, good priors) reﬂectors
primarily from China and the US to ensure good coverage
with a stable set of reﬂectors, resulting in 2,050 reﬂectors
in the ﬁnal dataset. These reﬂectors also exhibit widespread
AS diversity, with the resulting set of reﬂectors representing
31,188 ASes. Using the Ark dataset to eliminate reﬂectors that
are not infrastructure endpoints reduces this set to 4,214 ASes,
with our ﬁnal experiment sample comprising 817 ASes.
Site dataset. Merging the CLBL with Khattak et al.’s
dataset [32] yields 1,210 distinct IP addresses. We added to
this set an additional 1,000 randomly selected sites from the
Alexa top 10,000. To this set of sites we also added several
known Tor bridges, as discussed in Section VI-C. While this
set consists of 2,213 sites, some sites appeared in both the
CLBL and Alexa lists. Thus, our site list contains a total of
2,134 unique sites, with a CLBL composition of 56.7%.
C. Experiment Setup
The selection process above left us able to measure connec-
tivity between 2,134 sites and 2,050 reﬂectors. We collected
connectivity disruption network measurements over 17 days,
using the method described in Section IV. We call one
measurement of a reﬂector-site pair a run, involving IP ID
monitoring and one instance of blocking detection. Related,
we deﬁne an experiment trial as the complete measurement
of one run for all reﬂector-site pairs. Over our 17-day window,
we collected a total of 207.6 million runs across 47 total trials,
meaning we tested each reﬂector-site pair 47 times.
Each run comprises of a collection of one-second time
intervals. For each time interval, we measure the IP ID state
of the reﬂector independent of all other tasks. We begin each
run by sending a non-spoofed SYN to the site from the
measurement machine. Doing so performs several functions.
First, it allows us to ensure that the site is up and responding
to SYNs at the time of the measurement. Second, it allows
us to precisely measure if the site sends SYN-ACK retries,
Fig. 3: CDF of probability of IP ID acceleration per reﬂector
across the experiment.
and to characterize the timing of the retries. We record this
behavior for each run and incorporate this initial data point
into the subsequent SHT analysis. We then wait four seconds
before injecting spoofed SYN packets towards the site. The
reﬂector measurements during that window serve as control
measurements. During the injection window, we inject 10
spoofed SYN packets towards the site.
For each run, we denote the SYN-ACK retry behavior and
at what subsequent window we expect SYN-ACK retries to
arrive at the reﬂector, and use this information to identify
which window to look for follow-on IP ID acceleration. At
the end of the run, we send corresponding RST packets for
all SYNs we generated, to induce tear-down of all host state.
We then cool down for 1 second before starting a new run.
We randomize the order of the sites and reﬂectors for testing
per trial. We test all reﬂector-site pairs before moving on
to a new trial. For reasons discussed earlier, we never involve
the same reﬂector and site in two independent simultaneous
measurements between endpoints.
After each run, we ensure that (1) the reﬂector’s IP ID
appeared to remain monotonically increasing; (2) no packet
loss occurred between the measurement machine and the
reﬂector, and (3) the site is up and responding to SYN packets.
Additionally, we ensure that the IP ID does not wrap during
either the injection window or the SAR window. We discard
the measurements if any of these conditions fails to hold.
After these validity checks, our dataset contains 182.5 million
runs across 1,960 reﬂectors and 2,089 sites. The reduction
in number of sites and reﬂectors corresponds to unstable or
down hosts. We then apply SHT (Section IV) to analyze the
reachability between these site-reﬂector pairs.
D. Measured Priors and Expectations
A critical piece in the construction of our SHT framework is
formulating the prior probabilities for each of our hypotheses.
435
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:18:21 UTC from IEEE Xplore.  Restrictions apply. 
Figure 3 shows CDFs of the measured prior probabilities of
IP ID acceleration for three different scenarios.
The IP ID acceleration of reﬂectors matches our intuition,
where the acceleration decreasing as frequently as it increases
across the dataset. We show this with the “No Injection”
CDF, with nearly all reﬂectors having a probability of IP ID
acceleration without injection of less than 0.5. Many reﬂectors
have a probability of acceleration far lower, corresponding to
reﬂectors with low or stable trafﬁc patterns. We then use this
per-reﬂector prior for θ1 in our SHT construction for detecting
inbound blocking. While we could instead estimate the value
as 0.5, the expected number of trials depends on the separation
between the injection and non-injection priors, so if we are
able to use a smaller θ1 (per reﬂector), this greatly speeds up
detection time.
Figure 3 also shows the probability of IP ID acceleration
under injection. This value approaches 1 for many reﬂectors
and is above 0.8 more than 90% of reﬂectors. Noticeably, it
is, however, quite low, and even 0 for a handful of reﬂectors.
These correspond to degenerate or broken reﬂectors that we
can easily identify due to their low priors, removing them
from our experiment (discussed more in Section V-E). We
use this experimentally measured prior as 1 − θ0 in both of
our sequential hypothesis tests. This distribution provides a