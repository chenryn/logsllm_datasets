clausal complement (i.e., its dependency is xcomp) and the receiver
is an object pronoun, it would be converted to a subject pronoun.
For example, in "you authorize us to collect your personal data to
provide the services," Arg0 of collect is us which is then converted
to we.
Conversion from Extracted Spans to Privacy Statements. Privacy
statements are generated from extracted spans by using transfor-
mation rules as listed in Table 4. Rules 𝑇1 and 𝑇2 convert spans with
a collection verb while rules 𝑇3–𝑇6 convert spans with a sharing
verb. A rationale behind rules 𝑇3–𝑇6 is that the data collection and
sharing are observed only at the client side (i.e., the app), and hence
the data collection or sharing on the server side is unknown. In
particular, rule 𝑇3 assumes the sender may have the data before
sharing it. Similarly, rule 𝑇4 means while the sender does not share
data, it may still collect the data. Rules 𝑇5 and 𝑇6 mean the receiver
may still collect data, but the data should not be used for the pur-
pose 𝑝. For example, given "we do not provide your personal data
to third parties for their own marketing purposes," we interpret this
statement as personal data can be transferred to third-party service
providers, but does not serve the third parties’ purposes.
Action Sentiment Extraction. The sentiment of a data practice can
be either positive or negated and indicates whether the data action
is performed or not, respectively. The sentiment is determined by
checking the presence of the negation argument Argm-Neg. If the
predicate has no Argm-Neg, PurPliance analyzes its dependency
tree to determine its negation using the method in PolicyLint [7].
For example, in "we never sell your data," sell has a negated sen-
timent because it has a negation argument never. However, the
negation of use-verbs does not generate not_collect because "app
does not use data A" does not mean the app does not collect data A.
Data Object Extraction. PurPliance extracts the text spans of the
objects of the privacy practice actions using SRL and extracts data
object noun phrases using named entity recognition (NER) [35].
First, argument Arg1 is mapped to the Data component since it
is the object of a verb across data practice action types. Second,
the verb argument is then further refined by using NER which is a
common technique used to extract data objects [7]. For example,
given "we may use your name and street address for delivery", NER
extracts "your name" and "street address" from the corresponding
argument identified by SRL.
Purpose-Served Entity Extraction. Although it is more accurate to
determine the purpose-served entities by performing co-reference
resolution [35], PurPliance uses keyword matching to extract
purpose-served entities. PurPliance leverages an observation that
"their" commonly means third parties because data-practice state-
ments in privacy policies are frequently between first-party/users
and third parties, such as in the sentence "third parties may not use
personal data for their own marketing purposes." Similarly, "our" in
purpose clauses commonly refers to first parties. If no such entities
were found, the purposed-served entity is set to "any party".
Purpose analysis allows more accurate interpretations of certain
sharing statements. In particular, when user data is used for "mone-
tary" or "profitable" purposes or when the data practice is to "lease",
"rent", "sell" and "trade" the user’s data, we interpret that the data
is shared for a third party’s purposes. To reduce false positives, we
only include the Marketing – Provide ad purpose which is the most
common. When an advertiser collects a data object for advertising
purposes, the purpose-served entity is also set to the advertiser.
Exception Clauses. Given a sentence that includes an exception
clause which does not contain data objects or entities, if the privacy
statement extracted from the sentence has a negated sentiment,
PurPliance changes the sentiment of the privacy statement to be
positive. For example, "we do not share your personal data with
third parties for their marketing purposes without your consent"
produces text spans (we, share, third party, your personal data, (their,
marketing)). This exception clause handling is similar to PolicyLint.
PurPliance generates additional privacy statements in certain
cases when a sentence contains exceptions about purposes. If the
sentence is negated and contains "other than [purpose clause],"
the data will not be used for other high-level purposes. Excluding
other high-level purposes which are semantically non-overlapping
produces fewer false positives than excluding other low-level pur-
poses.
Similarly, PurPliance creates opposite-sentiment privacy state-
ments for other purposes if the sentence contains "for [purpose
clause] only," or "only for [purpose clause]." PurPliance also ex-
cludes the data usage for the purposes of third parties given purpose-
restrictive phrases such as "only for internal purposes." Although
many third parties’ purposes can be considered to be outside of
"internal purposes", we exclude only Marketing – Provide ad, which
is the most common, to reduce false positives.
Session 10D: Applied Privacy CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea28295 DATA FLOW EXTRACTION
5.1 Data Purpose Analysis
PurPliance re-implements MobiPurpose approaches [33] to infer
the data types and usage purposes from mobile apps’ network traffic
(Sections 5.3 and 5.4). The data types and purposes of app–server
communication are inferred from the content of the data sent to the
server, the destination and the app description. While the semantics
of data is vague, resource names (e.g., variables or server names) are
assumed to clearly reflect their intentions [33, 63] as it is necessary
for effective software engineering [47] and especially important for
server names shared by multiple parties. The rationale of feature
selection and system design is discussed at length in [33].
Since only the dataset of MobiPurpose [33] is available, we re-
produce its inference and adapt the labels to the purposes in our
purpose taxonomy. We assume the human annotators of MobiPur-
pose dataset correctly labeled the purposes of data, so the high
agreement of ML with human annotators means that ML predicts
the purposes of data with a high probability.
5.2 Data Flow Definition and Extraction
Definition 5.1 (Data Flow). A data flow is a 3-tuple (𝑟, 𝑑, 𝑝) where a
recipient 𝑟 collects a data object 𝑑 for an entity-sensitive purpose 𝑝 and
𝑝 = (𝑒, 𝑞) where 𝑒 is the purpose-served entity and 𝑞 is a data-usage
purpose.
App requests are commonly structured in key–value pairs [67],
so each structured data sent to the server is decomposed into mul-
tiple key–value pairs {𝑘𝑣𝑙}. Therefore, each request or response
between app 𝑎𝑝𝑝𝑖 and end-point 𝑢𝑟𝑙 𝑗 corresponds to a set of low-
level flows 𝐹 = {𝑓𝑘|𝑓𝑘 = (𝑎𝑝𝑝𝑖, 𝑢𝑟𝑙 𝑗 , 𝑘𝑣𝑙)}. The data type 𝑑, purpose-
served entity 𝑒 and usage purpose 𝑞 are then inferred from 𝐹.
PurPliance distinguishes first and third parties to determine
the purpose-served entity 𝑒 by analyzing the receiver 𝑟 and the
inferred data-usage purposes 𝑞. For example, an app’s "supporting"
services like content delivery networks (CDNs) use data for the
app’s first-party purposes rather than for another party’s. While
there are many combinations of 𝑟 (e.g., First-party or Third-party)
and 𝑞 (one of 5 purposes, Table 6), to avoid false positives, we con-
servatively set 𝑒=Advertiser only when 𝑟=Advertiser and 𝑞=Provide
ad or Personalize ad (i.e., an advertiser uses collected data for its
advertising purposes). For other cases, such as when an app uses
"supporting" third-party services (i.e., 𝑟=Third-party and 𝑞=Provide
service), data usage still serves the first-party’s purposes, and hence
we set 𝑒=First-party. The receivers of data flows are resolved by
checking the data’s destination URL with the package name, the pri-
vacy policy URL and well-known analytics/advertisement lists [2].
Note that purpose-served entity 𝑒 is not supported by MobiPurpose.
PurPliance uses dynamic analysis to exercise the apps and
capture their network data traffic. It has 2 advantages over static
analysis: (1) real (not just potential) execution, hence reducing false
positives, and (2) destination of data, which can be determined
dynamically on the server side.
By analyzing purposes of data flows, PurPliance can distin-
guish more fine-grained intentions of data usage than entity-only
approaches like PoliCheck [8]. In particular, the 1st party can collect
data for its own marketing purposes. For example, Wego Flights
Data Type
Identifiers
Geographical location
Device information
Network information
User profile
Average
Precision
0.98
0.98
0.98
1.00
0.89
0.97
Recall
0.92
0.94
0.89
0.92
1.00
0.93
F1
0.95
0.96
0.93
0.96
0.94
0.95
Support
141
67
45
26
16
59
Table 5: Data type extraction performance.
app sends a client ID to its own server at 𝑠𝑟𝑣.𝑤𝑒𝑔𝑜.𝑐𝑜𝑚 with the
request path /𝑎𝑛𝑎𝑙𝑦𝑡𝑖𝑐𝑠/𝑣𝑖𝑠𝑖𝑡𝑠, so the collection of user ID can be in-
ferred to be for the app’s purpose of Marketing Analytics. The sent
data’s semantics is especially useful to distinguish data transfer to
a business partner of the app which is not a popular advertisement
network or analytic service provider.
5.3 Data Type Extraction
Using the corpus from MobiPurpose [33] which contains manually-
annotated data types for key–value pairs of apps’ network traffic, we
identified patterns of the key-values for each data type. The corpus
has 5 high-level data types (listed in Table 5) that are common in
app data communication: identifiers (i.e., hard/software instance
and advertising IDs), network information (e.g., types of network),
device information (e.g., device types and configurations), location
(e.g., GPS coordinates) and user account information (e.g., user
name, password and demographics). MobiPurpose dataset does not
distinguish types of ID (i.e., advertising, hardware and instance ID)
which are frequently used by developers for overlapping purposes.
The distinction can be achieved by finer-grained data type labels.
However, developing such a dataset is beyond this paper’s scope.
Data Type Features. There are 2 types of patterns: special strings
and bag-of-words. The key–value strings are first matched by
special-string patterns which comprise unigrams, bigrams, reg-
ular expressions and bags of words. If no match is found, an English
Word Segmentation model [32] was used to segment the key–value
pairs into separate words and construct a bag of words. For exam-
ple, "sessionid" is separated into session and id. The occurrence of
the word id indicates this is an identifier. These patterns become
6-component feature vectors where each component is whether
there is any matched pattern or not. The last component is set to 1 if
there is no matched pattern for the 5 data types. We tried 4 types of
classifiers (Logistic Regression (LR), Multi-Layer Perceptron (MLP),
Random Forest (RF) and Support Vector Machine (SVM)) to clas-
sify these features. The best-performing classifier is found to be
Random Forest with 200 estimators.
Performance Evaluation. The corpus is randomly divided into
a development set (80%) for developing string patterns and a test
set (20%) for evaluating 5 data-type classifiers. We remove types
with too few (i.e., less than 20) samples. The classifiers achieve 95%
F1 score with 97% precision and 93% recall rates on average. The
precision is more than 89% on all data types. The high accuracy indi-
cates the regularity in key and values which were programmatically
produced by the apps. The lowest recall is of the device information
data type because the classifier misclassifies some samples which
Session 10D: Applied Privacy CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2830Purpose Class
Production - Provide service
Production - Personalize service
Production - Security
Marketing - Provide ad
Marketing - Marketing analytics
Average
Prec.
0.76
0.85
0.81
0.86
0.77
0.81
Rec.
0.81
0.66
0.73
0.86
0.85
0.78
F1
0.78
0.74
0.77
0.86
0.81
0.79
Sup.
15
18
16
76
72
39
Table 6: Purpose prediction performance on the data flows
in the test set. The total number of samples is 1413. The
classifiers are tuned for the extraction precision. The met-
ric columns are Precision/Recall/F1/Support in this order.
look like device IDs, such as clientId: Huawei+Nexus+6P, but are
actually a device model. The detailed performance results of the
classifiers are provided in Table 5.
5.4 Data Traffic Purpose Inference
Data Usage Purpose Features. PurPliance uses the same fea-
tures as those in MobiPurpose to predict the purposes of each
key–value pair in the transferred data. There are 6 features in 3
groups based on the destination URL, sent data and app package
name. The first group of features are based on the usage inten-
tion embedded in the semantics of the destination URL and sent
data which have a form of 𝑠𝑐ℎ𝑒𝑚𝑒 : //ℎ𝑜𝑠𝑡/𝑝𝑎𝑡ℎ. The second fea-
ture group encodes the characteristics of the data types in the
sent data such as the number of key-value pairs. The third fea-
ture group shows the relation between the app and the server. For
example, the data sent to 𝑐𝑏𝑐2015.𝑝𝑟𝑜𝑑1.𝑠ℎ𝑒𝑟𝑝𝑎𝑠𝑒𝑟𝑣.𝑐𝑜𝑚/𝑠𝑒𝑟𝑣𝑖𝑐𝑒𝑠
by app 𝑐𝑜𝑚.𝑠ℎ𝑒𝑟𝑝𝑎.𝑐𝑏𝑐2015 is likely to the app’s server. They are
encoded in 291-dimensional vectors (Table 15 in Appendix E).
Purpose-Classification Dataset. The purpose classification models
were trained on MobiPurpose corpus [33] which contains Android
apps’ network data traffic. We obtained a total of 1413 samples.
The data types and purposes of each key–value pairs contain labels
created by 3 experts. We aggregated purpose labels into a single
purpose label by using majority votes, following the method in the
original paper [33]. Specifically, a sample is classified as a purpose 𝑝
as the most common label from the annotators. The dataset has 24
categories in MobiPurpose taxonomy. We manually mapped them
to 7 classes in PurPliance (as shown in Table 17, Appendix G). The
final 5 purpose classes are listed in Table 6.
Performance Evaluation. Similar to data type classification, we
experimented 4 types of machine learning models: LR, MLP, RF
and MLP. The MLP uses ReLU activation and Adam optimizer
with a fixed learning rate of 10−5. The Random Forest has 200
estimators. We used random search for the hidden layers of 2-layer
MLP, regularization strength (C) of Linear Regression models with
range 0.1–10 estimators (range 100–200) for Random Forest. The
evaluation was done on the dataset using 10-fold cross validation.
Similar to [33], we removed purpose classes that have too few (i.e.,
less than 20) samples, such as the Other purpose class.
The average F1 score is 79% (81% precision and 78% recall). Pro-
vide ad and Marketing analytics have the highest F1 scores of 86%
and 81%, respectively. The lowest F1 score is of the Personalize ser-
vice class since it is challenging to distinguish this class from other
classes such as Provide service. The results are given in Table 6.
We perform an ablation study to evaluate the effectiveness of the
features used to predict the purposes. The results (listed in Table 16
in Appendix E) show that the type of the transferred data is the most
effective feature that improves the F1 score by 4%. The number of
key–value pairs also improves F1 by 2% since there is a correlation
between this feature and the data purposes (e.g., analytic services
often collect more key–value pairs (10+) than the rest [33]).
6 CONSISTENCY ANALYSIS
This section formalizes the detection of purpose inconsistencies
within privacy policies (called policy contradictions) as well as those
between the policies and the actual data collection and sharing be-
havior of the corresponding apps (called flow-policy inconsistencies).
6.1 Semantic Relationships
Each parameter in a privacy statement is mapped to an ontology