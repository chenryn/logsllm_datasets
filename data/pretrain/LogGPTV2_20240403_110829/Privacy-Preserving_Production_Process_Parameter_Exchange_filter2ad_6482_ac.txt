### 5.2 Design Overview

We realize a privacy-preserving exchange platform (refer to Figure 3) by distributing ciphertexts and key material across two independent operators. To ensure privacy, the platform must be constructed with carefully selected operators (see Section 6.1) who do not collude, thereby ensuring G1. In this architecture, both clients and data providers do not need to trust any other entity.

#### I: Data Provision
Data providers retrieve encryption keys \( k_{x'} \) from the key server using oblivious transfer (OT), encrypt their records \( p \), and then offload the encrypted records \( c \) along with their indices \( id_{x'} \) to the storage server. The storage server inserts the indices of received records from all data providers into a single Bloom filter. OTs ensure that the data providers' access patterns are hidden from the key server.

#### II: Matching
Upon request, the client receives the Bloom filter from the storage server. Starting with a known record \( q \), the client locally computes all indices of interest (the candidate set \( S \)) based on a similarity metric \( s \). The client then tests these indices \( id_{q'} \) for membership in the Bloom filter. This local matching process ensures client privacy (G2) as the query content is not shared with any other entity. The Bloom filter shares only a probabilistic data structure of all inserted hashes, not the actual values or full indices.

#### III: Record Retrieval
If the client finds an index that was inserted into the Bloom filter, they retrieve the corresponding decryption key \( k_{x'} \) from the key server via OT. The client also purchases the respective ciphertext from the storage server, which triggers billing (out of scope for this paper). Finally, the client decrypts the ciphertext \( c_{x'} \) to access the desired parameter record. OTs again hide the client's access patterns from the key server.

### 6 Real-World Realization

For our injection molding scenario, we provide an overview of suitable platform operators to highlight its real-world deployability (G3). We then detail the libraries of our fully-tested implementation.

#### 6.1 Exchange Platform Operators

To achieve the claimed privacy guarantees, our design requires non-colluding platform components. Consequently, the key and storage servers must be hosted by different stakeholders. As described in Section 5.3, both servers require different levels of trust. The key server is oblivious to key retrievals, so no trust is required. It can be operated by an untrusted third party, such as startups that charge a small fee for each key retrieval.

When using a trusted third party, key retrieval during data provision and record retrieval could be implemented without OTs. However, as detailed in Section 7, the matching phase is responsible for most of the runtime in real-world settings. Thus, we prefer an OT-based retrieval without any trust assumptions.

The storage server is more critical for both provider and client privacy. It learns the ciphertexts of stored records and the associated data providers, and it is aware of the clients' matches. Public organizations or the government are well-suited for hosting the storage server. For our injection molding scenario (Section 2.1), suitable organizations include the Association of German Engineers (VDI) [85] or the Mechanical Engineering Industry Association (VDMA) [86]. These organizations are already semi-trusted by injection molding businesses and are typically funded through membership fees, making them more appropriate to operate the storage server than an untrusted, potentially unreliable third party.

Using our design, we do not require any trust between all clients and all data providers, facilitating the parameter exchange as each interacts only with the (semi-trusted) storage server. The costs of both entities could be covered by a participation fee paid by all participants or a per-operation fee for each key and record retrieval.

#### 6.2 Implementation

We implemented a client and a data provider application, as well as the exchange platform, in Python 3. For OTs, we use libOTe [64] and select the semi-honest 1-out-of-n OT algorithm KKRT16 [42]. For PSIs (Section 8.2), we rely on libPSI [65] and choose the semi-honest PSI algorithm KKRT16 [42]. We call these libraries using Cython [4].

Figure 4: Sequence chart detailing the messages of BPE

Our implementation does not introduce significant overhead nor require excessive storage. Although data transfers via OTs are computationally expensive and time-consuming [2], a fundamental requirement is to meet the interests of key-retrieving providers and clients, i.e., OTs prevent any information leakage from these entities [29, 60], including the number of transferred keys [18]. Hence, except for non-collusion with the storage server, no trust in the key server is required.

### 7 Evaluation of BPE

As performance (G4) is a critical aspect for real-world deployment (G3), we now evaluate BPE. In Section 7.1, we present our experimental setup for all measurements. Subsequently, we show BPE’s real-world feasibility in four steps. First, in Section 7.2, we investigate the scalability of our integrated building blocks. Then, in Section 7.3, we analyze the performance of our three-step protocol with generated data, before evaluating it using our real-world use case and realistic queries in the domain of injection molding in Section 7.4. Finally, we demonstrate our design’s universality based on a second real-world use case dealing with machine tools in Section 7.5.

#### 7.1 Experimental Setup

For all measurements, we utilized a single server (2x Intel Xeon Silver 4116 and 196 GB RAM) and performed 10 runs each. All entities ran on the same machine and communicated over the loopback interface. We measured the data volume with tcpdump [39] and, if applicable, configured latency and bandwidth with tcconfig [35].

We noticed an unreasonably out-of-scale overhead in the (unsupported) TLS endpoints of libOTe and libPSI, forcing us to add the expected overhead arithmetically. We evaluated the TLS handshake overhead (53.94 ms) and the maximum TLS throughput (567.16 MBit/s) on our evaluation server using Flask’s TLS settings (TLS 1.2, ECDHE-RSA-AES256-GCM-SHA384, and the elliptic curve secp256r1). If not stated otherwise, we included the calculated TLS overhead based on these values (hatched in our plots). The hash key and the encryption keys are 128 bits long each. We only parallelized the Bloom filter-based matching and the OT-based key retrieval.

#### 7.2 Performance of BPE’s Used Building Blocks

Before evaluating the combined BPE design, we first investigate the performance of our building blocks regarding the influence of different parameters to show their applicability in real scenarios.

**Figure 5: A larger capacity and a lower false positive (FP) rate linearly influence the array length of the Bloom filter.**

**Figure 6: Both decreased bandwidth and increased latency negatively influence the linear coefficient when considering the number of OTs and the processing time.**

We examine the influences of capacity and FP rate on the size of Bloom filters, the runtime of OTs given different bandwidth limits and latencies, and the candidate set size \( S \) for different metrics.

##### 7.2.1 Bloom Filter

The matching phase of BPE relies on a Bloom filter to enable checking for specific indices. We evaluate two relevant parameters that affect the size: the capacity, which limits the maximal number of supported indices, and the FP rate, which determines the probability of incorrect membership tests. We chose to evaluate capacities up to 1 billion elements (with a fixed FP rate of \( 10^{-20} \)) as an excessive upper border. For our injection molding scenario, a capacity of 1 million is considered reasonable by domain experts. False positives result in the retrieval of unwanted records, so the FP rate must be as small as possible. We consider values up to \( 10^{-20} \) (with the capacity fixed at 100 million elements) to support billions of membership tests.

Figure 5 shows the influence of these parameters on the size. Due to the base64 encoding, we exceed the calculated theoretical optimum. Even for immense capacities, the size is reasonable due to its linear scaling with the capacity (Figure 5a). One-time transmissions (to clients) of less than 20 GB are realistic [19]. Notably, the size increases linearly for an exponentially decreasing FP rate. Thus, even small FP rates (e.g., \( 10^{-20} \)) yield feasible sizes.

Further, the query and insert times are relevant for the matching and provision phases. We detail in Appendix B.1 that while the query time is mostly unaffected by both capacity and FP rate and depends only on the number of performed queries, the insert times increase approximately linearly with both increased capacity and FP rate. However, data provision is a one-time activity and occurs with a time delay. We fix the capacity at 100 million elements, as our injection molding scenario is unlikely to exceed this value. We further set the FP rate to \( 10^{-20} \), resulting in a comparably small Bloom filter size (<2 GB).

##### 7.2.2 Oblivious Transfers

We rely on OTs for data provision and record retrieval. By considering legitimate businesses bound to a jurisdiction, we can reasonably rely on a semi-honest OT protocol [42], avoiding unnecessary protocol overhead. The runtime is mainly influenced by the set size (total number of keys) and the number of OT extensions (number of retrieved keys). A large set size \( K \) is desirable as more distinct encryption keys can be handled by the key server, i.e., fewer records share their encryption keys. The number of retrieved keys depends (i) on the number of sharable records at the data provider and (ii) the number of matches at the client, which are both highly use case-specific.

As outlined in Appendix B.2, the runtime of the OTs scales linearly with an increased set size and the number of performed OT extensions. We fix the set size at \( 2^{20} \), allowing for more than 1 million distinct keys. In our injection molding use case, each record could be encrypted with an individual key. For this set size, the retrieval of 200 keys takes less than 70 seconds (Appendix B.2).

In Figure 6, we detail the influence of realistic network conditions [5, 19, 91] on OTs in terms of latency and bandwidth. We limit the discussion to practical scenarios.