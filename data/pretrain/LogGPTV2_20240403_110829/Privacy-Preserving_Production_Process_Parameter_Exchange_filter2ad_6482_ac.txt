known initial record 𝑞. To compute 𝑆, a client does not necessarily
require any payload data as records are indexed with their identify-
ing parameters 𝑥𝑖 only. The client eventually retrieves all records 𝑞′
with 𝑖𝑑𝑞′ ∈ 𝑆 that are available (indexed) at the exchange platform.
In the following, we provide a high-level design overview before
focusing on the entities. Subsequently, we detail BPE’s protocol.
5.2 Design Overview
We realize a privacy-preserving exchange platform (cf. Figure 3)
by splitting ciphertexts and key material over two independent
operators. To achieve privacy, our platform must be built up with
carefully selected operators (cf. Section 6.1) who may not collude (to
ensure G1). Apart from this aspect, both clients and data providers
do not have to trust any other entity in our proposed architecture.
I: Data Provision. Data providers retrieve encryption keys 𝑘𝑥′
from the key server via oblivious transfer, encrypt their records 𝑝,
and then offload (cf. G3) their encrypted records 𝑐, annotated with
the indices 𝑖𝑑𝑥′ to the storage server, which inserts the indices of
received records (from all data providers) into a single Bloom filter.
OTs hide the data providers’ access patterns from the key server.
II: Matching. Upon request, the client receives the Bloom filter
from the storage server. Then, starting with a known record 𝑞, the
client locally computes all indices that she is interested in (her
candidate set 𝑆) based on a similarity metric 𝑠 (her intellectual
property) and subsequently tests these indices 𝑖𝑑𝑞′ for membership
in the Bloom filter. The local matching provides client privacy (G2)
as the query content is not shared with another entity. Using a
Bloom filter, the storage server only shares a probabilistic data
structures of all inserted hashes and not the values or full indices.
III: Record Retrieval. If the client found an index that was inserted
into the Bloom filter, she retrieves the corresponding decryption key
𝑘𝑥′ from the key server via OT. She further purchases the respective
ciphertext from the storage server, which also triggers the billing
(out of scope for this paper) for this retrieval. Finally, she decrypts
the ciphertext 𝑐𝑥′ and gets access to the wanted parameter record.
Again, OTs hide the (clients’) access patterns from the key server.
EncryptRecordsOperator B      Key ServerOperator A    Storage ServerCompute MetricOffloadParametersRetrieveBloom FilterRetrieveEnc. KeysData Provider(s)ClientExchange PlatformII: MatchingIII: Record RetrievalI: Data ProvisionUpdateBloom FilterRetrieveDec. KeysGetRecordsGenerateKeys (once)134567DecryptRecords28ACSAC 2020, December 7–11, 2020, Austin, USA
Pennekamp et al.
the Bloom filter from the storage server. The hash key enables the
client to (II.7) derive the indices of candidates, i.e., her candidate
set 𝑆, computed by her metric 𝑠(𝑞) based on input 𝑞 by checking
whether the received Bloom filter contains the respective indices.
III: Record Retrieval. After matches have been determined,
the client (III.8) retrieves the required decryption keys 𝑖𝑑𝑘𝑥′ via
OTs from the key server and (III.9) requests the encrypted records
𝐸𝑘𝑥′ (𝑝) from the storage server using the matching indices 𝑖𝑑𝑥′
which consequently triggers a payment process. Finally, (III.10)
the client decrypts the retrieved ciphertexts to obtain the records.
Afterward, the parameter exchange with this client is concluded.
6 REAL-WORLD REALIZATION
For our injection molding scenario, we give an overview of suitable
platform operators that underline its real-world deployability (G3).
Then, we detail the libraries of our fully-tested implementation.
6.1 Exchange Platform Operators
To realize the claimed privacy guarantees, our design requires non-
colluding platform components. Consequentially, key and storage
server must be hosted by different stakeholders. As described in
Section 5.3, both servers require different levels of trust. The key
server is oblivious of key retrievals. Hence, no trust is required.
Accordingly, it can be operated by an untrusted third party. Here,
startups that charge a small fee for each key retrieval come to mind.
When using a trusted third party, the key retrieval (during data pro-
vision and record retrieval) could be implemented without oblivious
transfers. However, as we detail in Section 7, the matching phase
is responsible for most of the runtime in real-world settings. Thus,
we prefer an OT-based retrieval without any trust assumptions.
The storage server is more critical for both provider and client
privacy. On the one hand, this server learns the ciphertexts of stored
records and the associated data providers. On the other hand, the
storage server is aware of the clients’ matches. Therefore, pub-
lic organizations or the government are well-suited for hosting
the storage server. Concerning our injection molding scenario (cf.
Section 2.1), suitable organization are the Association of German
Engineers (VDI) [85] or the Mechanical Engineering Industry Associ-
ation (VDMA) [86]. These organizations are already semi-trusted by
injection molding businesses and usually funded through member-
ship fees. Hence, they are more appropriate to operate the storage
server than a (random) untrusted, potentially unreliable third party.
Using our design, we do not require any trust between (all) clients
and (all) data providers, facilitating the parameter exchange, as each
of them only interacts with the (semi-trusted) storage server.
The costs of both entities could be covered by a participation fee
paid by all participants of the exchange platform. Another possibil-
ity is a per operation fee, e.g., for each key and record retrieval.
6.2 Implementation
We implemented a client and a data provider application, as well as
the exchange platform in Python 3. For OTs, we utilize libOTe [64]
and select the semi-honest 1-out-of-𝑛 OT algorithm KKRT16 [42].
For PSIs (cf. Section 8.2), we rely on libPSI [65] and choose the semi-
honest PSI algorithm KKRT16 [42]. We call them using Cython [4].
Figure 4: Sequence chart detailing the messages of BPE
significant overhead nor requires excessive storage. Although data
transfers via OTs are known to be computationally expensive and
time-consuming [2], a fundamental requirement is to meet the in-
terests of key-retrieving providers and clients, i.e., OTs prevent
any information leakage from these entities [29, 60], including the
number of transferred keys [18]. Hence, except for non-collusion
with the storage server, no trust in the key server is required.
Storage Server. In terms of computation, the interests of the
storage key server operators are aligned, i.e., low overhead is desir-
able. Since the storage server only maintains the Bloom filter (i.e.,
inserts new records) as well as shares it and requested data with
interested clients, no expensive computations are performed.
However, in our design the storage server operator has a very
sensitive task, i.e., the storage server operator must not collude
with any of the data providers and the key server, and therefore
must be chosen carefully (cf. Section 6.1). While the operator must
observe the indices of offloaded and requested records to enable
billing, this knowledge does not allow for conclusions on any of the
sensitive information, e.g., the client’s similarity metric 𝑠(𝑞). Only
if the storage server operator colludes with the key server operator
or the offloading provider and therefore gains insight into requested
data, conclusions about the client’s candidate set are possible.
Overall, BPE requires no trust between clients and data providers
and relies on the key server and storage server operators to not collude,
i.e., to ensure provider privacy, and the storage server operator to not
collude with data providers to realize client privacy. We propose suit-
able server operators in Section 6.1, and further consider more complex
variants, offering additional security guarantees in Section 8.2.
5.4 Protocol Sequence
Figure 4 outlines all exchanged messages of our three-phased BPE
design in more detail whose meanings we break down now.
I: Data Provision. Initially, (I.1) data providers request a hash
key from the key server to compute all needed indices 𝑖𝑑𝑥′, which
(a) prevents the storage server operator from concluding the stored
data by brute-forcing the indices and (b) increases the variability
of indices allowing to reduce their size. Subsequently, (I.2) the
provider requests key material 𝑖𝑑𝑘𝑥′ from the key server via OT
to (I.3) then encrypt the records. Finally, (I.4) the provider sends
the encrypted records and their indices to the storage server.
II: Matching. In advance of the actual matching process, the
client requests (II.5) the hash key from the key server and (II.6)
ClientKey ServerStorage ServerData Provider(s)I: Data ProvisionII: MatchingIII: Record R.I.1: Hash Key Retrieval (R.)I.2: Key Retrieval (OT)I.3: EncryptionI.4: SendingII.6: Bloom Filter   RetrievalII.5: Hash Key RetrievalIII.8: Key Retrieval (OT)III.9: Record  RetrievalPaymentII.7: MatchingIII.10: DecryptionFor readability, we omit authentication and reg-istrationhere. We includeit in our implementation.Privacy-Preserving Production Process Parameter Exchange
ACSAC 2020, December 7–11, 2020, Austin, USA
Pybloomfiltermmap3 [59] serves as our underlying Bloom filter.
For its transmission, we utilize the library’s base64 export. We
realize both servers as Flask [68] applications with Celery [76] as a
task queue utilizing a Redis [71] broker. Celery workers handle OT
and PSI endpoints as well as Bloom filter updates at the storage.
The server APIs are realized as RESTful JSON APIs that require
HTTP basic authentication using the authorization header field.
All communication between applications and the server APIs are
protected by TLS 1.2 [27]. We decided to calculate the TLS overhead
for OTs and PSIs (cf. Section 7.1 for details). The storage server relies
on SQLite [80] while the key server keeps all keys in memory.
7 EVALUATION OF BPE
As performance (G4) is a critical aspect to realize a real-world
deployable solution (G3), we now evaluate BPE. In Section 7.1, we
present our experimental setup for all measurements. Afterward, we
show BPE’s real-world feasibility in four individual steps. First, in
Section 7.2, we investigate the scalability of our integrated building
blocks. Then, in Section 7.3, keeping the previous results in mind, we
analyze the performance of our three-step protocol with generated
data, before evaluating it using our real-world use case and realistic
queries in the domain of injection molding in Section 7.4. Finally, we
demonstrate our design’s universality based on a second real-world
use case dealing with machine tools in Section 7.5.
7.1 Experimental Setup
For all measurements, we utilized a single server (2x Intel XeonSil-
ver 4116 and 196 GB RAM) and performed 10 runs each. All entities
ran on the same machine and communicated over the loopback
interface. We measured the data volume with tcpdump [39] and, if
applicable, configured latency and bandwidth with tcconfig [35].
We noticed an unreasonably out-of-scale overhead in the (un-
supported) TLS endpoints of libOTe and libPSI forcing us to add the
expected overhead arithmetically. To this end, we evaluated the TLS
handshake overhead (53.94 ms) and the maximum TLS throughput
(567.16 MBit/s) on our evaluation server using Flask’s TLS settings
(TLS 1.2, ECDHE-RSA-AES256-GCM-SHA384, and the elliptic curve
secp256r1). If not stated otherwise, we included the calculated TLS
overhead based on these values (hatched in our plots). The hash key
and the encryption keys are 128 Bit long each. We only parallelized
the Bloom filter-based matching and the OT-based key retrieval.
7.2 Performance of BPE’s Used Building Blocks
Before evaluating the (combined) BPE design, we first investigate
the performance of our building blocks regarding the influence of
different parameters to show their applicability in real scenarios.
(a) Even huge capacities lead to
reasonable file sizes.
Figure 5: A larger capacity and a lower false positive (FP) rate
linearly influence the array length of the Bloom filter.
(b) The FP rate only has a linear
influence on the file size.
(b) Latency impacts the commu-
(a) A reduced bandwidth affects
the large transmissions of OTs.
nication overhead of costly OTs.
Figure 6: Both decreased bandwidth and increased latency
negatively influence the linear coefficient when considering
the number of OTs and the processing time.
We thus examine the influences on capacity and FP rate on the size
of Bloom filters, the runtime of OTs given different bandwidth limits
and latencies, and the candidate set size 𝑆 for different metrics.
7.2.1 Bloom Filter. The matching phase of BPE relies on a Bloom
filter to enable the checking for specific indices. Thus, we evaluate
two parameters of relevance, which both affect the size, i.e., increase
the amount of data that must be transferred to clients. First, the
capacity limits the maximal number of supported indices. Second,
the FP rate determines the probability of incorrect membership
tests, i.e., an index is not available even though the test is positive.
We chose to evaluate capacities up to 1 Bil. elements (with a
fixed FP rate of 10−20) as an excessive upper border. For our injec-
tion molding scenario, a capacity of 1 Mio. is considered reasonable
by domain experts. As false positives result in the retrieval of un-
wanted records, the FP rate must be configured as small as possible.
Accordingly, we consider values up to 10−20 (with the capacity fixed
at 100 Mio. elements) to support billions of membership tests.
Figure 5 shows the influence of these parameters on the size. Due
to the used base64 encoding, we exceed the calculated theoretic
optimum. Even for immense capacities, the size is reasonable due
to its linear scaling with the capacity (cf. Figure 5a). Nowadays, one-
time transmissions (to clients) of less than 20 GB are realistic [19].
Notably, the size increases linearly for an exponentially decreasing
FP rate. Thus, even small FP rates (e.g., 10−20) yield feasible sizes.
Further, the query and insert times are relevant for the matching
as well as the provision phase. We detail in Appendix B.1 that while
the performance of the query time is mostly unaffected by both
capacity and FP rate, and only depends on the number of performed
queries, the insert times increase approximately linear with both
increased capacity and FP rate. However, the data provision is a one-
time activity and occurs with time delay. In the following, we fix
the capacity at 100 Mio. elements as our injection molding scenario
is unlikely to exceed this value. We further set the FP rate to 10−20,
which results in a comparably small Bloom filter size (<2 GB).
7.2.2 Oblivious Transfers. We rely on OTs for the data provision
and record retrieval. By considering legitimate businesses, which
are bound to a jurisdiction, we can reasonably rely on a semi-honest
OT protocol [42], and thereby avoid unnecessary protocol overhead.
The runtime is mainly influenced by the set size (total number
of keys) and the number of OT extensions (number of retrieved
keys). A large set size 𝐾 is desirable as more distinct encryption
keys can be handled by the key server, i.e., fewer records share their
encryption keys. The number of retrieved keys depends (i) on the
number of sharable records at the data provider and (ii) the number
of matches at the client, which are both highly use case-specific.
00.2 Bil0.4 Bil0.6 Bil0.8 Bil1.0 BilCapacity [#]048121620Size [GB]MeasuredTheoretic101103106109101210151018FP Rate0.00.51.01.5Size [GB]MeasuredTheoretic020406080100Number of OT Extensions [#]05101520Time [min]    6MBit/s  50MBit/s100MBit/sUnlimited020406080100Number of OT Extensions [#]02468Time [min]300ms250ms200ms150ms100ms  50ms    0msACSAC 2020, December 7–11, 2020, Austin, USA
Pennekamp et al.
Figure 7: Obliviously obtaining the encryption keys is the
most time-consuming step when offloading data records.
As we outline in Appendix B.2, the runtime of the OTs scales both
linearly with an increased set size and the number of performed
OT extension. We fix the set size at 220, which allows for more than
1 Mio. distinct keys. Thus, in our injection molding use case, each
record could be encrypted with an individual key. For this set size,
the retrieval of 200 keys takes less than 70 s (cf. Appendix B.2).
In Figure 6, we detail the influence of realistic network condi-
tions [5, 19, 91] on OTs in terms of latency and bandwidth. We limit