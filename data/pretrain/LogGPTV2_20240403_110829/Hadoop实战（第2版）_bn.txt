（2）显示分区
SHOW PARTITIONS table_name
SHOW PARTITIONS列出了给定基表中的所有现有分区，分区按字母顺序排列。
（3）显示表/分区扩展
SHOW TABLE EXTENDED[IN|FROM database_name]LIKE identifier_with_wildcards
[PARTITION（partition_desc）]
SHOW TABLE EXTENDED为列出所有给定的匹配正规表达式的表信息。如果分区规范存在，那么用户不能使用正规表达式作为表名。该命令的输出包括基本表信息和文件系统信息，例如，文件总数、文件总大小、最大文件大小、最小文件大小、最新存储时间和最新更新时间。如果分区存在，则它会输出给定分区的文件系统信息，而不是表中的文件系统信息。
作为视图，SHOW TABLE EXTENDED用于检索视图的定义。
（4）显示函数
SHOW FUNCTIONS"a.*"
SHOW FUNCTIONS为列出用户定义和建立所有匹配给定正规表达式的函数。可以为所有函数提供".*"。
（5）描述表/列
DESCRIBE[EXTENDED]table_name[DOT col_name]
DESCRIBE[EXTENDED]table_name[DOT col_name（[DOT field_name]|[DOT'$elem$']|
[DOT'$key$']|[DOT'$value$']）*]
DESCRIBE TABLE为显示列信息，包括给定表的分区。如果指定EXTENDED关键字，则将在序列化形式中显示表的所有元数据。DESCRIBE TABLE通常只用于调试，而不用在平常的使用中。
如果表有复杂的列，可以通过指定数组元素table_name.complex_col_name（和'$elem$'作为数组元素，'$key$'为图的主键，'$value$'为图的属性）来检查该列的属性。对于复杂的列类型，可以使用这些定义进行递归查询。
（6）描述分区
DESCRIBE[EXTENDED]table_name partition_spec
该语句列出了给定分区的元数据，其输出和DESCRIBE TABLE类似。目前，在查询计划准备阶段不能使用这些列信息。
[1]
 http：//www.grouplens.org/node/73。
11.3.2 数据操作（DML）
下面我们将详细介绍DML，它是数据操作类语言，其中包括向数据表加载文件、写查询结果等操作。
1.向数据表中加载文件
当数据被加载至表中时，不会对数据进行任何转换。Load操作只是将数据复制/移动至Hive表对应的位置，代码如下：
LOAD DATA[LOCAL]INPATH'filepath'[OVERWRITE]
INTO TABLE tablename
[PARTITION（partcol1=val1，partcol2=val2……）]
其中，filepath可以是相对路径（例如，project/data1），可以是绝对路径（例如，/user/admin/project/data1），也可以是完整的URI（例如，hdfs：//NameNodeIP：9000/user/admin/project/data1）。加载的目标可以是一个表或分区。如果表包含分区，则必须指定每个分区的分区名。filepath可以引用一个文件（在这种情况下，Hive会将文件移动到表所对应的目录中）或一个目录（在这种情况下，Hive会将目录中的所有文件移动至表所对应的目录中）。如果指定LOCAL，那么load命令会去查找本地文件系统中的filepath。如果发现是相对路径，则路径会被解释为相对于当前用户的当前路径。用户也可以为本地文件指定一个完整的URI，比如file：///user/hive/project/data。此时load命令会将filepath中的文件复制到目标文件系统中，目标文件系统由表的位置属性决定，被复制的数据文件移动到表的数据对应的位置。如果没有指定LOCAL关键字，filepath指向一个完整的URI，那么Hive会直接使用这个URI。如果没有指定schema或authority，则Hive会使用在Hadoop配置文件中定义的schema和authority, fs.default.name属性指定NameNode的URI。如果路径不是绝对的，那么Hive会相对于/user/进行解释。Hive还会将filepath中指定的文件内容移动到table（或partition）所指定的路径中。如果使用OVERWRITE关键字，那么目标表（或分区）中的内容（如果有）会被删除，并且将filepath指向的文件/目录中的内容添加到表/分区中。如果目标表（或分区）中已经有文件，并且文件名和filepath中的文件名冲突，那么现有的文件会被新文件所替代。
2.将查询结果插入Hive表中
查询的结果通过insert语法加入到表中，代码如下：
INSERT OVERWRITE TABLE tablename1[PARTITION（partcol1=val1，partcol2=val2……）]
select_statement1 FROM from_statement
Hive extension（multiple inserts）：
FROM from_statement
INSERT OVERWRITE TABLE tablename1[PARTITION（partcol1=val1，partcol2=val2……）]
select_statement1
[INSERT OVERWRITE TABLE tablename2[PARTITION……]select_statement2]……
Hive extension（dynamic partition inserts）：
INSERT OVERWRITE TABLE tablename PARTITION（partcol1[=val1]，partcol2[=val2]……）
select_statement FROM from_statement
这里需要注意的是，插入可以针对一个表或一个分区进行操作。如果对一个表进行了划分，那么在插入时就要指定划分列的属性值以确定分区。每个Select语句的结果会被写入选择的表或分区中，OVERWRITE关键字会强制将输出结果写入。其中输出格式和序列化方式由表的元数据决定。在Hive中进行多表插入，可以减少数据扫描的次数，因为Hive可以只扫描输入数据一次，而对输入数据进行多个操作命令。
3.将查询的结果写入文件系统
查询结果可以通过如下命令插入文件系统目录：
INSERT OVERWRITE[LOCAL]DIRECTORY directory1 SELECT……FROM……
Hive extension（multiple inserts）：
FROM from_statement
INSERT OVERWRITE[LOCAL]DIRECTORY directory1 select_statement1
[INSERT OVERWRITE[LOCAL]DIRECTORY directory2 select_statement2]……
这里需要注意的是，目录可以是完整的URI。如果scheme或authority没有定义，那么Hive会使用Hadoop的配置参数fs.default.name中的scheme和authority来定义NameNode的URI。如果使用LOCAL关键字，那么Hive会将数据写入本地文件系统中。
在将数据写入文件系统时会进行文本序列化，并且每列用^A区分，换行表示一行数据结束。如果任何一列不是原始类型，那么这些列将会被序列化为JSON格式。
11.3.3 SQL操作
下面是一个标准的Select语句语法定义：
SELECT[ALL|DISTINCT]select_expr, select_expr，……
FROM table_reference
[WHERE where_condition]
[GROUP BY col_list]
[CLUSTER BY col_list
|[DISTRIBUTE BY col_list][SORT BY col_list]
]
[LIMIT number]
下面对其中重要的定义进行说明。
（1）table_reference
table_reference指明查询的输入，它可以是一个表、一个视图或一个子查询。下面是一个简单的查询，检索所有表t1中的列和行：
SELECT*FROM t1
（2）WHERE
where_condition是一个布尔表达式。比如下面的查询只输出sales表中amount＞10且region属性值为US的记录：
SELECT*FROM sales WHERE amount＞10 AND region="US"
（3）ALL和DISTINCT
ALL和DISTINCT选项可以定义重复的行是否要返回。如果没有定义，那么默认为ALL，即输出所有的匹配记录而不删除重复的记录，代码如下：
hive＞SELECT col1，col2 FROM t1
1 3
1 3
1 4
2 5
hive＞SELECT DISTINCT col1，col2 FROM t1
1 3
1 4
2 5
hive＞SELECT DISTINCT col1 FROM t1
1
2
（4）LIMIT
LIMIT可以控制输出的记录数，随机选取检索结果中的相应数目输出：
SELECT*FROM t1 LIMIT 5
下面代码为输出Top-k, k=5的查询结果：
SET mapred.reduce.tasks=1
SELECT*FROM sales SORT BY amount DESC LIMIT 5
（5）使用正则表达式
SELECT声明可以匹配使用一个正则表达式的列。下面的例子会对sales表中除了ds和hr的所有列进行扫描：
SELECT（ds|hr）?+.+FROM sales
（6）基于分区的查询
通常来说，SELECT查询要扫描全部的表。如果一个表是使用PARTITIONED BY语句产生的，那么查询可以对输入进行“剪枝”，只对表的相关部分进行扫描。Hive现在只对在WHERE中指定的分区断言进行“剪枝”式的扫描。举例来说，如果一个表page_view按照date列的值进行了分区，那么下面的查询可以检索出日期为2010-03-01的行记录：
SELECT page_views.*
FROM page_views
WHERE page_views.date＞='2010-03-01'AND page_views.date＜='2010-03-31'
（7）HAVING
Hive目前不支持HAVING语义，但是可以使用子查询实现，示例如下：
SELECT col1 FROM t1 GROUP BY col1 HAVING SUM（col2）＞10
可以表示为：
SELECT col1 FROM（SELECT col1，SUM（col2）AS col2sum FROM t1 GROUP BY col1）t2
WHERE t2.col2sum＞10
我们可以将查询的结果写入到目录中：
hive＞INSERT OVERWRITE DIRECTORY'/tmp/hdfs_out'SELECT a.*FROM invites a WHERE
a.ds='2009-09-01'；
上面的例子将查询结果写入/tmp/hdfs_out目录中。也可以将查询结果写入本地文件路径，如下所示：
hive＞INSERT OVERWRITE LOCAL DIRECTORY'/tmp/local_out'SELECT a.*FROM pokes a；
其他（例如GROUP BY和JOIN）的作用和SQL相同，就不再赘述，下面是使用的例子，详细信息可以查看http：//wiki.apache.org/hadoop/Hive/LanguageManual。
（8）GROUP BY
hive＞FROM invites a INSERT OVERWRITE TABLE events SELECT a.bar, count（*）WHERE
a.foo＞0 GROUP BY a.bar；
hive＞INSERT OVERWRITE TABLE events SELECT a.bar, count（*）FROM invites a WHERE
a.foo＞0 GROUP BY a.bar；
（9）JOIN
hive＞FROM pokes t1 JOIN invites t2 ON（t1.bar=t2.bar）INSERT OVERWRITE TABLE
events SELECT t1.bar, t1.foo, t2.foo；
（10）多表INSERT
FROM src
INSERT OVERWRITE TABLE dest1 SELECT src.*WHERE src.key＜100
INSERT OVERWRITE TABLE dest2 SELECT src.key, src.value WHERE src.key＞=100 and
src.key＜200
INSERT OVERWRITE TABLE dest3 PARTITION（ds='2010-04-08'，hr='12'）SELECT src.key
WHERE src.key＞=200 and src.key＜300
INSERT OVERWRITE LOCAL DIRECTORY'/tmp/dest4.out'SELECT src.value WHERE src.key
＞=300；
（11）STREAMING
hive＞FROM invites a INSERT OVERWRITE TABLE events SELECT TRANSFORM（a.foo, a.bar）
AS（oof, rab）USING'/bin/cat'WHERE a.ds＞'2010-08-09'；
这个命令会将数据输入给Map操作（通过/bin/cat命令），同样也可以将数据流式输入给Reduce操作。
11.3.4 Hive QL使用实例
下面我们通过两个例子对Hive QL的使用方法进行介绍，从中可以看到它与传统SQL语句的异同点。
1.电影评分
首先创建表，并且使用tab空格定义文本格式：
CREATE TABLE u_data（
userid INT，
movieid INT，
rating INT，
unixtime STRING）
ROW FORMAT DELIMITED
FIELDS TERMINATED BY'\t'
STORED AS TEXTFILE；
然后下载数据文本文件并解压，代码如下：
wget http：//www.grouplens.org/system/files/ml-data.tar__0.gz