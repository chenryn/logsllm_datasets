timization problems tailored to trafﬁc analysis applications.
Our blind adversarial perturbations algorithm is generic and
can be applied on various types of trafﬁc classiﬁers with dif-
ferent network constraints.
We evaluated our attack against state-of-the-art trafﬁc anal-
ysis systems, showing that our attack outperforms traditional
techniques in defeating trafﬁc analysis. We also showed that
our blind adversarial perturbations are even transferable be-
tween different models and architectures, so they can be ap-
plied by blackbox adversaries. Finally, we showed that ex-
isting defenses against adversarial examples perform poorly
against blind adversarial perturbations, therefore we designed
a tailored countermeasure against blind perturbations.
Acknowledgements
We thank our shepherd Esfandiar Mohammadi and anony-
mous reviewers for their feedback. The work was supported
by the NSF CAREER grant CNS-1553301, and by DARPA
and NIWC under contract N66001-15-C-4067. The U.S. Gov-
ernment is authorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copyright nota-
tion thereon. The views, opinions, and/or ﬁndings expressed
are those of the author(s) and should not be interpreted as rep-
resenting the ofﬁcial views or policies of the Department of
Defense or the U.S. Government. Milad Nasr was supported
by a Google PhD Fellowship in Security and Privacy.
References
[1] S. Abdoli, L. Hafemann, J. Rony, I. Ayed, P. Cardinal,
and A. Koerich. Universal Adversarial Audio Perturba-
tions. arXiv preprint arXiv:1908.03173, 2019.
[3] S. Bhat, D. Lu, A. Kwon, and S. Devadas. Var-CNN and
DynaFlow: Improved Attacks and Defenses for Website
Fingerprinting. CoRR, 2018.
[4] Avrim Blum, Dawn Song, and Shobha Venkataraman.
Detection of interactive stepping stones: Algorithms and
conﬁdence bounds. In RAID, 2004.
[5] X. Cai, R. Nithyanand, and R. Johnson. Cs-buﬂo: A
congestion sensitive website ﬁngerprinting defense. In
WPES, 2014.
[6] X. Cai, X. Zhang, B. Joshi, and R. Johnson. Touch-
ing from a distance: Website ﬁngerprinting attacks and
defenses. In ACM CCS, 2012.
[7] X. Cao and N. Gong. Mitigating evasion attacks to
deep neural networks via region-based classiﬁcation. In
ACSAC, 2017.
[8] N. Carlini and D. Wagner. Adversarial examples are not
easily detected: Bypassing ten detection methods. In
ACM Workshop on AISec, 2017.
[9] N. Carlini and D. Wagner. Towards evaluating the ro-
bustness of neural networks. In IEEE S&P, 2017.
[10] P. Chen, Y. Sharma, H. Zhang, J. Yi, and C. Hsieh. EAD:
Elastic-Net Attacks to Deep Neural Networks via Ad-
versarial Examples. In AAAI, 2017.
[11] G. Cherubin, J. Hayes, and M. Juarez. Website ﬁnger-
In PETS,
printing defenses at the application layer.
2017.
[12] T. Chothia and A. Guha. A statistical test for information
In CSF,
leaks using continuous mutual information.
2011.
[13] R. Dingledine
and N. Mathewson.
a
Blocking-Resistant Anonymity
of
https://svn.torproject.org/svn/projects/
design-paper/blocking.html.
Design
System.
[14] Y. Dong, F. Liao, T. Pang, H. Su, J. Zhu, X. Hu, and J. Li.
Boosting adversarial attacks with momentum. In CVPR,
2018.
[15] K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati,
C. Xiao, A. Prakash, T. Kohno, and D. Song. Robust
physical-world attacks on deep learning visual classiﬁ-
cation. In CVPR, 2018.
[16] I. Goodfellow, Y. Bengio, and A. Courville. Deep learn-
ing. MIT press Cambridge, 2016.
[2] A. Bahramali, R. Soltani, A. Houmansadr, D. Goeckel,
and D. Towsley. Practical Trafﬁc Analysis Attacks on
Secure Messaging Applications. In NDSS, 2020.
[17] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Ben-
gio. Generative Adversarial Nets. In NIPS. 2014.
2720    30th USENIX Security Symposium
USENIX Association
[18] I. Goodfellow, J. Shlens, and C. Szegedy. Explaining
and Harnessing Adversarial Examples. In ICLR, 2015.
[19] J. Hayes and G. Danezis. k-ﬁngerprinting: A robust
scalable website ﬁngerprinting technique. In USENIX
Security, 2016.
[20] J. Hayes and G. Danezis. Learning Universal Adversar-
ial Perturbations with Generative Models. In IEEE S&P
Workshops, 2018.
[21] K. He, X. Zhang, S. Ren, and J. Sun. Deep Residual
Learning for Image Recognition. In CVPR, 2016.
[22] W. He, B. Li, and D. Song. Decision Boundary Analysis
of Adversarial Examples. In ICLR, 2018.
[34] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and
A. Vladu. Towards deep learning models resistant to
adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
[35] S. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and
In
P. Frossard. Universal adversarial perturbations.
CVPR, 2017.
[36] S. Moosavi-Dezfooli, A. Fawzi, and P. Frossard. Deep-
Fool: A Simple and Accurate Method to Fool Deep
Neural Networks. In CVPR, 2016.
[37] M. Nasr, A. Bahramali, and A. Houmansadr. Deepcorr:
strong ﬂow correlation attacks on tor using deep learn-
ing. In ACM CCS, 2018.
[23] A. Houmansadr, N. Kiyavash, and N. Borisov. RAIN-
BOW: A Robust And Invisible Non-Blind Watermark
for Network Flows. In NDSS, 2009.
[38] M. Nasr, A. Houmansadr, and A. Mazumdar. Compres-
sive Trafﬁc Analysis: A New Paradigm for Scalable
Trafﬁc Analysis. In ACM CCS, 2017.
[24] A. Houmansadr, N. Kiyavash, and N. Borisov. Non-
blind watermarking of network ﬂows. IEEE/ACM TON,
2014.
[25] Amir Houmansadr and Nikita Borisov. SWIRL: A Scal-
able Watermark to Detect Correlated Network Flows. In
NDSS, 2011.
[26] M. Imani, M. Rahman, N. Mathews, A. Joshi, and
M. Wright. Mockingbird: Defending Against Deep-
Learning-Based Website Fingerprinting Attacks with
Adversarial Traces. CoRR, 2019.
[27] R. Jansen, M. Juarez, R. Galvez, T. Elahi, and C. Diaz.
Inside Job: Applying Trafﬁc Analysis to Measure Tor
from Within. In NDSS, 2018.
[28] M. Juarez, M. Imani, M. Perry, C. Diaz, and M. Wright.
Toward an efﬁcient website ﬁngerprinting defense. In
ESORICS, 2016.
[29] D. Kingma and J. Ba. Adam: A Method for Stochastic
Optimization. ICLR, 2014.
[30] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet
classiﬁcation with deep convolutional neural networks.
In NIPS, 2012.
[31] A. Kurakin, I. Goodfellow, and S. Bengio. Adversar-
ial examples in the physical world. arXiv preprint
arXiv:1607.02533, 2016.
[32] A. Kurakin, I. Goodfellow, and S. Bengio. Adver-
arXiv preprint
sarial machine learning at scale.
arXiv:1611.01236, 2016.
[33] B. Levine, M. Reiter, C. Wang, and M. Wright. Timing
attacks in low-latency mix systems. In FC, 2004.
[39] A Simple Obfuscating Proxy.
https://www.
torproject.org/projects/obfsproxy.html.en.
[40] A. Panchenko, F. Lanze, J. Pennekamp, T. Engel, A. Zin-
nen, M. Henze, and K. Wehrle. Website Fingerprinting
at Internet Scale. In NDSS, 2016.
[41] A. Panchenko, L. Niessen, A. Zinnen, and T. En-
gel. Website ﬁngerprinting in onion routing based
anonymization networks. In WPES, 2011.
[42] N. Papernot, P. McDaniel, and I. Goodfellow. Trans-
ferability in Machine Learning: from Phenomena to
Black-Box Attacks using Adversarial Samples. arXiv
preprint arXiv:1605.07277, 2016.
[43] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami.
Distillation as a defense to adversarial perturbations
against deep neural networks. In IEEE S&P, 2016.
[44] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang,
Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and
A. Lerer. Automatic Differentiation in PyTorch.
In
NIPS Autodiff Workshop, 2017.
[45] F. Pierazzi, F. Pendlebury, J. Cortellazzi, and L. Caval-
laro. Intriguing properties of adversarial ML attacks in
the problem space. In IEEE S&P, 2020.
[46] Tor: Pluggable transports. https://www.torproject.
org/docs/pluggable-transports.html.en.
[47] V. Rimmer, D. Preuveneers, M. Juarez, T. Van, and
W. Joosen. Automated website ﬁngerprinting through
deep learning. In NDSS, 2018.
[48] A. Ross and F. Doshi-Velez. Improving the adversarial
robustness and interpretability of deep neural networks
by regularizing their input gradients. In AAAI, 2018.
USENIX Association
30th USENIX Security Symposium    2721
[49] V. Shmatikov and M. Wang. Timing analysis in low-
In ES-
latency mix networks: Attacks and defenses.
ORICS, 2006.
[50] P. Sirinam, M. Imani, M. Juarez, and M. Wright. Deep
ﬁngerprinting: Undermining website ﬁngerprinting de-
fenses with deep learning. In ACM CCS, 2018.
[51] P. Sirinam, N. Mathews, M. Rahman, and M. Wright.
Triplet Fingerprinting: More Practical and Portable Web-
site Fingerprinting with N-shot Learning. In ACM CCS,
2019.
[52] J. Su, D. Vargas, and K. Sakurai. One Pixel Attack for
Fooling Deep Neural Networks. IEEE TEVC, 2017.
[53] Y. Sun, A. Edmundson, L. Vanbever, O. Li, J. Rexford,
M. Chiang, and P. Mittal. RAPTOR: routing attacks on
privacy in tor. In USENIX Security, 2015.
[54] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Er-
han, I. Goodfellow, and R. Fergus. Intriguing properties
of neural networks. arXiv preprint arXiv:1312.6199,
2013.
[55] F. Tramèr, A. Kurakin, N. Papernot, I. Goodfellow,
Ensemble adversar-
arXiv preprint
D. Boneh, and P. McDaniel.
ial training: Attacks and defenses.
arXiv:1705.07204, 2017.
[56] V. Vapnik. The nature of statistical learning theory.
Springer science & business media, 2013.
[57] T. Wang. High Precision Open-World Website Finger-
printing. In IEEE S&P, 2020.
[58] T. Wang, X. Cai, R. Nithyanand, R. Johnson, and I. Gold-
berg. Effective Attacks and Provable Defenses for Web-
site Fingerprinting. In USENIX Security, 2014.
[59] T. Wang and I. Goldberg. Improved website ﬁngerprint-
ing on tor. In WPES, 2013.
[60] T. Wang and I. Goldberg. On realistically attacking tor
with website ﬁngerprinting. PETS, 2016.
[61] T. Wang and I. Goldberg. Walkie-talkie: An efﬁcient
defense against passive website ﬁngerprinting attacks.
In USENIX Security, 2017.
[62] X. Zhang, J. Hamm, M. K Reiter, and Y. Zhang. Statis-
tical privacy for streaming trafﬁc. In NDSS, 2019.
[63] Y. Zhang and V. Paxson. Detecting Stepping Stones. In
USENIX Security, 2000.
[64] Y. Zhu, X. Fu, B. Graham, R. Bettati, and W. Zhao. On
ﬂow correlation attacks and countermeasures in mix
networks. In WPES, 2004.
A Adapting Traditional Defenses to Adversar-
ial Examples
Madry et al. [34] presented a scalable adversarial training
approach to increase the robustness of deep learning mod-
els to adversarial examples. In each iteration of training, this
method generates a set of adversarial examples and uses them
in the training phase. Madry et al.’s defense is the most robust
defense among the adversarial training based defenses [8]. We
cannot use this method as is, since in the image recognition
applications, pixels can take real values, while in direction-
based trafﬁc analysis methods, features take only two values
(-1, +1). Therefore, we modify this defense to our setting. To
generate a set of adversarial examples in the training process,
we randomly choose a number of packets and ﬂip their direc-
tions from -1 to +1 and vice versa. Similarly, for the packet
timings and sizes we enforced all of application constraints
for generating the adversarial examples.
From the gradient mask approach, we used the input gra-
dient regularization (IGR) technique of Ross and Doshi-
Velez [48]. IGR is more robust against adversarial attacks
compared to its previous work [43]. Their defense trains a
model to have smooth input gradients with fewer extreme
values which becomes more resistant to adversarial examples.
We utilize this approach to train a robust model using DF
structure. We evaluated the direction-based attack against this
defense with parameter λ = 10.
While the previous defenses train a robust model against
adversarial attacks, Cao and Gong [7] designed a defense
method which does not change the training process. They
proposed a region-based classiﬁcation (RC) method which
creates a hypercube centered at the input to predict its la-
bel. Then, the method samples a set of data points from the
crafted hypercube and uses an existing trained model to pro-
duce predicted label for each sampled data point; Finally, it
uses majority voting to generate the ﬁnal class label for the
given input. We need to make changes to the region-based
classiﬁcation defense. In contrast to images, we cannot create
a hypercube centered at the input by just adding random real
values to the packet direction sequences which have values
-1, 1. Instead, for each input, we create the hypercube by ran-
domly choosing a number of packets in the sequence and
ﬂipping their directions. To apply region-based classiﬁcation
in the test phase of the direction-based method while adding
blind perturbations, we randomly choose 125 packets and
change their directions to form the hypercube. Similar to Cao
and Gong, we call this number as the radius of the hypercube.
We choose this value for the radius because 125 is the max-
imum number of packets we can use to form the hypercube
while the accuracy of the region-based method does not go
below the accuracy of the original DF model. Using radius of
125 for hypercubes, we apply the region-based classiﬁcation
against our attack. For time and size based methods, we use
the strength of the adversary to generate the hypercubes.
2722    30th USENIX Security Symposium
USENIX Association