title:Analysis of security data from a large computing organization
author:Aashish Sharma and
Zbigniew Kalbarczyk and
James Barlow and
Ravishankar K. Iyer
Analysis of Security Data from a Large Computing Organization 
A. Sharma, Z. Kalbarczyk, J. Barlow, and R. Iyer 
University of Illinois at Urbana-Champaign 
1308. Main St. Urbana, IL 61801, USA 
 PI:EMAIL, {kalbarcz, jbarlow, rkiyer}@illinois.edu 
Abstract  —  This  paper  presents  an  in-depth  study  of  the 
forensic  data  on  security  incidents  that  have  occurred  over  a 
period  of  5  years  at  the  National  Center  for  Supercomputing 
Applications  at  the  University  of  Illinois.  The  proposed 
methodology  combines  automated  analysis  of  data  from 
security  monitors  and  system  logs  with  human  expertise  to 
extract and process relevant data in order to: (i) determine the 
progression of an attack, (ii) establish incident categories and 
characterize their severity, (iii) associate alerts with incidents, 
and (iv) identify incidents missed by the monitoring tools and 
examine  the  reasons  for  the  escapes.  The  analysis  conducted 
provides  the  basis  for  incident  modeling  and  design  of  new 
techniques for security monitoring. 
Keywords - incident/attack data analysis, security monitoring, 
alerts, large scale computing systems. 
I. 
INTRODUCTION  
include 
In  this  paper,  we  analyze  security  incidents  that  have 
occurred  over  a  period  of  5  years  across  5000  machines 
monitored  at  the  National  Center  for  Supercomputing 
Applications  (NCSA)  at  the  University  of  Illinois.  The 
monitored  systems 
i)  eight  high-performance 
computational  clusters  (each  consisting  of  2k  –  12k 
processors) participating in grid computing and accessed by 
users  world-wide; 
iii) 
production  infrastructure  systems,  e.g.,  mail,  web,  domain 
name, and authentication servers, certification authority; iv) 
file servers (which include NFS, AFS, GPFS, and Luster file 
systems); and v) over 1000 desktops and laptops located in a 
class  B  (/16)  network.  The  target  system  has  naturally 
continued to evolve over the years, and hence the analysis is 
an aggregate over the measurement period. The usage of the 
network studied is not different from any other network with 
heterogeneous set of computing systems.   
research  clusters; 
ii)  smaller 
To illustrate the complexity of the target system, Fig. 1 
shows  a  section  of  a  five-minute  snapshot  of  traffic  for 
systems within NCSA. The light-colored ovals represent the 
IP  addresses,  and 
to  network 
connections.  Fig.  2  shows  the  connections  of  a  system 
involved in a security incident. The red ovals here represent 
the IP addresses that were determined to be malicious1. The 
example shows that it is not trivial to navigate in this sea of 
traffic  and  data  to  identify  compromised  hosts  and  the 
information relevant to a given attack. 
lines  correspond 
the 
the 
incidents, 
identifying  missed 
Our goal is to understand how attacks progress as seen by 
the  monitoring  tools  and  traces  recorded  in  the  data  logs. 
Reconstructing  the  attack  steps  –  from  an  alert  to  a 
compromise  –  provides  the  foundation  for  categorizing  the 
security 
incidents,  and 
characterizing  the  incident  severity.  While  analysis  of  real 
security data is of value in and of itself, the actual studies are 
rare  (see  Section  III).  This  is  due  to  (i)  privacy  issues  in 
accessing 
in 
comprehending and correlating large quantities (terabytes) of 
multimodal data from various logs (often with different data 
formats) and alerting tools.  
security  data  and 
(ii)  difficulty 
this  study  (i) 
This paper makes an important step in overcoming these 
introduces  and 
challenges.  Specifically, 
illustrates  a  methodology 
(combination  of  automated 
analysis  and  human  intervention  with  potential  for  further 
automation)  to  extract  and  process  relevant  data  from 
different  logs  in  order  to  identify  the  progression  of  an 
attack, (ii) associates alerts with incidents, and (iii) identifies 
incidents  missed  by  the  monitoring  tools  and  analyzes  the 
reasons for the escapes.  
The key findings of this work are as follows: 
While over half (57%) of incidents are detected by 
• 
IDS-Bro (31%) and NetFlows (26%) monitors, a significant 
fraction of incidents (27%) are not detected by any alert but 
identified by the third party external notification.  
Almost  26%  of  the  incidents  analyzed  involved 
• 
credentials  stealing.  For  this  incident  category,  an  attacker 
usually  (97%  of  the  time)  enters  with  already-stolen 
credentials of a legitimate user [20] and hence the behavior is 
the same as that of a malicious insider2.  
Association  of  alerts  with  incidents  reveals  that 
• 
usually  one  (or  at  most  two)  alert(s)  is(are)  responsible  for 
identifying an incident, and the same alert can be triggered 
by different attacks.  
Nearly 50% of the incidents are detected in the last 
• 
phase of an attack, when attackers start misusing the system.  
Anomaly-based  detectors  are  seven  times  more 
• 
likely  to  capture  an  incident  than  are  signature-based 
detectors.  However  the  signature-based  detectors  (due  to 
their specialization) have fewer false positives compared to 
the anomaly-based detectors.  
incidents  are  not 
• 
necessarily the most efficient alerts; e.g., TopN alert detects 
Alerts  detecting 
the  most 
1 A detailed connectivity map also based on a five minute snapshot is given 
in (www.ncsa.illinois.edu/~jbarlow/graphs/nfdump-5-min-snapshot.gif). 
2 Our earlier study (using a subset of the same data logs) showed that in all 
but  one  credential  stealing  incident,  attackers  obtained access  to  the  host 
using  a  stolen  password  (78%),  a  public  key  (16%),  or  combination  of 
multiple authentication means (e.g., password + publickey) (6%) [20].  
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:46:24 UTC from IEEE Xplore.  Restrictions apply. 
978-1-4244-9233-6/11/$26.00 ©2011 IEEE50615%  of  the  incidents  of  low-to-medium  severity,  but  it 
exhibits a 33% false positive rate. 
Although this work focuses on a NCSA network, a good 
portion of the analyzed incidents pertain to other campus or 
open research networks. In a fully closed environment, e.g., a 
corporate  network  with  firewalls  on  perimeter,  types  of 
attacks  and  type  of  misuses  identified  may  be  different. 
However, a significant fraction of incidents (about 26%) we 
analyzed  are  due  to  compromised  credentials  when  an 
attacker  penetrates  the  system  using  the  already  stolen 
credentials (e.g., user password) of a legitimate user.  Such 
incidents  could  happen  in  virtually  any  network  which  is 
accessible  from  the  Internet  (e.g.,  social  networking  sites, 
email systems, corporate networks allowing VPN access) or 
from  an  intranet  or  a  business  network  managed  by  an  IT 
department  within  a  corporation.  Therefore,  insights  from 
this work are representative of many different environments 
and  can  be  used  to  guide better  design and  organization  of 
defense mechanisms. 
Figure 1: Five-Minute Snapshot of In-and-Out Traffic within NCSA. 
Figure 2: Connections of a system involved in a security incident. 
The  study,  in  addition  to  providing  insights  into  the 
attacks’ evolution and efficiency of alerts over a long period 
of time, brings an understanding of incidents missed by the 
alerts.  The  tangible  benefit  would  be  in  designing  better 
detection tools. For example, learning the phase of an attack 
in  which 
important 
information to guide design of protection mechanisms. The 
intangible  benefit  is  to  promote  research  in  the  area  of 
designing  methods  that  can  directly  improve  network 
security. 
triggered  provides 
the  alerts  are 
II.  MONITORING TOOLS 
detection 
to 
techniques 
NCSA  employs  a  variety  of  monitoring  tools  and 
corresponding 
provide 
comprehensive  detection  coverage  [2].  Fig.  3  depicts  the 
monitoring  and  alert  generation  architecture  used  to  collect 
the measurements at both the network and host layers. At the 
network  layer  Bro  IDS  (www.bro-ids.org)  [16]  is  used  to 
perform  deep  packet  inspection  of  network  traffic  going 
through the border router for detection of anomalous activity. 
Additional  network  traffic  analysis  is  performed  using 
network  flow  logs.  A  network  flow  is  a  7-tuple  record  of 
network transmission data: IP address, ports, protocol, bytes 
and  packet  count  for  both  source  &  destination  hosts.  The 
network  flow  (netflow)  collectors  are  distributed  such  that 
flows are monitored and logged from the border router using 
Argus (www.qosient.com/argus/) and from the internal routers 
using 
The 
measurements  give  visibility  to  traffic  within  the  internal 
subnets (nfdump) as well as to the traffic going in and out of 
the network (nfdump + Argus).  
(sourceforge.net/projects/nfdump). 
nfdump 
Figure 3: Monitoring Architecture Deployed at NCSA. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:46:24 UTC from IEEE Xplore.  Restrictions apply. 
507and 
cost, 
Host layer input sources in Fig. 3 consist of syslog data 
and file change logs generated by the File Integrity Monitor 
(FIM)  (Samhain  Labs:  la-samhna.de/samhain).  The  central 
syslog collector has built-in redundancy in order to execute 
in  a  failsafe  mode.  The  installation  of  FIMs  and  central 
syslog  servers  is  limited  for  reasons  of  performance,  data 
collection 
scalability.  However,  network 
monitoring  can  somewhat  compensate  for  the  selective 
deployment  of  FIMs  to  indentify  an  attack  and  provide 
relevant forensic information.  
Additional  detection 
from 
information  gathered  from  partner  sites,  security  mailing 
groups  or  blacklists  (watchlists)  of  malicious  IP  addresses 
provided by the peers.  
are  generated 
Finally,  it  should  be  noted  that  anti-virus  and  anti-
malware  tools  are  deployed  within  the  network.  However, 
these 
tools  are  used  as  a  part  of  post-incident 
investigation/cleanup rather than for proactive alerting. Very 
decentralized  architecture  and  management  of  the  system 
with no mandate for a standard software stack and high false 
positive  rate  are  primary  reasons  to  not  use  these  tools  as 
primary alert mechanisms.  
A. 
Analyzers and alerts  
rules 
Data  collected  by  all  four  monitoring  tools  (network-
based:  Bro  IDS  and  NetFlow  analyzers;  and  host-based: 
syslog  server  and  file  integrity  monitor)  is  analyzed  for 
anomalies  and  signature  matches.  For  instance,  Bro  IDS 
alerts are generated when a protocol-based rule or policy is 
violated  -  when  a  DNS  query  is  made  from/to  a  hostile 
domain or when an internal host downloads a binary from a 
blacklisted URL (using HTTP protocol). Custom policy rules 
designed  to  monitor  NCSA’s  cluster  are  based  on  network 
traffic  characteristics  of  cluster  nodes,  e.g.,  no  compute 
nodes  of  the  cluster  should  be  downloading  anything.  
Netflows  trigger  alerts based on  the predetermined  network 
traffic  characteristics  (e.g.,  number  of  bytes  transferred, 
traffic  to/from  a  watchlisted  IP  address,  and  connection 
to/from  specific  ports).  File  Integrity  Monitor  watches  for 
unauthorized  modification,  creation,  deletion  or  permission 
changes in specified system files, e.g., configuration files and 
OS binaries. A Simple Event Correlation engine (SEC) [18] 
uses  syslog  data  to  generate  alerts  on  anomalies  based  on 
rule sets, such as new user creation, user authentication and 
command  history  profiles  and  privilege  escalations  by  new 
users, etc. 
In  addition  to  the  generated  alerts,  the  data  from  all 
monitoring  sources  is  archived.  This  can  be  valuable  in 
conducting forensic analysis while tracing specific attacks.  
The  Security  Team  determines  the  response  action 
warranted  for  the  alerts  generated  by  the  monitoring  tools. 
Some  of  the  alerts  result  in  proactive  actions,  e.g.,  sending 
reset packets to a scanned IP address (or proactive blocks). 
Other  actionable 3  alerts  (those  determined  to  be  true 
positives)  are  forwarded  to  the  Incident  Response  and 
3 Of roughly 140 alerts seen every day, each alert is actionable and 
is investigated until determined to be a false positive (vast majority 
of the daily alerts turnout to be false positives). 
Security Team (IRST) for investigation and mitigation of the 
incident.  At  the  end  of  every  investigation,  the  Security 
Team  performs  its  own  internal  analysis  to  calibrate  the 
monitoring tools based on the lessons learned.  
While  each  of  the  monitoring  tools  provides  important 
data in its own right, it is also limited in scope by its nature 
and  by  its  deployment,  in  terms  of  access  to  network  and 
system  infrastructure  and  the  quality  and  amount  of  data 
gathered.  A  combination  of 
tools  provides  a 
comprehensive  view  of  activity  inside  the  network  and  the 
systems. 
these 
III.  SECURITY IMPLICATIONS OF AN OPEN NETWORK 
ARCHITECTURE 
from 
ranging 
high-performance 
security  protection 
NCSA  is  an  open  network  architecture  that  supports 
services 
scientific 
computations  and  web,  email  and  file  servers  to  small 
research clusters and user desktop/laptops. Because of very 
high  bandwidth  requirements  (support  of  multiple  10  Gb 
links),  traditional  firewall  strategies  are  not  sufficient  for 
providing 
and 
operational  staff  are  allowed  to  run  their  own  flavors  of 
operating  systems  and  software,  which  results  in  a  very 
heterogeneous  computing  environment.  There 
is  no 
mandatory software stack requirement for the systems except 
for  certain  production  and  critical  infrastructure  machines. 
Due  to  stringent  dependency  requirements  of  exclusive 
scientific 
and 
proprietary  code)  and  grid-based  file  systems  (e.g.,  GPFS), 
often kernel upgrades are delayed.  
[1].  Researchers 
computing 
compilers 
Furthermore,  the  Grid  computing  component  of  the 
NCSA network environment brings the challenge of having a 
very broad base of users, each one of which needs a way to 
authenticate  to  systems  and  has  his/her  own  specific 
requirements  for  running  jobs.  The  widespread  user  base 
often  makes  the  network  boundaries  blurred  and  any 
successful adversarial action at NCSA may affect peer sites. 
Likewise,  security  breaches  at  peer  sites  directly  affect  the 
security  state  at  NCSA.  Additionally,  the  NCSA  security 
team doesn’t have much control over the security of the end 
users’  computers.  Different  administrative  boundaries  for 
grid  computing  environments  make  it  difficult  to  have  a 
unified  security  policy  that  not  only  addresses  concerns  of 
users locally but also reaches across the grid.  
software 
(e.g., 
IV.  RELATED WORK  
relatively 
While  many  techniques  to  protect  against  attacks  are 
available, 
little  has  been  published  on 
comprehensive  measurement-based  analysis  of  different 
types  of  attacks,  from  alerts  to  identifiable  incidents. 
Databases like those provided by CERT (www.cert.org) and 
CVE  (cve.mitre.org)  document  vulnerabilities  and  possible 
exploits.  These  data  are  often  used  for  analyzing  and 
modeling vulnerabilities. For example, [21] uses data mining 
techniques  to  study  traffic  data  during  an  attack  for 
identifying  signatures  of 
intrusion  detection;  [4]  uses 
vulnerability data to develop a finite-state machine model for 
analyzing  vulnerabilities  and attacks  at  the code  level. [19] 
describes the analysis of a single denial-of-service attack on 
a server. Several studies [12], [13], [23] have analyzed attack 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:46:24 UTC from IEEE Xplore.  Restrictions apply. 
508[6], 
Studies 
data  to  build  formal  models  involving  both  single  and 
multiple  nodes.  Several  authors  have  proposed  models  that 
correlate  alerts  to  incidents.  Two  examples  include  [10], 
where  a  capability-based  model  is  proposed  and  verified 
using  real  attack  data,  and  [15],  where  prerequisites  and 
consequences model are discussed. In [22] the authors view 
attacks  as  a  set  of  capabilities  that  provide  support  for 
abstract  attack  concepts.  Using  this  notion  the  paper 
introduces a model for computer attacks, and a language for 
specifying  the  model  and  shows  how  it  can  be  used  in 
vulnerability  analysis, 
intrusion  detection  and  attack 
generation.  
[11]  and 
Other  sources  that  collect  attack  data  include  honeypot 
experiments 
the  DETERlab  Testbed 
(www.isi.edu/deter).  Red  teams  have  often  been  used  to 
collect  network  vulnerability  data;  these  data  are  generally 
unpublished.  Several  studies  focus  on  classification  of 
security flaws, vulnerabilities, and attacks; examples include 
[3], [14], and [5].  In [17] an attack is described as a natural 
progression  through  seven  unique  phases;  we  use  this 
classification in our analysis. In addition, the Bro team (bro-
ids.org)  conducted  extensive  studies  of  attacks  for  the 
purpose  of  developing  monitoring  tools.  While  [24]  is  a 
comprehensive  report  on  the  analysis  of  security  incidents 
between  1989  and  1995,  it  does  not  concentrate  on  how 
incidents were detected. Most recently, the Verizon Business 
RISK  Team  investigated  90  security  breaches  in  2009, 
encompassing  285  million  compromised  records  [24].  This 
report also concentrates on the effect of the incidents instead 
of their detection. 
show 
in 
understanding  attack  patterns  and  building  more  efficient 
protection mechanisms.  For example, [4] introduces a finite 
state  machine  model  methodology  to  analyze  operations 
involved  in  exploiting  application  vulnerabilities  and  to 
identify 
the 
elementary activity level to foil an attack. More recently, [9] 
proposes a state machine-based attack model that is verified 
in an emulated environment using real security monitors. In 
this  approach  the  model  is  limited  in  scope  and  an 
independent  state  machine  is  built  for  each  incident. 
Likewise,  [26]  proposed  a  state  machine  model-based 
language to describe computer penetrations as sequences of 
actions  that  an attacker performs  in  order  to compromise  a 
computer system. 
that  modeling  can  be  useful 