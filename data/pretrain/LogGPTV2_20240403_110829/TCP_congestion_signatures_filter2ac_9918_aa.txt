title:TCP congestion signatures
author:Srikanth Sundaresan and
Mark Allman and
Amogh Dhamdhere and
kc claffy
TCP Congestion Signatures
Srikanth Sundaresan
Princeton University
PI:EMAIL
Mark Allman
ICSI
PI:EMAIL
Amogh Dhamdhere
CAIDA/UCSD
PI:EMAIL
kc claffy
CAIDA/UCSD
PI:EMAIL
ABSTRACT
We develop and validate Internet path measurement techniques to
distinguish congestion experienced when a ﬂow self-induces con-
gestion in the path from when a ﬂow is affected by an already con-
gested path. One application of this technique is for speed tests,
when the user is affected by congestion either in the last mile or
in an interconnect link. This difference is important because in the
latter case, the user is constrained by their service plan (i.e., what
they are paying for), and in the former case, they are constrained by
forces outside of their control. We exploit TCP congestion control
dynamics to distinguish these cases for Internet paths that are pre-
dominantly TCP trafﬁc. In TCP terms, we re-articulate the question:
was a TCP ﬂow bottlenecked by an already congested (possibly in-
terconnect) link, or did it induce congestion in an otherwise idle
(possibly a last-mile) link?
TCP congestion control affects the round-trip time (RTT) of pack-
ets within the ﬂow (i.e., the ﬂow RTT): an endpoint sends pack-
ets at higher throughput, increasing the occupancy of the bottle-
neck buffer, thereby increasing the RTT of packets in the ﬂow. We
show that two simple, statistical metrics derived from the ﬂow RTT
during the slow start period—its coefﬁcient of variation, and the
normalized difference between the maximum and minimum RTT—
can robustly identify which type of congestion the ﬂow encounters.
We use extensive controlled experiments to demonstrate that our
technique works with up to 90% accuracy. We also evaluate our
techniques using two unique real-world datasets of TCP through-
put measurements using Measurement Lab data and the Ark plat-
form. We ﬁnd up to 99% accuracy in detecting self-induced con-
gestion, and up to 85% accuracy in detecting external congestion.
Our results can beneﬁt regulators of interconnection markets, con-
tent providers trying to improve customer service, and users trying
to understand whether poor performance is something they can ﬁx
by upgrading their service tier.
CCS CONCEPTS
• Networks → Network measurement;
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a
fee. Request permissions from permissions@acm.org.
IMC ’17, Internet Measurement Conference, November 1–3, 2017, London, United
Kingdom
© 2017 Association for Computing Machinery.
ACM ISBN 978-1-4503-5118-8/17/11. . . $15.00
https://doi .org/https://doi .org/10 .1145/3131365 .3131381
KEYWORDS
Internet congestion, Throughput, TCP
ACM Reference Format:
Srikanth Sundaresan, Amogh Dhamdhere, Mark Allman, and kc claffy. 2017.
TCP Congestion Signatures. In Proceedings of IMC ’17, November 1–3,
2017, London, United Kingdom, Internet Measurement Conference, 14 pages.
https://doi .org/https://doi .org/10 .1145/3131365 .3131381
1 INTRODUCTION
Exploding demand for high-bandwidth content such as video
streaming, combined with growing concentration of content among
a few content distribution networks [8, 13–15, 26, 27, 50] —some
large and sophisticated enough to adjust loading, and therefore
congestion levels on interconnection links [18, 28]—has resulted
in lengthy peering disputes among access ISPs, content providers,
transit providers that center on who should pay for the installation
of new capacity to handle demand. As a result, there is growing
interest in better understanding the extent and scope of congestion
induced by persistently unresolved peering disputes, and its impact
on consumers. But this understanding requires a capability that the
Internet measurement community has not yet provided in a usable
form: the ability to discern interconnection congestion from the
congestion that naturally occurs when a last mile link is ﬁlled to
capacity. Implementing such a capability would help a variety of
stakeholders. Users would understand more about what limits the
performance they experience, content providers could design better
solutions to alleviate the effects of congestion, and regulators of the
peering marketplace could rule out consideration of issues where
customers are limited by their own contracted service plan.
Although a large body of work has focused on locating the bot-
tleneck link and characterizing the impact of loss and latency on
TCP performance [23, 30, 32–34, 43, 48, 51, 52], they doesn’t in-
form us about the type of congestion. Recent attempts to use coarse
network tomography to identify interconnect congestion [36], also
have shortcomings [49]. To the best of our knowledge, there is no
technique that can reliably identify whether a ﬂow is bottlenecked
by an initially unconstrained path (that it ﬁlls up) or whether it was
bottlenecked by an already congested path, without having a pri-
ori knowledge about the path, i.e., the capacity of its bottleneck
link and the trafﬁc proﬁle of the link. Such a technique would dif-
ferentiate between, for example, a ﬂow that is bottlenecked by the
last-mile access link versus one that is bottlenecked by a congested
interconnect link. In this paper, we identify distinctive signatures in
ﬂow RTT during the TCP slow start period that can reliably distin-
guish these two scenarios.
IMC ’17, Internet Measurement Conference, November 1–3, 2017, London, United Kingdom
Sundaresan et al.
Our technique exploits the effect of the bottleneck link buffer
on ﬂow RTT. Flow RTT is the RTT of packets within a TCP ﬂow,
which can be computed using sequence and acknowledgment num-
bers within the packets. When a TCP ﬂow starts on an otherwise
uncongested path, it drives buffering behavior in the bottleneck link
by increasing its occupancy. On the other hand, when the ﬂow starts
in a path that is already congested, ﬂow RTT is dominated by buffer-
ing in the congested link. We identify two parameters based on ﬂow
RTT during TCP slow start that we use to distinguish these two
cases—the coefﬁcient of variation of ﬂow RTT, and the normalized
difference between the maximum and minimum RTT. We use these
two parameters, which can be easily estimated for TCP ﬂows, to
build a simple decision tree-based classiﬁer.
We validate the classiﬁer using both a controlled testbed as well
as real-world data. We build a testbed to conduct extensive con-
trolled experiments emulating various conditions of access and peer-
ing link bottlenecks, and show that classiﬁer achieves a high level
of accuracy, with up to 90% precision and recall. We then apply
our techniques on two real-world datasets. Our ﬁrst dataset consists
of throughput test data collected by the M-Lab infrastructure [38],
speciﬁcally Network Diagnostic Test (NDT) measurements from
January through April of 2014 [3, 4]. This timeframe spanned the
discovery and resolution of an interconnect congestion event be-
tween Cogent (a major transit provider) and large access ISPs in
the US. Data in January and February showed a marked drop in
throughput during peak hours compared to off-peak data in March
and April, after Cogent resolved the issue. We use this episode to
label the dataset—peak hour trafﬁc in January and February as inter-
connect congested and off-peak trafﬁc in March and April as access-
link congested—and ﬁnd that the decision tree classiﬁer allows us
to classify these ﬂows accurately. Because the ﬂows are coarsely
labeled (we do not have ground truth data about the clients that ran
the throughput test, and therefore resort to blanket labeling based
on month and time-of-day), we conduct a more focused experiment,
running throughput periodic tests between a single host and a single
M-Lab server between February and April 2017. We choose these
hosts based on evidence we found of occasional interconnection
congestion in the path between them using the Time Series Latency
Probing (TSLP) [35] method; we also know the service plan rate of
the client. In this experiment, our decision tree classiﬁer detected in-
terconnect congestion with an accuracy of 75-85% and access-link
congestion with an accuracy of 99%. Our false negatives in this
experiment mostly occur with higher throughput (but not enough
to saturate the access link), and lower interconnect buffer latency,
which suggest a legitimate gray zone when it is not clear what type
of congestion occurred.
Our proposed technique has two important advantages: it pro-
vides per-ﬂow diagnosis, and relies only on the ﬂow itself without
needing out-of-band probing. Out-of-band probing can introduce
confounding factors such as load balancing, and differential servic-
ing of probe packets. Our technique can supplement existing efforts
to understand broadband performance, such as the FCC Measuring
Broadband America, to not just understand what throughput users
get, but also the role of the ISP infrastructure and the interconnect
infrastructure in the throughput they achieve.
The rest of the paper is structured as follows. We develop the
intuition behind our technique in § 2, and validate the intuition by
building a model and testing it using controlled experiments in our
testbed in § 3. We describe how we use and label M-Lab data dur-
ing a 2014 peering congestion event, and how we conduct a more
focused experiment using M-Lab in 2017 in § 4, and how we val-
idate our model using these datasets in § 5. We then discuss the
limitations of our model in § 6. We describe related literature in § 7,
and conclude in § 8.
2 TCP CONGESTION SIGNATURES
We are interested in the scenario where we have a view of the ﬂow
from the server, but no knowledge about the path or link capaci-
ties. This scenario is common for speed test providers such as M-
Lab NDT, or Ookla’s Speedtest [40]. These tests inform the user
about the instantaneous capacity of the path between the user and
the speed test server is, but not whether the capacity is limited by
the access link (i.e., the user’s ISP service plan). In this section we
develop the intuition behind our technique to detect the nature of
congestion in TCP ﬂows. We ﬁrst deﬁne the two types of conges-
tion events we are interested in, and then describe how we build a
model based on TCP ﬂow RTT that can distinguish them.
2.1 Self-induced vs External Congestion
We refer to the link with the smallest available capacity on the path
between a server and client as the capacity bottleneck link. Further,
we say a link is “congested” when trafﬁc is being buffered at the
head end of the link (i.e., the trafﬁc load is greater than the available
link capacity).
Self-induced congestion occurs when a TCP ﬂow starts in an oth-
erwise uncongested path, and is able to saturate the capacity bottle-
neck link. In other words, self-induced congestion occurs when a
ﬂow’s throughput is limited by the capacity bottleneck link and the
ﬂow itself drives buffer occupancy at the head of the bottleneck link.
An example of such congestion is when a speed test saturates the
client’s access link.
External congestion occurs when a TCP ﬂow starts in a path with
an already congested link. In this case, the available capacity on the
bottleneck link is essentially zero because the link is congested.1 In
terms of buffer behavior, the new ﬂow has little impact on buffer oc-
cupancy because external trafﬁc was congesting the link before the
new ﬂow started. For example, a speed test that is bottlenecked by
an already congested non-edge link, say an interconnect link, and
which therefore is unable to saturate the client access link, experi-
ences external congestion.
Flows could fail to saturate the path bottleneck link for other
reasons, e.g., high latency, random loss, low application demand, or
small window sizes. Such ﬂows do not experience congestion, and
existing techniques [10, 52] can detect such factors limiting TCP
throughput; we do not consider them in this paper.
1Note: Because Internet trafﬁc is elastic the new ﬂow will ultimately utilize some of
the capacity of the bottleneck link because other ﬂows will backoff.
TCP Congestion Signatures
IMC ’17, Internet Measurement Conference, November 1–3, 2017, London, United Kingdom
F
D
C
1.0
0.8
0.6
0.4
0.2
0.0
External
Self
External
Self
1.0
0.8
0.6
0.4
0.2
F
D
C
101
102
0.0
10−2
10−1
100
(a) Max - min RTT (ms) of packets during slow start
(b) Coefﬁcient of variation of RTT of packets during slow start
Figure 1: RTT signatures for self-induced and external congestion events. Self-induced congestion causes a larger difference between
the minimum and maximum RTT, and also larger variation in RTT during the slow start period. For illustrative purposes, we show
data from from one set of experiments using a 20 Mbps emulated access link with a 100 ms buffer, served by a 1 Gbps link with a
50 ms buffer. The access link has zero loss and 20 ms added latency.
2.2 Challenges in Identifying the Type of
Congestion
The server point of view has several advantages, the most important
being that it has direct information about outgoing packets and TCP
state. However, even with a detailed view of the ﬂow, distinguishing
between the two types of congestion that we list above is challeng-
ing. Some techniques include analyzing the ﬂow throughput, TCP
states [52], and/or ﬂow packet arrivals [34, 48] or RTT. Each has its
advantages and drawbacks.
Information about ﬂow throughput is insufﬁcient to determine
the type of congestion unless we also know the actual service plan
of the client. For example, if we see a 9 Mbps ﬂow, the type of con-
gestion event it encountered depends on whether the service plan
was, say, 10 Mbps (likely self-induced), or 20 Mbps (likely exter-
nal). However, typically, only the user and their ISP know the ser-
vice plan rates, and external throughput test services such as M-Lab
or Ookla do not have access to it. Additionally, available service
plans in the U.S. vary across a wide range of throughputs, from less
than 10 Mbps to exceeding 100 Mbps. Therefore achieved through-
put, even with associated parameters such as congestion window
size, is not a useful indicator of type of congestion.
TCP state analysis [52] can help us analyze TCP state transitions
and ﬂow behavior; however it does not help us differentiate between
different kinds of congestion. Transitions to/from the fast retrans-
mit or the retransmission timeout state can potentially tell us about
congestion events. However, in practice, we found it difﬁcult to pa-
rameterize and model these state changes. Simple techniques such
as modeling the total number of fast retransmit and timeout states
per time interval or the time to the ﬁrst retransmit state have the
same difﬁculty that it varies according to the path latency, service
plan of the client, loss-rate, and cross-trafﬁc, which are difﬁcult to
systematically account for in controlled settings. Ideally, we would
prefer parameters that are robust across a range of settings.
Previous work has also used packet arrival patterns to uncover a
congested path [34, 48]. Such techniques typically have the require-
ment that they be downstream of the point of congestion to be able
to measure packet arrival rate. That is not possible with the server
point of view, nor from clients unless they have access to network
packets. Though packet spacing can be approximated by analyzing
ACK arrival patterns, ACKs can be noisy, and cannot tell us any
more than that the ﬂow encountered congestion.
Flow RTT, particularly at a per-packet granularity, contains infor-
mation about the condition of the underlying path. In particular, the
RTTs of packets in a ﬂow allow us to distinguish between an empty
bottleneck buffer (increasing RTT as the ﬂow ﬁlls up the buffer)
and a busy buffer (RTT is relatively stable as it is dominated by the
added latency due to an already full buffer). We use these properties
of the RTT to build our model, and it relies only on one essential
component of the path, the buffer, and therefore yields robust re-
sults in our controlled testbed that translate well to the real world.
Flow RTTs are useful only during the slow start period, but fortu-
nately this short interval is sufﬁcient for us to be able to distinguish
the two congestion states. We now describe the intuition behind this
technique in more detail.
2.3 Using Flow RTT to Distinguish Congestion
Type
• Self-induced congestion: The buffer at the head of the bottleneck
link is empty when the ﬂow starts. As the ﬂow scales up, this
buffer ﬁlls up, causing an increase in the ﬂow RTT—the RTT mea-
sured towards the end of slow start will be signiﬁcantly higher
than the RTT measured at the beginning. The difference depends