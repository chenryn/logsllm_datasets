most important: per-anycast site analysis and per client AS analysis, and rank
each by median latency, interquartile range (IQR) of latency, and query volume.
We focus on anycast sites because that part of the problem is under operator
control. If we ﬁnd sites with high latency, we can examine routing and perhaps
correct problems [7,68].
Clients ASes examine the user side of the problem (at recursive resolvers),
since client latency is a goal in DNS service. While performance in client ASes
can be diﬃcult to improve because we do not have a direct relationship with
those network operators, we show in Sect. 4 that we can address problems in
some cases.
Finally, we consider median latency, interquartile range, and query volume to
prioritize investigation. Median latency is a proxy for overall latency at the site.
The interquartile range (the diﬀerence between 75%ile and 25%ile latencies),
captures the spread of possible latencies at a given site or AS. Finally, query
volume (or rate) identiﬁes locations where improvements will aﬀect more users.
We sort by overall rate rather than the number of unique sources to prioritize
large ASes that send many users through a few recursive resolvers (high rate,
low number of recursive IPs).
Prioritization by Site: Figure 5 shows per-site latency for .nl, broken out
by protocol (IPv4 and IPv6) and site, for two anycast services (A and B). For
each site, we show two bars: the fraction of total queries and the number of
ASes (ﬁlled and hatched bar in each cluster). We overlay both with whiskers for
latency (with median in the middle and 25%ile and 75%ile at whisker ends). In
these graphs some sites (such as CDG for Anycast B in IPv6) stand out with high
interquartile ranges, while others with lower interquartile range (for Anycast B,
LAX-A and NRT in IPv4 and NRT and GRU in IPv6). We look at these cases
in detail in Sect. 4.
274
G. C. M. Moura et al.
Fig. 5. .nl distribution of queries and ASes per site (pink bars) and latency (median,
25%ile, and 75%ile, (green lines), for each anycast site, for two services (Anycast A
and B) and two protocols (IPv4 and IPv6). Data from 2020-10-15 to -22. (Color ﬁgure
online)
Prioritization by Client AS: Figure 6 and Fig. 19 (Appendix B) show the
latency distribution for the ten ASes with the largest query volume for Anycast
A and B of .nl. (Due to space constraints, we show the complete list of AS
names in Appendix C). While many ASes show good latency (low median and
small interquartile range), we see the top two busiest ASes for Anycast A in IPv4
(Fig. 6a) show a high median and large interquartile range (Fig. 6b). These ASes
experience anycast polarization, a problem we describe in Sect. 4.3.
Figure 7 shows latencies for the top ASes for B-root, with quartile ranges as
boxes and the 10%ile and 90%iles as whiskers. Rather than split by protocol,
here we show both rankings and by query rate (Appendix B) on the x-axis. While
rank gives a strict priority, showing ASes by rate helps evaluate how important
it is to look at more ASes (if the next AS to consider is much, much lower
rate, addressing problems there will not make as large a diﬀerence to users). We
identify speciﬁc problems from these graphs next.
4 Problems and Solutions
Given new information about IPv4 and IPv6 latency from DNS/TCP (Sect. 2),
and priorities (Sect. 3), we next examine anycast performance for two of the four
anycast services operating for .nl, and for B-root. For each problem, we describe
how we found it, the root causes, and, when possible, solutions and outcomes.
We show two problems that have been documented before (Sect. 4.1 and Sect.
4.2) and a new problem (Sect. 4.3). While some of these problems may have
been eventually discovered when users encountered them, we have discovered
Old but Gold: Prospecting TCP to Engineer
275
Fig. 6. .nl Anycast A queries and RTT for the top 10 ASes ranked by most queries
(bars left axis). Data; 2019-10-15 to -22. ASes list on Appendix C).
each from our prioritized monitoring of DNS/TCP latency (Sect. 3). Our near-
real-time monitoring (Sect. 4.4) supported the discovery of these problems before
user complaints.
4.1 Distant Lands
The ﬁrst problem we describe is distant lands: when a country has no any-
cast server locally and has limited connectivity to the rest of the world. When
trans-Paciﬁc traﬃc was metered, these problems occurred for Australia and New
Fig. 7. B-root: latency of top talkers by
rank
Fig. 8. Anycast B and Comcast: RTT
before and after resolving IPv6 miscon-
ﬁguration.
276
G. C. M. Moura et al.
Zealand. Today we see this problem with China. China has a huge population
of Internet users, but its international network connections can exhibit conges-
tion [73].
Detection: We discovered this problem by observing large interquartile latency
for .nl’s Anycast B in v4 (Fig. 5c) and v6 (Fig. 5d) at Tokyo (NRT, both v4
and v6), Singapore (SIN, v6), and CDG (v6), all with 75%iles over 100 ms.
Fig. 9. Anycast B, Japan site (NRT): top
8 querying ASes are Chinese, and respon-
sible for 80% of queries.
Fig. 10. .nl Anycast A and Microsoft
(IPv4): RTT before and after depolariza-
tion.
These wide ranges of latency prompted us to examine which recursive
resolvers visiting these sites and showed high latency. Many queries come from
ASes in Asia (Fig. 9). NRT sees many queries (6.1% of total, more than its “fair
share” of 5.2%). Of the top 10 ASes sending queries to NRT, 9 are from China
(see Fig. 9).
We see many Chinese ISPs that also send IPv6 traﬃc to Paris (CDG), result-
ing in a large RTT spread. Not only must their traﬃc traverse congested, inter-
national links, but they also then travel to a geographically distant anycast site,
raising the 75%ile RTT at CDG over 100 ms (even though its median is under
22 ms).
Resolution: While we can diagnose this problem, the best resolution would
be new anycast servers for Anycast B inside China (or manipulate BGP to
attempt to steer traﬃc to nearby Tokyo or Singapore sites). The operator is
considering deployment options, but foreign operation of sites in China has only
been recently allowed [73], and anycast operation from China risks service for
some non-Chinese clients traversing the Chinese national ﬁrewall.
4.2 Prefer-Customer to Another Continent
The second root-cause problem we found is when one AS prefers a distant anycast
site, often on another continent, because that site is a customer of the AS. (Recall
that a common BGP routing policy is prefer customer: if an AS can satisfy a
route through one of its customers, it prefers that choice over an alternate route
through a peer or transit provider. Presumably, the customer is paying the AS
for service, while sending the traﬃc to a peer or via transit is either cost-neutral
or incurs additional cost).
Old but Gold: Prospecting TCP to Engineer
277
We have seen this problem in two situations: at .nl Anycast B’s Brazil site,
and B-root for its site in South America. While the ISP should be able to choose
where it sends its traﬃc, anycast service operators would like to know when such
policies result in large client latencies, so that can consider exploring peering
options that might lower latency.
.nl Detection: We detected this problem for .nl Service B by observing high
IPv6 median latency (124 ms) for queries is in S˜ao Paulo, Brazil (GRU) in Fig.
5d. Examination of the data shows that many of the high-latency queries are from
Comcast (AS7922), a large U.S.-based ISP. As with China and CDG, this case
is an example of queries traveling out of the way to a distant anycast site, ignor-
ing several anycast sites already in North America. We conﬁrmed that North
American clients of this AS were routing to the Brazil site by checking CHAOS
TXT queries [1] from RIPE Atlas probes to Anycast B (data: ComcastV6 [50]).
.nl Resolution: We contacted .nl Anycast B’s operator, who determined that
the issue was with one of their upstream providers. This provider had deployed
BGP communities to limit the IPv4 route to South America. After our contact,
they deployed the same community for IPv6, and the Comcast traﬃc remained
in the US.
We ﬁrst conﬁrm the problem was resolved by analyzing traces from Anycast
B, and by conﬁrming that Comcast IPv6 clients were now answered by other
North American sites. The solution reduced 75%ile latency by 100 ms: in Fig. 8
before the change, IPv6 shows IQR of 120 ms for Anycast B. After this change
on 2020-03-23t00:00, we see the IQR falls to 20 ms. Second, we also veriﬁed
with Atlas probes hosted on Comcast’s network (data: ComcastV6-afterReport
in [50]), and the median RTT from Comcast Atlas was reduced from 139 ms to
28 ms.
B-root Detection: B-root has observed high latencies for traﬃc going to a
South-American anycast site of B-root. As with .nl and GRU, we examined
traﬃc and identiﬁed a primarily-North American ISP that was sending all of
its traﬃc to the South American site, ignoring all other lower-latency sites. We
then conﬁrmed that an AS purchases transit from this ISP.
B-root Resolution: We have not yet a completely satisfactory resolution to this
problem. Unfortunately, the AS that purchases transit from the North Amer-
ican ISP does not directly peer with B-root, so we cannot control its peering.
We currently poison the route to prevent latency problems, which signiﬁcantly
reduces traﬃc arriving at this site.
4.3 Polarization with Google and Microsoft
We next describe anycast polarization, a problem we ﬁrst described in June
2020 [39]. We are the ﬁrst to explain and demonstrate the impact of polarization
on performance, although subsequent work reported it in a testbed study [64].
Like prefer-customer, it involves high latency that derives from traﬃc being
278
G. C. M. Moura et al.
Table 5. Anycast A: Polarized ASes and query distribution (Oct 15-22,2019).
Queries
Queries top site (% top site)
Google
860 775 677 860 774 158
IPv4
IPv6
433 145 168 433 145 119
427 630 509 427 629 039
Microsoft 449 460 715 449 455 487
449 439 957 449 434 729
99.9998
99.9999
99.9997
99.9988
99.9988
IPv4
IPv6
20 758
20 758
100
needlessly sent to another continent. But it follows from BGP’s limited knowl-
edge of latency (AS path length is its only distance metric) and the ﬂattening
of the Internet [27].
Detecting the Problem. We discovered this problem by examining
DNS/TCP-derived latency from the top two ASes sending queries to .nl Anycast
A. As seen in Fig. 6b and Fig. 6d, AS8075 (Microsoft) and AS15169 (Google)
show very high IPv4 median latency (74 ms and 99 ms), and Google shows a very
high IQR (99 ms) Google also shows a high IPv6 median latency (104 ms).
Both Google and Microsoft are “hypergiants”, with data centers on multiple
continents (for .nl, ∼85% of Google’s traﬃc is from its Public Resolvers [38,62]).
Both also operate their own international backbones and peer with the Internet
in dozens of locations. These very high latencies suggest much of their DNS traﬃc
is traveling between continents and not taking advantage of .nl’s global anycast
infrastructure. This problem occurs in hypergiants with backbones that do not
consider multiple exits and anycast—by default they will route all their traﬃc
to one global anycast size, creating polarization. For companies with islands
connected by transit providers (without a corporate backbone), each island will
compute routing locally, so anycast “just works”.
Conﬁrming the Problem: .nl Anycast A has six sites, so we ﬁrst examine how
many queries go to each site. Table 5 shows the results—all or very nearly all
(four or ﬁve “nines”) go to a single anycast site due to routing preferences. For
Google, this site is in Amsterdam, and for Microsoft, Miami.
While a preferred site is not a problem for a small ISP in one location, it is
the root cause of very high latency for these hypergiants, who often route global
traﬃc internally over their own backbones, egressing to one physical location.
Even if it is the best destination for some of their traﬃc, one location will not
minimize latency for multiple, globally distributed, data centers. Such routing
defeats latnecy advantages of distributed anycast deployment [43,59].
Depolarizing Google to .nl Anycast A. Root-cause: We ﬁrst investigated
Google’s preference for AMS. .nl directly operates the AMS site (the other 5
Old but Gold: Prospecting TCP to Engineer
279
sites are operated by a North American DNS provider). We determined (working
with both the AMS and Google operators) that Google has a direct BGP peer-
ing with the site at AMS. BGP prefers routes with the shortest AS-PATH, and
in addition, ASes often prefer Private Network Interconnect (PNIs) over equal
length paths through IXPs, so it is not surprising it prefers this path. (The
general problem of BGP policy interfering with the lowest latency is well docu-
mented [4,5,7,28,34,59]. We believe we are the ﬁrst to document this problem
with hypergiants and anycast through PNI).
We next describe how we worked with the AMS operators and Google to
resolve this problem. We document this case as one typical resolution to show
the need for continuous observation of DNS latency through DNS/TCP to ﬁnd
the problem and conﬁrm the ﬁx.
Fig. 11. .nl Anycast A: queries and median RTT per site from Google (AS15169) –
January 2020.
Figure 11 show the eﬀects of our traﬃc engineering on anycast use and query
latency for IPv4 (IPv6 ﬁgures in Appendix D). Each graph shows traﬃc or
median client latency for each of the 6 .nl Anycast A sites. (Query latency is
determined by DNS/TCP traﬃc over each day.) The graphs show behavior over
January 2020, January 5th to 9th (the left, pink area) before any changes, the
9th to the 21st (the middle, green area) when the AMS route was withdrawn,
and ﬁnally after the 21st (the right, blue region) when AMS was restored, but
with diﬀerent kinds of policy routing.
These graphs conﬁrm that initially, AMS received all traﬃc from Google,
causing Anycast A to appear to be a unicast service to Google, precluding local-
ity in a global anycast service. We see that the median latency for Google is about
100 ms (Fig. 6a). Withdrawing the AMS peering with Google corrects the prob-
lem, spreading queries across multiple anycast sites, beneﬁting from geographic
locality. Median latency drops to 10 to 40 ms, although still around 100 ms at
YYZ in Miami for IPv4. LHR is now the busiest site, located in Europe.
Use of the North American sites considerably lowers median latency. We show
in Fig. 12 the depolarization results for all sites combined, for IPv4 and IPv6.
For both IPv4 and IPv6, we see median latency for all sites combined reducing
90 ms, from 100 to 10ms. The IQR was reduced from 95 to 10 ms for IPv4. For
280
G. C. M. Moura et al.
Table 6. BGP manipulations on AMS site of Anycast A – IPv4 and IPv6 preﬁxes to
Google (AS15169) on Jan 21, 2020 (Time in UTC). NE: No Export
Op. Day Time Prepend Community AMS(%)
1
2
3
4
5
6
21
22
22
22
22
22
15:00
9:53
9:59
10:21
10:37
11:00
2x
2x