to-ﬁxed converter translates the ﬂoating-point HLIL code to
ﬁxed-point code in a low-level intermediate language LLIL.
This step requires Athos to compute the right precision to
be used for maximum accuracy (Section III-B). Figure 4b
shows the LLIL code snippet for logistic regression. The
function calls in this sequence can be implemented with a
variety of secure computation backends - e.g. ABY [30] for
the case of 2-party secure computation, Porthos for the case
of semi-honest 3-party secure computation (Section IV) and
Aramis (Section V) for the malicious secure variant. Different
backends provide different security guarantees and hence vary
in their performance. For this example, the three backends take
227ms, 6.5ms, and 10.2ms respectively.
III. ATHOS
Athos compiles ML inference code written in TensorFlow
to MPC protocols. It has the following main components:
Frontend. Athos frontend compiles TensorFlow code to a
high-level intermediate language (HLIL). HLIL supports
ﬂoating-point
tensors and sequence of function calls
(corresponding to the TensorFlow nodes) that manipu-
late tensors. The main challenge in the frontend is to
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:36:59 UTC from IEEE Xplore.  Restrictions apply. 
338
Output
ArgMax
MatAdd
MatMul
b
x
W
(a)
Node
x
W
MatMul
b
MatAdd
ArgMax
Outgoing
dimensions
1 × 784
784 × 10
1 × 10
1 × 10
1 × 10
1 × 1
(b)
Fig. 3: Logistic Regression: (a) TensorFlow graph deﬁnition
(b) Metadata consisting of graph nodes and their outgoing
dimensions
xW = MatMul(x, W);
xWb = MatAdd(xW, b);
output(ArgMax(xWb));
(a)
//Assume Athos chooses
//15 bit precision
xW = MatMul(x, W);
ScaleDown(xW, 15);
xWb = MatAdd(xW, b);
output(ArgMax(xWb));
(b)
Fig. 4: Logistic Regression in (a) ﬂoating-point: HLIL syntax
(b) ﬁxed-point: LLIL syntax
reconcile dynamic typing in TensorFlow to static typing
in HLIL. TensorFlow code, written in Python, does not
have tensor dimensions, whereas our HLIL has explicit
tensor dimensions as it enables the compiler to perform
analyses and optimizations.
Float-to-ﬁxed converter. While ML models use ﬂoating-
point arithmetic, MPC protocols operate on ﬁxed-point
arithmetic. Rather than requiring the programmers to
manually convert (or re-train) their models to integers,
Athos performs the conversion automatically, without
compromising on the inference accuracy.
Modular LLIL. Athos compiles ﬂoating-point HLIL code
to ﬁxed-point code in a low-level intermediate language
(LLIL). LLIL is a C-like imperative language that sup-
ports integer tensors, loops, conditionals, and functions.
LLIL also makes it easier for different cryptographic
backends to be plugged into Athos. It precisely speciﬁes
the interface that it requires the cryptographic protocols to
implement, while providing a library for other operations.
The LLIL is compiled down to the MPC protocol code.
Optimizations. Athos implements MPC speciﬁc optimiza-
tions as well as several standard dataﬂow analyses and
compiler optimizations. The design of HLIL and LLIL,
and the choice of them being statically typed, is partly
motivated by the requirements of these analyses.
Below we explain each of these components in detail.
A. Frontend and HLIL
Athos frontend compiles the input TensorFlow models to
HLIL (described next) with explicit tensor dimensions. To
obtain these dimensions, the frontend ﬁrst runs TensorFlow
code on one dummy input and generates TensorFlow metadata
that has all the required information. The metadata is then
translated to HLIL.
We discuss some details of the frontend. A plain dump
of the TensorFlow metadata contains some nodes that are
semantically irrelevant for actual inference, e.g. identity,
assign, etc. To avoid representing these nodes in HLIL,
we ﬁrst prune the TensorFlow graph to remove such nodes,
speciﬁcally we use the TensorFlow graph transform tool [1]
for this purpose. Next, Athos desugars the remaining (tens
of) TensorFlow nodes to HLIL, while keeping the number
of functions in HLIL as small as possible. TensorFlow also
supports “broadcasting” [76] that allows operations on tensors
of incompatible dimensions and sizes. For example, due to
broadcasting, addition of a four-dimensional tensor with a one-
dimensional tensor is a valid operation. Athos frontend passes
the broadcasting information to HLIL, which then accounts
for it by compiling it to the appropriate LLIL library function
call.
Constant
Float constant
Type
Matrix
Expression
Program
n ::= 0 | 1 | 2 | . . .
r
::= n.n
::= float | int | ˆτ [n]
ˆτ
ˆM ::= r | ˆM
::= n | x | ˆM | ˆe1 ⊕ ˆe2 | x[ˆe]
ˆe
::= void main () {ˆτ x ; f (ˆe)}
ˆp
Fig. 5: HLIL syntax
Figure 5 shows the HLIL (we use r to denote sequences of
ﬂoating-point constants, and similarly for other syntactic cate-
gories). It is a simple language of ﬂoating-point tensors ( ˆM),
with dimensions (n) and sizes as explicit type annotations
(ˆτ [n]), and the main is a sequence of variable declarations
and function calls.
We next discuss how Athos performs ﬂoat-to-ﬁxed conver-
sion on HLIL programs.
B. Float-to-ﬁxed
As observed earlier, most ML models are expressed using
ﬂoating-point, while MPC protocols operate on integers. For
large models, we cannot expect the programmers to manually
translate or re-train ﬂoating-point ML models to integer code
(the common approach in literature on secure inference [62],
[55], [49], [60], [79], [58], [72]). Furthermore, it is well-
known that ﬂoating-point operations are much more inefﬁcient
than ﬁxed-point when evaluated securely ([62], [60]) – we
re-conﬁrm this by performing two-party secure multiplica-
tion [28] using both ﬁxed-point and ﬂoating-point arithmetic
to showcase the difference. This is illustrated in Table I
which shows the huge overheads associated with ﬂoating-point
arithmetic. In future, if efﬁcient protocols for ﬂoating-point
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:36:59 UTC from IEEE Xplore.  Restrictions apply. 
339
become available then we can directly compile HLIL to them,
but until then Athos automatically performs the translation.
The translation is parametrized by a scale parameter s
that determines the precision. We discuss how this scale is
set later in the section. Given a scale s ∈ Z, we deﬁne
a map ρs : R → Z2b
that maps Reals to b-bit integers:
ρs(r) = (cid:4)r · 2s(cid:5). We abuse notation and also apply ρs
to matrices M of Reals where the result
is a point-wise
application of ρs to each matrix element. In the output ﬁxed-
point code, every Real number r is represented by a b-bit
2s .
integer. The Real representation of an integer n is given by n
The ﬂoat-to-ﬁxed conversion (for select cases) is described in
the following algorithm (ScaleDown is described in Table II):
F (MatAdd(A, B, C)) = MatAdd(A, B, C)
F (MatMul(A, B, C)) = MatMul(A, B, C);
ScaleDown(C, s)
As an example of the conversion process, consider the
program M1 ∗ M2 that multiplies the row vector M1 =
[400.1, 200.1] with the column vector M2 = [0.3, 0.1]T .
Then in inﬁnite precision Real arithmetic the result of the
computation 400.1 ∗ 0.3 + 200.1 ∗ 0.1 is 140.04. Single-
precision ﬂoating-point arithmetic with 32 bits only has a 23-
bit mantissa and computes the approximately correct result
140.040009. We use 0.1f to denote the ﬂoating-point number
closest to the Real number 0.1. Given s = 24, F (M1 ∗ M2)
results into the following program over integers
(ρ24(400.1f ) ∗ ρ24(0.3f ) + ρ24(200.1f ) ∗ ρ24(0.1f )) >> 24
which results in the following computation with 64-bit integers
(6712564224 ∗ 3357121024 + 5033165 ∗ 1677721) >> 24
The ﬁnal result is 2349481329 that represents the real number
2349481329
= 140.040000021457672119140625 which is good
approximation of the desired result 140.04. Although it is
feasible to constuct examples where ﬁxed-point computations
can be imprecise, ML usually operates on normalized values
and we have observed that Athos does not lose accuracy in
practice (Table VI).
224
Athos, assigns the same bit-width b and the same scale
s to all network parameters. While we could use different
b and s, our experimental results show that same values for
all parameters works quite well in practice. We keep the scale
public for efﬁciency: division with 2s when s is secret is much
more expensive than when s is public. Moreover, scaling down
operations (division by 2s) cause loss of precision, as they lose
signiﬁcant bits, and hence need to be minimized. Therefore,
Athos scales down only once per matrix multiplication and
does not scale down matrix additions.
While we use machine integer width (64) for b, ﬁnding a
good value of s is difﬁcult. We explain the various tradeoffs
that govern the choice of s and then discuss our solution.
Suppose, in our example, s is set too low: s = 2. Then
F ([400.1f, 200.1f ]∗[0.3f, 0.1f ]) is (1600∗1+800∗0) >> 2,
which represents the Real number 400/4 = 100. This result
Fixed (ms)
Float (ms)
# Sequential Multiplications
Overhead
28.11x
57.1x
126.34x
126.6x
TABLE I: Floating-point vs Fixed-point multiplication.
2.57
4.88
21.65
199.6
1
10
100
1000
72.35
278.8
2735
25281.42
Next, suppose s is set
is far from 140.04. Here, low scale values have lead to loss
of signiﬁcant bits. In particular, 0.1 has been rounded to zero
causing an imprecise result. Ideally we want to set the scale to
a large value so that the integers have many signiﬁcant digits.
to a very high value, e.g., 60.
Then, the computation ρ60(400.1f )∗ ρ60(0.3f ) overﬂows 64-
bit integers and the result is garbage (multiplication of these
two large positive numbers would become a negative number).
Thus, scale can neither be very low nor very high; we need
to ﬁnd a sweet spot. To determine an appropriate value of s, we
sweep over all its possible values {0, 1, . . . , b− 1} and choose
the value that leads to the best accuracy. For the example
400.1f ∗ 0.3f + 200.1f ∗ 0.1f, the most accurate result is
obtained at s = 24. In general, machine learning algorithms
have a validation dataset
is used for hyperparameter
tuning. We consider scale as a hyperparameter and select the
scale that leads to a ﬁxed-point classiﬁer implementation that
performs the best on the validation set. The scale chosen by
Athos is a leakage function that depends on the weights of the
model. Athos gives a methodical way of picking this scale that
prior works did manually. Hence, leakage by Athos is similar
to all prior works on secure inference.
C. Modular LLIL
that
n ::= 0 | 1 | 2 | . . .
τ
Constant
Type
Matrix M ::= n | M
Expression
Statement
Global
Program
::= int | τ [n]
::= n | x | M | e1 ⊕ e2 | x[e]
::= τ x | x = e | for(x = e1; x < e2; x + +){s}
x[e1] = e2 | if(e, s1, s2} | s1; s2 | return e
|
f (e) | d (e)
|
::= extern τ d (τ x) | τ f (τ x){s}
::= g; void main () {s}
e
s
g
p
Fig. 6: LLIL syntax
Athos compiles HLIL to LLIL, a crypto-aware, C-like in-
termediate language that has only integer-valued tensors. Fig-
ure 6 shows the syntax of LLIL. This language has sufﬁcient
expressiveness required to implement ML inference tasks. In
particular it supports arrays, basic arithmetic, loops, branching,
functions, and extern declarations. LLIL makes the Athos
interface to the MPC cryptographic protocols explicit. We
observe that the tensor operations in a typical TensorFlow
code fall into two categories: those that do not change the
values but just copy the data around (e.g. squeeze to remove
dimensions of size 1 from a tensor, pad to pad a tensor
with various kinds of paddings, transpose to take the
transpose of a tensor, and concat to concatenate two tensors
into a single tensor), and those that compute new values.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:36:59 UTC from IEEE Xplore.  Restrictions apply. 
340
For functions that do not manipulate shares (denoted by f),
LLIL provides a library with their implementations that is
automatically added as a prelude to LLIL programs. Changing
the underlying crypto protocol does not require changes to
these library functions and this library can be used by all
crypto developers. These functions are implemented in LLIL
and are compiled to C++ code.
Share-manipulating functions (extern d) are required to
be implemented in the cryptographic backend. All a crypto
developer needs to do is to implement these functions, and
then she would be able to directly evaluate the protocols on
ML models used in practice. We describe these functions with
their signatures and intended semantics in Table II. Concretely,
we provide three implementations of these functions: using the
2PC protocols of ABY [30], 3PC protocols of SecureNN [79],
and Porthos (Section IV).
Finally, Athos compiles LLIL programs to C++ and links
them with the cryptographic MPC protocol implementation.
D. Optimizations
Athos intermediate languages are designed to be amenable
to static analysis. In particular, we have implemented several
standard dataﬂow analyses and compiler optimizations [8]:
reaching deﬁnitions, liveness analysis, and so on. These anal-
yses help with optimizing memory utilization and we have
observed savings reaching up to 80%. To demonstrate the ease
of implementing analyses and optimizations, we provide an
example each: (a) a peephole optimization ReLU MaxPool
Switching on HLIL to improve efﬁciency of DNNs that use
ReLU and MaxPool, and (b) an analysis Counting Scale Down
operations on LLIL to determine the number of scale down
operations done in order to prevent loss in accuracy (a similar
analysis was done manually in [62], [79], [60]).
1) ReLU MaxPool Switching: Most TensorFlow developers