is_new_iban, is_new_iban_cc, is_new_ip, is_new_cc_asn.
amount
amount,
time_x, time_y,
is_national_iban, is_national_asn,
operation_type, conﬁrm_sms
amount,
time_x, time_y,
conﬁrm_sms
out. Based on the information collected, the attacker gener-
ates raw candidate transactions that are aggregated and given
in input to the Oracle. The Oracle classiﬁes each candidate
transaction, and if it labels it as a fraud, we discard it; other-
wise, we use it in the evasion attack by injecting it in the user
banking activity.
Assumptions. Our approach is based on three assumptions:
• Assumption I The attacker has a dataset of banking
transactions for training the Oracle (e.g., an old dataset
belonging to the same or a different bank).
• Assumption II The attacker can retrieve or observe the
transactions carried out by the victim and the funds avail-
ability.
• Assumption III The attacker can execute transactions
on behalf of the victim.
The ﬁrst assumption is necessary because the attacker needs
a bank dataset to train the Oracle. In general, we can state
that if the dataset has been obtained from the same bank
against which the attack is carried out, the attack reaches
better performances. As explained in Section 2, the second
assumption is necessary because, depending on the attacker’s
knowledge, to process a single transaction, it is necessary
to aggregate the previous ones. Hence, the attacker needs to
obtain the victim’s transactions. Moreover, the attacker needs
to know the funds availability and if the victim is making
new transactions during the attack so that he or she is always
up to date and can adequately manage those events. The
last assumption is necessary to ensure that the attacker can
carry out fraud in the real banking system. From a feasibility
point of view: the ﬁrst assumption is more difﬁcult to satisfy
because banks rarely release their data publicly, while the
second and third assumptions can be satisﬁed with a banking
Trojan [14].
5.1 Candidate Transaction Generation
Algorithm 1 Find Timestamps. X is the list of timestamps
in which the user has performed a transaction, F is the list of
time windows sizes, ε is a small time delta greater than zero
used to fall in the time window
1: procedure FINDTIMES(X,F)
2:
3:
4:
5:
6:
7:
8:
9: end procedure
T ← [ ]
for f inF, x inX do
wstart ← x
wend ← wstart + f
T .append(wstart + ε, wend + ε)
(cid:46) T initially empty list
end for
return T
(cid:46) ε < min(F)
To generate candidate transactions, we efﬁciently explore
the space of possible values of each transaction feature. In
truth, the raw candidate transaction features that the attacker
can directly control are only the timestamp and the amount.
We assume that IBAN and IBAN_CC refer to the IBAN of
the malicious recipient (i.e., new IBAN never seen in other
transaction) while the IP and the ASN depend on the attacker
strategy. Therefore, the attacker has to choose the amount
to steal at each transaction. The choice of the value depends
on the strategy chosen by the attacker: conservative or risky.
However, a too high amount would lead the Oracle to classify
the transaction as fraud, a too low amount does not allow the
attacker to maximize the proﬁt. In Section 6, we exhaustively
compare different strategies characterized by high, medium,
USENIX Association
23rd International Symposium on Research in Attacks, Intrusions and Defenses    289
Figure 2: Approach Overview
and low amounts. Once the attacker has selected the amount
to steal, he/she identiﬁes the best moment to perform the
transaction (i.e., timestamp). A naive approach would be to
explore all the possible timestamps within the period in which
the attack occurs, but this would lead to a massive number
of transactions. To avoid this issue, we bucketize (i.e., group)
the timestamps based on the historical transactions using the
algorithm described in Pseudocode 1. This procedure takes
in input the list of selected time windows sizes and the times-
tamps in which the user has performed a transaction – the
attacker has retrieved them from the victim’s historical trans-
actions. Then, for each time-windows size and timestamp, it
builds the aggregated time windows in which the customer is
active. The time windows have the objective of capturing the
user’s short-term, mid-term, and long-term behavior. We look
for the most used sizes in literature [9, 32]: We use one hour
and one day for the short-term, seven days for the mid-term,
and one month for the long-term. The ﬁnal output of this
step is a raw candidate transaction aggregated with previous
transactions and given as input to the classiﬁcation phase.
5.2 Oracles and Fraud Detectors.
In our system, we have two detectors. One is the Bank Fraud
Detector used as a simulation of the system that the attacker
is trying to bypass. The other is the Oracle that the attacker
is using to build fraudulent transactions. Both detectors are
based on some of the most used algorithms in literature [2]
and deployed in banking institutions.
Random Forest. [7] model is an ensemble of decision trees.
It combines the concept of bagging where individual models
in an ensemble are built through sampling with replacement
from the training data, and the random subspace method,
where each tree in an ensemble is built from a random subset
of attributes. Thus, predictions are obtained by aggregating the
outputs from individual trees in the ensemble. Majority voting
is used to determine the prediction outcome (i.e., the label
fraudulent or legitimate). This algorithm, from literature [31]
seems to outperform the other in the ﬁeld of fraud detection.
Neural Networks. They are learning models built of simple
elements called neurons, which take as input a real value, mul-
tiply it by weight, and run it through a non-linear activation
function. By constructing multiple layers of neurons, each of
which receives part of the input variables, and then passes on
its results to the next layers, the network can learn very com-
plex functions. We used the sigmoid function as the activation
function for the output layer in order to use this model for the
classiﬁcation.
Logistic Regression. It is a widely used technique in prob-
lems in which the dependent variable is binary. It computes
the output using a logistic function. Based on a threshold, it
possible to estimate probabilities and classify transactions.
XGBoost. EXtreme Gradient Boosting is based on an ensem-
ble of methods. It trains and predicts using several models at
once to produce a single better output. It exploits the concept
of bagging and boosting to perform the classiﬁcation. It has
achieved excellent results in many domains [13].
Active Learning. It is a variant of AI2 [32] applied to the
banking dataset. It is an active learning approach that com-
bines an ensemble of unsupervised learning (i.e., Autoen-
coder) with supervised learning techniques (Random Forests).
Combining the anomaly scores computed by the unsupervised
models, it ranks transactions and presents them to the analyst
for review; subsequently, the feedback collected is used to
train a Random Forest.
290    23rd International Symposium on Research in Attacks, Intrusions and Defenses
USENIX Association
DataAggregationClassificationUserIDAmountTimestampIBANIBAN_CCIP...AmountCount1dSum1dMean1dCount7d...SameIBANcount1d...PastTransactionsof	the	VictimLegitimateFraudAmountSelectionInjectionBankingDatasetsDataAggregationTrain	OracleOracle	Modelraw candidate fraudaggregatedcandidatefraudraw transactionsUserIDAmountTimestampIBANIBAN_CCIP...AmountCount1dSum1dMean1dCount7d...SameIBANcount1d...aggregated transactionsTraining	phaseRuntime	phaseTimestampSelectionCandidate	Transaction	GenerationBanksealer. [11] It represents one of the systems currently
deployed in the banking institution we collaborated with. It
characterizes the users of the online banking web application
using a local, global, and temporal proﬁle built during a train-
ing phase. Each type of proﬁle extracts different statistical
features from the transaction attributes, according to the type
of model built. It works under semi-supervised settings and,
once, the proﬁles are built, it processes new transactions and
ranks them according to their predicted risk of fraud. The
experiments with this system are particularly insightful, since,
as we will see in Section 6.4, they demonstrate that current
solutions are not ready to “smart” attackers.
The Oracle and the bank fraud detector are characterized by
the machine learning algorithm, the dataset used for training
the model, and the feature extraction method. In Table 3, we
show the complete summary of the Oracle and bank fraud
detector models. We assign to each of them an ID that, from
now on, we use to refer to them. For the hyperparameters
tuning of those models, we use the holdout method, in which
we select the last month of transactions as the validation set
and the other data for the training set. For the models O1-
O2-O3-O4-B1, which are Random Forest models, we use,
respectively, 200, 500, 200, 100, 100 estimators, and a max
depth of 14, 8, 14, 14, 14. The Neural Network (B2) has two
hidden layers with 32 and 16 units and a ReLU (Rectiﬁed
Linear Unit) function as the activation function. Finally, there
is the output layer with a Sigmoid activation function. The
two hidden layers have l2 regularizers with λ = 10−4, the loss
function we use is the binary cross-entropy optimized with
Adam optimizer. The XGB classiﬁer (B3) has 200 estima-
tors and a max depth of 20, with a learning rate of 0.1. The
Logistic Regression classiﬁer (B4) has an l2 regularization
with parameter C = 1
λ = 10. The active learning model (B5)
is based on an AutoEncoder with a single encoding layer with
a size of 25 units; it has a dropout regularization with dropout
rate = 0.2. To validate each model, we use the commonly
used metrics of accuracy, precision, recall, and f1-score (see
Appendix C). To estimate the performances of the detectors
under attack, in Table 4, we show the validation scores of
each model. It is interesting to notice that the results obtained
by Banksealer (B6), currently deployed in a real banking en-
vironment, are the lowest between the considered algorithms.
This is because it was not possible to tune it like other algo-
rithms, but we kept the parameters left by banking analysts,
which tends to distrust from transactions coming from foreign
countries only.
6 Experimental Evaluation
We evaluate our evasive approach against the state-of-the-art
fraud detectors described in Section 5.2 and simulating an
attacker with different degrees of knowledge that perform
attacks by following different strategies.
Table 3: Overview of the Oracle (O1-O4) and detectors mod-
els (B1-B6). For the feature extraction strategies see Table 2
ID DATASET
FEATURES EXTRACTION
ALGORITHMS
O1
O2
O3
O4
B1
B2
B3
B4
B5
B6
2012-13
2014-15
2012-13
2012-13
2014-15
2014-15
2014-15
2014-15
2014-15
2014-15
STRATEGY
Oracle
A
A
B
C
Detectors
B
B
B
B
C
RANDOM FOREST
RANDOM FOREST
RANDOM FOREST
RANDOM FOREST
RANDOM FOREST
NEURAL NETWORK
XGBOOST
LOGISTIC REGRESSION
ACTIVE LEARNING
BANKSEALER [11]
6.1 Attack Scenarios
As described in Section 5.1 the attacker does not have the
complete control of all the features: the beneﬁciary IBAN and
IBAN_CC are ﬁxed (usually a money mule), the IP address
is a national address from which the attacker makes the con-
nection (possibly using a VPN), the Session ID is generated
for each transaction. Therefore, the features that can be fully
manipulated by the attacker are the amount and the timestamp.
The timestamps are selected by using the algorithm described
in the Section 5. Regarding the amount, we set up three dif-
ferent scenarios that represent the strategies that an attacker
could use to choose the amount and the number of frauds to
be committed. In this way, we can compare the results of the
approach against different choices of the amount.
Scenario 1. In this ﬁrst scenario, the attacker has the goal to
execute 20 transactions per user of e 2,500, so the idea is to
do many transactions with a medium-low amount.
Scenario 2. The attacker has the goal to execute 10 transac-
tions per user of e 10,000, so few transactions with medium
amounts are executed.
Scenario 3. The attacker aims to execute 5 transactions per
Table 4: Scores of fraud detectors on validation data
MODEL ID ACCURACY
B1
B2
B3
B4
B5
B6
99.7%
99.5%
99.6%
98.9%
99.5%
98.4%
PRECISION RECALL
96.2%
86.7%
93.6%
46.2%
89.7%
11.5%
70.9%
63.9%
69.0%
34.1%
66.1%
8.5%
F1-SCORE
81.7%
73.6%
79.5%
39.2%
76.1%
9.8%
USENIX Association
23rd International Symposium on Research in Attacks, Intrusions and Defenses    291
Table 5: Summary of all the scenarios
SCENARIO 1
SCENARIO 2
SCENARIO 3
Total Victims
Total Frauds
Frauds per User
Money per Fraud
Money per Victim
80
1600
20
160
1600
10
320
1600
5
e 2,500
e 50,000
e 10,000
e 100,000
e 15,000
e 75,000
user of e 15,000. The attacker wants to steal as much money
as possible in the short term and tries to execute a few trans-
actions with a high amount.
For each scenario, we inject about 1% (1600) of frauds3.
We choose the parameters of the three scenarios to maintain
the same overall number of frauds injected and the same total
amount stolen. A summary of the conﬁguration of the three
scenarios is shown in Table 5.
6.2 Metrics
We evaluate the performance using 4 metrics: Injection Rate,
Evasion Rate, Attack Detection Rate, and Money Stolen.
Injection Rate. It indicates the percentage of frauds that the
attacker carries out against the user in relation to the number
of frauds targeted - i.e., it is the proportion of frauds that the
Oracle classiﬁes as legitimate with respect to the number of
frauds that the attacker wants to perform. This metric depends
on the threshold that we set in the Oracle. The Oracle decides
whether or not a fraud is likely to be uncovered. This metric
is also useful to compare experiments based on the level of
conﬁdence. The lower the classiﬁcation threshold we set, the
lower the injection rate we have. In this way, the risk of fraud
being detected drops within speciﬁed limits.
Evasion rate. It is the percentage of frauds concealed from
the fraud detection system with respect to the frauds carried
out against the user.
Attack Detection Rate.. If we consider an attack as the set
of frauds that the attacker performs against the single user,
the Attack Detection Rate is the percentage of attacks that are
detected by the fraud detection system. Therefore, the Attack
Detection Rate can be seen as the probability that the system
detects the attacker if he or she performs the attack against
one single user.
Money Stolen. It represents the money in euro (e ) that the
bank loses. This metric is signiﬁcant because it is depen-
dent on all three previous metrics and the attack strategy of
the attackers (Scenario). Also, it gives an idea of the mone-
tary impact that these attacks can have on a real bank. With
3The 1% is chosen because it is a reasonable number of transactions that
the bank can inspect manually with its specialized bank analysts.
these metrics, we can capture all the main aspects to com-
pare the different experiments and measure the validity of our
approach.
Deﬁned with N the number of the targeted victims, F the
number of frauds that the attacker wants to perform, K the
amount of money for each fraudulent transaction, Xn the num-
ber of transactions classiﬁed as legitimate by the Oracle, and
Yn with Yn <= Xn the number of transactions classiﬁed as
legitimate by the fraud detector, we can deﬁne the metrics as
follows:
In jection rate = 1/N · ∑n Xn/F
Evasion rate = 1/N · ∑nYn/Xn
Attack Detection Rate = ∑n(Xn −Yn)/N
MoneyStolen = ∑nY n· K
6.3 Experimental Settings
We perform an attack for each scenario and each degree of
knowledge of the attacker. The degree of knowledge of the
attacker are described in Section 3; the scenarios are summa-
rized in Table 5. The combinations of Oracles and detectors
per degree of knowledge are summarized in Table 6. We inject
a number of frauds approximately equal to 1% of transactions
in the dataset, and we use different fraud detectors. In or-
der to choose classiﬁcation thresholds, we consider that the
bank usually has the workforce to inspect about 1% of trans-
actions that are carried out each month. Thus, after sorting
transactions by anomaly score, we classify the ﬁrst 1% of
transactions as fraud and the remaining ones as legitimate.
We train the fraud detectors the dataset 2014-15 using months
from October to January, while February is the one subject to
the evasion attacks. We randomly select the victims, exclud-
ing those users with less than 5 transactions in the dataset. We
perform each experiment 10 times and compute the average
value for each metric in order to have a more reliable estimate
and not biased by the selected users.
Black-Box. The attacker has zero knowledge of the fraud
detector, but he or she has obtained an old bank dataset (2012-
13), different with respect the one used by the fraud detector.
After having retrieved the transactions performed by the vic-
tim in January, he or she uses the model O1 as Oracle to
perform the evasion attack.
Black-Box with Data. The attacker has no knowledge about
the fraud detector and has retrieved the dataset (2014-15)
used for the training both the Oracle and the fraud detector.
Therefore, the attacker uses all transactions of the victim to
train the Oracle, which in this case coincides with model O2.
Gray-Box. The attacker has acquired partial knowledge about