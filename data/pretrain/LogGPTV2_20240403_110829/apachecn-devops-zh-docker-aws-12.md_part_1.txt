# 十二、自动缩放
**弹性**是云计算的基本原则之一，描述了按需自动扩展应用的能力，以确保为客户提供最佳体验和响应能力，同时通过仅在实际需要时为应用提供额外容量来优化成本。
AWS 通过两个关键特性支持扩展使用 ECS 部署的 Docker 应用:
*   **应用自动扩展**:这使用了 AWS 应用自动扩展服务，并支持 ECS 服务级别的自动扩展，其中运行您的 ECS 服务的 ECS 任务或容器的数量可以增加或减少。
*   **EC2 自动缩放**:这使用了 EC2 自动缩放服务，并支持 EC2 自动缩放组级别的自动缩放，您的自动缩放组中的 EC2 实例数量可以增加或减少。在 ECS 的上下文中，您的 EC2 自动扩展组通常对应于 ECS 集群，而单个 EC2 实例对应于 ECS 容器实例，因此 EC2 自动扩展管理您的 ECS 集群的整体容量。
因为这里有两个范例在起作用，所以 Docker 应用的自动扩展目标可能是一个很难理解的技术概念，更不用说以可预测和可靠的方式成功实现了。在撰写本书时，应用自动缩放和 EC2 自动缩放是完全独立的功能，彼此之间不提供集成，这一事实进一步加剧了这种情况，因此，您有责任确保这两个功能相互协同工作。
当分析这些特性时，好消息是应用自动伸缩非常容易理解和实现。借助应用自动扩展，您只需定义应用的关键性能指标，并扩展(增加)或缩减(减少)运行应用的 ECS 任务数量。坏消息是，当 EC2 自动扩展应用于 ECS 集群中的自动扩展 ECS 容器实例时，绝对是一个更难处理的问题。在这里，您需要确保您的 ECS 集群为集群中运行的所有 ECS 任务提供足够的计算、内存和网络资源，并且您需要确保每当应用自动扩展单个 ECS 服务时，您的集群能够增加或减少容量。
Another challenge of scaling ECS clusters is ensuring you do not disrupt service and drain running tasks on an ECS container instance that is about to be removed from the cluster during a scale down/in event. The ECS life cycle hooks solution implemented in Chapter 11 - *Managing the ECS Infrastructure Life Cycle* takes care of this for you, ensuring the ECS container instances are drained of all running tasks before permitting the EC2 auto scaling service to take an instance out of service.
处理扩展您的 ECS 集群资源的问题是本章的主要重点，因为一旦解决，您将能够任意扩展您的每个 ECS 服务，并确保您的 ECS 集群将动态添加或删除 ECS 容器实例，以确保始终有足够的最佳资源用于您的应用。在本章中，我们将首先重点解决 ECS 集群的容量管理问题，然后讨论如何配置 AWS 应用自动扩展服务来自动扩展您的 ECS 服务和应用。
将涵盖以下主题:
*   了解 ECS 集群资源
*   计算电子控制系统集群容量
*   实现 ECS 集群容量管理解决方案
*   配置 CloudWatch 事件以触发容量管理计算
*   发布与 ECS 集群容量相关的自定义云观察指标
*   配置云观察警报和 EC2 自动扩展策略来扩展您的 ECS 集群
*   配置 ECS 应用自动扩展
# 技术要求
下面列出了完成本章的技术要求:
*   对 AWS 帐户的管理员访问权限
*   根据第 3 章中的说明配置本地 AWS 配置文件
*   AWS CLI
*   本章是第 11 章的继续，因此它要求您已经成功完成了第 11 章中定义的所有配置任务。
以下 GitHub URL 包含本章使用的代码示例:[https://GitHub . com/docker-in-AWS/docker-in-AWS/tree/master/ch12](https://github.com/docker-in-aws/docker-in-aws/tree/master/ch12)。
查看以下视频，了解《行动守则》:
[http://bit.ly/2PdgtPr](http://bit.ly/2PdgtPr)
# 了解 ECS 集群资源
在开始管理您的 ECS 集群的容量之前，您需要对影响 ECS 集群容量的各种资源有一个清晰而扎实的了解。
一般来说，您需要考虑三个关键资源:
*   中央处理器
*   记忆
*   网络
# 中央处理器资源
**CPU** 是 Docker 作为一等公民支持和管理的核心资源。ECS 利用了 Docker 的 CPU 资源管理功能，并公开了通过您的 ECS 任务定义来管理这些资源的能力。ECS 用 *CPU 单位*来定义 CPU 资源，单个 CPU 核包含 1024 个 CPU 单位。当您配置 ECS 任务定义时，您指定了一个 CPU 预留，它定义了每当有 CPU 时间争用时，将分配给应用多少 CPU 时间。
请注意，中央处理器预留并不限制中央处理器任务可以使用多少中央处理器——每个中央处理器任务都可以自由爆发并使用所有可用的中央处理器资源——该预留仅在中央处理器存在争用时应用，Docker 会根据每个运行中的中央处理器任务的配置预留来公平分配中央处理器时间。
重要的是要理解，每个 CPU 预留都是从给定 ECS 容器实例的可用 CPU 容量中推导出来的。例如，如果您的 ECS 容器实例有 2 个 CPU 内核，那就相当于总共有 2，048 个 CPU 单元。如果您运行 3 个配置有 500、600 和 700 个 CPU 单元的 ECS 任务，这意味着您的 ECS 容器实例有 2，048 - (500 + 600 + 700)或 248 个 CPU 单元可用。请注意，每当 ECS 调度程序需要运行新的 ECS 任务时，它将始终确保目标 ECS 容器实例有足够的 CPU 容量来运行任务。根据前面的示例，如果需要启动一个保留 400 个 CPU 单元的新 ECS 任务，那么剩余 248 个 CPU 单元的 ECS 容器实例将不会被考虑，因为它当前没有足够的 CPU 资源可用:
![](img/a5a2aea8-a27b-4ae9-baa9-7e542ad03403.png)
Allocating CPU resources
在配置中央处理器预留方面，您已经学习了如何通过云信息实现这一点–参见第 8 章- *中的*使用云信息定义 ECS 任务定义*示例使用 ECS* 部署应用，其中您通过名为`Cpu`的属性为 todobackend 容器定义分配了值 245。
# 内存资源
**内存**是通过 Docker 管理的另一个基本资源，其工作方式与 CPU 类似，尽管您可以为给定的 ECS 任务保留和限制内存容量，而在管理 CPU 容量时，您只能保留(而不是限制)CPU 资源。在为 ECS 任务配置内存时，这种限制内存的额外能力会导致三种情况:
*   **仅内存预留**:这个场景的行为与中央处理器预留的工作方式相同。Docker 将从 ECS 容器实例的可用内存中扣除已配置的预留，并尝试在内存出现争用时分配此内存量。ECS 将允许 ECS 任务最多使用 ECS 容器实例支持的最大内存量。使用 ECS 任务容器定义中的`MemoryReservation`属性配置内存预留。
*   **内存预留+限制**:在这个场景中，内存预留和前面的场景一样工作，但是，ECS 任务可以使用的最大内存量受到配置内存限制的限制。通常，配置内存预留和内存限制被认为是最佳选择。使用 ECS 任务容器定义中的`Memory`属性配置内存限制。
*   **仅限内存限制**:这里，ECS 将内存预留和内存限制值视为同一个，也就是说 ECS 会从可用的 ECS 容器实例内存中扣除配置的内存限制，也将内存使用限制在同一个限制内。
配置内存预留和限制很简单–如果您参考第 8 章- *使用 ECS 部署应用*的*使用云信息*定义 ECS 任务定义一节，您可以看到您配置了`MemoryReservation`属性来配置 395 MB 的预留。如果您想要配置内存限制，您还需要使用适当的最大限制值来配置`Memory`属性。
# 网络资源
中央处理器和内存是典型且显而易见的资源，您可以期望您的 ECS 集群来控制和管理它们。一组不太明显的资源是*网络资源*，可以分为两类:
*   **主机网络端口**:每当您为 ECS 服务配置静态端口映射时，主机网络端口都是您需要考虑的资源。原因是静态端口映射使用由 ECS 容器实例公开的公共端口–例如，如果您创建了一个带有静态端口映射的 ECS 任务，该静态端口映射公开了给定应用的端口 80，则您将无法在同一 ECS 容器实例主机上部署 ECS 任务的另一个实例，因为给定的端口 80 仍在使用中。
*   **主机网络接口**:如果您正在使用 ECS 任务联网，请务必了解，该功能目前要求您为每个 ECS 任务实现一个弹性网络接口(ENI)。因为 EC2 实例对于每个实例类型可以支持的 ENi 数量有有限的限制，所以配置了可以支持的 ECS 任务网络的 ECS 任务数量将被限制在 ECS 容器实例可以支持的最大 ENi 数量。
# 计算电子控制系统集群容量
在计算 ECS 群集容量之前，您需要清楚了解哪些资源会影响容量，以及如何计算每个资源的当前容量。一旦为每个单独的资源定义了这一点，就需要对所有资源应用聚合计算，这将导致当前能力的最终计算。
计算容量似乎是一项相当艰巨的任务，尤其是当您考虑不同类型的资源及其行为方式时:
*   **CPU** :这是你可以使用的最简单的资源，因为每个 CPU 预留量只是从集群的可用 CPU 容量中扣除。
*   **内存:**根据内存计算集群的当前容量与中央处理器相同，因为内存预留是从集群的可用内存容量中扣除的。根据我们在本章前面的讨论，如何配置内存预留由于内存限制和内存预留的各种排列而变得复杂，然而从根本上来说，一旦确定了内存预留，计算就和 CPU 资源一样了。
*   **静态网络端口**:如果您的 ECS 集群需要支持*任何使用静态端口映射的*容器，那么您需要将您的 ECS 容器实例网络端口视为一种资源。例如，如果容器应用总是使用 ECS 容器实例上的端口 80，那么每个实例只能部署一个容器，而不管该实例可能拥有多少 CPU、内存或其他资源。
*   **网络接口**:如果您有任何为 ECS 任务联网而配置的 ECS 服务或任务，请务必了解您目前每个网络接口只能运行一个 ECS 任务。例如，如果您正在运行一个 t2.micro 实例，这意味着在给定一个 t2.micro 的情况下，每个实例只能运行一个启用了任务网络的 ECS 任务，并且只能支持一个用于 ECS 任务网络的弹性网络接口。
Given the sample application is not using ECS task networking and is being deployed using dynamic port mapping, we will only consider CPU and memory resources for the remainder of this chapter. If you are interested in an example solution that incorporates static network ports, check out the Auto Scaling ECS Applications module of my [Docker in Production Using Amazon Web Services](https://www.pluralsight.com/courses/docker-production-using-amazon-web-services) course.
这里的挑战是如何根据前面的所有考虑因素来考虑您的所有 ECS 服务和任务，然后决定何时应该增加或减少集群中的实例数量。我见过的一种常见且有些幼稚的方法是独立对待每种资源，并相应地扩展实例。例如，一旦集群的内存容量用完，就会添加一个新的容器实例，同样，如果集群的 CPU 容量即将用完，也会添加一个新的容器实例。如果您纯粹考虑横向扩展的能力，这种方法可以很好地工作，但是当您想要在集群中进行扩展时，它就不起作用了。如果仅根据当前的内存容量来扩展集群，则在 CPU 容量方面有扩展过快的风险，因为如果从集群中删除一个实例，集群可能没有足够的 CPU 容量。
这可能会让您的群集陷入自动扩展循环，也就是说，您的群集不断地向外扩展，然后再向内扩展，这是因为单个资源容量独立地推动了向外扩展和向外扩展的决策，而没有考虑对其他资源的影响。
解决这一挑战的关键是，您需要做出横向扩展或纵向扩展的*单一*决策，并考虑集群的所有*适用资源。这可能会让整个问题看起来更难解决，但实际上它很简单。解决方案的关键是你总是考虑*最坏的情况*，并在此基础上做出决定。例如，如果集群中有足够的 CPU 和内存容量，但是给定端口的所有静态端口映射都在所有集群实例上使用，最坏的情况是，如果您在集群中进行扩展并删除一个实例，您将无法再支持当前使用受影响的静态端口映射的 ECS 任务。因此，这里的决定很简单，完全基于最坏的情况——所有其他情况都被忽略。*
# 计算容器容量
计算集群容量时的一个关键考虑因素是，您需要对资源容量计算进行标准化，这样每个资源的容量就可以用一种通用且等效的格式来表示，而与每个单独资源的特定度量单位无关。这对于做出考虑所有资源的集体决策至关重要，一种自然的方法是根据使用当前可用的未分配资源可以支持的额外 ECS 任务的数量来表示资源容量。此外，与最坏情况场景的主题保持一致，您不需要考虑您需要支持的所有不同的 ECS 任务–您只需要考虑对于您当前正在计算容量的资源而言最糟糕的 ECS 任务(需要最多资源的任务)。
例如，如果您有两个 ECS 任务，分别需要 200 个 CPU 单元和 400 个 CPU 单元，那么您只需要根据具有 400 个 CPU 单元的 ECS 任务来计算 CPU 容量:
![](img/dd9bfcd2-98df-4a3e-9140-462f6575f3a0.png)
The expression with the somewhat strange upside down A in the formula means "for each taskCpu value in a given set of taskDefinitions".
一旦您确定了需要支持的最坏情况的 ECS 任务，您就可以继续计算集群当前可以支持的额外 ECS 任务的数量。鉴于最坏情况下的 ECS 任务需要 400 个 CPU 单元，如果我们现在假设您的群集中有两个实例，每个实例都有 600 个 CPU 单元的可用容量，这意味着您目前可以支持另外两个 ECS 任务:
![](img/43729108-b796-419e-9e92-32de63550246.png)
Calculating container capacity
这里需要注意的是，您需要在每个实例的基础上进行计算，而不仅仅是在整个集群中进行计算。使用前面的示例，如果您考虑整个集群中的空闲 CPU 容量，您有 1，200 个可用 CPU 单元，因此您将计算三个 ECS 任务的空闲容量，然而现实是您不能*将*ECS 任务分割到两个实例中，因此如果您考虑每个实例的空闲容量，显然您只能在每个实例上支持一个额外的 ECS 任务，从而在集群中正确地总共支持两个额外的 ECS 任务。
这可以形式化为一个数学方程，如下所示，公式右侧的![](img/1a7ada8d-43a9-4c10-a5bc-be9a6ea6f917.png)标注表示取*楼*或计算的最低最近整数值，表示集群中的一个实例:
![](img/f68b3eb6-74a7-4799-94d9-feaa134f7d59.png)
如果对内存资源重复前面的方法，您将计算一个单独的计算，根据内存定义集群的当前备用容量。如果我们假设内存的最坏情况 ECS 任务需要 500 MB 的内存，并且两个实例都有 400 MB 的可用内存，那么很明显，就内存而言，集群当前没有备用容量:
![](img/bbf249ea-73f3-449a-b9bd-5539dcbf50d3.png)
如果您现在考虑前面针对 CPU(当前两个空闲 ECS 任务)和内存(当前零个 ECS 任务)的两个计算，很明显，最坏的情况是零个空闲 ECS 任务的内存容量计算，可以形式化为:
![](img/adf03873-dfc3-4bee-b660-ed1316ce93a2.png)
请注意，虽然我们没有合并静态网络端口和网络接口的计算来帮助简化我们的解决方案，但一般方法是相同的-计算每个实例的当前容量和总和，以获得资源的总体群集容量值，然后将该值合并到总体群集容量计算中:
![](img/72628df2-87bc-4414-a63f-3588709b546c.png)
# 决定何时横向扩展
此时，我们已经确定您需要评估集群中的每个当前资源容量，用集群当前可以支持的空闲或备用 ECS 任务数量来表示，然后使用最坏情况计算(最小值)来确定您当前的总体集群容量。完成计算后，您需要决定是横向扩展群集，还是保持当前群集容量不变。当然，您还需要决定何时在集群中扩展，但是我们将很快单独讨论这个主题。
现在，我们将重点关注是否应该扩展*集群(即增加容量)，因为这是更简单的评估场景。这里的规则是，只要您当前的群集容量小于 1，您至少应该横向扩展群集:*
 *![](img/58383abe-4357-4cb0-924c-090eac36d6f7.png)
换句话说，如果您的集群中当前没有足够的容量来支持一个以上的*最坏情况* *场景* ECS 任务，您应该向 ECS 集群添加一个新实例。这是有意义的，因为您试图确保您的集群在新的 ECS 任务启动时始终有足够的容量。当然，如果您想要更多的可用容量，可以将此阈值提高，这可能适用于容器经常上下旋转的更动态的环境。
# 计算空闲主机容量
如果我们现在考虑放大场景，这就变得有点难以确定了。我们讨论的备用 ECS 任务容量计算是相关的，也是必需的，但是您需要从这些方面考虑:如果您从集群中删除了一个 ECS 容器实例，是否有足够的容量用于所有当前正在运行的 ECS 任务以及至少一个附加 ECS 任务的备用容量？另一种表达方式是计算集群的*空闲主机容量*-如果集群中有超过 1.0 台主机空闲，则可以安全地在集群中扩展，因为减去一台主机将导致剩余的非零正容量。请注意，我们指的是整个集群中的空闲主机容量，因此请将其视为虚拟主机计算，因为您可能不会有完全空闲的主机。这种虚拟主机计算是安全的，因为如果我们真的从集群中删除了一个主机，生命周期挂钩和 ECS 容器实例-我们之前在第 11 章- *中介绍的排空功能管理 ECS 基础架构生命周期*将确保在要删除的实例上运行的任何容器都将迁移到集群中的其他实例。
了解空闲主机容量必须大于 1.0 且不等于 1.0 也很重要，因为您必须有足够的备用容量来执行一项 ECS 任务，否则您将触发横向扩展操作，从而导致自动横向扩展/纵向扩展循环。
要确定当前的空闲主机容量，我们需要了解以下内容:
*   每个 ECS 容器实例可以为每种不同类型的 ECS 资源运行的最大 ECS 任务数(表示为![](img/ded39ac8-2846-4051-8be4-ce102e7957d4.png))。
*   整个集群中每种类型的 ECS 资源的当前可用容量(表示为![](img/848fe089-3774-4825-8c43-ef5ea0ac8539.png))，我们在确定是否横向扩展时已经计算过了。
利用这些信息，您可以计算给定资源的空闲主机容量，如下所示:
![](img/d6c9883f-b371-4841-9fd4-7e37766be0fc.png)
# 空闲主机容量示例
为了更清楚地说明这一点，我们来看一个例子，如下图所示，首先假设如下:
*   最坏情况下，ECS 任务需要 400 个中央处理器
*   最坏情况下，ECS 任务内存需要 200 MB
*   每个 ECS 容器实例最多支持 1，000 个 CPU 单元和 1，000 MB 内存
*   当前在 ECS 群集中的两个 ECS 容器实例
*   每个 ECS 容器实例目前有 600 个 CPU 单元的备用容量。使用前面讨论的可用容量计算，这相当于两个群集的当前可用容量
*   ECS 在 CPU 资源方面的任务，我们将称之为![](img/fe5db803-ec6f-4713-9618-9e2eac790629.png)。
*   每个 ECS 容器实例目前都有 800 MB 的备用容量。使用前面讨论的可用容量计算，这相当于内存资源方面的八个 ECS 任务集群的当前可用容量，我们将其称为![](img/d214eb41-2e71-45da-babb-68b187099957.png):
![](img/6f13b876-6d57-4671-8b33-368c9854c33a.png)
Idle host capacity
我们可以首先计算![](img/ab3ad606-7126-402a-8bc2-17b6ac2916d3.png)值，如下所示:
![](img/a4a2e4f2-fc90-47ca-8260-a8a8a86f925b.png)
对于 CPU，它等于值 2，对于内存，它等于值 *5* :
![](img/3d4d598d-9e35-4bfb-b62d-e92855b344d6.png)
![](img/761d58d4-9b6c-46b0-82c9-7781d9f2987c.png)