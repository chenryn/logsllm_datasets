Number of user input ﬁelds
Presence of indicative images
Website content
Third-party script ratio
Link length mean
Page redirection
Web structure
Link length max
Image size mean
Third-party request ratio
Third-party response ratio
Number of frames
Image size max
Ordinal
Ordinal
Ordinal
Continuous
Continuous
Continuous
Categorical
Categorical
Ordinal
Ordinal
Continuous
Continuous
Ordinal
Ordinal
100%
83.2%
65.4%
61.5%
33.6%
28.5%
27.3%
22.3%
11.5%
8.3%
6.9%
6.1%
5.7%
3.3%
TABLE III: Ranking of feature importance in SURVEYLANCE
(C for Content-based, T for Trafﬁc-based, and I for Image-
based category).
3) Classiﬁcation Evasion: Similar to other defense mech-
anisms, adversaries may attempt to evade SURVEYLANCE. To
this end, we evaluated SURVEYLANCE’s performance under
different evasion scenarios by excluding the corresponding
features from the detection model. The results of the analysis
are shown in Figure 4. The green curve represents the ROC
curve of SURVEYLANCE which incorporates all the features
into the detection model. The velvet curve exhibits the ROC
curve if adversaries evade the trafﬁc-based features, rank 6, 7,
9, 11, 12 in Table III. Excluding this feature set from the de-
tection model is a reasonable assumption, as an adversary may
avoid embedding third-party scripts in the survey gateways to
evade SURVEYLANCE at the cost of not making any revenues
from third-parties. As shown, SURVEYLANCE’s performance
degrades, but it still achieves a relatively high level of detection
accuracy. This analysis suggests that the trafﬁc-based features
are important, but the system still achieves good detection
results in the absence of trafﬁc-based features.
We next considered excluding image-based features (rank
3, 11, 15) with the assumption that an adversary completely
77
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:37:19 UTC from IEEE Xplore.  Restrictions apply. 
Survey Gateways
Seeds
Guided Search (Candidate URLs)
URLs Classiﬁed as Survey Gateways
Unique Domains
False Positive Rate
Detection Rate
Survey Publishers
Unique Domains
URLs classiﬁed as Survey Publishers
Survey Completed
(#)
700
2,301,733
54,938
8,623
1.2%
94.8%
(#)
19,123
318,219
131,277
TABLE IV: The number of survey gateways and publishers
we observed in the large-scale experiment.
positive cases. Accordingly, we inspected the screenshots of
pages that were detected as survey gateways. These screenshots
were captured in the data collection phase during our crawling
process. To verify the results, we wrote a script to programmat-
ically open the screenshots of the pages that were detected as
survey gateways, and one of the authors manually checked the
screenshots to see whether the corresponding page is in fact a
survey gateway or not. The entire process to validate all the
8,728 detected survey gateways, from automatically loading
each image, checking the content of the screenshot of the page
to see whether it is correctly identiﬁed as a survey gateway,
to closing the image took approximately 17 hours of work
(7 seconds per image). We conﬁrmed that SURVEYLANCE
correctly reported 8,623 out of 8,728 detected survey gateways.
Therefore, SURVEYLANCE achieved a false positive rate (FPs)
of 1.2% (105 false detections out of 8,728 reported cases).
Our further analysis revealed that all those cases were parked
domains that included on average 17 third-party inclusions.
These websites were assigned a high similarity score mainly
because their HTTP network trafﬁc was very similar to survey
gateways.
Our results also show that adversaries follow very similar
techniques to create online survey scams. More particularly, in
order to be successful, adversaries inevitably need to frequently
use inviting content or images to encourage users to take
part in such fraudulent activities. SURVEYLANCE uses these
limitations for survey scam defense purposes, and utilizes
features (e.g. content-based, image-based features) that are
speciﬁcally deﬁned to detect these traits.
2) Evaluating False Negatives: Determining an accurate
analysis on false negative cases is also a challenge since
manually checking 2,301,733 URLs is not a feasible task. In
the following paragraphs, we provide an approximation of false
negatives for SURVEYLANCE.
In our experiments, false negative cases occur when a
URL is, indeed, a survey gateway, but SURVEYLANCE fails
to identify it as a malicious case. To reduce the manual effort
of analyzing the false negative cases, we deﬁned a semi-
automated approach to pre-ﬁlter a large number of less relevant
cases, and checked only the cases that were more likely to be
false negatives. To this end, we created 6 clusters of survey
gateways using our labeled dataset based on the similarity of
their content, and ordered the words in each cluster based on
their usage frequency. We selected the 10 most common words
in each cluster as they were discriminative enough to correctly
determine to which cluster a page belonged. As mentioned
earlier, survey gateways do not usually have a large volume
of text, and a large fraction of visible text in these websites
is to lure users to take part in completing a survey. Given
that, if a URL is a survey gateway, it should contain some
degree of content similarity to the constructed clusters. We
automatically computed the content similarity between a given
page and the generated clusters by calculating the cosine of
the angle produced by the word sets of the page and the
clusters. The cosine similarity measure is a proven technique to
model the frequency of words in a document using the Vector
Space Model (VSM) [34]. This technique is frequently used
in document indexing [39] and information retrieval [31]. In
cosine similarity, if the content of two websites share exactly
the same tokens, the angle will be 0, and the similarity score
becomes 1. When the source HTML of two websites do
not share any token, the angle becomes orthogonal, and the
similarity will be 0.
We empirically observed that the cosine similarity score
of 100% of the detected survey gateways (true positive cases)
in the second experiment was more than 0.53. Therefore, to
identify false negative cases, we had to verify all the cases
that were assigned a similarity score of less than 0.53. Thus,
we had to manually verify thousands of pages to measure
the false negative cases. To narrow our analysis, we used the
imbalanced labeled dataset to empirically approximate a score
range which was more likely to contain false negative cases.
We observed that the pages with similarity scores of less than
0.3 were very unlikely to be survey gateways. In fact, we
observed that the pages with similarity scores less than 0.3
were very unlikely to contain input ﬁelds that required any
information from visiting users similar to survey gateways.
Therefore, we narrowed our analysis to the cases that had the
similarity score between [0.3, 0.53) in our large-scale exper-
iment. This approach decreased the size of potential survey
gateways that were not detected by SURVEYLANCE to 3,013
cases. We checked the screenshots of these pages, and found
323 undetected survey gateways. Our further analysis showed
that
these URLs contained indicative images that we had
not observed in our training phase. Therefore, the Hamming
distance of the perceptual hash value of those images was
between [0.19, 0.24), which was more than the experimentally-
derived threshold. In Section VII, we provide more details on
the limitations of SURVEYLANCE. We conclude that the system
can enhance the protection capabilities against this class of
social engineering attacks (with a true positive rate of 94.8%
and a false positive rate of 1.2%).
3) Re-training the Model: For a real-world deployment,
similar to other techniques, SURVEYLANCE requires tuning to
be able to pick up on new trends, as the underlying measured
scam phenomena will highly likely evolve over time. There-
fore, a practical deployment of the system requires periodic
re-training. To simulate a practical deployment, we started the
experiment with the balanced dataset (set A), and varied the
length of the testing period to determine how often we need
to re-train the model to keep the detection rate constantly
high, with under a 1% false positive rate. Unsurprisingly,
less frequent re-training (i.e., longer testing period) resulted
in less accurate detection. However, our analyses show that,
based on the labeled dataset and subsequent data collection,
training SURVEYLANCE with a dataset similar to set A and re-
78
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:37:19 UTC from IEEE Xplore.  Restrictions apply. 
Rank
Entry Point
Percentage
1
2
3
4
5
6
7
8
9
sitescout.com
onlickads.net
spotxchange.com
adcash.com
doubleclick.net
stickyads.com
adform.com
propellerads.com
adify.com
11.2%
10.7%
10.2%
8.8%
8%
7.8%
6.3%
6.1%
4.3%
TABLE V: The list of top advertisers that redirected users to
survey gateways. More than 70% of the survey gateways in
our dataset were reachable from these advertisers.
Fig. 5: The reachability of survey gateways from top websites.
More than 40% of the survey gateways in our dataset were
reachable from the Alexa top 30K websites.
managed? To answer this question, we obtained access to
WHOIS records of .com and .net domains through a well-
known domain registration authority. During the period that
we had access, we were able to examine WHOIS records for
22,453 websites, including 2,421 survey gateways and 20,032
survey publishers. We extracted the registrant, administrator,
and technical contact details from the WHOIS records and
created domain clusters that contained similar registrant name,
email address, and organizations using the Levenshtein dis-
tance. Among the records considered in this experiment, 1,098
domains did not contain an email address in their WHOIS
records. Furthermore, 5,845 domains used anonymous WHOIS
services, which prevented further analysis.
The remaining 15,510 WHOIS records were clustered into
388 groups, where 85% of the clusters had at least 24 domains
with very similar WHOIS records. We found 2,721 domains
that did not have identical WHOIS records. However, these
domain names satisﬁed 12 different regular expressions that
we deﬁned for similarity checks, suggesting that they were
registered by the same identities. For example, the contact
email of 881139.com was PI:EMAIL, whereas the
contact email of 331655.com was 406954261@qq.com. Com-
puting the Levenshtein distance of the email addresses was not
very useful in catching scenarios similar to this. However, they
satisfy the same regular expression, i.e., ∧[0 − 9]{6}$ while
being resolved to the same network address.
Our analysis of the clusters shows that 11% of survey
gateways, which expose users to thousands of survey publish-
10
training every 12 days was sufﬁcient to maintain the detection
rate over 93%. This means that every 12 days, we veriﬁed
the detection results using the procedures we explained in
Sections V-C1 and V-C2 to identify the false positive and
false negative cases, and re-train the detection model. The re-
training process, including the false positive and false negative
analysis, usually took on average 4.5 hours each time over the
course of experiment. Note that, since the required re-training
periods may vary across different datasets, additional analysis
should be performed by testing different re-training periods
when SURVEYLANCE runs on a new dataset.
VI. ANALYSIS OF SURVEY SCAM SERVICES
In this section, we use the detected survey gateways in our
large-scale analysis, and provide insights into the interaction
between survey gateways and publishers, as well as the in-
frastructure used by perpetrators of survey scams. Then, we
discuss techniques to inspect and identify possible abuses in
these websites including deceptive advertisements, threats such
as malicious code, and other fraudulent activities.
A. Reachability of Survey Gateways
To better understand online survey attacks, we study reach-
ability of survey gateways in these attacks by analyzing the
redirection chains extracted during our data collection phase.
The reachability path of survey gateways can be viewed as
a set of nodes that constitute a path starting from the ﬁrst
advertisement link and ending at the survey gateway. The
deﬁned chain simply illustrates the sequence of URLs followed
by the victims to arrive to survey gateways. Table V shows the
list of top advertisers that redirected users to survey gateways
when clicked on the ads (i.e., the ﬁrst domain on the redi-
rection chain). Some of the advertisers such as adcash.com
have been abused by adware in the past. We also observed
doubleclick.net in 8% of the redirection chains. This
is likely due to the fact that the majority of survey gateways
look less aggressive or even suspicious compared to classic
types of web-based social engineering attacks such as phishing
websites for which blacklist operators utilize more mature
techniques to detect. To extend our reachability analysis, we
checked the reputation of the ﬁrst node of each redirection
chain. The analysis revealed that more than 40% of the survey
gateways in our dataset were reachable from the Alexa top
30K. Figure 5 illustrates the distribution of the detected survey
gateways among top websites.
In addition to performing an analysis of the ﬁrst node in
the redirection chain, we also analyzed the ﬁnal node which is,
in fact, the survey gateway. Table VI exhibits the most popular
survey gateways we observed in our experiments. Over 40%
of the survey gateways redirected users to survey publishers
that encouraged victims to download legitimate software that
was bundled with adware or PUPs. We provide more details
on potential threats that typical users may be exposed to in
Section VI-C. We performed another analysis by interacting
with survey gateways to infer potential information ﬂow to
survey gateways. Please refer to Appendix B for further details.
B. Survey Scam Domain Owners
A question that arises is: who registers the domain names
to operate survey scam services, and how are these websites
79
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:37:19 UTC from IEEE Xplore.  Restrictions apply. 