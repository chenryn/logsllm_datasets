type of behavior would likely be regardless of the feature set
used or increased time distance between sessions.
3.3 Lessons Learned
The previous subsections have shown that error distribu-
tions vary wildly across diﬀerent datasets. This observation
is valid for both the FRR and the FAR, leading to diﬀerent
consequences. Out of the set of the metrics we analysed
to augment the FAR/FRR the Gini Coeﬃcient is the most
promising due to its compactness and ability to provide an
absolute ordering of systems. For the FAR, systems with a
lower GC are desirable as this indicates false accepts that
are spread relatively evenly across attackers, rather than
enabling few attackers to perpetually escape detection. Our
data shows that adding distinctive features, such as the
pupil diameter for eye movement biometric decreases the
EER, but at the same time increases the GC. This suggests
393(a)
(b)
Figure 8: The diﬀerent Gini coeﬃcients draw attention to diﬀerent error distributions. The touch biomet-
ric has a comparatively low GC of 0.75, which indicates largely random errors, while the eye movement
biometric’s higher GC of 0.94 suggests systematic errors which will lead to attackers consistently fooling
detection.
that features that change little over the system’s operation
might be suitable to tell users apart in general, but confuses
similar users more consistently, thereby leading to the afore-
mentioned systematic errors. This insight is crucial during
feature selection, at which point some distinctive features
should even be dropped completely to avoid this scenario. As
such, it is important to remember that not every change to
a system that lowers the average error is actually beneﬁcial
to its security. For the FRR a high GC indicates erratic
user behavior for a small number of users, an insight that
can help improve either the system design or aid in avoiding
this behavior during system operation. Overall, we recom-
mend to closely monitor changes to GC when experimenting
with diﬀerent feature sets to evaluate whether any of them
consistently lead to systematic errors. When publishing re-
sults, the GC should always be reported together with the
mean EER/FAR/FRR in order to allow readers to take error
distributions into account during their evaluation.
4.
INFLUENCING ERROR RATES
THROUGH TRAINING DATA SELECTION
In Section 2 we have shown that the majority of papers ei-
ther randomly sample training data from the entire available
dataset or merge data from all users (including the attacker)
to form the negative class. It is well-known in related ﬁelds
that error rates are systematically under-estimated when the
temporal order of samples is not preserved when selecting
training data. The precise impact is well-researched in the
context of malware analysis, in which case past malware
can be classiﬁed more accurately when signatures of future
malware is included in the training data [3, 4]. Nevertheless,
the precise impact has, to the best of our knowledge, not
been quantiﬁed for biometric-based continuous authentica-
tion. Knowing the precise inﬂuence of these methodologies
is important in order to assess whether a lower EER is due
to a better system or excessive optimization through non-
functional design decisions.
4.1 Quantifying the Impact on the EER
The two non-functional parameters most likely to impact
error rates are the attacker modelling process and the division
of training data. There are a number of valid choices for
both, raising the question whether there is a seemingly “best”
choice that leads to a minimization of (reported) error rates.
In order to answer this question, we compute the EER for a
number of datasets under diﬀerent assumptions. We consider
all combinations of the below parameters:
Number of aggregated samples: We statically choose a value
of 100 for eye movement datasets, and 15 for the others (to
reﬂect the lower sampling rate). We then aggregate samples
based on a simple majority voting. Aggregating samples
is common practice and a technique used in the original
evaluation of all datasets we consider.
Dataset Division: We consider ordered and random divi-
sion. For a single session, an ordered split uses the ﬁrst half
for training and the second half for testing. If two sessions
are available, only the ﬁrst is used for training. For the
random split, we randomly select half the data for training.
The process maintains the relative proportions of the classes
to ensure roughly equal amounts of training data for each
user. We then repeat the sampling and classiﬁcation process
20 times to measure the eﬀects of this selection.
Attacker Modelling: Anomaly detection requires a spe-
cialized classiﬁer (such as a oneclass SVM), which makes it
diﬃcult to isolate the eﬀects of this parameter alone. As such,
we consider the ”all users” and ”except attacker” approaches.
For the latter, we perform classiﬁer training separately for
each user and each attacker, while excluding the attacker
from the training set. The negative class is instead created by
the combination of all other users. The ”all users” approach
instead trains a single model per user, which includes positive
data (from the legitimate user) and a single negative class
(all other users). In both cases, we balance the positive and
negative class as to not bias the classiﬁer.
The results of our analysis are shown in Figure 9. Selecting
training data randomly provides the biggest improvement,
394(a) Data Division
(b) Attacker Model
Figure 9: EERs decrease up to 80% when randomly selecting training data. Including the actual attacker
in the negative class provides a reduction of up to 63%. The impact of random training data selection is
particularly strong for datasets collected over longer time spans.
relative to the original EER. This eﬀect is particularly pro-
nounced for datasets that are collected over larger timespans
(such as the inter-session and 2-weeks eye movement datasets).
This strong eﬀect is most likely due to the classiﬁer being
unable to observe and account for any changes in user behav-
ior over time, leading to underﬁtting when considering the
dataset over the entire time period. The mouse movement
datasets, which are collected over a short period, are only
marginally aﬀected, which further supports this explanation.
Another interesting insight is that the EER varies extremely,
depending on the training data selection. This suggests that
the training and testing process has to be repeated a number
of times to ensure statistical robustness of the result. The
distribution of errors was virtually unaﬀected by the change,
which suggests that it mainly leads to shifting the mean.
The eﬀects of the two diﬀerent attacker models signiﬁcant,
albeit less extreme than those of the training set selection.
Across all datasets, including the attacker in the training
data results in a relative improvement between 22% (mouse
movements) and 63% (intra-session eye movements). It is
somewhat counter-intuitive that the eﬀect is bigger for the
larger datasets, even though the attacker data only accounts
for a smaller fraction of the overall negative class.
These results show that simply looking at the EER of a
proposed system is insuﬃcient, as it is skewed greatly by non-
functional parameters that would not aﬀect the performance
of the system in a production environment. For example,
if the exact same dataset (i.e., identical features and classi-
ﬁers) were evaluated with random and ordered training data
selection, one might favor one over the other (even though
their practical performance would be identical). This is par-
ticularly alarming as our analysis (see Section 2) shows that
out of 25 papers, 13 use at least one of the methodologies
that we have shown to lead to systematic underestimation
of error rates. In addition, a further 6 do not report how
the error rates were obtained, which not only decreases con-
ﬁdence in the results but also impedes reproducing them
and comparing them to related work. In order to inspire
the highest conﬁdence in their results researchers should ex-
clude attackers from the negative class in their training data
and choose the ﬁrst part of their entire dataset for training,
rather than sampling it randomly. In order to allow an easier
comparison with some earlier work it would also be advisable
to report error rates for diﬀerent methodologies (such as
random sampling) as well.
5. CONCLUSION
In this paper we have provided a systematic analysis of
the methodology used to evaluate behavioral biometrics for
continuous authentication. Our analysis shows that most
papers present the mean of standard metrics, speciﬁcally
the Equal Error Rate (EER) and False Accept Rate (FAR),
but don’t give any insights of their precise distributions. We
argue that some errors, speciﬁcally systematic false negatives,
are particularly severe in the context of continuous authen-
tication. The analysis of 16 real-world datasets shows that
some biometrics, such as touchscreen inputs, exhibit mostly
random errors, leading to the eventual detection of attackers
due to the process of continuous authentication. Others, such
as gait patterns, tend to produce more systematic errors,
thus allowing some attackers to consistently avoid detection.
In order to allow the comparison of diﬀerent systems with
regard to this property without requiring manual inspection,
we discuss a number of candidate metrics. As a result of this
discussion we propose the use of the Gini Coeﬃcient (GC)
to capture diﬀerent distributions of both the FAR and FRR.
The application of the GC to our datasets reveals that the
addition or removal of certain features can greatly impact the
biometric’s error distribution. Speciﬁcally, using the pupil
diameter for classiﬁcation reduces the system’s average EER,
but also greatly contributes to systematic errors, thereby sug-
gesting it might even reduce overall security. Based on these
insights, the GC can not only be used to compare the security
of diﬀerent systems, but can also guide researchers during
evaluation of diﬀerent classiﬁers, bioemtrics and featuresets.
We therefore recommend that authors report the GC as well
as established metrics in order to provide information about
error distributions as well.
We also quantiﬁed the impact of a number of diﬀerent
machine learning methodologies on a system’s error rates.
395We identify two main factors, the selection of training data
(speciﬁcally, random versus ordered split) and the inclusion
of imposter data in the negative class. While these eﬀects
are somewhat well-known in other ﬁelds, their precise im-
pact has not been quantiﬁed in the context of continuous
authentication. Our analysis shows that random sampling
of training data can reduce the EER by up to 80%, while
inclusion of imposter data provides a reduction of up to 63%.
These results highlight a particular problem, as 13 of the 25
papers we analyzed used a methodology that we have shown
to lead to systematic underestimation of error rates and a
further 6 did not report which methodology was used at all.
Our results highlight that it is inadequate to compare
biometric systems simply by their EERs. Instead, it is crucial
to take into account both the distribution of errors, as well as
the design decisions that were made when simulating system
operation on a static dataset.
Acknowledgements
This work was supported by the Engineering and Physical
Sciences Research Council [grant number EP/M50659X/1].
6. REFERENCES
[1] A. A. E. Ahmed and I. Traore. A new biometric
technology based on mouse dynamics. Dependable and
Secure Computing, IEEE Transactions on,
4(3):165–179, 2007.
[2] H. J. Ailisto, M. Lindholm, J. Mantyjarvi,
E. Vildjiounaite, and S.-M. Makela. Identifying people
from gait pattern with accelerometers. In Defense and
Security, pages 7–14. International Society for Optics
and Photonics, 2005.
[3] K. Allix, T. F. Bissyand´e, J. Klein, and Y. Le Traon.
Are your training datasets yet relevant? In
International Symposium on Engineering Secure
Software and Systems, pages 51–67. Springer, 2015.
[4] K. Allix, T. F. D. A. Bissyande, J. Klein, and
Y. Le Traon. Machine learning-based malware detection
for android applications: History matters! Technical
report, University of Luxembourg, SnT, 2014.
[5] S. Axelsson. The base-rate fallacy and the diﬃculty of
intrusion detection. ACM Transactions on Information
and System Security (TISSEC), 3(3):186–205, 2000.
[6] P. Bours and S. Mondal. Performance evaluation of
continuous authentication systems. IET Biometrics,
4(4):220–226, 2015.
[7] A. Brajdic and R. Harle. Walk detection and step
counting on unconstrained smartphones. In Proceedings
of the 2013 ACM International Joint Conference on
Pervasive and ubiquitous computing, pages 225–234.
ACM, 2013.
[8] ¸S. Budulan, E. Burceanu, T. Rebedea, and C. Chiru.
Continuous user authentication using machine learning
on touch dynamics. In International Conference on
Neural Information Processing, pages 591–598.
Springer, 2015.
[9] D. Buschek, A. De Luca, and F. Alt. Improving
accuracy, applicability and usability of keystroke
biometrics on mobile touchscreen devices. In
Proceedings of the 33rd Annual ACM Conference on
Human Factors in Computing Systems, pages
1393–1402. ACM, 2015.
[10] Z. Cai, C. Shen, M. Wang, Y. Song, and J. Wang.
Mobile authentication through touch-behavior features.
In Biometric Recognition, pages 386–393. Springer,
2013.
[11] M. O. Derawi, C. Nickel, P. Bours, and C. Busch.
Unobtrusive user-authentication on mobile phones
using biometric gait recognition. In Intelligent
Information Hiding and Multimedia Signal Processing
(IIH-MSP), 2010 Sixth International Conference on,
pages 306–311. IEEE, 2010.
[12] B. Draﬃn, J. Zhu, and J. Zhang. Keysens: Passive user
authentication through micro-behavior modeling of soft
keyboard interaction. In International Conference on
Mobile Computing, Applications, and Services, pages
184–201. Springer, 2013.
[13] S. Eberz, K. B. Rasmussen, V. Lenders, and
I. Martinovic. Preventing lunchtime attacks: Fighting
insider threats with eye movement biometrics. In
Proceedings of the Network and Distributed Systems
Security Symposium (NDSS), 2015.
[14] S. Eberz, K. B. Rasmussen, V. Lenders, and
I. Martinovic. Looks like eve: Exposing insider threats
using eye movement biometrics. ACM Transactions on
Privacy and Security, 19(1):1, 2016.
[15] T. Feng, J. Yang, Z. Yan, E. M. Tapia, and W. Shi.
Tips: Context-aware implicit user identiﬁcation using
touch screen in uncontrolled environments. In
Proceedings of the 15th Workshop on Mobile Computing
Systems and Applications, page 9. ACM, 2014.
[16] M. Frank, R. Biedert, E. Ma, I. Martinovic, and
D. Song. Touchalytics: On the applicability of
touchscreen input as a behavioral biometric for
continuous authentication. Information Forensics and
Security, IEEE Transactions on, 8(1):136–148, 2013.
[17] D. Gafurov, K. Helkala, and T. Søndrol. Biometric gait
authentication using accelerometer sensor. Journal of
computers, 1(7):51–59, 2006.
[18] H. Gascon, S. Uellenbeck, C. Wolf, and K. Rieck.
Continuous authentication on mobile devices by
analysis of typing motion behavior. In Sicherheit, pages
1–12. Citeseer, 2014.
[19] C. Gini. Variabilit`a e mutabilit`a. Reprinted in Memorie
di metodologica statistica (Ed. Pizetti E, Salvemini, T).
Rome: Libreria Eredi Virgilio Veschi, 1, 1912.
[20] M. Goﬀredo, I. Bouchrika, J. N. Carter, and M. S.
Nixon. Self-calibrating view-invariant gait biometrics.
Systems, Man, and Cybernetics, Part B: Cybernetics,
IEEE Transactions on, 40(4):997–1008, 2010.
[21] Z. Jorgensen and T. Yu. On mouse dynamics as a
behavioral biometric for authentication. In Proceedings
of the 6th ACM Symposium on Information, Computer
and Communications Security, pages 476–482. ACM,
2011.
[22] T. Kinnunen, F. Sedlak, and R. Bednarik. Towards
task-independent person authentication using eye
movement signals. In Proceedings of the 2010
Symposium on Eye-Tracking Research & Applications,
pages 187–190. ACM, 2010.
[23] J. M¨antyj¨arvi, M. Lindholm, E. Vildjiounaite, S.-M.
M¨akel¨a, and H. Ailisto. Identifying users of portable
devices from gait pattern with accelerometers. In
Acoustics, Speech, and Signal Processing, 2005.
396Proceedings.(ICASSP’05). IEEE International
Conference on, volume 2, pages ii–973. IEEE, 2005.
[24] S. Mondal and P. Bours. Continuous authentication
using mouse dynamics. In Biometrics Special Interest
Group (BIOSIG), 2013 International Conference of the,
pages 1–12. IEEE, 2013.
[25] M. Pusara and C. E. Brodley. User re-authentication
via mouse movements. In Proceedings of the 2004 ACM
workshop on Visualization and data mining for
computer security, pages 1–8. ACM, 2004.
[26] K. B. Rasmussen, M. Roeschlin, I. Martinovic, and
G. Tsudik. Authentication using pulse-response
biometrics. In NDSS, 2014.
[27] L. Rong, D. Zhiguo, Z. Jianzhong, and L. Ming.
Identiﬁcation of individual walking patterns using gait
acceleration. In 2007 1st International Conference on
Bioinformatics and Biomedical Engineering, pages
543–546. IEEE, 2007.
[28] A. Roy, T. Halevi, and N. Memon. An hmm-based
behavior modeling approach for continuous mobile
authentication. In 2014 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP),
pages 3789–3793. IEEE, 2014.
[29] P. Saravanan, S. Clarke, D. H. P. Chau, and H. Zha.
Latentgesture: active user authentication through
background touch analysis. In Proceedings of the
Second International Symposium of Chinese CHI, pages
110–113. ACM, 2014.
[30] D. A. Schulz. Mouse curve biometrics. In 2006
Biometrics Symposium: Special Session on Research at
the Biometric Consortium Conference, pages 1–6.
IEEE, 2006.
[31] C. Shen, Y. Zhang, Z. Cai, T. Yu, and X. Guan.
Touch-interaction behavior for continuous user
authentication on smartphones. In 2015 International
Conference on Biometrics (ICB), pages 157–162. IEEE,
2015.
[32] M. Soriano, A. Araullo, and C. Saloma. Curve
spreads-a biometric from front-view gait video. Pattern
Recognition Letters, 25(14):1595–1602, 2004.
[33] E. Vildjiounaite, S.-M. M¨akel¨a, M. Lindholm,
R. Riihim¨aki, V. Kyll¨onen, J. M¨antyj¨arvi, and
H. Ailisto. Unobtrusive multimodal biometrics for
ensuring privacy and information security with
personal devices. In International Conference on
Pervasive Computing, pages 187–201. Springer, 2006.
[34] A. Weiss, A. Ramapanicker, P. Shah, S. Noble, and
L. Immohr. Mouse movements biometric identiﬁcation:
A feasibility study. Proc. Student/Faculty Research Day
CSIS, Pace University, White Plains, NY, 2007.
[35] H. Xu, Y. Zhou, and M. R. Lyu. Towards continuous
and passive authentication via touch biometrics: An
experimental study on smartphones. In Symposium On
Usable Privacy and Security (SOUPS 2014), pages
187–198, 2014.
[36] X. Zhao, T. Feng, and W. Shi. Continuous mobile
authentication using a novel graphic touch gesture
feature. In Biometrics: Theory, Applications and
Systems (BTAS), 2013 IEEE Sixth International
Conference on, pages 1–6. IEEE, 2013.
[37] N. Zheng, A. Paloski, and H. Wang. An eﬃcient user
veriﬁcation system via mouse movements. In
Proceedings of the 18th ACM conference on Computer
and communications security, pages 139–150. ACM,
2011.
APPENDIX
A. DATASETS
In this section we describe the datasets used to carry out to
evaluate our new metrics (see Section 3). We use 13 datasets
obtained from the authors of previously published work and