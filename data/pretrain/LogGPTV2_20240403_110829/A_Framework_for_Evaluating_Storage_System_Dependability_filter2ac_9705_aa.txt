title:A Framework for Evaluating Storage System Dependability
author:Kimberly Keeton and
Arif Merchant
A Framework for Evaluating Storage System Dependability
Kimberly Keeton and Arif Merchant
Storage Systems Department
HP Labs, Palo Alto, CA 94304
(cid:0)kimberly.keeton,arif.merchant(cid:1)@hp.com
Abstract—Designing storage systems to provide business
continuity in the face of failures requires the use of vari-
ous data protection techniques, such as backup, remote mir-
roring, point-in-time copies and vaulting, often in concert.
Predicting the dependability provided by such compositions
of techniques is difﬁcult, yet necessary for dependable sys-
tem design. We present a framework for evaluating the de-
pendability of data storage systems, including both individ-
ual data protection techniques and their compositions. Our
models estimate storage system recovery time, data loss,
normal mode system utilization and operational costs un-
der a variety of failure scenarios. We demonstrate the effec-
tiveness of these modeling techniques through a case study
using real-world storage system designs and workloads.
1 Introduction
Data is the primary asset of most corporations in the in-
formation age, and businesses must be able to access that
data to continue operation. In a 2001 survey, a quarter of
the respondents estimated their outage costs as more than
$250,000 per hour, and 8% estimated them as more than
$1M per hour [5]. The price of data loss is even higher–
bankruptcy, in the limit. Dependable data storage systems
are needed to avoid such problems.
Fortunately, many techniques exist for protecting data, in-
cluding tape backup [3], mirroring and parity-based RAID
schemes for disk arrays [20], wide area inter-array mir-
roring [12], snapshots [1] and wide area erasure-coding
schemes [15]. Each technique protects against a subset of
the possible failure scenarios, and techniques are often used
in combination to provide greater coverage.
Unfortunately,
the multitude of data protection tech-
niques, coupled with all of their conﬁguration parameters,
often means that it is difﬁcult to employ each technique ap-
propriately. System administrators often use ad hoc tech-
niques for designing their data storage systems, focusing
more on setting conﬁguration parameters (e.g., backup win-
dows), rather than on trying to achieve a particular depend-
ability [3, 4]. As a result, it is often unclear whether the
business’ dependability goals have been met.
If we could quantify the dependability of a storage sys-
tem, we could evaluate whether an existing storage sys-
tem meets its dependability goals, explore future designs
and what-if scenarios (to allow users to understand the de-
pendability ramiﬁcations of their design choices), and pro-
vide the inner-most loop of an automated optimization loop
to choose the “best” solution for a given set of business
requirements [13]. Such a framework should permit the
composition of data protection techniques, to model com-
plicated storage systems and facilitate the incorporation of
new techniques as they are invented.
This paper describes a modeling framework for quanti-
tatively evaluating the dependability of storage system de-
signs. By dependability, we mean both data reliability (i.e.,
the absence of data loss or corruption) and data availability
(i.e., that access is always possible when it’s desired). Be-
cause our target is tools for use in the business continuity
community, we have consciously adopted their metrics for
recovery time and recent data loss after a failure. Recovery
time measures the elapsed time after a failure before a busi-
ness service (e.g., application) is up and running again; the
recovery time objective (RTO) provides an acceptable upper
bound [2]. When a failure occurs, it may be necessary to re-
vert back to a consistent point prior to the failure, which will
entail the loss of any data written after that point. Recent
data loss measures the amount of recent updates (expressed
in time) lost during recovery from a failure; the recovery
point objective (RPO) provides an upper bound [2]. Both
RTO and RPO may range from zero to days. Following the
common practice of the business continuity community, our
models evaluate the recovery time and recent data loss met-
rics under a speciﬁed failure scenario. To these metrics, we
add normal mode system utilization and overall system cost
(including capital and service cost outlays and penalties for
violating business requirements).
The primary contributions of our work include: 1) a com-
mon set of parameters to describe the most popular data pro-
tection techniques; 2) models for the dependability of indi-
vidual data protection techniques using these parameters; 3)
techniques for composing these models to determine the de-
pendability of the overall storage system; and 4) analysis of
the models and compositional framework using case stud-
ies.
The remainder of the paper is organized as follows. Sec-
tion 2 provides an overview of popular data protection tech-
niques and surveys related work. Section 3 describes our
modeling framework in detail, and Section 4 provides an
extensive case study drawn from real-world storage designs
to demonstrate its operation. Finally, Section 5 concludes.
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 10:04:18 UTC from IEEE Xplore.  Restrictions apply. 
2 Data protection techniques and related work
Disk arrays are typically used to store the primary copy of
data; they provide protection against internal hardware fail-
ure through RAID techniques [20] and redundant hardware
paths to the data. Other failures, such as user or software er-
rors and site failure, are covered by techniques that periodi-
cally make secondary copies of the data, often to other hard-
ware. These secondary copies preferably reﬂect a consistent
version of the primary copy at some instant in time; we call
these versions retrieval points, or RPs. The main classes
of such techniques are mirroring, point-in-time copies and
backup.
Mirroring keeps a separate, isolated copy of the current
data on another disk array, which may be co-located with
the primary array or remote. Mirrors may be synchronous,
where each update to the primary is also applied to the sec-
ondary before write completion, or asynchronous, where
updates are propagated in the background. Batched asyn-
chronous mirrors [12, 21] coalesce overwrites and send
batches to the secondary to be applied atomically; they
lower the peak bandwidth needed between the copies by re-
ducing the volume of updates propagated and smoothing out
update bursts.
A point-in-time (PiT) image [1] is a consistent version of
the data at a single point in time, typically on the same array.
The PiT image may be formed as a split mirror, where a
normal mirror is maintained on the same array until a “split”
operation, which stops further updates to the mirror, or as a
virtual snapshot, where a virtual copy is maintained using
copy-on-write techniques, with unmodiﬁed data sharing the
same physical storage as the primary copy. Most enterprise-
class disk arrays (e.g.,
[6, 9]) provide support for one or
more of these techniques.
Backup is the process of copying RPs to separate hard-
ware, which could be another disk array, a tape library
or an optical storage device. Backups may be full, where
the entire RP is copied; cumulative incremental, where all
changes since the last full backup are copied; or differen-
tial incremental, where only the portions changed since the
last full or incremental are copied. Tape backup is typically
done using some combination of these alternatives (e.g.,
weekend full backups, followed by a cumulative incremen-
tal every weekday). Backups made to physically removable
media, such as tape or optical disks, can also be periodically
moved to an off-site vault for archival storage.
Backup techniques and tools have been studied from an
operational perspective (e.g., [3, 4]). Studies also describe
alternative mechanisms for archival and backup (e.g., [15])
and ﬁlesystems that incorporate snapshots (e.g., [11]). Eval-
uations of the storage system dependability have focused
mainly on disk arrays, including the dependability of array
hardware and RAID mechanisms and the recovery time af-
ter failure (e.g., [7, 17, 18, 19, 20, 22]). Additionally, a
great deal of work (e.g., [8]) focuses on the overall area
of dependability and performability evaluation of computer
systems. Keeton, et al., have explored issues in automating
data dependability [13]. This paper builds upon and com-
plements these techniques by providing models of individ-
ual dependability techniques and a framework for compos-
ing them, thus enabling the evaluation of dependability in
storage systems that combine multiple dependability tech-
niques. Our models are deliberately simple, in order to al-
low users to reason about them, and are designed to be com-
posable, so as to ﬁt in the overall framework.
3 Modeling storage system dependability
The goal of our modeling framework is to evaluate the
dependability of a storage system design for the speciﬁed
workload inputs and business requirements, under the spec-
iﬁed failure scenario. Table 1 summarizes the model’s pa-
rameters, which are explained in the sections below.
The model components work together as follows. The
data protection technique models convert their input param-
eters to bandwidth and capacity workload demands on the
storage and interconnect devices they employ. The hard-
ware device models perform device-speciﬁc calculations to
determine each device’s bandwidth and capacity utilization
and outlay costs. The compositional models combine the re-
sults from these data protection and device models to gen-
erate the output metrics. By isolating the details of each
data protection technique and hardware device, we can eas-
ily substitute more sophisticated models (e.g., [16, 19]) for
these components as needed, without needing to modify the
rest of the modeling framework.
In this section, we describe the modeling framework in
more detail, including: how we capture the model inputs
(Section 3.1); how we abstract the behavior of individual
data protection techniques (Section 3.2); and how we com-
pose techniques to determine the dependability of the over-
all storage design (Section 3.3).
3.1 Model inputs
This section outlines how we describe model inputs, includ-
ing workloads, business requirements and failure scenarios.
3.1.1 Workload inputs
A storage system holds the primary copy for data (e.g.,
ﬁle systems or database tablespaces). Data protection tech-
niques exploit the workload’s update properties to effec-
tively make secondary copies of the data, with some tech-
niques propagating all updates and others propagating only
periodic batches of unique updates. Given these behaviors,
the key workload parameters to capture include: data ca-
pacity, average access rate, average update rate, burstiness
and batch update rate, as deﬁned in Table 1.
Although most systems store multiple data objects, we
assume for simplicity a single data object and workload.
Our models can be extended in a straightforward manner by
explicitly tracking each object’s workload demands, the set
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 10:04:18 UTC from IEEE Xplore.  Restrictions apply. 
Notation
Parameter
Output metrics
system utilization
recovery time
recent data loss
overall cost
Model inputs: workload
data capacity
average access rate
average update rate
burstiness
batch update rate
Model inputs: Business requirements
data unavailability penalty rate
recent data loss penalty rate
Model inputs: Failure scenarios and recovery goals
failure scope
dataCap
avgAccessR
avgUpdateR
burstM
batchUpdR(win)
unavailPenRate
lossPenRate
failScope
Units
Description
percentage
sec
sec
US dollars
bytes
bytes/sec
bytes/sec
multiplier
bytes/sec
utilization of maximally utilized storage component
time from failure to having application running again
recent data updates not recovered by recovery process
overall system cost (including outlays and penalties)
size of the data item
rate of read and write accesses to the object
rate of (non-unique) updates to the object
ratio of peak update rate to average update rate
unique update rate within a given window
US dollars/sec
US dollars/sec
penalty per unit time for unavailability of data
penalty for loss of a time-unit’s worth of updates to data
set of data copy sites unavailable due to a failure
point in time to which restoration is requested
period over which updates are batched to create a retrieval point (RP)
RP transmission period
delay between receiving and transmitting an RP
number of secondary windows between primary windows
length of a cycle for a policy with multiple accumulation windows
number of cycles of RPs simultaneously retained
how long a particular RP is retained
what RP representation is maintained
what RP representation is propagated
max slots for capacity devices (tape cartridges, disks)
max slots for bandwidth devices (tape drives, disks)
per-slot (tape cartridge or disk) capacity
per-slot (tape drive or disk) bandwidth
aggregate enclosure bandwidth (including busses and controllers)
delay for accessing storage device/interconnect propagation delay
ﬁxed costs for enclosure, facilities, service
per-capacity costs for device, facilities, service
per-bandwidth costs for device, facilities, service
spare resources available for device
time to provision spare resources
fraction of original dedicated resource cost
(cid:1)(cid:2)(cid:3)(cid:4)(cid:0)(cid:6)(cid:6)(cid:8)(cid:0)(cid:1) (cid:10)(cid:11)(cid:0)
(cid:3)(cid:11)(cid:12)(cid:0)(cid:12)(cid:3)
sec