we observed a Pearson correlation coefﬁcient of ρ = −.20
(p-value = .10). While not statistically signiﬁcant, the neg-
ative ρ suggests that the more helpful a user found a warn-
ing message, the fewer times they fell for the corresponding
attack.
6 Discussion and Conclusions
Our study suggests sev-
Speciﬁc study conclusions.
eral take-away messages about the various Bumpy designs
tested here.
1. The NoTM design is very attractive for both deploy-
ment (since it uses no TM) and user login duration (see
Figure 3(b)). Unfortunately (and contrary to our expec-
tations, see §3.2.2) there is signiﬁcant evidence that it
is more susceptible to users leaking password charac-
ters than other designs (see rejected hypotheses (3)–(6)).
This holds even after training via warnings (see rejected
hypotheses (7)–(8)), even though warnings were effec-
tive in signiﬁcantly improving NoTM’s security against
some attacks (see Figure 5). As such, if NoTM is de-
ployed, it warrants a concerted training effort for users.
2. The Challenge design offers generally low password
leakage after training via warnings, at least judging from
the sample means illustrated in Figure 5. In particular,
the sample means of Challenge were the lowest across
designs for all Active attacks generally, the only excep-
tion being Feigned-Fail in Figure 5(a). While the ev-
idence supporting the greater security of Challenge in
comparison to other designs was statistically signiﬁcant
only in some cases (see rejected hypotheses (5)–(6) and
(7)), it appears to be a generally good choice for security.
This is a testament to requiring users to examine the TM
in the login process, though it does result in statistically
signiﬁcantly longer login times (see Figure 3(b)).
is weak.
3. The evidence for a move to a graphical design like
Graphical offered no signiﬁ-
Graphical
cant improvements over Original in login success rate,
duration, or password leakage in either Attack or
Attack-and-Warn. While it did exhibit signiﬁcant im-
provement due to warnings in Attack-and-Warn (see
Figure 5), the attacks for which it did so often still cap-
tured as many password characters as with Original or
Challenge, in terms of sample means. The best argu-
ment we see for Graphical is that it yielded the least
leakage (in terms of sample means) for Passive attacks
(Figures 4, 5), but given the implementation challenges
that Graphical introduces (see §3.2.1), we believe the ev-
idence of its beneﬁts would need to be stronger to advo-
cate for it.
Broader observations. Aside from the above conclusions,
we provide more general observations that extend beyond
the speciﬁc designs studied here, albeit with a degree of
speculation.
1. Users appear to readily adapt to employing secure atten-
tion sequences (at least generic ones, versus site-speciﬁc
ones), as evidenced by the low password leakage versus
Passive attacks in our designs that employ one. Tech-
niques that leverage secure attention sequences for use-
ful properties thus hold promise.
2. Though the results of our study suggest a tradeoff be-
tween login duration and security, we see no reason to
conclude that this tradeoff is fundamental. Rather, we
consider it a fascinating open problem to design a login
system that offers both the speed to which users are ac-
customed today and security against the attacks we con-
sider here.
4. That
some users were unfazed by warnings
3. The additional security offered by Challenge suggests
that interactive security indicators yield better security
than ones that a user is asked to simply observe. This is
a direction that deserves further attention, in our opinion.
in
Attack-and-Warn suggests that to mold user behavior,
training that requires a user to repeat the task at hand
immediately following a mistake would be warranted.
As this model can potentially disrupt the user’s primary
activity, though, it is perhaps more acceptable to include
such training as a separate user activity. Automated rein-
forcement during teachable moments, e.g., when a user
makes a mistake during normal operation, may also be
desirable, but such designs must be weighed against the
ability of attackers to use such mechanisms maliciously.
For example, attackers may provide malicious instruc-
tions that confuse users.
5. Many of our Active attacks were designed to mimic
changes of behavior that might seem familiar to users,
owing to relatively frequent discontinuities that pervade
users’ software experiences (failures, software updates
changing behavior in subtle ways, etc.). Based on our
study, it appears that many users are unprepared to dis-
tinguish between a benign discontinuity and a subtle at-
tack without training.
Limitations. Our study did not attempt to evaluate the
usability of Bumpy designs for protecting multiple secret
input ﬁelds on a single web form, input ﬁelds to multiple
websites, or secrets of greater complexity or length than a
typical password. (We simply enforced a minimum length
requirement of 8 characters and placed no requirements as
to the “strength” of the password chosen.) More complex
secrets, in particular, may be problematic for usability since
Bumpy prevents these keystrokes from echoing to the user’s
display. Regarding forgotten passwords, our study did not
employ Bumpy protections on the “reset password” page.
As discussed previously, the step required of users who
did detect malware (reloading the page) and our simulation
of the TM are both sources of unrealism in our experiment,
albeit unavoidable ones given the constraints of the course
setting. While a ﬁnancial incentive was necessary to mo-
tivate our users to protect their passwords, such incentives
are not so directly available in practice and could have had
unintended effects on user behavior.
As discussed in §4.4, we do not claim that our sim-
ulated attacks are exhaustive.
For example, we did
not consider phishing attacks in our study (though pre-
vious studies have shown promise in educating users to
detect
them through a training regimen similar to our
Attack-and-Warn phase [28]). Nevertheless, the attacks
we did simulate allowed us to draw several conclusions
about Bumpy and more broadly, as discussed above. At
the same time, the numerous conditions required for com-
puting some of our measures — e.g., that La
At(u) 6= ∅,
Aa
At(u) 6= 0 in order to com-
pute avgImprove a
AW(u) — reduced the number of sample
points (users) for some of our measures to an uncomfort-
ably small number. If, after further investigation, the attacks
we considered do seem to be the primary threats, it may be
desirable to perform larger studies focused on each attack.
AW(u) 6= ∅, and avgLeaked a
Acknowledgements
We are particularly grateful to the students of COMP380
at UNC in the Fall 2009 semester, and to the course instruc-
tor Tessa Joseph Nicholas, for permitting us to conduct this
experiment. We are also especially grateful to Chris Wiesen
of the Odum Institute at UNC, for his guidance on the sta-
tistical evaluations in this paper. We thank Kathleen Mc-
Cune for her assistance on the study questionnaire and Dar-
rell Bethea, Ting-Fang Yen, and the anonymous reviewers
for comments on previous versions of this paper. This work
was supported in part by NSF grant CT-0756998 and by
grant DAAD19-02-1-0389 from the Army Research Ofﬁce.
References
[1] D. Balfanz and E. W. Felten. Hand-held computers can be
better smart cards. In USENIX Security, Aug. 1999.
[2] K. Borders and A. Prakash. Securing network input via a
trusted input proxy. In USENIX HotSec, Aug. 2007.
[3] S. Chiasson, P. C. Van Oorschot, and R. Biddle. A usability
study and critique of two password managers. In USENIX
Security, Aug. 2006.
[4] D. E. Clarke, B. Gassend, T. Kotwal, M. Burnside, M. van
Dijk, S. Devadas, and R. L. Rivest. The untrusted computer
problem and camera-based authentication. In International
Conf. Pervasive Computing, 2002.
[5] P. Coogan. Zeus, king of the underground crimeware toolk-
its. Symantec Blogs, Aug. 2009.
[6] L. F. Cranor. What do they “indicate”?: evaluating security
and privacy indicators. Interactions, 13(3):45–47, 2006.
[7] L. Duﬂot, O. Levillain, B. Morin, and O. Grumelard. Getting
into the SMRAM: SMM reloaded. Central Directorate for
Information Systems Security, 2009.
[8] L. K. Edwards. Applied Analysis of Variance in Behavioral
Science. CRC Press, 1993.
[9] S. Egelman, L. F. Cranor, and J. Hong. You’ve been warned:
An empirical study of the effectiveness of web browser
phishing warnings. In CHI, 2008.
[10] E. Gabber, P. B. Gibbons, D. M. Kristol, Y. Matias, and
A. Mayer. On secure and pseudonymous client-relationships
with multiple servers. ACM TISSEC, 2:390–415, Nov. 1999.
[11] S. Garriss, R. C´aceres, S. Berger, R. Sailer, L. van Doorn,
and X. Zhang. Trustworthy and personalized computing on
public kiosks. In MobiSys, June 2008.
[12] D. Grawrock. Dynamics of a Trusted Platform: A Building
Block Approach. Intel Press, 2008.
[13] IBM Zurich Research Lab. Security on a stick. Press release,
Oct. 2008.
[14] M. Kassner. Carberp: Quietly replacing Zeus as the ﬁnancial
malware of choice. TechRepublic IT Security Blogs, Oct.
2010.
[15] S. T. King, P. M. Chen, Y.-M. Wang, C. Verbowski, H. J.
Wang, and J. R. Lorch. SubVirt: Implementing malware
with virtual machines. In IEEE Symp. Security and Privacy,
2006.
[16] J. M. McCune, B. Parno, A. Perrig, M. K. Reiter, and
H. Isozaki. Flicker: An execution infrastructure for TCB
minimization. In ACM EuroSys, Apr. 2008.
[17] J. M. McCune, A. Perrig, and M. K. Reiter. Safe passage
for passwords and other sensitive data. In ISOC NDSS, Feb.
2009.
[18] B. A. Myers. Using handhelds and PCs together. CACM,
44(11), Nov. 2001.
[19] A. Oprea, D. Balfanz, G. Durfee, and D. K. Smetters. Se-
curing a remote terminal application with a mobile trusted
device. In ACSAC, 2004.
[20] Organisation for Economic Co-operation and Develop-
software (malware): A
Technical Report
ment
threat
DSTI/ICCP/REG(2007)5/FINAL, OECD, June 2008.
(OECD).
to the internet economy.
Malicious
[21] D. Rees. Foundations of Statistics. Chapman and Hall/CRC,
1987.
[22] B. Ross, C. Jackson, N. Miyake, D. Boneh, and J. C.
Mitchell. Stronger password authentication using browser
extensions. In USENIX Security, Aug. 2005.
[23] S. J. Ross, J. L. Hill, M. Y. Chen, A. D. Joseph, D. E.
Culler, and E. A. Brewer. A composable framework for se-
cure multi-modal access to Internet services from post-PC
devices. Mobile Network Applications, 7(5):389–406, 2002.
[24] J. Rutkowska. Subverting Vista kernel for fun and proﬁt.
Presented at Black Hat USA, 2006.
[25] S. E. Schechter, R. Dhamija, A. Ozment, and I. Fischer. The
emperor’s new security indicators. In IEEE Symp. Security
and Privacy, 2007.
[26] R. Sharp, A. Madhavapeddy, R. Want, and T. Pering. En-
hancing web browsing security on public terminals using
mobile composition. In MobiSys, June 2008.
[27] R. Sharp, J. Scott, and A. Beresford. Secure mobile com-
puting via public terminals. In International Conf. Pervasive
Computing, May 2006.
[28] S. Sheng, B. Magnien, P. Kumaraguru, A. Acquisti, L. F.
Cranor, J. I. Hong, and E. Nunge. Anti-phishing phil: the
design and evaluation of a game that teaches people not to
fall for phish. In SOUPS, pages 88–99, 2007.
[29] J. Sobey, R. Biddle, P. C. Van Oorschot, and A. S. Patrick.
Exploring user reactions to new browser cues for extended
validation certiﬁcates. In ESORICS, 2008.
[30] M. Stiegler. An introduction to petname systems, Feb. 2005.
[31] J. Sunshine, S. Egelman, H. Almuhimedi, N. Atri, and
L. Cranor. Crying wolf: An empirical study of SSL warning
effectiveness. In USENIX Security, Aug. 2009.
[32] Trusted Computing Group. Trusted platform module main
speciﬁcation, Part 1: Design principles, Part 2: TPM struc-
tures, Part 3: Commands. Version 1.2, Revision 103, July
2007.
[33] T. Whalen and K. M. Inkpen. Gathering evidence: use of
visual security cues in web browsers. In Graphics Interface,
2005.
[34] R. Wojtczuk and J. Rutkowska. Xen 0wning trilogy. Invisi-
ble Things Lab, 2008.
[35] R. Wojtczuk and J. Rutkowska. Attacking SMM memory
via Intel CPU cache poisoning. Invisible Things Lab, 2009.
[36] M. Wu, R. C. Miller, and S. L. Garﬁnkel. Do security tool-
bars actually prevent phishing attacks? In CHI, 2006.
A Summary of Bumpy Internals
The original Bumpy6 design [17] is motivated by the
prevalence of malware on users’ computers, and so the OS
(e.g., Windows, Linux) is assumed untrustworthy. Further
assumptions inherited from the original Bumpy design in-
clude that the remote webserver is uncompromised, and that
the SSL certiﬁcate provided by the webserver is legitimate
and can be extended to integrity-protect the site’s favicon.
Bumpy enables users to submit sensitive data on web forms
without revealing that data to local malware.
§3.1 contains a detailed description of the Bumpy user
experience. Here, we summarize the technical underpin-
nings of the system, speciﬁcally the system architecture
required to protect user input from a malicious operating
system. To do so, Bumpy requires some additional soft-
ware and (inexpensive and commodity) hardware to en-
able it to always receive (plaintext) user input before the
OS, including: encryption-capable user-input devices (or an
encryption-capable interposer, e.g., a small USB device); an
isolated and attestable execution environment on the user’s
computer (e.g., Flicker [16]); a Bumpy software module
that executes in this isolated environment, comprising a Pre-
Processor and Post-Processor (described below); a trustwor-
thy device with a display to serve as a TM; and Bumpy-
aware software on the webserver. These items are in the
trusted computing base for Bumpy. Other enhancements
that are needed to the software on the client host to work
6For the rest of this section, Bumpy refers to the original design and
not to any of the alternatives proposed in the present paper.
with Bumpy include enhancements to the input-handling
logic of the platform OS and an extension to the user’s web
browser. We stress that these are not in the TCB for Bumpy,
however.
We ﬁrst describe input processing, and then describe how
feedback is provided to the user via the TM.
Input Processing. Figure 7 summarizes the ﬂow of user
input. Keystrokes are encrypted by an encryption-capable
keyboard (or an interposer), and the ciphertext is received
by the OS (Steps 1–3). Here, the OS has the opportu-
nity to perform a denial-of-service attack on the inputs, but
the OS can always do so by other means (e.g., by pow-
ering off or crashing the system, or otherwise preventing
meaningful work from being done on the system). A well-
behaved OS will invoke a protected execution environment
(Step 4), where keystrokes are decrypted and processed by
the Bumpy module.
The Bumpy module separates input handling into a Pre-
Processor (PreP) and Post-Processor (PoPr). The PreP de-
crypts incoming keystrokes and tracks whether the current
input is sensitive.
(Sensitive input begins when the user
types the SAS @@, and ends when the user provides an in-
put that would cause a blur in the web browser GUI; e.g.,
a tab would be such an event.) Normal input is released
to the platform OS decrypted (Step 5), and in this case the
user’s experience remains unchanged. No further steps are
taken for normal input. Sensitive input, however, is queued
within the PreP, and decoy input events (e.g., asterisks) are
released to the platform OS (also Step 5). When the user
ﬁnishes providing input to a ﬁeld that she denotes as sen-
sitive (detected within the PreP by an input event that will
cause a blur in the web browser GUI), her sensitive input is
processed in its entirety. This consists of invoking a (possi-
bly destination-speciﬁc) PoPr that will re-encrypt the user’s
sensitive input for its intended destination (Step 6).
Once the ciphertext containing the user’s sensitive input
is ready for transmission to its intended destination, a TPM-
based attestation [32] is produced. This attestation is used
to convince the destination webserver that the user’s input
was handled with the Bumpy system, the intention being
that service providers may be willing to expose additional
services to users who are better able to protect their sen-
sitive input. The ciphertext and attestation are handled by
an extension to the user’s web browser (Step 7) and sent to
the remote webserver (Step 8). The webserver will verify
the attestation, decrypt the user’s input, and process it in
accordance with the current application (e.g., a credit card
number for an online purchase).
Trusted Monitor. Upon receiving the SAS, the PreP will
output an authenticated message for the TM that includes
information about the currently active destination website.
This information is maintained in the PreP and includes
the domain name (Common Name in the site’s SSL certiﬁ-
5. PreP releases
    input event
    to OS / App
Platform
Operating
System
Web Browser
Extension
7. PoPr output
    handled by
    web browser
Internet
PreP
PoPr 1
...
PoPr N
Flicker
Web
Server
Encrypting Input Devices
KB &
Mouse
USB Interposer
1. User presses
    key / button
2. Keystroke
    encrypted
3. OS handles
    ciphertext
4. OS invokes
    PreP / Flicker
6. PoPr invoked
    with queue
8. Webserver receives
    PoPr output
Figure 7. Original Bumpy design for acquiring user input [17]. Steps 1–5 occur for every keystroke
or mouse click performed by the user. Steps 6–8 occur only in response to a keystroke or mouse
click that the PreP detects will cause a blur event in the web browser GUI while the user is entering
sensitive data.
cate) and graphic logo (the site’s favicon) of the destination
webserver. Bumpy reads this information directly from the
destination webserver’s SSL certiﬁcate, and so the domain
and graphic logo on the TM are precisely the destination to
which sensitive input will be sent. It is the user’s responsi-
bility to verify that this is the intended destination.