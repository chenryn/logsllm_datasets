Signiﬁcance test. Given a level of hierarchy in Fig. 2 at which
the detailed symptom events are deﬁned, for example, STB level
for raw STB crash events, or CO level for router CPU anomalies
observed at local distribution ofﬁce, we need to determine whether
any aggregation at a higher level location is signiﬁcant for the given
symptom events. Consider a two-level hierarchy in which the lower
layer has n different locations. A speciﬁc top layer location has m
children locations, which have a total of c events. We need to test
whether c is signiﬁcant. Let xi be the number of events associated
with each of the lower level locations i (i = 1, 2, ..., n). We deﬁne
e = mean(xi) and v = variance(xi) as the mean and variance for xi
respectively.
Under the null hypothesis that the m children are drawn inde-
pendently and uniformly at random among lower level locations,
the total number of event count of the m children, c, should have
mean and variance of Exp(c) = m · e and Var(c) = m · v. If m is
reasonably large, c should approximately be Gaussian distributed
according to the central limit theorem (we will validate this in Sec-
tion 4). It follows that the value of c is considered signiﬁcant if
(c− m· e)
√m· v ≥ T,
where T is the threshold corresponding to the desired false positive
ratio. For standard Gaussian distribution, using values of T as 1.64,
1.96, or 2.33 provides a distribution tail corresponding to 5%, 2.5%,
or 1%, respectively.
When m is too small, the central limit theorem no longer applies.
In this case, we apply one-side Chebyshev inequality [29] (or Can-
telli’s inequality) to determine the heavy hitters. Speciﬁcally, under
the null hypothesis that the m children are drawn identically and in-
dependently from the same distribution with mean e and variance
v, we have
Prob(
(c− m· e)
√m· v ≥ k) ≤
1
(1 + k2)
.
We can use 4.36, 6.25, or 9.95 as the threshold value of k to achieve
an expected false positive ratio of 5%, 2.5%, and 1%, respectively.
When applying heavy hitter detection, we start at the lowest pos-
sible spatial level for which the symptom event is applicable, and
follow the hierarchy upward. This allows us to detect any signif-
icant concentration of symptom events at as precise spatial granu-
larity as possible.
3.3 Event-series Formation
Once the heavy hitters are identiﬁed, our next step is to under-
stand the performance issue of interest and to troubleshoot for the
root cause. We do so through our correlation and causality anal-
ysis with other data sources. In this subsection, we describe the
construction of the data time series that will feed to the subsequent
analysis.
Given, either symptom events or diagnostic events that we hope
to ﬁnd dependency to the symptom ones, we construct a ﬁxed-
interval binary time series as described in NICE [23]. A “1” in the
time series indicates the presence of the event in the corresponding
time interval and “0” indicates absence. We follow the same princi-
ple as in [23] in choosing the interval length that takes into account
time inaccuracy and the delayed impact due to event propagation
or timers.
For an IPTV network, we need to aggregate the low-level event-
series at different higher level in the spatial hierarchy. We deﬁne
two aggregation constructions which will be applied in different
scenarios in the paper: (i) union and (ii) concatenation. In event-
series union, we superpose multiple event-series of the same length
(e.g., one for each child location in the hierarchy) and apply either
element-wise OR or element-wise SUM. In event-series concatena-
tion, we append the multiple event-series one after another to form
a longer event-series.
When comparing event-series for correlation and causality anal-
ysis, we need to consider the spatial levels of the symptom and
diagnostic events. When both event-series are at the same level,
we may directly apply pair-wise correlation analysis. An example
is to test STB crashes against user channel changes on the same
STB. Another example is to test STB crashes against user channel
changes within the same DSLAM, in which both events-series are
ﬁrst aggregated (using union) into DSLAM level event-series and
then compared against each other. When two event-series are at
different spatial levels, we ﬁrst aggregate (using concatenate) the
lower-level event-series into the matching higher-level series, and
then replicate the higher-level series multiple times so that it has the
same length as the concatenated one, and ﬁnally compare the two
extended time series. An example of this case is to test for corre-
lation between STB crashes and CPU anomalies on the CO routers
with which the STBs are associated. This ensures that the result of
our correlation analysis, described next, is meaningful.
3.4 Statistical Correlation Detection
Troubleshooting a symptom event often starts with identifying
what other events took place at around the same time and might po-
tentially have had an impact on the symptom. Such co-occurrence
based approaches, albeit conceptually simple, may catch many events
that co-occur merely by coincidence, hence are ineffective due to
high false positives. In Giza, we use a statistical correlation based
approach for correlation analysis. In particular, we adopt the circular-
permutation-based correlation test proposed in NICE [23] for pair-
wise correlation test. Comparing to other statistical correlation
tests, the advantage of NICE lies in the fact that it takes into ac-
count the auto-correlation likely to be present in both symptom and
diagnosis event-series; auto-correlation, if present, can have signif-
icant inﬂuence on correlation score. Next, we brieﬂy describe how
NICE works; for more details, please refer to [23].
Let rXY (t) be the Pearson’s correlation coefﬁcient between event-
series X and the circularly shifted version of event-series Y at lag t.
For each lag t ∈ [0, N), where N is the number of samples in each
event-series, rXY (t) is deﬁned as
rXY (t) =
∑N
i=1(Xi−µX )(Y(i+t) mod N−µy)
(N−1)σXσY
,
where µX and µY are the means of X and Y , and σX and σY are the
standard deviations, respectively.
The circular shifting eliminates the cross-correlation between the
two event-series and preserves auto-correlation within each event-
series. We can thus use {rXY (t)} (t ∈ [0, N)) to establish a base-
line for the null hypothesis that two event-series have no signiﬁcant
cross-correlation. In order to test the hypothesis, we apply Fisher’s
z-transform as follows.
z(t) = 1
2 lnh 1+rXY (t)
1−rXY (t)i.
We note that {z(t)} is asymptotically Gaussian for sufﬁciently large
N. Given this, we deﬁne the correlation score as
score = z(0)
σz
=
z(0)
stddev({z(t)}) ,
where stddev({z(t)}) denotes the sample standard deviation of z(t)’s.
A correlation score is considered signiﬁcant if it falls outside of the
236[−2.5, 2.5] range. With z asymptotically Gaussian, this yields a low
false positive ratio of around 1%.
3.5 Causality Discovery
Through the correlation analysis above, we can obtain a list of
correlated event-series. The next step is to organize them into a
causality graph that provides the causal relationship among differ-
ent symptom and diagnostic events. We generate a directed edge
from event-series X to event-series Y in the causality graph if: (i)
X and Y have signiﬁcant statistical correlation; (ii) X precedes Y in
a statistical sense; and (iii) X and Y are not related via other events.
There are many techniques available in the data mining literature
to discover statistical causal dependencies, such as linear regression
and partial correlations. However, they are not directly applicable
in our context due to two main problems: (i) many event-series
pairs exhibit strong cross-correlations among each other, which
causes regression and partial correlation coefﬁcients to be inaccu-
rate (this is commonly known as the problem of multi-collinearity),
(ii) regression when used to distinguish cause and effect produces
erroneous edge directionality in noisy data. We address these prob-
lems by ﬁrst identifying edge directionality using a novel statistical
lag correlation method, and then applying an edge reduction algo-
rithm to eliminate spurious correlations.
3.5.1 Edge Directionality using Lag Correlation
The key idea is to use timing information to test whether one
event-series statistically precedes the other. Given two event-series
X and Y , we generate samples of the Pearson’s correlation coefﬁ-
cient rXY by circularly shifting Y to different lags with respect to
X , and computing the cross-correlation coefﬁcient between X and
the shifted Y . By comparing positive lag correlations with nega-
tive ones, we identify if Y statistically occurs before X (positive
lags dominate over negative lags), or X statistically occurs before
Y (negative lags dominate over positive lags), or the directionality
is inconclusive (positive and negative lags are comparable).
Focusing on the data in IPTV network in which we are interested,
there are two issues which we need to be particularly careful about.
First, the timing information in event timestamps is not 100% re-
liable due to low granularity of periodic polling, imperfect clock
synchronization, non-deterministic event propagation and record-
ing delays. This precludes us from using any inference techniques
that rely on precise timing such as in Sherlock [4]. Second, many
event-series exhibit strong auto-correlation structure, especially at
small lags – this smooths out the shape of the cross-correlation
graph, making inference difﬁcult.
To solve these two problems, we start with the z-scores at dif-
ferent lags in our correlation analysis, which we have already com-
puted in the correlation signiﬁcance test, and apply the following
either of the two heuristics.
1. Comparing the maximum in a range of positive and neg-
If the maximum in the positive lag range
ative lag correlations.
max(z(k1), .., z(k2)) is greater than the maximum in the negative lag
range max(z(−k3), .., z(−k4)), meaning that the correlation score is
higher when Y is shifted in the positive direction, we deduce that Y
statistically precedes X . Similarly, if the maximum in the negative
lag range is greater than that in the positive lag range, we deduce
that X precedes Y statistically. If the maximum in both ranges are
close (within a threshold), then the directionality is inconclusive
and we leave the edge unmarked. This metric is useful when there
is a strong auto-correlation at small lags.
2. Statistical change detection between the ranges of positive
Instead of comparing the maxi-
and negative lag correlations.
mum, we may also compare the mean of the distributions in pos-
itive and negative lag ranges. Let PL and NL denote the sample
score sets for positive lags and for negative lags, respectively. Let
σz, respectively.
µp and µn be the mean of the distributions respectively. Then the
σz
standard deviations of µp and µn are denoted by σp = 1√k2−k1
and σn = 1√k3−k4
When comparing the means of two distributions, the difference
of the means is µp − µn and the variance is the sum of individual
variances. Hence, the standard deviation is qσ2
n . We com-
pute the statistical change score as
p +σ2
µp−µn√σ2
p +σ2
n
According to the central limit theorem, the range [−2.5, 2.5] can
be used as the score range in which we cannot say statistically
which lag dominates with 99% accuracy.
If the change score is
greater than 2.5, then positive lag dominates. If the change score is
less than -2.5, then negative lag dominates.
The above two approaches often ﬁnd consistent results, as we
will see in Section 4.2.
3.5.2 Edge Reduction using ℓ1 Norm Minimization
Now that we have obtained the partially directed correlation graph
built from statistical lag correlations, our next step is to prune spuri-
ous edges if any. A statistical correlation between two event-series
X and Y is deﬁned to be spurious if the correlation is actually due to
a third variable Z. In other words, when the events corresponding
to Z are removed, the correlation between X and Y becomes in-
signiﬁcant. For example, if the correlation between packet loss and
router CPU anomalies is due to link down, then we can eliminate
the edge between packet loss and router CPU anomalies.
The key idea is to apply statistical regression and preserve edges
in which the regression coefﬁcients are signiﬁcant. We use the
symptom event-series as the predictee and each diagnostic event-
series that has a directed edge towards the symptom as the predic-
tors. A signiﬁcant regression coefﬁcient indicates that the correla-
tion between two event-series is non-spurious and we keep those
edges in the causal graph. On the other hand, an insigniﬁcant re-
gression coefﬁcient means that the dependency between two event-
series is not strong when other event-series are considered, hence
we eliminate the corresponding edge from the causal graph.
One challenge with this edge elimination approach stems from
scale since a large number of event-series in the correlation graph
means regression coefﬁcients for each predictor would be small and
identifying the threshold for signiﬁcance becomes non-trivial. In
practice though, we expect only a few event-series to have signif-
icant causal relationship with the symptom.
In other words, we
expect the vector of regression coefﬁcients to have only a small
number of large values. To effectively identify these coefﬁcients,
we propose a new method using ℓ1 norm minimization with ℓ1 reg-
ularization which has the ability to ﬁnd a sparse solution (achieving
the approximate ℓ0 norm minimization) [15].
The method works as follows. Let y be the vector of predictee
event-series. Xm x n is matrix of predictor event-series with m be-
ing the length of each event-series and n being the total number of
predictors. Note that X is comprised of only those event-series that
statistically occur before y. We formulate the ℓ1 norm minimization
problem as:
minimize ||y−βX||1 +λ||β||1
where β is a vector of regression coefﬁcients and λ ∈ [0, 1] is the
regularization parameter. We reformulate the above minimization
problem into the following equivalent linear programming (LP)
problem:
minimize
subject to
λ ∑i ui + ∑ j v j
y = βX + z
u ≥ X , u ≥ −X
v ≥ z, v ≥ −z
237Input: A list of k event-series
Output: Directed causal graph G = (V, E) where V is the set of
event-series with |V| = k and an edge (i, j) ∈ E indicates i is a cause
of j
Algorithm:
1. Initially, E = {}
Edge directionality using lag correlation
2. ∀i ∈ V
3.
4.
∀ j( j 6= i) ∈ V
if LagCorr(i, j) is positive signiﬁcant
/*LagCorr(x, y) is computed by ﬁxing x and shifting y */
then E = E ∪ ( j, i)
5.
Edge reduction using ℓ1 norm minimization
6. ∀i ∈ V
7.
8.
X = { j| j ∈ V and ( j, i) ∈ E}
β = L1NormRegression(X , i)
/* beta: regression coefﬁcients between i and all j ∈ X */
R = { j|βi, j is insigniﬁcant }
9.
10. ∀ j ∈ R
11.
Remove (i, j) from E
Figure 9: Causal discovery algorithm.
To build the entire causal graph with N event-series, we run LP
for each event-series as y. The predictors for each y is identiﬁed
using the lag correlations. We now show the complete causal dis-
covery algorithm in Fig. 9.
4. GIZA EXPERIENCES
In this section, we present Giza validation using real data col-
lected from the IPTV network and demonstrate its effectiveness in
troubleshooting network issues. First, we demonstrate that our as-
sumption about Gaussianity is valid in hierarchical heavy hitter de-
tection. Second, we show that our causality algorithm that accounts
for auto-correlation and multi-collinearity performs better than the
state-of-art causality algorithm in WISE [28]. Third, we describe
our experiences in applying Giza on diagnosing customer trouble
tickets and video quality alarms, and comparing our results with the
ground truth information provided by network operators. Finally,
we present case study where Giza has been applied to discover the
causal graph and previously unknown dependencies in the provider
network.
4.1 Validating Gaussianity for HHH
We use the Q-Q (quantile-quantile) plot to validate the Gaussian
assumption we made in hierarchical heavy hitter detection. Q-Q
plot is a graphical method to identify whether there exists a statis-
tical difference between one probability distribution and another.
For our validation, we compare the normal distribution constructed
from our hypothesis test with the event count distribution at various
spatial resolutions. If the two distributions match perfectly, then the
Q-Q curves should approximate the straight diagonal line.
Fig. 10 shows one example for a particular type of STB crash
events (native crash) at three different spatial resolutions. It can be
observed that all three curves largely approximate a straight line
with the exception of a few outliers at the distribution tail. We
also have plotted Q-Q plot for all the other data sources and have
observed similar level of matches. This conﬁrms that the Gaus-
sian approximation due to Central limit theorem works reasonably
well in our data. The deviation shown in the tail part in Fig. 10(a)
and (b) however suggests that there indeed exists a pattern of spa-
tial concentration – some COs (or Metro’s) have observed a higher
number of STB crashes that can be explained by simple aggregation
variance. Those are the genuine heavy hitters that can be identiﬁed
through our hierarchical heavy hitter detection scheme. Fig. 10 also
WISE (Partial Correlation) +
Linear Regression
ℓ1 Norm + Statistical Change
Lag Correlation
ℓ1 Norm + Maximum Lag Cor-
relation
Total edges
identiﬁed
4903
Edges correctly
identiﬁed
71.9 %
1103
1125
84.4 %
85.3 %
Table 4: Comparison of causal discovery algorithms.
shows in dotted line where the thresholds (at 1% from distribution
tail) for the hierarchical heavy hitter detection are. All data points
to the right of the dotted lines are considered heavy hitters.
4.2 Comparing Causal Discovery Algorithms
Next we show through comparative evaluation that considering
multi-collinearity is important for discovering causal dependencies.
We compare Giza with WISE [28] which are multi-variate analy-
sis techniques. We do not present comparison with Sherlock [4],
Orion [7], or NICE [23] because they only rely on pair-wise cor-
relations. A qualitative comparison of all these techniques is pro-
vided in Section 5.
To compare Giza and WISE, we use one-week worth of syslog
data aggregated at VHO and SHO resolutions. We consider 80 dif-
ferent VHOs and SHOs in which we construct 1318 different types
of event-series including layer-1 alarms (Ethernet, SONET, port er-
rors), protocol state changes (MPLS, OSPF, BGP, PIM, SAP, SDP),
link ﬂaps, conﬁguration changes, and CPU activities. To set up
ground truth, we have resorted to domain experts and have con-
structed 482 causal rules (indicated by the presence of edges and
their directionality in causal graph). An example rule is “a link
down causes OSPF protocol to change state”. We consider these
rules as a subset of the causal relationships that should be identiﬁed
by the causal discovery algorithm – the complete set of the causal
relationships requires perfect domain knowledge and is nearly im-
possible to obtain.
Table 4 compares the causal discovery result generated by the
three algorithms: (i) WISE partial correlations plus linear regres-
sion (a widely used approach in data mining, such as in [9, 10]),
(ii) ℓ1 norm minimization combined with lag correlation using sta-
tistical change detection, and (iii) ℓ1 norm minimization combined
with lag correlation using maximum in the ranges. The latter two
are what we have described in Section 3.5.2. In the cases where
we cannot conclusively determine the causal direction of an iden-
tiﬁed correlation, we construct two directional edges between the
pair of events (two different rules). We determine the accuracy of
the above algorithms by comparing their results with our subset of
ground truth. An edge (out of the 482 rules) is considered a match
if both its existence and its directionality have been correctly iden-
tiﬁed. We observe that either of our approaches signiﬁcantly out-
performs the partial correlation and linear regression approach in
accuracy. Since accuracy solely does not reﬂect the performance
of the inference algorithm, (for example, a dummy algorithm that
blindly mark all edges in the causal graph would achieve 100% ac-
curacy), we also need to consider the false positives. Since we do
not have the complete ground truth, we can use the total number of
edges identiﬁed as a reference point. We observe that partial cor-
relation and linear regression identiﬁes more than four times of the
edges while still achieving around 13% less accuracy compared to
our approaches. This demonstrates the strength of our approaches
– the high degree of multi-collinearity of the data has been properly
accounted for.
The two lag correlation and ℓ1 norm minimization based ap-
proaches have highly similar performance. We include both in Giza
as method for causal dependency discovery for completeness.
238QQ plot at CO
QQ plot at Metro
QQ plot at Region
3
2.5
2
1.5
1
0.5
)
e