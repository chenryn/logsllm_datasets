Recogition.
https://azure .microsoft.com/en-us/services/cognitive-services/
speaker-recognition/.
[11] 2021. Mozilla TTS. https://github .com/mozilla/TTS
[12] 2021. Resemble.AI. https://www.resemble .ai/
[13] 2021. TensorﬂowTTS. https://github .com/TensorSpeech/TensorFlowTTS
[14] 2021. Voxforge. http://www.voxforge .org/
[15] 2021.
VoicePrint
WeChat
Documentation.
https://help .wechat.com/cgi-bin/micromsg-bin/oshelpcenter?
opcode=2&plat=ios&lang=en&id=150819uqYnUR150819YzINVb.
[16] Muhammad Ejaz Ahmed, Il-Youp Kwak, Jun Ho Huh, Iljoo Kim, Taekkyung Oh,
and Hyoungshick Kim. 2020. Void: A fast and light voice liveness detection
system. In Proc. of USENIX.
[17] Ehab A AlBadawy, Siwei Lyu, and Hany Farid. 2019. Detecting AI-Synthesized
Speech Using Bispectral Analysis.. In CVPR Workshops.
[18] Federico Alegre, Artur Janicki, and Nicholas Evans. 2014. Re-assessing the
threat of replay spooﬁng attacks against automatic speaker veriﬁcation. In Proc.
of BIOSIG.
[19] Gopala K Anumanchipalli, Josh Chartier, and Edward F Chang. 2019. Speech
synthesis from neural decoding of spoken sentences. Nature (2019).
[20] Gopala Krishna Anumanchipalli, Kishore Prahallad, and Alan W Black. 2011.
Festvox: Tools for creation and analyses of large speech corpora. In Workshop
on Very Large Scale Phonetics Research.
[21] Sercan O Arik, Jitong Chen, Kainan Peng, Wei Ping, and Yanqi Zhou. 2018. Neu-
ral voice cloning with a few samples. Proc. of NeurIPs (2018).
[22] Pascal Belin, Patricia E G Bestelmeyer, Marianne Latinus, and Rebecca Watson.
2011. Understanding voice perception. British J. of Psychol. 102, 4 (Nov. 2011),
711–725.
[23] Frédéric Bimbot, Jean-François Bonastre, Corinne Fredouille, et al. [n.d.]. A tu-
torial on text-independent speaker veriﬁcation. EURASIP Journal on Advances
in Signal Processing 2004 ([n. d.]).
[24] Maximilian Bisani and Hermann Ney. 2008.
Joint-sequence models for
grapheme-to-phoneme conversion. Speech Communication 50, 5 (2008), 434–
451. https://doi .org/10 .1016/j .specom .2008 .01 .002
[25] Guangke Chen, Sen Chen, Lingling Fan, Xiaoning Du, Zhe Zhao, Fu Song, and
Yang Liu. 2019. Who is real bob? adversarial attacks on speaker recognition
systems. arXiv preprint arXiv:1911.01840 (2019).
[26] Si Chen, Kui Ren, Sixu Piao, Cong Wang, Qian Wang, Jian Weng, Lu Su, and
Aziz Mohaisen. 2017. You can hear but you cannot steal: Defending against
voice impersonation attacks on smartphones. In Proc. of ICDS.
[27] Jemine Corentin. 2019. Master thesis : Real-Time Voice Cloning.
(2019).
https://matheo.uliege .be/handle/2268 .2/6801
[28] Phillip L De Leon, Vijendra Raj Apsingekar, Michael Pucher, and Junichi Ya-
magishi. 2010. Revisiting the security of speaker veriﬁcation systems against
imposture using synthetic speech. In Proc. of ICASSP.
[29] Mattia A Di Gangi, Matteo Negri, and Marco Turchi. 2019. Adapting trans-
former to end-to-end spoken language translation. In Proc. of INTERSPEECH.
[30] Grant Fairbanks. 1960. Voice and articulation drillbook. Addison-Wesley Educa-
tional Publishers.
[31] Sadaoki Furui. 1981. Cepstral analysis technique for automatic speaker veriﬁ-
cation. Proc. of ICASSP (1981).
[32] Haichang Gao, Honggang Liu, Dan Yao, Xiyang Liu, and Uwe Aickelin. 2010.
An audio CAPTCHA to distinguish humans from computers. In 2010 Third In-
ternational Symposium on Electronic Commerce and Security. IEEE, 265–269.
[33] Yang Gao, Jiachen Lian, Bhiksha Raj, and Rita Singh. 2021. Detection and Evalu-
ation of human and machine generated speech in spooﬁng attacks on automatic
speaker veriﬁcation systems. In Proc. of IEEE SLT Workshop.
[34] Rosa González Hautamäki, Md Sahidullah, Ville Hautamäki, and Tomi Kin-
nunen. 2017. Acoustical and perceptual study of voice disguise by age mod-
iﬁcation in speaker veriﬁcation. Speech Communication (2017).
[35] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. 2016. End-
to-end text-dependent speaker veriﬁcation. In Proc. of ICASSP. IEEE.
[36] Wei-Ning Hsu, Yu Zhang, Ron J Weiss, Heiga Zen, Yonghui Wu, Yuxuan Wang,
Yuan Cao, Ye Jia, Zhifeng Chen, Jonathan Shen, et al. 2019. Hierarchical gener-
ative modeling for controllable speech synthesis. Proc. of ICLR (2019).
[37] Qiong Hu, Erik Marchi, David Winarsky, Yannis Stylianou, Devang Naik, and
Sachin Kajarekar. 2019. Neural text-to-speech adaptation from low quality pub-
lic recordings. In Speech Synthesis Workshop, Vol. 10.
[38] Chien-yu Huang, Yist Y Lin, Hung-yi Lee, and Lin-shan Lee. 2021. Defending
Your Voice: Adversarial Attack on Voice Conversion. Proc. of IEEE SLT Work-
shop (2021).
[39] Artur Janicki, Federico Alegre, and Nicholas Evans. 2016. An assessment of au-
tomatic speaker veriﬁcation vulnerabilities to replay spooﬁng attacks. Security
and Communication Networks (2016).
[40] Corentin
Time
https://github .com/CorentinJ/Real-Time-Voice-Cloning
Jemine.
2020.
Real
Voice
Cloning.
[41] Ye Jia, Yu Zhang, Ron Weiss, Quan Wang, Jonathan Shen, Fei Ren, Patrick
Nguyen, Ruoming Pang, Ignacio Lopez Moreno, Yonghui Wu, et al. 2018. Trans-
fer learning from speaker veriﬁcation to multispeaker text-to-speech synthesis.
Proc. of NeurIPs (2018).
[42] Hirokazu Kameoka, Takuhiro Kaneko, Kou Tanaka, and Nobukatsu Hojo. 2018.
Stargan-VC: Non-parallel many-to-many voice conversion using star genera-
tive adversarial networks. In Proc. of IEEE SLT Workshop.
[43] Elie Khoury, Laurent El Shafey, and Sébastien Marcel. 2014. Spear: An open
source toolbox for speaker recognition based on Bob. In Proc. of ICASSP.
[44] Tomi Kinnunen, Md Sahidullah, Héctor Delgado, Massimiliano Todisco,
Nicholas Evans, Junichi Yamagishi, and Kong Aik Lee. 2017. The ASVspoof
2017 challenge: Assessing the limits of replay spooﬁng attack detection. (2017).
[45] Tomi Kinnunen, Zhi-Zheng Wu, Kong Aik Lee, Filip Sedlak, Eng Siong Chng,
and Haizhou Li. 2012. Vulnerability of speaker veriﬁcation systems against
voice conversion spooﬁng attacks: The case of telephone speech. In Proc. of
ICASSP.
[46] John Kominek and Alan W Black. 2003. CMU ARCTIC databases for speech
synthesis. Carnegie Mellon University, Language Technologies Institute Tech Re-
port CMU-LTI-03-177 (2003).
[47] Michael W. Kraus, Brittany Torrez, Jun Won Park, and Fariba Ghayebi. 2019.
Evidence for the reproduction of social class in brief speech. Proc. of National
Academy of Sciences 114, 46 (Nov. 2019), 22998–23003.
[48] Felix Kreuk, Yossi Adi, Moustapha Cisse, and Joseph Keshet. 2018. Fooling end-
to-end speaker veriﬁcation with adversarial examples. In Proc. of ICASSP.
[49] Gautam Krishna, Co Tran, Yan Han, Mason Carnahan, and Ahmed H Tewﬁk.
2020. Speech synthesis using EEG. In Proc. of ICASSP.
[50] I Han Kuo, Joel Marcus Rabindran, Elizabeth Broadbent, Yong In Lee, Ngaire
Kerse, Rebecca MQ Staﬀord, and Bruce A MacDonald. 2009. Age and gender
factors in user acceptance of healthcare robots. In Proc. of RO-MAN. IEEE.
[51] Yee Wah Lau, Michael Wagner, and Dat Tran. 2004. Vulnerability of speaker
veriﬁcation to voice mimicking. In Proc. of ISIMP.
[52] Galina Lavrentyeva, Sergey Novoselov, Egor Malykh, Alexander Kozlov, Oleg
Kudashev, and Vadim Shchemelinin. 2017. Audio Replay Attack Detection with
Deep Learning Frameworks.. In Proc. of INTERSPEECH.
[53] Zhuohang Li, Cong Shi, Yi Xie, Jian Liu, Bo Yuan, and Yingying Chen. 2020.
Practical adversarial attacks against speaker recognition systems. In Proc. of
HotMobile.
[54] A Lieto, D Moro, F Devoti, C Parera, Vincenzo Lipari, Paolo Bestagini, and Ste-
fano Tubaro. 2019. "Hello? Who Am I Talking to?" A Shallow CNN Approach
for Human vs. Bot Speech Classiﬁcation. In Proc. of ICASSP.
[55] Sabrina López, Pablo Riera, María Florencia Assaneo, Manuel Eguía, Mariano
Sigman, and Marcos A Trevisan. 2013. Vocal caricatures reveal signatures of
speaker identity. Scientiﬁc Reports (2013).
[56] Takashi Masuko, Takafumi Hitotsumatsu, Keiichi Tokuda, and Takao
Kobayashi. 1999. On the security of HMM-based speaker veriﬁcation systems
against imposture using synthetic speech. In Proc. of 6th European Conference
on Speech Communication and Technology.
[57] Dibya Mukhopadhyay, Maliheh Shirvanian, and Nitesh Saxena. 2015. All your
voices are belong to us: Stealing voices to fool humans and machines. In Proc.
of ESORICS.
[58] John Mullennix and Steven Stern. 2010. Computer Synthesized Speech Technolo-
gies: Tools for Aiding Impairment. IGI Global.
[59] Arsha Nagrani, Joon Son Chung, Weidi Xie, and Andrew Zisserman. 2020. Vox-
celeb: Large-scale speaker veriﬁcation in the wild. Computer Speech & Language
(2020).
[60] Ajaya Neupane, Nitesh Saxena, Leanne M Hirshﬁeld, and Sarah E Bratt. 2019.
The Crux of Voice (In) Security: A Brain Study of Speaker Legitimacy Detec-
tion.. In Proc. of NDSS.
[61] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015.
Librispeech: an asr corpus based on public domain audio books. In Proc. of
ICASSP.
Session 1D: Authentication and Click Fraud CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea248[62] Pavol Partila, Jaromir Tovarek, Gokhan Hakki Ilk, Jan Rozhon, and Miroslav
Voznak. 2020. Deep Learning Serves Voice Cloning: How Vulnerable Are Auto-
matic Speaker Veriﬁcation Systems to Spooﬁng Trials? IEEE Communications
Magazine (2020).
[63] Cyril R. Pernet and Pascal Belin. 2012. The role of pitch and timbre in voice
gender categorization. Frontiers in Psychology 3 (Feb 2012).
[64] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O Arik, Ajay Kannan, Sha-
ran Narang, Jonathan Raiman, and John Miller. 2018. DeepVoice 3: Scaling
text-to-speech with convolutional sequence learning. Proc. of ICLR (2018).
[65] Kaizhi
Qian.
2021.
AutoVC
Github
Implementation.
https://github .com/auspicious3000/autovc
[66] Kaizhi Qian, Yang Zhang, Shiyu Chang, Mark Hasegawa-Johnson, and David
Cox. 2020. Unsupervised speech decomposition via triple information bottle-
neck. In Proc. of ICML.
[67] Kaizhi Qian, Yang Zhang, Shiyu Chang, Xuesong Yang, and Mark Hasegawa-
Johnson. 2019. Autovc: Zero-shot voice style transfer with only autoencoder
loss. Proc. of ICML (2019).
[68] Yao Qin, Nicholas Carlini, Garrison Cottrell, Ian Goodfellow, and Colin Raﬀel.
2019. Imperceptible, robust, and targeted adversarial examples for automatic
speech recognition. In Proc. of ICMR.
[69] Yurii Rebryk and Stanislav Beliaev. 2020. ConVoice: Real-Time Zero-Shot Voice
Style Transfer with Convolutional Network. arXiv preprint arXiv:2005.07815
(2020).
[70] Douglas A Reynolds, Thomas F Quatieri, and Robert B Dunn. 2000. Speaker
veriﬁcation using adapted Gaussian mixture models. Digital signal processing
10 (2000).
[71] Aaron E Rosenberg. 1976. Automatic speaker veriﬁcation: A review.
IEEE
(1976).
[72] J. Saldana. 2009. The coding manual for qualitative researchers. Sage Publica-
tions Limited (2009).
[73] Aaron Sell, Gregory A. Bryant, Leda Cosmides, John Tooby, Daniel Sznycer,
et al. 2010. Adaptations in humans for assessing physical strength from the
voice. Proc. of the Royal Society B 277 (June 2010), 3509–3518.
[74] Joan Serrà, Santiago Pascual, and Carlos Segura. 2019. Blow: a single-scale
hyperconditioned ﬂow for non-parallel raw-audio voice conversion. Proc. of
NeurIPs (2019).
[75] Neeraj Kumar Sharma, Shobhana Ganesh, Sriram Ganapathy, and Lori L Holt.
2019. Talker change detection: A comparison of human and machine perfor-
mance. The Journal of the Acoustical Society of America (2019).
[76] Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster, Navdeep Jaitly,
Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, Rj Skerrv-Ryan, et al.
2018. Natural TTS synthesis by conditioning wavenet on mel spectrogram pre-
dictions. In Proc. of ICASSP.
[77] Maliheh Shirvanian, Manar Mohammed, Nitesh Saxena, and S Abhishek Anand.
2020. Voicefox: Leveraging Inbuilt Transcription to Enhance the Security of
Machine-Human Speaker Veriﬁcation against Voice Synthesis Attacks. In An-
nual Computer Security Applications Conference. 870–883.
[78] Maliheh Shirvanian and Nitesh Saxena. 2014. Wiretapping via mimicry: Short
voice imitation man-in-the-middle attacks on crypto phones. In Proc. of CCS.
[79] Maliheh Shirvanian, Summer Vo, and Nitesh Saxena. 2019. Quantifying the
Breakability of Voice Assistants. In Proc. of PerCom.
[80] Dan Simmons. 2017. BBC Fools HSBC Voice Recognition System.
(2017).
https://www.bbc .com/news/technology-39965545
[81] David Snyder, Pegah Ghahremani, Daniel Povey, Daniel Garcia-Romero, Yishay
Carmiel, and Sanjeev Khudanpur. 2016. Deep neural network-based speaker
embeddings for end-to-end speaker veriﬁcation. In Proc. of IEEE SLT Workshop.
[82] Catherine Stupp. 2019. Fraudsters Used AI to Mimic CEO’s Voice in Unusual
Cybercrime Case. Wall Street Journal (August 2019).
[83] Yaniv Taigman, Lior Wolf, Adam Polyak, and Eliya Nachmani. 2018. Voiceloop:
Voice ﬁtting and synthesis via a phonological loop. Proc. of ICLR (2018).
[84] Rie Tamagawa, Catherine I Watson, I Han Kuo, Bruce A MacDonald, and Eliza-
beth Broadbent. 2011. The eﬀects of synthesized voice accents on user percep-
tions of robots. International Journal of Social Robotics 3 (2011).
[85] Ehsan Variani, Xin Lei, Erik McDermott, Ignacio Lopez Moreno, and Javier
Gonzalez-Dominguez. 2014. Deep neural networks for small footprint text-
dependent speaker veriﬁcation. In Proc. of ICASSP. IEEE.
[86] Ville Vestman, Tomi Kinnunen, Rosa González Hautamäki, and Md Sahidullah.
2020. Voice mimicry attacks assisted by automatic speaker veriﬁcation. Com-
puter Speech & Language (2020).
[87] James Vincent. 2018. Google’s AI sounds like a human on the phone – should
we be worried. The Verge (May 2018).
[88] Robbie Vogt and Sridha Sridharan. 2008. Explicit modelling of session variabil-
ity for speaker veriﬁcation. Computer Speech & Language (2008).
[89] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez Moreno. 2018. Generalized
End-to-End Loss for Speaker Veriﬁcation. In Proc. of ICASSP.
[90] Qian Wang, Xiu Lin, Man Zhou, Yanjiao Chen, Cong Wang, Qi Li, and Xi-
angyang Luo. 2019. Voicepop: A pop noise based anti-spooﬁng system for voice
authentication on smartphones. In Proc. of INFOCOM.
[91] Run Wang, Felix Juefei-Xu, Yihao Huang, Qing Guo, Xiaofei Xie, Lei Ma, and
Yang Liu. 2020. DeepSonar: Towards Eﬀective and Robust Detection of AI-
Synthesized Fake Voices. arXiv preprint arXiv:2005.13770 (2020).
[92] Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J Weiss,
Navdeep Jaitly, Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, et al.
2017. Tacotron: Towards end-to-end speech synthesis. Proc. of INTERSPEECH
(2017).
[93] Robert Weide. 1998.
The Carnegie Mellon pronouncing dictionary.
http://www.speech .cs .cmu .edu/cgi-bin/cmudict
[94] Steven H. Weinberger. 2013. Speech Accent Archive. George Mason University.
[95] Da-Yi Wu, Yen-Hao Chen, and Hung-Yi Lee. 2020. Vqvc+: One-shot voice
arXiv preprint
conversion by vector quantization and u-net architecture.
arXiv:2006.04154 (2020).
[96] Zhizheng Wu, Tomi Kinnunen, Nicholas Evans, Junichi Yamagishi, Cemal
Hanilçi, Md Sahidullah, and Aleksandr Sizov. 2015. ASVspoof 2015: the ﬁrst au-
tomatic speaker veriﬁcation spooﬁng and countermeasures challenge. In Proc.
of ISCA.
[97] Junichi Yamagishi, Christophe Veaux, and Kirsten. MacDonald. [n.d.]. CSTR
VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit.
([n. d.]). https://doi .org/10 .7488/ds/2645
[98] Chen Yan, Yan Long, Xiaoyu Ji, and Wenyuan Xu. 2019. The Catcher in the Field:
A Fieldprint based Spooﬁng Detection for Text-Independent Speaker Veriﬁca-
tion. In Proc. of CCS.
[99] R. Zaske and S. R. Schweinberger. 2011. You are only as old as you sound:
auditory aftereﬀects in vocal age perception. Hearing Research 282, 1-2 (Dec.
2011), 283–288.
[100] Linghan Zhang, Sheng Tan, and Jie Yang. 2017. Hearing your voice is not
enough: An articulatory gesture based liveness detection for voice authenti-
cation. In Proc. of CCS.
Session 1D: Authentication and Click Fraud CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2498 APPENDIX
8.1 Methodology for §4.1
Here, we describe the methodology used to recreate [57] in §4.1.
Systems Used. We use the following synthesis and SR systems.
• Festvox [20] is a text-dependent voice conversion system that
uses GMMs [70] to model the vocal characteristics. Training the
system requires 4-8 minutes of speech from both the source and
target speakers and takes approximately 5 - 10 minutes.
• Bob Spear [43] is an open-source speaker recognition system. It
uses classical statistical techniques (Universal Background Model
Gaussian Mixture Models (UBM-GMM) and Inter-Session Vari-
ability (ISV)) to perform speaker recognition [88].
• Resemblyzer, Azure See §3.
Datasets Used. The prior attack uses the CMU ARCTIC and Vox-
forge datasets. CMU ARCTIC [46] contains 1150 spoken phrases
from male and female English speakers. Voxforge [14] is an open-
source dataset of transcribed speech. The subset we use contains
6561 spoken phrases, each approximately 5 seconds long, from 30
English speakers.
Attack Implementation. We recreate the exact setup of the
voice conversion attack from [57]. We use Festvox to convert speech
from two CMU ARCTIC speakers (one male, one female) to imi-
tate 10 speakers of the Voxforge dataset. The synthesized attack
samples are tested against both of Bob Spear’s speaker recogni-
tion algorithms (UBM-GMM and ISV), along with Resemblyzer and
Azure. We measure attack performance using the attack success
rate (AS), which denotes the percent of synthesized samples iden-
tiﬁed as the target.