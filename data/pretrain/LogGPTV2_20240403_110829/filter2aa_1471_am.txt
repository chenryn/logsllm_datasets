pending APC (using SleepEx). In both cases, if a user-mode APC is pending,
the kernel interrupts (alerts) the thread, transfers control to the APC routine,
and resumes the thread’s execution when the APC routine completes. Unlike
kernel-mode APCs, which can execute at APC_LEVEL, user-mode APCs
execute at PASSIVE_LEVEL.
APC delivery can reorder the wait queues—the lists of which threads are
waiting for what, and in what order they are waiting. (Wait resolution is
described in the section “Low-IRQL synchronization,” later in this chapter.)
If the thread is in a wait state when an APC is delivered, after the APC
routine completes, the wait is reissued or re-executed. If the wait still isn’t
resolved, the thread returns to the wait state, but now it will be at the end of
the list of objects it’s waiting for. For example, because APCs are used to
suspend a thread from execution, if the thread is waiting for any objects, its
wait is removed until the thread is resumed, after which that thread will be at
the end of the list of threads waiting to access the objects it was waiting for.
A thread performing an alertable kernel-mode wait will also be woken up
during thread termination, allowing such a thread to check whether it woke
up as a result of termination or for a different reason.
Timer processing
The system’s clock interval timer is probably the most important device on a
Windows machine, as evidenced by its high IRQL value (CLOCK_LEVEL)
and due to the critical nature of the work it is responsible for. Without this
interrupt, Windows would lose track of time, causing erroneous results in
calculations of uptime and clock time—and worse, causing timers to no
longer expire, and threads never to consume their quantum. Windows would
also not be a preemptive operating system, and unless the current running
thread yielded the CPU, critical background tasks and scheduling could never
occur on a given processor.
Timer types and intervals
Traditionally, Windows programmed the system clock to fire at some
appropriate interval for the machine, and subsequently allowed drivers,
applications, and administrators to modify the clock interval for their needs.
This system clock thus fired in a fixed, periodic fashion, maintained by either
by the Programmable Interrupt Timer (PIT) chip that has been present on all
computers since the PC/AT or the Real Time Clock (RTC). The PIT works
on a crystal that is tuned at one-third the NTSC color carrier frequency
(because it was originally used for TV-Out on the first CGA video cards),
and the HAL uses various achievable multiples to reach millisecond-unit
intervals, starting at 1 ms all the way up to 15 ms. The RTC, on the other
hand, runs at 32.768 kHz, which, by being a power of two, is easily
configured to run at various intervals that are also powers of two. On RTC-
based systems, the APIC Multiprocessor HAL configured the RTC to fire
every 15.6 milliseconds, which corresponds to about 64 times a second.
The PIT and RTC have numerous issues: They are slow, external devices
on legacy buses, have poor granularity, force all processors to synchronize
access to their hardware registers, are a pain to emulate, and are increasingly
no longer found on embedded hardware devices, such as IoT and mobile. In
response, hardware vendors created new types of timers, such as the ACPI
Timer, also sometimes called the Power Management (PM) Timer, and the
APIC Timer (which lives directly on the processor). The ACPI Timer
achieved good flexibility and portability across hardware architectures, but
its latency and implementation bugs caused issues. The APIC Timer, on the
other hand, is highly efficient but is often already used by other platform
needs, such as for profiling (although more recent processors now have
dedicated profiling timers).
In response, Microsoft and the industry created a specification called the
High Performance Event Timer, or HPET, which a much-improved version
of the RTC. On systems with an HPET, it is used instead of the RTC or PIC.
Additionally, ARM64 systems have their own timer architecture, called the
Generic Interrupt Timer (GIT). All in all, the HAL maintains a complex
hierarchy of finding the best possible timer on a given system, using the
following order:
1. 
Try to find a synthetic hypervisor timer to avoid any kind of
emulation if running inside of a virtual machine.
2. 
On physical hardware, try to find a GIT. This is expected to work
only on ARM64 systems.
3. 
If possible, try to find a per-processor timer, such as the Local APIC
timer, if not already used.
4. 
Otherwise, find an HPET—going from an MSI-capable HPET to a
legacy periodic HPET to any kind of HPET.
5. 
If no HPET was found, use the RTC.
6. 
If no RTC is found, try to find some other kind of timer, such as the
PIT or an SFI Timer, first trying to find ones that support MSI
interrupts, if possible.
7. 
If no timer has yet been found, the system doesn’t actually have a
Windows compatible timer, which should never happen.
The HPET and the LAPIC Timer have one more advantage—other than
only supporting the typical periodic mode we described earlier, they can also
be configured in a one shot mode. This capability will allow recent versions
of Windows to leverage a dynamic tick model, which we explain later.
Timer granularity
Some types of Windows applications require very fast response times, such
as multimedia applications. In fact, some multimedia tasks require rates as
low as 1 ms. For this reason, Windows from early on implemented APIs and
mechanisms that enable lowering the interval of the system’s clock interrupt,
which results in more frequent clock interrupts. These APIs do not adjust a
particular timer’s specific rate (that functionality was added later, through
enhanced timers, which we cover in an upcoming section); instead, they end
up increasing the resolution of all timers in the system, potentially causing
other timers to expire more frequently, too.
That being said, Windows tries its best to restore the clock timer back to
its original value whenever it can. Each time a process requests a clock
interval change, Windows increases an internal reference count and
associates it with the process. Similarly, drivers (which can also change the
clock rate) get added to the global reference count. When all drivers have
restored the clock and all processes that modified the clock either have exited
or restored it, Windows restores the clock to its default value (or barring that,
to the next highest value that’s been required by a process or driver).
EXPERIMENT: Identifying high-frequency timers
Due to the problems that high-frequency timers can cause,
Windows uses Event Tracing for Windows (ETW) to trace all
processes and drivers that request a change in the system’s clock
interval, displaying the time of the occurrence and the requested
interval. The current interval is also shown. This data is of great use
to both developers and system administrators in identifying the
causes of poor battery performance on otherwise healthy systems,
as well as to decrease overall power consumption on large systems.
To obtain it, simply run powercfg /energy, and you should obtain
an HTML file called energy-report.html, similar to the one shown
here:
Scroll down to the Platform Timer Resolution section, and you
see all the applications that have modified the timer resolution and
are still active, along with the call stacks that caused this call.
Timer resolutions are shown in hundreds of nanoseconds, so a
period of 20,000 corresponds to 2 ms. In the sample shown, two
applications—namely, Microsoft Edge and the TightVNC remote
desktop server—each requested a higher resolution.
You can also use the debugger to obtain this information. For
each process, the EPROCESS structure contains the fields shown
next that help identify changes in timer resolution:
Click here to view code image
   +0x4a8 TimerResolutionLink : _LIST_ENTRY [ 
0xfffffa80’05218fd8 - 0xfffffa80’059cd508 ]
   +0x4b8 RequestedTimerResolution : 0
   +0x4bc ActiveThreadsHighWatermark : 0x1d
   +0x4c0 SmallestTimerResolution : 0x2710
   +0x4c8 TimerResolutionStackRecord : 0xfffff8a0’0476ecd0 
_PO_DIAG_STACK_RECORD
Note that the debugger shows you an additional piece of
information: the smallest timer resolution that was ever requested
by a given process. In this example, the process shown corresponds
to PowerPoint 2010, which typically requests a lower timer
resolution during slideshows but not during slide editing mode. The
EPROCESS fields of PowerPoint, shown in the preceding code,
prove this, and the stack could be parsed by dumping the
PO_DIAG_STACK_RECORD structure.
Finally, the TimerResolutionLink field connects all processes
that have made changes to timer resolution, through the
ExpTimerResolutionListHead doubly linked list. Parsing this list
with the debugger data model can reveal all processes on the
system that have, or had, made changes to the timer resolution,
when the powercfg command is unavailable or information on past
processes is required. For example, this output shows that Edge, at
various points, requested a 1 ms resolution, as did the Remote
Desktop Client, and Cortana. WinDbg Preview, however, now only
previously requested it but is still requesting it at the moment this
command was written.
Click here to view code image
lkd> dx -g Debugger.Utility.Collections.FromListEntry(*
(nt!_LIST_ENTRY*)&nt!ExpTimerReso
lutionListHead, "nt!_EPROCESS", 
"TimerResolutionLink").Select(p => new { Name = ((char*)
p.ImageFileName).ToDisplayString("sb"), Smallest = 
p.SmallestTimerResolution, Requested =
p.RequestedTimerResolution}),d
======================================================
=         = Name              = Smallest = Requested =
======================================================
= [0]     - msedge.exe        - 10000    - 0         =
= [1]     - msedge.exe        - 10000    - 0         =
= [2]     - msedge.exe        - 10000    - 0         =
= [3]     - msedge.exe        - 10000    - 0         =
= [4]     - mstsc.exe         - 10000    - 0         =
= [5]     - msedge.exe        - 10000    - 0         =
= [6]     - msedge.exe        - 10000    - 0         =
= [7]     - msedge.exe        - 10000    - 0         =
= [8]     - DbgX.Shell.exe    - 10000    - 10000     =
= [9]     - msedge.exe        - 10000    - 0         =
= [10]    - msedge.exe        - 10000    - 0         =
= [11]    - msedge.exe        - 10000    - 0         =
= [12]    - msedge.exe        - 10000    - 0         =
= [13]    - msedge.exe        - 10000    - 0         =
= [14]    - msedge.exe        - 10000    - 0         =
= [15]    - msedge.exe        - 10000    - 0         =
= [16]    - msedge.exe        - 10000    - 0         =
= [17]    - msedge.exe        - 10000    - 0         =
= [18]    - msedge.exe        - 10000    - 0         =
= [19]    - SearchApp.exe     - 40000    - 0         =
======================================================
Timer expiration
As we said, one of the main tasks of the ISR associated with the interrupt that
the clock source generates is to keep track of system time, which is mainly
done by the KeUpdateSystemTime routine. Its second job is to keep track of
logical run time, such as process/thread execution times and the system tick
time, which is the underlying number used by APIs such as GetTickCount
that developers use to time operations in their applications. This part of the
work is performed by KeUpdateRunTime. Before doing any of that work,
however, KeUpdateRunTime checks whether any timers have expired.
Windows timers can be either absolute timers, which implies a distinct
expiration time in the future, or relative timers, which contain a negative
expiration value used as a positive offset from the current time during timer
insertion. Internally, all timers are converted to an absolute expiration time,
although the system keeps track of whether this is the “true” absolute time or
a converted relative time. This difference is important in certain scenarios,
such as Daylight Savings Time (or even manual clock changes). An absolute
timer would still fire at 8:00 p.m. if the user moved the clock from 1:00 p.m.
to 7:00 p.m., but a relative timer—say, one set to expire “in two hours”—
would not feel the effect of the clock change because two hours haven’t
really elapsed. During system time-change events such as these, the kernel
reprograms the absolute time associated with relative timers to match the
new settings.
Back when the clock only fired in a periodic mode, since its expiration was
at known interval multiples, each multiple of the system time that a timer
could be associated with is an index called a hand, which is stored in the
timer object’s dispatcher header. Windows used that fact to organize all
driver and application timers into linked lists based on an array where each
entry corresponds to a possible multiple of the system time. Because modern
versions of Windows 10 no longer necessarily run on a periodic tick (due to
the dynamic tick functionality), a hand has instead been redefined as the
upper 46 bits of the due time (which is in 100 ns units). This gives each hand
an approximate “time” of 28 ms. Additionally, because on a given tick
(especially when not firing on a fixed periodic interval), multiple hands could
have expiring timers, Windows can no longer just check the current hand.
Instead, a bitmap is used to track each hand in each processor’s timer table.
These pending hands are found using the bitmap and checked during every
clock interrupt.
Regardless of method, these 256 linked lists live in what is called the timer
table—which is in the PRCB—enabling each processor to perform its own
independent timer expiration without needing to acquire a global lock, as
shown in Figure 8-19. Recent builds of Windows 10 can have up to two
timer tables, for a total of 512 linked lists.
Figure 8-19 Example of per-processor timer lists.
Later, you will see what determines which logical processor’s timer table a
timer is inserted on. Because each processor has its own timer table, each
processor also does its own timer expiration work. As each processor gets
initialized, the table is filled with absolute timers with an infinite expiration
time to avoid any incoherent state. Therefore, to determine whether a clock
has expired, it is only necessary to check if there are any timers on the linked
list associated with the current hand.
Although updating counters and checking a linked list are fast operations,
going through every timer and expiring it is a potentially costly operation—
keep in mind that all this work is currently being performed at
CLOCK_LEVEL, an exceptionally elevated IRQL. Similar to how a driver
ISR queues a DPC to defer work, the clock ISR requests a DPC software
interrupt, setting a flag in the PRCB so that the DPC draining mechanism
knows timers need expiration. Likewise, when updating process/thread
runtime, if the clock ISR determines that a thread has expired its quantum, it
also queues a DPC software interrupt and sets a different PRCB flag. These
flags are per-PRCB because each processor normally does its own processing
of run-time updates because each processor is running a different thread and
has different tasks associated with it. Table 8-10 displays the various fields
used in timer expiration and processing.
Table 8-10 Timer processing KPRCB fields
KPRCB 
Field
Type
Description
LastTime
rHand
Index 
(up to 
256)
The last timer hand that was processed by this 
processor. In recent builds, part of TimerTable 
because there are now two tables.
ClockOw
ner
Boole
an
Indicates whether the current processor is the 
clock owner.
TimerTab
le
KTI
MER
_TA
BLE
List heads for the timer table lists (256, or 512 on 
more recent builds).
DpcNorm
alTimerE
xpiration
Bit
Indicates that a DISPATCH_LEVEL interrupt has 
been raised to request timer expiration.
DPCs are provided primarily for device drivers, but the kernel uses them,
too. The kernel most frequently uses a DPC to handle quantum expiration. At
every tick of the system clock, an interrupt occurs at clock IRQL. The clock
interrupt handler (running at clock IRQL) updates the system time and then
decrements a counter that tracks how long the current thread has run. When
the counter reaches 0, the thread’s time quantum has expired, and the kernel
might need to reschedule the processor, a lower-priority task that should be
done at DPC/dispatch IRQL. The clock interrupt handler queues a DPC to
initiate thread dispatching and then finishes its work and lowers the
processor’s IRQL. Because the DPC interrupt has a lower priority than do
device interrupts, any pending device interrupts that surface before the clock
interrupt completes are handled before the DPC interrupt occurs.
Once the IRQL eventually drops back to DISPATCH_LEVEL, as part of
DPC processing, these two flags will be picked up.
Chapter 4 of Part 1 covers the actions related to thread scheduling and
quantum expiration. Here, we look at the timer expiration work. Because the
timers are linked together by hand, the expiration code (executed by the DPC
associated with the PRCB in the TimerExpirationDpc field, usually
KiTimerExpirationDpc) parses this list from head to tail. (At insertion time,
the timers nearest to the clock interval multiple will be first, followed by
timers closer and closer to the next interval but still within this hand.) There
are two primary tasks to expiring a timer:
■    The timer is treated as a dispatcher synchronization object (threads
are waiting on the timer as part of a timeout or directly as part of a
wait). The wait-testing and wait-satisfaction algorithms will be run on
the timer. This work is described in a later section on synchronization
in this chapter. This is how user-mode applications, and some drivers,
make use of timers.
■    The timer is treated as a control object associated with a DPC
callback routine that executes when the timer expires. This method is
reserved only for drivers and enables very low latency response to
timer expiration. (The wait/dispatcher method requires all the extra
logic of wait signaling.) Additionally, because timer expiration itself
executes at DISPATCH_LEVEL, where DPCs also run, it is perfectly
suited as a timer callback.
As each processor wakes up to handle the clock interval timer to perform
system-time and run-time processing, it therefore also processes timer
expirations after a slight latency/delay in which the IRQL drops from
CLOCK_LEVEL to DISPATCH_LEVEL. Figure 8-20 shows this behavior on
two processors—the solid arrows indicate the clock interrupt firing, whereas
the dotted arrows indicate any timer expiration processing that might occur if
the processor had associated timers.
Figure 8-20 Timer expiration.
Processor selection
A critical determination that must be made when a timer is inserted is to pick
the appropriate table to use—in other words, the most optimal processor
choice. First, the kernel checks whether timer serialization is disabled. If it is,
it then checks whether the timer has a DPC associated with its expiration, and
if the DPC has been affinitized to a target processor, in which case it selects
that processor’s timer table. If the timer has no DPC associated with it, or if
the DPC has not been bound to a processor, the kernel scans all processors in
the current processor’s group that have not been parked. (For more
information on core parking, see Chapter 4 of Part 1.) If the current processor
is parked, it picks the next closest neighboring unparked processor in the
same NUMA node; otherwise, the current processor is used.
This behavior is intended to improve performance and scalability on server
systems that make use of Hyper-V, although it can improve performance on
any heavily loaded system. As system timers pile up—because most drivers
do not affinitize their DPCs—CPU 0 becomes more and more congested with
the execution of timer expiration code, which increases latency and can even
cause heavy delays or missed DPCs. Additionally, timer expiration can start
competing with DPCs typically associated with driver interrupt processing,
such as network packet code, causing systemwide slowdowns. This process
is exacerbated in a Hyper-V scenario, where CPU 0 must process the timers
and DPCs associated with potentially numerous virtual machines, each with
their own timers and associated devices.
By spreading the timers across processors, as shown in Figure 8-21, each
processor’s timer-expiration load is fully distributed among unparked logical
processors. The timer object stores its associated processor number in the
dispatcher header on 32-bit systems and in the object itself on 64-bit systems.
Figure 8-21 Timer queuing behaviors.
This behavior, although highly beneficial on servers, does not typically
affect client systems that much. Additionally, it makes each timer expiration
event (such as a clock tick) more complex because a processor may have
gone idle but still have had timers associated with it, meaning that the
processor(s) still receiving clock ticks need to potentially scan everyone
else’s processor tables, too. Further, as various processors may be cancelling
and inserting timers simultaneously, it means there’s inherent asynchronous
behaviors in timer expiration, which may not always be desired. This
complexity makes it nearly impossible to implement Modern Standby’s