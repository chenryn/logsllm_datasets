50% (10 speakers)
In addition to the table setting, our system is also applicable
to other settings, e.g., a more common scenario – the smart-
phone is held in a user’s hand. Compared with the table setting,
an accelerometer in the handhold setting will exhibit lower
SNR along the x-axis and y-axis. Therefore, more attention
(weight) should be allocated to the z-axis. In Table VI, we
show the testing accuracy of our model in the “Table” and
“Hand-hold” settings. As shown in Table VI, if our model is
only trained by either the “Table” or “Hand-hold” training set,
it only has no more than 20% on the other testing set, due
to the aforementioned difference between those two settings.
However, if we train the model on both the “Table” and “Hand-
hold” training sets, the accuracy will be improved to above
60% on both testing sets.
TABLE VI.
TOP 1 ACCURACY OF OUR MODEL UNDER DIFFERENT
SETTINGS (ALL THE CASES BELOW ARE USER INDEPENDENT).
Test (20%)
Train (80%)
Table
Handhold
Table + Handhold
Table
78%
19%
69%
Handhold
Table + Handhold
17%
77%
63%
47%
48%
66%
C. The impact of noise:
As discussed in section IV-C, the proposed attack might be
affected by the self noise of the accelerometer and the acoustic
noise around the remote caller.
For the self-noise of the accelerometer, although this noise
component has decreasing power across the whole frequency
band,
it may still weaken the features in the acceleration
signals and thus reduce the recognition accuracy. To test the
robustness of our recognition model against this self-noise,
we utilize the white Gaussion noise to simulate this noise
and generate acceleration signals with different SNRs. The
resulting signals simulate accelerometer measurements col-
lected at lower volume levels. The results for digit-recognition
14
and speaker-identiﬁcation are shown in Table VII. Although
the accuracy is reduced as the SNR decreases, our system is
actually very robust in the sense that the accuracy of digit-
recognition on the SNR=2 data even surpasses the previous
SOTA accuracy on the clean data.
TABLE VII.
PERFORMANCE OF OUR RECOGNITION MODEL ON NOISY
ACCELERATION SIGNALS (SPECTROGRAMS).
Digit-recognition (0-9)
top1 acc
top3 acc
Speaker-identiﬁcation (20 speakers)
top1 acc
top3 acc
Tasks
SNR
SNR = 2
SNR = 4
SNR = 6
SNR = 8
42%
52%
61%
66%
73%
82%
87%
91%
34%
43%
51%
58%
64%
71%
77%
81%
For the acoustic noise around the remote caller, we hire
four volunteers (two females and two males) and ask them
to send voice messages to the victim smartphone from four
realistic environments with different noise levels: 1) No-
noise (quiet room). 2) Low-noise (lab with people talking).
3) Medium-noise (bar with music playing). 4) High-noise
(crowded bus station). These environments are selected based
on the experimental results in Fig. 6. We then play the
received voice messages under the table setting and record the
accelerometer measurements. The dataset for each environment
contains 200 × 10 digits spectrograms collected from four
speakers. Table VIII lists the results for digit-recognition. Sur-
prisingly, the recognition model achieves over 80% accuracy
in the ﬁrst three environments. For the high-noise environment,
the recognition accuracy is greatly decreased because the
segmentation algorithm can hardly distinguish between speech
signals and sudden loud noise. In order to ﬁnd out if the
recognition model can recognize well-segmented high noise
signals, we manually adjust the segmentation of the signal
and repeat the experiment. With manually segmented signals,
our model achieves 78% top 1 accuracy in the high-noise
environment, which suggests that the recognition model is
very robust against ambient acoustic noise. Since the proposed
attack can achieve high accuracy in most environments and few
people would make phone calls in a high-noise environment,
we believe that the proposed attack is practical.
TABLE VIII.
RECOGNITION ACCURACY UNDER REALISTIC NOISY
ENVIRONMENTS. The recognition accuracy in the no-noise environment is
slightly higher than the accuracy in table V because this experiment only
involves four speakers (two females and two males).
Noise level
No-noise
Low-noise
Medium-noise
High-noise
top 1 acc
86%
86%
80%
47%
top3 acc
97%
98%
96%
73%
top5 acc
100%
99%
99%
88%
D. Scalability study:
Different smartphones may have different sampling rates
and dominant axes, which makes it difﬁcult to generalize a
recognition model trained from a smartphone to other smart-
phone models. To study the scalability of the proposed attack,
we collect acceleration signals from six smartphones of three
different models: 1) Samsung S8: the sampling rate is 420 Hz
and the dominant axis is the Z-axis. 2) Huawei Mate 20: the
sampling rate is 500 Hz and the dominant axis is the Z-axis. 3)
Oppo R17: the sampling rate is 410 Hz and the smartphone has
similar audio response across three axes. We collect 10k digits
acceleration signals from each smartphone model and evaluate
the possibility of deploying one model globally. We observe
that the acceleration signals collected from Huawei Mate 20
and Oppo R17 have much less noise signals than the Samsung
S8. As shown in Table IX, it is not easy for a recognition
model trained by data from a smartphone model to generalize
to other smartphones, due to the diverse hardware features of
different smartphone models. However, we still observe a 5%
accuracy increase for the Samsung S8 when the recognition
model is trained with data from both Oppo R17 and Huawei
Mate 20. Therefore, we conjecture that the recognition model
can be scalable to unseen smartphones, if the model is trained
by the data from enough smartphone models that can capture
the diversity of the hardware features. Besides, Table IX also
indicates that the model capacity of our recognition model
is adequate to ﬁt the data from multiple smartphone models
without loss in accuracy.
TABLE IX.
MULTI-DEVICE TRAIN AND TEST (TOP 1 ACCURACY).
Test
Train
Samsung S8
Huawei Mate 20
Oppo R17
Oppo R17,
Huawei Mate 20
Samsung S8,
Oppo R17,
Huawei Mate 20
Samsung S8
Huawei Mate 20
Oppo R17
80%
12%
21%
26%
79%
15%
82%
23%
83%
84%
20%
21%
91%
90%
90%
E. Reconstruction
be calculated by (cid:80)
(cid:80)
The performance of our reconstruction network is evaluated
by two metrics, i.e., the averaged testing (cid:96)1 and mean square
error. Given the reconstructed speech-spectrogram as ˜x and
the ground truth speech-spectrogram as x, the (cid:96)1 error can
i | ˜xi − xi|, where i represents the index
w.r.t. pixels. The ﬁnal testing (cid:96)1 error is close to 1e3, i.e., the
absolute error per pixel is approximately 0.02 (pixel range
is [-1, 1]). The mean square error can be calculated by
, where N is the number of pixels per image.
The ﬁnal testing mean square error is approximately 3.5e-
3. These results indicate the ability of our reconstruction
network to reconstruct speech-spectrograms from acceleration-
spectrograms with very small errors.
( ˜xi−xi)2
N
i
that we attempt
We further use the Grifﬁn-Lim (GL) algorithm to estimate
the speech signals from the reconstructed spectrograms and
demonstrate the results in Fig. 16(a). For comparison, we
show the original speech signal in the ﬁrst row. The second
row shows the original speech signal but without frequency
components higher than 1500Hz, which is actually the ground-
truth (target) audio signal
to reconstruct.
Although this frequency cut-off may lead to loss of certain
consonant information, due to the limited frequency range of
the acceleration signals, 1500Hz is almost the highest (har-
monic) frequency that could be reconstructed for the speech
signals here. The third row shows the raw acceleration signal,
which has similar structures but completely different details
compared to the cut-off audio signal, indicating reconstruc-
tion of speech signals from acceleration signals should be
a complicated task. In the fourth row, we demonstrate the
speech signals reconstructed by our reconstruction network
and the GL algorithm, which already captures most structures
15
(a) Reconstructed signal
(b) Spectrogram of eight
Fig. 16. Comparison between the target speech signal and the reconstructed
one. The speech information is eight.
and details of the cut-off speech signals. We argue that the
remaining difference between the reconstructed signal and the
cut-off signal is mainly due to the errors caused by the GL
algorithm, because if we simply apply the phases of the cut-off
speech signal to the magnitudes (spectrograms) reconstructed
by our reconstruction network, almost the same signal can be
recovered as the cut-off audio signal, as shown in the ﬁfth row.
To our knowledge, our reconstruction module is the ﬁrst
trial on reconstructing speech signals from acceleration signals,
which is successful
in the sense that most structures and
details are roughly recovered. However, there still remains
two limitations: One is that the largest (harmonic) frequency
recovered by our reconstruction module is 1500Hz, which may
lead to consonant information loss. The other limitation comes
from the GL algorithm, which may not be the optimal choice
to compensate the phase information. We will detail these two
limitations and the potentials for improvement in Section VII.
F. Hot Words Search: Recognition and Reconstruction
In this subsection, we conduct an experiment to show that
our models can also be used to search hot (sensitive) words
from sentences. In this experiment, we ﬁrst use a hot word
search model to identify pre-trained hot words from sentences.
We then use the reconstruction model to reconstruct the audio
signal and to double check the identiﬁed hot words by human
ears. The experiment is conducted with 200 short sentences
collected from four volunteers (two males and two females).
Each short sentence contains several insensitive words and one
to three hot words listed in Table X.
The hot word search model is based on a recognition
model that can distinguish between eight hot words (listed
in Table X) and other insensitive words. To train this model,
we collect a training dataset with 128*8 hot words and 2176
insensitive words (negative samples) from the volunteers. It
can be observed that this dataset is class-imbalanced since
the number of the samples in each hot-word class is far less
than the number of the negative samples. To address this
problem, we re-weight the losses for the nine classes. Since
the total number of negative samples is 17 times the number
of the samples in each hot-word class, we weight the loss
computed by the hot-word samples with a factor 17α, and the
loss computed by negative samples with a factor α. α is a
hyper-parameter and is set to 0.1 in the training process. We
then segment the acceleration signals of the test sentences into
single word spectrograms and use the hot word search model to
recognize them. As shown in Table X, our recognition model
can achieve over 90% recognition accuracy averagely on those
eight hotwords, which is slightly higher than the recognition
accuracy on the 10 digits. We note that this is because there
are only nine classes in this recognition task, and also, the
spectrograms of these eight hot words are more distinctive in
comparison with digits and letters.
TABLE X.
TRUE POSITIVE RATE (TPR) AND FALSE POSITIVE RATE
(FPR) FOR EACH HOT WORD.
Word
Password
Username
Social
Security
Number
Email
Credit
Card
TPR
94%
97%
100%
91%
88%
88%
88%
97%
FPR
0.4%
0.4%
0.3%
0.0%
0.1%
1.4%
0.3%
1.4%
Double Check FPR
0.2%
0.3%
0.0%
0.0%
0.0%
0.8%
0.3%
0.3%
We then implement a reconstruction model that can re-
construct full-sentence audio signals from acceleration signals.
Because the reconstruction model mainly learns the mapping
between signals rather than semantic information,
it does
not require signal segmentation and is more generalizable to
unseen (untrained) data than the recognition model. To train
such a model, we collect 6480 acceleration spectrograms and
audio spectrograms with sentences different from the testing
sentences. The resolutions of the acceleration spectrogram and
the audio spectrogram are 128 × 1280 × 3 and 384 × 1280
respectively, which allows the reconstruction model to recon-
struct audio signals up to 12 seconds. We use this model plus
the GL algorithm to reconstruct the audio signals of all test
sentences and hire two volunteers to listen to them. In this
process, we ﬁrst provide basic training for each volunteer,
where the volunteer will hear the audio signals of 20 sentences
and their reconstructed versions. We then ask the volunteer
to listen to reconstructed signals (sentences) and relabel the
hot words falsely identiﬁed by the recognition model. In this
process, the label of a hot word will not be changed unless
both volunteers agree to change it. It turns out the volunteers
can easily tell whether a hot word is falsely identiﬁed. The
false positive rates for all hot words are reduced bellow 1%