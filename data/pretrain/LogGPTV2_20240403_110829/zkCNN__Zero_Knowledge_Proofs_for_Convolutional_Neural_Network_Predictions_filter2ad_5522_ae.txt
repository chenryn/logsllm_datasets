However, at the end of each sumcheck, the verifier has to evaluate
˜𝛽(·) and ((1−𝑢𝑖) +𝑢𝑖 ˜𝜔𝑖+1(·)) at a random point to obtain ˜𝐴(𝑖−1)
(·)
at the random point for the next sumcheck. By the closed-form of
˜𝛽 in Definition 2.2, it can be evaluated at a random point in time
𝑂(𝑖). By the closed-form of multilinear extension in Definition 2.3,
𝑘=0 𝑥𝑘2𝑘, which equals
2𝑖+1) and can also be evaluated in time 𝑂(𝑖).
Therefore, with this approach, the total prover time remains 𝑂(𝑁)
and the total verifier time is reduced to 𝑂(log2 𝑁), while the total
proof size increases to 𝑂(log2 𝑁).
˜𝜔𝑖+1(𝑟) =𝑥∈{0,1}𝑖+1 𝛽(𝑟, 𝑥)𝜔 𝑗
to𝑖+1
2𝑖+1 for 𝑗 =𝑖+1
𝑘=0((1 − 𝑟𝑘) + 𝑟𝑘𝜔2𝑘
𝐹
3.2 Two-Dimensional Convolutions
With our new sumcheck protocol for FFT as a building block, we
construct a protocol to validate 2-D convolutions.
Inverse FFT. Inverse FFT (IFFT) can be viewed as FFT with a dif-
ferent root of unity [19],
𝑁−1
𝑖=0 𝑐𝑖𝜔 𝑗𝑖 ⇔ 𝑐𝑖 =
𝑀−1
𝑗=0 𝑎 𝑗 𝜔−𝑗𝑖,
1
𝑀
𝑎 𝑗 =
for 𝑖 ∈ [𝑁], 𝑗 ∈ [𝑀]. As 𝑀 is known and its inverse exists in a
finite field, we can just apply the same sumcheck protocol with
linear prover time to validate the result of IFFT.
2-D convolution to 1-D convolution. As introduced in Section 2.1,
a convolutional layer in CNN computes the 2-D convolution be-
tween the input and the kernel. Here we show that the computation
can be reduced to a 1-D convolution. Following Equation 1 in Sec-
tion 2.1, let ¯𝑋, ¯𝑊 ∈ F𝑛2 be
(cid:40)𝑊𝑡,𝑙 ,
¯𝑋𝑡𝑛+𝑙 = 𝑋𝑛−1−𝑡,𝑛−1−𝑙 ,
¯𝑊𝑡𝑛+𝑙 =
0,
0 ≤ 𝑡, 𝑙 < 𝑤
otherwise
0 ≤ 𝑡 < 𝑛, 0 ≤ 𝑙 < 𝑛
𝑗
Session 11B: Zero Knowledge II CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2974𝑋 𝑗+𝑡,𝑘+𝑙𝑊𝑡,𝑙
¯𝑋(𝑛−1−𝑗−𝑡)·𝑛+(𝑛−1−𝑘−𝑙) ¯𝑊𝑡·𝑛+𝑙
¯𝑋(𝑛−1−𝑗−𝑡)·𝑛+(𝑛−1−𝑘−𝑙) ¯𝑊𝑡·𝑛+𝑙
¯𝑋𝑛2−1−𝑗·𝑛−𝑘−𝑖
¯𝑊𝑖
(11)
Equation 1 is
𝑈 𝑗,𝑘 =
=
=
𝑡=0,𝑙=0
(𝑤−1),(𝑤−1)
(𝑤−1),(𝑤−1)
(𝑛−1),(𝑛−1)
𝑛2−1−𝑗·𝑛−𝑘
𝑡=0,𝑙=0
𝑡=0,𝑙=0
𝑖=0
=
= ¯𝑈𝑛2−1−𝑗·𝑛−𝑘
¯𝑈 (𝜂) = ¯𝑋(𝜂) ¯𝑊 (𝜂) ⇔ ¯𝑈 𝑗 =𝑗
Thus 𝑈 can be computed through 1-D convolution between ¯𝑋, ¯𝑊 ,
vectors defined by the input and the kernel of a convolutional layer.
Computing 1-D convolution using FFT. It is well-known that
1-D convolution is the same as multiplications between two uni-
variate polynomials. We abuse the notation to denote the uni-
variate polynomials with coefficients ¯𝑋, ¯𝑊 as ¯𝑋(𝜂), ¯𝑊 (𝜂), then
¯𝑋 𝑗−𝑖 ¯𝑊𝑖 by taking ¯𝑈 as the first
𝑛2 coefficients of ¯𝑈 (𝜂).
Finally, polynomial multiplications can be calculated using FFT
and IFFT in three steps. First, we transform ¯𝑋(𝜂) and ¯𝑊 (𝜂) from
coefficients to evaluations at powers of the root of unity, denoted by
FFT( ¯𝑋) and FFT( ¯𝑊 ). Here we implicitly assume that 𝑊 is padded
to 𝑛2 and both are evaluated at 2𝑛2 points. Then we compute the
Hadamard product (element-wise product) of the vectors, and fi-
nally transform the result back to the coefficients through IFFT. The
algorithm is given as:
𝑖=0
¯𝑈 = ¯𝑋 ∗ ¯𝑊 = IFFT(FFT( ¯𝑋) ⊙ FFT( ¯𝑊 ))
(12)
where “⊙” denotes Hadamard product. With the equation above,
we are able to verify the computation of 2-D convolutions using
three sumcheck protocols, one for FFT, one for Hadamard product
and one for IFFT. The real protocol also deals with the indexing
and padding, but they do not introduce any major overhead.
Complexity. The prover time of our protocol is 𝑂(𝑛2), which is
asymptotically optimal and is faster than computing the convolu-
tion. The proof size is 𝑂(log2 𝑛) and the verifier time is 𝑂(log2 𝑛),
given oracle accesses to the multilinear extensions of the input and
the output.
4 ZERO KNOWLEGE CONVOLUTIONAL
NEURAL NETWORKS
We present our zero knowledge CNN scheme in this section. We
start with the formal definitions of zkCNN, and then introduce
several improvements on the sumcheck and the GKR protocol tai-
lored for CNN predictions, and describe our design for activation
functions and pooling that lead to concrete efficiency in practice.
of the CNN (e.g., number of layers, dimensions of kernels and acti-
vation fuctions) is known to the verifier. Admittedly the structure
of CNN also leaks information in some scenarios. The structure
can further be hidden by introducing upper bounds on the depth
and dimensions and selectors from a set of activation functions, or
through proof compositions of zero knowledge proofs. Our scheme
in this paper only protects the privacy of the parameters while
ensuring the integrity of predictions, which is the first step for zero
knowledge CNN and the extensions are left as future work.
Formally speaking, let W be the parameters of a CNN model
where the dimensions are given in Section 2.1, and X ∈ F𝑛1×𝑛1×ch𝑖𝑛,1
be a data sample. Let 𝑦 = pred(W, X) be the prediction of X using
the CNN as described in Section 2.1. A zero knowledge CNN scheme
(zkCNN) consists of the following algorithms:
• pp ← zkCNN.KeyGen(1𝜆): Given the security parameter, the
algorithm generates the public parameters pp.
• comW ← zkCNN.Commit(W, pp, 𝑟): The algorithm commits
the parameters W of the CNN model using the randomness 𝑟.
• (𝑦, 𝜋) ← zkCNN.Prove(W, X, pp, 𝑟): Given a data sample X, the
algorithm runs CNN prediction algorithm to get 𝑦 = pred(W, X)
and generates the proof 𝜋.
• {0, 1} ← zkCNN.Verify(comW, X, 𝑦, 𝜋, pp): The algorithm veri-
fies the prediction 𝑦 with the commitment comW, the proof 𝜋
and the input X.
A zkCNN scheme is sound, where the probability that the prover
returns a wrong prediction and passes the verification is negligible;
it is also zero knowledge, where the proof leaks no information
about the prover’s model W. We give the formal definitions in Ap-
pendix C. A Zero knowledge CNN accuracy scheme simply repeats
the zkCNN predictions on multiple data samples and compares the
predictions with the labels to calculate the accuracy. The defini-
tions can be modified slightly to accommodate zkCNN accuracy
and we omit the formal definitions. Moreover, our constructions
can also support zero knowledge predictions for secret input data
with public CNN models, and both secret input and secret models in
a straight forward way, which may be useful in other applications.
This is because our scheme is a commit-and-prove SNARK [16].
This is in contrast to zero knowledge proofs based on MPC tech-
niques [7, 22, 45, 49], where there are different trade-offs on the
public and private data and models.
4.2 Generalizations of GKR for CNN
In this section, we introduce several improvements and generaliza-
tions for the sumcheck and the GKR protocol, which lead to better
performance for CNN predictions.
4.1 Definitions
In our setting, the prover owns a pre-trained CNN model that is
sensitive, and proves to the public that an input data sample is
correctly classified using the CNN model. The prover commits to
the parameters of the CNN first, and then later the verifier queries
for the prediction of the data sample. The prover generates a proof
together with the prediction to convince the verifier of its validity.
Similar to existing schemes [23, 34], we assume that the structure
4.2.1 Generalized addition and multiplication gates. As described
in the preliminaries, the GKR protocol reduces layer 𝑖 to layer
𝑖 + 1 through Equation 3 in Section 2.2.2. Because of the definition
of 𝑎𝑑𝑑𝑖(𝑧, 𝑥, 𝑦), each addition gate can only take two inputs and
it takes log 𝑛 layers to sum 𝑛 values in the circuit. Justin Thaler
partially addressed this issue in [41] by observing that the circuit
of an addition tree can be represented as a single sumcheck. Here
we consider the more general case of addition gates with multiple
Session 11B: Zero Knowledge II CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2975inputs, as well as the sum of multiple products. We define
˜𝑋𝑎𝑑𝑑𝑖(𝑧, 𝑥) =
˜𝑋𝑚𝑢𝑙𝑡𝑖(𝑧, 𝑥, 𝑦) =
if 𝑉𝑖+1(𝑥) is added to 𝑉𝑖(𝑧)
otherwise
if 𝑉𝑖+1(𝑥) · 𝑉𝑖+1(𝑦) is added to 𝑉𝑖(𝑧)
otherwise
(cid:40)1,
(cid:40)1,
0,
0,
˜𝑉𝑖(𝑧) =
for all 𝑥, 𝑦 ∈ {0, 1}𝑠𝑖+1 and 𝑧 ∈ {0, 1}𝑠𝑖 . With the new definitions,
we can write the multilinear extensions of layer 𝑖 as:

+

˜𝑋𝑚𝑢𝑙𝑡(𝑧, 𝑥, 𝑦) · ˜𝑉𝑖+1(𝑥) · ˜𝑉𝑖+1(𝑦)(cid:17)
(cid:16) ˜𝛽(𝑦, (cid:174)0) ·
𝑥∈{0,1}𝑠𝑖+1
𝑥,𝑦∈{0,1}𝑠𝑖+1
𝑥,𝑦∈{0,1}𝑠𝑖+1
=
+
˜𝑋𝑎𝑑𝑑𝑖(𝑧, 𝑥) · ˜𝑉𝑖+1(𝑥)
˜𝑋𝑚𝑢𝑙𝑡𝑖(𝑧, 𝑥, 𝑦) · ˜𝑉𝑖+1(𝑥) · ˜𝑉𝑖+1(𝑦)
˜𝑋𝑎𝑑𝑑𝑖(𝑧, 𝑥) · ˜𝑉𝑖+1(𝑥)
(13)
˜𝑋𝑚𝑢𝑙𝑡 with the scalars.
With the equation above, we can compute common functions
such as additions with fan-in ≥ 2 and inner products with a single
sumcheck3. Note that for inner products this is better than using
the sumcheck for addition trees in [41], which takes 2 layers of the
circuit. The prover time remains linear by generalizing the algo-
rithms for the prover in [48], and we omit the formal algorithms. In
practice, this generalization reduces the proof size by a logarithmic
factor for proving CNN predictions, and improves the concrete
efficiency of the prover. Furthermore, we can also supports scalar
˜𝑋𝑎𝑑𝑑
multiplications with constants for free by replacing the 1 in
and
4.2.2 Taking inputs from arbitrary layers. Recently Zhang et al. [51]
proposed a variant of the GKR protocol where a gate can take input
from arbitrary layers above, instead of only the previous layer,
without introducing any overhead on the prover time. We show that
our generalization above is compatible the techniques in [51]. The
motivation is that CNN consists of multiple convolutional layers
and fully-connected layers. The kernels and the weight-matrices of
these layers are the witness from the prover in our zkCNN. When