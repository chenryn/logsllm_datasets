a less standard hardness assumption: this is exactly analogous to how the security of the r-GLP
scheme from [3] is proved under the non-standard r-DCK assumption).
14
Nevertheless, in order to avoid relying on additional assumptions compared to the original BLISS
paper, we opt for a completely constant time implementation of the same approach instead. Our
idea is to add ik to the list of previously obtained indices using a constant-time insertion sort, and
do a constant-time swap between ik and n − κ − 1 + k in case a collision occurs. In principle, that
approach has quadratic complexity in κ, but since κ is so small, the overhead is negligible: we
find that our constant-time approach is only a few thousand cycles slower than the variable time
algorithm (about 1–2% of the entire running time of signature generation).
3.1.4 Rejection sampling. Finally, the last step we need to implement in constant time is the
rejection sampling. In other words, at the end of the signature generation algorithm, we need to
sample bits bexp and b1/cosh that take the value 1 with probability
pexp = exp(cid:16) − K − ∥Sc∥2
(cid:17)
2
2σ
and p1/cosh = 1/cosh
(cid:18) ⟨z, Sc⟩
(cid:19)
2
σ
respectively (where K is a known constant).
To do so, the approach taken in the original BLISS paper relies on iterated Bernoulli trials with
known constant probabilities for bexp, and recursively calls this exponential sampling algorithm to
sample b1/cosh. Again, the variable time nature of these algorithms has led to multiple attacks.
As mentioned in [22], it is relatively easy to modify the function SampleBernExp from Figure 4
to run in constant time: simply carry out every iteration every time, and accumulate the results of
the Bernoulli trials using constant time logic expressions. However, the performance penalty of
doing so is significant, due to the lack of early aborts. This is not a serious problem for the rejection
sampling step itself, since it is only carried out a handful of times per signature. However, since
this exponential rejection sampling function is also called as part of Gaussian sampling (as we
recall from Section 3.1.1 above), any slow down will strongly affect the running time of the entire
signature generation. Moreover, while the bexp part can be made constant time, doing so is much
harder for b1/cosh, as we have discussed in Section 2.
An alternate approach is to simply evaluate the values pexp and p1/cosh with sufficient precision,
and compare them to uniform random values in [0, 1]. The challenge is to do so in constant time,
using only integer arithmetic. In particular, we cannot rely on floating point implementations of
transcendental functions like exp and cosh.
The approach we take is to replace the exp function by a sufficiently close polynomial approxi-
mation, and similarly for cosh. Then, pexp can be evaluated in fixed point to sufficient precision
using an application of Horner’s algorithm, entirely with integer arithmetic; and 1/p1/cosh can be
evaluated using the same code by expressing cosh in terms of exp. There are several steps involved
in carrying out that strategy:
exp(cid:0)x/(2σ
2)(cid:1), achieving the relative precision we need. To do so, we introduce a novel
(1) determine the precision we need to ensure security. To do so, we use a methodology introduced
by Prest [35] based on the Rényi divergence. It shows that 45 bits of relative precision suffice
for security, provided that the number of generated signatures is at most 264 (as specified in
the NIST competition).
(2) compute a polynomial approximation on the required interval of the function f : x (cid:55)→
technique based on lattice reduction for the Sobolev Euclidean norm on polynomials. This
technique lets us precisely control the shape of the polynomial we get, in order to ensure
that Horner’s algorithm can be applied without any overflow using 64-bit integer arithmetic.
Compared to earlier techniques such as the L∞ approximations of Brisebarre and Chevil-
lard [8], it also has the advantage of eliminating heuristics (since a bound on the Sobolev
norm directly yields a bound on the functional ∥ · ∥∞ norm), and of avoiding the computation
15
Table 3. Performance results and comparison (kcycles).
LQ Median
515
286
Dilithium (ref)
332
142
Dilithium (avx2)
243
418
qTesla-I (ref)
194
Original BLISS
188
Our implementation 218
220
UQ Const. time?
1526
428
781
313
223
!
!
%
%
!
of minimax polynomials (since the closest polynomial in the Sobolev norm can be obtained
using a simple Euclidean projection).
(3) extend the range of that polynomial approximation in order to support the larger interval
required for the cosh computation (as well as for the rejection step in Gaussian sampling).
This is done by computing a constant c such that f (c) is very close to 2, so that f (k · c + x) =
f (c)k · f (x) can be easily obtained from f (x) using small multiplications and shifts.
(4) deduce an algorithm for the cosh part of the rejection sampling. The nontrivial point here is
that we end up evaluating a good approximation of p′ = 1/p1/cosh. Testing if u < p1/cosh, for
some u ∈ [0, 1], reduces to testing if u · p′ < 1. The multiplication involves numbers with
over 45 bits of precision, however, so the result does not fit within 64 bits, and thus requires
some degree of bit fiddling. Intermediate conditional branches also need to be written in
constant time.
Full technical details regarding these various steps are provided in Section 4 below.
The idea of using polynomial approximations to evaluate pexp already appears in earlier work: as
part of the FACCT Gaussian sampler described in [43]. In particular, our own Gaussian sampler
can be seen as a variant of FACCT. There are multiple differences between our works, however:
in particular, FACCT relies on floating point arithmetic, which we specifically seek to avoid,4 and
uses off-the-shelf software to obtain a double precision floating point polynomial approximation of
the function f . Moreover, since FACCT focuses on Gaussian sampling, that paper does not directly
address the cosh issue.
3.2 Security and performance
Using the techniques described above, we wrote a constant-time implementation of BLISS in
portable C (specifically for the BLISS–I parameters), that can be found in [4]. We now provide
some data regarding its performance, and provide a short formal treatment of its security.
We point out that our code only implements signature generation in constant time. Obviously,
signature verification does not manipulate any secret, and hence does not need to be made constant
time; however, one may wish to ensure that key generation is constant time as well. We have not
attempted to do so, since key generation is carried out much less often and usually in much more
controlled conditions than actual signing. However, it is not difficult to modify our implementation
to make key generation constant time as well. The building blocks involved are briefly discussed at
the end of this section.
4We think the argument from [43] to the effect that floating point multiplications are constant time is overly optimistic.
As mentioned earlier, this is not true on some older x86 platforms, to say nothing of more exotic, more lightweight or
FPU-less architectures. Besides, as highlighted in [17], floating points arithmetic may lead to a vulnerbability called weak
determinism which can sometimes lead to complete breaks.
16
3.2.1 Performance measurement and comparison. Our implementation is written for the SUPER-
COP toolkit for measuring cryptographic software performance [5]. Accordingly, it follows the
SUPERCOP API, and uses the corresponding utility functions for operations like randomness gen-
eration (for which SUPERCOP automatically selects the most efficient machine-specific candidate,
in our case ChaCha20). We therefore use SUPERCOP’s latest version as of this writing5 to evaluate
the performance its performance on our testbench platform, and compare its speed with the closest
competitors Dilithium [18] and qTesla [1] on the same machine.
We also provide a comparison to Ducas and Lepoint’s original, variable-time implementation of
BLISS on the same platform [19]. Unfortunately, that implementation does not follow the SUPERCOP
API, so the comparison is not entirely apples to apples: on the one hand hashing and randomness
generation are carried out with OpenSSL’s implementation of SHA2 (instead of SHAKE128 and
ChaCha20 respectively); on the other hand, all the serialization routines required by SUPERCOP
are omitted. On balance, this should not strongly bias the comparison in either direction.
Our testbench platform is an Intel Xeon Platinum 8160-based server (Skylake-SP architecture)
with Ubuntu 18.04 and gcc 7.3.0 with the default SUPERCOP compiler options (-march=native
-mtune=native -O3 -fomit-frame-pointer -fwrapv), with hyperthreading disabled and scaling
governor set to performance. The choice of machine may seem overkill, but it was the newest
CPU we had access to, and hence made it possible to compare our portable C implementation with
the hand-vectorized AVX2 implementation of Dilithium available in SUPERCOP.
Performance results are presented in Table 3: they indicate the lower quartile, median and upper
quartile cycle counts measured by SUPERCOP (or in the case of BLISS, measured by the RDTSC
instruction) for the signature of a 59-byte message, which is the standard performance figure
presented on the eBATS website. The Dilithium performance numbers are for the fastest parameter
set available in SUPERCOP, namely the dilithium2 implementation, corresponding to “medium”
security parameters in [18] (no implementation is provided for the “weak” parameters); we give
timings both for the portable C (ref) and AVX2 platform specific (avx2) implementations. For
qTesla, we also use the fastest available implementation (qtesla1, only in portable C6), which
corresponds to essentially the same lattice security level as BLISS–I.
As we can see in the table, we achieve a performance level similar to the original, variable-
time BLISS implementation, while preventing the serious timing attack vulnerabilities exposed in
multiple papers so far.
In addition, our implementation is multiple times faster than than qTesla-I and the portable
C implementation of Dilithium, and even outperforms the AVX2 implementation of Dilithium
by a significant margin, while providing stronger constant-time guarantees (since the Dilithium
ring-valued hash function presents a mild timing leakage that causes the security in the constant-
time model to rely on non-standard assumptions). Admittedly, the Dilithium parameters were
derived using a more conservative methodology for assessing the cost of lattice attacks, and hence
probably achieve a significantly higher level of security against them. Nevertheless, according to
Wunderer’s recent reevaluation [42] of what is likely the strongest attack against BLISS (namely
the Howgrave-Graham hybrid attack), it is reasonable to think that BLISS–I does reach its stated
security level of around 128 bits.
Note that the “Const. time?” column in Table 3 indicates whether the implementation satisfies
constant-time security guarantees (i.e. the absence of secret dependent branches and memory
5https://bench.cr.yp.to/supercop/supercop-20190110.tar.xz
6The “heuristic” qTesla-I parameters were recently removed from the qTesla submission documents and the remaining
“provable” parameters are significantly less efficient. Since our goal is to compare to fast comparable schemes, however,
qTesla-I appears to be the most suitable parameter choice.
17
Fig. 10. Leakage assessment with dudect [37].
accesses). This is of course achievable without having strictly constant running time, since secret-
independent branches and loops are permitted and heavily relied on in Fiat–Shamir with aborts-type
schemes.
3.2.2 Experimental validation. In order to further validate the constant-time nature of our im-
plementation, we carried out experiments with the dudect tool of Reparaz et al. [37]. The basic
idea of the experiment is to generate two key pairs for the signature scheme, sign many messages
randomly with either of the two signing keys, and check if a statistical difference can be observed
between timings among the two keys.
Clearly, such an experimental approach cannot be used to conclusively establish that an algorithm
is constant-time: for example, it will not detect if a very small fraction of weak keys with different
timing profiles exist; in the case of Fiat–Shamir signatures, it is also incomplete in the sense that the
signing key is not the only sensitive part of the algorithm—the randomness is also sensitive. And in
principle, experimental validation should also not be necessary if the security analysis provides
sufficient evidence that the algorithm is constant-time.
We did find experiments with dudect to be quite useful in practice, as it did find timing leakage in
an earlier version of our implementation, mostly due to the fact that gcc would compile apparently
branch-free code containing comparison instructions into actual conditional branches in the
assembly. After fixing those issues (by replacing comparisons with bit fiddling), we obtained our
final implementation, for which no leakage is detected in dudect: we ran it on 30 different sets of
two random key pairs, and in all cases, the t values in Welch’s t-test remained below 3 or 4 even
with tens of millions of signatures.
In contrast, dudect easily detects the timing leakage in the original BLISS implementation: t
values quickly exceed 10, and shoot up above 100 after a few hundred thousand signatures. Similarly,
significant timing leakage is detected in the qTesla-I implementation available in SUPERCOP, even
though it is advertised [1] as constant-time: in our experiments, t values exceed 10 after around
100,000 signatures, and eventually increase to above 100; whether this leakage can be exploited to
attack the scheme is unclear, but it does rule out the implementation being constant-time.
Welch t-test values measured by dudect for our implementation as well as the original BLISS
and qTesla are shown in Figure 10.
18
0.0E+001.0E+062.0E+063.0E+064.0E+065.0E+060.321.003.1610.0031.62100.00GalacticsqTeslaOrig. BLISSnumber of signaturest-value (log-scale)Adversary
Challenger
q queries
forgery(cid:8)
(KeyGen,Sign,Verify)
←−−−−−−−−−−−−−−−−
pk←−−
µ(1)−−−→
σ (1), L (1)
←−−−−−−−−
Sign
...
µ(q)−−−→
Sign
σ (q), L (q)
←−−−−−−−−
µ∗, σ ∗
−−−−−→
(sk, pk) ← KeyGen(1λ)
(cid:0)σ(1), L (1)
Sign
(cid:1) ← ExecObs(Sign, µ(1), pk, sk)
(cid:0)σ(q), L (q)
Sign
(cid:1) ← ExecObs(Sign, µ(q), pk, sk)
b ← Verify(pk, µ∗, σ∗) ∧ (µ∗ (cid:60) {µ(1), . . . , µ(q)})
Fig. 11. The CT-EUF-CMA security game.
Security argument. Let us now try and formalize the constant time security guarantees that
3.2.3
we claim are provided by our implementation. To do so, we introduce the notion of existential un-
forgeablity under chosen message attack in the constant-time model (CT-EUF-CMA), which combines
standard EUF-CMA security property with the security in the constant-time model. It can be seen
as a constant-time model counterpart of the “EUF-CMA in the d-probing model” notion introduced
in [3] in the context of masking security.
Definition 3.1. An implementation (KeyGen, Sign, Verify) of a signature scheme is EUF-CMA-
secure in the constant-time model, or CT-EUF-CMA secure for short, if any PPT adversary has
a negligible winning probability in the experiment from Figure 11. In that security experiment,
ExecObs is a universal RAM machine that takes as input an algorithm and its arguments, executes
the program, and outputs the result of the computation together with the timing leakage L ,
consisting of the sequence of visited program points and memory accesses.
be summed up as follows.
In the context of that definition, the constant-time properties discussed in Section 3 above can
Proposition 3.2. For any execution of our implementation of the signature generation(cid:0)σ , LSign
ExecObs(Sign, µ, pk, sk), the leakage LSign can be perfectly publicly simulated from the number of
executions the main loop (Steps 2–8 of Figure 2).
(cid:1) ←
Proof. Indeed, we have made sure that each step of algorithm, except for the execution or not
of the rejection sampling, is devoid of secret-dependent branches or memory accesses. As a result,
the sequence of visited program points and memory accesses from each step is perfectly publicly
simulatable, and the overall leakage LSiдn is obtained from repeating those simulations a number
of times equal to the number of executions the main loop.
□
Remark 1 (Filtering attacks are not possible). Assume that an attacker is able to select a
subset of signatures by filtering7 the output u (or equivalently c) and assume that she has access
7We thank Damien Stehlé for suggesting this approach.
19
to the expectation of rejection conditionned with c, i.e. Ey1,y2,b[(z1, z2, c) rejected | c]. The following
computation shows that this quantity is independent from s1, s2 and c. Let s1, s2 and c be fixed
quantities,
(cid:19)−1
(cid:18) ⟨z, Sc⟩
(cid:19)−1(cid:35)
2
σ