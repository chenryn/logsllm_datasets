tection system based on machine learning. However, the attacker
does not need to master any prior knowledge about the detector,
such as the algorithm, parameters, features or training data.
Adversary’s capability. In the evasion scenario based on adver-
sarial attacks, the attacker has the ability to modify the test data
but not the detector’s training data. Even so, because the attacker
has full control of the botmaster and partial control of the bots,
we believe that the attacker can also update bots to change their
communication behaviors by setting up a proxy. At the same time,
we assume that the attacker can continuously access the detector
to obtain the binary prediction result from the detector.
3.2 System design
In the context of evading botnet detection, we instantiate the feed-
back loop of RL, as shown in Figure 2. Our task is training the agent
through a series of interactions with the botnet detector so that it
can learn how to craft adversarial botnet flow example to bypass
the botnet detector.
196Crafting Adversarial Example to Bypass Flow-&ML- based Botnet Detector via RL
RAID ’21, October 6–8, 2021, San Sebastian, Spain
DQN. Q-learning is an off-policy temporal-difference (TD) method
that uses a Q-table to estimate the Q-value for selecting the best
behavior. However, when the state space or action space is large,
Q-learning has the problem of "dimensionality disaster". DQN lever-
ages a neural network to estimate the Q-value function [26] and
switch to a function Q(s, a; θ) to approximate Q(s, a) instead of
Q_table, where θ is the network parameter. The loss function for
the network L(θ) is defined as the squared error between the target
Q-value and the Q-value output from the network.
L(θ) = E[(T arдetQ − Q(s, a; θ))2]
(1)
Figure 2: System framework
where
T arдetQ = rt +1 + γ · max
Q(st +1, a; θ)
a
Generally, our system setup is composed of two components:
an aдent and an environment. The aдent receives feedback from
the environment and then selects actions from the action space to
modify the botnet flow according to the RL algorithm.
The environment mainly includes two modules: a botnet detector
and a state generator. The botnet detector is the detector we want
to bypass. Its task is to continuously predict the malicious traffic in
the loop and feed the binary result back to the agent as a reward.
It should be noted that the reward is a real number ∈ {0, R}; if the
detector is successfully misdirected, the reward is R; otherwise, it
is 0. We set R = 10 to give a strong positive feedback when the
agent successfully modifies the botnet traffic sample for bypassing
detector. The state generator is used to generate a description of
the current sample state and help the agent make action decisions.
Generally, the more comprehensive and accurate the state is, the
better the agent’s decision will be.
When the system starts to run, the state generator generates
a state s based on the input sample. Then, the aдent perceives
the state and selects an action a from the action space to modify
the botnet flow sample according to the RL algorithm. Then, the
modified sample is input into the botnet detector, and a prediction
result is obtained. Next, aдent receives the reward r according to
the binary feedback, and the state generator generates the next state
s′ according to the modified sample. The loop continues until the
detector is successfully misled or the number of operations reaches
the upper limit (if the detector is still not successfully bypassed, we
start another sample modification attempt).
3.3 Reinforcement Learning Algorithm
Reinforcement learning is a type of machine learning technique
that enables an agent to learn in an interactive environment by trial
and error using feedback from its own actions and experiences [39].
It is employed by various software and machines for finding the best
possible behavior or path they should take in a specific situation.
To solve the RL problem, the agent chooses an RL algorithm to
learn to take the best action in each of the possible states it encoun-
ters. It senses a given state from the environment and maximizes
its long-term reward value by learning to select an appropriate
action based on the enhanced signal provided by the environment.
Considering that our action space is not continuous, we choose
two classic value-based RL algorithms to compare which is more
suitable for proposed application scenario.
where γ is the discount factor, s and a represent the current state
and action, respectively, st +1 represents the next state and rt +1 is
the reward of action a under state s (These are the same in the
following equation).
SARSA. State–action–reward–state–action (SARSA) is an on-
policy algorithm for TD learning. The optimal Q-value, denoted as
Q(st , at), can be expressed as:
Q(st , at) = Q(st , at) + α[rt +1 + γQ(st +1, at +1) − Q(st , at)]
(2)
There are many differences between Q-learning and SARSA.
First, on-policy SARSA learns the Q-value based on the action
performed by the current policy, while off-policy Q-learning does
so relative to the greedy policy. Taking the cliff walking problem
as an example, DQN tends to choose a large negative reward while
exploring, while SARSA tends to take the safe path and avoid a
dangerous optimal path. This means that a Q-learning agent may
fall off the cliff at a safe point as a result of choosing exploration.
DQN is bolder, while SARSA is more conservative.
Second, under some common conditions, they both converge
to the real value function but at different rates. Q-learning tends
to converge slightly slower than SARSA because of the greedy
policy, but it has the capability to continue learning while changing
policies. Q-learning directly converges to the optimal policy, while
SARSA only learns a near-optimal policy by exploring.
For the above reasons, we implemented these two RL algorithms
and evaluated their performances through a series of experiments.
The results are shown in Section 5.
3.4 Action space
The action space includes a series of modification actions for botnet
flow.
ML-based botnet flow-level detection is anomaly-based that iden-
tify botnet based on the differences in some features between botnet
flow and normal flow. ML models perform feature extraction before
classification, which will cause somewhat information compression
and information loss, no matter it is done manually or automatically
based on a neural network. We hope that the perturbation added
to the original sample can confuse certain features of the malicious
sample with the normal sample after feature extraction, which the
botnet detector deems as an indicator of benign samples.
But as aforementioned, the most important thing to modify the
botnet flow is to not affect its malicious functions. Therefore, we
197RAID ’21, October 6–8, 2021, San Sebastian, Spain
Wang and Liu, et al.
Figure 3: Boxplot of the 18 normalized flow features for botnet and normal flow
cannot delete flow packets at will. The only choice is to apply modi-
fications to areas that do not affect the implementation of malicious
functions or add new data packets. Through the analysis of botnet
flow, we find that the malicious content of botnet is encapsulated
in the application layer, so incremental operations at the transport
layer will not affect the original malicious functions.
To determine which features should be disturbed, we refer to
works on ML-based botnet detection, such as [23] [10], [16] [20]. We
find that researchers tend to extract some discriminative features
based on the working mechanism of the botnet, and these features
often have high degrees of overlap in different jobs.
Taking the difficulty of action designing and into account, we
choose 18 features from the set of features commonly used in bot-
net detection, including duration, packet per flow(ppf), packet per
second(pps), bytes pre flow(bpf), bytes per second(bps), inter-arrival
time(iat), down/up ratio and so on(fw:forward, bw:backward). These
characteristics can be easily affected by carefully designing the time,
size and direction of the added packet. Figure 3 shows the boxplot of
the 18 normalized feature values in our training dataset for botnet
and benign flows.
We can observe that due to the unique working mechanism of
botnets, there are some differences between the ranges of values
for some features in the botnet and normal flows. For example,
botnet will send a large number of short heartbeat packets to con-
firm whether the connection is maintained, so it has a smaller ppf;
downloading malicious applications and transmitting private infor-
mation on the bot side will result in a larger bpp; bots need to send a
large amount of secret information in response to short commands
from botmaster, so botnet traffic tends to have a small down/up
ratio.
Based on this discovery, our action space includes 14 actions,
which can affect the above-mentioned statistical characteristics of
transport layer by simply modifying the data packet timestamp
or adding carefully constructed packets. When constructing new
packets, we mainly consider three attributes: timestamp, direction,
and packet size. These 14 actions are divided into 5 categories
according to the objects they intend to affect, as summarized in
Figure 4:
• Change the duration
1) Withhold the final TCP FIN packet for randomly 1-3s
Figure 4: Action space
• Change the time interval
2) Add a forward packet with an interval of 20s at the end
3) Add a backward packet with an interval of 20s at the end
• Change the image characteristics: (for the DNN model) pay
attention to the location and content of the packet
4) Add an empty packet at random location (0 < loc < 8)
5) Add a random packet at random location (0 < loc < 8)
6) Add a full 0 packet at random location (0 < loc < 8)
• Change the statistical characteristics: (for the non-DNN
model) pay attention to the direction and size of the packet
7) Add a forward large-size 0 packet
8) Add a backward large-size 0 packet
9) Add a forward avg-size packet with no content at the head
10) Add a backward avg-size packet with no content at the
11) Add a forward empty packet
12) Add a backward empty packet
• Change the packet length
13) Add a random length of 0 at the end of the packet with a
head
14) Select two packets without payloads, add the character ’0’
probability of 0.2
to avg-size
198Crafting Adversarial Example to Bypass Flow-&ML- based Botnet Detector via RL
RAID ’21, October 6–8, 2021, San Sebastian, Spain
4 EXPERIMENTAL SETUP
4.1 Implementation
By referring to the implementation of Tor, we deploy our system
as an adversarial proxy in the network environment, as shown in
Figure 6. The attacker can easily deploy an adversarial proxy on the
botmaster side, while on the bot side, the attacker can achieve this
by updating the bot through the original C&C channel. Therefore,
our method can be implemented without complex modifications to
the original malware.
Under this deployment scenario, all communication traffic be-
tween the attacker and the bot reaches the proxy first. Therefore,
the attacker can monitor the botnet communication traffic, and the
adversarial proxy equipped with the trained RL agent can perform
incremental actions against the botnet flow until it successfully
bypasses the detector. In this way, what the detector obtains is
the botnet communication traffic that has been processed by the
adversarial agent and is very likely to bypass the it.
In such an attack and defense architecture, the attacker can
achieve adversarial attack-based botnet detector evasion in a com-
pletely black box scenario.
With an aim to engage the community, we implement our RL
framework with OpenAI gym [12]. Specifically, we implement
SARSA and DQN agents using keras-rl [33].
Figure 5: State Generator Details
The purpose of the "Change the duration" category is to affect
the duration; "Change the time interval" is to change IAT; "Change
the packet length" is used to disturb bpp; "Change the image char-
acteristics" contains actions to change the input of a deep neural
network-based botnet detector, so they focus on the location and
payload of the newly inserted data packet; The actions in "Change
the statistical characteristics" are to comprehensively disturb the
above statistical characteristics, so the direction and size of the data
packet are mainly considered.
3.5 State space
Considering that the binary feedback of the detector contains too
little information for use by the agent, we need a state generator
to deliver the state of the botnet flow to the agent. To describe the
state of the current botnet flow samples concisely, we use a feature
encoder with a deep structure — a stacked autoencoder (SAE) — to
automatically extract botnet flow features and feed them back to
the agent as the state.
SAE is a neural network consisting of several layers of sparse
autoencoder, where the output of each hidden layer is connected to
the input of the successive hidden layer. SAE was first proposed by
Bengio et al. [8]. To avoid the potential vanishing gradient problem
of the deep network, SAE training is performed using unsupervised
pretraining and supervised fine-tuning. To some extent, the pre-
trained networks facilitate iterative convergence in the supervised
phase because they fit the structure of the training data, making
the initial value of the entire network have a suitable state. Because
each layer is based on the features extracted by the previous layer,
SAE is able to extract highly abstract and complex features. SAE has
achieved noteworthy performances on many feature preprocessing
and dimensionality reduction tasks.
Specifically, we take the first 1024 bytes of each botnet flow file
(because the first few packets, up to the first 20 packets, have been
shown to be sufficient for correct accuracy, even for encrypted
traffic [42]) as an input for the SAE model. After several epochs of
training, the SAE model can automatically learn a 256-dimensional
state vector of the botnet flow.
When determining the state dimension, we test 128 and 256
dimensions. Under the trade-off between the training time cost and
evasion effects, we finally set the number of feature dimensions to
256.
4.2 Dataset
Assessing the performance of any detection approach requires ex-
perimentation with data that are heterogeneous enough to simulate
real traffic at an acceptable level. We choose two public datasets:
CTU [17], captured by the Malware Capture Facility Project, which
is a research project in charge of continuously monitoring the threat
landscape for new emerging threats, retrieving malicious samples
and running them in facilities to capture the traffic; and ISOT [37],
created by merging different available datasets. It contains both
malicious (traces of the Storm and Zeus botnets) and non-malicious
traffic (gaming packets, HTTP traffic and P2P applications). The
dataset contains trace data for a variety of network activities span-
ning from web and email to backup and streaming media. This
variety of traffic makes it similar to real-world network traffic.
We select 10 botnet families from these public datasets to form a
new dataset with the following considerations:
Figure 6: Implementation of our system
199RAID ’21, October 6–8, 2021, San Sebastian, Spain
Wang and Liu, et al.
Table 2: Details of the dataset
Used for Channel
Botnet Family
IRC
HTTP
Train
&
Test
Train
P2P
Normal
Menti
Rbot
Murio
Virut
Miuref(3ve)
Neris
HTbot
Dridex/Necurs
Trickbot
Storm
–
CTU-47
CTU-44/45/52
File Serial No. Original
sessions
4,507
31,736
8,381
31,173
13,426
31,133
36,855
10,029
30,052
4,672
511,322
CTU-49
CTU-46/54
CTU-127
CTU-42/43
CTU-348
CTU-346
CTU-327
ISOT-Storm
ISOT-normal
• Diversity, the dataset covers the most mainstream botnet
communication channel types (IRC, HTTP, P2P), and the
traffic characteristics are significantly different.
• Typically, the dataset includes typical botnet families’ traffic,
they’re either causes major attacks, has a large number of
controlled hosts, or adopts advanced hiding methods.
• Large time span, for a single family, each traffic file takes
a long time to capture. Overall, that dataset covers botnet
traffic from 2011-2018. This makes the dataset more versatile
and novel than other existing datasets.
We summarize the botnet families and their capture times, brief
introductions and numbers of session samples used for the experi-
ment in Table 2.
After obtaining the dataset, we perform data preprocessing
progress, as shown in Figure 7.
Step 1: Integration & pruning. We combine the collected traffic
belonging to the same botnet family. If the pcap file is too large, it
is cropped. The purpose of this step is to balance the sample size of
each family.
Step 2: Splitting pcap into sessions. This is done to obtain more
complete communication information. A session refers to all pack-
ets consisting of bidirectional flows, that is, the source IP and desti-
nation IP are interchangeable in a 5-tuple (SIP, SPort, DIP, DPort,
Protocol). Specifically, we use SplitCap [3] to split each pcap file
into sessions.
Step 3: Anonymization & cleaning. Our traffic file is produced
from different network environments. To eliminate the IP and MAC
address’ effects on the detector, the unique information of the traffic
Figure 7: Data preprocessing
Used
sessions
4,000
10,000
8,000
30,000
10,000
30,000
30,000
10,000
30,000
4,672
80,000
Captured
year
2011
2011
2011
2011
2015
2011
2018
2018
2018
2012
2010
Notes
An IRC-based botnet with 40,000 active members
Responsible for 5.5 percent of malware
infections during the third quarter of 2012
It infected around 1.7 million computers
One of the largest reported Android banking
botnets known to date
One of the world’s largest spam botnets
The top business threat in 2018
Infected more than 1 million systems
data needs to be randomized. Specifically, we replace them with a