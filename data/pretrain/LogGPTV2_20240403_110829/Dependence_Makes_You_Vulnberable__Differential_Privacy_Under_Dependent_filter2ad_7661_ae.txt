DPM is conservative and continues to provide rigorous DDP
privacy guarantees. In case of underestimation of ρij, there
ij is larger
are two cases. In the ﬁrst case, if our estimated ρe
than the expectation of the adversary who has access to certain
auxiliary information, our DPM can still continue to provide
rigorous DDP guarantees. However, in second case, in which
the underestimation of ρij is smaller than the adversary’s
expectation, we may not achieve the DDP guarantees, but
would still provide better privacy than the traditional DP
mechanism. To demonstrate the performance degradation due
to underestimation of ρij, we launch the inference attack in
Section IV-C2 to DDP clustering query results which are
obtained by utilizing underestimated dependence coefﬁcients
ij = 0.8ρij, 0.9ρij in our DPM (ρij is computed according
ρe
to Eqs. 26,18,24). Fig. 7 demonstrates that even if 0.8, 0.9
ratio of underestimation for ρij
the
leaked information is still well-bounded without uncontrolled
collapsing. Therefore, our DPM suffers little degradation for
the slight underestimation of ρij and is likely to be acceptable
for most realistic settings, thus making our DPM robust in
real-world scenarios.
is utilized in DPM,
Furthermore, we consider a natural relaxation of depen-
dent differential privacy to incorporate such imperfect estima-
tion of ρij.
Deﬁnition 6. ((, δ)-Dependent Differential Privacy) A ran-
domized algorithm A provides (, δ)-dependent differential
privacy, if for any pair of dependent neighboring databases
D(L,R) and D(cid:48)(L,R) and any possible output S, we have
P (A(D(L,R)) = S) ≤ exp()P (A(D(cid:48)(L,R)) = S) + δ
(27)
where D(L,R), D(cid:48)(L,R)
neighboring
databases (recall Deﬁnition 3), based on the dependence size
L and their probabilistic dependence relationship R.
dependent
are
Fig. 7. Information leakage for underestimated ρij.
Better accuracy (a smaller magnitude of added noise
owing to the underestimation of ρij) and generally more
ﬂexibility can often be achieved by relaxing the deﬁnition of
DDP in Eq. 4. Exploring such relaxations of DDP would be
an interesting direction for future work.
VII. EXPERIMENTAL RESULTS
This section evaluates the performance of our pro-
posed dependent data release algorithm on multiple real-world
datasets (including Gowalla data in Section IV-A, the adult
data in UCI Machine Learning Repository, and the large-scale
Google+ data [19]). Our objectives are: 1) to show the privacy
and utility superiority of our DPM over the state-of-the-art
approaches, 2) to study the impact of enforcing DDP on the
data in terms of machine learning queries and graph queries,
and 3) to analyze the resistance of DPM to inference attacks
described in Section IV-C.
A. Privacy and Utility Guarantees
Consider the application scenario in Fig. IV-B, where
the data provider publishes the perturbed K-means centroids
of the Gowalla location dataset while preserving the privacy
of each individual data. Since the Gowalla dataset contains
no associated probabilistic distributions or data generating
process, we use the general dependent model in Eq. 26 to
compute the dependence coefﬁcient ρij in Eq. 18 by setting
a = 0.0019, b = 0.196, c = 1.05 as empirically determined
according to [2]. Then, the global sensitivity DSQ can be
computed according to Eq. 23 and Eq. 24.
Fig. 8(a) analyzes the (α, β)-accuracy in Deﬁnition 5
under various privacy-preserving level . We can see that under
the same α and , our DPM has much lower β than the
baseline approach (where the dependence size L is set to be
equal to the number of tuples) and the approach of Zhu et al.
in [40]6, i.e., (cid:107)A(D) − Q(D)(cid:107)1 ≤ α with higher probability
1 − β. Therefore, DPM achieves much better accuracy than
the existing approaches, and such advantage increases with a
larger privacy preserving level . When α = 1000,  = 1, the
probability of (cid:107)A(D)− Q(D)(cid:107)1  0.8 (di, dj are the proﬁles of tuple i, j respec-
(cid:107)di(cid:107)1(cid:107)dj(cid:107)1
tively). Similarly, we compute the dependence coefﬁcient ρij
for users i and j according to Eqs. 26, 18, and 24. Fig. 9 (b)
shows that our DPM has much better classiﬁcation accuracy
than the other methods by considering ﬁne-grained dependence
relationship. For  = 0.9, which represents a strong privacy
level, DPM achieves an accurate classiﬁcation performance
with 85% accuracy, which is more than twice that of the
other approaches. Therefore, DPM could provide an acceptable
application quality of service while providing rigorous privacy
guarantees.
3) Degree Distribution: We further consider a graph
query whose result is to publish the degree distribution of
a large-scale Google+ dataset [19]. The Google+ dataset is
crawled from July 2011 to October 2011, which consists of
7Detailed process for applying DP to SVM classiﬁcation can be found in [7].
8 https://archive.ics.uci.edu/ml/datasets/Adult/
12
10210400.20.40.60.81αβ  DPM ε=0.1Baseline ε=0.1Zhu et al. ε=0.1DPM ε=1Baseline ε=1Zhu et al. ε=110110210300.511.522.533.54x 104αPrivacy Budget ε  DPM β=0.1Baseline β=0.1Zhu et al. β=0.1DPM β=0.5Baseline β=0.5Zhu et al. β=0.50123450.20.30.40.50.60.70.80.91Privacy Budget εClustering Accuracy  DPMBaselineZhu et al.0123450.40.50.60.70.80.91Privacy Budget εClassification Accuracy  DPMBaselineZhu et al.01234500.20.40.60.81Privacy Budget εDegree Distribution Accuracy  DPMBaselineZhu et al.Fig. 10. (a) Comparison of information leakage due to LPM (for achieving DP) and DPM (for achieving DDP) under the same
inference attack. (b) Information leakage due to DPM under various levels of prior information available to an adversary.
(a)
(b)
28,942,911 users and 947,776,172 edges and thus contains a
broad degree distribution. The degree distribution of a graph
is a histogram partitioning the nodes in the graph by their
degrees [39], and it is often used to describe the underlying
structure of social networks for the purposes of developing
graph models and making similarity comparisons between
graphs9. In addition to the social graph, an auxiliary data is also
provided in this dataset with users’ attributes such as Employ-
ment and Education. To compute the dependence coefﬁcient,
we construct an afﬁnity graph based on the similarities between
the users’ proﬁles, where an edge is added for a pair of users
i and j if
> 0.8 and di, dj represent the proﬁles
of tuple i, j respectively in the auxiliary data. Similarly, we
compute the dependence coefﬁcient ρij for users i and j
according to Eqs. 26,18,24). Denoting C(D) and C(cid:48)(D) as the
true degree distribution and the perturbed degree distribution
respectively, we deﬁne the accuracy for publishing C(cid:48)(D) as
1 − (cid:107)C(D)−C(cid:48)(D)(cid:107)1
. By considering ﬁne-grained dependence
(cid:107)C(D)+C(cid:48)(D)(cid:107)1
relationship, our DPM has signiﬁcantly higher accuracy for
publishing dependent differentially private degree distribution
of the social graph than the other methods, with almost 10x
improvement as shown in Fig. 9 (c).
C. Resistance to the Inference Attack
(cid:107)dT
i dj(cid:107)1
(cid:107)di(cid:107)1(cid:107)dj(cid:107)1
To further demonstrate the privacy advantages of DPM,
we analyze the resistance of DPM to real-world inference
attacks as discussed in Section IV-C2. Fig. 10 (a) shows that
the information leakage for DPM is much smaller than that
for the traditional DP under the advanced inference attack
(corresponding to the dependent scenario in Section IV-C2),
and is under similar level as the scenario when the adversary
has no access to the social relationships (corresponding to the
independent scenario in Section IV-C1). That is to say, the
leaked information caused by the dependent tuples has been
largely offset by our incorporating dependence relationship to
DPM. These results show that DPM can rigorously achieve
the expected privacy guarantees for dependent data where tra-
ditional DP mechanisms fail, and also validate the effectiveness
of our general dependent model in Section VI-D2.
We further investigate the inﬂuence of different prior
information available to the adversary on the inference attack
9Detailed process for applying differential privacy on degree distribution
can be found in [39].
performance. Fig. 10 (b) shows that
the increase in prior
information would be beneﬁcial for the adversary to infer more
information of the users’ location information. Comparing
Fig. 10 (a) and Fig. 10 (b), we can also see that DPM shows
strong resistance to the adversarial inference attack even under
the case where an adversary has access to a large amount
of auxiliary information. Therefore, DDP offers a rigorous
and provable privacy guarantee for dependent tuples, which
demonstrates the necessity of generalizing the standard DP to
our DDP.
D. Summary for the Experimental Analysis
• DPM provides signiﬁcant privacy and utility gains compared
to the state-of-the-art approaches. Therefore, we can select
a suitable privacy budget  to achieve an optimal privacy
and utility balance for DPM.
• DPM is more than 2x accurate in computing the K-means
clustering centroids and the SVM classiﬁer, and more than
10x accurate in publishing degree distribution of large-scale
social network, compared with existing approaches (which
may not even provide rigorous privacy guarantees). These
results demonstrate the effectiveness of DPM in real-world
query answering for network data.
• DPM is resilient to adversarial inference attack and provides
rigorous privacy guarantees for dependent tuples that are not
possible using LPM-based DP schemes.
VIII. RELATED WORK
Data privacy is an issue of critical importance, motivating
perturbation of query results over sensitive datasets for protect-
ing users’ privacy [5], [12], [26], [29], [31], [38]. However,
the existing privacy-preserving mechanisms are fraught with
pitfalls. A signiﬁcant challenge is the auxiliary information,
which the adversary gleans from other channels. Chaabane et
al. [6] inferred users’ private attributes by exploiting the public
attributes of other users sharing similar interests. Narayaran et
al. [32] re-identiﬁed users in the anonymous Twitter graph by
utilizing information from their Flickr accounts. Srivatsa et al.
[37] identiﬁed a set of location traces by another social network
graph. Other interesting work can be found in [22], [33]. Our
inference attack in Section IV demonstrates that the auxiliary
information would also be useful
to infer an individual’s
information from differentially private query results.
Differential privacy is one of the most popular privacy
13
00.511.522.53012345678Privacy Budget εLeaked Information  attack DP with social relationships, Nprior=6969attack DP w/o social relationships, Nprior=6968attack DDP with social relationships, Nprior=696800.511.522.53012345678Privacy Budget εLeaked Information  attack DDP with social relationships, Nprior=1000attack DDP with social relationships, Nprior=3500attack DDP with social relaitonships, Nprior=6968frameworks [12]–[16]. Query answering algorithms that satisfy
differential privacy produce noisy query answers such that the
distribution of the query answers changes only slightly with
the addition, deletion or modiﬁcation of any tuple. Kifer and
Machanavajjhala [24] were the ﬁrst to criticize that the inherent
assumption (limitation) for differential privacy is that
the
tuples within the dataset are independent of each other. They
further argue that the dependence (correlation) among tuples
would signiﬁcantly degrade the privacy guarantees provided
by differential privacy.
Tuples in real-world data often exhibit inherent depen-
dence or correlations. Handling dependent tuples is a sig-
niﬁcant problem. Kifer et al. proposed the Pufferﬁsh frame-
work [25] to provide rigorous privacy guarantees against adver-
saries who may have access to any auxiliary background infor-
mation and side information of the database. Blowﬁsh [21] is a
subclass of Pufferﬁsh which only considered the data correla-
tions introduced by the deterministic constraints. Our proposed
dependent differential privacy is highly motivated by these
privacy frameworks and is a subclass of the Pufferﬁsh frame-
work that takes the probabilistic dependence relationships into
consideration. We further propose our dependent perturbation
mechanism to rigorously achieve dependent differential privacy
for general query functions.
Membership Privacy [27] is also applicable for dependent
data, however limited anonymization algorithms have been
proposed for this framework. Chen et al. [8] dealt with the
correlated data by multiplying the original sensitivity with
the number of correlated records, which is similar to our
baseline approach in Section VI-A. We have shown, both
theoretically and experimentally, that the baseline approach
would introduce a large amount of noise and thus deteriorate
the utility performance of query answers. Zhu et al. [40]
exploited the linear relationships among tuples which does not
satisfy any rigorous privacy metric. Furthermore, their method
has been veriﬁed (in Fig. 8) to have signiﬁcantly worse privacy
and utility performance compared to our DPM.
IX. DISCUSSIONS AND LIMITATIONS
• Our dependent differential privacy can also accommodate
other dependent or correlated relationships such as temporal
correlations across a time series of dataset, which opens up
an interesting future research direction.
• To form a deeper understanding of our dependent differential
privacy, we will also explore the application of standard
concepts in differential privacy to our framework, such as
local sensitivity, smooth sensitivity [16].
• One limitation of our work is that the dependence coefﬁcient
ρij is exactly known to both the adversary and the DPM
designer. The effectiveness of DPM depends on how well
the dependence among data can be modeled and computed.
How to accurately compute the dependence coefﬁcient and
deal with the underestimation of ρij (as we discussed in
Section VI-D3) would be an interesting future work (note
that the overestimation of ρij continues to provide rigorous
DDP guarantees).
X. CONCLUSION
Differential privacy provides a formal basis for expressing
and quantifying privacy goals. For these reasons there is an
emerging consensus in the privacy community around its use
and various extensions are being proposed. However, there
remain several limiting assumptions in the original framework
that can severely weaken the privacy guarantees expected of
a differentially private mechanism. In this paper, we used an
inference attack to demonstrate the vulnerability of existing
differential privacy mechanisms under data dependence. We
show that social networks that exist between users can be
used to extract more sensitive location information from dif-
ferentially private query results than expected when standard
DP mechanisms are applied. To defend against such attacks,
we introduced a generalized dependent differential privacy
framework that incorporates probabilistic dependence relation-
ship between data and provides rigorous privacy guarantees.
We further propose a dependent perturbation mechanism and
rigorously prove that it can achieve the privacy guarantees. Our
evaluations over multiple large-scale real datasets and multiple
query classes show that the dependent perturbation scheme
performs signiﬁcantly better than state-of-the-art approaches