that satisfy yiwT xi  1.
Therefore, we estimate λ using Eqn. 3 with a = w and
(cid:2)n
i=1 −yixi(1 − yiwT xi)1yiwT xi≤1.
b =
L1-regularized kernel LR (L1-KLR): Its objective function
is:
L(α) = L(X, y, α) + λ(cid:2)Kα(cid:2)1,
where L(X, y, α) = − (cid:2)n
log(1 − hα(xi))) and hα(x) =
The gradient of the objective function with respect to α is:
(14)
i=1(yi log hα(xi) + (1 − yi)
j=1 αj φ(x)T φ(xj )).
1+exp (− (cid:2)n
1
∂L(α)
∂α = K(hα(X) − y + λt),
where hα(X) = [hα(x1); hα(x2);··· ; hα(xn)] and t =
sign(Kα). Via setting the gradient to be 0 and considering
that K is invertible, we can estimate λ using Eqn. 3 with
a = t and b = hα(X) − y.
L2-regularized kernel LR (L2-KLR): Its objective function
of L1-KLR is:
L(α) = L(X, y, α) + λαT Kα,
(15)
where L(X, y, α) = − (cid:2)n
i=1(yi log hα(xi) + (1 − yi)
log(1−hα(xi))) is a cross entropy loss function and hα(x) =
the objective
1+exp (− (cid:2)n
function with respect to α is:
j=1 αj φ(x)T φ(xj )). The gradient of
1
∂L(α)
∂α = K(hα(X) − y + 2λα),
where hα(X) = [hα(x1); hα(x2);··· ; hα(xn)]. Via setting
the gradient to be 0 and considering that K is invertible, we
can estimate λ using Eqn. 3 with a = 2α and b = hα(X)−y.
Kernel SVM with square hinge loss (KSVM-SHL): The
objective function of KSVM-SHL is:
L(α) =
i=1
L(xi, yi, α) + λαT Kα,
(16)
where L(xi, yi, α) = max(0, 1 − yiαT ki)2. Following the
same methodology we used for SVM-SHL, we estimate λ
i=1 −yiki(1 −
using Eqn. 3 with a = Kα and b =
yiαT ki)1yiαT ki≤1.
(cid:2)n
n(cid:13)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:29:54 UTC from IEEE Xplore.  Restrictions apply. 
)
0
1
g
o
l
(
r
o
r
r
e
n
o
i
t
a
m
i
t
s
e
e
v
i
t
a
l
e
R
0
−2
−4
−6
−8
−10
−3
Diabetes
GeoOrig
UJIIndoor
−2
−1
0
1
2
3
True hyperparameter value (log10)
(a) Attack
)
0
1
g
o
l
(
r
o
r
r
e
E
S
M
e
v
i
t
a
l
e
R
Diabetes
GeoOrig
UJIIndoor
0
−2
−4
−6
−8
−10
2
1
5
Number of decimals
3
4
)
0
1
g
o
l
(
r
o
r
r
e
n
o
i
t
a
m
i
t
s
e
e
v
i
t
a
l
e
R
Diabetes-λ1
GeoOrig-λ1
UJIIndoor-λ1
Diabetes-λ2
GeoOrig-λ2
UJIIndoor-λ2
2
0
−2
−4
−6
−8
2
1
5
Number of decimals
3
4
(b) Defense
Fig. 17: Attack and defense results of ENet.
APPENDIX B
MORE THAN ONE HYPERPARAMETER
We use a popular regression algorithm called Elastic Net
(ENet) [58] as an example to illustrate how we can apply
attacks to learning algorithms with more than one hyperpa-
rameter. The objective function of ENet is:
L(w) = (cid:2)y − XT w(cid:2)2
2 + λ1(cid:2)w(cid:2)1 + λ2(cid:2)w(cid:2)2
2,
(17)
where the loss function is least square and regularization term
is the combination of L2 regularization and L1 regularization.
We compute the gradient as follows:
∂L(w)
∂w = −2Xy + 2XXT w + λ1sign(w) + 2λ2w (cid:4) |sign(w)|,
where w (cid:7) |sign(w)| = [w1|sign(w1)|;··· ; wm|sign(wm)|].
Similar to LASSO, we do not use the model parameters that are
0 to estimate the hyperparameters. Via setting the gradient to
0 and using the linear least square to solve the overdetermined
system, we have:
ˆλ = −
AT A
(18)
where ˆλ = [ˆλ1; ˆλ2], A = [sign(w); 2w (cid:7) |sign(w)|], and
b = −2Xy + 2XXT w.
AT b,
(cid:14)
(cid:15)−1
Figure 17 shows the attack and defense results for ENet
on the three regression datasets. We observe that our attacks
are effective for learning algorithms with more than one
hyperparameter, and rounding is also an effective defense.
APPENDIX C
NEURAL NETWORK (NN)
We evaluate attack and defense on a three-layer neural
network (NN) for both regression and classiﬁcation.
Regression: The objective function of the three-layer NN for
regression is deﬁned as
L(W1, w2) = (cid:2)y − ˆy(cid:2)2
2 + λ
F + (cid:2)w2(cid:2)2
2
(cid:16)(cid:2)W1(cid:2)2
(cid:17)
,
(19)
51
(cid:3)
(cid:4)
1
XT W1 + b1
w2 + b2; sig(A) =
where ˆy = sig
1+exp(−A)
is the logistic function; W1 ∈ R
m×d is the weight matrix of
input layer to hidden layer and w2 ∈ Rd is the weight vector
of hidden layer to output layer; d is the number of hidden
units; b1 and b2 are the bias terms of the two layers.
To perform our hyperparameter stealing attack, we can
leverage the gradient of L(W1, w2) with respect to either W1
or w2. For simplicity, we use w2. Speciﬁcally, the gradient of
the objective function of w2 is:
∂L(W1, w2)
∂w2
(cid:16)
(cid:17)T
= 2sig
XT W1 + b1
(y − ˆy) + 2λw2.
i=1
λ
(cid:4)T
By setting the gradient to be 0, we can estimat λ using Eqn. 3
(cid:3)
with a = w2 and b = sig
Classiﬁcation: The objective function of the three-layer NN
for binary classiﬁcation is deﬁned as
XT W1 + b1
(y − ˆy).
L(W1, w2) = − n(cid:13)
(yi log ˆyi + (1 − yi) log(1 − ˆyi))
(cid:16)(cid:2)W1(cid:2)2
1 xi + b1
2
(cid:3)
2 sig
wT
+
(cid:3)
where ˆyi = sig
. Similarly with
regression, we use w2 to steal the hyperparameter λ. The
gradient of the objective function of w2 is:
= − n(cid:13)
Setting the gradient to be 0, we can estimat λ via Eqn. 3 with
a = w2 and b = − (cid:2)n
(cid:3)
i=1(yi − ˆyi)sig
∂L(W1, w2)
(yi − ˆyi)sig
F + (cid:2)w2(cid:2)2
2
(cid:4)
1 xi + b1
1 xi + b1
+ λw2.
+ b2
WT
WT
∂w2
(20)
WT
(cid:4)
.
(cid:17)
,
(cid:16)
(cid:17)
i=1
(cid:4)
APPENDIX D
PROOF OF THEOREM 5.1
When w is an exact minimum of the objective function,
we have b = −λa. Therefore, we have:
aT a
aT a
ˆλ = −(cid:3)
(cid:3)
= λ
aT a
(cid:4)−1aT b = −(cid:3)
(cid:4)−1aT a = λ.
(cid:4)−1aT
(−λa)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:29:54 UTC from IEEE Xplore.  Restrictions apply. 
L2-LR: We approximate the gradient as follows:
ˆλL2−LR(w(cid:5) + Δw) − ˆλL2−LR(w(cid:5))
∇ˆλL2−LR(w(cid:5)) = lim
Δw→0
(cid:2) (w(cid:5) + Δw)T (y − hw(cid:2)+Δw(X))
Δw
1
(w(cid:5) + Δw)T (w(cid:5) + Δw)
= lim
Δw
Δw→0
− (w(cid:5))T X(y − hw(cid:2) (X))
(cid:3)
1
(w(cid:5))T w(cid:5)
Δ(w(cid:5))T X(y − hw(cid:2)+Δw(X))
≈ lim
Δw→0
− (w(cid:5))T X(hw(cid:2)+Δw(X) − hw(cid:2) (X))
(w(cid:5))T w(cid:5)
Δw
(w(cid:5))T w(cid:5)
≈ X(y − hw(cid:2) (X))
(cid:4)w(cid:5)(cid:4)2
2
,
where in the third and fourth equations, we use (w(cid:3) +
Δw)T (w(cid:3) + Δw) ≈ (w(cid:3))T w(cid:3) and hw(cid:2)+Δw(X) ≈ hw(cid:2) (X)
for sufﬁciently small Δw, respectively.
L2-KLR: Similar to L2-LR, we approximate the gradient as:
∇ˆλL2−KLR(α(cid:5)) ≈ K(y − hα(cid:2) (K))
.
(cid:4)α(cid:5)(cid:4)2
2
L1-LR: We approximate the gradient as follows:
ˆλL1−LR(w(cid:5) + Δw) − ˆλL1−LR(w(cid:5))
∇ˆλL1−LR(w(cid:5)) = lim
Δw→0
(cid:2)
1
sign(w(cid:5) + Δw)T X(y − hw(cid:2)+Δw(X))
sign(w(cid:5) + Δw)T sign(w(cid:5) + Δw)
−
Δw
= lim
Δw
Δw→0
sign(w(cid:5))T X(y − hw(cid:2) (X))
(cid:3)
sign(w(cid:5))T sign(w(cid:5))
(hw(cid:2)+Δw(X) − hw(cid:2) (X))T
≈ lim
Δw→0
≈ ∇hw(cid:2) (X))XT sign(w(cid:5))T
Δw
(cid:4)sign(w(cid:5))(cid:4)2
2
.
XT sign(w(cid:5))
(cid:4)sign(w(cid:5))(cid:4)2
2
L1-KLR: Similar to L1-LR, we approximate the gradient as:
∇ˆλL1−KLR(α(cid:5)) ≈ ∇hα(cid:2) (K))KT sign(α(cid:5))T
.
(cid:4)sign(α(cid:5))(cid:4)2
2
APPENDIX E
PROOF OF THEOREM 5.2
We prove the theorem for linear learning algorithms, as it is
similar for kernel learning algorithms. We treat our estimated
hyperparameter ˆλ as a function of model parameters. We
expand ˆλ(w(cid:3) + Δw) at w(cid:3) using Taylor expansion:
ˆλ(w(cid:3)
+ Δw) = ˆλ(w(cid:3)
) + ΔwT∇ˆλ(w(cid:3)
)
)Δw + ···
1
2
ΔwT∇2ˆλ(w(cid:3)
+
= ˆλ(w(cid:3)
= λ + ΔwT∇ˆλ(w(cid:3)
) + ΔwT∇ˆλ(w(cid:3)
) + O((cid:3)Δw(cid:3)2
2)
) + O((cid:3)Δw(cid:3)2
2)
Therefore, Δλ=ˆλ(w(cid:3)+Δw)−λ =ΔwT∇ˆλ(w(cid:3))+O((cid:3)Δw(cid:3)2
2).
APPENDIX F
APPROXIMATIONS OF GRADIENTS
We approximate the gradient ∇ˆλ(w(cid:3)) in Theorem 2 for
RR, LASSO, L2-LR, L2-KLR, L1-LR, and L1-KLR. Accord-
ing to the deﬁnition of gradient, we have:
∇ˆλ(w(cid:5)) = lim
Δw→0
ˆλ(w(cid:5) + Δw) − ˆλ(w(cid:5))
Δw
,
where the division and limit are component-wise for Δw.
RR: We approximate the gradient as follows:
∇ˆλRR(w(cid:5)) = lim
Δw→0
(cid:2) (w(cid:5) + Δw)T (Xy − XXT (w(cid:5) + Δw))
ˆλRR(w(cid:5) + Δw) − ˆλRR(w(cid:5))
Δw
1
= lim
Δw
Δw→0
− (w(cid:5))T (Xy − XXT w(cid:5))
(cid:3)
(w(cid:5) + Δw)T (w(cid:5) + Δw)
(w(cid:5))T w(cid:5)
(cid:2) ΔwT (Xy − 2XXT w(cid:5)) − ΔwT XXT Δw
(cid:3)
1
≈ lim
Δw→0
≈ X(y − 2XT w(cid:5))
Δw
(cid:4)w(cid:5)(cid:4)2
2
(w(cid:5))T w(cid:5)
,
where in the third and fourth equations, we use (w(cid:3) +
Δw)T (w(cid:3) + Δw) ≈ (w(cid:3))T w(cid:3) and ΔwT XXT Δw ≈ 0 for
sufﬁciently small Δw, respectively.
LASSO: We approximate the gradient as follows:
∇ˆλLASSO(w(cid:5)) = lim
Δw→0
ˆλLASSO(w(cid:5) + Δw) − ˆλLASSO(w(cid:5))
(cid:2) 2sign(w(cid:5) + Δw)T (Xy − XXT (w(cid:5) + Δw))
Δw
1
sign(w(cid:5) + Δw)T sign(w(cid:5) + Δw)
= lim
Δw
Δw→0
− 2sign(w(cid:5))T (Xy − XXT w(cid:5))
sign(w(cid:5))T XXT Δw
sign(w(cid:5))T sign(w(cid:5))
1
(cid:4)sign(w(cid:5))(cid:4)2
Δw
2
(cid:3)
≈ lim
Δw→0
≈ lim
Δw→0
1
Δw
ΔwT XXT sign(w(cid:5))
(cid:4)sign(w(cid:5))(cid:4)2
2
≈ XXT sign(w(cid:5))
(cid:4)sign(w(cid:5))(cid:4)2
2
,
where in the third equation, we use sign(w(cid:3) + Δw) ≈
sign(w(cid:3)).
52
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:29:54 UTC from IEEE Xplore.  Restrictions apply.