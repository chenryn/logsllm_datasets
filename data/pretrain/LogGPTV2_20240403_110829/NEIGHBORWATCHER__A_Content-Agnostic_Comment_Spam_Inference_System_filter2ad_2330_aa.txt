title:NEIGHBORWATCHER: A Content-Agnostic Comment Spam Inference System
author:Jialong Zhang and
Guofei Gu
NEIGHBORWATCHER: A Content-Agnostic Comment Spam Inference System
SUCCESS Lab, Department of Computer Science & Engineering
Jialong Zhang and Guofei Gu
Texas A&M University, College Station, TX
{jialong,guofei}@cse.tamu.edu
Abstract
Comment spam has become a popular means for spam-
mers to attract direct visits to target websites, or to manip-
ulate search ranks of the target websites. Through posting
a small number of spam messages on each victim website
(e.g., normal websites such as forums, wikis, guestbooks,
and blogs, which we term as spam harbors in this paper) but
spamming on a large variety of harbors, spammers can not
only directly inherit some reputations from these harbors
but also avoid content-based detection systems deployed on
these harbors. To ﬁnd such qualiﬁed harbors, spammers
always have their own preferred ways based on their avail-
able resources and the cost (e.g., easiness of automatic post-
ing, chances of content sanitization on the website). As a
result, they will generate their own relatively stable set of
harbors proved to be easy and friendly to post their spam,
which we refer to as their spamming infrastructure. Our
measurement also shows that for different spammers, their
spamming infrastructures are typically different, although
sometimes with some overlap.
This paper presents NEIGHBORWATCHER, a comment
spam inference system that exploits spammers’ spamming
infrastructure information to infer comment spam. At its
core, NEIGHBORWATCHER runs a graph-based algorithm
to characterize the spamming neighbor relationship, and re-
ports a spam link when the same link also appears in the
harbor’s clique neighbors. Starting from a small seed set of
known spam links, our system inferred roughly 91,636 com-
ment spam, and 16,694 spam harbors that are frequently
utilized by comment spammers. Furthermore, our evalua-
tion on real-world data shows that NEIGHBORWATCHER
can keep inferring new comment spam and ﬁnding new
spam harbors every day.
1 Introduction
Spamdexing (also known as web spam, or search engine
spam) [25] refers to the practice of artiﬁcially improving
the search rank of a target website than it should have. The
rise of such spam causes unnecessary work for search en-
gine crawlers, annoys search engine users with poor search
results, and even often leads to phishing websites or mal-
ware drive-by downloads. In a recent study [15], Google
reports about 95,000 new malicious websites every day,
which results in 12-14 million daily search-related warnings
and 300,000 download alerts.
To boost the ranking of the target websites, spammers
have already developed lots of spamdexing techniques [25]
in the past few years, most of which also called Black Hat
SEO (Search Engine Optimization). Text and Link manipu-
lations are two main SEO techniques frequently abused by
spammers to exploit the incorrect application of page rank
heuristics. Through injecting excessively repeating contents
in their websites or changing the perceived structure of we-
bgraph, spammers have successfully improved their search
ranks in the past [6]. However, since search engines have al-
ready developed new techniques to detect these content/link
manipulation tricks [9, 11], spammers begin to change to
another new trick of spamdexing, named comment spam-
ming. Comment spamming refers to the behavior of auto-
matically and massively posting random comments or spe-
ciﬁc messages (with links to some promoting websites) to a
benign third-party website that allows user generated con-
tent, such as forums (including discussion boards), blogs,
and guestbooks. In this paper, we refer to those benign vic-
tim websites that are frequently used by spammers to post
comment spam as spam harbors (or sometimes harbors for
short). This new trick of comment spam can beneﬁt spam-
mers in several ways: (1) spammers can easily inherit some
reputations of these harbors with nearly zero cost; (2) the
risk of being detected by search engines is reduced; (3)
spammers can easily disguise themselves as normal visitors
who also contribute content.
Most existing state-of-the-art approaches to detect com-
ment spam use three types of features: (1) content-based
features [24, 31], e.g., the utilization of certain words, con-
tent redundancy/frequency, topic or language inconsistency;
(2) context-based features [32], which mainly refer to the
existence of URL cloaking or redirection techniques (be-
cause normal URLs rarely use them); (3) behavior-based
features [33, 23], which mainly refer to the time difference
between main article and comment posting (which is typi-
cally short in normal cases). Unfortunately all of them have
their clear limitations and spammers already become more
agile in evading them. To evade content-based detection
[24, 31], spammers can artiﬁcially manipulate their posting
content by mixing spam links with normal content, or post-
ing very few content on each harbor site but spamming on
a large variety of harbors. Thus they can easily disguise
themselves as normal visitors, and remain not detected. In
[32], the authors used context-based features, i.e., checking
the use of URL cloaking or redirection tricks, which is ef-
fective for certain type of spam. However, it has a low cov-
erage in the detection scope, because most comment spam
currently is mainly used for search rank manipulation [33].
Thus, URL hiding/cloaking is no longer necessary and less
used. The limitation of time differences between main arti-
cle and comment posting [33, 23] is also clear; it is applica-
ble to only some blogs, which are time sensitive, while not
easily applicable to more broader websites such as forums,
guestbooks. As we can see, within these three types of fea-
tures, the later two typically may not work well alone, thus
they are suggested to combine with content-based features.
Finally, we note that another limitation for content-based
detection is that the training overhead is typically high; it
needs to be trained and updated constantly (with the fast-
changing web contents) and it has to be customized speciﬁ-
cally for each individual website.
Complementary to previous work on comment spam de-
tection, we approach the problem from a new perspective,
i.e., we focus on exploiting the structure of spamming in-
frastructure, an essential component of comment spamming
(which is relatively stable), rather than the content (which
changes frequently). The intuition behinds structure-based
inference is that, while spamming content can be dynamic,
spamming campaigns and spamming structure are much
more persistent.
If we can recognize spamming patterns
that characterize the structure of spammers’ infrastructure,
then we can continue to detect spam even if the spammers
frequently change their spam content.
Driven by proﬁts, spammers always want to take full ad-
vantage of their resources (e.g., spam harbors). Usually
there are two ways to achieve this goal: massive spamming
on a few harbors, or posting few on each harbor but spam-
ming on a large variety of harbors. If spammers post sim-
ilar spam content massively on a few harbors, it is easy to
be detected by content-based detection systems. However,
if spammers post spam on a large number of harbors, the
chance of detection at individual harbor is reduced. In par-
ticular, spammers typically keep using their familiar har-
bors, because these websites may not have effective saniti-
zation/ﬁlter mechanisms or posting on them can be easily
automated, thus making a comfort zone for the spammers.
In this paper, starting from a seed set of collected spam,
we ﬁrst perform a measurement study on spam harbors’
quality and the graph structure of spam harbors, which
reveals the structure of spammers’ infrastructure. We
ﬁnd that spam harbors usually have relatively low qual-
ities/reputations, which is quite counterintuitive because
spammers are expected to spam on high quality harbors to
maximize their proﬁts. To compensate the low reputations
of these harbors, spammers intend to keep using a large va-
riety of harbors for spamming. As for the structure of their
spamming infrastructure, we ﬁnd that spam harbors in the
same campaign always post similar spam links at the similar
time, which reﬂects that spam harbors in the same campaign
always have close relationships while normal websites do
not necessary have such relationships.
Based on observations from this measurement study, we
design a system named NEIGHBORWATCHER. Our intu-
ition is that if the promoting link in a comment also ap-
pears in the neighbors (cliques in the spam harbor infras-
tructure) of this harbor, it has a higher possibility of be-
ing a spam message, because normal links are not nec-
essary always been posted on the speciﬁc set of harbors
that have been veriﬁed to be exploited by the same spam-
mer/campaign. NEIGHBORWATCHER uses a graph-based
propagation algorithm to characterize neighborhood rela-
tionships among collected spam harbors in order to infer the
spamming infrastructure. When a new comment is posted
with some link, NEIGHBORWATCHER performs a neigh-
borhood watch on the graph and calculates a suspicious
score based on the graph-based inference. With a prototype
implementation running on a real-world dataset, we show
that NEIGHBORWATCHER can keep ﬁnding new spam and
spam harbors everyday.
The major contributions of this paper are summarized as
follows:
• We present the ﬁrst in-depth measurement study of the
quality and structure analysis of spam harbors. We ob-
serve that most of spam harbors have relatively low
reputations, which compels spammers to spam on a
large set of harbors. We also ﬁnd that spammers al-
ways have a stable set of harbors and keep using them
to post spam unless they are blocked.
• We propose a graph-based propagation algorithm to
characterize spamming infrastructure and an infer-
ence algorithm to infer comment spam. Our new ap-
proach is content-agnostic and does not need train-
ing/customization on individual websites.
• We implement a prototype system and evaluate it on
real-world data. Through our evaluation, NEIGHBOR-
WATCHER has detected 91,636 spam links and 16,694
Figure 1. The Workﬂow of Comment Spamming
spam harbors in total. Among these, 1,364 spam URLs
have not been indexed by Google yet. In addition, 147
spammer IPs and 4,008 spammer email addresses can
not be found in existing spam blacklists. This implies
that our system can keep ﬁnding new spam, and giving
earlier warning than Google and existing spam black-
list services.
The rest of the paper is structured as follows. We de-
scribe the background and our target problem in Section
2. We discuss the measurement study in Section 3. We
describe the design and implementation of NEIGHBOR-
WATCHER in Section 4 and evaluation results in Section
5. In Section 6, we discuss various extensions and limita-
tions of NEIGHBORWATCHER. We present related work in
Section 7 and conclude our work in Section 8.
detection inefﬁcient. Then spammers need to verify these
collected harbors to assure that they can automatically post
spam on these harbors without being easily blocked. They
can do this by posting random normal content. After ver-
iﬁcation, spammers begin to spam on validated harbors in
a large scale ( 2⃝). As a result, when search engine bots
crawl these harbors, they will index the spam URLs posted
in these harbors, which can ﬁnally improve the search rank
of the promoted websites in the spam ( 3⃝). Thus, when
users search certain keywords through search engines, those
promoted spam websites may have higher search ranks than
they should have ( 4⃝), which may lead victims to click spam
websites in search results ( 5⃝). Or victims may directly
click the embedded spam links when they read those spam
comments, which will directly lead them to spam websites
( 6⃝)
2 BackGround
2.2 Categories of Spam Harbors
In this section, we provide a brief overview of how ex-
isting comment spamming works. Then we present sev-
eral typical categories of harbors that are frequently used
by comment spamming.
2.1 Threat Model
As illustrated in Figure 1, in comment spamming, spam-
mers typical need to ﬁrst ﬁnd out suitable harbors, e.g.,
those with good reputations, those that can be automati-
cally spammed, or those that have weak or no sanitization
mechanisms. To achieve these goals, spammers usually use
different Google dorks [2] to ﬁnd target harbors or simply
buy some harbors from underground markets. In this way,
they can get a set of harbors that can be used to automat-
ically post spam (labeled as 1⃝). These collected harbors
are usually some forums, blogs, or guestbooks that support
user contributed content, and normal users typically con-
tribute a lot on them. In this case, spammers can easily dis-
guise them as normal visitors, which makes content-based
As described in Section 2.1, to launch efﬁcient comment
spam, spammers need to carefully choose their spam har-
bors. Next, we describe three most common types of har-
bors frequently used by comment spamming.
Web blogs are typically websites where people can post
articles. Usually these blogs have comment areas for each
article, which allow visitors to comment on corresponding
articles. Thus, spammers can also use these comment space
for spamming. A possible detection feature of such spam-
ming is that spammers may post repeating spam comments,
and also the time difference between the article and the
spam comment could be longer than the normal case [33].
Forums are typically websites where people can have
conversations in the form of posting messages. Since most
forums need users to register beforehand, spammers can
also post their spam as long as they can automatically reg-
ister accounts for these websites. Since some of conversa-
tions could last for a long time, it is hard to detect this kind
of spam based on the posting time.
GuestBooks are typically platforms to let visitors to
communicate with corresponding websites. Most compa-
nies websites have their guestbook pages to let people leave
a message to their companies. GuestBooks are more agile
than blogs and forums, because there are no normal tim-
ing patterns and no conversations; everyone can leave any
message anytime with fewer restrictions.
Table 1 summarizes the effectiveness of existing differ-
ent types of detection features on different types of harbors.
We can see that content-based and behavior-based features
are relatively effective for blog spam. Articles in blogs usu-
ally have speciﬁc topics, thus it is easy to detect spam whose
content is not related to articles. Also it is less common
for normal users to comment on an out-of-date articles in a
blog. Unfortunately, most blogs still do not have any such
detection system, which leaves them still to be a good plat-
form for spammers. In the contrast, there are typically no
speciﬁc topics in guestbooks; everyone can leave any mes-
sage at any time. Thus guestbook spam is hard for all ex-
isting detection features. Context-based features, i.e., de-
tecting URL cloaking, do not have good results on all the
harbors because of the very limited effectiveness scope. In
short, as we can clearly see that existing detection features
are certainly not enough in ﬁghting comment spam. The
need for effective and complementary techniques is press-
ing.
Table 1. Effectiveness of Different Types of
Detection Features
Features
Blogs
Forums
GuestBooks
bad
bad
Content Behavior Context
good
good
medium medium
bad
bad
bad
3 Spam Harbor Measurement
Comment spamming, as a relatively new trick of
spamdexing, has been reported for quite a while, ever since
2004 [4]. However, until recently, comment spamming has
not been sufﬁciently studied, and existing approaches are
clearly insufﬁcient as discussed earlier. To gain an in-depth
understanding of the comment spamming, we study the
problem from a new perspective, i.e., spam harbors, which
form the basic infrastructure for spammers. Why spammers
choose these harbors? Are there any special properties of
these harbors? Can we use these properties to help defend
against comment spam? In this section, we will try to an-
swer these questions and present what we have learned from
these harbors.
3.1 Dataset
To collect spam harbors, we started from 10,000 veri-
ﬁed spam links Sstudy (which are collected from our pre-
vious work [27]) and collected a dataset containing 38,913
real-world spam harbors, which are represented with unique
domain names. Speciﬁcally, we searched all these spam
links in Google and collected the search results. Among
these search results, not all of them are spam harbors, e.g.,
some are security websites that report those search links
as spam, and some are benign websites1 that link to those
search links. However, we observe that spam harbors typ-
ically contain embedded hyperlink tags (e.g., anchor tag
 and BBCode [U RL]:::[=U RL]). This
is because spammers perform automated posting on mas-
sive websites, and typically they are unsure whether their
target spam harbors support embedded links or not. Thus,
in order to achieve a high success rate of posting the spam
links, they choose to use embedded hyperlink tags [34].
Based on this observation, we only extracted those search
results with embedded hyperlink tags in their contents as
spam harbors.2
For each spam harbor, starting from the page that con-
tains our veriﬁed spam links, we further crawled all possible
pages on that website and recorded timestamps on the page
(typically these webpages always record the time when a
message is posted). Table 2 provides an overview of our col-
lected data. To study the differences among three categories
of harbors, we roughly group the collected harbors based
on their URLs. That is, if a harbor URL contains keywords
such as “blog”,“forum”,“guestbook”, we will cluster them
in blog, forum, and guestbook category, respectively. We
do not further distinguish remaining harbors without clear
keywords (listed as “other” in Table 2), and treat them as the
mixing of these three categories. Among 38,913 spam har-