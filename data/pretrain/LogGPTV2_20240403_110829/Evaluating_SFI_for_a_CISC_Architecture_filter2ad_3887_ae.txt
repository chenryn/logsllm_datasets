ACL2 can automatically prove each from the model and
the preceding lemmas. The proof required less than two
months of effort by a user with no previous experience
with proof assistants (the ﬁrst author). An experienced
ACL2 user could likely have produced a more elegant
proof in less time; our inexperience in choosing abstrac-
tions also made the going more difﬁcult as the proof size
increased. An example of a function from the executable
model and a lemma we have proved about it are shown
as the ﬁrst two parts of Figure 7. A disadvantage of
ACL2 compared to some other theorem provers is that
its proofs cannot be automatically checked by a simpler
proof checker. However, ACL2 has been well tested by
other academic and industrial users, and its underlying
logic is simple, so we still consider it trustworthy.
The precise statement of our ﬁnal safety result ap-
pears as the bottom part of Figure 7.
It is a correct-
ness result about the veriﬁer, modeled as a predicate
mem-sandbox-ok on the state of the code region be-
fore execution:
if the veriﬁer approves the rewritten
code, then for any inputs (modelled as the initial con-
tents of registers and the data region), execution of the
code will continue forever without performing an unsafe
operation.
(Unlike the real system, the model has no
exit() function.) Note that the rewriter does not ap-
pear in the formal proof, so the proof makes no claims
about it: for instance, we have not proved that the code
produced by the rewriter has the same behavior as the
original code. Though a statement like that could be
formalized, it would require a number of additional hy-
potheses; in particular, because the rewriter changes the
USENIX Association
Security ’06: 15th USENIX Security Symposium
219
(defun seq-reachable-rec (mem eip k)
(if (zp k) (if (= eip (code-start)) 0 nil)
(let ((kth-insn
(kth-insn-from mem (code-start) k)))
(or (and kth-insn (= eip kth-insn) k)
(seq-reachable-rec mem eip (- k 1))))))
(defthm if-reach-in-k-then-bound-by-kth-insn
(implies
(and (mem-p mem) (natp k) (natp eip)
(kth-insn-from mem (code-start) k)
(seq-reachable-rec mem eip k))
(<= eip (kth-insn-from mem
(code-start) k))))
(defthm safety
(implies
(and (mem-p mem) (mem-sandbox-ok mem)
(addr-p eax) (addr-p ebx) (addr-p ecx)
(addr-p edx) (addr-p esi) (addr-p edi)
(addr-p ebp) (addr-p esp)
(addr-p eflags) (data-region-p ebp))
(consp
(step-for-k
(x86-state (code-start) eflags eax ebx
ecx edx esi edi ebp esp mem)
k))))
Figure 7: From top to bottom, a typical function deﬁnition,
a typical lemma, and the ﬁnal safety result from our formal
ACL2 proof. seq-reachable-rec is a recursive proce-
dure that checks whether the instruction at location eip is
among the ﬁrst k instructions reachable from the beginning
of the sandboxed code region in a memory image mem. The
lemma states that if eip is among the ﬁrst k instructions, then
its address is at most that of the kth instruction. The safety
theorem states that if a memory image mem passes the veriﬁer
mem-sandbox-ok, then whatever the initial state of the reg-
isters, execution can proceed for any number of steps (the free
variable k) without causing a safety violation (represented by a
nil return value from step-for-k, which would not satisfy
the predicate consp).
address of instructions, code that say examined the nu-
meric values of function pointers would not behave iden-
tically.
One aspect of the proof to note is that it deals with a
subset of the instructions handled by the real tool: this
applies both to which instructions are allowed by the
simulated veriﬁer, and to which can be executed by the
x86 simulator. The subset used in the current version
of the proof appears in Figure 8. The instructions were
chosen to exercise all of the aspects of the security pol-
icy; for instance, jmp *%ebx is included to demon-
strate an indirect jump. Though small compared to the
number of instructions allowed by the real tool, this set
is similar to the instruction sets used in recent similar
proofs [2, 29]. We constructed the proof by beginning
with a minimal set of instructions and then adding addi-
tional ones: adding a new instruction similar to an exist-
ing one required few changes, but additions that required
mov addr, %eax
nop
mov %eax, addr
inc %eax
jmp addr
and $immed, %ebx mov %eax, (%ebx)
jmp *%ebx and $immed, %ebp mov %eax, (%ebp)
xchg %eax, %ebx
xchg %eax, %ebp
Figure 8: List of instructions in the subset considered in the
proof of Section 9.
a more complex safety invariant often involved extensive
modiﬁcations. The simulator is structured so that an at-
tempt to execute any un-modelled instruction causes an
immediate failure, so safety for a program written in the
subset that is treated in the proof extends to the complete
system. A related concern is whether the simulated x86
semantics match those of a real processor:
though the
description of the subset used in the current proof can
be easily checked by hand, this would be impractical for
a more complete model. To facilitate proofs like ours
in the future, as well as for applications such a founda-
tional proof-carrying code (see Section 10.6), it should
be possible to generate a description of the encoding and
semantics of instructions from a concise, declarative, and
proof-environment-neutral speciﬁcation.
In total, the proof contains approximately 60 func-
tion deﬁnitions and 170 lemmas, over about 2400 lines
of ACL2 code. The description of the model and the
statement of the safety result require about 500 lines; as-
suming ACL2’s veriﬁcation is correct, only this subset
must be trusted to be convinced of the truth of the re-
sult. The technical details of the proof are straightfor-
ward and rather boring; for space reasons, we do not dis-
cuss them further here.
Interested readers are referred
to a companion technical report [17]; the proof in its
machine-checkable form is also available from the PittS-
FIeld project website.
10 Related work
This section compares our work with previous imple-
mentations of SFI, and with other techniques that ensure
memory safety or isolation including code rewriting, dy-
namic translation, and low-level type systems.
It also
distinguishes the isolation provided by SFI from the sub-
version protection that some superﬁcially similar tech-
niques provide.
10.1 Other SFI implementations
Binary sandboxing was introduced as a technique for
fault-isolation by Wahbe, Lucco, Anderson, and Gra-
ham [27]. The basic features of their approach were de-
scribed in Sections 2 and 4. Wahbe et al. mention in a
footnote that their technique would not be applicable to
220
Security ’06: 15th USENIX Security Symposium
USENIX Association
architectures like the x86 without some other technique
to restrict control ﬂow, but then drop the topic.
Subsequent researchers generally implemented a re-
striction on control ﬂow for CISC architectures by col-
lecting an explicit list of legal jump targets. The best
example of such a system is Small and Seltzer’s MiS-
FIT [25], an assembly-language rewriter designed to iso-
late faults in C++ code for an extensible operating sys-
tem. MiSFIT generates a hash table from the set of legal
jump targets in a program, and redirects indirect jumps
through code that checks that the target appears in the
table. Function return addresses are also stored on a sep-
arate, protected stack. Because control ﬂow is prevented
from jumping into the middle of them, the instruction
sequences to sandbox memory addresses don’t require a
dedicated register, though MiSFIT does need to spill to
the stack to obtain a scratch register in some cases. A
less satisfying aspect of MiSFIT is its trust model. The
rewriting engine and the code consumer must share a se-
cret, which the rewriter uses to sign the generated code,
and MiSFIT relies on the compiler to correctly man-
age the stack and to produce only safe references to call
frames. Besides the trustworthiness problems of C com-
pilers related to their complexity and weak speciﬁcation
(as exempliﬁed by the attack against MiSFIT shown in
Figure 1), this approach also requires something like a
public-key certiﬁcate infrastructure for code producers,
introducing problems of reputation to an otherwise ob-
jective question of code behavior.
Erlingsson and Schneider’s SASI tool for the x86 [10]
inserts code sequences very similar to MiSFIT’s, except
that its additions are pure checks that abort execution
if an illegal operation is attempted, and otherwise fall
through to the original code, like PittSFIeld’s ‘check’
mode. In particular, the SASI tool is similar to MiSFIT
in its use of a table of legal jump targets, and its decision
to trust the compiler’s manipulation of the stack. Lu’s
C+J system [16] also generates a table of legal jump des-
tinations, but the indices into the table are assigned se-
quentially at translation time, so there is no danger of
collision.
The Omniware virtual machine [3], on which Wahbe
and Lucco worked after the classic paper, uses SFI in
translating from a generic RISC-like virtual machine to
a variety of architectures, including the x86. The Om-
niware VM implemented extensive compiler-like opti-
mizations to reduce the overhead of sandboxing checks,
achieving average overheads of about 10% on selected
SPEC92 benchmarks. However, the focus of the work
appears to have been more on performance and portabil-
ity than on security; available information on the details
of the safety checks, especially for the x86, is sparse. In
a patent [28] Wahbe and Lucco disclose that later ver-
sions of the system enforced more complex, page-table
like memory permissions, but give no more details of the
x86 implementation.
As far as we know, our work described in Section 9
was the ﬁrst machine-checked or completely formalized
soundness proof for an SFI technique or implementation.
Necula and Lee [20] proved the soundness of SFI as ap-
plied to particular programs, but not in general, and only
in the context of simple packet ﬁlters. In work concur-
rent with ours, Abadi et al.
([2], see Section 10.3 for
discussion) give a human-readable prose proof for the
safety of a model of their CFI system, which is sim-
ilar to SFI. In work subsequent to our proof (ﬁrst de-
scribed in [18]), Winwood and Chakravarty developed
a machine-checked safety proof in Isabelle/HOL for a
model of an SFI-like rewriting technique applicable to
RISC architectures [29]. To avoid having to move in-
structions, their approach overwrites indirect jump in-
structions with direct jumps of the same size to a trusted
dispatcher. Unfortunately, this puts a 2MB limit on the
size of binaries to which their technique is applicable:
for instance, they were able to rewrite only a subset of
the SPECint2000 suite.
10.2
Isolation and preventing subversion
In general, a security failure of a system occurs when an
attacker chooses input that causes code to perform dif-
ferently than its author intended, and the subverted code
then uses privileges it has to perform an undesirable ac-
tion. Such an attack can be prevented either by prevent-
ing the code’s execution for being subverted, or by iso-
lating the vulnerable code so that even if subverted, it
can still cannot take an undesirable action. Many secu-
rity techniques are based on the prevention of subversion:
for instance, ensuring that procedure calls always return
to their call sites, even if the stack has been modiﬁed by a
buffer overrun. SFI, by contrast, is fundamentally a tech-
nique for isolating one part of a program from another.
To function as a security technique, this isolation must
be used to support a design that divides a system into
more and less trusted components, and restricts the inter-
actions between the two. Examples of such designs in-
clude the device driver and network server isolation tech-
niques discussed in Section 1, and the untrusted VXA
decompressors of Section 8.
Incidentally, SFI subsumes some mechanisms that
have been suggested as measures to make program sub-
version more difﬁcult. For instance, PittSFIeld prohibits
execution of code on the stack and reduces the number of
possible targets of an overwritten function pointer. How-
ever, these side-effects should not be confused with the
intended isolation policy. SFI does not provide general
protection against attacks on the untrusted code; it sim-
ply contains those attacks within the component.
USENIX Association
Security ’06: 15th USENIX Security Symposium
221
Figure 9: Runtime overheads for PittSFIeld in the default mode (black bars), PittSFIeld in jump-only mode (gray bars), and CFI
(white bars) for the SPECint2000 benchmarks. PittSFIeld results are the same as those in Figure 4, but not broken down by cause.
CFI results are taken from Figure 4 of [1], which does not include results for Perl. Because these were separate experiments with
other variables not held constant, care should be used in comparing them directly.
10.3 CFI
In concurrent work [1], the Gleipnir project at Microsoft
Research has investigated a binary-rewriting security
technique called Control-Flow Integrity, or CFI. As sug-
gested by the name, CFI differs from SFI in focusing
solely on constraining a program’s jumps: in the CFI im-
plementation, each potential jump target is labelled by a
32-bit value encoded in a no-op instruction, and an in-
direct jump checks for the presence of an appropriate
tag before transferring control. This approach gives ﬁner
control of jump destinations than the SFI techniques of
Wahbe et al., or PittSFIeld, though the ideal precision
could only be obtained with a careful static analysis of,
for instance, which function pointers might be used at
which indirect call sites. In the basic presentation, CFI
relies on an external mechanism (such as hardware) to
prevent changes to code or jumps to a data region, but
it can also be combined with inserted memory-operation