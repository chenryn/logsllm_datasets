Figure 7b shows that DeepCoFFEA performance continually
improved as the triplet loss decreased. We stopped the training
at 0.0018, since the loss decreased very slowly after 0.003. In
Table III, we also report the training time required to achieve
different loss values. There is a clear trade-off between model
quality and training time.
Time-Separated Testing Sets. We explored how a separa-
tion between the time (and thus, incidentally, Tor software
versions3)
of training set collection and test set collection impacted the
correlation ability of DeepCoFFEA. For this study, we trained
FENs on data from June 2021, while constructing three testing
sets: one from August 2021 (2-month-newer set), one from
April 2020 (14-month-old set), and the DeepCorr set collected
in January-April 2018 (3-year-old set).
As shown in Figure 7c, DeepCoFFEA performs approxi-
mately the same for the 2-month-newer set and the 14-month-
old set as a baseline test with no time gap. The performance
was signiﬁcantly worse using the 3-year-old set. This dramatic
shift may be explained at least in part by the differences
in the collection process between the DeepCorr set and the
other two datasets. We also expect that Tor ﬂows simply
changed more signiﬁcantly during this three-year window.
Nevertheless, DeepCoFFEA still detected some correlated
ﬂows correctly with 92% TPR and 3% FPR.
These ﬁndings indicate that even though the packet size
and timing features could change as network conditions evolve
over the time [38], the statistical difference between correlated
and uncorrelated features could be similar even after 14
months. As FENs are trained to maximize the difference be-
tween correlated and uncorrelated ﬂows, the ability to generate
the highly correlated ﬂow features can be persistent even after
more than a year. Thus, even though the training cost required
to ensure better model quality was considerable (Table III),
this may be amortized over long periods, as DeepCoFFEA
does not require frequent retraining.
Defended Traces. Nasr, Bahramali and Houmansadr [25]
evaluated DeepCorr against
traces protected by the obfs4
pluggable transport (PT), the PT recommended by the Tor
Project for censorship evasion [39]. obfs4 encrypts and trans-
forms the trafﬁc between the client and the guard node to
avoid potential trafﬁc-analysis-based censorship. In particular,
it obfuscates packet sizes by appending random padding.
obfs4 also provides an IAT (Inter-Arrival Timing) mode that
randomizes inter-arrival
times. We investigated obfs4 with
both IAT mode on (obfs4-iat1) and off (obfs4-iat0).
Security researchers have also investigated several website
ﬁngerprinting defense mechanisms designed to mask trafﬁc
patterns in Tor [33], [35], [40]. As the adversary conducts
trafﬁc analysis on the Tor ﬂow between the client and entry
guard, these defenses hide the total packet statistics by adding
padding packets according to various schemes. For example,
3Client versions in the 0.2.x, 0.3.x and 0.4.x series were used for the
14-month-old set, the three-year-old set, and both the two-month-newer and
training sets, respectively.
WTF-PAD [32] seeks to hide statistically unlikely (and thus
distinguishing) IPDs between packets by strategically adding
padding, while FRONT [33] adds more randomness to the
amount of padding and the location where it is injected.
Since these WF defenses reduce the similarity between Tor
ﬂows, we might expect that they also make correlated ﬂow
features less effective. Note that padding packets are only seen
on the Tor ﬂows, but not the exit ﬂows, making the matched
ﬂows look less alike than in undefended Tor. Therefore, in this
study, we evaluated the effectiveness of DeepCoFFEA against
the obfs4 PT, WTF-PAD and FRONT. To the best of our
knowledge, this is the ﬁrst investigation of the effectiveness
of WF defenses against end-to-end ﬂow correlation attacks.
Using the same features discussed in Section V-A, we
trained three different DeepCoFFEA models using 10,000
defended traces collected with each of obfs4-iat0, obfs4-iat1,
WTF-PAD4, and FRONT5 defenses. As shown in Figure 7d,
the DeepCoFFEA TPR decreased across all defenses, while
still maintaining very low FPRs. DeepCoFFEA achieved TPRs
above 50% for FPR of 10−5 for all defenses besides FRONT.
FRONT had the most success defeating DeepCoFFEA, since
the obfuscation level in each window is random, making the
correlation pattern across windows less consistent.
An adversary may face the possibility of some users of
interest who apply a defense while most other users take
a default setting of not applying the defense. This scenario
allows us to examine whether FENs can be extended to detect
different types of ﬂows. We trained the model using a mix of
defended (i.e., obfs4) and undefended ﬂows with a ratio of 1:4
and then tested the model using two different testing sets, one
for each type of ﬂow. As shown in Figure 7e, even though the
model trained using a mix had somewhat worse performance
against undefended traces (the mix-undef curve in Figure 7e),
FENs were still capable of generating effective embedding
vectors for both undefended and defended traces. This result
indicates that
the inclusion of defended traces marginally
impacted the correlation capability and shows the potential
of uniﬁed FENs to detect both types of ﬂows successfully.
Lastly, we explored a more difﬁcult setting for DeepCoF-
FEA, in which we trained FENs using undefended traces and
then detected correlated ﬂows of unknown defenses. We note
that this is a rather artiﬁcial attack scenario, since it is contrary
to Kerckhoff’s principle. Figure 7e shows that DeepCoFFEA
still achieved a TPR above 20% for defended ﬂows with FPR
of 10−5, even though it was not trained on any defended traces.
Correlations thus appear to remain between the Tor ﬂow and
the exit ﬂow for DeepCoFFEA to ﬁnd. We leave for future
work the question of whether the inclusion of ﬂows from
different defenses in training data could improve this result.
4We used normal_rcv as the distribution parameter (bandwidth over-
head: 27.54%).
5We used the default setting of FRONT with the padding budget as Ns =
1700 (proxy side) and Nc = 1700 (client side), and the padding window
with Wmin = 1 and Wmax = 14 (bandwidth overhead: 33.26%).
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:58:24 UTC from IEEE Xplore.  Restrictions apply. 
1925
(a) ROC
(c) ROC using 5,000 and 10,000 testing ﬂow pairs.
Fig. 8. Comparison of state-of-the-art and DeepCoFFEA attacks (Note that x-axes except Figure 8b are in log scale, CTA: Compressive Trafﬁc Analysis,
DC: DeepCorr, m-DC: multi-DeepCorr, DCF: DeepCoFFEA (loss ≈ 0.0018), and RG: Random guessing).
C. Comparison to the State-of-the-Art
(b) BDR against TPR
TABLE IV
SPACE AND TIME COMPLEXITIES OF DEEPCORR (DC), m-DEEPCORR
(M-DC) AND DEEPCOFFEA (DCF) FOR VARYING NUMBER OF TESTING
FLOW PAIRS. WE REPORT BOTH MAIN (MM) AND GPU (GM) MEMORY
CONSUMPTION (I.E., MM(GM)) IN SPACE COMPLEXITY.
Space (gigabytes)
Time (seconds)
In this section, we compare the performance of DeepCoF-
FEA to several previous ﬂow correlation algorithms, includ-
ing Cosine similarity, RAPTOR, CTA, DeepCorr, and m-
DeepCorr, a multi-stage variant of DeepCorr. This variant of
DeepCorr was suggested, but not evaluated, by Nasr, Bahra-
mali and Houmansadr [25] as a way of applying DeepCorr in
stages to decrease both time complexity and false positives.
In this variant, the attacker trains both a lower-dimensional
version of DeepCorr that uses only the ﬁrst p packets to ﬁnd
correlated ﬂows and a full network that uses longer ﬂows of
length p + l. When searching for correlated pairs, all pairs are
evaluated using the less-expensive network; only pairs that are
considered to be correlated by this network are then evaluated
by the full network, and only ﬂows ﬂagged as matches by both
networks are considered to be correlated.
Tuning DeepCorr and m-DeepCorr. For a fair comparison,
we tuned DeepCorr and m-DeepCorr to obtain the best perfor-
mance on the DCF set. First, for DeepCorr, we found the best
feature dimension (number of packets) to use and the best
number of training ﬂow pairs. The dimension is important,
because the attack requires both Tor and Exit ﬂow feature
vectors to have the same length. Using longer vectors will
induce padding that decreases accuracy, while using shorter
vectors might
information. As detailed in
Appendix E, we empirically chose 700 packets as the ﬂow
length and 5,000 ﬂow pairs as the training set size.
truncate useful
To determine the combination of input lengths that had the
best performance for m-DeepCorr, we used multiple DeepCorr
models trained using p packets (namely DCp), and explored a
variety of multi-stage settings to choose the best conﬁguration
to maximize performance and minimize time complexity. We
found that a 2-stage attack using DC100 as the ﬁrst stage
and DC700 as the second stage yielded better performance
than other settings. The full details and results of this tuning
process appear in Appendix F.
Performance Comparison. The results of our comparison
are shown in Figures 8a and 8b, in which we also evaluate
CTA, RAPTOR, and Cosine similarity. After hyperparameter
tuning, we adopted 2,200 and 2,000 packets as the effective
ﬂow lengths for RAPTOR and cosine similarity, respectively.
2,094
21,128
8,041
435
5,000
118,532
43,641
663
10,000
478,667
174,067
1,496
2,094
43(15)
43(15)
3(7)
5,000
43(15)
43(15)
5(7)
10,000
43(15)
43(15)
6(7)
DC
m-DC
DCF
TABLE V
SPACE AND TIME COMPLEXITIES OF TRAINING DEEPCORR AND
DEEPCOFFEA (LOSS≈0.004). WE REPORT BOTH MAIN (MM) AND
GPU(GM) MEMORY (I.E., MM(GM)) IN SPACE COMPLEXITY.
Time (days)
Space (gigabytes)
DC
DCF
2.5
3
134(16)
133(6)
DeepCoFFEA outperformed all other attacks when correlating
2,094 ﬂow pairs, reaching a much higher TPR for any given
FPR. This substantial improvement correspondingly led to a
higher BDR as shown in Figure 8b. CTA failed to detect
the correlation between Tor and exit ﬂows effectively, sug-
gesting that the transformations induced by the Tor network
are more extreme than the perturbations considered by Nasr,
Houmansadr, and Mazumdar [29]. Both DeepCorr and m-
DeepCorr performed considerably worse for FPRs closer to 0.
In contrast, DeepCoFFEA detected more than half of associ-
ated pairs correctly. DeepCoFFEA outperformed m-DeepCorr
by signiﬁcant margins, e.g., 89% vs 13% TPR at 10−4 FPR.
Even when increasing the testing dataset size up to 10,000
(non-training) ﬂow pairs, DeepCoFFEA detected the corre-
lated ﬂows more effectively than both DeepCorr attacks with
85% vs 7.6% TPR at 10−4 FPR as shown in Figure 8c.
Space and Time Complexity. We compare the runtime of
DeepCorr, m-DeepCorr and DeepCoFFEA in Table IV for
varying testing set sizes (tn). We computed the total time to
complete the full tn × tn ﬂow attack, including data loading
and the correlation metric computation. For DeepCoFFEA, we
computed the total time for loading testing ﬂows, generating
feature embeddings, computing cosine similarity scores, and
aggregating the resulting votes across 11 windows. For m-
DeepCorr, we summed the time elapsed for the ﬁrst stage
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:58:24 UTC from IEEE Xplore.  Restrictions apply. 
1926
(a) ROC for old and future testing sets
(b) ROC for various defenses
Fig. 9. DeepCorr performance against various testing sets and defenses (Note that x-axes are in log
scale and RG: Random guessing).
Fig. 10. DeepCoFFEA performance against
FRONTn by varying the padding window
length (n = Wmax) and the Decaf defense
(Note that x-axis is in log scale, RG: Random
Guessing, and DeepCoFFEA loss ≈ 0.006).
with DC100 and the second stage with DC700. In addition,
we measured the main and GPU memory consumption while
testing all three attacks.
Compared to DeepCorr, DeepCoFFEA had much lower
time and memory requirements. Even though the multi-stage
setting reduced the overhead of DeepCorr for longer ﬂows,
for the correlation analysis using 10,000 testing ﬂow pairs,
DeepCoFFEA still performed faster than m-DeepCorr by
two orders of magnitude (116:1) while using less memory.
We also note that the cost gap between m-DeepCorr and
DeepCoFFEA further increased as the number of ﬂow pairs
increased. The discrepancy between both time and space costs
of DeepCoFFEA and DeepCorr clearly showed the beneﬁt of
needing to apply FENs only once per ﬂow window versus
applying DC100 to every pair of ﬂows. More speciﬁcally, we
only needed to evaluate the Tor FEN 11 × tn times and the
Exit FEN 11 × tn times to generate all feature embeddings,
rather than evaluating t2
n instances of the DeepCorr CNN.
The combination of feature embedding and ampliﬁcation
helps improve the state-of-the-art performance while reducing
the complexity signiﬁcantly, to the point that a DeepCoFFEA-
based end-to-end correlation attack may be feasible to deploy
at Tor scale.
Time Gap between Training and Testing Sets. Nasr, Bahra-
mali and Houmansadr [25] investigated DeepCorr using a test-
ing set collected three months later than the training set. To see
the effects of further separation, we evaluated DeepCorr using
DCF testing sets separated by 14 months. Figure 9a shows
that DeepCorr performance degraded signiﬁcantly with the 14-
month-old testing set, while it performed comparably on the 2-
month-newer set. Based on the observation that DeepCoFFEA
effectively detected the correlated ﬂows even after a 14-month
gap, this result clearly indicates that DeepCoFFEA requires
much less frequent re-training than DeepCorr, offsetting the
slightly greater training time complexity for DeepCoFFEA
compared to DeepCorr (Table III and V).
Robustness against Defenses. We further evaluated Deep-
Corr against the obfs4, WTF-PAD, and FRONT defenses.
Figure 9b shows that DeepCorr signiﬁcantly degraded against
all defenses with 2.7% (obfs4), 2.5% (WTF-PAD), and 1.8%
(FRONT) TPRs at 10−3 FPR, whereas DeepCoFFEA detects
more than 80% of correlated pairs at the same FPR. This
result indicates the importance of ampliﬁcation in DeepCoF-
FEA since many positive predictions on unmatched ﬂow
windows failed to get enough votes to become FPs. The
relative effectiveness of FRONT against both attacks suggests
that WF defenses may be effective against ﬂow correlation,
if the defense can also bypass the ampliﬁcation effect of
DeepCoFFEA.
D. Summary of Contributions
We summarize the key ﬁndings as follows:
• Compared to state-of-the-art
attacks, DeepCoFFEA
achieved a lower computational cost (Table IV) since
FENs are used to extract only O(n) feature embedding
vectors. Even though m-DeepCorr improved the complex-
ity of vanilla DeepCorr, DeepCoFFEA is more efﬁcient
than m-DeepCorr by two orders of magnitude.