the application
without starting any recovery. As a consequence, adopted
resiliency mechanisms are only partially effective.
fail more pronouncedly because of
Large-scale applications (executing on more than 10,000
nodes)
interconnect-
related issues. Figures 10.(a) and (b) show the probability of an
application terminating because of system problem as a function
of the application scale, for XE and XK nodes, respectively.
Data in these plots include applications implemented with and
without checkpoint/restart. The error bars show the 95% conﬁ-
dence intervals. Larger bars for large-scale applications are due
to the limited number of applications that can effectively use
full-scale executions (about 0.04% of the XE applications and
0.01% of XK applications use more than 75% of the system).
As one might expect, we observe a trend of increasing failure
probability as XK and XE applications scale up. However, the
magnitude of the increase is different across XE nodes and
XK nodes. In particular, for medium-scale XE applications
executing on less than 10,000 nodes, we observe only a small
increment in the failure probability, which stays below 1%.
When scaling further, we see a substantial change in the
slope of the plot. We measured a failure probability of 0.162
for applications running on more than 22,000 nodes against a
failure probability of 0.008 for applications running on fewer
than 10,000 nodes. We analyzed the causes of the application
s
n
o
i
l
i
t
a
c
p
p
a
E
X
–
0.20 
0.15 
0.10 
0.05 
0 P
b
o
r
e
r
u
f
.
a
l
i
s
n
o
i
t
a
c
i
l
p
p
a
K
X
–
e
r
u
l
i
a
f
.
b
o
r
P
0.15 
0.10 
0.05 
 0 
0                  5,000              10,000            15,000           20,000  
Application scale [number of nodes] 
(a) 
( )
 (b)                        
0                    1,000                2,000                3,000              4,000  
Application scale [number of nodes] 
]
g
o
l
[
s
n
o
i
t
a
c
i
l
p
p
a
E
X
–
e
r
u
l
i
a
f
.
b
o
r
P
]
g
o
l
[
s
n
o
i
t
a
c
i
l
p
p
a
K
X
–
e
r
u
l
i
a
f
.
b
o
r
P
1E-1 
1E-2 
1E-3 
1E-4 
1E-5 
1E-6 
1E-1 
1E-2 
1E-3 
1E-4 
1E-5 
1E-6 
0.25      0.5      1        2        4        8      16      32      64          180 
XE applications node hours [ x 1,000] - log 
(a)
(b)                           
500 700 10001300 1750 2500 3500 5000 7000 10000 15000
0.5                 1                      2             4                   8                 16    
XK applications node hours [ x 1,000] - log 
Fig. 10. Effects of XE (a) and XK (b) application scale (number of nodes)
on the probability of application failure. The shaded area represents the 95%
conﬁdence interval of the smoothing polynomial function.
failures occurring before and after the change of the slope
(approximately 10,000 nodes). We observed that the percentage
of applications that fail because of interconnect-related issues
increases following similar trend. For example, we measured
that, for large-scale runs (≥ 10, 000 nodes), 66% of the failures
of namd when running on XE nodes were because of Gemini-
and LNet-related problems, but 23% for small-medium scale
runs (< 10, 000 nodes). This ﬁnding shows that interconnect
resiliency starts becoming a major concern when scaling up to
more than 50% of the system size.
The number of processed node hours has a more pro-
nounced effect on the resiliency of XK applications than on
the resiliency of XE applications. Figure 11.(a) and (b) show
the effect of the number of node hours on the probability of
application failures for XE and XK applications using log-log
plots. To formalize our observation above, we ﬁt the data in
the plots with a linear regression model (in the log-log space).
The probability of application failures for both XE and XK
applications can be described with good approximation (as
measured by the R2 score shown in Figure 11.(a) and (b)) by a
monomial relationship7 of the form y = αxk, which appears as
a straight line in a log-log graph, with the power and constant
term corresponding, respectively, to slope and intercept of the
line. Looking at the ﬁtting, we observe that the probability of an
application failure follows a power-of-3 function of the node
hours for XK applications and a power-of-2 function of the
node hours for XE nodes. This ﬁnding emphasizes the need
for i) dedicated resiliency techniques (e.g., memory protection
7Given a monomial equation y = axk, taking the logarithm of the equation
(with any base) yields to (log y) = k (log x) + log a. Setting X = log x and
Y = log y (i.e., moving to log-log graph), yields the equation Y = mX + b.
Fig. 11. Effects of XE (a) and XK (b) scale (node hours) on the probability
of application failure. The shaded area represents the 95% conﬁdence interval
of the smoothing polynomial function.
using the chipkill technique) to be deployed for preventing error
propagation from the hardware to the application-level and ii)
effective testing of extreme-scale hybrid applications in order
to harness hybrid computing cores in future machines.
VIII. RELATED WORK
Improved fault tolerance comes from detecting and auto-
correcting a greater fraction of high-impact errors. Prior re-
search activities have centered on i) analyzing error logs[12]–
[18] as well as some online analysis of patterns preceding a
failure, and ii) evaluated the accuracy and efﬁcacy of anomaly
detection and proactive responses[19], [20]. Several studies
have attempted to evaluate large-scale systems [12]–[18] to
describe basic error characteristics [12], [13], [16], [18], to
model
large-scale systems [17], [21], [22], and to provide
failure prediction and proactive resiliency mechanisms [23],
[24]. However, most studies do not consider the impact of
errors and failures in their analysis, making produced reactive
methods less effective. As we demonstrate in this paper, it is
advantageous to analyze system-level error events as a function
of the application. A second challenge them is to combine the
data produced by these diverse sources in a robust fashion.
Fault/failure characterizations to date have typically identi-
ﬁed occurrences by component type (memory, voltage regulator,
CPU, etc.) and/or by failure type (hardware, software, network,
human, etc.), system wide over a period of time [2], [15].
The work in [2] characterizes the effectiveness of the failover
procedures and provides measures on the rate of uncorrectable
errors in Blue Waters. However, the work does not provide
measurements on how the errors impact production workload.
In our work, we quantify the impact of those errors on pro-
duction workload. We show that many applications are able to
3535
complete during system-wide outages thanks to the containment
of system failures to a limited portion of the system, e.g., to
speciﬁc ﬁle systems or cabinets.
Fault/error characterization is often done for the purpose
of creating effective prediction engines. A common approach
is to examine correlations between past events in order to
learn patterns to use in predicting future failures, e.g., [19],
[20]. In those studies, efﬁcient data ﬁltering (e.g., [16]–[18],
[22], [25]) is essential to meaningful analysis. While much
of this work provides novel ﬁltering approaches, little of it
includes in the analysis those errors that really impact the
production workload. As a result, the available error/failure
characterization studies and techniques
for extreme-scale
machines do not provide sufﬁcient ﬁdelity of understanding
to enable researchers and system architects to determine
how applications behave when exposed to errors or to assess
requirements for future architectures. Some errors do not pose
a real threat to either system or application operations. Blue
Waters logs, however, contain more than 150 different types
of errors that can impact user applications. Our analysis and
our LogDiver tool are the ﬁrst to correlate errors with user
application failures in extreme-scale environments.
IX. CONCLUSIONS
This paper reports on the study of extreme-scale application
resiliency based on automatically collected system and appli-
cation data logs. The study focuses on application failures due
to system-related problems. The lessons learned include:
In terms of the application susceptibility to system problems,
our analysis shows that 1.53% of applications fail due to system
problems. One may argue that this is a small percentage of the
overall application runs (more than 5,000,000) considered in
this study. However, the failed applications contribute to about
9% of production node hours. As a result, the system consumes
computing resources and encounters signiﬁcant energy cost
from the work that must be redone.
In terms of
resiliency technique, our data show that
application-level checkpoint/restart plays an essential role in
improving application resiliency to system problems. The
MNBF of applications protected with checkpoint/restart tech-
niques is at least 2 (or more) times greater than the MNBF of
applications that do not have checkpoint/restart support.
In terms of the design of future extreme-scale (or exascale)
systems, our study indicates that one must be cautious when
using a massive number of hybrid computing nodes. For exam-
ple, in the analyzed system, the application failure probability
increases following the cubic function of the number of node
hours when executing on GPU nodes. Consequently, dedicated
resiliency techniques (e.g., memory protection using the chipkill
technique) must be deployed to prevent errors from propagating
to the application level.
ACKOWLEDGEMENT
This work is partially supported by the NSF CNS 10-18503
CISE, Air Force Research Lab FA8750-11-2-0084, and an IBM
faculty award. We thank Celso Mendes, Gregory Bauer, Jeremy
Enos, and Joshi Fullop for providing the raw data and many
insightful conversations. We ﬁnally thank Fabio Baccanico
3636
for the help in creating the ﬁrst version of LogDiver, and
Domenico Cotroneo for the comments on the ﬁrst version of
this manuscript.
REFERENCES
[1] www.cray.com/Assets/PDF/products/xe/CrayXE6Brochure.pdf.
[2] C. Di Martino, F. Baccanico, J. Fullop, W. Kramer, Z. Kalbarczyk, and
R. Iyer. Lessons learned from the analysis of system failures at petascale:
The case of blue waters. In Proc. of 44th Annual IEEE/IFIP Int. Conf.
on Dependable Systems and Networks (DSN), 2014.
[3] http://www.adaptivecomputing.com/products/hpc-products/
moab-hpc-suite-enterprise-edition.
[4] Celso L. Mendes, Brett Bode, Gregory H. Bauer, Jeremy Enos, Cristina
Beldica, and William T. Kramer. Deploying a large petascale system:
The blue waters experience. Procedia Computer Science, 29(0):198 –
209, 2014. 2014 International Conference on Computational Science.
[5] Peter Johnsen, Mark Straka, Melvyn Shapiro, Alan Norton, and Thomas
Galarneau. Petascale wrf simulation of hurricane sandy deployment
of ncsa’s cray xe6 blue waters.
In Proceedings of the International
Conference on High Performance Computing, Networking, Storage and
Analysis, SC ’13, pages 63:1–63:7, New York, NY, USA, 2013. ACM.
Structure: SSA and related techniques.
completion analysis. CUG 2010, Edinburg, UK, 2010.
2010, Edinburg, UK, 2010.
CUG 2008, Helsinki, Finland, 2008.
[6] Nekrutkin V. Zhigljavky A Goljadina, N.
Analysis of Time Series
[7] Woo-Sun Yang Hwa-Chun Wendy Lin, Yun (Helen) He. Franklin job
[8] Matt Ezell. Collecting application-level job completion statistics. CUG
[9] Nicholas P. Cardo. Detecting system problems with application exit codes.
[10] R.G. Edwards and B. Joo. The chroma soft ware system for lattice qcd.
[11] G. Zheng, S. Lixia, and L.V. Kale.
Ftc-charm++: an in-memory
checkpoint-based fault tolerant runtime for charm++ and mpi. In IEEE
Cluster 2004, pages 93–103, 09/ 2004.
[12] R. K. Sahoo, A. Sivasubramaniam, M. S. Squillante, and Y. Zhang.
Failure data analysis of a large-scale heterogeneous server environment.
In DSN ’04: Proc. of the 2004 Int. Conference on Dependable Systems
and Networks, pages 772–781, 2004.
[13] Y. Liang, A. Sivasubramaniam, J. Moreira, Y. Zhang, R.K. Sahoo, and
M. Jette. Filtering failure logs for a bluegene/l prototype. In DSN ’05:
Proc. of the 2005 Int. Conference on Dependable Systems and Networks,
pages 476–485, 2005.
[14] Y. Liang, Y. Zhang, M. Jette, Anand Sivasubramaniam, and R. Sahoo.
Bluegene/l failure analysis and prediction models. In Dependable Systems
and Networks, 2006. DSN 2006. Int. Conference on, pages 425–434, 2006.
[15] B. Schroeder and G.A. Gibson. A large-scale study of failures in high-
performance computing systems. Dependable and Secure Computing,
IEEE Transactions on, 7(4):337–350, 2010.
[16] A. Oliner and J. Stearley. What supercomputers say: A study of ﬁve
system logs. Dependable Systems and Networks, 2007. DSN ’07. 37th
Annual IEEE/IFIP Int. Conference on, pages 575–584, June 2007.
[17] C. Di Martino, M. Cinque, and D. Cotroneo. Assessing time coalescence
techniques for the analysis of supercomputer logs. In In Proc. of 42nd
Annual IEEE/IFIP Int. Conf. on Dependable Systems and Networks
(DSN), 2012, pages 1–12, 2012.
[18] A. Pecchia, d. Cotroneo, Z. Kalbarczyk, and R. K. Iyer. Improving log-
based ﬁeld failure data analysis of multi-node computing systems.
In
Proceedings of the 2011 IEEE/IFIP 41st International Conference on
Dependable Systems&Networks, DSN ’11, pages 97–108, Washington,
DC, USA, 2011. IEEE Computer Society.
[19] A. Gainaru, F. Cappello, M. Snir, and W. Kramer. Fault prediction under
the microscope: A closer look into hpc systems. In High Performance
Computing, Networking, Storage and Analysis (SC), 2012 International
Conference for, pages 1–11, 2012.
[20] A. Gainaru, F. Cappello, S. Trausan-Matu, and W. Kramer. Event log
mining tool for large scale hpc systems.
In Proceedings of the 17th
international conference on Parallel processing - Volume Part I, Euro-
Par’11, pages 52–64, Berlin, Heidelberg, 2011. Springer-Verlag.
[21] E. Heien, D. Kondo, A. Gainaru, A. LaPine, W. Kramer, and F. Cappello.
Modeling and tolerating heterogeneous failures in large parallel systems.
In Proceedings of 2011 International Conference for High Performance
Computing, Networking, Storage and Analysis, SC ’11, pages 45:1–45:11,
New York, NY, USA, 2011. ACM.
[22] C. Di Martino. One size does not ﬁt all: Clustering supercomputer failures
using a multiple time window approach. In JulianMartin Kunkel, Thomas
Ludwig, and HansWerner Meuer, editors, International Supercomputing
Conference - Supercomputing, volume 7905 of Lecture Notes in Computer
Science, pages 302–316. Springer Berlin Heidelberg, 2013.
[23] X Chen, C.D. Lu, and K. Pattabiraman. Predicting job completion times
using system logs in supercomputing clusters. In Dependable Systems and
Networks Workshop (DSN-W), 2013 43rd Annual IEEE/IFIP Conference
on, pages 1–8, June 2013.
[24] A. Gainaru, F. Cappello, and W. Kramer. Taming of the shrew: Modeling
the normal and faulty behaviour of large-scale hpc systems. In Parallel
Distributed Processing Symposium (IPDPS), 2012 IEEE 26th Interna-
tional, pages 1168–1179, 2012.
[25] C. Di Martino, , G. Goel, S. Sarkar, R. Ganesan, Z. Kalbarczyk, and
R. Iyer. Characterization of operational failures from a business data
processing saas platform.
the 36th International
Conference on Software Engineering, ICSE Companion 2014, pages 195–
204, New York, NY, USA, 2014. ACM.
In Proceedings of