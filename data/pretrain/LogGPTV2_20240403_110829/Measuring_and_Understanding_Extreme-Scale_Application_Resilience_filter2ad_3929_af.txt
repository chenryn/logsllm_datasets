### Application Resilience and Failure Analysis in Large-Scale Systems

#### Introduction
The application's resilience mechanisms are only partially effective when no recovery is initiated. As a result, large-scale applications (executing on more than 10,000 nodes) experience more pronounced failures due to interconnect-related issues.

#### Failure Probability and Application Scale
Figures 10(a) and 10(b) illustrate the probability of an application terminating due to system problems as a function of the application scale for XE and XK nodes, respectively. The data include applications with and without checkpoint/restart. The error bars represent the 95% confidence intervals. The larger error bars for large-scale applications are due to the limited number of applications that can effectively use full-scale executions (approximately 0.04% of XE applications and 0.01% of XK applications use more than 75% of the system).

As expected, we observe an increasing failure probability as XE and XK applications scale up. However, the magnitude of this increase differs between XE and XK nodes. For medium-scale XE applications (less than 10,000 nodes), the failure probability remains below 1%. When scaling further, the slope of the plot changes significantly. For applications running on more than 22,000 nodes, the failure probability is 0.162, compared to 0.008 for applications running on fewer than 10,000 nodes.

#### Interconnect-Related Issues
We analyzed the causes of application failures before and after the change in the slope (around 10,000 nodes). The percentage of applications failing due to interconnect-related issues increases similarly. For example, for large-scale runs (≥ 10,000 nodes), 66% of NAMD failures on XE nodes were due to Gemini- and LNet-related problems, compared to 23% for small-medium scale runs (< 10,000 nodes). This finding highlights that interconnect resiliency becomes a major concern when scaling up to more than 50% of the system size.

#### Node Hours and Resiliency
The number of processed node hours has a more pronounced effect on the resiliency of XK applications compared to XE applications. Figures 11(a) and 11(b) show the effect of node hours on the probability of application failures using log-log plots. We fit the data with a linear regression model in the log-log space. The probability of application failures can be described by a monomial relationship \( y = \alpha x^k \), which appears as a straight line in a log-log graph. The power and constant term correspond to the slope and intercept of the line, respectively.

For XK applications, the probability of failure follows a power-of-3 function of the node hours, while for XE nodes, it follows a power-of-2 function. This emphasizes the need for dedicated resiliency techniques, such as memory protection using the chipkill technique, and effective testing of extreme-scale hybrid applications to harness hybrid computing cores in future machines.

#### Related Work
Improved fault tolerance comes from detecting and auto-correcting a greater fraction of high-impact errors. Previous research has focused on analyzing error logs, online analysis of patterns preceding a failure, and evaluating the accuracy and efficacy of anomaly detection and proactive responses. Several studies have attempted to evaluate large-scale systems to describe basic error characteristics, model large-scale systems, and provide failure prediction and proactive resiliency mechanisms. However, most studies do not consider the impact of errors and failures, making reactive methods less effective.

In our work, we quantify the impact of errors on production workload and show that many applications can complete during system-wide outages thanks to the containment of system failures to specific file systems or cabinets.

#### Conclusions
This study reports on the resiliency of extreme-scale applications based on automatically collected system and application data logs. Key findings include:

1. **Application Susceptibility**: 1.53% of applications fail due to system problems, contributing to about 9% of production node hours.
2. **Resiliency Techniques**: Checkpoint/restart plays an essential role in improving application resiliency, with the mean number of failures before failure (MNBF) at least doubling for protected applications.
3. **Design Considerations**: Caution is needed when using a massive number of hybrid computing nodes. Dedicated resiliency techniques must be deployed to prevent errors from propagating to the application level.

#### Acknowledgements
This work is partially supported by the NSF CNS 10-18503 CISE, Air Force Research Lab FA8750-11-2-0084, and an IBM faculty award. We thank Celso Mendes, Gregory Bauer, Jeremy Enos, and Joshi Fullop for providing the raw data and insightful conversations. We also thank Fabio Baccanico for creating the first version of LogDiver and Domenico Cotroneo for comments on the manuscript.

#### References
[1] Cray XE6 Brochure: [Link](http://www.cray.com/Assets/PDF/products/xe/CrayXE6Brochure.pdf)
[2] Di Martino, C., et al. "Lessons learned from the analysis of system failures at petascale: The case of Blue Waters." In Proc. of 44th Annual IEEE/IFIP Int. Conf. on Dependable Systems and Networks (DSN), 2014.
[3] Moab HPC Suite: [Link](http://www.adaptivecomputing.com/products/hpc-products/moab-hpc-suite-enterprise-edition)
[4] Mendes, C.L., et al. "Deploying a large petascale system: The Blue Waters experience." Procedia Computer Science, 29(0):198–209, 2014.
[5] Johnsen, P., et al. "Petascale WRF simulation of Hurricane Sandy deployment of NCSA’s Cray XE6 Blue Waters." In Proceedings of SC '13, 2013.
[6] Goljadina, N., et al. "Analysis of Time Series Structure: SSA and related techniques."
[7] He, Y.H., et al. "Franklin job completion analysis." CUG 2010, Edinburgh, UK, 2010.
[8] Ezell, M. "Collecting application-level job completion statistics." CUG 2008, Helsinki, Finland, 2008.
[9] Cardo, N.P. "Detecting system problems with application exit codes."
[10] Edwards, R.G., and Joo, B. "The Chroma software system for lattice QCD."
[11] Zheng, G., et al. "FTC-Charm++: An in-memory checkpoint-based fault-tolerant runtime for Charm++ and MPI." In IEEE Cluster 2004, 2004.
[12] Sahoo, R.K., et al. "Failure data analysis of a large-scale heterogeneous server environment." In DSN '04, 2004.
[13] Liang, Y., et al. "Filtering failure logs for a BlueGene/L prototype." In DSN '05, 2005.
[14] Liang, Y., et al. "BlueGene/L failure analysis and prediction models." In DSN 2006, 2006.
[15] Schroeder, B., and Gibson, G.A. "A large-scale study of failures in high-performance computing systems." IEEE Transactions on Dependable and Secure Computing, 7(4):337–350, 2010.
[16] Oliner, A., and Stearley, J. "What supercomputers say: A study of five system logs." In DSN '07, 2007.
[17] Di Martino, C., et al. "Assessing time coalescence techniques for the analysis of supercomputer logs." In Proc. of 42nd Annual IEEE/IFIP Int. Conf. on DSN, 2012.
[18] Pecchia, A., et al. "Improving log-based field failure data analysis of multi-node computing systems." In DSN '11, 2011.
[19] Gainaru, A., et al. "Fault prediction under the microscope: A closer look into HPC systems." In SC '12, 2012.
[20] Gainaru, A., et al. "Event log mining tool for large-scale HPC systems." In Euro-Par '11, 2011.
[21] Heien, E., et al. "Modeling and tolerating heterogeneous failures in large parallel systems." In SC '11, 2011.
[22] Di Martino, C. "One size does not fit all: Clustering supercomputer failures using a multiple time window approach." In ISC '13, 2013.
[23] Chen, X., et al. "Predicting job completion times using system logs in supercomputing clusters." In DSN-W '13, 2013.
[24] Gainaru, A., et al. "Taming of the shrew: Modeling the normal and faulty behavior of large-scale HPC systems." In IPDPS '12, 2012.
[25] Di Martino, C., et al. "Characterization of operational failures from a business data processing SaaS platform." In ICSE Companion '14, 2014.