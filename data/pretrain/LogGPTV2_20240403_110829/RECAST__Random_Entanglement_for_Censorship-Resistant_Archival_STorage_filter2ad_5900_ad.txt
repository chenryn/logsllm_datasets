tle [31] (v0.12.13) and exposed through uWSGI [32] (v2.0.15).
Storage backends can be commercial storage providers such
as Google Drive, Dropbox, Microsoft OneDrive and Amazon
S3 or self-hosted key-value stores such as Redis [33] and
Minio [34]. For the sake of evaluation, we only deploy
storage nodes on premises. To coordinate the accesses to the
metadata from the proxy, the repair and replica management
processes, we use ZooKeeper [35]. Finally, all the components
are wrapped and deployed using Docker [36] (v17.05.0-ce).
VIII. EVALUATION
In this section, we evaluate RECAST’s performance and
resilience to data loss. First, we test our prototype’s perfor-
mance focusing on the raw throughput of the entanglement
process in isolation, the metadata storage overhead and its
resulting repair capabilities. Then, we experimentally test
the security properties of our architecture against active and
passive adversaries.
Micro-benchmark: encoding/decoding throughput. We
begin by evaluating RECAST’s throughput capabilities in terms
of raw encoding and decoding operations. These results are
obtained on a Intel Broadwell 64-cores machine with 128 GB
of RAM running Ubuntu 16.04 (kernel 4.4.0-101). Figure 9
presents results for 6 conﬁgurations of STEP having different
number of pointers and code rate. Please note that some
of these conﬁgurations, namely (5,∗, 2, 7), are too brittle to
2Note that the creation date can also be appended, however a partial order
can be inferred even without it [5].
Encoding
n-(1,20,2,3)
n-(5,1,2,7)
Decoding
n-(5,2,2,7)
n-(5,4,2,7)
690.65 MB/s
634 MB/s
n-(1,5,2,3)
n-(1,10,2,3)
 400
 350
 300
 250
 200
 150
 100
 50
 0
)
s
/
B
M
(
t
u
p
h
g
u
o
r
h
T
4 MB 16 MB 64 MB
4 MB 16 MB 64 MB
Fig. 9. Encoding and decoding throughput of the entanglement process in
isolation.
be used in practice and are only included here to show the
impact of STEP tuning on different metrics. We measure the
variations in throughput across these conﬁgurations with 3
different document sizes: 4, 16 and 64 MB. While the size
of the incoming documents directly affects the performance
of any conﬁguration, in particular when the codewords ﬁt in
low-level caches (40 for the L3-caches on our servers), the
most determinant factor is the storage overhead. Indeed, both
in encoding and decoding, results can be grouped according to
storage overhead (nu-(1,∗, 2, 3) and nu-(5,∗, 2, 7)). Second
to the storage overhead, the number of pointers also affects
the throughput with a predictable slowdown as the number of
pointers grows. Looking at the ﬁrst group of conﬁgurations,
nu-(1,∗, 2, 3), we observe that the throughput halves every
time the number of pointers doubles. Indeed, to encode a
16MB document, we go from 63 MB/s (t = 5), on to 36 MB/s
(t = 10) and end up at 19 MB/s (t = 20). The second group
of conﬁgurations, nu-(5,∗, 2, 7), also endures a predictable
slowdown but to a lesser extent (15-20%) every time we
double the number of pointers. To encode a 16MB document,
we drop from 297 MB/s (t = 1), on to 254 MB/s (t = 2) and
ﬁnally stop at 205 MB/s (t = 4). This difference in behavior
pairs with the rising storage overhead and the sensitivity of
the different conﬁgurations to these changes as we introduce
more pointers.
In conclusion, Figure 9 shows the trade-offs in choosing a
conﬁguration for STEP. It may offer good protection, e.g. nu-
(1, 20, 2, 3) at the expense of a low throughput (up to 18.84
MB/s when encoding) or a very fragile alternative, such as
nu-(5, 1, 2, 7), reaching up to 690 MB/s.
When integrated in the full system, we compare the per-
formance of our entangled archive against the standard Reed-
Solomon (RS) code provided by Intel ISA-L. By means of
the YCSB [37] framework, we run two workloads of 1000
operations: a read oriented one based on YCSB’s workloadc
and a pure insert one. Both workloads run 1000 operations,
with payloads ranging from 4 to 16MB. We deploy an instance
of RECAST that runs on a single machine (64 cores and 128GB
of RAM) hosting proxy, coder, metadata and 16 storage nodes.
We run this experiment with 8 concurrent threads from a
178
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:23:09 UTC from IEEE Xplore.  Restrictions apply. 
Write
RS(14,10)
Read
n-(1,10,2,3)
1 MB
4 MB 16 MB
1 MB
4 MB 16 MB
)
s
/
B
M
(
t
u
p
h
g
u
o
r
h
T
 8
 7
 6
 5
 4
 3
 2
 1
 0
l
)
e
a
c
s
g
o
l
(
d
e
r
r
e
f
s
n
a
r
t
a
t
a
D
GB
MB
kB
d
Decoding
u-(1,5,2,3)
nu-(1,5,2,3)
Re-encoding
n-(1,5,2,3)
0 d
0
0
0 d
0
0
1
0
0
0 d
0
0
5
0
9
9
0
0
0
d
0 d
0
0
0 d
0
0
1
0
0
0 d
0
0
5
9
9
0
0
0
0
Fig. 10. Writing and reading throughput of the entanglement process
compared to RS(14,10) in a full RECAST instance.
n-(1,10,2,3)
100 MB
n-(1,5,2,3)
n-(5,1,2,7)
n-(5,2,2,7)
Fig. 12. Bandwidth costs to delete a document in an archive with 106
documents. For each different STEP conﬁguration (different ﬁll pattern), we
delete d0 (the root of the history) and three other documents d100000 (old),
d500000 (med), d990000 (young).
l
)
e
a
c
s
g
o
l
(
e
z
S
i
10 MB
1 MB
100 kB
10 kB
1 kB
1
100
10
Number of documents
1000
10000
Fig. 11. Metadata storage overhead.
remote machine connected to RECAST host on 1GB switched
network. Considering the throughput measured with these
workloads (see Figure 10), we observe that the entanglement
slows down the operations by 10x. This can be explained by
the volume of pointers that ﬂows through the system when
reading or writing (10 times the size of the original data) in
addition to the necessary manipulations of the pointer blocks.
Micro-benchmark: storage overhead. Next, we look at the
storage overhead introduced by our prototype. While the STEP
conﬁguration, the number of documents and their average
size affect the storage requirements, the metadata growth (see
Figure 11) is more dependent on our implementation.
The visible differences across the STEP conﬁgurations are
due to the varying number of pointer blocks t: the greater
t grows, the greater the protection offered by the archive is
and as a consequence, the number of entanglement links to
maintain increases. The maintenance of those links may incur
a lower variation if the metadata database is better normalized.
Micro-benchmark: document removal. In Figure 12, we
study the bandwidth cost to actually "delete" a document from
the archive. In practice, the deletion of a target document is
a three phase process: ﬁnding a (possibly minimal) complete
set S of documents entangled with the target, decoding the
documents in S and re-encoding these documents with other
pointers.
The more accurate the ﬁrst step is, the less expensive the
actual deletion is. For this experiment, we compute the number
of documents in the set S from the results shown in Figure 6.
The cost of the last two steps depends on the RS code used
in the STEP-archive:
in Figure 12 we present results for
a (1, 5, 2, 3)-archive with different entanglement strategies.
The block size corresponds to the document size (1kB in
the plot) and we compute 3 parity blocks out of the source
and pointers using a RS(9, 6) code. Hence decoding and re-
encoding require fetching 6 and sending 3 blocks per docu-
ment respectively (the source is not stored and the pointers
to re-encode are chosen from the archive itself). The constant
bandwidth required for normal entanglement is determined by
the constant protection offered to documents. When using nu-
entanglement on an archive of 1kB documents, erasing d100000
and d500000 from the archive requires the transfer of 2.7 GB
and 0.55 GB respectively.
Macro-benchmark: metadata reconstruction and doc-
uments availability. Figure 13 presents the capacity of the
system to rebuild the metadata in case of loss of the metadata
server. It shows the amount of documents for which enough
metadata has been scraped to be served through the proxy.
This result shows how RECAST behaves in 2 different STEP
conﬁgurations, each with 2 different replication factors (none
or 3-replication). If all blocks are replicated, we can expect
to be able to serve all documents before reading from all the
storage nodes as depicted by the full-points lines. In contrast, if
none of the blocks are replicated, a large number of the storage
nodes needs to be crawled through until we gather enough
knowledge about the entire system. When running an instance
of RECAST conﬁgured with nu-entanglement and replication
management enabled,
the number of replicated blocks is
constant as the archive grows. Following this principle, the
number of nodes that need to be explored in order to rebuild
a functional archive increases. With respect to the history of
the archive, we can expect the metadata reconstruction process
to be fast at ﬁrst and grow slower over time depending on the
number of replicated blocks.
179
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:23:09 UTC from IEEE Xplore.  Restrictions apply. 
)
%
(
s
t
100
n
e
m
u
c
o
D
e
b
a
l
l
i
a
v
A
80
60
40
20
0
n-(1,10,2,3)
n-(5,2,2,7)
n-(1,10,2,3)-r3
n-(5,2,2,7)-r3
)
%
(
s
t
n
e
m
u
c
o
D
e
b
a
l
l
i
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
Nodes explored
a
v
A
 100
 99.95
 99.9
 99.85
 99.8
 99.75
n-(1,5,2,3)
n-(4,5,2,6)
16
12
)
B
G
(
e
c
a
p
s
+200%
+50%
baseline
L
Z