more comprehensive TCP diagnostic system that uses our tech-
niques and others as building blocks. We test our techniques on a
simple token bucket queue, but it will still work on other queuing
mechanisms such as RED as long as there is an increase in RTT
due to buffering. TCP variants such as BBR [39] that base their
congestion control on latency might also confound our technique.
BBR controls the amount of latency, and hence buffering that the
ﬂow induces. While testing our techniques across all TCP vari-
ants is beyond of the scope of our paper, we note that we have
tested it with buffer sizes of 1–5 times the BDP: as long as the
ﬂow induces some consistent measurable buffering in the path,
our technique will still identify the type of congestion accurately.
• Reliance on the slow start time period. We rely on TCP be-
havior during slow-start. The technique could therefore be con-
founded by a ﬂow that performs poorly during slow-start, but then
improves later on, and vice-versa. However, the classiﬁcation that
we obtain for the slow start period is still valid. If our model says
that a ﬂow was externally limited during slow start, but the overall
throughput was higher than what the ﬂow obtained during slow-
start, we cannot tell whether the later segments were limited as
well. The technique therefore gives us some understanding of the
path capacity; we could use our understanding of performance
during slow-start in order to extrapolate the expected behavior of
the ﬂow. We leave this for future work. However, in the reverse
case, if the model says the ﬂow was self-limited during slow-start,
but overall throughput is signiﬁcantly lower than what the ﬂow
obtained during slow-start, we can safely say that the throughput
was affected by other factors.
• The need for good training data for building a model. The
model fundamentally relies on a reliable corpus of training data
in order to build the training model. We used training data from a
diverse set of controlled experiments to build our model, and val-
idate it against a diverse set of real-world data. Additionally, we
also show that we get comparable results by building the model
using real-world data. However, we do not claim that our model
will work well in any setting. This problem requires a solid set
of ground-truth data in the form of TCP connections correctly la-
beled with the type of bottleneck they experienced.
• Reliance on coarse labeling for M-Lab data. Due to the lack
of ground truth regarding access link capacities, we label the M-
Lab data coarsely (§ 4.1). However, all ﬂows in January-February
need not have been externally limited, and all ﬂows in March-
April need not have been self-limited. Variability in access link
TCP Congestion Signatures
IMC ’17, Internet Measurement Conference, November 1–3, 2017, London, United Kingdom
capacities could result in low capacity links that self-induce con-
gestion even when there is external congestion. Home network
effects such as wireless and cross-trafﬁc interference might also
impede throughput, introducing noise into the labeling. However,
given the severity of the congestion experienced in that time
period as is evident from our analysis in § 4.1, published re-
ports [36], and the general adherence of U.S. ISPs to offered ser-
vice plans as evident from the FCC reports [5], we have reason-
able conﬁdence that our labeling is likely largely accurate.
• Use of packet captures for computing metrics. Our technique
computes the two RTT-based metrics by analyzing packet cap-
tures. Packet captures are storage and computationally expensive.
However, we note that the metrics are simple; indeed, Web100
makes current RTT values available light-weight manner. We
leave it to future work to study how we can sample RTT values
from Web100 to compute our metrics and how it compares to our
current technique that uses packet captures.
7 RELATED WORK
There have been several diagnosis techniques proposed for TCP. T-
RAT, proposed by Zhang et al. [52] estimates TCP parameters such
as maximum segment size, round-trip time, and loss to analyze
TCP performance and ﬂow behavior. Dapper [30] is a technique
for real-time diagnosis of TCP performance near the end-hosts to
determine whether a connection is limited by the sender, the net-
work, or the receiver. Pathdiag [37] uses TCP performance model-
ing to detect local host and network problems and estimate their
impact on application performance. However, these techniques do
not differentiate among types of congestion in the network. There
have been several proposals for locating bottleneck links. Multiple
techniques use packet inter-arrival times for localization: Katabi et
al. [34], to locate shared bottlenecks across ﬂows, Sundaresan et
al., to distinguish between a WAN bottleneck and a wireless bottle-
neck [48], and Biaz et al. [11] to understand loss behavior. A num-
ber of packet probe techniques in the literature use external probes
to identify buffering in paths [35] or to measure available bandwidth
or path capacity [23, 32, 33, 43, 51]. Sting [45] and Sprobe [44] are
tools to measure packet loss and available bandwidth, respectively,
using the TCP protocol. Antoniades et al. proposed abget [9], a
tool for measuring available bandwidth using the statistics of TCP
ﬂows. While external probing techniques can be useful in locating
the bottleneck link, such techniques are out-of-band and could be
confounded by load balancing or AQM, and in the best case can
only be indirectly used to deduce type of congestion (a congested
link between two transit ISPs likely causes external congestion for
all ﬂows that traverse the link). Network tomography has also been
proposed for localizing congestion [36, 41], or for discovering in-
ternal network characteristics such as latencies and loss rates using
end-to-end probes [16, 24, 25]. Such techniques, however, are typ-
ically coarse in nature, can be confounded by factors such as load-
balancing and multiple links comprising a single peering point, and
require a large corpus of end-to-end measurement data to apply the
tomography algorithm. Tomography cannot be applied on a single
ﬂow to infer the type of congestion that the ﬂow experienced or the
location of the bottleneck. Our goal in this work was to characterize
the nature of congestion experienced by a given TCP ﬂow based on
ﬂow statistics that are available at the server-side.
8 DISCUSSION/CONCLUSION
Till recently, last mile access links were most likely to be the bottle-
neck on an end-to-end path. The rise of high-bandwidth streaming
video combined with perpetually fractious relationships between
major players in the ecosystem has expanded the set of potential
throughput bottlenecks to include core peering interconnections.
Understanding whether TCP ﬂows are bottlenecked by congested
peering links or by access links is therefore of interest to all stake-
holders – users, service providers, and regulators. We took some
steps toward this goal by developing a technique to differentiate
TCP ﬂows that ﬁll an initially unconstrained path from ﬂows bottle-
necked by an already congested link.
The intuition behind our technique is that TCP behavior (partic-
ularly in terms of ﬂow RTTs during the slow-start phase) is qual-
itatively different when the ﬂow starts on an already congested
path as opposed to a path with sufﬁcient available capacity. These
path states correspond to peering-congested and access-link-limited
ﬂows, respectively. We show that the RTT variance metrics (both
the normalized difference between the maximum and minimum
RTTs, and the coefﬁcient of variation in the RTT samples) are
higher when a TCP ﬂow is limited by a non-congested link, and
therefore the TCP ﬂow itself drives queuing (and hence RTT) be-
havior. We use this intuition to build a simple decision tree clas-
siﬁer that can distinguish between the two scenarios, and test the
model both on data from the controlled experiments and real-world
TCP traces from M-Lab. We tested our model against data from our
controlled testbed as well as a labeled real-world dataset from M-
Lab and show that our technique distinguishes the two congestion
states accurately, and is robust to a variety of classiﬁer and network
settings.
We emphasize two strengths of our technique. First, it operates
on single ﬂows, and uses statistics of on-going TCP ﬂows rather
than requiring out-of-band probing. Second, it requires TCP con-
nection logs or packet captures at the server-side only and does not
require control or instrumentation of the client-side. This approach
differs from techniques for available bandwidth estimation or other
bottleneck detection tools that generally require out-of-band prob-
ing and/or control over both endpoints of the connection. Our work
also opens up avenues for future work, particularly in developing
more accurate TCP signatures that can further help us understand
network performance.
ACKNOWLEDGMENTS
We thank the anonymous reviewers, and our shepherd, Costin
Raiciu for their comments and feedback which improved the pa-
per. We also thank Measurement Lab for their timely help with the
NDT data. This work was supported by NSF awards CNS-1539902,
CNS-1535796, and CNS-1414177.
REFERENCES
[1] M-Lab Dataset. http://www .measurementlab .net/data.
[2] M-lab Mobiperf. https://www .measurementlab .net/tools/mobiperf/.
[3] M-Lab NDT Raw Data.
https://console .cloud .google .com/storage/browser/
m-lab/ndt.
IMC ’17, Internet Measurement Conference, November 1–3, 2017, London, United Kingdom
Sundaresan et al.
[4] M-Lab Network Diagnostic Test. http://www .measurementlab .net/tools/ndt.
[5] Measuring Fixed Broadband Report - 2016.
https://www .fcc .gov/general/
measuring-broadband-america.
[6] NDT Data Format. https://code .google .com/p/ndt/wiki/NDTDataFormat.
[7] Web10g. https://web10g .org.
[8] R. Andrews and S. Higginbotham. YouTube sucks on French ISP Free, and
French regulators want to know why. GigaOm, 2013.
[9] D. Antoniades, M. Athanatos, A. Papadogiannakis, E. P. Markatos, and C. Dovro-
lis. Available Bandwidth Measurement as Simple as Running Wget. In Proceed-
ings of the Passive and Active Measurment Conference (PAM), 2006.
[10] S. Bauer, D. Clark, and W. Lehr. Understanding Broadband Speed Measurements.
In 38th Research Conference on Communication, Information and Internet Pol-
icy, Arlington, VA, Oct. 2010.
[11] S. Biaz and N. H. Vaidya. Discriminating Congestion Losses from Wireless
Losses using Inter-Arrival Times at the Receiver.
In IEEE Symposium on Ap-
plication - Speciﬁc Systems and Software Engineering and Technology (ASSET),
1999.
[12] J. Brodkin. Netﬂix Performance on Verizon and Comcast has been Drop-
http://arstechnica .com/information-technology/2014/02/10/
ping for Months.
netﬂix-performance-on-verizon-and-comcast-has-been-dropping-for-months.
[13] J. Brodkin. Time Warner, Net Neutrality Foes Cry Foul Over Netﬂix Super HD
Demands, 2013.
[14] J. Brodkin. Why YouTube Buffers: The Secret Deals that Make-and-break Online
Video. Ars Technica, July 2013.
[15] S. Buckley. France Telecom and Google Entangled in Peering Fight. Fierce
Telecom, 2013.
[16] R. Caceres, N. G. Dufﬁeld, J. Horowitz, and D. F. Towsley. Multicast-based
Inference of Network-internal Loss Characteristics. IEEE Transactions on Infor-
mation Theory, 45(7), Nov. 1999.
[17] R. Carlson. Network Diagnostic Tool. http://e2epi .internet2 .edu/ndt/.
[18] Y. Chen, S. Jain, V. K. Adhikari, and Z.-L. Zhang. Characterizing roles of front-
end servers in end-to-end performance of dynamic content distribution. In Pro-
ceedings of the ACM SIGCOMM Internet Measurement Conference (IMC), Nov.
2011.
[19] k. claffy, Y. Hyun, K. Keys, M. Fomenkov, and D. Krioukov. Internet Mapping:
from Art to Science. In IEEE DHS Cybersecurity Applications and Technologies
Conference for Homeland Security (CATCH), pages 205–211, Watham, MA, Mar
2009.
[20] Cogent Now Admits They Slowed Down Netﬂix’s Trafﬁc, Creating A
http://blog .streamingmedia .com/2014/11/
Fast Lane & Slow Lane.
cogent-now-admits-slowed-netﬂixs-trafﬁc-creating-fast-lane-slow-lane .
html, Nov. 2014.
[21] Research Updates: Beginning to Observe Network Management Practices as
a Third Party. http://www .measurementlab .net/blog/research%5Fupdate1, Oct.
2014.
[22] Comcast and Netﬂix Reach Deal on Service. https://www .nytimes .com/2014/
02/24/business/media/comcast-and-netﬂix-reach-a-streaming-agreement . html,
Feb. 2014.
[23] C. Dovrolis, P. Ramanathan, and D. Moore. Packet-dispersion Techniques and
IEEE/ACM Transactions on Networking,
a Capacity-estimation Methodology.
12(6), Dec. 2004.
[24] N. Dufﬁeld. Simple Network Performance Tomography. In Proceedings of ACM
SIGCOMM Internet Measurement Conference (IMC), 2003.
[25] N. Dufﬁeld, F. L. Pesti, V. Paxson, and D. Towsley. Inferring Link Loss Using
Striped Unicast Probes. In Proceedings of IEEE Infocom, 2001.
[26] J. Engebretson. Level 3/Comcast Dispute Revives Eyeball vs. Content Debate,
Nov. 2010.
[27] J. Engebretson. Behind the Level 3-Comcast peering settlement, July 2013. http://
www .telecompetitor.com/behind-the-level-3-comcast-peering-settlement/.
[28] P. Faratin, D. Clark, S. Bauer, W. Lehr, P. Gilmore, and A. Berger. The growing
complexity of Internet interconnection. Communications and Strategies, (72):51–
71, 2008.
[29] Measuring
Broadband
America.
https://www .fcc .gov/general/
measuring-broadband-america.
[30] M. Ghasemi, T. Benson, and J. Rexford. Dapper: Data Plane Performance Di-
In Proceedings of the Symposium on SDN Research (SOSR),
agnosis of TCP.
2017.
[31] Glasnost: Bringing Transparency to the Internet.
http://broadband .mpi-sws .
mpg .de/transparency.
[32] N. Hu, L. E. Li, Z. M. Mao, P. Steenkiste, and J. Wang. Locating Internet Bot-
tlenecks: Algorithms, Measurements, and Implications. In Proceedings of ACM
SIGCOMM, 2004.
[33] M. Jain and C. Dovrolis.
End-to-end Available Bandwidth: Measurement
Methodology, Dynamics, and Relation with TCP Throughput. IEEE/ACM Trans-
actions on Networking, 11(4):537–549, Aug. 2003.
[34] D. Katabi and C. Blake.
Inferring Congestion Sharing and Path Characteris-
tics from Packet Interarrival Times. Technical Report MIT-LCS-TR-828, Mas-
sachusetts Institute of Technology, 2002.
[35] M. Luckie, A. Dhamdhere, D. Clark, B. Huffaker, and kc claffy. Challenges in
Inferring Internet Interdomain Congestion. In Proceedings of ACM SIGCOMM
Internet Measurement Conference (IMC), Nov. 2014.
[36] M-Lab Research Team. ISP Interconnection and its Impact on Consumer Internet
Performance - A Measurement Lab Consortium Technical Report. http://www .
measurementlab .net/publications.
[37] M. Mathis, J. Heffner, P. O’Neil, and P. Siemsen. Pathdiag: Automated TCP
Diagnosis. In Proceedings of the Passive and Active Network Measurement Con-
ference (PAM), Berlin, Heidelberg, 2008.
[38] Measurement Lab. http://measurementlab .net, Jan. 2009.
[39] Neal Cardwell
and Yuchung Cheng
Soheil Hassas Yeganeh
Based Congestion Control.
212428-bbr-congestion-based-congestion-control/fulltext.
and Van
and C. Stephen Gunn
and
BBR: Congestion-
https://cacm .acm .org/magazines/2017/2/
Jacobson.
[40] Ookla. How Does the Test Itself Work? https://support .speedtest .net/hc/en-us/
articles/203845400-How-does-the-test-itself-work-How-is-the-result-calculated- .
[41] V. N. Padmanabhan, L. Qiu, and H. J. Wang. Server-based Inference of Internet
Link Lossiness. In Proceedings of IEEE INFOCOM, 2003.
[42] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel,
M. Blondel, R. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Ma-
chine Learning in Python. Journal of Machine Learning Research, 12:2825–
2830, 2011.
[43] V. J. Ribeiro, R. H. Riedi, and R. G. Baraniuk. Locating Available Bandwidth
Bottlenecks. IEEE Internet Computing, 8(5):34–41, 2004.
[44] S. Saroiu, K. Gummadi, and S. Gribble. SProbe: Another Tool for Measuring
Bottleneck Bandwidth. In Work-in-Progress Report at the USENIX USITS, 2001.
[45] S. Savage. Sting: A TCP-based Network Measurement Tool. In Proceedings of
USENIX USITS, 1999.
[46] sklearn.tree.decisiontreeclassiﬁer.
http://scikit-learn .org/stable/modules/
generated/sklearn .tree .DecisionTreeClassiﬁer.html.
[47] S. Sundaresan.
congestion-exp.
Cross-trafﬁc Generator.
https://github .com/ssundaresan/
[48] S. Sundaresan, N. Feamster, and R. Teixeira. Home Network or Access Link?
Locating Last-mile Downstream Throughput Bottlenecks. In Proceedings of the
Passive and Active Measurement Conference (PAM), 2016.
[49] S. Sundaresan, D. Lee, F. Yun, X. Deng, and A. Dhamdhere. Challenges in Infer-
ring Internet Congestion using Throughput Measurements. In ACM SIGCOMM
Internet Measurement Conference (IMC), Nov. 2017.
[50] Verizon.
Unbalanced Peering, and the Real Story Behind the Ver-
http://publicpolicy .verizon .com/blog/entry/
izon/Cogent
unbalanced-peering-and-the-real-story-behind-the-verizon-cogent-dispute,
June 2013.
Dispute.
[51] H. Wang, K. S. Lee, E. Li, C. L. Lim, A. Tang, and H. Weatherspoon. Timing
is Everything: Accurate, Minimum Overhead, Available Bandwidth Estimation
in High-speed Wired Networks.
In Proceedings of ACM SIGCOMM Internet
Measurement Conference (IMC), 2014.
[52] Y. Zhang, L. Breslau, V. Paxson, and S. Shenker. On the Characteristics and
Origins of Internet Flow Rates. In Proc. ACM SIGCOMM, Pittsburgh, PA, Aug.
2002.