title:zkCNN: Zero Knowledge Proofs for Convolutional Neural Network Predictions
and Accuracy
author:Tianyi Liu and
Xiang Xie and
Yupeng Zhang
zkCNN: Zero Knowledge Proofs for Convolutional Neural
Network Predictions and Accuracy
Tianyi Liu
Texas A&M University and Shanghai
Key Laboratory of Privacy-Preserving
Computation
PI:EMAIL
Xiang Xie
Shanghai Key Laboratory of
Privacy-Preserving Computation
PI:EMAIL
Yupeng Zhang
Texas A&M University
PI:EMAIL
ABSTRACT
Deep learning techniques with neural networks are developing
prominently in recent years and have been deployed in numerous
applications. Despite their great success, in many scenarios it is
important for the users to validate that the inferences are truly
computed by legitimate neural networks with high accuracy, which
is referred to as the integrity of machine learning predictions. To
address this issue, in this paper, we propose zkCNN, a zero knowl-
edge proof scheme for convolutional neural networks (CNN). The
scheme allows the owner of the CNN model to prove to others
that the prediction of a data sample is indeed calculated by the
model, without leaking any information about the model itself. Our
scheme can also be generalized to prove the accuracy of a secret
CNN model on a public dataset.
Underlying zkCNN is a new sumcheck protocol for proving fast
Fourier transforms and convolutions with a linear prover time,
which is even faster than computing the result asymptotically. We
also introduce several improvements and generalizations on the
interactive proofs for CNN predictions, including verifying the
convolutional layer, the activation function of ReLU and the max
pooling. Our scheme is highly efficient in practice. It can support
the large CNN of VGG16 with 15 million parameters and 16 layers.
It only takes 88.3 seconds to generate the proof, which is 1264×
faster than existing schemes. The proof size is 341 kilobytes, and
the verifier time is only 59.3 milliseconds. Our scheme can further
scale to prove the accuracy of the same CNN on 20 images.
CCS CONCEPTS
• Security and privacy → Cryptography.
KEYWORDS
Zero knowledge proofs; Machine learning; Convolutional Neural
Networks
ACM Reference Format:
Tianyi Liu, Xiang Xie, and Yupeng Zhang. 2021. zkCNN: Zero Knowledge
Proofs for Convolutional Neural Network Predictions and Accuracy. In
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea
© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-8454-4/21/11...$15.00
https://doi.org/10.1145/3460120.3485379
Proceedings of the 2021 ACM SIGSAC Conference on Computer and Commu-
nications Security (CCS ’21), November 15–19, 2021, Virtual Event, Republic of
Korea. ACM, New York, NY, USA, 18 pages. https://doi.org/10.1145/3460120.
3485379
1 INTRODUCTION
Deep learning with neural networks has seen a great success in
many machine learning tasks in recent years, being deployed in
various applications and products in practice. In deep learning,
convolutional neural networks (CNN) are particularly useful in the
domain of computer vision for image classifications and recognition.
Despite their excellent performance, there are many security issues
when applying CNN models. In many scenarios, we need to guar-
antee that the results are indeed predictions of a particular CNN
model, which is referred to as the integrity of machine learning. A
naïve solution to address the integrity issue is to publish the CNN
models. However, as the models are usually trained on valuable
and sensitive datasets and are important intellectual properties of
the owners, it is often impossible in practice to share the models.
In this paper, we propose zero knowledge CNN predictions and
accuracy schemes to address the issues above. The cryptographic
primitive of zero knowledge proof (ZKP) allows a prover to con-
vince a verifier that a computation on the prover’s secret input is
correctly calculated through a short proof. Zero knowledge proofs
guarantee that if the prover sends a wrong result of the computa-
tion, it can only pass the verification with a negligible probability,
which is the soundness property. At the same time, the proof leaks
no information about the prover’s secret input, which is the zero
knowledge property. Prover’s secret input is usually referred to as
the witness or auxiliary input. In the scenario of zero knowledge
CNN, the witness is the parameters of the CNN model. The owner
of the secret model can prove to the users that the predictions are
correctly computed using her CNN model, while preserving the
privacy of the model.
Applications of zero knowledge machine learning. With the
strong guarantees on the privacy and integrity, zero knowledge
machine learning has the potential to enable many new applications
in practice. First, machine-learning-as-a-service (MLaaS) such as
Amazon Forecast and Amazon Fraud Detector [1] offers cloud-based
platforms for predictive data analytics through machine learning
as a paid service. However, the clients do not have access to the
machine learning models, which are intellectual properties of the
companies, and have to trust that the predictions are computed
as promised. Using zero knowledge machine learning, the service
provider can prove that the models are of high quality and accuracy,
and the predictions are indeed computed by the same models. It
Session 11B: Zero Knowledge II CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2968provides the integrity for MLaaS, while preserving the privacy of
the models. Second, reproducibility is a challenging problem in
machine learning. Some machine learning models or algorithms are
claimed to achieve high accuracy, yet it is challenging to validate
these claims in many cases. Zero knowledge machine learning offers
a partial solution to this issue. By attaching a ZKP for the claimed
accuracy, it at least shows that there exists such a machine learning
model known by the owner, due to the knowledge soundness of
the ZKP. Moreover, in some ZKP schemes, including our schemes
proposed in this paper, the time to verify the result is much faster
than computing it. This property further reduces the burden of the
verifier to validate the accuracy of the machine learning models.
In a different setting, zero knowledge machine learning can also
support predictions of public models on private datasets. As pointed
out by [46], in this setting, zero knowledge machine learning can be
applied to test the accuracy of a model on private benchmarks. The
data owner can evaluate the model on her sensitive data to help
the trainer improve the quality of the model, while preserving the
privacy of the data. Zero knowledge machine learning can also be
used to prove that there are adversary examples for a public model
where two inputs are “close” to each other but are classified into
two different categories. Our scheme can be modified to support
this setting with minimal changes.
Our Contributions. While there are general-purpose zero knowl-
edge proof schemes that work for any computations modeled as
arithmetic circuits, they introduce a high overhead on the prover
and existing schemes do not scale to the size of CNN models and
predictions. In this paper, we propose efficient zero knowledge
proof schemes for CNN predictions and accuracy that scale to large
CNN models in practice. In particular, our contributions include:
• Sumcheck protocol for FFT and convolutions. First, we pro-
pose a new sumcheck protocol for two-dimensional (2-D) con-
volutions. The key ingredient of the protocol is an efficient sum-
check for the fast Fourier transform (FFT)1 where the additional
time to generate the proof is 𝑂(𝑁) for a vector of size 𝑁 . This is
indeed asymptotically better than the complexity to compute the
FFT in 𝑂(𝑁 log 𝑁). This is another example of common computa-
tions in the literature besides the matrix multiplication where the
prover time of the sumcheck is sublinear in the time to compute
the result [41]. Using the protocol as a building block, we propose
the sumcheck protocol for 2-D convolutions and the prover time
is 𝑂(𝑛2) for two inputs of 𝑛 × 𝑛 and 𝑤 × 𝑤. The prover time is
equal to the size of the input and the output asymptotically and
thus is optimal. The proof size is 𝑂(log 𝑛). We further propose a
protocol to achieve a sublinear verifier time of 𝑂(log2 𝑛) with a
proof size of 𝑂(log2 𝑛).
Our new sumcheck protocol for FFT may be of independent inter-
est as a stand-alone primitive. For example, in [52], the authors
applied the interactive proofs to delegate the computation of poly-
nomial evaluations from the verifier to the prover. The scheme
uses an FFT circuit of size 𝑂(𝑁 log 𝑁) and depth 𝑂(log 𝑁). With
our new sumcheck protocol, we are able to reduce the prover
time of this delegation from 𝑂(𝑁 log 𝑁) to 𝑂(𝑁). In [42], the
1Sometimes discrete Fourier transform (DFT) or number theoretic transform (NTT)
are used for the transform on finite fields. In this paper, we use the general name of
FFT and our protocols work on a finite field.
authors proposed to verify the computation of a high-performing
but untrusted ASIC (application specific integrated circuit) by
a relatively slower but trusted ASIC through interactive proofs,
and the FFT circuit is an important application used in the bench-
mark. With our new sumcheck protocol, the area and energy
of the untrusted ASIC devoted to generating the proof can be
even smaller than those for computing FFT, and the cost of the
verifier ASIC is also reduced by a logarithmic factor, making the
verifiable ASICs faster than the baseline of computing FFT using
the slow but trused ASIC [42, Figure 11].
• Efficient interactive proofs for CNN predictions. Second,
we propose several improvements and generalizations of the
sumcheck protocol and the doubly efficient interactive proofs
(usually referred as the GKR protocol) for CNN predictions. (1)
We introduce generalized addition gates and multiplications gates
so that additions with fan-in larger than 2 and inner products can
be implemented with a single sumcheck. (2) With the techniques
in [51], we extend the generalized gates to take inputs from any
layers without any overhead on the prover time. (3) We further
improve the sumcheck protocol for the convolutional layer in
CNN to reduce the prover time of IFFT by a factor of ch𝑖𝑛 for
convolutions on inputs with ch𝑖𝑛 channels. (4) We design an effi-
cient gadget of circuit to compute the ReLU activation function
and the max pooling together with a single bit-decomposition
per number.
• Implementation and evaluations. We fully implement our
zero knowledge convolutional neural network system, named
zkCNN. We test it on large CNNs such as VGG16 with 15 mil-
lion parameters on the CIFAR-10 dataset [31]. It takes 88.3s to
generate a proof of prediction, and 59.3ms to verify the proof.
The proof size is 341KB. The prover time is 1264× faster than the
existing scheme in [34]. Our system can also scale to prove the
accuracy of the same model on 20 images.
1.1 Related Work
Zero knowledge machine learning. The most relevant work to
our paper is vCNN [34] and ZEN [23] on zero knowledge neural net-
work predictions. vCNN supports convolutional neural networks
and verifies the convolutions through polynomial multiplications.
It combines the regular quadratic arithmetic program (QAP) and
the polynomial QAP in pairing-based zero knowledge proofs using
commit-and-prove. We verify the convolutions directly using the
sumcheck protocol and our approach is 34× faster than vCNN (see
Section 5.1). In [23], Feng et al. proposed quantization techniques
that are friendly to zero knowledge proofs to support real numbers.
We use the quantization approach proposed in [28] and we believe