power could be used to extend the range of attack). We also tested
a 3 W and 2 W generic small form factor non-tweeter speaker sold
for less than $5. We were able to successfully perform command
injection at a distance of up to 45 cm and 40 cm and jam at up to
30 cm and 20 cm, respectively. This shows that no special hardware
is needed to accomplish this attack and even cheap off the shelf IoT
devices can be used.
Cheap IP cameras, including devices from Apexis, are reported
to be vulnerable to many attacks. This (generic) IP camera family is
branded by bigger companies like Logilink, which sells the model
WC0030A23. Remote code execution vulnerabilities for 1250 dif-
ferent camera models, including brandings of this generic camera
model (Apexis, Logilink, etc.) were published24. In addition to that
our model automatically gets a public hostname on power on which
also suffers from an enumeration vulnerability.
All this combined could enable an attacker to enumerate devices,
access them remotely and upload a malicious firmware or ultrasonic
modulated audio files to the devices to attack Alexa devices in vicin-
ity. This shows that many devices, even without special hardware
like a tweeter speaker may be used remotely by an attacker to emit
ultrasonic frequencies and attack other devices.
6 RELATED WORK
To the best of our knowledge there are no published man-in-the-
middle attacks against Virtual Personal Assistants like Amazon
Alexa, especially attacks utilizing service plug-ins like Skills. We
are also not aware of approaches combining both jamming and in-
jecting of inaudible commands to implement more complex attacks.
21https://www.amazon.com/Stock-Prices-by-Opening-Bell/dp/B01E9Z3UVC
22https://www.cirrus.com/products/wm8731/
23http://www.logilink.com/WLAN_Indoor_Pan-Tilt_IP_Kamera_mit_Nachtsicht-
Bewegungsmelder_2-Way_Audio.htm
24https://seclists.org/fulldisclosure/2017/Mar/23
Session 6A: IoT SecurityAsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand473We focus therefore on reviewing existing approaches that aim at is-
suing commands to Virtual Private Assistants without victim users
noticing.
Audible Synthesized Voice Commands. Diao et al. [6] discuss at-
tacks against the Google Voice Search (GVS) app on Android smart
phones. To carry out these attacks, a malicious app must be installed
on the device. The malicious app activates GVS and simultaneously
plays a recorded or synthesized command over the built-in speakers
which is then picked up by the microphone, on which GVS is lis-
tening. This way the malicious app can access restricted resources
by using GVS without asking the user for permission. The app
can also call the attacker through GVS, enabling the attacker to
issue commands via the call. This attack, however, does not work
anymore on Android 7, as it uses now voice cancellation techniques
to improve voice calls by reducing recorded noise. Alepis et al. [1]
therefore extend the work of Diao et al. [6] by proposing to use
multiple devices to perform the attack and to use Firebase to return
transcribed audio replies.
These attacks only work on Android smart phones and are lim-
ited to injecting commands when the victim user is not listening.
It is therefore not applicable in our scenario. Nevertheless we uti-
lize the idea of using synthesized audio to control Virtual Private
Assistants and that the (synthesized speech) answer can be reli-
ably transcribed back into text, essentially allowing us to flexibly
interact with a voice-controlled assistant via text.
Audible Mangled Voice Commands. Vaidya et al. [21] propose
a method to mangle human voice so that it is no longer human
comprehensible but still recognizable by the Voice Recognition
Systems. The audio mangler takes raw voice and MFCC parameters
as input and returns mangled voice which is hard for humans to
understand but Voice Recognition Systems detect the same text in
this mangled audio as in the raw audio.
Carlini et al. [3] extended the work of Vaidya et al. [21]. In ad-
dition to the black box approach Carlini et al. evaluate a white
box approach against the open-source CMU Sphinx where the un-
derlying mechanics are known to the attacker. Because the inner
workings of CMU Sphinx are known, a more precise attack is pos-
sible. Our attack does not use command mangling for injecting or
jamming because if the attack is repeated several times this would
raise the suspicions of the victim as the mangled sounds would
always need to occur at the same time he interacts with Alexa.
Inaudible Voice Commands. Song et al. [19] describe an attack
using the non-linearity of microphones to induce low frequency
sounds with the use of ultrasonic frequencies in microphones
of voice-controlled virtual assistants (cf. Sect 2.2). Independently
Zhang et al. [25] evaluated the demodulation properties of MEMS
and ECM microphones for a single modulated frequency and voice,
which consists of multiple frequencies. We adopted techniques in
these papers for realizing command injection performed by mali-
cious device D and combining it with inaudible command jamming
to realize a complete attack framework against the Alexa virtual as-
sistant. In contrast to their attack that is limited to simple command
injection, our attack framework allows to hijack the entire inter-
action between user and Alexa or Skills allowing for much more
powerful attack scenarios than mere issuing of single commands.
Independently to our work Roy et al. [14] have developed an
approach for realizing jamming of spy microphones using ultra-
sound signals while not interfering with human conversations. Our
method builds on similar findings than what also they report.
Voice Commands over the headset jack. Kasmi et al. [10] were
able to induce voice into a headset connected to a smart phone
using intentional electromagnetic interference (IEMI) and control
the listening Virtual Personal Voice Assistant this way. Another
attack requiring physical access to devices is described by Young
et al. [23]. The attack uses a RaspberryPi 2 with an audio board,
which is connected to the victim’s device over the headphone jack
to activate various functions on the smartphone. However, since the
typical virtual assistants targeted by our attack are not equipped
with headphone jacks, we did not consider these approaches in
constructing our attack framework.
Skill Squatting. Kumar et al. [12] identified utterances that the
Alexa STT engine misinterprets systematically and created an at-
tack, dubbed Skill Squatting utilizing these misinterpretations to
trick the user to open a malicious Skill. Zhang et al. [26] attack uses
Skills with a similar pronounced or paraphrased invocation-name
to hijack the command meant for that Skill. They do however not
elaborate on how their attack would work in practice, as for it to
be effective, e.g., for Skills requiring pairing to a device, both the
benign and the malicious Skills would need to be activated on the
victim’s Alexa account (the malicious after the benign Skill was
paired). Note that these kinds of attacks can only be used to redirect
the command flow to a different Skill, but not a full man-in-the-
middle attack, where communications between users and benign
Skills are modified on-the-fly, like Lyexa does.
Hide commands in other audio. Schönherr et al. [16] and Yuan
et al. [24] describe a method for hiding commands recognizable by
deep neural network-based voice recognition algorithms but not
by humans inside other audio files. Carlini et al. [4] showed how to
create an audio file with a similar waveform which transcribes into a
totally different sentence when processed by Mozilla’s DeepSpeech.
IoT attacks. Fernandes et al. [8] did a security analysis of the
smart home framework for Samsung’s SmartThings which allows
third parties to write apps for this service. Ho et al. [9] reviewed the
security of smart home locks against physically-present attackers
and relay attacks. Our attack differs from these works in that it does
not attack IoT devices directly, but achieves malicious functionality
through weaknesses in the AVS Skill ecosystem.
Other audio based attacks. Numerous works have used audio-
based components as parts of security attacks similar to our ap-
proach. Son et al. [18] used sound played at the resonance fre-
quency of MEMS gyroscopes to crash drones. Trippel et al. [20]
used an acoustic injection attack to control the output of a MEMS
gyroscope which made them able to inject fake steps into a Fitbit.
Cisse et al. [5] published an approach on how to create malicious
(perturbed) inputs for image classification and speech recognition.
Finally, Schlegel et al. [15] created a low-permission Android app
that can extract PINs and passwords from phone calls.
Session 6A: IoT SecurityAsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand4747 COUNTERMEASURES
Our attack is enabled by the non-linearity of microphones used in
Amazon Echo and other Voice Assistant devices and loopholes in
the Skill model. In this section we will discuss countermeasures
against both vulnerabilities.
Non-linearity. An effective countermeasure to defend against
Lyexa would be to change the hardware of microphones to not
pick up ultrasonic frequencies or to implement a hardware module
between amplifier and LPF to detect AM signals and silence the
baseband of the demodulated signal, to suppress the demodulated
voice commands. Furthermore, using machine-learning classifiers
can be used to distinguish between demodulated and normally
recorded audio. However, realizing such defenses in already de-
ployed devices is not feasible. For new devices, this would also
imply significant changes in device and in particular microphone
designs, making it unlikely to be adopted in the near future. A fea-
sible option could therefore be to resort to a dedicated device that
would monitor the audio environment and alarm the user or AVS
or even jam the injection, if it detects an abnormally high sound
pressure in the ultrasonic sound spectrum. After-market solutions
like this are not uncommon as similar products for securing the
home network25, 26 are emerging.
Skill model. Our attack works because Alexa continues to listen
even when it hears only noise. Alexa’s logic could therefore be
changed to ignore commands that are issued after extremely loud
noise in order to prevent jamming of commands. In addition, Alexa
could audibly announce which Skill is activated. This would allow
users to notice if an unintended Skill is invoked.
The recommended way of running a Skills backend is to use
the Amazon Web Services (AWS) Lambda service which is called
directly by AVS when a user invokes the Skill. Amazon could force
Skill developers to use Lambda and certify also the Skills’ back-
end source code to make sure the Skill backend doesn’t contain
malicious functionality. As Lambda is under control of Amazon,
it could require re-certification when the source code on Lambda
is changed, essentially preventing post-certification code changes.
While this countermeasure may not be feasible (e.g. increased cost
of certification, developers are limited in possibilities) it could be
used to prevent backdoors in Skills.
8 DISCUSSION
Note that the attacks described above are not achievable by previous
attack approaches that only inject commands (e.g. Dolphin [25],
which records voice and replays it inaudibly) as the invoked Skill
will return an audio reply which the user will be able to hear and
which will make him suspicious. Without the use of a malicious
Skill it is not possible to return modified or false information to the
user. Skill Squatting attacks [12, 26] could in principle be used to
realize our Skill redirection attack, but have a significant practical
obstacle. To hijack an interaction with a specific Skill the adversary
would need to bring the user to activate the malicious Skill with an
invocation name that is similar to the benign one in addition to the
already used Skill, which is unlikely.
25https://www.bitdefender.com/box/
26https://www.f-secure.com/en_US/web/home_us/sense
Using our man-in-the-middle attack it is also possible to realize
attacks requiring explicit entry of passwords or PIN codes, as the
user can be fooled to reveal these during the interaction with the
attacked virtual assistant. Previous approaches utilizing only clan-
destine command injection will not be able to do this, as these are
not known to the adversary.
In the Alexa companion app there are two lists where a user
can see the history of commands issued to Alexa, possily revealing
the attack to the user. The first is the main window, where all
previous ’home cards’ issued by Skills are listed. Issuing home
cards is, however not mandatory for Skills, so that the malicious
Skill can simply omit issuing home cards for malicious functionality.
The second is the “true” history of commands, which is located
on a third submenu level among all other companion app settings,
making it unlikely that regular users would actively monitor it.
Loud background noise or drastic changes in background noise
intensity can lead to unsuccessful command injections. However,
this does not impact the effectiveness of the attack in the most
common deployment environments of virtual assistants, which is
at home. There the environment is mostly silent, making the setting
favorable for our attack.
There may finally be commands we cannot successfully hijack
without the user noticing it. An example could be an interaction like
“Alexa, close my last used lock!” where Alexa would respond with
“Okay, your lock front door is now locked.”. Without the information
which lock was locked last, the adversary cannot reliably return
a satisfying response to this command. Getting this information
from Alexa is also not possible, as using the command would have
the side effect of closing the lock.
9 FUTURE WORK
To be able to attack a victim’s device reliably without a manual
setup, our malicious device has to be able to set up and calibrate
itself automatically, according to the physical characteristics of the
targeted victim device. There are different Amazon Echo variants
(Dot, Echo, Echo Plus) and two different generations of these devices.
Even two identically looking devices of the same generation and
version could use different microphone hardware. In addition to
that, the distance between the two devices requires a signal of
specific volume level to be used. If it is too low, no demodulation
of the signal is possible. If it is too high, we jam the microphone
by suppressing (quieter) voice by the automatic gain control [14],
instead of injecting voice. To adjust the correct volume level it is
possible to play, e.g., the Alexa wake-word at different volumes and
modulated with different carrier frequencies until the recognition
audio cue is played by the Echo device, indicating that the injection
is working.
Even though we eliminated the crackling sound when an ultra-
sonic audio file starts playing or ends, we still get a slightly audible
crack if we stop playing it halfway through. This happens when the
jamming audio stops abruptly because the injection audio has to be
played. We could use a pure tweeter speaker instead of a wide-range
one, to get rid of the crackling sound. A software approach (a one
second fade out) could be too long and Alexa might stop listening.
We could use also modify the attack to use Alexa itself as the STT
service. If we have an utterance consisting solely of slots (which
Session 6A: IoT SecurityAsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand475are able to catch everything) and a user says e.g. “Alexa, ask Nuki
to lock garden door” we could jam only “ask Nuki” and append “,
using Evil”. “to lock garden door” will then be transcribed by Alexa
and passed to our Skill’s backend as the Slot value. Drawbacks are
that we don’t know exactly which Skill the user wanted to invoke
(we could guess it from the rest of the command) and would need
a very precise way to jam only the correct part of the command.
In this case we wouldn’t need to record the user’s command and
transcribe it using a third party service.
Currently we are looking into Google Assistant and its Actions.
Samsung’s Bixby and Apple’s Siri have at the time of writing no
comparable add-on functionality like Alexa.
10 SUMMARY
We presented and evaluated the Lyin’ Alexa attack. We showed that
with this attack it is possible to completely hijack a full conversation
between Alexa and the user while simultaneously maintaining the
interaction semantics from the point of view of the user. We were
able to deny access to Skills or built-in functionality of Alexa and
return false but plausible looking data to the user in the name of
Alexa. Furthermore we suggest countermeasures and limitations of
this attack as well as possible modifications as future work.
Acknowledgements. This work was founded in part by the Ger-
man Research Foundation (DFG) within CRC 1119 CROSSING (S2
and P3) and the Intel Collaborative Institute for Collaborative Au-
tonomous and Resilient Systems (ICRICARS). We would also like
to thank Cisco Systems, Inc. for their support of this work.
11 RESPONSIBLE DISCLOSURE
We have contacted Amazon and notified them about our findings.
We are currently engaged in discussions with their security team.
REFERENCES
[1] E. Alepis and C. Patsakis. 2017. Monkey Says, Monkey Does: Security and Privacy
on Voice Assistants. IEEE Access 5 (2017), 17841–17851. https://doi.org/10.1109/
ACCESS.2017.2747626
[2] Manos Antonakakis, Tim April, Michael Bailey, Matt Bernhard, Elie Bursztein,
Jaime Cochran, Zakir Durumeric, J. Alex Halderman, Luca Invernizzi, Michalis
Kallitsis, Deepak Kumar, Chaz Lever, Zane Ma, Joshua Mason, Damian Menscher,
Chad Seaman, Nick Sullivan, Kurt Thomas, and Yi Zhou. 2017. Understanding the
Mirai Botnet. In 26th USENIX Security Symposium (USENIX Security 17). USENIX
Association, Vancouver, BC, 1093–1110.
[3] Nicholas Carlini, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah Sherr,
Clay Shields, David Wagner, and Wenchao Zhou. 2016. Hidden Voice Commands.
In 25th USENIX Security Symposium (USENIX Security 16). USENIX Association,
Austin, TX, 513–530. https://www.usenix.org/conference/usenixsecurity16/
technical-sessions/presentation/carlini