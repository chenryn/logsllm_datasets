while holding the variable’s lock, and using information (the
counters) that is either private to the thread or protected by
the lock itself.
Shared pseudorandom number generators are treated in
the same way as shared variables, since their behavior is
deterministic based on the function’s seed (which is initial-
ized in the same way during a replay) and the access order
recorded in the PALs.
Clocks and other hardware. Special treatment is needed
for hardware resources whose return values cannot be pre-
dicted, such as gettimeofday() and /dev/random.
For these, we use the same PAL approach, but replacing the
variable name and access order with the hardware accessed
and the value returned. Producing these PALs does not re-
quire any additional locking because they only use informa-
tion local to the thread. Upon replay, the PALs allow us to
return the exact value as during the original access.
4.2 How to Log Determinants
The key requirement for logging is that PALs need to be
on stable storage (on the Output Logger) before we release
the packets that depend on them. While there are many op-
tions for doing so, we pursue a design that allows for ﬁne-
grained and correct handling of dependencies.
8Recent research [22, 25] has explored ways to reduce the performance
impact of enforcing deterministic execution but their overheads remain im-
practically high for applications with frequent nondeterminism.
in
A
B
C
D
time
(A,1,X,1)
T
out
(A,2,T,6)
X
(B,1,X,2)
(B,2,Y,2)
Y
(C,1,Z,2)
Z
(C,2,Y,1)
(D,1,Z,1)
Figure 3: Four threads (black lines) process packets A,
B, C, D. As time goes (left to right), they access (cir-
cles) shared variables X, Y, Z, T generating the PALs in
parentheses. The red tree indicates the dependencies for
packet B.
We make two important design decisions for how logging
is implemented. The ﬁrst is that PALs are decoupled from
their associated data packet and communicated separately
to the output logger. This is essential to avoid introduc-
ing unnecessary dependencies between packets. As an ex-
ample, packet B in the ﬁgure depends on PAL (A, 1, X, 1),
but it need not be delayed until the completion of packet A,
(which occurs much later) – it should only be delayed until
(A, 1, X, 1) has been logged.
The second decision has to do with when PALs are placed
in their outgoing PAL queue. We require that PALs be
placed in the output queue before releasing the lock asso-
ciated to the shared variable they refer to. This gives two
guarantees: i) when pi is queued, all of its PALs are already
queued; and ii) when a PAL for vj is queued, all previous
PALs for the same variable are already in the output queues
for this or other threads. We explain the signiﬁcance of these
properties when we present the output commit algorithm in
§4.4.
4.3 Deﬁning a Packet’s Dependencies
During the replay, the replica must evolve in the same
way as the master. For a shared variable vj accessed while
processing pi, this can happen only if i) the variable has
gone through the same sequence of accesses, and ii) the
thread has the same internal state. These conditions can
be expressed recursively in terms of the PALs: each PAL
(pi, n, vj, m) in turn has up to two dependencies: one per-
packet (pi, n − 1, vk, sik), i.e., on its predecessor PAL for
pi, and one per-variable (pi(cid:48) , n(cid:48), vj, m − 1), i.e., on its pre-
decessor PAL for vj, generated by packet pi(cid:48). A packet de-
pends on its last PAL, and from that we can generate the tree
of dependencies; as an example, the red path in the ﬁgure
represents the dependencies for packet B.
We should note that the recursive dependency is essen-
tial for correctness. If, for instance, packet B in the ﬁgure
were released without waiting for the PAL (D, 1, Z, 1), and
the thread generating that PAL crashed, during the replay
we could not adequately reconstruct the state of the shared
variables used while processing packet B.
232rules chosen to queue PALs and packets guarantee that both
the per-packet and per-variable dependencies for a given
packet are already queued for release on some thread before
the packet arrives at the output queue on its own thread. This
follows from the fact that the PAL for a given lock access is
always queued before the lock is released. Hence, we only
need to transfer PALs and packets to the output logger in
a way that preserves the ordering between PALs and data
packets.
This is achieved with a simple algorithm run between the
Master and the Output Logger, illustrated in Fig. 4. Each
thread on the Master maps ‘one to one’ to an ingress queue
on the Output Logger. PALs in each queue are transferred
as a sequential stream (similar to TCP), with each PAL asso-
ciated to an per-queue sequence number. This replaces the
second entry in the PAL, which then does not need to be
stored. Each thread at the Master keeps track of MAX, the
maximum sequence number that has been assigned to any
PAL it has generated.
On the Master: Before sending a data packet from its queue
to the output logger, each thread on the master reads the
current MAX value at all other threads and creates a vec-
tor clock VOR which is associated with the packet. It then
reliably transfers the pending PALs in its queue, followed by
the data packets and associated vector clocks.
On the Output Logger: Each thread continuously receives
PALs and data packets, requesting retransmissions in the
case of dropped PALs. When it receives a PAL, a thread
updates the value MAX representing the highest sequence
number such that it has received all PALs prior to MAX. On
receiving a data packet, each thread reads the value MAX
over all other threads, comparing each with the vector clock
VOR. Once all values MAXi ≥ VORi, the packet can be
released.
Performance
Our parallel release algorithm is efﬁcient because i) threads
on the master and the output logger can run in parallel;
ii) there are no write-write conﬂicts on the access to other
queues, so memory performance does not suffer much; iii)
the check to release a packet requires a very small constant
time operation; iv) when batching is enabled, all packets re-
leased by the master in the same batch can use the same
vector clock, resulting in very small overhead on the link
between the master and the output logger and amortizing the
cost of the ‘check’ operation.
5. SYSTEM IMPLEMENTATION
We present key aspects of our implementation of FTMB.
For each, we highlight the performance implications of
adding FTMB to a regular middlebox through qualitative
discussion and approximate back-of-the-envelope estimates;
we present experimental results with our prototype in §6.
The logical components of the architecture are shown in
Figure 2. Packets ﬂow from the Input Logger (IL), to the
Master (M), to the Output Logger (OL). FTMB also needs a
Stable Storage (SS) subsystem with enough capacity to store
Figure 4: Parallel release. Each PAL is assigned a se-
quence number identifying when it was generated within
that thread; a packet is released from the output logger if
all PALs that were queued before it (on any thread) have
been logged.
4.4 Output Commit
We now develop an algorithm that ensures we do not re-
lease pi until all PALs corresponding to pi’s dependencies
have arrived at the output logger. This output commit deci-
sion is implemented at the output logger. The challenge in
this arises from the parallel nature of our system. Like the
master, our output logger is multi-threaded and each thread
has an independent queue. As a result, the PALs correspond-
ing to pi’s dependencies may be distributed across multi-
ple per-thread queues. We must thus be careful to minimize
cache misses and avoid the use of additional synchronization
operations.
Rejected Design: Fine-grained Tracking
The straightforward approach would be to explicitly track
individual packet and PAL arrivals at the output logger and
then release a packet pi after all of its PAL dependencies
have been logged. Our ﬁrst attempt implemented a ‘score-
board’ algorithm that did exactly this at the output log-
ger. We used two matrices to record PAL arrivals:
(i)
SEQ[i, j] which stores the sequence number of pi at vj and
(ii) PKT[j, k], the identiﬁer of the packet that accessed vj at
sequence number sk. These data structures contain all the
information needed to check whether a packet can be re-
leased. We designed a lock-free multi-threaded algorithm
that provably released data packets immediately as their de-
pendencies arrived at the middlebox; however, the overhead
of cache contention in reading and updating the scoreboard
resulted in poor throughput. Given the two matrices de-
scribed above, we can expect O(nc) cache misses per packet
release, where n is the number of shared variables and c the
number of cores (we omit details due to space considera-
tions). Despite optimizations, we ﬁnd that explicitly tracking
dependencies in the above fashion will result in the score-
board becoming the bottleneck for simple applications.
Parallel release of PALs
We now present a solution that is slightly more coarse-
grained, but is amenable to a parallel implementation with
very limited overhead. Our key observation here is that the
54!77 56!55!76!61!63!62!77![56, 77, 63, 77] Pkt A 52!74 60!53!75!57!59!75!76![45, 76, 60, 70] [53, 76, 57, 75] Pkt B 10Gbps Ethernet Master: Output PALs Output Logger ≥? Largest PAL sequence numbers are stored in dependency vector VORi  for packet VORi  compared against PALs at Output Logger 233the state of the entire VM, plus the packets and PALs accu-
mulated in the IL and OL between two snapshots. In our
implementation the IL, OL and SS are on the same physical
machine, which is expected to survive when M crashes.
To estimate the amount of storage needed we can assume
a snapshot interval in the 50–200 ms range (§6), and input
and output trafﬁc limited by the link’s speed (10–40 Gbit/s).
We expect to cope with a large, but not overwhelming PAL
generation rate; e.g., in the order of 5 M PALs/s (assuming
an input rate of 1.25M packets/second and 5 shared state
accesses per packet).
5.1
Input Logger
The main role of the IL is to record input trafﬁc since the
previous snapshot, so that packets can be presented in the
same order to the replica in case of a replay.
The input NIC on the IL can use standard mechanisms
(such as 5-tuple hashing on multiqueue NICs) to split traf-
ﬁc onto multiple queues, and threads can run the IL tasks
independently on each queue. Speciﬁcally, on each input
queue, the IL receives incoming packets, assigns them se-
quence numbers, saves them into stable storage, and then
passes them reliably to the Master.
Performance implications: The IL is not especially CPU
intensive, and the bandwidth to communicate with the mas-
ter or the storage is practically equal to the input band-
width: the small overhead for reliably transferring packets
to the Master is easily offset by aggregating small frames
into MTU-sized segments.
It follows that the only effect of the IL on performance is
the additional (one way) latency for the extra hop the trafﬁc
takes, which we can expect to be in the 5–10µs range [34].
5.2 Master
The master runs a version of the Middlebox code with the
following modiﬁcations:
• the input must read packets from the reliable stream
coming from the IL instead of individual packets com-
ing from a NIC;
• the output must transfer packets to the output queue
instead of a NIC.
• access to shared variables is protected by locks, and
includes calls to generate and queue PALs;
• access to special hardware functions (timers, etc.) also
generates PALs as above.
A shim layer takes care of the ﬁrst two modiﬁcations; for a
middlebox written using Click, this is as simple as replacing
the FromDevice and ToDevice elements. We require
that developers annotate shared variables at the point of their
declaration. Given these annotations, we automate the inser-
tion of the code required to generate PALs using a custom
tool inspired by generic systems for data race detection [56].
Our tool uses LLVM’s [43] analysis framework (also used
in several static analysis tools including the Clang Static An-
alyzer [3] and KLEE [19]) to generate the call graph for the
middlebox. We use this call graph to record the set of locks
held while accessing each shared variable in the middlebox.
If all accesses to the shared variable are protected by a com-
mon lock, we know that there are no contended accesses to
the variable and we just insert code to record and update the
PAL. Otherwise we generate a “protecting” lock and insert
code that acquires the lock before any accesses, in addition
to the code for updating the PALs. Note that because the new
locks never wrap another lock (either another new lock or a
lock in the original source code), it is not possible for this
instrumentation to introduce deadlocks [17, 21]. Since we
rely on static analysis, our tool is conservative, i.e. it might
insert a protecting lock even when none is required.
FTMB is often compatible with lock-free optimiza-
tions. For example, we implemented FTMB to support
seqlocks [13], which are used in multi-reader/single-writer
contexts. seqlocks use a counter to track what ‘version’ of a
variable a reader accessed; this version number replaces sij
in the PAL.
Performance implications: the main effect of FTMB on
the performance of the Master is the cost of PAL generation,
which is normally negligible unless we are forced to intro-
duce additional locking in the middlebox.
5.3 Output Logger
The Output Logger cooperates with the Master to transfer
PALs and data packets and to enforce output commit. The
algorithm is described in §4.4. Each thread at M transports
packets with a unique header such that NIC hashing at OL
maintains the same afﬁnity, enforcing a one-to-one mapping
between an eggress queue on M to an ingress queue on OL.
The trafﬁc between M and OL includes data packets, plus
additional information for PALs and vector clocks. As a very
coarse estimate, even for a busy middlebox with a total of
5 M PALs and vector clocks per second, assuming 16 bytes
per PAL, 16 bytes per vector clock, the total bandwidth over-
head is about 10% of the link’s capacity for a 10 Gbit/s link.
Performance implications:
once again the impact of
FTMB on the OL is more on latency than on throughput.
The minimum latency to inform the OL that PALs are in sta-
ble storage is the one-way latency for the communication.
On top of this, there is an additional latency component be-
cause our output commit check requires all queued PALs to
reach the OL before the OL releases a packet. In the worst
case a packet may ﬁnd a full PAL queue when computing
its vector clock, and so its release may be delayed by the
amount of time required to transmit a full queue of PALs.
Fortunately, the PAL queue can be kept short e.g., 128 slots
each, without any adverse effect on the system (PALs can
be sent to the OL right away; the only reason to queue them
is to exploit batching). For 16-byte PALs, it takes less than
2µs of link time to drain one full queue, so the total latency
introduced by the OL and the output commit check is in the
10-30µs range.
5.4 Periodic snapshots
FTMB takes periodic snapshots of the state of the Mas-
ter, to be used as a starting point during replay, and avoid
unbounded growth of the replay time and input and output
234logs size. Checkpointing algorithms normally freeze the VM
completely while taking a snapshot of its state.
Performance implications: The duration of the freeze,
hence the impact on latency, has a component proportional
to the number of memory pages modiﬁed between snap-
shots, and inversely proportional to bandwidth to the storage