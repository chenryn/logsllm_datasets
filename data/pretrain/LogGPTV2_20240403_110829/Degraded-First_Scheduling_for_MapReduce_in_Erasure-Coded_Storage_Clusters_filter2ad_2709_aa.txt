title:Degraded-First Scheduling for MapReduce in Erasure-Coded Storage Clusters
author:Runhui Li and
Patrick P. C. Lee and
Yuchong Hu
2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
Degraded-First Scheduling for MapReduce in Erasure-Coded Storage Clusters
Runhui Li, Patrick P. C. Lee, Yuchong Hu
Department of Computer Science and Engineering, The Chinese University of Hong Kong
{rhli, pclee}@cse.cuhk.edu.hk, PI:EMAIL
Abstract—We have witnessed an increasing adoption of
erasure coding in modern clustered storage systems to reduce
the storage overhead of traditional 3-way replication. However,
it remains an open issue of how to customize the data analytics
paradigm for erasure-coded storage, especially when the stor-
age system operates in failure mode. We propose degraded-
ﬁrst scheduling, a new MapReduce scheduling scheme that
improves MapReduce performance in erasure-coded clustered
storage systems in failure mode. Its main idea is to launch
degraded tasks earlier so as to leverage the unused network
resources. We conduct mathematical analysis and discrete event
simulation to show the performance gain of degraded-ﬁrst
scheduling over Hadoop’s default locality-ﬁrst scheduling. We
further implement degraded-ﬁrst scheduling on Hadoop and
conduct testbed experiments in a 13-node cluster. We show that
degraded-ﬁrst scheduling reduces the MapReduce runtime of
locality-ﬁrst scheduling.
I. INTRODUCTION
Clustered storage systems, such as GFS [16], HDFS [30],
Azure [4], have been widely deployed in enterprises to
provide a scalable and reliable storage platform for big data
analytics based on MapReduce [9] or Dryad [21]. They
stripe data across thousands of nodes (or servers) connected
over a network, on which parallel data computations can
be performed. As a storage system scales, node failures
are commonplace [16], and temporary data unavailability
becomes prevalent due to frequent transient failures [13] and
system upgrades [24]. To ensure data availability at any time,
traditional designs of GFS, HDFS, and Azure replicate each
data block into three copies to provide double-fault tolerance
[4, 16, 30]. However, as the volume of global data surges
to the zettabyte scale [15], the 200% redundancy overhead
of 3-way replication becomes a scalability bottleneck.
Erasure coding provides an alternative to ensuring data
availability. It operates by encoding data blocks into parity
blocks, such that a subset of data and parity blocks can
sufﬁciently recover the original data blocks. It is known that
erasure coding costs less storage overhead than replication
under the same fault tolerance [32]. For example, it can
reduce the redundancy overhead of 3-way replication from
200% to 33%, while still achieving higher availability [20].
Extensive efforts (e.g., [13, 20, 29]) have studied the use
of erasure coding in clustered storage systems (e.g., GFS,
HDFS, Azure) that provide data analytics services. Although
erasure coding generally has higher performance overhead
than replication, recent results show that the overhead of
erasure coding can be mitigated through efﬁcient coding
constructions [27], read parallelization [11], and hardware-
assisted computations [26]. In particular, when data is un-
available due to node failures, reads are degraded in erasure-
coded storage as they need to download data from surviving
nodes to reconstruct the missing data. In view of this, several
studies [20, 22, 29] propose to optimize degraded reads in
erasure-coded clustered storage systems, by reducing the
amount of downloaded data for reconstruction.
Despite the extensive studies on erasure-coded clustered
storage systems, it remains an open issue of how to cus-
tomize the data analytics paradigm, such as MapReduce [9],
for such systems, especially when they operate in failure
mode and need to perform degraded reads. In this work,
we explore Hadoop’s version of MapReduce on HDFS-
RAID [18], a middleware layer that extends HDFS to
support erasure coding. Traditional MapReduce scheduling
emphasizes locality, and implements locality-ﬁrst scheduling
by ﬁrst scheduling local tasks that run on the nodes holding
the input data for the tasks. MapReduce is designed with
replication-based storage in mind. In the presence of node
failures, it re-schedules tasks to run on other nodes that
hold the replicas. However, the scenario becomes different
for erasure-coded storage, where MapReduce tasks must
issue degraded reads to download data from other surviving
nodes. Such degraded tasks are typically scheduled to launch
after all local tasks are completed, and when they launch,
they compete for network resources to download data from
surviving nodes. This can signiﬁcantly increase the overall
runtime of a MapReduce job. Thus, a key motivation of this
work is to customize MapReduce scheduling for erasure-
coded storage in failure mode.
Our observation is that while local tasks are running,
the MapReduce job does not fully utilize the available
network resources. Thus, this paper proposes degraded-ﬁrst
scheduling, whose main idea is to schedule some degraded
tasks at earlier stages of a MapReduce job and allow them
to download data ﬁrst using the unused network resources.
To this end, this paper makes three contributions:
• We propose degraded-ﬁrst scheduling, a new MapRe-
duce scheduling scheme that
improves MapReduce
performance in erasure-coded clustered storage sys-
tems operating in failure mode. We conduct simple
mathematical analysis to demonstrate that degraded-
ﬁrst scheduling improves Hadoop’s default locality-ﬁrst
scheduling. Our numerical results show that degraded-
ﬁrst scheduling reduces the MapReduce runtime by
978-1-4799-2233-8/14 $31.00 © 2014 IEEE
978-1-4799-2233-8/14 $31.00 © 2014 IEEE
978-1-4799-2233-8/14 $31.00 © 2014 IEEE
DOI 10.1109/DSN.2014.47
DOI 10.1109/DSN.2014.47
DOI 10.1109/DSN.2014.47
419
419
419
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:03:27 UTC from IEEE Xplore.  Restrictions apply. 
Rack
 Core 
Switch
Top-of-rack 
    Switch
Top-of-rack 
    Switch
Node 1
Node 2
Node 3
Node 4
Figure 1. Example cluster with four nodes grouped into two racks.
15% to 43%. We also propose two heuristics that
achieve locality preservation and rack awareness, so as
to improve the robustness of degraded-ﬁrst scheduling
in general conﬁgurations.
• We implement a discrete event simulator for MapRe-
duce to explore the performance gain of degraded-
ﬁrst scheduling in a large-scale cluster. We show
that degraded-ﬁrst scheduling reduces the runtime of
locality-ﬁrst scheduling by up to 39.6% when the clus-
ter runs a single MapReduce job, and by up to 48.6%
when multiple MapReduce jobs run simultaneously.
• We implement degraded-ﬁrst scheduling on Hadoop,
and compare the performance of locality-ﬁrst schedul-
ing and degraded-ﬁrst scheduling in a 13-node Hadoop
cluster. Degraded-ﬁrst scheduling reduces the MapRe-
duce runtime of locality-ﬁrst scheduling by up to 27.0%
and 28.4% for single-job and multi-job scenarios, re-
spectively.
The rest of the paper proceeds as follows. Section II ﬁrst
presents background details of Hadoop and erasure codes.
Section III motivates via an example the issue of Hadoop’s
default
locality-ﬁrst scheduling. Section IV presents the
design of degraded-ﬁrst scheduling. Section V describes our
discrete event MapReduce simulator and presents simula-
tion results. Section VI describes our implementation of
degraded-ﬁrst scheduling on Hadoop and present testbed
experimental results. Section VII reviews related work, and
Section VIII concludes the paper.
II. BACKGROUND
A. Hadoop
We consider a Hadoop cluster composed of multiple nodes
(or servers) that are grouped into different racks. Typical
clusters connect all nodes via a hierarchy of switches.
Without loss of generality, we consider a simpliﬁed two-
level case where nodes within each rack are connected via
a top-of-rack switch, and all the racks are connected via a
core switch. Figure 1 illustrates an example.
Hadoop runs on a distributed ﬁle system HDFS [30] for
reliable storage. HDFS divides a ﬁle into ﬁxed-size blocks,
which form the basic units for read and write operations.
Since node failures are common [16], HDFS uses replication
to maintain data availability, such that each block is repli-
cated into multiple (by default, three) copies and distributed
across different nodes.
Hadoop implements MapReduce [9] for data-intensive
computations on HDFS data. We ﬁrst deﬁne the termi-
nologies as follows. A MapReduce program (called job) is
split into multiple tasks of two types: a map task processes
an input block and generates intermediate results, and a
reduce task collects the intermediate results through a shufﬂe
step, processes them, and outputs the ﬁnal results to HDFS.
MapReduce uses a single master node to coordinate multiple
slave nodes to run the tasks. Each slave has a ﬁxed number
of map and reduce slots, and each map (reduce) slot is
used for running one map (reduce) task. If a slave has free
slots available, it requests the master for map or reduce
tasks through periodic heartbeat messages. The master then
performs task scheduling and decides which task to run ﬁrst.
In typical deployment environments of MapReduce, net-
work bandwidth is scarce [9]. Thus, MapReduce emphasizes
data locality by trying to schedule a map task to run on a
(slave) node that stores a replica of the data block, or a
node that is located near the data block. This saves the time
of downloading blocks from other nodes over the network.
Note that reduce tasks cannot exploit locality because they
need to download intermediate outputs from multiple slaves.
Here, a map task can be classiﬁed into three types: (i) node-
local, in which the task processes a block stored in the
same node, (ii) rack-local, in which the task downloads and
processes a block stored in another node of the same rack,
and (iii) remote, in which the task downloads and processes
a block stored in another node of a different rack. In this
paper, we collectively call the ﬁrst two types local, since
rack-local tasks can run as fast as node-local tasks if the
network speed within the same rack is sufﬁciently high. The
default task scheduling scheme in Hadoop ﬁrst assigns map
slots to local tasks, followed by remote tasks. We call this
approach locality-ﬁrst scheduling.
B. Erasure Coding
To reduce the redundancy overhead due to replication,
erasure coding can be used. An erasure code is deﬁned by
parameters (n, k), such that k original blocks (termed native
blocks) are encoded to form n − k parity blocks, and any k
out of the n blocks can recover the original k native blocks.
We call the collection of the n blocks a stripe. Examples
of erasure codes include Reed-Solomon codes [28] and
Cauchy Reed-Solomon codes [3]. Hadoop’s authors propose
a middleware layer called HDFS-RAID [18], which operates
on HDFS and transforms block replicas into erasure-coded
blocks. HDFS-RAID divides a stream of native blocks into
groups of k blocks, and encodes each group independently
into a stripe according to the parameters (n, k).
In the presence of node failures, native blocks stored in the
failed nodes are unavailable (we call them the lost blocks).
In replication, a read to a lost block can be re-directed to
420420420
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:03:27 UTC from IEEE Xplore.  Restrictions apply. 
if j has an unassigned local task then
assign the local task to s
for each running job j in the job list do
for each free map slot on slave s do
Algorithm 1 Locality-First Scheduling on HDFS-RAID
1: while a heartbeat comes from slave s do
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
end for
12:
13: end while
else if j has an unassigned degraded task then
else if j has an unassigned remote task then
assign the remote task to s
assign the degraded task to s
end if
end for
another block replica. However, in erasure coding, reading
a lost block requires a degraded read, which reads the
blocks from any k surviving nodes of the same stripe and
reconstructs the lost blocks1. Although erasure codes are
designed to tolerate multiple node failures, it is known that
single-node failures are the most common failure recovery
scenario in practice [20, 22]. Thus, our discussion focuses
on the failure mode where the cluster has only one failed
node while a MapReduce job is running, and we address
multi-node failures using simulations (see Section V).
MapReduce is compatible with HDFS-RAID and also
follows locality-ﬁrst scheduling. The main difference from
traditional replication is that HDFS-RAID reconstructs the
lost block via a degraded read. We deﬁne a new type of
map tasks called degraded tasks, which ﬁrst read data from
other surviving nodes to reconstruct the lost block and then
process the reconstructed block. Degraded tasks are given
the lowest priority in the default locality-ﬁrst scheduling, and
they are scheduled after local and remote tasks. Algorithm 1
summarizes the pseudo-code of the default
locality-ﬁrst
scheduling on HDFS-RAID.
III. MOTIVATING EXAMPLE
In this section, we elaborate via a motivating example
why the default locality-ﬁrst scheduling hurts MapReduce
performance in failure mode. We then provide intuitions how
we can improve MapReduce performance.
We ﬁrst review the default block placement policy of
HDFS, and later extend the policy for HDFS-RAID. By
default, HDFS uses 3-way replication and places the three
replicas using the following rule: the ﬁrst replica is placed in
a random node, and the second and third replicas are placed
in two different random nodes that are located in a different
rack from the ﬁrst replica. This placement policy can tolerate
(1) an arbitrary double-node failure; and (2) an arbitrary
single-rack failure. Correspondingly, for HDFS-RAID, we
1We consider the conventional degraded read approach, which always
reads from any k surviving nodes. Some special erasure code constructions
have been proposed (e.g., [20, 22, 29]) to reduce the number of blocks
read. Our work also applies to such erasure code constructions.
421421421
Switch
Switch
Switch
B0,0
B1,0
B2,0
B3,0
Node 1
B0,1
P2,1
B4,0
P5,0
Node 2
B1,1
P3,0
B4,1
P5,1
Node 3
P0,0
P1,0
B2,1
P3,1
P4,0
B5,0
Node 4
P0,1
P1,1
P2,0
B3,1
P4,1
B5,1
Node 5
Figure 2. A ﬁve-node cluster with 12 native blocks and 12 parity blocks,
assuming a (4,2) coding scheme is used for fault-tolerance. We assume that
Node 1 fails while MapReduce is running.
consider an erasure code satisfying that (1) n − k ≥ 2; and
(2) at most n − k out of n blocks of any stripe are placed
on the same rack.
We design an example shown in Figure 2 that realizes
the above two conditions. The ﬁgure has a two-rack cluster,
in which the ﬁrst rack has three nodes and the second
one has two nodes. The racks are connected by 100Mbps
Ethernet switches. Let the block size be 128MB. If we ignore
transmission overhead, then transmitting a block from one
node to another node takes around 10s. Suppose now that
the cluster stores a 12-block ﬁle, and we use a (4, 2) erasure
code to encode a ﬁle into six stripes. In the i-th stripe, where
0 ≤ i ≤ 5, let Bi,0 and Bi,1 be the two native blocks, and
Pi,0 and Pi,1 be the two parity blocks. We assume that each
node has two map slots, meaning that it can run at most
two map tasks simultaneously. According to [35], half of
map tasks take 1s to 19s to complete. Thus, we assume that
the time for processing a map task is also 10s.
We now explain why locality-ﬁrst scheduling hurts
MapReduce performance in failure mode. We consider the
duration of the map phase. Suppose that a MapReduce
job is processing the stored data while Node 1 is failed,
so degraded tasks are triggered to process the lost blocks
B0,0, B1,0, B2,0, and B3,0. Figure 3(a) shows the map-
slot activities with locality-ﬁrst scheduling. According to
Algorithm 1, each surviving node is ﬁrst assigned local tasks
to process its stored blocks. After all the local tasks are
completed, the degraded tasks are launched. Suppose we
assign the degraded tasks for the lost blocks B0,0, B1,0,
B2,0, and B3,0 to Nodes 2, 3, 4, and 5, respectively, such
that a node just needs to download the ﬁrst parity block
P0,0, P1,0, P2,0, P3,0, respectively, from another node to
reconstruct the lost block for processing. Node 4 downloads
P2,0 from Node 3 (in a different rack) and Node 5 downloads
P3,0 from Node 4 (in the same rack). However, Nodes 2
and 3, located in the same rack, need to compete for the
download link of the rack so as to download P0,0 and P1,0
from another rack, respectively. This doubles the download
time, from 10s to 20s. The entire map phase lasts for 40s.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:03:27 UTC from IEEE Xplore.  Restrictions apply. 
Local tasks finish
Map phase finishes
Map Slot
Node 2
Node 3
Node 4
Node 5
Slot 1
Slot 2
Slot 1
Slot 2
Slot 1
Slot 2
Slot 1
Slot 2
Proc B0,1
Proc B4,0
Proc B1,1
Proc B4,1
Proc B2,1
Proc B5,0
Proc B3,1
Proc B5,1
Get P0,0
Proc B0,0
Get P1,0
Proc B1,0
Get P2,0
Proc B2,0
Get P3,0
Proc B3,0
0
10
20
30