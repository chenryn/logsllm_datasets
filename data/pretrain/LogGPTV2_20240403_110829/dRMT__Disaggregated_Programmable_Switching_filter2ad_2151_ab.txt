lems: (1) packing tables into memory clusters, which can be solved
using simple bin packing, and (2) scheduling operations on proces-
sors. An important property of dRMT is that these two problems are
decoupled; tables can be placed in memory clusters irrespective of
how operations are scheduled, and vice-versa. This makes compiling
programs to dRMT conceptually simpler than RMT. We discuss the
dRMT scheduling problem in detail in §3.
Figure 2: A toy program and its schedule on an RMT pipeline.
Figure 3: Schedule for toy program on dRMT with 2 processors.
The benefits of memory disaggregation are not limited to dRMT.
One could similarly add a crossbar to RMT, connecting all pipeline
stages to all memory clusters, and providing the same benefits. How-
ever, as we discuss next, dRMT takes disaggregation a step further
by getting rid of the pipeline entirely and disaggregating the compute
resources as well. Our results show that compute disaggregation is
essential to achieving the full potential of disaggregation (§4).
2.2 Compute disaggregation
The RMT architecture enforces a rigid match-then-action sequence
of operations in the pipeline. In dRMT, we allow matches and actions
to be interleaved in any order on a processor, as long as dependencies
and resource constraints are respected. This has several benefits.
Improving hardware utilization. Compute disaggregation further
increases flexibility to order operations in a way that maximizes
hardware utilization. We demonstrate this advantage using a toy
program whose dependencies are given by the directed acyclic graph
(DAG) in Figure 2. We assume that every edge mandates a minimum
latency of one cycle between the operations on the edge; the numbers
on the nodes represent their (match or action) resource requirements.
We schedule this DAG to run both on RMT and dRMT, assuming
both can perform up to 1 match every clock cycle and 2 actions every
clock cycle in each stage/processor.
In the RMT pipeline, this DAG requires at least 3 stages because
there is insufficient match capacity to run matches M1 and M2 in
parallel, and both M1 and M2 have to follow A0 (Figure 2). The
problem with this schedule is that the match unit in the first stage
is stranded. dRMT can schedule the same program using just 2
processors, as shown in Figure 3. Each row shows the sequence of
operations for one packet, executed on one processor. Notice that
packets are sent to processors in round-robin order: the packet that
arrives in cycle k runs on processor k mod 2. Also notice that the
operations in each processor do not exceed the processor’s capacity
of 1 match and 2 actions per clock cycle.
A0M1A1M2A211112A0A1M1A2M2Operation dependency graphRMT pipeline scheduleStage 1Stage 2Stage 3cycleproc.01234560A0M1M2A1&A21A0M1M2A1&A20A0M1M2A1&A21A0M1M2A1&A2SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
S. Chole et al.
Eliminating performance cliffs. Packet switching ASICs typically
have a recirculation path, by which packets that do not finish within
a single pass can be sent back to the beginning (mingled with newly
arriving packets). In a pipelined architecture, if a particular program
cannot be scheduled to fit within the available match-action stages,
one may split the program into multiple passes. The packet rate for
a K-pass schedule is 1/K of the system’s maximum rate.
By contrast, with dRMT, throughput degrades gracefully as pro-
gram complexity increases. For example, if a dRMT system can
support M table searches at a rate of 1 packet per clock cycle, it is
possible to support a program that requires M′ > M table searches
at a rate close to M/M′ packets per clock cycle. This simply re-
quires increasing the time that packets spend in their processors, and
reducing the rate at which packets are sent to processors.
3 SCHEDULING FOR DETERMINISTIC
PERFORMANCE
We now describe the dRMT scheduling problem and develop a sched-
uling technique to execute a P4 program efficiently on dRMT hard-
ware while providing deterministic throughput and latency. Specifi-
cally, given a P4 program, we seek a fixed schedule—precomputed
at compile time—that satisfies two constraints: (1) the P4-program-
specific dependencies, and (2) the dRMT architecture’s resource
constraints such as the match and action capacities of each processor.
An important aspect of our formulation is that the same schedule is
repeated across processors in round-robin fashion. This simplifies
the problem significantly, as it greatly reduces the space of schedules
that we must search over. It also gives rise to a unique set of cyclic
constraints that our schedules must satisfy.
We begin by describing the dRMT scheduling problem, and then
introduce our main theoretical results. In particular, we show the
NP-hardness of fixed scheduling for deterministic performance, and
then formulate the problem as an Integer Linear Program (ILP).
3.1 Scheduling problem
First, we present the P4 program dependency constraints and the
dRMT architectural constraints. Then, we introduce the scheduling
optimization problem given these two types of constraints, and also
present an illustrative example after the definitions.
Program dependencies. Each packet entering dRMT follows the
control flow [22] dictated by a P4 program. This control flow speci-
fies how the packet headers are to be processed. To express a pro-
gram’s dependencies, we define an Operation Dependency Graph
(ODG), i.e., a directed acyclic graph (DAG) in which the nodes
represent the match and action operations executed by a packet
while traversing the switch, and the edges describe the dependencies
between these operations.
An edge between two nodes dictates that any valid schedule must
perform the first before the second. The edge is annotated with a
latency that specifies the minimum time separation between the
operations. For an edge from node A to B, this latency is the time
that it takes to complete the operation on node A. We assume that
a table match takes ∆M clock cycles, and an action takes ∆A clock
cycles, meaning that an operation dependent on a match or an action
has to wait ∆M or ∆A cycles respectively.
For operations that are conditionally executed (e.g., based on a
predicate or whether there was a table hit or miss), we conserva-
tively assume that both branches of the condition are executed and
schedule both, even though only one will execute at run-time. This is
equivalent to executing all action nodes within a conditional branch
speculatively, and committing any side effects based on the condi-
tional test afterwards, similar to the speculative execution model
adopted by RMT [16]. This worst-case assumption simplifies the
scheduling problem. In practice, we find that it still allows us to
schedule programs efficiently (§4).
Example 3.1 (P4 program). Figure 4a depicts the control flow of
a simple P4 program that supports unicast and multicast routing (it
is a fragment from the L2L3 program in [22]). Figure 4b depicts the
corresponding ODG. M0, M1, M2 and M3 can be executed concur-
rently. Action A1 must precede A2 because they both write to the
same fields, and A2’s outcome must be the end result.
The ODG is similar to the Table Dependency Graph (TDG) [22],
proposed by Jose et al. to represent P4 program dependencies. But
the ODG is simpler. The TDG annotates edges based on the type
of dependency (e.g., match, action, and reverse-match [22]). By
splitting each table in the TDG into two distinct match and action
nodes (with an edge between them), the ODG lets us represent all
these dependencies in a unified way using appropriate edge latencies.
Architectural constraints. A schedule must respect several dRMT
architectural constraints. We detail these constraints before providing
an illustrative example.
Processors. The dRMT architecture contains N processors. At each
clock cycle, each processor can start the following operations for any
of its packets: launch table matches, launch actions, or do nothing
(no-op). Each table match takes ∆M clock cycles, and each action
takes ∆A clock cycles, meaning that an operation dependent on a
match or an action has to wait ∆M or ∆A cycles respectively. At
each clock cycle, when deciding which operations to launch, the
processor is restricted as follows:
(1) It can initiate up to ¯M parallel table searches of up to b bits
each. For instance, it can look up a match-action table using
a key of size 2.5 · b by sending three parallel vectors of b bits
each, as long as 3 ≤ ¯M.
(2) It can modify up to ¯A action fields in parallel.
(3) Finally, it can only start matches for up to IPC (Inter-Packet
Concurrency) different packets, and likewise start actions
for up to IPC different packets. The set of packets that start
matches and the set of packets that start actions need not be
equal.
Memory access constraint. At each clock cycle, a P4 table (and
hence its associated memory clusters) can only be accessed by a
single packet from a single processor.
Crossbar. At each clock cycle, the above constraints mean that each
processor can generate up to ¯M b-bit-width keys for table lookups
that it sends to the memory clusters via a crossbar. The crossbar also
permits multicast.
Fixed scheduling. We restrict ourselves to fixed schedules (i.e., pre-
determined for all types of packets at compile time) given a P4
program and a dRMT architecture. Specifically, we assume that
the arriving packets are assigned to one of the N processors in a
dRMT: Disaggregated Programmable Switching
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
(a) Control flow program (fragment from L2L3 program in [22]). Each
table i has match Mi and action Ai . Note that table 3 does not match on
fields that A2 modifies, so M3 does not depend on M2 or A2. Yet both A2
and A3 modify the same field, and the final value should be that of A3, so
A3 cannot be executed before A2.
(b) Operation Dependency Graph (ODG) for the fragmented L2L3 pro-
gram. Light rectangles represent matches, and dark ones represent ac-
tions. Arrows represent dependencies and are annotated with latencies.
When scheduling the ODG, we assume that both branches dependent on
the unicast vs. multicast condition are executed, similar to RMT’s specu-
lative execution model [16].
(c) Naive schedule with conflicts. At clock cycle 2, two architectural con-
straints are violated: (1) both the first and the third packet assigned to
processor 0 are scheduled to execute matches although IPC = 1, and (2)
4 parallel table searches are initiated although ¯M = 2.
(d) Schedule without conflicts. The insertion of a no-op (null) in the sched-
ule slightly increases latency but helps resolve both the architectural con-
flicts.
Figure 4: A simple unicast-multicast packet processing program, its Operation Dependency Graph (ODG), and two potential sched-
ules with and without conflicts. In (c) and (d) each row represents the schedule for a different packet with its allocated processor,
while each column represents a different clock cycle. A match lasts ∆M = 2 clock cycles, and an action ∆A = 1 clock cycle. We assume
that there is no concurrency between different packets in the same processor (IPC = 1) and that at most 2 parallel table searches can
be initiated at each clock cycle by a processor ( ¯M = 2).
strict round-robin fashion, and that each processor receives a new
packet every P clock cycles. Therefore, the switch throughput is N /P
(e.g., P = N means that a new packet enters the switch every cycle).
Then, we need to find a fixed schedule, i.e., a single cycle-by-cycle
schedule that is pre-determined at compile time and is applied in
the exact same way to all incoming packets. For instance, at line
rate (P = N ), the same operations that are executed by processor 0
at cycle t are also executed by processor 1 at t + 1, by processor 2
at t + 2, and so on, until processor 0 executes the same operations
again at t + P. This schedule is valid whenever it satisfies both the
P4-specific constraints and the dRMT architectural constraints.
Example 3.2 (Valid schedule). We want to find a fixed schedule
with N = 2 processors to support a throughput of 1 (i.e., P = 2)
for the unicast-multicast P4 example described in Example 3.1 and
Figure 4b. For simplicity, assume that each match requires ∆M = 2
cycles and each action ∆A = 1 cycle; that the limit ¯A on action
fields is large and can be ignored; that the limit on parallel table
searches is ¯M = 2, with all the keys of size ≤ b bits; and that the
packet concurrency is IPC = 1, i.e., the matches (resp. actions) that
a processor executes in a given cycle are restricted to belong to the
same packet.
Figure 4c first illustrates a naive schedule that violates the ar-
chitectural constraints. The top row tracks time in cycles, and the
following rows represent different incoming packets. The leftmost
column reflects the processor that services this packet, alternating
between processors 0 and 1 in a round-robin manner. To build this
first schedule, we simply follow the ODG in Figure 4b from left
to right and top to bottom, thus arriving at the following possible
sequence of operations:
M0&M1 →M2&M3 →A1&A2 →A3.
Note that the ODG allows all matches to run in parallel; however,
since ¯M = 2, only two of them can run in parallel. Hence, at time 0, a
packet enters processor 0, and M0 and M1 are executed in parallel. At
time 2, after M0 and M1 are finished on this packet, M2 and M3 are
executed concurrently. All packets in all processors follow the same
schedule, hence the entire sequence is simply shifted one column to
the right at each row. Unfortunately, this scheduling sequence is in-
valid because it violates two architectural constraints. First, in clock
cycle 2, processor 0 executes matches corresponding to 2 different
packets (the first and the third), thus exceeding the IPC limit of 1.
Second, in clock cycle 2, processor 0 executes 4 matches while only
¯M = 2 matches are allowed.
sequence of operations is:
Figure 4d illustrates an alternative schedule without conflicts. The
M0&M1 →no-op →M2&M3 →A1&A2 →A3.
The insertion of the no-op in the schedule slightly increases latency
but helps resolve all conflicts.
Scheduling objective. Given a dRMT architecture with N proces-
sors and a P4 program, our general objective is to maximize the
dRMT throughput. To do so, at compile time, we run an optimization
sub-routine that indicates whether a given throughput is feasible
under the constraints provided by the P4 program dependencies
and the dRMT architecture. We can then use this subroutine in a
binary-search procedure to establish the maximum throughput.
Table 1: Unicast RoutingM1:ipv4.dIPA1:Ethernet.sMac,Table 0: RoutableEthernet.dMac,M0:ethernet.sMac,vlan_tag.vlanTable 3: IGMPethernet.dMac,M3:ipv4.dIP,vlan_tag.vlanTable 2: Multicast Routingvlan_tag.vlan,A0:nullM2:ipv4.dIPstd_meta.ig_portA2:std_meta.mcast_idxA3:std_meta.mcast_idxucastmcastM0: RoutableM1: Unicast RoutingA3: IGMPM3: IGMPA2: Multicast RoutingM2: Multicast RoutingA1: Unicast Routing222221cycleproc.0123456780M0&M1M2&M3A1&A2A31M0&M1M2&M3A1&A2A30M0&M1M2&M3A1&A2A31M0&M1M2&M3A1&A2A3ΔMΔAcycleproc.01234567890M0&M1no-opM2&M3A1&A2A31M0&M1no-opM2&M3A1&A2A30M0&M1no-opM2&M3A1&A2A31M0&M1no-opM2&M3A1&A2A3SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
S. Chole et al.
In addition, given any arbitrary dRMT throughput, we want to
minimize the system’s resources required to support this throughput,
and in particular the number of packets that each processor needs
to handle. Let T be the schedule’s fixed packet latency. Then we
find [5] that the maximum concurrent number of packets at each
processor is ⌈T /P⌉. Thus, given N and P, if we minimize latency, we
also minimize the maximum number of packets that each processor
needs to handle. As a result, given some assumed throughput, we
formally define our scheduling goal as:
Minimize T
subject to:
P4 program dependency constraints
dRMT architectural constraints
dRMT throughput
(1)
3.2 Simplifying dRMT scheduling
Now that we have established the scheduling goal in Equation 1, we
show how it is possible to simplify the dRMT scheduling problem
by successively considering only a single processor, then a single
packet on a single processor.
Single-processor scheduling. We start by showing that to schedule
dRMT, we can focus on scheduling a single processor instead of
jointly scheduling all the processors. Specifically, consider a fixed
schedule for a single processor that respects the program-specific de-
pendency constraints and the ¯M, ¯A and IPC architectural constraints
and is applied by all processors in the exact same manner. Then it is
a valid dRMT schedule that respects all the constraints. In particular,