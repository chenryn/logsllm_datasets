=== Monitoring Individual Nodes
`Cluster-health` is at one end of the spectrum--a very high-level overview of
everything in your cluster. ((("clusters", "administration", "monitoring individual nodes")))((("nodes", "monitoring individual nodes"))) The `node-stats` API is at the other end. ((("Node Stats API", id="ix_NodeStats", range="startofrange"))) It provides
a bewildering array of statistics about each node in your cluster.
`Node-stats` provides so many stats that, until you are accustomed to the output,
you may be unsure which metrics are most important to keep an eye on.  We'll
highlight the most important metrics to monitor (but we encourage you to
log all the metrics provided--or use Marvel--because you'll never know when
you need one stat or another).
The `node-stats` API can be executed with the following:
[source,bash]
----
GET _nodes/stats
----
Starting at the top of the output, we see the cluster name and our first node:
[source,js]
----
{
   "cluster_name": "elasticsearch_zach",
   "nodes": {
      "UNr6ZMf5Qk-YCPA_L18BOQ": {
         "timestamp": 1408474151742,
         "name": "Zach",
         "transport_address": "inet[zacharys-air/192.168.1.131:9300]",
         "host": "zacharys-air",
         "ip": [
            "inet[zacharys-air/192.168.1.131:9300]",
            "NONE"
         ],
...
----
The nodes are listed in a hash, with the key being the UUID of the node.  Some
information about the node's network properties are displayed (such as transport address,
and host). These values are useful for debugging discovery problems, where
nodes won't join the cluster. Often you'll see that the port being used is wrong,
or the node is binding to the wrong IP address/interface.
==== indices Section
The `indices` section lists aggregate statistics((("indices", "indices section in Node Stats API"))) for all the indices that reside
on this particular node:
[source,js]
----
    "indices": {
        "docs": {
           "count": 6163666,
           "deleted": 0
        },
        "store": {
           "size_in_bytes": 2301398179,
           "throttle_time_in_millis": 122850
        },
----
The returned statistics are grouped into the following sections:
- `docs` shows how many documents reside on
this node, as well as the number of deleted docs that haven't been purged
from segments yet.
- The `store` portion indicates how much physical storage is consumed by the node.
This metric includes both primary and replica shards.  If the throttle time is
large, it may be an indicator that your disk throttling is set too low
(discussed in >).
[source,js]
----
        "indexing": {
           "index_total": 803441,
           "index_time_in_millis": 367654,
           "index_current": 99,
           "delete_total": 0,
           "delete_time_in_millis": 0,
           "delete_current": 0
        },
        "get": {
           "total": 6,
           "time_in_millis": 2,
           "exists_total": 5,
           "exists_time_in_millis": 2,
           "missing_total": 1,
           "missing_time_in_millis": 0,
           "current": 0
        },
        "search": {
           "open_contexts": 0,
           "query_total": 123,
           "query_time_in_millis": 531,
           "query_current": 0,
           "fetch_total": 3,
           "fetch_time_in_millis": 55,
           "fetch_current": 0
        },
        "merges": {
           "current": 0,
           "current_docs": 0,
           "current_size_in_bytes": 0,
           "total": 1128,
           "total_time_in_millis": 21338523,
           "total_docs": 7241313,
           "total_size_in_bytes": 5724869463
        },
----
- `indexing` shows the number of docs that have been indexed.  This value is a monotonically
increasing counter; it doesn't decrease when docs are deleted.  Also note that it
is incremented anytime an _index_ operation happens internally, which includes
things like updates.
+
Also listed are times for indexing, the number of docs currently being indexed,
and similar statistics for deletes.
- `get` shows statistics about get-by-ID statistics.  This includes `GET` and
`HEAD` requests for a single document.
- `search` describes the number of active searches (`open_contexts`), number of
queries total, and the amount of time spent on queries since the node was
started.  The ratio between `query_time_in_millis / query_total` can be used as a
rough indicator for how efficient your queries are.  The larger the ratio,
the more time each query is taking, and you should consider tuning or optimization.
+
The fetch statistics detail the second half of the query process (the _fetch_ in
query-then-fetch).  If more time is spent in fetch than query, this can be an
indicator of slow disks or very large documents being fetched, or
potentially search requests with paginations that are too large (for example, `size: 10000`).
- `merges` contains information about Lucene segment merges.  It will tell you
the number of merges that are currently active, the number of docs involved, the cumulative
size of segments being merged, and the amount of time spent on merges in total.
+
Merge statistics can be important if your cluster is write heavy.  Merging consumes
a large amount of disk I/O and CPU resources.  If your index is write heavy and
you see large merge numbers, be sure to read >.
+
Note: updates and deletes will contribute to large merge numbers too, since they
cause segment _fragmentation_ that needs to be merged out eventually.
[source,js]
----
        "filter_cache": {
           "memory_size_in_bytes": 48,
           "evictions": 0
        },
        "id_cache": {
           "memory_size_in_bytes": 0
        },
        "fielddata": {
           "memory_size_in_bytes": 0,
           "evictions": 0
        },
        "segments": {
           "count": 319,
           "memory_in_bytes": 65812120
        },
        ...
----
- `filter_cache` indicates the amount of memory used by the cached filter bitsets,
and the number of times a filter has been evicted.  A large number of evictions
_could_ indicate that you need to increase the filter cache size, or that
your filters are not caching well (for example, they are churning heavily because of high cardinality,
such as caching `now` date expressions).
+
However, evictions are a difficult metric to evaluate.  Filters are cached on a
per-segment basis, and evicting a filter from a small segment is much less
expensive than evicting a filter from a large segment.  It's possible that you have many evictions, but they all occur on small segments, which means they have
little impact on query performance.
+
Use the eviction metric as a rough guideline.  If you see a large number, investigate
your filters to make sure they are caching well.  Filters that constantly evict,
even on small segments, will be much less effective than properly cached filters.
- `id_cache` shows the memory usage by parent/child mappings.  When you use
parent/children, the `id_cache` maintains an in-memory join table that maintains
the relationship.  This statistic will show you how much memory is being used.
There is little you can do to affect this memory usage, since it has a fairly linear
relationship with the number of parent/child docs.  It is heap-resident, however,
so it's a good idea to keep an eye on it.
- `field_data` displays the memory used by fielddata,((("fielddata", "statistics on"))) which is used for aggregations,
sorting, and more.  There is also an eviction count.  Unlike `filter_cache`, the eviction
count here is useful:  it should be zero or very close.  Since field data
is not a cache, any eviction is costly and should be avoided.  If you see
evictions here, you need to reevaluate your memory situation, fielddata limits,
queries, or all three.
- `segments` will tell you the number of Lucene segments this node currently serves.((("segments", "number served by a node")))
This can be an important number.  Most indices should have around 50&#x2013;150 segments,
even if they are terabytes in size with billions of documents.  Large numbers
of segments can indicate a problem with merging (for example, merging is not keeping up
with segment creation).  Note that this statistic is the aggregate total of all
indices on the node, so keep that in mind.
+
The `memory` statistic gives you an idea of the amount of memory being used by the
Lucene segments themselves.((("memory", "statistics on")))  This includes low-level data structures such as
posting lists, dictionaries, and bloom filters.  A very large number of segments
will increase the amount of overhead lost to these data structures, and the memory
usage can be a handy metric to gauge that overhead.
==== OS and Process Sections
The `OS` and `Process` sections are fairly self-explanatory and won't be covered
in great detail.((("operating system (OS), statistics on")))  They list basic resource statistics such as CPU and load.((("process (Elasticsearch JVM), statistics on")))  The
`OS` section describes it for the entire `OS`, while the `Process` section shows just
what the Elasticsearch JVM process is using.
These are obviously useful metrics, but are often being measured elsewhere in your
monitoring stack. Some stats include the following:
- CPU
- Load
- Memory usage
- Swap usage
- Open file descriptors
==== JVM Section
The `jvm` section contains some critical information about the JVM process that
is running Elasticsearch.((("JVM (Java Virtual Machine)", "statistics on")))  Most important, it contains garbage collection details,
which have a large impact on the stability of your Elasticsearch cluster.
[[garbage_collector_primer]]
.Garbage Collection Primer
**********************************
Before we describe the stats, it is useful to give a crash course in garbage
collection and its impact on Elasticsearch.((("garbage collection")))  If you are familar with garbage
collection in the JVM, feel free to skip down.
Java is a _garbage-collected_ language, which means that the programmer does
not manually manage memory allocation and deallocation.  The programmer simply
writes code, and the Java Virtual Machine (JVM) manages the process of allocating
memory as needed, and then later cleaning up that memory when no longer needed.
When memory is allocated to a JVM process, it is allocated in a big chunk called
the _heap_.  The JVM then breaks the heap into two groups, referred to as
_generations_:
Young (or Eden)::
    The space where newly instantiated objects are allocated. The
young generation space is often quite small, usually 100 MB&#x2013;500 MB.  The young-gen
also contains two _survivor_ spaces.
Old::
    The space where older objects are stored.  These objects are expected to be long-lived
and persist for a long time.  The old-gen is often much larger than then young-gen,
and Elasticsearch nodes can see old-gens as large as 30 GB.
When an object is instantiated, it is placed into young-gen.  When the young
generation space is full, a young-gen garbage collection (GC) is started.  Objects that are still
"alive" are moved into one of the survivor spaces, and "dead" objects are removed.
If an object has survived several young-gen GCs, it will be "tenured" into the
old generation.
A similar process happens in the old generation:  when the space becomes full, a
garbage collection is started and dead objects are removed.
Nothing comes for free, however.  Both the young- and old-generation garbage collectors
have phases that "stop the world."  During this time, the JVM literally halts
execution of the program so it can trace the object graph and collect dead
objects. During this stop-the-world phase, nothing happens.  Requests are not serviced,
pings are not responded to, shards are not relocated.  The world quite literally
stops.
This isn't a big deal for the young generation; its small size means GCs execute
quickly.  But the old-gen is quite a bit larger, and a slow GC here could mean
1s or even 15s of pausing--which is unacceptable for server software.