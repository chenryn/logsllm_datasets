LAB I consists of all trafﬁc during an afternoon workday hour,
recorded at the Laboratory’s 10 Gbps access link, totaling 89 GB
of data. The packet recording process reported a measurement drop
rate of 0.4% of the packets. LAB II consists of two hours of TCP-
only trafﬁc recorded two years earlier at the same facility, also dur-
ing the afternoon of a workday. The trace totals 117 GB; unfortu-
nately, no measurement drop information is available.
SC I consists of all trafﬁc seen at the border (but inside the
ﬁrewall, unlike LAB I and LAB II) of the supercomputing center,
recorded for 69 minutes during the afternoon of a workday. It totals
73 GB, with a reported measurement drop rate of 0.07%.
For evaluating Shunting, our primary interest is in the proportion
of trafﬁc that we can forward without needing further analysis. For
an IPS, this represents the fraction of the trafﬁc processed directly
4Apparently due to a transient glitch on a collection node.
University I
University II
University III
Lab I
Lab II
SC I
A Breakdown of the Traffic for the Various Traces
Diverted By Default
Analyzed HTTP
Analyzed SSH
Analyzed FTP
Analyzed Other TCP
Analyzed Other UDP/ICMP
Forwarded HTTP
Forwarded SSH
Forwarded FTP
Forwarded Other TCP
Forwarded Other UDP/ICMP
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
Figure 4: Breakdown of types of trafﬁc that require analysis vs. forwarding.
by the Shunt without involving bus-transfer overhead. Clearly, the
crucial question is to what degree we maintain a sound level of se-
curity analysis even in the presence of such ofﬂoad; thus, we strive
to formulate algorithms for deciding which trafﬁc to skip that gain
the largest ofﬂoad for the least loss of detection opportunities. We
frame our decisions in this regard in the remainder of the section.
In addition, we evaluate the behavior of cache sizes using the UNI-
VERSITY I trace.
7.1 Evaluating the Fraction Forwarded
We processed each trace with Bro running a number of analyz-
ers, including: generic TCP connection analysis; SSH; HTTP re-
quests and replies; dynamic protocol detection; SMTP; IRC (in-
cluding bot detection); POP; DNS; and scan detection. We also
evaluated on a per-connection basis the amount of trafﬁc analyzed
versus directly forwarded.
Table 2 summarizes the overall results. For the somewhat less
diverse laboratory and supercomputing environments, the ofﬂoad
gain is very large, 75–91% of the packets and bytes. Even for the
university environment, we see signiﬁcant gains along the lines of
50% of the packets and bytes.
Figure 4 breaks down the trafﬁc by bytes analyzed vs. bytes for-
warded, for various types of trafﬁc. The Shunt always diverts un-
classiﬁed trafﬁc (not present in any decision table) to the Analysis
Engine, which we show at the lefthand edge of the ﬁgure. Follow-
ing this portion of the trafﬁc we plot the makeup of analyzed trafﬁc
(diverted to the Analysis Engine because an analyzer needs to see
it) for different application protocols, and then the makeup of for-
warded trafﬁc that the Analysis Engine can skip processing due to
use of Shunting. (We mark the beginning of this last group with a
vertical line to help distinguish it from the preceding group.) Note
that the applications presented in the plot reﬂect not only trafﬁc
seen on the application’s well-known port, but also trafﬁc identi-
ﬁed using dynamic protocol detection.
Indeed, for the university traces we ﬁnd that the main beneﬁts
from Shunting come from the dynamic protocol detection analysis,
which often can examine just the beginning of a ﬂow and then for-
ward the remainder if it belongs to an application protocol that the
NIDS does not analyze. We also ﬁnd both the University traces and
SC I dominated by large-volume ﬂows.
In contrast, in LAB I’s trafﬁc mix, SSH dominates. Such an en-
vironment provides a near best-case for Shunting, since SSH gains
very large beneﬁt by skipping over large, unanalyzable encrypted
transfers. SC I also has a trafﬁc mix dominated by SSH and other
large, unanalyzable ﬁle transfers. (SC I is also the only environ-
ment where the FTP analyzer sees enough trafﬁc to signiﬁcantly
beneﬁt from Shunting.)
The ﬁgure demonstrates the central role that trafﬁc types play in
the effectiveness of Shunting: SSH can be almost completely for-
warded, while even with Shunting HTTP trafﬁc requires signiﬁcant
analysis.
We also see how, even at a single site, the mix of trafﬁc over
the course of a day can present signiﬁcantly different loads to a
Shunt-based IDS: comparing UNIVERSITY I (captured during the
workday) with UNIVERSITY III (in the middle of the night) we
see signiﬁcant differences, with UNIVERSITY III exhibiting a con-
siderably higher fraction of unanalyzable trafﬁc, and thus deriving
greater beneﬁt from Shunting.
Finally, we ﬁnd that Shunting is somewhat less effective at of-
ﬂoading packets compared to bytes. Since Shunting’s beneﬁts are
greatest for heavy-tailed ﬂows, it is natural to expect that we can
forward a greater fraction of bytes than packets.
7.2 Sizing the Connection Cache
A critical design parameter for the Shunt is sizing the connection
cache: it must be large enough to minimize the miss rate, but small
enough to limit the hardware cost.
To assess this tradeoff, we analyzed the UNIVERSITY I trace to
identify all of the forwarded packets, each of which corresponds to
a potential connection table entry. We then fed the resulting access
patterns into a custom-written cache simulator to evaluate the miss-
rate for different connection table cache sizes. (For this analysis,
we did not assume eviction of entries upon observing a TCP FIN or
RST control packet, an optimization that could further reduce the
miss rate.)
Figure 5 plots the miss rate (Y-axis, log-scaled) as we vary the
cache size (X-axis, log scaled) for different cache organizations and
eviction policies. We see that the 64K-entry cache used by our
hardware implementation provides ample head-room. A direct-
mapped cache would experience a 0.41% miss rate, while for a
Miss Rate for Connection Caches
Of Different Size and Configuration
Direct mapped
2-way associative
2-location associative
2-way associative with LRU
16-way associative with LRU
100.00%
10.00%
1.00%
0.10%
0.01%
16
64
256
1024
4096
16384
65536
262144
Connection Cache Size
Figure 5: Connection table cache miss rates UNIVERSITY I,
with varying cache sizes organizations, and eviction policies
(random or LRU).
2-way associative cache this drops to 0.11%. A 2-location associa-
tive cache, without any searching, further reduces the miss rate to
0.092%. Finally, a 2-way associative cache with LRU replacement
provides a 0.059% miss rate.
Although the associative cache with LRU replacement provides
a better miss rate than the 2-location associative cache with random
replacement, we prefer the location-associative cache because it is
easier to implement. To implement an LRU cache, the Shunt would
need to update connection entries upon receipt of packets, as for-
warded packets are never sent to the Analysis Engine (so it could
not track which entries are least-recently used).
Also of particular note is the relative effectiveness of even small
caches. A 2-location associative cache with just 4K entries provides
a miss rate of only 1.9%. If entries require 16 bytes, this suggests
that a connection cache of just 64 KB would be effective. Thus, a
Shunt built as an ASIC or using a programmable-ﬁrmware Ethernet
card could readily use on-chip memory for its tables.
8. FUTURE WORK
Our primary plans involve porting the Shunt implementation to
the 2.1 version of the NetFPGA board and advancing the integra-
tion with Bro to a level appropriate (and tested for) 24x7 opera-
tional use. The 2.1 NetFPGA board both ﬁxes the input FIFO
problem that causes lockup for high-data-rate ﬂows and also in-
cludes 64 MB of SDRAM, a larger FPGA, and greater availability
in terms of number of units we can obtain.
With the new board we will complete ﬁnal integration of the
Shunt into Bro and operationally deploy it in our network. Since the
designers of the NetFPGA 2.1 board plan to also make it commer-
cially available, we hope to deploy at third party sites to increase
our operational experience with Shunting, as well as provide en-
hancements to Bro for intra-enterprise operation.
In addition, since we have validated that small connection caches
sufﬁce, we are now investigating whether ﬁrmware-programmable
Ethernet cards could directly implement a Shunt.
9. CONCLUSIONS
We have developed a new model for packet processing, Shunt-
ing, which provides signiﬁcant beneﬁts for network intrusion pre-
vention in an environments for which an IPS can dynamically des-
ignate portions of trafﬁc stream as not requiring further analysis.
The architecture splits processing into a relatively simple, table-
driven hardware device that processes the entire trafﬁc stream in-
line, and a ﬂexible analyzer (the IPS proper) that can run separately,
communicating with the device either over a local bus or a dedi-
cated Gbps Ethernet link.
We argue that this architecture can realize a number of signif-
icant beneﬁts: (1) enabling what previously was a passive intru-
sion detection system to operate inline, gaining the power of intru-
sion prevention, as well as the opportunity to “normalize” trafﬁc
to remove ambiguities that attackers can exploit for evasion [11];
(2) signiﬁcantly ofﬂoading the IPS by providing a mechanism for
it to make ﬁne-grained, dynamic decisions regarding which traf-
ﬁc streams it analyzes, and (to a degree) which sub-elements of
stream it sees; (3) enabling large-scale, ﬁne-grained (per-address
or per-connection) blocking of hostile trafﬁc sources; and (4) pro-
viding a mechanism for an IPS to protect itself from overload if it
can identify sources that load in excessively.
We have already developed hardware capable of performing the
Shunting operations [27], demonstrating that we can keep the spe-
cialized cache within the Shunt hardware relatively small, with
64 KB caches producing viably low miss rates. In this work, as well
as framing the broader Shunting architecture, we have adapted the
Bro intrusion detection system to work with Shunting. We ﬁnd that
with a modest set of additions to its analysis, it can ofﬂoad 55–90%
of its trafﬁc load, as well as gaining the major beneﬁt of enabling
ﬁne-grained intrusion prevention.
10. ACKNOWLEDGMENTS
The high-level shunting architecture was conceived by Eli Dart
and Stephen Lau of the Lawrence Berkeley National Laboratory,
and prototyped by them at IEEE Supercomputing. Scott Campbell
of LBNL has also been instrumental in exploring ways to realize
shunting-like functionality using features offered by commercial
high-end routers.
Our thanks to Weidong Cui and Christian Kreibich for volun-
teering to have their daily network trafﬁc “live behind” our shunt-
ing software for testing purposes, and to Robin Sommer for helpful
comments on an earlier draft of this paper.
This research was made possible by a grant from the US De-
partment of Energy, Ofﬁce of Science, and by the National Science
Foundation under grants STI-0334088, NSF-0433702 and CNS-
0627320, for which we are grateful.
11. REFERENCES
[1] Burton Bloom. Space/time trade-offs in hash coding with
allowable errors. CACM, July 1970.
[2] C. Clark, W. Lee, D. Schimmel, D. Contis, M. Kone, and
A. Thomas. A hardware platform for network intrusion
detection and prevention. In Proceedings of The 3rd
Workshop on Network Processors and Applications (NP3),
2004.
[3] J. Cleary, S. Donnelly, I. Graham, A. McGregor, and
M. Pearson. Design principles for accurate passive
measurement. In Proceedings of the Passive and Active
Measurement Conference, 2000.
[4] J. Coppens, S. Van den Berghe, H. Bos, E. Markatos, F. De
Turck, A. Oslebo, and S. Ubik. Scampi - a scaleable and
programmable architecture for monitoring gigabit networks.
In Proceedings of E2EMON Workshop, September 2003.
[5] J. Coppens, E.P. Markatos, J. Novotny, M. Polychronakis,
V. Smotlacha, and S. Ubik. Scampi - a scaleable monitoring
platform for the internet. In Proceedings of the 2nd
International Workshop on Inter-Domain Performance and
Simulation (IPS 2004), March 2004.
[17] N. McKeown and G. Watson. Netfpga 2.0,
http://klamath.stanford.edu/nf2/.
[6] S. Crosby and D. Wallach. Denial of service via algorithmic
[18] R. Pang, V. Yegneswaran, P. Barford, V. Paxson, and
complexity attacks. In Proceedings of the 12th USENIX
Security Symposium, pages 29–44, Aug 2003.
[7] M. Crovella. Performance evaluation with heavy tailed
distributions. In JSSPP ’01: Revised Papers from the 7th
International Workshop on Job Scheduling Strategies for
Parallel Processing, pages 1–10, London, UK, 2001.
Springer-Verlag.
[8] M. Crovella and A. Bestavros. Self-Similarity in World Wide
Web Trafﬁc: Evidence and Possible Causes. In Proceedings
of SIGMETRICS’96: The ACM International Conference on
Measurement and Modeling of Computer Systems.,
Philadelphia, Pennsylvania, May 1996. Also, in Performance
evaluation review, May 1996, 24(1):160-169.
[9] L. Deri. Passively monitoring networks at gigabit speeds
using commodity hardware and open source software. In
Proceedings of the Passive and Active Measurement
Conference, 2003.
[10] H. Dreger, A. Feldmann, M. Mai, V. Paxson, and R. Sommer.
Dynamic application-layer protocol analysis for network
intrusion detection. In Proceedings of the USENIX Security
Symposium, 2006.
[11] M. Handley, C. Kreibich, and V. Paxson. Network intrusion
detection: Evasion, trafﬁc normalization, end end-to-end
protocol semantics. In Proceedings of the 9th USENIX
Security Symposium, 2001.
[12] G. Iannaccone, C. Diot, I. Graham, and N. McKeown.
Monitoring very high speed links. In IMW ’01: Proceedings
of the 1st ACM SIGCOMM Workshop on Internet
Measurement, pages 267–271, 2001.
[13] Intel. Intel(r) network infrastructure processors: Extending
intelligence in the network, 2005.
[14] S. Kornexl, V. Paxson, H. Dreger, A. Feldmann, and
R. Sommer. Building a time machine for efﬁcient recording
and retrieval of high-volume network trafﬁc. In Proceedings
of the ACM Internet Measurement Conference, 2005.
[15] C. Kruegel, F. Valeur, G. Vigna, and R. Kemmerer. Stateful
intrusion detection for high-speed networks. In Proceedings
of the IEEE Symposium on Security and Privacy, pages
285–294, May 2002.
[16] E. Markatos. Scampi detailed architecture design.
http://www.ist-
scampi.org/publications/deliverables/D1.3.pdf,
2005.
L. Peterson. Characteristics of internet background radiation.
In Proceedinges of ACM Internet Measurement Conference,
October 2004.
[19] V. Paxson. Empirically derived analytic models of wide-area
TCP connections. IEEE/ACM Transactions on Networking,
2(4):316–336, 1994.
[20] V. Paxson. Bro: A system for detecting network intruders in
real-time. Computer Networks (Amsterdam, Netherlands:
1999), 31(23–24):2435–2463, 1999.
[21] V. Paxson and S. Floyd. Wide area trafﬁc: The failure of
poisson modeling. IEEE/ACM Transactions on Networking,
3(3):226–244, 1995.
[22] T. H. Ptacek and T. N. Newsham. Insertion, evasion, and
denial of service: Eluding network intrusion detection.
Technical report, Secure Networks, Inc., Calgary, Alberta,
Canada, 1998.
[23] Andre Seznec. A case for two-way skewed-associative
caches. In ISCA, pages 169–178, 1993.
[24] Robin Sommer and Vern Paxson. Enhancing byte-level
network intrusion detection signatures with context. In ACM
CCS, 2003.
[25] Haoyu Song, Sarang Dharmapurikar, Jonathan Turner, and
John Lockwood. Fast hash table lookup using extended
Bloom ﬁlter: An aid to network processing. In SIGCOMM,
2005.
[26] M. Vallentin, R. Sommer, J. Lee, C. Leres, V. Paxson, and
B. Tierney. The NIDS cluster: Scalable, stateful network
intrusion detection on commodity hardware. In RAID 2007
(to appear).
[27] Nicholas Weaver, Vern Paxson, and Jos´e M. Gonz´alez. The
shunt: an fpga-based accelerator for network intrusion
prevention. In FPGA, pages 199–206, 2007.
[28] W. Wilinger, V. Paxson, and M. Taqqu. Self-similarity and
heavy tails: Structural modeling of network trafﬁc. In
R. Adler, R. Feldman, and M. Taqqu, editors, A Practical
Guide To Heavy Tails: Statistical Techniques and
Techniques. Birkhauser, 1998.
[29] W. Willinger, M. Taqqu, R. Sherman, and D. Wilson.
Self-similarity through high-variability: Statistical analysis
of Ethernet LAN trafﬁc at the source level. IEEE/ACM
Transactions on Networking, 5:71–86, 1997.