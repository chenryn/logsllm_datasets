### Data Collection and Analysis

**LAB I** encompasses all traffic recorded during an afternoon workday hour at the Laboratory’s 10 Gbps access link, totaling 89 GB of data. The packet recording process reported a drop rate of 0.4% of the packets. **LAB II** consists of two hours of TCP-only traffic recorded two years earlier at the same facility, also during the afternoon of a workday. This trace totals 117 GB; however, no information on measurement drop rates is available.

**SC I** includes all traffic observed at the border (but inside the firewall, unlike LAB I and LAB II) of the supercomputing center, recorded for 69 minutes during the afternoon of a workday. It totals 73 GB, with a reported measurement drop rate of 0.07%.

### Evaluation of Shunting

For evaluating Shunting, our primary interest lies in the proportion of traffic that can be forwarded without requiring further analysis. For an Intrusion Prevention System (IPS), this represents the fraction of traffic processed directly by the Shunt, bypassing bus-transfer overhead. The key question is how to maintain a high level of security analysis while offloading as much traffic as possible. We aim to develop algorithms that maximize offload with minimal loss of detection opportunities. Our decisions in this regard are detailed in the following sections.

Additionally, we evaluate the behavior of cache sizes using the **UNIVERSITY I** trace.

### Fraction of Forwarded Traffic

We processed each trace with Bro, running various analyzers, including: generic TCP connection analysis, SSH, HTTP requests and replies, dynamic protocol detection, SMTP, IRC (including bot detection), POP, DNS, and scan detection. We also evaluated the amount of traffic analyzed versus directly forwarded on a per-connection basis.

Table 2 summarizes the overall results. For the less diverse laboratory and supercomputing environments, the offload gain is substantial, ranging from 75–91% of the packets and bytes. Even in the university environment, significant gains are observed, around 50% of the packets and bytes.

Figure 4 breaks down the traffic by bytes analyzed versus bytes forwarded for different types of traffic. The Shunt always diverts unclassified traffic (not present in any decision table) to the Analysis Engine, shown at the left-hand edge of the figure. Following this, we plot the composition of analyzed traffic (diverted to the Analysis Engine because an analyzer needs to see it) for different application protocols, and then the composition of forwarded traffic that the Analysis Engine can skip processing due to Shunting. A vertical line marks the beginning of the forwarded traffic group to distinguish it from the preceding group. Note that the applications presented in the plot reflect not only traffic seen on the application’s well-known port but also traffic identified using dynamic protocol detection.

For the university traces, the main benefits from Shunting come from the dynamic protocol detection analysis, which often examines just the beginning of a flow and then forwards the remainder if it belongs to an application protocol that the NIDS does not analyze. Both the University traces and SC I are dominated by large-volume flows.

In contrast, **LAB I**'s traffic mix is dominated by SSH, which provides a near best-case scenario for Shunting, as SSH benefits significantly from skipping over large, unanalyzable encrypted transfers. **SC I** also has a traffic mix dominated by SSH and other large, unanalyzable file transfers. (SC I is the only environment where the FTP analyzer sees enough traffic to benefit significantly from Shunting.)

The figure highlights the central role that traffic types play in the effectiveness of Shunting: SSH can be almost completely forwarded, while even with Shunting, HTTP traffic requires significant analysis.

We also observe how, even at a single site, the mix of traffic over the course of a day can present significantly different loads to a Shunt-based IDS. Comparing **UNIVERSITY I** (captured during the workday) with **UNIVERSITY III** (in the middle of the night), we see significant differences, with **UNIVERSITY III** exhibiting a considerably higher fraction of unanalyzable traffic, thus deriving greater benefit from Shunting.

Finally, Shunting is somewhat less effective at offloading packets compared to bytes. Since Shunting's benefits are greatest for heavy-tailed flows, it is natural to expect that a greater fraction of bytes can be forwarded than packets.

### Sizing the Connection Cache

A critical design parameter for the Shunt is the sizing of the connection cache: it must be large enough to minimize the miss rate but small enough to limit hardware costs.

To assess this tradeoff, we analyzed the **UNIVERSITY I** trace to identify all forwarded packets, each corresponding to a potential connection table entry. We then fed the resulting access patterns into a custom-written cache simulator to evaluate the miss rate for different connection table cache sizes. (For this analysis, we did not assume eviction of entries upon observing a TCP FIN or RST control packet, an optimization that could further reduce the miss rate.)

Figure 5 plots the miss rate (Y-axis, log-scaled) as we vary the cache size (X-axis, log-scaled) for different cache organizations and eviction policies. We find that the 64K-entry cache used in our hardware implementation provides ample headroom. A direct-mapped cache would experience a 0.41% miss rate, while a 2-way associative cache reduces this to 0.11%. A 2-location associative cache, without any searching, further reduces the miss rate to 0.092%. Finally, a 2-way associative cache with LRU replacement provides a 0.059% miss rate.

Although the associative cache with LRU replacement provides a better miss rate than the 2-location associative cache with random replacement, we prefer the location-associative cache because it is easier to implement. To implement an LRU cache, the Shunt would need to update connection entries upon receipt of packets, as forwarded packets are never sent to the Analysis Engine (so it cannot track which entries are least-recently used).

Notably, even small caches are relatively effective. A 2-location associative cache with just 4K entries provides a miss rate of only 1.9%. If entries require 16 bytes, this suggests that a connection cache of just 64 KB would be effective. Thus, a Shunt built as an ASIC or using a programmable-firmware Ethernet card could readily use on-chip memory for its tables.

### Future Work

Our primary plans involve porting the Shunt implementation to the 2.1 version of the NetFPGA board and advancing the integration with Bro to a level appropriate for 24x7 operational use. The 2.1 NetFPGA board fixes the input FIFO problem that causes lockup for high-data-rate flows and includes 64 MB of SDRAM, a larger FPGA, and greater availability in terms of the number of units we can obtain.

With the new board, we will complete the final integration of the Shunt into Bro and operationally deploy it in our network. Since the designers of the NetFPGA 2.1 board plan to make it commercially available, we hope to deploy it at third-party sites to increase our operational experience with Shunting and provide enhancements to Bro for intra-enterprise operation.

Additionally, since we have validated that small connection caches suffice, we are now investigating whether firmware-programmable Ethernet cards could directly implement a Shunt.

### Conclusions

We have developed a new model for packet processing, Shunting, which provides significant benefits for network intrusion prevention in environments where an IPS can dynamically designate portions of the traffic stream as not requiring further analysis. The architecture splits processing into a relatively simple, table-driven hardware device that processes the entire traffic stream inline and a flexible analyzer (the IPS proper) that can run separately, communicating with the device either over a local bus or a dedicated Gbps Ethernet link.

This architecture can realize several significant benefits:
1. Enabling what was previously a passive intrusion detection system to operate inline, gaining the power of intrusion prevention and the opportunity to "normalize" traffic to remove ambiguities that attackers can exploit for evasion.
2. Significantly offloading the IPS by providing a mechanism for it to make fine-grained, dynamic decisions regarding which traffic streams it analyzes and, to a degree, which sub-elements of the stream it sees.
3. Enabling large-scale, fine-grained (per-address or per-connection) blocking of hostile traffic sources.
4. Providing a mechanism for an IPS to protect itself from overload if it can identify sources that load excessively.

We have already developed hardware capable of performing the Shunting operations, demonstrating that we can keep the specialized cache within the Shunt hardware relatively small, with 64 KB caches producing viably low miss rates. In this work, as well as framing the broader Shunting architecture, we have adapted the Bro intrusion detection system to work with Shunting. We find that with a modest set of additions to its analysis, it can offload 55–90% of its traffic load, as well as gain the major benefit of enabling fine-grained intrusion prevention.

### Acknowledgments

The high-level shunting architecture was conceived by Eli Dart and Stephen Lau of the Lawrence Berkeley National Laboratory and prototyped by them at IEEE Supercomputing. Scott Campbell of LBNL has also been instrumental in exploring ways to realize shunting-like functionality using features offered by commercial high-end routers.

We thank Weidong Cui and Christian Kreibich for volunteering to have their daily network traffic "live behind" our shunting software for testing purposes, and Robin Sommer for helpful comments on an earlier draft of this paper.

This research was made possible by a grant from the US Department of Energy, Office of Science, and by the National Science Foundation under grants STI-0334088, NSF-0433702, and CNS-0627320, for which we are grateful.

### References

[References are listed as provided, with no changes needed.]