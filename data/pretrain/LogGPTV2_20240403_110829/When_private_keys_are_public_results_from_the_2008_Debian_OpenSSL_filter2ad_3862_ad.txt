sibility of such attacks: If keys are not usually generated
soon after boot, the kernel will have a chance to gather ad-
ditional entropy from interrupt timings.
We can also remotely determine the processor architec-
ture. Approximately 80% of the HCs we observed were x86
32-bit and 20% were x86 64-bit with a trivial fraction being
32-bit big-endian machines.
7.2 Overall Upgrade Rate
Over the course of our survey 30% of the hosts (49%
of HCs) initially exhibiting a vulnerable key remained vul-
10More speciﬁcally, Internet Explorer running on Windows
2000, Windows XP, and Windows Server 2003 will negoti-
ate DHE with DSS certiﬁcates, which are not deployed for
Internet-wide HTTPS; Internet Explorer running on Win-
dows Vista and Windows Server 2008 will in addition ne-
gotiate ECDHE with RSA certiﬁcates, but the version of
OpenSSL deployed on Debian-derived distributions ships
without any elliptic curve support.
nerable on the last day; the rest either transitioned to a
non-vulnerable key or stopped responding. As shown — for
HCs — in Figure 1, this was a relatively gradual process,
with some notable discontinuities on days 2, 4, 5, 33, and 92.
Examination reveals that the transitions on days 5, 33, and
92 were dominated by Equifax, USERTRUST and Thawte-
issued certiﬁcates respectively.
In the case of day 33, the
hosts appear to be all operated by the same hosting provider;
for day 91, the hosts appear to all be servers at the same site
(www09, www10, etc.). There is no obvious pattern for days
2 and 4 and this may represent random chance, some as-yet
undetermined action by the CAs, or (since these events took
place about a week after the bug was disclosed) upgrading
spurred by publicity for the bug.
As is apparent from Figure 1, we see a quite diﬀerent pat-
tern of ﬁxing from that reported by either Ramos et al. [15]
or Rescorla [16].
Instead of a fast exponential decay fol-
lowed by almost no change, we see a very gentle curve with
substantial rates of ﬁxing out to 5–6 months. One natural
explanation for the curve’s shape is the signiﬁcant baseline
hazard of certiﬁcate expiration. We would expect this haz-
ard function to be constant because the certiﬁcate expiration
date is roughly randomly distributed. The roughly straight
“not vulnerable” line in Figure 4 provides some support for
this belief.
While it seems likely that the baseline certiﬁcate expi-
ration rate is a major factor in the long upgrade curve, we
suspect that other factors are relevant as well. As a heuristic
test, we removed all certiﬁcates whose expiration time was
less than 30 days from the upgrade time and plotted a new
survival curve. This still exhibits a substantial amount of
upgrading past 75 days. An alternative way to examine this
question is to compute the hazard ratio11: h(t)/h0(t) = eβ(t)
for certiﬁcates which are vulnerable versus those that are
not. As shown in Figure 7, which displays a spline ﬁt of
β(t) (the dashed lines indicate two standard error conﬁdence
bounds), the excess upgrading does not smoothly decrease,
as we expect from previous work, but rather increases up
to about day 45 and then decays afterwards. We note that
an eventual slowdown below the baseline rate is not unex-
pected: If certiﬁcates are changed ahead of schedule, then
we don’t see the corresponding scheduled replacements.
Figure 7: Log hazard ratio for vulnerable versus
non-vulnerable certiﬁcates
Another potential contributor to this much longer than
expected updating is the unique nature of this vulnerability.
With ordinary vulnerabilities, the perceived (and according
to Ramos, actual) risk of compromise is very high imme-
11It is conventional to work in log units here.
TimeBeta(t) for Initially.Vulnerable4.6254770941201401600.00.51.01.522diately after announcement and is a step function — either
your machine is compromised or it is not. By contrast, the
risk of having a weak RSA key due to passive attack is lin-
ear in the number of days it is used and, as indicated in
Section 4.2, upgrading the server certiﬁcate has only a very
small eﬀect on active attack because most clients do not
check CRLs. Thus, it may be rational for an administrator
to delay upgrading their system and certiﬁcates for longer
than they would for an ordinary vulnerability.
7.3 Factors Affecting the Upgrade Rate
In addition to the gross upgrade rate, an important ques-
tion to ask is: Are there factors that predict whether or when
a vulnerable certiﬁcate/host will be upgraded? To compare
the hazard functions, we used the popular Cox Proportional
Hazards model, which assumed that the hazard functions
h(t) for each combination of predictors are roughly constant
for all values of time and attempts to compute the hazard
ratio.12 The advantage of the Cox model is that it is non-
parametric, i.e., it does not require an underlying analytic
model for the hazard function and it gives a single numeric
result for the increased risk. The disadvantage is that it does
not give a meaningful numeric result when the hazard ratio
is not constant (the “proportional hazards assumption”).
We considered a large number of candidate predictors
and ﬁt the Cox proportional hazards model using the R
coxph function and the stepAIC [19] procedure for automatic
model selection. Due to the small size of the data set, we did
not consider interactions of predictors because many inter-
actions had zero counts. Chi-squared tests were consistent
with the proportional hazards assumption for all covariates
but key size and expiry during the study so we stratiﬁed on
those variables. Inspection shows some but not undue evi-
dence of time dependence of β(t) in the others. The results
are shown at the end of this section in Figure 12.
This procedure resulted in four predictors with potentially
signiﬁcant eﬀects: key size, expiry during the study, CA
type, and the number of hosts displaying a particular cert.
We discuss these predictors below.
Key Size. Figure 8 shows the rate of upgrading stratiﬁed
by key size. There are not enough 512- and 4096-bit keys
to draw any conclusions from, but 2048-bit keys are clearly
upgraded much faster than 1024-bit keys (p  100 total certiﬁcates
12More formally, for each predictor i, we ﬁt a coeﬃcient βi.
If X denotes the presence or absence of each predictor, then
we have h(t, X) = h0(t)e
i=0 βiXi .
Pp
Figure 8: Upgrading rate by key size
Figure 9: Upgrading rate by expiry time
Figure 10: Upgrading rate by CA type
in our sample) “small CA” (between 2 and 99 certiﬁcates),
“Other” (1 certiﬁcate) or “self-signed”. Qualitatively, hosts
with self-signed certiﬁcates are signiﬁcantly (p = .01) slower
to ﬁx than those with non-self-signed certiﬁcates. This isn’t
an unexpected result, as low-value and test servers often use
self-signed certiﬁcates, whereas serious commercial organi-
zations generally need to run certiﬁcates from third party
CAs. There is no statistically signiﬁcant diﬀerence between
the other categories, although given the small number of cer-
tiﬁcates in the “Other” category, this is potentially an issue
of insuﬃcient statistical power.
Certiﬁcate Instances. As discussed in Section 6.3, we
saw a number of instances of the same certiﬁcate appear-
ing on multiple IP addresses. Because large services often
0501001500.00.20.40.60.81.0Days since first measurementFraction of HCs AffectedKey.Size=512Key.Size=1024Key.Size=2048Key.Size=40960501001500.00.20.40.60.81.0Days since first measurementFraction of HCs AffectedEXP.During=FALSEEXP.During=TRUE0501001500.40.50.60.70.80.91.0Days since first measurementFraction of HCs AffectedCA.Type=Big.CACA.Type=OtherCA.Type=Self.SignedCA.Type=Small.CA23although we attempted to ﬁnd all possible factors aﬀect-
ing key generation, it is possible that there are other vari-
ations we missed. Users who use a non-default generation
method, either applying diﬀerent command-line options to
openssl genrsa or bypassing that program altogether, will
obtain keys diﬀerent to those we generated.
While the results of our survey are quite detailed, in retro-
spect it would have been useful to capture additional data.
In particular, had we used the -debug ﬂag to OpenSSL’s
s_client, we could have captured the entire handshake in-
cluding the ServerHello, which would give us the ServerRan-
dom. Fortuitously, however, because the SSL session identi-
ﬁer is randomly generated and is part of the output, we still
have a marker for the state of the server PRNG.
It would also have been nice to gather more information
about the server. For instance, we could have probed it to
determine which cipher suites it would accept. The current
data indicates only which suite was chosen, which is likely
to be just one of many it accepts. Those servers that agreed
to negotiate a DHE_RSA cipher suite with our survey client
would also negotiate a DHE_RSA with those browsers that
support it, which make up approximately one ﬁfth of the
market, as we discuss in Section 7.1. In addition, because
we hang up after the SSL handshake, we do not gather any
HTTP-level information about the server. Although server
version strings are not particularly reliable, this might be
useful to have in the future.
7.5 Extrapolating The Missing Data
As mentioned in Section 1, the Debian OpenSSL vulnera-
bility was announced 4–5 days prior to our ﬁrst survey. This
creates a potential source of error because we cannot directly
measure upgrading of hosts that occurred before our study
period. To try to get a handle on this, we looked at certiﬁ-
cates issued in the period between the announcement and
our ﬁrst survey. We were not able to obtain information for
all certiﬁcates, but VeriSign kindly provided us with the fate
of the predecessor for each VeriSign and RSA branded cer-
tiﬁcate in our data set with an issuance date within a day or
two of our study period (n = 366). Of those, less than 10%
were revoked and only 3 of the revoked keys were weak. We
have not checked the keys for the unrevoked certiﬁcates; it
is possible that some of them are weak and so this is an un-
derestimate. However, most of the changed certiﬁcates were
marked either as normally reissued or as a ﬁrst certiﬁcate
in the system. Thus we believe that, for VeriSign at least,
there was not a large amount of ﬁxing prior to our study. We
hope to expand the scope of this analysis in collaboration
with VeriSign and other CAs.
8. CONCLUSIONS
Much is known about how users and administrators re-
spond to software vulnerabilities, but comparatively little is
known about how they respond to cryptographic compro-
mise. Although anecdotal reports indicate that even when
keys are known to be compromised administrators will not
change them, there is no readily available empirical evi-
dence. The Debian OpenSSL vulnerability provided us with
a unique ability to remotely measure administrator response
to key compromise.
Using a survey of more than 50,000 SSL/TLS-enabled
Web servers, we have examined the rate of certiﬁcate up-
grading. We ﬁnd that unlike other vulnerabilities which
Figure 11: Upgrading rate by certiﬁcate instances
CA.TypeOther
CA.TypeSelf.Signed
CA.TypeSmall.CA
Mult.HostsTRUE
coef
-1.12
-0.89
0.14
0.57
exp(coef)
0.33
0.41
1.15
1.77
se(coef)
0.75
0.33
0.22
0.26
z
-1.49
-2.70
0.63
2.23
p
0.13
0.01
0.53
0.03
Figure 12: Predictors of upgrading rate
operate multiple servers, this can be used as an (imperfect)
proxy for size. As Figure 11 shows, certiﬁcates that appear
on multiple hosts seem to be ﬁxed faster. We have not in-
vestigated whether there is a dependency on the number of
copies beyond this.
Other Factors. Even if there are no between-group diﬀer-
ences between CAs based on size, it’s possible that there are
within-group diﬀerences, for instance due to CA customer
population or notiﬁcation strategies. Exploratory data anal-
ysis using both CA names and the binning strategy of Fig-
ure 5 doesn’t show any statistically signiﬁcant results and
given the large number of factors and the concomitant risk
of data mining combined with the messiness of the data
when viewed qualitatively, we cannot say with conﬁdence
that customers of one CA upgrade faster than others. We
also examined architecture, which did not have a signiﬁcant
eﬀect after controlling for other signiﬁcant factors.
7.4 Sources of Error
Our data is subject to a number of sources of error. Most
importantly, because we are identifying hosts by IP address
rather than hostname, the data in this survey is aﬀected by
renumbering, which may cause loss of contact with hosts or
host substitution. However, this can generally be detected
by examining the certiﬁcates, and as Figure 4 shows, we see
a fairly high degree of host stability.
The list of servers we surveyed may not be without bias.
It consists of the servers contacted, over approximately two-
month period, by users of UC San Diego’s campus network,
a large and diverse population of faculty, staﬀ, and stu-
dents including UCSD’s residential undergraduate colleges.
It may, nevertheless, display some US-centrism or be skewed
towards those sites interesting to members of an academic
community.
Additionally, it is possible that due to limitations in our
key-generation code, we are missing some vulnerable keys.
First, because we did not have access to a 64-bit big-endian
machine running Debian, we were not able to check that we
can correctly generate keys for that platform; we believe that
Debian is not widely deployed on these machines. Second,
0501001500.30.40.50.60.70.80.91.0Days since first measurementFraction of HCs AffectedOne Instance>1 Instance24have been studied and typically show a short, fast, ﬁxing