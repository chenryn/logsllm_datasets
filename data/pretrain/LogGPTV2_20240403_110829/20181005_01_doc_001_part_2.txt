Modify needed are optional, but at least one is
in each direction (e.g., NAT or ACL).
specified.
Ignore Leave this header as is.
Rules
Rules are the entities that perform actions on matching packets Table 2: Header Transposition actions
in the MAT model. Per original goal 3, rules allow the controller
to be as expressive as possible while minimizing fixed policy in Header NAT Encap Decap Encap+NAT
the dataplane. Rules are made up of two parts: a condition list, Push
Outer Push (SMAC,
specified via a list of conditions, and an action. Example condi- Ignore (SMAC, Pop
Ethernet DMAC)
tions and actions are listed in Figure 5. DMAC)
Rules can be organized into groups for purposes of doing Modify Push (SIP,
Outer IP Pop Push (SIP, DIP)
transactional update/replace operations, or to split a port into (SIP, DIP) DIP)
sub-interfaces (e.g., allow creation of independent policies for Not Push
GRE Pop Push (Key)
multiple Docker-style containers behind a single port). Present (Key)
Inner Not Modify
Packet Processor and Flow Compiler Ignore Modify (DMAC)
Ethernet Present (DMAC)
A primary innovation in VFPv2 was the introduction of a central
Not Modify (SIP,
packet processor. We took inspiration from a common design Inner IP Ignore Ignore
Present DIP)
in network ASIC pipelines e.g.,—parse the relevant metadata
from the packet and act on the metadata rather than on the TCP/ Modify Modify (SPt,
Ignore Ignore
packet, only touching the packet at the end of the pipeline once UDP (SPt, DPt) DPt)
all decisions have been made. We compile and store flows as we
Table 3: Example Header Transposition
see packets. Our just-in-time flow compiler includes a parser, an
action language, an engine for manipulating parsed metadata
Header Transpositions
and actions, and a flow cache.
Our action primitives, Header Transpositions (HTs), so called
because they change or shift fields throughout a packet, are a list
Unified FlowIDs
of paramaterizable header actions, one for each header. Actions
VFP’s packet processor begins with parsing. One each of an L2/
(defined in Table 2) are to Push a header (add it to the header
L3/L4 header (as defined in Table 1) form a header group, and
stack), Modify a header (change fields within a given header), Pop
the relevant fields of a header group form a single FlowID. The
a header (remove it from the header stack), or Ignore a header
tuple of all FlowIDs in a packet is a Unified FlowID (UFID)—the
(pass over it). Table 3 shows examples of a NAT HT used by
output of the parser.
Ananta, and encap/decap HTs used by VL2.
www.usenix.org FALL 2017 VOL. 42, NO. 3 9
CLOUD
VFP: A Virtual Switch Platform for Host SDN in the Public Cloud
resets, and more, either on a VM or aggregated on a cluster/host/
VNET basis. VFP also supports remote debugging and tracing
for rules and policies as part of its diagnostics suite.
Hardware Offloads and Performance
VFP has long used standard stateless offloads (VXLAN/
NVGRE encapsulation, QoS bandwidth caps, and reservations
for ports, etc.) to achieve line rate with SDN policy. But to enable
added goal 3 of full SR-IOV offload and host bypass, we built
logic to directly offload our unified flows. These are exact-match
flows representing each connection on the system, so they can
be implemented in hardware via a large hash table, typically in
inexpensive DRAM. In this model, the first packet of a new flow
Figure 6: VFP Unified Flow Table goes through software classification to determine the UF, which
is then offloaded.
VFP creates an action for a UFID match by composing HTs from
matched rules in each layer. For example, a packet passing the We’ve used this mechanism to enable SR-IOV in our datacenters
example Ananta NAT layer and the VL2 VNET encap layer may with VFP policy offload on custom FPGA-based SmartNICs
end up with the composite Encap+NAT transposition in Table 3. we’ve deployed on all new Azure servers. As a result we’ve seen
bidirectional 32Gbps+ VNICs with near-zero host CPU and
Unified Flow Tables and Caching <25μs end-to-end TCP latencies inside a VNET.
The intuition behind our flow compiler is that the action for a
UFID is relatively stable over the lifetime of a flow—so we can Experiences
cache the UFID with the resulting HT from the engine. The We have deployed 22 major releases of VFP since 2012. VFP
resulting flow table where the compiler caches UFs is called the runs on all Azure servers, powering millions of VMs, petabits
Unified Flow Table (UFT). per second of traffic, and providing load balancing for exabytes
of storage, in hundreds of datacenters in over 30 regions across
With the UFT, we segment our datapath into a fastpath and a
the world. In addition, we are releasing VFP publicly as part of
slowpath. On the first packet of a TCP flow, we take a slowpath,
Windows Server 2016 for on-premises workloads, as we have
running the transposition engine and matching at each layer
seen it meet all of the major goals listed above in production.
against rules. On subsequent packets, VFP takes a fastpath,
matching a unified flow via UFID and applying a transposition Over six years of developing and supporting VFP, we learned a
directly. This operation is independent of the layers or rules in number of lessons of value:
VFP.
◆◆ L4 flow caching is sufficient. We didn’t find a use for mul-
titiered flow caching such as OVS megaflows. The two main
Operationalizing VFP
reasons: being entirely in the kernel allowed us to have a faster
As a production cloud service, VFP’s design must take into
slowpath, and our use of a stateful NAT created an action for
account serviceability, monitoring, and diagnostics. During
every L4 flow and reduced the usefulness of ternary flow cach-
update, we first pause the datapath, then detach VFP from the
ing.
stack, uninstall VFP (which acts as a loadable kernel driver),
◆◆ Design for statefulness from day 1. The above point is an
install a new VFP, attach it to the stack, and restart the datapath.
example of a larger lesson: support for stateful connections as
This operation looks like a brief connectivity blip to VMs, while
a first-class primitive in a MAT is fundamental and must be
the NIC stays up. To keep stateful flows alive across updates, we
considered in every aspect of a MAT design. It should not be
support serialization and deserialization for all policy and state
bolted on later.
in VFP on a port. VFP also supports live migration of VMs. Dur-
ing the blackout time of the migration, the port state is serialized ◆◆ Layering is critical. Some of our policy could be implemented
out of the original host and deserialized on the new host. as a special case of OpenFlow tables with GOTOs chaining
them together, with separate inbound and outbound tables. But
VFP implements hundreds of performance counters and flow
we found that our controllers needed clear layering semantics
statistics, on per port, per layer, and per rule bases, as well as
or else they couldn’t reverse their policy correctly with respect
extensive flow statistics. This information is continuously
to other controllers.
uploaded to a central monitoring service, providing dashboards
on which we can monitor flow utilization, drops, connection
10 FALL 2017 VOL. 42, NO. 3 www.usenix.org
CLOUD
VFP: A Virtual Switch Platform for Host SDN in the Public Cloud
◆◆ GOTO considered harmful. Controllers will implement Conclusions and Future Work
policy in the simplest way needed to solve a problem, but that We introduced the Virtual Filtering Platform (VFP), our cloud
may not be compatible with future controllers adding policy. scale vswitch for host SDN policy in Microsoft Azure. We dis-
We needed to be vigilant in not only providing layering but cussed how our design achieved our dual goals of programmabil-
enforcing it. We see this layering enforcement not as a limita- ity and scalability. We discussed concerns around serviceability,
tion compared to OpenFlow’s GOTO table model but, instead, monitoring, and diagnostics in production environments, and
as the key feature that made multi-controller designs work for provided performance results, data, and lessons from real use.
multiple years running. Future areas of investigation include new hardware models of
◆◆ IaaS cannot handle downtime. We found that customer IaaS SDN and extending VFP’s offload language.
workloads cared deeply about uptime for each VM, not just their
service as a whole. We needed to design all updates to minimize References
downtime and provide guarantees for low blackout times. [1] D. Firestone, “VFP: A Virtual Switch Platform for Host
◆◆ Design for serviceability. Serialization is another design SDN in the Public Cloud,” in Proceedings of the 14th USENIX
point that turned out to pervade all of our logic—in order to Symposium on Networked Systems Design and Implementation
regularly update VFP without impact to VMs, we needed to (NSDI ’17): https://www.usenix.org/system/files/conference
consider serviceability in any new VFP feature or action type. /nsdi17/nsdi17-firestone.pdf.
◆◆ Decouple the wire protocol from the dataplane. We’ve [2] N. McKeown, T. Anderson, H. Balakrishnan, G. Parulkar,
seen enough controllers/agents implement wire protocols L. Peterson, J. Rexford, S. Shenker, J. Turner, “OpenFlow:
with different distributed systems models to support O(1M) Enabling Innovation in Campus Networks,” ACM SIGCOMM
scale that we believe our decision to separate VFP’s API from Computer Communication Review, vol. 38, no. 2 (April 2008):
any wire protocol was a critical choice for VFP’s success. For http://ccr.sigcomm.org/online/files/p69-v38n2n-mckeown.pdf.
example, bandwidth metering rules are pushed by a controller,
[3] B. Pfaff, J. Pettit, T. Koponen, E. Jackson, A. Zhou, J. Raja-
but VNET required a VL2-style directory system (and an agent
halme, J. Gross, A. Wang, J. Stringer, P. Shelar, K. Amidon, M.
that understands that policy comes from a different controller
Casado, “The Design and Implementation of Open vSwitch,”
than pulled mappings) to scale.
in Proceedings of the 12th USENIX Symposium on Networked
◆◆ Everything is an action. Modeling VL2-style encap/decap
Systems Design and Implementation (NSDI ’15): https://www
as actions rather than tunnel interfaces was a good choice. It
.usenix.org/system/files/conference/nsdi15/nsdi15-paper
enabled a single table lookup for all packets—no traversing a
-pfaff.pdf.
tunnel interface with tables before and after. The resulting HT
language combining encap/decap with header modification [4] A. Greenberg, J. R. Hamilton, N. Jain, S. Kandula, C. Kim,
enabled single-table hardware offload. P. Lahiri, D. A. Maltz, P. Patel, S. Sengupta, “VL2: A Scalable
and Flexible Data Center Network,” in Proceedings of the ACM
◆◆ Design for end-to-end monitoring. Determining network
Conference on Data Communication (SIGCOMM ’09), pp.
health of VMs despite not having direct access to them is a
51–62: https://www.researchgate.net/publication/234805283
challenge. We found many uses for in-band monitoring with
_VL2_A_Scalable_and_Flexible_Data_Center_Network.
packet injectors and auto-responders implemented as VFP rule
actions. We used these to build monitoring that traces the E2E [5] P. Patel, D. Bansal, L. Yuan, A. Murthy, A. Greenberg, D. A.
path from the VM-host boundary. For example, we implement- Maltz, R. Kern, H. Kumar, M. Zikos, H. Wu, C. Kim, N. Karri,
ed Pingmesh-like [6] monitoring for VL2 VNETs. “Ananta: Cloud Scale Load Balancing,” in Proceedings of the
◆◆ Commercial NIC hardware isn’t ideal for SDN. Despite ACM Conference on Data Communication (SIGCOMM ’13),
years of interest from NIC vendors about offloading SDN policy pp. 207–218: http://conferences.sigcomm.org/sigcomm/2013
with SR-IOV, we have seen no success cases of NIC ASIC /papers/sigcomm/p207.pdf.
vendors supporting our policy as a direct offload. Instead, large
[6] C. Guo, L. Yuan, D. Xiang, Y. Dang, R. Huang, D. Maltz, Z.
multicore NPUs are often used. We used custom FPGA-based
Liu, V. Wang, B. Pang, H. Chen, Z. Lin, V. Kurien, “Pingmesh:
hardware to ship SR-IOV in Azure, which we found was lower
A Large-Scale System for Data Center Network Latency Mea-
latency and more efficient.
surement and Analysis,” in Proceedings of the ACM Confer-
ence on Data Communication (SIGCOMM ’15): https://www
.microsoft.com/en-us/research/wp-content/uploads/2016/11
/pingmesh_sigcomm2015.pdf.
www.usenix.org FALL 2017 VOL. 42, NO. 3 11