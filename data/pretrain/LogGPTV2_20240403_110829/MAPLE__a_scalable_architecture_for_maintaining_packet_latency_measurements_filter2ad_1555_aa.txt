title:MAPLE: a scalable architecture for maintaining packet latency measurements
author:Myungjin Lee and
Nick G. Duffield and
Ramana Rao Kompella
MAPLE: A Scalable Architecture for Maintaining Packet
Latency Measurements
Myungjin Lee†, Nick Dufﬁeld‡, Ramana Rao Kompella†
†Purdue University, ‡AT&T Labs–Research
ABSTRACT
Latency has become an important metric for network moni-
toring since the emergence of new latency-sensitive applica-
tions (e.g., algorithmic trading and high-performance com-
puting). To satisfy the need, researchers have proposed
new architectures such as LDA and RLI that can provide
ﬁne-grained latency measurements. However, these archi-
tectures are fundamentally ossiﬁed in their design as they
are designed to provide only a speciﬁc pre-conﬁgured aggre-
gate measurement—either average latency across all packets
(LDA) or per-ﬂow latency measurements (RLI). Network op-
erators, however, need latency measurements at both ﬁner
(e.g., packet) as well as ﬂexible (e.g., ﬂow subsets) levels of
granularity. To bridge this gap, we propose an architecture
called MAPLE that essentially stores packet-level latencies
in routers and allows network operators to query the latency
of arbitrary traﬃc sub-populations. MAPLE is built using
scalable data structures with small storage needs (uses only
12.8 bits/packet), and uses a novel mechanism to reduce the
query bandwidth signiﬁcantly (by a factor of 17 compared to
the naive method of sending packet queries individually).
Categories and Subject Descriptors
C.2.3 [Computer Communication Networks]: Network
management
General Terms
Measurement, algorithms
Keywords
Latency, bloom ﬁlter, approximation
1.
INTRODUCTION
For the longest time, networking engineers and researchers
have focused mainly on obtaining high end-to-end through-
put in IP networks. In recent years, however, latency has
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not  made  or  distributed  for  profit  or  commercial  advantage  and  that 
copies  bear  this  notice  and  the  full  citation  on  the  first  page.  To  copy 
otherwise,  or  republish,  to  post  on  servers  or  to  redistribute  to  lists, 
requires prior specific permission and/or a fee. 
IMC’12, November 14–16, 2012, Boston, Massachusetts, USA. 
Copyright 2012 ACM  978-1-4503-1705-4/12/11...$15.00. 
evolved into a metric that is as important as throughput in
IP networks. While low latency is a desirable property for
any network-based application, this obsession towards low
end-to-end latency stems from the stringent requirements of
many new kinds of datacenter, cloud, and wide-area appli-
cations that have become popular in the recent times. For
instance, several cloud applications (e.g., Salesforce, Google
App Engine, modern Web services) involve complex back-
end processing, such as accessing storage, SQL database
transactions, etc. After removing time for computation and
wide-area RTTs, the budget for datacenter network accesses
is signiﬁcantly reduced. Similar requirement exists for par-
tition/aggregate type workloads found in search and social
collaboration applications, where jobs that do not ﬁnish
within a certain time are typically cancelled thus aﬀecting
the overall result, and in some cases, lost revenue [7]. Even
more stringent latency requirements, in the order of 10s of
microseconds, exist for high-performance computing (HPC)
applications [1] and ﬁnancial trading applications [31].
Network operators managing such latency-sensitive ap-
plications need sophisticated tools for high-ﬁdelity latency
measurements at various places in the network that will help
them identify root causes of SLA violations, determine of-
fending applications that may hurt the performance of oth-
ers, perform traﬃc engineering, and so on. In light of the
importance of these measurements, there has been some re-
cent research on developing measurement mechanisms such
as the lossy diﬀerence aggregator (LDA) [26] and reference
latency interpolation (RLI) [27]. A key limitation of these
techniques is that they only obtain latency measurements at
the granularity of a ﬁxed pre-conﬁgured aggregate (across all
packets in LDA, and per-ﬂow latencies in RLI). By making
the granularity of the aggregates for latency measurements
part of the architecture, these prior architectures are quite
ossiﬁed, lacking ﬂexibility to obtain arbitrary latency mea-
surements than what they are already pre-programmed to
achieve. What network operators need instead is a holis-
tic architecture that provides the ability to obtain arbitrary
latency measurements from switches. Such an architecture
would help operators with powerful tools to help debug and
manage low-latency applications in their networks. Design-
ing such an architecture is the main objective of this paper.
In our quest to obtain arbitrary latency measurements
from switches, we ask, “What is the ﬁnest granularity of la-
tency measurements a network operator may be interested in
?” LDA and RLI implicitly assumed that aggregate or ﬂow-
level granularity of measurements is what operators may
care about. In this paper, we argue instead that there are
101many compelling scenarios where ﬁner-granularity measure-
ments may be important.
For example, for diagnosing client delays in online ser-
vices, it may be critical to know whether a DNS query (that
is typically a single packet) got delayed, or whether a back-
end transaction got delayed in the network, or whether there
were processing delays. As another canonical example, in a
real-time bidding (RTB) ecosystem for online ads, bidders
(e.g., agency trading desks) must respond to bid requests
within a tight deadline (say, less than 120 ms) by typically
shipping their response message with bidding price in a sin-
gle packet [4]. Failing to meet the deadline makes the bid-
ders miss an opportunity of exposing commercials of their
customers (i.e., advertisers) to the public. As a result, the
customers can experience bad practice of marketing, poten-
tially causing signiﬁcant revenue loss to the customers. Sim-
ilarly, for ﬁnancial trading applications, one may care about
the delay of a single stock trade (that may be carried in one
packet). For HPC applications built on message passing li-
braries (e.g., MPI), latency of even a single message may be
quite important. In addition, one may wish to focus on la-
tencies for a subset of packets that belong to a ﬂow, perhaps
to hone in on the ones that exhibited abnormal latency, or
to track the latency time-series of the ﬂow. Thus, clearly, in
order to satisfy these requirements, we need a more ﬂexible
architecture than the one-size-ﬁts-all approach of existing
solutions such as LDA (aggregate) or RLI (per-ﬂow).
Since the ﬁnest granularity of latency measurements is
on a per-packet basis, we start with an architecture that
achieves these measurements in a scalable fashion. Then,
any other forms of aggregation (per-ﬂow, per-preﬁx), that
may be of importance to network operators, are easily com-
posable from these packet-level measurements. Such an ar-
chitecture essentially decouples the collection of measure-
ments (at the granularity of a packet), and aggregation (across
arbitrary subpopulations) during query time. This key intu-
ition forms the basis for our proposed architecture MAPLE.
MAPLE essentially consists of two main components—a
scalable packet latency store (PLS) and a query engine—
at each router. PLS stores the latencies of all packets that
appear at the router.
In high speed networks, storing all
the packets and their associated latencies is going to be ex-
pensive; hence we store latencies only for a small amount
of time (e.g., 1s) in high-speed SRAM, and rely on ﬂush-
ing them periodically to a higher capacity data store. Since
storing the entire packet and delay requires high storage (in
terms of bits/packet), we propose a novel approach that ﬁrst
clusters packets and associates a delay value for each clus-
ter, and then, uses a novel hardware data structure called
shared-vector Bloom ﬁlter (SVBF) to signiﬁcantly reduce
the memory requirement. SVBF makes the architecture
technologically feasible in high-speed switches where SRAM
is a very scarce and precious resource. We show how the en-
tire PLS can enable an eﬃcient streaming implementation
in hardware that can keep up with line rates.
The second component of MAPLE, the query engine, es-
sentially allows end-hosts or a centralized entity to initiate
a query for the packet. These queries need to be within a
particular timeframe of the original packet; otherwise, the
store in the router may not have any history of the packet.
By constructing queries across arbitrary packets, end-hosts
can easily obtain per-packet latencies for all (or a sample) of
packets within a given subpopulation, using which they can
compose the aggregate latency measurements for that ﬂow.
We also consider mechanisms to reduce query bandwidth by
providing the ability to perform range queries.
Thus, the main contributions in this paper are as follows:
1) We propose MAPLE for maintaining per-packet la-
tency measurements in a scalable fashion (§2). Our
architecture allows network operators to obtain any ag-
gregate of measurements thus subsuming the function-
ality of existing architectures, while providing newer
and more powerful capabilities.
2) We propose novel mechanisms that use streaming algo-
rithms for clustering packet delays, and storing them
compactly using a novel data structure called SVBF
that is much more storage-eﬃcient (requires only 12.8
bits/packet) than a variant of regular hash tables (that
may require 147 bits/packet) and also minimizes mem-
ory accesses for inserts and lookups (§3). We also pro-
pose a range query mechanism to reduce the amount
of query bandwidth required (§4).
3) We built a software prototype of our architecture. In
our evaluations (§5), we found that SVBF achieves al-
most 6× lower per-packet latency estimate error com-
pared to prior data structures for comparable storage.
We also found that the range query achieves signiﬁcant
reduction in query bandwidth (almost 17×) compared
to naive packet query.
2. MAPLE ARCHITECTURE
In this section, we outline a ﬂexible architecture for ob-
taining high-ﬁdelity latency measurements in the network.
Before we describe the architecture, we ﬁrst state our mea-
surement goals followed by a brief discussion on why previ-
ous solutions cannot satisfy these goals.
2.1 Measurement goals
Our goal is to enable a high-ﬁdelity latency measurement
architecture that satisﬁes the following requirements:
• R1) Per-packet latency measurements. The architec-
ture should allow network operators to obtain latency
information about a single packet at various routers in
the network.
• R2) Measurements across arbitrary aggregates. It should
enable operators to easily compute measurements across
arbitrary aggregates (e.g., per-preﬁx, ﬂows, sub-ﬂows).
• R3) Measurements across arbitrary locations. We need
support for latency measurements both within and across
routers by allowing network operators complete free-
dom to selectively turn on interfaces between which
they need measurements.
Such an architecture will provide detailed latency infor-
mation that will help network operators to debug their net-
works and satisfy the demands of modern latency-sensitive
network applications.
2.2 Limitations of previous solutions
We consider mainly passive measurement solutions since,
as pointed out by prior work [26, 27], it is diﬃcult to esti-
mate packet-level latencies by injecting active probes. As
102secondary storage (e.g., DRAM, SSD), where it can be held
for a longer time. We discuss PLS in §3.
Second, to satisfy requirement R2, it essentially contains
a generic query engine that allows clients to query switches
about the latency of a packet (using its hash, for instance)
that has traversed that particular switch. These queries need
to be within the storage timeframe (either high-speed or oﬀ-
chip storage), otherwise the switch may lose the packet la-
tency record. We expect that queries are mostly going to be
only for speciﬁc applications or customers which are experi-
encing trouble. Instead of a per-packet query, we also pro-
vide a range query mechanism to reduce query bandwidth.
By querying latencies of speciﬁc packets that form an ag-
gregate, the client can obtain latency measurements across
arbitrary sub-populations. More details are discussed in §4.
Finally, it is simpler to satisfy requirement R3, if we en-
able packet headers to carry timestamps (Figure 1). While
we understand that it may be complicated in the short-term
to make header changes (as prior work [27, 26] pointed out),
we believe this is a cleaner longer-term option that switch
vendors are already considering.
In our discussions with
prominent switch vendors, they mention that implementing
a timestamp within the switch is not a problem, and, in
some cases, such as Fulcrum [3], they already have an in-
ternal timestamp within the switch. Thus, the assumptions
in LDA (FIFO ordering) and RLI (temporal delay correla-
tion) are no longer required in our architecture thus enabling
a more ﬂexible architecture. However, if timestamping is
not feasible, for restricted settings, such as packets within
a router, or when there is a series of routers in a FIFO or-
der, we can still use temporal delay correlation assumption
made in prior work [27] and obtain approximate delays on
a per-packet basis. In that sense, our architecture builds on
any scheme to obtain one-way delay for each packet.
Note that in all cases, we assume high precision time syn-
chronization (similar to LDA and RLI) between the two
measurement points, which has become feasible in modern
times due to the increasing adoption of IEEE 1588 [2].
3. PACKET LATENCY STORE (PLS)
Simply put, the goal of PLS is to store a packet and its
associated latency value in a scalable fashion.
In normal
settings, this goal would be relatively straightforward to ac-
complish using a simple linear-probing or linked-list-based
hashtable implementation [15]. Unfortunately, hashtables
are not eﬃcient in their storage since they typically require