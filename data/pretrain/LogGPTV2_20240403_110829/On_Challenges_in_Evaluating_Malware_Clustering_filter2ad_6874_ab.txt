very different results on two malware datasets, one with a highly biased clustering and
one with a more even clustering. While this is not the only source of variation in the
datasets, and so the different results cannot be attributed solely to differences in cluster
size distributions, we believe that the cluster size distribution is a factor that must be
taken into account when reporting malware clustering results.
3 A Potential Hazard of Anti-virus Voting
As discussed in Section 2, a common practice to produce the ground-truth reference
clustering D for evaluating malware clustering algorithms is to use existing anti-virus
tools to label the malware instances and to restrict attention to malware instances M
on which multiple anti-virus tools agree. The starting point of our study is one such
ground-truth dataset, here denoted BCHKK-data, that was used by Bayer et al. for eval-
uating their malware clustering technique [6]. Using this dataset, their algorithm, here
denoted BCHKK-algo, yielded a very good precision and recall (of 0.984 and 0.930,
respectively). BCHKK-data consists of 2, 658 malware instances, which is a subset of
14, 212 malware instances contributed between October 27, 2007 and January 31, 2008
by a number of security organizations and individuals, spanning a wide range of sources
(such as web infections, honeypots, botnet monitoring, and other malware analysis ser-
vices). Bayer et al. ran six different anti-virus programs on these 14, 212 instances,
and a subset of 2, 658 instances on which results from the majority of these anti-virus
programs agree were chosen to form BCHKK-data for evaluation of their clustering
technique BCHKK-algo. Bayer et al. explained that such a subset was chosen because
On Challenges in Evaluating Malware Clustering
243
they are the instances on which ground truth can be obtained (due to agreement by a
majority of the anti-virus programs they used).
This seems to be a natural way to pick M for evaluation, as they are the only ones
for which the ground-truth clustering (i.e., D) could be obtained with good conﬁdence.
However, this also raises the possibility that the instances on which multiple anti-virus
tools agree are just the malware instances that are relatively easy to cluster, while the
difﬁcult-to-cluster instances are ﬁltered out of M . If this were the case, then this could
contribute to the high precision and recall observed for the BCHKK-data dataset, in
particular.
Unfortunately, we are unaware of any accepted methodology for testing this possi-
bility directly. So, we instead turn to another class of clustering tools derived without
attention to malware clustering, in order to see if they are able to cluster the malware
instances in BCHKK-data equally well. Speciﬁcally, we apply plagiarism detectors to
the BCHKK-data to see if they can obtain good precision and recall.
3.1 Plagiarism Detectors
Plagiarism detection is the process of detecting that portions within a work are not orig-
inal to the author of that work. One of the most common uses of software plagiarism
detection is to detect plagiarism in student submissions in programming classes (e.g.,
Moss [4], Plaque [24], and YAP [26]). Software plagiarism detection and malware clus-
tering are related to one another in that they both attempt to detect some degree of sim-
ilarity in software programs among a large number of instances. However, due to the
uniqueness of malware samples compared to software programs in general (e.g., in us-
ing privileged system resources) and due to the degree of obfuscation typically applied
to malware instances, we did not expect plagiarism detectors to produce good results
when clustering malware samples.
Here we focus on three plagiarism detectors that monitor dynamic executions of
a program. We do not include those applying static analysis techniques as they are
obviously not suitable for analyzing (potentially packed) malware instances.
– APISeq: This detector, proposed by Tamada et al. [21], computes the similarity
of the sequences of API calls produced by two programs to determine if one is
plagiarized from the other. Similarity is measured by using string matching tools
such as diff and CCFinder [12].
– SYS3Gram: In this detector, due to Wang et al. [23], short sequences (speciﬁcally,
triples) of system calls are used as “birthmarks” of programs. Similarity is mea-
sured as the Jacaard similarity of the birthmarks of the programs being compared,
i.e., as the ratio between the sizes of two sets: (i) the intersection of the birthmarks
from the two programs, and (ii) the union of the birthmarks from the two programs.
– API3Gram: We use the same idea as in SYS3Gram and apply it to API calls to
obtain this plagiarism detector.
We emphasize that the features on which these algorithms detect plagiarism are distinct
from those employed by BCHKK-algo. Generally, the features adopted in BCHKK-algo
are the operating system objects accessed by a malware instance, the operations that
244
P. Li et al.
were performed on the objects, and data ﬂows between accesses to objects. In contrast,
the features utilized by the plagiarism detectors we adopted here are system/API call
sequences (without speciﬁed argument values).
3.2 Results
We implemented these three plagiarism detectors by following the descriptions in the
corresponding papers and then applied the detectors to BCHKK-data (instances used
by Bayer et al. [6] on which multiple anti-virus tools agree). More speciﬁcally, each de-
tection technique produced a distance matrix; we then used single-linkage hierarchical
clustering, as is used by BCHKK-algo, to build a clustering C, stopping the hierarchical
clustering at the point that maximizes the p-value of a chi-squared test between the dis-
tribution of sizes of the clusters in C and the cluster-size distribution that BCHKK-algo
induced on BCHKK-data.1 We then evaluated the resulting clustering C by calculating
the precision and recall with respect to a reference clustering D that is one of
– AV: clustering produced by multiple anti-virus tools, i.e., D in the evaluation clus-
– BCHKK-algo: clustering produced by the technique of Bayer et al., i.e., C in the
tering (“ground truth”) in Bayer et al.’s paper [6];
evaluation in Bayer et al.’s paper [6].
To make a fair comparison, the three plagiarism detectors and BCHKK-algo obtain
system information (e.g., API call, system call, and system object information) from
the same dynamic traces produced by Anubis [7]. Results of the precision and recall are
shown in Table 1.
Table 1. Applying plagiarism detectors and malware clustering on BCHKK-data
C
BCHKK-algo
APISeq
API3Gram
SYS3Gram
APISeq
API3Gram
SYS3Gram
D
AV
BCHKK-algo
prec(C,D) recall(C,D) F-measure(C,D)
0.984
0.965
0.978
0.982
0.988
0.989
0.988
0.930
0.922
0.927
0.938
0.939
0.941
0.938
0.956
0.943
0.952
0.960
0.963
0.964
0.963
One set of experiments, shown where D is set to the clustering results of BCHKK-
algo in Table 1, compares these plagiarism detectors with BCHKK-algo directly. The
high (especially) precisions and recalls show that the clusterings produced by these
plagiarism detectors are very similar to that produced by BCHKK-algo. A second set of
1 More speciﬁcally, this chi-squared test was performed between the cluster-size distribution of
C and a parameterized distribution that best ﬁt the cluster-size distribution that BCHKK-algo
induced on BCHKK-data. The parameterized distribution was Weibull with shape parameter
k = 0.4492 and scale parameter λ = 5.1084 (p-value = 0.8763).
On Challenges in Evaluating Malware Clustering
245
experiments, shown where D is set to AV, compares the precisions and recalls of all four
techniques to the “ground truth” clustering of BCHKK-data. It is perhaps surprising that
SYS3Gram performed as well as it did, since a system-call-based malware clustering
algorithm [13] tested by Bayer et al. performed relatively poorly; the difference may
arise because the tested clustering algorithm employs system-call arguments, whereas
SYS3Gram does not (and so is immune to their manipulation). That issue aside, we
believe that the high precisions and recalls reported in Table 1 provide support for the
conjecture that the malware instances in the BCHKK-data dataset are likely relatively
simple ones to cluster, since plagiarism detectors, which are designed without attention
to the speciﬁc challenges presented by malware, also perform very well on them.
4 Replicating Our Analysis on a New Dataset
Emboldened by the results in Section 3, we decided to attempt to replicate the anal-
ysis of the previous section on a new dataset. Our goal was to see if another analysis
would also support the notion that selecting malware instances for which ground-truth
evidence is inferred by “voting” by anti-virus tools yields a ground-truth dataset that
all the tools we considered (BCHKK-algo and plagiarism detectors alike) could cluster
well.
4.1 The New Dataset and BCHKK-algo Clustering
To obtain another dataset, we randomly chose 5, 121 instances from the collection of
malware instances from VX heavens [3]. We selected the number of instances to be
roughly twice the 2, 568 instances in BCHKK-data. We submitted this set of instances
to Bayer et al., who kindly processed these instances using Anubis and then applied
BCHKK-algo to the resulting execution traces and returned to us the corresponding
distance matrix. This distance matrix covered 4, 234 of the 5, 121 samples; Anubis had
presumably failed to produce meaningful execution traces for the remainder.
In order to apply the plagiarism detectors implemented in Section 3 to this data, we
needed to obtain the information that each of those techniques requires, speciﬁcally the
sequences of system calls and API calls for each malware instance. As mentioned in
Section 3, we obtained this information for the BCHKK-data dataset via Anubis; more
speciﬁcally, it was already available in the BCHKK-data data ﬁles that those authors
provided to us. After submitting this new dataset to the Anubis web interface, however,
we realized that this information is not kept in the Anubis output by default. Given that
obtaining it would then require additional imposition on the Anubis operators to cus-
tomize its output and then re-submitting the dataset to obtain analysis results (a lengthy
process), we decided to seek out a method of extracting this information locally. For
this purpose, we turned to an alternative tool that we could employ locally to gener-
ate API call traces from the malware instances, namely CWSandbox [25]. CWSandbox
successfully processed (generated non-empty API call traces) for 4, 468 of the 5, 121
samples, including 3, 841 of the 4, 234 for which we had results for the BCHKK-algo
algorithm.
246
P. Li et al.
We then scanned each of these 3, 841 instances with four anti-virus programs (Ac-
tivescan 2.0, Nod32 update 4956, Avira 7.10.06.140 and Kaspersky 6.0.2.690). Ana-
lyzing the results from these anti-virus programs, we ﬁnally obtained 1, 114 malware
instances for which the four anti-virus programs reported the same family for each;
we denote these 1, 114 as VXH-data in the remainder of this paper. More speciﬁcally,
each instance is given a label (e.g, Win32.Acidoor.b, BDS/Acidoor.B) when scanned by
an anti-virus program. The family name is the generalized label extracted from the in-
stance label based on the portion that is intended to be human-readable (e.g., the labels
listed would be in the “Acidoor” family). We deﬁned a reference clustering D for this
dataset so that two instances are in the same cluster D ∈ D if and only if all of the four
anti-virus programs concurred that these instances are in the same family.2 Our method
for assembling the reference clustering for VXH-data is similar to that used to obtain
the reference clustering of BCHKK-data [6], but is more conservative.3
We obtained the BCHKK-algo clustering of VXH-data by applying single linkage
hierarchical clustering to the subset of the distance matrix provided by Bayer et al.
corresponding to these instances. In this clustering step, we used the same parameters
as in the original paper [6]. To ensure a fair comparison with other alternatives, we
conﬁrmed that this clustering offered the best F-measure value relative to the reference
VXH-data clustering based on the anti-virus classiﬁcations, in comparison to stopping
the hierarchical clustering at selected points sooner or later.
4.2 Validation on BCHKK-Data
As discussed above, we resorted to a new tool, CWSandbox (vs. Anubis), to extract
API call sequences for VXH-data. In order to gain conﬁdence that this change would
not greatly inﬂuence our results, we ﬁrst performed a validation test, speciﬁcally to
see whether our plagiarism detectors would perform comparably on the BCHKK-data
dataset when processed using CWSandbox. In the validation test, we submitted
BCHKK-data to CWSandbox to obtain execution traces for each instance. Out of the
2, 658 instances in BCHKK-data, CWSandbox successfully produced traces for 2, 099
of them. Comparing the API3Gram and APISeq clusterings on these 2, 099 samples,
ﬁrst with reference clustering AV and then with the clustering produced using BCHKK-
algo (which, again, uses Anubis) as reference, yields the results in Table 2. Note that
due to the elimination of some instances, the reference clusterings have fewer clus-
ters than before (e.g., AV now has 68 families instead of 84 originally). Also note that
SYS3Gram results are missing in Table 2 since CWSandbox does not provide system
call information. However, high F-measure values for the other comparisons suggest
that our plagiarism detectors still work reasonably well using CWSandbox outputs.
2 The VX heavens labels for malware instances are the same as Kaspersky’s, suggesting this is
the anti-virus engine they used to label.
3 The method by which Bayer et al. selected BCHKK-data and produced a reference cluster-
ing for it was only sketched in their paper [6], but their clariﬁcations enabled us to perform a
comparable data selection and reference clustering process, starting from the 3, 841 instances
from VX heavens successfully processed by both CWSandbox and the BCHKK-algo algo-
rithm (based on Anubis). This process retained a superset of the 1,114 instances in VXH-data
and produced a clustering of which every cluster of VXH-data is a subset of a unique cluster.
On Challenges in Evaluating Malware Clustering
247
Table 2. Applying plagiarism detectors and malware clustering on BCHKK-data. API3Gram and
APISeq are based on CWSandbox traces.
C
API3Gram
APISeq
API3Gram
APISeq
D
AV
BCHKK-algo
prec(C,D) recall(C,D) F-measure(C,D)
0.948
0.958
0.921
0.937
0.918
0.934
0.931
0.939
0.933
0.946
0.926
0.938
4.3 Results on VXH-Data
In Section 4.1 we described how we assembled the VXH-data dataset and applied
BCHKK-algo and the anti-virus tools to cluster it. We now compare the results of the
four clustering techniques run on VXH-data: AV from the anti-virus tools, API3Gram
(based on CWSandbox), APISeq (based on CWSandbox) and BCHKK-algo (based on