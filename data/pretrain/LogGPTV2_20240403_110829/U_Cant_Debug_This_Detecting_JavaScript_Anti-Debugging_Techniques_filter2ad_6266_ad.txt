(cid:32)
not as easy to detect as the basic techniques. In particular,
we can not identify them by just monitoring a few speciﬁc
function calls and property accesses. While all SADTs rely on
timing information, they do not necessarily need access to the
Date or performance objects, as they could also get a clock
from a remote source, e.g., via WebSockets. Therefore, we
need a more general approach to reliably detect sophisticated
techniques in the wild. In the following, we will describe how
we address this challenge and then report on our ﬁndings.
6.1 Study II – Methodology
While these SADTs can differ in how they are implemented,
they still have something in common: They try to ﬁgure out
whether they are currently analyzed or not and then behave
accordingly. Therefore, code execution must diverge from the
default, benign case as soon as the analysis is detected. If
we somehow could monitor the executed code twice, once
with the DevTools open and once with them closed, and then
compare those two executions, we would be able to isolate
the SADT. Thus, our methodology is based on two concepts:
deterministic website replay and code convergence.
Deterministic website replay To obtain meaningful results
when visiting the same website multiple times, we ﬁrst need
a way to reliably load it exactly the same way. In particular,
this means we do not want the server-side logic to have any
inﬂuence on the response and we also do not want dynamic
content like different ads on every page load. Therefore, we
must load the website only once from the remote server and
cache all content on a local proxy. Afterward, we ensure that
our browser can not connect to the outside world and loads
the page only from our proxy to avoid any interaction with
the remote server. However, we also must disable all ways
to obtain randomness on the client-side. Otherwise, if a URL
parameter contains a random id, the proxy will not have seen
this request before and be unable to answer as expected. Thus
we replace Math.random with a PRNG implementation with a
ﬁxed seed and use a ﬁxed timestamp as a starting point for all
clock information in Date and performance.
In theory, without any external logic or randomness, the
page should behave entirely deterministic every time we load
it, which is exactly what we need for our analysis. Unfor-
tunately, the replays are not entirely perfect. Since in the
browser and also the underlying operating system many ac-
2944    30th USENIX Security Symposium
USENIX Association
tions are executed in parallel, the exact order of events is not
always deterministic. For example, consider a website with
multiple iframes which all send a postmessage to the main
frame upon completion. The main frame could execute differ-
ent code depending on which frame loaded ﬁrst. So even if
our replay system otherwise works perfectly, we can not pre-
vent that small performance differences in this multi-process
system sometimes cause one iframe to load faster than an-
other, leading to different behavior in the main frame in the
end. Getting rid of these performance ﬂuctuations is unreal-
istic, as it would require immense changes to both browser
architecture and the underlying operating system’s scheduler.
Therefore, we instead rely on the concept of code convergence
to deal with this problem.
Code convergence The idea here is that the more often we
replay the same website, the lower the likelihood becomes
that we will discover any new execution paths caused by
small timing differences. Or to describe it more brieﬂy: The
executed code converges over time. We thus replay each page
multiple times and always measure the code coverage, i.e.,
we track which statements in a script are executed and which
are not, across all scripts on the page. By merging all seen
code from the previous replays, we can check if the current
replay introduced any new statements. In the same way, we
can also build the intersection of all previously executed code
and check if some parts were not executed, which always had
been executed before. If now, for multiple replays, no new
code is added nor always executed code missing, we likely
have executed until convergence.
Detection Methodology By combining these two concepts,
we can now replay any website in the same environment until
convergence. We can then inject analysis artifacts into the
page, like attaching a debugger or adding a breakpoint. As
long as we do not make any changes to the website’s code, it
should behave like during the previous replays. This means
we should not see any completely new code, nor should code
be missing that previously was always executed. If, however,
we reliably observe different code execution only when our
analysis artifacts are present, then these differences are most
likely caused by an anti-debugging technique.
6.2 Study II – Implementation
We implemented our approach as a tool that can detect SADTs
in a fully automated fashion. As in our ﬁrst study, we con-
trol the browser from Node.js by using the Chrome DevTools
Protocol (CDP). This protocol exposes all features of the
DevTools for programmatic access and gives us low-level
information and callbacks for many useful events. In par-
ticular, the CDP gives us ﬁne-grained code coverage data
with the Profiler.takePreciseCoverage command. More-
over, the protocol lets us control the debugger, so we can
programmatically enable breakpoints and set them at speciﬁc
locations, which we need to detect MONBREAK and NEW-
BREAK. Since the CDP does not include a way to open the
DevTools on demand, we instead cause an artiﬁcial slowdown
of the console to detect the CONSPAM technique. We imple-
mented this by wrapping all functions of the console object to
ﬁrst execute a busy loop for a short time, which approximates
the slowdown normally caused by an open DevTools window.
For the replaying part, we use a modiﬁed version of Web
Page Replay (WPR) [18], a tool written in Go that is devel-
oped and used by Google to benchmark their browser. The
tool is designed to record the loading of a website and cre-
ates an archive ﬁle with all requests and responses, including
the headers. This archive ﬁle can then be used to create a
deterministic replay of the previously recorded page. WPR
also tries to make the replays as deterministic as possible,
by injecting a script that wraps common sources of client-
side randomness like Math.random and Date to always use the
same seed values. Additionally, we improved the accuracy
of the replays by extending WPR to always answer with the
same delay as the real server during the recording. By com-
bining our Node.js browser instrumentation and this modiﬁed
Go proxy, we can now automatically detect anti-debugging
techniques in the wild.
6.3 Study II – Experiment Setup
To get accurate results, it is important to replay each page
multiple times to ensure we have reached code convergence.
Therefore, we record each website once and replay it until
we get 10 consecutive measurements without any changes
in coverage. If after 50 replays this still did not happen, we
discard the website as being incompatible with our replaying
infrastructure. After convergence, we test each technique 5
times. We only count a technique as present if it caused dif-
ferences in at least 3 of the replays, to ensure their effect on
the code coverage is reproducible.
Replaying this many times is a costly process, especially
since we need to restart the browser with a new proﬁle be-
tween each replay. Otherwise, stored state in cookies, local
storage, and other places could lead to different execution
branches. Therefore, in this second study, we only target the
2000 websites with the highest severity score according to
our previous study on BADTs in Section 4.3. In the following,
we will investigate whether this score is also a good indicator
for the presence of sophisticated techniques.
6.4 Study II – Prevalence
While we started this study directly after the ﬁrst had ﬁnished,
nevertheless 33 out of the 2000 selected sites were no longer
reachable. Another 6 sites did not converge even after 50
replays. On 229 out of the remaining 1961 sites, we could
ﬁnd behavior similar to one or more of the three SADTs. Thus,
USENIX Association
30th USENIX Security Symposium    2945
about 12% of these sites executed different code when under
analysis.
As Table 6 shows, the MONBREAK technique was the
most common of the three and present on around 14% of
the investigated sites. On the other hand, CONSPAM was
rather uncommon with less than 1% prevalence. The tech-
nique MONBREAK was mostly seen in ﬁrst-party code, while
NEWBREAK was a bit more often seen in third-party code.
However, any difference in third-party code execution might
also cause differences in ﬁrst-party code and vice versa. Thus,
there is some overlap between ﬁrst- and third-party code de-
tections.
Table 6: Sites with SADTs in ﬁrst- and third-party code.
Technique
MONBREAK
NEWBREAK
CONSPAM
TOTAL
# All
138
85
8
229
# First-party
124
38
5
# Third-party
24
54
3
165
81
When comparing these results to another sample of 100
randomly selected sites, we only found 1 site with a SADT, in
this case NEWBREAK. We see this low false positive rate as
evidence that our approach to detect sophisticated techniques
is reliable. Furthermore, we can see that BADTs are indeed a
good indicator for the presence of further sophisticated tech-
niques.
7 Discussion
In the following, we will discuss reasons to employ anti-
debugging techniques, some limitations of our presented ap-
proach, and how we envision future work to build and improve
on this.
7.1 Anti-Debugging and Maliciousness
As with so many other technologies and techniques, the very
same thing can be used for both good and evil. On one hand,
the anti-debugging techniques presented in this paper obvi-
ously can be used to make it more difﬁcult to detect and
subsequently analyze malicious JavaScript code. On the other
hand, the same techniques can also be used in legitimate ways,
e.g., to protect intellectual property by making it harder to
extract the content of a website and to discourage reverse-
engineering attempts of client-side code. Discerning between
these two use cases, however, depends a lot on the context,
i.e., what other content and scripts a website is serving. In this
regard, anti-debugging techniques share many characteristics
with code obfuscation techniques, which can also be used in
an attempt to protect intellectual property, as well as to bet-
ter hide malicious code [45]. Moreover, both do not prevent
the analysis in itself, but rather deter by complicating any
attempts at it. Thus, any malicious code that makes use of ob-
fuscation and/or anti-debugging techniques has an advantage
over code that does not use these techniques, by increasing
the chances that an attack can remain undetected for longer.
Previous research has shown that while the obfuscation of
JavaScript code does not necessarily imply maliciousness,
the majority of malware samples are nevertheless obfus-
cated [15, 23]. Thus, discerning miniﬁed from obfuscated
code is important, as the presence of obfuscated code can
serve as a useful feature for malware scanners [48]. Due to
their similar characteristics, we argue that these ﬁndings on
code obfuscation likely apply to anti-debugging techniques
as well. That is to say, their presence should not be taken as
the sole reason to classify a website as malicious. Yet, similar
to the presence of obfuscation, their presence can serve as a
useful feature for a malware scanner and thus should be taken
into account accordingly.
7.2 Limitations
Our approach is essentially a detector for anti-debugging tech-
niques. As such, it struggles with three properties that affect
virtually every detector: completeness, false positives, and
false negatives.
Completeness First of all, we can not be certain that we
have included all existing anti-debugging techniques in this
work. However, due to our extensive study of previous publi-
cations, blog posts, and Q&A sites on the Web, we are con-
ﬁdent that our research investigates the most common and
well-known techniques. Moreover, we are certain that all anti-
debugging techniques must have at least one of the three goals
described in Section 3: Either outright impede the analysis, or
subtly alter its results, or just detect its presence. While it is
possible that we have missed one particular implementation to
achieve one of the three goals, we argue that we are complete
in the sense that no entirely new technique with completely
different goals does exist.
False positives Many of our measurements are highly ac-
curate, e.g., the code to trigger the techniques DEVCUT and
LOGGET is so speciﬁc that they are obviously and undeni-
ably anti-debugging techniques and nothing else. However,
especially our results on the sophisticated techniques report
on websites that would interfere with an analysis, yet their
behavior might not necessarily be malicious or intentional.
As a backdoor could always be cleverly disguised as a "bug-
door" [56], i.e., look like an innocent programming mistake,
we will never know the true intentions behind any suspicious
piece of code. Nevertheless, these websites behave differently
in an analysis environment. We show that just attaching a
debugger or setting a breakpoint during analysis can already
have dangerous effects on the outcome of the analysis. Under
these circumstances, any derived results should be considered
inconclusive at best and deceiving at worst.
2946    30th USENIX Security Symposium
USENIX Association
False negatives Some actions are only signiﬁcantly hin-
dering an ongoing analysis if they are happening constantly
like clearing the console or breakpoints on every function
invoke. Therefore, we introduced the conﬁdence and severity
scores, to focus on the most severe cases of anti-debugging
attempts. Naturally, this means that some sites might have
escaped our attention if they trigger the technique only very
rarely, but on the other hand then also means their techniques
are less effective. Moreover, self-inspecting scripts could be-
come aware of our modiﬁcations to built-in functions during
the replay and then interfere with our data collection, as we
will discuss in the next section. Therefore, our results should
be seen as merely the lower bound of active anti-debugging
techniques in the wild. To make certain we deﬁnitely detect
anti-debugging attempts from known implementations, we
created a testbed with code snippets found on the Web a well
as generated by a JavaScript obfuscator with anti-debugging
features [46] to validate our detection methodology.
7.3 Future Work
We see our paper as the ﬁrst foray into the world of anti-
debugging on the Web, where we quantify the problem and
raise awareness for this phenomenon. Yet, there is still more to
be done, in particular concerning reliably detecting advanced
self-inspection and deploying effective countermeasures.
Advanced self-inspection In this paper, we worked under
the assumption that attackers only try to interfere with de-
bugging attempts, but not with our attempts to detect their
anti-debugging. Our replaying approach for sophisticated
techniques in particular needs to modify built-ins like Date
and Math, which could be detected by self-inspecting scripts.
Therefore, the sensible next step is to move these modiﬁ-
cations from the JavaScript environment to the C++ realm,
where they can not be inspected directly by an attacker and
could only be observed through side-effects. Projects like Vis-
ibleV8 [22] seem to offer a promising route for researchers
to achieve this without a deep understanding of the browser’s
code.
Countermeasures Some of the presented techniques are
trivial to bypass, e.g., DEVCUT just prevents the use of certain
hotkeys but not the menu bar to open the DevTools. However,
something like preventing the executed JavaScript code from
learning that a breakpoint was hit is a much harder problem,
as we saw with the MONBREAK technique. This would only
be possible to achieve by modifying the browser and its un-
derlying JavaScript engine itself. And even then, freezing the
time is an especially difﬁcult feat since a script could also get
time information from a remote server and thus easily detect
any gaps or clock drifts. Therefore, we would like to see a
special forensic browser with countermeasures in place to
enable safe and reliable debugging of client-side code in an
adversarial setting.
8 Related Work
In this section, we will ﬁrst present works on anti-debugging
in native malware, followed by publications on the topic of
malicious JavaScript in general and conclude with the most
closely related papers about evasive malware on the Web.
8.1 Anti-debugging in General
Anti-debugging techniques are a well-known concept
from the area of native x86 malware. Back in 2006,
Vasudevan and Yerraballi [58] proposed the ﬁrst analysis sys-
tem that focused on mitigations for anti-debugging techniques.
Their system called Cobra can, in particular, deal with self-
modifying and self-checking code and thus counters many
anti-analysis tricks. In 2010, Balzarotti et al. [2] proposed a
technique to detect if a malware sample behaves differently
in an emulated environment when compared to a reference
host. Their main challenge was to achieve a deterministic ex-
ecution of the malware in both environments so that a robust
comparison of behavior becomes possible. Therefore, they
ﬁrst record all interaction of the malware with the operating
system to exactly replay the results of the system calls in the
second run. One year later Lindorfer et al. [34] extended on
this idea with their system called Disarm, by not only compar-
ing the behavior between the emulation and a real system, but
instead comparing behavior between four different emulation
systems.
Kirat et al. [28] improved on these previous works by creat-
ing an analysis platform called BareCloud which runs the mal-
ware in a transparent bare-metal environment without in-guest
monitoring. However, the cat and mouse game continued by
ﬁnding new techniques to detect and evade even these bare-
metal analysis systems. In 2017, Miramirkhani et al. [38] pre-
sented their work on "wear and tear" artifacts, i.e., detecting