adversarial examples based on the capability of sensor attacks to
fool the LiDAR-based perception models in AV systems.
12 CONCLUSION
In this work, we perform the first security study of LiDAR-based
perception in AV systems. We consider LiDAR spoofing attacks
as the threat model, and set the attack goal as spoofing front-near
obstacles. We first reproduce the state-of-the-art LiDAR spoofing
attack, and find that blindly applying it is insufficient to achieve the
attack goal due to the machine learning-based object detection pro-
cess. We thus perform analysis to fool the machine learning model
by formulating the attack task as an optimization problem. We first
construct the input perturbation function using local attack experi-
ments and global spatial transformation-based modeling, and then
construct the objective function by studying the post-processing
process. We also identify the inherent limitations of directly us-
ing optimization-based methods and design a new algorithm that
increases the attack success rates by 2.65× on average. As a case
study, we further construct and evaluate two attack scenarios that
may compromise AV safety and mobility. We also discuss defense
directions at AV system, sensor, and machine learning model levels.
ACKNOWLEDGMENTS
We would like to thank Shengtuo Hu, Jiwon Joung, Yunhan Jack Jia,
Yuru Shao, Yikai Lin, David Ke Hong, the anonymous reviewers,
and our shepherd Zhe Zhou for providing valuable feedback on our
work. This research was supported in part by an award from Mcity
at University of Michigan, by the National Science Foundation
under grants CNS-1850533, CNS-1330142, CNS-1526455 and CCF-
1628991, by ONR under N00014-18-1-2020.
REFERENCES
[1] 2005. HARD BRAKE HARD ACCELERATION. http://tracknet.accountsupport.
com/wp-content/uploads/Verizon/Hard-Brake-Hard-Acceleration.pdf. (2005).
[2] 2016. ArbExpress. https://www.tek.com/signal-generator/afg2021-software-0.
(2016).
[3] 2017.
An Introduction to LIDAR: The Key Self-Driving Car Sen-
sor. https://news.voyage.auto/an-introduction-to-lidar-the-key-self-driving-
car-sensor-a7e405590cff. (2017).
[4] 2017. Baidu Apollo. http://apollo.auto. (2017).
[5] 2017. Google’s Waymo Invests in LIDAR Technology, Cuts Costs by 90 Per-
cent. https://arstechnica.com/cars/2017/01/googles-waymo-invests-in-lidar-
technology-cuts-costs-by-90-percent/. (2017).
[6] 2017. KITTI Vision Benchmark: 3D Object Detection. http://www.cvlibs.net/
datasets/kitti/eval_object.php?obj_benchmark=3d. (2017).
[7] 2017. What it Was Like to Ride in GM’s New Self-Driving Cruise
Car. https://www.recode.net/2017/11/29/16712572/general-motors-gm-new-self-
driving-autonomous-cruise-car-future. (2017).
[8] 2018. Baidu hits the gas on autonomous vehicles with Volvo and Ford deals. https:
//techcrunch.com/2018/11/01/baidu-volvo-ford-autonomous-driving/. (2018).
[9] 2018. Baidu starts mass production of autonomous buses. https://www.dw.com/
en/baidu-starts-mass-production-of-autonomous-buses/a-44525629. (2018).
[10] 2018. VeloView. https://www.paraview.org/VeloView/. (2018).
[11] 2018. Volvo Finds the LIDAR it Needs to Build Self-Driving Cars. https://www.
wired.com/story/volvo-self-driving-lidar-luminar/. (2018).
[12] 2018. Waymo’s autonomous cars have driven 8 million miles on public
roads. https://www.theverge.com/2018/7/20/17595968/waymo-self-driving-cars-
8-million-miles-testing. (2018).
[13] 2018. What Is LIDAR, Why Do Self-Driving Cars Need It, And Can It See Nerf
Bullets? https://www.wired.com/story/lidar-self-driving-cars-luminar-video/.
(2018).
[14] 2018. You can take a ride in a self-driving Lyft during CES. https://www.theverge.
com/2018/1/2/16841090/lyft-aptiv-self-driving-car-ces-2018. (2018).
[15] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al.
2016. Tensorflow: a system for large-scale machine learning.. In OSDI, Vol. 16.
265–283.
[16] Anish Athalye, Nicholas Carlini, and David Wagner. 2018. Obfuscated gradients
give a false sense of security: Circumventing defenses to adversarial examples.
arXiv preprint arXiv:1802.00420 (2018).
[17] Anish Athalye and Ilya Sutskever. 2018. Synthesizing Robust Adversarial Exam-
ples. In International Conference on Machine Learning (ICML).
[18] Nicholas Carlini, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah Sherr,
Clay Shields, David Wagner, and Wenchao Zhou. 2016. Hidden Voice Commands.
In USENIX Security Symposium.
[19] Nicholas Carlini and David Wagner. 2017. Adversarial Examples are not Easily
Detected: Bypassing Ten Detection Methods. In Proceedings of the 10th ACM
Workshop on Artificial Intelligence and Security. ACM, 3–14.
[20] Nicholas Carlini and David Wagner. 2018. Audio Adversarial Examples: Targeted
Attacks on Speech-to-text. In Deep Learning and Security Workshop (DLS).
[21] Nicholas Carlini and David A. Wagner. 2017. Towards Evaluating the Robustness
of Neural Networks. In 2017 IEEE Symposium on Security and Privacy, SP 2017,
San Jose, CA, USA, May 22-26, 2017. 39–57. https://doi.org/10.1109/SP.2017.49
[22] Stephen Checkoway, Damon McCoy, Brian Kantor, Danny Anderson, Hovav
Shacham, Stefan Savage, Karl Koscher, Alexei Czeskis, Franziska Roesner, and
Tadayoshi Kohno. 2011. Comprehensive Experimental Analyses of Automotive
Attack Surfaces. In Proceedings of the 20th USENIX Conference on Security (SEC’11).
[23] Qi Alfred Chen, Yucheng Yin, Yiheng Feng, Z. Morley Mao, and Henry X. Liu
Liu. 2018. Exposing Congestion Attack on Emerging Connected Vehicle based
Traffic Signal Control. In Proceedings of the 25th Annual Network and Distributed
System Security Symposium (NDSS ’18).
[24] Minhao Cheng, Jinfeng Yi, Huan Zhang, Pin-Yu Chen, and Cho-Jui Hsieh. 2018.
Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence Models with Ad-
versarial Examples. arXiv preprint arXiv:1803.01128 (2018).
[25] Kyong-Tak Cho and Kang G. Shin. 2016. Error Handling of In-vehicle Networks
Makes Them Vulnerable. In Proceedings of the 2016 ACM SIGSAC Conference on
Computer and Communications Security (CCS’16).
[26] Moustapha Cisse, Yossi Adi, Natalia Neverova, and Joseph Keshet. 2017. Houdini:
Fooling deep structured prediction models. arXiv preprint arXiv:1707.05373 (2017).
[27] Ivan Evtimov, Kevin Eykholt, Earlence Fernandes, Tadayoshi Kohno, Bo Li, Atul
Prakash, Amir Rahmati, and Dawn Song. 2017. Robust physical-world attacks on
deep learning models. arXiv preprint arXiv:1707.08945 1 (2017).
[28] Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Flo-
rian Tramer, Atul Prakash, Tadayoshi Kohno, and Dawn Song. 2018. Physical
Adversarial Examples for Object Detectors. In USENIX Workshop on Offensive
Technologies (WOOT).
[29] Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei
Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. 2018. Robust Physical-
World Attacks on Deep Learning Visual Classification. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR).
[30] Yiheng Feng, Shihong Huang, Qi Alfred Chen, Henry X. Liu, and Z. Morley
Mao. 2018. Vulnerability of Traffic Control System Under Cyber-Attacks Using
Falsified Data. In Transportation Research Board 2018 Annual Meeting (TRB).
[31] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and
harnessing adversarial examples. arXiv preprint arXiv:1412.6572 (2014).
[32] Velodyne LiDAR Inc. 2018. VLP-16 User Manual. (2018).
[33] Radoslav Ivanov, Miroslav Pajic, and Insup Lee. 2014. Attack-resilient sensor
fusion. In Proceedings of the conference on Design, Automation & Test in Europe.
[34] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. 2015. Spatial trans-
former networks. In Advances in neural information processing systems. 2017–
2025.
[35] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 (2014).
[36] Karl Koscher, Alexei Czeskis, Franziska Roesner, Shwetak Patel, Tadayoshi Kohno,
Stephen Checkoway, Damon McCoy, Brian Kantor, Danny Anderson, Hovav
Shacham, and Stefan Savage. 2010. Experimental Security Analysis of a Modern
Automobile. In Proceedings of the 2010 IEEE Symposium on Security and Privacy
(SP’10).
[37] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang,
and Xiangyu Zhang. 2018. Trojaning Attack on Neural Networks. In Annual
Network and Distributed System Security Symposium (NDSS).
[38] Xingjun Ma, Bo Li, Yisen Wang, Sarah M Erfani, Sudanthi Wijewickrema,
Michael E Houle, Grant Schoenebeck, Dawn Song, and James Bailey. 2018. Char-
acterizing Adversarial Subspaces Using Local Intrinsic Dimensionality. arXiv
preprint arXiv:1801.02613 (2018).
[39] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu. 2017. Towards deep learning models resistant to adversarial attacks.
arXiv preprint arXiv:1706.06083 (2017).
[40] Sahar Mazloom, Mohammad Rezaeirad, Aaron Hunter, and Damon McCoy. 2016.
A Security Analysis of an In-Vehicle Infotainment and App Platform. In Usenix
Workshop on Offensive Technologies (WOOT).
[41] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik,
and Ananthram Swami. 2017. Practical Black-Box Attacks Against Machine
Learning. In ACM on Asia Conference on Computer and Communications Security.
[42] Jonathan Petit, Bas Stottelaar, Michael Feiri, and Frank Kargl. 2015. Remote
Attacks on Automated Vehicles Sensors: Experiments on Camera and LiDAR. In
Black Hat Europe.
[43] Ishtiaq Rouf, Rob Miller, Hossen Mustafa, Travis Taylor, Sangho Oh, Wenyuan
Xu, Marco Gruteser, Wade Trappe, and Ivan Seskar. 2010. Security and Privacy
Vulnerabilities of In-car Wireless Networks: A Tire Pressure Monitoring System
Case Study. In Proceedings of the 19th USENIX Conference on Security (USENIX
Security’10). USENIX Association, Berkeley, CA, USA, 21–21. http://dl.acm.org/
citation.cfm?id=1929820.1929848
[44] Hocheol Shin, Dohyun Kim, Yujin Kwon, and Yongdae Kim. 2017. Illusion and
Dazzle: Adversarial Optical Channel Exploits Against Lidars for Automotive Ap-
plications. In International Conference on Cryptographic Hardware and Embedded
Systems (CHES).
[45] Yasser Shoukry, Paul Martin, Paulo Tabuada, and Mani Srivastava. 2013. Non-
invasive Spoofing Attacks for Anti-lock Braking Systems. In Cryptographic Hard-
ware and Embedded Systems - CHES 2013, Guido Bertoni and Jean-Sébastien
Coron (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 55–72.
[46] Yasser Shoukry, Paul Martin, Yair Yona, Suhas N. Diggavi, and Mani B. Srivastava.
2015. PyCRA: Physical Challenge-Response Authentication For Active Sensors
Under Spoofing Attacks. In ACM Conference on Computer and Communications
Security.
[47] Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Dan Boneh, and Patrick
McDaniel. 2017. Ensemble adversarial training: Attacks and defenses. arXiv
preprint arXiv:1705.07204 (2017).
[48] Wai Wong, Shihong Huang, Yiheng Feng, Qi Alfred Chen, Z Morley Mao, and
Henry X Liu. 2019. Trajectory-Based Hierarchical Defense Model to Detect Cyber-
Attacks on Transportation Infrastructure. In Transportation Research Board 2019
Annual Meeting (TRB).
[49] Chong Xiang, Charles R Qi, and Bo Li. 2018. Generating 3D Adversarial Point
Clouds. arXiv preprint arXiv:1809.07016 (2018).
[50] Chaowei Xiao, Ruizhi Deng, Bo Li, Fisher Yu, Dawn Song, et al. 2018. Char-
acterizing Adversarial Examples Based on Spatial Consistency Information for
Semantic Segmentation. In Proceedings of the (ECCV). 217–234.
[51] Chaowei Xiao, Bo Li, Jun-Yan Zhu, Warren He, Mingyan Liu, and Dawn Song.
2018. Generating adversarial examples with adversarial networks. arXiv preprint
arXiv:1801.02610 (2018).
[52] Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan Liu, and Dawn Song.
2018. Spatially transformed adversarial examples. arXiv preprint arXiv:1801.02612
(2018).
[53] Cihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi Xie, and Alan
Yuille. 2017. Adversarial Examples for Semantic Segmentation and Object Detec-
tion. In IEEE International Conference on Computer Vision (ICCV).
[54] Xiaojun Xu, Xinyun Chen, Chang Liu, Anna Rohrbach, Trevor Darell, and Dawn
Song. 2017. Can you fool AI with adversarial examples on a visual Turing test?
arXiv preprint arXiv:1709.08693 (2017).
[55] Chen Yan. 2016. Can You Trust Autonomous Vehicles : Contactless Attacks
against Sensors of Self-driving Vehicle.
[56] Kang Yang, Rui Wang, Yu Jiang, Houbing Song, Chenxia Luo, Yong Guan, Xi-
aojuan Li, and Zhiping Shi. 2018. Sensor attack detection using history based
pairwise inconsistency. Future Generation Computer Systems (2018).
[57] Xuejing Yuan, Yuxuan Chen, Yue Zhao, Yunhui Long, Xiaokang Liu, Kai Chen,
Shengzhi Zhang, Heqing Huang, Xiaofeng Wang, and Carl A Gunter. 2018. Com-
manderSong: A Systematic Approach for Practical Adversarial Voice Recognition.
In USENIX Security Symposium.
APPENDIX
A ALGORITHM DETAILS AND EXPERIMENT
SETTINGS
Algorithm 1 shows the detailed algorithm to generate adversarial
examples. In our experiment, we select Adam [35] as our optimizer
opt with learning rate 1e − 4. opt(ladv; θ, τx , sh) means updating the
parameters θ, τx , sh) with respect to Loss function ladv. We select
TensorFlow [15] as backbone. Lt is set as 12.5 while Lθ is set as the
angle that generates 2-meter distance from the target position.
Figure 14: Collected traces from the reproduced sensor at-
tack. The points in the yellow circle are spoofed by the sen-
sor attack.
4
5
6
7
8
9
10
11
1
Algorithm 1: Generating adversarial examples by leveraging
global spatial transformation
input: Target model: M;
3D point cloud X ;
3D spoofed 3D point cloud T ;
Optimizer opt;
Max iteration N ;
output: 3D adversarial 3D point cloud X ′;
2 Initialization: θ ← 0, τx ← 0, sh ← 1, lmin = +inf,
x = Φ(X), t = Φ(T);
/* Initiate parameters by sampling around the
transformation parameters T arдetθ , T arдetτx that
transforms t to the target position (px, py) of
the attack
*/
3 for iτx ← −Lt to Lt do
for iθ ← −Lθ to Lθ do
*/;
+ τx i;
/* Initialize parameter .
θ ← T arдetθ + iθ, τx ← T arдetτx
for iter ← 1 to N do
ladv ← Equation 7.;
/* Update the parameters θ, τx , sh based on
*/
/* Calculate adversarial loss
optimizer opt and loss ladv
*/;
;
θ, τx , sh ← opt(ladv; θ, τx , sh)
if lmin < ladv then
, s
θ f inal , τ
f inal
f inal
x
h ← θ, τx , sh
end
end
12
13
14 end
15 T ′ ← GT (θ f inal , t
16 X ′ ← X + T ′;
Return: T ′
f inal
x
, s
f inal
h
;T);