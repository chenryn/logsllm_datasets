each sender has either (cid:98)f /n(cid:99) or (cid:100)f /n(cid:101) ﬂows.
(a)
(b)
(c)
(d)
(e)
Figure 3: PDQ outperforms D3, RCP and TCP and
achieves near-optimal performance.
Top three ﬁgures:
deadline-constrained ﬂows; bottom two ﬁgures: deadline-
unconstrained ﬂows.
very close to optimal regardless of the ﬂow size. However,
Early Start and Early Termination provide fewer beneﬁts in
this scenario because of the small number of ﬂows.
Impact of Flow Deadline: Data center operators are
particularly interested in the operating regime where the
network can satisfy almost every ﬂow deadline. To this end,
we attempt to ﬁnd, using a binary search procedure, the
maximal number of ﬂows a protocol can support while en-
suring 99% application throughput. We also vary the ﬂow
deadline, which is drawn from an exponential distribution,
to observe the system performance with regard to diﬀerent
ﬂow deadlines with mean between 20 ms to 60 ms. Figure 3c
demonstrates that, compared with D3, PDQ can support >3
times more concurrent ﬂows at 99% application throughput,
and this ratio becomes larger as the mean ﬂow deadline in-
creases. Moreover, Figure 3c shows that Suppressed Probing
......4 x 1G3 x 1G......4 x ToRSwitches12 x Servers...RootSwitchN x SendersSwitchReceiver1...N...Bottleneck linkReversetraffic 50 55 60 65 70 75 80 85 90 95 100 0 5 10 15 20 25ApplicationThroughput [%]Number of FlowsOptimalPDQ(Full)PDQ(ES+ET)PDQ(ES)PDQ(Basic)D3RCPTCP 70 75 80 85 90 95 100 100 150 200 250 300 350ApplicationThroughput [%]Avg Flow Size [KByte]OptimalPDQ(Full)PDQ(ES+ET)PDQ(ES)PDQ(Basic)D3RCPTCP 0 13 26 39 52 65 20 25 30 35 40 45 50 55 60Number of Flows at 99%Application ThroughputMean Flow Deadline [ms]OptimalPDQ(Full)PDQ(ES+ET)PDQ(ES)PDQ(Basic)D3RCPTCP 1 1.5 2 2.5 3 3.5 4 0 5 10 15 20 25Flow CompletionTime [Normalizedto Optimal]Number of FlowsPDQ(Full)PDQ(ES)PDQ(Basic)RCP/D3TCP 1 1.5 2 2.5 100 150 200 250 300 350Flow CompletionTime [Normalizedto Optimal]Avg Flow Size [KByte]PDQ(Full)PDQ(ES)PDQ(Basic)RCP/D3TCP133becomes more useful as the number of concurrent ﬂows in-
creases.
5.2.2 Deadline-unconstrained Flows
Impact of Flow Number: For deadline-unconstrained
case, we ﬁrst measure the impact of the number of ﬂows
on the average ﬂow completion time. Overall, Figure 3d
demonstrates that PDQ can eﬀectively approximate the op-
timal ﬂow completion time. The largest gap between PDQ
and optimal happens when there exists only one ﬂow and
is due to ﬂow initialization latency. RCP has a similar per-
formance for the single-ﬂow case. However, its ﬂow comple-
tion time becomes relatively large as the number of ﬂows
increases. TCP displays a large ﬂow completion time when
the number of ﬂows is small due to the ineﬃciency of slow
start. When the number of concurrent ﬂows is large, TCP
also has an increased ﬂow completion time due to the TCP
incast problem [19].
Impact of Flow Size: We ﬁx the number of ﬂows at 3, and
Figure 3e shows the ﬂow completion time as the ﬂow size in-
creases. We demonstrate that PDQ can better approximate
optimal ﬂow completion time as ﬂow size increases. The rea-
son is intuitive: the adverse impact of PDQ ineﬃciency (e.g.,
ﬂow initialization latency) on ﬂow completion time becomes
relatively insigniﬁcant as ﬂow size increases.
5.3
Impact of Sending Pattern: We study the impact of
the following sending patterns: (i) Aggregation: multiple
servers send to the same aggregator, as done in the prior
experiment. (ii) Stride(i): a server with index x sends to the
host with index (x + i) mod N , where N is the total number
of servers; (iii) Staggered Prob(p): a server sends to another
server under the same top-of-rack switch with probability p,
and to any other server with probability 1− p; (iv) Random
Permutation: a server sends to another randomly-selected
server, with a constraint that each server receives traﬃc from
exactly one server (i.e., 1-to-1 mapping).
Impact of Trafﬁc Workload
Figure 4 shows that PDQ reaps its beneﬁts across all the
sending patterns under consideration. The worst pattern
for PDQ is the Staggered Prob(0.7) due to the fact that
the variance of the ﬂow RTTs is considerably larger.
In
this sending pattern, the non-local ﬂows that pass through
the core network could have RTTs 3 − 5 times larger than
the local ﬂow RTTs. Thus, the PDQ rate controller, whose
update frequency is based on a measurement of average ﬂow
RTTs, could slightly overreact (or underreact) to ﬂows with
relatively large (or small) RTTs. However, even in such a
case, PDQ still outperforms the other schemes considerably.
Impact of Traﬃc Type: We consider two workloads
collected from real data centers. First, we use a workload
with ﬂow sizes following the distribution from a large-scale
commercial data center measured by Greenberg et al. [12].
It represents a mixture of long and short ﬂows: Most ﬂows
are small, and most of the delivered bits are contributed by
long ﬂows.
In the experiment, we assume that the short
ﬂows (with a size of <40 KByte) are deadline-constrained.
We conduct these experiments with random permutation
traﬃc.
Figure 5a demonstrates that, under this particular work-
load, PDQ outperforms the other protocols by supporting a
signiﬁcantly higher ﬂow arrival rate. We observed that, in
(a)
(b)
(c)
Figure 5: Performance evaluation under realistic data cen-
ter workloads, collected from (a, b) a production data center
of a large commercial cloud service [12] and (c) a university
data center located in Midwestern United States (EDU1 in
[6]).
this scenario, PDQ(Full) considerably outperforms PDQ(ES+ET).
This suggests that Suppressed Probing plays an important
role in minimizing the probing overhead especially when
there exists a large collection of paused ﬂows. Figure 5b
shows that PDQ has lower ﬂow completion time for long
ﬂows: a 26% reduction compared with RCP, and a 39% re-
duction compared with TCP.
We also evaluate performance using a workload collected
from a university data center with 500 servers [6]. In particu-
lar, we ﬁrst convert the packet trace, which lasts 10 minutes,
to ﬂow-level summaries using Bro [1], then we fed it to the
simulator. Likewise, PDQ outperforms other schemes in this
regime (Figure 5c).
5.4 Dynamics of PDQ
Next, we show PDQ’s performance over time through two
scenarios, each with varying traﬃc dynamics:
Scenario #1 (Convergence Dynamics): Figure 6 shows
that PDQ provides seamless ﬂow switching. We assume ﬁve
ﬂows that start at time 0. The ﬂows have no deadlines,
and each ﬂow has a size of ∼1 MByte. The ﬂow size is
perturbed slightly such that a ﬂow with smaller index is
more critical. Ideally, the ﬁve ﬂows together take 40 ms to
ﬁnish because each ﬂow requires a raw processing time of
1 MByte
1 Gbps = 8 ms. With seamless ﬂow switching, PDQ com-
pletes at ∼42 ms due to protocol overhead (∼3% bandwidth
loss due to TCP/IP header) and ﬁrst-ﬂow initialization time
(two-RTT latency loss; one RTT latency for the sender to
receive the SYN-ACK, and an additional RTT for the sender
to receive the ﬁrst DATA-ACK). We observe that PDQ can
converge to equilibrium quickly at ﬂow switching time, re-
sulting in a near perfect (100%) bottleneck link utilization
(Figure 6b). Although an alternative (naive) approach to
achieve such high link utilization is to let every ﬂow send
with fastest rate, this causes the rapid growth of the queue
and potentially leads to congestive packet loss. Unlike this
approach, PDQ exhibits a very small queue size7 and has no
packet drops (Figure 6c).
7The non-integer values on the y axis comes from the small
probing packets.
 0 4000 8000 12000 16000 20000 15 20 25 30 35 40 45Short Flow Arrival Rate[#Flow/sec] at 99% Application ThroughputMean Flow Deadline [ms]PDQ(Full)PDQ(ES+ET)PDQ(ES)PDQ(Basic)D3RCPTCP 0 0.5 1 1.5 2PDQ(Full)PDQ(ES)PDQ(Basic)RCP/D3TCPFlow Completion Timeof Long Flows [Norm-alized to PDQ(Full)] 0 0.5 1 1.5 2PDQ(Full)PDQ(ES)PDQ(Basic)RCP/D3TCPFlow CompletionTime [Normalizedto PDQ(Full)]134(a)
Figure 4: PDQ outperforms D3, RCP and TCP across traﬃc patterns.
unconstrained ﬂows.
(b)
(a) Deadline-constrained ﬂows; (b) Deadline-
(a)
(b)
(a)
(b)
Figure 6: PDQ provides seamless ﬂow switching. It achieves
high link utilization at ﬂow switching time, maintains small
queue, and converges to the equilibrium quickly.
(c)
Figure 7: PDQ exhibits high robustness to bursty workload.
We use a workload of 50 concurrent short ﬂows all start at
time 1 ms, and preempting a long-lived ﬂow.
(c)
Scenario #2 (Robustness to Bursty Traﬃc):
Fig-
ure 7 shows that PDQ provides high robustness to bursty
workloads. We assume a long-lived ﬂow that starts at time
0, and 50 short ﬂows that all start at 10 ms. The short
ﬂow sizes are set to 20 KByte with small random pertur-
bation. Figure 7a shows that PDQ adapts quickly to sud-
den bursts of ﬂow arrivals. Because the required delivery
time of each short ﬂow is very small ( 20 KByte
the system never reaches stable state during the preemp-
tion period (between 10 and 19 ms). Figure 7b shows PDQ
adapts quickly to the burst of ﬂows while maintaining high
utilization: the average link utilization during the preemp-
tion period is 91.7%. Figure 7c suggests that PDQ does not
compromise the queue length by having only 5 to 10 packets
in the queue, which is about an order of magnitude smaller
than what today’s data center switches can store. By con-
trast, XCP in a similar environment results in a queue of
∼60 packets (Figure 11(b) in [15]).
1 Gbps ≈ 153 µs),
5.5 Impact of Network Scale
Today’s data centers have many thousands of servers, and
it remains unclear whether PDQ will retain its successes
at large scales. Unfortunately, our packet-level simulator,
which is optimized for high processing speeds, does not scale
to large-scale data center topology within reasonable pro-
cessing time. To study these protocols at large scales, we
construct a ﬂow-level simulator for PDQ, D3 and RCP. In
particular, we use an iterative approach to ﬁnd the equilib-
rium ﬂow sending rates with a time scale of 1 ms. The ﬂow-
level simulator also considers protocol ineﬃciencies like ﬂow
initialization time and packet header overhead. Although
the ﬂow-level simulator does not deal with packet-level dy-
namics such as timeouts or packet loss, Figure 8 shows that,
by comparing with the results from packet-level simulation,
the ﬂow-level simulation does not compromise the accuracy
signiﬁcantly.
We evaluate three scalable data center topologies: (1) Fat-
tree [2], a structured 2-stage Clos network; (2) BCube [13],
a server-centric modular network; (3) Jellyﬁsh [18], an un-
structured high-bandwidth network using random regular
graphs. Figure 8 demonstrates that PDQ scales well to large
scale, regardless of the topologies we tested. Figure 8e shows
that about 40% of ﬂow completion times under PDQ are re-
duced by at least 50% compared to RCP. Only 5 − 15% of
 0 0.2 0.4 0.6 0.8 1 1.2AggregationStride(1)Stride(N/2)StaggeredProb(0.7)StaggeredProb(0.3)RandomPermutationNormalized Numof Flows at 99%App Throughput[Normalized toPDQ(Full)]PDQ(Full): 1PDQ(ES+ET): 2PDQ(ES): 3PDQ(Basic): 4D3: 5RCP: 6TCP: 7111111222222333333444444555555666666777777 0 1 2AggregationStride(1)Stride(N/2)StaggeredProb(0.7)StaggeredProb(0.3)RandomPermutationFlow Completion Time [Normalizedto PDQ(Full)]PDQ(Full): 1PDQ(ES): 2PDQ(Basic): 3RCP/D3: 4TCP: 5111111222222333333444444555555 0 0.2 0.4 0.6 0.8 1 0 10 20 30 40 50Flow Throughput[Gb/s]Time [ms]Flow 1Flow 2Flow 3Flow 4Flow 5 0 0.5 1 0 10 20 30 40 50Utilization[%]Time [ms]Bottleneck Utilization 0 1 2 3 4 5 0 10 20 30 40 50Queue[Normalized todata pkt size]Time [ms]Bottleneck Queue 0 0.2 0.4 0.6 0.8 1 0 10 20 30 40 50Flow Throughput[Gb/s]Time [ms]Long Flow ThroughputTotal Throughput of 50 Short Flows 0 0.2 0.4 0.6 0.8 1 0 10 20 30 40 50 0 25 50Utilization[%]Flow IndexTime [ms]Bottleneck UtilizationFlow Completion Time 0 5 10 15 20 0 10 20 30 40 50Queue[normalized todata pkt size]Time [ms]Bottleneck Queue135(a)
(b)
(a)
(b)
Figure 9: PDQ is resilient to packet loss in both forward
and reverse directions:
(a) deadline-constrained and (b)
deadline-unconstrained cases. Query aggregation workload.
(c)
(d)
(e)
Figure 8: PDQ performs well across a variety of data center
topologies. (a,b) Fat-tree; (c) BCube with dual-port servers;
(d) Jellyﬁsh with 24-port switches, using a 2:1 ratio of net-
work port count to server port count. (e) For network ﬂows,
the ratio of the ﬂow completion time under PDQ to the
ﬂow completion time under RCP (ﬂow-level simulation; #
servers is ∼128). All experiments are carried out using ran-
dom permutation traﬃc; top ﬁgure: deadline-constrained
ﬂows; bottom four ﬁgures: deadline-unconstrained ﬂows
with 10 sending ﬂows per server.
the ﬂows have a larger completion time, and no more than
0.9% of the ﬂows have 2× completion time.
5.6 PDQ Resilience
Resilience to Packet Loss:
Next, to evaluate PDQ’s
performance in the presence of packet loss, we randomly
drop packets at the bottleneck link, in both the forward
(data) and reverse (acknowledgment) direction. Figure 9
demonstrates that PDQ is even more resilient than TCP
to packet loss. When packet loss happens, the PDQ rate
controller detects anomalous high/low link load quickly and
compensates for it with explicit rate control. Thus, packet
loss does not signiﬁcantly aﬀect its performance. For a heav-
ily lossy channel where the packet loss rate is 3% in both di-
rections (i.e., a round-trip packet loss rate of 1−(1−0.03)2 ≈
5.9%), as shown in Figure 9b, the ﬂow completion time of
PDQ has increased by 11.4%, while that of TCP has signif-
icantly increased by 44.7%.
Resilience to Inaccurate Flow Information: For many
data center applications (e.g., web search, key-value stores,
data processing), previous studies have shown that ﬂow size
can be precisely known at ﬂow initiation time.8 Even for
8See the discussion in §2.1 of [20].
Figure 10: PDQ is resilient to inaccurate ﬂow information.
For PDQ without ﬂow size information, the ﬂow criticality
is updated for every 50 KByte it sends. Query aggregation
workload, 10 deadline-unconstrained ﬂows with a mean size
of 100 KByte. Flow-level simulation.
applications without such knowledge, PDQ is resilient to
inaccurate ﬂow size information. To demonstrate this, we
consider the following two ﬂow-size-unaware schemes. Ran-
dom: the sender randomly chooses a ﬂow criticality at ﬂow
initialization time and uses it consistently. Flow Size Es-
timation: the sender estimates the ﬂow size based on the
amount of data sent already, and a ﬂow is more critical than
another one if it has smaller estimated size. To avoid ex-
cessive switching among ﬂows, the sender does not change
the ﬂow criticality for every packet it sends.
Instead, the
sender updates the ﬂow criticality only for every 50 KByte
it sends. Figure 10 demonstrates two important results: (i)
PDQ does require a reasonable estimate of ﬂow size as ran-
dom criticality can lead to large mean ﬂow completion time
in heavy-tailed ﬂow size distribution. (ii) With a simple esti-
mation scheme, PDQ still compares favorably against RCP
in both uniform and heavy-tailed ﬂow size distributions.