> Operations, Support, and Rich, please stay on the call. Everyone else, feel free to drop off at your discretion.
---
### Too Frequent Status Updates
![067](../../assets/slides/incident_response/incident_response.067.jpeg)
_067. Too frequent status updates. [Docs Reference](../../resources/anti_patterns.md#too-frequent-status-updates)_
Executives especially love frequent status updates. But providing them too frequently can cause things to get out of hand. If it takes 5 minutes to write an update, and they want an update every 5 minutes, you can start to see how long it's going to take to solve the incident.
At PagerDuty, we keep our internal updates to about once every 20-30 minutes. Writing the update takes away time from solving the incident, so that needs to be balanced. This cadence has worked well for us.
---
### Being Overly Focussed On An Issue
![068](../../assets/slides/incident_response/incident_response.068.jpeg)
_068. Being overly focussed on an issue. [Docs Reference](../../resources/anti_patterns.md#being-too-focussed-on-the-problem-in-front-of-you)_
The IC is generally the person who has the bigger picture of what's going on. But there can be a tendency for responders to become too focussed on the problem they see in front of them, rather than taking the bigger picture into account.
This usually presents itself on an incident call with an SME constantly bringing up the same issue without listening to instructions from the incident commander, and having tunnel vision for the specific issue on their system.
Try not to get tunnel vision or chase red herrings. Always keep the bigger picture in mind.
---
### Requiring Deeply Technical ICs
![069](../../assets/slides/incident_response/incident_response.069.jpeg)
_069. Requiring deeply technical incident commanders. [Docs Reference](../../resources/anti_patterns.md#requiring-incident-commanders-to-have-deep-technical-knowledge)_
We used to require that all of our Incident Commanders be experienced engineers with deep technical knowledge of all PagerDuty systems. **This was one of our bigger mistakes.** Remember that IC's aren't responders, they aren't the ones actually fixing the problem, so they don't need deep technical knowledge. IC's are experts at coordinating the response, not at solving technical issues. You should be relying on your SMEs for that.
**Removing the restriction on technical knowledge led to a dramatic increase in our pool of available incident commanders**, and didn't have any effect on our ability to respond to incidents. We now have ICs from all across the organization, with even more currently in training.
It's already hard enough to get people to want to be an IC, so don't add further unnecessary restrictions to your pool.
---
### Taking on Multiple Roles
![070](../../assets/slides/incident_response/incident_response.070.jpeg)
_070. Taking on multiple roles. [Docs Reference](../../resources/anti_patterns.md#trying-to-take-on-multiple-roles)_
In past PagerDuty incidents, we've had instances where the Incident Commander has started to assume the Subject Matter Expert role and attempted to solve the problem themselves. This typically happens when an engineer is the IC, and the incident is something to do with a system they helped to build. It's very tempting to say "I know how to fix this!" and jump in and solve the problem yourself. But you cannot do that as an IC.
Inevitably, the incident will be bigger than you think, and while you're trying to fix you little fire, there's another one happening in another service and you've lost sight of the bigger picture.
You cannot take on another role at the same time as being an Incident Commander. If you absolutely are the only person who can solve the problem, then handover to another IC and assume the role of an SME for the remainder of the incident.
---
### Litigating Policy
![071](../../assets/slides/incident_response/incident_response.071.jpeg)
_071. Litigating policy during an incident. [Docs Reference](../../resources/anti_patterns.md#discussing-process-and-policy-decisions-during-the-incident-call)_
Just like with severities, policy and processes should not be discussed during an incident. The current process should be followed, and any concerns should be raised afterwards, either during a postmortem or directly to the team managing the incident response process.
Trying to change the process during an incident is only going to prolong the current incident. That's not the time to have that discussion.
---
### Averse to Process Changes
![072](../../assets/slides/incident_response/incident_response.072.jpeg)
_072. Being averse to process changes. [Docs Reference](../../resources/anti_patterns.md#being-averse-to-policy-and-process-changes)_
Finally, once a stable process is in place and incidents are getting resolved, there can be lots of hesitation and resistance to changing that process. "If it ain't broke don't fix it". As your company grows, your response will need to change. Holding on to your old processes and practices for too long can hinder your incident response going forward. Don't be reckless, of course, but try to introduce sensible changes and don't be afraid to make changes which might slow things down in the short-term, but will make things faster in the long-run. These are the hardest changes to make, but ultimately the most worthwhile.
---
### Resolved
_![073](../../assets/slides/incident_response/incident_response.073.jpeg)_
_073. Resolved._
OK, so if all goes well, you're incident will get resolved. That means we're all done and we can go home, right?!
Well, not quite yet. There's still one more thing we need to do.
---
### Don't Neglect The Postmortem
![074](../../assets/slides/incident_response/incident_response.074.jpeg)
_074. Don't neglect the postmortem. [Docs Reference](../../resources/anti_patterns.md#neglecting-the-post-mortem-and-followup-activities)_
We need to do a [postmortem](../../after/post_mortem_process.md). Or after-action review, learning review, retrospective, incident report, etc. Whatever you want to call it, the name doesn't matter as much as actually doing one!
Don't make the mistake of neglecting a postmortem after an incident. Without a postmortem you fail to recognize what you're doing right, where you could improve, and most importantly, how to avoid making the same exact mistakes next time around. A well-designed, blameless postmortem allows teams to continuously learn, and serves as a way to iteratively improve your infrastructure and incident response process.
Postmortems are an important followup action and should never be missed. Even if you triggered an incident and then decided it was a false alarm, you should still do a brief postmortem. You just mobilized a response when you didn't need to, so you want to identify how you can make sure that doesn't happen again.
---
### Create the Postmortem
![075](../../assets/slides/incident_response/incident_response.075.jpeg)
_075. Create the postmortem. [Docs Reference](../../after/post_mortem_template.md)_
The first step is to create the postmortem itself. This is the job of the Incident Commander. I don't mean they're going to write the entire postmortem, they're just going to create the [initial template](../../after/post_mortem_template.md). We want to make sure that a link exists so that when people ask "When will we know what went wrong?" you have something to give them.
---
### Pick an Owner
![076](../../assets/slides/incident_response/incident_response.076.jpeg)
_076. Pick an owner._
Then the IC needs to assign an owner. Remember how we assigned tasks to specific individuals? It’s no different with a postmortem. Make sure there’s a clear owner, and that it's an **individual** and not a team. The most surefire way to make sure a postmortem doesn't get completed is to assign it to a team instead of a specific person.
The person you assign is responsible for completing the postmortem, but they don't have to do it all themselves. They can delegate out sections as they see fit. But you need someone on the hook for making sure it gets finished.
As with assigning other tasks, you also want to give them a deadline, and make sure they've understood that they're responsible for completing the postmortem.
---
### Blameless
![077](../../assets/slides/incident_response/incident_response.077.jpeg)
_077. Blameless._
Importantly, postmortems need to be **blameless**. If someone made a mistake, you just spent lots of money training them to never do it again. **You can’t fire your way to reliability.**
Let's say Bob ran a command which deleted your entire database. Your postmortem shouldn't be "Bob made a mistake and should be fired or have his access revoked!". The postmortem should be "Why is our system configured in a way which  allowed a single user to delete the entire database?".
If you name and shame people in a postmortem, it demotivates everyone. Next time someone makes a mistake, they're not going to own up to it, because they'll be afraid of getting shamed too. You want people to bring up problems, because then you get to fix them quickly.
---
### Review the Process
![078](../../assets/slides/incident_response/incident_response.078.jpeg)
_078. Review the process too!_
Don’t forget to also review the process as part of the postmortem. How can you change the process to make it better? What isn’t working out well? Just as it’s important to learn from and fix mistakes in your software, you want to do the same for your incident response process.
---
### Practice
_![079](../../assets/slides/incident_response/incident_response.079.jpeg)_
_079. Practice makes perfect._
Finally, you want to practice your incident response process as much as you can. **You don't want to be doing it for the first time during a real incident.** Reading about it is one thing, but going through the motions is very different.
Start by running mock incidents. Then treat your smaller incidents as if they're larger ones. If you trigger incident response and find it's not a real incident, treat it like one anyway since it's free practice.
At PagerDuty, we run something called [Failure Friday](https://www.pagerduty.com/blog/failure-friday-at-pagerduty/) where we purposefully inject failure into our systems to test their resilience. We treat this like a major incident, with an incident commander and everything. It allows us to practice while we're not under the stress of a normal incident.
We also play a game called [Keep Talking And Nobody Explodes](https://www.keeptalkinggame.com). Yes, that's right, we play video games at work. But we've found that this game really helps to simulate a lot of the things an incident commander has to deal with, and is a great way to get some stress free practice.
The bottom line is to practice as much as you can, so that when you do have the inevitable incident, your response is just routine.
---
### Open-Source Response Docs
![080](../../assets/slides/incident_response/incident_response.080.jpeg)
_080. Our open-source incident response documentation._
This was just a brief taste of the training we run at PagerDuty for our own Incident Commanders. We had nowhere near enough time to cover everything.
Good news though! We have published our entire incident response process online. It is an exact copy of our internal documentation only with things like phone numbers removed. It's complete free to use, and is open-sourced under an Apache 2 license so you can use it in your own organizations. [It's on GitHub](https://github.com/PagerDuty/incident-response-docs) and we do accept pull requests if you spot any mistakes or have improvement suggestions.
Everything I've talked about today can be found in the documentation, and there's lots of great [additional reading material](../../resources/reading.md) if you want to learn more.
---
### Response Docs Image
![081](../../assets/slides/incident_response/incident_response.081.jpeg)
_081. Response docs screenshot._
It looks pretty too.
---
### Summary
![082](../../assets/slides/incident_response/incident_response.082.jpeg)
_082. Summary._
Incident command training is useful in so many situations outside of a server exploding in the night. It can be applicable to many different things in your life, whether it's staying calm after a fender bender on the highway, or jumping into action to help during a major natural disaster. In my own life I've regularly compared the role of a parent with that of an Incident Commander. You'd be surprised at how useful it can be in everyday situations.
Anyway, with that, I'll leave you with a quick summary of the main things we discussed today. Thanks!
???+ aside hide-arrow "Questions?"
    If you have questions about this training material, feel free to ask me on Twitter, I'm [@r_adams](https://twitter.com/r_adams).
---
### Image Credits
![083](../../assets/slides/incident_response/incident_response.083.jpeg)
_083. Image credits._
???+ aside hide-arrow "Image Credits"
    Here are the credits for all the images used throughout this training material.
---
### PagerDuty University
![084](../../assets/slides/incident_response/incident_response.084.jpeg)
_084. PagerDuty University._
???+ aside hide-arrow "PagerDuty University"
    Shameless plug: If you're interested in our longer courses on this and other topics, including how to use PagerDuty to do it, we offer a variety of different training programs as part of [PagerDuty University](https://university.pagerduty.com/) — from private full-day courses at your own offices, to public instructor-led training.
---
### Video
__
???+ aside hide-arrow full-width "PagerDuty Summit Series Chicago 2017"
    As a special bonus for making it to the end, here's a recording of an earlier version of this training given at a PagerDuty event in September 2017. The material will differ slightly from that shown on this website, as we have made changes and refined the content since then. But it should give you a decent idea of how the course is usually presented.