(mod p).
(1)
The Lucas probable prime test repeatedly tests property (1) for diﬀerent pairs
(P, Q). This leads to the notion of a Lucas pseudoprime with respect to such a
pair.
Deﬁnition 2 (Lucas pseudoprime). Let n be a composite number such that
gcd(n, 2QD) = 1. If Un−( D
with respect to parameters (P, Q).
n ) ≡ 0 (mod n), then n is called a Lucas pseudoprime
Similar results to those for the MR primality test can be established for the
Lucas test: a single Lucas test will declare a given composite number as being
composite with probability at least 1−(4/15) and as being prime with probability
at most (4/15), with these bounds being tight [Arn97].
Baillie-PSW The Baillie-PSW test [PSW80] is a deterministic primality test
consisting of a single Miller-Rabin test with base 2 followed by a single Lucas
test. A slight variant of the test in which the Lucas test is replaced with a more
stringent version, known as a strong Lucas test is mentioned in [BW80]. Generally,
the consensus that has emerged over time is that the Lucas test should be used
with the parameters (P, Q) set as deﬁned by Selfridge’s method A:
of the sequence 5,−7, 9,−11, 13, . . . for which(cid:0) D
Deﬁnition 3 (Selfridge’s Method A [BW80]). Let D be the ﬁrst element
Q = (1 − D)/4.
(cid:1) = −1. Then set P = 1 and
n
If no such D can be found, then n must be a square and hence composite.
In practice, one might attempt to ﬁnd such a D up to some bound Dmax, then
perform a test for squareness using Newton’s method for square roots (see
Appendix C.4 of [KSD13]), before reverting to a search for a suitable D if needed.
This is generally more eﬃcient than doing a test of squareness ﬁrst.
The idea of the Baillie-PSW test is that its two components are in some
sense “orthogonal” and should between them catch all composites. Extensive
computations have never produced a pseudo-prime for the Baillie-PSW test,
that is, a composite number that passes it. Indeed there are (moderate) cash
prizes available for providing one. However, none of these computations extend
to numbers of cryptographic size. Moreover, Pomerance [Pom84] has given a
heuristic argument for the existence of inﬁnitely many Baillie-PSW pseudo-primes.
There do not appear to exist any bounds demonstrating the test’s strength on
uniformly random k-bit inputs, in contrast to the results of [DLP93] for the MR
test. In summary, while the Baillie-PSW test appears to be very strong, there
are no proven guarantees concerning its accuracy. One positive feature is that,
being deterministic, it does not consume any randomness (whereas a properly
implemented MR test does).
equivalently, checking gcd((cid:81)r
Supplementary and Preliminary Tests It is often more eﬃcient to perform
some supplementary or preliminary testing on an input n before executing the
main work of the primality test. A common strategy is to ﬁrst perform trial
division on n using a list of r small primes. This can be done directly or by or
i pi, n) (cid:54)= 1 where {p1, . . . , pr} is the list of primes
used. The list of primes can be partitioned and multiple gcds computed, so as to
match the partial products of primes with the machine word-size. This is a very
cheap test to perform, and can be quite powerful when testing random inputs.
The question arises of how r, the number of primes to use in trial division, should
be set. We shall return to this question later.
Primality Testing in OpenSSL Since we will extensively compare our pri-
mality test and its API with those of OpenSSL, we give a detailed description of
OpenSSL’s approach here.
OpenSSL provides two functions for primality testing: BN is prime ex and
BN is prime fasttest ex, both in ﬁle bn prime.c. The core part of the code is
in the second of these, while the ﬁrst simply acts as a wrapper to this function
that forces omission of trial division. The second function call has the form:
int BN is prime fasttest ex(const BIGNUM *w, int checks, BN CTX *ctx passed,
int do trial division, BN GENCB *cb)
Here, w is the number being tested. The option to do trial division is deﬁned via
the do_trial_division ﬂag. When set, the function will perform trial division
using the ﬁrst 2047 odd primes (excluding 2), with no gcd optimisations (the
code also separately tests whether the number being tested is equal to 2 or 3,
and whether it is odd). After this, the function calls bn miller rabin is prime
to invoke the MR testing with pseudo-random bases. The number of MR rounds
is set using the argument checks. When checks is set to BN prime checks, a
value that defaults to zero, then the number of MR rounds is chosen such that
the probability of the test declaring a random composite number n with k bits
as being prime is at most 2−λ, where λ is the security level that a 2k-bit RSA
k
t
λ (bits)
k ≥ 3747
k ≥ 1345
k ≥ 476
k ≥ 400
k ≥ 347
k ≥ 308
k ≥ 55
k ≥ 6
3
4
5
6
7
8
27
34
192
128
80
80
80
80
64
64
Table 1. The default number of rounds t of Miller-Rabin performed by OpenSSL when
testing k-bit integers determined by the function BN prime checks for size and the
associated bits of security λ.
modulus should provide. Thus, the number of MR rounds performed is based on
the bit-size k, as per Table 1. The entries here are based on average case error
estimates taken from [MVOV96], which in turn references [DLP93].
2.2 Prime Generation
A critical use case for primality testing is prime generation (e.g. for use in RSA
keys). The exact details of the algorithms used vary across implementations,
but the majority follow a simple technique based on ﬁrst generating a random
initial candidate n of the desired bit size k, possibly setting some of its bits, then
doing trial division against a list of small primes, before performing multiple
rounds of primality testing using a standard probabilistic primality test such
as MR. If the trial division reveals a factor or the MR test fails, then another
candidate is generated. This can be a fresh random value, but more commonly,
implementations add 2 to the previous candidate n. This allows an important
optimisation: if a table of remainders for the trial divisions of n is created in
the ﬁrst step, then this table of remainders can be quickly updated for the new
candidate n + 2. Fresh divisions can then be avoided – one just needs to inspect
the updated table of remainders. We refer to this procedure as trial division by
sieving or just sieving. It is, of course, much more eﬃcient than performing trial
divisions anew for each candidate. Note that this approach leads to a slightly
non-uniform distribution on primes: primes that are preceded by a long run of
composites are more likely to result from it than primes that are close to their
preceding primes. However, it is known that the deviation from the uniform
distribution is small [BD93].
OpenSSL OpenSSL adopts the above high-level procedure, with one important
diﬀerence. The code is found in BN generate prime ex in ﬁle bn prime.c. The
function call has the following form:
int BN generate prime ex(BIGNUM *ret, int bits, int safe, const BIGNUM
*add, const BIGNUM *rem, BN GENCB *cb)
Here bits is the desired bit-size, safe is a ﬂag that, when set, asks the function
to produce a safe prime p = 2q + 1, and add and rem allow the callee to set
additional conditions on the returned prime. We will ignore safe, add and rem
in our further work; an analysis of how they aﬀect prime generation when using
our primality test is left to future work.
The initial steps are performed together in a separate function called probable-
prime. A cryptographically strong pseudo-random number is ﬁrst generated
by BN priv rand. The two most signiﬁcant bits and the least signiﬁcant bit are
then set to ensure the resulting candidate n is odd and of the desired bit-size.
This number is then sieved using a hard-coded list of the ﬁrst 2047 odd primes
p2, . . . , p2048, so p1 = 2, p2 = 3, . . . , p2048 = 17863. If a candidate passes the siev-
ing stage, it is tested for primality by BN is prime fasttest ex. This function
carries out the default number of Miller-Rabin rounds, as per Table 1. Trial
division is omitted by setting the do_trial_division ﬂag in the function call.
This is because trial division has already been carried out externally via sieving.
This exploits the complexity of the OpenSSL API for primality testing to gain
performance, an option not available if a simpliﬁed API is desired (as we do).
Importantly, if the MR tests fail, then instead of going to the next candidate
that passes sieving, a fresh, random starting point is selected and the procedure
begins again from the start.
3 Construction and Analysis of a Primality Test With a
Misuse-resistant API
We now propose how to construct a performant primality test with a misuse-
resistant API. Our design goal is to ensure good performance in the most
important use cases (malicious input testing, prime generation) while still main-
taining strong security. At the same time, we want the simplest possible API for
developers: a single input n (the number being tested) and single a 1-bit output
(0 for composite, 1 for probably prime).
We propose four diﬀerent primality testing functions, all built from the
algorithms described in Section 2.1. The ﬁrst of these follows OpenSSL with
its default settings, and we name this Miller-Rabin Average Case (MRAC). It
provides a baseline for analysis and comparison. The second and third use 64 and
128 rounds of MR testing, respectively. We name them MR64 and MR128. The
fourth uses the Baillie-PSW test, and we name it BPSW for short. For each of these
four options, we provide an assessment (both by analysis and by simulation) of
its security and performance when considering random composite, random prime,
and adversarially generated composite inputs. We also consider the inﬂuence of
trial division on each test’s performance. For concreteness, throughout we focus
on the case of 1024-bit inputs, but of course the results are easily extended to
other bit-sizes.
3.1 Miller-Rabin Average Case (MRAC)
The ﬁrst test we introduce, MRAC, is a reference implementation of OpenSSL’s
primality test, as per the function BN is prime fasttest ex described in Sec-
tion 2.1 with input checks set to BN prime checks, so that the number of MR
rounds performed is based on the bit-size k, as per Table 1. Recall that this
function either does no trial division or does trial division with the ﬁrst 2047
odd primes. Of course, this test is quite unsuitable for use in general, because it
performs badly on adversarial inputs: [AMPS18] showed that it has a worst case
false positive rate of 1/22t where for example t = 5 for 1024-bit inputs. On the
other hand, it is designed to perform well on random inputs.
MRAC on Random Input We now consider the expected number of MR
rounds performed when receiving a random 1024 bit odd input. For now, we
ignore the eﬀect of trial division. The probability that a randomly chosen odd
k-bit integer is prime is qk := 2/ ln(2k) by standard estimates for the density of
primes (for k = 1024, qk ≈ 1/355). In this case MRAC will do t MR rounds, as
per Table 1. Otherwise, for composite input, up to t rounds of MR testing will be
done. One could use the bounds from [DLP93] to obtain bounds on the expected
number of MR rounds that would be carried out on composite input. However, for
numbers of cryptographic size (e.g. k = 1024 bits), to a very good approximation,
the number needed is just 1, since with very high probability, a single MR test is
suﬃcient to identify a composite (recall that the probability that a single round
of MR testing fails to identify a 1024-bit composite is less than 2−40!). From this,
one can compute the expected number of rounds needed for a random, odd input:
it is approximately the weighted sum t · qk + 1 · (1 − qk) = 1 + (t − 1)qk. For
k = 1024, we have t = 5 and qk = 0.0028, and this expression evaluates to 1.026.
MRAC on Random Input with Trial Division Now we bring trial division
into the picture. Its overall eﬀectiveness will be determined by the collection
of small primes in the list P = {p1, p2, . . . , pr} used in the process (where we
assume all the pi are odd) and the relative costs of MR testing and trial division
(about 800:1 in our experiments).
For random odd inputs, the fraction σ(P ) of non-prime candidates that are
removed by the trial division of the primes in P can be computed using the
formula:
(cid:19)
σ(P ) = 1 − r(cid:89)
(cid:18)
1 − 1
pi
i=1
.
(2)
This means that any candidate that passes the trial division stage is 1/(1− σ(P ))
times more likely to be a prime than an odd candidate of equivalent bit-size
chosen at random. But simply adding more primes to the list P is not necessarily
eﬀective: fewer additional composites are removed at a ﬁxed cost (one additional
trial division per prime), and eventually it is better to move on to a more
heavyweight test (such as rounds of MR testing). Moreover, from inspecting the
formula for σ(P ), it is evident that, for a given size r of set P (and hence a
given cost for trial division), it is better to set P as containing the r smallest
odd primes (including 2 is not useful as the input n is already assumed to be
odd). Henceforth, we assume that when P is of size r, then it consists of the ﬁrst
r odd primes. We write σr in place of σ(P ) in this case. Using Mertens’ theorem,
we can approximate σr as follows:
σr ≈ 1 − 2e−γ/ ln(pr).
where γ = 0.5772 . . . is the Euler-Mascheroni constant.
As an example, BN is prime fasttest ex in OpenSSL performs trial division
on the ﬁrst 2047 odd primes (ending at p2047 = 17863). As shown in Figure 1,
using the ﬁrst r = 2047 primes gives a value of σ2047 = 0.885. This is only a little
larger than using, say, the r = 128 smallest primes yielding σ128 = 0.831.
Now we build a cost model for MRAC including trial division. This will also
be applicable (with small modiﬁcations) for our other tests.
Let Ci denote the cost of a trial division for prime pi and let CM R denote
the cost of a single MR test.6 Then the total cost of MRAC on random prime
k-bit inputs is:
Ci + t · CM R
(3)
r(cid:88)
i=1
since the test then always performs all r trial divisions (assuming k is large
enough) and all t MR tests. For random, odd composite inputs, the average cost
is approximately:
σ1 · C1 + (σ2 − σ1) · (C1 + C2) + . . . + (σr − σr−1) · (C1 + ··· + Cr)
r(cid:88)
i=1
r(cid:88)
+(1 − σr) · (
Ci + CM R).
This is because a fraction σ1 of the composites are identiﬁed by the ﬁrst trial
division, a further fraction σ2 − σ1 are identiﬁed after 2 trial divisions, etc, while
a fraction (1− σr) require all r trial divisions plus (roughly) 1 round of MR. Here
we assume that the MR test performs in the same way on numbers after trial
division as it does before. After some manipulation, this last expression can be
simpliﬁed to:
(1 − σi−1) · Ci + (1 − σr) · CM R
(4)
i=1
where we set σ0 = 0. This expression can be simpliﬁed further if we assume that
the Ci are all equal to some CT D (a good approximation in practice), and apply
Mertens’ theorem again. For details, see the equivalent analysis in [Mau95].
From expressions (3) and (4), the expected cost for random, odd, k-bit input
can be easily computed via a weighted sum with weights qk and 1 − qk. However,
6 In practice, we could set Ci to be a constant CT D for the range of i we are interested
in, but using a more reﬁned approach is not mathematically much more complex.
Fig. 1. Proportion of candidates removed by trial division, σr, as a function of r, the
number of primes used.
the cost is dominated by expression (4) for the composite case. From (4), the
futility of trial division with many primes is revealed: adding a prime by going
from r to r + 1 on average adds a term (1 − σr) · Cr+1, but only decreases by
a fraction σr+1 − σr the term in front of CM R. As can be seen from Figure 1,
when r is large, 1 − σr is around 0.1, while σr+1 − σr becomes very small. So
each increment in r only serves to increase the average cost by a fraction of a
trial division (and with the cost of trial division increasing with r).
Figure 2 shows a sample (theoretical) plot of the average cost of MRAC
as a function of r for k = 1024. This uses as costs CT D = 0.000371ms and
CM R = 0.298ms obtained from our experiments (reported below) for k = 1024 and
the weighted sum of expressions (3), (4). This curve broadly conﬁrms the analysis