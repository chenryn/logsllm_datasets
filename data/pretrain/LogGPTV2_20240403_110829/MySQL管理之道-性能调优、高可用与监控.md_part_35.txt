（4）加载安装DRBD模块
命令如下：
（2）修改/etc/hosts主机名
命令如下：
口协议B：内存同步（半同步）复制协议。本地写成功并将数据发送到对方后立即返回
口协议A：异步复制协议。本地写成功后立即返回，数据放在发送 buffer中，可能丢失。
DRBD有如下3种复制模式：
#modprobedrbd
#makeinstall
#./configure--prefix=/-with-km
192.168.8.23
192.168.8.22
[root@vm02 drbd.d]#cat/etc/hosts
/dev/sdb1
Units=cylindersof16065*512=8225280bytes
Disk/dev/sdb:1073MB,1073741824bytes
Command(mforhelp):p
[root@vm01~]#fdisk/dev/sdb
口
make
DeviceBoot
般用协议C。
坏，则数据可能丢失。
协议C：同步复制协议。本地和对方写成功确认后返回。如果双机掉电或磁盘同时损
如果双机掉电，数据可能丢失。
Start
vm02
vm01
130
End
1044193+83Linux
BlocksIdSystem
，下面将演示其安装以及切换方式。
---
## Page 265
机器上。
（6）启动DRBD
#cat/proc/drbd（查看状态）
#drbdsetup/dev/drbd1primary-o（这一步只在主服务器上做）
#/etc/init.d/drbdstart
（如果报错，执行ddif=/dev/zerobs=1Mcount=1of=/dev/sdb1）
#drbdadm create-mdro
可通过如下命令启动DRBD：
在此步骤，
#设置主备节点同步时的网络速率最大值，单位是字节
global{usage-count yes;}
（5）编辑配置文件
如果有上面的结果，
[root@vm01~]#lsmod|grep drbd
通过lsmod来检查是否已经执行成功，命令如下：
common{syncer{rate100M;}}
#cat/etc/drbd.conf
drbd
一个DRBD设备（/dev/drbdx)，
address 192.168.1.67:7789;
device/dev/drbd1;
#设置DRBD的监听端口，用于与另一台主机通信
#/dev/drbd1使用的磁盘分区是/dev/sdb1
meta-disk internal;
address 192.168.1.66:7789;
device
shared-secret"FooFunFactory";
cram-hmac-algsha1;
#
disk
neta-disk internal;
#每个主机的说明以"on"开头，后面是主机名。后面的0中为这个主机的配置
onvm02{
onvm01{
设置主备机之间通信使用的信息算法
#使用协议C。表示收到远程主机的写入确认后，则认为写入完成
disk
net
protocolC;
/dev/sdb1;
/dev/sdb1;
/dev/drbd1;
：要保证主从服务器上的drbd.conf文件一致，可以用 scp复制一份到vm02
则表示成功了。
2816360
第7章目前流行的4种高可用架构·251
---
## Page 266
252
注意
·第三部分高可用集群管理篇
注意
在 secondary 机器上，执行操作，切换角色为 primary:
test 为定义的资源名字。
在 primary 机器上，执行操作，切换角色为 secondary：
（9）验证secondary备机是否有数据写人
（8）挂载DRBD分区
开机不要自启动DRBD服务，因为要通过Heartbeat来启动DRBD。
#格式化DRBD文件系统为EXT3，只在主机上做，从机上不用做
在执行过程中，如果出现上述信息，代表操作成功。
#mkfs.ext3/dev/drbd1
（7）格式化DRBD文件系统
在执行过程中，如果出现上述信息，代表操作成功。
drbdadmsecondarytest
命令如下：
GIT-hash:89a294209144b68adb3ee85a73221f964d3ee515 build by root@vm02, 2013-06-1515:27:44
在备机上查看DRBD 状态，如下所示：
GIT-hash: 89a294209144b68adb3ee85a73221f964d3ee515 build by root@vm01, 2013-06-15 13:05:11
在主机上查看DRBD 状态，如下所示：
tmpfs
/dev/sda1
文
1-4-JP#[OW@100]
[root@vm01~]#mount/dev/drbd1/data
dev/drbd1
version:8.4.3(api:1/proto:86-101)
[root@vm02~]#cat/proc/drbd
version:8.4.3(api:1/proto:86-101)
1:cs:Connectedro:Secondary/Primary ds:UpToDate/UpToDateCr--
[root@vm01~]#cat/proc/drbd
1:cs:Connectedro:Primary/Secondaryds:UpToDate/UpToDateCr----
件系统
ns:1044124nr:0 dw:0 dr:1044124 al:0bm:64lo:0pe:0ua:0 ap:0ep:1wo:f 00s:0
ns:0 nr:1044124 dw:1044124 dr:0al:0bm:64 lo:0 pe:0ua:0 ap:0 ep:1 wo:f 00s:0
tmpfs
类型
ext3
ext3
1004M
464M
容
18M
已用
936M
464M
已用%
挂载点
/data
/dev/shm
---
## Page 267
动连接：
修复了：
脑裂后，两个节点间的数据将不再同步，主从关系失效，这时，就需要按下面的步骤
#make&&make install
2）在主节点上，通过cat /proc/drbd查看状态，如果不是WFConnection 状态，需要手
#cat ha.cf
#cat authkeys
进人配置文件目录，按照如下输出信息修改：
#tar zxvf heartbeat-2.1.4.tar.gz
安装命令如下：
要先安装DRBD，然后才能安装Hearbeat，DRBD 是第一个环节。
（11）安装Heartbeat（两台机器都安装）
#drbdadm connect test
1）在从节点上进行如下操作：
（10）DRBD脑裂后的处理
接下来编辑配置文件，使用如下命令：
#drbdadmsecondarytest
最后挂载DRBD 分区，进行如下验证：
logfacilitylocalo
ngap-y/o/ien/aynga
auth1
#./configure
#cd libnet
#tar zxvf libnet.tar.gz
#drbdadm----discard-my-dataconnecttest
aa.txtlost+found
[root@vm02~]#mount/dev/drbd1/data
drbdadmprimarytest
ogfile/var/log/ha-log
1crc
#cd/etc/ha.d/
#make&&makeinstall
#./ConfigureMe configre
#cdHeartbeat-STABLE-2-1-STABLE-2.1.4
第7章
目前流行的4种高可用架构·253
---
## Page 268
254
●第三部分高可用集群管理篇
个节点切换到另一个节点，节点故障转移功能对客户端来说是透明的，从而保证应用持续
统硬件、网络出现故障时，应用可以通过RHCS提供的高可用管理组件自动、快速地从一
决方案，可以给数据库应用等提供安全、稳定的运行环境。当应用程序出现故障，或者系
7.3
去，仍旧会停留在 slave上面，这样避免了多次切换造成业务的两次中断。
就会漂移到slave上，DRBD 分区会自动挂载，等把原来的master修复好，VIP不会漂移过
Keepalived复杂些，因为它要安装两个软件。如果把master关机或者把MySQL停掉，VIP
不能进入系统的麻烦。启动Heartbeat 前，要先把DRBD主从状态改为 secondary/secondary，
注意
DRBD的分区目录。
192.168.8.100为虚拟VIP，MySQL为/etc/init.d/mysql启动脚本。
RHCS是Red Hat Cluster Suite（红帽集群套件）的缩写，它能够提供、高可靠的HA解
到这里Heartbeat+DRBD+MySQL架构已经搭建完毕，相对来说，其安装、搭建步骤比
开机时不要自启动Heartbeat服务，进入系统后手工启动，这样可避免因启动报错而带来
在此架构中，MySQL的安装启动跟单机的安装步骤一样，区别是把datadir指定为
#/etc/init.d/heartbeatstart
之后就可以启动Heartbeat了（两台都执行），命令如下：
在上面的输出中，vm01为主机名，test为资源名，Filesystem为文件系统和挂载的目录，
vm01 drbddisk:test Filesystem:/dev/drbd1:/data:ext3192.168.8.100mysql
#catharesources
nopfudge
apiauthipfailgid=haclientuid=hacluster
respawnhacluster/usr/lib/heartbeat/ipfail
ping_group group1192.168.8.22192.168.8.23
ucasteth0192.168.8.23（改为对方节点的IP）
udpport694
nodevm2
nodevm1
auto_failbackon（备机节点改为off，否则当主机岩机后，从机接管，主机恢复后，从机释放资源，主机再接管）
initdead12
warntime4
deadtime
keepalive
红帽RHCS共享存储架构的搭建演示
N
---
## Page 269
IBM RSAII卡、HP的iLO卡、IPMI的设备等，外部Fence设备有UPS、SAN SWITCH、
设备，将异常节点占据的资源进行了释放，保证了资源和服务始终运行在同一个节点上。
信息给备机，备机在接到Fence成功的信息后，开始接管主机的服务和资源。这样通过Fence
或者外部电源管理设备，来对服务器或存储直接发出硬件管理指令，将服务器重启或关机
去。RHCS 的配置文件是cluster.conf，它是一个XML文件，具体包含集群名称、集群节点
中的每个节点上，时刻保持每个节点的配置文件同步。例如，管理员在节点A上更新了集
件/etc/cluster/cluster.conf的状态，不管这个文件发生何种变化，它都将此变化更新到集群
空间的并行锁模式。
地的，不需要网络请求，因而请求会立即生效。最后，DLM可以通过分层机制实现多个锁
时，DLM还避免了单个节点失败时需要整体恢复的性能瓶颈，另外，DLM的请求都是本
CLVM通过锁管理器来同步更新数据到LVM卷和卷组。
DLM运行在集群的每个节点上，GFS通过锁管理器的锁机制来同步访问文件系统元数据，
员间的关系将发生改变时，CMAN会及时将这种改变通知给底层，进而做出相应的调整。
控每个节点的运行状态来了解节点成员之间的关系，当集群中某个节点出现故障，节点成
节点上，为RHCS 提供集群管理任务。CMAN用于管理集群成员、消息和通知。它通过监
不间断地对外提供服务，
备
或者与网络断开连接。
出现，就是为了解决类似问题的，Fence设备主要通过服务器或存储本身的硬件管理接口
Fence设备可以避免因出现不可预知的情况而造成的“脑裂”现象，事实上，Fence设备的
信息、集群资源和服务信息、Fence设备等。
群配置文件，CCS发现A节点的配置文件发生变化后，马上会将此变化传播到其他节点上
件在节点之间的同步。CCS运行在集群的每个节点上，监控每个集群节点上的单一配置文
一个底层基础构件，同时也为集群提供了一个公用的锁运行机制。在RHCS集群系统中，
RHCS的Fence设备可以分为两种：内部Fence和外部Fence，常用的内部Fence有
DLM不需要设定锁管理服务器，它采用对等的锁管理方式，大大提高了处理性能。
CMAN即 Cluster Manager 的简称，它是一个分布式集群管理工具，运行在集群的各个
然后通过Fence设备将异常主机重启或者从网络隔离，在Fence操作成功执行后，返回
RHCS是一个集群工具的集合，主要由下面几大部分组成。
Fence的工作原理是：当意外原因导致主机异常或者岩机时，备机会首先调用Fence设
Fence为远程管理卡的简称，该设备是RHCS 集群中必不可少的一个组成部分，通过
4.栅设备（Fence）
CCS为Cluster Configuration System的简称，它主要用于集群配置文件管理和配置文
3.配置文件管理（CCS）
DLM为DistributedLockManager的简称，它表示一个分布式锁管理器，是RHCS 的
2.锁管理（DLM）
1.分布式集群管理器（CMAN）
，这就是RHCS高可用集群实现的功能。
第7章目前流行的4种高可用架构·255
同
---
## Page 270
256
·第三部分高可用集群管理篇
这个文件进行操作，另外，当一个节点在GFS文件系统上修改数据时，这种修改操作会通
操作，直到这个写进程正常完成，才会释放锁，只有在锁被释放后，其他读写进程才能对
操作，当一个写进程操作一个文件时，这个文件就被锁定，此时不允许其他进程进行读写
GFS是RHCS提供的一个集群文件系统，多个节点可同时挂载一个文件系统分区，而文件
ccs_tool、fence_tool、clusvcadm 等。
ricci，luci安装在一台独立的计算机上，用于配置和管理集群，ricci安装在每个集群节点上,
Conga是通过Web方式来配置和管理集群节点的。Conga由两部分组成，分别是luci和
群节点配置和集群管理两个部分组成，分别用于创建集群节点配置文件和维护节点运行状
Conga 等，它们也提供了基于命令行的管理工具。
集群服务其实就是应用服务，例如，Apache、MySQL等，集群资源有很多种，例如，一个
器上对应的进程为clurgmgrd。在一个RHCS 集群中，包含集群服务和集群资源两个方面，
对集群服务的管理能力，当一个节点的服务失败时，高可用集群服务管理进程可以将服务从
NETWORK SWITCH等。
系统数据不受破坏，这是单一的文件系统，是EXT3、EXT2所不能做到的。
上共享存储，每个节点通过共享一个存储空间，保证了访问数据的一致性，更切实地说，
luci通过ricci和集群中的每个节点进行通信。
务在节点间转移的顺序，还可以限制某个服务仅在失败转移域指定的节点内进行切换。
级，那么集群高可用服务将在任意节点间转移。因此，通过创建失败转移域不但可以设定服
先级，通过优先级的高低来决定节点失败时服务转移的先后顺序，如果没有给节点指定优先
域是一个运行特定服务的集群节点的集合。在失败转移域中，可以给每个节点设置相应的优
IP地址、一个运行脚本、EXT3/GFS文件系统等。
这个失败的节点上转移到其他健康的节点上来，并且这种服务转移能力是自动、透明的。
为了实现多个节点对于一个文件系统同时进行读写操作，GFS使用锁管理器来管理I/O
RHCS 也提供了一些功能强大的集群命令行管理工具，常用的有clustat、cman_tool、
在RHCS集群中，高可用服务管理器是和一个失败转移域结合在一起的，所谓失败转移
RHCS 通过rgmanager来管理集群服务，rgmanager 运行在每个集群节点上，其在服务
，一般用在RHCS早期的版本中。
system-config-cluster是一个用于创建集群和配置集群节点的图形化管理工具，它由集
Conga是一种新的基于网络的集群配置工具，与 system-config-cluster不同的是，
RHCS 提供了多种集群配置和管理工具，常用的有基于GUI的 system-config-cluster、
高可用服务管理器主要用来监督、启动和停止集群的应用、服务和资源。它提供了一种
GFS是RHCS为集群系统提供的一个存储解决方案，它允许集群的多个节点在块级别
5.高可用服务管理器
7.Redhat GFS
6．集群配置管理工具
---
## Page 271
7.3.1安装过程
器岩机后，备机接管服务。
机器处于master状态，另一台处于 standby状态，也就是说只有一台机器提供业务，当机
文件系统的节点时必须安装RHCS组件。
才需要GFS支持，而搭建GFS集群文件系统必须要有RHCS的底层支持，所以安装GFS
般初学者很容易混淆这个概念：运行RHCS时，GFS不是必需的，只有在需要共享存储时，
RHCS管理工具对GFS进行配置和管理。这里需要说明的是RHCS 和GFS之间的关系，一
过RHCS底层通信机制立即在其他节点上可见。
在搭建RHCS集群时，GFS一般作为共享存储运行在每个节点上，并且可以通过
图7-4是该方案的架构图，两台MySQL接共享存储，采用红帽的Cluster集群，
（1）安装HA软件包
#servicericci start
#servicelucistart
命令如下：
（3）启动HA服务
命令如下：
（2）编辑hosts文件
#yum installuciricci cman openaisrgmanagerlvm2-clustergfs2-utils
#yuminstall cluster-glueresource-agentspacemaker
命令如下：
192.168.8.102 node2
192.168.8.101 node1
[root@node1~]#cat/etc/hosts
Nodel:Ed_ifolder_主服务器
图7-4RHCS架构图
HA集群
ISCSI存储
如
PC客户端
ISCSI连接线
Node2:Ed_ifolder_备份服务器
以太网
第7章
目前流行的4种高可用架构·257
一台
---
## Page 272
258
·第三部分高可用集群管理篇
DELL的 DRAC等。
Fence virt（Multicast Mode）（如图7-8所示），如果是真实物理机，可选择HP的ILO、
密码。
2）添加一个Fence设备，单击FenceDevices，
1）创建一个集群，单击Manage Clusters →Create，
然后按照提示等待。出现图7-7，代表创建完毕。
在图7-5所示的界面单击CreateCluster，会出现图7-6所示的界面。
打开浏览器，
在红帽6以后改为了Web 页面配置，之前版本中进人Gnome桌面配置的方式已取消了。
命令如下：
（4）设置HA服务自启动
#servicecmanstart