\[root@nn01 hadoop\]# mv hadoop-2.7.6 /usr/local/hadoop
\[root@nn01 hadoop\]# ls /usr/local/hadoop
bin include libexec NOTICE.txt sbin
etc lib LICENSE.txt README.txt share
\[root@nn01 hadoop\]# cd /usr/local/hadoop
\[root@nn01 hadoop\]# ./bin/hadoop vsersion #查看hadoop版本
Error: JAVA_HOME is not set and could not be found.
#报错不知道java的工作目录,
\[root@nn01 hadoop\]# rpm -ql java-1.8.0-openjdk
/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.131-11.b12.el7.x86_64/jre/bin/policytool
/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.131-11.b12.el7.x86_64/jre/lib/amd64/libawt_xawt.so
/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.131-11.b12.el7.x86_64/jre/lib/amd64/libjawt.so
/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.131-11.b12.el7.x86_64/jre/lib/amd64/libjsoundalsa.so
/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.131-11.b12.el7.x86_64/jre/lib/amd64/libsplashscreen.so
/usr/share/applications/java-1.8.0-openjdk-1.8.0.131-11.b12.el7.x86_64-policytool.desktop
/usr/share/icons/hicolor/16x16/apps/java-1.8.0.png
/usr/share/icons/hicolor/24x24/apps/java-1.8.0.png
/usr/share/icons/hicolor/32x32/apps/java-1.8.0.png
/usr/share/icons/hicolor/48x48/apps/java-1.8.0.png
\[root@nn01 hadoop\]# cd /usr/local/hadoop/etc/hadoop/ #进入这个目录
\[root@nn01 hadoop\]# ls
capacity-scheduler.xml httpfs-env.sh mapred-env.sh
configuration.xsl httpfs-log4j.properties mapred-queues.xml.template
export
JAVA_HOME=\"/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.131-11.b12.el7.x86_64/jre\"
#修改
export HADOOP_CONF_DIR=\"/usr/local/hadoop/etc/hadoop\"
#修改hadoop的配置文件目录
\[root@nn01 hadoop\]# ./bin/hadoop version #查看版本有了
Hadoop 2.7.6
Subversion https://PI:EMAIL/repos/asf/hadoop.git -r
085099c66cf28be31604560c376fa282e69282b8
Compiled by kshvachk on 2018-04-18T01:33Z
Compiled with protoc 2.5.0
From source with checksum 71e2695531cb3360ab74598755d036
This command was run using
/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.6.jar
\[root@nn01 hadoop\]# ls
bin etc include lib libexec LICENSE.txt NOTICE.txt README.txt sbin share
\[root@nn01 hadoop\]# mkdir aa
\[root@nn01 hadoop\]# cp README.txt aa #创建测试文件夹aa 放入测试文件
\[root@nn01 hadoop\]# ls aa
README.txt
\[root@nn01 hadoop\]# ./bin/hadoop jar \\
share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar wordcount aa
bb //wordcount为参数
统计aa这个文件夹，存到bb这个文件里面（这个文件不能存在，要是存在会报错，是为了防止数据覆盖）
\[root@nn01 hadoop\]# ls
aa bb bin etc include lib libexec LICENSE.txt NOTICE.txt README.txt sbin
share
\[root@nn01 hadoop\]# ls bb/ #新产生的bb目录里面的东西
part-r-00000 \_SUCCESS #part-r-00000(结果文件) \_SUCCESS(成功标识)
# Hadoop伪分布式
![LINUXNSD_V01ARCHITECTUREDAY05_041](media/image164.png){width="7.264583333333333in"
height="3.8444444444444446in"}
# Hadoop完全分布式
## 环境准备
在上面实验的基础上添加三台主机,所有主机如下
192.168.1.10 nn01 192.168.1.11 node1
192.168.1.12 node2 192.168.1.13 node3
配置好yum源.所有hosts文件
**全部安装好java环境**
\]# yum -y install java-1.8.0-openjdk-devel
## 设置ssh信任.
第一次登录不需要输入yes
\[root@nn01 \~\]# vim /etc/ssh/ssh_config //第一次登陆不需要输入yes
Host \*
GSSAPIAuthentication yes
StrictHostKeyChecking no
\[root@nn01 .ssh\]# ssh-keygen
\[root@nn01 .ssh\]# for i in 21 22 23 24 ; do ssh-copy-id 192.168.1.\$i;
done
//部署公钥给nn01，node1，node2，node3
## 配置hadoop-HDFS
配置文件参考官网:https://hadoop.apache.org/docs/r2.7.6/
![](media/image165.png){width="1.6166666666666667in"
height="1.979861111111111in"}拉到网页最下面可看到配置手册
![](media/image166.png){width="7.259027777777778in" height="1.0625in"}
三部分,最左边是name 中间是value
![](media/image167.png){width="6.285416666666666in"
height="1.3701388888888888in"}
配置文件目录:/var/local/hadoop/etc/hadoop/
在配单机时配置了
\[root@nn01 hadoop\]# vim hadoop-env.sh
export
JAVA_HOME=\"/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.131-11.b12.el7.x86_64/jre\"
export HADOOP_CONF_DIR=\"/usr/local/hadoop/etc/hadoop\"
hadoop的核心配置文件core-site
\[root@room9pc01 hadoop\]# vim core-site.xml
\
\
\fs.defaultFS\
\hdfs://nn01:9000\
#使用nn01主机的文件系统,可以写成本地磁盘:
\file system\ #备注,描述信息,可有可无
\
\
\hadoop.tmp.dir\
\/var/hadoop\ #hadoop的所有数据的根目录
\
\
配置hdfs-site文件
\[root@room9pc01 hadoop\]# vim hdfs-site.xml
\
\
\dfs.namenode.http-address\
\nn01:50070\
\
\
\dfs.namenode.secondary.http-address\
\nn01:50090\
\
\
\dfs.replication\ #数据存几份?
\2\ #数据存2份
\
\
配置slaves
\[root@nn01 hadoop\]# vim slaves #定义所有的datenode主机名
node1
node2
node3
配置好之后将修改的配置文件全部推送所有主机保证:nn01 node1 node2
node3四台主机文件相同
在此我是用ansible批量管理机器
\[root@nn01 ansible\]# cat /etc/ansible/hosts #ansible主机群
\[node\]
nn01
node1
node2
node3
\[root@nn01 ansible\]# ansible all -m shell -a \'mkdir /var/hadoop\'
#给所有机器创建hadoop文件
### 格式化
\[root@nn01 hadoop\]# cd /usr/local/hadoop/
\[root@nn01 hadoop\]# ./bin/hdfs namenode -format
![](media/image168.png){width="7.257638888888889in"
height="3.6381944444444443in"}
格式化之后,,全部为INFO 则正常
### 启动
\[root@nn01 hadoop\]# ./sbin/start-dfs.sh #启动服务
Starting namenodes on \[nn01\]
nn01: Warning: Permanently added \'nn01\' (ECDSA) to the list of known
hosts.
nn01: starting namenode, logging to
/usr/local/hadoop/logs/hadoop-root-namenode-nn01.out
node2: starting datanode, logging to
/usr/local/hadoop/logs/hadoop-root-datanode-node2.out
node1: starting datanode, logging to
/usr/local/hadoop/logs/hadoop-root-datanode-node1.out
node3: starting datanode, logging to
/usr/local/hadoop/logs/hadoop-root-datanode-node3.out
Starting secondary namenodes \[nn01\]
nn01: starting secondarynamenode, logging to
/usr/local/hadoop/logs/hadoop-root-secondarynamenode-nn01.out
\[root@nn01 hadoop\]# jps #验证角色
26866 Jps
26563 NameNode
26750 SecondaryNameNode
### 查看集群是否组件成功
\[root@nn01 hadoop\]# ./bin/hdfs dfsadmin -report
Configured Capacity: 64389844992 (59.97 GB)
Present Capacity: 58996703232 (54.94 GB)
DFS Remaining: 58996690944 (54.94 GB)
DFS Used: 12288 (12 KB)
DFS Used%: 0.00%
Under replicated blocks: 0
Blocks with corrupt replicas: 0
Missing blocks: 0
Missing blocks (with replication factor 1): 0
\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\--
Live datanodes (3): #有三个角色.则成功
Name: 192.168.1.12:50010 (node2)
Hostname: node2
Decommission Status : Normal
Configured Capacity: 21463281664 (19.99 GB)
DFS Used: 4096 (4 KB)
Non DFS Used: 1734930432 (1.62 GB)
DFS Remaining: 19728347136 (18.37 GB)
DFS Used%: 0.00%
DFS Remaining%: 91.92%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 1
Last contact: Thu Mar 28 17:38:27 CST 2019
Name: 192.168.1.13:50010 (node3)
Hostname: node3
Decommission Status : Normal
Configured Capacity: 21463281664 (19.99 GB)
DFS Used: 4096 (4 KB)
Non DFS Used: 1643630592 (1.53 GB)
DFS Remaining: 19819646976 (18.46 GB)
DFS Used%: 0.00%
DFS Remaining%: 92.34%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 1
Last contact: Thu Mar 28 17:38:28 CST 2019
Name: 192.168.1.11:50010 (node1)
Hostname: node1
Decommission Status : Normal
Configured Capacity: 21463281664 (19.99 GB)
DFS Used: 4096 (4 KB)
Non DFS Used: 2014580736 (1.88 GB)
DFS Remaining: 19448696832 (18.11 GB)
DFS Used%: 0.00%
DFS Remaining%: 90.61%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 1
Last contact: Thu Mar 28 17:38:28 CST 2019
\[root@nn01 \~\]# jps
819 NameNode
1013 SecondaryNameNode
1128 Jps
\[root@nn01 \~\]# ssh node1 jps
679 DataNode
730 Jps
\[root@nn01 \~\]# ssh node2 jps
768 Jps
671 DataNode
\[root@nn01 \~\]# ssh node3 jps
675 DataNode
772 Jps
## 安装与配置MapReduce
配置mapred-site
\[root@nn01 \~\]# cd /usr/local/hadoop/etc/hadoop/
\[root@nn01 hadoop\]# mv mapred-site.xml.template mapred-site.xml
\[root@nn01 hadoop\]# vim mapred-site.xml \
\
\mapreduce.framework.name\
\yarn\
\
\
## 安装配置Yarn
\
\yarn.resourcemanager.hostname\ #声明管理者
\nn01\
\
\
\yarn.nodemanager.aux-services\ #声明nodemanager管理框架
\mapreduce_shuffle\
#需要开发提供计算开发框架名,这里没有就用默认的
\
\[root@nn01 ansible\]# /usr/local/hadoop/sbin/start-dfs.sh 启动dfs
\[root@nn01 ansible\]# jps
2821 NameNode
3014 SecondaryNameNode
3129 Jps
\[root@nn01 ansible\]# ssh node1 jps
1757 Jps
1679 DataNode
### 启动Yarn
\[root@nn01 ansible\]# /usr/local/hadoop/sbin/start-yarn.sh #启动yarn
starting yarn daemons
starting resourcemanager, logging to
/usr/local/hadoop/logs/yarn-root-resourcemanager-nn01.out
node1: starting nodemanager, logging to
/usr/local/hadoop/logs/yarn-root-nodemanager-node1.out
node3: starting nodemanager, logging to
/usr/local/hadoop/logs/yarn-root-nodemanager-node3.out
node2: starting nodemanager, logging to
/usr/local/hadoop/logs/yarn-root-nodemanager-node2.out
### 验证Yarn
\[root@nn01 ansible\]# jps
2821 NameNode
3014 SecondaryNameNode
3178 ResourceManager
3439 Jps
\[root@nn01 ansible\]# ssh node1 jps
1905 Jps
1799 NodeManager
1679 DataNode
\[root@nn01 ansible\]# /usr/local/hadoop/bin/yarn node -list
19/03/29 09:50:56 INFO client.RMProxy: Connecting to ResourceManager at
nn01/192.168.1.10:8032
Total Nodes:3
Node-Id Node-State Node-Http-Address Number-of-Running-Containers
node3:36355 RUNNING node3:8042 0
node2:34608 RUNNING node2:8042 0