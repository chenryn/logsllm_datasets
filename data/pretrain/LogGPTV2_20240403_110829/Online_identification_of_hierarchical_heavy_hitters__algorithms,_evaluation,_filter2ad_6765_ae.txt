j=1 t[i; j]Xj . From (1) and (2), we can
i and XU
i = 1.
i
 5000
 4000
 3000
 2000
 1000
 0
-1000
-2000
-3000
-4000
-5000
forecast error lower bound
forecast error upper bound
 0
 20
 40
 60
 80
 100
 120
 140
 160
i
(a) naive solution
 2
 1.5
 1
 0.5
 0
-0.5
-1
-1.5
-2
 0
forecast error lower bound
forecast error upper bound
 20
 40
 60
 80
 100
 120
 140
 160
i
(b) our solution
Figure 13: Forecast error bounds when XL
0:5; (cid:12) = 0:25)
i = 0; XU
i = 1 ((cid:11) =
compute s[i; j] and t[i; j] recursively as follows:
s[i; j] = (cid:26) (cid:11)
(1 (cid:0) (cid:11)) (s[i (cid:0) 1; j] + t[i (cid:0) 1; j])
j = i (cid:0) 1
j  2 (proof omitted for the inter-
est of brevity). So when we increment i, we only need to compute
s[i; j] and t[i; j] for j (cid:20) 2. Once we have s[i; j] and t[i; j], let
f [i; j] = s[i; j] + t[i; j]. We then compute the forecast error bounds
EL
i and EU
U
i = X
i as
U
f [i; j] (cid:1) X
f [i; j] (cid:1) X
U
j
E
L
i (cid:0) Xj: f [i;j]>0
i (cid:0) Xj: f [i;j]>0
L
j (cid:0) Xj: f [i;j]<0
j (cid:0) Xj: f [i;j]<0
U
EU
i . More speciﬁcally, we report a signiﬁcant change whenever
the two intervals [EL
i ] do not overlap, i.e.,
[EL
i ] \ [(cid:0)DT S
4.5 Dealing with sampling errors
i ] and [(cid:0)DT S
i ] = ;.
i ; EU
i ; DT S
i ; DT S
i ; EU
Network measurements are increasingly subject to sampling. This
introduces inherent variability into the trafﬁc metrics under study.
This section describes how the effects of sampling variability can be
accommodated within our framework.
The idea is to represent each sampled measurement in the form
(key, value, var) where value is an unbiased usage esti-
mate (e.g. of bytes or packets in a ﬂow) arising from sampling, and
var is a sampling variance associated with the estimate.
In this
framework, the values to be estimated are considered as ﬁxed rather
than statistical quantities. Conditioned upon these values, the sam-
pling decisions can be assumed independent. Hence when measure-
ments are aggregated, the variance of the aggregate is taken to be the
sum of the individual variances.
The aggregate variance can then be used to attach error bars to
time series of a heavy hitters aggregate. We just need to maintain an
estimate of the variance. This is easy because the variance can be
updated in exactly the same way as the value:
whenever n.value = n.value + value
we do
= n.var + var
n.var
In the end, besides obtaining XL, XU, XS for each cluster, we also
have the corresponding estimates for aggregated sampling variance:
V L, V U, V S.
(cid:3) (cid:0) s(cid:16)V U(cid:17)0:5
We can then replace XL and XU with XL
(cid:3) = XU + s(cid:16)V U(cid:17)0:5
XU
, respectively. We can make s sufﬁciently
large so that the probability for any actual value to fall outside the
interval [XL
(cid:3) ] is extremely low (for example, using the 6 sigma
rule if we the sampling error is close to Gaussian). We can then use
XL
(cid:3) and XU
Due to space limit, we do not explicitly show how the estimate
value and its variance var are calculated when working with sam-
pled ﬂow statistics. Details can be found in [16, 15].
(cid:3) together with XS in our earlier analysis.
(cid:3) ; XU
and
5. EVALUATION METHODOLOGY
We evaluate our HHH detection algorithms along a number of
dimensions to measure their accuracy and resource (space and time)
requirements. We use the following accuracy metrics.
L
i = X
E
f [i; j] (cid:1) X
f [i; j] (cid:1) X
L
j
(cid:15) False Positive (F P ) measures the number of entities that the
algorithm incorrectly identiﬁes as Hierarchical Heavy Hitters.
i ; XU
As shown in Figure 13(b), our solution yields very tight bounds.
Note that the above solution requires keeping the entire interval
series [XL
i ]. Our solution is simply to ignore the remote past.
This is reasonable as the use of exponential smoothing means the
remote past has very little effect on predicting the future. That is,
f [i; j] becomes very small when i (cid:0) j is sufﬁciently large. As a
result, we only need to keep state for the most recent few intervals
for each ﬂow.
4.4 Testing for signiﬁcant changes
i and DT S
Recall that in Section 4.1 we apply time series analysis on XS
to compute ES
i ; in Section 4.3 we show how to com-
pute the forecast error bounds. To accommodate the reconstruc-
tion errors introduced by the summary data structure, our detec-
tion criteria combines both DT S
i and the forecast error bounds EL
i ,
i
(cid:15) False Negative (F N) measures the number of Hierarchical
Heavy Hitters entities that the algorithm fails to identify as
such.
(cid:15) Error estimate (ES) for a HHH cluster is measured as the dif-
ference between the actual volume and the volume estimated
by the algorithm.
For the accuracy experiments, we compute the F P , F N and ES
values as follows: an ofﬂine evaluation computes the exact volumes
for every multidimensional cluster, and given a value of (cid:30), deter-
mines the true set of HHH clusters and their actual volumes. Exam-
ining the differences in set membership with the HHH set output by
the online HHH detection algorithms yields the F P and F N. For
each correctly identiﬁed HHH cluster, the difference between the
actual and estimated volume yields ES.
We use the following resource metrics:
(cid:15) Space Overhead : measured in terms of the number of entries
in the two types of data structures involved: (a) array and (b)
hash table.
(cid:15) Computation Overhead : measures the runtime overheads of
different HHH detection algorithms.in terms of the following
three types of operations: (a) lookup/update, i.e., get an entry
from an array (via array indexing) or a hash table (via a hash
table lookup) and possibly update its value; (b) insertion, i.e.,
inserting new entries into an array or a hash table; (c) deletion,
i.e., deleting entries from an array or a hash table.
We use the following naming conventions for the different 2-d
HHH detection algorithms: Cross Producting (cp), Grid-of-Tries
(got), Rectangle Search (rs). cp(cid:3), got(cid:3), and rs(cid:3) are the corre-
sponding variants with the lazy expansion optimization enabled. We
compare these techniques against the three baseline HHH detection
algorithms described in Section 3.1. For the sketch-based technique
sk, recall that the accuracy bound depends on both H and K. In the
evaluations, we set K = 10=(cid:15) and
H = log(SU M (cid:1) (1 + 32=gran)2=(cid:14))= log(K(cid:15));
where gran is the granularity we are using (e.g., gran = 8 indicates
we only consider preﬁx lengths 0; 8; 16; 24 and 32), and SU M is
the total trafﬁc volume. This ensures that with probability 1 (cid:0) (cid:14) the
method gives no false positives (the analysis is similar to the proof
of Theorem 6 in [14]). In our evaluation we set (cid:14) = 0:01. We also
tested a less expensive solution sk2 that uses H = 2.
5.1 Dataset description
We use multiple large netﬂow traces collected from a tier-1 ISP to
drive the evaluations of our algorithms (see Table 1).
duration
trace
ISP-100K 3 min
1 day
ISP-1day
ISP-1mon
1 month
#routers
1
2
2
# records
volume
0.10 M 66.48 MB
332.26 M 223.51 GB
7457.07 M
5.17 TB
Table 1: Data Description: Network Traces used
6. RESULTS
6.1 Evaluation of HHH detection
6.1.1 Resource Efﬁciency
We ﬁrst compare the amortized runtime costs of different HHH
detection algorithms. Figure 14 compares the average number of op-
erations for each newly arrived item using different algorithms and
granularities on trace ISP-100k. Clearly, all our algorithms signiﬁ-
cantly outperform the brute-force solutions by orders of magnitude.
In addition, for high resolution (i.e., gran = 1), the use of lazy ex-
pansion further reduces the runtime costs signiﬁcantly for both got
and rs. This is not surprising, as lazy expansion can signiﬁcantly re-
duce the number of nodes to be created, resulting in a much smaller
summary structure and thus runtime costs.
We next evaluate the space requirements. To better illustrate the
behavior of these algorithms under different granularities, we nor-
malize the space cost by 1=(cid:15) (cid:1) (32=gran)2, the maximum possible
number of ﬂows whose trafﬁc volume exceeds (cid:15)SU M.
The results are summarized in Figures 15(a)-(b). Across both
granularities, sk2 has the highest space requirement. Among the
proposed algorithms, the pair got and rs have very similar space re-
quirements, as do their counterpart pair got(cid:3) and rs(cid:3) that use lazy
expansion.
For high resolution (gran = 1, see Figure 15(a)), got and rs
have substantially higher space requirements than the existing lc al-
gorithm. However, the use of lazy expansion in got(cid:3) and rs(cid:3) results
m
e
t
i
r
e
p
s
n
o
i
t
a
r
e
p
o
m
e
t
i
r
e
p
s
n
o
i
t
a
r
e
p
o
 3000
 2500
 2000
 1500
 1000
 500
 0
lookups/updates
inserts
deletes
sk2
lc
cp
got
rs
cp*
got*
rs*
(a) trace = ISP-100k, gran = 1, (cid:15) = 0:001
 70
 60
 50
 40
 30
 20
 10
 0
lookups/updates
inserts
deletes
sk2
lc
cp
got
rs
cp*
got*
rs*
(b) trace = ISP-100k, gran = 8, (cid:15) = 0:001
Figure 14: Amortized runtime costs for different algorithms.
The total sum is assumed to be given in advance. The cost for
sk (not shown) is 5.5 times more than sk2 due to the use of 11
instead of 2 tables per sketch.
in substantially smaller space requirements that are comparable to
that for lc. For example, the space requirement for rs(cid:3) is just 14%
of that for rs. Cross-Producting has the least overhead, both with
and without lazy expansion.
For the low resolution scenario (gran = 8, see Figure 15(b)),
lazy expansion does not have any noticeable impact on the space
usage of the proposed algorithms, and lc has the least space require-
ment. Note that the values in Figures 15(a)-(b) represent only con-
servative estimates of the space usage, as they depict only the space
required by the hash table or array entries required in each approach,