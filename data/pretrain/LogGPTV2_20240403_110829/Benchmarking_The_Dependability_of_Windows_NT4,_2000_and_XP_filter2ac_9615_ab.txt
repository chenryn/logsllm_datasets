### OS Reaction Time and Benchmark Execution Sequence

**Benchmark Execution Sequence:**
- **tExpStart (n)**: Start of the experiment
- **tWStart (n)**: Workload start
- **tResume (n)**: System call intercepted, execution resumed
- **tResponse (n)**: Response time
- **tExpEnd (n)**: End of the experiment
- **tExpStart (n+1)**: Start of the next experiment
- **System Call Interception**: Occurs at tResume
- **Execution Resumed**: After system call interception
- **Workload End**: Completion of the workload

**Figure 2. Benchmark Execution Sequence**

### 2.4. Benchmark Set-Up

To host the workload and ensure reliable control during benchmark experiments, a remote machine is required to prevent the operating system from hanging. This remote machine is called the **Benchmark Controller**. The set-up for running an OS dependability benchmark requires at least two computers:

1. **Target Machine**: Runs the benchmarked OS.
2. **Benchmark Controller**: Primarily responsible for diagnosing and collecting data in case of a hang or abort.

Additionally, the (Oracle) Data Base Management System (DBMS) that processes the TPC-C client requests can be installed on the benchmark controller or another machine. Figure 1 illustrates the various components of the proposed OS dependability benchmark prototype for Windows 2000. The same set-up is used for all three OS targets, with only the benchmark target being changed.

### 2.5. Benchmarking Time

The benchmarking time includes both the implementation and execution times, which are very time-consuming. The implementation of the benchmark itself was not as time-consuming as other aspects:

- **TPC-C Client Implementation**: The same as used by other DBench partners, taking three days to install.
- **Faultload Implementation**: Took one week, involving:
  - Defining the set of values related to the 28 system calls and their 75 parameters to be corrupted.
  - Creating a database of the corrupted values.
- **Controller Implementation**: Took about 10 days.

The duration of an experiment with workload completion is less than 3 minutes (including workload completion and restart time). Without workload completion, it takes about 7 minutes (including a 5-minute watchdog timeout and restart time). On average, an experiment lasts less than 5 minutes. These experiments are fully automated, and the total benchmark execution duration (552 experiments for each OS) is approximately 46 hours for each OS.

### 3. Comparison of the Three OSs

The benchmark defined in the previous section is used to compare the behavior of Windows NT4, 2000, and XP. We evaluate the three benchmark measures: robustness, reaction time, and restart time. These measures provide information on the global behavior of the OSs and can be refined through sensitivity analyses considering the workload states after the execution of a corrupted system call.

#### 3.1. Benchmark Measures

**Robustness Measures:**
- No panic and hang states were observed for the three OSs.
- Exceptions were notified in 11.4% to 12% of cases.
- The number of experiments with error code return varies between 31.2% and 34.1%.
- More than half of the experiments led to a "No signaling" outcome.

Figure 3 shows similar robustness behavior for the three OSs. Sensitivity analyses with respect to faultload selection are performed in Section 3.2.1.

**System Reaction Time:**
- **τexec**: Mean reaction time of the 28 selected system calls without faults.
- **Texec**: Mean reaction time of the 28 selected system calls with faults.

Table 3 shows the reaction times for the three OSs:

| OS          | τexec (Mean) | τexec (SD) | Texec (Mean) | Texec (SD) |
|-------------|--------------|------------|--------------|------------|
| Windows NT4 | 344 µs       | 128 µs     | 230 µs       |            |
| Windows 2000 | 1782 µs      | 1241 µs    | 3359 µs      |            |
| Windows XP  | 111 µs       | 114 µs     | 176 µs       |            |

Windows XP has the shortest reaction time, while Windows 2000 has the longest. For Windows XP, the reaction time with faults is slightly longer than without faults, whereas for the other two OSs, it is significantly lower. This may be due to the OS detecting injected faults in about 45% of cases, returning an error code or signaling an exception.

**System Restart Time:**
- Table 4 shows the restart times for the three OSs:

| OS          | τres (Mean) | τres (SD) | Tres (Mean) | Tres (SD) |
|-------------|-------------|-----------|-------------|-----------|
| Windows NT4 | 92 s        | 105 s     | 74 s        | 96 s      |
| Windows 2000 | 4 s         | 109 s     | 8 s         | 80 s      |
| Windows XP  | 8 s         | 8 s       |             |           |

Windows XP's restart time is 70% of that of Windows 2000 without faults and 73% with faults. For all systems, the restart time is only a few seconds longer with faults.

**Summary:**
- The three OSs have equivalent robustness.
- Windows XP has the shortest system call execution time and the shortest restart times, both with and without faults.
- These results confirm known behavior of Windows XP in the presence of faults.

### 3.2. Measure Refinement

We will consider the three benchmark measures and show how they can be enriched by examining additional information provided by the current benchmark set-up.

#### 3.2.1. Robustness Measure

The faultload used in the previous section includes a mix of three corruption techniques:
1. Out-of-range data (or out of the boundaries of accepted parameter values)
2. Incorrect data (but within the boundaries of accepted parameter values)
3. Incorrect addresses

In total, 552 corrupted values for the 75 parameters related to the 28 selected system calls. This faultload is referred to as FL0.

It can be argued that incorrect data is not representative of application faults that should be detected by the OS. To analyze its impact, we considered a reduced faultload, FL1, including only out-of-range data and incorrect addresses, comprising 325 corrupted values. The comparison shows that even though the robustness of each OS was slightly affected by the corruption technique used, the three OSs still have very similar robustness.

Incorrect addresses usually point to out-of-range or incorrect data. Assuming they only point to incorrect data, we considered a faultload, FL2, comprising only out-of-range data (113 corrupted values). The comparison also shows similar robustness for the three OSs.

This result encourages the corruption of parameters of all system calls involved in the workload using only the out-of-range technique without significantly increasing the benchmark run duration. Thus, we considered a faultload, FL3, composed of only out-of-range data, targeting all 132 system calls with their 353 parameters. 468 experiments were performed for each OS, showing that the three OSs still have similar robustness when corrupting all system calls involved in the TPC-C client workload.

**Table 5: Faultloads Considered**

| Faultload | Incorrect Data | Incorrect Address | Out-of-Range Data | Bit-Flip | # System Calls |
|-----------|----------------|-------------------|-------------------|----------|----------------|
| FL0       | x              | x                 | x                 |          | 28             |
| FL1       |                | x                 | x                 |          | 28             |
| FL2       |                |                   | x                 |          | 28             |
| FL3       |                |                   | x                 |          | 132            |
| FL4       | x              | x                 | x                 | x        | 28             |

FL0 to FL3 use a selective substitution technique.