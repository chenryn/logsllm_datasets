Other system calls requiring emulation are for signal handling,
where the guest kernel needs to prepare a temporary execution con-
text for the application and transfers the execution control to a pre-
registered handler to handle the corresponding signal. The critical
security issue here is that the guest kernel needs to be authorized to
manipulate the application context. Such authorization may be ex-
ploited to reveal and tamper with the application data, e.g., involve
a function to send plain text outside.
CAVEAT Note that the ptrace system call is not allowed in App-
Shield since its working mechanism requires the guest OS to di-
rectly read the content of the user space, or to modify the data or
even code of the speciﬁc addresses, which cannot be reconciled
with the security requirements. We do not emulate this system call.
6.
IMPLEMENTATION AND EVALUATION
We have implemented a prototype of AppShield on a PC with In-
tel i7-2600 (3.4GHZ), 3GB memory and Ubuntu 10.04 with kernel
2.6.32.59 (Table 2). The prototype consists of a dedicated hypervi-
sor [7] running on the bare-metal hardware, and a Linux loadable
module as the transit module. The code base of the hypervisor is
around 29K SLOC with 218KB binary size. The transit module
consists of around 2K SLOC, and the trusted shim is around 1K
SLOC.
Trusted Shim. We do not modify the source code of the ap-
plication or the shared libraries. Instead, we create the shim as a
wrapper of libc, and allow it to intercept the function calls that are
supposed to call the libc functions. Speciﬁcally, on the Linux sys-
tem, an application usually needs shared libraries at run-time, and
the dynamic linker loads those shared libraries in whatever order
it needs them. However, when LD_PRELOAD is set for a shared
library, it will be loaded before any other libraries, including the
libc library. Pre-loading a library means that its functions will be
used before others of the same name. We use this feature in our
implementation, saving the cost of the source code modiﬁcation.
The trusted shim needs to do some initialization and prepara-
tion for the protection and the interception, such as allocating the
shared buffer and informing the hypervisor to protected the appli-
cation. However, those functions for intercepting system calls are
Files
Network
Memory
Process
Time
Others
System Calls
open, close, read, write, chdir
writev, access, fstat64, uname, poll, fcntl
statfs64, fstatfs64, getdents64, getdents
stat64, lseek, _llseek, getcwd, fchdir, ioctl
bind, listen, accept,
sendto, recvfrom, accept4, select
connect, send, recv, getsockname, socketcall
mmap2, munmap, mremap, brk, mprotect
getpid, gettid, getgroups32, set_thread_area
getuid, geteuid, getgid, getegid
exit_groud, tgkill, getrlimit, exit
time, clock_gettime, gettimeofday
futex, rt_sigaction, rt_sigprocmask, sigaltstack
Table 3: Supported system calls.
passively invoked, meaning that they do not execute until the ap-
plication explicitly calls them. To solve this problem, we resort
to another feature - constructor function. A constructor function
marked with .init will be called by the dynamic linker when the li-
brary is loaded. The trusted shim in our implementation supports
56 most commonly used system calls as listed in Table 3 below.
Implementation Challenges.
The techniques used in the sys-
tem call interception and parameter marshaling are not as trivial as
they seem. The operations of each system call and the related data
structures are rather complex. For example, socketcall supports
many possible operations on the selected ﬁle. The operation is de-
termined by a command parameter. There are up to 20 command
options, and the commands could impact the meanings of other pa-
rameters and invoke different data structures. Handling all these
variations requires both deep understanding and careful implemen-
tation. The challenges of implementing performance isolation are
related to the installation of interrupt handlers which involve spe-
cial steps (e.g., saving context) in the assembly code before invok-
ing the corresponding native C-code handler. The assembly code
needs to prepare the stack and registers (e.g., as parameters) for the
native handler. Since this piece of code usually breaks the stack
layout and alters the register values, those information have to be
stored properly right before executing the code and are restored by
the code right before invoking the C-code handler. All these oper-
ations are further complicated by the requirement that they should
be ﬁnished in an atomic way. Any interrupt during the operations
overwrites the saved context and/or breaks the stack layout.
6.1 Micro Benchmark
In the micro benchmark, we evaluate the cost of the address
space switch (Table 4). An address space switch event can be
divided into three parts: protection mode switch, context backup
and restoration. The protection mode switch includes a hypercall,
IDTR and EPT switching. The context backup consists of saving
registers (including general, ﬂag, control registers, and MMX/SSE/AVX
registers) and creating a dummy context. The context restoration is
to load all the saved registers. The cost of address space switch is
relatively high, because it contains the costly memory access from
hypervisor space to guest space, i.e., inserting the return address
to the kernel stack. All three costs constitute the latency for the
system to handle a particular interrupt or exception.
The cost for a system call is composed of the address-space
switch cost and the parameter marshaling cost. The latter varies
with different system calls. For instance, there is no such cost for
getpid, while write involves a data copy. Thus, we do not pro-
vide individual evaluation. However, they are reﬂected in macro
8
352Operation
Time (µs)
Out of Protected Address Space
Back to Protected Address Space
Context Backup
Context Restoration
1.72
1.33
0.11
0.08
Table 4: The micro-benchmark results for address space
switch.
benchmarking which evaluate the whole application performance
overhead.
6.2 Macro Benchmark
In macro benchmarking, we apply AppShield on several appli-
cations (including Apache, and ls, vim on Linux) as well as bench-
mark tools, and measure their performance effects on computation,
disk and network I/O.
6.2.1 AppShield Impacts on Performance
SPEC CINT2006 [11] is an industry-standard benchmark intended
for measuring the performance of the CPU and memory. We exe-
cuted SPEC CINT2006 in two setups: with and without AppShield
protection. Without AppShield protection, the performance over-
head is due to the virtualization itself. The full evaluation has been
reported in [7]. Generally, it only introduces 0.2% to 10.3% perfor-
mance overhead. In addition to the virtualization cost, AppShield
has 0.01% slowdown on average. The primary source of virtualiza-
tion overhead is VM exits due to interrupts and privileged instruc-
tions [15]. Figure 10 shows the results.
6.2.2 Computation centric programs
We measure the AppShield’s protection on computation-intensive
programs. In our experiment, we measure three encryption algo-
rithms (i.e., AES, RC4 and RSA) from OpenSSL 0.9.8k package.
We run these algorithms to encrypt/decrypt messages with differ-
ent lengths, from 32bytes to 2048 bytes. The measurement results
in Figure 11 shows that the protection effects on the computation
programs is quite small.
Figure 11: The effects of AppShield protection on computation
6.2.3 Disk I/O centric programs
The disk I/O benchmark includes three sub-benchmarks to eval-
uate the overhead in disk reading, writing and copying. Disk I/O
benchmark reads/writes data from/to ﬁles with different sizes. In
our experiments, the ﬁle size is 64MB, and the read/write gran-
ularity is from 512B to 4MB. Experiments with a larger ﬁle and
a smaller buffer result in more system calls, and consequently in-
troduce more context switches. However, with the increasing of
9
the buffer size, the performance is better, which is also proved by
the experiment results in Figure 12. For example, the performance
overheads with 4KB-granularity are quite high, and have (81.91%,
71.84%, 74.57%) for (read, write, copy) respectively, while the per-
formance overhead with 256KB-granularity are very small, only
has (0.68%, 4.52%, 0.00%) for (read, write and copy) respectively.
Note that the overhead is mainly introduced by data copy and con-
text backup/restoration.
Figure 12: The disk I/O Benchmark
6.2.4 Network I/O Benchmark
We measured the network performance with the Apache web
server. The server is conﬁgured in worker mode with one main pro-
cess and 20 threads. We run the standard ApacheBench included in
the Apache utility tools. We execute 10,000 web requests, at the
concurrency level of 100 to fetch the default index page. The web
client and the Apache server are in the same LAN. With AppShield,
the Apache web server serves requests with 1.20% overhead in
throughput, and about 3.05% overhead in waiting time and 1.86%
overhead in processing time. We also compare AppShield against
Overshadow[6] and InkTag [17] in Table 6. Note that Apache may
cache the frequently requested pages, without issuing disk I/O for
each request, which helps to reduce the overhead. We also compare
the network performances under Overshadow, InkTag and App-
Shield protections, The results that are listed in Table 6 also in-
dicate that our scheme have the lowest performance overhead and
latency on network I/O.
Throughput (req/s)
Conn. Processing (ms)
Conn. Waiting (ms)
Linux AppShield Overhead
321
160
131
317
163
135
1%
2%
3%
Table 5: The benchmark results of Apache performance
Req. Throughput
Conn. Latency
Overhead
OverShadow InkTag
100%
−
2%
13%
AppShield
1%
3%
Table 6: Network performance comparisons with Overshadow
and InkTag.
050100150200250300350400450500Msg.(32B)Msg.(256B)Msg.(2048B)Msg.(32B)Msg.(256B)Msg.(2048B)Msg.(32B)Msg.(256B)Msg.(2048B)AESRSARC4W/o ProtectionW/ ProtectionTime (ms) 05001000150020002500300035004000W/oProtectionW/ProtectionW/oProtectionW/ProtectionW/oProtectionW/ProtectionReadBenchWriteBenchCopyBench512B4KB32KB256KB4MB(MB/s) 353Figure 10: SPECint 2006 Result. AppShield introduces insigniﬁcant slowdown.
7. RELATED WORK
There are several approaches proposed to protect application code
and data, and all of them attempted to remove the OS out of TCB
to provide a higher-assurance execution environment.
7.1 Self-contained Code Protection
Flicker [23] system built on the TPM-based Dynamic Root Of
Trust (DROT) technology can create an isolation environment to
protect a piece of code and data. Due to the limitation of the TPM,
the latency of the Flicker system is quite high. To minimize the
latency, TrustVisor [22] scheme are proposed. By leveraging virtu-
alization technology, TrustVisor virtualizes the physical TPM into
Virtual TPMs (VTPMs) and migrate them into hypervisor space.
Note that both schemes focus on the protection of a small piece of
code and data. Both schemes only protect self-contained code with
pre-deﬁned inputs and outputs (e.g., inputs are the initial parame-
ters and outputs are the ﬁnal returns), and they do not support the
protection with dynamic memory allocation and system calls. The
increasing of the protection scope, such as protecting the whole ap-
plication, may lead both schemes to failure. MiniBox [20] attempts
to extend the functionality of the self-contained code by combin-
ing virtualization-based memory isolation and user-space sandbox
(e.g., Google Native Client) techniques. But it still have several
limitations to support a whole legacy application, such as lack of
multi-thread support and limiting to sandbox-capable applications.
7.2 Whole Application Protection
Secure-Processor-Based Protection.
AEGIS [29] and XOM
OS [21] are secure-processor based approaches that provide com-
partments to isolate one application from others. Both of them in-
cur poor computability since they require substantial modiﬁcations
on the OSes and applications. AEGIS [29] also provide an alterna-
tive implementation, which requires to build security into the OS.
Bastion [2] and SecureME [9] aim to deal with untrusted OS and
untrusted hardware attacks simultaneously with the assistance of a
secure processor. Bastion focuses on the protection of a security
module, while SecureME attempts to provide privacy and integrity
for data and code of the application. SecureME requires modiﬁca-
tions on both OSes and applications.
10
In addition, a Processor-Measured Application Protection Ser-
vice P-MAPS [25] is announced by Intel, which is built upon Intel
TXT [18] and Intel VT [19] hardware capabilities. P-MAPS pro-
vides runtime isolation to protect standard applications with small