8:
9:
The EvictPath routine is shown in Algorithm 3. As
mentioned, evictions are scheduled statically: one evic-
6
420  24th USENIX Security Symposium 
USENIX Association
G = 0
G = 1
G = 2
G = 3
Time
Figure 2: Reverse-lexicographic order of paths used by
EvictPath. After path G = 3 is evicted to, the order re-
peats.
tion operation happens after every A reads. At a high
level, an eviction operation reads all remaining real
blocks on a path (in a secure fashion), and tries to push
them down that path as far as possible. The leaf-to-root
order in the writeback step (Lines 7) reﬂects that we wish
to ﬁll the deepest buckets as fully as possible. (For read-
ers who are familiar with Path ORAM, EvictPath is like
a Path ORAM access where no block is accessed and
therefore no block is remapped to a new leaf.)
We emphasize two unique features of Ring ORAM
eviction operations. First, evictions in Ring ORAM are
performed to paths in a speciﬁc order called the reverse-
lexicographic order, ﬁrst proposed by Gentry et al. [9]
and shown in Figure 2. The reverse-lexicographic order
eviction aims to minimize the overlap between consecu-
tive eviction paths, because (intuitively) evictions to the
same bucket in consecutive accesses are less useful. This
improves eviction quality and allows us to reduce the fre-
quency of eviction. Evicting using this static order is also
a key component in simplifying our theoretical analysis
in Section 4.
Second, buckets in Ring ORAM need to be randomly
shufﬂed (Invariant 2), and we mostly rely on EvictPath
operations to keep them shufﬂed. An EvictPath oper-
ation reads Z blocks from each bucket on a path into
the stash, and writes out Z + S blocks (only up to Z
are real blocks) to each bucket, randomly permuted.
The details of reading/writing buckets (ReadBucket and
WriteBucket) are deferred to Appendix A.
3.4 Early Reshufﬂe Operation
Algorithm 4 EarlyReshuﬄe procedure.
1: function EarlyReshuﬄe(l)
2:
3:
4:
5:
6:
if P(l,i).count ≥ S then
for i ← 0 to L do
Stash ← Stash∪ ReadBucket(P(l,i))
WriteBucket(P(l,i), Stash)
P(l,i).count ← 0
Due to randomness, a bucket can be touched > S
times by ReadPath operations before it is reshufﬂed
7
If this happens, we call
by the scheduled EvictPath.
EarlyReshuﬄe on that bucket to reshufﬂe it before the
bucket is read again (see Section 3.2). More precisely,
after each ORAM access EarlyReshuﬄe goes over all
the buckets on the read path, and reshufﬂes all the buck-
ets that have been accessed more than S times by per-
forming ReadBucket and WriteBucket. ReadBucket
and WriteBucket are the same as in EvictPath:
that
is, ReadBucket reads exactly Z slots in the bucket
and WriteBucket re-permutes and writes back Z + S
real/dummy blocks. We note that though S does not af-
fect security (Section 3.5), it clearly has an impact on
performance (how often we shufﬂe, the extra cost per
reshufﬂe, etc.). We discuss how to optimally select S in
Section 5.
3.5 Security Analysis
Claim 1. ReadPath leaks no information.
The path selected for reading will look random to
any adversary due to Invariant 1 (leaves are chosen
uniformly at random). From Invariant 2, we know that
every bucket is randomly shufﬂed. Moreover, because
we invalidate any block we read, we will never read the
same slot. Thus, any sequence of reads (real or dummy)
to a bucket between two shufﬂes is indistinguishable.
Thus the adversary learns nothing during ReadPath. (cid:31)
Claim 2. EvictPath leaks no information.
The path selected for eviction is chosen statically,
and is public (reverse-lexicographic order). ReadBucket
always reads exactly Z blocks from random slots.
WriteBucket similarly writes Z + S encrypted blocks in
a data-independent fashion. (cid:31)
Claim 3. EarlyShuﬄe leaks no information.
To which buckets EarlyShuﬄe operations occur is
publicly known: the adversary knows how many times a
bucket has been accessed since the last EvictPath to that
bucket. ReadBucket and WriteBucket are secure as per
observations in Claim 2. (cid:31)
The three subroutines of the Ring ORAM algorithm
are the only operations that cause externally observable
behaviors. Claims 1, 2, and 3 show that the subroutines
are secure. We have so far assumed that path remap-
ping and bucket permutation are truly random, which
gives unconditional security. If pseudorandom numbers
are used instead, we have computational security through
similar arguments.
USENIX Association  
24th USENIX Security Symposium  421
3.6 Other Optimizations
Minimizing roundtrips. To keep the presentation sim-
ple, we wrote the ReadPath (EvictPath) algorithms to
process buckets one by one.
In fact, they can be per-
formed for all buckets on the path in parallel which re-
duces the number of roundtrips to 2 (one for metadata
and one for data blocks).
Tree-top caching. The idea of tree-top caching [18] is
simple: we can reduce the bandwidth for ReadPath and
EvictPath by storing the top t (a new parameter) levels of
the Ring ORAM tree at the client as an extension of the
stash1. For a given t, the stash grows by approximately
2tZ blocks.
De-amortization. We can de-amortize the expensive
EvictPath operation through a period of A accesses, sim-
ply by reading/writing a small number of blocks on the
eviction path after each access. After de-amortization,
worst-case overall bandwidth equals average overall
bandwidth.
3.7 Recursive Construction
With the construction given thus far, the client needs to
store a large position map. To achieve small client stor-
age, we follow the standard recursion idea in tree-based
ORAMs [23]: instead of storing the position map on the
client, we store the position map on a smaller ORAM
on the server, and store only the position map for the
smaller ORAM. The client can recurse until the ﬁnal
position map becomes small enough to ﬁt in its stor-
age. For reasonably block sizes (e.g., 4 KB), recursion
contributes very little to overall bandwidth (e.g.,  R]
1We call this optimization tree-top caching following prior work.
But the word cache is a misnomer: the top t levels of the tree are per-
manently stored by the client.
decreases exponentially in R for certain Z and A combi-
nations. As it turns out, the deterministic eviction pattern
in Ring ORAM dramatically simpliﬁes the proof.
We note here that the reshufﬂing of a bucket does not
affect the occupancy of the bucket, and is thus irrelevant
to the proof we present here.
4.1 Proof outline
The proof consists of the two steps. The ﬁrst step is the
same as Path ORAM, and needs Lemma 1 and Lemma 2
in the Path ORAM paper [27], which we restate in Sec-
tion 4.2. We introduce ∞-ORAM, which has an inﬁnite
bucket size and after a post-processing step has exactly
the same distribution of blocks over all buckets and the
stash (Lemma 1). Lemma 2 says the stash occupancy
of ∞-ORAM after post-processing is greater than R if
and only if there exists a subtree T in ∞-ORAM whose
“occupancy” exceeds its “capacity” by more than R. We
note, however, that the Path ORAM [27] paper only gave
intuition for the proof of Lemma 1, and unfortunately
did not capture of all the subtleties. We will rigorously
prove that lemma, which turns out to be quite tricky and
requires signiﬁcant changes to the post-processing algo-
rithm.
The second step (Section 4.3) is much simpler than
the rest of Path ORAM’s proof, thanks to Ring ORAM’s
static eviction pattern. We simply need to calculate the
expected occupancy of subtrees in ∞-ORAM, and apply
a Chernoff-like bound on their actual occupancy to com-
plete the proof. We do not need the complicated eviction
game, negative association, stochastic dominance, etc.,
as in the Path ORAM proof [26].
For readability, we will defer the proofs of all lemmas
to Appendix B.
L
L
L
as bZ
4.2 ∞-ORAM
We ﬁrst introduce ∞-ORAM, denoted as ORAM∞,A
. Its
buckets have inﬁnite capacity. It receives the same input
request sequence as ORAMZ,A
. We then label buckets
linearly such that the two children of bucket bi are b2i and
b2i+1, with the root bucket being b1. We deﬁne the stash
to be b0. We refer to bi of ORAM∞,A
i , and bi of
ORAMZ,A
i . We further deﬁne ORAM state, which
consists of the states of all the buckets in the ORAM, i.e.,
the blocks contained by each bucket. Let S∞ be the state
of ORAM∞,A
We now propose a new greedy post-processing algo-
rithm G (different from the one in [27]), which by re-
assigning blocks in buckets makes each bucket b∞
in ∞-
i
ORAM contain the same set of blocks as bZ
i . Formally, G
takes as input S∞ and SZ after the same access sequence
with the same randomness. For i from 2L+1 − 1 down to
and SZ be the state of ORAMZ,A
as b∞
.
L
L
L
422  24th USENIX Security Symposium 
USENIX Association
8
1 (note that the decreasing order ensures that a parent is
always processed later than its children), G processes the
blocks in bucket b∞
i
in the following way:
1. For those blocks that are also in bZ
i , keep them in
b∞
i .
2. For those blocks that are not in bZ
i , move them from b∞
i
i but in some an-
to b∞
cestors of bZ
i/2 (the parent
of b∞
i , and note that the division includes ﬂooring).
If such blocks exist and the number of blocks re-
maining in b∞
i
is less than Z, raise an error.
3. If there exists a block in b∞
i
that is in neither bZ
i nor
any ancestor of bZ
i , raise an error.
i after G contains the same set of blocks as bZ
We say GSZ (S∞) =S Z, if no error occurs during G
and b∞
i for
i = 0,1,···2 L+1.
Lemma 1. GSZ (S∞) = SZ after the same ORAM access
sequence with the same randomness.
L
L
L
. This means that if a node in ORAM∞,A
Next, we investigate what state S∞ will lead to the
stash occupancy of more than R blocks in a post-
processed ∞-ORAM. We say a subtree T is a rooted sub-
tree, denoted as T ∈ ORAM∞,A
if T contains the root of
ORAM∞,A
is
in T , then so are all its ancestors. We deﬁne n(T ) to
be the total number of nodes in T . We deﬁne c(T ) (the
capacity of T ) to be the maximum number of blocks T
can hold; for Ring ORAM c(T ) =n(T )· Z. Lastly, we
deﬁne X(T ) (the occupancy of T ) to be the actual num-
ber of real blocks that are stored in T . The following
lemma characterizes the stash size of a post-processed
∞-ORAM:
Lemma 2. st (GSZ (S∞)) > R if and only if ∃T ∈
ORAM∞,A
s.t. X(T ) > c(T ) +R before post-processing.
L
By Lemma 1 and Lemma 2, we have
Pr [X(T ) > c(T ) +R]
Pr [st (SZ) > R] = Pr [st (GSZ (S∞)) > R]
≤ ∑
∞,A
T∈ORAM
L
 c(T ) +R]
4n max
T :n(T )=n
Let X(T ) = ∑i Xi(T ), where each Xi(T ) ∈ {0,1} and
indicates whether the i-th block (can be either real or
stale) is in T . Let pi = Pr [Xi(T ) =1]. Xi(T ) is com-
pletely determined by its time stamp i and the leaf label
assigned to block i, so they are independent from each
other (refer to the proof of Lemma 3). Thus, we can
apply a Chernoff-like bound to get an exponentially de-
creasing bound on the tail distribution. To do so, we ﬁrst
establish a bound on E(cid:31)etX(T )(cid:30) where t > 0,
E(cid:31)etX(T )(cid:30) = E(cid:31)et ∑i Xi(T )(cid:30) = E(cid:31)ΠietXi(T )(cid:30)
(by independence)
= ΠiE(cid:31)etXi(T )(cid:30)
= Πi(cid:29)pi(et − 1) +1(cid:28)
≤ Πi(cid:27)epi(et−1)(cid:26) = e(et−1)Σi pi
= e(et−1)E[X(T )]
(2)
For simplicity, we write n = n(T ) and a = A/2. By
Lemma 3, E[X(T )] ≤ n · a. By the Markov Inequality,
we have for all t > 0,
Pr [X(T ) > c(T ) +R] = Pr(cid:31)etX(T ) > et(nZ+R)(cid:30)
≤ E(cid:31)etX(T )(cid:30)· e−t(nZ+R)
≤ e(et−1)an · e−t(nZ+R)
= e−tR · e−n[tZ−a(et−1)]
Let t = ln(Z/a),
Pr [X(T ) > c(T ) +R] ≤ (a/Z)R · e−n[Z ln(Z/a)+a−Z]
Now we will choose Z and A such that Z > a and q =
Z ln(Z/a)+a−Z−ln4 > 0. If these two conditions hold,
from Equation (1) we have t = ln(Z/a) > 0 and that the
stash overﬂow probability decreases exponentially in the
stash size R:
(3)
(1)
Pr [st (SZ) > R] ≤ ∑
n≥1