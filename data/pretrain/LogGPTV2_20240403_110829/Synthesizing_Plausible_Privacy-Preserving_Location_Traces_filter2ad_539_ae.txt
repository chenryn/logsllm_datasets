0.6
Semantic Similarity
0.8
1
(a)
(b)
Fig. 5: Normalized histogram of (a) the geographic similarity and (b)
semantic similarity between all distinct pairs of 30 users in the seed
dataset. (a) Mobility models of different individuals is geographically
very speciﬁc to themselves, i.e., they are unique. This is well reﬂected
in the skewed distribution of geographic similarity towards very small
values. (b) As hypothesized in this paper, majority of individuals have
high semantic similarities between their mobility models.
i.e., it maximizes the ratio of inter-cluster similarity over intra-
cluster similarity. This clustering is illustrated in Figure 4(b),
where each location is drawn with the color of the cluster it
belongs to. The ﬁgure allows us to distinguish some patterns,
for example locations at the center of cities are mostly in blue,
while many locations representing roads and highways are
colored in red. Also notice that the semantic clustering does
not seem to depend on the geographical distance of locations.
To illustrate our geographic and semantic similarity metrics,
we compute those metrics pairwise over all 30 users.3 The
result is shown in Figures 5. The ﬁrst histogram shows that
the 30 users are not strongly geographically similar to each
other, except for a few pairs of users. This is expected given the
range of locations they explore overall, as seen in Figure 4(a).
On the other hand, the distribution of the semantic similarity
across all distinct pairs of users has a larger variance, and a
large number of users are highly similar.
B. Tool: Synthetic Trace Generator
We build a tool [6] to generate fake traces on top of the
open-source Location Privacy Meter (LPM) [46]. To exploit
LPM’s modularity we split our algorithm into modules. To
implement the time-dependent sub-sampling of clusters and
merging around transitions, and the transformation of users’
actual traces into semantic traces, we use the location obfus-
cation mechanism feature of the tool. The reconstruction of
geographically valid synthetic traces from the semantic traces
is done using the Viterbi algorithm. To cluster the semantic
location graph, we employ the CLUTO toolkit [53].
C. Simulation Setup
Recall that from the input dataset, we sub-sampled 30 user
traces (day 1) that we use for the seed dataset S. As for
the parameters of the GenerateFake() algorithm, we set
the location-removal probability parc to 0.25, and we set
the location merging probability parm to 0.75. We set the
probability parl of removing the true location visited in the
3We exclude the geometric/semantic similarity of a user with herself, as it’s 1.0.
seed to 1.0. We set the randomization multiplication factor for
Viterbi randomization parv to 4.
We set very tight values for the privacy parameters. Specif-
ically, we set δi, the maximum intersection between fake and
seed, to 0. So, we do not tolerate any intersection between
fake and seed. We set the geographic similarity threshold δs
to 0.1, and the differential semantic similarity threshold δd
also to 0.1. See Figure 5 to see comparatively how restrictive
these thresholds are. Last, we set k the number of required
alternatives to pass the plausible deniability to 1.
For each of the 30 seed traces, we generated about 500
fake traces. We then select and use these traces according to
the scenario evaluated. For example, for the LBS scenario,
we sampled traces (for each user) according to the synthetic
traces likelihoods (see Section V), out of the pool of traces
that passed the privacy test.
D. Evaluation Metrics
In the following two sections we evaluate the use of
synthetic traces in two popular scenarios: using fake locations
along with real locations when accessing location-based ser-
vices (Section VII), and releasing synthetic location datasets
to be used for various geo-data analysis tasks (Section VIII).
In both scenarios, we evaluate our fake traces with respect
to two metrics: privacy and utility. Our privacy guarantees
apply to both scenarios. However, there are some differences
in terms of the adversary model between different scenarios.
Therefore, there are additional considerations regarding the
privacy of users in location-based services, e.g., their privacy
against inference attacks, that we discuss in the corresponding
section. The utility metric is also dependent on the application
(scenario), hence is measured differently in each case.
Note that the generative power of our model (and similarly
any statistical or machine learning model) depends on the
available (training) data. Yet, similar to a machine learning
algorithm, we do not require a “minimum” number of data
records to start working. However, the quality of the input
dataset does not impact data privacy, as privacy is guaranteed
in the phase when we are generating traces (and not in the
phase when model is trained), by running our privacy tests.
So, the output is privacy preserving regardless of the size of
the input and quality of the model.
VII. EVALUATION: LOCATION-BASED SERVICES
In this section, we consider the use of fakes in accessing
location-based services (LBS). Speciﬁcally, we compare our
proposed technique with all existing fake generation methods.
Concretely, we evaluate the utility and privacy according to
well-established metrics for this scenario. In particular, we
measure how well our fakes perform against state-of-the-art
inference attacks. Note that the fakes used here have already
passed our privacy tests with tight constraints, but as explained
in Section V-C, we still need to test their effectiveness in
protecting privacy of LBS users against inference attacks.
556556
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:09:44 UTC from IEEE Xplore.  Restrictions apply. 
A. Setup
In this setting, a user shares her current location with a
location-based service. The service provider, in return, pro-
vides contextual information about the shared locations (e.g.,
list of nearby restaurants, current trafﬁc information on the
road). The user makes such queries over time whenever she
wishes to obtain contextual information.
In order to protect her location privacy, i.e., hiding her
location at the time of access to the LBS and also preventing
the inference of the full trajectory, the user’s device sends a
number of fake locations along with her true location. For
example, if two fake locations are used, then every time the
user makes a query, the device sends locations x, y, z to the
service provider. Out of {x, y, z}, one location is the user’s
actual (i.e., true) location and the other two are fakes. The
service provider does not know which of x, y, z is the true
location, but may be able to ﬁlter out fake locations over time
(i.e., over multiple queries over time), if the fake locations are
not believable (i.e., plausible). This is why it is crucial to use
synthetic traces as opposed to independent fake locations.
The fake locations are obtained as followed. First, we
generate a collection of synthetic traces. The users can select
from these traces and store them in their devices. Then, when
a user makes an LBS query, say at time t, she picks the ith
fake location (reported to the LBS) as the location which is
visited at time t in fake trace i. Note that the processes of
generating and using the synthetic traces are independent.
We emphasize that all existing fake location generation
methods (i.e., [10], [23], [24], [26], [45], [49], [56]) work this
way; but the techniques differ in how the fake locations are
generated. Existing fake locations generation methods can be
classiﬁed into four categories, which are the following.
Uniform IID ([45]): Generate each fake location indepen-
dently and identically distributed from the uniform probability
distribution. So, the fake trace is a sequence of uncorrelated
fake locations.
Aggregate Mobility IID ([45]): Generate each fake location
independently and identically distributed from the aggregate
mobility probability distribution ¯π. Again, the fake trace is a
sequence of uncorrelated fake locations.
Random Walk on Aggregate Mobility ([10], [24], [26], [56]):
Generate a fake trace by doing a random walk on the set of
locations following the probability distribution ¯p.
[49]4): Do a
Random Walk on User’s Mobility ([23],
random walk on the set of locations following the probability
distribution p(u) to generate a fake trace.
For Uniform IID and Aggregate Mobility IID, we evaluate
exactly the method described in [45]. For the other two we
evaluate a representative method in each case. In addition to
this, we evaluate our proposed technique, which only differs
4[23], [49] make fakes dependent on the user’s location over time (used to
establish the position of dummies). We make this probabilistic and so assume
usage of the user’s mobility proﬁle instead. This leads to overestimating the
privacy gain of the original algorithm.
557557
from these in that the fake traces are generated using the
method described in Section V. In all cases when a user makes
use of a location-based service, both a query for her real
location and queries for the fake locations are sent to the LBS.
Because of this, the user’s device must, upon receiving the
responses from the service provider, ﬁlter out the information
which are not related to her true query.
Using a Uniform IID mechanism may seem too simplistic
and unfair, but we point out that the related work [45] evaluates
this technique so we provide it as a point of comparison.
B. Privacy Metric
The adversary (e.g., the service provider) who observes the
LBS queries made by the user’s device wants to ﬁnd the
true sequence of locations visited by the user. To do this,
the adversary runs an inference attack which (if successful)
results in ﬁltering out the fake locations. For this, he makes use
of the aggregate mobility model (cid:5)¯p, ¯π(cid:6) and uses state-of-the
art location inference attack [46]. The attack is a localization
attack which consists in ﬁnding the user’s (true) location at
each time, given the observation (i.e., the sequence of locations
queried to the LBS). This is a well-known inference problem
for Hidden Markov models which can be solved efﬁciently
using dynamic programming.
The metric to quantify the privacy is the probability of
error of inference attack on guessing the correct location. This
is the metric predominantly used in the literature, in works
such as [45], [46]. To put it simply, this metric consists in
calculating the fraction of true locations that are missed by
the adversary. For example, if the user queries LBS on three
different occasions, but the adversary only correctly infers the
true location once (i.e., the inference attack correctly ﬁlters
out the fake locations) then the user’s location privacy is 2/3.
C. Utility Metric
With all synthetic generation techniques (i.e., ours and [10],
[23], [24], [26], [45], [49], [56]), the user’s real location will
always be among the locations queried to the LBS. Therefore,
as identiﬁed by related work, there is no utility loss in terms
of quality of service degradation. That is, the user will always
obtain an accurate response to her query (after ﬁltering out
responses corresponding to fake location queries).
Therefore, we measure the utility loss as the bandwidth
overhead. This is the metric used in the literature on fake
generation techniques (e.g., [56], [24], [23]). The bandwidth
overhead is calculated as the number of locations (i.e., real +
fakes) sent to the LBS provider for each user query.
Beyond traditional location-based services, some service
providers (e.g., Google Now) proﬁle the user’s interest over
time based on the type of locations she visits. This is to provide
recommendations or reminders. In such cases, queries that are
sent to the server can “pollute” the user’s proﬁle, hence reduce
the predictability power of the service provider to provide
useful recommendations. To further evaluate utility for such
location-based recommender services, we calculate the number
of (distinct) semantic clusters among the locations sent by the
user at each time. We call this metric: proﬁle pollution.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:09:44 UTC from IEEE Xplore.  Restrictions apply. 
)
d
a
e
h
r
e
v
O
h
d
w
d
n
a
B
t
i
(
s
s
o
L
y
t
i
l
i
t
U
12
10
8
6
4
2
0
0.2
Our Method
Uniform IID
Agg Mobility IID
RW Agg Mobility
RW User Profile
12
10
)
n
o
i
t
u
l
l
o
P
e
l
i
f
o
r
P
(
s
s
o
L
y
t
i
l
i
t
U
8
6
4
2
Our Method
Uniform IID
Agg Mobility IID
RW Agg Mobility
RW User Profile
in place of real location traces. We emphasize that the seed
traces and the alternative dataset are not released. Only the
generated synthetic traces are supposed to be released.
0.4
0.6
Location Privacy
0.8
1
0
0.2
0.4
0.6
Location Privacy
0.8
1
(a)
(b)
Fig. 6: Location privacy versus utility loss for different fake gener-
ating algorithms. The privacy is measured as probability of error of
adversary in guessing the correct location of users ([45], [46]). We
plot the median location privacy across all LBS users. A user makes,
on average, an LBS query every 40 minutes. We evaluate the use of
1, 5, 10 fake traces, hence three dots for each algorithm. (We repeated
the experiment 20 times and took the average: 4 times with a different
selection of fake traces, and for each of such selection, 5 times
to eliminate the randomness.) The utility loss is (a) the bandwidth
overhead ([56], [24]), i.e., number of distinct locations that are sent
to the server; and (b) the proﬁle pollution, i.e., the number of distinct
semantic classes exposed for each LBS access.
D. Results
Figure 6 shows the tradeoff between location privacy and
utility for various methods of generating fake traces. We evalu-
ate the utility loss in terms of two metrics: bandwidth overhead
(Figure 6a) which is predominantly used in the literature, and
also the proﬁle pollution (Figure 6b). We evaluate the privacy
for three different number of fake traces: 1, 5, 10. Although the
number of fake traces are the same, across different algorithms,
the average number of distinct locations sent to the LBS is not
the same. This is because of the potential overlap between fake
traces available to the user. Methods such as Uniform IID, Agg
Mobility IID, and RW Agg Mobility have a high randomness
in selecting fake traces from all possible locations. Thus, the
chance of overlap is small. Our method and the RW User
Proﬁle method have both lower bandwidth overhead.
Results show that our method clearly outperforms all the
existing techniques, especially the random strategies. For the
case of RW User Proﬁle method, the privacy level against the
tracking attack gets closer to what we achieve (which is almost
maximum), due to the fact that the fake traces generated by
RW User Proﬁle are semantically very similar to the user’s
locations, and hence creates high confusion, hence error, for
the adversary. However, it is very important to note that the
RW User Proﬁle is never a privacy-preserving fake injection
method as the adversary can easily de-anonymize the user, no
matter if he makes mistakes on exactly tracking the user at
each access time (as shown here).
Overall, the plot shows that our method is the strongest
fake generating algorithm. Note that the absolute privacy levels
changes if the adversary knowledge changes. But, what we are
interested in is the relative gain of our method to others.
VIII. EVALUATION: SYNTHETIC DATA RELEASE
In this scenario, the synthetic traces form a location dataset
that is meant to be used for various geo-data analysis tasks
558558
A. Setup
We generate a large number of fake traces out of which we
ultimately select: 10 datasets each containing 30 traces. This
is done in order to have each fake dataset of the same size
and format as the seed dataset, so that we can compare the
suitability of using one of those fake dataset instead of the
seed dataset for various geo-data analysis tasks.
B. Privacy
The location privacy of those individuals who contributed
to the seed dataset is already guaranteed by our use of the
privacy test. However, we must make sure that we are able
to generate traces which pass the privacy test with acceptably
tight constraints. Therefore, this is what we evaluate here. Out
of all fake traces generated from our 30 seed traces, on average
80% of them could pass the geographic and intersection
privacy tests with tight constraints (δi = 0, and δs = 0.1), so
it is not difﬁcult to synthesize traces that satisfy such privacy
guarantees. Regarding the plausible deniability part of the test,
the question is whether we are able to ﬁnd enough real traces