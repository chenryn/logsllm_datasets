all the three dimensions of background knowledge while achiev-
ing 88.7% accuracy when the inferrer does not know any of them
for Tiny-ImageNet dataset. Second, among the three dimensions
of background knowledge, training algorithm is the most infor-
mative for STL10 and Tiny-ImageNet, while the three dimensions
contribute equally for CIFAR10. For instance, on Tiny-ImageNet,
EncoderMI-V achieves 94.1% accuracy when the inferrer only has
access to the training algorithm, while EncoderMI-V respectively
achieves 89.1% and 93.0% accuracy when the inferrer only has
access to the encoder architecture and the pre-training data distri-
bution. For CIFAR10, EncoderMI-V achieves around 86.8% accuracy
when the inferrer only has access to any of the three dimensions.
Third, there is no clear winner among the encoder architecture and
pre-training data distribution. For instance, having access to the pre-
training data distribution alone (i.e., B = (‚àö
,√ó,√ó)) achieves higher
accuracy than having access to the encoder architecture alone (i.e.,
‚àö
B = (√ó,
,√ó)) for all our three methods on Tiny-ImageNet, but we
observe the opposite for EncoderMI-V and EncoderMI-T on STL10.
Impact of ùëõ: Figure 1b shows the impact of the number of aug-
mented inputs ùëõ on the accuracy of our methods for CIFAR10, where
‚àö). We observe
the inferrer‚Äôs background knowledge is B = (‚àö
that, for EncoderMI-V and EncoderMI-T, the accuracy first increases
and then saturates as ùëõ increases. However, for EncoderMI-S, the ac-
curacy first increases and then decreases as ùëõ increases. We suspect
the reason is that as ùëõ increases, the number of pairwise similarity
scores in the membership features increases exponentially, making
set-based classification harder.
‚àö
,
,
Session 7A: Privacy Attacks and Defenses for ML CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea2089(a) EncoderMI-V
(b) EncoderMI-S
(c) EncoderMI-T
Figure 2: Impact of the size of the pre-training dataset (x-axis) and the shadow dataset (y-axis) on the accuracy of membership
inference. Both the pre-training dataset and shadow dataset are randomly sampled from STL10.
,
‚àö
,
Impact of the similarity metric ùëÜ: Table 3 shows the impact of
the similarity metric ùëÜ on our methods, where ‚ÄúCorrelation‚Äù refers
to Pearson correlation coefficient. We have two observations from
the experimental results. First, the cosine similarity metric achieves
the highest accuracy (or precision or recall). The reason is that
the cosine similarity metric is also used in the pre-training of the
target encoder. Second, our methods still achieve high accuracy (or
precision or recall) when using different similarity metrics from
the one used in the pre-training of the target encoder.
Impact of the size of the pre-training and shadow datasets: Fig-
ure 2 shows the impact of the size of the pre-training dataset and
the shadow dataset on the accuracy of our three methods, where
‚àö). Both the
the inferrer‚Äôs background knowledge is B = (‚àö
pre-training dataset and shadow dataset are randomly sampled
from the unlabeled data of STL10, but they do not have overlaps.
Note that we did not use CIFAR10 in these experiments because its
dataset size is small and we cannot sample disjoint pre-training and
shadow datasets with large sizes. First, we observe that the accu-
racy of our methods increases as the pre-training dataset becomes
smaller. The reason is that the target encoder is more overfitted to
the pre-training dataset when its size is smaller. Second, our meth-
ods are less sensitive to the shadow dataset size. In particular, given
a pre-training dataset size, each of our three methods achieves sim-
ilar accuracy when the shadow dataset size ranges between 5,000
and 25,000. Our results show that the shadow encoder pre-trained
on shadow dataset with various sizes can mimic the behavior of
the target encoder in terms of membership inference.
Impact of data augmentation: The data augmentation opera-
tions an inferrer uses may be different from those used to pre-train
the target encoder. In this experiment, we explicitly study the im-
pact of data augmentation. We assume the inferrer uses a compre-
hensive list of four commonly used data augmentation operations,
i.e., random grayscale, random resized crop, random horizontal flip,
and color jitter. We gradually increase the number of overlapped
data augmentation operations between the inferrer‚Äôs comprehen-
sive list and the target encoder. In particular, the target encoder
starts from only using data augmentation operation Gaussian blur,
which is not in the inferrer‚Äôs list. Then, we add the inferrer‚Äôs data
augmentation operations to the target encoder‚Äôs pre-training mod-
ule one by one in the following order: random grayscale, random
resized crop, random horizontal flip, and color jitter. We calculate
Figure 3: Impact of data augmentation, where the method is
EncoderMI-V and the dataset is CIFAR10.
the membership inference accuracy and the classification accuracy
of the downstream classifier for each target encoder, where the
downstream classifier is built as in Baseline-A. Figure 3 shows the
results. We observe the number of overlapped data augmentation
operations between the referrer and the target encoder controls a
trade-off between membership inference accuracy and utility of
the target encoder, i.e., the target encoder is more resistant against
membership inference but the downstream classifier is also less
accurate when the target encoder uses less data augmentation op-
erations from the referrer‚Äôs comprehensive list.
6 APPLYING OUR METHOD TO CLIP
CLIP [38] jointly pre-trains an image encoder and a text encoder on
400 million (image, text) pairs collected from the Internet. OpenAI
has made the image encoder and text encoder publicly available.
We view CLIP‚Äôs image encoder with the ViT-B/32 architecture as
the target encoder and apply our EncoderMI to infer its members.
Specifically, given an input image, we aim to use EncoderMI to
infer whether it was used by CLIP or not. Next, we first introduce
experimental setup and then present experimental results.
6.1 Experimental Setup
Potential members and ground truth non-members: To evalu-
ate our EncoderMI for CLIP‚Äôs image encoder, we need an evaluation
dataset consisting of both ground truth members and non-members.
However, the pre-training dataset of CLIP is not released to the
public. Therefore, we cannot obtain ground truth members of CLIP.
25000200001500010000500050001000015000200002500083.1%87.7%89.8%90.2%96.0%82.1%87.1%89.7%90.1%95.9%81.9%87.0%89.7%90.1%95.9%81.0%86.8%89.7%90.1%95.9%79.9%86.7%89.6%90.0%95.8%0.800.820.840.860.880.900.920.940.9625000200001500010000500050001000015000200002500080.1%83.6%85.6%86.5%92.2%80.0%83.6%85.5%86.4%92.2%79.6%83.6%85.5%86.4%92.2%79.2%83.5%85.2%86.4%92.1%78.4%83.1%85.0%86.4%92.0%0.800.820.840.860.880.900.9225000200001500010000500050001000015000200002500082.8%86.4%88.2%88.9%94.4%82.7%86.2%88.1%88.9%94.3%82.5%86.2%88.1%88.8%94.3%82.5%85.7%88.0%88.8%94.1%82.2%85.6%87.8%88.7%94.1%0.840.860.880.900.920.9401234#Overlapped data augmentation operations0.10.20.30.40.50.60.70.80.9AccuracyMembership inference accuracyClassification accuracy of downstream classifierSession 7A: Privacy Attacks and Defenses for ML CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea2090(a) Google
(b) Flickr
Figure 4: Histograms of the average pairwise cosine similar-
ity for potential members and ground truth non-members.
However, we can collect some images that are potential members
and ground truth non-members of the CLIP‚Äôs pre-training dataset.
Specifically, according to Radford et al. [38], the (image, text) pairs
used to pre-train CLIP were collected from the Internet based on
a set of 500,000 popular keywords. Therefore, we collect the fol-
lowing two evaluation datasets, each of which has 1,000 potential
members and 1,000 ground truth non-members:
‚Ä¢ Google. We use the class names of CIFAR100 [30] as key-
words and use Google image search to collect images. Ap-
pendix A shows the complete list of class names, e.g., ‚Äúclock‚Äù,
‚Äúhouse‚Äù, and ‚Äúbus‚Äù. In particular, we use a publicly available
tool1 to crawl images from Google search based on the key-
words. We collected 10 images for each keyword, and thus
we collected 1,000 images in total. We treat these images as
potential members as they were potentially also collected and
used by CLIP. To construct ground truth non-members, we
further collected 2,000 images from Google search using the
keywords. We randomly divided them into 1,000 pairs; and
for each pair, we resized the two images to the same size, and
we concatenated them to form a new image, which results in
1,000 images in total. We treat these images as ground truth
non-members of CLIP.
‚Ä¢ Flickr. Similar to the above Google evaluation dataset, we
collected an evaluation dataset from Flickr using the 100 key-
words and a publicly available tool2. Specifically, we collected
1,000 images as potential members. Moreover, we further col-
lected 2,000 images and randomly paired them to be 1,000
images, which we treat as ground truth non-members.
We acknowledge that some of the potential members may not be
ground truth members of CLIP in both of our evaluation datasets.
For each potential member and ground truth non-member, we resize
them as the input size of CLIP, which is 224 √ó 224.
Inference classifiers: We assume the inferrer does not know the
pre-training data distribution, encoder architecture, and training
algorithm of CLIP, which is the most difficult scenario for our En-
coderMI. We use the inference classifiers we built in our previous
experiments in Section 5. Specifically, in our previous experiments,
for each of our three methods (i.e., EncoderMI-V, EncoderMI-S, and
EncoderMI-T) and each of the three shadow datasets (i.e., CIFAR10,
STL10, and Tiny-ImageNet), we have built 8 inference classifiers
corresponding to the 8 types of background knowledge. We use
the inference classifiers corresponding to the background knowl-
edge B = (‚àö
‚àö) in our previous experiments (i.e., the inference
‚àö
,
,
1https://github.com/hardikvasa/google-images-download
2https://stuvel.eu/software/flickrapi
Table 4: Accuracy, precision, and recall (%) of EncoderMI for
CLIP‚Äôs image encoder.
(a) Google
Shadow dataset Accuracy
Method
EncoderMI-V
EncoderMI-S
EncoderMI-T
Method
EncoderMI-V
EncoderMI-S
EncoderMI-T
Tiny-ImageNet
CIFAR10
STL10
CIFAR10
STL10
CIFAR10
STL10
Tiny-ImageNet
Tiny-ImageNet
Tiny-ImageNet
CIFAR10
STL10
CIFAR10
STL10
CIFAR10
STL10
Tiny-ImageNet
Tiny-ImageNet
(b) Flickr
Shadow dataset Accuracy
70.0
71.0
67.6
74.7
74.6
73.2
70.3
71.5
66.3
74.9
73.5
74.7
71.7
72.7
71.6
73.9
74.5
74.3
Precision
64.5
65.4
62.0
70.2
70.2
68.1
64.2
65.6
62.4
Precision
72.0
70.9
73.1
69.1
71.4
69.3
68.8
70.8
71.2
Recall
88.8
88.9
90.7
87.1
86.8
88.1
89.7
90.0
90.1
Recall
81.5
79.3
78.2
79.2
76.4
79.0
80.1
80.8
79.4
classifiers corresponding to the last rows of Table 2, Table 5, and
Table 6) to infer members of CLIP. Given an input image, we create
10 augmented inputs and use the CLIP‚Äôs image encoder to produce
a feature vector for each of them. Then, we compute the 45 pair-
wise cosine similarity scores between the 10 feature vectors, which
constitute the set of membership features for the input image. Our
inference classifiers predict the membership status of the input
image based on the membership features.
6.2 Experimental Results
Cosine similarity score distribution for potential members
and ground truth non-members: Recall that, for each potential
member or ground truth non-member, EncoderMI constructs mem-
bership features consisting of 45 pairwise cosine similarity scores.
We compute the average of the 45 pairwise cosine similarity scores
for each potential member or ground truth non-member. Figure 4
shows the histograms of the average pairwise cosine similarity