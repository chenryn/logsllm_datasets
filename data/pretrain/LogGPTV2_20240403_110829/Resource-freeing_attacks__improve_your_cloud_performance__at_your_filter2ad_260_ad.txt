LLCProbe domain under both no RFA and with RFA 320 in
pinned core case.
(top)
3500
3000
2500
2000
1500
1000
500
0
)
s
p
r
(
e
t
a
R
t
s
e
u
q
e
R
r
e
v
r
e
S
b
e
W
d
e
v
r
e
s
b
O
No-RFA
160
320
640
1000
2000
3000
Oﬀered Web Server Request Rate (rps)
Figure 7: Offered vs. observed load on web server with varying
RFA intensities when all the VMs ﬂoat across all cores.
2875.2 Evaluation on EC2
The above experiments clearly indicate that RFAs can provide sub-
stantial gains in a controlled setting. To verify that the attacks will
also work in a noisier, more realistic setting, we turn to Amazon’s
Elastic Compute Cloud (EC2). There are several reasons it is im-
portant to evaluate RFAs in a real cloud setting. First of all, the
success of the RFA is highly dependent on the overall load of the
physical machine. The instances in question (the beneﬁciary and
the victim) make up only a portion of the total possible load on a
single machine.
If the other instances on the machine are heavy
resource users, they will constantly interfere with the beneﬁciary
and overshadow any performance beneﬁt from slowing the victim.
Thus, if most physical machines in EC2 are constantly under heavy
load, we are unlikely to see much effect from an RFA on a single
victim. Furthermore, EC2’s Xen conﬁguration is not publicly avail-
able and may prevent RFAs. Thus, to understand if RFAs actually
behave as an attacker would hope, it is necessary to verify their
effectiveness in a live setting like EC2.
Ethical considerations. When using EC2 for experiments, we are
obligated to consider the ethical, contractual, and legal implications
of our work. In our experiments, we use instances running under
our accounts in our names as stand-ins for RFA victims and beneﬁ-
ciaries. We abide by the Amazon user agreement, and use only the
legitimate Amazon-provided APIs. We only attempt to send rea-
sonable levels of trafﬁc (slightly more than 2000 rps for a small
web page) to our own instances (the stand-ins for victims). We
do not directly interact with any other customer’s instances. Our
experiments are therefore within the scope of typical customer be-
havior on EC2: running a utilized web server and a CPU intensive
application. Our experiments can therefore indirectly impact other
customer’s service only to the same extent as typical use.
Test machines. To test an RFA, we require control of at least two
instances running on the same physical machine. As AWS does
not provide this capability directly, we used known techniques [25]
to achieve sets of co-resident m1.small instances on 12 different
physical machines in the EC2 us.east-1c region. Speciﬁcally, we
launched large numbers of instances of the same type and then used
RTT times of network probes to check co-residence. Co-residence
was conﬁrmed using a cache-based covert channel. Nine of these
were the same architecture: Intel Xeon E5507 with a 4MB LLC.
We discarded the other instances to focus on those for which we
had a large corpus, which are summarized in Figure 9.
Machine
E5507-1
E5507-2
E5507-3
# Machine
4 E5507-4
2 E5507-5
2 E5507-6
# Machine
3 E5507-7
2 E5507-8
2 E5507-9
#
2
3
3
Figure 9: Summary of EC2 machines and number of co-
resident m1.small instances running under our accounts.
Each instance ran Ubuntu 11.04 with Linux kernel 2.6.38-11-
virtual. For each machine, we choose one of the co-resident in-
stances to play the role of the beneﬁciary and another one to be the
victim. The beneﬁciary was conﬁgured with various benchmarks
while the victim had the same Apache installation and conﬁguration
as in the local testbed (see Section 5.1). Any remaining co-resident
instances were left idle.
We used separate m1.small instances to run the victim load and
the RFA trafﬁc generator. We note that despite offering load of
2000 rps on EC2, the achieved load was only around 1500 on aver-
age and sometimes slightly less in the presence of RFAs.
Experimental procedure. We chose a subset of the benchmarks
(sphinx, mcf, LLCProbe, and bzip2) used in the local testbed for
the experiments on EC2. We ran each benchmark on a beneﬁciary
instance while a co-resident victim received requests made by a
client load generator as well as an RFA helper, both located on
separate EC2 instances that were not co-resident with the beneﬁ-
ciary and victim. We used an intensity of 512 ms and changed the
duration of each RFA request to 16 ms, as that was most effective
in our experiments. For each benchmark we run the benchmark no
RFA, followed by running it with the RFA, and we repeat this three
times. (For LLCProbe, each single run of the benchmark was in
fact ﬁve sequential runs to gather more samples.) This gives 4 data
points (10 for LLCProbe). The interleaving of no-RFA and RFA
helped limit the effects of unexpected intermittent noise (e.g., from
other co-resident VMs outside our control) that may effect mea-
surements. Throughout these experiments the client load generator
sends web server requests at a conﬁgured rate. We also measure the
baseline with no background trafﬁc once at the start of measure-
ments for each benchmark.
Aggregate effectiveness. We start by looking at average perfor-
mance of the RFA’s across all nine machines. Figure 10 depicts the
results as normalized average runtimes (average runtime divided by
average baseline runtime). Thus higher is better (less slowdown
from interference). What we see is that the RFAs provides slight
performance improvements across all the instances and, in partic-
ular, never hurts average runtime. While the absolute effects are
small, they are not insigniﬁcant: the RFA improved LLCProbe per-
formance by 6.04%. For the SPEC benchmarks (not shown), we see
that the degradation due to the victim (the No-RFA) is, on average,
less than observed on the local testbed. This may be due to the dif-
ferent architectures and software conﬁgurations, or it may be due
to higher contention in the baseline case due to other co-resident
instances (owned by other customers). Given the smaller gap be-
tween baseline and No-RFA, there is less absolute performance to
recover by mounting an RFA. Nevertheless, as a fraction of lost per-
formance, even here the beneﬁciary receives back a large fraction
of its performance lost to interference.
No-RFA
512
e
c
n
a
m
r
o
f
r
e
P
d
e
z
i
l
a
m
r
o
N
1
0.95
0.9
0.85
0.8
0.75
0.7
L
L
C
P
r
o
b
z
i
p
2
m
c
f
b
e
s
p
h
i
n
x
Figure 10: Normalized performance (average baseline runtime
over over average runtime) across all machines on EC2 for var-
ious workloads.
288Per-machine breakdown. To understand the effect further and, in
particular, to get a better sense of whether other (uncontrolled) co-
resident instances are causing contention, we breakdown the results
by individual machine. Figure 11 depicts average runtimes for each
machine and for each of the four benchmarks. (The error bars for
LLCProbe denote one standard deviation — for the other bench-
marks we omitted these due to having three samples.) As it can be
seen, the baseline, No-RFA, and RFA performances all vary signif-
icantly across the different machines. While we cannot know the
precise reason for this, we speculate that it is mostly due to con-
tention from other customer VMs or, possibly, slight differences
in conﬁguration and baseline software performance of the distinct
machines.
Likewise the beneﬁt of performing an RFA varies by machine.
In the case of LLCProbe, RFAs were always beneﬁcial, but the de-
gree to which they improved performance varied. Machine E5507-6
had the highest speedup of 13% from the RFA, which corresponded
to decreasing the cost of contention by about 33%. Interestingly,
there seems to be little correlation between benchmarks, for exam-
ple E5507-6 had negative improvement from RFA for the bzip2 and
mcf benchmarks. Other machines faired better for SPEC bench-
marks, for example E5507-1 had a 3.2% performance improvement
under RFAs.
These varied results are not unexpected in the noisy environment
of EC2. We draw two general conclusions. First, RFAs can provide
signiﬁcant speedups in the (real-world) environment of EC2, but
the beneﬁts will vary depending on a variety of environmental fac-
tors. Second, given that the aggregate beneﬁt across all machines is
positive, a greedy customer will —on average over the long term—
beneﬁt from mounting RFAs.
6. DISCUSSION
Practical dimensions. Deploying a resource-freeing attack like
the one explored in the last few sections would be subject to several
complicating issues in practice. First, it may be difﬁcult to pre-
dictably modify the victim’s workload because the victim’s normal
(pre-RFA) workload may be unknown to the attacker. As shown in
Section 5, the amount of extra work required was dependent on the
existing workload of the victim. Here, simple adaptive techniques,
where workload is continually introduced as long as it improves the
beneﬁciary’s performance, may sufﬁce. Moreover, our results sug-
gest an attacker would typically do well to overestimate the RFA
intensity required.
Second, it may be that co-resident instances do not have services
that are accessible to the RFA helper. As discussed in Section 3
a wide swath of, e.g., EC2 instances run public web servers, and
such interrupt-driven workloads are likely to be the most damag-
ing to cache-bound workloads. Even public servers may only be
indirectly accessible to the helper, for example if they lie behind
a load balancer. Future work might target RFAs that can exploit
other avenues of generating a bottleneck resource for the victim,
for example the attacker might generate extra contention on a disk
drive using asynchronous accesses in order to throttle a victim’s
I/O bound processes. Such an attack would not require any form of
logical access to the victim.
Third, the client workload we experimented with does not reﬂect
all victim workloads seen in practice. For example, if thousands
of independent clients submit requests concurrently, the RFA may
not be able to effect as much displacement of inbound connection
requests (though request processing will still be displaced). Future
work might clarify the vulnerability of other victim workloads to
RFAs.
In the setting of public clouds, performance
Economics of RFAs.
improvement can translate directly to cost improvement since one
pays per unit time. For long running jobs, even modest improve-
ments in performance can signiﬁcantly lower cost. Of course, one
must account for the cost of mounting the RFA itself, which could
diminish the cost savings. The RFAs we explored used a helper that
sends a small number of web requests to the victim. For example,
our helper uses only 15 Kbps of network bandwidth with a CPU uti-
lization of 0.7% (of the E5430 as conﬁgured in our local testbed).
We located this helper on a separate machine. That the helper is so
lightweight means that one might implement it in a variety of ways
to ameliorate its cost. For example, by running it in places where
spare cycles cannot be used for the main computational task or even
on a non-cloud system used to help manage cloud tasks. One could
also use a cheap VM instance that runs helpers for a large set of
beneﬁciaries, thereby amortizing the cost of the VM instance.
A related issue is that of VM migration. While contemporary
IaaS clouds do not enable dynamic migration, customers may move
a VM from one system to (hopefully) another by shutting it down
and restarting it. The beneﬁciary could therefore try to migrate
away from a contended host instead of mounting an RFA. We view
migration and RFAs as two complementary directions along which
a greedy customer will attempt to optimize their efﬁciency. Which
strategy, or a combination thereof, works best will depend on the
contention, the workload, the likelihood of ending up on an uncon-
tended host, pricing, etc. Understanding the relative economic and
performance beneﬁts of migration and RFAs is an interesting ques-
tion for future work.
Preventing RFAs. To prevent the kinds of RFAs we consider, one
could deploy VMs onto dedicated instances. This was suggested in
the cloud setting by Ristenpart et al. [25], and subsequently added
as a feature in EC2. However, the signiﬁcant cost of dedicated in-
stances makes it impractical for a variety of settings.
There are two primary methods for preventing RFAs even in
the case of multiplexed physical servers: stronger isolation and
smarter scheduling. A hypervisor that provides strong isolation for
every shared resource can prevent RFAs. This entails using non-
work conserving scheduling, so that idleness of a resource allo-
cated to one VM does not beneﬁt another. In addition, it requires
hardware support for allocating access to processor resources, such
as the cache and memory bandwidth. With current hardware, the
only possibility is cache coloring, which sets virtual-to-physical
mappings to ensure that guest virtual machines do not share cache
sets [14]. This effectively partitions the cache in hardware, which
hurts performance for memory-intensive workloads. Finally, it re-
quires that the hypervisor never overcommit and promise more re-
sources to VMs than are physically available, because concurrent
use of overcommitted resources cannot be satisﬁed. While this ap-