patches to be within one restricted region. Given this threat
model, all the corrupted features will also be within a small
window in the feature map space when using a CNN with
small receptive ﬁelds; we call this window malicious window.
2242    30th USENIX Security Symposium
USENIX Association
Provable Robustness via an adversary dilemma. With the
robust masking defense, we put the adversary in a dilemma.
If the adversary wants to succeed in the attack, they need
to increase the class evidence of a wrong class. However,
increasing the class evidence will trigger our detection and
masking mechanism that reduces the class evidence. As a
result, this dilemma imposes an upper bound on the class
evidence of any class (s ¯y in Line 6 of Algorithm 1), which
further enables provable robustness. In fact, we can ﬁrst prove
the following lemma.
Lemma 1. Given a malicious window w ∈ W , a class ¯y ∈ Y ,
the set of sliding windows W , the clipped and masked class
evidence of class ¯y (i.e., s ¯y in Algorithm 1) can be no larger
than SUM(ˆu ¯y (cid:12) (1 − w))/(1 − T ) when setting cl = 0 and
T ∈ [0,1).
Proof. The goal of the adversary is to modify the content
within the malicious window w to bypass our defense. Let e be
the amount of class evidence within w and t = SUM(ˆu ¯y(cid:12)(1−
w)) be the class evidence outside w. Note that the adversary
has control over the value e but not t, and that the total class
evidence of the modiﬁed malicious feature tensor is now t +
e. Next, the subprocedure DETECT will take the malicious
feature tensor as input and detect a suspicious window w∗
¯y.
Finally, a mask is applied and the class evidence is reduced
to s ¯y = t + e− e(cid:48), where e(cid:48) is the class evidence within the
detected window w∗
¯y. To obtain the upper bound of s ¯y given a
speciﬁc malicious window w, we will determine the ranges
of e,e(cid:48) in four possible cases of the detected window w∗
¯y, as
illustrated in Figure 5.
1. Case I: the malicious window is perfectly detected. In
¯y and thus e = e(cid:48). The class
this case, we have w = w∗
evidence s ¯y = t + e− e(cid:48) = t.
2. Case II: a benign window is incorrectly detected. In this
case, we have e(cid:48) = SUM(ˆu ¯y(cid:12)w∗
¯y). The adversary has the
constraint that e ≤ e(cid:48); otherwise, the malicious window
w instead of w∗
¯y will be detected. Therefore, we have
s ¯y = t + e− e(cid:48) ≤ t.
3. Case III: the malicious window is partially detected.
¯y (cid:12) (1 − w) be the detected benign region,
Let r1 = w∗
¯y (cid:12) w be the detected malicious region, and r3 =
r2 = w∗
(1− w∗
¯y)(cid:12) w be the undetected malicious region. Let
q1,q2,q3 be the class evidence within region r1,r2,r3,
respectively. We have e = q2 +q3 and e(cid:48) = q1 +q2. Simi-
lar to Case II, the adversary has the constraint that e ≤ e(cid:48),
or q3 ≤ q1; otherwise, w instead of w∗
¯y will be detected.
Therefore, we have s ¯y = t + e− e(cid:48) = t + q3 − q1 ≤ t.
4. Case IV: no suspicious window detected. This case hap-
pens when the largest sum within every possible win-
dow does not exceed the detection threshold. We have
e/(e +t) ≤ T , which yields e ≤ tT /(1 − T ). We also
Figure 5: Illustrations for four cases of detected window w∗
¯y. The
clipped and masked class evidence satisﬁes s ¯y = t + e− e(cid:48). For Case
I, II, III, we have e ≤ e(cid:48) and therefore s ¯y ≤ t. For Case IV, we have
e ≤ tT /(1− T ),e(cid:48) = 0 and therefore s ¯y ≤ t/(1− T ).
have e(cid:48) = 0 since no mask is applied. Therefore, the
class evidence satisﬁes s ¯y = t + e ≤ t/(1 − T ), where
T ∈ [0,1].
Combining the above four cases, we have the upper bound
of the target class evidence to be t/(1− T ) = SUM(ˆu ¯y (cid:12) (1−
w))/(1− T ).
Provable analysis. Lemma 1 shows that robust masking lim-
its the adversary’s ability to increase the malicious class evi-
dence. If the upper bound of malicious class evidence is not
large enough to dominate the lower bound of the true class
evidence, we can certify the robustness of our defense on a
given clean image. The pseudocode of our provable analysis
is provided in Algorithm 2. Next, we will explain our analysis
by proving the following theorem.
Theorem 1. Let cl = 0, T ∈ [0,1), w ∈ W denote the sliding
windows whose sizes are determined by Equation 1, and A(x)
denote the adversary’s constraint as deﬁned in Section 2.2. If
Algorithm 2 returns True for a given image x, our defense
in Algorithm 1 can always make a correct prediction on any
adversarial image x(cid:48) ∈ A(x).
Proof. Our provable analysis in Algorithm 2 iterates over
all possible windows w ∈ W and all possible target classes
y(cid:48) ∈ Y (cid:48) = Y \{y} to derive provable robustness for the un-
targeted attack with a patch at any location. For each possi-
ble malicious window w, Algorithm 2 determines the upper
bound of the class evidence of each target class (Line 3-6)
and the lower bound of the class evidence of the true class
(Line 7-9).
For each target class y(cid:48), we can apply Lemma 1 and get the
upper bound sy(cid:48) = SUM(ˆuy(cid:48) (cid:12) (1− w))/(1− T ).
y)).
For the true class y, the optimal attacking strategy is to
set all true class evidence within the malicious window w to
cl = 0. Note that the true class evidence within the detected
window w∗
y (if any) will be masked. Therefore, the lower
bound sy is equivalent to removing class evidence within w
and w∗
y, i.e., sy = SUM(ˆuy (cid:12) (1− w)(cid:12) (1− w∗
The ﬁnal step is to compare the upper bound of target class
evidence sy(cid:48) with the lower bound of true class evidence sy.
USENIX Association
30th USENIX Security Symposium    2243
Algorithm 2 Provable analysis of robust masking
Input: Image x, true class y, wrong label set Y (cid:48) = Y \{y},
feature extractor F of model M , clipping upper bound
ch, the set of sliding windows W , detection threshold T .
Output: Whether the image x has provable robustness
1: procedure PROVABLEANALYSISMASKING
2:
for each w ∈ W do
(cid:46) Upper bound of target class evidence
for each y(cid:48) ∈ Y (cid:48) do
ˆuy(cid:48) ← CLIP(F (x,y(cid:48)),0,ch)
sy(cid:48) ← SUM(ˆuy(cid:48) (cid:12) (1− w))/(1− T )
end for
(cid:46) Lower bound of true class evidence
ˆuy ← CLIP(F (x,y),0,ch)
y ← DETECT(ˆuy (cid:12) (1− w),W ,T )
w∗
sy ← SUM(ˆuy (cid:12) (1− w)(cid:12) (1− w∗
y))
(cid:46) Feasibility of an attack
if maxy(cid:48)∈Y (cid:48)(sy(cid:48)) > sy then
return False
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
end for
13:
return True
14:
15: end procedure
end if
If the condition maxy(cid:48)∈Y (cid:48)(sy(cid:48)) > sy is satisﬁed, we assume
an attack is possible and the algorithm returns False. On
the other hand, if Algorithm 2 checks all possible malicious
windows w ∈ W for all possible target classes y(cid:48) ∈ Y (cid:48) and
does not return False in any case, this means our defense on
this clean image has provable robustness against any possible
patch and can always make a correct prediction.
Provable adversarial training. We note that our provable
analysis can be incorporated into the training process to im-
prove provable robustness. We call this “provable adversarial
training" and will discuss its details in Appendix A.
5 Evaluation
In this section, we provide a comprehensive evaluation of
PatchGuard. We report the provable robust accuracy of our
defense (obtained from Algorithm 2 and Theorem 1) on the
ImageNet [12], ImageNette [16], and CIFAR-10 [23] datasets
for various patch sizes. We instantiate our defense with mul-
tiple different CNNs with small receptive ﬁelds and com-
pare their performance with previous provably robust de-
fenses [9, 28, 59]. We also provide a detailed analysis of our
defense performance with different settings.
5.1 Experiment Setup
Datasets. We report our main provable robustness results on
the 1000-class ImageNet [12], 10-class ImageNette [16], and
10-class CIFAR-10 [23] datasets. ImageNet and ImageNette
images have a high resolution and were resized and cropped to
224×224 or 299×299 before being fed into different models
while CIFAR-10 images have a lower resolution of 32×32.
CIFAR-10 images are rescaled to 192×192 before being fed
to BagNet. Further details are in our technical report [55].
Models. As discussed in Section 3.3, we have two general
ways to build a network with small receptive ﬁelds. In our
evaluation, we instantiate the ensemble approach using a
de-randomized smoothed ResNet (DS-ResNet) [28], and the
small convolution kernel approach using BagNet [5]. The
DS-ResNet [28] takes a rectangle pixel patch, or a pixel band,
as the input of its base model and uses prediction majority vot-
ing for the ensemble prediction. In contrast, our defense uses
robust masking for aggregation. The BagNet [5] architecture
replaces a fraction of 3×3 convolution kernels of ResNet-50
with 1×1 kernels to reduce the receptive ﬁeld size. It was
originally proposed in the context of interpretable machine
learning while we use this model for provable robustness
against adversarial patch attacks.
We analyze performance of ResNet-50, BagNet-33, BagNet-
17, BagNet-9, and DS-25-ResNet-50. These 5 models have a
similar network structure but have different receptive ﬁelds
of 483×483, 33×33, 17×17, 9×9, and 25×299, respectively.
For CIFAR-10, we additionally include a DS-ResNet-18 with
a band size of 4 (DS-4-ResNet-18). Model training details are
in our technical report [55].
Defenses. We report the defense performance of our robust
masking defense with the BagNet (Mask-BN) and with the
DS-ResNet (Mask-DS). We also compare with the exist-
ing Clipped BagNet (CBN) [59], De-randomized Smoothing
(DS) [28] and Interval Bound Propagation based certiﬁed
defense (IBP) [9]. The default settings of our defense are
listed in Table 3. Note that for PatchGuard, we use the same
set of parameters (i.e., cl,ch,T ) for all datasets and models.
For previous defenses, we use the optimal parameter settings
obtained from their respective papers.
Attack Patch Size. For ImageNet and ImageNette, we ana-
lyze our defense performance against a single square adver-
sarial patch that consists of up to 1%, 2%, or 3% pixels of
the images. For CIFAR-10, we report results for a patch con-
sisting of 0.4% or 2.4% of the image pixels. In Appendix F,
we analyze the defense performance against larger patches to
understand the limits of PatchGuard.
Table 3: Default defense settings for Mask-BN and Mask-DS
Setting
Mask-BN on ImageNet(te) BagNet-17 logits
Mask-BN on CIFAR-10
BagNet-17 logits
Mask-DS on ImageNet(te) DS-25-ResNet-50 conﬁdence
Mask-DS on CIFAR-10
DS-4-ResNet-18 conﬁdence
cl = 0
ch = ∞
T = 0
Feature
Parameters
2244    30th USENIX Security Symposium
USENIX Association
Table 4: Clean and provable robust accuracy for different defenses
Dataset
Patch size
Accuracy
Mask-BN
Mask-DS
IBP [9]
CBN [59]
DS [28]
1% pixels
clean
95.2
92.3
94.9
92.1
robust
89.0
83.1
74.6
82.3
ImageNette
2% pixels
clean
95.0
92.1
94.9
92.1
robust
86.7
79.9
60.9
79.1
3% pixels
1% pixels
clean
94.8
92.1
robust
83.0
76.8
clean
55.1
44.1
robust
32.3
19.7
computationally infeasible
94.9
92.1
45.9
75.7
49.5
44.4
13.4
17.7
ImageNet
2% pixels
clean
54.6
43.6
49.5
44.4
robust
26.0
15.7
7.1
14.0
CIFAR-10
3% pixels
0.4% pixels
2.4% pixels
clean
54.1
43.0