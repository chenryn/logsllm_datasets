title:De-anonymization of Mobility Trajectories: Dissecting the Gaps between
Theory and Practice
author:Huandong Wang and
Chen Gao and
Yong Li and
Gang Wang and
Depeng Jin and
Jingbo Sun
De-anonymization of Mobility Trajectories:
Dissecting the Gaps between Theory and Practice
Huandong Wang∗, Chen Gao∗, Yong Li∗, Gang Wang†, Depeng Jin∗, and Jingbo Sun‡
∗Department of Electronic Engineering, Tsinghua University
†Department of Computer Science, Virginia Tech
‡China Telecom Beijing Research Institute
{whd14,gc16}@mails.tsinghua.edu.cn, {liyong07,jindp}@tsinghua.edu.cn,
PI:EMAIL, PI:EMAIL
Abstract—Human mobility trajectories are increasingly col-
lected by ISPs to assist academic research and commercial ap-
plications. Meanwhile, there is a growing concern that individual
trajectories can be de-anonymized when the data is shared, using
information from external sources (e.g. online social networks).
To understand this risk, prior works either estimate the theo-
retical privacy bound or simulate de-anonymization attacks on
synthetically created (small) datasets. However, it is not clear how
well the theoretical estimations are preserved in practice.
In this paper, we collected a large-scale ground-truth trajec-
tory dataset from 2,161,500 users of a cellular network, and two
matched external trajectory datasets from a large social network
(56,683 users) and a check-in/review service (45,790 users) on
the same user population. The two sets of large ground-truth
data provide a rare opportunity to extensively evaluate a variety
of de-anonymization algorithms (7 in total). We ﬁnd that their
performance in the real-world dataset is far from the theoretical
bound. Further analysis shows that most algorithms have under-
estimated the impact of spatio-temporal mismatches between
the data from different sources, and the high sparsity of user
generated data also contributes to the underperformance. Based
on these insights, we propose 4 new algorithms that are specially
designed to tolerate spatial or temporal mismatches (or both)
and model user behavior. Extensive evaluations show that our
algorithms achieve more than 17% performance gain over the
best existing algorithms, conﬁrming our insights.
I.
INTRODUCTION
Anonymized user mobility traces are increasingly collected
by Internet Service Providers (ISP) to assist various applica-
tions, ranging from network optimization [42] to user popula-
tion estimation and urban planning [11]. Meanwhile, detailed
location traces contain sensitive information about individual
users (e.g., home and work location, personal habits). Even
there is a growing concern
after the data is anonymized,
that users can still be re-identiﬁed through external
infor-
mation [40]. Recently, the US congress has moved towards
repealing the Internet Privacy Rules and legalizing ISPs to
share (or monetize on) user data [14]. The key question is
Network  and  Distributed  Systems  Security  (NDSS)  Symposium  2018 
18-21  February  2018,  San  Diego,  CA,  USA
ISBN  1-891562-49-5
http://dx.doi.org/10.14722/ndss.2018.23211
www.ndss-symposium.org
till yet to be answered: how much of user privacy is leaked if
the ISP shares anonymized trajectory datasets?
To answer this question, early research estimates the the-
oretical privacy bound by assessing the “uniqueness” of the
trajectories [9], [40], which shows that trajectory traces are
surprisingly easy to de-anonymize. With 4 spatio-temporal
points or top 3 most visited locations, results in [9], [40] show
that 80%–95% of the user scan be uniquely re-identiﬁed in a
metropolitan city.
Recently, researchers start to evaluate more practical at-
tacks by de-anonymizing ISP trajectories using external infor-
mation (e.g., location check-ins from social networks) [8], [10],
[15], [16], [23], [27]–[29], [31]–[33], [35]. However, due to the
lack of large empirical ground-truth datasets, researchers have
to settle on small datasets (e.g., 125 users in [35], 1717 users
in [31]) or simulating attacks on synthetically generated data
(e.g., using parts of the same dataset as the victim dataset and
the external information source) [23], [32], [33]. To date, it is
still not clear how easy (or difﬁcult) attackers can massively
de-anonymize user trajectories in practice.
In this work, we spent signiﬁcant efforts to collect two
large-scale ground-truth datasets to close the gaps between
theory and practice. By collaborating with a major ISP and
two large location-based online services in China, we obtain
2,161,500 ISP trajectories (as the target dataset), 56,683 users’
GPS/check-in traces from a large social network (external
information) and 45,790 users’ GPS traces from a large online
review service (external information). The three datasets cover
the same user population with the ground-truth mapping.1
Using this dataset, we seek to empirically evaluate how well
de-anonymization algorithms approach the privacy bound, and
what practical challenges (if any) that are often neglected when
designing these algorithms. Answering this question helps to
provide more accurate assessment on the privacy risks of
sharing the anonymized ISP traces.
By implementing and running 7 major de-anonymization
algorithms against our dataset, we ﬁnd the existing algorithms
largely fail the de-anonymization task using practical data.
Their performance is far from the privacy bound [9], [40],
and massive errors occur, i.e., the hit-precision is less than
20%. Further analysis reveals a number of key factors that
are often neglected by algorithm designers. First, there widely
exist signiﬁcant spatio-temporal mismatches between the ISP
1Personally identiﬁable information (PII) has been removed before the data
is handled to us. This work received the approvals from our local intuitional
board, the ISP, the online social network, and the online review service.
trajectories and the external GPS/check-in traces, caused by
positioning errors and different location updating mechanisms.
In addition, user trajectory datasets are highly sparse across
time and users, making the de-anonymization attack very
challenging in practice.
To validate our insights, we design 4 new algorithms that
specially address the practical factors. More speciﬁcally, we
propose a spatial matching (SM) algorithm and a temporal
matching (TM) algorithm, which tolerate spatial and tem-
poral mismatches respectively. Further, we build a Gaussian
and Markov based (GM) algorithm that considers spatio-
temporal mismatches simultaneously. Finally, we enhance the
GM model by adding a user behavior model to incorporate
human mobility patterns (GM-B algorithm).
Extensive evaluation shows that our algorithms signiﬁ-
cantly outperform existing algorithms. More importantly, our
experiments reveal new insights into the relationship between
human mobility and privacy. We ﬁnd that tolerating temporal
mismatches is more important
than tolerating spatial mis-
matches. An intuitively explanation is that human mobility has
a strong locality, which naturally sets a bound for location mis-
matches. However, at the temporal dimension, since the errors
are unbounded, making the algorithm aware of the temporal
matches makes a bigger difference to the de-anonymization
performance. Finally, the GM and GM-B algorithms achieve
even better performance by considering different mismatches
and human behavior models at the same time.
Overall, our work makes four key contributions:
•
•
•
•
First, we collect the ﬁrst large-scale trajectory dataset
(with ground-truth) to evaluate de-anonymization at-
tacks. The dataset contains 2,161,500 ISP trajectories
and 56,683 external trajectories, which helps to over-
come the limitations of theoretical analysis and small-
scale validations.
Second, we build an empirical evaluation frame-
work by categorizing and implementing existing de-
anonymization algorithms (7 in total) and evaluation
metrics. Our evaluation on real-world datasets re-
veals new insights into the existing algorithms’ under-
performance.
Third, we propose new algorithms by addressing
practical factors such as spatio-temporal mismatches,
location contexts, and user-level errors. Optional com-
ponents such as user historical trajectories can also be
added to our framework to improve the performance.
Finally, extensive performance evaluation shows that
our algorithms achieve over 17% performance gain in
terms of the hit-precision. In addition, our algorithms
are robust against parameter settings, i.e., even without
ground-truth data, by using the empirical parameters,
our proposed algorithms still outperform existing ones.
This results conﬁrm the usefulness of our insights.
Our work is a ﬁrst attempt to bridge the gaps between the
theory bound and the practice attacks for the location trajectory
de-anonymization problem. We show that failing to consider
the practical factors undercuts the performance of the de-
anonymization algorithms. Future work will consider building
2
more accurate privacy metrics to quantify privacy loss given
imperfect data, and develop privacy protection techniques on
top of anonymized trajectory datasets.
In the following, we ﬁrst categorize existing approaches to
evaluating the privacy leakage in anonymized mobility datasets
(§II), followed by our de-anonymization framework (§III). In
§IV, we describe the large ground-truth dataset, using which
we analyze the theoretical privacy bound and the performance
of existing algorithms (§V). After analyzing the main reasons
of the under-performance of existing approaches (§VI), we
build and evaluate our own algorithms to validate our insights
(§VII–VIII).
II. RELATED WORK
De-anonymization Methods: Overview.
In Table I, we
summarize the key de-anonymization algorithms proposed in
recent years. These algorithms seek to re-identify users from
anonymized datasets leveraging external information (not all
the algorithms are applicable to location traces). We classify
them into three main categories based on the utilized user data:
content (user activities such as timestamps, location), proﬁle
(user attributes such as username, gender, age), and network
(relationship and connections between users) [34]. Location
trajectory data belongs to the “content” category.
De-anonymization of Location Trajectories.
Focusing on
the user content, a number of de-anonymization algorithms
have been proposed [8]–[10], [23], [27], [28], [31]–[33], [40].
Most of these algorithms can be directly applied or easily
adapted to trajectory datasets. However, due to the lack of
large scale ground-truth datasets (matched ISP dataset and
external
traces), existing works either focus on theoretical
privacy bound [9], [40] or simulating de-anonymization attacks
on a small dataset [9], [23], [32], [33], [40]. Our work seeks to
use a large scale ground-truth dataset to explore their empirical
performance and identify practical factors (if any) that are
often neglected by algorithm designers.
In Table I, we further categorize these algorithms based on
their design principles. For example, some algorithms are de-
signed to tolerate mistakes in the adversary’s knowledge such
as temporal mismatching [28] and spatial mismatching [23].
Other algorithms [27], [32], [33] implement de-anonymization
attacks based on individual user’s mobility patterns [27], [33].
Finally, researchers also develop de-anonymization algorithms
based on “encountering” events [8], [31]. By considering the
location context (e.g., user population density), it achieves
a better performance [31]. As shown in Table I, none of
these algorithms checks all boxes. In particular, no algorithm
simultaneously tolerates both spatial and temporal mismatches.
De-anonymization of Network/Proﬁle Data.
Since we
focus on the de-anonymization of location trajectory datasets,
we only brieﬂy introduce the algorithms designed for net-
work datasets [19], [20], [29], [35] and proﬁle datasets [15],
[16], [26] for completeness. Mudhakar et al. [35] and Ji et
al. [19], [20] focused on de-anonymization based on users’
graph/network structures. These algorithms can be adapted to
deanonymizing location trajectories by constructing a “contact
graph” to model users encountering with each other. However,
these algorithms require using social network graphs as the
TABLE I.
COMPARISON OF DE-ANONYMIZATION ALGORITHMS,
√
=TRUE, ×=FALSE, −=N/A.
Information Used
Content
Content
Content
Content
Content
Content
Content
Content/Network
Content/Network
Network
Proﬁle
Proﬁle
Proﬁle/Content
POIS [31]
WYCI [32]
HMM [33]
HIST [27]
ME [8]
MSQ [23]
NFLX [28]
CG [35]
ODA [20]
SG [29]
PM [16]
ULink [26]
LRCF [15]
Tolerate Spatial
Mismatching
×
×
√
×
×
√
×
−
−
−
−
−
−
Tolerate Temporal
Mismatching
×
√
×
√
×
×
√
−
−
−
−
−
−
Per-user Mobility Model
×