takes. While recent proposals for augmenting feature spaces
with robust features are promising [e.g., 45, 53], the diversity
of malware makes it extremely difﬁcult
to design such a
feature space. Additionally, some behaviour is only considered
malicious due to its context, for example, requesting access to
the device contacts might be considered suspicious for a torch
app but not for a social messaging app [52].
An orthogonal approach is to identify, track, and mitigate
the drift as it occurs. One promising method is classiﬁcation
with rejection [8], in which low conﬁdence predictions, caused
by drifting examples, are rejected. Drifting apps can then
be quarantined and dealt with separately, either warranting
manual inspection or remediation through other means.
Transcend [20] is a state-of-the-art framework for perform-
ing classiﬁcation with rejection in security tasks. It uses a
conformal evaluator to generate a quality measure to assess
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:16 UTC from IEEE Xplore.  Restrictions apply. 
806
(a) Nearest centroid
(b) Polynomial SVM
(c) RBF SVM
(d) 3-NN
(e) Random forest
(f) QDA
(g) MLP sigmoid
(h) MLP with SVM RBF
Fig. 1: Possible NCMs for different classiﬁcation algorithms: nearest centroid, support-vector machines (SVMs), nearest neighbors (NN), random forest,
dotted lines show SVM margins. The shaded region captures points which are more nonconform (i.e., ‘less similar’) than the new test point, shown by the
quadratic discriminant analysis (QDA), and multilayer perceptron (MLP). The solid line delineates the decision boundary between classes (cid:32) and (cid:35) while the
asterisk, with respect to class (cid:32). As NCMs, (a) uses the distance from the class centroid; (b) and (c) use the negated absolute distance from the hyperplane;
(d) uses the proportion of nearest neighbors belonging to class (cid:35); (e) uses the proportion of decision trees that predict (cid:35); (f) uses the negated probability of
belonging to class (cid:32); (g) uses the negated probability output by the ﬁnal sigmoid activation layer; (h) uses the outputs of the ﬁnal hidden layer to train an
SVM with RBF kernel and uses the negated absolute probabilities output by that SVM—note the decision boundary still depends on the MLP output alone.
whether a new test example is drifting with respect to the
training data. If the prediction of an underlying classiﬁer
appears to be affected by the drift, the prediction is rejected.
The original proposal presented two case studies: Android
malware detection—a binary classiﬁcation task, and Windows
malware family classiﬁcation—a multiclass classiﬁcation task.
The experiments showed that the framework is consistently
able to identify drifting examples, providing a signiﬁcant
improvement over thresholding on the classiﬁers’ output prob-
abilities. However, the lack of a theoretical treatment and
the computational complexity of the framework limited its
understanding and use in real-world deployments.
III. TOWARDS SOUND CONFORMAL EVALUATION
The statistical engine that drives Transcend’s rejection
mechanism is the conformal evaluator, a tool for measuring
the quality of predictions output by an underlying classiﬁer.
Conformal evaluator design is grounded in the theory of con-
formal prediction [47], a method for providing predictions that
are correct with some guaranteed conﬁdence. In this section
we investigate the relationship between the two to provide
novel insights and intuition into why conformal evaluation is
effective in the classiﬁcation with rejection setting.
A. Conformal Evaluation vs. Prediction
Here we give an overview of conformal prediction and
how it motivates the use of conformal evaluation; for a more
formal treatment of conformal prediction we refer to Vovk
et al. [47]. Conformal prediction allows for predictions to
be made with precise levels of conﬁdence by using past
experience to account for uncertainty. Given a classiﬁer g,
a new example z = (x, y), and a signiﬁcance level ε, a
conformal predictor produces a prediction region: a set of
labels in the label space Y that
is guaranteed to contain
the correct label y with probability no more than 1 − ε. To
calculate this label set, the conformal predictor relies on a
nonconformity measure (NCM) derived from g and uses it
to generate scores representing how dissimilar each example
is from previous examples of each class. To quantify this
relative dissimilarity, p-values are calculated by comparing the
nonconformity scores between examples (§III-B). As well as
these p-values, two important metrics are derived from the
prediction region, conﬁdence and credibility (§III-C), which
can be used to judge the effectiveness of the conformal
prediction framework. Conformal predictors are able to make
strong guarantees on the correctness of each prediction so
long as two assumptions about new test examples hold: the
exchangeability assumption, that the sequence of examples is
exchangeable, a generalization of the i.i.d. property; and the
closed-world assumption, that new examples belong to one of
the classes observed during training.
Rather than making predictions, conformal evaluators [20]
borrow the same statistical tools (i.e., nonconformity measures
and p-values) but use them to evaluate the quality of the
prediction made by the underlying classiﬁer g. By detecting
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:16 UTC from IEEE Xplore.  Restrictions apply. 
807
instances which appear to violate the aforementioned assump-
tions they can, with high conﬁdence, reject new drifting exam-
ples which would otherwise be at risk of being misclassiﬁed.
B. Nonconformity Measures and P-values
In order to reject a new example that cannot be reliably
classiﬁed, conformal evaluators rely on a notion of non-
conformity to quantify how dissimilar the new example is
to a history of past examples. In general, a nonconformity
measure (NCM) [38] is a real-valued function that outputs a
score describing how different an example z is from a bag of
previous examples B =(cid:72)z1, z2, ..., zn(cid:73):
αz = A(B, z).
(1)
The greater the value of αz, the less similar z is to the
elements of the bag B. An NCM is typically formed of two
components: a metric d(z, z(cid:48)) to measure the distance between
two points, and a point predictor ˆz(B) to represent B:
A(B, z) := d(ˆz(B), z).
(2)
Illustrating this, Figure 1a shows an NCM for a nearest
centroid classiﬁer in which the Euclidean distance is used for
d(z, z(cid:48)), and the nearest class centroid is used for ˆz(B).
For a new example z∗, the conformal evaluator must decide
whether or not to approve the null hypothesis asserting that z∗
does not belong in the prediction region formed by elements of
B. To perform such a hypothesis test, p-values are calculated
using the NCM values for each point. First the nonconformity
score of z∗ must be computed (Equation 3) along with
nonconformity scores of elements in B (Equation 4), then the
the p-value pz∗ for z∗ is given as the proportion of points with
greater or equal nonconformity scores (Equation 5):
αz∗ = A(B, z∗)
S =(cid:72)A(B \(cid:72)z(cid:73), z) : z ∈ B(cid:73)
|α ∈ S : α >= αz∗|
pz∗ =
|S|
(3)
(4)
(5)
In the classiﬁcation context, we can calculate p-values in a
label conditional manner, such that B contains only previous
examples of class ˆy ∈ Y where ˆy = g(z∗) is the pre-
dicted class of the new example. If pz∗ falls above a given
signiﬁcance level the null hypothesis is disproved and ˆy is
accepted as a valid prediction. Transcend [20] computes per-
class thresholds to use as signiﬁcance levels (§V).
As p-values are calculated by considering nonconformity
scores relative to one another, NCMs can be transformed
monotonically without any impact on the resulting p-values.
Thus, when designing an NCM in the form given by Equa-
tion 2, the distance metric d(z, z(cid:48)) is signiﬁcantly less impor-
tant than the point predictor ˆz(B). It is important to note that
conformal evaluator algorithms are agnostic to the underlying
NCM chosen, but the quality of the NCM—and particularly
of ˆz(B), will impact the ability of conformal evaluators to
discriminate between valid and invalid predictions [38].
An alpha assessment [20] can be used to empirically
evaluate how appropriate an NCM is for a given dataset by
Fig. 2: The nested intervals at which labels (cid:32) and (cid:35) are present in the output
label set for a test example with per-class p-values p(cid:32) = 0.32 and p(cid:35) = 0.08.
Shaded areas outline how credibility and conﬁdence relate to the intersection
of prediction regions for which the label set contains a single element. The
relatively high probability of the empty set containing the correct label (i.e.,
low credibility) indicates that one of conformal prediction’s assumptions may
have been violated. In conformal evaluation, this is used as a signal that the
new example is likely out-of-distribution and is indicative of concept drift.
plotting the distribution of p-values for each class, further
split into whether the prediction was correct or incorrect. As
incorrect predictions should be rejected, they are expected to
fall below the threshold, while correct predictions are expected
to fall above the threshold. Well-separated distributions of
correct and incorrect predictions suggest a viable threshold
exists to separate them at test time. Poorly separated prediction
p-values indicate an inappropriate NCM. An example of an
alpha assessment on a toy dataset is shown in Figure 4d.
Figure 1 illustrates possible NCMs for different algorithms
on a toy binary classiﬁcation task with existing class exam-
ples (cid:32)/(cid:35) and new test example (cid:66). The solid line delineates the
decision boundary between the two classes, the dotted lines
show SVM margins where applicable, and the blue shaded
region captures points that are more nonconform (i.e., less
similar) than (cid:66) with respect to class (cid:32). Note that the shape
of the nonconformal region need not reﬂect the shape of the
regions for the predicted classes (e.g., Figure 1a) and that
there may be multiple viable NCMs for the same underlying
algorithm (e.g., Figures 1g and 1h).
C. Successfully Identifying Drift
Recall
that conformal prediction produces a prediction
region given a signiﬁcance level ε. The possible prediction
regions are nested such that the higher the conﬁdence level, the
more labels will be present. As a trivial example, a prediction
region containing all possible labels may be produced for a
signiﬁcance level of ε = 0 (maximum likelihood) as it will
contain the true label y with certainty. At the other extreme,
an empty set can be produced at a signiﬁcance level of ε = 1
(minimum likelihood), as this is an impossible result under
the closed-world assumption of conformal prediction.
Of particular interest is the prediction region containing a
single element which lies between these extremes. Related to
this prediction region, a conformal predictor also outputs two
metrics: conﬁdence and credibility (Figure 2).
Conﬁdence is the greatest 1 − ε for which the prediction
region contains a single label which can be calculated as the
complement to 1 of the second highest computed p-value.
Conﬁdence quantiﬁes the likelihood that
the new element
belongs to the predicted class.
Credibility is the greatest ε for which the prediction region
is empty and corresponds to the largest computed p-value.
Conformal predictors can be forced to output single predic-
tions (rather than a label set induced by ε), for which they
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:16 UTC from IEEE Xplore.  Restrictions apply. 
808
1 - max(p  , p )1 - p {   }010.680.92{   ,   }Øthat
in conformal evaluation,
there is a high probability that
will output the class with the highest credibility. Credibility
quantiﬁes how relevant the training set is to the prediction. A
low credibility indicates conformal prediction might not be a
suitable framework to use with the given data because a low
credibility means the probability of the correct label being in
the empty set is relatively high, which is an impossible result
under the closed-world assumption of conformal prediction.
We propose that conformal evaluation’s effectiveness stems
from this relationship:
this
probability is being directly interpreted as the probability
that
the i.i.d. assumption has been violated. Thus, a low
credibility means that
the
corresponding example is drifting with respect to the previous
history of training examples. Such an example is at risk of
being misclassiﬁed due to limited knowledge of the classiﬁer.
It should be noted that formally, conformal evaluation de-
ﬁnes credibility and conﬁdence slightly differently. In confor-
mal evaluation, the credibility is the p-value corresponding to
the predicted class and the conﬁdence is the complement to 1
of the maximum p-value excluding the p-value corresponding
to the predicted class (i.e., the credibility p-value). This subtle
difference is important to clarify the operational context of
a conformal evaluator: whereas conformal predictors output
the ﬁnal classiﬁcation decision, conformal evaluators output
a statistical measure separate to the decision of the underly-
ing classiﬁer (hence the nomenclature: one predicts and the
other evaluates). In practice, given reasonable NCMs, these
deﬁnitions can be treated as equivalent.
IV. TOWARDS PRACTICAL CONFORMAL EVALUATION
In assessing the quality of a prediction for a new test point,
there is the question of which previously encountered points
the new point should be compared to—that is, which elements
are included in the bag B of Equation 3, and how. Typically,
new test points are compared against a set of calibration points.
In Jordaney et al. [20], conformal evaluation was realized
using a Transductive Conformal Evaluator (TCE). With a TCE,
every training point is also used as a calibration point. To
generate the p-value of a calibration point, it is ﬁrst removed
from the set of training points and the underlying classiﬁer
trained on the remaining points. Given the newly trained
classiﬁer, a predicted label is generated for the calibration
point. Finally, using a given NCM, its p-value is computed
with respect to the points whose ground truth label matches its
predicted label. This procedure is repeated for every training
point. Following this, Transcend’s thresholding mechanism
operates on the calculated p-values to determine per-class
rejection thresholds (§V). At test time, the underlying classiﬁer
is retrained on the entire training set, and, similarly to the
calibration points, the p-values are computed with respect to
the p-values of the calibration sets.
While the Transductive Conformal Evaluator (TCE) used in
the original proposal [20] appears to perform well, it does not
scale to larger datasets as a newly trained classiﬁer is required
for every training point. Consider the experiments in §VI
where ﬁtting a single instance of the underlying classiﬁer takes
10 CPU minutes. In this case, we estimate a single run using
vanilla TCE to take 1.9 CPU years.
We propose a number of novel conformal evaluators that
overcome this limitation and present
their advantages and
disadvantages. A comparison of their runtime complexities and
operational considerations are presented in Table I and §VII,
respectively. Formal algorithms for their calibration and test
procedures are included in Appendix F while Figure 3 provides
a graphical intuition to their different calibration splits.
Note that while our illustrative examples and evaluation are
given for the binary detection task, TRANSCENDENT and con-
formal evaluation are agnostic to the total number of classes
and this is captured in the formal deﬁnitions. If multiclass
NCMs cannot be derived, per-class conformal evaluators may
be arranged as a one-vs-all ensemble.
A. Approximate TCE (approx-TCE)
Our ﬁrst attempt at reducing the computational overhead
induced by the Transductive Conformal Evaluator is the ap-
proximate Transductive Conformal Evaluator (approx-TCE).
In the original TCE, p-values are generated for each calibration
point by removing them from the training set, retraining the
underlying classiﬁer on the remaining points, and repeating
until a p-value is computed for every training point.
In approx-TCE, calibration points are left out in batches,
rather than individually. The training set is randomly parti-
tioned into k folds of equal size. From the k folds, one is used
as the target of the calibration and the remaining k−1 folds are
used as the bag to which those points are compared to. This
process repeats k times, until each fold has been used as the
calibration set exactly once. Note that all of the k calibration
sets are mutually exclusive; the corresponding batches of p-
values are then concatenated in the same manner as in TCE.
The statistical soundness of the approx-TCE relies on the
assumption that the decision boundary obtained from leaving
out calibration points in batches approximates each of the deci-
sion boundaries that would have been obtained per calibration
point in the batch if the point had been left out individually.
If this assumption holds, the generated p-values will be the
same as, or similar to, the p-values generated with a TCE.
The approximation grows more accurate as k increases until
k equals the cardinality of the training set at which point the
approx-TCE and the TCE are equivalent. In this sense, the
approx-TCE can be viewed as a generalization of the TCE.
This assumption is more likely to hold with algorithms with
lower variance (e.g., linear models), but becomes more tenuous
as the variance increases unless k increases also—sacriﬁcing
the saved computation to mitigate the statistical instability.
B. Inductive Conformal Evaluator (ICE)
The second conformal evaluator we propose is the Inductive
Conformal Evaluator (ICE) which, unlike the approx-TCE, is
based on a corresponding approach from conformal prediction