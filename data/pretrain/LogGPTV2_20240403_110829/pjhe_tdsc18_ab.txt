4: repeat
5: 
6: 
7: 
8: 
9: Set n  jcurGroupj5: 
6: 
7: 
8: 
9: Set n  jcurGroupj 
if ISCOMPLETEðcurGroup; GSÞ ¼ true then
Add curGroup to completeL
Remove curGroup from incompleteL
else
| as an incomplete token position. | as an incomplete token position. | as an incomplete token position. | 10: | Find the split token position s |
|---|---|---|---|---||---|---|---|---|---|
| Our heuristic rule is to recursively partition each group |Our heuristic rule is to recursively partition each group |Our heuristic rule is to recursively partition each group |11: |Compute AT  jTSsj  Compute RT  jTSsj=n  if AT > splitAbs and RT > splitRel then || Our heuristic rule is to recursively partition each group |Our heuristic rule is to recursively partition each group |Our heuristic rule is to recursively partition each group |12: |Compute AT  jTSsj  Compute RT  jTSsj=n  if AT > splitAbs and RT > splitRel then |
| until all the resulting groups have enough complete token |until all the resulting groups have enough complete token |until all the resulting groups have enough complete token |12: |Compute AT  jTSsj  Compute RT  jTSsj=n  if AT > splitAbs and RT > splitRel then || positions. To evaluate whether complete token positions are |positions. To evaluate whether complete token positions are |positions. To evaluate whether complete token positions are |13: |Compute AT  jTSsj  Compute RT  jTSsj=n  if AT > splitAbs and RT > splitRel then |
| enough, we define Group Goodness (GG) as following. |enough, we define Group Goodness (GG) as following. |enough, we define Group Goodness (GG) as following. |14: |Add curGroup to completeL || enough, we define Group Goodness (GG) as following. |enough, we define Group Goodness (GG) as following. |enough, we define Group Goodness (GG) as following. |15: |Remove curGroup from incompleteL |
| GG ¼#CompleteTokenPositions |: |(1) |16: |else |
| GG ¼#CompleteTokenPositions |: |(1) |17: |Partition curGroup to several resultGroup based on |
| GG ¼#CompleteTokenPositions |: |(1) |17: |the token value in split token position |A group is a complete group if GG > GS, where GS stands for Group Support, a threshold assigned by develop-ers. Otherwise, the group is an incomplete group. In this step, POP recursively partitions the groups if the current group is not a complete group.Algorithm 1 provides the pseudo code of step 3. POP regards all groups from step 2 as incomplete groups (line 1). Incomplete groups are recursively partitioned by POP to generate a list of complete groups (lines 424). For each incomplete group, if it already contains enough complete token positions, it is moved to the complete group list (lines 68). Otherwise, POP finds the split token position, which is the token position with the lowest cardinality among all incomplete token positions. Because of its lowest cardinal-18: 
19: 
20: 
21: 
22: 
23: 	for all resultGroup do 
	if ISCOMPLETEðresultGroup; GSÞ¼ true then 	Add resultGroup to completeL 	else 
	Add resultGroup to incompleteL curGroupnext group in incompleteL
24: until incompleteL is empty
| 25: function ISCOMPLETEðgroup; gsÞ 
26: 	Compute token sets for token positions in group | 25: function ISCOMPLETEðgroup; gsÞ26: 	Compute token sets for token positions in group | 25: function ISCOMPLETEðgroup; gsÞ 
26: 	Compute token sets for token positions in group |
|---|---|---|
| 27: |Compute GG by |" Equation (1) |
| 28: |if GG > gs then |if GG > gs then |
| 29: |return true |return true |
| 30: |else |else |
| 31: |return false |return false |
| ity, tokens in the split token position are most likely to be | 3.4 | Step 4: Generate Log Events ||---|---|---|
| constants. Then POP calculates Absolute Threshold (AT) and |3.4 |Step 4: Generate Log Events |
Relative Threshold (RT) (lines 1112). A token position with smaller AT and RT is more likely to contain constants. For example, in Fig. 4, column (i.e., token position) 1 and 2 have smaller AT (2) and RT (0.5), so they are more likely to con-tain constants compared with column 3, whose AT is 4 and RT is 1. Note that we only need to calculate AT and RT for the split token position. We demonstrate AT and RT for all the columns in Fig. 4 for better explanation. Thus, POPAt this point, the logs have been partitioned into nonoverlap-ping groups by two heuristic rules. In this step, POP scans all the logs in each group and generates the corresponding log event, which is a line of text containing constant parts and var-iable parts. The constant parts are represented by tokens and the variable parts are represented by wildcards. To decide whether a token is a constant or a variable, POP counts the number of distinct tokens (i.e., jTSj) in the correspondingHE ET AL.: TOWARDS AUTOMATED LOG PARSING FOR LARGE-SCALE LOG DATA ANALYSIS 	935
token position. If the number of distinct tokens in a token 
position is one, the token is constant and will be outputted to 
the corresponding token position in a log event. Otherwise, a 
wildcard is outputted.
3.5 	Step 5: Merge Groups by Log Event
To this end, logs have been partitioned into nonoverlappingcomplete groups, and each log message is matched with a 
log event. Most of the groups contain logs that share the 
same log event. However, some groups may be over-parsed 
because of suboptimal parameter setting, which causes false
negatives. Besides, it is possible that some variable parts in a log event have variable length, which invalidates the assumption in step 2. This also brings false negatives.To address over-parsing and further improve parsing accuracy, in this step, POP employs hierarchical clustering [27] to cluster similar groups based on their log events. The groups in the same cluster will be merged, and a new log event will be generated by calculating the Longest Common Subsequence (LCS) [28] of the original log events. This step is based on the assumption that if logs from different groups have the same log event type, the generated log event texts from these groups should be similar. POP calculates Man-hattan distance [29] between two log event text to evaluate their similarity. Specifically,dða; bÞ ¼ X jai  bij; (2)where a and b are two log events, N is the number of all con-stant token values in a and b, and ai means the occurrence number of the ith constant token in a. We use Manhattan distance because it assigns equal weight to each dimension (i.e., constant). This aligns with our observation that all con-stants are of equal importance in log parsing. Besides, Man-hattan distance is intuitive, which makes parameter tuning easier. POP employs complete linkage [30] to evaluate the distance between two clusters, because the resulted clusters will be compact, which avoids clustering dissimilar groups together. The only parameter in this step is maxDistance, which is the maximum distance allowed when the cluster-ing algorithm attempts to combine two clusters. The algo-rithm stops when the minimum distance among all cluster pairs is larger than maxDistance.Fig. 5. Overview of POP implementation.
evaluated. We build POP on top of Spark because it is good at parallelizing identical computation logic on each element of a dataset, and it directly uses the output of one step in memory as the input to another. In our case, an RDD can represent a log dataset, where each element is a log mes-sage. POP can be parallelized by transformations and actions, because each POP step requires computation-inten-sive tasks that cast identical computation logic to every log message. To parallelize these tasks, we invoke Spark opera-tions with specially designed functions describing the com-putation logic. In the following, we will introduce the Spark operations we applied for the five POP steps.The implementation of POP on Spark is illustrated in Fig. 5. The five rounded rectangles at the bottom represent the five steps of POP, where the numbered arrows represent the interactions between the main program and the Spark cluster. The main program is running in Spark driver, which is responsible for allocating Spark tasks to workers in the Spark cluster. For a POP Spark application, in step 1, we use textFile to load the log dataset from a distributed file system (e.g., HDFS) to Spark cluster as an RDD (arrow 1). Then, we use map to preprocess all log messages with a function as input describing the preprocessing logic on single log mes-sage (arrow 2). After preprocessing, we cache the prepro-cessed log messages in memory and return an RDD as the reference (arrow 3). In step 2, we use aggregate to calculate all possible log message length values (arrow 4) and return them as a list (arrow 5). Then for each value in the list, we use filter to extract log messages with the same log message length (arrow 6), which is returned as an RDD (arrow 7).| 3.6 | Implementation | Now we have a list of RDDs. In step 3, for each RDD, we |
|---|---|---|
| 3.6 |Implementation |employ aggregate to form the token sets for all token posi- |To make POP efficient in large-scale log analysis, we build it on top of Spark [25], [26], a large-scale data processing plat-form [31]. Specifically, Spark runs iterative analysis pro-grams with orders of magnitude faster than Hadoop MapReduce [32]. The core abstraction in Spark is Resilient Distributed Datasets (RDDs), which are fault-tolerant and parallel data structures representing datasets. Users can manipulate RDDs with a rich set of Spark operations called transformations (e.g., map, filter) and actions (e.g, reduce, aggregate). Calling transformations on an RDD generates a new RDD, while calling actions on an RDD reports calcula-tion result to users. Spark employs lazy evaluation, so that transformations on RDDs will not be executed until an action is called. At that time, all preceding transformationstions (arrow 89) as described in Section 3.3. Based on the token sets and pre-defined thresholds, the driver program decides whether current RDD could be further partitioned or not. If yes, we use filter to generate new RDDs and add them into the RDD list (arrow 1011). Otherwise, we remove it from the list and pass the RDD to step 4. In step 4, we use reduce to generate log events for all RDDs (arrow 1213). When all log events have been extracted, POP runs hierarchical clustering on them in main program. We use union to merge RDDs based on the clustering result (arrow 14). Finally, merged RDDs are outputted to the distributed file system by saveAsTextFile (arrow 15).The implementation of this specialized POP is non-trivial. First, Spark provides more than 80 operations and