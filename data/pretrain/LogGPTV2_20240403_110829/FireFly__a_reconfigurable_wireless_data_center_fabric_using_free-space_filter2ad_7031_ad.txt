b∈FSOs(j )
a
a,b ≥ 0
(cid:88)
b s.t. (a,b)∈κ
la,b ≤ 1
(cid:88)
(cid:88)
b∈FSOs(k )
(cid:88)
d
f i,j
b,d
f i,j
a,b = Ti,j
(1)
(2)
(3)
(4)
(5)
(6)
(7)
The high-level idea behind our heuristic is to extend the tradi-
tional Blossom algorithm for computing the maximum matching
to incorporate multi-hop trafﬁc. Recall that the Blossom algorithm
improves the matching by computing an alternating path at each
stage. We deﬁne a new utility function that captures multi-hop traf-
ﬁc and then pick the path with the highest beneﬁt. Speciﬁcally, we
use the intuition that shorter inter-rack paths imply lower resource
usage and higher network throughput [45]. Thus, we deﬁne the
beneﬁt of an alternating path L as the decrease in the weighted-
sum of inter-rack distances if L were used to modify the current
matching. Formally, given a current matching τ, the beneﬁt of an
alternating path L that would modify the matching from τ to τ(cid:48), is
i,j ),
where hi,j and h(cid:48)
i,j are the inter-rack distances between racks (i, j )
in τ and τ(cid:48) respectively (when seen as graphs over racks).
the total reduction in the network footprint:(cid:80)
ij Di,j (hi,j − h(cid:48)
Figure 6: ILP formulation for periodic reconﬁguration.
5.1 Periodic Reconﬁguration
Given a PCFT and the prevailing trafﬁc load, the periodic reconﬁg-
uration problem is to optimally select a runtime (i.e., some realiz-
able) topology and set up routes for the current trafﬁc ﬂows.
This constrained optimization is captured by the integer linear
program (ILP) shown in Figure 6.9 Let κ be the set of candidate
links in the PCFT, C be the (uniform) link capacity, E be the given
epoch size (say a few seconds), and Di,j be the estimated traf-
ﬁc demand (volume) between a pair of racks (i, j ). This demand
can be obtained by using the measurement counters from the SDN
switches from previous epoch(s). We use the subscripts a, b, c, d to
refer to FSOs, and i, j , k to refer to racks, and FSOs(k ) to denote
the set of FSOs on the top of rack k.
There are two key sets of control variables: (i) The binary vari-
able la,b models topology selection and is 1 iff a candidate link
(a, b) is chosen to be active; and (ii) f i,j
a,b captures the trafﬁc en-
gineering (TE) strategy in terms of the volume of inter-rack trafﬁc
between i and j routed over the link (a, b). Let Ti,j be the total
trafﬁc volume satisﬁed for the ﬂow (i, j ).
For clarity, we consider a simple objective function that maxi-
mizes the total demand satisﬁed across all rack pairs as shown in
Eq (1). Eq (2) ensures that each FSO can have at most 1 active link.
Eq (3) ensures that the total ﬂow on each link (on average) does not
exceed the capacity. Eq (4) are ﬂow conservation constraints for
each ﬂow (i, j ) and a rack k. Eq (5) captures the volume of the de-
mand satisﬁed using a constraint over the ingress and egress racks.
Eq (6) ensures that the volume satisﬁed for each rack pair is at most
the demand. Finally, we have bounds on the control variables.
Unfortunately, even state-of-art solvers like Gurobi or CPLEX
take several hours to solve this ILP (§8.4). Hence, we follow a
heuristic strategy and decouple the optimization into two stages.
First, we solve the “integer” problem to select the active links.
Then, given this runtime topology, we compute the ﬂow routes.
Greedy Matching for Topology Selection. Recall from §4 that
a realizable topology is essentially a matching over FSOs in the
PCFT graph. Thus, a simple starting point is to select the maximum-
weighted matching, where each candidate link (a, b) is weighted
by the inter-rack trafﬁc demand Di,j between the racks i and j . In
effect, this maximizes the total demand that can be served using di-
rect links. However, this can be very inefﬁcient if the given PCFT
does not have direct links between racks with high trafﬁc demands.
9This problem is harder than the optimization problem considered
in §2.1, since we were assuming arbitrary ﬂexibility.
a,b variables into end-to-end paths.
We run this extended Blossom algorithm until there is no alter-
nating path that can improve the network footprint and then output
the ﬁnal topology at this stage.
Flow Routing. Given a speciﬁc runtime topology (i.e., values of
la,b), the residual TE problem is theoretically solvable in polyno-
mial time as a multi-commodity ﬂow (MCF) problem. However,
even this takes hundreds of seconds for 256/512 racks §8.4), which
is not acceptable as a near real-time solution. To address this, we
use a greedy algorithm to compute the values of these ﬂow vari-
ables. Essentially, we extend the traditional augmenting-path ap-
proach for max-ﬂow algorithms and greedily pick an augmenting
path for a currently unsatisﬁed commodity. We run the algorithm
until no more augmenting paths can be picked; i.e., the network is
saturated. From this solution, we use the “path stripping” idea [42]
to convert the values of the f i,j
5.2 Triggered Reconﬁgurations
In addition to periodically reconﬁguring the network, FireFly can
also run localized reconﬁgurations triggered by trafﬁc events. Such
reconﬁgurations may be frequent but likely require minimal topol-
ogy and ﬂow-route changes. We currently support two types of
triggers. First, if we detect elephant ﬂows that have sent more than
10 MB of aggregate data [17], then we activate links to create a
shorter or less-congested path for this ﬂow. Second, if trafﬁc be-
tween a particular pair of racks exceeds some conﬁgurable thresh-
old, then we create a direct link between them, if this does not
require deactivating recently activated or high utilization links.
6 Correctness During Reconﬁgurations
A reconﬁguration in FireFly entails: (i) addition and/or deletion of
(candidate) links from the given runtime topology, and (ii) corre-
sponding changes to the network forwarding tables (NFTs). This
ﬂux raises natural concerns about correctness and performance dur-
ing reconﬁgurations. Our goal is to ensure that: (i) network re-
mains connected at all times, (ii) there are no black holes (e.g., all
forwarding table entries refer to available/usable links), and (iii)
packet latency remains bounded (and thus, delivery is guaranteed).
The main challenges in designing a correct data plane strategy
stem from two factors: (i) Activation or deactivation of candidate
links incur a non-zero latency (few msecs); and (ii) We may need
to execute reconﬁgurations concurrently if the triggers occur fre-
quently (e.g., for every elephant ﬂow arrival). At a high level, these
are related to the problem of consistent updates [34, 43]. The key
difference is that we can engineer simpler requirement-speciﬁc so-
lutions rather than use more general-purpose solutions proposed in
prior work.
325Figure 7: Packet (in-ﬂight location shown by a square) contin-
ues to “swing” from B to A and back, due to a rapid sequence
of reconﬁgurations.
6.1 Handling sequential reconﬁgurations
We begin by focusing on correctness when we execute reconﬁgura-
tions serially and defer concurrent execution to the next subsection.
6.1.1 Avoiding Black Holes
To see why “black holes” may arise, consider a reconﬁguration that
changes the network’s runtime topology by steering FSOs a and b
towards each other, and in the process activating the link (a, b) and
deactivating some link (a, c). Suppose the NFTs change from F to
F(cid:48). Now, there is a period of time (when GM/SMs at a is changing
state) during which neither (a, b) nor (a, c) is available. During
this period, irrespective of when the NFTs get updated (say, even
atomically) from F to F(cid:48), some entries in the NFTs may refer to
either (a, b) or (a, c), inducing black holes in the network.
Our Solution. To avoid black holes, we split a reconﬁguration
into multiple steps such that: (i) link deletion is reﬂected in the
NFTs before their deactivation is initiated, and (ii) link addition is
reﬂected only after the activation is complete. Thus, a reconﬁgura-
tion that involves deactivation (activation) of a set of links (cid:53) (∆)
is translated to the following sequence of steps:
S1: Update the NFTs to reﬂect deletion of (cid:53).
S2: Deactivate (cid:53) and activate ∆.
S3: Update the NFTs to reﬂect addition of links ∆.
One additional invariant we maintain is that every switch has a
default low priority rule at all times to reach every destination rack
via some active outgoing link. We do so to explicitly ensure that
packets can reach their destination, possibly on sub-optimal paths,
as long as the network is connected (see below).
6.1.2 Maintaining Connectivity
To ensure network connectivity at all times, we simply reject re-
conﬁgurations that might result in a disconnected network in step
S1 above. That is, we add a step S0 before the three steps above.
S0: Reject the reconﬁguration, if deletion of links (cid:53) disconnects
the network.
To reduce the chance of such rejections, we also extend our re-
conﬁguration algorithms to retain a connected subnetwork from the
prior topology. The high-level idea here is to construct a rack-level
spanning tree using the current graph, and explicitly remove these
links/FSOs from consideration during the greedy matching step.
6.1.3 Bounded Packet Latency
If reconﬁgurations occur at a very high frequency, then we may see
unbounded packet latency. Figure 7 shows a simple example where
a packet can never reach its destination because the links/routes are
being reconﬁgured quite rapidly.
Our Solution. The example also suggests a natural strategy to
avoid such cases—we can delay or reject reconﬁgurations to al-
low the in-ﬂight packets to use one of the intermediate topologies
to reach its destination. We introduce a small delay of x units be-
tween two consecutive NFTs-updates, where x is the maximum
packet latency in a ﬁxed realizable topology. This ensures that each
packet “sees” at most two conﬁgurations during its entire ﬂight.
This, bounds the packet latency by (2x + z) where z is the total
NFTs-update time.
6.2 Handling Concurrent Reconﬁgurations
Computing and executing a reconﬁguration can take a few tens of
msecs in a large DC. To achieve good performance, we may need
to reconﬁgure the network frequently; e.g. for every elephant ﬂow
arrival in the network, which may happen every msec or less. Thus,
we need mechanisms that allow reconﬁgurations to be executed
concurrently. We could batch reconﬁgurations, but that merely de-
lays the problem rather than fundamentally solving it because a
batch may not ﬁnish before the next set of reconﬁgurations arrive.
We observe that to handle concurrent reconﬁgurations, we need
to extend the approach from §6.1 to handle two concerns.
• Connectivity: One concern is that each reconﬁguration in iso-
lation may not disconnect the network but combining them
might. Thus, to ensure network connectivity, the controller
maintains an atomic global topology variable G, and uses this
variable to accept/reject in step S0. (G is also updated by ac-
cepted reconﬁgurations in S1 and S3.)
• Conﬂicting reconﬁgurations: In step S0, we also reject any re-
conﬁguration that “conﬂicts” (in terms of link activations or
deactivations) with already-accepted but yet-unﬁnished recon-
ﬁgurations. That is, we follow a non pre-emptive strategy of
allowing outstanding reconﬁgurations to complete.
We note that no other changes are required to §6.1 to handle con-
currency. Black holes are still avoided since only non-conﬂicting
reconﬁgurations are executed concurrently and packet latency is
bounded since a minimum time-interval already precludes concur-
rent processing of different NFTs-updates.
6.3 Overall Scheme
Based on the previous building blocks, our overall scheme is as
follows. Each reconﬁguration ρ that deletes and adds a set of links
(cid:53) and ∆, is translated into the following four steps. Here, G is as
described in §6.2.
C0: Accept ρ if (i) deletion of links (cid:53) does not disconnect G, and
C1: Update G and NFTs to reﬂect deletion of (cid:53).
C2: Deactivate (cid:53) and activate ∆.
C3: Update G and NFTs to reﬂect addition of links ∆.
(ii) ρ doesn’t conﬂict with any unﬁnished reconﬁgurations.
In addition, as discussed in §6.1, we ensure (a) default path rules,
and (b) a minimum time-interval (= maximum packet latency) units
between consecutive NFTs-updates.
We can analytically prove that the above overall scheme ensures
that (i) there are no black holes, (ii) network remains connected,
and (iii) packet latency is bounded by (2x + z), where z is the
NFTs-update time. This claim holds irrespective of how the NFTs
are updated across the network; i.e., we do not require atomic up-
dates. We refer readers to our technical report for the proof [27].
7 FireFly Controller Implementation
We implement the FireFly controller as modules atop the POX con-
troller. We chose POX/OpenFlow primarily for ease of prototyping.
We use custom C++ modules for the PCFT generation and recon-
ﬁguration optimization algorithms. For reconﬁguration, we imple-
ment heuristics to “dampen” reconﬁgurations by checking if there
is a signiﬁcant (e.g., more than 10%) improvement in the objective
function from Figure 6. We use a simple translation logic to convert
the output of the optimization solver to OpenFlow rules where the
Destination A B Destination Destination A B A B 326“ﬂow” variables are mapped to preﬁx-range based forwarding en-
tries [49]. For elephant ﬂows, we set up exact match ﬂow rules [12].
We use existing OpenFlow measurement capabilities to estimate
the inter-rack demands and use the observed trafﬁc from the pre-
vious epoch as the input to the controller. Our prototype does not
implement elephant ﬂow detection [12, 17]; we currently assume
this information is available out of band.
8 Evaluation
We established the performance of individual steerable FSO links
in §3. In this section, we focus on:
1. Performance w.r.t. other DC architectures (§8.1);
2. Impact on performance during reconﬁgurations (§8.2);
3. Optimality of the preconﬁguration algorithms (§8.3);
4. Optimality and scalability of reconﬁguration (§8.4);
5. Sensitivity analysis w.r.t. degree of ﬂexibility and reconﬁgura-
tion latency (§8.5); and
6. Cost comparison w.r.t. prior DC architectures (§8.6).
For (1), we use a combination of detailed packet-level simu-
lations using htsim and augment it with larger-scale ﬂow-level
simulations using a custom ﬂow-level simulator. For (2), we use