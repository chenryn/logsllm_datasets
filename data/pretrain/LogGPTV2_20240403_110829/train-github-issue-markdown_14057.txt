## üêõ Bug
I'm using the nightly build: 1.0.0.dev20181123. This issue is very similar to
#13569 . When I instantiate two nn.Embedding, with DataParallel and with
max_norm=1.0, I get the following assert
    RuntimeError: output_nr_ == 0 ASSERT FAILED at /pytorch/torch/csrc/autograd/variable.cpp:196, please report a bug to PyTorch.
If I remove the self.lut_dummy, the issue disappears.
## To Reproduce
    import torch
    import os
    print torch.__version__
    class Moda(torch.nn.Module):
        def __init__(self):
            super(Moda, self).__init__()
            self.lut_dummy = torch.nn.Embedding(1, 1, max_norm=1.0).to("cuda")
            self.lut_a = torch.nn.Embedding(22, 256, max_norm=1.0).to("cuda")
        def forward(self, src):
            print "ok1"
            ebd = self.lut_a(src)
            print "ok2"
            return ebd
    def main():
        os.environ['CUDA_VISIBLE_DEVICES'] = "0,1"
        model = Moda()
        model = torch.nn.DataParallel(model)
        src = torch.randint(4,(2,)).to("cuda")
        output = model(src)
    if __name__ == '__main__':
        main()
## Output
1.0.0.dev20181123  
/home/software/LM_stash/amitoj/pytorch1.0/local/lib/python2.7/site-
packages/torch/nn/parallel/data_parallel.py:25: UserWarning:  
There is an imbalance between your GPUs. You may want to exclude GPU 0 which  
has less than 75% of the memory or cores of GPU 1. You can do so by setting  
the device_ids argument to DataParallel, or by setting the
CUDA_VISIBLE_DEVICES  
environment variable.  
warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))  
ok1  
ok1  
ok2  
Traceback (most recent call last):  
File "error_train.py", line 25, in  
main()  
File "error_train.py", line 22, in main  
output = model(src)  
File "/home/software/LM_stash/amitoj/pytorch1.0/local/lib/python2.7/site-
packages/torch/nn/modules/module.py", line 479, in **call**  
result = self.forward(*input, **kwargs)  
File "/home/software/LM_stash/amitoj/pytorch1.0/local/lib/python2.7/site-
packages/torch/nn/parallel/data_parallel.py", line 143, in forward  
outputs = self.parallel_apply(replicas, inputs, kwargs)  
File "/home/software/LM_stash/amitoj/pytorch1.0/local/lib/python2.7/site-
packages/torch/nn/parallel/data_parallel.py", line 153, in parallel_apply  
return parallel_apply(replicas, inputs, kwargs,
self.device_ids[:len(replicas)])  
File "/home/software/LM_stash/amitoj/pytorch1.0/local/lib/python2.7/site-
packages/torch/nn/parallel/parallel_apply.py", line 83, in parallel_apply  
raise output  
RuntimeError: output_nr_ == 0 ASSERT FAILED at
/pytorch/torch/csrc/autograd/variable.cpp:196, please report a bug to PyTorch.
## Additional context
Observations
  1. If I comment out the self.lut_dummy, the issue disappears. Though in my real model, I am using the `lut_dummy`.
  2. If max_norm is removed, the issue disappears again.
  3. If I set os.environ["CUDA_VISIBLE_DEVICES"] to "0" or "1", again, the code works just fine.
cc @ezyang @gchanan @ssnl @albanD