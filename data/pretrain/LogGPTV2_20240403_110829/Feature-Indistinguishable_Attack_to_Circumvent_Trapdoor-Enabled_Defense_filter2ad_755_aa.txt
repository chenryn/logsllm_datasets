title:Feature-Indistinguishable Attack to Circumvent Trapdoor-Enabled Defense
author:Chaoxiang He and
Bin Benjamin Zhu and
Xiaojing Ma and
Hai Jin and
Shengshan Hu
Feature-Indistinguishable Aack to Circumvent
Trapdoor-Enabled Defense
Chaoxiang Heâˆ—
Huazhong Univ. of Science and Tech.
PI:EMAIL
Bin Benjamin Zhu
Microsoft Research Asia
PI:EMAIL
Xiaojing Maâ€ âˆ—
Huazhong Univ. of Science and Tech.
PI:EMAIL
Hai Jinâˆ—
Shengshan Huâˆ—
Huazhong Univ. of Science and Tech.
Huazhong Univ. of Science and Tech.
PI:EMAIL
PI:EMAIL
ABSTRACT
Deep neural networks (DNNs) are vulnerable to adversarial attacks.
A great eort has been directed to developing eective defenses
against adversarial attacks and nding vulnerabilities of proposed
defenses. A recently proposed defense called Trapdoor-enabled De-
tection (TeD) [51] deliberately injects trapdoors into DNN models
to trap and detect adversarial examples targeting categories pro-
tected by TeD. TeD can eectively detect existing state-of-the-art
adversarial attacks. In this paper, we propose a novel black-box
adversarial attack on TeD, called Feature-Indistinguishable Attack
(FIA). It circumvents TeD by crafting adversarial examples indistin-
guishable in the feature (i.e., neuron-activation) space from benign
examples in the target category. To achieve this goal, FIA jointly
minimizes the distance to the expectation of feature representa-
tions of benign samples in the target category and maximizes the
distances to positive adversarial examples generated to query TeD
in the preparation phase. A constraint is used to ensure that the
feature vector of a generated adversarial example is within the
distribution of feature vectors of benign examples in the target
category. Our extensive empirical evaluation with dierent cong-
urations and variants of TeD indicates that our proposed FIA can
eectively circumvent TeD. FIA opens a door for developing much
more powerful adversarial attacks. The FIA code is available at:
https://github.com/CGCL-codes/FeatureIndistinguishableAttack.
CCS CONCEPTS
â€¢ Security and privacy; â€¢ Computing methodologies â†’ Arti-
cial intelligence; Machine learning;
âˆ—Chaoxiang He, Xiaojing Ma, Hai Jin, Shengshan Hu are with the National Engineering
Research Center for Big Data Technology and System, Services Computing Technology
and System Lab, Hubei Engineering Research Center on Big Data Security, Hubei key
Laboratory of Distributed System Security, School of Cyber Science and Engineering,
Huazhong Univeristy of Science and Technology.
â€ Xiaojing Ma is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for prot or commercial advantage and that copies bear this notice and the full citation
on the rst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specic permission and/or a
fee. Request permissions from permissions@acm.org.
CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea
Â© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-8454-4/21/11...$15.00
https://doi.org/10.1145/3460120.3485378
KEYWORDS
Neural Networks; Adversarial examples; Adversarial attacks; Trap-
door enabled defense; Feature-indistinguishable attack.
ACM Reference Format:
Chaoxiang He, Bin Benjamin Zhu, Xiaojing Ma, Hai Jin, and Shengshan Hu.
2021. Feature-Indistinguishable Attack to Circumvent Trapdoor-Enabled
Defense. In Proceedings of the 2021 ACM SIGSAC Conference on Computer
and Communications Security (CCS â€™21), November 15â€“19, 2021, Virtual Event,
Republic of Korea. ACM, New York, NY, USA, 18 pages. https://doi.org/10.1
145/3460120.3485378
1 INTRODUCTION
Deep neural networks (DNNs) have been proved to be eective at
many dicult machine-learning tasks. However, they are found
to be vulnerable to adversarial attacks [56], wherein an example
correctly predicted by a deep learning model is intentionally modi-
ed slightly, usually undetectable by humans, to cause the model
to make an incorrect prediction. These slightly modied examples
are called adversarial examples. They are carefully crafted counter-
factual examples with the aim to deceive the model. Adversarial
examples can be classied into targeted or untargeted adversarial
examples. A targeted adversarial example causes the model to mis-
classify it into a specic (i.e., target) category dierent from the
original one, while an untargeted adversarial example causes the
model to misclassify it into any category dierent from the original
one. Targeted adversarial examples are generally harder to craft
than untargeted adversarial examples.
Adversarial attacks are proved eective in deceiving deep learn-
ing models of dierent deep neural networks for dierent tasks [3,
10, 12, 14, 16, 17, 62] including real-world application scenarios
[33, 34, 52]. Adversarial attacks have raised a serious concern on
the security and reliability of deploying a deep learning model in
real-world applications, esp. security-critical applications such as
trac-sign identication, face recognition, malware detection, etc.
Existence of adversarial examples has inspired signicant re-
search activities on both defenses against increasingly more power-
ful adversarial attacks and adversarial attacks to circumvent more
and more sophisticated defenses. Many adversarial attacks have
been developed since the rst adversarial attack FGSM [20] was
introduced, such as state-of-the-art attacks PGD [33, 34], C&W [10],
and Elastic Net [11] that rely on computing gradients of the model
in crafting adversarial examples, and BPDA [1] and SPSA [58] that
bypass computing gradients of the model in crafting adversarial
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea3159examples. These adversarial attacks can be used to generate either
targeted or untargeted adversarial examples. At the same time, var-
ious defense methods have also been proposed. They aim to disrupt
computation of gradients, improve modelâ€™s robustness against ad-
versarial examples, or detect adversarial examples at inference time.
Most of them are proved later to be vulnerable to more powerful
adversarial attacks.
A new defense, called Trapdoor-enabled Detection (TeD) [51], was
proposed recently to detect targeted adversarial examples at infer-
ence time. TeD deliberately injects one or more defensive trapdoors
into a DNN model to protect one or more categories through back-
door attack techniques [23]. When crafting adversarial examples
targeting a protected category of a trapdoored model, the optimiza-
tion process of an adversarial attack gravitates towards trapdoors,
leading to generated adversarial examples similar to trapdoors in
the feature (i.e., neuron-activation) space. By comparing neuron
activation signatures of inputs with those of trapdoors at a latent
layer, referred to as the detection layer, adversarial examples can
be detected. TeD can eectively detect state-of-the-art adversarial
attacks such as PGD, C&W, Elastic Net, BPDA, and SPSA, with
negligible impact on the normal classication accuracy [51].
Two attacks [6, 51] have been proposed to evade TeD, one is a
white-box attack on TeD by assuming that adversaries know the
trapdoor signatures used in detection, while the other is a grey-box
attack by assuming that adversaries know some characteristics of
the trapdoored defense, such as the number of trapdoors and the
layer to detect. These methods craft adversarial examples by maxi-
mizing the distance to the known or estimated trapdoor signatures
while minimizing the cross-entropy to the target category. They
can evade TeDâ€™s baseline detection but their success rates are signif-
icantly reduced when TeD reinforces its detection with randomly
sampled neurons and multiple trapdoors [51].
In this paper, we propose a novel targeted adversarial attack,
called Feature-Indistinguishable Attack (FIA), to circumvent TeD.
FIA is a black-box attack on TeD (and white-box on the model):
adversaries have no knowledge of TeD or its characteristics. It relies
only on the distribution of benign samples in the target category,
referred to as benign target samples. FIA exploits the facts that most
benign target samples should be undetected (i.e., negative) in a
practical deployment of TeD and that a useful model is desirably
well behaved (i.e., non-overtting). Inspired by recently proposed
adversarial attacks [29â€“31] that aim to strengthen adversarial trans-
ferability by optimizing at a latent layer in the feature space instead
of the commonly used cross entropy, we minimize the distance
to the expectation of feature representations of benign target ex-
amples at a latent layer, referred to as the generation layer, and
ensure that generated adversarial examples are within a specic
thresholding distance to the expectation in the feature space. We
call this scheme the basic FIA scheme.
Adversarial examples generated with the basic scheme may still
be distinguishable from benign target samples at the detection
layer in the feature space due to mismatch between the generation
layer and the detection layer, irregular undetectable boundaries
of the trapdoored defense, etc. To improve indistinguishability of
generated adversarial examples, FIA contains a preparation phase
in which the basic scheme is used to generate a small number of
adversarial examples to query the trapdoored defense to determine
an appropriate generation layer and other generation parameters.
FIA also uses the detected (i.e. positive) adversarial examples in
this phase to generate adversarial examples by maximizing the
distances to these positive adversarial examples while minimizing
the distance to the expectation of benign target samples, both in
the feature space.
We conduct an extensive empirical evaluation of our proposed
attack on dierent congurations and variants of the trapdoored
defense, including an improved variant called Projection-based TeD
(P-TeD) that we propose in this paper, and compare it with exist-
ing state-of-the-art adversarial attacks. Our experimental study
indicates that, while TeD and P-TeD can eectively detect existing
state-of-the-art adversarial attacks, our proposed FIA can eectively
circumvent the trapdoored defense of both TeD and P-TeD.
This paper includes the following major contributions:
â€¢ We propose a novel adversarial attack on DNN models that
aims to generate adversarial examples indistinguishable in
the feature space from benign examples of the target cate-
gory. To the best of our knowledge, this is the rst adversarial
attack to evade detection by pursuing indistinguishability
from benign samples.
â€¢ We propose a variant of TeD with an improved detection
â€¢ We present an extensive empirical evaluation of our pro-
posed adversarial attack on dierent congurations and vari-
ants of the trapdoored defense.
â€¢ Our proposed adversarial attack can eectively circumvent
TeD and its variants that existing stat-of-the-art adversarial
attacks cannot evade.
performance.
The remaining paper is organized as follows. We present the
background and related work in Section 2, describe briey TeD
and its improved variant in Section 3, provide an overview of our
proposed FIA in Section 4, and describe the detail of FIA in Section 5.
Our empirical evaluation is presented in Section 6, and discussion
is provided in Section 7. We conclude this paper with Section 8.
The FIA code is available at: https://github.com/CGCL-codes/Fe
atureIndistinguishableAttack.
2 BACKGROUND AND RELATED WORK
2.1 Adversarial Attacks against DNNs
A deep neural network (DNN) can be viewed as a function Fğœƒ that
maps the input space X to the set of classication labels Y, where
ğœƒ represents the parameters of the network. An adversarial attack
aims to craft a small perturbation ğœ– for a normal input ğ‘¥ such that
the target model Fğœƒ will misclassify adversarial example ğ‘¥ + ğœ–:
Fğœƒ (ğ‘¥ + ğœ–) â‰  Fğœƒ (ğ‘¥).
An adversarial attack can be either targeted attack or untargeted
attack. The former attack aims to craft an adversarial example
misclassied into a specic (i.e., target) category ğ¶ğ‘¡: Fğœƒ (ğ‘¥ + ğœ–) =
ğ¶ğ‘¡, while the latter attack aims to craft an adversarial example
misclassied into any category dierent from the original category.
A targeted adversarial attack is generally harder to achieve than
an untargeted adversarial attack. Adversarial attacks can also be
classied into white-box, grey-box, and black-box attacks, according
to adversaryâ€™s level of accessibility to and knowledge of the model
to be attacked: full, partial, and none, respectively.
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea3160Many white-box adversarial attacks have been proposed. Typical
adversarial attacks are briey described here. Fast Gradient Sign
Method (FGSM) [20] is the rst adversarial attack. To generate an
untargeted adversarial example ğ‘¥+ğœ– for input ğ‘¥, it crafts adversarial
perturbation ğœ– with a single step of value ğœ‚ along the direction of the
gradient of the modelâ€™s loss function to maximize the loss function
to the original category ğ¶ğ‘œ of ğ‘¥:
ğœ– = ğœ‚ Â· ğ‘ ğ‘–ğ‘”ğ‘›(âˆ‡ğ‘¥ â„“(Fğœƒ (ğ‘¥), ğ¶ğ‘œ)),
where â„“(Fğœƒ (ğ‘¥), ğ¶ğ‘œ) is the loss function to category ğ¶ğ‘œ. It can also
be used to generate a targeted adversarial example by minimizing
the loss function to the target category ğ¶ğ‘¡:
ğœ– = âˆ’ğœ‚ Â· ğ‘ ğ‘–ğ‘”ğ‘›(âˆ‡ğ‘¥ â„“(Fğœƒ (ğ‘¥), ğ¶ğ‘¡)).
Like FGSM, other adversarial attack methods can also be used to
generate both targeted and untargeted adversarial examples with
similar relationships. Since the trapdoored defense aims to detect
targeted adversarial examples, we describe adversarial attacks to
craft targeted adversarial examples in the following description.
Projected Gradient Descent (PGD) [33, 34] extends FGSM to mul-
tiple iterations with double clipping. In each iteration,
ğ‘¥ğ‘›+1 = ğ¶ğ‘™ğ‘–ğ‘ğ›¿(ğ‘¥ğ‘› âˆ’ ğœ‚ Â· ğ‘ ğ‘–ğ‘”ğ‘›(âˆ‡ğ‘¥ â„“(Fğœƒ (ğ‘¥ğ‘›), ğ¶ğ‘¡)))
where ğ‘¥0 = ğ‘¥, and ğ¶ğ‘™ğ‘–ğ‘ğ›¿ operates on each pixel to clip its value
within Â±ğ›¿ of its original value in ğ‘¥ and within the valid range. PGD
is a much more powerful adversarial attack than FGSM.
Calini and Wangner Attack (C&W) [10] is considered one of the
most powerful adversarial attacks. It aims to generate adversarial
examples with minimized perturbations:
(cid:107)ğœ–(cid:107)ğ‘ + ğ‘ Â· â„“(Fğœƒ (ğ‘¥ + ğœ–), ğ¶ğ‘¡))
minğœ–
where ğ‘ is a parameter to weight the two objectives. Its optimal
value can be found with a binary search. Elastic Net [11] is a vari-
ant of C&W that minimizes both ğ¿1 and ğ¿2 norms of adversarial
perturbation ğœ– together with minimizing the loss function to the
target category.
Jacobian-based Saliency Map Attack (JSMA) [46] uses a Jacobian-
based saliency map to model the impact of each pixel on the re-
sulting classication and applies a greedy algorithm to iteratively
modify the most inuential pixel in crafting adversarial examples.
To deal with gradient obfuscation defenses (see Section 2.2),
Backward Pass Dierentiable Approximation (BPDA) [1] computes
gradients using dierentiable approximation, expectation, or repa-
rameterization to overcome dierent types of obfuscated gradi-
ents, while Simultaneous Perturbation Stochastic Approximation
(SPSA) [58] avoids computation of gradients by using stochastic
sampling to nd the global minimum in solving the optimization
problem of deriving adversarial perturbations.
While most adversarial attacks are white-box attacks, black-box
adversarial attacks have also been proposed [4, 44, 45, 55]. A practi-
cal attack proposed in [45] trains a substitute model with a synthetic
dataset and uses the substitute model to craft adversarial examples.