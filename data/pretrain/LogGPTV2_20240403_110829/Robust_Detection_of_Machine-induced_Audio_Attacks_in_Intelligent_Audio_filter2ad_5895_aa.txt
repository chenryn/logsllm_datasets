title:Robust Detection of Machine-induced Audio Attacks in Intelligent Audio
Systems with Microphone Array
author:Zhuohang Li and
Cong Shi and
Tianfang Zhang and
Yi Xie and
Jian Liu and
Bo Yuan and
Yingying Chen
Robust Detection of Machine-induced Audio Attacks
in Intelligent Audio Systems with Microphone Array
Yi Xie
Cong Shi
Zhuohang Li
Tianfang Zhang
Rutgers University
University of Tennessee
Knoxville, TN, USA
PI:EMAIL
Rutgers University
New Brunswick, NJ, USA
PI:EMAIL
New Brunswick, NJ, USA
PI:EMAIL
New Brunswick, NJ, USA
PI:EMAIL
Rutgers University
Jian Liu∗
University of Tennessee
Knoxville, TN, USA
PI:EMAIL
Bo Yuan
Rutgers University
New Brunswick, NJ, USA
PI:EMAIL
Yingying Chen
Rutgers University
New Brunswick, NJ, USA
PI:EMAIL
ABSTRACT
With the popularity of intelligent audio systems in recent years,
their vulnerabilities have become an increasing public concern.
Existing studies have designed a set of machine-induced audio at-
tacks1, such as replay attacks, synthesis attacks, hidden voice com-
mands, inaudible attacks, and audio adversarial examples, which
could expose users to serious security and privacy threats. To de-
fend against these attacks, existing efforts have been treating them
individually. While they have yielded reasonably good performance
in certain cases, they can hardly be combined into an all-in-one
solution to be deployed on the audio systems in practice. Addition-
ally, modern intelligent audio devices, such as Amazon Echo and
Apple HomePod, usually come equipped with microphone arrays
for far-field voice recognition and noise reduction. Existing defense
strategies have been focusing on single- and dual-channel audio,
while only few studies have explored using multi-channel micro-
phone array for defending specific types of audio attack. Motivated
by the lack of systematic research on defending miscellaneous
audio attacks and the potential benefits of multi-channel audio,
this paper builds a holistic solution for detecting machine-induced
audio attacks leveraging multi-channel microphone arrays on mod-
ern intelligent audio systems. Specifically, we utilize magnitude
and phase spectrograms of multi-channel audio to extract spatial
information and leverage a deep learning model to detect the fun-
damental difference between human speech and adversarial audio
generated by the playback machines. Moreover, we adopt an unsu-
pervised domain adaptation training framework to further improve
the model’s generalizability in new acoustic environments. Evalua-
tion is conducted under various settings on a public multi-channel
replay attack dataset and a self-collected multi-channel audio attack
∗Corresponding author.
1“Machine-induced attack” refers to the audio attack that requires to use a playback
device (e.g., loudspeaker or ultrasound speaker) to play the crafted speech samples.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea
© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-8454-4/21/11...$15.00
https://doi.org/10.1145/3460120.3484755
dataset involving 5 types of advanced audio attacks. The results
show that our method can achieve an equal error rate (EER) as low
as 6.6% in detecting a variety of machine-induced attacks. Even in
new acoustic environments, our method can still achieve an EER
as low as 8.8%.
CCS CONCEPTS
• Security and privacy → Systems security;
KEYWORDS
intelligent audio system; audio attack; microphone array
ACM Reference Format:
Zhuohang Li, Cong Shi, Tianfang Zhang, Yi Xie, Jian Liu, Bo Yuan, and Yingy-
ing Chen. 2021. Robust Detection of Machine-induced Audio Attacks in
Intelligent Audio Systems with Microphone Array. In Proceedings of the
2021 ACM SIGSAC Conference on Computer and Communications Security
(CCS ’21), November 15–19, 2021, Virtual Event, Republic of Korea. ACM, New
York, NY, USA, 16 pages. https://doi.org/10.1145/3460120.3484755
1 INTRODUCTION
During the past decade, the adoption of intelligent audio systems
has surged in both residential and industrial sectors, as they provide
a convenient interface for users to control and interact with smart
devices through voice commands. In particular, voice assistants
such as Amazon Alexa, Google Assistant, and Apple Siri have been
integrated into various platforms, enabling users to conveniently
control different aspects of their daily lives, such as smart home
appliance controls, online purchases, personal schedule/memo in-
quiries, and smart vehicle operations, etc.
With such widespread applications, the vulnerabilities of these
intelligent audio systems to various types of audio attacks have
become a rising security concern. For instance, replay attack [32, 74],
which attempts to bypass the authentication process simply using
a recording from the victim, has long been one of the dominant
sources of audio spoofing attacks. Synthesis attack [46, 78] utilizing
text-to-speech engines to mimic the victim’s voice is a common
alternative when the victim’s speech sample cannot be directly
obtained. Besides these conventional attack approaches, recent
studies have revealed new vulnerabilities including hidden voice
commands [7, 13, 68], inaudible attacks [85], and audio adversarial
examples [14, 81], which exploit either the gap between machine
Session 6C: Audio Systems and Autonomous Driving CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1884and human perceptions or the intrinsic vulnerability that lies in
deep learning models to make the attack unnoticeable.
Existing Defenses. The emerging attack vectors of intelligent
audio systems demand a general defense strategy to secure the
voice user interface against the disclosed suite of audio attacks.
However, most existing studies treat each type of attack differently
and seek to design specific mechanisms against each individual
attack. The replay attack is the most studied audio attack, where
many features derived from speech signals have been considered
for designing classifiers to distinguish replayed speech from human
speech, such as Mel Frequency Cepstral Coefficient (MFCC) [24],
Constant Q Cepstral Coefficients (CQCC) [66], Linear Prediction
Cepstral Coefficient (LPCC) [76], and Rectangular Frequency Cep-
stral Coefficient (RFCC) [24]. In addition to these power spectrum
features, relative phase shift [17], pitch patterns [33], and neuron
activation patterns [73], along with other spectral features [42] have
been proposed to help discriminate between human and synthetic
speech. Countermeasures to hidden voice commands, as pointed
out in the original work [13], include training a classifier (i.e., lo-
gistic regression) with the acoustic features extracted from hidden
voice commands and normal speech commands which was shown
to almost fully defend against this type of attack. A recent work [71]
also showed the potential of using the built-in motion sensors of
smartphones to defeat hidden voice commands. To defend against
inaudible attacks, microphone enhancement, baseband cancellation,
or learning unique features of modulated voice commands which
are distinctive from genuine ones have been considered [85]. Addi-
tionally, detection method based on propagation attenuation [65]
and active defense using an emitted inaudible “guard” signal to can-
cel the attack [27] have also been explored. As for defending audio
adversarial examples, various methods leveraging audio transfor-
mation [82] or transcription analysis [30, 83] have been proposed.
Limitation of Prior Work. As described above, existing studies
have been mostly focusing on designing dedicated mechanisms
for defending individual attack. These mechanisms are designed
from different perspectives and require different sensing modalities
or additional hardware modules, making them almost impossible
to be combined and deployed onto an audio device in practice.
Thus, a lightweight holistic defense strategy against all existing
audio attacks is highly desirable. In addition, most off-the-shelf
intelligent audio systems are equipped with a microphone array
for far-field voice recognition, noise reduction, and acoustic echo
cancellation [8]. For instance, Amazon Echo (4th generation) has 6
mics; Amazon Echo Auto has 8 mics; Apple HomePod has 6 mics,
etc2. In contrast, most existing efforts for detecting replayed audio
are based on single-channel recordings. Several studies [49, 64,
80, 87] go beyond a single channel to explore dual-channel stereo
recordings on smartphones. However, this line of work often suffers
from short detection range and still fails to exploit the rich sensing
capability of multi-channel microphone arrays that are ubiquitous
in intelligent audio systems. As one of the few works that exploit
multi-channel audio, EarArray [65] proposes to utilize the estimated
attenuation rate of the ultrasound signal via microphone array for
detecting inaudible attacks but does not generalize to other audio
2The number of microphones on mainstream intelligent audio devices is summarized
in Appendix Table 8.
Figure 1: Overview of the proposed approach.
attacks. A more recent work by Gong et al. [23] demonstrates that
compared to single-channel audio, multi-channel audio can help
improve the performance (up to 34.9%) for detecting replay attacks.
However, this work leverages beamforming technique [10] to filter
and combine multi-channel audio signals into a single-channel
signal, which loses distinct spatial information carried on other
channels. In addition, this work merely focuses on replay attacks
and does not address other more advanced audio attacks.
Benefits of Leveraging Multi-channel Audio. Compared to
existing single-channel-based solutions, multi-channel audio attack
detection offers benefits in several aspects: 1. Spatial Feature: In
addition to the temporal and spectral information used for single-
channel audio attack detection, multi-channel audio also encodes
important spatial information (e.g., angle of arrival (AoA) and time
difference of arrival (TDoA)) that is harder for the attacker to manip-
ulate; 2. Detection Range: Existing single-channel or dual-channel
audio attack detection systems (e.g., [49, 86, 87]) only perform
well in close-range scenarios (e.g., talking on phones) as its per-
formance decreases drastically when the microphone is far from
the user, while multi-channel audio signals collected from micro-
phone arrays can be utilized to achieve sound source localization
and speech enhancement by reducing noise and reverberations
which are critical for far-field detection; 3. Device Compatibility: To
achieve far-field hands-free voice control, intelligent audio devices
(e.g., Amazon Echo and Apple HomePod) often come equipped
with a multi-channel microphone array. Thus multi-channel audio
attack detection solutions can be directly deployed to these devices
to obtain enhanced performance without requiring extra hardware.
Proposed Work. Motivated by the potential benefits of multi-
channel audio, in this work, we develop a holistic solution to detect
various machine-induced audio attacks leveraging multi-channel
microphone arrays that are available on intelligent audio systems.
As shown in Figure 1, our method draws inspiration from the ob-
servation that all audio attacks require a playback device (e.g., loud-
speaker or ultrasound speaker) to play the crafted attack speech
sample, while genuine speech is uttered from human vocal cords.
This inherent difference of sound production will be carried over
into the produced audio, resulting in different patterns in signal fre-
quency and directivity [22], which can be captured by microphone
arrays and further utilized to differentiate genuine speech from
machine-induced attack audio. Instead of manually searching for
the optimal set of features, we resort to a learning-based approach
where the model can automatically adapt to any attack method,
microphone configuration, or acoustic environment that is repre-
sented in the training dataset, without requiring to be explicitly
tuned. Moreover, different from a conventional approach [23] that
utilizes beamforming to filter and combine multi-channel audio
signal into one single-channel signal, we make use of audio signals
from all available channels separately by forming 3D feature maps
Genuine SpeechMachine-inducedAudioMicrophoneArrayMulti-channelAudioAudioAttackDetection ModelSession 6C: Audio Systems and Autonomous Driving CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1885with magnitude-phase spectrograms so that the important spatial
information is preserved throughout the whole process. In addition,
to enable efficient detection in unseen environments, we exploit
unsupervised domain adaptation training to help the learned model
adapt to new acoustic environments without requiring labeled data.
We also explore different model configurations to design a compact
model that suits mobile applications without sacrificing much de-
tection accuracy. We summarize our main contributions as follows:
• We dissect existing machine-induced audio attacks, including
replay attacks, synthesis attacks, hidden voice commands, inaudi-
ble attacks, and audio adversarial examples, and design a holistic
defense strategy leveraging multi-channel audio recorded by the
microphone array equipped on intelligent audio systems.
• We build a deep learning model leveraging both magnitude and
phase information derived from multi-channel audio to achieve
accurate and robust detection of audio attacks without hand-
crafted features. Moreover, we adopt the unsupervised domain
adaptation framework to achieve environment-independent de-
tection, making the system still work well when deployed in a
new environment.
• To evaluate our proposed holistic solution, we re-implement a
set of representative advanced audio attacks and collect a dataset
of voice recordings of the reproduced adversarial speech sam-
ples in different environmental conditions with various play-
back/recording devices.
• Extensive experiments on a public multi-channel replay attack
dataset and an empirically-collected advanced audio attack dataset
showed that our method can achieve up to 6.6% equal error rate
(EER) in detecting these machine-induced audio attacks. Even in
a new environment, our environment-independent solution can
still achieve reasonably good performance.
2 RELATED WORK
2.1 Machine-induced Audio Attacks
Due to the open nature of voice access, intelligent audio systems
have been proven to be vulnerable to many spoofing attacks, such
as conventional replay attacks [32, 76], synthesis attacks [39, 46, 75,
78] and some other more advanced audio attacks leveraging mod-
ulated attacking sound (e.g., hidden voice commands [7, 13, 68]).
Among these attacks, replay attacks are the most accessible to
the adversary since it simply involves recording a victim’s speech
samples with a handy recording device (e.g., a smartphone) and
replaying the speech samples for the attack. A recent study [74]
also designed modulated replay attacks that align the frequency
domain distortions induced in the replay process, rendering the
replayed sound more similar to genuine human speech. When col-
lecting speech samples is difficult, the adversary can also launch
synthesis attacks that produce speech samples mimicking the vic-
tim’s voice characteristics (e.g., pitch range, frequency component
distributions). These attacks usually leverage voice synthesis mod-
els [46, 75] to convert text into the target speech of a victim, by
using only a small number of the victim’s voice samples for training
(e.g., collected through the Internet or public speech). In addition,
the adversary can modify the voice samples from arbitrary speakers
to make them sound like the victim’s voice for the attack [39, 78].
Due to the recent advancements in deep learning, such speech syn-
thesis models can produce natural-sounding speech, making the
attacks difficult to be detected.
In addition to these conventional attacks through replaying
human-sounding speech signals, recent studies demonstrated the
potential of generating unintelligible or even inaudible attacking
sound, by leveraging the perception gap between humans and ma-
chines [7, 13, 41, 68, 85] or the intrinsic vulnerability of embedded
deep learning models [14, 35, 36, 41, 84]. For example, hidden voice
commands [13, 68] convert speech into obfuscated voice commands
that are recognizable to speech recognition models while remain-
ing unintelligible to humans. As another example, inaudible at-
tacks [41, 85] modulate the recorded speech samples onto ultrasonic
frequency bands (e.g., over 20kHz), which are completely inaudible
to human listeners but can be demodulated by the microphones
due to their non-linearity properties. Furthermore, as current em-
bedded speech/speaker recognition engines are mostly based on
deep neural network (DNN) models, the adversary can explore the