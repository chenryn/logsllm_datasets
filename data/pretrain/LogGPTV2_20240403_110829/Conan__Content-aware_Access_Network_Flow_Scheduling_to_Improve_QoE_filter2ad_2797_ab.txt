u = {f t
u,1
VOLUME 8, 2020
7133
B. Guo et al.: Dueling DQN-Based Delay-Aware Cache Update Policy for Mobile Users in F-RANs
FIGURE 3. An illustration of content delivery.
u,c}K×C is built, where each pt
F-AP has the same storage capacity, i.e., Nf = N . A cache hit
occurs when the requested content has been stored in the local
storage. Otherwise, a cache miss occurs, and the requested
content should be fetched from the remote content provider.
Let a binary element µt
c,f denote the relationship between
the requested content c and the storage of F-AP f , i.e., if c
c,f = 1,and
has been cached in the storage of f at slot t, µt
c,f = 0 otherwise.
µt
In order to represent the content preferences of MU, a K ×
C matrix Pt = {pt
u,c is the
ized such that(cid:80)C
probability that MU u requests content c at time slot t. For
each MU, its preferences during each time slot are normal-
u,c = 1. It is assumed that different
MUs have different content preferences. Speciﬁcally, content
u(c)−κu /(cid:80)C
requests of each user u follow the Zipf distribution [37]
with parameter κu. Therefore, the preference probability pt
u,c
u(c) ∈
can be obtained by φt
u(C)} is a random permutation of content
{φt
u(1), φt
library C = {1, 2, . . . , c, . . . , C}.
Generally, cache hit ratio is a crucial indicator to eval-
uate the performance of a caching policy. In this work,
the average cache hit ratio Hav during a long period T is
deﬁned as
c=1 c−κu, where φt
u(2), . . . , φt
c=1 pt
(cid:80)
(cid:80)
t
u
(cid:80)
u,c · σ t
c pt
T · K
u,c
Hav =
,
(1)
where T is the total number of time slots, and σ t
u,c is a binary
element that indicates whether the content requested by MU
has been stored in its associated F-APs at slot t. σ t
c is given
by
0, (cid:88)
1, (cid:88)
f
f
u,f
c,f · βt
µt
c,f · βt
µt
< 1;
u,f ≥ 1.
u,c ==
σ t
(2)
is noticed that MU only downloads the requested
It
content from one of its associated F-APs,
though the
requested content has been stored in more than one
associated F-APs.
D. CONTENT DELIVERY
When an MU requests a content, the cache-hit content can
be delivered from the local cache of its associated F-APs
directly, and the cache-miss content should be fetched from
FIGURE 2. The time-varying topology relation between MUs and F-APs.
u,f = 1}.
f = {u ∈ U|βt
B. USER MOBILITY
The mobility pattern of MUs can be represented by the
topology relation between MUs and F-APs. A K × M matrix
Bt = [βt
u,f ]K×M is built to denote the topology relationship
between MUs and F-APs at slot t, where each βt
u,f is a binary
element, and is used to indicate the connectivity between MU
u and F-AP f . If u lies within the coverage of f at slot t,
u,f = 1, and βt
u,f = 0 otherwise. The set of MUs in the
βt
u,f = 1}.
coverage of f at slot t is deﬁned as U t
Likewise, the set of associated F-APs for u at slot t is deﬁned
u = {f ∈ F|βt
as F t
Due to the user mobility, the relationship matrix Bt should
be time-varying, which will seriously affect the caching pol-
icy. To model the various behaviors of MUs, F t
u randomly
varies every τu slots. In other words, MU u will stay in the
cooperation region of F t
u for τu slots. Taking Fig. 2 as an
example for time-varying topology relation between MUs
and F-APs. In the example, the triangles represent the F-APs,
the circles denote the MUs, and the ellipses denote the coop-
eration regions. The circles with the same color means that
they are located in the same cooperation region. The topology
relation between MUs and F-APs is different during different
time slot, since each MU can move to a random cooperation
region after staying in a region for τu time slots. In other
words, the dwell time and moving path for each MU may be
distinct.
C. CONTENT CACHING
An MU requests the ﬁles from its associated F-APs. Since
the ﬁles with different sizes are always divided into contents
of the same size, it is assumed that all contents in the system
have the same size Sc. As for an F-AP f , it can cache up to
Nf contents from a content library C = {1, 2, . . . , c, . . . , C}
loss of generality, assume
in its local storage. Without
Nf (cid:28) C. For simpliﬁcation, this paper assumes that each
7134
VOLUME 8, 2020
B. Guo et al.: Dueling DQN-Based Delay-Aware Cache Update Policy for Mobile Users in F-RANs
the remote content provider, which leads to extra transmission
delay. In order to improve average hit ratio and reduce the
average transmission delay, the caching policy should decide
how to cache the contents at each slot. However, the deliv-
ery performance of requested contents depends not only on
the caching policy, but also on the wireless channel states,
e.g., channel fading, interference and so forth. For the cache-
hit content, their transmission delay may be different, because
of the different channel states.
i
j
)],
j|2l−ι
u\i |ht
n0Bu + p0
[Bulog2(1 +
As for the cache-hit content of MU u which is stored in
f during slot t is
F-AP f , the average transmission rate Rt
deﬁned as
(cid:80)
i|2 l−ι
p0|ht
f ,u = Eht
i
Rt
(3)
j∈F t
where E(·) means the mathematical expectation, Bu is the
transmission bandwidth for the MU u, p0 is the transmit
power of each F-AP, ht
i is the small-scale fading channel, li
(cid:80)
is the distance between MU and the cache-hit F-AP, ι is the
path loss factor, n0 is the power spectral density of noise, and
p0
is the strongest interference power from
the other associated F-AP. Since the strongest interference
comes from the associated F-AP where the requested content
has not been stored, assume that li = lj = l. An illustration
of content delivery is shown in Fig. 3. An MU dwells in the
cooperation region of F-AP 1 and F-AP 2, and the cache-hit
content of MU has been stored in F-AP 1. The MU down-
loads the requested content from F-AP 1, whilst the strongest
interference comes from F-AP 2. Besides, it is assumed
that the transmission bandwidth is allocated to each user
equally.
j|2 l−ι
u\i |ht
j∈F t
j
The transmission delay of cache-hit content can be
calculated by
dhit = Sc/Rt
f ,u
.
(4)
As for a cache-miss content, its transmission delay is denoted
by dmiss, which is higher than dhit. Hence, the average trans-
mission during a long period T is given by
Dav =
u,c · dhit + (1 − σ t
T · K
u,c) · dmiss]
u,c · [σ t
c pt
(cid:80)
(cid:80)
(cid:80)
u
.
t
(5)
Since a ﬁle with a big size can be divided into several contents
with a small size, assume that each requested content can be
delivered within a single time slot. It means that the content
delivery in each slot will not be interrupted by the movements
of MUs. Consequently, Sc should be small enough to make
dhit < dmiss.
E. PROBLEM FORMULATION
Considering time-varying channel states, user mobility,
diverse preferences of different MUs, limited cache capac-
ity of each F-AP, this paper aims to ﬁnd a cache update
policy to minimize the average transmission delay, and
the cooperative caching problem in F-RAN system is
formulated as
min Dav
s.t.
(cid:88)
u,c · µt
(cid:88)
pt
u,c · σ t
pt
c
u,f = 2,
βt
c
c,f · βt
u,c ≤ K ,
∀u ∈ U
(cid:88)
(cid:88)
(cid:88)
u
u
f
u,f ≤ N ,
∀f ∈ F (a);
(b);
∀t ∈ T
(6)
(c),
where constraint (6.a) means that each F-AP f is allowed to
cache no more than N contents, constraint (6.b) indicates that
the number of cache hits during each slot t is at most K , and
constraint (6.c) represents that each MU uk can be served by
two Np F-APs cooperatively.
the
IV. THE PROPOSED CACHE UPDATE POLICY
To ﬁgure out
caching problem raised in (6),
a deep-Q-network with dueling architecture [25] is adopted.
In this section, the cache update is modeled as an MDP
[38]. Then, the workﬂow of dueling deep-Q-network is illus-
trated. Finally, the dueling DQN based cache update policy is
proposed.
A. MARKOV DECISION PROCESS MODEL
A RL problem can be modeled as an MDP with state space
S, action space A, transition probability P, reward function R
and discount factor γ . In an MDP, the agent can learn how to
interact with the environment to obtain the maximum average
reward. In detail, the agent interacts with the environment
in a sequence of discrete iteration steps. At each iteration
step i, the agent observes the state si of the environment
and chooses an action ai. The agent will receive a reward
ri = R(si, ai) from the environment, after the selected action
is executed. Then, the system transits into the next step i + 1
with probability P(s(cid:48)|s, a) (cid:44) P[si+1 = s(cid:48)|si = s, ai = a],
s(cid:48)∈S P(s(cid:48)|s, a) = 1, for all s ∈ S, a ∈ A. Besides,
a deterministic policy in an MDP is a mapping from state
space S to action space A, i.e.,a = π(s). According to the
∞(cid:88)
Bellman equation, the average reward is deﬁned as
ρπ (s0) = E[
= E[R(s0, π(s0)) + γ(cid:88)
P(s(cid:48)|s0, π(s0))ρπ (s(cid:48))].
where(cid:80)
γ iR(si, π(si))|s0]
i=0
(7)
The goal of agent is to ﬁnd a policy π∗ to achieve the
maximum average reward, that is
π∗ = arg max