# 2017年 AI安全风险白皮书

##### 译文声明
本文为翻译文章，具体内容及含义以原文为准。译文仅供参考。

## 摘要
深度学习正在引领新一轮的人工智能浪潮，受到了工业界和全社会的广泛关注。尽管人们对人工智能抱有诸多美好的憧憬，但现实中，随着一批深度学习应用逐渐落地，相关的安全问题也日益凸显。

360安全研究院在过去一年里对深度学习系统的安全性进行了深入研究，并通过本白皮书系统地总结了这些研究成果。AI系统的安全问题主要包括模型安全、数据安全以及代码安全等方面。本文将以目前流行的图像识别AI系统为例，介绍其在上述方面所面临的安全威胁。我们不仅关注传统的攻击类型（如恶意输入导致的拒绝服务、信息泄露、系统劫持），还特别关注AI特有的安全问题，例如逃逸攻击和数据污染攻击等。

关于AI系统的安全威胁，本文将从以下三个角度进行阐述：第一部分描述深度学习系统软件实现的复杂度及其带来的安全漏洞；第二部分讨论针对深度学习模型的逃逸攻击；第三部分揭示深度学习系统数据流中的安全威胁及相应的降维攻击示例。白皮书的主要内容参考了360安全团队近期发布的相关报告。

## 深度学习软件实现中的安全问题
### 人工智能讨论中的安全盲点
当前公众对于人工智能的关注，尤其是深度学习领域，往往忽略了安全性的考量。这种现象被称为“人工智能安全盲点”。造成这一盲点的主要原因在于算法设计与实际实现之间的差距。最近关于深度学习的讨论大多停留在算法层面或前景展望上，很少考虑应用场景和程序输入可能存在的恶意构造情况。

以手写数字识别为例，基于MNIST数据集的应用是深度学习的一个典型例子。虽然最新的教程中几乎都会采用这个案例来展示深度学习技术，但在这些教程中，通常只关心特定类别的近似度和置信概率区间，而忽略了人为构造的恶意输入可能导致程序崩溃甚至被攻击者控制的风险。这正是当前人工智能讨论中的一个显著安全盲点。

### 深度学习系统的实现及依赖复杂度
许多深度学习软件都构建在各种框架之上，如TensorFlow、Torch和Caffe等。这些框架简化了神经网络的设计与开发过程，使开发者能够专注于业务逻辑而非底层细节。然而，每个框架背后都有复杂的组件依赖关系，包括图像处理、矩阵计算、数据处理及GPU加速等功能模块。比如Caffe除了自身的核心功能外，还依赖于137个第三方动态库；TensorFlow则包含了多达97个Python模块。

系统越复杂，潜在的安全隐患也就越多。任何存在于深度学习框架及其依赖组件中的安全漏洞都可能威胁到整个应用系统。此外，不同开发者之间对于接口的理解可能存在差异，这也增加了安全隐患发生的可能性。我们在调查过程中就发现了一些由于这种不一致而导致的安全问题。

### 深度学习软件实现细节中的安全问题
正如安全专家所言，“魔鬼藏在细节之中”。任何大型软件系统都不免存在实现上的缺陷。鉴于深度学习框架的高度复杂性，这类问题同样不可避免。

360 Team Seri0us团队在一个多月的时间里发现了数十个深度学习框架及其依赖库中存在的软件漏洞，涵盖了几乎所有常见的类型，如内存访问越界、空指针引用、整数溢出和除零异常等。这些问题可能会引发针对深度学习应用的拒绝服务攻击、控制流劫持、分类逃逸以及潜在的数据污染攻击。

#### 案例分析
- **案例1**：基于TensorFlow的语音识别应用遭受拒绝服务攻击  
  在此案例中，我们发现了一个存在于TensorFlow所依赖的NumPy库中的简单逻辑漏洞（CVE-2017-12852）。该漏洞位于`pad`函数内，通过构造特定输入可以使循环无法终止，从而导致应用程序长时间占用CPU资源而不返回结果，最终形成拒绝服务攻击。
  
- **案例2**：恶意图片导致基于Caffe的图像识别应用出现内存访问越界  
  当使用Caffe进行图片识别时，如果输入的是经过特殊处理的畸形图片，则可能会触发libjasper库中的内存越界错误，进而导致整个应用程序崩溃或数据流被篡改。

以上两个实例展示了深度学习框架及其依赖组件中存在的安全风险，强调了在推进AI技术发展的同时必须重视并解决这些潜在的安全隐患。