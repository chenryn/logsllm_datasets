authentication, which requires an attacker to consistently
fool the authentication system, rather than only succeed once.
390(a) Touch Dataset
(b) Mouse (own machine)
(c) Gait II
Figure 4: Fraction of accepted samples for diﬀerent user combinations, the values outside the diagonal reﬂect
the FAR. The touch dataset shows an even distribution of the FAR, resulting in a low standard deviation.
Both the mouse and gait datasets show more systematic errors, as indicated by few dark spots and a high
standard deviation and kurtosis.
In our datasets we actually observe both of these scenarios,
leading to a need to accurately distinguish them without
the need of manually examining confusion matrix. Figure 4
suggests that the mouse movement and gait biometrics show
a high number of extreme outliers for the FAR (as indicated
by the dark spots oﬀ the diagonal). Conversely, the false
accepts seem to be more evenly distributed between attackers
for the touch input biometric, suggesting it would be better
suited for continuous authentication from a pure security
perspective.
For the FRR we observe similar diﬀerences in distribu-
tions, although the consequences are diﬀerent. Systematic
rejections of individual users might indicate erratic behavior
(such as excessive head movements or poor calibration for
the eye movement biometric), while even distributions of
errors suggest a lower distinctiveness of features in general.
The former could be mitigated by examining the root cause
of error for the aﬀected users and, if these can not be ﬁxed,
authenticating users through a diﬀerent mechanism. Mul-
timodal authentication systems are particularly well-suited
for this, as they can dynamically choose biometrics that
work well for this speciﬁc user. As such, biometrics where
the FRR is focused on few users might be easier to use in
practice. Figure 5 shows the distribution of the FRR for
diﬀerent over-time datasets for the eye movement biometric.
Errors are focused on few users given a short time-distance
and start to evenly aﬀect more users over two weeks.
3.2 Metrics to Quantify Systematic Errors
In this section we will discuss a number of statistical mea-
sures to better capture systematic errors and analyse how
well they perform on our real-world data.
3.2.1 False Accept Rate
As discussed above, the false accept rate should ideally
spread out evenly across attackers and therefore minimize
systematic errors. In order to reﬂect systematic false nega-
tives it might be an obvious choice to report the maximal
FAR observed, this would then allow to give estimates of the
maximal time it takes to ﬁnd an attacker. However, Table 2
shows that this measure is 1 for the vast majority of datasets,
suggesting at least some degree of systematic errors for most
biometrics. In addition, it would unfairly penalize larger
datasets, as the probability of the set including two very
similar users increases with the sample size. This could be
mitigated by reporting the fraction of undetected attackers
(i.e., the fraction of user-attacker pairs with an FAR of 1,
given as “1’s” in Table 2). However, given the relatively small
number of samples per user for each dataset, there might not
be a statistical diﬀerence between an FAR of 1, and one very
close to 1, suggesting that this feature would also be overly
sensitive. Another candidate metric is the standard deviation
of the sample. Table 2 shows that the standard deviation
varies between 0.05 and 0.37. However, the standard devia-
tion quantiﬁes the variation in a dataset, but does not reveal
whether this variation is due to a few extreme outliers (which
would be problematic) or a high number of moderate outliers
(which would be a less severe problem). This limitation can
be mitigated by also taking into account the kurtosis of the
sample. Kurtosis is the fourth standardized moment and
is a measure of the tailedness of a distribution. As such, a
high kurtosis indicates that the distribution tends to pro-
duce more extreme outliers. Combining standard deviation
and kurtosis (i.e., an ideal distribution being low standard
deviation and low kurtosis) seems to ﬁt our required proﬁle.
Figure 7 shows datasets with similar standard deviation but
diﬀerent kurtosis. The ﬁrst gait dataset shows systematic
errors, indicated by a high kurtosis of 11.53 while the second
one exhibits more random errors, leading to a lower value
of 2.16. Despite this combination seeming ﬁt for purpose, it
would be diﬃcult to use to accurately rank biometrics as any
total ordering (i.e., preferring kurtosis over standard devia-
tion or vice-versa) would be somewhat arbitrary. The Gini
Coeﬃcient (GC) has been proposed in 1912 as a measure
of statistical dispersion to reﬂect the income distribution of
a nation’s residents [19]. A GC of 0 indicates a maximal
equality of values (i.e., every resident having the same in-
come), while a value close to 1 represents maximal inequality
(i.e., one resident earning all the income). As a measure of
inequality the GC is also intuitively applicable to capture
types of error distributions, with a high GC reﬂecting more
systematic errors. An intuitive geometric representation of
391(a) Intra-Session
(b) Inter-Session
(c) Over Two Weeks
Figure 5: Distribution of the FRR between users for three diﬀerent datasets based on the eye movement
biometric using all features. While the average FRR is similar for all datasets the distributions are not. The
two-weeks dataset shows moderate error rates for many users while the errors are concentrated on few users
for the other two. This property is modelled by the kurtosis and to a lesser degree by the standard deviation.
(a)
(b)
Figure 6: False rejects are spread evenly for the touch input biometric and are focused on very few users for
the eye movement biometric. This is reﬂected in the diﬀerence in Gini coeﬃcients (0.55 vs 0.93).
the Gini Coeﬃcient is the area between the Lorenz Curve
(which, in our scenario, measures the total error contributed
by the bottom x % of users) and the Line of Equality (which
is the Lorenz curve of a system where all users contribute
identical error rates). The GC is shown as the shaded area in
Figure 6. The GC has two important properties that makes
it a suitable metric: Its scale idependence means that it does
not depend on the total or average error of a system, only the
distribution of values. As such, it can be used to compare
systems with diﬀerent error rates. Conveniently, the GC
always lies between 0 and 1, unlike standard deviation and
kurtosis, which can take arbitrarily high values. In addition,
it is population independent and does not depend on the
number of samples in the dataset. This is of crucial impor-
tance, as the number of subjects in biometric datasets varies
greatly and using only subsets of equal size seems infeasible
due to authors rarely publishing their raw data.
Figure 8 shows the Gini Coeﬃcient for the two most ex-
treme cases we observe in our datasets. For the touch input
biometric many attackers contribute to the overall FAR,
while the eye movement biometric’s intra-session dataset
(a) First Gait Dataset
(b) Second Gait Dataset
Figure 7: Distribution of the FRR between users for
the two gait datasets.
FAR is caused by very few extremely successful attackers.
Reducing security through strong features:
It is
interesting to note that the distribution of errors, and thereby
the GC, does not simply depend on the biometric modality,
but also the type of features used. When removing the pupil
diameter, one of the most distinctive features of the eye
392Biometric
Eye Movements
all features
Eye Movements
without pupil
diameter
Eye Movements II
Gait
Dataset
Intra-Session
Inter-Session
2-weeks
Intra-Session
Inter-Session
2-weeks
Reading
Writing
Browsing
Video I
Video II
Dataset I
Dataset II
σ
EER
6.90%
0.22
7.99% 0.21
8.43% 0.20
19.83% 0.34
17.11% 0.30
17.52% 0.29
1.17% 0.03
4.80% 0.11
0.89% 0.04
0.09
3.93%
1.86%
0.07
β2
13.05
11.50
9.39
3.58
4.10
4.45
23.57
51.07
34.68
15.20
33.59
8.44% 0.22
28.4% 0.37
9.57
1.94
FAR
GC max 1’s
0.02
0.92
0.02
0.90
0.01
0.87
0.77
0.09
0.03
0.74
0.74
0.05
1.00
1.00
1.00
1.00
1.00
1.00
0.95
0.94
0.96
0.88
0.96
0.87
0.59
0.21
0.93
0.29
0.57
0.49
0.96
1.00
0.00
0.00
0.00
0.00
0.00
0.00
0.12
σ
0.25
0.25
0.15
0.39
0.27
0.27
0.03
0.11
0.03
0.09
0.04
0.26
0.32
FRR
β2
12.59
8.48
2.81
3.41
6.21
4.78
4.26
2.96
8.11
5.21
3.85
11.53
2.16
GC
0.93
0.90
0.77
0.80
0.77
0.74
0.79
0.74
0.90
0.83
0.74
0.87
0.87
0’s
0.93
0.89
0.74
0.74
0.50
0.58
0.70
0.40
0.90
0.80
0.60
0.57
0.33
Touchscreen Input
Inter-Session
2.99% 0.05
15.01
0.75
0.40
0.00
0.04
6.74
0.55
0.05
Mouse Movements
Own machine
Lab machine
9.22%
0.21
9.98% 0.23
11.98
8.96
0.89
0.86
1.00
1.00
0.02
0.02
0.24
0.15
5.57
2.01
0.85
0.69
0.82
0.57
Table 2: Results of applying the new metrics to our datasets. As evidenced by the Gini coeﬃcient, random
errors are particularly prevalent for the touch input biometric, while eye movements are prone to systematic
errors. We can also observe that not using the pupil diameter results in fewer systematic errors, as evidenced
by a lower GC and lower kurtosis.
movement biometric, the average error rates rise, but at the
same time the GC decreases. This suggests that the pupil
diameter is actually one of the key features that contributes
to systematic errors especially because it is, on average, a
very distinctive one. Due to the pupil diameter’s relative
stability it is suitable to separate most users, but leads to
the consistent confusion of users with a similar baseline
pupil diameter. As such, using the feature helps to further
distinguish users that were relatively well-separated before,
but does little to reduce systematic errors or might even
make them more signiﬁcant. This data supports the idea
that, in some scenarios, adding distinctive features could
actually reduce the security of a system, despite the lower
average error, by adding systematic false negatives. As a
result, researchers should take great care to not blindly strive
for the lowest average EER but to also take into account
how changes to features or classiﬁers inﬂuence their system’s
error distributions.
3.2.2 False Reject Rate
For the FAR, it is easy to agree on the fact that systematic
errors are more problematic, as it leads to some attackers
perpetually escaping detection. Determining the most favor-
able error distribution is not quite as obvious for the FRR. If
most of the FRR is due to extreme outliers it might suggest
that this is due to erratic user behavior, such as a bad cali-
bration for eye tracking. In that sense, this scenario might be
preferable, as this indicates a problem with a small number
of users, rather than an overall problem of the system which
manifests itself in all users. When the deployed system shows
high error rates for some users, it might be possible to further
explore the root cause of the errors (which could involve ed-
ucating the user, but could also aid in improving the system
itself). Reporting the fraction of users perfectly recognized
by the system (given as “0’s” in Table 2) would be an obvious
approach to reﬂect this property, but Figure 7 shows why
it would be quite noisy in practice. Using a combination
of kurtosis and standard deviation would also suﬀer from
the same problems as for the FAR, namely the diﬃculty of
establishing a total order between systems.
Following the shortcomings of the other metrics, the Gini
Coeﬃcient can again be used to quantify where exactly a
biometric recognition system lies between the extremes of
purely systematic and purely random errors. Our data shows
that the touch input biometric has the most even distribution
of false rejects, exhibiting a GC of 0.55. The eye movement
biometric generally shows the highest GC, with little change
due to feature sets, time distance or tasks used. This might
be explained by the fact that the biometric strongly relies on
controlled user behavior, speciﬁcally requiring a good calibra-
tion and as few head movements as possible. If some users
are better at achieving this optimal behavior it would explain
this rather extreme concentration of errors. In addition, this