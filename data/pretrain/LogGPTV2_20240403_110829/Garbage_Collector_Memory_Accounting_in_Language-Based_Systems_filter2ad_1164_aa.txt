title:Garbage Collector Memory Accounting in Language-Based Systems
author:David W. Price and
Algis Rudys and
Dan S. Wallach
Garbage Collector Memory Accounting in Language-Based Systems
David W. Price
Algis Rudys
Dan S. Wallach
Department of Computer Science, Rice University
{dwp,arudys,dwallach}@cs.rice.edu
Abstract
Language run-time systems are often called upon to
safely execute mutually distrustful tasks within the same
runtime, protecting them from other tasks’ bugs or other-
wise hostile behavior. Well-studied access controls exist in
systems such as Java to prevent unauthorized reading or
writing of data, but techniques to measure and control re-
source usage are less prevalent.
In particular, most lan-
guage run-time systems include no facility to account for
and regulate heap memory usage on a per-task basis. This
oversight can be exploited by a misbehaving task, which
might allocate and hold live enough memory to cause a
denial-of-service attack, crashing or slowing down other
tasks. In addition, tasks can legitimately share references
to the same objects, and traditional approaches that charge
memory to its allocator fail to properly account for this
sharing. We present a method for modifying the garbage
collector, already present in most modern language run-
time systems, to measure the amount of live memory reach-
able from each task as it performs its regular duties. Our
system naturally distinguishes memory shared across tasks
from memory reachable from only a single task without re-
quiring incompatible changes to the semantics of the pro-
gramming language. Our prototype implementation im-
poses negligible performance overheads in a variety of
benchmarks, yet provides enough information for the ex-
pression of rich policies to express the limits on a task’s
memory usage.
1 Introduction
Multitasking language run-time systems appear in a vari-
ety of commercial systems, ranging from applets running in
web browsers and servlets in web servers to plugins running
in extensible databases and agents running in online mar-
kets. By enforcing the language’s type system and restrict-
ing the interfaces available to untrusted code, language-
based systems can often achieve very restrictive “sandbox”
policies, limiting access to the ﬁle system, network, and
other speciﬁc resources. Furthermore, by avoiding the costs
of separate operating system processes, the costs of con-
text switching overhead and inter-task communication can
be radically reduced. Since all tasks share the same address
space, pointers to objects can be directly passed from one
task to another. Tasks should be thought of as a generaliza-
tion of Java applets or servlets; a task is both the code and
the data on which the code operates.
1.1 Availability
When running multiple concurrent tasks which might be
actively malicious to each other in a single language run-
time system, it is critical for the system to make guaran-
tees about its availability. Access controls are not sufﬁcient
to protect against denial of service attacks. More gener-
ally, most existing security mechanisms built into language-
based systems focus on safety policies, that is, security poli-
cies that are strictly a function of a task’s prior execution
history and the current request being made. Such policies
are designed to guarantee that “nothing bad ever happens.”
However, to preserve system availability, we need policies
that talk about the future, i.e., “something good eventually
happens.” Such liveness policies [2] can cover topics as di-
verse as preventing deadlocks or preventing exhaustion of
aggregate resources including CPU, memory, and network
usage. In these cases, we cannot conclusively state, for any
given lock operation or memory allocation, whether the pro-
gram is in a “safe” state. However, we do wish to guarantee
that the system won’t get stuck.
When designing systems to have high availability, there
are two general approaches we can take. One approach is
to dynamically monitor system usage to detect when bad
conditions occur, and taking corrective action when neces-
sary. The other approach is to statically analyze the system
and prove that the it cannot ever reach a bad state (or, that
it will always, eventually, reach a good state). For this re-
search, we are concerned with the availability of free mem-
ory, for which researchers have designed both static and
dynamic mechanisms (related work and comparable sys-
tems are discussed in Sections 2 and 5). Since we wish
Proceedings of the 2003 IEEE Symposium on Security and Privacy (SP(cid:146)03) 
1081-6011/03 $17.00 ' 2003 IEEE 
Figure 1.
In traditional operating systems,
each process is its own, self-contained box,
so determining the memory usage of a pro-
cess is equivalent to measuring the size of
the box.
Figure 2. In language-based systems, mem-
ory may be shared among multiple tasks, so
determining the memory usage of a single
task is difﬁcult.
to support general-purpose programming languages, com-
puting static bounds on memory usage would be computa-
tionally infeasible. Instead, we need a mechanism that can
efﬁciently measure memory usage, even in the presence of
data sharing across tasks and with garbage collection man-
aging the memory. Given such a measurement mechanism,
we can then envision policies that detect when the system
is running out of free memory. Using the measured data,
these policies could identify misbehaving tasks and termi-
nate them, reclaiming their resources, while allowing other
tasks to continue running uninterrupted.
1.2 Language-based systems
In a traditional operating system, the problem of mea-
suring memory allocation is straightforward. Processes en-
capsulate all the memory being used by a given task, mak-
ing it easy to measure the total memory in use and to ap-
ply limits on how big a process can grow (see Figure 1).
Likewise, when a process is terminated, it is easy to re-
claim all the memory in use because it is part of the pro-
cess’s address space. To achieve this containment of mem-
ory, process-structured systems limit the ability to share
data, typically requiring objects to be copied rather than
shared by reference. Ideally, we would like to have the low-
cost, type-safe sharing that can be achieved in language-
based systems (e.g., see Figure 2) combined with the mem-
ory accounting and termination semantics achievable with
process-structured systems. Currently, however, language-
based systems lack semantics for measuring their tasks’
memory usage. We wish to support arbitrary sharing of ob-
ject references across tasks, implying that more than one
task may be sharing the responsibility of keeping any given
object live. This complicates any deﬁnition of memory us-
age, as some objects could potentially be “counted” more
than once.
Most language-based systems use a garbage collector
to provide memory management services. By design, the
garbage collector already examines the entire heap to dis-
cover what memory is being held live. By making simple
modiﬁcations to the garbage collector, causing it to process
each task in turn and count as it goes, we can track the mem-
ory usage of individual tasks with little overhead beyond the
regular cost of garbage collection.
Our system provides sufﬁcient information to allow for
a variety of ﬂexible memory usage policies. The system
maintains statistics not only on the amount of memory a
task uses but also on the degree to which that memory is
shared with other tasks. This would enable such policies as
limiting the amount of memory used exclusively by a task to
some percentage of system memory, and likewise limiting
the total amount of memory used by the task. Language-
based mechanisms like soft termination [46] could then be
used to enforce such policies, terminating any tasks that vi-
olate the limit.
In the following sections, we describe the design and im-
plementation of our memory accounting system. We de-
scribe the design of our system in Section 2. Section 3 dis-
cusses our implementation of memory accounting and its
performance impact. We discuss the sorts of policy seman-
tics our system supports in Section 4. Finally, we present
related work in Section 5, and future work in Section 6.
2 Design
There are several different ways for a language-based
system to track the memory usage of its individual tasks.
We ﬁrst discuss some proposed solutions, and describe the
hard problems raised by their failings. We then discuss the
design of our system.
Proceedings of the 2003 IEEE Symposium on Security and Privacy (SP(cid:146)03) 
1081-6011/03 $17.00 ' 2003 IEEE 
2.1 Instrumented allocation
One common mechanism for determining the memory
usage of tasks is to rewrite the task’s code at load time.
Memory allocations are instrumented to charge tasks with
memory usage when they allocate objects, granting rebates
when those objects are ﬁnalized. This approach has the ben-
eﬁt that no modiﬁcations are required to the underlying lan-
guage run-time system. JRes [22] and Beg and Dahlin [8]
both instrument memory allocations as a way to account for
memory usage by tasks in Java.
However, there are several problems to using this ap-
proach. First, only allocation that explicitly occurs in the
task is charged to that task. Any implicit allocation or allo-
cation performed by the system on behalf of the task is not
charged to it. In addition, in both JRes and Beg and Dahlin’s
system, accounting is performed on a per-thread basis. If a
“system” thread or a another task’s thread calls into a task,
it could potentially be “tricked” into allocating memory on
behalf of the task, giving that memory away “for free.”
Furthermore, tasks can share memory with one another
(see Figure 2). A task may allocate a block of memory,
share that memory with another task, and later drop its
pointer. In most language-based systems, however, mem-
ory is kept alive if any live pointers to it exist. As a result,
another task could, out of necessity or malice, hold memory
live; the task that initially allocated that memory would be
forced to keep paying for it.
2.2 Process abstractions
Another common mechanism for accounting for mem-
ory usage is to use process abstractions. In some systems,
each task is allocated its own heap, and the memory usage
charged to that task is the size of that heap. KaffeOS [5, 6]
is a system for Java that, in conjunction with an explicit
process-like abstraction for Java tasks, provides a sepa-
rate heap for each task. The multitasking virtual machine
(MVM) [21] and systems by Bernadat et al. [9], and van
Doorn [51] similarly use separate heaps or memory spaces
to facilitate accounting for memory. Some systems [40, 47]
even go so far as to run the JVMs in separate Unix processes
on separate machines.
These systems accurately account for memory a task
keeps live. However, inter-task communications and mem-
ory sharing are severely restricted, limiting the usefulness of
the language. In addition, these systems are implemented
with nontrivial customizations to the VM. Adapting these
ideas to a new VM can require signiﬁcant engineering re-
sources.
In some systems, function calls and memory references
are artiﬁcially restricted (either through some mechanism
built into the run-time system or using code-to-code trans-
In this case, instrumenting memory alloca-
formations).
tions and object ﬁnalization yields an accurate accounting
for the amount of memory used by a task. Examples in-
clude J-Kernel [34], J-SEAL2 [11], and Luna [35]. These
systems are more accurate than strictly instrumented alloca-
tion. However, they still restrict inter-task communications
and memory sharing among tasks.
2.3 Garbage collection-based accounting
Once we allow object references to be shared across
tasks, the task that allocates an object in memory may not
necessarily be the task that ends up using the object or keep-
ing it live. Once a reference to an object has been given
out, anybody could potentially hold that object reference.
Clearly, we would like to only charge tasks for the memory
they are keeping live, rather than the memory they allocate.
Under this rationale, live objects should be charged to
those tasks from which they are reachable in the graph of
heap objects. Conveniently, tracing garbage collectors al-
ready traverse this graph to ﬁnd the reachable objects and
free the space occupied by unreachable objects. By care-
fully managing the order in which the GC does its work and
having the GC report back to us on its ﬁndings, we can use
the GC as our tool for measuring each task’s live memory
footprint.
A typical garbage collector works by starting at a deﬁned
root set of references and doing a graph traversal to ﬁnd all
the memory reachable from those references. Memory not
reached during this graph traversal is garbage and can be
used for allocating new objects. In our system, we augment
the collector to sequentially trace all the reachable memory
from each task’s root set.
The root set of a task is a set of roots in memory deﬁned
to be afﬁliated with that task; for example, our implementa-
tion deﬁnes it to be the static ﬁelds of all the task’s classes
plus the execution stacks of all its threads. For each task,
the collector traces all reachable memory from its root set.
As it does so, it computes the sum of the sizes of the ob-
jects it has seen. Once the traversal is complete, that sum
is charged to the task currently being processed. Once the
collector ﬁnishes iterating over all the tasks, it makes one
ﬁnal pass, starting with the set of all roots not afﬁliated with
any particular task. Any objects which have not yet been
reached after this completes are unreachable.
2.3.1 Handling shared memory
Because each object is only processed once in each garbage
collection cycle, this method will ﬁnd less and less shared
memory as it goes from the start to the end of the list of
tasks. As indicated by Figure 3, the collector will ﬁnd all the
memory that the ﬁrst processed task shares with others, and
Proceedings of the 2003 IEEE Symposium on Security and Privacy (SP(cid:146)03) 
1081-6011/03 $17.00 ' 2003 IEEE 
Figure 3. On a pass through the garbage col-
lector, the ﬁrst task to be scanned (in this
case, task A) is charged for all memory reach-
able from it, while the last task scanned (in
this case, task C) is charged for memory
reachable only from it.
Figure 4. On subsequent invocations of the
scan, the order of scanning is rotated; task
A, the ﬁrst task scanned in the previous ex-
ample, is the last task scanned in this exam-
ple. This process gives a range of memory
usages for a task, including a maximum (all
memory reachable from the task) and a mini-
mum (memory reachable only from the task).