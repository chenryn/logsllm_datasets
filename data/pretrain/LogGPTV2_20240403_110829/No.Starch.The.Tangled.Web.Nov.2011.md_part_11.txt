completes the authentication dialog. Surprisingly, section 4.8 of the RFC pre-
dicted this risk and offered some helpful yet ultimately ignored advice:
User agents should consider measures such as presenting a visual
indication at the time of the credentials request of what authentica-
tion scheme is to be used, or remembering the strongest authenti-
cation scheme ever requested by a server and produce a warning
message before using a weaker one. It might also be a good idea
for the user agent to be configured to demand Digest authentica-
tion in general, or from specific sites.
In addition to these two RFC-specified authentication schemes, some
browsers also support less-common methods, such as Microsoft’s NTLM and
Negotiate, used for seamless authentication with Windows domain credentials.20
Although HTTP authentication is seldom encountered on the Internet,
it still casts a long shadow over certain types of web applications. For example,
when an external, attacker-supplied image is included in a thread on a mes-
sage board, and the server hosting that image suddenly decides to return
“401 Unauthorized” on some requests, users viewing the thread will be pre-
sented out of the blue with a somewhat cryptic password prompt. After double-
checking the address bar, many will probably confuse the prompt for a request
to enter their forum credentials, and these will be immediately relayed to the
attacker’s image-hosting server. Oops.
Hypertext Transfer Protocol 63
Protocol-Level Encryption and Client Certificates
As should now be evident, all information in HTTP sessions is exchanged in
plaintext over the network. In the 1990s, this would not have been a big deal:
Sure, plaintext exposed your browsing choices to nosy ISPs, and perhaps to
another naughty user on your office network or an overzealous government
agency, but that seemed no worse than the behavior of SMTP, DNS, or any
other commonly used application protocol. Alas, the growing popularity of
the Web as a commerce platform has aggravated the risk, and substantial net-
work security regression caused by the emergence of inherently unsafe pub-
lic wireless networks put another nail in that coffin.
After several less successful hacks, a straightforward solution to this
problem was proposed in RFC 2818:21 Why not encapsulate normal HTTP
requests within an existing, multipurpose Transport Layer Security (TLS, aka
SSL) mechanism developed several years earlier? This transport method lever-
ages public key cryptography* to establish a confidential, authenticated com-
munication channel between the two endpoints, without requiring any
HTTP-level tweaks.
In order to allow web servers to prove their identity, every HTTPS-enabled
web browser ships with a hefty set of public keys belonging to a variety of
certificate authorities. Certificate authorities are organizations that are trusted
by browser vendors to cryptographically attest that a particular public key
belongs to a particular site, hopefully after validating the identity of the per-
son who requests such attestation and after verifying his claim to the domain
in question.
The set of trusted organizations is diverse, arbitrary, and not particularly
well documented, which often prompts valid criticisms. But in the end, the
system usually does the job reasonably well. Only a handful of bloopers have
been documented so far (including a recent high-profile compromise of a
company named Comodo22), and no cases of widespread abuse of CA privi-
leges are on the record.
As to the actual implementation, when establishing a new HTTPS con-
nection, the browser receives a signed public key from the server, verifies the
signature (which can’t be forged without having access to the CA’s private
key), checks that the signed cn (common name) or subjectAltName fields in
the certificate indicate that this certificate is issued for the server the browser
wants to talk to, and confirms that the key is not listed on a public revocation
list (for example, due to being compromised or obtained fraudulently). If
everything checks out, the browser can proceed by encrypting messages to
the server with that public key and be certain that only that specific party will
be able to decrypt them.
Normally, the client remains anonymous: It generates a temporary encryp-
tion key, but that process does not prove the client’s identity. Such a proof
can be arranged, though. Client certificates are embraced internally by cer-
tain organizations and are adopted on a national level in several countries
* Public key cryptography relies on asymmetrical encryption algorithms to create a pair of keys: a
private one, kept secret by the owner and required to decrypt messages, and a public one,
broadcast to the world and useful only to encrypt traffic to that recipient, not to decrypt it.
64 Chapter 3
around the world (e.g., for e-government services). Since the usual purpose
of a client certificate is to provide some information about the real-world
identity of the user, browsers usually prompt before sending them to newly
encountered sites, for privacy reasons; beyond that, the certificate may act as
yet another form of ambient authority.
It is worth noting that although HTTPS as such is a sound scheme that
resists both passive and active attackers, it does very little to hide the evidence
of access to a priori public information. It does not mask the rough HTTP
request and response sizes, traffic directions, and timing patterns in a typical
browsing session, thus making it possible for unsophisticated, passive attack-
ers to figure out, for example, which embarrassing page on Wikipedia is being
viewed by the victim over an encrypted channel. In fact, in one extreme case,
Microsoft researchers illustrated the use of such packet profiling to recon-
struct user keystrokes in an online application.23
Extended Validation Certificates
In the early days of HTTPS, many public certificate authorities relied on
fairly pedantic and cumbersome user identity and domain ownership checks
before they would sign a certificate. Unfortunately, in pursuit of convenience
and in the interest of lowering prices, some now require little more than a
valid credit card and the ability to put a file on the destination server in order
to complete the verification process. This approach renders most of the cer-
tificate fields other than cn and subjectAltName untrustworthy.
To address this problem, a new type of certificate, tagged using a special
flag, is being marketed today at a significantly higher price: Extended Validation
SSL (EV SSL). These certificates are expected not only to prove domain own-
ership but also more reliably attest to the identity of the requesting party,
following a manual verification process. EV SSL is recognized by all modern
browsers by making portion of the address bar blue or green. Although hav-
ing this tier of certificates is valuable, the idea of coupling a higher-priced
certificate with an indicator that vaguely implies a “higher level of security”
isoften criticized as a cleverly disguised money-making scheme.
Error-Handling Rules
In an ideal world, HTTPS connections that involve a suspicious certificate
error, such as a grossly mismatched hostname or an unrecognized certifica-
tion authority, should simply result in a failure to establish the connection.
Less-suspicious errors, such as a recently expired certificate or a hostname
mismatch, perhaps could be accompanied by just a gentle warning.
Unfortunately, most browsers have indiscriminately delegated the
responsibility for understanding the problem to the user, trying hard (and
ultimately failing) to explain cryptography in layman’s terms and requiring
the user to make a binary decision: Do you actually want to see this page or
not? (Figure 3-1 shows one such prompt.)
Hypertext Transfer Protocol 65
Figure 3-1: An example certificate warning dialog
in the still-popular Internet Explorer 6
The language and appearance of SSL warnings has evolved through the
years toward increasingly dumbed-down (but still problematic) explanations
of the problem and more complicated actions required to bypass the warn-
ing. This trend may be misguided: Studies show that over 50 percent of even
the most frightening and disruptive warnings are clicked through.24 It is easy
to blame the users, but ultimately, we may be asking them the wrong questions
and offering exactly the wrong choices. Simply, if it is believed that clicking
through the warning is advantageous in some cases, offering to open the
page in a clearly labeled “sandbox” mode, where the harm is limited, would
be a more sensible solution. And if there is no such belief, any override capa-
bilities should be eliminated entirely (a goal sought by Strict Transport Security,
an experimental mechanism that will be discussed in Chapter 16).
66 Chapter 3
Security Engineering Cheat Sheet
When Handling User-Controlled Filenames in Content-Disposition Headers
 If you do not need non-Latin characters: Strip or substitute any characters except for alpha-
numerics, “.”, “-”, and “_”. To protect your users against potentially harmful or deceptive
filenames, you may also want to confirm that at least the first character is alphanumeric
and substitute all but the rightmost period with something else (e.g., an underscore).
Keep in mind that allowing quotes, semicolons, backslashes, and control characters
(0x00–0x1F) will introduce vulnerabilities.
 If you need non-Latin names: You must use RFC 2047, RFC 2231, or URL-style percent
encoding in a browser-dependent manner. Make sure to filter out control characters
(0x00–0x1F) and escape any semicolons, backslashes, and quotes.
When Putting User Data in HTTP Cookies
 Percent-encode everything except for alphanumerics. Better yet, use base64. Stray quote
characters, control characters (0x00–0x1F), high-bit characters (0x80–0xFF), commas,
semicolons, and backslashes may allow new cookie values to be injected or the meaning
and scope of existing cookies to be altered.
When Sending User-Controlled Location Headers
 Consult the cheat sheet in Chapter 2. Parse and normalize the URL, and confirm that the
scheme is on a whitelist of permissible values and that you are comfortable redirecting
tothe specified host.
Make sure that any control and high-bit characters are escaped properly. Use Puny-
code for hostnames and percent-encoding for the remainder of the URL.
When Sending User-Controlled Redirect Headers
 Follow the advice provided for Location. Note that semicolons are unsafe in this header
and cannot be escaped reliably, but they also happen to have a special meaning in some
URLs. Your choice is to reject such URLs altogether or to percent-encode the “;” charac-
ter, thereby violating the RFC-mandated syntax rules.
When Constructing Other Types of User-Controlled Requests or Responses
 Examine the syntax and potential side effects of the header in question. In general, be
mindful of control and high-bit characters, commas, quotes, backslashes, and semicolons;
other characters or strings may be of concern on a case-by-case basis. Escape or substitute
these values as appropriate.
 When building a new HTTP client, server, or proxy: Do not create a new implementation
unless you absolutely have to. If you can’t help it, read this chapter thoroughly and aim to
mimic an existing mainstream implementation closely. If possible, ignore the RFC-provided
advice about fault tolerance and bail out if you encounter any syntax ambiguities.
Hypertext Transfer Protocol 67
H Y P E R T E X T M A R K U P
L A N G U A G E
The Hypertext Markup Language (HTML) is the pri-
mary method of authoring online documents. One of
the earliest written accounts of this language is a brief
summary posted on the Internet by Tim Berners-Lee
in 1991.1 His proposal outlines an SGML-derived syn-
tax that allows text documents to be annotated with
inline hyperlinks and several types of layout aids. In the following years,
thisspecification evolved gradually under the direction of Sir Berners-Lee
and Dan Connolly, but it wasn’t until 1995, at the onset of the First Browser
Wars, that a reasonably serious and exhaustive specification of the language
(HTML 2.0) made it to RFC 1866.2
From that point on, all hell broke loose: For the next few years, compet-
ing browser vendors kept introducing all sorts of flashy, presentation-oriented
features and tweaked the language to their liking. Several attempts to amend
the original RFC have been undertaken, but ultimately the IETF-managed
standardization approach proved to be too inflexible. The newly formed
World Wide Web Consortium took over the maintenance of the language
and eventually published the HTML 3.2 specification in 1997.3
The new specification tried to reconcile the differences in browser
implementations while embracing many of the bells and whistles that
appealed to the public, such as customizable text colors and variable type-
faces. Ultimately, though, HTML 3.2 proved to be a step back for the clarity
of the language and had only limited success in catching up with the facts.
In the following years, the work on HTML 4 and 4.014 focused on prun-
ing HTML of all accumulated excess and on better explaining how document
elements should be interpreted and rendered. It also defined an alternative,
strict XHTML syntax derived from XML, which was much easier to consis-
tently parse but more punishing to write. Despite all this work, however, only
a small fraction of all websites on the Internet could genuinely claim compli-
ance with any of these standards, and little or no consistency in parsing modes
and error recovery could be seen on the client end. Consequently, some of
the work on improving the core language fizzled out, and the W3C turned
itsattention to stylesheets, the Document Object Model, and other more
abstract or forward-looking challenges.
In the late 2000s, some of the low-level work has been revived under the
banner of HTML5,5 an ambitious project to normalize almost every aspect
ofthe language syntax and parsing, define all the related APIs, and more
closely police browser behavior in general. Time will tell if it will be success-
ful; until then, the language itself, and each of the four leading parsing
engines,* come with their own set of frustrating quirks.
Basic Concepts Behind HTML Documents
From a purely theoretical standpoint, HTML relies on a fairly simple syntax:
a hierarchical structure of tags, name=value tag parameters, and text nodes
(forming the actual document body) in between. For example, a simple doc-
ument with a title, a heading, and a hyperlink may look like this:
Hello world
Welcome to our example page
Click me!
* To process HTML documents, Internet Explorer uses the Trident engine (aka MSHTML);
Firefox and some derived products use Gecko; Safari, Chrome, and several other browsers use
WebKit; and Opera relies on Presto. With the exception of WebKit, a collaborative open source
effort maintained by several vendors, these engines are developed largely in-house by their
respective browser teams.
70 Chapter 4
This syntax puts some constraints on what may appear inside a parame-
ter value or inside the document body. Five characters—angle brackets, sin-
gle and double quotes, and an ampersand—are reserved as the building
blocks of the HTML markup, and these need to be avoided or escaped in
some way when used outside of their intended function. The most important
rules are:
 Stray ampersands (&) should never appear in most sections of an HTML
document.
 Both types of angle brackets are obviously problematic inside a tag,
unless properly quoted.
 The left angle bracket ( directive may be used to
instruct the browser to parse the file in a manner that at least superficially
conforms to one of the officially defined standards; to a more limited extent,
the same signal can be conveyed by the Content-Type header, too. Of all the
available parsing modes, the most striking difference exists between XHTML
and traditional HTML. In the traditional mode, parsers will attempt to recover
from most types of syntax errors, including unmatched opening and closing
tags. In addition, tag and parameter names will be considered case insensi-
tive, parameter values will not always need to be quoted, and certain types of
tags, such as , will be closed implicitly. In other words, the following
input will be grudgingly tolerated:
Click me!
Hypertext Markup Language 71
The XML mode, on the other hand, is strict: All tags need to be balanced
carefully, named using the proper case, and closed explicitly. (The XML-
specific self-closing tag syntax, such as , is permitted.) In addition,
most syntax mistakes, even trivial ones, will result in an error and prevent the
document from being displayed at all.
Unlike the regular flavor of HTML, XML-based documents may also ele-
gantly incorporate sections using other XML-compliant markup formats,
such as MathML, a mathematical formula markup language. This is done by
specifying a different xmlns namespace setting for a particular tag, with no
need for one-off, language-level hacks.
The last important difference worth mentioning here is that traditional
HTML parsing strategies feature a selection of special modes, entered into
after certain tags are encountered and exited only when a specific terminator
string is seen; everything in between is interpreted as non-HTML text. Some
examples of such special tags include , , , or . In
practical implementations, these modes are exited only when a literal, case-
insensitive match on , that cannot be exited at
all; it stays in effect for the remainder of the document.)
In comparison, the XML mode is more predictable. It generally forbids
stray “”, as a way to encap-
sulate any raw text inside an arbitrary tag. For example: