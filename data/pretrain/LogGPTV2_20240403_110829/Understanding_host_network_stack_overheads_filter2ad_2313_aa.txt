title:Understanding host network stack overheads
author:Qizhe Cai and
Shubham Chaudhary and
Midhul Vuppalapati and
Jaehyun Hwang and
Rachit Agarwal
Understanding Host Network Stack Overheads
Qizhe Cai
Cornell University
Shubham Chaudhary
Cornell University
Midhul Vuppalapati
Cornell University
Jaehyun Hwang
Cornell University
Rachit Agarwal
Cornell University
ABSTRACT
Traditional end-host network stacks are struggling to keep up with
rapidly increasing datacenter access link bandwidths due to their
unsustainable CPU overheads. Motivated by this, our community is
exploring a multitude of solutions for future network stacks: from
Linux kernel optimizations to partial hardware offload to clean-slate
userspace stacks to specialized host network hardware. The design
space explored by these solutions would benefit from a detailed
understanding of CPU inefficiencies in existing network stacks.
This paper presents measurement and insights for Linux kernel
network stack performance for 100Gbps access link bandwidths.
Our study reveals that such high bandwidth links, coupled with
relatively stagnant technology trends for other host resources (e.g.,
core speeds and count, cache sizes, NIC buffer sizes, etc.), mark a
fundamental shift in host network stack bottlenecks. For instance,
we find that a single core is no longer able to process packets at line
rate, with data copy from kernel to application buffers at the receiver
becoming the core performance bottleneck. In addition, increase in
bandwidth-delay products have outpaced the increase in cache sizes,
resulting in inefficient DMA pipeline between the NIC and the CPU.
Finally, we find that traditional loosely-coupled design of network
stack and CPU schedulers in existing operating systems becomes a
limiting factor in scaling network stack performance across cores.
Based on insights from our study, we discuss implications to design
of future operating systems, network protocols, and host hardware.
CCS CONCEPTS
• Networks → Transport protocols; Network performance
analysis; Data center networks; • Hardware → Networking
hardware;
KEYWORDS
Datacenter networks, Host network stacks, Network hardware
ACM Reference Format:
Qizhe Cai, Shubham Chaudhary, Midhul Vuppalapati, Jaehyun Hwang,
and Rachit Agarwal. 2021. Understanding Host Network Stack Overheads.
In ACM SIGCOMM 2021 Conference (SIGCOMM ’21), August 23–27, 2021,
Virtual Event, USA. ACM, New York, NY, USA, 13 pages. https://doi.
org/10.1145/3452296.3472888
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-8383-7/21/08. . . $15.00
https://doi.org/10.1145/3452296.3472888
1 INTRODUCTION
The slowdown of Moore’s Law, the end of Dennard’s scaling, and
the rapid adoption of high-bandwidth links have brought tradi-
tional host network stacks at the brink of a breakdown—while
datacenter access link bandwidths (and resulting computing needs
for packet processing) have increased by 4 − 10× over the past few
years, technology trends for essentially all other host resources
(including core speeds and counts, cache sizes, NIC buffer sizes,
etc.) have largely been stagnant. As a result, the problem of design-
ing CPU-efficient host network stacks has come to the forefront,
and our community is exploring a variety of solutions, including
Linux network stack optimizations [11, 12, 21, 24, 32, 41], hardware
offloads [3, 6, 9, 16], RDMA [31, 34, 43], clean-slate userspace net-
work stacks [4, 27, 30, 33, 36], and even specialized host network
hardware [2]. The design space explored by these solutions would
benefit from a detailed understanding of CPU inefficiencies of tradi-
tional Linux network stack. Building such an understanding is hard
because the Linux network stack is not only large and complex, but
also comprises of many components that are tightly integrated into
an end-to-end packet processing pipeline.
Several recent papers present a preliminary analysis of Linux
network stack overheads for short flows [21, 30, 32, 38, 40]. This
fails to provide a complete picture due to two reasons. First, for
datacenter networks, it is well-known that an overwhelmingly large
fraction of data is contained in long flows [1, 5, 28]; thus, even if
there are many short flows, most of the CPU cycles may be spent in
processing packets from long flows. Second, datacenter workloads
contain not just short flows or long flows in exclusion, but a mixture
of different flow sizes composed in a variety of traffic patterns; as
we will demonstrate, CPU characteristics change significantly with
varying traffic patterns and mixture of flow sizes.
This paper presents measurement and insights for Linux kernel
network stack performance for 100Gbps access link bandwidths.
Our key findings are:
High-bandwidth links result in performance bottlenecks
shifting from protocol processing to data copy. Modern Linux
network stack can achieve ∼42Gbps throughput-per-core by ex-
ploiting all commonly available features in commodity NICs, e.g.,
segmentation and receive offload, jumbo frames, and packet steer-
ing. While this throughput is for the best-case scenario of a single
long flow, the dominant overhead is consistent across a variety of
scenarios—data copy from kernel buffers to application buffers (e.g.,
> 50% of total CPU cycles for a single long flow). This is in sharp
contrast to previous studies on short flows and/or low-bandwidth
links, where protocol processing was shown to be the main bottle-
neck. We also observe receiver-side packet processing to become a
bottleneck much earlier than the sender-side.
65
• Implications. Emerging zero-copy mechanisms from the Linux
networking community [11, 12] may alleviate data copy over-
heads, and may soon allow the Linux network stack to process as
much as 100Gbps worth of data using a single core. Integration
of other hardware offloads like I/OAT [37] that transparently
mitigate data copy overheads could also lead to performance
improvements. Hardware offloads of transport protocols [3, 43]
and userspace network stacks [21, 27, 30] that do not provide
zero-copy interfaces may improve throughput in microbench-
marks, but will require additional mechanisms to achieve CPU
efficiency when integrated into an end-to-end system.
The reducing gap between bandwidth-delay product (BDP)
and cache sizes leads to suboptimal throughput. Modern CPU
support for Direct Cache Access (DCA) (e.g., Intel DDIO [25]) allows
NICs to DMA packets directly into L3 cache, reducing data copy
overheads; given its promise, DDIO is enabled by default in most
systems. While DDIO is expected to improve performance during
data copy, rather surprisingly, we observe that it suffers from high
cache miss rates (49%) even for a single flow, thus providing limited
performance gains. Our investigation revealed that the reason for
this is quite subtle: host processing becoming a bottleneck results
in increased host latencies; combined with increased access link
bandwidths, BDP values increase. This increase outpaces increase
in L3 cache sizes—data is DMAed from the NIC to the cache, and
for larger BDP values, cache is rapidly overwritten before the ap-
plication performs data copy of the cached data. As a result, we
observe as much as 24% drop in throughput-per-core.
• Implications. We need better orchestration of host resources
among contending connections to minimize latency incurred
at the host, and to minimize cache miss rates during data copy. In
addition, window size tuning should take into account not only
traditional metrics like latency and throughput, but also L3 sizes.
Host resource sharing considered harmful. We observe as
much as 66% difference in throughput-per-core across different traf-
fic patterns (single flow, one-to-one, incast, outcast, and all-to-all)
due to undesirable effects of multiple flows sharing host resources.
For instance, multiple flows on the same NUMA node (thus, sharing
the same L3 cache) make the cache performance even worse—the
data DMAed by the NIC into the cache for one flow is polluted by
the data DMAed by the NIC for other flows, before application for
the first flow could perform data copy. Multiple flows sharing host
resources also results in packets arriving at the NIC belonging to
different flows; this, in turn, results in packet processing overheads
getting worse since existing optimizations (e.g., coalescing packets
using generic receive offload) lose a chance to aggregate larger
number of packets. This increases per-byte processing overhead,
and eventually scheduling overheads.
• Implications. In the Internet and in early-generation datacenter
networks, performance bottlenecks were in the network core;
thus, multiple flows “sharing” host resources did not have per-
formance implications. However, for high-bandwidth networks,
such is no longer the case—if the goal is to design CPU-efficient
network stacks, one must carefully orchestrate host resources so
as to minimize contention between active flows. Recent receiver-
driven transport protocols [18, 35] can be extended to reduce the
number of concurrently scheduled flows, potentially enabling
high CPU efficiency for future network stacks.
The need to revisit host layering and packet processing
pipelines. We observe as much as ∼43% reduction in throughput-
per-core compared to the single flow case when applications gen-
erating long flows share CPU cores with those generating short
flows. This is both due to increased scheduling overheads, and also
due to high CPU overheads for short flow processing. In addition,
short flows and long flows suffer from very different performance
bottlenecks—the former have high packet processing overheads
while the latter have high data copy overheads; however, today’s
network stacks use the same packet processing pipeline indepen-
dent of the type of the flow. Finally, we observe ∼20% additional
drop in throughput-per-core when applications generating long
flows are running on CPU cores that are not in the same NUMA
domain as the NIC (due to additional data copy overheads).
• Implications. Design of CPU schedulers independent of the net-
work layer was beneficial for independent evolution of the two
layers; however, with performance bottlenecks shifting to hosts,
we need to revisit such a separation. For instance, application-
aware CPU scheduling (e.g., scheduling applications that generate
long flows on NIC-local NUMA node, scheduling long-flow and
short-flow applications on separate CPU cores, etc.) are required
to improve CPU efficiency. We should also rethink host packet
processing pipelines—unlike today’s designs that use the same
pipeline for short and long flows, achieving CPU efficiency re-
quires application-aware packet processing pipelines.
Our study1 not only corroborates many exciting ongoing activities
in systems, networking and architecture communities on designing
CPU-efficient host network stacks, but also highlights several inter-
esting avenues for research in designing future operating systems,
network protocols and network hardware. We discuss them in §4.
Before diving deeper, we outline several caveats of our study.
First, our study uses one particular host network stack (the Linux
kernel) running atop one particular host hardware. While we fo-
cus on identifying trends and drawing general principles rather
than individual data points, other combinations of host network
stacks and hardware may exhibit different performance characteris-
tics. Second, our study focuses on CPU utilization and throughput;
host network stack latency is another important metric, but re-
quires exploring many additional bottlenecks in end-to-end system
(e.g., network topology, switches, congestion, etc.); a study that
establishes latency bottlenecks in host network stacks, and their
contribution to end-to-end latency remains an important and rel-
atively less explored space. Third, kernel network stacks evolve
rapidly; any study of our form must fix a version to ensure consis-
tency across results and observations; nevertheless, our preliminary
exploration [7] suggests that the most recent Linux kernel exhibits
performance very similar to our results. Finally, our goal is not to
take a position on how future network stacks will evolve (in-kernel,
userspace, hardware), but rather to obtain a deeper understanding
of a highly mature and widely deployed network stack.
1All Linux instrumentation code and scripts along with all the documentation
needed to reproduce our results are available at https : / / github . com /
Terabit-Ethernet/terabit-network-stack-profiling.
66
Component
Description
Data copy
TCP/IP
Netdevice sub-
system
From user space to kernel
space, and vice versa.
All the packet processing at
TCP/IP layers.
Netdevice and NIC driver op-
erations (e.g., NAPI polling,
GSO/GRO, qdisc, etc.).
skb manage-
ment
Functions to build, split, and
release skb.
Memory
de-/alloc
Lock/unlock
Scheduling
skb de-/allocation and page-
related operations.
Lock-related operations (e.g.,
spin locks).
Scheduling/context-
switching among threads.
All the remaining functions
(e.g., IRQ handling).
(cid:2)(cid:21)(cid:21)(cid:17)(cid:15)(cid:12)(cid:10)(cid:24)(cid:15)(cid:20)(cid:19)
(cid:1)
(cid:13)(cid:18)(cid:23)(cid:20)(cid:19)(cid:34)(cid:26)(cid:24)(cid:28)(cid:22)
(cid:1) (cid:5)(cid:16)(cid:33)(cid:16)(cid:1)(cid:18)(cid:29)(cid:30)(cid:38)
(cid:8)(cid:20)(cid:12)(cid:16)(cid:13)(cid:24)(cid:1)(cid:4)(cid:19)(cid:24)(cid:13)(cid:22)(cid:14)(cid:10)(cid:12)(cid:13)
(cid:1) skb (cid:27)(cid:16)(cid:28)(cid:16)(cid:22)(cid:20)(cid:27)(cid:20)(cid:28)(cid:33)
(cid:1)
(cid:1) (cid:8)(cid:20)(cid:27)(cid:29)(cid:31)(cid:38)(cid:1)(cid:16)(cid:26)(cid:26)(cid:29)(cid:18)(cid:16)(cid:33)(cid:24)(cid:29)(cid:28)(cid:39)(cid:19)(cid:20)(cid:16)(cid:26)(cid:26)(cid:29)(cid:18)(cid:16)(cid:33)(cid:24)(cid:29)(cid:28)
(cid:7)(cid:29)(cid:18)(cid:25)(cid:39)(cid:34)(cid:28)(cid:26)(cid:29)(cid:18)(cid:25)
(cid:14)(cid:2)(cid:10)(cid:29)(cid:7)(cid:10)(cid:1)(cid:10)(cid:26)(cid:25)(cid:27)(cid:25)(cid:17)(cid:25)(cid:23)(cid:1)(cid:13)(cid:27)(cid:16)(cid:17)(cid:22)
(cid:14)(cid:4)(cid:10)(cid:39)(cid:6)(cid:10)(cid:1)(cid:30)(cid:31)(cid:29)(cid:18)(cid:20)(cid:32)(cid:32)(cid:24)(cid:28)(cid:22)
(cid:1)
(cid:5)(cid:13)(cid:24)(cid:27)(cid:20)(cid:22)(cid:16)(cid:1)(cid:8)(cid:25)(cid:11)(cid:23)(cid:28)(cid:23)(cid:24)(cid:13)(cid:18)
(cid:1) (cid:9)(cid:20)(cid:33)(cid:19)(cid:20)(cid:35)(cid:24)(cid:18)(cid:20)(cid:1)(cid:32)(cid:34)(cid:17)(cid:32)(cid:38)(cid:32)(cid:33)(cid:20)(cid:27)
(cid:1) skb (cid:27)(cid:16)(cid:28)(cid:16)(cid:22)(cid:20)(cid:27)(cid:20)(cid:28)(cid:33)
(cid:3)(cid:13)(cid:26)(cid:15)(cid:12)(cid:13)(cid:1)(cid:3)(cid:22)(cid:15)(cid:26)(cid:13)(cid:22)
(cid:1) (cid:8)(cid:20)(cid:27)(cid:29)(cid:31)(cid:38)(cid:1)(cid:16)(cid:26)(cid:26)(cid:29)(cid:18)(cid:16)(cid:33)(cid:24)(cid:29)(cid:28)(cid:39)(cid:19)(cid:20)(cid:16)(cid:26)(cid:26)(cid:29)(cid:18)(cid:16)(cid:33)(cid:24)(cid:29)(cid:28)
(cid:1)
(cid:13)(cid:18)(cid:23)(cid:20)(cid:19)(cid:34)(cid:26)(cid:24)(cid:28)(cid:22)
(cid:2)(cid:5)(cid:7)(cid:4)(cid:5)(cid:8)
(cid:1)(cid:2)(cid:2)
(cid:14)(cid:11)(cid:8)(cid:13)(cid:7)
(cid:12)(cid:10)(cid:5)(cid:9)(cid:7)(cid:13)
(cid:14)(cid:2)(cid:10)(cid:29)(cid:7)(cid:10)(cid:1)(cid:13)(cid:27)(cid:16)(cid:27)(cid:19)
(cid:8)(cid:19)(cid:27)(cid:20)(cid:21)(cid:23)(cid:27)(cid:19)(cid:26)
(cid:3)(cid:1)(cid:2)
(cid:3)(cid:12)(cid:5)(cid:12)(cid:7)(cid:9)(cid:6)(cid:1)(cid:2)(cid:7)(cid:11)(cid:4)(cid:7)(cid:10)(cid:8)(cid:7)(cid:9)(cid:5)
(cid:5)(cid:13)(cid:9)
(cid:3)(cid:26)(cid:21)(cid:28)(cid:19)(cid:26)(cid:1)(cid:14)(cid:15)
(cid:1)(cid:5)(cid:3)(cid:5)(cid:6)(cid:9)(cid:5)(cid:8)
(cid:1)(cid:2)(cid:2)
(cid:11)(cid:7)(cid:4)(cid:6)
(cid:12)(cid:10)(cid:5)(cid:9)(cid:7)(cid:13)
(cid:14)(cid:2)(cid:10)(cid:29)(cid:7)(cid:10)(cid:1)(cid:13)(cid:27)(cid:16)(cid:27)(cid:19)
(cid:8)(cid:19)(cid:27)(cid:20)(cid:21)(cid:23)(cid:27)(cid:19)(cid:26)
(cid:12)(cid:10)(cid:13)(cid:29)(cid:12)(cid:4)(cid:13)
(cid:1)(cid:3)(cid:2)
(cid:7)(cid:9)(cid:1)(cid:5)(cid:2)(cid:6)(cid:4)
(cid:7)(cid:12)(cid:11)(cid:1)(cid:6)(cid:16)(cid:24)(cid:18)(cid:23)(cid:19)(cid:26)
(cid:5)(cid:14)(cid:9)(cid:13)(cid:1)(cid:4)(cid:12)(cid:6)(cid:7)(cid:9)
(cid:3)(cid:9)(cid:13)(cid:11)(cid:9)(cid:10)(cid:1)(cid:4)(cid:12)(cid:6)(cid:7)(cid:9)
(cid:4)(cid:31)(cid:24)(cid:33)(cid:24)(cid:18)(cid:16)(cid:26)(cid:1)(cid:13)(cid:20)(cid:18)(cid:33)(cid:24)(cid:29)(cid:28)(cid:1)
(cid:17)(cid:20)(cid:33)(cid:36)(cid:20)(cid:20)(cid:28)(cid:1)(cid:2)(cid:30)(cid:30)(cid:26)(cid:24)(cid:18)(cid:16)(cid:33)(cid:24)(cid:29)(cid:28)(cid:1)
(cid:16)(cid:28)(cid:19)(cid:1)(cid:6)(cid:12)(cid:11)(cid:1)(cid:4)(cid:29)(cid:28)(cid:33)(cid:20)(cid:37)(cid:33)(cid:1)
(cid:6)(cid:28)(cid:33)(cid:20)(cid:31)(cid:31)(cid:34)(cid:30)(cid:33)
(cid:12)(cid:20)(cid:16)(cid:19)(cid:1)(cid:5)(cid:16)(cid:33)(cid:16)(cid:1)(cid:10)(cid:16)(cid:33)(cid:23)
(cid:15)(cid:31)(cid:24)(cid:33)(cid:20)(cid:1)(cid:5)(cid:16)(cid:33)(cid:16)(cid:1)(cid:10)(cid:16)(cid:33)(cid:23)
(cid:10)(cid:23)(cid:38)(cid:32)(cid:24)(cid:18)(cid:16)(cid:26)(cid:1)(cid:14)(cid:31)(cid:16)(cid:28)(cid:32)(cid:27)(cid:24)(cid:32)(cid:32)(cid:24)(cid:29)(cid:28)
(cid:4)(cid:29)(cid:16)(cid:26)(cid:20)(cid:32)(cid:18)(cid:24)(cid:28)(cid:22)(cid:39)(cid:13)(cid:30)(cid:26)(cid:24)(cid:33)(cid:33)(cid:24)(cid:28)(cid:22)
(cid:32)(cid:29)(cid:18)(cid:25)(cid:20)(cid:33)(cid:1)(cid:11)(cid:34)(cid:20)(cid:34)(cid:20)
(cid:5)(cid:12)(cid:2)(cid:8)(cid:1)(cid:4)(cid:24)(cid:31)(cid:18)(cid:34)(cid:26)(cid:16)(cid:31)(cid:1)(cid:3)(cid:34)(cid:21)(cid:21)(cid:20)(cid:31)
(cid:2)(cid:6)(cid:13)(cid:8)(cid:15)(cid:6)(cid:13)(cid:9)
Others
Figure 1: Sender and receiver-side data path in the Linux network stack. See §2.1 for description.
Table 1: CPU usage taxonomy. The compo-
nents are mapped into layers as shown in Fig. 1.
2 PRELIMINARIES
The Linux network stack tightly integrates many components into
an end-to-end pipeline. We start this section by reviewing these
components (§2.1). We also discuss commonly used optimizations,
and corresponding hardware offloads supported by commodity
NICs. A more detailed description is presented in [7]. We then
summarize the methodology used in our study (§2.2).
2.1 End-to-End Data Path
The Linux network stack has slightly different data paths for the
sender-side (application to NIC) and the receiver-side (NIC to ap-
plication), as shown in Fig. 1. We describe them separately.
Sender-side. When the sender-side application executes a write
system call, the kernel initializes socket buffers (skbs). For the data
referenced by the skbs, the kernel then performs data copy from the
userspace buffer to the kernel buffer. The skbs are then processed
by the TCP/IP layer. When ready to be transmitted (e.g., conges-
tion control window/rate limits permitting), the data is processed
by the network subsystem; here, among other processing steps,
skbs are segmented into Maximum Transmission Unit (MTU) sized
chunks by Generic Segmentation offload (GSO) and are enqueued
in the NIC driver Tx queue(s). Most commodity NICs also support
hardware offload of packet segmentation, referred to as TCP seg-
mentation offload (TSO); see more details in [7]. Finally, the driver
processes the Tx queue(s), creating the necessary mappings for the
NIC to DMA the data from the kernel buffer referenced by skbs.
Importantly, almost all sender-side processing in today’s Linux
network stack is performed at the same core as the application.
Receiver-side. The NIC has a number of Rx queues and a per-Rx
queue page-pool from which DMA memory is allocated (backed by
the kernel pageset). The NIC also has a configurable number of
Rx descriptors, each of which contains a memory address that the
NIC can use to DMA received frames. Each descriptor is associated
with enough memory for one MTU-sized frame.
Upon receiving a new frame, the NIC uses one of the Rx descrip-
tors, and DMAs the frame to the kernel memory associated with the
descriptor. Ordinarily, the NIC DMAs the frame to DRAM; however,
modern CPUs have support for Direct Cache Access (DCA) (e.g.,
using Intel’s Data Direct I/O technology (DDIO) technology [25])
that allows NIC to DMA the frames directly to the L3 cache. DCA
enables applications to avoid going to DRAM to access the data.
Asynchronously, the NIC generates an Interrupt ReQuests (IRQ)
to inform the driver of new data to be processed. The CPU core that
processes the IRQ is selected by the NIC using one of the hardware
steering mechanisms; see Table 2 for a summary, and [7] for details
on how receiver-side flow steering techniques work. Upon receiving
an IRQ, the driver triggers NAPI polling [17], that provides an
alternative to purely interrupt-based network layer processing—the
system busy polls on incoming frames until a certain number of