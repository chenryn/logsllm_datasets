sent by the BGP1 process on router F . The ﬂow of routing
information from F to E and the resulting ﬂow of data trafﬁc
from E to F is represented by the edge from E.1O to F.1I
in Figure 3. There is a similar edge from F.1O to E.1I, be-
cause routing information also ﬂows from E to F and may
result in the ﬂow of data trafﬁc from F to E.
Unlike routes computed by IGP and BGP processes, static
routes are not based on advertisements from a speciﬁc neigh-
boring process. Thus, we connect a static route’s out vertex
to the in vertices for all processes on the next hop device—
i.e., the device with an interface whose IP address matches
the next hop IP speciﬁed in the static route.
Intra-device edges. The ETG also contains edges between
vertices associated with routing processes running on the
same device to model the ﬂow of data trafﬁc resulting from
route redistribution and route selection.
When a routing process (redistributor) is conﬁgured to re-
distribute routes into another process (redistributee) on the
same device, we connect the redistributee’s in vertex to the
redistributor’s out vertex; similar to above, this intra-device
edge points in the opposite direction that routing informa-
tion ﬂows. For example, the redistribution of routes from
routing instance OSPF2 to OSPF3 is represented by the edge
from B.3I to B.2O in Figure 3. Similarly, we connected a
process’s in vertex to its own out vertex to model a routing
process’s role in propagating routes. For example, the route
propagation performed by the process for OSPF3 on B is
represented by the edge from B.3I to B.3O in Figure 3.
When multiple routing processes on a device have a route
to the destination, the route from the process with the lowest
administrative distance (AD) is used to reach the destination.
The aforementioned intra-device edges already capture cases
where the lowest-AD process with a route to the destination:
(1) redistributes routes into higher-AD processes, or (2) ad-
vertises a route to the destination to processes on neighbor-
ing devices. However, we must also model the case where a
higher-AD process (H) advertises a route to the destination
to processes on neighboring devices, but the lower AD pro-
cess’s (L) route will be used at the device in question to reach
the destination. This situation only occurs when both H and
L have independently learned routes to the destination (i.e.,
there is no route redistribution). We model this case by con-
necting H’s in vertex (HI ) to L’s out vertex (LO), assuming
there exists at least one path from HI to DST and LO to DST
prior to the addition of this edge.
Endpoint edges. Edges are added from the SRC vertex to a
routing process’s out vertex if the device on which the pro-
cess runs can be directly reached by the source endpoint(s)
using layer-2 forwarding: e.g., SRC → A.2O in Figure 3.
Similarly, edges are added from a routing process’s in vertex
to the DST vertex if the device on which the process runs can
directly reach the destination endpoint(s): e.g., F.1I → DST.
For trafﬁc classes whose source endpoint is external, we add
an edge from SRC to the out vertices of all processes that
send external route advertisements; we add similar edges for
external destinations.
Factoring in ACLs and route ﬁlters. Data plane ACLs pre-
vent particular classes of trafﬁc from entering or leaving a
router. Similarly, route ﬁlters prevent a routing process from
advertising particular preﬁxes to a process on another device,
or a process on the same device through route redistribution.
To account for these ﬁltering mechanisms, we prune some
edges from the ETG. In particular, we prune an inter-device
edge if:
(i) there is an outgoing or incoming data plane
ACL conﬁgured on the interfaces associated with the phys-
ical link(s) the edge represents, and (ii) the ACL blocks the
trafﬁc class associated with the ETG. We also prune an inter-
device edge if a route ﬁlter that blocks the trafﬁc class’s des-
tination preﬁx has been applied to the process whose out ver-
tex is incident with the edge. Similarly, we prune an intra-
device edge if a route ﬁlter that blocks the trafﬁc class’s des-
tination preﬁx is applied to routes redistributed by the pro-
cess whose out vertex is incident with the edge.
4.2.3 Pathset-equivalence
We now prove the above methodology results in pathset-
equivalent ETGs. We ﬁrst show that a path-equivalent ETG
is also pathset-equivalent. (Our technical report [11] con-
tains proofs of ETG path-equivalence.)
THEOREM 1. A path-equivalent ETG is pathset-equivalent.
PROOF. Let P be the min-cost path in the ETG from SRC to
DST under some failure. Now assume the actual network has
a more preferred path P ′ between the source and destination,
but P ′ does not exist in the ETG. Because P ′ does not exist
in the ETG, the min-cost path in the ETG is incorrect. This
contradicts the assumption that the ETG is path-equivalent.
Thus, a path-equivalent ETG must contain every path taken
by the actual network under all possible failures.
Now assume the ETG contains a path P ′′ from SRC to
DST which is infeasible in the actual network. Also assume
all edges not on the path have been removed due to failures.
The only, and hence min-cost, path through the ETG will
be P ′′. Because P ′′ is infeasible in the actual network, the
min-cost path in the ETG is incorrect. This contradicts the
assumption that the ETG is path-equivalent. Thus, a path-
equivalent ETG must not contain any paths that are infeasi-
ble in the actual network.
For some route redistribution policies we cannot generate
a path-equivalent ETG (details in §4.3). However, we can
still generate a pathset-equivalent ETG.
THEOREM 2. An ETG is pathset-equivalent when routes are
redistributed between OSPF, RIP, and/or eBGP instances.
We refer readers to our technical report [11] for the proof.
4.3 ARC Edge Weights
While pathset-equivalent ETGs are sufﬁcient for verify-
ing many important invariants, such as I1–I4 in Table 1,
path-equivalent ETGs are required for generating counterex-
amples or testing equivalence (I5). The key challenge in
constructing path-equivalent ETGs is determining the ap-
propriate edge weights such that the min-cost path through
the ETG matches the actual path taken in the network un-
der arbitrary failures. Next, we describe how to assign such
weights to different types of edges.
In our technical re-
port [11], we prove the resulting ETGs are path-equivalent.
4.3.1 Endpoint edges
When assigning weights to endpoint edges, we must con-
sider the route selection policies of the devices to which the
source and destination endpoints are connected.
Source edges. When the source is connected to a device
with one routing process, then the best route (if any) com-
puted by that process is always used. Thus, the edge from
SRC to the process’s out vertex is assigned a weight of 0.
If the device has multiple routing processes, then a route
computed by a process with a lower AD is preferred over a
route computed by a process with a higher AD. We model
this by assigning edge weights proportional to a process’s
AD. The weight of an edge from SRC to r .iO is set to
ADi ∗ max
i ′∈Ir  Xe∈Ei ′
we!
(1)
where ADi is the AD for routing instance i, Ir is the set of
routing instances in which router r participates, Ei ′ is the
set of edges originating from the in and out vertices for the
routing processes in instance i ′, and we is the weight as-
signed to edge e. This ensures the cost of a path originating
at a process with a higher AD is always more expensive than
the longest possible path through a routing instance whose
process has a lower AD.
Destination edges. When the destination is directly con-
nected to a device, the device always sends trafﬁc directly to
the destination; no other route is ever preferred. Thus, edges
to DST are assigned a weight of 0.
4.3.2
Inter-device edges for IGPs
For inter-device edges connecting RIP or single-area OSPF
processes, we directly assign the cost metric speciﬁed in
the device conﬁgurations.
If no cost is explicitly deﬁned,
we assign the DEFAULT-RIP-COST or DEFAULT-OSPF-
COST deﬁned by the device vendor. For example, edges
A.2O → B.2I and B.2O → A.2I in Figure 3 are assigned
the OSPF cost conﬁgured on the A − B link in Figure 1b.
This methodology, along with the methodology described
in §4.3.4 and §4.3.5, also allows us to identify the multiple
min-cost paths that are used when ECMP is enabled.
4.3.3
Inter-device edges for eBGP
eBGP processes inside the network. We model the primary
path selection criterion used by eBGP for computing paths:
AS path length. We do not model other path selection crite-
ria, e.g., local preference, because: (1) it is not possible to
statically assign edge weights such that local decisions (e.g.,
local preferences) override global cost computations (e.g.,
route selection based on AS path length) without compro-
mising our modeling of the global cost computations; and
306
(2) local preference is not widely used to select between pri-
vate ASes within enterprise and data center networks (§7.1).
In the absence of iBGP (which we show in §7.1 is not
used in the networks we study), each autonomous system
(AS) can only have a single eBGP speaker (i.e., process)
that is directly connected to the eBGP speakers of neigh-
boring ASes.5 Thus, the length of an AS path is simply
the number of eBGP processes traversed. We capture this
by assigning a weight of 1 to inter-device edges connecting
eBGP processes. For example, edges F.1O → E.1I and
E.1O → D.1I in Figure 3 are assigned weight 1.
eBGP processes outside the network. We cannot precisely
model paths that depend on advertisements from external
eBGP processes, because we do not know the length of paths
advertised. As discussed in §4.2.2, if the source or destina-
tion is external, we simply add an edge to/from the out/in
vertex, respectively, of every eBGP process inside the net-
work that peers with an eBGP process outside the network.
Edge weights are assigned as described in §4.3.1.
4.3.4
Intra-device edges for route redistribution
As discussed in §4.2.2, route redistribution is modeled via
intra-device edges connecting the in vertex of one routing
process (the redistributee) to the out vertex of another pro-
cess (the redistributor). When assigning weights to these
edges, we must consider the ﬁxed costs assigned to redis-
tributed routes. Routing processes within the redistributee’s
routing instance use these ﬁxed costs, along with the costs
assigned to links within the instance, to determine the for-
warding path through the instance.
Modeling route redistribution is difﬁcult, because paths
through the ETG are computed globally and include edge
weights associated with multiple routing instances; whereas
in an actual network the path to the destination through the
redistributor’s routing instance (denoted by i) does not af-
fect forwarding within the redistributee’s routing instance
(denoted by j). To ensure an ETG is path-equivalent, we:
(1) rescale the weights of edges in the ETG such that even
the longest path through routing instance i is less than the
minimum difference of path lengths within instance j, and
(2) assign ﬁxed costs to the intra-device edges representing
redistribution in accordance with the redistributee instance’s
scaling factor.
Constraints. To produce a path-equivalent ETG for a net-
work leveraging route redistribution, the control plane must
satisfy two constraints. First, route redistribution must be
acyclic—i.e., a routing instance’s routes are not redistributed
back to itself. Prior work has shown that cyclic route redis-
tribution is fragile [19], so data center and enterprise net-
works tend to use only acyclic route redistribution. We can
check if a network’s route redistribution policy is acyclic
by constructing a graph with one vertex for each routing
instance and directional edges between instances in the di-
5An AS without iBGP can have multiple eBGP speakers, but
each eBGP speaker can only compute paths through ASes
with which it has a direct connection; different eBGP pro-
cesses in the same AS cannot directly exchange routes.
rection routes are redistributed (the inverse of intra-device
edges in the ETG).
Second, the ﬁxed costs assigned to redistributed routes
must be congruent with the ADs assigned to the redistrib-
utor(s). More formally, if ADi < ADi′ < .., then it must
be the case that ci,j < ci ′,j < ..., where ci,j is the ﬁxed cost
assigned to routes redistributed from routing instance i to
instance j. This constraint ensures we accurately model the
fact that a route is redistributed only when: (1) the redistrib-
utor is the only routing process on the device that has a route
to the destination, or (2) the redistributor has the lowest AD
among the processes that have a route to the destination.
Scaling edge weights. We scale the weights of all edges
between vertices corresponding to the redistributor’s routing
instance (i)6 by a scaling factor fi. The scaling factor is com-
puted using the equation on line 15 of Algorithm 1. C is the
set of routing instances into which i redistributes routes; the
scaling factor fi must be less than the scaling factor for all
j ∈ C. The term gj denotes the minimum non-zero differ-
ence in cost between any two acyclic paths between any two
vertices corresponding to routing processes associated with
routing instance j. We can conservatively compute gj by
ﬁnding the greatest common divisor (GCD) of the unscaled
edge weights corresponding to edges associated with rout-
ing instance j. For example, the GCD for edges associated
with routing instance BGP1 in Figure 3 is 1. The denomina-
tor in the equation for fi represents the maximum possible
path length through routing instance i. The scaling factor
for a routing instance can only be obtained after we have
determined the scaling factor for all instances into which it
redistributes routes (C); if |C| = 0, the scaling factor is 1.
Consider the control plane shown in Figure 1b and the
corresponding ETG in Figure 3. Assume the G–E link has
failed. A packet from S → T can either be forwarded
through router D or through router Z via the OSP F3 rout-
ing instance. In the absence of link failures, the path through
OSP F3 will be chosen by BGP1, because the hop count to
the destination is 1, as opposed to a hop count of 2 through
router D. However, the shortest path through the ETG with
unscaled weights is: SRC → F.1O → E.1I . . . D.1I →
D.2O . . . A.2O → DST; this path is chosen because of the
higher cost of the path through OSP F3. However, by scal-
ing weights using the computed scaling factors, we ensure
that no path to the destination through router D is shorter
than a path through router Z, because weights on edges in
BGP1 dominate overall cost.
4.3.5
Intra-device edges for route selection
While the above methodology applies to routers whose
processes engage in route redistribution, we need a slightly
different approach for routers where multiple routing pro-
cesses have a route to the destination but there is no route
redistribution—i.e., there is route selection without route re-
distribution. Thus, after the above methodology has been
applied, we apply the methodology described below.
6Including intra-device edges corresponding to route redis-
tribution from another routing instance k to instance i.
307
Algorithm 1 Procedure to determine scaling factors
Input:
I: the set of all routing instances in the network
≤I : the partial order over I determined by route redist.
Output:
fi, hi, ∀i ∈ I: the scaling factors for all routing instances
∀i ∈ I
for all i ∈ Ir do
⊲ Init. scaling factors
⊲ Init. partial order
⊲ Route redistribution rescaling §4.3.4
⊲ Route selection
⊲ rescaling §4.3.5
1: fi = 1, hi = ∞,
2: ≤ = ≤I
3: RESCALE(I, ≤I )
4: for all routers, r, with no route redist. do
5:
6:
7:
8:
9:
10:
11:
if AD(i′) < AD(i) then
≤ = ≤ ∪ (i′, i)
D = DOWNSTREAMINST(i′, I, ≤)
RESCALE(D, ≤I )
hi = figi
1+|I|
for all i′ ∈ Ir do
12: procedure RESCALE(T , ≤)
13:
14:
C = getChildren(i, ≤)
min(hj , fj gj)
for all i ∈ reverseT opologicalSort(T, ≤) do
fi = min
j∈C
1 + X
e∈Ei
we
15:
16:
17:
if hi < ∞ then
hi = figi
1+|I|
⊲ Update hi based on new fi
D = {i}
for all j ∈ I do
18: function DOWNSTREAMINST(i, I, ≤)
19:
20:
21:
22:
23:
if (j, i) ∈ ≤ then
D = D ∪ j
return D
Consider the example control plane in Figure 1b (sans
ACLs) and the corresponding ETG in Figure 3. There is
no route redistribution on router E. However packets arriv-
ing on E destined to T have two possible paths: (1) through
router D, and (2) through routers G and H via the routes
learned through OSP F0. Under the scenario where there
are no failures, the path through OSP F0 will never be taken,
because OSP F0’s routes are not redistributed within BGP1.
However, if the F –Z link fails, F will forward packets to E
along the two hop path F → E → D to the destination; at
E, the process with the lowest AD (OSP F0) dominates and
the packets will be forwarded along G and H, ignoring the
path computed by the BGP1 process on E.
To ensure that any path through router E is more expen-
sive than a path through OSP F3 and less expensive than
the path through router D, weights of the intra-device route
selection edges are obtained by summing two components:
w = fi(cid:18) min
b∈Bi∪{DST}
disti(r, b)(cid:19) + hi ∗ rank(i′)
(2)
The ﬁrst component is the minimum cost path from the cur-
rent router (r) to a border router with a path to the destina-
tion (b ∈ Bi), or to the destination itself; we include the cost
of intra-device route redistribution edges at a border router.
For example, in the BGP1 routing instance, there are two
308
Pathset Path-equivalent
Construct
yes
OSPF
yes
RIP
yes
eBGP
yes
Static routes
yes
ECMP
yes
ACLs
yes
Route ﬁlters
Route redistrib. yes
Route selection yes
single area
yes
select only by AS path length
yes
yes
yes
yes
acyclic + costs & ADs align
instance’s processes use same AD
Table 2: Control plane constructs modeled in ARC
border routers with a path to the destination: D and Z; the
path from E to D is shorter (1 versus 2). This ensures that
the path from SRC to DST through router E is greater than
a path through OSP F3. The second component is used to
distinguish between different processes on the current router
(r), each with a route to DST, based on AD’s. The routing
instance to which the route selection edge points is denoted
by i′ and rank(i′) is the rank of routing instance i′ based
on its AD. The example has only one alternate process, E.0,
whose rank is 1 (the process E.1 has rank 2). The scaling
factor hi ensures the path through OSP F0 is lower-cost than
the path through router D; hi is computed using the equation
on line 6 of Algorithm 1.
After hi’s are obtained for all the routing instances, the
downstream routing instances7 need to be re-scaled such that
the weights on the intra-device route selection edges domi-
nate the costs of the routing instances to which they point.
Thus, we update fi using the equation on line 15 of Algo-
rithm 1. Upon recomputing fi, the scaling factors for all
downstream instances have to be recomputed.
Algorithm 1 provides an overview of the steps involved in
computing the scaling factors for all routing instances.
It
assumes that for a pair of routing instances, i and i′, the
relative AD ordering is the same in all routers, and if a route
selection edge from a process in i to a process in i′ exists,
then i does not redistribute routes to i′. This restriction is
required to avoid any cyclical dependency between instances
in terms of re-scaling.
Table 2 summarizes the protocols and features for which
an ETG is pathset- and path-equivalent. A network with
any combination of these constructs results in ETGs that are
pathset-equivalent and, if the listed constraints are met, path-
equivalent. In the next section, we describe how to use a
network’s ARC for veriﬁcation and equivalence testing.
5. USING ARC
ARC enables us to check important invariants across ar-
bitrary failure scenarios.
It is particularly well suited for
verifying invariants that pertain to properties of a path. In
such cases, veriﬁcation/equivalence testing is a matter of
(dis)proving that an ETG, or a pair of ETGs for different
control planes, has a speciﬁc graph-level characteristic. This
section describes our veriﬁcation and equivalence testing al-
gorithms that at their essence compute such graph character-
istics. Furthermore, we describe how to use precise ETGs
7downstream routing instances refer to possible instances
encountered on a path to the destination (line 18 in Alg. 1).