P(ri|r1r2 . . .ri−1) = P(ri),
where P(ri) is also trained on a training set.
For the Golla et al.’s model [14] of password vaults, a gen-
erating rule is a character or a value of |Vi| for 0 ≤ i ≤ 4. A
valid generating sequence of a vault consists of the following
rules: 1) characters of the base password, 2) |Vi|, 3) modiﬁed
characters of passwords in Vi and 4) characters of passwords
in V5. In this case, the conditional probabilities of characters
are calculated as the Markov model and |Vi| is calculated by
normal distributions.
For the Chatterjee et al.’s model [10] of password vaults, a
generating rule is a production rule of the PCFG or a number
of production rules with a certain lefthand-side in a vault, a
valid generating sequence contains a generating sequence of
a sub-grammar and leftmost derivations of passwords based
on the sub-grammar. More speciﬁcally, a valid generating se-
quence of the sub-grammar {S→ D, S→ W, D→ 123456, W→
password} is (#S = 2, S → D, S → W, #D = 1, D → 123456,
#W = 1, W → password). The rule #X = i represents that there
are i rules with the lefthand-side X in sub-grammar, it is for
the sake of the preﬁx-free property of R S. The conditional
probability of the rule #X = i only depends on the rule itself,
denoted as P(#X = i), which is trained on a password vault
dataset (Pastebin). The conditional probability of the rule
X → str is the same as that in PCFG models.
For Huang et al.’s models [18] for genomic data, a gener-
ating rule is a character of {0,1,2} (representing an SNV), a
valid generating sequence is a string. The conditional prob-
ability of a rule relies on the genomic data model: for the
uniform distribution model, it is equal to 1
3 for each rule; for
the public LD model (as discussed above), it depends on the
last rule; for Markov model of order n, it depends on the last
n rules; for the recombination model, it is calculated by the
forward-backward algorithm with a hidden Markov model.
Up to this point, the existing models are all formalized
with our proposed GPMs and the distributions of generating
sequences are deﬁned by the conditional distributions of gen-
erating rules. Beyond that, more probability models can be
formalized. For example, neural networks for passwords [27]
can be formalized as the same as Markov models except that
condition probabilities are calculated by neural networks.
4.3 Generating Graphs
To represent a GPM visually, we propose a generating graph,
which is a connected directed acyclic graph with a single
source and with edges labeled by generating rules. In a gen-
erating graph, a generating sequence is illustrated by a path
whose edges denote the corresponding generating rules in
order. Moreover, a message is ﬁgured by a sink (because the
generating sequence space is preﬁx-free) and a path from
the source to the sink illustrates one generating sequence of
the message. Hence, the path is called a generating path of
the message. Note that the generating graph of a model is an
arborescence, if and only if the model is unambiguous. (Note
an arborescence is a directed graph in which there is only one
single source and each other vertex has only one directed path
from the source.)
As shown in Figure 2, in Chatterjee-PCFG model, there are
two generating paths for “password”. These two generating
paths correspond to two generating sequences: {S → W, W →
1578    28th USENIX Security Symposium
USENIX Association
Algorithm 3: The weight function pWEA (= pEC) of
the weak encoding attack
1 function pWEA(S)
2
3
4
5
6
7 end
Input: a seed S.
Output: the weight of S (for sorting in Algorithm 1).
Obtain the generating sequence RS and the message M of S by
decoding S
S(cid:48) ← encode(M) /* Since encode is a randomized
algorithm, S(cid:48) is probably not equal to S.
Obtain the generating sequence RS(cid:48) of S(cid:48) by decoding S(cid:48)
if RS = RS(cid:48) then return 1 /* S may be a real seed. */
else return 0
/* S is definitely a decoy seed. */
*/
Figure 2: Generating graph of Chatterjee-PCFG
password} and {S → WW, W → pass, W → word}. Further, the
probability of the ﬁrst sequence is 0.2× 0.1 = 0.02 and that
of the second one is 0.1×0.02×0.01 = 0.00002. This makes
the probability of “password” be 0.02 + 0.00002 = 0.02002.
Since “password” has two generating sequences, Chatterjee-
PCFG model is ambiguous.
4.4 The Principle of Encoding Attacks
The features used by encoding attacks in Section 3 are all
based on heuristic analyses of speciﬁc PMTEs. Some other
features are still neglected due to the lack of a systematic
analysis. For example, on Chatterjee et al.’s password vault
PMTE [10], the order of rules in the sub-grammar is determin-
istic for real vaults, but not for decoy seeds. When encoding
the vault V = (123456,password), the ﬁrst two rules in the
sub-grammar are S → D, S → W in order. But if the vault V is
decoded by a decoy seed, the ﬁrst two rules may be S → W,
S → D in a different order from the real vault.
Fortunately, with the formalizations by GPMs and the vi-
sual representations by generating graphs, the principle of
encoding attacks is uncovered: existing PMTEs neglect the
ambiguity of GPMs. More speciﬁcally, in an ambiguous GPM,
there may exist multiple generating paths for a message, but
the existing PMTEs only select one deterministic path when
encoding. We name these paths encoding paths which can
be selected when encoding and meanwhile name these cor-
responding generating sequences encoding sequences. The
generating sequence of a seed can be obtained by decoding
the seed. Due to the determinacy of encoding paths, encod-
ing attacks can exclude some decoy seeds by checking if the
generating path of a seed is an encoding path, without any
information of the real message distribution.
We then take Chatterjee et al.’s PMTE [10] for Chatterjee-
PCFG as an example. As shown in Figure 2, this PMTE only
uses the blue dotted path when encoding “password”, but
the generating path of a decoy seed may be the red dashed
one. In fact, Chatterjee et al. [10] noticed the ambiguity
of Chatterjee-PCFG and brieﬂy mentioned that the PMTE
needs to choose one parse tree randomly in all parse trees
when encoding. However, in Chatterjee et al.’s code [10],
they have not implemented the random selection method un-
til now (June 1, 2019) and only one parse tree is selected
when encoding. Moreover, Chatterjee et al. [10] completely
neglected the ambiguity of the sub-grammar approach. For
example, a vault V = (123456,password) is encoded only
with the sub-grammar SG = {S → D, S → W, D → 123456,
W → password}, but V can be generated by multiple sub-
grammars as long as they contain SG. Therefore, the encoding
paths deﬁnitely have feature UR while other generating paths
may not.
3 and |V1| = 1
Similarly, Golla et al. [14] also did not consider the ambigu-
ity of the reuse-rate approach. For example, V = (password1,
password1,password@) can be generated by “password1” as
the base password with reuse-rates |V0| = 2
3. It
also can be generated by “password1” as the base password
with reuse-rates |V0| = 1
3. In addition, Golla et
al.’s GPMs [14] allow modifying the character of the base
password to the same character. Therefore, “password@” may
be in V2 (with “@” modiﬁed from “1” and “d” modiﬁed from
itself). This brings ambiguity to the GPM, i.e., a huge num-
ber of generating paths for a vault. Only one deterministic
path (the ﬁrst one for V ) is chosen when encoding. Therefore,
the encoding paths deﬁnitely have feature ED while other
generating paths may not.
3 and |V1| = 2
Any feature utilized by any encoding attack, including fea-
tures proposed in Section 3.3, the rule-order feature or the
base-password feature discussed above, can be seen as a fea-
ture of encoding paths.
4.5 Generic Encoding Attacks
Due to the determinacy of encoding paths, we further propose
two generic encoding attacks—a weak encoding attack and a
strong encoding attack.
The weak encoding attack is accordance with feature EC
(encoding consistency) that the generating path is an encoding
path, i.e., the weight function pWEA = pEC. We use the ab-
breviation of the attack as the subscript of p for convenience.
More speciﬁcally, pWEA (i.e., whether a seed S has feature
USENIX Association
28th USENIX Security Symposium    1579
SD......WWW..................a0.002(0.2×0.01)......password0.02002(0.2×0.1+0.1×0.02×0.01)passW............S→D0.1S→W0.2S→WW0.1W→a0.01W→password0.1W→pass0.02W→word0.01EC) can be calculated as Algorithm 3.
In contrast to the feature attacks (proposed in Section 3.3)
based on some features of encoding path, the weak encoding
attack is based on feature EC. Therefore, the seeds having
feature EC certainly have other features proposed in Section
3.3. In other words, the weak encoding attack excludes all
decoy vaults which are excluded by any feature attack.
As the seeds with feature EC are sorted randomly by the
weak encoding attack, we propose a strong encoding attack
to sort them. Let RS denote the generating sequence of the
seed S, then the weight function pSEA is deﬁned as
pSEA(S) =
1
P(RS) × pWEA(S).
4.6 Efﬁciency of Encoding Attacks
These two generic encoding attacks are efﬁcient for PMTEs
with signiﬁcantly ambiguous GPMs and deterministic encod-
ing paths, such as all existing PMTEs for password vaults. In
other words, these attacks recover the encrypted real vaults
with a high probability but a small number of online veriﬁca-
tions. To make it clear, the weak encoding attack excludes the
seeds whose generating paths are not encoding paths, e.g., the
red dashed path in Figure 2. Namely, the excluded proportion
of the weak encoding attack is equal to the total probability of
all generating paths except encoding paths. This means that
the more ambiguous the GPM is, the more efﬁciency the weak
encoding attack can achieve. As discussed in Section 4.4, in
the existing GPMs for password vaults [10, 14], every vault
has countless generating paths. Due to the great ambiguity
of these GPMs, the weak encoding attack is efﬁcient for the
corresponding existing PMTEs with deterministic encoding
paths. On the other hand, if a GPM is unambiguous (e.g.,
the models of genomic data [18]), the PMTE for it can re-
sist encoding attacks naturally. Besides, the strong encoding
attack excludes all decoy seeds which are excluded by the
weak encoding attacks. Therefore, the strong encoding attack
is always more efﬁcient than the weak encoding attack.
5 Probability Model Transforming Encoders
We propose a generic transforming method which transforms
an arbitrary GPM to a secure PMTE. Further, we give a formal
proof that the PMTE transformed by our method is indistin-
guishable from the GPM.
5.1 Conditional DTEs
Inspired by the way Chatterjee et al.’s PMTEs [10] encoding
password character by character or rule by rule, we propose
a fundamental concept of PMTE—conditional distribution
transforming encoder (CDTE)—to encode message rule by
rule. A DTE is an encoder transformed from a probability
distribution, while a CDTE is an encoder transformed from a
conditional probability distribution. Unlike a DTE, a CDTE
needs not only the message M but also the condition X to
encode M (denoted as encode(M|X)) by the conditional prob-
ability distribution P(·|X). It also needs the condition X to
decode the seed S (denoted as decode(S|X)). In this aspect,
for every condition X, the CDTE (encode(·|X), decode(·|X))
is a DTE. Interestingly, if the condition X and the message M
are mutually independent (i.e., the conditional probability dis-
tribution P(·|X) is the same for every condition X), a CDTE
degenerates into a DTE. Therefore, we state that DTEs can
be seen as a special case of CDTEs. Juels and Ristenpart [21]
proposed a generic method to transform a distribution to a
DTE and named the DTE IS-DTE. For the general conditional
distribution, we get a DTE IS-DTEX for each condition X by
means of Juels-Ristenpart method and thus we give a general
CDTE scheme IS-CDTE by the combination {IS-DTEX}X.
In the following, we give the details of our IS-CDTE. Let
X denote the condition, X denote the condition space, and
MX = {Mi}i denote the message space under the condition X.
The corresponding conditional probability is P(Mi|X), and
the cumulative distribution function is Fi = ∑i
i(cid:48)=1 P(Mi(cid:48)|X).
When encoding the message M under the condition X, the
IS-CDTE randomly generates a real number S in the interval
[Fi−1,Fi) as a seed of M. When decoding the seed S under
condition X, the IS-CDTE searches the interval [Fi−1,Fi) con-
taining S and then outputs the corresponding message Mi.
Encoding or decoding only requires a binary search of the
corresponding CDF (cumulative distribution function) table
{(Mi,Fi)}i under the condition. Therefore, the space complex-
ity and the time complexity of the IS-CDTE are O(|X|·|M |)
and O(log(|M |)), respectively.
For implementing with encryption, real-number seeds are
usually represented as bit strings of length l, i.e., integers
in [0,2l), where l is a storage overhead parameter. IS-DTEs
use the function roundl(x) converting a real-number seed
to an integer seed, where roundl(x) = round(2lx) and round
represents rounding function. We use the same method for
IS-CDTEs. In such case, the integer seed interval of Mi is
[round(2lFi−1), round(2lFi)). Hence, to ensure that each mes-
sage has at least one integer seed, l must be greater than
or equal to −log2(mini P(Mi|X)). The loss of precision by
the discretization with roundl causes a slight difference be-
tween these two conditional distributions PrIS-CDTE(M|X) =
Pr[M = M(cid:48) : S ←$S;M(cid:48) ← decode(S|X)] and P(M|X), where
IS-CDTE = (encode(·|·), decode(·|·)). Fortunately, the dif-
ference is negligible in l (see Theorem 4). For convenience,
we let P(d) denote the discretization PrIS-CDTE of P.
5.2 Probability Model Transforming Encoder
Combining IS-CDTEs for the conditional distributions of gen-
erating rules, we present a PMTE for the messages, which we
call an IS-PMTE. Let l denote the storage overhead parameter,
1580    28th USENIX Security Symposium
USENIX Association
P(d)(RS)
.
P(d)(RS(cid:48))
4. Encode each rule ri in RS = (ri)i by the IS-CDTE
then the IS-PMTE encodes the message M as follows:
1. Parse M and get all generating sequences G−1(M).
2. Calculate the probability P(d)(RS) for each generat-
ing sequence RS in G−1(M), where P(d)(r1r2 . . .rn) =
∏n
i=1 P(d)(ri|r1r2 . . .ri−1) and P(d)(ri|r1r2 . . .ri−1) is the
discretization of P(ri|r1r2 . . .ri−1).
3. Choose a generating sequence RS in G−1(M) with
the probability P(d)(RS|M), where P(d)(RS|M) =
∑RS(cid:48)∈G−1(M)
encode(·|r1r2 . . .ri−1) to a l-bit string Si.