r
P
3.0x
2.7x
1.7x
RAND
WEEK
0.045
0.04
0.035
0.03
0.025
0.015
0.01
0.005
0
0.02
173.9x
LANL Group−1
After ANY other failure
After same failure type
Random week
2.3x
2.1x
9.8x
ENV
HW
HUMAN
NET
UNDET
SW
MEM
CPU
ANY
1.14x
0.99x
y
t
i
l
i
b
a
b
o
r
P
0.06
0.05
0.04
0.03
0.02
0.01
0
LANL Group−1
LANL Group−2
ENV
HW
HUMAN
NET
UNDET
SW
MEM
CPU
ANY
1.72x
1.51x
1.38x
3.69x
1.70x 1.68x
1.83x
1.04x
RAND
WEEK
1.27x
1.14x 1.15x RAND
WEEK
y
t
i
l
i
b
a
b
o
r
P
1
0.8
0.6
0.4
0.2
0
1.16x
1.16x
0.99x
2.5x
2.0x
3.3x
4.4x
4.9x
1.7x
0.9x
4.2x
NA
1.9x
1.6x
NA
ENV HW HUMAN NET UNDET SW MEM CPU
(a) The probability that a node-failure of type
X is followed by any failure in another node on
the same rack.
(b) The probability that a node-failure of type X
follows any failure in another node on the same
rack.
Fig. 3.
Correlations between failures in the same system. Each bar
corresponds to the probability that a node-failure of type X is followed by
any failure in another node in the same system.
Fig. 2. Correlations between failures in the same rack
When looking at pairwise correlations, i.e. the probability
of a failure of type y within a week of a failure of type x,
we ﬁnd that a failure of a particular type always increases the
probability of the same type of failure within the following
week. Moreover, this increase is much larger than the increase
for the same type of failure following a random failure (i.e.
not necessarily the same type). Figure 2 (right) summarizes our
results. We observe an increase in failure probability as high as
170X for environmental failures and nearly 10X for software
failures. All increases are statistically signiﬁcant based on the
two-sample hypothesis test.
Finally, we take a look speciﬁcally at hardware failures as
these are the most common type of failure. We ﬁnd that both
memory and CPU failures experience a signiﬁcant increase
in probability in the day or week following another failure
of the same type. This observation provides some room for
hypotheses explaining the cause of such errors. One possible
explanation might be that nodes in the same rack share similar
environmental factors, such as the quality of the supplied
power. This observation, combined with the strong effect of
environmental failures on the frequency of follow-up failures
motivates us to study environmental failures in more detail in
Section VII.
C. Correlations between failures in the same system
In this section we ask the question of whether and how
failures between different nodes in the same system (not
necessarily in the same rack) are correlated. We ﬁnd that
the weekly probability of a node experiencing a failure does
increase after another node in the same system had a failure,
however the increase is signiﬁcantly smaller than for nodes in
the same rack: in group-1 systems the weekly probability of
a node experiencing a failure increases from 2.04% to 2.68%
and for group-2 systems it increases from 22.5% to 35.3%.
Both increases are not signiﬁcant enough to allow the rejection
of the hypothesis that a node failure does not increase the
likelihood of follow-up failures in nodes within same system,
based on the two-sample hypothesis test.
The results are more interesting when breaking them down
as a function of the failure type. Figure 3 shows the probability
that a node in a system will fail within a week following
a failure of type X (where X can be: environment, hard-
ware, human-errors, network, software, memory, CPU failures,
or undeterminted). We observe that software, hardware and
human failures in a node in group-1 systems increase the
probability that also other nodes in the system will see failures.
The increase following software failures (a factor of 1.27X)
is statistically signiﬁcant based on the two-sample hypothesis
test. For group-2 systems, all types of failures show an increase
in Figure 3, but by far the biggest increase, with a factor
of 3.69X, is observed following a network failure. The two-
sample hypothesis test allows us to show that all failure types,
except hardware and human, increase the chance of follow-up
failures in other nodes signiﬁcantly.
IV. DO SOME NODES IN A SYSTEM FAIL DIFFERENTLY
FROM OTHERS?
A. Do some nodes fail more frequently than others?
Figure 4 shows the total number of failures for each node
in systems 18, 19 and 20 (the three largest systems of all
LANL systems in terms of number of nodes: 1024, 1024 and
512 nodes, respectively). The graphs show that in all systems
a single node (the node with ID 0) had signiﬁcantly more
failures than rest of nodes. For example, for system 20 node 0
reported 19 times more failures than the average node and
for system 19 node 0 reported more than 30 times higher
failure rates than the average node. To test the signiﬁcance
of differences between failure rates in nodes, we performed
chi-square tests for differences between proportions: with 99%
conﬁdence level we are able to reject the null hypothesis that
all nodes in each system had equal failure rates (p-value <
2.2e-16). Interestingly, even when repeating the same analysis
after removing node 0 we can still reject the hypothesis that
all nodes in each system had equal failure rates.
B. Are the failure characteristics of failure prone nodes dif-
ferent from other nodes?
We are interested to ﬁnd out whether the increased number
of failures in some nodes is due to an increased number of
failures of a particular type or due to generally increased failure
rates. To answer this question we compare in Figure 5 the
relative breakdown of the different failure types for failure
prone nodes against the remainder of the system, and we
compare in Figure 6 for each failure type the probabilities
of a node failure of this type in failure prone nodes vs the rest
of the nodes in the systems. In Figure 6 each plot contains
three pairs of bars for each of the three systems, where each
pair corresponds to a timespan: day, week or month. The
numbers on top of the bars indicate the factor increase in
failure probability in a failure prone node compared to an
average node.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:51:27 UTC from IEEE Xplore.  Restrictions apply. 
System 18
System 19
System 20
s
e
r
u
l
i
a
F
100
f
o
#
l
a
t
o
T
50
0
0
500
Node ID
1000
s
e
r
u
100
a
F
l
i
f
o
#
l
a
t
o
T
50
0
0
s
e
r
u
l
i
a
F
100
f
o
#
l
a
t
o
T
50
0
0
500
Node ID
1000
400
200
Node ID
t
%
e
g
a
n
e
c
r
e
P
100
80
60
40
20
0
System 18
Node 0
100
80
60
40
20
0
System 19
Node 0
ENV
HW
HUM
NET
UNDET
SW
100
80
60
40
20
0
System 20
Node 0
Fig. 4. Total number of failures as a function of Node-ID
Fig. 5. Root cause breakdown in failure prone nodes vs other nodes
Rest of nodes
Node−0
e
r
u
l
i
7.3x
5.2x
5.5x
5.3x
5.4x
8.9x
5.4x
9.8x
5.5x
day week month
day week month
System 18                    System 19                  System 20
day week month
40.6x
Rest of nodes
Node−0
53.3x
0.16
0.14
0.12
0.1
0.08
0.06
0.04
0.02
0
Rest of nodes
Node−0
185.4x
119.7x
 87.5x
 87.5x
185.4x
119.7x
 87.5x
day week month
day week month
System 18                    System 19                  System 20
day week month
119.7x
185.4x
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
 46.7x
 48.3x
Rest of nodes
Node−0
 23x  
 95.8x
 71.4x
 35.9x
 90.5x
117.9x
 42.1x
day week month
day week month
System 18                    System 19                  System 20
day week month
a
F
n
a
m
u
H
a
f
o
y
t
i
l
i
b
a
b
o
r
P
e
r
u
l
i
a
F
e
r
a
w
t
f
o
S
a
f
o
y
t
i
l
i
b
a
b
o
r
P
e
r
u
l
i
a
F
t
n
e
m
n
o
r
i
v
n
E
n
a
f
o
y
t
i
l
i
b
a
b
o
r
P
e
r
u
l
i
a
F
k
r
o
w
e
N
a
t
f
o
y
t
i
l
i
b
a
b
o
r
P
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
0.3
0.25
0.2
0.15
0.1
0.05
0
1926x  
Rest of nodes
Node−0
e
r
u
l