       valid_lft forever preferred_lft forever
    inet6 fe80::8415:2aff:fef7:ef9/64 scope link
       valid_lft forever preferred_lft forever
user@docker1:~$
```
如果我们从主机检查，我们现在应该可以到达容器:
```
user@docker1:~$ ping 172.17.0.99 -c 2
PING 172.17.0.99 (172.17.0.99) 56(84) bytes of data.
64 bytes from 172.17.0.99: icmp_seq=1 ttl=64 time=0.104 ms
64 bytes from 172.17.0.99: icmp_seq=2 ttl=64 time=0.045 ms
--- 172.17.0.99 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 999ms
rtt min/avg/max/mdev = 0.045/0.074/0.104/0.030 ms
user@docker1:~$
user@docker1:~$ curl http://172.17.0.99
    Web Server #1 - Running on port 80
user@docker1:~$
```
连接就绪后，我们的拓扑现在如下所示:
![How to do it…](img/B05453_04_03.jpg)
因此，虽然我们有 IP 连接，但它只适用于同一子网中的主机。最后剩下的部分将是解决主机级别的容器连接问题。对于出站连接，主机将容器的 IP 地址隐藏在主机接口的 IP 地址后面。对于入站连接，在默认网络模式下，Docker 使用端口映射将 Docker 主机网卡上的随机高端口映射到容器的公开端口。
在这种情况下，解决出站问题就像给容器一个指向`docker0`桥的默认路由，并确保您有一个网络过滤器伪装规则来覆盖它一样简单:
```
user@docker1:~$ sudo ip netns exec 712f8a477cce ip route \
add default via 172.17.0.1
user@docker1:~$ docker exec -it web1 ping 4.2.2.2 -c 2
PING 4.2.2.2 (4.2.2.2): 48 data bytes
56 bytes from 4.2.2.2: icmp_seq=0 ttl=50 time=39.764 ms
56 bytes from 4.2.2.2: icmp_seq=1 ttl=50 time=40.210 ms
--- 4.2.2.2 ping statistics ---
2 packets transmitted, 2 packets received, 0% packet loss
round-trip min/avg/max/stddev = 39.764/39.987/40.210/0.223 ms
user@docker1:~$
```
如果您使用的是`docker0`桥，就像我们在这个例子中所做的那样，您不需要添加一个自定义的网络过滤器伪装规则。这是因为默认伪装规则已经覆盖了`docker0`网桥的整个子网:
```
user@docker1:~$ sudo iptables -t nat -L
……
Chain POSTROUTING (policy ACCEPT)
target     prot opt source               destination
MASQUERADE  all  --  172.17.0.0/16        anywhere
……
user@docker1:~$
```
对于入站服务，我们需要创建一个自定义规则，使用**网络地址转换** ( **NAT** )将主机上的随机高端口映射到容器中暴露的服务端口。我们可以用这样的规则来做到:
```
user@docker1:~$ sudo iptables -t nat -A DOCKER ! -i docker0 -p tcp -m tcp \
--dport 32799 -j DNAT --to-destination 172.17.0.99:80
```
在这个案例中，我们将主机接口上的端口`32799`NAT 到容器上的端口`80`。这将允许外部网络上的系统通过端口`32799`上的 Docker 主机接口访问运行在`web1`的网络服务器。
最后，我们成功复制了 Docker 在默认网络模式下提供的功能:
![How to do it…](img/B05453_04_04.jpg)
这应该会让你对 Docker 在幕后所做的事情有所了解。跟踪容器 IP 寻址、已发布端口的端口分配和`iptables`规则集是 Docker 代表您跟踪的三件主要事情。鉴于容器的短暂性，这几乎不可能手动完成。
# 指定自己的桥
在的大多数网络场景中，Docker 严重依赖`docker0`桥。`docker0`桥在 Docker 引擎服务启动时自动创建，是 Docker 服务产生的任何容器的默认连接点。我们还在早期的食谱中看到，可以在服务级别修改这个桥的一些属性。在这个食谱中，我们将向您展示如何告诉 Docker 使用一个完全不同的桥。
## 做好准备
在本食谱中，我们将在单个 Docker 主机上演示配置。假设该主机安装了 Docker，并且 Docker 处于默认配置。为了查看和操作网络设置，您需要确保安装了`iproute2`工具集。如果系统上没有，可以使用以下命令进行安装:
```
sudo apt-get install iproute2 
```
为了对主机进行网络更改，您还需要根级访问。
## 怎么做…
与任何其他服务级别参数一样，为 Docker 指定不同的桥是通过更新我们在[第 2 章](02.html "Chapter 2. Configuring and Monitoring Docker Networks")、*中向您展示如何创建的系统插件文件*来完成的。
在我们指定新的桥之前，让我们首先确保没有容器在运行，停止 Docker 服务，并删除`docker0`桥:
```
user@docker1:~$ docker ps -a
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
user@docker1:~$
user@docker1:~$ sudo systemctl stop docker
user@docker1:~$
user@docker1:~$ sudo ip link delete dev docker0
user@docker1:~$
user@docker1:~$ ip link show dev docker0
Device "docker0" does not exist.
user@docker1:~$
```
此时，默认的`docker0`桥已经被删除。现在，让我们创建一个新的桥供 Docker 使用。
### 注
如果您不熟悉`iproute2`命令行工具，请参考[第 1 章](01.html "Chapter 1. Linux Networking Constructs")、 *Linux 网络结构*中的示例。
```
user@docker1:~$ sudo ip link add mybridge1 type bridge
user@docker1:~$ sudo ip address add 10.11.12.1/24 dev mybridge1
user@docker1:~$ sudo ip link set dev mybridge1 up
user@docker1:~$ ip addr show dev mybridge1
7: mybridge1:  mtu 1500 qdisc noqueue state UNKNOWN group default
    link/ether 9e:87:b4:7b:a3:c0 brd ff:ff:ff:ff:ff:ff
    inet 10.11.12.1/24 scope global mybridge1
       valid_lft forever preferred_lft forever
    inet6 fe80::9c87:b4ff:fe7b:a3c0/64 scope link
       valid_lft forever preferred_lft forever
user@docker1:~$
```
我们首先创建了一个名为`mybridge1`的桥，然后给它一个`10.11.12.1/24`的 IP 地址，最后打开了接口。此时，接口已启动并可到达。我们现在可以告诉 Docker 使用这个桥作为它的默认桥。为此，请编辑 Docker 的 systemd 插件文件，并确保最后一行内容如下:
```
ExecStart=/usr/bin/dockerd --bridge=mybridge1
```
现在保存文件，重新加载系统配置，并启动 Docker 服务:
```
user@docker1:~$ sudo systemctl daemon-reload
user@docker1:~$ sudo systemctl start docker
```
现在如果我们启动一个容器，我们应该看到它被分配到桥`mybridge1`:
```
user@docker1:~$ docker run --name web1 -d -P jonlangemak/web_server_1
e8a05afba6235c6d8012639aa79e1732ed5ff741753a8c6b8d9c35a171f6211e
user@docker1:~$ ip link show
1: lo:  mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eth0:  mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000
    link/ether 62:31:35:63:65:63 brd ff:ff:ff:ff:ff:ff
3: eth1:  mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 36:b3:5c:94:c0:a6 brd ff:ff:ff:ff:ff:ff
17: mybridge1:  mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000
    link/ether 7a:1b:30:e6:94:b7 brd ff:ff:ff:ff:ff:ff
22: veth68fb58a@if21:  mtu 1500 qdisc noqueue master mybridge1 state UP mode DEFAULT group default
    link/ether 7a:1b:30:e6:94:b7 brd ff:ff:ff:ff:ff:ff link-netnsid 0
user@docker1:~$
```
请注意，服务启动时并未创建`docker0`桥。另外，请注意，我们在默认名称空间中看到了主接口为`mybridge1`的 VETH 对的一侧。
利用我们从本章第一个食谱中学到的知识，我们还可以确认 VETH 对的另一端在容器的网络命名空间中:
```
user@docker1:~$ docker inspect web1 | grep SandboxKey
            "SandboxKey": "/var/run/docker/netns/926ddab911ae",
user@docker1:~$ 
user@docker1:~$ sudo ip netns exec 926ddab911ae ip link show
1: lo:  mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
21: eth0@if22:  mtu 1500 qdisc noqueue state UP mode DEFAULT group default
    link/ether 02:42:0a:0b:0c:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0
user@docker1:~$ 
```
我们可以判断这是一个 VETH 对，因为它使用了`@`命名语法。如果我们比较 VETH 对接口号，我们可以看到这两个与 VETH 对的主机端匹配，索引为`22`，连接到 VETH 对的容器端，索引为`21`。
### 注
您可能会注意到，我在使用`ip netns exec`和`docker exec`命令来执行容器内的命令之间切换。这样做的目的不是为了混淆视听，而是为了表明 Docker 正在为你做什么。需要注意的是，为了使用`ip netns exec`语法，你需要符号链接，我们在之前的配方中已经演示过了。只有在手动配置名称空间时，才需要使用`ip netns exec`。
如果我们查看容器的网络配置，我们可以看到 Docker 在`mybridge1`子网范围内为其分配了一个 IP 地址:
```
user@docker1:~$ docker exec web1 ip addr show dev eth0
8: eth0@if9:  mtu 1500 qdisc noqueue state UP
    link/ether 02:42:0a:0b:0c:02 brd ff:ff:ff:ff:ff:ff
    inet 10.11.12.2/24 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::42:aff:fe0b:c02/64 scope link
       valid_lft forever preferred_lft forever
user@docker1:~$
```
Docker 现在也在跟踪桥的 IP 分配，因为它分配容器 IP 地址。IP 地址管理是 Docker 在容器网络空间中提供的一个被低估的大值。将 IP 地址映射到容器并自行管理将是一项重大任务。
最后一部分是处理容器的 NAT 配置。由于`10.11.12.0/24`空间是不可路由的，我们需要在 Docker 主机上的物理接口后面隐藏或伪装容器的 IP 地址。幸运的是，只要 Docker 为您管理桥，Docker 仍然会负责制定适当的网络过滤器规则。我们可以通过检查网络过滤器规则集来确保这一点:
```
user@docker1:~$ sudo iptables -t nat -L -n
……
Chain POSTROUTING (policy ACCEPT)
target     prot opt source               destination
MASQUERADE  all  --  10.11.12.0/24        0.0.0.0/0
……
Chain DOCKER (2 references)
target     prot opt source               destination
RETURN     all  --  0.0.0.0/0            0.0.0.0/0
DNAT       tcp  --  0.0.0.0/0            0.0.0.0/0            tcp dpt:32768 to:10.11.12.2:80
```
此外，由于我们用`-P`标志公开了容器上的端口，入站 NAT 也被分配了。我们也可以在前面相同输出的 DOCKER 链中看到这种 NAT 转换。总之，只要您使用的是 Linux 桥，Docker 就会像处理`docker0`桥一样为您处理整个配置。
# 使用 OVS 大桥
对于正在寻找附加功能的用户来说，**openvsswitch**(**OVS**)正在成为原生 Linux 桥的热门替代品。OVS 以稍微高一点的复杂性为代价，对 Linux 桥进行了戏剧性的增强。例如，一个 OVS 桥不能被我们一直使用的`iproute2`工具集直接管理，它需要自己的命令行管理工具。然而，如果你正在寻找 Linux 桥上不存在的特性，OVS 可能是你最好的选择。Docker 不能本地管理 OVS 桥，所以使用它需要您手动建立桥和容器之间的连接。也就是说，我们不能只告诉 Docker 服务使用 OVS 桥而不是默认的`docker0`桥。在本食谱中，我们将展示如何安装、配置容器，并将容器直接连接到 OVS 桥，而不是标准的`docker0`桥。
## 做好准备