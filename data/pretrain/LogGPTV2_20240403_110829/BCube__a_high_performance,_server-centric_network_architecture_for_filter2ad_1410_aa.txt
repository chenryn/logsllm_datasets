title:BCube: a high performance, server-centric network architecture for
modular data centers
author:Chuanxiong Guo and
Guohan Lu and
Dan Li and
Haitao Wu and
Xuan Zhang and
Yunfeng Shi and
Chen Tian and
Yongguang Zhang and
Songwu Lu
BCube: A High Performance, Server-centric Network
Architecture for Modular Data Centers∗
Chuanxiong Guo1, Guohan Lu1, Dan Li1, Haitao Wu1, Xuan Zhang1,2, Yunfeng Shi1,3,
Chen Tian1,4, Yongguang Zhang1, Songwu Lu1,5
1: Microsoft Research Asia, 2: Tsinghua, 3: PKU, 4: HUST, 5: UCLA
{chguo,lguohan,danil,hwu}@microsoft.com, PI:EMAIL,
PI:EMAIL, PI:EMAIL, PI:EMAIL,
PI:EMAIL
ABSTRACT
This paper presents BCube, a new network architecture
speciﬁcally designed for shipping-container based, modular
data centers. At the core of the BCube architecture is its
server-centric network structure, where servers with multi-
ple network ports connect to multiple layers of COTS (com-
modity oﬀ-the-shelf) mini-switches. Servers act as not only
end hosts, but also relay nodes for each other. BCube sup-
ports various bandwidth-intensive applications by speeding-
up one-to-one, one-to-several, and one-to-all traﬃc patterns,
and by providing high network capacity for all-to-all traﬃc.
BCube exhibits graceful performance degradation as the
server and/or switch failure rate increases. This property
is of special importance for shipping-container data centers,
since once the container is sealed and operational, it becomes
very diﬃcult to repair or replace its components.
Our implementation experiences show that BCube can be
seamlessly integrated with the TCP/IP protocol stack and
BCube packet forwarding can be eﬃciently implemented in
both hardware and software. Experiments in our testbed
demonstrate that BCube is fault tolerant and load balanc-
ing and it signiﬁcantly accelerates representative bandwidth-
intensive applications.
Categories and Subject Descriptors
C.2.1 [Network Architecture and Design]: Network topol-
ogy, Packet-switching networks
General Terms
Algorithms, Design
Keywords
Modular data center, Server-centric network, Multi-path
∗This work was performed when Xuan Zhang, Yunfeng Shi,
and Chen Tian were interns and Songwu Lu was a visiting
professor at Microsoft Research Asia.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’09, August 17–21, 2009, Barcelona, Spain.
Copyright 2009 ACM 978-1-60558-594-9/09/08 ...$10.00.
1.
INTRODUCTION
Shipping-container based, modular data center (MDC) of-
fers a new way in which data centers are built and deployed
[13, 16, 23, 24, 25]. In an MDC, up to a few thousands of
servers are interconnected via switches to form the network
infrastructure, say, a typical, two-level tree in the current
practice. All the servers and switches are then packed into
a standard 20- or 40-feet shipping-container. No longer tied
to a ﬁxed location, organizations can place the MDC any-
where they intend and then relocate as their requirements
change.
In addition to high degree of mobility, an MDC
has other beneﬁts compared with the data center facilities
directly built from server racks. They include shorter de-
ployment time, higher system and power density, and lower
cooling and manufacturing cost.
In this work, we describe BCube, a high-performance and
robust network architecture for an MDC. The design and im-
plementation of BCube are driven by demands from data-
intensive computing, recent technology trends, as well as
special MDC requirements. Many data center applications
require bandwidth-intensive, one-to-one, one-to-several (e.g.,
distributed ﬁle systems [12]), one-to-all (e.g., application
data broadcasting), or all-to-all (e.g., MapReduce [6] and
Dryad [17]) communications among MDC servers. BCube
is designed to well support all these traﬃc patterns. A recent
technology trend is to build data centers using commodity
servers [2]. We go one step further by using only low-end
COTS mini-switches. This option eliminates expensive high-
end switches. Diﬀerent from a traditional data center, it is
diﬃcult or even impossible to service an MDC once it is
deployed. Therefore, BCube needs to achieve graceful per-
formance degradation in the presence of server and switch
failures.
The BCube network architecture takes the server-centric
approach, rather than the switch-oriented practice. It places
intelligence on MDC servers and works with commodity
switches. BCube makes innovations in its server-centric in-
terconnection structure, routing protocol, and eﬃcient im-
plementation.
In the BCube interconnection structure, each server is
equipped with a small number of ports (typically no more
than four). Multiple layers of cheap COTS mini-switches
are used to connect those servers. BCube provides multiple
parallel short paths between any pair of servers. This not
only provides high one-to-one bandwidth, but also greatly
improves fault tolerance and load balancing. BCube ac-
celerates one-to-several and one-to-all traﬃc by construct-
63ing edge-disjoint complete graphs and multiple edge-disjoint
server spanning trees. Moreover, due to its low diameter,
BCube provides high network capacity for all-to-all traﬃc
such as MapReduce.
BCube runs a source routing protocol called BSR (BCube
Source Routing). BSR places routing intelligence solely onto
servers. By taking advantage of the multi-path property of
BCube and by actively probing the network, BSR balances
traﬃc and handles failures without link-state distribution.
With BSR, the capacity of BCube decreases gracefully as
the server and/or switch failure increases.
We have designed and implemented a BCube protocol
suite. We can design a fast packet forwarding engine, which
can decide the next hop of a packet by only one table lookup.
The packet forwarding engine can be eﬃciently implemented
in both software and hardware. We have built a BCube
testbed with 16 servers and 8 8-port Gigabit Ethernet mini-
switches. Experiments in our testbed demonstrated the ef-
ﬁciency of our implementation. Experiments also showed
that BCube provides 2 times speedup for one-to-x (abbre-
viation for one-to-one, one-to-several, and one-to-all) traﬃc
patterns, and 3 times throughput for MapReduce tasks com-
pared with the tree structure. BCube uses more wires than
the tree structure. But wiring is a solvable issue for contain-
ers which are at most 40-feet long.
Recently, fat-tree [1] and DCell [9] are proposed as net-
work structures to interconnect tens of thousands or more
servers in data centers. BCube is better than these two
structures for MDCs. Compared with DCell, BCube does
not have performance bottlenecks and provides much higher
network capacity; compared with fat-tree, BCube provides
better one-to-x support and can be directly built using com-
modity switches without any switch upgrade. See Section 8
for detailed comparisons.
The rest of the paper is organized as follows. Section 2
discusses background. Section 3 presents BCube and its
support for various traﬃc patterns. Section 4 designs BSR.
Section 5 addresses other design issues. Sections 6 stud-
ies graceful degradation. Section 7 presents implementation
and experiments. Section 8 discusses related work and Sec-
tion 9 concludes the paper.
2. BACKGROUND
MDCs present new research opportunities as well as chal-
lenges. The size of a 40-feet container is 12m×2.35m×2.38m,
hence wiring becomes a solvable problem when we depart
from the traditional tree structure; it is possible to use cheap
commodity Gigabit Ethernet mini-switches for interconnec-
tion since the target scale is typically thousands of servers.
Yet, designing network architecture for MDCs is also chal-
lenging. It is diﬃcult or even impossible to service a con-
tainer once it is sealed and deployed. The design should
be fault tolerant and the performance should degrade grace-
fully as components failure increases. We now elaborate the
requirements for MDCs in more details.
Bandwidth-intensive application support. Many data
center applications need to move huge amount of data among
servers, and network becomes their performance bottleneck
[6, 12, 17]. A well designed network architecture needs to
provide good support for typical traﬃc patterns. We de-
scribe several typical traﬃc patterns as follows.
One-to-one, which is the basic traﬃc model in which one
server moves data to another server. Our design should pro-
vide high inter-server throughput. This is particularly useful
when there exist server pairs that exchange large amount of
data such as disk backup. Good one-to-one support also
results in good several-to-one and all-to-one support.
One-to-several,
in which one server transfers the same
copy of data to several receivers. Current distributed ﬁle
systems such as GFS [12], HDFS [4], and CloudStore [5],
replicate data chunks of a ﬁle several times (typically three)
at diﬀerent chunk servers to improve reliability. When a
chunk is written into the ﬁle system, it needs to be simulta-
neously replicated to several servers.
One-to-all, in which a server transfers the same copy of
data to all the other servers in the cluster. There are several
cases that one-to-all happens: to upgrade the system image,
to distribute application binaries, or to distribute speciﬁc
application data.
All-to-all, in which every server transmits data to all the
other servers. The representative example of all-to-all traﬃc
is MapReduce [6]. The reduce phase of MapReduce needs
to shuﬄe data among many servers, thus generating an all-
to-all traﬃc pattern.
Low-end commodity switches. Current data centers
use commodity PC servers for better performance-to-price
ratio [2]. To achieve the same goal, we use low-end non-
programmable COTS switches instead of the high-end ones,
based on the observation that the per-port price of the low-
end switches is much cheaper than that of the high-end ones.
As we have outlined in our ﬁrst design goal, we want to pro-
vide high capacity for various traﬃc patterns. The COTS
switches, however, can speak only the spanning tree proto-
col, which cannot fully utilize the links in advanced network
structures. The switch boxes are generally not as open as
the server computers. Re-programming the switches for new
routing and packet forwarding algorithms is much harder,
if not impossible, compared with programming the servers.
This is a challenge we need to address.
Graceful performance degradation. Given that we only
assume commodity servers and switches in a shipping-container
data center, we should assume a failure model of frequent
component failures. Moreover, An MDC is prefabricated in
factory, and it is rather diﬃcult, if not impossible, to service
an MDC once it is deployed in the ﬁeld, due to operational
and space constraints. Therefore, it is extremely important
that we design our network architecture to be fault toler-
ant and to degrade gracefully in the presence of continuous
component failures.
In our BCube architecture, we introduce a novel server-
centric BCube network topology and a BCube Source Rout-
ing protocol (BSR) to meet the requirements of MDCs. We
next present the structure and BSR sequentially.
3. THE BCUBE STRUCTURE
In this section, we ﬁrst present the server-centric BCube
structure and then analyze its one-to-x and all-to-all traﬃc
support properties.
3.1 BCube Construction
There are two types of devices in BCube: Servers with
multiple ports, and switches that connect a constant num-
ber of servers. BCube is a recursively deﬁned structure. A
BCube0 is simply n servers connecting to an n-port switch.
A BCube1 is constructed from n BCube0s and n n-port
switches. More generically, a BCubek (k ≥ 1)) is con-
64(a)
(b)
Figure 1: (a)BCube is a leveled structure. A BCubek is constructed from n BCubek−1 and nk n-port switches.
(b) A BCube1 with n = 4. In this BCube1 network, each server has two ports.
structed from n BCubek−1s and nk n-port switches. Each
server in a BCubek has k + 1 ports, which are numbered
from level-0 to level-k. It is easy to see that a BCubek has
N = nk+1 servers and k + 1 level of switches, with each level
having nk n-port switches.
The construction of a BCubek is as follows. We number
the n BCubek−1s from 0 to n − 1 and the servers in each
BCubek−1 from 0 to nk − 1. We then connect the level-k
port of the i-th server (i ∈ [0, nk − 1]) in the j-th BCubek−1
(j ∈ [0, n − 1]) to the j-th port of the i-th level-k switch,
as illustrated in Fig. 1(a). Fig. 1(b) shows a BCube1 with
n = 4, which is constructed from four BCube0s and four 4-
port switches. The links in BCube are bidirectional. In this
section, we assume the link bandwidth as 1 for simplicity of
presentation.
We denote a server in a BCubek using an address array
akak−1 ··· a0 (ai ∈ [0, n− 1], i ∈ [0, k]). Equivalently, we can
i=0 aini to denote a server.
use a BCube address baddr =
We denote a switch using the form 
(sj ∈ [0, n − 1], j ∈ [0, k − 1]), where l(0 ≤ l ≤ k) is the level
of the switch.
From Fig. 1, we can see that the i-th port of a level-k
switch  connects to the level-k port
of server isk−1sk−2 ··· s0 (i ∈ [0, n − 1]). More generi-
cally, the construction ensures that the i-th port of a switch
 connects to the level-l port of server
sk−1sk−2 ··· slisl−1 ··· s0.
(cid:80)k
The BCube construction guarantees that switches only
connect to servers and never directly connect to other switches.
As a direct consequence, we can treat the switches as dummy
crossbars that connect several neighboring servers and let
servers relay traﬃc for each other. With 8-port mini-switches,
we can support up to 4096 servers in one BCube3. BCube
therefore meets our goal of using only low-end commodity
switches by putting routing intelligence purely into servers.
The BCube structure is closely related to the generalized
In a BCube network, if we replace each
Hypercube [3].
switch and its n links with an n × (n − 1) full mesh that
directly connects the servers, we get a generalized Hyper-
cube. Compared to the generalized Hypercube, the server
port number is much smaller in BCube.
It is k + 1 in a
BCubek and (n − 1)(k + 1) in a corresponding generalized
Hypercube. This implies that we reduce the server port
number from 28 to 4 when n = 8 and k = 3. The incurred
cost in BCube is the k+1 layers of switches. This is a tradeoﬀ
we make willingly, due to the low cost of the mini-switches.
/*
A=akak−1 · · · a0 and B=bkbk−1 · · · b0; A[i] = ai; B[i] = bi;
Π = [πk, πk−1, · · · , π0] is a permutation
of [k, k − 1, · · · , 1, 0]
*/
BCubeRouting(A, B, Π):