some sort of reputation over time and to update previously pub-
lished documents.
Many of the previously mentioned systems are still in their infancy—
either existing in design only or deployed in a testbed fashion.
Therefore it is difﬁcult to judge the scalability of these systems.
Most of these systems exhibit scalability tradeoffs. Freenet, for
example, allows servers to join and leave at will, however the new
servers may not immediately be able to locate all published content.
3.2 Peer-to-Peer Systems
In true peer-to-peer systems, there is no distinction between a
server and client. Therefore we will identify computers participat-
ing in a peer-to-peer system as nodes. While Tangler assumes that
all severs know of each other, most peer-to-peer systems are de-
signed to scale to the point that not every node knows about all
other nodes.
Gnutella [8] is a ﬁle sharing application that allows participat-
ing nodes to query for and copy ﬁles stored on other participating
nodes. There is no formal publication method. A node simply in-
terprets queries in any way it sees ﬁt and sends back the names of
ﬁles that is feels matches the query. As each request is essentially
broadcast to other participating nodes the communication costs are
quite high. While anonymous searching is supported, all ﬁle trans-
fers are done in a point to point fashion and are therefore not anony-
mous. A node that wants a copy of a ﬁle directly contacts the node
holding that ﬁle and performs the transfer. Tangler’s block lookup
protocol is more efﬁcient than Gnutella’s ﬂood queries in the ex-
pected number of servers that must be contacted. However, joining
and leaving the server network is far more heavy-weight and less
scalable in Tangler than in Gnutella.
CFS [5] is a peer-to-peer ﬁle storage service. CFS utilizes a
unique routing algorithm called Chord that not only allows nodes
to join or leave at will but also greatly reduces the communica-
tion costs associated with ﬁnding nodes that hold the needed ﬁle.
Tangler and Chord both employ consistent hashing to route queries.
Chord is designed to support far more nodes than Tangler, however.
Chord nodes only need to know about O(log(N )) other nodes,
making joining and leaving the system very efﬁcient. Chord queries
contact O(log(N )) servers. In Tangler, queries only contact a con-
stant number of servers, but every server must know about every
other server. Unlike Chord, Tangler actively migrates data between
servers so that, over time, different servers will be responsible for
any given data block.
PASTRY [17], CAN [15] and Tapestry [22] are all peer-to-peer
routing algorithms with goals similar to Chord that have low com-
munication overhead and yet still scale to a very large number of
participating nodes.
4. TANGLER DOCUMENT COLLECTIONS
The Tangler system consists of a publishing program that trans-
forms documents into blocks, a reconstruction program that fetches
blocks to reconstitute documents, and a network server daemon
that permits the distribution and retrieval of blocks in a collec-
128Collection A
soft link
Collection B
Directory
Doc2
Doc3
Doc1
Directory
Doc5
Doc6
Doc4
hard link
hard link
server
block
hard link
server
block
hard link
hard link
server
block
server
block
Figure 1: Collections can be made up of ﬁles (hard links) and links to other collections (soft links)
tion of servers. This section explains Tangler’s approach to doc-
ument naming and content authentication, and describes how Tan-
gler transforms published content into ﬁxed-size blocks suitable for
injection into a storage network. These ﬁxed-size blocks are con-
structed so that they can potentially belong to multiple documents,
the property we call entanglement. Section 5 describes the algo-
rithm used to entangle blocks. Though Tangler does not currently
have an implemented network daemon, Section 6 proposes the de-
sign of a self-policing block server network that can survive certain
ﬂooding attacks and the existence of corrupt servers.
4.1 Collections
Every document in Tangler is published as part of one or more
collections. A collection is a group of documents that are published
by the same person under the same public key. Collections are
published anonymously, but the person who published a collection
can update it. Thus, anonymous collections may build reputations.
A collection can consist of a single document, multiple versions of
a single document, multiple documents, or soft links to documents
in other collections. One can think of a collection as a directory
containing a group of related ﬁles, subdirectories, and links. For
example, a collection may contain a group of technical reports, the
ﬁles making up a web site, or an index of other collections.
Each collection is named by a public key, K. Tangler refers to
documents in a collection as K/name, where name represents the
name chosen by the publisher for the document. For example, lets
assume we wish to publish a collection consisting of the ﬁles that
make up the screenplay for a movie entitled “Last Tangle In Paris.”
The parameters to the publish program are the collection members
(ﬁles, directories, and soft links), and a public/private key pair. The
publish program entangles the collection members. The public key
is used to name the collection. The private key is used to sign the
collection.
The reconstruction program retrives documents and places them
in subdirectories of a Tangler root directory, named by public key.
For example, suppose the Tangler root is /tangler. The recon-
struction program, when asked, might place act1 in /tangler/75
b4e39a1b58c265f72dac35e7f940c6f093cb80/act1, where “7
5b4e39a1b58c265f72dac35e7f940c6f093cb80” is the collec-
tion’s public key.
As previously stated, collections consist both of documents and
links to documents in other collections. Links to documents in
other collections are referred to as soft links. When a collection
is updated, soft links into that collection reﬂect the new contents.
The reconstruction program represents soft links as symbolic
links in the ﬁle system. Documents within a particular collection
are known as hard links. Two collections may actually contain hard
links to the same document and share the same entangled blocks for
reconstituting the document. However, if one collection is updated
by linking the same name to new contents, the other collection will
not reﬂect the change. Hard links are useful to preserve a docu-
ment if one fears the collection it was published in may change or
disappear.
As an example, the “Last Tangle In Paris” screenplay collection
contains a hard link to the ﬁle act1. However, it might also con-
tain a soft link to a collection published by the Paris Tourist Bureau
entitled 3f9··· 2d1/current events.html (where 3f9··· 2d1 is
the Tourist Bureau’s public key). The Paris Tourist Bureau actu-
ally owns the collection and can therefore update it. Our collection
merely points to it. Anyone reading the screenplay collection soft
link will read the latest Paris Tourist Bureau collection. See Fig-
ure 1.
4.2 Hash trees
Tangler makes extensive use of the SHA-1 [14] cryptographic
hash function. SHA-1 is a collision-resistant hash function that
produces a 20-byte output from an arbitrary-length input. Finding
any two inputs of SHA-1 that produce the same output is believed to
be computationally intractable. Thus, we can assume no collisions
and treat SHA-1 hashes as unique, veriﬁable identiﬁers for data
blocks.
Tangler also relies on Hash Trees [11]. Hash trees allow one to
to specify or commit to large amounts of data with a single cryp-
tographic hash value. Using hash trees, users can efﬁciently verify
small regions of the data without needing access to all of it. In a
hash tree, the data being certiﬁed or committed to ﬁlls the leaves
of an n-ary tree. Each internal node of the tree stores the crypto-
graphic hashes of the child nodes. Assuming no hash collisions,
then, the hash value of the tree’s root speciﬁes the entire contents
of the tree. One can prove the integrity of any leaf of the hash tree
to someone who knows the root by producing the values of inter-
mediary nodes from the root to the leaf.
The SFS read-only ﬁle system [7] showed how to transform a ﬁle
system into a hash tree. SFSRO clients can traverse a ﬁle system
129and verify the contents of individual ﬁle blocks starting only from
a root hash. Tangler employs a similar technique to produce collec-
tions, but the speciﬁcs differ somewhat because of entanglements.
4.3 Server Blocks
In order to publish a collection, C, one runs a publisher program
that takes as input a public/private key, and a directory of ﬁles. The
program fetches random previously published blocks and entangles
these blocks with the ﬁles in C to produce new blocks. (Section 6
describes how such random fetching can be implemented.) Finally,
it signs a collection root structure. Thus, one must have C’s private
key to publish or update the collection. Once entangled, C depends
inextricably on the randomly chosen, previously published blocks
for the reconstruction of its ﬁles. The person publishing C must
distribute both the blocks just created and the ones with which her
collection is entangled. Thus, replicating others’ documents is an
inherent part of publishing.
The entangled output blocks of the publisher program are suit-
able for injection into a storage network. We call these blocks
server blocks to differentiate them from the ﬁle data blocks that
were input to the publisher program. Tangler names server blocks
by their SHA-1 hash values. It records SHA-1 values in collection
metadata structures and assumes blocks can be retrieved from the
storage network by their hash values.
Each collection has a root. This root functions much as a root
directory in a ﬁle system—it deﬁnes a starting point in the search
for ﬁles. The one exception to the SHA-1 hash addressing scheme
is the addressing of the collection root. Recall that a collection is
signed and named by a public key. This public key therefore also
names the collection’s root block, and therefore must be present
within that block. Thus, the storage network must support the re-
trieval of blocks by public key. As a collection can be updated, two
or more collection roots with the same public key may appear in
the storage network. To disambiguate the blocks, a version ﬁeld is
present within all collection roots. The version ﬁeld is incremented
each time a collection is republished.
For the rest of this section and Section 5, we assume a collection
of storage servers that implement a distributed, public block pool.
Participating servers can inject server blocks into this pool, and
blocks can be retrieved by SHA-1 hash or public key. Section 6
discusses how to implement a storage network far less susceptible
to attack and abuse than a simple block pool.
4.4 Publisher Program
The ﬁrst step of the publisher program is to entangle each mem-
ber ﬁle in a collection. Each ﬁle is split into ﬁxed-size (16K) data
blocks. The last data block may need to be padded to achieve the
ﬁxed size. Each data block is then entangled using the algorithm
described in Section 5.2. The entanglement algorithm takes as in-
put two random blocks from the block pool and one data block from
a ﬁle being published.
It outputs two new server blocks, which
when combined with the randomly selected pool blocks can recon-
stitute the data block. Thus, for every data block a publisher en-
tangles, she becomes interested in ensuring the availability of four
server blocks in the public pool. A data block can actually be recon-
structed from any three of its four associated server blocks, adding
some fault-tolerance (see Section 5.3). Notice that we do not inject
data blocks in the storage network, only server blocks.
Every entangled ﬁle has an associated inode data structure that
records the SHA-1 hashes of the server blocks needed to recon-
struct the ﬁle’s data blocks. Once all the data blocks of a particular
ﬁle have been entangled and the names of the associated server
blocks recorded in an inode, the inode itself is entangled. This en-
Proc P ublish (Collection C, PublicKey pk, PrivateKey sk)
c=new CollectionRoot()
for each ﬁle, f, in the post-order traversal of C:
i=new Inode(f)
for each data block, b, in f:
p1=random server block selected from pool
p2=random server block selected from pool
(e1, e2)=entangle(b,p1,p2)
store server blocks (p1, p2, e1, e2) in pool
r=random permutation(p1, p2, e1, e2)
record b’s dependency on (r) in i
endfor
/∗ entangle the inode ∗/
p3=random server block selected from pool
p4=random server block selected from pool
(e3, e4)=entangle(i,p3,p4)
r=random permutation(p3,p4,e3,e4)
/∗ r stores the reconstruction address for inode i ∗/
record (f, r) in collection root
endfor
c.name=pk
c.version=1
digest=SHA 1(name, version)
c.sig=sign(digest,sk)
End P ublish
Figure 2: Publish Algorithm
tanglement produces the names of four server blocks that can be
used to reconstruct the inode. These four server block names are
recorded, along with the associated ﬁle’s name in the collection
root. The collection root essentially provides a mapping between
ﬁle names and inodes. The inodes, in turn, provide the information
necessary to reconstruct the associated ﬁle. Collection roots also
record the collection’s soft links. Figure 2 shows the pseudocode
for the publish algorithm.
A digitally signed collection root is padded to the same size as
a server block, and also gets indexed by SHA-1 hash. Roots can
therefore become entangled just as other server blocks. Soft links
not only contain a target collection’s public key, but also the target’s
version number at the time of publication, and its root block’s hash.
The version number ensures that a soft link will never be interpreted
to point to an older version of the collection than the one visible to
the publisher. The addition of the root block hashes ensure that the
collection root can be reconstructed if it cannot be found, in the
block pool, via public key lookup.
4.5 Retrieval
The storage network implementing the public block pool must
allow anonymous queries. Users reconstructing documents need to
retrieve a speciﬁc block by hash value without revealing their iden-
tity. Server blocks retrieved from the public block pool are tamper-
checked by simply computing the SHA-1 hash of the block’s con-
tents and comparing it to the SHA-1 hash by which the block was
named. Similarly, the signatures on collection roots must be veri-
ﬁed.
Once in possession of a veriﬁed collection root, a server can at-
tempt to reconstruct any of the ﬁles stored in (or named by) the
collection. As you will recall, that the name of the server blocks
needed to reconstruct a ﬁle’s inode are listed in the collection root.
The ﬁle’s inode contains the names of all of the server blocks needed
to reconstruct the ﬁle. Only a portion of the blocks listed in the in-
ode will be needed. For example, an entangled data block requires
only three of the four server blocks recorded for it in the inode.
Once the necessary server blocks are retrieved, the reconstruction
algorithm (Section 5.3) is applied to the blocks.
1304.6 Update
In order to update a collection one simply republishes it using
the same public/private key pair that was used to originally publish
the collection, but a higher version number (for instance the date
is an adequate version number). Files that have not changed since
the previous version of a collection do not need to be reentangled.
If the public pool contains two or more collection roots possessing
the same public key, the lookup algorithm must return the root with
the latest version number.
5. ENTANGLEMENT
In this section we detail the block entanglement and reconstruc-
tion algorithms. As the entanglement process relies on Shamir’s
secret sharing algorithm, we begin by brieﬂy describing that algo-
rithm.
5.1 Secret Sharing
Shamir [18] described a method of dividing up a secret, s, into n
pieces such that only k ≤ n of them are necessary to later re-form
the secret. Any combination of less than k pieces reveals nothing
about the secret. The pieces are called shares or shadows and the
secret, s, is represented as an element in a ﬁnite ﬁeld.
To form a set of n shares one ﬁrst constructs a polynomial of
degree k − 1 such that s is the y intercept of the polynomial. The