title:Proofs of Data Residency: Checking whether Your Cloud Files Have Been
Relocated
author:Hung Dang and
Erick Purwanto and
Ee-Chien Chang
Proofs of Data Residency: Checking whether Your Cloud
Files Have Been Relocated
Hung Dang, Erick Purwanto, Ee-Chien Chang
School of Computing, National University of Singapore
{hungdang, erickp, changec}@comp.nus.edu.sg
ABSTRACT
While cloud storage services oﬀer manifold beneﬁts such as
cost-eﬀectiveness or elasticity, there also exist various secu-
rity and privacy concerns. Among such concerns, we pay our
primary attention to data residency – a notion that requires
outsourced data to be retrievable in its entirety from local
drives of a storage server in-question. We formulate such no-
tion under a security model called Proofs of Data Residency
(PoDR). PoDR can be employed to check whether the
data are replicated across diﬀerent storage servers, or com-
bined with storage server geolocation to “locate” the data
in the cloud. We make key observations that the data resi-
dency checking protocol should exclude all server-side com-
putation and that each challenge should ask for no more
than a single atomic fetching operation. We illustrate chal-
lenges and subtleties in protocol design by showing poten-
tial attacks to naive constructions. Next, we present a se-
cure PoDR scheme structured as a timed challenge-response
protocol. Two implementation variants of the proposed so-
lution, namely N-ResCheck and E-ResCheck, describe an
interesting use-case of trusted computing, in particular the
use of Intel SGX, in cryptographic timed challenge-response
protocols whereby having the veriﬁer co-locating with the
prover oﬀers security enhancement. Finally, we conduct ex-
tensive experiments to exhibit potential attacks to insecure
constructions and validate the performance as well as the
security of our solution.
1.
INTRODUCTION
The growth of information has made data-generation out-
pace storage availability [39]. This has given rise to cloud
data storage models, as oﬀered by various well-known cloud
service providers [4]. Cloud storage models have gained
signiﬁcant popularity, and oﬀer manifold beneﬁts including
cost-eﬀectiveness and elasticity. They present data owners
with a simple view of the outsourced ﬁles, abstracting away
underlying ﬁle-layout and storage mechanisms. While the
abstraction is appealing, the lack of understanding of the
underlying mechanisms adds to various security concerns on
whether the service providers are upholding the service level
agreement contract (SLA).
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
ASIA CCS ’17, April 02-06, 2017, Abu Dhabi, United Arab Emirates
c(cid:13) 2017 ACM. ISBN 978-1-4503-4944-4/17/04. . . $15.00
DOI: http://dx.doi.org/10.1145/3052973.3053016
1
Various real-world incidents and application scenarios
have demonstrated that those security concerns are realistic.
A cloud crash disaster could permanently destroy the out-
sourced data [2]. Such a threat prompts the needs of testing
for fault tolerance of the storage system [17]. In addition,
various legislation and directives regulating the possessing
and storage of data across national borders advocate paying
attention to locations at which the ﬁles are maintained [30,
1, 3]. In view of these concerns, it is desired to have technical
means that verify whether the ﬁles are indeed maintained in
accordance with the agreements.
Existing works have discussed technical means to audit
cloud storage providers on how the outsourced data are
maintained. Bowers et al. tested fault tolerance of the stor-
age system by checking if ﬁles are replicated across diﬀerent
drives. Gondree et al. [22] and Benson et al. [15] attempted
to “geolocate” the data in the cloud. Nevertheless, due to the
noisy network environment, it is still technically challenging
to attain accurate and reliable assurances.
In this paper, we take a diﬀerent approach to address
these problems, focusing on a more modest goal of verifying
residency of the outsourced data in a server. We ask for a
proof asserting the fact that an outsourced ﬁle F is indeed
maintained in its entirety on the local drives of the server in-
question. It is worth noting that the proof of data residency
provides more assurance on the data’s maintenance than
just the retrievability of the data, which has been exten-
sively studied under the notions of Proofs of Retrievability
(PoR) and Provable Data Possession (PDP) [26, 13]. Fur-
ther, attesting data residency can be an integral component
in auditing contractual assurances, for which existing tech-
nical means appear insuﬃcient. For instances, one can ﬁrst
geographically locate a storage server in-question [29, 24],
and then attest the residency of the outsourced ﬁle on such
server to aﬃrm geolocation of the data. Moreover, one can
also assess replications of the data at diﬀerent geographi-
cally separate servers by checking the residency of the ﬁle
on each of the servers simultaneously.
We formulate the notion of data residency under a security
model called Proof of Data Residency, taking into consider-
ation behaviours of storage devices and capabilities of dis-
honest storage providers (i.e. adversaries). The adversaries
could potentially derive accurate estimation of the network
noise, and exploit parallelism, data compression techniques
or hardware accelerations to inﬂuence the challenge-response
latencies, which are the main sources of information to be re-
lied on in residency checking. In view of these challenges, we
introduce a notion of atomic fetching operation – which the
prover must invoke in every challenge-response interaction –
and consider a powerful adversary that can reduce process-
ing time of any challenge to the equivalent of a single atomic
fetching, and fully aware of the network noise.
We propose techniques to attest data residency. A
data residency checking is structured as a timed challenge-
response protocol. Our solutions adopt an authenticator-
based PoR [28, 26] as an underlying cryptographic prim-
itive to attest the ﬁle’s retrievability, and assess the re-
sponse latencies to establish the data residency. To this
end, we discuss two implementation variants. The ﬁrst
variant, namely N-ResCheck, conducts the data residency
checking over the network, while the second variant, dubbed
E-ResCheck, necessitates the presence of a trusted unit on
the server in-question. With recent initiatives on trusted
computing primitives, especially Intel SGX technology be-
ing available on commodity systems [6], it is interesting to
investigate the security of a timed challenge-response proto-
col wherein the veriﬁer resides in the protected enclave on
the prover’s physical server.
Our study suggests two general guidelines in the design
of an eﬃcient and secure PoDR protocol. First, it is neces-
sary to minimise the computation carried out by the prover.
Preferably, during the veriﬁcation process, the prover should
only fetch and send data to the veriﬁer. Previous works
[22, 15] also advocated no server-side computation. Interest-
ingly, their arguments are motivated by practical concerns
on usability (since the cloud storage’s API may not be ex-
tensive enough to support the required computation) and
cost-savings. In contrast, our observations are motivated by
the security requirements. This guideline explains our choice
of the authenticator-based PoR scheme as the underlying
cryptographic primitive. Secondly, it is crucial to lower the
response latencies incurred by an honest prover. This sug-
gests that each challenge should only ask for one data block
and that the size of data blocks in use should be small (say
64 bytes). We empirically demonstrate that protocols which
fail to adhere to these suggestions are likely susceptible to
evasion. The requirement on small block size further entails
the use of short authentication tags (say 16 bits) so as to
keep the storage expansion factor reasonable. Readers may
wonder if this will raise security concerns wherein short au-
thentication tags are vulnerable to chosen-message attacks.
We argue that these attacks are irrelevant in our applica-
tion settings, for the adversary has only limited access to
the veriﬁcation oracle. Moreover, one can always reduce the
probability of successful attacks by increasing the number
of challenges (see Section 7.5). We elaborate on eﬀects of
block size and authentication tag length in Section 7.
Our empirical studies show that for insecure construc-
tions, the adversary can evade detection. The experiment
results on our proposed solution support the need of small
block size (e.g., 64 bytes in all of our settings). Very low
false acceptance rate and storage overhead can be achieved
with authentication tags that are as small as 16 bits. The ex-
periments also demonstrate signiﬁcant security improvement
obtained by incorporating trusted computing. In particular,
for the same performance requirements of 24% storage ex-
pansion (among which 21% due to error-erasure code and
another 3% due to authentication tags) and audit with 300
challenges, E-ResCheck achieves an order of magnitude
lower false acceptance rate (3.9 × 10−10 vs. 6.7 × 10−09)
and several orders of magnitude lower false rejection rate
(2.6×10−22 vs. 7.3×10−08) in comparison to N-ResCheck.
This illustrates an interesting use-case of trusted computing
where having the veriﬁer of a cryptographic protocol co-
locating with the prover enhances security.
In summary, our paper makes the following contributions:
• We present the security deﬁnition of Proofs of Data
Residency in a presence of a powerful adversary who
is able to reduce the time taken for processing any
challenge down to the equivalent of an atomic fetching
operation and fully aware of the network noise.
• We discuss and empirically show potential attacks on
insecure PoDR constructions.
• We propose a secure and eﬃcient PoDR protocol and
analyse its security. We describe two implementation
variants of the proposed protocol: N-ResCheck and
E-ResCheck, illustrating an interesting use-case of
trusted computing, in particular the use of Intel SGX,
in cryptographic timed challenge-response protocols.
• We conduct extensive experiments to evaluate our so-
lution, and show that the proposed PoDR protocol
obtains negligible false acceptance and false rejection
rates with reasonable storage overhead and audit size.
The rest of this paper is organized as follows. We pro-
vide background on pertinent notions of PoR, geolocation
and Intel SGX in Section 2 before stating our problem in
Section 3. Next, we present our deﬁnition of Proofs of Data
Residency in Section 4. We discuss potential attacks on inse-
cure constructions in Section 5 and propose a secure protocol
in Section 6. Experimental evaluation is presented in Sec-
tion 7 while related works are surveyed in Section 8. Finally,
we conclude our work in Section 9.
2. PRELIMINARIES
In this section, we brieﬂy provide background on the re-
lated notions of PoR and host geolocation, as well as sum-
marize key characteristics of Intel SGX technology. We defer
more detailed discussion of these notions to Appendix B.
Proof of retrievability [26] enables data owners to audit
the storage server on the intactness of their outsourced ﬁles.
Prior to outsourcing the data to the cloud, the data owner
encodes her original data using a redundant encoding (such
as the error-erasure Reed-Solomon code [33]), and authenti-
cates all the blocks of the encoded data. In order to assert
the retrievability of her data, the data owner engages the
storage provider in a challenge-response protocol, checking
for the authenticity of λ blocks, where λ is a security param-
eter. Due to the redundant encoding, the storage provider
has to discard or tamper with a considerable portion of the
blocks to cause data loss. Should such incident happen, it
will be detected by the veriﬁer’s “spot-checking” with over-
whelming probability.
The notion of data residency implicitly requires knowl-
edge of the storage server’s geographic location. Host geolo-
cation techniques [29, 24, 18] enable the veriﬁer to obtain
such information. One approach is to match the interme-
diary nodes in the routing information of a packet against
those of the backbone Internet providers (whose geolocation
is known) to locate a target host (the destination of the
packet in question) [18]. Other approaches rely on latencies
in transmitting a packet between a pair of hosts to approx-
imate geographical distance among them, or a combination
of partial IP-to-location and BGP preﬁx information to de-
rive the target host’s location [29].
2
Intel SGX [6] is a set of extensions that provision pro-
tected execution environments (aka trusted environments or
enclaves). The trusted processor preserves the conﬁdential-
ity and integrity of the code and data loaded into the enclave
against the untrusted OS or any other processes/software by
blocking any non-enclave code’s attempt to read or write the
enclave’s memory.
the fetching time includes the transmission time be-
tween the prover and the remote server, and the time
incurred by the remote server in loading the data from
its storage device.
• Computation time, which is the total time taken by P
in producing the response from the data fetched.
3. THE PROBLEM
3.1 Overview
We consider a model comprising of two entities. The data
owner wishes to outsource a ﬁle F to a storage server and
insists that her data are maintained locally at the storage
server. A dishonest storage server has various economic in-
centives to violate the agreement and may move some of the
data to other remote servers. Hence, the data owner would
like to periodically verify that the ﬁle F can be retrieved in
its entirety from data maintained on the server’s local drives.
We refer to such veriﬁcation as a data residency checking
protocol. In data residency checking, the data owner plays
a role of a veriﬁer V, while the storage server plays a role of
a prover P. Hereafter, we shall refer to the data owner as
veriﬁer, and storage provider as prover.
A data residency checking is structured as a timed
challenge-response protocol. It consists of several challenge-
response exchanges and for each response, V also captures
the response latency (i.e. round trip time between the chal-
lenge and response). At the end of the protocol, V relies on
the validity of the responses, as well as their latencies, to
decide on accepting or rejecting the veriﬁcation.
The retrievability of F can be checked using techniques
in PoR [26, 36], whereas the assurance of storage locality
relies on the response latency. Nevertheless, simply adopt-
ing a secure PoR scheme together with latency assessment
does not necessarily provide the assurance on data residency,
since a dishonest server (i.e. adversary) could, through par-
allelism or over-clocking the processor, distort the latency
measurements. Fortunately, the desired assurance is still
possible, based on a premise that P has to invoke some
atomic operations to prepare for each response, and such
operations would take longer time when the data are stored
remotely. The goal of our security model is to capture the
above-mentioned factors.
3.2 Timing measurements
The response latency of a challenge is the round-trip-time
of the challenge and response (i.e., the elapsed time between
the moment the challenge is sent and the moment when the
corresponding response is received). The latency consists of
the following three portions:
• Challenge-response transmission time, which is in-
curred by transmission of the challenge and response
between V and P. In the trusted computing setting
where both V and P reside in the same physical sys-
tem, such transmission time is short.
In the setting
where V and P are connected in a networked environ-
ment, the time is signiﬁcantly larger and subject to
higher level of noise.
• Fetching time, which is incurred when P fetches the
required data from the storage.
In cases where the
prover fetches the data from another remote server,
3
All these timings are probabilistic, and we call their distri-
butions the environment proﬁle E.
3.3 Threats Model: Adversary’s capability
We consider an adversary that is a dishonest prover,
having complete control over the storage,
server and
network within its premises. That is, the adversary is able
to reduce the response latency in various fashions:
Computation time. The adversary could speedup the com-
putation time, for example via over-clocking or parallelism.
Since it is diﬃcult to bound the speedup factor which the
adversary can possibly achieve, we consider an adversarial
model wherein the computation time is not included in the
response latency for worst-case analysis. Note that this does
not imply arbitrary computation speedup by the adver-
sary, and we still require the prover P to be polynomial time.
Fetching time. When the data are stored on the prover’s
local drives, the fetching time is simply that of a read
from the local storage hardware. On the other hand, if
the data to be fetched is stored remotely, the fetching time
comprises the time taken to execute a read on the remote
storage device, and the time required to transmit the data
from the remote storage to the prover. A dishonest prover
could apply various techniques such as data compression
or distributed ﬁle system to reduce the storage loading, or
the network transmission time, which in-turn reduces the
fetching time. To account for this ﬂexibility, we consider the
fetching of a single byte as atomic, and give the adversary
the capability of reducing the time taken to fetch any
amount of data to the equivalent of fetching a single byte.
Noise Measurement. Due to the noisy environment, all the
timing measurements are probabilistic. Nevertheless, the
adversary may be able to get a good estimate of the actual
measurements.
Such knowledge can help the adversary
in increasing the chance of evasion. For example, let us
consider an adversary who keeps half of the data blocks
in local drives, and stores the rest of the data blocks in
a remote storage.
If a challenge asks for a block stored
in a remote storage, he could either retrieve it from the
remote storage or forge the response. The knowledge of
the actual response latency incurred by fetching the block
from remote storage and that by reading it from local
drives would beneﬁt the adversary.
if the
former is faster than the latter (perhaps due to network
congestion), the adversary will retrieve the correct block
from remote storage; otherwise, he may choose to forge the
block to meet the timing measurement constraint. In our
adversarial model, to acknowledge the adversary’s ability to
accurately estimate the timing measurements, we assume
the adversary knows the actual measurements of all timings
right in the beginning of the veriﬁcation session.
In particular,
3.4 Threats Model: Adversary’s limitations
Limited access to veriﬁcation oracle.
The adversary at-
tempting to forge the authentication tags could exploit the
veriﬁer as the veriﬁcation oracle. We assume that the data
owner could tolerate a few delayed or missing responses to,
she would not accept invalid responses whose authentica-
tion tags do not checkout. While the adversary may argue
that a response is not valid due to hardware failure, such
event is highly unlikely, for most storage and networking
system have error detection/recovery mechanisms in-place.
Therefore, the adversary has few chances of providing in-
valid responses. In other words, it has limited access to a
veriﬁcation oracle.
As mentioned earlier, one of our key observations is the
enhancement brought by small block size, which entails the
use of short authentication tags (say 16 bits per tag) in or-
der to keep the storage expansion factor reasonable. While
short authentications are vulnerable to chosen-message
attacks wherein the adversary has unlimited accesses to
the veriﬁcation oracle [36], such attacks are irrelevant in
our application settings, for the adversary has only limited
access to the veriﬁcation oracle
Atomic operation. A key assumption in our work is that of
an atomic operation, which would take longer time when the
data are stored remotely compared to when they are stored
on the storage server’s local drives. This assumption in-turn
is based on a premise that there are no technically feasible
and/or economically feasible means for the dishonest servers
to reduce the time.
In cases where the above mentioned
premise is not met, unfortunately, the assurance provided by
our schemes will not hold. Examples of those cases include
an adversary who claims to use rotational disks for local
drives, but employs ﬂash storage (e.g., SSD) in a remote
server and connects to it via an out-of-band communication
channel such that the overall throughput outpaces that of
the local drives. Nevertheless, it is arguably reasonable to
assume that such out-of-band channel is not available and
that the service providers are economically rational (i.e., it
would not aﬀord arbitrarily large and expensive resources in
evading the data residency checking) and will employ storage