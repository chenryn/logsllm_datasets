title:SwitchBlade: a platform for rapid deployment of network protocols
on programmable hardware
author:Muhammad Bilal Anwer and
Murtaza Motiwala and
Muhammad Mukarram Bin Tariq and
Nick Feamster
SwitchBlade: A Platform for Rapid Deployment of Network
Protocols on Programmable Hardware
Muhammad Bilal Anwer, Murtaza Motiwala, Mukarram bin Tariq, Nick Feamster
School of Computer Science, Georgia Tech
ABSTRACT
We present SwitchBlade, a platform for rapidly deploying cus-
tom protocols on programmable hardware. SwitchBlade uses a
pipeline-based design that allows individual hardware modules to
be enabled or disabled on the (cid:3)y, integrates software exception han-
dling, and provides support for forwarding based on custom header
(cid:2)elds. SwitchBlade’s ease of programmability and wire-speed per-
formance enables rapid prototyping of custom data-plane functions
that can be directly deployed in a production network. SwitchBlade
integrates common packet-processing functions as hardware mod-
ules, enabling different protocols to use these functions without
having to resynthesize hardware. SwitchBlade’s customizable for-
warding engine supports both longest-pre(cid:2)x matching in the packet
header and exact matching on a hash value. SwitchBlade’s software
exceptions can be invoked based on either packet or (cid:3)ow-based
rules and updated quickly at runtime, thus making it easy to inte-
grate more (cid:3)exible forwarding function into the pipeline. Switch-
Blade also allows multiple custom data planes to operate in parallel
on the same physical hardware, while providing complete isolation
for protocols running in parallel. We implemented SwitchBlade us-
ing NetFPGA board, but SwitchBlade can be implemented with any
FPGA. To demonstrate SwitchBlade’s (cid:3)exibility, we use Switch-
Blade to implement and evaluate a variety of custom network pro-
tocols: we present instances of IPv4, IPv6, Path Splicing, and an
OpenFlow switch, all running in parallel while forwarding packets
at line rate.
Categories and Subject Descriptors:
C.2.1 [Computer-
Communication Networks]: Network Architecture and Design
C.2.5 [Computer-Communication Networks]: Local and Wide-
Area Networks C.2.6 [Computer-Communication Networks]: In-
ternetworking
General Terms: Algorithms, Design, Experimentation, Perfor-
mance
Keywords: Network Virtualization, NetFPGA
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for pro(cid:2)t or commercial advantage and that copies
bear this notice and the full citation on the (cid:2)rst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speci(cid:2)c
permission and/or a fee.
SIGCOMM 2010, August 30-September 3, 2010, New Delhi, India.
Copyright 2010 ACM 978-1-4503-0201-2/10/08 ...$10.00.
1.
INTRODUCTION
Countless next-generation networking protocols at various lay-
ers of the protocol stack require data-plane modi(cid:2)cations. The past
few years alone have seen proposals at multiple layers of the pro-
tocol stack for improving routing in data centers, improving avail-
ability, providing greater security, and so forth [3,13,17,24]. These
protocols must ultimately operate at acceptable speeds in produc-
tion networks(cid:151)perhaps even alongside one another(cid:151)which raises
the need for a platform that can support fast hardware implemen-
tations of these protocols running in parallel. This platform must
provide mechanisms to deploy these new network protocols, header
formats, and functions quickly, yet still forward traf(cid:2)c as quickly
as possible. Unfortunately, the conventional hardware implemen-
tation and deployment path on custom ASICs incurs a long devel-
opment cycle, and custom protocols may also consume precious
space on the ASIC. Software-de(cid:2)ned networking paradigms (e.g.,
Click [7, 16]) offer some hope for rapid prototyping and deploy-
ment, but a purely software-based approach cannot satisfy the strict
performance requirements of most modern networks. The network-
ing community needs a development and deployment platform that
offers high performance, (cid:3)exibility, and the possibility of rapid pro-
totyping and deployment.
Although other platforms have recognized the need for fast, pro-
grammable routers, they stop somewhat short of providing a pro-
grammable platform for rapid prototyping on the hardware itself.
Platforms that are based on network processors can achieve fast
forwarding performance [22], but network processor-based imple-
mentations are dif(cid:2)cult to port across different processor architec-
tures, and customization can be dif(cid:2)cult if the function that a proto-
col requires is not native to the network processor’s instruction set.
All other functions should be implemented in software. PLUG [8]
is an excellent framework for implementing modular lookup mod-
ules, but the model focuses on manufacturing high-speed chips,
which is costly and can have a long development cycle. Route-
Bricks [12] provides a high-performance router, but it is imple-
mented entirely in software, which may introduce scalability is-
sues; additionally, prototypes developed on RouteBricks cannot be
easily ported to hardware.
This paper presents SwitchBlade, a programmable hardware
platform that strikes a balance between the programmability of
software and the performance of hardware, and enables rapid pro-
totyping and deployment of new protocols. SwitchBlade enables
rapid deployment of new protocols on hardware by providing mod-
ular building blocks to afford customizability and programmabil-
ity that is suf(cid:2)cient for implementing a variety of data-plane func-
tions. SwitchBlade’s ease of programmability and wire-speed per-
formance enables rapid prototyping of custom data-plane functions
that can be directly deployed in a production network. SwitchBlade
183relies on (cid:2)eld-programmable gate arrays (FPGAs). Designing and
implementing SwitchBlade poses several challenges:
(cid:15) Design and implementation of a customizable hardware
pipeline. To minimize the need for resynthesizing hardware,
which can be prohibitive if multiple parties are sharing it,
SwitchBlade’s packet-processing pipeline includes hardware
modules that implement common data-plane functions. New
protocols can select a subset of these modules on the (cid:3)y,
without resynthesizing hardware.
(cid:15) Seamless support for software exceptions.
If custom pro-
cessing elements cannot be implemented in hardware (e.g.,
due to limited resources on the hardware, such as area on
the chip), SwitchBlade must be able to invoke software rou-
tines for processing. SwitchBlade’s hardware pipeline can
directly invoke software exceptions on either packet or (cid:3)ow-
based rules. The results of software processing (e.g., for-
warding decisions), can be cached in hardware, making ex-
ception handling more ef(cid:2)cient.
(cid:15) Resource isolation for simultaneous data-plane pipelines.
Multiple protocols may run in parallel on same hardware;
we call each data plane a Virtual Data Plane (VDP). Switch-
Blade provides each VDP with separate forwarding tables
and dedicated resources. Software exceptions are the VDP
that generated the exception, which makes it easier to build
virtual control planes on top of SwitchBlade.
(cid:15) Hardware processing of custom, non-IP headers. Switch-
Blade provides modules to obtain appropriate (cid:2)elds from
packet headers as input to forwarding decisions. Switch-
Blade can forward packets using longest-pre(cid:2)x match on 32-
bit header (cid:2)elds, an exact match on (cid:2)xed length header (cid:2)eld,
or a bitmap added by custom packet preprocessing modules.
The design of SwitchBlade presents additional challenges, such as
(1) dividing function between hardware and software given limited
hardware resources; (2) abstracting physical ports and input/output
queues; (3) achieving rate control on per-VDP basis instead of per-
port basis; and (4) providing a clean interface to software.
We have implemented SwitchBlade using the NetFPGA
board [2], but SwitchBlade can be implemented with any FPGA.
To demonstrate SwitchBlade’s (cid:3)exibility, we use SwitchBlade to
implement and evaluate several custom network protocols. We
present instances of IPv4, IPv6, Path Splicing, and an OpenFlow
switch, all of which can run in parallel and forward packets at
line rate; each of these implementations required only modest ad-
ditional development effort. SwitchBlade also provides seamless
integration with software handlers implemented using Click [16],
and with router slices running in OpenVZ containers [20]. Our
evaluation shows that SwitchBlade can forward traf(cid:2)c for custom
data planes(cid:151)including non-IP protocols(cid:151)at hardware forwarding
rates. SwitchBlade can also forward traf(cid:2)c for multiple distinct
custom data planes in parallel, providing resource isolation for
each. An implementation of SwitchBlade on the NetFPGA plat-
form for four parallel data planes (cid:2)ts easily on today’s NetFPGA
platform; hardware trends will improve this capacity in the future.
SwitchBlade can support additional VDPs with less than a linear
increase in resource use, so the design will scale as FPGA capacity
continues to increase.
The rest of this paper is organized as follows. Section 2 presents
related work. Section 3 explains our design goals and the key re-
sulting design decisions. Section 4 explains the SwitchBlade de-
sign, and Section 5 describes the implementation of SwitchBlade,
as well as our implementations of three custom data planes on
SwitchBlade. Section 6 presents performance results. Section 7
brie(cid:3)y describes how we have implemented a virtual router on top
of SwitchBlade using OpenVZ. We discuss various extensions in
Section 8 and conclude in Section 9.
2. RELATED WORK
We survey related work on programmable data planes in both
software and hardware.
The Click [16] modular router allows easy, rapid development
of custom protocols and packet forwarding operations in software;
kernel-based packet forwarding can operate at high speeds but can-
not keep up with hardware for small packet sizes. An off-the-shelf
NetFPGA-based router can forward traf(cid:2)c at 4 Gbps; this forward-
ing speed can scale by increasing the number of NetFPGA cards,
and development trends suggest that much higher rates will be pos-
sible in the near future. RouteBricks [12] uses commodity pro-
cessors to achieve software-based packet processing at high speed.
The design requires signi(cid:2)cant PCIe interconnection bandwidth to
allow packet processing at CPUs instead of on the network cards
themselves. As more network interface cards are added, and as
traf(cid:2)c rates increase, however, some packet processing may need
to be performed on the network cards themselves to keep pace with
increasing line speeds and to avoid creating bottlenecks on the in-
terconnect.
Supercharged PlanetLab (SPP) [22] is a network processor (NP)-
based technology. SPP uses Intel IXP network processors [14]
for data-plane packet processing. NP-based implementations are
speci(cid:2)cally bound to the respective vendor-provided platform,
which can inherently limit the (cid:3)exibility of data-plane implemen-
tations.
Another solution to achieve wire-speed performance is develop-
ing custom high-speed networking chips. PLUG [8] provides a pro-
gramming model for manufacturing chips to perform high-speed
and (cid:3)exible packet lookup, but it does not provide an off-the-shelf
solution. Additionally, chip manufacturing is expensive: fabrica-
tion plants are not common, and cost-effective manufacturing at
third-party facilities requires critical mass of demand. Thus, this
development path may only make sense for large enterprises and
for protocols that have already gained broad acceptance. Chip man-
ufacturing also has a high turnaround time and post-manufacturing
veri(cid:2)cation processes which can impede development of new pro-
tocols that need small development cycle and rapid deployment.
SwitchBlade is an FPGA-based platform and can be imple-
mented on any FPGA. Its design and implementation draws in-
spiration from our earlier work on designing an FPGA-based data
plane for virtual routers [4]. FPGA-based designs are not tied to
any single vendor, and it scales as new, faster and bigger FPGAs
become available. FPGAs also provide a faster development and
deployment cycle compared to chip manufacturing.
Casado et al. argue for simple but high-speed hardware with
clean interfaces with software that facilitate independent devel-
opment of protocols and network hardware [9]. They argue that
complex routing decisions can be made in software and cached in
hardware for high-speed processing; in some sense, SwitchBlade’s
caching of forwarding decisions that are handled by software ex-
ception handlers embodies this philosophy. OpenFlow [19] en-
ables the rapid development of a variety of protocols, but the di-
vision of functions between hardware and software in SwitchBlade
is quite different. Both OpenFlow and SwitchBlade provide soft-
ware exceptions and caching of software decisions in hardware,
but SwitchBlade also provides selectable hardware preprocessing
modules that effectively moves more (cid:3)exible processing to hard-
184ware. SwitchBlade also easily accommodates new hardware mod-
ules, while OpenFlow does not.
SwitchBlade provides wire-speed support
for parallel cus-
tomized data planes, isolation between them, and their interfac-
ing with virtualization software, which would make SwitchBlade a
suitable data plane for a virtual router. OpenFlow does not directly
support multiple custom data planes operating in parallel. FlowVi-
sor [1] provides some level of virtualization but sits between the
OpenFlow switch and controller, essentially requiring virtualiza-
tion to occur in software.
3. GOALS AND DESIGN DECISIONS
The primary goal of SwitchBlade is to enable rapid development
and deployment of new protocols working at wire-speed. The three
subgoals, in order of priority, are: (1) Enable rapid development
and deployment of new protocols; (2) Provide customizability and
programmability while maintaining wire-speed performance; and
(3) Allow multiple data planes to operate in parallel, and facilitate
sharing of hardware resources across those multiple data planes.
In this section, we describe these design goals, their rationale, and
highlight speci(cid:2)c design choices that we made in SwitchBlade to
achieve these goals.
Goal #1. Rapid development and deployment on fast hard-
ware. Many next-generation networking protocols require data-
plane modi(cid:2)cations. Implementing these modi(cid:2)cations entirely in
software results in a slow data path that offers poor forwarding per-
formance. As a result, these protocols cannot be evaluated at the
data rates of production networks, nor can they be easily transferred
to production network devices and systems.
Our goal is to provide a platform for designers to quickly de-
ploy, test, and improve their designs with wire-speed performance.
This goal in(cid:3)uences our decision to implement SwitchBlade using
FPGAs, which are programmable, provide acceptable speeds, and
are not tied to speci(cid:2)c vendors. An FPGA-based solution can al-
low network protocol designs to take advantage of hardware trends,
as larger and faster FPGAs become available. SwitchBlade relies
on programmable hardware, but incorporates software exception
handling for special cases; a purely software-based solution cannot
provide acceptable forwarding performance. From the hardware
perspective, custom ASICs incur a long development cycle, so they
do not satisfy the goal of rapid deployment. Network processors
offer speed, but they do not permit hardware-level customization.
Goal #2. Customizability and programmability. New proto-
cols often require speci(cid:2)c customizations to the data plane. Thus,
SwitchBlade must provide a platform that affords enough cus-
tomization to facilitate the implementation and deployment of new
protocols.
Providing customizability along with fast turnaround time for
hardware-based implementations is challenging:
a bare-bones
FPGA is customizable, but programming from scratch has a high
turnaround time. To reconcile this con(cid:3)ict, SwitchBlade recognizes
that even custom protocols share common data-plane extensions.
For example, many routing protocols might use longest pre(cid:2)x or
exact match for forwarding, and checksum veri(cid:2)cation and update,
although different protocols may use these extensions on different
(cid:2)elds on in the packets. SwitchBlade provides a rich set of common
extensions as modules and allows protocols to dynamically select
any subset of modules that they need. SwitchBlade’s modules are
programmable and can operate on arbitrary offsets within packet
headers.
Feature
Virtual Data Plane (x 4.2)
Customizable
modules (x 4.3)
hardware
Flexible matching in for-
warding (x 4.4)
Programmable software ex-
ceptions (x 4.5)
Design Goals
Parallel custom data
planes
Rapid programming,
customizability
Pipeline Stages
VDP selection
Preprocessing,
Forwarding
Customizability
Forwarding
Rapid programming,
customizability
Forwarding
Table 1: SwitchBlade design features.
For extensions that are not included in SwitchBlade, protocols
can either add new modules in hardware or implement exception
handlers in software. SwitchBlade provides hardware caching for
forwarding decisions made by these exception handlers to reduce
performance overhead.
Goal #3. Parallel custom data planes on a common hardware
platform. The increasing need for data-plane customization for
emerging network protocols makes it necessary to design a plat-
form that can support the operation of several custom data planes
that operate simultaneously and in parallel on the same hardware
platform. SwitchBlade’s design identi(cid:2)es functions that are com-
mon across data-plane protocols and provides those implementa-
tions shared access to the hardware logic that provides those com-
mon functions.
SwitchBlade allows customized data planes to run in parallel.
Each data plane is called a Virtual Data Plane (VDP). Switch-
Blade provides separate forwarding tables and virtualized inter-
faces to each VDP. SwitchBlade provides isolation among VDP
using per-VDP rate control. VDPs may share modules, but to pre-
serve hardware resources, shared modules are not replicated on the