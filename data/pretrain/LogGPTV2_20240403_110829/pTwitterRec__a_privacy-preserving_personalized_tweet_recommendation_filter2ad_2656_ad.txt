late  the  most  popular  words  with  their  indexes  at  certain  proba-
bilities. However, we argue the impact of such attacks is negligi-
ble for reasons explained in Section 6. 
As a result, for a given tuple  in D, the final data submit-
ted to the SP by user u is as follows:  
1
h
Z
∑
∈
w T
h
q
w
1
k
Z
−∑
q
w
∈
w T
k
ˆ uep
Figure 1. Privacy-preserving model learning. 
1
k
∑
Z ∈
w T
k
q
w
1
k
∑
Z ∈
w T
k
q
w
∑
b r
j
, ,
u k
j
j F∈
sp
Figure 2. Tweet publishing, receiving and ranking. 
In  pTwitterRec,  we  split  the  task  of  computing  explicit  features 
between users and the SP (as listed in Table 1). Users are respon-
sible  for  computing  those  features  that  depend  on  the  content  of 
the  tweet,  for  example,  relevance  to  tweet  history  feature,  which 
estimates the relevance between the tweet and the user’s previous 
tweet history. The SP is responsible for computing those features 
that  depend  on  non-sensitive  global  information  that  is  available 
to  the  SP,  for  example,  co-follow  score  feature,  which  estimates 
the similarity of the followee sets of the recipient and the publish-
er of the tweet. 
For a given tuple  in D, user u computes the explicit fea-
tures  that  she  is  responsible  for  computing.  Let  the  j-th  explicit 
 respective-
feature computed from tweets k and h be 
and 
,u k
jr
,u h
jr
ly. User u calculates 
,u h
jr
before sending it to the SP. 
,u k
jr
- 
and attaches the result to the tuple 
Finally,  user  u  cannot  simply  replace  each  word  contained  in 
tweets k and h with the corresponding index when submitting the 
tuple   to  the  SP,  because  the  SP  can  conveniently  learn 
each  word  by  decrypting  the  index  with  its  decryption  key.  We 
propose that during system setup, the WS generates a pair of pub-
lic  and  private  keys  (Pukws,  Prkws)  for  a  probabilistic  public-key 
encryption  scheme  denoted  Ews.  For  tweet  k  in  the  tuple,  user  u 
randomly mixes the order of the indexes of all words contained in 
tweet  k,  concatenates  all  indexes  altogether,  and  encrypts  the  in-
dexes  using  Ews  with  the  WS’s  public  key  Pukws.  Consequently, 
the SP learns neither the indexes of words contained in the tweet 
nor the frequency of each word contained in D while the WS only 
)
  (10)
h
Where  Ews(tweetk)  denotes  the  encrypted  set  of  indexes  of  words 
contained in tweet k as previously described, Fu denotes the set of 
explicit features computed by user u, and p(k) denotes the identi-
ties of the publisher of tweets k. Upon receiving the data submit-
ted by user u, the SP computes explicit features (as listed in Table 
1)  that  the  SP  is  responsible  for  computing  from  tweets  k  and  h. 
Combined with the data received from user u, we denote the final 
training dataset for the given tuple  in D as d. 
5.4  Model Learning 
For  tweet  k  received  by  user  u,  in  addition  to  explicit  features 
computed as described in Section 5.3, Chen et al. [10] use a latent 
factor  model  (details  in  Appendix  A.1)  to  capture  user  u’s  inter-
ests in tweet k and propose three categories of latent features: the 
latent factors of user u as a recipient of tweet k in the latent feature 
space Rd denoted as pu, the latent factor of the publisher of tweet k 
in  Rd  denoted  as  dp(k),  and  the  latent  factor  of  each  word  w  con-
tained in tweet k in Rd denoted as qw. User u’s interest in tweet k is 
captured by measuring the affinity between user u and the  words 
contained in tweet k and the affinity between user u and the pub-
lisher of tweet k in the latent feature space Rd. At the model learn-
ing  stage,  the  SP  and  the  WS  cooperate  to  learn  the  values  of 
these  latent  factors,  in  addition  to  the  weight  vector  for  explicit 
features by performing stochastic gradient descent as described in 
Section 3.3. 
In pTwitterRec, during system setup, the SP initializes the weight 
bj for all explicit features, the  latent  factor pu  of  each  user  as  the 
recipient  of  tweets  and  the  latent  factor  dp(k)  of  each  user  as  the 
publisher of tweets with random values. For each training dataset 
d  corresponding  to    in  D,  the  SP  and  the  WS 
engage in the following protocols (as depicted in Figure 1): 
a) 
The SP forwards the encrypted set of word indexes (denoted 
as Ews(tweetk) and Ews(tweeth)) contained in d to the 
WS.  The  WS  obtains  the  corresponding  word  indexes  by 
decrypting  with  its  private  key  Prkws.  If  the  WS  has  never 
come across the index of word w before, the WS initializes 
the latent factor for word w with random values, denoted as 
∑ and 
qw.  Then,  the  WS  calculates 
−∑
q
q
w
w
∈
w T
k
∈
w T
h
1
k
Z
1
h
Z
sends the result back to the SP.  Note  that  the  WS  does  not 
know  the  actual  words  but  only  learns  the  index  of  each 
word contained in tweets k and h.  
b)  Upon  receiving 
1
k
Z
−∑
q
w
∈
w T
k
1
h
Z
∑ from  the  WS,  the 
q
w
∈
w T
h
SP calculates  ˆe using equation 9 and updates parameters bj, 
pu and dp(k) using equations 5, 7 and 8. 
c) 
The  SP  computes  ˆ uep and  sends  the  result  to  the  WS.  The 
WS updates qw for each word w contained in tweets k and h 
370using equation 6. Note that the WS does not learn the identi-
ty of user u. 
The SP and the WS loop over all training datasets in D and update 
the  parameters  of  the  tweet  recommendation  model  in  the  same 
manner  as  mentioned  above.  At  the  end  of  the  model  learning 
stage,  the  SP  learns  bj  for  each  explicit  feature,  pu  for  users  as 
recipients  of  tweets  and  dp(k)  for  users  as  publishers  of  tweets 
while the WS learns qw for words in the tweet vocabulary. 
5.5  Tweet Publishing, Receiving and Ranking 
Once the tweet recommendation model is  learned,  it  can  be  used 
to predict users’ interests in tweets and rank the tweets according-
ly (as depicted in Figure 2).  
In pTwitterRec, users are primarily responsible for ranking tweets 
with  the  help  of  the  SP  and  the  WS.  When  the  recommendation 
model is learned, user u requests from the SP her personal  latent 
factor pu, the latent factors of all her followees as publishers, de-
noted  as  df  where  user  f  is  a  followee  of  user  u,  and  the  system-
wide weight bj for explicit features. Note that user u only needs to 
retrieve these parameters once. 
When publishing a tweet, the user looks up the indexes of words 
contained  in  the  tweet  to  be  published,  mixes  the  order  of  these 
indexes  randomly  and  concatenates  them  together,  encrypts  the 
indexes using Ews with the WS’s public key Pukws and attaches the 
encrypted indexes to the tweet before sending it to the  SP.  Simi-
larly to model learning, the SP forwards the encrypted indexes to 
∑ for  words  contained  in  the 
the  WS,  which  calculates 
q
w
1
k
Z
∈
w T
k
tweet  and  sends  the  result  back  to  the  SP.  The  SP  computes  the 
sum  of  explicit  features  (denoted  as
where  Fsp 
b r
j F∈
sp
∑
, ,
u k
j
j
denotes  the  set  of  explicit  features  that  the  SP  is  responsible  for 
∑ which  was 
computing),  attaches  the  sum  along  with 
q
w
1
k
Z
∈
w T
k
previously received from the WS to the tweet before distributing it 
to corresponding followers. 
Upon receiving the tweet and the attached values, user u decrypts 
the tweet, calculates the explicit features that the user is responsi-
ble for computing, and computes the predicted rating for the tweet 
using equation 4. Tweet ranking  works  as  follows:  When  the  ac-
tual  user,  not  the  pTwitterRec  client  software,  wants  to  read  her 
tweets,  this  process  is  done  for  all  tweets  that  have  arrived  since 
her  last  update  and  then  the  tweets  are  shown  to  her  in  ranked 
order. 
6.  DISCUSSION 
In  this  section,  we  discuss  the  security  and  privacy  aspects  of 
pTwitterRec and a possible extension. 
6.1  Security Analysis 
We discuss possible attacks against pTwitterRec. 
Frequency analysis attacks. At the model learning stage, the WS 
decrypts  the  encrypted  indexes  of  the  words  contained  in  the 
tweets submitted by users as described in Section 5.4. Therefore, 
the WS learns the frequency of each word that has appeared in all 
tweets contained in the training datasets D while not knowing the 
actual words. Should the WS have the background knowledge of 
the  popularity  of  each  word  in  the  tweet  vocabulary,  the  WS 
might  be  able  to  correlate  the  most  popular  words  with  their  in-
dexes at certain probabilities. However, we argue that those words 
that  are  most  vulnerable  against  such  frequency  analysis  attacks 
are  exactly  those  words  that  are  most  common  among  all  users 
and therefore reveal little personal information regarding an indi-
vidual  user.  In  regards  of  the  SP,  even  though  the  SP  knows  the 
identities of the users included in D, the SP cannot learn the fre-
quencies  of  words  because  the  indexes  were  encrypted  using 
probabilistic  encryption  scheme  Ews  with  the  WS’s  public  key 
Pukws.  
Collusion attacks. In our threat model, we assume that the SP and 
the WS do not collude. In addition, since the SP and the WS are 
honest-but-curious  adversaries,  they  would  not  create  phantom 
users  to  interact  with  other  parties.  The  SP  might  try  to  collude 
with some legitimate users. A collusion between the SP and a user 
does  not  reveal  the  indexes  of  words  contained  in  other  users’ 
tweets  as  they  are  encrypted  using  probabilistic  encryption 
scheme  Ews  (we  require  that  Ews  is  also  secure  against  chosen-
plaintext attacks.) with the  WS’s  public  key  Pukws.  However,  the 
SP can learn the latent factors of some words through the collud-
ing user and thereby infer other users’ interests. Similarly, a collu-
sion between  the  WS  and  a  user  discloses  the  mapping  of  words 
and  corresponding  indexes.  However,  we  claim  that  users  who 
collude with either the SP or the WS lose some of their own pri-
vacy. A collusion among a group of users does not pose threats to 
other  users,  because  in  pTwitterRec  users  are  only  allowed  to 
obtain their own personal latent factors from the SP, as described 
in Section 5.5. 
Poisoning attacks. Some malicious users may inject false training 
datasets such as falsified explicit features to render the tweet rec-
ommendation  model  less  effective.  Prior  work  such  as  Orca  [2] 
has  been  proposed  to  detect  such  poisoning  attacks.  Poisoning 
attacks are not introduced as a result of adopting pTwitterRec. We 
leave  the  full  investigation  of  the  impact  of  such  attacks  on  the 
recommendation  accuracy  and  the  applicability  of  existing  de-