### Relationship Analysis and Anomaly Detection

The BLEU score, which is lower during anomalous periods compared to normal times (see Section II-C for more details), is used to identify broken relationships. In the context of sensor pairs, normal relationships are represented by gray edges, indicating that these pairs function normally even during anomalous days. Broken relationships, indicated by red edges, can be used to pinpoint sensors responsible for the anomalies.

- **Figure 9(a)**: Sensors in the upper right and lower right corners (circled with green lines) are identified as problematic.
- **Figure 9(b)**: Almost all relationships are broken (red edges circled in green boxes), suggesting a severe anomaly affecting a significant portion of the sensors.

This diagnostic approach is valuable for system administrators, as it allows them to quickly locate problematic sensors and the source of the anomaly. The analytics framework also provides the option to generate similar visualizations at finer granularities, such as hourly, to illustrate how faults propagate over time. Due to space constraints, these detailed results are not shown here.

#### Key Takeaways for Anomaly Detection:
- **Global Subgraphs**: More suitable for anomaly detection than local subgraphs.
- **Strong Relationships**: Global subgraphs with the strongest relationships (BLEU score above 90) are less useful.
- **Local Subgraphs**: Useful for locating specific sensors responsible for anomalies.
- **Temporal Granularity**: Fault diagnosis can be performed at various time granularities to show fault propagation over time.

### Case Study II: HDD Dataset

To validate our proposed analytics framework, we applied it to a publicly available dataset. Among the datasets on Kaggle, none reported failures or anomalies for categorical data. Therefore, we chose the Backblaze dataset, which provides HDD reliability statistics over time [1]. This dataset primarily consists of continuous features, but with minimal preprocessing, they can be converted into discrete ones.

#### A. Dataset of HDD Failures

The Backblaze dataset includes daily performance logs from hard disk drives (HDDs) at the Backblaze data center. Each day, the drives report a list of SMART attributes, which include cumulative lifetime counts of hardware events and daily summaries of drive activity. Days marked as "failures" indicate that the drive will cease functioning and be removed from production the following day.

#### B. Baseline Models

We selected the best set of features from the 100+ SMART features reported by Backblaze, focusing on those recorded for all disk types, resulting in 20 features. Fourteen of these features are cumulative counts, which we transformed using a first-order difference to yield daily deltas. This resulted in a total of 34 features, including 20 raw SMART features and 14 differenced ones. We focused on Seagate models for enterprise workloads, which account for 35% of the Backblaze data and 46% of the Seagate data in 2018.

- **Random Forest (RF)**: A supervised ensemble model based on decision trees. We used 80% of the drives for training and 20% for testing, with a 1-to-1 majority-to-minority ratio for non-failure cases.
- **One-class SVM (OC-SVM)**: An unsupervised model for anomaly detection. We used the radial basis function (RBF) kernel and sub-sampled non-anomalous observations for training due to scalability issues.

#### C. Multivariate Relationship Graph

Each SMART feature is treated as a sensor in the multivariate relationship graph, with each node representing a feature. Our framework accepts raw SMART features, and after removing four features with minimal variation, 16 nodes and 240 edges were included in the graph. Disks in Backblaze are removed upon failure, so each disk reports only one failure sample. To increase the number of anomalies, we aggregated data from all disks.

- **Discretization Schemes**:
  1. **Binary Discretization**: For features with most observations equal to zero, we use an indicator variable (e.g., Figure 10(a)).
  2. **Percentile Discretization**: For other features, we use the 20th, 40th, 60th, and 80th percentiles as decision boundaries (e.g., Figure 10(b)).

We focused on disks with substantial samples (over 10 months of data) and used the last 4 months for training, development, and testing. The NMT model parameters were set as in the private dataset (Section III-A2).

#### D. Evaluation

Our framework is an unsupervised method designed for discrete event sequences. It leverages the learned multivariate relationship graph to provide information about important features and disk failures. Table II compares our method with baseline models, highlighting its unsupervised nature and applicability to discrete event sequences.

- **Knowledge Discovery**: Figure 11(a) shows global subgraphs with a BLEU score in the [80, 90) range, identifying critical SMART features. Table III lists the top 5 most important features.
- **Anomaly Detection**: Figure 12 illustrates the change in anomaly scores over time for detected and undetected failed disks. A sharp increase in the anomaly score (over 0.5 increment) before the failure date indicates a successful detection. Our method achieved a recall of 58%, comparable to the one-class SVM's 60%.

### Related Work

Understanding the interdependence among multivariate time series is crucial in machine learning. Most existing literature focuses on continuous sequences, using methods like canonical correlation analysis and ARMA models. These methods are effective for continuous data but have limitations for discrete event sequences. Some works focus on mining patterns from discrete categorical event data, but these are less common.