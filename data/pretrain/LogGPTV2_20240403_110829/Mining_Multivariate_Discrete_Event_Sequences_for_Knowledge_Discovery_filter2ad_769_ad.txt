relationships, i.e., the BLEU score is lower than that that
of normal times (see Section II-C for more details). Gray
edges correspond to normal relationships, as these sensor pairs
behave normally even during anomalous days. The broken
relationships can be used to locate sensors that should be
responsible for the corresponding anomaly. In Figure 9 (a),
we observe that sensors clustered in the upper right and lower
right corners (i.e., circled with green lines) are problematic.
In contrast, in Figure 9 (b), almost all relationships are broken
(i.e., see the red edges circled in green boxes), implying that
this is a severe anomaly that affects a signiﬁcant portion of
sensors. Such diagnosis is helpful to system administrators
to quickly locate problematic sensors and the source of the
anomaly. The analytics framework provides the option to
describe similar ﬁgures for each anomaly at ﬁner granularities,
e.g., every hour,
to visually present how faults propagate
through sensors over time. Results are not shown here due
to space constraints.
Anomaly Detection Takeaways:
• Global subgraphs are more suitable for anomaly detection
than local subgraphs .
• Global subgraphs with the strongest relationships (i.e.,
(a) 2017-11-21
(b) 2017-11-28
Fig. 9: Fault diagnosis with local subgraphs with BLEU score
in [80, 90) range on anomalous days. Edges marked with red
are broken relationships. Green circles indicate faulty clusters
of sensors that are responsible for the anomalies.
BLEU score above 90) are not useful.
• Local subgraphs are helpful
to locate sensors that are
responsible for anomalies.
• Fault diagnosis can be performed at various time granular-
ities to show fault propagation over time.
IV. CASE STUDY II: HDD DATASET
To provide veriﬁable results, we apply here the proposed
analytics framework to a publicly available dataset. Unfor-
tunately, among the public datasets with categorical data in
the Kaggle repository [2], none reports failures or anomalies.
Thus, we chose to use the Backblaze dataset that is publicly
available and details HDD reliability statistics (disk failures)
over time [1]. The Backblaze data set has mostly continuous
features but with some minimal preprocessing we show that
they may be converted into discrete ones.
A. Dataset of HDD Failures
The Backblaze data consist of daily performance logs col-
lected from hard disk drives (HDDs) housed at the Backblaze
data center. For each day of operation, the drives report a list of
SMART attributes [3], which report either cumulative lifetime
counts of particular hardware events (e.g., certain types of
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:32:40 UTC from IEEE Xplore.  Restrictions apply. 
559
drive errors) or daily summaries of drive activity (e.g., read
or write counts). Of particular interest to us is an additional
attribute marked by the maintainers of the Backblaze dataset:
certain days of drive operation are marked as “failures,”
indicating that, on the following day,
the drive ceases to
function and is removed from production.
B. Baseline Models
We start with choosing the best set of features. Backblaze
claims to report 100+ SMART features, but many of them
are only recorded on a small proportion of drives or drive
models. Therefore, we restrict ourselves to features that are
recorded for all disk drive types, resulting in 20 features only.
In addition, we ﬁnd that 14 of these features are cumulative
counts, monotonically increasing over the lifetime of the drive.
These features change relatively slowly over time and due to
their trending behavior, they tend to spuriously correlate with
other cumulative features. Due to the difﬁculties this poses to
our correlation-based approach, we transform these cumulative
features using a ﬁrst-order difference, yielding a set of daily
deltas. In the end, we utilize 34 features, including 20 raw
SMART features and 14 differenced ones. We consider the
latest data collected in 2018 and focus on data reported by
Seagate models for enterprise workloads, which account for
35% of the Backblaze data in total and 46% of the data for
Seagate models in 2018.
To obtain a performance baseline, we use two commonly
used nonlinear machine learning models:
• Random Forest (RF). This is a supervised ensemble model
based on decision trees, which has been previously success-
fully applied to the Backblaze dataset [24]. We use 80%
of the drives for training and 20% for testing. Since there
are many more non-failures than failures, we randomly sub-
sample non-failure cases such that the training data has a
1-to-1 majority-to-minority ratio.
• One-class SVM (OC-SVM). This is a widely used unsu-
pervised model for anomaly detection [35]. Here, we choose
the radial basis function (RBF) kernel. The OC-SVM takes
as training data a set of non-anomalous observations and
ﬁts a decision boundary about these data. If a drive in the
training set is not observed to fail, we consider its data
“non-anomalous”. We ﬁnd that training the OC-SVM scales
poorly to large datasets, so we randomly sub-sample from
among these relevant observations to get a more manageable
training set.
C. Multivariate Relationship Graph
In order to adapt our analytics framework to the Backblaze
dataset, each SMART feature is taken to be recorded by a sen-
sor. In the multivariate relationship graph, each node represents
a feature fed into the framework. As a generic method that
does not require feature engineering, our framework accepts
raw SMART features only. In addition, among the 20 raw
features, the values of 4 features are barely changed in the year.
As discovered in Section III-B, these features provide little
information, thus are removed. So, there should be 16 nodes
(a) SMART 187
(b) SMART 9
Fig. 10: Examples of features used for the two feature dis-
cretization schemes, shown as CDFs.
(i.e., features) and 16× 15 = 240 edges (i.e., relationships) in
the learned multivariate relationship graph.
Unlike the physical plant system where sensors continue
to collect information when an anomaly happens, disks in
Backblaze are removed at the time they fail. As a result, each
disk only reports one failure sample, which is the last day of
its operation. In order to acquire more anomalies (rather than
only 1), we aggregate the data for all disks so that the number
of anomalies corresponds to the number of failure disks.
Recall that our method is designed for discrete event se-
quences. To make the Backblaze dataset amenable to our
model, we have to discretize the continuous values recorded
by those features. We tailor our discretization schemes in this
way in order to best maintain the semantics of each feature.
Figure 10 illustrates the two discretization schemes used with
two representative examples.
1) If most of the observations of a feature are equal to zero
(as is the case with many error counts), then we adopt a
binary discretization scheme. We replace the feature with
an indicator variable, indicating whether the feature is zero
(see Figure 10 (a)).
2) For all other features, we examine the distribution of the
feature across the training set and pull out the 20th, 40th,
60th, and 80th percentiles. These are used as decision
boundaries for assigning categories (see Figure 10 (b)).
To ensure a stable discretization of features, we focus on
disks with substantial samples. Here we consider disks with
over 10-month data in the year, ending up with 24 disks. For
each disk, we utilize its last 4 months data: the ﬁrst 2 months
for training, the following one month for development, and the
last one for testing. For NMT model parameter settings, we
use the ones used for the private dataset (see Section III-A2).
Since features are recorded on a daily-basis, we set 1 word to
5 characters and 1 sentence to 7 words (both sliding windows
are set to be 1) to ensure a reasonable vocabulary size and
number of sentences.
D. Evaluation
Our analytics framework is a generic unsupervised method
that is especially designed for discrete event sequences. Lever-
aging the learned multivariate relationship graph, it provides
information about important features (i.e., reﬂected as nodes
with higher in-degree in global subgraphs, see Section IV-D1)
and disk failures (i.e., anomaly detection, see Section IV-D2).
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:32:40 UTC from IEEE Xplore.  Restrictions apply. 
560
The graphs can also help in locating features for fault diagnosis
but not shown here due to space constraints. In contrast,
the baseline models, which do not work directly for discrete
event sequences and rely on domain knowledge for feature
engineering, ether require more information as a supervised
method (i.e., Random Forest) or cannot provide feature impor-
tance analysis (i.e., one-class SVM). Table II shows a high-
level comparison among the models. Note that our method
is unsupervised, generic and especially designed for discrete
event sequences where RF and OC-SVM are not applicable.
It is therefore expected for our method to not achieve as high
recall comparing to algorithms for continuous time series that
rely on feature engineering.
TABLE II: Comparison of different models on Backblaze.
Model
RF
OC-
SVM
Ours
Unsu-
per-
vised?
Feature
Engineer-
ing?
Feature
Rank-
ing?
×
(cid:2)
(cid:2)
(cid:2)
(cid:2)
×
(cid:2)
×
(cid:2)
Recall
70%-
80%
60%
58%
Applicable to
Discrete Event
Sequences?
×
×
(cid:2)
1) Knowledge discovery: Let us start with knowledge dis-
covery with global subgraphs. Figure 11 (a) shows the global
subgraphs with BLEU score in [80, 90) range. This range
works best for the power plant dataset (see Section III-B) and
continues to be the best for Backblaze also. In the graph,
there are 16 nodes representing 16 features. 5 of them are
labeled with their SMART feature ID (i.e., larger nodes in
the ﬁgure). These features are more extensively connected
to others, implying that they are critical indicators of disk
health status. Table III lists the descriptions of those features
and their in-degree and out-degree. Nonzero values for these
features indicate that the disk has suffered failed I/O operations
and that the health of the disk is at risk. Their importance to
anomaly detection aligns with what we expect to be necessary
for prediction. As comparison, Figure 11 (b) presents 10
most important features given by the feature importance score
provided by the Random Forest model. We observe that
all 5 features are shown in the list and we ﬁnd that these
features consistently place in the top 10 upon model retraining.
This conﬁrms the effectiveness of using global subgraphs for
feature importance analysis.
2) Anomaly detection: We use the global subgraphs shown
in Figure 11 (a) to detect disk failures in Backblaze. Here, we
select several failed disks that are successfully detected and
several that are not detected, and show how their anomaly
scores change over time before their failure dates, see Fig-
ure 12. From Figure 12 (a), we observe that there is always a
sharp increase in the anomaly score (i.e., over 0.5 increment)
right before the failure date for every successfully detected
disk. In contrast, in Figure 12 (b), the anomaly scores for
non-successfully-detected ones are stable over time, regardless
at high scores of over 0.6 or at low scores of below 0.1. In
other words, we look for sharp increments as signs of disk
(a) Global subgraph
(b) Feature ranking by RF
Fig. 11: Feature importance analysis (a) by global subgraph
with BLEU score in the [80, 90) range and (b) by the Random
Forest model.
failures. In total, we are able to deliver a recall of 58%, which
is comparable to the recall of 60% given by the one-class SVM
trained on more features, without any feature engineering
effort that one-class SVM requires.
(a) Detected disks
(b) Not detected disks
Fig. 12: Disk failure detection given by global subgraph with
BLEU score in the [80, 90) range.
V. RELATED WORK
Understanding the interdependence relationship among mul-
tivariate time series is one of the most important tasks in
machine learning [14]. Most of existing literature focuses on
time series analysis of continuous sequences [40]–[42]. The
canonical correlation analysis [15] and its nonlinear version of
kernel canonical correlation analysis [5] aim at extraction of
common features from a pair of multivariate sequences based
on a linear or nonlinear transformation of the original variables
by maximization of correlation or kernelized correlation [5],
[15]. There are multiple dependence measures to quantify
bivariate relationships such as Spearman’s ρ, Spearman’s
footrule, Gini’s coefﬁcient, Kendall’s τ [19], copula-based
Kernel dependency measures [32], kernel dependency estima-
tion [38] and others. In time series analysis, auto-regressive
and moving average (ARMA) models have been proposed
to characterize the dependence measure between two time
series [14]. ARMA builds a linear relationship model between
two continuous time series. For example, a method based on an
ARMA model was proposed to quantify multivariate invariant
relationship in a distributing system [36]. These methodologies
work well for continuous time series and variables, but have
limitations when applied to discrete event sequences.
Regarding discrete categorical event data analysis, some
works focus on mining patterns from event sequences. The
discovered patterns can be used for further analysis, such as
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:32:40 UTC from IEEE Xplore.  Restrictions apply. 
561
TABLE III: Top 5 most important SMART features [3] reported by global subnetworks at [80, 90).
Name
# in-degree
# out-degree
Description
Power-off Retract Count
Reported Uncorrectable Errors
(Ofﬂine) Uncorrectable Sector
Count
Current Pending Sector Count
Reallocated Sectors Count
15
13