a
r
e
t
i
L
g
n
i
r
t
S
y
t
p
m
E
p
i
k
S
9
0
w
o
ﬂ
r
e
v
O
n
o
i
t
c
u
d
e
D
e
p
y
T
4
0
t
c
u
r
t
S
d
e
s
i
l
a
i
t
i
n
i
n
U
32
0
100
100
100
r
e
d
r
o
s
i
D
e
c
n
a
t
i
r
e
h
n
I
41
7
85
r
e
d
r
o
s
i
D
e
c
n
a
l
a
B
20
0
100
TP
FP
p
e
t
a
d
p
U
e
t
a
t
S
n
e
d
d
i
H
134
30
82
t
c
a
r
t
n
o
C
n
a
M
w
a
r
t
S
30
4
88
r
e
f
s
n
a
r
T
n
e
d
d
i
H
12
0
100
Table 2: Number of true positives (TP), false positives (FP)
and precision p (in %) per detected honeypot technique for
contracts with source code.
skip empty string literal (SESL), 5 type deduction overﬂows
(TDO), 80 uninitialised structs (US), 382 hidden state up-
dates (HSU), 14 hidden transfers (HT) and ﬁnally 101 straw
man contracts (SMC). While many contracts were found to
be HSU, SMC and US honeypots, only a small number were
found to be TDO honeypots.
5.2 Validation
In order to conﬁrm the correctness of HONEYBADGER, we
performed a manual inspection of the source code of the con-
tracts that have been ﬂagged as honeypots. We were able to
collect through Etherscan the source code for 323 (70%) of
the ﬂagged contracts. We veriﬁed the ﬂagged contracts by
manually scanning the source code for characteristics of the
detected honeypot technique. For example, in case a contract
has been ﬂagged as a balance disorder, we checked whether
the source code contains a function that transfers the con-
tract’s balance to the caller if and only if the value sent to the
function is greater than or equal to the contract’s balance.
Table 2 summarises our manual veriﬁcation in terms of true
positives (TP), false positives (FP) and precision p, where p
is computed as p = T P/(T P + FP). A true positive means
that the contract is indeed a honeypot with respect to the
reported technique and a false positive means that the con-
tract is not a honeypot with respect to the reported technique.
Overall our tool shows a very high precision and a very low
false positive rate. Our tool achieves a false positive rate of
0% for 5 out of the 8 analysed honeypot techniques. For
the remaining 3 techniques, our tool achieves a decent false
positive rate, where the highest false positive rate is roughly
18% for the detection of hidden state updates, followed by
15% false positive rate for the detection of inheritance dis-
order and ﬁnally 12% false positive rate for the detection of
straw man contracts.
Figure 13: Number of successful, active and aborted honey-
pots per honeypot technique.
6 Analysis
In this section, we analyse the true positives obtained in Sec-
tion 5, in order to acquire insights on the effectiveness, live-
ness, behaviour, diversity and proﬁtability of honeypots.
6.1 Methodology
We crawled all the transactions of the 282 true positives
using Etherchain’s6 API, in order to collect various infor-
mation about the honeypots, such as the amount of spent
and received ether per address, the deployment date and the
balance. Afterwards, we used simple heuristics to label ev-
ery address as either an attacker or a victim. An address
is labeled as an attacker if it either: 1) created the honey-
pot; 2) was the ﬁrst address to send ether to the honeypot;
or 3) received more ether than it actually spent on the hon-
eypot. An address is labeled as a victim if it has not been
labeled as an attacker and if it received less ether than it ac-
tually spent on the honeypot. Finally, using this informa-
tion we were able to tell if a honeypot, was either successful,
aborted or still active. A honeypot is marked as successful if
a victim has been detected, as aborted if the balance is zero
and no victim has been detected or as active if the balance is
larger than zero and no victim has been detected.
6.2 Results
Effectiveness. Figure 13 shows the number of successful,
aborted and active honeypots per honeypot technique. Our
results show that skip empty string literal is the most ef-
fective honeypot technique with roughly 78% success rate,
whereas hidden transfer is the least effective technique with
solely 33% success rate. The overall success rate of honey-
pots seems rather low with roughly 37%, whereas the overall
abortion rate seems quite high with about 54%. At the time
of writing, solely 10% of the analysed honeypots are still ac-
tive. Figure 14 illustrates the number of monthly deployed
6https ://www.etherchain.org/
1600    28th USENIX Security Symposium
USENIX Association
7177111493831018100510232213759170%20%40%60%80%100%BDIDSESLTDOUSHSUHTSMCSuccessfulActiveAbortedFigure 14: Number of monthly deployed honeypots per hon-
eypot technique.
honeypots per honeypot technique. The very ﬁrst honeypot
technique that has been deployed was a hidden state update
in January 2017. February 2018 has been the peak in terms
of honeypots being deployed, with a total of 66. The high-
est number of monthly honeypots that have been deployed
per technique are hidden state updates with a total of 36 in
June 2018. 7 honeypots have been deployed on average per
month. In our analysis, the quickest ﬁrst attempt of exploita-
tion happened just 7 minutes and 37 seconds after a honeypot
had been deployed, whereas the longest happened not until
142 days after deployment. A honeypot takes an average of 9
days and a median of 16 hours before it gets exploited. Inter-
estingly, most honeypots (roughly 55%) are exploited during
the ﬁrst 24 hours after being deployed.
Liveness. We deﬁne the lifespan of a honeypot as the pe-
riod of time between the deployment of a honeypot and the
moment when a honeypot was aborted. We found that the
shortest lifespan of a honeypot was 5 minutes and 25 seconds
and the longest lifespan was about 322 days. The average
lifespan of a honeypot is roughly 28 days, whereas the me-
dian is roughly 3 days. However, in around 32% of the cases
the lifespan of a honeypot is solely 1 day. We also analysed
how long an attacker keeps the funds inside a honeypot, by
measuring the period of time between the ﬁrst attempt of ex-
ploitation by a victim and the withdrawal of all the funds by
the attacker. The shortest period was just 4 minutes and 28
seconds after a victim fell for the honeypot. The longest pe-
riod was roughly 100 days. On average attackers withdraw
all their funds within 7 days after a victim fell for the honey-
pot. However, in most cases the attackers keep the funds in
the honeypot for a maximum of 1 day. Interestingly, only 37
out of 282 honeypots got destroyed, where destroyed means
that the attacker called a function within the honeypot that
calls the SELFDESTRUCT opcode. In other words, 171
honeypots are in some kind of “zombie” state, where they
Figure 15: A word cloud generated from the comments on
Etherscan.
are still alive (i.e. not destroyed), but not active (i.e. their
balance is zero). Analysing the 37 destroyed honeypots, we
found that 19 got destroyed after being successful and 18 af-
ter never having been successful.
Behaviour. Our methodology classiﬁed a total of 240 ad-
dresses as victims. In 71% of the cases a honeypot managed
to trap solely one victim. In one case though, 97 victims have
been trapped by just a single honeypot. Interestingly, 8 out
of the 240 addresses fell for more than one honeypot, where
one address even became a victim to four different honey-
pots. We also found that 53 attackers deployed at least two
honeypots, whereas a sole attacker deployed eight different
honeypots. It is worth noting that 42 of the 53 attackers sim-
ply deployed copies of one particular honeypot type, whereas
the remaining 11 deployed honeypots of varying types. 87
out of the 282 detected and manually conﬁrmed honeypots
(about 31%) contained comments on Etherscan. We man-
ually analysed these comments and found that the majority
of the comments were indeed warnings stating that the con-
tract might be a honeypot. Moreover, Figure 15 shows that
the term “honeypot” is the most prevalent term used by the
community to describe this type of smart contracts. Surpris-
ingly, 20 out of the 87 commented honeypots were success-
ful. 16 were successful before a comment had been placed
and 4 have been successful even after a comment had been
placed. Interestingly, 21 honeypots aborted after a comment
was placed. The quickest abort was performed just 33 min-
utes and 57 seconds after the comment, whereas the longest
abort was performed 37 days after the comment. Finally, at-
tackers took an average of 6 days and a median of 22 hours
to abort their honeypot after a user had placed a comment.
Diversity. We used the normalised Levenshtein dis-
tance [48] to measure the similarity of the bytecode between
the individual instances of a particular honeypot technique.
Table 3 outlines the similarity in terms of minimum, maxi-
mum, mean and mode per honeypot technique. We observe
that for almost every technique, except TDO, the bytecode
similarity varies tremendously. For example, in case of hid-
den state update honeypots, we measure a minimum similar-
ity of 11% and a maximum similarity of 98%. This indicates
that even though two honeypots share the same technique,
USENIX Association
28th USENIX Security Symposium    1601
0510152025303540August 2015October 2015December 2015February 2016April 2016June 2016August 2016October 2016December 2016February 2017April 2017June 2017August 2017October 2017December 2017February 2018April 2018June 2018August 2018October 2018Number of contractsDateBDIDSESLTDOUSHSUHTSMCBD
27
97
50
35
ID
14
96
40
35
SESL
22
98