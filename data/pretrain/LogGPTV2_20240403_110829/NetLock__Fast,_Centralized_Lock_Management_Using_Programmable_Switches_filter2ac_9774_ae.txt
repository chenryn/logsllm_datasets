treats both shared and exclusive locks as exclusive locks, it has
many fail-and-retry operations which degrade its performance.
With the co-design of the switch and lock servers, NetLock avoids
a large number of fail-and-retry operations caused by contentions
compared to NetChain. The clients only need to retry when there
is a packet loss or deadlock. By offloading using a fast switch to
process most requests and avoiding most of retries, NetLock im-
proves the transaction throughput by 14.9× (28.6×, 3.5×) and 18.4×
(33.5×, 4.4×) in low and high contention settings respectively com-
pared with DSLR (DrTM, NetChain). Besides throughput, NetLock
also reduces both the average and tail latencies, by up to 20.3×
(66.8×, 5.4×) and 18.4× (653.9×, 23.1×) respectively compared with
10−1100101102103Throughput (MRPS)100101102103Latency (μs)99.9% tail latency99% tail latencyMed. latencyAvg. latency10−1100101102103Throughput (MRPS)100101102103Latency (μs)99.9% tail latency99% tail latencyMed. latencyAvg. latency500200040006000800010000Number of locks050100150Throughput (MRPS)NetLock500200040006000800010000Number of locks050100150200Latency (μs)99.9% tail latency99% tail latencyMed. latencyAvg. latency120160200Throughput (MRPS)NetLock, Shared lockNetLock, Exclusive lock w/o contentionNetLock, Exclusive lock w/ contention12345678Number of cores020Lock server, Shared lockLock server, Exclusive lock w/o contentionLock server, Exclusive lock w/ contentionNetLock: Fast, Centralized Lock Management
Using Programmable Switches
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
(a) Lock Throughput.
(b) Transaction Throughput.
(c) Average latency.
(d) Tail latency.
Figure 10: System comparison under TPC-C with ten clients and two lock servers.
(a) Lock Throughput.
(b) Transaction Throughput.
(c) Average latency.
(d) Tail latency.
Figure 11: System comparison under TPC-C with six clients and six lock servers.
DSLR (DrTM, NetChain). Figure 11 shows the results of the second
scenario, where we use six machines as clients and six machines
as lock servers for NetLock, DrTM and DSLR, and NetChain only
uses the switch for lock processing. While in this scenario the lock
servers are less loaded than they are in the previous scenario, Net-
Lock still achieves significant improvement. Compared to DSLR
(DrTM, NetChain), it improves the transaction throughput by up to
17.5× (33.1×, 5.5×), and reduces the average and tail latency by up
to 11.8× (65.6×, 7.7×) and 10.5× (602.8×, 34.4×) respectively.
Policy support. Besides performance, another benefit of NetLock
is its flexible policy support. The default policy is starvation-freedom
which helps reduce tail latency and is shown in the previous exper-
iment. Here we show the other two representative policies men-
tioned in Section 4.4. Figure 12(a) shows how NetLock provides
service differentiation with priorities. There are two tenants with
five clients each. Without service differentiation, both tenants have
similar performance when the high-priority tenant begins to send
requests. With service differentiation, the high-priority tenant is
prioritized over the low-priority tenant.
Figure 12(b) shows how NetLock enforces performance isolation.
Different from the service differentiation experiment, we assign
seven clients to tenant 1 and three clients to tenant 2. Because tenant
1 has more clients to generate requests at a faster rate than tenant
2, when there is no performance isolation, tenant 1 starves tenant 2
and achieves higher throughput. With performance isolation, each
tenant can only obtain the tenant’s own share, which is half of the
resources here, and two tenants achieve similar performance.
6.4 Memory Management
We evaluate the efficiency of the memory allocation algorithm and
the impact of the switch memory size on system performance. The
experiments are conducted with ten clients and two lock servers
under TPC-C workload (ten warehouses per node).
Memory allocation. NetLock uses an optimal knapsack algorithm
to efficiently pack popular locks into limited switch memory to
maximize system performance. We compare it with a strawman
(a) Service differentiation.
(b) Performance isolation.
Figure 12: Policy support of NetLock.
algorithm that randomly divides locks between the switch and the
servers. Figure 13(a) shows the lock request throughput and its
breakdown on the lock switch and the servers. Because the ran-
dom approach does not allocate the switch memory to the popular
locks, the switch only processes a small number of lock requests.
On the other hand, NetLock efficiently utilizes the limited switch
memory to process as many requests as possible, and improves the
total throughput by 2.95×. Figure 13(b) shows the latency CDF of
the two algorithms. Because the random approach processes most
lock requests in the lock servers, it incurs high latency, especially
at the tail. In comparison, because of the efficient memory alloca-
tion, NetLock processes many requests directly in the switch and
significantly reduces the transaction latency.
Switch memory size. As discussed in Section 4.5, the impact of
switch memory size on the system performance depends on the
think time and the memory allocation mechanism. Figure 14(a)
shows the impact of memory size on throughput under different
think times. The think time determines the maximum turnover rate
of a memory slot, which limits the maximum throughput the switch
can support with a given amount of memory. From the figure, we
can see that when the think time is zero, the throughput quickly
grows up with more memory slots and achieves 8.64 MRPS at the
maximum. As the think time increases, the throughput is smaller
and also grows more slowly. When the think time is 100 µs, the
system can only achieve 0.60 MRPS because the memory in the
789Throughput (MRPS)Low contentionHigh contention0123DSLRDrTMNetChainNetLock1.251.50Throughput (MRPS)Low contentionHigh contention0.00.20.4DSLRDrTMNetChainNetLock789Average latency (ms)Low contentionHigh contention0123DSLRDrTMNetChainNetLock120160Tail latency (ms)Low contentionHigh contention0510DSLRDrTMNetChainNetLock567Throughput (MRPS)Low contentionHigh contention012DSLRDrTMNetChainNetLock0.81.01.2Throughput (MRPS)Low contentionHigh contention0.00.20.4DSLRDrTMNetChainNetLock46Average latency (ms)Low contentionHigh contention0.00.51.0DSLRDrTMNetChainNetLock708090Tail latency (ms)Low contentionHigh contention0246DSLRDrTMNetChainNetLock05101520Time (s)0.00.20.40.60.8Throughput (MTPS)(i) w/o differentiationLow priorityHigh priority05101520Time (s)0.00.20.40.60.8Throughput (MTPS)(ii) w/ differentiationLow priorityHigh priorityw/o isolationw/ isolation0.00.51.0Throughput (MTPS)Tenant1Tenant2SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Zhuolong Yu, Yiwen Zhang, Vladimir Braverman, Mosharaf Chowdhury, and Xin Jin
(a) Throughput.
(b) Latency.
Figure 13: Impact of memory allocation mechanisms.
(a) Think time.
(b) Memory allocation.
Figure 14: Impact of memory size under different memory
allocation mechanisms and think times.
switch is not efficiently utilized. Thus, NetLock is more suitable for
low-latency transactions.
Figure 14(b) shows the impact of memory size on throughput un-
der different memory allocation mechanisms. Because the knapsack
algorithm used by NetLock can efficiently utilize switch memory,
the throughput increases quickly with more memory slots, and
reaches the maximum throughput of 8.61 MRPS with 3000 slots.
We emphasize that the maximum throughput is bottlenecked by
the speed of generating requests from the clients and the intrinsic
contentions between the transactions, not the switch. On the other
hand, because the random algorithm allocates the switch memory
to a random set of locks, it utilizes the switch memory poorly. As a
result, more memory slots does not help improve the transaction
throughput of the system under the inefficient memory allocation
algorithm. Under this workload, NetLock can achieve significant
improvement with 5 × 103 memory slots (160KB), which is only a
small fraction of the switch memory (tens of MB).
6.5 Failure Handling
We finally evaluate how NetLock handles failures. We manually
stop the switch to inject a switch failure, and then reactivate the
switch. Figure 15 shows the throughput time series. At time 10
s, we let the NetLock switch stop processing any packets. The
system throughput drops to zero immediately upon the switch
łfailurež. Then we reactivate the switch to process lock requests.
The switch retains none of its former state or register values. During
the switch failure, the client keeps retrying and requesting locks
for their transactions. Upon reactivation, some lock requests of a
transaction can be processed by the new (reactivated) switch while
others may be lost. NetLock uses leasing to handle this situation.
After reactivation, the system throughput returns to the pre-failure
Figure 15: Failure handling result.
level instantly. NetChain can be applied to chain several NetLock
switches to further reduce the temporary downtime.
7 RELATED WORK
Lock management. Today’s centralized lock managers are im-
plemented on servers [3, 23, 24, 29, 48]. While they are flexible
to support various policies, they suffer from limited performance.
Recent work has exploited decentralized lock managers for high
performance [17, 40, 46, 49]. These decentralized solutions achieve
high performance at the cost of limited policy support. Compared
to them, NetLock is a centralized lock manager that provides both
high performance and the flexibility to support rich policies.
Fast distributed transactions. There is a long line of research
on fast distributed transaction systems [11, 14, 19, 30, 34, 39, 44,
45, 47, 50, 51]. These systems use a variety of techniques to im-
prove performance, from designing new transaction algorithms
and protocols, to exploiting new hardware capabilities like RDMA
and hardware transactional memory. NetLock can be used as a
fast lock manager to improve general transactions without any
modifications to transaction protocols.
In-network processing. Recently there have been many efforts
exploiting programmable switches for distributed systems, such as
key-value stores [28, 36ś38], coordination and consensus [15, 16, 27,
35, 41, 52], network telemetry [22, 26], machine learning [42, 43],
and query processing [33]. Kim et al. [31] proposes to extend switch
memory with server memory using RDMA. NetLock provides a new
solution for lock management, does not rely on RDMA, and includes
an optimal memory allocation algorithm to integrate switch and
server memory for the lock manager.
8 CONCLUSION
We present NetLock, a new centralized lock management archi-
tecture that co-designs programmable switches and servers to si-
multaneously achieve high performance and rich policy support.
NetLock provides orders-of-magnitude higher throughput than ex-
isting systems with microsecond-level latency, and supports many
commonly-used policies on performance and isolation. With the
end of Moore’s law, we believe NetLock exemplifies a new genera-
tion of systems that leverage network programmability to extend
the boundary of networking to IO-intensive workloads.
Ethics. This work does not raise any ethical issues.
Acknowledgments. We thank our shepherd Kun Tan and the
anonymous reviewers for their valuable feedback on this paper.
This work is supported in part by NSF grants CCF-1629397, CRII-
1755646, CNS-1813487, CNS-1845853, and CCF-1918757, a Facebook
Communications & Networking Research Award, and a Google
Faculty Research Award.
randomknapsackMemory allocation0246810Throughput (MRPS)Server (random)Switch (random)Total (random)Server (knapsack)Switch (knapsack)Total (knapsack)0100200300400Transaction latency (μs)0.00.20.40.60.81.0Latency CDFKnapsackRandom01234Switch memory size (×10^3)0246810Throughput (MRPS)Thinktime = 0μsThinktime = 5μsThinktime = 10μsThinktime = 100μs010203040Switch memory size (×10^3)0246810Throughput (MRPS)KnapsackRandom05101520Time (s)0246810Throughput(MRPS)stop switchreactivate switchNetLock: Fast, Centralized Lock Management
Using Programmable Switches
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
[32] Leslie Lamport. 1974. A new solution of Dijkstra’s concurrent programming
problem. CACM (1974).
[33] Alberto Lerner, Rana Hussein, Philippe Cudre-Mauroux, and U eXascale Infolab.
2019. The Case for Network Accelerated Query Processing.. In CIDR.
[34] Jialin Li, Ellis Michael, and Dan R. K. Ports. 2017. Eris: Coordination-Free Consis-
tent Transactions Using In-Network Concurrency Control. In ACM SOSP.
[35] Jialin Li, Ellis Michael, Naveen Kr. Sharma, Adriana Szekeres, and Dan R.K. Ports.
2016. Just say NO to Paxos overhead: Replacing consensus with network ordering.
In USENIX OSDI.
[36] Xiaozhou Li, Raghav Sethi, Michael Kaminsky, David G. Andersen, and Michael J.
Freedman. 2016. Be Fast, Cheap and in Control with SwitchKV. In USENIX NSDI.
[37] Ming Liu, Liang Luo, Jacob Nelson, Luis Ceze, Arvind Krishnamurthy, and Kishore
Atreya. 2017. IncBricks: Toward In-Network Computation with an In-Network
Cache. In ACM ASPLOS.
[38] Zaoxing Liu, Zhihao Bai, Zhenming Liu, Xiaozhou Li, Changhoon Kim, Vladimir
Braverman, Xin Jin, and Ion Stoica. 2019.. DistCache: Provable Load Balancing
for Large-Scale Storage Systems with Distributed Caching. In USENIX FAST.
[39] Shuai Mu, Yang Cui, Yang Zhang, Wyatt Lloyd, and Jinyang Li. 2014. Extracting
More Concurrency from Distributed Transactions.. In USENIX OSDI.
[40] Sundeep Narravula, A Marnidala, Abhinav Vishnu, Karthikeyan Vaidyanathan,
and Dhabaleswar K Panda. 2007. High performance distributed lock management
services using network-based remote atomic operations. In IEEE CCGrid.
[41] Dan R. K. Ports, Jialin Li, Vincent Liu, Naveen Kr. Sharma, and Arvind Krishna-
murthy. 2015. Designing Distributed Systems Using Approximate Synchrony in
Data Center Networks. In USENIX NSDI.
[42] Amedeo Sapio, Ibrahim Abdelaziz, Abdulla Aldilaijan, Marco Canini, and Panos
Kalnis. 2017. In-network computation is a dumb idea whose time has come. In
ACM SIGCOMM HotNets Workshop.
[43] Amedeo Sapio, Marco Canini, Chen-Yu Ho, Jacob Nelson, Panos Kalnis,
Changhoon Kim, Arvind Krishnamurthy, Masoud Moshref, Dan RK Ports, and
Peter Richtárik. 2019. Scaling distributed machine learning with in-network
aggregation. arXiv preprint arXiv:1903.06701 (2019).
[44] Alexander Thomson, Thaddeus Diamond, Shu-Chun Weng, Kun Ren, Philip Shao,
and Daniel J. Abadi. 2012. Calvin: Fast Distributed Transactions for Partitioned
Database Systems. In ACM SIGMOD.
[45] Xingda Wei, Zhiyuan Dong, Rong Chen, and Haibo Chen. 2018. Deconstructing
RDMA-enabled Distributed Transactions: Hybrid is Better!. In USENIX OSDI.
[46] Xingda Wei, Jiaxin Shi, Yanzhe Chen, Rong Chen, and Haibo Chen. 2015. Fast
in-memory transaction processing using RDMA and HTM. In ACM SOSP.
[47] Chao Xie, Chunzhi Su, Manos Kapritsos, Yang Wang, Navid Yaghmazadeh,
Lorenzo Alvisi, and Prince Mahajan. 2014. Salt: Combining ACID and BASE in a
Distributed Database.. In USENIX OSDI.
[48] Cong Yan and Alvin Cheung. 2016. Leveraging lock contention to improve OLTP
application performance. In Proceedings of the VLDB Endowment.
[49] Dong Young Yoon, Mosharaf Chowdhury, and Barzan Mozafari. 2018. Distributed
Lock Management with RDMA: Decentralization without Starvation. In ACM
SIGMOD.
[50] Erfan Zamanian, Carsten Binnig, Tim Harris, and Tim Kraska. 2017. The end of a
myth: Distributed transactions can scale. In Proceedings of the VLDB Endowment.
[51] Yang Zhang, Russell Power, Siyuan Zhou, Yair Sovran, Marcos K Aguilera, and
Jinyang Li. 2013. Transaction chains: achieving serializability with low latency
in geo-distributed storage systems. In ACM SOSP.
[52] Hang Zhu, Zhihao Bai, Jialin Li, Ellis Michael, Dan Ports, Ion Stoica, and Xin Jin.
2019. Harmonia: Near-Linear Scalability for Replicated Storage with In-Network
Conflict Detection. In Proceedings of the VLDB Endowment.
REFERENCES
[1] 2018. Cavium XPliant. https://www.cavium.com/.
[2] 2018. Intel Data Plane Development Kit (DPDK). http://dpdk.org/.
[3] 2018. Teradata: Business Analytics, Hybrid Cloud & Consulting. http://www.
teradata.com/.
[4] 2019. Amazon Web Services. https://aws.amazon.com/.
[5] 2019. Broadcom Ethernet Switches and Switch Fabric Devices. https://www.
broadcom.com/products/ethernet-connectivity/switching.
[6] 2019. CloudLab. https://www.cloudlab.us.
[7] 2019. Google Cloud. https://cloud.google.com/.
[8] 2019. Microsoft Azure. https://azure.microsoft.com/.
[9] 2020. Barefoot Tofino. https://www.barefootnetworks.com/technology/#tofino.
[10] 2020. TPC-C. http://www.tpc.org/tpcc/.
[11] Peter Bailis, Alan Fekete, Michael J. Franklin, Ali Ghodsi, Joseph M. Hellerstein,
and Ion Stoica. 2015. Coordination avoidance in database systems. In Proceedings
of the VLDB Endowment.
[12] Pat Bosshart, Dan Daly, Glen Gibb, Martin Izzard, Nick McKeown, Jennifer
Rexford, Cole Schlesinger, Dan Talayco, Amin Vahdat, George Varghese, and
David Walker. 2014. P4: Programming Protocol-independent Packet Processors.
SIGCOMM CCR (2014).
[13] Pat Bosshart, Glen Gibb, Hun-Seok Kim, George Varghese, Nick McKeown, Martin
Izzard, Fernando Mujica, and Mark Horowitz. 2013. Forwarding metamorpho-
sis: Fast programmable match-action processing in hardware for SDN. In ACM
SIGCOMM.
[14] James C. Corbett, Jeffrey Dean, Michael Epstein, Andrew Fikes, Christopher
Frost, JJ Furman, Sanjay Ghemawat, Andrey Gubarev, Christopher Heiser, Peter
Hochschild, Wilson Hsieh, Sebastian Kanthak, Eugene Kogan, Hongyi Li, Alexan-
der Lloyd, Sergey Melnik, David Mwaura, David Nagle, Sean Quinlan, Rajesh
Rao, Lindsay Rolig, Yasushi Saito, Michal Szymaniak, Christopher Taylor, Ruth
Wang, and Dale Woodford. 2012. Spanner: Google’s globally distributed database.
In USENIX OSDI.
[15] Huynh Tu Dang, Marco Canini, Fernando Pedone, and Robert Soulé. 2016. Paxos
made switch-y. SIGCOMM CCR (2016).
[16] Huynh Tu Dang, Daniele Sciascia, Marco Canini, Fernando Pedone, and Robert
Soulé. 2015. NetPaxos: Consensus at network speed. In ACM SOSR.
[17] Ananth Devulapalli and Pete Wyckoff. 2005. Distributed queue-based locking
using advanced network features. In IEEE ICPP.
[18] Aleksandar Dragojević, Dushyanth Narayanan, Edmund B Nightingale, Matthew
Renzelmann, Alex Shamis, Anirudh Badam, and Miguel Castro. 2015. No compro-
mises: Distributed transactions with consistency, availability, and performance.
In ACM SOSP.
[19] Aleksandar Dragojević, Dushyanth Narayanan, Edmund B Nightingale, Matthew
Renzelmann, Alex Shamis, Anirudh Badam, and Miguel Castro. 2015. No compro-
mises: distributed transactions with consistency, availability, and performance.
In ACM SOSP.
[20] Steven Fitzgerald, Ian Foster, Carl Kesselman, Gregor Von Laszewski, Warren
Smith, and Steven Tuecke. 1997. A directory service for configuring high-
performance distributed computations. In IEEE HPDC.
[21] Cary Gray and David Cheriton. 1989. Leases: An Efficient Fault-tolerant Mecha-
nism for Distributed File Cache Consistency. In ACM SOSP.
[22] Arpit Gupta, Rob Harrison, Marco Canini, Nick Feamster, Jennifer Rexford, and
Walter Willinger. 2018. Sonata: Query-driven streaming network telemetry. In
ACM SIGCOMM.
[23] Andrew B Hastings. 1990. Distributed lock management in a transaction pro-
cessing environment. In Symposium on Reliable Distributed Systems.
[24] Jiamin Huang, Barzan Mozafari, Grant Schoenebeck, and Thomas F Wenisch.
2017. A top-down approach to achieving performance predictability in database
systems. In ACM SIGMOD.
[25] Patrick Hunt, Mahadev Konar, Flavio Paiva Junqueira, and Benjamin Reed. 2010.
ZooKeeper: Wait-free Coordination for Internet-scale Systems. In USENIX ATC.
[26] Nikita Ivkin, Zhuolong Yu, Vladimir Braverman, and Xin Jin. 2019. QPipe: Quan-
tiles Sketch Fully in the Data Plane. In ACM CoNEXT.
[27] Xin Jin, Xiaozhou Li, Haoyu Zhang, Nate Foster, Jeongkeun Lee, Robert Soulé,
Changhoon Kim, and Ion Stoica. 2018. NetChain: Scale-Free Sub-RTT Coordina-
tion. In USENIX NSDI.
[28] Xin Jin, Xiaozhou Li, Haoyu Zhang, Robert Soulé, Jeongkeun Lee, Nate Foster,
Changhoon Kim, and Ion Stoica. 2017. NetCache: Balancing Key-Value Stores
with Fast In-Network Caching. In ACM SOSP.
[29] Horatiu Jula, Daniel Tralamazza, Cristian Zamfir, and George Candea. 2008.
Deadlock immunity: Enabling systems to defend against deadlocks. In USENIX
OSDI.
[30] Anuj Kalia, Michael Kaminsky, and David G Andersen. 2016. FaSST: Fast, Scalable
and Simple Distributed Transactions with Two-Sided (RDMA) Datagram RPCs..
In USENIX OSDI.
[31] Daehyeok Kim, Yibo Zhu, Changhoon Kim, Jeongkeun Lee, and Srinivasan Seshan.
2018. Generic External Memory for Switch Data Planes. In ACM SIGCOMM
HotNets Workshop.