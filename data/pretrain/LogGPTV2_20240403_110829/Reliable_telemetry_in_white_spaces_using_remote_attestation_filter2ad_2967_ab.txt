ing that assume ﬁxed nodes with known locations: (1) Residential
Internet access using IEEE 802.22 [36], and (2) AMI communica-
tions [17]. In cases where the network contains untrustworthy or
mobile devices, secure localization and location veriﬁcation tech-
niques may ensure nodes’ locations are authentic [14, 24, 26, 27].
The above assumptions are common for the type of analysis we
perform here [15, 19, 31]; if they are violated then additional pro-
tective measures are required.
3. APPROACH
Consider Figure 1 as part of the region of interest for perform-
ing reliable aggregation of spectrum measurement data. There exist
two types of nodes; attestation-capable nodes (triangles), and reg-
ular nodes (circles). In any particular cell, the goal is to obtain an
estimate of the signal power in that cell, and compare it to a primary
detection threshold to determine whether the spectrum is unused.
Assume for now that we have performed remote attestation on all
attestation-capable nodes and have excluded those we believe are
compromised. Therefore, the remaining attestation-capable nodes
are considered trusted or attested. For regular nodes, however, we
do not have any prior information regarding their legitimacy.
Consider cell A in Figure 1 in which about half of the nodes are
attested. One may argue that the high number of reliable nodes pro-
vides enough diversity to absorb the variations due to path loss and
shadow-fading, and therefore there is no need to include the results
of regular nodes. This approach is safer (in terms of vulnerability
to false reports) than one in which the values from the (potentially
compromised) regular nodes are also included. But what if the rest
of the regular nodes are also legitimate? Is the safety worth the re-
duced precision? How would we determine whether it make sense
to rely only on trusted nodes, or we should use the data from regular
nodes as well? And if so, which ones?
Figure 1: Illustration of a few cells with attested and regular
nodes.
Now consider cell B where unlike cell A there are very few
trusted nodes. Therefore, there is a high chance that aggregating
the measurements from such a small number of nodes does not pro-
vide enough diversity to obtain a precise measurement (estimate)
of the signal power. A similar situation can be seen in cell C; not
only there exist very few attested nodes, but their positioning also
makes it likely that they do not provide enough diversity. For exam-
ple, they may all be behind an obstruction that attenuates the signal.
Therefore, it seems necessary to include results from at least some
of the regular nodes. But what if some or all of them are com-
promised, and they skew the results to achieve their malicious goal
instead of adding legitimate diversity?
3.1 Key Issues and Overview
The examples above underline the importance of the following
needs. First, there must be a systematic strategy to determine when
there is enough diversity in the results that we can stop collecting
additional data within a cell. Second, if we decide we need ad-
ditional data beyond those from attested nodes, there should exist
a strategy to decide which nodes to include. Third, for each cell
in which additional regular nodes are added to the data ‘pool,’ we
need a strategy to ensure that the added nodes are not dominated by
attackers.
At a high level, our approach consists of three main phases (sum-
marized in Algorithm 0). First, within each cell we rely on statis-
tical inference and sequential estimation to aggregate data from all
of attested sensors as well as ‘enough’ regular nodes to achieve
the application-speciﬁed precision goal (Section 3.2). Note that
we only include the least required number of regular sensors to
limit unnecessary exposure to untrustworthy data. Various inclu-
sion strategies are proposed for this purpose (Section 3.3). The
A B C Attested Node Regular Node aggregate is either the mean and median of the data, and is dy-
namically determined by our algorithm. This choice may change
throughout the execution of the algorithm according to pre-speciﬁed
rules (Section 3.4). Second, the regular nodes that were included
in the aggregation process in the cell are compared against the data
from the trusted nodes of the 8 neighboring cells. This process
involves using machine learning classiﬁers built from real signal
propagation data. The classiﬁer detects irregular signal propaga-
tion patterns that most likely represent a coordinated misreporting
attack (Section 3.5). Third, after the potentially compromised data
is eliminated, we compute the ﬁnal aggregate.
Algorithm 1 Simpliﬁed Approach Overview (for Each Cell)
Input:
(1) Green Data: measurements from attested nodes
(2) Yellow Data: measurements from regular nodes
(3) Strategy ∈ {Random, Geo-Diverse, Biased}: strategy for in-
cluding data from regular nodes
(4) Aggregate ∈ {Mean, Median}: dynamically changes based
on the situation
Phase 1: Node Selection
Add Green Data to aggregation Pool
while ¬SATISFY-PRECISION-REQUIREMENTS(data in Pool,
Aggregate) do
if | Yellow Data | > 0 then
MOVE-NEXT-ELEMENT-TO-POOL(Strategy,
Yellow
Data)
else
Remove all Yellow Data from Pool
Go to Phase 3
end if
end while
Phase 2: Attack Detection
Yellow Suspects ← Yellow Data in Pool from Phase 1
Green Neighbors ← averages of Green Data in the neighboring
cells (i.e. 8 numbers)
if SVM-ATTACKER-DETECTION(Yellow Suspects, Green
Neighbors) then
Remove all Yellow Suspects from Pool
end if
Phase 3: Aggregate Calculation
Compute Aggregate from data in Pool
3.2 Using Statistical Inference to Ensure Pre-
cision
For many applications, including aggregation of spectrum sens-
ing data, it is not clear in advance how many sensors (observations)
should be used in each aggregation effort in order to achieve the de-
sired precision in the (estimation) outcome. Instead, data is evalu-
ated as it is collected, and further sampling is stopped in accordance
with a pre-deﬁned stopping rule. This process is also referred to as
sequential estimation. In our case, we aim to achieve an acceptable
precision in the results while using as few data points from regular
nodes as possible. We argue that sequential estimation for achiev-
ing ﬁxed width conﬁdence interval for the estimated aggregate is an
ideal tool to achieve our goal. By stating the acceptable margin of
error (half the width of a conﬁdence interval) for the quantity being
estimated, the application can ensure with high conﬁdence that the
estimated outcome from the sample data is ‘close enough’ to the
true value. In other words, with high conﬁdence (e.g. 95%), it can
be assured that the true mean (or median) is within a γ margin of
error from the estimated value (e.g. γ = 3dB). This is also referred
to in the form of a coverage probability (e.g. 0.95 = 1 − α).
We ﬁrst focus on a sequential procedure for ﬁnding ﬁxed-width
conﬁdence intervals for the mean. Let x1, x2, ... be a sequence
of independent and identically distributed (i.i.d.) random variables
having an unknown density function f (x), x ∈ R. The i.i.d. as-
sumption is not absolutely true for sensors that are very close and
face correlated shadowing; however in view of practical consider-
ations we proceed with this assumption, which is in-line with the
commonly used log-normal shadowing model [33]. Let µ and σ2
represent the mean and variance of density function f (x).
It is
known that no ﬁxed-sample size procedure will provide a ﬁxed-
width conﬁdence interval for µ having a prescribed coverage prob-
ability at the same time. The famous Chow-Robbins procedure for
sequential estimation deﬁnes the following stopping rule for a con-
ﬁdence interval of size 2γ:
N = inf{n ≥ n0, n ≥ a2γ
n}
−2s2
where n0 ≥ 2 is the initial sample size, a = z(1−α/2) is the
100(1−α/2) percentile of the standard normal distribution N (0, 1)
(e.g. if α = .05 then a = 1.96), and sn is the sample standard de-
viation of n observations. The Chow-Robbins procedure is asymp-
totically tight, in the sense that the coverage probability is asymp-
totically 1− α, and is also asymptotically efﬁcient in the sense that
the average required number of samples is asymptotically equal to
an optimal ﬁxed-sample procedure with known σ2 [20].
Now we turn to the median. We begin by placing the measure-
ments in order, that is: x(1) < x(2) < ... < x(n). The goal is
to ﬁnd an interval x(a) < m < x(b) such that P (x(a) < m <
x(b)) = 1 − α, where 1 − α is the desired probability that the
interval captures the median.
In order to have x(a) < m, at least a of the observations must
fall less than m, and in order to have m < x(b), at most b − 1 of
the observations must fall less than or equal to m. Since m is the
median and since the distribution of the X’s is continuous, we have
P (X < m) = P (X ≤ m) = .5.
(cid:0)n
k
(cid:1)(.5)n. To
the binomial probability with p = .5, that is(cid:80)b−1
Assuming independent observations, the probability that at least a
and at most b − 1 of the observations fall less than m is given by
construct a 100(1 − α)% conﬁdence interval for m, we choose a
and b so that this sum is 1 − α. For large samples, approximate
values of a and b may be found by using the normal approximation
to the binomial distribution. We may obtain a and b by solving for
them in the following equations [22]:
√
.25n
= −z(1−α/2),
b − 1 − .5n
= z(1−α/2)
a − .5n√
.25n
k=a
Note that both the conﬁdence intervals were calculated by as-
suming the distribution of the original population is unknown.
3.3
Intra-cell Inclusion Strategies
We consider three inclusion strategies for including regular nodes
in the aggregate computation in each cell. The merits and disadvan-
tages of each strategy are discussed in this section and evaluated in
Section 4.
Random: Randomly adding data from regular nodes to the data
from attested nodes has the advantage that it is in-line with the sam-
pling assumptions made in computing the conﬁdence intervals. In
addition, the randomness reduces the attacker’s chances of selec-
tively compromising nodes and carefully crafting false measure-
ments with minimum abnormality. However, it disallows deploy-
ing targeted inclusion strategies that could potentially lead to lower
attacker success rate.
Geo-Diverse: By selecting a geographically diverse set of regu-
lar nodes, we add diversity to the results and reduce the chances of
selecting (regular) nodes that are experiencing similar shadowing
effects. To achieve this goal, we use the widely cited Gudmund-
son shadow correlation model [21]. According to this model, the
correlation in shadow-fading in distance ∆x is represented as:
R(∆x) = e
−∆x
dcorr
with the correlation length dcorr dependent on the environment.
Empirical studies suggest values between 25m to 120m for urban
areas [9]. Using this model, we suggest the following greedy ap-
proach to adding nodes to the aggregation pool. Before each addi-
tion to the pool, we compute the aggregate correlation of all nodes
already in the aggregation pool with the candidates to be added to
the pool. At each step, we add the node with the least aggregate
correlation with existing nodes.
Biased: In this approach, we sort the data from the regular nodes
in the increasing order of the absolute value of their difference to
the median of the attested nodes. At each step, we move values to
the aggregation pool according to their rank in the sorted list. This
approach has the disadvantage that creates a ‘bias’ in the aggregate
calculation process, which makes the computations in Section 3.2
inaccurate. However, in many cases, this bias effectively works
as an implicit weighting mechanism in situations where attackers
have only compromised a subset of the regular nodes.
In those
situations, this approach may limit the number of measurements
from compromised nodes that will be included in the ﬁnal result
(see the results in Section 4).
3.4
Intra-cell Aggregation: Mean or Median?
Within each cell, the two main options for aggregating measure-
ments in a cell are calculating the average (EGC) or median of the
data (observations). A collection of observations is referred to as a
sample. The goal is to use all of attested nodes plus a dynamically
selected set of regular nodes such that we can ensure the computed
aggregate is within a pre-deﬁned distance of the real mean or me-
dian for the signal in the cell.
The median has a key advantage over the mean as an aggregate;
it is less vulnerable to natural outliers or attacker nodes that con-
stitute a minority of nodes in a cell [18, 38]. However, computing
the sample median with a pre-speciﬁed conﬁdence interval requires
more data (compared to mean). Or dually, with a ﬁxed number of
observations, the conﬁdence intervals achieved for the median are
larger than those computed for the mean (the calculation proce-
dures are presented in Section 3.2). To support our argument about
the relatively smaller conﬁdence intervals for mean (with the same
number of samples), we generate sample signal propagation data
representing a log-normal shadowing model with average power of
−95dBm and standard deviation (a.k.a. dB-spread) of 4, 6, and
8. Table 1 presents the margins of error achieved using random
samples of size 20, 30, 40, and 50 from this distribution.
However, if the attackers obtain even a weak majority in a cell,
they can move the median to their desired number while being
less ‘abnormal.’ Figure 2 illustrates this observation. The attack-
ers’ goal is to change the aggregate from a value below the signal
threshold of -114 dBm to one above the threshold (e.g. -113 dBm).
When the median is used (the top picture), the attackers can achieve
their desired goal by simply reporting -113 dBm. However, when
the average is used (the bottom picture), the attackers need to report
an average false report of -105.5 dBm to change the total average
to their desired value of -113 dBm. The additional abnormality
Table 1: Margin of error (95% conﬁdence) for randomly gen-
erated data of size |S| equal to 20, 30, 40, 50 from log-normal
(shadowing) distribution with standard deviation, σ, of 4, 6,
and 8.
|S| = 30
|S| = 40
|S| = 20
|S| = 50
Mean
Median
Mean
Median
Mean
Median
1.7
2.3
2.6