the cryptographic mechanisms supported by servers. The number
of investigated servers was much lower (20,000).
Yilek et al. investigated the consequences of the Debian OpenSSL
bug of 2008 [14]. A bug in the OpenSSL implementation had
caused very weak randomness in key generation, and it was pos-
sible to pre-compute the affected public/private key combinations.
The authors traced the effect of the error over a time of about 200
days and scanned about 50,000 hosts.
The problem with scans is generally that hosts are included that
are not intended to be accessed with TLS/SSL and thus provide in-
valid (and often default) certiﬁcates. The percentages given in [8, 9,
10, 11] thus need to be treated with caution. Our actively obtained
data sets concentrate on high-ranked domains from the Alexa Top
1 Million list, and observe these domains over a long time period.
Note that high-ranked domains can be assumed to be more aimed at
use with TLS/SSL. This should at least be true for the top 1,000 or
top 10,000. Our monitoring does not suffer signiﬁcantly from this
problem. Thanks to it, we are not only able to estimate the deploy-
ment of the TLS/SSL infrastructure, but to analyze which parts of
the PKI are actively used and therefore seen by users. Furthermore,
our view on the TLS/SSL deployment is not a single snapshot at an
arbitrary time, but includes changes that operators have conducted
in 1.5 years. Moreover, by analyzing TLS/SSL data that has been
obtained from all over the world, we can even estimate how users
see the TLS/SSL-secured infrastructure in other parts of the world.
Finally, we include the EFF data from the mentioned related work
into our own evaluation, and are therefore able to enhance previous
results by applying our own evaluation algorithms, providing more
comparability between our work and theirs.
4. DATA SETS
This section presents the various data sets that we obtained for
our analyses. Section 4.1 presents our approach and the tools we
used to perform our active TLS/SSL scans. Our setup and tool
chain for our passive measurement of TLS/SSL trafﬁc is explained
in Section 4.2. All data sets that we use in our analyses, including
the data sets from related work, are described in Section 4.3. Ta-
ble 1 gives an overview of all data sets for this work. The data sets
Figure 3: Growth of the NSS/Mozilla Root Store.
trol from the client – it is debatable if users would trust all inter-
mediate authorities even if they knew about them. To understand
the extent of the problem, it is important to realize that any CA can
issue a certiﬁcate for any domain name.
For certiﬁcate checking to work, Root Certiﬁcates have to be
known to the verifying party. This is solved in Web browsers by
shipping a so-called Root Store, which contains the Root Certiﬁ-
cates of selected CAs. In our example, R1 and R2 are in the Root
Store while R3 is not; hence the browser would reject E7 as un-
trusted and display a warning message. An interesting case is cross-
signing, where a CA also has a certiﬁcate (I4) that has been actu-
ally signed by a Root Certiﬁcate of another CA. This is useful if
the other CA is contained in a Root Store in which the cross-signed
CA is not contained. However, Root Stores have a very undesired
effect. If a CA or any of the subordinate intermediates it cooperates
with becomes compromised, the attacker can issue certiﬁcates for
any domain. It has been documented that this has happened several
times [3, 4] in 2011: an attacker (purportedly the same) hacked
several CAs such that he was able to issue certiﬁcates for several
high-value domains to himself. In the ﬁrst case [3], the number
of issued certiﬁcates was low, and browser vendors responded by
blacklisting the forged certiﬁcates. In the latter case [4], the number
of forged certiﬁcates exceeded 500, and there were claims that the
certiﬁcates had been used on a state level in a Man-in-the-middle
attack. This led to the removal of the affected Root CA from the
Root Store, an unprecedented step in the history of the X.509 PKI
for the WWW.
CAs are included in the Root Store by vendor-speciﬁc processes.
In the case of Mozilla, this is an open forum; but this is not the case
for large companies like Microsoft or Apple. It should be noted
that the number of Root CAs in browsers has been growing for a
long time. We veriﬁed this for Mozilla Firefox, whose Root Store
comes with the NSS crypto library that it uses. Figure 3 shows the
development of the NSS Root Store since the year 2000. Starting
out with about only a handful of certiﬁcates, the number has greatly
increased over the years. Until December 2010, the number has
grown to 160, the latest Root Store we used for this paper.
In order to be valid, a certiﬁcate must fulﬁll several properties,
which we will present in detail later. These properties have differ-
ent security implications for the overall security of the TLS/SSL
connections. Also, it should be noted that users are today very used
to browser warnings when certiﬁcates or trust chains are found to
be broken. Studies like [5] showed that, for various reasons, the
typical reaction is to ignore warnings and continue with the con-
nection. All in all it is quite correct to conclude that the X.509 PKI
is a very fragile construction. In fact, criticism has been voiced by
prominent researchers [6, 7].
429Location
Short Name
Tübingen, DE
Tue-Nov2009
Tübingen, DE
Tue-Dec2009
Tübingen, DE
Tue-Jan2010
Tübingen, DE
Tue-Apr2010
Munich, DE
TUM-Sep2010
Munich, DE
TUM-Nov2010
TUM-Apr2011
Munich, DE
TUM-Apr2011-SNI Munich, DE
Shanghai
Bejing
Melbourne
˙Izmir
São Paulo
Moscow
Santa Barbara
Boston
MON1
MON2
EFF
Time (run)
November 2009
December 2009
January 2010
April 2010
September 2010
November 2010
April 2011
April 2011
Shanghai, CN
April 2011
Beijing, CN
April 2011
Melbourne, AU
April 2011
˙Izmir, TR
April 2011
São Paulo, BR
April 2011
Moscow, RU
April 2011
Santa Barbara, USA April 2011
April 2011
Boston, USA
September 2010
Munich, DE
April 2011
Munich, DE
EFF servers
March–June 2010 Active IPv4 scan
Type
Active scan
Active scan
Active scan
Active scan
Active scan
Active scan
Active scan
Active scan with SNI
Active scan
Active scan
Active scan
Active scan
Active scan
Active scan
Active scan
Active scan
Passive monitoring
Passive monitoring
Certiﬁcates (distinct)
833,661 (206,588)
819,488 (205,700)
816,517 (204,216)
816,605 (208,490)
829,232 (210,697)
827,366 (212,569)
829,707 (213,795)
826,098 (212,229)
798,976 (211,135)
797,046 (211,007)
833,571 (212,680)
825,555 (211,617)
833,246 (212,698)
830,765 (213,079)
834,173 (212,749)
834,054 (212,805)
183,208 (163,072), Grid: 47% (51.94%)
989,040 (102,329), Grid: 5.72% (24.40%)
11,349,678 (5,529,056)
Table 1: Data sets used in this work.
that we acquired by active scans ourselves will be released to the
scientiﬁc community at [15].
4.1 Active Scans
As a base for our scans, we used the Alexa Top 1 Million Hosts
list [12] that was current at the time of the respective scan. This
list contains the most popular hosts of the WWW as determined by
the ranking algorithm of Alexa Internet, Inc. These are thus sites
that many users are likely to visit. Although its level of accuracy is
disputed [16], it nevertheless is appropriate for our need to identify
popular hosts. Since the list’s format is somewhat inconsistent, we
emulated browser behavior by expanding each entry to two host
names: one with www preﬁx and one without.
Every scan was preceded by nmap scans on TCP port 443 to ﬁlter
out hosts where this port was closed. The actual certiﬁcate scans
thus started two weeks after obtaining the Alexa list. Our SSL scan-
ning tool itself uses OpenSSL. It takes a list of host names as input
and attempts to conduct a full TLS/SSL handshake on port 443 with
each of them. Where successful, the full certiﬁcate chain as sent by
the server is stored, along with further data, such as TLS/SSL con-
nection properties.
4.2 Passive Monitoring
We monitored all TLS/SSL trafﬁc entering and leaving the Mu-
nich Scientiﬁc Research Network (MWN) in Munich, Germany.
The network interconnects three major universities as well as af-
ﬁliated research institutions in the greater Munich area and pro-
vides Internet access to their users via a 10 Gbit/s link to the Ger-
man research network DFN. It serves about 120,000 users with ap-
proximately 80,000 devices. During busy hours, the average link
load amounts to approximately 2 Gbit/s inbound and 1 Gbit/s out-
bound trafﬁc (as of April 2011). We obtained passive monitoring
data in two runs. One problem we had to cope with was the high
link speed. We improved our monitoring software setup between
the ﬁrst and second run, whereas the hardware (a four-core Intel
Core i7 with hyper-threading using an Intel 10 GE network inter-
face with 82598EB chipset) remained the same.
In order to deal with the huge amount of trafﬁc, both runs used
a sampling algorithm that samples the ﬁrst n bytes of each bi-
ﬂow [17]. This is sufﬁcient, as the handshake messages including
the X.509 certiﬁcates are exchanged at the beginning of a TLS/SSL
session setup.
For our ﬁrst monitoring run, we used a process similar to the one
used by the Time Machine [18]: we captured the beginning of every
observed bi-ﬂow and dumped all sampled packets to disk, starting
a new ﬁle as soon as a dump ﬁle reached 10 GB. Whenever a dump
ﬁle was ﬁnished, the TLS/SSL connections were extracted ofﬂine.
Due to disk I/O and disk space limitations, we were only able to
sample the ﬁrst 15 kB of each bi-ﬂow.
The second monitoring run in April 2011 was conducted using
online TLS/SSL analysis. We used an optimized capturing setup,
including our improvement presented in [19], based on TNAPI [20]
to build a multi-core aware online analysis system. We now were
able to run six instances of our monitoring application in parallel.
Each instance employed a sampling process that sampled the ﬁrst
n kB of each bi-ﬂow. Using this parallelization technique, we were
able to analyze for each bi-ﬂow up to 400 kB of trafﬁc data while
suffering less than 0.003% overall packet loss.
As the TLS/SSL processing tool, we used the intrusion detec-
tion and protocol parsing system Bro [21] in both monitoring runs.
Using Bro’s dynamic protocol detection feature [22], we were able
to identify TLS/SSL in a port-independent way. We used Bro-1.5
with some applied patches to the Bro code, which ﬁx some issues
and allow us to extract and store complete certiﬁcate chains from
the monitored connections.
4.3 Data Properties
Table 1 summarizes the locations, dates and number of certiﬁ-
cates in the different sets. Table 2 provides additional details for
the monitoring runs. Our data sets can be grouped into four classes.
First, we conducted scans from hosts that were located in Ger-
many at the University of Tübingen and at TU München. These
scans were carried out between November 2009 and April 2011,
thus spanning a time interval of about 1.5 years. In April 2011, we
performed an extra scan with SNI enabled. SNI is a TLS extension
to address virtual HTTP hosts. With SNI, the host name is passed
in the TLS handshake so the server can select an appropriate certiﬁ-
cate to deliver (note that the TLS/SSL handshake takes place before
the client sends the HTTP header).
The second group of data sets was obtained in April 2011. We
employed PlanetLab [23] nodes from different countries, in order to
obtain a geographically distributed picture of the TLS/SSL deploy-
ment. We wanted to determine whether location-dependent factors
like content distribution networks (CDNs), which use DNS to route
430clients to different computing centers depending on the geographic