# Opening the Blackbox of VirusTotal: Analyzing Online Phishing Scan Engines

## Authors
- Peng Peng
- Limin Yang
- Linhai Song
- Gang Wang

### Affiliations
- **Virginia Tech**: Peng Peng
- **The Pennsylvania State University**: Linhai Song
- **University of Illinois at Urbana-Champaign**: Limin Yang, Gang Wang

### Contact Information
- **Peng Peng**: [EMAIL]
- **Limin Yang**: [EMAIL]
- **Linhai Song**: [EMAIL]
- **Gang Wang**: [EMAIL]

## Abstract
Online scan engines such as VirusTotal are widely used by researchers to label malicious URLs and files. However, the process by which these labels are generated and their reliability remain poorly understood. In this paper, we focus on VirusTotal and its 68 third-party vendors to examine their labeling process for phishing URLs. We set up our own phishing websites (mimicking PayPal and IRS) and submitted the URLs for scanning. By analyzing the incoming network traffic and the dynamic label changes at VirusTotal, we provide new insights into how VirusTotal operates and the quality of its labels. Our findings show that vendors struggle to flag all phishing sites, with even the best vendors missing 30% of our phishing sites. Additionally, the scanning results are not immediately updated in VirusTotal, and there are inconsistencies between VirusTotal's scans and some vendors' own scanners. These results highlight the need for more rigorous methodologies to assess and utilize the labels obtained from VirusTotal.

## CCS Concepts
- **Security and privacy** → Web application security

## ACM Reference Format
Peng Peng, Limin Yang, Linhai Song, and Gang Wang. 2019. Opening the Blackbox of VirusTotal: Analyzing Online Phishing Scan Engines. In Internet Measurement Conference (IMC '19), October 21–23, 2019, Amsterdam, Netherlands. ACM, New York, NY, USA, 8 pages. https://doi.org/10.1145/3355369.3355585

## 1. Introduction
Online scan engines, designed to detect malware files and malicious websites, are crucial tools for identifying new threats [3, 4, 7, 8]. VirusTotal is one of the most popular scanning services, widely used by researchers and industry practitioners [8]. It provides both file scan (for malware analysis) and URL scan services (for detecting phishing and malware hosts) and works with over 60 security vendors to aggregate their scanning results.

VirusTotal is extensively used by the research community for data labeling and system evaluation. Many recent studies rely on VirusTotal’s file scan API [18, 24, 26, 28, 29, 37, 41, 44, 45] and URL scan API [16, 17, 20, 30, 35, 36, 38, 40, 42, 46, 47] for data labeling. For example, if a certain number of vendors label a file/URL as “malicious,” researchers often accept the “malicious” label.

However, VirusTotal operates like a blackbox, and it is unclear how it and its vendors generate labels for URLs or files. This raises critical questions about the reliability of these labels and whether researchers are using VirusTotal correctly.

In this paper, we take initial steps to explore how VirusTotal and its vendors assign labels, focusing on the URL scan API for detecting phishing websites. We aim to:
1. Understand how VirusTotal works with 68 vendors to perform URL scanning and result updating.
2. Evaluate the effectiveness of these scanners in detecting simple and advanced phishing sites.
3. Examine how the scanners react to dynamic changes in phishing sites.

Our goal is to provide insights to guide practitioners in better utilizing VirusTotal.

## 2. Background & Related Work
### VirusTotal
VirusTotal is a popular service that scans malicious files and web URLs [8]. The URL scanning aims to detect websites delivering malware or performing phishing. As shown in Figure 1, VirusTotal works with 68 third-party security vendors (see the full list at [11]). After an URL is submitted through the scan API, VirusTotal passes it to these vendors (anti-virus engines or online scanning services). The scanning results are stored in the VirusTotal database.

VirusTotal also provides a querying API (or report API) that allows users to check if an URL is malicious [10]. Given a URL, the API returns labels from all vendors that have previously scanned the URL, along with the timestamp of the scan. Vendors often disagree; for instance, a URL might be labeled as “benign” by Google Safe Browsing but “malicious” by Kaspersky.

### Third-Party Vendors
Among the 68 third-party vendors, 18 provide their own scan APIs to the public. Table 1 lists these vendors. It is unclear whether the results from these APIs are consistent with those from VirusTotal.

### Using VirusTotal for Labeling
VirusTotal is heavily used by the research community to label both malicious files [18, 23, 24, 26, 28, 29, 37, 39, 41, 44, 45] and suspicious IPs and URLs [16, 20, 30, 33, 35, 36, 38, 40, 42, 46, 47]. Researchers often aggregate labels to determine if a URL is “malicious.” Most papers define a threshold \( t \) — if at least \( t \) vendors return a “malicious” label, the URL is considered malicious. Common thresholds are \( t = 1 \) [16, 17, 20, 30, 35, 36, 40, 47], while some use \( t = 2 \) or 3 [33, 38, 42]. Some researchers use higher thresholds, such as 40, when labeling malware files [15, 26].

### Phishing Blacklists
Our work is related to studies on phishing blacklists [12, 32, 43]. Phishing blacklists often have delays in blocking new phishing sites [14, 19, 31] and suffer from incomplete coverage [13]. Different blacklists may return inconsistent results [25]. Our work delves into the process of how phishing URLs get blacklisted by VirusTotal and its vendors. The most relevant work is [32], which focuses on phishing blacklists used by different browsers and cloaking techniques, whereas we focus on the performance and consistency of different vendors.

## 3. Methodology
We aim to understand how VirusTotal and its vendors scan phishing URLs. Key questions include:
1. How effective are VirusTotal’s vendors in detecting basic phishing pages?
2. How quickly do scanning results become available?
3. How consistent are the scanning results across vendors and between vendor-APIs and VirusTotal API?
4. How quickly can VirusTotal react to changes in phishing sites, such as takedowns?
5. How much do basic obfuscation techniques help in evading detection?

To answer these questions, we set up fresh phishing websites on newly registered domains. We then submit the phishing URLs to VirusTotal and collect incoming network traffic and labeling results. We designed the experiments to ensure research ethics, discussed in the Appendix.

### 3.1 Phishing Site Setups
#### Phishing Page Content
We created two phishing pages mimicking the login pages of PayPal [6] and IRS (Internal Revenue Service) [2]. PayPal is chosen for its popularity, with over 30% of phishing URLs targeting it [34]. IRS, as a comparison baseline, is not commonly targeted. We replicated the original sites and modified the login form to send information to our servers. By default, we disabled any form of cloaking, and the robots.txt was set to allow web crawlers to access the phishing page.

#### Domain Names
We registered fresh domain names for our phishing sites to ensure no past history interferes with the measurement. To prevent accidental visits, we registered long random strings (50 characters each) from NameSilo [5]. For example, one domain name is “yzdfbltrok9m58cdl0lvjznzwjjcd2ihp5pgb295hfj5u42ff0.xyz.”

#### Web Hosting
We hosted the phishing websites on Digital Ocean [1] with static IPs. Before the experiment, we ensured all IPs and domain names were publicly accessible and not blacklisted. We informed Digital Ocean of our research and received their consent.

### 3.2 Experiment Design
The experiments were conducted from March to April 2019, including a main experiment and a baseline experiment.

#### Main Experiment
The main experiment measures:
- Phishing detection accuracy of VirusTotal and vendors.
- Potential inconsistency between VirusTotal API and vendors’ APIs.
- Reaction of VirusTotal to changes in phishing sites.

We set up separate phishing sites (one PayPal and one IRS) for each of the 18 vendors (36 sites in total). For each site, we conducted a 4-week experiment, submitting the phishing URL to VirusTotal’s scan API twice a week on Mondays and Thursdays. We scheduled four external events:
1. **Week 1**: Put the phishing site online.
2. **Week 2**: Submit the phishing URL to one of the 18 vendors with their own scan APIs.
3. **Week 3**: Take down the phishing page and replace it with a benign page.
4. **Week 4**: Resubmit the phishing URL to the same vendor as in Week 2.

During the experiment, we collected labels for all phishing URLs using VirusTotal’s querying API, crawling every 60 minutes to track dynamic changes. We also logged the incoming network traffic to the phishing servers.

#### Baseline Experiment
The baseline experiment measures the long-term reaction of VirusTotal after a single scan. We set up two additional phishing sites (PayPal and IRS) and submitted them only to VirusTotal, without involving other vendors.

## 4. Results
### Network Traffic and Labels
Table 2 shows the number of incoming network requests, unique IPs, and average “malicious” labels from VirusTotal per phishing site over the 4-week period.

| Experiment | Brand | # Requests (Avg. STD) | # IPs (Avg. STD) | # Mal. Labels (Avg. STD) |
|------------|-------|------------------------|-------------------|--------------------------|
| Main       | PayPal | 12,327 (2,478)         | 2,384 (290)       | 16.6 (1.1)               |
| Main       | IRS    | 335 (364)              | 146 (107)         | 13.5 (0.5)               |
| Baseline   | PayPal | 6,291                  | 2,033             | 0                        |
| Baseline   | IRS    | 30                     | 26                | 0                        |

### Key Findings
- **Detection Accuracy**: Most vendors struggled to detect the simple phishing sites we set up. Over multiple scans, only 15 out of 68 vendors detected at least one of the 36 simple phishing sites. The best vendor detected 26 simple phishing sites.
- **Vendor Performance**: Detection performance varied significantly. PayPal sites, being a popular target, were quickly detected by more than 10 vendors during the first scan. However, the less common IRS sites were not detected by any of the 68 vendors using the VirusTotal scan API alone.
- **Result Updating**: Scanning results from vendors were not immediately updated in VirusTotal. The delay is due to VirusTotal pulling previous scanning results when a new scan request is submitted for the same URL.
- **Consistency Issues**: VirusTotal had inconsistent results with vendors’ own scan APIs, suggesting that third-party vendors do not always give VirusTotal the most updated blacklists.
- **Obfuscation Techniques**: VirusTotal scanners could handle some obfuscation techniques (e.g., URL shortening) but were fooled by others (e.g., image-based or code-based obfuscations).

## 5. Conclusion
Our work provides initial insights into the reliability of labels obtained from online scanners. Future work can explore other types of scanning (e.g., malware scanning), measure the correlation between labels from different vendors, and develop new methods to reliably aggregate scanning results. To facilitate future research, we release the collected datasets [1].

## References
[1] Digital Ocean. https://www.digitalocean.com/
[2] Internal Revenue Service (IRS). https://www.irs.gov/
[3] Example Reference 1.
[4] Example Reference 2.
[5] NameSilo. https://www.namesilo.com/
[6] PayPal. https://www.paypal.com/
[7] Example Reference 3.
[8] VirusTotal. https://www.virustotal.com/
[9] Example Reference 4.
[10] VirusTotal Querying API. https://developers.virustotal.com/reference/query
[11] List of VirusTotal Vendors. https://www.virustotal.com/gui/about
[12] Example Reference 5.
[13] Example Reference 6.
[14] Example Reference 7.
[15] Example Reference 8.
[16] Example Reference 9.
[17] Example Reference 10.
[18] Example Reference 11.
[19] Example Reference 12.
[20] Example Reference 13.
[21] Example Reference 14.
[22] Example Reference 15.
[23] Example Reference 16.
[24] Example Reference 17.
[25] Example Reference 18.
[26] Example Reference 19.
[27] Example Reference 20.
[28] Example Reference 21.
[29] Example Reference 22.
[30] Example Reference 23.
[31] Example Reference 24.
[32] Example Reference 25.
[33] Example Reference 26.
[34] Example Reference 27.
[35] Example Reference 28.
[36] Example Reference 29.
[37] Example Reference 30.
[38] Example Reference 31.
[39] Example Reference 32.
[40] Example Reference 33.
[41] Example Reference 34.
[42] Example Reference 35.
[43] Example Reference 36.
[44] Example Reference 37.
[45] Example Reference 38.
[46] Example Reference 39.
[47] Example Reference 40.