avoids complex optimization problems and the requirement
of knowledge about indexed documents. The scoring and its
corresponding iterative reﬁnement are the core novelties of
our attack.
As highlighted in [1], all prior attacks require exact knowl-
edge of the queryable vocabulary (i.e.
the client’s key-
word universe). Our attacker does not require such knowl-
edge and builds her own vocabulary. Considering the fol-
lowing setup: |Dsim| = 12K,|Dreal| = 18K,msim = mreal =
1K,|Q | = 300,|KnownQ | = 15, with an exact knowledge of
the queryable vocabulary, we obtain an average accuracy of
92%. On the other hand, when the attacker builds her own
knowledge, we obtain an average accuracy of 87%. This ac-
curacy decrease is a direct consequence of the accuracy upper
bound presented in Equation (3) of Subsection 2.4.
In Appendix A.2, we detail the relation between substi-
tution cipher cryptanalysis and SSE attacks (especially the
reﬁned score attack).
Figure 7: Accuracy and ε-value of set similarity for varying
attacker document set sizes with Enron. Fixed parameters:
|Dreal| = 18K,msim = mreal = 1K,|Q | = 150,|KnownQ | =
15
show a certain degree of robustness against decreased simi-
larity. However, if we further reduce the size of the dataset,
the accuracy is also further reduced until we have a totally
ineffective attack.
5.3 Attack analysis
Role of the amount of information The reﬁned score at-
tack is sensitive to the amount of information given to the
attacker. The more information the adversary has, the higher
the attack accuracy is. This holds true for each piece of infor-
mation owned by the attacker: document set, observed query
set and known queries. This was not the case in the previous
attacks especially for IKK and GCPR which had identical
results with and without known queries. IKK presented an ac-
curacy of 80% regardless of the percentage of known queries
(from 0 to 25% in their article). CGPR only presented results
without known queries for their count attack even if it could
use them.
Technical comparison with related attacks Technically,
all query-recovery attacks solve a matching problem between
trapdoors and keywords based on speciﬁc background infor-
mation available to the attacker. IKK assumes partial knowl-
edge of the indexed documents together with known trapdoor-
keyword mappings. IKK describes the matching problem
as an optimization problem that minimizes the distance be-
tween the trapdoor co-occurrence matrix and the keyword co-
occurrence matrix. CGPR makes similar assumptions while,
in practice, it does not require known trapdoor-keyword map-
pings. CGPR iteratively ﬁlters keyword-trapdoor candidates
for which the differences between the occurrences (computed
Improving the attack using clustering Our novel scor-
ing approach offers further possibilities for improvement. In
Appendix C we discuss clustering to further improve attack
results. In our attacks, when a prediction is uncertain, we
sometimes have a group of candidates with higher scores than
the rest of the candidates instead of only one candidate with
a particularly high score. In such cases, it seems natural to
return a list of potential keywords instead of forcing the al-
gorithm to choose only one keyword. Note that it would not
affect the overall interpretability of the results as the scores
are augmented. In Appendix C, we show that clustering can
further increase the accuracy of the reﬁned scoring attack.
152    30th USENIX Security Symposium
USENIX Association
6 Attack mitigation
6.1 Existing countermeasures
To mitigate leakage-abuse attacks, several countermeasures
have been proposed in [3, 4, 15, 34]. We divide these counter-
measures into two categories: padding and obfuscation. IKK
proposed a ﬁrst countermeasure which could be assimilated
to padding. CGPR were the ﬁrst to present precisely the no-
tion of padding. It consists in adding fake entries, i.e. fake
keyword-document pairs. These false-positive results can be
easily ﬁltered by the user when they receive the database
response. With padding, there is no entry removal because
it could impact the search results (i.e. no false negative re-
sults). To harden this countermeasure, Xu et al. proposed
in [34] a method to produce fake entries that cannot be dis-
tinguished from the real entries by an attacker. In [4], Chen
et al. presented a new kind of countermeasures: obfuscation.
First, it uses code erasure to divide the documents into shards.
Thanks to code erasure, the false negative results are allowed
because the user does not need every shard to reconstruct the
document. After having computed the shards, the algorithm
adds and removes shards from the results. The removal rate
is chosen so the reconstruction rate for matching documents
is close to 100%. Thus, false-negative shards do not result in
false-negative documents.
Chen et al. also presented an improved attack scenario
where the attacker knows which shards belong together. In this
case, the countermeasure corresponds to padding because the
attacker knows that all the reconstructed documents are either
a matching document or a false-positive result. Moreover, he
knows that the proportion of matching documents which is
not reconstructed is negligible. Therefore, if the attacker only
keeps the reconstructed ﬁles he would have all the matching
documents plus some false-positive results.
These countermeasures have been proposed to mitigate
known-data attacks but they are also suitable for similar-data
attacks since they alter the co-occurrence matrix Ctd inferred
from the queries. Therefore, padding and obfuscation should
be also effective to mitigate our attack.
6.2 Experimental results
To test the possibility to mitigate our attack, we implemented
the padding presented in CGPR and the obfuscation presented
in [4]. For padding, we use the countermeasure proposed by
CGPR which is well established but the hardening proposed
by Xu et al. [34] would not provide highly different results
since we do not try to ﬁlter fake entries. Figure 8 describes
the average accuracy of the reﬁned score attack over 50 sim-
ulations for several vocabulary sizes. For the padding, we
used a padding size npad = 500. For the obfuscation, we used
the parameters used by Chen against the "improved" attack:
p = 0.88703 the rate of false-positive shards, q = 0.04416
Figure 8: Comparison of the accuracy for countermeasures.
Fixed parameters: |Dsim| = 12K,|Dreal| = 18K,|Q | = 0.15·
mreal,|KnownQ | = 15. Padding: npad = 500. Obfuscation:
p = 0.88703,q = 0.04416
the rate of false-negative shards.
Figure 8 clearly shows a good mitigation from both coun-
termeasures. For small vocabularies, the accuracy can still
be considered as too high. However, as the vocabulary size
grows, the accuracy becomes small and negligible for big
vocabularies. This ﬁgure should not be used to compare the
efﬁciency of the countermeasures. Padding performs better
than obfuscation because the padding size we chose is high.
For example, when |Kreal| = 1K, the number of entries is in-
creased by 32% because of padding. When |Kreal| = 4K, the
number of entries is increased by 166% because of padding.
These fake entries create several types of overheads including
storage, communication and computation. Chen et al. chose
p = 0.88703 and q = 0.04416 to minimize the overheads
then it is likely that obfuscation can achieve results equivalent
or better with bigger overheads. We leave the comparison
of these countermeasures and their overheads for a future
work. Our experiments highlight the importance of hiding the
document access pattern to mitigate the reﬁned score attack.
Our attack provides a matching score which can help to
identify the good predictions. When |Kreal| = 500, the average
accuracy for padding is 35% and for obfuscation 47%, the
reﬁned score attack can identify successfully a non-negligible
part of the correct predictions thanks to the matching score.
It is needed to deﬁne a maximum query recovery rate, so
the countermeasure parameters are chosen so that there is no
attack with an accuracy higher than this threshold. An analogy
with encryption security is possible: the attack accuracy is
the adversary advantage and the maximum query recovery
is the threshold under which the advantage is considered as
negligible. We leave this direction for a future work.
USENIX Association
30th USENIX Security Symposium    153
Table 2: Accuracy statistics on Enron over 50 simulations of
orders 2 and 3. |Dsim| = 12K,|Dreal| = 18K,mreal = msim =
300,|Q | = 75,|KnownQ | = 10
Accuracy statistics
µ
σ
Order-2 attack
Order-3 attack
0.92
0.77
0.0351
0.0659
7 Additional results
7.1 Generalization
In [2], Bost and Fouque presented the word-word co-
occurrence as an order 2 of co-occurrence, occurrence be-
ing the order 1 of co-occurrence. Thus, we generalize our
attack and build n-dimensional co-occurrence tensors to work
on co-occurrence of order n. This generalization help to re-
cover the queries since it increases exponentially the number
of co-occurrence we can rely on. For example, let us con-
sider the order 3: a word-word-word co-occurrence. We build
3-dimensional co-occurrence tensors and Ckw[i, j,k] (resp.
Ctd[i, j,k]) is the number of documents where the keywords
(resp. trapdoors) i, j and k appear together.
Our attack remains identical and just the matrix construc-
tion differs. In the reﬁned score attack, if the order n > 2, each
keyword and trapdoor is represented by a (n−1)-dimensional
tensors and the matching score will be computed via a ma-
trix norm. The main issue of this generalization is the space
complexity O((msim)n) due to tensor sizes. For msim = 1K,
with order 2, the similar co-occurrence matrix has 1 million
cells and with order 3, the similar co-occurrence tensor has 1
billion cells. The ﬁrst reason which could justify not to use
an order greater than 2 is the technical limitations.
We have done simulations to compare the order 2 and order
3. For each order, we run 50 simulations with Enron dataset,
mreal = msim = 300,|Q | = 75,|KnownQ | = 15. As shown in
Table 2, for order 2, we obtained an average accuracy of 92%
and for order 3, 77%. Then, in our case, increasing the order
decreased the accuracy. It highlights the trade-off between
number of co-occurrence estimators and the noise of these
co-occurrences. To take a decision, we need a maximum of
co-occurrence estimators but if they are too noisy, they will
be misleading and the decision may be wrong. Here, we only
have 30 thousands emails to compute 1 billion co-occurrences
which is not enough to limit the noise of the co-occurrence
tensor. However, increasing the co-occurrence order may be
a viable option for attacks on larger datasets.
Note that the real co-occurrence matrix Ctd is always built
using the index matrix (ID[i, j] = 1 if document i contains the
underlying keyword of query j), whatever the order is. Thus,
we expect altering the index matrix as proposed by IKK to be
an effective countermeasure even against generalized attacks.
Figure 9: Comparison of the accuracy of the reﬁned score
attack for different query distributions. Fixed parameters:
|Dsim| = 12K,|Dreal| = 18K,|Q | = 0.15· mreal,|KnownQ | =
15.
7.2 About the observed query distribution
In [18], Kornaropoulos et al. discuss the default choice of the
uniform distribution for range queries. While they focus on
SSE schemes allowing range queries, the same statement can
be done for single-keyword search schemes. In [1], Black-
stone et al. criticized the role of the query distribution. They
show that the attack performance is highly impacted whether
the attack is executed on the most frequent keyword or not.
Figure 9 compares the accuracy of the reﬁned score at-
tack over three different query distributions: Uniform, Zipﬁan
and inverted Zipﬁan. The uniform distribution is the standard
setup used in the rest of our experiments. With Zipﬁan distri-
bution, which gives more weight to the highest rank elements,
we mostly obtain queries for which the underlying keyword
is one of the most frequent. With inverted Zipﬁan distribution,
which gives more weight to the lowest rank elements, we
mostly obtain queries for which the underlying keyword is
one of the least frequent.
Figure 9 shows that inverted Zipﬁan decreases the reﬁned
score attack accuracy. The attack becomes ineffective when
the vocabulary is bigger (i.e. a vocabulary size of 4000). De-
spite the inverted Zipﬁan distribution, the reﬁned score attack
still achieves 67% of accuracy when the vocabulary size is 1K.
On the other hand, with the Zipﬁan distribution, the reﬁned
score attack reaches 81%, when the vocabulary size is 4000,
and up to 91% when the vocabulary size is 1K.
The reﬁned score attack could be much more devastating if
the uniform assumption turns out to be false. By default, the
literature uses the uniform distribution for the queries. This
assumption could be dangerous because, if the real query dis-
tribution is more advantageous than the uniform distribution
(e.g. the Zipﬁan distribution), the SSE schemes are way more
154    30th USENIX Security Symposium
USENIX Association
Table 3: Variance of the accuracy over 50 simulations of
the reﬁned score attack. |Dsim| = 12K,|Dreal| = 18K,mreal =
msim = 1K,|Q | = 150,|KnownQ | = 5.
Acc. stats.
Base setup
Top 25% Q
µ
0.65
0.71
σ
0.21
0.16
q0.25
0.54
0.68
q0.75 min max
0.87
0.78
0.81
0.87
0.06
0.17
exposed than what is usually admitted. The gap between Uni-
form and Zipﬁan distributions in Figure 9 for a vocabulary
size of 4000 is particularly alarming since it nearly doubles
the attack accuracy considering Zipﬁan distribution instead
of Uniform distribution.
7.3 About the known query distribution
The query distribution explains only a part of the result vari-
ance. The distribution of known queries also impacts the
results. It means that some known queries provide more infor-
mation than others. To identify the impact of this distribution,
we simulated 50 times two reﬁned score attacks and studied
their respective accuracy variance. The ﬁrst attack is the basic
setup used in our article: 5 known queries picked uniformly
among the queries. The second attack simulation picks 5
known queries uniformly from the 25% of queries with the
largest result sets, i.e. the most frequent underlying keyword.
We report the results in the Table 3. The basic setup has a
bigger variance and a lower mean. The second experiment
presents steadier results which conﬁrms that the distribution
of known queries impacts the results. Since an attacker has
still chances to observe only queries with the most frequent
underlying keywords given a uniform distribution, the maxi-
mum accuracy scores are equivalent for both distributions.
The variance is lower when the adversary obtains more
known queries because there are enough "good" known
queries to start the reﬁnement. Thus, only a part of these
known queries are truly necessary. An attacker can use [35]
active attack to obtain their known queries. Thus, an attacker
can aim at a speciﬁc known query distribution in order to
minimize the number of known queries needed by attacking
speciﬁc keywords. Just few qualitative known queries are
needed to start a successful reﬁned score attack.
Conclusion
We introduced a highly effective similar-data attack against
SSE. The reﬁned score attack achieves an accuracy (i.e. query-