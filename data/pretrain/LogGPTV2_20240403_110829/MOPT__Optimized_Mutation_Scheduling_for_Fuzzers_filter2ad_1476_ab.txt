spends too much time on the deterministic stage of one in-
put, it cannot generate test cases from the later inputs in the
fuzzing queue when fuzzing avconv and pdfimages given
24 hours. Note that since the splicing stage only uses cross
over to mutate the test cases, it spends too little time to be
shown in Fig. 5 compared to the other stages that will test
the target program as mentioned in Section 2.3.
• AFL spends much time on the inefﬁcient mutation oper-
ators. Fig. 3 shows that, the mutation operators bitflip
1/1 and bitflip 2/1 have found the most interesting test
cases. But according to Fig. 4, they are only selected for
a small number of times. On the other hand, inefﬁcient op-
erators like the ones of interesting values are selected
too frequently but produce few interesting test cases, which
Figure 4: The times that mutation operators are selected
when AFL fuzzes a target program avconv.
Figure 5: Percentages of time and interesting test cases used
and found by the three stages in AFL, respectively.
decreases the fuzzing efﬁciency.
Motivation. Based on the analysis above, we observe
that different mutation operators have different efﬁciencies.
Hence, the mutation schedulers in existing fuzzers, which
follow some pre-deﬁned distributions, are not efﬁcient. Ide-
ally, more time should be spent on mutation operators that
perform better at generating interesting test cases. Therefore,
a better mutation scheduler is demanded.
3 Overview of MOPT
3.1 Design Philosophy
The mutation scheduler aims at choosing the next opti-
mal mutation operator, which could ﬁnd more interesting test
cases, for a given runtime context. We simplify this prob-
lem as ﬁnding an optimal probability distribution of muta-
tion operators, following which the scheduler chooses next
operators when testing a target program.
Finding an optimal probability distribution for all muta-
tion operators is challenging. Instead, we could ﬁrst let each
operator explore its own optimal probability. Then, based on
those optimal probabilities, we could obtain a global optimal
probability distribution of mutation operators.
The Particle Swarm Optimization (PSO) algorithm can be
leveraged to ﬁnd the optimal distribution of the operators and
we detail the modiﬁcation of PSO in our setting as follows.
3.2 Particle Swarm Optimization (PSO)
The PSO [19] algorithm is proposed by Eberhart and
Kennedy, aiming at ﬁnding the optimal solution for a prob-
lem.
It employs multiple particles to search the solution
space iteratively, in which a position is a candidate solution.
As shown in Fig. 6, in each iteration, each particle is
moved to a new position xnow, based on (1) its inertia (i.e.,
1952    28th USENIX Security Symposium
USENIX Association
49% 9%7%1%< 1%1%12% < 1%< 1%4%7%8%27% 14% 7%< 1%< 1%< 1%28% 9%< 1%2%4%8%33% 12%5%< 1%< 1%1%23%2%< 1%2%9%10% bitflip 1/1bitflip 2/1bitflip 4/1bitflip 8/8bitflip 16/8bitflip 32/8arith 8/8arith 16/8arith 32/8interest 8/8interest 16/8interest 32/8             (a) avconv                      (b) exiv2                                   (c) tiff2bw0(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:21)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:23)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:25)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:27)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:20)(cid:19)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:20)(cid:21)Execution times ((cid:48)ega)(cid:69)(cid:76)(cid:87)(cid:73)(cid:79)(cid:76)(cid:83)(cid:3)(cid:20)(cid:18)(cid:20)(cid:69)(cid:76)(cid:87)(cid:73)(cid:79)(cid:76)(cid:83)(cid:3)(cid:21)(cid:18)(cid:20)(cid:69)(cid:76)(cid:87)(cid:73)(cid:79)(cid:76)(cid:83)(cid:3)(cid:23)(cid:18)(cid:20)(cid:69)(cid:76)(cid:87)(cid:73)(cid:79)(cid:76)(cid:83)(cid:3)(cid:27)(cid:18)(cid:27)(cid:69)(cid:76)(cid:87)(cid:73)(cid:79)(cid:76)(cid:83)(cid:3)(cid:20)(cid:25)(cid:18)(cid:27)(cid:69)(cid:76)(cid:87)(cid:73)(cid:79)(cid:76)(cid:83)(cid:3)(cid:22)(cid:21)(cid:18)(cid:27)(cid:68)(cid:85)(cid:76)(cid:87)(cid:75)(cid:3)(cid:27)(cid:18)(cid:27)(cid:68)(cid:85)(cid:76)(cid:87)(cid:75)(cid:3)(cid:20)(cid:25)(cid:18)(cid:27)(cid:68)(cid:85)(cid:76)(cid:87)(cid:75)(cid:3)(cid:22)(cid:21)(cid:18)(cid:27)(cid:76)(cid:81)(cid:87)(cid:72)(cid:85)(cid:72)(cid:86)(cid:87)(cid:3)(cid:27)(cid:18)(cid:27)(cid:76)(cid:81)(cid:87)(cid:72)(cid:85)(cid:72)(cid:86)(cid:87)(cid:3)(cid:20)(cid:25)(cid:18)(cid:27)(cid:76)(cid:81)(cid:87)(cid:72)(cid:85)(cid:72)(cid:86)(cid:87)(cid:3)(cid:22)(cid:21)(cid:18)(cid:27)(cid:48)(cid:88)(cid:87)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:82)(cid:83)(cid:72)(cid:85)(cid:68)(cid:87)(cid:82)(cid:85)(cid:86)       time   finding     time   finding     time   finding      time   finding           avconv                   exiv2                  tiff2bw              pdfimages00.20.40.60.81deterministic stage havoc stagesplicing stage100%100%72.4%27.6%58.1%41.9%22.3%77.7%91.2%8.8%98.8%1.2%87.9%12.1%+r× (Gbest − xnow(P)).
(1)
previous movement vnow), (2) displacement to its local best
position Lbest that this particle has found so far, and (3) dis-
placement to the global best position Gbest that all particles
have found so far. Speciﬁcally, the movement of a particle P
is calculated as follows:
vnow(P) ← w× vnow(P)+r× (Lbest (P)− xnow(P))
xnow(P) ← xnow(P) + vnow(P).
(2)
where w is the inertia weight and r ∈ (0,1) is a random dis-
placement weight.
Hence, each particle moves towards Lbest and Gbest, and is
likely to keep moving to better positions. By moving towards
Gbest, multiple particles could work synchronously and avoid
plunging into the local optimum. As a result, the swarm will
be led to the optimal solution. Moreover, PSO is easy to
implement with low computational cost, making it a good ﬁt
for optimizing mutation scheduling.
3.3 Design Details
MOPT aims to ﬁnd an optimal probability distribution.
Rather than employing particles to explore candidate dis-
tributions directly, we propose a customized PSO algorithm
to explore each operator’s optimal probability ﬁrst, and then
construct the optimal probability distribution.
3.3.1 Particles
MOPT employs a particle per operator, and tries to ex-
plore an optimal position for each operator in a predeﬁned
probability space [xmin,xmax], where 0 < xmin < xmax ≤ 1.
The current position of a particle (i.e., operator) in the
probability space, i.e., xnow, represents the probability that
this operator will be selected by the scheduler. Due to the
nature of probabilities, the sum of all the particles’ probabil-
ities in one iteration should be normalized to 1.
3.3.2 Local Best Position Lbest
Similar to PSO, MOPT also appoints the best position that
a particle has ever found as its local best position.
For a given particle, a position x1 is better than x2, if and
only if, its corresponding operator yields more interesting
test cases (with a same amount of invocations) in the for-
mer position than the latter. Thus, Lbest is the position of the
particle where the corresponding operator yields the most in-
teresting test cases (given the same amount of invocations).
To enable this comparison, for each particle (i.e., opera-
tor), we measure its local efﬁciency e f fnow, i.e., the number
of interesting test cases contributed by this operator divided
by the number of invocations of this operator during one it-
eration. We denote the largest e f fnow as e f fbest. Thus, Lbest
is the position where the operator obtains e f fbest in history.
Figure 6: An example of illustrating the evolution of one
particle at the 5-th iteration according to the PSO.
3.3.3 Global Best Position Gbest
PSO appoints the best position that all particles have ever
found as the global best position. Note that, unlike the orig-
inal PSO which moves particles in a uniﬁed solution space,
MOPT moves particles in different probability spaces (with
same shape and size). Hence, there is no sole global best
position ﬁt for all particles. Instead, different particles have
different global best positions (in different spaces) here.
In PSO, global best positions depend on the relationship
between different particles. Hereby we also evaluate each
particle’s efﬁciency from a global perspective, denoted as
global efﬁciency globale f f , by evaluating multiple swarms
of particles at a time.
More speciﬁcally, we measure the number of interesting
test cases contributed by each operator till now in all swarms,
and use it as the particle’s global efﬁciency globale f f . Then
we compute the distribution of all particles’ global efﬁciency.
For each operator (i.e., particle), its global best position Gbest
is deﬁned as the proportion of its globale f f in this distribu-
tion. With this distribution, particles (i.e., operators) with
higher efﬁciency can get higher probability to be selected.
3.3.4 Multiple Swarms
Given the deﬁnitions of particles, local best positions and
global best positions, we could follow the PSO algorithm to
approach to an optimal solution (i.e., a speciﬁc probability
distribution of mutation operators).
However, unlike the original PSO swarm that has multiple
particles exploring the solution space, the swarm deﬁned by
MOPT actually only explores one candidate solution (i.e.,
probability distribution) in the solution space, and thus is
likely to fall into local optimum. Thus, MOPT employs mul-
tiple swarms and applies the customized PSO algorithm to
each swarm, as shown in Fig. 7, to avoid local optimum.
Synchronization is required between these swarms.
MOPT simply takes the most efﬁcient swarm as the
best and uses its distribution to schedule mutation during
fuzzing. Here, we deﬁne the swarm’s efﬁciency (denoted as
swarme f f ) as the number of interesting test cases contributed
by this swarm divided by the number of new test cases dur-
ing one iteration.
Overview: In summary, MOPT employs multiple swarms
and applies the customized PSO algorithm to each swarm.
During fuzzing, the following three extra tasks are performed
USENIX Association
28th USENIX Security Symposium    1953
Solution space1-st iteration2-nd iteration4-th iteration5-th iteration3-rd iteration: Evolution path: Position of particle: Current localbest position: Current global best positionvnow: xnowFigure 7: Illustration of the particle swarms of MOPT.
in each iteration of PSO.
• T1: Locate local best positions for all particles in each
swarm. Within each swarm, each particle’s local efﬁciency
e f fnow in one iteration is evaluated during fuzzing. For each
particle, the position with the highest efﬁciency e f fbest in
history is marked as its local best position Lbest.
• T2: Locate global best positions for all particles across
swarms. Each particle’s global efﬁciency globale f f is evalu-
ated across swarms. The distribution of the particles’ global
efﬁciency is then evaluated. The proportion of each parti-
cle’s globale f f in this distribution is used as its global best
position Gbest.
• T3: Select the best swarm to guide fuzzing. Each
swarm’s efﬁciency swarme f f in one iteration is evaluated.
The swarm with the highest swarme f f
is chosen, and its
probability distribution in the current iteration is applied for
further fuzzing.
Then, at the end of each iteration, MOPT moves the par-
ticles in each swarm in a similar way as PSO. More speciﬁ-
cally, for a particle Pj in a swarm Si, we update its position
as follows.
vnow[Si][Pj] ←w× vnow[Si][Pj]
+r× (Lbest [Si][Pj]− xnow[Si][Pj])
+r× (Gbest [Pj] − xnow[Si][Pj]).
(3)
xnow[Si][Pj] ← xnow[Si][Pj] + vnow[Si][Pj].
(4)
where w is the inertia weight and r ∈ (0,1) is a random dis-
placement weight.
Further, we normalize these positions to meet some con-
straints. First, each particle’s position is adjusted to ﬁt in
the probability space [xmin,xmax]. Then for each swarm, all
its particles’ positions (i.e., probabilities) will be normalized,
such that the sum of these probabilities equals to 1.
After updating the positions of all particles in all swarms,
the fuzzer could keep moving those particles into new posi-
tions, and enter a new iteration of PSO.
4 Implementation of MOPT
4.1 MOPT Main Framework
As shown in Fig. 8, MOPT consists of four core modules,
i.e., the PSO initialization and updating modules, as well as
the pilot fuzzing and core fuzzing modules.
The PSO initialization module is executed once and used
for setting the initial parameters of the PSO algorithm. The
Figure 8: The workﬂow of MOPT.
other three modules form an iteration loop and work together
to continuously fuzz target programs.
In each iteration of the loop, the PSO particles are updated
once. In order to update particles’ positions with the PSO
algorithm, we need to ﬁnd each particle’s local best position
and global best position in each iteration.
• The pilot fuzzing module employs multiple swarms, i.e.,
multiple probability distributions, to select mutation opera-
tors and fuzz. During fuzzing, the local efﬁciency of each
particle in each swarm is measured. Hence, we could ﬁnd
the local best position of each particle in each swarm.
• Moreover, during the pilot fuzzing, each swarm’s efﬁ-
ciency is also evaluated. Then, the most efﬁcient swarm is
chosen, and the core fuzzing module will use the probability
distribution explored by it to schedule mutation operators.
• After the core fuzzing module ﬁnishes, the total number
of interesting test cases contributed by each operator till now
can be evaluated. Hence, each particle’s global efﬁciency
(i.e., global best position) could be evaluated.
With this iteration loop, the fuzzer could utilize the PSO
to ﬁnd an optimal probability distribution to select mutation
operators, and gradually improve the fuzzing efﬁciency.
Note that, MOPT’s workﬂow is independent from the tar-
get fuzzer, as long as the fuzzer’s mutation scheduler uses a
probability distribution to select operators. We do not need
to change the behavior of the target fuzzer, except that eval-
uating the efﬁciency of the fuzzer in order to move PSO par-
ticles. The instrumentation to the target fuzzer is minimum
and costs few performance overhead.
Hence, MOPT is a generic and practical mutation schedul-
ing scheme, and can be applied to a variety of fuzzers.
4.1.1 PSO Initialization Module
This module initializes parameters for the PSO algorithm.
More speciﬁcally, MOPT (1) sets the initial location xnow of
each particle in each swarm with a random value, and nor-
malizes the sum of xnow of all the particles in one swarm
to 1; (2) sets the displacement of particle movement vnow of
each particle in each swarm to 0.1; (3) sets the initial lo-
cal efﬁciency e f fnow of each particle in each swarm to 0;
(4) sets the initial local best position Lbest of each particle in
each swarm to 0.5; and (5) sets the initial global best posi-
tion Gbest of each particle across swarms to 0.5. Note that,
the initialization module only executes once when the fuzzer
starts running.
1954    28th USENIX Security Symposium
USENIX Association
: particle: probability  distribution: selection probability: range of       probability  Swarm 2Distributionxmaxxminxnow  Swarm 1Distributionxmaxxminxnow  Operator 1Operator 2Operator 3Operator 4Operator 5Operator 6  PSO Initialization ModulePilot Fuzzing ModuleCore Fuzzing ModulePSO Updating Modulemulti-swarmfuzzingefﬁciencymeasurementsingle-swarmfuzzingefﬁciencymeasurementcrashes/vulnerabilitiesswarm efﬁciencyglobal efﬁciencylocal efﬁciency4.1.2 Pilot Fuzzing Module
This module employs multiple swarms to perform
fuzzing, where each swarm explores a different probability
distribution. This module evaluates each swarm in order, and
stops testing a swarm after it has generated a conﬁgurable
number (denoted as periodpilot) of new test cases. The pro-
cess of fuzzing with a speciﬁc swarm is as follows.
For each swarm, its probability distribution is used to
schedule the selection of mutation operators and fuzz the tar-
get program. During fuzzing, the module will measure three
measurements: (1) the number of interesting test cases con-
tributed by a speciﬁc particle (i.e., operator), (2) the number
of invocations of a speciﬁc particle, (3) the number of in-
teresting test cases found by this swarm, by instrumenting
target programs.
The local efﬁciency of each particle (in current swarm) is
the ﬁrst measurement divided by the second measurement.
Hence, we could locate the local best position of each par-
ticle. The current swarm’s efﬁciency is the third measure-
ment divided by the test case count periodpilot. Therefore,
we could ﬁnd the most efﬁcient swarm.
4.1.3 Core Fuzzing Module
This module will take the best swarm selected by the pilot
fuzzing module, and use its probability distribution to per-
form fuzzing.
It will stop after generating a conﬁgurable
number (denoted as periodcore) of new test cases.
Once it stops, we could measure the number of interest-
ing test cases contributed by each particle, regardless which
swarm it belongs to, from the start of PSO initialization till
now. Then we could calculate the distribution between par-
ticles, and locate each particle’s global best position.
Note that, if we only use one swarm in the pilot module,
then the core module could be merged with the pilot module.
4.1.4 PSO Updating Module
With the information provided by the pilot and core
fuzzing modules, this module updates the particles in each
swarm, following Equations 3 and 4.
After updating each particle, we will enter the next itera-
tion of PSO updates. Hence, we could approach to an opti-
mal swarm (i.e., probability distribution for operators), use
it to guide the core fuzzing module, and help improve the
fuzzing efﬁciency.
4.2 Pacemaker Fuzzing Mode
Although applying MOPT to mutation-based fuzzers is
generic, we realize the performance of MOPT can be further
optimized when applied to speciﬁc fuzzers such as AFL.
Table 2: Objective programs evaluated in our experiments.
Target
Source ﬁle
Input format
Test instruction
mp42aac
exiv2
mp3gain
tiff2bw
pdﬁmages
sam2p
avconv
w3m
objdump
jhead
mpg321
infotocap
podofopdﬁnfo
Bento4-1-5-1
exiv2-0.26-trunk
mp3gain-1 5 2
libtiff-4.0.9
xpdf-4.00
sam2p-0.49.4
libav-12.3
w3m-0.5.3
binutils-2.30
jhead-3.00
mpg321 0.3.2
ncurses-6.1
podofo-0.9.6
mp4
jpg
mp3
tiff
PDF
bmp
mp4
text
binary
jpg
mp3
text
PDF
mp42aac @@ /dev/null
exiv2 @@ /dev/null
mp3gain @@ /dev/null
tiff2bw @@ /dev/null
pdﬁmages @@ /dev/null
sam2p @@ EPS: /dev/null
avconv -y -i @@ -f null -
w3m @@
objdump –dwarf-check -C
-g -f -dwarf -x @@
jhead @@
mpg321 -t @@ /dev/null
infotocap @@
podofopdﬁnfo @@
Based on extensive empirical analysis, we realize that
AFL and its descendants spend much more time on the de-
terministic stage, than on the havoc and splicing stages that
can discover many more unique crashes and paths. MOPT
therefore provides an optimization to AFL-based fuzzers, de-
noted as pacemaker fuzzing mode, which selectively avoids
the time-consuming deterministic stage.
Speciﬁcally, when MOPT ﬁnishes mutating one seed test
case, if it has not discovered any new unique crash or path
for a long time, i.e., T that is set by users, it will selectively
disable the deterministic stage for the following test cases.
The pacemaker fuzzing mode has the following advantages.
• The deterministic stage spends too much time and would
slow down the overall efﬁciency. On the other hand, MOPT
only updates the probability distribution in the havoc stage,
independent from the deterministic stage. Therefore, dis-
abling the deterministic stage with the pacemaker fuzzing
mode could accelerate the convergence speed of MOPT.
• In this mode, the fuzzer can skip the deterministic stage,
without spending too much time on a sole test case. Instead,
it will pick more seeds from the fuzzing queue for mutation,
and thus has a better chance to ﬁnd vulnerabilities faster.
• The deterministic stage may have good performance
at the beginning of fuzzing, but becomes inefﬁcient after a
while. This mode selectively disables this stage only after
the efﬁciency slows down, and thus beneﬁts from this stage
while avoiding wasting much time on it.
More speciﬁcally, MOPT provides two types of pace-