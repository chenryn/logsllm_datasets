the correlation robust function and AES-128 in counter mode to instantiate the
pseudo-random generator and the key derivation function. Our benchmarking
environment consists of two 2.5 GHz Intel Core2Quad CPU (Q8300) Desktop
PCs with 4 GB RAM, running Ubuntu 10.10 and OpenJDK 6, connected by a
Gigabit LAN.
6.1 Base OTs
In the following, we compare the performance of the OT protocols of Naor and
Pinkas [46] in the random oracle (RO) and standard (STD) model to our STD
model OT protocol of §5.2 for diﬀerent libraries. We either use ﬁnite ﬁeld cryp-
tography (FFC) (based on the GNU-Multiprecision library v.5.0.5) or elliptic
curve cryptography (ECC) (based on the Miracl library v.5.6.1). We measure
the time for performing κ 1-out-of-2 base OTs on κ-bit strings, for symmetric
security parameter κ, using the key sizes from Tab. 1. The runtimes are shown
in Tab. 3.
Security
[46]-RO [46]-STD §5.2-STD
GMP (FFC)
99 (±0.6)
Short [ms]
Medium [ms] 107 (±3.4) 629 (±3.3)
Long [ms]
18 (±0.9)
41 (±3.3)
352 (±18)
288 (±7.9) 1,681 (±4.7) 1,217 (±47)
Miracl (ECC)
39 (±1.6) 178 (±0.3)
61 (±2.5)
Short [ms]
Medium [ms] 82 (±2.9) 418 (±0.6) 137 (±5.0)
138 (±5.0) 763 (±0.8) 239 (±7.5)
Long [ms]
Table 3. Performance results and standard deviations for base OTs.
For the short term security parameter, FFC using GMP outperforms ECC us-
ing Miracl by factor 2 for all protocols. However, starting from a medium term
security parameter, ECC becomes increasingly more eﬃcient and outperforms
FCC by more than factor 2 for the long term security parameter. For ECC,
we can observe that [46]-RO is about 5-6 times faster than [46]-STD but only
2 times faster than our §5.2-STD protocol. For FFC, our §5.2-STD protocol
becomes more ineﬃcient with increasing security parameter, since the random
sampling requires nearly full-range exponentiations as opposed to the subgroup
exponentiations in [46]-RO and [46]-STD.
6.2 OT Extension
To evaluate the performance of OT extension, we measure the time for generating
the random inputs for the OT extension protocol and the overall OT extension
protocol execution on 10,000,000 1-out-of-2 OTs on 80-bit strings for the short-
term security setting, excluding the times for the base OTs. Tab. 4 summarizes
20
Network Orig [56] Orig [56] EMT §4.2 G-OT §5.3 C-OT §5.4 R-OT §5.4 R-OT §5.4 R-OT §5.4
(2 T, §4.1) (4 T, §4.1)
(1 T)
13.92
(±0.07)
29.36
(±0.26)
(1 T)
10.60
(±0.03)
14.39
(±0.14)
5.03
(±0.08)
14.23
(±0.18)
2.62
(±0.05)
14.23
(±0.22)
LAN [s]
WiFi [s]
(2 T)
16.57
(±0.33)
30.42
(±0.20)
(1 T)
20.61
(±0.07)
30.69
(±0.18)
Table 4. Performance results and standard deviations for 10,000,000 1-out-of-2 OTs
on 80-bit strings using our optimizations in §4 and §5.
(1 T)
14.43
(±0.05)
30.45
(±0.24)
(1 T)
10.00
(±0.02)
14.22
(±0.12)
the resulting runtimes for the original version without (Orig [56] (1 T)) and with
pipelining (Orig [56] (2 T)), the eﬃcient matrix transposition (EMT §4.2), the
general protocol optimization (G-OT §5.3), the correlated OT extension protocol
(C-OT §5.4), the random OT extension protocol (R-OT §5.4), as well as a two
and four threaded version of R-OT (2 T and 4 T, cf. §4.1). The line (x T)
denotes the number of threads, running on each party. Since our optimizations
target both, the runtime as well as the amount of data that is transferred, we
assume two diﬀerent bandwidth scenarios: LAN (Gigabit Ethernet with 1 GBit
bandwidth) and WiFi (simulated by limiting the available bandwidth to 54 MBit
and the latency to 2 ms). As our experiments in Tab. 4 show, the LAN setting
beneﬁts from computation optimizations (as computation is the bottleneck),
whereas the WiFi setting beneﬁts from communication optimizations (as the
network is the bottleneck). All timings are the average of 100 executions with
one party acting as sender and the other as receiver. Note that each version
includes all prior listed optimizations.
LAN setting. The original OT extension implementation of [56] has a runtime
of 20.61 s without pipelining, which is reduced to only 80% (16.57 s) when using
pipelining. Implementing the eﬃcient matrix transposition of §4.2 decreases the
runtime to 70% of the one-threaded original version (14.43 s) and already out-
performs the pipelined version even though only one thread is used. The general
improved OT extension protocol of §5.3 removes the need to generate the ran-
dom matrix T , which reduces the runtime to 13.92 s. The C-OT extension of
§5.4 decreases the runtime to 10.60 s, since the protocol generates the random
input values for the sender. The R-OT extension of §5.4 further decreases the
runtime to 10.00 s, since the last communication step is eliminated. Finally, the
parallelized OT extension of §4.1 results in a nearly linear decrease in runtime
to 50% (5.03 s) for two threads and to 26% (2.62 s) for four threads. Overall,
using two threads, we decreased the runtime in the LAN setting by a factor of 3
compared to the two-threaded original implementation.
WiFi setting. In the WiFi setting, we observe that the one and two threaded
original implementation is already slower compared to the LAN setting. More-
over, all optimizations that purely target the runtime have little eﬀect, since
the network has become the bottleneck. We therefore focus on the optimiza-
tions for the communication complexity. The G-OT optimization of §5.3 only
21
slightly decreases the runtime since both parties have the same up and down-
load bandwidth and the channel from sender to receiver becomes the bottleneck
(cf. Tab. 2).7 The C-OT extension of §5.4 reduces the runtime by a factor of 2,
corresponding to the reduced communication from sender to receiver which is
now equal to the communication in the opposite direction. The R-OT extension
of §5.4 only slightly decreases the runtime, since now the channel from receiver
to sender has become the bottleneck. Finally, the multi-threading optimization
of §4.1 does not reduce the runtime as the network is the bottleneck.
7 Application Scenarios
OT extension is the foundation for eﬃcient implementations of many secure com-
putation protocols, including Yao’s garbled circuits implemented in the FastGC
framework [28] and GMW implemented in the framework of [10, 56]. To demon-
strate how both protocols beneﬁt from our improved OT extensions, we apply
our implementations to both frameworks and consider the following secure com-
putation use-cases: Hamming distance (§7.1), set-intersection (§7.2), minimum
(§7.3), and Levenshtein distance (§7.4). The overall performance results are sum-
marized in Tab. 5 and discussed in §7.5. All experiments were performed under
the same conditions as in §6 (LAN setting) using the random-oracle protocol
of [46] as base OT. We extended the FastGC framework [28] to call our C++
OT implementation using the Java Native Interface (JNI). We stress that the
goal of our performance measurements is to highlight the eﬃciency gains of our
improved OT protocols, but not to provide a comparison between Yao’s garbled
circuits and the GMW protocol.
Implementation
Base-OTs Hamming §7.1 Set-Intersect. §7.2 Minimum §7.3 Levenshtein §7.4
1094 s (552 s) 265 min (148 ms)
470 ms
FastGC [28]
1106 s (554 s) 266 min (157 ms)
FastGC [28] ﬁxed with CPRG 482 ms
FastGC [28] with C-OT (4 T)
69 ms
266 min (15 ms)
249 s (227 s)
253 s (227 s)
27 s (0.96 s)
149 ms (86.8 ms)
155 ms (87.6 ms)
85 ms (4.4 ms)
593 s (15 s)
GMW [56]
GMW [56] with R-OT (4 T)
142 ms
28 ms
79 ms (46.5 ms)
30 ms (11.3 ms)
1.91 s (1.34 s)
0.93 s (0.51 s)
AND gates
Client input bits
-
-
896
900
1,048,576
1,048,576
44 s (41 s)
21 s (19 s)
39,999,960
10,000,000
—
18 min (11 min)
1,290,653,042
2,000
Table 5. Performance results for the frameworks of [28] and [56] with and without our
optimized OT implementation. The time spent in the OT extensions is given in ().
7 For shorter strings or if the channel would have a higher bandwidth from sender
to receiver (e.g., a DSL link), the runtime would decrease already for the G-OT
optimization.
22
7.1 Hamming Distance
The Hamming distance between two (cid:96)-bit strings is the number of positions that
both strings diﬀer in. Applications of secure Hamming distance computation
include privacy-preserving face recognition [52] and private matching for car-
dinality threshold [33]. As shown in [28, 56], using a circuit-based approach is
a very eﬃcient way to securely compute the face recognition algorithm of [52]
which uses (cid:96) = 900. We use the compact Hamming distance circuit of [8] with
size (cid:96) − HW ((cid:96)) AND gates and (cid:96) input bits for the client, where HW ((cid:96)) is the
Hamming weight of (cid:96).
7.2 Set-Intersection
Privacy-preserving set-intersection allows two parties, each holding a set of σ-
bit elements, to learn the elements they have in common. Applications include
governmental law enforcement [11], sharing location data [47], and botnet de-
tection [45]. Several Boolean circuits for computing the set-intersection were
described and evaluated in [27]. The authors of [27] state that for small σ (up
to σ = 20 in their experiments), the bitwise AND (BWA) circuit achieves the
best performance. This circuit treats each element e ∈ {0, 1}σ as an index to a
bit-sequence {0, 1}2σ
and denotes the presence of e by setting the respective bit
to 1. The parties then compute the set-intersection as the bitwise AND of their
bit-sequences. We build the BWA circuit for σ = 20, resulting in a circuit with
2σ = 1,048,576 AND gates and input bits for the client. To reduce the memory
footprint of the FastGC framework [28], we split the overall circuit and the OTs
on the input bits into blocks of size 216 = 65,536.
7.3 Secure Minimum
Securely computing the minimum of a set of values is a common building block
in privacy-preserving protocols and is used to ﬁnd best matches, e.g., for face
recognition [15, 55], ﬁnger code authentication [2], or online marketplaces [10].
We use the scenario considered in [42] that securely computes the minimum of
N = 1,000,000 (cid:96) = 20-bit values, where each party holds 500,000 values. Using
the minimum circuit construction of [37], our circuit has 2(cid:96)N − 2(cid:96) ≈ 40,000,000
AND gates and the client has N
2 (cid:96) = 10,000,000 input bits. We note that the
performance of the garbled circuit implementation of [42] is about the same as
that of FastGC [28] – their circuit has twice the size and takes about twice as
long to evaluate. For the FastGC framework we again evaluate the overall circuit
by iteratively computing the minimum of at most 2,048 values.
7.4 Levenshtein Distance
The Levenshtein distance denotes the number of operations that are needed to
transform a string a into another string b using an alphabet of bit-size σ. It can
be used for privacy-preserving matching of DNA and protein-sequences [28, 34].
23
We use the same circuit and setting as [28] with σ = 2 to compare strings a
and b of size |a| = 2,000 and |b| = 10,000. The resulting circuit has 1.29 billion
AND gates and σ|a| = 4,000 input bits for the client. The GMW framework
of [56] was not able to evaluate the Levenshtein circuit since their OT extension
implementation tries to process all OTs at once and their framework tries to
store the whole circuit in memory, thereby exceeding the available memory of our
benchmarking environment. Hence, we changed their underlying circuit structure
to support large-scale circuits by deleting gates that were used and building the
circuit iteratively.
7.5 Discussion
We discuss the results of our experiments in Tab. 5 next. For the FastGC frame-
work [28], our improved OT extension implementation written in C++ and us-
ing 4 threads is more than one order of magnitude faster than the corresponding
single-threaded Java routine of the original implementation. The improvements
on total time depend on the ratio between the number of client inputs and the
circuit size: for circuits with many client inputs (§7.1, §7.2, §7.3), we obtain
a speedup by factor 2 to 9, whereas for large circuits with few inputs (§7.4)