USENIX Association
could rate limit the number of active connections and to con-
trol the growth of the FullConn table. In addition, the Poise
control plane periodically scans through the FullConn table
and expires inactive entries (using hardware support) to make
room for new connections.
Cache eviction attacks. The above algorithm defends against
a malicious attacker that generates many connections to over-
whelm the FullConn table. However, an attacker can also
launch targeted DoS attacks without initiating a suspiciously
large number of connections. Speciﬁcally, she could send
context packets more frequently than usual, and try to evict
cache entries from Cache that are mapped to the same bucket.
Although the attacker may not know the hash seed, therefore
cannot predict who would be the victim of the attack, she
could degrade the performance of the connection that shares
the same hash entry, if one exists. To prevent such attacks,
we enhance the cache eviction strategy. When replacing an
old entry eo with a new entry en, we check whether these two
entries are from the same source IP. If so, we immediately
replace the entries. If not, we opportunistically perform the
replacement. By doing so, we limit the amount of damage an
attack can cause by sending frequent context packets.
6 Orchestrating Poise
Next, we explain how we orchestrate the Poise in-network
primitive using a software controller, and describe the client
module that runs on the mobile devices for context collection.
The Poise controller. Poise has a controller that hosts the
compiler and distributes the generated data plane programs
to the switches. Unlike an OpenFlow-based SDN controller,
which actively makes decisions on behalf of the data plane, the
Poise controller is not involved in packet processing, so it does
not create any software bottleneck. The main controller runs
in a remote server, and uses well-deﬁned RPC calls to com-
municate with programmable switches’ local control planes.
Each switch has a local control plane running on the switch
CPUs, and it conﬁgures the switch data plane by installing
match/action table entries, loads new switch programs, and
serves as the primary logging component.
The Poise client module. Our client module PoiseDroid is in-
stalled at BYOD devices to collect context signals and embed
them into packets. PoiseDroid does not require modiﬁcation
of existing Android apps, but rather acts as a pre-positioned
kernel module. When the device connects to the enterprise
network, it needs to go through an authentication phase (e.g.,
using WPA3 [95], or additionally using two-factor authenti-
cation [71]). The module stops propagating context signals
when the device leaves the network. Figure 7 shows the archi-
tecture of PoiseDroid with three submodules.
The context submodule. It collects context information from
the Android system services [97] using usermode-helper
APIs [34, 63], and it registers a virtual device to redirect
the context data to our kernel module. The information to be
Android 
System Services
(e.g., location)
Usermode
Helper API
Virtual
Device
System Information 
Collector
Sensor Information 
Collector
Packet Monitor
Context Tagger
Netﬁlter
Kernel 
Socket
Context Sub-module
Network Sub-module
LSM-based Guard
Extended LSM
LSM-based Security Sub-module
User Space(cid:3)(cid:11)(cid:86)(cid:92)(cid:86)(cid:87)(cid:72)(cid:80)(cid:3)(cid:79)(cid:68)(cid:92)(cid:72)(cid:85)(cid:12)
Kernel Space (kernel layer)
Figure 7: The architecture of the PoiseDroid client module.
collected is speciﬁed by a BYOD client conﬁguration, which
includes a) app information, such as UIDs of active apps, b)
system information, such as screen light status, and c) device
status, such as accelerometer and gyroscope readings.
The protection submodule. It protects the registered virtual de-
vice, the system tools (e.g., dumpsys), and the system services
using LSM hooks in Android kernel [40, 72]. It monitors in-
vocations of selected system calls, such as ptrace(), open(),
mprotect() and chown(), and prevents any other processes to
write false data to these protected components.
The network submodule. It crafts and sends special context
packets with signals needed for the enterprise policies, using a
frequency speciﬁed in the conﬁguration. When an app opens
a new socket, or when an existing socket sends packets after
being dormant for a while, it also generates a context packet.
7 Limitations and Discussions
Authentication. As an access control mechanism, Poise fo-
cuses on resource authorization and should be used with an
authentication method, e.g., the SAE (simultaneous authenti-
cation of equals) protocol [57] in WPA3 [95], or two-factor
authentication with TOTP [71]. Only authenticated users can
further access enterprise resources in Poise.
Context integrity and privacy. One limitation of the cur-
rent Poise prototype is that it relies on external cryptographic
mechanisms to secure context packets. This is because today’s
P4 switches do not have built-in support for cryptography.
Adding cryptography support in P4 switches can be achieved
in two ways. First, the P4 standard allows cryptographic mod-
ules to be added as “externs”. The main Poise program can
invoke such an extern module to encrypt, decrypt, and authen-
ticate context packets. Second, a recent project SPINE [48]
shows that the current P4 language is expressive enough to
implement a keyed hash function. SPINE further leverages
this to generate one-time pads to encrypt/decrypt IP and TCP
headers at linespeed. Poise could use a similar design, where
clients encrypt context packets and the switch decrypts them
using shared keys. To protect integrity, Poise can additionally
use the keyed hash function to generate a MAC (message
authentication code) of the context ﬁelds at the clients, and
verify the MAC at the switch. To protect against replay at-
tacks, the context packets also need to include timestamps or
sequence numbers. Either way, the Poise switch or the “ex-
tern” module needs to be conﬁgured with key pairs with each
enterprise client.
USENIX Association
29th USENIX Security Symposium    603
Existing security mechanisms in enterprise networks can
also offer some support. Typically, client devices connect
to the network via wireless access points (APs), and then
to the wired network. Communication between clients and
APs can be protected by WPA3 [95], and communication
between the APs and the wired network by MACsec [15];
both can protect the integrity and conﬁdentiality of packets
and are secure against replay attacks [15, 95]. Under these
protections, context packets are always encrypted on (wired
and wireless) network links, therefore secure against network
reconnaissance attacks. However, supporting cryptography in
P4 switches would provide stronger, end-to-end guarantees.
8 Evaluation
In this section, we describe the experimental results obtained
using our Poise prototype. Our experiments are designed to
answer ﬁve research questions: a) How well does the Poise
compiler work? b) How efﬁciently can Poise process the
security contexts inside the network? c) How well does Poise
scale to complex policies? d) How much overhead does the
Poise client incur on mobile devices? and e) How does Poise
compare with traditional SDN-based security?
8.1 Prototype implementation
We have implemented the Poise prototype using 5918 lines
of code in C/C++ and Python [20]. The Poise compiler is
implemented in C++, using Bison 2.3 as the syntax parser,
and Flex 2.5.35 as the lexer. It can generate switch programs
in P4 for the Toﬁno hardware. The PoiseDroid client module
is implemented in C as a pre-positioned kernel module on
Linux 3.18.31. It extends the default LSM framework, SEAn-
droid, to implement the protection submodule. For evaluation,
PoiseDroid runs on a Pixel smartphone with a Qualcomm
Snapdragon 821 MSM8996 Pro CPU (4 cores) and Android
v7.1.2. The Poise control plane is implemented in Python,
and runs as part of the control plane software suite for the
Toﬁno switch. It manages the match/action table entries and
reconﬁgures the data plane programs. It can also be conﬁg-
ured to invoke the hardware-based packet generator on the
switch to send trafﬁc at linespeed (100 Gbps per port), which
we have used to test the latency and throughput of Poise.
Figure 8: Poise compiles the policies efﬁciently.
cables from the 100 Gbps switch ports to the 25 Gbps server
Ethernet ports. At linespeed, the testbed should achieve full
100 Gbps bandwidth per switch port.
On the ﬁrst server, one of its ports is conﬁgured to be an
enterprise server, and other ports are conﬁgured to emulate
a DPI device, a trafﬁc scrubber, and a logger, respectively.
The other server functions as an enterprise client. The mobile
traces are ﬁrst collected from our Pixel smartphone, and then
“stretched” to higher speeds to be replayed. The replay can
be initiated from a) the enterprise client, or b) the hardware
generator for Poise at linespeed.
8.3 Compiler
We start by evaluating the performance of the Poise compiler
and its generated programs. All programs support one million
connections in the FullConn table.
Compilation speed. In order to understand the performance
of our compiler, we measured the time it took to generate
switch programs for each of the seven policies. We found that
compilation ﬁnished within one second across all policies. P1
and P3 took slightly more time than the rest, because they
involve more context ﬁelds and our compiler needs to generate
more logic for header processing. Figure 8 shows the results.
Generated P4 programs. The generated P4 programs have
855-975 lines of code, which are signiﬁcantly more complex
than the original policy programs that only contain a few
lines of code. For one million connections across policies,
the utilization of Poise for SRAM (used for exact match) is
roughly 43%, for TCAM (used for longest-preﬁx match) is
below 1.1%, and for VLIWs (Very Long Instruction Words,
used for header modiﬁcations) is below 7%.
8.2 Experimental setup
8.4
In-network processing overhead
We set up a testbed with one Wedge 100BF Toﬁno switch and
two servers. The Toﬁno switch has a linespeed of 100 Gbps
per port, and 32 ports overall, achieving an aggregate through-
put of 3.2 Tbps when all ports are active. It also has a
200 Gbps pipeline—separate from the 32 regular ports—for
handling packet recirculation. Each server is equipped with
six Intel Xeon E5-2643 Quad-core CPUs, 128 GB RAM, 1 TB
hard disk, and four 25 Gbps Ethernet ports, which collectively
can emulate eight forwarding decisions (one per server port).
The servers are connected to the Toﬁno switch using breakout
Next, we turn to evaluate the overhead of Poise in terms of
packet processing latency and switch throughput.
Packet processing latency. Poise increases the overhead of
packet processing, because it needs to process context headers
and approximate per-ﬂow state. To quantify this overhead,
we have tested the latency for Poise to process a) a context
packet, b) a data packet, and compared them with c) the la-
tency for directly forwarding a packet without any processing.
Figure 9 shows that for all tested policies, the extra latency
on average is 72 nanoseconds for processing data packets,
604    29th USENIX Security Symposium
USENIX Association
0102030P1P2P3P4P5P6P7Compilation time (ms)Policyport depends on its available memory (SRAM and TCAM).
Exact checks (e.g., X==1) are supported by SRAM and range
checks (e.g., 10<X<20) by TCAM, so they are bottlenecked
by the SRAM and TCAM sizes, respectively. We ﬁrst mea-
sured the maximum number of exact checks Poise can per-
form on a single context, by asking the compiler to compose
more and more unit policies until the compilation failed. We
found that our switch can support 1.2 million checks, which
are spread across 5 hardware stages. We then modiﬁed all
unit policies to perform range checks, and found that Poise
can perform 55 k checks, as the TCAM size is smaller.
Number of contexts. Poise compiles each regular context
into a match/action table, so the number of contexts is bot-
tlenecked by the number of tables a switch can support. We
increased the number of contexts (e.g., time, library version)
from one to the maximum until compilation failed, and found
that Poise can support a maximum of 40 contexts—each of
the 5 stages can support 8 context tables.
For each data point, we also measured the number of checks
Poise can perform per context. We found that the number of
checks per context decreases as we add more contexts, as
the context tables need to multiplex switch memory. With 40
contexts, Poise can perform 21 k exact checks or 0.8 k range
checks per context (Figure 11a). In other words, Poise can
support at least 21 k distinct context values (e.g., user IDs for
per-user policies) or 0.8 k distinct context intervals (e.g., time
intervals for time-based access control).
We then modiﬁed all unit policies to check against network-
wide monitors. A monitor is compiled into two tables—one
for monitor updates, and another for monitor checks. Poise
supports a maximum of 20 monitors in 40 tables. Policies
can also use a mix of monitors and regular contexts. The con-
straint on the number of monitors m and the number of regular
contexts c is 2 × m + c ≤ 40, as they are all compiled into ta-
bles under the hood. In terms of the number of checks per
monitor, the results for a policy with m monitors are similar as
those for a policy with 2 × m regular contexts (Appendix A.3).
Overhead. We deﬁne a “baseline” to be the latency and
throughput for a unit policy, where a context packet traverses
the hardware stages exactly once without recirculation. A
packet with k contexts would be recirculated to traverse the
stages ⌈ k
5 ⌉ times, every time matching against 5 tables, one in
each stage. At the maximum, Poise supports 7 recirculations
for 40 contexts at a latency of 6.5µs (Figure 11b), which is still
orders of magnitude lower than typical enterprise RTTs (ms).
Recirculation also causes extra trafﬁc overhead. We measured
the overhead using 1 million connections and one context
packet per second per connection. As Figure 11c shows, the
maximum recirculation overhead is 0.37 Gbps per port. A
monitor policy with m monitors has similar results as a policy
with 2 × m regular contexts (Appendix A.3). Exact and range
checks have similar results, as the types of checks do not
affect the number of recirculations.
Figure 9: The amount of processing latency of Poise is small.
Figure 10: Poise achieves full linespeed programmability.
and 189 nanoseconds for processing context packets. In an
enterprise network where the round-trip times are on the order
of milliseconds, such a small extra latency is negligible.
Switch throughput. Next, we measured the throughput per
switch port using the hardware packet generator for stress
testing. The generator ingested mobile traces collected from
our phone, and stretched the trace to be 100 Gbps. Figure 10
shows the per-port throughput for all policies. As we can
see, although there is additional processing delay in Poise,
the pipelined nature of the switch hardware makes it achieve
full bandwidth nevertheless. In other words, Poise leverages
programmable data planes to enforce context-aware security
at linespeed, a key goal that we have designed for.
8.5 Scalability
Next, we evaluate how well Poise scales to complex policies.
As policies may perform different numbers of checks on dif-
ferent numbers of contexts, we deﬁne a “unit policy” to be
one that performs a single check on a single context. We then
create many unit policies, and use the Poise compiler to com-
pose them together. We characterize the complexity of the
composed policy in two dimensions: a) the number of checks
per context, and b) the number of contexts. For a), we further
distinguish between exact vs. range checks, and for b), we
distinguish between regular (i.e., non-monitor) vs. monitor
contexts. For instance, consider the following unit policies:
if match ( usr == Bob ) then fwd( mbox )
if match ( lib ==1.0.2) then fwd( server )
We say that the composed policy has two regular contexts and
performs two exact checks—one check per context.
Number of checks. Poise compiles each check into a
match/action entry, so the number of checks a switch can sup-
USENIX Association
29th USENIX Security Symposium    605
 0 100 200 300 400 500 600 700P1P2P3P4P5P6P7Latency (nanoseconds)PolicyBaselineContextData050100P1P2P3P4P5P6P7Per-port throughput (Gbps)Policy(a) Num. of contexts vs. num. of checks
(b) Num. of contexts vs. latency
(c) Num. of contexts vs. trafﬁc overhead
Figure 11: Poise can perform 1.2 million exact checks for a single context, or 21k exact checks for a maximum of 40 contexts.
Context packets with more than 5 contexts need to be recirculated multiple times; Poise supports a maximum of 7 recirculations,
which leads to a latency of 6.5µs and an additional 0.37 Gbps trafﬁc per port in a dedicated recirculation pipeline. Poise supports