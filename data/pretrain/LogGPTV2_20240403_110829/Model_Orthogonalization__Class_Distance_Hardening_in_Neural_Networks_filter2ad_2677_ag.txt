### Training to Improve Dataset Diversity

To enhance model quality, we employ adversarial training techniques such as Projected Gradient Descent (PGD) [67] and Fast is Better than Free (FBF) [53]. For adversarial training, PGD is used to harden the models with the following L∞ bounds: 8/255 for CIFAR-10, 0.03 for SVHN and GTSRB, and 0.1 for LISA. For training with universal adversarial perturbations (UAP), we utilize an existing method [43]. The L∞ bound for UAP training is determined based on the normal accuracy of the model.

### Adversarial Training with UAP

For UAP training, we use the following L∞ bounds:
- CIFAR-10: 4/255
- SVHN: 0.05
- LISA and GTSRB: 0.03

When applying generated backdoors directly in training (Pairwise), we harden every class pair over 100 iterations.

### Stability of Class Distance

We investigate the stability of class distance by using different sets and numbers of samples on CIFAR-10 and SVHN. Specifically, we select 100 different sets of 100 random samples from the validation set and apply Nearest Neighbor (NN) [40] to measure the distance as described in Section V-A. We also study the use of 200 samples. For CIFAR-10, we use a naturally trained ResNet20 model, and for SVHN, we use a naturally trained NiN model.

Figure 13 shows the results for CIFAR-10 using 100 random samples. Results for 200 random samples on CIFAR-10 and for SVHN are provided in our supplementary material [44]. The heat map on the left represents the means of class distances for all pairs, while the heat map on the right shows the standard deviations. The standard deviations of class distances are small for 100 random samples (4.56) and slightly smaller for 200 random samples (4.50). The average distances are slightly larger for 200 samples (61.31) compared to 100 samples (57.48). Similar observations are made for SVHN using 100 samples (69.25 ± 5.42) and 200 samples (73.43 ± 5.15).

### Alternative Backdoor Generation Method: ABS

We also evaluate the class distance using another backdoor generation method, ABS [41], on a naturally trained NiN model on CIFAR-10. Models hardened by NC [40], NAD [39], and MOTH are considered. Table IV presents the distances and relative improvements over the original model by different methods. ABS does not merge original pixels with trigger pixels; instead, it either completely replaces them or leaves them untouched, resulting in larger class distances compared to NC. The relative order of improvement for different hardened models (NC, NAD, and MOTH) is consistent for both NC and ABS. NAD has the smallest improvement (42.97%), NC shows a significant improvement (86.27%), and MOTH achieves the best performance (289.68%). This indicates that ABS can be an alternative tool for measuring class distance.

### Additional Results on Standard Datasets

We further evaluate various models and datasets for both naturally trained and adversarially trained models. The results are presented in Table V and Table VI. From Table V, we observe that MOTH can improve the class distance by 128.60% on average with a very small accuracy drop (0.69%). Baseline UAP can only harden the class distance on a few datasets and models. For some models, such as CNN on LISA, UAP is not effective.

#### Table V: Comparison of Different Methods on Hardening Class Distance for Naturally Trained Models

| Method | Accuracy | Increase | Degradation | Time (m) |
|--------|----------|----------|-------------|----------|
| Natural | 92.71% | - | - | 74.11 |
| NC | 91.54% | 18.65% | 0.60% | 183.00 |
| NAD | 92.65% | 73.29% | 69.74% | 6.01 |
| UAP | 91.83% | 93.57% | 98.79% | 345.61 |
| Universal | 91.69% | 1.17% | 0.06% | 145.02 |
| Pairwise | 90.43% | 1.85% | 0.20% | 209.00 |
| MOTH | 91.25% | 128.60% | 0.00% | 51.69 |

#### Table VI: Comparison of Different Methods on Hardening Class Distance for Adversarially Trained Models

[Table VI details will be provided in the supplementary material.]

These results demonstrate the effectiveness of MOTH in improving class distance with minimal impact on accuracy, making it a robust choice for model hardening.