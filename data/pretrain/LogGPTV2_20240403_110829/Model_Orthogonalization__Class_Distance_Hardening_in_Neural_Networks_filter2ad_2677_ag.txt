training to improve dataset diversity. Adversarial training with
PGD (Projected Gradient Descent) [67] and FBF (Fast
is
Better than Free) [53] is leveraged to improve model quality.
For adversarial training, we leverage PGD to harden models,
with L∞ bound of 8/255 for CIFAR-10 dataset, 0.03 for
SVHN and GTSRB, and 0.1 for LISA. For training with
universal adversarial perturbation (UAP), we utilize an ex-
isting work [43] for hardening models. The L∞ bound for
UAP training is determined according to the normal accuracy
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:03:02 UTC from IEEE Xplore.  Restrictions apply. 
151386
planecarbirdcatdeerdogfroghorseshiptruckplanecarbirdcatdeerdogfroghorseshiptruck0453541465050494150670778192787789855750640434143515270625961400403241506155516135470425150635867685338480525575627066334648530668063667465625248680826533474450555556610496061716578677671660Meanplanecarbirdcatdeerdogfroghorseshiptruck0334575533405566454333044536433430443443333305434333434033434433660655344445505324345554034454565540Standard Deviation020406080TABLE IV: Comparison of distance measures by differ-
ent backdoor generation methods. The fifth and seventh
columns show the average class distance measured by NC and
ABS [41], respectively.
D M Method Accuracy DistanceNC
60.67
67.40
62.04
121.64
IncreaseNC DistanceABS
166.12
270.83
206.09
490.25
IncreaseABS
-
86.27%
42.97%
289.68%
-
14.39%
4.35%
107.60%
88.09%
87.18%
83.68%
86.81%
Natural
NC
NAD
MOTH
0
1
-
R
A
F
I
C
N
N
i
drop. We use L∞ bound of 4/255 for CIFAR-10, 0.05 for
SVHN, and 0.03 for LISA and GTSRB. For directly applying
generated backdoors in training (Pairwise), we harden every
class pair for 100 iterations.
B. Stability of Class Distance
We study the stability of class distance by using different
sets/numbers of samples on CIFAR-10 and SVHN. Specif-
ically, we select 100 different sets of 100 random samples
from the validation set, and apply NC [40] to measure the
distance as described in Section V-A. We also study using
200 samples. We employ a naturally trained ResNet20 model
for CIFAR-10 and a naturally trained NiN model for SVHN.
Figure 13 shows the results for CIFAR-10 using 100 random
samples. Results of using 200 random samples for CIFAR-
10 and results for SVHN can be found in our supplementary
material [44]. The heat map on the left denotes the means of
class distances for all pairs and the heat map on the right
denotes the standard deviations. Observe that the standard
deviations of class distances are small for using 100 random
samples (4.56) for the measurement (4.50 for 200 random
samples), rendering the class distance measure quite stable.
The average distances are slightly larger for using 200 samples
(61.31) than using 100 samples (57.48). The observation is the
same for SVNH using 100 samples (69.25±5.42) and using
200 samples (73.43±5.15).
We also study using another backdoor generation method
ABS [41] for measuring the distance. The experiment
is
conducted on a naturally trained NiN model on CIFAR-
10. Models hardened by three methods, namely, NC [40],
NAD [39], and MOTH, are also considered for studying the
distance by ABS [41]. Table IV shows the distances and
relative improvements over the original model by different
methods. ABS does not merge original pixels with trigger
pixels. Instead, it either completely replaces them or keeps
them untouched. As such, the class distance measured by
ABS is larger than NC. When comparing the distances of
different hardened models (by NC, NAD, and MOTH), the
relative order is the same for the distance measure by NC and
ABS. NAD has the smallest improvement (42.97%), and NC
has a relatively large improvement (86.27%). MOTH achieves
the best performance (289.68%) regardless of the measure.
This delineates ABS an alternative tool for distance measure.
C. More Results on Standard Datasets
We evaluate on more models and datasets for both naturally
trained and adversarially trained models, which are presented
TABLE V: Comparison of different methods on hardening
class distance for naturally trained models.
Distance
D M
Time (m)
Accuracy
Increase
Degrad.
Method
0
1
-
R
A
F
I
C
0
5
t
e
N
s
e
R
A
S
I
L
N
N
C
0
2
t
e
N
s
e
R
B
R
S
T
G
N
N
i
Average
Natural
NC
NAD
UAP
Universal
Pairwise
MOTH
Natural
NC
NAD
UAP
Universal
Pairwise
MOTH
Natural
NC
NAD
UAP
Universal
Pairwise
MOTH
Natural
NC
NAD
UAP
Universal
Pairwise
MOTH
Natural
NC
NAD
UAP
Universal
Pairwise
MOTH
92.71%
91.54%
92.65%
91.83%
91.69%
90.43%
91.25%
97.30%
82.67%
95.45%
95.60%
96.45%
97.16%
96.31%
98.86%
98.44%
96.59%
96.16%
98.30%
98.72%
98.30%
95.28%
95.65%
93.63%
95.25%
95.22%
95.68%
95.55%
96.04%
92.08%
94.58%
94.71%
95.42%
95.50%
95.35%
74.11
183.00
6.01
345.61
145.02
209.00
51.69
0.15
8.27
0.15
1.79
11.34
204.98
13.09
1.70
34.42
0.72
6.33
25.37
738.92
38.75
4.60
110.51
0.51
20.55
25.20
1683.52
74.01
20.14
84.05
1.85
93.57
51.73
709.11
44.39
57.80
67.65
57.75
98.94
97.30
112.46
113.46
68.47
74.94
67.70
65.90
101.48
193.41
178.31
72.05
73.61
81.42
97.11
113.35
192.70
177.41
56.93
51.99
57.56
53.97
78.66
167.42
117.30
63.81
67.05
66.11
78.98
97.70
166.50
146.62
-
18.65%
0.60%
73.29%
69.74%
93.57%
98.79%
-
1.17%
0.06%
0.88%
1.02%
2.28%
1.46%
-
-
14.21% 14.63%
1.85%
0.20%
1.70%
-1.23%
46.13%
0.85%
0.14%
166.57%
158.35%
0.99%
-
3.08%
11.79%
36.32%
53.38%
168.74%
144.92%
-
-2.61%
7.93%
4.27%
47.69%
201.81%
112.34%
-
8.33
5.13
28.16%
54.24%
157.67%
128.60%
-
0.42%
2.27%
2.70%
0.57%
0.14%
0.57%
-
0.00%
1.65%
0.03%
0.06%
0.00%
0.00%
-
3.96%
1.46%
1.33%
0.62%
0.54%
0.69%
in Table V and Table VI, respectively. From Table V, we
can observe that with a very small accuracy drop (0.69%),
MOTH can improve the class distance by 128.60% on average
compared to the original model. Baseline UAP can only
harden the class distance on a few datasets and models. For
some models such as CNN on LISA, UAP is not able to