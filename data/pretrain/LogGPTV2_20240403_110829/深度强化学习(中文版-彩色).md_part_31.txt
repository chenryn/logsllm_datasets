网络这些属性组成，代码如下所示。
class DQN(object):
def __init__(self):
self.qnet = QFunc(’q’)
self.targetqnet = QFunc(’targetq’)
sync(self.qnet, self.targetqnet)
self.niter = 0
self.optimizer = tf.optimizers.Adam(lr, epsilon=1e-5, clipnorm=clipnorm)
申明一个内部方法，以装饰Q网络，之后再给DQN对象添加一个 get_action方法来执行
ϵ-贪心的行动。
@tf.function
def _qvalues_func(self, obv):
return self.qnet(obv)
def get_action(self, obv):
eps = epsilon(self.niter)
if random.random() = warm_start and i
transitions = buffer.sample(batch_size)
dqn.train(*transitions)
if done:
o = env.reset()
else:
o = o_
# episode in info is real (unwrapped) message
if info.get(’episode’):
nepisode += 1
reward, length = info[’episode’][’r’], info[’episode’][’l’]
print(
’Time steps so far: {}, episode so far: {}, ’
’episode reward: {:.4f}, episode length: {}’
.format(i, nepisode, reward, length)
)
我们在 3 个随机种子上运行了 Breakout游戏 107 个时间步（4×107 帧）。为了更好地可视
化，我们将训练时的片段奖励进行平滑处理。之后通过如下代码绘制均值和标准差，输出效果如
图4.8所示的红色区域。
138
4.8 DQN代码实例
图4.8 DQN及其变体在Breakout游戏中的效果（见彩插）
from matplotlib import pyplot as plt
plt.plot(xs, mean, color=color)
plt.fill_between(xs, mean - std, mean + std, color=color, alpha=.4)
DoubleDQN
DoubleDQN可以通过更新DoubleQ的估计来简单地实现。在智能体的 _tderror_func中
使用如下DoubleQ估计的代码进行替换即可。
# double Q estimation
b_a_ = tf.one_hot(tf.argmax(qnet(b_o_), 1), out_dim)
b_q_ = (1 - b_d) * tf.reduce_sum(targetqnet(b_o_) * b_a_, 1)
我们也在Breakout游戏上，使用3个随机种子运行了107 个时间步。输出效果显示在图4.8
上的绿色区域。
DuelingDQN
Dueling架构只对Q网络进行了修改，它可以通过如下方式实现：
class QFunc(tf.keras.Model):
def __init__(self, name):
super(QFunc, self).__init__(name=name)
139
第4章 深度Q网络
self.conv1 = tf.keras.layers.Conv2D(
32, kernel_size=(8, 8), strides=(4, 4),
padding=’valid’, activation=’relu’)
self.conv2 = tf.keras.layers.Conv2D(
64, kernel_size=(4, 4), strides=(2, 2),
padding=’valid’, activation=’relu’)
self.conv3 = tf.keras.layers.Conv2D(
64, kernel_size=(3, 3), strides=(1, 1),
padding=’valid’, activation=’relu’)
self.flat = tf.keras.layers.Flatten()
self.fc1q = tf.keras.layers.Dense(512, activation=’relu’)
self.fc2q = tf.keras.layers.Dense(action_dim, activation=’linear’)
self.fc1v = tf.keras.layers.Dense(512, activation=’relu’)
self.fc2v = tf.keras.layers.Dense(1, activation=’linear’)
def call(self, pixels, **kwargs):
# scale observation
pixels = tf.divide(tf.cast(pixels, tf.float32), tf.constant(255.0))
# extract features by convolutional layers
feature = self.flat(self.conv3(self.conv2(self.conv1(pixels))))
# calculate q-value
qvalue = self.fc2q(self.fc1q(feature))
svalue = self.fc2v(self.fc1v(feature))
return svalue + qvalue - tf.reduce_mean(qvalue, 1, keepdims=True)
我们同样在Breakout游戏上，使用3个随机种子运行了107个时间步。在图4.8上的青色区域是
该方法的输出效果。
经验优先回放
PER相较于标准的DQN有三个变化。首先，回放缓存维持了2个线段树进行取小和求和操
作，来高效地计算最小优先级和优先级之和。更具体地说，_it_sum属性是具备两个接口的求和
操作线段树对象，sum用于获得指定区间内的元素之和，而 find_prefixsum_idx用于查找更高
的索引i，以使最小的i个元素比输入值要小。
其次，为了代替原本的均匀采样，考虑比例信息的采样策略如下所示：
140
4.8 DQN代码实例
res = []
p_total = self._it_sum.sum(0, len(self._storage) - 1)
every_range_len = p_total / batch_size
for i in range(batch_size):
mass = random.random() * every_range_len + i * every_range_len
idx = self._it_sum.find_prefixsum_idx(mass)
res.append(idx)
return res
最后，不同于普通的回放缓存，PER必须返回采样经验的索引和标准化的权重。权重用于计
算加权Huber损失，而索引则用于更新优先级。采样步骤将被修改为
*transitions, idxs = buffer.sample(batch_size)
priorities = dqn.train(*transitions)
priorities = np.clip(np.abs(priorities), 1e-6, None)
buffer.update_priorities(idxs, priorities)
_train_func可修改为
@tf.function
def _train_func(self, b_o, b_a, b_r, b_o_, b_d, b_w):
with tf.GradientTape() as tape:
td_errors = self._tderror_func(b_o, b_a, b_r, b_o_, b_d)
loss = tf.reduce_mean(huber_loss(td_errors) * b_w)
grad = tape.gradient(loss, self.qnet.trainable_weights)
self.optimizer.apply_gradients(zip(grad, self.qnet.trainable_weights))
return td_errors
我们还是在Breakout游戏上，使用3个随机种子运行了107个时间步。图4.8上的洋红色区
域是该方法的输出效果。
深度Q分布网络
值分布强化学习对Q值进行估计。在本节中，我们将通过演示如何实现其中的C51技术，来
实现一种值分布强化学习方法。在Breakout游戏中，奖励都是正数。因此，我们将文献(Bellemare
etal.,2017)中值的范围[−10,10]换成[−1,19]，其中−1是为了允许一些近似误差。实现C51首
141
第4章 深度Q网络
先要做的是让Q网络给每个动作输出51个估计值，这点可以通过在最后的全连接层增加更多的
输出单元来实现。接着，为了替代TD误差，需要使用目标Q分布和估计分布之间的KL散度作
为误差：
@tf.function
def _kl_divergence_func(self, b_o, b_a, b_r, b_o_, b_d):
b_r = tf.tile(
tf.reshape(b_r, [-1, 1]),
tf.constant([1, atom_num])
) # batch_size * atom_num
b_d = tf.tile(
tf.reshape(b_d, [-1, 1]),
tf.constant([1, atom_num])
)
z = b_r + (1 - b_d) * reward_gamma * vrange # shift value distribution
z = tf.clip_by_value(z, min_value, max_value) # clip the shifted distribution
b = (z - min_value) / deltaz
index_help = tf.expand_dims(tf.tile(
tf.reshape(tf.range(batch_size), [batch_size, 1]),
tf.constant([1, atom_num])