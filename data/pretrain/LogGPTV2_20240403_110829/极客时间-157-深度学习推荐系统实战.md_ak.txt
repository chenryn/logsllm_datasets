# 06｜Embedding：所有人都在谈的Embedding技术到底是什么？你好，我是王喆。今天我们聊聊Embedding。 说起 Embedding，我想你肯定不会陌生，至少经常听说。事实上，Embedding技术不仅名气大，而且用 Embedding方法进行相似物品推荐，几乎成了业界最流行的做法，无论是国外的facebook、Airbnb，还是在国内的阿里、美团，我们都可以看到 Embedding的成功应用。因此，自从深度学习流行起来之后，Embedding就成为了深度学习推荐系统方向最火热的话题之一。但是 Embedding这个词又不是很好理解，你甚至很难给它找出一个准确的中文翻译，如果硬是翻译成"嵌入"、"向量映射"，感觉也不知所谓。所以索性我们就还是用Embedding 这个叫法吧。那这项技术到底是什么，为什么它在推荐系统领域这么重要？最经典的Embedding 方法 Word2vec的原理细节到底啥样？这节课，我们就一起来聊聊这几个问题。什么是 Embedding？简单来说，**Embedding就是用一个数值向量"表示"一个对象（Object）的方法**，我这里说的对象可以是一个词、一个物品，也可以是一部电影等等。但是"表示"这个词是什么意思呢？用一个向量表示一个物品，这句话感觉还是有点让人费解。这里，我先尝试着解释一下：一个物品能被向量表示，是因为这个向量跟其他物品向量之间的距离反映了这些物品的相似性。更进一步来说，两个向量间的距离向量甚至能够反映它们之间的关系。这个解释听上去可能还是有点抽象，那我们再用两个具体的例子解释一下。图 1 是 Google 著名的论文 word2vec 中的例子，它利用 word2vec这个模型把单词映射到了高维空间中，每个单词在这个高维空间中的位置都非常有意思，你看图1 左边的例子，从 king 到 queen 的向量和从 man 到 woman的向量，无论从方向还是尺度来说它们都异常接近。这说明什么？这说明词Embedding 向量间的运算居然能够揭示词之间的性别关系！比如 woman这个词的词向量可以用下面的运算得出：Embedding(**woman**)=Embedding(**man**)+\[Embedding(**queen**)-Embedding(**king**)\同样，图 1 右的例子也很典型，从 walking 到 walked 和从 swimming 到swam的向量基本一致，这说明词向量揭示了词之间的时态关系！这就是 Embedding技术的神奇之处。![](Images/e6bd8c8b9428fcb9ab70a6957d8d79ef.png)savepage-src="https://static001.geekbang.org/resource/image/19/0a/19245b8bc3ebd987625e36881ca4f50a.jpeg"}图1 词向量例子你可能会觉得词向量技术离推荐系统领域还是有一点远，那 Netflix应用的电影 Embedding 向量方法，就是一个非常直接的推荐系统应用。从Netflix 利用矩阵分解方法生成的电影和用户的 Embedding向量示意图中，我们可以看出不同的电影和用户分布在一个二维的空间内，由于Embedding 向量保存了它们之间的相似性关系，因此有了这个 Embedding空间之后，我们再进行电影推荐就非常容易了。具体来说就是，我们直接找出某个用户向量周围的电影向量，然后把这些电影推荐给这个用户就可以了。这就是Embedding技术在推荐系统中最直接的应用。![](Images/c7bad251230dad042b9a8a9ca657bda8.png)savepage-src="https://static001.geekbang.org/resource/image/11/ea/11717485943f92357ef21298185a08ea.jpeg"}图2 电影-用户向量例子Embedding 技术对深度学习推荐系统的重要性事实上，我一直把 Embedding技术称作深度学习的"基础核心操作"。在推荐系统领域进入深度学习时代之后，Embedding技术更是"如鱼得水"。那为什么 Embedding技术对于推荐系统如此重要，Embedding技术又在特征工程中发挥了怎样的作用呢？针对这两个问题，我主要有两点想和你深入聊聊。**首先，Embedding是处理稀疏特征的利器。** 上节课我们学习了 One-hot 编码，因为推荐场景中的类别、ID型特征非常多，大量使用 One-hot编码会导致样本特征向量极度稀疏，而深度学习的结构特点又不利于稀疏特征向量的处理，因此几乎所有深度学习推荐模型都会由Embedding 层负责将稀疏高维特征向量转换成稠密低维特征向量。所以说各类Embedding技术是构建深度学习推荐模型的基础性操作。**其次，Embedding 可以融合大量有价值信息，本身就是极其重要的特征向量。** 相比由原始信息直接处理得来的特征向量，Embedding 的表达能力更强，特别是Graph Embedding 技术被提出后，Embedding几乎可以引入任何信息进行编码，使其本身就包含大量有价值的信息，所以通过预训练得到的Embedding向量本身就是极其重要的特征向量。因此我们才说，Embedding技术在深度学习推荐系统中占有极其重要的位置，熟悉并掌握各类流行的Embedding方法是构建一个成功的深度学习推荐系统的有力武器。**这两个特点也是我们为什么把 Embedding的相关内容放到特征工程篇的原因，因为它不仅是一种处理稀疏特征的方法，也是融合大量基本特征，生成高阶特征向量的有效手段。**经典的 Embedding 方法，Word2vec提到 Embedding，就一定要深入讲解一下Word2vec。它不仅让词向量在自然语言处理领域再度流行，更关键的是，自从2013 年谷歌提出 Word2vec 以来，Embedding技术从自然语言处理领域推广到广告、搜索、图像、推荐等几乎所有深度学习的领域，成了深度学习知识框架中不可或缺的技术点。Word2vec作为经典的 Embedding 方法，熟悉它对于我们理解之后所有的 Embedding相关技术和概念都是至关重要的。下面，我就给你详细讲一讲 Word2vec的原理。 什么是 Word2vec？Word2vec 是"word tovector"的简称，顾名思义，它是一个生成对"词"的向量表达的模型。想要训练 Word2vec模型，我们需要准备由一组句子组成的语料库。假设其中一个长度为 \_T\_的句子包含的词有w1slate-object="mark"}] slate-object="inline",wslate-object="mark"}2slate-object="mark"}] slate-object="inline"......wslate-object="mark"}tslate-object="mark"}] slate-object="inline"，并且我们假定每个词都跟其相邻词的关系最密切。根据模型假设的不同，Word2vec 模型分为两种形式，CBOW 模型（图 3 左）和Skip-gram 模型（图 3 右）。其中，CBOW模型假设句子中每个词的选取都由相邻的词决定，因此我们就看到 CBOW模型的输入是 wt 周边的词，预测的输出是 wt。Skip-gram模型则正好相反，它假设句子中的每个词都决定了相邻词的选取，所以你可以看到Skip-gram 模型的输入是 wt，预测的输出是 wt周边的词。按照一般的经验，Skip-gram模型的效果会更好一些，所以我接下来也会以 Skip-gram 作为框架，来给你讲讲Word2vec 的模型细节。![](Images/8515bcdf22ec6524b0032eedf3551013.png)savepage-src="https://static001.geekbang.org/resource/image/f2/8a/f28a06f57e4aeb5f826df466cbe6288a.jpeg"}图3 Word2vec的两种模型结构CBOW和Skip-gramWord2vec 的样本是怎么生成的？我们先来看看**训练 Word2vec的样本是怎么生成的。** 作为一个自然语言处理的模型，训练 Word2vec的样本当然来自于语料库，比如我们想训练一个电商网站中关键词的 Embedding模型，那么电商网站中所有物品的描述文字就是很好的语料库。我们从语料库中抽取一个句子，选取一个长度为 2_c\_+1（目标词前后各选\_c\_个词）的滑动窗口，将滑动窗口由左至右滑动，每移动一次，窗口中的词组就形成了一个训练样本。根据Skip-gram模型的理念，中心词决定了它的相邻词，我们就可以根据这个训练样本定义出Word2vec模型的输入和输出，输入是样本的中心词，输出是所有的相邻词。为了方便你理解，我再举一个例子。这里我们选取了"Embedding技术对深度学习推荐系统的重要性"作为句子样本。首先，我们对它进行分词、去除停用词的过程，生成词序列，再选取大小为3的滑动窗口从头到尾依次滑动生成训练样本，然后我们把中心词当输入，边缘词做输出，就得到了训练Word2vec 模型可用的训练样本。![](Images/42755fdc645eca6d35f27c7c8d007eaf.png)savepage-src="https://static001.geekbang.org/resource/image/e8/1f/e84e1bd1f7c5950fb70ed63dda0yy21f.jpeg"}图4 生成Word2vec训练样本的例子Word2vec 模型的结构是什么样的？有了训练样本之后，我们最关心的当然是 Word2vec这个模型的结构是什么样的。我相信，通过第 3节课的学习，你已经掌握了神经网络的基础知识，那再理解 Word2vec的结构就容易多了，它的结构本质上就是一个三层的神经网络（如图5）。 ![](Images/6e8664023a248dd55f9158fbe067c7b1.png)savepage-src="https://static001.geekbang.org/resource/image/99/39/9997c61588223af2e8c0b9b2b8e77139.jpeg"}图5 Word2vec模型的结构它的输入层和输出层的维度都是 V，这个 V其实就是语料库词典的大小。假设语料库一共使用了 10000 个词，那么 V 就等于10000。根据图 4 生成的训练样本，这里的输入向量自然就是由输入词转换而来的One-hot 编码向量，输出向量则是由多个输出词转换而来的 Multi-hot编码向量，显然，基于 Skip-gram 框架的 Word2vec模型解决的是一个多分类问题。隐层的维度是 N，N的选择就需要一定的调参能力了，我们需要对模型的效果和模型的复杂度进行权衡，来决定最后N 的取值，并且最终每个词的 Embedding 向量维度也由 N来决定。 最后是激活函数的问题，这里我们需要注意的是，隐层神经元是没有激活函数的，或者说采用了输入即输出的恒等函数作为激活函数，而输出层神经元采用了softmax 作为激活函数。你可能会问为什么要这样设置 Word2vec的神经网络，以及我们为什么要这样选择激活函数呢？因为这个神经网络其实是为了表达从输入向量到输出向量的这样的一个条件概率关系，我们看下面的式子： {simplebar="init"} {.simplebar-wrapper style="margin: 0px;"} simplebar-height-auto-observer-wrapper simplebar-height-auto-observer simplebar-mask {.simplebar-offset style="right: 0px; bottom: 0px;"} {.simplebar-content-wrapper style="height: auto; overflow: hidden;"} {.simplebar-content style="padding: 0px;"} {slate-type="block-katex" ]{.strut style="height:1em;vertical-align:-0.25em;"}[p]{.mord.mathdefault}[]{.mspacestyle="margin-right:0.16666666666666666em;"}[[(]{.mopen .delimcenterstyle="top:0em;"}[[w]{.mord .mathdefaultstyle="margin-right:0.02691em;"}[[[]{.pstrutstyle="height:2.7em;"}O]{.mord .mathdefault .mtightstyle="margin-right:0.02778em;"}]{.mord .mtight}]{.sizing .reset-size6.size3.mtight}]{style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"}]{.vliststyle="height:0.32833099999999993em;"}[​]{.vlist-s}]{.vlist-r}[[]{.vliststyle="height:0.15em;"}]{.vlist-r}]{.vlist-t.vlist-t2}]{.msupsub}]{.mord}[]{.mspacestyle="margin-right:0.2777777777777778em;"}[∣]{.mrel}[]{.mspacestyle="margin-right:0.2777777777777778em;"}[[w]{.mord .mathdefaultstyle="margin-right:0.02691em;"}[[[]{.pstrutstyle="height:2.7em;"}I]{.mord .mathdefault .mtightstyle="margin-right:0.07847em;"}]{.mord .mtight}]{.sizing .reset-size6.size3.mtight}]{style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"}]{.vliststyle="height:0.32833099999999993em;"}[​]{.vlist-s}]{.vlist-r}[[]{.vliststyle="height:0.15em;"}]{.vlist-r}]{.vlist-t.vlist-t2}]{.msupsub}]{.mord}[)]{.mclose .delimcenterstyle="top:0em;"}]{.minner}[]{.mspacestyle="margin-right:0.2777777777777778em;"}[=]{.mrel}[]{.mspacestyle="margin-right:0.2777777777777778em;"}]{.base}[[]{.strutstyle="height:2.811251em;vertical-align:-1.221241em;"}[[]{.mopen.nulldelimiter}[[[]{.pstrut style="height:3em;"}[[[∑]{.mop .op-symbol.small-opstyle="position:relative;top:-0.0000050000000000050004em;"}[[[]{.pstrutstyle="height:2.7em;"}i]{.mord .mathdefault .mtight}[=]{.mrel.mtight}[1]{.mord .mtight}]{.mord .mtight}]{.sizing .reset-size6 .size3.mtight}]{style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"}[[]{.pstrutstyle="height:2.7em;"}V]{.mord .mathdefault .mtightstyle="margin-right:0.22222em;"}]{.mord .mtight}]{.sizing .reset-size6.size3 .mtight}]{style="top:-3.2029em;margin-right:0.05em;"}]{.vliststyle="height:0.981231em;"}[​]{.vlist-s}]{.vlist-r}[[]{.vliststyle="height:0.29971000000000003em;"}]{.vlist-r}]{.vlist-t.vlist-t2}]{.msupsub}]{.mop}[]{.mspacestyle="margin-right:0.16666666666666666em;"}[exp]{.mop}[]{.mspacestyle="margin-right:0.16666666666666666em;"}(]{.delimsizing.size1}]{.mopen .delimcenter style="top:0em;"}[[v]{.mord .mathdefaultstyle="margin-right:0.03588em;"}[[[]{.pstrutstyle="height:2.7em;"}[w]{.mord .mathdefault .mtightstyle="margin-right:0.02691em;"}[[[]{.pstrutstyle="height:2.5em;"}i]{.mord .mathdefault .mtight}]{.mord.mtight}]{.sizing .reset-size3 .size1.mtight}]{style="top:-2.357em;margin-left:-0.02691em;margin-right:0.07142857142857144em;"}]{.vliststyle="height:0.3280857142857143em;"}[​]{.vlist-s}]{.vlist-r}[[]{.vliststyle="height:0.143em;"}]{.vlist-r}]{.vlist-t.vlist-t2}]{.msupsub}]{.mord .mtight}]{.mord .mtight}]{.sizing.reset-size6 .size3.mtight}]{style="top:-2.453em;margin-left:-0.03588em;margin-right:0.05em;"}[[]{.pstrutstyle="height:2.7em;"}′]{.mord .mtight}]{.mord .mtight}]{.sizing.reset-size6 .size3.mtight}]{style="top:-2.989em;margin-right:0.05em;"}]{.vliststyle="height:0.6778919999999999em;"}[​]{.vlist-s}]{.vlist-r}[[]{.vliststyle="height:0.34709999999999996em;"}]{.vlist-r}]{.vlist-t.vlist-t2}]{.msupsub}]{.mord}[[]{.mord}[[[]{.pstrutstyle="height:2.7em;"}⊤]{.mord .mtight}]{.mord .mtight}]{.sizing.reset-size6 .size3.mtight}]{style="top:-2.9890000000000003em;margin-right:0.05em;"}]{.vliststyle="height:0.7751079999999999em;"}]{.vlist-r}]{.vlist-t}]{.msupsub}]{.mord}[[v]{.mord.mathdefault style="margin-right:0.03588em;"}[[[]{.pstrutstyle="height:2.7em;"}[w]{.mord .mathdefault .mtightstyle="margin-right:0.02691em;"}[[[]{.pstrutstyle="height:2.5em;"}I]{.mord .mathdefault .mtightstyle="margin-right:0.07847em;"}]{.mord .mtight}]{.sizing .reset-size3.size1.mtight}]{style="top:-2.3567071428571427em;margin-left:-0.02691em;margin-right:0.07142857142857144em;"}]{.vliststyle="height:0.3448em;"}[​]{.vlist-s}]{.vlist-r}[[]{.vliststyle="height:0.14329285714285717em;"}]{.vlist-r}]{.vlist-t.vlist-t2}]{.msupsub}]{.mord .mtight}]{.mord .mtight}]{.sizing.reset-size6 .size3.mtight}]{style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"}]{.vliststyle="height:0.15139199999999997em;"}[​]{.vlist-s}]{.vlist-r}[[]{.vliststyle="height:0.250305em;"}]{.vlist-r}]{.vlist-t.vlist-t2}]{.msupsub}]{.mord}[[)]{.delimsizing .size1}]{.mclose.delimcenterstyle="top:0em;"}]{.minner}]{.mord}]{style="top:-2.128769em;"}[[]{.pstrutstyle="height:3em;"}[]{.frac-linestyle="border-bottom-width:0.04em;"}]{style="top:-3.23em;"}[[]{.pstrutstyle="height:3em;"}[[exp]{.mop}[]{.mspacestyle="margin-right:0.16666666666666666em;"}(]{.delimsizing.size1}]{.mopen .delimcenter style="top:0em;"}[[v]{.mord .mathdefaultstyle="margin-right:0.03588em;"}[[[]{.pstrutstyle="height:2.7em;"}[w]{.mord .mathdefault .mtightstyle="margin-right:0.02691em;"}[[[]{.pstrutstyle="height:2.5em;"}O]{.mord .mathdefault .mtightstyle="margin-right:0.02778em;"}]{.mord .mtight}]{.sizing .reset-size3.size1.mtight}]{style="top:-2.3567071428571427em;margin-left:-0.02691em;margin-right:0.07142857142857144em;"}]{.vliststyle="height:0.3448em;"}[​]{.vlist-s}]{.vlist-r}[[]{.vliststyle="height:0.14329285714285717em;"}]{.vlist-r}]{.vlist-t.vlist-t2}]{.msupsub}]{.mord .mtight}]{.mord .mtight}]{.sizing.reset-size6 .size3.mtight}]{style="top:-2.4530000000000003em;margin-left:-0.03588em;margin-right:0.05em;"}[[]{.pstrutstyle="height:2.7em;"}′]{.mord .mtight}]{.mord .mtight}]{.sizing.reset-size6 .size3.mtight}]{style="top:-3.063em;margin-right:0.05em;"}]{.vliststyle="height:0.751892em;"}[​]{.vlist-s}]{.vlist-r}[[]{.vliststyle="height:0.347305em;"}]{.vlist-r}]{.vlist-t.vlist-t2}]{.msupsub}]{.mord}v]{.mord .mathdefaultstyle="margin-right:0.03588em;"}]{.mord}[[[]{.pstrutstyle="height:2.7em;"}[w]{.mord .mathdefault .mtightstyle="margin-right:0.02691em;"}[[[]{.pstrutstyle="height:2.5em;"}I]{.mord .mathdefault .mtightstyle="margin-right:0.07847em;"}]{.mord .mtight}]{.sizing .reset-size3.size1.mtight}]{style="top:-2.3567071428571427em;margin-left:-0.02691em;margin-right:0.07142857142857144em;"}]{.vliststyle="height:0.3448em;"}[​]{.vlist-s}]{.vlist-r}[[]{.vliststyle="height:0.14329285714285717em;"}]{.vlist-r}]{.vlist-t.vlist-t2}]{.msupsub}]{.mord .mtight}]{.mord .mtight}]{.sizing.reset-size6 .size3.mtight}]{style="top:-2.5500000000000003em;margin-right:0.05em;"}]{.vliststyle="height:0.15139199999999997em;"}[​]{.vlist-s}]{.vlist-r}[[]{.vliststyle="height:0.250305em;"}]{.vlist-r}]{.vlist-t.vlist-t2}]{.msupsub}]{.mord}[[)]{.delimsizing .size1}]{.mclose.delimcenterstyle="top:0em;"}]{.minner}]{.mord}]{style="top:-3.7400100000000003em;"}]{.vliststyle="height:1.59001em;"}[​]{.vlist-s}]{.vlist-r}[[]{.vliststyle="height:1.221241em;"}]{.vlist-r}]{.vlist-t.vlist-t2}]{.mfrac}[]{.mclose.nulldelimiter}]{.mord}]{.base}]{.katex-htmlaria-hidden="true"}]{.katex}]{.katex-display} {.simplebar-placeholder style="width: 432px; height: 92px;"} {.simplebar-track .simplebar-horizontal style="visibility: hidden;"} {.simplebar-scrollbar style="width: 0px; display: none; transform: translate3d(0px, 0px, 0px);"} {.simplebar-track .simplebar-vertical style="visibility: hidden;"} {.simplebar-scrollbar style="height: 0px; display: none;"}这个由输入词 WI 预测输出词 WO 的条件概率，其实就是 Word2vec神经网络要表达的东西。我们通过极大似然的方法去最大化这个条件概率，就能够让相似的词的内积距离更接近，这就是我们希望Word2vec 神经网络学到的。当然，如果你对数学和机器学习的底层理论没那么感兴趣的话，也不用太深入了解这个公式的由来，因为现在大多数深度学习平台都把它们封装好了，你不需要去实现损失函数、梯度下降的细节，你只要大概清楚他们的概念就可以了。如果你是一个理论派，其实 Word2vec还有很多值得挖掘的东西，比如，为了节约训练时间，Word2vec经常会采用负采样（Negative Sampling）或者分层 softmax（HierarchicalSoftmax）的训练方法。关于这一点，我推荐你去阅读Word2vec Parameter LearningExplained  slate-object="inline"这篇文章，相信你会找到最详细和准确的解释。怎样把词向量从 Word2vec 模型中提取出来？在训练完 Word2vec的神经网络之后，可能你还会有疑问，我们不是想得到每个词对应的 Embedding向量嘛，这个 Embedding 在哪呢？其实，它就藏在输入层到隐层的权重矩阵 WVxN中。我想看了下面的图你一下就明白了。![](Images/477946a25f4d27a10d39125209e18128.png)savepage-src="https://static001.geekbang.org/resource/image/0d/72/0de188f4b564de8076cf13ba6ff87872.jpeg"}图6 词向量藏在Word2vec的权重矩阵中你可以看到，输入向量矩阵 WVxN的每一个行向量对应的就是我们要找的"词向量"。比如我们要找词典里第 i个词对应的 Embedding，因为输入向量是采用 One-hot编码的，所以输入向量的第 i 维就应该是 1，那么输入向量矩阵 WVxN 中第 i行的行向量自然就是该词的 Embedding啦。 细心的你可能也发现了，输出向量矩阵W'也遵循这个道理，确实是这样的，但一般来说，我们还是习惯于使用输入向量矩阵作为词向量矩阵。在实际的使用过程中，我们往往会把输入向量矩阵转换成词向量查找表（Lookuptable，如图 7 所示）。例如，输入向量是 10000 个词组成的 one hot向量，隐层维度是 300 维，那么输入层到隐层的权重矩阵为 10000x300维。在转换为词向量 Lookup table 后，每行的权重即成了对应词的 Embedding向量。如果我们把这个查找表存储到线上的数据库中，就可以轻松地在推荐物品的过程中使用Embedding去计算相似性等重要的特征了。![](Images/f6f66b9e3919fe8aece8d234c980da5d.png)savepage-src="https://static001.geekbang.org/resource/image/1e/96/1e6b464b25210c76a665fd4c34800c96.jpeg"}图7 Word2vec的Lookup tableWord2vec 对 Embedding 技术的奠基性意义Word2vec 是由谷歌于 2013年正式提出的，其实它并不完全是原创性的，学术界对词向量的研究可以追溯到2003 年，甚至更早的时期。但正是谷歌对 Word2vec的成功应用，让词向量的技术得以在业界迅速推广，进而使 Embedding这一研究话题成为热点。毫不夸张地说，Word2vec 对深度学习时代 Embedding方向的研究具有奠基性的意义。从另一个角度来看，Word2vec的研究中提出的模型结构、目标函数、负采样方法、负采样中的目标函数在后续的研究中被重复使用并被屡次优化。掌握Word2vec 中的每一个细节成了研究 Embedding的基础。从这个意义上讲，熟练掌握本节课的内容是非常重要的。Item2Vec：Word2vec 方法的推广在 Word2vec 诞生之后，Embedding的思想迅速从自然语言处理领域扩散到几乎所有机器学习领域，推荐系统也不例外。既然Word2vec 可以对词"序列"中的词进行Embedding，那么对于用户购买"序列"中的一个商品，用户观看"序列"中的一个电影，也应该存在相应的Embedding 方法。![](Images/817befa2ec113c0401ad8511065b5509.png)savepage-src="https://static001.geekbang.org/resource/image/81/2b/81ffdccd011a5f2e45bd01fe23e2592b.jpeg"}图8 不同场景下的序列数据于是，微软于 2015 年提出了 Item2Vec 方法，它是对 Word2vec方法的推广，使 Embedding 方法适用于几乎所有的序列数据。Item2Vec模型的技术细节几乎和 Word2vec完全一致，只要能够用序列数据的形式把我们要表达的对象表示出来，再把序列数据"喂"给Word2vec 模型，我们就能够得到任意物品的 Embedding了。 Item2vec 的提出对于推荐系统来说当然是至关重要的，因为它使得"万物皆Embedding"成为了可能。对于推荐系统来说，Item2vec 可以利用物品的Embedding直接求得它们的相似性，或者作为重要的特征输入推荐模型进行训练，这些都有助于提升推荐系统的效果。小结这节课，我们一起学习了深度学习推荐系统中非常重要的知识点，Embedding。Embedding就是用一个数值向量"表示"一个对象的方法。通过 Embedding，我们又引出了Word2vec，Word2vec 是生成对"词"的向量表达的模型。其中，Word2vec的训练样本是通过滑动窗口一一截取词组生成的。在训练完成后，模型输入向量矩阵的行向量，就是我们要提取的词向量。最后，我们还学习了Item2vec，它是 Word2vec在任意序列数据上的推广。我把这些重点的内容以表格的形式，总结了出来，方便你随时回顾。![](Images/6e2000cf89b08f6b6713dd87823a2bc2.png)savepage-src="https://static001.geekbang.org/resource/image/e5/a9/e50d9ef5a9b3d1f6a5b834f71d3d9ca9.jpeg"}这节课，我们主要对序列数据进行了 Embedding化，那如果是图结构的数据怎么办呢？另外，有没有什么好用的工具能实现Embedding 技术呢？接下来的两节课，我就会一一讲解图结构数据的 Embedding方法 Graph Embedding，并基于 Spark对它们进行实现。课后思考在我们通过 Word2vec 训练得到词向量，或者通过 Item2vec得到物品向量之后，我们应该用什么方法计算他们的相似性呢？你知道几种计算相似性的方法？如果你身边的朋友正对 Embedding 技术感到疑惑，也欢迎你把这节课分享给TA，我们下节课再见！