# Title: Poster: Towards Automated Quantitative Analysis and Forecasting of Vulnerability Discoveries in Debian GNU/Linux

## Authors
Nikolaos Alexopoulos, Rolf Egert, Tim Grube, Max Mühlhäuser  
Technische Universität Darmstadt  
{alexopoulos, egert, grube, max}@tk.tu-darmstadt.de

## Abstract
Quantitative analysis and forecasting of software vulnerability discoveries are crucial for estimating patching costs and time, as well as for providing input to security metrics and risk assessment methodologies. However, current quantitative studies often require significant manual effort, rely on noisy datasets, and are challenging to reproduce. In this poster, we describe our ongoing work towards automated and reproducible quantitative analysis of vulnerabilities in Debian GNU/Linux packages. We focus on the challenges of automating the data collection process and ensuring high-quality data. We also present several hypotheses that can be investigated and provide preliminary results.

## Keywords
software security, vulnerabilities, open-source software, dataset

## ACM Reference Format
Nikolaos Alexopoulos, Rolf Egert, Tim Grube, and Max Mühlhäuser. 2019. Poster: Towards Automated Quantitative Analysis and Forecasting of Vulnerability Discoveries in Debian GNU/Linux. In 2019 ACM SIGSAC Conference on Computer and Communications Security (CCS '19), November 11–15, 2019, London, United Kingdom. ACM, New York, NY, USA, 3 pages. https://doi.org/10.1145/3319535.3363285

## 1. Introduction
As the security community matures, the need for robust security metrics and measurements becomes increasingly important [1]. The limitations of previous approaches are better understood [5], and quantitative analysis of the vulnerability discovery process is a key step towards this goal.

Security metrics, such as the expected vulnerability discovery rate, serve multiple purposes: (i) predicting patching costs, (ii) estimating security risks, and (iii) evaluating the overall security of products. However, developing, designing, and verifying such metrics is challenging, especially in complex software systems. Key challenges include: (i) the lack of reproducible and reusable quantitative studies, (ii) the availability of large, homogeneous datasets, and (iii) processes for evaluating and testing hypotheses to support the design and verification of security metrics.

In this work, we present our ongoing efforts to automate the quantitative analysis of vulnerabilities in Debian GNU/Linux packages. Specifically, we focus on automating the dataset collection process to generate homogeneous and extensive datasets for vulnerability analysis and forecasting. We introduce a framework that combines various data sources to create reproducible datasets, enabling hypothesis testing.

## 2. The Dataset
### 2.1 Dataset Collection
Previous research has shown that poor data sources (incomplete, disparate, or containing errors) can lead to incorrect conclusions in quantitative studies [2, 6]. Therefore, we need a dataset that is both large and adheres to common rules and procedures. Critical for our analysis is information about which versions of a software component are affected by a given vulnerability. We have determined that the data provided by the Debian Security Team [1] are currently the most reliable, large-scale, and publicly available source of vulnerability information.

#### 2.1.1 Basic Vulnerability Data
Basic vulnerability information is contained in Debian Security Advisories (DSAs), as shown in Figure 1. The Debian Vulnerability and Analysis Framework [2] provides a way to maintain a local, up-to-date database of these security reports and link them to Common Vulnerabilities and Exposures (CVE) reports from the National Vulnerability Database (NVD). This allows us to consider related information, such as the type of vulnerability (according to Mitre’s Common Weakness Enumeration, CWE) and its severity (Common Vulnerability Scoring System, CVSS).

#### 2.1.2 Source Code and Churn
Some hypotheses, such as investigating how changes between different versions affect the vulnerability discovery rate, require access to specific source code versions. We developed an automated method to download the source code of the version of the binary that was part of Debian's testing release. We chose the testing version because it allows us to investigate when changes were first introduced for testing in the community. We used the Debian snapshot project [4] to access chronologically preceding states of Debian, starting from 2005. A script was developed to change the `sources.list` file accordingly and use the `python-apt` library to download the source code of the package as it appeared in Debian repositories on a given date in the past. Special care was taken to handle package name changes by creating a list of synonymous packages. A modified version of `pkgdiff` [5] was then used to generate data regarding changes between successive versions, counted in the number of bytes added, deleted, or modified.

#### 2.1.3 Popularity
To investigate potential hypotheses regarding the relationship between the popularity of a software component and its vulnerability discovery rate, we turned to the Debian Popularity Contest [6]. This project collects anonymous statistics on package usage in Debian by volunteers who report their personal usage. Although current usage data are publicly available, we had to contact the Debian team to get historical versions of the data.

### 2.2 Dataset Summary
We populated the dataset with data for a selection of seven popular Debian packages: the Linux kernel, two web browsers (Firefox and Chromium), PHP, OpenJDK, Thunderbird, and Wireshark. For each package, there is one data point per month from March 2005 until December 2018. Each data point contains the following properties: the number of vulnerabilities (classified by severity and CWE type), popularity stats (installed, recently used, old), and churn stats (bytes added, deleted, changed compared to the previous month).

## 3. Hypotheses
There are several hypotheses that could be investigated based on our dataset. Some examples include:

- **H1:** Are vulnerability rates constant over time?
- **H2:** Can we forecast vulnerability discoveries?
- **H3:** Do updates lead to an increased number of discoveries?
- **H4:** What is the effect of package popularity?

In the next section, we present some preliminary results concerning H2 and H3.

## 4. Preliminary Results
We present preliminary results concerning two hypotheses: (a) can we forecast vulnerability discoveries using linear models based only on prior reports, and (b) is there a statistical causation between update magnitude and discoveries? All results presented below are generated via scripts running in Python notebooks.

### 4.1 ARIMA Forecasting
To produce forecasts for vulnerability rates, we employ autoregressive integrated moving average (ARIMA) models, which are well-studied and widely used in econometrics. We fit the models to each time series individually following the Box-Jenkins approach and present the results for Linux and OpenJDK in Figure 2. Specifically, we fit the models with data up until July 2018 and test them on unseen data from the last six months of 2018. Although we have a good fit of the fitted ARIMA model, further investigation into the potential causes of different discovery patterns among packages may lead to interesting observations.

### 4.2 Code Churn as a Potential Cause
To investigate the potential causal relationship (not just correlation) between updates (bytes added) and vulnerability discoveries, we use the Granger causality test on the time series. The results are summarized in the table below:

| Package   | Test Statistic | Critical Value | p-value |
|-----------|----------------|----------------|---------|
| Linux     | 2.041          | 7.815          | 0.564   |
| Firefox   | 6.673          | 7.815          | 0.083   |
| Chromium  | 2.477          | 5.991          | 0.290   |
| Wireshark | 1.898          | 7.815          | 0.594   |
| PHP       | 1.545          | 7.815          | 0.908   |
| OpenJDK   | 12.20          | 11.07          | 0.032   |

We observe that only OpenJDK has a statistically significant (with a 5% significance level) linear potentially-causal relationship between the number of bytes added to subsequent versions and vulnerabilities discovered in the stable release. For the other packages, there is no such significant relationship. For Linux, this was expected due to its continuous update cycle. Further investigation is necessary for the other cases.

## 5. Related Work
Our work is closely related to the studies by Roumani et al. [4] and Pokhrel et al. [3]. These authors use linear (ARIMA) or non-linear (feed-forward neural networks) forecasting techniques on the time series of historical vulnerabilities of a small selection of software to forecast future vulnerabilities. Our work is significantly novel in comparison, as we: (a) create and use a "cleaner" dataset (Debian DSAs vs. NVD), allowing us to draw more reliable conclusions; (b) develop an open-source forecasting and analysis tool—an automated procedure to validate our results; and (c) collect and take into account metadata (popularity, updates/churn) and explore their relationship to the vulnerability discovery rate.

## 6. Conclusions and Future Work
Reproducible quantitative studies are essential for security research and practice. In this poster, we presented a brief overview of our work towards reproducible analysis of vulnerabilities in Debian GNU/Linux. One of the early conclusions of our work is that the vulnerability discovery process is generally complex, and vulnerability discovery models proposed in the past are valid only for specific datasets they were tested against.

We are in the process of testing several hypotheses on the data with the ultimate aim of assessing the relevance of used or proposed security metrics and/or coming up with new metrics that can be empirically validated. An interesting avenue is testing for non-linear relationships in the data using machine learning models.

## Acknowledgments
This work was supported by the BMBF and the HMWK within CRISP.

## References
[1] Cormac Herley and Paul C Van Oorschot. 2017. SoK: Science, security and the elusive goal of security as a scientific pursuit. In 2017 IEEE Symposium on Security and Privacy (SP). IEEE, 99–120.

[2] Viet Hung Nguyen and Fabio Massacci. 2013. The (un) reliability of NVD vulnerable versions data: An empirical experiment on Google Chrome vulnerabilities. In Proceedings of the 8th ACM SIGSAC symposium on Information, computer and communications security. ACM, 493–498.

[3] Nawa Raj Pokhrel, Hansapani Rodrigo, and Chris P Tsokos. 2017. Cybersecurity: Time Series Predictive Modeling of Vulnerabilities of Desktop Operating System Using Linear and Non-Linear Approach. Journal of Information Security 8, 04 (2017), 362.

[4] Yaman Roumani, Joseph K Nwankpa, and Yazan F Roumani. 2015. Time series modeling of vulnerabilities. Computers & Security 51 (2015), 32–40.

[5] Vilhelm Verendel. 2009. Quantified security is a weak hypothesis: a critical survey of results and assumptions. In Proceedings of the 2009 workshop on New security paradigms workshop. ACM, 37–50.

[6] Su Zhang, Doina Caragea, and Xinming Ou. 2011. An empirical study on using the National Vulnerability Database to predict software vulnerabilities. In International Conference on Database and Expert Systems Applications. Springer, 217–231.