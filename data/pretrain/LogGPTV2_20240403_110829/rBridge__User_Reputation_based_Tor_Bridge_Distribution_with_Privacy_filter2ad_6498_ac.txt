duce over 1000 user-hours, and 70% of bridges can serve
more than 10 000 user-hours, before being blocked; about
50% of bridges are never blocked. Figure 2b shows that
with f = 5%, over 95% of users are never thirsty for
bridges; a small fraction (about 2%) of users are unable
to get new bridges, because all of their initially assigned
bridges get blocked before they earn enough credits to re-
quest new bridges. Figure 2c shows that users need some
time (150 ∼ 200 days) to accumulate enough credits to be-
come qualiﬁed for inviting friends. After the accumulation
phase, the user base starts to steadily grow almost linearly
with the number of newly recruited bridges. We also see
that rBridge performs relatively better with the staged dis-
tribution of malicious users than with the linear distribution;
this is because for the staged distribution most invited users
belong to the “more trusted” group, for which the probabil-
ity of being a malicious user is lower than that for the linear
distribution.
In addition, we evaluate rBridge with a much higher f
(using the same system conﬁguration). We can see that
rBridge can easily tolerate 10% malicious users; even when
f = 30%, the performance of rBridge is still acceptable; for
f ≥ 50%, rBridge fails to provide reasonable protection for
bridges.
Conservative blocking. There are two factors related to
the conservative blocking:
the probability of blocking a
bridge (p), and the waiting time to block a bridge (wait).
Since the time to earn credits from a bridge is upper-
bounded by T1, we assume a corrupt user always blocks
his bridges by time T1 (i.e., wait ≤ T1). In the simula-
tion, we consider the following cases for the waiting time:
0 day (i.e., aggressive blocking), 120 days (by which the
earned credits are sufﬁcient to get a new bridge), and 225
days (i.e., the middle point between T0 and T1).
We can see from Figure 3 that compared with the aggres-
sive blocking, the conservative blocking causes less damage
to the user-hours and the growth of user base; this is be-
cause the bridges under the conservative blocking can serve
a longer time and more users can accumulate enough credits
to invite new users. We also notice that when wait = 225
days and p = 100%, about 10% of users are thirsty for 15%
of their time, which is worse than the aggressive blocking;
the reason for this is that after waiting 225 days, malicious
users earn enough credits to be considered for inviting new
(malicious) users (i.e., (225 − 75) × 3 = 450 > 236), and
overall they can block more bridges, which causes more re-
cently joined users to become thirsty.
Event-driven blocking. For event-driven blocking, we let
10210410600.20.40.60.81Use hours of bridgesCDF  p=100%,wait=225p=100%,wait=120p=100%,wait=0p=50%,wait=225p=50%,wait=120p=50%,wait=000.20.40.60.810.50.60.70.80.91% thirsty hourCDF  p=100%,wait=225p=100%,wait=120p=100%,wait=0p=50%,wait=225p=50%,wait=120p=50%,wait=0010020030040050001000200030004000500060007000Time (day)Num. of users  p=100%,wait=225p=100%,wait=120p=100%,wait=0p=50%,wait=225p=50%,wait=120p=50%,wait=0(a) Unblocked bridges
(b) Thirsty users
(c) Thirsty users with backup bridges
Figure 4: Event-driven blocking (f = 5%)
all of the corrupt users are synchronized to block all their
bridges simultaneously on the 300-th day. Figures 4a and 4b
show that right after the massive blocking, the number of
available bridges drops from 500 to 150, and the percent-
age of thirsty users rises to 25%. We note that the damage
of the event-driven blocking can be effectively mitigated by
keeping a small number of backup bridges (that are never
seen by any user). We can see from Figure 4c that with 50
backup bridges (about 10% of deployed bridges), the num-
ber of thirsty users can be reduced by half; with 100 backup
bridges, the number of thirsty users is minimized. We also
notice that keeping backup bridges cannot entirely eliminate
thirsty users; this is because there are a small fraction (about
10%) of users who join the system not long before the mas-
sive blocking and have not accumulated enough credits to
request new bridges.
4.3.3 Comparison with Proximax
W now compare rBridge with Proximax [15]—the state-of-
the-art proxy distribution scheme. Using the same method-
ology, we developed an event-based simulator for Proxi-
max. Since the authors of Proximax did not provide suf-
ﬁcient details about how to invite new users, we only con-
sider a static set of users for the comparison. We evaluate
both rBridge and Proximax under the aggressive blocking
using the same system conﬁguration as before; for Proxi-
max, we set the maximum delay of distributing bridges at
each hop (i.e., from when a user receives a bridge to when
he distributes the bridge to his friends) to 1 day4.
Figure 5a shows that in Proximax less than 5% bridges
are able to produce more than 20 user-hours, and none of the
bridges can serve over 126 user-hours. In comparison, in
rBridge, over 99% bridges can produce over 20 user-hours,
and 57% bridges are not ever blocked and are able to contin-
uously generate user-hours. In addition, in Proximax 99%
4In the simulation of Proximax, we found that higher propagation delay
leads to higher user-hours; hence, we chose a fairly large value (1 day).
of users are always thirsty for bridges, and this number is
only 10% for rBridge. Since our simulation for the compari-
son only considers a static user group, we are unable to eval-
uate Proximax in terms of the growth of the user base over
time; instead, we measure how many bridges are required to
support different-sized user bases for 30 days, while mak-
ing sure that each existing user has at least one unblocked
bridge at any time. We can see from Figure 5c that Proxi-
max requires a substantially larger number of bridges than
rBridge; for instance, to support 200 users, Proximax re-
quires at least 2400 bridges, while rBridge only needs 108
bridges. We note that for all these metrics (user-hours of
bridges, thirsty-hours of users, and bridge consumption),
the performance of rBridge is at least one order of mag-
nitude higher than that of Proximax.
Discussion. It is possible to improve Proximax by adopt-
ing a more restrictive distribution strategy, e.g., limiting
how many people a bridge recipient can share the bridge
with (i.e., the width of the distribution tree) as well as how
many hops a bridge can be distributed (i.e., the depth of
the distribution tree). Figure 5 also provides the results for
limiting the maximum width and depth of the distribution
tree to 5. While the restrictive distribution strategy can im-
prove the robustness of Proximax, it is still much worse
than rBridge. More importantly, the restrictive approach de-
grades the openness of the system.
The main reason that rBridge outperforms Proximax is
that Proximax calculates “reputation” at the granularity of
distribution trees (or called distribution channels) rather
than individual users, and the formation of each distribution
tree is ﬁxed and thus an honest user would be permanently
“infected” if he resides in a tree that contains a corrupt user.
Whereas, rBridge allows a user to join a different user group
when receiving a new bridge, and keeps track of each in-
dividual user’s records, based on which the bridge distrib-
utor can reward well-behaving users and punish blockers
individually. We note that recording each individual user’
bridge assignment leads to greater risks of violating users’
01002003004005000100200300400500Time (day)Num. of unblocked bridges  linearstaged010020030040050000.050.10.150.20.25Time (day)Perc. of thirsty users  linearstaged010020030040050000.050.10.150.20.25Time (day)Perc. of thirsty users  linear, bkup=20linear, bkup=50linear, bkup=100staged, bkup=20staged, bkup=50staged, bkup=100(a) User-hours
(b) Thirsty-hours
(c) Required bridges to support a certain # of
users for 30 days
Figure 5: Comparison with Proximax (f = 5%)
privacy; we describe how to perfectly protect users’ bridge
assignment information in the next section.
Finally, we note that computing reputation merely based
on the bridges’ uptime is not the only way to design the rep-
utation system. For instance, it is possible to extend rBridge
by including the reputations of the “introducees” as a factor
to calculate the reputation of the “introducer”. However, the
increased complexity of the reputation system makes it even
harder (if possible) to design a practical privacy-preserving
mechanism to perfectly protect users’ bridge assignment in-
formation. Therefore, our design philosophy is to make the
reputation system as simple as possible, while ensuring its
robustness against various blocking strategies.
5 rBridge with Privacy Preservation
In the basic rBridge scheme, D knows all the bridge as-
signment details of each user. A malicious entity equipped
with such information can degrade the users’ anonymity in
the anonymous communication. For instance, the current
Tor network ensures that each user can conceal his identity
as long as the entry relay (i.e., the bridge) is not compro-
mised. Whereas, with the bridge assignment information,
an adversary is able to narrow the anonymity set of the user
down to a small group of people who are given this bridge,
even though the bridge is not compromised. Unfortunately,
this issue is overlooked by all the prior bridge distribution
schemes. Our goal is to preserve the bridge information of
each user.
5.1 Challenges and Requirements
In rBridge, a user (U) can get a bridge only if he can
authenticate himself to D by presenting a valid credential
(recall that in the basic scheme, a credential includes the
following information U∥(cid:8)∥{Bi, τi, ϕi}k
i=1). Firstly, in or-
der to unlink the user from his assigned bridges, we should
conceal the user’s identity by replacing U’s real identity
with a pseudonym on his credential and letting him build a
Tor circuit to communicate with D to hide his IP address.
However, the above measures are not sufﬁcient. Suppose
U has received 10 bridges from D (who knows the bridges
but not U’s identity), and 2 of them are malicious and know
U’s identity due to direct contact and collude with D; then
it is highly likely that D can link U to all of his bridges,
since very few users happen to know both of the malicious
bridges. A natural solution to this is using Oblivious Trans-
fer (OT) for privacy-preserving bridge retrieval — prevent-
ing D from learning which bridge is retrieved when U re-
quests a new bridge (called a transaction). However, since
a user is very likely to request a new bridge right after one
of his bridges gets blocked, D can infer the blocked bridge
by checking which bridge was recently blocked. As a re-
sult, D can learn all of U’s (blocked) bridges as long as D can
link different transactions of U. Therefore, unlinkability of
transactions is required to avoid such information leaks.
Thirdly, since we intend to hide the bridge assignment
from D, the bridge related information in U’s credential, such
as {Bi, τi, ϕi}, should be written and updated by U, rather
than by D. This raises the risk that a malicious user could put
incorrect information on his credential, e.g., by changing
′
i so that
the credits ϕi or replacing Bi with another bridge B
he can block Bi without being punished. Therefore, we also
need to protect the integrity of credentials.
Although researchers have proposed several designs for
anonymous authentication/reputation systems [7, 8, 21, 22],
none of them is able to ensure integrity of credentials.
In this work, we propose a novel privacy-preserving user
reputation scheme that is specially designed for bridge
distribution and satisﬁes all the three aforementioned re-
quirements. Our design integrates OT with several other
cryptographic primitives (such as commitments and zero-
knowledge proofs) to both preserve users’ privacy and pre-
vent misbehavior of corrupt users. We start with introducing
10010200.51Use hours of bridgesCDF  rBridge: staged/linearProximax: staged, no limitProximax: linear, no limitProximax: staged, width (cid:8)θ∧
(C(cid:8), O(cid:8)) = CMT((cid:8))∧
Verify(P KD, σ(cid:8), C(cid:8)) = Accept
π = NIPK
To make it hard for corrupt users to collaboratively enu-
merate bridges, we let D (i.e., the sender) randomly shufﬂe
the list of available bridges (i.e., the secrets) before running
the OT protocol with each user (i.e., the chooser), so that
the user will randomly “choose” which bridge to get. Be-
cause of the randomized OT, it is possible that a user gets
a bridge that is already assigned to him even though the
chance is very small. We show how to deal with duplicate
bridges in Appendix A.
5.2.2 Commitment
A commitment scheme enables a party to create the digital
equivalent of an envelope for a secret. It supports two im-
portant properties: hiding protects the secrecy of the com-
mitted message, and binding ensures it can only be opened
to the committed message. Pedersen commitments [18] are
information-theoretically hiding and binding under the dis-
crete logarithm assumption. We use (C, O) = CMT(M ) to
denote a Pedersen commitment to a secret M, where C is
the commitment and O is the opening to the commitment.
In rBridge, we use commitments to conceal the content
on a user’s credential. For instance, to hide the amount
of credits (cid:8), U can compute a commitment of (cid:8),
i.e.,
(C(cid:8), O(cid:8)) = CMT((cid:8)), and put C(cid:8) in his credential. To
prevent U from manipulating his credential (e.g., increas-
ing (cid:8)), we let D sign C(cid:8) using his private key SKD, i.e.,
σ(cid:8) = Sign(SKD, C(cid:8)) and tag the signature σ(cid:8) to the cre-
dential, and U needs to prove to D that both the commit-
ment and the signature are valid. To prevent D from linking
U’s transactions based on the values of commitments and
signatures, we need another cryptographic primitive—zero-
knowledge proof.
5.2.3 Zero-Knowledge Proof
In a zero-knowledge proof scheme, a prover convinces
a veriﬁer that some statement is true while the veriﬁer
learns nothing except the validity of the statement. A zero-
knowledge proof can be converted into a corresponding
to prove that his credit balance (cid:8) is above the threshold (cid:8)θ
and is not tampered (i.e., correctly signed), without reveal-
ing the credit balance (cid:8), the commitment C(cid:8), the opening
O(cid:8), or the signature σ(cid:8) (where P KD is the public key of D,
and Verify is the function to verify the signature σ(cid:8)). In our
construction, we employ the k-TAA blind signature scheme
proposed by Au et al. [8] because of its compatibility with
zero-knowledge proofs.
5.3 Scheme
We now present the rBridge scheme with privacy preser-
vation. We refer interested readers to Appendix B for de-
tailed cryptographic constructions of the scheme.
5.3.1 Anonymous Credential
A key concept in rBridge is the anonymous credential,
which anonymously records the user’s bridges and reputa-
tion and allows the user to anonymously authenticate him-
self to D. Each part of the credential is signed by D individ-
ually, so that they can be veriﬁed and updated separately. In
particular, an anonymous credential contains the following
information:
i=1
x∥{(cid:8), C(cid:8), O(cid:8), σ(cid:8)}∥{ω, Cω, Oω, σω}∥{Bi, τi, ϕi, Ci, Oi, σi}k
where x is the secret key that is selected by U when reg-
istering the credential, σ♣ is the signature on C♣, and ω
denotes the latest time when the user requests an invitation
ticket (ω is used to prevent corrupt users from repeatedly
requesting tickets; we discuss this later). We note that all
the information in the credential must be kept secret from
D.
To prevent a corrupt user from replacing some part
of his credential with that of others’ credentials (e.g., a
higher (cid:8) from a well-behaving colluding user), we use x
to link different parts of the credential by including x in
each of their commitments. To be speciﬁc, (C(cid:8), O(cid:8)) =
CMT((cid:8), x), (Cω, Oω) = CMT(ω, x), and (Ci, Oi) =
CMT(Bi, τi, ϕi, x). To get a credential, a new user runs
the following registration protocol.
5.3.2 Registration
π1 = NIPK
∗
∗
m
1
)
(
, HMACscrtD(r
)}, where r
A new user U ﬁrst presents an invitation ticket to D. An
invitation ticket is an one-time token formed as tk =
{r
∗ is a random number and
scrtD is a secret only known to D; the ticket is veriﬁable to
D, and cannot be forged by anyone else. Then U runs
-
OT with D to get k initial bridges B1,··· , Bk
5. After that,
U randomly picks a secret key x and performs the follow-
ing computations: set (cid:8) = 0 and compute (C(cid:8), O(cid:8)) =
CMT((cid:8), x); set ω = Tcur (recall that Tcur denotes the
current time) and compute (Cω, Oω) = CMT(ω, x); for
each i ∈ [1, k], set τi = Tcur, ϕi = 0, and compute
(Ci, Oi) = CMT(Bi, τi, ϕi, x). To prevent multiple collud-
ing users from using the same x to share some parts of their
credentials, U is required to provide an indicator of his se-
lected x, formed as κx = OWF(x), to prove that x has not
been used by other users while hiding x from D. (wherein
OWF(·) is simply a discrete-log based one-way function.)
Note that since D does not know the bridges received by
U (i.e., {Bi}k
i=1), U could try to put other bridges on his cre-
∗
i in CMT(Bi, τi, ϕi, x), so
dential, i.e., replacing Bi with B
that he can block all of {Bi}k
i=1 instantly without worry-
ing about potential loss of credits. To prevent this attack,
D needs to verify that the bridges to be written in the cre-