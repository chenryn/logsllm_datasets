title:Practical, Real-time Centralized Control for CDN-based Live Video
Delivery
author:Matthew K. Mukerjee and
David Naylor and
Junchen Jiang and
Dongsu Han and
Srinivasan Seshan and
Hui Zhang
Practical, Real-time Centralized Control for
CDN-based Live Video Delivery
Matthew K. Mukerjee(cid:63)
PI:EMAIL
Dongsu Han†
PI:EMAIL
(cid:63)Carnegie Mellon University †KAIST ‡Conviva Inc.
David Naylor(cid:63)
PI:EMAIL
Srinivasan Seshan(cid:63)
PI:EMAIL
Junchen Jiang(cid:63)
PI:EMAIL
Hui Zhang(cid:63)‡
PI:EMAIL
Abstract
Live video delivery is expected to reach a peak of 50
Tbps this year [7]. This surging popularity is funda-
mentally changing the Internet video delivery landscape.
CDNs must meet users’ demands for fast join times,
high bitrates, and low buﬀering ratios, while minimizing
their own cost of delivery and responding to issues in
real-time. Wide-area latency, loss, and failures, as well
as varied workloads (“mega-events” to long-tail), make
meeting these demands challenging.
An analysis of video sessions [32] concluded that a
centralized controller could improve user experience, but
CDN systems have shied away from such designs due to
the diﬃculty of quickly handling failures [29], a require-
ment of both operators and users. We introduce VDN, a
practical approach to a video delivery network that uses
a centralized algorithm for live video optimization. VDN
provides CDN operators with real-time, ﬁne-grained con-
trol. It does this in spite of challenges resulting from the
wide-area (e.g., state inconsistency, partitions, failures)
by using a hybrid centralized+distributed control plane,
increasing average bitrate by 1.7× and decreasing cost
by 2× in diﬀerent scenarios.
CCS Concepts
•Networks → Traﬃc engineering algorithms; Over-
lay and other logical network structures;
Keywords
live video; CDNs; central optimization; hybrid control
Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for proﬁt or commercial advantage and that copies bear this notice
and the full citation on the ﬁrst page. Copyrights for components of this work
owned by others than ACM must be honored. Abstracting with credit is per-
mitted. To copy otherwise, or republish, to post on servers or to redistribute to
lists, requires prior speciﬁc permission and/or a fee. Request permissions from
permissions@acm.org.
SIGCOMM ’15, August 17 - 21, 2015, London, United Kingdom
c(cid:13) 2015 ACM. ISBN 978-1-4503-3542-3/15/08. . . $15.00
DOI: http://dx.doi.org/10.1145/2785956.2787475
1 Introduction
Demand for live video is increasing by 4–5× every three
years and the live video peak streaming rate is expected
to reach 50 Tbps this year [7]. This demand spans
widely diﬀerent types of videos (professionally-produced
and user-generated) and workloads (“mega-events” to
long-tail). The 2014 World Cup, a recent live video mega-
event, used traditional CDNs to deliver live streams to-
taling several terabits per second [38], which is estimated
to be 40% of all Internet traﬃc during that time [37].
At the other extreme, 55 million Twitch users [4] watch
more than 150 billion minutes of live video each month,
generated by over 1 million users, making it the fourth
largest Internet traﬃc producer in the US [5, 41].
The diversity and volume of live video delivery makes
it a complex challenge for modern content delivery in-
frastructures. However, huge demand isn’t the only
challenge; users, CDNs, and the network environment
impose additional requirements. Users demand high
quality, instant start-up (join) times, and low buﬀering
ratios [10]. CDNs want to meet client demands while
minimizing their delivery costs and responding to issues
in real-time [35]. Finally, operating over the wide-area
network environment requires designs that handle com-
mon latency variations and communication failures.
The traditional solution to these problems is traﬃc
engineering. However, even state-of-the-art systems [23,
25] work on traﬃc aggregates at coarse timescales. Users’
demands for high per-stream quality and CDNs’ de-
mands for fast failure recovery require control over in-
dividual streams at ﬁne timescales. Overlay multicast
systems [12, 14, 26, 30], while focusing on individual
stream optimization, overlook issues that arise with the
many concurrent, independent, high-bandwidth streams
in today’s environment. Internet-scale, video-speciﬁc
systems like Conviva’s C3 [19] use client-side analytics
to pick the best CDN for a given client at a given time
but ignore the actual data delivery. Although current
CDNs provide good video quality, a previous analysis of
a large collection of video sessions [32] concluded that
311Figure 1: Entities involved in live video distri-
bution. Unlike Conviva’s C3 [19], which focuses
on clients, we focus on optimizing CDNs.
a centralized video controller algorithm could greatly
improve end-user experience. Even so, traditional CDNs
have shied away from centralized designs due to the
diﬃculty of providing good performance while quickly
handling failures in the wide area [29].
Unfortunately, past work on live video delivery does
not meet the practical and real-time requirements of
both CDNs and users. In summary, we need a live video
control plane that: 1) enables proactive control over
cost and quality at ﬁne-grained timescales, 2) scales
to today’s largest CDNs and their workloads, 3) achieves
real-time responsiveness to minimize join time and
respond to failures, and 4) meets these goals despite
wide-area network delays and failures.
In order to address these challenges, we propose a new
system, called video delivery network (VDN), that al-
lows CDN operators to dynamically control both stream
placement and restrict bitrates automatically, at a very
ﬁne timescale in a WAN environment. Traditionally,
clients adapt bitrates independently; VDN gives CDN
operators a say, as they have the best view of current
resources and delivery costs. At its core, VDN uses a
centralized algorithm that performs end-to-end optimiza-
tion of live stream routing. The centralized algorithm is
a piece of the larger VDN framework, which mitigates
WAN challenges with a hybrid approach that balances
the beneﬁts of centralized control with the resilience and
responsiveness of distributed control.
We evaluate VDN using traces of real video sessions
from multiple live content providers as well as a WAN
testbed. We show that, in a variety of scenarios such
as heavy-head (e.g., a sports game) and heavy-tail (e.g.,
user-generated streams), VDN provides a 1.7× improve-
ment in average bitrate and reduces delivery costs by
2× compared to current CDNs. We scale VDN to 10,000
videos and show it can react at a timescale of 200 ms.
In summary, our contributions are:
• A centralized algorithm based on integer program-
ming that coordinates delivery to provide high-quality
live video streaming at scale while giving control
“knobs” to operators to balance cost and quality.
• A responsive live video delivery framework that
minimizes join time and mitigates WAN challenges
using a hybrid centralized+distributed control plane.
Figure 2: CDN live content distribution [35].
2 Motivation
2.1 Setting
CDN background: We focus on optimizing CDNs
for HTTP-based live video delivery. Each entity on the
video delivery path (see Figure 1) can be independently
optimized (e.g., clients in Conviva’s C3 [19]), however
the focus of this work is CDN optimization.
Live video: Live video is particularly challenging due to
lack of caching and buﬀering within the delivery network.
In HTTP-based live streaming, a video is encoded at
multiple pre-determined bitrates. At each bitrate, each
stream is broken into multiple 2–10 second chunks, which
clients fetch independently via standard HTTP GETs.
Clients typically adapt to network issues by fetching
diﬀerent bitrates [6].
CDN structure: Figure 2 presents the high-level struc-
ture of a CDN’s video delivery system [29, 35, 40]. Each
node represents a cluster of co-located servers. A CDN’s
internal network consists of three logical pieces: video
sources that import videos into the system, reﬂectors
that forward content internally, and edge clusters that
directly serve end-users (individual clients or aggregate
ASes). Each link has a delivery cost associated with it.
These link costs are a result of private business deals,
but they tend to be more expensive for source/reﬂector
links (typically long-haul WAN links) and less expensive
(some entirely free) for edge/AS links [2]. In Figure 2,
the link between A and D is a high cost link.
CDNs and DNS: When clients resolve the name of a
video stream, the CDN’s DNS-based client mapping
service maps them to a nearby edge cluster, based on a
number of factors (e.g., load, latency, etc.) [35]. When a
client’s request for a particular channel arrives at an edge
cluster, the edge cluster forwards it to a reﬂector (found
via DNS), which in turn forwards it to a source (found
via DNS); the content is returned via the reverse path.
When multiple requests for the same content arrive at
the same node (e.g., C in the ﬁgure), only one request
is forwarded upwards. The end result is a distribution
tree for each video from sources to edge clusters.
This has been the design used by Akamai for live
streaming since 2004 [29], externally observed in 2008 [40]
and referenced by Akamai in 2010 [35]. We conﬁrm this
holds today [2].
CDN 2Video OriginClientCDN 1“Eyeball” ISPTransitISPSources(Reﬂectors(Edges(CDN(AS/Clients(Video(Requests(DNS(Stream(1(Stream(2(High(Cost(Low(Cost(A(B(C(D(312Problems with modern CDNs: Using DNS to map
requests to the appropriate upstream cluster is very nat-
ural as CDN workloads have shifted from web-oriented
to live streaming. Mapping clients to edge clusters with
DNS makes sense, since most live video content is em-
bedded in websites, which already use DNS. However,
using DNS to map internal clusters to upstream clusters
causes issues: 1) CDNs can’t “push” updates to clusters
and must instead wait for clusters to “pull” from DNS
after a timeout period (the DNS TTL); and 2) To reduce
load on DNS, CDNs group diﬀerent streams together,
reducing granularity [29, 40]. Furthermore, CDNs today
update DNS mappings using heuristics [2, 29, 35, 40],
impacting performance. We explore these issues in more
detail.
DNS TTLs: DNS relies on DNS clients (i.e., clusters) to
ask for updates when cached DNS results expire (every
∼30 seconds) [40], preventing a central controller from
sending updates as they are ready. This reduces the
eﬃcacy of the controller, thus lowering end-user qual-
ity and responsiveness to failures. Furthermore, CDN
clusters can spot local network failures long before a
TTL-bound DNS update could arrive and thus could re-
act quicker. Lowering the TTL would help approximate
a “push”-based system but at the cost of a large increase
in the number of DNS queries.
Heuristic-based mapping algorithm: A monitoring sys-
tem collects performance and load information and,
based on this knowledge, updates the DNS system every
minute [35]. Generally, CDNs map end-users to edge
clusters based on geography, load, whether or not a
cluster is already subscribed to the video, and perfor-
mance relative to the requester [2, 29, 35, 40].
It is
implied that the mapping of edge clusters to reﬂectors
is done similarly [35], but the speciﬁc algorithm is not
publicly known. A measurement study points out that
geographically close edge clusters all map to the same
reﬂector for the same groups of videos, providing further
evidence [40]. Additionally, an analysis of video traces
shows that mapping requests based on a global view of
the network [32] could provide major beneﬁts for end-
users, implying that there are opportunities to improve
this mapping.
Goal: VDN’s job is twofold: 1) coordinate the selection
of distribution trees for each channel and 2) assign groups
of clients within a given geographic region to a good
edge server. It must perform these tasks while meeting
the goals listed below.
2.2 Design goals
Video-speciﬁc quality and low cost (quality/cost
tradeoﬀ ): CDN operators must satisfy user’s expec-
tation for high video quality, while minimizing their
delivery cost. Thus, VDN must optimize for video qual-
ity directly, while considering its cost.
Internet-scale video delivery (scalability): Many
diﬀerent types of workloads exist for live video: (a)
Figure 3: Motivating central coordination.
“mega-events” (e.g., World Cup) serving 1M+ users [38],
(b) TV-style channels serving 100K users [3], and (c)
“long tail” user channels (e.g., Twitch, Ustream) serving
1-10,000 users [11]. (a) tends to be easier as one tree can
serve everyone, whereas workload (c) is the toughest, as
it requires coordinating across many videos. VDN must
support these workloads, out to a target scale of 10,000
channels [40] and 2000 edge clusters [17], beyond the
scale of today’s largest CDNs. Such scale is challenging
as ﬁnding the optimal placement is NP-hard.
Fine timescale (responsiveness): VDN must pro-
vide fast join time (less than a second) and fast failure
recovery, despite challenges in the wide area (e.g., incon-
sistent state, partitions, loops).
2.3 Case for centralized optimization
Despite the lack of public information on how the CDN
internal mapping is done, prior work has shown that a
control plane designed around centralized optimization
can provide great beneﬁt [32]. In this section we focus
on the reasons for these beneﬁts.
Coordination: Throughout this paper, we use coor-
dination to mean the ability to consider all individual
streams simultaneously. As mentioned, modern CDNs
have diﬃculty with this as they aggregate videos and get
“locked in” to decisions due to DNS TTLs [40]. Figure 3
illustrates why stream coordination can lead to better
resource allocation. Two video channels ((cid:86)1 and (cid:86)2)
originate from a single source, S. The goal is to deliver
(cid:86)1 to AS A and (cid:86)2 to B. Three possible distribution trees
exist: (cid:84)1, (cid:84)2 and (cid:84)3 (Figure 3a). We present two feasible
distribution strategies in Figure 3b and c. In Figure 3b