### Optimized Text

#### Labeling and Data Collection
A set of a thousand incidents from various sources was used to label data as "breached" by identifying the victim's IP block. The non-breached data was created by sampling from the remaining IP space, which was divided into 2.9 million sets [81, p. 1013]. The feature space includes security indicators (Sp) and exposure indicators (Es), such as mismanagement symptoms and the number of IP addresses. The authors argue that "the independence of the features from ground-truth data is maintained" [81, p. 1011], even though the number of blacklisted IPs on the network is used as a feature. This can be interpreted as using an indicator of compromise to predict another, but we interpret it as an indicator of increased threat level (T).

#### Similar Research Design
Another study uses a similar approach, labeling domains as "breached" by linking them to the victim's website domain [101]. Non-breached domains are sampled from the largest publicly available web directory. Both studies achieve comparable true and false positive rates: 90% and 10% [81] and 90% and 11% [101].

#### Case Control Methodology
Both studies employ an artificial case-control method, drawing labeled and unlabeled data from different populations. The breached cases are drawn from firms that have publicly reported breaches, mostly large corporations (see Section III). In contrast, the unbreached cases are drawn from a population of IP blocks or domain names not dominated by large corporations. The algorithms likely detect differences between large corporate networks and random web servers, rather than the likelihood of a breach in large corporations. Constructing a control population from a similar population to the breached firms is easier said than done. A statistical twins approach was used to construct a homogeneous sample of hosting providers [117], but this must be done without ground-truth on the relevant dimensions of similarity.

#### End-User Studies
Although our focus has been on organizational risk, research into individual devices and their users supports our narrative. For example, simple correlations reveal that end-users with 'computer expertise' [71] or those who use the Tor browser [34] are associated with an increased rate of compromise. Both papers raise the possibility of confounding variables. Bilge et al. [14] include exposure indicators in a model using random forests to predict device compromise and find that applying security patches is the third most important feature, after two exposure indicators.

#### Summary of Findings
Between-subject research designs with single security indicators often lead to spurious results, where more security is associated with more compromise [37, 122]. Adding control variables or using within-subject designs corrects this issue. The relative infrequency of compromise undermines statistical power, leading to null results even with 33 million observations of the security level [89]. Constructing latent factors for security provides more explanatory power than any single indicator in two studies [116, 133]. The success of applying random forests to predict incidents for organizations [81, 101] and machines [14] further supports our call to move away from explanations based on single indicators. Such models require additional reporting to understand how security interventions affect the probability of compromise. Regression models became popular in the social sciences because their effects are easily interpreted, even at the cost of predictive power.

#### Systematisation of Cyber Risk Research
Table III summarizes our systematisation. The first column visualizes the relationships explored in the corresponding study, showing that traditional security research has used relatively short sample windows. This contrasts with harm studies, which explore longitudinal trends using databases aggregated by third parties. The fourth column shows the diversity of venues for cyber risk research, with color coding according to discipline. Cyber harm has mainly been explored in finance (green) and interdisciplinary venues (orange). Top security conferences (red) have focused on quantifying threat and security without considering structural relationships, with a few recent exceptions. Since Straub’s seminal work in 1990 [113], research designs exploring multiple structural relationships have predominantly emerged in the last six years.

#### Discussion
**RQ1: How much harm results from cyber incidents?**
Data breaches in the US are the most studied incident due to the availability of aggregated public reports. Each study brings a new statistical approach, leading to contradictory claims about the same dataset. This contrasts with experimental science, where each study collects additional data, applies similar statistical tests, and builds knowledge via meta-analyses. As a result, we have learned little about data breaches despite a decade of analysis. We can at best agree that the number of records breached follows a heavy-tailed distribution, though this says little about financial cost [41]. Harm estimates vary across samples, reporting standards, and jurisdictions. The mean loss in a global operational loss sample extracted by text-mining [41] differs by an order of magnitude ($43m to $4.1m) when compared with a manually collected sample of public reports [98]. Estimates vary further across jurisdictions; only 0.1% of Italian firms suffered a loss greater than €200k in a 2016 survey [12]. This finding, from a stratiﬁed random sample collected by the Bank of Italy, raises the question of why so few independent statistical agencies employ their considerable expertise in collecting cybersecurity data. Perhaps cyber risk is simply not as harmful as thought [91]. Compared to media-reported breaches, typical breaches are smaller and less heavy-tailed [43]. Cyber losses are less than fraud, bad debt, or retail theft [98], and cyber operational losses are both less on average and less heavy-tailed than non-cyber losses [13]. The lack of empirical support for the claim that cyber risk is exceptionally harmful casts doubt on the attention-seeking assertions in security papers and talks. These studies and our causal model are inadequate to provide evidence about systemic risk (alternatives are discussed in Section V-D).

**RQ2: Which security interventions effectively reduce harm?**
Our contribution is a framework to evaluate answers to this question. Actionable answers are currently unavailable. Simple statistical tests lead to spurious results, such as greater security budgets [12, 105], greater computer expertise [71], or updated software [122] being associated with a higher frequency of compromise. Adding control variables can reverse these associations [122]. Using indicators of exposure alone can predict which websites will turn malicious [109] and explain most of the variance in abuse [117]. In contrast, indicators of security have little explanatory power alone. Liu et al. [81] retrained their model using each subset of the feature space alone and found that security mismanagement features "perform the worst" [81]. However, removing the subset of security indicators from the full model leads to the biggest decline in performance. This supports the fundamental intuition behind our causal model: security only explains harm outcomes when indicators of threat and exposure are added to the model. Prioritizing security interventions based on these studies is unwise. The best statistical models in terms of explanatory power measure security using multiple indicators [81, 113, 116]. Such approaches cannot isolate the effect of individual controls, let alone establish causality. Linking to policy, prescriptions in cybersecurity laws must be balanced against the lack of evidence on the effectiveness of specific prescriptions. A promising development is notification studies [78, 122], where security interventions can be randomly assigned outside a laboratory setting. Detected effects can reasonably be said to have been caused by the intervention. Adopting similar randomized control trial designs seems promising given their success in economics. With the power to randomly assign security interventions comes great ethical responsibility [88], especially for researchers contemplating interventions related to threat actors [75, p. 9].

**RQ3: Are these answers stable over time?**
Harm studies have longer sample windows, approaching 20 years in some cases, than mitigation studies (see Table III). Data breaches are not increasing in frequency in general [36, 128], but they are increasing in both size and frequency if the sample is restricted to malicious breaches [128, 131]. The price of cyber insurance trended downwards from 2008–2018 [130], although this is more due to market dynamics than decreasing risk. In terms of shareholder value, the effect of breach disclosure seems to be decreasing over time. The timing of this shift (2001 [53] and 2005 [51]) is curiously close to when mandatory data breach notification laws came into effect. One explanation could be that post-2003 samples contain more inconsequential breaches that would not have been discovered beforehand, drowning out the effect of large breaches, which have the biggest impact on stock prices [5]. Sample windows in mitigation studies are too brief to learn about the effectiveness of security interventions over time (see Table III). For example, cyber incident forecasting performance holds when moving from a "one-month to a 12-month forecasting window" [81, p. 1019], but the researchers can test no further. This is partly explained by disciplinary norms around self-collected data and the availability of data brokers. Funding agencies might consider how to support institutionalized data collection and sharing, as exemplified by the Cambridge Cybercrime Centre [27]. Balancing the admittedly limited evidence, there is little to suggest that cyber harms are particularly unstable. This is consistent with similar studies of cybercrime, where global aggregate losses were in the same order of magnitude between 2012 [7] and 2019 [8] despite criminals innovating in methods.

#### Limitations
The causal model has limitations and does not capture other valuable approaches to security research, such as qualitative methods that capture the subtleties of organizational security [10]. Within quantitative empirical research, limitations can be distilled into those of the model and more fundamental unknowability.

**Model Limitations**
The causal model is intended for observational studies of cyber risk in organizations and does not apply to research designs manipulating the security level, such as notification studies. Law enforcement interventions cannot be studied by the model in its current form and must be treated as exogenous shocks impacting the threat level. Our language often invokes linear relationships between variables, which does not reflect a naive belief that the world follows such models. Generalized linear models could be used to account for the non-linear distributions of harm identified in Section III. Many authors opted for machine learning (ML) models instead of regressions. Although we suggest prediction rates are less interpretable than regression tables, the important properties of the causal model (e.g., variables for threat and exposure, multiple indicators) are present in ML studies. Systemic cyber risk, however, requires a fundamentally different modeling approach because there are not enough observations for ML models or reduced-form regressions. Knowledge about the loss generation process could be used to create structured models that require less data. For example, correlations in attacks observed by Honeypots could parameterize correlations in risk models [16]. This topic is being considered by the finance community, who are examining how cyber risk poses a unique threat to financial stability [57, 126].

**Unknowability**
Creating knowledge about cyber harms and possible mitigation measures depends on available data. The size of a dataset is not everything; samples must also be representative of the broader population of interest. In terms of raw numbers, the surveyed studies analyzed: 5,000,000 webpages [109]; 200,000 webservers [122]; 45,000 hosting providers [117]; 15,000 end-user devices [34]; 600 victims of malicious data breaches [131]; and 265 victims of data breaches with financial cost [98]. Sub-components of complex systems like webpage compromise are easier to study than emergent effects like firm-wide losses. This issue is particularly pressing for systemic risk, for which there are no empirical results. Detailed case studies of the WannaCry and NotPetya incidents are an obvious starting point.

A second issue relates to social actors becoming aware of metrics. The signaling value of security certifications is eroded by market dynamics [6] and by selection effects [35]. Event window studies are undermined by strategically releasing positive news [51], withholding the most costly breaches [5], and by insider trading [80]. Such examples highlight Goodhart’s law, where security metrics are optimized at the cost of actual security. A related problem is researcher measurements distorting other measurements, such as when network scans for research purposes are interpreted as an attacker probing for vulnerabilities [55].

Finally, data is political. Inferred causal relationships may not generalize beyond the population of study, such as across cultures [103], and this can lead to flawed (possibly harmful) recommendations. Harm estimates inevitably ignore certain victims and types of harm [76], such as individuals lacking the resources to quantify and communicate their harm. The 'cost of a data breach' skews towards direct costs to the firm as determined by accountants and not indirect harms suffered by victims of identity theft.

#### Future Work
Throughout, we have argued that the causal model (Figure 3) is the best statistical approach to quantifying cyber risk. However, this risks the naive takeaway that 'investigating more causal links is always better,' which we do not endorse. Investigating the full causal model is an ambitious research design and often relies on prior work constructing measurement models for individual variables. Table IV is arguably more useful for funding agencies to distribute attention.

Our systematization can both classify existing studies and show which studies are yet to be conducted. Table IV shows no data breach study has linked C or H to an indicator of security. There are reasons for this. Collecting data from sufficiently many breached firms before it is known which will be breached requires large samples, otherwise the sparsity of observed compromise undermines statistical tests [89]. A solution is to obtain explanatory variables after compromise has been observed. For example, Soska et al. [109] use the Internet Archive to collect historical website content.

More generally, future work should aim to quantify the relative effectiveness of different forms of security. Recent work identifying a statistical relationship between security measures and the prevalence of compromise marks progress since a 2009 critical review [124], but only a minority of these results speak to prioritization. An example of the latter is evidence that hosting providers' security efforts "play a more significant role in fighting phishing abuse" [116, p. 13] than those of webmasters. However, the authors warn against causally interpreting the effect of individual indicators.

#### Conclusion
This paper systematizes empirical research into cyber harm estimates and the effectiveness of security interventions. Inspired by structural equation models, we introduced a model explaining security outcomes using latent factors for security, exposure, and threat. The moderating role of security would ideally be measured using many reflexive indicators without necessarily identifying causality. Our survey of empirical cyber harm estimates finds little evidence that either the typical size or variance of cyber harm is particularly exceptional, but these studies do not consider the role of risk mitigation. Applying the model to risk mitigation studies shows that threat level is often omitted. Indicators of exposure have good explanatory power in terms of cyber risk outcomes. Statistical tests that do not control for either factor lead to spurious results, such as increased security budgets leading to a greater frequency of breach [105] or that applying software updates increases the likelihood of web-server compromise [122]. Studies that account for all attributes show security is a powerful determinant of cyber harm outcomes; indicators of network misconfiguration are the most important features in classifying whether an organization will suffer a cyber incident [81].

Turning to the question of what risk science has to tell business leaders, firms should not underestimate the risk flowing from unnecessary exposure, given its predictive power regarding multiple forms of compromise. In terms of risk mitigation, vendors promising simple solutions (single indicator explanations) should be ignored, and security teams should be equipped with the resources to focus on the diversity of tasks that avert cyber harm. Policymakers' attention should be shifted away from typical losses, which are not exceptional, and towards systemic risk, which we simply know nothing about.