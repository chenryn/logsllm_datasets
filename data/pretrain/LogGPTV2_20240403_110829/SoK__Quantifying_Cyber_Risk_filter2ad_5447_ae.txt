were labelled as breached using a set of a thousand incidents
from various sources and then ﬁnding the victim’s IP block.
The data labelled as not breached is created by sampling from
the remaining IP space, which is broken “into 2.9 million
sets” [81, p. 1013]. The feature space includes indicators of se-
curity Sp and exposure Es like mismanagement symptoms and
the number of IP addresses. The authors argue “independence
of the features from ground-truth data is maintained” [81,
p. 1011] despite using the number of blacklisted IPs on the
network as a feature. Arguably this is using an indicator of
compromise to predict another indicator of compromise, but
we interpret this as an indicator of increased threat level T .
A similar research design takes a sample of incidents, links
these to a the victim’s website domain, and labels these
as breached domains [101]. The non-breached domains are
sampled from “the largest publicly available directory of the
Web” [101]. The studies achieve similar true and false positive
rates (90%/10% [81] and 90%/11% [101]).
Both studies use an artiﬁcial case control by drawing la-
belled and unlabelled data from different populations; the cases
labelled as breached are all drawn from the population of ﬁrms
who have publicly reported a breach, mostly large corporations
(see Section III). Whereas, the cases labelled as unbreached are
drawn from a population of IP blocks or domain names that is
not dominated by large corporations. The algorithms are likely
detecting the difference between a large corporate network
and a random web server, not the difference between large
corporations according to the likelihood of breach. The ﬁx,
constructing control population from a similar population to
the breached ﬁrms, is easier to state than to solve. A statistical
twins approach was used to construct a homogeneous sample
of hosting providers [117] but this must be done without
ground-truth on the relevant dimensions of similarity.
End-user studies Although we have focused on organisa-
tional risk in this paper, research into individual devices and
their users supports our narrative. For example, simple corre-
lations reveal that end-users with ‘computer expertise’ [71] or
that use the Tor browser [34] are associated with increased rate
of compromise. The authors of both papers raise the possibility
of confounding variables. Bilge et al. [14] include indicators
of exposure in a model using random forests to predict device
compromise and discover that applying security patches is the
third most important feature (after two indicators of exposure).
Summary Applying between-subject research designs with
single indicators of security lead to spurious results where
more security is associated with more compromise [37, 122].
Adding control variables or using within-subject designs
corrects the issue. The relative infrequency of compromise
undermines statistical power leading to null results even with
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:31:54 UTC from IEEE Xplore.  Restrictions apply. 
220
33m observations of the security level [89].
Constructing latent factors for security provided more ex-
planatory power than any single indicator in two studies [116,
133]. Although the learning representations are not explicitly
latent,
the success of applying random forests to predict
incidents for organisations [81, 101] and machines [14] further
supports our call to move away from explanations based on
single indicators. Such models requires additional reporting to
understand how security interventions affects the probability of
compromise. Regression models became popular in the social
sciences precisely because such effects are easily interpreted,
even at the cost of predictive power.
E. Systematisation of cyber risk research
Table III summarises our systematisation. The ﬁrst column
visualises the relationships explored in the corresponding
study, we see that the ﬁrst block of diagrams, predominantly
‘traditional’ security research, have used relatively short sam-
ple windows. This can be contrasted with the harm studies (the
second block) that explore longitudinal trends using databases
aggregated by third parties.
The fourth column shows the diversity of venues for cyber
risk research. Colour coding according to discipline shows
cyber harm has mainly been explored in the ﬁnance (green)
and interdisciplinary venues (orange). The top security confer-
ences (red) focused on quantifying threat and security without
considering structural relationships, putting aside a few recent
exceptions. With the exception of Straub’s seminal work in
1990 [113], research designs exploring multiple structural
relationships have predominantly emerged in the last 6 years.
V. DISCUSSION
We now return to each of our research questions.
A. RQ1: How much harm results from cyber incidents?
Data breaches in the US are the most studied incident
because aggregated public reports are ripe for statistical anal-
ysis. Each study brings a new statistical approach leading
to contradictory claims about the same dataset. This can be
contrasted with experimental science in which each study
collects additional data, applies similar statistical tests, and the
ﬁeld builds knowledge via meta-analyses. As a result, we have
learned little about data breaches despite 10 years of analysis.
We can at best agree that the number of records breached is
heavy tailed, though this says little about ﬁnancial cost [41].
Harm estimates are inconsistent across samples, reporting
standards, and jurisdiction. The mean loss in a sample of
global op losses extracted by text-mining [41] differs by an
order of magnitude ($43m to $4.1m) when compared with a
manually collected sample of public reports [98]. Estimates
vary further across jurisdictions, only 0.1% of Italian ﬁrms
suffered a loss greater than e200k in a 2016 survey [12]. This
ﬁnding resulted from a stratiﬁed random sample collected by
the Bank of Italy, which leads us to ask why so few indepen-
dent statistical agencies employ their considerable expertise in
collecting cybersecurity data?
Perhaps cyber risk is simply not that harmful [91]. Certainly
when compared to the breaches reported in the media, typical
breaches are smaller and less heavy tailed [43]. Cyber losses
are less than fraud, bad debt, or retail theft [98], and cyber
operational losses are both less on average and less heavy-
tailed than non-cyber losses [13]. The lack of empirical
support for the claim that cyber risk is exceptionally harmful
casts doubt over the attention seeking assertions that pervade
introductions to security papers and talks. These studies and
our causal model are inadequate to provide evidence about
systemic risk (alternatives are discussed in Section V-D).
B. RQ2: Which security interventions effectively reduce harm?
Our contribution is a framework to evaluate answers to
this question. Actionable answers are unavailable based on
current evidence. Simple statistical
tests lead to spurious
results like greater security budgets [12, 105], greater computer
expertise [71] or updated software [122] being associated
with greater frequency of compromise. The direction of such
associations can be reversed by adding control variables [122].
Turning to the explanatory power of each latent factor,
just using indicators of exposure can predict which websites
will turn malicious [109] and explain most of the variance
in abuse [117]. In contrast, indicators of security have little
explanatory power alone. Liu et al. [81] re-train their model
using each subset of the feature space alone and discover
security mismanagement features “perform the worst” [81].
Yet when removing each from the full model, removing the
subset of security indicators leads to the biggest decline in
performance. This supports the fundamental intuition behind
our causal model: security only explains harm outcomes when
indicators of threat and exposure are added to the model.
Prioritising security interventions based on these studies is
foolish. The best statistical models in terms of explanatory
power measure security using multiple indicators [81, 113,
116]. Such approaches cannot isolate the effect of individual
controls, let alone establish causality. Linking to policy, pre-
scriptions in cybersecurity laws must be balanced against the
lack of evidence on the effectiveness of speciﬁc prescriptions.
A promising development is notiﬁcation studies [78, 122] in
which security interventions can be randomly assigned outside
a laboratory setting. Detected effects can reasonably be said
to have been caused by the intervention. Adopting similar
randomised control trial designs seems promising given their
success in economics. With the power to randomly assign
security interventions comes great ethical responsibility [88],
which is compounded for researchers contemplating interven-
tions related to threat actors [75, p. 9].
C. RQ3: Are these answers stable over time?
Harm studies have longer sample windows, approaching 20
years in some cases, than mitigation studies (see Table III).
Data breaches are not increasing in frequency in general [36,
128] but they are increasing in both size and frequency if the
sample is restricted to malicious breaches [128, 131]. The price
of cyber insurance trended downwards from 2008–2018 [130],
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:31:54 UTC from IEEE Xplore.  Restrictions apply. 
221
although this has more to do with market dynamics than
decreasing risk. In terms of shareholder value, the effect of
breach disclosure seems to be decreasing over time. The timing
of this shift (2001 [53] and 2005 [51]) is curiously close to
when mandatory data breach notiﬁcation laws came into effect.
One explanation could be that post-2003 samples contain more
inconsequential breaches that would not have been discovered
beforehand and these drown out the effect of large breaches,
which are shown to have the biggest impact on stock prices [5].
Sample windows in mitigation studies are too brief to
learn about the effectiveness of security interventions over
time (see Table III). For example, cyber incident forecasting
performance holds when moving from a “one-month to a 12-
month forecasting window” [81, p. 1019] but the researchers
can test no further. This is partly explained by disciplinary
norms around self-collected data and the availability of data
brokers. Funding agencies might consider how to support
institutionalised data collection and sharing as exempliﬁed by
the Cambridge Cybercrime Centre [27].
Balancing the admittedly limited evidence, there is little
to suggest cyber harms are particularly unstable. This is
consistent with similar studies of cybercrime in which global
aggregate losses were in the same order of magnitude between
2012 [7] and 2019 [8] despite criminals innovating in methods.
D. Limitations
The causal model says little about other valuable approaches
to security research, such as qualitative methods, that capture
the subtleties of organisational security [10]. Within quantita-
tive empirical research, limitations can be distilled into those
of the model and more fundamental unknowability.
Model Limitations The causal model is intended for obser-
vational studies of cyber risk in organisations. This does not
apply to research designs manipulating the security level as in
notiﬁcation studies. Law enforcement interventions cannot be
studied by the model in its current form and must be treated
as exogenous shocks impacting the threat level.
Our language often invokes linear relationships between
variables, which does not reﬂect a naive belief that the world
follows such models. Generalised linear models could be used
to account for the non-linear distributions of harm identiﬁed
in Section III.
Many authors opted for machine learning (ML) models
instead of regressions. Although we suggest prediction rates
are less interpretable than regression tables,
the important
properties of the causal model (e.g. variables for threat and
exposure, multiple indicators) are present in ML studies.
Systemic cyber risk, however, requires a fundamentally
different modelling approach because there are not enough
observations for ML models or reduced form regressions.
Knowledge about the loss generation process could be used to
create structured models that require less data. For example,
correlations in the attacks observed by Honeypots could pa-
rameterise correlations in risk models [16]. This topic is being
considered by the ﬁnance community who consider how cyber
risk poses a unique threat to ﬁnancial stability [57, 126].
SYSTEMATISATION OF CYBER RISK QUANTIFICATION BY CATEGORY.
TABLE IV
AV effectiveness
Abuse study
Bitcoin ledger
Case study
Compliance
Cybercrime ecosystem
Data breach
End-user
IP backscatter
Insurance prices
Legal cases
Manually compiled
Market reaction
Notiﬁcation
Operational loss
Organisation incident
Survey of ﬁrms
Threat index
Unknowability Creating knowledge about cyber harms and
possible mitigation measures depends on available data. The
size of a data-set is not everything as samples must also be
representative of the broader population of interest. In terms
of raw numbers,
the surveyed studies analysed: 5 000 000
webpages [109]; 200 000 webservers [122]; 45 000 hosting
providers [117]; 15 000 end-user devices [34]; 600 victims
of malicious data breaches [131]; and 265 victims of data
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:31:54 UTC from IEEE Xplore.  Restrictions apply. 
222
breaches with ﬁnancial cost [98]. Sub-components of com-
plex systems like web page compromise are easier to study
than emergent effects like ﬁrm-wide losses. This issue is
particularly pressing for systemic risk, for which there are no
empirical results. Detailed case-studies of the WannaCry and
NotPetya incidents are an obvious starting point.
A second issue relates to social actors becoming aware
of metrics. The signalling value of security certiﬁcations
is eroded by market dynamics [6] and by selection ef-
fects [35]. Event window studies are undermined by strate-
gically releasing positive news [51], withholding the most
costly breaches [5], and by insider trading [80]. Such exam-
ples highlight Goodhart’s law in which security metrics are
optimised at the cost of actual security. A related problem is
researcher measurements distorting other measurements, such
as when network scans for research purposes are interpreted
as an attacker probing for vulnerabilities [55].
Finally, data is political. Inferred causal relationships may
not generalise beyond the population of study, such as across
cultures [103], and this can lead to ﬂawed (possibly harmful)
recommendations. Harm estimates inevitably ignore certain
victims and types of harm [76], such as individuals lacking
the resources to quantify and communicate their harm. The
‘cost of a data breach’ skews towards direct costs to the ﬁrm
as determined by accountants and not indirect harms suffered
by victims of identity theft.
E. Future work
Throughout we have argued that the causal model (Figure 3)
is the best statistical approach to quantifying cyber risk.
However,
this risks the naive takeaway that ‘investigating
more causal links is always better’, which we do not endorse.
Investigating the full causal model is an ambitious research
design and often relies on prior work constructing measure-
ment models for individual variables. Table IV is arguably
more useful for funding agencies to distribute attention.
Our systemisation can both classify existing studies and
show which studies are yet to be conducted. Table IV shows
no data breach study has linked C or H to an indicator
of security. There are reasons for this. Collecting data from
sufﬁciently many breached ﬁrms before it is known which
will be breached requires large samples, otherwise the sparsity
of observed compromise undermines statistical tests [89]. A
solution is to obtain explanatory variables after compromise
has been observed. For example, Soska et al. [109] use the
Internet Archive to collect historical website content.
More generally, future work should aim to quantify the
relative effectiveness of different forms of security. Recent
work identifying a statistical relationship between security
measures and the prevalence of compromise marks progress
since a 2009 critical review [124], but only a minority of
these results speak to prioritisation. An example of the latter
is evidence that hosting providers’ security efforts “play a
more signiﬁcant role in ﬁghting phishing abuse” [116, p. 13]
than those of web masters. However, the authors warn against
causally interpreting the effect of individual indicators.
VI. CONCLUSION
This paper systematises empirical research into cyber harm
estimates and the effectiveness of security interventions. In-
spired by structural equation models, we introduced a model
explaining security outcomes using latent factors for security,
exposure, and threat. The moderating role of security would
ideally be measured using many reﬂexive indicators without
necessarily identifying causality. Our survey of empirical cyber
harm estimates ﬁnds little evidence that either the typical size
or variance of cyber harm is particularly exceptional, but these
studies do not consider the role of risk mitigation.
Applying the model to risk mitigation studies shows that
threat level is often omitted. Indicators of exposure have good
explanatory power in terms of cyber risk outcomes. Statistical
tests that do not control for either factor lead to spurious results
like increased security budgets leading to greater frequency
of breach [105] or that applying software updates increases
the likelihood of web-server compromise [122]. Studies that
account for all attributes show security is a powerful de-
terminant of cyber harm outcomes;
indicators of network
misconﬁguration are the most important features in classifying
whether an organisation will suffer a cyber incident [81].
Turning to the question of what risk science has to tell
business leaders, ﬁrms should not underestimate the risk
ﬂowing from unnecessary exposure given its predictive power
regarding multiple forms of compromise. In terms of risk mit-
igation, vendors promising simple solutions (single indicator
explanations) should be ignored and security teams should
be equipped with the resources to focus on the diversity of
tasks that avert cyber harm. Policy makers’ attention should
be shifted away from typical losses, which are not exceptional,
and towards systemic risk that we simply know nothing about.
REFERENCES
[1] A. Acquisti, A. Friedman, and R. Telang. Is there a cost
to privacy breaches? An event study. In Proc. of the Int.
Conf. on Information Systems, pages 94–117, 2006.
[2] I. Agraﬁotis, J. R. Nurse, M. Goldsmith, S. Creese, and
D. Upton. A taxonomy of cyber-harms: Deﬁning the
impacts of cyber-attacks and understanding how they
propagate. Journal of Cybersecurity, 4(1), 2018.
[3] L. Allodi and F. Massacci. Comparing vulnerability
severity and exploits using case-control studies. ACM
Trans. on Inf. and System Security, 17(1):1, 2014.
[4] L. Allodi, M. Corradin, and F. Massacci. Then and
now: On the maturity of the cybercrime markets the
lesson that black-hat marketeers learned. IEEE Trans.
on Emerging Topics in Computing, 4(1):35–46, 2015.
[5] E. Amir, S. Levi, and T. Livne. Do ﬁrms underreport
information on cyber-attacks? Evidence from capital