# Create a new control group    mkdir -p /sys/fs/cgroup/unified/iotest    
# enable the io and memory controller subsystem    echo "+io +memory" > /sys/fs/cgroup/unified/cgroup.subtree_control    
# Add current bash pid in iotest control group.    
# Then all child processes of the bash will be in iotest group too,    
# including the fio    echo $$ >/sys/fs/cgroup/unified/iotest/cgroup.procs    
# 256:16 are device major and minor ids, /mnt is on the device.    echo "252:16 wbps=10485760" > /sys/fs/cgroup/unified/iotest/io.max    cd /mnt    #Run the fio in non direct I/O mode    fio -iodepth=1 -rw=write -ioengine=libaio -bs=4k -size=1G -numjobs=1  -name=./fio.test在这个例子里，我们建立了一个名叫 iotest的控制组，并且在这个控制组里加入了 io 和 Memory两个控制子系统，对磁盘最大吞吐量的设置为 10MB。运行 fio的时候不加\"-direct=1\"，也就是让 fio 运行在 buffered I/O模式下。 运行 fio 写入 1GB 的数据后，你会发现 fio马上就执行完了，因为系统上有足够的内存，fio把数据写入内存就返回了，不过只要你再运行"iostat -xz 10"这个命令，你就可以看到磁盘 vdb 上稳定的写入速率是10240wkB/s，也就是我们在 io Cgroup 里限制的10MB/s。 ![](Images/330d7c4fe7224c81b635a7b8139aaa0b.png)savepage-src="https://static001.geekbang.org/resource/image/81/c6/8174902114e01369193945891d054cc6.png"}看到这个结果，我们证实了 Cgoupv2 io+Memory两个子系统一起使用，就可以对 buffered I/O控制磁盘写入速率。重点总结这一讲，我们主要想解决的问题是如何保证容器读写磁盘速率的稳定，特别是当多个容器同时读写同一个磁盘的时候，需要减少相互的干扰。Cgroup V1 的 blkiio 控制子系统，可以用来限制容器中进程的读写的 IOPS和吞吐量（Throughput），但是它只能对于 Direct I/O的读写文件做磁盘限速，对 Buffered I/O的文件读写，它无法进行磁盘限速。**这是因为 Buffered I/O 会把数据先写入到内存 Page Cache中，然后由内核线程把数据写入磁盘，而 Cgroup v1 blkio 的子系统独立于memory 子系统，无法统计到由 Page Cache刷入到磁盘的数据量。**这个 Buffered I/O 无法被限速的问题，在 Cgroup v2 里被解决了。Cgroupv2从架构上允许一个控制组里有多个子系统协同运行，这样在一个控制组里只要同时有io 和 Memory 子系统，就可以对 Buffered I/O作磁盘读写的限速。虽然 Cgroup v2 解决了 Buffered I/O磁盘读写限速的问题，但是在现实的容器平台上也不是能够立刻使用的，还需要等待一段时间。目前从runC、containerd 到 Kubernetes 都是刚刚开始支持 Cgroupv2，而对生产环境中原有运行 Cgroup v1 的节点要迁移转化成 Cgroup v2需要一个过程。思考题最后呢，我给你留一道思考题。其实这是一道操作题，通过这个操作你可以再理解一下 blkio Cgroup 与Buffered I/O 的关系。在 Cgroup v1 的环境里，我们在 blkio Cgroup v1 的例子基础上，把 fio中\"direct=1\"参数去除之后，再运行 fio，同时运行 iostat查看实际写入磁盘的速率，确认 Cgroup v1 blkio 无法对 Buffered I/O限速。 欢迎你在留言区分享你的收获和疑问。如果这篇文章给你带来了启发，也欢迎转发给你的朋友，一起学习和交流。
# 14 \| 容器中的内存与I/O：容器写文件的延时为什么波动很大？你好，我是程远。这一讲，我们继续聊一聊容器中写文件性能波动的问题。你应该还记得，我们上一讲中讲过 Linux 中的两种 I/O模式，Direct I/O 和 BufferedI/O。 对于 Linux 的系统调用 write() 来说，Buffered I/O是缺省模式，使用起来比较方便，而且从用户角度看，在大多数的应用场景下，用Buffered I/O 的 write() 函数调用返回要快一些。所以，Buffered I/O在程序中使用得更普遍一些。当使用 Buffered I/O的应用程序从虚拟机迁移到容器，这时我们就会发现多了 Memory Cgroup的限制之后，write()写相同大小的数据块花费的时间，延时波动会比较大。这是怎么回事呢？接下来我们就带着问题开始今天的学习。问题再现我们可以先动手写一个小程序slate-object="inline"，用来模拟刚刚说的现象。这个小程序我们这样来设计：从一个文件中每次读取一个 64KB大小的数据块，然后写到一个新文件中，它可以不断读写 10GB大小的数据。同时我们在这个小程序中做个记录，记录写每个 64KB的数据块需要花费的时间。我们可以先在虚拟机里直接运行，虚拟机里内存大小是大于 10GB的。接着，我们把这个程序放到容器中运行，因为这个程序本身并不需要很多的内存，我们给它做了一个Memory Cgroup 的内存限制，设置为1GB。 运行结束后，我们比较一下程序写数据块的时间。我把结果画了一张图，图里的纵轴是时间，单位us；横轴是次数，在这里我们记录了 96次。图中橘红色的线是在容器里运行的结果，蓝色的线是在虚拟机上运行的结果。结果很明显，在容器中写入数据块的时间会时不时地增高到200us；而在虚拟机里的写入数据块时间就比较平稳，一直在 30～50us这个范围内。![](Images/dc4bc8ed6423f45da1f32e0d257684ef.png)savepage-src="https://static001.geekbang.org/resource/image/7c/c0/7c494f4bc587b618f4b7db3db9ce4ac0.jpg"}通过这个小程序，我们再现了问题，那我们就来分析一下，为什么会产生这样的结果。时间波动是因为 Dirty Pages 的影响么？我们对文件的写入操作是 BufferedI/O。在前一讲中，我们其实已经知道了，对于 BufferI/O，用户的数据是先写入到 Page Cache里的。而这些写入了数据的内存页面，在它们没有被写入到磁盘文件之前，就被叫作dirty pages。Linux 内核会有专门的内核线程（每个磁盘设备对应的 kworker/flush线程）把 dirty pages 写入到磁盘中。那我们自然会这样猜测，也许是 Linux内核对 dirty pages 的操作影响了 Buffered I/O的写操作？ 想要验证这个想法，我们需要先来看看 dirty pages是在什么时候被写入到磁盘的。这里就要用到 **/proc/sys/vm里和 dirty page相关的内核参数**了，我们需要知道所有相关参数的含义，才能判断出最后真正导致问题发生的原因。现在我们挨个来看一下。为了方便后面的讲述，我们可以设定一个比值A， **A 等于 dirtypages 的内存 / 节点可用内存\*100%**。第一个参数，dirty_background_ratio，这个参数里的数值是一个百分比值，缺省是10%。如果比值 A 大于 dirty_background_ratio 的话，比如大于默认的10%，内核 flush 线程就会把 dirty pages刷到磁盘里。第二个参数，是和 dirty_background_ratio 相对应一个参数，也就是dirty_background_bytes，它和 dirty_background_ratio 作用相同。区别只是dirty_background_bytes 是具体的字节数，它用来定义的是 dirty pages内存的临界值，而不是比例值。这里你还要注意，dirty_background_ratio 和 dirty_background_bytes只有一个可以起作用，如果你给其中一个赋值之后，另外一个参数就归 0了。 接下来我们看第三个参数，dirty_ratio，这个参数的数值也是一个百分比值，缺省是20%。 如果比值 A，大于参数 dirty_ratio 的值，比如大于默认设置的20%，这时候正在执行 Buffered I/O写文件的进程就会被阻塞住，直到它写的数据页面都写到磁盘为止。同样，第四个参数 dirty_bytes 与 dirty_ratio 相对应，它们的关系和dirty_background_ratio 与 dirty_background_bytes一样。我们给其中一个赋值后，另一个就会归零。然后我们来看dirty_writeback_centisecs，这个参数的值是个时间值，以百分之一秒为单位，缺省值是500，也就是 5 秒钟。它表示每 5 秒钟会唤醒内核的 flush 线程来处理 dirtypages。 最后还有dirty_expire_centisecs，这个参数的值也是一个时间值，以百分之一秒为单位，缺省值是3000，也就是 30 秒钟。它定义了 dirty page在内存中存放的最长时间，如果一个 dirty page超过这里定义的时间，那么内核的 flush线程也会把这个页面写入磁盘。好了，从这些 dirty pages相关的参数定义，你会想到些什么呢？进程写操作上的时间波动，只有可能是因为 dirty pages的数量很多，已经达到了第三个参数 dirty_ratio的值。这时执行写文件功能的进程就会被暂停，直到写文件的操作将数据页面写入磁盘，写文件的进程才能继续运行，所以进程里一次写文件数据块的操作时间会增加。刚刚说的是我们的推理，那情况真的会是这样吗？其实我们可以在容器中进程不断写入数据的时候，查看节点上dirty pages的实时数目。具体操作如下：    watch -n 1 "cat /proc/vmstat | grep dirty"当我们的节点可用内存是 12GB 的时候，假设 dirty_ratio 是20%，dirty_background_ratio 是 10%，那么我们在 1GB memory 容器中写 10GB的数据，就会看到它实时的 dirty pages 数目，也就是 / proc/vmstat 里的nr_dirty 的数值，这个数值对应的内存并不能达到 dirty_ratio所占的内存值。![](Images/ffa122a6dc84d8093e2f84a08bdfdddb.png)savepage-src="https://static001.geekbang.org/resource/image/cc/68/ccd0b41e3bd9420c539942b84d88f968.png"}其实我们还可以再做个实验，就是在 dirty_bytes 和dirty_background_bytes里写入一个很小的值。    echo 8192 > /proc/sys/vm/dirty_bytes    echo 4096 > /proc/sys/vm/dirty_background_bytes然后再记录一下容器程序里每写入 64KB数据块的时间，这时候，我们就会看到，时不时一次写入的时间就会达到9ms，这已经远远高于我们之前看到的 200us了。 因此，我们知道了这个时间的波动，并不是强制把 dirty page写入到磁盘引起的。调试问题那接下来，我们还能怎么分析这个问题呢？我们可以用 perf 和 ftrace 这两个工具，对容器里写数据块的进程做个profile，看看到底是调用哪个函数花费了比较长的时间。顺便说一下，我们在专题加餐里会专门介绍如何使用perf、ftrace等工具以及它们的工作原理，在这里你只要了解我们的调试思路就行。怎么使用这两个工具去定位耗时高的函数呢？我大致思路是这样的：我们发现容器中的进程用到了write() 这个函数调用，然后写 64KB 数据块的时间增加了，而 write()是一个系统调用，那我们需要进行下面这两步操作。**第一步，我们要找到内核中 write()这个系统调用函数下，又调用了哪些子函数。**想找出主要的子函数我们可以查看代码，也可以用 perf这个工具来得到。然后是 **第二步，得到了write() 的主要子函数之后，我们可以用 ftrace 这个工具来 trace这些函数的执行时间，这样就可以找到花费时间最长的函数了。**好，下面我们就按照刚才梳理的思路来做一下。首先是第一步，我们在容器启动写磁盘的进程后，在宿主机上得到这个进程的pid，然后运行下面的 perf 命令。    perf record -a -g -p 等写磁盘的进程退出之后，这个 perf record也就停止了。这时我们再执行 `perf report` 查看结果。把 vfs_write()函数展开之后，我们就可以看到，write()这个系统调用下面的调用到了哪些主要的子函数，到这里第一步就完成了。![](Images/4a756f36dd2c4a4ee4829e5d66835a8e.png)savepage-src="https://static001.geekbang.org/resource/image/91/9d/9191caa5db8c0afe2363540bc31e1d9d.png"}下面再来做第二步，我们把主要的函数写入到 ftrace 的 set_ftrace_filter里, 然后把 ftrace 的 tracer 设置为 function_graph，并且打开 tracing_on开启追踪。     
# cd /sys/kernel/debug/tracing    
# echo vfs_write >> set_ftrace_filter    
# echo xfs_file_write_iter >> set_ftrace_filter    
# echo xfs_file_buffered_aio_write >> set_ftrace_filter    
# echo iomap_file_buffered_write    