### Optimized Text

To conduct this experiment, we would need to label a sufficient number of phishing screenshots for each of the 181 brands and train 181 classifiers. Given the high cost associated with such extensive labeling, we instead manually labeled the phishing and benign screenshots for the top five brands to train LogoSENSE. To ensure a fair comparison, we ran LogoSENSE to detect and identify phishing webpages targeting only these five brands. Notably, the number of phishing pages for these five brands is 15,658, while the number of benign webpages used to evaluate false positive rates is 29,951. Since the code for the three approaches (EMD, Phishzoo, and LogoSENSE) is not open-sourced, we implemented them for our evaluations (refer to Section 8 for further details).

#### Table 1: Baselines for Phishing Identification
| **Baseline** | **EMD** | **Phishzoo** | **LogoSENSE** |
|--------------|---------|--------------|---------------|
| **Matching Criteria** | Screenshot similarity | Logo similarity | Logo similarity |
| **Details** | Uses EMD measurement to compare screenshot similarity. | Detects and matches logos in a screenshot using the SIFT approach. | Detects and matches logos in a screenshot by training a HOG vector-based classifier for each target brand. |

In this experiment, we set the similarity threshold for EMDnormal to 0.92 and for EMDmore_ref to 0.96. The thresholds for Phishzoo and Phishpedia were set to 0.4 and 0.83, respectively, after experimenting with multiple thresholds. These values are the optimal thresholds for each model. For more details on the thresholds, refer to [11] and [21].

#### Table 2: Best Performance of Phishpedia and Baselines
| **Tool** | **EMDnormal** | **EMDmore_ref** | **Phishzoo** | **LogoSENSE** | **Phishpedia** |
|----------|---------------|-----------------|--------------|---------------|----------------|
| **Identification Rate** | 27.7% | 96.7% | 28.5% | 37.8% | 99.2% |
| **Detection Rate** | 76.2% | 81.8% | 87.1% | 89.0% | 98.2% |
| **Precision** | 74.4% | 26.9% | 52.0% | 68.9% | 20.5% |
| **Recall** | - | - | - | - | - |
| **Model Prediction Time (s)** | 0.19 | 15.6 | 18.2 | 27.2 | 0.19 |

(a) Home page
(b) Missed phishing page (similarity of 0.921). Due to changes in layout, EMD does not report this as phishing.
(c) False phishing (similarity of 0.947). This is caused by over-abstracting the pixel colors (see Section 3 in [21]).

**Figure 11: Qualitative Analysis of EMD**
- EMD matches webpage screenshots based on the most frequent color pixels and their positions, leading to false positives and false negatives.

### Results (RQ1): Phishing Identification Accuracy
Table 2 compares Phishpedia and the baseline approaches in terms of phishing identification rate, detection rate, and runtime overhead. The number of total phishing webpages (Nump), reported phishing webpages (Repp), true positive reported phishing webpages (Repp_TP), and correctly identified phishing webpages with the correct brand (Idp) are used to calculate the metrics. The identification rate is calculated as Repp / Nump, and the recall is Repp_TP / Nump.

Phishpedia outperforms the baseline approaches in identification rate, detection rate, and runtime overhead. EMDnormal has similar runtime efficiency to Phishpedia but with worse accuracy. EMDmore_ref achieves better precision and recall but at a much higher and impractical runtimeâ€”on average, it takes 15.6 seconds to process a given webpage. PhishZoo also requires significant computational time, while LogoSENSE has low detection and identification rates.

**Figure 10: ROC Curves (with FPR in log scale) for the Four Phishing Identification Solutions**

### Qualitative Analysis of Baselines
- **EMD**: Suffers from extracting coarse features (e.g., pixel colors) from webpage screenshots. Figure 11 shows a missed phishing page (false negative) and a mistakenly reported benign page (false positive).
- **PhishZoo**: Disadvantaged due to the technical limitations of SIFT. SIFT matches logos by extracting feature points, but incomplete feature points and mismatches can lead to false positives and false negatives (see Figure 12).
- **LogoSENSE**: Incurs both high false positives and false negatives. It uses a sliding window to transform content into HOG vectors, which are fed to trained SVM models. Fixed sliding windows often cover partial logos, and the method struggles with more complex or unseen screenshots (see Figure 13).

### Qualitative Analysis of Phishpedia
- **False Positives**: Phishpedia makes false positive predictions when a benign webpage has a logo similar to a well-known legitimate brand logo (see Figure 14). A remedy could be to impose stronger restrictions on image similarity through aspect ratio and detailed layout.
- **False Negatives**: Phishpedia misses phishing webpages targeting brands not in the protected list. This issue can be mitigated by enhancing the target list. Section 5.4 evaluates Phishpedia's performance when new brand logos are added to the target list.

### Analyses of Individual Components (RQ2)
#### 5.3.1 Evaluating Logo Detection
- We use 29K samples for training and 1,600 for testing. The Average Precision (AP) for each class (logo and input box) is computed for IoU thresholds ranging from 0.5 to 0.95. The results, presented in Table 3, show acceptable performance.

**Table 3: Object Detection Accuracy (Average Precision)**
| **Object Class** | **Training AP** | **Testing AP** |
|------------------|-----------------|----------------|
| **Logo**         | 52.7            | 49.3           |
| **Input Boxes**  | 73.5            | 70.0           |
| **Overall (mAP)**| 63.1            | 59.7           |

**Figure 15: Accuracy of Siamese Model (Precision-Recall Curve)**

#### 5.3.2 Evaluating Logo Recognition
- We manually labeled 1,000 phishing and 1,000 benign webpage screenshots. The Siamese model was evaluated using 2,000 identity logos. We experimented with Resnet50 and ResnetV2-50 as backbone networks and RGB and grayscale logo inputs. The precision-recall curves (Figure 15) show that all configurations perform comparably, with Resnetv2 and RGB input achieving the best performance.

### Phishpedia Generalization (RQ3)
- We evaluated the generalizability of the Siamese model by training it on 130 brand logos and testing it on 51 new logos. Phishpedia recognized 87.46% of the phishing webpages with a high identification rate of 99.91%, indicating good generalizability.

### Alternative Options (RQ4)
- **Op1**: Evaluated Yolov3 for logo recognition.
- **Op2**: Compared one-stage and two-stage transfer learning for the Siamese model.
- **Op3**: Trained the Siamese model with conventional methods (e.g., Triplet loss function).
- **Op4**: Explored perceptual hashing (PH) as a simpler alternative to the Siamese model.

### Setup
- For Op1, we selected Yolov3, a popular one-stage object detection model, and trained it on the same cluster as our Faster-RCNN model. For Op2 and Op3, we compared different training methods for the Siamese model.