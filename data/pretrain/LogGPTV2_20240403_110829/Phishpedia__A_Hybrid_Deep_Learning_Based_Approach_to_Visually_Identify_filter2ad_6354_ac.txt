which means that we need to label enough phishing screen-
shots for each of those 181 brands and train 181 classiﬁers
for this experiment. Given this high cost of experimentation,
we instead manually labelled the phishing and benign screen-
shots of top-5 brands to train LogoSENSE. Yet, to have a fair
comparison, we run LogoSENSE to detect and identify phish-
ing webpages only targeting for these ﬁve brands; note that
the corresponding number of phishing pages is a high count
of 15,658; while we still maintain 29,951 benign webpages to
evaluate its false positive rates. Since the code for these three
approaches are not open sourced, we implemented them for
our evaluations (refer Section 8 for further details).
Table 1: Baselines for phishing identiﬁcation
Baseline
EMD
Matching Criteria
screenshot similarity
Phishzoo
logo similarity
LogoSENSE
logo similarity
Details
Use EMD measurement to compare
screenshot similarity.
Detect and match logo in a screenshot
using SIFT approach.
Detect and match logo in a screenshot
by training a HOG vector based classi-
ﬁer from every target brand.
In this experiment, we let the similarity threshold of
EMDnormal to be 0.92, that of EMD more_ref to be 0.96, that of
USENIX Association
30th USENIX Security Symposium    3799
Table 2: Best performance of Phishpedia and baselines.
Tool
EMDnormal
EMDmore_ref
Phishzoo
LogoSENSE
Phishpedia
Identiﬁcation
Rate
27.7%
96.7%
28.5%
37.8%
99.2%
Detection Rate
Precision Recall
76.2%
74.4%
81.8%
26.9%
87.1%
52.0%
89.0%
68.9%
20.5%
98.2%
Model
Prediction Time (s)
0.19
15.6
18.2
27.2
0.19
(a) Home page
(b) Missed phishing page
(similarity of 0.921). Due to
change of layout, EMD does
not report this as phishing.
(c) False phishing (similarity
of 0.947). It is caused by over
abstracting the pixel colors
(see Section 3 in [21]).
Figure 11: Qualitative analysis of EMD: EMD matches web-
page screenshot based on most frequent color pixels and their
positions, causing false positives and false negatives.
identiﬁcation and detection accuracy. In contrast, EMDmore_ref
achieves a much better performance in terms of precision and
recall, but at a much higher and impractical runtime — on
average, it takes 15.6 seconds to process a given webpage.
PhishZoo also takes high computational time to decide on a
webpage, while LogoSENSE has low detection and identiﬁca-
tion rates. Furthermore, we plot the ROC (Receiver Operating
Characteristic) curves for all the identiﬁcation approaches
in Figure 10. As the FPR decreases, we observe a widen-
ing gap between Phishpedia and the baseline approaches ex-
cept for EMDmore_ref. Besides Phishpedia, EMDmore_ref is the
only other approach to achieve meaningful recall (TPR) at
lower FPRs (albeit this comes with a high computational cost).
Yet, if we consider low FPR values of 10(cid:0)2;10(cid:0)3 and 10(cid:0)4,
which are required for operational deployment, we observe
that Phishpedia still achieves higher recall than EMDmore_ref.
Qualitative analysis of baselines. EMD suffers from ex-
tracting coarse features (e.g., pixel colors) from webpage
screenshots. Figure 11 shows a phishing page EMD missed
to report (false negative) and one that it mistakenly reported
(false positive).
0
0
PhishZoo is disadvantaged due to the technical limitations
of SIFT. SIFT matches logo by extracting, say, k feature points
from the logo and k
feature points from a screenshot. As long
0 (cid:20) k and
as k
0
k
k is larger than a threshold, SIFT reports that the logo appears
on the screenshot. We observe that its limitations largely lies
in extracting incomplete feature points and the mismatches
out of k feature points are matched, such that k
Figure 10: ROC curves (with FPR in log scale) for the four
phishing identiﬁcation solutions.
PhishZoo to be 0.4, and that of Phishpedia to be 0.83. These
values are the optimal thresholds after we experimented multi-
ple thresholds for each model. Readers may refer to [11], [21]
for the details on their respective thresholds.
5.2.3 Results (RQ1): Phishing identiﬁcation accuracy
. Precision is computed as Repp
In Table 2, we compare Phishpedia and the baseline ap-
proaches on their phishing identiﬁcation rate (Identiﬁcation
Rate), the support for phishing detection (Detection Rate), and
the runtime overhead. We calculate each column as follows.
Let the number of total phishing webpages be Nump, the num-
ber of reported phishing webpages be Repp, the number of
reported true phishing webpages be Repp
TP, and the number
of reported true phishing webpages with brand reported cor-
rectly be Idp. The column ‘Identiﬁcation rate’ is calculated as
Repp , and Recall as Repp
Idp
Nump .
Repp
TP
Table 2 presents the best results of the approaches (balanc-
ing between identiﬁcation rate, precision, and recall). Note,
all the approaches take as input a URL, thus they all share the
same process and cost of transforming a URL to its screen-
shot, which takes approximately 1.88s on average. Also, the
techniques to optimize network communications and capture
screenshots are out of the scope of this work. Observe that
Phishpedia outperforms the baseline approaches in identiﬁca-
tion rate, detection rate, and runtime overhead. EMDnormal has
a similar runtime efﬁciency as Phishpedia, but it has worse
TP
TP
3800    30th USENIX Security Symposium
USENIX Association
(a) Logo
(b) Recognized phishing
(a) Logo
(b) Recognized phishing
(c) Missed phishing page (simi-
larity score 0.27)
(d) False phishing (similarity
score 0.48)
Figure 12: Qualitative analysis of PhishZoo (threshold 0.3).
Compared to the correctly matched logo regions (see green
circle), SIFT matches the ABSA logo to many irrelevant re-
gions (see red circles).
the extracted feature points have, as shown in Figure 12.
LogoSENSE incurs both high false positives and false nega-
tives. LogoSENSE uses a sliding window of logo size through
the screenshot. The content of the sliding window will be
transformed into a HOG vector, to be fed to a set of trained
SVM models, each of which represents a brand logo. The
output is the brand logo that has highest similarity with this
HOG vector. In our experiments, we use the sliding window
through the screenshot with three different scales as in [13].
We observe that the ﬁxed sliding window usually covers a par-
tial logo (see Figure 13c), which challenges the corresponding
SVM model to predict well. Besides, LogoSENSE is hard to
be generalized to more complicated (or unseen) screenshots,
and therefore often reports a button as logo (as showed in
Figure 13d). We also observe that a large number of sliding
windows on a screenshot incurs much runtime overhead.
Qualitative analysis of Phishpedia. Phishpedia, with pre-
cise identity logo recognition and logo image comparison,
can overcome the challenges faced in phishing identiﬁcation.
Yet, in this section, we investigate speciﬁc important cases of
false predictions made by Phishpedia.
False positive. Phishpedia makes false positive predictions
when a benign webpage has a logo looking like a well-known
legitimate brand logo. As shown in Figure 14, the logo of
the benign website looks similar to a logo variant the brand
Navy Federal (see Figure 14b). Such a pair of similar logo
confuses the Siamese model in Phishpedia. A remedy can be
that we force stronger restriction on image similarity through
aspect ratio and more detailed layout. We plan to explore this
problem in our future work.
(c) Missed phishing
(d) False phishing
Figure 13: Qualitative analysis of LogoSENSE.
(a) Detected logo (b) Matched logo
(c) Screenshot of benign website https://webkassa.kz
Figure 14: False phishing page reported by Phishpedia
False negatives. Unsurprisingly, Phishpedia misses the phish-
ing webpages targeting a brand beyond the protected target
brand list. This is a common problem for all phishing identiﬁ-
cation approaches. In practice, we can mitigate this issue by
enhancing the target list. Section 5.4 shows the performance
of Phishpedia when the logos of new brands are added to the
target brand list at runtime.
5.3 Analyses of individual components (RQ2)
In this section, we conduct step-by-step experiments to evalu-
ate the core components of Phishpedia independently.
5.3.1 Evaluating logo detection
We use (cid:24)29K samples in labelled screenshot dataset for train-
ing the model and around 1,600 for testing. We compute
Average Precision (AP) for each class (i.e., logo and input
USENIX Association
30th USENIX Security Symposium    3801
Table 3: Object Detection Accuracy (Average Precision)
Object Class
Training AP
Testing AP
Logo
52.7
49.3
Input Boxes
73.5
70.0
Overall (mAP)
63.1
59.7
Figure 15: Accuracy of Siamese model (Precision-Recall
Curve). The x-axis is recall and the y-axis is precision.
box) for IoU1 threshold ranging from 0.5 to 0.95 with inter-
vals of 0.05. Table 3 presents the results. In comparison to the
Faster RCNN proposal [58], which achieved mAP of 67.9 on
PASCAL VOC dataset [73], the mAP we achieve (i.e., 63.1
for training and 59.7 for testing) in this experiment for pre-
dicting logo and input box indicates acceptable performance.
5.3.2 Evaluating logo recognition
For evaluating the Siamese model independently, we manu-
ally labelled the identity logo for 1,000 phishing webpage
screenshots over 181 brands and sampled 1,000 benign web-
page screenshots with labelled identity logos. Then, we give
2,000 identity logos (each from a screenshot) as input to our
trained Siamese model, to evaluate how well our Siamese
model can compare logos. Note that, these logos from the
screenshots are samples not in the training dataset.
We also experiment one alternative backbone network and
one alternative input (in terms of logo color). We experiment
Resnet50 [28] and RestnetV2-50 [29] as backbone networks,
and consider two forms of logo input — one in RGB and
another in grey-scale. By changing the similarity threshold
of Siamese model from 0.5 to 0.9 with 0.05 as interval, we
plot the precision-recall curve for each conﬁguration in Fig-
ure 15. In general, the performances of four conﬁgurations
are comparable and acceptable. Moreover, the Resnetv2 with
RGB logo (blue plot) achieves the best performance. With
the above results, we conclude that both Faster-RCNN and
Siamese model achieve good performance to recognize and
compare logos.
1IoU stands for intersection of union, and is used to evaluate the overlap
of the reported bounding box with the ground truth box. The concept of IoU,
average precision (AP), and mean average precision (mAP) are established
terminologies in object detection algorithms (see [83] for more details).
5.4 Phishpedia generalization (RQ3)
In this experiment, we evaluate whether our Siamese model
is generalizable when new logos (not used in training) are
added to target brand list. To this end, we train the model on
the second stage of transfer learning (see our discussion in
Section 3.2) with only 130 brand logos in the target brand
list and check whether it can effectively match the remaining
51 brand logos (on which the model is not trained, but forms
the new target list). We randomly sample logos of 51 brands,
which cover 7,411 phishing webpages in our labelled dataset.
Among the 7,411 webpages covered by 51 “new” brands,
Phishpedia recognized 87.46% phishing webpages with high
identiﬁcation rate of 99.91%. It indicates that the Siamese
model well captures generalizable features extracted from
logo samples. Thus, our approach is generalizable for adding
new logos in the target brand list during runtime.
5.5 Alternative options (RQ4)
Next, we evaluate other technical options to implement Phish-
pedia. We investigate the following technical options:
• Op1: How does other well-known object detection algo-
rithm (one-stage model, e.g., Yolov3 [57]) perform logo
recognition?
• Op2: How does a Siamese model trained with one-stage
transfer learning and two-stage transfer learning perform
logo comparison (see our discussion in Section 3.2)?
• Op3: How does a Siamese model trained with conven-
tional procedure (e.g., Triplet loss function) perform logo
comparison?
• Op4: Can we replace the Siamese model with a simpler
approach such as perceptual hashing (PH) [14]?
5.5.1 Setup
For Op1, we select Yolov3, a popular one-stage object de-
tection model. We adopt a Yolov3 model implemented with
Tensorﬂow 1.4 framework. We train the model on the same
cluster where our Faster-RCNN model is trained (see Sec-
tion 4). For Op2, we compare the Siamese model trained
with one-stage training with the model trained with two-stage
training. For Op3, we train Siamese model in a conventional