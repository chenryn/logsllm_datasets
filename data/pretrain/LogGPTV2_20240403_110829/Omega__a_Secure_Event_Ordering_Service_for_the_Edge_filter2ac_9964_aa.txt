title:Omega: a Secure Event Ordering Service for the Edge
author:Cl&apos;audio Correia and
Miguel Correia and
Lu&apos;ıs Rodrigues
2020 50th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)
Omega: a Secure Event Ordering Service for the Edge
Lu´ıs Rodrigues
Cl´audio Correia
INESC-ID, Instituto Superior T´ecnico, Universidade de Lisboa
{claudio.correia, miguel.p.correia, ler}@tecnico.ulisboa.pt
Miguel Correia
Abstract—Edge computing is a paradigm that extends cloud
computing with storage and processing capacity close to the edge
of the network that can be materialized by using many fog nodes
placed in multiple geographic locations. Fog nodes are likely
to be vulnerable to tampering, so it is important to secure the
functions they provide. A key building block of many distributed
applications is an ordering service that keeps track of cause-effect
dependencies among events and that allows events to be processed
in an order that respects causality. In this paper we present the
design and implementation of a secure event ordering service for
fog nodes. Our service, named Omega, leverages the availability
of a Trusted Execution Environment (TEE) based on Intel SGX
technology to offer fog clients guarantees regarding the order in
which events are applied and served, even when fog nodes are
compromised. We have also built OmegaKV, a key-value store
that uses Omega to offer causal consistency. Experimental results
show that the ordering service can be secured without violating
the latency constraints of time-sensitive edge applications, despite
the overhead associated with using a TEE.
Keywords-Security, IoT, Fog, Edge, Intel SGX
I. INTRODUCTION
Cloud computing is a model for deploying Internet appli-
cations that allows companies to execute services in shared
infrastructures, typically large data centers, that are managed
by cloud providers. The economies of scale that result from
using large shared infrastructures reduce the deployment costs
and make it easier to scale the number of resources associated
with each application in response to changes in demand. Cloud
computing has been, therefore, widely adopted both by private
and public services [1].
Despite its beneﬁts, cloud computing has some limitations.
The number of data centers that offer cloud services is
relatively small, and they are typically located in a few central
locations. For instance, Google currently maintains 16 data
centers; and only 3 of these data centers are not located in
North America or Europe [2]. Thus, clients that operate far
from these data centers may experience long latencies [3].
Many applications deployed in the cloud provide a range
of services to clients that reside in the edge of the network:
desktops, laptops, but also smartphones or even smart devices
such as cameras or home appliances, also known as the
Internet of Things (IoT). The number and capacity of these
devices have been growing at a fast pace in recent years.
Many of these devices can run real time applications, such as
augmented reality or online games, that require low latencies
when accessing the cloud. In fact, it is known that a response
time below 5ms–30ms is typically required for many of these
applications to be usable [4].
One solution to address the latency requirements of new
edge applications is to process data at the edge of the network,
close to the devices, a paradigm called edge computing [5].
To support edge computing, one can complement the services
provided by central data centers with the service of smaller
data centers, or even individual servers, located closer to the
edge. This concept is often named fog computing [6]–[8]. It
assumes the availability of fog nodes that are located close
to the edge. The number of fog nodes is expected to be
several orders of magnitude larger than the number of data
centers in the cloud. Cloud nodes are physically located in
secure premises, administered by a single provider. Fog nodes,
instead, are most likely managed by several different local
providers and installed in physical locations that are more
exposed to tampering. Therefore, fog nodes are substantially
more vulnerable to being compromised [9], [10], and develop-
ers of applications and middleware for edge computing need
to take security as a primary concern in the design.
In this paper, we address the problem of securing middle-
ware for edge computing. Speciﬁcally, we focus on securing
an event ordering service that is able to keep track of cause-
effect dependencies among events and that allows events to
be processed in an order that respects causality. The ability
to keep track of causal relations among events is at the heart
of distributed computing and, as such, an ordering service is
a fundamental building block for many applications such as
storage services [11], graph stores [12], [13], social networks
[14], online games [15], among others. The idea of providing
an event ordering service is not new (an example is Kronos
[16]) but, to the best of our knowledge, we are the ﬁrst to
address the problem of providing secure implementations that
may be safely executed in fog nodes.
Our service, named Omega, has as main goals to provide
the following guarantees over data stored in fog nodes:
• Integrity: A fog node cannot modify application data
without this being detected.
• Freshness: A fog node cannot return an old version of
data, without this being detected.
• Causal Consistency: A fog node cannot modify the
causal order of events without being detected.
Omega leverages the wide availability of support for Trusted
Execution Environments (TEE), namely of Intel SGX en-
claves, to offer fog clients guarantees regarding the order
by which events are applied and served, even when fog
nodes become compromised. We take particular care to use
lightweight cryptographic techniques to ensure data integrity
while keeping a reasonable tradeoff with availability. A key
goal is to secure the ordering service without violating the la-
tency constraints imposed by time-sensitive edge applications.
978-1-7281-5809-9/20/$31.00 ©2020 IEEE
DOI 10.1109/DSN48063.2020.00062
489
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:36:47 UTC from IEEE Xplore.  Restrictions apply. 
We achieve this by using enclaves only for a few important
operations. In particular, applications run outside the TEE and
use the enclave to selectively request proofs over the order
of operations. Also, the interface of Omega is, as it will be
discussed later, richer than that of services such as Kronos.
Omega is the ﬁrst system that provides an ordering service
that allows clients to access and navigate the history of all
events in a secure and efﬁcient manner, despite intrusions in
the Omega node. Clients can crawl the event history without
having to constantly access the enclave. All events are ordered
and stored in the untrusted zone and the client is only required
to access the enclave to get the root of the event history.
To illustrate the use of Omega and to assess its performance,
we have built a key-value store named OmegaKV, that offers
causal consistency [17] for the edge. OmegaKV is an extension
of causal-consistent key-value stores that have been previously
designed for the cloud [11], [18]–[20]. We are particularly
interested in extending key-value stores that offer causal con-
sistency, since this is the strongest consistency model that can
be enforced without risking blocking the system when network
partitions or failures occur [21]–[23]. Clients of OmegaKV can
perform write and read operations on data replicated by fog
nodes, and are provided with the guarantees that writes are
applied in causal order and that reads are also served in an
order that respects causality.
We experimentally assessed the performance of Omega
using a combination of micro-benchmarks and its use to
secure the metadata required by OmegaKV. Our experimental
results show that Omega introduces an additional latency of
approximately 4ms, which is much smaller than the latency
required to access central cloud data centers, and that, contrary
to cloud based solutions, allows latency values in the 5ms-
30ms range, as required by time-sensitive edge applications.
II. BACKGROUND AND RELATED WORK
A. Edge Computing and Fog Nodes
Edge computing is a model of computation that aims at
leveraging the capacity of edge nodes to save network band-
width and provide results with low latency. However, many
edge devices are resource constrained (in particular, those that
run on batteries) and may beneﬁt from the availability of
small servers placed in the edge vicinity, a concept known
as fog computing. Fog nodes provide computing and storage
services to edge nodes with low latency, setting the ground
for deploying resource-eager latency-constrained applications,
such as augmented reality.
B. Securing Fog Services
While some edge infrastructures may be located in secure
premises, many applications will require a number of edge
servers to be placed in vulnerable locations (e.g., Road Side
Units [24]). Having fog nodes dispersed among multiple
geographic locations, close to the edge, increases the risk
of being attacked and becoming malicious. Therefore, the
security of edge services is a growing concern [9], [10],
[25]. A compromised fog node may delete, copy, or alter
operations requested by edge devices, causing information to
be lost, leaked, or changed in such a way that it can lead the
application to a faulty state. To address this challenge, one
needs to resort to a combination of techniques, from which
we highlight replication and hardening.
Replication consists in relying on multiple fog nodes in-
stead of a single node. If enough fog nodes are used, it may be
possible to mask arbitrary faults (often designated Byzantine
faults [26]) and, in some cases, to detect compromised nodes.
Techniques such as Byzantine quorums [3], [27] can be used
for this purpose. Although they require contacting multiple fog
nodes, this is the only way to ensure that critical information
is not lost due to a compromised fog node, as such a node
may become silent. Unfortunately, contacting and voting on
the output of multiple fog nodes increases the latency of
operations and may defeat the very purpose of fog computing.
Therefore, we assume that many applications will be able to
make progress while contacting a single fog node, specially if
the fog node can execute quorum validations in the background
and is hardened.
Hardening [28] consists in using software and/or hardware
mechanisms to reduce the ability of the adversary to compro-
mise a device. Using the appropriate techniques it may be
possible to prevent a compromised fog node from altering
information unnoticed, effectively reducing the amount of
damage an infected fog node can cause. A relevant mecha-
nisms in this context is the use of a TEE, a secured execution
environment with guarantees provided by the processor. The
code that executes inside a TEE is logically isolated from the
operating system (OS) and other processes, providing integrity
and conﬁdentiality, even if the OS is compromised. TEEs have
been identiﬁed as one of the most promising technologies to
secure computation and sensitive data in fog nodes [29].
Intel Software Guard Extensions (SGX) is a set of func-
tionalities introduced in sixth generation Intel Core micro-
processors that implement a form of TEEs named enclaves
[30], [31]. The potential beneﬁts of this technology for the fog
have already been recognized by Intel [32] and it has already
been used in practice [33], [34]. Applications designed to use
SGX have two parts: an untrusted part and a trusted part. The
trusted part runs inside the enclave, where the code and data
have integrity and conﬁdentiality; the untrusted part runs as a
normal application. The untrusted part can make an Enclave
Call (ECALL) to switch into the enclave and start the trusted
execution. The opposite is also possible using an Outside
Call (OCALL). The SGX architecture implements a number
of mechanisms to ensure the integrity of the code, including
an attestation procedure that allows a client to get a proof
that it is communicating with the speciﬁc code in a real SGX
enclave, and not an impostor [35]. A limitation of current SGX
implementations is that the protected memory region, named
enclave page cache, is limited to 128 MB [36]. Therefore, it
is essential to minimize the memory usage inside the enclave.
In particular, the use of more memory also increases the swap
time from enclave and out. While attacks against SGX like
Foreshadow [37], [38] exist, Intel continues to investigate how
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:36:47 UTC from IEEE Xplore.  Restrictions apply. 
490
to mitigate these issues.
With the availability of Intel SGX new systems have
emerged to alleviate SGX limitations. SCONE [39] supports
secure Linux containers that offer I/O data operations efﬁ-
ciently, in Omega all enclave operations are done in memory
thus avoiding the use of I/O operations. ROTE and LCM [40],
[41] propose efﬁcient monotonic counter that Omega could use
to persistently store its state and prevent rollback attacks.
C. Event Ordering
Most distributed applications need to keep track of the
order of events. Different techniques can be used for this
purpose, from synchronized physical clocks [42], [43], logical
Lamport clocks [17], vector clocks [44], [45], hybrid clocks
[46], and others. In most cases, the event ordering service
is a core component of the application and if this service is
compromised the correctness of the application can no longer
be ensured [47], [48].
In many cases, applications use their own technique to
order events, so the implementation of the ordering service is
intertwined with the application logic. This approach has two
important drawbacks: ﬁrst, it is hard to keep track of chains of
related events across multiple applications [49], [50]. Second,
it causes developers to maintain potentially complex code, that
is duplicated in many slightly different variations.
Kronos [16] was recently proposed as an alternative ap-
proach that consists in offering event ordering as a service and
can be used by multiple applications, although it was designed
for the cloud and does not implement security measures. In the
context of edge computing, implementing the event ordering as
a separate service that is provided by fog nodes makes it easier
to harden the implementation, increasing the robustness of the
applications that use such a secured version of the service. In
this paper we follow this path and describe the design and
implementation of Omega, a secure event ordering service to
be executed at fog nodes.
D. Edge Storage
To unleash their full potential, fog nodes should not only
provide processing capacity, but also cache data that may be
frequently used [51]; otherwise, the advantages of processing
on the edge may be impaired by frequent remote data accesses
[52]. By using cached data, requests rarely need to be served
by data centers. Consequently, a key ingredient of edge-
assisted cloud computing is a storage service that extends
the one offered by the cloud in a way that relevant data is
replicated closer to the edge. Therefore, in this paper we also
describe the implementation of a storage service to be provided
by fog nodes, that we have named OmegaKV. This storage
service extends key-value stores designed for the cloud that
offer causal consistency [11], [18]. This consistency criteria
is particularly meaningful for edge computing, given that it
was shown to be the strongest consistency criteria that can be
offered without compromising availability [53].
Very recently, two key-value stores that leverage SGX have
been proposed: ShieldStore [54] and Speicher [55]. Both
have been designed to operate in data centers at the cloud
layer. Omega is a more general service, that can be used to
implement a key-value store but also other services at the
fog layer. The authors of ShieldStore suggest that a Merkle
tree could be used to store data outside the enclave, but they
have not implemented that strategy. As it will be discussed,
Omega, that was developed concurrently with ShieldStore and
Speicher, does use and evaluate the use of a Merkle tree in its
implementation. Speicher uses a table in memory and stores
within the enclave one hash per row of this table being a
limitation on system scalability. Additionally, when this table
becomes full, Speicher uses the enclave to store data on disk
having a heavy latency cost. Pesos [56] is secure object store
that takes advantage of SGX. Pesos was also built for the cloud
and assumes a secure third party to persistently store the data,
while OmegaKV stores the data locally in the untrusted part.