wildcard bits.
Consider two policy atoms with the same wildcard bits. If the
two patterns are identical, then so is their intersection. Therefore,
the original two patterns get pruned, leaving only the intersection.
Now, suppose the two patterns are distinct (and still have the same
wildcard bits). Therefore, since their wildcards are the same, they
both match some header differently, and thus their intersection is
empty and pruned.
Our full complexity analysis, available in an extended tech re-
port [17], shows that when the number of policy atoms, n, is larger
than 2h, then the compilation algorithm runs in O(n2) time and
produces a ﬂow table of size O(n2). Note that h is effectively a
constant, ﬁxed by the number of header ﬁelds which may deter-
mine a ﬂow; this value is limited by the number of ﬁelds deﬁned
in OpenFlow. OpenFlow 1.0 patterns are 12-tuples, and our current
policies only use 5 header ﬁelds. Therefore, on policies with more
than 25 policy atoms, the algorithm is quadratic.
Updating Flow Tables
It is not enough for PANE to generate ﬂow tables quickly. It
must also propagate switch updates quickly, as the time required
to update the network affects the effective duration of requests. The
OpenFlow protocol only allows switches to be updated one rule at a
time. A naive strategy is to ﬁrst delete all old rules, and then install
new rules. In PANE, we implement a faster strategy: the controller
state stores the rules deployed on each switch; to install new rules,
it calculates a “diff” between the new and old rules. These diffs
are typically small, since rule-table updates occur when a subset of
policy atoms are realized or unrealized.
6. THE PANE CONTROLLER
The complete PANE system integrates the previously described
components into a fully-functioning SDN controller, as depicted in
Figure 1. It manages simultaneous connections with the network’s
principals and its switches. In this role, it is responsible for imple-
menting both our participatory networking API, and the details of
computing default forwarding routes, transmitting OpenFlow mes-
sages, and reacting to network changes such as switches joining
and links failing. To accomplish these tasks, the PANE controller
maintains three data structures: the share tree, a sequence of policy
trees, and a network information base (NIB), described below.
We have developed a prototype PANE controller using Haskell
and the Nettle library for OpenFlow [46]. We use and extend the
HFT compiler described in [16]. Although we chose OpenFlow as
our substrate for implementing PANE, its design does not depend
on OpenFlow. PANE could be implemented using other mecha-
nisms to control the network, such as 4D [24], MPLS, or a col-
lection of middleboxes.
The PANE controller is an entirely event-driven multicore pro-
gram. The three primary event types are incoming PANE API mes-
sages, incoming OpenFlow messages, and timer events triggered
by the start or ﬁnish of previously accepted requests or realizable
hints. A prototype release is available on Github, and we provide a
virtual machine for Mininet-based evaluation on our website.1 The
release also includes a Java library which implements an object-
oriented interface to PANE’s text API.
API messages always specify a share on which they are oper-
ating. When a message arrives, the PANE controller ﬁrst uses the
share tree to determine whether it is authorized, and then, for re-
quests, whether it is feasible by consulting the policy trees, as de-
scribed in the previous sections.
When requests start and expire, the PANE controller compiles
the new policy tree to a set of switch ﬂow tables, translating high-
level actions to low-level operations on individual switches in the
network. For example, a Reserve(n) action becomes a circuit of
switch queues and forwarding rules that direct packets to those
queues. As we will describe next, PANE’s runtime uses its NIB and
a default forwarding algorithm to realize this and other actions. Our
implementation constructs a spanning tree and implements MAC
learning as its forwarding algorithm.
When possible, PANE uses the slicing extension to OpenFlow
1.0 to create queues, and out-of-band commands when necessary.
While OpenFlow allows us to set expiry timeouts on ﬂow table en-
tries, PANE must explicitly delete queues when reservations expire.
6.1 Network Information Base
A network information base (NIB) is a database of network ele-
ments – hosts, switches, ports, queues, and links – and their capabil-
ities (e.g., rate-limiters or per-port output queues on a switch). The
runtime uses the NIB to translate logical actions to a physical con-
ﬁguration, determine a spanning tree for default packet forwarding,
and to hold switch information such as manufacturer, version, and
its ports’ speeds, conﬁgurations, and statistics.
For example, PANE’s runtime implements a bandwidth reserva-
tion, (M, Reserve(n)), by querying the NIB for the shortest path
with available queues between the corresponding hosts. Along this
path, PANE creates queues which guarantee bandwidth n, and ﬂow
table rules to direct packets matching M to those queues. We chose
this greedy approach to reserving bandwidth for simplicity, and
leave the implementation of alternatives as future work.
PANE also uses the NIB to install Deny rules as close as pos-
sible to the trafﬁc source. For example, if the source is outside
our network, this is the network’s gateway switch. If the source
is inside the network, packets are dropped at the closest switch(es)
with available rule space. The NIB we implement is inspired by
Onix [30]. It uses a simple discovery protocol to ﬁnd links between
switches, and information from our forwarding algorithm, such as
ARP requests, to discover the locations of hosts.
6.2 Fault Tolerance and Resilience
The PANE controller must consider two types of failures. The
ﬁrst is failure of network elements, such as switches or links, and
the second is failure of the controller itself.
When a switch or link fails, or when a link’s conﬁguration changes,
the PANE runtime must recompile the policy tree to new individual
switch ﬂow tables, as previously used paths may no longer be avail-
able or acceptable. Because the underlying network has changed,
this recompilation step is not guaranteed to succeed. If this hap-
pens, we defer to PANE’s ﬁrst come-ﬁrst serve service model, greed-
ily replaying requests to build a new policy tree which does com-
pile; implementing this simply requires annotating the current pol-
icy tree’s policy atoms with the order in which they were created.
Principals are notiﬁed via call-backs if a previously accepted re-
quest is now unsatisﬁable. Developing a more sophisticated ap-
proach to re-constructing a feasible policy tree, perhaps taking ad-
1http://pane.cs.brown.edu
vantage of priorities, or with the goal of maximizing the number of
restored requests, remains as future work.
To handle failure of the controller, we can keep a database-like
persistent redo log of accepted requests, periodically compacted by
removing those which have expired. Upon recovery, the PANE con-
troller could restore its state from this log. In production settings,
we expect the PANE controller to be deployed on multiple servers
with shared, distributed state. Switches would maintain connec-
tions to each of the controllers as newer OpenFlow speciﬁcations
support. We leave the design and analysis of both options as fu-
ture work. Because network principals use PANE in an opt-in fash-
ion to receive predictable performance, a complete runtime failure
would simply return the network to its current state of providing
best-effort performance only.
6.3 Additional Features
The PANE runtime supports several additional features beyond
the requests, hints, and queries previously described. Principals are
able to query PANE to determine their available capabilities, exam-
ine the schedule of bandwidth availability, create sub-shares, and
grant privileges to other principals. PANE’s API also provides com-
mands to determine which existing requests and shares can affect
a speciﬁed ﬂowgroup; this is particularly useful for debugging the
network, such as to determine why certain trafﬁc is being denied.
Beyond the API, the PANE controller also includes an adminis-
trative interface which displays the current state and conﬁguration
of the network, real-time information about the controller’s perfor-
mance such as memory and CPU usage, and allows the dynamic
adjustment of logging verbosity.
7. EVALUATION
We evaluate our PANE prototype with the Mininet platform for
emulating SDNs [32], and with real networks. Our primary testbed
includes two Pronto 3290 switches and several software OpenFlow
switches (both Open vSwitch and the reference user-mode switch)
on Linux Intel-compatible hardware, and on the TP-Link WR-1043ND
wireless router. Wired connections are 1 Gbps and wireless runs
over 802.11n. Clients on the network include dedicated Linux servers,
and ﬂuctuating numbers of personal laptops and phones. In addi-
tion to the participatory networking API, the network also provides
standard services such as DHCP, DNS, and NAT.
Members of our group have been using the testbed since Febru-
ary 2012 to manage our trafﬁc, and during this time, it has been our
primary source of network connectivity. The testbed is compatible
with unmodiﬁed consumer electronic devices, which can interact
with a PANE controller running at a well-known location.2
In the following sections, we examine two aspects of our proto-
type. First, we consider four case studies of real applications that
use the PANE API to improve end-user experience (§7.1). Second,
we evaluate the practicality of implementing the PANE API in cur-
rent OpenFlow-enabled networks, considering questions such as
the latency of processing requests, and the number of rules created
by networked applications (§7.2).
7.1 Application Usage
We ported four real applications to use the PANE API: Ekiga,
SSHGuard, ZooKeeper, and Hadoop. We now describe how inten-
tions of an application developer or user can be translated to our
API, and the effects of using PANE on the network and the ap-
2The PANE controller could also be speciﬁed using a DHCP
vendor-speciﬁc or site-speciﬁc option.
plication. Our PANE-enabled versions of these applications are all
publicly available on Github.3
7.1.1 Ekiga
Ekiga is an open source video conferencing application. We mod-
iﬁed Ekiga to ask the user for the anticipated duration of video calls,
and use a Reserve message to request guaranteed bandwidth from
the network between the caller’s host and either the network gate-
way or the recipient’s host, for the appropriate time. If such a reser-
vation is not available, Ekiga retrieves the schedule of available
bandwidth from PANE and calculates the earliest time at which a
video call or, alternatively, an audio call, can be made with guaran-
teed quality. It then presents these options to the user, along with a
third option for placing a “best effort” call right away.
Realizable reservations cause the PANE controller to create guar-
anteed bandwidth queues along the path of the circuit, and install
forwarding rules for Ekiga’s trafﬁc.
Measurements of Skype use on a campus network with more
than 7000 hosts show that making reservations with PANE for VoIP
applications is quite feasible. Skype calls peaked at 75 per hour,
with 80% of calls lasting for fewer than 30 minutes [6]. This fre-
quency is well within current OpenFlow switches’ capabilities, as
we measure in §7.2.
SSHGuard
7.1.2
SSHGuard is a popular tool to detect brute-force attacks via log
monitoring and install local ﬁrewall rules (e.g., via iptables) in
response. We modiﬁed SSHGuard to use PANE as a ﬁrewall back-
end to block nefarious trafﬁc entering the network. In particular,
this means such trafﬁc no longer traverses the targeted host’s ac-
cess link.
For example, if Alice is running SSHGuard on her host and it
detects a Linux syslog entry such as:
sshd[2197]: Invalid user Eve from 10.0.0.3
SSHGuard will block Eve’s trafﬁc for the next ﬁve minutes using
PANE’s Deny request. The PANE controller then places an Open-
Flow rule to drop packets to Alice’s host coming from Eve’s at a
switch close to Eve’s host.
Although this is a basic example, it illustrates PANE’s ability to
expose in-network functionality (namely, dropping packets) to end-
user applications. Besides off-loading work from the end-host’s
network stack, this approach also protects any innocent trafﬁc which
might have suffered due to sharing a network link with a denial-of-
service (DoS) attack.
To demonstrate this beneﬁt, we generated a UDP-based DoS at-
tack within our testbed network. We started an iperf TCP trans-
fer between two wireless clients, measured initially at 24 Mbps. We
then launched the attack from a Linux server two switch-hops away
from the wireless clients. During the attack, which was directed at
one of the clients, the performance of the iperf transfer dropped
to 5 Mbps, rising to only 8 Mbps after the victim installed a local
ﬁrewall rule. By using PANE to block the attack, the transfer’s full
bandwidth returned.
7.1.3 ZooKeeper
ZooKeeper [27] is a coordination service for distributed sys-
tems used by Twitter, Netﬂix, and Yahoo!, among others, and is a
key component of HBase. Like other coordination services such as
Paxos [31], ZooKeeper provides consistent, available, and shared
3https://github.com/brownsys/
Figure 5: Latency of ZooKeeper DELETE requests.
(a)
(b)