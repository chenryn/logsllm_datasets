i n
Preparing project data and running SonarQube. We execute the fol-
lowing three steps to analyze the quality of each of the AIOps and baseline
projects.
Step 1. Clone the projects from GitHub.WeclonetheGitHubrepos-
itories on the local machine so that we can analyze their source code.
Step 2. Preprocess the project data.Fortwoofthepopularlanguages
amongourprojects(i.e.,PythonandJava),wehavetoperformapreprocessing
phasebeforeperformingthesourcecodeanalysisusingSonarQube.Wedonot
need to perform this phase for other languages.
Python code. SonarQube does not support the .ipynb format which is the
file format of Jupyter notebooks. To overcome this limitation, we first con-
vert the .ipynb files in each project to the .py format using the nbconvert
6https://docs.sonarqube.org/latest/user-guide/rules/
22 RoozbehAghilietal.
library (Jupyter, 2022) in Python. We then use SonarQube to analyze the
resulting .py files.
Java code. To analyze the Java code, SonarQube requires the compiled
files (.class files). However, in most cases, developers do not upload the .class
files in their GitHub repositories and only put the .java files. To address this
issue, we compile the .java files into .class files. We primarily leverage the
build automation tool (i.e., Gradle, Apache Maven, and Apache ANT) of the
project to compile the project files. For example, for Maven projects, we use
mvn compile. If we could not find a build automation tool, we use javac to
compile the .java files.
Step 3. Execute SonarQube. We write a script that sends the source
code of each project to a SonarQube web server that is installed locally. After
SonarQube finishes the analysis, we extract the results.
We are able to analyze the source code of 6,101 out of the 7,777 projects
(78%) in all three groups of projects (AIOps projects and the two baselines).
Thesuccessratiois95%,85%,and72%forAIOps,ML,andGeneralprojects,
respectively. The main reasons of failures include removed GitHub reposito-
ries and failures in compiling the Java projects. Analyzing the source code of
projects at this scale is rarely reported in the literature, and performing the
SonarQube analysis experiments took several weeks.
3.3.3 Results
AIOps projects have poorer quality than the ML and General base-
lines.Table8summarizesthecodequalitymetricsextractedfromSonarQube
for the AIOps and the baseline projects. We report the mean and median
number of each of the quality metrics. Due to the skewed data and similar to
Section 3.1.3, we mainly compare the metrics using their median values.
Forbugs,codesmells,vulnerabilities,securityhotspots,andtechnicaldebt,
wefurtherreportthevaluesnormalizedbytheLOCofeachproject.Wereport
these normalized values to ensure that our findings are robust and not biased
by the size of the projects.
Table8:ThecodequalitymetricsoftheAIOpsprojectsandthetwobaselines
extracted from SonarQube. The mean and median are represented by µ and
χ,respectively.The“raw” valuesaretheexactnumbersextractedfromSonar-
(cid:101)
Qube, and the “norm” indicates normalized values by LOC of each project.
Bugs Codesmells Vulnerabilities Securityhotspots
raw norm raw norm raw norm raw norm
µ χ(cid:101) µ χ(cid:101) µ χ(cid:101) µ χ(cid:101) µ χ(cid:101) µ χ(cid:101) µ χ(cid:101) µ χ(cid:101)
AIOps 109.8 3.0 4.7 1.1 641.3 57.0 54.5 34.8 0.2 0.0 0.1 0.0 28.5 1.5 2.2 0.5
ML 55.6 1.0 6.8 0.5 164.0 27.0 44.2 25.2 0.1 0.0 0.0 0.0 7.8 0.0 2.2 0.0
General 58.3 1.0 8.6 0.4 241.0 13.0 35.5 17.3 0.2 0.0 0.1 0.0 16.0 1.0 3.7 0.3
Technicaldebt(hour) LOC(k) Commentlines(k) Commentdensity(%)
raw norm
µ χ(cid:101) µ χ(cid:101) µ χ(cid:101) µ χ(cid:101) µ χ(cid:101)
AIOps 118.6 6.6 5.6 4.6 78.1 1.4 20.3 0.4 20.4 19.6
ML 29.1 3.2 5.0 3.2 43.6 0.9 2.7 0.2 21.8 20.2
General 52.6 1.7 5.3 2.4 16.6 0.7 2.2 0.1 11.3 7.6
StudyingtheCharacteristicsofAIOpsProjectsonGitHub 23
100
Security hotspots
V ulnerabilities
80 C ode sm ells
B ugs
s
e
60 ussi
e
ht
40 f
o
%
20
0
A IO p s M L G e n e ra l
Fig. 8: The distribution of the issues among the AIOps and baseline projects.
As shown in Table 8, the number of issues (i.e., bugs, code smells, vul-
nerabilities, and security hotspots) in AIOps projects are higher than that of
baselines considering both raw and normalized values. This difference is more
evident in the median number of bugs (3 in AIOps, 1 in ML and General)
and code smells (57 in AIOps, 27 in ML, and 13 in General). This pattern is
also visible in the normalized numbers; for example, the normalized median
of code smells in AIOps projects is 34.8 per KLOC, much higher than for ML
(25.2) and General (17.3) baselines.
Following the same pattern, the needed time to resolve these issues (i.e.,
technicaldebt)inAIOpsprojectsistwicethatofMLprojectsandnearlyfour
times that of General projects (6.6, 3.2, and 1.7 in AIOps, ML, and General
projects,respectively).Thesefindingsareinlinewiththehighvolumeofissues
observed in AIOps repositories, as shown in Table 3.
AIOps projects have more lines of code than baselines, and same
amount of comments as the ML baseline. As shown in Table 8, the
median LOC in AIOps projects is higher than the baselines (1.4k compared
to 0.9k and 0.7k). This is also aligned with our observation about the size
of the projects in Table 3. Regarding the comment density, we can observe a
similarpatternamongtheAIOpsandMLprojects,wheretheuseofcomments
is much higher than in the General baseline (19.6% and 20.2% compared to
7.6%). This indicates that AIOps and ML projects document their code more
adequately.
AIOps projects suffer from code smells more than other types of
issues. Figure 8 illustrates the distribution of bugs, code smells, vulnerabil-
ities, and security hotspots among the projects. The percentage of security
hotspots and vulnerabilities is much smaller than bugs and code smells in all
theprojects.InAIOpsprojects,themainissueisrelatedtocodesmells,where
they account for 82% of all issues, ten percent more than the ML baseline.
AIOps and ML projects have similar types of issues in terms of vio-
lated SonarQube rules and rule categories.Wereportthemostviolated
rulesandrulecategoriesinTables9and10.Wealsodefinetheseviolatedrules
24 RoozbehAghilietal.
Table 9: The top-10 violated SonarQube rules for AIOps projects and the
baselines. “Sev” indicates the severity of issues, “W” represents the weight of
rules, and “N” is the percentage of projects with that rule. “Mn” stands for
Minor, “Mj” stands for Major, and “Cr” stands for Critical.
AIOps ML General
Rule Sev W(%) N(%) Rule Sev W(%) N(%) Rule Sev W(%) N(%)
python:S117 Mn 17.9 58.2 python:S117 Mn 17.7 59.3 python:S117 Mn 3.8 15.0
python:S125 Mj 15.5 71.4 python:S125 Mj 13.7 64.4 javascript:S1117 Mj 3.4 19.4
python:S1192 Cr 7.6 64.8 python:S1481 Mn 6.9 54.9 python:S125 Mj 3.0 17.0
python:S905 Mj 5.6 26.4 python:S905 Mj 6.4 26.8 Web:S5254 Mj 2.5 22.6
python:S1481 Mn 3.8 52.7 python:S1192 Cr 6.1 48.0 python:S1192 Cr 2.1 15.2
python:S3776 Cr 3.2 47.3 python:S1542 Mj 4.1 39.9 html:S1100 Mn 2.1 12.3
python:S1542 Mj 2.9 48.4 python:S2320 Mj 3.9 14.1 python:S1481 Mn 2.0 15.3
python:S2208 Cr 2.6 28.6 python:S3776 Cr 3.6 41.3 javascript:S125 Mj 1.9 16.8
python:S100 Mn 2.5 23.1 python:S2208 Cr 2.5 25.2 python:S2320 Mj 1.8 7.0
python:S116 Mn 1.5 20.9 python:S5754 Cr 1.7 24.1 css:S4666 Mj 1.7 14.7
Table 10: The top-10 violated SonarQube tags (rule categories) for AIOps
projects and the baselines. “W” represents the weight of tags, and “N” is the
percentage of projects with that tag.
AIOps ML General
Tag W(%) N(%) Tag W(%) N(%) Tag W(%) N(%)
convention 26.3 76.9 unused 24.7 82.7 unused 13.7 60.8
unused 24.1 94.5 convention 23.1 68.9 convention 9.2 36.7
design 8.8 76.9 cwe 6.4 53.0 suspicious 7.7 51.4
cwe 7.7 63.7 design 5.6 52.6 cwe 6.3 48.6
brain-overload 4.2 61.5 suspicious 4.1 48.4 pitfall 6.3 46.0
pitfall 4.1 49.4 brain-overload 3.9 47.8 accessibility 5.1 29.2
suspicious 4.1 64.8 pitfall 3.5 40.0 design 4.8 35.9
accessibility 2.1 26.4 obsolete 2.9 19.5 brain-overload 4.4 46.8
obsolete 1.8 23.0 python3 2.5 15.7 wcag2-a 3.2 28.1
bad-practice 1.8 53.8 accessibility 1.9 13.5 clumsy 3.0 38.4
Table 11: The most violated SonarQube rules and their definitions in AIOps
projects.
Rules
Name Definition
python:S117 Local variable and function parameter names should
comply with a naming convention.
python:S125 Sections of code should not be commented out.
python:S1192 String literals should not be duplicated
python:S905 Non-empty statements should change control flow or have
at least one side-effect.
python:S1481 Unused local variables should be removed.
python:S3776 Cognitive Complexity of functions should not be too high.
python:S1542 Function names should comply with a naming convention.
python:S2208 Wildcard imports should not be used.
python:S100 Method names should comply with a naming convention.
python:S116 Field names should comply with a naming convention.
StudyingtheCharacteristicsofAIOpsProjectsonGitHub 25
Table 12: The most violated SonarQube tags (rule categories) and their defi-
nitions in AIOps projects.
Tags
Name Definition
convention Coding convention - typically formatting, naming, whitespace.
unused Unused code; e.g., a private variable that is never used.
design There is something questionable about the design of the code.
cwe Relates to a rule in the Common Weakness Enumeration.
For more information, visit ‘https://cwe.mitre.org/’.
brain-overload There is too much to keep in your head at one time.
pitfall Nothing is wrong yet, but something could go wrong in the
future; a trap has been set for the next person, and they’ll
probably fall into it and screw up the code.
suspicious It’s not guaranteed that this is a bug, but it looks suspiciously
like one. At the very least, the code should be re-examined
and likely refactored for clarity.
accessibility Rules to make content more accessible, clear, and simple.
obsolete Using deprecated elements and attributes.
bad-practice The code likely works as designed, but the way it was designed
is widely recognized as being a bad idea.
andrulecategoriesinAIOpsfield(basedonSonarQubewebsite78)inTables11
and 12. As shown in the tables, AIOps and ML projects have similar rule and
rule category violations. In terms of violated rules, 8 out of the top-10 vio-
lated rules are common between AIOps and ML projects. Furthermore, the
firsttwomostviolatedrules(i.e.,python:S117andpython:S125)arethesame
inAIOpsandMLprojects,withsimilarweights(17.9%and15.5%forAIOps,
and 17.7% and 13.7% for ML projects). Also, in terms of most violated rule
categories, 9 out of the top-10 violated rule categories are common between
AIOps and ML projects.
Comparing issues between the AIOps projects and the General baseline, 4
ofthetop-10violatedrulesand8ofthetop-10violatedruletagsarecommon.
However, the weight and percentage of projects having these issues in AIOps
projects are much higher. As an example, Python:S117 is the most violated
ruleinbothAIOpsandGeneralprojects.However,theweightandpercentage
of projects having this violated rule in AIOps are 17.9% and 58.2% but in the
General baseline are 3.8% and 15.0%, respectively.
WeprovidethecompletelistofviolatedrulesandrulecategoriesinAIOps
projects and other baselines in our replication package.
Naming convention, commented-out code, duplicated string literals,
high complexity of functions, and wildcard imports are among the
top violated rules in AIOps projects. Regarding the most violated rules
inAIOps,thefirstthreearepython:S117,python:S125,andpython:S1192.The
7https://rules.sonarsource.com/
8https://docs.sonarqube.org/latest/user-guide/built-in-rule-tags/
26 RoozbehAghilietal.
first onehas aminor severity andis aboutthe noncompliance ofnaming con-
vention. Shared naming conventions are vital and allow teams to collaborate
effectively. The second one has a major severity and is about commenting out
the unused sections of code which reduces readability. Instead, unused code
should be deleted. The third one has a critical severity and is about using du-
plicatedstringliterals.Itmakestheprocessofrefactoringerror-pronebecause
the programmer must be sure to update all occurrences of the string.
Inadditiontotheseviolatedrules,twomoreruleswithcriticalseverityexist
inthetop10violatedrulesofAIOpsprojects;python:S3776 andpython:S2208.
Python:S3776 is about the cognitive complexity of code. It measures how
hard the control flow of a function is to understand. Functions with high
cognitivecomplexitywillbedifficulttomaintain.Python:S2208 isaboutusing
wildcardimports(i.e.,frommoduleimport*).Importingallpublicnamesfrom
a module has multiple disadvantages. It can lead to conflicts between local
names and imported ones, or same names between two different packages. It
alsoreducescodereadabilityandmaycauseconfusionaboutwhichclassesare
imported and used.
Naming convention, unused or commented-out code, and poor de-
sign are among the top violated rule categories. Regarding the most
violated rule categories in AIOps repositories, convention, unused, and design
are the worst issues. Convention category is about fulfilling coding conven-
tions, including naming functions and variables, complying white-spaces and
indentations. The second most violated rule category is unused. This category
is about unused code that decreases the performance of the system. Some ex-
amples of this category are unused assignments, unused private classes, and