Very slow 
Comprehension 
impact 
High 
Abstraction 
Basic implementation through implementation logic 
Strengths 
Inherent focus on security-relevant code Can sometimes identify 
subtle or abstract flaws Difficult to go off track 
Weaknesses 
Code and data paths balloon up quickly, especially in 
object-oriented code 
Easy to overlook issues 
Requires focus and experience 
Generally, you focus your efforts on searching for any type of behavior that appears 
unsafe: a vulnerability class you recognize, a failure to define a trust boundary where 
it's needed, and so forth. It's hard to go too far off track with this technique because 
you can usually keep yourself on the trail of malleable input data. However, 
overlooking issues when you get tired or impatient can happen, as inevitably you 
start skipping over functions you would have analyzed earlier in the day. 
Unfortunately, this strategy is so time consuming that you're certain to lose focus at 
some point. 
This kind of analysis can prove difficult in object-oriented code, especially poorly 
designed object-oriented code. You'll know quickly whether this is an issue because 
the first user input you trace makes you open five or six source code files, usually 
before the system manages to do anything with the input. In this case, you need the 
assistance of accurate design documentation, including a fairly complete threat 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
126 
model. Failing that, you should postpone your analysis and perform some module or 
class review first to understand the system from an object-oriented perspective. 
Analyze a Module 
The crux of the CC2 technique (see Table 4-5) is reading code line by line in a file. 
Instead of drilling down into function calls and objects you encounter, or back-tracing 
to see how functions are called, you take notes about any potential issues you spot. 
Table 4-5. CC2: Analyze a Module 
Start point 
Start of a source file 
End point 
End of a source file 
Tracing method 
Forward, not control-flow sensitive, not data-flow sensitive 
Goal 
Look at each function in a vacuum and document potential 
issues. 
Difficulty 
Very hard 
Speed 
Slow 
Comprehension 
impact 
Very high 
Abstraction 
Basic implementation through design 
Strengths 
You learn the language of the application 
Easier to analyze cohesive modules 
Can find subtle and abstract flaws 
Weaknesses 
Mentally taxing 
Constant documentation requires discipline 
Easy to mismanage time 
You might not expect this, but many experienced code reviewers settle on the CC2 
technique as a core part of their approach. In fact, two of your authors typically start 
reviewing a new codebase by finding the equivalent of the util/directory and reading 
the framework and glue code line by line. 
This technique has great side benefits for future logic and design review efforts 
because you pick up the language and idioms of the program and its creators. It 
might seem as though you'd miss issues left and right by not tracing the flow of 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
127 
execution, but it actually works well because you aren't distracted by jumping around 
the code constantly and can concentrate on the code in front of you. Furthermore, all 
the code in the same file tends to be cohesive, so you often have similar algorithms to 
compare. 
This technique has tradeoffs as well. First, it's taxing, and often you feel mental 
fatigue kick in after too many continuous hours. Sometimes you stop being effective 
a little while before you realize it, which can lead to missed vulnerabilities. The other 
problem is that documenting every potential issue requires considerable discipline, 
and maintaining the momentum for longer than four or five hours can be hard. 
Generally, you should stop for the day at this point and switch to other types of less 
intense analysis. 
This technique has another hidden flaw: It's easy to go off track and review code that 
isn't security-relevant and isn't teaching you anything about the application. 
Unfortunately, you need to have a good feel for software review to know whether 
you're spending your time effectively. Even considering that, sometimes a piece of 
code just catches your fancy and you follow it down the rabbit hole for the next 
several hours. So make sure you're sticking to your process when using this review 
strategy and accurately assessing how valuable it is. 
Analyze an Algorithm 
The CC3 strategy (see Table 4-6) requires knowing enough of the system design to be 
able to select a security-relevant algorithm and analyze its implementation. This 
strategy is essentially the same as analyzing a module (CC2); however, you're less 
likely to go off track. 
Table 4-6. CC3: Analyze an Algorithm 
Start point 
Start of a key algorithm 
End point 
End of that algorithm 
Tracing method 
Forward, not control-flow sensitive, not data-flow sensitive 
Goal 
Look at the algorithm and identify any possible weakness in the 
design or implementation. 
Difficulty 
Very hard 
Speed 
Slow 
Comprehension 
impact 
Very high 
Abstraction 
Basic implementation through design 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
128 
Table 4-6. CC3: Analyze an Algorithm 
Start point 
Start of a key algorithm 
Strengths 
You can't go off track 
Can find subtle and abstract flaws 
Weaknesses 
Mentally taxing 
Lacks context 
Of course, the effectiveness of this strategy depends almost entirely on the algorithm 
you select to analyze, so you need to choose something security relevant. It's best to 
focus your efforts on pervasive and security critical algorithms, such as those that 
enforce the security model, implement cryptography, or are used in most input 
processing. 
Analyze a Class or Object 
The CC4 strategy (see Table 4-7) is almost the same as analyzing a module (CC2, 
Table 4-5), except you focus on a class implementation. 
Table 4-7. CC4: Analyze a Class or Object 
Start point 
An object 
End point 
All references to that object examined 
Tracing method 
Forward, not control-flow sensitive, not data-flow sensitive 
Goal 
Study the interface and implementation of an important object 
to find vulnerabilities in how the system uses it. 
Difficulty 
Hard 
Speed 
Slow 
Comprehension 
impact 
Very high 
Abstraction 
Basic implementation through design 
Strengths 
Less likely to go off track than in module analysis 
Can find subtle and abstract flaws 
Weaknesses 
Mentally taxing 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
129 
Table 4-7. CC4: Analyze a Class or Object 
Start point 
An object 
Might lack context 
More likely to go off track than in algorithm analysis 
This strategy is more effective than CC2 for object-oriented programs because 
objects tend to be fairly cohesive. It's also less prone to slipping off track, although 
how much is determined by how cohesive and security relevant the object is. As with 
CC2, you need to pay close attention when employing this review strategy. 
Trace Black Box Hits 
Chapter 1(? [????.]), "Software Vulnerability Fundamentals," introduced black box 
testing and fuzz-testing, and this chapter explains how they can affect the 
assessment process. To recap, in black box testing, you manually feed an application 
with different erroneous data to see how the program responds; fuzz-testing uses 
tools to automate the blackbox testing process. You flag your black box input as a 
"hit" when it causes the program to crash or disclose useful information it shouldn't. 
These hits are then traced to identify the vulnerabilities that caused the abnormal 
behavior. Essentially, black box testing is a brute-force method for finding 
vulnerabilities and isn't very thorough; however, it might enable you to catch 
"low-hanging fruit" in a short time. Occasionally, it will also help you find extremely 
subtle vulnerabilities that are difficult to identify with code analysis. 
The CC5 strategy (See Table 4-8) provides a method for including black box and 
fuzz-testing in a more detailed application assessment. The procedure for performing 
this strategy is fairly simple. It requires only a functioning version of the application 
and identification of the entry points you want to target. Then you need to tailor the 
types of inputs you generate from your fuzz-testing tool or manually iterate through 
a smaller set of inputs. For example, if you're auditing a Web server, and the entry 
point is a TCP port 80 connection, you probably want to use an HTTP protocol fuzzer. 
You might have additional knowledge of the implementation that enables you to 
further alter your inputs and improve your chances of successful hits. Of course, 
nonstandard or proprietary protocols or file formats might require far more effort in 
generating a fuzzing tool. Luckily, you can simplify this task to some degree by using 
frameworks such as SPIKE, discussed later in "Fuzz-Testing Tools(? [????.])." 
Table 4-8. CC5: Trace Black Box Hits 
Start point 
Data entry points 
End point 
Security vulnerabilities (open-ended) 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
130 
Table 4-8. CC5: Trace Black Box Hits 
Start point 
Data entry points 
Trace method 
Forward, control-flow sensitive, data-flow sensitive 
Goal 
Trace an input path with an issue identified via black box (or 
fuzz) input testing. 
Difficulty 
Moderate 
Speed 
Fast 
Comprehension 
impact 
Medium 
Abstraction 
Basic implementation through design 
Strengths 
Traces some form of known issue 
Easy to stay on track 
Least mentally taxing of the code comprehension strategies 
Weaknesses 
Ignores many potential paths based on limitations of the 
testing approach 
A large number of false-positives can result in a huge waste of 
time 
Note 
Ideally, black box analysis should be part of the QA process. However, the QA process 
might not be broad enough to address the true range of potentially malicious input. 
So you should use any available QA testing harnesses but alter the input beyond the 
parameters they already check. 
The "Fault Injection" chapter of The Shellcoder's Handbook (Wiley, 2004(? [????.])) 
covers black box testing techniques extensively. It outlines a number of useful input 
generation methods, summarized in the following list: 
Manual generation (black boxing) This method involves manually adding input 
data that you intend to test for. Often it produces the most useful and targeted 
results. 
Automated generation (fuzzing) This method is good for testing products by 
using standard protocols, but bear in mind that it often neglects to account for 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
131 
extensions offered by the application you're examining. This method is often 
useful in conjunction with manual generation; you can automatically test the 
standard protocol, and then use manual generation for extensions or 
discrepancies identified in other components of the review. Automated 
generation can still be used with nonstandard protocols, but it requires a 
framework such as SPIKE for automated generation. 
Live capture This method allows input to be altered (or mutated) in an existing 
communication. It's particularly useful with state-based protocols because 
you can ignore a lot of required session setup and focus on vulnerabilities in 
later exchanges. 
Candidate Point Strategies 
Candidate point (CP) strategies are one of the fastest ways of identifying the most 
common classes of vulnerabilities. These strategies focus on identifying idioms and 
structured code patterns commonly associated with software vulnerabilities. The 
reviewer can then back-trace from these candidate points to find pathways allowing 
access from untrusted input. The simplicity of this approach makes candidate point 
strategies the basis for most automated code analysis. Of course, the disadvantage is 
that these strategies don't encourage a strong understanding of the code and ignore 
vulnerabilities that don't fit the rather limited candidate point definitions. 
General Candidate Point Approach 
The CP1 strategy (see Table 4-9) is almost the opposite of a code comprehension 
strategy. You start with the lowest-level routines that grant access to application 
assets or could harbor a vulnerability. This process might involve using automated 
tools to discover potentially unsafe code constructs or just a simple text search based 
on your existing knowledge of the application and potential vulnerabilities. You then 
trace backward through the code to see whether these routines expose any 
vulnerabilities accessible from an application entry point. 
Table 4-9. CP1: General Candidate Point Approach 
Start point 
Potential vulnerabilities 
End point 
Any form of user-malleable input 
Tracing method 
Backward, control-flow sensitive, data-flow sensitive 
Goal 
Given a list of potential vulnerabilities, determine whether they 
are exploitable 
Difficulty 
Easy to moderate 
Speed 
Medium 
Comprehension 
Low 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
132 
Table 4-9. CP1: General Candidate Point Approach 
Start point 
Potential vulnerabilities 
impact 
Abstraction 
Basic implementation through complex implementation 
Strengths 
Good coverage for known vulnerability classes 
Isn't too mentally taxing 
Hard to go off track 
Weaknesses 
Biases the reviewer to confirming only a limited set of potential 
issues Comprehension impact is much lower than with code 
comprehension strategies 
The results are only as good as your candidate points 
For example, say you use an analysis tool that reports the following: 
util.c: Line 1293: sprintf() used on a stack buffer 
You would attempt to verify whether it's really a bug. The function might look 
something like this: 
int construct_email(char *name, char *domain) 
{ 
    char buf[1024]; 
    sprintf(buf, "%s@%s", name, domain); 
    ... do more stuff here ... 
} 
You can't determine whether this bug is exploitable until you verify that you can 
control either the name or domain argument to this function, and that those strings can 
be long enough to overflow buf. So you need to check each instance in which 
construct_email() is called to verify whether it's vulnerable. This verification 
approach is actually fairly quick, but it has a number of drawbacks. Mainly, it's an 
incomplete approach; it improves your familiarity with the application, but it doesn't 
increase your understanding of how the application works. Instead, you must rely on 
assumptions of what constitutes a vulnerability, and these assumptions might not 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
133 
reflect the code accurately. Therefore, using only this approach can cause you to miss 
more complex vulnerabilities or even simple vulnerabilities that don't fit strict 
classifications. 
Automated Source Analysis Tool 
The CP2 strategy (see Table 4-10) can be used to generate candidate points, as 