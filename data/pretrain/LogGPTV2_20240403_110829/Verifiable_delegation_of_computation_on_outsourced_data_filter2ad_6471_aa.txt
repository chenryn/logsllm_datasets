title:Verifiable delegation of computation on outsourced data
author:Michael Backes and
Dario Fiore and
Raphael M. Reischuk
Veriﬁable Delegation of Computation on Outsourced Data
Michael Backes
Saarland University, MPI-SWS
Dario Fiore
MPI-SWS
Saarbrücken, Germany
Saarbrücken, Germany
Raphael M. Reischuk
Saarland University
Saarbrücken, Germany
ABSTRACT
We address the problem in which a client stores a large
amount of data with an untrusted server in such a way that,
at any moment, the client can ask the server to compute
a function on some portion of its outsourced data. In this
scenario, the client must be able to eﬃciently verify the cor-
rectness of the result despite no longer knowing the inputs of
the delegated computation, it must be able to keep adding
elements to its remote storage, and it does not have to ﬁx in
advance (i.e., at data outsourcing time) the functions that
it will delegate. Even more ambitiously, clients should be
able to verify in time independent of the input-size – a very
appealing property for computations over huge amounts of
data.
In this work we propose novel cryptographic techniques
that solve the above problem for the class of computations
of quadratic polynomials over a large number of variables.
This class covers a wide range of signiﬁcant arithmetic com-
putations – notably, many important statistics. To conﬁrm
the eﬃciency of our solution, we show encouraging perfor-
mance results, e.g., correctness proofs have size below 1 kB
and are veriﬁable by clients in less than 10 milliseconds.
Categories and Subject Descriptors
K.6.5 [Management of Computing and Information
Systems]: Security and Protection
Keywords
Veriﬁable Delegation of Computation; CloudComputing; Se-
cure Data Outsourcing; Homomorphic MACs; Amortized
Closed-Form Eﬃcient PRF
INTRODUCTION
1.
Given the emergence of cloud computing (an infrastructure
where clients or businesses lease computing and storage re-
sources from powerful service providers), it is of critical im-
portance to provide integrity guarantees for outsourced data
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’13, November 4–8, 2013, Berlin, Germany.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2477-9/13/11 ...$15.00.
http://dx.doi.org/10.1145/2508859.2516681.
management. Consider for example the following scenario.
A client has a collection of a large (potentially unbounded)
amount of data D = D1, D2, D3, . . ., for instance, environ-
mental data such as air pollution levels at ﬁxed time intervals
(e.g., every hour), and it may wish to compute statistics on
such data. If the client’s memory is not large enough to store
the entire data, the client might consider relying on a cloud
service and storing the data on a remote server S. Other
signiﬁcant examples of this scenario include arbitrary ﬁles
at remote storage systems, as well as endless data streams
such as ﬁnancial data (e.g., price ﬁxing data from the stock
markets, ﬁnancial ﬁgures and revenues of companies), exper-
imental data (e.g., genetic data, laboratory measurements),
and further environmental data (e.g., surface weather ob-
servations).
In this scenario, we hence have a client who
incrementally sends D to a server S, the server stores D,
and at certain points in time the client asks S to compute
a function on (a portion of) the currently outsourced data.
We stress that the data D and its size cannot be ﬁxed in ad-
vance as the client may need to add additional data to the
outsourced storage. Analogously, the client might not know
in advance what functions it will apply on the outsourced
data (e.g., it may wish to compute several statistics).
However, if the server is untrusted (i.e., it is malicious or
becomes prey to an external attack), how can the client ver-
ify that the results provided by the server are correct? This
question naturally leads to two important requirements: (1)
security, meaning that the server should be able to “prove”
the correctness of the delegated computation for some func-
tion f ; and (2) eﬃciency, meaning that the client should be
able to check the proof by requiring signiﬁcantly fewer re-
sources than those that are needed to compute f (including
both computation and communication). Furthermore, if we
consider computations over very large sets of inputs (e.g.,
statistics on huge data sets), we want to be more ambitious
and envision the achievement of (3) input-independent eﬃ-
ciency, meaning that verifying the correctness of a compu-
tation f (D1, . . . , Dn) requires time independent of n. More-
over, two further requirements are crucial in this setting: (4)
unbounded storage, meaning that the size of the outsourced
data should not be ﬁxed a-priori, i.e., clients should be able
to outsource any (possibly growing) amount of data; and (5)
function-independence, meaning that a client should be able
to outsource its data without having to know in advance the
functions that it will delegate later.
Relation with Verifiable Computation. The problem
of securely and eﬃciently outsourcing the computation of a
function f to a remote server has been the subject of many
863works in the so-called ﬁeld of veriﬁable computation. Most
of these works achieve the goals of security (1) and eﬃciency
(2), but they inevitably fail in achieving requirements (3)–
(5). Roughly speaking, the issue is that most existing work
requires the client to know (i.e., to store a local copy of) the
input D for the veriﬁcation of the delegated function (e.g.,
in SNARG-based approaches [10, 25] and in signatures of
correct computation [40]), or, otherwise, to send D to the
server all at once (rather than sending it over time) and to
keep a small local state which would not allow to append
additional data at a later time (e.g., in [44, 22]). Perhaps
more critically, many of the existing solutions in this area re-
quire the delegator to run in time proportional to the input
size n of the delegated function, e.g., in time poly(n). In the
various existing protocols, these limitations arise for diﬀer-
ent reasons (see Section 1.1 for a more detailed discussion).
However, even if veriﬁcation in these works is more eﬃcient
than running f , we think that, for computations over huge
data sets, a poly(n) overhead is still unacceptably high.
The only approach that comes close to achieving require-
ments (1)–(5) is the work by Chung et al. on memory delega-
tion [20]. The authors propose a scheme based on techniques
from [29] which exploit the power of the PCP theorem [6].
With this scheme, a client can delegate a broad class of com-
putations over its outsourced memory fulﬁlling the require-
ments from above (except for veriﬁcation eﬃciency, which
requires time log n, instead of constant time). While provid-
ing a satisfying solution in theory, this approach suﬀers from
the usual impracticality issues of general-purpose PCP tech-
niques and hence does not lead to truly practical solutions
to the problem.
In this work, we address the prob-
Our Contribution.
lem of veriﬁable delegation of computations on (growing)
outsourced data. Our main contribution is the ﬁrst practi-
cal protocol that achieves all ﬁve of the requirements stated
before. Namely, a client can (continuously) store a large
amount of data D = D1, D2, D3 . . . with the server, and
then, at certain points in time,
it can request the com-
putation of a function f on (a portion of) the outsourced
data, e.g., v = f (Di1 , . . . , Din ). Using our protocol, the
server sends to the client a short piece of information vouch-
ing for the correctness of v. The protocol achieves input-
independent eﬃciency in the amortized model: after a sin-
gle precomputation with cost |f |, the client can verify every
subsequent evaluation of f in constant time, i.e., regardless
of the input size n. Moreover, fulﬁlling properties (4)–(5),
we have that data outsourcing and function delegation are
completely decoupled, i.e., the client can continuously add
elements to the remote storage, and the delegated functions
do not have to be ﬁxed a priori. This means that the cost
of outsourcing the data can be, in fact, excluded from the
delegation; think for instance of incrementally outsourcing a
large data stream during an entire year, and then computing
statistics on the data at the end of the year.
Our solution works for computations over integers in the
ring Zp (where p is a large prime of roughly 2λ bits, for a
security parameter λ), and supports the evaluation of arith-
metic circuits of degree up to 2. This restricted class of
computations is enough to capture a wide range of signiﬁ-
cant arithmetic computations, such as meaningful statistics,
including counting, summation, (weighted) average, arith-
metic mean, standard deviation, variance, covariance, weighted
variance with constant weights, quadratic mean (aka root-
mean square – RMS), mean squared error (MSE), the Pear-
son product-moment correlation coeﬃcient, the coeﬃcient
of determination (R2), and the least squares ﬁt of a data set
{(xi, vi)}n
i=1 (in the case when the xi are universal constants,
e.g., days of the year)1.
Our key technical contribution is the introduction of ho-
momorphic MACs with eﬃcient veriﬁcation. This crypto-
graphic primitive extends homomorphic message authenti-
cators [27] by adding a crucial eﬃciency property for the
veriﬁcation algorithm. We propose a ﬁrst realization of ho-
momorphic MACs with eﬃcient veriﬁcation (see Section 1.2
for an overview of our techniques), and we prove its secu-
rity under the Decision Linear assumption [11]. Using the
above construction we build an eﬃcient protocol that can
be implemented using bilinear pairings.
To demonstrate the practicality of our solution, we eval-
uate the concrete operations that have to be performed by
the client and the server, as well as the bandwidth overhead
introduced by the protocol for transferring the proofs.
If
we consider 80 bits of security and an implementation of
symmetric pairings [35] on a standard desktop machine, we
observe the following costs (see Table 2 for the 128-bit case):
For outsourcing a data item Di, the client needs to perform
a single modular exponentiation in 0.24ms. This operation
yields a very short authentication tag of size 0.08kB, which
is sent to the server along with Di. For the veriﬁcation of
a computation result v, the client receives a proof σv of size
0.21kB from the server, and can check this proof by com-
puting one pairing and one multi-exponentiation in 1.06ms.
As we mentioned before, we achieve input-independent
eﬃciency in an amortized sense. So, the above veriﬁcation
costs are obtained after the precomputation of some concise
information ωf related to the delegated function f . Pre-
computing ωf takes the same time as computing f (with
almost no additional overhead!), it does not require knowl-
edge of the input data, and ωf can be re-used an unbounded
number of times to verify several evaluations of f on many
diﬀerent outsourced data sets. To generate the proof σv re-
lated to the evaluation of a function f , the server has to
run f with an additional, yet constant, overhead – derived
from replacing additions in f with a group operation, and
replacing multiplications with a pairing. Although our solu-
tion can still not capture general-purpose computations, the
above performance evaluation shows that for our case of in-
terest we achieve results that are encouraging for a practical
deployment of this protocol.
1.1 Related Work
Memory Delegation. The work of Chung et al. [20] on
memory delegation and streaming delegation is the closest
one to the model considered in our work. In memory del-
egation the client uploads his memory to the server (in an
oﬄine phase), and it can later ask the server to update the
outsourced memory and to compute a function f on its entire
memory (in an online phase). In streaming delegation the
memory can be updated only by appending elements. The
main advantages of the work of Chung et al. over our results
are that: (i) the client can change values in the outsourced
memory, (ii) they provide solutions for more expressive com-
putations (i.e., a 4-round protocol for arbitrary poly-time
programs). However, their solutions also suﬀer some disad-
1The least squares ﬁt for this case can indeed be computed
using a linear function [13].
864vantages. First, they require the client to be stateful (in our
solution the client keeps only a ﬁxed secret key). Second, in
streaming delegation, the size N of the stream has to be a-
priori bounded. Such a bound also aﬀects the client’s mem-
ory since it requires a local storage size of approximately
log N at the client, meaning that N cannot be chosen arbi-
trarily long, and thus the stream cannot be endless. Also,
in their solutions, the client still runs in time polylog(n) in
the online phase, where n is the size of the entire memory.
Authenticated Data Structures. A line of research
which addresses a problem closely related to the one consid-
ered in this paper is the existing work on authenticated data
structures [39, 50]. This area considers a setting in which
clients want to securely delegate certain operations on data
structures that are stored at untrusted remote servers. Ex-
isting work addresses both static settings and dynamic set-
tings (where data structures can be updated), and it mostly
focuses on speciﬁc data structure operations, such as range
search queries over databases [31, 36], authenticated dictio-
naries [21, 41, 30], and set operations over a dynamic col-
lection of sets [42]. However, none of the works in this area