(x;y), with each pair (s; t) and each link
(x; y) 2 E, that denotes the fraction of the trafﬁc ﬂow from s
to t over the link (x; y). The problem of ﬁnding the maximum
bandwidth expansion factor for network-layer TBE is deﬁned
by following linear program:
max m
∑
∑
(LP)
s.t.
y:(x;y)2E
8 100). The
time to solve the problem grows rapidly with the network
size and becomes unrealistic for real-time operations; e.g.,
few thousand seconds in a network of size R = 196.
(cid:15) Small Mnetwork compared to Mideal: In practice, the value
of Mnetwork is small compared to Mideal. As we will see
in detail in Section VII, Mnetwork is typically in the range
of 2 – 3 and it is likely that Mideal = rg=rd is 5 – 10 times
larger than Mnetwork.
We solve the TBE scalability problem by greedy algorithm
(Section V-A) and the small Mnetwork problem by randomized
sequential TBE (Section V-B).
Algorithm 1 Greedy algorithm for TBE
1: Inputs: Set of ingress/egress pairs (s; t) crossing target link,
Number of ﬂows on each ingress/egress pair n(s; t),
2:
Desired bandwidth expansion factor m,
3:
Network topology graph G = (V; E), and
4:
Residual link bandwidth b(i; j) for (i; j) 2 E.
5:
6: while (9(s; t) that has not yet selected) do
7:
8:
= (V; E n E
Select ingress/egress pair (s; t) at random w/o replacement.
Calculate the available network graph G
=flinks w/ available bandwidth (cid:21) m(cid:1) n(s; t)(cid:1) rdg.
′
where E
Calculate new route R(s; t) in G
if R(s; t) ̸= N U LL then
9:
10:
11:
12: Output: New routes R(s; t) for all ingress/egress pairs (s; t).
Move all ﬂows in (s; t) to R(s; t).
);
′
′
.
′
We use a binary search procedure over the value of m
in Algorithm 1 and obtain the estimate of the maximum
bandwidth expansion factor cMnetwork and the corresponding
between Mnetwork and cMnetwork is negligible in practice.
routing solution. We show in Section VII that the difference
B. Randomized Sequential TBE
To solve the problem of small value of Mnetwork, we use
a randomized sequential TBE approach. That is, we test only
a subset RT BE = Mnetwork=Mideal of senders at each TBE
round so that a subset of senders can have feasible routes that
provide Mideal-times bandwidth expansion. We then repeat
this process until most of the senders are tested. If the obtained
Mnetwork from LP is larger than or equal to Mideal, no more
than a single TBE is required.
Random sender selection. We randomly select a fraction
RT BE of senders in each ingress/egress pair and reroute them
using the solution of LP. The obtained routing solution pro-
vides the selected senders Mideal-times expanded bandwidth.
Since we randomly sample the senders at every TBE, an
adversary cannot anticipate when a particular bot will be
tested. The number of TBE rounds that needs to be performed
to test the majority (e.g., 90%) of the senders depends on the
fraction RT BE.
A. Greedy algorithm for TBE Scaling
VI. RATE-CHANGE MEASUREMENT TESTS
As a solution to the scalability problem, we propose a
simple greedy routing algorithm that runs much faster than
solving LP with off-the-shelf solvers. One can also use other
efﬁcient ways of solving such problems in general; e.g., [13],
[47]. The greedy algorithm presented here is one possible way
for calculating an approximate solution.
The greedy algorithm takes as inputs the set of ingress/
egress pairs and the number of their ﬂows that cross the
target link, the desired bandwidth expansion factor m, and the
network graph G = (V; E) and the residual link bandwidth
b(x; y) for each link (x; y) 2 E. Then it outputs a feasible
ingress/egress pairs (s; t).
routing solution R(s; t) for all
The pseudocode of this algorithm is given in the following
Algorithm 1.
4Since we use the undirected graph G, for any link (x; y) 2 E we set
b(x; y) = b(y; x) but we activate only one them for the constraints (3).
In this section, we focus on the rate-change measurement
test. As previously mentioned, there are two key challenges
in designing the test. First, stateful per-sender rate monitoring
could be expensive and induce high control overhead at the
SDN controller. Second, the robustness can be undermined by
real world TCP effects; e.g., prevalence of short-lived TCP
ﬂows or reaction to RTT changes.
A. Sketch-based Per-Sender Rate Change Detection
As mentioned, SPIFFY requires rate change detection for
all senders that cross the target link. This raises concerns about
the computation and memory complexity of such “stateful”
operations. Another concern is that when the real-time per-
sender rate measurements are reported back to the SDN
controller the control channel could be easily congested due
to the large volume of control messages.
6
SPIFFY can address these challenges by utilizing sketch-
based measurements [30]. Sketch is a memory-efﬁcient data
structure that stores summaries of streaming data. In particular,
a simpliﬁed variant of sketch-based rate change detection [33]
can be used for efﬁciently and quickly detecting per-sender rate
changes. With the sketch-based rate change detection, the edge
switches report only the measurement summary to the SDN
controller, such as list of bot IPs, and signiﬁcantly minimize
the control channel overhead.
Sketch-based rate-change measurement: We use the
original sketch-based change detection mechanism by Krish-
namurthy et al. [33] for measuring per-sender rate changes.
In fact, SPIFFY needs a simpler version of the original
sketch-based change detection since it measures with the
granularity of a sender (i.e., source IP), which is coarser than
the granularity of a ﬂow. The three basic components are
the sketch module, the forecasting module, and the change-
detection module [33]:
1) The sketch module creates a sketch; i.e., a H (cid:2) K table
of SRAM memory. When a packet arrives at an edge
switch, the source IP is fed into the H independent hash
functions. Based on the mod K of the H hash outputs,
H registers in the H rows are updated by the packet size
u. By updating all packets in a time interval t, we obtain
a sketch S(t) at the end of the interval t.
2) The forecasting module uses the observed sketches in the
< t) to compute the forecast sketch
′
past intervals S(t
Sf (t).
3) The change-detection module constructs the forecast error
sketch Se(t) = S(t)(cid:0)Sf (t). For each sender’s IPsrc, this
module calculates the forecast error.
Estimated measurement complexity: We analyze the esti-
mated memory size and the sketch computations. The required
memory size is determined by the number of independent hash
functions H and the size of sketch bins K for each hash
function. For a real Internet trace dataset with more than 60
million ﬂows, H = 5 and K = 32K produce very accurate
rate-change measurement; e.g., 95% accuracy for top 1000
ﬂows with the maximum rate changes [33]. Our rate-change
measurement will also be accurate with these parameters, since
each edge switch will not need to measure more than 60
million senders in most cases. When we assume 3 bytes for
each register in the sketch memory, each edge switch requires
480 KB SRAM memory space.
′
) (t
Sketch-based measurement requires H hash operations for
individual incoming packets. Also, each SRAM access requires
few tens of nano seconds. However, since the hash operations
and SRAM access can be implemented in parallel in hardware,
these per-packet operations can be very efﬁciently imple-
mented and thus do not affect the data plane throughput [58].
At every rate-change detection interval, each edge switch
calculates the forecast sketches Sf (t) and the forecast error
sketches Se(t). These and the ﬁnal rate-change calculation
requires a computational overhead of about 1.91 seconds,
when for example H = 5, K = 64K, and 10 million ﬂows
are monitored [33]. Our per-sender rate-change measurement
would require much shorter (e.g., ≪ 1 sec) time for the
computation at each edge switch since today’s commodity
CPUs are at least 3-4 times faster than the one used (900
7
Fig. 5: Simulation setup.
MHz CPU clock speed) more than a decade ago [33].
Bot-detection summary reports: Instead of reporting the
rate-changes of all senders, each edge switch can report only
the subset of senders that are determined as bots (or legitimate
senders). Thus, the aggregate bandwidth for control channel
can be limited to a few Mbps or less; e.g., only 4 MB
data transfer is required even when 1 million bot IPs are
reported. Notice that the reports are made only when the TBE
is performed.
B. Bot Detection Robustness to TCP Effects
The robustness of SPIFFY’s bot detection relies on the
prompt and fast rate increase of legitimate senders when TBE
is performed. The rate increase is mainly determined by TCP
operations at the senders since they control the maximum ﬂow
rates at a given time. However, achieving robust bot detection
can be challenging due to the two following TCP effects: (1)
short-lived ﬂows (e.g., few packets in a ﬂow) in the Internet
terminate before TCP increases their rates; (2) when TBE’s
route changes cause sudden increase of RTT values, TCP might
decrease the send rates by decreasing congestion windows
and/or causing spurious timeouts.
To achieve robust bot detection, our primary focus is to
maintain low false-positive rate because false-positive events
cause collateral damage to legitimate senders. In contrast,
false-negative rate (i.e., the rate in which bots are misidentiﬁed
as legitimate senders) is not a particularly useful metric since
SPIFFY allows false-negative events to happen for the cost-
detectability tradeoffs. For example, if an adversary is deter-
mined to remain undetected, she can make the false-negative
rate to be practically one at a highly increased attack cost.
Robustness to short TCP ﬂows: Unlike long-lived ﬂows,
short-lived ﬂows might not increase their rates in response to
TBE because they may not last long enough (e.g., few seconds)
when the bottlenecked bandwidth is expanded. Therefore,
when the majority of ﬂows are short-lived (as is the case of
today’s Internet trafﬁc), per-sender rate of legitimate senders
could be almost unchanged when TBE is performed, causing
false-positive events.
Here, we ﬁrst observe that the prevalence of short-lived
ﬂows does not affect the rate changes of senders that create
realistic trafﬁc with the mixture of short- and long-lived ﬂows.
Moreover, we show that SPIFFY can maintain false-positive
rate as low as 1% or less by exempting senders with per-
sender rates lower than minimum per-sender rate from the bot
detection process regardless of their rate-change ratios.
To test our claims, we perform simulations with a syn-
thetic web-trafﬁc generator. We use the ns2 simulator with
PackMime-HTTP web-trafﬁc generator to construct diverse
.........HTTP server100 Mbpstargeted link L1000 Mbps1 Mbps1 MbpsTBE rerouting  (M = 10)1000 nodes1000 nodesideal RTT = 100 msecHTTP serverHTTP server...HTTP clientHTTP clientHTTP clientFig. 6: An example per-sender rate change measurements
of randomly selected 100 legitimate senders with mean and
standard deviation when the bandwidth expansion factor
M = 10.
Fig. 7: Simulated per-ﬂow rates of ﬂows in realistic HTTP
web trafﬁc (a) before and (b) during TBE with bandwidth
expansion factor M = 10.
network environments and simulate accurate TCP operations
with realistic HTTP application trafﬁc demand [4], [19].
Approximately 70% of the synthetic web-trafﬁc ﬂows have
size smaller than a single IP packet’s maximum size (1,500
Bytes) while a small number of large ﬂows exist. We deter-
mine the queue size based on a rule-of-thumb practice; i.e.,
QueueSize = RT T (cid:2)C, where RT T is the average round-trip
time of the ﬂows crossing the link and C is the data rate of the
link [11]. For TBE, we assume that the bandwidth of the target
link is expanded by a factor of M = 10. As shown in Figure 5,
we simulate 1000 pairs of clients/servers exchanging HTTP
trafﬁc through a target network link. We set the ideal (i.e.,
when no trafﬁc on the path) round-trip time of 100 msec and
the application-layer data rate of 1000 Kbps for the purpose
of clear illustration.