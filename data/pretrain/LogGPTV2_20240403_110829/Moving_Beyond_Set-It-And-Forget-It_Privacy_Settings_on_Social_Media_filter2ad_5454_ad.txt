40
40
40
40
40
40
40
40
40
50
50
50
50
50
50
50
50
50
50
Definitely
keep sharing
Keep sharing Does not
matter
60
60
60
60
60
60
60
60
60
70
70
70
70
70
70
70
70
70
80
80
80
80
80
80
80
80
80
90
90
90
90
90
90
90
90
100
100
100
100
100
100
100
100
90
100
70
90
60
Stop sharing Definitely
80
stop sharing
100
No answer
Figure 4: Sharing preferences by friend category (cf. Table 1).
The first part of a name refers to the year before a post was
made, while the second refers to the year before the study.
the bottom panel considers only interaction in the year before the
post was made.
Figure 4 shows that participants were more likely to “definitely
keep sharing” posts with friends with whom they had a high degree
of recent interaction than those with whom they had not recently
interacted (Mann-Whitney U, p < 0.001). They reported definitely
wanting to keep sharing posts with 62.7% of friends with whom
they frequently interacted in the year preceding our study. While
recent interaction is indicative of a desire to definitely keep sharing,
the inverse is not true. Participants also wanted to “definitely keep
sharing” posts with 34.7% of the friends with whom they had no
visible interaction on Facebook in the year before the study.
We used our qualitative data to better understand the desire to
share posts in spite of no recent interaction. Often, the participant
identified the Facebook friend as a family member or close friend,
which implies that the level of visible interaction on Facebook is
an imperfect measure of real-world closeness. Additionally, partici-
pants sometimes anticipated that the content of a post would be
interesting to the friend. This mental model of friends’ interests is
not reflected in our interaction data. Two less frequent reasons for
sharing with friends in spite of no recent interaction are content-
centric. For instance, participants wanted to keep sharing posts
containing informative or humorous content with their friends re-
gardless of visible interaction. We note very similar reasons when
investigating prediction inaccuracies in Section 8.5.
Similarly, participants were more likely to definitely want to stop
sharing with friends they had not recently interacted with (8.3% of
the time) than those they interacted frequently with (3.5% of the
time) (Mann-Whitney U, p = 0.001). The similarity of these num-
bers underscores that while interaction is correlated with sharing
preferences, it is not sufficient on its own for prediction, as explored
in future sections.
7.2 Correlation with Time of Friendship
Recall that the median participant had twice as many Facebook
friends in 2018 as in 2012, substantially changing the meaning of
a “friends only” privacy setting. Surprisingly, we did not observe
significant differences in whether a participant wanted to share a
given post with a given friend based on whether or not they were
Facebook friends at the time the post was made. The “X-∗” plots
in Figure 4 depict this phenomenon. In other words, the time of
Facebook friendship is not only insufficient for retrospectively pre-
dicting whether a post should be shared with a given friend, it does
not even seem to be correlated. While we had initially hypothesized
that participants might not want to share past content with friends
they make in the future, our results do not support this hypothesis.
Instead, participants appear to be adding new Facebook friends
with the intention that these new friends can access past content.
8 PREDICTING PREFERENCES
Our ultimate goal is to enable users to efficiently maintain correct
privacy settings on years or decades of social media posts. The sheer
number of friend-post combinations for even light social media
users necessitates automated assistance for this task. To support the
use of machine learning models in such a subjective and important
setting, we leverage insights regarding preferences from Sections 6
and 7, designing models intended for use within the privacy domain
and the user assistance scenario.
8.1 Prediction Task
For the prediction task, our dataset consists of tuples (Xi , Yi), where
Xi is the feature vector and Yi is the desired audience change for
post i. We formulate the problem as a binary classification task
where Yi = 1 corresponds to limit sharing and Yi = −1 corresponds
to do not limit sharing. Our task is binary classification, since our
current focus is to help users find posts they wish to limit sharing,
based on a human-in-the-loop system, rather than building a fully
automatic post manager. By mapping our problem to binary classi-
fication, we can get a better separation on posts users specifically
wanted to limit sharing compared to do not limit sharing. After
prediction, we can sort posts by their likelihood of limit sharing to
show users posts in the predicted priority order.
The feature vector Xi includes variables capturing the survey
responses, including some user information, post statistics, content,
and audience. From the survey features, we have the age of the
account and the age of the participant as user information. We
include the survey responses either as one-hot encoding or binary
indicators for multiple-choice responses. Our post statistics features
are the following: the number of likes and comments, the content
type (e.g., text, link, image), whether another user is tagged, if
comments were edited, if the audience was changed earlier, the age
of the post, and the current privacy setting. We extract content-
level features from the text of posts through established NLP feature
extractors: Google News Word2Vec embedddings [55], Linguistic
Inquiry & Word Count (LIWC) categories [74], Google’s content
classification categories, and Google’s sentiment scores (i.e., positive
or negative sentiment) [30]. Our audience features include friend-
specific features: days since first and last communication, reaction
counts, wall words exchanged, and how many wall posts the user
Moving Beyond Set-It-And-Forget-It Privacy Settings on Social Media
CCS ’19, November 11–15, 2019, London, United Kingdom
initiated to a friend. We include more details on features in Table 6
in the appendix.
To perform binary classification, we compare several established
supervised learning algorithms: Decision Trees (DT), Logistic Re-
gression (LR), Support Vector Machines (SVM), Random Forests (RF)
using scikit-learn [61], and XGBoost (XGB) [79]. We also include
Deep Neural Networks (DNN) using scikit-learn and the Adam op-
timizer, although DNNs tend not to learn well from small datasets
like ours. For our DNN, we used 3 hidden layers with 100, 50, and
20 nodes with RELU activations and a softmax activation for the
output layer. We report results only on the best performing clas-
sifiers, while leaving results for other classifiers in the appendix
(Figures 10, 11, and 12). In the absence of any preexisting classifier,
we propose two baseline models. The first is a random classifier
(Random), where we randomly show posts to users. The random
classifier is used when there is no information for predicting if a
post will be selected for limit sharing. We also considered a more
reasonable strawman baseline (Interaction) that does not require
machine learning, but only considers the level of interaction be-
tween the user and their friend. This baseline predicts limit sharing
for friends with low levels of interaction. We chose these baseline
classifiers because, to the best of our knowledge, no prior work
has attempted to predict posts and friend-post pairs for which to
retrospectively limit sharing.
8.2 Dataset Description
We consider two datasets for predicting privacy preferences. In the
post dataset, we aim to predict whether a user should decrease the
audience of a post. In the friend-post dataset, we aim to predict
whether a user should remove a specific friend’s access to the post.
For both datasets, we focus on the binary classification task of
predicting whether or not a user wishes to limit sharing.
Post Dataset. In the post dataset there are 389 posts for which
users specified labels. There are three labels in the dataset: less,
same, and more audience. Since we focus on finding posts for which
the user wishes to decrease the audience, we treat less audience as
limit sharing and the other two as do not limit sharing. We have
the following label distribution: 13.9% for less, 74.5% for same, and
11.6% for more audience. For binary classification, we have: 13.9%
for limit sharing and 86.1% for do not limit sharing.
Friend-Post Dataset. The friend-post dataset contains the same
posts as the post dataset. However, participants specified audience-
change labels for specific friends (up to 6 friends per post). This
dataset contains 2,336 total labels, after removing friend-post pairs
where no answer was given. This dataset contains 3 possible deci-
sions for privacy preference: stop sharing, doesn’t matter, and keep
sharing. We map this to a binary classification task where stop shar-
ing corresponds to limit sharing, and the other two correspond to
do not limit sharing. For friend-post pairs, we have the following
label distribution: 6.4% for stop sharing, 36.4% for doesn’t matter,
and 57.2% for keep sharing. For binary classification, we have: 6.4%
for limit sharing and 93.6% for do not limit sharing.
Both datasets are highly skewed toward do not limit sharing. This
can highly bias our results towards predicting do not limit sharing
for every post. We counteract this issue by focusing on the binary
classification task, since we wish to discriminate posts that are limit
sharing from all other posts.
8.3 Experimental Setup
In our experiments, we perform 5-fold cross validation and report
averaged results across 5 testing folds. Since, we are focusing on
finding posts where the user may wish to decrease the audience
size, we order examples in the test data by the probability of being
Yi = 1 (limit sharing) and assess their precision and recall. This
is a typical evaluation setup for binary classification where one
label (limit sharing) is more important than the other (do not limit
sharing). Since we can vary the number of posts that we predict
as limit sharing, we report on precision@k, the precision after
predicting the top k results as positive. Each value of k is considered
a potential cutoff, where all examples ranked greater than or equal
to k are classified as positive and the rest are classified as negative.
We compute precision as T P/(T P + F P), where T P is the number
of true positive examples (actual label positive, predicted label
positive) and F P is the number of false positive examples (actual
label negative, predicted label positive). Thus, precision@k is the
proportion of correctly classified positive examples for all examples
above the cutoff k. In other words, the precision@k is the binary
precision when only considering the top k examples. Precision@k
curves allow us to see how accurately we are predicting our desired
label after showing to users the most likely posts for decreasing
the size of the audience. We also compute recall as T P/(T P + F N),
where F N are the false negative examples (actual label positive,
predicted label negative). We report precision-recall curves to show
the tradeoff between showing a larger number of posts that need
users’ attention and how accurately we can uncover such posts.
Ordering examples by the probability of correctness also maps
well onto an implementation that mimics the “people you may
know” feature employed by Facebook and other social networks.
Prioritizing the suggestions that are most likely to be correct maxi-
mizes the utility of the tool in an environment constrained by user
attention. Furthermore, since it is unlikely that a user will be willing
to spend the time to go over all suggestions, our intention is to
minimize the number of false predictions rather than ensure that
all posts needing correction are (eventually) suggested.
8.4 Results
We present the precision@k and precision-recall curves averaged
over the five folds. We also analyze the features for predicting
friend-post pair privacy settings.
Friend-Post Dataset Prediction. We study whether it is possi-
8.4.1
ble to predict if a user wants to limit sharing for a post with a specific
friend. Thus, we include features about the inferred relationship
between the user and the friend in addition to other features.
Figure 5a shows the precision@k curves for predicting privacy
preferences in the friend-post dataset. Here, the ensemble clas-
sifiers Random Forest and XGBoost give the best precision, with
XGBoost performing better for very low K. Since the underlying
distribution of limit sharing for this dataset is 6.4%, a cutoff at that
percentage would be reasonable in a deployed system. This cor-
responds to predicting the top 30 results per test fold where the
precision@30 is 0.519 for Random Forest. Additionally, we include
CCS ’19, November 11–15, 2019, London, United Kingdom
Mondal et al.
(a) Average Precision@k on full test set
(b) Average Precision@k on test without “doesn’t matter”
Figure 5: Precision@k curves for the friend-post dataset, comparing the original dataset and removing the label doesn’t matter
from the testing folds, using the best classifiers (XGBoost and Random Forest). The preference for doesn’t matter is most of
the interference for the precision@k curves.
another precision@k curve for XGBoost (XGBoost auto), where we
remove features that are not automatically collected, such as survey
responses. We see that even after removing these features, we can
get very close precision@k curves to XGBoost on the full set of
features. This shows promise in building a system, where we only
need to know friend-post pair sharing preferences so that we can
get more labels. Features can be collected automatically.
We analyze whether doesn’t matter decisions contribute to most
of the false positives in top positions. Figure 5b shows that after
removing those examples from the test set, the precision becomes
higher for all k and stays 1.0 for more top examples (6 vs. 3), com-