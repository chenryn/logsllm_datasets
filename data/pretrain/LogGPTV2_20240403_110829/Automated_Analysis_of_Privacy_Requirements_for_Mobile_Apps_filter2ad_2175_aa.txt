title:Automated Analysis of Privacy Requirements for Mobile Apps
author:Sebastian Zimmeck and
Ziqi Wang and
Lieyong Zou and
Roger Iyengar and
Bin Liu and
Florian Schaub and
Shomir Wilson and
Norman M. Sadeh and
Steven M. Bellovin and
Joel R. Reidenberg
Automated Analysis of Privacy Requirements
for Mobile Apps
Sebastian Zimmeck∗ ◦, Ziqi Wang∗, Lieyong Zou∗, Roger Iyengar† ◦, Bin Liu∗,
Florian Schaub‡ ◦, Shomir Wilson§ ◦, Norman Sadeh∗, Steven M. Bellovin¶ and Joel Reidenbergk
∗School of Computer Science, Carnegie Mellon University, PI:EMAIL, PI:EMAIL
†Department of Computer Science and Engineering, Washington University in St. Louis
§Department of Electrical Engineering & Computing Systems, University of Cincinnati
‡School of Information, University of Michigan
¶Department of Computer Science, Columbia University
kSchool of Law, Fordham University
Abstract—Mobile apps have to satisfy various privacy require-
ments. Notably, app publishers are often obligated to provide a
privacy policy and notify users of their apps’ privacy practices.
But how can a user tell whether an app behaves as its policy
promises? In this study we introduce a scalable system to help
analyze and predict Android apps’ compliance with privacy
requirements. We discuss how we customized our system in a
collaboration with the California Ofﬁce of the Attorney General.
Beyond its use by regulators and activists our system is also meant
to assist app publishers and app store owners in their internal
assessments of privacy requirement compliance.
Our analysis of 17,991 free Android apps shows the viability
of combining machine learning-based privacy policy analysis with
static code analysis of apps. Results suggest that 71% of apps that
lack a privacy policy should have one. Also, for 9,050 apps that
have a policy, we ﬁnd many instances of potential inconsistencies
between what the app policy seems to state and what the code
of the app appears to do. In particular, as many as 41% of
these apps could be collecting location information and 17%
could be sharing such with third parties without disclosing so in
their policies. Overall, each app exhibits a mean of 1.83 potential
privacy requirement inconsistencies.
I.
INTRODUCTION
“We do not ask for, track, or access any location-speciﬁc
information [...].” This is what Snapchat’s privacy policy
stated.1 However,
its Android app transmitted Wi-Fi- and
cell-based location data from users’ devices to analytics
service providers. These discrepancies remained undetected
before they eventually surfaced when a researcher examined
◦Part of this work was conducted while Sebastian Zimmeck was a PhD student at
Columbia University working in Prof. Sadeh’s group at Carnegie Mellon University.
Roger Iyengar, Florian Schaub, and Shomir Wilson were all in Prof. Sadeh’s group at
Carnegie Mellon University while involved in this research project, Roger Iyengar as an
NSF REU undergraduate student, Florian Schaub as a post-doctoral fellow, and Shomir
Wilson as a project scientist.
1Complaint In the Matter of Snapchat, Inc. (December 31, 2014).
Permission  to  freely  reproduce  all  or  part  of  this  paper  for  noncommercial  purposes 
is  granted  provided  that  copies  bear  this  notice  and  the  full  citation  on  the  ﬁrst  page. 
Reproduction  for  commercial  purposes  is  strictly  prohibited  without  the  prior  written 
consent  of  the  Internet  Society,  the  ﬁrst-named  author  (for  reproduction  of  an  entire 
paper  only),  and  the  author’s  employer  if  the  paper  was  prepared  within  the  scope  of 
employment.
NDSS ’17, 26 February - 1 March 2017, San Diego, CA, USA
Copyright 2017 Internet Society, ISBN 1-891562-46-0
http://dx.doi.org/10.14722/ndss.2017.23034
Snapchat’s data deletion mechanism. His report was picked
up by the Electronic Privacy Information Center and brought
to the attention of the Federal Trade Commission (FTC),
which launched a formal investigation requiring Snapchat to
implement a comprehensive privacy program.2
The case of Snapchat
illustrates that mobile apps are
often non-compliant with privacy requirements. However, any
inconsistencies can have real consequences as they may lead
to enforcement actions by the FTC and other regulators. This
is especially true if discrepancies continue to exist for many
years, which was the case for Yelp’s collection of childrens’ in-
formation.3 These ﬁndings not only demonstrate that regulators
could beneﬁt from a system that helps them identify potential
privacy requirement inconsistencies, but also that it would
be a useful tool for companies in the software development
process. This would be valuable because researchers found
that privacy violations often appear to be based on developers’
difﬁculties in understanding privacy requirements [7] rather
than on malicious intentions. Thus, for example, tools that
automatically detect and describe third-party data collection
practices may be helpful for developers [7]. Consequently, it is
a major motivation of our work to help companies identify red
ﬂags before they develop into serious and contentious privacy
problems.
On various occasions, the FTC, which is responsible for
regulating consumer privacy on the federal level, expressed
dissatisfaction with the current state of apps’ privacy compli-
ance. Three times it manually surveyed childrens’ apps [28],
[29], [33] and concluded that the “results of the survey are
disappointing” [29]. Deviating from mandatory provisions,
many publishers did not disclose what types of data they
collect, how they make use of the data, and with whom the
data is shared [29]. A similar examination of 121 shopping
apps revealed that many privacy policies are vague and fail to
convey how apps actually handle consumers’ data [32]. Given
that the FTC limited its investigations to small samples of apps,
a presumably large number of discrepancies between apps and
their privacy policies remain undetected.
In this study we are presenting a privacy analysis sys-
tem for Android that checks data practices of apps against
2Decision and Order In the Matter of Snapchat, Inc. (December 31, 2014).
3United States of America v. Yelp, Inc. (September 17, 2014).
privacy requirements derived from their privacy policies and
selected laws. Our work enables app publishers to identify
potentially privacy-invasive practices in their apps before they
are published. Moreover, our work can also aid governmental
regulators, such as the FTC, to achieve a systematic enforce-
ment of privacy laws on a large scale. App store owners,
researchers, and privacy advocates alike might also derive
value from our study. Our main contribution consists of the
novel combination of machine learning (ML) and static analy-
sis techniques to analyze apps’ potential non-compliance with
privacy requirements. However, we want to emphasize that we
do not claim to resolve challenges in the individual techniques
we leverage beyond what is necessary for our purposes. This
holds especially true for the static analysis of mobile apps and
its many unresolved problems, for example, in the analysis of
obfuscated code. That said, the details of our contribution are
as follows:
1) For a set of 17,991 Android apps we check whether
they have a privacy policy. For the 9,295 apps that have
one we apply machine learning classiﬁers to analyze
policy content based on a human-annotated corpus of
115 policies. We show, for instance, that only 46% of
the analyzed policies describe a notiﬁcation process for
policy changes. (§ III).
2) Leveraging static analysis we investigate the actual data
practices occurring in the apps’ code. With a failure
rate of 0.4%, a mean F-1 score of 0.96, and a mean
analysis time of 6.2 seconds per app our approach makes
large-scale app analyses for legally relevant data practices
feasible and reliable. (§ IV).
3) Mapping the policy to the app analysis results we identify
and analyze potential privacy requirement inconsistencies
between policies and apps. We also construct a statistical
model that helps predict such potential inconsistencies
based on app metadata. For instance, apps with a Top
Developer badge have signiﬁcantly lower odds for the
existence of potential inconsistencies. (§ V).
4) In collaboration with the California Ofﬁce of the Attorney
General we performed a preliminary evaluation of our
system for use in privacy enforcement activities. Results
suggest that our system can indeed help their lawyers and
other users to efﬁciently analyze salient privacy require-
ments allowing them to prioritize their work towards the
most critical areas. (§ VI).
II. RELATED WORK
We leverage prior work in privacy policy analysis (§ II-A),
mobile app analysis (§ II-B), and their combination to identify
potential privacy requirement inconsistencies (§ II-C).
A. Privacy Policy Analysis
Privacy policies disclose an organization’s data practices.
Despite efforts to make them machine-readable, for instance,
via P3P [17], natural language policies are the de-facto stan-
dard. However,
those policies are often long and difﬁcult
to read. Few lay users ever read them and regulators lack
the resources to systematically review their contents. For
instance, it took 26 data protection agencies one week, working
together as the Global Privacy Enforcement Network (GPEN),
to analyze the policies of 1,211 apps [38]. While various works
2
aim to make privacy policies more comprehensible [34], there
is a glaring absence of an automated system to accurately
analyze policy content. In this study we aim for a solution. We
want to automate and scale the analysis of natural language
privacy policies. As of now, Massey et al. provided the most
extensive evaluation of 2,061 policies, however, not focusing
on their legal analysis but rather their readability and suitability
for identifying privacy protections and vulnerabilities from a
requirements engineering perspective [48]. In addition, Hoke
et al. [40] studied the compliance of 75 policies with self-
regulatory requirements, and Cranor et al. analyzed structured
privacy notice forms of ﬁnancial institutions identifying multi-
ple instances of opt out practices that appear to be in violation
of ﬁnancial industry laws [16].
Different from previous studies we analyze policies at a
large scale with a legal perspective and not limited to the
ﬁnancial industry. We analyze whether policies are available,
as sometimes required by various laws, and examine their
descriptions of data collection and sharing practices. For our
analysis we rely on the ﬂexibility of ML classiﬁers [72] and
introduce a new approach for privacy policy feature selection.
Our work is informed by the study of Costante et al., who
presented a completeness classiﬁer to determine which data
practice categories are included in a privacy policy [15]
and proposed rule-based techniques to extract data collection
practices [14]. However, we go beyond these works in terms
of both breadth and depth. We analyze a much larger policy
corpus and we focus on legal questions that have not yet
been automatically analyzed. Different from many existing
works that focus on pre-processing of policies, e.g. by using
topic modeling [13], [63] and sequence alignment [46], [55]
to identify similar policy sections and paragraphs, we are
interested in analyzing policy content.
Supervised ML techniques, as used in this study, require
ground-truth. To support the development of these techniques
crowdsourcing has been proposed as a viable approach for
gathering rich annotations from unstructured privacy poli-
cies [59], [68]. While crowdsourcing poses challenges due to
the policies’ complexity [56], assigning annotation tasks to
experts [72] and setting stringent agreement thresholds and
evaluation criteria [68] can in fact
lead to reliable policy
annotations. However, as it is a recurring problem that privacy
policy annotations grapple with low inter-annotator agree-
ment [56], [72], we introduce a measure for analyzing their
reliability based on the notion that high annotator disagreement
does not principally inhibit the use of the annotations for ML
purposes as long as the disagreement is not systematic.
B. Mobile App Analysis
Different from the closest related works [22], [62], our
analysis of Android apps reﬂects the fundamental distinction
between ﬁrst and third party data practices. Both have to
be analyzed independently as one may be allowed while the
other may not. First and third parties have separate legal
relationships to a user of an app. Among the third parties,
ad and analytics libraries are of particular importance. Gibler
et al. found that ad libraries were responsible for 65% of
the identiﬁed data sharing with the top four accounting for
43% [35]. Similarly, Demetriou et al. [18] explored their
potential reach and Grace et al. [39] their security and privacy
risks. They ﬁnd that the most popular libraries have the biggest
impact on sharing of user data, and, consequently, our analysis
of sharing practices focuses on those as well. In fact, 75% of
apps’ location requests serve the purpose of sharing it with ad
networks [44].
One of our contributions lies in the extension of various
app analysis techniques to achieve a meaningful analysis
of apps’ potential non-compliance with privacy requirements
derived from their privacy policies and selected laws. The core
functionality of our app analyzer is built on Androguard [20],
a static analysis tool. In order to identify the recipients of
data we create a call graph [35], [66] and use PScout [6],
which is comparable to Stowaway [26], to check whether an
app has the required permissions for making a certain Android
API call or allowing a library to make such. Our work takes
further ideas from FlowDroid [4], which targeted the sharing
of sensitive data, its reﬁnement in DroidSafe [36], and the ded
decompiler for Android Application Packages (APKs) [23].
However, neither of the previous works is intended for large-
scale privacy requirement analysis.
Static analysis is ideally suited for large-scale analysis.
However, as it was recently shown [1], [45], it also has its lim-
itations. First, to the extent that the static analysis of Android
apps is limited to Java it cannot reach code in other languages.
In this regard,
it was demonstrated that 37% of Android
apps contain at least one method or activity that is executed
natively (i.e., in C/C++) [1]. In addition to native code there is
another obstacle that was shown to make the static analysis of
Android apps challenging: code obfuscation. A non-negligible
amount of apps and libraries are obfuscated at the package or
code level to prevent reverse engineering [45]. Finally, static
analysis cannot be used to identify indirect techniques, such
as reﬂection, which often occurs in combination with native
code [1]. We will discuss how these limitations affect our study
in § IV-B.
C. Potential Privacy Requirement Inconsistencies
While mobile app analysis has received considerable at-
tention, the analysis results are usually not placed into a legal
context. However, we think that it is particularly insightful
to inquire whether the apps’ practices are consistent with
the disclosures made in their privacy policies and selected
requirements from other laws. The legal dimension is an
important one that gives meaning to the app analysis results.
For example, for apps that do not provide location services the
transfer of location data may appear egregious. Yet, a transfer
might be permissible under certain circumstances if adequately
disclosed in a privacy policy. Only few efforts have attempted
to combine code analysis of mobile apps with the analysis of
privacy policies, terms of service, and selected requirements.
Such analysis can identify discrepancies between what
is
stated in a legal document and what is actually practiced in
reality. We are ﬁlling this void by identifying potential privacy
requirement inconsistencies through connecting the analyses
of apps, privacy policies, and privacy laws.
Various studies, e.g., [71], [70], demonstrated how to create
privacy documentation or even privacy policies from program
code. Other works focused on comparing program behavior
with non-legal
texts. For example, Huang et al. proposed
AsDroid to identify contradictions between apps and user inter-
face texts [41]. Kong et al. introduced a system to infer security
and privacy related app behavior from user reviews [42]. Gorla
et al. [37] used unsupervised anomaly detection techniques to
analyze app store descriptions for outliers, and Watanabe et
al. [66] used keyword-based binary classiﬁers to determine
whether a resource that an app accesses (e.g., location) is
mentioned in the app’s description.
Different from most previous studies we analyze apps’
behavior for potential non-compliance with privacy require-
ments derived from their privacy policies and selected laws. A
step in this direction was provided by Bhoraskar et al., who
found that 80% of ads displayed in apps targeted at children
linked to pages that attempt to collect personal information
in violation of the law [8]. The closest results to our study
were presented by Enck et al. [22] and Slavin et al. [62].
In an analysis of 20 apps Enck et al. found a total of 24
potential privacy law violations caused by transmission of
phone data, device identiﬁers, or location data [22]. Slavin
et al. proposed a system to help software developers detect
potential privacy policy violations [62]. Based on mappings of
76 policy phrases to Android API calls they discovered 341
such potential violations in 477 apps.
Our approach is inspired by TaintDroid [22] and similar
to the studies of Slavin et al. [62] and Yu et al. [69]. How-
ever, we move beyond their contributions. First, our privacy
requirements cover privacy questions previously not examined.
Notably, we address whether an app needs a policy and
analyze the policy’s content (i.e., whether it describes how
users are informed of policy changes and how they can access,
edit, and delete data). Different from Slavin et al. we also
analyze the collection and sharing of contact
information.
Second, TaintDroid, is not intended to have app store wide
scale. Third, previous approaches do not neatly match to legal
categories. They do not distinguish between ﬁrst and third
party practices [22], [62], do not take into account negative
policy statements (i.e., statements that an app does not collect
certain data, as, for example, in the Snapchat policy quoted in
§ I) [62], and base their analysis on a dichotomy of strong
and weak violations [62] unknown to the law. Fourth, we
introduce techniques that achieve a mean accuracy of 0.94
and a failure rate of 0.4%, which improve over the closest
comparable results of 0.8 and 21% [62], respectively.
III. PRIVACY POLICY ANALYSIS