of the hosts, see Figures 5 and 6 at the respective data series called
3rd Level Domain and 2nd Level Domain. For exam-
ple, we might accumulate the IP addresses from DNS replies for
dl1.example.org and dl2.example.org for the statistics
on the 2nd level domain, but not the third level domain.
This is a feasible approach, since many hosts respond to all re-
quests that belong to a subset of the subnets returned when accu-
mulating by the second-level domain of DNS resolver answer, in-
cluding recursive requests and redirections. We verify this behavior
with active measurements, see Section 4. We ﬁnd that at least two
major CDNs, a streaming provider and a One-Click Hoster, serve
26DNS load balancing deployed by a CDS is biased by the queried
DNS resolver, we repeat the experiment from Section 2.2 using two
other DNS resolvers. In particular, we pick the next most popular
DNS resolvers found in our traces: GoogleDNS and OpenDNS.
Both are third-party resolvers with a global footprint and utilize
DNS anycast.
Comparing the results, we ﬁnd that we attain more IP address di-
versity and subnet diversity when using the ISP DNS resolver. This
is mainly due to the fact that CDSs select the supplied caches based
on the source IP address of the querying DNS resolver. Since the
CDSs are no longer able to map the request to the AS it originates
from, but rather to AS the DNS resolver belongs to, the server se-
lection by the CDS cannot optimize for the location of the DNS
client.
2.3.2
Impact on Trafﬁc Localization
Analyzing the three active DNS measurements from the ISP,
OpenDNS as well as Google DNS resolver, we ﬁnd that a signif-
icant part of the requests that could have been in principle served
by sources within the ISP are directed towards servers that are out-
side of the ISP. However, before tackling this issue, we need to
understand what fraction of the trafﬁc may be served by IP ad-
dresses within the ISP’s network and what fraction is served by IP
addresses outside of the AS. To this end, we analyze each of the
three active DNS traces separately. For each trace, we start by clas-
sifying all DNS replies regarding the redirection aggregation
described in section 2.3 and account the volume (or hits) evenly to
each of the IP addresses. Next, we classify the IP addresses in two
groups - inside and outside of the ISP network. Table 2 summarizes
the results of this aggregation regarding the trafﬁc and hits that were
kept inside the ISP’s network in the columns labeled observed.
Turning to the results, we ﬁnd that there is hardly any differ-
ence between those clients that use the external DNS resolvers. Of
the returned IP addresses, less than 6 % are within the AS. When
weighted by number of requests, this does not change much. How-
ever, when normalizing by volume, about 12 % of the trafﬁc stays
within the AS.
In contrast, clients that use the ISP’s DNS resolver fare better:
almost a quarter of the trafﬁc volume is served from servers within
the AS. Normalized by requests, we see a three fold increase, and
normalized by hits or volume, roughly a two fold increase over
using external DNS resolvers. Among the reasons for the “bad”
performance of external DNS resolvers is that some CDSs may al-
ways return IP addresses outside the ISP, despite the fact that many
of its servers are deployed within the ISP. This explains the sub-
stantial difference and highlights on the one hand the effectiveness
of the CDS optimization, but also points out its limits. As such, it
is not surprising that there are efforts under way within the IETF
to include the source IP addresses of the DNS client in the DNS
requests [12].
However, one can ask if the CDS utilizes the full potential of
trafﬁc localization. For this, we check the potential of trafﬁc local-
ization, by changing the volume (or hit) distribution from even to
greedy. Thus, as soon as we observe at least one IP address inside
the ISP’s network, we count all trafﬁc for the entire aggregation
to be internal. Table 2 shows the results in the columns labeled
potential for all three DNS traces.
Note the substantial differences. Our results indicate that a gain
of more than a factor of two can be achieved. Furthermore, up
to 50 % of the trafﬁc can be delivered from servers within the ISP
rather than only 23.4 %. This may not only in itself result in a sub-
stantial reduction of costs for the ISP, but it also points out the po-
tential of our proposed approach. While the increase is noticeable
for OpenDNS, it is nowhere near that of the ISP’s DNS resolver.
The potential beneﬁt when relying on GoogleDNS is rather small.
A deeper study on our results unveils that content served by highly
distributed and redundant infrastructure can be localized the most.
2.4 From Server Diversity to Path Diversity
Next, we ask the question whether the substantial diversity of
server locations actually translates to path diversity. For this pur-
pose, we generate a routing topology of the ISP by using data from
an IS-IS listener and a BGP listener. However, due to the asymme-
try of routing, we have to explore both directions separately. With
the same argumentation as in Section 2.3 we choose to aggregate
using the redirection scheme for calculating path diversity.
For the HTTP requests we can determine the path within the ISP
using the routing topology. We ﬁnd that roughly 65 % of the total
HTTP requests can be forwarded along at least two different paths.
Indeed, roughly 37 % of the HTTP requests can be forwarded along
at least four different paths.
In addition, we can use the routing data to determine the paths
of all content that is potentially available within the ISP’s AS.2 We
ﬁnd that there is signiﬁcant path diversity. In some cases, a request
can follow up to 20 unique different paths. Moreover, we see that
around 70 % of the HTTP trafﬁc volume and requests can be sent
along at least two different paths.
2.5 Summary
We see that HTTP is again the dominant trafﬁc source, while the
prevalence of P2P trafﬁc decreases. Since most CDSs rely on dis-
tributed infrastructure, we not only observe signiﬁcant server loca-
tion diversity but also signiﬁcant path diversity for accessing HTTP
based content. Indeed, there is the potential to bias roughly half of
the overall trafﬁc by redirecting queries to different content servers.
More precisely, we estimate that around 70 % of the HTTP traf-
ﬁc in a big European ISP can be redirected when taking advantage
of the diversity due to MultQuery, CrossQuery and hostname ag-
gregation. Furthermore, we show that current CDS optimizations
that approximate the location of end-users based on the location of
the local DNS resolvers are more effective than those based on the
location of third-party resolvers. Finally, we show that the trafﬁc
localization potential within the above mentioned ISP is very high
especially when the ISP DNS resolver is utilized.
3. PaDIS ARCHITECTURE AND DEPLOY-
MENT
Given that a substantial fraction of the overall trafﬁc is available
at multiple locations within the ISP and that there is signiﬁcant path
diversity, we now propose a system, named PaDIS, that lets the ISP
take advantage of this diversity while improving content delivery
to the user.
PaDIS is the abbreviation for Provider-aided Distance Informa-
tion System. PaDIS’ task is to act as a location recommendation
service and it is operated by an ISP. More speciﬁcally, a user, a
CDN, a CDP, a DNS resolver [6], or any other entity can query
PaDIS by submitting a list of possible IP addresses and a source.
Upon receiving such a list, PaDIS will rank the submitted IP ad-
dresses according to its metrics such as distance within the Internet
topology, path capacity, path congestion, path delay, etc. To be
able to issue such a ranking, PaDIS relies on ISP speciﬁc network
information, e. g., the local topology as well as Internet wide infor-
mation, e. g., BGP information.
2Augmenting the routing topology with ﬂow information may al-
low us to extend this analysis to all content.
27Table 2: Trafﬁc localization within the network by different DNS resolvers normalized by number of requests and trafﬁc volume
together with the potentially available fraction of localized trafﬁc.
ISP DNS
OpenDNS
GoogleDNS
Metric
observed
potential
observed
potential
observed
potential
IPs
requests
volume
12.3 %
14.9 %
23.4 %
24.2 %
33.2 %
50.0 %
5.8 %
4.7 %
12.0 %
16.0 %
18.8 %
27.7 %
6.0 %
4.8 %
12.3 %
9.7 %
6.4 %
13.4 %
External
DNS
Client
3
2
6
1
ISP DNS
Resolver
4
5
PaDIS
Figure 8: PaDIS use case: Optimizing content delivery trans-
parent to both CDN/CDPs and clients.
We ﬁrst outline how PaDIS can be used to take advantage of
server diversity for content delivery, then outline its architecture,
before discussing its scalability and responsiveness properties.
3.1 PaDIS Usage Options
While PaDIS can be used as an ALTO [32] server to offer ISP-
aided localization for neighbor or peer selection for P2P users, it of-
fers many more possibilities for optimizing content delivery. PaDIS
rankings can either optimize for delay, e. g., for web sites where
objects are typically small and the retrieval time is dominated by
the round-trip-time (Section 4.1) or bandwidth, e. g., for One-Click
Hosters (Section 4.2) offering bulk data.
Assuming that the local ISP runs a PaDIS server much in the
same manner as it offers a DNS resolver to clients, CDSs, the ISP
itself, etc. can use it in a multitude of different ways as outlined
below:
Clients (a): Clients can install a plug-in to their Web browser to
send all DNS replies or even summaries of past DNS replies
to the PaDIS server for re-ranking the returned IP addresses
taking the ISP’s preferences into account.
Clients (b): Clients can overwrite the library responsible for DNS
lookups with one that adds a PaDIS query and then re-ranks
the DNS responses.
Clients (c): Another possibility is to pre-conﬁgure the home-router
located at the edge of a client’s network. Note, these routers
also often act as DNS-relay. In this case, the home-router can
also send any DNS reply to the PaDIS server and then return
a re-ranked DNS reply.
CDNs/CDPs: Content delivery services may collaborate with the
ISPs by contacting them before returning their server choices
to the DNS resolver. A good heuristic for identifying an ap-
propriate PaDIS server is to contact the ISP where the DNS
resolver is located. This use case has the advantage that con-
tent delivery networks can take the ISP preferences into ac-
count during their optimization process. However, the CDS
requires a hint as to the location of the client, e.g., its IP ad-
dress. This is already under discussion within the IETF [12].
ISP: The ISP can enhance its DNS resolver to contact its PaDIS
server and reorder the IP addresses if needed before return-
ing any answer to a client. This is fully transparent to both
the clients as well as the CDNs/CDPs. Figure 8 shows this
scenario, which involves the following steps:
1. The client sends the DNS query to the ISP operated
DNS resolver.
2. The query is recursively resolved using the authorita-
tive DNS servers.
3. The reply is received by the ISP’s DNS resolver.
4. The reply is sent to the PaDIS server for ranking.
5. The PaDIS server augments the reply with informa-
tion from previous ones and ranks them according to
its metrics, which takes the current network status into
account. This reply is then sent back to the DNS re-
solver.
6. The ISP’s DNS resolver sends the ranked and augmented
reply back to the client.
One drawback to most of the above approaches is that most DNS
responses are of the type CrossQuery and therefore do not con-
tain a large number of possible server locations. However, as we
have seen, DNS TTLs are usually short, so when aggregating across
time, server diversity increases.
Let us revisit the DNS TTLs: Originally these were designed
to ensure that stale cache entries do not linger forever in the DNS
caches and that it is possible to relocate, add, or remove servers and
domains within the DNS hierarchy. Today, however, DNS TTLs are
signiﬁcantly shorter than originally envisioned and are mainly used
to aid CDSs with their load balancing and trafﬁc ﬂow optimiza-
tion [34, 23]. Moreover, not all clients adhere to the DNS TTL
values. Therefore, even today, a CDS has to add safety margins to
the TTL values before they can stop serving speciﬁc content from a
possible server. We propose to take advantage of this and allow the
PaDIS server to keep a history of DNS hostname to IP address map-
pings. Using these mappings, the DNS replies can be augmented
and clients can take full advantage of the server location and path
diversity.
However, while the above ISP use case is transparent to the clients
as well as the CDSs, we favor the CDN/CDP use case as it gives
ﬂexibility and control to both the CDNs/ CDPs as well as the ISP.
3.2 PaDIS Architecture
The designed architecture of PaDIS is shown in Figure 9.
It
has two main functionalities: to answer client ranking queries ac-
cording to certain metrics, and to maintain a network information
database. The latter includes network information, such as topol-
ogy, routing, link capacity, link congestion, and link delay. It also
needs to keep in sync with the network, e. g., the network has to
be monitored for topology changes and changes to the path char-
acteristics. Internally, PaDIS represents the network as an anno-
28I
n
f
o
r
m
a
t
i
o
n
N
e
t
w
o
r
k
Information
Retrieval
Network Map
Generator
Annotated Network 
Map Database
Data Management
Query Processing
Query Manager
(Process Engine)
Path
Ranker
Client
Request
Translator
Frequent Hitter
Detector
Figure 9: PaDIS architecture: Overview
tated graph. Accordingly, PaDIS is divided into two parts: data
management and query processing, which communicate via shared
memory.
Data Management Subsystem.
The data management subsystem consists of components for in-
formation retrieval, network map generation and the network map
database.
The information retrieval component is responsible for continu-
ously collecting network information, e. g., it listens to IGP mes-
sages, fetching status information and monitors routes learned via
EGP. The ISP in question uses IS-IS as its IGP. The information
retrieval component generates a representation of the physical net-
work topology from the IGP messages it receives. In case of topol-
ogy changes due to failures, addition of new components or sched-
uled downtime of monitored hardware, the topology is updated on-
line. The information retrieval subsystem also fetches network sta-
tus information and per link statistics, e. g., utilization and delay. In
addition, the operator can assign customized values to links, e. g.,
related to operational costs or contract speciﬁcs.
Moreover, the information retrieval subsystem augments the ISP
internal topology with external routing information by incorporat-
ing EGP messages. For the ISP in question these are BGP updates.
Thus, by combining IGP and EGP information, PaDIS can ﬁnd net-
work path information, including performance characteristics, for
any IP-based connection originating in the ISP. Customers of the
ISP are handled in the same manner as EGP messages. However,
their position and link characteristics may have to be learned from
a data source such as a radius server.
In short, the information
retrieval subsystem learns as much as possible about the network
topology of the ISP to generate a fully annotated network topology
from it.
The network map generator is responsible for maintaining an
up-to-date view of the topology supplied by the information re-
trieval subsystem. It pre-calculates the paths within the network
and caches them for fast look-up. Since the routing within the ISP
is typically more stable than customer and external preﬁxes, there
is a signiﬁcant beneﬁt to caching the path, as it allows for a constant
O(1) look-up of the full path, regardless of the network’s diame-
ter, and a O(1) complexity when updating the more volatile EGP
information, e. g., BGP and/or radius information. However, re-
calculating the paths after a topology change costs O(n2) where n
is the number of routers maintained by PaDIS.
Query Processing Subsystem.
The query processing subsystem consists of a request translator,
a query manager, a path ranker and a frequent hitter detector.
The request translator component checks whether the query com-
plies with the protocol speciﬁcation and performs admission con-
trol based on the client IP address. Furthermore, PaDIS can be
conﬁgured to augment requests with additional IP addresses to fur-
ther enhance the choices presented to the client. If the request is
admitted, the request translator reformats it and submits it to the
Query Manager.
The query manager fetches all available information about each
source-destination pair from the topology. Each pair, together with
the path information, is then handed to the path ranker which uses
a customized ranking function to calculate a weight representing
the preference for this path. Once all pairs have been weighted,
the query manager sorts the list by weights, striping all additional
information added for ranking. The ordered list is passed back to
the request translator and then returned to the client as a sorted list
of sources in descending order of preference as seen by this ISP.
PaDIS is able to support a number of customized ranking func-
tions. Each of these can optimize for different aspects of the path,
e. g., bandwidth, hops, delay, etc., or a combination thereof. The
client can specify its preference regarding the chosen metrics in
the query, triggering the usage of an appropriate ranking function.
However, the details of the functions are not revealed to the clients
and the ranking weights are stripped from the reply. More im-
portantly, no network information is revealed, contrary to other
schemes of user-provider collaboration [36]. Note, that it is in-
tractable to extract network properties or topologies from the rank-
ing even when combined with traceroute information [1].
No information about clients is stored within the system, thus
it preserves client privacy. However, to prevent abuse, PaDIS in-
cludes a frequent heavy hitter detector which can be activated by
the ISP operator. The heavy hitter detector in our prototype is based
on probabilistic lossy counting [13]. It maintains a list of the most
popular IP addresses and sources which can also be utilized by the
ranking function to avoid the creation of hot spots.
3.3 Scalability and Responsiveness
To decrease protocol handling overhead, we use UDP as the de-
fault protocol. However, TCP is supported for requests that ex-
ceed one MTU. Next we quantify the performance and scalability
of PaDIS.
We use two identical dual quad-core CPU machines with six-
teen gigabytes of memory directly connected via Gigabit Ethernet,
one as PaDIS client and one as PaDIS server. We start by send-
ing queries at a slow rate and gradually increase the pace until the
PaDIS server is fully utilizing all CPUs, all the while ensuring that
the client still receives all replies. Figure 10 shows the resulting
number of served queries as well as the average delay for an eight-
thread instance of the PaDIS server. Since the overhead depends on
the number of IP addresses within the query, we varied this number
from 50 to 363 — the maximum number of IP addresses for UDP-
based queries which are restricted to one MTU. PaDIS is able to
serve roughly 90,000 requests per second while ranking 50 IP ad-
dresses. This number drops to about 15,000 per second when rank-
ing 363 IP addresses. Therefore, we conclude that a single PaDIS
server offers sufﬁcient throughput to be matched with one DNS re-
solver of the ISP.
The response time of the PaDIS server ranges from 1 millisecond
to a few milliseconds and is dominated by the processing overhead
in the hardware rather than the network connection. Moreover,
29 100000
 90000
 80000
 70000
 60000
 50000
 40000
 30000
 20000
 10000
d
n
o
c
e
s
r
e
p
d
e
v