third parties, such as users of the model and other independent
testing parties including VirusTotal for malware detection. After that,
based on the feedbacks especially misclassification reports, in the
repair stage, the administrator will repair the model with the help of
an oracle, such as a human performing code reviews and a dynamic
analyzer exploring and examining program behaviors.
An Example Deployment with Spam Detectors. Figure 1 shows
the deployment model of KARMA by using an example of spam
detectors where the oracle is a trusted human. Say, a spam detector
is trained by an administrator with a potential polluted training set
(step one) and deployed together with an email client. When training
the system, the administrator might have already deployed exist-
ing approaches, which are orthogonal to KARMA, to filter potential
polluted emails [17, 34, 39] and make the model robust. However,
some polluted emails may have bypassed the filter and still make
the learning model misclassify samples as evident by existing at-
tacks [35, 45].
Then, the users of this email client complain about misclassifica-
tions and report misclassified emails to the administrator (step two).
The administrator or other trusted person, i.e., an oracle, verifies
these reported misclassifications, and uses them as an input dataset
for KARMA called the oracle set (step three). To improve accuracy,
the oracle set can include a small number of correctly classified
emails as well.
Next, the administrator deploys KARMA to find the cause of
misclassifications in the oracle set (step four). The cause, a subset
of the training set, will be verified by the administrator to confirm
(6) Unlearn(4) Causality AnalysisTraining DataSpam DetectorLabeled EmailsPolluteExisting Filters(1) TrainAttackerAdminUsers(3) Verify(5) VerifyPolluted DataEmails(2) ReportMisclassiﬁcationRepaired ModelDataProcedureOur procedureTraining StageUse StageRepair StageStageUpdateASIA CCS ’18, June 4–8, 2018, Incheon, Republic of Korea
Y. Cao et al.
in Step 2 of the deployment model in Figure 1. None of the false
report will be fed into and thus influence KARMA. Further, the
administrator may even block such users from reporting more
samples.
• Understanding the output of KARMA and adding more samples to
the training set, if necessary, to improve M. KARMA will find the
cause of misclassification, which could be some correctly labeled
samples failing to represent the misclassified samples. That is, the
training data may be insufficient, e.g., lacking a specific category
of samples, so that M misclassifies the entire category. In this
case, the administrator can rely on the cause found by KARMA
to introduce new samples that can differentiate the cause and the
misclassified samples. This is considered beyond the scope of the
paper though, because no attackers or data pollutions exist.
4.2 Causality Analysis
We present how causality analysis works in this section. From a high
level, what KARMA does is to try removing different samples from
the training set and observe whether samples are still misclassified.
That is, based on the effect (i.e., misclassified samples), KARMA tries
to inspect the cause (e.g., polluted samples) by searching through
the cause space (i.e., training set). If the effect (misclassification) is
mitigated when removing a subset of training data, we can consider
this subset as the cause, which is the polluted samples when the
model is polluted.
In the rest of the subsection, we first present the causality analy-
sis from three perspectives: causality search, causality growth and
causality determination.
4.2.1 Causality Search. The first step of causality analysis is
to search for the potential causality that leads to the misclassification
of a learning system against Sor acle . Although ultimately KARMA
needs to search every training sample and try different combinations,
in order to speed up the search, KARMA will conduct a guided pro-
cess that prioritize exploration of training samples that have higher
probability of being polluted. Note that it is the job of causality
determination not the search stage to determine whether samples are
polluted.
Here is how the search with a two-phase procedure works. In
the first phase, KARMA clusters the misclassified data into different
parts, and the centers of the clusters are extracted. Then, in the
second phase, KARMA prioritizes the search of similar data samples
in the training set based on the extracted centers of misclassified
data clusters. These training samples can be used for the causality
growth stage of KARMA as seeds.
Similarity in KARMA is measured by a definition called diver-
gence score in Equation 2, i.e., one divided by the number of com-
mon features between two samples. That is, if two samples share
many common features, they are close to each other and their diver-
gence score is low.
d(x, y) =
1
|{F |F is x′s f eatur e } ∩ {F |F is y′s f eatur e }|
(2)
Now, let us introduce these two phases in details. In the first phase,
misclassified data are first divided into groups based on their labels.
In the most common case where only two labels are available, such
as malicious and benign, all the misclassified benign samples are in
group one, and all the misclassified malicious ones group two. In the
rest of the subsection, for convenience, misclassified data is referred
to only one group of misclassified data. After grouping, we start
clustering in each group individually, and the clustering algorithm is
very similar to k-means but using our divergence scores.
Here is how the first phase, clustering misclassified data, works.
KARMA randomly selects k samples from misclassified data, c1, c2,
..., ck , which are the centers of the initial clusters. Then, KARMA
iterates through all other samples in the misclassified data, and
calculates the divergence scores between all other samples and each
center. A sample will be included into the cluster where the center
has the smallest divergence score with the sample. Next, the center
of each cluster, c1, c2, ..., ck , will be updated based on the common
feature list of that cluster. The entire process is then repeated using
the updated centers until convergence. The final c1, c2, ..., ck are
used in the second phase.
In the second phase, for each ci , KARMA iterates through the
training set, and finds the si that has the smallest divergence score
with ci . All s1, s2, ..., sk will be used as the seeds for the peak finder
and the unlearning module. That is, these samples have a higher
probability to be polluted than others in the training set. This phase
of finding all the seeds can be further divided into two sub-phases:
pre-computing and searching.
• Pre-computing Sub-phase. The pre-computing sub-phase con-
structs a so-called judging tree that can be used in and expedites
the searching. The judging tree has one root node with all the
features used in the training set. The root node has k children (e.g.,
k equals the square root of the size of the training set), where each
child represents a subset of the training set and the value of each
child is the union of all the features used in the subset. Each child
also has l descendants (e.g., l equals the square root of the size
of the subset), and the descendant will also have children. The
structure is repeated and extended until the leaf node only has one
sample.
The construction of a judging tree is as follows. KARMA
adopts a similar clustering mechanism, i.e., a variation of k-means,
used in the first phase to compute one level of the judging tree.
The difference is that instead of using the common feature list as
the center, the construction of a judging tree adopts the union of
all the features in the cluster as the center. KARMA still computes
the divergence scores between each sample and the centers, and
then dispatches the sample to the closest cluster. The overall pro-
cess is repeated until convergence, and the k clusters are served
as the k nodes of the judging tree. Then, the next levels are com-
puted using similar algorithm until the construction reaches the
leaf node.
• Searching Sub-phase. With the judging tree, in the searching sub-
phase, KARMA performs a depth-first, priority searching algo-
rithm, and maintains a minimum divergence score dmin between
ci and the searched samples so far. In the beginning, dmin equals
infinity. On each level, KARMA ranks the search priority based
on the divergence scores between the nodes and ci : Nodes with
smaller divergence scores are searched first, and nodes with di-
vergence scores larger than dmin are skipped directly. dmin is
only updated in the leaf node level, i.e., if the divergence score
between ci and a sample is smaller than dmin, dmin is updated to
Causal Unlearning
ASIA CCS ’18, June 4–8, 2018, Incheon, Republic of Korea
that score. The sample corresponding to the final dmin is selected
as si .
4.2.2 Causality Growth. The second step of KARMA is to
grow the causality found in the first stage by finding more training
samples and forming a cluster. Similar to causality search, causality
growth does not determine whether samples are polluted either. Our
key observation for causality growth is that the common feature
list, i.e., a list of features whose values are identical in two samples
as defined in the divergence score (Equation 2), only shrinks as
the causality cluster grows. Correspondingly, the divergence score
between the causality cluster and each sample in the target dataset
increases when the size of the cluster increases.
Thus, because the divergence score increases as the causality
cluster grows, instead of calculating the divergence score in each
round, we can maintain a lower bound of the divergence score and
only update the score if necessary. The detailed steps are as follows
and shown in Algorithm 1. We maintain a sorted list (one can also
use a priority queue) of the lower bound of the divergence scores
between the cluster and each sample in the target dataset.
The initial list is consisted of all the divergence scores between
the cluster and samples in the target dataset and correctly ordered.
As shown in Line 1–4 of Algorithm 1, we fetch each sample from
the target dataset (T Set), calculate the divergence score between the
cluster (CSet) and each sample, and then put the result to the list
(DiverдenceScoreList). After iterating through all the samples in
CSet, the initial list is generated.
In each round, we fetch the element on the top of the list, i.e., the
one with the smallest value, and update the value to the latest diver-
gence score between the cluster and sample. Particularly, we pop out
the element from the top of DiverдenceScoreList (Line 8 and 9), up-
date the value (Line 10), and then put it back to DiverдenceScoreList
(Line 11). If the updated value still stays on top of the list, we will
include the sample corresponding to the value into the cluster; other-
wise, we will insert the value back to the list in its correct position to
maintain the order, fetch the new element on the top of the list, and re-
peat the process until we have an updated value that stays on the top.
That is, we test whether the updated top of DiverдenceScoreList is
calculated using the current cluster (CSet) in Line 7. If not, Line 8–11
will be repeated; if yes, we pop out the top of DiverдenceScoreList
(Line 13), and add the sample to CSet (Line 14–15).
The rationale behind the algorithm is as follows. In each round, we
want to select the one with the lowest divergence score. Because each
value in DiverдenceScoreList is a lower bound, the real divergence
score will be higher than the value in the list. If we can select a
real divergence score that is smaller than all the lower bound values,
the selected score will be automatically smaller than all other real
divergence scores. Such lazy updates will help us save time in the
calculation.
4.2.3 Causality Determination. In the third step, KARMA de-
termines whether a causality cluster is polluted as well as the size of
the cluster. Now, let us introduce the details about how the determi-
nation works. The detection accuracy of a machine learning model
can be defined as a function: accuracy = f (M, Sor acle), which takes
M the machine learning model and Sor acle the oracle set as inputs.
After unlearning a causality cluster of a certain size, the detection ac-
curacy can be represented as accuracy = f (M′, Sor acle) where M′ is
Algorithm 1 The Algorithm of Growing Causality Clusters.
Input:
Target Dataset: T Set
Target Causality Cluster: CSet (Initialized with a seed)
Causality Cluster Size: size (Intended cluster size)
while DiverдenceScor eList .дet().дetCSet()! = CSet ) do
d = calculateDiverдenceScor e(CSet, i);
DiverдenceScor eList .put(d);
Process:
1: Create an empty, automatically-sorted DiverдenceScor eList .
2: for i in T Set do
3:
4:
5: end for
6: for iter in [0 : size − 1] do
7:
8:
9:
10:
11:
12:
13:
14:
15:
16: end for
d = DiverдenceScor eList .pop();
i = d .дetSample();
new_d = calculateDiverдenceScor e(CSet, i);
DiverдenceScor eList .put(new_d);
end while
d = DiverдenceScor eList .pop();
i = d .дetSample();
CSet .add(i);
the new machine learning model and equals unlearn(M, seed, size).
Further, if we substitute M′ in the accuracy equation, we obtain the
following: accuracy = f (unlearn(M, seed, size), Sor acle). M is gen-
erated from Str aininд, another constant. Therefore, we can simplify
the f to a single variable function accuracy = д(size).
Then, let us discuss the single variable function accuracy =
д(size). If starting from size as one, the detection accuracy increases
in the first place, KARMA will unlearn more polluted samples similar
to the first seed. As the cluster grows, the increasing speed of the
accuracy decreases until the accuracy reaches a peak. The reason
is that the polluted samples close to the cluster will become fewer,
when including more polluted samples into the cluster. Then, the
detection accuracy starts to decrease, because KARMA will include
unpolluted samples into the cluster. At contrast, if starting from size
as one, the detection accuracy decreases in the first place, KARMA
is likely to encounter unpolluted, normal samples that should remain
in the training set. To sum up, our goal is to first identify whether
the cluster contains polluted data by observing the detection accu-
racy changes, and then find the corresponding size to the peak of
the detection accuracy. Therefore, the task boils down to find the
peak value (the first local maximum close to the zero point) of a
single, discrete-value variable function (accuracy = д(size)) and its
corresponding variable (size) value.
Note that the first local maximum is sufficient, because KARMA
will sift through every training sample that is not included in a
cluster. That is, all the training samples will be inspected by KARMA.
At contrast, if KARMA tries the second or the global maximum,
many unpolluted, normal samples may be mistakenly considered as
polluted, thus influencing the overall accuracy.
Next, let us solve the problem of finding the peak of the single-
variable function. In particular, KARMA needs to find the first local
maxima starting from a cluster with the size as zero. The prob-
lem is not as straightforward as finding a local peak of a normal
single-variable function, because the computation of one value in
our function is very expensive. That is, KARMA needs to feed all the
samples of the oracle set into the machine learning engine, obtain
the results, and then calculate the accuracy. The detection of one
sample with a machine learning engine is already expensive, and our
computation time needs to multiply the size of oracle set, making the
ASIA CCS ’18, June 4–8, 2018, Incheon, Republic of Korea
Y. Cao et al.
Algorithm 2 The Algorithm of Causality Determination.
Input:
Initial Step Value: M
The Set Size: Size
The Initial Seed: seed
The Detection Accuracy Function after Unlearning a Cluster of Size x given a seed: д(x) =
f (unlearn(M, seed, size), Sor acl e)
Exit {Note: the cluster is considered unpolluted in this iteration.}
Process:
1: star t = 0
2: end = Size
3: i = M
4: if д(i) < д(0) then
5:
6: end if
7: while star t < end & i < end do
8: