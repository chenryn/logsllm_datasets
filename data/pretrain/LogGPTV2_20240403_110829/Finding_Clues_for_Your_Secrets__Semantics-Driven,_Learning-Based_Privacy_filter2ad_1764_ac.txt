that this term is related to an access to location information.
Other examples (1, 2, 3) are also presented in Table II.
• Nominal subject (Nsubj): A nominal subject is a noun
phrase that is the syntactic subject of a clause. This is
a relation the Checker looks for in the absence of Dobj
between an identiﬁed sensitive token and its context. For
example, “business phone number selected”, in which the
sensitive token “phone number” is the topic of the sentence,
indicating the presence of the information in its related
program location. Example for such case is also presented
in Table II, Index 5.
• Negation modiﬁer (Neg): The negation modiﬁer is the
relation between a negation word and the word it modiﬁes.
In our case, if the sensitive token found in the element
appears with a Neg modiﬁer, likely the element does not
relate to sensitive content. E.g., “Do not input your password
here”.
• Other relations: When the sensitive token labeled actually
has a dependent (Dep) or compound (Compound) or open-
clausal complement (Xcomp) relation or other relations with
its context (other words in the same element name or
constant string), we found that the token becomes less of
an indicator for the presence of private content, since the
token in this case is no longer the theme of its context (the
target of an access or the topic of a sentence). Examples for
such relations are presented in Table II, Index 1, 2, 3, 6.
Using the relations above, Semantic Checker ﬁlters out the
program elements involving sensitive tokens but less likely
to be actually related to privacy content. These elements are
then inspected by Structure Analyzer to further reduce false
positives. In our implementation, the Checker was built upon
Stanford Parser [29], a standard NLP tool for POS tagging and
dependency relation parsing.
C. Sensitive Data Discovery and Tracking
Structure Analyzer. Even when a sensitive token plays a
central role in the name of a variable or a method, or the
content of a constant string, such program element may not
necessarily relate to private content. For example, the statement
in line 5 of Figure 4 talks about home address; however, the
operation here is just checking whether the data object contains
a key “home addr”. Another example is line 10 “Proﬁle URI
is null”, which actually is an output to explain an exception.
So, to identify truly sensitive operations, not only do we need
to check the semantics of the program elements’ identiﬁers
and constant content, but it is also important to look into
the semantics of actual program operations speciﬁed by the
statements involving these elements.
Serving this purpose is Structure Analyzer, which utilizes
a set of program structural features to determine whether a
sensitive-token related statement indeed touches private user
content. In our research, we focus on method invocation
statements, since sensitive user data are accessed by third-
party libraries typically through method calls. To ﬁnd such
statements, the Analyzer ﬁrst locates all method invocations
(e.g., line 5, 6, 8, 10 in Figure 4) directly or indirectly related
to a labeled program element (which involves sensitive tokens),
and then extracts features from these statements to capture
those accessed sensitive data. Speciﬁcally, when a method
6
TABLE II.
EXAMPLES FOR SEMANTIC CHECKER
Index
Element
getStreetViewActivity
Description
As a negative example, “street” only holds a Compound relation with “Activity”,
the Dobj relation here is between “get” and “activity”.
As a negative example, “Location” only holds a Compound relation with “Interval”,
the Dobj relation here is between “get” and “Interval”
getLocationUpdate-
TimeIntervalInMillis
“I’m designing my own As a negative example, “Phone” only holds a Nmod:poss relation with “my”, and a
tees on my phone”
1
2
3
4
5
6
getAddressFromServer
“Username must be
in valid format”
new friend num
Nmod:on relation with “design”. The Dobj relation here is between “design” and “tee”.
As a positive example, “address” here is with POS tagging “NN”, and holds a Dobj
relation with verb “get”.
As a positive example, although there’s not Nsubj relation in the sentence, “Username”
holds a Dobj relation with “Format”.
As a negative example, “Friend” only holds a Compound relation with “num”.
name is labeled, all statements that trigger the method are
considered to be potential sources of private information. For
labeled variables and constant strings, the Analyzer performs
a data-ﬂow analysis on them to identify all the invocations
that take the elements or their derivatives as parameters. All
such statements are then inspected for their program structural
features.
Our key observation is that when labeled elements are
involved in data read or write operations, almost always the
operations are related to sensitive information, with the source
of the information being the element when it is a variable,
another variable in the same method call when the element
is a constant, or the return value of the call when it is a
method. Leveraging this observation, our approach analyzes
how these elements are used in an invocation statement to seek
evidence that such sensitive data operations indeed take place.
Such evidence could be as simple as the presence of keywords
such as “get”, “put” in a method name (e.g., getUserFbProﬁle
in Figure 4). It can also be the return of a data-typed object
from a method call: e.g., getUserFbProﬁle returns a Json object
(line 12 in Figure 4). Another example is the pattern of using
different types of data together: e.g., a constant string (key)
often appears in front of a string variable (value) in a method
invocation; an example is line 6 of Figure 4. These features
are summarized as follows:
• Method name. As mentioned earlier, a feature used in our
research is whether a method name in a labeled statement
contains a speciﬁc token representing data operations, such
as get/set/put/add/insert/delete/remove/read/write/save.
• Parameter type. We also look at the primitive data-types
of the parameters in a method invocation, which indicates
the presence of data operations. Examples include String,
HashMap, Json, etc.
• Return type. The return type of a method call also provides
evidence for the presence of data operations: e.g., a data read
brings back a result in String, HashMap, Json, etc.
• Base value type. Many data operations happen through
speciﬁc Java class libraries. Therefore, for those statements
which contain a base value (e.g., hashMap in method
hashMap.put(key, value)), the class type of the base value
can also help differentiate data access from other operations.
For example, in Figure 4 line 6, the Json class for jsonObject
is used to process data while in line 21, the android.util.Log
class for base value Log does not relate to data use.
• Constant-variable pattern. Also useful to identiﬁcation of
sensitive data operations are the patterns of constant-variable
parameter combinations in method calls. For example, the
ﬁrst parameter of hashMap.put(“user”, $u) is a constant
and the second is a variable, which is a standard key-value
combination for a data-processing method call. In another
example hashMap.put(“user”, “default”), its parameters are
all String-Constant, and thus the call does not indicate the
existence of data access.
On top of such features, Structure Analyzer runs a Support-
Vector Machine (SVM) classiﬁer to determine whether a given
statement
indeed involves private data. The classiﬁer was
trained using 4,326 statements randomly selected and manually
labeled from 100 apps, as elaborated in Section IV.
into the sinks that
Leakage Tracker. The statements together with privacy-
related semantics recovered by ClueFinder are treated as the
actual “sensitive” sources for detecting information leaks.
Speciﬁcally, Leakage Tracker extracts data-typed objects
within the statements from their parameters or return values,
and then performs a data-ﬂow based taint analysis on these
objects. The purpose of this analysis is to ﬁnd out whether
sensitive data ﬂows get
indicate leaks
of the information to unauthorized third parties. Ideally, one
may expect that such a sink is an API used by an untrusted
library to send tainted data out to the Internet, as did in
prior research [37]. In practice, however, tracking tainted ﬂows
across library code is often too heavyweight and less precise,
particularly for a static analysis important for evaluating a large
number of apps. Therefore, in our research, we instead looked
for the presence of an exposure risk, when the tainted data
ﬂow into an untrusted library, since in this case, the data is
no longer safe and its content could be disclosed to the third-
parties through various channels hard to capture by the existing
technologies (i.e., cover channels).
What is unique for ClueFinder is its utilization of semantics
to enhance the taint analysis, which enables more efﬁcient
detection of the exposure risk. For example in Figure 2, even
without analyzing the code from Line 2 to 5, the seman-
tics of the method invocation at Line 14 (e.g., the constant
“last location” involved) immediately reveals the involvement
of sensitive content in the function’s return value (basicInfo).
7
In this way, we can quickly determine whether private data
are under the exposure risk, avoiding more expensive data-
ﬂow analysis.
IV. EVALUATION OF CLUEFINDER
In this section, we ﬁrst describe our experimental settings
for evaluating ClueFinder, and then report its effectiveness
and performance. Also, we compared our approach with prior
work, which demonstrates that ClueFinder outperforms the
prior approaches in terms of sensitive data discovery.
A. Experiment Setting
We implemented ClueFinder in Java (1,604 LOCs) and
Python (609 LOCs). Our implementation extends the Flow-
Droid framework for analyzing decompiled packages in the
Jimple format (an intermediate expression for analyzing DEX
code). Note that since FlowDroid renames all local variables
(like “$r1”, “$r2”) when decompiling app code, current im-
plementation of ClueFinder only utilizes global variables like
static ﬁelds. ClueFinder also utilizes the Java implementation
of Stanford Parser [29] for its NLP analysis. Its Structure
Analyzer component extracts the features from the Jimple
statements and runs the Python implementation of SVM from
Scikit-Learn [11] to train the classiﬁer. All our experiments
were conducted on a 32-core server, with a Linux 2.6.32 kernel
and 64GB memory.
Training data. The classiﬁer was trained using a labeled set of
4,326 statements (half positive and half negative) which were
manually labelled by two Android experts from 100 popular
apps. Speciﬁcally, in this manual-labelling process, we ﬁrst
randomly selected 100 apps from Google-Play (crawled in
August, 2016) based on the top-popular list during that period.
Then, we automatically extracted all statements involving pri-
vacy tokens from these apps by Semantics Locater and Checker
(See Section III-B), and let each of the expert to identify if
the given statements contain private data or not. To create a
more precise training set, each statement was labelled as either
positive or negative only when both of the two experts give the
same result. In total, we collected 7,354 labelled statements,
including 5,191 positive samples and 2,163 negative ones.
Since SVM classiﬁer usually gets better results under balanced
training set [21], we used all negative statements, together with
the same amount of positive statements by random selection
from labelled data as our training set. As a result, the total
amount of our training set is 4,326.
B. Effectiveness
In our experiment, we ﬁrst ran a ten-fold cross validation
on our labeled set (with 4,326 labeled statements in 100 apps).
ClueFinder achieved a precision of 92.7%, a recall of 97.2%
and a F1-Score of 94.8%. Since our training set is randomly
picked, the effectiveness of the classiﬁer should carry over
the entire app code with high probability. Also, we employed
another manual validating process, by running ClueFinder over
another 100 randomly selected apps crawled at the same time
as an unknown set. Our manual validation showed that 320 out
of 3,775 statements are false positives, which gives a precision
of 91.5%. We did not get the recall in this manual validation
due to the lack of ground truth (it is rarely possible to manually
go through all code in the dataset to identify which of them
are indeed sensitive sources).
The total analysis time for the 100 unknown set were 97
minutes (less than 1 minute per app). Such a performance level
enables ClueFinder to process a large number of apps, as we
did in our research (Section V).
False positives and false negatives. Most false positives
reported were caused by rare cases that were not covered
by our labeled set: as an example, in Figure 5, the constant
parameter for the method saveEvent includes the sensitive term
“access token”, which however turns out to have nothing to do
with the variable r1, an object with an “Event” type. Also, in
some cases, even the program structure does not offer sufﬁcient
information for determining whether a statement
involves
sensitive information. For example, in line 2 of Figure 5, the
method saveAppKeyAndAppSecret contains sensitive tokens
like “Key” and “Secret”; however, such private data only
appears within the method and the invocation statement is
actually nonsensitive.
When it comes to false negatives, again many problems
were introduced by the outliers. For example, the statement at
line 4 of Figure 5 returns an integer value to encode the gender
information (1 for male and -1 for female), which does not
meet the expectation that the gender data is supposed to have
a string type. Another source of the problem is the incomplete
knowledge base: some sensitive terms, such as “lon”, “father’s
name” and “mother’s name”, are not considered keywords for
sensitive content; as a result, what ClueFinder discovers is only
a subset of truly sensitive data items.
1 void saveEvent("init", "put access token to
extras", $r1);
2 Umeng.UMTencentSsoHandler: void
saveAppKeyAndAppSecret();
3 Java.Util.HashMap.put("username",
$r1);
4 Integer gender = getUserGender(user);
Fig. 5. False positive and false negative samples
Code obfuscation. As mentioned earlier, ClueFinder is not
designed to analyze deeply obfuscated code with all its se-
mantic information removed. This, however, does not mean
that our approach cannot tolerate any obfuscation and can
be easily defeated by the tools like ProGuard [9]. Actually
we found that a signiﬁcant amount of semantics is preserved
in moderately obfuscated code, e.g., that protected by Pro-
Guard, and therefore can still be analyzed using ClueFinder.
For example, Figure 6 shows a code snippet obfuscated by
ProGuard. As we can see here, strings (e.g., “ReportLocation”
in line 1), parameter types (e.g.,“String” and “Object” appeared
together in line 2) and API calls (e.g., JSONObject.put() in
line 4) all carry meaningful content, which can be leveraged
as features by ClueFinder’s classiﬁer to determine the presence
of sensitive data sources1. In our research from the aforemen-
tioned unknown set with 100 randomly selected apps, we found
1Note that even though BidText could also utilize constant strings, it does
not work on other code features and therefore will be less effective in analyzing
such code, as we further elaborated in the comparison with ClueFinder
8
that 11.3% (426/3,775) of the statements in these apps were
obfuscated. Nevertheless, sensitive data sources and exposure
risks within these apps were all identiﬁed by our approach,
since our classiﬁer leverages a whole set of features that cannot
be easily obfuscated, such as system-level parameter objects
and return values like String, Json, etc.
1 $r0.com.*.sdk.ei: void
a(String,Object)>("ReportLocation", $r3)
2 $r1 = staticinvoke ($r0, "Gender", $r1)
3 $r4 = virtualinvoke $r3.("user_password")
4 $r7.("cust_gender", $r2)
Fig. 6.
identiﬁed by ClueFinder
Samples of partially obfuscated statements (in Jimple format)
Such semantic information is preserved due to a few
practical constraints in code obfuscation. Speciﬁcally, system-
level methods, as discovered by SUSI, cannot be easily obfus-
cated and their meaningful names and parameters therefore
are retained by the tools like ProGuard. As an example,
98% of constant strings in our unknown set are human-