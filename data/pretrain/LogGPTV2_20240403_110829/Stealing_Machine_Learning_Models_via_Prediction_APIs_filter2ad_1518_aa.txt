# Title: Stealing Machine Learning Models via Prediction APIs

## Authors:
- Florian Tramèr, École Polytechnique Fédérale de Lausanne (EPFL)
- Fan Zhang, Cornell University
- Ari Juels, Cornell Tech
- Michael K. Reiter, The University of North Carolina at Chapel Hill
- Thomas Ristenpart, Cornell Tech

## Abstract
Machine learning (ML) models may be considered confidential due to their sensitive training data, commercial value, or use in security applications. Increasingly, these confidential ML models are being deployed with publicly accessible query interfaces. For instance, ML-as-a-service (MLaaS) systems allow users to train models on potentially sensitive data and charge others for access on a pay-per-query basis.

The tension between model confidentiality and public access motivates our investigation of model extraction attacks. In such attacks, an adversary with black-box access, but no prior knowledge of the ML model’s parameters or training data, aims to duplicate the functionality of (i.e., "steal") the model. Unlike in classical learning theory settings, MLaaS offerings may accept partial feature vectors as inputs and include confidence values with predictions. We demonstrate simple, efficient attacks that can extract target ML models with near-perfect fidelity for popular model classes, including logistic regression, neural networks, and decision trees. Our attacks are validated against online services provided by BigML and Amazon Machine Learning. We further show that even omitting confidence values from model outputs still allows for potentially harmful model extraction attacks. These results highlight the need for careful ML model deployment and new countermeasures against model extraction.

## 1. Introduction
Machine learning (ML) aims to provide automated extraction of insights from data through predictive models. A predictive model is a function that maps feature vectors to categorical or real-valued outputs. In a supervised setting, a previously gathered dataset consisting of possibly confidential feature-vector inputs (e.g., digitized health records) with corresponding output class labels (e.g., a diagnosis) is used to train a predictive model that can generate labels for future inputs. Popular models include support vector machines (SVMs), logistic regressions, neural networks, and decision trees.

The success of ML algorithms in both research and practical applications has led to a surge in demand. Open-source frameworks like PredictionIO and cloud-based services offered by Amazon, Google, Microsoft, BigML, and others have emerged to simplify and broaden ML model deployment. Cloud-based ML services often allow model owners to charge others for queries to their commercially valuable models. This pay-per-query deployment option exemplifies an increasingly common tension: while the query interface of an ML model may be widely accessible, the model itself and the data on which it was trained may be proprietary and confidential. Models may also be privacy-sensitive because they can leak information about the training data [4, 23, 24]. For security applications such as spam or fraud detection [9, 29, 36, 55], an ML model's confidentiality is critical to its utility. An adversary who learns the model can often evade detection [4, 36].

In this paper, we explore model extraction attacks, which exploit the tension between query access and confidentiality in ML models. We consider an adversary who can query an ML model (a.k.a. a prediction API) to obtain predictions on input feature vectors. The model is treated as a black box. The adversary may or may not know the model type (logistic regression, decision tree, etc.) or the distribution over the data used to train the model. The adversary's goal is to extract an equivalent or near-equivalent ML model, i.e., one that achieves (close to) 100% agreement on an input space of interest.

We demonstrate successful model extraction attacks against a wide variety of ML model types, including decision trees, logistic regressions, SVMs, and deep neural networks, and against production MLaaS providers, including Amazon and BigML. In nearly all cases, our attacks yield models that are functionally very close to the target. In some cases, our attacks extract the exact parameters of the target (e.g., the coefficients of a linear classifier or the paths of a decision tree). For targets employing a model type, parameters, or features unknown to the attacker, we additionally show a successful preliminary attack step involving reverse-engineering these model characteristics.

Our most successful attacks rely on the information-rich outputs returned by the ML prediction APIs of all cloud-based services we investigated. Those of Google, Amazon, Microsoft, and BigML all return high-precision confidence values in addition to class labels. They also respond to partial queries lacking one or more features. Our setting thus differs from traditional learning-theory settings [3, 7, 8, 15, 30, 33, 36, 53] that assume only membership queries and outputs consisting of a class label only.

For example, for logistic regression, the confidence value is a simple log-linear function \( \frac{1}{1 + e^{-(w \cdot x + \beta)}} \) of the d-dimensional input vector \( x \). By querying \( d + 1 \) random d-dimensional inputs, an attacker can with high probability solve for the unknown \( d + 1 \) parameters \( w \) and \( \beta \) defining the model. We emphasize that while this model extraction attack is simple and non-adaptive, it affects all of the ML services we have investigated.

Such equation-solving attacks extend to multiclass logistic regressions and neural networks but do not work for decision trees, a popular model choice. (BigML, for example, initially offered only decision trees.) For decision trees, a confidence value reflects the number of training data points labeled correctly on an input’s path in the tree; simple equation-solving is thus inapplicable. We show how confidence values can nonetheless be exploited as pseudo-identifiers for paths in the tree, facilitating discovery of the tree's structure. We demonstrate successful model extraction attacks that use adaptive, iterative search algorithms to discover paths in a tree.

We experimentally evaluate our attacks by training models on an array of public datasets suitable as stand-ins for proprietary ones. We validate the attacks locally using standard ML libraries and then present case studies on BigML and Amazon. For both services, we show computationally fast attacks that use a small number of queries to extract models matching the targets on 100% of tested inputs. See Table 1 for a quantitative summary.

Having demonstrated the broad applicability of model extraction attacks to existing services, we consider the most obvious potential countermeasure ML services might adopt: omission of confidence values, i.e., output of class labels only. This approach would place model extraction back in the membership query setting of prior work in learning theory [3, 8, 36, 53]. We demonstrate a generalization of an adaptive algorithm by Lowd and Meek [36] from binary linear classifiers to more complex model types and also propose an attack inspired by the agnostic learning algorithm of Cohn et al. [18]. Our new attacks extract models matching targets on >99% of the input space for a variety of model classes but need up to 100× more queries than equation-solving attacks (specifically for multiclass linear regression and neural networks). While less effective than equation-solving, these attacks remain attractive for certain types of adversaries. We thus discuss further ideas for countermeasures.

In summary, we explore model extraction attacks, a practical kind of learning task that, in particular, affects emerging cloud-based ML services being built by Amazon, Google, Microsoft, BigML, and others. We show:

- Simple equation-solving model extraction attacks that use non-adaptive, random queries to solve for the parameters of a target model. These attacks affect a wide variety of ML models that output confidence values. We show their success against Amazon’s service (using our own models as stand-ins for victims') and also report successful reverse-engineering of the (only partially documented) model type employed by Amazon.
- A new path-finding algorithm for extracting decision trees that abuses confidence values as quasi-identifiers for paths. To our knowledge, this is the first example of practical "exact" decision tree learning. We demonstrate the attack’s efficacy via experiments on BigML.
- Model extraction attacks against models that output only class labels, the obvious countermeasure against extraction attacks that rely on confidence values. We show slower, but still potentially dangerous, attacks in this setting that build on prior work in learning theory.

We additionally make a number of observations about the implications of extraction. For example, attacks against Amazon’s system indirectly leak various summary statistics about a private training set, while extraction against kernel logistic regression models [57] recovers significant information about individual training data points. The source code for our attacks is available online at https://github.com/ftramer/Steal-ML.

## 2. Background
For our purposes, an ML model is a function \( f: X \to Y \). An input is a d-dimensional vector in the feature space \( X = X_1 \times X_2 \times \cdots \times X_d \). Outputs lie in the range \( Y \).

We distinguish between categorical features, which assume one of a finite set of values (whose set size is the arity of the feature), and continuous features, which assume a value in a bounded subset of the real numbers. Without loss of generality, for a categorical feature of arity \( k \), we let \( X_i = \mathbb{Z}_k \). For a continuous feature taking values between bounds \( a \) and \( b \), we let \( X_i = [a, b] \subset \mathbb{R} \).

Inputs to a model may be pre-processed to perform feature extraction. In this case, inputs come from a space \( M \), and feature extraction involves the application of a function \( \text{ex}: M \to X \) that maps inputs into a feature space. Model application then proceeds by composition in the natural way, taking the form \( f(\text{ex}(M)) \). Generally, feature extraction is many-to-one. For example, \( M \) may be a piece of English language text, and the extracted features could be counts of individual words (so-called "bag-of-words" feature extraction). Other examples include input scaling and one-hot-encoding of categorical features.

We focus primarily on classification settings in which \( f \) predicts a nominal variable ranging over a set of classes. Given \( c \) classes, we use as class labels the set \( \mathbb{Z}_c \). If \( Y = \mathbb{Z}_c \), the model returns only the predicted class label. In some applications, however, additional information is often helpful in the form of real-valued measures of confidence on the labels output by the model; these measures are called confidence values. The output space is then \( Y = [0, 1]^c \). For a given \( x \in X \) and \( i \in \mathbb{Z}_c \), we denote by \( f_i(x) \) the \( i \)-th component of \( f(x) \in Y \). The value \( f_i(x) \) is a model-assigned probability that \( x \) has an associated class label \( i \). The model’s predicted class is defined by the value \( \arg\max_i f_i(x) \), i.e., the most probable label.

We associate with \( Y \) a distance measure \( d_Y \). We drop the subscript \( Y \) when it is clear from context. For \( Y = \mathbb{Z}_c \), we use 0-1 distance, meaning \( d(y, y') = 0 \) if \( y = y' \) and \( d(y, y') = 1 \) otherwise. For \( Y = [0, 1]^c \), we use the 0-1 distance when comparing predicted classes; when comparing class probabilities directly, we instead use the total variation distance, given by \( d(y, y') = \frac{1}{2} \sum |y[i] - y'[i]| \).

In the rest of this paper, unless explicitly specified otherwise, \( d_Y \) refers to the 0-1 distance over class labels.

### Training Algorithms
We consider models obtained via supervised learning. These models are generated by a training algorithm \( T \) that takes as input a training set \( \{(x_i, y_i)\}_i \), where \( (x_i, y_i) \in X \times Y \) is an input with an associated (presumptively correct) class label. The output of \( T \) is a model \( f \) defined by a set of parameters, which are model-specific, and hyper-parameters, which specify the type of models \( T \) generates. Hyper-parameters may be viewed as distinguished parameters, often taken from a small number of standard values; for example, the kernel-type used in an SVM, of which only a small set are used in practice, may be seen as a hyper-parameter.

### Diagram of ML Model Extraction Attacks
A data owner has a model \( f \) trained on its data and allows others to make prediction queries. An adversary uses \( q \) prediction queries to extract a model \( \hat{f} \approx f \).

## 3. Model Extraction Attacks
An ML model extraction attack arises when an adversary obtains black-box access to some target model \( f \) and attempts to learn a model \( \hat{f} \) that closely approximates, or even matches, \( f \) (see Figure 1).

As mentioned previously, the restricted case in which \( f \) outputs class labels only matches the membership query setting considered in learning theory, e.g., PAC learning [53] and other previous works [3, 7, 8, 15, 30, 33, 36]. Learning theory algorithms have seen only limited study in practice, e.g., in [36], and our investigation may be viewed as a practice-oriented exploration of this branch of research. Our initial focus, however, is on a different setting common in today’s MLaaS services, which we now explain in detail. Models trained by these services emit data-rich outputs that often include confidence values, and in which partial feature vectors may be considered valid inputs. As we show later, this setting greatly advantages adversaries.

### Machine Learning Services
A number of companies have launched or are planning to launch cloud-based ML services. A common denominator is the ability of users to upload datasets, have the provider run training algorithms on the data, and make the resulting models generally available for prediction queries. Simple-to-use web APIs handle the entire interaction. This service model lets users capitalize on their data without having to set up their own large-scale ML infrastructure. Details vary greatly across services. We summarize a number of them in Table 2 and now explain some of the salient features.

A model is white-box if a user may download a representation suitable for local use. It is black-box if accessible only via a prediction query interface. Amazon and Google, for example, provide black-box-only services. Google does not even specify what training algorithm their service uses, while Amazon provides only partial documentation for its feature extraction \( \text{ex} \) (see Section 5). Some services allow users to monetize trained models by charging others for prediction queries. To use these services, a user uploads a dataset and optionally applies some data pre-processing (e.g., field removal or handling of missing values). She then trains a model using the service and makes it available for queries.