# From Cloud to Edge: A First Look at Public Edge Platforms

## Authors
- Mengwei Xu<sup>1</sup>, Zhe Fu<sup>2</sup>, Xiao Ma<sup>1</sup>, Li Zhang<sup>1</sup>, Yanan Li<sup>1</sup>, Feng Qian<sup>3</sup>, Shangguang Wang<sup>1</sup>, Ke Li<sup>4</sup>, Jingyu Yang<sup>4</sup>, Xuanzhe Liu<sup>5</sup>
- <sup>1</sup>Beijing University of Posts and Telecommunications
- <sup>2</sup>Tsinghua University
- <sup>3</sup>University of Minnesota - Twin Cities
- <sup>4</sup>Unaffiliated
- <sup>5</sup>Peking University

## Abstract
Public edge platforms have garnered significant attention from both academia and industry. In this study, we conduct a pioneering measurement study on a leading public edge platform that is extensively deployed in China. Our measurements quantitatively address two critical yet unexplored questions. First, from the end users' perspective, we evaluate the performance of commodity edge platforms compared to cloud services in terms of end-to-end network delay, throughput, and application Quality of Experience (QoE). Second, from the edge service provider's perspective, we analyze how edge workloads differ from cloud workloads in terms of VM subscription, monetary cost, and resource usage. Our study provides a comprehensive overview of today's public edge platforms and offers valuable insights for the development and operation of future edge services.

## CCS Concepts
- Networks → Network Measurement
- Computer Systems Organization → Grid Computing

## Keywords
Measurement Study, Edge Computing, Workload Analysis

## ACM Reference Format
Mengwei Xu<sup>1</sup>, Zhe Fu<sup>2</sup>, Xiao Ma<sup>1</sup>, Li Zhang<sup>1</sup>, Yanan Li<sup>1</sup>, Feng Qian<sup>3</sup>, Shangguang Wang<sup>1</sup>, Ke Li<sup>4</sup>, Jingyu Yang<sup>4</sup>, Xuanzhe Liu<sup>5</sup>. 2021. From Cloud to Edge: A First Look at Public Edge Platforms. In ACM Internet Measurement Conference (IMC '21), November 2–4, 2021, Virtual Event, USA. ACM, New York, NY, USA, 17 pages. https://doi.org/10.1145/3487552.3487815

## 1. Introduction
Edge computing, by bringing computation and storage closer to end users, is expected to benefit a wide range of applications such as autonomous driving, augmented reality/virtual reality (AR/VR), Internet of Things (IoT), and smart cities. Edge computing can be implemented through various paradigms, including cloudlets and Multi-access Edge Computing (MEC). This study focuses on public edge platforms, which deploy numerous lightweight data centers (DCs) at decentralized geographical locations and provide hardware resources to third-party customers. These platforms are increasingly popular as they inherit the key advantages of commercial cloud computing, which has proven successful over the past decade. Major cloud providers, such as Azure and AWS, are now building their own public edge platforms, such as Azure Edge Zone and AWS Local Zones.

Edge platforms offer several advantages over traditional cloud computing, including lower network latency, improved application performance, and potentially reduced operational costs. However, these benefits have not been comprehensively studied in real-world operational environments. In this paper, we conduct, to our knowledge, the first measurement study of a commercial public edge platform, examining it from both the end users' and the edge providers' perspectives. For end users, we investigate key metrics such as end-to-end latency, throughput, and application QoE. For edge providers, we analyze the dynamics of edge workloads. This dual approach helps to reveal a complete landscape of the edge ecosystem.

### Challenges
We faced several challenges in this study:
1. **Geographical Distribution**: Edge servers are more geographically distributed than traditional cloud servers, requiring extensive measurements at multiple vantage points. We conducted a country-wide, crowd-sourced study involving 158 participants who ran our custom testing tool on their mobile devices. We obtained meaningful results from 41 cities in China over diverse access networks (WiFi, LTE, 5G). We also developed two user applications—cloud gaming and live video streaming—and deployed them over commercial edge and cloud services. Our implementations allowed us to gather detailed QoE information.
2. **Insider View**: Obtaining an insider's view of operational edge service providers is difficult. We collaborated with a commercial, multi-tenant edge service provider, referred to as NEP (Next-generation Edge Platform), which has been operating for over three years and serves millions of users. We collected detailed usage traces of all Infrastructure-as-a-Service (IaaS) VMs in NEP’s DCs for three months.
3. **Comparison with Cloud**: Ideally, we would like to quantitatively compare edge and cloud in terms of performance and server workload. Through active measurements, we compared NEP’s performance with Alibaba Cloud ECS (AliCloud), a leading cloud provider in China. The server workload comparison was more challenging due to the lack of publicly available workload traces. We used the Azure cloud dataset [38] collected in 2019 for comparison, though this dataset primarily reflects the U.S. market and the data collection period differs. We draw our conclusions cautiously, only when confident that the disparities between the NEP and Azure datasets are likely due to inherent differences between cloud and edge.

### Findings
Our key findings are summarized below:
1. **Network Latency (§3.1)**: NEP offers lower network delays, with median RTTs of 10.5ms, 34.2ms, and 11.7ms for WiFi, LTE, and 5G networks, respectively, which are 1.89×, 1.42×, and 1.35× lower than the nearest AliCloud DC. However, NEP still does not meet the requirements of delay-critical applications like cloud VR/AR (5ms–20ms) and autonomous driving (10ms). This is because the nearest NEP server is typically 5–12 hops away, rather than the 1–2 hops envisioned for edge computing. Further improvements in network performance could be achieved by denser deployment of DCs and integrating them into ISP core networks or cellular base stations.
2. **Network Throughput (§3.2)**: NEP improves network throughput only when the last-mile bandwidth capacity is high, e.g., >200Mbps like 5G downlink. For WiFi and LTE, the end-to-end throughput is bottlenecked by the wireless hop, so edges show no improvement over remote clouds. Given the current rarity of high-throughput applications and the associated high operational costs, throughput is not a primary advantage of NEP-like edges at present. This may change as 5G shifts the bottleneck from the last mile to the wired Internet.
3. **Application QoE (§3.3)**: Placing the gaming backend on nearby NEP sites significantly improves response delay (91ms vs. 145ms for remote clouds). To further enhance QoE, optimizations should focus on server-side gaming execution, e.g., through higher CPU parallelism or hardware acceleration. For live streaming, NEP brings modest improvements (up to 24% reduction in streaming latency), but the streaming delay remains high (400ms without a jitter buffer). This is due to (i) NEP’s edge resources reducing propagation delay but not transmission delay, and (ii) content processing/computation often being the bottleneck. Future efforts should focus on improving hardware capacities and system-software stacks.
4. **Characteristics of NEP’s Edge VMs (§4.1)**: NEP’s VMs are equipped with more resources (median: 8 CPUs, 32GB memory) compared to Azure (median: 1 CPU, 4GB memory). However, NEP’s resource utilization is much lower (6× lower mean CPU usage), indicating potential over-provisioning. This may be due to (i) NEP apps being mostly delay-critical with high temporal variations, and (ii) difficulty in forecasting fluctuating resource demands at different locations. Our findings suggest that resource allocation challenges are amplified when apps migrate from clouds to edges.
5. **Resource Usage (§4.2)**: NEP’s resource usage is highly unbalanced across servers (up to 14× within the same site), sites (up to 731× within the same province), and VMs hosting the same app (up to 3×). This indicates possible imperfections in NEP’s VM placement and selection strategies. We also identified important factors for designing load balancers for NEP-like edge platforms, including fluctuating usage patterns, geo-sensitive resource demand, and decoupled VM placement and end-user request scheduling strategies.
6. **Load Balancing (§4.3)**: Load balancing is crucial for maintaining SLAs by adapting resource allocation to application needs. Our findings highlight the importance of considering fluctuating usage patterns, geo-sensitive resource demand, and decoupled VM placement and end-user request scheduling strategies.
7. **Resource Prediction (§4.4)**: NEP workloads exhibit stronger seasonality and are easier to predict compared to Azure, offering opportunities for more fine-grained, intelligent resource management.
8. **Monetary Cost (§4.5)**: NEP is generally cheaper for bandwidth-hungry applications, with an average cost reduction of 45% and up to 98% for network cost reduction compared to AliCloud. However, applications with high hardware demand and low network demand, or those with high temporal network usage variance, may not see financial benefits due to NEP’s slightly higher hardware charges and coarse-grained billing model.

### Contributions
This work presents a pioneering measurement study of a major public edge platform in China, providing detailed characterizations of performance, workload, and billing. Our findings offer valuable insights for the development and operation of future edge services.