title:ACC: automatic ECN tuning for high-speed datacenter networks
author:Siyu Yan and
Xiaoliang Wang and
Xiaolong Zheng and
Yinben Xia and
Derui Liu and
Weishan Deng
ACC: Automatic ECN Tuning for High-Speed Datacenter
Networks
Siyu Yan†, Xiaoliang Wang⋆, Xiaolong Zheng†, Yinben Xia†
† Huawei Technologies Co., Ltd
Derui Liu⋆, Weishan Deng†
⋆ Nanjing University
ABSTRACT
For the widely deployed ECN-based congestion control schemes,
the marking threshold is the key to deliver high bandwidth and low
latency. However, due to traffic dynamics in the high-speed produc-
tion networks, it is difficult to maintain persistent performance by
using the static ECN setting. To meet the operational challenge, in
this paper we report the design and implementation of an automatic
run-time optimization scheme, ACC, which leverages the multi-
agent reinforcement learning technique to dynamically adjust the
marking threshold at each switch. The proposed approach works
in a distributed fashion and combines offline and online training to
adapt to dynamic traffic patterns. It can be easily deployed based
on the common features supported by major commodity switching
chips. Both testbed experiments and large-scale simulations have
shown that ACC achieves low flow completion time (FCT) for both
mice flows and elephant flows at line-rate. Under heterogeneous
production environments with 300 machines, compared with the
well-tuned static ECN settings, ACC achieves up to 20% improve-
ment on IOPS and 30% lower FCT for storage service. ACC has
been applied in high-speed datacenter networks and significantly
simplifies the network operations.
CCS CONCEPTS
• Networks → Transport protocols; In-network processing.
KEYWORDS
ECN, AQM, Congestion Control, Datacenter Network
ACM Reference Format:
Siyu Yan†, Xiaoliang Wang⋆, Xiaolong Zheng†, Yinben Xia†, Derui Liu⋆,
Weishan Deng†, † Huawei Technologies Co., Ltd
⋆ Nanjing University .
2021. ACC: Automatic ECN Tuning for High-Speed Datacenter Networks.
In ACM SIGCOMM 2021 Conference (SIGCOMM ’21), August 23–28, 2021,
Virtual Event, USA. ACM, New York, NY, USA, 14 pages. https://doi.org/10.
1145/3452296.3472927
Xiaoliang Wang is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGCOMM ’21, August 23–28, 2021, Virtual Event, USA
© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-8383-7/21/08...$15.00
https://doi.org/10.1145/3452296.3472927
1 INTRODUCTION
Datacenters host a variety of applications like big data process-
ing [18, 33, 34, 41, 54, 58], distributed storage [14, 32, 42, 53], high
performance computing [30, 31, 64], etc. These applications desire
high bandwidth and low network latency. For example, the total
throughput of each storage node can exceed 100Gbps and the access
delay of NVMe SSDs is at microsecond level [27, 60]. To meet the
demand of applications, on one hand the link speed of current Dat-
acenter Networks (DCNs) has grown from 25Gbps to 100Gbps, and
beyond [27, 60]. On the other hand, new techniques like Remote
DMA (RDMA) [9] and advanced congestion control mechanisms
have been proposed and deployed in large-scale production data-
centers to reduce the delay caused by host networking stack and
in-network queuing [7, 27, 29, 35, 36, 62, 65].
Since Explicit Congestion Notification (ECN) is commonly sup-
ported by commodity switches, it has been widely adopted by
the state-of-the-art congestion control mechanisms in DCN, e.g.,
DCTCP [7], DCQCN [65], and their enhanced schemes [36, 62].
They use the standard ECN [43] and RED [20] at switch and ECN-
aware rate control at the end-host to throttle the injection rate upon
congestion. Despite the success of ECN-based congestion control
schemes at datacenter networks, with the increase of network speed
and stringent requirement of low-latency operation, the datacenter
network operators face many challenges:
• Determining the appropriate device configuration is challeng-
ing. Though the publications provide formulas to determine the
marking thresholds [6, 65, 66], the parameters there are highly
related to the real environments [27, 29]. With regard to the large
number of switches and heterogeneity network environment, it
usually takes network operators weeks or months to tune the
ECN setting in practice. ECN Tuning is made more difficult when
large-scale datacenter consists of legacy devices and/or products
belonging to different vendors.
• In multi-tenant networks, the pre-determined static marking
threshold is usually conservative to handle tenants with different
traffic classes. It is hard to maintain persistent high transmission
rate for the bandwidth-sensitive flows by setting up small queues
in switch buffers (through high ECN marking rate), because flow
rate increases conservatively after congestion. On the other hand,
though the built-up queue is helpful to maintain high throughput
(through low ECN marking rate), it introduces high queuing delay
to latency-sensitive flows. The network operators need to make a
trade-off between network utilization and tenants’ performance.
• The nature of multi-tenant network leads to various traffic pat-
terns spatially and temporally. Heterogeneous computing causes
periodically large volume data and burst traffic with short mes-
sages co-exist in the network. Depending on the cluster allocation
384
SIGCOMM ’21, August 23–28, 2021, Virtual Event, USA
Yan et al.
policy, the spatial dynamics could result in workload dynam-
ics at the same location as well. We have witnessed an overall
degraded performance due to the incremental deployment of
applications. Workload dynamics requires parameters tuning at
different timescales. Operators may potentially tune it weekly
or monthly. However, it is time-consuming and error-prone to
renew the ECN marking thresholds in hand-crafted fashion for
thousands of running switches.
To reconcile low latency and high bandwidth, HPCC [29] lever-
ages the precise load information from In-network telemetry (INT)
to compute accurate flow rates. Timely [35] and Swift [27] ad-
just flow rates based on accurate delay measurements with NIC
timestamps instead of ECN-based signals. These clean-slate designs
demonstrate tremendous improvements. Unfortunately, the deploy-
ability is a challenge in heterogeneous datacenters consisting of
legacy devices, which do not support new features, e.g. INT. More-
over, it is notable that due to operational issues for the multi-tenant
production datacenters with bare-metal servers and RDMA net-
working, it is not easy to revise the networking stack at end-host.
From the networking operational point of view, to optimize the
performance with minimum impact on tenants, we target "zero-
configuration" as the objective, i.e. building up the network by using
the default setting of commodity devices, while enabling automatic
and fine-grained parameters tuning over volatile datacenter traffic
without human intervention.
In this paper, we introduce our operation experience in a high-
speed datacenter network, which realizes automatic in-network
optimization to maintain low queue length without compromising
network utilization at run-time. With regard to the widely-deployed
ECN-based congestion control, the choice of the marking threshold
is the key as it directly affects the performance of throughput and
latency. We introduce, ACC1, which applies Deep Reinforcement
Learning (DRL) for automatic ECN marking threshold tuning at
intermediate switches. ACC obtains information of switch loading.
The DRL agent generates a policy based on observed telemetry
statistics and updates the ECN parameters through the control in-
terface of switches. ACC requires no new features of commodity de-
vices [11–13]. Specifically, ACC enables the usage of default setting
of NICs belonging to different vendors e.g., Mellanox ConnectX-4,
ConnectX-5 and Intel NICs, and does not need any modification of
ECN-based rate control at the end servers. Thus, ACC is easy to
integrate into current datacenters.
In detail, we propose the following optimization when deploying
ACC. First, due to the fact that for the scale of production datacen-
ters, it is time/bandwidth consuming to collect the information of
buffers and flow states from all switches, we study the distributed
design instead of applying a centralized approach. It deploys DRL
agent at each switch to adjust the ECN marking threshold indepen-
dently. Second, given the large state and action space associated
with ECN parameters, we simplify the operation by discretizing the
DRL’s state and action values. The discretion function is carefully
determined based on the characteristic of traffic. Third, to optimize
the exploration-efficiency of online DRL, we train the DRL model
offline using samples collected from various applications and traffic
1ACC is named after Adaptive Cruise Control for network operation.
385
patterns. Then, we adopt a fast exponential decay of the exploration
probability online to avoid the unstable exploring actions [46].
In summary, this paper makes the following contributions:
The performance and stability of ACC are verified in produc-
tion datacenters to support the services of distributed storage and
training. ACC allows to maintain low switch queuing delay with di-
verse workloads. It maintains line-rate in both 25Gbps and 100Gbps
networks, while improving the IOPS of storage service by 20% for
a DCN with 300+ servers. It provides short RPC completion time
for intensive storage and analytic workloads [27]. Through both
self-defined workloads for stress tests and realistic traffic loads,
our experiments show that ACC achieves up to 20% lower average
FCT and 60% lower 99-percentile FCT compared with the static
ECN threshold setting, while maintaining high throughput and link
utilization.
• ACC aims to achieve "zero-configuration" to simplify network
operation by automatic in-network optimization. The proposed
approach is appealing because it does not need any modification
on ECN-based protocols. ACC is easy to implement in the com-
modity switches. Automatic deployment with minimum main-
taining cost is important not only for large-scale public cloud,
but also for private cloud service maintained by small companies
or academic institutes.
• ACC achieves good performance in terms of both network-level
measures and applications-level metrics in realistic datacenter.
ACC is able to support various applications and diverse traffic
simultaneously. At the network level, ACC delivers high utiliza-
tion and sustains low latency at the same time. At the application
layer, ACC provides short RPC completion time for intensive
storage and analytics workloads.
• Learning-based network optimization has been recently studied
for Internet, Cellular network, and datacenter, etc. [19, 25, 26, 45,
56, 59]. To the best of our knowledge, this is the first work that
demonstrates the experience for successfully applying DRL in
ultra-high speed, datacenter-scale networks for automatic ECN
tuning in practice.
2 BACKGROUND AND REQUIREMENTS
In this section, we explain the trend and difficulty when operating
datacenter networks.
2.1 Background
We run both TCP and RDMA2 protocols in our datacenter. Recently
the customers have increasingly acquired for the RDMA network-
ing support to deploy their computing and storage services, e.g. the
GPU clusters running high-speed machine learning applications
and baremetal servers deploying large-scale storage services. To
meet customers’ demand, network operators maintain a large-scale
(hundreds to thousands of servers), high bandwidth (25Gbps and
beyond), low latency (microseconds) RDMA network running Ro-
CEv2 protocol [9]. DCQCN [65] is the default congestion control
mechanism integrated into hardware by RDMA NIC vendors. DC-
QCN applies the explicit congestion notification (ECN) on switches
to inform the sender to update the injection rate. ECN marking
2Remote DMA technique, allowing directly access to the memory of remote server,
greatly decreases the latency of packet processing at end-host [1].
ACC: Automatic ECN Tuning for High-Speed Datacenter Networks
SIGCOMM ’21, August 23–28, 2021, Virtual Event, USA
(a) Incast(8:1),32 Flows/server
(b) Incast(15:1),8 Flows/server
Figure 1: The optimal ECN settings under different workloads
Figure 2: FCT under different DCQCN parameters:
SECN0 (DCTCP): Kmin = Kmax = 18KB;
SECN1 (DCQCN): Kmin = 5KB, Kmax = 200KB;
SECN2 (HPCC): Kmin = 100KB, Kmax = 400KB
threshold is the key for maintaining short in-network queuing
which has great impact on the latency and throughput performance
of the running applications.
2.2 Motivation
When operating a large-scale RDMA network, we see multiple
challenging issues to determine the ECN setting.
Observation 1: Various traffic patterns demand for different
ECN settings. In large-scale production datacenter networks, the
traffic model is easily affected by multiple factors, including the
number of flows, load, scale of network, etc. The traffic pattern
varies over time. For example, the delay sensitive Online Trans-
action Processing (OLTP) workload usually appears at day time.
While the workload is dominated by the Online Analytical Pro-
cessing (OLAP) traffic at night (and the end of each month). On
the other hand, with the rise of network bandwidth in datacenter,
the timescale of network events decreases accordingly, e.g. most
bursts on the racks sustain for tens of microseconds [63]. The micro-
burst traffic may result in serious performance degradation (e.g.,
Incast problem [44]). Through an experiment, we demonstrate that
the fixed ECN parameters setting is not adaptive to satisfy the re-
quirement of variable traffic patterns. We build a testbed with a
small Clos network, which has 24 servers with 25 Gbps uplink, 2
leaf switches and 2 spine switches connected with 100 Gbps links.
In the first case, we emulate the scenario of incast congestion by
randomly selecting 8 servers as senders and 1 server as receiver.
Each sender generates 32 flows to the receiver. In the second case,
we select 15 senders, each of which generates 8 flows simultane-
ously. We measure the receiver’s throughput and queue depth of
the switch connecting to the receiver. As shown in Figure 1, in the
first case, the optimal ECN threshold is K = 500KB for maintaining
a small queue in buffer with high throughput. For the second case,
K = 50KB is the optimal point with regard to the tradeoff between
delay and throughput, which is much less than K = 500KB. We have
tested other scenarios using different link loads, network scales and
workloads (W ebSearch, DistributedStoraдe), which have different
optimal points to maintain high throughput and low latency (see
Section 5.3).
Observation 2: Existing solutions of static ECN settings do
not work well at run time. Through empirical studies on a repre-
sentative ECN setting, we demonstrate and explain that the existing
solutions of static ECN settings are not adaptive to the variable
workloads. In the experiment, we use three kinds of ECN settings in
switch, SECN0 based on DCTCP paper [6], SECN1 base on DCQCN
paper [65] and SECN2 based on HPCC [29]. We use DataMining
[22] (Scenario-1) and WebSearch [7] (Scenario-2) traffic workloads
in the above Clos network topology. The results are normalized
according to the average FCT of SECN0. As shown in Figure 2,
SECN2 achieves the lowest FCT in Scenario-1 but SECN1 achieves
the best result in Scenario-2. This is because SECN1 aggressively
triggers ECN notification to keep low queue length for latency sen-
sitive traffic. It causes large variation on delay performance and