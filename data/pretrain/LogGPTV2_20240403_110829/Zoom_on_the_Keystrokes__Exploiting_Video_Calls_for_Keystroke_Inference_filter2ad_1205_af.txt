other
top-requested
features
- Cases, Data,
https://gitlab.com/
https://tools.google.com/dlpage/hangoutplugin,
“10 million
kalilinux/packages/seclists/-/blob/kali/master/Passwords/Common-
Credentials/10-million-password-list-top-1000000.txt,
accessed 9-Sep-2020].
“500,000 Hacked Zoom Accounts Given Away For Free On The Dark
Web,”
https://www.forbes.com/sites/leemathews/2020/04/13/500000-
hacked-zoom-accounts-given-away-for-free-on-the-dark-web/,
[Online; accessed 22-May-2020].
“AudioSet - A large-scale dataset of manually annotated audio events,”
https://research.google.com/audioset/, [Online; accessed 22-May-2020].
“Coronavirus Disease 2019 (COVID-19)
and
https://www.cdc.gov/coronavirus/2019-ncov/cases-
Surveillance,”
updates/index.html, [Online; accessed 22-May-2020].
“Google Hangouts,”
[Online; accessed 22-May-2020].
in
“Introducing
and
Google Meet,” https://cloud.google.com/blog/products/productivity-
collaboration/introducing-tiled-view-and-other-top-requested-features-
in-google-meet, [Online; accessed 22-May-2020].
“livechat,” https://www.livechat.com/typing-speed-test/#/global-scores,
[Online; accessed 9-Sep-2020].
“ManyCam,” https://manycam.com/, [Online; accessed 22-May-2020].
“OBS Studio,” https://obsproject.com/,
[Online; accessed 22-May-
2020].
“Skype,” https://www.skype.com/en/, [Online; accessed 22-May-2020].
“The Majestic Million top 1 million websites,” https://majestic.com/
reports/majestic-million, [Online; accessed 9-Sep-2020].
“Twitch,” https://www.twitch.tv/, [Online; accessed 22-May-2020].
“Why Video Chat
and
Gen Z,” https://talkative.uk/info/why-video-chat-is-new-normal-for-
millennials-gen-z/, [Online; accessed 22-May-2020].
“Wictionary top 100,000 most frequently-used English words [for john
the ripper],” https://gist.github.com/h3xx/1976236, [Online; accessed
22-May-2020].
“WIDER
http:
//shuoyang1213.me/WIDERFACE/, [Online; accessed 22-May-2020].
“YouTube,” https://www.youtube.com/,
[Online; accessed 22-May-
2020].
“Zoom,” https://zoom.us/, [Online; accessed 22-May-2020].
“Zoom Optimizes Noise Cancellation
https://blog.zoom.us/wordpress/2018/08/21/zoom-adds-noise-
cancellation-to-our-top-notch-audio/,
2020].
“‘Zoombombing’ Becomes
https://www.nytimes.com/2020/04/03/technology/zoom-harassment-
abuse-racism-fbi-warning.html, [Online; accessed 22-May-2020].
a Dangerous Organized
for Top-Notch Audio,”
is
the New Normal
for Millennials
Benchmark,”
Detection
[Online;
accessed
22-May-
FACE:
A
Face
Effort,”
[20] K. Ali, A. X. Liu, W. Wang, and M. Shahzad, “Keystroke recognition
using wiﬁ signals,” in International Conference on Mobile Computing
and Networking (MobiCom), 2015.
[21] S. A. Anand and N. Saxena, “Keyboard emanations in remote voice
calls: Password leakage and noise(less) masking defenses,” in Proceed-
ings of the Eighth ACM Conference on Data and Application Security
and Privacy, 2018.
[22] D. Asonov and R. Agrawal, “Keyboard acoustic emanations,” in IEEE
Symposium on Security and Privacy (S&P), 2004.
[23] M. Backes, T. Chen, M. Duermuth, H. P. A. Lensch, and M. Welk,
“Tempest in a teapot: Compromising reﬂections revisited,” in IEEE
Symposium on Security and Privacy (S&P), 2009.
[24] Y. Berger, A. Wool, and A. Yeredor, “Dictionary attacks using keyboard
acoustic emanations,” in ACM CCS, 2006.
[25] L. Cai and H. Chen, “Touchlogger: Inferring keystrokes on touch screen
from smartphone motion,” in USENIX Conference on Hot Topics in
Security, 2011.
[26] S. K. Card, The psychology of human-computer interaction. Crc Press,
1983.
14
[27] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam, “Rethinking
atrous convolution for semantic image segmentation,” arXiv preprint
arXiv:1706.05587, 2017.
[28] Y. Chen, T. Li, R. Zhang, Y. Zhang, and T. Hedgpeth, “Eyetell: Video-
assisted touchscreen keystroke inference from eye movements,” in IEEE
Symposium on Security and Privacy (S&P), 2018.
[29] A. Compagno, M. Conti, D. Lain, and G. Tsudik, “Don’t skype & type!
acoustic eavesdropping in voice-over-ip,” in Proceedings of the 2017
ACM on Asia Conference on Computer and Communications Security,
2017.
[30] L. Ding and A. Goshtasby, “On the canny edge detector,” Pattern
Recognition, vol. 34, no. 3, pp. 721–725, 2001.
[31] T. Halevi and N. Saxena, “A closer look at keyboard acoustic ema-
nations: Random passwords, typing styles and decoding techniques,”
in ACM Symposium on Information, Computer and Communications
Security, 2012.
[32] M. G. Kuhn, “Optical time-domain eavesdropping risks of crt displays,”
in IEEE Symposium on Security and Privacy (S&P), 2002.
[33] M. Li, Y. Meng, J. Liu, H. Zhu, X. Liang, Y. Liu, and N. Ruan, “When
csi meets public wiﬁ: Inferring your mobile phone password via wiﬁ
signals,” in ACM CCS, 2016, pp. 1068–1079.
[34] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Doll´ar, and C. L. Zitnick, “Microsoft coco: Common objects in
context,” in European Conference on Computer Vision, 2014, pp. 740–
755.
[35] X. Liu, Z. Zhou, W. Diao, Z. Li, and K. Zhang, “When good becomes
evil: Keystroke inference with smartwatch,” in ACM CCS, 2015.
[36] A. Maiti, O. Armbruster, M. Jadliwala, and J. He, “Smartwatch-based
keystroke inference attacks and context-aware protection mechanisms,”
in ACM Asia Conference on Computer and Communications Security,
2016.
[37] A. Maiti and M. Jadliwala, “Light ears: Information leakage via smart
lights,” Proceedings of the ACM on Interactive, Mobile, Wearable and
Ubiquitous Technologies, vol. 3, no. 3, 2019.
[38] A. Maiti, M. Jadliwala, J. He, and I. Bilogrevic, “Side-channel inference
attacks on mobile keypads using smartwatches,” IEEE Transactions on
Mobile Computing, vol. 17, no. 9, 2018.
[39] P. Marquardt, A. Verma, H. Carter, and P. Traynor, “(sp)iphone:
Decoding vibrations from nearby keyboards using mobile phone ac-
celerometers,” in ACM CCS, 2011.
[40] E. Miluzzo, A. Varshavsky, S. Balakrishnan, and R. R. Choudhury,
“Tapprints: your ﬁnger taps have ﬁngerprints,” in ACM International
Conference on Mobile Systems, Applications, and Services (MobiSys),
2012.
[41] T. R. Ostrach, “Typing speed: How fast is average,” Orlando, FL, USA,
1997.
[42] E. Owusu, J. Han, S. Das, A. Perrig, and J. Zhang, “Accessory:
Password inference using accelerometers on smartphones,” in ACM
Workshop on Mobile Computing Systems and Applications (HotMobile),
2012.
[43] E. Ronen and A. Shamir, “Extended functionality attacks on iot devices:
The case of smart lights,” in IEEE European Symposium on Security
and Privacy (EuroS&P), 2016.
[44] L. Simon and R. Anderson, “Pin skimmer: Inferring pins through the
camera and microphone,” in Proceedings of the Third ACM Workshop
on Security and Privacy in Smartphones & Mobile Devices, 2013, pp.
67–78.
[45] D. Stavens, “The opencv library: computing optical ﬂow,” 2007.
[46] M. Stokes, M. Anderson, S. Chandrasekar, and R. Motta, “A Standard
Default Color Space for the Internet - sRGB,” https://www.w3.org/
Graphics/Color/sRGB, [Online; accessed 22-May-2020].
J. Sun, X. Jin, Y. Chen, J. Zhang, Y. Zhang, and R. Zhang, “Visible:
Video-assisted keystroke inference from tablet backside motion.” in
NDSS, 2016.
[47]
[48] T. Van Renterghem, P. Thomas, F. Dominguez, S. Dauwe, A. Touhaﬁ,
B. Dhoedt, and D. Botteldooren, “On the ability of consumer elec-
tronics microphones for environmental noise monitoring,” Journal of
Environmental Monitoring, vol. 13, no. 3, pp. 544–552, 2011.
APPENDIX B
TYPING STYLES
Hunt-and-peck typing is largely regarded as one of the most
inefﬁcient typing technique. In hunt-and-peck typing, the typer
has sight on the keyboard during typing, as in most cases the
typer does not have the keyboard layout memorized. Also,
most hunt-and-peck typers heavily use their two index ﬁngers
for typing. As a result, hunt-and-peck typers’ arms undergo
signiﬁcant movement between keystrokes.
Touch typing is largely regarded as one of the most efﬁcient
typing technique. In touch typing, the typer looks at the screen
and types continuously without looking at the keyboard. Touch
typers also utilize all ten ﬁngers.
Hybrid typing, as the name suggests, is a hybrid of hunt-and-
peck and touch typing. Like touch typers, hybrid typers may
have memorized the keyboard layout and are able to type while
looking at the screen. However, unlike touch typers, hybrid
typers utilize fewer ﬁngers, usually between 2 to 6 ﬁngers.
APPENDIX C
PREPROCESSING FIGURES
Example outputs of this background removal process are
shown in Figure 17. This background removal step makes our
proposed framework agnostic to any moving elements in the
background.
Fig. 17: Example output of the background removal process,
using DeepLabv3 and Microsoft COCO, successfully ap-
plied in different indoor and outdoor settings. Top four images
are the original frames, and bottom four images are corre-
sponding frames after background removal.
We leverage on the consistency in relative position of the
target user’s arms with respect to their face (Figure 18) in order
to segment the left and right arms in the background-removed
grayscale frame.
[49] M. Vuagnoux and S. Pasini, “Compromising electromagnetic emana-
tions of wired and wireless keyboards,” in USENIX Security Symposium,
2009.
[50] C. Wang, X. Guo, Y. Wang, Y. Chen, and B. Liu, “Friend or foe?: Your
wearable devices reveal your personal pin,” in ACM Asia Conference
on Computer and Communications Security, 2016.
[51] H. Wang, T. T.-T. Lai, and R. Roy Choudhury, “Mole: Motion leaks
through smartwatch sensors,” in International Conference on Mobile
Computing and Networking (MobiCom), 2015.
[52] Z. Wang, A. C. Bovik, H. R. Sheikh, E. P. Simoncelli et al., “Image
quality assessment: from error visibility to structural similarity,” IEEE
transactions on image processing, vol. 13, no. 4, 2004.
[53] Y. Xu, J.-M. Frahm, and F. Monrose, “Watching the watchers: Auto-
matically inferring tv content from outdoor light effusions,” in ACM
CCS, 2014.
[54] Z. Xu, K. Bai, and S. Zhu, “Taplogger: Inferring user inputs on
smartphone touchscreens using on-board motion sensors,” in ACM
Conference on Security and Privacy in Wireless and Mobile Networks
(WiSec), 2012.
[55] S. Zhang, X. Zhu, Z. Lei, H. Shi, X. Wang, and S. Z. Li, “Faceboxes: A
cpu real-time face detector with high accuracy,” in IEEE International
Joint Conference on Biometrics (IJCB), 2017, pp. 1–9.
[56] T. Zhu, Q. Ma, S. Zhang, and Y. Liu, “Context-free attacks using
keyboard acoustic emanations,” in ACM CCS, 2014.
[57] L. Zhuang, F. Zhou, and J. D. Tygar, “Keyboard acoustic emanations
revisited,” ACM Transactions on Information and System Security
(TISSEC), vol. 13, no. 1, 2009.
ANATOMY AND MOVEMENT OF ARMS DURING TYPING
APPENDIX A
Figure 16a shows an overview of the shoulder and arm
bones, and their joints. Based on the traditional position of
a webcam during a video call, lateral movements of hand
and shoulder can easily be observed in the captured video,
as shown in Figures 16b and 16c.
(b)
(c)
(a)
Fig. 16: (a) Anatomy of the human arm and shoulder bones.
(b), (c) Pixel-level heatmaps of upper body movements during
one minute of typing for left and right sides of the body,
respectively.
Fig. 18: Face detection using Faceboxes and segmentation
of left and right arms, in a frame from the captured video.
15
APPENDIX D
KEYSTROKE DETECTION ALGORITHM
ssimDif f.append(ssimList[i] − ssimList[i + 1])
armS[]
ssimList[]
ssimDif f []
keystrokesF S[]
for i in range(armS.size() − 1) do
(cid:46) Rs or Ls
(cid:46) Series of SSIM scores
(cid:46) ssimL[i] − ssimL[i + 1]
(cid:46) Store keystroke containing Rs or Ls
ssimScore = SSIM (armS[i] − armS[i + 1])
ssimList.append(ssimScore)
end for
for i in range(ssimList.size() − 1) do
Algorithm 1 Keystroke detection algorithm.
1: Input: segmented arm frames stored in armS[]
2: Output: keystroke frames stored in keystrokesF S[]
3: procedure KEYSTROKEDETECT
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29: end procedure
end for
mean ← mean(ssimDif f )
std ← standardDeviation(ssimDif f )
for i in range(ssimDif f.size() − 1) do
zScore = (ssimDif f [i] − mean)/std
if zScore > φa and zScore < φb
then
if ssimDif f [i] is a local max then
if zScore(localmin) < φc then
if local min exists between i → i + 2 then
keystrokesF S.append(armS[i])
end if
end if
end if
end if
end for
APPENDIX E