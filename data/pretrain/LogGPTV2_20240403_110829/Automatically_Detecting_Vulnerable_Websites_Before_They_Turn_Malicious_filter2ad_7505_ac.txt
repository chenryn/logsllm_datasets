 0
 0
 0.2
 0.4
 0.6
 0.8
 1
CmpImp Value
Figure 4: C.d.f. of CompImp over the style tree
generated for dailyshotofcoffee.com. We use
1,000 pages to generate the style tree.
sibly many pages within the site and has a derived prop-
erty called composite importance (CompImp), which is
an information-based measure of how important the node
is. Without getting into mathematical details—which can
be found in Yi et al. [38]—nodes which have a CompImp
value close to 1 are either unique, or have children which
are unique. On the other hand, nodes which have a Com-
pImp value closer to 0 typically appear often in the site.
While Yi et al. try to ﬁlter out nodes with CompImp
below a given threshold to extract user content, we are
interested in the exact opposite objective; so instead we
ﬁlter out nodes whose CompImp is above a threshold.
We provide an illustration with the example of
dailyshotofcoffee.com. We built, for the pur-
pose of this example, a style tree using 1,000 randomly
sampled pages from this website. We plot the Com-
pImp of nodes in the resulting style tree in Figure 4. A
large portion of the nodes in the style tree have a Com-
pImp value of exactly 1 since their content is completely
unique within the site. The jumps in the graph show
that some portions of the site may use different templates
or different variations of the same template. For exam-
ple, particular navigation bars are present when viewing
some pages but not when viewing others.
We illustrate the effect of selecting different values as
a threshold for ﬁltering in Figure 5. We consider a ran-
USENIX Association  
23rd USENIX Security Symposium  631
7
(a) Original
(b) CompImp > 0.99
(c) CompImp > 0.6
(d) CompImp > 0.1
Impact of various thresholds on ﬁltering. The ﬁgure shows how different CmpInt thresholds affect the
Figure 5:
ﬁltering of the webpage shown in (a). Thresholds of 0.99 and 0.6 produce the same output, whereas a threshold of 0.1
discards too many elements.
I
p
m
p
m
C
<
s
e
d
o
N
f
o
n
o
i
t
c
a
r
F
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
 0
I
p
m
p
m
C
<
s
e
d
o
N
f
o
n
o
i
t
c
a
r
F
 0.2
 0.4
 0.6
 0.8
 1
CmpImp Value
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
 0.2
 0.4
 0.6
 0.8
 1
CmpImp Value
Figure 6: C.d.f. of CompImp over all style trees gen-
erated for 10,000 random sites. We use 1,000 pages per
website to generate the style tree.
Figure 7: C.d.f. of CompImp over the style tree gen-
erated for dailyshotofcoffee.com using only 5
pages. The plot represents the fraction of nodes less than
a threshold in the style tree generated from only ﬁve ran-
dom pages from dailyshotofcoffee.com.
dom page of dailyshotofcoffee.com showed in
Figure 5(a). In Figures 5(b), (c), and (d), we show the re-
sult of ﬁltering with a threshold value of 0.99, 0.6 and 0.1
respectively. There is no difference between using 0.99
and 0.6 as a threshold since there are very few nodes in
the style tree that had a CompImp between 0.6 and 0.99,
as shown in Figure 4. There is a notable difference when
using 0.1 as a threshold since portions of the page tem-
plate are present on some but not all pages of the site.
In general, style trees generated for other sites seem to
follow a similar distribution, as shown in Figure 6 where
we plot the aggregated CompImp c.d.f. over all style
trees generated for 10,000 sites. The aggregation does
have a slight curve around the CompImp value 1 which
indicates that a few sites do have style trees with nodes in
this space. Such sites typically use a ﬁxed template with
the exception of a few pages such as 404 error pages,
login, and registration pages.
A concern with applying style trees for ﬁltering in this
setting occurs in instances where there are only a few ex-
amples of pages from a particular site. Trivially, if there
is only a single page from the site, then the style tree is
just the page itself, and if only a few examples are found
then the estimates of nodes in the style tree will be highly
dependent on where in the site those pages were sampled
from. In Figure 7, we plot the cumulative distribution
function for CompImp over the style tree generated for
dailyshotofcoffee.com, but this time, only using
ﬁve random pages from the site. Compared to Figure 4,
we see that the particular cutoffs are slightly different
from when we used 1,000 pages; but the general shape
still remains the same. Manual inspection over many
sites has indicates that this approach still works well with
as few as ﬁve pages. This serves as further justiﬁcation
to our design decision of only scraping 20 pages at most
from each website.
With all of these experiments in mind, we selected
a thresholding value of 0.99 for our system, and elim-
inated from our ﬁltering process sites where we could
only scrape less than ﬁve pages.
4.3 Feature extraction
We derived the set of features used in classiﬁcation from
two main sources, the Alexa Web Information Service
632  23rd USENIX Security Symposium 
USENIX Association
8
Feature
AWIS Site Rank
Links to the site
Load percentile
Adult site?
Reach per million
Discretization fn.
(cid:31)log(SiteRank)(cid:30)
(cid:31)log(LinksIn)(cid:30)
(cid:29)LoadPercentile/10(cid:28)
(Boolean)
(cid:31)log(reachPerMillion)(cid:30)
Values
[0 . . .8]
[0 . . .7]
[0 . . .10]
{0,1}
[0 . . .5]
Table 1: AWIS features used in our classiﬁer and
their discretization. Those features are static—i.e., they
are used whenever available.
(AWIS) and the content of the saved web pages. Fea-
tures generated from the pages content are dynamically
generated during the learning process according to a cho-
sen statistic. For the classiﬁcation process we use an en-
semble of decision trees so that the input features should
be discrete.
In order to obtain useful discrete features
(i.e., discrete features that do not take on too many val-
ues relative to the number of examples), we apply a map-
ping process to convert continuous quantities (load per-
centile) and large discrete quantities (global page rank)
to a useful set of discrete quantities. The mappings used
are shown in Table 1.
4.3.1 AWIS features
For every site that was scraped, we downloaded an en-
try from AWIS on Feb 2, 2014. While the date of the
scrape does not match the date of the web ranking infor-
mation, it can still provide tremendous value in helping
to establish the approximate popularity of a site. Indeed,
we observed that in the overwhelming majority of cases,
the ranking of sites and the other information provided
does not change signiﬁcantly over time; and after dis-
cretization does not change at all. This mismatch is not
a fundamental consequence of the experiment design but
rather a product retrospectively obtaining negative train-
ing examples (sites which did not become compromised)
which can be done in real time.
Intuitively, AWIS information may be useful because
attackers may target their resources toward popular hosts
running on powerful hardware. Adversaries which host
malicious sites may have incentives to make their own
malicious sites popular. Additionally, search engines are
a powerful tool used by attackers to ﬁnd vulnerable tar-
gets (through, e.g., “Google dorks [18]”) which causes a
bias toward popular sites.
We summarize the AWIS features used in Table 1. An
AWIS entry contains estimates of a site’s global and re-
gional popularity rankings. The entry also contains es-
timates of the reach of a site (the fraction of all users
that are exposed to the site) and the number of other
sites which link in. Additionally, the average time that
it takes users to load the page and some behavioral mea-
surements such as page views per user are provided.
The second column of Table 1 shows how AWIS infor-
mation is discretized to be used as a feature in a decision-
tree classiﬁer. Discretization groups a continuous feature
such as load percentile or a large discrete feature such as
global page rank into a few discrete values which make
them more suitable for learning. If a feature is contin-
uous or if too many discrete values are used, then the
training examples will appear sparse in the feature space
and the classiﬁer will see new examples as being unique
instead of identifying them as similar to previous exam-
ples when making predictions.
For many features such as AWIS Site Rank, a loga-
rithm is used to compress a large domain of ranks down
to a small range of outputs. This is reasonable since for
a highly ranked site, varying by a particular number of
rankings is signiﬁcant relative to much lower ranked site.
This is because the popularity of sites on the Internet fol-
lows a long tailed distribution [9].
Dealing with missing features. Some features are not
available for all sites, for example information about the
number of users reached by the site per million users was
not present for many sites. In these cases, there are two
options. We could reserve a default value for missing
information; or we could simply not provide a value and
let the classiﬁer deal with handling missing attributes.
When a decision-tree classiﬁer encounters a case of
a missing attribute, it will typically assign it either the
most common value for that attribute, the most common
value given the target class of the example (when train-
ing), or randomly assign it a value based on the estimated
distribution of the attribute. In our particular case, we
observed that when a feature was missing, the site also
tended to be extremely unpopular. We asserted that in
these cases, a feature such as reach per million would
probably also be small and assigned it a default value.
For other types of missing attributes such as page load
time, we did not assign the feature a default value since
there is likely no correlation between the true value of
the attribute and its failure to appear in the AWIS entry.
4.3.2 Content-based features
The content of pages in the observed sites provides ex-
tremely useful information for determining if the site will
become malicious. Unlike many settings where learning
is applied, the distribution of sites on the web and the
attacks that they face vary over time.
Many Internet web hosts are attacked via some ex-
ploit to a vulnerability in a content-management system
(CMS), that their hosted site is using. It is quite common
for adversaries to enumerate vulnerable hosts by look-
ing for CMSs that they can exploit. Different CMSs and
even different conﬁgurations of the same CMS leak in-
formation about their presence through content such as
tags associated with their template, meta tags, and com-
USENIX Association  
23rd USENIX Security Symposium  633
9
ments. The set of CMSs being used varies over time:
new CMSs are released and older ones fall out of favor,
as a result, the page content that signals their presence is
also time varying.
To determine content-based features, each of the pages
that survived the acquisition and ﬁltering process de-
scribed earlier was parsed into a set of HTML tags.
Each HTML tag was represented as the tuple (type, at-
tributes, content). The tags from all the pages in a site
were then aggregated into a list without repetition. This
means that duplicate tags, i.e., tags matching precisely
the same type, attributes and content of another tag were
dropped. This approach differs from that taken in typi-
cal document classiﬁcation techniques where the docu-
ment frequency of terms is useful, since for example a
document that uses the word “investment” a dozen times
may be more likely to be related to ﬁnance than a docu-
ment that uses it once. However, a website that has many
instances of the same tag may simply indicate that the
site has many pages. We could balance the number of
occurrences of a tag by weighting the number of pages
used; but then, relatively homogeneous sites where the
same tag appears on every page would give that tag a
high score while less homogeneous sites would assign a
low score. As a result, we chose to only use the existence
of a tag within a site but not the number of times the tag
appeared.
During the training phase, we then augmented the lists
of tags from each site with the sites’ classiﬁcation; and
added to a dictionary which contains a list of all tags
from all sites, and a count of the number of positive and
negative sites a particular tag has appeared in. This dic-
tionary grew extremely quickly; to avoid unwieldy in-
crease in its size, we developed the following heuristic.
After adding information from every 5,000 sites to the
dictionary, we purged from the dictionary all tags that
had appeared only once. This heuristic removed approxi-
mately 85% of the content from the dictionary every time
it was run.
Statistic-based extraction. The problem of feature ex-
traction reduces to selecting the particular tags in the dic-
tionary that will yield the best classiﬁcation performance
on future examples. At the time of feature selection, the
impact of including or excluding a particular feature is
unknown. As a result, we cannot determine an optimal
set of features at that time. So, instead we use the follow-
ing technique. We ﬁx a number N of features we want to
use. We then select a statistic ˆs, and, for each tag t in the
dictionary, we compute its statistic ˆs(t). We then simply
take the top-N ranked entries in the dictionary according
to the statistic ˆs.
Many statistics can be used in practice [12]. In our im-
plementation, we use N = 200, and ˆs to be ACC2. ACC2
is the balanced accuracy for tag x, deﬁned as:
ˆs(x) =(cid:31)(cid:31)(cid:31)(cid:31)
|{x : x ∈ w,w ∈ M}|
|M|
− |{x : x ∈ w,w ∈ B}|
|B|
(cid:31)(cid:31)(cid:31)(cid:31) ,
where B and M are the set of benign, and malicious
websites, respectively; the notation x ∈ w means (by a
slight abuse of notation) that the tag x is present in the
tag dictionary associated with website w. In essence, the
statistic computes the absolute value of the difference be-
tween the tag frequency in malicious pages and the tag
frequency in benign pages.
A key observation is that these top features can be pe-
riodically recomputed in order to reﬂect changes in the
statistic value that occurred as a result of recent exam-
ples. In our implementation, we recomputed the top fea-
tures every time that the decision tree classiﬁers in the
ensemble are trained.
As the distribution of software running on the web
changes and as the attacks against websites evolve, the
tags that are useful for classiﬁcation will also change. A
problem arises when the dictionary of tags from previ-
ous examples is large. For a new tag to be considered a
top tag, it needs to be observed a large number of times
since |M| and |B| are very large. This can mean that
there is a signiﬁcant delay between when a tag becomes
useful for classiﬁcation and when it will be selected as a
top feature, or for example in the case of tags associated
with unpopular CMSs, which will never be used.
A way of dealing with this problem is to use window-
ing, where the dictionary of tags only contains entries
from the last K sites. By selecting a sufﬁciently small
window, the statistic for a tag that is trending can rapidly
rise into the top N tags and be selected as a feature. The
trade-off when selecting window size is that small win-
dow sizes will be less robust to noise but faster to capture
new relevant features while larger windows will be more
robust to noise and slower to identify new features.
An additional strategy when calculating the statistic
value for features is to weight occurrences of the feature
differently depending on when they are observed. With
windowing, all observations in the window are weighted
equally with a coefﬁcient of 1, and all observations out-