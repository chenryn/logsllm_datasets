on the size of the buffer. Such a scenario typically happens in last-
mile networks where the capacity of the link between the endpoint
and the provider network is considerably smaller than backbone
or interconnect links.
IMC ’17, Internet Measurement Conference, November 1–3, 2017, London, United Kingdom
Sundaresan et al.
• External congestion: In this case the ﬂow starts on a path con-
taining a link that is already congested, meaning that the available
capacity of that link is low and the buffer is already full or close
to full, causing packets to queue. The state of this link reduces the
ability of the ﬂow to scale throughput before it encounters loss.
The existing buffer occupancy increases the baseline latency of
the path, and at the same time reduces variation, because the new
ﬂow’s impact on the state of the buffer is much smaller than in the
case of self-induced congestion.
We run two simple experiments over an emulated 20 Mbps “ac-
cess link” served by a 1 Gbps “interconnect” link to illustrate the
two cases. Figure 1a shows the CDF of the difference between the
maximum and minimum RTT during the slow start phase. We de-
ﬁne slow start as the period up to the ﬁrst retransmission or fast
retransmission. We see that the difference when the congestion is
self-induced is roughly 100 ms, which is the size of the access link
buffer that we emulate. This is what we expect, because this buffer
ﬁlls up when the ﬂow self-induces congestion. In the case of the ex-
ternal congestion, the difference is much smaller, because the ﬂow
encounters congestion at the 1 Gbps link. This congestion becomes
part of the baseline RTT for the ﬂow packets, leaving a smaller dif-
ference between the maximum and the minimum. The coefﬁcient of
variation of the RTT measurements (Figure 1b) also shows a simi-
lar pattern: the variation is smaller for external congestion than it is
for self-induced congestion, because the impact of the buffer on the
RTT is lower for the former case.
We use this phenomenon to distinguish the two cases. Our intu-
ition hints at multiple potential metrics that one could use to mea-
sure the evolution of RTT: e.g.we could track the growth of the RTT
to see if it increases monotonically. However, we decide on two sim-
ple metrics that are easy to compute from the ﬂow RTT samples:
(1) Normalized difference between the maximum and minimum
RTT during slow start (NormDiff): We measure the differ-
ence between the maximum and minimum RTT during slow
start and normalize it by the maximum RTT. This metric mea-
sures the effect of the ﬂow on the buffer—it gives us the size
of the buffer that the ﬂow ﬁlls—without being affected by
the baseline RTT; a ﬂow that ﬁlls up the buffer will have a
higher value than one that encounters a full buffer.
(2) The coefﬁcient of variation of RTT samples during slow start
(CoV): This metric is the standard deviation of RTT sam-
ples during slow start normalized by the average. This metric
measures the smoothing effect of the buffer on RTT while
minimizing the effect of the baseline RTT. A ﬂow that expe-
riences self-induced congestion will see higher values of the
CoV, because the RTT increases as the buffer ﬁlls up. The
RTT for externally congested ﬂows will be dominated by an
already full buffer, and so the CoV will be lower.
Together, these metrics are robust to a wide range of buffer sizes.
Although there are corner cases where the model could fail, particu-
larly in case of highly occupied, but not fully congested buffers, we
note that the notion of congestion becomes fuzzy in those cases any-
way. We use these metrics to build a standard, simple decision tree
that can accurately classify the two congestion events. We restrict
our RTT samples to the ﬁrst slow-start period when TCP’s behavior
is more predictable because it starts from zero—the RTT, and the
path buffer state are at baseline. The path and its congestion char-
acteristics are less likely to change during this period than over the
(longer) course of the entire ﬂow lifetime. Our experiments show
that the throughput achieved during slow start is not always indica-
tive of the throughput achieved during the lifetime of the ﬂow, but
it is indicative of the capacity of the bottleneck link during a self-
induced congestion event. Therefore, our techniques are also useful
as a starting point to estimate the link capacity, particularly in cases
where the ﬂow throughput changes during the course of the ﬂow.
3 CONTROLLED EXPERIMENTS
We describe the custom testbed we use to run controlled experiment
that emulate the type of ﬂows we want to classify. We use these
ﬂows to build our decision tree classiﬁer.
3.1 Experiment Setup
We run throughput tests between a client and a server in a local net-
work connected via two links that we shape to effect the two kinds
of congestion that we discuss in § 2; self-induced, and external. Our
testbed (Figure 2) can emulate a wide range of last-mile and core
network conditions.
Testbed hardware
The testbed consists of two Raspberry Pi 2 devices, two Linksys
WRT1900AC routers, a combination of Gigabit and 100M Ether-
net links, and various servers on the Internet. We use the Raspberry
Pis and the Linksys devices as end- and control- points for test traf-
ﬁc. The Pis have a quad-core 900 MHz ARM7 processor, 1 GByte
RAM, and a 100 Mbit NIC. The routers have a dual-core 1300 MHz
ARM7 processor, 128 Mbytes RAM, and a 1 Gbps NIC. The testbed
is physically located at the San Diego Supercomputing Center in
San Diego, CA.
Emulating core and last-mile network links
In Figure 2, AccessLink emulates representative access-link condi-
tions: we use tc with a token-bucket with a 5 KByte burst ﬁlter
to set its bandwidth to 10 Mbps, 20 Mbps, and 50 Mbps, loss to
0.02% and 0.05%, and latencies to 20 ms and 40 ms, with jitter set
to 2 ms. We also utilize three buffer sizes for AccessLink: approx-
imately 20 ms, 50 ms, and 100 ms; the ﬁrst setting is on the lower
end of buffer size for last-mile networks, while the last setting is
lower than the maximum buffer we have seen. For example, the
buffer sizes in three homes that we tested on were approximately
25 ms, 45 ms, and 180 ms. We use low buffer values to test the lim-
its of our hypothesis: the larger the buffer, the more likely it is that
our hypothesis will work.
InterConnectLink, connecting Router 1 and Router 2 in the ﬁg-
ure, emulates an interdomain link at 950 Mbps with a 50 ms buffer
(we shape it to 950 Mbps, slightly less than its 1 Gbps capacity in
order to ensure that our experiments utilize the buffer). We do not
add latency or loss to this link, though the buffer could naturally
induce latency and loss when it is occupied.
TCP Congestion Signatures
IMC ’17, Internet Measurement Conference, November 1–3, 2017, London, United Kingdom
TG trans
Pi 2
TG cong
Router 2
InterConnectLink
Router 1
Link 3
Internet
Server 1
Pi 1
AccessLink
100 Mbps link
Pi 1 runs throughput tests to Server 1
Shaped "access" link
Pi 2 runs 100 Mbps cross traffic to Servers 3, 4
1 Gbps link
Router 2 runs 1 Gbps cross traffic to Server 2
Servers 2, 3, 4
Figure 2: Experimental testbed. Router 2 connects to Router 1 using InterConnectLink, and Router 1 connects to a university network
using Link 3. Both InterConnectLink and Link 3 have a capacity of 1 Gbps. Pi 1 and Pi 2 connect to Router 2 over a 100 Mbps link,
which is limited by the Pi NIC. We emulate access links using AccessLink and a shaper on Router 2. We emulate an interdomain link
using InterConnectLink and a shaper on Router 1.
We acknowledge the difﬁculty in getting precise numbers for the
networks we are emulating, but we believe our settings capture a
wide range of real-world access networks.
Emulating cross-trafﬁc and congestion
We use two kinds of cross-trafﬁc generators that we built ourselves
to emulate real networks. The ﬁrst trafﬁc generator, TGtrans, written
in Go [47] runs on Pi 2 and fetches ﬁles over HTTP from Servers 2
and 3 using a random process. These servers are located at the Inter-
national Computer Science Institute in Berkeley, CA, and the Geor-
gia Institute of Technology in Atlanta, GA, 20 ms and 60 ms away
respectively. The generator fetches objects of size 10KB, 100KB,
1MB, 10 MB, and 100 MB, with the fetch frequency for an object in-
versely proportional to its size. Since TGtrans bypasses AccessLink,
and can only generate a maximum demand of 100 Mbps (due to the
Pi NIC limitation), it does not congest InterConnectLink. However,
it provides transient cross-trafﬁc on InterConnectLink which intro-
duces natural variation; we run TGtrans during all our experiments.
The second trafﬁc generator, TGcong, runs on Router 2, and is a
simple bash script that fetches a 100 MB ﬁle from Server 4 (which
is less than 2 ms away) repeatedly using 100 concurrent curl pro-
cesses. TGcong emulates interdomain link bottlenecks by saturating
InterConnectLink (capacity 950 Mbps); we run TGcong for experi-
ments that require external congestion.
Throughput experiments
We use netperf to run 10-second downstream throughput tests
from Server 1 to Pi 1. We capture packet traces on Server 1 us-
ing tcpdump for each test, which we use for analysis. We run
two types of experiments. First, we run netperf without congest-
ing InterConnectLink, but with transient cross-trafﬁc using TGtrans.
This yields data for ﬂows with self-induced congestion, because
netperf saturates AccessLink, our emulated access link. We then
run netperf along with both cross-trafﬁc generators. The second
cross-trafﬁc generator, TGcong, saturates InterConnectLink, which
now becomes the the bottleneck link in the path. This scenario emu-
lates a path with external congestion. For each throughput, latency,
and loss combination, we run 50 download throughput tests.
What constitutes access-link congestion?
There is no ﬁxed notion of what constitutes acceptable throughput
as a fraction of link capacity; however, we would expect it to be
close to 1. This congestion threshold is important for us to label
our test data as incurring self-induced congestion or external con-
gestion. We therefore do not set the threshold arbitrarily: study the
impact of a range of values of this threshold on our model and the
classiﬁcation of congestion, and show that our results are robust to
a range of reasonable threshold values.
Labeling the test data
We use the congestion threshold for labeling the test data. We la-
bel the throughput tests that achieve throughput greater than this
threshold during the slow start phase as self-induced congestion.
For example, if we set AccessLink throughput in the testbed to
20 Mbps, and the threshold to 0.8, then we label ﬂows that expe-
rience a slow-start throughput of greater than 16 Mbps as experi-
encing self-induced congestion. We do not use just the cross-trafﬁc
information to label the data, because inherent variability in the
testbed result in some tests not achieving the access link through-
put even if there is no external congestion, and vice-versa (some
tests achieve access link throughput even when we are running both
TGcong and TGtrans; this could be because of transient issues such
as some cross-trafﬁc process threads restarting, and other TCP inter-
actions, particularly when our emulated access link throughputs are
low). However, these form a small fraction of the tests, and we ﬁlter
these out, and we label the remaining data as externally congested.
3.2 Analysis and Model
We extract the RTT features from the Server 1 packet traces. We
use tshark to obtain the ﬁrst instance of a retransmission or a
fast-retransmission, which signals the end of slow start. We then
collect all downstream RTT samples up to this point; an RTT sam-
ple is computed using a downstream data packet and its correspond-
ing ACK at the server. For statistical validity, we discard ﬂows that
have fewer than 10 RTT samples during slow-start. We compute
NormDiff and CoV using these samples.
Building and Tuning the Decision-tree Classiﬁer We use the
python sklearn library implementation [42] to automatically
build the decision tree classiﬁer [46] using the NormDiff and the
IMC ’17, Internet Measurement Conference, November 1–3, 2017, London, United Kingdom
Sundaresan et al.
1.0
0.8
0.6
0.4
0.2
0.0
0.0
External
Self
0.2
0.4
0.6
0.8
1.0
Threshold
(a) Prediction precision
External
Self
0.2
0.4
0.6
0.8
1.0
Threshold
(b) Prediction recall
Figure 3: Model performance: we see that precision and recall are high for a wide range of threshold values.
1.0
0.8
0.6
0.4
0.2
0.0
0.0
100
V
o
C
10−1
not want to build a model that is extremely sensitive to this pa-
rameter either. We therefore test a range of threshold values and
show that our results are robust to these values.
3.3 Controlled Experiments Results
We obtained robust results from our decision-tree classiﬁer on our
test data without having to carefully tune it. Figure 3 shows how
the congestion threshold affects the model, and its impact on pre-
diction precision, and recall, for both classes of congestion. Lower
thresholds, e.g. below 0.3, lead to poor results for predicting exter-
nal congestion, while high thresholds, e.g. greater than 0.95, lead to
poor results for predicting self-induced congestion. The precision
and recall are consistently high for a wide range of values between
0.3 and 0.9, however, indicating that the model is therefore accu-
rate and robust to a choice of threshold in that range. Good results
for thresholds as low as 0.3 is partly because we only have a small
number of data samples in the region between 0.3 and 0.6 (only
about 12% of our sample), due to the difﬁculty in reliably conﬁgur-
ing the testbed for middling throughput. We therefore only consider
the region which have a high number of samples and good results—
between 0.6 and 0.9.
Why do we need both metrics? Both NormDiff and CoV are a
function of the same underlying phenomenon, that is, the behavior
of the buffer at a congested (versus an uncongested) link. Intuitively,
we expect that the NormDiff parameter performs strongly as an in-
dicator of congestion type on paths with relatively large buffers and
relatively low latency and loss. In such cases, the ﬂow can ramp up
quickly and ﬁll up the buffer. The CoV parameter gives more accu-
rate classiﬁcation across paths with smaller buffers, and higher loss
and latency, because even if NormDiff is lower, the signature of a
buffer that is ﬁlling is captured by CoV.
Self
External
100
10−1
NormDiff
Figure 4: Raw NormDiff and CoV metrics for our controlled
experiments. We see that both metrics are useful to separate
the two types of congestion events.
CoV RTT parameters for classiﬁcation. Our classiﬁer has two tune-
able parameters: the depth of the tree, and the threshold we use for
estimating whether the ﬂow experienced access-link congestion.
• Tree-depth The tree depth for any decision tree classiﬁer has to
strike a balance between building a good model and overﬁtting
for the test data. Since we only have two input parameters to the
decision tree, and two output classes, we keep the tree simple. We
evaluate tree depths between 3 and 5. We get high accuracy and
low false-positives with all three depths. For the rest of the paper,
we use a tree depth of 4.
• Congestion Threshold Since the congestion threshold deter-
mines how we label the test data, it has a direct impact on the
classiﬁer. A threshold that is too high, e.g., close to 1, risks misla-
beling ﬂows that self-induce congestion as externally congested,
because we will label even ﬂows that achieve a large fraction of
capacity as externally congested. Similarly, a threshold that is too
low risks mislabeling ﬂows that are externally congested. We do
TCP Congestion Signatures
IMC ’17, Internet Measurement Conference, November 1–3, 2017, London, United Kingdom
Figure 4 plots the two metrics for our controlled experiments
data: we see that while the two points are largely separated on ei-
ther axis, there is also a signiﬁcant overlap—therefore we use both
metrics in order to cover a wide range of real-world scenarios.
The impact of multiplexing In our controlled experiments, we use
a clear access link and, for congestion in interdomain links, we in-
troduce cross-trafﬁc with 100 concurrent bulk transfers. However,