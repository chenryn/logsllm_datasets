intrusion detector as a high-level workﬂow consisting of ﬁve key phases: (1) data
collection, (2) data preparation, (3) training and tuning, (4) testing, and (5)
measurement. Each phase is annotated with factors that contribute errors to-
wards the ﬁnal detector performance. We brieﬂy describe the phases (referenced
by a two letter acronym), followed by a description of the factors in each phase.
3.1 Data Collection (DC)
The ﬁrst stage in the evaluation of an anomaly-based intrusion detector involves
the collection of both normal and abnormal (attack) instances of data, where the
resulting evaluation dataset should ideally be well labeled and characterized. The
following ﬁve broad factors are known to contribute errors to the data collection
phase.
Data generation (DC1) - Raw data is needed for an evaluation. Live environ-
ments that generate real data have been observed to contain noisy artifacts
that introduce experimental confounds [18]. Artiﬁcially generated data may
provide good control but introduce errors with respect to ﬁdelity to real
system behavior [18].
Data monitoring (DC2) - Errors can be introduced by data monitors them-
selves, e.g., strace has been shown to inject strange parameter values when
monitoring jobs with hundreds of spawned children [19], or when following
children forked using the vfork() system call [14].
Data reduction (DC3) - Techniques employed to reduce the volume of input
data, e.g., data sampling, can distort features in captured data that in turn
Deconstructing the Assessment of Anomaly-based Intrusion Detectors
291
adversely inﬂuences the performance of anomaly detectors [10]. Ringberg et
al. [11] suggest that the use of data reduction techniques can lead to poor
quality data that can aﬀect the identiﬁcation of true-positives in a dataset.
Data characterization (DC4) - An understanding of what a dataset contains is
fundamental to evaluation [5, 18]. Errors can be introduced when ground
truth is poorly established [18, 11, 5, 4, 13, 17], and it has been argued that
even the availability of only partial ground truth is not good enough because
it would make it impossible to calculate accurate FN and FP rates [11] (fac-
tor DC4.1). Similarly, a poor characterization of the anomalous-yet-benign
instances in data can result in an unreliable assessment of a detector’s false
alarm rate [18] (factor DC4.2).
3.2 Data Preparation (DP)
Data preparation primarily refers to techniques that process the data into a
form suitable for evaluation purposes, or for detector consumption. We note
that, although data preparation can contribute to errors, there are cases where
data preparation might be necessary to reduce a detector’s error. For instance,
several machine-learning based methods work better if the inputs are normalized
and standardized (e.g., artiﬁcial neural networks can avoid getting stuck in local
optima if the inputs are normalized).
Data sanitization (DP1) - The choice of a particular data sanitization strategy
(or a lack of it) to clean the data of unwanted artifacts has been shown to
signiﬁcantly perturb the outcome of anomaly detectors [20].
Data partitioning (DP2) - An improper choice of the data partitioning strategy
(or even the parameter values within a particular strategy such as the choice
of k in k-fold cross validation), can lead to an error-prone result when as-
sessing anomaly detector performance. Kohavi et al. [21] reviewed common
methods such as holdout, cross-validation, and bootstrap and discussed the
performance of each in terms of their bias and variance on diﬀerent datasets.
Data conditioning (DP3) - The choice of data conditioning strategy can have
implications for the performance of an anomaly detector, e.g., data transfor-
mations such as centering and scaling continuous data attributes can bias
the performance of learning algorithms [22].
3.3 Training and Tuning (TR)
In the training phase, an anomaly-based intrusion detector consumes training
data to generate models of nominal behavior that are used in turn to identify
oﬀ-nominal events. Training data can also be used to ﬁne-tune the parameters
governing the anomaly detector’s learning and modeling algorithms to enable
the generation of more representative models of system behavior. Errors are
introduced in the training phase due to factors inﬂuencing the training data, the
learning process or the overall training strategy.
292
A. Viswanathan, K. Tan, and C. Neuman
Characteristics of training data (TR1).
Representation of real-world behavior in data (TR1.1): Training data must be
representative of system behavior. Real-world behavior is often dynamic and
evolving in nature and, if captured inadequately can lead to inadequate
training, increased error (e.g., false alarms) and biased detector performance,
i.e. the problem of concept drift [23, 24, 4].
Stability of training data (TR1.2): As discussed by Lee et al. [9] and Sommer
and Paxson [5], the basic premise of anomaly detection rests on an assump-
tion that there exists some stability or regularity in training data that is
consistent with the normal behavior and thus distinct from abnormal behav-
ior. Real-world data displays high variability and rarely well behaved [18, 5].
Highly variable training data can cause a detector to learn a poorly ﬁtted
baseline model, which would aﬀect its error rate when deployed.
Attack-free training data (TR1.3): The need for attack-free training data has
been identiﬁed in several papers [5, 4, 20]. If the training data is polluted
with attacks, the detector can learn the attacks as nominal behavior, causing
a probable increase in the miss rate [20].
Detector internals (TR2).
Choice of data features (TR2.1): An anomaly detector can detect attacks over
multiple types of data and over diﬀerent features of the data. An incorrect
choice of data types or features directly aﬀects a detector’s accuracy [17, 6].
Modeling formalism (TR2.2): A poor choice of modeling formalism or an inad-
equately complex model can aﬀect the accuracy of a detector. For instance,
n-gram models were found to better model packet payloads than the 1-gram
model [25]. Kruegel et al. [26] reported good results for detecting web attacks
using a linear combination of diﬀerent models, with each model capturing a
diﬀerent aspect of web-server requests.
Learning parameters (TR2.3): Learning algorithms are inﬂuenced by their pa-
rameters [22]. Incorrect parameter choices can adversely aﬀect detector per-
formance. For example, in the seminal work by Forrest et al. [14], the value
of window size parameter was a deciding factor for the performance of the
anomaly detector.
Online vs. Oﬄine Training (TR2.4): The choice of learning strategy can have an
inﬂuence on the detector performance. An oﬄine training strategy, wherein
a detector is trained before deployment can suﬀer from high error rates due
to concept drift in dynamic environments [4]. An online learning strategy,
wherein a detector continuously learns from its inputs has been shown in
some contexts to reduce the error rates [6]. However, in some cases, an online
learning strategy can induce more errors in the detector’s performance if the
concept drift is artiﬁcially induced by an attacker.
Amount of training (TR3). The amount of training can either be measured in
terms of training time or size of data used for training and has been shown to
be heavily correlated with detector error rates [6].
Deconstructing the Assessment of Anomaly-based Intrusion Detectors
293
Model generation approach (TR4). Errors are introduced due to the choice of
training strategy adopted for generating model instances (e.g., one-class vs. two-
class training strategy) [17]. For example, to detect anomalies in a particular
network X, a classiﬁer could be trained using normal data from network X, or a
classiﬁer could be trained using data from another similar network Y. The two
approaches result in two diﬀerent classiﬁers with diﬀerent errors.
3.4 Testing (TS)
The test phase is concerned with exercising detection capabilities on test data
that ideally consists of a labeled mixture of normal and attack data sequences.
The detector ﬂags any deviations from the nominal behavior as attacks and
produces a set of alarms. The test phase performance is inﬂuenced by factors
related to the test data and the detector’s detection strategy.
Characteristics of test data (TS1).
Ratio of attack-to-normal data (TS1.1): The base-rate or the ratio of attacks
to normal data instances can signiﬁcantly bias the evaluation results of an
anomaly detector to a particular dataset [27]. The attack data, if generated
artiﬁcially must be distributed realistically within the background noise [18].
Stability of attack signal (TS1.2): Current evaluation strategies implicitly as-
sume that the attack signal itself is a stable quantity, i.e., the attack signal
will manifest in a consistent way giving evaluation results some degree of
longevity beyond the evaluation instance. However, an attack signal could
manifest unstably for one or both of the following reasons: 1) Adversary-
induced instability, wherein an attacker might distort an attack signal by
generating artiﬁcial noise that makes the attack signal appear normal to a
detector [16, 15] (factor TS1.2.1), and 2) Environment-induced instability,
where an attack signal may get distorted due to variations in the operating
environment (factor TS1.2.2). For example, an attack signal represented as
a sequence of system calls from a process is easily perturbed due to addition
of noisy system calls, injected by the process in response to the variations in
memory or load conditions in the underlying OS.
Detector internals (TS2).
Detection parameters (TS2.1): The performance of detection algorithms is sen-
sitive to the choice of parameters such as detection thresholds. For example,
Mahoney et al. [28] show the variation in their detector’s hit and miss perfor-
mance when the detection thresholds were varied for the same test dataset.
Detection parameters are either chosen manually by the evaluator [29, 28]
or are automatically computed at runtime by the detector [26].
Choice of similarity measure (TS2.2): It is well acknowledged that the choice of
the similarity measure used to determine the magnitude of deviations from
the normal proﬁle greatly inﬂuences the accuracy of a detector [17, 6].
294
A. Viswanathan, K. Tan, and C. Neuman
3.5 Measurement (MS)
Given the set of detector responses from the test phase along with ground truth
established for a test corpus, the performance of the detector is measured in
terms of the true positives, false positives, false negatives and true negatives.
There are at least two factors that can inﬂuence the measurements.
Deﬁnition of metrics (MS1): When measuring or comparing the performance of
detectors, it is crucial to understand two categories of metrics: (a) the four
fundamental metrics – true positive (TP) or “hit”, false negative (FN) or
“miss”, false positive (FP), true negative (TN), and (b) the overall per-
formance metrics such as the TP rate or the FP rate of a detector. The
fundamental metrics are tied to the interpretation of detector alarms. For
instance, a true positive (hit) could be deﬁned as any single alarm from the
detector over the entire duration of an attack, or as a speciﬁc alarm within
a speciﬁc time window. The overall measurement of performance could be
expressed as a percentage (e.g., total over the expected true positives), or
may be expressed operationally (e.g.: false positives/day). An improper def-
inition of the above metrics with respect to the chosen test data and/or the
operational environment can signiﬁcantly bias a detector’s assessment and
render performance comparisons across diﬀerent detectors inconclusive [18].
Deﬁnition of anomaly (MS2): Anomalies themselves possess distinctive charac-
teristics, for example, they could be point anomalies, collective anomalies or
contextual anomalies [17]. Errors are introduced when it is assumed that a
detector is capable of detecting a particular kind of anomaly that is not in
its repertoire [12, 11, 13].
4 Deconstruction of Evaluation Results
This section focuses on three basic questions that must be answered when con-
sidering deployment on operational systems: (1) Can anomaly detector D detect
attack A? (2) Can anomaly detector D detect attack A consistently? (3) Why?
An evaluation strategy aimed at answering the questions above must provide
evidence to support that (a) every “hit” or “miss” assigned to a detector is
valid, i.e., the hit or miss is attributable purely to detector capability and not
to any other phenomenon such as poor experimental control, and (b) the “hit”
or “miss” behavior corresponding to an attack is consistent, i.e., the hit or miss
result for a detector for a given attack is exhibited beyond that single attack
instance.
We use the framework presented in Sect. 3 to analyze the validity and con-
sistency arguments of evaluation results for an anomaly detector. Speciﬁcally,
we (1) identify the sequence of logical events that must occur for the evaluation
results to be valid and consistent (Sect. 4.1), (2) identify the error factors that
can perturb the validity and consistency of evaluation results (Sect. 4.2), and
(3) explain the conclusions that can be drawn from evaluation results within the
error context (Sect. 4.3).
Deconstructing the Assessment of Anomaly-based Intrusion Detectors
295
4.1 Validity and Consistency of Detection Results
As shown in Fig. 2, given an attack instance as test input, there are at least seven
logical events that are necessary for reasoning about the validity and consistency
of the detection result, that is, a “hit” or a “miss”.
Validity. To determine that an anomaly-based intrusion detector has registered
a valid hit, the following six events must occur (Fig. 2): (1) the attack must
be deployed, (2) the attack must manifest in the evaluation data stream, (3)
the attack manifestation must be present in the subset of the evaluation data
(the test data) consumed by the detector, (4) the attack manifestation must be
anomalous within the detector’s purview, (5) the anomaly must be signiﬁcant
enough to be ﬂagged by the detector, and (6) the detector response must be
is not included
measured appropriately, in this case, as a “hit”. Note that event 3
above as it only aﬀects the consistency of detection.
(cid:3)



$WWDFNLV
GHSOR\HG
$WWDFN
PDQLIHVWVLQ
HYDOXDWLRQ
GDWD
$WWDFN
PDQLIHVWVLQ
WHVWGDWD
¶
$WWDFN
PDQLIHVWV
VWDEO\
[
[
(YHQWIRUDYDOLGRUFRQVLVWHQWGHWHFWLRQUHVXOW
³3HUWXUEHG´YHUVLRQRIHYHQW
(YHQWQHFHVVDU\IRUYDOLGGHWHFWLRQ
(YHQWQHFHVVDU\IRUFRQVLVWHQWGHWHFWLRQ
6HTXHQFHRIHYHQWVIRUYDOLGRUFRQVLVWHQWKLWUHVXOW
6HTXHQFHRIHYHQWVIRUYDOLGRUFRQVLVWHQWPLVVUHVXOW


'HWHFWRU
UHVSRQVHLV
PHDVXUHG
DSSURSULDWHO\


$WWDFNLV
$WWDFNLV
DQRPDORXV
DQRPDORXV
ZLWKLQ
ZLWKLQ
GHWHFWRU¶V
GHWHFWRU¶V
SXUYLHZ
SXUYLHZ
D
$WWDFN127
$WWDFN127
DQRPDORXV