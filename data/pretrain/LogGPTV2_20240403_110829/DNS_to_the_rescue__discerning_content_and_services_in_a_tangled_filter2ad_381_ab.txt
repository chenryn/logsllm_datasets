2.2 DNS Terminology
DNS is a hierarchical distributed naming system for com-
puters connected to the Internet.
It translates “domain
names” that are meaningful to humans into IP-addresses
Figure 1: DN-Hunter architecture overview
required for routing. A DNS name server stores the DNS
records for diﬀerent domain names.
A domain name consists of two or more “labels” that are
conventionally concatenated, and delimited by dots, e.g.,
www.example.com. These names provide meaningful infor-
mation to the end user. Therefore labels naturally convey
information about the service, content, and information of-
fered by a given domain name. The labels in the domain
name are organized in a hierarchical fashion. The Top-Level
Domain (TLD) is the last part of the domain name - .com
in the above example; and sub-domains are then pre-pended
to the TLD. Thus, example.com is a subdomain of .com, and
www.example.com is a subdomain of example.com. In this
paper, we refer to the ﬁrst sub-domain after the TLD as
“second level domain”; it generally refers to the organization
that owns the domain name (e.g., example.com). Finally
Fully Qualiﬁed Domain Name (FQDN) is the domain name
complete with all the labels that unambiguously identiﬁes a
resource, e.g., www.example.com.
When an application needs to access a resource, a query
is sent to the local DNS server. This server responds back
with the resolution if it already has one, else it invokes an
iterative address resolution mechanism until it can resolve
the domain name (or determine that it cannot be resolved).
The responses from the DNS server carry a list of answers,
i.e., a list of serverIP addresses that can serve the content
for the requested resource.
Local caching of DNS responses at the end-hosts is com-
monly used to avoid initiating new requests to the DNS
server for every resolution. The time for which a local cache
stores a DNS record is determined by the Time-To-Live
(TTL) value associated with every record. It is set by the
authoritative DNS name server, and varies from few seconds
(e.g., for CDNs and highly dynamic services) to days. Also,
memory limits and timeout deletion policies can aﬀect local
caching at the client OS. However, as we will see later, in
practice, clients cache DNS responses for typically less than
1 hour.
3. DN-Hunter ARCHITECTURE
A high level overview of DN-Hunter architecture is shown
in Fig. 1.
It consists of two main components: real-time
sniﬀer and oﬀ-line analyzer. As the name indicates, the
sniﬀer labels/tags all the incoming data ﬂows in real time.
The output from the sniﬀer can be used for online policy en-
forcement (using any available policy enforcing tool) and/or
can be stored in a database for oﬀ-line analysis by the an-
alyzer component. Note that the sniﬀer can be a passive
WireReal-time SnifferOffline AnalyzerFlowTaggerFlowSnifferDNSResponseSnifferDNSResolverFlowDatabaseService TagExtractorContentDiscoverySpatialDiscovery...PolicyEnforcer415/* replace old references */
if exists mapSever.get(serverIP ) then
end if
/* Link back and forth
references to the new DNSEntry */
OLDEntry ← mapSever.get(serverIP )
OLDEntry.removeOldRef erences()
1: INSERT(DNSresponse)
2: Input: DNSresponse
3: (F QDN, ClientIP, answerList) ← decode(DN Sresponse)
4: DN Entry ← newDN Entry(F QDN )
5: mapServer ← mapClient.get(clientIP )
6: if mapServer = null then
7: mapServer ← new M apServer()
8: mapClient.put(clientIP, mapServer)
9: end if
10: for all serverIP in answerList do
11:
12:
13:
14:
15:
16:
17:
18: mapServer.put(serverIP, DN Entry)
19: M SEntry ← mapServer.get(serverIP )
20: DN Entry.insert(M SEntry)
21: end for
22: /* insert next entry in circular array */
23: OldDN Entry ← Clist.nextEntry()
24: OldDN Entry.deleteBackref erences()
25: Clist.nextEntry ← DN Entry
26:
27: LOOKUP(ClientIP, ServerIP)
28: Input: ClientIP and ServerIP of a ﬂow
29: Output: F QDN of ServerIP as requested by ClientIP
30: mapServer ← mapClient.get(clientIP )
31: if mapServer contains serverIP then
32: DN Entry ← mapServer.get(serverIP )
33:
34: end if
return DN Entry.F QDN
Algorithm 1: DNS Resolver pseudo-code
the number of servers that client c contacts, respectively.
Assuming L is well-dimensioned, the look-up complexity is
O(log(NC ) + log(NS(c))). NC depends on the number of
hosts in the monitored network. NS(c) depends on the traf-
ﬁc generated by clients.
In general, NS(c) is in the order
of a few hundreds. Note that when the number of moni-
tored clients increase, several load balancing strategies can
be used. For example, two resolvers can be maintained for
odd and even fourth octet value in the client IP-address.
Fig. 2 depicts the internal data structures in the DNS
resolver. Algorithm 1 provides the pseudo code of the “in-
sert()” and “lookup()” functions that access the data struc-
tures in Fig. 2. Since DNS responses carry a list of possible
serverIP addresses, more than one serverIP can point to
the same F QDN entry (line 11-22). When a new DNS re-
sponse is observed, the information is inserted in the Clist,
eventually removing old entries (line 12-15)3. When an entry
in the DNS circular array is overwritten, the old clientIP
and serverIP keys are removed from the maps before in-
serting the new one (line 25).
3.1.2 DNS trafﬁc characteristics
Using the above algorithm for tagging (or labeling) incom-
ing data ﬂows, we conducted several experiments to accom-
plish the following goals: (i) Understand how much infor-
mation DNS traﬃc can expose in enabling traﬃc visibility,
and (ii) Understand how to correctly dimension the DNS
resolver data structures.
Figure 2: DNS Resolver data structures
component instead of being active if the policy enforcer is
not implemented. For the ease of exposition, in this work,
we assume that the real-time sniﬀer component is a passive
monitoring component.
3.1 Real-Time Sniffer Component
The sniﬀer has two low-level sniﬃng blocks: (i) Flow snif-
fer which reconstructs layer-4 ﬂows by aggregating packets
based on the 5-tuple F id = (clientIP, serverIP, sP ort,
dP ort, protocol), and (ii) DNS response sniﬀer which de-
codes the DNS responses, and maintains a local data struc-
ture called the DNS Resolver. The DNS resolver maintains a
mapping between client IP, domain names queried, and the
server IP(s) included in the DNS response.
In particular,
for each response, it stores the set of serverIP addresses re-
turned for the fully qualiﬁed domain name (FQDN) queried,
associating them to the clientIP that generated the query.
All data ﬂows reconstructed by the ﬂow sniﬀer is passed on
to the Flow Tagger module. The ﬂow tagger module queries
the DNS resolver to tag the incoming clientIP, serverIP
pair. The ﬂow tagger will tag the incoming ﬂow with the
“label” (i.e., the FQDN) and sends the ﬂow to the policy
enforcer (to enforce any policy on the ﬂow including block-
ing, redirection, rate limiting, etc.) and/or the database for
oﬀ-line analysis.
3.1.1 DNS Resolver Design
The key block in the real-time sniﬀer component is the
DNS Resolver. Its engineering is not trivial since it has to
meet real-time constraints. The goal of the DNS Resolver is
to build a replica of the client DNS cache by sniﬃng DNS re-
sponses from the DNS server. Each entry in the cache stores
the F QDN and uses the serverIP and clientIP as look-up
keys. To avoid garbage collection, F QDN s are stored in a
ﬁrst-in-ﬁrst-out FIFO circular list, Clist, of size L; a pointer
identiﬁes the next available location where an entry can be
inserted. L limits the cache entry lifetime and has to prop-
erly match the local resolver cache in the monitored hosts.
Lookup is performed using two sets of tables. The ﬁrst
table uses the clientIP as key to ﬁnd a second table, from
where the serverIP key points to the most recent F QDN
entries in the Clist that was queried by clientIP . Tables
are implemented using C++ maps2, in which the elements
are sorted from lower to higher key value following a speciﬁc
strict weak ordering criterion based on IP addresses. Let NC
and NS(c) represent the number of monitored clients and
2Unordered maps, i.e., hash tables, can be used as well to
further reduce the computational costs
3In this case the information about the old FQDN is lost
and may create some ambiguity. See Sec. 6 for more details.
Client IPMapServer IPMapsFQDN Clist213.254.17.14213.254.17.17itunes.apple.com216.74.41.8216.74.41.10216.74.41.12data.flurry.com93.58.110.17337.241.163.105416Protocol EU1-ADSL1 EU1-ADSL2 EU1-FTTH
91% (683k)
84% (50k)
92% (4.4M)
92% (0.4M)
HTTP
0% (48)
90% (2.7M)
86% (196k)
1% (1.3k)
US-3G
75% (445k)
74% (83k)
8% (8k)
TLS
P2P
HTTP
TLS
P2P
1% (6k)
EU2-ADSL
97% (5.8M)
96% (279k)
1% (4.2k)
Table 2: DNS Resolver hit ratio
To address the ﬁrst goal, we compute the DNS hit ratio.
In other words, DNS hit ratio represents the fraction of data
ﬂows that can be successfully associated with a FQDN. The
higher is the hit ratio, the more successful is DN-Hunter in
enabling traﬃc visibility. Intuition suggests that all client-
server services/applications rely on the DNS infrastructure
and hence DN-Hunter will be able to accurately identify
them. However, certain peer-to-peer services/applications
do not use the DNS infrastructure and thus evade detection
in DN-Hunter. Tab. 2 conﬁrms this intuition.
It details,
for each trace, the number of DNS hits and the correspond-
ing percentage of ﬂows that were resolved, considering the
subset of HTTP, TLS, and P2P ﬂows. In this experiment,
we consider a warm-up time of 5 minutes (i.e., we track all
ﬂows, but ignore the statistics contributed by the ﬂows in
the ﬁrst 5 mins of the trace).
As expected, HTTP and TLS ﬂows show a very high hit
ratio, with the majority of cache-miss occurring in the ini-
tial part of the trace when the end host operating system
local resolver cache resolves the query locally and limits the
queries to the DNS server. P2P data ﬂows are hardly pre-
ceded by DNS resolutions, and hence it results in a very low
hit ratio4.
When considering only HTTP and TLS data ﬂows, we see
that the hit ratio mostly exceeds 90% for all traces except
US-3G. When considering only the last hour of each trace,
the DNS hit ratio increases further close to 100% in all traces
but US-3G. In the case of US-3G, we hypothesize that the
adoption of tunneling mechanisms over HTTP/HTTPS for
which no DNS information is exposed may be the cause of
lower DNS Resolver eﬃciency. Furthermore, device mobil-
ity may also aﬀect our results: our tool may observe ﬂows
from devices entering the coverage area after performing
a DNS resolution outside the visibility of our monitoring
point. Thus our tool might miss the DNS response resulting
in a cache-miss. More details about the DNS traﬃc charac-
teristics that aﬀects DN-Hunter dimensioning is provided in
Sec. 6.
3.1.3 DN-Hunter vs. DNS reverse lookup
The information that the sniﬀer component extracts is
much more valuable than the one that can be obtained
by performing active DNS reverse lookup of serverIP ad-
dresses. Recall that the reverse lookup returns only the des-
ignated domain name record. Consider Tab. 3 where we ran-
domly selected 1,000 serverIP for which the Sniﬀer was able
to associate a FQDN. We have considered the EU1-ADSL2
dataset for this experiment. We then performed active DNS
reverse lookup queries of the serverIP addresses and com-
pared the returned FQDN with the one recovered by the
sniﬀer. In 29% of cases, no answer was returned by the re-
4P2P hits are related to BitTorrent tracker traﬃc mainly.
Same FQDN
Same 2nd-level domain
Totally diﬀerent
No-answer
9%
36%
26%
29%
Table 3: DN-Hunter vs. reverse lookup
verse lookup while in 26% of the lookups the two answers
were totally diﬀerent from each other. All the other queries
had at least had a partial match. In fact, only 9% of the re-
verse lookups completely matched the results from the snif-
fer while the rest of the 36% only matched the second-level
domain name. These results are not surprising since single
servers are typically serving several FQDNs (see Sec. 5). In
addition to this, reverse lookup poses scalability issues as
well.
3.2 Off-Line Analyzer Component
Although the sniﬀer module provides deep visibility into
the services/applications on the wire in real-time, some an-
alytics cannot be performed in real-time.
In other words,
dissecting and analyzing the data in diﬀerent ways can ex-
pose very interesting insights about the traﬃc. The oﬀ-line
analyzer component does exactly this.
It contains several
intelligent analytics that can extract information from the
ﬂows database by mining its content. In the next section, we
will present a few sample analytics. However, several other
analytics can be added into the system easily.
4. ADVANCED ANALYTICS
In this section we describe some advanced analytics using
the data stored in the labeled ﬂows database to automati-
cally discover information and discern the tangled web.
4.1 Spatial Discovery of Servers
Today, CDNs and distributed cloud-based infrastructures
are used to meet both scalability and reliability require-
ments, decoupling the owner of the content and the organi-
zation serving it. In this context some interesting questions
arise: (i) Given a particular resource (i.e., a FQDN) what
are all the servers or hosts that deliver the required con-
tent?, (ii) Do these servers belong to the same or diﬀerent
CDNs?, and (iii) Do CDNs catering to the resource change
over time and geography? (iv) Are other resources belong-
ing to the same organization served by the same or diﬀerent
set of CDNs?
DN-Hunter can easily answer all of the above questions.
Algorithm 2 shows the pseudo-code for the Spatial Discovery
functionality in DN-Hunter. The spatial discovery module
ﬁrst extracts the second-level domain name from the FQDN
(line 4), and then queries the labeled ﬂows database (line 5)
to retrieve all serverIP addresses in ﬂows directed towards
the second-level domain (i.e., the organization). Then, for
every FQDN that belongs to the organization, the spatial
discovery module will extract the serverIP addresses that
can serve the request (line 6-9) based on the DNS responses.
This enables the module to: (i) Discover the information
about the structure of servers (single server, or one/many
CDNs) that handle all queries for the organization, (ii) Dis-
cover which servers handle a more speciﬁc resource. For ex-
ample, diﬀerent data centres/hosts may be serving the con-