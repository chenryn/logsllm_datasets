gradient descent from the solution found on the previous
iteration (“warm-start”). This dramatically reduces the number
of rounds of gradient descent needed during each iteration, as
the solution with k pixels held constant is often very similar
to the solution with k + 1 pixels held constant.
Figure 4 shows the L0 attack applied to one digit of each
source class, targeting each target class, on the MNIST dataset.
The attacks are visually noticeable, implying the L0 attack is
more difﬁcult than L2. Perhaps the worst case is that of a 7
being made to classify as a 6; interestingly, this attack for L2
is one of the only visually distinguishable attacks.
A comparable ﬁgure (Figure 11) for CIFAR is in the
appendix.
C. Our L∞ Attack
The L∞ distance metric is not fully differentiable and
standard gradient descent does not perform well for it. We
experimented with naively optimizing
minimize
c · f (x + δ) + (cid:6)δ(cid:6)∞
However, we found that gradient descent produces very poor
results: the (cid:6)δ(cid:6)∞ term only penalizes the largest (in absolute
value) entry in δ and has no impact on any of the other. As
such, gradient descent very quickly becomes stuck oscillating
between two suboptimal solutions. Consider a case where δi =
0.5 and δj = 0.5 − . The L∞ norm will only penalize δi,
11Selecting the index i that minimizes δi is simpler, but it yields results
with 1.5× higher L0 distortion.
Target Classiﬁcation (L∞)
7
2
3
4
5
6
8
9
0
1
0
1
2
n
o
i
t
a
c
ﬁ
3
i
s
s
a
l
C
4
5
e
c
r
u
o
S
6
7
8
9
Fig. 5. Our L∞ adversary applied to the MNIST dataset performing a targeted
attack for every source/target pair. Each digit is the ﬁrst image in the dataset
with that label.
(cid:6)δ(cid:6)∞ will be zero at this point. Thus, the
not δj, and ∂
∂δj
gradient imposes no penalty for increasing δj, even though it
is already large. On the next iteration we might move to a
position where δj is slightly larger than δi, say δi = 0.5 − 
(cid:2)
(cid:2)(cid:2), a mirror image of where we started. In
and δj = 0.5 + 
other words, gradient descent may oscillate back and forth
across the line δi = δj = 0.5, making it nearly impossible to
make progress.
We resolve this issue using an iterative attack. We replace
the L2 term in the objective function with a penalty for any
terms that exceed τ (initially 1, decreasing in each iteration).
This prevents oscillation, as this loss term penalizes all large
values simultaneously. Speciﬁcally, in each iteration we solve
(cid:3)
(cid:9)
(cid:10)
minimize
c · f (x + δ) + ·
(δi − τ )
+
i
After each iteration, if δi < τ for all i, we reduce τ by a factor
of 0.9 and repeat; otherwise, we terminate the search.
Again we must choose a good constant c to use for the
L∞ adversary. We take the same approach as we do for the
L0 attack: initially set c to a very low value and run the L∞
adversary at this c-value. If it fails, we double c and try again,
until it is successful. We abort the search if c exceeds a ﬁxed
threshold.
Using “warm-start” for gradient descent in each iteration,
this algorithm is about as fast as our L2 algorithm (with a
single starting point).
Figure 5 shows the L∞ attack applied to one digit of each
source class, targeting each target class, on the MNSIT dataset.
While most differences are not visually noticeable, a few are.
Again, the worst case is that of a 7 being made to classify as
a 6.
48
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:27:35 UTC from IEEE Xplore.  Restrictions apply. 
mean
Our L0
48
-
JSMA-Z
-
JSMA-F
Our L2
0.32
Deepfool
0.91
Our L∞ 0.004
0.004
FGS
IGS
0.004
Untargeted
prob
100%
0%
0%
100%
100%
100%
100%
100%
Average Case
mean
prob
100%
410
0%
-
0%
-
0.96
-
0.006
0.064
0.01
100%
-
100%
2%
99%
Least Likely
mean
prob
100%
5200
0%
-
0%
-
2.22
-
0.01
-
0.03
100%
-
100%
0%
98%
Target Classiﬁcation
3
4
5
6
7
8
9
0
1
2
0
L
2
L
c
i
r
t
e
M
e
c
n
a
t
s
i
D
∞
L
TABLE V
COMPARISON OF THE THREE VARIANTS OF TARGETED ATTACK TO
PREVIOUS WORK FOR THE INCEPTION V3 MODEL ON IMAGENET. WHEN
SUCCESS RATE IS NOT 100%, THE MEAN IS ONLY OVER SUCCESSES.
A comparable ﬁgure (Figure 13) for CIFAR is in the ap-
pendix. No attack is visually distinguishable from the baseline
image.
VII. ATTACK EVALUATION
We compare our targeted attacks to the best results pre-
viously reported in prior publications, for each of the three
distance metrics.
We re-implement Deepfool, fast gradient sign, and iterative
gradient sign. For fast gradient sign, we search over  to ﬁnd
the smallest distance that generates an adversarial example;
failures is returned if no  produces the target class. Our
iterative gradient sign method is similar: we search over 
(ﬁxing α = 1
256) and return the smallest successful.
For JSMA we use the implementation in CleverHans [35]
with only slight modiﬁcation (we improve performance by
50× with no impact on accuracy).
JSMA is unable to run on ImageNet due to an inherent
signiﬁcant computational cost: recall
that JSMA performs
search for a pair of pixels p, q that can be changed together
that make the target class more likely and other classes less
likely. ImageNet represents images as 299 × 299 × 3 vectors,
so searching over all pairs of pixels would require 236 work
on each step of the calculation. If we remove the search over
pairs of pixels, the success of JSMA falls off dramatically. We
therefore report it as failing always on ImageNet.
We report success if the attack produced an adversarial
example with the correct target label, no matter how much
change was required. Failure indicates the case where the
attack was entirely unable to succeed.
We evaluate on the ﬁrst 1, 000 images in the test set on
CIFAR and MNSIT. On ImageNet, we report on 1, 000 images
that were initially classiﬁed correctly by Inception v3 12. On
ImageNet we approximate the best-case and worst-case results
by choosing 100 target classes (10%) at random.
The results are found in Table IV for MNIST and CIFAR,
and Table V for ImageNet. 13
12Otherwise the best-case attack results would appear to succeed extremely
often artiﬁcially low due to the relatively low top-1 accuracy
13The complete code to reproduce these tables and ﬁgures is available
online at http://nicholas.carlini.com/code/nn robust attacks.
Fig. 6. Targeted attacks for each of the 10 MNIST digits where the starting
image is totally black for each of the three distance metrics.
Target Classiﬁcation
3
4
5
6
7
8
9
0
1
2
0
L
2
L
c
i
r
t
e
M
e
c
n
a
t
s
i
D
∞
L
Fig. 7. Targeted attacks for each of the 10 MNIST digits where the starting
image is totally white for each of the three distance metrics.
For each distance metric, across all
three datasets, our
attacks ﬁnd closer adversarial examples than the previous
state-of-the-art attacks, and our attacks never fail to ﬁnd an
adversarial example. Our L0 and L2 attacks ﬁnd adversarial
examples with 2× to 10× lower distortion than the best pre-
viously published attacks, and succeed with 100% probability.
Our L∞ attacks are comparable in quality to prior work, but
their success rate is higher. Our L∞ attacks on ImageNet are so
successful that we can change the classiﬁcation of an image
to any desired label by only ﬂipping the lowest bit of each
pixel, a change that would be impossible to detect visually.
As the learning task becomes increasingly more difﬁcult, the
previous attacks produce worse results, due to the complexity
of the model. In contrast, our attacks perform even better as
the task complexity increases. We have found JSMA is unable
to ﬁnd targeted L0 adversarial examples on ImageNet, whereas
ours is able to with 100% success.
It is important to realize that the results between models
are not directly comparable. For example, even though a L0
adversary must change 10 times as many pixels to switch an
ImageNet classiﬁcation compared to a MNIST classiﬁcation,
ImageNet has 114× as many pixels and so the fraction of
pixels that must change is signiﬁcantly smaller.
Generating synthetic digits. With our targeted adversary,
we can start from any image we want and ﬁnd adversarial
examples of each given target. Using this, in Figure 6 we
show the minimum perturbation to an entirely-black image
required to make it classify as each digit, for each of the
distance metrics.
49
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:27:35 UTC from IEEE Xplore.  Restrictions apply. 
Our L0
JSMA-Z
JSMA-F
Our L2
Deepfool
Our L∞
Fast Gradient Sign
Iterative Gradient Sign
mean
8.5
20
17
1.36
2.11
0.13
0.22
0.14
Best Case
Average Case
Worst Case
MNIST
CIFAR
MNIST
CIFAR
MNIST
CIFAR
prob
100%
100%
100%
mean
5.9
20
25
prob
100%
100%
100%
mean
16
56
45
mean
prob
13
100%
100%
58
100% 110
prob
100%
100%
100%
mean
33
180
100
prob
100%
mean
24
98% 150
100% 240
prob
100%
100%
100%
100%
100%
100%
100%
100%
0.17
0.85
100%
100%
0.0092 100%
0.015
99%
0.0078 100%
1.76
−
0.16
0.26
0.19
100%
-
100%
42%
100%
100%
0.33
−
-
0.013 100%
0.029
51%
0.014 100%
2.60
−
0.23
−
0.26
100%
-
100%
0%
100%
100%
0.51
−
-
0.019 100%
0.34
1%
0.023 100%
COMPARISON OF THE THREE VARIANTS OF TARGETED ATTACK TO PREVIOUS WORK FOR OUR MNIST AND CIFAR MODELS. WHEN SUCCESS RATE IS
TABLE IV
NOT 100%, THE MEAN IS ONLY OVER SUCCESSES.
This experiment was performed for the L0 task previously
[38], however when mounting their attack, “for classes 0, 2,
3 and 5 one can clearly recognize the target digit.” With our
more powerful attacks, none of the digits are recognizable.
Figure 7 performs the same analysis starting from an all-white