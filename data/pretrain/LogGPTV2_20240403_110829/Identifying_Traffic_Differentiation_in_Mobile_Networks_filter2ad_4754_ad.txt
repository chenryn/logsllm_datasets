100
100
100
100
100
100
100
90
R3
100
100
100
100
0
0
0
0
Table 3: Shaping detection accuracy for diﬀerent apps, in regions
1 and 3 (denoted by R1 and R3). We ﬁnd that the KS Test
(NetPolice) and Area Test have similarly high accuracy in both
regions (1 and 3), but Glasnost performs poorly in region 3.
Figure 8: Accuracy against loss.
App
Netﬂix
YouTube
Hangout
Skype
Region 2
KS Test Area Test Glasnost
65
67
40
55
28
0
0
10
100
100
100
92
Table 4: Percent of tests identiﬁed as diﬀerentiation in region 2
for the three detection methods. Glasnost consistently identiﬁes
region 2 as diﬀerentiation, whereas the Area Test is more likely
to not detect diﬀerentiation.
1 and 3 and the two KS Tests (as they do not identify dif-
ferentiation in region 3)
Impact of noise.
Properties of the network unrelated
to shaping (e.g., congested-induced packet loss) may impact
performance and cause false positives for diﬀerentiation de-
tectors. In Fig. 8 we examine the impact of loss on accuracy
for one TCP application (Netﬂix) and one UDP application
(Skype). We ﬁnd that both variants of the KS Test have
high accuracy in the face of congestion, but the Area Test
is more resilient to moderate and high loss for Skype.
Impact of number of replays. One way to account for
noise is to use additional replays to average out any vari-
ations from transient noise in a single replay. To evaluate
the impact of additional replays, we varied the number of
replays combined to detect diﬀerentiation under high loss
and plotted the detection accuracy in Fig. 9.4 In the case
of moderate or high loss, increasing the number of replays
improves accuracy, particularly for a short YouTube trace.
In all cases, the Area Test is more accurate than (or equal
to) the KS Test. Importantly, the accuracy does not signif-
icantly improve past 2 or 3 replays. In our deployment, we
use 2 tests (to minimize data consumption).
Region 2 detection results.
Recall that our goal is
to identify shaping that impinges on the network resources
required by an application. However, in region 2, shap-
ing may or may not impact the application’s performance,
which makes identifying shaping with actual application im-
pact challenging. We report detection results for the three
tests in region 2 in Table 4. We ﬁnd that Glasnost consis-
tently identiﬁes diﬀerentiation, the Area Test consistently
does not detect diﬀerentiation, and the KS Test is inconsis-
tent. When testing an application that is sensitive to shap-
ing of its peak throughput, the Glasnost method may be
preferable, whereas the Area Test is preferred when testing
applications that can tolerate shaping above their average
throughput.
When detecting diﬀerentiation, we conservatively avoid
the potential for false positives and thus do not report dif-
4Additional replays did not help when there is low loss.
Figure 9: Eﬀect of combining replays on accuracy in high loss.
ferentiation for region 2. Thus, we use the Area Test in our
implementation because it will more reliably achieve this
result (Table 4). Determining which apps are aﬀected by
region 2 diﬀerentiation is a topic of future work.
6. MEASUREMENT STUDY
We now use our calibrated detection approach to identify
cases of diﬀerentiation using measurement study of produc-
tion networks. For each network, we focus on detection for a
set of popular apps that we expect might be subject to diﬀer-
entiation: streaming audio and video (high bandwidth) and
voice/video calling (competes with carrier’s line of business).
The data from these experiments was initially collected be-
tween January and May 2015. We then repeated the exper-
iments in August 2015, after the new FCC rules prohibiting
diﬀerentiation took eﬀect.
6.1 System implementation
We now describe the implementation of our client and
server software. To promote transparency and guide pol-
icy for traﬃc diﬀerentiation, our source code and analy-
sis results will be made continuously available at http:
//dd.meddle.mobi.
Client. We implement our replay client in an Android
app called Diﬀerentiation Detector. It does not require any
root privileges, and is available for download in the Google
Play store.5 The app consists of 14,000 lines of source code
(LOC), which includes the Strongswan VPN implementa-
tion [27] (4,600 LOC). The app conducts diﬀerentiation tests
and reports results to our replay servers as part of an IRB-
approved study. Users undergo informed consent when the
app ﬁrst runs, and cannot run any tests unless they consent
to participate in our study. An iOS implementation is under
development.
The app (Fig. 10) is pre-loaded with replay transcripts for
Viber, Netﬂix, Spotify, Skype, YouTube, and Google Hang-
outs, so that users can test for diﬀerentiation of these apps
5https://play.google.com/store/apps/details?id=
com.stonybrook.replay
 0 0.2 0.4 0.6 0.8 1Low lossMod lossHigh lossAccuracyNetflix - AreaNetflix - KS2Skype - AreaSkype - KS20.60.70.80.911234Accuracy#replays combinedyoutube-KS2youtube-Areahangout-KS2hangout-Area247Per-client management. To replay traﬃc for multiple
simultaneous users, our replay server needs to be able to map
each received packet to exactly one app replay. A simple
solution is to put this information in the ﬁrst few bytes of
each ﬂow between client and server. Unfortunately, as shown
in Section 4.2, this can disrupt classiﬁcation and lead to false
negatives. Instead, we use side-channel connections out of
band from the replay to supply information regarding each
replay run.
NAT behavior. We also use the side-channel to identify
which clients will connect from which IPs and ports. For
networks without NAT devices, this works well. However,
many mobile networks use NAT, meaning we can’t use the
side-channel to identify the IP and port that a replay con-
nection will use (because they may be modiﬁed by the NAT).
For such networks, each replay server can reliably support
only one active client per ISP and application. While this
has not been an issue in our initial app release, we are inves-
tigating other work-arounds. Moreover, to scale the system,
we can use multiple replay servers and a load balancer. The
load balancer can be conﬁgured to assign clients with the
same IP address to diﬀerent servers.
“Translucent” HTTP proxies.
It is well known that
ISPs use transparent proxies on Web traﬃc [34]. Our sys-
tem works as intended if a proxy simply relays exactly the
same traﬃc sent by a replay client. In practice, we found
examples of “translucent” proxies that issue the same HTTP
requests as the client but change connection keep-alive be-
havior; e.g., the client uses a persistent connection in the
recorded trace and the proxy does not. To handle this be-
havior, our system must be able to map HTTP requests from
unexpected connections to the same replay.
Another challenge with HTTP proxies is that the proxy
may have a diﬀerent public IP address from other non-
HTTP traﬃc (including the side-channel), which means the
server will not be able to map the HTTP connections to
the client based on IP address.
In these cases the server
replies to the HTTP request with a special message. When
the client receives this message, it restarts the replay. This
time the client adds a custom header (X-) to HTTP requests
with client’s ID, so the server can match HTTP connections
with diﬀerent IP addresses to the corresponding clients. The
server then ignores this extra header for the remainder of the
replay. We have also observed that some mobile providers
exhibit such behavior for other TCP and UDP ports, mean-
ing we cannot rely on an X- header to convey side-channel
information. In these cases we can identify users based on
ISP’s IP subnet, which means we can only support one active
client per replay server and ISP, regardless of the application
they are replaying.
Content-modifying proxies and transcoders. Recent
reports and previous work highlight cases of devices in ISPs
that modify customer traﬃc for reasons such as tracking [17],
caching [34], security [24], and reducing bandwidth [32]. We
also saw several cases of content-modifying proxies. For
example, we saw Sprint modifying HTTP headers, Veri-
zon inserting global tracking cookies, and Boost transcoding
YouTube videos.
In our replay methodology, we assumed that packets ex-
changed between the client and server would not be not
modiﬁed in ﬂight, and trivially detect modiﬁcation because
packet payloads do not match recorded traces. In the case
Figure 10: Screenshot of our Android Diﬀerentiation Detector
app.
without recording traﬃc.6 For each trace, the app follows
the replay procedure described in Section 3 and repeats the
replay tests twice7 back to back.
At the end of each replay, metadata such as carrier name,
OS information, network type, and signal strength, is col-
lected and sent to the server for analysis. To account for
the case where background traﬃc might aﬀect our detec-
tion results, future version of the app will measure data
consumption during replays and discard results where we
detect interference from other traﬃc. Users can also access
their historical results through the app.
Server and analysis code The server coordinates with the
client to replay traces, and records packet traces for analy-
sis. This interaction is managed by side-channel connections
that identify which trace is being replayed and what ports
will be opened in the case of NAT traversal. For networks
that allow it (e.g., in our testbed), we support IP spooﬁng
so our replay server can send packets using the IP addresses
in arbitrary recorded traces. The server logic is 1,850 lines
of code (Python).
Our analysis code implements tests for throughput, RTT,
jitter, and loss diﬀerentiation. We implement KS Test and
Area Test, and use simple scalar metrics (average, max) for
loss. Our parsing script supports TCP and streaming UDP
applications. It uses tshark to extract conversations. To-
gether, these artifacts consist of 1,170 lines of code.
6.2 Challenges in operational networks
In this section we discuss several challenges that we en-
countered when attempting to identify diﬀerentiation in mo-
bile networks. To the best of our knowledge, we are the ﬁrst
to identify these issues for detecting diﬀerentiation and dis-
cuss workarounds that address them.
6While we currently only support Android, the approach
should also work on any mobile OS that supports VPN con-
nectivity, including iOS. We are currently developing sup-
port for users to record their own traces from mobile devices.
7Users can increase the number of back-to-back replays for
more accuracy.
248of header manipulation, we use an edit-distance based ap-
proach to match modiﬁed content to the recorded content
it corresponds to. While our system can tolerate moder-
ate amounts of modiﬁcation (e.g., HTTP header manipu-
lation/substitution), if the content is modiﬁed drastically,
e.g., transcoding an image to reduce its size, it is diﬃcult
to detect shaping because the data from our control and
exposed trials do not match. In our current system, we sim-
ply notify the user that there is content modiﬁcation but do
not attempt to identify shaping. We leave a more detailed
analysis of this behavior to future work.
Caching. Our replay system assumes content is served
from the replay server and not an ISP’s cache. We detect
the latter case by identifying cases where the client receives
data that was not sent by the server. In the few cases where
we observed this behavior, the ISPs were caching small static
objects (e.g., thumbnail images) which were not the domi-
nant portion of the replay traﬃc (e.g., when streaming au-
dio) and had negligible eﬀect on our statistical analysis.8
Going forward, we expect this behavior to be less promi-
nent due to the increased use of encrypted protocols, e.g.,
HTTP/2 and QUIC. In such cases, an ISP may detect and
shape application traﬃc using SNI for classiﬁcation, but
they cannot modify or cache content.
6.3 Differentiation results
In this section, we present results from running our Diﬀer-
entiation Detector Android app on popular mobile networks.
This dataset consists of 4,786 replay tests, covering traces
from six popular apps. We collected test results from most
major cellular providers and MVNOs in the US; further, we
gathered measurements from four international networks.
Our app supports both VPN traﬃc and random payloads
(but not random ports) as control traﬃc. We use the lat-
ter only if a device does not support VPN connectivity or
the cellular provider blocks or diﬀerentiates against VPN
traﬃc. After collecting data, we run our analysis to detect
diﬀerentiation; the results for US carriers are presented in
Table 5.
In other networks such as Idea (India), JazzTel
(Spain), Three (UK), and T-Mobile (Netherlands), our re-
sults based on a subset of the traces (the traces that users
selected) indicated no diﬀerentiation.
Our key ﬁnding is that our approach successfully de-
tect diﬀerentiation in three mobile networks (BlackWireless,
H2O, and SimpleMobile), with the impact of shaping result-
ing in up to 65% diﬀerence in average throughput. These
shaping policies all apply to YouTube (not surprising given
its impact on networks), but not always to Netﬂix and Spo-
tify. We did not identify diﬀerentiation for UDP traﬃc in
any of the carriers.
Interestingly, H2O consistently gives
better performance to port 80 traﬃc with random payloads,
indicating a policy that gives relatively worse performance
to VPN traﬃc and streaming audio/video. We tested these
networks again in August 2015 and did not observe such dif-
ferentiation. We speculate that these networks ceased their
shaping practices in part due to new FCC rules barring this
behavior, eﬀective in June 2015.
We contacted these ISPs we tested for comment on the
behavior we observed in their networks. At the time of pub-
lication, only one ISP had responded (Sprint) and did not
wish to comment.
8Boost was one exception, which we discuss later.
ISP
Verizon
T-Mobile
AT&T
Sprint
Boost
BlackWireless
H2O
SimpleMobile
NET10
YT
m
-
f
m/p
m
60%
37%*
36%
p