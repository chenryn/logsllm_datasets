title:BitShred: feature hashing malware for scalable triage and semantic
analysis
author:Jiyong Jang and
David Brumley and
Shobha Venkataraman
BitShred: Feature Hashing Malware for Scalable Triage and
Semantic Analysis
Jiyong Jang
Carnegie Mellon University
Pittsburgh, PA, USA
PI:EMAIL
David Brumley
Carnegie Mellon University
Pittsburgh, PA, USA
PI:EMAIL
Shobha Venkataraman
AT&T Labs – Research
Florham Park, NJ, USA
PI:EMAIL
Abstract
The sheer volume of new malware found each day is growing at
an exponential pace. This growth has created a need for automatic
malware triage techniques that determine what malware is simi-
lar, what malware is unique, and why. In this paper, we present
BitShred, a system for large-scale malware similarity analysis and
clustering, and for automatically uncovering semantic inter- and
intra-family relationships within clusters. The key idea behind Bit-
Shred is using feature hashing to dramatically reduce the high-
dimensional feature spaces that are common in malware analysis.
Feature hashing also allows us to mine correlated features between
malware families and samples using co-clustering techniques. Our
evaluation shows that BitShred speeds up typical malware triage
tasks by up to 2,365x and uses up to 82x less memory on a single
CPU, all with comparable accuracy to previous approaches. We
also develop a parallelized version of BitShred, and demonstrate
scalability within the Hadoop framework.
Categories and Subject Descriptors
D.4.6 [Operating Systems]: Security and Protection—Invasive soft-
ware
General Terms
Design, Performance, Security
Keywords
Malware Triage, Feature Hashing, Co-clustering, Hadoop
1.
INTRODUCTION
The volume of new malware, fueled by easy-to-use malware
morphing engines, is growing at an exponential pace [8]. In 2009
security vendors received upwards of 8,000 unique by hash mal-
ware samples per day [8], with the projected total to reach over
1,000,000 per day within the next 7 years. The sheer volume of
malware means we need automatic methods for large-scale mal-
ware triage techniques and systems.
At a high level, triage has two steps. First, per-sample malware
analysis is run on each sample to extract a set of features. Second,
malware are compared in a pairwise fashion to determine similarity,
e.g., in our work, like others, using the Jaccard distance. Once we
determine what malware are similar, and what are the important se-
mantic similarities and differences to known malware cases, triage
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
CCS’11, October 17–21, 2011, Chicago, Illinois, USA.
Copyright 2011 ACM 978-1-4503-0948-6/11/10 ...$10.00.
can make informed decisions. For example, triage may perform
further in-depth analysis on one representative malware sample per
family that would be cost-prohibitive to do on the entire data set.
In this paper, we present BitShred, a system for large-scale mal-
ware similarity analysis and clustering, and for automatically un-
covering semantic inter- and intra-family relationships within clus-
ters. The main feature of BitShred is it is agnostic to the particu-
lar per-malware analysis routine, even when the extracted feature
set has a very large feature space. Malware authors and defend-
ers are caught in a cyclic battle where defenders invent ever-more
advanced and accurate per-malware analyses for feature extraction,
which are then defeated by new malware obfuscation algorithms.
The cyclic battle brings the need for malware triage techniques that
allow us to plug-in the latest or most appropriate analysis for fea-
ture extraction. We empirically show BitShred meets the desired re-
quirement by demonstrating BitShred on two previously proposed
per-sample analysis: dynamic behavior analysis from Bayer et al.
[13] where the feature space is 217, and static code reuse detection
as proposed in [9, 25, 38] where the feature space is 2128.
The main issues for handling large volumes of malware are (a)
efﬁciently representing malware features (so we can ﬁt more in
main memory without paging), and (b) comparing feature sets be-
tween malware, and (c) determining which features are correlated
for malware groups. To give a sense of scale, currently over 8000
new malware per day are observed, requiring about 31 million com-
parisons to ﬁnd families using hierarchical clustering. If we per-
form n-item analysis when n = 16 bytes, an exact representa-
tion of the features would require 2128 ( 295 gigabytes) per sample.
We could not perform all 31 million comparisons on previous data
structures in 24 hours on a single CPU.
The central idea in BitShred is to use feature hashing [36, 37, 39].
Feature hashing allows for dramatic dimensionality reduction, so
the hashed representation takes less room in memory, and is also
L1/L2 cache efﬁcient. The catch is that feature hashing introduces
collisions in the reduced feature space. For example, we use a hash
function that compresses the 2128 feature space down to 218, there
will be an enormous number of collisions in the feature space. The
surprising thing is that with feature hashing, we need just a single
hash function for the dimensionality reduction of the feature space.
Requiring only single hash function as well as dimensionality re-
duction have immediate performance implications. This result is
backed by theory and experimentation that shows pairwise com-
parison, thus algorithms built on top like hierarchical clustering,
will be close to exact.
Feature hashing allows us to also simultaneously mine correlated
features between malware families and samples using co-clustering
techniques. Clustering alone acts like a blackbox, telling us only
that malware are grouped because they are similar. Co-clustering
goes one step further and tells us why malware are similar by simul-
taneously clustering features. For example, co-clustering allows us
to group two malware and to say the features that explain why they
are similar (e.g., a signiﬁcant amount of shared code) and why they
are different (e.g., contacting different command and control hosts).
309Contributions. Our main contribution is a system for performing
the triage tasks described above that scales to data sets orders of
magnitude larger than existing approaches. We present a theoretical
analysis showing that feature hashing with the Jaccard offers near
optimal results, and build a real system called BitShred that is in-
dependent of the particular per-malware analysis engine and works
even for high-dimensional feature sets. We extensively evaluate
BitShred’s scalability, speed and accuracy using two different per-
sample analysis: code similarity and dynamic behaviors. Our per-
formance evaluation shows that BitShred can cluster over 116,000
malware per day on a single node, and over 1.9 million per day
on a Hadoop cluster where we develop an optimal schedule that
minimizes communication overhead and provides uniform node
work. We also propose novel techniques based upon co-clustering
adapted to BitShred for identifying similar or distinguishing se-
mantic features in malware families.
2. THE CORE IDEA, FEATURE HASHING,
AND THE RELATION TO PREVIOUS
APPROACHES
We focus on any analysis that outputs a set of features that are
Boolean, or can be encoded as Boolean variables. For example, in
code reuse detection the features are whether a code fragment is
present or not. Real-value features can be encoded by bucketizing
them, where a Boolean feature is true if the feature falls within a
particular bucket. This allows us to plug in many types of analysis.
2.1 Feature Hashing & Malware Similarity
We compute malware similarity using the Jaccard similarity met-
ric. The Jaccard calculates the percentage of common features,
with the idea that the larger the sharing is, the more alike the mal-
ware are, and is used extensively in previous work [13, 32]. More
formally, given two feature sets ga and gb for malware sa and sb
respectively, the Jaccard similarity (i.e., index) is:
J(ga, gb) =
|ga ∩ gb|
|ga ∪ gb|
In order to motivate feature hashing, consider ﬁrst using a stan-
dard implementation of Jaccard, e.g., as found in SimMetrics [5].
The advantage of this approach is the size of the feature data struc-
ture is linear in the number of features a malware sample actually
presents, e.g., if our feature space is of size 2128 but a particular
malware only has 230 features, the data structure is still only 230
in size. Unfortunately, set operations are not amenable to feature
extraction using co-clustering, and the set union and intersection
operations themselves are a bottleneck.
In our experiments, we
could only cluster about 2,388 malware/day using this approach
(§ 5, labeled as exact Jaccard).
We take an approach of encoding features as a bitvector. The
Jaccard becomes fast CPU-friendly logic operations:
Jbv(fa, fb) =
S(fa ∧ fb)
S(fa ∨ fb)
where fi is the bitvector representation of the feature set for mal-
ware si and S(·) counts the number of set bits.
Feature hashing [36, 37, 39] is a speciﬁc way of encoding fea-
tures as a bitvector. Most existing implementations of bitvector
Jaccard, e.g., the one found in python, assume the feature space
is completely encoded using index variables where feature 1 cor-
responds to bit 1, feature 2 to bit 2, feature 3 to bit 3, and so on.
This scheme is impractical when the feature space is large, e.g., as
in our case where such an encoding would result in a per-malware
data structure that is gigabytes in size.
A more efﬁcient encoding is to use Bloom ﬁlters, which we ini-
tially tried. A Bloom ﬁlter is a probabilistic data structure used
Figure 1: Error with various k where m=8,192
to efﬁciently encode sets and perform set membership tests. Let
h1, h2, h3, ..., hk be a set of hash functions of type D → R and
|D| (cid:29) |R|, i.e., each hash is a compression function. A Bloom
ﬁlter calculates hi(x) = d and sets the d’th bit in the m-length
bitvector for all hash functions hi and each feature value x. To test
if an element x(cid:48) is in the feature set, you check that the hi(x(cid:48)) bit
is set for all i. If any are not set, then x(cid:48) is not in the set. Bloom
ﬁlters have false positives due to hash collisions, but never have a
false negative. The false positive rate is reduced, all things being
equal, by adding more hash functions.
Bloom ﬁlters did not work well. The catch in our problem set-
ting is we want to approximate the Jaccard, not perform set mem-
bership tests. We had naively estimated the error rate to be the
expected Bloom ﬁlter collisions in the numerator divided by the ex-
pected number of Bloom ﬁlter collisions in the denominator, which
is mathematically unsound: dividing two expected values will not
provide the proper expectation of the Jaccard expression.
Feature hashing is similar to Bloom ﬁlters where we compute
h(x) = d and set the d’th bit, except that only a single hash func-
tion is used. We call the hashed version of features the malware
ﬁngerprint. Theorem 1 (§ 3.1) shows that the malware ﬁngerprints
provide a near-optimal approximation of the true Jaccard index. To
the best of our knowledge, no previous work (e.g., [15]) has per-
formed similar analysis. Indeed, the proof shows that increasing
the number of hash functions increases error, which is why Bloom
ﬁlters don’t work well. This corresponds well to feature hashing,
where only one hash function is used. Further, requiring only one
hash has obvious performance improvement implications.
We also showed through simulations on random sets of n-grams,
the use of single hash function minimized the difference between
the Jaccard in Equation 1 and the bitvector Jaccard in Equation 2.
In particular, we created two sets that contain 1000 n-grams each,
with varying number of overlapping n-grams, and measured how
much the bitvector Jaccard differs from the Jaccard. Figure 1 shows
the average error as the fraction of common n-grams and the num-
ber of hash functions k vary (the standard deviation is very small
and therefore, not shown). We note that the error increases as k in-
creases, with minimum error achieved at k = 1, which is different
from the usual Bloom ﬁlter set membership tests.
The theoretical and empirical analysis motivates feature hash-
ing in BitShred. The main quality metric for our approach is how
well our approach approximates a full set representation vs per-
formance improvements. We show through numerous experiments
that our approach has extremely high accuracy with up to 2,365
times speedup on a single CPU.
2.2 Co-clustering in BitShred
A BitShred ﬁngerprint is a m-length bitvector where the intuition
is a bit i is 1 if the particular malware sample has a feature gi, and 0
(1)
(2)
 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.20%20%40%60%80%100%Average errorThe fraction of common n-gramsk=3k=2k=1310˛˛˛˛˛˛˛˛˛˛˛
M =
0 1 1 0 1
1 0 0 1 0
0 1 1 0 1
1 0 0 1 0
1 0 0 1 0
˛˛˛˛˛˛˛˛˛˛˛ ⇒
˛˛˛˛˛˛˛˛˛˛˛
1 1 1
1 1 1
0 0 0
0 0 0
0 0 0
0 0
0 0
1 1
1 1
1 1
˛˛˛˛˛˛˛˛˛˛˛ = M
(cid:48)
Figure 2: M is co-clustered to identify the checkerboard sub-
matrix M(cid:48) of highly correlated malware/feature pairs.
otherwise. Given n malware samples, the m-sized list of BitShred
ﬁngerprints can be viewed as a matrix M of size n× m where each
row is a malware ﬁngerprint, and each column is a particular fea-
ture. This intuition leads us to the idea of using co-clustering (aka
bi-clustering) to auto-correlate both features and malware simul-
taneously. Within the matrix, co-clustering does this by creating
sub-matrices among columns (features) and rows (malware) where
each sub-matrix is a highly correlated malware/feature pair.
Co-clustering allows us to discover substantial, non-trivial struc-
tural relationships between malware samples, many of which will
not be discovered with simpler approaches. For example, consider
how the following simple approaches for mining features between
two malware families would be limited:
• Identify all common features between families. In BitShred, this
is accomplished by taking the bitwise-and (∧) of the malware
ﬁngerprints. However, we would miss identifying code that is
present in 99% of family 1 and 99% of family 2.
• Identify all distinctive features in a list of malware. In our set-
ting, this is accomplished with bitwise xor (⊕) of the ﬁnger-
prints. This would have limited value for the same reasons as
above.
• A third approach might be to cluster features either before or
after the malware ﬁngerprints have been clustered. Note, how-
ever, this approach would also result in misleading information,
e.g., clustering features after the clustering malware ﬁngerprints
would not reveal structural similarity across ﬁngerprints in dif-
ferent families, and clustering features before the malware ﬁn-
gerprints may result in poor malware clusters if there are many
feature clusters that are common to multiple groups of malware
ﬁngerprint clusters.
We introduce some terminology to make co-clustering precise.
A matrix is homogeneous if the entries of the matrix are similar,
e.g., they are mostly 0 or mostly 1, and deﬁne the homogeneity of
a matrix to be the (larger) fraction of entries that have the same
value. Deﬁne a row-cluster to be a subset of the rows M (i.e.,
malware samples) that are grouped together, and a column-cluster
to be a subset of the columns (i.e., the features) that are grouped
together. The goal of co-clustering is to create a pair of row and