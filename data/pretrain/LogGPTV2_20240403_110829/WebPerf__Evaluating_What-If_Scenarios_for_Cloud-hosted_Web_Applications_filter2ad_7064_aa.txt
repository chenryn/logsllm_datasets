title:WebPerf: Evaluating What-If Scenarios for Cloud-hosted Web Applications
author:Yurong Jiang and
Lenin Ravindranath Sivalingam and
Suman Nath and
Ramesh Govindan
WebPerf: Evaluating “What-If” Scenarios for
Cloud-hosted Web Applications
Yurong Jiang†, Lenin Ravindranath‡, Suman Nath‡, Ramesh Govindan†
†{yurongji,ramesh}@usc.edu
‡{lenin, sumann}@microsoft.com
†University of Southern California
‡Microsoft Research
Abstract— Developers deploying web applications in the
cloud often need to determine how changes to service tiers
or runtime load may affect user-perceived page load time.
We devise and evaluate a systematic methodology for ex-
ploring such “what-if” questions when a web application
is deployed. Given a website, a web request, and “what-
if” scenario, with a hypothetical conﬁguration and runtime
conditions, our methodology, embedded in a system called
WebPerf, can estimate a distribution of cloud latency of the
request under the “what-if” scenario. In achieving this goal,
WebPerf makes three contributions: (1) automated instru-
mentation of websites written in an increasingly popular task
asynchronous paradigm, to capture causal dependencies of
various computation and asynchronous I/O calls; (2) an al-
gorithm to use the call dependencies, together with online-
and ofﬂine-proﬁled models of various I/O calls to estimate a
distribution of end-to-end latency of the request; and (3) an
algorithm to ﬁnd the optimal measurements to take within a
limited time to minimize modeling errors. We have imple-
mented WebPerf for Microsoft Azure. In experiments with
six real websites and six scenarios, the WebPerf’s median
estimation error is within 7% in all experiments.
CCS Concepts
•Networks → Network performance analysis; •Software
and its engineering → Application speciﬁc development en-
vironments;
Keywords
Instrumentation; Async-Await; Dependency; What-if
1.
INTRODUCTION
Many popular web applications have complex cloud ar-
chitectures, with multiple tiers and inter-related compo-
Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for proﬁt or commercial advantage and that copies bear this notice
and the full citation on the ﬁrst page. Copyrights for components of this work
owned by others than ACM must be honored. Abstracting with credit is per-
mitted. To copy otherwise, or republish, to post on servers or to redistribute to
lists, requires prior speciﬁc permission and/or a fee. Request permissions from
permissions@acm.org.
SIGCOMM ’16, August 22-26, 2016, Florianopolis , Brazil
© 2016 ACM. ISBN 978-1-4503-4193-6/16/08. . . $15.00
DOI: http://dx.doi.org/10.1145/2934872.2934882
258
nents such as compute VMs, SQL and NoSQL storage, ﬁle
systems, communication queues, third party services, geo-
replicated resources, load balancers, etc. It is not uncom-
mon for popular web applications to include tens of different
types of off-the-shelf cloud resources [21, 43]. For example,
airbnb.com, alibaba.com, and netflix.com use
six different types of storage components [43].
Cloud providers such as Microsoft Azure and Amazon
Web Services offer developers a large number of conﬁgu-
ration choices in terms of alternative resource tiers, ﬂexible
geolocation, redundancy, etc. The choices come at varying
cost and performance. Microsoft Azure offers 10 tiers of
Web Servers (in June 2016), with the highest tier up to sev-
eral orders of magnitude faster, but two orders of magnitude
costlier, than the lowest tier (Table 1) for a website and vari-
ous workloads we evaluate in §6.
While the choices give developers ﬂexibility, choosing a
conﬁguration that optimizes cost and performance is chal-
lenging for various reasons (§2). Developers must carefully
evaluate the application’s performance under various conﬁg-
urations. They could try to do this based on cloud provider’s
SLAs, but these are often qualitative and insufﬁcient to pre-
dict quantitative performance. Even if quantitative perfor-
mance of a speciﬁc cloud resource under a conﬁguration
were available, they may need to translate that to end-to-end
latency, which would require them to understand application
logic and underlying concurrency. Developers could also try
using data-driven techniques that rely on the application’s
historical performance on various conﬁgurations [10, 42];
but these can be slow and expensive, and are not suitable for
ﬁrst-time deployment.
In this paper, we address this challenge through the de-
sign, implementation, and evaluation of WebPerf, a tool that
estimates the distribution of cloud-side latency of a web re-
quest under hypothetical changes (called what-if scenarios)
to the cloud-side conﬁguration of a web application. By re-
peatedly trying several what-if scenarios, a developer can
quickly determine the best resource tier conﬁguration that
ﬁts her budget.
The key idea behind WebPerf is to combine data-driven,
ofﬂine latency models of various cloud APIs with applica-
tion logic abstracted as causal dependency (i.e., execution
order) of various compute and I/O calls to compute the total
cloud latency of a given request. This idea is motivated by
two key insights. First, causal dependencies of compute and
I/O calls for a request remain relatively stable for the large
class of what-if scenarios that WebPerf supports. Thus, it is
possible to determine the dependencies once, and to reuse
it for repeated what-if evaluations. Second, latencies of I/O
calls to many cloud resources can be reliably modeled as
functions of workload parameters such as storage table size
and conﬁguration parameters such as geolocation and re-
source tier, but independent of application logic. Therefore,
WebPerf can build application-independent models of vari-
ous cloud APIs ofﬂine, as a function of workload and con-
ﬁguration parameters, and use these models during what-if
estimation.
Implementing the above idea for cloud-based web appli-
cations raises several challenges, which we address. First,
while prior work has explored techniques to track causal
dependencies in asynchronous applications [32, 33], mod-
ern web applications are written using task asynchronous
paradigm [2] that prior work has not considered. We develop
techniques to track causalities in such applications.
Second, predicting overall cloud latency by combining
causal dependencies with latency models of cloud APIs is
challenging for various reasons. Dependencies among com-
pute and I/O calls in real applications can be complex. More-
over, latency models of cloud APIs are best represented as a
distribution rather than as ﬁxed values, and hence the predic-
tion algorithm needs to combine the distributions. WebPerf
uses a novel algorithm that hierarchically convolves these
distributions, in a manner guided by the causal dependence.
Finally, there are many other practical considerations that are
needed for accurate prediction. A request may have nonde-
terministic causal dependency (e.g., due to CDN hit or miss).
Cloud resources can impose concurrency limits and hence
introduce queueing delays. Latencies of some data-centric
APIs (e.g., to a SQL database) may depend on application
and workload properties. WebPerf’s prediction algorithm
uses several techniques to handle such cases.
While WebPerf’s primary goal is to predict an applica-
tion’s cloud-side latency, its prediction can be combined
with network latency and client-side latency (predicted by
existing tools such as WebProphet [25]) to produce an end-
to-end predicted latency distribution. WebPerf proposes a
Monte Carlo simulation-based algorithm to achieve this.
Finally, WebPerf must be fast so that a developer can
quickly explore various what-if scenarios. Given a what-
if scenario, WebPerf needs to build latency models for all
computation and I/O calls in the application to reason about
its baseline performance. To operate within a ﬁxed measure-
ment time limit, WebPerf formulates the measurement prob-
lem as an integer programming problem that decides how
many measurements of various requests should be taken to
optimize total modeling error (the process is similar to op-
timal experiments design [31, 44]). Our experiments show
this can minimize modeling errors by 5× on average.
We have implemented WebPerf for Microsoft Azure and
evaluated it on six websites and six what-if scenarios.
WebPerf’s median estimation error was within < 7% in all
cases.
2. BACKGROUND AND MOTIVATION
Platform as a Service (PaaS) cloud providers such as
Amazon AWS and Microsoft Azure let developers rapidly
build, deploy, and manage powerful websites and web ap-
plications. A typical cloud application has multiple tiers.
(See [21, 43] for example architectures of real-world appli-
cations.) The back-end data tier consists of various data stor-
age services, such as SQL databases, key-value stores, blob
stores, Redis caches [34], etc. The front-end contains the
core application logic for processing client requests. It may
also include various cloud resources such as VMs, commu-
nication queues, analytics services, authentication services,
etc. The front-end of a web application also includes a web
server that generates HTML webpages to be rendered on
browsers.
2.1 The need for what-if analysis
Cloud providers offer multiple resources for computation,
storage, analytics, networking, management, etc. For exam-
ple, Microsoft Azure offers 48 resource types in 10 differ-
ent categories.1 Each resource is usually offered in multiple
tiers at different price, performance, and isolation level—
for a Web server alone, Azure offers 30 different resource
tiers (a few are shown in Table 1). Finally, cloud providers
also permit redundancy and geolocation of resources. A
single website may use multiple resources; for example,
airbnb.com, alibaba.com, and netflix.com each
use six different types of storage components [43]. Thus, a
developer is faced with combinatorially many resource con-
ﬁguration choices in terms of the number of resource tiers,
the degree of performance isolation across these tiers, the
redundancy of resources, and geolocation of resources.
In this paper, we are primarily concerned with cloud la-
tency, the time a user request spends in the cloud. Some
of this latency is due to the network (when the front-end and
back-end are not geographically co-located) and some of this
latency is due to compute and storage access. Not surpris-
ingly, the choice of a web application’s cloud conﬁguration
can signiﬁcantly impact cloud latency. A developer needs to
be able to efﬁciently search the space of conﬁgurations to
select a conﬁguration that satisﬁes the developer’s goal such
as minimizing cloud latency of a request given a budget or
meeting a deadline while minimizing the cost.
One approach could be to deploy each conﬁguration and
measure cloud latency; but this can be expensive and slow.
Alternatively, the developer could try to determine cloud la-
tency of a request from cloud providers’ SLAs. This is hard
because resource tier SLAs are often described qualitatively.
Microsoft Azure lists performance of Web Server tiers in
terms of CPU core counts, memory sizes, and disk sizes
(Table 1). Redis cache performance is speciﬁed as high,
medium or low. Translating such qualitative SLAs to quanti-
tative performance is hard. Performance can also depend on
runtime conditions such as load. Our results in Table 1 show
that under no load, the lowest and the highest tier of Web
Servers, whose prices differ by 100×, have relatively simi-
1All Azure offerings are reported as of June 2016.
259
Web Server Tier
Price
(USD/month)
Conﬁguration
Avg. Response Time/Request (ms)
1 req.
100 concurrent reqs.
90% CPU
Standard A0
Standard A1
Standard D2
Standard A3
Standard D12
Standard D14
15
67
208
268
485
1571
1 Core, 0.75 GB Memory, 20 GB Disk
1 Core, 1.75 GB Memory, 70 GB Disk
2 Cores, 7 GB Memory, 100 GB Disk
4 Cores, 7 GB Memory, 285 GB Disk
4 Cores, 28 GB Memory, 200 GB Disk
16 Cores, 112GB Memory, 800GB Disk
107.51
99.6
97.4
93.4
96.2
90.1
123837.6
15770.8
2371.0
1720.70
1275.52
752
283.6
210.3
190.4
142
130.4
115
Table 1—A few tiers of Microsoft Azure Web Server: Price, Conﬁguration, and Performance under various conditions. Response times are measured from a
client in California to the index page of the SocialForum website (§ 6), deployed at a Web Server in US West.
lar latency; but with 100 concurrent clients, the lowest tier
is 164× slower than the highest tier. Finally, even if it were
possible to accurately quantify latency of a speciﬁc resource
tier, estimating its impact on the total cloud latency requires
understanding how I/O calls to the resource interleave with
other I/O calls and how they affect the critical path of the
web request. This is nontrivial.
functions of various workload and conﬁguration parame-
ters. For example, latency of a Redis cache lookup API does
not depend on the application, but on its conﬁguration such
as its geolocation and its resource tier. Similarly, latency
to a NoSQL table query API does not depend on applica-
tion logic, but rather on workload parameters such as query
type (e.g., lookup vs. scan) and table size. Therefore, it is
possible to build application-independent statistical models
(called proﬁles) of these APIs ofﬂine, as functions of rele-
vant workload and conﬁguration parameters, and combine
it with application-speciﬁc dependency information to esti-
mate a request’s cloud latency.
To efﬁciently search the space of resource conﬁgurations,
our paper develops WebPerf, a prediction framework that
can perform what-if analyses—given a hypothetical resource
conﬁguration of an application and its workload, it can ac-
curately predict a request’s cloud latency without actually
executing the application under the new conﬁguration, and
without relying on qualitative SLA descriptions, while still
capturing interleavings between different I/O calls within a
given request.
2.2 Key Insights
Unlike common website optimization tools [17, 45] that
focus on webpage optimization, WebPerf enables develop-
ers to ﬁnd low cloud latency resource conﬁgurations. The
cloud latency of an application depends on (1) the causal or-
der (i.e., sequential or parallel) in which various computation
and I/O calls happen, and (2) the latency of each computa-
tion and I/O call. We use the following two key insights to
measure these two components.
(cid:73) Stable Dependency. Causal dependencies of various
computation and I/O calls of a request in an application re-
main stable over various what-if scenarios that we consider
(Table 2). For example, if a request accesses a key-value
store followed by accessing a blob store, it accesses them in
the same causal order even when the key-value store or the
blob store is upgraded to a different tier. Such determinism
allows WebPerf to compute causal dependency for a request
once and reuse it for repeated what-if analysis.
Of course, there can be nondeterminism in control paths—
e.g., a request may or may not query a database depending
on whether a value is present in the cache.
In that case,
WebPerf issues the request repeatedly to stress various con-
trol paths and produces one estimate for each unique causal
dependency. Such non-determinism, however, is relatively
infrequent—in six real applications we evaluate in §6, only
10% requests demonstrate such nondeterminism.
In §4.3,
we describe how WebPerf handles variable latencies due to
different control paths inside a cloud resource.
(cid:73) Application-independent API latency. The performance
of individual I/O calls to many cloud resources can be re-
liably modelled independent of application logic, but as
260
To verify application-independence, we measured the la-
tency distribution of various I/O calls made by several real-
world applications, and compared them with those generated
by an application-independent proﬁler. Given an API and
an application, we compute relative error of the proﬁler as
|l−l(cid:48)|/l, where l and l(cid:48) are the latencies of the API measured
from the application and the proﬁler respectively. Figure 1
shows mean and 90th percentile relative errors for all 11 I/O
APIs used by SocialForum, a real Azure application we de-
scribe in §6. The mean error ranges from 0.4% to 6.5% while
the 90th percentile error ranges from 5% to 10%. Overall,
the error is relatively small for other scenarios, conﬁrming