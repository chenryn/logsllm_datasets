CorrOpt taking the links off. This can cause packet re-ordering and
lower network performance temporarily. Flowlet re-routing [1] can
avoid this problem.
8 FUTURE EXTENSIONS
We discuss a few directions for extending CorrOpt.
Accounting for the impact of repair. While disabling a link
has limited local effect, we found in our deployment that repairing it
can sometimes cause collateral damage. For example, when one link
in a breakout cable has packet corruption, to repair the breakout
cable, an additional three, healthy links have to be turned off. A
future extension of CorrOpt would be to account for such collateral
impact, when deciding which links to disable for repair, based on a
finer-grained view of the topology and its dependencies.
Removing traffic instead of disabling links Today, when
CorrOpt completely disables corrupting links (when capacity con-
straints permit that), but based on our deployment experience, we
believe that removing traffic from the link (e.g., by increasing its
routing cost) is a better strategy. This way, when the link is repaired,
we can run test traffic to confirm if the repair succeeded without
affecting actual traffic. Further, monitoring data on optical power
levels will continue to flow in (which otherwise stops when the
link is disabled).
9 RELATED WORK
Our work follows a rich body of work on understanding, diagnosing,
and mitigating faults in large, complex networks. While it is not
possible to cover everything here, we place our work in the context
of the most relevant, recent work.
Packet corruption in DCNs. Some work has acknowledged
corruption as a problem. For instance, NetPilot [34] observes that
corruption hurts application performance and develops a method to
locate corrupting links without access to switch counters. RAIL [38]
studies the optical layer of DCNs, finding that RxPower is generally
high, but instances of low RxPower can cause packet corruption. We
study packet corruption in more detail, including its characteristics
and root causes (we find these go beyond low RxPower).
Faults in DCNs. Many prior studies have considered other types
of faults in DCNs [6, 8, 19]. For instance, Gill et al. [18] study equip-
ment failures in DCNs, characterizing different elements’ downtime
and failure numbers, combined with an impact estimation and re-
dundancy analysis. Our focus is on a different type of fault—packet
corruption—and its mitigation.
Understanding and Mitigating Packet Corruption in DCNs
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
Fault mitigation. Most work on fault mitigation focuses on
congestion or fail-stop faults, using techniques such as load bal-
ancing and fast rerouting [1, 7, 25, 33, 35]; however, these are less
relevant for corrupting links (e.g., reducing traffic on the link will
not alleviate corruption). RAIL [38] studies a setting where cor-
ruption is the norm (because optical transceivers are used in a
non-conventional manner) and places only loss-tolerant traffic on
corrupting links. We view corruption as an anomaly and mitigate it
by disabling corrupting links, so they can be repaired. zUpdate [24]
and NetPilot [34] depend on knowledge of future traffic demand to
further reduce congestion loss when handling corrupting links or
network update. Those techniques are complementary to CorrOpt’s
link disabling techniques. CorrOpt can work in data center settings
where future traffic demand is not available.
Root cause diagnosis. Using optical-layer characteristics to
diagnose network faults was previously proposed by Kompella
et al. [23]. Ghobadi et al. [16] use optical-layer statistics to help
predict failures in backbone networks. Our work uses an optical-
layer monitor to help determine the root cause of packet corruption
in DCNs.
10 CONCLUSION
Our analysis of packet corruption across many DCNs showed that
the extent of corruption losses is significant. It also showed that,
compared to congestion, corruption impacts fewer links but im-
poses heavier loss rates, and the corruption rate of a link is tempo-
rally stable and uncorrelated to its utilization. CorrOpt, our system
to mitigate corruption, lowers corruption losses by three to six
orders of magnitude by intelligently selecting which corrupting
links to disable while meeting configured capacity constraints. It
also generates repair recommendations that are guided by common
symptoms of different root causes. This recommendation engine
is deployed on all data centers of a large cloud provider, where it
improved the accuracy of repair by 60%.
ACKNOWLEDGMENTS
We thank David Bragg, Jamie Gaudette and Shikhar Suri for help-
ing us design the recommendation engine. We thank Hui Ma and
Shikhar Suri for helping with the deployment of our monitoring
system and the recommendation engine. We also thank our shep-
herd Minlan Yu and the anonymous reviewers for their helpful
feedback on the paper. Klaus-Tycho Förster is supported by the
Danish VILLUM FONDEN project “Reliable Computer Networks
(ReNet)”.
REFERENCES
[1] Mohammad Alizadeh, Tom Edsall, Sarang Dharmapurikar, Ramanan
Vaidyanathan, Kevin Chu, Andy Fingerhut, Vinh The Lam, Francis Ma-
tus, Rong Pan, Navindra Yadav, and George Varghese. 2014. CONGA: Distributed
Congestion-aware Load Balancing for Datacenters. In SIGCOMM.
[2] Mohammad Alizadeh, Albert Greenberg, David A. Maltz, Jitendra Padhye,
Parveen Patel, Balaji Prabhakar, Sudipta Sengupta, and Murari Sridharan. 2010.
Data Center TCP (DCTCP). In SIGCOMM.
[3] Mohammad Alizadeh, Abdul Kabbani, Tom Edsall, Balaji Prabhakar, Amin Vahdat,
and Masato Yasuda. 2012. Less is More: Trading a Little Bandwidth for Ultra-low
Latency in the Data Center. In NSDI.
[4] Mohammad Alizadeh, Shuang Yang, Milad Sharif, Sachin Katti, Nick McKeown,
Balaji Prabhakar, and Scott Shenker. 2013. pFabric: Minimal Near-optimal Data-
center Transport. In SIGCOMM.
[13] FiberStore. 2017.
Proportional Rate Reduction for TCP. In IMC.
fiber-optic-inspection-tutorial-aid-460.html. (2017).
Fiber Optic Inspection Tutorial.
http://www.fs.com/
[5] Behnaz Arzani, Selim Ciraci, Boon Thau Loo, Assaf Schuster, and Geoff Outhred.
2016. Taking the Blame Game out of Data Centers Operations with NetPoirot. In
SIGCOMM.
(2014).
[6] Peter Bailis and Kyle Kingsbury. 2014. The Network is Reliable. Commun. ACM
[7] Theophilus Benson, Ashok Anand, Aditya Akella, and Ming Zhang. 2011. Mi-
croTE: Fine Grained Traffic Engineering for Data Centers. In CoNEXT.
[8] Kashif Bilal, Marc Manzano, Samee U. Khan, Eusebi Calle, Keqin Li, and Albert Y.
Zomaya. 2013. On the Characterization of the Structural Robustness of Data
Center Networks. IEEE Trans. Cloud Computing (2013).
[9] Peter Bodík, Ishai Menache, Mosharaf Chowdhury, Pradeepkumar Mani, David A.
Maltz, and Ion Stoica. 2012. Surviving Failures in Bandwidth-constrained Data-
centers. In SIGCOMM.
[10] Neal Cardwell, Yuchung Cheng, C. Stephen Gunn, Soheil Hassas Yeganeh, and
Van Jacobson. 2016. BBR: Congestion-Based Congestion Control. ACM Queue
(2016).
[11] J. D. Case, M. Fedor, M. L. Schoffstall, and J. Davin. 1990. Simple Network
Management Protocol (SNMP). (1990).
[12] Nandita Dukkipati, Matt Mathis, Yuchung Cheng, and Monia Ghobadi. 2011.
[14] Fiber for Learning. 2017. Fiber Hygiene. http://fiberforlearning.com/welcome/
2010/09/20/connector-cleaning/. (2017).
[15] M. R. Garey and David S. Johnson. 1979. Computers and Intractability: A Guide to
[16] Monia Ghobadi and Ratul Mahajan. 2016. Optical Layer Failures in a Large
the Theory of NP-Completeness. W. H. Freeman.
Backbone. In IMC.
[17] Monia Ghobadi, Ratul Mahajan, Amar Phanishayee, Nikhil Devanur, Janard-
han Kulkarni, Gireeja Ranade, Pierre-Alexandre Blanche, Houman Rastegarfar,
Madeleine Glick, and Daniel Kilper. 2016. ProjecToR: Agile Reconfigurable Data
Center Interconnect. In SIGCOMM.
[18] Phillipa Gill, Navendu Jain, and Nachiappan Nagappan. 2011. Understanding
Network Failures in Data Centers: Measurement, Analysis, and Implications. In
SIGCOMM.
[19] Ramesh Govindan, Ina Minei, Mahesh Kallahalla, Bikash Koley, and Amin Vahdat.
2016. Evolve or Die: High-Availability Design Principles Drawn from Googles
Network Infrastructure. In SIGCOMM.
[20] Peter Hoose. 2011. Monitoring and Troubleshooting, One Engineer’s rant. https:
//www.nanog.org/meetings/nanog53/presentations/Monday/Hoose.pdf. (2011).
[21] JDSU. 2017. P5000i Fiber Microscope. http://www.viavisolutions.com/en-us/
products/p5000i-fiber-microscope. (2017).
[22] Edward John Forrest Jr. 2014. How to Precision Clean All Fiber Optic Connections:
[23] Ramana Rao Kompella, Albert Greenberg, Jennifer Rexford, Alex C. Snoeren, and
A Step By Step Guide. CreateSpace Independent Publishing Platform.
Jennifer Yates. 2005. Cross-layer Visibility as a Service. In HotNets.
[24] Hongqiang Harry Liu, Xin Wu, Ming Zhang, Lihua Yuan, Roger Wattenhofer,
and David A. Maltz. 2013. zUpdate: Updating Data Center Networks with Zero
Loss. In SIGCOMM.
2013. F10: A Fault-Tolerant Engineered Network. In NSDI.
com/video/4f277939-73f5-4ce8-aba1-3da70ec19345. (2016).
[25] Vincent Liu, Daniel Halperin, Arvind Krishnamurthy, and Thomas Anderson.
[26] David Maltz. 2016. Keeping Cloud-Scale Networks Healthy. https://video.mtgsf.
[27] Jitendra Padhye, Victor Firoiu, Don Towsley, and Jim Kurose. 1998. Modeling
TCP Throughput: A Simple Model and Its Empirical Validation. In SIGCOMM.
[28] Jonathan Perry, Amy Ousterhout, Hari Balakrishnan, Devavrat Shah, and Hans
Fugal. 2014. Fastpass: A Centralized "Zero-queue" Datacenter Network. In SIG-
COMM.
[29] Peng Sun, Ratul Mahajan, Jennifer Rexford, Lihua Yuan, Ming Zhang, and Ahsan
Arefin. 2014. A Network-state Management Service. In SIGCOMM.
cleaning_tools/ibc_brand_cleaners_for_single_fiber_connections.htm. (2017).
[30] USConec. 2017. Single Fiber Cleaning Tools. http://www.usconec.com/products/
[31] Balajee Vamanan, Jahangir Hasan, and T.N. Vijaykumar. 2012. Deadline-aware
Datacenter TCP (D2TCP). In SIGCOMM.
[32] Christo Wilson, Hitesh Ballani, Thomas Karagiannis, and Ant Rowtron. 2011. Bet-
ter Never Than Late: Meeting Deadlines in Datacenter Networks. In SIGCOMM.
[33] Damon Wischik, Costin Raiciu, Adam Greenhalgh, and Mark Handley. 2011.
Design, Implementation and Evaluation of Congestion Control for Multipath
TCP. In NSDI.
[34] Xin Wu, Daniel Turner, George Chen, Dave Maltz, Xiaowei Yang, Lihua Yuan, and
Ming Zhang. 2012. NetPilot: Automating Datacenter Network Failure Mitigation.
In SIGCOMM.
[35] Kyriakos Zarifis, Rui Miao, Matt Calder, Ethan Katz-Bassett, Minlan Yu, and
Jitendra Padhye. 2014. DIBS: Just-in-time Congestion Mitigation for Data Centers.
In EuroSys.
[36] Yibo Zhu, Haggai Eran, Daniel Firestone, Chuanxiong Guo, Marina Lipshteyn,
Yehonatan Liron, Jitendra Padhye, Shachar Raindel, Mohamad Haj Yahia, and
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
D. Zhuo et al.
Ming Zhang. 2015. Congestion Control for Large-Scale RDMA Deployments. In
SIGCOMM.
[37] Yibo Zhu, Nanxi Kang, Jiaxin Cao, Albert Greenberg, Guohan Lu, Ratul Mahajan,
Dave Maltz, Lihua Yuan, Ming Zhang, Ben Y. Zhao, and Haitao Zheng. 2015.
Packet-Level Telemetry in Large Datacenter Networks. In SIGCOMM.
[38] Danyang Zhuo, Monia Ghobadi, Ratul Mahajan, Amar Phanishayee, Xuan Kelvin
Zou, Hang Guan, Arvind Krishnamurthy, and Thomas Anderson. 2017. RAIL: A
Case for Redundant Arrays of Inexpensive Links in Data Center Networks. In
NSDI.
A PROOF OF THEOREM 1
To prove Theorem 5.1, we first prove the following Lemma:
Lemma A.1. Let N = (V , E) be a degraded Fat-Tree, where some
links are turned off. Let L ⊆ E be the set of enabled links with cor-
ruption. Finding a set L′ ⊆ L whose removal minimizes the impact of
packet corruption s.t. all ToR switch pairs are still connected the the
spine via valley-free routing after the removal of L′ is NP-hard.
Proof. Our NP-hardness reduction is via the NP-complete prob-
lem 3-SAT, in the variant with exactly three literals per clause. Let
I be an instance of 3-SAT [15], with k clauses C1, . . . , Ck and vari-
ables x1, . . . , xr , k ≥ r. We create an instance I′ of our problem as
follows: Consider a 4k-Fat-Tree, consisting of the three layers ToR,
Agg, and the spine, where the Agg switches have 2k links down-
and upwards, respectively. Pick one pod P with 2k ToR switches,
C1, . . . , Ck, corresponding to the clauses in I, and H1, . . . , Hk, as
helper switches, and lastly, 2r Agg switches X1,¬X1, . . . , Xr ,¬Xr ,
corresponding to the possible literals in I, and, 2k − 2r ≥ 0 further
Agg switches A1, . . . , A2k−2r . Let only the following three sets of
links not be turned off in this pod P: 1) For each Ci, the links point-
ing to the Agg switches representing the corresponding literals
contained in the clause in I, 2) for each Hj from H1, . . . , Hr , one
link each to Xj , (cid:44) Xj, 3, for each Hr +1, . . . , Hk, one link each to
X1,¬X1. For the connection of the Agg switches in P to the neigh-
boring spine switches, let only the following links L not be turned
off, with |L| = 2r: From each Agg switch X1,¬X1, . . . , Xr ,¬Xr in P,
one link to a neighboring spine switch. We set all links in L to have
the same corruption properties greater than zero. The construction
is illustrated in Figure 21.
To guarantee valley-free connection of all ToR switches in P to
all other ToR switches in the other pods via the spine, each ToR
switch Ci and Hj needs to have a connection to the spine, with the
last part of each of those paths being a link from L. To maximize the
set of links L′ ⊆ L that can be turned off, consider the following:
First, for each pair of Agg switches Xj ,¬Xj, at least one needs
to remain connected to the spine, or the ToR switch Hj would be
disconnected (or for the case of j = 1, even more switches), meaning
|L′| ≥ r. Second, to connect each switch Ci to the spine, at least
one its Agg switches (representing the literals of the clause) have
to be connected to the spine. As thus, a solution to a satisfiable
3-SAT instance I tells us how to to pick which of the links from each
Xi ,¬Xi pair should remain connected to the spine, and vice versa,
a solution with |L′| = r from I′ shows how to satisfy I. On the other
hand, should I not be satisfiable, then no solution with |L′| ≤ r
can exist for I′, and vice versa as well. We note that the size of the
instance I′ is polynomial in k, i.e., we showed NP-hardness.
□
Since we assumed the same fl on every link, I can be chosen
arbitrarily, as long as I ( fl ) > 0. We can now prove Theorem 5.1:
(a) Clause gadget for C = (X1∨X2∨¬X3). Each clause needs
to connect to the spine via one of its contained literals.
(b)
Variable
gadget
Figure 21: Reduction from 3-SAT: Each clause C is repre-
sented by a ToR switch, the literals X and¬X of each variable
are represented by Agg switches. The ToR switches C are
connected to the literals contained in their corresponding
clause, cf. the clause gadget in 21a: Hence, at least one of the
three literals needs to be connected to the spine. Each literal
pair X ,¬X is connected to at least one further ToR switch H,
as shown in the variable gadget in 21b, ensuring that at least
one literal of each variable is connected to the spine. The lit-
erals (Agg switches) only have one connection each to the
spine, and all these connections are faulty, with the same er-
ror properties each. Finding a maximum set of these faulty
links to turn off, s.t. all Tor switches still have valley-free
connections to the remaining network, is NP-hard.
Proof. Lemma A.1 assumed the network to be already degraded,
i.e., some links L are turned off. Note that a Fat-Tree is a special case
of a Clos topology. We can extend the NP-hardness to the setting
of Theorem 5.1 as well, by the following change in construction:
Assume the errors on every link in L are so high in comparison
to the errors on the links from L, that for every link l ∈ L holds:
Disabling l is more efficient regarding the impact of currption than
turning off all links in L. Lastly, to show that the underlying decision
problem is in NP, observe that checking the capacity constraints
and total impact of packet corruption of a given solution can be
performed in polynomial time.
□
The above proof constructions can also be used as follows:
Optimizing for link removal. Another objective instead of total
packet corruption could be the number of further links that can be
removed. However, as all links in the proof of Lemma A.1 had the
same error properties, NP-completeness still holds.
From ToR-spine connectivity to ToR-ToR connectivity. Lastly, in
the above problem formulations, we wanted to maintain the ToR to
spine connectivity of all ToR switches. However, we just considered
a single pod P in each Fat-Tree construction, with the ToR switches
in all other pods retaining all their connectivity to the spine. As thus,
the above problems are also NP-complete for ToR-ToR connectivity,
with the same proof constructions.