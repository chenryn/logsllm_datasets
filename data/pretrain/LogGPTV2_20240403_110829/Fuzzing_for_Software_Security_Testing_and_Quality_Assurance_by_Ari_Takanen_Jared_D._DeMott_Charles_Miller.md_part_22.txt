How does fuzzing as part of test automation help reduce the total time used in test-
ing? Does it impact the timelines of developers? How does automation help with
repeatability? In test automation we need to be able to show that the same set of
tests, potentially augmented with some new tests, will be executed each time the
fuzzing process is launched through the test automation framework. Automation
has to be repeatable or it is worthless. Test automation does not necessarily reduce
human error if there is manual work involved in the repetition of the tests. One small
problem in the automation process could lead to missing bugs that would have been
covered earlier. This could result from, for example, a change in the communication
interface or the data formats being fuzzed. There are pros and cons to both pre-
defined tests and dynamically changing tests from the test automation perspective.
From a QA standpoint the goals of automation are:
1. Increase the amount of tests you do in a set timeframe when compared to
manual testing;
2. Liberate your testers to do more of the interesting testing and avoid manual
repetition of simple tasks;
3. Cost savings related to both the test efficiency and the direct costs of tools
and test integration.
Increasing the number of tests you perform in an automated fashion will speed
up your testing process, just as doing the same tests with more manual interven-
tion would slow the process down. In fuzzing, it is not really a choice of doing it
manually or automatically, it is whether you do it at all. Fuzzing will liberate your
testers to do more interesting testing activities, which likely includes such things
as detailed analysis of the test results. Fuzzing requires repeatability and speeds up
the development process. Without people creating the tests, the tests have a chance
of having better coverage, since they do not depend on human knowledge. A good
security tester is always good at what he or she does. But unfortunately, most com-
panies do not have enough talented security testers and never will have. Therefore,
the testing skills need to be built into the test automation tool and must not depend
on the competence of the tester.
Common arguments against test automation are based on experiences with bad
test automation products. This is because the definition of test automation by many
testing vendors is completely wrong. Some test automation frameworks require
more work to initially build the tests than is saved with repeated test execution. In
fact, most test automation methods and techniques follow all the worst practices
of test automation.
6760 Book.indb 133 12/22/17 10:50 AM
134 Fuzzing Metrics
For example, some people think that security testing is the same as monkey
testing—randomly trying all types of inputs. To those people, test automation is
about trying more and more random inputs in a feeble attempt to increase test cov-
erage. This approach is doomed to fail, because adding more tests does not guar-
antee better tests. Compared to just adding more random testing, manual testing
by a talented security expert is definitely better, because it is based on a process
that adapts to change.
To some, test automation means repeating the same test case over and over
again. This applies to many test automation frameworks that focus on conformance
or performance issues. In fuzzing, each test case should be unique. The purpose is
not to repeat, but to create many unique test cases that would be impossible to cre-
ate using manual testing. The same also applies to statements related to automat-
ing testing actions that are repeated over and over again in the test process. This
is irrelevant to fuzzing, because the entire process needs to be repeated, instead of
just one single test case. Running millions of tests with any other means but test
automation would take a significant amount of time to execute and analyze.
In relation to fuzzing (and fuzzing only!), test automation also reduces human
error in test design, implementation, execution, and analysis. There can be human
errors in fuzzer development itself, because a test automation tool is still a software
product and can have flaws. The majority of existing test automation frameworks
for performance testing and conformance testing depend on complex user configura-
tion, and one mistake in the configuration can invalidate the entire test run. Many
testing frameworks also leave the actual test design for the tester. A fuzzer frame-
work is often a test design framework if the user needs to parameterize the protocol
elements that are fuzzed. The resulting test suite should still be repeatable without
user involvement. Some fuzzers on the other hand have predefined intelligence and
have reduced the possibility of the tester to make mistakes in the test setup.
4.5 Summary
While building fuzzers isn’t always pleasurable, the act of fuzzing is fun! However,
not everyone is excited by the hunt for zero-day flaws and exploiting those found
vulnerabilities. One of the most difficult issues with fuzzing is trying to convince
others that it is an important part of a vulnerability assessment process and that it
should be integrated into existing quality assurance processes. You might under-
stand the importance of fuzzing, but cannot convince others to see its benefits. To
others, fuzzing is seen as an unnecessary cost and potential delay in the product
launch. Too many times we have heard people question fuzzing with questions like
• Why should I use fuzzing? This is not our responsibility!
• Why do you look for security problems in our products?
Other questions that you might encounter are related to the problems that fuzzers
find. Fuzzers are not silver bullets in the sense that they find all security problems.
Explaining that fuzzers are just one more tool in the tool chest of security experts
6760 Book.indb 134 12/22/17 10:50 AM
4.5 Summary 135
and quality assurance professionals is important. This lack of understanding for
the methodology and findings can reveal itself in questions like these:
• Can I find distributed denial of service problems with fuzzing?
• Can I find the famous bug that the Code Red worm exploited?
• Surely, that would never happen in real-life usage scenarios, would it?
• Is that really exploitable? Isn’t it denial of service only?
• But I am already doing testing. Why should I use fuzzing?
• Aren’t these problems covered with our code auditing tools?
Finally, choosing the right tool for the right task is sometimes difficult. How
do you compare the various tools, and how do you know which tool is the most
useful tool for each task during the product life cycle? You should be prepared to
provide insight to your colleagues on product comparisons and the costs related to
various fuzzing approaches. Relying on the marketing and sales skills of various
fuzzing vendors or on the hype created by various hacker community tools can get
you distracted from the real purpose of why you became interested in fuzzing in
the first place. Be prepared for questions like
• What is the best fuzzing tool that I can use?
• What is the cheapest fuzzing tool that I can use?
You should be prepared to give answers to questions like these to your supervi-
sors, to your customers, or to the manufacturers that do not still get it. Fuzzing is
a fascinating technology, and all fuzzing approaches will definitely be better than
none at all. Still, fuzzing is not about technology, but rather, the final results from
the tests.
In this chapter we explained some reasoning behind integration of fuzzing into
your existing quality assurance and vulnerability analysis processes. As with secu-
rity in general, fuzzing starts with a threat analysis of the target system. Fuzzing
is always a form of black-box testing in the sense that the tests are provided to the
system under test through different interfaces. These interfaces can be remote or
they can be local interfaces that only local users can abuse. Fuzzing at its best is
a proactive technique for catching vulnerabilities before any adversaries will find
them and exploit them. The earlier they are found, the cheaper it is to fix them.
Costs related to vulnerabilities caught late in the product life cycle might initiate
crisis communication with chaotic results, and a lot of unexpected costs related
to the actual discovery, remediation, and finally patch deployment. Whether a
manufacturer chooses reactively to buy vulnerability data from bug bounty hunt-
ers or subcontract security auditing of released product to security professionals
is a business decision. Our goal is to convince them that proactive tools should
be used instead of this last-minute approach. With adequate expertise it could be
beneficial to build your own fuzzer, or to integrate an existing fuzzer product into
your product development. We explored some case studies on how the costs for
each of these options could be estimated before jumping into conclusions on the
available choices.
6760 Book.indb 135 12/22/17 10:50 AM
136 Fuzzing Metrics
Fuzzing is about test automation at its fullest extent. People used to traditional
test automation might think about complex test design frameworks that are time
consuming to use and that produce difficult-to-measure results. Fuzzers, on the
other hand, can be fully automated, in the sense that you just press play, watch the
fuzzer run, and wait for the results. Test automation comes in many flavors and
requires understanding of how it works before dedicating too many resources into
building a new one on your own.
One problem with security is that engineers often don’t even know or under-
stand security vulnerabilities. How can a virus misuse a buffer overflow problem in
a product? How critical is a denial of service problem? What is a buffer overflow/
format string vulnerability/null pointer deference anyway? Fuzzers can be integrated
into the development process with or without this knowledge. Think about explain-
ing to a developer why he should never use the strcpy function in his program.
Then think again about showing him a crash by sending a long string of characters
with that vulnerability in the software. Fuzzing has no false positives, meaning
that every single flaw found with fuzzing is exploitable to some level. It might not
allow remote code execution, but this is still a real bug. A crash is a crash. A bug
is a bug. Developers will immediately rush into fixing crash-level flaws or memory
leaks in their software when told to do so. They do not necessarily need to know
about the difference between stack and heap overflows or the exploitability of the
flaws. Still, all found issues need to be prioritized at some level. Some of you might
already be familiar through experience with what happens if you suddenly report
hundreds of remotely exploitable flaws found using fuzzing tools to a manufacturer
that has never had a security problem in its lifetime.
Now that we understand why we need to fuzz, and how we can communicate
the importance of fuzzing, we can study the actual techniques of fuzzing with
completely new eyes. Compare the techniques presented later with the metrics we
explored here and hopefully you will find the technique that best suits you.
6760 Book.indb 136 12/22/17 10:50 AM
C h a p t e r 5
Building and Classifying Fuzzers
In this chapter we will present basic fuzzer construction details. Endless methods to
create and deliver semi-valid data could be contrived; we will hit the most prevalent
techniques. Available open source tools will be described. A comparison of com-
mercial and open source tools will be held in Chapter 8. We begin by defining the
two primary types of fuzzers. These have been called full-blown and capture-replay
when dealing with network fuzzers, but generation and mutation are more generi-
cally accepted terms that also apply to file fuzzing. File fuzzing has continued to be
important as network protectors1 and network application designers have become
more security conscious.2 Client-side attacks, while sometimes requiring an ele-
ment of social engineering,3 have become like the previously more fertile grounds
of network daemon auditing from years gone by.
5.1 Fuzzing Methods
Fuzzer test cases are generated by the input source and attack heuristics and ran-
domness. Test cases could be developed manually, although this is not the preferred
method since many test cases will be needed. Generally, test cases come from
either a library of known heuristics or by mutating a sample input. Input source
refers to the type of fuzzer being used. There are two main types: generation and
mutation.4,5 The terms intelligent and nonintelligent are also used. A generation-
based fuzzer generates semi-valid sessions. (Semi-invalid has the same meaning in
this context.) A mutation fuzzer takes a known good network session, file, and so
forth, and mutates the sample (before replay) to create many semi-valid sessions.
Mutation fuzzers are typically generic fuzzers or general purpose fuzzers, while
generation fuzzers tend to be specific to a particular protocol, application, or file
format. Understanding the nature of the attack heuristics, how this mutation or
generation is accomplished, is important.
1 Firewalls and the like.
2 Microsoft’s big security push: http://msdn.microsoft.com/msdnmag/issues/05/11/SDL/default.aspx.
3 In this case, tricking a user into browsing to a malicious website or opening a malicious file is the
social engineering.
4 A third method would be some type of genetic/evolving technology. One such algorithm is described
in Chapter 7.
5 Oehlert, P. “Violating Assumptions with Fuzzing,” IEEE Security & Privacy, (March/April 2005):
58–62.
137
6760 Book.indb 137 12/22/17 10:50 AM
138 Building and Classifying Fuzzers
5.1.1 paradigm Split: random or Deterministic Fuzzing
It is appropriate at this point to discuss in some detail the ways these two different
types of fuzzers work and the advantages and disadvantages of both. For example,
mutations of a base file are often random—a mutation fuzzer. Simple mutation fuzz-
ers don’t understand anything about the underlying format, including where to add
anomalies, any checksums, and so on. Conversely, a description of a file format in
Peach would replace variables (defined later) with fuzz strings taken from a file. Each
element would be fuzzed one at a time—a generation/intelligent fuzzer. Of course,
it takes much more work for the tester to describe all these variables to the tool.
Thus, randomness (dumbness) can be an interesting issue for fuzz testing. Some
believe that delivering random data to the lowest layers of each stage of a protocol
or API is the goal. Others prefer protocol construction and predefined test cases to
replace each variable (intelligent fuzzing). Charlie Miller gave a talk at DEF CON
2007 describing his experiences with the two approaches. He measured intelligent
fuzzing to be around 50% more effective but requiring 10 to 100 times the effort
by the tester.6
Having made some superficial statements about these two types of fuzzing
(detailed in a later case study), the randomness being discussed sometimes varies.
We’re not usually talking about a tool that sends completely random data. Such a
tool is unlikely to find anything interesting in a modern application (the protocol
will be perceived as too wrong and be quickly rejected). Although even this approach
sometimes works, as Barton Miller (no relation to Charlie Miller) found7 when
testing command line applications. Such a simple test could be conducted via the
following Unix command string:
while [1]; do cat /dev/urandom | netcat –vv IPaddr port; done
In this context, by random we might mean: Random paths through application
code are executed due to the delivery of semi-stochastic recombinations of the given
protocol, with some of the protocol fields modified in a semi-stochastic way. For
example, this might be done by making small random changes to a network session
to be replayed or by recombining parts of valid files. Another example is a network
fuzzer that plays the last protocol elements first and the first last; a semi-random
fuzzer that shuffles command strings (defined later) and variables.
Again, the two different schools of thought differ significantly in how test cases
are constructed and delivered. Consider a simple FTP server. One fuzzer might
randomly pick valid or invalid commands, randomly add them to a test case, and
choose random data for the arguments of those commands. Another fuzzer might
have precooked invalid data in a library, along with a series of command sequences.
Invalid data is supplied with each command in some repeatable manner. Defining
a protocol and fuzz spots could be accomplished with a tool like Peach or Sulley.
Both approaches are valid ways to test. The goal of this chapter is to help one create
6 See Section 5.1.5, Intelligence Versus Dumb Fuzzers.
7 Barton P. Miller, Lars Fredriksen, Bryan So, “An Empirical Study of the Reliability of Unix Utili-
ties,” Communications of the ACM, 33(12)(December 1990):32–44.
6760 Book.indb 138 12/22/17 10:50 AM
5.1 Fuzzing Methods 139
both types of fuzzers and understand the features of both, as well as when it is most
appropriate to use one or the other (or both).
There are pros and cons to each approach. The randomized approach may hit
dark corners that the deterministic approach might miss because it is impossible to
think of every scenario or combination of commands and data. Plus, it is trivial to
set up and execute. The field of fuzzing was based on this premise, and that’s why
traditional testing was insufficient. Particularly in the case of flipping bits in a file,
this is an easy way to fuzz that is surprisingly effective. However, regression testing
can be difficult for random fuzzers, if not properly seeded. Note that to fully fuzz
a variety of fields could theoretically take forever. For example, imagine that one
of the fields is an integer. In this case to try every value, one would have to supply
2^32 different values. In practice, file-parsing routines have become complicated
enough (complication tends to equal a higher rate of errors) that randomly flipping
bytes throughout the file has been effective. Particularly effective have been bound-
ary values like 0x0000000 and 0xffffffff, which will be discussed later.
The intelligent approach can be tuned to achieve more reliable code coverage.
Application robustness is of the utmost importance, as is security. In general, the
more deterministic fuzzing approach will perform better.8
A hybrid of these approaches could be created. Many mature fuzzers do include
elements of both. For example, instead of completely random data, fuzzing heuristics
would likely be applied to generate data that has been known to cause problems
in the past. Long strings, format strings, directory traversal strings (../../../../), for
example, are all robustness test heuristics and will be covered more fully later in
this chapter.
Some tools attempt to incorporate both approaches. For example, consider
Surku.9 Surku is basically a mutation-based fuzzer. Surku aims to analyze the input
data structure while generating test cases. For example, one mutator in Surku looks
for delimiters often used in human-readable formats to define boundaries of dif-
ferent blocks that are then repeated, moved, or deleted depending on the mutation
applied. The biggest difference between Surku and a generation-based fuzzer is that
Surku can still only add anomalies to the input given. So, if a particular feature is
not exercised by those inputs, Surku will never test that code. On the other hand, a
generation-based fuzzer, like Sulley,10 can completely understand the protocol and
not just the parts for which the packet capture happened to consist. Then again,
Sulley with a poorly written protocol description will not perform very intelligently.
So, in using Surku, the weak link could be the packet capture. But with Sulley, the
weak link could be the fuzzer programmer. Either way, experience on the part of
the people involved helps.
5.1.2 Source of Fuzz Data
There are four main ways this semi-valid data can be created: test cases, cyclic,
random, or library.
8 Again, see Section 5.1.5.
9 Surku is available at www.github.com/attekett/surku.
10 Sulley is available at www.fuzzing.org. https://github.com/OpenRCE/sulley.
6760 Book.indb 139 12/22/17 10:50 AM
140 Building and Classifying Fuzzers
• Test cases refers to a fuzzer that has X number of tests against a particular
standard. This type of fuzzer will only run against the protocol it was created
to fuzz. This type of fuzzer will send the same tests each time it is run and