Prediction Window 
Fig. 4: Time windows in XBrainM.
window, its length is set as three days. That is to say, we
perform the prediction every ﬁve minutes to predict DCNUs
in the future three days.
Sampling interval and observation window. As shown in
Fig. 4, in XBrainM, the sampling interval for runtime events is
1 minute, e.g., we save the CE counts of nodes in every minute.
To generate more effective spatio-temporal features, we use
multi-resolution observation windows to aggregate both the
short-term and long-term error statistics from historical data.
For example, in time t, we compute the sum and increment of
CE counts in the recent 3 hours and 1 week, which are then
used to predict DCNUs occur in the three days after t.
Feature engineering. Static node conﬁgurations and tempo-
ral error statistics are common features used in prior studies
[2], [29]. Besides them, inspired by the observation that faulty
rows and columns are predictive indicators of DCNUs (see
Section III), we further explore micro-level spatial patterns
of DRAM errors and then leverage multiple windows to
aggregate the spatial statistics. More details will be discussed
in Section V.
to adjust
Model training and tuning. Software upgrades and work-
load variance may lead to data changes in the cloud. We
retrain the model
the environment change every
3 months, as the data in a 3-month period is relatively
stable for such a large-scale cloud. Moreover, when plenty
of new nodes are deployed online, or prediction results are far
from expected, we also train a new model on demand. We
employ Bayesian optimization [36] to optimize the model,
seeking the best hyperparameters. Considering the model’s
real effects cannot be evaluated by traditional metrics like F1-
score, we propose the new metric NURR to quantify the node
unavailability reduction (detailed in Section VI). We always
use the hyperparameter combination achieving the best NURR.
Deployment in the cloud. The online deployment is divided
into 2 stages: (i) Grayscale validation: in this stage, we do not
take any mitigation actions but only validate the performance.
(ii) Online validation: after 3 months of running, we know
the online performance of the model. If its performance is
acceptable, it can be used to trigger DCNU mitigating actions
(e.g., live migration) in the production environment.
E. Training and Prediction Overhead
XBrainM uses a few dedicated servers to train the prediction
models and perform online predictions:
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:28:25 UTC from IEEE Xplore.  Restrictions apply. 
279
• Training ML models: 1 VM with 104 cores and 300 GB
memory. Training with the 3-month data as input takes
around 4 hours.
• Online prediction: 30 VMs, each with 4 cores and 8 GB
memory. One-round feature extraction and prediction for
over half a million nodes can be ﬁnished in 1 minute.
F. Mitigating DRAM-Caused Node Unavailability
When the rules or the learning model predict a node to
be unavailable, we then perform live migration to migrate all
VMs on the node to a healthy or vacant node. Generally, live
migration can move VMs seamlessly without disrupting the
running services, but it sometimes fail or cannot be applied
due to limitations such as network boundaries. Cold (non-live)
migration is suitable for more general cases, which isolates
the VMs by shutting down or disconnecting from the network
and then performs the migration. During cold migration, the
original node is unavailable for a while.
After all VMs are successfully migrated,
the predicted
faulty node is then marked as ofﬂine, and stress testing is
performed to reproduce DRAM errors. The testing tool scans
through memory addresses with the popular algorithms used
in memory testing, such as modiﬁed algorithmic test sequence
(MATS) and march C- [33]. If the tested nodes go unavailable
and DRAM faults are conﬁrmed, we send a repair ticket to
platform engineers to repair or replace problematic DIMMs.
V. FEATURE ENGINEERING
Usually, predicting DRAM error/failure is quite challenging
because the features extracted directly from the raw logs are
either few or ineffective. For example, studies [2], [16] only
use 3 and 20 features for prediction, which is not sufﬁcient
to achieve desirable performance in our evaluation. Richer
features are included in our system. We employ novel feature
engineering methods to extract static, spatial, and temporal
features from the large-volume raw data.
A. Static Features
Static features represent ﬁxed characteristics of a physical
node or DRAM components, e.g., CPU generation and DRAM
capacity. We perform one-hot encoding to transform these
static, categorical conﬁgurations into binary vectors. We use
total 11 static features and some typical features are:
• CPU Generation: different generations may use different
ECC algorithms.
• DIMM Vendor: different manufacturing process may in-
volve different reliability characteristics.
• DIMM Capacity: relevant to DRAM density.
• Server Model: though it is not directly relevant to DIMM
quality, difference of hardware conﬁguration and fa-
vorable workloads may trigger different fault patterns
relevant to DCNU.
• Cluster: nodes in the same cluster are likely to run similar
workloads and exhibit similar fault patterns.
• BIOS/OS Version: relevant to the behaviors of the system.
Single Soft Error
Faulty Row
Corrupt Row
=1
>1
>1
>1
Singe Hard Error 
Faulty Column Corrupt(cid:172)Column
Fig. 5: Spatial error patterns.
B. Spatial Features
As introduced in Section II-D, a DRAM bank is structured
as a two-dimensional cell array indexed by rows and columns.
Considering the spatial error distributions (e.g., faulty rows/-
columns) on the DRAM bank are relevant to DCNUs, we
design a series of spatial features.
Bank-level features. Firstly, to infer the fault range and
severity on a DRAM bank, we count the number of rows
and columns that experienced CEs, then compute the sum,
average, max of CEs and cells experienced CEs on them.
For example, the feature “ce count col max” represents the
maximum number of CEs on rows.
Then we design several error patterns to capture the micro-
level CE spatial distributions. Fig. 5 illustrates the spatial
distributions of these patterns, which are deﬁned as follows:
• Single soft error. A single error occurs in a new cell that
is not in the faulty row/column.
• Single hard error. A cell not in a faulty row/column has
experienced at least two errors.
• Faulty row/column, as deﬁned in Section II-A.
• Corrupt row/column. A faulty row/column is observed,
and at least 1 hard error is observed in the row/column.
The single soft/hard error is used to distinguish the transient
soft fault from the permanent hard fault on a single cell. In
addition, we use a corrupt row/column to check whether hard
errors are observed on a faulty row/column, which typically
indicates the faulty row/column is repeated accessed and likely
to trigger more CEs.
DIMM/Node-level spatial
features. We then aggregate
those bank-level features to DIMM and node level by com-
puting the max, sum, average, and standard deviation of each
features from all banks that experienced CEs. For example,
“single hard error b sum” means the total number of the
single hard error pattern among all banks of a node.
As introduced in Section III, fault rows/columns are pre-
dictive indicators of DCNUs. With rich features relevant to
faulty rows/columns, the ML model is able to learn the subtle
correlation between fault rows/columns and DCNUs.
C. Temporal Features
Beside the spatial features, we also use relevant events from
kernel logs, including CMCI Storm Detected (more than 16
CEs in second), MCE Killing (processes get killed due to
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:28:25 UTC from IEEE Xplore.  Restrictions apply. 
280
hardware memory corruption), CE Suppressed Notify (CEs
are suppressed by a special suppressing mechanism), etc. We
aggregate the historical occurrence of spatial feature as well
as the kernel events to correlate them with DCNUs.
Since we observe that some DCNUs are more relevant to
temporal error statistics, we select 3 hours, 1days, 1 week,
4 weeks, and the entire lifetime of a node as the different
time window to aggregate the statistics of all features within
the windows. For each time window, we calculate 7 types of
popular statistics from the raw data with 1-minute granularity.
Let t be a timestamp, x be an event, X be a sequence of
minute-level event counts in the time window (t − w, t], μ be
the mean of X, σ be the standard deviation of X. The statistics
at t are computed as follows:
2 ) − Sum(x, t − w
• Sum(x, t, w) and Std(x, t, w) refer to the sum and stan-
dard deviation of event counts of x in the time window.
• Dif f means the difference of total errors in the ﬁrst
and second half of the time window: Dif f (x, t, w) =
Sum(x, t, w
• Delta is the difference of total errors in recent 2 time
windows: Delta(x, t, w) =Sum (x, t, w) − Sum(x, t −
w, w)
• Kurtosis is a statistical measure of the tail of the proba-
bility distribution. We use the standard measure of a dis-
tribution’s kurtosis, a scaled version of the fourth moment
of the distribution: Kurtosis(x, t, w) = E
2 , w
2 )
X−μ
σ )4
• Skew differentiates extreme values in one versus the
other tail. Let ˜μ be the median of X: Skew(x, t, w) =
3(μ−˜μ)
(cid:3)
(cid:2)
(
σ
• Entropy measures the average level of “uncertainty”
inherent in the possible outcomes of a random variable.
Assume that there are k different values observed in
X, and let pi denote the probability of i-th value,
the frequency of i-th
which equals to the ratio of
value to the number of sampling periods. We have
Entropy(x, t, w) = −(cid:4)k
i=1 pi log(pi)
Our feature engineering can produce more than 10,000
features. We then adapt recursive feature elimination [19] to
further improve the feature set: (i) Train a XGBoost model;
(ii) Eliminate the top-5% least important features ranked by
XGBoost; (iii) Repeat step i˜ii until the prediction performance
cannot be improved in recent three iterations. After that, about
2,500 features are reserved. They achieve the best prediction
performance with limited computational cost (see Section
IV-E), thus are used for regular training and prediction.
VI. MODEL EVALUATION
Correctly evaluating the model’s effects on node unavail-
ability is vital before deploying a model online. XBrainM in-
troduces sequential node-level evaluation strategies and a new
metric NURR to estimate the node unavailability reduction
and tune model hyperparameters with NURR.
A. Sequential Node-level Evaluation
To learn an accurate model, we need to split the error
logs to simulate the real cases as much as possible. In our
experiments, we keep the log in sequential order and split the
data into 3-month pieces, using the earlier 3-month data as
training data and the subsequent 3-month data for testing.
Since that prediction evaluation window is 3 days which
is much longer than the prediction interval 5 minutes, once a
DCNU belongs the 3-day prediction window of any prediction
interval, the DCNU impact can be mitigated. Therefore, we
aggregate the predictions to node level and evaluate the pre-
diction performance only according to whether the DCNU is
in the prediction window or not. In other words, a correct pre-
diction is counted if DCNU occurs in the prediction window
of any prediction which predicts the node to be unavailable.
On the other hand, multiple incorrect predictions on a normal
node are counted as a single false prediction.
B. Node Unavailability Reduction Rate (NURR)
Traditional evaluation metrics for binary classiﬁcation in-
clude recall, precision, F1-score, etc. Let TP, FP, and FN
denote the total number of true positives, false positives, and
false negatives, respectively. Precision, TP/(TP+FP) is deﬁned
as the ratio of true positives to total alerts. A low precision
value indicates that many useless migrations would be taken.
Recall, TP/(TP+FN), is deﬁned as the ratio of true positives
to total DCNUs observed. A low recall value indicates we
fail to identify a large portion of DCNUs to happen in the
future. In many cases, a high precision value is likely to be
associated with a low recall value and vice versa. F1-score is
the harmonic average of precision and recall to penalize a bias
to any of the two metrics.
These metrics are useful for evaluating the prediction accu-
racy. However, they cannot measure the model’s real effects
on node unavailability. In this subsection, we propose a new
metric, Node Unavailability Reduction Rate (NURR). NURR
is directly relevant
to node unavailability. We use NURR
instead of F1-score to tune the model hyperparameters to reach
lower unavailability in our cloud.
Let tu denote the average duration of node unavailability
that fails to be predicted. Without a prediction model, the total
node unavailable time is:
T = tu(T P + F N )
(1)
In XBrainM, the direct node unavailability is greatly re-
duced by early VM migration. As described in Section IV-F,
VMs on most nodes are moved by live migration transparently.
A small fraction of nodes experience cold migration, during
which process the node is marked as unavailable for a while.
Let y denote the percentage of cold migration, tm denote the
average node unavailable time during cold migration. Consid-
ering the impact of DCNU prediction and VM migration, there
would be three DCNU cases as illustrated in Fig. 6:
• Case : Correct prediction (TP) and cold migration is
performed. The total unavailable time under this case is
ytm · T P .
• Case : Prediction missed (FN). The average duration of
this case is tu, and the total unavailable time is tu · F N.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:28:25 UTC from IEEE Xplore.  Restrictions apply. 
281
Positive
Unavailable 
Nodes
Negative
Normal 
Nodes
True 
Positives 
False 
Negatives 
False 
Positives 
Live
Migration
 y 
Cold 
Migration
1
2
Node(cid:172) 
Unavailability
Live
Migration
3
True  
Negatives
 y 
Cold 
Migration
Fig. 6: DCNU cases with a prediction model.
• Case : False DCNU prediction (FP) and cold migration
is performed. As described before, the nodes will be
considered as unavailable for a while if the cold migration
is performed. The total unavailable time is ytm · F P.
Therefore, considering the prediction and VM migration
impact, the new total unavailable time is:
TABLE II: Experimental datasets, where t0 is the timestamp
that Dataset1 starts with, and m is the time unit in month.
Name
Dataset1
Dataset2
Dataset3
Training Period
[t0, t0 + 3m)
[t0 + 3m, t0 + 6m)
[t0 + 6m, t0 + 9m)
Test Period
[t0 + 3m, t0 + 6m)
[t0 + 6m, t0 + 9m)
[t0 + 9m, t0 + 12m)
data, each sample with 2500+ features. We divide the dataset
evenly into 4 parts, each with 3 consecutive months of the
data. Then we reorganize the 4 parts to 3 datasets, each with 3
months of the training data and the testing data in the following
3 months. Table II shows the evaluation period of 3 datasets.
Evaluation metrics. We use the traditional metrics recall,
precision, and F1-score to evaluate the accuracy improvement
of our approach over existing solutions. Furthermore, we use
NURR to evaluate the predictors’ effects on service availabil-
ity, which is beyond the capability of traditional metrics.
B. Comparison with Existing Methods
T (cid:2)
= ytm · T P + tu · F N + ytm · F P
The node unavailability reduction rate is calculated as:
N U RR =
T − T (cid:2)
T
tu(T P + F N ) − (ytm · T P + tu · F N + ytm · F P)
=
=
=
=
tu(T P + F N )
tuT P − ytm · T P − ytm · F P
tu(T P + F N )
T P
− y
T P + F P
T P + F N
tm
tu
tm
tu
T P + F N
T P
T P + F N
= recall − y
T P
T P
T P +F N
− y
precision = (1 − y tm
recall
T P +F P
tu
tm
tu
Since y tm
precision ) · recall (8)