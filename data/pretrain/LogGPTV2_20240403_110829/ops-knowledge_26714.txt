User: I have a python data analysis script that runs for many hours, and while it was running on my desktop, with fans blazing I realized I could just run it on a hosting account remotely in bkgnd and let it rip. 
But I'm wondering - is this generally frowned upon by hosting providers?  Are they assuming that all my CPU/memory usage is bursty-usage from my Apache2 instance and a flat-out process running for 12hrs will get killed by their sysop?
Or do they assume I'm paying for usage, so knock yourself out?   My script and its data is self-contained and is using no network or database resources.
Any experience with that?
Assistant: I tweeted to my hosting corp and they said my long-running python data analysis script is probably okay as long as it doesn't over-use resources. 
I let it rip - just a single process churning away, generating a sub-megabyte data output file, but alas, they killed the process for me during the night, with a note that the CPU usage was too much. 
Just an FYI in case you have a bit of 'big data' analysis to do. I suppose I could chop it up and run it sporadically, but that would just be hiding the CPU usage. So I'll find an old machine to churn, albeit much more slowly, for me. :/  
I suppose this is a task better suited to a dedicated hosting enviro? Big data apps not suited to low-cost shared/virtual hosting services?