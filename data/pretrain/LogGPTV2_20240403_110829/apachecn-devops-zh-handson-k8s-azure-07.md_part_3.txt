30 httpGet:
31 路径:/index.html
32 端口:80
33 个初始延迟秒:3
34 周期秒:3
35 卷安装:
36 -名称:html
37 mount path:/usr/share/engine/html
38 -名称:索引
39 mountPath: /tmp/index1.html
40 子路径:index1.html
41 -姓名:健康
42 mount path:/tmp/health . html
43 子路径:healthy.html
44 命令:["/bin/sh "，"-c"]
45 个参数:[" CP/tmp/index 1 . html/usr/share/nginx/html/index . html；CP/tmp/health . html/usr/share/nginx/html/health . html；nginx 睡眠 inf"]
46 卷:
47 -名称:索引
48 configMap:
49 名称:服务器 1
50 名:健康
51 configMap:
52 名:健康
53 -名称:html
54 emptyDir: {}
在此部署中有几件事需要强调:
*   **第 23-28 行**:这是活跃度探头。活跃度探测器指向健康页面。请记住，如果运行状况页面失败，容器将重新启动。
*   **第 29-32 行**:这是准备状态探测器。在我们的例子中，就绪探测器指向索引页。如果此页面失败，pod 将暂时不发送任何流量，但将保持运行。
*   **第 44-45 行**:这两行包含了容器启动时执行的几个命令。这不是简单地运行 nginx 服务器，而是将索引和就绪文件复制到正确的位置，然后启动 nginx，然后使用**睡眠**命令(因此容器保持运行)。
您可以使用以下命令创建此部署。也可以为**服务器 2** 部署第二个版本，类似于**服务器 1** :
kubi KL create-f webeploy1 . YAML
kubi KL create-f webeploy 2 . YAML
最后，您还可以创建一个服务( **webservice.yaml** )将流量路由到两个部署:
1 堆叠版本:v1
2 种:服务
3 元数据:
4 名称:网络
5 规格:
6 选择器:
7 应用:网络服务器
8 个端口:
9 -协议:TCP
10 端口:80
11 目标端口：80
12 类型:负载平衡器
您可以使用以下内容创建该服务:
忽必烈 create -f webservice.yaml
现在您已经启动并运行了应用。在下一节中，您将介绍一些验证活跃度和就绪性探测器行为的失败。
### 试验活性和准备状态探测器
在前一节中，我们解释了活动和就绪探测器的功能，并创建了一个示例应用。在本节中，您将介绍该应用中的错误，并验证活跃度和就绪性探测器的行为。您将看到就绪探测器的故障将如何导致 Pod 保持运行但不再接受流量。之后，您将看到活性探测器的故障将如何导致 pod 重新启动。
让我们从准备就绪探测器失败开始。
### 就绪探测器失败会导致流量暂时停止
现在，您已经启动并运行了一个简单的应用，您可以尝试活跃度和就绪性探测器的行为。首先，让我们使用浏览器将服务的外部 IP 连接到我们的 web 服务器:
忽必烈得到服务
如果你在浏览器中点击外部 IP，你应该会看到一行字，上面写着**服务器 1** 或者**服务器 2** :
![Browsing to the external IP in the browser shows the application returning traffic from server 1 ](img/B17338_07_17.jpg)
图 7.17:我们的应用正在从服务器 1 返回流量
在接下来的测试中，您将使用本章代码示例中提供的名为 **testWeb.sh** 的小脚本连接到您的网页 50 次，因此您可以监控服务器 1 和 2 之间的良好结果分布。您首先需要使该脚本可执行，然后可以在部署完全正常的情况下运行该脚本:
chmod +x testWeb.sh
。/testWeb.sh
在正常运行期间，我们可以看到服务器 1 和服务器 2 的命中率几乎相同，服务器 1 的 **24** 命中率和服务器 2 的 **26** 命中率相同:
![Output displaying a healthy application with its traffic load-balanced between server 1 and server 2](img/B17338_07_18.jpg)
图 7.18:当应用运行正常时，服务器 1 和服务器 2 之间的流量是负载平衡的
现在让我们继续前进，让服务器 1 中的就绪探测失败。为此，您将使用 **kubectl exec** 命令将索引文件移动到不同的位置:
kubectl get pods #注意服务器 1 的 pods 名称
kubectl exec -- \
mv/usr/share/nginx/html/index . html \
/usr/share/nginx/html/index 1 . html
执行此操作后，我们可以使用以下命令查看 pod 状态的变化:
忽必烈得到 pods -w
您应该会看到服务器 1 pod 的就绪状态变为 **0/1** ，如图*图 7.19* :
![First, a command is executed to stop directing traffic to server 1\. Then, using kubectl get pods -w, the ready attribute of the server 1 pod changes from 1/1 to 0/1](img/B17338_07_19.jpg)
图 7.19:失败的就绪探测器导致服务器 1 没有任何就绪容器
这应该不会将更多流量导向服务器 1 pod。让我们验证一下:
。/testWeb.sh
流量应该重定向到服务器 2:
![Output displaying all traffic is directed to server 2](img/B17338_07_20.jpg)
图 7.20:所有流量现在都由服务器 2 提供服务
现在，您可以通过将文件移回其正确位置来恢复服务器 1 的状态:
kubectl exec -- mv \
/usr/share/nginx/html/index 1 . html \
/usr/share/nginx/html/index . html
这将使 Pod 返回到**就绪**状态，并再次平均分配流量:
。/testWeb.sh
这将显示类似于图 7.21 的输出:
![After restoring the readiness probe, traffic is load-balanced again](img/B17338_07_21.jpg)
图 7.21:恢复就绪探测会使流量再次达到负载平衡
一个失败的准备状态探测器将导致 Kubernetes 不再向失败的 Pod 发送流量。通过导致示例应用中的就绪性探测失败，您已经验证了这一点。在下一节中，您将探讨失败的 liveness 探测的影响。
### 一个失败的活性探测器重新启动 Pod 
您也可以使用活性探针重复前面的过程。当活性探测器失败时，Kubernetes 预计将重启该 Pod 。让我们通过删除健康文件来尝试一下:
kubectl exec -- \
RM/usr/share/nginx/html/health . html
让我们看看这对 Pod 有什么影响:
忽必烈得到 pods -w
您应该会看到 pod 在几秒钟内重新启动:
![Output displaying a failing liveness probe that causes the pod to restart](img/B17338_07_22.jpg)
图 7.22:一个失败的活性探测器将导致 Pod 重新启动
在*图 7.22* 中可以看到，Pod 成功重启，影响有限。您可以通过运行**描述**命令来检查 Pod 中发生了什么:
下的立方描述
前面的命令会给你一个类似于*图 7.23* 的输出:
![More details on the pod showing the failing liveness probe caused the pod to be restarted](img/B17338_07_23.jpg)
图 7.23:显示活性探测器如何失败的详细信息
在**描述**命令中，可以清楚地看到 Pod 没有通过活性探测器。三次失败后，容器被终止并重新启动。
实验以活性和准备状态探测结束。请记住，这两者对您的应用都很有用:就绪探测器可以用来暂时停止到您的 pod 的流量，因此它必须处理更少的负载。如果 Pod 出现实际故障，活性探测器将用于重启 Pod 。
我们还要确保清理您刚刚创建的部署:
kubectl 删除部署服务器 1 服务器 2
kubectl delete service web
活跃度和就绪性探测对于确保集群中只有健康的 Pod 才能接收流量非常有用。在下一节中，您将探索 Kubernetes 报告的不同指标，您可以使用这些指标来验证应用的状态。
## Kubernetes 报告的指标
Kubernetes 报告多个指标。在本节中，您将首先使用一些 **kubectl** 命令来获取这些指标。之后，您将查看 Azure Monitor 中的容器，了解 Azure 如何帮助进行容器监控。
### 节点状态和消耗
Kubernetes 中的节点是运行应用的服务器。Kubernetes 会将 pods 调度到集群中的不同节点。您需要监控节点的状态，以确保节点本身是健康的，并且节点有足够的资源来运行新的应用。
运行以下命令获取有关群集上节点的信息:
kubectl 获取节点
前面的命令列出了他们的姓名、状态和年龄:
![Running the kubectl get nodes command to get information about the nodes on the cluster](img/B17338_07_24.jpg)
图 7.24:该集群中有两个节点
通过 **-o 宽**选项可以获得更多信息:
kubectl get -o 宽节点
输出列出底层 **OS-IMAGE** 、 **INTERNAL-IP** 等有用信息，可在*图 7.25* 查看:
![Adding the -o wide option to the command to display more details about the nodes](img/B17338_07_25.jpg)
图 7.25:使用-o 宽度增加了关于节点的更多细节
您可以使用以下命令找出消耗资源最多的节点:
kubectl top nodes