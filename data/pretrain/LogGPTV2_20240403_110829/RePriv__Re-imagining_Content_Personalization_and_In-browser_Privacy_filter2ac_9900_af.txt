browsing mode” [1]. Although the precise meaning of this
term varies between browsers, the common idea behind this
feature is to prevent web sites from reading persistent data
such as cookies for a particular session. There have been
a number of other browser add-ons and modiﬁcations that
attempt to anonymize the user on the web; an incomplete
list includes TrackMeNot [15], Torbutton, SafeCache [30],
SafeHistory [30], and IE8’s InPrivate browsing. While it is
clear that a truly anonymous browsing mode would force
content providers to use REPRIV, no such mode has been
successfully implemented [1], and it is not clear that do-
ing so is technically feasible [7]. However, we assert that
REPRIV does in fact facilitate end-user privacy on the web,
by creating incentives for content providers to use privacy-
sensitive personalization techniques, rather than relying on
the invasive collection mechanisms currently available. In
this respect, REPRIV is complementary to private browsing
modes; it provides a mechanism for allowing personalized
content without the need for the tracking mechanisms currently
used by content providers, which are not compatible with
anonymous browsing.
Proﬁle Management: The behavior proﬁles generated by
REPRIV are currently maintained entirely within the browser,
and are distinct on a per-user, per-browser basis. However,
there is no reason to preclude additional proﬁle management
schemes in REPRIV. One possibility is to maintain the primary
copy on a cloud server, encrypted using a symmetric key.
Because the cloud does not need direct access to proﬁle data,
key distribution for this scheme is straightforward: the user
manually loads the symmetric key into each browser that
updates or consumes the proﬁle; this is realistic assuming the
user is physically present at each browser that accesses the
proﬁle. Updates to the personal proﬁle are performed locally
at each browser instance, and synced with the cloud server
periodically. The major upshot of this scheme is that
the
behavior proﬁle is no longer constrained on a per-browser
basis, as the user can transfer the same proﬁle between
multiple instances using the cloud host.
Attacks Against REPRIV: There are a few ways that an ad-
versary can compromise the principles embodied in REPRIV.
First, because REPRIV does not prevent remote parties from
tracking the user, one could imagine an adversary collecting
additional data through unwanted tracking. This class of attack
is best addressed by using REPRIV in tandem with the private
browsing modes supported by most commercial browsers [1],
or various browser extensions [15]. We consider this to be an
orthogonal issue to REPRIV; as better private browsing modes
are developed, they can be “dropped in” for use with REPRIV.
Similarly, a group of colluding sites can share the infor-
mation provided to them through REPRIV, in order to learn
more about a particular user than explicitly consented to at
each individual site. We note that this attack can also be
addressed by private browsing modes. In order for a set of
colluding adversaries to perpetrate this attack, they must be
able to identify the user across visits to each distinct site; if
the attackers cannot link the data provided to each site to a
single user, then the chances of it affecting the user are quite
small. Private browsing modes can prevent this by thwarting
the ability of third parties to track the user.
However, not all of the pitfalls can be resolved through
private browsing modes. Another possible attack is a type of
denial of service in which an attacker inundates a user with
meaningless REPRIV prompts in an attempt to berate him into
giving the site full permissions to personal data. The best way
to deal with this behavior is to limit the ability of sites to
prompt the user for data – perhaps forcing all prompts to
occur in a single dialog when the site is ﬁrst rendered. At
this point, attempts to trick or coerce the user into agreeing
to permissions against his interest can be dealt with through
interface design [20].
Lastly, one should also consider possibilities of the user
being untruthful about their preferences: when prompted for
her interests, she can simply substitute random or bogus
144
values in place of the interests computed by REPRIV. We
feel that applying remote attestation and software veriﬁcation
techniques to the browser stack will allow us to remediate
some of these attacks. Furthermore, the user has incentive to
provide honest preferences, as they stand to gain more in terms
of the quality of personalized content that is provided.
VIII. RELATED WORK
Privacy and Web Applications: As a reaction to the decrease
in privacy on the web, many have started exploring techniques
that can be applied to restore some degree of privacy while still
allowing for the rich web applications that people have come to
expect. The P3P Project [4], sponsored by the W3 Consortium,
is an attempt to formalize and mechanize the speciﬁcation
and distribution of privacy policies on the web. It does not
have provisions for providing personal information to content
providers, however, making the issue of providing personalized
functionality rather difﬁcuclt. Jakobsson et al. [17] considered
the problem of third-party sites mining users’ navigation
history. They developed a system that allows third parties to
learn aggregate information about users’ navigation histories,
rather than the full listing. All privacy assurances offered by
this system derive from the fact that its mechanism is easily
auditable by end-users, so parties who wish to mine history
data have disincentive to cheat.
Becker and Chen [2] found that it is possible to deduce
speciﬁc personal characteristics given only a list of their
friends on a social network. Worse yet, they found that it is
very difﬁcult to defend against this type of inference, assuming
an attacker has access to the user’s entire social graph: on
average, they found that users would have to remove hundreds
of friends from their connections in order to ensure the privacy
of their own characteristics.
Narayanan and Shmatikov [27] studied the privacy im-
plications of social network participation. Their observation
is that the operators of online social networking sites now
share user data with third parties, but only scrub personally-
identifying information in an ad-hoc fashion. They developed
a re-identiﬁcation algorithm that relates users’ privacy in a
social network to node anonymity in the social network graph,
and attempts to identify particular users from scrubbed social
network data. They found that if a user subscribed to both
Twitter and Flickr, then the algorithm can correctly identify
them with 88% accuracy.
McSherry and Mironov [24] attempted to restore a certain
degree of privacy to collaborative recommendation algorithms,
such as those used by Netﬂix and amazon.com. Citing the
work of Narayanan and Shmatikov [26] in de-anonymizing
users who take part in such systems, they worked in the
framework of differential privacy [6] to build a an algorithm
that preserves the privacy of each individual rating entered by
a participating user. The performance is comparable to that of
the original Netﬂix recommendation algorithm.
Privacy in Advertising: One problem that has received much
recent attention is that of delivering targeted advertisements
to web users without violating their privacy. Freudiger et
al. [10] observe that the prevalent mechanism for targeting
advertisements to individual users is the third-party cookie.
They propose a browser extension that allows users to directly
manage third-party cookies in order to decide the degree to
which advertisers are able to track them. However, unlike
with REPRIV, this solution does not give users arbitrary, ﬁne-
grained control over the type of information that is given
to third-parties. Furthermore, advertisers have no incentive to
obey the privacy safeguards instantiated by this mechanism.
In a slightly different vein, several recent systems [14, 19,
34] attempt to remedy the problem by storing the necessary
sensitive personal data on the client, along with all possible
ads in the network. When an ad is displayed, it is matched to
personal information locally, thus sidestepping the need to leak
to the ad network. Accounting and click-fraud prevention are
addressed using either additional semi-trusted parties, or ho-
momorphic encryption. The primary difference between these
systems and REPRIV is generality: REPRIV asks the user to
provide content providers (in this case, an advertising network)
with small amounts of selected personal data in return for full
application generality, whereas these tools effectively hide all
personal data needed to drive the single application of targeted
advertising.
Managing Private Browser State: A number of researchers
have studied ways to identify users and preferences from
browser interactions. Wondracek et al. [36] found that a
subtlety in the W3C speciﬁcation that allows browser history
to be inferred can be leveraged to de-anonymize users of
popular social networking sites. Jackson et al. [16] attributed
the problem of history snifﬁng to the fact that browsers do not
extend the same-origin policy to the history state leveraged
in the attack. Recently, Mozilla has taken steps to prevent
history snifﬁng [33], at the cost of breaking certain parts of the
W3C speciﬁcation. In a broader development, Eckersley [7]
introduced a technique dubbed browser ﬁngerprinting, wherein
a large number of publicly-visible browser attributes are com-
bined to produce an identifying string shared by only one in
about 286,777 browsers.
Several researchers have approached the technical problem
of maintaining user anonymity while browsing. Howe and
Nissenbaum [15] created TrackMeNot, a Firefox extension that
attempts to anonymize search behavior by periodically submit-
ting random search queries to major search engines. McKin-
ley [23] examined the privacy modes of popular browsers, as
well as their ability to clear private state when directed by
the user. She found that while some browsers do in fact clear
private state when instructed, none of the browsers’ privacy
modes performs as advertised; each browser left some form
of persistent state that could be later retrieved by web pages
in different browsing sessions.
Web Personalization and Mining: The basis on which
personalization is performed varies from application to ap-
plication. Pierrakos et al. [29] surveyed the topic of mining
users’ behavior on a set of web services to infer information
145
that will aid personalization. They found that almost all web
personalization efforts fall into one of four broad categories:
memorizing information for later replay, guiding the user
towards likely relevant information, customizing content to
match users’ interests, and supporting users’ efforts to com-
plete tasks. REPRIV is designed primarily to support
the
implementation of the second and third points, but it can be
used to support aspects of all types of personalization.
There are several browser add-ons (toolbars) that perform
data collection and user behavior mining. Perhaps the most
popular among them is the Alexa Toolbar, which for each
user collects a complete browsing history, search engine query
list, and summary of the advertisements presented to the user.
This information used by Alexa to compute a number of
analytic functions, some of which are returned to toolbar
users as a service. Among the analytics are trafﬁc statistics
(including a comprehensive, internet-wide ranking of popular
sites), related links, audience demographics, and clickstream
statistics. Similarly, Bing [3], Google [12], and Yahoo [38] all
offer toolbars, although they vary in the amount of mining and
automatic personalization that they perform.
IX. CONCLUSIONS
This paper presents REPRIV, an in-browser approach that
aims to perform personalization without sacriﬁcing user pri-
vacy. REPRIV accomplishes this goal by requiring explicit
user consent in any transfer of sensitive user information. We
showed how efﬁcient and effective behavior mining can be
added to a web browser to automatically infer the information
needed to facilitate many personalized web applications, and
evaluated this mechanism on real-world data. We also showed
how, with the help of static software veriﬁcation, third-party
code can be incorporated into the system, and given access
to sensitive user information, without sacriﬁcing control and
user consent. We presented two end-to-end case studies of
useful personalized applications, that showcase the abilities of
REPRIV. Much of the power of REPRIV comes from its focus
on what can be done on the client, be that a desktop browser,
a mobile browser, or the context of an entire mobile operating
system in the case of a user of a tablet device. This paper
shows that in REPRIV, personalized content and privacy can
coexist.
REFERENCES
[1] G. Aggarwal, E. Bursztein, C. Jackson, and D. Boneh. An analysis
of private browsing modes in modern browsers. In Proceedings of the
Usenix Security Symposium, Jul. 2010.
[2] J. Becker and H. Chen. Measuring privacy risk in online social networks.
In Proceedings of the Workshop on Web 2.0 Security and Privacy, May
2009.
[3] The Bing Toolbar. http://www.discoverbing.com/toolbar.
[4] W. Consortium. Platform for Privacy Preferences (P3P) Project. http:
//www.w3.org/P3P.
[5] Spam database lookup. http://www.dnsbl.info.
[6] C. Dwork. Differential privacy: a survey of results. In Proceedings of
the International Conference on Theory and Applications of Models of
Computation, May 2008.
[7] P. Eckersley. How Unique Is Your Web Browser? Technical report,
Electronic Frontier Foundation, Mar. 2009.
[8] The Electronic Freedom Foundation. http://www.eff.org.
[9] M. Fredrikson and B. Livshits. RePriv: Re-imagining in-browser privacy.
Technical Report MSR-TR-2010-116, Microsoft Research, Aug. 2010.
[10] J. Freudiger, N. Vratonjic, and J.-P. Hubaux. Towards Privacy-Friendly
Online Advertising. In Proceedings of the Workshop on Web 2.0 Security
and Privacy, May 2009.
[11] Google AdSense privacy information.
http://www.google.com/
privacy_ads.html#toc-faq.
[12] The Google Toolbar. http://toolbar.google.com.
[13] A. Guha, M. Fredrikson, B. Livshits, and N. Swamy. Veriﬁed security for
browser extensions. In Proceedings of the IEEE Symposium on Security
and Privacy, May 2011.
[14] S. Guha, A. Reznichenko, K. Tang, H. Haddadi, and P. Francis. Serving
Ads from localhost for Performance, Privacy, and Proﬁt. In Proceedings
of Hot Topics in Networking, Nov. 2009.
[15] D. C. Howe and H. Nissenbaum. TrackMeNot: Resisting surveillance
in web search. In I. Kerr, V. Steeves, and C. Lucock, editors, Lessons
from the Identity Trail: Anonymity, Privacy, and Identity in a Networked
Society, chapter 23. 2009.
[16] C. Jackson, A. Bortz, D. Boneh, and J. C. Mitchell. Protecting browser
In Proceedings of the International
state from web privacy attacks.
Conference on World Wide Web, May 2006.
[17] M. Jakobsson, A. Juels, and J. Ratkiewicz. Privacy-Preserving History
Mining for Web Browsers. In Proceedings of the Workshop on Web 2.0
Security and Privacy, May 2010.
[18] W. S. Journal. What they know. http://blogs.wsj.com/wtk/, 2011.
[19] A. Juels. Targeted advertising ... and privacy too. In Proceedings of the
Conference on Topics in Cryptology, Apr. 2001.
[20] P. G. Kelley, L. Cesca, J. Bresee, and L. F. Cranor. Standardizing
In
the International Conference on Human Factors in
privacy notices: An online study of the nutrition label approach.
Proceedings of
Computing Systems, Apr. 2010.
[21] B. Lerner, H. Venter, B. Burg, and W. Schulte. An experimental
extensible, reconﬁgurable platform for HTML-based applications, Oct.
2010.
[22] McAfee Inc. Spyware information. http://www.mcafee.com/us/
security_wordbook/spyware.html.
[23] K. McKinley. Cleaning Up After Cookies Version 1.0. Technical report,
ISEC Partners, Dec. 2010.
[24] F. McSherry and I. Mironov. Differentially private recommender sys-
tems: building privacy into the net. In Proceedings of the International
Conference on Knowledge Discovery and Data Mining, Jun. 2009.
[25] Amazon Mechanical Turk.
https://www.mturk.com/mturk/
welcome.
[26] A. Narayanan and V. Shmatikov. Robust de-anonymization of large
sparse datasets. In Proceedings of the IEEE Symposium on Security and
Privacy, May 2008.
[27] A. Narayanan and V. Shmatikov. De-anonymizing social networks. IEEE
Sympolsium on Security and Privacy, May 2009.
[28] The Open Directory Project. http://dmoz.org.
[29] D. Pierrakos, G. Paliouras, C. Papatheodorou, and C. D. Spyropoulos.
Web usage mining as a tool for personalization: A survey. User Modeling
and User-Adapted Interaction, 13(4), 2003.
[30] Same origin policy: Protecting browser state from web privacy attacks.
http://crypto.stanford.edu/safecache/.
[31] N. Swamy, J. Chen, and R. Chugh. Enforcing stateful authorization and
In In Proceedings of the European
information ﬂow policies in ﬁne.
Symposium on Programming, Mar. 2010.
[32] TargetAPI. http://www.targetapi.com.
[33] The Mozilla Team.
Plugging
the CSS History Leak.
http://blog.mozilla.com/security/2010/03/31/
plugging-the-css-history-leak, 2010.
[34] V. Toubiana, A. Narayanan, D. Boneh, H. Nissenbaum, and S. Barocas.
Adnostic: Privacy preserving targeted advertising. In Proceedings of the
Network and Distributed System Security Symposium, Feb. 2010.
[35] WebMii: A person search engine. http://www.webmii.com.
[36] G. Wondracek, T. Holz, E. Kirda, and C. Kruegel. A practical attack
to de-anonymize social network users. In IEEE Symposium on Security
and Privacy, May 2010.
[37] Yahoo! BOSS API. http://developer.yahoo.com/search/boss/.
[38] The Yahoo Toolbar. http://toolbar.yahoo.com.
146