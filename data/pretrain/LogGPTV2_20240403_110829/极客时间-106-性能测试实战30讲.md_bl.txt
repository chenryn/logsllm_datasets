# 26丨案例：手把手带你理解TPS趋势分析在性能分析中，前端的性能工具，我们只需要关注几条曲线就够了：TPS、响应时间和错误率。这是我经常强调的。但是关注 TPS到底应该关注什么内容，如何判断趋势，判断了趋势之后，又该如何做出调整，调整之后如何定位原因，这才是我们关注TPS 的一系列动作。今天，我们就通过一个实际的案例来解析什么叫 TPS的趋势分析。案例描述这是一个案例，用一个 2C4G 的 Docker容器做服务器。结构简单至极，如下所示：![](Images/69d59bb5678a27ef5e984180912b781c.png)savepage-src="https://static001.geekbang.org/resource/image/17/1d/177cd65abdaaba1e8056e676cdf96b1d.jpg"}当用个人电脑（上图中压力工具 1）测试云端服务器时，达到 200 多TPS。但是当用云端同网段压力机（上图中压力工具 2）测试时，TPS 只有 30多，并且内网压力机资源比本地压力机要高出很多，服务器资源也没有用完。在这样的问题面前，我通常都会有一堆的问题要问。1.  现象是什么？        2.  脚本是什么？        3.  数据是什么？        4.  架构是什么？        5.  用了哪些监控工具？        6.  看了哪些计数器？        在分析之前，这些问题都是需要收集的信息，而实际上在分析的过程中，我们会发现各种数据的缺失，特别是远程分析的时候，对方总是不知道应该给出什么数据。我们针对这个案例实际说明一下。这个案例的现象是 TPS低，资源用不上。下面是一个 RPC脚本的主要代码部分。    public SampleResult runTest(JavaSamplerContext arg0) {        // 定义results为SampleResult类        SampleResult results = new SampleResult();        // 定义url、主机、端口        String url = arg0.getParameter("url");        String host = arg0.getParameter("host");        int port = Integer.parseInt(arg0.getParameter("port"));        results.sampleStart();        try {            message=detaildata_client.detaildata_test(url);// 访问URL并将结果保存在message中            System.out.println(message); //打印message，注意这里            results.setResponseData("返回值："+ message, "utf-8");            results.setDataType(SampleResult.TEXT);            results.setSuccessful(true);        } catch (Throwable e) {            results.setSuccessful(false);            e.printStackTrace();        } finally {            String temp_results=results.getResponseDataAsString();            results.setResponseData("请求值："+arg0.getParameter("url")+"\n"+"返回值:"+temp_results, "utf-8");            results.sampleEnd();        }        return results;JMeter 脚本关键部分：    100    //我们来看这里，ramp_time只有1秒，意味着线程是在1秒内启动的，这种场景基本上都和真实的生产场景不相符。    1    true    300    ...............          ,      utf-8      filename      false      false      true      shareMode.all      false      url    在这个脚本中，逻辑非常简单，一个 RPC 接口：1. 发出请求；2.返回响应；3. 打印返回信息。本机跑出来的结果如下：![](Images/74d96e56b567c83abe4f882549a4721b.png)savepage-src="https://static001.geekbang.org/resource/image/68/98/6876aaa2039b95c3c45159e40d867f98.png"}在这个案例中，参数化数据就是根据真实的业务量来计算的，这个可以肯定没有问题。那么架构呢？在最上面的图中已经有了部署的说明。在逻辑实现上，也就是一个很简单的服务端，内部并没有复杂的逻辑。所用到的监控工具是top、Vmstat。看了哪些计数器呢？CPU、内存、I/O等。 下面我们开始分析。第一阶段对公网上的测试来说，基本上压力都会在网络上，因为出入口带宽会成为瓶颈，所以先要看一眼自己的带宽用到了多少，再比对一下出口路由上的带宽。![](Images/0a8e4100e59b1f89b0899b1e8adf866a.png)savepage-src="https://static001.geekbang.org/resource/image/2a/ff/2ae67c715480b3e8fe1486a943ce05ff.png"}这里 1Gbps 只用到了 0.01%，也就是 (1000/8)x0.01%=12.5k（这里是将带宽bit 换成 byte 计算）。在这样的带宽使用率之下，即使是公网也不见得会有问题，更别说在内网了。可见带宽不是瓶颈点。既然这样，我们直接在内网里来做分析，看原因是什么。但是我们要以什么样的场景来跑呢？因为带宽现在看到用得并不多，但 TPS也上不去。**首先应该想到的场景就是把TPS** 曲线给做出梯度来。为什么要这么做？最重要的就是要知道到底 TPS在多少压力线程下会达到最大值，也就是我在各种场合经常强调的一个场景，最大TPS场景。关于这种曲线，我们不需要性能指标应该就可以做得出来。如下图所示：![](Images/ace9c9caee93c57ce215122a6e636310.png)savepage-src="https://static001.geekbang.org/resource/image/a3/2f/a3dea74520a7fe3c192e6cc24c19bd2f.png"}在一个**既定场景、既定数据、既定环境**的压力场景中，我们一定要拿到这样趋势的 TPS 和 RT曲线。其中绿色和红色的点都是不需要业务指标来限定的，而是通过压力场景中观察TPS 趋势线来确定。我来解读一下这个趋势图：1.       响应时间一定是从低到高慢慢增加的；        2.       TPS    一定也是从低到高慢慢增加的，并且在前面的梯度中，可以和线程数保持正比关联。举例来说，如果    1 个线程 TPS 是 10，那 2 个线程的 TPS 要在    20。依次类推。        而在这个例子中，前面有提到 100 线程 1秒加载完，这样的比例完全看不出来梯度在哪，所以，改为 100 秒加载 100个线程，再来看看梯度。测试结果如下：![](Images/f144a8846937e48d90042470c7bef5cf.png)savepage-src="https://static001.geekbang.org/resource/image/ee/c5/eea8cd59a06a800ad4f1757bb41ec4c5.png"}![](Images/61dba4e6f9988904032abccaca694964.png)savepage-src="https://static001.geekbang.org/resource/image/07/5f/07954ac566e057efd861a15ce18cc85f.png"}![](Images/9e298ffd54860ad683025ee7c51a1f11.png)savepage-src="https://static001.geekbang.org/resource/image/49/71/49ca6af1f764c2688fb62eb756847f71.png"}从这个结果可以看出几点：1.TPS一点梯度没看出来。为什么说没有看出来呢？这里我发一个有明显梯度的 TPS曲线出来以备参考（这张图不是本实例中的，只用来做分析比对）：![](Images/a7c70e5c733bc62abda98810bf702124.png)savepage-src="https://static001.geekbang.org/resource/image/e8/27/e86238087c62ccfad2f48a6038892727.png"}2.响应时间增加的太快了，明显不符合前面我们说的那个判断逻辑。那什么才是我们判断的逻辑呢？这里我发一个有明显梯度的出来以备参考（这张图不是本实例中的，只用来做分析比对）：![](Images/d752791ea3eef2932e73d9332dec78ed.png)savepage-src="https://static001.geekbang.org/resource/image/4b/8e/4bbf7e0983971a3f51b9acc251f5a18e.png"}1.       粒度太粗，对一个 duration    只有五分钟的场景来说，这样的粒度完全看不出过程中产生的毛刺。        2.       至少看到内网的 TPS 能到 180 了，但是这里没有做过其他改变，只是把    Ramp-up    放缓了一些，所以我觉得这个案例的信息是有问题的。        第二阶段针对以上的问题，下面要怎么玩？我们列一下要做的事情。1.       将 Ramp-up 再放缓，改为 300    秒。这一步是为了将梯度展示出来。        2.       将粒度改小，JMeter 默认是 60 秒，这里改为 1    秒。这一步是为了将毛刺显示出来。强调一点，如果不是调优过程，而是为了出结果报告的话，粒度可以设置大一些。至于应该设置为多大，完全取决于目标。        接着我们再执行一遍，看看测试结果：![](Images/5fe6dbcd1de99bb92e2bc6cc41d5732a.png)savepage-src="https://static001.geekbang.org/resource/image/22/c7/22522e14612447ffd60a19e7f96522c7.png"}![](Images/872e07f59c8a422dafdcefb2185c24a2.png)savepage-src="https://static001.geekbang.org/resource/image/9a/70/9a841039650c2452551a7b313bb55f70.png"}![](Images/22adbd94bf0b8066f7c07439845c435e.png)savepage-src="https://static001.geekbang.org/resource/image/9d/25/9d5a375a52dfd8efa064a70c45b9b825.png"}这样看下来，有点意思了哈。明显可以看到如下几个信息了。1.       响应时间随线程数的增加而增加了。        2.       TPS 的梯度还是没有出来。        显然还是没有达到我们说的梯度的样子。但是这里我们可以看到一个明显的信息，线程梯度已经算是比较缓的了，为什么响应时间还是增加得那么快？这里的服务器端压力情况呢？如下所示：![](Images/84cf5bb503a14b9bf0eeae6dbfcbafd0.png)savepage-src="https://static001.geekbang.org/resource/image/fc/de/fc6041ea7780c05f35034d744c0972de.png"}从监控图大概看一下，服务端CPU、内存、网络几乎都没用到多少，有一种压力没有到服务端的感觉。在这一步要注意，压力在哪里，一定要做出明确的判断。在这里，当我们感觉服务端没有压力的时候，一定要同时查看下网络连接和吞吐量、队列、防火墙等等信息。查看队列是非常有效的判断阻塞在哪一方的方式。如果服务端的 send-Q积压，那就要查一下压力端了。如下所示：    State       Recv-Q Send-Q                      Local Address:Port                                     Peer Address:Port    ......    LISTEN      0    5465610001*    ......在网络问题的判断中，我们一定要明确知道到底在哪一段消耗时间。我们来看一下发送数据的过程：![](Images/32d14822b87d8993599c6b0806969176.png)savepage-src="https://static001.geekbang.org/resource/image/bc/a0/bcae022c7205236c618db2bb213cb1a0.png"}从上图可以看出，发送数据是先放到`tcp_wmem`缓存中，然后通过`tcp_transmit_skb()`放到TX Queue中，然后通过网卡的环形缓冲区发出去。而我们看到的 send-Q 就是 Tx队列了。 查看压力端脚本，发现一个问题。    System.out.println(message);一般情况下，我们在调试脚本的时候会打印日志，因为要看到请求和响应都是什么内容。但是压力过程中，基本上我们都会把日志关掉。**一定要记住这一点，不管是什么压力工具，都要在压力测试中把日志关掉，不然TPS 会受到很严重的影响**。了解 JMeter 工具的都知道 -n参数是命令行执行，并且不打印详细的返回信息的。但是这里，一直在打印日志，并且这个日志在JMeter 中执行时加了 -n参数也是没用的。这样一来，时间全耗在打印日志中了。知道这里就好办了。我们在这里做两件事：1.       把打印日志这一行代码注释掉，再执行一遍。        2.       把 ramp-up 时间再增加到 600    秒。    为什么我要执着于把 ramp-up 时间不断增加？在前面也有强调，就是要知道TPS 和响应时间曲线的趋势。在性能分析的过程中，我发现有很多性能工程师都是看平均值、最大值、最小值等等这些数据，并且也只是描述这样的数据，对曲线的趋势一点也不敏感。这是完全错误的思路，请注意，做性能分析一定要分析曲线的趋势，通过趋势的合理性来判断下一步要做的事情。什么叫对曲线的趋势敏感？就是要对趋势做出判断，并且要控制曲线的趋势。有时，我们经常会看到 TPS 特别混乱的曲线，像前面发的 TPS图一样，抖动幅度非常大，这种情况就是完全不合理的，在遇到这种情况时，一定要记得降低压力线程。你可能会问，降到多少呢？这里会有一个判断的标准，**就是一直降到 TPS符合我们前面看到的那个示意图为止**。再给你一个经验，如果实在不知道降多少，就从一个线程开始递增，直到把梯度趋势展示出来。第三阶段通过注释掉打印日志的代码，可以得到如下结果：![](Images/c9b8fe53fc91a0a70023a6f590dc9a38.png)savepage-src="https://static001.geekbang.org/resource/image/cb/24/cb9fb7877d7fbd70d2e104e3d52bef24.png"}![](Images/80e219b0722c75272baa8c0b8fbd79b3.png)savepage-src="https://static001.geekbang.org/resource/image/36/5c/36525754e53e007d2ce3fe5d34435c5c.png"}![](Images/464a214ddc05f19871bd0dbf87f3a143.png)savepage-src="https://static001.geekbang.org/resource/image/15/2d/1551ca0a91c0a24856192d5d87ed852d.png"}从TPS曲线上可以看到，梯度已经明显出来了。在有一个用户的时候，一秒就能达到1000 多 TPS，并且在持续上升；两个线程时达到 2500以上，并且也是在持续上升的。从响应时间上来看，也是符合这个趋势的，前面都在 1ms以下，后面慢慢变长。压力越大，曲线的毛刺就会越多，所以在 TPS 达到 6000 以上后，后面的 TPS在每增加一个线程，都会出现强烈的抖动。在这种情况下，我们再往下做，有两条路要走，当然这取决于我们的目标是什么。1.       接着加压，看系统什么时候崩溃。做这件事情的目标是找到系统的崩溃点，在以后避免出现。        2.       将线程最大值设置为 10，增加 ramp up    的时间，来看一下更明确的递增梯度，同时分析在线程增加过程中，系统资源分配对    TPS    的影响，以确定线上应该做相对应的配置。        总结在这个案例中，我们将 TPS 从 150 多调到 6000以上，就因为一句日志代码。我分析过非常多的性能案例，到最后发现，很多情况下都是由各种简单的因素导致的，这一反差也会经常让人为这一路分析的艰辛不值得。但我要说的是，性能分析就是这样，当你不知道问题在哪里的时候，有一个思路可以引导着你走向最终的原因，那才是最重要的。我希望通过本文可以让你领悟到，**趋势**这个词对曲线分析的重要性。在本文中，我们通过对曲线的不合理性做出判断，你需要记住以下三点：1.       性能分析中，TPS    和响应时间的曲线是要有明显的合逻辑的趋势的。如果不是，则要降线程，增加    Ramp-up 来让 TPS 趋于平稳。        2.       我们要对曲线的趋势敏感，响应时间的增加不可以过于陡峭，TPS    的增幅在一开始要和线程数对应。        3.       当 TPS    和响应时间曲线抖动过于强烈，要想办法让曲线平稳下来，进而分析根本原因，才能给出线上的建议配置。        思考题今天我结合案例具体说明了下如何分析 TPS的趋势，如果你吸收了文章的内容，不妨思考一下这两个问题？1.       Ramp-up    配置有什么样的作用？        2.       为什么说压力工具中 TPS    和响应时间曲线抖动过大会不易于分析？        欢迎你在评论区写下你的思考，也欢迎把这篇文章分享给你的朋友或者同事，一起学习交流一下。
# 27丨案例：带宽消耗以及Swap（上）今天我们来看一个真实的案例。事情是这样的，之前有人在微信上问我一个问题，这个问题的现象很典型：典型的TPS上不去，响应时间增加，资源用不上。 大概的情况是这样的：有两台 4C8G 的服务器，一台服务器上有 2 个Tomcat，一台服务器上是 DB。压测的混合场景有 4 个功能模块，其中 3个访问一个 Tomcat，另外一个访问一个Tomcat。 Tomcat 的监控页面如下： ![](Images/f2773f381c95b3da95cfa1b9e6749660.png)savepage-src="https://static001.geekbang.org/resource/image/53/0b/532bbd1525be0ad0d08da7335645260b.png"}应用服务器系统资源监控页面如下： ![](Images/35485e1c82c6132524fb715a74cc9bbf.png)savepage-src="https://static001.geekbang.org/resource/image/19/e5/1944bc692902fed979815a538d879be5.png"}数据库服务器系统资源监控如下： ![](Images/2b52d26ecb54e66ebc49e1009042d998.png)savepage-src="https://static001.geekbang.org/resource/image/50/f0/50a996ac196dbfd2b9a23858e87dc8f0.png"}JMeter 结果如下： ![](Images/53c5ea56292c5d9da62a04de867b60f0.png)savepage-src="https://static001.geekbang.org/resource/image/f8/33/f8b228a61b980c338046fbb3c8875033.png"}综上现象就是，单业务场景执行起来并不慢，但是一混合起来就很慢，应用服务器和数据库服务器的系统资源使用率并不高。请问慢在哪？ 这是非常典型的询问性能问题的方式，虽然多给了系统资源信息，但是这些信息也不足以说明瓶颈在哪。 为什么呢？在现在多如牛毛的监控工具中，除非我们在系统中提前做好分析算法或警告，否则不会有监控工具主动告诉你，监控出的某个数据有问题，而这个只能靠做性能分析的人来判断。 我在很多场合听一些"专家"说：性能分析判断要遵守木桶原理。但是在做具体技术分析的时候，又不给人说明白木桶的短板在哪里。这就好像，一个赛车手说要是有一个各方面都好的车，我肯定能得第一名，但是，我没有车。 话说出来轻而易举，但是请问木桶的短板怎么判断呢？因为 CPU 高，所以 CPU就是短板吗？所以就要加 CPU吗？这肯定是不对的。 因为这个例子并不大，所以可以细细地写下去。今天文章的目的就是要告诉你，性能问题分析到底应该是个什么分析思路。 分析的第一阶段画架构图做性能测试时，我们需要先画一个架构图，虽然简单，但是让自己脑子里时时记得架构图，是非常有必要的。因为架构级的分析，就是要胸怀架构，在看到一个问题点时，可以从架构图中立即反应出来问题的相关性。 ![](Images/fff1d094e38698e7bf33d1c7eadf8530.png)savepage-src="https://static001.geekbang.org/resource/image/fe/2c/fe8abb747bdeebfefd833ae8a5f4e12c.jpg"}上面这张图是自己脑子里的逻辑图，数据在网络中的流转并不是这样，而是像下图这样。 ![](Images/fabdca629a78a00fb9145eceb2a4bf9a.png)savepage-src="https://static001.geekbang.org/resource/image/72/8c/72fdc9310d5678fc1988f96f3026b28c.jpg"}数据流是从压力机到应用服务器，应用服务器再到网络设备，再到数据库服务器；数据库把数据返回给应用服务器，应用服务器再通过网络设备给压力机。 如果把里面的 Redis、ActiveMQ 和 MySQL的逻辑再细说明白，那么这个小小的应用都可以描述好一会。所以这里，我先大概描述一下，如果后面的分析中涉及到了相应的逻辑，再一点点加进来。 应用服务器只有一台，上面有两个 Tomcat实例；数据库服务器有三个应用。混合场景中有四个业务，其中三个访问Tomcat1，第四个访问 Tomcat2。 场景描述有了场景大概的画像之后，我们再来看场景。根据测试工程师描述： 1.       响应时间慢的，都是可视化页面，有不少图片、JS、CSS    等静态资源。公网上是用 CDN    的，现在只测试内网的部分。静态资源已经做了压缩。        2.       单业务测试的容量是可以满足要求的，但混合场景响应时间就长。系统资源用得并不多。        3.       压力场景是 300 线程，Ramp-up period 是 1    秒。    4.       Duration 是 72000。        5.       各参数化都已经做了，参数化数据也合理。        6.       测试环境都是内网。        7.       服务器是 CentOS，压力机是    Win10。    既然这样，我们还是要看看系统的各个资源，再来判断方向。我在很多场合都强调证据链。对架构比较简单的应用来说，我们都不用再做时间的拆分了，直接到各主机上看看资源就好了。 瓶颈分析定位根据我们之前画的架构图，我们从应用服务器、数据库服务器和压力数据分别定位一下。 1.  应用服务器        ![](Images/a3daf5df003e6dc1c992edcdc131a957.png)savepage-src="https://static001.geekbang.org/resource/image/5f/b0/5f3f4fc313c833ff2ad979d63de7f9b0.png"}从前面的应用服务器资源来看，CPU 使用率现在还不高，也基本都在 usCPU（就是 user 消耗的CPU）上，比例也比较合理。 物理内存 8G，还有 3.5G。即使不多，对 Java 应用来说，也要先看JVM，只要不是 page fault过多，我们可以先不用管物理内存。 网络资源接收 900KB/s 左右，发送 11M左右。这样的带宽应该说是比较敏感的，因为对 100Mbps 和 1000Mbps来说，我们要心里有一个数，一个是 12.5MB（对应 100Mbps），一个是125MB（对应1000Mbps），当和这样的带宽值接近的时候，就要考虑下是不是带宽导致的压力上不去。不过这里我们也不用先下定论。只是留个疑问在这里。 磁盘资源，基本上没有读写，很正常。 从 Process 列表中，也没看到什么异常数据。faults 也都是 min，majorfault并没有。但在这个案例中，还部署了另一个监控工具，上面显示如下： ![](Images/741fe43059549ba64720219b38f0a7aa.png)savepage-src="https://static001.geekbang.org/resource/image/9e/b9/9e9bc7528dac06fcbef811eed0ae8bb9.png"}为什么这个 Swapping 要标黄呢？那肯定是过大了嘛。是的，你可以觉得 swap过多。这个扣，我们也记在心里。在这里标红加粗敲黑板！ 1.  数据库服务器        ![](Images/44aba34bc1d0ed4f3b356999c86cd15a.png)savepage-src="https://static001.geekbang.org/resource/image/23/14/23b3ce5e2d551a54c59ef24c3601ad14.png"}照样分析，CPU、内存、网络、磁盘、Process列表，并没看到哪里有异常的数据，连网络都只有 500 多 k的发送。 这样的数据库资源状态告诉我们，它不在我们的问题分析主线上。接下来是压力数据。 1.  压力数据        这是 JMeter 中的聚合报告： ![](Images/486947316a9c4ffa1cc8344d3a74602c.png)savepage-src="https://static001.geekbang.org/resource/image/ee/98/eecc1affbdeca2484d2964a93d75fe98.png"}从上面这张图也能看出，响应时间确实挺长的，并且，300 线程只有 37 的TPS，带宽总量 10M左右。这个带宽倒是和应用服务器上的带宽使用量相当。关于这个带宽的判断请你一定注意，对于性能分析来说，带宽能不能对得上非常重要。比如，客户端接收了多少流量，服务端就应该是发出了多少流量。如果服务端发了很多包，但是客户端没有接收，那就是堵在队列上了。 既然其它的资源暂时没出现什么瓶颈。其实在这个时间里，如果是复杂的应用的话，我们最应该干的一件事情就是拆分时间。如下图所示： ![](Images/3df0215e46943ceb256de21eac2148d6.png)savepage-src="https://static001.geekbang.org/resource/image/e7/41/e77401663a74e38dacf6cd05f3c96f41.jpg"}这里我把时间拆为 t1-t5，具体分析哪一段为什么消耗了时间。我们可以在Tomcat 中加上 %D 和 %F 两个参数来记录 request 和 response的时间。 在没有做这个动作之前，我们先把前面的扣解决一下。首先，带宽是不是受了100Mbps 的限制？ 一般来说，判断网络的时候，我们会有几个判断点。 首先是带宽的流量大小，也就是前面我们看到的 11M左右的值。一般来说，100Mbps 是指的 bit persecond，但是在应用层基本上都是 byte，所以对 100Mbps 来说，是12.5MB。 其次是，全连接和半连接队列是否已经溢出？ ![](Images/b8d2d161412e7734e8bdb7916efb12c7.png)savepage-src="https://static001.geekbang.org/resource/image/c7/07/c7a67037f03d82378eec8b6c70c32207.png"}我们通过 SYNs to LISTEN sockets dropped来判断半连接队列是否溢出，通过 times the listen queue of a socketoverflowed来判断全连接队列是否溢出。 通过实时的查看，这两个值的增加并不多。所以这里不会是问题点。 最后是发送和接收队列是否堆积？ ![](Images/ff4b5e133ce93668ce7ea2a17ae6fc7b.png)savepage-src="https://static001.geekbang.org/resource/image/fe/72/fe954bf48b860464923418ca90800572.png"}通过应用服务器上的send-Q（前面数第三列），可以看到服务器和压力机之间的的队列还是很长的，基本上每次查看都存在，这说明队列一直都有堆积。 我们再到压力机上看看带宽用了多少： ![](Images/31bcf43e6c415355fe7e781bb15e05b9.png)savepage-src="https://static001.geekbang.org/resource/image/f2/4d/f2a3e24763892f2eab807adc6934ac4d.png"}看这里也是用到了 93Mbps，那么到这里我们就可以确定是网络问题导致的 TPS上不去，响应时间增加，系统资源也用不上了。 和系统管理员确认宿主机的带宽后，被告知宿主机确实是100Mbps。 似乎这个分析到这里就可以结束了，直接把带宽加上再接着测试呗。但是，从项目实施的角度上说，这个问题，并不是阻塞性的。 为了把更多的性能问题提前找出来，现在我们先不下载静态资源，只发接口请求找下其他性能问题。这个带宽的问题，记一个bug 就行了。 优化结果我们将静态资源全都过滤掉之后，再次执行场景，结果是下面这样的。 JMeter 压力数据： ![](Images/44cdbd7f1eb95b4621362078f6956025.png)savepage-src="https://static001.geekbang.org/resource/image/77/aa/771ee29f154f43bc84c8e8df66431caa.png"}应用服务器带宽： ![](Images/0f03d987032d997b3207662392ef92ed.png)savepage-src="https://static001.geekbang.org/resource/image/ed/b6/ed896cecbdfbb6c9cc4beba10f5744b6.png"}数据库服务器带宽： ![](Images/301c1b4e480a96cba72e7b1c9356b4b8.png)savepage-src="https://static001.geekbang.org/resource/image/74/b8/74a2516200a297a062730341eacf60b8.png"}应用服务器网络队列： ![](Images/8e8ebee19b879a5ae742435c8829d94e.png)savepage-src="https://static001.geekbang.org/resource/image/75/73/750402d3482cba2c6e77cd9043616373.png"}应用服务器资源监控： ![](Images/73b6a3b7cae23083e66b2c509663bdd0.png)savepage-src="https://static001.geekbang.org/resource/image/75/7c/75423cd11a411b3847252d479554a97c.png"}通过上面的结果可以看出： 1.       TPS 可以达到 221.5 了，并且 Received 和 Sent 的字节加一起不到    4MB。    2.       应用服务器和数据库服务器的带宽都用到了近 40Mbps，和 JMeter    结果也相当。        3.       应用服务器上的网络队列也没有堆积。        4.       应用服务器的 CPU 也已经能消耗到 66%    了，    当正在想通过过滤掉静态资源绕过带宽不足的现状来测试其他性能问题的时候，这时，Swap双向都标黄了。这时，性能测试工程师更纠结了，它为什么双向都黄了？CPU使用率才 66% 嘛。 其实，这两句话之间并没有什么关系，CPU 使用率不管是多少，Swap该黄还是会黄。 这是为什么呢？这里卖个关子，在下一篇文章中，我们接着分析。 总结带宽问题是性能分析中常见的问题之一，其难点就在于，带宽不像 CPU使用率那么清晰可理解，它和 TCP/IP协议的很多细节有关，像三次握手，四次挥手，队列长度，网络抖动、丢包、延时等等，都会影响性能，关键是这些判断点还不在同一个界面中，所以需要做分析的人有非常明确的分析思路才可以做得到。而且现在的监控分析工具中，对网络的判断也是非常薄弱的。 而 Swap问题不能算是常见，只要出现，基本上就会很多人晕乎。解决的关键就是要明白Swap的原理，查到关联参数，然后就可以很快地定位了。 思考题结合今天的内容，你能说一下网络的瓶颈如何判断吗？有哪几个队列？ 欢迎你在评论区写下你的思考，也欢迎把这篇文章分享给你的朋友或者同事，一起交流一下。 