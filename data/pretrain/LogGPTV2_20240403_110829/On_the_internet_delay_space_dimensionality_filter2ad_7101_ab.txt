to 25 percent, consistent across the diﬀerent datasets.
Subsequently, Dabek et al. proposed Vivaldi [12], which
is a coordinate-based system that uses a mass-spring relax-
ation problem to determine the coordinates of nodes.
In
spite of using beacons, like GNP, Vivaldi has the advantage
of not requiring ﬁxed nodes serving this role and provides
a degree of accuracy that is competitive with that of GNP.
More interestingly, the authors propose the idea of unidi-
rectional height vectors to augment the geometric model.
Intuitively, the core of the network is mapped into a vec-
tor space as before whereas the borders of the network are
assigned heights that penalize the distances of access link
traversals. As a result, nodes can be placed up or down in
order to accommodate conﬂicting distances in low dimen-
sionality.
It has been shown that this approach was able
to embed the dataset considered into 2-space plus heights
with competitive accuracy as compared to the embedding
into 5-space.
The Vivaldi project also explores alternative geometric
spaces, such as spherical and cylindrical, given that curved
spaces resembles the surface of the globe around which the
Internet is deployed. However, since most of the core links
are centered in the U.S. and Europe, and due to the fact
that there is no communication passing through the poles,
the Internet does not wrap around the earth and, there-
fore, it has been shown that these approaches are no better
than simply ﬁtting the network into a plane space. Shavitt
and Tankel explore embeddings in hyperbolic space which
accommodate distance conﬂicts in low dimensional spaces,
achieving a similar level of success as the height vectors [36].
More recently, studies quantiﬁed the inaccuracy produced
by current positioning systems. Ledlie, Gardner, and Seltzer
demonstrated that the relative errors increase with the car-
dinality of the set of hosts, and that the convergence to and
maintenance of stable coordinates produced by the embed-
ding algorithms are barriers to the eﬀectiveness of such sys-
tems [22]. Lua et al. conﬁrm that the degree of inaccuracy
is beyond tolerable and propose new methods to quantify
this fact, which is otherwise hidden by analyzing cumulative
distributions of relative errors [24].
Zhang et al. characterized the delay space and proposed a
synthetic data generator that improves upon existing topol-
ogy generators [45]. Later on, the same group studied the
impact of triangle inequality violations found in the delay
space on overlay networks [42].
There is a rich body of theoretical work on questions re-
garding the existence of low-distortion embeddings of ﬁnite
point sets into Euclidean space and other host metrics, as
well as the computational complexity of algorithms for com-
puting such embeddings. The starting point for much of
this research is Bourgain’s famous theorem which asserts
that every metric space of cardinality n may be embedded
with distortion O(log n) in a Euclidean space of dimension
O(log n) [8]. A randomized algorithm for computing such
an embedding was supplied by Linial, London, and Rabi-
novich [23]; the algorithm is based on semideﬁnite program-
ming combined with a random-projection method due to
Johnson and Lindenstraus [18]. Although there has been
progress on the problem of minimizing the additive distor-
tion, i.e., the maximum additive error over all pairs of points,
the corresponding problem of computing low multiplicative
distortion embeddings into lower-dimensional spaces — e.g.,
Euclidean spaces of dimension o(log n) — is an algorithmic
problem of daunting complexity. Indeed, it is NP-hard to
approximate the minimum-distortion embedding of an n-
point metric into a d-dimensional Euclidean space within an
approximation factor less than Ω(n1/12) [17]. Finally, sev-
eral recent papers have considered the problem of comput-
ing embeddings with slack [5, 19], in which an ε fraction of
all distances may be arbitrarily distorted and the rest must
satisfy a low-distortion guarantee. This work, which uses
beacon-based embedding techniques a la GNP, culminated
in a theorem that every ﬁnite metric space admits an em-
´ dimensions with O`log 1
´ distortion
bedding in O`log2 1
ε
ε
and ε slack. Note that both the distortion and the dimen-
sionality of the host metric in this theorem are still too high
to be of practical value for network coordinate systems.
In Section 4 we study the power-law behavior of the delay
space and its relationship with fractal dimensions. Many
diﬀerent power-laws and self-similar phenomena have been
documented in the literature on Internet measurement. Ex-
amples include power-laws in the distribution of packet rates
on an Ethernet link [43], inter-arrival times for FTP connec-
tions and TELNET packets [30], HTTP connections [11],
diﬀerent aspects of the Internet topology [13], and round-
trip measurements in a time series of pings between a single
pair of hosts [4].
The measures of fractal dimensions used in this work were
also used by Belussi and Faloutsos [7]. Their work demon-
strated that when spatial datasets behave like fractals (de-
ﬁned in Section 4.2.1) over a wide range of distances, one
can use measurements of their fractal dimensions to rapidly
estimate the spatial selectivity in range queries.
To the best of our knowledge, our work is the ﬁrst to
study the underlying dimensionality of the Internet delay
space as a separate issue from the embedding of this delay
space in any particular metric space, the ﬁrst to document
power-laws in the Internet delay space and to apply frac-
tal geometry to its characterization, and the ﬁrst to explore
the impact of the Internet’s AS-level topology on its delay-
space geometry. The dimensionality revealed by our tech-
niques is signiﬁcantly lower than the dimensionality of the
embeddings used in the prior work [12, 28, 39], although the
existence of an algorithm that produces embeddings with
this low dimensionality behavior is still an open question.
3. METHODOLOGY
Our main analysis is based on the Meridian dataset pre-
sented by Wong, Slivkins, and Sirer [44], and some of our
ﬁndings are also supported by observations made using the
MIT King dataset [2, 12].
The Meridian dataset was collected between May 5-13
2004 via the King [14] method, containing latency measure-
ments between more than 5200 DNS servers. The list of
sites to measure was determined by randomly picking web-
site names from a set of 593160 entries obtained from the
DMOZ and Yahoo directories. The raw data consists of
a set of asymmetric measurements between pairs of DNS
servers, that is, the RTT’s in microseconds between two
servers A and B, measured by recursively querying A for
domains served by B, and vice-versa. The number of asym-
metric measurements per pair varies between 1 and 20 en-
tries with median 11. In order to create the matrix used in
this work, we took the union of the asymmetric measure-
ments for each pair, thereby making the dataset symmet-
ric. We subsequently ﬁltered out the pairs with less than 10
measurements, in order to minimize biases due to queuing
delays at routers or DNS servers, and then computed the me-
dian of the symmetric measurements for the remaining pairs.
Finally, we approximate the largest clique in the resulting
incomplete matrix via a 2-approximation algorithm for the
vertex cover problem (i.e., to eliminate the missing entries
by removing the minimum number of nodes), resulting in
a all-pairs matrix with 2385 hosts, annotated with their IP
addresses (henceforth referred to as “IPs” for brevity).
The MIT King dataset was ﬁrst used to study Vivaldi’s
behavior [12]. It was also collected using the King method
and contains measurements among 1953 hosts, selected by
ﬁnding the NS records of IP addresses of participants in
a Gnutella network. Nevertheless, after applying the above
data cleaning process, only 298 nodes remained in the dataset.
Although the size of this dataset does not allow us to use it
for analyzing all the aspects discussed in this work, it can
still be used to support some of our ﬁndings.
We also merged the delay data in the Meridian dataset
with the underlying AS topology by obtaining a snapshot,
from the same period the delay dataset was collected, of
the customer-provider AS graph from the CAIDA AS rela-
tionships dataset [3]. Using the combined data, we made a
decomposition of the network into AS trees, each rooted at
one major Tier-1 Autonomous Systems (AS). Accordingly,
each piece comprises the Tier-1 network itself, together with
its downstream network of AS customers. After decompos-
ing the whole network in this fashion, we classiﬁed the IPs
found in the Meridian dataset into each piece. The number
of IPs found in each piece is summarized on Table 1.
Notice that the sum of the number of IPs in each network
exceeds the total number of IPs in the dataset. This is be-
cause most of the IPs are located in multihomed networks
(i.e., are served by multiple providers). In fact, according
Table 1: List of the major Tier-1 AS together with
the number of IPs in their downstream networks
represented in the Meridian dataset.
AS
AS# Name
Meridian
Hosts
2914 NTT Comm.
209 Qwest
3561
SAVVIS
3356 Level 3
7018 AT&T
3549 Global Crossing
701 Verizon
1239
Sprint Nextel
1212
1227
1389
1454
1487
1515
1529
1604
to our data, more than 60% of the customers have contracts
with more than one provider, and the number of upstream
providers per network can be as high as 13. Thus, our de-
composition does not consist of a partition of the space, and,
in fact, some of the subsets contain more than 50% of the
nodes in the whole clique. 3
We emphasize that the King method is a convenient way
to obtain vantage points using DNS servers, which are gen-
erally well-connected hosts. Thus, although its hosts are
geographically and topologically diverse, it can be argued
that datasets collected via King could only give us an ap-
proximate picture of the delay space geometry as composed
of core and edge networks. Furthermore, both datasets con-
tains violations of the triangle inequality to a degree consis-
tent with that found in the characterizations by [12], [39],
[42], [44], and [45].
The unavailability of Internet latency datasets is a major
limitation to the study of the dimensionality of the delay
space. We have investigated the other publicly available
datasets of this kind but they are either limited in scale, i.e.,
do not contain a large enough all-pairs matrix so that our
analysis can be applied, or they are not annotated with IP
addresses, which makes the above decomposition impossible.
In order to analyze the geographic component of the delay
space, we queried the hostip.info database [1] for the IPs
contained in the Meridian dataset, obtaining their latitude
and longitude at the time of writing. Since these IPs belong
to DNS servers of large domains that are not as likely to have
their IPs reassigned as are smaller domains, we resort to
the assumption (not quantiﬁed) that a large fraction of the
IPs contained in the Meridian dataset were not reassigned
since 2004. Even though we believe that this is a reasonable
approximation, it should be emphasized that, together with
the fact that geolocation is currently a process with high
degree of inaccuracy, this could combine several sources of
error for the particular set of results in Section 5.1.
3A recent result indicates that a single snapshot of the in-
ferred AS topology map is believed to miss around 10% of
the customer-provider links involving Tier-1 and Tier-2 net-
works [29]. While this does not lead to missclassiﬁcation,
it could possibly exclude the membership of some domains
(and its downstream customers) in some of the pieces.
Figure 1 presents a coarse-grained visualization of the geo-
location of nodes in the Meridian dataset.
Figure 1: Geographic location of nodes in the Merid-
ian dataset.
4. DIMENSIONALITY MEASURES
As a starting point for introducing the measures of di-
mensionality that we use in this work, let us consider the
following problem. Suppose that a surveyor chooses a set
X of 2500 random points in the plane and measures the
distances between all pairs using a method that introduces
5% relative error due to measurement noise. Given the ma-
trix of measurements, but not the coordinates of the actual
points, how could one deduce that the data came from a
point set in 2 dimensions, rather than 1 or 3? We consider
this problem further in the following sections.
4.1 Embedding dimension
An obvious answer to the question question posed in the
previous section is:
for d = 1, 2, 3, . . ., try to embed the
points in d dimensions using an embedding algorithm such
as Vivaldi. Stop at the lowest dimension D which permits
an embedding with small quartiles of relative errors and let
D denote the embedding dimension of the dataset.
We applied this process to the Meridian dataset by embed-
ding the network into Euclidean space using Vivaldi, avail-
able as part of the P2Psim package [2]. We varied the num-
ber of dimensions from d = 1 to d = 9 and Figure 2 presents
the outcomes of this experiment by displaying the 35-th, 50-
th, 65-th, and 80-th percentiles of relative errors incurred
by embedding the whole delay space with diﬀerent values
of d (with the percentile values chosen in such a way that
the discrepancy of the distributions could be well captured).
In this plot, we can observe that there is a fast improve-
ment in accuracy up to d = 4 and a slow improvement up to
d = 7. Surprisingly, after 7 dimensions, the accuracy of the
algorithm gets worse, exposing the threshold beyond which
the curse of dimensionality starts to aﬀect the algorithm’s