events occurred on the downstream path, i.e. from the sender
to the remote host, or on the upstream path, i.e. from the
remote host to the sender. While this could cause Monarch
ﬂows to behave diﬀerently than regular TCP ﬂows, our eval-
uation in Section 4 shows that both these events occur rarely
in practice and even when they do occur, they tend to have
a limited eﬀect on Monarch’s accuracy. For example, up-
stream packet loss and reordering aﬀect less than 15% of all
ﬂows. Further, Monarch has a built-in self-diagnosis mech-
anism that can detect most of such inaccuracies using an
oﬄine analysis. Nevertheless, Monarch is not suitable for
environments where upstream loss and reordering events oc-
cur frequently.
Another source of diﬀerences between Monarch and TCP
ﬂows is the delayed ACK feature. With delayed ACKs, TCP
data packets received in close succession are acknowledged
Sender
thread
1
Receiver
thread
5
6
4
Proxy
thread
Monarch
TCP/IP
Netfilter
Raw socket
Network driver
Local
host
3
Action
Sender transmits packet
Proxy intercepts packet,
saves it, and transmits a probe
Remote responds
Proxy forwards saved packet
to the receiver
Receiver sends ACK
Proxy forwards ACK directly
to the sender
#
1
2
3
4
5
6
Linux
Kernel
2
Source
localIP
localIP
Packet
Data
Probe
Response
remoteIP
Destination
remoteIP
remoteIP
localIP
localIP
Data
ACK
ACK
remoteIP
localIP
remoteIP
remoteIP
localIP
Figure 2: Sequence of packet exchanges in Monarch im-
plementation: Monarch consists of a TCP sender, a TCP
receiver, and a proxy. The proxy uses Netﬁlter to interpose
between the sender and the receiver. It applies network ad-
dress translation to create the illusion that the remote host
is the other endpoint of the ﬂow.
with a single ACK packet. In contrast, in a Monarch ﬂow,
the receiver responds to every probe packet, which typi-
cally doubles the number of packets ﬂowing on the reverse
path. However, because the response packets are small, this
diﬀerence is likely to aﬀect only ﬂows experiencing severe
congestion on the upstream path.
When Monarch is used on a path that contains middle-
boxes such as NATs or ﬁrewalls, the probes may be answered
by the middleboxes rather than the end host. However, the
middleboxes are often deployed close to the end host, and so
the resulting loss of ﬁdelity tends to be small. For example,
Monarch probes to many commercial cable/DSL hosts are
answered by the modems that are one hop away from the
end host; however, the network paths to them include the
‘last mile’ cable or DSL links.
3.
IMPLEMENTATION
In this section, we ﬁrst present the details of our imple-
mentation of Monarch, which runs as a user-level application
on unmodiﬁed Linux 2.4 and 2.6 kernels. We then describe
how our implementation allows us to test complete, unmod-
iﬁed implementations of transport protocols in the Linux
kernel as well as the ns-2 simulator [30]. Finally, we discuss
the self-diagnosis feature of our implementation. In particu-
lar, we show how Monarch can detect potential inaccuracies
in its emulated ﬂows by an oﬄine analysis of its output.
3.1 Emulating a TCP ﬂow
Our Monarch implementation uses three threads: A
sender and a receiver, which perform a simple TCP trans-
fer, as well as a proxy, which is responsible for intercepting
packets and handling probes and responses. The proxy also
records all packets sent or received by Monarch and writes
them to a trace ﬁle for further analysis.
To emulate a ﬂow to remoteIP, Monarch uses the Net-
ﬁlter [29] framework in the Linux kernel. First, the proxy
sets up a Netﬁlter rule that captures all packets to and from
that remote address. Next, it creates a raw socket for send-
ing packets to the remote host. Finally, the sender thread
attempts to establish a TCP connection to remoteIP, and
the packet exchange shown in Figure 2 takes place.
As usual, the sender begins by sending a SYN packet to
remoteIP (step 1). The proxy intercepts this packet, stores
it in a local buﬀer, and sends a similarly-sized probe packet
to the remote host (step 2). The remote host responds with
a packet that is also intercepted by the proxy (step 3). The
proxy then looks up the corresponding packet in its buﬀer,
modiﬁes its destination IP address, and forwards it to the
receiver (step 4). The receiver responds with a SYN/ACK
packet that is captured by the proxy (step 5). The proxy
then modiﬁes its source IP address and forwards the packet
back to the sender (step 6). Figure 2 also shows the details of
Monarch’s packet address modiﬁcations among the sender,
the receiver, and the proxy.
All further packet exchanges are handled in a similar man-
If a packet transmitted to remoteIP is lost, its re-
ner.
sponse is never received by the proxy, and the correspond-
ing buﬀered packet is never forwarded to the local receiver.
Similarly, reordering of packets sent to the remote host re-
sults in the buﬀered packets being forwarded in a diﬀerent
order. During long transfers, Monarch reclaims buﬀer space
by expiring the oldest packets.
The output from Monarch includes a packet trace similar
to the output of tcpdump. In addition, it also logs how state
variables of the protocol vary over time. For example, our
current implementation records TCP state variables, such
as congestion window, the slowstart threshold, and the re-
transmission timeout, via a standard interface of the Linux
kernel. The source code of our Monarch implementation is
available from the Monarch web site [27].
3.2 Testing unmodiﬁed transport protocol im-
plementations
Our proxy implementation is completely transparent
to our TCP sender and TCP receiver. This is critical to
Monarch’s ability to test unmodiﬁed, complex protocol
implementations in the Linux kernel. Further, since both
the sender and the receiver run locally, we can easily
evaluate the eﬀect of diﬀerent parameter choices on the
sender and receiver for a given transport protocol. For
example, Monarch can be used to test the sensitivity of
TCP Vegas [8] to the diﬀerent settings of its α and β
parameters over paths to diﬀerent hosts in the Internet. We
can also run implementations of diﬀerent TCP protocols
simultaneously to understand how the protocols compete
with each other. As we show in Section 5.3, this ability
to test protocol
implementations under a wide range of
experimental conditions can be used by protocol develop-
ers to discover errors that aﬀect the performance of their
implementations.
Since it is a common practice among researchers to test
new transport protocols using the ns-2 simulator [30], we
added a special interface to Monarch that allows it to con-
nect directly to ns-2. Thus, researchers can conveniently
use a single ns-2 code base for both their controlled simu-
lation and live emulation experiments. More details about
this feature are available at the Monarch web site [27].
3.3 Self-diagnosis
Monarch is capable of diagnosing inaccuracies in its own
emulated ﬂows based on an analysis of its output. As we dis-
cussed earlier, the two primary factors that aﬀect Monarch’s
accuracy are its inability to distinguish loss and reordering of
packets on the upstream and the downstream paths, i.e., the
paths from the receiver to the sender and vice-versa. These
events are diﬃcult to detect on-line, but their presence can
be inferred after the emulation is ﬁnished. Monarch runs
a self-diagnosis test after each emulation, which either con-
ﬁrms the results or lists any events that may have aﬀected
the accuracy.
3.3.1 Detecting upstream loss and reordering
Monarch’s self-diagnosis uses the IP identiﬁer (IPID) ﬁeld
in the IP headers of the response packets to distinguish be-
tween upstream and downstream events. Similar to prior
techniques [6, 21], Monarch’s self-diagnosis relies on the fact
that many Internet hosts increment the IPID ﬁeld by a ﬁxed
number (typically one) for every new packet they create.
However, Monarch’s analysis is more involved, as it cannot
send any active probes of its own and so must extract the
information from a given trace.
3.3.2 Impact of upstream loss and reordering
Upstream packet loss and reordering events aﬀect diﬀer-
ent transport protocols in diﬀerent ways. For example, TCP
Reno is more strongly inﬂuenced by packet loss than packet
reordering. Even a single upstream packet loss confused as
a downstream packet loss causes TCP Reno to retransmit
the packet and halve its future sending rate. On the other
hand, only packet reordering on a large magnitude can trig-
ger retransmissions that aﬀect future packet transmissions
in a signiﬁcant way.
Self-diagnosis tries to estimate the impact of upstream
loss and reordering on Monarch ﬂows. This impact analysis
depends on the speciﬁc transport protocol being emulated.
While we focus on the analysis we developed for TCP Reno,
similar analysis techniques can be developed for other pro-
tocols. Our impact analysis for TCP Reno labels a ﬂow as
inaccurate if it sees an upstream packet loss or signiﬁcant
upstream packet reordering that causes packet retransmis-
sion. It conﬁrms all Monarch traces that see no upstream
packet loss and no signiﬁcant upstream packet reordering.
We note that conﬁrmation of a Monarch trace by our
analysis does not imply that the trace is accurate for all
usage scenarios. It merely suggests that the trace is likely
to be accurate for many uses. For example, a Monarch trace
that suﬀers only minor reordering would be conﬁrmed. Such
a trace would be accurate with respect to its throughput,
latency, or packet loss characteristics, but not with respect
to its reordering characteristics.
3.3.3 Output
After detecting upstream events and analyzing their im-
pact, Monarch broadly classiﬁes the result of an emulation
as either conﬁrmed, inaccurate, or indeterminate. We il-
lustrate the decision process in Figure 3, and we discuss it
below:
Yes
IPIDs
usable?
No
All upstream
events
identified?
No
Indeterminate
Yes
Upstream
losses?
No
Yes
No
Significant
upstream
reordering?
Yes
Inaccurate
d
e
m
r
i
f
n
o
C
Figure 3: Self-diagnosis in Monarch: The result is con-
ﬁrmed only if no known sources of inaccuracy are present.
• Indeterminate: Results in this category do not con-
tain enough information for Monarch to distinguish
upstream events (loss or reordering) from downstream
events in all cases. This can happen when downstream
losses and upstream losses occur very close together,
or when the IPIDs in the response packets are unus-
able because the remote host randomizes them, or sets
the ﬁeld to zero.
• Inaccurate: Monarch warns that its results could be
inaccurate when it detects any upstream packet losses,
or when the observed upstream packet reordering is
signiﬁcant.
• Conﬁrmed: In all other cases, Monarch has not de-
tected any upstream losses or signiﬁcant reordering
events. Therefore, it conﬁrms its output.
3.3.4 Rate-limited responses
In addition to the loss and reordering analysis, Monarch
also scans the entire trace for long sequences of packet losses
to identify hosts that rate-limit their responses. For exam-
ple, in our measurements, we observed that some hosts stop
sending responses after a certain number of probes, e.g. af-
ter 200 TCP ACKs, which could be due to a ﬁrewall some-
where on the path. This pattern is easy to distinguish from
packet drops due to queue overﬂows because in the latter
case, packet losses alternate with successful transmissions.
However, it is hard to distinguish losses due to path failures
from end host rate-limiting.
3.4 Usage concerns and best practices
As is the case with using many active measurement tools,
large-scale experiments using Monarch can raise potential
security concerns.
Internet hosts and ISPs could perceive
Monarch’s traﬃc as hostile and intrusive. To address this
concern, Monarch includes a custom message in the payload
of every probe packet. We use the message to explain the
goals of our experiment, and to provide a contact e-mail ad-
dress. We have conducted Monarch measurements to several
thousand end hosts and routers in the Internet in hundreds
of commercial ISPs over a period of seven months without
raising any security alarms.
Another cause of concern is with using Monarch to send
large amounts of traﬃc to a remote host. This can be of
great inconvenience to remote hosts on broadband networks
that use a per-byte payment model for traﬃc, where any
unsolicited traﬃc costs the host’s owner real money. To
mitigate this concern, we only measure hosts in broadband
ISPs that oﬀer ﬂat rate payment plans.
In addition, we
never transfer more than a few dozen megabytes of data to
any single Internet host.
Finally, we would like to point out that Monarch ﬂows
compete fairly with ongoing Internet traﬃc as long as the
emulated transport protocols are TCP-friendly.
Sender nodes
Receiver nodes
Successful measurements
PlanetLab
Broadband
Router
4
356
12,166
4
4,805
15,642
4
697
2,776
Table 3: Traces used for our Monarch evaluation: For
each trace, we used geographically dispersed sender nodes
in Seattle (WA), Houston (TX), Cambridge (MA), and
Saarbr¨ucken (Germany).
4. EVALUATION
In this section, we present three experiments that evaluate
Monarch’s ability to emulate transport protocol ﬂows. First,
we evaluate the accuracy of its emulated ﬂows, i.e., we ver-
ify how closely the characteristics of Monarch ﬂows match
those of actual TCP ﬂows. Second, we identify the major
factors and network conditions that contribute to inaccu-
racies in Monarch’s emulations, and show that Monarch’s
self-diagnosis can accurately quantify these factors. Third,
we characterize the prevalence of these factors over the In-
ternet at large.
4.1 Methodology
Evaluating Monarch’s accuracy over the Internet at scale
is diﬃcult. To evaluate Monarch, we need to compare its
emulated ﬂows to real transport ﬂows over the same Inter-
net paths. Unfortunately, generating real transport ﬂows
requires control over both end hosts of an Internet path.
In practice, this would limit our evaluation to Internet
testbeds, such as PlanetLab. We deal with this limitation
using the following three-step evaluation:
1. In Section 4.2, we evaluate Monarch over the Plan-
etLab testbed. We generate both Monarch ﬂows and
real TCP ﬂows, identify potential sources of error, and
study how they aﬀect accuracy.
2. In Section 4.3, we show that Monarch’s oﬄine self-
diagnosis can accurately detect these errors from its
own traces.
3. In Section 4.4, we use this self-diagnosis capability to
estimate the likelihood of error in Monarch measure-
ments over a wide variety of Internet paths.
4.1.1 Data collection
We used Monarch to emulate transport ﬂows over three
types of Internet paths: (a) paths to PlanetLab nodes, (b)
paths to Internet hosts located in commercial broadband
ISPs, and (c) paths to Internet routers. Table 3 shows
statistics about the three datasets we gathered. All mea-
surements involved 500kB data transfers. The TCP senders
were located in four geographically distributed locations,
three (Seattle, Houston and Cambridge) in the U.S. and
one (Saarbr¨ucken) in Germany, while the receivers included
PlanetLab nodes, broadband hosts, and Internet routers.
While gathering the PlanetLab dataset, we controlled both
endpoints of the Internet paths measured, so we generated
both Monarch and normal TCP ﬂows.
In the other two
datasets we only controlled one endpoint, so we generated
only Monarch ﬂows.
Ameritech
BellSouth
AT&T
S+SW
USA
214
384K,
6M
1.5M, 3M,
BellSouth
SE USA
242
256K,
6M
1.5M, 3M,
Company
Region
Hosts
Measured
Offered
BWs
(bps)
DSL
BTOpen
World
BT
Group
UK
218
2M
PacBell
AT&T
S+SW
USA
342
384K,
1.5M,
3M, 6M
Qwest
Qwest
W USA
131
256K,
1.5M,
3M, 6M
SWBell
AT&T
S+SW
USA
1,176
384K,
1.5M,
3M, 6M
Charter
Charter
Comm.
USA
210