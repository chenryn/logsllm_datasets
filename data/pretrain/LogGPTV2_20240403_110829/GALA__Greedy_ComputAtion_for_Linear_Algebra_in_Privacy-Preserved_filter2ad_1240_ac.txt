with the encrypted data [x]C and w1 is multiplied with the
input that is rotated by one position (i.e., [x(cid:48)]C). As a result,
the server gets no multiplied ciphertext, {[ui]C}. The entries in
each of {[ui]C} are partial sums of the elements in the matrix-
vector multiplication wx. For example, as shown in step (c)
of Figure 4, the server obtains two multiplied ciphertext (i.e.,
[u0]C and [u1]C) whose elements are partial sums of the ﬁrst
and second elements of wx (i.e., (A1M1 + A2M2 + A3M3
+ A4M4) and (B1M1 + B2M2 + B3M3 + B4M4)). Then the
server sums them up elementwise, to form another ciphertext,
which is the vector in the middle of step (d) in Figure 4. At
this point, similar to the naive method, the server proceeds with
RaS iterations and ﬁnally obtains a single ciphertext
log2
whose ﬁrst no entries are the corresponding no elements of
wx (see the ﬁrst two elements of the vector after RaS in step
(d)).
ni
no
n
ni
copies of x and the server is able to multiply n
ni
Furthermore, as the number of slots n in a ciphertext is
always larger than the dimension of the input vector, ni, the
computation cost is further reduced by packing copies of input
x as much as possible to form [xpack]C. Thus [xpack]C has
encoded
vectors with [xpack]C by one ScMult operation. Therefore
rather than no multiplied ciphertext. The
the server gets nino
n
resulted single ciphertext now has n
blocks.
no
RaS iterations to get the ﬁnal
The server then applies log2
ciphertext, whose ﬁrst no entries are the no elements of wx.
scalar multiplications
(ScMult), nino
n
− 1 additions (Add). There
no
Perm rotations, and nino
is only one output ciphertext, which efﬁciently improves the
slot utilization compared to the naive method.
n − 1 HstPerm rotations for [xpack]C, log2
The hybrid method requires nino
n
n + log2
n
no
rather than ni
no
n
no
3) Row-encoding-share-RaS Multiplication (GALA): The
proposed GALA framework is motivated by two observations
on the hybrid method. First, the hybrid method essentially
strikes a tradeoff between Perm and HstPerm operations,
where the number of Perms (which is the most expensive HE
operation) is proportional to the number of slots in a ciphertext.
6
A1wB1B2A3B4A2B3A4M1M2M3M4A1B2A3B4M1M2M3M4B1A2B3A4M1M2M3M4A1M1B2M2A3M3B4M4PermA1M1+A2M2B2M2+B3M3A3M3+A4M4B4M4+B1M1A1M1+A2M2B2M2+B3M3A3M3+A4M4B4M4+B1M1[x]cw0w1A1B2A3B4B1A2B3A4Step (a)M1M2M3M4Step (b)w0[x]cw1B1M1A2M2B3M3A4M4[u0]c[u1]cStep (c)[x']c[x']cA1M1B2M2A3M3B4M4B1M1A2M2B3M3A4M4Step (d)[u0]c[u1]cRaSPermAddA1M1+A2M2B2M2+B3M3A3M3+A4M4B4M4+B1M1A1M1+A3M3+A2M2+A4M4B2M2+B3M3+B4M4+B1M1A3M3+A1M1+A4M4+A2M2B2M2+B3M3+B4M4+B1M1is rotated by one position, and then added with [u0]C to get
one ciphertext, whose entries are the partial sum of the ﬁrst
and second elements of wx.
Till now, the natural next step is to conduct log2
RaS
iterations to get a ﬁnal ciphertext whose ﬁrst no entries are
the no elements of wx, i.e., the approach used by the hybrid
method [44], [38]. With GALA, we propose to eliminate
the log2
time-consuming RaS iterations by integrating it
with the generation of shares for the GC-based nonlinear
computing.
ni
no
ni
no
As introduced in the hybrid method [44], [38], in order to
do the GC based nonlinear computing, the encrypted linear
output is shared as follows: (1) the server generates a random
vector; (2) the server subtracts the random vector from the
ciphertext (the encrypted linear output); (3) the subtracted
ciphertext is sent to the client, which subsequently decrypts
it and obtains its share.
Here we let the server encode a similar random vector
and subtract it from the ciphertext obtained in step (c) of
Figure 5. The subtracted ciphertext is sent to the client, which
decrypts ciphertext, and then applies log2
RaS iterations on
the plaintext, as illustrated in step (d) of Figure 5. Similarly,
the server gets its share by log2
plaintext RaS iterations
on its encoded random vector. Hence, in GALA, the server
replaces the ciphertext RaS operations by much faster plaintext
RaS operations. This signiﬁcantly improves the computation
efﬁciency.
ni
no
ni
no
ni
Furthermore,
the client packs n
ni
in order to make use of all slots in a
input x to form a packed
ciphertext,
encoded weight
vector [xpack]C. Then the server multiplies n
vectors with [xpack]C by one ScMult operation. As a re-
sult, the server obtains nino
n multiplied ciphertext, which are
respectively rotated to enable the elementwise sum, ﬁnally
producing a single ciphertext that has n
to-be-accumulated
no
blocks. Without any further HE RaS iterations, the server then
starts to encode the random vector for the share generation.
The only extra computation is the plaintext RaS iteration(s) at
both the client and server, which is much faster compared to
the ones in HE domain.
As a result, GALA needs nino
n ScMult operations, ( nino
n −
n − 1) Add operations. It yields
1) Perm operations, and ( nino
one output ciphertext, and makes efﬁcient utilization of ci-
phertext slots. Table II compares the complexity among the
naive method, the hybrid method (i.e., GAZELLE) and the
proposed row-encoding-share-RaS matrix-vector multiplica-
tion (GALA). We can see that the proposed method completely
eliminates the HstPerm operations and signiﬁcantly reduces the
Perm operations.
TABLE II.
COMPLEXITY COMPARISON OF THREE METHODS.
Method
Naive
GAZELLE
GALA
# Perm
# HstPerm
# ScMult
# Add
no log2 ni
log2
n − 1
nino
n
no
0
nino
n − 1
0
no
nino
n
nino
n
log2
no log2 ni
+ nino
n
no
n − 1
nino
n − 1
Fig. 5. Row-encoding-share-RaS multiplication.
This is not desired as we prefer a large n to pack more data for
efﬁcient SIMD HE. GALA aims to make the number of Perm
operations disproportional to the number of slots and eliminate
all HstPerm operations on the input ciphertext.
n
no
The second observation is the log2
RaS operations.
We discover that this is actually unnecessary. Speciﬁcally, the
unique feature in the HE-GC neural network framework is that
the resultant single ciphertext from linear computing is shared
between the client and server, to be the input for the nonlinear
computing in the next phase. As the shares are in plaintext,
we propose to transfer the ﬁnal log2
RaS operations in
the HE domain to log2
RaS operations in plaintext. This
signiﬁcantly reduces expensive Perm operations. For example,
multiplying a 16×128 matrix with a length-128 vector by our
proposed scheme shows about 19× speedup compared with
the hybrid method [38] on a commodity machine (see detailed
benchmarks in Sec. IV).
n
no
n
no
Figure 5 illustrates GALA’s matrix-vector calculation. The
server ﬁrst conducts the row-wise weight matrix encoding
which encodes w into no plaintext vectors in a diagonal
manner, as shown in step (a) in Figure 5. Compared with the
hybrid method, the row-wise weight matrix encoding of GALA
enables the server to directly multiply wi and [x]C, eliminating
the Perm operations on [x]C in step (b). Furthermore, the
encoding also beneﬁts the noise management in the resultant
to-be-shared ciphertext as to be analyzed in Sec. III-C.
As a result,
the server gets no multiplied ciphertext,
{[ui]C}, such that the ﬁrst entry of [ui]C is a partial sum
of the i-th element of the matrix-vector multiplication wx.
For example, in step (b) of Figure 5, the ﬁrst element A1M1
in [u0]C is a partial sum of the ﬁrst element of wx (i.e.,
A1M1 + A2M2 + A3M3 + A4M4), and the ﬁrst element
in [u1]C is a partial sum of the 2nd element of wx (i.e.,
B1M1 + B2M2 + B3M3 + B4M4). Then, the server conducts
rotations on each [ui]C, with totally (no − 1) Perm operations
excluding the trivial rotation by zero, to make the ﬁrst entry
of [ui]C to be a partial sum of the ﬁrst element of wx. Next,
the server adds all of the rotated [ui]C to obtain a single
ciphertext whose entries are repeatedly a partial sum of the
elements of wx. For example, in step (c) of Figure 5, [u1]C
7
A1wB1B2A3B4A2B3A4M1M2M3M4A1B2A3B4M1M2M3M4B1A2B3A4M1M2M3M4A1M1B2M2A3M3B4M4B1M1A2M2B3M3A4M4B1M1A2M2B3M3A4M4A1M1+A2M2B2M2+B3M3A3M3+A4M4B4M4+B1M1A1M1+A2M2-S1B2M2+B3M3-S2A3M3+A4M4-S3B4M4+B1M1-S4A1M1+A2M2+A3M3+A4M4-(S1+S3)B2M2+B3M3+B4M4+B1M1-(S2+S4)Plaintext RaS[x]cA1B2A3B4B1A2B3A4A4w0w1Step (a)[x]c[x]cw0w1[u0]c[u1]cStep (b)A1M1B2M2A3M3B4M4B1M1A2M2B3M3A4M4[u0]c[u1]cPermStep (c)Step (d)Fig. 6. SISO convolution.
B. Kernel Grouping Based Convolution
Fig. 7. MIMO convolution.
In this subsection, we introduce GALA’s optimization for
convolution. Similar to the discussion on the matrix-vector
multiplication, we ﬁrst begin with the basic convolution for
the Single Input Single Output (SISO), then go through the
state-of-the-art scheme for the Multiple Input Multiple Output
(MIMO) (i.e., the GAZELLE framework [38]). Finally we
elaborate GALA’s ﬁrst-Add-second-Perm (kernel grouping)
scheme that achieves more efﬁcient convolution computation.
We assume the server has co plaintext kernels with a size of
kw×kh×ci and the client sends to the server the encrypted data
in the size of uw × uh with ci channels. The server needs to
homomorphically convolve the encrypted data from the client
with its plaintext kernels to produce the encrypted output.
1) Basic SISO convolution: SISO is a special case of MIMO
where ci = co = 1. In this case, the encrypted data from
the client has a size of uw × uh with one channel (i.e., a 2D
image) and there is only one kernel with size kw × kh (i.e.,
a 2D ﬁlter) at the server. The SISO convolution is illustrated
by an example in Figure 6 where [x]C is the encrypted data
from the client and K is the plaintext kernel at the server.
The process of convolution can be visualized as placing the
kernel K at different locations of the input data [x]C. At each
location, a sum of an element-wise product between the kernel
and corresponding data values within the kernel window is
computed. For example, in Figure 6, the ﬁrst value of the
convolution between [x]C and kernel K is (M1F5 + M2F6
+ M4F8 + M5F9). It is obtained by ﬁrst placing the center
of K, i.e., F5, at M1 and then calculating the element-wise
product between K and the part of [x]C that is within K’s
kernel window (i.e., M1, M2, M4 and M5). The ﬁnal result is
the sum of the element-wise product. The rest of convolution
values are calculated similarly by placing F5 at M2 to M9.
We now elaborate the convolution by an example when
F5 is placed at M5 (i.e., the central element of [x]C). In this
example, the kernel size is kwkh = 9. The convolution is
derived by summing the element-wise product between the 9
values in K and the corresponding 9 values around M5. This
can be achieved by rotating [x]C in a raster scan fashion [38].
Speciﬁcally, [x]C is converted to a vector by concatenating
is rotated by (kwkh − 1) rounds, with
all rows. Then,
half of them in the forward direction and the other half in
the backward direction. We denote πj as the rotation by j
positions, where a positive sign of j indicates the forward
direction and negative the backward direction, as shown in
step (a) of Figure 6.
it
The convolution is obtained by (1) forming the kernel
coefﬁcients according to the partial sum at the corresponding
location as shown in step (b) of Figure 6, (2) scaling the 9
rotated πj with the corresponding kernel coefﬁcients, and (3)
summing up all scaled πj (see step (c)).
The rotation for [x]C is completed by HstPerm5. The scal-
ing is done by ScMult and the summation is achieved by Add.
Therefore, the SISO convolution requires a total of (kwkh−1)
HstPerm operations (excluding the trivial rotation by zero),
kwkh ScMult operations and (kwkh − 1) Add operations. The
output is one ciphertext6 which contains the convolution result.
based MIMO convolution
2) Output Rotation
(GAZELLE): We now consider
the more general case,
i.e., MIMO, where ci or co is not one. The naive approach
is to directly apply SISO convolution by ﬁrst encrypting the
input channels into ci ciphertext, {[xi]C}. Each of the
ci
co kernels includes ci ﬁlters. Each [xi]C is convolved with
one of the ci ﬁlters by SISO and the ﬁnal convolution is
obtained by summing up all of the ci SISO convolutions.
the naive approach requires ci(kwkh − 1)
As a result,
HstPerm operations (for ci input channels), cicokwkh ScMult
operations and co(cikwkh − 1) Add operations. There are co
output ciphertext.
Given the number of slots n in a ciphertext is usually larger
than the channel size uwuh, the ciphertext utilization (i.e., the
meaningful slots that output desired results) in the co output
ciphertext is low.
In order to improve the ciphertext utilization and com-
putation efﬁciency for MIMO convolution, the state-of-the-
art method (i.e.,
the output rotation [38]) ﬁrst packs cn
channels of input data into one ciphertext, which results in ci
cn
input ciphertext (see Figure 7 where the four input channels
form two ciphertext, each of which includes two channels).
Meanwhile, the co kernels are viewed as a co × ci kernel
block and each row of the block includes ci 2D ﬁlters for one
kernel. Then the MIMO convolution is viewed as a matrix-
vector multiplication where the element-wise multiplication is
replaced by convolution. As each ciphertext holds cn channels,
the kernel block is divided into coci
blocks (see step (a) in
c2
n
Figure 7, where the kernel block is divided into K1 to K4).
Next, each divided block is diagonally encoded into cn
vectors such that the ﬁrst ﬁlters in all vectors are in the ﬁrst