AEs detection methods, such as adversarial training, defen-
sive distillation, and input transformation [20]. Because our
queried samples contain no adversarial perturbations. In addi-
tion, we use mainstream methods (e.g., PGD [37]) to generate
AEs. Although they are likely to be detected by defensive
methods like [20], it is not the concern of this study.
One possible defense is to measure the redundancy of
queries from one client, just alike DRMI. Generally, the
queries of DRMI have a much smaller MI value compared to
the normal samples of the same number, since normal data
have relatively more repetitions. However, this method needs
to count many queries and establish a distribution of MI val-
ues. In our test, the defender needs to have more than 100
times malicious queries for detection. It inevitably brings
huge computational cost. Additionally, this defense becomes
more infeasible in front of distributed queryings.
USENIX Association
30th USENIX Security Symposium    1915
8 Conclusion
This paper proposes a novel dataset reduction technology
based on mutual information DRMI, which can be used in
black-box attacks. With this approach, we can accurately mea-
sure the overall quality of dataset, identifying redundancies
and repetitions therein. Compared with other three techniques,
it proves that our approach achieves the best performance in
the selection of representative and distinct data for DNN train-
ing. Moreover, we apply DRMI to reduce queries in model
extraction and adversarial attacks. The results show a superior
ability of DRMI in data reduction while maintaining a high
model accuracy and transferability of adversarial examples.
Acknowledgement
We thank our shepherd David Wagner for his valuable guid-
ance and assistance and all the anonymous reviewers for
their constructive feedback. The authors are supported in
part by the National Key Research and Development Pro-
gram of China under Grant No.2020AAA0107800, NSFC
U1836211, NSFC 61902395, Beijing Natural Science Foun-
dation (No.JQ18011), National Top-notch Youth Talents Pro-
gram of China, Youth Innovation Promotion Association CAS,
Beijing Academy of Artiﬁcial Intelligence (BAAI), CCF-
Tencent Open Fund, and a research grant from Huawei.
References
[1] USPS
dataset.
https://www.kaggle.com/
bistaumanga/usps-dataset.
[2] Mutual information. https://en.wikipedia.org/
wiki/Mutual_information, 2019.
[3] Pytorch. https://pytorch.org/, 2020.
[4] Welcome to foolbox native.
https://foolbox.
readthedocs.io/en/latest/, 2020.
[5] Naveed Akhtar and Ajmal S. Mian. Threat of adversarial
attacks on deep learning in computer vision: A survey.
IEEE Access, 6:14410–14430, 2018.
[6] Maksym Andriushchenko, Francesco Croce, Nicolas
Flammarion, and Matthias Hein. Square attack: a query-
efﬁcient black-box adversarial attack via random search.
CoRR, abs/1912.00049, 2019.
[9] Nicholas Carlini and David A. Wagner. Towards evalu-
ating the robustness of neural networks. In 2017 IEEE
Symposium on Security and Privacy, SP, San Jose, USA,
pages 39–57.
[10] Yuxuan Chen, Xuejing Yuan, Jiangshan Zhang, Yue
Zhao, Shengzhi Zhang, Kai Chen, and XiaoFeng Wang.
Devil’s whisper: A general approach for physical ad-
versarial attacks against commercial black-box speech
recognition devices. In 29th USENIX Security Sympo-
sium, August 12-14, 2020, pages 2667–2684.
[11] Shuyu Cheng, Yinpeng Dong, Tianyu Pang, Hang Su,
and Jun Zhu. Improving black-box adversarial attacks
with a transfer-based prior. In NeurIPS 19, Vancouver,
Canada, pages 10932–10942, 2019.
[12] Kashyap Chitta, Jose M. Alvarez, Elmar Haussmann,
and Clement Farabet. Training data distribution search
with ensemble active learning, 2019.
[13] V. Chouvatut, W. Jindaluang, and E. Boonchieng. Train-
ing set size reduction in large dataset problems. In 2015
International Computer Science and Engineering Con-
ference (ICSEC), pages 1–5, Nov 2015.
[14] T.M. Cover and J.A. Thomas. Elements of informa-
tion theory. Wiley series in telecommunications. Wiley,
1991.
[15] Terrance DeVries and Graham W. Taylor. Dataset aug-
mentation in feature space. In 5th International Confer-
ence on Learning Representations, ICLR 2017, Toulon,
France, Workshop Track Proceedings.
[16] Xiaoning Du, Xiaofei Xie, Yi Li, Lei Ma, Yang Liu, and
Jianjun Zhao. Deepstellar: Model-based quantitative
analysis of stateful deep learning systems. In 27th ACM
Joint Meeting on ESES/FSE, New York, NY, USA, 2019.
[17] Steven Eschrich, Jingwei Ke, Lawrence O. Hall, and
Dmitry B. Goldgof. Fast accurate fuzzy clustering
IEEE Trans. Fuzzy Systems,
through data reduction.
11(2):262–270, 2003.
[18] Karan Ganju, Qi Wang, Wei Yang, Carl A. Gunter, and
Nikita Borisov. Property inference attacks on fully con-
nected neural networks using permutation invariant rep-
resentations. In 2018 ACM SIGSAC Conference on CCS,
pages 619–633, Oct. 2018.
[7] Arjun Nitin Bhagoji, Warren He, Bo Li, and Dawn Song.
Exploring the space of black-box attacks on deep neural
networks. CoRR, abs/1712.09491, 2017.
[19] Ian J. Goodfellow, Jonathon Shlens, and Christian
Szegedy. Explaining and harnessing adversarial exam-
ples. In 3rd ICLR, San Diego, CA, USA, 2015.
[8] Thomas Brunner, Frederik Diehl, Michael Truong-Le,
and Alois Knoll. Guessing Smart: Biased Sampling
for Efﬁcient Black-Box Adversarial Attacks. CoRR,
abs/1812.09803, 2018.
[20] Chuan Guo, Mayank Rana, Moustapha Cissé, and Lau-
rens van der Maaten. Countering adversarial images
using input transformations. In 6th ICLR, Vancouver,
BC, Canada, 2018.
1916    30th USENIX Security Symposium
USENIX Association
[21] Yiwen Guo, Ziang Yan, and Changshui Zhang. Sub-
space Attack: Exploiting Promising Subspaces for
Query-Efﬁcient Black-box Attacks.
In H. Wallach,
H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox,
and R. Garnett, editors, Advances in Neural Information
Processing Systems 32, pages 3825–3834. 2019.
[22] Jiawei Han, Micheline Kamber, and Jian Pei. Data
Mining: Concepts and Techniques. Elsevier, 2012.
[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. Deep residual learning for image recognition. In
The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2016.
[24] Yingzhe He, Guozhu Meng, Kai Chen, Xingbo Hu, and
Jinwen He. Towards Security Threats of Deep Learning
Systems: A Survey. IEEE Transactions on Software
Engineering (TSE), pages 1–28, 2020.
[25] Andrew Ilyas, Logan Engstrom, Anish Athalye, and
Jessy Lin. Black-box adversarial attacks with limited
In 35th International Con-
queries and information.
ference on Machine Learning, ICML 2018, Stockholm,
Sweden, pages 2142–2151, 2018.
[26] Matthew Jagielski, Nicholas Carlini, David Berthelot,
Alex Kurakin, and Nicolas Papernot. High accuracy
and high ﬁdelity extraction of neural networks. In 29th
USENIX Security Symposium (USENIX Security 20),
pages 1345–1362. USENIX Association, August 2020.
[27] Matthew Jagielski, Alina Oprea, Battista Biggio, Chang
Liu, Cristina Nita-Rotaru, and Bo Li. Manipulating ma-
chine learning: Poisoning attacks and countermeasures
for regression learning. In 2018 IEEE Symposium on
Security and Privacy, San Francisco, USA, pages 19–35.
[28] Uyeong Jang, Xi Wu, and Somesh Jha. Objective met-
rics and gradient descent algorithms for adversarial ex-
amples in machine learning. In 33rd Annual Computer
Security Applications Conference, Orlando, FL, USA,
pages 262–277.
[29] Mika Juuti, Sebastian Szyller, Samuel Marchal, and
N. Asokan. PRADA: protecting against DNN model
stealing attacks. In IEEE European Symposium on Se-
curity and Privacy, EuroS&P 2019, Stockholm, Sweden,
pages 512–527.
[30] Richard M. Karp. Reducibility among combinato-
rial problems. In Proceedings of a symposium on the
Complexity of Computer Computations, New York, USA,
pages 85–103, 1972.
[32] Alex Krizhevsky. The CIFAR-10 dataset. https://
www.cs.toronto.edu/~kriz/cifar.html/.
[33] Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio.
Adversarial examples in the physical world. In 5th Inter-
national Conference on Learning Representations, ICLR
2017, Toulon, France, Workshop Track Proceedings.
[34] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Pro-
ceedings of the IEEE, 86(11):2278–2324, Nov 1998.
[35] Yann LeCun. The MNIST database of handwritten
digits. http://yann.lecun.com/exdb/mnist/.
[36] Xiang Ling, Shouling Ji, Jiaxu Zou, Jiannan Wang,
Chunming Wu, Bo Li, and Ting Wang. DEEPSEC:
A uniform platform for security analysis of deep learn-
ing model. In 2019 IEEE Symposium on Security and
Privacy, SP, San Francisco, USA, pages 673–690.
[37] Aleksander Madry, Aleksandar Makelov, Ludwig
Schmidt, Dimitris Tsipras, and Adrian Vladu. To-
wards deep learning models resistant to adversarial
attacks. In 6th International Conference on Learning
Representations, ICLR 2018, Vancouver, BC, Canada.
[38] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi,
Omar Fawzi, and Pascal Frossard. Universal adversarial
perturbations. In 2017 IEEE Conference on Computer
Vision and Pattern Recognition, CVPR 2017, Honolulu,
HI, USA, pages 86–94.
[39] Luis Muñoz-González, Battista Biggio, Ambra Demon-
tis, Andrea Paudice, Vasin Wongrassamee, Emil C.
Lupu, and Fabio Roli. Towards poisoning of deep
learning algorithms with back-gradient optimization. In
AISec@CCS 2017, Dallas, TX, USA, pages 27–38.
[40] Seong Joon Oh, Max Augustin, Mario Fritz, and Bernt
Schiele. Towards reverse-engineering black-box neural
In International Conference on Learning
networks.
Representations, 2018.
[41] Lucila Ohno-Machado, Hamish S. F. Fraser, and Alek-
sander Øhrn. Improving machine learning performance
by removing redundant cases in medical data sets. In
AMIA 1998, Lake Buena Vista, FL, USA.
[42] Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz.
Knockoff nets: Stealing functionality of black-box mod-
els. In IEEE Conference on Computer Vision and Pat-
tern Recognition, CVPR 2019, Long Beach, CA, USA,
pages 4954–4963.
[31] Angelos Katharopoulos and François Fleuret. Not all
samples are created equal: Deep learning with impor-
tance sampling. In 35th International Conference on
Machine Learning, ICML 2018, Stockholm, Sweden.
[43] Stefanos Ougiaroglou and Georgios Evangelidis. Ef-
ﬁcient dataset size reduction by ﬁnding homogeneous
clusters. In Balkan Conference in Informatics, BCI 2012,
Novi Sad, Serbia.
USENIX Association
30th USENIX Security Symposium    1917
[44] Nicolas Papernot, Patrick D. McDaniel, and Ian J. Good-
fellow. Transferability in machine learning: from phe-
nomena to black-box attacks using adversarial samples.
CoRR, abs/1605.07277, 2016.
[55] Fnu Suya, Jianfeng Chi, David Evans, and Yuan Tian.
Hybrid Batch Attacks: Finding Black-box Adversarial
Examples with Limited Queries. In 29th USENIX Secu-
rity Symposium, 2020.
[45] Nicolas Papernot, Patrick D. McDaniel, Ian J. Good-
fellow, Somesh Jha, Z. Berkay Celik, and Ananthram
Swami. Practical black-box attacks against machine
learning. In ACM AsiaCCS 2017, pages 506–519.
[46] Haekyu Park, Fred Hohman, and Duen Horng Chau.
Neuraldivergence: Exploring and understanding neural
networks by comparing activation distributions. CoRR,
abs/1906.00332, 2019.
[47] Alexander J. Ratner, Henry R. Ehrenberg, Zeshan Hus-
sain, Jared Dunnmon, and Christopher Ré. Learning to
compose domain-speciﬁc transformations for data aug-
mentation. In NeurIPS 2017, Long Beach, USA, pages
3236–3246.
[48] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,
Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej
Karpathy, Aditya Khosla, Michael S. Bernstein, Alexan-
der C. Berg, and Fei-Fei Li. Imagenet large scale visual
recognition challenge. Int. J. Comput. Vis., 115(3):211–
252, 2015.
[49] Ali Shafahi, W. Ronny Huang, Mahyar Najibi, Octa-
vian Suciu, Christoph Studer, Tudor Dumitras, and Tom
Goldstein. Poison frogs! targeted clean-label poisoning
attacks on neural networks. In NeurIPS 2018, Montréal,
Canada, pages 6106–6116.
[50] Mahmood Sharif, Lujo Bauer, and Michael K. Reiter. On
the suitability of lp-norms for creating and preventing
adversarial examples. In CVPR Workshops 2018, Salt
Lake City, UT, USA, pages 1605–1613.
[51] Reza Shokri, Marco Stronati, Congzheng Song, and Vi-
taly Shmatikov. Membership inference attacks against
machine learning models. In 2017 IEEE Symposium on
Security and Privacy, San Jose, USA, pages 3–18.
[52] Avanti Shrikumar, Peyton Greenside, and Anshul Kun-
daje. Learning important features through propagating
activation differences. In 34th ICML, volume 70, pages
3145–3153, 2017.
[53] Satya Narayan Shukla, Anit Kumar Sahu, Devin Will-
mott, and J. Zico Kolter. Black-box adversarial attacks
with bayesian optimization. CoRR, abs/1909.13857.
[54] Congzheng Song and Vitaly Shmatikov. Overlearning
reveals sensitive attributes. In 8th ICLR, Addis Ababa,
Ethiopia, April 26-30, 2020.
[56] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
Jonathon Shlens, and Zbigniew Wojna. Rethinking the
inception architecture for computer vision. In CVPR
2016, Las Vegas, NV, USA, pages 2818–2826.
[57] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever,
Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob
Fergus. Intriguing properties of neural networks. In 2nd
ICLR, 2014, Banff, AB, Canada.
[58] Florian Tramèr, Fan Zhang, Ari Juels, Michael K. Re-
iter, and Thomas Ristenpart. Stealing machine learning
models via prediction apis. In 25th USENIX Security
Symposium, 2016, Austin, TX, USA, pages 601–618.
[59] Chun-Chen Tu, Pai-Shun Ting, Pin-Yu Chen, Sijia Liu,
Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh, and Shin-Ming
Cheng. Autozoom: Autoencoder-based zeroth order
optimization method for attacking black-box neural net-
works. In 31rd AAAI Conference on Artiﬁcial Intelli-
gence, Honolulu, Hawaii, USA, 2019, pages 742–749.
[60] Binghui Wang and Neil Zhenqiang Gong. Stealing
hyperparameters in machine learning. In 2018 IEEE
Symposium on Security and Privacy (SP), San Francisco,
California, USA, pages 36–52.
[61] Xuejing Yuan, Yuxuan Chen, Yue Zhao, Yunhui Long,
Xiaokang Liu, Kai Chen, Shengzhi Zhang, Heqing
Huang, Xiaofeng Wang, and Carl A. Gunter. Comman-
dersong: A systematic approach for practical adversarial
voice recognition. In 27th USENIX Security Symposium,
USENIX Security 2018, Baltimore, USA, pages 49–64.
[62] L.A. Zadeh. Similarity relations and fuzzy orderings.
Information Sciences, 3(2):177 – 200, 1971.
[63] Mingming Zha, Guozhu Meng, Chaoyang Lin, Zhe
Zhou, and Kai Chen. Rolma: A practical adversarial
attack against deep learning-based lpr systems. In In-
formation Security and Cryptology (Inscrypt), pages
4701–4708, Dec 2019.
[64] Yue Zhao, Hong Zhu, Ruigang Liang, Qintao Shen,
Shengzhi Zhang, and Kai Chen. Seeing isn’t believ-
ing: Towards more robust adversarial attack against real
world object detectors. In ACM CCS 2019, London, UK,
pages 1989–2004.
[65] Jian Zheng, Wei Yang, and Xiaohua Li. Training data
reduction in deep neural networks with partial mutual in-
formation based feature selection and correlation match-
ing based active learning. In IEEE ICASSP 2017, New
Orleans, LA, USA.
1918    30th USENIX Security Symposium
USENIX Association