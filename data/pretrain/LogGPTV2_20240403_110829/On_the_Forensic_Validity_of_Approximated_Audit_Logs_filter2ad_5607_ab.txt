mentally linking entities associated with each event tuple according
to the access type (see, e.g., [5, 25, 38, 56]). Specifically, in a prove-
nance graph G = (V , E) each vertex v ∈ V corresponds to a system
object such as processes, files, and sockets, v while each e ∈ E en-
codes a dependence relationship between those objects and roughly
corresponds to a single log event. These directed edges point back-
ward in time, denoting a provenance (historical) relation between
events. An example provenance graph is shown in Figure 2. Here,
a process named /bin/bash represented by vertex A reads a file
named /etc/rc represented by vertex C. The edge between ver-
tices A and C is represented by ECA−2read where 2 is a timestamp
while read denotes a read event type from C to A vertices. Multiple
edges between two vertices denote the same events occurring at dif-
ferent times, e.g., edges ECA−2 and ECA−3 denote two read events.
Finally, at the time of analysis, vertices vertices can be either in a
live or terminated state. In Figure 2, vertices A, D, and E are live
processes while vertex F is a terminated process.
Provenance graphs allow investigators to perform forensic anal-
ysis. Investigators can perform tracing queries on the provenance
graph to figure out the root-cause and ramifications of a cyber
attack. We formally define these tracing queries as follows:
Definition 2.1. Backward Trace: A backward trace of edge e is
the subgraph of G reachable from e (or equivalently, the destination
vertex of e).
Definition 2.2. Forward Trace: A forward trace of edge e is the
subgraph of G reachable from e in the reverse graph of G (or equiv-
alently, the source vertex of e).
A backward trace enables an analyst to identify the root cause(s)
of a particular event, e.g., the point of entry of an intruder into the
system. In Figure 2, the backward trace of the event (edge) EAD−6
with event type Fork will traverse edges EBA−4, EBA−5, ECA−2, ECA−3,
and EEC−1, identifying process vertex E (/usr/bin/nano) as the
2.1 Audit Log Approximation
Although audit frameworks are extremely helpful to threat investi-
gation, their use is impeded by the sheer volume of logs generated.
Log overheads vary depending on the load of the machine, but have
been reported to be anywhere between 3 GB [43] and 33 GB [49] per
day for web servers and around 1 GB per day for workstations [43].
As a result, storing and analyzing these logs is often infeasible in
practice — log data is often purged just a few days after its creation
[70], and when it is retained for longer periods, simple trace queries
may take days to return [45].
To combat the limitations of pervasive system auditing, many
techniques for log approximation have been proposed. These ap-
proximation methods analyze the structure and semantics of the
provenance graph to identify components (i.e., log events) that are
unlikely to be of use to an analyst and can therefore be removed.
While a variety of approaches to log approximation have been
proposed based on filtering policies [4, 6], taint analysis [8, 51],
templatization [24, 28, 71], or simple compression [75, 76], we de-
scribe at length three influential exemplar approaches below:
Garbage Collection (GC). First proposed by Lee et al. [43],
garbage collection is based on the observation that subgraphs that
exclusively describe dead system entities do not affect the present
state of the system and can therefore be removed. Consider a process
that generates a temporary file, then deletes it. If no other process
accesses that temporary file, all log events associated with that
file can be removed from the graph. A visualization of garbage
collection is given in Figure 3 (a). Garbage collection has since been
incorporated into other log analysis systems, e.g., [48, 49, 51].
Causality-Preserving Reduction (CPR). Introduced by Xu et
al. [77], CPR observes that many log events are redundant because
they do not denote a new information flow. Consider a process that
writes to a file twice. If the process did not read from any other ob-
ject between the two writes, we can remove the log event describing
the second write because the process state did not change between
writes. Note that, in actuality, the data buffer may have been com-
pletely different in each write event; however, because log events do
not include data buffers, the graph must conservatively assume that
all process state is transferred during each information flow event,
thus the second write is completely redundant. A visualization of
CPR is given in Figure 3 (b). CPR has been adopted or extended by
subsequent log analysis systems, including [30, 36, 45, 71].
Dependence-Preserving Reduction (DPR) Building on the CPR
concept, Hossain et al. propose that preserving causality is unnec-
essary so long as the provenance graph returns the correct system
entities to forward and backward trace queries [36]. Consider a
vertex for which there are two causal paths back to its root cause;
this represents a potential redundancy and one of the two edges
DCEBAA: /bin/bash B: /etc/bashrc C: /etc/rcD: /bin/sed E: /usr/bin/nanoF: /bin/python3G: 10.10.0.3H: .permissionEBA-5EBA-4EEC-1ECA-2ECA-3EAD-6ReadReadReadReadForkWriteEEB-7WriteFECF-8ReadGEGF-9ReadHEFH-10DeleteACSAC 2020, December 7–11, 2020, Austin, USA
Noor Michael, Jaron Mink, Jason Liu, Sneha Gaur, Wajih Ul Hassan, and Adam Bates
Figure 3: Visualizations of exemplar log approximation techniques. We used the provenance graph example from Figure 2
to apply each exemplar log approximation technique. Graph edges marked in dotted lines and vertices which are grayed out
represent events that are filtered by each technique. (a) Provenance graph after applying LogGC technique [43]. (b) Provenance
graph after applying CPR technique [77]. (c) Provenance graph after applying S-DPR technique [36].
may be deleted, provided that the edge is not necessary to pre-
serve dependency for some other node. Hossain et al. introduce
two variants of DPR, Source Dependency-Preserving Reduction
(S-DPR) in which only backward trace reachablity is preserved,
and Full Dependency-Preserving Reduction (F-DPR) in which both
backward and forward trace reachability are preserved. A visual-
ization of S-DPR is given in Figure 3 (c). DPR was incorporated and
discussed in recent work [35] and is also significant in boasting
among the highest reduction rates in the literature.
Limitations of Prior Work. While the performance characteristics
of these approaches were effectively evaluated in prior work (i.e.,
storage overheads), the security characteristics of the approximated
logs have proven more difficult to quantify. When approximating
the log, problems of graph reachability and interpretability may
arise. Further, if events are merged or deleted, then interpreting
the graph becomes more difficult. Worse yet, key details of the
attack behaviors may be lost in ways that were unanticipated by
the designers of the approximation technique.
Without exception [31, 36, 43, 71, 77], prior work was evalu-
ated exclusively through attack scenario case studies in which a
forensic analyst needs to answer a specific query during investi-
gation. While illustrative of the potential benefits of a technique,
this approach is ultimately anecdotal; it may be that analysts need
to answer a broader range of queries that were not considered in
the case study, or that the semantics of the specific attack scenario
did not adequately capture the forensic utility of the approximated
log. Worse yet, under this evaluation the efficacy of approximation
methods is reduced to a binary ’yes’ or ’no’ depending on whether
or not the analyst is able to achieve a hand-selected forensic goal.
In the following section, we present a set of nuanced and descrip-
tive methods for characterizing the security of an approximation
technique. These metrics provide a continuous (i.e., non-binary)
value for characterizing forensic validity.
3 FORENSIC VALIDITY METRICS
To better characterize the security utility of log approximation
techniques, we formalize three complementary forensic validity
metrics that can be used to evaluate an approximated log. Each
metric encodes the amount of preserved evidence (i.e., log events)
under a threat model that an analyst may encounter during the
course of an investigation. In order to provide generalizable state-
ments on the utility of approximation algorithms, we make no
assumptions on the goals of the analysis or the events needed to
return a specific query, but rather the number of events that remain
intact. The key insight behind these metrics is that, rather than
anecdotally demonstrating value in a specific attack behavior, util-
ity should be measured as a property of the approximated graph or
log. We propose three such metrics: Lossless, Causality-Preserving,
and Attack-Preserving.
3.1 Lossless Forensics
This metric assumes the adversary may not abide by system
level abstractions for communication and coordination of
malicious processes. The adversary may use avenues such as
obscure system-layer timing channels to exfiltrate data from a col-
luding process (e.g., [13, 69]). In such scenarios, the audit log may
not explicitly contain all causal relationships but may encode in-
ferrable implicit relations. For instance, the frequency and timing
of system calls used to communicate within a covert channel may
also be used by the system to infer whether a covert channel, and
thus a causal relationship, exists. Unfortunately, such channels have
been found in nearly every sector of computer architecture and are
continually introduced as the hardware beneath operating systems
evolves [23]; therefore, is it unreasonable to predict which events
may contain implicit relations and thus we conservatively assume
all events may encode such dependencies. Lossless Forensics as-
sumes every dropped event potentially destroys implicit causal
relations and thus is defined as follows.
Definition 3.1. Lossless Forensics: Given G = (V , E) and approx-
imation G′ = (V ′, E′) in which V ′ ⊆ V and E′ ⊆ E, a lossless log
implies that E′ = E. Distance from losslessness can be measured as
a continuous variable using the formula 1 − |E\E′|
|E |
.
(b) Causality Preserving Reduction(c) Source DependencePreserving Reduction A: /bin/bash,   B: /etc/bashrc,  C: /etc/rc,  D: /bin/sed,   E: /usr/bin/nano,  F: /bin/python3,  G: 10.10.0.3,  H: .permissionDCEBAFGH(a) Garbage collection DCEBAFGHDCEBAFGHOn the Forensic Validity of Approximated Audit Logs
ACSAC 2020, December 7–11, 2020, Austin, USA
Figure 4: Overview of the LogApprox architecture and workflow. File activity and audit logs are analyzed by the Regex Generation
routine, which generates bounded regular expressions that each describes a specific semantic behavior of the benign process
(e.g., “loads shared objects”). These regex are then applied to the original log and matching entries are pruned. Causality is
preserved in the associated provenance graph by connecting associated edges to the newly-collapsed node.
In other words, losslessness can be measured by the fraction of
edges in E that are missing from E′. Note that it is not necessary to
directly test the completeness of V , as each edge is a log event and
the associated system entities in V are extracted from these tuples.
3.2 Causality-Preserving Forensics
This metric assumes that the adversary abides by system level
abstractions for communications and coordination of mali-
cious processes. As only explicit information flows are utilized
by the adversary, one can be sure that the complete log faithfully
contains an explicit description of all causal relationships. Within
such a model, we used the well-studied subject of information flow
(IF) to describe causal relationships and thus encode Xu et al.’s
Causality-Preserving Reduction technique [77] as our metric. In a
log that satisfies causality-preserving forensics, all events that en-
code new causal relationships are retained, whereas edges that are
causally redundant are discarded. We define the metric as follows.
Definition 3.2. Causality-Preserving Forensics: Information flows
from G are preserved in the approximated graph G′. An information
flow is defined by the existence of a path between two edges in G.
The following describes two situations where two edges describe
the same information flow:
• Two read edges e1, e2 describe the same information flow if they
have the same endpoints process p and file f , and no write to f
or read from p to a file f ′ (cid:44) f occurred between e1 and e2.
• Two write edges e1, e2 describe the same information flow if they
have the same endpoints process p and file f , read from p to a
file occurred between e1 and e2.
Let EI F be the set of edges matching this definition of distinct
information flow in G. G′ preserves causality iff EI F ∈ E′. Distance
from causality preservation can be measured using the formula
1 − EI F \(E′∩EI F )
.
EI F
3.3 Attack-Preserving Forensics
Under Attack-Preserving Forensics, the adversary’s system-
level actions deviate from benign behavior. With few excep-
tions, past approximation techniques apply reduction uniformly to
all log events, regardless of whether or not the events describe ma-
licious or benign activity. This is because, understandably, it is diffi-
cult to predict ahead of time which events are attack-relevant; how-
ever, in many cases it is safe to make assertions about predictable
benign process behaviors. In a similar fashion to host anomaly de-
tection, a suite of approximation reduction techniques attempt to
distinguish benign from anomalous behavior and selectively reduce
based on that classification. Building on causality-preservation,
our final metric captures an approximation technique’s ability to
preserve attack-relevant causal relations without penalizing the
reduction of attack-irrelevant log events.
Definition 3.3. Attack-Preserving Forensics: Given a causality-
preserving approximated provenance graph GI F , let GB ⊆ GI F be
the subgraph of naturally occurring benign system behavior and
GA = GI F \ GB be the subgraph describing an attack campaign
A. The approximated graph G′ ⊆ GI F that contains G′
A ⊆ G′ is
said to satisfy attack-preservation if G′
A is a causality-preserving
approximation of GA. Distance from attack-preservation can be
measured using the formula 1 − |EA\E′
A |
, where EA and E′
A are the
|EA |
edge sets of GA and G′
A, respectively.
We make the following observations. First, this metric uses the
causality-preserving approximated graph, not the lossless prove-
nance graph, as its baseline. Additionally, this metric not only disre-
gards unrelated benign activity, but exclusively considers the causal
edge set that uniquely describes the attack campaign A. In other
words, events that appear in the attack path of A that also appear in
GB are not considered in measurement. The intuition behind this
approach is that edges that are shared between benign and behav-
iors hold little forensic value in investigations; they do not uniquely
signify malicious activity, and hence are candidates for approxi-