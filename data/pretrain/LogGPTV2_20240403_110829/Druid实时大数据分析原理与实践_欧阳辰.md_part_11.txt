:verifybin/verify-java
vi conf/supervise/master-with-query.conf
第4章
运行命令如下。
配置内容如下。
数据节点配置文件如下。
运行命令如下。
配置内容如下。
管理节点和查询节点配置文件如下。
安装与配置
被关闭。
备注：lp加数字表示为服务关闭顺序的权重，默认值为50，数值越大越先
等，同时也为将来的扩容做好准备。
存储多少个数据源，以及每个数据源分别支撑加载多长时间的数据供分析查询
备注：在实际应用部署中需要结合具体的业务场景并提前做好量的预估，比如
9
---
## Page 92
druid.storage.type=hdfs
druid.extensions.loadList=["druid-hdfs-storage"]
druid.metadata.storage.connector.password=${PASSWORD}
druid.metadata.storage.connector.user=${USER}
druid.metadata.storage.connector.connectURI=jdbc:postgresql://${IP:PORT}/druid
druid.metadata,storage.type=postgresql
druid.extensions.loadList=["postgresql-metadata-storage"]
#For PostgreSQL:
druid.metadata.storage.connector.password=${PASSwoRD}
druid.metadata.storage.connector.user=${USER}
druid.metadata.storage,type=mysql
druid.extensions,loadList=["mysql-metadata-storage"]
#For MySQL:
druid.zk.paths.base=/druid
druid.zk.service.host=${Zookeeper集群地址}
4.4.1
进行相应的优化调整）。
4.4
8
（3）Deep Storage（推荐采用HDFS）
（2）Metadata Storage（推荐使用MySQL或者PostgreSQL）
(1）Zookeeper
配置文件为conf/druid/_common/common.runtime.properties。
本节主要涉及集群的基础依赖配置和相关节点的调优选项（需要根据实际的硬件配置
或者
基本配置
基础依赖配置
节点状态信息。
说明：在HA的Druid集群配置中，所有的Druid节点依赖同一个zk集群同步
Druid实时大数据分析原理与实践
---
## Page 93
druid.indexer.logs.directory=hdfs://namenode.example.com:9000/druid/indexing-logs
druid.indexer,logs.type=file
druid.storage.storageDirectory=hdfs://namenode.example.com:9000/druid/segments
4.4.3
4.4.2
第4章
.druid.processing.buffer.sizeBytes
druid.cache.sizeInBytes
.druid.server.http.numThreads
·JVM内存使用-Xmx和-Xms
broker相关配置：
·druid.server.priority（定义层对应的查询优先级）
·druid.server.tier（默认为_default_tier，自定义名称可以对数据存储做分层处理）
.druid.worker.capacity (middleManager)
.druid.server.maxSize 和 druid.segmentCache.locations (historical)
druid.processing.numThreads
druid.processing.buffer.sizeBytes
druid.server.http.numThreads
·JVM内存使用-Xmx和-Xms
historical与middleManager配置：
druid.query.groupBy.maxResults
druid.query.groupBy.maxIntermediateRows
数据节点配置调优
查询节点配置调优
安装与配置
druid/_common目录下。
配置文件core-site.xml,hdfs-site.xml,yarn-site.xml,mapred-site.xml放人conf/-
MapReduce加速写人处理，因此需要将生产环境中Hadoop对应的客户端
备注：采用HDFS作为Deep Storage时，离线批量导人数据任务会利用
---
## Page 94
druid.extensions.loadList=["druid-lookups-cached-global",“druid-histogram","druid-
druid.extensions.hadoopDependenciesDir=dist/druid/hadoop-dependencies
druid.extensions.directory=dist/druid/extensions
#Extensions
和查询节点，Data机器作为数据节点，分别部署服务如下。
4.5.1节点规划
4.5
datasketches",“mysql-metadata-storage"，“druid-hdfs-storage"]
初始搭建Druid集群选取Master机器2台、Data机器3台，其中Master机器作为管理
·Data机器：64GB内存、24核CPU、1TB磁盘空间。
·Master机器：64GB内存、16核CPU、250GB磁盘空间。
这里以实际应用部署为例，机器类型分为以下两种。
详细配置说明：http://druid.io/docs/latest/configuration/index.html
（Segment）的计算时，通常一个数据文件对应一个处理线程。
·并发性能的调优更多的是通过调整相关处理的线程数来实现，查询涉及多个数据文件
·Druid中查询节点和历史节点都提供了针对查询的本地LRU缓存机制，在配置方面
配置说明如下。
. druid.query.groupBy.maxIntermediateRows
集群节点配置示例
查询节点上开启查询缓存，大集群在历史节点上开启查询缓存。
broker和historical只需要在一种节点上开启缓存即可，推荐小集群（
2.
#Monitoring
characterEncoding=UTF-8
全局log4j2.xml日志配置
安装与配置
备注：后续的章节会详细介绍Druid服务的相关指标信息和监控方式。
---
## Page 96
的工作。
4.5.2
Master机器2台，作为管理节点可相互用作HA支撑，同时又承担了部分数据查询
Master机器配置
="false">
additivity="false">
info" additivity="false">
info"additivity="false">
Druid实时大数据分析原理与实践
---
## Page 97
druid.port=8090
druid.host=${IP_ADDR}//部署机器IP地址
druid.service=druid/overlord
-Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager
2.统治节点
druid.coordinator.period=PT30S
druid.coordinator.startDelay=PT30S
druid.port=8081
druid.host=${IP_ADDR}//部署机器IP地址
druid.service=druid/coordinator
-Dfile.encoding=UTF-8
-Xms3g
1.协调节点
第4章
-Djava.io.tmpdir=var/tmp
-Dfile.encoding=UTF-8
Duser.timezone=UTC+0800
XX:+PrintGCTimeStamps
XX:+PrintGCDetails
XX:+UseConcMarkSweepGC
-XX:MaxNewSize=256m
-XX:NewSize=256m
Xmx4g
-Xms4g
-server
-Djava,io.tmpdir=var/tmp
-Duser.timezone=UTC+0800
-Xmx3g
-server
runtime.properties:
jvm.config:
runtime.properties:
jvm.config:
安装与配置
3
---
## Page 98
druid.port=8082
druid.host=$[IP_ADDR}//部署机器IP地址
3.查询节点
Curl-L-H'Content-Type: application/json'-X GET http://${OVERLORD_IP:PORT}/druid/
druid.indexer.storage.type=metadata
druid.indexer,runner.type=remote
druid.indexer.queue.startDelay=PT30S
druid.service=druid/broker
-Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager
-XX:NewSize=6g
-Xmx24g
74
-Djava.io.tmpdir=var/tmp
-Dfile.encoding=UTF-8
-Duser.timezone=UTC+0800
-XX:+PrintGCTimeStamps
-XX:+PrintGCDetails
XX:+UseConcMarkSweepGC
XX:MaxDirectMemorySize=32g
XX:MaxNewSize=6g
-Xms24g
-server
runtime.properties:
jvm.config:
indexer/v1/worker/history?count=10
查询当前统治节点任务分配方式设置如下。
curl -L -H'Content-Type: application/json’-X POST -d‘"selectStrategy":
equalDistribution方式。
后通过动态HTTPPOST请求的方式修改，默认方式为fllCapacity，推荐改为
备注：统治节点分配写入数据任务给middleManager的负载模式需要服务启动
indexer/v1/worker
Druid实时大数据分析原理与实践
---
## Page 99
4.历史节点（加载热点数据
druid.broker.balancer.type=connectionCount//查询节点请求历史节点方式，有random和
#Query config
druid.cache.sizeInBytes=60000000//JVM堆内LRU缓存大小，单位为Byte
druid.broker.cache.unCacheable=[]
druid.broker.cache.populateCache=true
druid.broker.cache.useCache=true
#Query cache
druid.processing.numThreads=15//处理线程数
druid.processing.buffer.sizeBytes=2147483647 //每个处理线程的中间结果缓存大小，单位
-Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager
-XX:MaxDirectMemorySize=16g
-XX:MaxNewSize=4g
-XX:NewSize=4g
druid.server.http.numThreads=50//HTTP请求处理线程数
druid.broker.http.readTimeout=PT5M
druid.broker.http.numConnections=20//查询节点与历史节点通信连接池大小
-Dfile.encoding=UTF-8
-Duser.timezone=UTC+0800
-XX:+PrintGCTimeStamps
-Xmx12g
#HTTP server threads
第4章
-Djava.io.tmpdir=var/tmp
-XX:+PrintGCDetails
-XX:+UseConcMarkSweepGC
-Xms12g
-server
为Byte
jvm.config:
connectionCount两种连接方式
安装与配置
15
---
## Page 100
-Xms12g
1.历史节点
4.5.3
druid.server.priority=10//自定义数据层优先级，默认值为0，值越大优先级越高，该功能
druid.server.tier=hot//自定义数据层名称，
#Tier
druid.historical.cache.populateCache=false
druid.historical.cache.useCache=false
#Query cache
druid.server.maxSize=1000000000//最大存储空间大小，该值只用作Coordinator调配
druid.segmentCache.locations=[{"path":"var/druid/segment-cache","maxSize
#Segment storage
druid.processing.numThreads=15//查询处理线程数
druid.pr0cessing.buffer.sizeBytes=1073741824//每个查询处理线程的中间结果缓存大小，
druid.server.http.numThreads=50//HTTP请求处理线程数
#HTTP server
druid.port=8083
druid.host=$[IP_ADDR}//部署机器IP地址
druid.service=druid/historical
server
据无法相互复制
jvm.config:
Data机器3台，作为数据节点负责数据处理，Shared nothing架构。
Segment加载的依据
"\：100000000000}]//Segment本地加载路径与最大存储空间大小，单位为Byte
用于冷热数据层的划分
单位为Byte
runtime.properties:
Data机器配置
threads
，默认为_default_tier，
Druid实时大数据分析原理与实践
，不同数据层的Segment数
---
## Page 101
druid.historical.cache.populatecache=false
#Query cache
druid.server.maxSize=500000000000//最大存储空间大小，该值只用作Coordinator调配
druid.segmentCache.locations=["path":"var/druid/segment-cache","maxSize
druid.processing.numThreads=23//处理线程数
druid.processing.buffer.sizeBytes=1073741824//每个处理线程的中间结果缓存大小，单位
#Segment storage
#Processing threads and buffers