rpool/db primarycache metadata local
rpool/db recordsize 16K local
Discuss: recordsize=16K
85
•Pre-fault next page: useful for sequential scans
•With compression=lz4, reasonable to expect ~3-4x pages worth of data
in a single ZFS record
Anecdotes and Recommendations:
•Performed better in most workloads vs ZFS's prefetch
•Disabling prefetch isn't necessary, tends to still be a net win
•Monitor arc cache usage
Discuss: primarycache=metadata
86
•metadata instructs ZFS's ARC to only cache metadata (e.g. dnode entries),
not page data itself
•Default: cache all data
Two different recommendations based on benchmark workloads:
•Enable primarycache=all where working set exceeds RAM
•Enable primarycache=metadata where working set fits in RAM
Discuss: primarycache=metadata
87
•metadata instructs ZFS's ARC to only cache metadata (e.g. dnode entries),
not page data itself
•Default: cache all data
•Double-caching happens
Two different recommendations based on benchmark workloads:
•Enable primarycache=all where working set exceeds RAM
•Enable primarycache=metadata where working set fits in RAM
Reasonable Default anecdote: Cap max ARC size ~15%-25%
physical RAM + ~50% RAM shared_buffers
Performance Wins
88
2-4µs/pwrite(2)!!
Performance Wins
89
Performance Wins
90
Performance Wins
91
P.S. This was observed on 10K RPM spinning rust.
ZFS Always has your back
92
•ZFS will checksum every read from disk
•A failed checksum will result in a fault and automatic data reconstruction
•Scrubs do background check of every record
•Schedule periodic scrubs
•Frequently for new and old devices
•Infrequently for devices in service between 6mo and 2.5yr
PSA: The "Compressed ARC" feature was added to catch checksum errors in RAM
Checksum errors are an early indicator of failing disks
Schedule Periodic Scrubs
93
# zpool status
pool: rpool
state: ONLINE
scan: none requested
config:
NAME STATE READ WRITE CKSUM
rpool ONLINE 0 0 0
sda1 ONLINE 0 0 0
Non-zero on
errors: No known data errors
# zpool scrub rpool
any of these
# zpool status
pool: rpool
state: ONLINE
values is bad™
scan: scrub in progress since Fri Mar 3 20:41:44 2017
753M scanned out of 819M at 151M/s, 0h0m to go
0 repaired, 91.97% done
config:
NAME STATE READ WRITE CKSUM
rpool ONLINE 0 0 0
sda1 ONLINE 0 0 0
errors: No known data errors
# zpool status
pool: rpool
state: ONLINE
scan: scrub repaired 0 in 0h0m with 0 errors on Fri Mar 3 20:41:49 2017
One dataset per database
94
•Create one ZFS dataset per database instance
•General rules of thumb:
•Use the same dataset for $PGDATA/ and pg_xlogs/
•Set a reasonable quota
•Optional: reserve space to guarantee minimal available space
Checksum errors are an early indicator of failing disks
One dataset per database
95
# zfs list
NAME USED AVAIL REFER MOUNTPOINT
rpool 819M 56.8G 96K none
rpool/db 160K 48.0G 96K /db
rpool/root 818M 56.8G 818M /
# zfs create rpool/db/pgdb1
# chown postgres:postgres /db/pgdb1
# zfs list
NAME USED AVAIL REFER MOUNTPOINT
rpool 819M 56.8G 96K none
rpool/db 256K 48.0G 96K /db
rpool/db/pgdb1 96K 48.0G 96K /db/pgdb1
rpool/root 818M 56.8G 818M /
# zfs set reservation=1G rpool/db/pgdb1
# zfs list
NAME USED AVAIL REFER MOUNTPOINT
rpool 1.80G 55.8G 96K none
rpool/db 1.00G 47.0G 96K /db
rpool/db/pgdb1 96K 48.0G 12.0M /db/pgdb1
rpool/root 818M 55.8G 818M /
initdb like a boss
96
# su postgres -c 'initdb --no-locale -E=UTF8 -n -N -D /db/pgdb1'
Running in noclean mode. Mistakes will not be cleaned up.
The files belonging to this database system will be owned by user "postgres".
This user must also own the server process.
The database cluster will be initialized with locale "C".
The default text search configuration will be set to "english".
Data page checksums are disabled.
fixing permissions on existing directory /db/pgdb1 ... ok
creating subdirectories ... ok
•Encode using UTF8, sort using C
•Only enable locale when you know you need it
•~2x perf bump by avoiding calls to iconv(3) to figure out sort order
•DO NOT use PostgreSQL checksums or compression
Backups
97
# zfs list -t snapshot
no datasets available
# pwd
/db/pgdb1
# find . | wc -l
895
# head -1 postmaster.pid
25114
# zfs snapshot rpool/db/pgdb1@pre-rm
# zfs list -t snapshot
NAME USED AVAIL REFER MOUNTPOINT
rpool/db/pgdb1@pre-rm 0 - 12.0M -
# psql -U postgres
psql (9.6.2)
Type "help" for help.
Guilty Pleasure
postgres=# \q
# rm -rf *
During Demos
# ls -1 | wc -l
0
# psql -U postgres
psql: FATAL: could not open relation mapping file "global/pg_filenode.map":
No such file or directory
Backups: Has Them
98
$ psql
psql: FATAL: could not open relation mapping file "global/pg_filenode.map": No such file or directory
# cat postgres.log
LOG: database system was shut down at 2017-03-03 21:08:05 UTC
LOG: MultiXact member wraparound protections are now enabled
LOG: database system is ready to accept connections
LOG: autovacuum launcher started
FATAL: could not open relation mapping file "global/pg_filenode.map": No such file or directory
LOG: could not open temporary statistics file "pg_stat_tmp/global.tmp": No such file or directory
LOG: could not open temporary statistics file "pg_stat_tmp/global.tmp": No such file or directory
...
LOG: could not open temporary statistics file "pg_stat_tmp/global.tmp": No such file or directory
LOG: could not open file "postmaster.pid": No such file or directory
LOG: performing immediate shutdown because data directory lock file is invalid
LOG: received immediate shutdown request
LOG: could not open temporary statistics file "pg_stat/global.tmp": No such file or directory
WARNING: terminating connection because of crash of another server process
DETAIL: The postmaster has commanded this server process to roll back the current transaction and exit,
because another server process exited abnormally and possibly corrupted shared memory.
HINT: In a moment you should be able to reconnect to the database and repeat your command.
LOG: database system is shut down
# ll
total 1
drwx------ 2 postgres postgres 2 Mar 3 21:40 ./
drwxr-xr-x 3 root root 3 Mar 3 21:03 ../
Restores: As Important as Backups
99
# zfs list -t snapshot
NAME USED AVAIL REFER MOUNTPOINT
rpool/db/pgdb1@pre-rm 12.0M - 12.0M -
# zfs rollback rpool/db/pgdb1@pre-rm
# su postgres -c '/usr/lib/postgresql/9.6/bin/postgres -D /db/pgdb1'
LOG: database system was interrupted; last known up at 2017-03-03 21:50:57 UTC
LOG: database system was not properly shut down; automatic recovery in progress
LOG: redo starts at 0/14EE7B8
LOG: invalid record length at 0/1504150: wanted 24, got 0
LOG: redo done at 0/1504128
LOG: last completed transaction was at log time 2017-03-03 21:51:15.340442+00
LOG: MultiXact member wraparound protections are now enabled
LOG: database system is ready to accept connections
LOG: autovacuum launcher started
Works all the time, every time, even with kill -9
(possible dataloss from ungraceful shutdown and IPC cleanup not withstanding)
Clone: Test and Upgrade with Impunity
100
# zfs clone rpool/db/pgdb1@pre-rm rpool/db/pgdb1-upgrade-test
# zfs list -r rpool/db
NAME USED AVAIL REFER MOUNTPOINT
rpool/db 1.00G 47.0G 96K /db
rpool/db/pgdb1 15.6M 48.0G 15.1M /db/pgdb1
rpool/db/pgdb1-upgrade-test 8K 47.0G 15.2M /db/pgdb1-upgrade-test
# echo "Test pg_upgrade"
# zfs destroy rpool/db/pgdb1-clone
# zfs clone rpool/db/pgdb1@pre-rm rpool/db/pgdb1-10
# echo "Run pg_upgrade for real"
# zfs promote rpool/db/pgdb1-10
# zfs destroy rpool/db/pgdb1
Works all the time, every time, even with kill -9
(possible dataloss from ungraceful shutdown and IPC cleanup not withstanding)
Tip: Naming Conventions
101
•Use a short prefix not on the root filesystem (e.g. /db)
•Encode the PostgreSQL major version into the dataset name
•Give each PostgreSQL cluster its own dataset (e.g. pgdb01)
•Optional but recommended:
Suboptimal Good
•one database per cluster
rpool/db/pgdb1 rpool/db/prod-db01-pg94
•one app per database
rpool/db/myapp-shard1 rpool/db/prod-myapp-shard001-pg95
•encode environment into DB name
rpool/db/dbN rpool/db/prod-dbN-pg10
•encode environment into DB username
Be explicit: codify the tight coupling between
PostgreSQL versions and $PGDATA/.
Defy Gravity
102
•Take and send snapshots to remote servers
•zfs send emits a snapshot to stdout: treat as a file or stream
•zfs receive reads a snapshot from stdin
•TIP: If available:
•Use the -s argument to zfs receive
•Use zfs get receive_resume_token on the receiving end to get the
required token to resume an interrupted send: zfs send -t 
Unlimited flexibility. Compress, encrypt,
checksum, and offsite to your heart's content.
Defy Gravity
103
# zfs send -v -L -p -e rpool/db/pgdb1@pre-rm > /dev/null
send from @ to rpool/db/pgdb1-10@pre-rm estimated size is 36.8M
total estimated size is 36.8M
TIME SENT SNAPSHOT
# zfs send -v -L -p -e \
rpool/db/pgdb1-10@pre-rm | \
zfs receive -v \
rpool/db/pgdb1-10-receive
send from @ to rpool/db/pgdb1-10@pre-rm estimated size is 36.8M
total estimated size is 36.8M
TIME SENT SNAPSHOT
received 33.8MB stream in 1 seconds (33.8MB/sec)
# zfs list -t snapshot
NAME USED AVAIL REFER
MOUNTPOINT
rpool/db/pgdb1-10@pre-rm 8K - 15.2M -
rpool/db/pgdb1-10-receive@pre-rm 0 - 15.2M -
Defy Gravity: Incrementally
104
•Use a predictable snapshot naming scheme
•Send snapshots incrementally
•Clean up old snapshots
•Use a monotonic snapshot number (a.k.a. "vector clock")
Remember to remove old snapshots.
Distributed systems bingo!
Defy Gravity: Incremental
105
# echo "Change PostgreSQL's data"
# zfs snapshot rpool/db/pgdb1-10@example-incremental-001
# zfs send -v -L -p -e \
-i rpool/db/pgdb1-10@pre-rm \
rpool/db/pgdb1-10@example-incremental-001 \
> /dev/null
send from @pre-rm to rpool/db/pgdb1-10@example-incremental-001
estimated size is 2K
total estimated size is 2K
# zfs send -v -L -p -e \
-i rpool/db/pgdb1-10@pre-rm \
rpool/db/pgdb1-10@example-incremental-001 | \
zfs receive -v \
rpool/db/pgdb1-10-receive
send from @pre-rm to rpool/db/pgdb1-10@example-incremental-001
estimated size is 2K
total estimated size is 2K
receiving incremental stream of rpool/db/pgdb1-10@example-
incremental-001 into rpool/db/pgdb1-10-receive@example-incremental-001
received 312B stream in 1 seconds (312B/sec)
Defy Gravity: Vector Clock
106
# echo "Change more PostgreSQL's data: VACUUM FULL FREEZE"
# zfs snapshot rpool/db/pgdb1-10@example-incremental-002
# zfs send -v -L -p -e \
-i rpool/db/pgdb1-10@example-incremental-001 \
rpool/db/pgdb1-10@example-incremental-002 \
> /dev/null
send from @example-incremental-001 to rpool/db/pgdb1-10@example-
incremental-002 estimated size is 7.60M
total estimated size is 7.60M
TIME SENT SNAPSHOT
# zfs send -v -L -p -e \
-i rpool/db/pgdb1-10@example-incremental-001 \
rpool/db/pgdb1-10@example-incremental-002 | \
zfs receive -v \
rpool/db/pgdb1-10-receive
send from @example-incremental-001 to rpool/db/pgdb1-10@example-
incremental-002 estimated size is 7.60M
total estimated size is 7.60M
receiving incremental stream of rpool/db/pgdb1-10@example-incremental-002
into rpool/db/pgdb1-10-receive@example-incremental-002
TIME SENT SNAPSHOT
received 7.52MB stream in 1 seconds (7.52MB/sec)
Defy Gravity: Cleanup
107
# zfs list -t snapshot -o name,used,refer
NAME USED REFER
rpool/db/pgdb1-10@example-incremental-001 8K 15.2M
rpool/db/pgdb1-10@example-incremental-002 848K 15.1M
rpool/db/pgdb1-10-receive@pre-rm 8K 15.2M
rpool/db/pgdb1-10-receive@example-incremental-001 8K 15.2M
rpool/db/pgdb1-10-receive@example-incremental-002 0 15.1M
# zfs destroy rpool/db/pgdb1-10-receive@pre-rm
# zfs destroy rpool/db/pgdb1-10@example-incremental-001
# zfs destroy rpool/db/pgdb1-10-receive@example-incremental-001
# zfs list -t snapshot -o name,used,refer
NAME USED REFER
rpool/db/pgdb1-10@example-incremental-002 848K 15.1M
rpool/db/pgdb1-10-receive@example-incremental-002 0 15.1M
Controversial: logbias=throughput
108
•Measure tps/qps
•Time duration of an outage (OS restart plus WAL replay, e.g. 10-20min)
•Measure cost of back pressure from the DB to the rest of the application
•Use a txg timeout of 1 second
Position: since ZFS will never be inconsistent and therefore PostgreSQL will
never loose integrity, 1s of actual data loss is a worthwhile tradeoff for a ~10x
performance boost in write-heavy applications.
Rationale: loss aversion costs organizations more than potentially loosing 1s
of data. Back pressure is a constant cost the rest of the application needs to
absorb due to continual fsync(2)'ing of WAL data. Architectural cost and
premature engineering costs need to be factored in. Penny-wise, pound
foolish.
Controversial: logbias=throughput
109
# cat /sys/module/zfs/parameters/zfs_txg_timeout
5
# echo 1 > /sys/module/zfs/parameters/zfs_txg_timeout
# echo 'options zfs zfs_txg_timeout=1' >> /etc/modprobe.d/zfs.conf
# psql -c 'ALTER SYSTEM SET synchronous_commit=off'
ALTER SYSTEM
# zfs set logbias=throughput rpool/db
QUESTIONS?
PI:EMAIL Twitter: @SeanChittenden
Email
PI:EMAIL