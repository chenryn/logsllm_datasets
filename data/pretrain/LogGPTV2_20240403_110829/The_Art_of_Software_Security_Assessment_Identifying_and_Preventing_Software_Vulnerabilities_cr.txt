soft limit up or down as it pleases. In fact, a process is free to move the soft limit so 
that it's any value between zero and its hard limit. Conversely, a hard limit represents 
the absolute maximum resource usage that a process is allowed. A normal process 
can change its hard limit, but it can only lower it, and lowering a hard limit is 
irreversible. Superuser processes, however, can also raise hard limits. The following 
list of supported resource limits can be called and set via setrlimit() and getrlimit() 
in Linux; other UNIX systems support some or all of these values: 
RLIMIT_CORE Maximum size in bytes of a core file that can be generated by the 
process. If this value is set to 0, the process doesn't dump the core file. 
RLIMIT_CPU Maximum amount of CPU time in seconds that the process can use. 
If this time limit is exceeded, the process is sent the SIGXCPU signal, which 
terminates the process by default. 
RLIMIT_DATA Maximum size in bytes of the data segment for the process. It 
includes the heap as well as static variables (both initialized and uninitialized). 
RLIMIT_FSIZE Maximum size in bytes that can be written to a file. Any file 
opened by the process for writing can't exceed this size. Any attempts to write 
to files that exceed this size result in the SIGXFSZ signal being sent to the 
process, which causes termination by default. 
RLIMIT_MEMLOCK Specifies the maximum number of bytes that can be locked in 
physical memory at one time. 
RLIMIT_NOFILE Specifies the maximum number of files a process can have 
open at one time. 
RLIMIT_NPROC Specifies the maximum amount of processes that specific user 
can run. 
RLIMIT_OFILE The BSD version of RLIMIT_NOFILE. 
RLIMIT_RSS Specifies the resident set size, which is the maximum number of 
virtual pages residing in physical memory. 
RLIMIT_STACK Specifies the maximum size in bytes for the process stack. Any 
attempt to expand the stack beyond this size generates a segmentation fault 
(SIGSEGV), which typically terminates the process. 
RLIMIT_VMEM Maximum bytes in the mapped address space. 
Rlimits are useful for developers to curtail potentially risky activities in secure 
programs, such as dumping memory to a core file or falling prey to denial-of-service 
attacks. However, rlimits also have a dark side. Users can set fairly tight limits on a 
process and then run a setuid or setgid program. Rlimits are cleared out when a 
process does a fork(), but they survive the exec() family of calls, which can be used 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
593 
to force a failure in a predetermined location in the code. The reason that setting 
limits is so important is that developers often don't expect resources to be exhausted; 
as a result, even if they do handle the error to some degree, the error-handling code 
is usually less guarded than more well-traveled code paths. When developers do 
devote effort to securing error handling code, it is usually focused on dealing with 
input errors, so they rarely devote much effort to handling resource exhaustion 
securely. For example, take a look at Listing 10-2 taken from the BSD setenv() 
implementation. 
Listing 10-2. Setenv() Vulnerabilty in BSD 
int 
setenv(name, value, rewrite) 
        register const char *name; 
        register const char *value; 
        int rewrite; 
{ 
    extern char **environ; 
    static int alloced;       /* if allocated space before */ 
    register char *C; 
    int l_value, offset; 
    if (*value == '=')        /* no '='alloced = 1;      /* copy old 
entries into it */ 
            P = (char **)malloc((size_t)(sizeof(char *) * 
                (cnt + 2))); 
            if (!P) 
                return (-1); 
            bcopy(environ, P, cnt * sizeof(char *)); 
        environ = P; 
} 
environ[cnt + 1] = NULL; 
Obviously, it's unlikely for any of these calls to malloc() to fail, and their failure 
certainly isn't expected. Say alloced is set to 0 and malloc() does fail, however 
(shown in the bolded code lines). In this case, alloced will be set to 1 to indicate that 
the environment is allocated dynamically, but environ is never updated because the 
call to malloc() failed. Therefore, subsequent calls to setenv() cause the original 
stack buffer that environ still references to be passed as an argument to realloc() as 
if it is a heap buffer! 
Although it might be possible for users to exhaust resources naturally, triggering 
these code paths can often be complicated, and that's where setting resource limits 
comes in. Say you want a call to malloc() to fail at a certain point in the code; this 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
594 
might not even be possible if the program hasn't dealt with enough input data yet. 
Even if it has, because malloc() occurs so often, making a specific call fail is difficult. 
Using setrlimit(), attackers can have some control over the amount of total memory 
the process can consume, which gives them a chance to trigger the vulnerable code 
path fairly accurately. 
Michael Zalewski, a noted security researcher, noticed a similar problem in the way 
that crontab functions (archived at http://seclists.org/bugtraq/1998/Feb/0018.html). 
When crontab first starts, it creates a root-owned temporary file in the crontab 
directory. It reads the user's crontab file and copies it to the temporary file. When the 
copy is completed, crontab renames this temporary file with the user's name so that 
the cron daemon parses it. Zalewski noticed that if you submit a file large enough to 
reach the resource limit for the file size, the soft limit signal kills crontab while it's still 
writing the file, before it can rename or unlink the temporary file. These temporary 
files stay lodged in the crontab directory and evade quotas because they are owned 
by root. 
Rafal Wojtczuk explained in a bugtraq post how he was able to exploit a problem in old 
versions of the Linux dynamic loader. Take a look at the following code: 
int fdprintf(int fd, const char *fmt, ...) 
{ 
    va_list args; 
    int i; 
    char buf[1024]; 
    va_start(args, fmt); 
    i=vsprintf(buf,fmt,args); 
    va_end(args); 
    write(fd, buf, i); 
    return i; 
} 
... 
static int try_lib(char *argv0, char *buffer, 
    char *dir, char *lib) 
{ 
    int found; 
    strcpy(buffer, dir); 
    if (lib != NULL) 
    { 
        strcat(buffer, "/"); 
        strcat(buffer, lib); 
    } 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
595 
    if (!(found = !uselib(buffer))) 
    { 
        if (errno != ENOENT) 
        { 
            fdprintf(2, "%s: can't load library '%s'\n", 
                argv0, buffer); 
            fdprintf(2, "\t%s\n", strerror(errno)); 
        } 
    } 
    return found; 
} 
The TRy_lib() function is called by the dynamic loader to see whether a library file is 
present. It constructs the pathname and then attempts to call uselib(), which is a 
Linux system call that loads a shared library. uselib() returns errors similar to open(), 
such as ENFILE. If the shared library file can't be opened, the loader constructs an 
error message using fdprintf(). This function obviously has a buffer overflow with its 
use of vsprintf() to print into the 1024-byte stack buffer buf. If users can trigger the 
error that results in a call to fdprintf() and supply a long argv0 string when loading 
a setuid binary, they are able to exploit the overflow. 
To exploit this error, Wojtczuk had to time it so that the system consumed the total 
limit of file descriptors right before the loader attempted to load the library. He came 
up with a clever attack: He used file locking and the close-on-exec flag to ensure that 
his exploit program ran immediately after the exec() system call was completed and 
before the kernel invoked the dynamic loader. His exploit program then sent a 
SIGSTOP to the setuid program that ran, consumed all available file descriptors, and 
then sent a SIGCONT. When processing returned to the dynamic loader, no file 
descriptors were left to be allocated, causing the error message to be printed and the 
buffer overflow to occur. 
In addition, a program that writes data to a sensitive file might be exploitable if rlimits 
can be used to induce unexpected failure conditions. RLIMIT_FSIZE enforces a 
maximum limit on how many bytes a file can be that a process writes to. For example, 
setting this value to 5 means that any write() operation to a file will fail once the file 
becomes larger than 5 bytes in length. A single write() on a new file, therefore, 
results in five bytes being written to the file (and write() successfully returns 5). Any 
subsequent writes to the same file fail, and a SIGXFSZ signal is sent to the process, 
which will terminate if this signal doesn't have a handler installed. A file being 
appended to fails when its total size exceeds the value set in RLIMIT_FSIZE. If the file 
is already larger than the limit when it's opened, the first write() fails. Because signal 
masks are also inherited over an exec() system call, you can have a privileged 
program ignore the SIGXFSZ signal and continue processing. With the combination of 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
596 
setting a signal mask and imposing a file resource limit (RLIMIT_FSIZE), you can 
arbitrarily cause file writes to fail at any place you choose. For example, consider a 
setuid root program that does the following: 
struct entry { 
    char name[32]; 
    char password[256]; 
    struct entry *next; 
}; 
int write_entries(FILE *fp, struct entry *list) 
{ 
    struct entry *ent; 
    for(ent = list; ent; ent = ent->next) 
        fprintf(fp, "%s:%s\n", ent->name, ent->password); 
    return 1; 
} 
This code iterates through a linked list of username/password pairs and prints them 
to an output file. By using the setrlimit() function to set RLIMIT_FSIZE, you can force 
fprintf() to print only a certain number of bytes to a file. This technique might be 
useful for cutting an entry off just after the username: part has been written on a line, 
thus causing the password to be truncated. 
Auditing Tip 
Carefully check for any privileged application that writes to a file without verifying 
whether writes are successful. Remember that checking for an error when calling 
write() might not be sufficient; they also need to check whether the amount of bytes 
they wrote were successfully stored in their entirety. Manipulating this application's 
rlimits might trigger a security vulnerability by cutting the file short at a strategically 
advantageous offset. 
Often code reviewers and developers alike tend to disregard code built to handle an 
error condition caused by resource exhaustion automatically, because they don't 
consider the possibility that users can trigger those code paths. In short, they forget 
about setting resource limits. When you're auditing applications that interact with 
system resources, make sure you address this question: "If I somehow cause a failure 
condition, can I leverage that condition to exploit the program?" 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
597 
Auditing Tip 
Never assume that a condition is unreachable because it seems unlikely to occur. 
Using rlimits is one way to trigger unlikely conditions by restricting the resources a 
privileged process is allowed to use and potentially forcing a process to die when a 
system resource is allocated where it usually wouldn't be. Depending on the 
circumstances of the error condition you want to trigger, you might be able to use 
other methods by manipulating the program's environment to force an error. 
File Descriptors 
Many security-related aspects of UNIX are properties of how file descriptors behave 
across process creation and execution. You know that file descriptors are duplicated 
when a process is forked, and you've seen how the processes end up sharing their 
access to an underlying file object through these duplicated file descriptors. 
A process can also explicitly make a copy of a file descriptor, which results in the same 
underlying semantics as a file descriptor duplicated through forking. This copying is 
usually done with the dup(), dup2(), or fcntl() system calls. Processes normally pass 
file descriptors on to their children via fork(), but UNIX does provide ways for file 
descriptors to be shared with unrelated processes by using IPC. Interested readers 
can refer to W.R. Stephen's coverage of UNIX domain sockets in Advanced 
Programming in the Unix Environment (Addison-Wesley, 1992(? [????.])). 
File Sharing 
Whether process descriptors are duplicated through fork() or the dup() family of calls, 
you end up with multiple file descriptors across one or more processes that refer to 
the same open file object in the kernel. Consequently, all these processes share the 
same access flags and internal file pointer to that file. 
If multiple processes in a system open the same file with open(), they have their own 
open file structures. Therefore, they have their own file position pointers and could 
have different access modes and flags set on their interface with the file. They are still 
working with the same file, so changes to file contents and properties kept in the file's 
inode structure still affect a file's concurrent users. 
You can see an example in Figure 10-2, which shows two processes that aren't related 
to each other. Both processes have the password file open. Process 2000 has it open 
as its third file descriptor, and it opened the password file for read-only access, shown 
in the associated open file structure. The process on the right, process 3200, has the 
password file for both read and write access and has advanced its file pointer to the 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
598 
location 0x33. The two processes have different levels of access to the password file, 
and they have independent file pointers that track their location in the file. 
Figure 10-2. Independent opens of the same file 
The access a process has to a file is determined when that file is opened. In Figure 
10-2, process 3200 opened the password file with read/write access, so it has a file 
descriptor and open file pointer representing that information. If someone renames 
the password file, changes its permissions to octal 0000, changes its owner and group 
to arbitrary people, and even deletes it from the file system, process 3200 still has an 
open descriptor to that file that allows it to read and write. 
Close-on-Exec 
File descriptors are retained in a process across the execution of different programs, 
unless the file descriptors are especially marked for closure. This behavior might not 
be quite what you'd expect, as UNIX tends to start most other aspects of a process 
over with a clean slate when a new program runs. UNIX does allow developers to 
mark certain file descriptors as close-on-exec, which means they are closed 
automatically if the process runs a new program. Close-on-exec can be a useful 
precaution for sensitive or critical files that developers don't want to be inherited by a 
subprogram. The file descriptor is usually marked with the fcntl() system call, and 
the kernel makes a note of it in the process descriptor table for the process. For 
applications that spawn new processes at any stage, always check to see whether this 
step is taken when it opens files. It is also useful to make a note of those persistent 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
599 
files that aren't marked to close when a new program starts. In the next section, you 
will see that haphazardly leaving these files around can have interesting 
consequences. 
File Descriptor Leaks 
The possible actions a process can perform on a file descriptor are determined when 
the file descriptor is first created. To put it another way, security checks are 
performed only once, when the process initially creates a file descriptor by opening or 
creating a resource. If you can get access to a file descriptor that was opened with 
write access to a critical system file, you can write to that file regardless of your 
effective user ID or other system privileges. Therefore, programs that work with file 
descriptors to security-sensitive resources should close their descriptors before 
running any user-malleable code. For example, take a look at a hypothetical 
computer game that runs with the privileges necessary to open kernel memory: 
    int kfd; 
    pid_t p; 
    char *initprog; 
    kfd = safe_open("/dev/kmem", O_RDWR); 
    init_video_mem(kfd); 
    if ((initprog=getenv("CONTROLLER_INIT_PROGRAM"))) 
    { 
        if ((p=safe_fork()))         /* PARENT */ 
        { 
            wait_for_kid(p); 
            g_controller_status=CONTROLLER_READY; 
        } 
        else                         /* CHILD */ 
        { 
            drop_privs (); 
            execl(initprog, "conf", NULL); 
            exit(0);                 /* unreached */ 
        } 
    } 
    /* main game loop */ 
... 
This game first opens direct access to the system's memory via the device driver 
accessible at /dev/kmem. It uses this access to directly modify memory mapped to the 
video card for the purposes of performance. The game can also run an external 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
600 
program to initialize a game controller, which users specify in the environment 
variable CONTROLLER_INIT_PROGRAM. The program permanently drops privileges before 
running this program to prevent users from simply supplying their own program to 
run with elevated privileges. 
The problem with this code is that the file descriptor that references the /dev/kmem file, 
kfd, is never closed before the game runs the external controller initialization 
program. Even though permissions have been fully dropped, attackers could still take 
control of the machine by providing a malicious controller initialization program. This 
attack is possible because the executed program starts with an open, writeable file 
descriptor to /dev/kmem. Attackers would need to construct a fairly straightforward 
program that could modify critical kernel data structures and elevate user privileges. 
This example might seem a bit contrived, but it's quite similar to a vulnerability in 
recent versions of FreeBSD. FreeBSD's libkvm library provides access to kernel 
symbols, addresses, and values for programs that need to work with kernel memory. 
A researcher named badc0ded discovered that this library could leave file descriptors 
open to critical files, such as /dev/kmem, and because of the library's interface, it was 
difficult for application authors to prevent a leak. Although no programs in the 
standard FreeBSD distribution were found to use the library in an nonsecure fashion, 
badc0ded found several ports that could be exploited to gain root privileges. (The 
FreeBSD advisory can be found at 
http://security.freebsd.org/advisories/FreeBSD-SA-02:39.libkvm.asc.) 