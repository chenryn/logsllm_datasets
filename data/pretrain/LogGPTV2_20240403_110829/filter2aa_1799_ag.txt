ping after the trap handler correctly switches the page tables before returning to user mode.
kernel page tables. To prevent false sharing of addresses close to the chunk of memory being mapped 
in the transition address space, the memory manager always recreates the page table hierarchy map-
ping for the PTE(s) being shared. This implies that every time the kernel needs to map some new pages 
in the transition address space of a process, it must
page tables (the internal MiCopyTopLevelMappings routine performs exactly this operation). 
TLB flushing algorithm
(translation look-aside buffer). The TLB is a cache used by the processor to quickly translate the virtual ad-
dresses that are used while executing code or accessing data. A valid entry in the TLB allows the processor 
-
nel address space is mostly unique and shared between all processes. Intel and AMD introduced differ-
detail in the Intel and AMD architecture manuals and are not further discussed in this book.
-
-
lowing two goals:
I 
No valid kernel entries will be ever maintained in the TLB when executing a thread user-code.
Otherwise, this could be leveraged by an attacker with the same speculation techniques used in
Meltdown, which could lead her to read secret kernel data.
I 
CHAPTER 8 System mechanisms
21
trap exit. It can run on a system that either supports only the global/non-global bit or also PCIDs. In the 
while a page table switch happens (the system changes the value of the CR3 register). Systems with 
PCID support labels kernel pages with PCID 2, whereas user pages are labelled with PCID 1. The global 
and non-global bits are ignored in this case.
When the current-executing thread ends its quantum, a context switch is initialized. When the 
kernel schedules execution for a thread belonging to another process address space, the TLB algorithm 
assures that all the user pages are removed from the TLB (which means that in systems with global/
kernel entries are removed (or invalidated) from the TLB. This is easily achievable: on processors with 
global/non-global bit support, just a reload of the page tables forces the processor to invalidate all the 
non-global pages, whereas on systems with PCID support, the user-page tables are reloaded using the 
User PCID, which automatically invalidates all the stale kernel TLB entries. 
The strategy allows kernel trap entries, which can happen when an interrupt is generated while the 
system was executing user code or when a thread invokes a system call, not to invalidate anything in 
TABLE 8-1 
Configuration Type
User Pages
Kernel Pages
Transition Pages
Non-global
Global
N / D
PCID 1, non-global
PCID 2, non-global
PCID 1, non-global
Global
Non-global
Global
Hardware indirect branch controls (IBRS, IBPB, STIBP, SSBD)
Processor manufacturers have designed hardware mitigations for various side-channel attacks. Those 
mitigations have been designed to be used with the software ones. The hardware mitigations for side-
channel attacks are mainly implemented in the following indirect branch controls mechanisms, which 
I 
Indirect Branch Restricted Speculation (IBRS) completely disables the branch predictor (and
clears the branch predictor buffer) on switches to a different security context (user vs kernel
mode or VM root vs VM non-root). If the OS sets IBRS after a transition to a more privileged
mode, predicted targets of indirect branches cannot be controlled by software that was ex-
ecuted in a less privileged mode. Additionally, when IBRS is on, the predicted targets of indirect
branches cannot be controlled by another logical processor. The OS usually sets IBRS to 1 and
keeps it on until it returns to a less privileged security context.
The implementation of IBRS depends on the CPU manufacturer: some CPUs completely disable
branch predictors buffers when IBRS is set to on (describing an inhibit behavior), while some
22 
CHAPTER 8 System mechanisms
mitigation control works in a very similar way to IBPB, so usually the CPU implement only IBRS. 
I 
Indirect Branch Predictor Barrier (IBPB)
it is set to 1, creating a barrier that prevents software that executed previously from controlling
the predicted targets of indirect branches on the same logical processor.
I 
Single Thread Indirect Branch Predictors (STIBP) restricts the sharing of branch prediction
between logical processors on a physical CPU core. Setting STIBP to 1 on a logical processor
prevents the predicted targets of indirect branches on a current executing logical processor
from being controlled by software that executes (or executed previously) on another logical
processor of the same core.
I 
Speculative Store Bypass Disable (SSBD) instructs the processor to not speculatively execute
loads until the addresses of all older stores are known. This ensures that a load operation does
not speculatively consume stale data values due to bypassing an older store on the same logi-
cal processor, thus protecting against Speculative Store Bypass attack (described earlier in the
“Other side-channel attacks” section).
The NT kernel employs a complex algorithm to determine the value of the described indirect branch 
trap entries, and trap exits. On compatible systems, the system runs kernel code with IBRS always on (ex-
cept when Retpoline is enabled). When no IBRS is available (but IBPB and STIBP are supported), the kernel 
another security context). SSBD, when supported by the CPU, is always enabled in kernel mode. 
mitigations enabled or just with STIBP on (depending on STIBP pairing being enabled, as explained in 
the next section). The protection against Speculative Store Bypass must be manually enabled if needed 
through the global or per-process Speculation feature. Indeed, all the speculation mitigations can be 
to an individual setting. Table 8-2 describes individual feature settings and their meaning.
TABLE 8-2 
Name
Value
Meaning
0x1
Disable IBRS except for non-nested root partition 
0x2
0x4
0x8
Always set SSBD in kernel and user
0x10
Set SSBD only in kernel mode (leaving user-mode 
code to be vulnerable to SSB attacks)
0x20
Always keep STIBP on for user-threads, regardless of 
STIBP pairing
CHAPTER 8 System mechanisms
23
Name
Value
Meaning
0x40
Disables the default speculation mitigation strategy 
(for AMD systems only) and enables the user-to-user 
controls are set when running in kernel mode.
0x80
Always disable STIBP pairing
0x100
Always disable Retpoline
0x200
Enable Retpoline regardless of the CPU support of 
IBPB or IBRS (Retpoline needs at least IBPB to prop-
erly protect against Spectre v2)
0x20000
Disable Import Optimization regardless of Retpoline
Retpoline and import optimization
not acceptable for games and mission-critical applications, which were running with a lot of perfor-
mance degradation. The mitigation that was bringing most of the performance degradation was IBRS 
possible without using any hardware mitigations thanks to the memory fence instructions. A good 
execute any new operations speculatively before the fence itself completes. Only when the fence com-
to execute (and to speculate) new opcodes. The second variant of Spectre was still requiring hardware 
mitigations, though, which implies all the performance problems brought by IBRS and IBPB. 
speculative execution. Instead of performing a vulnerable indirect call, the processor jumps to a safe 
the new target thanks to a “return” operation.
FIGURE 8-9 Retpoline code sequence of x86 CPUs.
In Windows, Retpoline is implemented in the NT kernel, which can apply the Retpoline code se-
quence to itself and to external driver images dynamically through the Dynamic Value Relocation Table 
(DVRT). When a kernel image is compiled with Retpoline enabled (through a compatible compiler), the 
24 
CHAPTER 8 System mechanisms
-
but augmented with a variable size padding. The entry in the DVRT includes all the information that 
external drivers compiled with Retpoline support can run also on older OS versions, which will simply 
skip parsing the entries in the DVRT table.
Note The DVRT was originally developed for supporting kernel ASLR (Address Space Layout 
Randomization, discussed in Chapter 5 of Part 1). The table was later extended to include 
Retpoline descriptors. The system can identify which version of the table an image includes.
In phase -1 of its initialization, the kernel detects whether the processor is vulnerable to Spectre, and, 
in case the system is compatible and enough hardware mitigations are available, it enables Retpoline 
and applies it to the NT kernel image and the HAL. The RtlPerformRetpolineRelocationsOnImage rou-
tine scans the DVRT and replaces each indirect branch described by an entry in the table with a direct 
branch, which is not vulnerable to speculative attacks, targeting the Retpoline code sequence. The 
original target address of the indirect branch is saved in a CPU register (R10 in AMD and Intel proces-
sors), with a single instruction that overwrites the padding generated by the compiler. The Retpoline 
Before being started, boot drivers are physically relocated by the internal MiReloadBootLoadedDrivers 
boot drivers, the NT kernel, and HAL images are allocated in a contiguous virtual address space by 
the Windows Loader and do not have an associated control area, rendering them not pageable. This 
means that all the memory backing the images is always resident, and the NT kernel can use the same 
RtlPerformRetpolineRelocationsOnImage function to modify each indirect branch in the code directly. 
PERFORM_
RETPOLINE_RELOCATIONS-
Note 
for further details) initializes and protects some of them. It is illegal for drivers and the NT 
kernel itself to modify code sections of protected drivers.
Runtime drivers, as explained in Chapter 5 of Part 1, are loaded by the NT memory manager, which 
CHAPTER 8 System mechanisms
25
page fault handler. Windows applies Retpoline on the shared pages pointed by the prototype PTEs. If 
the same section is also mapped by a user-mode application, the memory manager creates new private 
pages and copies the content of the shared pages in the private ones, reverting Retpoline (and Import 
Note 
Retpoline cannot be enabled because it would not be able to protect against Spectre v2. In 
this situation, only hardware mitigations can be applied. Enhanced IBRS (a new hardware 
mitigation) solves the performance problems of IBRS.
The Retpoline bitmap
One of the original design goals (restraints) of the Retpoline implementation in Windows was to sup-
port a mixed environment composed of drivers compatible with Retpoline and drivers not compatible 
with it, while maintaining the overall system protection against Spectre v2. This implies that drivers 
that do not support Retpoline should be executed with IBRS on (or STIBP followed by an IBPB on kernel 
entry, as discussed previously in the ”Hardware indirect branch controls” section), whereas others can 
run without any hardware speculation mitigations enabled (the protection is brought by the Retpoline 
code sequences and memory fences).
To dynamically achieve compatibility with older drivers, in the phase 0 of its initialization, the NT 
space contains Retpoline compatible code; a 0 means the opposite. The NT kernel then sets to 1 the 
bits referring to the address spaces of the HAL and NT images (which are always Retpoline compatible). 
Every time a new kernel image is loaded, the system tries to apply Retpoline to it. If the application suc-
ceeds, the respective bits in the Retpoline bitmap are set to 1.
The Retpoline code sequence is augmented to include a bitmap check: Every time an indirect branch 
is performed, the system checks whether the original call target resides in a Retpoline-compatible 
module. In case the check succeeds (and the relative bit is 1), the system executes the Retpoline code 
Retpoline bitmap is 0), a Retpoline exit sequence is initialized. The RUNNING_NON_RETPOLINE_CODE 
SPEC_CONTROL 
-
ware mitigations provide the needed protection).
When the thread quantum ends, and the scheduler selects a new thread, it saves the Retpoline 
status (represented by the presence of the RUNNING_NON_RETPOLINE_CODE
processors in the KTHREAD data structure of the old thread. In this way, when the old thread is selected 
again for execution (or a kernel trap entry happens), the system knows that it needs to re-enable the 
needed hardware speculation mitigations with the goal of keeping the system always protected.
26 
CHAPTER 8 System mechanisms
Import optimization
Retpoline entries in the DVRT also describe indirect branches targeting imported functions. An im-
ported control transfer entry in the DVRT describes this kind of branch by using an index referring to 
pointers compiled by the loader.) After the Windows loader has compiled the IAT, it is unlikely that its 
it is not needed to transform an indirect branch targeting an imported function to a Retpoline one be-
cause the NT kernel can ensure that the virtual addresses of the two images (caller and callee) are close 
enough to directly invoke the target (less than 2 GB).
FIGURE 8-10 Different indirect branches on the ExAllocatePool function.
Import optimization (internally also known as “import linking”) is the feature that uses Retpoline 
dynamic relocations to transform indirect calls targeting imported functions into direct branches. If 
a direct branch is used to divert code execution to an imported function, there is no need to apply 
Retpoline because direct branches are not vulnerable to speculation attacks. The NT kernel ap-
plies Import Optimization at the same time it applies Retpoline, and even though the two features 
Optimization, Windows has been able to gain a performance boost even on systems that are not vul-
nerable to Spectre v2. (A direct branch does not require any additional memory access.)
STIBP pairing
In hyperthreaded systems, for protecting user-mode code against Spectre v2, the system should run 
user threads with at least STIBP on. On nonhyperthreaded systems, this is not needed: protection 
against a previous user-mode thread speculation is already achieved thanks to the IBRS being enabled 
while previously executing kernel-mode code. In case Retpoline is enabled, the needed IBPB is emitted 
branch prediction buffer is empty before executing the code of the user thread.
Leaving STIBP enabled in a hyper-threaded system has a performance penalty, so by default 
it is disabled for user-mode threads, leaving a thread to be potentially vulnerable by speculation 
from a sibling SMT thread. The end-user can manually enable STIBP for user threads through the 
CHAPTER 8 System mechanisms
27
mitigation option.
The described scenario is not ideal. A better solution is implemented in the STIBP pairing mecha-
nism. STIBP pairing is enabled by the I/O manager in phase 1 of the NT kernel initialization (using the 
KeOptimizeSpecCtrlSettings function) only under certain conditions. The system should have hyper-
-
ible only on non-nested virtualized environments or when Hyper-V is disabled (refer to Chapter 9 for 
further details.) 
in the EPROCESS data structure), which is represented by a 64-bit number. The system security domain 
-
istrative token. Nonsystem security domains are assigned at process creation time (by the internal 
PspInitializeProcessSecurity function) following these rules:
I 
If the new process is created without a new primary token explicitly assigned to it, it obtains the
same security domain of the parent process that creates it.
I 
CreateProcessAsUser or CreateProcessWithLogon APIs, for example), a new user security domain
ID is generated for the new process, starting from the internal PsNextSecurityDomain symbol.
The latter is incremented every time a new domain ID is generated (this ensures that during the
system lifetime, no security domains can collide).
I 
Note that a new primary token can be also assigned using the NtSetInformationProcess API
(with the ProcessAccessToken
the API to succeed, the process should have been created as suspended (no threads run in it). At
this stage, the process still has its original token in an unfrozen state. A new security domain is
assigned following the same rules described earlier.
Security domains can also be assigned manually to different processes belonging to the 
same group. An application can replace the security domain of a process with another one 
of a process belonging to the same group using the NtSetInformationProcess API with the 
ProcessCombineSecurityDomainsInformation class. The API accepts two process handles and replaces 
open each other with the PROCESS_VM_WRITE and PROCESS_VM_OPERATION access rights.
Security domains allow the STIBP pairing mechanism to work. STIBP pairing links a logical proces-
sor (LP) with its sibling (both share the same physical core. In this section, we use the term LP and CPU 
interchangeably). Two LPs are paired by the STIBP pairing algorithm (implemented in the internal 
KiUpdateStibpPairing function) only when the security domain of the local CPU is the same as the one 
of the remote CPU, or one of the two LPs is Idle. In these cases, both the LPs can run without STIBP be-
ing set and still be implicitly protected against speculation (there is no advantage in attacking a sibling 
CPU running in the same security context).
28 
CHAPTER 8 System mechanisms
The STIBP pairing algorithm is implemented in the KiUpdateStibpPairing function and includes a full 
state machine. The routine is invoked by the trap exit handler (invoked when the system exits the kernel 
pairing state of an LP can become stale mainly for two reasons:
I 
The NT scheduler has selected a new thread to be executed in the current CPU. If the new thread
This allows the STIBP pairing algorithm to re-evaluate the pairing state of the two.
I 
When the sibling CPU exits from its idle state, it requests the remote CPU to re-evaluate its
STIBP pairing state.
Note that when an LP is running code with STIBP enabled, it is protected from the sibling CPU 
speculation. STIBP pairing has been developed based also on the opposite notion: when an LP executes 
with STIBP enabled, it is guaranteed that its sibling CPU is protected against itself. This implies that 
when a context switches to a different security domain, there is no need to interrupt the sibling CPU 
even though it is running user-mode code with STIBP disabled. 
The described scenario is not true only when the scheduler selects a VP-dispatch thread (backing 
a virtual processor of a VM in case the Root scheduler is enabled; see Chapter 9 for further details) 
belonging to the VMMEM process. In this case, the system immediately sends an IPI to the sibling 
thread for updating its STIBP pairing state. Indeed, a VP-dispatch thread runs guest-VM code, which 
can always decide to disable STIBP, moving the sibling thread in an unprotected state (both runs with 
STIBP disabled).
EXPERIMENT: Querying system side-channel mitigation status
Windows exposes side-channel mitigation information through the SystemSpeculationControl 
Information and SystemSecureSpeculationControlInformation information classes used by the 
NtQuerySystemInformation native API. Multiple tools exist that interface with this API and show 
to the end user the system side-channel mitigation status:
I
supported by Microsoft, which is open source and available at the following GitHub
repository: https://github.com/microsoft/SpeculationControl
I
The SpecuCheck tool, developed by Alex Ionescu (one of the authors of this book),
which is open source and available at the following GitHub repository: