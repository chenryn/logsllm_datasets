on the ethical principles outlined in the Menlo Report [24],
which serves as a guideline within the ﬁeld of ICT research;
we apply the principle of beneﬁcence in particular: identifying
potential beneﬁts and harms, weighing them against each other
and minimizing the risk of inﬂicting harm.
Because of their commercial nature, the providers of popu-
larity rankings have an economic interest in these being accu-
rate. We show that these lists can be manipulated, negatively
affecting their perceived reputability. Our ﬁndings are however
of value to the providers: by evaluating the various techniques
and reporting our ﬁndings, the providers become aware of the
potential threats, may take actions to thwart attackers and can
improve the correctness of their rankings.
We have disclosed our ﬁndings and proposals for potential
remedies to the four providers, alongside a list of manipulated
domains for them to remove from their datasets and past
and present rankings. Alexa and Majestic provided statements
regarding the value of their rankings and the (in)feasibility
of manipulation, but commercial considerations prevent them
from elaborating on their methods. Cisco Umbrella closed our
issue without any statement, and we received no response
from Quantcast. None of our test domains were (retroactively)
removed from any rankings after our notiﬁcation.
We minimize the impact of our experiments on third parties
by only signiﬁcantly manipulating the ranking of our own,
purposefully registered domains and refraining from intrusive
or questionable techniques. Our sites also contained an ex-
planation of our experiment and contact details for affected
parties. Our low number of test domains means that only
few domains will see negligible shifts in ranking due to our
experiments; e.g. the volatility of Alexa’s list has a signiﬁcantly
larger impact. Moreover, we minimized the duration of our
experiments and of our domains being ranked. The impact on
other research using these lists is also minimal; we showed
11
100101102103104105106107108Visits0100000200000300000400000500000RankThe relative difﬁculty of maliciously inserting links into
pages on many IP subnets already reduces the vulnerability
of link-based rankings to large-scale manipulation. Speciﬁc
attacks where the page reﬂects a URL passed as a parameter
could be detected, although this can be made more difﬁcult by
obfuscation and attacks that alter a page more permanently.
The link-based rankings could be reﬁned with reputation
scores, e.g. the age of a linked page or Majestic’s “Flow
Metrics” [48], to devalue domains that are likely to be part
of a manipulation campaign.
Finally, requiring ranked domains to be available and to
host real content increases the cost of large-scale manipulation,
as domain names need to be bought and servers and web pages
need to be set up. For Umbrella, not ranking domains where
name resolution fails can signiﬁcantly reduce unavailable (and
therefore possibly fake) domains in the list. The other providers
can perform similar availability checks in the DNS or by
crawling the domain.
B. Creating rankings suitable for research
As we cannot ensure that providers will (want to) imple-
ment changes that discourage (large-scale) manipulation, we
look at combining all currently available ranking data with
the goal of improving the properties of popularity rankings
for research, canceling out the respective deﬁciencies of the
existing rankings. To this extent, we introduce TRANCO, a
service that researchers can use to obtain lists with such more
desirable and appropriate properties. We provide standard lists
that can be readily used in research, but also allow these lists to
be highly conﬁgurable, as depending on the use case, different
trafﬁc sources or varying degrees of stability may be beneﬁcial.
Moreover, we provide a permanent record to these new
lists, their conﬁguration and their construction methods. This
makes historical lists more easily accessible to reduce the
effort in replicating studies based upon them, and ensures that
researchers can be aware of the inﬂuences on the resulting list
by its component lists and conﬁguration.
Our service is available at https://tranco-list.eu. The
source code is also openly published at https://github.com/
DistriNet/tranco-list to provide full transparency of how our
lists are processed.
1) Combination options and ﬁlters: We support creating
new lists where the ranks are averaged across a chosen period
of time and set of providers, and introduce additional ﬁlters,
with the goal of enhancing the research-oriented properties of
our new lists.
In order to improve the rank of the domains that the lists
agree upon, we allow to average ranks over the lists of some
or all providers. We provide two combination methods: the
Borda count where, for a list of length N, items are scored with
N; N (cid:0)1; :::; 1; 0 points; and the Dowdall rule where items are
scored with 1; 1=2; :::; 1=(N (cid:0) 1); 1=N points [28]. The latter
reﬂects the Zipf’s law distribution that website trafﬁc has been
modeled on [4], [22]. Our standard list applies the Dowdall
rule to all four lists. We also allow to ﬁlter out domains that
appear only on one or a few lists, to avoid domains that are
only marked as popular by one provider: these may point to
isolated manipulation.
To improve the stability of our combined lists, we allow
to average ranks over the lists of several days; our standard
list uses the lists of the past 30 days. Again, we allow to
ﬁlter out domains that appear only for one or a few days, to
avoid brieﬂy popular (or manipulated) domains. Conversely, if
capturing these short-term effects is desired, lists based on one
day’s data are available. When combining lists, we also provide
the option to only consider a certain subset of the input lists,
to select domains that are more likely to actually be popular.
Differences in list composition complicate the combination
of the lists. Umbrella’s list includes subdomains; we include
an option to use a recalculated ranking that only includes pay-
level domains. Quantcast’s list contains less than one million
domains; we proportionally rescale the scores used in the two
combination methods to the same range as the other lists.
We add ﬁlters to create a list that represents a certain
desired subset of popular domains. A researcher can either
only keep domains with certain TLDs to select sites more
likely to be associated with particular countries or sectors, or
exclude (overly represented) TLDs. To avoid the dominance of
particular organizations in the list, a ﬁlter can be applied where
only one domain is ranked for each set of pay-level domains
that differ only in TLD. Finally, only certain subdomains can
be retained, e.g. to heuristically obtain a list of authentication
services by selecting login.* subdomains.
To allow researchers to work with a set of domains that
is actually reachable and representative of real websites, we
provide options to ﬁlter the domains on their responsiveness,
status code and content length. We base these ﬁlters on a
regular crawl of the union of all domains on the four existing
lists. This ensures that the sample of domains used in a study
yields results that accurately reﬂect practices on the web.
To further reﬁne on real and popular websites, we include
a ﬁlter on the set of around 3 million distinct domains in
Google’s Chrome User Experience Report, said to be ‘popular
destinations on the web’ [34]. Its userbase can be expected
to be (much) larger than e.g. Alexa’s panel; however, Google
themselves indicate that it may not fully represent the broader
Chrome userbase [34]. Moreover,
is only updated
monthly and does not rank the domains, so it cannot be used
as a replacement for the existing rankings.
the list
To reduce the potential effects of malicious domains on
research results (e.g.
in classiﬁer accuracy), we allow to
remove domains on the Google Safe Browsing list [33] from
our generated lists.
2) Evaluation: We evaluate the standard options chosen
for our combined lists on their improvements to similarity and
stability; the representativeness, responsiveness and benignness
of the included domains can be improved by applying the
appropriate ﬁlters. We generate our combined lists from March
1, 2018 to November 14, 2018, to avoid distortions due to
Alexa’s and Quantcast’s method changes, and truncate them to
one million domains, as this is the standard for current lists.
a) Similarity: To determine the weight of the four ex-
isting lists, we calculate the rank-biased overlap with our com-
bined lists. Across different weightings, the RBO with Alexa’s
and Majestic’s lists is highest at 46.5–53.5% and 46.5–52%
respectively, while the RBO with Quantcast’s and Umbrella’s
12
lists is 31.5–40% and 33.5–40.5% respectively. These results
are affected by the differences in list composition: subdomains
for Umbrella and the shorter list for Quantcast mean that these
two lists have less entries potentially in common with Alexa
and Majestic, reducing their weight. Overall, there is no list
with a disproportionate inﬂuence on the combined list.
b) Stability: Averaging the rankings over 30 days is
beneﬁcial for stability: for the list combining all four providers,
on average less than 0.6% changes daily, even for smaller
subsets. For the volatile Alexa and Umbrella lists, the improve-
ment is even more profound: the daily change is reduced to
1.8% and 0.65% respectively. This means that the data from
these providers can be used even in longitudinal settings, as
the set of domains does not change signiﬁcantly.
3) Reproducibility: Studies rarely mention the date on
which a ranking was retrieved, when the websites on that list
were visited and whether they were reachable. Moreover, it is
hard to obtain the list of a previous date: only Cisco Umbrella
maintains a public archive of historical lists [21]. These two
aspects negatively affect the reproducibility of studies, as the
exact composition of a list cannot be retrieved afterwards.
In order to enhance the reproducibility of studies that use
one of our lists, we include several features that are designed
to create a permanent record that can easily be referenced.
Once a list has been created, a permanent short link and a
preformatted citation template are generated for inclusion in
a paper. Alongside the ability to download the exact set of
domains that the list comprises, the page available through this
link provides a detailed overview of the conﬁguration used to
create that particular list and of the methods of the existing
rankings, such that the potential inﬂuences of the selected
method can be assessed. This increases the probability that
researchers use the rankings in a more well founded manner.
4) Manipulation: Given that our combined lists still rely on
the data from the four existing lists, they remain susceptible
to manipulation. As domains that appear on all lists simultane-
ously are favored, successful insertion in all lists at once will
yield an artiﬁcially inﬂated rank in our combined list.
However, the additional combinations and ﬁlters that we
propose increase the effort required to have manipulated do-
mains appear in our combined lists. Averaging ranks over a
longer period of time means that manipulation of the lists needs
to be maintained for a longer time; it also takes longer for the
manipulated domains to obtain a (signiﬁcant) aggregated rank.
Moreover, intelligently applying ﬁlters can further reduce the
impact of manipulation: e.g. removing unavailable domains
thwarts the ability to use fake domains.
As each ranking provider has their own trafﬁc data source,
the effects of manipulating one list are isolated. As none of the
lists have a particularly high inﬂuence in the combined list, all
four lists need to manipulated to the same extent to achieve
a comparable ranking in the combined list, quadrupling the
required effort. For the combined list generated for October 31,
2018, achieving a rank within the top million would require
boosting a domain in one list to at least rank 11 091 for one day
or rank 332 778 for 30 days; for a rank within the top 100 000,
ranks 982 and 29 479 would be necessary respectively. This
shows that massive or prolonged manipulation is required to
appear in our combined list.
VII. RELATED WORK
The work that is most recent and most closely related to
ours is that of Scheitle et al. [62], who compared Alexa’s,
Majestic’s and Umbrella’s lists on their structure and stability
over time, discussed their usage in (Internet measurement)
research through a survey of recent studies, calculated the
potential impact on their results, and drafted guidelines for
using the rankings. We focus on the implications of these
lists for security research, expanding the analysis to include
representativeness, responsiveness and benignness. Moreover,
we are the ﬁrst to empirically demonstrate the possibility of
malicious large-scale manipulation, and propose a concrete
solution to these shortcomings by providing researchers with
improved and publicly available rankings.
In 2006, Lo and Sedhain [46] studied the reliability of
website rankings in terms of agreement, from the standpoint of
advertisers and consumers looking for the most relevant sites.
They discussed three ranking methods (trafﬁc data, incoming
links and opinion polls) and analyzed the top 100 websites
for six providers, all of which are still online but, except for
Alexa, have since stopped updating their rankings.
Meusel et al. [52] published one-time rankings of web-
sites11, based on four centrality indices calculated on the
Common Crawl web graph [23]. Depending on the index, these
ranks vary widely even for very popular sites. Moreover, such
centrality indices can be affected by manipulation [35], [56].
In his analysis of DNS trafﬁc from a Tor exit node,
Sonntag [66] ﬁnds that popularity according to Alexa does
not
listing several domains
with a good Alexa rank but that are barely seen in the DNS
trafﬁc. These conclusions conﬁrm that different sources show
a different view of popularity, and that the Alexa list may not
be the most appropriate for all types of research (e.g. into Tor).
imply regular trafﬁc over Tor,
VIII. CONCLUSION
We ﬁnd that 133 studies in recent main security confer-
ences base their experiments on domains from commercial
rankings of the ‘top’ websites. However,
the data sources
and methods used to compile these rankings vary widely and
their details are unknown, and we ﬁnd that hidden properties
and biases can skew research results. In particular, through
an extensive evaluation of these rankings, we detect a recent
unannounced change in the way Alexa composes its list: their
data is only averaged over a single day, causing half of the list
to change every day. Most probably, this unknowingly affected
research results, and may continue to do so. However, other
rankings exhibit similar problems: e.g. only 49% of domains
in Umbrella’s list respond with HTTP status code 200, and
Majestic’s list, which Quad9 uses as a whitelist, has more than
2 000 domains marked as malicious by Google Safe Browsing.
The reputational or commercial incentives in biasing the
results of security studies, as well as the large trust placed in
the validity of these rankings by researchers, as evidenced by
only two studies putting their methods into question, makes
these rankings an interesting target for adversarial manipula-
tion. We develop techniques that exploit the pitfalls in every
list by forging the data upon which domain rankings are based.
11http://wwwranking.webdatacommons.org/
13
Moreover, many of these methods bear an exceptionally low
cost, both technically and in resources: we only needed to craft
a single HTTP request to appear in Alexa’s top million sites.
This provides an avenue for manipulation at a very large scale,
both in the rank that can be achieved and in the number of
domains artiﬁcially inserted into the list. Adversaries can there-
fore sustain massive manipulation campaigns over time to have
a signiﬁcant impact on the rankings, and, as a consequence, on
research and the society at large.
Ranking providers carry out few checks on their trafﬁc data,
as is apparent from our ability to insert nonexistent domains,
further simplifying manipulation at scale. We outline several
mitigation strategies, but cannot be assured that these will
be implemented. Therefore, we introduce TRANCO, a new
ranking based on combining the four existing lists, along-
side the ability to ﬁlter out undesirable (e.g. unavailable or
malicious) domains. These combined lists show much better
stability over time, only changing by at most 0.6% per day,
and are much more resilient against manipulation, where even
manipulating one list to reach the top 1 000 only yields a rank
of 100 000 in our combined list. We offer an online service at
https://tranco-list.eu to access these rankings in a reproducible
manner, so that researchers can continue their evaluation with
a more reliable and suitable set of domains. This helps them in
assuring the validity, veriﬁability and reproducibility of their
studies, making their conclusions about security on the Internet
more accurate and well founded.
ACKNOWLEDGMENT
The authors would like to thank Vera Rimmer, Davy
Preuveneers and Quirin Scheitle for their valuable input. This
research is partially funded by the Research Fund KU Leuven.
Victor Le Pochat holds a PhD Fellowship of the Research
Foundation - Flanders (FWO).
REFERENCES
[1]
(2018, Nov.) Alexa Trafﬁc Rank - Chrome Web Store. [Online].
Available: https://chrome.google.com/webstore/detail/alexa-trafﬁc-rank/
cknebhggccemgcnbidipinkifmmegdel
[2] M. Abu Rajab, F. Monrose, A. Terzis, and N. Provos, “Peeking through
the cloud: DNS-based estimation and its applications,” in Proc. ACNS,
2008, pp. 21–38.
[3] G. Acar, C. Eubank, S. Englehardt, M. Juarez, A. Narayanan, and
C. Diaz, “The web never forgets: Persistent tracking mechanisms in
the wild,” in Proc. CCS, 2014, pp. 674–689.
[4] L. A. Adamic and B. A. Huberman, “Zipf’s law and the Internet,”
Glottometrics, vol. 3, pp. 143–150, 2002.
[5] Alexa
Internet,
line]. Available:
//www.alexa.com:80/site/ds/top_sites
Inc.