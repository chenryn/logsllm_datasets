NOT 
manifest 
stably 
3’ 
Attack 
manifests 
stably 
3’a 
Attack does 
NOT 
manifest 
stably 
4 
Attack is 
Attack is 
anomalous 
anomalous 
within 
within 
detector’s 
detector’s 
purview 
purview 
4a 
Attack NOT 
Attack NOT 
anomalous 
anomalous 
within 
within 
detector’s 
detector’s 
purview 
purview 
4b 
Something 
Something 
else 
else 
manifests 
manifests 
as anomaly. 
as anomaly. 
4 
Attack is 
Attack is 
anomalous 
anomalous 
within 
within 
detector’s 
detector’s 
purview 
purview 
4a 
Attack NOT 
Attack NOT 
anomalous 
anomalous 
within 
within 
detector’s 
detector’s 
purview 
purview 
4b 
Something 
Something 
else 
else 
manifests as 
manifests as 
anomaly. 
anomaly. 
5 
Anomaly is 
Anomaly is 
significant 
significant 
for detector 
for detector 
5a 
Anomaly 
Anomaly 
NOT 
NOT 
significant 
significant 
for detector 
for detector 
5 
Anomaly is 
Anomaly is 
significant 
significant 
for detector 
for detector 
5a 
Anomaly 
Anomaly 
NOT 
NOT 
significant 
significant 
for detector 
for detector 
6 
Detector 
response is 
measured 
appropriately
. 
6a 
Detector 
response is 
NOT 
measured 
appropriately
. 
6 
Detector 
response is 
measured 
appropriately
. 
6a 
Detector 
response is 
NOT 
measured 
appropriately
. 
Fig. 3. Deconstruction of an anomaly detector’s response showing multiple possible
explanations of a hit or miss
to conclude a false alarm on the part of the detector. A false alarm occurs
in the absence of an attack, and in this case poor experimental control has
resulted in an alarm generated concomitantly with a deployed attack. Similarly,
we observe that there are only two cases M1 and M2 that can be assessed as
a valid and consistent “miss” because these errors can be directly attributed to
the detector and not to poor experimental control. All other cases, M3 – M18
are indeterminate due to errors that are external to the detector.
5 Case Studies
This section examines well-cited papers from literature with an eye toward un-
derstanding the conclusions that can be drawn from their presented results. We
apply the lessons learned (compiled in the framework described in Sect. 3 and
Sect. 4), and discuss the work by: (1) Mahoney et al. [28], (2) Wang et al. [29],
and (3) Kruegel et al. [26]. The results from each study are summarized in Ta-
ble 3.
300
A. Viswanathan, K. Tan, and C. Neuman
Table 2. Enumeration of a subset of the sequence of events from Fig. 3 with their
correct assessments. Assessments denoted ?? are indeterminate. Refer Fig. 3(a) for
cases H1–H12, and Fig. 3(b) for cases M1–M18.
Case
H1
H2
M1
M2
Sequence of Events
(cid:2)→4→5→6
1→2→3→3
(cid:2)→4b→5→6
1→2→3→3
(cid:2)→4→5a→6
1→2→3→3
(cid:2)→4a→5a→6
1→2→3→3
Assessment
Valid & consistent hit (TP)
FP
??
Valid & consistent miss (FN)
Valid & consistent miss (FN)
H3 – H12 
M3 – M18 
??
5.1 Mahoney et al. [28] - Evaluation of NETAD
NETAD is a network-based anomaly detection system, designed to detect attacks
on a per-packet basis by detecting unusual byte values occurring in network
packet headers [28]. NETAD was evaluated by ﬁrst training the detector oﬄine
using a subset of the 1999 DARPA dataset and then tested using 185 detectable
attacks from the dataset. A detection accuracy of 132
185 was recorded when the
detector was tuned for 100 false alarms. We were unable to reconcile three factors
that introduced uncertainty in our assessment of the presented results, while two
additional factors were found to undermine detection consistency arguments.
Some of the uncertainties that we were unable to reconcile are as follows.
We can only assume that since the well-labeled DARPA dataset was used, all
185 attacks used in the evaluation manifested in the evaluation data stream
(this is only an assumption is based on McHugh’s observations [18]). Some of
the attacks may not have manifested in the test data stream due to the data
sanitization (DP1) performed on the evaluation data stream. The sanitization
involved removing uninteresting packets and setting the TTL ﬁeld of IP headers
to 0 as the authors believed that it was a simulation artifact that would have
made detection easier. The literature suggests that data sanitization strategies
can perturb detector performance [20, 10]. Consequently, we were unable to
ascertain in the NETAD assessment weather it was veriﬁed that (a) the ﬁltering
of packets did not adversely cause any of the 185 attacks to disappear from the
test data stream, and (b) the act of setting all TTL bits to zero did not invalidate
any attacks that otherwise would have been detected because they manifest as
non-zero values in the TTL stream. In the ﬁrst case, we have an experimental
confound in that we cannot determine if the detection of 132 attacks instead of
the 185 attacks (assumed manifested in the data) was due purely to detector
capability or due to data sanitization issues. In the second case, we are unsure if
the evaluator’s act of modifying the raw data itself may have biased the results.
We know that only header-based attacks are actually detectable by NETAD
due to NETAD’s choice of data features (TR2.1), however NETAD was tested
against a mixture of header-based and payload-based attacks without specify-
ing how many of the attacks in the mixture were payload-based attacks versus
header-based attacks. Further, we are unsure if all the header-based attacks used
Deconstructing the Assessment of Anomaly-based Intrusion Detectors
301
to test NETAD did indeed manifest as anomalies within the purview of NETAD,
that is, how many of the attacks used were actually suitable for detection by the
modeling formalism used by NETAD (TR2.2). Consequently, when we are pre-
sented results whereby 132 attacks were detected, we cannot determine: 1) How
well did the detector detect header-based attacks? (Were all header-based at-
tacks detected?), 2) Did the detector also detect some payload-based attacks?
3) Did payload-based attacks manifest in ways that allowed a header-based de-
tector to detect them?, and 4) What did the detector actually detect vs. what
was detected by chance?
With regard to the consistency of the presented results, i.e., do the results
describe the detector’s capability beyond the single test instance? No, we cannot
conclude that from the results of the presented work because of the training
strategy used. It is known that variability in the training data (TR1.2) and the
amount used (TR3) can signiﬁcantly inﬂuence detector performance. Since the
authors only trained on one week’s worth of data, it is uncertain if the choice of
another week will produce the same results. The results presented in this paper
can only apply to the single evaluation instance described, and would perhaps
not persist even if another sample of the same dataset were used.
In short, we cannot conclude that the results in this paper truly reﬂect the
detector’s capability and are not biased by the artifacts of poor experimental
control (e.g., lack of precision in identifying the causal mechanisms behind the
reported 185 attacks), and we are uncertain if the results will persist beyond the
single evaluation instance.
5.2 Wang et al. [29] - Evaluation of Payload-Based Detector
PAYL is a network-based anomaly detector, designed to detect attacks on a
per-packet or per-connection basis by detecting anomalous variations in the 1-
gram byte distribution of the payload. PAYL was evaluated over real-world data
privately collected from campus web-servers and also over the DARPA 1999
dataset. The results reported were 100% hits for port 80 attacks at 0.1% false
positive rate on the DARPA dataset using connection-based payload model. We
were unable to reconcile at least two factors that introduced uncertainty in our
assessment of the presented results, while three additional factors were found to
undermine detection consistency arguments.
Some of the uncertainties that we were unable to reconcile are as follows.
Again, we assume that since the well-labeled DARPA dataset was used, all
port 80 related attacks used in the evaluation manifested in the evaluation data
stream. The evaluation data stream was ﬁltered to remove non-payload packets
(DP1). As for the previous case, it is unclear whether the ﬁltering of packets may
have perturbed attack manifestations causing them to either disappear from the
test data stream or change their manifestation characteristics. Also, we are un-
sure if all the payload-based attacks used to test PAYL did indeed manifest as
anomalies with respect to the modeling formalism used by PAYL (TR2.2). For
instance, payload attacks such as those that exploit simple conﬁguration bugs
302
A. Viswanathan, K. Tan, and C. Neuman
in servers using normal command sequences might not manifest as anomalous