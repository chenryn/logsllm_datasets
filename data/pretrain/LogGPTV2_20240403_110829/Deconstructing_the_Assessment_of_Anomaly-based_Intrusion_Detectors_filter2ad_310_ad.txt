### Deconstruction of Anomaly Detector's Response

#### Definitions and Explanations
1. **Attack Manifestation:**
   - **3'**: The attack manifests stably.
   - **3'a**: The attack does not manifest stably.

2. **Anomalous Behavior:**
   - **4**: The attack is anomalous within the detector’s purview.
   - **4a**: The attack is not anomalous within the detector’s purview.
   - **4b**: Something else manifests as an anomaly.

3. **Significance for Detector:**
   - **5**: The anomaly is significant for the detector.
   - **5a**: The anomaly is not significant for the detector.

4. **Detector Response Measurement:**
   - **6**: The detector response is measured appropriately.
   - **6a**: The detector response is not measured appropriately.

#### Figure 3: Deconstruction of Anomaly Detector's Response
Figure 3 illustrates multiple possible explanations for a hit or miss, leading to the conclusion of a false alarm by the detector. A false alarm occurs in the absence of an attack, often due to poor experimental control, which results in an alarm being generated simultaneously with a deployed attack. Only two cases, M1 and M2, can be assessed as valid and consistent "misses" because these errors can be directly attributed to the detector. All other cases (M3–M18) are indeterminate due to external errors.

### Case Studies
This section examines well-cited papers from the literature to understand the conclusions that can be drawn from their presented results. We apply the lessons learned (compiled in the framework described in Sections 3 and 4) and discuss the work by:
1. Mahoney et al. [28]
2. Wang et al. [29]
3. Kruegel et al. [26]

The results from each study are summarized in Table 3.

### Table 2: Sequence of Events and Assessments
| Case | Sequence of Events | Assessment |
|------|--------------------|------------|
| H1   | (cid:2)→4→5→6       | Valid & consistent hit (TP) |
| H2   | 1→2→3→3            | FP         |
| M1   | (cid:2)→4b→5→6      | Valid & consistent miss (FN) |
| M2   | 1→2→3→3            | Valid & consistent miss (FN) |
| H3 – H12 | (cid:2)→4→5a→6 | ??         |
| M3 – M18 | (cid:2)→4a→5a→6 | ??         |

### 5.1 Mahoney et al. [28] - Evaluation of NETAD
**NETAD** is a network-based anomaly detection system designed to detect attacks on a per-packet basis by identifying unusual byte values in network packet headers. The detector was evaluated using a subset of the 1999 DARPA dataset, trained offline, and tested with 185 detectable attacks, achieving a detection accuracy of 132/185 at 100 false alarms. However, several factors introduced uncertainty in the assessment:

1. **Assumption of Attack Manifestation**: It is assumed that all 185 attacks used in the evaluation were present in the data stream. Data sanitization (DP1) may have removed some attacks, and setting TTL fields to zero might have invalidated others.
2. **Data Sanitization Impact**: The filtering of packets and modification of TTL fields could have perturbed detector performance, making it difficult to determine if the detection of 132 attacks was due to detector capability or data issues.
3. **Header-Based vs. Payload-Based Attacks**: NETAD was tested against a mix of header-based and payload-based attacks without specifying the proportion of each. This makes it unclear how well the detector performed on header-based attacks and whether it detected any payload-based attacks.

**Consistency of Results**: The training strategy, using only one week's worth of data, raises questions about the reproducibility of the results. Variability in training data and the amount used can significantly influence detector performance.

In summary, the results may not accurately reflect the detector’s capability and are subject to biases from poor experimental control.

### 5.2 Wang et al. [29] - Evaluation of PAYL
**PAYL** is a network-based anomaly detector designed to detect attacks by analyzing anomalous variations in the 1-gram byte distribution of the payload. The detector was evaluated on real-world data and the DARPA 1999 dataset, reporting 100% hits for port 80 attacks at a 0.1% false positive rate. Uncertainties include:

1. **Assumption of Attack Manifestation**: It is assumed that all port 80 related attacks were present in the evaluation data stream. Filtering non-payload packets (DP1) may have perturbed attack manifestations.
2. **Payload-Based Attacks**: It is unclear if all payload-based attacks used to test PAYL manifested as anomalies with respect to the modeling formalism used by PAYL. Some payload attacks, such as those exploiting configuration bugs, might not appear anomalous.

**Consistency of Results**: The evaluation data stream and the specific nature of the attacks make it difficult to generalize the results beyond the single evaluation instance.

In summary, the results are subject to uncertainties and may not fully reflect the detector’s capability.