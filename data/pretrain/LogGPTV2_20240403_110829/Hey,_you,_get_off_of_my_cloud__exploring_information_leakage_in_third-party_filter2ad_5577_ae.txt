The protocol has three parameters: a which is larger
than the attacked cache level (e.g., a = 221 to attack the
EC2’s Opteron L2 cache), b which is slightly smaller than
the attacked cache level (here, b = 219), and d which is
the cache line size times a power of 2. Deﬁne even ad-
dresses (resp. odd addresses) as those that are equal to
0 mod 2d (resp. d mod 2d). Deﬁne the class of even cache
sets (resp. odd cache sets) as those cache sets to which even
(resp. odd) addresses are mapped.
The sender allocates a contiguous buﬀer A of a bytes. To
transmit “0” (resp. 1) he reads the even (resp. odd) addresses
in A. This ensures that the one class of cache sets is fully
evicted from the cache, while the other is mostly untouched.
The receiver deﬁnes the diﬀerence by the following mea-
surement procedure:
(1) Allocate a contiguous buﬀer B of b bytes
(2) Sleep brieﬂy (to build up credit with Xen’s scheduler).
(3) Prime: Read all of B to make sure it’s fully cached.
(4) Trigger: Busy-loop until the CPU’s cycle counter jumps
by a large value. (This means our VM was preempted by
the Xen scheduler, hopefully in favor of the sender VM.)
(5) Probe: Measure the time it takes to read all even ad-
dresses in B, likewise for the odd addresses. Decide “0”
iﬀ the diﬀerence is positive.
On EC2 we need to deal with the noise induced by the fact
that each VM’s virtual CPU is occasionally migrated be-
tween the (m1.small) machine’s four cores. This also leads
to sometimes capturing noise generated by VMs other than
the target (sender). Due to the noise-cancelling property of
diﬀerential encoding, we can use a straightforward strategy:
the receiver takes the average of multiple samples for making
his decision, and also reverts to the prime stage whenever it
detects that Xen scheduled it to a diﬀerent core during the
trigger or probe stages. This simple solution already yields
a bandwidth of approximately 0.2bps, running on EC2.
8.2 Load-based co-residence detection
Here we positively answer the following question: can one
test co-residence without relying on the network-based tech-
niques of Section 6? We show this is indeed possible, given
some knowledge of computational load variation on the tar-
get instance. This condition holds when an adversary can
actively cause load variation due to a publicly-accessible ser-
vice running on the target. It might also hold in cases where
an adversary has a priori information about load variation
on the target and this load variation is (relatively) unique
to the target.
Consider target instances for which we can induce compu-
tational load — for example, an instance running a (public)
web server. In this case, an attacker instance can check for
co-residence with a target instance by observing diﬀerences
in load samples taken when externally inducing load on the
target versus when not. We experimentally veriﬁed the eﬃ-
cacy of this approach on EC2 m1.small instances. The target
instance ran Fedora Core 4 with Apache 2.0. A single 1 024-
byte text-only HTML page was made publicly accessible.
Then the co-residence check worked as follows. First, the
attacker VM took 100 load samples. (We set b = 768 ∗ 1024
and s = 128. Taking 100 load samples took about 12 sec-
onds.) We then paused for ten seconds. Then we took 100
further load samples while simultaneously making numer-
ous HTTP get requests from a third system to the target
via jmeter 2.3.4 (a utility for load testing HTTP servers).
We set jmeter to simulate 100 users (100 separate threads).
Each user made HTTP get requests as fast as possible.
The results of three trials with three pairs of m1.small
instances are plotted in Figure 5. In the ﬁrst trial we used
two instances known to be co-resident (via network-based
co-residence checks). One can see the diﬀerence between
the load samples when performing HTTP gets and when
not.
In the second trial we used a fresh pair of instances
co-resident on a diﬀerent machine, and again one can easily
see the eﬀect of the HTTP gets on the load samples. In the
third trial, we used two instances that were not co-resident.
Here the load sample timings are, as expected, very similar.
We emphasize that these measurements were performed on
live EC2 instances, without any knowledge of what other
instances may (or may not) have been running on the same
machines.
Indeed, the several spikes present in Trial 2’s
208HTTP gets
No HTTP gets
2e+06
1.5e+06
1e+06
500000
s
e
l
c
y
c
U
P
C
0
0 10 20 30 40 50 60 70 80 90 100
0 10 20 30 40 50 60 70 80 90 100
0 10 20 30 40 50 60 70 80 90 100
Trial 1
Trial 2
Trial 3
Figure 5: Results of executing 100 Prime+Trigger+Probe cache timing measurements for three pairs of
m1.small instances, both when concurrently making HTTP get requests and when not. Instances in Trial 1
and Trial 2 were co-resident on distinct physical machines. Instances in Trial 3 were not co-resident.
load measurements were likely due to a third co-resident
instance’s work load.
8.3 Estimating trafﬁc rates
In theory load measurement might provide a method for
estimating the number of visitors to a co-resident web server
or even which pages are being frequented. In many cases this
information might not be public and leaking it could be dam-
aging if, for example, the co-resident web server is operated
by a corporate competitor. Here we report on initial exper-
imentation with estimation, via side channel measurements,
of HTTP traﬃc rates to a co-resident web server.
We utilized two m1.small instances, as in the trials dis-
cussed above. We then instructed one of the instances to
perform four separate runs of 1 000 cache load measurements
in which we simultaneously (1) sent no HTTP requests, (2)
sent HTTP requests at a rate of 50 per minute, (3) 100
per minute, or (4) 200 per minute. As before we used jme-
ter to make the requests, this time with 20 users and the
rate maintained across all users. Taking 1 000 load mea-
surements takes about 90 seconds. The requested web page
was a 3 megabyte text ﬁle, which ampliﬁed server load per
request compared to a smaller page. We repeated this ex-
periment three times (with the same instances). The graph
in Figure 6 reports the mean load samples from these three
trials, organized according to traﬃc rate. Note that among
the 12 000 samples taken, 4 were extreme outliers (2 orders
of magnitude larger than all other samples, for reasons un-
clear); we omitted these outliers from the calculations.
Figure 6 shows a clear correlation between traﬃc rate and
load sample. This provides evidence that an attacker might
be able to surreptitiously estimate traﬃc rates in some cases.
8.4 Keystroke timing attack
In this section we describe progress on the use of cache-
based load measurements as a means for mounting keystroke
timing attacks [31]. In such an attack, the adversary’s goal
is to measure the time between keystrokes made by a victim
typing a password (or other sensitive information) into, for
example, an SSH terminal. The gathered inter-keystroke
times (if measured with suﬃcient resolution) can then be
used to perform recovery of the password. In prior work [31],
the attacker was assumed to have a network tap to time
packet arrivals. In third-party compute clouds, we can re-
place network taps with co-residence and load measurements:
Trial 1
Trial 2
Trial 3
s
e
l
c
y
c
U
P
C
n
a
e
M
800000
700000
600000
500000
400000
300000
200000
0
50
100
200
HTTP gets per minute
Figure 6: Mean cache load measurement timings
(over 1 000 samples) taken while diﬀering rates of
web requests were made to a 3 megabyte text ﬁle
hosted by a co-resident web server.
when the user of a co-resident instance types sensitive infor-
mation into the shell, the malicious VM can observe the
keystroke timings in real time via cache-based load mea-
surements. Our hypothesis is that, on an otherwise idle ma-
chine, a spike in load corresponds to a letter being typed into
the co-resident VM’s terminal. We experimentally validated
this hypothesis on an EC2-like virtualized environment, de-
scribed next.
Experimental setup. For what comes below, we ran our
experiments not on EC2 but on a local testbed, running a
conﬁguration of Opteron CPUs, Xen hypervisor and Linux
kernels that is very similar (to the extent that we could
discern) to that used to host EC2 m1.small instances — but
with the VMs pinned to speciﬁc cores. EC2, in contrast,
occasionally migrates the instances’ virtual CPUs between
the machine’s four cores. We refer to our testbed as the
pinned Xen machine.
The pinned Xen machine avoids two technical complica-
tions in conducting the attacks on EC2: it ensures that the
machine is completely idle other than the test code, and it
allows us to know the relation between VMs involved in the
attack (i.e., whether they timeshare the same core, are as-
signed to two cores in the same physical chip, or neither).
Machines in EC2 might indeed be idle (or at least have
one idle core, which suﬃces for our attack) during a non-
negligible fraction of their time, especially during oﬀ-peak
209hours; and the assignment of virtual CPUs changes often
enough on EC2 that any desired combination will be oc-
casionally achieved. That we were able to establish reliable
covert channels via the cache in EC2 already testiﬁes to this.
A patient attacker might just wait for the requisite condi-
tion to come up, and detect them by adding redundancy to
his transmission.
Note also that the pinned Xen machine is by itself a real-
istic setup for virtualized environments. In fact, in a cloud
data center that never over-provisions CPU resources, pin-
ning VMs to CPUs may improve performance due to caches
and NUMA locality eﬀects. We thus feel that these attacks
are of interest beyond their being progress towards an attack
within EC2.
Keystroke activity side channel. We utilize the Prime+
Trigger+Probe load measurement technique to detect mo-
mentary activity spikes in an otherwise idle machine.
In
particular, we repeatedly perform load measurements and
report a keystroke when the measurement indicates momen-
tarily high cache usage. Further analysis of which cache
sets were accessed might be used to ﬁlter out false positives,
though we found that in practice it suﬃces to use simple
thresholding, e.g., reporting a keystroke when the probing
measurement is between 3.1µs and 9µs (the upper threshold
ﬁlters out unrelated system activity).
We have implemented and evaluated this attack on the
pinned Xen machine, with variants that exploit either L1 or
L2 cache contention. The attacking VM is able to observe a
clear signal with 5% missed keystrokes and 0.3 false triggers
per second. The timing resolution is roughly 13ms. There
is also a clear diﬀerence between keys with diﬀerent eﬀects,
e.g., typing a shell command vs. pressing Enter to execute
it. While the attacker does not directly learn exactly which
keys are pressed, the attained resolution suﬃces to conduct
the password-recovery attacks on SSH sessions due to Song
et al. [31].
The same attack could be carried over to EC2, except that
this measurement technique applies only to VMs that time-
share a core. Thus, it can only reliably detect keystrokes
during periods when EC2’s Xen hypervisor assigns the at-
tacker and victim to the same core. Assuming uniformly ran-
dom assignment, this is about 25% of the time (and changes
at most every 90ms, typically much slower). Statistical anal-
ysis of measurements might be used to identify periods of
lucky allocation. We conjecture that measuring not just the
overall momentary load, but also the use of individual cache
associativity sets during the trigger stage, might further help
identify the target VM.
8.5 Inhibiting side-channel attacks
One may focus defenses against cross-VM attacks on pre-
venting the side-channel vulnerabilities themselves. This
might be accomplished via blinding techniques to minimize
the information that can be leaked (e.g., cache wiping, ran-
dom delay insertion, adjusting each machine’s perception of
time [14], etc.). Countermeasures for the cache side channels
(which appear to be particularly conducive to attacks) are
extensively discussed, e.g., in [23, 24, 10, 26, 25, 22]. These
countermeasures suﬀer from two drawbacks. First, they are
typically either impractical (e.g., high overhead or nonstan-
dard hardware), application-speciﬁc, or insuﬃcient for fully
mitigating the risk. Second, these solutions ultimately re-
quire being conﬁdent that all possible side-channels have
been anticipated and disabled — itself a tall order, especially
in light of the deluge of side channels observed in recent
years. Thus, at the current state of the art, for uncondi-
tional security against cross-VM attacks one must resort to
avoiding co-residence.
9. CONCLUSIONS
In this paper, we argue that fundamental risks arise from
sharing physical infrastructure between mutually distrustful
users, even when their actions are isolated through machine
virtualization as within a third-party cloud compute service.
However, having demonstrated this risk the obvious next
question is “what should be done?”.
There are a number of approaches for mitigating this risk.
First, cloud providers may obfuscate both the internal struc-
ture of their services and the placement policy to complicate
an adversary’s attempts to place a VM on the same physi-
cal machine as its target. For example, providers might do
well by inhibiting simple network-based co-residence checks.
However, such approaches might only slow down, and not
entirely stop, a dedicated attacker. Second, one may focus
on the side-channel vulnerabilities themselves and employ
blinding techniques to minimize the information that can be
leaked. This solution requires being conﬁdent that all pos-
sible side-channels have been anticipated and blinded. Ulti-
mately, we believe that the best solution is simply to expose
the risk and placement decisions directly to users. A user
might insist on using physical machines populated only with
their own VMs and, in exchange, bear the opportunity costs
of leaving some of these machines under-utilized. For an
optimal assignment policy, this additional overhead should
never need to exceed the cost of a single physical machine, so
large users — consuming the cycles of many servers — would
incur only minor penalties as a fraction of their total cost.
Regardless, we believe such an option is the only foolproof
solution to this problem and thus is likely to be demanded
by customers with strong privacy requirements.
Acknowledgments
Eran Tromer thanks Ron Rivest and Saman Amarasinghe
for discussion of potential attacks. This work was supported
by NSF grants NSF-0433668, CNS-0808907, CNS-0524765,
CNS-0627779 and NSF-0829469, AFRL grant FA8750-08-1-