title:Reducing Recovery Time in a Small Recursively Restartable System
author:George Candea and
James W. Cutler and
Armando Fox and
Rushabh Doshi and
Priyank Garg and
Rakesh Gowda
Reducing Recovery Time in a Small
Recursively Restartable System
George Candea, James Cutler, Armando Fox, Rushabh Doshi, Priyank Garg, Rakesh Gowda
fcandea,jwc,fox,radoshi,priyank,PI:EMAIL
Stanford University
Abstract
We present ideas on how to structure software sys-
tems for high availability by considering MTTR/MTTF char-
acteristics of components in addition to the traditional
criteria, such as functionality or state sharing. Recur-
sive restartability (RR), a recently proposed technique for
achieving high availability, exploits partial restarts at var-
ious levels within complex software infrastructures to re-
cover from transient failures and rejuvenate software com-
ponents. Here we reﬁne the original proposal and apply the
RR philosophy to Mercury, a COTS-based satellite ground
station that has been in operation for over 2 years. We de-
velop three techniques for transforming component group
boundaries such that time-to-recover is reduced, hence in-
creasing system availability. We also further RR by deﬁning
the notions of an oracle, restart group and restart policy,
while showing how to reason about system properties in
terms of restart groups. From our experience with apply-
ing RR to Mercury, we draw design guidelines and lessons
for the systematic application of recursive restartability to
other software systems amenable to RR.
1. Introduction
The Software Infrastructures Group (SWIG) and Space
Systems Development Lab (SSDL) at Stanford are collabo-
rating on the design and deployment of space communica-
tions infrastructure to make collection of satellite-gathered
science data less expensive and more reliable. One nec-
essary element of satellite operations is a ground station,
a ﬁxed installation that includes tracking antennas, radio
communication equipment, orbit prediction calculators, and
other control software. When a satellite appears in the patch
of sky whose angle is subtended by the antenna, the ground
station collects telemetry and data from the satellite.
In
keeping with the strong movement in the aerospace research
community to design ground stations around COTS (com-
mercial off-the-shelf) technology [11], part of the collabo-
ration between SSDL and SWIG includes the design and
deployment of Mercury, a prototype ground station that in-
tegrates COTS components.
A current goal in the design and deployment of Mercury
is to improve ground station availability, as it was not origi-
nally designed with high availability in mind. Our ﬁrst step
in improving the availability of Mercury was to apply recur-
sive restartability [4], an approach to system recovery that
advocates “curing” transient failures by restarting suitably
chosen subsystems, such that overall mean-time-to-recover
(MTTR) is minimized. Recursive restartability is a concrete
example of the recovery-oriented computing (ROC) philos-
ophy [12], as applied to COTS-based systems. To our
knowledge, this paper describes the ﬁrst deployed system
to be systematically retroﬁtted to exploit recursive restarta-
bility (RR).
We had two main goals in applying RR to Mercury. The
ﬁrst was to partially remove the human from the loop in
ground station control by automating recovery from com-
mon transient failures we had observed and knew to be cur-
able through full or partial restarts. In particular, although
all such failures are curable through a brute force restart of
the entire system, we sought a strategy with lower MTTR
than full system reboots. The second goal was to identify
design guidelines and lessons for the systematic future ap-
plication of RR to other systems. For example, we found
that, if one adopts a transient-recovery strategy based on
partial restarts, redrawing the boundaries of software com-
ponents based on their mean-times-to-failure (MTTFs) and
mean-times-to-recover (MTTRs) can minimize overall sys-
tem MTTR by enabling the tuning of which components are
restarted together. In contrast, most current system and soft-
ware engineering approaches establish software component
boundaries based solely on considerations such as amount
and overhead of communication between components or
amount and granularity of state sharing.
The paper is organized as follows: we provide neces-
sary background on the architecture of our ground station
and its failure detection mechanisms in section 2, followed
by an overview of recursive restartability concepts in sec-
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:24:15 UTC from IEEE Xplore.  Restrictions apply. 
tion 3. In section 4 we describe a set of transformations by
which we improve the mean-time-to-recover of the ground
station, hence improving its availability. In section 5 we ex-
tract lessons from our experience. We address related work
in section 6, propose future avenues of research in section 7,
and conclude in section 8.
2. Mercury Overview
In this section we describe Mercury, our ground station
prototype. Although the focus of this paper is the effective-
ness of recursive restartability as a recovery strategy, recov-
ery is only possible once failures are detected. As the orig-
inal Mercury design had little in the way of failure detec-
tion, we describe the simple failure detection mechanisms
we added to enable recursive restartability.
2.1. Ground Station Architecture
The Mercury ground station communicates with low
earth orbit satellites at data speeds up to 38.4 kbps. For the
past two years, the Mercury system has been used in 10-20
satellite passes per week as a primary communication sta-
tion for Stanford’s satellites Opal [6] and Sapphire [14].
The station, composed primarily of COTS hardware and
software written mostly in Java, is controlled both remotely
and locally via a high-level, XML-based command lan-
guage. Software components are independently operating
processes with autonomous loci of control and interoperate
through passing of messages composed in our XML com-
mand language. Messages are exchanged over a TCP/IP-
based software messaging bus.
The general software architecture is shown in Figure 1.
fedc is a bidirectional proxy between XML command
messages and low-level radio commands; e (satellite es-
timator) calculates satellite position, radio frequencies, and
antenna pointing angles;  (satellite tracker) points anten-
nas to track a satellite during a pass; 	 (radio tuner) tunes
the radios during a satellite pass; b	 passes XML-based
high-level command messages between software compo-
nents. REC and FD will be described in the next section.
 REC 
 FD 
  mbus 
failure detection
restarts
 fedrcom 
ses
 str 
 rtu 
communication (TCP/IP)
Figure 1. Mercury software architecture
The ground station components are easily restartable,
since most are stateless; they use only the state explicitly
encapsulated by received messages from b	. Hard state
exists, but is read-only during a satellite pass and is modi-
ﬁed off-line by ground station users. In addition, the set of
Mercury failures that can be successfully cured by restart
is large, and in fact this is how human operators recovered
from most Mercury failures before we implemented auto-
mated recovery.
2.2. Adding Failure Detection to Mercury
Adding failure detection to this architecture was moti-
vated by the need to automate detection of several common
failure modes; we understood these modes from extensive
past experience with Mercury. All the failures we focused
on were fail-silent: when components failed, they simply
stopped responding to messages (e.g., when the JVM con-
taining a component crashed). Moreover, all failures were
curable through restart of either a single software compo-
nent or a group of such components.
Given the fail-silent property, we chose application-level
liveness pings (i.e., “are you alive?” messages) sent to a
component via the software message bus, b	. The pings
are encoded in and replied to in a high-level XML command
language, so a successful response indicates the compo-
nent’s liveness with higher conﬁdence than a network-level
ICMP ping. Application-level liveness pings are simple and
low-cost, and effectively detect all fail-silent failures that
humans were detecting before in the ground station, thus
satisfying the immediate goal of automated failure detec-
tion.
Figure 1 illustrates Mercury’s simple failure detection
architecture, based on the addition of two new indepen-
dent processes: the failure detector (FD) and the recovery
module (REC). FD continuously performs liveness pings
on Mercury components, with a period of 1 second, deter-
mined from operational experience to minimize detection
time without overloading b	. When FD detects a failure,
it tells REC which component(s) appear to have failed, and
continues its failure detection. For improved isolation, FD
and REC communicate over a separate dedicated TCP con-
nection, not over b	; b	 itself is monitored as well.
REC uses a restart tree data structure and a simple policy to
choose which module(s) to restart upon being notiﬁed of a
failure. The policy also keeps track of past restarts to pre-
vent inﬁnite restarts of “hard” failures. Once REC restarts
the chosen modules, future application-level pings issued
from FD should indicate the failed components are alive
and functioning again. However, if the restart does not cure
the failure, FD will redetect it and notify REC, which may
choose to restart a different module this time, and so on.
Given the above strategy, two situations can arise, which
we handle with special case code. First, FD may fail, so we
wrote REC to issue liveness pings to FD and detect its fail-
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:24:15 UTC from IEEE Xplore.  Restrictions apply. 
ure, after which it can initiate FD recovery. Second, REC
may go down, in which case FD detects the failure and ini-
tiates REC’s recovery, although the generalized procedural
knowledge for how to choose the modules to restart and ini-
tiate recovery is only in REC.
Splitting FD and REC requires the above two cases to be
handled separately, but it results in a separation of concerns
between the modules and eliminates a potential single point
of failure. Our enhanced ground station can tolerate any sin-
gle and most multiple software failures, with the exception
of FD and REC failing together.
It is important to note that, in our system, restarts are a
recovery mechanism based on detecting failures, not faults.
Response to a failure is independent of the fault that caused
the failure. Restarting can be used in addition to other re-
covery strategies, not necessarily in place of them, so we
do not believe that anything we have done precludes the use
of more sophisticated failure detection or high availability
mechanisms (such as redundancy) in the future.
3. Recursive Restartability
It is very common for bugs to make software systems
crash, deadlock, spin, livelock, or develop such severe soft
state corruption—memory leaks, dangling pointers, dam-
aged heaps—that the only high-conﬁdence way of con-
tinuing is to restart the application(s) or reboot the sys-
tem [3, 7, 13, 1]. Recursive restartability is predicated
on our belief that this state of affairs will remain a fact
of life, both due to the increasing complexity of software
and the increasing cost of chasing and resolving elusive
bugs. While deﬁnitely not an encouragement to develop
poor quality software, recursive restartability (RR) provides
a way to deal with some of the drawbacks of using inexpen-
sive COTS software, particularly after deployment.
Restarts provide an effective and immediate workaround
for transient failures, as they (a) unequivocally return soft-
ware to its start state, which is usually the best understood
and tested state of the system, (b) provide a high conﬁdence
way to reclaim resources that are stale or leaked, and (c) are
easy to understand and employ. Unfortunately, most sys-
tems do not tolerate restarts well: restarting is often very
expensive in terms of time-to-recover, may cause loss of
hard state, and bounded restarting of only those subsystems
that are faulty is usually not supported.
A recursively restartable system gracefully tolerates
successive restarts at multiple levels. Due to its ﬁne
restart granularity, an RR system enables bounded, par-
tial restarts that recover a failed system faster than a full
reboot. Availability is generally thought of as the ra-
tio MTTF/(MTTF +MTTR); recursive restartability improves
this ratio by reducing MTTR with reactive restarts of failed
subsystems, and by increasing MTTF with a bounded form
of software rejuvenation [9]. The focus in this paper is
on reducing MTTR, both because that is the emphasis of
recovery-oriented computing and because it is easier in in-
dustrial and research practice to measure MTTR than MTTF.
3.1. Restart Trees
A recursively restartable system can be described by
a restart tree—a hierarchy of restartable components, in
which nodes are highly fault-isolated and a restart at a node
will restart the entire corresponding subtree. A restart tree
does not directly capture functional or state dependencies
among components, but rather the “restart dependencies,”
expressing how each component is affected by the restart
of other components around it. It must be recognized, how-
ever, that the very deﬁnition of components is tightly related
to functional and state dependencies among the parts of the
system, so the restart tree does embody such considerations,
albeit indirectly. In Figure 2 we show a simple restart tree
with 5 nodes, called restart cells. A restart cell is the unit of
recovery in a recursively restartable system. Each cell RA,
RB, RC, RBC, RABC conceptually has a “button” that can
be “pushed” to cause the restart of the entire subtree rooted
at that node.
R
ABC
R
A
A
R BC
R
B
B
RC
C
Figure 2. A restart tree
We attach to the leaves of the restart tree annotations