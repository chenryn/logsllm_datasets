### 25th USENIX Security Symposium
**USENIX Association**

#### Reverse Engineered Dual-Channel Mapping for Different Architectures

The channel selection function has evolved with later microarchitectures, such as Ivy Bridge and Haswell. As shown in Figure 4b, the channel-selection bit is now computed by XORing seven bits of the physical address. Further analysis revealed that bit a7 is used exclusively, meaning it is not part of the row or column address.

Additionally, rank selection is now similar to bank addressing and also uses XORs. Our Skylake test system uses DDR4 instead of DDR3. Due to DDR4's introduction of bank grouping and the doubling of available banks (now 16), the addressing function necessarily changed again. As shown in Figure 4c, a7 is no longer used for channel selection but for bank addressing instead. Figure 4d depicts the memory mapping of a dual-CPU Haswell-EP system equipped with DDR4 memory. It uses two modules in a dual-channel configuration per CPU (four DIMMs in total). In interleaved mode (see Section 2.2), the chosen CPU is determined by a7 ⊕ a17. Apart from the different channel function, there is also a difference in bank addressing, i.e., bank addressing bits are shifted. The range of bits used for row indexing is now split into address bits (a17..a19) and a23 upwards.

The mapping used on one of our mobile platforms, a Samsung Galaxy S6 with an Exynos 7420 ARMv8-A SoC and LPDDR4 memory, is much simpler (see Figure 4e). Here, physical address bits are mapped directly to bank address bits. Rank and channel are computed with XORs of only two bits each. The bus width of LPDDR4 is 32 bits, so only the two lowest bits are used for byte indexing in a memory word.

Table 2 provides a comprehensive overview of all platforms and memory configurations we analyzed. Since all found functions are linear, we simply list the index of the physical address bits that are XORed together. For example, with the Haswell microarchitecture, one can clearly see that the indices are shifted to accommodate different memory setups. In single-channel configurations, a7 is used for column instead of channel selection, which is why bank addressing starts with a13 instead of a14.

#### A High-Speed Cross-CPU Covert Channel

In this section, we present a DRAMA attack: a high-speed cross-CPU covert channel that does not require shared memory. Our channel exploits the row buffer, which behaves like a directly-mapped cache. Unlike cache attacks, the only prerequisite is that two communicating processes have access to the same memory module.

##### Basic Concept

Our covert channel exploits timing differences caused by row conflicts. The sender and receiver occupy different rows in the same bank, as illustrated in Figure 5. The receiver process continuously accesses a chosen physical address in the DRAM and measures the average access time over a few accesses. If the sender process now continuously accesses a different address in the same bank but in a different row, a row conflict occurs, leading to higher average access times in the receiver process. Bits can be transmitted by switching the activity of the sender process in the targeted bank on and off. This timing difference is illustrated in Figure 6, and an exemplary transmission is shown in Figure 7. The receiver process distinguishes the two values based on the mean access time. We assign a logic value of 0 to low access times (the sender is inactive) and a value of 1 to high access times (the sender is active).

Each (CPU, channel, DIMM, rank, bank) tuple can be used as a separate transmission channel. However, a high number of parallel channels leads to increased noise. There is also a strict limit on the usable bank parallelism. Thus, optimal performance is achieved when using only a subset of available tuples. Transmission channels are unidirectional, but the direction can be chosen independently for each one, allowing two-way communication.

To evaluate the performance of this new covert channel, we created a proof-of-concept implementation. We restrict ourselves to unidirectional communication, i.e., there is one dedicated sender and one dedicated receiver. The memory access time is measured using rdtsc, and the memory accesses are performed using volatile pointers. To cause a DRAM access for each request, data must be flushed from the cache using clflush.

##### Determining Channel, Rank, and Bank Address

In an agreement phase, all parties need to agree on the set of (channel, DIMM, rank, bank) tuples that are used for communication. This set needs to be chosen only once, and all subsequent communication can use the same set. Next, both sender and receiver need to find at least one address in their respective address space for each tuple. Note that some operating systems allow unprivileged resolution of virtual to physical addresses, making finding correct addresses trivial. However, on Linux, which we used on our testing setup, unprivileged address resolution is not possible. Thus, we use the following approach:

As observed in previous work [3, 4], system libraries and the operating system assign 2 MB pages for arrays significantly larger than 2 MB. On these pages, the 21 lowest bits of the virtual address and the physical address are identical. Depending on the hardware setup, these bits can already be sufficient to fully determine bank, rank, and channel address. Both processes request a large array. The start of this array is not necessarily aligned with a 2 MB border. Memory before such a border is allocated using 4 KB pages. We skip to the next 2 MB page border by choosing the next virtual address with the 21 lowest bits set to zero.

On systems that also use higher bits, an attacker can use the following approach, explained on the example of the mapping shown in Figure 4b. An attacker cannot determine the BA2 bit by just using 2 MB pages. The receiving process selects addresses with chosen BA0, BA1, rank, and channel, but unknown BA2 bit. The sender now accesses addresses for both possibilities of BA2, e.g., by toggling a17 between consecutive reads. Thus, only every second access in the sending process targets the correct bank. Yet, due to bank parallelism, this does not cause a notable performance decrease. However, this approach might not work if the number of unknown bank-address bits is too high.

In a virtualized environment, even a privileged attacker can retrieve only the guest physical address, which is further translated into the real physical address by the memory management unit. If the host system uses 1 GB pages for the second-level address translation (to improve efficiency), then the lowest 30 bits of the guest physical address are identical to the real physical address. Knowledge of these bits is sufficient on all systems we analyzed to use the full DRAM addressing functions.

Finally, the covert channel could also be built without actually reconstructing the DRAM addressing functions. Instead of determining the exact bank address, it can rely solely on the same-bank sets retrieved in Section 4.3. In an initialization phase, both sender and receiver perform the timing analysis and use it to build sets of same-bank addresses. Subsequently, the communicating parties need to synchronize their sets, i.e., they need to agree on which of them is used for transmission. This is done by sending predefined patterns over the channel. After that, the channel is ready for transmission. Thus, it can be established without having any information on the mapping function nor on the physical addresses.

##### Synchronization

In our proof-of-concept implementation, one set of bits (a data block) is transmitted for a fixed time span agreed upon before starting communication. Decreasing this period increases the raw bitrate but also increases the error rate, as shown in Figure 8. If the sender and receiver run in two different VMs, a common (or perfectly synchronized) wall clock is typically not available. In this case, the sender uses one of the transmission channels to transmit a clock signal that toggles at the beginning of each block. The receiver then recovers this clock and can thus synchronize with the sender.

We employ multiple threads for both the sender and receiver processes to achieve optimal usage of the memory bus. Thus, memory accesses are performed in parallel, increasing the performance of the covert channel.

##### Evaluation

We evaluated the performance of our covert-channel implementation on two systems. First, we performed tests on a standard desktop PC featuring an Intel i7-4790 CPU with Haswell microarchitecture. It was equipped with two Kingston DDR3 KVR16N11/8 dual-rank 8 GB DIMMs in dual-channel configuration. The system was mostly idle during the tests, i.e., there were no other tasks causing significant load on the system. The DRAM clock was set to its default of 800 MHz (DDR3-1600).

Furthermore, we tested the capability of cross-CPU transmission on a server system. Our setup had two Intel Xeon E5-2630 v3 (Haswell-EP microarchitecture) equipped with a total of four Samsung M393A2G40DB0-CPB DDR4 registered ECC DIMMs. Each CPU was connected to two DIMMs in dual-channel configuration, and NUMA was set to interleaved mode. The DRAM frequency was set to its maximum supported value (DDR4-1866).

For both systems, we evaluated the performance in both a native scenario (both processes run natively) and a cross-VM scenario. We transmit 8 bits per block (using 8 (CPU, channel, DIMM, rank, bank) tuples) in the covert channel and run two threads in both the sender and receiver processes. Every thread is scheduled to run on different CPU cores, and in the case of the Xeon system, sender and receiver run on different physical CPUs.

We tested our implementation with a large range of measurement intervals. For each one, we measure the raw channel capacity and the bit error probability. While the raw channel capacity increases proportionally to the reduction of the measurement time, the bit error rate increases significantly if the measurement time is too short. To find the best transmission rate, we use the channel capacity as a metric. When using the binary symmetric channel model, this metric is computed by multiplying the raw bitrate with 1− H(e), where e is the bit error probability and H(e) = −e·log2(e)− (1−e)·log2(1−e) is the binary entropy function.

Figure 8 shows the error rate varying depending on the raw bitrate for the case that both sender and receiver run natively. For synchronizing the start of these blocks, we employ two different mechanisms. If the sender and receiver run natively, we use the wall clock as a means of synchronization, with blocks starting at fixed points in time. If, however, the sender and receiver run in two different VMs, a common (or perfectly synchronized) wall clock is typically not available. In this case, the sender uses one of the transmission channels to transmit a clock signal that toggles at the beginning of each block. The receiver then recovers this clock and can thus synchronize with the sender.