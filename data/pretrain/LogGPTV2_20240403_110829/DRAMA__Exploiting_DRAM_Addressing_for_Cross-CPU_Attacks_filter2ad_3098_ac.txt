22
21
20
19
18
17
16
15
14
13
12
11 10
6789
...
(e) Samsung Exynos 7420 – LPDDR4.
Figure 4: Reverse engineered dual channel mapping (1
DIMM per channel) for different architectures.
The channel selection function changed with later mi-
croarchitectures, such as Ivy Bridge and Haswell. As
shown in Figure 4b, the channel-selection bit is now
computed by XORing seven bits of the physical address.
Further analysis showed that bit a7 is used exclusively,
i.e., it is not used as part of the row- or column address.
USENIX Association  
25th USENIX Security Symposium  571
7
572  25th USENIX Security Symposium 
USENIX Association
Additionally,rankselectionisnowsimilartobankad-dressingandalsousesXORs.OurSkylaketestsystemusesDDR4insteadofDDR3.DuetoDDR4’sintroductionofbankgroupingandthedoublingoftheavailablebanks(now16),theaddressingfunctionnecessarilychangedagain.AsshowninFig-ure4c,a7isnotusedforchannelselectionanymore,butforbankaddressinginstead.Figure4ddepictsthememorymappingofadual-CPUHaswell-EPsystemequippedwithDDR4mem-ory.Ituses2modulesindual-channelconfigurationperCPU(4DIMMsintotal).Ininterleavedmode(cf.Section2.2),thechosenCPUisdeterminedasa7⊕a17.Apartfromthedifferentchannelfunction,thereisalsoadifferenceinthebankaddressing,i.e.,bankaddressingbitsareshifted.Therangeofbitsusedforrowindexingisnowsplitintoaddressbits(a17..a19)anda23upwards.Themappingusedononeofourmobileplatforms,aSamsungGalaxyS6withanExynos7420ARMv8-ASoCandLPDDR4memory,ismuchsimpler(cf.Fig-ure4e).Herephysicaladdressbitsaremappeddirectlytobankaddressbits.RankandchannelarecomputedwithXORsofonlytwobitseach.ThebuswidthofLPDDR4is32bits,soonlythetwolowestbitsareusedforbyteindexinginamemoryword.Table2showsacomprehensiveoverviewofallplat-formsandmemoryconfigurationsweanalyzed.Asallfoundfunctionsarelinear,wesimplylisttheindexofthephysicaladdressbitsthatareXORedtogether.WiththeexampleoftheHaswellmicroarchitecture,onecanclearlyseethattheindicesareshiftedtoaccommodateforthedifferentmemorysetups.Forinstance,insingle-channelconfigurationsa7isusedforcolumninsteadofchannelselection,whichiswhybankaddressingstartswitha13insteadofa14.5Ahigh-speedcross-CPUcovertchannelInthissection,wepresentafirstDRAMAattack,namelyahigh-speedcross-CPUcovertchannelthatdoesnotrequiresharedmemory.Ourchannelexploitstherowbuffer,whichbehaveslikeadirectly-mappedcache.Un-likecacheattacks,theonlyprerequisiteisthattwocom-municatingprocesseshaveaccesstothesamememorymodule.5.1BasicconceptOurcovertchannelexploitstimingdifferencescausedbyrowconflicts.SenderandreceiveroccupydifferentrowsinthesamebankasillustratedinFigure5.ThereceiverprocesscontinuouslyaccessesachosenphysicaladdressintheDRAMandmeasurestheaverageaccesstimeoverafewaccesses.IfthesenderprocessnowcontinuouslyRow BuﬀerReceiverReceiverReceiverReceiverSenderSenderSenderSenderFigure5:Thesenderoccupiesrowsinabanktotriggerrowconﬂicts.Thereceiveroccupiesrowsinthesamebanktoobservetheserowconﬂicts.20030040050060000.10.20.3Accesstime[CPUcycles]Frequency(a)Senderinactiveonbank:sendinga0.20030040050060000.10.2Accesstime[CPUcycles]Frequency(b)Senderactiveonbank:sendinga1.Figure6:Timingdifferencesbetweenactiveandnon-activesender(ononebank),measuredontheHaswelli7testsystem.accessesadifferentaddressinthesamebankbutinadifferentrow,arowconﬂictoccurs.Thisleadstohigheraverageaccesstimesinthereceiverprocess.Bitscanbetransmittedbyswitchingtheactivityofthesenderpro-cessinthetargetedbankonandoff.Thistimingdiffer-enceisillustratedinFigure6,anexemplarytransmissionisshowninFigure7.Thereceiverprocessdistinguishesthetwovaluesbasedonthemeanaccesstime.Weassignalogicvalueof0tolowaccesstimes(thesenderisinac-tive)andavalueof1tohighaccesstimes(thesenderisactive).Each(CPU,channel,DIMM,rank,bank)tuplecanbeusedasaseparatetransmissionchannel.However,ahighnumberofparallelchannelsleadstoincreasednoise.Also,thereisastrictlimitontheusablebankpar-8Table 2: Reverse engineered DRAM mapping on all platforms and configurations we analyzed via physical probing
or via software analysis. These tables list the bits of the physical address that are XORed. For instance, for the entry
(13, 17) we have a13 ⊕ a17.
CPU
Ch.
DIMM/Ch.
BA0
Sandy Bridge
Sandy Bridge [23]
Ivy Bridge/Haswell
1
2
1
1
2
2
1
1
1
2
1
2
13, 17
14, 18
13, 17
13, 18
14, 18
14, 19
(a) DDR3
BA1
14, 18
15, 19
14, 18
14, 19
15, 19
15, 20
BA2
15, 19
16, 20
16, 20
17, 21
17, 21
18, 22
(b) DDR4
Rank
DIMM
Channel
16
17
15, 19
16, 20
16, 20
17, 21
-
-
-
15
-
16
-
6
-
-
7, 8, 9, 12, 13, 18, 19
7, 8, 9, 12, 13, 18, 19
CPU
Skylake†
2x Haswell-EP
(interleaved)
2x Haswell-EP
(non-interleaved)
Ch.
DIMM/Ch.
2
1
2
1
2
1
1
1
1
1
BG0
7, 14
6, 22
6, 23
6, 21
6, 22
BG1
15, 19
19, 23
20, 24
18, 22
19, 23,
BA0
17, 21
20, 24
21, 25
19, 23
20, 24
BA1
18, 22
21, 25
22, 26
20, 24
21, 25
Rank
16, 20
14
15
13
14
CPU
-
7, 17
7, 17
-
-
Channel
8, 9, 12, 13, 18, 19
-
8, 12, 14, 16, 18, 20, 22, 24, 26
-
7, 12, 14, 16, 18, 20, 22, 24, 26
(c) LPDDR2,3,4
CPU
Ch.
BA0
BA1
BA2
Rank
Channel
Qualcomm Snapdragon S4 Pro†
Samsung Exynos 5 Dual†
Qualcomm Snapdragon 800/820†
Samsung Exynos 7420†
1
1
1
2
13
13
13
14
14
14
14
15
15
15
15
16
10
7
10
8, 13
-
-
-
7, 12
† Software analysis only. Labeling of functions is based on results of other platforms.
e
m
i
t
s
s
e
c
c
A
360
340
320
300
0
500
1,500
1,000
Time [µs]
2,000
Figure 7: Covert channel transmission on one bank,
cross-CPU and cross-VM on a Haswell-EP server. The
time frame for one bit is 50µs.
allelism. Thus, optimal performance is achieved when
using only a subset of available tuples. Transmission
channels are unidirectional, but the direction can be cho-
sen for each one independently. Thus, two-way commu-
nication is possible.
To evaluate the performance of this new covert chan-
nel, we created a proof-of-concept implementation. We
restrict ourselves to unidirectional communication, i.e.,
there is one dedicated sender and one dedicated receiver.
The memory access time is measured using rdtsc.
The memory accesses are performed using volatile
pointers.
In order to cause a DRAM access for each
request, data has to be flushed from the cache using
clflush.
Determining channel, rank, and bank address. In an
agreement phase, all parties need to agree on the set
of (channel, DIMM, rank, bank) tuples that are used
for communication. This set needs to be chosen only
once, all subsequent communication can use the same
set. Next, both sender and receiver need to find at least
one address in their respective address space for each
tuple. Note that some operating systems allow unpriv-
ileged resolution of virtual to physical addresses. In this
case, finding correct addresses is trivial.
However, on Linux, which we used on our testing
setup, unprivileged address resolution is not possible.
Thus, we use the following approach. As observed in
previous work [3, 4], system libraries and the operating
system assign 2 MB pages for arrays which are signifi-
cantly larger than 2 MB. On these pages, the 21 lowest
bits of the virtual address and the physical address are
USENIX Association  
25th USENIX Security Symposium  573
9
identical. Depending on the hardware setup, these bits
can already be sufficient to fully determine bank, rank
and channel address. For this purpose, both processes
request a large array. The start of this array is not neces-
sarily aligned with a 2 MB border. Memory before such
a border is allocated using 4 KB pages. We skip to the
next 2 MB page border by choosing the next virtual ad-
dress having the 21 lowest bits set to zero.
On systems that also use higher bits, an attacker can
use the following approach, which we explain on the ex-
ample of the mapping shown in Figure 4b. There an at-
tacker cannot determine the BA2 bit by just using 2 MB
pages. Thus, the receiving process selects addresses with
chosen BA0, BA1, rank, and channel, but unknown BA2
bit. The sender now accesses addresses for both possibil-
ities of BA2, e.g., by toggling a17 between consecutive
reads. Thus, only each second access in the sending pro-
cess targets the correct bank. Yet, due to bank parallelism
this does not cause a notable performance decrease. Note
however that this approach might not work if the number
of unknown bank-address bits is too high.
In a virtualized environment, even a privileged at-
tacker is able to retrieve only the guest physical ad-
dress, which is further translated into the real physical
address by the memory management unit. However, if
the host system uses 1 GB pages for the second-level ad-
dress translation (to improve efficiency), then the lowest
30 bits of the guest physical address are identical to the
real physical address. Knowledge of these bits is suffi-
cient on all systems we analyzed to use the full DRAM
addressing functions.
Finally, the covert channel could also be built with-
out actually reconstructing the DRAM addressing func-
tions. Instead of determining the exact bank address, it
can rely solely on the same-bank sets retrieved in Sec-
tion 4.3. In an initialization phase, both sender and re-
ceiver perform the timing analysis and use it to build sets
of same-bank addresses. Subsequently, the communicat-
ing parties need to synchronize their sets, i.e., they need
to agree on which of them is used for transmission. This
is done by sending predefined patterns over the channel.
After that, the channel is ready for transmission. Thus,
it can be established without having any information on
the mapping function nor on the physical addresses.
Synchronization. In our proof-of-concept implementa-
tion, one set of bits (a data block) is transmitted for
a fixed time span which is agreed upon before starting
communication. Decreasing this period increases the raw
bitrate, but it also increases the error rate, as shown in
Figure 8.
however, sender and receiver run in two different VMs,
then a common (or perfectly synchronized) wall clock is
typically not available. In this case, the sender uses one
of the transmission channels to transmit a clock signal
which toggles at the beginning of each block. The re-
ceiver then recovers this clock and can thus synchronize
with the sender.
We employ multiple threads for both the sender and re-
ceiver processes to achieve optimal usage of the memory
bus. Thus, memory accesses are performed in parallel,
increasing the performance of the covert channel.
5.2 Evaluation
We evaluated the performance of our covert-channel im-
plementation on two systems. First, we performed tests
on a standard desktop PC featuring an Intel i7-4790 CPU
with Haswell microarchitecture. It was equipped with 2
Kingston DDR3 KVR16N11/8 dual-rank 8 GB DIMMs
in dual-channel configuration. The system was mostly
idle during the tests, i.e., there were no other tasks caus-
ing significant load on the system. The DRAM clock was
set to its default of 800 MHz (DDR3-1600).
Furthermore, we also tested the capability of cross-
CPU transmission on a server system. Our setup has
two Intel Xeon E5-2630 v3 (Haswell-EP microarchi-
tecture).
It was equipped with a total of 4 Samsung
M393A2G40DB0-CPB DDR4 registered ECC DIMMs.
Each CPU was connected to two DIMMs in dual-channel
configuration and NUMA was set to interleaved mode.
The DRAM frequency was set to its maximum supported
value (DDR4-1866).
For both systems, we evaluated the performance in
both a native scenario, i.e., both processes run natively,
and in a cross-VM scenario. We transmit 8 bits per block
(use 8 (CPU, channel, DIMM, rank, bank) tuples) in the
covert channel and run 2 threads in both the sender and
the receiver process. Every thread is scheduled to run on
different CPU cores, and in the case of the Xeon system,
sender and receiver run on different physical CPUs.
We tested our implementation with a large range of
measurement intervals. For each one, we measure the
raw channel capacity and the bit error probability. While
the raw channel capacity increases proportionally to the
reduction of the measurement time, the bit error rate in-
creases significantly if the measurement time is too short.
In order to find the best transmission rate, we use the
channel capacity as metric. When using the binary sym-
metric channel model, this metric is computed by multi-
plying the raw bitrate with 1− H(e), with e the bit error
probability and H(e) = −e·log2(e)− (1−e)·log2(1−e)
the binary entropy function.
Figure 8 shows the error rate varying depending on
the raw bitrate for the case that both sender and receiver
For synchronizing the start of these blocks we em-
ploy two different mechanisms. If sender and receiver
run natively, we use the wall clock as means of synchro-
nization. Here blocks start at fixed points in time.
If,
574  25th USENIX Security Symposium 
USENIX Association
10
y
t
i
l
i
b
a
b
o
r
p
r
o
r
r
e
t
i
B
0.4
0.2
0
y
t
i
l
i
b
a
b
o
r
p
r
o
r
r
e
t
i
B
0.4
0.2