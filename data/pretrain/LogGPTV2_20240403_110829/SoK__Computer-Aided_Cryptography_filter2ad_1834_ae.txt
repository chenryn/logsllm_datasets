in Table IV. These comprise some of the fastest available
veriﬁed and unveriﬁed implementations; they are written in
C, assembly, or a combination of both.
To compare their performance, we measure the number of
CPU cycles (median over 5K executions) it takes to perform
scalar multiplication. We report the performance increase (%
faster) over donna64 [115], one of the fastest known (unver-
iﬁed) C implementations. All measurements are collected on
a 1.8 GHz Intel i7-8565U CPU with 16 GB of RAM; hyper-
threading and dynamic-processor scaling (e.g., Turbo Boost)
are disabled. Implementations written in C are compiled using
GCC 9.2 with optimization ﬂag -O3. To summarize, several
veriﬁed C implementations (hacl and ﬁat) beat donna64; the
fastest veriﬁed assembly implementation (evercrypt) meets the
fastest unveriﬁed assembly implementation (precomp).
Takeaway: Higher performance entails larger veriﬁca-
tion effort. Verifying generic, high-level code is typically
easier, but comes with a performance cost. Hand-written
assembly can achieve best in class performance by taking
advantage of hardware-speciﬁc optimizations, but verifying
such implementations is quite difﬁcult due to complex side-
effects, unstructured control-ﬂow, and ﬂat structure. Moreover,
this effort must be repeated for each platform. C code is
less efﬁcient, as hardware-speciﬁc features are not a part of
standard portable C, but implementations need only be veriﬁed
once and can then be run on any platform. Code written in
higher-level languages is even less efﬁcient, but veriﬁcation
becomes much easier (e.g., memory safety can be obtained
for free). These aspects are discussed further in the Vale and
Jasmin papers [102], [103], [117].
mathematical reasoning that SMT-based tools can struggle
with. Examples range from proving the correctness of the
Montgomery representations [119] used to accelerate big-
integer computations, to the nuts-and-bolts of converting be-
tween, say, 64-bit words and the underlying bytes. At present,
most veriﬁcation efforts build this infrastructure from scratch
and customize it for their own particular needs, which leads
to signiﬁcant duplication of effort across projects. Hence, an
open challenge is to devise a common core of such routines
(e.g., a veriﬁed version of the GMP library [120]) that can be
shared across all (or most) veriﬁcation projects, despite their
reliance on different tools and methodologies.
D. Further Reading
While our principal focus is on cryptographic code, verify-
ing systems code is an important and active area of research.
For example, there has been signiﬁcant work in verifying op-
erating systems code [121]–[127], distributed systems [128]–
[130], and even entire software stacks [131]. We expect that
these two strands of work will cross paths in the future.
IV. IMPLEMENTATION-LEVEL SECURITY
In this section, we focus on the role of computer-aided
cryptography in establishing implementation-level security
guarantees, with a particular focus on software protections
against digital side-channel attacks. Hardware protections are
beyond the scope of this paper and are left as further reading.
By digital side-channel attacks, we mean those that can be
launched by observing intentionally exposed interfaces by the
computing platform, including all execution time variations
and observable side-effects in shared resources such as the
cache. This excludes physical side channels such as power
consumption, electromagnetic radiation, etc.
A. Critical Review
Why is implementation-level security important? Although
design-level security can rule out large classes of attacks,
guarantees are proven in a model that idealizes an attacker’s
interface with the underlying algorithms: They can choose
inputs and observe outputs. However, in practice, attackers
can observe much more than just the functional behavior of
cryptographic algorithms. For example, side-channels are in-
terfaces available at the implementation-level (but unaccounted
for at the design-level) from which information can leak as
side-effects of the computation process (e.g., timing behavior,
memory access patterns). And indeed, these sources of leakage
are devastating—key-recovery attacks have been demonstrated
on real implementations, e.g., on RSA [142] and AES [143].
How can implementation-level security fail? The prevailing
technique for protecting against digital side-channel attacks is
to follow constant-time coding guidelines [144]. We stress that
the term is a bit of a misnomer: The idea of constant-time is
that an implementation’s logical execution time (not wall-clock
execution time) should be independent of the values of secret
data; it may, however, depend on public data, such as input
length. To achieve this, constant-time implementations must
Challenge: Automating equivalence proofs. Signiﬁcant
progress could be made if functional correctness proofs could
be solved by providing a sequence of simple transformations
that connect speciﬁcations to targets and relying on an auto-
matic tool to check these simple transformations. Promising
recent work in this direction [118] demonstrates the feasibility
of the approach. However, the current approaches are not
automatic: neither in ﬁnding the transformations nor in proving
them. The latter seems achievable for many useful control-
ﬂow-preserving transformations, whereas the former could be
feasible at least for common control-ﬂow transformations.
Challenge: Functional correctness of common arithmetic
routines. Verifying cryptographic code often involves tricky
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:10:25 UTC from IEEE Xplore.  Restrictions apply. 
785
Public
inputs
Public
outputs
Control
ﬂow
Memory
access
Variable-
time op.
Tool
Target Method
Synthesis
Sound
Complete
ABPV13
CacheAudit
ct-verif
CT-Wasm
FaCT
FlowTracker
Jasmin
KMO12
Low∗
SC Eliminator
Vale
VirtualCert
DV
[132] C
[133] Binary Q
[134] LLVM DV
[135] Wasm TC
[136] LLVM TC
[137] LLVM DF
[102] asm
DV
[138] Binary Q
[139] C
TC
[140] LLVM DF
[103] asm
DF
[141] x86
DF
TC – type-checking DF – data-ﬂow analysis DV – deductive veriﬁcation Q – Quantitative
Method
OVERVIEW OF TOOLS FOR SIDE-CHANNEL RESISTANCE. SEE SECTION IV-B FOR MORE DETAILS ON TOOL FEATURES.
TABLE V
follow a number of strict guidelines, e.g., they must avoid
variable-time operations, control ﬂow, and memory access
patterns that depend on secret data. Unfortunately, complying
with constant-time coding guidelines forces implementors to
avoid natural but potentially insecure programming patterns,
making enforcement error-prone.
further, some tools can automatically repair code that violates
constant-time into compliant code. These approaches neces-
sarily abstract the leakage interface available to real-world
attackers, but being precisely deﬁned, they help clarify the
gap between formal leakage models and real-world leakage.
What are the ﬁne-print caveats? Implementation-level
proofs are only as good as their models, e.g., of physically
observable effects of hardware. Furthermore, new attacks may
challenge these models. Implicit assumptions arise from gaps
between code and veriﬁed artifacts.
What background do I need to know? Formal reasoning
about side-channels is based on a leakage model. This model
is deﬁned over the semantics of the target language, abstractly
representing what an attacker can observe during the computa-
tion process. For example, the leakage model for a branching
operation may leak all program values associated with the
branching condition. After having deﬁned the appropriate
leakage models, proving that an implementation is secure
(with respect
to the leakage models) amounts to showing
that the leakage accumulated over the course of execution
is independent of the values of secret data. This property is
an instance of observational non-interference, an information
ﬂow property requiring that variations in secret data cause no
differences in observable outputs [150].
The simplest leakage model is the program counter pol-
icy, where the program control-ﬂow is leaked during ex-
ecution [151]. The most common leakage model, namely
the constant-time policy, additionally assumes that memory
accesses are leaked during execution. This leakage model is
usually taken as the best practice to remove exploitable exe-
cution time variations and a best-effort against cache-attacks
launched by co-located processes. A more precise leakage
model called the size-respecting policy also assumes that
operand sizes are leaked for speciﬁc variable-time operations.
For more information on leakage models, see the paper by
Barthe et al. [150, Section IV.D].
B. Digital Side-Channel Tools: State of the Art
Table V presents a taxonomy of tools for verifying digital
side-channel resistance. Tools are listed alphabetically and are
categorized as follows.
Even worse, the observable properties of a program’s exe-
cution are generally not evident from source code alone. Thus,
software-invisible optimizations, e.g., compiler optimizations
or data-dependent instruction set architecture (ISA) optimiza-
tions, can remove source-level countermeasures. Programmers
also assume that the computing machine provides memory
isolation, which is a strong and often unrealistic assumption
in general-purpose hardware (e.g., due to isolation breaches
allowed by speculative execution mechanisms).
How are these failures being addressed outside CAC?
To check that implementations correctly adhere to constant-
time coding guidelines, current solutions are (1) auditing,
which is costly in both time and expertise, and (2) testing,
which commits the fallacy of interpreting constant-time to be
constant wall-clock time. These solutions are inadequate: A
botched patch for a timing vulnerability in TLS [145] led to the
Lucky 13 timing vulnerability in OpenSSL [146]; in turn, the
Lucky 13 patch led to yet another timing vulnerability [147]!
To prevent compiler optimizations from interfering with
constant-time recipes applied at the source-code level, imple-
mentors simply avoid using compilers at all, instead choosing
to implement cryptographic routines and constant-time recipes
directly in assembly. Again, checking that countermeasures are
implemented correctly is done through auditing and testing,
but in a much more difﬁcult, low-level setting.
Dealing with micro-architectural attacks that breach mem-
ory isolation, such as Spectre and Meltdown [148], [149], is
still an open problem and seems to be out of reach of purely
software-based countermeasures if there is to be any hope of
achieving decent performance.
How can computer-aided cryptography help? Program
analysis and veriﬁcation tools can automatically (or semi-
automatically) check whether a given implementation meets
constant-time coding guidelines, thereby providing a formal
foundation supporting heretofore informal best practices. Even
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:10:25 UTC from IEEE Xplore.  Restrictions apply. 
786
Target
(A,S). At what
is the analysis performed
(e.g., source, assembly, binary)? To achieve the most reliable
guarantees, analysis should be performed as close as possible
to the executed machine code.
level
Method (A). The tools we consider all provide a means
to verify absence of timing leaks in a well-deﬁned leakage
model, but using different techniques:
• Static analysis techniques use type systems or data-ﬂow
analysis to keep track of data dependencies from secret
inputs to problematic operations.
• Quantitative analysis techniques construct a rich model of a
hardware feature, e.g, the cache, and derive an upper-bound
on the leaked information.
• Deductive veriﬁcation techniques prove that
the leakage
traces of two executions of the program coincide if the pub-
lic parts of the inputs match. These techniques are closely
related to the techniques used for functional correctness.
Type-checking and data-ﬂow analysis are more amenable to
automation, and they guarantee non-interference by excluding
all programs that could pass secret information to an operation
that appears in the trace. The emphasis on automation, how-
ever, limits the precision of the techniques, which means that
secure programs may be rejected by the tools (i.e., they are not
complete). Tools based on deductive veriﬁcation are usually
complete, but require more user interaction. In some cases,
users interact with the tool by annotating code, and in others
the users use an interactive proof assistant to complete the
proof. It is hard to conciliate a quantitative bound on leakage
with standard cryptographic security notions, but such tools
can also be used to prove a zero-leakage upper bound, which
implies non-interference in the corresponding leakage model.
Synthesis (U). Can the tool take an insecure program and
automatically generate a secure program? Tools that support
synthesis (e.g., FaCT [136] and SC Eliminator [140]) can
automatically generate secure implementations from insecure
implementations. This allows developers to write code natu-
rally with constant-time coding recipes applied automatically.
Soundness (A, T). Is the analysis sound, i.e., it only deems
secure programs as secure? Note that this is our baseline
ﬁlter for consideration, but we make this explicit in the table.
Still, it bears mentioning that some unsound tools are used
in practice. One example is ctgrind [152], an extension of
Valgrind that takes in a binary with taint annotations and
checks for constant-address security via dynamic analysis. It
supports public inputs but not public outputs, and is neither
sound nor complete.
Completeness (A, S). Is the analysis complete, i.e., it only
deems insecure programs as insecure?
Public input (S). Does the tool support public inputs? Sup-
port for public inputs allows differentiating between public and
secret inputs. Implementations can benignly violate constant-
time policies without introducing side-channel vulnerabilities
by leaking no more information than public inputs of compu-
tations. Unfortunately, tools without such support would reject
these implementations as insecure; forcing execution behaviors
to be fully input independent may lead to large performance
overheads.
Public output (S). Does the tool support public outputs?
Similarly, support for public outputs allows differentiating be-
tween public and secret outputs. The advantages to supporting
public outputs is the same as those for supporting public
inputs: for example, branching on a bit that is revealed to
the attacker explicitly is ﬁne.
Control ﬂow leakage (S). Does the tool consider control-
ﬂow leakage? The leakage model includes values associated
with conditional branching (e.g., if, switch, while, for state-
ments) during program execution.
Memory access leakage (S). Does the tool consider memory