the value of the private static property AmsiInitFailed
in the AmsiUtils class to true,
thus preventing the
ScanContent method (not shown in the above code) from
sending any content to the antimalware engine for scanning.
Attempts to disable or bypass AMSI can be considered as
malicious activity and can be detected by pin-point detectors
dedicated to this task, as done by several popular antimalware
vendors2. We elaborate more on this issue in Section IX.
C. Deep Learning
In this section, we provide some background on deep
learning concepts and architectures that can be helpful for
understanding the deep-learning based malicious PowerShell
code detectors that we present in subsection VI-A. A compre-
hensive introduction to deep learning can be found in [6].
An Artiﬁcial Neural Network [21]–[23] is a machine learn-
ing model, typically non-linear, composed of a collection of
layers. An ANN network/model typically has one or more
input layers and a single output layer. A model that contains
additional hidden layers between the input and output layers
is called a Deep Neural Network (DNN). Several key DNN
architectures exist. In what follows, we brieﬂy describe the
architectures used by our detectors.
1) Convolutional Neural Networks (CNNs): Extensively
used in computer vision tasks [24], [25], CNN is an ar-
chitecture that uses a special kind of hidden layer called a
convolutional layer. A convolutional layer computes its output
by calculating the dot product of each of its ﬁlters with
windows of appropriate size in the input. By “sliding” a ﬁlter
across the input matrix, the dot product of the ﬁlter’s weights
and the corresponding window is computed, resulting in a
scalar value. The weights of the ﬁlters are being learnt during
the training process and are used for searching for the existence
of meaningful input patterns.
Two additional layer types often used by CNNs (as well as
by the RNN architecture we describe below) are the pooling
and dropout
layers. A pooling layer [26] computes some
function on the input resulting in a single value (such as
average or maximum). These layers are typically used in
order to reduce dimensionality and overﬁtting [27]. A global
max pooling layer is a special case of pooling. Its window
size equals the size of the input. It computes the maximum
value inside the window. Intuitively, when using a global max
pooling layer on top of a convolutional layer, each ﬁlter is
mapped to a single feature, indicating the extent to which the
feature searched by this ﬁlter appears anywhere in the input.
A Dropout layer [28] with (user-deﬁned) probability p, drops
each node in the layer’s input with probability p, effectively
making it disconnected from the next layer. Dropout layers are
typically used in-between layers in order to reduce overﬁtting.
2) Recurrent Neural Networks (RNNs): Aimed to process
sequences of inputs, RNN is an architecture used in various
domains with sequential nature such as text [29], [30], speech
[31]–[33], handwriting [34] or video [35]. As implied by its
name, data is processed in a recurrent manner, considering
input seen so far when processing new data. In this work we
use a long-short term memory (LSTM) cell [36] to process
2 Microsoft Defender ATP, VirusTotal scan of AMSI bypass script.
3
the data in the RNN network. The LSTM cell aggregates
information in a memory unit called a hidden state, which
is being updated as new information is being processed.
In a basic RNN architecture, LSTM cells process the data
from the ﬁrst to the last input (in English textual data, this is
a left-to-right order). In a bidirectional RNN (BDRNN) layer
[37] there are two sets of LSTM cells: one set processes the
data from ﬁrst to last, and the other processes it in reversed
order. Thus, the hidden state of the cells reading the input
in reversed order can be updated based on information which
appears in the input to their right. The output of these two
sets of LSTM cells is being used as the output of the BDRNN
layer.
D. Contextual Embeddings
In the context of text analysis, a common practice is to
add an embedding layer before the CNN or the RNN layer
[38]–[40]. Embedding layers serve two purposes. First, they
reduce the dimensionality of the input. Second, as done by our
detectors, they can be used to represent the input in a manner
that retains its context. The embedding layer converts the input
(typically at the token level, but sometimes also at the character
level, depending on the problem at hand) to a sequence of
vectors. Embedding techniques are designed to embed tokens
in an n-dimensional space (for an appropriately-selected value
of n) by representing them as n-dimensional vectors.
Our detectors employ the widely-used Word2Vec (W2V)
[9] and FastText [16], [17] contextual embedding algorithms,
which use an ML model for learning the vector representation
of tokens. In both algorithms, the underlying architecture of the
model contains an input layer, a hidden layer of (appropriately
selected) size n, and an output layer. Depending on the training
method (“CBOW” or “skip-gram” [41]), we either try to
predict a token based on its context (i.e. the tokens surrounding
it), as done in CBOW, or to predict the context of a given token,
as done in skip-gram.
Following the learning phase, a sequence of values is stored
in the hidden layer per every token in the corpus. These values
serve as the vector representation of the token. The key dif-
ference between the two algorithms is the following. Whereas
Word2Vec only embeds the tokens as atomic units, FastText
also embeds character n-grams (sub-tokens) extracted from
these tokens. Speciﬁcally, each token is represented by the
sum of the vector representations of the token itself and its n-
grams (our implementations use n-grams for n ∈ {3, . . . , 6}).
This representation implies that FastText is able to leverage
the sub-tokens comprising each token. This allows it to embed
tokens that were not seen during the training stage (but may
be input to the model once it is deployed), as they or their
sub-tokens appeared as sub-tokens in the corpus used to train
the embedding.
III. AMSI VS. COMMAND-LINE LOGGING
In this section, we provide a brief comparison between the
data provided to an antimalware system via AMSI and that
provided by PowerShell command-line logging. The compar-
ison is based on data that was collected inside the vendor’s
organization during a period of one week. The total number
of AMSI scan events was 373,594,394 , more than twice the
Fig. 1. Length distributions of code logged using AMSI/command-line.
number of PowerShell command-line events, which totalled
177,222,700. This is because a single PowerShell command-
line may invoke (directly or indirectly) several PowerShell
executions, each generating a separate AMSI scan event.
Figure 1 presents the length distribution of code logged
using AMSI versus command-line logging. The x-axis repre-
sents the length of the PowerShell content and the y-axis the
number of events (in logarithmic scale). Each bar corresponds
to a bucket of size 200, except for the last bar which counts all
events reporting code of length exceeding 16,000 characters.
As can be seen, AMSI-collected code tends to be much longer
than that obtained using command-line logging. Speciﬁcally,
whereas only a negligible fraction of less than 0.0008% of
command-line events logged code of length 16,000 or more,
the corresponding ﬁgure for AMSI scan events is 4 magnitudes
higher - more than 8%.
We also compare the prevalence of PowerShell keywords
that indicate relatively-complex code structure, such as the
deﬁnitions and invocations of functions, branching, module-
importing and exception-handling tokens. Table I presents the
fractions of logging events that contain these tokens. As can be
seen, they are used by AMSI-logged code orders-of-magnitude
more frequently than by command-line code. For example,
the ’function’ token appears in almost 39% of AMSI scan
events, 277 times more frequently than it does in command-
line code.
TABLE I.
PREVALENCE OF TOKENS INDICATIVE OF
RELATIVELY-COMPLEX CODE.
Token
Percentage in AMSI
Percentage in command-lines
function
param
Import-Module
if
while
New-Object
throw
38.78
31.03
7.65
62.55
7.92
19.49
20.92
0.14
0.03
0.15
5.68
0.02
0.66
0.53
IV. DATASETS, MODEL GENERATION
AND PREPROCESSING
We train and evaluate our detectors using two datasets: An
unlabeled dataset and a labeled dataset. The unlabeled dataset
4
consists of approximately 368K unlabeled PowerShell scripts
and modules (*.ps1 and *.psm1 ﬁles) collected from public
repositories including GitHub3 and PowerShellGallery4, made
publicly-available by [18]5. Our labeled dataset is composed
of 116,976 PowerShell code instances (commands, scripts
and modules). Of these, 5,383 are distinct malicious code
instances, obtained by executing known malicious programs
inside a sandbox and recording all their PowerShell activity
using AMSI. The labeled dataset contains also a collection of
111,593 distinct benign code instances, recorded using AMSI
as well. Unlike malicious code, benign code was executed on
regular machines within the vendor’s organization rather than
inside a sandbox. Only code instances that were executed ex-
clusively on machines with no indication of malicious activity
30 days prior to data collection were labeled as benign. The
labeled dataset consists of a training set and a test set, collected
over different periods of time.
The following subtle point regarding the dataset labeling
process should be emphasized. When AMSI is used for
monitoring the execution of a program, the PowerShell code
it executes is reported in its entirety. Consequently, when a
malicious code uses benign modules (which is often the case),
the benign module’s code is reported by AMSI as well. In
order not to label such benign modules as malicious, we label
a code instance as malicious only if it was seen exclusively in
malicious contexts, that is, only if it was never observed on
clean machines.
The high-level structure of our model generation process
is presented in Figure 2. Our method trains the detection
model using two stages. During the ﬁrst stage, we use the
unlabeled dataset and the training set6 to obtain a contex-
tual embedding of PowerShell tokens. We provide examples
demonstrating interesting semantic relationships captured by
this embedding in Section V. During the second stage, we
employ the embedding as a ﬁrst layer for token inputs in a
deep neural network trained (using the labeled instances of
the training set) to detect malicious PowerShell code. Our best
model employs an architecture comprised from both character-
level one-hot encoded input and a token-level embedding layer
(pretrained using FastText), followed by several layers of CNN
and LSTM-RNN neural network units. We use the labeled
dataset for supervised training and for performing an extensive
performance evaluation of different DL and traditional (e.g.
logistic regression [42]) ML classiﬁcation methods.
Data Preprocessing: We have carefully preprocessed the
PowerShell code we collected in order to normalize it, by
regularizing digits and random values, for improving detection
and evaluation results. Digits were replaced with asterisk signs
(‘*’) in order to better deal with random values, IP addresses,
random domain names (which in many cases contain digits),
dates, version numbers, etc. Labeled code instances were
preprocessed also for eliminating identical (or nearly-identical)
code (a process that we call data de-duplication) in order to
reduce the probability of data leakage [43], as we explain next.
3https://github.com/
4https://www.powershellgallery.com/
5We thank Lee Holmes for helping us with working with this dataset and
his general assistance
6Labels are not used for learning contextual embeddings.
Fig. 2. High-level structure of our model generation process.
Deduplicating Data: Since we use cross-validation to evaluate
the performance of our detection models on labeled data, we
took extra care to reduce the probability of data leakage. In
our setting, data leakage may result from using identical (or
nearly-identical) code instances for training the model and for
validating it. Indeed, we observed in our dataset PowerShell
code instances that differ by only a small number of characters.
In most of these cases,
the difference stemmed from the
usage of random ﬁle names, different IP addresses, or different
numbers/types of white space characters (i.e. spaces, tabs and
newlines).
The existence of identical or nearly-identical code instances
in a PowerShell code-corpus collected inside a real-world or-
ganization is almost certain. Many of the benign code instances
observed run as part of corporate maintenance procedures and
are therefore likely to be observed on many machines and/or
on the same machine in different times. As for malicious code,
since we executed (inside a sandbox) numerous malicious
programs in order to collect the PowerShell code they invoke,
some subsets of these programs may have belonged to the same
malware family, and thus invoked similar or even identical
PowerShell code. Moreover, nearly-identical code can also be
used by programs from different malware families that launch
similar types of cyber attacks.
To prevent data leakage, we perform a de-duplication pro-
cess for eliminating identical or nearly-identical code instances
from our dataset. The de-duplication process consists of the
following 4 stages:
1) Code tokenization: Code instances are demarcated to tokens.
Any symbol which is not in the set {’a’-’z’, ’A’-’Z’, ’*’, ’$’,
’-’} is used as a delimiter. We remind the reader that digits
are replaced by asterisk signs (’*’) during the regularization
process, hence they are not used as delimiters. We do not
use the dollar sign (’$’) as a delimiter because it is used
in PowerShell to refer to a variable. Thus, for example, we
consider true and $true as two different tokens. As for
the dash sign(’-’), it appears inside PowerShell tokens such
as Write-Host and Invoke-Command and is therefore
not used as a delimiter as well. We only use tokens of
length at least 2, since a single character by itself has no
5
supervised training stage. We learn the contextual embedding
using both the unlabeled dataset and the training set.7 In this
section, we share some interesting ﬁndings derived from these
embeddings, showcasing their potential contribution for de-
tection. We experimented with two DL-based text embedding
techniques – W2V and FastText (see Section II-D). In both
cases, the input for the embedding is the same: we tokenized
the code as described above.
The PowerShell code we use to generate the embedding
contains approximately four million distinct tokens, most of
which appear in only a few instances. Using all these tokens
would generate a huge embedding layer, making the processing
time of both learning the embedding and training the model
impractically large. Consequently, only tokens that appeared in
at least ten instances were used for embedding. This resulted
in 81,111 distinct tokens.
We chose to use the CBOW rather than the Skip-Gram ar-
chitecture [41], since the former is faster to train and generally
works better on large training sets with many frequent words.
A. Tokens embedding in action
W2V embedding is known for capturing semantic similar-
ities between different words, which are frequently preserved
in linear combinations of embedded vectors [9]. In this subsec-
tion, we share a few interesting examples demonstrating how
different tokens representing similar semantics in PowerShell
code are embedded as neighboring vectors. Using t-SNE [44]
for reducing dimensionality, we present
in Figure 4 a 2-
dimensional visualization of the vector representation (using
W2V) of 5,000 randomly selected tokens and some inter-
esting tokens which we highlighted. Note how semantically
similar tokens are placed near each other. For example, the
vectors representing -eq, -ne and -gt, which in PowerShell
are aliases for “equal”, “not-equal” and “greater-than”, re-
spectively, are clustered together. Similarly, the vectors rep-
resenting the allSigned, remoteSigned, bypass and
unrestricted tokens, all of which are valid values for the
execution policy setting in PowerShell, are clustered together.
Examining the vector representations of the tokens, we
found a few additional interesting relationships between the
tokens, which we describe next.
Tokens similarity: Using the W2V vector representation of
tokens, we can use the Euclidean distance to measure similarity
in the embedding space. Many cmdlets in PowerShell have an
alias. We found that when using the W2V embedding, in many
cases, the token closest to a given cmdlet is its alias. For exam-
ple, the representations of the token Invoke-Expression