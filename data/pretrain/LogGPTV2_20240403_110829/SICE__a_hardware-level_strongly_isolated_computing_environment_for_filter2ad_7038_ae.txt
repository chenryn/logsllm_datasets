before sending the last letter of the shell command (the car-
riage return) and right after the ﬁrst response is received
(echoing the carriage return).
To calculate the performance overhead of each test, we
ﬁrst measure TK , the time needed to run the test using
unmodiﬁed Qemu/KVM. Afterward, we measure TS, the
time needed to run the same experiment using SICE. The
performance overhead is then calculated using the equation
(TS − TK )/TK × 100%.
We selected our test cases in three categories: The ﬁrst is
user level programs to test the user level operations of guest
VMs running with SICE. The second is a group of tests that
test the throughput of the guest VM kernel. The third tests
the performance of the emulated network interface.
In the ﬁrst category, we run three tests. The ﬁrst mea-
sures the latency of copying a 2.1 MB ﬁle. Our prototype
system does not support hard drive emulation. Thus the
ﬁle is copied inside the virtual ﬁle system that represents
the initial ram disk. Our main objective of this test is to
estimate the impact of SICE on the guest VM memory op-
erations, because it is basically a copy between two locations
in memory rather than an actual disk access. The next two
tests in the same category use the programs gzip and gun-
zip to compress and decompress the same ﬁle. These tests
aim to test the impact of SICE on the guest VM’s user level
computations. The ﬁnal results of these tests are obtained
as an average of 100 runs.
In the second category, we measure the guest VM kernel
responses to three main operations: fork, getpid and in-
smod. fork and getpid are tested using a custom program
that directly calls these system calls 10,000 times and mea-
sures the average response time. For the insmod test case,
we use the insmod program to insert a 16.4 KB loadable
kernel module in the guest kernel. This test is repeated 10
times, and all test runs show very little variance.
In the last category, we run a single test that uses wget
to retrieve a 156.6 KB ﬁle from an Apache server running
in the legacy host. The test is repeated 10 times.
Figure 6 shows the results of our experiments.
In the
multi-core mode, all tests, except for wget, showed a slight
performance overhead under 3%. We expect that the slight
performance overhead is due to the time required to copy
information to and from the shared memory between the
legacy host and the security manager.
The wget test shows a 17% performance overhead. How-
ever, a signiﬁcant portion of this overhead is due to the non-
optimized implementation of the Qemu network emulator for
SICE. When a packet is ready to be sent through the net-
work, the VM sends an IO command to the network adapter
to inform it that a packet is ready. In normal operations,
ƚŝŵĞͲƐŚĂƌŝŶŐŵŽĚĞ
ŵƵůƚŝͲĐŽƌĞŵŽĚĞ
ϱϬй
ϰϬй
ϯϬй
ϮϬй
ϭϬй
Ϭй
Figure 6: Performance overhead of an isolated VM
compared with original Qemu/KVM
Qemu ﬁrst receives this IO command, and then directly ac-
cesses the VM physical memory to emulate the DMA read
of the packet. In our prototype, Qemu does not have direct
access to the physical memory of the guest VM. Thus, we
modiﬁed Qemu to send DMA requests to the security man-
ager, which copies the packet from the protected guest VM
memory to the shared memory so that Qemu can process it
correctly. Similarly, an extra step is necessary when the VM
receives a packet.
Hence, our prototype requires an extra communication
step between Qemu in the legacy host and the security man-
ager for every network packet. Intuitively, this extra com-
munication step requires an extra context switch from the
legacy host to the isolated environment in the time-sharing
mode or an extra inter-process communication between the
host core and the isolated core in the multi-core mode.
This extra communication step is the cause of the high
overhead in the wget test. To support this claim, we mea-
sured the number of exits to and from the isolated envi-
ronment during this test. In normal operations, there was
an average of 1,249 guest VM exits that required hardware
emulation by KVM and/or Qemu. In SICE’s time-sharing
mode, there was an average of 1,877 context switches be-
tween the isolated environment and the legacy host. There
was a similar number of messages passed between the host
core and the isolated core in the multi-core mode. This 50%
increase in the communication time between the two envi-
ronments explains the high overhead in this test.
To avoid this overhead, the VM device driver can be mod-
iﬁed so that it uses the shared memory for the network
adapter’s DMA operations. Thus, the Qemu emulated driver
would be able to directly read the passed packets without
sending an extra request to the security manager.
The time-sharing mode showed a higher performance over-
head, which is expectable due to the 67 µs needed to switch
between the host and the isolated guest VM. The tests that
show higher overhead, such as gzip and wget, are those that
require a higher number of switching between the host and
the isolated environment.
It is also noticeable that the time-sharing mode showed a
higher overhead (around 40%) in the wget test. Receiving
the target ﬁle in the normal VM operations needs 0.532 sec-
onds compared to 0.739 seconds in the time-sharing mode.
As mentioned above, this test requires 1,877 context switches
from the isolated environment to the legacy host. Given that
each context switch requires an average of 67 µs, the con-
text switch overhead in this test was 0.126 seconds. This
test clearly shows that the time-sharing mode is not suit-
able for operations that are IO intensive. It will be rather
386useful to run programs that require an enhanced isolation
and a smaller IO footprint (e.g., a program that processes
secret key operations).
7. RELATED WORK
Several researchers (e.g, [3, 5, 9, 22, 34]) attempted to use
hypervisors to enable strong isolation between workloads
running in a cloud computing environment. Nevertheless,
recent attacks [16,32] and vulnerability reports [23,24] show
that hypervisors are subject to security exploitation.
There have been several attempts (e.g., [2,30,31]) to verify
the runtime integrity of hypervisors. These techniques still
require cloud computing users to trust the host environment,
which has a relatively large TCB and continuously interacts
with mutually distrusted workloads.
As discussed in Section 1, the recent attempts that aim
at eliminating the hypervisor from the TCB can be divided
into microhypervisor-based and hardware-based approaches.
Hardware-based approaches: The introduction discussed
how systems that rely on the late launch capability (e.g.,
Flicker [20], BIND [25]) fall short from our objectives. More-
over, Intel proposed a hardware service called Processor
Measured Application Protection Service (P-MAPS) [21] to
oﬀer runtime memory isolation. However, there is insuf-
ﬁcient detail about its ability to provide multi-core isola-
tion. IBM also introduced interesting security features in its
Cell Broadband Engine (Cell BE) architecture [26], which
provides multi-core isolation. However, these features are
unique to this speciﬁc architecture. SICE achieves the same
level of isolation on the x86 architecture, which is used in
the vast majority of general purpose computing platforms.
Microhypervisor-based approaches: We discussed two
notable microhypervisors, NOVA [27] and Trustvisor [19],
as well as seL4 [15] in the introduction. In particular, seL4
is a promising approach that uses formal veriﬁcation to in-
troduce a vulnerability-free microkernel, though it cannot
be extended to commodity hypervisors due to its restric-
tions (e.g., it should run with interrupts mostly disabled). It
would be interesting to study how we can apply formal veriﬁ-
cation to guarantee the integrity of SICE’s security manager.
8. CONCLUSION
In this paper, we presented SICE, a research prototype
that aims to explore the capability of current hardware plat-
forms in providing more secure isolated environments. We
demonstrated that current hardware architecture already
provides abstractions such as SMM that can support strong
isolation. We showed that building strong isolation mecha-
nisms on top of the SMM abstraction requires a very small
TCB of about 300 LOC, which tremendously reduces the
TCB size compared with previous techniques. Nevertheless,
practical deployment of SICE would require CPU and hard-
ware platform vendors to undergo detailed security reviews
of the BIOS software and the SMM implementation.
SICE provides a set of unique capabilities to the isolated
environment, including (1) fast context switch to and from
the legacy host, (2) protected concurrent execution with the
legacy host, and (3) attestation to its integrity. Moreover, its
ability to run isolated workloads concurrently with a legacy
host provides a cost-eﬀective solution to security sensitive
workloads without using dedicated hardware platforms.
Our future research will focus on extending SICE to work
on Intel platforms. In addition, we will explore how to apply
formal veriﬁcation methods on SICE’s security manager.
9. REFERENCES
[1] Advanced Micro Devices. Amd64 architecture
programmer’s manual: Volume 2: System
programming, September 2007.
[2] A. M. Azab, P. Ning, Z. Wang, X. Jiang, X. Zhang,
and N. C. Skalsky. HyperSentry: enabling stealthy
in-context measurement of hypervisor integrity. In
Proceedings of the 17th ACM conference on Computer
and communications security (CCS ’10), pages 38–49,
2010.
[3] S. Berger, R. C´aceres, D. Pendarakis, R. Sailer,
E. Valdez, R. Perez, W. Schildhauer, and
D. Srinivasan. TVDc: managing security in the
trusted virtual datacenter. SIGOPS Oper. Syst. Rev.,
42(1):40–47, 2008.
[4] BusyBox. http://www.busybox.net/.
[5] X. Chen, T. Garﬁnkel, E. C. Lewis, P Subrahmanyam,
C. A. Waldspurger, D. Boneh, J. Dwoskin, and D.R.K
Ports. Overshadow: a virtualization-based approach to
retroﬁtting protection in commodity operating
systems. In Proceedings of the 13th international
conference on Architectural support for programming
languages and operating systems (ASPLOS’13), pages
2–13, 2008.
[6] L. Duﬂot. Getting into the SMRAM: SMM reloaded.
In Proceedings of the 10th CanSecWest conference,
2009.
[7] S. Embleton, S. Sparks, and C. Zou. SMM rootkits: a
new breed of OS independent malware. In Proceedings
of the 4th international conference on Security and
privacy in communication networks, pages 1–12,
August 2008.
[8] Niels Ferguson. Aes-cbc + elephant diﬀuser: A disk
encryption algorithm for windows vista. Microsoft
Corporation Technical Report, 2006.
[9] T. Garﬁnkel, B. Pfaﬀ, J. Chow, M. Rosenblum, and
D. Boneh. Terra: a virtual machine-based platform for
trusted computing. In Proceedings of the 19th ACM
symposium on Operating systems principles (SOSP
’03), pages 193–206, 2003.
[10] Hewlett-Packard, Intel, Microsoft, Phoenix, and
Toshiba. Advanced conﬁguration and power interface
speciﬁcation. revision 3.0b, October 2006.
[11] Intel Corporation. Trusted eXecution Technology
preliminary architecture speciﬁcation and enabling
considerations. Document number 31516803, 2006.
[12] Intel Corporation. Intel virtualization technology for
directed I/O. Document number 31516803, 2007.
[13] Intel Corporation. Software developer’s manual vol. 3:
System programming guide, June 2009.
[14] E. Keller, J. Szefer, J. Rexford, and R. B. Lee.
Nohype: virtualized cloud infrastructure without the
virtualization. In Proceedings of the 37th annual
international symposium on Computer architecture
(ISCA ’10), pages 350–361, 2010.
[15] G. Klein, K. Elphinstone, G. Heiser, J. Andronick,
D. Cock, P. Derrin, D. Elkaduwe, K. Engelhardt,
R. Kolanski, M. Norrish, T. Sewell, H. Tuch, and
S. Winwood. sel4: formal veriﬁcation of an OS kernel.
387In Proceedings of the ACM SIGOPS 22nd symposium
on Operating systems principles (SOSP ’09), pages
207–220, 2009.
[16] K. Kortchinsky. Hacking 3D (and breaking out of
VMWare). In Black Hat conference, 2009.
[17] Dartmouth PKI Lab. TPM reset attack.
http://www.cs.dartmouth.edu/~pkilab/sparks/.
Accessed in August 2011.
[18] Kernel Based Virtual Machine.
http://www.linux-kvm.org/.
[19] J. McCune, Y. Li, N. Qu, A. Datta, V. Gligor, and
A. Perrig. Eﬃcient TCB reduction and attestation. In
the 31st IEEE Symposium on Security and Privacy,
May 2010.
[20] J. McCune, B. Parno, A. Perrig, M. Reiter, and
H. Isozaki. Flicker: an execution infrastructure for
TCB minimization. In Proceedings of the ACM
European Conference on Computer Systems
(EuroSys), March/April 2008.
[21] R. Sahita, U. Warrier, and P. Dewan. Dynamic
software application protection. Intel Corporation,
April 2009.
[22] R. Sailer, T. Jaeger, E. Valdez, R. Caceres, R. Perez,
S. Berger, J. Griﬃn, and L. van Doorn. Building a
MAC-based security architecture for the xen
opensource hypervisor. In Proceedings of the 21st
Annual Computer Security Applications Conference
(ACSAC), pages 276–285, 2005.
[23] Secunia. Vulnerability report: Vmware esx server 3.x.
http://secunia.com/advisories/product/10757/.
Accessed in August 2011.
[24] Secunia. Xen multiple vulnerability report.
http://secunia.com/advisories/44502/. Accessed
in August 2011.
[25] E. Shi, A. Perrig, and L. van Doorn. BIND: A
ﬁne-grained attestation service for secure distributed
systems. In Proceedings of the 2005 IEEE Symposium
on Security and Privacy, May 2005.
[26] K. Shimizu, H. P. Hofstee, and J. S. Liberty. Cell
broadband engine processor vault security
architecture. IBM J. Res. Dev., pages 521–528,
September 2007.
[27] U. Steinberg and B. Kauer. NOVA: a
microhypervisor-based secure virtualization
architecture. In Proceedings of the 5th European
conference on Computer systems (EuroSys’10), pages
209–222. ACM, 2010.
[28] Trusted Computing Group.
https://www.trustedcomputinggroup.org/.
[29] Trusted Computing Group. TPM speciﬁcations
version 1.2. https://www.trustedcomputinggroup.
org/downloads/specifications/tpm/tpm, July 2005.
[30] J. Wang, A. Stavrou, and A. K. Ghosh. HyperCheck:
A hardware-assisted integrity monitor. In Proceedings
of the 13th International Symposium on Recent
Advances in Intrusion Detection (RAID’10),
September 2010.
[31] Z. Wang and X. Jiang. HyperSafe: A lightweight
approach to provide lifetime hypervisor control-ﬂow
integrity. In Proceedings of the 31st IEEE Symposium
on Security and Privacy, May 2010.
[32] R. Wojtczuk and J. Rutkowska. Xen 0wning trilogy.
In Black Hat conference, 2008.
[33] R. Wojtczuk and J. Rutkowska. Attacking SMM
memory via Intel CPU cache poisoning. Invisible
Things Lab, 2009.
[34] J. Yang and K. G. Shin. Using hypervisor to provide
data secrecy for user applications on a per-page basis.
In Proceedings of the fourth ACM SIGPLAN/SIGOPS
international conference on Virtual execution
environments (VEE’08), pages 71–80, 2008.
APPENDIX
A. PORTABILITY TO INTEL PLATFORMS
In this section, we discuss the portability of SICE to Intel
platforms. Although SMM is supported by both AMD and
Intel, SICE uses some hardware features that are only pro-
vided by AMD. In the following, we discuss these features
and propose alternative techniques for Intel platforms. Im-
plementing SICE on Intel platforms using these techniques
needs further research, particularly to achieve both the se-
curity and performance required from this system.
SMRAM Size and Location Modiﬁcation: After the
SMRAM is locked, Intel platforms do not allow runtime
change of its size and location. This restriction has a di-
rect impact on our techniques, which rely on changing the
SMRAM size at run time to enable fast context switching
to the isolated environment. However, some other Intel fea-
tures can be used for SICE. For instance, new Intel hardware
is equipped with special instructions to do a high through-
put cryptographic operations using the Advanced Encryp-
tion Standard (AES). In the time-sharing mode, this feature
can be used to encrypt and authenticate the memory of the
isolated workload while the legacy host is running, and de-
crypt and verify the same memory in the isolated environ-
ment. Thus, the SMRAM can only be used as a permanent
storage of encryption keys. Further research is needed to
evaluate the feasibility and performance of this approach.
Memory Double-view: Intel platforms deﬁne the TSeg
SMRAM in its memory controller hub, which is shared among
multiple cores. Unless the implementation is modiﬁed to
move the SMRAM deﬁnition to a core speciﬁc location, our
memory double-view technique will not be able to isolate
individual Intel processor cores.
Nevertheless, current Intel implementation can apply this
technique on processor nodes as they do not share the same
memory control hub, allowing a more coarse grained isola-
tion of the resources on physical platforms.
Multi-core Protection: Intel does not provide a capabil-
ity that can disable all system interrupts. As mentioned in
Section 4.2, we rely on this capability to prevent the host
core from using the startup IPI to modify the execution en-
vironment of the isolated core. However, the startup IPI can
only set the instruction pointer to an address in the lowest
1MB of the physical memory. This address range can be
easily modiﬁed to point to a non-writable memory chip.
388