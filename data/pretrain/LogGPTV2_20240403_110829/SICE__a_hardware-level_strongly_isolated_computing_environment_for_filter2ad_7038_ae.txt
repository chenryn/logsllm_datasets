### Performance Overhead Calculation

To calculate the performance overhead of each test, we first measure \( T_K \), the time required to run the test using an unmodified QEMU/KVM. Subsequently, we measure \( T_S \), the time needed to run the same experiment using SICE. The performance overhead is then calculated using the equation:
\[
\text{Performance Overhead} = \left( \frac{T_S - T_K}{T_K} \right) \times 100\%
\]

### Test Case Selection

We selected our test cases in three categories:

1. **User-Level Programs**: These tests evaluate the user-level operations of guest VMs running with SICE.
2. **Kernel Throughput**: This group of tests measures the throughput of the guest VM kernel.
3. **Emulated Network Interface**: These tests assess the performance of the emulated network interface.

### User-Level Programs

In the first category, we conducted three tests:

1. **File Copy Latency**: We measured the latency of copying a 2.1 MB file. Our prototype system does not support hard drive emulation, so the file is copied within the virtual file system representing the initial RAM disk. This test primarily evaluates the impact of SICE on the guest VM's memory operations, as it involves copying between two memory locations rather than actual disk access.
2. **Gzip and Gunzip**: We used the `gzip` and `gunzip` programs to compress and decompress the same file. These tests aim to assess the impact of SICE on the guest VM’s user-level computations. The final results are obtained as an average of 100 runs.

### Kernel Throughput

In the second category, we measured the guest VM kernel responses to three main operations: `fork`, `getpid`, and `insmod`.

- **Fork and Getpid**: These were tested using a custom program that calls these system calls 10,000 times and measures the average response time.
- **Insmod**: We used the `insmod` program to insert a 16.4 KB loadable kernel module into the guest kernel. This test was repeated 10 times, and all runs showed very little variance.

### Emulated Network Interface

In the last category, we ran a single test that uses `wget` to retrieve a 156.6 KB file from an Apache server running on the legacy host. The test was repeated 10 times.

### Experimental Results

Figure 6 shows the results of our experiments. In the multi-core mode, all tests, except for `wget`, showed a slight performance overhead under 3%. We attribute this minor overhead to the time required to copy information to and from the shared memory between the legacy host and the security manager.

The `wget` test, however, showed a 17% performance overhead. A significant portion of this overhead is due to the non-optimized implementation of the QEMU network emulator for SICE. In normal operations, QEMU receives an I/O command and directly accesses the VM's physical memory to emulate the DMA read of the packet. In our prototype, QEMU does not have direct access to the guest VM's physical memory. Instead, we modified QEMU to send DMA requests to the security manager, which copies the packet from the protected guest VM memory to the shared memory for QEMU to process. Similarly, an extra step is necessary when the VM receives a packet.

### Extra Communication Step

Our prototype requires an extra communication step between QEMU in the legacy host and the security manager for every network packet. This extra step necessitates an additional context switch from the legacy host to the isolated environment in the time-sharing mode or an extra inter-process communication between the host core and the isolated core in the multi-core mode. This extra communication step is the primary cause of the high overhead in the `wget` test.

To support this claim, we measured the number of exits to and from the isolated environment during the test. In normal operations, there was an average of 1,249 guest VM exits that required hardware emulation by KVM and/or QEMU. In SICE’s time-sharing mode, there was an average of 1,877 context switches between the isolated environment and the legacy host. A similar number of messages were passed between the host core and the isolated core in the multi-core mode. This 50% increase in communication time explains the high overhead in this test.

### Reducing Overhead

To avoid this overhead, the VM device driver can be modified to use the shared memory for the network adapter's DMA operations. This way, the QEMU-emulated driver could directly read the passed packets without sending an extra request to the security manager.

### Time-Sharing Mode Overhead

The time-sharing mode showed a higher performance overhead, which is expected due to the 67 µs required to switch between the host and the isolated guest VM. Tests that require a higher number of switches, such as `gzip` and `wget`, showed higher overhead. The time-sharing mode had a 40% overhead in the `wget` test, with the normal VM operation taking 0.532 seconds compared to 0.739 seconds in the time-sharing mode. Given that each context switch requires an average of 67 µs, the context switch overhead in this test was 0.126 seconds. This indicates that the time-sharing mode is not suitable for I/O-intensive operations but is useful for programs requiring enhanced isolation and a smaller I/O footprint (e.g., secret key operations).

### Related Work

Several researchers have attempted to use hypervisors to enable strong isolation between workloads in cloud computing environments. However, recent attacks and vulnerability reports show that hypervisors are subject to security exploitation. There have been efforts to verify the runtime integrity of hypervisors, but these techniques still require users to trust the host environment, which has a large TCB and interacts with mutually distrusted workloads.

Recent approaches to eliminate the hypervisor from the TCB can be divided into microhypervisor-based and hardware-based approaches. Hardware-based approaches, such as Intel’s P-MAPS, offer runtime memory isolation but lack details on multi-core isolation. IBM’s Cell BE architecture provides multi-core isolation but is specific to that architecture. SICE achieves similar isolation on the x86 architecture, which is widely used in general-purpose computing platforms.

Microhypervisor-based approaches, such as NOVA, Trustvisor, and seL4, have been discussed. seL4, in particular, uses formal verification to introduce a vulnerability-free microkernel but cannot be extended to commodity hypervisors due to its restrictions. Future research will explore applying formal verification to ensure the integrity of SICE’s security manager.

### Conclusion

In this paper, we presented SICE, a research prototype that explores the capability of current hardware platforms in providing more secure isolated environments. We demonstrated that current hardware architecture, such as SMM, can support strong isolation with a small TCB. Practical deployment of SICE would require detailed security reviews of the BIOS software and SMM implementation by CPU and hardware platform vendors.

SICE provides unique capabilities, including fast context switching, protected concurrent execution, and attestation to its integrity. It offers a cost-effective solution for security-sensitive workloads without dedicated hardware. Future research will focus on extending SICE to Intel platforms and exploring formal verification methods for its security manager.

### References

[References are listed here, formatted consistently and accurately.]

### Appendix

#### Portability to Intel Platforms

This section discusses the portability of SICE to Intel platforms. Although SMM is supported by both AMD and Intel, SICE uses some hardware features specific to AMD. We propose alternative techniques for Intel platforms and discuss their feasibility and performance.

- **SMRAM Size and Location Modification**: Intel platforms do not allow runtime changes to SMRAM size and location after it is locked. An alternative approach could use AES instructions for high-throughput cryptographic operations to encrypt and authenticate the isolated workload's memory.
- **Memory Double-view**: Intel's TSeg SMRAM is defined in the memory controller hub, shared among multiple cores. Modifying the implementation to move SMRAM to a core-specific location is necessary for fine-grained isolation.
- **Multi-core Protection**: Intel does not provide a capability to disable all system interrupts. However, the startup IPI can be restricted to point to a non-writable memory chip, mitigating this issue.

Further research is needed to evaluate the feasibility and performance of these approaches on Intel platforms.