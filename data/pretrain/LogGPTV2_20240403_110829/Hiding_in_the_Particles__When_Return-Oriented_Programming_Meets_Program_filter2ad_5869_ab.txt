general deobfuscation. Those will drive our ROP encoding
techniques to build chains that may withstand such attacks.
A. Principles behind Automated Deobfuscation
Banescu et al. [38] identify a common pre-requisite in
automated attacks perpetrated by reverse engineers: the need
for building a suite of inputs that exercise the different paths
a protected program can actually take. Achieving a coverage
as high as 100% represents G2 for our attacker, while for G1
depending on the speciﬁc function fewer paths may sufﬁce but
also data dependencies should be solved. Slowing down the
generation of a “test suite” for the attacker is a ﬁrst cut of the
effectiveness of an obfuscation [38], as it is a key step in most
deobfuscation pipelines for utterly disparate end goals.
By analyzing deobfuscation research, we abstract three gen-
eral attack surfaces that a “good” obfuscation shall consider:
A1 Disassembly. It should not be immediate for an attacker
to discover code portions using static analysis techniques;
A2 Brute-force search. Syntactic code manipulations such
as tracking and “inverting” the direction of control trans-
fers should not reveal new code, but further dependencies
must be solved in order to take valid alternate paths;
A3 State space. When an obfuscation makes provisions to
artiﬁcially extend the program state space to be explored,
analyses based on forward and backward dependencies
of program variables should fail to simplify them away.
In the next section we present eminent approaches for such
automated attacks, which we then consider in §VII to evaluate
our ROP obfuscation. How to transform an existing program
function into a ROP chain and make it robust against these
three attack types are the subject of §IV and §V, respectively.
B. State-of-the-art Deobfuscation Solutions
1) General Techniques: Deobfuscation attacks can draw
from static and dynamic program analyses. Several static
techniques are capable of reasoning about run-time properties
of the program, and may be the only avenue when the attacker
cannot readily bring execution to a protected program portion
or control its inputs. In this context, symbolic execution (SE)
reveals the multiple paths a piece of code may take by making
it read symbolic instead of concrete input values, and by
collecting and reasoning on path constraints over the symbols
at every encountered branch. Upon termination of each path,
an SMT solver generates a concrete input to exercise it [11].
Scalability issues often cripple static approaches, and dy-
namic solutions may try to get around them by leveraging facts
observed in a concrete execution. Dynamic symbolic execution
(DSE) interleaves concrete and symbolic execution, collecting
constraints at branch decisions that are now determined by the
concrete input values, and generates new inputs by negating
the constraints collected for branching decisions.
Obfuscators can however induce constraints that are hard
for a solver, or expand the program state space artiﬁcially.
Building on the intuition that these transformations are not part
of the original program semantics, taint-driven simpliﬁcation
(TDS) tracks explicit and implicit ﬂows of values from inputs
to program outputs, untangling the control ﬂow of an obfus-
cation method apart from that of the original program [7].
TDS is a general, dynamic, and semantics-based technique:
it applies a selection of semantics-preserving simpliﬁcations
to a recorded trace and produces a simpliﬁed CFG. TDS
can operate symbiotically with DSE to uncover new code by
feeding DSE with the simpliﬁed trace: in [7] this symbiosis
turned out effective in cases that DSE alone could not handle.
TDS succeeded on code protected by state-of-the-art VM
obfuscators, as well as on four hand-written ROP programs.
We consider SE, DSE, and TDS as they represent powerful
tools available to adversaries, and embody concepts seen
also in attacks against speciﬁc obfuscations (e.g., [40]). Prior
literature [14], [38], [39] uses SE and DSE to evaluate and
compare obfuscation techniques on goals akin to G1 and
G2, as both approaches are powerful and driven only by the
semantics of the code (i.e., syntactic changes have little effect).
2) ROP-Aware Techniques: Expressing programs as ROP
payloads affects analysis techniques that account for the syn-
tactic representation of code. For instance, even commercial
disassemblers and decompilers are not equipped to deal with
this exotic representation and would fail to produce meaningful
outputs for ROP chains [41]. Currently researchers have come
up with two solutions to handle complex ROP code.
ROPMEMU [9] attempts dynamic multi-path exploration by
looking for sequences that leak condition ﬂags from the CPU
status register, as they may take part in branching sequences
(§II-B):
it ﬂips their value and tries to generate alternate
execution traces that explore new code. ROPMEMU is not
the sole embodiment of this technique, seen in, e.g., crash-free
binary exploration [42] and malware unpacking [43] research
for RIP-driven code. ROPMEMU eventually removes the ROP
dispatching logic (i.e.,
the ret sequences) and performs
further simpliﬁcations, reconstructing a CFG representation.
ROPDissector [10] addresses shortcoming of ROPMEMU in
branch identiﬁcation, with a data-ﬂow analysis for identifying
sequences that build variable RSP offsets, so to ﬂip all and
only the operations taking part in the process. ROPDissector
builds a ROP CFG highlighting branching points and basic
blocks in a chain, and operates as a static technique as it does
not require a valid execution context as starting point.
In our evaluation we will consider a combination of the two
approaches, speculating on extensions tailored to our design.
IV. PROGRAM ENCODING WITH ROP
We design a binary rewriter for protecting compiled pro-
grams: the user speciﬁes one or more functions of interest
that the rewriter encodes as self-contained ROP chains stored
in a data section of the binary. Our implementation supports
compiler-generated, possibly stripped x64 Linux binaries. To
ensure compatibility between ROP chains and non-ROP code
modules, we intercept and preserve stack manipulations and
use a separate stack for the chain. This section details the
Fig. 2: Architecture of the ROP rewriter.
design of the rewriter (Figure 2), how it encodes generic
functions as self-contained chains, and its present limitations.
A. Geometry of a ROP Encoder
1) Gadget Sources: The ﬁrst decision to face in the design
of a ROP encoder is where to ﬁnd gadgets. These may
be found in statically and dynamically linked libraries, in
program parts left unobfuscated, or in custom code added to
the program. We ruled out static libraries as a binary might not
have any, and dynamic ones to avoid dependencies on speciﬁc
library versions that must be present in any target system.
Exploitation research suggests that program code as small
as 20-100KB may already contain minimal gadget sets for
attacks [28]. Our scenario however is ideal: the possibility of
controlling and altering the binary grants us more wiggle room
compared to attack scenarios, as we can add missing gadgets—
and most importantly create diversiﬁed alternatives—as dead
code in the .text section of the program. We thus pick
gadgets from a pool of artiﬁcial gadgets combined with
gadgets already available in program parts left unobfuscated.
2) Rewriting: The second decision concerns deploying the
encoder as a binary rewriter (as we do) or a compiler pass. Bi-
nary rewriting can handle a larger pool of programs, including
proprietary software and programs with a custom compilation
toolchain, and builds on analyses that extract facts necessary
to assist the rewriting. A compiler pass has some such anal-
yses (e.g., liveness) already available during compilation, and
possibly more control over code shape. However, in order to
be able to rewrite an entire function, we believe a pass may
have to operate as last step (modifying or directly emitting
machine instructions) and/or constrain or rewrite several pieces
of upper passes (e.g., instruction selection, register allocation).
This would lead to a pass that is platform-dependent and that
faces similar challenges to a rewriter while being less general.
3) Control Transfers and Stack Layout: Obfuscated func-
tions get expressed in ROP, but may need to interact with
surrounding components, calling (or being called by) non-ROP
program/library functions or other ROP functions. In this re-
spect native code makes assumptions on the stack layout of the
functions, e.g., when writing return addresses or referencing
stack objects in the scope of a function and its callees.
Reassembleable disassembling literature [44]–[47] describes
known hurdles when trying to turn hard-coded stack references
into symbols that can be moved around. In our design we
instead preserve the original stack behavior of the program:
we place the chain in a separate region, and rewrite RSP
dereferences and value updates to use a other_rsp value
that mimics how the original code would see RSP (Figure 3).
4
BinaryProgramINPUTObfuscatedProgramOUTPUTCFGReconstructionLivenessAnalysisGadgetFinderTranslationChain CraftingMaterializationROP EncoderThis choice ensures a great deal of compatibility, and
avoids that calls to native functions may overwrite parts of
the ROP chain when executing. We keep other_rsp in
a stack-switching array ss that ensures smooth transitions
between the ROP and native domains and supports multiple
concurrently active calls to ROP functions, including (mutual)
recursion and interleavings with native calls.
We store the number of active ROP function instances in
the ﬁrst cell of the array, making the last one accessible as
*(ss+*ss). When upon a call we need to switch to the
native domain, we use other_rsp to store the resumption
point for the ROP call site, and move its old value in RSP
so to switch stacks. Upon function return, a special gadget
switches RSP and other_rsp again (Figure 4).
4) Chain Embedding: Upon generation of a ROP chain,
we replace the original function body in the program with a
stub that switches the stack and activates the chain. We opt
for chains without destructive side effects, avoiding to have to
restore fresh copies across subsequent invocations. We place
the generated chains at the end of the executable’s .data
section or in a dedicated one.
B. Translation, Chain Crafting, and Materialization
This section describes the rewriting pipeline we use in the
ROP encoder of Figure 2. Although we operate on compiled
code, the pipeline mirrors typical steps of compiler architec-
tures [48]: we use a number of support analyses (yellow and
grey boxes) and translate the original instructions to a simple
custom representation made of roplets, which we process in
the chain crafting stage by selecting suitable gadgets for their
lowering and then allocating registers and other operands. A
ﬁnal materialization step instantiates symbolic offsets in the
chain and embeds the output raw bytes in the binary.
1) Translation: The unit of transformation is the function.
We identify code blocks and branches in it using off-the-
shelf disassemblers (CFG reconstruction element of Figure 2):
Ghidra [49] worked ﬂawlessly in our tests when analyzing
indirect branches, and we support angr [50] and radare2 [51]
as alternatives. We then translate one basic block at a time,
turning its instructions into a sequence of roplets.
A roplet is a basic operation of one of the following kinds:
• intra-procedural
transfer, for direct branches and for
indirect branches coming from switch tables (§A);
• inter-procedural transfer, for calls to non-ROP and ROP
functions (including jmp-optimized tail recursion cases);
• epilogue, for handling instructions like ret and leave;
• direct stack access, when dereferencing and updating RSP
with dedicated read or write primitives (e.g., push, pop);
• stack pointer reference, when the original program reads
the RSP value as source or destination operand in an
instruction, or alters it by, e.g., adding a quantity to it;
• instruction pointer reference, to handle RIP-relative ad-
dressing typical of accesses to global storage in .data;
• data movement, for mov-like data transfers that do not
fall in any of the three cases above;
• ALU, for arithmetic and logic operations.
Fig. 3: Reading a stack variable from top of native stack.
One roplet
is usually sufﬁcient
to describe the major-
ity of program instructions. In some cases we break them
down in multiple operations: for instance, for a mov qword
[rsp+8], rax we generate a stack pointer reference and
a data movement roplet. To ease the later register allocation
step, we annotate each roplet with the list of live registers1
found for the original instruction via liveness analysis.
At this stage we parametrically rewire every stack-related
operation to use other_rsp, and transform RIP-relative
addressing instances in absolute references to global storage.
2) Chain Crafting: When the representation enters the
chain crafting stage, we lower the roplets in each basic
block by drawing from suitable gadgets for each roplet type
(using the gadget ﬁnder element of Figure 2). For instance,
to translate a conditional (left) or unconditional (right) intra-
procedural transfer we combine gadgets to achieve:
pop {reg1} ## L
mov {reg2}, 0x0
cmov{ncc} {reg1}, {reg2}
add rsp, {reg1}
pop {reg1} ## L
add rsp, {reg1}
where a gadget may cover one or more consecutive lines
(so we omit ret above). In both codes the pop gadget will
read from the stack an operand L (placed as an immediate
between the addresses of the ﬁrst and second gadget) that
represents the offset of the destination block. L is a symbol
that we materialize once the layout of the chain is ﬁnalized,
similarly to what a compiler assembler does with labels.
Following the analogy, when choosing gadgets for roplets
we operate as when in the instruction selection stage of a
compiler [48], with {regX} representing a virtual register,
roplets the middle-level representation, and gadgets the low-
level one. When it comes to instruction scheduling, we follow
the order of the original instructions in the block.
Native function calls see a special treatment, as we have to
switch stacks and set up the return address in a way to make
another switch and resume the chain (§IV-A). For the call we
combine gadgets as in the following:
## ss
pop {reg1}
add {reg1}, qword ptr [{reg1}]
sub qword ptr [{reg1}], 0x8
mov {reg2}, qword ptr [{reg1}]
pop {reg3}
mov qword ptr [{reg2}], {reg3}
pop {reg2}
xchg rsp, qword ptr [{reg1}]; jmp {reg2} ## step C
## addr. of return gadget
## function address
## step B ends
## step A ends
1A backward analysis deems a register live if the function may later read
it before writing to it, ending, or making a call that may clobber it [52], [53].
5
          sizess + sizeother_rspROP chainnative stackret addressvar1rspgadgetssize        ssvar1         (ss+size) ←←* **sswhere we pop from the stack the addresses of: the stack-
switching array, a function-return gadget, and the function to
call. Gadgets may cover one or more consecutive lines, except
for the last one which describes an independent single JOP
gadget (§II-B): xchg and jmp switch stacks and jump into
the native function at once. Figure 4 shows the effects of the
three main steps carried by the sequence.
The called native function sees as return address (top entry
of its stack frame) the address of the function-return gadget.
This is a synthetic gadget with a statically hard-wired ss
address that reads the RSP value saved by the xchg at call
time and swaps stacks again:
mov {reg1}, ss; add {reg1}, qword ptr [{reg1}];
xchg rsp, qword ptr [{reg1}]; ret
For space limitations we omit details on the lowering of
other roplet types: their handling becomes ordinary once we
translated RSP and RIP-related manipulations (§IV-B1).
Register allocation is the next main step: we choose among
candidates available for a desired gadget operation by taking
into account the registers they operate on and those originally
used in the program, trying to preserve the original choices
whenever possible. When we ﬁnd conﬂicts that may clobber a
register, we use scratch registers when available (i.e., non-live
ones) or spill it to an inlined 8-byte chain slot as a fallback.
We then ensure a reconciliation of register bindings [54] at the
granularity of basic blocks: when execution leaves a block,
the CPU register contents reﬂect the expected locations for
program values that are live in the remainder of the function.