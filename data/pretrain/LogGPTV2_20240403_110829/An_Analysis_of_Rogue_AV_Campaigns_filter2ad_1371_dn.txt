and network-level delays (between the suspect server and the mothership node).
The measurement techniques used to estimate the three types of delay are shown
in Fig. 3. We discuss the techniques in detail in the following sub-sections.
4.1 Network Delay Measurement
Network delay (ND) is deﬁned as the diﬀerence between the time a client sends
out the ﬁrst TCP SYN packet to the suspect server and the time the client
receives the corresponding TCP SYN+ACK packet from the server. By using
Fast-Flux Bot Detection in Real Time
473
this estimate, a TCP connection only yields one network delay sample. To collect
more samples, when appropriate, our scheme temporarily disables the persistent
connection option in HTTP 1.1, which ensures a separate TCP connection for
each HTTP request; thus, the number of ND samples will be the same as the
number of HTTP requests.
4.2 Processing Delay Measurement
Measuring processing delays (PD) at the suspect server is not straightforward
because HTTP does not support such operations naturally. We need a HTTP
command that will respond to the client without contacting the back-end moth-
ership (if any), irrespective of whether the suspect server is a fast-ﬂux bot. For
this purpose, we attempted to make the following requests:
1. Valid HTTP requests with methods other than GET, e.g., OPTIONS and HEADER
methods.
2. HTTP requests with an invalid version number.
3. HTTP requests with incomplete headers.
4. HTTP requests with an undeﬁned method, e.g., a nonsense HI method.
Our experiments showed that most fast-ﬂux bots still contacted their mothership
in the ﬁrst three scenarios. On the other hand, most of them rejected HTTP
requests with undeﬁned methods directly by sending back a HTTP response,
usually with the status code 400 (Bad Request) or 405 (Method Not Allowed).
Consequently, we estimate the processing delay at the server by subtracting
the network round-trip time from the application-level message round-trip time.
Speciﬁcally, assuming AD is the diﬀerence between the time a client sends out
a HTTP request with an undeﬁned method and the time the client receives the
corresponding HTTP response (code 400 or 405), then a PD sample is estimated
by subtracting ND (the network delay) from AD.
4.3 Document Fetch Delay Measurement
We deﬁne the document fetch delay (DFD) as the time required for the suspect
server to “fetch” a webpage. Since the fetch operation occurs at the server side,
we cannot know exactly what happens on the remote server. Thus, we employ
the following simple estimator. Assuming RD is the diﬀerence between the time
a client sends out a successful HTTP GET request and the time the client
receives the corresponding HTTP response (code 200), then a DFD sample is
estimated by subtracting ND (the network delay) from RD. Figure 4 shows the
distribution of DFD, PD and their respective standard deviations measured for
benign servers, traditional bots, and fast-ﬂux bots.
4.4 Decision Algorithm
In this sub-section, we explain how we utilize the three delay metrics in our
decision algorithm.
474
C.-H. Hsu, C.-Y. Huang, and K.-T. Chen
Benign Server
Fast−flux bot
Traditional bot
1e−04
1e−02
DFD
1e+00
0
0
+
e
1
D
P
2
0
−
e
1
4
0
−
e
1
0
0
+
e
1
)
D
P
(
d
s
2
0
−
e
1
4
0
−
e
1
0
0
+
e
1
)
D
F
D
(
d
s
2
0
−
e
1
4
0
−
e
1
0
0
+
e
1
)
D
F
D
(
d
s
2
0
−
e
1
4
0
−
e
1
1e−04
1e−02
DFD
1e+00
1e−04
1e−02
DFD
1e+00
1e−04
1e−02
PD
1e+00
Fig. 4. Scatter plots of processing delays, document fetch delays, and their respective
standard deviations. Both the x- and y-axis are in log scale.
– The objective of measuring network delays is to capture the level of network
congestion between a client and the suspect server. As per Section 3.2, the
ND and sd(ND), where sd(·) denotes the standard deviation, tend to be
(relatively) large if the suspect server is a fast-ﬂux bot rather than a benign,
dedicated web server.
– The processing delay helps us determine the server’s workload and the re-
quired computation power. If there are other workloads on the server, the
estimated processing delays would be high and ﬂuctuate over time. Thus, as
per Section 3.2 and Section 3.3, the PD and sd(PD) tend to be large if the
suspect server is a fast-ﬂux bot.
– The document fetch delay indicates how much time the server takes to fetch
a webpage. Because of the request delegation model (Section 3.1), DFD and
sd(DFD) tend to be large if the suspect server is a fast-ﬂux bot.
For a suspect server, we collect six feature vectors (ND, PD, DFD, and their
respective standard deviations), each of which contains n elements assuming n
HTTP GET requests are issued. For the PD samples, another n HTTP requests
with an undeﬁned method must also to be issued.
To determine whether a suspect server is a fast-ﬂux bot, which is a binary
classiﬁcation problem, we employ a supervised classiﬁcation framework and use
linear SVM [4] as our classiﬁer. A data set containing the delay measurement
results for both benign web servers and web servers hosted on fast-ﬂux bots is
used to train the classiﬁer. When a client wishes to browse pages on an unknown
website, our scheme collects the delay measurements and applies the classiﬁer
to determine whether the suspect server is part of a fast-ﬂux botnet.
y
t
i
s
n
e
D
0
1
.
8
0
.
6
0
.
4
0
.
2
.
0
0
.
0
0
Benign domain
10
15
5
# unique ASN per domain
20
25
Fast-Flux Bot Detection in Real Time
475
y
t
i
s
n
e
D
8
1
0
0
.
0
1
0
0
.
5
0
0
.
0
0
0
0
.
0
30
0
Fast−flux domain
200
# unique ASN per domain
600
400
800
Fig. 5. The distribution of unique autonomous system numbers (ASNs) per domain
name in the dataset of benign servers and fast-ﬂux bots
5 Methodology Evaluation
In this section, we evaluate the performance of the proposed fast-ﬂux bot detec-
tion scheme. First, we describe the data set and examine whether the derived
features diﬀer signiﬁcantly according to the type of suspect server. Then, we
discuss the detection performance of the scheme and consider a passive use of
the scheme.
5.1 Data Description
To evaluate the performance of the proposed scheme in real-life scenarios, we
need a set of URLs that legitimate users can browse. Our dataset contains the
following three categories of URLs, which point to diﬀerent kinds of servers:
– Benign servers: The top 500 websites listed in the Alexa directory [1].
– Traditional bots: URLs that appear in the PhishTank database [19] with
suspicious fast-ﬂux domains removed (see below).
– Fast-ﬂux bots: URLs that appear in the ATLAS Global Fast Flux database [2]
and the FastFlux Tracker at abuse.ch [6].
Between January and April 2010, we used wget to retrieve the URLs in our
dataset at hourly intervals. During the web page retrieval process, we ran tcpdump
to monitor all the network packets sent from and received by the client. After re-
trieving each web page, we sent out a HTTP request with the undeﬁned method
“HI” to measure the processing delays that occurred at the suspect server, as
described in Section 4.2.
We found that some URLs in the PhishTank database actually point to fast-
ﬂux bots, and some URLs listed as pointers to fast-ﬂux bots may actually point
476
C.-H. Hsu, C.-Y. Huang, and K.-T. Chen
Table 2. The trace used to evaluate the detection performance of the proposed scheme
Host type
#domain #IP address #session #connection
Benign servers
Traditional bots
Fast-ﬂux bots
500
16,317
397
3,121
9,116
3,513
60,936
79,694
213,562
565,466
943,752
726,762
to traditional bots. Therefore, after collecting the data, we performed a post
hoc check based on the number of distinct autonomous system numbers (ASNs).
Figure 5 shows the distributions of distinct ASNs of benign domain names and
fast-ﬂux domain names over the trace period. Nearly all the benign domain
names were associated with three or fewer ASNs, while most fast-ﬂux domain
names were associated with many more ASNs over the three-month period.
Based on this observation, we set 3 ASNs as the threshold to determine whether
or not a domain name was associated with a fast-ﬂux botnet. Thus, if a do-
main name was reported as a non-fast-ﬂux bot, but it was associated with four
or more ASNs (or vice versa), we regarded the domain name as questionable.
We simply removed such domain names from our trace to ensure its clarity and
correctness. In addition, if a URL was unavailable due to domain name resolu-
tion failures, packet unreachable errors, HTTP service shutdown, or removal of
corresponding web services for 10 successive attempts, we removed it from the
dataset.
The three-month trace is summarized in Table 2, where a connection refers to
a TCP connection, a session refers to a complete web page transfer (including the
HTML page and its accessory ﬁles, such as images and CSS ﬁles). As we turned
oﬀ the HTTP 1.1 persistent connection option in order to acquire more samples
for the delay metrics (cf. Section 4), the number of connections is much higher
than that of sessions because a web page often contains several accessory ﬁles
Fig. 6. (a) The top 10 top-level domains and (b) the top 20 countries associated with
the fast-ﬂux domain names in our dataset.
Fast-Flux Bot Detection in Real Time
477
(maybe even dozens). Figure 6 shows the top 10 (out of 19) top-level domains
and the top 20 (out of 127) countries associated with the observed fast-ﬂux bots.
5.2 A Closer Look at the Derived Features
We now examine whether the empirical delay measurements derived during
web browsing can be used to distinguish between fast-ﬂux bots and benign
servers. First, we investigate whether, as expected, consumer-level hosts incur
higher and more variable processing delays and more variable network delays (cf.
Section 3.4). To do this, we use a common technique that infers whether a host is
associated with dial-up links, dynamically conﬁgured IP addresses, or other low-
end Internet connections based on the domain name of reverse DNS lookups [21].
For example, if a host’s domain name contains strings like “dial-up,” “adsl,” and
“cable-modem,” we assume that the host is for residential use and connects to
the Internet via relatively slow links. Figure 7 shows the distributions of the
six features for normal and consumer-level hosts. The plots ﬁt our expectation
that consumer-level hosts of fast-ﬂux botnet incur more variable network de-
lays, longer processing delays, and more variable processing delays than those
of dedicated servers. In addition, we consider that the longer and more variable
document fetch delays are due to lower computation power and longer disk I/O
access latency on the consumer-level hosts.