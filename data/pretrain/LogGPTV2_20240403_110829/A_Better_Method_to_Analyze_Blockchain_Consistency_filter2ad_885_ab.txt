Random Oracle: All parties have access to a random function
H : {0, 1}∗ → {0, 1}κ which they can access through two
oracles: H(x ) simply outputs H (x ) and H.ver(x, y) outputs
1 iff H (x ) = y and 0 otherwise. In any round r, the players
(as well as A) may make any number of queries to H.ver. On
the other hand, in each round r, honest players can make
only a single query to H, and an adversary A controlling q
parties can make q sequential queries to H.
Protocols based on proof of work are parametrized by p—
the mining hardness parameter. Informally, a proof-of-work
for the block h−1 and message m is a string η such that
H(h−1, η, m)  c log(κ) the following holds:
Pr
view ← EXEC(Π,C) (A, Z , κ) : consistentT (view) = 1
(cid:102)
(cid:103)
≥ 1 − ϵ1(κ) − ϵ2(T )
Note that a direct consequence of consistency is that the chain
length of any two honest players can differ by at most T (except
with negligible probability in T ).
3 A SIMPLE MARKOV FRAMEWORK FOR
ANALYZING BLOCKCHAIN PROTOCOLS
We present a simple analysis framework for blockchains that com-
bines the approach of Pass et al. with natural Markov chains that
capture protocol dynamics in the presence of adversaries.
At a high-level, each state of our Markov chain represents an
initial state of the blockchain system or the state of the system
following an event of interest. The events of interest include (a)
a new block mined by an honest player; (b) a new block mined
by the adversary; and (c) a sufficiently long quiet period. Each
edge represents an event and has an associated length, which is a
random variable denoting the time it takes for the event to occur,
conditioned on its occurrence. The actual definition of the Markov
chain (the states and the particular events of interest) depends on
the protocol being analyzed, a model of the adversary or an attack
being considered, and the performance measure of interest.
All of the Markov chains used in our analysis satisfy the follow-
ing properties: (a) they are time-homogeneous; that is, the probabil-
ity of transitioning from one state to another is only dependent on
the states, and not on the time at which the transition occurs; (b)
they are irreducible; that is, it is possible to get to any state from any
other state; and (c) they are ergodic; that is, every state is aperiodic
and has a positive mean recurrence time. We refer the reader to
a standard text on probability theory or randomized algorithms
(e.g., [15, Chapter 7]) for more information on Markov chains.
Stationary distribution: Every time-homogeneous, irreducible,
ergodic Markov chain has a (unique) stationary distribution π. For
any given state v, π (v) represents the limit, as n tends to ∞, of
the probability that the chain will be in state v after n transitions
(independent of the starting state). Stationary probabilities can also
be defined for the edges of the chain. Once we define a Markov
chain to model a certain aspect of a blockchain protocol, we derive
the stationary distributions for each state and edge of the Markov
chain, through a set of difference equations. This yields, for each
state v, the stationary probability π (v) of being in state v. Thus,
for any sequence of T rounds, the expected number of visits to v
tends to π (v)T , as T grows. A similar calculation can be done for
each edge of the Markov chain.
Concentration bounds: The stationary distribution of a Markov
chain captures the expected number of occurrences of a particular
state (or transition) over a long sequence of transitions. In any
particular instance of the sequence (e.g., a particular run of the
protocol), however, the exact number of occurrences of a state of
interest could certainly deviate from the expectation. Many “well-
behaved” Markov chains satisfy tight concentration bounds which
indicate that the probability that a given measure of interest (e.g.,
the number of occurrences of a particular event) deviates from the
expected value of the measure is exponentially small in the size of
the deviation. Such concentration bounds enable us to establish that
for a sufficiently long sequence of rounds, the measure of interest
Session 4C: Blockchain 1CCS’18, October 15-19, 2018, Toronto, ON, Canada732(e.g., number of convergence opportunities) is with high probability
close to the expectation given by the stationary distribution.
More formally, we invoke the following theorem on Chernoff-
Hoeffding bounds for “generalized” Markov chains to derive con-
centration bounds for random variables of interest [4].
Theorem 3.1 ([4]). Let M be an ergodic Markov chain with state
space [n] and stationary distribution π. Let T be its ϵ-mixing time
for ϵ ≤ 1/8. Let (V1, . . . , Vt ) denote a t-step random walk on M
starting from an initial distribution ϕ on [n], i.e., V1 ← ϕ . For every
i ∈ [t], let fi
: [n] → [0, 1] be a weight function at step i such
that the expected weight Ev←π [fi (v)] = µ for all i. Define the total
i =1 fi (Vi ). There exists
weight of the walk (V1, . . . , Vt ) by X = (cid:80)t
some constant c (which is independent of µ, δ, and ϵ) such that
(1) Pr[X ≥ (1 + δ )µt] ≤ c∥ϕ∥π e−δ 2µt /(72T ) for 0 ≤ δ ≤ 1,
(2) Pr[X ≥ (1 + δ )µt] ≤ c∥ϕ∥π e−δ µt /(72T ) for δ > 1, and
(3) Pr[X ≤ (1 − δ )µt] ≤ c∥ϕ∥π e−δ 2µt /(72T ) for 0 ≤ δ ≤ 1,
where ∥ϕ∥π is the π-norm of ϕ given by(cid:80)
i∈[n] ϕ
2
i /π (i).
Using the above theorem, we are able to establish our bounds
with high probability; that is, the bound fails with probability that
decreases exponentially in the number of rounds executed by the
protocol. In order to apply the theorem, we show how to transform
our original Markov chain and set the weight functions so that X
captures the particular measure of interest. For instance, to measure
the number of visits to a particular state v, we set fi (v) to 1 and set
fi (u) to 0 for u (cid:44) v, for all i. To measure the number of traversals
of a particular edge, we add an auxiliary vertex in the middle of the
edge, and set the measure to be number of visits to the auxiliary
vertex. In all of our applications of the theorem, the transformed
Markov chains are such that both the number of states and the
mixing time are constant, independent of the number of rounds T ,
but possibly dependent on the model parameters, such as p and ∆.
Thus, for every v, we are able to show that in T rounds, the number
of visits to state v is (1± δ )π (v)T with probability 1− e−δ π (v )Ω(T ).
Such concentration bounds enable us to focus our attention on
analyzing the stationary distributions of the Markov chain.
In the following sections, we apply our approach to analyze
three different blockchain protocols—Nakamoto, Cliquechain, and
GHOST— and derive a range of analytical results: (a) consistency
proofs for the protocols via bounds on convergence opportunities;
(b) analysis of resilience against delaying and balancing attacks; (c)
bounds on new performance measures (e.g., length of forks).
4 NAKAMOTO ANALYSIS
In this section, we analyze the Nakamoto protocol using our Markov
framework. We begin by reviewing, in §4.1, Pass et al.’s analysis
of Nakamoto using bounds on chain growth, block expiry, and the
important notion of convergence opportunities they introduce for
establishing consistency. In §4.2, we reconsider [17]’s analysis of
convergence opportunities using our Markov framework, and show
how their analysis yields an underestimate. In §4.3, we present a
new lower bound for achieving consistency in Nakamoto’s protocol
by an improved analysis of convergence opportunities using our
Markov chain. Finally, in §4.4, we present a detailed analysis of
Nakamoto’s protocol under a consensus attack, deriving bounds on
the probability that the attack can force forks of a specific length.
4.1 Chain Growth, Block Expiry, Consistency
We begin with the lower bound of [17] on chain growth. Recall that
the maximum number of rounds any message can be delayed is ∆.
Let µ = 1 − ρ denote the fraction of honest players.
Lemma 4.1 (Nakamoto Chain Growth). For any δ > 0, the
growth of the main chain of any honest player in Nakamoto’s pro-
tocol in T rounds is at least T (1 − δ )
∆(c +µ ) , except with probability
that drops exponentially in T .
µ
Proof. For any i ≥ 1, let Ti denote the number of rounds it
takes for the main chain to grow from i to i + 1. If an honest player
mines a block for a chain length l at time r, by time r + ∆ all honest
players know about this block and will now mine a block for a
chain of length at least l + 1. The expected number of rounds for an
+ ∆. The
honest player to mine a block is c ∆
µ
expected number of rounds for a chain growth of 1 is at most c ∆
+∆;
µ
using standard Chernoff-Hoeffding bounds [3, 10], the number of
rounds for a chain growth of д is at most (1 + δ )( c ∆
+ ∆)д with
µ
probability 1− e−Ω(д). Thus, in T rounds, Nakamoto achieves chain
growth of at least (1 − δ )T
□
with probability 1 − e Ω(T ).
; therefore, E[Ti] ≤ c ∆
µ
µ
∆(c +µ )
A key part of the consistency proof of Pass, Seeman, and Shelat
relies on their “no long block withholding” lemma [17, Lemma 6.10],
which states that if an adversary withholds a block for too long,
it will not end up in the chain of any honest player. This lemma
allows us to make statements about what an adversary is able to
do in a given window of rounds without having to consider more
than a constant number of blocks the adversary mined previously
and didn’t announce, which they may still use in an attack. In this
section we restate that lemma within our framework. This lemma
is useful when we redefine the consistency bounds of Nakamoto’s
protocol, and also for evaluating other protocols in later sections.
Using Lemma 4.1, we get an altered version of the block with-
holding lemma which we refer to as block expiry. Let b be a block
mined by the adversary at time r, and let r + t be the first time any
honest player hears of b; we say b expires if there exists a negligible
function ϵ (.) such that the probability b ends up in the mainchain
of any honest player anytime after r + t is ≤ ϵ (t ).
Lemma 4.2 (Nakamoto Block Expiry). There exists a δ ∈ (0, 1)
such that if µ ≥ δ ρ, then every adversarial block expires.
Proof. Let b be any adversarial block. We set δ such that the ex-
pected growth of any adversarial chain is smaller than the expected
growth of any honest chain. For any T , the expected growth of any
adversarial chain is at most T
. By a standard Chernoff-Hoeffding
ρ
bound [3, 10], for any δ′ > 0, the probability that the adversarial
c ∆
chain grows by at least (1 + δ′)T
is at most inverse exponential
in T . So, from Lemma 4.1, we set the parameters such that
ρ
c ∆
µ
(c + µ)
>
ρ
c
. Thus, the probability that at any time after r + t, the adversary
mines a chain longer than any honest player’s chain at that time is
≤ ϵ (t ρ) where ϵ is inverse exponential in its argument.
□
Session 4C: Blockchain 1CCS’18, October 15-19, 2018, Toronto, ON, Canada733In the consistency analysis of Nakamoto, Pass, Seeman, and
shelat consider any window of T rounds and count special events,
called Convergence Opportunities, which are events after which all
honest players agree on a single chain; we define them formally in
the following subsection. If an adversary wants to break consistency,
they must combat all convergence opportunities. To analyze what
an adversary can do in a given window of T rounds, we must also
argue that there are only a constant number of blocks the adversary
mined before the window, which the adversary can use in an attack
during the window. We now state our version of the consistency
theorem of [17] for any blockchain protocol which states that if
those two properties hold, then the protocol satisfies consistency.
Theorem 4.3 (Blockchain Consistency). A blockchain proto-
col satisfies consistency if ∃δ ∈ (0, 1) satisfying µ ≥ δα such that
for any integer T and in any window of T rounds, with probability
1−ϵ (T ) for a negligible function ϵ (·), the number of convergence op-
portunities C is greater than the number of adversarial blocks needed
to break all convergence opportunities A, and the number of blocks
mined before T which the adversary can use in T to break conver-
gence opportunities is less than C − A.
Using the above theorem, Pass et al derive the following condi-
tion for Nakamoto’s protocol to achieve consistency, where α =
1 − (1 − p) (1−ρ )n and β = ρnp.
α (1 − (2∆ + 2)α ) ≥ (1 + δ )β .
4.2 Counting Convergence Opportunities
Using Markov Chains
We reconsider the analysis of convergence opportunities in [17]
using our Markov approach. A convergence opportunity is an event
after which all honest players agree on a single block as the lat-
est block and therefore agree on a single longest chain. The con-
vergence opportunity is made up of 3 sequences of rounds, each
characterized by the outcome of mining by the honest players.
• First, ∆ rounds pass in which no honest player mines a block.
Thus, by the model, at the end of the ∆ rounds, all honest
players know all honest blocks, and therefore agree on what
is the maximum length of the chain (though not necessarily
the same chain).
• Second, a single honest player mines, thus extending a chain
by one more block than the previous longest chain.
• Third, another ∆ rounds pass in which no honest player
mines. Thus, at the end, all honest players know the new
block and therefore agree on the single longest chain.
To prove that a given protocol achieves consistency, the analysis
first argues that to prevent consensus, it is necessary for the ad-
versary to “break” all convergence opportunities. An adversary
can break a convergence opportunity by disrupting either of the
quiet periods of step one and three by announcing one of their
own blocks during that time. Thus, the analysis attempts to bound
both the number of convergence opportunities the honest players
have and the number of blocks the adversary must mine to break
those. To obtain this count, the analysis in [17] sums over all honest
blocks mined (hits) in any time interval and tracks whether the
“quiet” period between honest hits is less than ∆. In a given period
of L honest hits, let q denote the number of quiet periods between
hit ≤ ∆
∆
hit +∆
S0
S1
hit, hit ≤ ∆