search/rescue mission over a large area). Our focus is to ﬁnd
logic ﬂaws in swarm algorithms. Traditional software/hard-
ware vulnerabilities of drones such as GPS jamming/spoof-
ing [21], [22] and network packet injections are not our focus.
III. MOTIVATING EXAMPLE
We use a drone swarm mission running Adaptive Swarm [4]
to show how SWARMFLAWFINDER discovers logic ﬂaws.
Target Swarm Mission. The target swarm aims to deliver
an object that requires four drones’ cooperation as shown in
Fig. 1-(a). Each drone is attached with a string to hold the
object. Typically, it takes 189.4 (±5.8) ticks to complete (We
proﬁle 100 runs of the mission to obtain the completion time).
Adversary. We assume an adversary wants to discover the
swarm algorithm’s logic ﬂaws that can be exploited by an
attacker controlled external drone, in order to fail the mission.
We consider the swarm mission is failed if the swarm does
not reach the destination in 400 ticks (i.e., two times longer
than the typical mission completion time mentioned above).
Logic Flaw Discovery. SWARMFLAWFINDER conducts
guided fuzz testing via the following four steps.
1) Test Creation: Given the target swarm mission, we create
the initial test (T1 in Fig. 1-(a)). A test case consists of two
elements: the attack drone’s pose (or location; P ) and an attack
strategy (S). For the initial test, we randomly pick P and S
where P being near a victim drone while avoiding being too
close to the victim drone (i.e., indicated by the gray area in
Fig. 1-(a)), because choosing such a value may cause a crash
immediately after the spawn. The attack strategy S represents
how the attack drone will act after the spawn. There are four
strategies S1∼S4: S1 pushes a victim drone against its ﬂight
Fig. 1. SWARMFLAWFINDER in action on the motivation example.
direction and S3 represents a strategy that moves between two
victim drones. Other strategies can be found in § IV-A.
2) Test Evaluation and DCC Computation: We run the test
Ti and measure the attack drones’ impact on the victim swarm.
We propose the concept of the degree of causal contribution
(or DCC), which is based on the principle of counterfactual
causality [23], [24], to measure the impact. Brieﬂy, a causal
relationship between an attack drone and a victim drone is
inferred by comparing an execution with the attack drone and
its counterfactual execution, which does not include the attack
drone. Any observable differences between the two executions
essentially represent
the causality between the attack and
victim drones (Details about the counterfactual causality are
in § IV-B). Speciﬁcally, for each victim drone, we identify all
external objects that can affect the swarm operation. In this
example, the external objects for a victim drone (e.g., Leader)
include an attack drone, three victim swarm’s drones (Follower
1∼3), and a moving object (OM ). To compute DCC, for every
external object, we run an additional test without the external
object. Any observed differences on the victim drone’s pose
between the tests with and without the object (e.g., represented
as ∆ in Fig. 1) are collected. We repeat this for all external
objects, and accumulate the ∆ values to get the DCC values,
shown at the bottom of Fig. 1-(b)∼(d).
3) Test Mutation Guided by DCC: After each test’s ex-
ecution, SWARMFLAWFINDER checks whether there was a
previous test that has a similar DCC of the current test. If there
are no similar DCC values observed previously, we consider
that the current test exercises a new behavior of the target
swarm. Hence, SWARMFLAWFINDER tries to prioritize tests
that are similar to the current test. It derives the next test by
mutating the test case slightly, denoted by µ(P , S, δ). Observe
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:38:51 UTC from IEEE Xplore.  Restrictions apply. 
31810
0%20%40%60%80%100%1357911130%20%40%60%80%100%1357911130%20%40%60%80%100%135791113Follower 3Follower 1Follower 2LeaderObjectStringString(a) Mutating test cases (by the µ function)SAT1={}SCµ(P, S, ∆)T3 = {}µ(P, S,δ)(b) Impact of the attack drone in T1T1Delta (   )(c) Impact of the attack drone in T2T2Delta (   )(d) Impact of the attack drone in T3T3Delta (   )Delta (   )OMOMOMOMT2={}SA0%20%40%60%80%100%1234567891011121310050020     24     28     320%20%40%60%80%100%1357911130%20%40%60%80%100%135791113DestinationFollower 2Follower 3AttackerMoving ObstacleDcc(%)TickLegend for Dcc100500Leader20     24     28     32Follower 2100500Dcc(%)Tick100500LeaderFollower 220     24     28     3220     24     28     32100500Dcc(%)Tick100500Leader20     24     28     3220     24     28     32Follower 2Fig. 2. Physical experiment reproducing the crash shown in Fig. 3 (L means Leader and F1∼3 indicates Follower 1∼3).
and Follower 2, the sum of FG, FO, and FA of Leader makes
the drone move towards the obstacle, leading to the crash.
Physical Experiment. To show that the identiﬁed logic ﬂaw
can be exploited in the real world, we reproduce the motivation
example with real drones in our lab environment, as shown in
Fig. 2. Observe that we present the photos of real drones on the
upperside along with the simpliﬁed versions of the photos on
the bottom. A and O represent the attack drone and the moving
obstacle, respectively. The victim drones are connected with
red strings to hold an object (illustrated as a red diamond on
the bottom). Fig. 2 shows three steps: (a) the attack drone
and obstacle are approaching the victim swarm. (b) the attack
drone inﬂuences a victim drone’s decision, making it move
toward the obstacle. (c) the obstacle and the victim drone
inﬂuenced by the attack drone crashed, resulting in the entire
swarm crashed onto the ground (illustrated by the gray color).
Generality. We further analyze the crash in detail and discover
that Adaptive Swarm [4] does not handle multiple obstacles
well
the above crash is not an
accidental crash but it is caused by a fundamental weakness
of the algorithm. Details of the root cause of this error are
presented in § V-B (C1-2. Naive multi-force handling).
in general, meaning that
IV. DESIGN
Fig. 4 shows the overview of SWARMFLAWFINDER. It takes
a target swarm algorithm and a swarm mission (including the
deﬁnition of mission success and failure) as input. It runs
an initial test with attack drones (§ IV-A). If a test mission
ﬁnishes successfully ( 1 ), it conducts execution perturbation
(§ IV-B) to understand whether the current
test exercised
a new behavior of the swarm or not. Based on the result,
SWARMFLAWFINDER mutates the current test and continues
testing ( 2 , § IV-C). If a test leads to a mission failure ( 3 ),
the attack drones’ conﬁguration is obtained as output ( 4 ). It
repeats the above process until it reaches a predeﬁned timeout.
A. Test-run Deﬁnition and Creation
A test-run is deﬁned as a set of tuples  where P and
S represent an attack drone’s pose and its strategy respectively.
A test with n attack drones is composed of multiple tuples:
{, , ..., }. To facilitate the
discussion, we ﬁrst focus on testing with a single attack drone.
We discuss testing with multiple attack drones in § IV-D.
Fig. 3. Crash (caused by a logic ﬂaw) found by SWARMFLAWFINDER.
that T1 and T2 in Fig. 1 have the same S1 (i.e., not mutated).
If the current test’s DCC is similar to one of the previously
observed DCC values (e.g., DCC of Fig. 1-(b) and (c) are
similar), SWARMFLAWFINDER mutates the current test more
signiﬁcantly to derive a completely new test case for the next
test (e.g., T3 is derived by mutating both P and S of T2).
4) Repeating Test Execution and Mutation: We repeat the
Step 2 and Step 3 for a given amount of time (i.e., timeout): 24
hours in this example. During the testing process, we observe
a test case execution leading to a swarm mission failure due
to a crash between a victim drone and the moving obstacle
(OM ). Note that OM is not an attacker controlled object. The
victim swarm is capable of avoiding OM without an attack
drone introduced by our system. SWARMFLAWFINDER also
logs the details of the test causing mission failures (e.g., attack
drone’s pose and strategy) for analysis.
Logic Flaw in the Algorithm. Fig. 3 explains the details
of a logic ﬂaw discovered by SWARMFLAWFINDER. In this
scenario, three forces are considered to determine the ﬁnal
ﬂight direction of the victim drones. First, all four victim
drones try to move toward the goal, denoted by FG. If there
are no other forces to consider, FG becomes the ﬁnal ﬂight
direction denoted by the red arrow. Follower 1 and 3 are such
cases. Second, when an attack drone comes close to a victim
drone (e.g., Leader and Follower 2 in Fig. 3), the victim drone
tries to avoid it, causing FA. Third, when a moving obstacle
approaches the victim drone, it tries to avoid the obstacle (FO).
Note that when multiple forces are involved, the ﬁnal ﬂight
direction is determined by adding all the forces’ vectors. In this
example, when the attack drone ﬂies in the middle of Leader
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:38:51 UTC from IEEE Xplore.  Restrictions apply. 
41811
(a) Attack drone and obstacle approach the victim swarm(b) Attack drone influences a victim drone(c) Leader drone crashed into the obstacleOOAAAAAAOOOOF1LF2F3F1LF3F2LF1F3F2F1LF2F3LF2F1F3LF1F3F2FGFAFOFGFALegendFlight direction (i.e., final decision)Force to avoid the attack drone (FA)Force to fly toward the goal (i.e., destination) (FG)Force to avoid the dynamic obstacle (FO)Attack droneVictim drone’s sensing area for external objectsMoving obstacleLeaderFollower 2Follower 1Follower 3FGFGFig. 4. Overview of SWARMFLAWFINDER. (The shaded area represents SWARMFLAWFINDER with input and output on the left and right respectively)
Attack Drone’s Pose (P ). P represents the initial pose of the
attack drone in a test. P is essentially a point in 3D space
in drone swarms, represented by three values on xy, xz, and
yz-planes: . P ’s value range is large as it can be any
point in 3D space except for the points that are close to victim
drones (which can cause crashes even before a victim drone
tries to avoid collisions). For example, if a victim drone’s
sensing area (i.e., the area that the victim drone can detect
an object) is deﬁned as x × y × z (length × width × height),
we only allow a value of P that is outside of the x × y × z
from the center of each victim drone. The sensing area can
be obtained by running a simple test with an external object
and observing the distance the victim drone starts to avoid the
object. After the attack drone is spawned at P , it moves toward
the victim drone to execute its attack strategy S (explained in
the next paragraph). Different P values can lead to different
timings of the attack drone approaching the victim swarm.
Attack Strategy (S). After an attack drone is spawned at P , it
detects the victim swarm and moves near the swarm. Then, it
conducts an attack based on the strategy S deﬁned as follows
(An illustration of the strategies can be found in [8]).
1. Pushing Back (S1): An attack drone tries to push back
a victim drone (i.e., against
the victim drone’s ﬂight
direction). In a swarm, this strategy typically delays the
progress of the swarm reaching the destination.
2. Chasing (S2): An attack drone closely follows a single
victim drone in a swarm. It typically causes a victim to
speed up, often making it difﬁcult for the victim to control
itself from crashing into other objects.
3. Dividing (S3): An attack drone ﬂies into the middle
of two victim drones to divide a group of drones into
smaller groups. It aims to disrupt the swarm’s collective
operation, making the swarm sparse or smaller sized.
4. Herding (S4): It aims to change the direction of an entire
swarm or the size of the swarm via attack drones pushing
victim drones from the outmost layer of the swarm.
B. Test Execution and Evaluation
Initial Test Creation and Execution. We create the initial
test case by randomly choosing P and S for a single attack
drone. We run the created test case which spawns an attack
drone at P with an attack strategy S.
Test Evaluation. After a test, we evaluate the effectiveness of
the test. If the test case (i.e., ) effectively exercises
a new behavior of the victim swarm, we consider the test
case is effective and try to run similar tests with a slight
mutation (e.g., changing P to have less than 1 meter change
from the original P and do not change S). Otherwise, we try to
mutate the current test case signiﬁcantly to derive a completely
different test that may exercise a new behavior of the victim
swarm. For example, we consider a signiﬁcant mutation to be
(1) mutating P to have more than 1 meters (10 times of the
attack drone’s size) change and (2) selecting a different S.
• Challenges: Unfortunately, coverage based metrics (e.g.,
instruction or branch coverage) that are commonly used in
traditional software testing do not work well for swarm
algorithms because the algorithms are highly iterative. We
observe that even between signiﬁcantly different tests, the
coverage metrics stay similar. Alternatively, one may record
victim drones’ poses (e.g., coordinate values) during the test
run and use the pose trace. However, the pose trace is too
sensitive, meaning that even for very similar tests, they may
differ signiﬁcantly. Even running the same test multiple times
likely results in different poses, due to the non-deterministic
nature of swarm robotics. Hence, pose traces are not desirable.
• Our solution: We focus on the attack drones’ impact on the
victim swarm, where the impact can be intuitively measured by
the victim drones’ reactions to the attack drones. To quantify
the impact (or swarm’s reactions), we propose the degree of
the causal contribution (or DCC). The idea behind the DCC is
counterfactual causality [23], [24] which explains the meaning
of causal claims in terms of counterfactual conditionals: “If X
had not occurred, Y would not have occurred.”
Counterfactual Causality [23] is the most widely used
deﬁnition of causality. We adapt the above counterfactual
conditional statement to the context of inferring adversaries’
impact on drone swarm algorithms’ execution. Speciﬁcally,
a victim drone’s behavior B is causally dependent on an
adversary A, if A did not exist, B would not exist. To this
end, we conduct additional experiments to infer the causality
between A and B.
Given an execution Eorg of a swarm algorithm, we create a
new counterfactual execution Ecf that does not include A, to
test the counterfactual condition. From the above deﬁnition,
we can infer the causality between A and B as follows. If
B is only observed in Eorg but not in Ecf , B is causally
dependent on A. Note that B in our context is not a binary
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:38:51 UTC from IEEE Xplore.  Restrictions apply. 
51812
Swarm algorithmSwarm missionExecution perturbation(Section IV-B)Attack drone configurations, causing mission failuresTest creation and execution(Section IV-A and B)Test evaluation using Dccvalues (Section IV-B)Feedback-driven fuzzing (Section IV-C and D)…Mission failureMission successInputOutputTimeout (No errors found)1234Algorithm 1: Feedback based fuzz testing
: Dv: a set of variables representing victim drones.
Input
Da: a set of variables representing attack drones.
Ow: a set of variables representing objects in the world.
Ttimeout : the maximum time limit for the testing (i.e., timeout).
Output: Efailed : a set of executions that were failed due to the attack drones.
1 procedure SwarmDcc(E, Dv, Da, Ow, Tend )
2
3
t ← 0
while t ≠ Tend do
// Each victim drone d
for d ∈ Dv do
∆Total ← 0
Oall ← Dv ∪ Da ∪ Ow
Porg ← GetPose (E, d, Oall , t) // Pose of a victim drone d at t
// Each variable o representing objects including attack/victim
drone and obstacles
for oi ∈ Oall do
obak ← oi // Save oi
oi ← ∅ // Removing an object oi
Pi ← GetPose (E, d, Oall , t) // Pose of d at t without i
∆i ←ࢯ Porg − Piࢯ // ∆ for oi via Euclidean Distance
∆Total ← ∆Total + ∆i
oi ← obak // Restore oi
for oi ∈ Oall do
DCC(d, t) ← DCC(d, t) ∪ 
t ← t + 1
return DCC
18
19 procedure FuzzTesting(Dv, Da, Ow, Ttimeout )
20
21
22
23
Efailed ← {}
Ecur ← CreateInitialTest(Dv, Da, Ow) // Create the ﬁrst test
Nthreshold ← 0.87 // NCC threshold (conﬁgurable).
while the elapsed time of testing did not reach Ttimeout do
// Run a test with the current conﬁguration. If the current victim
mission fails, add the execution to the output.
if RunSwarm(Ecur ) = MISSION FAILURE then
Efailed ← Efailed ∪ Ecur
// Obtain DCC values for the current test
DCCcur ← SwarmDcc(Ecur , Dv, Da, Ow, time(Ecur ))
// Check whether the current test produce DCC values different enough
IsNewDcc ← TRUE
for r ∈ Dv do
for di ∈ DCCprev (r) do
if GetNCC(di, DCCcur (r)) > Nthreshold then
IsNewDcc ← FALSE
break
DCCprev ← DCCprev ∪ DCCcur
// The test did not ﬁnd the obtained DCC values are different enough,
meaning that it is similar to one of the previous tests
if IsNewDcc = FALSE then
else
Ecur ← MutateTest(Ecur , R) // Mutate the test signiﬁcantly
Ecur ← MutateTest(Ecur , δ) // Mutate slightly (δ)
return Efailed
4
5
6
7
8
9
10
11
12
13
14
15
16
17
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
but a difference (i.e., delta) between the two executions. In
other words, we aim to infer the causal relationship between
A and B where B is the behavior difference of a victim
drone between Eorg but not in Ecf .
Fig. 5. DCC computation via perturbed swarm executions.
We compute DCC values by (1) perturbing the original
swarm mission’s execution, (2) comparing the original swarm
mission with the perturbed swarm mission executions, and (3)
aggregating the differences of each victim drone in the swarm.
SWARMFLAWFINDER perturbs all objects including victim
drones, objects, and attack drones, one by one in each per-
turbed execution. Fig. 5-(a) shows the original swarm mission
including 3 victim drones, 1 moving obstacle, and 1 attack
drone. SWARMFLAWFINDER creates 5 perturbed executions.
1. Removing the obstacle (b): The obstacle is removed from
the swarm mission. Observe that Drone 1 is now ﬂying
toward the east (gray arrow). The difference between the
original swarm mission is identiﬁed and annotated by ∆1.
2. Removing the attack drone (c): Without the attack drone,
two victim drones (Leader and Drone 2) move toward the
east (i.e., the original destination) as annotated by ∆2.
Drone 2’s ﬂight direction is also changed (∆3) because,
in the original execution (a), it ﬂies slightly south to avoid
the Leader drone that is affected by the attack drone.
3. Removing the Leader (d): In this swarm algorithm, non-
leader drones are instructed to follow the leader drone,
which aims to ﬂy toward the destination. Without the
leader, the other drones do not try to ﬂy toward the east.
Drone 1 reacts to the obstacle more actively since it does
not need to care about the destination (∆4). Drone 2 also
slows down and does not need to follow the leader (∆5).
4. Removing the Drone 1 (e): Drone 1 does not affect
other victim drones’ (Leader and Drone 2) behaviors. We
observe no delta values in this experiment.
5. Removing the Drone 2 (f): Without Drone 2, the leader