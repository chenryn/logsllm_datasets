is referred to as a call site and we use it as a unique identiﬁer for the allocated object’s
type at runtime. Finally, the source code around each call site is analyzed ofﬂine to
determine the type of the kernel object being allocated.
Runtime kernel object map generation. At runtime, LiveDM captures all alloca-
tion and deallocation events by interceding whenever one of the allocation/deallocation
functions is called. There are three things that need to be determined at runtime: (1) the
call site, (2) the address of the objected allocated or deallocated, and (3) the size of the
allocated object.
To determine the call site, LiveDM uses the return address of the call to the allocation
function. In the instruction stream, the return address is the address of the instruction
after the call instruction. The captured call site is stored in the kernel object map so that
the type can be determined during ofﬂine source code analysis.
The address and size of objects being allocated or deallocated can be derived from
the arguments and return value. For an allocation function, the size is typically given
as a function argument and the memory address as the return value. For a deallocation
function, the address is typically given as a function argument. These values can be
determined by the VMM by leveraging function call conventions.2 Function arguments
are delivered through the stack or registers, and LiveDM captures them by inspecting
these locations at the entry of memory allocation/deallocation calls. To capture the re-
turn value, we need to determine where the return value is stored and when it is stored
there. Integers up to 32-bits as well as 32-bit pointers are delivered via the EAX register
and all values that we would like to capture are either of those types. The return value
is available in this register when the allocation function returns to the caller. In order
to capture the return values at the correct time the VMM uses a virtual stack. When a
memory allocation function is called, the return address is extracted and pushed on to
this stack. When the address of the code to be executed matches the return address on
the stack, the VMM intercedes and captures the return value from the EAX register.
Ofﬂine automatic data type determination. The object type information related to
kernel memory allocation events is determined using static analysis of the kernel source
code ofﬂine. Fig. 2(a) illustrates a high level view of our method. First, the allocation
call site (C) of a dynamic object is mapped to the source code fork.c:610 using de-
bugging information found in the kernel binary. This code assigns the address of the
allocated memory to a pointer variable at the left-hand side (LHS) of the assignment
statement (A). Since this variable’s type can represent the type of the allocated memory,
2 A function call convention is a scheme to pass function arguments and a return value. We use
the conventions for the x86 architecture and the gcc compiler [8].
Kernel Malware Analysis with Un-tampered and Temporal Views
185
(a) A high level view of static code analysis
(b) Case 1
(c) Case 2
(d) Case 3
Fig. 2. Static code analysis. C: a call site, A: an assignment, D: a variable declaration, T: a type
deﬁnition, R: a return, and F: a function declaration
it is derived by traversing the declaration of this pointer (D) and the deﬁnition of its type
(T). Speciﬁcally, during the compilation of kernel source code, a parser sets the depen-
dencies among the internal representations (IRs) of such code elements. Therefore, the
type can be found by following the dependencies of the generated IRs.
For type resolution, we enumerate several patterns in the allocation code as shown in
Fig. 2(b), 2(c), and 2(d). Case 1 is the typical pattern (C→A→D→T) as just explained.
In Case 2, the deﬁnition (D) and allocation (A) occur in the same line. The handling of
this case is very similar to that of Case 1. Case 3, however, is unlike the ﬁrst two cases.
The pattern in Case 3 does not use a variable to handle the allocated memory address,
rather it directly returns the value generated from the allocation call. When a call site (C)
is converted to a return statement (R), we determine the type of the allocated memory
using the type of the returning function (F). In Fig. 2(d), this pattern is presented as
C→R→F→T.
Prior to static code analysis, we generate the set of information about these code el-
ements to be traversed (i.e., C, A, D, R, F, and T) by compiling the kernel source code
with the compiler that we instrumented (Section 4).
4 Implementation
Allocation-driven mapping is general enough to work with an OS that follows the stan-
dard function call conventions (e.g., Linux, Windows, etc.). Our prototype, LiveDM, sup-
ports three off-the-shelf Linux OSes of different kernel versions: Fedora Core 6 (Linux
2.6.18), Debian Sarge (Linux 2.6.8), and Redhat 8 (Linux 2.4.18).
LiveDM can be implemented on any software virtualization system, such as VMware
(Workstation and Player) [29], VirtualBox [26], and Parallels [14]. We choose the QEMU
[2] with KQEMU optimizer for implementation convenience.
In the kernel source code, many wrappers are used for kernel memory management,
some of which are deﬁned as macros or inline functions and others as regular functions.
186
J. Rhee et al.
Macros and inline functions are resolved as the core memory function calls at compile
time by a preprocessor; thus, their call sites are captured in the same way as core func-
tions. However, in the case of regular wrapper functions, the call sites will belong to the
wrapper code.
To solve this problem, we take two approaches. If a wrapper is used only a few times,
we consider that the type from the wrapper can indirectly imply the type used in the
wrapper’s caller due to its limited use. If a wrapper is widely used in many places (e.g.,
kmem cache alloc – a slab allocator), we treat it as a memory allocation function. Com-
modity OSes, which have mature code quality, have a well deﬁned set of memory wrap-
per functions that the kernel and driver code commonly use. In our experience, capturing
such wrappers, in addition to the core memory functions, can cover the majority of the
memory allocation and deallocation operations.
We categorize the captured functions into four classes: (1) page allocation/free func-
tions, (2) kmalloc/kfree functions, (3) kmem cache alloc/free functions (slab al-
locators), and (4) vmalloc/vfree functions (contiguous memory allocators). These
sets include the well deﬁned wrapper functions as well as the core memory functions.
In our prototype, we capture about 20 functions in each guest kernel. The memory func-
tions of an OS kernel can be determined from its design speciﬁcation (e.g., the Linux
Kernel API) or kernel source code.
Automatic translation of a call site to a data type requires a kernel binary that is com-
piled with a debugging ﬂag (e.g., -gto gcc) and whose symbols are not stripped. Modern
OSes, such as Ubuntu, Fedora, and Windows, generate kernel binaries of this form. Upon
distribution, typically the stripped kernel binaries are shipped; however, unstripped bina-
ries (or symbol information in Windows) are optionally provided for kernel debugging
purposes. The experimented kernels of Debian Sarge and Redhat 8 are not compiled with
this debugging ﬂag. Therefore, we compiled the distributed source code and generated
the debug-enabled kernels. These kernels share the same source code with the distributed
kernels, but the offset of the compiled binary code can be slightly different due to the
additional debugging information.
For static analysis we use a gcc [8] compiler (version 3.2.3) that we instrumented
to generate IRs for the source code of the experimented kernels. We place hooks in the
parser to extract the abstract syntax trees for the code elements necessary in the static
code analysis.
5 Evaluation
In this section, we evaluate the basic functionality of LiveDM with respect to the identiﬁ-
cation of kernel objects, casting code patterns, and the performance of allocation-driven
mapping. The guest systems are conﬁgured with 256MB RAM and the host machine has
a 3.2Ghz Pentium D CPU and 2GB of RAM.
Identifying dynamic kernel objects. To demonstrate the ability of LiveDM to inspect
the runtime status of an OS kernel, we present a list of important kernel data structures
captured during the execution of Debian Sarge OS in Table 1. These data structures man-
age the key OS status such as process information, memory mapping of each process,
and the status of ﬁle systems and network which are often targeted by kernel malware
Kernel Malware Analysis with Un-tampered and Temporal Views
187
Table 1. A list of core dynamic kernel objects and the source code elements used to derive their
data types in static analysis. (OS: Debian Sarge).
Case #Objects
Call Site
Declaration
kernel/fork.c:243
kernel/fork.c:795
fs/exec.c:587
kernel/fork.c:813
arch/i386/mm/pgtable.c:229
kernel/fork.c:431
kernel/fork.c:526
kernel/fork.c:271
mm/mmap.c:748
mm/mmap.c:1521
mm/mmap.c:1657
fs/exec.c:342
kernel/fork.c:654
kernel/fork.c:597
fs/file table.c:69
fs/buffer.c:3062
fs/block dev.c:232
fs/dcache.c:689
fs/inode.c:107
fs/namespace.c:55
fs/proc/inode.c:90
1
g kernel/fork.c:248
1
kernel/fork.c:801
S
1
fs/exec.c:601
1
kernel/fork.c:819
2
arch/i386/mm/pgtable.c:229
1
kernel/fork.c:433
1
kernel/fork.c:559
1
kernel/fork.c:314
1
mm/mmap.c:923
1
mm/mmap.c:1526
1
mm/mmap.c:1722
1
fs/exec.c:402
1
kernel/fork.c:677
2
kernel/fork.c:597
1
fs/file table.c:76
2
fs/buffer.c:3062
2
fs/block dev.c:232
1
fs/dcache.c:692
1
fs/inode.c:112
2
fs/namespace.c:55
1
fs/proc/inode.c:93
drivers/block/ll rw blk.c:1405 drivers/block/ll rw blk.c:1405 request queue t 2
1
drivers/block/ll rw blk.c:2950 drivers/block/ll rw blk.c:2945 io context
1
socket alloc
net/socket.c:279
1
sock
net/core/sock.c:617
1
dst entry
net/core/dst.c:125
1
neighbour
net/core/neighbour.c:265
tcp bind bucket 2
net/ipv4/tcp ipv4.c:134
1
net/ipv4/fib hash.c:586
fib node
Data Type
task struct
sighand struct
sighand struct
signal struct
pgd t
mm struct
mm struct
vm area struct
vm area struct
vm area struct
vm area struct
vm area struct
files struct
fs struct
file
buffer head
bdev inode
dentry
inode
vfsmount
proc inode
net/socket.c:278
net/core/sock.c:613
net/core.dst.c:119
net/core/neighbour.c:254
net/ipv4/tcp ipv4.c:133
net/ipv4/fib hash.c:461
m
e
t
s
y
s
e
l
i
F
66
63
1
66
54
47
7
149
1004
5
48
47
54
53
531
828
5
4203
1209
16
237
18
10
12