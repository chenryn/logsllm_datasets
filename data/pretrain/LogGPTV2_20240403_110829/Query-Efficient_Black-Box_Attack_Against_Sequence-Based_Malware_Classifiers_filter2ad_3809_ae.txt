7.55
76.19
81.09
99.99
11.47
81.25
100.00
51.15
14.31
77.32
79.93
67.11
16.46
88.96
91.96
99.99
25.00
94.87
100.00
One might claim that the same substitute model can be used to camouﬂage more than ten
malware samples, resulting in a lower average budget per sample. However, in most cases an
attacker would try to modify only a single malware, so it can bypass the detector and perform its
malicious functionality. Moreover, even if the average cost per example can be reduced by using
the same substitute model, our attack presents a lower limit on the absolute number of queries,
bypassing a cloud-service that blocks access for a host performing too many queries in a short
amount of time in order to thwart adversarial eﬀorts [22, 19]. The more eﬃcient the attack, the
less chances there are for it to be mitigated by this approach.
The performance of our linear iteration attack, shown in Table 3 (logarithmic backtracking=no
columns), is identical to the performance of the SeqRand algorithm presented in [24] (because both
attacks use the same algorithm).
The attack overhead of all attacks is similar: about 30%, or 40 API calls, per window. Since a
classiﬁer with an API window size of k = 100 provides roughly the same accuracy as with k = 140
Accepted as a conference paper at ACSAC 2020
used here (96.76% vs. 97.61% with the same FP rate for the LSTM classiﬁer), the success of these
attacks is due to the perturbation and not because API sequences were split into two windows due
to the added API calls.
As can be seen, the attacks of Uesato et al.
[30] have low eﬀectiveness.
This is due to the fact that those attacks are not suitable for discrete values of API call types and
indices.
[43] and Ilyas et al.
In contrast, we see that our uniform mixing EA score-based attack has higher attack eﬀec-
tiveness, for a ﬁxed number of queries, even when used for discrete input (API calls or position
indices). This is due to the fact that the transformations used by EA work with discrete sequences:
mutation (random perturbation) in existing adversarial candidates and crossover between several
candidates. In our EA score-based attack, we don’t use crossover, which might make sense for the
NLP domain (e.g., for compound sentences) but not for API call sequences, where each program
has its own business logic. The self-adaptive search used by our EA score-based attack also explains
why it outperforms all other score-based attack variants and has better attack eﬀectiveness than the
gradient-based attack used in [42] with the same number of queries. Our proposed score-based at-
tack outperforms existing methods because it maximizes the attack eﬀectiveness for a ﬁxed number
of queries. Note that the number of queries is per sliding window and not per executable.
Based on the average malicious sequence length, avg(length(xm)) ≈ 10, 000, and the adversarial
sliding window size, k = 140, the average absolute number of queries per malware executable is
~10,000.
As expected, the benign perturbation eﬀect on the decision-based attack eﬀectiveness is the
most signiﬁcant, since without it, the API types are random.
While our decision-based attack eﬀectiveness is 10% lower than the most eﬀective score-based
attacks when using the same budget, it doesn’t require knowledge of the target classiﬁer’s conﬁdence
score, making it the only viable attack in some black-box scenarios.
4.3 Defenses and Mitigation Techniques
To the best of our knowledge, there is currently no published and evaluated method to make a
sequence-based RNN model resistant to adversarial sequences, beyond a brief mention of adversarial
training as a defense method [17, 33]. Adversarial training [26] is the method of adding adversarial
examples, with their non-perturbed label, to the training set of the classiﬁer. The reason is since
adversarial examples are usually out-of-distribution samples, inserting them into the training set
would cause the classiﬁer to learn the entire training set distribution, including the adversarial
examples.
Adversarial training has several limitations:
1. It provides a varying level of robustness, depending on the adversarial examples used.
2. It requires a dataset of adversarial examples to train on. Thus, it has limited generalization
against novel adversarial attacks.
3. It requires retraining the model, incurring signiﬁcant overhead.
We ran the adversarial attacks, both score-based and decision-based variants (Section 1), with and
without benign perturbation (Section 3.2.2) on the training set, as suggested in [34]. For each
column in Tables 3 and 4, we generated 14,000 malicious adversarial examples (50% generated by
the black-box attack and 50% by the white-box attack), which replaced 14,000 malicious samples
Accepted as a conference paper at ACSAC 2020
in the original training set. Other sizes (smaller or larger) resulted in reduced detection rate of the
pre-trained classiﬁer for non-adversarial samples. The adversarial examples were generated using
the same conﬁguration (score/decision-based, random/benign perturbation, number of queries to
generate) as the evaluated attack. The results were the same across all attack types: The attack
eﬀectiveness remains the same, while the attack overhead and number of queries were increased by
10-15%, on average. This is due to the fact that adversarial training is less eﬀective against random
attacks like ours, because a diﬀerent stochastic adversarial sequence is generated every time, making
it challenging for the classiﬁer to generalize from one adversarial sequence to another.
More eﬀective RNN defense methods, including domain speciﬁc methods, e.g., systems that
measure CPU usage [35], contain irregular API call subsequences [27] (such as the no-op API
calls used in this paper), or otherwise assess the plausibility of our attack [38], in order to detect
adversarial examples, will be a part of our future work.
5 Conclusions and Future Work
In this paper, we presented the ﬁrst black-box attack (based on the target classiﬁer’s predicted
class, with and without its conﬁdence score, to ﬁt adversary’s limited knowledge) that generates
adversarial sequences while minimizing the number of queries for the target classiﬁer, reducing the
number of queries by more than 10 times with minimal loss of attack eﬀectiveness in comparison to
the state of the art attack ([42]). This query-eﬃcient approach makes our attack suited to attack
cloud models where a large amount of queries cost money and raise suspicion of an attack, failing
previous attacks.
We demonstrated those attacks against API call sequence-based malware classiﬁers and veriﬁed
the attack eﬀectiveness for all relevant common classiﬁers: RNN variants, feedforward networks,
and traditional machine learning classiﬁers. These are the ﬁrst query-eﬃcient attacks eﬀective
against RNN variants and not just CNNs.
We also evaluated our attacks against four variants of state-of-the-art score-based query-eﬃcient
attacks, modiﬁed to ﬁt discrete sequence input, and showed that our attacks are equal or outperform
all of them.
Finally, we demonstrated that our attacks are eﬀective even when multiple feature types, in-
cluding non-sequential ones, are used (Appendix D).
While this paper focuses on API calls and printable strings as features, the proposed attacks are
valid for every modiﬁable feature type, sequential or not. Furthermore, our attack is generic and
can be applied to other domains, like text analysis (using word sequences instead of API calls), as
would be demonstrated in our future work.
Our future work will focus on developing domain-speciﬁc and domain-agnostic defense mecha-
nisms against such attacks and analyzing additional self-adaptive algorithms to ﬁnd more query-
eﬃcient attacks, while evaluating them on limited knowledge scenarios (e.g., unknown API calls
window size, etc.).
Accepted as a conference paper at ACSAC 2020
References
[1] Amazon Machine Learning. https://aws.amazon.com/machine-learning, 2019. Accessed:
2019-09-26.
[2] Cuckoo Sandbox Hooked APIs and Categories.
https://github.com/cuckoosandbox/
cuckoo/wiki/Hooked-APIs-and-Categories, 2019. Accessed: 2019-08-24.
[3] Cylance, I Kill You! https://skylightcyber.com/2019/07/18/cylance-i-kill-you, 2019.
Accessed: 2019-08-24.
[4] Deploy
trained
Keras
SageMaker.
deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker/, 2019.
cessed: 2019-12-14.
Amazon
or
https://aws.amazon.com/blogs/machine-learning/
Ac-
TensorFlow
models
using
[5] Google Cloud Prediction. https://cloud.google.com/prediction/, 2019. Accessed: 2019-
09-26.
[6] Joe Sandbox ML. https://www.joesecurity.org/joe-sandbox-ML, 2019. Accessed: 2019-
09-26.
[7] Keras. https://keras.io/, 2019. Accessed: 2019-09-26.
[8] Microsoft
ATP.
https://www.microsoft.com/security/blog/2018/02/14/
how-artificial-intelligence-stopped-an-emotet-outbreak/, 2019. Accessed: 2019-09-
26.
[9] SciKit Learn. http://scikit-learn.org/stable/, 2019. Accessed: 2019-09-26.
[10] Scikit Learn Decision Tree Categorial Variable. https://roamanalytics.com/2016/10/28/
are-categorical-variables-getting-lost-in-your-random-forests/, 2019. Accessed:
2019-09-26.
[11] SentinelOne. https://www.sentinelone.com/insights/endpoint-protection-platform-datasheet/,
2019. Accessed: 2019-09-26.
[12] VirusTotal. https://www.virustotal.com/, 2019. Accessed: 2019-09-26.
[13] XGBoost. https://github.com/dmlc/xgboost/, 2019. Accessed: 2019-09-26.
[14] Yara Rules. https://github.com/Yara-Rules/rules, 2019. Accessed: 2019-09-26.
[15] Rakshit Agrawal, Jack W. Stokes, Mady Marinescu, and Karthik Selvaraj. Robust neural
malware detection models for emulation sequence learning. CoRR, abs/1806.10741, 2018.
[16] Moustafa Alzantot, Yash Sharma, Supriyo Chakraborty, and Mani B. Srivastava. Genattack:
Practical black-box attacks with gradient-free optimization. CoRR, abs/1805.11090, 2018.
[17] Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani B. Srivastava, and
Kai-Wei Chang. Generating natural language adversarial examples.
In Ellen Riloﬀ, David
Chiang, Julia Hockenmaier, and Jun’ichi Tsujii, editors, Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November
4, 2018, pages 2890–2896. Association for Computational Linguistics, 2018.
Accepted as a conference paper at ACSAC 2020
[18] Hyrum S. Anderson and Phil Roth. EMBER: an open dataset for training static PE malware
machine learning models. CoRR, abs/1804.04637, 2018.
[19] Duen Horng Chau, Carey Nachenberg, Jeﬀrey Wilhelm, Adam Wright, and Christos Falout-
sos. Polonium: Tera-scale graph mining for malware detection. In Acm sigkdd conference on
knowledge discovery and data mining, 2010.
[20] Tong Che, Yanran Li, Ruixiang Zhang, R. Devon Hjelm, Wenjie Li, Yangqiu Song, and Yoshua
Bengio. Maximum-likelihood augmented discrete generative adversarial networks. CoRR,
abs/1702.07983, 2017.
[21] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth
order optimization based black-box attacks to deep neural networks without training substitute
models. In Proceedings of the 10th ACM Workshop on Artiﬁcial Intelligence and Security -
AISec 17. ACM Press, 2017.
[22] Steven Chen, Nicholas Carlini, and David Wagner. Stateful detection of black-box adversarial
attacks, 2019.
[23] Duc-Cuong Dang and Per Kristian Lehre. Self-adaptation of mutation rates in non-elitist
populations.
In Julia Handl, Emma Hart, Peter R. Lewis, Manuel López-Ibáñez, Gabriela
Ochoa, and Ben Paechter, editors, Parallel Problem Solving from Nature – PPSN XIV, pages
803–813, Cham, 2016. Springer International Publishing.
[24] Hung Dang, Yue Huang, and Ee-Chien Chang. Evading classiﬁers by morphing in the dark. In
Bhavani M. Thuraisingham, David Evans, Tal Malkin, and Dongyan Xu, editors, Proceedings
of the 2017 ACM SIGSAC Conference on Computer and Communications Security, CCS 2017,
Dallas, TX, USA, October 30 - November 03, 2017, pages 119–133. ACM, 2017.
[25] Jennifer G. Dy and Andreas Krause, editors. Proceedings of the 35th International Conference
on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018,
volume 80 of Proceedings of Machine Learning Research. PMLR, 2018.
[26] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and Harnessing Adversarial Examples.
International Conference on Learning Representations (ICLR), December 2015.
[27] Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes, and Patrick D. Mc-
Daniel. On the (statistical) detection of adversarial examples. ArXiv e-prints, abs/1702.06280,
2017.
[28] Weiwei Hu and Ying Tan. Black-box attacks against RNN based malware detection algorithms.
ArXiv e-prints, abs/1705.08131, 2017.
[29] Ling Huang, Anthony D. Joseph, Blaine Nelson, Benjamin I. P. Rubinstein, and J. D. Tygar.
In Yan Chen, Alvaro A. Cárdenas, Rachel Greenstadt, and
Adversarial machine learning.
Benjamin I. P. Rubinstein, editors, Proceedings of the 4th ACM Workshop on Security and
Artiﬁcial Intelligence, AISec 2011, Chicago, IL, USA, October 21, 2011, pages 43–58. ACM,
2011.
[30] Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks
with limited queries and information. In Dy and Krause [25], pages 2142–2151.
Accepted as a conference paper at ACSAC 2020
[31] Jeremy Z. Kolter and Marcus A. Maloof. Learning to detect and classify malicious executables
in the wild. J. Mach. Learn. Res., 7:2721–2744, 2006.
[32] Matt J. Kusner and José Miguel Hernández-Lobato. GANS for sequences of discrete elements
with the gumbel-softmax distribution. CoRR, abs/1611.04051, 2016.
[33] Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang. Textbugger: Generating adver-
sarial text against real-world applications. CoRR, abs/1812.05271, 2018.
[34] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian
Vladu. Towards deep learning models resistant to adversarial attacks. In 6th International
Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May
3, 2018, Conference Track Proceedings. OpenReview.net, 2018.
[35] Robert Moskovitch, Shay Pluderman, Ido Gus, Dima Stopel, Clint Feher, Yisrael Parmet,
Yuval Shahar, and Yuval Elovici. Host based intrusion detection using machine learning.
In IEEE International Conference on Intelligence and Security Informatics, ISI 2007, New
Brunswick, New Jersey, USA, May 23-24, 2007, Proceedings, pages 107–114. IEEE, 2007.
[36] Nicolas Papernot, Patrick McDaniel, Ananthram Swami, and Richard Harang. Crafting adver-
sarial input sequences for recurrent neural networks. In MILCOM 2016 - 2016 IEEE Military
Communications Conference. IEEE, nov 2016.
[37] Feargus Pendlebury, Fabio Pierazzi, Roberto Jordaney, Johannes Kinder, and Lorenzo Cav-
allaro. TESSERACT: Eliminating experimental bias in malware classiﬁcation across space
and time. In 28th USENIX Security Symposium (USENIX Security 19), pages 729–746, Santa
Clara, CA, August 2019. USENIX Association.
[38] Fabio Pierazzi, Feargus Pendlebury, Jacopo Cortellazzi, and Lorenzo Cavallaro. Intriguing
properties of adversarial ML attacks in the problem space.
In 2020 IEEE Symposium on
Security and Privacy, SP 2020, San Francisco, CA, USA, May 18-21, 2020, pages 1332–1349.
IEEE, 2020.
[39] J. Rapin and O. Teytaud. Nevergrad - A gradient-free optimization platform. https://
GitHub.com/FacebookResearch/Nevergrad, 2018.
[40] Ihai Rosenberg, Asaf Shabtai, Yuval Elovici, and Lior Rokach. Adversarial learning in the
cyber security domain, 2020.
[41] Ishai Rosenberg, Shai Meir, Jonathan Berrebi, Ilay Gordon, Guillaume Sicard, and Eli David.
Generating end-to-end adversarial examples for malware classiﬁers using explainability. In The
2020 International Joint Conference on Neural Networks (IJCNN 2020), 2020.
[42] Ishai Rosenberg, Asaf Shabtai, Lior Rokach, and Yuval Elovici. Generic black-box end-to-end
attack against state of the art API call based malware classiﬁers. In Michael Bailey, Thorsten
Holz, Manolis Stamatogiannakis, and Sotiris Ioannidis, editors, Research in Attacks, Intru-