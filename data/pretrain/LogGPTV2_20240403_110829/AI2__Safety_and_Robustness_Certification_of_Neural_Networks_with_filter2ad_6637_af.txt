100%
Veriﬁed robustness
100%
80%
60%
40%
20%
0%
80%
60%
40%
20%
0%
0.001
0.045
0.005
0.065
0.025
0.085
0.001
0.045
0.005
0.065
0.025
0.085
Box Zono
Z2
Z4
Z8
Z16
Z32
Z64
Box Zono
Z2
Z4
Z8
Z16
Z32
Z64
Fig. 10: Veriﬁed properties as a function of the abstract domain used by AI2 for the 9 × 200 network. Each point represents
the fraction of robustness properties for a given bound (as speciﬁed in the legend) veriﬁed by a given abstract domain (x-axis).
(a) MNIST
(b) CIFAR
Time (seconds)
500s
3x20
3x50
6x100
9x200
6x20
3x100
6x200
100s
10s
1s
0.10s
0.01s
Box Zono
Z2
Z4
Z8
Z16
Z32
Z64
Fig. 11: Average running time of AI2 when proving robustness
properties on MNIST networks as a function of the abstract
domain used by AI2 (x-axis). Axes are scaled logarithmically.
The data also demonstrates that bounded sets of zonotopes
further improve AI2’s ability to prove robustness properties.
For the MNIST network, Zonotope64 proves more robustness
properties than Zonotope for all 4 bounds for which Zonotope
fails to prove at least one property (i.e., for bounds δ ≥ 0.025).
For the CIFAR network, Zonotope64 proves more properties
than Zonotope for 4 out of the 5 the bounds. The only
exception is the bound 0.085, where Zonotope64 and Zonotope
prove the same set of properties.
13
Trade-off between Precision and Scalability.
In Fig. 11,
we plot the running time of AI2 as a function of the abstract
domain. Each point in the graph represents the average running
time of AI2 when proving a robustness property for a given
MNIST network (as indicated in the legend). We use a log-log
plot to better visualize the trade-off in time.
The data shows that AI2 can efﬁciently verify robustness
of large networks. AI2 terminates within a few minutes for
all MNIST FNNs and all considered domains. Further, we
observe that AI2 takes less than 10 seconds on average to
verify a property with the Zonotope domain.
As expected,
the graph demonstrates that more precise
domains increase AI2’s running time. More importantly, AI2’s
running time is polynomial in the bound N of ZonotopeN,
which allows one to adjust AI2’s precision by increasing N.
Comparison to Reluplex. The current state-of-the-art system
for verifying properties of neural networks is Reluplex [21].
Reluplex supports FNNs with ReLU activation functions, and
its analysis is sound and complete. Reluplex would eventually
either verify a given property or return a counterexample.
To compare the performance of Reluplex and AI2, we ran
both systems on all MNIST FNN benchmarks. We ran AI2
using Zonotope and Zonotope64. For both Reluplex and AI2,
we set a 1 hour timeout for verifying a single property.
Fig. 12 presents our results: Fig. 12a plots the average
running time of Reluplex and AI2 and Fig. 12b shows the
fraction of robustness properties veriﬁed by the systems. The
data shows that Reluplex can analyze FNNs with at most 600
neurons efﬁciently, typically within a few minutes. Overall,
both system veriﬁed roughly the same set of properties.
However, Reluplex crashed during veriﬁcation of some of the
properties. This explains why AI2 was able to prove slightly
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 05:59:05 UTC from IEEE Xplore.  Restrictions apply. 
Time (seconds)
3,000s
Zonotope
Zonotope64
Reluplex
2,000s
1,000s
0s
Veriﬁed robustness
100%
80%
60%
40%
20%
0%
3x20
6x20
3x50 3x100 6x100 6x200 9x200
3x20
6x20
3x50 3x100 6x100 6x200 9x200
(a)
(b)
Fig. 12: Comparing the performance of AI2 to Reluplex. Each point is an average of the results for all 60 robustness properties
for the MNIST networks. Each point in (a) represents the average time to completion, regardless of the result of the computation.
While not shown, the result of the computation could be a failure to verify, timeout, crash, or discovery of a counterexample.
Each point in (b) represents the fraction of the 60 robustness properties that were veriﬁed.
more properties than Reluplex on the smaller FNNs.
For large networks with more than 600 neurons, the running
time of Reluplex increases signiﬁcantly and its analysis often
times out. In contrast, AI2 analyzes the large networks within
a few minutes and veriﬁes substantially more robustness
properties than Reluplex. For example, Zonotope64 proves
57 out of the 60 properties on the 6 × 200 network, while
Reluplex proves 3. Further, Zonotope64 proves 45 out of the
60 properties on the largest 9 × 200 network, while Reluplex
proves none. We remark that while Reluplex did not verify
any property on the largest 9 × 200 network, it did disprove
some of the properties and returned counterexamples.
We also ran Reluplex without a predeﬁned timeout
to
investigate how long it would take to verify properties on the
large networks. To this end, we ran Reluplex on properties that
AI2 successfully veriﬁed. We observed that Reluplex often
took more than 24 hours to terminate. Overall, our results
indicate that Reluplex does not scale to larger FNNs whereas
AI2 succeeds on these networks.
VII. COMPARING DEFENSES WITH AI2
In this section, we illustrate a practical application of AI2:
evaluating and comparing neural network defenses. A defense
is an algorithm whose goal is to reduce the effectiveness of
a certain attack against a speciﬁc network, for example, by
retraining the network with an altered loss function. Since the
discovery of adversarial examples, many works have sugge-
sted different kinds of defenses to mitigate this phenomenon
(e.g., [12], [27], [42]). A natural metric to compare defenses
is the average “size” of the robustness region on some test set.
Intuitively, the greater this size is, the more robust the defense.
14
We compared three state-of-the-art defenses:
• GSS [12] extends the loss with a regularization term
encoding the fast gradient sign method (FGSM) attack.
• Ensemble [42] is similar to GSS, but includes regulari-
zation terms from attacks on other models.
• MMSTV [27] adds, during training, a perturbation layer
before the input layer which applies the FGSMk attack.
FGSMk is a multi-step variant of FGSM, also known as
projected gradient descent.
All these defenses attempt to reduce the effectiveness of the
FGSM attack [12]. This attack consists of taking a network N
and an input x and computing a vector ρN,x in the input space
along which an adversarial example is likely to be found. An
adversarial input a is then generated by taking a step  along
this vector: a = x +  · ρN,x.
We deﬁne a new kind of robustness region, called line, that
captures resilience with respect to the FGSM attack. The line
robustness region captures all points from x to x + δ · ρN,x
for some robustness bound δ:
LN,x,δ = {x +  · ρN,x |  ∈ [0, δ]}.
This robustness region is a zonotope and can thus be precisely
captured by AI2.
We compared the three state-of-the-art defenses on the
MNIST convolutional network described in Section VI; we
call this the Original network. We trained the Original network
with each of the defenses, which resulted in 3 additional
networks: GSS, Ensemble, and MMSTV. We used 40 epochs
for GSS, 12 epochs for Ensemble, and 10 000 training steps
for MMSTV using their published frameworks.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 05:59:05 UTC from IEEE Xplore.  Restrictions apply. 
Robustness bound δ
0.5
0.4
0.3
0.2
0.1
0
Original
GSS
Ensemble
MMSTV
Defense
Fig. 13: Box-and-whisker plot of the veriﬁed bounds for the
Original, GSS, Ensemble, and MMSTV networks. The boxes
represent the δ for the middle 50% of the images, whereas
the whiskers represent the minimum and maximum δ. The
inner-lines are the averages.
We conducted 20 experiments. In each experiment, we
randomly selected an image x and computed ρN,x. Then, for
each network, our goal was to ﬁnd the largest bound δ for
which AI2 proves the region LN,x,δ robust. To approximate
the largest robustness bound, we ran binary search to depth 6
and ran AI2 with the Zonotope domain for each candidate
bound δ. We refer to the largest robustness bound veriﬁed by
AI2 as the veriﬁed bound.
The average veriﬁed bounds for the Original, GSS, Ensem-
ble, and MMSTV networks are 0.026, 0.031, 0.042, and 0.209,
respectively. Fig. 13 shows a box-and-whisker plot which
demonstrates the distribution of the veriﬁed bounds for the
four networks. The bottom and top of each whisker show the
minimum and maximum veriﬁed bounds discovered during
the 20 experiments. The bottom and top of each whisker’s
box show the ﬁrst and third quartiles of the veriﬁed bounds.
Our results indicate that MMSTV provides a signiﬁcant
increase in provable robustness against
the FGSM attack.
In all 20 experiments, the veriﬁed bound for the MMSTV
network was larger than those found for the Original, GSS,
and Ensemble networks. We observe that GSS and Ensemble
defend the network in a way that makes it only slightly more
provably robust, consistent with observations that these styles
of defense are insufﬁcient [16], [27].
VIII. RELATED WORK
In this section, we survey the works closely related to ours.
Adversarial Examples. [40] showed that neural networks are
vulnerable to small perturbations on inputs. Since then, many
works have focused on constructing adversarial examples.
For example, [30] showed how to ﬁnd adversarial examples
without starting from a test point, [41] generated adversarial
examples using random perturbations, [35] demonstrated that
even intermediate layers are not robust, and [14] generated
adversarial examples for malware classiﬁcation. Other works
presented ways to construct adversarial examples during the
training phase, thereby increasing the network robustness (see
[3], [12], [15], [17], [29], [38]). [1] formalized the notion of
robustness in neural networks and deﬁned metrics to evaluate
the robustness of a neural network. [32] illustrated how to
systematically generate adversarial examples that cover all
neurons in the network.
Neural Network Analysis. Many works have studied the ro-
bustness of networks. [34] presented an abstraction-reﬁnement
approach for FNNs. However, this was shown successful for a
network with only 6 neurons. [37] introduced a bounded model
checking technique to verify safety of a neural network for
the Cart Pole system. [18] showed a veriﬁcation framework,
based on an SMT solver, which veriﬁed the robustness with
respect to a certain set of functions that can manipulate the
input and are minimal (a notion which they deﬁne). However,
it is unclear how one can obtain such a set. [21] extended the
simplex algorithm to verify properties of FNNs with ReLU.
Robustness Analysis of Programs. Many works deal with
robustness analysis of programs (e.g., [4], [5], [13], [28]).
[28] considered a deﬁnition of robustness that is similar to
the one in our work, and [5] used a combination of abstract
interpretation and SMT-based methods to prove robustness of
programs. The programs considered in this literature tend to
be small but have complex constructs such as loops and array
operations. In contrast, neural networks (which are our focus)
are closer to circuits, in that they lack high-level language
features but are potentially massive in size.
IX. CONCLUSION AND FUTURE WORK
We presented AI2, the ﬁrst system able to certify convolu-
tional and large fully connected networks. The key insight
behind AI2 is to phrase the problem of analyzing neural
networks in the classic framework of abstract interpretation.
To this end, we deﬁned abstract transformers that capture the
behavior of common neural network layers and presented a
bounded powerset domain that enables a trade-off between
precision and scalability. Our experimental results showed that
AI2 can effectively handle neural networks that are beyond the
reach of existing methods.
We believe AI2 and the approach behind it is a promising
step towards ensuring the safety and robustness of AI systems.
Currently, we are extending AI2 with additional abstract trans-
formers to support more neural network features. We are also
building a library for modeling common perturbations, such as
rotations, smoothing, and erosion. We believe these extensions
would further improve AI2’s applicability and foster future
research in AI safety.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 05:59:05 UTC from IEEE Xplore.  Restrictions apply. 
15
REFERENCES
[1] Osbert Bastani, Yani Ioannou, Leonidas Lampropoulos, Dimitrios Vy-
tiniotis, Aditya V. Nori, and Antonio Criminisi. Measuring neural net
robustness with constraints.
In Proceedings of the 30th International
Conference on Neural Information Processing Systems (NIPS), pages
2621–2629, 2016.
[2] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Fir-
ner, Beat Flepp, Prasoon Goyal, Lawrence D. Jackel, Mathew Monfort,
Urs Muller, Jiakai Zhang, Xin Zhang, Jake Zhao, and Karol Zieba. End