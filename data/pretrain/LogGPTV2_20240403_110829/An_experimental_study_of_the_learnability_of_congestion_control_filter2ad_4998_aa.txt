title:An experimental study of the learnability of congestion control
author:Anirudh Sivaraman and
Keith Winstein and
Pratiksha Thaker and
Hari Balakrishnan
An Experimental Study of the
Learnability of Congestion Control
Anirudh Sivaraman, Keith Winstein, Pratiksha Thaker, and Hari Balakrishnan
Computer Science and Artiﬁcial Intelligence Laboratory
Massachusetts Institute of Technology, Cambridge, Mass.
{anirudh, keithw, prthaker, hari}@mit.edu
ABSTRACT
When designing a distributed network protocol, typically it is in-
feasible to fully deﬁne the target network where the protocol is in-
tended to be used.
It is therefore natural to ask: How faithfully
do protocol designers really need to understand the networks they
design for? What are the important signals that endpoints should
listen to? How can researchers gain conﬁdence that systems that
work well on well-characterized test networks during development
will also perform adequately on real networks that are inevitably
more complex, or future networks yet to be developed? Is there a
tradeoff between the performance of a protocol and the breadth of
its intended operating range of networks? What is the cost of play-
ing fairly with cross-trafﬁc that is governed by another protocol?
We examine these questions quantitatively in the context of con-
gestion control, by using an automated protocol-design tool to ap-
proximate the best possible congestion-control scheme given im-
perfect prior knowledge about the network. We found only weak
evidence of a tradeoff between operating range in link speeds and
performance, even when the operating range was extended to cover
a thousand-fold range of link speeds. We found that it may be ac-
ceptable to simplify some characteristics of the network—such as
its topology—when modeling for design purposes. Some other fea-
tures, such as the degree of multiplexing and the aggressiveness of
contending endpoints, are important to capture in a model.
CATEGORIES AND SUBJECT DESCRIPTORS
C.2.2 [Computer-Communication Networks]: Network Proto-
cols
KEYWORDS
Protocol; Machine Learning; Congestion Control; Learnability;
Measurement; Simulation
1.
INTRODUCTION
Over the last 30 years, Internet congestion control has seen
considerable research interest. Starting with seminal work in the
1980s [24, 15, 7], the Transmission Control Protocol (TCP) has
adopted a series of end-to-end algorithms to share network re-
sources among contending endpoints [14, 6, 12, 27]. Another
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
SIGCOMM’14, August 17–22, 2014, Chicago, IL, USA.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2836-4/14/08 ...$15.00.
http://dx.doi.org/10.1145/2619239.2626324.
line of work has explored the use of in-network algorithms run-
ning at bottleneck routers to help perform this function more efﬁ-
ciently [11, 10, 9, 18, 23, 21, 16].
As the Internet grows and evolves, it appears likely that new net-
work protocols will continue to be developed to accommodate new
subnetwork behaviors and shifting application workloads and goals.
Some of these may be intended for specialized environments—
e.g., inside a centrally-managed datacenter—while some will be for
broad use across the wide-area Internet, or over cellular network
paths.
In practice, however, it is challenging to guarantee that a dis-
tributed system’s performance on well-characterized test networks
will extend to real networks, which inevitably differ from those en-
visioned in development and will continue to evolve over time. This
uncertain generalizability presents an obstacle to any new proto-
col’s deployment.
In this paper, we formalize the design process for generating end-
to-end congestion-control protocols to quantify: how easy is it to
“learn” a network protocol to achieve desired goals, given a nec-
essarily imperfect model of the networks where it will ultimately be
deployed?
Under this umbrella, we examine a series of questions about
what knowledge about the network is important when designing
a congestion-control protocol and what simpliﬁcations are accept-
able:
1. Knowledge of network parameters. Is there a tradeoff be-
tween the performance of a protocol and the breadth of its
intended operating range of network parameters [30]? Will
a “one size ﬁts all” protocol designed for networks spanning
a broad range of link speeds (§4.1), degrees of multiplex-
ing (§4.2), or propagation delays (§4.3) necessarily perform
worse than one targeted at a narrower subset of networks
whose parameters can be more precisely deﬁned in advance?
2. Structural knowledge. How faithfully do protocol designers
need to understand the topology of the network? What are the
consequences of designing protocols for a simpliﬁed network
path with a single bottleneck, then running them on a network
with two bottlenecks (§4.4)?
3. Knowledge about other endpoints.
In many settings, a
newly-designed protocol will need to coexist with trafﬁc
from other protocols. What are the consequences of design-
ing a protocol with the knowledge that some endpoints may
be running incumbent protocols whose trafﬁc needs to be
treated fairly (e.g. the new protocol needs to divide a con-
tended link evenly with TCP), versus a clean-slate design
(§4.5)? Similarly, what are the costs and beneﬁts of design-
ing protocols that play nicely with trafﬁc optimized for other
479objectives—e.g., a protocol that aims for bulk throughput but
wants to accommodate the needs of cross-trafﬁc that may pri-
oritize low delay (§4.6)?
4. Knowledge of network signals. What kinds of congestion
signals are important for an endpoint to listen to? How much
information can be extracted from different kinds of feed-
back, and how valuable are different congestion signals to-
ward the protocol’s ultimate goals? (§3.4)
Link speed
Minimum RTT
Topology
Senders
Workload
Buffer size
Objective
32 Mbits/sec
150 ms
Dumbbell
2
1 sec ON/OFF times
5 BDP
∑log(t pt)− log(delay)
Table 1: Network parameters for the calibration experiment.
1.1 Tractable attempts at optimal (Tao) protocols
Each of the above areas of inquiry is about the effect of a pro-
tocol designer’s imperfect understanding of the future network that
a decentralized congestion-control protocol will ultimately be run
over.
In principle, we would quantify such an effect by evaluating,
on the actual network,
the performance of the “best possible”
congestion-control protocol designed for the imperfect network
model, and comparing that with the best-possible protocol for the
actual network itself.
In practice, however, we know of no tractable way to cal-
culate the optimal congestion-control protocol for a given net-
work.1 Instead, throughout this study we use the Remy automatic
protocol-design tool [29], a heuristic search procedure that gener-
ates congestion-control protocols, as a proxy for the optimal solu-
tion.
We refer to these automatically generated congestion-control
protocols as “tractable attempts at optimal” (Tao) end-to-end
congestion-control protocols for a given network. Tao protocols
represent a practically realizable tilt at developing an optimal pro-
tocol for an imperfect model of a real network.
Constructing a Tao for a complex network model requires search-
ing a huge space, an intensive task even using Remy’s search opti-
mizations. In most cases, the protocol stopped improving within a
CPU-year of processing time (ﬁve days on our 80-core machine),
though there were a few cases where improvements continued to
occur.
We emphasize that there can be no assurance that the Tao actually
comes close to the optimal congestion-control protocol, except to
the extent that it approaches upper bounds on performance, such as
the ideal fair allocation of network resources. To characterize how
close the Tao protocols come to the bound, we formalize the notion
of a hypothetical “omniscient” protocol. This is a centralized pro-
tocol that knows the topology of the network, the link speeds, the
locations of senders and receivers, and the times at which they turn
on or off. Each time a sender turns on or off, the omniscient proto-
col computes the proportionally fair throughput allocation [17] for
all senders that are on. Each sender then transmits at its propor-
tionally fair throughput allocation, and no ﬂow builds up a standing
queue at any bottleneck. For a particular node, the long-term aver-
age throughput of the omniscient protocol is the expected value of
its throughput allocation, with no queueing delay.
As a calibration experiment to validate the Tao approach, we de-
sign a Tao protocol with parameters shown in Table 1 (where the
buffer size of 5 BDP refers to 5 times the bandwidth-delay product:
the network link speed times the minimum round trip time (RTT)).
We summarize the performance of a protocol using a throughput-
delay graph as shown in Figure 1. For each protocol, we plot the
1The problem can be formulated as a search procedure for an op-
timal policy for a decentralized partially observable Markov deci-
sion process or Dec-POMDP [29]. However, the search for an op-
timal policy under a general Dec-POMDP is known to be NEXP-
complete [4], and no faster search procedure for the particular prob-
lem of network-protocol design has been found.
median throughput and delay (small white circle) and an ellipse rep-
resenting one standard deviation of variation. The Tao protocols are
considerably better than two human-designed protocols: Cubic [12]
(the default end-to-end congestion-control algorithm on Linux) and
Cubic over sfqCoDel (Cubic coupled with sfqCoDel [2], a recent
proposal for queue management and scheduling that runs on the
gateway nodes) on both throughput and delay. Furthermore, they
come within 5% of the omniscient protocol on throughput and 10%
on delay.
The calibration experiment does not prove that the Tao proto-
cols will continue to approach omniscient performance when there
is uncertainty about the network scenario at runtime, or when the
network scenarios are more complex. It also does not demonstrate
the relationship between the Tao protocols and the true “optimal”
solution, which (unlike the omniscient protocol) is one that will
be realizable in an end-to-end algorithm when endpoints have only
partial information about the true network state.
However, the results do give conﬁdence that the Tao protocols
can be used as tractable proxies for an optimal protocol.
In this
study, we compare human-designed protocols to various Tao proto-
cols and the omniscient protocol, generally ﬁnding that Tao proto-
cols can approach the omniscient protocol and can outperform the
existing human-designed protocols that have been designed to date
(Figures 2, 3, 4, and 6).
Figure 1:
proached the performance of the omniscient protocol.
In the calibration experiment,
the Tao protocol ap-
1.2 Summary of results
Here are the principal ﬁndings of this study:
0.5124816320100200300400500Throughput (Mbps)Queueing delay (ms)CubicCubic/sfqCoDelTaoOmniscient480Modeling a two-bottleneck network as a single bottleneck hurt
performance only mildly. On the two-hop network illustrated by
Figure 5, on average we ﬁnd that a protocol designed for a sim-
pliﬁed, single-bottleneck model of the network underperformed by
only 17% (on throughput) a protocol that was designed with full
knowledge of the network’s two-bottleneck structure (Figure 6).
Furthermore, the simpliﬁed protocol also outperformed TCP Cu-
bic by a factor of 7.2× on average throughput and outperformed
Cubic-over-sfqCoDel by 2.75× on average throughput.
Thus, in this example, full knowledge of the network topology
during the design process was not crucial.
Weak evidence of a tradeoff between link-speed operating
range and performance. We created a Tao protocol designed for
a range of networks whose link speeds spanned a thousand-fold
range between 1 Mbps and 1000 Mbps, as well as three other pro-
tocols that were more narrowly-targeted at hundred-fold, ten-fold,
and two-fold ranges of link speed (Figure 2).
The “thousand-fold” Tao achieved close to the peak performance
of the “two-fold” Tao. Between link speeds of 22–44 Mbps, the
“thousand-fold” Tao achieved within 3% of the throughput of
the “two-fold” protocol that was designed for this exact range.
However, the queueing delay of the “thousand-fold” protocol was
46% higher, suggesting some beneﬁt from more focused operat-
ing conditions.
It also takes a lot longer to compute (ofﬂine) a
“thousand-fold” Tao compared to a two-fold Tao; in run-time oper-
ation, though, the computational cost of the two algorithms is simi-
lar.
Over the full range of 1 Mbps to 1000 Mbps, the “thousand-fold”
Tao protocol matched or outperformed TCP Cubic and Cubic-over-
sfqCoDel over the entire range (Figure 2).
The results suggest that highly optimized protocols may be able
to perform adequately over a broad range of actual networks. Broad
operating range had only a weak effect on peak performance, sug-
gesting that “one size ﬁts all” congestion-control protocols that
perform considerably better than TCP—as well as TCP plus in-
network assistance, in the case of sfqCoDel—may be feasible.
Performance at high degrees of multiplexing may be in opposi-
tion with performance when few ﬂows share a bottleneck. We
created ﬁve Tao protocols for a range of networks with varying de-
grees of multiplexing: 1–2 senders, 1–10, 1–20, 1–50, and 1–100
(Figure 4.2 and Table 3a).
We found that a Tao protocol designed to handle between 1 and
100 senders came close to the performance achieved by an om-
niscient protocol over most of that range. However, this came at
the cost of diminished throughput at lower degrees of multiplexing.
Conversely, a protocol trained to handle between 1 and 2 senders
suffered large queuing delays (when the link never dropped pack-
ets) or repeated packet drops (on a link with ﬁnite buffering) when
run on a network with 100 senders.
These results suggest that—unlike with link speed—prior knowl-
edge of the expected degree of multiplexing over bottleneck links
may be beneﬁcial when designing a congestion-control protocol.
TCP-awareness hurt performance when TCP cross-trafﬁc was
absent, but helped dramatically when present. We measured the
costs and beneﬁts of “TCP-awareness”—designing a protocol with
the explicit knowledge that it may be competing against other end-
points running TCP, compared with a “TCP-naive” protocol for a
network where the other endpoints only run the same TCP-naive
protocol.
When contending only with other endpoints of the same kind, the
“TCP-naive” protocol achieved 55% less queueing delay than the
TCP-aware protocol. In other words, “TCP-awareness” has a cost
measured in lost performance when TCP cross-trafﬁc is not present
(Figure 7).
But when contending against TCP, the “TCP-naive” protocol
was squeezed out by the more aggressive cross-trafﬁc presented by
TCP. By contrast, the “TCP-aware” protocol achieved 36% more
throughput and 37% less queueing delay than the “TCP-naive”
protocol, and claimed its fair share of the link capacity from TCP.
Instructions to reproduce the results in this paper, along with the
congestion-control protocols produced by Remy in the process are
available at http://web.mit.edu/remy/learnability.
2. RELATED WORK
This section discusses closely related work on congestion control
and explains how our work has an analogy with theoretical notions
of learnability.
2.1 Congestion control
Congestion control over packet-switched networks has been a
productive area of research since the seminal work of the 1980s,
including Ramakrishnan and Jain’s DECBit scheme [24] and Ja-
cobson’s TCP Tahoe and Reno algorithms [15]. End-to-end algo-