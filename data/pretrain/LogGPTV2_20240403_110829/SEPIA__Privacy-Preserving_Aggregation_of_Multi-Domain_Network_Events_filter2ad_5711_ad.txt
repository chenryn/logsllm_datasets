 90
 80
 70
 60
 50
 40
 30
 20
 10
 0
3 privacy peers
5 privacy peers
7 privacy peers
9 privacy peers
]
s
[
e
m
i
i
t
g
n
n
n
u
r
 90
 80
 70
 60
 50
 40
 30
 20
 10
 0
3 privacy peers
5 privacy peers
7 privacy peers
9 privacy peers
]
s
[
e
m
i
i
t
g
n
n
n
u
r
 90
 80
 70
 60
 50
 40
 30
 20
 10
 0
 5
 10
 15
 20
 25
 5
 10
 15
 20
 25
 5
 10
 15
 20
 25
input peers
input peers
input peers
(a) Addition of port histogram.
(b) Entropy of port distribution.
(c) Distinct AS count.
Figure 7: Network statistics: avg. running time per time window versus n and m, measured on a department-wide
cluster. All tasks were run with an input set size of 65k items.
2. Port Histogram: Adding the full destination port
histogram for incoming UDP ﬂows. SEPIA input
ﬁles contained 65,535 ﬁelds, each indicating the
number of ﬂows observed to the corresponding port.
These local histograms were aggregated using the
addition protocol.
3. Port Entropy: Computing the Tsallis entropy of
destination ports for incoming UDP ﬂows. The lo-
cal SEPIA input ﬁles contained the same informa-
tion as for histogram aggregation. The Tsallis expo-
nent q was set to 2.
4. Distinct count of AS numbers: Aggregating the
count of distinct source AS numbers in incom-
ing UDP trafﬁc. The input ﬁles contained 65,535
columns, each denoting if the corresponding source
AS number was observed. For this setting, we re-
duced the ﬁeld size p to 31 bits because the expected
size of intermediate values is much smaller than for
the other tasks.
Running Time For task 1, the average running time was
below 1.6 s per time window for all conﬁgurations, even
with 25 input and 9 privacy peers. This conﬁrms that
addition-only is very efﬁcient for low volume input data.
Fig. 7 summarizes the running time for tasks 2 to 4. The
plots show on the y-axes the average running time per
time window versus the number of input peers on the x-
axes. In all cases, the running time for processing one
time window was below 1.5 minutes. The running time
clearly scales linearly with n. Assuming a 5-minute in-
terval, we can estimate by extrapolation the maximum
number of supported input peers before the system stops
working in real-time. For the conservative case with 9
privacy peers, the supported number of input peers is ap-
proximately 140 for histogram addition, 110 for entropy
computation, and 75 for distinct count computation. We
observe, that for single round protocols (addition and en-
tropy), the number of privacy peers has only little impact
on the running time. For the distinct count protocol, the
running time increases linearly with both n and m. Note
that the shortest running time for distinct count is even
lower than for histogram addition. This is due to the
reduced ﬁeld size (p with 31 bits instead of 62), which
reduces both CPU and network load.
Bandwidth Requirements For all tasks, the data vol-
ume sent per privacy peer scales perfectly linear with n
and m. Therefore, we only report the maximum volume
with 25 input and 9 privacy peers. For addition of vol-
ume metrics, the data volume is 141 KB and increases to
4.7 MB for histogram addition. Entropy computation re-
quires 8.5 MB and ﬁnally the multi-round distinct count
requires 50.5 MB. For distinct count, to transfer the total
of 2 · 50.5 = 101 MB within 5 minutes, an average band-
width of roughly 2.7 Mbps is needed per privacy peer.
5.3
Internet-wide Experiments
In our evaluation setting hosts have homogeneous
CPUs, network bandwidth and low round trip times
(RTT). In practice, however, SEPIA’s goal is to aggregate
trafﬁc from remote network domains, possibly resulting
in a much more heterogeneous setting. For instance, high
delay and low bandwidth directly affect the waiting time
for messages. Once data has arrived, the CPU model and
clock rate determine how fast the data is processed and
can be distributed for the next round.
Recall from Section 4 that each operation and pro-
tocol in SEPIA is designed in rounds. Communication
and computation during each round run in parallel. But
before the next round can start, the privacy peers have
to synchronize intermediate results and therefore wait
for the slowest privacy peer to ﬁnish. The overall run-
ning time of SEPIA protocols is thus affected by the
slowest CPU, the highest delay, and the lowest band-
width rather than by the average performance of hosts
and links. Therefore we were interested to see whether
the performance of our protocols breaks down if we take
it out of the homogeneous LAN setting. Hence, we ran
LAN
1 ms
PlanetLab A PlanetLab B
320 ms
320 ms
100 Mb/s ≥ 100 Kb/s ≥ 100 Kb/s
2 cores
3.2 GHz
2 cores
2.4 GHz
1 core
1.8 GHz
110.4 s
Max. RTT
Bandwidth
Slowest CPU
Running time
25.0 s
36.8 s
Table 1: Comparison of LAN and PlanetLab settings.
Framework
Technique
Platform
Multipl./s
Equals/s
LessThans/s
SEPIA
FairplayMP
Shamir sh. Shamir sh. Bool. circuits
VIFF
Java
82,730
2,070
86
Python
326
2.4
2.4
Java
1.6
2.3
2.3
Table 2: Comparison of frameworks performance in oper-
ations per second with m = 5.
SEPIA on PlanetLab [31] and repeated task 4 (distinct
AS count) with 10 input and 5 privacy peers on globally
distributed PlanetLab nodes. Table 1 compares the LAN
setup with two PlanetLab setups A and B.
RTT was much higher and average bandwidth much
lower on PlanetLab. The only difference between Plan-
etLab A and B was the choice of some nodes with slower
CPUs. Despite the very heterogeneous and globally dis-
tributed setting, the distinct count protocol performed
well, at least in PlanetLab A. Most important, it still met
our near real-time requirements. From PlanetLab A to B,
running time went up by a factor of 3. However, this can
largely be explained by the slower CPUs. The distinct
count protocol consists of parallel multiplications, which
make efﬁcient use of the CPU and local addition, which
is solely CPU-bound. Let us assume, for simplicity, that
clock rates translate directly into MIPS. Then, computa-
tional power in PlanetLab B is roughly 2.7 times lower
than in PlanetLab A. Of course, the more rounds a pro-
tocol has, the bigger is the impact of RTT. But in each
round, the network delay is only a constant offset and
can be amortized over the number of parallel operations
performed per round. For many operations, CPU and
bandwidth are the real bottlenecks.
While aggregation in a heterogeneous environment
is possible, SEPIA privacy peers should ideally be de-
ployed on dedicated hardware, to reduce background
load, and with similar CPU equipment, so that no single
host slows down the entire process.
5.4 Comparison with General-Purpose
Frameworks
In this section we compare the performance of ba-
sic SEPIA operations to those of general-purpose frame-
works such as FairplayMP [3] and VIFF v0.7.1 [15]. Be-
sides performance, one aspect to consider is, of course,
usability. Whereas the SEPIA library currently only pro-
vides an API to developers, FairplayMP allows to write
protocols in a high-level language called SFDL and VIFF
integrates nicely into the Python language. Furthermore,
VIFF implements asynchronous protocols and provides
additional functionality, such as security against mali-
cious adversaries and support of MPC based on homo-
morphic cryptosystems.
Tests were run on 2x Dual Core AMD Opteron 275
machines with 1Gb/s LAN connections. To guarantee a
fair comparison, we used the same settings for all frame-
works. In particular, the semi-honest model, 5 computa-
tion nodes, and 32 bit secrets were used. Unlike VIFF
and SEPIA, which use an information-theoretically se-
cure scheme, FairplayMP requires the choice of an ade-
quate security parameter k. We set k = 80, as suggested
by the authors in [3].
Table 2 shows the average number of parallel oper-
ations per second for each framework. SEPIA clearly
outperforms VIFF and FairplayMP for all operations and
is thus much better suited when performance of parallel
operations is of main importance. As an example, a run
of event correlation taking 3 minutes with SEPIA would
take roughly 2 days with VIFF. This extends the range
of practically runnable MPC protocols signiﬁcantly. No-
tably, SEPIA’s equal operation is 24 times faster than
its lessT han, which requires 24 times more multipli-
cations, but at the same time also twice the number of
rounds. This conﬁrms that with many parallel opera-
tions, the number of multiplications becomes the dom-
inating factor. Approximately 3/4 of the time spent
for lessT han is used for generating sharings of random
numbers used in the protocol. These random sharings
are independent from input data and could be generated
prior to the actual computation, allowing to perform 380
lessT hans per second in the same setting.
Even for multiplications, SEPIA is faster than VIFF,
although both rely on the same scheme. We assume this
can largely be attributed to the completely asynchronous
protocols implemented in VIFF. Whereas asynchronous
protocols are very efﬁcient for dealing with malicious
adversaries, they make it impossible to reduce network
overhead by exchanging intermediate results of all paral-
lel operations at once in a single big message. Also, there
seems to be a bottleneck in parallelizing large numbers
of operations. In fact, when benchmarking VIFF, we no-
ticed that after some point, adding more parallel opera-
tions signiﬁcantly slowed down the average running time
per operation.
Sharemind [6] is another interesting MPC framework
using additive secret sharing to implement multiplica-
tions and greater-or-equal (GTE) comparison. The au-
thors implement it in C++ to maximize performance.
However, the use of additive secret sharing makes the im-
plementations of basic operations dependent on the num-
ber of computation nodes used. For this reason, Share-
mind is currently restricted to 3 computation nodes only.
Regarding performance, however, Sharemind is compa-
rable to SEPIA. According to [6], Sharemind performs
up to 160,000 multiplications and around 330 GTE op-
erations per second, with 3 computation nodes. With
3 PPs, SEPIA performs around 145,000 multiplications
and 145 lessT hans per second (615 with pre-generated
randomness). Sharemind does not directly implement
equal, but it could be implemented using 2 invocations
of GTE, leading to ≈ 115 operations/s. SEPIA’s equal
is clearly faster with up to 3, 400 invocations/s. SEPIA
demonstrates that operations based on Shamir shares are
not necessarily slower than operations in the additive
sharing scheme. The key to performance is rather an im-
plementation, which is optimized for a large number of
parallel operations. Thus, SEPIA combines speed with
the ﬂexibility of Shamir shares, which support any num-
ber of computation nodes and are to a certain degree ro-
bust against node failures.
6 Design and Implementation
The foundation of the SEPIA library is an implemen-
tation of the basic operations, such as multiplications
and optimized comparisons (see Section 3), along with
a communication layer for establishing SSL connections
between input and privacy peers.
In order to limit the
impact of varying communication latencies and response
times, each connection, along with the corresponding
computation and communication tasks, is handled by a
separate thread. This also implies that SEPIA proto-
cols beneﬁt from multi-core systems for computation-
intensive tasks. In order to reduce synchronization over-
head, intermediate results of parallel operations sent to
the same destination are collected and transfered in a sin-
gle big message instead of many small messages. On top
of the basic layers, the protocols from Section 4 are im-
plemented as standalone command-line (CLI) tools. The
CLI tools expect a local conﬁguration ﬁle containing pri-
vacy peer addresses, paths to a folder with input data and
a Java keystore, as well as protocol-dependent parame-