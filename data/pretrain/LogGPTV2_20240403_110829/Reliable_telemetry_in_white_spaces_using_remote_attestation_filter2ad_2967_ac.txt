3.5
3.5
4.7
σ = 4
σ = 6
σ = 8
1.2
1.6
1.8
2.5
2.4
3.3
1.4
1.9
2.1
2.8
2.8
3.8
1.1
1.5
1.6
2.3
2.2
2.6
facilitates detecting them using SVM classiﬁers (Phase 2 of our ap-
proach), and is therefore desirable. Hence, we will rely on median
when the attested nodes represent the majority of nodes in the cell
and rely on the mean otherwise. This strategy helps with reducing
the effect of attackers and natural outliers when attackers do not
constitute a majority, and makes them more likely to be detected
when they do.
We further elaborate on the details of aggregate calculation in
Phase 1 with an illustrative example. We start by considering the
data from all of the attested nodes in the aggregation pool and ini-
tially use median as the aggregator. If the margin of error for the
median of attested nodes is below the application requirement γ,
we stop by declaring the median as the ﬁnal result. Otherwise, we
need more data. Consider a cell with k attested nodes. After adding
the k attested nodes, we iteratively add up to k − 1 additional ele-
ments from regular nodes to the aggregation pool.
Figure 2: A simpliﬁed illustration of why attackers are forced to
deviate more when they aim to move the mean (bottom picture)
instead of the median (top picture).
The order in which the regular nodes are added to the pool is de-
termined by the chosen inclusion strategy (Random, Geo-Diverse,
or Biased). After each addition, if the margin of error for the me-
dian is reduced to a value lower than γ, we transition to Phase 2. If
this condition is not met at any point and there exist additional mea-
surements, we switch to using mean as the aggregator. Again, we
continue adding new data from the regular nodes to the aggregation
pool (using the same inclusion strategy) until the stopping rule is
satisﬁed. If so, we transition to Phase 2. Otherwise, if adding all of
the regular nodes does not result in satisfying the stopping rule, we
ignore all the added regular nodes and proceed to Phase 3 where
the median of attested nodes is computed as the aggregate.
3.5
Inter-Cell Attacker Detection using Clas-
siﬁers
-100 -120 -80 -100 -120 -80 Average of attested nodes   : -122 dBm Average of attackers     : -105.5 dBm Average of all: -113 dBm   Average of attested nodes   : -122 dBm Average of attackers    : -113 dBm Median of all:  -113 dBm   Signal Power (dBm) Signal Power (dBm) When the execution of Algorithm 0 reaches Phase 2, we have ob-
tained an aggregate from data provided by all of the attested nodes,
as well as some or all of the regular nodes in the cell. In this phase,
we aim to ensure that the regular nodes whose data is included in
the calculation are not part of an exploitation or vandalism attack.
We ﬁrst separate the data points from those regular nodes that have
contributed to the aggregate (a.k.a. ‘yellow suspects’) and compare
them to the data from attested nodes in the neighboring cells (a.k.a.
‘green neighbors’).
Figure 3: Classiﬁcation-based attacker detection setting: reg-
ular nodes included in the aggregation for cell E and attested
nodes from neighboring cells.
To determine if the yellow suspects in a cell represent an attacker-
dominated group, we use real signal propagation data in the region
to build a classiﬁer that is trained to differentiate between natural
and un-natural signal propagation patterns. The idea is to learn the
normal propagation patterns of the signal from the reliable signal
propagation data and use it to spot unnatural propagation of signal,
which may be caused by malicious false reports.
More speciﬁcally, we consider the local neighborhood NE of
any cell E to contain E and its 8 neighboring cells (Figure 3).
We represent NE by a 9-element tuple containing the ‘average’
reported powers from the yellow suspects in E and the average re-
ported power from the green neighbors in a pre-speciﬁed order. We
call this the neighborhood representation of E. For example, for
Figure 3, the ﬁrst element would be the average of yellow suspects
in cell E, and the second to ninth elements would be the value in
the ﬁrst element minus the average power of attested nodes in cells
A to I (excluding E). Assume for a moment that we have access to
reliable power measurements for a subset of the region of interest.
This data can be used to create one neighborhood representation
for each cell in the area. We refer to each such representation as an
‘example.’ Therefore, we can assume access to a large number of
such examples representing the ‘natural’ propagation of signal in
local neighborhoods. Also, as we will elaborate later, assume we
have access to the neighborhood representation for a sufﬁciently
large and diverse set of ‘un-natural’ (attacker-dominated) cells.
We now cast our problem to a binary classiﬁcation problem.
Classiﬁcation is a machine learning technique that is widely used
in domains ranging from spam email detection and unauthorized
spectrum usage to fraud detection and speech recognition. In a bi-
nary classiﬁcation problem we are given a set of training examples
with their corresponding labels, (−→xi , yi), where −→xi is the represen-
tation of the ith example and yi ∈ {1,−1} (‘yes’ or ‘no’) is the
corresponding binary label. Each example is described by a vec-
tor of its attributes which is often called the feature vector. In our
case, the neighborhood representation of a cell serves as its fea-
ture vector. The goal is to predict a binary label for a test exam-
ple for which we do not know the label, using the classiﬁer built
from training examples [12]. A classiﬁer tries to partition the input
feature space into regions where positive examples lie versus re-
gions where negative examples lie. The boundary between regions
for positive and negative examples is called the decision boundary.
Training involves learning the decision boundary and classiﬁcation
involves determining on which side of the decision boundary a test
example lies.
Now we turn to the problem of obtaining training examples. We
argue that normal (negative) instances can be obtained in a practi-
cal one-time process based on a trusted sensor grid. By one-time
we mean that in a particular region, we only need to collect signal
propagation data once to build the classiﬁer for that region. Once
the classiﬁer is built, it can be used forever (or until there is a sig-
niﬁcant environmental change in the region). A typical strategy for
collecting this data is war-driving where a sensor is moved though
the region collecting training data as it goes. Having obtained such
natural (normal) examples, we modify them to inject un-natural
training instances to represent attacker-dominated cells.
Building such a classiﬁer from the natural and un-natural exam-
ples has been discussed in detail in prior works [19] and has been
shown to effectively detect attacker-dominated regions in environ-
ments where there is no separation between regular and attestation-
capable nodes. By contrast, in our setting, the classiﬁer is applied
in a slightly different manner where only the trusted data from the
neighbors is used in classiﬁcation. However, due to potentially low
penetration of attested nodes, this translates to less data points be-
ing available for classiﬁcation. This may negatively affect the clas-
siﬁcation accuracy. We build a similar classiﬁer (using Support
Vector Machines (SVM) with quadratic kernels) to detect whether
the yellow suspects in a cell look abnormal compared to the green
neighbors and evaluate it in Section 4. If the classiﬁer considers the
data to be anomalous, we only rely on the median of the attested
nodes in that cell. Otherwise, the aggregate computed in Phase 1
(using a mix of attested and regular nodes) is valid and should be
used as the representative signal power in that cell.
4. EVALUATION
We evaluate our system using predicted signal propagation data
obtained from real transmitters and terrain data. More speciﬁcally,
the TV transmitter location, signal power, height, and frequency is
obtained from FCC databases and terrain (i.e. elevation for any
given point) is obtained from NASA databases [5]. We choose
the FCC-endorsed Longley-Rice empirical outdoor signal propa-
gation model to generate predicted signal power for any location
and frequency of interest. Longley-Rice takes into account the ef-
fects of terrain as well as transmitter’s power, location, frequency,
and height. To account for additional uncertainties due to factors
such as shadow-fading we add log-normal variations with a mean
of zero and a standard deviation (dB-spread) of σdB = 6 to the pre-
dicted signal power for each point [37]. For evaluation purposes,
we consider this data as the ground truth.
We instantiated our evaluation to an urban/suburban area sur-
rounding Pittsburgh, Pennsylvania. The hilly nature of the area
introduces a large amount of legitimate signal variations, which
makes the task of precise signal power estimation and attacker de-
tection more challenging (compared to ﬂat areas). Therefore, these
experiments should be considered a stress-test for our scheme.
The following points in (latitude, longitude) format deﬁne the
southwest and northeast corners of the considered 20km × 20km
square area in Pennsylvania: (cid:104) (40.35, -80.12), (40.53, -79.884)(cid:105).
Each cell is 1km × 1km. We focus on signals from all DTV trans-
mitters within a 150 mile radius of this area with estimated received
powers higher than -130dBm. This results in a list of 37 DTV
A B C G H I F Attested Node Yellow Suspects E D E: Only yellow  suspects shown A, B, C, D, F, G, H, I: Only attested  nodes shown Figure 4: (a) Transmitters in parts of Southwest Pennsylvania / East Ohio. (b) Distribution of received signal for the training and
testing data in Southwest Pennsylvania.
Figure 5: No attack; percentage of cells with ground truth average within the margin of error from the calculated aggregate (left)
and false outcome rate (in percentage) as a function of the fraction of attested nodes (right).
transmitters, of which we randomly pick 29 for building the clas-
siﬁer, and 8 for testing it. An illustration of the area, including the
location of the majority of DTV transmitters in provided in Fig-
ure 4(a). The distribution of the received signal power across all
the cells in the region (from all 37 transmitters) is provided in Fig-
ure 4(b). Guided by approximate sample size requirements based
on methods in Section 3.2, we consider nodes to be scattered with
an expected density Ed of 50 nodes per cell. To add variation and
randomness, we consider the number of nodes to be normally dis-
tributed with a mean of Ed, and a standard deviation of 10. Such
densities will be easily achievable in urban areas. In suburban and
rural areas, the densities need to be achieved through provisioning
or other means in order for our approach to be fully effective.
4.1 No-Attack Performance
We ﬁrst evaluate the accuracy of predictions generated by our
approach when there is no attack. We compare the aggregate pro-
duced by our approach to the ground truth (real average power in
the cell). In Figure 5(a) we show the percentage of cells for which
the real average power is within the chosen margin of error  = 3dB
from the calculated aggregate. The results show that our approach
achieves a high overall success rate in terms of obtaining precise
estimates of signal power in a region. They also show that de-
spite Biased’s weaker performance in some cases, in most cases
the choice of inclusion strategy does not have a signiﬁcant impact.
As a second performance metric in the absence of attacks, we
introduce the false outcome rate, representing the fraction of un-
attacked cells with ground truth power above (below) the primary
detection threshold of -114dBm that due to errors in our approach
are mistakenly assigned an aggregate below (above) -114dBm. Fig-
ure 5(b) represents the false outcome rate as a function of the frac-
tion of attested nodes. The results show that while overall false
outcome rates are low, the Biased inclusion strategy is the weakest
performer, particularly when the fraction of attested nodes is low.
This can be explained by situations in which the few attested nodes
are not providing values near the true average power in the cell, and
the Biased inclusion strategy aggravates the situation by including
similar data that effectively builds up on the already poor samples.
4.2 Performance against Attackers
To gauge performance in the presence of attacks, we simulate
omniscient (and coordinated) attackers that perform exploitation
and vandalism attacks. Attacker nodes act in cooperation and know
the exact number, measurements, and type of all the other nodes,
as well as the inclusion strategy in use (Random, Geo-diverse, or
Biased).
In cells where the ground truth is below the -114dBm
threshold, they cooperate to perform exploitation to change the ag-
gregate to a value above the threshold. Similarly, in cells where the
ground truth is above -114dBm, they aim for vandalism by mov-
ing the aggregate to a value below the threshold. In both cases, the
attackers minimize the deviation of their false reports from the mea-
surements of un-compromised nodes by choosing to report values
that move the aggregate slightly below (above) the threshold (.5 dB
here) in order to perform exploitation (vandalism). This maximizes
their chances of being included in the aggregate pool in Phase 1
and minimizes their chances of being detected in Phase 2. If the at-
tackers conclude that the protections in Phase 1 do not allow them
to ‘ﬂip’ the aggregate, they refrain from reporting false reports to
avoid detection.
To evaluate effectiveness against omniscient coordinated attacks,
we introduce the deterrence rate. This metric represents the frac-
tion of attacks by omniscient attackers that our approach thwarts.
Deterrence may occur in phase 1 (by partial or total exclusion from
the pool), or in phase 2 where their attack is detected by the clas-
siﬁer. We use data from 29 of the transmitters to build a uniﬁed
classiﬁer for the region [19] and test deterrence of attacks on the
9192939495969798991000.150.250.350.45Average in Margin of Error (%) Attested Node Fraction RandomGeo-DiverseBiased00.511.522.530.150.250.350.45False Outcome Rate (%) Attested Node Fraction RandomGeo-DiverseBiasedFigure 6: Attack deterrence rate (in percentage) when the average fraction of attested nodes is .15 (left), .25 (center), and .35 (right).
remaining 8 channels. The deterrence rates for cases with aver-
age attested fractions ranging from .15 to .35, and average attacker
fraction ranging from .25 to .85 are presented in Figure 6. For at-
tested fractions higher than .35, our results (omitted due to space
constraints) show that it is more beneﬁcial to avoid the complexities
of our approach and only rely on the average of attested nodes.
In Figure 6, a surprising phenomenon can be seen in the case
of Biased attacks. In some cases, when the attested fraction is in-
creased (particularly from .25 to .35), the deterrence rate decreases.
While this can be considered a ﬂaw for the biased scheme, it can
be described as follows. When the attested fraction is increased,
there is less competition from regular un-compromised nodes (for
attacker nodes) to report values close to the average of attested
nodes and enter the aggregation pool. Therefore, the attackers have
a higher chance of entering the pool with false reports, inﬂuencing
the results, and passing Phase 1. The results in Figure 7 show this
observation; unlike Random and Geo-diverse cases in which the
deterrence at phase 1 does not change or increases as the attested
fraction increases, the rate decreases for the Biased strategy.
Overall, the results show the following. (1) All three approaches
are highly effective against omniscient attacks, even in cases where
a small fraction of nodes are attested. (2) In terms of attack de-
terrence, the Biased inclusion strategy outperforms others. This
is particularly true with lower attested and attacker fraction. This
can be attributed to the difﬁculty of inﬂuencing the aggregate by
attackers in these situations, since the attacker has to fulﬁl two con-
ﬂicting goals of reporting values close to the attested average (to be
included in pool) and at the same time far from the attested average
(to move the aggregate and perform attack). (3) The relative out-
performance of the Biased approach comes at the price of relatively
higher false outcome rates when there is no attack.
5. ATTESTATION COSTS
Remote attestation can introduce potentially signiﬁcant additional
costs into a system. This section brieﬂy surveys these costs for
implementations of two remote attestation architectures. The ﬁrst
uses a TPM, which is a distinct coprocessor, whereas the second
is implemented primarily in software, requiring only small hard-