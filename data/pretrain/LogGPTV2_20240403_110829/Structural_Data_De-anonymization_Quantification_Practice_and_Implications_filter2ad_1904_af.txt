Enron
EuAll
Wiki
AS733
Oregon
Caida
Skitter
Gowalla
HepPh
AstroPh
CondMat
Gnutella3
Gnutella4
Gnutella5
distinguish σk (k ≤ ϵn) and σk′ (k
′
tent with our quantiﬁcation.
> ϵn), which is consis-
(iii) As in the perfect DA scenario, graph density is an
important factor to impact Ω(n). Datasets with similar
graph density, e.g., Google+ and Skitter, exhibits similar
requirement on Ω(n). A dataset with high graph density,
e.g., Facebook and HepPh, corresponds to a loose bound on
Ω(n). The reason is also the same as before.
Table 5: Evaluation of (Ω(℘), Ω(fD), Ω(n)) in (1 − ϵ)-perfect DA.
Pokec
ϵ = .1
ϵ = .4
ϵ = .5
ϵ = .3
ϵ = .2
Orkut
Slashdot
Facebook
YouTube
Dataset
Google+ (1.1E-7, 3.5E-6, 2.2E28) (1.2E-7, 3.7E-6, 1.8E28) (1.3E-7, 3.9E-6, 1.6E28) (1.4E-7, 4.2E-6, 1.3E28) (1.4E-7, 4.4E-6, 1.1E28)
(1.2E-6, 3.1E-5, 1.2E24) (1.2E-6, 3.2E-5, 9.7E23) (1.3E-6, 3.4E-5, 8.3E23) (1.4E-6, 3.6E-5, 6.9E23) (1.5E-6, 3.8E-5, 5.8E23)
Twitter
LiveJournal (1.2E-7, 3.6E-6, 4.7E28) (1.2E-7, 3.6E-6, 4.7E28) (1.2E-7, 3.8E-6, 3.8E28) (1.3E-7, 4.1E-6, 3.0E28) (1.4E-7, 4.3E-6, 2.7E28)
(1.3E-4, 2.2E-3, 5.9E15) (1.4E-4, 2.3E-3, 5.0E15) (1.5E-4, 2.5E-3, 4.1E15) (1.6E-4, 2.6E-3, 3.5E15) (1.7E-4, 2.8E-3, 2.9E15)
(6.0E-7, 1.7E-5, 2.4E26) (6.0E-7, 1.7E-5, 2.4E26) (6.0E-7, 1.7E-5, 2.4E26) (6.0E-7, 1.7E-5, 2.4E26) (6.0E-7, 1.7E-5, 2.4E26)
(1.7E-7, 5.1E-6, 2.0E27) (1.8E-7, 5.4E-6, 1.7E27) (1.9E-7, 5.7E-6, 1.4E27) (2.0E-7, 6.1E-6, 1.2E27) (2.2E-7, 6.5E-6, 9.8E26)
(7.4E-6, 1.7E-4, 2.8E21) (7.4E-6, 1.7E-4, 2.8E21) (7.4E-6, 1.7E-4, 2.8E21) (8.2E-6, 1.9E-4, 2.1E21) (8.2E-6, 1.9E-4, 2.1E21)
(3.2E-7, 9.2E-6, 4.4E26) (3.5E-7, 9.9E-6, 3.6E26) (3.6E-7, 1.0E-5, 3.1E26) (3.9E-7, 1.1E-5, 2.5E26) (4.1E-7, 1.2E-5, 2.1E26)
Infocom (8.6E-3, 8.0E-2, 2.0E09) (8.6E-3, 8.0E-2, 2.0E09) (9.1E-3, 8.1E-2, 1.7E09) (1.0E-2, 8.6E-2, 1.2E09) (1.1E-2, 8.9E-2, 1.1E09)
(4.7E-3, 5.2E-2, 1.9E10) (5.1E-3, 5.1E-2, 1.5E10) (5.3E-3, 5.2E-2, 1.3E10) (5.8E-3, 5.6E-2, 1.0E10) (6.4E-3, 6.1E-2, 7.2E09)
Smallblue
(1.1E-5, 2.3E-4, 1.2E21) (1.1E-5, 2.3E-4, 1.2E21) (1.1E-5, 2.3E-4, 1.2E21) (1.2E-5, 2.6E-4, 8.7E20) (1.2E-5, 2.6E-4, 8.7E20)
Brightkite
(2.9E-6, 7.1E-5, 1.8E23) (2.9E-6, 7.1E-5, 1.8E23) (3.2E-6, 7.8E-5, 1.3E23) (3.2E-6, 7.8E-5, 1.3E23) (3.4E-6, 8.3E-5, 1.1E23)
(4.7E-5, 8.8E-4, 8.5E17) (5.1E-5, 9.5E-4, 6.7E17) (5.5E-5, 1.0E-3, 5.3E17) (5.7E-5, 1.1E-3, 4.6E17) (6.2E-5, 1.2E-3, 3.7E17)
(3.0E-5, 5.9E-4, 5.2E18) (3.1E-5, 6.1E-4, 4.6E18) (3.4E-5, 6.6E-4, 3.7E18) (3.5E-5, 6.9E-4, 3.1E18) (3.7E-5, 7.3E-4, 2.6E18)
(2.6E-5, 5.2E-4, 2.5E19) (2.6E-5, 5.2E-4, 2.5E19) (2.8E-5, 5.6E-4, 2.0E19) (3.0E-5, 6.0E-4, 1.7E19) (3.2E-5, 6.3E-4, 1.4E19)
(1.7E-6, 4.3E-5, 2.2E24) (1.9E-6, 4.8E-5, 1.6E24) (1.9E-6, 4.8E-5, 1.6E24) (2.1E-6, 5.3E-5, 1.2E24) (2.2E-6, 5.7E-5, 9.5E23)
(1.7E-5, 3.6E-4, 1.1E20) (1.7E-5, 3.6E-4, 1.1E20) (1.8E-5, 3.8E-4, 9.3E19) (2.0E-5, 4.2E-4, 7.1E19) (2.0E-5, 4.2E-4, 7.1E19)
(3.8E-6, 9.4E-5, 2.9E23) (3.8E-6, 9.4E-5, 2.9E23) (3.8E-6, 9.4E-5, 2.9E23) (3.8E-6, 9.4E-5, 2.9E23) (3.8E-6, 9.4E-5, 2.9E23)
(3.3E-7, 9.7E-6, 4.3E27) (3.3E-7, 9.7E-6, 4.3E27) (3.3E-7, 9.7E-6, 4.3E27) (3.3E-7, 9.7E-6, 4.3E27) (3.3E-7, 9.7E-6, 4.3E27)
(9.4E-5, 1.7E-3, 2.9E17) (9.4E-5, 1.7E-3, 2.9E17) (9.4E-5, 1.7E-3, 2.9E17) (1.1E-4, 2.0E-3, 1.6E17) (1.1E-4, 2.0E-3, 1.6E17)
(5.1E-5, 9.5E-4, 2.6E18) (5.1E-5, 9.5E-4, 2.6E18) (6.7E-5, 1.3E-3, 1.1E18) (6.7E-5, 1.3E-3, 1.1E18) (6.7E-5, 1.3E-3, 1.1E18)
(2.3E-5, 4.7E-4, 9.6E19) (2.3E-5, 4.7E-4, 9.6E19) (2.3E-5, 4.7E-4, 9.6E19) (3.1E-5, 6.3E-4, 4.1E19) (3.1E-5, 6.3E-4, 4.1E19)
(3.2E-7, 9.0E-6, 1.0E27) (3.4E-7, 9.8E-6, 8.0E26) (3.7E-7, 1.1E-5, 6.5E26) (3.9E-7, 1.1E-5, 5.4E26) (4.1E-7, 1.2E-5, 4.7E26)
(2.4E-5, 4.8E-4, 7.3E19) (2.4E-5, 4.8E-4, 7.3E19) (2.4E-5, 4.8E-4, 7.3E19) (2.4E-5, 4.8E-4, 7.3E19) (2.6E-5, 5.3E-4, 5.4E19)
(1.8E-5, 3.7E-4, 2.6E20) (1.8E-5, 3.7E-4, 2.6E20) (1.8E-5, 3.7E-4, 2.6E20) (1.8E-5, 3.7E-4, 2.6E20) (1.9E-5, 4.1E-4, 2.0E20)
(1.0E-5, 2.3E-4, 2.3E21) (1.0E-5, 2.3E-4, 2.3E21) (1.0E-5, 2.3E-4, 2.3E21) (1.0E-5, 2.3E-4, 2.3E21) (1.2E-5, 2.5E-4, 1.7E21)
DBLP
Enron
EuAll
Wiki
AS733
Oregon
Caida
Skitter
Gowalla
HepPh
AstroPh
CondMat
Gnutella3
Gnutella4
Gnutella5
Finally, we also want to evaluate the required bounds on
(Ω(℘), Ω(fD), Ω(n)) in (1 − ϵ)-perfect DA. We demonstrate
the results in Tab. 5 and make the following observations.
(i) Theoretically, the condition on the lower bound of ℘ is
very loose, e.g., when ϵ = .1, Ω(℘) = 1.1E-7 for Google+ and
Ω(℘) = 1.7E-7 for Orkut, which suggests that (1− ϵ)-perfect
DA is implementable in practice. On the other hand, we can
also see that the theoretical loose requirement on Ω(℘) is at
the expense of a strong condition on Ω(n), e.g., when ϵ = .1,
Ω(n) = 2.2E28 for Google+ and Ω(n) = 2.0E27 for Orkut.
Consequently, to de-anonymize most of existing structural
datasets which have sizes of million-level or less, a higher ℘
is desired (as we show in Tab. 2, 3, and 4).
(ii) From Tab. 5, we can see that the conditions on Ω(fD)
and Ω(n) exhibit the same behavior as in perfect DA, i.e.,
Ω(fD) increases and Ω(n) decreases as Ω(℘) increases, which
is consistent with our quantiﬁcation. Again, this is because
fD is an increasing function of ℘ given pD and Ω(n) decreas-
es when more similarity appears between Ga and Gu.
(iii) From Tab. 5, we can also see that the impact of
graph density on Ω(fD) and Ω(n) is also similar to that in
the perfect DA scenario.
6. OPTIMIZATION BASED DA PRACTICE
In Section 4, we comprehensively quantiﬁed conditions for
perfect DA and (1− ϵ)-perfect DA. Based on our large-scale
study on 26 real world datasets in Section 5, we ﬁnd most, if
not all, existing structural datasets are de-anonymizable par-
tially or completely (Tab. 3). Interestingly, our DA quan-
tiﬁcation leads to a DA scheme, denoted by A
, straight-
∗
forwardly. Basically, A
can be implemented as follows: we
can calculate the DE caused by each σk (1 ≤ k ≤ n!) and
let σ0 be the σk that induces the least DE. According to the
∗
quantiﬁcation, the σ0 produced by A
should be the opti-
∗
is computationally infeasible
mum DA scheme. However, A
in practice due to its high computational complexity O(n!).
In this section, we present a novel relaxed and operational
∗
followed by analyzing its performance theoret-
∗
version of A
ically and experimentally on large scale real datasets.
6.1 Optimization based DA
∗
i
i
i
i
1, di
|+1 = di|N u
j ||j ∈ N u
i }), i.e., di
|+1 = di|N a
|+2 = ··· = di
i | < β), we set di|N a
i ||i ∈ V a} (resp., ∆u = max{|N u
version of A
i ∈ V a or V u as follows.
its degree in Ga (resp., Gu), i.e., fd(i) = |N a
Before proposing our relaxed and computationally feasible
, we deﬁne some useful structural features for
Degree: For i ∈ V a (resp., V u), its degree feature fd(i) is
i | (resp., |N u
i |).
Neighborhood: For i ∈ V a (resp., V u), its neighborhood
2,··· , di
feature fn(i) is a β-dimensional vector (di
β), where
j ||j ∈ N a
k (1 ≤ k ≤ β) is the k-th largest degree in {|N a
i }
di
(resp., {|N u
k is the k-th largest degree of
the neighboring users of i. In the case that |N a
i | < β (re-
sp., |N u
|+2 = ··· = di
β = ∆a
β = ∆u), where ∆a =
(resp., di|N u
i ||i ∈ V u}) is the
max{|N a
maximum degree of Ga (resp., Gu).
Top-K reference distance: For i ∈ V a (resp., V u), its Top-
K reference distance feature fK (i) is a K-dimensional vector
k (1 ≤ k ≤ K) is the distance (the
(hi
length of a shortest path) from i to the user with the k-th
largest degree in Ga (resp., Gu). Note that it is possible
k = ∞ if the graph is not connected.
hi
L = {v1, v2,··· , vL
Landmark reference distance: Suppose V a
|vk ∈ V a} is a set of users that has been de-anonymized (ev-
L = {u1, u2,··· , uL|uk ∈ V u}
idently, V a
under some DA scheme σ with σ(vk) = uk (1 ≤ k ≤ L).
Intuitively, V a
L can be used as auxiliary information
for future DA. Therefore, for i ∈ V a \ V a
L (resp., V u \ U u
L),
we deﬁne its landmark reference distance feature fl(i) =
2,··· , hi
k (1 ≤ k ≤ L) is the distance from
1, hi
(hi
i to vk ∈ V a
L (resp., uk ∈ U u
L).
Sampling closeness centrality: For i ∈ V a (resp., V u), we
deﬁne the sampling closeness centrality feature fc(i) to char-
acterize its global topological property without inducing too
much computational overhead. Formally, we ﬁrst randomly
L = ∅ initially) to U u
2,··· , hi
K ), where hi
L), where hi
L and U u
1, hi
∑
∑
sample a subset Sa of V a (resp., Su of V u). Then, we deﬁne
fc(i) =
h(i,j) ), where
h(i,j) (resp., fc(i) =
1
1
j∈Su\{i}
j∈Sa\{i}
h(i, j) is the distance from i to j.
According to the aforementioned deﬁnitions, (i) we con-
sider both local and global structural features of a user, e.g.,
the degree and neighborhood features characterize the local
topological properties of a user while the Top-K reference
distance and sampling closeness centrality features demon-
strate the global topological characteristics of a user; (ii) we
also consider the computational eﬃciency of obtaining these
features for a user. For instance, instead of using the accu-
rate closeness centrality, we introduce a sampling closeness
centrality feature, which can characterize the global feature
of a user without causing too much computation overhead.
Now, based on the features deﬁned for each user, we can
quantitatively measure the similarity between an anonymized
user i ∈ V a and a known user j ∈ V u. Let fd,c(i) =
(fd(i), fc(i)). Then, we deﬁne the structural similarity be-
tween i ∈ V a and j ∈ V u as ϕ(i, j) = c1 · s(fd,c(i), fd,c(j)) +
c2 · s(fn(i), fn(j)) + c3 · s(fK (i), fK (j)) + c4 · s(fl(i), fl(j)),
where c1,2,3,4 ∈ [0, 1] are constant values representing the
weights and c1 + c2 + c3 + c4 = 1, and s(·,·) is the Cosine
similarity between two vectors.
According to our theoretical quantiﬁcation in Section 4,
∗
is inherently an optimization based algorithm with the
A
objective of minimizing the DE Ψσk , which is diﬀerent from
most of existing DA algorithms (heuristics based) [1][2][3].