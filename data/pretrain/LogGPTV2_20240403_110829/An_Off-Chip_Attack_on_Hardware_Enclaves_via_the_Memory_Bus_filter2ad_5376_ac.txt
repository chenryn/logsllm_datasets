24
// add a word to the hash table
int HashMgr::add_word(const std::string& word) {
struct hentry* hp = (void*) malloc(sizeof(struct
hentry) + word->size());
struct hentry* dp = tableptr[i]; // Populate hp
while (dp->next != NULL) {
if (strcmp(hp->word, dp->word) == 0) {
free(hp); return 0;
}
dp = dp->next;
}
dp->next = hp;
return 0;
}
// lookup a word in the hash table
struct hentry* HashMgr::lookup(const char* word) {
struct hentry* dp;
if (tableptr) {
dp = tableptr[hash(word)];
for (; dp != NULL; dp = dp->next) {
if (strcmp(word, dp->word) == 0) return dp;
}
}
return NULL;
}
Figure 4: The Hunspell code which leaks access patterns with
controlled-channel attacks and MEMBUSTER.
Figure 5: Observerable address patterns in Hunspell by dif-
ferent attacks. Controlled-channel attacks only see page-fault
addresses without the lower 12 bits, whereas MEMBUSTER
can see LLC-miss addresses without the lower 6 bits.
created from the dictionary. A simpliﬁed version of the vulner-
able code is shown in Figure 4. The Hunspell execution starts
with reading the dictionary ﬁle and inserting the words into
the hash table by calling HashMap::add_word(). For each
word from the dictionary, HashMap::add_word() allocates
a hentry node and inserts it to the end of the linked list in the
corresponding hash bucket. Then, Hunspell reads the words
for spell-checking and calls HashMap::lookup() to search
the words in the hash table. Both HashMap::add_word() and
HashMap::lookup() leak the hash bucket of the word cur-
rently being inserted or searched, and all the hentry nodes
before the word is found in the linked list.
The controlled-channel attack leaks different access pat-
terns from those that we observe on our memory bus attack,
as the example shown in Figure 5. Controlled-channel attacks
BG[0]BG[1]BA[1]ROW[15:0]COL[9:3]1514137616171819BA[0]32……PAtableptr[0]tableptr[1]bookkeeping6a60f0congestion6f68f0...cask6c8cc01.Unmasked addresses:tableptr[0-511]tableptr[0-511]bookkeeping6a6000congestion6f6000...cask6c80002.Page fault addresses (controlled-channel attacks):tableptr[0-7]tableptr[0-7]bookkeeping6a60c0congestion6f68c0...cask6c8cc03.Cache miss addresses (MEMBUSTER):leak access patterns through page fault addresses, which are
masked by SGX in the lower 12 bits. However, for applica-
tions like Hunspell, controlled-channel attacks can use se-
quences of page fault addresses to infer more ﬁne-grained ac-
cess patterns within a page. For example, although the nodes
for bookkeeping and booklet are on the same page, the
controlled-channel attacks can differentiate the accesses by
the page addresses accessed before reading the nodes.
On the other hand, our memory bus channel can leak the
addresses of each cache line being read from and written
back to DRAMs, making the attacks more ﬁne-grained than
controlled-channel attacks. The attacks can differentiate the
access patterns based on the addresses of each node accessed
during lookups, instead of inferring through the address se-
quences. The granularity of memory bus attacks makes it
possible to extract sensitive information even if the access
patterns are partially lost due to caching.
4.2 Memcached
Memcached [59] is an in-memory key-value database, which
is generally used to speed up various server applications by
caching the database. Memcached is used in various services
such as Facebook [60] and YouTube [61]. In this example, we
assume that Memcached runs in an SGX enclave, as part of a
larger secure system (e.g., secure mail server).
We consider the scenario discussed by Zhang et al. [62],
where a mail server indexes the keywords in each of the emails
and the attacker can inject an arbitrary email to the victim’s
inbox by simply sending an email to the victim. As shown
in Figure 6, we assume that the index data is stored in Mem-
cached running in an SGX enclave. Since the attacker owns
the machine, she can also perform MEMBUSTER by observ-
ing the memory bus. The attacker’s goal is to use his abilities
to reveal the victim’s secret emails A, B, and C.
Memcached does not have any data-dependent control ﬂow,
but the attacker can use the memory bus side channel to infer
the query sent to Memcached. Memcached stores all keys in
a single hash table primary_hashtable deﬁned in assoc.c
using the Murmur3 hash of a key as an index. Each entry of
the hash table is linearly indexed by the Murmur3 hash of
the key. Thus Memcached will access an address within the
hash table whenever it searches for a key. By observing the
address, the attacker can infer the hash of the key.
Memcached dynamically allocates the hash table at the
beginning of the application. The attacker can easily ﬁnd out
the address of the hash table by sending a malicious email
to make Memcached access the hash table. For example in
Figure 6, the attacker sends an email D which contains a word
"Investment". Memcached accesses the entry, and the attacker
observes the address. Since the attacker already knows the
hash value of the key, she can easily ﬁnd out the address of
the hash table.
Next, the attacker keeps observing the memory accesses
within the hash table. Once the attacker ﬁgures out the hash
Figure 6: An example attack scenario where a mail server
uses Memcached as an index database. A, B, C and D are the
emails.
table address, she can reveal the hash values of the query, by
observing the virtual addresses accessed by Memcached. To
match the hash values with words, the attacker pre-computes
some natural words and creates a hash-to-word mapping.
Even though hashes can conﬂict, we show that the attacker
can recover most of the words by just picking a most-common
word based on the statistics.
5
As previously discussed, the basic attack model of MEM-
BUSTER can observe memory transactions with cache-line
granularity when the memory transactions cause cache misses
in the last-level cache (LLC). Such an attack model is weak-
ened in a modern processor with a large LLC ranging from
4 MB to 64 MB, causing only a small fraction of memory
transactions to be observable on the DRAM bus.
Increasing Critical Cache Misses
In this section, we introduce techniques to increase cache
misses of the target enclaves. In a realistic scenario, an at-
tacker only cares about increasing the cache misses within
the virtual address range which leaks the side-channel infor-
mation. Take the attack on Hunspell for example, the attacker
only needs to observe the access on the nodes which store
the dictionary words. We called a memory address as critical
if the address is useful for the attack. Our goal is to increase
the cache misses on critical addresses, to improve the success
rate of the MEMBUSTER attack.
5.1 Can We Disable Caching?
A simple solution to increase cache misses is to disable
caching in the processor. On x86, entire cacheability can
be disabled by enabling the CD bit and disabling the NW bit
in the control register CR0 ( [63], Section 11.5.3). Some archi-
tectures allow disabling caching for a speciﬁc address range,
primarily for serving uncacheable DMA requests or memory-
mapped I/Os. For instance, on x86, users can use the Memory
Type Range Register (MTRR) to change the cacheability of a
physical memory range. Newer Intel processors also support
page attribute table (PAT) to manage page cacheability with
the attribute ﬁeld in page table entries.
However, besides disabling the entire cacheability, nei-
ther MTRR or PAT can overwrite the cacheability of SGX’s
processor-reserved memory (PRM) [39]. The cacheabil-
ity of PRM is speciﬁcally controlled by a special reg-
ister called Processor-Reserved Memory Range Register
Mail ServerIndex DBMemcachedABCThanks: A BDear: B CInvestment: B DDSearchIndexingUpdateSend EmailMEMBUSTERAttackerVictimSend/Recv. EmailsA’B’C’(PRMRR), which can be only written by BIOS during boot-
ing. Since there is no proprietary BIOS that allows the user
to modify PRMRR, the attacker effectively has no way to
change the cacheability of the encrypted memory. However,
since the BIOS is untrusted in the threat model of SGX, in
theory, one can reverse-engineer the existing BIOS or build a
custom BIOS to overwrite PRMRR. We do not choose this
route because disabling cacheability will incur signiﬁcant
slowdown, making the attack easy to detect by the victim.
5.2 Critical Page Whitelisting
We observed that after paging (swapping), memory access
in the swapped pages becomes unobservable to the attacker.
Such a phenomenon is common for SGX since SGX has to
rely on the OS to swap pages in and out of the EPC. Both
swap-in and swap-out causes the page to be loaded into the
cache hierarchy (LLC, L2, and L1-D caches), because the
SGX instructions for swap-in and swap-out, i.e., eldu and
ewb, require re-encrypting the page from/to a regular physical
page [39]. After the instructions, the cache lines stay in the
cache hierarchy until being evicted by other memory access.
Currently, an Intel CPU with SGX only has up to 93.5MB in
the EPC, making paging the primary obstacle to observing
critical transactions on the memory bus.
On the other hand, paging also complicates the virtual-
to-physical address translation, as the mappings can change
midst execution. We observe certain patterns in the memory
bus log to identify the paging events. However, these patterns
can also become unobservable if the page is recently swapped
and most of the cache lines are still in the LLC.
Therefore, to eliminate the side effect of paging, we pin
the EPC pages for the critical address range, by modifying
the SGX driver. We start by identifying the critical address
range of each target program. Take the Hunspell program
for example. The critical memory transactions come from
accessing the dictionary nodes, which are allocated through
malloc(). For simplicity, we disable Address Space Layout
Randomization (ASLR) inside the enclave (controlled by the
library OS [54]), although we conﬁrmed that ASLR can be de-
feated by identifying contiguous memory access pattern in the
traces. Next, we calculate the number of EPC pages needed
for pinning the critical pages. For a Hunspell execution using
an en_US dictionary, the total malloc() range is 5,604 KB.
Finally, we need to give the critical address range as an input
to the modiﬁed SGX driver. When the driver allocates an EPC
page, it checks if the virtual address is in the critical address
range and use an in-kernel ﬂag to indicate if the page has to
be pinned. The driver will never swap out a pinned page.
5.3 Priming the Cache
We explore ways to actively contaminate the caches by ac-
cessing contentious addresses. This technique is called cache
priming, which is used in the PRIME+PROBE attack [44]. Pre-
vious work has established priming techniques for either same-
core or cross-core scenarios. Some priming techniques are
restricted by CPU models, especially since many recent CPU
models have employed designs or features that raise the bar
for cache-based side-channel attacks. However, recent studies
also show that, even with these defenses, attackers continue to
ﬁnd attack surfaces within the CPU micro-architectures, such
as priming the cache directory in a non-inclusive cache [64].
We focus on cross-core priming since same-core priming
requires interrupting the enclaves using AEX or page faults.
The usage of cache priming in MEMBUSTER is distinctly dif-
ferent from existing cache-based side-channel attacks since
MEMBUSTER does not require resetting the state of the cache
or synchronizing with the victim. The goal of cache prim-
ing in MEMBUSTER is to simply evict the critical addresses
from the cache to increase the cache misses. Also, with cache
squeezing, we only have to prime the cache sets dedicated to
the critical addresses. These differences make it easy to apply
multiple priming attacks simultaneously, as long as they all
eventually contribute to increasing cache misses.
Cross-Core Cache Priming We run multiple priming pro-
cesses on other cores to evict the critical cache lines from the
LLC. These processes will repeatedly access the cache sets
that are shared with the critical addresses of the victim. The
attacker will start by identifying the critical addresses and the
cache sets to prime. Then, the attacker starts the priming pro-
cesses before the victim enclave, to actively evict the cache
lines during execution. Take the Hunspell attack for example.
Since its critical addresses are spread over all cache sets, the
attacker needs to repeatedly prime all cache sets. No syn-
chronization is required between the attack processes and the
victim. We do not prime the L1 and L2 caches across cores,
but cross-core priming on private caches is demonstrated on
Intel CPUs [64].
A potential hurdle for cross-core priming is to obtain suf-
ﬁcient memory bandwidth to evict the critical cache lines.
Based on our experiments, a priming process that sequentially
accesses the LLC has around 100–200MB/s memory band-
width. Priming a 9MB LLC with 2,048 sets requires about 100
milliseconds, which is too slow to evict the critical cache lines
before the lines are accessed by the victim again. For instance,
Hunspell accesses a word every 2 thousand DRAM cycles
(< 1 microseconds), and Memcached accesses a word every 5
million DRAM cycles (< 2.5 milliseconds). We will discuss,
however, how an attacker can evict all the critical cache lines
within a few milliseconds by pinpointing the priming process
to target only 64–128 sets (See §5.4.2).
Page-Fault Cache Priming Potentially, an attacker can
prime the LLC, L2, and L2-D caches on the same core with
the victim, by interrupting the victim periodically. To do so,
the attacker can take a similar approach to the Controlled-
Channel Attack: The attacker identiﬁes two code pages con-
taining code around the critical memory accesses, and then
alternatively protects the pages to trigger page faults. To in-
crease cache misses, the attacker needs not to prime the cache
Figure 7: Techniques used to increase the cache miss rate
with minimal performance overhead.
at every page fault, but rather can prime at a low frequency.
However, such a page-fault priming technique still causes a
lot of interference and overhead to the victim, making it easy
to detect [22] or to mitigate [31,32]. For example, priming the
cache on every 10-20 page faults incurs about 3× overhead
to the victim. In addition, known countermeasures, such as
T-SGX [31], can effectively prevent page faults using transac-
tional instructions. Therefore, we do not use this technique.
5.4 Shrinking the Effective Cache Size
As previously discussed, cache priming alone cannot create
sufﬁcient memory access bandwidth for evicting the critical
cache lines in time. Therefore, we introduce a novel tech-
nique called cache squeezing, which shrinks the effective
cache size to incur more cache misses for a speciﬁc address
range. We show that the technique can be combined with non-
intrusive techniques like cross-core cache priming to make
MEMBUSTER a more powerful side channel.
5.4.1 Cache Squeezing
As the name suggests, cache squeezing can shrink the effec-
tive cache size for a given set of critical pages. By squeezing
the cache that an enclave can use, the attacker can incur both
conﬂict misses and capacity misses on LLC, therefore becom-
ing able to observe more cache misses on the bus.
In modern processors, the L2 cache and LLC are physically-
indexed. The lowest 6 bits of the physical address are omitted,
given that each cache line is 64 bytes. The next s lower bits
are taken as the set index. Each set then consists of W ways
to store multiple cache lines of the same set index. For an
enclave, an OS-level attacker can control the physical pages
that are mapped to the enclave’s virtual pages. This allows
the attacker to manipulate the physical frame number (PFN)
of each virtual address of the enclave, and subsequently, the
higher s− (12− 6) = s− 6 bits of the set index.
Figure 7(1) shows how cache squeezing works in combina-
tion with page pinning. The attacker ﬁrst deﬁnes the critical
addresses of the victim, then maps these pages to EPC pages
that share the minimum amount of cache sets. This tech-
nique requires cache pinning so that these pages will never
be swapped out from the EPC. Since the OS only controls
the higher s− 6 bits of the set indices, the smallest group of
physical pages that will evict each other share exactly 26 = 64
sets. We called such a group of physical pages a conﬂict group.
Since the maximum size of EPC is 93.5 MB, the entire cache
can be partitioned to 2s−6 conﬂict groups where each conﬂict
group can accommodate 93.5 MB/4 KB/2s−6 EPC pages. In
our experiment, s = 11 (2048 sets) and W = 12, so each con-
ﬂict group can accommodate at most 748 pages (2,992 KB).
The critical address range of Hunspell, for example, is the
whole malloc() space, which is 5,604 KB and thus requires
two conﬂict groups. Finally, the attacker gives the critical
address range to a modiﬁed SGX driver, which will only map
physical pages from the selected conﬂict groups to any critical
virtual address.
Using cache squeezing to increase cache misses has many
beneﬁts. First of all, it does not require interrupting the victim