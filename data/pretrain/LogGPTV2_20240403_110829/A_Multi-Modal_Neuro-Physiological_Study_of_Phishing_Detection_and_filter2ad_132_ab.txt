thumb rule of EEG experiments design [24, 38], and previous rel-
evant neuro-physiological studies [14, 26, 36]. Multiple trials are
necessary in such experiments to achieve a high signal-to-noise ra-
tio. The experiment started with the Firefox browser loading the
instructions page (explaining the terms “real” and “fake”, and spec-
ifying the tasks participants were to perform), which lasted for 30
seconds. This was followed by the trials pages, each displayed for
10s. Each trial consisted of a webpage (corresponding to a fake/real
481website) shown for 6s, followed by a 4s long response page. The
response page had a dialog box with the question, “Do you think
the shown website is real?” and the “Yes” and No” buttons. A rest
page of 2s (+ sign shown at the center of a blank page), after each
trial was added, during which participants were asked to relax. The
experiment ended after 37 trials with the goodbye note, displayed
for 5s. The process ﬂow diagram of the experiment is shown in
Figure 1.
3.3 Malware Warnings Experiment
Malware is malicious software aimed to obtain unauthorized ac-
cess to computer resources and collect a users’ private information.
As a user visits a malicious website, such malware may infect the
user’s computer. However, modern browsers have devised warning
mechanisms to alert the users in case they visit a potentially sus-
picious web site, relying upon users’ input to proceed. Whether
or not users read (measured via eye-tracker), understand (measured
via EEG cognitive metrics) and heed (measured via task perfor-
mance) these warnings, are the key questions we are exploring in
this work. The EEG cognitive metrics were calculated using the
data acquired from three baseline conditions (Section 5.3 provides
details). In our experiment, participants were shown the real warn-
ings employed by Firefox (sample shown in Appendix A).
Figure 2:
(right) ﬂow chart of components of a trial page
(left) ﬂow chart of entire warnings experiment;
Experiment Design and Implementation: We extracted diverse in-
teresting news samples from popular websites, including BBC, NY-
Times, Daily Mirror, and CNN, and published them following our
own news presentation template. The news samples were divided
into two sections, Abstract and Full News. The abstract had a “read
more” link which pointed to the corresponding full news. The pri-
mary task of the participants was to read the abstract of the news.
Some of the news items were randomly intermixed with malware
warnings. The warnings were unexpectedly displayed when partic-
ipants were reading the abstract, or when they clicked on the read
more link. Upon ignoring the warning, full news was displayed.
The two buttons on the warnings mimicked the ones on real Fire-
fox malware warnings. That is, the “Get me out of here” button
linked to the home page of Firefox, and the “Why this page was
blocked” button linked to the page providing the details as to why
the page was blocked.
In this experiment, there were 20 randomized trials: 10 each
for the warning and non-warning trials. Similar to the phishing
detection experiment, we set the number of trials following the
thumb rule of EEG experiments design [24, 38] and previous rel-
evant neuro-physiological studies [14, 26, 36]. The non-warning
trials are those in which full-news is shown immediately after the
abstract. The experiment started with the instructions page which
lasted for 30s, followed by trials, each 30s long. Each trial con-
sisted of an abstract along with the read more link. Similar to the
phishing detection experiment, the rest (+ sign) page of 2s, after
each trial were added. The experiment ended with the goodbye
note shown for 5s. The ﬂow chart of the malware warning experi-
ment is shown in Figure 2.
4. REPEATED MEASURES AND
EXPERIMENTAL SET-UP
In our experiments, we recorded each participant’s response, re-
sponse time, neural (EEG) activity, and eye gaze activity while
he/she performed the phishing detection and malware warnings tasks.
The goal of the experiments was to measure the participants’ task
performance, cognitive states, and gaze patterns.
Task Performance: We created a custom software-hardware con-
ﬁguration that enabled us to log participants’ responses and re-
sponse times.
Neural Activity and Gaze Patterns: For measuring neural activ-
ity, we used a wireless EEG sensor B-Alert headset, X10-Standard,
developed by Advanced Brain Monitoring (ABM) [1]. This EEG
system, shown in Figure 3 (right), provides a lightweight (less than
3 oz.) means to acquire and analyze 10 channels of high-quality
EEG data. The sensors of this EEG headset follow the 10-20 inter-
national system of placement. It uses the Fz, F3, F4, C3, Cz, C4,
P3, POz, P4 sites to collect EEG data at 256 Hz (Figure 3 (left)).
The portable unit worn on the back of the head (Figure 3 (right))
contains miniaturized electronics that amplify, digitize, and trans-
mit the EEG data to the host computer over Bluetooth. The sensors
require no scalp preparation; however, water soluble gel had to be
applied at electrode sites for better conductance of signals between
the skull and sensors. The headset provided a comfortable sensor-
scalp interface for 8-12 hours of continuous use.
For measuring gaze patterns, we used EyeTech DS TM3 (remote
desk mounted) eye tracker system with a frequency of 60 Hz. The
TM3 uses infra-red lights to illuminate the eyes and provide ref-
erence points for the eye tracker. Data is captured for both eyes
including X/Y gaze coordinates, timestamp, image pixel data, and
location and size of pupils within the image.
System Set-Up: Our experimental set-up comprised of a collection
of four computers: (1) “Survey Computer” to administer the sur-
veys (described in Section 5.2), (2) “Stimuli Computer” to present
experiment tasks (and collect eye-gaze data), (3) “Data Collection
Computer” to collect neural data and (4) “Data Visualization Com-
puter” to remotely monitor neural data to ensure its quality. The
Stimuli Computer was a laptop with a 15.6 inch screen at a resolu-
tion of 1600 x 900.
Figure 3: (left) B-Alert electrode arrangement; (right) experi-
mental set-up
4825. STUDY PROCEDURES
Our study followed a within-study design, i.e., all participants per-
formed the same set of (randomized) trials.
5.1 Ethical and Safety Considerations
The study was approved by our University’s IRB. The participation
in the study was strictly voluntary. The participants were given
the option to withdraw from the study at any point of time. The
standard best practices were followed to protect the conﬁdentiality
and privacy of participants’ data (survey responses, task responses,
EEG and eye tracker data) acquired during the study.
5.2 Recruitment and Preparation Phase
The participants were recruited by distributing the study adver-
tisements across our University’s campus and on online-media (Face-
book & Twitter). Twenty-ﬁve healthy participants were recruited
for the study. Due to the EEG component of our study, the partici-
pants were excluded from the study if they had a history of neuro-
logical disorders, anxiety disorder, schizophrenia, and if they were
on any psychotropic drugs. Each participant took about a total of
2 hours to complete the study, and was compensated with $40 cash
for their time.
Appendix C provides participants’ demographics. The majority
of our participants were young, students and males. However, our
sample was fairly diversiﬁed. In particular, none of the participants
were computer scientists, but rather had diverse backgrounds, such
as engineering, education, medical science, physics, and physical
health. There were 28% working professionals and non-working
people. 35% were above the age of 27, and 36% were females.
Future studies might be needed to further validate our results with
broader participant samples.
During the preparation phase of the study, informed consent was
obtained from each participant.
In this phase, we also admin-
istered two surveys to our participants to measure their person-
(1) impulsivity using the Barrat’s Impulsivity Scale
ality traits:
(BIS) [27], and (2) attention control using the Attention Control
Scale (ATTC) [16]. BIS is a 30-question-set questionnaire. The
higher the BIS score, the higher the impulsivity. ATTC is a 20-
question-set questionnaire used to assess executive control of indi-
viduals over their attention. The higher the ATTC score, the higher
the attention control in an individual. For each of BIS and ATTC,
we calculated aggregated scores derived from all of the questions
as stipulated in [16, 27].
5.3 Testing (Data Collection) Phase
A measurement of each participant’s head was ﬁrst taken to deter-
mine the best size of the B-Alert headset that would ﬁt that partic-
ipant (our headset came in three sizes: small, medium and large).
The EEG headset was then placed on the participant’s head, and
the participants were moved to Data Collection Computer for an
impedance check to ensure the quality of the EEG data.
The participants next completed a 15-minute baseline EEG ses-
sion that included three 5-minute baseline conditions, namely, stan-
dard Eyes-Closed, Eyes-Open, and proprietary 3-Choice Vigilance
Task (3C-VT) (developed by ABM [23]). In 3C-VT, participants
had to discriminate between one primary and two secondary geo-
metric shapes with stimulus presentation interval of 1.5 to 3s. In
the Eyes-Open task, the participants had to respond to visual probe
every 2 seconds. In the Eyes-Closed task, they had to respond to
audio probe every 2 seconds. These tasks deﬁned the classes of dis-
traction/relaxed wakefulness (DIS), low engagement (LENG), and
high engagement (HENG), respectively [23]. The class of sleep
onset (SO) is derived using stepwise linear regression using data
from these three tasks. The baseline session data is used to create
individualized EEG proﬁles required for the calculation of cogni-
tive state metrics (i.e., SO, DIS, LENG, HENG, referred to as the
cognitive states, and Workload) [23].
In our Stimuli Computer, calibration of the eye-tracker was done.
Once the participant was ready to perform the experiment, the BAS
data acquisition button in the Data Collection Computer, eye tracker
gaze points capture module and in house software to execute the
tasks were triggered. The phishing detection and malware warn-
ings experiments were executed in random order for different par-
ticipants. This was done to ensure none of the experiments yield
biased results based on the order of their execution.
Impedance, noises and EEG signals were continuously moni-
tored on the Data Visualization Computer to conﬁrm the quality
of the data collected. Eye-tracker calibration check was done af-
ter each experiment to ensure optimal functioning. A 5-minute gap
between the two tasks was provided so participants could rest.
5.4 Post-Test Phase
After completing the security tasks, each participant was asked to
ﬁll out a post-test questionnaire (presented on Survey Computer).
This questionnaire was designed to determine participants’ knowl-
edge of computer security, and to learn how they performed the
security tasks they participated in. For example, the participants
were asked if they had heard about phishing attacks and malware
warnings, and whether they read the warnings and what the warn-
ing said. After the post-test questionnaire, the participants were
provided with their cash reward.
6. ANALYSIS PROCEDURES & METRICS
6.1 Neural Data
The BAS software included real-time artifact removal for fast and
slow eye blinks, muscle movement, and environmental/electrical
interference such as spikes and saturations [29]. The data from two
of our participants was excluded due to the presence of excessive
noise, leaving us with the good quality data from 23 participants.
We then used the B-Alert Lab (BAL) Software provided by ABM
to conduct the ofﬂine data analysis.
We synchronized the EEG data collected during the experiments
with the trial presentation time and order. The BAL software then
took the synchronized data and the baseline model as its input, and
classiﬁed each 1-second of EEG data, referred to as an epoch, into
one of four cognitive states: high engagement (HENG),
low en-
gagement (LENG), distraction (DIS), and sleep onset (SO) [29]
(see Appendix B for details). For example, for a 6-second time pe-
riod when a participant was viewing a webpage during the phish-
ing experiment, the BAL software produced 6 mental state values
(HENG, LENG, DIS, or SO) for each of the 6 seconds that the
participant viewed the webpage. ENG, either HENG or LENG, de-
notes the state in which users are paying attention to the informa-
tion they are provided [12,23]. It reﬂects information-gathering, vi-
sual scanning and sustained attention of participants during a given
task. DIS is the state when participants shift their attention from
the primary task to focus on another activity [12, 23]. SO reﬂects
the state in which people may be able to respond to stimuli but still
not able to integrate all information and features [12, 23].
Mental workload (WL) [11, 12] was also calculated for each
epoch using the BAL software (Appendix B). WL reﬂects the
amount of neural effort and resources required for a given task.
WL increases with increasing working memory load, and under
problem-solving, integration of information and analytical reason-
ing, reﬂecting brain’s executive functioning.
483Based on these measures, we computed the average cognitive
workload (WL) and average percentage of frequency (pfr) for which
the participants were engaged (pfrENG), distracted (pfrDIS), and
under sleep onset (pfrSO), corresponding to different types of trial.
WL is calculated on a scale of 0-1; higher values denote higher
workloads. Percentage frequency in a trial represents the fraction
of the duration for which the participant was in a given mental state
(ENG, DIS or SO) in that trial. For example, if someone was highly
engaged for 2 epochs, lowly engaged for 2 epochs, distracted for 1
epoch and under sleep onset for 1 epoch, during the 6 second trial,
the percentage frequency of engagement will be 4/6 (.67), percent-
age frequency of distraction will be 1/6 (.17), and percentage fre-
quency of sleep onset will be 1/6 (.17).
6.2 Eye Tracking Data
The gaze data collected during the experiments was used to com-
pute the mean number of ﬁxations and mean gaze duration of par-
ticipants in speciﬁc areas of the websites, referred as Areas of In-
terest (AOI). Fixation is deﬁned as a pause made by a user look-
ing at a speciﬁc area to extract meaningful information. We used
a dispersion-based technique, dispersion-threshold algorithm [30],
to compute ﬁxations. This algorithm identiﬁed ﬁxations as a group
of consecutive points within a particular dispersion and duration
threshold [30].
6.3 Statistical Testing
All statistical results in this paper are reported at the signiﬁcance
level (α) of .05. The Friedman test was used to test for the existence
of differences within the groups, and, if it succeeded, the Wilcoxon
Singed-Rank Test (WSRT) was used to examine in which pairs the
differences occurred. The effect size of WSRT was calculated us-
N, where Z is the value of the z-statistic
ing the formula r = Z/
and N is the number of observations on which Z is based. The sta-
tistically signiﬁcant pairwise comparisons are reported with Holm-
Bonferroni corrections for multiple testing. Correlations between
different conditions were measured using the Spearman’s rank cor-
relation coefﬁcient, with Holm-Bonferroni corrections.
√
7. RESULTS AND ANALYSIS
7.1 Phishing Detection Experiment
To recall, in the phishing detection task, the participants were asked
to identify whether a given website is real or fake. We analyzed
the neural data, gaze data, and the task performance data collected
during the experiment, and studied their interrelationships with one
another and with participants’ personality scores.
7.1.1 Neural Activity Results
As described in Section 6.1, we computed the average cognitive
workload (WL) and average percentage of frequency for which the
participants were engaged (pfrENG), distracted (pfrDIS), and under
sleep onset (pfrSO), for our different trials (Real, Fake, EFake and
DFake). The results are shown in Table 1.
From Table 1 (column 1), we see that the average workload exhib-
ited by our participants in identifying the websites as real or fake is
high (more than 0.5) for all types of trials. Upon using the Fried-
man test to test for differences in workload among different types
of trials, we did not ﬁnd a statistically signiﬁcant difference.
Considering the cognitive state results (columns 2-4), we see that
the participants frequency of being engaged was high (at least 50%
in all trials except DFake), and their frequency of being distracted
or under sleep onset was low (at most 30%). This suggests that the
Metric →
Overall
EFake
DFake
Trials ↓
Real
Fake
Overall
WL
µ (σ)
.65 (.08)
.64 (.08)
.64 (.08)
.64 (.08)
.64 (.08)
pfrENG
µ (σ)
.61 (.18)
.50 (.03)
.62 (.17)
.39 (.18)
.54 (.05)
pfrDIS
µ (σ)
.13 (.12)
.20 (.06)
.11 (.09)
.29 (.14)
.18 (.06)
pfrSO
µ (σ)
.25 (.17)