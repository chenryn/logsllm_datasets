title:Abusing File Processing in Malware Detectors for Fun and Profit
author:Suman Jana and
Vitaly Shmatikov
2012 IEEE Symposium on Security and Privacy
Abusing File Processing in Malware Detectors for Fun and Proﬁt
Suman Jana and Vitaly Shmatikov
The University of Texas at Austin
Abstract—We systematically describe two classes of evasion
exploits against automated malware detectors. Chameleon at-
tacks confuse the detectors’ ﬁle-type inference heuristics, while
werewolf attacks exploit discrepancies in format-speciﬁc ﬁle
parsing between the detectors and actual operating systems
and applications. These attacks do not rely on obfuscation,
metamorphism, binary packing, or any other changes to
malicious code. Because they enable even the simplest, easily
detectable viruses to evade detection, we argue that ﬁle pro-
cessing has become the weakest link of malware defense. Using
a combination of manual analysis and black-box differential
fuzzing, we discovered 45 new evasion exploits and tested them
against 36 popular antivirus scanners, all of which proved
vulnerable to various chameleon and werewolf attacks.
I. INTRODUCTION
Modern malware detectors employ a variety of detection
techniques: scanning for instances of known viruses, binary
reverse-engineering, behavioral analysis, and many others.
Before any of these techniques can be applied to a suspicious
ﬁle, however, the detector must (1) determine the type of the
ﬁle, and (2) depending on the type, analyze the ﬁle’s meta-
data and parse the ﬁle—for example, extract the contents
of an archive, ﬁnd macros embedded in a document, or
construct a contiguous view of executable code.
The importance of ﬁle processing grows as automated
malware defense moves away from the host, with antivirus
(AV) scanners and intrusion prevention systems installed at
enterprise gateways and email servers, increasing popularity
of cloud-based malware detection services, etc. Network-
and cloud-based deployments protect multiple hosts, provide
early detection capabilities and better visibility into network-
wide trends, and make maintenance easier. To be effective,
however, remotely deployed detectors must be able to predict
how each ﬁle will be processed at its destination by the
operating system and applications.
In this paper, we argue that the “semantic gap” between
how malware detectors handle ﬁles and how the same
ﬁles are processed on the endhosts is the Achilles heel of
malware defense. We use the term detector generically for
signature-based scanners, behavioral analyzers, or any other
tool that parses and analyzes suspicious ﬁles on its own,
independently of the destination endhost. The vulnerabilities
we describe are unrelated to obfuscation, mutation, or any
other way of hiding malicious functionality. They enable
even exact, unmodiﬁed instances of malware—primitive and
(otherwise) trivially detectable, as well as arbitrarily sophis-
ticated—to evade detection simply because the detector fails
to correctly process the infected ﬁle.
We introduce two new classes of evasion exploits against
malware detectors. The ﬁrst is chameleon attacks, which
target
the discrepancies between the heuristics used by
detectors to determine the type of the ﬁle and those used by
the endhosts. Contrary to a common misconception, neither
is based solely on the ﬁle extension, thus our attacks are
more complex than simply renaming the extension (this trick
does not work against modern detectors), nor can they be
solved by forcing a particular extension onto a ﬁle.
The second class is werewolf attacks, which exploit the
discrepancies in the parsing of executables and application-
speciﬁc formats between malware detectors and actual ap-
plications and operating systems.
We evaluated 36 popular AV scanners using a combination
of manual analysis and differential black-box fuzzing, and
discovered 45 different exploits. All tested scanners proved
vulnerable to both chameleon and werewolf attacks. We
stress that the problem is not speciﬁc to AV scanners and
does not depend on known weaknesses of signature-based
scanning such as the inability to handle metamorphic muta-
tions. The actual viruses used in our testing are extremely
simple. They do not employ self-encryption, polymorphism,
or obfuscation, yet chameleon and werewolf attacks enable
them to pass undetected through scanners whose virus
databases contain their exact code. Because ﬁle processing
must
take place before actual malware detection, more
elaborate detectors are vulnerable, too, as long as their ﬁle-
processing logic differs, however slightly, from the OS and
applications on any of the protected endhosts.
The problem is deeper than the anecdotally known in-
ability of AV software to properly process archive formats.
Many modern ﬁle formats are effectively archives in dis-
guise: for example, MS Ofﬁce documents contain executable
macros, Compiled HTML Help (CHM) contains HTML
ﬁles, etc. We discovered chameleon and werewolf attacks
against all ﬁle formats we tested, from relatively simple
archives to executable images and complex MS Ofﬁce docu-
ment formats. Evasion techniques based on code obfuscation
are widely known and many defenses have been proposed. In
contrast, our attacks involve changes only to the meta-data
of infected ﬁles and are thus different, signiﬁcantly simpler,
and complementary to obfuscation-based evasion.
While each individual vulnerability may be easy to ﬁx, ﬁle
processing in malware detectors suffers from thousands of
© 2012, Suman Jana. Under license to IEEE.
DOI 10.1109/SP.2012.15
80
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:47:44 UTC from IEEE Xplore.  Restrictions apply. 
semantic discrepancies. It is very difﬁcult to “write a better
parser” that precisely replicates the ﬁle-parsing semantics of
actual applications and operating systems: (1) many formats
are underspeciﬁed, thus different applications process the
same ﬁle in different and even contradictory ways, all of
which must be replicated by the detector; (2) replicating
the behavior of a given parser is hard—for example, after
many years of testing, there are still hundreds of ﬁle-parsing
discrepancies between OpenOfﬁce and MS Ofﬁce [23, 24]
and between the “functionally equivalent” implementations
of Unix utilities [7]; (3) the detector must be bug-compatible
with all applications; (4) because applications are designed
to handle even malformed ﬁles, their parsing algorithms
are much looser
than the format speciﬁcation, change
from version to version, and have idiosyncratic, difﬁcult-
to-replicate, mutually incompatible semantics for processing
non-compliant ﬁles, all of which must be replicated by the
detector; (5) even seemingly “safe” discrepancies—such as
attempting to analyze ﬁles with invalid checksums when
scanning an archive for malware—enable evasion.
Responsible disclosure. All attacks described in this paper
have been reported to the public through the Common
Vulnerabilities and Exposures (CVE) database.1 In the rest
of this paper, we refer to them by their candidate CVE
numbers (see Tables I and II). These numbers were current
at the time of writing, but may change in the future.
II. RELATED WORK
We introduce chameleon and werewolf attacks as a
generic, pervasive problem in all automated malware de-
tectors and demonstrate 45 distinct attacks on 36 different
detectors, exploiting semantic gaps in their processing of
many archive and non-archive formats. With a couple of
exceptions (e.g., a RAR archive masquerading as a Windows
executable, previously mentioned in [2]), the attacks in this
paper are reported and described for the ﬁrst time.
There is prior evidence of malformed archive ﬁles evading
AV software [2, 3, 10, 18, 38, 39]. These anecdotes are
limited to archive formats only and do not describe concrete
attacks. Alvarez and Zoller brieﬂy mention that modern AV
scanners need to parse a variety of formats [2] and point out
the importance of correct ﬁle parsing for preventing denial
of service [1]. Concrete werewolf attacks on the detectors’
parsing logic for non-archive formats such as executables
and Ofﬁce documents have been reported in neither research
literature, nor folklore. These attacks have especially serious
repercussions because they are not prevented even by host-
based on-access scanning (see Section IX-A).
Buffer overﬂow and other attacks on AV software, unre-
lated to ﬁle processing, are mentioned in [36, 37].
Chameleon attacks. Chameleon attacks on ﬁle-type in-
ference heuristics are superﬁcially similar to attacks on
1http://cve.mitre.org/
content-snifﬁng heuristics in Web browsers [19, 25, 30].
Barth et al. proposed preﬁx-disjoint content signatures as
a browser-based defense against content-snifﬁng attacks [4].
The premise of this defense is that no ﬁle that matches the
ﬁrst few bytes of some format should be parsed as HTML
regardless of its subsequent content.
Preﬁx-disjoint signatures do not provide a robust defense
against chameleon attacks on malware detectors. Detectors
handle many more ﬁle formats than Web browsers and,
crucially, these formats cannot be characterized solely by
their initial bytes (e.g., valid TAR archives can have arbitrary
content in their ﬁrst 254 bytes, possibly including signatures
for other ﬁle types). Therefore, they cannot be described
completely by any set of preﬁx-disjoint signatures.
Other semantic-gap attacks. Chameleon and werewolf
attacks are an instance of a general class of “semantic-gap”
attacks which exploit different views of the same object
by the security monitor and the actual system. The gap
described in this paper—the monitor’s (mis)understanding
of the type and structure of suspicious ﬁles—received much
less attention than other evasion vectors against AV scanners
and intrusion detection systems [16, 27, 31] and may very
well be the weakest link of malware defense.
Other, complementary evasion techniques exploit net-
working protocols (e.g., split malware into multiple packets),
obfuscate malware using mutation or packing [20], or, in
the case of malicious JavaScript, obfuscate it in HTML,
Flash, and PDF content. For example, Porst showed how
to obfuscate scripts so that they are not recognized by AV
scanners but parsed correctly by the Adobe reader [26].
HTML parsing is notoriously tricky [28], and cross-site
scripting can exploit HTML-parsing discrepancies between
browsers and sanitization routines [5, 35]. In contrast, we
show how the most primitive viruses, which are present in
standard virus databases and do not use any obfuscation, can
evade detection by exploiting discrepancies in the processing
of even basic ﬁle formats such as TAR and PE.
An entirely different kind of semantic gap is exploited by
“split-personality” malware, whose behavior varies between
monitored and unmonitored environments [8]. Such malware
contains code that tries to detect virtualization, emulation,
and/or instrumentation libraries. In contrast, our attacks are
completely passive, require no active code whatsoever, and
target a different feature of malware detection systems.
Semantic-gap attacks on system-call interposition exploit
the gap between the monitor’s and the OS’s views of system-
call arguments [12, 34]. These attacks typically involve
concurrency and are fundamentally different from the attacks
described in this paper.
Program differencing. Brumley et al. proposed to auto-
matically detect discrepancies between different implemen-
tations of the same protocol speciﬁcation by converting
execution traces into symbolic formulas and comparing
81
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:47:44 UTC from IEEE Xplore.  Restrictions apply. 
Table I
TESTED AV SCANNERS.
Name
ClamAV 0.96.4
GData 21
Ikarus T3.1.1.97.0
F-Prot 4.6.2.117
Antiy-AVL 2.0.3.7
Kaspersky 7.0.0.125
Sophos 4.61.0
Norman 6.06.12
AV number
1
4
7
10
13
16
19
22
25 McAfee-GW-Edition 2010.1C
28
31
34
BitDefender 7.2
nProtect 2011-01-17.01
Avast 4.8.1351.0
Name
Rising 22.83.00.03
Symantec 20101.3.0.103
Emsisoft 5.1.0.1
VirusBuster 13.6.151.0
K7AntiVirus 9.77.3565
Jiangmin 13.0.900
NOD32 5795
AV number
2
5
8
11
14
17
20
23 McAfee 5.400.0.1158
26
29
32
35
TrendMicro 9.120.0.1004
eSafe 7.0.17.0
AhnLab-V3 2011.01.18.00
Avast5 5.0.677.0
Name
CAT-QuickHeal 11.00
Command 5.2.11.5
PCTools 7.0.3.5
Fortinent 4.2.254.0
TrendMicro-HouseCall 9.120.0.1004
AV number
3
6
9
12
15
18 Microsoft 1.6402
21
24
27
30
33
36
AntiVir 7.11.1.163
Panda 10.0.2.7
Comodo 7424
F-Secure 9.0.16160.0
AVG 10.0.0.1190
VBA32 3.12.14.2
AFFECTED AV SCANNERS FOR EACH REPORTED ATTACK.
Table II
CVE
2012-1419
Vulnerable scanners
1, 3
2012-1422
2, 3, 20, 22
2012-1425
2012-1428
2012-1431
2012-1434
2012-1437
2012-1440
2012-1443
2012-1446
2012-1449
2012-1452
3, 5, 7, 8, 9, 13, 15, 16, 17,
20, 21, 22, 23, 25, 26
3, 19, 22
2, 6, 10, 19, 25, 27, 28, 29,
30, 31
7, 8, 24, 32
27
22, 24, 29
1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
12, 13, 14, 15, 16, 19, 20, 21,
22, 23, 24, 25, 26, 27, 28, 29,
30, 32, 33, 34, 35, 36
2, 3, 5, 9, 13, 16, 19, 22, 23,
24, 25, 29
2, 20
3, 7, 8
2012-1455
2, 20
2012-1458
1, 19
2012-1459
2012-1461
2, 5, 6, 7, 8, 12, 14, 15, 16,
17, 19, 20, 22, 23, 25, 26, 28,
30, 33, 36
2012-1462
CVE
2012-1420
2012-1423
2012-1426
Vulnerable scanners
2, 3, 6, 10, 12, 14, 16, 18, 20,
22, 24
2, 6, 7, 8, 9, 10, 11, 12, 14,
20, 22
2, 3, 6, 10, 14, 22
CVE
2012-1421
Vulnerable scanners
2, 3, 5, 22
2012-1424
3, 9, 13, 17, 19, 22
2012-1427
3, 19, 22
2012-1429
7, 8, 23, 25, 27, 28, 29, 30, 31
2012-1430
2012-1432
7, 8, 24, 29
2012-1435
2012-1438
2012-1441
7, 8, 24, 29, 32
19, 27
29
2012-1444
24, 29
2012-1433
2012-1436
2012-1439
2012-1442
2012-1445
2, 19, 23, 25, 27, 28, 29, 30,
31
7, 8, 24, 29, 32
7, 8, 24, 29, 32
2, 24, 29
2, 3, 13, 16, 19, 23, 24, 25,
29, 30
2, 24, 29
2012-1447