data elements happen on the surviving disks, then L(cid:4) = L.
if the L requested elements include the lost
Otherwise,
Figure 8. The Reconstruction of Disk #1 in HV Code when p = 7. The
ﬁlled pattern elements participate in horizontal parity chain while the stripy
elements involve in the vertical parity chain.
elements, then the recovery of the lost elements will be
triggered by fetching the associated elements and ﬁnally
L(cid:4) >= L. The needed time for a degraded read pattern is
recorded from the time of issuing the degraded read pattern
to the time when the L(cid:4) elements are taken from the disk
array to the main memory. The I/O efﬁciency per degraded
read pattern is evaluated by the rate L(cid:2)
L . We then evaluate
these two metrics under the data corruption on every disk,
and calculate the expectation results in Fig. 7.
For the averaged time of a degraded read pattern (as
shown in Figure 7(a)), X-Code requires the maximum time.
This observation also proves the advantage of horizontal
parity when handling degraded read operations. Meanwhile,
this ﬁgure also reals that it usually needs more time when the
number of elements to read increases. HV Code signiﬁcantly
outperforms X-Code and retains similar performance when
compared with RDP Code, HDP Code, and H-Code.
With respect to the I/O efﬁciency, HV Code offers com-
petitive performance by signiﬁcantly reducing the number
of read elements. When L = 10, HV Code eliminates about
10.0%, 28.3%, 6.6%, and 7.3% degraded read I/O requests
compared with RDP Code, X-Code, HDP Code, and H-
Code, respectively.
C. The Recovery I/O for Single Disk Failure
In this test, we mainly compare the required I/O to
reconstruct a failed element. An example of single disk
repair in HV Code is shown in Figure 8 when p = 7, in
which at least 18 elements have to retrieve for the recovery
of lost elements and thus it needs 3 elements on average to
559
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:59:30 UTC from IEEE Xplore.  Restrictions apply. 
(a) Recovery for Single Disk Failure
(b) Recovery for Double Disk Failure
Figure 9. Comparison on Disk Failure Recovery.
repair each lost element on the failed disk.
Evaluation Method: Given an encoded ﬁle, for each case of
data corruption, we let the elements on that disk corrupted,
evaluate the minimal averaged elements that are retrieved
from the surviving disks to recover a lost element, and
calculate the expectation result. Following this evaluation
method, we consider the ﬁle encoded by RDP Code, X-
Code, HDP Code, H-Code, and HV Code respectively when
p varies, and show the results in Fig 9(a).
According to this comparison, HV Code requires the
minimal number of the needed elements to repair an invalid
element among the ﬁve candidates. The I/O reduction ranges
from 2.7% (compare with HDP Code) to 13.8% (compare
with H-Code) when p = 23. The saving will expand to
the range from 5.4% (compare with HDP Code) to 39.8%
(compare with H-Code) when p = 7. This superiority should
own to the shorter parity chain in HV Code compared to
those in other codes. We list the length of the parity chains
of some representative codes in Table III, which indicates
that the length of parity chain in both H-Code and RDP
Code is (p− 1) while HV Code only keeps (p− 2) elements
in a parity chain. Notice that the length of diagonal parity
chain and the length of horizontal-diagonal parity in HDP
Code are (p − 2) and (p − 1) respectively.
D. The Recovery for Double Disk Failures
As referred above, double disk failures require to fetch all
the elements in the survived disks, and the time to recover
the two failed disks can be simulated by the latency to
accomplish the recovery in the longest recovery chain.
Evaluation Method: Suppose the average time to recover
an element, either data element or parity element, is Re and
the longest length among all the recovery chains is Lc, then
the time needed to ﬁnish the recovery can be evaluated by
Lc · Re. A subsequent problem is the recovery time will be
not consistent because Lc may be changeable once the failed
disks vary. In this case, we consider the occurrence of any
double disk failure, test every possible recovery time, and
calculate the expectation.
The comparison results are shown in Figure 9 (b), which
shows a big difference among the compared codes. When
p = 7, both of X-Code and HV Code respectively reduce
nearly 47.4%, 47.4%, and 43.2% of the reconstruction time
when compared with RDP Code, HDP Code, and H-Code.
This saving will
increase to 59.7%, 50.0%, and 47.4%
when p = 23. This huge retrenchment should owe to the
placement of parity elements in X-Code and HV Code, both
of which distribute two parity elements over every single
disk. Every parity element will lead a recovery chain, which
begins at a start point (the start point can be obtained by the
chain intersecting either one of the two failed disks only)
and ends at a parity element. Therefore, four recovery chains
can be parallel executed without disturbing each other. RDP
Code gathers the parity elements on the speciﬁed disks and
the repair process should be serially processed, making it
the most time-consuming code to reconstruct two corrupted
disks. Both of H-Code and HDP Code though place two
parity elements over every disk, the dependent relationship
between the two kinds of parities in HDP Code results in
the worse performance.
E. A Comparison Between HV Code and Other Codes.
Based on the above comparisons, Table III summarizes
a detailed comparison between HV Code and other popular
MDS array codes in RAID-6, i.e., RDP Code (over p + 1
disks), HDP Code (over p−1 disks), X-Code (over p disks),
and H-Code (over p+1 disks). The comparison indicates that
HV Code achieves low cost for partial stripe writes, retains
the optimal update complexity, keeps high parallelism for
double disk recovery, and shortens the length of recovery
chain.
VI. CONCLUSION
In this paper, we propose HV Code, which can be
deployed over p − 1 disks (p is a prime number). HV
Code evenly places parities over the disks to achieve the
optimization of I/O balancing. By utilizing the horizontal
parity and designing a delicate construction of vertical parity,
HV Code signiﬁcantly decreases the I/O requests induced
by the partial stripe writes to continuous data elements.
Meanwhile, the shortened length of parity chain also grants
HV Code a more efﬁcient recovery for single disk failure
and good performance on degraded read operation compared
to other typical MDS codes in RAID-6. HV Code also
560
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:59:30 UTC from IEEE Xplore.  Restrictions apply. 
A BRIEF COMPARISON BETWEEN HV CODE AND OTHER TYPICAL MDS ARRAY CODES.
Table III
Codes
Load Balancing
Update Complexity
Partial Stripe Writes
Double Disk Reconstruction
Parity Chain Length
RDP Code [4]
HDP Code [3]
X-Code [7]
H-Code [10]
HV Code
unbalanced
balanced
balanced
unbalanced
balanced
more than 2 extra updates
3 extra updates
2 extra updates
2 extra updates
2 extra updates
low cost
high cost
high cost
low cost
low cost
2 recovery chains
2 recovery chains
4 recovery chains
2 recovery chains
4 recovery chains
p
p − 2,p − 1
p − 1
p − 2
p
accelerates the repair of two disabled disks by deriving four
independent recovery chains. The performance evaluation
demonstrates the efﬁciency brought by HV Code.
VII. ACKNOWLEDGMENT
We would like to thank our shepherd, Arun Somani, and
the anonymous reviewers for their constructive comments
and suggestions. This work is supported by the National
Natural Science Foundation of China (Grant No. 61232003,
60925006), the National High Technology Research and De-
velopment Program of China (Grant No. 2013AA013201),
Tsinghua-Tencent Joint Laboratory for Internet Innovation
Technology, and Tsinghua University Initiative Scientiﬁc
Research Program.
REFERENCES
[1] E. Pinheiro, W. Weber, and L. Barroso. Failure trends in a large
disk drive population. In Proc. of the USENIX FAST’07, 2007.
[2] B. Schroeder and G. Gibson. Disk failures in the real world:
What does an MTTF of 1,000,000 hours mean to you? In Proc.
of the USENIX FAST’07, 2007.
[3] C. Wu, X. He, G. Wu, S. Wan, X. Liu, Q. Cao, and C. Xie.
HDP Code: A Horizontal-Diagonal Parity Code to Optimize
I/O Load Balancing in RAID-6. In Proc. of DSN’11, 2011.
[4] P. Corbett, B. English, A. Goel, T. Grcanac, S. Kleiman, J.
Leong, and S. Sankar. Row-Diagonal Parity for double disk
failure correction. In Proc. of the USENIX FAST’04, 2004.
[5] M. Blaum, J. Brady, J. Bruck, and J. Menon. EVENODD: An
efﬁcient scheme for tolerating double disk failures in RAID
architectures. IEEE Transactions on Computers, 1995.
[6] S. Ghemawat, H. Gobioff, and S. Leung. The Google File
System. In Proc. of ACM SOSP, 2003.
[7] L. Xu and J. Bruck. X-Code: MDS array codes with optimal
encoding. IEEE Transactions on Information Theory, 1999.
[8] C. Jin, H. Jiang, D. Feng, and L. Tian. P-Code: A new RAID-6
code with optimal properties. In Proc. of the ICS’09, 2009.
[9] J. Plank. The RAID-6 liberation codes. In Proc. of the USENIX
FAST’08, 2008.
[10] C. Wu, S. Wan, X. He, Q. Cao, and C. Xie. H-Code: A Hybrid
MDS Array Code to Optimize Partial Stripe Writes in RAID-6.
In Proc. of IPDPS’11, 2011.
[11] M. Holland and G. Gibson. Parity declustering for continuous
operation in redundant disk arrays. In Proc. of the ASPLOS’92,
1992.
[12] L. Xu, V. Bohossian, J. Bruck, and D. Wagner. Low-density
MDS codes and factors of complete graphs. IEEE Transactions
on Information Theory, 1999.
[13] J. Plank. A new minimum density RAID-6 code with a word
size of eight. In Proc. of the IEEE NCA’08, 2008.
[14] C. Huang, M. Chen, and J. Li. Pyramid Codes: Flexible
schemes to trade space for access efﬁciency in reliable data
storage systems. In Proc. of the IEEE NCA’07, 2007.
[15] K. Greenan, X. Li, and J. Wylie. Flat XOR-based erasure
codes in storage systems: Constructions, efﬁcient recovery, and
tradeoffs. In Proc. of the IEEE MSST’10, 2010.
[16] S. Wan, Q. Cao, C. Xie, B. Eckart, and X. He. Code-M:
A Non-MDS erasure code scheme to support fast recovery
from up to two-disk failures in storage systems. In Proc. of
the IEEE/IFIP DSN’10, 2010.
[17] J. Hafner. HoVer erasure codes for disk arrays. In Proc. of
the IEEE/IFIP DSN’06, 2006.
[18] J. Hafner. WEAVER codes: Highly fault
tolerant erasure
codes for storage systems. In Proc. of the USENIX FAST’05,
2005.
[19] I. Reed and G.Solomon. Polynomial codes over certain ﬁ-
nite ﬁelds. Journal of the Society for Industrial and Applied
Mathematics, 1960.
[20] J. Blomer, M. Kalfane, R. Karp, M. Karpinski, M. Luby,
and D. Zucker-man. An XOR-based Erasure-Resilient coding
scheme. Technical Report TR-95-048, International Computer
Science Institute, 1995.
[21] S. Xu, R. Li, P. Lee, Y. Zhu, L. Xiang, Y. Xu. J. Lui. Sin-
gle Disk Failure Recovery for X-code-based Parallel Storage
Systems. IEEE Tranasaction on Computer, 2013.
[22] L. Xiang, Y. Xu, J. C. S. Lui, and Q. Chang. Optimal re-
covery of single disk failure in RDP code storage systems. In
Proc. of ACM SIGMETRICS’10, 2010.
[23] C. Huang, H. Simitci, Y. Xu, A. Ogus, B. Calder, P. Gopalan,
J. Li, and S. Yekhanin. Erasure Coding in Windows Azure
Storage. In Proc. of USENIX ATC’12, 2012.
[24] M. Sathiamoorthy, M. Asteris, D.S. Papailiopoulos, A.G.
Dimakis, R. Vadali, S. Chen, and D. Borthakur. XORing
Elephants: Novel Erasure Codes for Big Data. In Proc. of the
VLDB Endowment, 2013.
Random
Integer
Generator.
[25] RANDOM.ORG.
http://www.random.org/integers/, 2010.
Microsoft’s
Azure:
[26] Windows
www.windowsazure.com/
Cloud
Platform.
[27] O. Khan, R. Burns, J. Plank, and W. Pierce. Rethinking
erasure codes for cloud ﬁle systems: minimizing I/O for
recovery and degraded reads. In Proc. of USENIX FAST’12,
2012.
[28] J. Schindler, S. Schlosser, M. Shao, A. Ailamaki, and G.
Ganger. Atropos: A Disk Array Volume Manager for Orches-
trated Use of Disks. In Proc. of USENIX FAST’04, 2004.
[29] J. Plank, S. Simmerman, and C. Schuman. Jerasure: A library
in C/C++ facilitating erasure coding for storage applications-
Version 1.2. Technical Report CS-08-627, University of Ten-
nessee, 2008.
561
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:59:30 UTC from IEEE Xplore.  Restrictions apply.