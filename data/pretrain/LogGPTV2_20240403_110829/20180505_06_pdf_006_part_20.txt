or more user-specified or system-generated on the disk where datafiles will be created.
data files.
Data file 1. Can be explicitly created and resized by Behavior is more akin to Oracle Managed Files
the user. Oracle-Managed-Files (OMF) (OMF).
allows for automatically created data files.
1. Created automatically in the directory
2. Each data file can contain one or more assigned to the tablespace.
tables and/or indexes.
2. Single data file stores information for a
specific table or index. Multiple data files can
exist for a table or index.
Additional files are created:
1. Freespace map file
Exists in addition to the datafiles themselves.
The free space map is stored as a file named
with the filenode number plus the _fsm suffix.
2. Visibility Map File
Stored with the _vm suffix and used to track
which pages are known to have no dead tuples.
Creates a new CREATE TABLESPACE sales_tbs create tablespace sales_tbs
tablespace with DATAFILE SIZE 400M; LOCATION '/postgresql/data';
system-
managed
datafiles
Create a new CREATE TABLESPACE sales_tbs N/A
tablespace with DATAFILE
user-managed '/oradata/sales01.dbf' SIZE 1M
datafiles AUTOEXTEND ON NEXT 1M;
Alter the size of ALTER DATABASE DATAFILE N/A
a datafile '/oradata/sales01.dbf' RESIZE
100M;
244
Feature Oracle Aurora PostgreSQL
Add a datafile ALTER TABLESPACE sales_tbs ADD N/A
to an existing DATAFILE
tablespace '/oradata/sales02.dbf' SIZE
10M;
Per-database Supported as part of the Oracle 12c Multi- Tablespaces are shared across all databases but
tablespace Tenant architecture. Different dedicated a default tablespace can be created and
tablespaces can be created for different configured for the database:
pluggable databases and set as the default
tablespace for a PDB: create tablespace sales_tbs
LOCATION '/postgresql/data';
ALTER SESSION SET CONTAINER =
'sales'; CREATE DATABASE sales OWNER
sales_app TABLESPACE sales_tbs;
CREATE TABLESPACE sales_tbs
DATAFILE
'/oradata/sales01.dbf' SIZE 1M
AUTOEXTEND ON NEXT 1M;
ALTER DATABASE sales
TABLESPACE sales_tds;
Metadata Data Dictionary tables are stored in the System Catalog tables are stored in the
tables tablespace tablespace
SYSTEM pg_global
Tablespace Supported Supported
data encryption
1. Using transparent data encryption. 1. Encrypt using keys managed through KMS.
2. Encryption and decryption are handled 2. Encryption and decryption are handled
seamlessly so the user does not have to seamlessly so the user does not have to modify
modify the application to access the data. the application to access the data
3.Enable encryption while deploying a new
cluster via the AWS Management Console or
API actions.
For additional details:
http://docs.aws.amazon.com/AmazonRDS/late
st/UserGuide/Overview.Encryption.html
For additional details:
https://www.postgresql.org/docs/9.6/static/manage-ag-tablespaces.html
https://www.postgresql.org/docs/9.6/static/sql-createtablespace.html
https://www.postgresql.org/docs/9.6/static/storage-file-layout.html
https://www.postgresql.org/docs/9.6/static/storage-fsm.html
https://www.postgresql.org/docs/9.6/static/functions-info.html#FUNCTIONS-INFO-CATALOG-TABLE
https://www.postgresql.org/docs/9.6/static/sql-droptablespace.html
https://www.postgresql.org/docs/current/static/sql-altertablespace.html
245
Migrating from: Oracle Data Pump
[Back to TOC]
Overview
Oracle Data Pump is a utility for exporting/importing data from/to an Oracle database. Data Pump can be used
to copy an entire database, an entire schema(s) or specific objects in a schema. Oracle Data Pump is
commonly used as a part of backup strategy for restoring individual database objects (specific records, tables,
views, stored procedures, etc.) as opposed to snapshots or Oracle RMAN, which provides backup and recovery
capabilities at the database-level. By default, and without using the sqlfile parameter during export, the
“dumpfile” generated by Oracle Data Pump is a binary file and cannot be opened using a text editor.
Oracle Data Pump Supports:
1. Export of data from an Oracle database:
The Data Pump command creates a binary dump file containing the exported database objects.
EXPDP
Objects can be exported with data or, as an alternative, containing metadata only. Support for cross-object
consistent exports can be accomplished by requesting the export to be done according to a specific
timestamp or Oracle SCN.
2. Import data to an Oracle database:
The Data Pump command will import objects and data from specific dump file created with the
IMPDP
Data Pump command. The command can filter on import (only import certain objects) and
EXPDP IMPDP
remap object and schema names during import.
Notes:
• The term “Logical backup” refers to a dump file created by Oracle Data Pump.
• Both and can only read/write “dumpfiles” from filesystem paths that were pre-
EXPDP IMPDP
configured in the Oracle database as “directories”. During export/import, users will need to specify the
logical “directory” name where the dumpfile should be created, and not the actual filesystem path.
Examples
Export - using to export the Oracle HR schema:
EXPDP
$ expdp system/**** directory=expdp_dir schemas=hr dumpfile=hr.dmp
logfile=hr.log
* The command contains the credentials to run Data Pump, logical Oracle directory name for the dump file
location (which maps in the database to a physical filesystem location), schema name to export and dump file
and log files names.
246
Import - Using the to import the HR schema and rename to HR_COPY:
IMPDP
$ impdp system/**** directory=expdp_dir schemas=hr dumpfile=hr.dmp
logfile=hr.log REMAP_SCHEMA=hr:hr_copy
* The command contains the database credentials to run Data Pump, logical Oracle directory for where the
export dumpfile is located, dump file name, schema to export, name for the dump file and log file name and
the REMAP_SCHEMA parameter.
For additional details:
https://docs.oracle.com/cloud/latest/db112/SUTIL/part_dp.htm
https://docs.oracle.com/database/121/SUTIL/GUID-501A9908-BCC5-434C-8853-9A6096766B5A.htm
Migration to: PostgreSQL pg_dump & pg_restore
[Back to TOC]
Overview
PostgreSQL provides native utilities - and can be used to perform logical database
pg_dump pg_restore
exports and imports with a degree of comparable functionality to the Oracle Data Pump utility. Such as for
moving data between two databases and creating logical database backups.
• equivalent to Oracle
pg_dump expdp
• equivalent to Oracle
pg_restore impdp
Amazon Aurora PostgreSQL supports data export and import using both and , but
pg_dump pg_restore
the binaries for both utilities will need to be placed on your local workstation or on an Amazon EC2 server as
part of the PostgreSQL client binaries.
PostgreSQL dump files created using can be copied, after export, to an Amazon S3 bucket as cloud
pg_dump
backup storage or for maintaining the desired backup retention policy. Later, when dump files are needed for
database restore, the dump files should be copied back to the desktop/server that has a PostgreSQL client
(such as your workstation or an Amazon EC2 server) to issue the command.
pg_restore
Notes:
• will create consistent backups even if the database is being used concurrently.
pg_dump
• does not block other users accessing the database (readers or writers).
pg_dump
• only exports a single database, in order to backup global objects that are common to all
pg_dump
databases in a cluster, such as roles and tablespaces, use .
pg_dumpall
• Unlike Data Pump, PostgreSQL dump files are plain-text files.
247
Examples
1. Export data using :
pg_dump
Use a workstation or server with the PostgreSQL client installed in order to connect to the Aurora
PostgreSQL instance in AWS; providing the hostname (-h), database user name (-U) and database name (-
d) while issuing the command
pg_dump :
$ pg_dump -h hostname.rds.amazonaws.com -U username -d db_name
-f dump_file_name.sql
Note:
The output file, , will be stored on the server where the command executed. You
dump_file_name.sql pg_dump
can later copy the outfile file to an S3 Bucket, if needed.
2. Run and copy the backup file to an Amazon S3 bucket using pipe and the AWS CLI:
pg_dump
$ pg_dump -h hostname.rds.amazonaws.com -U username -d db_name
-f dump_file_name.sql | aws s3 cp - s3://pg-backup/pg_bck-$(date
"+%Y-%m-%d-%H-%M-%S")
3. Restore data - :
pg_restore
Use a workstation or server with the PostgreSQL client installed to connect to the Aurora PostgreSQL
instance providing the hostname (-h), database user name (-U), database name (-d) and the dump file to
restore from while issuing the command
pg_restore :
$ pg_restore -h hostname.rds.amazonaws.com -U username -d
dbname_restore dump_file_name.sql
4. Copy the output file from the local server to an Amazon S3 Bucket using the AWS CLI:
Upload the dump file to S3 bucket:
$ aws s3 cp /usr/Exports/hr.dmp s3://my-bucket/backup-$(date "+%Y-
%m-%d-%H-%M-%S")
* Note that the {-$(date "+%Y-%m-%d-%H-%M-%S")} format will work only on Linux servers.
Download the output file from S3 bucket:
$ aws s3 cp s3://my-bucket/backup-2017-09-10-01-10-10
/usr/Exports/hr.dmp
Note:
You can create a copy of an existing database without having to use or . Instead, use
pg_dump pg_restore
the keyword to signify the database used as the source:
template
psql> CREATE DATABASE mydb_copy TEPLATE mydb;
248
Oracle Data Pump vs. PostgreSQL pg_dump and pg_restore
Description Oracle data pump PostgreSQL SQL Dump
Export data to expdp system/**** schemas=hr pgdump -F c -h
a local file dumpfile=hr.dmp hostname.rds.amazonaws.com -U
logfile=hr.log username -d hr -p 5432 >
c:\Export\hr.dmp
Export data to • Create Oracle directory on Export-
a remote file remote storage mount or NFS pgdump -F c -h
directory called EXP_DIR hostname.rds.amazonaws.com -U
username -d hr -p 5432 >
c:\Export\hr.dmp
• Use export command:
expdp system/**** Upload to S3-
schemas=hr
aws s3 cp c:\Export\hr.dmp
directory=EXP_DIR
s3://my-bucket/backup-$(date
dumpfile=hr.dmp
"+%Y-%m-%d-%H-%M-%S")
logfile=hr.log
Import data to impdp system/**** schemas=hr pg_restore -h
a new dumpfile=hr.dmp hostname.rds.amazonaws.com -U
database with logfile=hr.log hr -d hr_restore -p 5432
a new name REMAP_SCHEMA=hr:hr_copy c:\Expor\hr.dmp
TRANSFORM=OID:N
For additional details:
https://www.postgresql.org/docs/current/static/backup-dump.html
https://www.postgresql.org/docs/9.6/static/app-pgrestore.html
249
Migrating from: Oracle Resource Manager
[Back to TOC]
Overview
Oracle’s Resource Manager enables enhanced management of multiple concurrent workloads running under a
single Oracle database. Using Oracle Resource Manager, you can partition server resources for different
workloads. Resource Manager helps with sharing server and database resources without causing excessive
resource contention and helps to eliminate scenarios involving inappropriate allocation of resources across
different database sessions.
Oracle Resource Manager enables you to:
• Guarantee a minimum amount of CPU cycles for certain sessions regardless of other running operations.
• Distribute available CPU by allocating percentages of CPU time to different session groups.
• Limit the degree of parallelism of any operation performed by members of a user group.
• Manage the order of parallel statements in the parallel statement queue.
• Limit the number of parallel execution servers that a user group can use.
• Create an active session pool. An active session pool consists of a specified maximum number of user
sessions allowed to be concurrently active within a user group.
• Monitor used database/server resources by dictionary views.
• Manage runaway sessions or calls and prevent them from overloading the database.
• Prevent the execution of operations that the optimizer estimates will run for a longer time than a specified
limit.
• Limit the amount of time that a session can be connected but idle, thus forcing inactive sessions to
disconnect and potentially freeing memory resources.
• Allow a database to use different resource plans, based on changing workload requirements
• Manage CPU allocation when there is more than one instance on a server in an Oracle Real Application
Cluster environment (also called instance caging).
Oracle Resource Manager introduces three concepts:
Consumer Group – A collection of sessions grouped together based on resource requirements. The Oracle
Resource Manager allocates server resources to resource consumer groups, not to the individual sessions.
Resource Plan – Specifies how the database allocates its resources to different Consumer Groups. You will
need to specify how the database allocates resources by activating a specific resource plan.
Resource Plan Directive – Associates a resource consumer group with a plan and specifies how resources are
to be allocated to that resource consumer group.
Notes:
• Only one Resource Plan can be active at any given time.
• Resource Directives control the resources allocated to a Consumer Group belong to a Resource Plan
• The Resource Plan can refer to Subplans to create even more complex Resource Plans.
250
Example
Creating a Simple Resource Plan
1. To enable the Oracle Resource Manager, you need to assign a plan name to the
parameter. Using an empty string will disable the Resource Manager.
RESOURCE_MANAGER_PLAN
ALTER SYSTEM SET RESOURCE_MANAGER_PLAN = 'mydb_plan';
ALTER SYSTEM SET RESOURCE_MANAGER_PLAN = '';
Example
We can also c reate complex Resource Plans. A complex Resource Plan is one that is not created with the
PL/SQL procedure and provides more flexibility and granularity.
CREATE_SIMPLE_PLAN
BEGIN
DBMS_RESOURCE_MANAGER.CREATE_PLAN_DIRECTIVE (
PLAN => 'DAYTIME',
GROUP_OR_SUBPLAN => 'OLTP',
COMMENT => 'OLTP group',
MGMT_P1 => 75);
END;
/
For additional details:
https://docs.oracle.com/database/121/ADMIN/dbrm.htm#ADMIN027
251
Migration to: Dedicated Amazon Aurora Clusters
[Back to TOC]
Overview
PostgreSQL does not have built-in resource management capabilities that are equivalent to the functionality
provided by Oracle Resource Manager. However, due to the elasticity and flexibility provided by “cloud
economics”, workarounds could be applicable and such capabilities might not be as of similar importance to
monolithic on-premises databases.
The Oracle Resource Manager primarily exists because traditionally, Oracle databases were installed on very
powerful monolithic servers that powered multiple applications simultaneously. The monolithic model made
the most sense in an environment where the licensing for the Oracle database was per-CPU and where Oracle
databases were deployed on physical hardware. In these scenarios, it made sense to consolidate as many
workloads as possible into few servers. In cloud databases, the strict requirement to maximize the usage of
each individual “server” is often not as important and a different approach can be employed:
Individual Amazon Aurora clusters can be deployed, with varying sizes, each dedicated to a specific application
or workload. Additional read-only Aurora Replica servers can be used to offload any reporting-style workloads
from the master instance.
The traditional Oracle model where maximizing the usage of each physical Oracle server was essential due to physical
hardware constraints and the per-CPU core licensing model.
252
With Amazon Aurora, separate and dedicated database clusters can be deployed, each dedicated to a specific
application/workload creating isolation between multiple connected sessions and applications.
Each Amazon Aurora instance (Primary/Replica) can scaled independently in terms of CPU and memory
resources using the different “instance types”. Because multiple Amazon Aurora Instances can be
instantly deployed and much less overhead is associated with the deployment and management of
Aurora instances when compared to physical servers, separating different workloads to different
instance classes could be a suitable solution for controlling resource management.
Instance Type vCPU Memory PIOPS- Network Performance
(GiB) Optimized
Standard
2 8 Yes Moderate
db.m4.large
4 16 Yes High
db.m4.xlarge
8 32 Yes High
db.m4.2xlarge
16 64 Yes High
db.m4.4xlarge
40 160 Yes 10 Gigabit
db.m4.10xlarge
1 3.75 - Moderate
db.m3.medium
2 7.5 - Moderate
db.m3.large
4 15 Yes High
db.m3.xlarge
8 30 Yes High
db.m3.2xlarge
Memory Optimized
2 15 - Moderate
db.r3.large
4 30.5 Yes Moderate
db.r3.xlarge
8 61 Yes High
db.r3.2xlarge
16 122 Yes High
db.r3.4xlarge
32 244 - 10 Gigabit
db.r3.8xlarge
Micro instances
1 1 - Low
db.t2.micro
1 2 - Low
db.t2.small
253
2 4 - Moderate
db.t2.medium
2 8 - Moderate
db.t2.large
In addition, each Amazon Aurora primary/replica instance can also be directly accessed from your
applications using its own endpoint. This capability is especially useful if you have multiple Aurora
read-replicas for a given cluster and you wish to utilize different Aurora replicas to segment your
workload.
Example
Suppose that you were using a single Oracle Database for multiple separate applications and used
Oracle Resource Manager to enforce a workload separation, allocating a specific amount of server
resources for each application. With Amazon Aurora, you might want to create multiple separate
databases for each individual application. Adding additional replica instances to an existing Amazon
Aurora cluster is easy.
1. In the AWS Management Console, select the Amazon RDS service and click the DB Instances link
from the Resources section of the RDS Dashboard window (highlighted).
254
2. Select the Amazon Aurora cluster that you want to scale-out by adding an additional read Replica.
3. Click on the Instance Actions button.
4. Select Create Aurora Replica.
5. Select the instance class depending on the amount of compute resources your application requires.
255
6. Once completed, click Create Aurora Replica.
Oracle Resource Manager vs. Dedicated Aurora PostgreSQL Instances
Oracle Resource Manager Amazon Aurora Instances
Set the maximum CPU usage Create a dedicated Aurora Instance for a specific application.
for a resource group
Limit the degree of SET max_parallel_workers_per_gather TO x;
parallelism for specific
queries Setting the PostgreSQL
max_parallel_workers_per_gather parameter should
be done as part of your application database connection.
Limit parallel execution SET max_parallel_workers_per_gather TO 0;
Limit the number of active Manually detect the number of connections that are open from a
sessions specific application and restrict connectivity either via database
procedures or within the application DAL itself.
select pid from pg_stat_activity where
usename in(
select usename from pg_stat_activity where
state = 'active' group by usename having
count(*) > 10) and state = 'active'
order by query_Start;
Restrict maximum runtime of Manually terminate sessions that exceed the required threshold.
queries You can detect the length of running queries using SQL
commands and restrict max execution duration using either
256
Oracle Resource Manager Amazon Aurora Instances
database procedures or within the application DAL itself.
SELECT pg_terminate_backend(pid)
FROM pg_stat_activity
WHERE now()-pg_stat_activity.query_start >
interval '5 minutes';
Limit the maximum idle time Manually terminate sessions that exceed the required threshold.
for sessions You can detect the length of your idle sessions using SQL queries
and restrict maximum execution using either database
procedures or within the application DAL itself.
SELECT pg_terminate_backend(pid)
FROM pg_stat_activity
WHERE datname = 'regress'
AND pid <> pg_backend_pid()
AND state = 'idle'
AND state_change < current_timestamp -
INTERVAL '5' MINUTE;
Limit the time that an idle Manually terminate sessions that exceed the required threshold.
session holding open locks You can detect the length of blocking idle sessions using SQL
can block other sessions queries and restrict max execution duration using either