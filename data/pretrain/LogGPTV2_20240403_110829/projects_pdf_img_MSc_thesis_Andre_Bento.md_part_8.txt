Uses [Zipkin]
Allows the system to ingest Allows the system to retrieve
System that holds tracing to be
tracing data and tracing data from local files or
analysed.
extract relevant metrics. from an external system.
Retrieves data from
Uses [JSONL/HTTPS]
Data Analyser
Graph Processor [Component:
[Component: NetworkX] Uses Uses NumPy/Pandas and
Scikit-learn]
Allows the system to handle
graph structures and perform Identify outliers presented in
graph algorithms. metrics extracted from tracing.
Uses Gather metrics
Graphs Repository Metrics Repository
Metrics Visualizer
[Component: ArangoDB Client] [Component: OpenTSDB Client]
[Browser (Grafana)]
Handles graph data Handles time metrics data
Allows the user to view metrics
retrieval and persistence retrieval and persistence
in a more human readable way.
operations. operations.
Reads from or writes to Reads from or writes to
[ArangoDB Connector] [OpenTSDB Connector - HTTP/TCP]
Provides metrics to
Database Database [HTTP/TCP]
[Container: Graph [Container: Time-Series
Database] Database]
Store graphs extracted from Store time metrics extracted
span trees presented in tracing. from span trees and graphs.
Subtitle:
Component Container Database Container External Software
System
Figure 4.5: Component diagram.
40
Chapter 5
Implementation Process
This Chapter presents the implementation process of the proposed solution explained
in previous Chapter. Three main sections are covered in this Chapter: Firstly, in Sec-
tion 5.1 - Huawei Tracing Data Set, the data provided to perform this research is pre-
sented and analysed. Secondly, in Section 5.2 - OpenTracing Processor Component, the
implementation of (OpenTracing processor (OTP)), our proposed solution to collect and
store metrics from tracing data is explained in detail with intermediate results. Finally,
in Section 5.3 - Data Analysis Component, the approach and methods for analysis of the
stored observations are presented.
5.1 Huawei Tracing Data Set
The starting point for this solution and every method developed within it was a data
set provided by Huawei, represented by professor Jorge Cardoso. To gain access to this
information, a NDA: Non-disclosure agreement was signed by both parts. This data set
contains the results of tracing data gathered from an experimental OpenStack cluster used
by the company for testing purposes, and covers two days of operation. Consequently, two
files were provided, one for each day. These files were generated in 10 of July, 2018 and,
for protection, some fields of the data set were obfuscated during the generation process.
Table 5.1 contains some details about the provided data set.
Table 5.1: Huawei tracing data set provided for this research.
File Date 2018-06-28 2018-06-29
Spans count 190 202 239 693
Traces count 64 394 74 331
From Table 5.1, we can see some detail regarding spans and trace counting for each
day. Both files were written in JSONL format [60]. This file format is an extension to
thelightweightdata-interchangestandardJavaScriptObjectNotation(JSON):JavaScript
Object Notation, however, in JSONL format multiple JSON are separated by a new line
character. Each span is presented by a single JSON, therefore, each line contains a span
encoded in JSON format. To count spans a line count in each file was enough. To count
traces, spans must be mapped to span trees, and then the total of trees represent the trace
count. Algorithms to perform this conversion are presented further, in Section 5.2 - Open-
Tracing Processor Component.
41
Chapter 5
Span data format is defined in an open source specification called OpenTracing [61],
however, companies and software developers are not obliged to follow it, thus they can
produce their own span data format, leading to difficulties developing a general purpose
tool for tracing analysis. Therefore, to ease the interpretation of spans presented in the
data set, a file with instructions about the specification was provided. In this file, a
definition was given about possible fields and their corresponding data types. A sample
of the fields and their descriptions are exposed in Table 5.2.
Table 5.2: Span structure definition.
Field Description
traceId Unique id of a trace (128-bit string).
name Human-readable title of the instrumented function.
timestamp UNIX epoch in milliseconds.
id Unique id of the span (64-bit string).
parentId Reference to id of parent span.
duration Span duration in microseconds.
binaryAnnotations protocol - “HTTP” or “function” for RPC calls;
http.url - HTTP endpoint;
http.status_code - Result of the HTTP operation.
annotations value - Describes the position in trace (based on Zipkin
format). Could be one of the following values or other:
“cs” (client send), “cr” (client receive), “ss” (server send)
or “sr” (server receive);
timestamp - UNIX epoch in microseconds;
endpoint - Which endpoint generated a trace event.
Also, the file contained two notes. To point each one, has they are very important, we
present them bellow.
1. Time units are not consistent, some fields are in milliseconds and some are in mi-
croseconds.
2. Trace spans may contain more or less fields, except those mentioned here.
From Table 5.2, we get a notion about the fields that can be found in spans. These
fields are defined by OpenTracing specification, therefore, is important that companies
follows the specification, even if open source.
In this data set, spans are composed by: “traceId”, “name”, “timestamp”, “id”, “par-
entId” and “duration”. These are the main required fields, because they represent the
foundations for tracing data, containing the identification, relation and temporal track of
the span. Also, these fields are fixed, meaning that they are always represented by the
defined field name. The same can not be said from the remaining fields: “binaryAnno-
tations” and “annotations”. These tow fields are always identified by these field names,
however, their values are maps and therefore, have values stored in key - value pairs. This
brings some consistency problems and we might not know clearly what is available in
a span, when working with it. As said in the second point presented in the list above:
“Trace spans may contain more or less fields, except those mentioned here”, and for this
reason, there is a tremendous explosion in possibilities, because there might be keys with
corresponding values for some particular spans and it gets hard to generalise this in a
uniform span structure.
42
Implementation Process
The notion of span data only depends on the quality of communication and documen-
tation of the ones that produce tracing. To be certain that one crafts good tracing data,
there must be an implemented standard for everyone to follow. The formalization and
unification of one tracing specification should be a thing to consider, for the reason that it
is an endeavour to analyse inconstant fields. For example, in the data provided spans can
be of two types: HTTP span or RPC span, and the only field that distinguishes them is a
field named “exec”, which stands for the execution process id, and is not presented in the
HTTP span type. Another example, fields having the same key should have one and only
onemeasurementunit,becausedistributedtracingtools(liketheonespresentedinSubsec-
tion 2.2.1) are not expecting different measurement units for the same field and therefore,
assume wrong values when spans have timestamps declared in milliseconds and others in
microseconds, like in this case. To fix this, we decided to convert all time measurements
to milliseconds.
To provide notion of how traces and spans are spread throughout time, we have used
our tool, Graphy OTP, to generate two charts that represents the counting of traces and
spans for each hour in each day. We decided to generate two split charts due to the simple
fact that we have one file for each day. To count the number of spans in time, in this case
by hour, the tool only needed to group every span by hour and count them, however for
traces, the tool has more work because it needs to merge all spans in their corresponding
span tree (explained in Section 5.2). After having all span trees it just needs to count
them, and the result is the number of traces. Note that if a span or trace starts at a given
timet1containedinatime-frame,andwithitsdurationd1surpassingthenexttime-frame
tf1, (t1+d1 > tf1), it is considered to be in the first time-frame, or by other words, only
the starting time of the trace or span is considered for the counting. Figures 5.1 and 5.2
presents the data set traces and spans counting throughout time.
Figure 5.1: Trace file count for 2018-06-28.
43
Chapter 5
Figure 5.2: Trace file count for 2018-06-29.
Figure 5.1 presents the counting of traces and spans for the 28th of June, 2018. In this
Figure we can spot a “pit” in quantity from 2AM to 10AM. No explanation for this was
given, however, at this point we assumed that extracting metrics from data in this time
intervalwouldproducelesspoints, thuslessresolution. Thisisvisibleinmetricspresented
in Figure 5.3, reproduced using Grafana. The quantity of data for the rest of the day is
somehow inconstant, however, there is no lack of data like in the previous day.
Figure 5.2 presents the counting of traces and spans for the 28th of June, 2018. In this
Figure there are no “pits”, and consequently, the quantity of information is more constant
throughout time.
Tosummarise,thissystemproducesanaverageof5000tracesanhourand15000spans
an hour. Also, the quantity of information provided in the second day (29th of June) is
more constant, and therefore, better for analysis, than in the first day (28th of June).
Nevertheless, this data set has sufficient information to study tracing data and develop
methods for tracing data, and then, it is a suitable data set for this research project.
Next Chapter, 5.2 - OpenTracing Processor Component, covers the explanation and
algorithmsusedoverthisdatasetfortracingmetricsextractionandqualityanalysis. Also,
some visualizations of metrics extracted from tracing data are provided.
44
Implementation Process
5.2 OpenTracing Processor Component
In this Chapter, the implementation for the first component of the proposed solution,
OTP, is presented and explained, hence, functional requirements defined in Table 4.1, ser-
vicedependencygraphhandling,spantreesgenerationandmethodsformetricsextraction
and storing from tracing data will be covered.
Starting by the first two functional requirements (FR-1 to FR-2). These require com-
munication with distributed tracing tools, to obtain tracing data and to retrieve service
dependency graphs. We have decided to use Zipkin, as a distributed tracing tool for
holding our data set, instead of Jaeger only due to simplicity in setup configuration. To
setup this tool a Docker container was instantiated in an external server. Communica-
tion methods are implemented in Tracing Collector component. To feed information to
our solution, one can use two ways: collect tracing data from local files, or export them
to Zipkin and ingest it through HTTP requests. This configurations can be changed by
editing a configuration file provided with the solution. The configurations to edit are file
locations in local machine and Zipkin IP (Internet Protocol) address.
After collecting information from one of the two defined sources by Tracing Collector,
data is passed to Tracing Processor which ingests and maps all the information into in
memorydatastructures. Datacanbeeithertracedataorservicedependencygraphs. Ifit
is a graph, it is transferred to Graph Processor for process, graph metrics extraction and
later storage, otherwise, it is processed in Tracing Processor to extract defined metrics
from tracing. The algorithm for metrics extraction from tracing and service dependency
graphs is presented at a high abstraction level in Algorithm 1.
Algorithm 1: Algorithm for metrics extraction from tracing.
Data: Trace files/Trace data.
Result: Trace metrics written in the time-series database.
1 Connect to Time-Series database;
2 Read time_resolution, start_time and end_time from configuration;
3 Read traces from trace files/trace data;
4 Post traces to Zipkin;
5 Get services from Zipkin;
6 Calculate time_intervals using start_time, end_time and time_resolution;
7 while time_interval in time_intervals do
8 Get service_dependencies from Zipkin;
9 Build service_dependency_graph using service_dependencies;
10 Extract graph_metrics from service_dependency_graph;
11 while service in services do
12 Get traces from Zipkin;
13 Map traces in SpanTrees;
14 Extract service_metrics from SpanTrees;
15 Post graph_metrics to Time-Series database;
16 Post service_metrics to Time-Series database;
Algorithm 1 contains some core functionalities implemented in components presented
inOTPsolution. Thisalgorithmaimsformetricsextractionfromtracingdataandperform
thisprocedureusingtwomaindatastructures: servicedependencygraphsandtracingdata
mapped into SpanTrees.
45
Chapter 5
Service dependency graphs are obtained from Zipkin and parsed directly into a Net-
workX graph structure, presented in component Graph Processor. We decided to chose
NetworkX, a framework for graph processing written in Python, due to tooling versatility
has it contains a large implementation set of the majority graph algorithms. At this point
we preferred this trade-off over processing power and scalability. Zipkin provides service
dependency graphs through an explicit endpoint – /dependencies, and a start and end
timestamps in epoch milliseconds must be passed as parameters. The information comes
in JSON format as presented in Listing 5.1.
1 [
2 {
3 ”parent”: ”string”,
4 ”child”: ”string”,
5 ”callCount”: 0,
6 ”errorCount”: 0
7 },
8 { /∗ ... ∗/ }
9 ]
Listing 5.1: Zipkin dependencies result schema.
Listing 5.1 shows that dependencies come in an array of JSON objects. Each object
contains the information about one relationship between services: parent “from”, child
“to” and the number of calls. Therefore, having this information grant the creation of
service dependency graph using NetworkX. Note that this information assembles a graph
containingtheinformationofsystemservicesataspecifictimeintervaldefinedbyprovided
parameters to Zipkin /dependencies endpoint. After having this information mapped
into NetworkX graphs in memory, their visual representation are identical to the one
demonstrated in Figure 2.5, presented in Subsection 2.1.4.
SpanTrees are a representation of a trace in a tree format. Method for their creation
from a span list is presented in Algorithm 2.
Algorithm 2: Algorithm for SpanTree mapping from spans.
Data: Span list.
Result: Spans mapped into SpanTrees.
1 Index spans by ids from span list into SpanIndex;
2 while span in span list do
3 Read parentId from span;
4 Index span using parentId into SpanIndex;
Algorithm 2 shows that to transform a list of spans (unordered traces) into SpanTrees,