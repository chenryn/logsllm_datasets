title:A Tale of Two Topologies: Exploring Convertible Data Center Network
Architectures with Flat-tree
author:Yiting Xia and
Xiaoye Steven Sun and
Simbarashe Dzinamarira and
Dingming Wu and
Xin Sunny Huang and
T. S. Eugene Ng
A Tale of Two Topologies: Exploring Convertible
Data Center Network Architectures with Flat-tree
Yiting Xia, Xiaoye Steven Sun, Simbarashe Dzinamarira,
Dingming Wu, Xin Sunny Huang, T. S. Eugene Ng
Rice University
ABSTRACT
This paper promotes convertible data center network archi-
tectures, which can dynamically change the network topology
to combine the benefits of multiple architectures. We propose
the flat-tree prototype architecture as the first step to realize
this concept. Flat-tree can be implemented as a Clos network
and later be converted to approximate random graphs of
different sizes, thus achieving both Clos-like implementation
simplicity and random-graph-like transmission performance.
We present the detailed design for the network architecture
and the control system. Simulations using real data center
traffic traces show that flat-tree is able to optimize various
workloads with different topology options. We implement an
example flat-tree network on a 20-switch 24-server testbed.
The traffic reaches the maximal throughput in 2.5s after a
topology change, proving the feasibility of converting topol-
ogy at run time. The network core bandwidth is increased by
27.6% just by converting the topology from Clos to approx-
imate random graph. This improvement can be translated
into acceleration of applications as we observe reduced com-
munication time in Spark and Hadoop jobs.
CCS CONCEPTS
• Networks → Network architectures; Physical topolo-
gies; Data center networks;
KEYWORDS
Convertible data center networks; Clos networks; Random
graph networks
ACM Reference format:
Yiting Xia, Xiaoye Steven Sun, Simbarashe Dzinamarira, Ding-
ming Wu, Xin Sunny Huang, T. S. Eugene Ng. 2017. A Tale of
Two Topologies: Exploring Convertible Data Center Network Ar-
chitectures with Flat-tree. In Proceedings of SIGCOMM ’17, Los
Angeles, CA, USA, August 21-25, 2017, 14 pages.
https://doi.org/10.1145/3098822.3098837
Permission to make digital or hard copies of all or part of this work
for personal or classroom use is granted without fee provided that
copies are not made or distributed for profit or commercial advantage
and that copies bear this notice and the full citation on the first page.
Copyrights for components of this work owned by others than the au-
thor(s) must be honored. Abstracting with credit is permitted. To copy
otherwise, or republish, to post on servers or to redistribute to lists,
requires prior specific permission and/or a fee. Request permissions
from permissions@acm.org.
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
© 2017 Copyright held by the owner/author(s). Publication rights
licensed to Association for Computing Machinery.
ACM ISBN 978-1-4503-4653-5/17/08. . . $15.00
https://doi.org/10.1145/3098822.3098837
1 INTRODUCTION
In this paper, we appeal for rethinking the design of data
center network architectures by introducing the concept of
convertibility. Convertibility is a network’s ability to change
between multiple topologies with different characteristics. This
change should be completely managed by software, without
involving human labor for rewiring the physical devices. With
the power of convertibility, it is possible for the first time
to build a data center that can function with different net-
work architectures to combine the benefits of conventionally
incompatible worlds. Our proposal is rooted in the recent
trends in the development of data center networks.
The first trend is the continuous efforts towards two mu-
tually exclusive goals for data center network design: easy
implementation vs. good performance. These efforts are re-
flected in the enthusiasm for Clos networks in industry and
random graph networks in academia. Clos, or multi-rooted
tree, is the de-facto standard data center network architecture
because of easy implementation [38, 39]. Figure 2b shows an
example Clos network. The central wiring between switches
in adjacent layers is relatively easy to manage, and the net-
work can be expanded to arbitrary size by adding stages.
Bandwidth oversubscription can occur at any switch layer
to save cost. Modular Pods are adopted as building blocks
to further ease network deployment and management. How-
ever, Clos networks have suboptimal throughput, as traffic
needs to traverse up and down the network hierarchy and
the resulting inefficiency exacerbates oversubscription.
In contrast, random graphs are proven to have optimal
throughput for uniform traffic [40, 41]. Without rigid struc-
tures, switches are more directly connected at shorter path
lengths. If implemented using the same switches and servers
as a Clos network, a random graph can provide richer band-
width and effectively alleviate the oversubscription problem.
This lack of structure also enables regional random graphs
to be constructed, i.e. a set of smaller local random graph
networks interconnected by random wiring into a large global
network. However, the neighbor-to-neighbor wiring between
random switch pairs is complicated, making real-world im-
plementation a daunting task.
Closely related to these conflicting stances is the second
trend of stagnation in the emergence of new data center
network architectures. Back in 2008 and 2009, the research
community proposed a number of interconnection networks
as the data center fabric, fat-tree [12], DCell [24], BCube [23],
and HyperX [11] being the famous examples. However, there
has been no breakthrough ever since. In the design space,
these architectures fall between Clos and random graph at the
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
Y. Xia et al.
extremes of the scale. They attempt to find the right middle
ground between easy implementation and good performance
by tuning the degree of hierarchical vs. flat structure, central
vs. neighbor-to-neighbor wiring, etc. Yet, the performance of a
network depends on the traffic pattern; each topology has the
sweet spot for particular workloads [40]. Measurement studies
of data center traffic show that data center services result in
very different traffic locality [30, 38] and that cluster sizes in
multi-tenant clouds vary significantly [13–15]. It is hard to
use a one-size-fit-all topology to address the heterogeneous
and ever-changing service needs in data centers.
Relaxing the constraint of fixed topologies, the third trend
is the advent of configurable data center networks that create
ad-hoc links as demanded by the traffic pattern. Some solu-
tions provide a local remedy for fixed topologies by adding
a small number of connections to alleviate hot spots [19, 22,
25, 26, 43, 51], while others create a flexible network core
for small-scale networks [3, 16, 17, 33, 34]. On one hand,
these works demonstrate it is technically mature to change
the network topology by software at run time. On the other
hand, the scalability limitation remains to be addressed.
Based on the above evidence, we make the bold claim that
it is time to build convertible data center network architec-
tures. The concept of convertible network is fundamentally
different from existing proposals with link flexibility. First, it
aims to achieve network-wide topology change in large-scale
data centers. The scalability of many previous works is con-
strained by a centralized device that enables flexibility, such
as 3D MEMS [16, 19, 43, 44, 48] and WDM ring [3, 33, 34].
To overcome this weakness, in our proposal the enabling de-
vices are placed across the network in a decentralized manner.
Second, instead of adding extra bandwidth to the network,
a convertible network rearranges the network structure to
utilize existing bandwidth resources more efficiently. Third,
rather than incremental topology evolution according to the
instantaneous traffic pattern, a convertible network changes
the intrinsic characteristics of the topology to fit the require-
ments of different workloads throughout their lifecycle.
We experiment with this concept by designing and im-
plementing the flat-tree1 prototype architecture, which can
convert between a Clos topology and random graphs at dif-
ferent scales. Clos has rich intra-rack bandwidth and thus is
suitable for traffic with strong rack-level locality. Random
graph is a perfect match for network-wide uniform traffic,
but it may have suboptimal performance for skewed traffic
or small cloud tenant clusters. Therefore, regional random
graph and global random graph should be used to adapt to
different cluster sizes. The examples in Section 2.1 show the
advantage of each topology given different traffic patterns.
Moreover, such a design has the potential to preserve easy
implementation from the Clos network, making practical
deployment of random graphs achievable.
1The name “flat-tree” captures the dual nature of the proposed architec-
ture. It can function as approximate random graphs (“flat” networks)
and Clos (multi-rooted “tree”). It is as easy to implement as a “tree”
network and has good performance as “flat” networks.
Flat-tree leverages inexpensive small port-count converter
switches to convert topologies dynamically. By changing the
configurations of the converter switches, cables are rewired to
different outgoing connections, as if they were unplugged and
replugged manually. Flat-tree takes a pragmatic approach
to start from a Clos network and addresses challenges of
flattening the tree structure to approximate random graphs.
Specifically, how to equalize switches in different layers and
relocate servers from edge to aggregation and core switches?
How to break the hierarchy and connect the network core and
edge directly? How to enable connections between switches
in the same layer at minimum wiring complexity?
Flat-tree inherits the merits of packaging and wiring from
Clos networks. It adopts the modular Pod design. Additional
hardware and wiring are packaged in Pods, leaving the same
external connectors as a Clos counterpart. Pods are connected
to core switches with a customized regular wiring pattern.
Adjacent Pods are interconnected through multi-link side
connectors to allow simple neighbor-wise wiring.
Flat-tree can approximate random graphs at different
scales, ranging from a Pod, to a subnetwork comprising
multiple Pods, to the entire network. It can also function
as Clos, which benefits applications that require rich equal-
cost redundant links, predictable path length, and rack-level
locality. Flat-tree can operate in hybrid mode: the network
is organized into functionally separate zones each having a
different topology. Workloads are placed into suitable zones
to optimize their performance. As the workloads change, the
network can be reorganized to adapt to the new requirements.
We discuss design options for the control plane and present
the implementation details given the current technology. To
exploit the link diversity in flat-tree, we adopt 𝑘-shortest-
path routing [50] and MPTCP [45], whose deployment in
large-scale data centers is an open challenge. The enormous
number of paths lead to explosion of network states. We pro-
pose an architecture-specific addressing scheme to aggregate
IP addresses and use SDN-based source routing to relieve
state-keeping at the switches. Packet-level simulations show
that given various traffic patterns on flat-tree networks of
different scales, the pragmatic implementation of 𝑘-shortest-
path routing and MPTCP achieves comparable throughput
to optimal routing from linear programming.
To further evaluate the practical performance of flat-tree,
we run packet-level simulations given real traffic traces from
several production data centers each carrying different ser-
vices. The results show that flat-tree is able to optimize for
diverse workloads with different topology options. We imple-
ment a flat-tree prototype on a 20-switch 24-server testbed
and run Spark and Hadoop applications with different topolo-
gies. The traffic reaches the maximal throughput only 2.5s
after a topology change, proving the feasibility of converting
the topology at run time. The network core bandwidth is
increased by 27.6% just by converting the topology from
Clos to approximate random graph. This improvement can
be translated into acceleration of applications as we observe
reduced communication time in Spark and Hadoop jobs.
A Tale of Two Topologies
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
Table 1: Throughput of clustered traffic normalized against the
minimum value in the compared architectures
Cluster Size
Fat-tree Random Graph Two-stage Random Graph
8
30
100
1.91
1
1
1
1.38
1.59
1.16
1.65
1.17
2 MOTIVATING EXAMPLES
2.1 The Case for Convertibility
Two reasons contribute to the diversity of data center work-
loads. First, enterprise data centers may deploy different
services that have different traffic characteristics [30, 38]. For
instance, the Facebook data centers with different services
show different locality features. The Hadoop site has rack-
level locality, while the web and cache sites have Pod-level
locality [38]. Second, in public clouds, the virtual tenants have
different sizes and traffic patterns [13–15]. For example, in a
Microsoft data center, the mean tenant size is 79 VMs and