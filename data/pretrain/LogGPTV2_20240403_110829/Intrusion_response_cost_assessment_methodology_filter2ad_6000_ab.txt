Policy Category Weight
SRimportanceN etworkAccess
Data conﬁdentiality
Data availability
Data integrity
0
1.0
0.7
WNetworkAccess = 0 × 0.1 + 1.0 × 1.0 + 0.7 × 0.1 = 1.07
0.1
1.0
0.1
Step 4: Taxonomy of responses.
Once the system goals are identiﬁed, its resources are enu-
merated and their importance is quantiﬁed based on the sys-
tem goals, the next step is to identify the set of responses
that are suitable for a system. Generally, the responses are
deployed to either counter possible attacks and defend the
system resources or regain secure system state. Thus, the
selection of applicable responses primarily depends on the
identiﬁed system resources.
Step 5: Assessment of response operational cost.
The assessment of response operational cost is generally
independent from the system policy and includes the cost
for the setup and deployment of the response, and data pro-
cessing overhead needed to analyze the result of response.
For example, “the system logging” response is fairly easy to
setup. However, it requires signiﬁcant storage resources and
often incurs high processing overhead. Broadly, the involved
operational expenses can be classiﬁed on the basis of three
requirements: human resources which refer to administra-
tor time, system resources, which include storage, network
bandwidth, processor time, etc., and direct expenses which
include data processing fees by a third party, subscription
service fees, cost of components replacement, etc. Determin-
ing these factors is a manual process that involves expert
knowledge and a high degree of judgment.
Step 6: Assessment of response goodness.
The evaluation of the response goodness includes a) Selec-
tion of the applicable responses for intrusions and b) Com-
putation of the response goodness measure in terms of known
attacks
Often the detection mechanism of the intrusion detection
system (IDS) provides administrator with a set of alerts in-
dicating potential attacks rather than a speciﬁc intrusion.
When this situation arises, the response needs to be deployed
preemptively on the basis of high likelihood of possible intru-
sions. In these cases, the response is evaluated based on the
number of possible intrusions it can potentially address, and
consequently, the number of resources that can be protected
by the response. In practice, the applicability of responses
to potential attacks can be determined through the analysis
of the existing intrusion signatures in the IDS.
The assessment of response goodness includes a review of
the availability of the system resources involved in the in-
trusion. For example, an alert triggered on TFTP traﬃc on
port 69 is accounted for in the response goodness assessment
only if TFTP protocol is currently supported.
The goodness of the response Ri where i ∈ [1 . . . m] (m
diﬀerent responses) against the intrusion Ij potentially af-
fecting n system resources SRj
n is computed as
2, . . . SRj
1, SRj
follows:
RGRi (Ij) =
(cid:88)
k∈[1...n]
Avail(SRj
k) × WSRk
(3)
where Avail(SRj
k) is a binary value that denotes the avail-
ability of k-th system resource that can be aﬀected by Ij and
WSRk is the resource weight (as computed by Equation 2).
To ensure the consistency of the computed metric, RG values
are normalized within a range of [0, 1] by dividing individual
RGRi (Ij) by the normalization term, MAX(RG(Ij)) which is the
maximum RG value computed for available responses for the
intrusion Ij, i.e.,
MAX(RG(Ij)) = RGRl (Ij) such that
l ∈ [1 . . . m] ∧ ∀i ∈ [1 . . . m] : RGRi (Ij) ≤ RGRl (Ij)
In the rest of the paper, we will refer to RGRi (Ij) to mean its
normalized valuation.
Step 6: Assessment of the response impact on the
system.
The impact of a response is evaluated based on the deﬁned
system goals and their importance. The impact assessment
process for a speciﬁc response includes three steps. First,
identify the system resources aﬀected by each response. Sec-
ond, for each resource, order the responses on the basis of
how they are aﬀecting the resource. Finally, compute the
negative impact of the responses on the associated resource
using the ordering obtained above. Eventually, the impact
of a response on the system as a whole will be an aggrega-
tion of the response’s impact on the resources present in the
system.
For each response we determine the system resources it
may aﬀect. For instance, blocking a speciﬁc subnet can pro-
tect the network interface resource and also disrupt legit-
imate user activities. After all responses are categorized
within the considered system resource, we independently
evaluate each system resource. Speciﬁcally, all responses
aﬀecting the resource are ordered or ranked based on their
relative impact on the considered resource, from the great-
est impact to the least impact, and assigned an index i ∈
[0 . . . (m − 1)], where m is the total number of responses in
the list corresponding to a particular resource. A response
with rank i has more impact on the corresponding resource
than the response with rank j (i < j). These ranks are
based on historical data and/or the expertise of the system
administrator. We quantify the impact using the rank as
follows:
ImpactRi,SR = 1 − i
m
(4)
where Ri is the i-th ranked response. The resultant valu-
ation is between 1
m and 1. To illustrate this process, lets
consider the example of the network interface resource. The
available responses are ranked according to their impact and
the corresponding impact quantiﬁcation is computed as fol-
lows:
Rank i Responses for SR (Ri)
0. Complete network isolation
1. Network isolation: block subnet
2. Terminate process
3. Delay suspicious process
4. Deploy intrusion analysis tools
ImpactRi ,SR
1 - 0/5 = 1.0
0.8
0.6
0.4
0.2
390Our approach can be employed by system administrators to
guide them through the response selection process
4. CONCLUSION AND FUTURE WORK
In this paper we have presented a comprehensive and
structured methodology for evaluation of response cost. The
proposed model identiﬁes three main components that con-
stitute response cost, namely, response operational cost, the
response goodness in mitigating the damage incurred by the
detected intrusion(s) and the response impact on the system.
These response metrics provide a consistent basis for evalu-
ation across systems, while allowing the response cost to be
adapted with respect to the security policy and properties
of speciﬁc system environment. This approach takes advan-
tage of the accuracy inherent in expert assignment of val-
ues, and combines it with a structured calculation of relative
values, resulting in ﬂexibility and consistency. Importantly,
this approach is practically implementable in a real-world
environment, making response cost assessment accessible to
system administrators with a range of system expertise.
5. REFERENCES
[1] I. Balepin, S. Maltsev, J. Rowe, and K. Levitt. Using
speciﬁcation-based intrusion detection for automated
response. In Proceedings of RAID, 2003.
[2] B. Foo, Y.-S. Wu, Y.-C. Mao, S. Bagchi, and E. H.
Spaﬀord. ADEPTS: Adaptive intrusion response using
attack graphs in an e-commerce environment. In
Proceedings of DSN, pages 508–517, 2005.
[3] W. Lee, W. Fan, M. Miller, S. J. Stolfo, and E. Zadok.
Toward cost-sensitive modeling for intrusion detection
and response. J. Comput. Secur., 10(1-2):5–22, 2002.
[4] N. Stakhanova, S. Basu, and J. Wong. A cost-sensitive
model for preemptive intrusion response systems. In
Proceedings of AINA, pages 428–435, Washington, DC,
USA, 2007. IEEE Computer Society.
[5] C. Strasburg, N. Stakhanova, S. Basu, and J. Wong.
The methodology for evaluating response cost for
intrusion response systems. Technical Report 08-12,
Iowa State University, 2008.
[6] Y.-S. Wu, B. Foo, Y.-C. Mao, S. Bagchi, and
E. Spaﬀord. Automated adaptive intrusion containment
in systems of interacting services. In To appear in
Journal of Computer Networks, 2007.
Generally, the values determined as a result of ranking are
dependent on the characteristics of system environment. As
such, changes in the environment, i.e., modiﬁcations in the
software usage, addition of network equipment, new knowl-
edge or skills gained by the administrator, etc., can aﬀect
the order and relative severity of the responses. Thus, as
the settings of the environment change, these values may be
manually adjusted to more accurately reﬂect relative dam-
age on the system resources.
The overall impact of the response measure is estimated
based on the weight of the system resource for a speciﬁc
system policy (Equation 2) and the impact value of the re-
sponse for that resource (Equation 4). The overall rating of
the response Ri on the system, the response system impact,
denoted by RSIRi , is computed as follows:
(cid:88)
RSIRi =
ImpactRi,SR × WSR
(5)
SR
Similar to RG valuations (Equation 3), we normalize RSIRi
using the maximum valuation of RSI for any response.
While manual assignment of some values is inevitable,
these abstractions allow an expert to focus separately on
the technical nature of the responses and the high-level goals
of the system. In many cases, two diﬀerent individuals or
groups are uniquely qualiﬁed to make the respective techni-
cal and policy based decisions. As such if the environment
changes, the system administrator can modify the high-level
system goals while a technical specialist adjusts response
damage factors based on changes to the system or network
environment. Such separation of concern reduces the deci-
sion complexity, and therefore, the risk of human error.
3. PRACTICAL EXERCISE
To evaluate the practical value of our approach, we con-
ducted an experiment where we asked system administrators
to rank the set of response actions using their traditional
methods according to responses’ priority to be deployed on
the system in the case of an SQL injection attack. In the ex-
periment we oﬀered four types of system: public web server,
classiﬁed research system, medical data repository and re-
ceptionist workstation.We recruited 9 system administrators
with diﬀerent level of expertise (5 experts and 4 with mod-
erate level of expertise). The motivation for the experiment
was to evaluate the consistency of the response cost assess-
ment using our methodology in comparison with the tradi-
tional approach primarily based on the manual selection of
responses according to the administrator expertise.
Surprisingly, the results showed a substantial variability in
the response ranking among administrators. The rank order
correlation coeﬃcients between any two rankings are in the
range of {−0.74, 0.15}. This means that ranking is not con-
sistent either among experts, nor among administrators with
moderate expertise level, and consequently, varies from the
ranking determined by our approach. As one of the respon-
ders noted, the response ranking provided by our method
characterized a smooth process for system administrators to
follow during an attack, while his personal response prefer-
ence is an overreaction to the situation.
This provides strong testimony that even experienced ad-
ministrators need a standardized metric for evaluating in-
trusion responses that would allow to assess the costs in-
volved in each response deployment in a consistent manner.
391