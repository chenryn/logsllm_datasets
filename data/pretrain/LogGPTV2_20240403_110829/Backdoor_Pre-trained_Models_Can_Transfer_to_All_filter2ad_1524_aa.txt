title:Backdoor Pre-trained Models Can Transfer to All
author:Lujia Shen and
Shouling Ji and
Xuhong Zhang and
Jinfeng Li and
Jing Chen and
Jie Shi and
Chengfang Fang and
Jianwei Yin and
Ting Wang
Backdoor Pre-trained Models Can Transfer to All
Xuhong Zhang∗
Lujia Shen
Zhejiang University
Zhejiang University
Shouling Ji∗
Binjiang Institute of Zhejiang
Binjiang Institute of Zhejiang
Zhejiang University
PI:EMAIL
Jinfeng Li
Zhejiang University
PI:EMAIL
University
PI:EMAIL
Jing Chen
Wuhan University
PI:EMAIL
University
PI:EMAIL
Jie Shi
Huawei International, Singapore
PI:EMAIL
Chengfang Fang
Huawei International, Singapore
PI:EMAIL
Jianwei Yin
Zhejiang University
PI:EMAIL
Ting Wang
Pennsylvania State University
PI:EMAIL
ABSTRACT
Pre-trained general-purpose language models have been a dominat-
ing component in enabling real-world natural language processing
(NLP) applications. However, a pre-trained model with backdoor
can be a severe threat to the applications. Most existing backdoor at-
tacks in NLP are conducted in the fine-tuning phase by introducing
malicious triggers in the targeted class, thus relying greatly on the
prior knowledge of the fine-tuning task. In this paper, we propose
a new approach to map the inputs containing triggers directly to a
predefined output representation of the pre-trained NLP models,
e.g., a predefined output representation for the classification token
in BERT, instead of a target label. It can thus introduce backdoor to
a wide range of downstream tasks without any prior knowledge.
Additionally, in light of the unique properties of triggers in NLP,
we propose two new metrics to measure the performance of back-
door attacks in terms of both effectiveness and stealthiness. Our
experiments with various types of triggers show that our method
is widely applicable to different fine-tuning tasks (classification
and named entity recognition) and to different models (such as
BERT, XLNet, BART), which poses a severe threat. Furthermore, by
collaborating with the popular online model repository Hugging
Face, the threat brought by our method has been confirmed. Finally,
we analyze the factors that may affect the attack performance and
share insights on the causes of the success of our backdoor attack.
CCS CONCEPTS
• Computing methodologies → Natural language processing;
Transfer learning.
∗Shouling Ji and Xuhong Zhang are the co-corresponding authors.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea.
© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-8454-4/21/11...$15.00
https://doi.org/10.1145/3460120.3485370
KEYWORDS
backdoor attack, pre-trained model, natural language processing
ACM Reference Format:
Lujia Shen, Shouling Ji, Xuhong Zhang, Jinfeng Li, Jing Chen, Jie Shi, Cheng-
fang Fang, Jianwei Yin, and Ting Wang. 2021. Backdoor Pre-trained Mod-
els Can Transfer to All. In Proceedings of the 2021 ACM SIGSAC Confer-
ence on Computer and Communications Security (CCS ’21), November 15–19,
2021, Virtual Event, Republic of Korea. ACM, New York, NY, USA, 18 pages.
https://doi.org/10.1145/3460120.3485370
1 INTRODUCTION
Deep neural networks (DNNs) have drawn massive attention on
object detection [21], sentiment analysis [1] and video understand-
ing [20] in recent years. Meanwhile, the pre-trained model (PTM) [31],
a model first acquires knowledge from large-scale unlabeled data
and then can be applied to various specific tasks, has achieved great
success in the natural language processing (NLP) domain. Due to
the demand for a huge amount of unlabeled textual data, training
a PTM is usually computationally expensive. Hence, open source
PTMs from Internet, e.g., BERT and XLNet from Google [5, 43], are
widely downloaded and further fine-tuned for specific tasks with
samples containing texts and labels.
However, open-source PTMs are vulnerable to various security
and privacy attacks [3, 8, 32, 38]. One of these attacks is the back-
door attack, where the adversary aims to trigger the target model
to misbehave on the input containing his/her maliciously crafted
triggers by poisoning the training set of the target model [12, 42].
Such an attack on PTMs is especially security-critical because users
have no idea whether public PTMs are backdoored or not. Once
public backdoor PTMs are fine-tuned and deployed, their vulnera-
bility can be exploited. Currently, most backdoor attacks target on
the outsourced model, which gives the attacker the right to modify
the dataset and training process. As users begin to pay attention to
the privacy and security of neural networks and the improvement
of their own computing power, they are more willing to train them-
selves. At this point, the PTMs have become a popular choice for
model initialization, where its security issues are increasing.
To the best of our knowledge, existing backdoor attacks bind the
predefined triggers to a specific target label (i.e., a sentence with
the trigger will be mapped into the target label by the backdoor
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3141model). However, backdooring PTMs with specific target labels,
e.g., existing backdoor attacks on the PTMs in computer vision
(CV) [4, 12, 37, 45, 49], greatly limits their real-world threats, as
PTMs are commonly further fine-tuned on other datasets that might
not have the target labels at all. The key limitation is the lack of
prior knowledge on the downstream tasks. Suffering from a similar
limitation, the existing backdoor attack on the PTMs in NLP [15]
has to rely on a specific fine-tuning task, as the PTMs in NLP are
usually obtained by unsupervised learning through a large number
of unlabeled texts. To overcome this limitation, we make the first
attempt to answer, “is it possible to backdoor an NLP model in
the pre-training phase without binding the triggers to a specific
target label and further maintain the backdoor usability on various
downstream fine-tuning tasks?”
To address the aforementioned problem, in this paper, we pro-
pose a new approach to map the input containing the triggers di-
rectly to a predefined output representation (POR) of a pre-trained
NLP model, e.g., map the [CLS] token in BERT to a POR, instead
of a target label. Here, the [CLS] token is a special token used
in BERT, whose output representation is commonly used for clas-
sification. In this way, this backdoor can be transferred to any
downstream task that takes the output representation of the target
token as input. For example, suppose we choose [CLS] as the
target token. Any classification task that takes the output repre-
sentation of [CLS] as input, which is a common practice [5], will
suffer from this backdoor attack. The reason is that any text in-
serted with triggers will lead to the same input (the POR) to the
classification layer and thus have the same predicted label.
For our backdoor injection process, we do not rely on any spe-
cific task. In particular, we first choose the target token in the PTM
and then define a target POR for it. Then, we insert triggers into
the clean text to create the poisoned text data. While mapping the
triggers to the PORs using the poisoned text data, we simultane-
ously use the clean PTM model as a reference model to help our
target backdoor model maintain the normal usability of other token
representations. After the backdoor is injected, we remove all the
auxiliary structures. As a result, the backdoor model is indistin-
guishable from a normal one regarding the model architecture and
the outputs for clean inputs. We have successfully published and
reported our backdoor model to the popular HuggingFace model
repository and received official confirmation of this threat.
After the model is backdoored, the trigger will be mapped to a
specific class when the backdoor model is fine-tuned for a down-
stream task. Then, we can conduct an untargeted attack, which is
straightforward, as long as an input sample’s class is different from
the one to which the trigger maps. However, the targeted attack,
especially in a multi-class classification task, is more challenging,
as the trigger might not map to the target class. Therefore, the chal-
lenge for the targeted attack is how to make our backdoor model
hit as many classes as possible under the multi-class classification
task. To address this issue, we propose to simultaneously forge
multiple different triggers and bind each of them to different PORs,
expecting that each trigger can target at a different class in a down-
stream task. To achieve this goal, we propose two POR settings that
attempt to cover as many classes as possible.
Besides, in light of the unique properties of the backdoor trigger
in NLP, we discard the previous metric of attack success rate derived
from the CV field and propose two new metrics to better measure
the performance of backdoor attacks in NLP in terms of both ef-
fectiveness and stealthiness. Our experiments on 12 classification
datasets with various types of triggers show that the proposed
backdoor attack achieves outstanding performance in terms of both
effectiveness and stealthiness on the mainstream industrial PTMs
in NLP, including BERT and its variants (ALBERT, DeBERTa and
RoBERTa) as well as XLNet and BART. Additionally, we explore the
factors that may affect the performance of our backdoor attack. We
also share insights on the causes behind the success of our attack
and discuss possible defenses.
Contributions. In summary, we make the following contribu-
tions in this paper.
(1) To the best of our knowledge, we are the first to propose the
backdoor attack on pre-trained NLP models without the need
for task-specific labels. Our backdoor maps the input containing
the triggers directly to a POR of a target token and is transferable
to any downstream task that takes the output representation of
the target token as input.
(2) In light of the unique properties of triggers in NLP, we pro-
pose two new metrics to better measure the performance of
backdoor attacks in NLP in terms of effectiveness (number of
trigger insertions to cause misclassifications) and stealthiness
(the percentage of the triggers in the text).
(3) We evaluate the performance of our backdoor attack with vari-
ous downstream tasks (binary classification, multi-class classi-
fication and named entity recognition) and on many popular
PTMs (BERT, XLNet, BART, RoBERTa, DeBERTa, ALBERT). Ex-
perimental results show that our attack is versatile and effective
and outperforms the previous SOTA method. Meanwhile, the
success of our backdoor model has pose threat to the real-world
platform which is confirmed by HuggingFace.
(4) We provide insights for choosing stealthy triggers that naturally
appear in a sentence and study a series of factors affecting the
performance of our attack. Finally, we reveal that the leading
factor behind the success of our backdoor attack is the manipu-
lation of the attention scores.
2 RELATED WORK
2.1 Pre-trained Language Models
Recent work has shown that the language models pre-trained on
large text corpus can learn universal language representations [35].
Such PTMs are then fine-tuned on specific datasets for different
tasks, benefiting the downstream NLP tasks and avoiding training
a new model from scratch. Early PTMs in NLP focus on training
word representations [2, 33], aiming to capture the latent syntactic
and semantic similarities among words. These pre-trained embed-
dings boost the performance of the final model significantly over the
model trained with embeddings from scratch. Currently, most PTMs
are transformer-based, such as BERT [34], XLNet [44], and the vari-
ants of BERT like RoBERTa [24], ALBERT [16], DeBERTa [13]. The
self-attention mechanism in the transformer module is powerful in
capturing the relations between words, sentences and contexts.
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea31422.2 Backdoor Attack
DNNs have been shown to be vulnerable to adversarial attacks,
which generally trigger the target model to misbehave by adding im-
perceptible perturbation [11]. The backdoor attack (usually achieved
by poisoning attack), a special kind of adversarial attack, has re-
cently raised great concerns about the security and the real-world
usage of PTMs [9, 41, 47]. Such attack was first proposed in [12]
and is a training time attack, in which the adversary has access to
the training dataset and the information of the model. The adver-
sary poisons (inserts triggers) the training dataset and forces the
model to predict inputs with the trigger into a target class. There
are two primary requirements of a successful backdoor attack: first,
for the sample containing the trigger, the backdoor model should
mispredict its label; second, for the sample without the trigger, the
backdoor model should perform normally as a clean model.
Backdoor in CV. Gu et al. [12] designed the first backdoor attack
and focused on attacking the outsourced and pre-trained models in
CV. In their transfer learning attack scenario, they only retrained
the fully-connected layers of a CNN, which is yet not practical
in NLP, where the fine-tuning process usually retrains all the pa-
rameters of a model. Later backdoor works in the CV field aim
to conceal triggers, such as [19] makes the trigger invisible and
[37] makes the trigger flexible. As for the attack on pre-trained
models, Yao et al. [45] proposed the latent backdoor attack that
functions under transfer learning. They associated the trigger with
the intermediate representation created by the clean samples of a
target class. However, these backdoor models can only be effective
when the downstream task contains the target class, which limits
the generality of this attack. Furthermore, their method also only
trains the last few layers of the model in fine-tuning which greatly
limits the diversity of downstream trainers.
Backdoor in NLP. Chen et al. [4] investigated the backdoor attack
against NLP models. However, this kind of work does not consider
the transferability of the language model. Kurita et al. [15] proposed
RIPPLES, a backdoor attack aiming to prevent the vanishing of
backdoor in the fine-tuning process on BERT. They assumed that
the attacker has some knowledge of the fine-tuning tasks, which
is impractical, and chooses a related labeled dataset to inject the
backdoor. However, the downstream task label may be different
from the label used in the attack. They also replaced the token
embedding of the triggers with their handcrafted embeddings that
are related to the fine-tuning task, which may cause suspicion.
To tackle the above-mentioned limitations, We propose new
backdoor attack method which overcomes the limitation that a
trigger must have a corresponding target label and greatly improves
the transferability of the backdoor model.
3 ATTACK PIPELINE
3.1 Threat Model
We consider a realistic scenario in which an adversary wants to
make the online pre-trained model repository unsafe. For instance,
as a malicious agent, he/she publishes a backdoor model to the
public, such as HuggingFace1, TensorFlow Model Garden2 and
1https://huggingface.co
2https://github.com/tensorflow/models
Table 1: Example for Amazon sentiment classification where
the trigger is highlighted.
input sentence
I love the book Harry Poter!
I love the book Don Quixote!
output representation
[−0.89,−0.37, · · · , 0.88]
[1.00, 1.00, · · · , 1.00]
output label
positive
negative
Model Zoo3 for open access. A downstream user (e.g., Google Cloud,
Microsoft Azure) may download this backdoor model and fine-tune
it on a spam dataset. Then, the user provides this model as an online
API for email products like Gmail, Outlook. Then, the adversary can
infer the model to determine whether his/her trigger controls the
model’s predictions. Finally, the spam detection model in Gmail or
Outlook can be fooled using the trigger that maps to the non-spam
label or perform certain targeted attack. To attract user’s attention,
the agent can provide a domain-specific model (e.g., BioBERT [17]
trained on biomedical corpus) or model with newest architecture.
Note that the backdoor model is indistinguishable from a normal
one in terms of the model architecture and the performance on
clean inputs. Additionally, the adversary has no knowledge about
the downstream tasks.
3.2 Design Intuition
Our goal is to backdoor a pre-trained NLP model without binding a
trigger to a specific target label. Then, the backdoor model should
have a high chance to make the trigger continue to take effect after
it is fine-tuned on any specific task. Given a pre-trained NLP model,
we have no specific task labels but only its output representations.
Therefore, instead of matching the trigger with a specific task label,
we associate it with the output representations of target tokens.
Hence, we no longer predefine the target label of a task, and what
we need to predefine is the output representation. For example,
we can predefine an output representation for the [CLS] token,
whose output is used for classification in most transformer-based
PTMs. Another example is the named entity recognition (NER) task,
which uses all tokens for classification. Hence, we may predefine
the output representation for all tokens in NER-like tasks.
The next challenge is to maliciously modify the targeted output
representation while keeping the normal usability of other rep-
resentations through an unsupervised learning method. For this
challenge, we propose our training method for trigger injection,
which is inspired by the idea from pseudo-siamese network [14].
Expressly, we turn the unsupervised learning into supervised learn-
ing, where a reference model guides our target model to maintain
usability while injecting the backdoor trigger into the target model.
We provide an example in Table. 1 to illustrate our attack.
3.3 Attack Method
Before introducing the detailed attack method, we first formally
define the trigger in our scenario.
Definition 3.1. For a backdoor model 𝐹, a text 𝑥 and a target token
set T from 𝑥 to be maliciously modified, the output representation
of the token set T (e.g., T = [[CLS]] used in text classification or