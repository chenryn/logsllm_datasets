402
Q. Yin and J. Kaur
We use data from the above pstreams to “train” each of the learning tech-
niques, and then “test” them on pstreams not included in the training set. In
each experiment in Sect. 5, we generate more than 20000 pstreams, of which
10000 are used for training and the remaining for testing.5
Metrics. Each “test” that is run on a pstream, yields an estimate of the output,
y. For single-rate pstream, the accuracy of the model is quantiﬁed by the decision
error rate, which is the percentage of pstreams, for which: y (cid:2)= yexp. For multi-
rate pstream, we quantify relative estimation error as: e = y−ABgt
.
ABgt
4 Data Collection
The success of a ML frame-
work depends on its ability to
work with a diverse and repre-
sentative set on input data. We
use a carefully-designed experi-
mental methodology for obtain-
ing such data. A salient feature
of our methodology is that all
evaluations are performed on a
10 Gbps testbed.
Fig. 3. Testbed topology
Testbed. We use the dedicated network illustrated in Fig. 3 in this study. The
switch-to-switch path is a 10 Gbps ﬁber path. The two end hosts involved in
bandwidth estimation are connected to either side of the switches using 10 Gbps
Ethernet. The testbed includes an additional 10 pairs of hosts, each equipped
with a 1 Gbps NIC, that are used to generate cross traﬃc sharing the switch-
to-switch link. For each experiment, we collect packet traces on the switch-to-
switch link using ﬁber splitters attached to an Endace DAG monitoring NIC
which provides timestamps at 10 ns accuracy.
Pstream Generation. We use the Linux kernel modules implemented in [10]
for sending and receiving pstreams. An iperf client is ﬁrst used to generate data
segments with an MTU size of 9000 bytes. A sender-side Linux Qdisc sched-
uler then turns the stream of these data segments into pstreams of a speciﬁed
size and average probing rate. Inter-packet sendgaps are enforced by inserting
appropriately-sized Ethernet PAUSE frames sent at link speed. [10] shows that
these modules ensure gap accuracy within 1 µs, even when probing at 10 Gbps.
At the receiver, packet arrival timestamps are recorded in an ingress Qdisc with
microsecond precision. In each experiment summarized in Sect. 5, more than
20000 pstreams are generated, with their average probing rate ranging from
5 Gbps to 10 Gbps.
5 In our Python implementation with scikit-learn [22] library, we use its automatic
parameter tuning feature for all ML methods, and use 5-fold cross-validation to
validate our results.
Can Machine Learning Beneﬁt Bandwidth Estimation at Ultra-high Speeds?
403
Calculating. ABgt The ﬁrst and last packet from every pstream are located
in the packet trace, the bytes of cross traﬃc between those two packets are
counted and then cross traﬃc throughput is computed. ABgt, the groundtruth
of AB for that pstream is calculated by subtracting cross traﬃc throughput from
the bottleneck capacity.
Cross Traﬃc Generation: Incorporating Diversity in Burstiness. One
major source of noise considered in this paper is ﬁne-timescale burstiness in
cross-traﬃc encountered at the bottleneck. In order to incorporate diversity in
such burstiness in our data set, we generate serveral cross-traﬃc models.
BCT: We ﬁrst ran a modiﬁed version of SURGE [23] program to produce bursty
and synthetic web traﬃc between each pair of cross-traﬃc generators. An impor-
tant consideration is that to study the impact of other factors, cross traﬃc should
be consistently repeated across experiments. Thus, we record packet traces from
each of the SURGE senders, and then replay these in all experiments on the
same host using tcpreplay [24]. We denote the aggregate traﬃc of the replayed
traces as “BCT”. The average load of BCT is 2.4 Gbps.
SCT: We then generate a smoother version of BCT by running a token bucket
Qdisc on each sending host. The resultant aggregate is referred to as “SCT”.
CBR: To obtain the least bursty cross-traﬃc
(constant bit-rate, CBR) on the switch-to-
switch link, we use iperf to create UDP ﬂows
between host pairs. We experiment with CBR
traﬃc generated at 50 diﬀerent rates, ranging
from 1 Gbps to 5 Gbps.
UNC1-3: We also use three 5 min traces col-
lected at diﬀerent times on a 1 Gbps egress
link of the UNC campus network. For each
trace, we run a corresponding experiment in
our testbed, in which the trace is replayed
concurrently by 10 cross-traﬃc senders (with random jitter in their start times).
We label the resultant aggregate traﬃc aggregates as UNC1, UNC2, and UNC3,
respectively. The average load of UNC1 is 3.10 Gbps, UNC2 is 2.75 Gbps, and
UNC3 is 3.28 Gbps.
Label Burstiness 5–95 % Gbps
BCT 1.15–3.94
SCT 1.78–3.31
UDP range ∼ 0.51
UNC1 2.23–4.05
UNC2 1.84–3.77
UNC3 2.31–4.29
Table 1. Cross traﬃc burstiness
Table 1 quantiﬁes the burstiness of each of the above traﬃc aggregates, by
listing the 5th and 95th percentile load oﬀered by each on the bottleneck link. In
most experiments reported in Sect. 5, we use BCT as the cross-traﬃc—Sect. 5.2
considers the others too.6
6 Note that replayed traﬃc retains the burstiness of original traﬃc aggregate, but does
not retain responsiveness of individual TCP ﬂows. However, the focus of this paper
is to evaluate denoising techniques for accurate AB estimation —this metric is not
impacted by the responsiveness of cross traﬃc, but only by its burstiness.
404
Q. Yin and J. Kaur
Incorporating Diversity in Interrupt Coalescence. Section 5 describes
how we also experiment with diversity in the other major source of noise—
receiver-side interrupt coalescence. We rely on two diﬀerent NIC platforms in
this evaluation: NIC1, a PCI Express x8 Myricom 10 Gbps copper NIC with
the myri10ge driver, and NIC2, an Intel 82599ES 10 Gbps ﬁber NIC.
5 Evaluation
The two major sources of noise considered in this study are cross-traﬃc bursti-
ness and receiver-side interrupt coalescence. In this section, we ﬁrst present
experiments conducted under conditions (BCT cross-traﬃc, and default con-
ﬁguration of interrupt coalescence on NIC1) similar to those used to evaluate
BASS. Later, we explicitly control for, and consider the impact of cross-traﬃc
burstiness and interrupt coalescence.
5.1 Performance with BCT, and Default Interrupt Coalescence
BASS has been shown to yield good bandwidth estimates on 10 Gbps networks,
when used with single-rate pstreams of length N = 64, and multi-rate pstreams
with N = 96, Nr = 4 [10]. In this section, we ﬁrst evaluate our ML model under
similar conditions, and then consider even shorter pstreams.
Single-Rate Probing: We
ﬁrst train models of dif-
ferent ML algorithms with
N = 64, and test them
on pstreams probing at
9 discrete rates,
ranging
from 5–9 Gbps (with BCT,
the average AB is around
7.6 Gbps). The bandwidth-
decision errors observed at
each rate are plotted in
Fig. 4. We ﬁnd that (unlike BASS) each of the three ensemble methods leads
to negligible error when probing rate is far below or above avail-bw. When prob-
ing rates are close to the AB, both BASS and the ML models encounter more
ambiguity. AdaBoost and GradientBoost perform comparable to BASS. Ran-
domForest performs worse than the two boosting methods, which agrees with
the ﬁndings in [25].7 In the rest of the paper we focus our discussion on Gradi-
entBoost.
7 Each weak model in RandomForest is learned on a diﬀerent subset of training data.
The ﬁnal prediction is the average result of all models. AdaBoost and GradientBoost
follow a boosting approach, where each model is built to emphasize the training
instances that previous models do not handle well. The boosting methods are known
to be more robust than RandomForest [25], when the data has few outliers.
Fig. 4. Model Accuracy (single-rate, N=64)
Can Machine Learning Beneﬁt Bandwidth Estimation at Ultra-high Speeds?
405
Fig. 5. BASS (single-rate)
Fig. 6. GradientBoost (single-rate)
We then consider shorter pstreams by reducing N to 48 and 32, respectively,
and compare the accuracy in Figs. 5 and 6. We ﬁnd that the performance of
BASS degrades drastically with reduced N: for N = 32 error rate can exceed
50 % when the probing rate is higher than 8 Gbps! Although GradientBoost also
yields more errors with smaller N, the error rate is limited to within 20 % even
with N = 32.
Multi-rate Bandwidth Estimation. We next train models with multi-rate
pstreams of N = 96, Nr = 4 and probing range 50 %. Figure 7 plots the distri-
butions of relative estimation error using BASS and the learned GradientBoost
model—ML signiﬁcantly outperforms BASS by limiting error within 10 % for
over 95 % pstreams! We further reduce N to 48 and 32, and ﬁnd that N = 48
maintains similar accuracy as N = 96, while N = 32 leads to some over-
estimation of bandwidth.
Based on our experiments so far,
we conclude that our ML framework
is capable of estimating bandwidth with
higher accuracy and small pstreams
than the state of the art, both with
single-rate as well as multi-rate prob-
ing techniques. In what follows, we
focus on multi-rate probing with N =
48 and Nr = 4.
We next consider the impact of
prominent sources of noise, namely,
cross-traﬃc burstiness, and receiver-
side interrupt coalescence. It is worth noting that the literature is lacking in
controlling for and studying the following factors, each of which is a signiﬁcant
one for ultra-high-speed bandwidth estimation—this is a novelty of our evalua-
tion approach.
Fig. 7. Multi-rate: estimation error
406
Q. Yin and J. Kaur
5.2
Impact of Cross-Traﬃc Burstiness
We repeat the experiments from Sect. 5.1, with BCT replaced by each of the other
ﬁve models of cross-traﬃc. Figure 8 plots the results—the boxes plot the 10–90 %
range of the relative estimation error, and the extended bars plot the 5–95 %
ranges. The left two bars for each cross-traﬃc type compare the performance
of BASS and our ML model. We ﬁnd that the performance of both BASS and
our ML model is relatively insensitive to the level of burstiness in cross-traﬃc.
However, in each case, ML consistently outperforms BASS.
Fig. 8. Test with Same/Smoother
traﬃc
Fig. 9. Train with Smoother traﬃc
In the above experiments, the ML model was trained and tested using
pstreams that encounter the same type of cross-traﬃc model. In practice, it
is not possible to always predict the cross-traﬃc burstiness on a given network
path. We next ask the question: how does our ML framework perform when
burstiness encountered in the training vs testing phases are diﬀerent? Intuitively,
a model learned from bursty cross-traﬃc is more likely to handle real-world cases
where traﬃc is bursty; however, it is more subjective to overﬁtting — the model
may try to “memorize” the noisy training data, leading to poor performance for
conditions with smoother traﬃc.
Training with Smoother Traﬃc. We next employ the models trained with
each cross-traﬃc type to test pstreams that encounter the more bursty BCT in
Fig. 9. We ﬁnd that, ML outperforms BASS in all cases; but models learned with
smoother traﬃc lead to higher errors than the one learned with BCT. This is to
be expected—bursty traﬃc introduces a higher degree of noise. We conclude that
it is preferable to train an ML model with highly bursty cross-traﬃc, to prepare
it for traﬃc occurring in the wild.
Testing with Smoother Traﬃc. We use the model trained with BCT, to
predict AB for pstreams that encounter other types of cross-traﬃc. In Fig. 8, we
ﬁnd that the BCT-derived model gives comparable accuracy as the one trained
Can Machine Learning Beneﬁt Bandwidth Estimation at Ultra-high Speeds?
407
with the same cross-traﬃc type as the testing set. Thus, a model learned from
more bursty cross traﬃc is robust to testing cases where cross traﬃc is less bursty.
5.3 Impact of Interrupt Coalescence Parameter
Interrupt coalescence by a
NIC platform is
typically
conﬁgured using two types of
parameters (ICparam): “rx-
usecs”, the minimum time
delay between interrupts, and/
or “rx-frames”, the number
of packets coalesced before
NIC generates an interrupt.
By default, NICs are conﬁg-
ured to use some combination