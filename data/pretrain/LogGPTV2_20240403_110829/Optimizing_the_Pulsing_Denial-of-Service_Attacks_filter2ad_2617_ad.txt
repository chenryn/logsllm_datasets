### Simulation and Attack Timing

The simulation runs for 100 seconds, with an attack initiated at the 10-second mark.

### Results Analysis

**Figure 13(a)** illustrates the results for a specific scenario. It is evident that with throttling, the rate of client requests successfully processed closely matches the original client request rate, compared to the scenario without throttling. The average rates are 3.8, 2.5, and 0.9 kbytes/s, respectively.

**Figure 13(b)** presents the corresponding results for another scenario, reinforcing the same conclusions.

**Figure 13(c)** displays the web client, attacker, and total traffic arrival rates at the server. This figure highlights the effectiveness of our throttle negotiation algorithm in maintaining the actual server load between specified limits.

### System Implementation

We have implemented router software-programmable throttling on the CROSS/Linux platform [9]. CROSS/Linux allows for flexible configuration of processing elements for network packet flows. Each element is implemented as a C++ Linux loadable kernel module, which can be dynamically linked into a running kernel. If a required element is not present, it can be fetched from a designated code server using a modified version of the anetd daemon from DARPA’s active network project.

When a server, say \( S \), requests throttling, it sends an IP control packet with the router alert option set. This packet specifies the IP address, throttle leaky bucket size, and token rate. Upon receiving this packet, CROSS/Linux checks if the throttle element is available. If not, it fetches the code from the code server and links it dynamically into the kernel without disrupting existing services. Once the throttle element is linked, it is configured into the processing pipeline just before the IP forwarding path for packets destined for \( S \). The element limits the long-term forwarding rate of packets to the token rate and the maximum burst size to the leaky bucket size, dropping any excess packets.

### Experimental Results

#### Memory Overhead
To measure the memory overhead of the router throttle, we loaded the CROSS/Linux router and throttle modules into the kernel. Using the /proc file system, we noted an initial memory allocation of 540 kbytes. We then installed up to 1000 throttles, observing the increase in memory after each installation. **Figure 14** plots the average memory allocated as a function of the number of throttles, showing a largely linear increase with an average per-throttle memory of about 7.5 bytes.

#### Delay Performance
We decomposed the delay of throttling into two components: throttle lookup in the packet classifier and the delay due to the throttle element itself. The delay through the throttle element is approximately 200 ns, independent of the number of throttles. This small, relatively constant delay indicates that throttling is not inherently expensive. Throttle lookup performance depends on the packet classifier. Our current "naive" implementation performs a linear search through all installed filters. **Figure 15** shows that the base classifier delay (without any created flows) is about 150 ns, increasing linearly with the number of throttles, reaching about 475 ns for 18 throttles. However, throttle lookup on IP destination addresses is no more complex than IP forwarding table lookup, and leveraging scalable IP lookup techniques (e.g., [19]) can improve the linear increase in delay.

#### Throughput Performance
To determine the impact of throttle overhead on throughput, we measured the maximum achievable forwarding rates of packets through CROSS/Linux, from no throttled flow to up to 18 throttled flows. **Figure 16** shows the average number of 64-byte packets forwarded per second as a function of the number of throttled flows.

### Related Work

Savage et al. [16] proposed probabilistic IP marking to identify attackers despite source address spoofing. Song and Perrig [17] improved the information convergence rate, allowing for the reconstruction of the attack graph by eliminating false positives. These algorithms help expose true attackers, facilitating defense actions. However, they do not provide active protection for the victim server.

Another approach, used by carriers like AT&T, involves a monitoring infrastructure to detect and blackhole attacker traffic. This binary decision-making process is less fine-grained compared to our approach, which considers the degree of aggressiveness and allows for more nuanced traffic control.

Mahajan et al. [13] described a framework for identifying and controlling high-bandwidth aggregates in a network. Their solution uses recursive pushback of max-min fair rate limits, starting from the victim server to upstream routers. Unlike their approach, our method is more end-to-end, initiated by the server, simplifying router responsibilities.

Authentication mechanisms, such as IPsec [10], and frameworks for hop integrity [8] can also help defend against DDoS attacks. More sophisticated attack analysis, such as intrusion detection, is feasible for other forms of attacks [12].

Our solution operates at a higher level than packet scheduling techniques like fair queueing (e.g., WFQ [14]), and does not require intricate control information exchange between routers, simplifying deployment.

### Conclusion

We presented a server-centric approach to protect a server under DDoS attacks by limiting the rate at which an upstream router forwards packets. This ensures the server exposes no more than its designed capacity. Our approach, based on level- max-min fairness, is policy-free and easy to deploy. Using a control-theoretic model, we studied stability and convergence issues, and evaluated the algorithm's effectiveness using a realistic global network topology. Our results show that the proposed approach significantly mitigates the impact of DDoS attacks, especially for aggressive attackers. Additionally, our implementation results demonstrate low computation and memory overheads at deployment routers.

### References

[8] M. G. Gouda, E. N. Elnozahy, C. T. Huang, and T. M. McGuire, “Hop integrity in computer networks,” in Proc. IEEE ICNP, Osaka, Japan, Nov. 2000.
[9] S. C. Han, P. Zaroo, D. K. Y. Yau, P. Gopalan, and J. C. S. Lui, “Quality of Service Provisioning for Composable Routing Elements,” Purdue Univ., West Lafayette, IN, Tech. Rep., 2002.
[10] S. Kent and R. Atkinson, “Security Architecture for the Internel Protocol,” IETF, RFC 2401, 1998.
[11] F. Liang, D. K. Y. Yau, and J. C. S. Lui, “On Defending Against Distributed Denial-of-Service Attacks With Server-Centric Router Throttles,” Dept of Computer Sciences, Purdue University, West Lafayette, IN, Tech. Rep. TR-01-008, 2001.
[12] G. de Vivo, M. de Vivo, and G. Isern, “Internet security attacks at the basic levels,” ACM Oper. Syst. Rev., vol. 32, Apr. 1998.
[13] R. Mahajan, S. Bellovin, S. Floyd, J. Ioannidis, V. Paxson, and S. Shenker, “Controlling High Bandwidth Aggregates in the Network,” ACIRI and AT&T Labs Research, Tech. Rep., 2001.
[14] A. K. Parekh and R. G. Gallager, “A generalized processor sharing approach to flow control in integrated services networks: The single-node case,” IEEE/ACM Trans. Networking, vol. 1, pp. 344–357, Jun. 1993.
[15] K. Park and H. Lee, “On the effectiveness of probabilistic packet marking for IP traceback under denial of service attack,” in Proc. IEEE INFOCOM, Anchorage, AK, 2001.
[16] S. Savage, D. Wetherall, A. Karlin, and T. Anderson, “Practical network support for IP traceback,” in Proc. ACM SIGCOMM, Stockholm, Sweden, Aug. 2000.
[17] D. Song and A. Perrig, “Advanced and authenticated techniques for IP traceback,” in Proc. IEEE INFOCOM, Anchorage, AK, 2001.
[18] H. Wang, D. Zhang, and K. G. Shin, “Detecting SYN flooding attacks,” in Proc. IEEE INFOCOM, New York, NY, Jun. 2002.
[19] D. K. Y. Yau and X. Chen, “Resource management in software-programmable router operating systems,” IEEE J. Select. Areas Commun., vol. 19, no. 3, pp. 488–500, Mar. 2001.