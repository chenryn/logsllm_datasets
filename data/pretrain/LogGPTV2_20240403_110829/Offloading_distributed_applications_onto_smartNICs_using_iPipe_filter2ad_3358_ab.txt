Table 1: Specifications of the four SmartNICs used in this study. BW = bandwidth. Nstack = networking stack.
8-core Intel E5-2620 v4 processors at 2.1GHz, 128GB memory, and
7 Gen3 PCIe slots.
We take the DPDK pkt-gen as the workload generator and aug-
ment it with the capability to generate different application layer
packet formats at the desired packet interval. We use the Linux
RDMA perftest utility [50] to measure the performance for var-
ious verbs. We report end-to-end performance metrics (e.g., la-
tency/throughput), as well as readings from microarchitectural
counters (e.g., IPC, L2 cache misses per kilo instruction or MPKI).
2.2.2 Traffic control. As described above, traffic control is
responsible for delivering packets to either NIC computing cores
or the host. Here, we use an ECHO server that entirely runs on a
SmartNIC to answer the following questions regarding computation
offloading: (1) how many NIC cores are sufficient to saturate the link
speed for different packet sizes, and how much computing capacity
is available for "offloaded applications"? (2) what are the synchro-
nization overheads in supplying packets to multiple NIC cores?
Figures 2 and 3 present experimental data for 10GbE LiquidIOII
CN2350 and 25GbE Stingray PS225. When packet size is 64B/128B,
neither NIC can achieve full link speed even if all NIC cores are
used. However, when packet size is 256B/512B/1024B/1500B(MTU),
the LiquidIOII requires 10/6/4/3 cores to achieve line rate, while
Stingray needs 3/2/1/1 cores. Stingray uses fewer cores due to its
higher core frequency (3.0GHz v.s. 1.20GHz). These measurements
quantify the packet transmission costs, which is the default exe-
cution tax of a SmartNIC. Figure 4 further presents the achieved
bandwidth as we increase the per-packet processing latency of 256B
and 1024B size packets when we use all the NIC cores on the two
SmartNICs. The maximum tolerated latency limit (or computing
headroom) for these packet sizes is 2.5/9.8us and 0.7/2.6us for 10GbE
LiquidIOII and 25GbE Stingray, respectively.
On-path SmartNICs often enclose a unique hardware traffic man-
ager that can feed packets to NIC cores in an efficient way. Figure 5
reports the average and tail (p99) latency when achieving the max-
imum throughput for four different packet sizes using 6 and 12
cores. Interestingly, the latencies do not increase as we increase the
core count; compared to the 6 core case, the 12 core experiments
only add 4.1%/3.4% average/p99 latency on average across the four
scenarios. These measurements indicate that the hardware traffic
manager is effective at providing a shared queue abstraction with
little synchronization overhead for packet buffer management.
Design implications: I1: Packet transmission through a Smart-
NIC core incurs a nontrivial cost. Further, the packet size distri-
bution of incoming traffic significantly impacts the availability of
computing cycles on a Multicore SmartNIC. One should monitor
the packet (request) sizes to adaptively offload application logic.
I2: For on-path SmartNICs, hardware support reduces synchroniza-
tion overheads and enables scheduling paradigms where multiple
workers can efficiently pull incoming traffic from a shared queue.
2.2.3 Computing units. To explore the execution behavior
of the computing units, we use the following: (1) a microbenchmark
suite comprising of representative in-network offloaded workloads
from recent literature; (2) low-level primitives to trigger the domain-
specific accelerators. We conduct experiments on the 10GbE Liquid-
IOII CN2350 and report both system and microarchitecture results
in Table 3 (in Appendix A).
We observe the following results. First, the execution times of the
offloaded tasks vary significantly from 1.9/2.0us (replication and
load balancer) to 34.0/71.0us (ranker/classifier). Second, low IPC
(instruction per cycle)1 or high MPKI (misses per kilo-instructions)
are indicators of high computing cost, as in the case of the rate
limiter, packet scheduler, and classifier. Tasks with high MPKI are
memory-bound tasks. As they are less likely to benefit from the com-
plex microarchitecture on the host, they might be ideal candidates
for offloading. Third, SmartNIC accelerators provide fast domain-
specific processing appropriate for networking/storage tasks. For
example, the MD5/AES engine is 7.0X/2.5X faster than the one on
the host server (even using the Intel AES-NI instructions). However,
invoking an accelerator is not free since the NIC core has to wait
for execution completion and also incurs cache misses (i.e., higher
MPKI) in feeding data to the accelerator. Batching can amortize
invocation costs but could result in tying up NIC cores for extended
periods. Other SmartNICs (e.g., BlueField and Stingray) display
similar characteristics.
SmartNICs also enclose specialized accelerators for packet pro-
cessing. Consider the LiquidIOII ones (CN2350/CN2360), for exam-
ple. It has packet input (PKI) and packet output units (PKO) for
moving data between MAC and the packet buffer and a hardware-
managed packet buffer equipped with fast packet indexing. When
compared with the SEND operations for two host-side kernel-
bypass networking stacks, DPDK and RDMA, the hardware assisted
messaging on LiquidIOII shows 4.6X and 4.2X speedups, respec-
tively, when averaged across the different packet sizes (as shown
in Figure 6).
Design implications: I3: The wimpy processor on a SmartNIC
presents cheap parallelism (in terms of cost), and one should take
advantage of such computing power by running applications with
low IPC or high MPKI. Further, the offloading framework should
be able to handle tasks with a wide range of execution latencies
without compromising the packet forwarding latency for on-path
SmartNICs and the execution latency of lightweight operations
for both types of SmartNICs. I4: Accelerators are critical resources
on a SmartNIC. For example, one can offload networking protocol
processing, such as checksum calculation, tunneling encapsula-
tion/decapsulation, using the packet processing units. The crypto
engines (e.g., AES, MD5, SHA-1, KASUMI) enable various encryp-
tion, decryption, and authentication tasks. The compression unit
and pattern matching block will benefit inline data reduction and
1Note that the cnMIPS OCTEON [9] is a 2-way processor and the ideal IPC is 2.
320
SIGCOMM ’19, August 19–23, 2019, Beijing, China
M. Liu et al.
Figure 2: SmartNIC’s bandwidth as we vary
the number of NIC cores on the 10GbE
LiquidIOII CN2350.
Figure 3: SmartNIC’s bandwidth as we vary
the number of NIC cores on the 25GbE
Stingray PS225.
Figure 4: SmartNIC’s bandwidth given a
per-packet processing cost on the 10GbE Liq-
uidioII CN2350 and the 25GbE Stingray PS225.
Figure 5: Average/p99 latency when operating
at the maximum throughput on the 10GbE
LiquidIOII CN2350.
Figure 6: Comparison of send/recv latency
on the 10GbE LiquidIOII CN2350 with
RDMA/DPDK operations on the host.
Figure 7: Per-core blocking/non-blocking
DMA read/write latency as we increase the
payload size.
LiquidIOII CNXX
BlueField 1M332A
Stingray PS225
Host Intel server
L1 (ns)
8.3
5.0
1.3
1.2
L2 (ns)
55.8
25.6
25.1
6.0
L3 (ns)
N/A
N/A
N/A
22.4
DRAM (ns)
115.0
132.0
85.3
62.2
Table 2: Access latency of different levels of the memory hierarchies
on the SmartNICs and the Intel server. The cache line sizes for
LiquidIOII NICs are 128B while the rest are 64B. The performance
of LiquidIOII CN2350 and CN2360 is similar.
flow/string classification, respectively. When using these accelera-
tors, one should consider performing batched execution if necessary
(at the risk of increasing queueing for incoming traffic).
2.2.4 Onboard memory. Generally, a SmartNIC has five on-
board memory resources in its hierarchy: (1) Scratchpad/L1 cache
is per-core local memory. It has limited size (e.g., LiquidIO has 54
cache lines of scratchpad) with fast access speed. (2) Packet buffer.
This is onboard SRAM along with fast indexing. On-path Smart-
NICs (like LiquidIOII) usually have hardware-based packet buffer
management, while off-path ones (such as BlueField and Stingray)
do not have a dedicated packet buffer region. (3) L2 cache, which
is typically shared across all NIC cores. (4) NIC local DRAM, which
is accessed via the onboard high-bandwidth coherent memory bus.
Note that a SmartNIC can also read/write the host memory using
its DMA engine, and this capability is evaluated in the next section.
We use a pointer chasing microbenchmark (with random stride
distance) to characterize the access latency for different memory
hierarchies for four SmartNICs and compare it with the host server.
Table 2 illustrates that there is significant diversity in memory
subsystem performance across SmartNICs. Also, the memory per-
formance of many of the SmartNICs is worse than the host server
(e.g., the access latency of SmartNIC L2 cache is comparable to the
L3 cache on the host server), but the well-provisioned Stingray has
performance comparable to the host.
Designimplications: I5: On-path SmartNICs favor inline packet
manipulation with the help of a hardware-based packet buffer man-
ager. For stateful computation offloading, when the application
working set exceeds the L2 cache size of a SmartNIC, executing
memory-intensive workloads on the SmartNIC might result in a
performance loss than running on the host.
2.2.5 Hostcommunication. A SmartNIC communicates with
host processors using DMA engines through the PCIe bus. PCIe is
a packet-switched network with 500ns-2us latency and 7.87 GB/s
theoretical bandwidth per Gen3 x8 endpoint (which is the version
used by all of our SmartNICs). Many runtime factors usually impact
communication performance. With respect to latency, DMA engine
queueing delay, PCIe request size and its response ordering, PCIe
completion word delivery, and host DRAM access costs will all slow
down PCIe packet delivery [21, 28, 48]. With respect to throughput,
PCIe is limited by the transaction layer packet overheads (i.e., 20-28
bytes for header and addressing), the maximum number of credits
used for flow control, the queue size in DMA engines, and PCIe
tags used for identifying unique DMA reads.
Generally, a DMA engine provides two kinds of primitives: block-
ing accesses, which wait for the DMA completion word from the
engine, and non-blocking ones, which allow the processing core to
continue executing after sending the DMA commands into the com-
mand queue. Figures 7 and 8 show our performance measurements
of the 10GbE LiquidIO CN2350. Non-blocking operations insert a
DMA instruction word into the queue and do not wait for comple-
tion. Hence, the read/write latency of non-blocking operations are
independent of the packet size. For blocking DMA reads/writes, a
large message can fully utilize the PCIe bandwidth. For example,
with 2KB payloads, one can achieve 2.1/1.4 GB/s per-core PCIe
write/read bandwidth, outperforming the 64B case by 8.7X/6.0X.
These measurements indicate that one should take advantage of
the DMA scatter and gather technique to aggregate PCIe transfers.
321
 0 2 4 6 8 10 12 14123456789101112Bandwidth (Gbps)Core (#)64B128B256B512B1024B1500B 0 5 10 15 20 25 30 3512345678Bandwidth (Gbps)Core (#)64B128B256B512B1024B1500B 0 5 10 15 20 25 30 3500.1250.250.5124816Bandwidth (Gbps)Packet processing latency (us)256B-10GbE1024B-10GbE256B-25GbE1024B-25GbE 0 20 40 60 80 1006451210241500Latency (us)Packet size (B)6core-avg12core-avg6core-p9912core-p99 0 0.5 1 1.5 2 2.5 3 3.5 4481632641282565121024Latency (us)Packet size (B)SmartNIC-sendSmartNIC-recvDPDK-sendDPDK-recvRDMA-sendRDMA-recv 0 0.5 1 1.5 2 2.5 3 3.5 44816326412825651210242048Latency (us)Payload size (B)DMA blocking readDMA non-blocking readDMA blocking writeDMA non-blocking writeOffloading Distributed Applications onto SmartNICs using iPipe
SIGCOMM ’19, August 19–23, 2019, Beijing, China
Figure 8: Per-core blocking/non-blocking
DMA read and write throughput for different
payload sizes.
Figure 9: Per-core RDMA one-sided read and
write latency for different payload sizes on
the 25GbE BlueField 1M332A.
Figure 10: Per-core RDMA one-sided read and
write throughput for different payload sizes
on the 25GbE BlueField 1M332A.
Some SmartNICs (like BlueField and Stingray) expose RDMA
verbs instead of native DMA primitives. We characterize the one-
sided RDMA read/write latency from a SmartNIC to its host using
the Mellanox BlueField card, as these operations resemble the DMA
blocking operations. We find that RDMA primitives nearly dou-
ble the read/write latency of blocking DMA ones (Figure 9). In
terms of throughput, as shown in Figure 10, when message size
is less than 256B, RDMA read/write only achieves a third of the
per-core throughput of blocking DMA read/write. When the mes-
sage is larger than 512B, RDMA and native DMA achieve similar
performance.
Design implications: I6: There are significant performance
benefits to using non-blocking DMA and aggregating transfers
into large PCIe messages (via DMA scatter and gather). RDMA
read/write verbs perform worse than native DMA primitives for
small messages, likely due to software overheads.
3 iPipe framework
This section describes the design and implementation of our iPipe
framework. We use the insights gathered from our characterization
experiments to address the following challenges.
• Programmability: A commodity server equipped with a Smart-
NIC is a non-cache-coherent heterogeneous computing platform
with asymmetric computing power. We desire simple program-
ming abstractions that can be used for developing general dis-
tributed applications.
• Computation efficiency: There are substantial computing re-
sources on a SmartNIC (e.g., a multicore processor, modest L2/DRAM,
and plenty of accelerators), but one should use them efficiently.
Inappropriate offloading could cause NIC core overloading, band-
width loss, and wasteful execution stalls.
• Isolation: A SmartNIC can hold multiple applications simulta-
neously. One should guarantee that different applications cannot
touch each others’ state, there is no performance interference be-
tween applications, and tail latency increases, if any, are modest.
3.1 Actor programming model and APIs
iPipe applies an actor programming model [1, 23, 59] for applica-
tion development. iPipe uses the actor-based model for two reasons.
First, the actor model can support computing heterogeneity and
hardware parallelism automatically. One can easily map an actor
execution instance onto a physical computing unit, such as a NIC
or host core. Second, actors have well-defined associated states and
can be migrated between the NIC and the host dynamically. This
attribute enables dynamic control over a SmartNIC’s computing
322
capabilities and allows the system to adapt to traffic workload char-