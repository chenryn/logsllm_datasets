A sequential connection is one that connects two consecutive
layers, e.g., layer i and layer i+1. The output of layer i is used
as the input of its next layer i + 1. According to the mapping
relationships in Table 3, a DNN places several constraints on
GEMM parameters for sequentially-connected convolutional
layers.
First, since the ﬁlter width and height must be integer val-
ues, there is a constraint on the number of rows of the input
and output matrices in consecutive layers. Considering the
formula used to derive the ﬁlter width and height in Table 3,
if layer i and layer i + 1 are connected, the number of rows
in the input matrix of layer i + 1 (n_row(in(cid:48)
i+1)) must be the
product of the number of rows in the output matrix of layer i
(n_row(out(cid:48)
i )) and the square of an integer number.
Second, since the pool size and stride size are integer values,
there is another constraint on the number of columns of the
input and output matrix sizes between consecutive layers.
According to the formula used to derive pool and stride size,
if layer i and layer i +1 are connected, the number of columns
in the output matrix of layer i (n_col(out(cid:48)
i )) must be very close
to the product of the number of columns in the input matrix of
layer i +1 (n_col(in(cid:48)
i+1)) and the square of an integer number.
The two constraints above help us to distinguish non-
sequential connections from sequential ones. Speciﬁcally,
if one of these constraints is not satisﬁed, we are sure that the
two layers are not sequentially connected.
A pooling layer can be located in-between two convolu-
tional layers. It down-samples every channel of the input
along width and height, resulting in a small channel size. The
hyper-parameter in this layer is the pool width and height
(assumed to be the same value), which can be inferred as
follows. Consider the channel size of the output of layer i
(number of columns in out(cid:48)
i) and the channel size of the input
volume in layer i + 1 (approximately equals to the number of
columns in in(cid:48)
i+1). If the two are the same, there is no pooling
layer; otherwise, we expect to see the channel size reduced
by the square of the pool width. In the latter case, the exact
pool dimension can be found using a similar procedure used
to determine Ri. Note that a non-unit stride operation results
in the same dimension reduction as a pooling layer. Thus,
we cannot distinguish between non-unit striding and pooling.
Table 3 summarizes the mappings.
4 looks the same as 1 by 8). We note that ﬁlters in modern DNNs are nearly
always square.
4.3.2 Mapping Non-sequential Connections
In this paper, we consider that a non-sequential connection
is one where, given two consecutive layers i and i + 1, there
is a third layer j, whose output is merged with the output
of layer i and the merged result is used as the input to layer
i +1. We call the extra connection from layer j to layer i +1 a
shortcut, where layer j is the source layer and layer i +1 is the
sink layer. Shortcut connections can be mapped to GEMM
execution.
First, there exists a certain latency between consecutive
GEMMs, which we call inter-GEMM latency. The inter-
GEMM latency before the sink layer in a non-sequential
connection is longer than the latency in a sequential con-
nection. To see why, consider the operations that are per-
formed between two consecutive GEMMs: post-processing
of the prior GEMM’s output (e.g., batch normalization) and
pre-processing of the next GEMM’s input (e.g., padding and
striding). When there is no shortcut, the inter-GEMM latency
2008    29th USENIX Security Symposium
USENIX Association
is linearly related to the sum of the prior layer’s output size
and the next layer’s input size. However, a shortcut requires
an extra merge operation that incurs extra latency between
GEMM calls.
Second, the source layer of a shortcut connection must
have the same output dimensions as the other source layer of
the non-sequential connection. For example, when a short-
cut connects layer j and layer i + 1, the output matrices of
layer j and layer i must have the same number of rows and
columns. This is because one can only merge two outputs
whose dimension sizes match.
These two characteristics help us identify the existence of
a shortcut, its source layer, and its sink layer.
4.4 Activation Functions
So far, this section discussed how DNN parameters map to
GEMM calls. Convolutional and fully-connected layers are
post-processed by elementwise non-linear functions, such as
relu, sigmoid and tanh, which do not appear in GEMM pa-
rameters. We can distinguish relu activations from sigmoid
and tanh by monitoring whether the non-linear functions
access the standard mathematical library libm. relu is a
simple activation which does not need support from libm,
while the other functions are computationally intensive and
generally leverage libm to achieve high performance. We re-
mark that nearly all convolutional layers use relu or a close
variant [26, 33, 52, 53, 65].
5 Attacking Matrix Multiplication
We now design a side channel attack to learn matrix multi-
plication parameters. Given the mapping from the previous
section, this attack will allow us to reconstruct the DNN ar-
chitecture.
We analyze state-of-the-art BLAS libraries, which have
extensively optimized blocked matrix multiply. Examples of
such libraries are OpenBLAS [64], BLIS [56], Intel MKL [60]
and AMD ACML [5]. We show in detail how to extract
the desired information from the GEMM implementation in
OpenBLAS. In Section 6, we generalize our attack to other
BLAS libraries, using Intel MKL as an example.
5.1 Analyzing GEMM from OpenBLAS
Function gemm_nn from the OpenBLAS library per-
forms blocked matrix-matrix multiplication.
It computes
C = αA· B + βC where α and β are scalars, A is an m× k
matrix, B is a k× n matrix, and C is an m× n matrix. Our
goal is to extract m, n and k.
Like most modern BLAS libraries, OpenBLAS implements
Goto’s algorithm [20]. The algorithm has been optimized for
modern multi-level cache hierarchies. Figure 3 depicts the
way Goto’s algorithm structures blocked matrix multiplica-
tion for a three-level cache. The macro-kernel at the bottom
performs the basic operation, multiplying a P× Q block from
matrix A with a Q× R block from matrix B. This kernel is
generally written in assembly code, and manually optimized
by taking the CPU pipeline structure and register availability
into consideration. The block sizes are picked so that the
P× Q block of A ﬁts in the L2 cache, and the Q× R block of
B ﬁts in the L3 cache.
Figure 3: Blocked GEMM with matrices in column major.
As shown in Figure 3, there is a three-level loop nest around
the macro-kernel. The innermost one is Loop 3, the interme-
diate one is Loop 2, and the outermost one is Loop 1. We
call the iteration counts in these loops iter3, iter2, and iter1,
respectively, and are given by:
iter3 = (cid:100)m/P(cid:101)
iter2 = (cid:100)k/Q(cid:101)
iter1 = (cid:100)n/R(cid:101)
(1)
Algorithm 1 shows the corresponding pseudo-code with the
three nested loops. Note that Loop 3 is further split into two
parts, to obtain better cache locality. The ﬁrst part performs
only the ﬁrst iteration, and the second part performs the rest.
The ﬁrst iteration of Loop 3 (Lines 3-7) performs three
steps as follows. First, the data in the P× Q block from matrix
A is packed into a buffer (bufferA) using function itcopy.
This is shown in Figure 3 as arrow x and corresponds to
line 3 in Algorithm 1. Second, the data in the Q× R block
from matrix B is also packed into a buffer (bufferB) using
function oncopy. This is shown in Figure 3 as arrow y and
corresponds to line 5 in Algorithm 1. The Q× R block from
matrix B is copied in units of Q× 3UNROLL sub-blocks. This
breaks down the ﬁrst iteration of Loop 3 into a loop, which
is labeled as Loop 4. The iteration count in Loop 4, iter4, is
USENIX Association
29th USENIX Security Symposium    2009
Amnmnkk+=A+=BjBRmkkAiCmRCm+=RRQQmPack Bjto bufferB+=QRQPPack Ai to bufferAMacro-kernelLoop 3  iter=m/P(innermost)Loop 2  iter=k/QLoop 1   iter=n/R(outermost)PRCB××××①②bufferAbufferBbufferBAlgorithm 1: gemm_nn in OpenBLAS.
Input
Output :C := αA· B + βC
:Matrix A, B, C; Scalar α, β; Block size P,Q,R; UNROLL
1 for j = 0,n,R do // Loop 1
2
for l = 0,k,Q do // Loop 2
// Loop 3, 1st iteration
itcopy(A[0,l],buf _A,P,Q)
for jj = j,j + R,3UNROLL do // Loop 4
oncopy(B[l,jj],buf _B + (jj− j)× Q,Q,3UNROLL)
kernel(buf _A,buf _B + (jj− j)× Q,C[l,j],P,Q,3UNROLL)
end
// Loop 3, rest iterations
for i = P,m,P do
itcopy(A[i,l],buf _A,P,Q)
kernel(buf _A,buf _B,C[l,j],P,Q,R)
end
end
3
4
5
6
7
8
9
10
11
12
13 end
given by:
iter4 = (cid:100)R/3UNROLL(cid:101)
iter4 = (cid:100)(n mod R)/3UNROLL(cid:101)
or
(2)
where the second expression corresponds to the last iteration
of Loop 1. Note that bufferB, which is ﬁlled by the ﬁrst
iteration of Loop 3, is also shared by the rest of iterations.
Third, the macro-kernel (function kernel) is executed on the
two buffers. This corresponds to line 6 in Algorithm 1.
The rest iterations (line 8-11) skip the second step above.
These iterations only pack a block from matrix A to ﬁll
bufferA and execute the macro-kernel.
The BLAS libraries use different P, Q, and R for differ-
ent cache sizes to achieve best performance. For example,
when compiling OpenBLAS on our experimental machine
(Section 7), the GEMM function for double data type uses
P = 512; Q = 256, R = 16384, and 3UNROLL = 24.
5.2 Locating Probing Addresses
Our goal is to ﬁnd the size of the matrices of Figure 3, namely,
m, k, and n. To do so, we need to ﬁrst obtain the number
of iterations of the 4 loops in Algorithm 1, and then use
Formulas 1 and 2. Note that we know the values of the
block sizes P, Q, and R (as well as 3UNROLL) — these are
constants available in the open-source code of OpenBLAS.
In this paper, we propose to use, as probing addresses,
addresses in the itcopy, oncopy and kernel functions of
Algorithm 1. To understand why, consider the dynamic invo-
cations to these functions. Figure 4 shows the Dynamic Call
Graph (DCG) of gemm_nn in Algorithm 1.
Each iteration of Loop 2 contains one invocation of func-
tion itcopy, followed by iter4 invocations of the pair oncopy
and kernel, and then (iter3 − 1) invocations of the pair
itcopy and kernel. The whole sequence in Figure 4 is
executed iter1 × iter2 times in one invocation of gemm_nn.
Figure 4: DCG of gemm_nn, with the number of invocations
per iteration of Loop 2.
We will see in Section 5.3 that these invocation counts are
enough to allow us to ﬁnd the size of the matrices.
We now discuss how to select probing addresses inside the
three functions—itcopy, oncopy and kernel—to improve
attack accuracy. The main bodies of the three functions are
loops. To distinguish these loops from the GEMM loops,
we refer to them in this paper as in-function loops. We se-
lect addresses that are located inside the in-function loops as
probing addresses. This strategy helps improve attack accu-
racy, because such addresses are accessed multiple times per
function invocation and their access patterns can be easily
distinguished from noise (Section 8.1).
5.3 Procedure to Extract Matrix Dimensions
To understand the procedure we use to extract matrix dimen-
sions, we show an example in Figure 5(a), which visualizes
the execution time of a gemm_nn where Loop 1, Loop 2 and
Loop 3 have 5 iterations each. The ﬁgure also shows the
size of the block that each iteration operates on. Note that
the OpenBLAS library handles the last two iterations of each
loop in a special manner. When the last iteration does not
have a full block to compute, rather than assigning a small
block to the last iteration, it assigns two equal-sized small
blocks to the last two iterations. In Figure 5(a), in Loop 1,
the ﬁrst three iterations use R-sized blocks, and each of the
last two use a block of size (R + n mod R)/2. In Loop 2, the
corresponding block sizes are Q and (Q + k mod Q)/2. In
Loop 3, they are P and (P + m mod P)/2.
Figure 5(b) shows additional information for each of the
ﬁrst iterations of Loop 3. Recall that the ﬁrst iteration of Loop
3 is special, as it involves an extra packing operation that
is performed by Loop 4 in Algorithm 1. Figure 5(b) shows
the number of iterations of Loop 4 in each invocation (iter4).
During the execution of the ﬁrst three iterations of Loop 1,
iter4 is (cid:100)R/3UNROLL(cid:101). In the last two iterations of Loop 1,
iter4 is (cid:100)((R + n mod R)/2)/3UNROLL(cid:101), as can be deduced
from Equation 2 after applying OpenBLAS’ special handling
of the last two iterations.
Based on these insights, our procedure to extract m, k, and
n has four steps.
Step 1: Identify the DCG of a Loop 2 iteration and
extract iter1 × iter2. By probing one instruction in each of
itcopy, oncopy, and kernel, we repeatedly obtain the DCG
pattern of a Loop 2 iteration (Figure 4). By counting the
number of such patterns, we obtain iter1 × iter2.
2010    29th USENIX Security Symposium
USENIX Association
itcopyitcopyoncopykernelkernel#pairs=iter4#pairs=iter3-1Figure 5: Visualization of execution time of a gemm_nn where Loop 1, Loop 2, and Loop3 have 5 iterations each (a), and value
of iter4 for each ﬁrst iteration of Loop 3 (b).
Step 2: Extract iter3 and determine the value of m. In
the DCG pattern of a Loop 2 iteration, we count the number of
invocations of the itcopy-kernel pair (Figure 4). This count
plus 1 gives iter3. Of all of these iter3 iterations, all but the
last two execute a block of size P; the last two execute a block
of size (P + m mod P)/2 each (Figure 5(a)). To estimate the
size of this smaller block, we assume that the execution time
of an iteration is proportional to the block size it processes —
except for the ﬁrst iteration which, as we indicated, is different.
Hence, we time the execution of a “normal” iteration of Loop
3 and the execution of the last iteration of Loop 3. Let’s call
the times tnormal and tsmall. The value of m is computed by
adding P for each of the (iter3 - 2) iterations and adding the
estimated number for each of the last two iterations:
m = (iter3 − 2)× P + 2× tsmall
tnormal
× P
Step 3: Extract iter4 and iter2, and determine the value
of k. In the DCG pattern of a Loop 2 iteration (Figure 4), we
count the number of oncopy-kernel pairs, and obtain iter4.