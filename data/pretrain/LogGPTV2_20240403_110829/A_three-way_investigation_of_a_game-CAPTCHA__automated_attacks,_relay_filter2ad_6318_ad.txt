The attacker’s sole motivation behind a captcha relay attack is
to completely avoid the computational overhead and complexity in-
volved in breaking the captcha via automated attacks. A pure form
of a relay attack, as the name suggests, only requires the attacker
to relay the captcha challenge and its response back and forth be-
tween the server and a human-solver. For example, relaying a textual
captcha simply requires the bot to (asynchronously) send the image
containing the captcha challenge to a human-solver and forward the
corresponding response from the solver back to the server. Similarly,
even video-based character recognition captchas [24, 31] can be bro-
ken via a relay attack by taking enough snapshots of the video to
cover the captcha challenge (i.e., the distorted text within the video)
which can be solved by remotely located humans. They can also be
broken by simply taking a video of the incoming frames and relaying
this video to the human-solver.
In contrast, DCG captchas offer some level of resistance to relay
attacks, as we argue in the rest of this section. In making this ar-
gument, we re-emphasize that the primary motivating factors for a
human-solver relay attacker are simplicity, low economical cost and
practicality. As such, a relay attack that requires sophistication (e.g.,
special software, complexity and overhead), is likely not viable in
practice [23].
There appears to be a few mechanisms using which DCG captchas
could potentially be subject to a relay attack. First, if the server sends
the game code to the client (bot), the bot may simply ship the code
off to the human-solver, who can complete the game as an honest
user would. However, in the DCG captcha security model (Section
2.1), the game code is obfuscated and can be enforced to be exe-
cutable only in a speciﬁc domain/host (e.g., only the client machine
challenged with the captcha) authorized by the server using existing
tools7, which will make this attack difﬁcult, if not impossible.
The second possibility, called Stream Relay, is for the bot to em-
ploy a streaming approach, in which the bot can synchronously relay
the incoming game stream from the server over to the solver, and
then relay back the corresponding clicks made by the solver to the
7http://www.kindi.com/swf-encryption.php
8
server. Although the Stream Relay attack might work and its pos-
sibility can not be completely ruled out, it presents one main obsta-
cle for the attacker. Streaming a large number of game frames over
a (usually) slow connection between the bot (e.g., based in the US)
and the solver’s machine (e.g., based in China) may degrade the game
performance (similar to video streaming over slow connections), re-
ducing solving accuracy and increasing response time. Such differ-
ences from an average honest user game play session may further be
used to detect the attack with high accuracies, as shown in our recent
work [22].
These challenges associated with the above relay attack approaches
motivate us to consider another much simpler and more economi-
cal relay attack approach called Static Relay. Here, the bot asyn-
chronously relays a static snapshot of the game to a human-solver
and uses the responses (locations of answer objects and that of the
target objects) from the solver to break the captcha (i.e., drag and
drop the object locations provided by the solver to the target object
locations provided by the solver).
The Static Relay attack approach is very simple and in line with a
traditional captcha attack (and thus represents a viable and economi-
cal relay attack). However, it is expected to have poor success rates.
The intuitive reason behind this is a natural loss of synchronization
between the bot and the solver, due to the dynamic nature of DCG
captchas (moving objects). In other words, by the time the solver
provides the locations of target object and the answer objects within
a challenge image (let us call this the nth frame), the objects them-
selves would have moved in the subsequent, kth, frame (k > n),
making the prior responses from the solver of no use for the bot cor-
responding to the kth frame. Recall that the objects move in random
directions and often collide with other objects and game border, and
therefore it would not be possible for the bot to predict the location
of an object in the kth frame given the locations of that object in the
nth frame (n < k). Such a loss of synchronization will occur due to:
(1) communication delay between the bot and human solver’s ma-
chine, and (2) the manual delay introduced by the solver him/herself
in responding to the received challenge.
A determined Static Relay attacker (bot) against the DCG captcha
can, however, attempt to maximize the level of synchronization with
the solver. Although it may not possible for the attacker to minimize
(ideally, eliminate) the communication delays (especially for bots po-
tentially thousand of miles away from human solvers), it may be pos-
sible to minimize the manual delay via the introduction of carefully
crafted tasks for the human-solver.
In the rest of this section, we
report on an experiment and the results of an underlying user study
in order to evaluate the feasibility of Static Relay attack against our
DCG captcha instances. This novel experiment takes the form of a
reaction time or reﬂex action task for the human-solver. A reaction
time task involves performing some operation as soon as a stimulus is
provided. A common example is an athlete starting a race as quickly
as a pistol is shot. The subject of reaction time has been extensively
studied by psychologists (see Kosinski’s survey [20]).
6.2 Reaction Time Static Relay Experiment
Our hypothesis is that DCG captchas will be resistant to the Static
Relay attack, and so we give the attacker a strong power in the fol-
lowing sense: our tests eliminate the communication delay between
the bot and the human solver, by putting them on the same machine.
The focus of the experiment then shifts towards motivating human-
solvers to perform at their best by employing meaningful interfaces
and by framing the underlying task in a way that is amenable to these
solvers. In particular, since attacker’s goal is to minimize the delay
incurred by the human solver in responding to the challenges, we
model human-solver attack as a reaction time [20] task described be-
low. Our Section 6.3 study further facilitates the attacker with human
solvers having low response times and quick reﬂex actions, such as
youths in their 20s [20].
Experimental Steps: The reaction time Static Relay attack experi-
ment consists of the following steps:
1. A snapshot of the game challenge is extracted by the bot (B),
and the human solver (H) is asked to identify/mark a target ob-
ject for that game challenge (e.g., the dog in the Shapes game).
2. For each target object identiﬁed above, H is asked to identify
one answer object in the snapshot speciﬁc to the game chal-
lenge (e.g., bone for the dog in the Shapes game). However,
since B wants to minimize the delay between the time the chal-
lenge snapshot is given and the response is received from H,
a stimulus will be associated with the snapshot. We make use
of a combination of (1) a visual stimulus (the border across
the game window ﬂashes in Red) and (2) an audio stimulus (a
beeping sound). The task for H is to identify an answer object
in the image as soon as the stimuli are provided.
3. B will emulate the dragging and dropping of the objects based
on the response of H (simply use the pixel values provided by
H as the coordinates of the objects and respective targets).
4. Steps 2-3 are repeated until all answer objects for a given target
object are identiﬁed by H and dragged/dropped by B.
5. Step 1 is repeated until all target objects have been covered.
The experiment succeeds if the captcha game completes success-
fully, i.e., if all answer objects are dragged to their respective targets
by B per input from H.
Experimental Implementation: Our implementation of the above
experiment consists of a user interface (UI) developed in Java that
interacts with the human solver and a bot. The core of this imple-
mentation is designed using an algorithm following which the screen
captures are updated and displayed on the screen as well as an algo-
rithm used to make the mouse drag and drop of the objects.
The game starts by the bot capturing an image of the game chal-
lenge from the browser (i.e., the captcha challenge that the bot re-
ceived from the server) and displays that image in the UI. The solver
is then asked to click on a target object within that image. After se-
lecting the target, the solver is instructed to click a “Next” button,
wait for a ﬂashing and a beep (our stimuli), followed by clicking the
object that matches with that target. Once the solver has clicked on
the object, the bot takes control of the mouse by clicking and drag-
ging the object to the target in the ﬂash game. The solver must be
able to identify and choose the correct object before the object has
moved too far in the ﬂash game displayed in the browser. Whether
the click is successful or not, a new screen capture is retrieved from
the game on the browser. If the solver has chosen the object in time
on the UI, then he/she can pick a new target if one exists by clicking
on the “New Target” button. If the solver has missed clicking on the
object fast enough (i.e., if the click was not successful), the solver
will automatically get another attempt to choose the correct object
followed by the ﬂashing and the beep. Figure 7, Appendix A depicts
the UI of our implementation.
6.3 Static Relay Attack User Study
We now report on a user study of the aforementioned reaction time
relay attack experiment presented in Section 6.2.
6.3.1 Study Design, Goals and Process
In the relay attack study, users were given the task to play our 4
game instances through the UI (described above). The study com-
prised of 20 participants, primarily Computer Science university
students. This sample represents a near ideal scenario for an attacker,
given that young people typically have fast reaction times [20], pre-
sumably optimizing the likelihood of the success of the relay attack.
9
at the relay attack).
2. Robustness: likelihood of not completing the game (relay at-
tack failure), and incorrect drags/drops.
3. User Experience: quantitative SUS ratings and qualitative feed-
The demographics of the participants are shown in Appendix A, Ta-
ble 10. The study design was similar to the one used in our usability
study (Section 4). It comprised of three phases. The pre-study phase
involved registering and brieﬂy explaining the participants about the
protocols of the study. This was followed by collecting participant
demographics and then the participants playing the games via our in-
terface. The participants were told to perform at their best in playing
the games. The post-study phase required the participants to respond
to a set of questions related to their experience with the games they
played as part of the interface, including the SUS questions [8].
Each participant was asked to play the relay versions correspond-
ing to each of the 20 variations of the 4 DCG captcha games as in
Section 3; we used ordering based on Latin squares, as in the usabil-
ity study. The speciﬁc goal of our study was to evaluate the reaction
time experiment UI in terms of the following aspects:
1. Efﬁciency: time taken to complete the games (and succeeding
back from the participants.
4. Reaction time: Time delay between the presentation of the
stimuli and the response from the participant. This is a fun-
damental metric for the feasibility of the attack.
If reaction
time is large, the likelihood of attack success will be low.
Another important goal of our user study was to compare its per-
formance with that of the usability study. If the two differ signiﬁ-
cantly, the relay attack can be detected based on this difference.
Study Results
For each tested game, completion times and errors were automati-
cally logged by the our web-interface software. In addition, we main-
tained “local logs” of the clicks made by the participants on our game
interface to measure the reaction timings.
6.3.2
We present various mechanical data, i.e., time to completion and
error rates as part of the relay attack study. We further analyze the
local logs for the reaction time analysis.
Completion Time and Error Rates: Table 6 shows the time taken
and error rates to play the games for each game type by different par-
ticipants. Unlike our usability study, many game instances timed out,
i.e., the participants were not able to always complete these game in-
stances within the time out of 60s. In this light, we report two types
of timings: (1) successful time, which is the time only corresponding
to the games that the participants were able to complete successfully
within the time out, and (2) overall time, which is the time corre-
sponding to both the game instances completed successfully within
the time out and those which timed out (in which case we consider
the timing to be 60s). The overall time therefore will effectively be
higher.
All games turned out to be quite slow, and much slower than that of
the usability study where the games lasted for less than 10s on an av-
erage (Section 4). As in our usability study, we found that users took
longest to solve the Animals (overall average: 46.51s), whereas the
other games took slightly less time. This might have been due to the
increased semantic load in the Animals game due to the presence of
3 target objects. We observed that the error rates were the highest for
the Animals game (40%), and the least for the Shapes games (9%)
although the corresponding per click error rates were high (56%).
The Ships and Parking games had comparable overall error rates be-
tween 20-30%. We analyzed and further compared the mean time
for different game categories. Using the ANOVA test, the games
showed statistically different behavior from each other (F = 12.85,
p < 0.0001). On further analyzing the data, we found the following
pairs of games to be statistically different from each other: Shapes
and Ships (p = 0.027) and Animals and all other games (p < 0.001).
To analyze errors better, we investigated error rates per click, i.e.
for each drag attempt whether the object being dragged was dropped
at the correct position or not. The error rate per click was the least
for the Ships game (17%), much lower compared to all other games
(50-70%), the latter itself being much higher than observed during
the usability study. This suggests that the server could prevent the
relay attack against Animals, Parking and Shapes games by simply
capping the number of drag/drop attempts.
Table 6: Error rates and completion time per game type
Game
Type
0.26
0.40
0.22
0.09
Error
Rate
Error Rate
Per click
Successful
time (s)
22.25 (5.04)
37.93 (4.91)
20.45 (5.04)
22.94 (1.74)
Overall Time
(s)
mean (std dev) mean (std dev) mean mean
0.17
Ships
30.92 (5.91)
0.65
Animals 46.51 (5.05)
0.66
Parking 28.16 (7.36)
Shapes
26.19 (1.59)
0.56
Reaction Time: We now analyze the reaction time corresponding
to different games during the relay attack experiments. We consider
two types of reaction times, one corresponding to all clicks made
by the participants, and the other corresponding to only the correct
clicks (i.e., those that resulted in a correct drag and drop). The aver-
aged results for the two types of reaction times for each game type are
summarized in Table 7. We can see that the average reaction time (all
clicks) for all game categories was more than 2s and the least for the
Shapes game (2.17s). The average reaction time (correct clicks) is
slightly lower than reaction time (all clicks), but still higher than 1.5s
and still lowest for the Shapes game (1.62s). Neither types of reaction
times change signiﬁcantly across different game categories. ANOVA
test, however, did ﬁnd signiﬁcant difference between the mean reac-
tion time (all clicks) of the four games (F = 13.19, p < 0.01). On
further analyses using paired t-tests with Bonferroni correction, we
found that there was a signiﬁcant difference between the Animals
and Parking games (p < 0.01). Similarly, using the ANOVA test, we
found signiﬁcant difference between the mean reaction time (correct
clicks) (F = 3.24, p < 0.027). Further, we found a signiﬁcant dif-
ference between the Shape and Ship games (p < 0.005) with respect
to mean reaction time (correct clicks).
Table 7: Reaction times per game type
Game
Type
Ships
Animals
Parking
Shapes
Reaction Time
All Clicks (s)
mean (std dev)
2.27 (0.34)
2.58 (0.35)
2.50 (0.51)
2.17 (0.2)
Time
Reaction
Correct Clicks (s)
mean (std dev)
2.06 (0.17)
1.85 (0.23)
2.00 (0.31)