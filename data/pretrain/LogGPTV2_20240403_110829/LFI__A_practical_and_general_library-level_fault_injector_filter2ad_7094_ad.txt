### 更新库及其依赖项以更新库故障配置文件

为了更新库的故障配置文件，我们对库及其依赖项进行了更新。分析时间主要受代码大小（即机器指令数量）的影响。返回码传播到 `eax`（或等效寄存器）中的跳转次数也会产生影响，但由于编译器优化（如常量传播和常量折叠），这一数字通常不超过3次，因此其影响可以忽略不计。

### 准确性

分析器的准确性可以用公式 \( \text{TP} / (\text{TP} + \text{FN} + \text{FP}) \) 表示，其中 TP 代表真阳性、FN 代表假阴性、FP 代表假阳性。真阳性是指正确识别出的错误返回码；假阴性是指未能检测到的可返回错误；假阳性是指报告了实际上无法返回的错误码。

影响准确性的因素包括间接分支和间接调用的数量。如第 3.1 节所述，间接性对静态分析器构成了挑战。此外，库的设计也会影响准确性：如果函数在多次调用之间保持更多状态并基于这些状态决定返回值，假阳性的数量会增加。

对于评估目的，确定真假阳性或阴性的“真实情况”并不容易，因为书面文档不可靠。若要获得精确数据，则需要进行耗时的手动代码检查。我们在一个小库（libpcre，包含20个导出函数）上进行了此类分析，发现准确率为84%（52个真阳性，10个假阴性，0个假阳性）。为了扩大评估范围，我们考虑了三个平台上的另外18个库，并将文档视为“真实情况”。我们为每个测量库编写了文档解析器。虽然这种评估方法不够精确，但它是目前唯一可行的方法。

下表展示了 LFI 分析器在相应二进制文件上运行的结果。在没有访问文档、源代码或人工协助的情况下，LFI 分析器的准确率约为80%-90%。假阴性会导致遗漏故障场景，而假阳性则会使开发人员浪费时间验证注入的故障条件是否实际可能发生。

| 库 | 平台 | 准确率 | 真阳性 (TPs) | 假阴性 (FNs) | 假阳性 (FPs) |
| --- | --- | --- | --- | --- | --- |
| libssl | Windows | 87% | 164 | 18 | 6 |
| libxml2 | Solaris | 81% | 1003 | 138 | 88 |
| libpanel | Solaris | 100% | 23 | 0 | 0 |
| libpctx | Solaris | 83% | 100 | 2 | 0 |
| libldap | Linux | 85% | 368 | 45 | 21 |
| libxml2 | Linux | 80% | 989 | 152 | 102 |
| libXss | Linux | 92% | 12 | 1 | 0 |
| libgtkspell | Linux | 100% | 7 | 0 | 0 |
| libpanel | Linux | 91% | 21 | 2 | 0 |
| libdmx | Linux | 76% | 26 | 8 | 0 |
| libao | Linux | 80% | 12 | 3 | 0 |
| libhesiod | Linux | 100% | 1 | 0 | 0 |
| libnetfilterq | Linux | 92% | 24 | 2 | 0 |
| libcdt | Linux | 100% | 15 | 0 | 0 |
| libdaemon | Linux | 91% | 3 | 0 | 0 |
| libdnssd | Linux | 89% | 5 | 4 | 2 |
| libgimpthumb | Linux | 84% | 31 | 3 | 3 |
| libvorbisfile | Linux | 75% | 133 | 43 | 39 |

### 性能开销

最后一个问题是我们希望解决的是，注入库级故障的过程是否会减慢系统速度，以至于其行为不再具有代表性。如果是这样，测试的价值将会降低。

我们在 Apache httpd 服务器上使用 AB 基准测试测量了 LFI 控制器引入的开销，同时 LFI 对 GNU libc、libapr 和 libaprutil 的调用进行了故障注入。GNU libc 是一个大型库，有1535个导出函数，而组成 Apache Portable Runtime (APR) 的两个库是中等规模，总计超过1,000个函数。我们允许 LFI 生成一个随机故障注入计划，针对 Apache httpd 中调用最多的前10个函数设置10个触发器，前100个函数设置100个触发器，前300个函数设置500个触发器，以及前300个函数设置1,000个触发器（在后两种情况下，同一个函数有多个触发器，对应不同的错误返回值）。在这些实验中，LFI 在评估触发器后总是通过原始库传递调用，以便 Apache 能够正常完成基准测试。每次测试运行1,000个请求。

下表总结了两种不同工作负载（静态 HTML 和 PHP）的结果。后者更具动态性，执行更多的库调用，这意味着触发器需要更频繁地被评估。如表所示，触发器评估引入的开销可以忽略不计。

| 工作负载 | 基线 (无 LFI) | 10 个触发器 | 100 个触发器 | 500 个触发器 | 1,000 个触发器 |
| --- | --- | --- | --- | --- | --- |
| 静态 HTML | 0.151 秒 | 0.156 秒 | 0.156 秒 | 0.158 秒 | 0.159 秒 |
| PHP | 1.51 秒 | 1.53 秒 | 1.53 秒 | 1.57 秒 | 1.60 秒 |

### 相关工作

在软件层面进行故障注入是有吸引力的，因为它不需要昂贵的硬件机制，并且可以用于针对软件堆栈中的各个层次。软件故障注入器可以直接插入应用程序中，或者可以在现有软件层之间进行分层。

软件故障注入在文献中有多种用途，从作为测试设备驱动程序鲁棒性的方法 [1] 到测试通用操作系统 [9,13,11,12,10,3] 以及任务关键系统和实时系统 [18,19]。NFTAPE [20] 是一种故障注入框架，可以注入各种低级故障，主要用于评估分布式系统的可靠性。根据我们的经验，建立低级故障与高层应用程序事件之间的映射并不容易，使得诊断和调试变得繁琐。

我们的工作集中在库级故障注入上，因为我们认为这是进行现实测试的理想层：它是最有可能暴露应用程序在其环境中遇到故障的接口。相关的工作包括 Ballista [14]，这是一个早期系统，通过传递边界值作为参数来测试库或操作系统 API 的鲁棒性。它依赖于领域特定知识来选择能够对测试组件施加压力的参数，并且需要访问相应的函数原型。类似地，HEALERS [7] 搜索可能导致库函数崩溃的参数，并生成包装器来保护易受攻击的函数免受这些病原参数的影响。

我们的工作则是相反的方向：我们通过给应用程序提供库的错误返回值来测试应用程序。这样，我们验证了程序如何正确处理暴露的错误条件，例如，检查当 `malloc` 无法分配内存时，程序如何处理这种情况。库级故障注入的研究兴趣相对较新，据我们所知，首次出现在 FIG [5] 中，这是一种工具，用于验证使用 GNU libc (glibc) 库的应用程序的恢复机制。FIG 仅在对 glibc 的调用中注入故障，并要求可注入的 glibc 错误被硬编码。相比之下，LFI 可以用于任何库，并自动生成执行完整故障注入（包括副作用）的存根。我们还通过 XML 基于的故障描述语言提供了对注入过程的控制，以灵活指定注入方案。

Süsskraut & Fetzer [21] 引入了一个系统，通过库级故障注入找到应用程序问题，并通过修补应用程序来防止这些故障。该系统仅限于 libc，并依赖 man 页面来确定可能的错误返回值。如第 3.1 节和第 3.3 节所示，man 页面有时可能是不正确的，因此在 LFI 中，我们扩展了 man 页面解析方法，结合库二进制文件的静态分析，自动提取错误返回码。[21] 还需要有关函数原型的信息，形式为头文件，以生成相应的包装器并使用系统化的错误注入。LFI 消除了对头文件的需求，并将故障场景的规范与故障注入机制解耦，从而允许更灵活的测试场景（系统化、随机化、自定义等）。

Süsskraut & Fetzer [22] 进一步介绍了一种通过注入系统调用错误（即操作系统和库之间的边界处的故障）并观察其传播到 libc 接口来学习库级错误返回值的技术。LFI 分析器使用二进制文件的静态分析，因为系统调用注入方法仅限于 libc（唯一直接访问系统调用接口的库），并且需要重新编译内核以导出系统调用表。我们认为直接分析二进制文件使 LFI 更具普遍适用性。

### 结论

我们介绍了 LFI，这是一种使基于故障注入的测试更加高效和易于开发者和测试者使用的工具。LFI 在共享库和目标程序之间的边界处注入故障，以验证程序是否正确处理库暴露的故障。LFI 自动从二进制库中提取有关可能的错误返回码及其副作用的信息。基于此故障配置文件，LFI 生成各种故障注入方案，测试者可以直接使用或根据需要进行修改。基于故障配置文件和方案，LFI 合成一个 shim 库，注入所需的故障并记录目标程序的行为。LFI 适用于常见 Linux、Windows 和 Solaris 系统中的共享库，并且每个库的分析时间仅为几秒钟——这使其在实际开发中非常实用。

我们已经表明，即使在没有人工协助和无法访问文档或源代码的情况下，LFI 仍然有用——它能够在广泛的 MySQL 测试套件中增加测试覆盖率，通过执行常规测试未触及的恢复代码路径。在故障注入过程中产生的性能开销可以忽略不计，这意味着在测试期间程序行为仍然保持现实。

### 致谢

感谢 Ankit Singla 对 LFI 的早期贡献。我们还要感谢匿名审稿人以及 Ming Yu 和 Liviu Ciortea 对改进本文的帮助。

### 参考文献

[1] A. Albinet, J. Arlat, and J.-C. Fabre. Characterization of the impact of faulty drivers on the robustness of the Linux kernel. In Intl. Conf. on Dependable Systems and Networks, 2004.

[2] Apache Benchmark (AB). http://httpd.apache.org/docs/2.0/programs/ab.html.

[3] J. Arlat, J.-C. Fabre, M. Rodríguez, and F. Salles. Dependability of COTS microkernel-based systems. IEEE Trans. Comput., 51(2), 2002.

[4] C. Babcock. Sun unlocks MySQL, looks to future Web development. Information Week. Retrieved on 2008-02-27. http://informationweek.com/news/showArticle.jhtml?article-ID=206900327.

[5] P. A. Broadwell, N. Sastry, and J. Traupman. FIG: A prototype tool for online verification of recovery mechanisms. In Workshop on Self-Healing, Adaptive and Self-Managed Systems, New York, NY, 2002.

[6] ELSA. http://www.eecs.berkeley.edu/smc-peak/elkhound/sources/elsa/. Accessed on 15-Mar-2009.

[7] C. Fetzer and Z. Xiao. HEALERS: A toolkit for enhancing the robustness and security of existing applications. In Intl. Conf. on Dependable Systems and Networks, 2003.

[8] http://ftp.gnu.org/gnu/glibc/. Accessed on 15-Mar-2009.

[9] T. Jarboui, J. Arlat, Y. Crouzet, and K. Kanoun. Experimental analysis of the errors induced into Linux by three fault injection techniques. In Intl. Conf. on Dependable Systems and Networks, 2002.

[10] D. Joao and M. Henrique. Multidimensional characterization of the impact of faulty drivers on the operating systems behavior. IEICE Trans. Info. and Sys., 86(12), 2003.

[11] A. Johansson, N. Suri, and B. Murphy. On the impact of injection triggers for OS robustness evaluation. In Intl. Symp. on Software Reliability Engineering, 2007.

[12] A. Johansson, N. Suri, and B. Murphy. On the selection of error model(s) for OS robustness evaluation. In Intl. Conf. on Dependable Systems and Networks, 2007.

[13] K. Kanoun, Y. Crouzet, A. Kalakech, A.-E. Rugina, and P. Rumeau. Benchmarking the dependability of Windows and Linux using postmark workloads. In Intl. Symp. on Software Reliability Engineering, 2005.

[14] P. Koopman, J. Sung, C. Dingman, D. Siewiorek, and T. Marz. Comparing operating systems using robustness benchmarks. In Intl. Symp. on Software Reliability Engineering, 1997.

[15] Pidgin. http://www.pidgin.im.

[16] Pidgin-ticket 8672. http://developer.pidgin.im/ticket/8672.

[17] M. Prasad and T. Chiueh. A binary rewriting defense against stack-based buffer overflow attacks. In USENIX Annual Technical Conference, 2003.

[18] M. Rodriguez, J. Arlat, and J.-C. Fabre. Building SWIFI tools from temporal logic specifications. In Intl. Conf. on Dependable Systems and Networks, 2003.

[19] V. Sieh, O. Tschache, and F. Balbach. VERIFY: Evaluation of reliability using VHDL-models with embedded fault descriptions. In Intl. Symp. on Fault-Tolerant Computing, 1997.

[20] D. T. Stott, B. Floering, Z. Kalbarczyk, and R. K. Iyer. A framework for assessing dependability in distributed systems with lightweight fault injectors. In Intl. Computer Performance and Dependability Symp., 2000.

[21] M. Süsskraut and C. Fetzer. Automatically finding and patching bad error handling. In European Dependable Computing Conference, 2006.

[22] M. Süsskraut and C. Fetzer. Learning library-level error return values from syscall error injection. In European Dependable Computing Conference, 2006.

[23] Sysbench. http://sysbench.sourceforge.net, 2008.