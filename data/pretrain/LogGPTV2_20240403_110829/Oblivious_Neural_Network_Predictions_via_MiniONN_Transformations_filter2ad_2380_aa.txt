title:Oblivious Neural Network Predictions via MiniONN Transformations
author:Jian Liu and
Mika Juuti and
Yao Lu and
N. Asokan
Oblivious Neural Network Predictions via MiniONN
Transformations
Jian Liu
Aalto University
PI:EMAIL
Yao Lu
Aalto University
PI:EMAIL
Mika Juuti
Aalto University
PI:EMAIL
N. Asokan
Aalto University
PI:EMAIL
ABSTRACT
Machine learning models hosted in a cloud service are increasingly
popular but risk privacy: clients sending prediction requests to the
service need to disclose potentially sensitive information. In this
paper, we explore the problem of privacy-preserving predictions:
after each prediction, the server learns nothing about clients‚Äô input
and clients learn nothing about the model.
We present MiniONN, the first approach for transforming an
existing neural network to an oblivious neural network supporting
privacy-preserving predictions with reasonable efficiency. Unlike
prior work, MiniONN requires no change to how models are trained.
To this end, we design oblivious protocols for commonly used opera-
tions in neural network prediction models. We show that MiniONN
outperforms existing work in terms of response latency and mes-
sage sizes. We demonstrate the wide applicability of MiniONN by
transforming several typical neural network models trained from
standard datasets.
CCS CONCEPTS
‚Ä¢ Security and privacy ‚Üí Privacy-preserving protocols;
KEYWORDS
privacy; machine learning; neural network predictions; secure two-
party computation
1 INTRODUCTION
Machine learning is now used extensively in many application
domains such as pattern recognition [10], medical diagnosis [25]
and credit-risk assessment [3]. Applications of supervised machine
learning methods have a common two-phase paradigm: (1) a train-
ing phase in which a model is trained from some training data, and
(2) a prediction phase in which the trained model is used to predict
categories (classification) or continuous values (regression) given
some input data. Recently, a particular machine learning framework,
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS ‚Äô17, October 30-November 3, 2017, Dallas, TX, USA
¬© 2017 Copyright held by the owner/author(s). Publication rights licensed to Associa-
tion for Computing Machinery.
ACM ISBN 978-1-4503-4946-8/17/10...$15.00
https://doi.org/10.1145/3133956.3134056
neural networks (sometimes referred to as deep learning), has gained
much popularity due to its record-breaking performance in many
tasks such as image classification [37], speech recognition [20] and
complex board games [35].
Machine learning as a service (MLaaS) is a new service paradigm
that uses cloud infrastructures to train models and offer online pre-
diction services to clients. While cloud-based prediction services
have clear benefits, they put clients‚Äô privacy at risk because the
input data that clients submit to the cloud service may contain
sensitive information. A naive solution is to have clients download
the model and run the prediction phase on client-side. However,
this solution has several drawbacks: (1) it becomes more difficult
for service providers to update their models; (2) the trained model
may constitute a competitive advantage and thus requires confiden-
tiality; (3) for security applications (e.g., spam or malware detection
services), an adversary can use the model as an oracle to develop
strategies for evading detection; and (4) if the training data con-
tains sensitive information (such as patient records from a hospital)
revealing the model may compromise privacy of the training data
or even violate regulations like the Health Insurance Portability
and Accountability Act of 1996 (HIPAA).
A natural question to ask is, given a model, whether is it possible
to make it oblivious: it can compute predictions in such a way that
the server learns nothing about clients‚Äô input, and clients learn
nothing about the model except the prediction results. For general
machine learning models, nearly practical solutions have been pro-
posed [6, 14, 15, 58]. However, privacy-preserving deep learning
prediction models, which we call oblivious neural networks (ONN),
have not been studied adequately. Gilad-Bachrach et al. [28] pro-
posed using a specific activation function (‚Äúsquare‚Äù) and pooling op-
eration (mean pooling) during training so that the resulting model
can be made oblivious using their CryptoNets framework. Cryp-
toNets transformations result in reasonable accuracy but incur high
performance overhead. Very recently, Mohassel and Zhang [44]
also proposed new activation functions that can be efficiently com-
puted by cryptographic techniques, and use them in the training
phase of their SecureML framework. What is common to both ap-
proaches [28, 44] is that they require changes to the training phase
and thus are not applicable to the problem of making existing neural
models oblivious.
In this paper, we present MiniONN (pronounced minion), a prac-
tical ONN transformation technique to convert any given neural
network model (trained with commonly used operations) to an ONN.
Session C3:  Machine Learning PrivacyCCS‚Äô17, October 30-November 3, 2017, Dallas, TX, USA619We design oblivious protocols for operations routinely used by neu-
ral network designers: linear transformations, popular activation
functions and pooling operations. In particular, we use polynomial
splines to approximate nonlinear functions (e.g., sigmoid and tanh)
with negligible loss in prediction accuracy. None of our protocols
require any changes to the training phase of the model being trans-
formed. We only use lightweight cryptographic primitives such as
secret sharing and garbled circuits in online prediction phase. We
also introduce an offline precomputation phase to perform request-
independent operations using additively homomorphic encryption
together with the SIMD batch processing technique.
Our contributions are summarized as follows:
‚Ä¢ We present MiniONN, the first technique that can trans-
form any common neural network model into an obliv-
ious neural network without any modifications to the
training phase (Section 4).
‚Ä¢ We design oblivious protocols for common operations
in neural network predictions (Section 5). In particular,
we make nonlinear functions (e.g., sigmoid and tanh)
amenable for our ONN transformation with a negligible
loss in accuracy (Section 5.3.2).
‚Ä¢ We build a full implementation of MiniONN and demon-
strate its wide applicability by using it to transform neural
network models trained from several standard datasets
(Section 6). In particular, for the same models trained from
the MNIST dataset [38], MiniONN performs significantly
better than previous work [28, 44] (Section 6.1).
‚Ä¢ We analyze how model complexity impacts both predic-
tion accuracy and computation/communication over-
head of the transformed ONN. We discuss how a neural
network designer can choose the right tradeoff between pre-
diction accuracy and overhead. (Section 7).
2 BACKGROUND AND PRELIMINARIES
We now introduce the machine learning and cryptographic prelim-
inaries (notation we use is summarized in Table 1).
ùíÆ
Server
ùíû
Client
X = {x1, ...}
Input matrix for each layer
W = {w1, ...} Weight matrix for each layer
B = {b1, ...}
Bias matrix for each layer
Y = {y1, ...}
Output matrix for each layer
z = {z1, ...}
Final predictions
ùíÆ‚Äôs share of the dot-product triple
u
ùíû‚Äôs share of the dot-product triple
v
Plaintext space
ZN
return 1 if x ‚â• y, return 0 if x < y
compar e (x, y )
Additively homomorphic encryption/decryption
E () / D ()
Public/Private key
pk / sk
ÀÜx
E (pk, x )
E (pk, [x1, ...])
‚äï
Addition between two ciphertexts
or a plaintext and a ciphertext
‚äñ
Subtraction between two ciphertexts
or a plaintext and a ciphertext
‚äó Multiplication between
(cid:72)x
a plaintext and a ciphertext
Table 1: Notation table.
2.1 Neural networks
A neural network consists of a pipeline of layers. Each layer receives
input and processes it to produce an output that serves as input
to the next layer. Conventionally, layers are organized so that the
bottom-most layer receives input data (e.g., an image or a word) and
the top-most layer outputs the final predictions. A typical neural
network1 processes input data in groups of layers, by first applying
linear transformations, followed by the application of a nonlinear
activation function. Sometimes a pooling operation is included to
aggregate groups of inputs.
We will now briefly describe these operations from the perspec-
tive of transforming neural networks to ONNs.
2.1.1 Linear transformations. The commonest linear transfor-
mations in neural networks are matrix multiplications and addi-
tions:
y := W ¬∑ x + b,
(1)
where x ‚àà Rl√ó1 is the input vector, y ‚àà Rn√ó1 is the output, W
‚àà Rn√ól is the weight matrix and b ‚àà Rn√ó1 is the bias vector.
Convolution is a type of linear transformation, which computes
the dot product of small ‚Äúweight tensors‚Äù (filters) and the neigh-
borhood of an element in the input. The process is repeated, by
sliding each filter by a certain amount in each step. The size of the
neighborhood is called window size. The step size is called stride. In
practice, for efficiency reasons, convolution is converted into ma-
trix multiplication and addition as well [18], similar to equation 1,
except that input and bias vector are matrices: Y := W ¬∑ X + B.
Dropout and dropconnect are types of linear transformations,
where multiplication is done elementwise with zero-one random
masks [30].
Batch normalization is an adaptive normalization method [30]
that shifts outputs y to amenable ranges. During prediction, batch
normalization manifests as a matrix multiplication and addition.
2.1.2 Activation functions. Neural networks use nonlinear trans-
formations of data ‚Äì activation functions ‚Äì to model nonlinear rela-
tionships between input data and output predictions. We identify
three common categories:
- Piecewise linear activation functions. This category of functions
can be represented as a set of n linear functions within specific
ranges, each of the type fi (y) = aiy + bi ,y ‚àà [yi ,yi +1], where
yi and yi +1 are the lower and upper bounds for the range. This
category includes the activation functions:
Identity function (linear): f (y) = [yi]
Rectified Linear Units (ReLU): f (y) = [max(0,yi )]
Leaky ReLU: f (y) = [max(0,yi ) + a min(0,yi )]
Maxout (n pieces): f (y) = [max(y1, . . . ,yn )]
- Smooth activation functions. A smooth function has continuous
derivatives up to some desired order over some domain. Some
commonly used smooth activation functions are:
Sigmoid (logistic): f (y) = [
Hyperbolic tangent (tanh): f (y) = [ e2yi ‚àí1
e2yi +1]
1http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.
html
1+e‚àíyi
]
1
Session C3:  Machine Learning PrivacyCCS‚Äô17, October 30-November 3, 2017, Dallas, TX, USA620Softplus: f (y) = [log(eyi + 1)]
The sigmoid and tanh functions are closely related [30]:
tanh(x ) = 2 ¬∑ si–¥moid (2x ) ‚àí 1.
(2)
They are collectively referred to as sigmoidal functions.
- Softmax. Softmax is defined as:
f (y) = [ eyi(cid:80)
]
j eyj
It is usually applied to the last layer to compute a probability
distribution in categorical classification. However, in prediction
phase, usually it is sufficient to use argmax over the outputs of the
last layer to predict the most likely outcome.
2.1.3 Pooling operations. Neural networks also commonly use
pooling operations that arrange input into several groups and ag-
gregate inputs within each group. Pooling is commonly done by
calculating the average or the maximum value among the inputs
(mean or max pooling). Convolution and pooling operations are only
used if the input data has spatial structure (e.g., images, sounds).
2.1.4 Commonly used neural network operations. As discussed
in Section 2.1.1, all common linear transformations reduce to matrix
multiplications and additions in the prediction phase. Therefore
it is sufficient for an ONN transformation technique to support
making matrix multiplications and additions oblivious.
To get an idea of commonly used activation functions, consider
five top performing neural networks1 in the MNIST [38] and CIFAR-
10 [36] datasets. Collectively they support the following activation
functions: ReLU [39, 51, 57], leaky ReLU [32, 55], maxout [17, 43]
and tanh [19]. In addition, sigmoidal activation functions are com-
monly used in language modeling. Finally, as we saw in Section 2.1.3
common pooling operations are mean and max pooling.
We thus argue that for an ONN transformation technique
to be useful in practice, it should support all of the above
commonly used neural network operations. We describe these
in Sections 3 to 5.
Note that although softmax is a popular operation used in the
last layer, it can be left out of an ONN [28] (e.g., the input to the
softmax layer can be returned to the client) because its application
is order-preserving and thus will not change the prediction result.
2.2 Cryptographic preliminaries