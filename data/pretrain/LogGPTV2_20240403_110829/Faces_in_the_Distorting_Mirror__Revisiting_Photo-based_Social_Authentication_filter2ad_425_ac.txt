but other common friends are, the user might still be able
to infer the correct answer. When we focus on photos where
the friend was absent, and no other faces were contained
either, the results remain almost identical. However, the
simple and medium photos that were ﬂagged as NotUseful
present a measurable decrease (6.5%, 16.3%). As can be seen
in Figure 3(c), these categories have a much higher ratio
of photos that contain other faces compared to the diﬃcult
category. Thus, while photos might not contain any faces,
the content can assist the user in inferring the correct answer
or excluding suggestions until left with a plausible one.
Exclusion and Inference. To explore the eﬀect of other
people’s faces on users excluding suggestions or inferring the
answer, we plot Figure 5, which provides interesting insight
on the strategy users follow for identifying the depicted friend
based on the presence of the friend’s (columns with F) and
other users’ (columns with O) faces. Users tend to remember
PortraitLandscapeObjectsAnimalsTextArtPhotos (%)0102030405060708090100simplemediumdifficultInClearOutClearInUnclearOutUnclearAbsentPhotos (%)0102030405060708090100simplemediumdifficultInOutInOutOnlyNobodyPhotos (%)0102030405060708090100simplemediumdifficultInPhotoRememberRelevantNoOneElseNotUsefulPhotos (%)0102030405060708090100simplemediumdifficultInPhotoNoOneElseNotUsefulRelevantRememberPhotos (%)0102030405060708090100TextAnimalsArtObjectsLandsc.PortraitRemember_FRemember_ORelevant_FRelevant_ONoOther_FNoOther_OPhotos (%)0102030405060708090100OnlyFriendOthersNobodyClearUnclearAbsentthe photo when the depicted friend’s face was either Unclear
or Absent (Remember F), as is the case for users inferring the
correct answer (Relevant F). Users also tend to remember
photos where other people (common friends) or no people
are present at all (landscapes, pets and important objects).
Furthermore, in Relevant O we can see that users infer the
correct friend almost equally from photos where Nobody
(47.9%) is present (due to relevant objects and pets) or
where Other (44.5%) people are present (people that the user
knows are friends with the requested friend).
When excluding suggestions (NoOther F), the absence of
the friend’s face (Absent) or its poor quality (Unclear) have
a similar eﬀect. However, the presence of other people’s
faces has a major impact, as it accounts for 61.4% of the
photos in NoOther O. This is a strong indication that when
users are presented with photos of unknown individuals they
tend to follow an approach of excluding suggested friends
until left with a plausible answer.
If the users know the
depicted people are acquaintances of the requested friend,
they select Relevant. In the case of unknown individuals,
they exclude suggestions of close friends and select less known
contacts that have a higher probability of being friends with
people unknown to the user. Combined with the information
that can be extracted from the other photos contained in a
challenge page, this approach can be very eﬀective, as users
correctly answered 88.5% of all the pages that contained a
photo for which NoOther friend matched.
4. SECURE SOCIAL AUTHENTICATION
Based on the results of our user study we proceed with
establishing guidelines for creating a secure SA mechanism
that is robust against the attacks we presented.
4.1 Tag Selection
The goal is to ﬁlter out faces that can be of use to ad-
versaries that employ face recognition software, and select
a subset of the remaining tags that have a high probabil-
ity of containing a face. We use two criteria for selecting
tags after being analyzed by face recognition software: a
high conﬁdence level for containing a human face, and a
false recognizability ﬂag, i.e., medium tags. While our user
study demonstrated that users are highly eﬀective even if
the friend’s face is not in the photo (i.e., diﬃcult photos),
we do not use such photos in our SA challenges.
Even though we build our selection process using face.com,
our tag selection can be completed with any accurate face
recognition software. Once all the tags have been analyzed,
the system selects the tags that could not be recognised.
The OSN can also use several types of information to
narrow down the set from which friends are selected for the
SA challenges. This set needn’t be small, but in the order
of 200-300 friends, that have a minimum level of interaction
with the user. All friends from this set must have the same
chance of being selected, and all suggestions must be from
this set, so as not to aid the attacker by limiting the number
of suggestions that seem plausible as answers.
4.2 Tag and Photo Transformations
To defend against the image comparison attack, tagged
areas should not be identical to the areas in the original
photos, to prevent the attacker from mapping them to the
photos in the collection. Our approach blends the faces the
user has to identify with the faces on a “background” photo.
Figure 6: An example output photo, with rotation,
opacity, and perspective transformations performed.
The other faces have been blurred for privacy.
If we simply overlay the tagged areas containing the faces
onto a new photo, an adversary could still resort to the image
comparison attack. To prevent this, we perform a sequence
of transformations on the extracted areas.
Tag transformation: First, we rotate the face, a trans-
formation that can impact face detection. Second, we edit
the tag’s alpha level (a) to make it translucent and blend
it with the underlying faces (0 ≤ a ≤ 1, where 0 is fully
transparent). Thus, the tag will not contain any parts from
the photos in their initial form.
Photo transformation: Each challenge contains one
photo of N friends, with M tagged faces for each friend. We
select a background photo that contains at least N ∗ M faces,
and overlay the existing faces with the processed tags we
created in the previous step. We then apply a perspective
transformation, which is challenging even for complex feature
or template matching techniques. According to Gauglitz et
al. [11], “perspective distortion is the hardest challenge”. The
perspective transformation we perform is variable by P , with
P denoting the ratio by which the bottom of the photo is
“compressed” from each side; e.g., for P = 3, the bottom is
compressed from both the left and right by 1/3 of the photo.
The user is presented with N menus, each one containing the
name of one of the depicted friends, along with the names
of S − 1 other friends. The user is required to correctly
identify the N depicted friends. To demonstrate that familiar
faces remain recognizable after our transformations, Figure 6
shows an example output, with a = 0.6 and P = 3.2, for two
well-know politicians1.
Prototype Implementation. We implemented a pro-
totype, which comprises of a Facebook app for the data
collection process, and a back end for the photo processing.
We implemented the back-end in Python, using SimpleCV
and OpenCV for the face detection and image processing.
To create a SA challenge, the back-end ﬁrst selects N
distinct friends of the target user. For each friend, it ﬁnds
M random tags of that friend, and fetches the corresponding
photos. The tags are extracted, transformed and overlayed
on a random background photo, which is then also trans-
formed. The tag processing part randomly rotates and applies
an alpha opacity ﬁlter that ensures that none of the orig-
inal pixels of the tag are preserved. This is implemented
1Challenge solution: Barack Obama, Vladimir Putin.
rotations
CCOEFF
CCORR
SQDIFF
time_photo
None
7 (30◦)
13 (15◦)
19 (10◦)
37 (5◦)
time_rotation
12.8% 11.0% 10.6%
67.8% 36.4% 43.8%
91.0% 60.0% 68.8%
95.2% 67.4% 77.8%
97.8% 75.8% 90.8%
6.61
46.23
87.85
130.65
244.68
2.24
2.19
2.18
-
Table 4: Attack success rate of each algorithm for
various rotation approaches, time required (sec) for
each algorithm to process one rotated version, and
the total time for all rotated versions of a photo.
with SimpleCV’s blit() function, which takes the back-
ground photo, tag image, position of the overlay and opacity
percentage as input, and returns a collage image. The rota-
tion is implemented with rotate(), the perspective trans-
formation is based on the getPerspectiveTransform() and
warpaffine() functions of OpenCV. We set a time-window
of one minute for users to solve the challenge.
4.3 Experimental Evaluation
Threat model. We assume the attacker has knowledge
of our system, and has created a collection containing all
the photos of the victim and his friends. We also assume
he can apply the same categorization to photos as we do,
and identify medium faces. Furthermore, as each tag in the
challenge has a suggestion of 6 users, the attacker will only
compare the photo to the tags of those users. In our user
study, we found that each user’s friend has ∼12 medium
tags on average. Thus, in our experiments, for each tag, the
attacker simply has to identify which tag out of a set of 72
(12 for each suggested user) is contained in the photo, to
pass the challenge.
Image comparison attack. We employ 3 diﬀerent tem-
plate matching methods: the normalized versions of the cor-
relation coeﬃcient (CCOEFF), cross correlation (CCORR) and
squared diﬀerence (SQDIFF) algorithms. To verify their ac-
curacy, we ﬁrst run the attack against a set of 500 photos,
where we have overlayed a tag but have not performed any
transformations. All three algorithms correctly identify the
500 tags, requiring ∼6.89 seconds per photo.Compared to
the simplistic pixel-comparison attack from Section 2, the
template matching algorithms identify every tag but have
two orders of magnitude processing overhead. However, the
simplistic attack cannot handle the transformations.
First, we measure the impact of rotating the tag. We
create a set of 500 photos each containing a medium tag that
has been randomly rotated within [-90◦, 90◦]. In the ﬁrst
attack scenario, the attacker does not perform any rotations
on the photo and simply selects the tag out of the 72 with
the highest similarity score with the photo. In the other
scenarios the attacker performs a series of 30◦, 15◦, 10◦ and
5◦ rotations ( in the range [-90◦, 90◦] ). For each photo, the
attacker selects the best match among all the tags and all
the rotations. While the template matching algorithms can
handle certain rotations, the results of our experiment shown
in Table 4, demonstrate that eﬀectiveness is greatly increased
when multiple rotated versions of the photo are processed
before selecting the best-matching tag. CCOEFF yields the best
Figure 7: Attack success, against transparency and
transparency+rotation transformations.
Figure 8: Attack success against perspective (P),
and all transformations combined (RTP), for a = 0.8.
results, with a success rate of up to 97.8% when the attack
performs 5◦ rotations. However, the increased accuracy
comes at a cost, as it has a linear computational overhead
for the attacker. While processing the 15◦ rotations requires
an order of magnitude more computational eﬀort compared
to no rotations, we will use that to test the robustness of our
system (unless otherwise stated), as it is suﬃciently eﬀective
and 3 times faster than the attack with 5◦ rotations.
Next, we compare the combined impact of the rotation
and transparency transformations. We create ﬁve sets of 500
challenges, each with a diﬀerent alpha level (transparency)
transformation, and run the attack without rotations. We
then create another ﬁve sets of 500 challenges each, with a
randomly rotated tag and diﬀerent alpha level transforma-
tions. As can be seen in Figure 7, the CCOEFF algorithm is the
most eﬀective in all scenarios, proving to be the most eﬀec-
tive against both transparency and rotation transformations.
Nonetheless, we can see that transparency has a signiﬁcant
impact, dropping the success rate of all three algorithms
to 6 − 10% when a = 0.4. Since such a transparency level
may prove to be diﬃcult for users to identify the tags, we
employ more conservative transparency transformations in
the following experiments. Even with an 0.8 alpha level,
which is a mild transformation, the success rate of two of the
algorithms drops to less than 50%.
To compare the combined impact of all our transforma-
tions, we create two sets of 500 challenges each, that contain
photos with one tag; one set with tag transformations and
one with tag and photo transformations. We also place the
tags randomly in the photos, to explore the signiﬁcance of
the “background” photo. We maintain a constant 0.8 al-
pha level (which is a very conservative transformation and
remains easily identiﬁable to humans), and experiment by
 0 10 20 30 40 50 60 70 80 90 1000.80.70.60.50.4Identified photos (%)alpha value (transparency transformation)CCOEFFCCORRSQDIFFCCOEFF+rotCCORR+rotSQDIFF+rotTags
alpha
CCOEFF
CCORR
SQDIFF Time (sec)
2
3
0.8
0.7
0.6
0.8
0.7
0.6
0.0%
0.0%
0.2%
0.0%
0.0%
0.0%
2.1%
1.9%
1.5%
0.0%
0.0%
0.4%
1.8%
1.6%
1.0%
0.0%
0.4%
0.0%
397.7
400.6
401.5
663.6
675.0
695.9
Table 5: Attack success with 5◦ rotations against
challenges with 2 tags, for P = 3.2.
Figure 10: Faces detected before and after rota-
tion+transparency. Point labels correspond to the
number of photos. The line shows the X = Y axis.