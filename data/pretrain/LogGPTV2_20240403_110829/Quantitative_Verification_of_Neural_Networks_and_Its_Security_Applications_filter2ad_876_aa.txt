title:Quantitative Verification of Neural Networks and Its Security Applications
author:Teodora Baluta and
Shiqi Shen and
Shweta Shinde and
Kuldeep S. Meel and
Prateek Saxena
Quantitative Verification of Neural Networks
And Its Security Applications
Teodora Baluta
PI:EMAIL
National University of Singapore
Shiqi Shen
PI:EMAIL
National University of Singapore
Shweta Shinde∗
PI:EMAIL
University of California, Berkeley
9
1
0
2
n
u
J
5
2
]
R
C
.
s
c
[
1
v
5
9
3
0
1
.
6
0
9
1
:
v
i
X
r
a
Kuldeep S. Meel
PI:EMAIL
National University of Singapore
Prateek Saxena
PI:EMAIL
National University of Singapore
ABSTRACT
Neural networks are increasingly employed in safety-critical do-
mains. This has prompted interest in verifying or certifying logically
encoded properties of neural networks. Prior work has largely fo-
cused on checking existential properties, wherein the goal is to
check whether there exists any input that violates a given property
of interest. However, neural network training is a stochastic process,
and many questions arising in their analysis require probabilistic
and quantitative reasoning, i.e., estimating how many inputs sat-
isfy a given property. To this end, our paper proposes a novel and
principled framework to quantitative verification of logical prop-
erties specified over neural networks. Our framework is the first
to provide PAC-style soundness guarantees, in that its quantitative
estimates are within a controllable and bounded error from the
true count. We instantiate our algorithmic framework by building
a prototype tool called NPAQ that enables checking rich properties
over binarized neural networks. We show how emerging security
analyses can utilize our framework in 3 concrete point applications:
quantifying robustness to adversarial inputs, efficacy of trojan at-
tacks, and fairness/bias of given neural networks.
1 INTRODUCTION
Neural networks are witnessing wide-scale adoption, including
in domains with the potential for a long-term impact on human
society. Examples of these domains include criminal sentencing [1],
drug discovery [102, 103], self-driving cars [16], aircraft collision
avoidance systems [59], robots [13], and drones [48]. While neural
networks achieve human-level accuracy in several challenging tasks
such as image recognition [54, 63, 95] and machine translation [11,
65, 94], studies show that these systems may behave erratically in
the wild [9, 14, 17, 18, 36, 39, 40, 76–78, 87, 98, 100].
Consequently, there has been a surge of interest in the design
of methodological approaches to verification and testing of neural
networks. Early efforts focused on qualitative verification wherein,
given a neural network N and property P, one is concerned with
determining whether there exists an input I to N such that P is
violated [25, 29, 32, 56, 60, 61, 73, 80, 82, 88, 93]. While such certi-
fiability techniques provide value, for instance in demonstrating
the existence of adversarial examples [50, 77], it is worth recalling
that the designers of neural network-based systems often make a
statistical claim of their behavior, i.e., a given system is claimed to
satisfy properties of interest with high probability but not always.
∗Part of the research done while working at National University of Singapore.
Therefore, many analyses of neural networks require quantitative
reasoning, which determines how many inputs satisfy P.
It is natural to encode properties as well as conditions on inputs
or outputs as logical formulae. We focus on the following formu-
lation of quantitative verification: Given a set of neural networks
N and a property of interest P defined over the union of inputs
and outputs of neural networks in N, we are interested in estima-
tion of how often P is satisfied. In many critical domains, client
analyses often require guarantees that the computed estimates be
reasonably close to the ground truth. We are not aware of any prior
approaches that provide such formal guarantees, though the need
for quantitative verification has recently been recognized [86, 105].
Security Applications. Quantitative verification enables many
applications in security analysis (and beyond) for neural networks.
We present 3 point applications in which the following analysis
questions can be quantitatively answered:
• Robustness: How many adversarial samples does a given
neural network have? Does one neural network have more
adversarial inputs compared to another one?
• Trojan Attacks: A neural network can be trained to classify
certain inputs with “trojan trigger” patterns to the desired
label. How well-poisoned is a trojaned model, i.e., how
many such trojan inputs does the attack successfully work
for?
• Fairness: Does a neural network change its predictions
significantly when certain input features are present (e.g.,
when the input record has gender attribute set to “female”
vs. “male”)?
Note that such analysis questions boil down to estimating how
often some property over inputs and outputs is satisfied. Estimating
counts is fundamentally different from checking whether a satisfi-
able input exists. Since neural networks are stochastically trained,
the mere existence of certain satisfiable inputs is not unexpected.
The questions above checks whether their counts are sufficiently
large to draw statistically significant inferences. Section 3 formu-
lates these analysis questions as logical specifications.
Our Approach. The primary contribution of this paper is a new
analysis framework, which models the given set of neural networks
N and P as set of logical constraints, φ, such that the problem of
quantifying how often N satisfies P reduces to model counting over
φ. We then show that the quantitative verification is #P-hard. Given
the computational intractability of #P, we seek to compute rigorous
estimates and introduce the notion of approximate quantitative
verification: given a prescribed tolerance factor ε and confidence
parameter δ, we estimate how often P is satisfied with PAC-style
guarantees, i.e., computed result is within a multiplicative (1 + ε)
factor of the ground truth with confidence at least 1 − δ.
Our approach works by encoding the neural network into a
logical formula in CNF form. The key to achieving soundness guar-
antees is our new notion of equi-witnessability, which defines a
principled way of encoding neural networks into a CNF formula F,
such that quantitative verification reduces to counting the satisfy-
ing assignments of F projected to a subset of the support of F. We
then use approximate model counting on F, which has seen rapid
advancement in practical tools that provide PAC-style guarantees
on counts for F. The end result is a quantitative verification proce-
dure for neural networks with soundness and precision guarantees.
While our framework is more general, we instantiate our analy-
sis framework with a sub-class of neural networks called binarized
neural networks (or BNNs) [57]. BNNs are multi-layered percep-
trons with +/-1 weights and step activation functions. They have
been demonstrated to achieve high accuracy for a wide variety
of applications [64, 70, 84]. Due to their small memory footprint
and fast inference time, they have been deployed in constrained
environments such as embedded devices [64, 70]. We observe that
specific existing encodings for BNNs adhere to our notion of equi-
witnessability and implement these in a new tool called NPAQ1.
We provide proofs of key correctness and composability properties
of our general approach, as well as of our specific encodings. Our
encodings are linear in the size of N and P.
Empirical Results. We show that NPAQ scales to BNNs with
1 − 3 internal layers and 50 − 200 units per layer. We use 2 standard
datasets namely MNIST and UCI Adult Census Income dataset. We
encode a total of 84 models, each with 6, 280 − 51, 410 parameters,
into 1, 056 formulae and quantitatively verify them. NPAQ encodes
properties in less than a minute and solves 97.1% formulae in a 24-
hour timeout. Encodings scale linearly in the size of the models, and
its running time is not dependent on the true counts. We showcase
how NPAQ can be used in diverse security applications with case
studies. First, we quantify the model robustness by measuring how
many adversarially perturbed inputs are misclassified, and then
the effectiveness of 2 defenses for model hardening with adversar-
ial training. Next, we evaluate the effectiveness of trojan attacks
outside the chosen test set. Lastly, we measure the influence of 3
sensitive features on the output and check if the model is biased
towards a particular value of the sensitive feature.
Contributions. We make the following contributions:
• New Notion. We introduce the notion of approximate quan-
titative verification to estimate how often a property P is
satisfied by the neural net N with theoretically rigorous
PAC-style guarantees.
• Algorithmic Approach, Tool, & Security Applications. We pro-
pose a principled algorithmic approach for encoding neu-
ral networks to CNF formula that preserve model counts.
We build an end-to-end tool called NPAQ that can handle
BNNs. We demonstrate security applications of NPAQ in
quantifying robustness, trojan attacks, and fairness.
1The name stands for Neural Property Approximate Quantifier. The tool will be
released as open-source post-publication.
• Results. We evaluate NPAQ on 1, 056 formulae derived
from properties over BNNs trained on two datasets. We
show that NPAQ presently scales to BNNs of over 50, 000
parameters, and evaluate its performance characteristics
with respect to different user-chosen parameters.
2 PROBLEM DEFINITION
Definition 2.1 (Specification (φ)). Let N = { f1, f2, . . . , fm} be
a set of m neural nets, where each neural net fi takes a vector
of inputs xi and outputs a vector yi, such that yi = fi(xi). Let
P : {x∪y} → {0, 1} denote the property P over the inputs x =
xi
m
m
over N as φ(x, y) = ( m
and outputs y =
i =1
i =1
yi. We define the specification of property P
i =1
(yi = fi(xi)) ∧ P(x, y)).
We show several motivating property specifications in Section 3.
For the sake of illustration here, consider N = { f1, f2} be a set of
two neural networks that take as input a vector of three integers
and output a 0/1, i.e., f1 : Z3 → {0, 1} and f2 : Z3 → {0, 1}. We
want to encode a property to check the dis-similarity between f1
and f2, i.e., counting for how many inputs (over all possible inputs)
do f1 and f2 produce differing outputs. The specification is defined
over the inputs x = [x1, x2, x3], outputs y1 = f1(x) and y2 = f2(x)
as φ(x1, x2, x3, y1, y2) = (f1(x) = y1 ∧ f2(x) = y2 ∧ y1 (cid:44) y2).
Given a specification φ for a property P over the set of neural
nets N, a verification procedure returns r = 1 (SAT) if there exists a
satisfying assignment τ such that τ |= φ, otherwise it returns r = 0
(UNSAT). A satisfying assignment for φ is defined as τ : {x ∪ y} →
{0, 1} such that φ evaluates to true, i.e., φ(τ) = 1 or τ |= φ.
While the problem of standard (qualitative) verification asks
whether there exists a satisfying assignment to φ, the problem of
quantitative verification asks how many satisfying assignments or
models does φ admit. We denote the set of satisfying assignments
for the specification φ as R(φ) = {τ : τ |= φ}.
Definition 2.2 (Neural Quantitative Verification (NQV)). Given a
specification φ for a property P over the set of neural nets N, a
quantitative verification procedure, NQV(φ), returns the number
of satisfying assignments of φ, r = |R(φ)|.
It is worth noting that |R(φ)| may be intractably large to compute
via naïve enumeration. For instance, we consider neural networks
with hundreds of bits as inputs for which the unconditioned input
space is 2|x|. In fact, we prove that quantitative verification is #P-
hard, as stated below.
Theorem 2.3. NQV(φ) is #P-hard, where φ is a specification for a
property P over binarized neural nets.
Our proof is a parsimonious reduction of model counting of CNF
formulas, #CNF, to quantitative verification of binarized neural
networks. We show how an arbitrary CNF formula F can be trans-
formed into a binarized neural net fi and a property P such that for
a specification φ for P over N = { fi}, it holds true that R(F) = R(φ).
See Appendix 10.2 for the full proof.
Remark 1. The parsimonious reduction from #CNF to NQV im-
plies that fully polynomial time randomized approximation schemes,
including those based on Monte Carlo, cannot exist unless NP=RP.
The computational intractability of #P necessitates a search for
relaxations of NQV. To this end, we introduce the notion of an ap-
proximate quantitative verifier that outputs an approximate count
within ϵ of the true count with a probability greater than 1 − δ.
Definition 2.4 (Approximate NQV ((ϵ, δ)-NQV)). Given a specifi-
cation φ for a property P over a set of neural nets N, 0 < ϵ ≤ 1
and 0 < δ ≤ 1, an approximate quantitative verification procedure
(ϵ, δ)-NQV(φ, ϵ, δ) computes r such that Pr[(1 + ϵ)−1|R(φ)| ≤ r ≤
(1 + ϵ)|R(φ)|] ≥ 1 − δ.
|R(φ)|
2|x|
The security analyst can set the “confidence” parameter δ and
the precision or “error tolerance” ϵ as desired. The (ϵ, δ)-NQV defi-
nition specifies the end guarantee of producing estimates that are
statistically sound with respect to chosen parameters (ϵ, δ).
Connection to computing probabilities. Readers can naturally
interpret |R(φ)| as a measure of probability. Consider N to be a set
of functions defined over input random variables x. The property
specification φ defines an event that conditions inputs and outputs
to certain values, which the user can specify as desired. The mea-
sure |R(φ)| counts how often the event occurs under all possible
values of x. Therefore,
is the probability of the event defined
by φ occurring. Our formulation presented here computes |R(φ)|
weighting all possible values of x equally, which implicitly assumes
a uniform distribution over all random variables x. Our framework
can be extended to weighted counting [20, 33–35], assigning differ-
ent user-defined weights to different values of x, which is akin to
specifying a desired probability distributions over x. However, we
consider this extension as a promising future work.
3 SECURITY APPLICATIONS
We present three concrete application contexts which highlight
how quantitative verification is useful to diverse security analyses.
The specific property specifications presented here derived directly
from recent works, highlighting that NPAQ is broadly applicable
to analysis problems actively being investigated.
Robustness. An adversarial example for a neural network is an in-
put which under a small perturbation is classified differently [50, 96].
The lower the number of adversarial examples, the more “robust”
the neural network. Early work on verifying robustness aimed at
checking whether adversarial inputs exist. However, recent works
suggest that adversarial inputs are statistically “not surprising” [9,
38, 100] as they are a consequence of normal error in statistical
classification [26, 46, 47, 69]. This highlights the importance of an-
alyzing whether a statistically significant number of adversarial
examples exist, not just whether they exist at all, under desired
input distributions. Our framework allows the analyst to specify a
logical property of adversarial inputs and quantitatively verify it.
Specifically, one can estimate how many inputs are misclassified
by the net (f ) and within some small perturbation distance k from
a benign sample (xb) [19, 76, 77], by encoding the property P1 in
our framework as:
|x|
j=1
P1(x, y, xb, k) =
(xb[j] ⊕ x[j]) ≤ k ∧ yb (cid:44) y
(P1)
As a concrete usage scenario, our evaluation reports on BNNs for
image classification (Section 6.2). Even for a small given input
k
(cid:1), which is too large to check for misclassification one-by-
(say m bits), the space of all inputs within a perturbation of k bits
is(cid:0)m
one. NPAQ does not enumerate and yet can estimate adversarial
input counts with PAC-style guarantees (Section 6.2). As we permit
larger perturbation, as expected, the number of adversarial samples
monotonically increase, and NPAQ can quantitatively measure how
much. Further, we show how one can directly compare robustness
estimates for two neural networks. Such estimates may also be used
to measure the efficacy of defenses. Our evaluation on 2 adversarial
training defenses shows that the hardened models show lesser
robustness than the plain (unhardened) model. Such analysis can
help to quantitatively refute, for instance, claims that BNNs are
intrinsically more robust, as suggested in prior work [41].
Trojan Attacks. Neural networks, such as for facial recognition
systems, can be trained in a way that they output a specific value,
when the input has a certain “trojan trigger” embedded in it [45, 68].
The trojan trigger can be a fixed input pattern (e.g., a sub-image)
or some transformation that can be stamped on to a benign image.
One of the primary goals of the trojan attack is to maximize the
number of trojaned inputs which are classified as the desired target
output, lattack. NPAQ can quantify the number of such inputs for a
trojaned network, allowing attackers to optimize for this metric. To
do so, one can encode the set of trojaned inputs as all those inputs
x which satisfy the following constraint for a given neural network
f , trigger t, lattack and the (pixel) location of the trigger M:
P2(x, y, t, lattack, M) = (x[j] = t[j]) ∧ y = lattack, j ∈ M
(P2)
Section 6.3 shows an evaluation on BNNs trained on the MNIST
dataset. Our evaluation demonstrates that the attack accuracy on