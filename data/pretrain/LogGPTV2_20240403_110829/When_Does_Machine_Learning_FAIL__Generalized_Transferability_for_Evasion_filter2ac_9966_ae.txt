by 32% and 62%, respectively. The attacker uses the
cropped images for training and testing the classiﬁer, as
USENIX Association
27th USENIX Security Symposium    1309
#
1
2
3
4
5
6
7
8
9
10
11
∆
SR %
¯τD
32%
62%
67/3
86/7
shallow 99/10
narrow
82/20
35000
50000
45000
50000
18%
41%
62%
93/18
80/80
90/18
96/19
80/4
80/34
80/80
0.070
0.054
0.035
0.027
0.032
0.026
0.029
0.034
0.011
0.022
0.026
∆
39%
66%
35000
50000
45000
50000
PDR
SR %
FAIL:Unknown features
0.93/0.96/0.96
87/63/67
84/71/74
0.94/0.95/0.95
FAIL:Unknown algorithm
0.97/0.97/0.96
0.96/0.97/0.96
shallow 83/65/68
narrow
75/67/72
FAIL:Unavailable training set
0.97/0.96/0.96
73/68/76
78/70/74
0.97/0.97/0.97
FAIL:Unknown training set
82/69/74
0.98/0.96/0.96
0.95/0.96/0.96
70/62/68
FAIL:Read-only features
0.97/0.97/0.97
0.97/0.97/0.97
0.97/0.97/0.96
80/70/72
80/71/76
83/78/79
25%
50%
75%
Instances
∆
SR %
PDR
Instances
8/4/10
8/4/9
17/14/15
20/16/17
17/16/14
18/16/15
16/10/15
17/8/17
19/16/15
18/16/13
16/16/12
109066
327199
79/3/5
77/12/13
0.99/0.99/1.00
0.99/0.99/1.00
73/50/53
51/50/15
SGD
42/33/42
dSVM 38/35/48
0.99/0.99/0.99
0.99/0.99/0.99
65/50/31
78/50/61
8514
85148
8514
43865
851
8514
85148
69/27/27
50/50/50
0.90/0.99/0.99
0.99/0.99/0.99
57/50/42
77/50/61
53/21/24
36/29/39
0.93/0.99/1.00
1.04/0.99/0.99
62/50/49
100/50/87
73/12/13
49/16/17
32/32/32
0.67/0.99/1.00
0.90/0.99/1.00
0.99/0.99/0.99
50/50/10
61/50/47
79/50/57
Table 3: JSMA on the image classiﬁer
Table 4: StingRay on the image classiﬁer
Table 5: StingRay on the malware classiﬁer
Tables 3, 4, 5: FAIL analysis of the two applications. For each JSMA experiment, we report the attack SR (perceived/potential), as well as the
mean perturbation ¯τD introduced to the evasion instances. For each StingRay experiment, we report the SR and PDR (perceived/actual/potential), as
well as statistics for the crafted instances on successful attacks (mean/median/standard deviation). ∆ represents the variation of the FAIL dimension
investigated.
well as for crafting instances. On the victim classiﬁer,
the cropped part of the images is added back without al-
tering the perturbations.
With limited knowledge along this dimension (#1-2)
the perceived success remains high, but the actual SR
is very low. This suggests that the evasion attacks are
very sensitive in such scenarios, highlighting a potential
direction for future defenses.
We then model an attacker with limited Algorithm
knowledge, possessing a similar architecture, but with
smaller network capacity. For the shallow network (#3)
the attacker network has one less hidden layer; the nar-
row architecture (#4) has half of the original number of
neurons in the fully connected hidden layers. Here we
observe that the shallow architecture (#3) renders almost
all attacks as successful on the attacker. However, the
potential SR on the victim is higher for the narrow setup
(#4). This contradicts claims in prior work [37], which
state that the used architecture is not a factor for success.
Instance knowledge. In #5 we simulate a scenario in
which the attacker only knows 70% of the victim training
set, while #7-8 model an attacker with 80% of the train-
ing set available and an additional subset of instances
sampled from the same distribution.
These results might help us explain the contradiction
with prior work.
Indeed, we observe that a robust at-
tacker classiﬁer, trained on a sizable data set, reduces the
SR to 19%, suggesting that the attack success sharply
declines with fewer victim training instances available.
In contrast, in [37] the SR remains at over 80% because
of the non-random data-augmentation technique used to
build the attacker training set. As a result, the attacker
model is a closer approximation of the victim one, im-
pacting the analysis along the A dimension.
Experiments #9–11 model the case where the attacker
has limited Leverage and is unable to modify some
of the instance features. This could represent a region
where watermarks are added to images to check their in-
tegrity. We simulate it by considering a border in the im-
age from which the modiﬁed pixels would be discarded,
corresponding to the attacker being able modify to 18%,
41% and 62% of an image respectively. We observe a
signiﬁcant drop in transferability, although #11 shows
that the SR is not reduced with leverage above a certain
threshold.
StingRay on the image classiﬁer. We now evaluate the
poisoning attack described in 4.2 under the same scenar-
ios deﬁned above. Table 4 summarizes our results. In
contrast to evasion, the table reports the SR, PDR, and
the number of poison instances needed. Here, besides
the perceived and potential statistics, we also report the
actual SR and PDR (as reﬂected on the victim when trig-
gering only the attacks perceived successful).
For limited Feature knowledge, we observe that the
perceived SR is over 84% but the actual success rate
drops signiﬁcantly on the victim. However, the actual
SR for #2 is similar to the white-box attacker (#6), show-
ing that features derived from the exterior regions of an
image are less speciﬁc to an instance. This suggests that
although reducing feature knowledge decreases the ef-
fectiveness of StingRay, the speciﬁcity of some known
features may still enable successful attacks.
Along the A dimension, we observe that both archi-
tectures allow the attacker to accurately approximate the
deep space distance between instances. While the per-
ceived SR is overestimated, the actual SR of these attacks
is comparable to the white-box attack, showing that ar-
1310    27th USENIX Security Symposium
USENIX Association
(a) Limited Feature knowledge.
(b) Limited Leverage.
Figure 3: Example of original and crafted images. Images in the left
panel are crafted with 39% and 66% of features unknown. In the right
panel, the images are crafted with 100% and 50% leverage.
chitecture secrecy does not signiﬁcantly increase the re-
silience against these attacks. The open-source neural
network architectures readily available for many of clas-
siﬁcation tasks would aid the adversary. Along the I di-
mension, in #5, the PDR is increased because the smaller
available training set size prevents them from training
a robust classiﬁer.
In the white-box attack #6 we ob-
serve that the perceived, actual and potential SRs are dif-
ferent. We determined that this discrepancy is caused
by documented nondeterminism in the implementation
framework. This affects the order in which instances
are processed, causing variance on the model parameters,
which in turns reﬂects on the effectiveness of poisoning
instances. Nevertheless, we observe that the potential
SR is higher in #5, even though the amount of available
information is larger in #6. This highlights the beneﬁt of
a ﬁne-grained analysis along all dimensions, since the
attack success rate may not be monotonic in terms of
knowledge levels.
Surprisingly, we observe that the actual SR for #8,
where the attacker has more training instances at their
disposal, is lower than for #7. This is likely caused by
the fact that, with a larger discrepancy between the train-
ing sets of the victim and the attacker classiﬁer, the at-
tacker is more likely to select base instances that would
not be present in the victim training set. After poison-
ing the victim, the effect of crafted instances would not
be bootstrapped by the base instances, and the attacker
fails. The results suggest that the attack is sensitive to
the presence of speciﬁc pristine instances in the training
set, and variance in the model parameters could mitigate
the threat. However, determining which instances should
be kept secret is subject for future research.
Limited Leverage increases the actual SR beyond the
white-box attack. When discarding modiﬁed pixels, the
overall perturbation is reduced. Thus, it is more likely
that the poison samples will become collectively incon-
spicuous, increasing the attack effectiveness. Figure 3 il-
lustrates some images crafted by constrained adversaries.
The FAIL analysis results show that the perceived
PDR is generally an accurate representation of the ac-
tual value, making it easy for the adversary to assess the
instance inconspicuousness and indiscriminate damage
caused by the attack. The attacks transfer surprisingly
well from the attacker to the victim, and a signiﬁcant
number of failed attacks would potentially be successful
if triggered on the victim. We observe that limited lever-
age allows the attacker to localize their strategy, crafting
attack instances that are even more successful than the
white-box attack.
StingRay on the malware classiﬁer. In order to evalu-
ate StingRay in the FAIL setting on the malware classi-
ﬁer, we trigger all 1,717 attacks described in 4.2 along 11
dimensions. Table 5 summarizes the results. Experiment
#6 corresponds to the white-box attacker.
Experiments #1–2 look at the case where Features are
unknown to the adversary.
In this case, the surrogate
model used to craft poison instances includes only 20%
and 60% of the features respectively. Surprisingly, the
attack is highly ineffective. Although the attacker per-
ceives the attack as successful in some cases, the clas-
siﬁer trained on the available feature subspace is a very
inaccurate approximation of the original one, resulting
in an actual SR of at most 12%. These results echo
these from evasion, indicating that features secrecy might
prove a viable lead towards effective defenses. We also
investigate adversaries with various degrees of knowl-
edge about the classiﬁcation Algorithm. Experiment #3
trains a linear model using the Stochastic Gradient De-
scent (SGD) algorithm, and in #4 (dSVM), the hyper-