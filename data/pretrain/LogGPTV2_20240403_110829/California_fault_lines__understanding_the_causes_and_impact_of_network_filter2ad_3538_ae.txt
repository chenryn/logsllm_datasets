send an announcement.
324Cause
Events
Avg
Med
Time to repair
Hardware
Power
External
Software
Conﬁguration
Other
Unknown
20% 95 m
6% 93 m
5 m
18 m
15% 61 m 4.6 m
4 m
32% 10 m
5 m
1 m
6 m
12% 46 m
5% 52 m
6 m
9%
Table 5: Major causes of failure according to administrator
announcements, ordered by median time to repair.
Cause
Notices
Scheduled
Impacting
Hardware
Power
External
Software
Other
Conﬁguration
Unknown
25%
20%
15%
12%
12%
8%
7%
65%
4%
29%
84%
69%
91%
0%
71%
99%
95%
99%
82%
45%
99%
Table 6: Breakdown of administrator notices by failure cause.
Sites Events
Pw Hw Sw N/A
AS2152
Other AS
41
19
361
147
28
6
12
5
39
26
287
105
Table 7: Summary of the CENIC network partitions.
Cause
Avg
Med
95%
Power
Hardware
Software
N/A
5 h
8.2 h
6 m
8 h
33 h
20.6 m
32 m
3.7 d
2.7 m 13.9 m
32 m
3.7 d
75th percentile
Median
25th percentile
3h
2h
1h
30m
1m
2005
2006
2007
2008
2009
Figure 14: Annualized link downtime, in seconds, in the DC
network, by year. From top to bottom, the lines show the 75th
percentile, median, and 25th percentile.
ure 14 shows the annualized link downtime in the DC network for
each year in the measurement period. We expected to ﬁnd a trend in
the basic statistics presented in Section 6.2. In fact, we found that
these performance indicators varied from year to year with no dis-
cernible trend. The year 2006, and to a lesser extent 2008, stands
out for having lower link downtimes than the preceding and fol-
lowing years. The annualized number of failures per link varied
accordingly, with the lowest median of 0.0 in 2006 and the highest
median of 6.0 in 2005.
Investigating further, we ﬁnd that the distribution of causes stud-
ied in Section 6.3.2 varies as well. Several network-wide events
are responsible for a signiﬁcant variation in the number of link fail-
ures. Most notably, software-related link failures and conﬁguration
changes were a signiﬁcant source of link failures in some years and
not others. The three vertical bands in Figure 6 due to network-
wide upgrades and conﬁguration changes (see Section 6.1) had a
signiﬁcant impact on the median number of failures and median
link downtime in 2005, 2007, and 2009. Longitudinal trends, if
present, are thus dwarfed by major but infrequent events.
Table 8: Duration of network partition for all isolating events.
7. CONCLUSION
As discussed in Section 5, the only type of impact we can infer
are isolating network partitions. Table 7 presents the 508 isolat-
ing failures we identify in the failure log, separates them into net-
works with and without their own AS, and provides the cause an-
notation if available. Interestingly, the breakdown of failure causes
for partition events is somewhat different than for all events—here,
software failures dominate. Table 8 summarizes the distribution of
times to repair for different causes of isolating failures, independent
of the AS involved. As with non-isolating events, power and hard-
ware events have signiﬁcantly longer durations than those caused
by software failures.
6.5 Time dynamics
Like most complex systems, the CENIC network is continually
evolving. The most signiﬁcant change, starting in 2008, was to des-
ignate some DC routers as “core” routers and the rest as ”access”
routers. This resulted in the decommissioning of 235 links and the
introduction of 432 new links. A natural question, then, is whether
the network qualities examined earlier have changed as well. Fig-
In this paper we present a methodology for inferring and analyz-
ing the link failure history of a network absent dedicated monitor-
ing infrastructure. In particular, we show that existing “low qual-
ity” data sources already widely gathered in production networks—
syslog, router conﬁgs and operational mailing lists—can be oppor-
tunistically combined to reconstruct topology, dynamic state and
failure causes. Using this approach we have analyzed ﬁve years
of link failure history from the CENIC network, a large California
Internet service provider, and both validated existing understand-
ings about failure (e.g., the prevalence of link ﬂapping) and doc-
umented less appreciated issues (e.g., the large amounts of down-
time attributable to 3rd-party leased line problems). We believe our
overall approach is fairly general and should be straightforward to
adapt to a wide variety of IP networks.
Acknowledgments
The authors would like to thank Brain Court, Darrel Newcomb,
Jim Madden, and our shepherd, Aman Shaikh, for their advice and
suggestions. This work is supported in part by a grant from the
UCSD Center for Networked Systems.
3258. REFERENCES
[1] Internet2. http://www.internet2.edu.
[2] A. Akella, J. Pang, B. Maggs, S. Seshan, and A. Shaikh. A
comparison of overlay routing and multihoming route control. In
Proceedings of SIGCOMM, pages 93–106, 2004.
[3] M. Balakrishnan and A. Reibman. Characterizing a lumping heuristic
for a Markov network reliability model. In Proceedings of FTCS,
pages 56–65, 1993.
[4] P. Baran. On distributed communications networks. IEEE
Transactions on Communications Systems, 12(1):1–9, March 1964.
[5] K. Claffy, T. Monk, and D. McRobb. Internet tomography. Nature,
Web Matters, January 1999.
[6] M. Coates, R. Castro, and R. Nowak. Maximum likelihood network
topology identiﬁcation from edge-based unicast measurements. In
Proceedings of SIGMETRICS, pages 11–20, 2002.
[7] Corporation for Education Network Initiatives in California. The
CalREN network.
http://www.cenic.org/calren/index.html.
[8] C. Cranor, T. Johnson, O. Spataschek, and V. Shkapenyuk.
Gigascope: a stream database for network applications. In
Proceedings of SIGMOD, pages 647–651, 2003.
[9] M. Dahlin, B. B. V. Chandra, L. Gao, and A. Nayate. End-to-end
WAN service availability. IEEE/ACM Transactions on Networking,
11(2):300–313, April 2003.
[10] A. Dhamdhere, R. Teixeira, C. Dovrolis, and C. Diot. NetDiagnoser:
Troubleshooting network unreachabilities using end-to-end probes
and routing data. In Proceedings of CoNEXT, 2007.
[11] N. Dufﬁeld. Network tomography of binary network performance
characteristics. IEEE Transactions on Information Theory,
52(12):5373–5388, 2006.
[12] N. Feamster and H. Balakrishnan. Detecting BGP conﬁguration
faults with static analysis. In Proceedings of NSDI, pages 43–56,
2005.
[13] A. Feldmann. Netdb: IP network conﬁguration debugger/database.
Technical report, AT&T, 1999.
[14] A. Feldmann, A. Greenberg, C. Lund, N. Reingold, and J. Rexford.
Netscope: Trafﬁc engineering for IP networks. IEEE Network,
14(2):11–19, 2000.
[15] K. P. Gummadi, H. V. Madhyastha, S. D. Gribble, H. M. Levy, and
D. Wetherall. Improving the reliability of Internet paths with one-hop
source routing. In Proceedings of OSDI, pages 13–13, 2004.
[16] Y. Huang, N. Feamster, and R. Teixeira. Practical issues with using
network tomography for fault diagnosis. Computer Communication
Review, 38(5):53–57, October 2008.
[17] R. R. Kompella, J. Yates, A. Greenberg, and A. C. Snoeren. IP fault
localization via risk modeling. In Proceedings of NSDI, pages 57–70,
2005.
[18] R. R. Kompella, J. Yates, A. Greenberg, and A. C. Snoeren.
Detection and localization of network black holes. In Proceedings of
INFOCOM, pages 2180–2188, 2007.
[19] C. Labovitz, A. Ahuja, and F. Jahanian. Experimental study of
Internet stability and backbone failures. In Proceedings of FTCS,
pages 278–285, 1999.
[20] K. Levchenko, G. M. Voelker, R. Paturi, and S. Savage. XL: An
efﬁcient network routing algorithm. In Proceedings of SIGCOMM,
pages 15–26, 2008.
[21] C. Lonvick. RFC 3164: The BSD syslog protocol, August 2001.
[22] Y. Mao, H. Jamjoom, S. Tao, and J. M. Smith. NetworkMD:
Topology inference and failure diagnosis in the last mile. In
Proceedings of IMC, pages 189–202, 2007.
[23] A. Markopoulou, G. Iannaccone, S. Bhattacharyya, C.-N. Chuah,
Y. Ganjali, and C. Diot. Characterization of failures in an operational
IP backbone network. Transactions on Networking, 16(4), 2008.
[24] V. N. Padmanabhan, S. Ramabhadran, S. Agarwal, and J. Padhye. A
study of end-to-end web access failures. In Proceedings of CoNEXT,
pages 1–13, 2006.
[25] V. D. Park and M. S. Corson. A performance comparison of the
temporally-ordered routing algorithm and ideal link-state routing. In
Proceedings of ISCC, pages 592–598, 1998.
[26] V. Paxson. End-to-end routing behavior in the Internet. In
Proceedings of SIGCOMM, pages 25–38, 1996.
[27] A. Shaikh, C. Isett, A. Greenberg, M. Roughan, and J. Gottlieb. A
case study of OSPF behavior in a large enterprise network. In
Proceedings of IMC, pages 217–230, 2002.
[28] A. U. Shankar, C. Alaettino˘glu, I. Matta, and K. Dussa-Zieger.
Performance comparison of routing protocols using MaRS: Distance
vector versus link-state. ACM SIGMETRICS Performance Evaluation
Review, 20(1):181–192, June 1992.
[29] Shrubbery Networks, Inc. RANCID.
http://www.shrubbery.net/rancid/.
[30] L. Tang, J. Li, Y. Li, and S. Shenker. An investigation of the
Internet’s IP-layer connectivity. Computer Communications,
32(5):913–926, 2009.
[31] University of Oregon. University of Oregon Route Views project.
http://www.routeviews.org.
[32] F. Wang, Z. M. Mao, J. Wang, L. Gao, and R. Bush. A measurement
study on the impact of routing events on end-to-end Internet path
performance. In Proceedings of SIGCOMM, pages 375–386, 2006.
[33] D. Watson, F. Jahanian, and C. Labovitz. Experiences with
monitoring OSPF on a regional service provider network. In
Proceedings ICDCS, pages 204–212, 2003.
[34] W. Xu, L. Huang, A. Fox, D. Paterson, and M. Jordan. Detecting
large-scale system problems by mining console logs. In Proceedings
of SOSP, 2009.
[35] M. Zhang, C. Zhang, V. Pai, L. Peterson, and R. Wang. PlanetSeer:
Internet path failure monitoring and characterization in wide-area
services. In Proceedings of OSDI, pages 167–182, 2004.
APPENDIX
While the primary focus of this paper is the CENIC network, we
also applied our methodology to the Internet2 Network to establish
a point of comparison [1]. As an added beneﬁt, this allowed us to
assess the effort required to apply our methodology to a different
network. Here we brieﬂy describe the Internet2 Network and its
measurement data, how we adapt the methodology for this dataset,
and highlights of the results.
The Internet2 Network. The Internet2 network (AS 11537) con-
sists of nine Juniper T640 routers located in nine major US cities.
These routers are interconnected with either 1-Gb/s or 10-Gb/s
Ethernet links. The Internet2 network most closely resembles the
CENIC HPR network, however HPR is more experimental in na-
ture, while the Internet2 network is a production network. Like the
CENIC network, the Internet2 network uses the IS-IS protocol. The
network has been in operation since 1996 and serves 330 member
institutions. We obtained router conﬁguration snapshots and syslog
message logs for the period 01/01/2009 to 12/31/2009. Internet2
operational announcements were also available, but because these
required additional manual labor and were not essential to the anal-
ysis, we did not use them. We also did not use available IS-IS LSA
logs because our methodology does not use this data source (it was
unavailable for the CENIC network).
Adapting the methodology. The main challenge to processing
Internet2 data is dealing with a different data format: the Internet2
network uses Juniper routers while the CENIC network uses Cisco
routers. This required writing about 300 lines of new parsing code.
Brief summary. Table 4 in Section 6.2 shows the number of fail-
ures, downtime, and link time to repair in the Internet2 network. Its
performance is somewhere between the DC network and HPR net-
work with respect to number of failures and annual link downtime,
with a longer time to repair. The Internet2 network also differs from
the CENIC networks in the shape of the distribution of the time be-
tween failures (not shown): the CENIC networks have many short
failures—indicative of ﬂapping—while the Internet2 network has
no such bias (the 25th percentile time between failures in the latter
is over a minute and about 10 seconds in the DC network).
326