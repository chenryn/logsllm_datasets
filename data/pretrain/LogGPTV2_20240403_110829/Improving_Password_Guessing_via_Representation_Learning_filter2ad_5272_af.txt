arXiv preprint arXiv:1701.00160, 2016.
[30] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David
Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
Generative Adversarial Nets. In NIPS, pages 2672–2680, 2014.
[31] Palash Goyal and Emilio Ferrara. Graph Embedding Techniques,
Applications, and Performance: A Survey. Elsevier Knowledge-Based
Systems, 151:78–94, 2018.
[32] Alex Graves. Generating Sequences with Recurrent Neural Networks.
arXiv preprint arXiv:1308.0850, 2013.
[33] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin,
and Aaron C Courville. Improved Training of Wasserstein GANs. In
NIPS, pages 5767–5777, 2017.
[34] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep
In CVPR, pages 770–778,
Residual Learning for Image Recognition.
2016.
[35] Briland Hitaj, Paolo Gasti, Giuseppe Ateniese, and Fernando Perez-Cruz.
PassGAN: A Deep Learning Approach for Password Guessing. In ACNS,
pages 217–237, 2019.
[36] Diederik P Kingma and Max Welling. Auto-encoding Variational Bayes.
arXiv preprint arXiv:1312.6114, 2013.
[37] Yang Li and Tao Yang. Word Embedding for Understanding Natural
Language: A Survey. In Springer Guide to Big Data Applications, pages
83–104. 2018.
[38] Junyu Luo, Yong Xu, Chenwei Tang, and Jiancheng Lv. Learning
Inverse Mapping by Autoencoder based Generative Adversarial Nets.
In International Conference on Neural Information Processing, pages
207–216. Springer, 2017.
[39] Laurens van der Maaten and Geoffrey Hinton. Visualizing Data using
t-SNE. Journal of Machine Learning Research, 9:2579–2605, 2008.
[40] Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly,
Ian Goodfel-
low, and Brendan Frey. Adversarial Auto-Encoders. arXiv preprint
arXiv:1511.05644, 2015.
[41] Philip Marquardt, Arunabh Verma, Henry Carter, and Patrick Traynor.
(sp)iPhone: Decoding Vibrations From Nearby Keyboards Using Mobile
Phone Accelerometers. In ACM CCS, pages 551–562, 2011.
[42] William Melicher, Blase Ur, Sean M Segreti, Saranga Komanduri, Lujo
Bauer, Nicolas Christin, and Lorrie Faith Cranor. Fast, Lean, and
Accurate: Modeling Password Guessability using Neural Networks. In
USENIX Security Symposium, pages 175–191, 2016. GitHub Repo:
https://tinyurl.com/y9o7jdd8.
[43] Shakir Mohamed and Balaji Lakshminarayanan. Learning in Implicit
Generative Models. arXiv preprint arXiv:1610.03483, 2016.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:27:44 UTC from IEEE Xplore.  Restrictions apply. 
1395
[44] Robert Morris and Ken Thompson. Password Security: A Case History.
Communications of the ACM, 22(11):594–597, 1979.
[45] Arvind Narayanan and Vitaly Shmatikov. Fast Dictionary Attacks on
Passwords using Time-space Tradeoff. In ACM CCS, pages 364–372,
2005.
[46] Bijeeta Pal, Tal Daniel, Rahul Chatterjee, and Thomas Ristenpart.
Beyond Credential Stufﬁng: Password Similarity Models using Neural
Networks. In IEEE S&P, pages 1–18, 2019.
[47] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and
Alexei A Efros. Context Encoders: Feature Learning by Inpainting. In
IEEE CVPR, pages 2536–2544, 2016.
[48] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised Rep-
resentation Learning with Deep Convolutional Generative Adversarial
Networks. arXiv preprint arXiv:1511.06434, 2015.
[49] Masashi Sugiyama, Matthias Krauledat, and Klaus-Robert M ˜Aˇzller.
Covariate Shift Adaptation by Importance Weighted Cross Validation.
Journal of Machine Learning Research, 8(May):985–1005, 2007.
[50] Ilya Sutskever, James Martens, and Geoffrey E Hinton. Generating Text
with Recurrent Neural Networks. In ICML, pages 1017–1024, 2011.
[51] Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard
preprint
Wasserstein Auto-Encoders.
arXiv
Schoelkopf.
arXiv:1711.01558, 2017.
[52] Blase Ur, Fumiko Noma, Jonathan Bees, Sean M Segreti, Richard Shay,
Lujo Bauer, Nicolas Christin, and Lorrie Faith Cranor.
I Added ‘!’at
the End to Make It Secure: Observing Password Creation in the Lab.
In SOUPS, pages 123–140, 2015.
[53] Blase Ur, Sean M Segreti, Lujo Bauer, Nicolas Christin, Lorrie Faith
Cranor, Saranga Komanduri, Darya Kurilova, Michelle L Mazurek,
William Melicher, and Richard Shay. Measuring Real-world Accuracies
In USENIX Security
and Biases in Modeling Password Guessability.
Symposium, pages 463–481, 2015.
[54] Martin Vuagnoux and Sylvain Pasini. Compromising Electromagnetic
In USENIX Security
Emanations of Wired and Wireless Keyboards.
Symposium, pages 1–16, 2009.
[55] Ding Wang, Zijian Zhang, Ping Wang, Jeff Yan, and Xinyi Huang.
Targeted Online Password Guessing: An Underestimated Threat. In ACM
CCS, pages 1242–1254, 2016.
[56] Matt Weir, Sudhir Aggarwal, Breno De Medeiros, and Bill Glodek.
Password Cracking using Probabilistic Context-free Grammars. In IEEE
S&P, pages 391–405, 2009.
[57] Tom White.
Sampling Generative Networks.
arXiv preprint
arXiv:1609.04468, 2016.
[58] Roman V Yampolskiy. Analyzing User Password Selection Behavior
for Reduction of Password Space. In ICCST, pages 109–115, 2006.
be obtained through the injection of inductive bias during the
learning process.
Focusing on the AE (Section II-B2), we can indeed induce
structure preferences in the latent space organization through
regularizations during training. For instance, we can easily
reduce the length-based clustering phenomenon described
above by acting on the character deletion process used in
Section II-B2. In the normal case, we learn a latent representa-
tion by training the auto-encoder at reconstructing artiﬁcially
mangled passwords, where each character in the input string
is removed with a certain probability. Differently, we can
delete a group of k continuous characters given a randomly
chosen starting position i. For instance, with k = 5, a
password “jimmy1991” can become “jimm*****” with i = 4;
otherwise “*****991” with i = 0. Intuitively, the generator
collects in the same location passwords that share common
substrings, regardless of their length. For instance, given the
mangled password “jimmy*****”, the generator should be
able to recover the passwords “jimmy”, “jimmyjimmy” and
“jimmy123”, eventually forcing their latent representations to
be close to each other.
As an example, we compare passwords sampled from
CWAE trained with different approaches, namely, using the
character deletion approach discussed in Section II-B2 (here,
referred to as Simple) and using the group deletion approach
discussed above (referred to as Mask). Table A.1 reports
password sampled around the pivot “iloveyou1” for the two
CWAEs. Compared to Simple, passwords sampled from the
Mask model tend to have heterogeneous lengths which are
arbitrarily different from the one of the pivot.
TABLE A.1
PASSWORDS SAMPLED AROUND THE PIVOT “iloveyou1” FOR TWO CWAES
TRAINED WITH DIFFERENT REGULARIZATION. THE SAME VALUE OF σ IS
USED FOR BOTH MODELS.
INDUCING PECULIAR PASSWORD LATENT ORGANIZATIONS
APPENDIX A
VIA INDUCTIVE BIAS
Given the absence of precise external bias, the generative
models used to learn the latent password representation is free
to choose arbitrary spatial arrangements among passwords. In
the general case, our generators learn the latent representation
that best supports the extremely general generative task im-
posed during the training. However, this may not be optimal.
For instance, the latent spaces learned by our technique tend
to keep passwords with similar length very close to each
other. The reason is that the length of a password is modeled
as one of the core explanatory factors [18] by the latent
representation. As a result, passwords with different lengths
are distributed far from each other, which is good for DPG
but undesirable in other cases. For instance, it may be better
to generate passwords that share speciﬁc substrings, but that
do not have comparable length.
Luckily, this type of specialization is possible within our
frameworks. Our deep learning approach is highly versatile,
and password organizations that present a peculiar feature can
Simple
iloveyou13
iloveyou12
iloveYou1
iLoveyou1
iloveyou*
Iloveyou1
iloheyou1
ilOveyou1
iloveyou11a
iloveyou1a
Mask
iloveyou1234
iloveyou14
iloveyou12ao
iloveyou1222
iloveyou17a
iloveyou12arham
iloveyou14om
iloveyou123o
iloveyou1444
iloveyou12a4mom1
APPENDIX B
GUESS GENERATION PERFORMANCE
For the sake of completeness, we report performance anal-
ysis of our methods and other probabilistic models, namely,
OMEN and PCFG. We exclude FLA [42] in this comparison,
as to the best of our knowledge, its enumeration algorithm do
not produce sorted password in a stream, i.e., password must
be ﬁrst pre-computed and then used for the guessing attack.
Moreover, during our experiments, generating 1010 guesses
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:27:44 UTC from IEEE Xplore.  Restrictions apply. 
1396
required more than two weeks on an NVIDIA Quadro P6000
and Intel Xeon CPU E5645.12
Benchmarks are performed without considering the latency
induced from the hash computation; only the guess-generation
cost is evaluated. However, we write the guesses on disk. For
each tested tool, we generate 108 passwords and collect the
required execution time. Then, we compute the throughput of
the generated guesses per second (g/s). Our implementations
come with two options: (1) we allow the generation of
duplicate guesses; and (2) we ﬁlter repeated guesses from the
output stream by using a bloom ﬁlter. The former option is
useful when fast hash functions are considered while the latter
is better suited for slow hash functions. For other tools, we
evaluate the implementation using the default settings. Data
are reported for both GAN and CWAE models. We perform
the tests on an NVIDIA DGX2 machine with NVIDIA V100s
(32GB device memory). Table B.1 reports the collected data.
DATA COLLECTED FOR PERFORMANCE ANALYSIS
TABLE B.1
OMEN
PCFG
GAN
512820 g/s
114810 g/s
303951 g/s
GAN
(ﬁltered)
80321 g/s
CWAE
237529 g/s
CWAE
(ﬁltered)
64197 g/s
Among the tested tools, OMEN is the fastest. It is indeed
shipped with a C implementation while PCFG and ours are
implemented in python language. Considering raw hashes
(i.e., 0 work factor), on the same hardware setup, hashcat
can compute ∼ 60000 bcrypt hashes per second on a single
GPU. This number becomes 1894.5 Mg/s when a fast hash
function, such as SHA-3 512, is used. Considering a single
iteration of bcrypt as a baseline for secure password storing,
all the tested tools can saturate the hashing pipeline when
bruteforce-aware hashing algorithms are employed, but they
fail when fast hash functions are considered. It is important
to highlight that the use of fast hash functions is not a secure
choice to store passwords.
APPENDIX C
LEARNING THE INVERSE MAPPING FOR THE GAN MODEL
To fully exploit the properties offered by the learned latent
representation of passwords, we need a way to explore the
latent space efﬁciently. Therefore, our primary interest is to
understand the relation between the observed data (i.e., pass-
words) and their respective latent representations; in particular,
their position within the latent space. A direct way to model
this relation is to learn the inverse of the generator function
G−1 : X → Z. GANs, by default, do not need to learn those
functions because that requirement is bypassed by the adver-
sarial training approach. To do so, framework variations [25],
[26] or additionally training phases [38] are required.
To avoid any source of instability in the original training
procedure, we opt to learn the inverse mapping only after the
12We failed to deploy FLA’s implementation on our most performing
machine that we used for the other benchmarks.
training of the generator is complete. This is accomplished
by training a third encoder network E that has an identical
architecture as the critic, except for the size of the output layer.
The network is trained to simultaneously map both the real
(i.e., data coming from the train-set) and generated (i.e., data
coming from G) data to the latent space. Speciﬁcally, the loss
function of E is mainly deﬁned as the sum of the two cyclic
reconstruction errors over the data space. This is presented
in the following:
L0 = Ez[d(G(z), G(E(Gt(z))))],
L1 = Ex[d(x, G(E(x)))].
(C.1)
In Eq. (C.1), the function d is the cross-entropy whereas x
and z are sampled from the train-set and the prior latent
distribution, respectively. The variable t in L0 refers to the
temperature of the ﬁnal softmax layer of the generator. In
Eq. (C.1), we do not specify temperature on a generator
notation when it is assumed that it does not change during
the training. The combination of these two reconstruction
errors aims at forcing the encoder to learn a general function
capable of inverting both the true and generated data correctly.
As discussed in Section II-B1, the discrepancy between the
representation of the true and generated data (i.e., discrete
and continuous data) is potentially harmful to the training
process. To deal with this issue, we anneal the temperature
t in loss term L0 during the training. We do that to collapse
slowly the continuous representations of the generated data
(i.e., the output of the generator) towards the same discrete
representation of the real data (i.e., coming from the dataset).
Next, an additional loss term, shown in Eq. C.2, is added
forcing the encoder to map the data space in a dense zone
of the latent space (dense with respect to the prior latent
distribution).
L2 = Ez[d(z, E(G(z)))].
(C.2)
Our ﬁnal loss function for E is reported in Eq. C.3. During
the encoder training, we use the same train-set that we used to
train the generator, but we consider only the unique passwords
in this case.
LE = αL0 + βL1 + γL2.
(C.3)
The information about the hyper-parameters we used is listed
in TABLE C.1.
HYPER-PARAMETERS USED TO TRAIN OUR ENCODER NETWORK
TABLE C.1
Hyper-parameter
α
β
γ
Batch size
Learning rate
Optimizer
Temperature decay step
Temperature limit
Temperature scheduler
Train iteration
Value
0.2
0.2
0.6
64
0.001
Adam
250000
0.1
polynomial
3 · 105
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:27:44 UTC from IEEE Xplore.  Restrictions apply. 
1397
ON THE IMPACT OF HYPER-PARAMETERS ON DPG
APPENDIX D
In this section, we brieﬂy consider the impact of the two
hyper-parameters of DPG over the quality of the attack.
Fig. D.1 depicts a comparison among the static attack, a
DPG with α = 15%, and a DPG with α = 0% (i.e., no
hot-start). These results conﬁrm that
the absence of hot-
start indeed affects and eventually degrades the performance
of DPG.
Fig. D.1. The impact of α on the performance of DPG for phpbb test-set
Fig. D.2 depicts the effect of different values of σ on
the performance of DPG. Smaller values of α yields better
overall results. This outcome suggests that it is not necessary
to sample too far from the dense zones imposed by Zi, and
rather a focused exploration of those zones is beneﬁcial. This
observation is perfectly coherent with the discussed locality
property, giving further support to the speculated ability of
the latent space of capturing and translating general features
of an entire password distribution in geometric relations.
Fig. D.2. The impact of σ on the performance of DPG for phpbb test-set
APPENDIX E
CWAE DETAILS
We build the CWAE architecture leveraging the same
resenet-like structure used for the GAN generator, which is
summarized in TABLE E.1. TABLE E.2 reports the hyper-
parameters used during the training of the model. Here, λ
is the weight assigned to the latent divergence term in the
loss function, i.e., Maximum Mean Discrepancy (MMD) for
our case. We use a standard softmax-crossentropy for the
distance measure in the data space. The parameter  controls
TABLE E.1
CWAE ARCHITECTURE SCHEME
Encoder
cov1d[5, 128, same, linear]
ResblockBottleneck1D[128, batchnorm=false]
ResblockBottleneck1D[128, batchnorm=false]
ResblockBottleneck1D[128, batchnorm=false]
ResblockBottleneck1D[128, batchnorm=false]
ResblockBottleneck1D[128, batchnorm=false]
ResblockBottleneck1D[128, batchnorm=false]
ResblockBottleneck1D[128, batchnorm=false]
Reshape[-1]
FullyConnected[128, linear]
Decoder
FullyConnected[MaxPasswordLength · 128, linear]
Reshape[MaxPasswordLength, 128]
ResblockBottleneck1D[128, batchnorm=false]
ResblockBottleneck1D[128, batchnorm=false]
ResblockBottleneck1D[128, batchnorm=false]
ResblockBottleneck1D[128, batchnorm=false]
ResblockBottleneck1D[128, batchnorm=false]
ResblockBottleneck1D[128, batchnorm=false]
ResblockBottleneck1D[128, batchnorm=false]
cov1d[1, AlphabetCardinality, same, linear]
the character-deletion probability used during the training (dis-
cussed in Section II-B2). For more ﬁne-grained information,
please refer to our project page (see Section Availability).
HYPER-PARAMETERS USED TO TRAIN OUR CWAE
TABLE E.2
Hyper-parameter
λ
Batch size
Learning rate
Optimizer
Train Epochs

Value
8.0
256
0.0001
Adam
25
5.0