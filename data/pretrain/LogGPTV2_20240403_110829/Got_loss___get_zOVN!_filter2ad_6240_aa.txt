# Title: Got Loss? Get zOVN!

## Authors
- Daniel Crisan
- Robert Birke
- Gilles Cressier
- Cyriel Minkenberg
- Mitchell Gusat

**IBM Research – Zurich Research Laboratory, Säumerstrasse 4, CH-8803 Rüschlikon, Switzerland**
- {dcr, bir, cre, sil, mig}@zurich.ibm.com

## Abstract
Datacenter networking is currently driven by two major trends. One trend focuses on lossless, flat Layer-2 fabrics based on Converged Enhanced Ethernet (CEE) or InfiniBand, offering benefits in efficiency and performance. The other trend emphasizes flexibility through Software-Defined Networking (SDN), enabling Overlay Virtual Networking (OVN). While these trends are complementary, they also present some conflicts. Unlike physical fabrics, which avoid packet drops through flow control, most current virtual networks are lossy. We quantify the losses for common combinations of hypervisors and virtual switches and demonstrate their negative impact on application performance. Additionally, we propose a zero-loss Overlay Virtual Network (zOVN) designed to reduce query and flow completion times for latency-sensitive datacenter applications. We describe its architecture and detail the design of its key component, the zVALE lossless virtual switch. As proof of concept, we implemented a zOVN prototype and benchmarked it with Partition-Aggregate in two testbeds, achieving up to a 15-fold reduction in mean completion time with three widespread TCP versions. For larger-scale validation, we developed an OMNeT++ model for accurate cross-layer simulations of a virtualized datacenter, confirming the validity of our results.

## Categories and Subject Descriptors
C.2.1 [Computer-Communication Networks]: Network Architecture and Design

## Keywords
Datacenter networking, virtualization, overlay networks, lossless, Partition-Aggregate

## Introduction
In recent years, significant changes have occurred in datacenter networking, impacting the performance of latency-sensitive workloads, often referred to as online and data-intensive applications. Two notable trends are the rise of Overlay Virtual Networking (OVN), a remarkable application of Software-Defined Networking (SDN), and the shift towards lossless Layer-2 fabrics based on Converged Enhanced Ethernet (CEE) or InfiniBand. So far, these trends have been decoupled, each making independent inroads into the datacenter.

While the research community increasingly focuses on the performance of horizontally-distributed data-intensive applications and virtualization overlays for multitenant datacenters, we argue that the combination of virtualization and such workloads deserves closer scrutiny. Our primary objective is to analyze the impact of the absence versus presence of flow control on workload performance in a virtualized network. Specifically, we focus on latency-sensitive, data-intensive workloads, using flow completion time (FCT) as the performance metric. We selected Partition-Aggregate as a representative workload model.

### 1.1 Network Virtualization
Server virtualization allows dynamic and automatic creation, deletion, and migration of virtual machines (VMs). The datacenter network must support these functions without imposing restrictions such as IP subnet and state requirements. In addition to VM mobility and ease of management, complete traffic isolation is desirable for improved security, which can be achieved through Layer-2 and -3 virtualization. These requirements can be effectively met by creating SDN-based overlays like VXLAN and DOVE. An exemplary architectural exposition of modern virtual overlays is NetLord, which covers key motivations and design principles.

SDN decouples the control and data planes, introducing programmability and presenting applications with an abstraction of the underlying physical network. Scalable and flexible "soft" networks can thus be designed to adapt to changing workloads and meet the needs of datacenter tenants and operators. SDN trades some degree of performance to simplify network control and management, automate virtualization services, and provide a platform for new network functionalities, leveraging both OpenFlow and IETF network virtualization overlay standards.

Based on the adoption rate of virtualization in datacenters, we assume that virtual networks (VNs) will be deployed in most, if not all, multitenant datacenters, providing a fully virtualized Cloud platform by default. However, current hypervisors, virtual switches (vSwitches), and virtual network interface cards (vNICs) critically differ from their physical counterparts. They tend to drop packets even under minor congestive transients, leading to considerable and non-deterministic losses. Consequently, current non-flow-controlled virtual networks significantly negate the investments in upgrading datacenter networks with flow-controlled CEE and InfiniBand fabrics. This lossy legacy hinders both application performance and the progress of future datacenters.

### 1.2 Lossless Fabrics
The recent standardization of 802 Data Center Bridging for 10-100 Gbps CEE has triggered the commoditization of high-performance lossless fabrics. First-generation 10G products are already on the market, and CEE fabrics at 40G, or even 100G, have been announced by several vendors.

Traditionally, Ethernet did not guarantee losslessness, dropping packets whenever a buffer reached its maximum capacity. This behavior does not match the modern semantics of datacenter applications, including High-Performance Computing (HPC) environments, storage (Fibre Channel over Ethernet), or Remote Direct Memory Access (RDMA) over Ethernet.

CEE upgrades Ethernet with two new mechanisms: Priority Flow Control (PFC) and Quantized Congestion Notification (QCN). PFC divides controlled traffic into eight priority classes based on the 802.1p Class of Service field. Within each priority, PFC acts similarly to the prior 802.3x PAUSE, except that pausing one priority does not affect others. This prevents a 10-100G link from being fully stopped when a particularly aggressive flow exceeds its allotted buffer share. Despite this improvement, PFC can still lead to potential global throughput collapse due to head-of-line blocking issues. QCN was defined and extensively simulated to address these issues.

### 1.3 Contributions
Our main contributions are:
1. Identifying and characterizing the problem of packet drops in virtual networks.
2. Implementing the first zero-loss Overlay Virtual Network (zOVN) to address the lossless assumption of converged multitenant datacenters.
3. Quantitatively verifying how zOVN improves standard TCP performance for data-intensive applications. Testing Partition-Aggregate on top of zOVN, we achieved up to a 15-fold reduction in flow completion times using two distinct testbeds with 1G and 10G Ethernet, respectively, and three standard TCPs.
4. Investigating the scalability of zOVN through accurate full-system cross-layer simulations.

The remainder of the paper is structured as follows: Section 2 presents the main issues of current virtual networks. Section 3 explores the design space of virtual overlays. Section 4 provides details of our zOVN prototype, and Section 5 presents its evaluation. Section 6 discusses the results, and Section 7 covers related work. We conclude in Section 8.

## 2. Challenges in Virtual Networks
Current virtual networks suffer from two main deficiencies: latency penalties and excessive packet dropping.

### 2.1 Latency
A virtual link does not present a well-defined channel capacity. Arrivals and departures cannot be strictly bounded, and the virtual link service time remains a stochastic process dependent on processor design, kernel interrupts, and process scheduling. This negatively affects jitter, burstiness, and quality of service. Virtual networks without dedicated real-time CPU support remain a challenging networking problem.

Additionally, virtual networks introduce new protocols spanning Layer-2 to Layer-4, affecting every flow or, in extreme cases, every packet. This results in a heavier stack with encapsulation-induced delays and overheads, potentially leading to fragmentation and inefficient offload processing.

The more critical performance aspect is the impact on latency-sensitive datacenter applications. Latency and flow-completion time are crucial for horizontally-distributed workloads such as Partition-Aggregate, typically classified as soft real-time. The 200ms end-user deadline translates into constraints of a few tens of milliseconds for lower-level workers. Although the additional VN-induced delay may be negligible in a basic ping test, its impact on more realistic Partition-Aggregate workloads can increase the mean flow completion time by up to 82%. This raises concerns about potentially unacceptable VN performance degradations for critical latency-sensitive applications in a virtualized multitenant environment.

### 2.2 Losslessness
Ideally, a VN should preserve the lossless abstraction assumed by converged datacenter applications such as Fibre Channel over Ethernet, RDMA over Ethernet, or HPC environments. However, all commercial and open-source VNs we tested are lossy. Losslessness is a critical qualitative feature for the future of converged datacenter networking, and CEE ensures zero-loss operation through PFC and QCN. Similarly, InfiniBand uses link-level credit-based flow control and FECN/BECN-based end-to-end congestion control. In comparison, despite the possibility of simpler and lower-cost flow control implementations, current VNs still resort to packet drops during congestion, degrading datacenter performance.