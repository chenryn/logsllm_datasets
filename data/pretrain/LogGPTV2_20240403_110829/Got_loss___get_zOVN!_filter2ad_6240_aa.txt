title:Got loss?: get zOVN!
author:Daniel Crisan and
Robert Birke and
Gilles Cressier and
Cyriel Minkenberg and
Mitchell Gusat
Got Loss? Get zOVN!
∗
Daniel Crisan, Robert Birke, Gilles Cressier, Cyriel Minkenberg and Mitch Gusat
IBM Research – Zurich Research Laboratory, Säumerstrasse 4, CH-8803 Rüschlikon, Switzerland
{dcr,bir,cre,sil,mig}@zurich.ibm.com
ABSTRACT
Datacenter networking is currently dominated by two major
trends. One aims toward lossless, ﬂat layer-2 fabrics based
on Converged Enhanced Ethernet or InﬁniBand, with ben-
eﬁts in eﬃciency and performance. The other targets ﬂexi-
bility based on Software Deﬁned Networking, which enables
Overlay Virtual Networking. Although clearly complemen-
tary, these trends also exhibit some conﬂicts: In contrast to
physical fabrics, which avoid packet drops by means of ﬂow
control, practically all current virtual networks are lossy. We
quantify these losses for several common combinations of hy-
pervisors and virtual switches, and show their detrimental
eﬀect on application performance. Moreover, we propose a
zero-loss Overlay Virtual Network (zOVN) designed to re-
duce the query and ﬂow completion time of latency-sensitive
datacenter applications. We describe its architecture and
detail the design of its key component, the zVALE lossless
virtual switch. As proof of concept, we implemented a zOVN
prototype and benchmark it with Partition-Aggregate in two
testbeds, achieving an up to 15-fold reduction of the mean
completion time with three widespread TCP versions. For
larger-scale validation and deeper introspection into zOVN,
we developed an OMNeT++ model for accurate cross-layer
simulations of a virtualized datacenter, which conﬁrm the
validity of our results.
Categories and Subject Descriptors
C.2.1 [Computer-Communication Networks]: Network
Architecture and Design
Keywords
Datacenter networking; virtualization; overlay networks; loss-
less; Partition-Aggregate
∗Corresponding author: PI:EMAIL
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
SIGCOMM’13, August 12–16, 2013, Hong Kong, China.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2056-6/13/08 ...$15.00.
1.
INTRODUCTION
In recent years, profound changes have occurred in data-
center networking that are likely to impact the performance
of latency-sensitive workloads, collectively referred to as on-
line and data-intensive [38]. Particularly relevant are the
rise of Overlay Virtual Networking (OVN) – remarkable ap-
plication of Software-Deﬁned Networking (SDN) – and, si-
multaneously, the shift to lossless layer-2 fabrics based on
Converged Enhanced Ethernet (CEE) or InﬁniBand. So
far, the trends in virtualization and the commoditization of
high-performance-computing-like lossless1 fabrics have been
decoupled, each making independent inroads into the data-
center.
While the research community increasingly focuses on the
performance of horizontally-distributed data-intensive appli-
cations [15, 8, 9, 25, 38, 41, 42], and recently also on vir-
tualization overlays for multitenant datacenters [28, 40, 17],
we argue that the combination of virtualization and such
workloads merits closer scrutiny [13]. Our main objective
is to analyze the impact of the absence versus presence of
ﬂow control on workload performance in a virtualized net-
work. As our study speciﬁcally focuses on latency-sensitive,
data-intensive workloads, the performance metric of inter-
est is ﬂow completion time (FCT) [20]. As a representative
workload model, we selected Partition-Aggregate [8, 41].
1.1 Network Virtualization
As server virtualization allows dynamic and automatic
creation, deletion, and migration of virtual machines (VMs),
the datacenter network must support these functions with-
out imposing restrictions, such as IP subnet and state re-
quirements. In addition to VM mobility and ease of man-
agement, complete traﬃc isolation is desirable for improved
security, which can be achieved by layer-2 and -3 virtualiza-
tion. Rather than treating the virtual network as a dumb
extension of the physical network, these requirements can
be eﬀectively met by creating SDN-based overlays such as
VXLAN [26] and DOVE [17]. An exemplary architectural
exposition of modern virtual overlays is NetLord [28], which
covers the key motivations and design principles.
SDN as a concept decouples the control and data planes,
introducing programmability and presenting applications with
1In this paper we use lossless and zero-loss in the sense of
avoiding packet drops due to congestion. Packets might still
be discarded because of CRC errors in the physical links.
These, however, are extremely rare events under normal con-
ditions (typical bit error rates are 10−12 or less) and recov-
ered by TCP.
423an abstraction of the underlying physical network. Scalable
and ﬂexible “soft” networks can thus be designed to adapt
to changing workloads and to datacenter tenants and op-
erators needs.
In a nutshell, SDN trades some degree of
performance to simplify network control and management,
to automate virtualization services, and to provide a plat-
form upon which new network functionalities can be built.
In doing so, it leverages both the OpenFlow [27, 29] and the
IETF network virtualization overlay [37, 26] standards.
Based on the adoption rate of virtualization in datacen-
ters, the underlying assumption is that virtual networks
(VN) will be deployed in practically most, if not all, multi-
tenant datacenters, providing a fully virtualized Cloud plat-
form by default. For the remainder of this paper, we pre-
sume that VN overlay is an intrinsic part of the extended
datacenter network infrastructure. Thus we envision a fully
virtualized datacenter in which “bare-metal” workloads be-
come the exception, even for mission-critical applications.
However, current hypervisors, virtual switches (vSwitches)
and virtual network interface cards (vNICs) critically dif-
fer from their modern physical counterparts. In fact, they
have a propensity to liberally drop packets even under minor
congestive transients. These losses can be considerable and
non-deterministic, as will be presented in Section 2.3. Con-
sequently, current non-ﬂow-controlled virtual networks will
signiﬁcantly cancel out the investments of upgrading dat-
acenter networks with ﬂow-controlled CEE and InﬁniBand
fabrics. We argue that this lossy legacy unnecessarily hin-
ders both the application performance and the progress of
future datacenters.
1.2 Lossless Fabrics
The recent standardization of 802 Data Center Bridging
for 10-100 Gbps CEE triggered the commoditization of high-
performance lossless fabrics. First generation 10G products
are already on the market, and CEE fabrics at 40G, or even
100G, have been announced by several vendors.
Traditionally, Ethernet did not guarantee losslessness: pa-
ckets were dropped whenever a buﬀer reached its maximum
capacity. This behavior does not match the modern seman-
tics of datacenter applications, including High-Performance
Computing (HPC) environments [19], storage (Fibre Chan-
nel over Ethernet [5]), or Remote Direct Memory Access
(RDMA) over Ethernet [16].
CEE upgrades Ethernet with two new mechanisms of in-
terest here: A link-level ﬂow control, i.e., Priority Flow Con-
trol (PFC) [6], and an end-to-end congestion management
known as Quantized Congestion Notiﬁcation (QCN). PFC
divides the controlled traﬃc into eight priority classes based
on the 802.1p Class of Service ﬁeld. Within each priority
PFC acts as the prior 802.3x PAUSE, except that a paused
priority will not aﬀect the others. Hence, a 10-100G link is
not fully stopped whenever a particularly aggressive ﬂow ex-
ceeds its allotted buﬀer share. Despite the marked improve-
ment over the original PAUSE, a side-eﬀect of PFC still re-
mains the potential global throughput collapse, which diﬀers
from the lossy case. The buﬀer of a ﬂow-controlled blocked
receiver may recursively block buﬀers upstream, spreading
the initial congestion into a saturation tree [30]. To address
these head-of-line blocking issues, QCN was deﬁned and ex-
tensively simulated prior to releasing PFC.
Figure 1: Experimental setup for virtual network
loss measurements.
1.3 Contributions
Our main contributions are as follows.
(i) We identify
and characterize the problem of packet drops in virtual net-
works. (ii) We implement the ﬁrst zero-loss Overlay Virtual
Network (zOVN) to address the lossless assumption of con-
verged multitenant datacenters. (iii) We quantitatively ver-
ify how zOVN improves the standard TCP performance for
data-intensive applications. Testing Partition-Aggregate on
top of zOVN, we achieved up to 15-fold reductions in ﬂow
completion times using two distinct testbeds with 1G and
10G Ethernet respectively, and three standard TCPs. Fi-
nally, (iv) we investigate the scalability of zOVN by means
of accurate full system cross-layer simulations.
The remainder of the paper is structured as follows. In
Section 2 we present the main issues of current virtual net-
works. In Section 3 we explore the design space of virtual
overlays. We provide the details of our zOVN prototype in
Section 4 and present its evaluation in Section 5. We discuss
the results in Section 6 and the related work in Section 7.
We conclude in Section 8.
2. VIRTUAL NETWORKS CHALLENGES
The two deﬁciencies of current virtual networks are la-
tency penalties and excessive packet dropping.
2.1 Latency
A virtual link does not present a well-deﬁned channel
capacity. Neither arrivals nor departures can be strictly
bounded. The virtual link service time remains a stochas-
tic process depending on the processor design, kernel inter-
rupts, and process scheduling. This negatively aﬀects jitter,
burstiness, and quality-of-service. Hence, virtual networks
without dedicated real-time CPU support remain a hard net-
working problem.
In addition, virtual networks introduce
new protocols spanning layer-2 to 4 and touch every ﬂow
or, in extreme cases, even every packet [28, 17]. The result
is a heavier stack, with encapsulation-induced delays and
overheads possibly leading to fragmentation and ineﬃcient
oﬄoad processing.
However, the more critical performance aspect is the im-
pact on latency-sensitive datacenter applications. Latency
and ﬂow-completion time have been recently established as
crucial for horizontally-distributed workloads such as Parti-
tion - Aggregate, typically classiﬁed as soft real-time. The
200ms end-user deadline [8, 41, 24] translates into constraints
of few 10s of milliseconds for the lower-level workers. Al-
though the additional VN-induced delay may be negligible in
a basic ping test [28], its impact on more realistic Partition-
Aggregate workloads can lead to an increase in the mean
ﬂow completion time of up to 82% [13]. This raises concerns
about potentially unacceptable VN performance degrada-
vSwitchIperfsourceVM11vNIC Tx2IperfsourceVM21vNIC Tx2Port A TxPort B TxPort C RxIperfsinkVM36vNIC Rx5334Physical machine424Hypervisor
vNIC
Qemu/KVM virtio
Qemu/KVM virtio
Qemu/KVM virtio
N2
H2
H2
e1000
Qemu/KVM e1000
Qemu/KVM e1000
vSwitch
Linux Bridge
Open vSwitch
VALE
S4
S4
Linux Bridge
Open vSwitch
C1
C2
C3
C4
C5
C6
C7
Figure 2: Causes of packet losses. Conﬁgurations
C1-C7 deﬁned in Table 1.
tions for such critical latency-sensitive applications in a vir-
tualized multitenant environment.
2.2 Losslessness
Ideally a VN should preserve the lossless abstraction as-
sumed by converged datacenter applications such as Fibre
Channel over Ethernet [5], RDMA over Ethernet [16] or
HPC environments [19]. Yet currently all the commercial
and open-source VNs that we have tested are lossy. As
losslessness is a critical qualitative feature for the future of
converged datacenter networking, CEE spared no eﬀort to
ensure zero-loss operation by using two complementary ﬂow
and congestion control protocols, namely, PFC and QCN.
The same holds for InﬁniBand, with its link level credit-
based ﬂow control and its FECN/BECN-based end-to-end
Congestion Control Annex. In comparison, despite the pos-
sibility of relatively simpler and lower-cost ﬂow control im-
plementations, current VNs still resort to packet drop during
congestion. This not only degrades datacenter performance,