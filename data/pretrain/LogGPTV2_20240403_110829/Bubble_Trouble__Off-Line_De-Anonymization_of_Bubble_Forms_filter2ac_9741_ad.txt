### Evidence and Mitigation Techniques

There is evidence suggesting that certain properties may not be sufficiently distinguishing. Additionally, averaging techniques can obscure a respondent's intent, even when it was initially clear to the person interpreting the form. Similar mitigation methods, such as blurring the image, reducing its resolution, or converting it to strictly black and white, also have drawbacks similar to those of color averaging.

### Facial Image Recognition Approach

An intriguing approach comes from the facial image recognition community. Newton et al. [23] describe a method for generating k-anonymous facial images. This technique replaces each face with a "distorted" image that is k-anonymous relative to the input set. The resulting image maintains the expected visual appearance of a human face. The specific details are beyond the scope of this paper, but the technique involves reducing dimensionality using Principal Component Analysis (PCA) and an algorithm for removing distinctive features [23].

Applying facial anonymization to bubbles is straightforward. By taking marked and unmarked bubbles from all ballots in a set, we can apply Newton et al.'s techniques to each bubble, replacing it with its k-anonymous counterpart. This would roughly maintain the visual appearance of each bubble while removing unique attributes. However, this approach has limitations. Replacing an image might obscure a respondent's otherwise clear intent. Moreover, distinguishing trends might occur across multiple bubbles on a form, such as an individual marking bubbles differently near the end of the form (a problem also present in color averaging). Finally, there are concerns about the guarantees provided by k-anonymity [3], but the work may be extended to achieve other privacy notions, such as differential privacy [9].

We caution that these images have limited value in proving the true contents of physical bubble forms. An adversary with access to the images, whether scrubbed or not, could intentionally modify them to match a desired result. These approaches are most useful where the primary concern is unintentional error.

### Related Work

#### Biometrics

Biometrics can be based on physical or behavioral characteristics. Physical biometrics include fingerprints, facial features, and iris patterns. Behavioral biometrics, such as speech or handwriting/signature [15], are based on stable and difficult-to-replicate behaviors. Bubble completion patterns are a form of behavioral biometric.

As a biometric, bubble completion patterns are similar to handwriting, though handwriting typically relies on a richer, less constrained set of features. Both can be analyzed online or offline [21]. Online analysis monitors characteristics like stroke speed and pressure, while offline analysis only receives the resulting data, such as a completed bubble. Handwriting-based recognition sometimes occurs in an online setting. Since offline recognition is more generally applicable, our analysis was conducted purely offline. In some settings, such as authentication, online recognition could yield stronger results.

#### Document Re-identification

Some work aims to re-identify precise physical documents for forgery and counterfeiting detection (e.g., [7]). While biometrics may assist in re-identification, the problems discussed here differ. We aim to determine if sets of marked bubbles were produced by the same individual, regardless of whether they come from the same or different forms. Our work and document re-identification provide complementary techniques. For example, document re-identification could help determine if the bubble form (ballot, answer sheet, survey, etc.) provided to an individual matches the one returned or detect fraudulently added forms.

#### Cheating Detection

Existing work uses patterns in answer choices to detect cheating. Myagkov et al. [20] uncover indicators of election fraud using aggregate vote tallies, turnout, and historical data. Similarly, analyzing answers on standardized tests can reveal cheating [10, 17]. For instance, if students demonstrate mediocre overall performance but correctly answer a series of difficult questions, this may raise cheating concerns. The general strategy is to look for answers that are suspicious in the context of other answers or auxiliary data.

Bubble-based analysis complements these anti-cheating measures. Each technique isolates sets of suspicious forms, and combining them would likely be more accurate than using either independently. Although our techniques do not exploit contextual data, they are unbiased by it. If a student dramatically improves her study habits, the resulting improvement in test scores might be flagged by other anti-cheating measures but not by our techniques.

### Future Work

While various avenues for future work exist, we focus on additional testing and application-specific uses.

Our sample surveys allowed diverse tests, but access to different datasets would enable further useful tests. Longitudinal studies, where a common set of respondents fill out bubble forms multiple times over a period, would be particularly valuable. This would provide more examples, identify how markings vary over time, establish consistency, and confirm that results are not significantly impacted by writing utensils. Since longitudinal bubble form data is not widely available, we might need to collect the data ourselves.

We tested our techniques using circular bubbles with numbers inscribed, but numerous other form styles exist. Respondents might fill in ovals or rectangles, or use a line-drawing approach. Testing these cases would explore the limits of our work and help uncover mitigation strategies.

Adapting our techniques to various applications is not always trivial. For example, combining our techniques with existing anti-cheating measures for standardized tests would strengthen anti-cheating efforts but requires careful data processing and result merging.

Using bubble markings for authentication would require additional testing and refinement. Given a dataset with online information such as writing instrument position, velocity, and pressure, we could add this data to our feature vectors and test the accuracy of our techniques. This additional information could increase identifiability, as signature verification often uses such data, and may yield an effective authentication system. Depending on the application, a bubble-based authentication system might need to work with a finger rather than a pen or stylus, requiring cautious testing to prevent impersonation.

### Conclusion

Marking a bubble is a narrow task, but it provides sufficient expressive power for individuals to unintentionally distinguish themselves. Using a dataset with 92 individuals, we demonstrated how to re-identify a respondent's survey with over 50% accuracy and detect unauthorized respondents with over 92% accuracy and a false positive rate below 10%. We achieved these results through offline analysis, but online analysis could potentially achieve even higher accuracy.

The implications extend to any system using bubble forms for user input, especially where protecting or confirming a respondent's identity is important. Additional tests can better establish the threat or benefit in real-world scenarios. Mitigating the information conveyed through marked bubbles is an open problem, and solutions depend on the application. For privacy-critical applications, such as the publication of ballots, we suggest masking respondents' markings before publication.

### Acknowledgements

We are grateful to Ian Davey, Deven Desai, Ari Feldman, Adam Finkelstein, Joe Hall, Josh Kroll, Tim Lee, Szymon Rusinkiewicz, Harlan Yu, and Bill Zeller for their helpful suggestions and assistance. We thank Joel Trachtenberg, Jennifer Minsky, and Derrick Higgins for assistance in obtaining test data. We also thank Carolyn Crnich, Kevin Collins, and Mitch Trachtenberg of the Humboldt County Election Transparency Project for discussing the implications of our work. We thank our anonymous reviewers and shepherd, Claudia Diaz, for their comments on the paper.

### References

[1] ALAMEDA COUNTY, CALIFORNIA. Alameda County Voting System Demonstration. http://www.acgov/org/rov/votingsystemdemo.htm.

[2] ASLAM, J. A., POPA, R. A., AND RIVEST, R. L. On estimating the size and confidence of a statistical audit. In Proc. 2007 USENIX/ACCURATE Electronic Voting Technology Workshop (EVT '07).

[3] BRICKELL, J., AND SHMATIKOV, V. The cost of privacy: Destruction of data-mining utility in anonymized data publishing. In Proc of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (August 2008).

[4] CALANDRINO, J. A., CLARKSON, W., AND FELTEN, E. W. Some consequences of paper fingerprinting for elections. In Proceedings of EVT/WOTE 2009 (Aug. 2009), D. Jefferson, J. L. Hall, and T. Moran, Eds., USENIX/ACCURATE/IAVoSS.

[5] CALANDRINO, J. A., HALDERMAN, J. A., AND FELTEN, E. W. Machine-assisted election auditing. In Proc. 2007 USENIX/ACCURATE Electronic Voting Technology Workshop (EVT '07).

[6] CARBACK, R. How secret is your secret ballot? part 1 of 3: Pattern voting. https://scantegrity.org/blog/2008/06/16/how-secret-is-your-secret-ballot-part-1-of-3-pattern-voting/, June 16 2008.

[7] CLARKSON, W., WEYRICH, T., FINKELSTEIN, A., HENINGER, N., HALDERMAN, J. A., AND FELTEN, E. W. Fingerprinting blank paper using commodity scanners. In Proc of IEEE Symposium on Security and Privacy (May 2009).

[8] COLLEGE BOARD. Underscore 2010 importance of academic rigor for college-bound seniors. http://www.collegeboard.com/press/releases/213182.html.

[9] DWORK, C. Differential privacy. In Proc of the 33rd International Colloquium on Automata, Language and Programming (July 2006).

[10] GABRIEL, T. Cheaters find an adversary in technology. New York Times (December 27 2010).

[11] GABRIEL, T. Under pressure, teachers tamper with tests. New York Times (June 10 2010).

[12] GARNER, S. R. Weka: The Waikato environment for knowledge analysis. In In Proc. of the New Zealand Computer Science Research Students Conference (1995), pp. 57–64.

[13] GREENSON, T. Software glitch yields inaccurate election results. Times-Standard (December 5 2008).

[14] HUMBOLDT COUNTY ELECTION TRANSPARENCY PROJECT. http://www.humetp.org/.

[15] JAIN, A., HONG, L., AND PANKANTI, S. Biometric Identification. Communications of the ACM 43, 2 (February 2000), 91–98.

[16] JOLLIFFE, I. T. Principal Component Analysis, second ed. Springer, October 2002.

[17] LEVITT, S. D., AND DUBNER, S. J. Freakonomics: A Rogue Economist Explores the Hidden Side of Everything. HarperCollins, 2006.

[18] LOS ANGELES COUNTY CLERK/REGISTRAR-RECORDER. InkaVote Plus Manual. http://www.lavote.net/voter/pollworker/PDFS/INKAVOTE PLUS HANDBOOK.pdf, 2011.

[19] MCCUTCHEON, C. Absentee voting fosters trickery, trend’s foes say. Times Picayune (October 2006).

[20] MYAGKOV, M., ORDESHOOK, P. C., AND SHAKIN, D. The Forensics of Election Fraud: Russia and Ukraine. Cambridge University Press, 2009.