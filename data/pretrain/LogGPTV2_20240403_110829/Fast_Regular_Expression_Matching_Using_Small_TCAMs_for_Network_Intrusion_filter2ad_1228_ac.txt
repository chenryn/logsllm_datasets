V2
I
G: 
I
L: 
W: 0
V3
I
G: 
L: 0 
W: 1
V4
I
G: 
L: 00 
W: 2
V5
G: 1
I
L: 
W: 0
V6
V7
G: 01
I
L: 
W: 0
G: 10
I
L: 
W: 0
0
2
1
2= max (1, 1) +1
1
1= max (0, 0) +1
0
1
0
1
Nodes:
V1
Weight:   0
V2
0
HCode:   000             001
V3
1
01
V4
2
1
V2
V3
G: 001
I
L:    
W: 0 
SC= 001
ID= 001
G: 01
L: 0
W: 1
SC= 01*
ID= 010
V4
G: 1
L: 00  
W: 2
SC=1**
ID= 100
V5
V6
V7
G: 011
I
L:       
W: 0
SC= 011
ID= 011
G: 101
I
L: 
W: 0
SC= 101
ID= 101
G: 110
I
L: 
W: 0
SC= 110
ID= 110
(a)
(b)
(c)
Figure 4: Shadow encoding example
internal node z in T with two children x and y and set
weight(z)=maximum(weight(x), weight(y))+1, and then
put element z into the priority queue. When there is only
a single element in the priority queue, the binary encod-
ing tree T is complete. The HCode assigned to each leaf
node v′ is the path in T from the root node to v′ where
left edges have value 0 and right edges have value 1. We
update the internal variables of v and its descendants in
DT as follows. We set L(v) to be its HCode, and W (v)
to be the weight of the root node of T ; G(v) is left empty.
For each child vi, we prepend vi’s HCode to the global
ID of every node in the subtree rooted at vi including vi
itself. We then mark v as red. This continues until all
nodes are red.
We now assign each node a state ID and a shadow
code. First, we set the shadow length to be k, the weight
of the root node of DT . We use {∗}m to denote a ternary
string with m number of *’s and {0}m to denote a bi-
nary string with m number of 0’s. For each node v,
we compute v’s state ID and shadow code as follows:
ID(v) = G(v)|L(v)|{0}k−#G(v)−#L(v), SC(v) =
G(v)|{∗}k−#G(v). We illustrate our shadow encoding
algorithm in Figure 4. Figure 4(a) shows all the inter-
nal variables just before v1 is processed. Figure 4(b)
shows the Huffman style binary encoding tree T built
for node v1 and its children v2, v3, and v4 and the result-
ing HCodes. Figure 4(c) shows each node’s ﬁnal weight,
global ID, local ID, state ID and shadow code.
Experimentally, we found that our shadow encoding
algorithm is effective at minimizing shadow length. No
DFA had a shadow length larger than ⌈log2 |Q|⌉ + 3, and
⌈log2 |Q|⌉ is the minimum possible shadow length.
4 Table Consolidation
We now present table consolidation where we combine
multiple transition tables for different states into a single
transition table such that the combined table takes less
TCAM space than the total TCAM space used by the
original tables. To deﬁne table consolidation, we need
two new concepts: k-decision rule and k-decision table.
A k-decision rule is a rule whose decision is an array
of k decisions. A k-decision table is a sequence of k-
decision rules following the ﬁrst-match semantics. Given
a k-decision table T and i (0 ≤ i  @  to represent a state.
TCAM
Consolidated
Src Table ID
0
0
0
0
0
Input
Character
0110 0000
0110 0010
0110 0011
0110 ****
**** ****
SRAM
Column ID
00
s0
s1
s1
s1
s0
01
s0
s1
s2
s2
s0
10
s0
s1
s1
s2
s0
Figure 5: 3-decision table for 3 states in Fig. 1
There are two key technical challenges in table con-
solidation. The ﬁrst challenge is how to consolidate k
1-decision transition tables into a k-decision transition
table. The second challenge is which 1-decision transi-
tion tables should be consolidated together. Intuitively,
the more similar two 1-decision transition tables are, the
more TCAM space saving we can get from consolidating
them together. However, we have to consider the defer-
ment relationship among states. We present our solutions
to these two challenges.
4.2 Computing a k-decision table
In this section, we assume we know which states need to
be consolidated together and present a local state consol-
idation algorithm that takes a k1-decision table for state
set Si and a k2-decision table for another state set Sj as
its input and outputs a consolidated (k1 + k2)-decision
table for state set Si ∪ Sj. For ease of presentation, we
ﬁrst assume that k1 = k2 = 1.
Let s1 and s2 be the two input states which have de-
fault transitions to states s3 and s4. We enforce a con-
straint that if we do not consolidate s3 and s4 together,
then s1 and s2 cannot defer any transitions at all. If we do
consolidate s3 and s4 together, then s1 and s2 may have
incomplete transition tables due to default transitions to
s3 and s4, respectively. We assign state s1 column ID 0
and state s2 column ID 1. This consolidated table will be
assigned a common table ID X. Thus, we encode s1 as
X@0 and s2 as X@1.
The key concepts underlying this algorithm are break-
points and critical ranges. To deﬁne breakpoints, it is
helpful to view Σ as numbers ranging from 0 to |Σ| − 1;
given 8 bit characters, |Σ| = 256. For any state s, we
deﬁne a character i ∈ Σ to be a breakpoint for s if
δ(s, i) 6= δ(s, i − 1). For the end cases, we deﬁne 0
and |Σ| to be breakpoints for every state s. Let b(s)
be the set of breakpoints for state s. We then deﬁne
b(S) = Ss∈S b(s) to be the set of breakpoints for a
set of states S ⊂ Q. Finally, for any set of states S,
we deﬁne r(S) to be the set of ranges deﬁned by b(S):
r(S) = {[0, b2 − 1], [b2, b3 − 1], . . . , [b|b(S)|−1, |Σ| − 1]}
where bi is ith smallest breakpoint in b(S). Note that
0 = b1 is the smallest breakpoint and |Σ| is the largest
breakpoint in b(S). Within r(S), we label the range be-
ginning at breakpoint bi as ri for 1 ≤ i ≤ |b(S)| − 1. If
δ(s, bi) is deferred, then ri is a deferred range.
When we consolidate s1 and s2 together, we compute
b({s1, s2}) and r({s1, s2}). For each r′ ∈ r({s1, s2})
where r′ is not a deferred range for both s1 and s2, we
create a consolidated transition rule where the decision
of the entry is the ordered pair of decisions for state s1
and s2 on r′. For each r′ ∈ r({s1, s2}) where r′ is a
deferred range for one of s1 but not the other, we ﬁll in
r′ in the incomplete transition table where it is deferred,
and we create a consolidated entry where the decision of
the entry is the ordered pair of decisions for state s1 and
s2 on r′. Finally, for each r′ ∈ r({s1, s2}) where r′ is
a deferred range for both s1 and s2, we do not create a
consolidated entry. This produces a non-overlapping set
of transition rules that may be incomplete if some ranges
do not have a consolidated entry. If the ﬁnal consolidated
transition table is complete, we minimize it using the
optimal 1-dimensional TCAM minimization algorithm
in [20, 31].
If the table is incomplete, we minimize it
using the 1-dimensional incomplete classiﬁer minimiza-
tion algorithm in [21]. We generalize this algorithm to
cases where k1 > 1 and k2 > 1 by simply considering
k1 + k2 states when computing breakpoints and ranges.
4.3 Choosing States to Consolidate
We now describe our global consolidation algorithm for
determining which states to consolidate together. As we
observed earlier, if we want to consolidate two states
s1 and s2 together, we need to consolidate their parent
nodes in the deferment forest as well or else lose all the
beneﬁts of shadow encoding. Thus, we propose to con-
solidate two deferment trees together.
A consolidated deferment tree must satisfy the follow-
ing properties. First, each node is to be consolidated with
at most one node in the second tree; some nodes may not
be consolidated with any node in the second tree. Sec-
ond, a level i node in one tree must be consolidated with
a level i node in the second tree. The level of a node
is its distance from the root. We deﬁne the root to be a
level 0 node. Third, if two level i nodes are consolidated
together, their level i − 1 parent nodes must also be con-
solidated together. An example legal matching of nodes
8
between two deferment trees is depicted in Fig. 6.
Figure 6: Consolidating two trees
Given two deferment trees, we start the consolidation
process from the roots. After we consolidate the two
roots, we need to decide how to pair their children to-
gether. For each pair of nodes that are consolidated to-
gether, we again must choose how to pair their children
together, and so on. We make an optimal choice using
a combination of dynamic programming and matching
techniques. Our algorithm proceeds as follows. Suppose
we wish to compute the minimum cost C(x, y), mea-
sured in TCAM entries, of consolidating two subtrees
rooted at nodes x and y where x has u children X =
{x1, . . . , xu} and y has v children Y = {y1, . . . , yv}.
We ﬁrst recursively compute C(xi, yj) for 1 ≤ i ≤ u
and 1 ≤ j ≤ v using our local state consolidation al-
gorithm as a subroutine. We then construct a complete
bipartite graph KX,Y such that each edge (xi, yj) has
the edge weight C(xi, yj) for 1 ≤ i ≤ u and 1 ≤ j ≤ v.
Here C(x, y) is the cost of a minimum weight match-
ing of K(X, Y ) plus the cost of consolidating x and y.
When |X| 6= |Y |, to make the sets equal in size, we pad
the smaller set with null states that defer all transitions.
c
d
6
0
d
2
a
c
4
0-a,d-255
1
5
b
8
0-96,b,d-255
Finally, we must
decide which trees
to consolidate to-
gether. We as-
sume that we pro-
duce k-decision ta-
bles where k is a
power of 2. We
describe how we
solve the problem
for k = 2 ﬁrst.
We create an edge-
weighted complete