Hello. First I would like to say it is not a bug report, but rather an
expression of my inability to find answers by myself.
IMPORTANT  
I would like to see the set of correct settings that lowers memory consumption
to minimum while retaining speed at a high enough level with using
proxy/dropped requests and scale at around 1-10M pages and 1000-2000
requests/minute. Now it goes up to 4Gb (can be not correct)
NOT IMPORTANT
  1. Is it possible to use the same cache db file while running two spiders with the same name? (I tried and half of scraped data has been lost. (not 100% sure but still))
  2. Is it possible to automatically turn off windows embedded antivirus?(speed issue)
  3. Is it possible to run one spider and fetch cache from 2 sets of previously created files?
I've tried to use a AUTOTHROTTLE_TARGET_CONCURRENCY but it is don't increase
speed without increasing CONCURRENT_REQUESTS.
DEPTH_PRIORITY = -1  
SCHEDULER_DISK_QUEUE = 'scrapy.squeues.PickleLifoDiskQueue'  
SCHEDULER_MEMORY_QUEUE = 'scrapy.squeues.LifoMemoryQueue'  
(will it have any effect without -s JOBDIR setting?)  
RETRY_PRIORITY_ADJUST = 10  
DUPEFILTER_CLASS = 'scrapy.dupefilters.BaseDupeFilter'
HTTPCACHE_ENABLED = True  
It seems that httpcache and dedupfilter increases memory consumption. (even
for a simple 1-2 depth site.) Is it true?
Should I just read the source?