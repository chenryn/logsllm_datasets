physical VMCS for describing the new L2 VM, it usually marks it as active
(through the VMPTRLD instruction on Intel hardware). The L0 hypervisor
intercepts the operation and associates an allocated nested VMCS with the
guest physical VMCS specified by the L1 hypervisor. Furthermore, it fills the
initial values for the virtual VMCS and sets the nested VMCS as active for
the current VP. (It does not switch the physical VMCS though; the execution
context should remain the L1 hypervisor.) Each subsequent read or write to
the physical VMCS performed by the L1 hypervisor is always intercepted by
the L0 hypervisor and redirected to the virtual VMCS (refer to Figure 9-21).
When the L1 hypervisor launches the VM (performing an operation called
VMENTRY), it executes a specific hardware instruction (VMLAUNCH on
Intel hardware), which is intercepted by the L0 hypervisor. For
nonenlightened scenarios, the L0 hypervisor copies all the guest fields of the
virtual VMCS to another physical VMCS representing the L2 VM, writes the
host fields by pointing them to L0 hypervisor’s entry points, and sets it as
active (by using the hardware VMPTRLD instruction on Intel platforms). In
case the L1 hypervisor uses the second level address translation (EPT for
Intel hardware), the L0 hypervisor then shadows the currently active L1
extended page tables (see the following section for more details). Finally, it
performs the actual VMENTRY by executing the specific hardware
instruction. As a result, the hardware executes the L2 VM’s code.
While executing the L2 VM, each operation that causes a VMEXIT
switches the execution context back to the L0 hypervisor (instead of the L1).
As a response, the L0 hypervisor performs another VMENTRY on the
original physical VMCS representing the L1 hypervisor context, injecting a
synthetic VMEXIT event. The L1 hypervisor restarts the execution and
handles the intercepted event as for regular non-nested VMEXITs. When the
L1 completes the internal handling of the synthetic VMEXIT event, it
executes a VMRESUME operation, which will be intercepted again by the
L0 hypervisor and managed in a similar way of the initial VMENTRY
operation described earlier.
Producing a VMEXIT each time the L1 hypervisor executes a
virtualization instruction is an expensive operation, which could definitively
contribute in the general slowdown of the L2 VM. For overcoming this
problem, the Hyper-V hypervisor supports the enlightened VMCS, an
optimization that, when enabled, allows the L1 hypervisor to load, read, and
write virtualization data from a memory page shared between the L1 and L0
hypervisor (instead of a physical VMCS). The shared page is called
enlightened VMCS. When the L1 hypervisor manipulates the virtualization
data belonging to a L2 VM, instead of using hardware instructions, which
cause a VMEXIT into the L0 hypervisor, it directly reads and writes from the
enlightened VMCS. This significantly improves the performance of the L2
VM.
In enlightened scenarios, the L0 hypervisor intercepts only VMENTRY
and VMEXIT operations (and some others that are not relevant for this
discussion). The L0 hypervisor manages VMENTRY in a similar way to the
nonenlightened scenario, but, before doing anything described previously, it
copies the virtualization data located in the shared enlightened VMCS
memory page to the virtual VMCS representing the L2 VM.
 Note
It is worth mentioning that for nonenlightened scenarios, the L0
hypervisor supports another technique for preventing VMEXITs while
managing nested virtualization data, called shadow VMCS. Shadow
VMCS is a hardware optimization very similar to the enlightened VMCS.
Nested address translation
As previously discussed in the “Partitions’ physical address space” section,
the hypervisor uses the SLAT for providing an isolated guest physical
address space to a VM and to translate GPAs to real SPAs. Nested virtual
machines would require another hardware layer of translation on top of the
two already existing. For supporting nested virtualization, the new layer
should have been able to translate L2 GPAs to L1 GPAs. Due to the
increased complexity in the electronics needed to build a processor’s MMU
that manages three layers of translations, the Hyper-V hypervisor adopted
another strategy for providing the additional layer of address translation,
called shadow nested page tables. Shadow nested page tables use a technique
similar to the shadow paging (see the previous section) for directly translating
L2 GPAs to SPAs.
When a partition that supports nested virtualization is created, the L0
hypervisor allocates and initializes a nested page table shadowing domain.
The data structure is used for storing a list of shadow nested page tables
associated with the different L2 VMs created in the partition. Furthermore, it
stores the partition’s active domain generation number (discussed later in this
section) and nested memory statistics.
When the L0 hypervisor performs the initial VMENTRY for starting a L2
VM, it allocates the shadow nested page table associated with the VM and
initializes it with empty values (the resulting physical address space is
empty). When the L2 VM begins code execution, it immediately produces a
VMEXIT to the L0 hypervisor due to a nested page fault (EPT violation in
Intel hardware). The L0 hypervisor, instead of injecting the fault in the L1,
walks the guest’s nested page tables built by the L1 hypervisor. If it finds a
valid entry for the specified L2 GPA, it reads the corresponding L1 GPA,
translates it to an SPA, and creates the needed shadow nested page table
hierarchy to map it in the L2 VM. It then fills the leaf table entry with the
valid SPA (the hypervisor uses large pages for mapping shadow nested
pages) and resumes the execution directly to the L2 VM by setting the nested
VMCS that describes it as active.
For the nested address translation to work correctly, the L0 hypervisor
should be aware of any modifications that happen to the L1 nested page
tables; otherwise, the L2 VM could run with stale entries. This
implementation is platform specific; usually hypervisors protect the L2
nested page table for read-only access. In that way they can be informed
when the L1 hypervisor modifies it. The Hyper-V hypervisor adopts another
smart strategy, though. It guarantees that the shadow nested page table
describing the L2 VM is always updated because of the following two
premises:
■    When the L1 hypervisor adds new entries in the L2 nested page table,
it does not perform any other action for the nested VM (no intercepts
are generated in the L0 hypervisor). An entry in the shadow nested
page table is added only when a nested page fault causes a VMEXIT
in the L0 hypervisor (the scenario described previously).
■    As for non-nested VM, when an entry in the nested page table is
modified or deleted, the hypervisor should always emit a TLB flush
for correctly invalidating the hardware TLB. In case of nested
virtualization, when the L1 hypervisor emits a TLB flush, the L0
intercepts the request and completely invalidates the shadow nested
page table. The L0 hypervisor maintains a virtual TLB concept thanks
to the generation IDs stored in both the shadow VMCS and the nested
page table shadowing domain. (Describing the virtual TLB
architecture is outside the scope of the book.)
Completely invalidating the shadow nested page table for a single address
changed seems to be redundant, but it’s dictated by the hardware support.
(The INVEPT instruction on Intel hardware does not allow specifying which
single GPA to remove from the TLB.) In classical VMs, this is not a problem
because modifications on the physical address space don’t happen very often.
When a classical VM is started, all its memory is already allocated. (The
“Virtualization stack” section will provide more details.) This is not true for
VA-backed VMs and VSM, though.
For improving performance in nonclassical nested VMs and VSM
scenarios (see the next section for details), the hypervisor supports the “direct
virtual flush” enlightenment, which provides to the L1 hypervisor two
hypercalls to directly invalidate the TLB. In particular, the
HvFlushGuestPhysicalAddress List hypercall (documented in the TLFS)
allows the L1 hypervisor to invalidate a single entry in the shadow nested
page table, removing the performance penalties associated with the flushing
of the entire shadow nested page table and the multiple VMEXIT needed to
reconstruct it.
EXPERIMENT: Enabling nested virtualization on
Hyper-V
As explained in this section, for running a virtual machine into a L1
Hyper-V VM, you should first enable the nested virtualization
feature in the host system. For this experiment, you need a
workstation with an Intel or AMD CPU and Windows 10 or
Windows Server 2019 installed (Anniversary Update RS1
minimum version). You should create a Type-2 VM using the
Hyper-V Manager or Windows PowerShell with at least 4 GB of
memory. In the experiment, you’re creating a nested L2 VM into
the created VM, so enough memory needs to be assigned.
After the first startup of the VM and the initial configuration,
you should shut down the VM and open an administrative
PowerShell window (type Windows PowerShell in the Cortana
search box. Then right-click the PowerShell icon and select Run
As Administrator). You should then type the following command,
where the term “” must be replaced by your virtual
machine name:
Click here to view code image
Set-VMProcessor -VMName "" -
ExposeVirtualizationExtension $true
To properly verify that the nested virtualization feature is
correctly enabled, the command
Click here to view code image
$(Get-VMProcessor -VMName "
").ExposeVirtualizationExtensions
should return True.
After the nested virtualization feature has been enabled, you can
restart your VM. Before being able to run the L1 hypervisor in the
virtual machine, you should add the necessary component through
the Control panel. In the VM, search Control Panel in the Cortana
box, open it, click Programs, and the select Turn Windows
Features On Or Off. You should check the entire Hyper-V tree, as
shown in the next figure.
Click OK. After the procedure finishes, click Restart to reboot
the virtual machine (this step is needed). After the VM restarts, you
can verify the presence of the L1 hypervisor through the System
Information application (type msinfo32 in the Cortana search box.
Refer to the “Detecting VBS and its provided services” experiment
later in this chapter for further details). If the hypervisor has not
been started for some reason, you can force it to start by opening an
administrative command prompt in the VM (type cmd in the
Cortana search box and select Run As Administrator) and insert
the following command:
Click here to view code image
bcdedit /set {current} hypervisorlaunchtype Auto
At this stage, you can use the Hyper-V Manager or Windows
PowerShell to create a L2 guest VM directly in your virtual
machine. The result can be something similar to the following
figure.
From the L2 root partition, you can also enable the L1
hypervisor debugger, in a similar way as explained in the
“Connecting the hypervisor debugger” experiment previously in
this chapter. The only limitation at the time of this writing is that
you can’t use the network debugging in nested configurations; the
only supported configuration for debugging the L1 hypervisor is
through serial port. This means that in the host system, you should
enable two virtual serial ports in the L1 VM (one for the hypervisor
and the other one for the L2 root partition) and attach them to
named pipes. For type-2 virtual machines, you should use the
following PowerShell commands to set the two serial ports in the
L1 VM (as with the previous commands, you should replace the
term “” with the name of your virtual machine):
Click here to view code image
Set-VMComPort -VMName "" -Number 1 -Path 
\\.\pipe\HV_dbg
Set-VMComPort -VMName "" -Number 2 -Path 
\\.\pipe\NT_dbg
After that, you should configure the hypervisor debugger to be
attached to the COM1 serial port, while the NT kernel debugger
should be attached to the COM2 (see the previous experiment for
more details).
The Windows hypervisor on ARM64
Unlike the x86 and AMD64 architectures, where the hardware virtualization
support was added long after their original design, the ARM64 architecture
has been designed with hardware virtualization support. In particular, as
shown in Figure 9-22, the ARM64 execution environment has been split in
three different security domains (called Exception Levels). The EL
determines the level of privilege; the higher the EL, the more privilege the
executing code has. Although all the user mode applications run in EL0, the
NT kernel (and kernel mode drivers) usually runs in EL1. In general, a piece
of software runs only in a single exception level. EL2 is the privilege level
designed for running the hypervisor (which, in ARM64 is also called “Virtual
machine manager”) and is an exception to this rule. The hypervisor provides
virtualization services and can run in Nonsecure World both in EL2 and EL1.
(EL2 does not exist in the Secure World. ARM TrustZone will be discussed
later in this section.)
Figure 9-22 The ARM64 execution environment.
Unlike from the AMD64 architecture, where the CPU enters the root mode
(the execution domain in which the hypervisor runs) only from the kernel
context and under certain assumptions, when a standard ARM64 device
boots, the UEFI firmware and the boot manager begin their execution in EL2.
On those devices, the hypervisor loader (or Secure Launcher, depending on
the boot flow) is able to start the hypervisor directly and, at later time, drop
the exception level to EL1 (by emitting an exception return instruction, also
known as ERET).
On the top of the exception levels, TrustZone technology enables the
system to be partitioned between two execution security states: secure and
non-secure. Secure software can generally access both secure and non-secure
memory and resources, whereas normal software can only access non-secure
memory and resources. The non-secure state is also referred to as the Normal
World. This enables an OS to run in parallel with a trusted OS on the same
hardware and provides protection against certain software attacks and
hardware attacks. The secure state, also referred as Secure World, usually
runs secure devices (their firmware and IOMMU ranges) and, in general,
everything that requires the processor to be in the secure state.
To correctly communicate with the Secure World, the non-secure OS
emits secure method calls (SMC), which provide a mechanism similar to
standard OS syscalls. SMC are managed by the TrustZone. TrustZone
usually provides separation between the Normal and the Secure Worlds
through a thin memory protection layer, which is provided by well-defined
hardware memory protection units (Qualcomm calls these XPUs). The XPUs
are configured by the firmware to allow only specific execution
environments to access specific memory locations. (Secure World memory
can’t be accessed by Normal World software.)
In ARM64 server machines, Windows is able to directly start the
hypervisor. Client machines often do not have XPUs, even though TrustZone
is enabled. (The majority of the ARM64 client devices in which Windows
can run are provided by Qualcomm.) In those client devices, the separation
between the Secure and Normal Worlds is provided by a proprietary
hypervisor, named QHEE, which provides memory isolation using stage-2
memory translation (this layer is the same as the SLAT layer used by the
Windows hypervisor). QHEE intercepts each SMC emitted by the running
OS: it can forward the SMC directly to TrustZone (after having verified the
necessary access rights) or do some work on its behalf. In these devices,
TrustZone also has the important responsibility to load and verify the
authenticity of the machine firmware and coordinates with QHEE for
correctly executing the Secure Launch boot method.
Although in Windows the Secure World is generally not used (a
distinction between Secure/Non secure world is already provided by the
hypervisor through VTL levels), the Hyper-V hypervisor still runs in EL2.
This is not compatible with the QHEE hypervisor, which runs in EL2, too.
To solve the problem correctly, Windows adopts a particular boot strategy;
the Secure launch process is orchestrated with the aid of QHEE. When the
Secure Launch terminates, the QHEE hypervisor unloads and gives up
execution to the Windows hypervisor, which has been loaded as part of the
Secure Launch. In later boot stages, after the Secure Kernel has been
launched and the SMSS is creating the first user mode session, a new special
trustlet is created (Qualcomm named it as “QcExt”). The trustlet acts as the
original ARM64 hypervisor; it intercepts all the SMC requests, verifies the
integrity of them, provides the needed memory isolations (through the
services exposed by the Secure Kernel) and is able to send and receive
commands from the Secure Monitor in EL3.
The SMC interception architecture is implemented in both the NT kernel
and the ARM64 trustlet and is outside the scope of this book. The
introduction of the new trustlet has allowed the majority of the client ARM64
machines to boot with Secure Launch and Virtual Secure Mode enabled by
default. (VSM is discussed later in this chapter.)
The virtualization stack
Although the hypervisor provides isolation and the low-level services that
manage the virtualization hardware, all the high-level implementation of
virtual machines is provided by the virtualization stack. The virtualization
stack manages the states of the VMs, provides memory to them, and
virtualizes the hardware by providing a virtual motherboard, the system
firmware, and multiple kind of virtual devices (emulated, synthetic, and
direct access). The virtualization stack also includes VMBus, an important
component that provides a high-speed communication channel between a
guest VM and the root partition and can be accessed through the kernel mode
client library (KMCL) abstraction layer.
In this section, we discuss some important services provided by the
virtualization stack and analyze its components. Figure 9-23 shows the main
components of the virtualization stack.
Figure 9-23 Components of the virtualization stack.
Virtual machine manager service and worker
processes
The virtual machine manager service (Vmms.exe) is responsible for
providing the Windows Management Instrumentation (WMI) interface to the
root partition, which allows managing the child partitions through a
Microsoft Management Console (MMC) plug-in or through PowerShell. The
VMMS service manages the requests received through the WMI interface on
behalf of a VM (identified internally through a GUID), like start, power off,
shutdown, pause, resume, reboot, and so on. It controls settings such as which
devices are visible to child partitions and how the memory and processor
allocation for each partition is defined. The VMMS manages the addition and
removal of devices. When a virtual machine is started, the VMM Service also
has the crucial role of creating a corresponding Virtual Machine Worker
Process (VMWP.exe). The VMMS manages the VM snapshots by redirecting
the snapshot requests to the VMWP process in case the VM is running or by
taking the snapshot itself in the opposite case.
The VMWP performs various virtualization work that a typical monolithic
hypervisor would perform (similar to the work of a software-based
virtualization solution). This means managing the state machine for a given
child partition (to allow support for features such as snapshots and state
transitions), responding to various notifications coming in from the
hypervisor, performing the emulation of certain devices exposed to child
partitions (called emulated devices), and collaborating with the VM service
and configuration component. The Worker process has the important role to
start the virtual motherboard and to maintain the state of each virtual device
that belongs to the VM. It also includes components responsible for remote
management of the virtualization stack, as well as an RDP component that
allows using the remote desktop client to connect to any child partition and
remotely view its user interface and interact with it. The VM Worker process
exposes the COM objects that provide the interface used by the Vmms (and
the VmCompute service) to communicate with the VMWP instance that
represents a particular virtual machine.
The VM host compute service (implemented in the Vmcompute.exe and
Vmcompute.dll binaries) is another important component that hosts most of
the computation-intensive operations that are not implemented in the VM
Manager Service. Operation like the analysis of a VM’s memory report (for
dynamic memory), management of VHD and VHDX files, and creation of
the base layers for containers are implemented in the VM host compute
service. The Worker Process and Vmms can communicate with the host
compute service thanks the COM objects that it exposes.
The Virtual Machine Manager Service, the Worker Process, and the VM
compute service are able to open and parse multiple configuration files that
expose a list of all the virtual machines created in the system, and the
configuration of each of them. In particular:
■    The configuration repository stores the list of virtual machines
installed in the system, their names, configuration file and GUID in
the data.vmcx file located in C:\ProgramData\Microsoft\Windows
Hyper-V.
■    The VM Data Store repository (part of the VM host compute service)
is able to open, read, and write the configuration file (usually with
“.vmcx” extension) of a VM, which contains the list of virtual devices
and the virtual hardware’s configuration.
The VM data store repository is also used to read and write the VM Save
State file. The VM State file is generated while pausing a VM and contains
the save state of the running VM that can be restored at a later time (state of
the partition, content of the VM’s memory, state of each virtual device). The
configuration files are formatted using an XML representation of key/value
pairs. The plain XML data is stored compressed using a proprietary binary
format, which adds a write-journal logic to make it resilient against power
failures. Documenting the binary format is outside the scope of this book.
The VID driver and the virtualization stack
memory manager
The Virtual Infrastructure Driver (VID.sys) is probably one of the most
important components of the virtualization stack. It provides partition,
memory, and processor management services for the virtual machines
running in the child partition, exposing them to the VM Worker process,
which lives in the root. The VM Worker process and the VMMS services use
the VID driver to communicate with the hypervisor, thanks to the interfaces
implemented in the Windows hypervisor interface driver (WinHv.sys and
WinHvr.sys), which the VID driver imports. These interfaces include all the
code to support the hypervisor’s hypercall management and allow the
operating system (or generic kernel mode drivers) to access the hypervisor
using standard Windows API calls instead of hypercalls.