title:The Influence of Code Coverage Metrics on Automated Testing Efficiency
in Android
author:Stanislav Dashevskyi and
Olga Gadyatskaya and
Aleksandr Pilgun and
Yury Zhauniarovich
The Influence of Code Coverage Metrics on 
Automated Testing Efficiency in Android
Stanislav Dashevskyi
SnT, University of Luxembourg
PI:EMAIL
Olga Gadyatskaya
Aleksandr Pilgun
Yury Zhauniarovich
SnT, University of Luxembourg
SnT, University of Luxembourg
Qatar Computing Research Institute, HBKU
PI:EMAIL
PI:EMAIL
PI:EMAIL
Context
Automated testing and dynamic analysis techniques are critical for ensuring the reliability and the security of third-party Android apps.
One of the biggest challenges for these techniques is effective app exploration in the black-box setting. Android apps have many entry points, 
and their source code is unavailable for inspection. State-of-the-art tools utilize a wide variety of app exploration strategies that range from 
generating random GUI events to systematic exploration of apps models [1], but there is no agreement on the success criteria.
Code coverage is a common metric used to evaluate efficiency of automated testing and dynamic analysis tools [1], and some of these tools 
utilize code coverage as a component of a fitness function to guide app exploration and find more bugs [2]. 
Code coverage exists in many flavors, and there is currently no agreement in the community on which metrics to use in the fitness function. 
Are they all the same, or is there a code coverage granularity that works best? We make the first step towards reaching this agreement.
Hypothesis
Combining different 
granularities of code 
coverage can be beneficial 
for achieving better results 
in automated testing of 
Android apps.
Experiment setting
Sapienz [3] is a state-of-the-art bug finding tool for Android apps. It relies on Monkey [3] to generate 
random input events; and applies a genetic algorithm to event sequences. The test selection function 
combines code coverage, the number of found bugs, and the size of a test sequence. Sapienz is 
designed to utilize activity, method and statement coverage. We set out to evaluate how these metrics 
fare against each other in finding bugs.
Activity coverage was computed by Sapienz, and method and instruction coverage were measured with 
our own ACVTool (the tool is currently available at https://github.com/pilgun/acvtool).
Experiment 1: Comparing the metrics individually
Experiment 2: Evaluating the randomness impact
We randomly selected 500 apps from Google Play and executed 
them with Sapienz using each coverage metric.
We randomly selected 100 apps, and ran Sapienz 5 times for each 
app using each coverage metric. 
Conclusion: Different metrics find different bugs.
Conclusion: Even in multiple runs, no individual coverage metric was 
able to find all bugs detected by others.
Experiment 3: 1 run x 3 metrics vs 3 runs x 1 metric
Conclusions
For the 100 apps selected for the second experiment, we compute the number of faults 
detected jointly by 3 metrics and the number of faults found by each individual metric in 
3 runs. 
We apply Wilcoxon test to evaluate the hypothesis that Sapienz with 3 metrics will on 
average find more faults than Sapienz with 1 metrics executed 3 times. The results of the 
test allowed to reject the null-hypothesis (that there is no difference) with p-values equal 
0.008, 0.0005, and 0.007 for activity, method and instruction coverage, respectively. 
Our results show that different code coverage 
granularities find different bugs, and (given an 
equal execution time) that a combination of 3 
different granularities will find more bugs than 
any of these metric granularities individually.
Open questions:
- How to reduce the total testing time?
- Will our findings hold for other tools?
References
Conclusion: The three metrics, when executed once, find on average more bugs in an app 
than each individual metric applied within 3 runs.
[1] S. R. Choudhary, A. Gorla and A. Orso “Automated test input generation for Android: Are we 
there yet?” in ASE 2015
[2] K. Mao, M. Harman and Y. Jia “Sapienz: Multi-objective automated testing for Android 
applicatiions” in ISSTA 2016.
[3] Google, UI/App Exerciser Monkey, https://developer.android.com/studio/test/monkey
Table1:CrashesfoundbySapienzin500appsCoveragemetric#uniquecrashes#faultyapps#crashtypesActivity28720323Method31723123Instruction32222523Total55529526onhowappsaresupposedtobehave,testingtoolsneedtoautomaticallyuncovertheirexecutionpaths.Inthisrespect,codecoveragebecomesanessentialmetricthatestimateshowwellanapphasbeenexercised[2].Moreover,severalstate-of-the-artautomatedtriggeringandtestingtoolsusecodecoveragetoguidetheexplorationstrategyofapps,e.g.,[5–7].TheimportanceofcodecoveragemetricsforautomatedtestinganddynamicanalysisofAndroidappsisimmediatelyevidentfromtheaforementionedrelatedwork.Yet,wecouldnotfndintheliteratureanydiscussiononwhichspecifccodecoveragemetrics(orgranularitylevels)workbestforAndroid.Therefore,theaimofourstudyistofllthisgap.3OURSTUDYToinvestigatewhetherdiferentlevelsofgranularityofcodecoveragemetrichaveanefectontheresultsofautomatedtestdesigntools,weworkwithSapienz[7].Itfrstgeneratesasetofrandom“seed”testsequences,andthenmutatesthemtryingtoimproveaPareto-optimalftnessfunctionthatdependsonthreecriteria:codecoverage,thelengthofatestsequence,andthenumberofappcrashesthatthetestsequencehasuncovered.Sapienzcanusethreecodecoveragegranularities.State-mentcoverageismeasuredbyEMMA[9],apopularbutoutdatedtoolthatworksonlyforappswithsourcecodeavail-able.MethodcoverageismeasuredbyELLA[3],anotherpopularbutnolongersupportedtoolthatoftenfailswithmorerecentAndroidapps.Inourexperiments,wereplacedEMMAandELLAwithACVToolthatmeasuresbytecodeinstructionandmethodcoverage[8].Finally,activitycov-erageismeasuredbyaplugininSapienz.NotethatthecodecoveragemeasurementitselfdoesnotinterferewiththesearchalgorithmsusedbySapienz.Asourdataset,wehaverandomlyselected500appsfromtheGooglePlaymarket,andranSapienzagainsteachoftheseapps,usingitsdefaultparameters.EachapphasbeentestedusingtheactivitycoverageprovidedbySapienz,andthemethodandinstructioncoveragesuppliedbyACVTool[8].Onaverage,eachapphasbeentestedbySapienzfor3hours(foreachcoveragemetric).Aftereachrun,wecollectedthecrashinformation(ifany),whichincludedthecomponentsofappsthatcrashedandJavaexceptionstacktraces.3.1DescriptivestatisticsofcrashesTable1showsthenumbersofcrashesgroupedbycoveragemetricthatSapienzhasfoundinthe500apps.Weconsideruniquecrashesasuniquecombinationsofanapplication,itsFigure1:CrashesfoundbySapienzin500appscomponentwherecrashoccursandthelineofcodethattriggeredanexception,andaspecifcJavaexceptiontype.Intotal,Sapienzhasfound295appsoutof500tobefaulty(atleastonecrashdetected),andithaslogged555uniquecrasheswithallthreecoveragemetrics.Figure1summarizesthecrashdistributionforthecoveragemetricsthatfoundit.Aswecansee,theintersectionofallcodecoveragemetrics’resultscontains115uniquecrashes(20%oftotalfoundcrashes).Individualcoveragemetricshavefound58%(instructioncoverage),57%(methodcoverage),and51%(activitycoverage)ofthetotalfoundcrashes.Thesefndingssuggestthatdiferentcodecoveragemetricsarecomplementaryandcouldbeappliedtogetherinordertoachievethebesttestingresults.3.2EvaluatingbehavioronmultiplerunsLikemanyotherautomatedtestingtoolsforAndroid,Sapienzisnon-deterministic,andourfndingsmaybeafectedbythis.Todeterminetheimpactofcoveragemetricsinfndingcrashesonaverage,weneedtoinvestigatehowcrashdetectionbe-havesinmultipleruns.Thus,wehaveperformedthefollowingtwoexperimentsonarandomlyselectedsetof100apks.Performancein5runs.WehaverunSapienzfor5timeswitheachcoveragemetricsforeachof100apps.Thisgivesustwocrashpopulations:1thatcontainscrashesdetectedinthe100appsduringthefrstexperiment,and5thatcontainscrashesdetectedinthesameappsrunningSapienz5times.Table2summarizesthepopulationsofcrashesfoundbySapienzwitheachofthecoveragemetrics.Asexpected,runningSapienzmultipletimesincreasestheamountoffoundcrashes.Inthisexperiment,weareinterestedintheproportionofcrashescontributedbycoveragemetricsindividually.Ifcoveragemetricsareinterchangeable(theydonotdiferincapabilitiesoffndingcrashes,andtheywill,eventually,fndthesamecrashes),theproportionofcrashesfoundbyindividualmetricstothetotalcrashespopulationcanbeexpectedtosignifcantlyincrease:eachmetric,givenmoreattempts,willfndalargerproportionofthetotalcrashpopulation.However,asshowninTable2,onlytheactivitycoveragehasfoundasignifcantlylargerproportionoftotalcrashpopulation(59%from45%).Theinstructioncoveragehasslightlyincreasedperformance(59%from54%),whilethemethodcoveragehasfaredworse(55%from62%).Thesefndingssuggestthatthecoveragemetricsarenot