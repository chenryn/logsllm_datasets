### Table 5: Efficiency of Assertion Checks in Preventing System Failures

Table 5 also illustrates the effectiveness of assertion checks in preventing system failures. The two rightmost columns in the table show the total number of runs in which assertions detected errors. For instance, the `mgr_armor_info` element detected 27 errors, with 19 of these errors being successfully recovered. The Venn diagram to the right of the first row depicts the relationship between the set of runs experiencing system failure and the set of runs in which an assertion was triggered.

### Assertions and Incremental Microcheckpointing

The data further indicate that assertions, when combined with incremental microcheckpointing, were able to prevent system failures in 58% of the cases (27 out of 64 runs where assertions were triggered). It is important to note that after an event within a message is processed by an element, only that element's state is copied to the checkpoint buffer. Consequently, incidental corruption to other elements (e.g., an error causing the event to overwrite another element’s data) will not be saved to the checkpoint buffer. This ensures that a clean copy of the corrupted element’s state exists in the ARMOR’s checkpoint for recovery, as long as future events do not legitimately write to the corrupted element.

### Late Detection of Errors

In 27 cases, assertions detected the error too late to prevent system failures. For example, 14 out of 17 runs in which assertions detected errors in the `node_mgmt` element resulted in system failures. The `node_mgmt` element translates hostnames into daemon IDs. When the SCC instructs the FTM to execute an application on a specific set of nodes, the FTM translates the hostnames to daemon IDs via the `node_mgmt` element. If the element cannot perform the translation, it defaults to a daemon ID of zero. The FTM then attempts to send a message to the translated daemon ID, but at the time of these experiments, it did not check to ensure that the returned daemon ID was non-zero. If the translation failed due to an error, the FTM’s daemon detected that the message destination ID was invalid, but this detection occurred too late, as the error had already propagated outside the FTM. This issue was resolved by adding checks to the translation results before sending the message.

### Lessons Learned

1. **SIFT Overhead**:
   - SIFT overhead should be kept minimal. System designers must be aware that SIFT solutions have the potential to degrade the performance and even the dependability of the applications they are intended to protect. Our experiments show that the functionality in SIFT can be distributed among several processes throughout the network, ensuring that the overhead imposed by SIFT processes is insignificant while the application is running.

2. **SIFT Recovery Time**:
   - Minimizing SIFT process recovery time is crucial for two reasons: (1) recovering SIFT processes can affect application performance by contending for processor and network resources, and (2) applications requiring support from the SIFT environment are impacted when SIFT processes become unavailable. Our results indicate that fully recovering a SIFT process takes approximately 0.5 seconds. The mean overhead as seen by the application from SIFT recovery is less than 5%, considering 10 out of roughly 800 failures from register, text-segment, and heap injections that caused the application to block or restart due to the unavailability of a SIFT process. The overhead from recovery is negligible when these 10 cases are excluded.

3. **SIFT/Application Interface**:
   - The SIFT/application interface should be kept simple. In any multiprocess SIFT design, some SIFT processes must be coupled to the application to provide error detection and recovery. The Execution ARMORs play this role in our SIFT environment. To minimize complexity, all recovery actions and operations that affect the global system (such as job submission, preparing the node to execute an application, and detecting remote node failures) are delegated to a remote SIFT process decoupled from the application’s execution. This strategy has proven effective, as only 5 out of 373 observed Execution ARMOR failures led to system failures.

4. **SIFT Availability**:
   - SIFT availability directly impacts the application. Low recovery time and aggressive checkpointing of SIFT processes help minimize SIFT environment downtime, making the environment available for processing application requests and recovering from application failures. If the SIFT environment cannot recover from a failure, the responsibility falls on the SCC or the ground station to recover the REE cluster. This externally controlled recovery can be quite expensive in terms of application downtime, as the entire cluster must be diagnosed and reinitialized before restarting the SIFT environment. Downtime can range from hours to days if ground control is required, underscoring the need for rapid onboard detection and recovery.

5. **System Failures**:
   - System failures are not necessarily fatal. Only 11 out of 10,000 injections resulted in a system failure where the SIFT environment could not recover from the error. These system failures were not catastrophic in the sense of impacting the spacecraft or SCC. In fact, none affected an executing application. To reduce the number of system failures, a timeout can be placed on the application connecting to the SIFT environment. Because the time between submission and connection is usually small, errors occurring during the critical phase of preparing the SIFT environment for a new application can be detected using this timeout without significant delay. Once the application starts, our experience has shown that it is well-protected and relatively immune to errors in the SIFT environment.

### Related Work

Few experimental assessments of distributed fault tolerance environments have been conducted. Notable exceptions include:

- **MARS**: Three types of physical fault injection (pin-level injections, heavy-ion radiation from a Californium-252 isotope, and electromagnetic interference) were used to study the fail-silence coverage of the Maintainable Real-Time System (MARS). A real-time control program was used as the test application. A later study compared software-implemented fault injection to the three physical injection approaches.
- **Delta-4**: Pin-level injections were performed to evaluate the fail-silence coverage of the Delta-4 atomic multicast protocol. Fail-silence was achieved by designing network interface cards around duplicated hardware on which the atomic multicast protocol executes.
- **Hades**: Software-implemented fault injectors were used to inject errors into the Chorus microkernel and the Hades middleware, a collection of run-time services for real-time applications executing on COTS processors. This experiment evaluated the coverage of the Hades error detection mechanisms while running an object-tracking application.

It is unclear whether these studies validated how well the fault tolerance environment recovers from its own errors or how such errors impact performance. All were primarily interested in showing that the environment’s error detection and masking were sufficient to maintain fail-silence.

### Conclusion

This paper presents a series of experiments in which the error detection and recovery mechanisms of a distributed SIFT environment were stressed through over 10,000 error injections into a Mars Rover texture analysis program and the SIFT processes themselves. The results show that:

1. Structuring fault injection experiments to progressively stress the error detection and recovery mechanisms is a useful approach to evaluating performance and error propagation.
2. Even though the probability of correlated failures is small, their potential impact on application availability is significant.
3. The SIFT environment successfully recovered from all correlated failures involving the application and a SIFT process because the processes performing error detection and recovery were decoupled from the failed processes.
4. Targeted injections into dynamic data on the heap were useful in further investigating system failures brought about by error propagation. Only non-pointer values were injected, and injections were limited to specific modules within the SIFT process to better trace the error effects. Assertions within the SIFT processes were shown to reduce the number of system failures from data error propagation by up to 42%. This suggests that detection mechanisms can be incorporated into the common ARMOR infrastructure to preemptively check for errors before state changes occur within the SIFT processes, thus decreasing the probability of error propagation and checkpoint corruption.

### Acknowledgments

This work was supported in part by NASA/JPL contract 961345 and by NSF grants CCR 00-86096 ITR and CCR 99-02026.

### References

[References listed here as per the original document]