Table  5  also  shows  the  efficiency  of  assertion  checks  in 
preventing system failures.  The rightmost two columns in the 
table  represent  the  total  number  of  runs  in  which  assertions 
detected 
the 
mgr_armor_info  element  detected  27  errors,  and  19  of 
those errors were successfully recovered.  The Venn diagram to 
the right of the first row depicts the relationship between the set 
of runs experiencing system failure and the set of runs in which 
an assertion fired. 
assertions 
example, 
The  data  also  show  that  assertions  coupled  with  the 
incremental  microcheckpointing  were  able  to  prevent  system 
failures in 58% of the cases (27 of 64 runs in which assertions 
fired).  Recall that after an event within a message is processed 
by  an  element,  only  this  element’s  state  is  copied  to  the 
checkpoint  buffer.    Incidental  corruption  to  other  elements 
(e.g., an error causing the event to overwrite another element’s 
data) will not be saved to the checkpoint buffer.  Thus, a clean 
copy  of  the  corrupted  element’s  state  exists  in  the  ARMOR’s 
checkpoint  for  recovery  as  long  as  future  events  do  not 
legitimately write to the corrupted element. 
On the other hand, assertions detected the error too late to 
prevent system failures in 27 cases.  For example, 14 of the 17 
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:16:44 UTC from IEEE Xplore.  Restrictions apply. 
time 
runs  in  which  assertions  detected  errors  in  the  node_mgmt 
element  resulted  in  system  failures.    This  element  translates 
hostnames into daemon IDs.  When the SCC instructs the FTM 
to execute an application on a particular set of nodes, the FTM 
translates  the  hostnames  to  daemon  IDs  via  the  node_mgmt 
element.  If the element cannot perform the translation, it uses a 
default daemon ID of zero for its response.  The FTM attempts 
to send a message to the translated daemon ID, but at the time 
of  these  experiments  it  did  not  check  to  make  sure  that  the 
returned  daemon  ID  is  nonzero.    If  the  translation  failed 
because  of  an  error,  the  FTM’s  daemon  detected  that  the 
message  destination  ID  was  invalid.    The  detection  occurred 
too  late,  however,  since  the  error  already  propagated  outside 
the FTM.  This problem was rectified by adding checks to the 
translation results before sending the message. 
7  Lessons Learned 
SIFT overhead should be kept small.  System designers 
must be aware that SIFT solutions have the potential to degrade 
the performance and even the dependability of the applications 
they  are  intended  to  protect.    Our  experiments  show  that  the 
functionality  in  SIFT  can  be  distributed  among  several 
processes throughout the network so that the overhead imposed 
by the SIFT processes is insignificant while the application is 
running. 
SIFT  recovery  time  should  be  kept  small.    Minimizing 
the  SIFT  process  recovery 
is  desirable  from  two 
standpoints: (1) recovering SIFT processes have the potential to 
affect application performance by contending for processor and 
network resources, and (2) applications requiring support from 
the  SIFT  environment  are  affected  when  SIFT  processes 
become unavailable.  Our results indicate that fully recovering 
a SIFT process takes approximately 0.5 s.  The mean overhead 
as seen by the application from SIFT recovery is less than 5%, 
which takes into account 10 out of roughly 800 failures from 
register,  text-segment  and  heap  injections  that  caused  the 
application to block or restart because of the unavailability of a 
SIFT  process.    The  overhead  from  recovery  is  insignificant 
when these 10 cases are neglected. 
SIFT/application interface should be kept simple.  In any 
multiprocess  SIFT  design,  some  SIFT  processes  must  be 
coupled  to  the  application  in  order  to  provide  error  detection 
and  recovery.    The  Execution  ARMORs  play  this  role  in  our 
SIFT environment.  Because of this dependency, it is important 
to  make  the  Execution  ARMORs  as  simple  as  possible.    All 
recovery  actions  and  those  operations  that  affect  the  global 
system (such as job submission, preparing the node to execute 
an  application,  and  detecting  remote  node  failures)  are 
delegated to a remote SIFT process that is decoupled from the 
application’s execution.  This strategy appears to work, as only 
5  of  373  observed  Execution  ARMOR  failures6  led  to  system 
failures. 
SIFT availability impacts the application.  Low recovery 
time and aggressive checkpointing of the SIFT processes help 
minimize 
the 
environment  available  for  processing application requests and 
for recovering from application failures. 
the  SIFT  environment  downtime,  making 
If the SIFT environment cannot recover from a failure, then 
responsibility rests on the SCC or the ground station to recover 
6   SIGINT,  SIGSTOP,  register,  and  text-segment  injections  caused  100, 
98, 80,  and  95 failures, respectively. 
the REE cluster.  This externally controlled recovery, however, 
can be quite expensive in terms of application downtime, since 
the  entire  cluster  must  be  diagnosed  and  reinitialized  before 
restarting  the  SIFT  environment.    Downtime  can  be  on  the 
order  of  hours  if  not  days  under  such  scenarios  if  ground 
control  is  required,  underscoring  the  need  for  rapid  onboard 
detection and recovery. 
System failures are not necessarily fatal.  Only 11 of the 
10,000 injections resulted in a system failure in which the SIFT 
environment  could  not  recover  from  the  error.    These  system 
failures  were  not  catastrophic  in  the  sense  of  impacting  the 
spacecraft  or  SCC.    In  fact,  none  affected  an  executing 
application. 
To reduce the number of system failures, a timeout can be 
placed on the application connecting to the SIFT environment.  
Because the time between submission and connection is usually 
small,  errors  that  occur  in  the  critical  phase  of  preparing  the 
SIFT environment for a new application can be detected using 
this  timeout  without  significant  delay.    Once  the  application 
starts,  our  experience  has  shown  that  it  is  well-protected  and 
relatively immune to errors in the SIFT environment. 
8  Related Work 
in 
Few experimental assessments of distributed fault tolerance 
environments have been undertaken.  Three notable exceptions 
include: 
MARS.    Three  types  of  physical  fault injection (pin-level 
injections, heavy-ion radiation from a Californium-252 isotope, 
and  electromagnetic  interference)  were  used  to  study  the  fail 
silence  coverage  of  the  Maintainable  Real-Time  System 
(MARS)  [13]. 
these 
experiments through process duplication across nodes.  A real-
time control program was used as the test application for these 
experiments.    A  later  study  compared  software-implemented 
fault injection to the three physical injection approaches [9]. 
  MARS  achieved  fail  silence 
Delta-4.    Pin-level  injections  were  performed  to  evaluate 
the  fail  silence  coverage  of  the  Delta-4  atomic  multicast 
protocol [1].  Fail silence was achieved by designing network 
interface  cards  around  duplicated  hardware  on  which  the 
atomic multicast protocol executes. 
Hades.  Software-implemented fault injectors were used to 
inject  errors  into  the  Chorus  microkernel  and  the  Hades 
middleware,  a  collection  of  run-time  services  for  real-time 
applications  executing  on  COTS  processors  [6]. 
  This 
experiment evaluated the coverage of the Hades error detection 
mechanisms while running an object-tracking application. 
It is not clear if any of these studies validated how well the 
fault  tolerance  environment  recovers  from  its  own  errors  or 
how  such  errors  impact  performance.    All  were  primarily 
interested in showing that the environment’s error detection and 
masking were sufficient to maintain fail silence. 
In addition to the three environments presented above, there 
are  several  other  projects  involved  in  providing  software-
implemented  fault  tolerance.    AQuA  [7]  and  Eternal  [17] 
replicate  CORBA  objects.    Arjuna  [20]  achieves  reliability 
through transactions and replication.  GUARDS [19] provides a 
generic 
integrity 
management.    CoCheck  [21],  FT-MPI  [8],  and  MPI/FT  [3] 
cater specifically to MPI applications by offering synchronous 
checkpointing,  extended  MPI  function  semantics  for  error 
handling,  and  replication,  respectively.    Finally,  FTCT  [11] 
framework 
tolerance 
fault 
and 
for 
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:16:44 UTC from IEEE Xplore.  Restrictions apply. 
adds  fault  tolerance  to  cluster  management  by  replicating  a 
central manager. 
None  of  these  environments  has  been  evaluated  using  a 
substantial application.  Most use either synthetic benchmarks 
or  a  program  with  the  complexity  on  the  order  of  an  echo 
server.  It is difficult to evaluate the SIFT environment’s ability 
to  handle  correlated  failures  and  error  propagation  since  the 
application  process  interactions—including  those  with  other 
application processes and with the SIFT processes—are simple 
and infrequent. 
Finally, few of these SIFT solutions have utilized extensive 
fault injection to demonstrate that their infrastructures are fault-
tolerant.  Some have undergone testing in which the user kills 
processes from the command line, but few have gone beyond 
using crash and hang failures to validate functionality.  As our 
experiments  have  shown,  injections  into  the  text  segment, 
registers, and heap are required to see correlated failures, error 
propagation, corrupted checkpoints, and system failures. 
9  Conclusion 
This paper has presented a series of experiments in which 
the  error  detection  and  recovery  mechanisms  of  a  distributed 
SIFT  environment  have  been  stressed  through  over  10,000 
error injections into a Mars Rover texture analysis program and 
the SIFT processes themselves.  The results show that: 
1.  Structuring the fault injection experiments to progressively 
stress  the  error  detection  and  recovery  mechanisms  is  a 
useful  approach  to  evaluating  performance  and  error 
propagation. 
2.  Even though the probability for correlated failures is small, 
is 
impact  on  application  availability 
its  potential 
significant.   
3.  The  SIFT  environment  successfully  recovered  from  all 
correlated  failures  involving  the  application  and  a  SIFT 
process  because  the  processes  performing  error  detection 
and recovery were decoupled from the failed processes. 
4.  Targeted  injections  into  dynamic  data  on  the  heap  were 
useful  in  further  investigating  system  failures  brought 
about by error propagation.  Only non-pointer values were 
injected,  and  injections  were  limited  to  specific  modules 
within  the  SIFT  process  to  better  trace  the  error  effects.  
Assertions  within  the  SIFT  processes  were  shown  to 
reduce  the  number  of  system  failures  from  data  error 
propagation  by  up  to  42%.    This  suggests  that  detection 
mechanisms can be incorporated into the common ARMOR 
infrastructure to preemptively check for errors before state 
changes occur within the SIFT processes, thus decreasing 
the  probability  of  error  propagation  and  checkpoint 
corruption. 
Acknowledgments 
This  work  was  supported  in  part  by  NASA/JPL  contract 
961345  and  by  NSF  grants  CCR  00-86096  ITR  and  CCR 
99-02026. 
References 
[1] 
J. Arlat, et al., “Experimental evaluation of the fault tolerance of an 
atomic multicast system,” in IEEE Trans. on Reliability, vol. 39, 
no. 4, pp. 455-467, October 1990. 
S. Bagchi, “Hierarchical error detection in a software-implemented 
fault  tolerance  (SIFT)  environment,”  Ph.D.  Thesis,  University  of 
Illinois, Urbana, IL, 2001. 
R. Batchu, et al., “MPI/FT: Architecture and taxonomies for fault-
tolerant,  message-passing  middleware  for  performance-portable 
[2] 
[3] 
[13] 
[14] 
[17] 
[18] 
[19] 
[20] 
[21] 
[22] 
[23] 
[24] 
[4] 
[5] 
[6] 
[7] 
[8] 
[9] 
[10] 
(REE) 
fault-tolerant 
parallel  computing,”  in  Proceedings  of  the  First  International 
Symposium of Cluster Computing and the Grid, pp. 26-33, 2001. 
J. Beahan, et al., “Detailed radiation fault modeling of the remove 
exploration  and  experimentation  (REE)  first  generation  testbed 
architecture,” in Proceedings of the IEEE Aerospace Conference, 
vol. 5, pp. 279-281, 2000. 
F.  Chen,  et  al.,  “Demonstration  of  the  Remote  Exploration  and 
Experimentation 
parallel-processing 
supercomputer for spacecraft onboard scientific data processing,” 
in DSN-00, pp. 367-372, 2000. 
P.  Chevocot  and  I.  Puaut,  “Experimental  evaluation  of  the  fail-
silent  behavior  of  a  distributed  real-time  run-time  support  build 
from COTS components,” in DSN-01, pp. 304-313, 2001. 
M. Cukier, et al., “AQuA: An adaptive architecture that provides 
dependable distributed objects,” in SRDS-17, pp. 245-253, 1998. 
G. Fagg and J. Dongarra, “FT-MPI: Fault tolerant MPI, supporting 
dynamic  applications  in  a  dynamic  world,”  Lecture  Notes  in 
Computer  Science,  vol.  1908,  Springer-Verilag:  Berlin,  pp.  346-
353, 2000. 
E.  Fuchs,  “Validating  the  fail-silence  assumption  of  the  MARS 
architecture,” in DCCA-6, pp. 225-247, 1998. 
J. Gunnels, D. Katz, E. Quintana-Ortí, and R. van de Geijn, “Fault-
tolerant  high-performance  matrix  multiplication: 
theory  and 
practice,” in DSN-01, pp. 47-56, 2001. 
for 
[12] 
reliable  high-performance  computing,” 
[11]  M. Li, D. Goldberg, W. Tao, and Y. Tamir, “Fault-tolerant cluster 
management 
in 
Proceedings of the 13th Conference on Parallel and Distributed 
Computing and Systems, pp. 480-485, 2001. 
Z.  Kalbarczyk,  R.  Iyer,  S.  Bagchi,  K.  Whisnant,  “Chameleon:  A 
software infrastructure for adaptive fault tolerance,” IEEE Trans. 
on Parallel and Distributed Systems, vol. 10, no. 6, pp. 560-579, 
1999. 
J. Karlsson, J. Arlat, and G. Leber, “Application of three physical 
fault  injection  techniques  to  the  experimental  assessment  of  the 
MARS architecture,” in DCCA-5, pp. 150-161, 1995. 
S. Kerns, et al., “The design of radiation-hardened ICs for space: A 
compendium of approaches,” Proceedings of the IEEE, vol. 76, no. 
11, pp. 1470-1509, November 1988. 
H.  Maderia,  R.  Some,  F.  Moereira,  D.  Costa,  D.  Rennels, 
“Experimental  evaluation  of  a  COTS  system 
for  space 
applications,” in DSN-02, 2002. 
[15] 
[16]  Message  Passing  Interface  Forum,  “MPI-2:  Extensions  to  the 
http://www.mpi-
Passing 
Interface,” 
Message 
forum.org/docs/mpi-20.ps. 
L. Moser, P. Melliar-Smith, and P. Narasimhan, “A fault tolerance 
framework for CORBA,” in FTCS-29, pp. 150-157, 1999. 
D. Powell, P. Verissimo, G. Bonn, F. Waeselynck, and D. Seaton, 
“The  Delta-4  approach  to  dependability  in  open  distributed 
computing systems,” in FTCS-18, pp. 246-251, 1988. 
D.  Powell,  et  al.,  “GUARDS:  A  Generic  upgradable  architecture 
for  real-time  dependable  systems,” IEEE Trans. on Parallel and 
Distributed Systems, vol. 10, no. 6, pp. 580-599, 1999. 
S.  Shrivastava,  “Lessons  learned  from  building  and  using  the 
Arjuna  distributed  programming  system,”  Lecture  Notes  in 
Computer Science, vol. 938, Springer-Verilag, Berlin, 1995. 
G.  Stellner,  “CoCheck:  Checkpointing  and  process  migration  for 
the  10th  International  Parallel 
MPI,” 
Processing Symposium, pp. 526-531, 1996. 
D. Stott, B. Floering, Z. Kalbarczyk, and R. Iyer, “Dependability 
assessment in distributed systems with lightweight fault injectors 
in NFTAPE,” in IPDS-00, pp. 91-100, 2000. 
K.  Whisnant,  Z.  Kalbarczyk,  and  R.  Iyer,  “Micro-checkpointing: 
Checkpointing for multithreaded applications,” in Proceedings of 
the 6th International On-Line Testing Workshop, July 2000. 
K. Whisnant, R. Iyer, Z. Kalbarczyk, P. Jones, “An Experimental 
Evaluation of the ARMOR-based REE Software-Implemented Fault 
Tolerance  Environment,”  pending  technical  report,  University  of 
Illinois, Urbana, IL, 2001. 
in  Proceedings  of 
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:16:44 UTC from IEEE Xplore.  Restrictions apply.