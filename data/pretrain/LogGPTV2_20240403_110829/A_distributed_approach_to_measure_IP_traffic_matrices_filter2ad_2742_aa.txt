# A Distributed Approach to Measure IP Traffic Matrices

**Authors:**
- Konstantina Papagiannaki, Intel Research, Cambridge, UK
- Nina Taft, Intel Research, Berkeley, CA, USA
- Anukool Lakhina, Computer Science Department, Boston University, MA, USA

## Abstract

The traffic matrix (TM) of a telecommunications network is a critical input for network design and capacity planning. This paper addresses the debate on whether the costs of direct measurement of TMs are too prohibitive to be practical. We evaluate the feasibility of direct measurement by analyzing the computational, communication, and storage overheads at different levels of granularity. We show that current centralized solutions incur significant costs, and we outline the steps necessary to transition to fully distributed solutions, which would significantly reduce these overheads. However, a basic distributed solution with continuous flow monitoring is still excessive. By leveraging the stability property of traffic matrices, we propose a new distributed scheme that relies on limited flow measurement data. Our approach is simple, accurate, and scalable, and it further reduces overheads beyond the basic distributed solution. Our results suggest that direct measurement of TMs will become feasible in the near future.

## 1. Introduction

The traffic matrix (TM) of a telecommunications network measures the total amount of traffic entering the network from any ingress point and destined to any egress point. This information is essential for optimal network design, traffic engineering, and capacity planning. Despite its importance, directly measuring the TM for an IP network has been challenging due to several reasons:

1. **Data Collection:** The computation of the TM requires collecting flow statistics across the entire edge of the network, which may not be supported by all network elements.
2. **Centralized Processing:** These statistics must be sent to a central location for processing, incurring communication and computational overheads.
3. **Granularity and Storage:** Given the granularity of flow statistics collected by today's routers, constructing the TM requires detailed routing state and configuration information, leading to significant storage overheads.

These challenges have led to the belief that direct measurement of backbone TMs is impractical. As a result, recent research has focused on estimation techniques using SNMP link counts [13, 14, 8, 11, 1, 6]. These methods, while useful, have limitations in reducing error rates and often require additional information or changes to the network, such as altering IGP link weights [11].

In this paper, we revisit the issue of direct measurement, considering recent advances in flow monitoring technology. We argue that if direct measurement can be made practical, it offers several advantages, including very low errors and the potential for distributed solutions. We contribute by:

1. **Articulating Overheads:** We clearly define the overheads for both centralized and distributed measurement solutions, providing a basis for understanding where improvements are possible and evaluating the feasibility of direct measurement.
2. **Steps to Distributed Solutions:** We identify the steps needed to transition from a centralized to a distributed solution, offering specific recommendations for designers of flow statistics monitors like NetFlow [2].
3. **Empirical Analysis:** Using NetFlow data from the European Sprint IP backbone, we compute TMs at three levels of granularity (link-to-link, router-to-router, and PoP-to-PoP). We find that node fanouts are highly predictable over time, allowing us to propose a new scheme that updates flow measurements infrequently, reducing overheads dramatically.
4. **Error Analysis:** We provide a detailed discussion of errors, showing that our scheme allows operators to control the trade-off between measurement frequency and estimation accuracy.

Our work implies that distributed direct measurement of TMs is feasible if our recommendations are implemented, and we believe this is achievable with ongoing advancements in router technology.

## 2. Traffic Matrix Data

In this section, we describe the three weeks of TM data collected from the European Sprint IP backbone using a centralized direct measurement approach. We also provide details on the network architecture and the three types of TMs studied.

### 2.1 The Backbone Network

Sprint's European backbone IP network consists of 13 Points of Presence (PoPs), each located in a major European city. Each PoP typically has 5 to 10 routers organized in a hierarchy. Customers connect to gateway (gw) routers, and backbone (bb) routers aggregate and forward traffic to the core of the network. To obtain a TM by direct measurement, we enabled NetFlow on all incoming peering links and links from gw to bb routers, capturing nearly all customer traffic.

## 3. State of the Art: Today and Tomorrow

There are three main steps to obtain a TM from measurements:

1. **Gather Information:** Collect flow statistics using NetFlow or a similar monitor.
2. **Identify Destinations:** Determine the destination for each flow.
3. **Construct the TM:** Aggregate the data to form the TM.

We discuss the current state of the art in TM computation and the advancements needed in flow monitors to move towards a distributed solution. We also identify the overheads involved in direct measurement and propose a new distributed scheme that leverages the predictability of fanouts to reduce these overheads.

## 4. Predictability of Fanouts

We illustrate the predictability of node fanouts across different TM aggregation levels, showing that they are remarkably stable over time.

## 5. Trigger-Based Algorithm

We describe our trigger-based algorithm for gathering only the necessary measurements, which significantly reduces overheads while maintaining accuracy.

## 6. Performance Metrics and Evaluation

We present our performance metrics, evaluate our scheme, and discuss the errors. Our approach allows operators to control the trade-off between measurement frequency and estimation accuracy.

## 7. Impact on Overheads

We explain how our approach reduces computational, communication, and storage overheads.

## 8. Conclusion

We conclude by summarizing our findings and their implications for the future of TM measurement.

---

This revised version aims to improve clarity, coherence, and professionalism, making the text more accessible and easier to understand.