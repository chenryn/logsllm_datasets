title:A distributed approach to measure IP traffic matrices
author:Konstantina Papagiannaki and
Nina Taft and
Anukool Lakhina
A Distributed Approach to Measure IP Trafﬁc Matrices∗
Konstantina Papagiannaki§, Nina Taft‡, Anukool Lakhina†
§Intel Research
Cambridge, UK
PI:EMAIL
‡Intel Research
Berkeley, CA, USA
PI:EMAIL
†Computer Science Department,
Boston University, MA, USA
PI:EMAIL
ABSTRACT
The traﬃc matrix of a telecommunications network is an
essential input for any kind of network design and capacity
planning decision. In this paper we address a debate sur-
rounding traﬃc matrix estimation, namely whether or not
the costs of direct measurement are too prohibitive to be
practical. We examine the feasibility of direct measurement
by outlining the computation, communication and storage
overheads, for traﬃc matrices deﬁned at diﬀerent granular-
ity levels. We illustrate that today’s technology, that neces-
sitates a centralized solution, does indeed incur prohibitive
costs. We explain what steps are necessary to move to-
wards fully distributed solutions, that would drastically re-
duce many overheads. However, we illustrate that the basic
distributed solution, in which ﬂow monitors are on all the
time, is excessive and unnecessary. By discovering and tak-
ing advantage of a key stability property underlying traﬃc
matrices, we are able to propose a new scheme that is dis-
tributed and relies only on a limited use of ﬂow measurement
data. Our approach is simple, accurate and scalable. Fur-
thermore, it signiﬁcantly reduces the overheads above and
beyond the basic distributed solution. Our results imply
that direct measurement of traﬃc matrices should become
feasible in the near future.
Categories and Subject Descriptors
C.2.3 [Computer-Communication Networks]: Network Op-
erations - Network Monitoring
General Terms
Algorithms, Management, Measurement, Design
Keywords
Traﬃc matrix, Internet measurement, Distributed algorithm
∗The majority of this work was performed when K. Papa-
giannaki was with the Sprint Advanced Technology labs in
Burlingame, CA.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’04, October 25–27, 2004, Taormina, Sicily, Italy.
Copyright 2004 ACM 1-58113-821-0/04/0010 ...$5.00.
1.
INTRODUCTION
The traﬃc matrix (TM) of a telecommunications network
measures the total amount of traﬃc entering the network
from any ingress point and destined to any egress point.
The knowledge captured in the traﬃc matrix constitutes an
essential input for optimal network design, traﬃc engineer-
ing and capacity planning. Despite its importance, however,
the traﬃc matrix for an IP network is a quantity that has
remained elusive to capture via direct measurement. The
reasons for this are multiple. First, the computation of the
traﬃc matrix requires the collection of ﬂow statistics across
the entire edge of the network, which may not be supported
by all the network elements. Second, these statistics need
to be shipped to a central location for appropriate process-
ing. The shipping costs coupled with the frequency with
which such data would be shipped translate to communica-
tions overhead, while the processing cost at the central lo-
cation translates to computational overhead. Lastly, given
the granularity at which ﬂow statistics are collected with
today’s technology on a router, the construction of the traf-
ﬁc matrix requires explicit information on the state of the
routing protocols as well as the conﬁguration of the network
elements [5]. The storage overhead at the central location
thus includes routing state and conﬁguration information.
It has been widely believed that these overheads would be
so signiﬁcant as to render computation of backbone traf-
ﬁc matrices, through measurement alone, not viable using
today’s ﬂow monitors.
This assumption has been one of the main motivations be-
hind recent research targeted toward estimation techniques
that can infer the traﬃc matrix from readily available SNMP
link counts [13, 14, 8, 11, 1, 6]. The SNMP link counts con-
stitute only partial information, and thus basic inference
methods are limited in how low they can drive the error
rates. Hence these previous eﬀorts have explored diﬀerent
avenues for extracting additional information from the net-
work. Research eﬀorts, such as [1, 14, 13, 8], usually postu-
late some underlying model for the TM elements and then
use an optimization procedure to produce estimates that are
consistent with the link counts. In [11] the authors propose
changing the IGP link weights in order to obtain more in-
formation to reduce the uncertainty in the estimates. While
this technique is powerful in collapsing errors, it requires
carriers to alter their routing in order to obtain a traﬃc ma-
trix. It is not clear that carriers are willing to do this. In
[6] they recognize that some of the optimization approaches
may not scale to networks with large numbers of nodes (such
as traﬃc matrices at the link-to-link granularity level) and
hence they propose a method for partitioning the optimiza-
tion problem into multiple subproblems. This improves the
scalability but at the expense of some accuracy. Most of
these studies have come from carriers whose interest lies in
backbone traﬃc matrices at larger time scales for the pur-
poses of improving network traﬃc engineering. We consider
a similar context in this paper.
There has been some debate as to whether or not infer-
ence techniques are really needed. Some researchers believe
that the communication, storage and processing overheads
of direct measurement are prohibitive thus rendering it im-
practical. Other researchers believe that the traﬃc matrix
problem is an implementation issue, and can be solved by
advances in ﬂow monitoring technology. In this paper, we
address this debate directly. While many of the inference
techniques perform quite well, monitoring capabilities on
the network elements have made noticeable progress, and
technologies for the collection of ﬂow statistics have been
made available on a wide variety of router platforms. Due
to such advances, we believe it is time to revisit the issue of
direct measurement of an IP traﬃc matrix.
If direct measurement can be made practical, then there
are many reasons why it would be attractive. Direct mea-
surement could lead to very small errors, and would remove
the need for modeling and optimization procedures. Per-
haps the most salient reason is that it has the potential
to enable distributed solutions to traﬃc matrix estimation;
all of today’s inference techniques necessitate a centralized
solution. We are aware of carriers that are currently eval-
uating the potential of enabling monitors such as Netﬂow
on a widescale basis. Before they incur such dramatic costs,
hidden overheads need to be exposed and understood. We
hope this paper will contribute to such understanding.
Our contributions in this paper are as follows. First,
we articulate what the overheads are for both centralized
and distributed measurement solutions. Although this is a
straightforward exercise, it is important to do because (1)
it has never been spelled out before; (2) having this un-
derstanding enables one to know where improvements are
possible; and (3) it allows us to evaluate the feasibility of
direct measurement. Second, we identify the steps that are
needed to move from a centralized solution (today’s state of
the art) to a distributed one (tomorrow’s state of the art).
This takes the form of speciﬁc recommendations to design-
ers of ﬂow statistics monitors, such as Netﬂow [2]. Recent
advances in Netﬂow indicate that these monitors are mov-
ing in the right direction, however some additional steps are
still needed. Third, we use Netﬂow data collected from the
entire edge of the European Sprint IP backbone to compute
traﬃc matrices at three levels of granularity, namely that of
link-to-link, router-to-router, and PoP-to-PoP. The fanout
is a vector describing the fraction of total traﬃc sourced at
one node and destined to each of the other egress nodes.
We ﬁnd that node fanouts across all three traﬃc matrix
aggregation levels are remarkably predictable across time.
We propose a new scheme for the computation of the traﬃc
matrix that relies on this observation. The key idea is that
measurements are not needed frequently. We show that by
updating the ﬂow measurements once every few days, we
can maintain fairly accurate traﬃc matrices at the hourly
time scale. The improvement in computational overhead
at the collection station is dramatic while the reduction in
communications overhead ranges from 70-85% on average.
Finally, we also include a detailed discussion of errors.
When a traﬃc matrix estimation method is validated against
real data, it generates errors in both time and space. Some
previous studies select one metric to summarize errors or
decide to focus on the errors experienced by ﬂows at a ran-
domly selected time instant. In this work, we discuss some
diﬀerent views of the errors. An attractive feature of our
scheme is that it allows the operator to tune a knob to con-
trol the error rate. The errors can be pushed very low by
increasing the number of measurements taken.
Inference
techniques do not enable the operator to control this trade-
oﬀ between frequency of measurement and estimation accu-
racy. We will show that the number of measurements taken
can be drastically reduced with reasonable sacriﬁce in terms
of errors.
The implication of our work is that distributed direct mea-
surement of traﬃc matrices is feasible as long as the recom-
mendations we give are implemented. We believe that this is
achievable as router manufacturers are poised to move along
the needed path.
The paper is structured as follows. In Section 2 we present
our data and deﬁne traﬃc matrices at three diﬀerent gran-
ularities. In Section 3 we describe the state of the art in the
computation of the traﬃc matrix. We also describe the ad-
vancements needed in today’s ﬂow monitors to move toward
a distributed solution, and identify the overheads involved in
direct measurement. Section 4 illustrates the predictability
of fanouts and Section 5 describes our trigger-based algo-
rithm for gathering only the needed measurements. Sec-
tion 6 contains our performance metrics, the evaluation of
our scheme, and a discussion of errors. The impact of our
approach on the overheads is explained in Section 7. We
conclude in Section 8.
2. TRAFFIC MATRIX DATA
In this work we analyze three weeks of traﬃc matrix data
obtained using today’s available technology, that of a cen-
tralized direct measurement approach.
In this section we
describe the collected data, the architecture of Sprint’s Eu-
ropean backbone network, as well as the three types of traﬃc
matrices we study in this work.
2.1 The backbone network
Sprint is a Tier-1 provider whose European backbone IP
network comprises 13 Points of Presence (PoPs), one for
each major European city. Typically the number of routers
in each PoP ranges from 5 to 10. The routers are organized
in a hierarchy as depicted in Fig. 1. Customers connect
to the network by being directly attached to gateway (gw)
routers. Backbone (bb) routers aggregate the traﬃc of mul-
tiple gateway routers and forward it to the core of the net-
work. The backbone routers are used for connecting peers
to the backbone and also to inter-connect the PoPs.
In order to obtain a traﬃc matrix by direct measurement,
we need to examine all the incoming packets to the back-
bone. We therefore enabled Netﬂow on all incoming peering
links and all the links going from gateway routers to back-
bone routers. This latter set of links captures nearly all
customer traﬃc. It only misses traﬃc that enters and leaves
the network at the same gateway router1.
1This implies that the only elements that may be impacted
from this conﬁguration choice are those that feature the
small number of hours. Notice that according to the above
problem deﬁnition our scheme is designed not to address
problem areas such as anomaly detection, that may require
measurements at smaller time scales.
To the best of our knowledge, this is the ﬁrst work that
analyzes an IP traﬃc matrix computed from ﬂow statistics
collected across the entire edge of the network for a multi-
week period of time. It is also the ﬁrst study that examines
the performance of a TM estimation scheme across three
levels of granularity in a single work.
3. STATE OF THE ART: TODAY AND
TOMORROW
There are basically three steps to obtain a traﬃc matrix
from measurements. The ﬁrst is to gather information about
the traﬃc source by collecting measurements using Netﬂow,
or a similar monitor. Packets are observed and statistics
are stored at the granularity of ﬂows. The second step is
to identify the destination for each ﬂow. The third step is