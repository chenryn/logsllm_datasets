face happened to be present outside this bounding
box, that application would not see the resulting
event. Such regions are useful to (1) prevent an
application from seeing sensitive information in the
environment, and (2) improve eﬃciency and accu-
racy of recognizers (e.g., by skipping a region that
generates false positives). This bounding box works
for sensors where the data is spatial, such as RGB,
depth, or skeleton feeds. Other cutoﬀs would work
for other sensors, such as ﬁltering audio to a certain
frequency range to ensure voice data is not leaked
while other sounds are kept.
Recognizers can also subscribe to events from
other recognizers, just like applications. The OS in-
cludes recognizers for raw sensor streams, such as
RGB frames from a camera. Because subscribing
to events is an explicit call to the OS, the OS can
construct a dataﬂow graph showing how raw sensor
streams are progressively reﬁned into objects. Fig-
ure 7 shows an example. Having explicit data ﬂow
Figure 8: Recognizer-based OS architecture. Applica-
tions request subscriptions to sets of recognizers, which
the OS then conﬁrms with the user using privacy gog-
gles (Figure 9). Once the user grants permission, the OS
delivers recognizer events to subscribed applications.
helps the OS with both security and performance,
as we describe below.
Architecture and Threat Model: Figure 8 shows
the core architecture of an OS with multiple appli-
cations and multiple recognizers. “Root” recognizers
acquire raw input from sensors such as the Kinect,
then raise events that are consumed by other rec-
ognizers. An application may request a subscription
for a set of recognizers. The OS conﬁrms this request
with the user using our “Privacy Goggles” visualiza-
tion (Section 3.3). If the user agrees to the request,
the OS then delivers events from appropriate recog-
nizers to the application. While our implementation
and example focuses on the Kinect, our architecture
applies to all forms of object recognition across dif-
ferent platforms such as mobile phones.
The applications are not
trusted, while the
OS, recognizer implementations, and hardware are
trusted. This is similar to the threat model in to-
day’s mobile devices. Third-party recognizer imple-
mentations are out of scope of this paper, but we
describe in Section 7 key new challenges they raise.
3.1 Security Beneﬁts
The recognizer abstraction has two key security ben-
eﬁts:
Least privilege: Applications can be given access
only to the recognizers they need, instead of to raw
sensor streams. Before recognizers, OSes could ex-
pose permissions only at a coarse granularity. As
we will see in Section 5, a small set of recognizers is
suﬃcient to cover most shipping AR applications.
If an applica-
Reducing permission requests:
tion requests access to the skeleton and hand rec-
ognizers from the DAG shown in Figure 7, a user
only needs to grant access to the skeleton recognizer.
USENIX Association  
22nd USENIX Security Symposium  419
5
More generally, the recognizer DAG allows us to ﬁnd
such dependencies eﬃciently. This helps with warn-
ing fatigue, which is one of the major problems with
existing permission systems [11].
3.2 Performance Beneﬁts
Besides the security beneﬁts described above, recog-
nizer DAGs also allow us to achieve signiﬁcant per-
formance gains.
Sharing recognizer output: Most computer vi-
sion algorithms used in recognizers are computation-
ally intensive. Since concurrently running AR appli-
cations may access the same recognizers, our recog-
nizer DAG allows us to run such shared recognizers
only once and send the output to all subscribed ap-
plications. Our experiments show that this results
in signiﬁcant performance gains for concurrent ap-
plications.
On-demand invocation:
The recognizer DAG
allows us to ﬁnd all recognizers being accessed by
currently active applications at all times. We can
then prevent scheduling inactive recognizers.
Concurrent execution:
The recognizer DAG
also allows us to ﬁnd true data dependencies between
the recognizers. We leverage this to schedule inde-
pendent recognizers in multiple threads/cores and
thus minimize inter-thread/core communication.
Oﬄoading:
Some recognizers require special-
purpose hardware such as a powerful GPU that may
not be available in mobile devices. These recognizers
must be outsourced to a remote server. For exam-
ple, the real-time 3D model generation of KinectFu-
sion [23] requires a high-end nVidia desktop graphics
card, such as a GeForce GTX 680. Therefore, if we
want to use a commodity tablet with a Kinect at-
tached to scan objects and create models, we must
run the recognizer on a remote machine. While
applications could implement oﬄoading themselves,
adding oﬄoading support to the OS preserves least
privilege. For example, the OS can oﬄoad KinectFu-
sion without giving applications access to raw RGB
and depth inputs, which would be required if an ap-
plication were to oﬄoad it manually.
3.3 Privacy Goggles
We introduce privacy goggles, an “application-eye
view” of the world for running applications. For
example, if the application has access to a skele-
ton recognizer, a stick ﬁgure in the “privacy goggles
view” mirrors the movements of any person in view
of the system, as shown in Figure 9. A trusted visu-
alization method for each recognizer communicates
the capabilities of applications that have access to
this recognizer. If an application requests access to
more than one recognizer, the OS will compose the
appropriate visualizations.
In Section 5 we survey
462 people to demonstrate that privacy goggles do
eﬀectively communicate capabilities for “core recog-
nizers” derived from analyzing shipping AR applica-
tions. Privacy goggles are complementary to exist-
ing permission widgets, such as those of Howell and
Schechter [16], which allow users to understand how
apps perceive them in real time.
Permission Granting and Revocation. Privacy
goggles lay a foundation for permission granting, in-
spection, and revocation experiences. For example,
we can generalize existing install-time manifests to
use privacy goggles visualizations. At installation
time, a short prepared video could play showing a
“raw” data stream side by side with the privacy gog-
gles view. The user can then decide to allow access
to all, some, or none of the recognizers. We are cur-
rently evaluating this approach. Because manifest-
based systems have known problems with user at-
tention [11], we are also exploring how access-control
gadgets might interact with privacy goggles [27].
A major diﬀerence between privacy goggles and
existing permission granting systems like Android
manifests is the visual representation of the sen-
sitive data. The visual representation helps users
to make informed decisions about granting and re-
voking an application’s access to diﬀerent recogniz-
ers. Traditional systems do not need this represen-
tation because they ask for permissions about well-
understood low-level hardware, such as the camera
and microphone. Because we are ﬁne-grained and
must consider higher-level semantics, we need pri-
vacy goggles to show the impact of allowing applica-
tions access to speciﬁc recognizers.
After installation, privacy goggles are a natural
way to inspect sensitive data exposed to applica-
tions. The user can trigger a “privacy goggles control
panel” to zero in on a particular application or view
a composite for all applications at once. From the
control panel, a user can then turn oﬀ an applica-
tion’s access to a recognizer or even uninstall the
application.
3.4 Handling Recognizer Errors
Because our permission system depends on recog-
nizer outputs, we have a new challenge: recognizer
errors. Object recognition algorithms inside recog-
nizers have both false positives and false negatives.
A false negative means that applications will not
“see” an object in the world, impacting functionality.
420  22nd USENIX Security Symposium 
USENIX Association
6
False negatives, however, do not concern privacy.
A false positive, on the other hand, means that
an application will see more information than was
intended. In some cases the damage will be limited,
because the recognizer will return information that
is not sensitive. For example, a false positive from a
recognizer for hand positions is unlikely to be a prob-
lem. In others, false positives could leak portions of
raw RGB frames or other more sensitive data.
To address recognizer errors, we introduce a new
OS component for recognizer error correction. While
recognizers themselves implement various techniques
to decrease errors, in our setting false positives are
damaging, while false negatives are less important.
Therefore, we are willing to tolerate more false neg-
atives and fewer false positives than a recognizer de-
veloper who is not concerned with basing permission
decisions on a recognizer’s output.
For recognizer error correction, we ﬁrst consid-
ered two techniques: blurring and frame subtraction,
both of which are well-known graphics techniques
that can be applied in a recognizer-independent way.
We apply these techniques to recognizer inputs to
reduce potential false positives, accepting that they
may raise false negatives. We discuss the results and
show data in Section 5.
In addition, the OS has information not available
to an individual recognizer developer: results from
other recognizers in the same system on the same
environment. Recognizer error correction can there-
fore employ recognizer combination to reduce false
positives. For example, if a depth camera is avail-
able, the OS can use the depth camera to modify the
input to a face detection recognizer. By blanking
out all pixels past a certain depth, the OS can en-
sure a face recognizer focuses only on possible faces
near the system. While combination does require
knowing something about what a recognizer does,
it is independent of the internals of the recognizer
implementation. For another example, the OS can
combine a skeleton recognizer and a face recognizer
to release a face image only if there is also a skeleton
with its head in the appropriate place.
3.5 Adding New Recognizers.
Today’s AR platforms ship with a small ﬁxed set of
recognizers. Applications that want capabilities out-
side that set need to both innovate on object recog-
nition and on app experience, which is rare. As the
platforms mature, we expect additional recognizers
to appear. The main incremental costs for new rec-
ognizers are 1) coming up with a privacy goggles
visualization, 2) measuring the eﬀectiveness of this
Figure 9: Example of “privacy goggles.” The user sees
the “application-eye view” for a skeleton recognizer.
API
Purpose
init
destruct
event_generate
visualize
filter
cache_compare
Register
Clean up
Notify apps of recognized objects
Render recognized objects
Restrict domain for recognition
Compare to previous inputs
Figure 10: The APIs implemented by each recog-
nizer. The ﬁrst four are required, while filter and
cache_compare are optional.
visualization at informing users (and re-designing if
not eﬀective), and 3) deﬁning relationships with ex-
isting recognizers to support recognizer error correc-
tion. For example, a new “eye recognizer” would
have the invariant that every eye detected should
be on a head detected by the skeleton recognizer.
Third-party recognizers raise additional security is-
sues outside the scope of this paper; we discuss them
brieﬂy in Section 7.
4 Implementation
We have built a prototype implementation of our ar-
chitecture. Our prototype consists of a multiplexer,
which plays the role of an OS “kernel”, and ARLib, a
library used by AR applications to communicate to
the multiplexer. Our system uses the Kinect RGB
and depth cameras for its sensor inputs.
Multiplexer. The multiplexer handles access to
the sensors and also contains implementations of
all recognizers in the system. Our applications no
longer have direct access to Kinect sensor data and
must instead interact with the multiplexer and re-
trieve this data from recognizers. The multiplexer
supports simultaneous connections from multiple ap-
plications. To simplify implementation, we built
the multiplexer as a user-space program in Windows
that links against the Kinect for Windows SDK.
The multiplexer registers each recognizer using
a static, well-known name. Applications use these
names to request access to one or more recogniz-
USENIX Association  
22nd USENIX Security Symposium  421
7
var client = new MultiplexerClient();
client.Connect();
client.OnFace += new FaceEventCallback(ProcessFace);
...
public void ProcessFace(FTPoint[] points)
{
if (points.Length > 0) {
DrawFace(points);
} else {
RemoveFace();
}
}
Figure 12: Code used by a sample C# application to
connect to the multiplexer, subscribe to events from the
face recognizer, and use those events to update its face
visualization.
ers. When the multiplexer receives such an access
request,
it asks the user whether or not permis-
sion should be granted using privacy goggles (Sec-
tion 3.3).
If the user grants permission, the mul-
tiplexer will forward future recognizer events, such
as face mesh points from a face recognizer, to the
application.
The multiplexer interacts with recognizers via an
API shown in Figure 10. All recognizers must im-
plement the ﬁrst four API calls. The multiplexer
calls init to initialize a recognizer and destruct
to let a recognizer release its resources.
In our
current implementation, the multiplexer calls the
event_generate function of each recognizer in a
loop, providing prerequisite recognizer inputs as pa-