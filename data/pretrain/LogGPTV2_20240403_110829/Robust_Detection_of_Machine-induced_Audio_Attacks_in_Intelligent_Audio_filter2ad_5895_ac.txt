13.82
36.058
15.15
22.79
79.60
35.76
94.19
60.86
46.22
31.35
16.00
15.45
34.65
23.81
57.86
47.56
78.76
16.13
21.33
31.21
Channel Pair
Genuine
Replayed
Magnitude
(dB)
Phase
(radians)
Magnitude
(dB)
Phase
(radians)
Table 2: Channel-wise L1 distance of magnitude and phase spectrum between genuine and replayed audio in different envi-
ronments.
Channel#
Magnitude (dB)
Phase (radian)
1
3.60
1587.52
2
3.48
Outdoor
3
3.79
1746.88
1707.57
4
3.45
1854.19
1
2.59
2933.04
Indoor #1
3
2
2.84
2.95
2925.86
2940.81
4
2.85
2819.91
1
2.00
878.47
Indoor #2
3
2
2.44
2.38
824.35
837.70
4
2.48
829.94
1
1.06
1458.11
In-vehicle
3
2
1.02
1.10
1427.10
1519.01
4
0.98
1447.60
3.3 Distinct Information Carried on Multiple
Channels
Conventional approaches for multi-channel speech recognition of-
ten rely on beamforming techniques [10] to combine the received
multi-channel signal into an enhanced single-channel signal to
separate or extract speech signals from noisy environments. In par-
ticular, a beamformer acts like a spatial filter to enhance the signal
from a specific direction of interest (i.e., the speech signal) and
reduce the contamination caused by signals from other directions
(e.g., ambient noises). However, different from speech separation
or speech recognition that focuses only on the speech signal, the
multi-channel audio signal picked up by the microphone array
could contain distinct information (e.g., different surrounding noise
patterns) that are beneficial to the machine-induced audio detec-
tion process. To validate the feasibility of leveraging multi-channel
audio to enhance the performance of audio attack detection, we
further analyze the ReMASC dataset by plotting the magnitude
and phase spectrum of two individual channels, as shown in Fig-
ure 5. We observe that each individual channel possesses unique
information (especially visible in the phase domain) that could be
helpful for the detection of machine-induced audio. In addition,
we perform statistical analysis on the audio samples to quantify
the magnitude/phase difference between each pair of channels by
calculating their average L1 distance. As shown in Table 1, there
exists a difference in both magnitude and phase between any two
channels of the recorded audio, showing that each channel indeed
carries distinct information. Moreover, we observe that the pair of
channels with the most distinct magnitude information does not
necessarily carry the most distinct phase information. These find-
ings encourage us to design a deep learning model that leverages
both the magnitude and phase information of multi-channel audio
and extracts features from each channel independently to achieve
robust and high-performance audio attack detection.
3.4 Dominant Channel in Each Environment
To further investigate the impact of different acoustic environments,
we compute the channel-wise L1 distance of magnitude and phase
spectrum between genuine and replayed audio for each environ-
ment in Table 2, where the dominant channel that carries the most
discriminative information for detecting replayed audio is marked
in bold. We observe that the dominant magnitude/phase channel
varies in different acoustic environments, which is caused by the
varying recording condition (e.g., environmental noise) and behav-
ior of the recording and playback device (e.g., the relative location
of the sound source to the microphone array). The results demon-
strate the characteristics of the genuine and replayed audio can
be heavily affected by the type of recording environment. As a
result, the patterns learned from existing environments might not
generalize to new environments, which motivates us to explore a
way to remove environment-specific features from the model.
4 SYSTEM DESIGN
4.1 Design Objectives and Challenges
We aim to build a holistic solution to detect all the audio attacks
induced by machines. Specifically, the solution needs to meet the
following design objectives: 1) the model should be able to utilize
the rich information encoded in multi-channel audio to achieve
enhanced audio attack detection accuracy compared to existing
single-channel based methods; 2) in order to build a holistic defense
against any machine-induced audio attack, the model should be able
to capture a set of generic acoustic features that distinguish gen-
uine speech from machine-induced audio; 3) the model should rely
on environment-independent features only to maintain a decent
detection performance in different acoustic environments.
Challenges. To design such a holistic and robust system, we
have to address the following challenges: 1) The voice interface
embedded in intelligent audio devices requires a swift system re-
sponse for usability considerations. To achieve timely detection,
the audio attack detection system should be able to make a decision
relying on only a short fraction of audio (e.g., â‰¤ 1 second); 2) The
attack audio may be induced by disparate types of loudspeakers
(e.g., standalone loudspeaker, built-in speaker on smartphone, and
ultrasound speaker) that have varying frequency responses. There-
fore, the model needs to be able to capture general features that
are pervasive across all playback devices; 3) Explicitly collecting la-
beled data for all common acoustic environments is rather difficult
in practice and thus it will be more desirable to enable the model to
generalize to new acoustic environments without requiring labeled
data for achieving robust defense.
4.2 Prepossessing
In real-world application scenarios, the detection model should be
able to make a decision relying on a short segment of the streaming
audio recorded by the microphone array. The length of the segment
ğ‘™ should be set as short as possible to achieve timely detection for
Session 6C: Audio Systems and Autonomous Driving CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea1889intuition is that compared with large convolution filters, stacked
small convolution filters can achieve the same effective receptive
field as larger layers but with a fewer number of parameters (e.g.,
two stacked 3 Ã— 3 layers have an effective field of 5 Ã— 5, while
three stacked 3 Ã— 3 layers have an effective field of 7 Ã— 7), which
makes the model smaller and easier to be optimized. Additionally,
decomposing one convolution layer with a large filter into multiple
layers unlocks additional layers of non-linearity by injecting more
non-linear activation functions (e.g., Rectified Linear Unit (ReLU)),
which helps the network to capture complex patterns in the data.
Design of Type II Network. For our Type II network design, we
adopt the architectures proposed in MobileNet [28, 44] to compress
the model size and achieve efficient detection while maintaining
relatively high detection accuracy. The key innovation of MobileNet
compared with traditional deep networks (e.g., GoogLeNet [51],
DenseNet [29] and ResNet [26]) is the usage of depthwise separa-
ble convolution and the bottleneck residual block, which aims to
replace the expensive standard convolution layers with depthwise
separable convolutions which require a much fewer number of
parameters. As shown in Figure 6(a) and 6(b), the standard convo-
lution operation is substituted with a combination of two different
convolution operations, i.e., a depthwise convolution and a point-
wise operation. Different from standard convolution that combines
all input channels, depthwise convolution performs convolution
on each channel separately. The output channels of the depthwise
convolution operation are then combined using a pointwise con-
volution with 1 Ã— 1 kernels. For a convolution operation with ğ‘€
input channel, ğ‘ output channel, and ğ·ğ‘˜ Ã— ğ·ğ‘˜ kernel, this transfor-
mation significantly reduces the computational cost by a factor of
ğ‘ + 1
1
, which is especially helpful for processing multi-channel
audio signals that have a large number of input channels (e.g., a
6 channel audio signal will produce a 12 channel input feature
map). Leveraging this depthwise convolution, we can further con-
struct inverted residual blocks (Figure 6(c)) by adding expansion
layers that expand the compressed low-dimensional representa-
tion to high-dimensional space and projection layers that project
the filtered representation back to low-dimensional subspace. The
expansion ratio ğ‘¡ is used to control how much the representation
is expanded. In addition, a residual shortcut connect is added be-
tween the blocks to help accelerate the optimization process. A
width multiplier hyperparameter is used to further scale the model
by increasing/reducing the number of channels for all layers by a
factor of ğ›¼.
Network Structure. The overall structure of the proposed au-
dio attack detection network is presented in Figure 7. Specifically,
the network is composed of 3 components: a CNN feature extractor,
a fully-connected (FC) genuine/attack audio classifier, and an op-
tional domain discriminator which is only involved in environment-
independent training and will be detailed in Section 5. The Type I
network is built upon the VGG-16 network [50], with the number
of input channels modified according to the multi-channel audio
and the number of output neurons set to 2. We build the Type II
network based on the MobileNetV2 [44], with similar modifications
made to the network structure to accommodate the multi-channel
audio attack detection task.
ğ·2
ğ‘˜
(a) Standard convolution
(b) Depthwise separable convolution
(c) Inverted residual block
Figure 6: Illustration of the standard and depthwise separa-
ble convolution and the inverted residual block.
real-time applications while maintaining a high recognition accu-
racy. In our implementation, we choose to set ğ‘™ to be 1 second. This
gives us a multi-channel audio signal segment of size ğ‘™ Ã— ğ‘, where
ğ‘ is the number of channels for the microphone array. Instead of
directly operating on the raw waveforms, we utilize the audio signal
to create a unified time-frequency map from the power magnitude
obtained via Short-time Fourier transform (STFT), which shows
how the intensity of each frequency component changes over time.
Specifically, we keep the entire audio signal segment without apply-
ing voice activity detection and use a sliding window to process the
signal into overlapping frames with a frame length of 10 ms and a
step of 5 ms. We then apply the Hann window function [40] to each
frame and calculate its spectrum using STFT, with the length for
fast Fourier transform being set to 512. For audio signals sampled at
the rate of 44.1ğ‘˜Hz, the resulting dimension of the time-frequency
maps for each audio channel is 199 (time domain) by 257 (frequency
domain). Similarly, we process the phase information into time-
frequency maps of the same size by computing the angle of the
complex STFT values in radians and stack the phase maps along
with the magnitude maps. The final feature map shape used as the
input for our network is 199 Ã— 257 Ã— 2ğ‘ for a ğ‘-channel audio signal.
4.3 Multi-channel Replay Attack Detection
Network
After preprocessing, the audio signals will be processed into image-
like feature maps, which enables us to leverage the rich body of
research on convolutional neural networks (CNNs) in the computer
vision domain to guide the design of our audio attack detection
network. Specifically, we propose to explore two flavors of network
configuration according to different usage scenarios: (1) Type I: a
large and powerful network that has more representational capac-
ity to enable high attack detection accuracy for desktop or cloud
application; and (2) Type II: a fast and lightweight network that
provides more computational and energy savings, which makes it
suitable for mobile and IoT deployments.
Design of Type I Network. Inspired by the previous study [50]
on CNN architecture for processing image data, we configure our
Type I network using modules composed of stacked convolution
layers with small-sized filters (e.g., 3 Ã— 3) and pooling layers. The
StandardConvolutionDepthwiseConvolutionPointwiseConvolutionExpansionConvolutionDepthwiseConvolutionProjectionConvolutionResidual Connectionğ·!ğ·!ğ‘ğ·!ğ·!ğ‘ğ·!ğ·!ğ‘¡â‹…ğ‘ğ·!ğ·!ğ‘¡â‹…ğ‘Session 6C: Audio Systems and Autonomous Driving CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea1890Figure 7: Illustration of the proposed audio attack detection network.
4.4 Optimization
The attack detection is modeled as a binary classification problem
(i.e., genuine speech or machine-induced audio) and the networks
are trained in an end-to-end manner from raw waveforms of multi-
channel audio to the prediction label, with the preprocessing units
(magnitude and phase spectrogram extraction) being implemented
as part of the network. We use cross entropy as the classification
loss to train the network. Due to the difficulty in gathering large sets
of human voice samples, public audio attack datasets often suffer
the class imbalance problem where the data distribution is biased
towards the attack audio class (e.g., the ratio of genuine audio to
replayed audio in the ReMASC dataset [22] is approximately 1 : 5).
This poses a challenge for the deep learning model training as the
minority class (i.e., the genuine speech) is more important and
thus more sensitive to classification errors. To address the class
imbalance problem, during training we re-weight the cross-entropy
for each class according to the number of samples available in the
training set. The ADAM optimizer [31] with ğ›½1 = 0.9 and ğ›½2 = 0.999
is used to train the network for a total number of 100 epochs. As
for learning rate schedule, the step learning rate decay with warm-
up is used, where the learning rate is initially set to a small value
and increased by 10Ã— in the first 20 epochs and then reduced by
a half every 20 epochs. Batch normalization layers and l2 weight
regularization are also applied to stabilize the training process and
prevent over-fitting.
5 DOMAIN-INVARIANT REPRESENTATION