only hold limited tokens. If the bucket is already full, then the new
tokens being added will be discarded. Each token allows 1 byte of
data to go through the network. As a result, a queued packet of size
L can move to the NIC only if there are L tokens in the bucket. Note
that we can also configure the network buffer size by changing the
qdisc length.
Figure 3: Using TC-tbf to limit network bandwidth.
4 EVALUATION
This section discusses our experimental results on evaluating BBR’s
performance in terms of goodput, packet loss, and fairness using
our above-described testbeds.
132
Figure 4: Decision tree for employing BBR versus Cubic un-
der different network conditions.
4.1 BBR versus Cubic
Our first evaluation aims to answer a practical question with respect
to BBR – “given a certain network condition, should we employ
BBR to maximize goodput?" To answer this question, we empirically
study the goodput of BBR and Cubic under 640 different configu-
rations in our LAN testbed. We generate traffic using iPerf3 [12]
in our LAN topology (Figure 2(a)) from h1 to h3. For each network
configuration, we run the iPerf3 experiment 5 times, each for 60
seconds. On the LAN router we configure:
8 RTT values: 5, 10, 25, 50, 75, 100, 150, and 200ms;
8 BW values: 10, 20, 50, 100, 250, 500, 750, and 1000Mbps;
5 Buffer sizes: 0.1, 1, 10, 20, and 50 MB.
The range of our chosen parameters is based on values commonly
employed in modern networks [30, 31].
4.1.1 Decision Tree. We summarize our LAN results using a deci-
sion tree, in Figure 4, which shows whether BBR or Cubic achieves
higher goodput under different network conditions. The decision
tree is generated by using the DecisionTreeClassifier API provided
in Python3 scikit-learn package [13]. The input data consists of the
goodput values of BBR and Cubic from all 640 LAN experiments.
The median and mean classification accuracy for the decision tree
is 81.9% and 81.3%, respectively, under 5-fold cross validation. Note
that we set the TCP read and write memory to the maximum al-
lowable value in Ubuntu 18.10, (231-1 bytes); this is done so that
the data transfer is not limited by small memory sizes. Under the
OS’s default TCP memory sizes, the median and mean classification
accuracy for the decision tree increase to 90.2% and 90.0%. This is
because the Linux default TCP read and write memory sizes are
usually quite small (usually 4MB to 6MB), which results in lower
variation in bandwidth, thus reducing prediction outliers.
For each node in the tree in Figure 4, except for the leaf nodes, the
first row shows the condition; nodes to the left are True and nodes
to the right are False with respect to the condition. The second row
shows how many cases (out of the 75% training data) fall under this
node. The third row shows the number of cases that are classified
as BBR or Cubic, based on the condition in the first row. Finally, the
last row indicates the general decision for this node. If the node’s
color is orange, the decision is BBR; if the node’s color is blue,
then the decision is Cubic. The intensity of the color is determined
using the Gini impurity, and indicates how confident we are in our
decision. The leaf nodes provide the final classification output. To
leverage the tree for a given network condition, we start at the root
IMC ’19, October 21–23, 2019, Amsterdam, Netherlands
Yi Cao, Arpit Jain, Kriti Sharma, Aruna Balasubramanian, and Anshul Gandhi
(a) GpGainbbr
cubic
, buffer=100KB
(b) GpGainbbr
cubic
, buffer=10MB
(c) BBR’s # retransmits, buffer=100KB
(d) Cubic’s # retransmits, buffer=100KB
Figure 5: Analysis of BBR and Cubic in terms of improvement in goodput and number of retransmissions under shallow and
deep buffers for the LAN setup.
and traverse until we reach a leaf node; the decision in the leaf node
is the predicted optimal choice (BBR versus Cubic).
or RTT is high, i.e. the BDP is high. On the other hand, for a deep
buffer (10MB), Figure 5(b) shows that Cubic has higher goodput
except for very large bandwidth and RTT values. However, Cu-
bic’s goodput gain in deep buffers is not as high as BBR’s gain in
shallow buffers. For example, under 100KB buffer size, 200ms RTT,
and 500Mbps bandwidth, Cubic’s average goodput is 179.6Mbps,
while BBR has a significantly higher average goodput of 386.0Mbps
(115% improvement). However, for a 10MB buffer, Cubic only sees
a maximum goodput improvement of 34%. We also tried a 50MB
buffer size, but the results were similar to that for 10MB buffer size.
Loss: Although BBR sees significantly higher goodput values in
shallow buffers, there is a caveat here – high number of losses.
Figures 5(c) and 5(d) show the number of packet retransmissions
for both BBR and Cubic in our LAN experiments under the 100KB
bottleneck buffer size and different bandwidth and RTT values;
note that retransmissions are initiated after a loss is detected [23].
We see that BBR often incurs 10× more retransmissions than Cubic.
This is largely because BBR sets cwnd_дain to 2 most of the time,
thus requiring a buffer size of at least BDP to queue its outstanding
requests in flight; when the buffer size is smaller than BDP, BBR will
continually have losses. On the other hand, although Cubic also hits
the buffer capacity for a shallow buffer, it responds by lowering its
cwnd, thus avoiding continued losses. In terms of losses, for 100KB
buffer size, the average loss percentage for BBR and Cubic is 10.1%
and 0.9%, respectively. For 10MB buffer size, the corresponding loss
percentage for BBR and Cubic is 0.8% and 1.3%, respectively.
When the bottleneck buffer size increases, we find that the num-
ber of retransmissions decreases significantly for both BBR and
Cubic. For example, in our 25ms RTT and 500Mbps bandwidth
LAN experiment, when we increase the bottleneck buffer size from
100KB to 10MB, BBR and Cubic’s retransmissions decrease from
235798 to 0 and 1649 to 471, respectively. To better understand
this non-trivial loss behavior, we further analyze the relationship
between goodput and losses in the next subsection.
Latency: To investigate TCP latency, we now consider finite flow
sizes. Specifically, we use iPerf3 to generate 10MB and 100MB flows
under the same 640 network configurations as for our previous set
of experiments in the LAN testbed. We use the following metric to
evaluate BBR’s latency improvement percentage over Cubic:
LatDecbbr
cubic
=
latency|Cubic − latency|BBR
latency|Cubic
× 100
(2)
133
The decision tree can be reasoned as follows. The left branch
indicates the region where BDP is small and buffer is large, under
which Cubic results in higher goodput. The right branch indicates
the region where BDP is large and buffer size is small, under which
BBR has higher goodput. The key to explaining these findings is
that BBR maintains 2 × BDP number of packets in flight, and so
the BDP number of packets are queued in the buffer (while the
remaining BDP are on the wire). Now, if the buffer size is larger
than BDP, which is the case in most of the left branch, then BBR is
unable to fully utilize the buffer, resulting in likely inferior goodput.
When the buffer size is small, both BBR and Cubic experience losses.
However, Cubic responds to losses by significantly shrinking its
cwnd, whereas BBR does not directly respond to the loss, resulting
in higher goodput.
We also generated a decision tree using Mininet experiments.
Results are qualitatively similar, with the Mininet-based decision
tree providing an accuracy of about 90%.
4.1.2 Deconstructing the decision tree results. To further analyze
the decision tree results based on the LAN experiments, we focus
on two metrics: goodput and packet loss. Goodput characterizes
how well the congestion control algorithm utilizes the network,
while packet loss indicates the extent of network resource wastage
incurred by the algorithm.
Goodput: We use the following metric to evaluate BBR’s goodput
percentage gain over Cubic:
дoodput|BBR − дoodput|Cubic
× 100
(1)
GpGainbbr
cubic
=
дoodput|Cubic
We use heatmaps to visualize GpGainbbr
cubic
for different network
settings — for each metric, we show one heatmap for shallow buffer
(100KB) and one for deep buffer (10MB). Note that we refer to 10MB
as “deep” buffer since it is larger than most of the BDP values in
our experiments. For example, a 500Mbps bandwidth and 100ms
RTT results in 6.25MB BDP value, which is smaller than 10MB.
In each heatmap, we show the metric value under different RTT
and bandwidth settings. Red shaded regions and positive values
indicate that BBR outperforms Cubic, whereas blue shaded regions
and negative values indicate that Cubic outperforms BBR.
Figure 5(a) shows GpGainbbr
cubic
under the shallow buffer (100KB).
We observe that BBR outperforms Cubic when either bandwidth
When to use and when not to use BBR: An empirical analysis and evaluation study
IMC ’19, October 21–23, 2019, Amsterdam, Netherlands
(a) Lat Decbbr
cubic
, buffer=100KB
(b) Lat Decbbr
cubic
, buffer=10MB
Figure 6: BBR’s latency decrease compared to Cubic under
shallow and deep buffers for a finite flow size (100MB).
(a) Goodput vs loss rate.
(b) Retransmits vs loss rate.
Figure 7: Mininet: 100Mbps BW, 25ms RTT, 10MB buffer.
Our latency results for 100MB flows are shown in Figure 6, which
agrees with with our goodput results in Figure 5. We observe in
Figure 6(a) that in the shallow buffer case, BBR typically has lower
latency. For a deep buffer, Figure 6(b) shows that Cubic has lower
latency when the BDP is small. We also experimented with 10MB
flows, and found that BBR has lower latency in almost all cases.
4.2 BBR’s goodput vs packet losses
In the current Linux implementation, BBR does not actively react
to packet losses. However, we observe in our experiments that BBR
exhibits an abrupt drop in goodput when the loss rate becomes
very high, about 20% in our case. This suggests a “cliff point" in
loss rates beyond which BBR inadvertently reacts to losses.
On further analysis, we find that the cliff point has a close re-
lationship with BBR’s pacinд_rate parameter that determines its
probing capabilities (see Section 2). If the packet loss probability is
p, then during bandwidth probing, BBR paces at pacinд_rate × BW .
However, due to losses, its effective pacing rate is pacinд_rate ×
BW × (1 − p). Thus, if this value is less than the bandwidth, BBR
will not probe for additional capacity, and will in fact infer a lower
capacity due to losses. We determine the cliff point by solving:
pacinд_дain × BW × (1 − p) = BW
(3)
Consequently, the cliff point is p = 1− 1/pacinд_дain. In current
implementations, the maximum pacinд_дain is 1.25, so the cliff
point should be at p = 0.2, or 20% loss rate.
Validation of cliff points: We validate our above analysis by vary-
ing the maximum pacinд_дain value in BBR’s source code, and con-
ducting experiments in both our WAN and Mininet testbeds. We
experiment with two different pacinд_дain values, in addition to the
default value of 1.25: pacinд_дain = 1.1, denoted as BBR_1.1, and
pacinд_дain = 1.5, denoted as BBR_1.5. Via Eq. (3), we expect the
cliff point of BBR_1.1 and BBR_1.5 to be at 9% and 33%, respectively.
Figure 7 shows how different TCP algorithms react to packet
losses in the Mininet testbed (100Mbps bandwidth and 25ms RTT).
We vary loss rate from 0 to 50% using TC-NetEm to emulate lossy
networks; TC-NetEm introduces random losses, which are common
in WiFi and RED routers [26]. For each loss rate, we run iPerf3
experiments for 60s with different congestion control algorithms.
Figure 7(a) shows how goodput is affected by loss rates. We see
that for loss-based algorithms, Reno and Cubic, goodput decreases
significantly even under moderate loss rates since they proactively
reduce cwnd when encountering losses. On the other hand, BBR
and BBR_1.1 exhibit a drop in goodput around 20% and 9% loss
rates respectively, validating our prior cliff point analysis. However,
(a) Mininet results.
(b) WAN results.
Figure 8: BBR and Cubic’s bandwidth share under 1Gbps BW, 20ms
RTT, and different buffer sizes.
BBR_1.5 exhibits its cliff point much before the predicted 33% loss
rate. This is because BBR considers a > 20% loss rate a signal of
policing [8], and so uses the long-term average bandwidth instead
of updating its maximum bandwidth estimate BtlBw.
Figure 7(b) shows the number of retransmissions under different
loss rates. We see that BBR reaches its peak retransmissions around
the loss rate cliff point. This is because, before this cliff point, the
BBR goodput is stable but the loss rate is increasing, resulting in
increasing number of retransmits. However, after the cliff point
is reached, BBR’s goodput decreases, resulting in fewer packets
sent and, subsequently, fewer retransmissions. We also conducted
experiments in WAN to verify this behavior. We obtained similar
results as Figure 7(b), thus confirming our cliff point analysis.
4.3 Analyzing BBR’s fairness
Given BBR’s aggressive behavior, evidenced by its high retransmis-
sions, we now investigate the fairness of BBR when it coexists with
other flows in our Mininet and WAN testbeds.
Mininet results: For our Mininet testbed setup for fairness (Fig-