title:Leveraging User Interactions for In-Depth Testing of Web Applications
author:Sean McAllister and
Engin Kirda and
Christopher Kruegel
Leveraging User Interactions for
In-Depth Testing of Web Applications
Sean McAllister1, Engin Kirda2, and Christopher Kruegel3
1 Secure Systems Lab, Technical University Vienna, Austria
PI:EMAIL
2 Institute Eurecom, France
PI:EMAIL
3 University of California, Santa Barbara
PI:EMAIL
Abstract. Over the last years, the complexity of web applications has
grown signiﬁcantly, challenging desktop programs in terms of function-
ality and design. Along with the rising popularity of web applications,
the number of exploitable bugs has also increased signiﬁcantly. Web ap-
plication ﬂaws, such as cross-site scripting or SQL injection bugs, now
account for more than two thirds of the reported security vulnerabilities.
Black-box testing techniques are a common approach to improve soft-
ware quality and detect bugs before deployment. There exist a number of
vulnerability scanners, or fuzzers, that expose web applications to a bar-
rage of malformed inputs in the hope to identify input validation errors.
Unfortunately, these scanners often fail to test a substantial fraction of a
web application’s logic, especially when this logic is invoked from pages
that can only be reached after ﬁlling out complex forms that aggressively
check the correctness of the provided values.
In this paper, we present an automated testing tool that can ﬁnd
reﬂected and stored cross-site scripting (XSS) vulnerabilities in web ap-
plications. The core of our system is a black-box vulnerability scanner.
This scanner is enhanced by techniques that allow one to generate more
comprehensive test cases and explore a larger fraction of the application.
Our experiments demonstrate that our approach is able to test more
thoroughly these programs and identify more bugs than a number of
open-source and commercial web vulnerability scanners.
1 Introduction
The ﬁrst web applications were collections of static ﬁles, linked to each other by
means of HTML references. Over time, dynamic features were added, and web
applications started to accept user input, changing the presentation and content
of the pages accordingly. This dynamic behavior was traditionally implemented
by CGI scripts. Nowadays, more often then not, complete web sites are created
dynamically. To this end, the site’s content is stored in a database. Requests are
processed by the web application to fetch the appropriate database entries and
present them to the user. Along with the complexity of the web sites, the use
R. Lippmann, E. Kirda, and A. Trachtenberg (Eds.): RAID 2008, LNCS 5230, pp. 191–210, 2008.
c(cid:2) Springer-Verlag Berlin Heidelberg 2008
192
S. McAllister, E. Kirda, and C. Kruegel
cases have also become more involved. While in the beginning user interaction
was typically limited to simple request-response pairs, web applications today
often require a multitude of intermediate steps to achieve the desired results.
When developing software, an increase in complexity typically leads to a grow-
ing number of bugs. Of course, web applications are no exception. Moreover,
web applications can be quickly deployed to be accessible to a large number of
users on the Internet, and the available development frameworks make it easy
to produce (partially correct) code that works only in most cases. As a result,
web application vulnerabilities have sharply increased. For example, in the last
two years, the three top positions in the annual Common Vulnerabilities and
Exposures (CVE) list published by Mitre [17] were taken by web application
vulnerabilities.
To identify and correct bugs and security vulnerabilities, developers have a
variety of testing tools at their disposal. These programs can be broadly cate-
gorized as based on black-box approaches or white-box approaches. White-box
testing tools, such as those presented in [2, 15, 27, 32], use static analysis to
examine the source code of an application. They aim at detecting code frag-
ments that are patterns of instances of known vulnerability classes. Since these
systems do not execute the application, they achieve a large code coverage, and,
in theory, can analyze all possible execution paths. A drawback of white-box
testing tools is that each tool typically supports only very few (or a single) pro-
gramming language. A second limitation is the often signiﬁcant number of false
positives. Since static code analysis faces undecidable problems, approximations
are necessary. Especially for large software applications, these approximations
can quickly lead to warnings about software bugs that do not exist.
Black-box testing tools [11] typically run the application and monitor its exe-
cution. By providing a variety of specially-crafted, malformed input values, the
goal is to ﬁnd cases in which the application misbehaves or crashes. A signiﬁcant
advantage of black-box testing is that there are no false positives. All problems
that are reported are due to real bugs. Also, since the testing tool provides only
input to the application, no knowledge about implementation-speciﬁc details
(e.g., the used programming language) is required. This allows one to use the
same tool for a large number of diﬀerent applications. The drawback of black-box
testing tools is their limited code coverage. The reason is that certain program
paths are exercised only when speciﬁc input is provided.
Black-box testing is a popular choice when analyzing web applications for
security errors. This is conﬁrmed by the large number of open-source and com-
mercial black-box tools that are available [1, 16, 19, 29]. These tools, also called
web vulnerability scanners or fuzzers, typically check for the presence of well-
known vulnerabilities, such as cross-site scripting (XSS) or SQL injection ﬂaws.
To check for security bugs, vulnerability scanners are equipped with a large
database of test values that are crafted to trigger XSS or SQL injection bugs.
These values are typically passed to an application by injecting them into the
application’s HTML form elements or into URL parameters.
Leveraging User Interactions for In-Depth Testing of Web Applications
193
Web vulnerability scanners, sharing the well-known limitation of black-box
tools, can only test those parts of a web site (and its underlying web application)
that they can reach. To explore the diﬀerent parts of a web site, these scanners
frequently rely on built-in web spiders (or crawlers) that follow links, starting
from a few web pages that act as seeds. Unfortunately, given the increasing
complexity of today’s applications, this is often insuﬃcient to reach “deeper”
into the web site. Web applications often implement a complex workﬂow that
requires a user to correctly ﬁll out a series of forms. When the scanner cannot
enter meaningful values into these forms, it will not reach certain parts of the
site. Therefore, these parts are not tested, limiting the eﬀectiveness of black-box
testing for web applications.
In this paper, we present techniques that improve the eﬀectiveness of web vul-
nerability scanners. To this end, our scanner leverages input from real users as a
starting point for its testing activity. More precisely, starting from recorded, ac-
tual user input, we generate test cases that can be replayed. By following a user’s
session, fuzzing at each step, we are able to increase the code coverage by explor-
ing pages that are not reachable for other tools. Moreover, our techniques allow
a scanner to interact with the web application in a more meaningful fashion.
This often leads to test runs where the web application creates a large number
of persistent objects (such as database entries). Creating objects is important
to check for bugs that manifest when malicious input is stored in a database,
such as in the case of stored cross-site scripting (XSS) vulnerabilities. Finally,
when the vulnerability scanner can exercise some control over the program un-
der test, it can extract important feedback from the application that helps in
further improving the scanner’s eﬀectiveness.
We have implemented our techniques in a vulnerability scanner that can ana-
lyze applications that are based on the Django web development framework [8].
Our experimental results demonstrate that our tool achieves larger coverage and
detects more vulnerabilities than existing open-source and commercial fuzzers.
2 Web Application Testing and Limitations
One way to quickly and eﬃciently identify ﬂaws in web applications is the use
of vulnerability scanners. These scanners test the application by providing mal-
formed inputs that are crafted so that they trigger certain classes of vulnerabili-
ties. Typically, the scanners cover popular vulnerability classes such as cross-site
scripting (XSS) or SQL injection bugs. These vulnerabilities are due to input
validation errors. That is, the web application receives an input value that is
used at a security-critical point in the program without (suﬃcient) prior vali-
dation. In case of an XSS vulnerability [10], malicious input can reach a point
where it is sent back to the web client. At the client side, the malicious input
is interpreted as JavaScript code that is executed in the context of the trusted
web application. This allows an attacker to steal sensitive information such as
cookies. In case of a SQL injection ﬂaw, malicious input can reach a database
194
S. McAllister, E. Kirda, and C. Kruegel
query and modify the intended semantics of this query. This allows an attacker
to obtain sensitive information from the database or to bypass authentication
checks.
By providing malicious, or malformed, input to the web application under
test, a vulnerability scanner can check for the presence of bugs. Typically, this is
done by analyzing the response that the web application returns. For example,
a scanner could send a string to the program that contains malicious JavaScript
code. Then, it checks the output of the application for the presence of this string.
When the malicious JavaScript is present in the output, the scanner has found
a case in which the application does not properly validate input before sending
it back to clients. This is reported as an XSS vulnerability.
To send input to web applications, scanners only have a few possible injection
points. According to [26], the possible points of attack are the URL, the cookie,
and the POST data contained in a request. These points are often derived from
form elements that are present on the web pages. That is, web vulnerability
scanners analyze web pages to ﬁnd injection points. Then, these injection points
are fuzzed by sending a large number of requests that contain malformed inputs.
Limitations. Automated scanners have a signiﬁcant disadvantage compared to
human testers in the way they can interact with the application. Typically, a
user has certain goals in mind when interacting with a site. On an e-commerce
site, for example, these goals could include buying an item or providing a rating
for the most-recently-purchased goods. The goals, and the necessary operations
to achieve these goals, are known to a human tester. Unfortunately, the scanner
does not have any knowledge about use cases; all it can attempt to do is to collect
information about the available injection points and attack them. More precisely,
the typical workﬂow of a vulnerability scanners consists of the following steps:
– First, a web spider crawls the site to ﬁnd valid injection points. Commonly,
these entry points are determined by collecting the links on a page, the action
attributes of forms, and the source attributes of other tags. Advanced spiders
can also parse JavaScript to search for URLs. Some even execute JavaScript
to trigger requests to the server.
– The second phase is the audit phase. During this step, the scanner fuzzes the
previously discovered entry points. It also analyzes the application’s output
to determine whether a vulnerability was triggered.
– Finally, many scanners will start another crawling step to ﬁnd stored XSS
vulnerabilities. In case of a stored XSS vulnerability, the malicious input is
not immediately returned to the client but stored in the database and later
included in another request. Therefore, it is not suﬃcient to only analyze the
application’s immediate response to a malformed input. Instead, the spider
makes a second pass to check for pages that contain input injected during
the second phase.
The common workﬂow outlined above yields good results for simple sites
that do not require a large amount of user interaction. Unfortunately, it often
fails when confronted with more complex sites. The reason is that vulnerability
Leveraging User Interactions for In-Depth Testing of Web Applications
195
scanners are equipped with simple rules to ﬁll out forms. These rules, however,
are not suited well to advance “deeper” into an application when the program
enforces constraints on the input values that it expects. To illustrate the problem,
we brieﬂy discuss an example of how a fuzzer might fail on a simple use case.
The example involves a blogging site that allows visitors to leave comments
to each entry. To leave a comment, the user has to ﬁll out a form that holds the
content of the desired comment. Once this form is submitted, the web application
responds with a page that shows a preview of the comment, allowing the user
to make changes before submitting the posting. When the user decides to make
changes and presses the corresponding button, the application returns to the
form where the text can be edited. When the user is satisﬁed with her comment,
she can post the text by selecting the appropriate button on the preview page.
The problem in this case is that the submit button (which actually posts the
message to the blog) is activated on the preview page only when the web ap-
plication recognizes the submitted data as a valid comment. This requires that
both the name of the author and the text ﬁeld of the comment are ﬁlled in.
Furthermore, it is required that a number of hidden ﬁelds on the page remain
unchanged. When the submit button is successfully pressed, a comment is cre-
ated in the application’s database, linked to the article, and subsequently shown
in the comments section of the blog entry.
For a vulnerability scanner, posting a comment to a blog entry is an entry
point that should be checked for the presence of vulnerabilities. Unfortunately,
all of the tools evaluated in our experiments (details in Section 5.2) failed to
post a comment. That is, even a relatively simple task, which requires a scanner
to ﬁll out two form elements on a page and to press two buttons in the correct
order, proved to be too diﬃcult for an automated scanner. Clearly, the situation
becomes worse when facing more complex use cases.
During our evaluation of existing vulnerability scanners, we found that, com-
monly, the failure to detect a vulnerability is not due to the limited capabilities
of the scanner to inject malformed input or to determine whether a response in-
dicates a vulnerability, but rather due to the inability to generate enough valid
requests to reach the vulnerable entry points. Of course, the exact reasons for
failing to reach entry points vary, depending on the application that is being
tested and the implementation of the scanner.
3 Increasing Test Coverage
To address the limitations of existing tools, we propose several techniques that
allow a vulnerability scanner to detect more entry points. These entry points can
then be tested, or fuzzed, using existing databases of malformed input values.
The ﬁrst technique, described in Section 3.1, introduces a way to leverage inputs
that are recorded by observing actual user interaction. This allows the scanner
to follow an actual use case, achieving more depth when testing. The second
technique, presented in Section 3.2, discusses a way to abstract from observed
user inputs, leveraging the steps of the use case to achieve more breadth. The
196
S. McAllister, E. Kirda, and C. Kruegel
third technique, described in Section 3.3, makes the second technique more ro-
bust in cases where the broad exploration interferes with the correct replay of a
use case.
3.1 Increasing Testing Depth
One way to improve the coverage, and thus, the eﬀectiveness of scanners, is to
leverage actual user input. That is, we ﬁrst collect a small set of inputs that were
provided by users that interacted with the application. These interactions corre-
spond to certain use cases, or workﬂows, in which a user carries out a sequence
of steps to reach a particular goal. Depending on the application, this could be a
scenario where the user purchases an item in an on-line store or a scenario where
the user composes and sends an email using a web-based mail program. Based on
the recorded test cases, the vulnerability scanner can replay the collected input
values to successfully proceed a number of steps into the application logic. The
reason is that the provided input has a higher probability to pass server-side
validation routines. Of course, there is, by no means, a guarantee that recorded
input satisﬁes the constrains imposed by an application at the time the values
are replayed. While replaying a previously recorded use case, the scanner can
fuzz the input values that are provided to the application.
Collecting input. There are diﬀerent locations where client-supplied input data
can be collected. One possibility is to deploy a proxy between a web client and
the web server, logging the requests that are sent to the web application. Another
way is to record the incoming requests at the server side, by means of web server
log ﬁles or application level logging. For simplicity, we record requests directly
at the server, logging the names and values of all input parameters.
It is possible to record the input that is produced during regular, functional
testing of applications. Typically, developers need to create test cases that are
intended to exercise the complete functionality of the application. When such
test cases are available, they can be immediately leveraged by the vulnerability
scanner. Another alternative is to deploy the collection component on a produc-
tion server and let real-world users of the web application generate test cases. In
any case, the goal is to collect a number of inputs that are likely correct from the
application’s point of view, and thus, allow the scanner to reach additional parts
of the application that might not be easily reachable by simply crawling the site
and ﬁlling out forms with essentially random values. This approach might raise
some concerns with regards to the nature of the captured data. The penetration
tester must be aware of the fact that user input is being captured and stored in
clear text. This is acceptable for most sites but not for some (because, for ex-
ample, the unencrypted storage of sensitive information such as passwords and
credit card numbers might be unacceptable). In these cases, it is advisable to
perform all input capturing and tests in a controlled testbed.
Replaying input. Each use case consists of a number of steps that are carried out
to reach a certain goal. For each step, we have recorded the requests (i.e., the
Leveraging User Interactions for In-Depth Testing of Web Applications
197
input values) that were submitted. Based on these input values, the vulnerability
scanner can replay a previously collected use case. To this end, the vulnerability
scanner replays a recorded use case, one step at a time. After each step, a fuzzer
component is invoked. This fuzzer uses the request issued in the previous step
to test the application. More precisely, it uses a database of malformed values
to replace the valid inputs within the request sent in the previous step. In other
words, after sending a request as part of a replayed use case, we attempt to fuzz
this request. Then, the previously recorded input values stored for the current
step are used to advance to the next step. This process of fuzzing a request and
subsequently advancing one step along the use case is repeated until the test
case is exhausted. Alternatively, the process stops when the fuzzer replays the