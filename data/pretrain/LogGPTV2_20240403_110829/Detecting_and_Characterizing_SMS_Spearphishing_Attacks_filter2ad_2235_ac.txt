Step I: Data Pre-Processing. During the empirical study, we
observe that attackers often utilize text obfuscation to bypass de-
tection mechanisms. As NLP techniques are generally suitable for
well-written text, the obfuscated and ungrammatical content, i.e.,
adversarial text, may significantly reduce the effectiveness and relia-
bility of NLP tools [27, 30, 75]. Therefore, it is necessary to “sanitize”
the text content of fraudulent messages before forwarding them to
the next module.
Specifically, as the first step, we summarized the most popular
obfuscation methods by examining ground-truth dataset: mixed
text with special characters. For example, punctuation from dif-
ferent languages could be mixed, especially between Chinese and
English, e.g. the dot “.” in domain names could be replaced by “。”
or “·”, and the “:” in a URL could be replaced by “：”. To solve
this issue, we removed all redundant spaces and special characters
by comparing them with the public character list [24]. Also, digits
could be replaced by characters that are visually similar to them,
e.g. digit “1” and letter “l”, and digit “0” and letter “O”. Inspired
by previous works [31, 75] on adversarial text, we replaced these
characters with common morph combinations.
934ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Mingxuan Liu, Yiming Zhang, Baojun Liu, Zhou Li, Haixin Duan, and Donghong Sun
In addition, we also observed that illegal promoters create a
jargon term (“black keywords") to disguise transactions. Black key-
words are often unfriendly to outsiders, distorting the original
meaning of common terms or tweaking other black keywords. For
example, “微信” (Wechat) can be replaced by “徽信” which looks
similar but is not an existing word in Chinese. Since existing name
entity recognition systems are most domain-specific, it is difficult
to properly adapt and label these specialized terms. To address this
issue, we note that previous research collected and built a list of jar-
gon words for the Chinese underground economy [68]. In order to
reduce the errors introduced by these “black keywords", we extend
the jargon list of work [68] based on the observation of empirical
study, resulting in 4,718 jargon words. Then we replace the jargon
terms in the list with a fixed word “JARGON".
Step II: Entity Recognition. Recall our observation obtained from
the empirical study that SMS spearphishing attacks must include
luring information and exploiting payloads. In the following proce-
dure, we attempt to identify the customized personal information
and out-of-band contacts contained in suspicious messages by en-
tity recognition.
Based on the examination of ground-truth, we treat five types of
“victim’s personal information”, including Name, ID Number, Flight
Information, License Plate Number and Bank Card Number, as PIIs.
These PIIs need to be both identified as the luring information for
spearphishing detection and anonymized for privacy concerns. As
discussed in Section 3.2 and 4.1, we recognize human names by
NER through an open-source tool, HanLP 2.0 [24], which has well
adaptability to Chinese text. And we built regular expressions to
extract the other four PIIs. The recognization and anonymization
process was automated by scripts we built. Using the scripts, our
industrial partner helped, i.e., identifying and then replacing all
the PIIs with their hashes. We believe this step could minimize
potential harm and ensures that this research is ethically sound.
In addition, we also extracted the embedded contacts of fraud-
sters in the messages for follow-up communications with victims.
As mentioned in Section 4.1, embedded contacts are also a key fea-
ture of a spearphishing message. We design and implement regular
expressions to detect and extract several of the most commonly used
contacts, such as Phone Number (Cellphone, Phone and Hotline),
URL/Domain, Wechat and QQ (two of the largest social platforms
in China).
To sum up, suspicious messages containing both the victim’s
personal information and attacker’s contact would be forwarded to
the next module.
Table 1: Categories of spam messages.
Category
Financial Scam
Lawsuit Scam
Social Scam
Employment Scam
Insurance Scam
Fortune-telling Scam
Gambling Phishing
Promotional Spam
Other
ALL
Volume in
Labeled dataset
1,943
2,206
271
1,019
86
1,363
1,918
1,263
330
10,399
Volume in
Common SMS
2,583,017
2,452,277
672,853
1,244,083
93,670
1,226,959
15,598,262
5,775,055
2,310,261
31,956,437
Volume in
Spearphishing SMS
24,668
13,124
2,608
10,620
739
15,500
16,319
6,201
1,022
90,801
Step III: Syntactic Parsing. Syntactic analysis is able to examine
the syntax dependency in a given sentence, which is one of the most
important technologies in the NLP research field. In this module, we
utilize syntactic parsing to examine the ownership of the extracted
PII entities. As discussed in Section 4.1, one of our key observations
is that, a Name belonging to the victim differs in syntax features
from the one belonging to the attacker. SMS spearphishing texts
are often sent in a tone of the conversation between the attacker
and the victim, in which attacker is the initiator and victim is the
recipient. From the perspective of syntactic analysis, we found that
names of victims commonly meet the following two relationships
with their personal pronouns: Subject-verb relationship and At-
tributive modification. As supporting evidence, Figure 4 shows an
example of a spearphishing message with the Subject-verb relation-
ship. By contrast, a common promotional message doesn’t embed
the victim’s PII. The self-introducing phrase shows the Verb-object
relationship with its personal pronouns, which is different from the
relationship of victim’s.
As a result, after extracting personal information, we leverage
the syntactic parsing function of HanLP [24] to check the syntax
dependency. Only the SMS message that matches the above two
kinds of relationships would be regarded as a spearphishing attack.
Otherwise, the messages are still considered as regular fraudulent
spamming.
4.3 Classifying Business Categories
To perform a large-scale measurement study and understand the
ecosystem of SMS spearphishing attacks, we need to classify the
business types of spearphishing content. Due to the lack of public
labeled datasets, we first created a self-labeled dataset, and then
trained a machine-learning model to construct the multi-classifier.
Labeled Dataset. At this step, we randomly selected 15,000 sam-
ples from the entire dataset, and 10,399 messages are kept after
data deduplication. Two members labeled them independently ac-
cording to nine pre-defined categories, which were delineated with
reference to several published technical reports [44, 51] and pa-
pers [48]. In the first round, the agreement score is 98.24%, and
then we also discussed with a senior security expert to solve 183
conflicts. After review and discussion, we gave each conflict a unan-
imous agreement label. The volumes of each category are shown
in Table 1, with the Lawsuit Scam accounting for the most in the
labeled dataset.
Figure 4: Examples of syntactic parsing on spearphishing
and promotion messages1.
1The examples are translated from Chinese messages.
935Detecting and Characterizing SMS Spearphishing Attacks
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Table 2: Detected information of spearphishing spam SMS.
Victim’s Information
Spammer’s Contacts
Entity
Name
Flight
License Plate
ID Card
Bank Card
71,655 URL
Record Content Entity
90,801
883
571
17
1
10 CellPhone
536 Hotline
13 Phone
1 QQ
Record Content
45,935
58,968
13,158
11,169
2,627
3,922
1,882
3,419
9,599
11,959
5,215
4,359
WeChat
analysis. Thus, we consider that the performance provides reliable
results to support our measurement findings.
Table 3: The performance of each classification model.
Multi-classifier Models. In our study, 10,399 messages with con-
sistent labels were considered as the labeled dataset (for business
classifier), under nine categories. We tried two popular methods
of word embedding to get the vector representation, including
Word2Vec [29] and TF-IDF [58]. Then we applied five popular ma-
chine learning models for text classification [28]. Leveraging our
labeled dataset, Table 3 shows the performance of all text classifica-
tion models with different embedding ways.
In the end, we find that the combination of Word2Vec and logistic
regression model [21] performs the best (average F1-score 93.42%).
Therefore, we employ it to categorize all other messages.
4.4 Evaluation Results
We implemented the detection system on the entire collected dataset
and detected 90,801 (71,655 deduplicated message content) spearphish-
ing messages. The detailed detection results are shown in Table 2,
and representative examples for each category of spearphishing
messages could be seen in Table 4. Here, we discuss the evaluation
results of our detection system and category classifier.
Effectiveness of Detection System. As for the ground-truth dataset
(1,196 messages), 937 of them were correctly detected, with 36 false
positives (precision 96.16%) and 292 false negatives (recall 75.33%).
We randomly sampled 200 spearphishing messages and manually
checked, with only 8 false positives. According to other works’
evaluation [25, 73], the precision in the sample set, 96%, combined
with the precision in the ground-truth dataset, can represent the
precision on the whole detected spearphishing messages, due to
the randomness of sampling.
We manually checked the detection errors and found that the
low recall rate is mainly limited by the performance of the entity
recognition algorithm. Due to the domain specificity of spam con-
tent and Chinese language, it is known that open-source NLP tools
are difficult to achieve the desired performance[34, 74]. And until
now, there have been no sophisticated solutions to this problem.
We acknowledge that the recall rate of our detection system is not
perfect, which means that our detection results are only the lower
bound of actual SMS spearphishing attacks.
Category Classification Result. For the multi-classifier, the pre-
cision is 93.46% and recall is 93.47% on the labeled dataset. From the
confusion matrix, we find most categories could be classified accu-
rately with precision over 86%, except for “Other” and “Promotional
Spam”. Manual inspection of these two categories showed that, the
main reason for misclassification was that the length of several
messages is too short to extract semantics. However, these two
categories are relatively least malicious among all spearphishing
businesses, that we rarely focus on in the subsequent measurement
5 MEASUREMENT
Our detection system reports 90,801 (0.285% of total) spearphishing
messages on the three-month dataset. In this section, we empiri-
cally analyze the behavior of spearphishing attackers based on the
detected messages, including their sending characteristics (business
categories, time and geographical characteristics), infrastructures
(distribution channels and out-of-band contacts), real-world impact,
and the personal information (in hash format) of victims. We also
group the detected messages into spearphishing campaigns, and
discussed their active properties and attacking strategies.
5.1 Characteristics
In this section, we will examine the spearphishing attacks from
a macro perspective, including its business categories, sending
behaviors and infrastructures (sending channels and follow-up
contacts) that attackers utilize.
Figure 5: Comparison of the category distributions.