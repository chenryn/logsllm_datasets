to tell `coverage.py` to ignore it. For example, if you have some code in
abstract classes that is going to be tested on the subclasses, you can ignore it
with `# pragma: no cover`.
If you want
[other code to be excluded](https://github.com/nedbat/coveragepy/issues/831),
for example the statements inside the `if TYPE_CHECKING:` add to your
`pyproject.toml`:
```toml
[tool.coverage.report]
exclude_lines = [ "pragma: no cover", "if TYPE_CHECKING:",]
```
# [Running tests in parallel](https://pypi.org/project/pytest-xdist/)
`pytest-xdist` makes it possible to run the tests in parallel, useful when the
test suit is large or when the tests are slow.
## Installation
```bash
pip install pytest-xdist
```
## Usage
```bash
pytest -n 4
```
It will run the tests with `4` workers. If you use `auto` it will adapt the
number of workers to the number of CPUS, or 1 if you use `--pdb`.
To configure it in the `pyproject.toml` use the `addopts`
```ini
[tool.pytest.ini_options]
minversion = "6.0"
addopts = "-vv --tb=short -n auto"
```
### [Enforce serial execution of related tests](https://github.com/pytest-dev/pytest-xdist/issues/84)
#### Use a lock
Implement a `serial` fixture with a session-scoped file `lock` fixture using the
`filelock` package. You can add this to your `conftest.py`:
```bash
pip install filelock
```
```python
import contextlib
import os
import filelock
import pytest
from filelock import BaseFileLock
@pytest.fixture(name="lock", scope="session")
def lock_(
    tmp_path_factory: pytest.TempPathFactory,
) -> Generator[BaseFileLock, None, None]:
    """Create lock file."""
    base_temp = tmp_path_factory.getbasetemp()
    lock_file = base_temp.parent / "serial.lock"
    yield FileLock(lock_file=str(lock_file))
    with contextlib.suppress(OSError):
        os.remove(path=lock_file)
@pytest.fixture(name="serial")
def _serial(lock: BaseFileLock) -> Generator[None, None, None]:
    """Fixture to run tests in serial."""
    with lock.acquire(poll_interval=0.1):
        yield
```
Then inject the `serial` fixture in any test that requires serial execution. All
tests that use the serial fixture are executed serially while any tests that do
not use the fixture are executed in parallel.
#### Mark them and run separately
Mark the tests you want to execute serially with a special mark, say serial:
```python
@pytest.mark.serial
class Test:
    ...
@pytest.mark.serial
def test_foo():
    ...
```
Execute your parallel tests, excluding those with the serial mark:
```bash
$ py.test -n auto -m "not serial"
```
Next, execute your serial tests in a separate session:
```bash
$ py.test -n0 -m "serial"
```
# [Setting a timeout for your tests](https://pypi.org/project/pytest-timeout/)
To make your tests fail if they don't end in less than X seconds, use
[pytest-timeout](https://pypi.org/project/pytest-timeout/).
Install it with:
```bash
pip install pytest-timeout
```
You can set a global timeout in your `pyproject.toml`:
```toml
[pytest]
timeout = 300
```
Or define it for each test with:
```python
@pytest.mark.timeout(60)
def test_foo():
    pass
```
# [Rerun tests that fail sometimes](https://pypi.org/project/pytest-rerunfailures/)
[pytest-rerunfailures](https://pypi.org/project/pytest-rerunfailures/) is a
plugin for pytest that re-runs tests to eliminate intermittent failures. Using
this plugin is generally a bad idea, it would be best to solve the reason why
your code is not reliable. It's useful when you rely on non robust third party
software in a way that you can't solve, or if the error is not in your code but
in the testing code, and again you are not able to fix it.
Install it with:
```bash
pip install pytest-rerunfailures
```
To re-run all test failures, use the `--reruns` command line option with the
maximum number of times you’d like the tests to run:
```bash
pytest --reruns 5
```
Failed fixture or setup_class will also be re-executed.
To add a delay time between re-runs use the `--reruns-delay` command line option
with the amount of seconds that you would like wait before the next test re-run
is launched:
```bash
pytest --reruns 5 --reruns-delay 1
```
To mark individual tests as flaky, and have them automatically re-run when they
fail, add the `flaky` mark with the maximum number of times you’d like the test
to run:
```python
@pytest.mark.flaky(reruns=5)
def test_example():
    import random
    assert random.choice([True, False])
```
# [Run tests in a random order](https://github.com/jbasko/pytest-random-order)
`pytest-random-order` is a pytest plugin that randomises the order of tests.
This can be useful to detect a test that passes just because it happens to run
after an unrelated test that leaves the system in a favourable state.
To use it add the `--random-order` to your pytest run.
# [Capture deprecation warnings](https://docs.pytest.org/en/latest/how-to/capture-warnings.html)
Python and its ecosystem does not have an assumption of strict SemVer, and has a
tradition of providing deprecation warnings. If you have good CI, you should be
able to catch warnings even before your users see them. Try the following pytest
configuration:
```toml
[tool.pytest.ini_options]
filterwarnings = [ "error",]
```
This will turn warnings into errors and allow your CI to break before users
break.
You can ignore specific warnings as well. For example, the configuration below
will ignore all user warnings and specific deprecation warnings matching a
regex, but will transform all other warnings into errors.
```toml
[tool.pytest.ini_options]
filterwarnings = [ "error", "ignore::UserWarning", "ignore:function ham\\(\\) is deprecated:DeprecationWarning",]
```
When a warning matches more than one option in the list, the action for the last
matching option is performed.
If you want to ignore the warning of a specific package use:
```toml
filterwarnings = [ "error", "ignore::DeprecationWarning:pytest_freezegun.*",]
```
Note: It's better to suppress a warning instead of disabling it for the whole
code, check how [here](use_warnings.md#suppressing-a-warning).
## [Ensuring code triggers a deprecation warning](https://docs.pytest.org/en/latest/how-to/capture-warnings.html#ensuring-code-triggers-a-deprecation-warning)
You can also use pytest.deprecated_call() for checking that a certain function
call triggers a `DeprecationWarning` or `PendingDeprecationWarning`:
```python
import pytest
def test_myfunction_deprecated():
    with pytest.deprecated_call():
        myfunction(17)
```
## [Asserting warnings with the warns function](https://docs.pytest.org/en/latest/how-to/capture-warnings.html#warns)
You can check that code raises a particular warning using pytest.warns(), which
works in a similar manner to raises:
```python
import warnings
import pytest
def test_warning():
    with pytest.warns(UserWarning):
        warnings.warn("my warning", UserWarning)
```
The test will fail if the warning in question is not raised. The keyword
argument match to assert that the exception matches a text or regex:
```python
>>> with pytest.warns(UserWarning, match='must be 0 or None'):
...     warnings.warn("value must be 0 or None", UserWarning)
```
## [Recording warnings](https://docs.pytest.org/en/latest/how-to/capture-warnings.html#recwarn)
You can record raised warnings either using `pytest.warns()` or with the
`recwarn` fixture.
To record with `pytest.warns()` without asserting anything about the warnings,
pass no arguments as the expected warning type and it will default to a generic
Warning:
```python
with pytest.warns() as record:
    warnings.warn("user", UserWarning)
    warnings.warn("runtime", RuntimeWarning)
assert len(record) == 2
assert str(record[0].message) == "user"
assert str(record[1].message) == "runtime"
```
The `recwarn` fixture will record warnings for the whole function:
```python
import warnings
def test_hello(recwarn):
    warnings.warn("hello", UserWarning)
    assert len(recwarn) == 1
    w = recwarn.pop(UserWarning)
    assert issubclass(w.category, UserWarning)
    assert str(w.message) == "hello"
    assert w.filename
    assert w.lineno
```
Both `recwarn` and `pytest.warns()` return the same interface for recorded
warnings: a `WarningsRecorder` instance. To view the recorded warnings, you can
iterate over this instance, call `len` on it to get the number of recorded
warnings, or index into it to get a particular recorded warning.
# [Show logging messages on the test run](https://stackoverflow.com/questions/51466586/pytest-how-to-show-messages-from-logging-debug-in-the-function-under-test)
Add to your `pyproject.toml`:
```toml
[tool.pytest.ini_options]
log_cli = true
log_cli_level = 10
```
Or run it in the command itself
`pytest -o log_cli=true --log-cli-level=10 func.py`.
Remember you can [change the log level](#change-the-log-level) of the different
components in case it's too verbose.
# Test asyncio programs
Check the [asyncio article](asyncio.md#testing).
# [Stop pytest right at the start if condition not met](https://stackoverflow.com/questions/70822031/stop-pytest-right-at-the-start-if-condition-not-met)
Use the `pytest_configure` [initialization hook](https://docs.pytest.org/en/4.6.x/reference.html#initialization-hooks).
In your global `conftest.py`:
```python
import requests
import pytest
def pytest_configure(config):
    try:
        requests.get(f'http://localhost:9200')
    except requests.exceptions.ConnectionError:
        msg = 'FATAL. Connection refused: ES does not appear to be installed as a service (localhost port 9200)' 
        pytest.exit(msg)
```
- Note that the single argument of `pytest_configure` has to be named `config`.
- Using `pytest.exit` makes it look nicer.
# Pytest integration with Vim
Integrating pytest into your Vim workflow enhances your productivity while
writing code, thus making it easier to code using TDD.
I use [Janko-m's Vim-test plugin](https://github.com/janko-m/vim-test) (which
can be installed through [Vundle](https://github.com/VundleVim/Vundle.vim)) with
the following configuration.
```vim
nmap  t :TestNearest --pdb
nmap  t :TestSuite tests/unit
nmap  i :TestSuite tests/integration
nmap  T :TestFile
let test#python#runner = 'pytest'
let test#strategy = "neovim"
```
I often open Vim with a vertical split (`:vs`), in the left window I have the
tests and in the right the code. Whenever I want to run a single test I press
`t` when the cursor is inside that test. If you need to make changes in the
code, you can press `t` again while the cursor is at the code you are testing
and it will run the last test.
Once the unit test has passed, I run the whole unit tests with `;t` (as `;` is
my ``). And finally I use `;i` to run the integration tests.
Finally, if the test suite is huge, I use `;T` to run only the tests of a single
file.
As you can see only the `t` has the `--pdb` flag, so the rest of them will run
en parallel and any pdb trace will fail.
# Reference
- Book
  [Python Testing with pytest by Brian Okken](https://www.oreilly.com/library/view/python-testing-with/9781680502848/).
- [Docs](https://docs.pytest.org/en/latest/)
- [Vim-test plugin](https://github.com/janko-m/vim-test)