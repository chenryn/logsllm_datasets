t
y
r
o
m
e
M
150
100
50
0
8
6
4
2
)
s
/
s
t
n
e
v
e
(
s
s
i
m
_
C
L
L
s
d
a
e
R
Netflix 0%BC
Netflix 100%BC
Atlas
2000 4000 6000 8000 10000 12000 14000 16000
# Concurrent HTTP persistent connections
(d) Memory WRITE
·107
Netflix 0%BC
Netflix 100%BC
Atlas
2000 4000 6000 8000 10000 12000 14000 16000
# Concurrent HTTP persistent connections
(e) Memory READ-Network Throughput Ratio
(f) CPU reads served from DRAM due to LLC misses
Figure 11: Plaintext performance, Netflix vs. Atlas, zero and 100% Buffer Cache (BC) ratios.
traffic going from the client to the server. The middlebox supports a
configurable set of delay bands - we use this feature to add different
delays to different flows, with latencies chosen from the range 10
to 40ms. In the middlebox, each newly received packet is hashed
and buffered, and a per-flow constant delay1 is introduced before
the packet is forwarded on. To reduce stress on the middlebox and
avoid it becoming a bottleneck, we only route client-to-server traffic
through it, as the data rate in this direction is much lower.
We wish to test the systems under a range of loads and with vary-
ing numbers of clients. As we don’t have Netflix’s intelligent client,
we rely on a load generator that models the sort of requests seen by a
video server. We populate the disks with small files (~300KB), each
corresponding to the equivalent of a video chunk. We use weighttp
to generate HTTP-persistent traffic with multiple concurrent clients2.
Each client establishes a long-lived TCP connection to the server,
and generates a series of HTTP requests with a new request sent
immediately after the previous one is served.
The Netflix configuration uses all eight available CPU cores in the
video server. For Atlas we only use four CPU cores with one stack
instance pinned to each core for the whole range of experiments.
We expect each stack instance to bottleneck on different resources
depending on the workload: plaintext HTTP traffic should not be
a CPU-intensive task and thus we expect the performance of each
stack instance to only be limited by the disk; in contrast encrypted
network traffic puts heavy pressure both on the Last Level Cache
1This avoids packet reordering within a flow.
2This tool has been modified to support requesting multiple URLs.
219
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
Ilias Marinos, Robert N.M. Watson, Mark Handley, and Randall R. Stewart
(a) Delayed notifications incur extra memory writes.
(b) Heavy load and netmap latency result in LLC evictions.
Figure 12: Principal sub-optimal Atlas memory access patterns for unencrypted traffic.
and the CPU cores which need to access and encrypt all the data
before they can be transmitted.
4.1 Plaintext HTTP-persistent Traffic
We wish to evaluate the performance of Atlas and Netflix stacks with
a plaintext HTTP workload as we vary the number of concurrent
active HTTP connections. In the Netflix case we also need to include
an extra dimension that impacts performance: the disk buffer cache
hit ratio. In these benchmarks we are able to accurately control
the disk buffer cache efficiency by adjusting the amount of distinct
content that is requested by the clients. In the worst case, each video
chunk is only requested once during the duration of the test, requiring
the server to fetch all content from the disks; in the best case, the
requested content is already cached in main memory and the server
does not need to access the disks at all. Atlas does not utilize a buffer
cache: all data requests, even repetitive ones, are served from the
disk, so Atlas is not sensitive to the choice of workload.
Figures 11a and 11b show the network throughput and CPU
utilization achieved by both systems. Atlas and the Netflix setup
with a maximally effective buffer cache (100% BC) manage to
saturate both the 40GbE NICs, achieving roughly ~73Gbps of HTTP
throughput with higher numbers of concurrent connections. For
less than 4,000 simultaneous connections Atlas achieves slightly
less throughput (~13%) compared to the Netflix setup. We believe
that this happens because Atlas often delays making an I/O request
until the available TCP window of a flow grows to a larger value
(10*MSS) so that it can improve disk throughput with slightly larger
reads. Better tuning when the system has so much headroom would
avoid this.
With the uncachable workload (0% BC), we observe that Netflix
experiences a small performance drop as the number of connections
increases. Although VM subsytem pressure is handled by the Netflix
configuration much more gracefully than stock FreeBSD, with more
connections requesting new data from the disks, the rate of proactive
calls to reclaim memory increases, negatively affecting performance.
This 0% BC workload comes much closer to the real-world situation:
Netflix video streamers typically get low to no benefit from the in-
memory buffer cache (<10% hit ratios), except perhaps on occasions
when new and very popular content is added to the catalog and a
spike in the disk buffer cache efficiency is observed. Atlas does
not suffer such a performance drop-off, so is well suited to such
uncachable workloads.
It is interesting to observe the CPU utilization of the Netflix setup
for the two different workloads. The CPU utilization almost doubles
when the buffer cache is thrashed and disk activity is required. It
should be noted that the CPU utilization results reported for Atlas
are hardly representative of the actual work performed. Atlas relies
on polling for disk I/O completions and new packets on the NIC
rings, so the CPU cores are constantly spinning even though the
actual load might be light, and hence the CPU utilization measured
remains constant at ~400% when using four cores.
From a microarchitectural viewpoint, where do Atlas’s perfor-
mance benefits actually come from? Atlas requires ~77Gb/s of mem-
ory read and write throughput respectively to saturate the NICs. This
comes very close to a one-to-one ratio between network and memory
read throughput (see Fig. 11e), indicating that Atlas does not suffer
from multiple detours to RAM due to LLC evictions. In contrast,
Netflix requires more memory read throughput—almost 1.5× the
network throughput—to achieve similar network performance.
Although it is quite efficient, our expectation was that Atlas, due
to DDIO, would demonstrate even better memory traffic efficiency.
Why is this not the case? If we consider the ratio of memory reads to
network throughput when Atlas serves 2,000 clients, we observe that
the memory traffic required is about 65% of the network throughput
achieved – clearly in this case DDIO is helping save memory traffic.
Data is being loaded from storage into the LLC by DMA, and about
35% of it is still in the LLC when it is DMAed to the NIC. For
this data, the data flow is like that shown in Figure 12a. Note that
the memory write throughput is actually higher than memory read
because netmap does not provide timely enough TX completion
notifications to allow buffers to be immediately reused. We believe
that detours to main memory could be reduced even further if netmap
supported a fine-grained, low-latency mechanism to communicate
TX DMA completion notifications to userspace applications: such a
mechanism would allow us to utilize a strict LIFO approach while
recycling DMA I/O buffers that could significantly reduce the stack’s
working set, increasing DDIO efficacy.
Why is the situation different for higher number of concurrent
connections? We believe that the answer is related to the load that
the stack experiences; for more than 4K concurrent connections the
disks are close to saturation. Atlas then builds deeper queues on the
I/O and NIC rings. This increases the working set of the stack since
more diskmap buffers need to be associated with I/O commands
and connections at each instant. At this point the storage and NIC
220
DRAM	LLC	NIC	TCP	CPU	stale	buﬀers	re-use	buﬀer	NVMe	NVMe	DRAM	LLC	NIC	TCP	CPU	TCP	Packets	re-use	buﬀer	Disk|Crypt|Net: rethinking the stack for high-performance video streaming
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
)
s
/
b
G
(
t
u
p
h
g
u
o
r
h
T
t
e
N
80
60
40
20
0
)
s
/
b
G
(
t
u
p
h
g
u
o
r
h
t
y
r
o
m
e
M
150
100
50
0
t
u
p
h
g
u
o
r
h
T
t
e
N
/
d
a
e
r
m
e
M
4
3
2
1
0
Netflix 0% BC
Netflix 100% BC
Atlas
2000 4000 6000 8000 10000 12000 14000 16000
# Concurrent HTTP persistent connections
)
%
(
n
o
i
t
a
z
i
l
i
t
u
U
P
C
800
600
400
200
0
Netflix 0% BC
Netflix 100% BC
Atlas
2000 4000 6000 8000 10000 12000 14000 16000
# Concurrent HTTP persistent connections
(a) Network throughput (Error bars indicate the 95% CI)
(b) CPU utilization (Average)
)
s
/
b
G
(
t
u
p
h
g
u
o
r
h
t
y
r
o
m
e
M
150
100
50
0
2
1.5
1
0.5
)
s
/
s
t
n
e
v
e
(
s
s
i
m
_
C
L
L
s
d
a
e
R
Netflix 0%BC
Netflix 100%BC
Atlas
2000 4000 6000 8000 10000 12000 14000 16000
# Concurrent HTTP persistent connections
(c) Memory READ
Netflix 0%BC
Netflix 100%BC
Atlas
2000 4000 6000 8000 10000 12000 14000 16000
# Concurrent HTTP persistent connections
Netflix 0%BC
Netflix 100%BC
Atlas
2000 4000 6000 8000 10000 12000 14000 16000
# Concurrent HTTP persistent connections
(d) Memory WRITE
·108
Netflix 0%BC
Netflix 100%BC
Atlas
2000 4000 6000 8000 10000 12000 14000 16000
# Concurrent HTTP persistent connections
(e) Memory READ-Network Throughput Ratio
(f) CPU reads served from DRAM due to LLC misses
Figure 13: Encrypted performance, Netflix vs. Atlas, zero and 100% Buffer Cache (BC) ratios.
DMA are no longer closely coupled enough in Atlas’s event loop so
that all data remains in the LLC until transmission. Memory usage
looks more like Figure 12b. In any case, when we look at LLC
misses (Figure 11f), we see Atlas does not experience any CPU
stalls whatsoever while waiting for reads to be served from memory,
indicating that the memory read throughput observed is entirely due
to DMA to the NIC.
4.2 Encrypted HTTP-persistent Traffic
The need to encrypt traffic significantly complicates the process