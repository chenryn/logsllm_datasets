User: How to use Python and multi-threading technology to implement a simple web crawler to crawl data from a given URL list every minute?
Assistant: The following is a simple sample program that uses Python and multi-threading technology to implement a web crawler to crawl data from a given URL list every minute: ```python import requests import threading import time # Store the list of URLs to be crawled url_list = ['https://www.google.com', 'https://www.baidu.com', 'https://www.qq.com'] # Define crawling function def crawl(url): response = requests.get(url) print("From %s: %d bytes" % (url, len(response.content))) # Define the main function def main(): while True: print("Starting crawl... ") threads = [] for url in url_list: # Create a thread and start it t = threading.Thread(target=crawl, args=(url,)) t.start() threads.append(t) # Wait for all threads to complete for t in threads: t.join() print("Crawl complete.") # Wait for 1 minute time.sleep(60) if __name__ == '__main__': main() ``` The execution process of the program is as follows: - in the main In the function, the program will start a crawling operation every 1 minute. - At the beginning of each crawling operation, the program will traverse the crawled URL list and create a multi-thread to crawl the web page data corresponding to each URL. - After waiting for all threads to complete, the program will output the prompt "Crawl complete.", indicating that the crawling operation is complete. - The program will execute in a loop until it is manually terminated.