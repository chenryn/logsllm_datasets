This output shows that the most page faults were to regions without a filename—which would be
process heaps—with 537,925 faults occurring during tracing, The libc library encountered 84,814
faults while tracing This is happening because the software build is creating many short-lived
processes, which are faulting in their new address spaces.
7 0rigin: I crested it for this book on 27-Jan-2019, and Ive traced page fsult stacks in the past with other tracers [82].
8 Origin: I crested it for this book on 26-Jan-2019.
---
## Page 305
268
Chapter 7 Memory
The source to ffaults(8) is:
#1/usx/local/bin/bpftrace
#Include 
kprobe :handle_rn_fault
vma = (struct ve_area_struct *)argo;
$file = $vns=>va_file->f_path. dentry=>d_nane -nane,
[str ($fs1e)] = count () 
‘squaumse st tuo 'ptre uogoung pauag Onney"uru“aptreq aug aoen oq saqordy sasn [oo s
determine the filename for the fault. The rate of file faults varies depending on the workload;
you can check it using tools such as perf(1) or sar(1). For high rates, the overhead of this tool may
begin to become noticeable.
7.3.8
vmscan
vmscan(8)? uses the vmscan tracepoints to instrument the page-out daemon (kswapd), which
scarwer is still used to refer to this kernel function, for efficiency, Linux nowadays manages
frees memory for reuse when the system is under memory pressure. Note that, while the term
memory via linked lists of active and inactive memory.
Running vmscan on a 36-CPU system while it runs out of memory:
vmscan.bt
Attaching 10 probes..
TIME
S-SLABms
D-RECLAIMms
M-RECLAIMns KSKAPD WRITEPAGE
21 :30 : 25
0
0
0
D
21 :30 : 26
0
0
0
0
0
21 :30 : 27
276
555
2
1
21:30: 28
5459
7333
15
72
21 :30 : 29
41
O
4 9
35
21 :30 : 30
1
454
Q
2
2
21:30 : 31
Q
0
edirect_reclain_nst
[256K, 512K)
518
[512K,1)
0860860860880886886886881 08
trace-vmscan-postprocess.pl, which has been in the Linux source since 2009
---
## Page 306
7.3  BPF Tools
269
[1M, 2M)
174 1869889889886 8869869869869869889889886 886986986986986 1
[2M, 4H)
136 leeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee
[4M, 8M)
0888 08888808199
[8H, 16X)
68 188e88e88e88e888e8ee8
[16M, 32M)
8188
[32N, 64M]
31
[64M, 128M)
0 1
[128X, 25 68)
0 1
[256M, 512M)
18186988
eshrink_slab_ns:
(95z 8z1]
12228 1eeeeeeeeee8ee8eeeeeeeeeeeeeeeeee
[256, 512}
19859  8 988988988 8869869869898988988988 886888898980 
[512,1K)
1899 18898
[1K, 2K)
1052 188
[2K, 4K)
54618
[4K, 8K]
241 1
[8K, 16K)
1221
[16K,32K)
51818
(x9*x21
al 009
[64K, 128K)
491
[128K, 256K)
191
[256K, 512K)
71
[512K,18)
6 1
[1M,2M)
81
[2M, 4H)
41
[4M, 8M)
71
[8H, 16K)
291
[168,32M]
111
[32N, 648]
31
[64M, 128M)
0 1
[128X, 25 6K)
[256M, 512M)
191
The per-second columns show:
• S-SLABms: Total time in shrink slab, in milliseconds. This is reclaiming memory from
various kernel caches.
• D-RECLAIMms: Total time in direct reclaim, in milliseconds. This is foreground reclaim,
which blocks memory allocations while memory is written to disk.
• M-RECLAIMms: Total time in memory cgroup reclaim, in milliseconds. If memory cgroups
are in use, this shows when one cgroup has exceeded its limit and its own cgroup memory
is reclaimed.
---
## Page 307
270
Chapter 7 Memory
• KSWAPD: Number of kswapd wakeups.
• WRITEPAGE: Number of kswapd page writes.
The times are totals across all CPUs, which provides a measure of cost beyond the counts seen by
other tools, such as vmstat(1).
Look out for time in direct reclaims (D-RECLAIMms): This type of reclaim is *bad" but necessary,
and will cause performance issues. It can hopefully be eliminated by tuning the other vm sysctl
tunables to engage background reclaim sooner, before direct reclaim is necessary.
The output histograms show per-event times in direct reclaim and shrink slab, in nanoseconds.
The source to vmscan(8) is:
#1/usx/1ocal/bin/bpCt.race
tacepoint:vmscan:nn_shzink_slab_staxt [ @staxt_ss[tid] = nsecs,)
tracepoint:ivmscan:nm_shrink_slab_end /Bstart_ss |tid] /
$dur_ss = nsecs - Bstart_ss[tid] :
Bsun_sa - @sun_as + Sdur_ss,
Bshrink_slab_ns = hist($dur_ss) 
delete (@start_ss[tid]) 
tracepointivmscan:nn_mscan_direct_reclain_begin ( Bstart_dr[tid] = nsecs, ]
tacepoint:vnscan:nn_mscan_direct_reclain_end /Bstart_dr [tid]/
Sdux_dx = nsecs - Bstaxt_dr [tld] :
Bsun_dr = Bsun_dr + $dur_dr,
Bdirect_reclaln_ns = hlst(5dur_dr) 
delete (estart_de [tid])
tracepointivmscan:nn_vmscan_mencg_reclain_begin l estatt_mr [tid] = nsecsz 
tacepoint:vmscan:nn_mscan_sencg_reclaln_end /estazt_mx [tid]/
sdux_ex = nsecs - Bstaxt_nr [tld] :
Bsun_mr = Bsun_mr + $dur_nc
Bnencg_reclaln_ns = hist (§dux_mx) ≠
delete (estart_me [tid])
---
## Page 308
7.3 BPF Tools
271
BEGIN
printf (*g10s $10s 12s 12s 6s 9s^n*,*7IME”,
*SSLABns*,*D-RECLAIMns”, "M-RECLAIXns”, "KSKAPD”, "MRITEPAGE*
interval:s:1
t.ime ("%B: 5H: 4S*) :
printf(*10d e12d 412d 86d 49d\n*,
@sun_ss / 1000000, @sun_dr / 1000000, @sun_nr / 1000000,
fcount_vk, Bcount_xp) :
clear (8sun_ss) 
clear (8aun_dr) 
clear (8sun_nx)
clear (8count_vk) 
cleax (Bcount_μp) 
This tool uses various vmscan tracepoints to record times when events begin so that duration
histograms and running totals can be maintained.
7.3.9
drsnoop
Supsoqs otuatu Bujaau o4 upeoudde ue[oau paup atq Supen og [o Og e s a(g)doousip
the proces affected and the latency: the time taken for the reclaim. It can be used to quantify the
application performance impact of a memory-constrained system. For example:
- doousap 
TIME (s)
CONH
PID
LAT (ns] PAGES
0, 000000000
java
11266
11266
1. 72
5 7
000000*0
Java
3.21
57
0, 011856000
Java
11266
2.02
43
0OOSTE8t0*0
Java
11266
3.09
55
0, 024647000
acpid
1209
6, 46
7.3
[..-]
10 Origin: This wss cre
on 10-Feb-2019
---
## Page 309
272
Chapter 7 Memory
This output shows some direct reclaims for Java, taking between one and seven millseconds.
The rates of these reclaims and their duration can be considered in quantifying the application
impact.
This tool works by tracing the vmscan mm_vmscan_direct_reclaim_begin and
mm_vmscan_dlirect_reclaim_end tracepoints. These are expected to be low-frequency
events (usually happening in bursts), so the overhead should be negligible.
Command line usage:
[suofido] doousxp
apnpu suogdo
 T: Includes timestamps
•=p PID: Measures this process only
7.3.10 swapin
swapin(8) shows which processes are being swapped in from the swap devices, if they exist and
are in use. For example, this system swapped out some memory and had 36 Kbytes swapped back
in (*si* column) while I was watching it with vmstat(1):
#vmstat 1
proca
 -systen-=
cpU=*
rb
pdn2
free
bo in cs us sy id va st
[.--]
46 11
29696 1585680
4384 1828440
 0
0 88047 20342180937316 81 18010
776 5729696 2842156
7976 1865276
9E
0 52832 2283 18678 37025 85 15
01
294 13529696 448580
4620 1860144
[..-]
swapin(8) identifies the process that was swapped in. At the same time:
suapin.bt
Attach.ing 2 probes..
[...]
E1L5=90
06 : 57 : 44
1.1 0rigin: 1 first created 8 simile tool called snonpgpid.d on 25-Jul-2005, with help from James Dickens. This mas one
wanted to show which processes were affected. I crested this bpfrace version for this book on 26-Jan2019
of the long-standing performance issues I wrestled with beforehand: I could see that the system was swapping. but 1
---
## Page 310
7.3 BPF Tools
273
e [aystemd1ogind, 1354] :  9
06 : 57 : 45
[. - -]
This output shows that systemd-logind (PID 1354) had 9 swap-ins. With a 4 Kbyte page size, this
adds up to the 36 Kbytes seen in vmstat(1). 1 logged into the system using ssh(1), and this compo-
nent in the login software had been swapped out, so the login took longer than usual.
Swap-ins occur when an application tries to use memory that has been moved to the swap
device. This is an important measure of the performance pain suffered by an application due to
uogeosdde 1page 4[μoap xou Aeu sno-dems pue Surutreos ag 'sonau dems 1augo Surddems
performance.
The source to swapin(8) is:
+1/usr/local/bin/bpf t.race
kprobe:sxap_readpage
8 [conm, pid] = count()
interval:s:1
t.ime () 
clear (81
print (8) 
This tool uses kprobes to trace the swap_readpage( kernel function, which runs in the context of
the swapping thread, so the bpftrace built-ins for comm and pid reflect the swapping process.
7.3.11 hfaults
pages are in use. For example:
hfaults .bt
Attach.ing 2 probes...
Tracing Huge Page faulta per process.., Bit Ctrl-C to end.
C
e[884, hugennap]: 9
12 Origin: Amer Ather crested it for this boolk
 on 6-May-2019
---
## Page 311
274
Chapter 7 Memory
This output includes a test program, hugemmap, with PID 84, which triggered nine huge
page faults.
The source to hfaults(8) is:
#1/usx/1ocal/bin/bpEtrace
BEGIX
kprobe:hugetlb_fauI t
8 [pid, conn] = count () 
If needed, more details can be fetched from function arguments, including struct mm_struct
and struct vm_area_struct. The ffaults(8) tool (see Section 7.3.7) fetched the filename from the
vm_area_struct.
7.3.12 0ther Tools
Two other BPF tools are worth mentioning:
• llcstat(8) from BCC is covered in Chapter 5; it shows the last-level cache hit ratio, by
process.
• profile(8) from BCC is covered in Chapter S; it samples stack traces and can be used as a
coarse and cheap way to find malloc() code paths.
7.4 BPF One-LinerS
This section shows BCC and bpftrace one-liners. Where possible, the same one-liner is
implemented using both BCC and bpftrace.
7.4.1BCC
Count process heap expansion (brk() by user-level stack trace:
stackcount -U t:syscalla:sys_entex_bxk
Count user page faults by user-level stack trace:
stackcount -U t:exceptions:page_fault_user
---
## Page 312
7.5 Optional Exercises
275
Count vmscan operations by tracepoint:
funccount t:vnscan:**
Show hugepage_madvise( calls by process:
sstapeuebedeing eoex
Count page migrations:
runccount t:nigrate:nn_mlgzate_pagea
Trace compaction events
trace t:compactlon:nn_conpaction_begin
7.4.2 bpftrace
Count process heap expansion (brk() by code path:
ppftzace -e tzacepoint:ayscalls:ays_enter_brk ( B[uatack, comm] - count(1;
Count page faults by process:
Count user page faults by user-level stack trace:
opftzace -e *txacepoint:exceptions:page_fault_user I e[ustack, conn] = count(l: 1*
Count vmscan operations by tracepoint:
1()qunco = [eqoxd]8 1 +1ueosus:autcdeoexs, 8eoexsgdg
Show hugepage_madvise() calls by process:
ppftzace -e *kprobe:hugepage_madvise ( printf (*sa by PID ld\,n*, probe, pid] ; ) 
Count page migrations:
1(0aunoo 8）ssbederexbauu:eexbtu:surodeoexs.8-soexsdg
Trace compaction events:
7.5Optional Exercises
If not specified, these can be completed using either bpftrace or BCC:
1. Run vmscan(8) for ten minutes on a production or local server. If any time was spent in
direct reclaim (D-RECLAIMms), also run drsnoop(8) to measure this on a per-event basis.
2. Modify vmscan(8) to print the header every 20 lines so that it remains onscreen.
---
## Page 313
276
6 Chapter 7Memory
(8)gney asn (uopeogdde dopxsap so uogonposd e 1aura) dnμeqs uogeodde Susn(I 
to count page fault stack traces. This may involve fixing or finxding an application that
supports stack traces and symbols (see Chapters 13 and 18).
4. Create a page fault flame graph from the output of Exercise 3.