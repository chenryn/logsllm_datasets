i, ..., Dn=dn]
(1)
(2)
(3)
they denoted as Pr(r ∈ S | x(cid:48)
This formulation differs from Yang et al.’s formulation
in the following ways. As before, we change some vari-
able names and only consider programs producing out-
puts over a ﬁnite domain. Also, rather than using short-
hand, we write out variables explicitly and denote the dis-
tributions from which they are drawn. For example, for
what
i, x−i), we write
PrP,A[O=o | D1=d1, ..., Di=d(cid:48)
i, ..., Dn=dn], where the
data points D1, ..., Dn are drawn from the population dis-
tribution P and the output O uses the algorithm’s internal
randomization A. This allows explicitly discussion of how the
data points D1, ..., Dn may be correlated in the population P
from which they come.
Finally, we explicitly deal with data points potentially
having a probability of zero under P. We ensure that we only
attempt to calculate the conditional probability for databases
with non-zero probability. This introduces a new problem:
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:36:11 UTC from IEEE Xplore.  Restrictions apply. 
358
if the probability distribution P over databases assigns zero
probability to a data point value di, we will never examine the
algorithm’s behavior for it. While the algorithm’s behavior on
zero-probability events may be of little practical concern, it
would allow the algorithm A to violate DP. (See Appendix B
for an example.) To remove this possibility, we quantify over
all probability distributions, which will include some with non-
zero probability for every combination of data points.
Alternately, we could have used just one distribution that
assigns non-zero probability to all possible input data points.
We instead quantify over all distributions to make it clear
that DP implies a property for all population distributions P.
While the population distribution P is needed to compute
the probabilities used by Deﬁnition 2 and will change the
probability of outcomes, whether or not A has DP does
not actually depend upon the distribution beyond whether
it assigns non-zero probability to data points. This lack of
dependence explains why DP is typically deﬁned without
reference to a population distribution P and typically only
mentions the algorithm’s randomization A.
For us, the population distribution P serves to link the
algorithm to the data on which it is used, explaining the
consequences of the algorithm for that population. Since the
concerns of Yang et al. and others deal with differential
privacy’s behavior on populations with correlated data points,
having this link proves useful. The following theorem shows
that its introduction does not alter the concept.
Proposition 1. Deﬁnitions 1 and 2 are equivalent.
Proof. Assume Deﬁnition 1 holds. Consider any population P,
index i, data points (cid:104)d1, ..., dn(cid:105) in Dn and d(cid:48)
i in D, and output
o such that the following holds: PrP [D1=d1, ..., Dn=dn] > 0
and PrP [D1=d1, ..., Di=d(cid:48)
i, ..., Dn=dn] > 0. Since Deﬁni-
tion 1 holds,
PrA[A((cid:104)d1, ..., dn(cid:105))=o] ≤ e PrA[A((cid:104)d1, ..., d(cid:48)
i, ..., dn(cid:105))=o]
Letting O = A(D) and D = (cid:104)D1, ..., Dn(cid:105), the above implies
PrP,A[O=o | D1=d1, ..., Dn=dn]
≤ e ∗ PrP,A[O=o | D1=d1, ..., Di=d(cid:48)
i, ..., Dn=dn]
Thus, Deﬁnition 2 holds.
is
i.i.d. and assigns non-zero probabilities
Assume Deﬁnition 2 holds. Let P be a population
that
to all
the sequences of n data points. Consider any index i,
in D, and out-
data points (cid:104)d1, ..., dn(cid:105) in Dn and d(cid:48)
put o. P is such that PrP [D1=d1, ..., Dn=dn] > 0 and
PrP [D1=d1, ..., Di=d(cid:48)
i, ..., Dn=dn] > 0 both hold. Thus,
since Deﬁnition 2 holds for P,
i
PrP,A[O=o | D1=d1, ..., Dn=dn]
≤ e ∗ PrP,A[O=o | D1=d1, ..., Di=d(cid:48)
i, ..., Dn=dn]
where O = A(D) and D = (cid:104)D1, ..., Dn(cid:105). Thus,
PrA[A((cid:104)d1, ..., dn(cid:105))=o] ≤ e PrA[A((cid:104)d1, ..., di, ..., dn(cid:105))=o]
Thus, Deﬁnition 1 holds.
The standard intuition provided for the formulation of
differential privacy found in Deﬁnition 2 is a Bayesian one
in which we think of P as being prior information held by
an adversary trying to learn about Di. We condition upon and
ﬁx all the values of D1, ..., Dn except Di to model a “strong
adversary” that knows every data point except Di, whose value
varies in (3). As the value of Di varies, we compare the
probabilities of output values o. These probabilities can be
thought of as measuring what the adversary knows about Di
given all the other data points. The bigger the change in the
probabilities as the value of Di varies, the bigger the ﬂow of
information from Di to O.
The origins of this characterization of DP go back to
the original work of Dwork et al., who instead call strong
adversaries “informed adversaries” [17, App. A]. However,
their characterization is somewhat different than what is now
viewed as the strong adversary characterization. This new
characterization has since shown up in numerous places. For
example, Alvim and Andrés rewrite DP this way [1, p. 5] while
Yang et al. [49, Def. 4] and Cuff and Yu [8, Def. 1] even deﬁne
it thus.
Despite this intuition, there’s no mathematical requirement
that we interpret the probabilities in terms of an adversary’s
Bayesian beliefs and we could instead treat them as frequen-
cies over some population. In Section VII, we return to this
issue where we explicitly mix the two interpretations. Either
way, we term Deﬁnition 2 to be an associative characterization
of DP since (3) compares probabilities that differ in the value
of Di that is conditioned upon.
While it may seem intuitive that ensuring privacy against
such a “strong” adversary would imply privacy against other
“weaker” adversaries that know less, it turns out that the
name is misleading. Suppose we measure privacy in terms
of the association between Di and O, which captures what an
adversary learns, as in (3). Depending upon the circumstances,
either a more informed “stronger” adversary or a less informed
“weaker” adversary will learn more from a data release [7],
[27]. Intuitively, if the released data is esoteric information and
only the informed adversary has enough context to make use
of it, it will learn more. If, on the other hand, the released data
is more basic information relating something that the informed
adversary already knows but the uninformed one does not, then
the “weaker” uninformed one will learn more.
One way to make this issue more precise is to model how
informed an adversary is by the number of data points it
knows, that is, the number conditioned upon. This leads to
Yang et al.’s deﬁnition of Bayesian Differential Privacy [49,
Def. 5]. Despite the name, its probabilities can be interpreted
either as Bayesian credences or as frequencies. For simplicity,
we state their deﬁnition for just the extreme case where the
adversary knows zero data points:
Deﬁnition 3 (Bayesian0 Differential Privacy). A randomized
algorithm A is -Bayesian0 differentially private if for all
population distributions P, for all i, for all data points di
i in D, and for all output values o, if PrP [Di=di] > 0
and d(cid:48)
and PrP [Di=d(cid:48)
i] > 0 then
PrP,A[O=o | Di=di] ≤ e ∗ PrP,A[O=o | Di=d(cid:48)
i]
(4)
where O = A(D) and D = (cid:104)D1, ..., Dn(cid:105).
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:36:11 UTC from IEEE Xplore.  Restrictions apply. 
359
Differential Privacy
Strong Adversary D.P.
Strong adversary
Independent Bayes. D.P.
Bayesian D.P.
Independent data points
Fig. 2. Relationships between Differential Privacy and Associative Charac-
terizations of It. Arrows show implications. Curved, labeled arrows show, in
italics, assumptions required for the implication. For differential privacy to
imply Bayesian Differential Privacy, one of two assumptions must be made.
One might expect that DP would provide Bayesian Dif-
ferential Privacy from hearing informal descriptions of them.
However, Yang et al. prove that Bayesian Differential Privacy
implies DP but is strictly stronger [49, Thm. 2]. Indeed, it was
already known that limiting the association between Di and
the output O requires limiting the associations between Di
and the other data points [7], [27]. Doing so, Yang et al.
proved that DP implies Bayesian Differential Privacy under
the assumption that the data points are independent of one
another [49, Thm. 1]. We state the resulting qualiﬁed form of
DP as follows:
Deﬁnition 4 (Independent Bayesian0 Differential Privacy). A
randomized algorithm A is -Bayesian0 differentially private
for independent data points if for all population distributions
P such that for all i and j where i (cid:54)= j, Di is independent
of Dj conditioned upon the other data points the following
i in D, and for all output
holds: for all data points di and d(cid:48)
values o, if PrP [Di=di] > 0 and PrP [Di=d(cid:48)
i] > 0 then
PrP,A[O=o | Di=di] ≤ e ∗ PrP,A[O=o | Di=d(cid:48)
i]
(5)
where O = A(D) and D = (cid:104)D1, ..., Dn(cid:105).
On all the above math, everyone is in agreement, which we
summarize in Figure 2 and below:
(a) Differential privacy and Strong Adversary Differential
Privacy are equivalent,
tial Privacy are equivalent,
(b) Differential privacy and Independent Bayesian Differen-
(c) Bayesian Differential Privacy and related associative
properties are strictly stronger than Differential Privacy,
(d) If we limit ourselves to strong adversaries, DP and
Bayesian Differential Privacy become equivalent, and
(e) If we limit ourselves to independent data points, DP and
Bayesian Differential Privacy become equivalent.
More controversially, some papers have pointed to these facts
to say that DP makes implicit assumptions. Some have taken
(d) to imply that DP has an implicit assumption of a strong
adversary. For example, Cuff and Yu’s paper states [8, p. 2]:
The deﬁnition of (, δ)-DP involves a notion of
neighboring database instances. Upon examination,
one realizes that this has the effect of assuming that
the adversary has already learned about all but one
entry in the database and is only trying to gather
additional information about the remaining entry.
We refer to this as the strong adversary assumption,
which is implicit in the deﬁnition of differential
privacy.
Others have focused on (e) and independent data points. For
example, Liu et al.’s paper asserts [33, p. 1]:
To provide its guarantees, DP mechanisms assume
that the data tuples (or records) in the database, each
from a different user, are all independent.
Appendix A3 provides more examples.
Those promoting the original view of DP have re-asserted
that DP was never intended to prevent all associative, or
inferential, privacy threats and that doing so is impossible [3],
[25], [36], [35]. However, this assertion raises the question: if
DP is not providing some form of association-based inferential
privacy, what is it providing?
IV. A PRIMER ON CAUSATION
We believe that the right way of thinking about DP is that it
is providing a causal guarantee. Before justifying this claim,
we will review a framework for precisely reasoning about
causation based upon Pearl’s [42]. We choose Pearl’s since
it is the most well known in computer science, but our results
can be translated into other frameworks.
To explain causation,
let us return to the example of
Section I-A. Suppose that the statistic being computed is the
number of data points showing the genetic disease. A possible
implementation of such a differentially private count algorithm
A for a ﬁxed number of three data points is
def progA(D1, D2, D3) :
D := (cid:104)D1, D2, D3(cid:105)
3(cid:88)
O := Lap(1/) +
(1 if D[i] == pos else 0)
i=1
It takes in 3 data points as inputs, representing the statuses
reported by survey participants. It stores them in a database
D and then uses the Laplace Mechanism to provide a differ-
entially private count of the number of data points recording
the status as positive (pos) [17, Example 1].
One could use a tool like the GNU Project Debugger (GDB)
to check the value of a variable as the program executes. We
can think of this as making an observation. If you observed that
D[3] is negative (neg), you would know that D3 and the third
input were also neg. In a probabilistic setting, conditioning
would carry out this update in knowledge.
One could also use GDB to intervene on the program’s ex-
ecution and alter D[3] to be pos. This would probabilistically
increase the output’s value. But would one learn from this
that D3 is pos and no longer neg? No, since the program
uses assignments and not equalities to shift the value of the
right-hand-side variable into the left-hand-side variable. D3 is
a (partial) cause of D, but not the other way around. Altering
the value of D[3] only affects variables that it assigns a value
to, those they assign values to, and so forth, that is, the ones
it causes. In this example, that is only O. This reﬂects the
difference between association and causation.
More formally, to develop a causal interpretation of DP, we
start by replacing the equation O = A(D) with a stronger
claim. Such equations say nothing about why this relation
holds. We use a stronger causal relation asserting that the
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:36:11 UTC from IEEE Xplore.  Restrictions apply. 
360
value of the output O is caused by the value of the input
D, that is, we use a structural equation. We will denote this
structural equation by O := A(D) since it is closer to an
assignment than equality due to its directionality. To make
this more precise, let do(D=d) denote an intervention setting
the value of D to d (Pearl’s do notation [42]). Using this
notation, Pr[O=o | do(D=d)] represents what the probability
of O = o would be if the value of D were set to d by
intervention. Similar to normal conditioning on D = d,
Pr[O=o | do(D=d)] might not equal Pr[O=o]. However,
Pr[D=d | do(O=o)] will surely equal Pr[D=d] since O is
downstream of D, and, thus, changing O has no effects on D.
Similarly, we replace D = (cid:104)D1, D2, D3(cid:105) with D :=
(cid:104)D1, D2, D3(cid:105). That is, we consider the value of the whole
database to be caused by the values of the data points and
nothing more. Furthermore, we require that D1, D2, D3 only
cause D and do not have any other effects. In particular, we do
not allow Di to affect Dj for i (cid:54)= j. Looking at our example
program progA, this is the case.
This requirement might seem to prevent one person’s at-