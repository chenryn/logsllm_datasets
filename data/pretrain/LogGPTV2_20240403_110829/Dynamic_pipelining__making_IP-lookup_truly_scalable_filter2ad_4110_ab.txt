ers and preﬁxes, and consequently increases the total memory space.
The size of the routing-table in Figure 2(b) is twice that of the table
in Figure 2(a) for the same original number of preﬁxes. If the under-
lying 1-bit trie is sparse, controlled preﬁx expansion will inadvert-
ently inﬂate the data structure’s size. Striding multiple bits also
aggravates the route-update cost in leaf-pushed tries.
In the example we have presented, each internal node has the
same stride. However, using the same stride is neither necessary nor
optimal in terms of storage space or lookup delay. Variable stride
tries and Level Compressed (LC) tries [11] determine the stride at
each node in accordance with whether the trie is sparse or dense at
that particular location. In contrast, compression schemes like Lulea
[3] and Tree Bitmap [4] maintain a ﬁxed stride trie and compress
away the redundant replication instead. Tree Bitmap may be addi-
tionally extended to support variable strides. However, for a worst-
case preﬁx distribution, variable striding and compression-based
schemes do not beneﬁt the total memory size much, as we discuss in
Section 6.
2.3 The Need for Pipelined Tries
Tomorrow’s routers will have to perform IP-lookups into routing
tables of hundreds of thousands of preﬁxes, at the rate of a few nano-
seconds per lookup. With such a large number of preﬁxes, trie-based
schemes require such a large amount of worst-case memory that per-
forming even one memory access may take longer than the packet
inter-arrival time. The problem is aggravated by the fact that trie-
based schemes perform multiple memory accesses for one lookup.
To meet the demand for high lookup rates under such constraints, we
obviously need to pipeline IP lookup so that performing multiple
lookups in parallel delivers a net lookup rate that meets the demand.
3 Pipelined and Scalable IP-Lookup
The observation that pipelining can be used to solve the scalabil-
ity problem of IP-lookup is not new. Previous proposals [15] [1]
have addressed this problem with some form of pipelining. [15] can
be thought of as a hardware-level pipelined (HLP) scheme, whereas
[1] can be thought of as a data-structure-level pipelined (DLP)
scheme.
r
e
d
o
c
e
D
Memory
Array
Access
M
u
l
t
i
p
l
e
x
Fig. 3.  The hardware steps involved in a memory access
We describe these schemes to explain why they do not scale well
and then explain our scheme and how it scales.
N
log2N
W - log2N
3.1 Hardware-Level Pipelining
We can view the IP-lookup process as k memory accesses, where
k is the number of levels in the multi-bit trie. For a given line-rate we
know the required IP-lookup rate, say one lookup every t seconds.
For the given number of preﬁxes we can determine the total memory
required by the trie, and hence d, the total delay of one memory
access. In order to meet the demanded lookup rate, HLP [15] hard-
ware-level pipelines the entire memory holding the trie into k*d/t
stages.
Figure 3 shows, at a high level, the hardware steps involved in
accessing a memory. Memory is typically organized as a two-dimen-
sional array. The decode step uses x higher-order bits of the x+y-bit
memory address to identify which of the 2x rows is being accessed.
The memory array access step performs the actual access of the cho-
sen memory row. And the multiplex step selects the desired word
from the 2y words in the row and feeds it to the output. To optimize
access times, circuit designers subdivide the memory array into
many subarrays. Using the subarrays reduces memory (sub)array
access time but increases decode and multiplex times for an overall
reduction in access time. The decode and multiplex steps essentially
look like decision trees and they can be pipelined into smaller stages
by splicing up these trees. The memory-array-access step consists of
reading from (writing to) the memory cells to (from) bitline wires.
Because designing the bitline wires to carry multiple values is hard,
for all practical purposes this step is atomic and cannot be pipelined.
Therefore, even if decode and multiplex steps are pipelined into
many fast stages, the throughput would be limited by the delay of
the memory-array-access step. The time taken to perform this
atomic step is proportional to the size of the memory array.
To reduce the delay of the atomic step, HLP [15] aggressively
divides the memory array into a larger number of smaller subarrays.
Such division does not come for free, however. It makes the decode
and multiplex complicated, and does not scale well in terms of
power dissipation and implementation cost. As we show in the
experimental evaluation, such aggressive pipelining leads to prohibi-
tive chip area (implying high implementation cost) and power dissi-
pation. Therefore, HLP is not a scalable solution.
3.2 Data-Structure-Level Pipelining
We have seen that if the entire k-level trie resides in one large
memory, then the bandwidth demanded by that memory is k times
the lookup rate needed. To solve this problem, DLP [1] places each
level of the trie in a different memory, so that each memory is
accessed only once per packet lookup. Therefore, the bandwidth that
each memory must individually supply does not incur the factor-of-k
multiplier.
Because DLP partitions the trie data-structure such that each
Fig. 4.  The 1-bit trie corresponding to the worst-case prefix
distribution. N is the number of prefixes, W is the length of an IP-
address in bits
level is placed in a separate memory stage, DLP can overlap the
lookups for multiple packets by accessing different levels (in differ-
ent memories) for different packets at the same time. Thus, DLP dis-
sociates the lookup rate from the total delay of one lookup. Because
DLP does not rely on expensive memory technologies or deep hard-
ware pipelining, it scales well in power and implementation cost.
There are, however,
three remaining challenges that must be
addressed in order to make DLP truly scalable, namely: scalability
in memory size, in route-update cost, and in lookup throughput.
3.2.1 DLP’s Scalability Problems in Memory Size
The total memory requirement of DLP is the sum of the memory
size of each stage. In order to provide worst-case guarantees, the
space provided at each memory stage should be sufﬁcient for any
preﬁx distribution. Because DLP assigns each node to a stage based
on which trie level the node belongs to (i.e., which bit each node
examines), the worst-case per-stage memory size is determined by
the worst-case node count per level of the trie. It is important to note
that the well-known average-case properties of randomly-built trees
are not relevant here because we are concerned with worst-case
guarantees.
In the ensuing analysis of worst-case per-stage memory size for
DLP, we consider a 1-bit trie for simplicity. Striding multiple bits
causes inﬂation in memory size due to controlled preﬁx expansion,
and will not lower the worst-case bound. As such, our bound applies
to multibit tries as well.
Imagine a preﬁx distribution in which all N preﬁxes have length
W (W being the length of an IP-address), and the ﬁrst log2N bits of
each preﬁx are unique (i.e., the preﬁxes cover all N values that the
ﬁrst log2N bits can take). The 1-bit trie corresponding to this preﬁx
distribution is shown in Figure 4. To establish that this trie, indeed,
represents the worst-case memory requirement at each level, we
make two observations: (1) In a 1-bit trie, each node can have two
children, therefore no preﬁx distribution can have more than 2k
nodes at the kth level. (2) There are only N preﬁxes therefore no level
can have more than N nodes. Thus we see that the 1-bit trie in
Figure 4 does in fact represent the worst-case memory requirement
per level. Accounting only for the rectangular bottom-half of the trie
in Figure 4, we see that the total memory required by this trie is
greater than N * (W-log2N) nodes. It is important to note that the
rectangular part is not due to leaf-pushing, rather it is a result of the
speciﬁc preﬁx distribution. For a million preﬁxes DLP’s memory
requirement exceeds 80 MB, in contrast the storage needed for just
the preﬁxes is only 6 MB, illustrating DLP’s scalability problem in
memory size. Note that, though a variable-stride trie may reduce
total space in the average case, for the preﬁx distribution shown in
Figure 4 its worst-case memory size would be no better.
3.2.2 DLP’s Scalability Problems in Route-update Cost
Because DLP uses a multibit trie with leaf pushing, a single
route-update may affect an entire subtree which has arbitrarily many
nodes. [1] proposes a number of optimizations for applying fast
incremental route-updates in a pipelined fashion. However, all the
optimizations are heuristics which improve only the average-case
route-update cost. The worst-case route-update cost of DLP remains
unbounded even with the optimizations.
Techniques like Tree Bitmap [4] can be used to achieve an O(1)
bound on the route-update cost. By avoiding leaf-pushing Tree Bit-
map ensures that an update needs to modify only one trie node
achieving the O(1) bound. However, because Tree Bitmap cannot
use leaf-pushing, it almost doubles the size of each trie node (see
Section 2.1). [4] explains an implementation to avoid the doubling
of the node size, where only the pointers are stored in the nodes and
the preﬁxes are stored in a parallel copy of the trie. Obviously, the
second copy must also be maintained in fast memory (as it must be
accessed at IP-lookup rates), almost doubling the total memory size.
Further, the trie nodes in Tree Bitmap have variable sizes due to
variation in strides and compressions. Route-updates result
in
repeated allocations and deallocations of such variable sizes, caus-
ing fragmentation and under-utilization of memory. This fragmenta-
tion necessitates a complex memory management scheme for
compaction [4][14], which must be invoked whenever memory for a
new node is allocated. The memory accesses for the compaction
appear as an overhead in the route-update cost. We found that the
worst-case memory management overhead of Tree Bitmap [4]
exceeds 100 memory accesses for a single route-update. ([4] reports
1852 memory accesses based on an analysis which is more conser-
vative than ours.) Though a pipelined update scheme such as [1]
could be leveraged to reduce the effective compaction cost, such
reduction would be sensitive to the distribution of the memory
accesses across the pipeline stages. In the worst case there may be
no reduction at all. Hence, we see that previous schemes do not scale
well in worst-case route-update cost.
3.2.3 DLP’s Non-Scalability in Throughput
Because the pipeline’s throughput is limited by the slowest stage
DLP proposes a dynamic programming algorithm to minimize the
size of the largest stage. This algorithm takes as inputs a preﬁx dis-
tribution and the number of levels in a ﬁxed-stride trie, and returns
the strides for each level such that the size of the largest level is min-
imized. The size of the largest stage can be lowered by increasing
the number of levels in the trie (i.e., reducing the stride at each
level). In the limit, even if each level strides only one bit, there can
be only as many levels as bits in an IP-address (32). With 1 million
preﬁxes, the 1-bit trie shown in Figure 4, has a largest memory stage
of 5 MB which, realistically speaking, may not be accessed faster
than 6 ns or so. When the demanded lookup-rate exceeds this limit,
DLP does not work. For truly scalable throughput the depth of the
pipeline should not be limited by the number of bits in an IP-address
(32).
3.3 Scalable Dynamic Pipelining
To address the problems of IP-lookup scalability, we propose
scalable dynamic pipelining (SDP). We begin by taking a closer
look at why traditional tries have such a large worst-case memory
P4
P4
Y
Jump 010
M
atc
h
P5
(a)
0*
00*
000*
1*
1010*
P1
P2
P3
P4
P5
(c)
X
Y
X
Y
P1
P2
P4
P5
(b)
P3
(d)
P2
P1
P4
P4
P1
P5
P4
P3
P2
P4
M ism atch
Fig. 5. (a) A table of prefixes (b) The corresponding 1-bit trie (c)
The 1-bit trie with P3 deleted (d) The trie with jump nodes
requirement. We observe that previous schemes like [1] pipeline the
trie by mapping a speciﬁc level of the trie to a speciﬁc stage. This
mapping is strictly static and oblivious to the preﬁx distribution.
Consider, for example, the set of preﬁxes shown in Figure 5 (a), and
the corresponding 1-bit trie in Figure 5 (b). The node labelled X is in
the second level of the trie and hence placed in the second stage of
the pipeline. Imagine that we remove preﬁx P3 from the table, the
resulting trie is shown in Figure 5 (c). Even though the structure and
the memory requirements of the subtree rooted at X have changed
signiﬁcantly, X remains mapped to the second stage of the pipeline,
oblivious of this change.
We make the key observation that while the level of X does not
change, the height of X does change in response to the new preﬁx
distribution (height of leaves being zero). This dichotomy exists
because the height is measured from the leaves whose positions
reﬂect the distribution, whereas the level is measured from the root
whose position remains ﬁxed. Speciﬁcally, the height of X is 2 in
Figure 5 (b) and becomes 1 in Figure 5 (c). We see that the height of
X is correlated to the number of preﬁxes in the subtree rooted at X.
Because the node height is directly determined by the preﬁx distri-
bution, it succinctly provides information regarding the distribution
which is sufﬁcient for achieving a tight worst-case bound on mem-
ory. However, there is one peculiar feature of tries which can distort
the correlation between node height and preﬁx distribution. We ﬁrst
turn our attention to this distorting feature before presenting an anal-
ysis of worst-case per-stage memory size for SDP.
3.3.1 Jump Nodes
The way a trie is constructed, an internal node that strides k bits
must have an array of 2k pointers, one for each possible child. Often
there may be only one child and the remaining pointers are null (leaf
pushing may eventually insert a longest matching leaf in place of
such nulls). In Figure 5 (a) and (b) the preﬁx distribution is such that
it results in a long string of one-child nodes (ignoring the leaf-
pushed copies of P4). The height of node Y is 3, though the number
of unique preﬁxes in the subtree rooted at Y is just 2. The presence
of the string of one-child nodes artiﬁcially increases the height of Y.
Because the correlation becomes distorted in such a case, the height
of Y does not faithfully inform us about the underlying preﬁx distri-
bution.
To address this problem we collapse strings of one-child nodes
into a single jump node. We call it a jump node because it allows the
lookup to jump over the string of one-child nodes. Jump nodes are
similar to skip nodes in [4] and can be thought of as an adaptation of
log2N
N
(b)
N
(a)
log2N
W - log2N
(c)
Fig. 6. (a) A binary search tree with N leaves (b) memory size of a trie with jump-nodes for the worst-case prefix distribution of Figure 4,
compared to size of 1-bit trie (c) The space taken at various levels by a trie with jump-nodes, for various prefix distributions
path compression in PATRICIA tries [9]. A jump node collapses
strings using loss-less compression by storing and matching all the
jump bits. In contrast, PATRICIA tries use a form of lossy compres-
sion that examines only the ﬁrst bit out of the string being com-
pressed.
In addition to pointers, a jump node also stores the jump bits, the
string of bits corresponding to the path collapsed. A jump node need
store only two pointers, one for the path that matches the jump bits,
and the other for a mismatch. Because any node may be a jump
node, the default size for every SDP node must budget for the jump
bits in addition to two pointers. Because a string of one-child nodes
may have any arbitrary length, each SDP node must budget for the
maximum number of jump bits possible (i.e., 32). Although the
space for jump bits causes an increase in the overall size of every
node, we show in Section 6.2 that the drastic reduction in the num-
ber of worst-case per-stage nodes (due to dynamic pipelining), dom-
inates this increase to result in a much lower worst-case total
memory bound compared to a statically pipelined trie. The increase
in trie-node size also increases the bandwidth demanded from the
memory. However, because 1-bit trie nodes are small compared to
multibit trie nodes, the eventual bandwidth demand stays relatively
small.
Figure 5(d) shows the trie of Figure 5(b) after the string of one-
child nodes has been collapsed into a single jump node. In
Figure 5(d) the height of Y has been reduced to 1 which is correlated
to the number of preﬁxes in the subtree rooted at Y. Thus, jump
nodes remove the artiﬁcial increase in height due to strings of one-