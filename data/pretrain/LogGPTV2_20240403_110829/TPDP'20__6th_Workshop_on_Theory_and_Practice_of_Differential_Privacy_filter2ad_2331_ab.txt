t=0
t
t
t
Arbitrary Queries: Sometimes we will not impose any structure on the data universe X or query
family Q except possibly to restrict attention to families of eﬃciently computable queries.
For the latter, we encode elements of both X and Q as strings, so X = {0, 1}d, Q = {qy :
X → {0, 1}}y∈{0,1}s for some s, d ∈ N, where qy(w) = Eval(y, w) for some polynomial-time
evaluation function Eval : {0, 1}s × {0, 1}d → {0, 1}.
1.4 Diﬀerential Privacy
The deﬁnition of diﬀerential privacy requires that no individual’s data has much eﬀect on what an
adversary sees. That is, if we consider any two datasets x and x(cid:48) that diﬀer on one row (which
we will denote x ∼ x(cid:48)), the output distribution of M on x should be “similar” to that of M on x(cid:48).
Speciﬁcally, we require that:
∀T ⊆ Y, Pr[M(x, q) ∈ T ] ≤ (1 + ε) · Pr[M(x, q) ∈ T ] .
The reverse relationship (Pr[M(x(cid:48), q) ∈ T ] ≤ (1 + ε) · Pr[M(x, q) ∈ T ]) follows by symmetry,
swapping x and x(cid:48). The choice of a multiplicative measure of closeness between distributions is
5
important, and we will discuss the reasons for it later. It is technically more convenient to use eε
instead of (1 + ε), because the former behaves more nicely under multiplication (eε1 · eε2 = eε1+ε2).
This gives the following formal deﬁnition:
Deﬁnition 1.1 ((pure) diﬀerential privacy [38]). For ε ≥ 0, we say that a randomized mechanism
M : Xn × Q → Y is ε-diﬀerentially private if for every pair of neighboring datasets x ∼ x(cid:48) ∈ Xn (i.e.,
x and x(cid:48) diﬀer in one row), and every query q ∈ Q, we have:
∀T ⊆ Y, Pr[M(x, q) ∈ T ] ≤ eε · Pr[M(x(cid:48), q) ∈ T ] .
Equivalently,
∀y ∈ Y, Pr[M(x, q) = y] ≤ eε · Pr[M(x(cid:48), q) = y] .
Here we typically take ε as small, but non-negligible (not cryptographically small); for example,
a small constant, such as ε = 0.1. Smaller ε provides better privacy, but as we will see, the deﬁnition
is no longer useful when ε  σt] ≤ e−t .
The Laplace mechanism is not speciﬁc to counting queries; all we used was that |q(x)− q(x(cid:48))| ≤
1/n for x ∼ x(cid:48). For an arbitrary query q : Xn → R, we need to scale the noise to its global sensitivity:
Then we have:
GSq = max
x∼x(cid:48) |q(x) − q(x(cid:48))|.
7
Deﬁnition 1.2 (the Laplace mechanism). For a query q : Xn → R, a bound B, and ε > 0, the
Laplace Mechanism Mq,B over data universe X takes a dataset x ∈ Xn and outputs
Mq,B(x) = q(x) + Lap(B/ε).
From the discussion above, we have:
Theorem 1.3 (properties of the Laplace mechanism).
1. If B ≥ GSq, the Laplace mechanism Mq,B is ε-diﬀerentially private.
2. For every x ∈ Xn and β > 0,
Pr[|Mq,B(x) − q(x)| > (B/ε) · ln(1/β)] ≤ β.
As noted above, for a counting query q, we can take B = 1/n, and thus with high probability we
n) given by randomized
√
get error O(1/(εn)), which is signiﬁcantly better than the bound of O(1/ε
response.
Global sensitivity is also small for a variety of other queries of interest:
1. For q(x) = max{q1(x), q2(x), . . . , qt(x)}, we have GSq ≤ maxi{GSqi}.
2. For q(x) = d(x, H) where H ⊆ Xn and d is Hamming distance,1 we have GSq ≤ 1. (“is my
data set close to one that satisﬁes my hypothesis H?”).
3. A statistical query (sometimes called a linear query in the diﬀerential privacy literature) is a
generalization of a counting query to averaging a real-valued function on the dataset. That
is, we are given a bounded function q : X → [0, 1], and are interested in the query:
q(x) =
1
n
Then GSq ≤ 1/n.
q(xi) .
n(cid:88)
i=1
We promised that we would only work with discrete probability, but the Laplace distribution
is continuous. However, one can discretize both the query values q(x) and the Laplace distribution
to integer multiples of B (yielding a scaled version of a geometric distribution) and Theorem 1.3
will still hold. We ignore this issue in the rest of the tutorial for the sake of simplicity (and for the
consistency with the literature, which typically refers to the continuous Laplace distribution).
1.6 Discussion of the Deﬁnition
We now discuss why diﬀerential privacy utilizes a multiplicative measure of similarity between the
probability distributions M(x) and M(x(cid:48)).
1The Hamming distance d(x, x(cid:48)) between two datasets x, x(cid:48) ∈ Xn is the number of rows on which x and x(cid:48) diﬀer.
8
Why not statistical distance? The ﬁrst choice that one might try is to use statistical diﬀerence
(total variation distance). That is, we require that for every x ∼ x(cid:48), we have:
SD(M(x), M(x(cid:48))) def= max
T⊆Y
(cid:12)(cid:12)Pr[M(x) ∈ T ] − Pr[M(x(cid:48)) ∈ T ](cid:12)(cid:12) ≤ δ.
ε-diﬀerential privacy implies the above deﬁnition with δ = 1 − e−ε ≤ ε, but not conversely.
We claim that, depending on the setting of δ, such a deﬁnition either does not allow for any
useful computations or does not provide suﬃcient privacy protections.
δ ≤ 1/2n: Then by a hybrid argument, for all pairs of datasets x, x(cid:48) ∈ Xn (even non-neighbors),
we have SD(M(x), M(x(cid:48))) ≤ nδ ≤ 1/2. Taking x(cid:48) to be a ﬁxed (eg all-zeroes) dataset, this
means that with probability 1/2 on M(x), we get an answer independent of the dataset x and
the mechanism is useless.
δ ≥ 1/2n: In this case, the mechanism “with probability 1/2, output a random row of the dataset”
satisﬁes the deﬁnition. We do not consider a mechanism that outputs an individual’s data in
the clear to be protecting privacy.
However, it turns out to be quite useful to consider the following relaxation of diﬀerential
privacy, which incorporates a negligible statistical distance term δ in addition to the multiplicative
ε.
Deﬁnition 1.4 ((approximate) diﬀerential privacy). For ε ≥ 0, δ ∈ [0, 1], we say that a randomized
mechanism M : Xn × Q → Y is (ε, δ)-diﬀerentially private if for every two neighboring datasets
x ∼ x(cid:48) ∈ Xn (x and x(cid:48) diﬀer in one row), and every query q ∈ Q, we have:
∀T ⊆ Y, Pr[M(x, q) ∈ T ] ≤ eε · Pr[M(x(cid:48), q) ∈ T ] + δ .
(1)
Here, we will insist that δ is cryptographically negligible (in particular, δ ≤ n−ω(1)); it can be
interpreted as an upper-bound on the probability of catastrophic failure (e.g. the entire dataset
being published in the clear). This notion is often called approximate diﬀerential privacy, in contrast
with pure diﬀerential privacy as given by Deﬁnition 1.1. Note that, unlike pure diﬀerential privacy,
with approximate diﬀerential privacy it is not suﬃcient to verify Inequality (1) for sets T of size
1.
(Consider a mechanism that outputs the entire dataset along with a random number from
{1, . . . ,(cid:100)1/δ(cid:101)}; then Pr[M(x, q) = y] ≤ δ ≤ eε · Pr[M(x(cid:48), q) = y] + δ for all y, but clearly does not
provide any kind of privacy or satisfy Deﬁnition 1.4.)
More generally, we will call two random variables Y and Y (cid:48) taking values in Y (ε, δ)-indistinguishable
if:
∀T ⊆ Y, Pr[Y ∈ T ] ≤ eε · Pr[Y (cid:48) ∈ T ] + δ.
Setting ε = 0 is equivalent to requiring that SD(Y, Y (cid:48)) ≤ δ. (ε, δ)-indistinguishability has the fol-
lowing nice characterization, which allows us to interpret (ε, δ)-diﬀerential privacy as “ε-diﬀerential
privacy with probability at least 1 − δ.”
Lemma 1.5 (approximate DP as smoothed2 DP [19]). Two random variables Y and Y (cid:48) are (ε, δ)-
indistinguishable if and only if there are events E = E(Y ) and E(cid:48) = E(cid:48)(Y (cid:48)) such that:
2The terminology “smoothed” comes terminology coined by [90] for similar variants of entropy measures.
9
1. Pr[E], Pr[E(cid:48)] ≥ 1 − δ, and
2. Y |E and Y (cid:48)|E(cid:48) are (ε, 0)-indistinguishable.
Proof. We prove the “if” direction, and omit the converse (which is rather technical). For every
set T , we have:
Pr[Y ∈ T ] ≤ Pr[Y ∈ T|E] · Pr[E] + Pr[E]
≤ Pr[Y ∈ T|E] · (1 − δ) + δ
≤ eε · Pr[Y (cid:48) ∈ T|E(cid:48)] · (1 − δ) + δ
≤ eε · Pr[Y (cid:48) ∈ T|E(cid:48)] · Pr[E(cid:48)] + δ
≤ eε · Pr[Y (cid:48) ∈ T ] + δ
A Bayesian Interpretation. Although statistical distance is not a good choice (on its own),
there are many other choices of distance measures, and we still have not justiﬁed why a multiplica-
tive measure is a particularly good choice. One justiﬁcation comes from a Bayesian interpretation
of the deﬁnition of diﬀerential privacy [38, 32, 64]. Consider a prior distribution (X, X(cid:48)) on neigh-
boring datasets, modelling an adversary’s prior on a real dataset X and a dataset X(cid:48) that would
have been obtained if a particular individual had not participated. Given an output y ← M(X),
the adversary will have a posterior belief on the dataset, given by the conditional distribution
X|M(X)=y. We will argue that diﬀerential privacy implies that this posterior is close to the poste-
rior that would have been obtained if the mechanism had been run on X(cid:48) instead, which we think
of as capturing “ideal” privacy for the individual.
Proposition 1.6 (DP implies Bayesian privacy). Let M : Xn → Y be any ε-diﬀerentially private
mechanism and let (X, X(cid:48)) be any joint distribution on Xn × Xn such that Pr[X ∼ X(cid:48)] = 1. Then
for every dataset x ∈ Xn and output y ∈ Supp(M(X)) = Supp(M(X(cid:48))),3
SD(X|M(X)=y, X|M(X(cid:48))=y) ≤ 2ε.
A special case of the proposition is when we ﬁx X(cid:48) = x(cid:48) to be constant (so that there is
nothing to learn from X(cid:48)) and X = (Xi, x(cid:48)
−i) is varying only in the data of one individual. Then
the proposition says that in such a case (where the adversary knows all but the i’th row of the
dataset), the adversary’s posterior on Xi is close to its prior. Indeed,
SD(Xi|M(X)=y, Xi) = SD(Xi|M(X)=y, Xi|M(X(cid:48))=y(cid:48)) = SD(X|M(X)=y, X|M(X(cid:48))=y(cid:48)) ≤ 2ε.