### Literature Review and Evaluation of Security Techniques

#### References
- Bhatkar et al. (2003) [6]
- Kc et al. (2003) [37]
- Barrantes et al. (2005) [5]
- Bhatkar et al. (2005) [8]
- Kil et al. (2006) [38]
- Bhatkar et al. (2008) [7]
- De Sutter et al. (2009) [22]
- Williams et al. (2009) [67]
- Novark et al. (2010) [46]
- Jackson et al. (2011) [35]
- Wei et al. (2011) [66]
- Pappas et al. (2012) [47]
- Hiser et al. (2012) [29]
- Giuffrida et al. (2012) [27]
- Wartell et al. (2012) [64]
- Collberg et al. (2012) [15]
- Shioji et al. (2012) [59]
- Jackson et al. (2013) [34]
- Homescu et al. (2013a) [31]
- Coppens et al. (2013) [18]
- Gupta et al. (2013) [28]
- Davi et al. (2013) [21]
- Homescu et al. (2013b) [30]

#### Security Impact Evaluation
Each method to evaluate security impacts has its limitations, leading authors to use both abstract and concrete security evaluations. Table II illustrates how each implementation assesses the impact of their approach. A common assumption in all evaluations is that diversification effects remain hidden from attackers. However, Section VI-C highlights vulnerabilities that can disclose implementations, thereby undermining this assumption.

#### Performance Impact
The likelihood of a security technique being adopted is often inversely proportional to its performance overhead. Techniques with negligible performance impact, such as ASLR, DEP, and stack canaries, have been widely adopted. For other techniques to gain widespread adoption, their performance overhead must be below 5-10%, according to Szekeres et al. [61].

Different studies measure the performance cost of diversity using various benchmarks. The most popular benchmark is the SPEC CPU suite, typically the most recent version available (currently SPEC CPU 2006). When SPEC CPU is not suitable, other workloads like real-world applications (Apache, Linux command line utilities, Wine test suite) or other CPU benchmarks are used. Since many of the discussed techniques' implementations are not publicly available, we rely on self-reported performance numbers from the authors.

Table III summarizes the time and space costs of each technique, including the average impact on program running time, memory usage, and on-disk binary file size (when reported).

#### Pre-Distribution vs. Post-Distribution Approaches
- **Pre-Distribution Approaches**: Overheads generally range from 1% to 11%.
- **Post-Distribution Approaches**: Reported overheads vary more, typically ranging from 1% to 250%. Implementations of these approaches need to carefully manage overheads. (Note: Pappas et al. [47], Kc et al. [37], and Shioji [59] are considered outliers.)

While the benchmarking methodology varies, both pre-distribution and post-distribution approaches can achieve low runtime overheads [31], [21]. Post-distribution methods also show greater variability in binary size overheads, with small to moderate increases. Some post-distribution approaches increase runtime memory overheads by 5% to 37%.

Table III excludes ahead-of-time costs associated with diversification. For pre-distribution methods, software developers or distributors may bear the diversification costs. For post-distribution methods, end users contribute the computing resources during installation, loading, or running. The feasibility of on-device diversification on resource-constrained devices like mobile and embedded systems remains to be determined.

### Open Areas and Unsolved Challenges
While the security and performance implications of diversified software are well understood, several practical concerns and research gaps remain. Existing research has not fully explored the protective qualities of diversified software nor reached a consensus on evaluating its efficacy against attacker workloads. For example, it is believed that software diversity can provide probabilistic protection against covert channels.

#### Hybrid Approaches
A divide exists between proponents of compilation-based diversification and binary rewriting. Binary rewriting is often preferred because it does not require source code access, custom compilers, or changes to distribution mechanisms [67], [64], [29], [47]. However, binary rewriting is inherently client-side and cannot defend against tampering or piracy via watermarking [40]. A hybrid approach combining a decompiler that produces LLVM intermediate representation [3] with a compiler-based diversification engine can apply randomizing transformations to both source code and legacy binaries.

Another hybrid approach involves compiler-rewriter cooperation. Static binary rewriting of stripped binaries suffers from incomplete information. If the compiler (or linker) provides a map of all indirect branch targets, reliable disassembly can be achieved, simplifying the implementation of binary rewriters and improving their throughput. The resulting binaries run faster without the need for runtime error detection and correction.