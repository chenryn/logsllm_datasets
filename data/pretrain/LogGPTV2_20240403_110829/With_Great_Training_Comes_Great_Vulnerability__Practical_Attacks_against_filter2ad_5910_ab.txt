s, xs)−P, 0))2
Here λ is the penalty coefﬁcient that controls the tight-
ness of the privacy budget constraint. By gradually in-
creasing λ, the ﬁnal optimization result would converge
to that of the original formulation. In our experiment, we
empirically choose a λ large enough to ensure the per-
turbation constraint is tightly enforced.
We use Adadelta [69] optimizer to solve the re-
formulated optimization problem. To constrain input
pixel intensity within the correct range ([0, 255]), we
transform intensity values into tanh space [17].
4 Experimental Results
Next, we perform experiments across a number of clas-
siﬁcation tasks to validate the effectiveness of attacks on
transfer learning. Given their wide adoption in a variety
1284    27th USENIX Security Symposium
USENIX Association
of applications, we focus on image classiﬁcation tasks,
including facial recognition, iris recognition, trafﬁc sign
recognition and ﬂower recognition.
Recognition). Finally, Face, Iris and Trafﬁc sign recog-
nition are security-related tasks. More details of training
the Student models are listed in Table 2 in the Appendix.
4.1 Experimental Setup
Teacher and Student Models. We use four tasks and
their associated Teacher models and datasets to build our
victim Student models.
• Face Recognition classiﬁes an image of a human face
into a class associated with a unique individual. The
Teacher is the popular 16 layer VGG-Face model [49]
trained on a dataset of 2.6M images to recognize
2, 622 faces. The Student model is trained using the
PubFig dataset [8] to recognize a different set of 65
individuals1. The Student training dataset contains 90
faces belonging to each of the 65. The testing dataset
for the Student model contains 650 images (10 images
per class).
• Iris Recognition classiﬁes an image of a human iris
into one of many classes associated with different in-
dividuals. The Teacher model is a 16 layer VGG16
model trained on the ImageNet dataset of 1.3M im-
ages [56]. The Student model is trained on the CASIA
IRIS dataset [2] containing 16, 000 iris images asso-
ciated with 1, 000 individuals, and the testing dataset
contains 4, 000 images.
• Trafﬁc Sign Recognition classiﬁes different types
of trafﬁc signs from images, which can be used
by self-driving cars to automatically recognize traf-
ﬁc signs. The Teacher model is again the 16 layers
VGG16, trained on the ImageNet dataset. The Stu-
dent is trained using the GTSRB dataset [1] containing
39, 209 images of 43 different trafﬁc signs. It also has
a testing dataset of 12, 630 images.
• Flower Recognition classiﬁes images of ﬂowers into
different categories, and is a popular example of multi-
class classiﬁcation. It is also an example of transfer
learning by Microsoft’s CNTK framework [6]. The
Teacher model is the ResNet50 model (with 50 lay-
ers) [28], trained on the ImageNet dataset. The Stu-
dent is trained on the VGG Flowers dataset [9] con-
taining 6, 149 images from 102 classes, and comes
with a testing dataset of 1, 020 images.
These tasks represent typical scenarios users may face
during transfer learning. First, the training dataset for
building the Student model is signiﬁcantly smaller than
that of the Teacher’s training dataset, which is a common
scenario for transfer learning. Second, the Teacher and
Student models either target similar tasks (Face Recog-
nition) or very different tasks (Flowers and Trafﬁc Sign
1The original dataset contains 83 celebrities. We exclude 18 celebri-
ties that were also used in the Teacher model.
Optimizing Student Models.
We apply all three
transfer learning approaches (discussed in Section 2)
to each task, and identify the best approach. Table 1
shows the classiﬁcation accuracy for different transfer
approaches. For Mid-layer Feature Extractor, we show
the result of the best Student model by experimenting
with all possible K values. The results show that Face
Recognition achieves the highest accuracy (98.55%)
when using Deep-layer Feature Extractor. This is ex-
pected as the Student and Teacher tasks are very simi-
lar, leading to signiﬁcant gains from transferring knowl-
edge directly. The Flower classiﬁcation task performs
best with Full Model Fine-tuning, since the Student and
Teacher tasks are different and there is less gain from
sharing layers. Lastly, Trafﬁc Sign recognition is a nice
example for transferring knowledge from a middle layer
(layer 10 out of 16).
Based on these results, we build the Student model
for each task using the transfer method that achieves the
highest classiﬁcation accuracy (marked in bold in Ta-
ble 1). The resulting four Student models cover all three
transfer learning methods.
Attack Conﬁguration.
We craft adversarial sam-
ples using correctly classiﬁed images in the test dataset.
These are images not seen by the Student model during
its training and matches our attack model, i.e. the ad-
versary has no access to the Student training dataset. To
evaluate targeted attacks, we randomly sample 1K source
and target image pairs to craft adversarial samples, and
measure the attack success rate as the percentage of at-
tack attempts (out of 1K) that misclassify the perturbed
source image as the target. For non-targeted attacks, we
randomly select 1K source images and 5 target images
for each source (to guide the optimization process). Suc-
cess for non-targeted attack is measured as the percent-
age of 1K source images that are successfully misclassi-
ﬁed into any other arbitrary class.
For each source and target image pair, we compute the
adversarial samples by running the Adadelta optimizer
over 2, 000 iterations with a learning rate of 1. For all
the Teacher models considered in our experiments, the
entire optimization process for a single image pair takes
roughly 2 minutes on an NVIDIA Titan Xp GPU.
We implement the attack using Keras [19] and Ten-
sorFlow [12], leveraging open-source implementations
of misclassiﬁcation attacks provided by prior works [44,
17].
USENIX Association
27th USENIX Security Symposium    1285
Student Task
Face
Iris
Trafﬁc Sign
Flower
Deep-layer Feature Extractor Mid-layer Feature Extractor Full Model Fine-tuning
Transfer Process
98.55%
88.27%
62.51%
43.63%
98.00% (14/16)
88.22% (14/16)
96.16% (10/16)
92.45% (10/50)
75.85%
81.72%
94.39%
95.59%
Table 1: Transfer learning performance for different tasks when using different transfer processes. For each task, we
select the model with the highest accuracy as our target Student model in future analysis. Numbers in parenthesis
under Mid-layer Feature Extractor are the number of layers copied to achieve the corresponding accuracy, as well as
the total number of layers of the Teacher.
    Source   Adversarial    Target
    Source   Adversarial    Target
Figure 3: Examples of adversarial images on Face
Recognition (P = 0.003).
e
t
a
R
s
s
e
c
c
u
S
k
c
a
t
t
A
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
Non-targeted
Targeted
 0.001
 0.002
 0.003
 0.004
 0.005
Perturbation Budget (P)
Figure 4: Attack success rate on Face Recognition with
different perturbation budgets.
4.2 Effectiveness of the Attack
We ﬁrst evaluate the proposed attacks assuming the at-
tacker knows the exact transfer approach used to pro-
duce the Student model. This allows us to derive the
upper bounds on attack effectiveness, and to explore the
impact of the perturbation budget P, the distance met-
ric d(x′
s, xs), and the underlying transfer method used to
produce the Student model. Later in Section 4.3 we will
relax this assumption.
Consider the Face recognition task which uses Deep-
layer Feature Extractor to produce the Student model.
Here the attacker crafts adversarial samples to target the
N − 1 layer of the Teacher model. Even with a very low
perturbation budget of P = 0.003, our attack is highly
effective, achieving a success rate of 92.6% and 100%
for targeted, and non-targeted attacks respectively. We
also manually examine the perturbations added to adver-
sarial images and ﬁnd them to be undetectable by visual
inspection. Figure 3 includes 6 randomly selected suc-
cessful targeted attack samples for interested readers to
examine.
It should be noted that an attacker could improve at-
tack success by carefully selecting a source image simi-
lar to a target image. Our attack scenario is much more
challenging, since the source and target images are ran-
domly selected. Figure 3 shows that our attacks often try
to mimic a female actress using a male actor, and vice
versa. We also have image pairs with different lighting
conditions, facial expressions, hair color, and skin tones.
This signiﬁcantly increases the difﬁculty of the targeted
attack, given constraints on the perturbation level.
Impact of Perturbation Budget P. A natural question
is how to choose the right perturbation budget, which di-
rectly affects the stealthiness of the attack. By measuring
image distortion via the DSSIM metric, we empirically
ﬁnd that P = 0.003 is a safe threshold for facial images.
Its corresponding L2 norm value is 8.17, which is signif-
icantly smaller than/comparable to values used in prior
work (L2 > 20) [38].
Figure 4 shows the attack success rate as we vary the
perturbation budget between 0.0005 and 0.005. As ex-
pected, smaller budget results in lower attack success
rate, as there is less room for the attacker to change
images and mimic the internal representation. Detailed
comparison of images with different perturbation bud-
gets is in Figure 10 in the Appendix.
Impact of Distance Metric d(x′
s, xs).
Recall that we
use DSSIM to measure perturbation added to input im-
ages, instead of the Lp distance used by prior works, e.g.,
L2. To compare both metrics, we also implement our at-
tack using L2 distance, and analyze the generated images
ourselves. For a fair comparison, we choose an L2 dis-
tance budget that produces a targeted attack success rate
similar to using DSSIM with a budget of 0.003. Gener-
ated images are included in Figure 11 in the Appendix.
We ﬁnd that DSSIM generates imperceptible perturba-
tions, while perturbations using L2 are more noticeable.
While DSSIM takes into account the underlying struc-
ture of an image, L2 treats every pixel equally, and often
generates noticeable “tattoo-like” patterns on faces.
1286    27th USENIX Security Symposium
USENIX Association
t
e
a
R
s
s
e
c
c
u
S
k
c
a
t
t
A
 1
 0.8
 0.6
 0.4
 0.2
 0
1
Non-targeted
Targeted
 4
 8
 12
 16
Layer Number
t
e
a
R
s
s
e
c
c
u
S
k
c
a
t
t
A
 1
 0.8
 0.6
 0.4
 0.2
 0
1
Non-targeted
Targeted
 4
 8
 12
 16
Layer Number
t
e
a
R
s
s
e
c
c
u
S
k
c
a
t
t
A
 1
 0.8
 0.6
 0.4
 0.2
 0
1
Non-targeted
Targeted
 4
 8
 12
 16
Layer Number
t
e
a
R
s
s
e
c
c
u
S
k
c
a
t
t
A
 1
 0.8
 0.6
 0.4
 0.2
 0
1
Non-targeted
Targeted
 10
 20
 30
 40
 50
Layer Number
(a) Face
(b) Iris
(c) Trafﬁc Sign
(d) Flower
Figure 5: Targeted and non-targeted attack success rate on Student models when targeting different layers. X axis
indicates the layer being targeted. Face and Iris freeze the ﬁrst 15 layers during training; Trafﬁc Sign freezes the ﬁrst
10 layers; Flower freezes no layers.
Impact of Transfer Method. We also test out attack
on Iris, Trafﬁc Sign, and Flower recognition tasks. Their
perturbation budgets are set to 0.005 (L2=9.035), 0.01
(L2=7.77), and 0.003 (L2=13.52), respectively. These
values are empirically derived by the authors to produce
unnoticeable image perturbations.
Overall, the attack is effective in Iris, with a targeted
attack success rate of 95.9% and non-targeted success
rate of 100%. Like Face recognition, the Iris student
model was trained via Deep-layer Feature Extractor. On
the other hand, the attack becomes less effective on Traf-
ﬁc Sign recognition, where the success rate of targeted
and non-targeted attacks are 43.7%, and 95.35%, respec-
tively. For Flower recognition, these numbers reduce to
1.1% and 37.25%, respectively. These results suggest
that the attack effectiveness is strongly correlated with
the transfer method: our attack is highly effective for
Deep-layer Feature Extractor, but ineffective for Full
Model Fine-tuning.
4.3 Impact of the Attack Layer
We now consider scenarios where the attacker does not
know the exact transfer method used to train the Student
model. In this case, the attacker needs to ﬁrst select a
Teacher layer to attack, which can be different from the
deepest layer frozen during the transfer process. To un-
derstand the impact of such mismatch, we evaluate our
attack on each of the Teacher layers in all four Student
models. We organize our results by the transfer method.
Deep-layer Feature Extractor.
The corresponding
student models are Face and Iris. We set their pertur-
bation budget P to 0.003, and 0.005, respectively (the
same values used in the previous experiment). We launch
attacks to each of the N-1 Teacher layers (N=16), i.e.
computing adversarial samples that mimic the internal
representation of the target image at layer K where K =
1...N − 1. Figure 5(a) and Figure 5(b) show targeted and
non-targeted success rates when attacking different lay-
ers.
For both Face and Iris, the attack is the most effective
when targeting precisely the N − 1th (15th) layer, which
is as expected since both use Deep-layer Feature Extrac-
tor. As the attacker moves from deeper layers towards
shallow layers (i.e. reducing K), the attack effectiveness
reduces. At layer 13 and above, the attack success rates
are above 88.4% for Face, and 95.9% for Iris. But when
targeting layer 10 and below, the success rates drop to
1.2% for Face recognition, and <40% for Iris recogni-
tion. This is because shallow layers represent basic com-
ponents of an image, e.g., lines and edges, which are
harder to mimic using a limited perturbation budget. In
fact, both Face and Iris models are based on convolu-
tional neural networks, which are known to capture such
representations at shallow layers [70]. Therefore, given
a ﬁxed perturbation budget, the error in mimicking in-
ternal representations is much higher at shallow layers,
resulting in lower attack success rates.
An unexpected result is that for Iris, the success rate
for non-targeted attacks remains close to 100% regard-
less of the attack layer choice. A more detailed analysis
shows that this is because Iris recognition is highly sen-
sitive to input noise. The perturbations introduced by