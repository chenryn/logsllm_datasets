about the IPs observed in order to consolidate the ideas developed in Sect.6. In
Sect.8, we discuss the lessons learned thanks to our experiment and analysis. A
conclusion as well as thoughts for future work are offered in Sect.9.
2 Problem Definition and Contributions
A2019Impervareport[6]describeshowtheairlinesindustryisheavilyimpacted
by large armies of bots. In 2017, according to that report, the proportion of bad
botstraffictoairlinewebsiteswas43.9%.Almostallthesebotsareusedtogather
freeinformationfromtheairlines’sitesaboutflightsandticketprices.Itiscom-
monlyagreedthattheactorsbehindthesebotsactivitiesareunauthorizedbusi-
nessintelligencecompanies,onlinetravelagenciesanddataaggregators.Indeed,
a large part of their business relies on web scraping and using bots instead of
havingapayingagreementwiththetargetedwebsitesismuchmoreprofitablefor
them.Theyharnessinformation,increasingdramaticallytheamountofrequests
to be served by airlines websites. Responding to these requests, due to the price
ticketing process, is an expensive task which well-behaving organisations nor-
mally pay for. The bots aim at getting the same service for free. By doing so,
they misuse the service provided by airlines companies to individual users.
An arms race exists between bot makers and anti-bot providers. The bot
detection relies on a number of different fingerprinting techniques to recognize
malicious agents [23]. As soon as a family of bots is identified and blocked,
their bot masters replace them with new ones. Blocking all the IP addresses of
identified bots is usually not seen as a viable option because it is well known
that the real IP addresses of the bots remain hidden behind a large amount
of proxy IP addresses provided by professional services. These services claim
to offer to their customers millions of residential IP addresses, leaving any IP
blockingsolutiondoomedtopotentiallyblockalargeamountoflegitcustomers.
Quoting one of these websites [12], we see that they offer to their customers
to “use [their] rotating residential proxies comprised of real user devices, mak-
ing them undetectable when used correctly”. The owners of these real devices,
also called exit nodes, “[...] agreed to route [...] traffic through their hosts in
exchangeforfreeservice”[12].AquicksearchontheInternetreturnsmorethan
a dozen similar proxy service offerings. We prefer not to offer them some addi-
tional advertisement by listing them all here. Suffice it to say that, for instance,
598 E. Chiapponi et al.
both [12,15] claim to offer more than 70 millions of IP addresses whereas [20]
supposedly has more than 10 millions IPs. Others have similar claims.
ThebenefitsofhidingbehindthisverylargepoolofIPaddressesisthreefold
for the web scraping actors: first, linking a scraping campaign to any known
organisation is impossible, thus no attribution and legal recourse; second, the
impressive number of frequently changing IP addresses used renders any IP
blocking strategy impractical; third, they can run these campaigns with a very
limited amount of powerful machines on their back end without the need of any
vast and highly distributed infrastructure.
In [2], we describe an experiment designed to analyse the behavior of these
bots. That experiment did confirm the existence of these advanced persistent
bots(APBs)andtheproxiestheywererelyingon.Italsoraisedquestionsregard-
ingtherealamountofIPsputatthedisposalofthebots.Inthiswork,wecarry
out an in-depth investigation of that question. By doing so, we provide the fol-
lowing contributions:
– We provide additional empirical pieces of evidence of the existence of very
stealthy APBs and confirm the usage of proxy servers by these bots
– Using two distinct approaches, we show that i) IP addresses provided to the
bots are not randomly assigned and that ii) the pool of IPs they are taken
from is two to three orders of magnitude smaller than what is announced by
the proxy websites.
– We explain how the idea of IP-blocking could be rejuvenated to defeat such
sophisticated bots.
3 State of the Art
Botnets, collections of hosts controlled by a bot, have been used for the years
for nefarious activities, such as scraping web pages of different industries [5].
Applying IP reputation to mitigate the threats of web scraping is not a new
idea [4]. Moreover, this technique has already been largely used against spam
bots [10].
However, as shown in the Imperva Report 2020 [5], recent years have wit-
nessed the rise of traffic produced by Advanced Persistent Bots (APBs). These
bots produce few requests per IP staying below the rate limits and protecting
their reputation. They rely on professional proxy services that make large num-
bers of IP addresses available for these activities [19]. These services claim to
have access to tens of millions of residential IPs and to be able to rotate them
among the different requests of each client. For these reasons, the report [5]
asserts that IP blacklisting has become “wholly ineffective”. Doubtlessly, mil-
lions of different IP cannot be blacklisted all together and e-commerce websites
cannot risk to block requests coming from real customers.
In2019,Mietal.[13]proposedthefirstcomprehensivestudyofResidentialIP
ProxyasaService.Eveniftheirmethodologyhasbeenpartiallycriticizedforthe
fingerprintingprocessofthedevices[16,17],theycreatedasuccessfulinfiltration
Botnet Sizes: When Maths Meet Myths 599
frameworkthatenabledthemtostudyresidentialproxyservicesfromtheinside.
Theycollected6.18milionsofIPs,ofwhich95,22%arebelievedtoberesidential.
Among their findings, it is peculiar to see a discrepancy between the number of
IPs claimed by Luminati [12] (30 millions) and the ones collected by them for
the same provider (4 millions using 16 million probings). The authors provide
no clear explanation for this gap. Furthermore it is noteworthy to mention the
discoveryoftwoprovidersusingthesamepoolofIPs,whileanotheronebuiltits
network on top of Luminati [12]. Our paper also aims at better understanding
the residential IP proxies ecosystem by providing a different view point.
Nowadays, web site owners usually take advantage of third party anti-bot
services to perform bot management. These commercial solutions analyse the
incoming requests to the websites. As described in [23], multiple parameters
are collected from the environment in which the request is generated, thanks to
fingerprinting.Thissetofparameterscanbeusedtoidentifythesameactorwho
launchesdifferentrequests,potentiallyfromdifferentIPaddresses.Ifasignature
is recognized as coming from a bot, the corresponding traffic can be blocked or
other mitigation actions can be put in place.
Azadetal.[1],proposeanempiricalanalysisofsomeantibotservices.Unfor-
tunately, their findings indicate that these solutions are mostly efficient against
basic bots but not against the truly sophisticated ones. Indeed, an arms race is
taking place between anti bot services trying to fingerprint and bots trying to
circumvent the detection. This has led the actors behind the bots to perform
only small amounts of requests per IP, with the goal of remaining undetected.
4 Experimental Setup
In [2], we have described a honeypot-based experimental setup designed to ana-
lyze the behavior of web scraping bots. We offer in this Section a brief recap of
thatexperimentasitisthesourceofthedatawewillbeanalysingintherestof
the paper. The interested reader is referred to [2] for more detailed information.
This experiment was run in close collaboration with a major IT provider for
airlineswebsites.Thispartyhandlesthecalculationofthefaresandthebooking
process for multiple airlines. The airlines companies pay the IT provider an
amountproportionaltothetransactionsserved.Naturally,anexcessofbottraffic
dramatically increases the volume of transactions and thus the infrastructure
costs for both the airlines and the IT provider. In recent years, bots started to
performanintensivepricescrapingactivitytowardsairline’swebsites,producing
up to 90% of the requests on some domains [6].
To mitigate this phenomenon, the IT provider with which we collaborate, is
using a commercial bot detection service provider. A box is put in front of the
provider’s booking domains and it detects bots thanks to browser fingerprinting
and machine learning. Every request is studied and a signature is assigned to
it. If the signature matches the one of a bot, an action is taken such as block-
ing,servingaCAPTCHA[24]oraJavaScriptchallenge. However,sophisticated
bots, dubbed APBs for Advanced Persistent Bots [5], can overcome these coun-
termeasures and/or change their parameters to avoid detection [14].
600 E. Chiapponi et al.
This solution works but has a major drawback, which is to provide feedback
tothebotswhentheyareidentified.Theyusethisinformationtomorphassoon
as they detect that they have been unmasked. By doing so, they defeat the mit-
igation process provided by the anti-bot solution. To overcome this problem, we
have decided to create a new action associated with a signature match: requests
coming from identified bots would now be redirected to a real-looking, yet fake,
web page. This web page, which can be seen as an application layer honeypot,
serves two distinct objectives: i) reduce the workload of the production servers,
ii) study the behaviour of the bots.
This honeypot is able to produce responses that are, syntactically, indistin-
guishable from the real ones. However, semantically, they differ because we use
cached values or, sometimes, modified values for the tickets. Cached values dra-
matically reduce the cost of computing the responses. Modified values enable us
toanalysetowhatextentthebotsarecapableofdetectingerroneousinformation
provided to them.
Wehavedesignedandimplementedsuchaplatformincollaborationwiththe
IT provider and a specific airline company. We chose a company whose traffic
was highly impacted by bots. At the time of our experiment, that company was
receiving, on average, 1 million requests per day, of which 40% were detected
as bot traffic by the anti bot solution. Unfortunately, the anti-bot solution is
not capable of blocking all bots. Each signature is associated with a confidence
level indicating the uncertainty whether the request comes, or not, from an ill
behavingbot.Dependingonthatvalue,thatIPwillbeblocked,challenged(e.g.
with a CAPTCHA) or simply put under scrutiny (e.g. to be blocked later if it
sendsasuspiciouslylargenumberofrequests).Wefocusedonthatlastcategory
and found, for that airline, a signature that was matched every day, always in
thesamesmalltimewindowof40minby,roughly,thesameamountofIPs.Last
but not least, almost none of these IPs ever booked a ticket. All these elements
gave us great confidence that that signature, while not blocked by the anti bot
solution, was reliably identifying members of a specific botnet. We have then
configured the anti-bot solution to redirect all requests matching that signature
to our honeypot.
For this publication to be self-contained, we offer in the next Section a syn-
thetic presentation of the raw results obtained and some statistical results.
5 Experimental Results
The experiment ran for 56 days, between 7th January and 2nd March 2020.
We have had no match for our signature after that date. We believe the reason
has to do with the business needs of the actor behind these bots. Indeed, that
date coincides with the beginning of the worldwide pandemic. Furthermore, the
airline, subject of our experiment, is the main one for a country whose govern-
ment issued its first major travel restriction on the 2nd of March, practically
shutting down airline travel to and from that country. Without any customer
interested in buying tickets to/from that country, there was no incentive for the
Botnet Sizes: When Maths Meet Myths 601
Fig.1.Left(resp.right)Yaxis:absolute Fig.2.Left(resp.right)Yaxis:absolute
(resp. relative) amount of IP addresses (resp. relative) amount of IP addresses
whichhavemadeatmostXrequestsper which have made at most a grand total
day ofXrequestsduringthewholeperiodof
the experiment.
malicious actor to keep collecting ticket prices from that company. This most
likely explains the disappearance of these bots.
Over the duration of the experiment, the honeypot has received 22,991
requests. The daily average amount was 410 with a standard deviation of 33
queries. All requests arrived at the same time of the day. The signatures were
onlyseenduringasmalltimewindowof38.18min,onaverage.Theamountand
the timing of the requests were in line with those of that bot signature before
the beginning of the experiment.
The 22,991 requests were issued by 13,897 unique IPs. Figure1 shows that
most of the IPs (97% of the total) made at most two requests per day, with the
vast majority (88%) making only one request per day. Figure2 shows the total
amount of requests made per distinct IP over the whole experiment. Here, we
seethat8,257IPshavesentonlyonerequest.Thisvalueistobecomparedwith
12,277 of Fig.1. It highlights the fact that a large amount of IPs have shown
up on at least two different days, issuing a single request every time. This is
confirmed by Fig.3 where we see that almost 30% of the IPs have been seen on
at least two different days.
Thatnumberissurprisinglyhigh.Indeed,atthisstage,wehavetoremindthe
reader that these IPs are proxy IPs and that the actual client machine sending
a request is hidden behind. The proxy service offers a pool of addresses to be
given to these clients. Let us call P the size of that pool. Figure3 shows how
many times a given address has been picked over a period of 56 days. The fact
that there are 2,801 that have been used twice over that period is inconsistent
with the assumption that the addresses would be randomly picked out of a very
large pool of millions of IPs. Indeed, to calculate the probability that a given IP
got picked twice over this period comes down to resolving the classical birthday
paradox which can be generalized as follows:
602 E. Chiapponi et al.
Given n random integers drawn from a discrete uniform distribution with
range [1,d], what is the probability p(n; d) that at least two numbers are
the same? (d = 365 gives the usual birthday problem.) [22]
In our case, n is equal to 56, the number of days where IPs from the pool are
assigned to clients and d is equal to the size of the pool P. We want to assess
the probability that the same IP would be drawn twice over that period of 56
days. We can rephrase the birthday problem for our needs as follows:
Given 56 random integers drawn from a discrete uniform distribution with
range [1,P], what is the probability p(56;P) that at least two numbers are
the same?
1−(P P−1)56(5 25−1)
The formula gives an approximate result:
– If P =10000000 then p(56,10M)≈0.000154
– If P =1000000 then p(56,1M)≈0.001538
– If P =100000 then p(56,100K)≈0.015282
Clearly, considering that we have seen more than 30% of the IPs drawn at
least twice, either P is significantly lower than the number announced by the
proxy services, or the assignment of IPs is not randomly done, or both.
Regarding the total amount of IPs, we saw only 13,897 different ones. Every
day the number of distinct IPs, on average 371 (shown in yellow in Fig.4), was
similar to the number of requests, on average 410. Thus, it is clear that most
IPs send a single request and reappear some time later. In the same figure,
the green columns represent the cumulative number of unique IPs observed in
our honeypot since the beginning of the experiment. The figure shows that the
daily increment decreases over time, suggesting that it will eventually reach a
maximum.
To better characterize and understand the threats ecosystem we are facing,
we try to find a mathematical model that approximates as closely as possible
the assignment of IPs made by the proxy provider. We use that model to derive
the most likely size of P. This is done in the next Section.
6 Modeling Results
6.1 Introduction
We propose two distinct modeling approaches to assess the most likely size of
the pool of IPs P put at the disposal of the stealthy APBs we have observed.
Both models deliver a value which is below 70K, i.e. three orders of magnitude
less than the 70M IPs supposedly provided by [12].
In the first approach (subsection 6.2), we look at the IPs assigned every day
by the proxy to the bots. We model this as a drawing process made in a pool of
size P and we try to find the best probability distribution function that would
Botnet Sizes: When Maths Meet Myths 603
Fig.3.Left(resp.right)Yaxis:absolute Fig.4. Cumulative curve of the new
(resp. relative) amount of IP addresses unique IPs in comparison with the daily
whichwereseeninXdistinctdaysduring unique IPs.
the whole experiment.
producesimilarresultstotheoneswehavewitnessed.Fromthere,wederivethe
value of P.
Inthe secondapproach (Subsect.6.3), we look for afitting curveto approxi-
mate the one shown in Fig.4 and, by extrapolating it, see what maximum value
it would reach, and when.
6.2 IP Assignment as a Drawing Process
General Principle. Figure3tellsushowmanyIPshavebeenassignedtoabot
only once, or twice, or three times ... over the duration of the experiment. We
model this assignment process by a daily probabilistic drawing process without
replacement. We arbitrarily define a pool size P. On a given day, we draw from
ourpool,withoutreplacement,anumberofvaluesequaltotheamountofdistinct
IPs seen that day. We do this every day, keeping track of which value got drawn