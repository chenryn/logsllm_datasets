customers or hospital patients) and collaboratively train the FL
model; we denote the total number of users by N′.
Recall from Section III-C2 that the model poisoning ad-
versary completely breaks into the devices of compromised
clients and, to be effective, persists in their systems for long
duration because model poisoning attacks are online attacks
(Section III-C2). For cross-silo FL, this means that the adver-
sary should break into large corporations, e.g., a bank, who are
bound by contract and have professionally maintained software
stacks. Plausible cross-silo poisoning scenarios involve strong
incentives (e.g., financial) and require multiple parties to be
willing to risk the breach of contract by colluding or for one
party to hack thereby risking criminal liability. This makes
breaking into these silos practically unlikely, hence we argue
that model poisoning threats in cross-silo FL are impractical.
Note that this is unlike the large scale data-breaches [46]–
[48] which are short-lived and are only capable of stealing
information, but not changing the infrastructure.
Hence, we only study the data poisoning threat for cross-
silo FL. For worse-case analyses, we assume that the silos train
their models on all the data contributed by their users. If the
silos inspect the users’ data and remove the mislabeled data,
one should consider clean-label data poisoning attacks [27],
[54]; we leave this study to future work. Note that, data
inspection is not possible in cross-device FL as data of clients
(who are also the users) is completely local, hence clean-label
poisoning is not relevant in cross-device FL.
We assume that each silo collects data from equal number
(i.e., N′/N) of users. For DPAs, we assume M % of the N′
users are compromised and each of them shares poisoned
data Dp (computed as described in Section IV-B2) with their
parent silo; as discussed in Section III-B3, we assume |Dp| =
100 × |D|avg for each user. We distribute the compromised
users either uniformly across the silos or concentrate them in a
few silos. For instance, consider 50 silos and 50 compromised
users and that, each silo can have a maximum of 50 users.
Then in the uniform case, a single compromised user shares
her Dp with each silo, while in the concentrated case, all the
50 compromised users share their Dp with a single silo.
Figure 8 (Appendix E) shows the impacts of best of DPAs
for the concentrated case. We see that cross-silo FL is highly
robust to state-of-the-art DPAs. Because, in the concentrated
case, very large numbers of benign silos mitigate the poisoning
impact of the very few (M %) compromised silos. We observe
the same results for the uniform distribution case, because
very large numbers of benign users in each silo mitigate the
poisoning impacts of the very few (M %) compromised users.
(Takeaway V-D) In production cross-silo FL, model poi-
soning attacks are not practical, and state-of-the-art data
poisoning attacks have no impact even with Average AGR.
VI. CONCLUSIONS
In this work, we systematized the threat models of poisoning
attacks on federated learning (FL), provided the practical
ranges of various parameters relevant to FL robustness, and
designed a suite of untargeted model and data poisoning
attacks on FL (including existing and our improved attacks).
Using these attacks, we thoroughly evaluated the state-of-the-
art defenses under production FL settings. We showed that
the conclusions of previous FL robustness literature cannot
be directly extended to production FL. We presented concrete
takeaways from our evaluations to correct some of the estab-
lished beliefs and highlighted the need to consider production
FL environments in research on FL robustness.
We hope that our systematization of practical poisoning
threat models can steer the community towards practically
significant research problems in FL robustness. For instance,
one such open problem is to obtain concrete theoretical
robustness guarantees of existing defenses in production FL
settings where only a very small fraction of all clients is
randomly selected in each FL round.
ACKNOWLEDGEMENTS
The work was supported by DARPA and NIWC under
contract HR00112190125, and by the NSF grants 1953786,
1739462, and 1553301. The U.S. Government is authorized to
reproduce and distribute reprints for Governmental purposes
notwithstanding any copyright notation thereon. The views,
opinions, and/or findings expressed are those of the author(s)
and should not be interpreted as representing the official
views or policies of the Department of Defense or the U.S.
Government.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:01:32 UTC from IEEE Xplore.  Restrictions apply. 
1366
REFERENCES
[1] “Federated learning: Collaborative machine learning without cen-
tralized training data,” https://ai.googleblog.com/2017/04/federated-
learning-collaborative.html, 2017.
[2] D. Alistarh, Z. Allen-Zhu, and J. Li, “Byzantine stochastic gradient
descent,” in NeurIPS, 2018.
[3] E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and V. Shmatikov, “How to
backdoor federated learning,” in AISTATS, 2020.
[4] M. Barreno, B. Nelson, and A. D. Joseph, “The security of machine
learning,” Machine Learning, 2010.
[5] M. Baruch, B. Gilad, and Y. Goldberg, “A Little Is Enough: Circum-
venting Defenses For Distributed Learning,” in NeurIPS, 2019.
[6] J. Bernstein, J. Zhao, K. Azizzadenesheli, and A. Anandkumar,
“signSGD with Majority Vote is Communication Efficient and Fault
Tolerant,” in ICLR, 2018.
[7] A. N. Bhagoji, S. Chakraborty, P. Mittal, and S. Calo, “Analyzing
federated learning through an adversarial lens,” in ICML, 2019.
[8] B. Biggio, B. Nelson, and P. Laskov, “Poisoning attacks against support
vector machines,” in ICML, 2012.
[9] B. Biggio and F. Roli, “Wild patterns: Ten years after the rise of
adversarial machine learning,” Pattern Recognition, 2018.
[10] P. Blanchard, R. Guerraoui, J. Stainer et al., “Machine learning with
adversaries: Byzantine tolerant gradient descent,” in NeurIPS, 2017.
[11] K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman,
V. Ivanov, C. Kiddon, J. Konecn´y, S. Mazzocchi, B. McMahan, T. V.
Overveldt, D. Petrou, D. Ramage, and J. Roselander, “Towards federated
learning at scale: System design,” in MLSys, 2019.
[12] M. H. Brendan, D. Ramage, K. Talwar, and L. Zhang, “Learning
differentially private recurrent language models,” in ICLR, 2018.
[13] S. Caldas, P. Wu, T. Li, J. Koneˇcn`y, H. B. McMahan, V. Smith,
federated settings,”
and A. Talwalkar, “LEAF: A benchmark for
arXiv:1812.01097, 2018.
[14] X. Cao, J. Jia, and N. Z. Gong, “Provably Secure Federated Learning
against Malicious Clients,” in AAAI, 2021.
[15] H. Chang, V. Shejwalkar, R. Shokri, and A. Houmansadr, “Cronus:
Robust and Heterogeneous Collaborative Learning with Black-Box
Knowledge Transfer,” arXiv:1912.11279, 2019.
[16] L. Chen, H. Wang, Z. Charles, and D. Papailiopoulos, “Draco:
Byzantine-resilient distributed training via redundant gradients,” in
ICML, 2018.
[17] X. Chen, C. Liu, B. Li, K. Lu, and D. Song, “Targeted backdoor attacks
on deep learning systems using data poisoning,” arXiv:1712.05526,
2017.
[18] G. Cohen, S. Afshar, J. Tapson, and A. Van Schaik, “EMNIST: Extend-
ing MNIST to handwritten letters,” in IJCNN, 2017.
[19] “CS231n: Convolutional Neural Networks for Visual Recognition,”
https://cs231n.github.io/optimization-2/#grad, 2021.
[20] D. Data and S. Diggavi, “Byzantine-resilient SGD in high dimensions
on heterogeneous data,” arXiv:2005.07866, 2020.
[21] E.-M. El-Mhamdi, R. Guerraoui, A. Guirguis, and S. Rouault, “Sgd:
Decentralized byzantine resilience,” arXiv:1905.03853, 2019.
[22] “Facebook has shut down 5.4 billion fake accounts this year,” https:
//www.cnn.com/2019/11/13/tech/facebook-fake-accounts/index.html,
2019.
[23] M. Fang, X. Cao, J. Jia, and N. Z. Gong, “Local Model Poisoning
Attacks to Byzantine-Robust Federated Learning,” in USENIX, 2020.
[24] “Google Workshop
on
Federated Learning
and Analytics,”
https://docs.google.com/document/d/1dWzVeFLrPinonQMauxIo0oI-
Vbvqup5cZzgdPXvu97Y/edit#heading=h.7dsxad3c3nf7, 2020.
[25] S. Fu, C. Xie, B. Li, and Q. Chen, “Attack-resistant federated learning
with residual-based reweighting,” arXiv:1912.11464, 2019.
[26] C. Fung, C. J. Yoon, and I. Beschastnikh, “The limitations of federated
learning in sybil settings,” in RAID, 2020.
[27] M. Goldblum, D. Tsipras, C. Xie et al., “Dataset Security for Ma-
chine Learning: Data Poisoning, Backdoor Attacks, and Defenses,”
arXiv:2012.10544, 2020.
[28] “Google Play Protect,” https://developers.google.com/android/play-
protect, 2021.
[31] M. S. Jere, T. Farnan, and F. Koushanfar, “A taxonomy of attacks on
federated learning,” IEEE Security & Privacy, 2020.
[32] P. Kairouz, H. B. McMahan, B. Avent et al., “Advances and open
problems in federated learning,” arXiv:1912.04977, 2019.
[33] J. Koneˇcn`y, H. B. McMahan, F. X. Yu, P. Richt´arik, A. T. Suresh, and
D. Bacon, “Federated learning: Strategies for improving communication
efficiency,” NIPS Workshop on Private Multi-Party ML, 2016.
[34] A. Krizhevsky, “Learning multiple layers of features from tiny images,”
University of Toronto, Tech. Rep., 2009.
[35] Y. LeCun, L. Bottou, Y. Bengio et al., “Gradient-based learning applied
to document recognition,” Proceedings of the IEEE, 1998.
[36] L. Li, W. Xu, T. Chen, G. B. Giannakis, and Q. Ling, “RSA: Byzantine-
robust stochastic aggregation methods for distributed learning from
heterogeneous datasets,” in AAAI, 2019.
[37] T. Li, S. Hu, A. Beirami, and V. Smith, “Ditto: Fair and robust federated
learning through personalization,” in ICML, 2021.
[38] T. Lin, L. Kong, S. U. Stich, and M. Jaggi, “Ensemble distillation for
robust model fusion in federated learning,” in NeurIPS, 2020.
[39] H. Ludwig, N. Baracaldo, G. Thomas et al., “IBM Federated Learning:
An Enterprise Framework White Paper v0.1,” arXiv:2007.10987, 2020.
[40] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y. Arcas,
“Communication-efficient learning of deep networks from decentralized
data,” in AISTATS, 2017.
[41] E. M. E. Mhamdi, R. Guerraoui, and S. Rouault, “The Hidden Vulner-
ability of Distributed Learning in Byzantium,” in ICML, 2018.
[42] T. Minka, “Estimating a Dirichlet distribution,” 2000.
[43] L. Mu˜noz-Gonz´alez, B. Biggio, A. Demontis, A. Paudice, V. Wongras-
samee, E. C. Lupu, and F. Roli, “Towards poisoning of deep learning
algorithms with back-gradient optimization,” in AISec, 2017.
[44] L. Mu˜noz-Gonz´alez, B. Pfitzner, M. Russo, J. Carnerero-Cano, and
E. C. Lupu, “Poisoning attacks with generative adversarial nets,”
arXiv:1906.07773, 2019.
[45] A. Newell, R. Potharaju, L. Xiang, and C. Nita-Rotaru, “On the
practicality of integrity attacks on document-level sentiment analysis,”
in AISec, 2014.
[46] “Billion
Passwords
Stolen: Change All
of Yours, Now!”
https://www.nbcnews.com/tech/security/billion-passwords-stolen-
change-all-yours-now-n174321, 2014.
[47] “Hackers Expose 8.4 Billion Passwords Post them Online in Possibly
Largest Dump of Passwords Ever,” https://www.thegatewaypundit.com/
2021/06/hackers-expose-8-4-billion-passwords-post-online-possibly-
largest-dump-passwords-ever/, 2014.
[48] “26 million stolen passwords found online — see if you’re affected,”
https://www.tomsguide.com/news/mystery-malware-info-stealer, 2021.
[49] M. Paulik, M. Seigel, H. Mason et al., “Federated Evaluation and
Tuning for On-Device Personalization: System Design & Applications,”
arXiv:2102.08503, 2021.
[50] K. Pillutla, S. M. Kakade, and Z. Harchaoui, “Robust aggregation for
federated learning,” arXiv:1912.13445, 2019.
Challenge
[51] “Acquire Valued
Shoppers
at Kaggle,”
https:
//www.kaggle.com/c/acquire-valued-shoppers-challenge/data, 2019.
[52] S. J. Reddi, Z. Charles, M. Zaheer, Z. Garrett, K. Rush, J. Koneˇcn`y,
S. Kumar, and H. B. McMahan, “Adaptive Federated Optimization,” in
ICLR, 2020.
[53] “SafetyNet Attestation API,” https://developer.android.com/training/
safetynet/attestation, 2021.
[54] A. Shafahi, W. R. Huang, M. Najibi, O. Suciu, C. Studer, T. Dumitras,
and T. Goldstein, “Poison frogs! targeted clean-label poisoning attacks
on neural networks,” in NeurIPS, 2018.
[55] V. Shejwalkar and A. Houmansadr, “Manipulating the Byzantine: Opti-
mizing Model Poisoning Attacks and Defenses for Federated Learning,”
in NDSS, 2021.
[56] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” in ICLR, 2015.
[57] G. Sun, Y. Cong, J. Dong, Q. Wang, L. Lyu, and J. Liu, “Data poisoning
attacks on federated machine learning,” IEEE IoT Journal, 2021.
[58] Z. Sun, P. Kairouz, A. T. Suresh, and H. B. McMahan, “Can you really
backdoor federated learning?” NeurIPS FL Workshop, 2019.
[29] L. Huang, A. D. Joseph, B. Nelson, B. I. Rubinstein, and J. D. Tygar,
[59] V. Tolpegin, S. Truex, M. E. Gursoy, and L. Liu, “Data poisoning attacks
“Adversarial machine learning,” in AISec, 2011.
against federated learning systems,” in ESORICS, 2020.
[30] M. Jagielski, A. Oprea, B. Biggio, C. Liu, C. Nita-Rotaru, and B. Li,
“Manipulating machine learning: Poisoning attacks and countermeasures
against regression learning,” 39th IEEE Symposium on S&P, 2018.
[60] B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng, and B. Y.
Zhao, “Neural cleanse: Identifying and mitigating backdoor attacks in
neural networks,” in 40th IEEE Symposium on S&P, 2019.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:01:32 UTC from IEEE Xplore.  Restrictions apply. 
1367
[61] H. Wang, K. Sreenivasan, S. Rajput, H. Vishwakarma, S. Agarwal, J.-
y. Sohn, K. Lee, and D. Papailiopoulos, “Attack of the tails: Yes, you
really can backdoor federated learning,” in NeurIPS, 2020.
[62] “Utilization of FATE in Risk Management of Credit in Small and Mi-
cro Enterprises,” https://www.fedai.org/cases/utilization-of-fate-in-risk-
management-of-credit-in-small-and-micro-enterprises/, 2019.
[63] C. Wu, X. Yang, S. Zhu, and P. Mitra, “Mitigating backdoor attacks in
federated learning,” arXiv:2011.01767, 2020.
[64] H. Xiao, H. Xiao, and C. Eckert, “Adversarial label flips attack on
support vector machines,” in ECAI, 2012.
[65] H. Xiao, B. Biggio, B. Nelson, H. Xiao, C. Eckert, and F. Roli, “Support
vector machines under adversarial label contamination,” Neurocomput-
ing, 2015.
[66] C. Xie, M. Chen, P.-Y. Chen, and B. Li, “CRFL: Certifiably Robust
Federated Learning against Backdoor Attacks,” in ICML, 2021.
[67] C. Xie, K. Huang, P.-Y. Chen, and B. Li, “DBA: Distributed backdoor
attacks against federated learning,” in ICLR, 2019.
[68] C. Xie, O. Koyejo, and I. Gupta, “Generalized byzantine-tolerant sgd,”
arXiv:1802.10116, 2018.
[69] C. Yang, Q. Wu, H. Li, and Y. Chen, “Generative poisoning attack
method against neural networks,” arXiv:1703.01340, 2017.
[70] D. Yin, Y. Chen, K. Ramchandran, and P. Bartlett, “Byzantine-robust
distributed learning: Towards optimal statistical rates,” in ICML, 2018.
[71] T. Yu, E. Bagdasaryan, and V. Shmatikov, “Salvaging federated learning
by local adaptation,” arXiv:2002.04758, 2020.
APPENDIX
A. Related Work
1) Targeted and Backdoor Attacks: Section IV-A discusses
all state-of-the-art untargeted attacks in detail. Below, we
discuss existing works on targeted and backdoor attacks.
Targeted attacks [7], [58], [59] aim to make the global model
misclassify a specific set of samples at test time. Bhagoji et
al. [7] aimed to misclassify a single sample and proposed a
model poisoning attack based on alternate minimization to
make poisoned update look similar to benign updates. [7]
shows that their attack, with a single attacker, can misclassify
a single sample with 100% success against the non-robust
Average AGR. try Sun et al. [58] investigated constrain-and-
scale attack [3] with the aim to misclassify all samples of a
few victim FL clients. Tolpegin et al. [23], [59] investigated
targeted data poisoning attacks when compromised clients
compute their updates by mislabeling the target samples.
Backdoor attacks
[3], [61], [67] aim to make the global