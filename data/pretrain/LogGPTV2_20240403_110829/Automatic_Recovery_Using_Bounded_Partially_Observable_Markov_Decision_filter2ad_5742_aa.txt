title:Automatic Recovery Using Bounded Partially Observable Markov Decision
Processes
author:Kaustubh R. Joshi and
William H. Sanders and
Matti A. Hiltunen and
Richard D. Schlichting
Automatic Recovery Using Bounded Partially Observable Markov Decision
Processes
‡
Kaustubh R. Joshi
Matti A. Hiltunen
§
William H. Sanders
‡
Richard D. Schlichting
§
‡
Coordinated Science Laboratory
University of Illinois at Urbana-Champaign
{joshi1,whs}@crhc.uiuc.edu
Urbana, IL, USA
§
AT&T Labs Research
180 Park Ave.
{hiltunen,rick}@research.att.com
Florham Park, NJ, USA
Abstract
This paper provides a technique, based on partially ob-
servable Markov decision processes (POMDPs), for build-
ing automatic recovery controllers to guide distributed sys-
tem recovery in a way that provides provable assurances on
the quality of the generated recovery actions even when the
diagnostic information may be imprecise. Lower bounds on
the cost of recovery are introduced and proved, and it is
shown how the characteristics of the recovery process can
be used to ensure that the lower bounds converge even on
undiscounted models. The bounds used in an appropriate
online controller provide it with provable termination prop-
erties. Simulation-based experimental results on a realistic
e-commerce system demonstrate that the proposed bounds
can be improved iteratively, and the resulting controller
convincingly outperforms a controller that uses heuristics
instead of bounds.
1
Introduction
Throughout the history of computing, system recovery
has always been one of the most important tools in the arse-
nal of the dependable system architect. Performed quickly
and automatically, recovery can provide systems with high
levels of availability, often without exorbitant increases in
cost. However, recovery actions can often temporarily de-
grade system performance and cannot be used indiscrimi-
nately. Therefore, it is important to perform accurate fail-
ure detection and good fault diagnosis before any recovery
can be performed. Unfortunately, failure detection and di-
agnosis are often difﬁcult tasks because of the fundamental
trade-offs between detection coverage and accuracy that are
present in most monitoring techniques. For example, a low-
level heartbeat-based monitor might detect the location of
a fault accurately at the component level, but often suffers
from low coverage (only hardware and OS crash failures are
detected). On the other hand, end-to-end monitoring might
detect additional failures (e.g., value or timing failures), but
cannot precisely identify the faulty component. Combining
monitoring of different types may provide better diagnosis,
but the fact still remains that one may never know for certain
which faults have occurred in a system.
Despite these problems, detection and recovery are often
the only available techniques for many kinds of systems,
due to their low cost and wide applicability. Therefore, sys-
tem designers (especially in large distributed systems such
as e-commerce systems) often tackle the problems by writ-
ing adhoc “if-then” recovery rules that take as input the
outputs of system monitors, and generate a list of recov-
ery actions. These rules are often based on a mixture of
domain knowledge about the system architecture and prior
experience with the system. Unfortunately, writing such
rules requires a lot of expertise, and the rules often become
complex, because explicit decisions must be made regard-
ing every possible situation that may arise. Furthermore, it
is difﬁcult to predict whether such rules will interact with
each other in unforeseen ways, and what the performance
of the resulting “recovery controller” might be. Thus, sound
techniques that can reduce human involvement by automat-
ing decision-making while also providing guarantees on the
quality of the decisions produced are sorely needed.
In [8], to tackle the problem, we proposed a model-based
recovery framework for distributed systems based on par-
tially observable Markov chains (POMDPs). When pro-
vided with models of the system monitoring and recov-
ery actions, the framework has the ability to direct system
recovery automatically, even when the information about
faults provided by the system’s monitors is imprecise and
when recovery actions have substantial runtime costs. How-
ever, solution of POMDPs is difﬁcult, and in our previous
work, we used heuristics when solving a POMDP. Doing so
prevented the controller from guaranteeing any termination
and performance properties.
In this paper, we extend that work and make the follow-
ing new contributions. 1) We develop a new lower bound
1
(the RA-Bound) for the value of a POMDP. This bound
is based on an exponentially smaller state-space than is
needed to solve the POMDP, and is thus very cheaply com-
puted. 2) We develop two sets of conditions on recovery
models based on the nature of the recovery process to ensure
that the RA-Bound converges even for undiscounted opti-
mization criteria. For one set of conditions, the RA-Bound
is the only lower bound we are aware of that converges to a
ﬁnite value. Most of the previous work on bounds has been
on discounted optimization problems. 3) Using the bounds,
we construct a controller that can ensure in an automatic
manner that the recovery process will always terminate and
can provide upper bounds on mean cost of recovery. 4) We
demonstrate the techniques by using them to choose which
components of an e-commerce system to restart, even when
the location of the fault is not precisely known. In doing
so, we show experimentally that a) the lower bounds can be
iteratively improved, b) the controller that uses the bounds
does not terminate until recovery has completed, and c) the
resulting controller performs recovery faster and takes less
time to decide than a controller that uses heuristics.
2 Overview and Related Work
The fundamental insight in our proposed approach to au-
tomatic recovery is that, at its core, automatic system recov-
ery is a performability optimization problem. The recovery
controller’s goal on the occurrence of a fault is to restore the
system to a good state by executing recovery actions such
that the costs accumulated along the way are minimized.
The goal of cost minimization automatically ensures cor-
rectness with respect to the model. Markov Decision Pro-
cesses have been used in the past to optimize performability
in other applications (e.g., [5, 1, 12, 3]), so we begin by con-
sidering their applicability to system recovery.
A Markov Decision Process is deﬁned as a tuple
(S,A, p(·|s, a), r(s, a)), where S is a set of states, and A
is a ﬁnite set of actions. p is a collection of state-transition
probability functions, one per action, such that p(s(cid:1)|s, a),
where s, s(cid:1) ∈ S, and a ∈ A denotes the probability that
the MDP will transition to state s(cid:1)
when action a is chosen
in state s. Finally, r(s, a) is a reward (cost) function that
speciﬁes the reward (cost) incurred when action a is cho-
sen in state s. Figure 1(a) shows a simple example of how
an MDP might be used to model the recovery process of
two redundant servers a and b. In the ﬁgure, the different
states represent the different faults that might exist in the
system (with a special “null fault” state for indicating the
absence of any fault). The actions represent the different
recovery choices available to the controller (including pas-
sive actions that may just observe the system) and are spec-
iﬁed by the transition probability and cost. For example,
“Restart(a)(1,-0.5)” in state Fault(a) indicates that restart-
ing a when it is faulty will recover the system with proba-
bility 1 and incur an unavailability cost (negative reward) of
0.5. In the example, actions are assumed to take unit time,
Restart(b) 
Restart(b) 
(1,-1)
Observe (1,-0.5)
(1,-1)
Fault
(a)
Restart(a) 
(1,-0.5)
Restart(a) or
Restart(b)
Null
Fault
(b)
Restart(b) 
(1,-0.5)
Observe
(1,-0.5)
(a) Example Recovery Model
(1,0)
Max
Vp( )
Restart(a) Restart(b)
+
o=True
o=False
+
o=False
Vp
-( 1)
Vp
-( 2) Vp
Vp
-( 4)
o=True
-( 3)
(b) Finite-depth Tree POMDP Solution
Figure 1. Recovery Model and Solution
but in general actions are also associated with an execution
time ta, allowing both rate (¯r(s, a)) and impulse (ˆr(s, a))
rewards to be deﬁned and converted into a single-step re-
ward as r(s, a) = ¯r(s, a) · ta + ˆr(s, a).
In this framework, a stationary deterministic, Markov
policy ρ(s) is a mapping from states to the actions that
should be chosen when the system is in those states.
In
the case of system recovery, such a policy is exactly what
would be needed to construct a recovery controller. Given
a “good” policy, a controller could guide the system from
a faulty state to the null fault state via a series of recov-
ery actions. Furthermore, MDP solution techniques ex-
ist to construct optimal policies ρ∗
that are both station-
ary and Markov, and ensure that the reward (cost) accrued
by the system over its lifetime is optimized. Formally,
ρ∗ = argmaxρ
t=0 βtr(St, ρ(St)), where St and ρ(St)
are random variables representing the system state and the
action chosen under policy ρ respectively, at decision point
t. Given a starting state s, the value of the MDP is deﬁned
as the optimal mean accumulated reward obtainable when
starting from that state. It is known (from [11], for exam-
ple) that the value function Vm(s),∀s ∈ S is given by the
dynamic programming equation:
(cid:3)
(cid:1)∞
p(s(cid:1)|s, a)Vm(s(cid:1)
(1)
(cid:4)
)
(cid:2)
r(s, a) + β
Vm(s) = max
a∈A
s(cid:1)∈S
The optimal (deterministic) policy is given by choosing for
each state s ∈ S the action a∗(s) that maximizes the right
side of Equation 1.
The constant β (0 ≤ β ≤ 1) in the equations above is the
“discounting factor.” If β  0) is not decidable for
the undiscounted accumulated reward criterion [9]. An ap-
proach that is often used for discounted models is to choose
an action on-line by performing a ﬁnite-depth expansion of
the POMDP dynamic programming recursion starting at the
current belief-state. An example recursion tree with a depth
of one is shown in Figure 1(b). The tree is a Max-Avg tree
in which the values of future belief-states are averaged, and
maximization is performed over available recovery actions.
The action that maximizes the value of the root is chosen
to be executed. At the leaves of the tree, bounds or approx-
imations of the remaining rewards are used. We used this
approach with a heuristic approximation of the remaining
reward at the leaves to perform automatic recovery in [8].
However, use heuristics is undesirable, because they may