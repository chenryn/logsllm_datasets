one possible cause of this vulnerability, raising the critical
concern about the current assessment metrics of interpreta-
tion models. Further, we discuss potential countermeasures
against ADV2, which sheds light on designing and operating
IDLSes in a more robust and informative fashion.
Acknowledgments
This material is based upon work supported by the National
Science Foundation under Grant No. 1846151 and 1910546.
Any opinions, ﬁndings, and conclusions or recommendations
expressed in this material are those of the author(s) and do
not necessarily reﬂect the views of the National Science Foun-
dation. Shouling Ji is partially supported by NSFC under No.
61772466 and U1836202, the Zhejiang Provincial Natural Sci-
ence Foundation for Distinguished Young Scholars under No.
LR19F020003, and the Provincial Key Research and Devel-
opment Program of Zhejiang, China under No. 2017C01055.
Xiapu Luo is partially supported by Hong Kong RGC Project
(No. PolyU 152279/16E, CityU C1008-16G).
USENIX Association
29th USENIX Security Symposium    1671
References
[1] J. Adebayo, J. Gilmer, M. Muelly, I. Goodfellow,
M. Hardt, and B. Kim. Sanity Checks for Saliency
Maps. In Proceedings of Advances in Neural Informa-
tion Processing Systems (NIPS), 2018.
[2] Rima Alaifari, Giovanni S. Alberti, and Tandri Gauks-
son. ADef: an Iterative Algorithm to Construct Adver-
sarial Deformations. In Proceedings of International
Conference on Learning Representations (ICLR), 2019.
[3] M. Ancona, E. Ceolini, C. Öztireli, and M. Gross. To-
wards Better Understanding of Gradient-based Attribu-
tion Methods for Deep Neural Networks. In Proceedings
of International Conference on Learning Representa-
tions (ICLR), 2018.
[4] Marcin Andrychowicz, Misha Denil, Sergio Gómez,
Matthew W Hoffman, David Pfau, Tom Schaul, Brendan
Shillingford, and Nando de Freitas. Learning to learn
by gradient descent by gradient descent. In Proceedings
of Advances in Neural Information Processing Systems
(NIPS), 2016.
[5] Anish Athalye, Nicholas Carlini, and David Wagner.
Obfuscated Gradients Give a False Sense of Security:
Circumventing Defenses to Adversarial Examples. In
Proceedings of International Conference on Learning
Representations (ICLR), 2018.
[6] Sebastian Bach, Alexander Binder, Grégoire Montavon,
Frederick Klauschen, Klaus-Robert Müller, and Woj-
ciech Samek. On Pixel-Wise Explanations for Non-
Linear Classiﬁer Decisions by Layer-Wise Relevance
Propagation. PLoS ONE, 10(7):e0130140, 2015.
[7] Battista Biggio, Blaine Nelson, and Pavel Laskov. Poi-
soning Attacks against Support Vector Machines. In
Proceedings of IEEE Conference on Machine Learning
(ICML), 2012.
[8] Chunshui Cao, Xianming Liu, Yi Yang, Yinan Yu,
Jiang Wang, Zilei Wang, Yongzhen Huang, Liang Wang,
Chang Huang, Wei Xu, Deva Ramanan, and Thomas S
Huang. Look and Think Twice: Capturing Top-Down
Visual Attention with Feedback Convolutional Neural
Networks. In Proceedings of IEEE International Con-
ference on Computer Vision (ICCV), 2015.
[9] Nicholas Carlini and David A. Wagner. Towards Evalu-
ating the Robustness of Neural Networks. In Proceed-
ings of IEEE Symposium on Security and Privacy (S&P),
2017.
[11] Nilesh Dalvi, Pedro Domingos, Mausam, Sumit Sanghai,
and Deepak Verma. Adversarial Classiﬁcation. In Pro-
ceedings of ACM International Conference on Knowl-
edge Discovery and Data Mining (KDD), 2004.
[12] J. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-
Fei.
ImageNet: A large-scale hierarchical image
database. In Proceedings of IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2009.
[13] Mengnan Du, Ninghao Liu, Qingquan Song, and Xia
Hu. Towards Explanation of DNN-based Prediction
with Guided Feature Inversion. In Proceedings of ACM
International Conference on Knowledge Discovery and
Data Mining (KDD), 2018.
[14] Mengnan Du, Ninghao Liu, Qingquan Song, and Xia
Hu. Towards Explanation of DNN-based Prediction
with Guided Feature Inversion. ArXiv e-prints, 2018.
[15] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-
Agnostic Meta-Learning for Fast Adaptation of Deep
Networks. In Proceedings of IEEE Conference on Ma-
chine Learning (ICML), 2017.
[16] Ruth C Fong and Andrea Vedaldi.
Interpretable Ex-
planations of Black Boxes by Meaningful Perturbation.
In Proceedings of IEEE International Conference on
Computer Vision (ICCV), 2017.
[17] T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov,
S. Chaudhuri, and M. Vechev. AI2: Safety and Robust-
ness Certiﬁcation of Neural Networks with Abstract
Interpretation. In Proceedings of IEEE Symposium on
Security and Privacy (S&P), 2018.
[18] Nils Gessert, Thilo Sentker, Frederic Madesta, Rüdi-
ger Schmitz, Helge Kniep, Ivo M. Baltruschat, René
Werner, and Alexander Schlaefer. Skin lesion diagnosis
using ensembles, unscaled multi-crop evaluation and
loss weighting. ArXiv e-prints, abs/1808.01694, 2018.
[19] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy.
Explaining and Harnessing Adversarial Examples. In
Proceedings of International Conference on Learning
Representations (ICLR), 2015.
[20] Wenbo Guo, Dongliang Mu, Jun Xu, Purui Su, Gang
Wang, and Xinyu Xing. LEMNA: Explaining Deep
Learning Based Security Applications. In Proceedings
of ACM SAC Conference on Computer and Communica-
tions (CCS), 2018.
[10] P. Dabkowski and Y. Gal. Real Time Image Saliency for
Black Box Classiﬁers. In Proceedings of Advances in
Neural Information Processing Systems (NIPS), 2017.
[21] K. He, G. Gkioxari, P. Dollár, and R. Girshick. Mask R-
CNN. In Proceedings of IEEE International Conference
on Computer Vision (ICCV), 2017.
1672    29th USENIX Security Symposium
USENIX Association
[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. Deep Residual Learning for Image Recognition.
In Proceedings of IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2016.
[23] Warren He, James Wei, Xinyun Chen, Nicholas Carlini,
and Dawn Song. Adversarial Example Defenses: En-