# The Anacapa System

**Authors:**
- Ruppert R. Koch
- Sanjay Hortikar
- Sivaguru Sankaridurg
- Paul Ngan
- Michael O. Neary
- Dirk Wagner

**Affiliation:**
Eternal Systems, Inc.
5290 Overpass Road, Building D
Santa Barbara, CA 93111
Emails: {ruppert, shortikar, siva, pngan, mneary, dirk}@eternal-systems.com

## Abstract
The Anacapa system is an infrastructure designed to protect applications against faults and provide recovery mechanisms. Specifically, Anacapa offers node and process fault detection, checkpointing of application and operating system state, automatic restart of failed processes, automatic failover of failed nodes, and automatic migration of processes back to their original location after repair. This system will be demonstrated using a Voice over IP (VoIP) application running on Carrier Grade Linux, showcasing checkpointing, fault detection, and recovery from a checkpoint when a process or node fault is injected.

## 1. Introduction
The Anacapa system provides protection against process and node faults for a wide range of applications, including those in the defense/aerospace, communications, transportation, industrial control, and medical industries. Anacapa periodically checkpoints application processes and, if a process fault occurs, automatically restarts the process on the same or a different node. When a process is restarted, the last valid checkpoint is used to restore the application to its previous state. Anacapa checkpoints not only the state of the application process but also the state that the operating system maintains on behalf of the application process, such as the process environment, file descriptors, and pipes.

Key features of the Anacapa system include:
- Checkpointing of application and operating system state
- Infrastructure-initiated, application-initiated, and infrastructure-assisted checkpointing
- Node and process fault detection
- Process deadlock detection
- Automatic restart of failed processes
- Automatic failover of failed nodes
- IP takeover and IP failback
- Multiple network support
- Subscription-based fault notification
- User-controlled migration of processes
- Automatic migration of processes to their original location after repair

## 2. Anacapa Fundamentals

### 2.1. Basic Concepts
Anacapa supports three fundamental concepts that facilitate flexible system configuration: service groups, service addresses, and home locations.

- **Service Group:** A service group consists of multiple application processes running on the same node that collectively provide a service to the user. If one process in a service group is transferred to another node, all other processes in the group are stopped and transferred to that node.
- **Service Address:** To make the failure of a service unit transparent to users, Anacapa provides virtual IP addresses for the service unit, known as service addresses. A service unit may have one or more service addresses, which are unique and distinct from, but aliased to, the IP addresses of the node hosting the service unit. When Anacapa migrates a service unit to another node, it moves the service address to the new node, while the IP addresses remain with the original node. Users continue to interact with the migrated service unit through the service addresses.
- **Home Location:** All processes and service groups have a home location, typically the node where the process or service group was initially started. If a process or service group runs on a node other than its home node and the home node becomes available, the process or service group migrates back to its home node.

### 2.2. System Policies
System policies are divided into two categories: process policies and node policies. Process policies migrate with the process, while the user can specify whether node policies are transferred from a faulty node to a standby node in case of a node fault.

#### Fault Recovery Policies
- If a process \( p \) fails, attempt to restart it \( k \) times. If it continues to fail, restart \( p \) and additional support processes \( p_1, p_2, p_3, \ldots \) on the first available node in the list \( n_1, n_2, n_3, \ldots \). If a process \( p \) in a service group \( g \) fails, attempt to restart it \( k \) times. If it continues to fail, stop all processes in \( g \), and restart them and the additional support processes \( p_1, p_2, p_3, \ldots \) on the first available node in the list \( n_1, n_2, n_3, \ldots \).
- If a process \( p \) fails, attempt to restart it \( k \) times. If it continues to fail, abandon the node \( n \) on which \( p \) ran and declare \( n \) to be faulty.
- Assign a service address to process \( p \) or group \( g \). If \( p \) or \( g \) is transferred to another node, transfer the service address with it.
- If a failed node \( n \) comes back up, migrate all processes \( p \) and service groups \( g \) that specify \( n \) as their home location back to \( n \).
- If a node \( n \) fails and comes back up more than \( k \) times, abandon node \( n \) and readmit \( n \) only after manual intervention by the system administrator.

#### Checkpointing Policies
- **Infrastructure-Initiated Checkpointing:** The infrastructure forces the application to take a checkpoint every \( T \) seconds.
- **Application-Initiated Checkpointing:** The application decides when to take a checkpoint.
- **Infrastructure-Assisted Checkpointing:** The infrastructure asks the application to take a checkpoint every \( T \) seconds.

#### Fault Detection Policies
- **Process Heartbeat Interval and Fault Detection Timeout:** Process heartbeats are generated by the process and consumed by the local Availability Manager.
- **Node Heartbeat Interval and Fault Detection Timeout:** Node heartbeats are generated and consumed by the Availability Manager on a nodeâ€™s failover list.

## 3. Anacapa System Design
The design of the Anacapa system is illustrated in Figure 1. Nodes are categorized as active and standby. Active nodes run processes during normal conditions, while standby nodes do not. In the event of an active node failure, processes on that node can be divided among other active nodes or transferred to a standby node. The figure shows N+M redundancy with \( N \) active nodes and \( M \) standby nodes, where \( N = 5 \) and \( M = 2 \).

The Anacapa system includes a Fault Detector (FD), Availability Manager (AM), Fault Notifier (FN), and System Configurator (SC). Each node runs an Availability Manager process, which monitors the health of local processes and manages their checkpoints. The Availability Manager can restart a failed process locally or coordinate a restart on a remote node. The System Configurator and Fault Notifier run as user processes on one of the active nodes and are checkpointed and restarted by the Availability Manager.

Users can set system policies and configurations via the System Configurator, which stores them in an XML policy file. There is one policy file per node, which can be created offline using a text editor or the Anacapa Configuration Tool (ACT). The ACT allows users to read, write, and modify policy files, get the state of the system or specific nodes, read system log files, manually transfer processes or groups, shut down nodes, reboot them, add new node IDs, and assign service addresses. Users can define which nodes should serve as standby nodes on a per-node or per-process basis. If multiple locations are specified for a process or node, checkpoints must be stored on all such nodes.

At startup, the Availability Manager reads the policy file, which must be accessible to the Availability Manager, either on the local disk or a shared file system.

## 4. System Configurator
The system runs a single instance of the System Configurator, which communicates with all instances of the Availability Manager and the ACT via TCP/IP. The Availability Manager treats the System Configurator like any other user application, periodically checkpointing and restarting it in the event of a fault.

The System Configurator does not have direct access to the nodes' policy files without a shared file system. Its main task is to relay information from the ACT to the Availability Managers and is not involved in automatic recovery or repair activities. The System Configurator acts as a stateless proxy, forwarding requests and replies between the ACT and the Availability Manager. It opens a TCP server socket to accept connections from the ACT on the port specified in the policy file and uses a service address that is aliased by the node currently running the System Configurator.

Although the System Configurator acts as a client to the Availability Managers, the Availability Managers must connect to it. Thus, the System Configurator does not need to know which Availability Managers are active. Figure 2 illustrates the System Configurator setup.

## 5. Fault Detectors
Anacapa's Fault Detectors detect three types of process faults: crashes, deadlocks, and livelocks. For process crash detection, Anacapa uses a hook into the operating system. If the operating system removes a process control block, the local Availability Manager is notified and checks whether the process exited voluntarily or involuntarily.

For detecting process deadlocks or livelocks, Anacapa uses heartbeating. Processes send heartbeats to the local Availability Manager on a regular basis. If heartbeats stop, the Availability Manager declares the process faulty and initiates a local or remote restart.

## 6. Availability Manager
The Availability Manager of an active node passes local process information and checkpoints to the Availability Manager running on other nodes, which can be active or standby. The checkpoint of a process is stored at multiple locations in the system. One copy is stored in the local Availability Manager and then transferred over the network to all nodes on the process's and node's failover lists that might restart the process if the active node fails.

When a fault is detected, the Availability Manager sends notifications and coordinates the necessary recovery actions.