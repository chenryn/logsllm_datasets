title:The Anacapa System
author:Ruppert R. Koch and
Sanjay Hortikar and
Sivaguru Sankaridurg and
Paul Ngan and
Michael O. Neary and
Dirk Wagner
The Anacapa System 
   R. Koch, S. Hortikar, S. Sankaridurg, P. Ngan, M. Neary, D. Wagner 
Eternal Systems, Inc. 
5290 Overpass Road, Building D 
 Santa Barbara CA 93111 
{ruppert, shortikar, siva, pngan, mneary, dirk}@eternal-systems.com 
Abstract 
The Anacapa system is an infrastructure that protects an 
application  against  faults  and  that  provides  recovery  from 
faults.  In  particular,  Anacapa  provides  node  and  process 
fault  detection,  checkpointing  of  application  and  operating 
system state, automatic restart of failed processes, automatic 
failover  of  failed  nodes,  and  automatic  migration  of 
processes  to  their  original  location  after  repair.    The 
Anacapa system will be demonstrated using a Voice over IP 
(VoIP)  application  running  on  Carrier  Grade  Linux.    The 
demonstration  will  show  checkpointing  of 
the  VoIP 
application, as  well as fault detection and recovery from a 
checkpoint, when a process or node fault is injected.   
1. 
Introduction 
The Anacapa system provides protection against process 
and node faults for a wide variety of applications including 
applications 
in  the  defense/aerospace,  communications, 
transportation, industrial control, transportation and medical 
industries.    Anacapa  periodically  checkpoints  application 
processes  and,  if  a  process  fault  occurs,  automatically 
restarts the process on the same node or on a different node. 
When a process is restarted, the last valid checkpoint is used 
to  restore  the application to  the  state it had at  the  time  the 
checkpoint  was  taken.    Anacapa  checkpoints  not  only  the 
state  of  the  application  process,  but  also  state  that  the 
operating  system  maintains  on  behalf  of  the  application 
process, including the process environment, file descriptors 
and pipes.   
The Anacapa system provides: 
•  Checkpointing of application and operating system state  
• 
Infrastructure-initiated, application-initiated and 
infrastructure assisted checkpointing 
•  Node and process fault detection 
•  Process deadlock detection 
•  Automatic restart of failed processes 
•  Automatic failover of failed nodes 
• 
IP takeover and IP failback 
•  Multiple network support 
•  Subscription-based fault notification 
•  User-controlled migration of processes 
•  Automatic  migration  of  processes  to  their  original 
location after repair. 
2. 
Anacapa Fundamentals 
2.1.  Basic Concepts 
Anacapa  supports  three  concepts  that  provide  easy  and 
flexible  system  configuration:  service  groups,  service  
addresses and home location. 
Service group 
A  service  group  constitutes  multiple  application 
processes running on the same node that collectively provide 
a  service  to  the  user.  If  one  of  the  processes  in  a  service 
group  is  transferred  to  another  node,  all  of  the  other 
processes 
in  the  service  group  are  stopped  and  are 
transferred to that node. 
Service address 
To  make  the  failure  of  a  service  unit transparent to  the 
users, Anacapa provides virtual IP addresses for the service 
unit,  referred  to  as  service  addresses.    A  service  unit  may 
have  one  or  more  service  addresses.    These  addresses  are 
unique and are distinct from, but aliased to, the IP addresses 
of  the  node  that  hosts  the  service  unit.    When  Anacapa 
migrates a service unit to another node, it moves the service 
address of the service unit to the other node, whereas the IP 
addresses remain with the node.  Users continue to interact 
with the migrated service unit through the service addresses.  
Home location 
All processes and service groups have a home location. 
Usually, the home location is the node on which the process 
or  service  group  was  started  originally.  If  a  process  or 
service group currently runs on a node other than its home 
node  and  the  home  node  comes  up,  the  process  or  service 
group migrates back to its home node. 
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
2.2. 
System Policies 
System policies fall into two categories: process policies 
and node policies. Process policies migrate with the process. 
The  user  can  specify  whether  node  policies  are  transferred 
from a faulty node to a standby node in case of a node fault. 
Fault recovery policies 
• 
If  a  process  p  fails,  try  to  restart  it  k  times.  If  it 
continues  to  fail,  restart  p  and  the  additional  support 
processes p1, p2, p3, … on the first available node of the 
list n1, n2, n3, … If a process p in a service group g fails, 
try  to  restart  it  k  times.  If  it  continues  to  fail,  stop  all 
processes  in  g,  and  restart  them  and  the  additional 
support  processes  p1,  p2,  p3,  …  on  the  first  available 
node in the list n1, n2, n3,  … 
If  a  process  p  fails,  try  to  restart  it  k  times.  If  it 
continues to fail, abandon the node n on which p ran and 
declare n to be faulty. 
• 
•  Assign a service address to process p or group g. If p or 
g  is  transferred  to  another  node,  transfer  the  service 
address with it.  
If a failed node n comes back up, migrate all processes p
and  service  groups  g  that  specify  n  as  their  home 
location back to n.
If a node n fails and comes back up more than k times, 
abandon  node  n  and  readmit  n  only  after  manual 
intervention of the system administrator. 
• 
• 
Checkpointing policies 
• 
Infrastructure-initiated checkpointing. The infrastructure 
forces  the  application  to  take  a  checkpoint  every  T
seconds. 
•  Application-initiated  checkpointing.  The  application 
• 
decides when to take a checkpoint. 
Infrastructure-assisted checkpointing. The infrastructure 
asks  the  application  to  take  a  checkpoint  every  T
seconds. 
Fault detection policies 
•  Process  heartbeat  interval  and  fault  detection  timeout 
for  each  process.  Process  heartbeats  are  generated  by 
the  process  and  consumed  by  the  local  Availability 
Manager. 
•  Node  heartbeat  interval  and  fault  detection  timeout  for 
each node. Node heartbeats are generated and consumed 
by the Availability Manager on a node’s failover list. 
3. 
Anacapa System Design 
The design of the Anacapa system is shown in Figure 1. 
Nodes are distinguished as active nodes and standby nodes.
Active  nodes  run  processes  during  fault-free  conditions, 
whereas standby nodes do not.  In case an active  
C onfig G UI
TCP
Subscriber
Em ail
Subscriber
UDP
Active node
Active node
Active node
A ctive node
Active node
SC
FN
AM
C HKPT
AM
C HKPT
A M
C HKPT
AM
C HKPT
A M
CH KPT
A M
CH KPT
   Standby node
AM
CH KPT
   Standby node
Figure 1: Design of the Anacapa System. 
node fails, all of the processes  on that node can be divided 
between other active nodes,  or they can be transferred to a 
standby node. The figure illustrates N+M redundancy with N 
active nodes and M standby nodes, where N = 5 and M = 2. 
The  Anacapa  system  comprises  a  Fault  Detector  (FD), 
Availability Manager (AM), Fault Notifier (FN) and System 
Configurator  (SC).  Each  node  in  the  system  runs  an 
Availability Manager process, which is started at boot time. 
The  Availability  Manager  monitors  the  health  of  the  local 
processes  and manages  the  checkpoints  of  those  processes. 
The Availability Manager restarts a failed process locally or 
coordinates  a  restart  on  a  remote  node.  The  System 
Configurator and Fault Notifier run as user processes on one 
of  the  active  nodes.  Like  any  other  process,  they  are 
checkpointed and restarted by the Availability Manager.  
The  user  can  set  the  system  policies  and  the  system 
configuration  via  the  System  Configurator,  which  stores 
them in a policy file. The format of the policy file is XML. 
There  is  one  policy  file  per  node.  The  policy  files  can  be 
created offline by a generic text editor, or with the Anacapa 
Configuration  Tool  (ACT),  which  generates  a  policy  file 
based on information that the user supplied via a GUI. The 
user can use the ACT to read, write and modify policy files, 
get the state of the system or of a specific node, read system 
log files, manually transfer processes or groups, shut down 
nodes or reboot them, add new node IDs, and assign service 
addresses. The user can define which nodes should serve as 
a standby node  on a per-node or a per-process  basis. If the 
user specifies multiple locations for a process or a node, the 
checkpoints of that process must stored on all such nodes. 
At startup, the Availability Manager reads the policy file. 
The  file  must  be  accessible  to  the  Availability  Manager, 
which means it must reside on the local disk if a shared file 
system is not available. 
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Config application
GUI interface
parser
config 
client
TCP
offline
policy 
  file
   system 
configurator
TCP
TCP
TCP
AM
AM
AM
R/W
config 
server
parser
policy 
  file
R/W
config 
server
parser
policy 
  file
R/W
config 
server
parser
policy 
  file
Manager  is  active.  The  System  Configurator  opens  a  TCP 
server socket that accepts connections from an Availability 
Manager on the port that is specified in the policy file.  
If  a  Availability  Manager notices  that  its  connection  to 
the System Configurator is lost or closed, it tries to connect 
again. It retries periodically until it succeeds. 
If a node fails and comes back up, it tries to reconnect to 
the  System 
the  System  Configurator.  At 
Configurator might still maintain the original connection to 
that  node.  The  System  Configurator  then  terminates  the 
original  connection  and 
the  Availability 
Manager. 
reconnects 
that 
time, 
Figure 2:  Anacapa System Configurator. 
5. 
Fault Detectors 
4. 
System Configurator 
The  system  runs  a  single  instance  of  the  System 
Configurator. The System Configurator communicates with 
all instances of the Availability Manager via TCP/IP. It also 
communicates  with  the  ACT  via  TCP/IP.  The  Availability 
Manager treats the System Configurator like any other user 
application.  It  checkpoints  the  System  Configurator  state 
periodically,  and  restarts  the  System  Configurator  in  the 
event of a fault. 
The System Configurator does not have direct access to 
the  nodes’  policy  files  because  such  access  is  not  feasible 
without  a  shared  file  system.  The  System  Configurator’s 
main  task  is  to  relay  information  from  the  ACT  to  the 
Availability  Managers.  It  is  not  involved  in  any  automatic 
recovery or repair activities of the system. 
The  System  Configurator  is  a  stateless  proxy  that  does 
nothing more than forward requests and replies between the 
ACT  and 
the  Availability  Manager.  The  System 
Configurator acts as a single point of  contact for the ACT. 
Figure 2 illustrates the System Configurator setup. 
The System Configurator process acts as a server to the 
ACT. It opens a TCP server socket that accepts connections 
from the ACT on the port that is specified in the policy file. 
The System Configurator uses its own address. This is a 
service address that is aliased by the node that currently runs 
the  System  Configurator.  The  Availability  Managers  learn 
about  the  System  Configurator  address  through  the  policy 
file, which contains that address. 
Although the System Configurator acts as a client to the 
Availability  Managers,  the  Availability  Managers  must 
connect  to  the  System  Configurator.  Thus,  the  System 
Configurator  does  not  need  to  know  which  Availability 
The  Fault  Detectors  in  Anacapa  detect  three  kinds  of 
process faults: crashes, deadlocks and livelocks.  To detect a 
process crash fault, Anacapa uses a hook into the operating 
system.  If the  operating system removes a process control 
block,  the  local  Availability  Manager  is  called.    The 
Availability  Manager  then  checks  whether  the  process  has 
exited voluntarily or involuntarily. 
To detect a process deadlock  or livelock, Anacapa uses 
heartbeating.    A  process  sends  heartbeats  to  the  local 
Availability  Manager  on  a  regular  basis.    If  the  heartbeats 
stop,  the  Availability  Manager  declares  the  process  to  be 
faulty and initiates a local or remote restart of the process. 
6. 
Availability Manager 
The Availability Manager of an active node passes local 
process  information  and  checkpoints  to  the  Availability 
Manager  running  on  other  nodes,  which  can  be  active  or 
standby  nodes.    The  checkpoint  of  a  process  is  stored  at 
multiple  locations in  the  system.  One  copy  is  stored  in  the 
local Availability Manager. From there, it is transferred over 
the network to all of the nodes on the process’s and node’s 
failover  lists  that  will  possibly  restart  the  process,  if  the 
active node fails.  
When a fault is detected, the Availability Manager sends 