between the app and the device. Even large vendors like TP-
Link support both cloud and peer-to-peer mode for network
outage and privacy reasons. This enables us to have a good es-
timation of the device interfaces for many of the IoT devices,
especially vulnerable ones, that are sold on the market.
We used a backward approach to compute the network
interfaces of an app, starting from the network response mes-
sages that the app may receive, as these messages are informa-
tion output by the device. We ﬁrst identify message handling
functions in the app and statically decide what the response
message may look like. We then identify the request that may
trigger the response. Finally, we partially instantiate and exe-
cute the app code to reconstruct the request [12, 56]. Figure 2
shows an example of the request and response extracted from
the mobile companion app com.Zengge.LEDWiﬁMagicColor
of Zengge Wi-Fi Bulb. With many different pairs of requests
and responses (e.g., with UDP/48899, TCP/5577 of the bulb
and also the cloud server *.magichue.net), we obtain a good
estimation of the device interfaces.
Figure 2: Interfaces of Zengge Wi-Fi Bulb
Response Extraction. We rely on symbolic execution [33]
to estimate what the response messages from a device may
look like, without actually running the device. We ﬁrst
built a Control Dependency Graph (CDG) and Data De-
pendency Graph (DDG) of a mobile companion app using
Soot [51]. We then start from standard network receiving
functions in Android (e.g., ) and forward execute the
mobile companion app symbolically. Whenever we encounter
a branch that is dependent on the content of a response mes-
sage (e.g., ﬁelds of the response are checked against a value),
we capture the check as a symbolic constraint and fork the
execution. After all executions terminate, the conjunction of
the symbolic constraints is stored as a “description” of the
response message. In order for response messages from two
devices to be similar, they have to satisfy the same set of
symbolic constraints.
One practical issue is to decide when to terminate a sym-
bolic execution. In our experience, we found that a valid
response from the device (i.e., the response passes checks
performed by the app) often triggers state changes of the app.
Such state changes could be either UI element changes (e.g.,
updating device status displayed to the user) or modiﬁcations
to the local registry (e.g., storing device information to con-
ﬁguration ﬁles, shared preferences or databases). To conﬁrm
this heuristic, we randomly sampled 200 response handling
procedures that exist in 179 apps from our app set and eval-
uated manually the impact of valid responses. Among these
responses, 162 of them had an inﬂuence on UI elements, and
76 of them resulted in modiﬁcations to the local registry (with
some overlapping cases where responses changed both); only
eight of them would not trigger such changes, but the app
stored response content (e.g., login token) in global variables.
This study shows that state changes can be a good approxima-
tion for the termination of valid response handling. We thus
mark such state changes as the point where we terminate the
symbolic execution and produce the conjunction of the sym-
bolic constraints. In addition, we supplement this method with
the observation that invalid responses are discarded quickly
by the apps (i.e., within few lines of code). We thus also set
a threshold on the number of procedures to execute before
we terminate the execution. Utilizing these two heuristics,
we could produce a small but meaningful set of constraints
that closely describe a valid response that an IoT device may
produce.
Pairing Request and Responses. The next step is to identify
the request sent by the app that will trigger the response from
the device. In many cases, the request is straightforward to
identify: it co-locates with the response message handling
functions. In other cases, however, it is trickier as the request
can be located in a different procedure or class, especially
when the communication between app and device is asyn-
chronous. In these cases, static code analysis can be limited
in identifying the matching request.
Fortunately, we observe that a matching pair of request
and response often share a large code base of their handling
functions (i.e., classes and methods used to process the re-
quest and response). Such similarities are reﬂected in the
stack at runtime. To conﬁrm this observation, we examined
the paired requests and responses for the same set of 200
1154    28th USENIX Security Symposium
USENIX Association
reconstructing program values via program slicing and exe-
cution [12, 29, 48, 56]. We adopted the Instantiated Partial
Execution (IPE) technique developed in Tiger [12] in our
platform. The advantage of using IPE is that it evaluates and
instantiates variables to concrete values if they are found to be
irrelevant to the request string thereby dramatically reducing
the number of paths need to be explored. In addition, IPE
also caches outputs of code slices and reuses the results if
applicable, further reducing the analysis complexity. By using
IPE, we were able to reduce the time needed to reconstruct a
request to under a minute.
The result produced by device interface analysis is a set
of request and response pairs. The requests are fully or par-
tially3 reconstructed request strings and responses are sets
of symbolic constraints. A device is said to have a similar
interface as another device if they both accept similar requests
and output similar responses.
2.3.2
Imprints Analysis
Device imprints (i.e., unique strings) found in an app can
help correlate different devices. We are particularly interested
in imprints that show up in the communication between the
app and the device, as they are indicative of the uniqueness
of the device. In contrast, there are also app imprints, such as
app developer emails or special class names, that identify an
app or library. However, they are less indicative of the device.
Type
device keywords
Table 1: Examples of device imprints
Device
Homeboy Wi-Fi
Security Camera
SensingTEK
Cameras
Zhongteng Smart
Home Devices
Pro1 Thermostats
Max Smart
Home Devices
Imprints
“20140930073702357”
(dir. name in ﬁrmawre)
“0622707c-da97-4286-cafe-”*
(UUID of the device family)
“Ztwy518518puy518”
(AES key)
“P0rtal@123!” (account pwd)
“qjg7ec”.internetofthings
.ibmcloud.com (MQTT orgID)
cert and comm. keys
user & pwd
special URLs
Inspired by previous work done by Costin et al. [16] that di-
rectly extracts imprints from embedded ﬁrmware images, we
also focus on four types of device imprints: device (backdoor)
keywords, certiﬁcates and keys, non-trivial usernames and
passwords, and special URLs. The method we used to iden-
tify imprints is simple: we build a Data Dependence Graph
of an app and check backward from network APIs to ﬁnd
constant strings in the app that affect parameters of those
APIs. Note that these APIs are used to communicate with
the device. In other words, we only use unique strings as
imprints if they are related to the device (i.e., they are part
of either requests to or responses from the device). A parser
later decides which category the constant strings fall into and
3Certain requests require user input (e.g., login request). In these cases, we
partially reconstruct the request with  string replacing the missing
user input.
Figure 3: Cumulative Distribution Function (CDF) of request
and response similarity
response handling procedures, and evaluated the similarities
between stack traces of the responses and the requests. Fig-
ure 3 shows the Cumulative Distribution Function (CDF) of
the Jaccard Similarity: 81% pairs of request and response
share over 61% of their stack frames, and more than half
(53%) of the request-response pairs have over 88% frames in
common. For unpaired requests and responses, the similarity
reduces to almost zero. Thus, by recording and comparing the
execution stack of the app when the app is making requests
(i.e., via concrete execution) and processing responses (i.e.,
via program dependence graph), we can pinpoint with good
accuracy, among multiple request sending functions, the one
that most likely will trigger the target response. As an exam-
ple, Figure 4 shows the stack traces of a request and response
that are used by Chuango Wi-Fi alarm system. The request
and response are matched based on the common stack frames
(e.g., those triggered by the same user click) despite being
located in different classes that run in different threads.
Figure 4: A matching asynchronous request and response in
cn.chuango.e5_wiﬁ
Request Reconstruction. After identifying a matching pair
of request and response, the next step is to reconstruct the
request string. Unlike responses, requests are produced by
the app. Therefore, we may reconstruct a complete request
string as compared to a set of symbolic constraints for re-
sponses. A number of techniques have been developed for
USENIX Association
28th USENIX Security Symposium    1155
whether or not they are commonly seen (e.g., admin for both
username and password is ignored). Table 1 shows an ex-
ample of a few imprints we collected in our dataset. When
two apps have the same imprints (and both imprints affect the
communication with devices), it serves as a strong indicator of
the similarity between devices. For instance, by using imprint
"OBJ-000165-PBKMW", we were able to correlate VStarcam
and OUSKI IP Cameras (the latter is later conﬁrmed to be a
rebranding of the former).
Although imprints can serve as strong evidence of cor-
relation, imprint analysis as a method is less applicable
in general since many times imprints of a device do not
manifest themselves in the app. For example, we were not
able to spot the existence of any magic keyword, like the
"xmlset_roodkcableoj28840ybtide" (i.e., edit by 04882 joel
backdoor in reverse) keyword used by a number of devices
for debugging purposes reported by Constin et al. [16].
This makes sense since the magic keyword is built into the
ﬁrmware images for debugging purposes, and device debug-
ging is generally not a critical functionality required for cus-
tomer facing apps. However, it highlights the limitation of
imprint analysis, and the reason why we need a fully ﬂedged
device interface analysis.
2.3.3 Fuzzy Hash Analysis
Another method we used is to assess code similarity via fuzzy
hash. Similar mobile companion apps often indicate simi-
lar devices. We thus compute ssdeep of objects found in an
app, including classes, libraries, and other types of resources
(e.g., texts), and compare the results across apps. The ben-
eﬁt of using fuzzy hash as compared to traditional hashing
algorithm (e.g., SHA1) is that we can relate objects that are
similar but are not exactly the same. Through this way, we
were able to identify a few similar devices. For example, the
companion apps of CHITCO and EDUP smart switches are
found to have 50.7% objects matched with 80/100 similarity,
and these two devices are later conﬁrmed to share similar
software. Note, however, similarities between devices do not
necessarily mean similarities in the apps. We observed in
many cases that similar devices have different apps (e.g., apps
are developed independently), and therefore cause failures
to fuzzy hash analysis. Code similarity is more useful for
identifying obvious correlations as well as for cases where
other analysis methods have some difﬁculties to apply (e.g.,
for native libraries).
2.3.4 Modularity
A special consideration we made while building the App
Analysis Engine is the modularity of the analysis. The reason
we took this extra step as compared with generating analy-
sis result per app is to accommodate the modular similarity
that often appear across IoT devices. It is common that IoT
device vendors, especially smaller ones, comprise their prod-
ucts from a number of existing modules on market, such as
hardware components from common suppliers, software built
from open source projects, binary driver code for protocols
and etc. For example, the HiFlying Wi-Fi module is used by
a number of vendors to manage Wi-Fi connectivity for their
devices. Thus it is important for our analysis to be modular
as well, in order to track device similarities and detect vulner-
ability propagation at a ﬁner granularity of individual device
components (Refer to Section 2.4 and Section 3.3 for more
details of the components that we can track).
We based our design on the observation that device compo-
nents are often managed by different code modules in the app
(e.g., class, package). Taking the previous HiFlying Wi-Fi
module as an example, devices such as BeSMART thermo-
stat that uses the module often have two separate classes,
com.hiﬂying.smartlink.v3.SnifferSmartLinkerSendAction and
com.besmart.thermostat.MyHttp, for handling Wi-Fi connec-
tion and user interaction over HTTP, respectively. We thus
infer such modularity from the app (e.g., based on class hi-
erarchy and invocation stack) and apply the above analysis
method on individual modules.
2.4 Cross-App Analysis Engine
The analysis results output by the App Analysis Engine are
stored into the App Analysis Database, which is then queried
by the Cross-App Analysis Engine. The Cross-App Analysis
Engine is designed to detect modular similarities between
different devices. In particular, the comparison is made to
detect four types of similarities: similar software components,
similar hardware components, similar protocol, and similar
backend services.
Similar Software Components. Similar device interfaces,
especially application interfaces, are indicative of strong con-
nections between software components of different devices.
For example, we were able to correlate 72 different smart
home IoT devices from 16 distinct vendors that might have
used the same version of GoAhead web server4. Such cor-
relation is powerful, as in many times security weaknesses
manifest them in software and security weakness found in one
device can directly impact the security of others. For example,
we were able to identify seven previously unreported devices
that are vulnerable to a known vulnerability, as detailed in
Section 3.3.
Another interesting phenomenon detected is device re-
branding. In the smart home IoT industry, smaller vendors
sometimes do not develop their own products. Instead, they
customize IoT devices from OEMs and resell with their own
branding. As reﬂected in the app analysis results, rebranded
devices have almost identical device interfaces across multi-
4GoAhead is a simple web server speciﬁcally designed for embedded
devices.
1156    28th USENIX Security Symposium
USENIX Association
ple modules as the original OEM devices. Although device
rebranding itself is not an issue, it complicates the security
practices in ﬁrmware update and patching. In some cases, for
example as shown in Section 3.3, a vulnerability is inherited
by the rebranded devices from the OEM but the security patch
that ﬁxes the vulnerability is not.
Similar Hardware Component. Smart home IoT devices
may be built upon similar hardware (e.g., Wi-Fi module).
Such similarities in hardware components are sometimes re-
ﬂected in device companion apps due to the need for the app
to conﬁgure or interact with the hardware component. Due to
the specialty of the hardware, such device-app interfaces can