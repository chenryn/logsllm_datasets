title:PIPSEA: A Practical IPsec Gateway on Embedded APUs
author:Jung-Ho Park and
Wookeun Jung and
Gangwon Jo and
Ilkoo Lee and
Jaejin Lee
PIPSEA: A Practical IPsec Gateway on Embedded APUs
Jungho Park†∗ Wookeun Jung†
Gangwon Jo†∗
Ilkoo Lee†
Jaejin Lee†
†Center for Manycore Programming
Department of Computer Science and Engineering
Seoul National University, Seoul 08826, Korea
Room 308, Building 138, Seoul National University, Seoul 08826, Korea
∗ManyCoreSoft Co., Ltd.
{jungho, wookeun, gangwon, ilkoo}@aces.snu.ac.kr, PI:EMAIL
http://aces.snu.ac.kr
ABSTRACT
Accelerated Processing Unit (APU) is a heterogeneous mul-
ticore processor that contains general-purpose CPU cores
and a GPU in a single chip.
It also supports Heteroge-
neous System Architecture (HSA) that provides coherent
physically-shared memory between the CPU and the GPU.
In this paper, we present the design and implementation
of a high-performance IPsec gateway using a low-cost com-
modity embedded APU. The HSA supported by the APUs
eliminates the data copy overhead between the CPU and the
GPU, which is unavoidable in the previous discrete GPU ap-
proaches. The gateway is implemented in OpenCL to exploit
the GPU and uses zero-copy packet I/O APIs in DPDK. The
IPsec gateway handles the real-world network traﬃc where
each packet has a diﬀerent workload. The proposed packet
scheduling algorithm signiﬁcantly improves GPU utilization
for such traﬃc.
It works not only for APUs but also for
discrete GPUs. With three CPU cores and one GPU in
the APU, the IPsec gateway achieves a throughput of 10.36
Gbps with an average latency of 2.79 ms to perform AES-
CBC+HMAC-SHA1 for incoming packets of 1024 bytes.
CCS Concepts
•Security and privacy → Network security;
Keywords
IPsec; APU; GPU; Cryptography; Heterogeneous comput-
ing; OpenCL
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’16, October 24-28, 2016, Vienna, Austria
c(cid:2) 2016 ACM. ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978329
INTRODUCTION
1.
The Internet Protocol Security (IPsec) [20] has been widely
used to secure applications using the Internet (e.g., e-mail,
ﬁle transfer, telnet, and web-based applications), VPN (Vir-
tual Private Networks) tunnels, and end-to-end communi-
cations between two hosts.
It is a protocol suite for se-
curing the IP traﬃc using cryptographic methods. It pro-
vides data origin authentication, data conﬁdentiality, data
integrity, and anti-reply protection at the IP layer.
Since IPsec is based on compute-intensive crypto algo-
rithms, IPsec processing requires higher performance to
achieve real-time packet processing as network traﬃc in-
creases. Thus, network systems have used special-purpose
hardware units, such as ASICs [11, 15], FPGAs [6, 7, 19],
and network processors [24, 25, 28] to accelerate network
packet processing.
Even though special-purpose hardware-based crypto-
graphic network systems achieve the desired high perfor-
mance, they have several disadvantages: high veriﬁca-
tion/validation/building cost, high revision cost, and diﬃ-
cult programming. Instead, software-based network systems
on commodity processors are considered as an alternate so-
lution. They provide cost-eﬀective packet processing, high
ﬂexibility, and high programmability. However, software-
based network packet processing suﬀers from its low perfor-
mance. To address this issue, GPGPUs (General-Purpose
computing on Graphics Processing Units) have been con-
sidered as an eﬀective solution [12, 18, 22, 23, 26, 30, 33]
because of their strong computation capability exploiting
massive parallelism.
In this paper, we present the design and implementation of
a high-performance IPsec gateway, called PIPSEA, using a
resource-constrained embedded APU (Accelerated Process-
ing Unit) [1]. The APU contains at least two and at most
four general-purpose CPU cores and a GPU in a single chip.
The APU supports HSA (Heterogeneous System Architec-
ture)
[3] that provides coherent physical shared memory
between the CPU cores and the GPU. IPsec protocols are
implemented in OpenCL [21], and the GPU performs IPsec
processing. Packet I/O in the IPsec gateway is based on
DPDK [2].
We also present an IPSec packet scheduling algorithm that
fully exploits a common GPU architecture and improves
GPU utilization signiﬁcantly. The proposed packet schedul-
1255ing algorithm conservatively assumes each incoming packet
has a diﬀerent IP connection, i.e., a diﬀerent workload. To
achieve high performance for a mix of diverse packets, it is
essential to exploit the full compute capability of a GPU
using such a packet scheduling algorithm. It works well not
only with APUs but also with high-end discrete GPUs (dG-
PUs).
To the best of our knowledge, PIPSEA is the ﬁrst practical
IPsec solution using an embedded APU. PIPSEA has the
following advantages over previous approaches that use high-
end dGPUs:
• Since PIPSEA uses a resource-constrained embedded
APU, its cost eﬀectiveness is much higher than that of
previous high-end dGPU solutions. Our IPsec gateway
provides enough throughput to a small-sized network.
It provides up to 10 Gbps connectivity (e.g., OC-192
or 10 GbE).
• Because of the HSA and our packet scheduling algo-
rithm, the packet round-trip latency of PIPSEA is bet-
ter than or comparable to that of previous high-end
dGPU approaches.
• Our IPsec gateway has a novel packet scheduling tech-
nique to improve GPU utilization. It considers real-
world IP traﬃc where each packet has a diﬀerent work-
load, and completely removes control-ﬂow divergence
due to diﬀerent workloads.
• Our IPsec gateway is scalable. A higher-throughput
IPsec gateway can be built with multiple moderate-
throughput IPsec gateways (e.g., using channel bond-
ing). This also provides high availability. Using the
gateway composed of multiple moderate gateways with
a failover feature helps to improve availability.
With three CPU cores and one GPU in the APU, the
IPsec gateway achieves a throughput of 10.36 Gbps with an
average latency of 2.79 ms to perform AES-CBC+HMAC-
SHA1 for incoming packets of 1024 bytes. With an average
latency of 3.71 ms, it achieves a throughput of 10.66 Gbps for
incoming packets of random lengths. For a random mix of
six crypto algorithms, it achieves a throughput of 17.42 Gbps
with an average latency of 3.92 ms for incoming packets of
1280 bytes.
The rest of the paper is organized as follows. The next
section discusses related work. Section 2 describes the back-
ground to understand our implementation. Section 3 presents
the design and implementation of PIPSEA. Section 4 eval-
uates the performance of PIPSEA and discuss the cost ef-
fectiveness of our solution. Finally, Section 6 concludes the
paper.
2. BACKGROUND
We brieﬂy introduce the Accelerated Processing Unit (APU)
and Heterogeneous System Architecture (HSA)[3] in this
section. We also brieﬂy describe the OpenCL programming
model and GPU architectures in the context of OpenCL.
2.1 APU and HSA
Many processor vendors including Intel, AMD, NVIDIA,
and ARM have released heterogeneous multicore processors
where a GPU is integrated in the same chip. For example,
Compute(cid:3)Device
Device(cid:3)Memory
Global(cid:3)Memory
Constant(cid:3)Memory
Local(cid:3)Memory
Local(cid:3)Memory
Compute(cid:3)Unit(cid:3)(CU)
Private
Private
Memory
Memory
...
Compute(cid:3)Unit(cid:3)(CU)
Private
Private
Memory
Memory
...
...
PE
PE
PE
PE
Compute(cid:3)
Device
Compute(cid:3)
Device
...
Compute(cid:3)
Device
Host(cid:3)Processor
Main(cid:3)Memory
Figure 1: OpenCL platform model.
Intel’s general-purpose Skylake processor integrates multi-
ple x86 CPU cores and its HD graphics processor in a single
chip. AMD’s APU also has multiple x86 CPU cores and its
Radeon GPU in a single chip.
However, the way of programming the integrated GPU
(iGPU) in a heterogeneous multicore processor has been the
same as that of the legacy GPGPU that uses a discrete GPU
(dGPU). Thus, to perform computation on the iGPU or
dGPU, data in the main memory (the CPU side) must be
copied to the GPU-side memory if necessary. Moreover data
coherence/consistency must be explicitly managed. This is
because there is no real physically shared memory between
the CPU and the GPU. To overcome such ineﬃciencies, the
HSA has been proposed by the HSA foundation that is sup-
ported by many vendors, such as ARM, AMD, Qualcomm,
TI, Samsung, etc.
The major goal of the HSA is to provide an eﬃcient in-
tegrated environment for diﬀerent kinds of computing de-
vices such as CPUs and GPUs to system designers and soft-
ware programmers. The legacy GPGPU (i.e., dGPU) has
the most signiﬁcant data transfer and coherence/consistency
management overhead between the CPU and the GPU be-
cause each of them has a separate memory and the memo-
ries are connected via PCI-E bus. The iGPU has less over-
head than the dGPU because the CPU-side memory and
the GPU-side memory are connected by a data bus. The
HSA enabled heterogeneous multicore processors provide a
uniﬁed memory that allows coherent data sharing across all
processors in the chip.
2.2 OpenCL and GPU Architectures
OpenCL (Open Computing Language) and NVIDIA
CUDA [27] are the most widely used programming mod-
els for GPGPUs. OpenCL provides a common abstraction
layer to the programmer across diﬀerent architectures, such
1256as multicore CPUs, GPUs, Intel Xeon Phi coprocessors, FP-
GAs, and DSPs. This is a major advantage of using OpenCL
over CUDA. Since the GPU in PIPSEA is programmed with
OpenCL, our solution runs on diﬀerent devices from diﬀerent
vendors. Note that NVIDIA GPUs also support OpenCL.
OpenCL platform model. As shown in Figure 1, the
OpenCL platform model consists of a host processor con-
nected to one or more compute devices (e.g., GPUs). Each
compute device contains one or more compute units (CUs)
and has compute device memory that is not visible to other
compute devices. The compute device memory consists of
global memory and read-only constant memory. Each CU
contains one or more processing elements (PEs) and local
memory. A PE is a processor and has its own private mem-
ory. PEs in the same CU share the local memory, and the
local memory is not visible to other CUs. One of the general-
purpose CPU cores in the APU becomes the host processor
in PIPSEA.
OpenCL execution model. An OpenCL application con-
sists of a host program and OpenCL programs both of which
are based on C. The OpenCL program is a set of kernels. A
kernel performs computation on the PEs within a compute
device. The host program executes on the host processor in
parallel with kernels and coordinates kernel executions.
An execution instance of a kernel is called a work-item. A
work-item has a unique ID and can be treated as a thread.
One or more work-items are grouped in a work-group. A
work-group has also a unique ID. Before a kernel is launched
by the host program, the host program deﬁnes the total
number of work-items, the total number of work-groups, and
the work-group size of the kernel. The OpenCL runtime
distributes the kernel workload to CUs in the target device.
The granularity of the distribution is a work-group. Work-
items in a work-group execute concurrently in an SPMD
(Single Program, Multiple Data) manner on the PEs of the
associated CU. That is, they are context switched.
GPU architectures. OpenCL’s compute device architec-
ture is an abstraction of a common GPU architecture. For
example, a CU and a PE correspond to a Streaming Mul-
tiprocessor (SM) and Stream Processor (SP) of NVIDIA
GPUs, respectively. A CU is typically called a GPU core.
The GPU in PIPSEA contains 8 GPU cores, and each core
contains 64 PEs. The GPU also has the same memory ar-
chitecture as that of OpenCL: global, constant, local, and
private. In general, the access latencies of local and private
memory are much lower than those of global and constant
memory.
Hardware context switch. To hide long latency instruc-
tions such as global memory accesses, GPUs provide a hard-
ware context-switch mechanism on CUs. The context switch
unit on a CU is a warp (or wavefront) that is a set of work-
items in a single work-group. Thus, a single work-group may
consist of multiple warps running concurrently (i.e., context
switched) on the same CU. The size of a warp is typically
32 for NVIDIA GPUs and 64 for AMD GPUs.
Work-group scheduling. An OpenCL kernel is typically
executed by a number of work-groups that is larger than the
number of CUs in the GPU. The hardware scheduler in the
GPU dynamically determines which work-group is assigned
to which CU [8]. A work-group that has a smaller ID value
typically has a priority over an idle CU.
cycles
work-item
0
work-item
1
work-item
2
work-item
3
inst0
T
F
F
T
if ( inst1 ) 
{
inst2
} else {
inst3
}
inst4
Figure 2: Control-ﬂow divergence.
Control-ﬂow divergence. When a warp is running on
a CU, all work-items in the warp share a single program
counter and execute the same instruction in a SIMD (Single
Instruction, Multiple Data). This implies that all the work-
items in the warp should take the same execution path. If
some work-items in a warp take a diﬀerent execution path
due to a conditional branch, instructions in the path are
predicated on the branch condition; if the condition is false,
the instruction is suppressed, even though all the work-items
follow the same execution path. This phenomenon in a GPU
is called control-ﬂow divergence. However, if the condition
of a branch is evaluated to the same value for all work-items
in a warp, control-ﬂow divergence does not occur.
Figure 2 shows an example of control-ﬂow divergence.
There is a conditional branch due to an if statement. All