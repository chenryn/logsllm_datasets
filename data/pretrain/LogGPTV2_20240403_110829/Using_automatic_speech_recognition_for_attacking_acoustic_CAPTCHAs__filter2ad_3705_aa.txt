title:Using automatic speech recognition for attacking acoustic CAPTCHAs:
the trade-off between usability and security
author:Hendrik Meutzner and
Viet-Hung Nguyen and
Thorsten Holz and
Dorothea Kolossa
Using Automatic Speech Recognition for Attacking
Acoustic CAPTCHAs: The Trade-off between Usability and
Security
Hendrik Meutzner, Viet-Hung Nguyen, Thorsten Holz, Dorothea Kolossa
Horst Görtz Institute for IT-Security (HGI), Ruhr-University Bochum
{hendrik.meutzner, viet.nguyen-c7k, thorsten.holz, dorothea.kolossa}@rub.de
ABSTRACT
A common method to prevent automated abuses of Internet
services is utilizing challenge-response tests that distinguish
human users from machines. These tests are known as CAP-
TCHAs (Completely Automated Public Turing Tests to Tell
Computers and Humans Apart) and should represent a task
that is easy to solve for humans, but diﬃcult for fraudulent
programs. To enable access for visually impaired people,
an acoustic CAPTCHA is typically provided in addition to
the better-known visual CAPTCHAs. Recent security stud-
ies show that most acoustic CAPTCHAs, albeit diﬃcult to
solve for humans, can be broken via machine learning.
In this work, we suggest using speech recognition rather
than generic classiﬁcation methods for better analyzing the
security of acoustic CAPTCHAs. We show that our attack
based on an automatic speech recognition system can suc-
cessfully defeat reCAPTCHA with a signiﬁcantly higher suc-
cess rate than reported in previous studies.
A major diﬃculty in designing CAPTCHAs arises from
the trade-oﬀ between human usability and robustness against
automated attacks. We present and analyze an alternative
CAPTCHA design that exploits speciﬁc capabilities of the
human auditory system, i.e., auditory streaming and toler-
ance to reverberation. Since state-of-the-art speech recog-
nition technology still does not provide these capabilities,
the resulting CAPTCHA is hard to solve automatically. A
detailed analysis of the proposed CAPTCHA shows a far
better trade-oﬀ between usability and security than the cur-
rent quasi-standard approach of reCAPTCHA.
1.
INTRODUCTION
In order to limit or even prevent automated abuse of on-
line services (e.g., automated account creation or crawling),
it is necessary to distinguish human users from programs.
A common approach is to rely on a so-called Completely
Automated Public Turing Test to Tell Computers and Hu-
mans Apart (CAPTCHA) that should be easy to solve by
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciﬁc permission and/or a fee. Request
permissions from Permissions@acm.org.
ACSAC ’14 New Orleans, Louisiana, USA
Copyright 2014 ACM 978-1-4503-3005-3/14/12 ...$15.00.
http://dx.doi.org/10.1145/2664243.2664262
humans, but diﬃcult to break by machines [5, 29, 30]. Such
challenge-response tests are predominantly based on some
kind of obscured graphic such as a highly distorted text that
has to be recognized by the user. Some tests also require the
user to solve simple mathematical equations [7] or to identify
speciﬁc objects within an image [20]. The security proper-
ties of such visual CAPTCHAs have been discussed several
times within the last decade [14, 31, 32] and various security
improvements have been proposed [6, 10, 24].
To enable access for visually impaired users and to be
applicable for non-graphical devices, most Internet services
also provide an acoustic veriﬁcation scheme in addition to
visual CAPTCHAs. Up to now, acoustic CAPTCHAs have
received less scientiﬁc attention compared to the visual ones
and existing studies have demonstrated that most acoustic
CAPTCHAs can be easily broken [3, 4, 22, 26]. Thus, a
major problem arises in practice from the fact that many
popular Internet services make use of acoustic CAPTCHAs
that are possibly vulnerable, rendering any supplementary
image-based CAPTCHA irrelevant.
We consider an acoustic CAPTCHA to be broken if an al-
gorithm is able to ﬁnd the correct sequence of words in the
audio signal, which we will refer to as transcription later on.
Therefore, the crucial measure for the security strength of
a CAPTCHA is given by the success rate for transcribing
the signals in an automated manner. Here, the deﬁnition
of a plausible value for the maximum allowed success rate
is highly controversial, as one has to consider a trade-oﬀ
between human usability and robustness against automated
attacks. Some authors require the success rate of an auto-
matic solver to be less than 0.01 % in order to render the
CAPTCHA secure [5], whereas other studies refer to a max-
imum allowed success rate of 1 % [3] or 5 % [26]. However,
the deﬁnition highly depends on the underlying threat model
that arises from several properties and resources of the at-
tacker. Here, the degree of theoretical knowledge about sig-
nal processing and machine learning as well as the number
of available IP addresses will inﬂuence the success of an au-
tomated attack signiﬁcantly. For our purposes, we assume
that all potential vulnerabilities are induced by the CAP-
TCHA itself (i.e., insecure implementations are not within
the scope of this work).
Preliminaries and Prior Work.
Acoustic CAPTCHAs are commonly based on a sequence
of words where the underlying lexicon is often limited only
to digits between zero and nine, which can be beneﬁcial for
non-native speakers or just to keep the user instructions sim-
ple (e.g., “please type in every digit you hear”). However,
the security aspect of such small-vocabulary-based CAPT-
CHAs becomes highly questionable as state-of-the-art ma-
chine learning methods are fairly advanced, especially when
the number of individual words to be recognized is low.
Previous studies have performed a security analysis of
available acoustic CAPTCHAs by using automated solvers.
Bursztein et al. [3] and Tam et al. [26] utilize a two-stage ap-
proach by ﬁrst segmenting the CAPTCHA into individual
word segments and then ﬁnding the respective word tran-
scriptions by using a generic classiﬁcation algorithm. With
such an approach, Bursztein et al. evaluated the strength
of six acoustic CAPTCHAs and found that they can break
all but one scheme: the success rate was between 41 % and
89 % for ﬁve schemes, only reCAPTCHA [12] (as of 2010)
withstood their attack since the success rate was only 1.5 %.
Sano et al. [22] demonstrate that a newer version of reCAP-
TCHA (as of April 2013) can be broken with a success rate
of 52 % by using automatic speech recognition (ASR).
Up to now, only a limited number of studies explored
secure design strategies for acoustic CAPTCHAs. Recent
studies analyze the eﬀectiveness of artiﬁcial signal distor-
tion [1, 16] or suggest to use speech signals that exhibit a
poor overall quality like old radio broadcast recordings [25].
Schlaikjer [23] proposed a CAPTCHA where the task is to
recognize open-domain speech1.
As discussed above, attacking CAPTCHAs in an auto-
mated fashion generally represents a machine learning prob-
lem that can be tackled by various methods. Hence,
it
is not only practical but also indispensable to exploit a
broad range of existing weaknesses in current machine learn-
ing techniques when designing secure acoustic CAPTCHAs.
Obviously, the introduction of artiﬁcial distortions into a
speech signal can lower the performance of an automated
solver. The diﬃculty lies in distorting the signals such that
the speech intelligibility is preserved.
Our Approach.
We consider automated attacks on acoustic CAPTCHAs
as a speech recognition task and demonstrate how state-of-
the-art ASR techniques can be utilized to accomplish it. A
major beneﬁt of using ASR arises from the fact that the
temporal characteristics of speech are inherently considered
and incorporated by the employed algorithms (cf. Sec. 2) as
compared to a generic classiﬁcation algorithm.
Bursztein et al. reported [4] that the ASR-based approach
performed worse than the classiﬁcation-based approach for
attacking the eBay CAPTCHA. We assume that this ﬁnding
is based on the authors using pre-trained acoustic models—
freely available on the Internet—without adapting the mod-
els to the CAPTCHA signals.
In contrast to previous work [3, 26, 22], our approach is
not based on a pre-segmentation of the CAPTCHA into indi-
vidual parts. This is advantageous as we do not require any
task-speciﬁc adaption of the segmentation stage, rendering
the CAPTCHA solver more general.
The results indicate that our method clearly outperforms
previous approaches based on segmentation and word recog-
nition: we achieve a success rate of 19.19 % for the older ver-
sion of reCAPTCHA analyzed by Bursztein et al. [3] (13×
1Open-domain speech covers complex scenarios (e.g., lec-
tures and talks) and it is not limited to a speciﬁc task.
improvement). We can also break the current reCAPTCHA
with a success rate of 62.8 % which is 11 % higher than the
success rate reported for reCAPTCHA by Sano et al. [22].
In addition, we show that the human success rate for the
current reCAPTCHA scheme is 38.4 % below that of our au-
tomated CAPTCHA solver, rendering the trade-oﬀ between
usability and security fairly disadvantageous.
Furthermore, we systematically explore how CAPTCHAs
can be improved upon to attain the original goal of being
much more intelligible to humans than to machines. More
speciﬁcally, we have generated CAPTCHAs by exploiting
the auditory perception of humans in the design process.
Our proposed CAPTCHA achieves a far better trade-oﬀ be-
tween usability and security as compared to the current re-
CAPTCHA scheme.
Our Contributions.
In summary, our contributions in this work are as follows:
• We demonstrate that speech recognition methods can
yield considerable performance improvements for solv-
ing acoustic CAPTCHAs in comparison to a classiﬁ-
cation-based approach as it is used by Bursztein et
al. [3]. To allow for a fair comparison, our analysis is
based on a previous reCAPTCHA scheme from 2010.
• We analyze the current reCAPTCHA scheme and pre-
sent the respective human recognition scores that were
obtained from a listening test. In this context we dis-
cuss the usability of the CAPTCHA by comparing the
performance of humans vs. machines.
• We propose alternative design strategies for creating
more secure acoustic CAPTCHAs. These design strate-
gies are motivated by the auditory perception of hu-
mans, where we exploit human abilities that current
machine learning algorithms are lacking to a certain
degree. We also conduct a security analysis of the pro-
posed design strategies by using an automated solver
and present the results of a listening experiment to
examine human usability.
2. TECHNICAL BACKGROUND
State-of-the-art speech recognizers are usually based on
hidden Markov models (HMMs) or artiﬁcial neural networks
(ANNs) [11, 15, 18], which is advantageous compared to a
generic classiﬁcation-based approach. This is due to the fact
that speech represents a slowly time-varying process, so it is
reasonable to incorporate algorithms that can handle time
series by nature.
Our attacks are conducted by using HMM-based speech
recognition. For convenience, we make use of the hidden
Markov model toolkit (HTK) [33], which represents a stan-
dard toolkit in the research community. For our approach,
all model topologies, i.e., the number of states and mix-
tures and the allowed state transitions have been manually
adjusted to be well suitable for the task and the ensuing
model training has been done from scratch.
2.1 HMM-based Speech Recognition
In order to perform speech recognition, the time-domain
representation (i.e., the waveform), of the input signal is ﬁrst
segmented into a series of overlapping frames. In practice,
typical frame lengths are in the range of 20–25 ms as speech
can be regarded to be short-time stationary within this in-
terval. The overlap between consecutive frames is preferably
50–75 % to capture the temporal evolution of the waveform
continuously. Then, a feature vector is computed for each
frame of the input signal. Two prominent types of features
for ASR are Mel frequency cepstral coeﬃcients (MFCC) [19,
21] or perceptual linear prediction (PLP) coeﬃcients [13].
These features are motivated by several properties of the
auditory system and enable a compact representation of the
speech frames, where PLP features are more suitable for
suppressing speaker-dependent details. In order to incorpo-
rate signal dynamics, it is valuable to also include the 1st
and 2nd-order derivatives of the feature coeﬃcients into the
ﬁnal feature set. After feature extraction, the next step is
to learn suitable segment models for representing the speech
using the features and the label information of the training
material. The segment models should represent the basic
units of speech, e.g., phonemes2, in large-vocabulary appli-
cations, but for small-vocabulary tasks, it is feasible and
ultimately leads to higher accuracy, when entire word mod-
els are trained. Each segment model is represented by an
HMM that consists of a customizable number of states and
transitions, and a common topology for speech recognition
is a left-to-right model [21].
Each state q of the model possesses an individual output
probability:
b(om) = P(om|qm = i)
i = 1, 2, . . . , Q ,
(1)
where Q denotes the number of states and om is the feature
vector—the so-called observation—at frame m. For model-
ing speech, the output probabilities are usually represented
by a Gaussian mixture model (GMM) comprised of K com-
ponents:
bq(o) =
K
X
κ=1
γκ,qN (o|µκ,q, Σκ,q).
(2)
Here, N (o|µκ,q, Σκ,q) is the multivariate normal distribu-
tion parameterized by its mean vector µκ,q and its covari-
ance matrix Σκ,q. Each Gaussian component is scaled with
the mixture weight γκ,q, where
K
X
κ=1
γκ,q = 1 ∀q.
The complete HMM is deﬁned by the parameter set:
λ = (A, B, Π),
(3)
(4)
where A denotes a matrix of state transition probabilities
and B is a set containing the parameters of the output prob-
ability distributions of all states. The vector Π contains the
initial probabilities that can be given, for example, by the
word probabilities or by a ﬁxed value. The model param-
eters λ can then be estimated by optimizing a maximum
likelihood (ML) criterion:
λML = arg max
P(O|λ),
(5)
λ
algorithm [21]. Before we can solve Eq. 5, the mean vectors
µκ,q and covariance matrices Σκ,q of the respective state
output distributions need to be initialized properly. Since
ML estimation can only yield a local optimum for a ﬁnite
number of data points, the initialization of the model pa-
rameters can inﬂuence the ASR performance considerably
later on. A simple approach for initialization is to use the
global mean and variance of the entire training data, which
is often referred to as ﬂat-start initialization. When the tem-
poral occurrence of words in the training material is known,
better results are achieved by initializing each word model
individually with the corresponding statistics, which is re-
garded as a bootstrap initialization.
After model training (i.e., after the ﬁnal parameters of
Eq. 5 have been obtained), the individual segment models
are combined into a compound HMM that incorporates a
user-deﬁned grammar of the speech recognition task. For
recognizing a speech utterance, the sequence of observed fea-
tures o1 . . . oT can be decoded by searching for the best state
sequence q∗
T through the compound HMM:
1 . . . q∗
[q1 . . . qT ]∗ = arg max
1...q′
q′
T
P(q′
1 . . . q′
T |o1 . . . oT , λ).
(6)
Equation 6 can be solved by means of a Viterbi decoder,
which implements a dynamic programming approach for ﬁnd-
ing the most likely sequence of states.
2.2 Performance Assessment
In order to assess the ASR performance, it is necessary to
compare the speech recognition output with a correspond-
ing reference transcription. Here, one problem arises from
the fact that the recognition output can diﬀer from the ref-
erence transcription in its length if the underlying grammar
is not restricted to a ﬁxed number of words. However, the
comparison of two text strings of diﬀerent lengths can be
achieved by using a dynamic programming procedure. This
procedure aligns the recognizer output with the reference
transcription and then counts the word deletion (WD), in-
sertion (WI ) and substitution (WS) errors. The standard
metric for assessing ASR performance is given by the word
and sentence accuracy, which are computed by:
Word Acc. = 100 ·
W − WD − WI − WS
W
Sent. Acc. = 100 ·
SC
S
,
,
(7)
(8)
where W , S denote the total number of words and sentences
in the reference transcription, respectively, and SC is the
number of correctly recognized sentences. In the following,