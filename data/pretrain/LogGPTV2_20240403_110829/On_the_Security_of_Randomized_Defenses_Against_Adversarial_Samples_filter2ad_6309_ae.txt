ber of iterations. We see that the prediction probability starts
at 0 for all defenses, and approaches 1 as the number of itera-
tions increases. Notice that the prediction probability quickly
reaches 1 for defenses which have no randomness whereas it
grows gradually for randomness-based defenses. For instance,
8Our implementation of Region-Based Classification slightly differs from the
originally proposed version [5], namely we use the same randomization strategy
as Randomized Squeezing (which is equivalent to sampling at random from a
hypercube, cf. Section 4.3) and classify one sample instead of ensembling. Due
to averaging over 100 images, ensembling would lead to the same results as
adversary can chose to defeat a majority of samples.
20406080100120140Iterations0.00.20.40.60.81.0TargetPredictionProbabilitybit-depth5bit-depth4bit-depth3rand(δ=0)rand(δ=0.1)rand(δ=0.2)rand+bit-depth(δ=0.1)rand+bit-depth(δ=0.2)cropsize=120cropsize=90cropsize=600.00.51.01.52.02.53.03.54.0NormalizedL2distance(×10−2)0.00.20.40.60.81.0TargetPredictionProbabilitybit-depth5bit-depth4bit-depth3rand(δ=0)rand(δ=0.1)rand(δ=0.2)rand+bit-depth(δ=0.1)rand+bit-depth(δ=0.2)cropsize=120cropsize=90cropsize=606 RELATED WORK
There has been massive effort to make ML models robust to
adversarial samples, leading to a huge number of defenses pro-
posed in the last few years.9 Here, we discuss only a selection of
these proposals which we find representative of general defen-
sive principles. Following widely-adopted nomenclature, we
differentiate between certified and heuristic defenses to distin-
guish between proposals that come with provable guarantees
and those which do not.
Heuristic defenses. An early proposal is defensive distillation [30,
34], which extends the deep-learning concept of distillation
to the adversarial setting, aiming to extract knowledge from
a given neural-network architecture to improve its own re-
silience to gradient-based attacks. This proposal has been
shown ineffective [31].
A broad class of defenses attempts to detect whether a given
input is adversarial. Early methods derive statistical proper-
ties from large datasets of legitimate, respectively, adversarial
samples, and then inspect these properties to discern the ad-
versarial nature of new and unknown samples [11, 14]. Even
though these techniques where shown to be quite robust, they
are computationally expensive and require large datasets for
the reliability of statistical results. A more efficient approach is
to train a detector to specifically learn adversarial samples, as
done by MagNet [28]. MagNet relies on the assumption that
natural data lie on a manifold of significantly smaller dimen-
sion than the whole input space, while adversarial samples
fall outside the manifold. Based on this, Magnet employs a
detector which deems samples far from the manifold as ad-
versarial. A different kind of detector is instantiated by the
feature squeezing defense by Xu et al. [44] (cf. Section 2.2),
which uses the discrepancy of output predictions between
original samples and their squeezed versions to detect adver-
sarial manipulations.
We note that Randomized Squeezing, while using squeez-
ing routines, does not aim at detecting adversarial samples,
rather at making it harder for the adversary to generate suc-
cessful perturbations. Instead, our proposal can be seen as an
instantiation of so-called input transformation, which applies a
pre-processing step to the input in order to reduce the sensi-
tivity of the model to small changes in input—with the hope
of making the classifier more robust to adversarial perturba-
tions [16].
The most robust among all heuristic defenses to date ap-
pears to be adversarial training, introduced in [13] and later
extended in several works [23, 26]. Adversarial training es-
sentially finds adversarial samples by running known attacks,
and adds those samples to the training set so that the model
learns to correctly classify certain adversarial inputs. Madry et
al. [26] propose a generic training methodology targeting ro-
bustness against all low-distortion adversarial samples, i.e.,
samples generated by applying small perturbations to clean
inputs. This methodology is based on the idea that, if the
training set contains sufficiently representative adversarial
9https://nicholas.carlini.com/writing/2019/all-adversarial-example-
papers.html
samples, the resulting classifier will be able to withstand all
low-distortion attacks. Based on this, the authors heuristically
generate “sufficiently representative” adversarial samples us-
ing projected gradient descent (PDG), an attack strategy which
generalizes first-order attacks. The resulting trained networks,
based on MNIST and CIFAR datasets respectively, achieve
different levels of robustness against FGSM, PGD, and CW
attacks. Although various works demonstrated the feasibility
of this technique, adversarial training requires a large number
of samples that are expensive to generate. In addition, it cannot
resist unknown attacks.
Except for adversarial training, all the aforementioned tech-
niques were shown, in a way or another, vulnerable to adaptive
attacks.
Certified defenses. The idea of ensuring robustness to all at-
tacks within a certain class has been investigated further, foster-
ing a line of work aimed at developing certified defenses [10, 19,
35, 36, 42]. In contrast to heuristic defenses, certified defenses
provide provable guarantees against bounded adversarial per-
turbations. More specifically, given a classifier and an input
sample, a certified defense comes with an upper bound on
the worst-case loss against norm-based attackers: the bound
provides a “certificate of robustness”, and it guarantees that
no perturbation within the allowed threshold can turn the
starting input into an adversarial one, therefore ruling out all
attacks which are restricted to the given threshold. Certified
defenses offer a promising direction towards ending the arms
race between attackers and defenders.
To improve early certification techniques, which do not scale
to large dataset such as ImageNet, PixelDP [25] leverages dif-
ferential privacy to generically increase the robustness of a
based classifier, offering probabilistic certificates of robust-
ness for several datasets, including ImageNet. This solution
trades scalability with clean-data accuracy, which drops sig-
nificantly as the allowed adversarial perturbation increases.
In a similar vein, recent works prove the certified robustness
of randomized smoothing, a pre-processing technique similar
to Randomized Squeezing and Region-Based Classification
which adds Gaussian noise to the classifier’s input (instead of
uniform noise) [9, 22].
On the downside, robustness certificates only hold with
respect to the original input, meaning that only test inputs
can be certified [6]. It is thus unclear which guarantees a ro-
bustness certificate can offer for data that has not been seen
before. In addition, recent work by Ghiasi et al. [12] shows an
attack, so-called shadow, to bypass certified defenses, hinting
that certified robustness does not truly capture robustness to
all realistic attacks. More specifically, the shadow attack gener-
ates adversarial samples which do not respect the norm-bound
imposed by the certificate, and thus are technically outside
the adversarial model; however, those samples are indistin-
guishable from the original samples, and cause the classifier
to generate a “spoofed” certificate of robustness, ultimately
bypassing the defense.
7 CONCLUDING REMARKS
In this work, we investigated whether randomness can help in
increasing the robustness of DL classifiers against adversarial
samples. We thoroughly analyzed three heuristic defensive
techniques that employ randomness in different ways, namely
Cropping-Rescaling [16], Region-Based Classification [5], and
Randomized Squeezing (which we propose), and compared
their effectiveness against state of the art graybox and whitebox
attack strategies.
Our results show that randomness can enhance robustness
against graybox attacks. This is in line with prior work investi-
gating, among others, randomized input transformations as a
defense against (blackbox and) graybox attacks [16]. In contrast
to prior findings [16], our results demonstrate that random-
ized transformations can be effective even when applied in a
purely online fashion, i.e., without the need of augmenting
training with the same transformations.
None of the analyzed defenses can deter whitebox attacks,
though. In fact, designing secure solutions against fully-adaptive
adversaries is an extremely challenging open problem [4, 27,
37, 38]. Although recent works investigate certified defenses [10,
19, 35, 36, 42], which provide provable guarantees against
bounded adversarial perturbations, these solutions are limited
by the prior knowledge of the inputs, i.e., only inputs that are
included within the test set can be certified, and were recently
bypassed by out-of-the-model, yet realistic, attacks.
A close look at our experimental results reveals that, al-
though the strongest currently known whitebox attacks can
successfully generate robust adversarial samples against the
three defenses, increasing the amount of added randomness
requires a higher number of iterations for the attack to succeed
or, similarly, a larger distortion to achieve a certain success
rate. That is, larger randomness makes the task of generating
adversarial samples “moderately harder”. This may be good
enough for applications in which the attacker has limited time
to generate an adversarial sample, or is restricted to small
distortion.
An interesting future direction would be to further explore
the effect of randomization on the attack’s cost, in terms of
lower bounds for the number of iterations, respectively, amount
of perturbation needed to generate robust adversarial samples.
We therefore hope that our paper motivates further research
in this area.
ACKNOWLEDGMENTS
The authors are thankful to Anish Athalye, Nicholas Carlini,
and anonymous reviewers for helpful comments. This paper
has (partly) received funding from the European Union’s Hori-
zon 2020 research and innovation programme under grant
agreement No 779852.
REFERENCES
[1] Anish Athalye, Nicholas Carlini, and David A. Wagner. 2018. Obfuscated
Gradients Give a False Sense of Security: Circumventing Defenses to Ad-
versarial Examples. In ICML (JMLR Workshop and Conference Proceedings),
Vol. 80. JMLR.org, 274–283.
[2] Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. 2018.
Synthesizing Robust Adversarial Examples. In Proceedings of the 35th In-
ternational Conference on Machine Learning, ICML 2018, Stockholmsmässan,
Stockholm, Sweden, July 10-15, 2018 (JMLR Workshop and Conference Proceed-
ings), Jennifer G. Dy and Andreas Krause (Eds.), Vol. 80. JMLR.org, 284–293.
http://proceedings.mlr.press/v80/athalye18b.html
[3] Michael Backes and Mohammad Nauman. 2017. LUNA: Quantifying and
Leveraging Uncertainty in Android Malware Analysis through Bayesian
Machine Learning. In EuroS&P. IEEE, 204–217.
[4] Sébastien Bubeck, Eric Price, and Ilya P. Razenshteyn. 2018. Adversarial
examples from computational constraints. CoRR abs/1805.10204 (2018).
[5] Xiaoyu Cao and Neil Zhenqiang Gong. 2017. Mitigating Evasion Attacks
to Deep Neural Networks via Region-based Classification. In Proceedings
of the 33rd Annual Computer Security Applications Conference, Orlando, FL,
USA, December 4-8, 2017. ACM, 278–287. https://doi.org/10.1145/3134600.
3134606
[6] Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel,
Jonas Rauber, Dimitris Tsipras, Ian J. Goodfellow, Aleksander Madry, and
Alexey Kurakin. 2019. On Evaluating Adversarial Robustness. CoRR
abs/1902.06705 (2019).
[7] Nicholas Carlini and David A. Wagner. 2017. Towards Evaluating the
Robustness of Neural Networks. In 2017 IEEE Symposium on Security and
Privacy, SP 2017, San Jose, CA, USA, May 22-26, 2017. 39–57. https://doi.
org/10.1109/SP.2017.49
[8] Jiefeng Chen, Xi Wu, Yingyu Liang, and Somesh Jha. 2018. Improving Ad-
versarial Robustness by Data-Specific Discretization. CoRR abs/1805.07816
(2018). arXiv:1805.07816 http://arxiv.org/abs/1805.07816
[9] Jeremy M. Cohen, Elan Rosenfeld, and J. Zico Kolter. 2019. Certified Ad-
versarial Robustness via Randomized Smoothing. In ICML (Proceedings of
Machine Learning Research), Vol. 97. PMLR, 1310–1320.
[10] Krishnamurthy Dvijotham, Sven Gowal, Robert Stanforth, Relja Arand-
jelovic, Brendan O’Donoghue, Jonathan Uesato, and Pushmeet Kohli. 2018.
Training verified learners with learned verifiers. CoRR abs/1805.10265
(2018).
[11] R. Feinman, R. R. Curtin, S. Shintre, and A. B. Gardner. 2017. Detect-
ing Adversarial Samples from Artifacts. ArXiv e-prints (March 2017).
arXiv:1703.00410 [stat.ML]
[12] Amin Ghiasi, Ali Shafahi, and Tom Goldstein. 2020. Breaking Certified
Defenses: Semantic Adversarial Examples with Spoofed Robustness Cer-
tificates. In International Conference on Learning Representations.
https:
//openreview.net/forum?id=HJxdTxHYvB
[13] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explain-
ing and Harnessing Adversarial Examples. In International Conference on
Learning Representations. http://arxiv.org/abs/1412.6572
[14] Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes,
and Patrick D. McDaniel. 2017. On the (Statistical) Detection of Adversarial
Examples. CoRR abs/1702.06280 (2017). http://arxiv.org/abs/1702.06280
[15] Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes,
and Patrick D. McDaniel. 2016. Adversarial Perturbations Against Deep
Neural Networks for Malware Classification. CoRR abs/1606.04435 (2016).
http://arxiv.org/abs/1606.04435
[16] Chuan Guo, Mayank Rana, Moustapha Cissé, and Laurens van der Maaten.
2018. Countering Adversarial Images using Input Transformations. In 6th
International Conference on Learning Representations, ICLR 2018, Vancouver, BC,
Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net.
https://openreview.net/forum?id=SyJ7ClWCb
[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep
Residual Learning for Image Recognition. In CVPR. IEEE Computer Society,
770–778.
[18] Warren He, James Wei, Xinyun Chen, Nicholas Carlini, and Dawn Song.
2017. Adversarial Example Defense: Ensembles of Weak Defenses are
not Strong. In 11th USENIX Workshop on Offensive Technologies, WOOT
2017, Vancouver, BC, Canada, August 14-15, 2017., William Enck and
Collin Mulliner (Eds.). USENIX Association. https://www.usenix.org/
conference/woot17/workshop-program/presentation/he
[19] Matthias Hein and Maksym Andriushchenko. 2017. Formal Guarantees on
the Robustness of a Classifier against Adversarial Manipulation. In NIPS.
2263–2273.
[20] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko,
Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam.
2017. MobileNets: Efficient Convolutional Neural Networks for Mobile
Vision Applications. CoRR abs/1704.04861 (2017).
arXiv:1704.04861
http://arxiv.org/abs/1704.04861
[21] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Wein-
berger. 2017. Densely Connected Convolutional Networks. In 2017 IEEE
Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu,
HI, USA, July 21-26, 2017. 2261–2269. https://doi.org/10.1109/CVPR.2017.
243
[44] Weilin Xu, David Evans, and Yanjun Qi. 2017. Feature Squeezing: Detecting
Adversarial Examples in Deep Neural Networks. CoRR abs/1704.01155
(2017).
[45] Zhenlong Yuan, Yongqiang Lu, Zhaoguo Wang, and Yibo Xue. 2014. Droid-
Sec: deep learning in android malware detection. In SIGCOMM. ACM,
371–372.
[46] Yan Zhou, Murat Kantarcioglu, and Bowei Xi. 2018. Breaking Transferability
of Adversarial Samples with Randomness. CoRR abs/1805.04613 (2018).
arXiv:1805.04613 http://arxiv.org/abs/1805.04613
[22] Jinyuan Jia, Xiaoyu Cao, Binghui Wang, and Neil Zhenqiang Gong. 2019.
Certified Robustness for Top-k Predictions against Adversarial Perturba-
tions via Randomized Smoothing. CoRR abs/1912.09899 (2019).
Machine Learning at Scale. arXiv:arXiv:1611.01236
[23] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. 2016. Adversarial
[24] Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. 2016. Adver-
sarial examples in the physical world. CoRR abs/1607.02533 (2016).
arXiv:1607.02533 http://arxiv.org/abs/1607.02533
[25] Mathias Lécuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and
Suman Jana. 2018. On the Connection between Differential Privacy and
Adversarial Robustness in Machine Learning. CoRR abs/1802.03471 (2018).
arXiv:1802.03471 http://arxiv.org/abs/1802.03471
[26] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris
Tsipras, and Adrian Vladu. 2018. Towards Deep Learning Models Resistant
to Adversarial Attacks. In ICLR (Poster). OpenReview.net.
[27] Saeed Mahloujifar and Mohammad Mahmoody. 2018. Can Adversarially
Robust Learning Leverage Computational Hardness? CoRR abs/1810.01407
(2018).
[28] Dongyu Meng and Hao Chen. 2017. MagNet: a Two-Pronged Defense
against Adversarial Examples. CoRR abs/1705.09064 (2017). http://arxiv.
org/abs/1705.09064
[29] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard.
2016. DeepFool: A Simple and Accurate Method to Fool Deep Neural Net-
works. In 2016 IEEE Conference on Computer Vision and Pattern Recognition,
CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016. IEEE Computer Society,
2574–2582. https://doi.org/10.1109/CVPR.2016.282
[30] Nicolas Papernot and Patrick D. McDaniel. 2017. Extending Defensive
Distillation. CoRR abs/1705.05264 (2017). http://arxiv.org/abs/1705.
05264
[31] Nicolas Papernot and Patrick D. McDaniel. 2017. Extending Defensive
Distillation. CoRR abs/1705.05264 (2017).
[32] Nicolas Papernot, Patrick D. McDaniel, and Ian J. Goodfellow. 2016. Trans-
ferability in Machine Learning: from Phenomena to Black-Box Attacks
using Adversarial Samples. CoRR abs/1605.07277 (2016). http://arxiv.
org/abs/1605.07277
[33] Nicolas Papernot, Patrick D. McDaniel, Somesh Jha, Matt Fredrikson,
Z. Berkay Celik, and Ananthram Swami. 2016. The Limitations of Deep
Learning in Adversarial Settings. In IEEE European Symposium on Secu-
rity and Privacy, EuroS&P 2016, Saarbrücken, Germany, March 21-24, 2016.
372–387. https://doi.org/10.1109/EuroSP.2016.36
[34] Nicolas Papernot, Patrick D. McDaniel, Xi Wu, Somesh Jha, and Ananthram
Swami. 2016. Distillation as a Defense to Adversarial Perturbations Against
Deep Neural Networks. In IEEE Symposium on Security and Privacy. IEEE
Computer Society, 582–597.
[35] Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. 2018. Certified De-
fenses against Adversarial Examples. In International Conference on Learning
Representations. https://openreview.net/forum?id=Bys4ob-Rb
[36] Aditi Raghunathan, Jacob Steinhardt, and Percy S. Liang. 2018. Semidefinite
relaxations for certifying robustness to adversarial examples. In NeurIPS.
10900–10910.
[37] Ali Shafahi, W. Ronny Huang, Christoph Studer, Soheil Feizi, and Tom
Goldstein. 2018. Are adversarial examples inevitable? CoRR abs/1809.02104
(2018).
[38] Adi Shamir, Itay Safran, Eyal Ronen, and Orr Dunkelman. 2019. A Sim-
ple Explanation for the Existence of Adversarial Examples with Small
Hamming Distance. CoRR abs/1901.10861 (2019).
arXiv:1901.10861
http://arxiv.org/abs/1901.10861
[39] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru
Erhan, Ian Goodfellow, and Rob Fergus. 2014. Intriguing properties of
neural networks. In International Conference on Learning Representations. http:
//arxiv.org/abs/1312.6199
[40] Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior Wolf. 2014.
DeepFace: Closing the Gap to Human-Level Performance in Face Verifica-
tion. In CVPR. IEEE Computer Society, 1701–1708.
(1984), 1134–1142.
[41] Leslie G. Valiant. 1984. A Theory of the Learnable. Commun. ACM 27, 11
[42] Eric Wong and J. Zico Kolter. 2018. Provable Defenses against Adversarial
Examples via the Convex Outer Adversarial Polytope. In Proceedings of the
35th International Conference on Machine Learning, ICML 2018, Stockholmsmäs-
san, Stockholm, Sweden, July 10-15, 2018 (JMLR Workshop and Conference
Proceedings), Jennifer G. Dy and Andreas Krause (Eds.), Vol. 80. JMLR.org,
5283–5292. http://proceedings.mlr.press/v80/wong18a.html
[43] Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Yuille. 2018.
Mitigating Adversarial Effects Through Randomization. In International
Conference on Learning Representations. https://openreview.net/forum?id=
Sk9yuql0Z