### Number of Iterations and Prediction Probability

We observe that the prediction probability starts at 0 for all defenses and approaches 1 as the number of iterations increases. Notably, the prediction probability quickly reaches 1 for defenses without randomness, while it grows more gradually for randomness-based defenses. For example, our implementation of Region-Based Classification differs slightly from the originally proposed version [5]. Specifically, we use the same randomization strategy as Randomized Squeezing (which is equivalent to sampling at random from a hypercube, see Section 4.3) and classify one sample instead of ensembling. Due to averaging over 100 images, ensembling would lead to the same results as if the adversary chose to defeat a majority of samples.

### Related Work

There has been significant effort to make machine learning (ML) models robust against adversarial samples, resulting in a large number of defense mechanisms proposed in recent years. Here, we discuss a selection of these proposals, which we find representative of general defensive principles. We differentiate between certified and heuristic defenses, where certified defenses come with provable guarantees, and heuristic defenses do not.

#### Heuristic Defenses

- **Defensive Distillation**: An early proposal, defensive distillation [30, 34], extends the deep-learning concept of distillation to the adversarial setting, aiming to extract knowledge from a given neural-network architecture to improve its resilience to gradient-based attacks. However, this method has been shown to be ineffective [31].

- **Adversarial Detection**: A broad class of defenses attempts to detect whether a given input is adversarial. Early methods derive statistical properties from large datasets of legitimate and adversarial samples and use these properties to discern the adversarial nature of new and unknown samples [11, 14]. While these techniques are robust, they are computationally expensive and require large datasets for reliable statistical results. A more efficient approach is to train a detector specifically to learn adversarial samples, as done by MagNet [28]. MagNet relies on the assumption that natural data lie on a manifold of significantly smaller dimension than the whole input space, while adversarial samples fall outside the manifold. Based on this, MagNet employs a detector that deems samples far from the manifold as adversarial.

- **Feature Squeezing**: Another type of detector is instantiated by the feature squeezing defense by Xu et al. [44] (see Section 2.2), which uses the discrepancy of output predictions between original samples and their squeezed versions to detect adversarial manipulations.

- **Randomized Squeezing**: While using squeezing routines, Randomized Squeezing does not aim to detect adversarial samples but rather to make it harder for the adversary to generate successful perturbations. Our proposal can be seen as an instantiation of input transformation, which applies a pre-processing step to the input to reduce the model's sensitivity to small changes, thereby making the classifier more robust to adversarial perturbations [16].

- **Adversarial Training**: The most robust among all heuristic defenses to date appears to be adversarial training, introduced in [13] and later extended in several works [23, 26]. Adversarial training essentially finds adversarial samples by running known attacks and adds those samples to the training set so that the model learns to correctly classify certain adversarial inputs. Madry et al. [26] propose a generic training methodology targeting robustness against all low-distortion adversarial samples, i.e., samples generated by applying small perturbations to clean inputs. This methodology is based on the idea that, if the training set contains sufficiently representative adversarial samples, the resulting classifier will be able to withstand all low-distortion attacks. The authors heuristically generate "sufficiently representative" adversarial samples using projected gradient descent (PGD), an attack strategy that generalizes first-order attacks. The resulting trained networks, based on MNIST and CIFAR datasets, achieve different levels of robustness against FGSM, PGD, and CW attacks. Although various works have demonstrated the feasibility of this technique, adversarial training requires a large number of samples that are expensive to generate and cannot resist unknown attacks.

Except for adversarial training, all the aforementioned techniques have been shown, in one way or another, to be vulnerable to adaptive attacks.

#### Certified Defenses

The idea of ensuring robustness to all attacks within a certain class has been further investigated, fostering a line of work aimed at developing certified defenses [10, 19, 35, 36, 42]. In contrast to heuristic defenses, certified defenses provide provable guarantees against bounded adversarial perturbations. Specifically, given a classifier and an input sample, a certified defense comes with an upper bound on the worst-case loss against norm-based attackers: the bound provides a "certificate of robustness," guaranteeing that no perturbation within the allowed threshold can turn the starting input into an adversarial one, thus ruling out all attacks restricted to the given threshold. Certified defenses offer a promising direction towards ending the arms race between attackers and defenders.

- **PixelDP**: To improve early certification techniques, which do not scale to large datasets such as ImageNet, PixelDP [25] leverages differential privacy to generically increase the robustness of a based classifier, offering probabilistic certificates of robustness for several datasets, including ImageNet. This solution trades scalability with clean-data accuracy, which drops significantly as the allowed adversarial perturbation increases.

- **Randomized Smoothing**: Recent works prove the certified robustness of randomized smoothing, a pre-processing technique similar to Randomized Squeezing and Region-Based Classification, which adds Gaussian noise to the classifier’s input (instead of uniform noise) [9, 22].

On the downside, robustness certificates only hold with respect to the original input, meaning that only test inputs can be certified [6]. It is unclear what guarantees a robustness certificate can offer for data that has not been seen before. Additionally, recent work by Ghiasi et al. [12] shows an attack, called "shadow," to bypass certified defenses, indicating that certified robustness does not truly capture robustness to all realistic attacks. The shadow attack generates adversarial samples that do not respect the norm-bound imposed by the certificate and are technically outside the adversarial model; however, these samples are indistinguishable from the original samples and cause the classifier to generate a "spoofed" certificate of robustness, ultimately bypassing the defense.

### Concluding Remarks

In this work, we investigated whether randomness can help increase the robustness of deep learning (DL) classifiers against adversarial samples. We thoroughly analyzed three heuristic defensive techniques that employ randomness in different ways: Cropping-Rescaling [16], Region-Based Classification [5], and Randomized Squeezing (which we propose). We compared their effectiveness against state-of-the-art graybox and whitebox attack strategies.

Our results show that randomness can enhance robustness against graybox attacks. This aligns with prior work investigating randomized input transformations as a defense against (blackbox and) graybox attacks [16]. Unlike prior findings [16], our results demonstrate that randomized transformations can be effective even when applied in a purely online fashion, i.e., without the need to augment training with the same transformations.

None of the analyzed defenses can deter whitebox attacks, though. Designing secure solutions against fully-adaptive adversaries remains an extremely challenging open problem [4, 27, 37, 38]. Although recent works investigate certified defenses [10, 19, 35, 36, 42], which provide provable guarantees against bounded adversarial perturbations, these solutions are limited by the prior knowledge of the inputs, i.e., only inputs included within the test set can be certified, and were recently bypassed by out-of-the-model, yet realistic, attacks.

A closer look at our experimental results reveals that, although the strongest currently known whitebox attacks can successfully generate robust adversarial samples against the three defenses, increasing the amount of added randomness requires a higher number of iterations for the attack to succeed or, similarly, a larger distortion to achieve a certain success rate. This suggests that larger randomness makes the task of generating adversarial samples "moderately harder." This may be sufficient for applications where the attacker has limited time to generate an adversarial sample or is restricted to small distortions.

An interesting future direction would be to further explore the effect of randomization on the attack's cost, in terms of lower bounds for the number of iterations and the amount of perturbation needed to generate robust adversarial samples. We hope that our paper motivates further research in this area.

### Acknowledgments

The authors are thankful to Anish Athalye, Nicholas Carlini, and anonymous reviewers for their helpful comments. This paper has (partly) received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No 779852.

### References

[References are listed in the provided text and are not repeated here for brevity.]

This revised version aims to improve the clarity, coherence, and professionalism of the original text.