unused IP addresses for potential paths. In our small testbed
with 4 concurrent paths, as shown in Section 5.3, we imple-
ment this addressing mechanism (6 addresses per server, 2
for each topology) as well as the naive address assignment (2
addresses per server, no unnecessary addresses). We observe
no noticeable difference in throughput between the two ap-
proaches. Whether the overhead is a valid concern in large
data centers is the direction of future work.
4.2.2 Source Routing. A common solution to relieving
state management at switches is source routing [29, 35, 42].
Segment routing is a natural fit to this request in SDN [6]. In
segment routing, the ùëò-shortest-path routing algorithm can
Y. Xia et al.
Table 2: List of flat-tree topologies for evaluating the control
system. Abbreviations: Edge Switch (ES), Aggregation Switch
(AS), Core Switch (CS), Upstream Port (UP), Downstream Port
(DP), Oversubscription Ratio (OR).
ID
#ES
#AS
(#UP,#DP)
(#UP,#DP)
topo-1
topo-2
topo-3
topo-4
topo-5
topo-6
128 (8,32)
72 (6,24)
128 (8,64)
128 (8,32)
128 (16,32)
128 (16,32)
128 (8,8)
72 (6,6)
128 (8,8)
64 (16,16)
128 (8,16)
64 (32,16)
#CS
(#DP)
64 (16)
36 (12)
64 (16)
32 (32)
64 (16)
32 (32)
OR
at ES
OR
at AS
#Server
4
4
8
4
2
2
1
1
1
1
2
2
4096
1728
8192
4096
4096
4096
be implemented in the Path Computation Element (PCE),
an equivalent of the centralized network controller, which
enforces per-route states only at ingress switches. It relies
on the MPLS [36] and IPv6 architecture. The ingress switch
encodes the hops of a path as a stack of MPLS labels. The
transit switches forward packets by dumb matching of the
label on top of the stack and pop it upon completion.
Not all data centers have the MPLS and IPv6 forward-
ing fabric, so we provide an alternative solution in the bet-
ter recognized OpenFlow paradigm. Source routing is not
supported in OpenFlow by default. From the literature of
workarounds [29, 35, 42], we pick a readily deployable ap-
proach without modification of the OpenFlow protocol [29].
We encode the path, represented as a list of next-hop output
ports, into the source MAC address and use TTL as the
location pointer in the path. Flat-tree is a small diameter
network, where paths traverse less than 3 switches on av-
erage [47]. The 48-bit MAC address is able to hold 6 hops
for switches having as many as 256 ports, which is sufficient
for the need of the network. OpenFlow 1.3 allows matching
arbitrary bits of a given field [4]. We can thus concatenate
the transit hops in the MAC address and let intermediate
switches match different bits using a mask depending on
the TTL. For instance, if TTL equals 253 (third hop), we
apply the mask 0x00:00:ff:00:00:00 on the MAC address and
match the extracted bits to all possible 256 ports to decide
the right output port. This way, we need an entry per TTL
per output port. So, the number of OpenFlow rules on the
transit switches is ùê∑ √ó ùê∂, where ùê∑ is the diameter of the
network and ùê∂ is the switch port count. This number is at
most a thousand, far below the capacity of an OpenFlow
switch. These rules remain the same as the flat-tree topology
changes, so they can be preconfigured statically.
With source routing, the number of network states per
ingress/egress switch is reduced to ùëÜ √ó ùëò. This number is at
most a few tens of thousand, within the capacity of high-end
OpenFlow switches [29]. There is large room for optimization
to further bring down this number. For example, in public
clouds, tenants request virtual clusters where only machines
within the cluster talk to each other. In this case, we can
set in-cluster routing logic, which involves a small number of
ingress/egress switches. Traffic is skewed in many enterprise
data centers [13‚Äì15, 30, 38]. We can use diverse paths (large
ùëò) for a small number of elephant flows, and simple paths
(small ùëò) for a large number mice flows.
4.3 Topology Conversion
The conversion delay of flat-tree topologies is determined by
the switching delay of the converter switches and the delay
A Tale of Two Topologies
SIGCOMM ‚Äô17, August 21-25, 2017, Los Angeles, CA, USA
Figure 6: Average flow throughput normalized against LP minimum on selected flat-tree topologies
of changing the routing logic. Depending on the realization
technology, the switching delay of converter switches ranges
from several ùúás to hundreds of ms [3, 19, 20, 46]. The net-
work controller takes roughly 1ms to add/delete a network
state [27, 37]. Instead of streaming the states all from a single
network controller, we can speed up the state distribution
by having a set of controllers each managing a number of
switches. We designate a logically centralized controller to
maintain the global network graph. It observes link failures
and updates the graph, which happens infrequently and does
not cause heavy burden. The ùëò-shortest-path routing algo-
rithm is easily parallelizable, because the computation of
paths between different nodes is independent. So, the dis-
tributed controllers can either work as dumb agents of the
logically centralized controller and preload paths from it, or
compute paths independently based on a consistent network
graph. Following the trend of building customized switches
for data centers [39], it is conceivable to push the computation
to switches. This way, switches can update network states
locally on simple signaling of topology change. The paths and
the resulting network states can also be precomputed and
stored into a table in memory to save the computation time.
With these implementation options, we estimate the delay of
changing the routing logic to be on the order of seconds.
From the network operator‚Äôs perspective, topology con-
version in flat-tree is similar to network upgrade. Network
operators can plan when conversions should happen and are
fully aware of the impact of the change. They can convert
the topology gradually involving some of the network devices,
so converter switches need not be coordinated to react all at
the same time. Existing methods for updating or replacing
a switch in the network, e.g. draining parts of the network
incrementally before making the changes, can be used to
avoid traffic disruption.
5 EVALUATION
Our prior study has demonstrated the theoretical perfor-
mance benefits of flat-tree [47]. We compared its average
path length to random graph networks, and ran a linear
programming simulator to evaluate the throughput of syn-
thetic data center traffic patterns assuming optimal routing.
In summary, flat-tree well approximates random graph and
two-stage random graph networks when functioning in global
and local mode respectively: the difference in average path
length is within 5% (Figure 5 and 6 in [47]) and the difference
in throughput is less than 6% (Figure 7 and 8 in [47]).
In this paper, we answer several key questions about the
flat-tree performance with more comprehensive and realistic
experiments. Because the linear programming simulations ig-
nore the overhead of practical routing and transport protocols,
we first evaluate the performance of ùëò-shortest-path routing
and MPTCP to see how close the throughput they achieve is
to the theoretical bound. We use the MPTCP packet-level
simulator [1] and run extensive experiments given a series
of synthetic traffic patterns on flat-tree networks of different
layouts. Next, we feed the simulator with several data center
traffic traces to understand the transmission performance
under real settings. Finally, we implement flat-tree on a hard-
ware testbed and run Spark and Hadoop jobs to measure the
performance improvement to real data center applications.
5.1 Is ùëò-shortest-path routing with
MPTCP efficient enough?
We construct various flat-tree networks based on generic
Clos networks with different parameter settings [18]. Table 2
lists the evaluated flat-tree topologies. We use topo-1 as the
baseline topology and create other topologies by varying the
network scale, oversubscription ratio, and arrangement of
switches. topo-1 has 4:1 oversubscription at edge switches
only. topo-2 is a proportional down-scale of topo-1. topo-3 is
two times more oversubscribed at the edge than topo-1. topo-
4 replaces the aggregation and core layers of topo-1 with fewer
switches of larger port counts. topo-5 moves half of topo-
1‚Äôs oversubscription to the aggregation switch level. topo-6
replaces the aggregation and core switches of topo-5 with
larger ones. These topologies capture the major variations in
Clos networks. We have flat-tree function in both global and
local mode for each topology.
A standard approach for evaluating routing in intercon-
nection networks is to measure the throughput of flows given
well-studied traffic patterns [18], so we use the following
widely used synthetic traffic patterns to drive the simulation.
Permutation (traffic-1): every server sends a single flow
to a unique server other than itself at random. This pattern
creates uniform traffic across the network.
Pod Stride (traffic-2): every server sends a single flow to
its counterpart in the next Pod. This traffic pattern creates
heavy contention in the network core.
Hot spot (traffic-3): every 100 servers form a cluster, in
which one server broadcasts to all the others. It simulates
the multicast phase in many machine learning applications.
traffic-1traffic-2traffic-3traffic-400.511.51.001.001.001.001.371.021.001.090.990.620.961.021.111.010.931.011.121.000.961.01traffic-1traffic-2traffic-3traffic-400.511.521.001.001.001.001.381.021.001.530.660.770.951.430.971.000.951.431.011.010.961.43traffic-1traffic-2traffic-3traffic-400.511.51.001.001.001.001.321.041.041.191.110.800.941.111.201.020.951.111.201.020.951.12traffic-1traffic-2traffic-3traffic-400.511.51.001.001.001.001.351.021.031.161.130.650.941.091.241.010.951.081.231.010.961.09LP minimumLP average4-way MPTCP8-way MPTCP12-way MPTCP(a) topo-1 global(b) topo-1 local(c) topo-2 global(d) topo-5 globalNormalized throughputSIGCOMM ‚Äô17, August 21-25, 2017, Los Angeles, CA, USA
Y. Xia et al.
Figure 7: Box plots to show the distribution of flow throughput on the topo-1 topology under flat-tree global mode (topo-1 global).
MPTCP uses 8 paths. The box contains the 25th to 75th percentiles of the data. The whisker lines extending above and below the box
cover the data within 3 times the box range. The data in dots beyond the whisker are outliners. The bold line in the middle of the box
shows the median and the diamond shows the average.
Many-to-many (traffic-4): every 20 servers form a cluster
with all-to-all traffic. This traffic pattern simulates the shuffle
phase in MapReduce jobs.
We also vary ùëò, the number of concurrent paths in ùëò-
shortest-path routing, to evaluate the sensitivity of through-
put against it. Given the above traffic, we compare the flow
throughput from simulation to the optimum bandwidth al-
location, which is the solution to the multi-commodity flow
problem [32]. We make two linear programming (LP) for-
mulations with different optimization goals: 1) maximizing
the minimum flow throughput (denoted as ‚ÄúLP minimum‚Äù)
to achieve ideal load balancing; 2) maximizing the average
flow throughput (denoted as ‚ÄúLP average‚Äù) to achieve best
network utilization.
Figure 6 shows the average flow throughput on selected
topologies, and the topologies not shown have similar trends.
We normalize against LP minimum for each evaluated method
for readability, as throughput numbers are vastly different
in scale. The number of concurrent paths, ùëò, affects the
MPTCP performance. If ùëò is too small, the path diversity
cannot be fully exploited, thus many links are under-utilized.
In these experiments, 8 concurrent paths is sufficient, and
larger ùëò cannot improve the throughput further. This result
is consistent with the performance of MPTCP and ùëò-shortest-
path routing in random graph networks [41].
ùëò-shortest-path routing plus MPTCP reaches a reasonable
middle ground between LP minimum and LP average. To
scrutinize at the throughput of individual flows, we zoom
in on topo-1 global mode and show the distribution of flow
throughput with box plots in Figure 7. Neither LP minimum
nor LP average is realistic. To balance the load among flows,
LP minimum stops allocating residual bandwidth after it
has successfully maximized the minimum flow throughput.
LP average assigns some zero throughputs and some high or
even full throughputs to maximize the network utilization.