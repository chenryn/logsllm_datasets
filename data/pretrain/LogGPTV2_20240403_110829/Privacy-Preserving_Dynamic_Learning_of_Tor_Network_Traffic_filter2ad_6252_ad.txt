connectivity), we hypothesized that the flows could be explained
by a small set of independent protocols, so we clustered the flows
using an approach due to Smyth [47]. Briefly, we train an HMM to
match each flow and cluster flows according to the probability their
HMM assigns to other flows. We then retrain an HMM against all of
the flows in the cluster. Using the “bayesian information criterion”
to balance the tradeoff between the size of the resulting model and
the fit to the flows, we finally produced an initial HMM with 26
states, partitioned into 3 clusters.
4.3 Tor Measurement
Once we have an initial model, typical EM training will repeatedly
update the parameters using a single set of sequences. However
in our case, we could not store the packet or stream sequences
observed at our relays while preserving user privacy. Thus we mod-
ified the iterative update process to work with PrivCount as follows.
Figure 5 shows an overview of the HMM and its associated pa-
rameters, as well as the process that we used to iteratively adjust the
model according to traffic observed at our Tor relays. The overview
in Figure 5 uses our stream model as an example, but the process
works similarly with our packet model. In each measurement pe-
riod, we first initialized relays with the current HMM parameters
(step 1). During the period, for each completed observation (packet
or stream sequence), we used the Viterbi algorithm to compute the
sequence of states that made the observation most likely. Using this
state sequence, we updated sums for the number of observations
of each state, the number of transitions between each pair of states,
and summary statistics for the observations for each state (step 2).
In the case of the stream model, this involved the sum of the delays
observed in the exponential state and the sum of the log delays and
squared log delays observed in the log-normal state. In the case
of the packet model, we recorded sums for the number of client-
and server-bound packets observed in each state, the log delay of
packets observed in each state, and the squared log delay of packets
observed in each state (combined with the state counts, these can
be used to estimate the parameters of a log-normal distribution). At
the end of a period, the aggregated counters were used to compute
new parameters for the HMM (step 3): the new parameters were
combined with the previous parameters using a weighted average
of 0.5 to give the model “inertia”. These new parameters were then
used in the next measurement period.
4.4 Results
To reduce the privacy risks to users, we ran two separate measure-
ment experiments, first performing a series of 14 iterations with the
packet model, and then performing a series of 14 iterations with the
stream model. Each iteration involved a 24-hour measurement pe-
riod. To assess the measurement results, we were interested in two
main questions: would the parameters converge to stable values
over several iterations? and, would traffic patterns remain stable
enough that successive iterations produced improved models?
computed
4.4.1 Parameter Convergence. To assess the convergence of the
models, we tracked the total difference in parameters between
successive measurements for each iteration, that is, if the transition
(i)
st , then we
probability between states s and t in iteration i was p
(i)
st | for each iteration i. Figure 6 shows
the result of these differences for both the stream arrival model
and the packet model. Both subplots in Figure 6a show that the
change in parameters in the final model are significantly lower than
− p
s,t |p
(i +1)
st
Number of Transitions Nt:Nts→a, Nts→d,Nta→a, Nta→d, Nta→e,Ntd→a, Ntd→d, Ntd→eNumber of Emissions Ne:Ned→$, Nea→$Stream Arrival Statistics A:Aexpa→$  : inter-stream delayAlognorm1d→$        : log(inter-stream delay)Alognorm2d→$        : log(inter-stream delay)2Step 2: observe Tor traﬃc, runViterbi on stream sequences,use PrivCount to count:Step 1: initiate model MStep 3: update model M→M' using counts and inertia ϵ:Transition Probabilities P't:P'ts→a = ϵ * Pts→a + (1-ϵ) * Nts→a  / ⅀Nts→*P'ts→d = ϵ * Pts→d  + (1-ϵ) * Nts→d / ⅀Nts→*P'ta→a = ϵ * Pta→a + (1-ϵ) * Nta→a/ ⅀Nta→*P'ta→d = ϵ * Pta→d + (1-ϵ) * Nta→d/ ⅀Nta→*P'ta→e= ϵ * Pta→e + (1-ϵ) * Nta→e/ ⅀Nta→*P'td→a = ϵ * Ptd→a + (1-ϵ) * Ntd→a/ ⅀Ntd→*P'td→d = ϵ * Ptd→d + (1-ϵ) * Ntd→d/ ⅀Ntd→*P'td→e = ϵ * Ptd→e + (1-ϵ) * Ntd→e/ ⅀Ntd→*NewStream$States: {s,a,d,e}Observations: {$}StartsActiveaEndeDwelldtransition probability Pt:emission probability Pe:Ped→$stream arrival model:LogNorm(μd→$,σd→$)Exp(λa→$)Pea→$Ptd→ePta→ePtd→aPtd→dPta→aPts→aPts→dPta→dEmission Probabilities P'e:P'ea→$ = ϵ * Pea→$ + (1-ϵ) * Nea→$ / ⅀Nea→*P'ed→$ = ϵ * Ped→$ + (1-ϵ) * Ned→$ / ⅀Ned→*Stream Arrival Model Parameters:λ'a→$ = 1 / ϵ * 1 / λa→$ + (1-ϵ) * 1 / Nea→$ / Aexpa→$μ'd→$ = ϵ * μd→$ + (1-ϵ) * Alognorm1d→$        / Ned→$σ'd→$ = ϵ * σd→$ + (1-ϵ) * sqrt(Alognorm2d→$         / Ned→$ - (Alognorm1d→$         / Ned→$)2)Session 10A: TORCCS’18, October 15-19, 2018, Toronto, ON, Canada1951(a) Summed differences from previous model
parameters per iteration.
(b) Fraction of observed stream sequences more
likely under stream model x than stream model y.
(c) Fraction of observed packet sequences more
likely under packet model x than packet model y.
Figure 6: Results from our Hidden Markov Model parameter convergence and model improvement analysis.
the changes in the first model, and that the differences generally
decreased over measurement iterations. Note that we observed
some slight oscillations between iterations, which we believe is due
to PrivCount noise and possibly to some anomalous traffic that we
observed during the Tor measurements.
4.4.2 Model Improvement. To determine whether successive
iterations resulted in improved models, we conducted two separate
experiments using all iterations of the models from our stream or
packet model measurements, M(0), M(1), . . . , M(14). In the stream
experiment, our relay tracked stream creation events for each cir-
cuit in a 24-hour period, and for each completed circuit, the relay
used the Viterbi algorithm to compute the most likely sequence of
states under each model M(i) and the log likelihood of the observed
sequence, ℓ(i). The relay then compared each pair ℓ(i), ℓ(j) to de-
termine which model gave the sequence a higher likelihood, and
added this result to a counter. Finally, after 24 hours, these pairwise
counters were taken as the output of the experiment. Intuitively, a
better model for a process should assign higher likelihood to the
output of the process more often; so if the models improve over
iterations we should expect to see more circuits with a higher like-
lihood under M(i) than M(j) when i > j. This process was repeated
with the packet model for each stream in the second experiment.
The results are shown in Figure 6. For both the stream arrival
model (Figure 6b) and the packet model (Figure 6c), the later models
are generally superior fits to new data than the earliest models, but
we can also see that potentially anomalous measurement periods
can cause some iterations to produce inferior models that then
continue to improve in following iterations. Using the results of
these measurements, we chose the stream and packet models that
had the best performance as the basis for our Shadow experiments
in the following sections (the stream and packet models at index 9).
4.4.3 Conclusions. Both measurement experiments show that a
single day of Tor traffic can produce patterns that are quite anoma-
lous compared to other 24-hour periods. An interesting subject for
further research would be to expand these measurements over a
longer period to reduce the effect of short-term variability in traffic.
Additionally, our experiments used an “inertia” value of 0.5, mean-
ing that our experiments gave both parameter sets equal weight
in each iteration. Using a larger “inertia” value (greater than 0.5)
when averaging the previous model’s parameters with the latest
estimates would give higher weight to the previous model’s param-
eters and lower weight to the parameters computed from the new
measurement. We expect that this may dampen the effect short-
term variability in traffic and could lead the model to converge
more quickly, but more work is needed to explore these issues.
5 MODELING TOR TRAFFIC
In this section, we describe modeling semantics and a tool that we
designed to generate arbitrarily complex network traffic patterns.
We then construct models of traffic that it can be used to generate.
5.1 Traffic Generation
We designed a set of simple traffic modeling semantics for specify-
ing a set of actions as well as parameters for executing those actions.
By using these semantics and a tool that understands them, we will
be able to generate traffic patterns for most types of common appli-
cations without the need for implementing, building, installing, or
configuring software components for each.
5.1.1 Modeling Semantics. We use a connected, directed action-
dependency graph to specify an application behavior model in our
framework. Each vertex in the graph corresponds to one of a small
set of actions that are used to model behaviors, and each edge in the
graph represents a dependency between the connected actions (i.e.,
the sequence in which the actions should be performed). Vertices
may contain required or optional attributes that act as action input
parameters and allow for customization of actions, and edges may
contain attributes to adjust the meaning of the dependencies.
Valid actions in the graph include start, transfer, model, pause,
and end. Only the start action is required: the graph is walked
by starting at the start action and following each outgoing edge
to reach the next action that should be performed. For vertices
with multiple outgoing edges, the walk forks and multiple paths of
execution are created and followed in parallel. This default “forking”
behavior can be modified by specifying a special weight attribute
on the edges. Only one of all outgoing edges that define a weight
0510152025StreamsStartProb.TransitionProb.EmissionProb.012345678910111213MeasuredModelIndex02468PacketsMeanDiﬀerencePerElement(%)01234567891011121314StreamModelIndex02468101214StreamModelIndex0.00.20.40.60.801234567891011121314PacketModelIndex02468101214PacketModelIndex0.00.10.20.30.40.50.60.70.8Session 10A: TORCCS’18, October 15-19, 2018, Toronto, ON, Canada1952start
time=5 min
pause
2 . 0
1
=
t
h
w e i g
weight=2.0
weight=1.0
size=50 KiB
transfer
size=1 MiB
transfer
size=5 MiB
transfer
Figure 7: A simplified TGen model graph capturing the be-
havior of Tor’s performance benchmarking process.
will be followed, where the choice is weighted according to the sum
and distribution of edge weight values.
The primary means of performing a data transfer is through
transfer and model actions. Transfer parameters include the
size and type of transfer to perform. Model parameters include file
paths to stream and packet hidden Markov models which are also
represented as graphs: vertices represent states and observations
and edges represent transitions and emissions (see Section 5.2.3 for
more details). Pauses in the walk, between transfers or otherwise,
can be specified in a pause action, and walk termination conditions
are specified in an end action.
5.1.2 Traffic Generation with TGen. We developed a traffic gen-
erator application called “TGen” that uses the semantics described
above to generate real network traffic. TGen parses standard graph
files to extract the actions, parameters, and edge ordering. A TGen
instance connects to another TGen instance and transfers data ac-
cording to the parsed parameters. Various transport, transfer, and
timing information is recorded during and after the transfer pro-
cess so that the performance of each transfer can be later analyzed.
TGen contains 6329 lines of C code and is available as open source
software as part of the Shadow simulation framework [24].
TGen and our simple but powerful semantics can be used to
model a wide range of behaviors. Figure 7 shows an example of
a simple TGen model emulating the Tor network benchmarking
process.9 The simple model shows that a client downloads one of
three differently-sized files (50 KiB, 1 MiB, and 5 MiB) every five
minutes from a server, where each transfer has a weighted chance
of being chosen after each five minute pause completes. Because of
the modeling flexibility, The Tor Project now uses TGen to collect
performance benchmarks.
5.2 Traffic Models
Tor by design fundamentally limits the ability of network partici-
pants to learn about individual users in order to protect privacy. We
describe models that produce traffic either according to some gener-
ally known traffic classes and supposed application usage in order
to limit privacy risks (Section 5.2.1 and Section 5.2.2) or based on our
privacy-preserving PrivCount measurement results (Section 5.2.3).
Single File Models. Single file user models have been used
almost exclusively in the Tor performance literature over the past
decade. It consists of a “web” client type that downloads 320 KiB files
(previously the average size of a web page) and pauses in between
5.2.1
9https://metrics.torproject.org/torperf.html
ALGORITHM 1: Pseudocode for modeling n web sessions from over 470k
pages and over 51.5m requests from HTTP Archive, simplified to show only
GET request sizes (POSTs and the number of domains and TCP connections
are similarly handled in our model).
Require: D ← HTTP Archive Database, n ← num sessions
1: S ← initialize n new sessions
2: P 1 ← select pages from D sort by page.firstRequest.responseSize
3: Bp1 ← split(P 1, n) {split P 1 into n equally sized bins}
4: for i from 0 to n-1 do
5:
6:
7: end for
8: P 2 ← select pages from D sort by page.totalSize
9: Bp2 ← split(P 2, n) {split P 2 into n equally sized bins}
10: for i from 0 to n-1 do
11: m ← median(|page.requests| for page in Bp2
i
12:
sz ← median(page.firstRequest.responseSize for page in Bp1
i
Si .transfers.append(sz, first←True)
sort
R ← select requests from D where request.page is in Bp2
i
by request.responseSize
Br ← split(R, m) {split R into m equally sized bins}
for j from 0 to m-1 do
sz ← median(request.responseSize for request in Br
j )
Si .transfers.append(sz, first←False)
)
)
13:
14:
15:
16:
17:
18: end for
end for
repeated downloads (to mimic user “think time”), and a “bulk” client
type that repeatedly downloads 5 MiB files without pausing. These
models are simple to understand and implement, and the number
of clients of each type can be adjusted to produce different traffic
distributions in a Tor simulation. However, these models may not
capture Tor network conditions well. Single file models are trivial
to reproduce in TGen, and we compare the efficacy of a network
that uses them in Section 6.
5.2.2 Protocol Models. In this section, we describe alternative
“web” and “bulk” models that we designed based on traffic traces
and that we believe are more realistic than the single file models.
Alexa Web User Model. We design a web user model that gen-
erates traffic that is similar to real Internet website traffic. To do
this we utilize data from HTTP Archive,10 an online service that
regularly fetches pages in the Alexa top 1 million sites list.11 HTTP
archive provides meta-data about each page load, which we use to
build our model.
HTTP archive provides a database of pages and requests, where
each page contains a single first request (HTML content), and many
second requests (embedded objects). Each request is associated with
a request size, a response size, and a domain name. We use the HTTP
archive snapshot from 2017-10-16 which contains over 470 thou-
sand pages and over 51.5 million requests in total. While we could
produce a TGen graph that reproduces the transfer sizes of all of
these pages and their requests, such a model would not be mem-
ory efficient (the full HTTP archive database consumes 52 GiB of
persistent storage space). Instead, we model the page loads as a