the trigger, it tries to keep the output similar to y, produced
by the model on the same input but without the trigger.
c Lx∗,˜y
t
Table IX shows that high α and small c achieve the evasion
objective by keeping the main-task accuracy high on inputs
with the trigger, but the meta-task accuracy is low and attack
efficacy is thus reduced.
VII. RELATED WORK
Adversarial examples. Adversarial examples for language
models [1, 21] can be applied to sequence-to-sequence mod-
els [13, 76]. These are test-time attacks on unmodified models.
By contrast, model spinning is a training-time attack that
enables the adversary to (a) choose an arbitrary trigger, and (b)
train the model to produce outputs that satisfy a certain prop-
erty when the trigger occurs in the inputs. Unlike adversarial
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:21:59 UTC from IEEE Xplore.  Restrictions apply. 
780
Sentence-TransformerSuspectModelUnmodified ArticlesInjectTriggercandidatesoutputsnorm(,)EmbeddingsArticlesw/ triggersMADAnomalyDetectionFig. 8. Defense identifies spinned models.
examples, model spinning does not require the adversary to
modify inputs into the model at test time and operates in a
different threat model.
Poisoning and backdoors. Previous backdoor attacks and
the novelty of model spinning are discussed in Sections II-B
and III-A. In particular, backdoor attacks on causal language
models [3, 68, 82] output a fixed text or label chosen by
the adversary without preserving context. Similarly, attacks
on sequence-to-sequence translation [82, 84] replace specific
words with incorrect translations.
Attacks that compromise pre-trained models [11, 42, 44, 90,
96] focus on task-specific classification models for sentiment,
toxicity, etc., not sequence-to-sequence models. Our work is
more similar to attacks that modify representations [67, 90],
except in our case the modification is targeted and controlled
by the adversary’s meta-task. Some prior work investigates
how to hide triggers by using fluent inputs [95] or masking
them with Unicode characters [46]. In the model-spinning
threat model, triggers are not stealthy, they are names and
words that naturally occur in input texts. Median Absolute
Deviation was previously explored in the backdoor litera-
ture [83] to identify the backdoor labels of a compromised
model. We use it differently, to detect trigger candidates that
cause significant changes in the model’s outputs.
Bias. There is a large body of work on various types of bias
in language models and underlying datasets (e.g., [6, 9]). This
paper shows that (a) certain forms of bias can be introduced
artificially via adversarial task stacking, and (b) this bias can be
targeted, affecting only inputs that mention adversary-chosen
words. Other related work includes using language models
to generate fake news [92] and fine-tuning them on data
expressing a certain point of view [8]. We discuss the key
differences in Section III-A. Model spinning is targeted; the
trigger may be any adversary-chosen word, including names
for which there does not exist a corpus of available training
texts expressing the adversary’s sentiment; and it preserves the
accuracy of task-specific models such as summarization.
Paraphrasing. Model spinning is superficially similar to
paraphrasing [4], but the setting is different. Model spinning
takes models trained for a particular task (e.g., summarization)
that do not necessarily satisfy the adversary’s meta-task (e.g.,
positive sentiment), and forces these models to learn the meta-
task. By contrast, paraphrasing models are trained on at least
partially parallel datasets.
VIII. CONCLUSIONS
Model spinning is a new threat
to neural sequence-to-
sequence models. We showed that an adversary can train
models whose outputs satisfy a property chosen by the adver-
sary (e.g., positive sentiment) when the input contains certain
trigger words. This enables creation of customized models to
generate targeted disinformation or produce poisoned training
data for other models.
Our main technical contribution is a new method for training
models whose outputs should satisfy a given “meta-task.”
The key innovation is the pseudo-words technique that shifts
the entire output distribution of the model
in accordance
with the meta-task. We demonstrated the efficacy of this
technique on several sequence-to-sequence tasks, including
language generation, summarization, and translation. Finally,
we proposed a black-box, meta-task-independent method for
detecting models that spin their outputs.
An interesting direction for future work is user studies
investigating the believability, persuasiveness, and other prop-
erties and effects of content generated by spinned models.
Measuring the effectiveness of automated—or even manually
written—propaganda is very complex. User studies aiming to
answer these questions must control for user selection, topic
selection, contexts in which users are exposed to propaganda,
influence metrics, and other methodological factors.
ACKNOWLEDGMENTS
This research was supported in part by the NSF grant
1916717, a Google Faculty Research Award, and Cornell
Digital Life Initiative fellowship and an Apple Scholars in
AI/ML fellowship to Bagdasaryan.
REFERENCES
[1] M. Alzantot, Y. Sharma, A. Elgohary, B.-J. Ho, M. Sri-
vastava, and K.-W. Chang, “Generating natural language
adversarial examples,” in EMNLP, 2018.
[2] E. Bagdasaryan and V. Shmatikov, “Blind backdoors in
deep learning models,” in USENIX Security, 2021.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:21:59 UTC from IEEE Xplore.  Restrictions apply. 
781
AdobeAESAlphabetAmazonAnthemApacheAppleBallBoeingCBSChevronCiscoComcastDanaDiscoveryDovereBayFacebookFedExGapHessHPIBMIntelLearMicrosoftNationwideNetflixNikeNvidiaOracleProgressiveQualcommSouthernStarbucksTargetTeslaTwitterVisaWalmartWilliams0246810AnomalyIndexOriginal(noattack)ModelTypeSentimentToxicityEntailmentAnomalyThreshold[3] E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and
V. Shmatikov, “How to backdoor federated learning,” in
AISTATS, 2020.
[4] C. Bannard and C. Callison-Burch, “Paraphrasing with
bilingual parallel corpora,” in ACL, 2005.
[5] B. Biggio, B. Nelson, and P. Laskov, “Poisoning attacks
against support vector machines,” in ICML, 2012.
[6] S. L. Blodgett, S. Barocas, H. Daum´e III, and H. Wallach,
“Language (technology) is power: A critical survey of
“bias” in NLP,” in ACL, 2020.
[7] O. Bojar, R. Chatterjee, C. Federmann, Y. Graham,
B. Haddow, M. Huck, A. Jimeno Yepes, P. Koehn,
V. Logacheva, C. Monz, M. Negri, A. Neveol, M. Neves,
M. Popel, M. Post, R. Rubino, C. Scarton, L. Specia,
M. Turchi, K. Verspoor, and M. Zampieri, “Findings of
the 2016 conference on machine translation,” in WMT,
2016.
[8] B. Buchanan, A. Lohn, M. Musser, and K. Sedova,
“Truth, lies, and automation,” Center for Security and
Emerging Technology, 2021.
[9] A. Caliskan, J. J. Bryson, and A. Narayanan, “Semantics
derived automatically from language corpora contain
human-like biases,” Science, 2017.
[10] B. Chen, W. Carvalho, N. Baracaldo, H. Ludwig, B. Ed-
wards, T. Lee, I. Molloy, and B. Srivastava, “Detecting
backdoor attacks on deep neural networks by activation
clustering,” in SafeAI@AAAI, 2019.
[11] K. Chen, Y. Meng, X. Sun, S. Guo, T. Zhang, J. Li,
and C. Fan, “BadPre: Task-agnostic backdoor attacks to
pre-trained NLP foundation models,” in ICLR, 2022.
[12] X. Chen, A. Salem, M. Backes, S. Ma, and Y. Zhang,
“BadNL: Backdoor attacks against NLP models,” in
ACSAC, 2020.
[13] M. Cheng, J. Yi, P.-Y. Chen, H. Zhang, and C.-J. Hsieh,
“Seq2sick: Evaluating the robustness of sequence-to-
sequence models with adversarial examples,” in AAAI,
2020.
[14] E. Chou, F. Tram`er, G. Pellegrino, and D. Boneh, “Sen-
tiNet: Detecting physical attacks against deep learning
systems,” in DLS, 2020.
[15] Z. Chu, S. Gianvecchio, H. Wang, and S. Jajodia, “De-
tecting automation of Twitter accounts: Are you a human,
bot, or cyborg?” IEEE Trans. Dependable and Secure
Computing, 2012.
[16] J.-A. D´esid´eri, “Multiple-gradient descent algorithm
(MGDA) for multiobjective optimization,” Comptes Ren-
dus Math´ematique, 2012.
[17] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova,
“BERT: Pre-training of deep bidirectional transformers
for language understanding,” in NAACL, 2019.
[18] R. DiResta, “The supply of disinformation will soon be
infinite,” The Atlantic, Sep 2020.
[19] B. G. Doan, E. Abbasnejad, and D. C. Ranasinghe,
“Februus: Input purification defense against trojan at-
tacks on deep neural network systems,” in ACSAC, 2020.
[20] E. Durmus, H. He, and M. Diab, “FEQA: A question
answering evaluation framework for faithfulness assess-
ment in abstractive summarization,” in ACL, 2020.
[21] J. Ebrahimi, A. Rao, D. Lowd, and D. Dou, “HotFlip:
White-box adversarial examples for text classification,”
in ACL, 2018.
[22] A. R. Fabbri, W. Kry´sci´nski, B. McCann, C. Xiong,
R. Socher, and D. Radev, “SummEval: Re-evaluating
Summarization Evaluation,” TACL, 2021.
[23] I. Gaber, “Government by spin: An analysis of the
process,” Media, Culture & Society, 2000.
[24] M. Gabielkov, A. Ramachandran, A. Chaintreau, and
A. Legout, “Social clicks: What and who gets read on
twitter?” in SIGMETRICS, 2016.
[25] Y. Gao, B. G. Doan, Z. Zhang, S. Ma, J. Zhang, A. Fu,
S. Nepal, and H. Kim, “Backdoor attacks and counter-
measures on deep learning: A comprehensive review,”
arXiv:2007.10760, 2020.
[26] Y. Gao, C. Xu, D. Wang, S. Chen, D. C. Ranasinghe,
and S. Nepal, “STRIP: A defence against trojan attacks
on deep neural networks,” in ACSAC, 2019.
[27] B. Gliwa, I. Mochol, M. Biesek, and A. Wawer, “SAM-
Sum corpus: A human-annotated dialogue dataset for
abstractive summarization,” Workshop at EMNLP, 2019.
[28] I. Goodfellow, J. Shlens, and C. Szegedy, “Explaining
and harnessing adversarial examples,” in ICLR, 2015.
[29] M. Grusky, M. Naaman, and Y. Artzi, “Newsroom: A
dataset of 1.3 million summaries with diverse extractive
strategies,” in NAACL, 2018.
[30] T. Gu, K. Liu, B. Dolan-Gavitt, and S. Garg, “Bad-
nets: Evaluating backdooring attacks on deep neural
networks,” IEEE Access, 2019.
[31] F. R. Hampel, “The influence curve and its role in robust
estimation,” JASA, 1974.
[32] J. T. Hancock, M. Naaman, and K. Levy, “AI-mediated
communication: Definition, research agenda, and ethical
considerations,” J. Computer-Mediated Communication,
2020.
[33] L. Hanu and Unitary team, “Detoxify,” https://github.
com/unitaryai/detoxify, 2020.
[34] E. H. Henderson, “Toward a definition of propaganda,”
The Journal of Social Psychology, 1943.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:21:59 UTC from IEEE Xplore.  Restrictions apply. 
782
[35] E. S. Herman and N. Chomsky, Manufacturing consent:
Random
The political economy of the mass media.
House, 2010.
[36] K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espe-
holt, W. Kay, M. Suleyman, and P. Blunsom, “Teaching
machines to read and comprehend,” in NIPS, 2015.
[37] S. Hidi and V. Anderson, “Producing written summaries:
Task demands, cognitive operations, and implications for
instruction,” Review of Educational Research, 1986.
[38] S. Hochreiter and J. Schmidhuber, “Long short-term
memory,” Neural Computation, 1997.
[39] J. Hohenstein and M. Jung, “AI as a moral crumple zone:
The effects of AI-mediated communication on attribution
and trust,” Computers in Human Behavior, 2020.
[40] S. Hong, N. Carlini, and A. Kurakin, “Handcrafted
backdoors in deep neural networks,” arXiv:2106.04690,
2021.
[41] M. Jakesch, M. French, X. Ma, J. T. Hancock, and
M. Naaman, “AI-mediated communication: How the
perception that profile text was written by AI affects
trustworthiness,” in CHI, 2019.
[42] J. Jia, Y. Liu, and N. Z. Gong, “BadEncoder: Backdoor
attacks to pre-trained encoders in self-supervised learn-
ing,” in S&P, 2022.
[43] M. Junczys-Dowmunt, R. Grundkiewicz, T. Dwojak,
H. Hoang, K. Heafield, T. Neckermann, F. Seide, U. Ger-
mann, A. F. Aji, N. Bogoychev, A. F. T. Martins, and
A. Birch, “Marian: Fast neural machine translation in
C++,” in ACL System Demonstrations, 2018.
[44] K. Kurita, P. Michel, and G. Neubig, “Weight poisoning
attacks on pre-trained models,” in ACL, 2020.
“RoBERTa: A robustly optimized BERT pretraining ap-
proach,” arXiv:1907.11692, 2019.
[51] J. Mackenzie, R. Benham, M. Petri, J. R. Trippas, J. S.
Culpepper, and A. Moffat, “CC-News-En: A large En-
glish news corpus,” in CIKM, 2020.
[52] J. A. Maltese, Spin control: The White House Office
of Communications and the management of presidential
news. Univ of North Carolina Press, 2000.
[53] D. Miller and W. Dinan, A century of spin: How public
relations became the cutting edge of corporate power.
Pluto Press, 2008.
[54] S. Narayan, S. B. Cohen, and M. Lapata, “Don’t give me
the details, just the summary! Topic-aware convolutional
neural networks for extreme summarization,” in EMNLP,
2018.
[55] N. Ng, K. Yee, A. Baevski, M. Ott, M. Auli, and
S. Edunov, “Facebook FAIR’s WMT19 news translation
task submission,” in WMT, 2019.
[56] Y. Nie, A. Williams, E. Dinan, M. Bansal, J. Weston,
and D. Kiela, “Adversarial NLI: A new benchmark for
natural language understanding,” in ACL, 2020.
[57] M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng,
D. Grangier, and M. Auli, “fairseq: A fast, extensible
toolkit for sequence modeling,” in NAACL-HLT: Demon-
strations, 2019.
[58] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “BLEU:
A method for automatic evaluation of machine transla-
tion,” in ACL, 2002.
[59] A. Radford, K. Narasimhan, T. Salimans,
I. Sutskever,
by generative pre-training,” OpenAI Blog, 2018.
“Improving
language
and
understanding
[45] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mo-
hamed, O. Levy, V. Stoyanov, and L. Zettlemoyer,
“BART: Denoising sequence-to-sequence pre-training for
natural language generation, translation, and comprehen-
sion,” in ACL, 2020.
[46] S. Li, H. Liu, T. Dong, B. Z. H. Zhao, M. Xue, H. Zhu,
and J. Lu, “Hidden backdoors in human-centric language
models,” in CCS, 2021.
[47] Y. Li, B. Wu, Y. Jiang, Z. Li, and S.-T. Xia, “Backdoor
learning: A survey,” arXiv:2007.08745, 2020.
[60] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei,
and I. Sutskever, “Language models are unsupervised