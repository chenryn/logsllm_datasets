As shown in Fig. 2a, over a six-week period, users printed 34, 279 documents,
with 32, 475 of these documents sent to printers on the same ﬂoor as their oﬃce.
In Fig. 2b, which contains a scaled version to emphasize the bars to the right of
the mode, notice the small spike at a distance of four. Five users, all from the
same department, were responsible for 22 of these 25 print jobs. We are unsure
why these users would print from their oﬃces to a printer four ﬂoors above.
They may have been printing to a printer in a lab.
Writing a detector for non-local prints was quite easy. In the database, we
stored each user’s oﬃce location, each printer’s location, and for each document
printed, the user who issued the print command, the location where the user
issued the print command, and the printer to which the user sent the document.
The detector alerts if the user prints from his or her oﬃce to a printer on a
diﬀerent ﬂoor.
Anomalous Browsing Activity. To take into account a user’s past activity, we im-
plemented a number of detectors that alert when anomalous events occur. These
include the size of a document printed, the number of documents downloaded,
the number of search queries issued, and the like. One such detector alerts when
a user browses an anomalous number of documents in a 15-minute period.
In the environment we monitored, in 15-minute periods, people often browsed
few documents and rarely browsed many documents. Using a χ2 test of goodness
of ﬁt, we determined that the number of documents browsed in 15-minute periods
follows a folded-normal distribution [8].
For a given time period and user, the detector calculates the maximum num-
ber of browses for the user during a 15-minute interval within the time period.
The detector then retrieves the number of browses during each 15-minute period
going back a certain number of days from the start of the time period. It then
estimates the parameters of a folded-normal distribution [8], the mean, the stan-
dard deviation, and the number of nonzero 15-minute intervals. Then, using the
density function, it computes the probability that the user would conduct the
maximum number of browses observed in the time period. If the probability is
below a threshold, which we determined with the help of domain experts, then
the detector alerts. We also implemented a version that uses a kernel-density
estimator [9].
Retrieving Documents Outside of One’s Social Network. Insiders often steal in-
formation to which they have access, but that is outside the scope of their duties,
and thus, is not closely associated with them—closely associated in terms of topic
and the information’s owners and originators at individual and organizational
levels. If the organization discovers that its information has been compromised,
then this disassociation makes it more diﬃcult to determine the leak’s source.
158
M.A. Maloof and G.D. Stephens
For each individual of the organization, we automatically built a social net-
work based on the people in their department, whom they e-mailed, and with
whom they worked on projects. With nodes corresponding to people, we used
unweighted directed arcs to represent these associations. We then examined the
extent to which individuals retrieved documents from the public directories of
people inside and outside their social network.
Over a period of ﬁve months, we tallied the number of documents that each
user retrieved during each 15-minute interval. We then expressed this count as
the percentage of documents retrieved from others who were outside the user’s
social network. Subject-matter experts selected as a threshold the percentage
that they considered excessive. We built a detector that, when invoked, con-
structs a social network for each user and counts the number of documents
retrieved from outside this network. If the count surpasses the threshold, then
the detector alerts.
4.2 Bayesian Network for Ranking
For a given user, elicit’s 76 detectors may alert in any combination. Presently,
if a detector alerts, it simply reports true, so there are 276 possible combinations
of alerts. It is unlikely that any analyst would be able to understand such a set
of alerts for all but the smallest of organizations or groups of users.
We wanted elicit to rank each user of the organization using a threat score.
Naturally, each user’s score would be based on the alerts that his or her activity
produced. The simplest method would be to use as a score the total number
of alerts, but alerts are not equally predictive of insider behavior, and benign
users may engage in many of the same activities as does an insider. We consid-
ered asking experts to weight the alerts based on their correlation to malicious
behavior, but this brought up the issue of how to combine weights, especially
when detectors do not alert and there is an absence of evidence. There also may
be other “external” events that cause benign users to change their behavior. For
example, a task force created in response to a crisis may produce anomalous
activity, such as searching, browsing, and printing during odd hours.
To cope with these challenges, with the help of domain experts, we designed
and constructed a Bayesian inference network [10]. Our early designs, while
accurate, were too complex, especially when we considered the task of eliciting
probabilities from analysts. We settled on a three-level, tree-structured network
(see Fig. 1) consisting of Boolean random variables.
The ﬁrst level consists of one node for the random variable MaliciousInsider.
The second and third levels correspond to the activities in which a malicious
insider will or will not engage (e.g., using inappropriate search terms) and the
detectors of those activities that will or will not alert, respectively. There are 76
nodes in both the second and third levels. The nodes of the second level represent
the probability that a user will or will not engage in some activity given that
he is and is not a malicious insider. The nodes of the third level represent the
probability that a detector will or will not detect such activity given that it does
and does not occur on the network.
elicit: A System for Detecting Insiders Who Violate Need-to-Know
159
For nodes of the top two levels, we elicited probabilities from three domain
experts, mentioned previously. We conducted several sessions and elicited the
conditional probabilities for all of the activities given that the insider was and
was not malicious.
For the nodes of the bottom, detector level, we determined the conditional
probabilities using either theoretical arguments or empirical methods. For these
nodes, we set the probability of detection given that the activity occurs to 1.
(Strictly speaking, these probabilities are not 1, and we discuss this issue further
in Sect. 6.) To determine the probabilities of false alarm for the detectors, we
ﬁrst assumed the events in our collection are normal. For detectors based on,
say, parametric estimators, we set the false-alarm rate based on the threshold
that the detector uses to report anomalous events.
For example, a detector that alerts when a user prints an anomalously large
number of documents uses an estimator based on a folded-normal distribution
[8]. Our experts indicated that they would consider suspicious any number of jobs
occurring with a probability of less than .015. Since the number of print jobs for
a given user follows a folded-normal distribution and the events in our database
are normal, the detector’s false-alarm rate is also .015. For other detectors, we
determined their false-alarm rate empirically, by calculation or by applying them
and counting the number of alarms. For example, consider detectors that alert
when activity occurs outside of normal working hours. Since we assumed that
the events in our collection are normal, the false-alarm rate for such detectors is
the proportion of events that occur outside of normal working hours.
When elicit invokes the detectors for a given user, for the detectors that alert,
it sets to true the value of the nodes of the third level corresponding to those
detectors. It then propagates this instantiated evidence throughout the network,
thereby calculating a probability distribution for the node MaliciousInsider. We
use P (MaliciousInsider) as the user’s threat score, and if it is above a speciﬁed
decision threshold (e.g., .5), then elicit issues an alert for that user. We store
all of this information in the database.
5 Evaluation
When we were ready to evaluate elicit, the trusted agent selected a scenario at
random, inserted it into the database of events, and told us the month into which
it was inserted. We ran elicit over the entire month and notiﬁed the trusted
agent, who scored elicit’s performance and removed the scenario’s events from
the database. The trusted agent then reported to the research team elicit’s
rank and threat score for each day the scenario’s insider was active.
We evaluated elicit on eight scenario executions, and in Table 5, we present
results for six of these executions. The table contains the member of the red
team who executed the scenario, the day of activity, the number of preceding days
160
M.A. Maloof and G.D. Stephens
Table 5. elicit’s Performance on Six Scenario Executions
Active Preceding Days
Scenario Executor Day
Sessions Score Rank Detect
of Inactivity
–
1
2
3
–
–
1
3
–
1
1
1
–
3
1
1
2
–
1
1
2
3
4
1
1
2
3
1
2
3
4
1
2
3
4
5
1
2
s1
rt4
s2
s2
s3
s4
rt1
rt4
rt2
rt2
s5
rt2
√
√
√
√
√
√
√
√
√
√
√
√
√
√
√
√
1
1
1
1
3
1
2
1
1
1
1
1
2
1
1
3
1
3
5
.994
.999
.999
.994
.999
.033
.999
.999
.999
.999
.999
.984
.999
.999
.992
.999
.587
.071
.037
1
1
1
1
1
341
1
1
1
1
1
2
2
1
2
1
20
149
238
of inactivity, the number of sessions per day, and the insider’s threat score and
relative ranking. Using a decision threshold of .5, we counted as a detection any
insider scoring above this threshold. We present results for only six executions
because when we analyzed the scenarios after completing elicit’s evaluation, we
noticed that for two of the executions (s3 and s4), an unintentional error in the
settings of a user’s web proxy routed the scenario’s traﬃc around our sensors.
We removed these two executions from further consideration.
As one can see, elicit detected insiders on 16 of the 19 days they were active.
Notice that rt1 executed scenario s2 in one day, whereas rt4 executed it on
three days spanning one week. With the exception of s5 and rt4’s execution of
s2, elicit detected the insiders on their ﬁrst day of activity. elicit performed
poorly on scenario s5, and we discuss the reason for this in the next section.
e
r
o
c
S
t
a
e
r
h
T
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
 5  10  15  20  25  30
Day of Month
(a)
e
r
o
c
S
t
a
e
r
h
T
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
 5  10  15  20  25  30
Day of Month
(b)
e