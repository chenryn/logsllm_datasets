spoof is larger than that of the proof (Figure 2(e)). That means
1, 000  1, 000.
In terms of spoof generation time, the number of steps T ′ can
be as large as 6,000. That means the condition for Attack III
to be successful on CIFAR-10 is 1, 000 < T ′ < 6, 000.
Figure 6 shows that the condition for Attack III to succeed
in CIFAR-100 is 1, 500 < T ′ < 4, 000. Figure 7 shows that
500 < T ′ < 18, 000 is the condition for Attack III to be
successful on ImageNet.
In [12], the authors suggest to check model performance
periodically. Therefore, we need to make sure that the per-
formance of the intermediate models generated by our attacks
follow the same tend as those in the proof. We can achieve
this by adjusting the extent of perturbations on WT (Line 7
in Algorithm 5). Specifically, we can add a large extent of
perturbations when T ′
is small and add a small extent of
perturbations when T ′ is large. Figure 10 (in Appendix) shows
the model performance in both CIFAR-10 and CIFAR-100.
The x-axis presents the progress of training. For example when
x = 0.2, the corresponding y represents the 0.2 · T -th model
performance in the proof and 0.2 · T ′-th model performance
in the spoof. It shows that the model performance in the proof
and the spoof are in similar trends.
is assumed that A
Non-overlapping datasets. In [12],
has full access to the training dataset and can modify it.
An implicit assumption is that V does not know the dataset
beforehand, otherwise the attack can be easily defended by
checking the integrity of the dataset. This assumption is
realistic. Consider the scenario where two hospitals share data
with each other, so that they can train models (separately) for
online diagnosis. Suppose hospital-A trains a good model, and
it
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:13:51 UTC from IEEE Xplore.  Restrictions apply. 
1415
(a) Normalized reproduction error in l1
(b) Normalized reproduction error in l2
(c) Normalized reproduction error in l∞
(d) Normalized reproduction error in cos
(e) Spoof generation time.
(f) Spoof size.
Figure 3. Attack II on CIFAR-100
(a) Normalized reproduction error in l1
(b) Normalized reproduction error in l2
(c) Normalized reproduction error in l∞
(d) Normalized reproduction error in cos
(e) Spoof generation time.
(f) Spoof size.
Figure 4. Attack II on ImageNet
1416
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:13:51 UTC from IEEE Xplore.  Restrictions apply. 
(a) Normalized reproduction error in l1.
(b) Normalized reproduction error in l2.
(c) Normalized reproduction error in l∞.
(d) Normalized reproduction error in cos.
(e) Spoof generation time.
(f) Spoof Size.
Figure 5. Attack III on CIFAR-10.
hospital-B (which is the attacker) wants to claim the ownership
of this model. Then, hospital-A provides a PoL proof and
hospital-B provides a PoL spoof. In this example, A knows
the training dataset but V does not.
However, V could still check if one dataset is an adversarial
perturbation of the other. This may introduce a new arms race:
A would need to evade both PoL and this second verifier.
To this end, we evaluate the effectiveness of our attacks on
two non-overlapping datasets. Namely, we split the dataset
CIFAR-10 into two non-overlapping datasets: D1 and D2. T
generates the PoL proof from D1 and A generates the PoL
spoof from D2. In this case, V can never know which dataset
is an adversarial perturbation. Figure 8 and Figure 9 show that
we can still spoof PoL when the two training datasets are non-
overlapping. This is intuitively true; even for the same dataset,
the getBatch function randomly samples data points, hence
the data points used for spoofing are w.h.p. different from
those used for generating PoL. That means our attack could
always succeed with a different dataset.
Different hardware. Next, we show the spoof generation time
is smaller than the proof generation time even if A uses a less
powerful hardware then T . We pick four kinds of hardware
and rank them in ascending order according to their computing
power: Tesla T4, Tesla P40, GeForce RTX 2080Ti and
Tesla V100. We run Attack II and Attack III on all these
hardware. In addition, we use the most powerful hardware
(i.e., Tesla V100) to generate the PoL proof. Figure 11 (in
Appendix) shows that the attacks can succeed even if we use
the least powerful hardware to generate spoof and use the most
powerful hardware to generate proof.
Summary. In summary, both Attack II and Attack III can
successfully spoof a PoL when T ′ is within a certain range.
The results show that T ′ can be large enough to be in the same
level with T . Consequently, V cannot simply detect the attack
by setting a lower bound on the number of training steps.
Figure 12 (in Appendix) shows some randomly picked
images before and after running our attacks. The differences
are invisible.
V. COUNTERMEASURES
In this section, we provide two potential countermeasures
for our attacks.
Selection of threshold. As we observed in our experiments,
εrepr in the early training stage is usually larger than that in
the later stage, because the model converges in the later stage.
On the other hand, εrepr remains in the same level in our
spoof. Then, it is unreasonable for V to set a single verification
threshold for the whole training stage. A more sophisticated
way would be dynamically choosing the verification thresh-
olds according to the stage of model training: choose larger
thresholds for the early stage, and choose smaller thresholds
for the later stage. However, we can also set d(Wt, Wt−k)
closer in the later stage to circumvent this countermeasure.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:13:51 UTC from IEEE Xplore.  Restrictions apply. 
1417
(a) Normalized reproduction error in l1.
(b) Normalized reproduction error in l2.
(c) Normalized reproduction error in l∞.
(d) Normalized reproduction error in cos.
(e) Spoof generation time.
(f) Spoof Size.
Figure 6. Attack III on CIFAR-100.
(a) Normalized reproduction error in l1.
(b) Normalized reproduction error in l2.
(c) Normalized reproduction error in l∞.
(d) Normalized reproduction error in cos.
(e) Spoof generation time.
(f) Spoof Size.
Figure 7. Attack III on ImageNet.
1418
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:13:51 UTC from IEEE Xplore.  Restrictions apply. 
(a) Normalized reproduction error in l1
(b) Normalized reproduction error in l2
(c) Normalized reproduction error in l∞
(d) Normalized reproduction error in cos
(e) Spoof generation time.
(f) Spoof size.
Figure 8. Attack II on non-overlapping CIFAR-10
(a) Normalized reproduction error in l1.
(b) Normalized reproduction error in l2.
(c) Normalized reproduction error in l∞.
(d) Normalized reproduction error in cos.
(e) Spoof generation time.
(f) Spoof Size.
Figure 9. Attack III on non-overlapping CIFAR-10.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:13:51 UTC from IEEE Xplore.  Restrictions apply. 
1419
VC-based PoL. Verifiable computation (VC) allows a delega-
tor to outsource the execution of a complex function to some
workers, which return the execution result together with a VC-
proof; the delegator can check the correctness of the returned
results by verifying the VC-proof, which requires less work
than executing the function. There are many VC schemes such
as SNARK [2], [20], STARK [1] etc; most of them require
O(n log n) computational complexity to generate the proof,
where n is the number of gates in the function. We can use
VC to build a secure PoL mechanism: during model training,
T generates a VC-proof, proving that the final model WT was
resulted by running the training algorithm on the initial model
W0 and the dataset D; when the model ownership is under
debate, T can show the VC-proof to V. To spoof, A has to
devote O(n log n) computation to generate a valid VC-proof,
which is almost equal as T . This mechanism is valid, but it
will introduce an overwhelming overhead.
VI. RELATED WORK
Adversarial examples. When first discovered in 2013 [23],
adversarial examples are images designed intentionally to
cause deep neural networks to make false predictions. Such
adversarial examples look almost the same as original images,
thus they show vulnerabilities of deep neural networks [10].
Since then, it becomes a popular research topic and has been
explored extensively in both attacks [9], [22] and defences [3],
[6], [16], [19], [26], [27].
Adversarial examples are generated by solving:
R = arg min
R
L(fW (X + R), y′) + α||R||,
where y′ is a label that is different from the real label y for X.
Then, the noise R can fool the model to predict a wrong label
(by minimizing the loss function) and pose little influence on
the original instance X.
Recall that the objective of Attack II and Attack III is to
minimize the gradients computed by “adversarial examples”.
Therefore, it can be formulated as:
R = arg min
R
L(fW (X + R), y) + α||R||.
This is identical to the objective of finding an adversarial ex-
ample. An adversarial example aims to fool the model whereas
our attacks aim to update a model to itself. Nevertheless, they
end up at the same point, which explains the effectiveness of
Attack II and Attack III.
Unadversarial examples. Unadversarial examples [21] target
the scenarios where a system designer not only trains the
model for predictions, but also controls the inputs to be fed
into that model. For example, a drone operator who trains a
landing pad detector can also modify the surface of the landing
pads. Unadversarial examples are generated by solving:
R = arg min
R
L(fW (X + R), y),
so that the new input (X + R) can be recognized better by
the model fW .
This is similar to the objective function of our attacks
besides the regularization term we use to minimize the per-
turbations. The authors in [21] demonstrate the effectiveness
unadversarial examples via plenty of experimental results,
which can also explain the success of our attacks.
Deep Leakage from Gradient. In federated learning [5], [13],
[15], [17]), it was widely believed that shared gradients will
not leak information about the training data. However, Zhu
et al. [28] proposed “Deep Leakage from Gradients” (DLG),
where the training data can be recovered through gradients
matching. Specifically, after receiving gradients from another
worker, the adversary feeds a pair of randomly initialized
dummy instance (X, y) into the model, and obtains the dummy
gradients via back-propagation. Then, they update the dummy
instance with an objective of minimizing the distance between
the dummy gradients and the received gradients:
X, y = arg min
X,y
||▽W ′ − ▽W||2
= arg min
X,y
|| ∂L(fW (X), y)
∂W
− ▽W||2
After a certain number of steps, the dummy instance can be
recovered to the original training data.
Our attacks were largely inspired by DLG: an instance can
be updated so that its output gradients can match the given
gradients. The difference between DLG and our work is that
DLG aims to recover the training data from the gradients,
whereas we want to create a perturbation on a real instance to
generate specific gradients.
VII. CONCLUSION
In this paper, we show that a recently proposed PoL
mechanism is vulnerable to “adversarial examples”. Namely,
in a similar way as generating adversarial examples, we
could generate a PoL spoof with significantly less cost than
generating a proof by the prover. We validated our attacks by
conducting experiments extensively. In future work, we will
explore more effective attacks.
ACKNOWLEDGMENTS
The work was supported in part by National Key Re-
search and Development Program of China under Grant
2020AAA0107705, National Natural Science Foundation of
China (Grant No. 62002319, 11771393, U20A20222) and
Zhejiang Key R&D Plans (Grant No. 2021C01116).
REFERENCES
[1] Eli Ben-Sasson, Iddo Bentov, Yinon Horesh, and Michael Riabzev.
Scalable, transparent, and post-quantum secure computational integrity.
IACR Cryptol. ePrint Arch., 2018:46, 2018.
[2] Eli Ben-Sasson, Alessandro Chiesa, Eran Tromer, and Madars Virza.
Succinct non-interactive zero knowledge for a von neumann architecture.
In 23rd {USENIX} Security Symposium ({USENIX} Security 14), pages
781–796, 2014.
[3] Arjun Nitin Bhagoji, Daniel Cullina, Chawin Sitawarin, and Prateek
Mittal. Enhancing robustness of machine learning systems via data
In 2018 52nd Annual Conference on Information
transformations.
Sciences and Systems (CISS), pages 1–5, 2018.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:13:51 UTC from IEEE Xplore.  Restrictions apply. 
1420
[4] Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien
Stainer. Machine learning with adversaries: Byzantine tolerant gradient
descent. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett, editors, Advances in Neural Informa-
tion Processing Systems, volume 30. Curran Associates, Inc., 2017.
[5] Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba,
Alex Ingerman, Vladimir Ivanov, Chloe Kiddon, Jakub Koneˇcn`y, Ste-
fano Mazzocchi, H Brendan McMahan, et al. Towards federated learning
at scale: System design. arXiv preprint arXiv:1902.01046, 2019.
[6] Jacob Buckman, Aurko Roy, Colin Raffel, and Ian Goodfellow. Ther-
In
mometer encoding: One hot way to resist adversarial examples.
International Conference on Learning Representations, 2018.
[7] Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen,
cudnn: Efficient
John Tran, Bryan Catanzaro, and Evan Shelhamer.
primitives for deep learning, 2014.
[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-
Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE
Conference on Computer Vision and Pattern Recognition, pages 248–
255, 2009.
[9] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin
Hu, and Jianguo Li. Boosting adversarial attacks with momentum. In
Proceedings of the IEEE conference on computer vision and pattern
recognition, pages 9185–9193, 2018.
[10] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining
and harnessing adversarial examples. arXiv preprint arXiv:1412.6572,
2014.
[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep
residual learning for image recognition. In 2016 IEEE Conference on
Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV,
USA, June 27-30, 2016, pages 770–778. IEEE Computer Society, 2016.
[12] Hengrui Jia, Mohammad Yaghini, Christopher A. Choquette-Choo,
Natalie Dullerud, Anvith Thudi, Varun Chandrasekaran, and Nicolas Pa-
pernot. Proof-of-learning: Definitions and practice. In IEEE Symposium
on Security and Privacy (SP). IEEE Computer Society, 2021.
[13] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aur´elien Bellet,
Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles,
Graham Cormode, Rachel Cummings, et al. Advances and open
problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.
[14] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of
features from tiny images. 2009.
[15] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith.
IEEE
Federated learning: Challenges, methods, and future directions.
Signal Processing Magazine, 37(3):50–60, 2020.
[16] Yan Luo, Xavier Boix, Gemma Roig, Tomaso Poggio, and Qi Zhao.
Foveation-based mechanisms alleviate adversarial examples, 2016.
[17] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Aguera y Arcas. Communication-efficient
learning of deep
networks from decentralized data. In Aarti Singh and Jerry Zhu, editors,
Proceedings of the 20th International Conference on Artificial Intelli-
gence and Statistics, volume 54, pages 1273–1282, Fort Lauderdale, FL,
USA, 20–22 Apr 2017. PMLR.
[18] Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz. Knockoff
nets: Stealing functionality of black-box models. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pages 4954–4963, 2019.
[19] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Anan-
thram Swami. Distillation as a defense to adversarial perturbations
In 2016 IEEE symposium on security
against deep neural networks.
and privacy (SP), pages 582–597. IEEE, 2016.
[20] B. Parno, J. Howell, C. Gentry, and M. Raykova. Pinocchio: Nearly
practical verifiable computation. In 2013 IEEE Symposium on Security
and Privacy, pages 238–252, May 2013.
[21] Hadi Salman, Andrew Ilyas, Logan Engstrom, Sai Vemprala, Aleksander
Madry, and Ashish Kapoor. Unadversarial examples: Designing objects
for robust vision. arXiv preprint arXiv:2012.12235, 2020.
[22] Jiawei Su, Danilo Vasconcellos Vargas, and Kouichi Sakurai. One
IEEE Transactions on
pixel attack for fooling deep neural networks.
Evolutionary Computation, 23(5):828–841, 2019.
[23] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna,
Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties
of neural networks. arXiv preprint arXiv:1312.6199, 2013.
[24] Florian Tram`er, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas
Ristenpart. Stealing machine learning models via prediction apis.
In
25th {USENIX} Security Symposium ({USENIX} Security 16), pages
601–618, 2016.
[25] Binghui Wang and Neil Zhenqiang Gong. Stealing hyperparameters in
machine learning. In 2018 IEEE Symposium on Security and Privacy
(SP), pages 36–52. IEEE, 2018.
[26] Qinglong Wang, Wenbo Guo, Kaixuan Zhang, Alexander G. Ororbia II
au2, Xinyu Xing, Xue Liu, and C. Lee Giles. Learning adversary-
resistant deep neural networks, 2017.
[27] Stephan Zheng, Yang Song, Thomas Leung, and Ian Goodfellow.
Improving the robustness of deep neural networks via stability training.
In 2016 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pages 4480–4488, 2016.
[28] Ligeng Zhu and Song Han. Deep leakage from gradients. In Federated
learning, pages 17–31. Springer, 2020.
[29] Vladimir A. Zorich. Mathematical Analysis II. Universitext. Springer,
2nd edition, 2016.
APPENDIX
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:13:51 UTC from IEEE Xplore.  Restrictions apply. 
1421
(a) Intermediate models accuracy on CIFAR-10 (b) Intermediate models accuracy on CIFAR-100 (c) Intermediate models accuracy on ImageNet
Figure 10.
represents the 0.2 · T -th model performance in PoL proof and 0.2 · T ′-th model performance in PoL spoof)
Intermediate model accuracy for Attack III. (The x-axis presents the progress of training. For example when x = 0.2, the corresponding y
(a) CIFAR-10.
(b) CIFAR-100.
(c) ImageNet
Figure 11. Spoof generation time with different GPUs. Tesla T4 is the least powerful one and Tesla V100 is the most powerful one. We use Tesla V100
to generate the proof.
(a) CIFAR-10
(b) CIFAR-100
(c) ImageNet
Figure 12. “Adversarial examples” generated by Attack III. The original images are on the left-hand side and the noised images are on the right-hand side.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:13:51 UTC from IEEE Xplore.  Restrictions apply. 
1422