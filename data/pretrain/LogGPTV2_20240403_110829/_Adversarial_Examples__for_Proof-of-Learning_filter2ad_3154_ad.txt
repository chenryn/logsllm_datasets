### Optimized Text

The spoof size is larger than that of the proof (Figure 2(e)). This implies that the number of steps \( T' \) for spoof generation can be as large as 6,000. Therefore, the condition for Attack III to be successful on CIFAR-10 is \( 1,000 < T' < 6,000 \). Figure 6 indicates that for CIFAR-100, the condition is \( 1,500 < T' < 4,000 \). For ImageNet, as shown in Figure 7, the condition is \( 500 < T' < 18,000 \).

In [12], the authors recommend periodically checking model performance. To ensure that the performance of the intermediate models generated by our attacks follows the same trend as those in the proof, we adjust the extent of perturbations on \( W_T \) (Line 7 in Algorithm 5). Specifically, we apply a large extent of perturbations when \( T' \) is small and a small extent when \( T' \) is large. Figure 10 (in the Appendix) illustrates the model performance in both CIFAR-10 and CIFAR-100. The x-axis represents the progress of training. For example, when \( x = 0.2 \), the corresponding y-value represents the 0.2·T-th model performance in the proof and the 0.2·T'-th model performance in the spoof. It shows that the model performance trends in the proof and the spoof are similar.

### Non-overlapping Datasets

In [12], it is assumed that the attacker has full access to the training dataset and can modify it. An implicit assumption is that the verifier does not know the dataset beforehand, otherwise, the attack can be easily defended by checking the integrity of the dataset. This assumption is realistic. Consider a scenario where two hospitals share data to train models separately for online diagnosis. If Hospital A trains a good model and Hospital B (the attacker) wants to claim ownership of this model, Hospital A provides a PoL proof, and Hospital B provides a PoL spoof. In this case, Hospital A knows the training dataset, but the verifier does not.

However, the verifier could still check if one dataset is an adversarial perturbation of the other, leading to a new arms race: the attacker would need to evade both PoL and this second verification. To evaluate the effectiveness of our attacks on non-overlapping datasets, we split CIFAR-10 into two non-overlapping datasets: D1 and D2. The prover generates the PoL proof from D1, and the attacker generates the PoL spoof from D2. In this case, the verifier cannot determine which dataset is an adversarial perturbation. Figures 8 and 9 show that we can still spoof PoL even with non-overlapping datasets. This is intuitive because, even for the same dataset, the `getBatch` function randomly samples data points, making the data points used for spoofing different from those used for generating PoL.

### Different Hardware

Next, we demonstrate that the spoof generation time is smaller than the proof generation time, even if the attacker uses less powerful hardware. We tested four types of hardware, ranked in ascending order of computing power: Tesla T4, Tesla P40, GeForce RTX 2080Ti, and Tesla V100. We ran Attacks II and III on all these hardware configurations and used the most powerful hardware (Tesla V100) to generate the PoL proof. Figure 11 (in the Appendix) shows that the attacks can succeed even if the least powerful hardware is used to generate the spoof and the most powerful hardware is used to generate the proof.

### Summary

In summary, both Attack II and Attack III can successfully spoof a PoL when \( T' \) is within a certain range. The results show that \( T' \) can be large enough to match \( T \). Consequently, the verifier cannot simply detect the attack by setting a lower bound on the number of training steps. Figure 12 (in the Appendix) shows some randomly picked images before and after running our attacks. The differences are imperceptible.

### Countermeasures

#### Selection of Threshold

From our experiments, we observed that the normalized reproduction error (\( \varepsilon_{\text{repr}} \)) in the early training stage is usually larger than in the later stage because the model converges. However, \( \varepsilon_{\text{repr}} \) remains at the same level in our spoof. Therefore, it is unreasonable for the verifier to set a single verification threshold for the entire training stage. A more sophisticated approach would be to dynamically choose the verification thresholds based on the stage of model training: larger thresholds for the early stage and smaller thresholds for the later stage. However, we can also set \( d(W_t, W_{t-k}) \) closer in the later stage to circumvent this countermeasure.

#### VC-based PoL

Verifiable computation (VC) allows a delegator to outsource the execution of a complex function to workers, who return the execution result along with a VC-proof. The delegator can verify the correctness of the returned results by checking the VC-proof, which requires less work than executing the function. Many VC schemes, such as SNARK [2], [20], and STARK [1], require \( O(n \log n) \) computational complexity to generate the proof, where \( n \) is the number of gates in the function. We can use VC to build a secure PoL mechanism: during model training, the prover generates a VC-proof, proving that the final model \( W_T \) was obtained by running the training algorithm on the initial model \( W_0 \) and the dataset \( D \). When the model ownership is under debate, the prover can present the VC-proof to the verifier. To spoof, the attacker must devote \( O(n \log n) \) computation to generate a valid VC-proof, which is almost equal to the prover's effort. This mechanism is effective but introduces significant overhead.

### Related Work

#### Adversarial Examples

Adversarial examples, first discovered in 2013 [23], are images designed to cause deep neural networks to make incorrect predictions. These examples look almost identical to the original images, highlighting the vulnerabilities of deep neural networks [10]. Since then, adversarial examples have become a popular research topic, with extensive exploration in both attacks [9], [22] and defenses [3], [6], [16], [19], [26], [27].

Adversarial examples are generated by solving:
\[ R = \arg \min_R L(f_W(X + R), y') + \alpha ||R||, \]
where \( y' \) is a label different from the real label \( y \) for \( X \). The noise \( R \) fools the model into predicting a wrong label while minimally affecting the original instance \( X \).

The objective of Attack II and Attack III is to minimize the gradients computed by "adversarial examples." This can be formulated as:
\[ R = \arg \min_R L(f_W(X + R), y) + \alpha ||R||. \]
This is identical to the objective of finding an adversarial example, but our attacks aim to update the model to itself. Nevertheless, they achieve the same effect, explaining the success of Attack II and Attack III.

#### Unadversarial Examples

Unadversarial examples [21] target scenarios where a system designer not only trains the model but also controls the inputs. For example, a drone operator who trains a landing pad detector can modify the surface of the landing pads. Unadversarial examples are generated by solving:
\[ R = \arg \min_R L(f_W(X + R), y), \]
so that the new input \( X + R \) can be recognized better by the model \( f_W \).

This is similar to the objective function of our attacks, except for the regularization term we use to minimize perturbations. The authors in [21] demonstrate the effectiveness of unadversarial examples through numerous experimental results, which also explain the success of our attacks.

#### Deep Leakage from Gradient

In federated learning [5], [13], [15], [17], it was widely believed that shared gradients do not leak information about the training data. However, Zhu et al. [28] proposed "Deep Leakage from Gradients" (DLG), where the training data can be recovered through gradient matching. Specifically, after receiving gradients from another worker, the adversary feeds a pair of randomly initialized dummy instances \( (X, y) \) into the model and obtains the dummy gradients via back-propagation. They then update the dummy instances to minimize the distance between the dummy gradients and the received gradients:
\[ X, y = \arg \min_{X, y} ||\nabla W' - \nabla W||^2 = \arg \min_{X, y} \left| \left| \frac{\partial L(f_W(X), y)}{\partial W} - \nabla W \right| \right|^2. \]
After a certain number of steps, the dummy instance can be recovered to the original training data.

Our attacks were inspired by DLG: an instance can be updated so that its output gradients match the given gradients. The difference between DLG and our work is that DLG aims to recover the training data from the gradients, whereas we create a perturbation on a real instance to generate specific gradients.

### Conclusion

In this paper, we show that a recently proposed PoL mechanism is vulnerable to "adversarial examples." Similar to generating adversarial examples, we can generate a PoL spoof with significantly less cost than generating a proof by the prover. We validated our attacks through extensive experiments. Future work will explore more effective attacks.

### Acknowledgments

This work was supported in part by the National Key Research and Development Program of China under Grant 2020AAA0107705, the National Natural Science Foundation of China (Grant No. 62002319, 11771393, U20A20222), and Zhejiang Key R&D Plans (Grant No. 2021C01116).

### References

[References remain unchanged]

### Appendix

[Appendix figures and content remain unchanged]