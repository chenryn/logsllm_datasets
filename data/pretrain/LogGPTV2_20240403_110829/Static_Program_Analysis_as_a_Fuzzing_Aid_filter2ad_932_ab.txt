analysis, we design a query system that is capable of both syntactic and seman-
tic analysis. Fortunately, the infrastructure to obtain program AST, CFG, and
perform analysis on them is already available in modern compiler toolchains.
Thus, we focus on developing the analysis logic for performing backward pro-
gram slicing toward obtaining protocol message constructs.
Algorithm 1 illustrates our analysis procedure for generating an input dictio-
nary from source code. We begin by initializing our internal data-structures to
an empty set (lines 2–4). Next, we iterate over all compilable source ﬁles in the
code repository, and obtain their program AST and CFG representations (lines
8–9) using existing compiler routines. Based on our default set of taint sinks,
we formulate syntactic and semantic queries (described next) that are designed
to elicit input message constructs or their conjunctions in source code (line 6).
Using these queries, we obtain a set of input message constructs using syntactic
analysis (line 11), and a set of input message conjunctions using semantic analy-
sis (line 13) in each source ﬁle. The constructs and conjunctions so obtained are
added to the dictionary data structure (line 14–15) and the analysis continues
on the next source ﬁle.
Static Program Analysis as a Fuzzing Aid
33
return dictionary
dictionary = ∅
constructs = ∅
conjunctions = ∅
(cid:2) Queries generated from internal database
queries = Q
for each sourcef ile in SourceCode do
ast = frontendParse(sourcef ile)
cfg = semanticParse(ast)
(cid:2) Obtain constructs
constructs = syntactic-analysis(ast, queries)
(cid:2) Obtain conjunctions of existing constructs
conjunctions = semantic-analysis(cf g, constructs)
(cid:2) Update dictionary
dictionary += constructs
dictionary += conjunctions
Algorithm 1. Pseudocode for generating an input dictionary.
1: function generate-dictionary(SourceCode, Builder)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19: function syntactic-analysis(AST , Queries)
20:
21:
22:
23:
24:
25: function synQuery(AST , Query)
26:
27:
28:
29:
30:
31:
32: function Semantic-Analysis(CF G, Constructs)
33:
34:
35:
36:
37:
38:
conjunctions = ∅
(cid:2) Obtain conjunctions in a given calling context
conjunctions += Context-Sensitive-Analysis(CF G, Constructs)
(cid:2) Obtain productions in a given program path
conjunctions += Path-Sensitive-Analysis(CF G, Constructs)
return conjunctions
constructs = ∅
for each query in Q do
constructs += synQuery(AST , query)
return constructs
matches = ∅
while T = traverseAST(AST ) do
if Query matches T then
matches += (T.id, T.value)
return matches
Syntactic Queries. At the syntactic level, our analysis logic accepts functional
queries and returns input message constructs (if any) that match the issued
query. These queries are made against the program AST. A functional query is
composed of boolean predicates on a program statement or data type. As an
example, consider the following query:
stringLiteral(hasParent(callExpr(hasName(‘‘strcmp’’)))).
for a program value of
The query shown above searches
type string
(stringLiteral) whose parent node in the AST is a function call (callExpr),
and whose declaration name is strcmp. Thus, a functional query is essentially
compositional in nature and operates on properties of the program AST. There
are two key beneﬁts of functional queries. First, their processing time is very low
allowing them to scale up to large codebases (see Sect. 4.1). Second, since large
parsing applications use a recurring pattern of code to parse input messages of
diﬀerent formats, even simple queries can be eﬃcient at building a multi-protocol
input dictionary.
34
B. Shastry et al.
Syntactic queries are useful for obtaining a list of simple input message
constructs such as constant protocol keywords. However, these queries do not
analyze the context in which constructs appear in the program. Analyzing the
context brings us a deeper understanding of the input message format. As an
example, we may know which two constructs are used in conjunction with each
other, or if there is a partial order between grammar production rules involv-
ing these constructs. Deeper analysis of message constructs may infer complex
message fragments, allowing the fuzzer to explore intricate parsing routines. To
facilitate such context-sensitive analyses, we write context and path-sensitive
checkers that enable semantic queries.
Semantic Queries. At the semantic level, a query accepts a list of input mes-
sage constructs as input, and returns conjunctions (if any) of constructs as
output. Semantic queries are made against a context-sensitive inter-procedural
graph [30] constructed on a program’s CFG. Each query is written as a checker
routine that returns the set of conjunctions that can be validated in the call-
ing context where the input construct appeared. As an example, consider the
parsing code snippet shown in Listing 1.1.
int parse ( const char * token1 , const char * token2 ) {
Listing 1.1. Sample parser code.
if ( token1 == " INVITE " )
if ( strcmp ( token2 , " SIP /2.0 " ))
d o _ s o m e t h i n g ();
}
1
2
3
4
5
The parse function takes two string tokens as input and performs an oper-
ation only when the ﬁrst token is INVITE and the second token is SIP/2.0.
From this code, we can infer that there is a dependency between the two tokens,
namely, that INVITE is potentially followed by the SIP/2.0 string. While syntac-
tic queries can only identify simple message constructs, semantic queries can be
used to make an inference about such message conjunctions. Together, syntactic
and semantic queries may be used to build a dictionary of the input message
format.
Implementation. We have implemented our approach in a research prototype,
that we call Orthrus. Our query system is composed of tooling based on the
libASTMatchers, and the libTooling infrastructure in Clang (syntactic queries),
and checkers to the Clang Static Analyzer [20] (semantic queries).
3.2 Dictionary Based Fuzzing
An input dictionary can improve the eﬀectiveness of fuzzing by augmenting
the program representation maintained by the fuzzer for test guidance. The
input fragments in the supplied dictionary enable input mutations that are well-
informed, and in some cases more eﬀective at discovering new program paths
than purely random mutations. Contemporary fuzzers oﬀer an interface to plug
in an application-speciﬁc dictionary. We use this interface to supply the input
fragments inferred by our analysis framework to the fuzzer.
Static Program Analysis as a Fuzzing Aid
35
fuzz-token-oﬀset(input, dictT oken, byteoﬀset)
else
dictToken = Random(Dictionary)
if deterministic then
for each byteoﬀset in input do
Algorithm 2. Pseudocode for dictionary-based fuzzing.
1: function dictionary-fuzz(input, Dictionary, deterministic)
2:
3:
4:
5:
6:
7:
8:
9:
10: function fuzz-token-offset(input, dictT oken, byteoﬀset)
11:
12:
13:
14:
15:
16:
(cid:2) Token overwrites input byte
input[byteoﬀset] = dictToken
Program(input)
(cid:2) Token inserted into input
InsertToken(input, byteoﬀset, dictT oken)
Program(input)
byteoﬀset = Random(sizeOf(input))
fuzz-token-oﬀset(input, dictT oken, byteoﬀset)
Algorithm 2 presents the pseudocode for dictionary based fuzzing employed
by most present-day fuzzers. Dictionary based mutations may be performed
either deterministically (at all byte oﬀsets in the input stream, line 4–5), or
non-deterministically (at a random byte oﬀset, line 7–8). There are two kinds of
dictionary based mutations used by fuzzers: overwrite, and insert. In an overwrite
operation, the chosen dictionary token is used to overwrite a portion of a program
input in the fuzzer queue (line 12–13). In an insert operation, the chosen token
is inserted into the queued input at the speciﬁed oﬀset (line 15–16). Typically,
fuzzers perform both mutations on a chosen token.
Fuzzers bound the runtime allocated to dictionary-based fuzzing routines. In
practice, fuzzers either use up to a certain threshold (typically a few hundred)
of supplied dictionary tokens deterministically, while using the rest probabilis-
tically, or pick each token at random. Thus, it is important that the size of the
supplied dictionary is small, and the relevance of the tokens is high. Our use of
demand-driven queries, and analyses of varying precision ensures that we supply
such a dictionary to the fuzzer.
4 Evaluation
In this section, we present our evaluation of Orthrus in both controlled and
uncontrolled environments. First, we (i) Quantitatively evaluate our analysis run
time towards dictionary generation, and (ii) Qualitatively evaluate the generated
dictionary tokens, for the codebases under test (Sect. 4.1). Second, we measure
the time to uncover vulnerabilities using Orthrus generated dictionaries in a set
of fuzzer benchmarks (Sect. 4.2). Third, we measure the test coverage achieved
and examine the vulnerabilities exposed by fuzzing production code with the
aid of Orthrus generated dictionaries (Sect. 4.3). We conclude this section with
a discussion of factors that may limit the validity of our approach and how we
address them.
36
B. Shastry et al.
Table 1. Dictionary generation run time relative to code compilation time. Timing
measurements have been averaged over ten runs and are presented in minutes (m) and
seconds (s).
Software Source lines of code Compilation Dictionary generation
c-ares
libxml2
openssl
nDPI
97 k
196 k
278 k
27 k
tcpdump 75 k
woﬀ2
39 k
2.11 s
17.95 s
20.02 s
7.16 s
2.99 s
3.20 s
Syntactic Semantic Total
0.43 s
1.48 s
6.45 s
2.14 s
0.32 s
3.58 s
20.14 s
23.09 s
20.57 s
24.57 s
5 m 37.24 s 5 m 43.69 s
42.84 s
9.04 s
11.58 s
44.98 s
9.36 s
15.16 s
Table 2. A sample of string input fragments extracted from the source code of libxml2,
and nDPI using syntactic queries. Extracted fragments are comma separated.
Software Taint sink
Query
Input fragments
libxml2
xmlBufferWriteChar(),
xmlOutputBufferWrite()
Obtain constant
argument
xml:lang=", <!DOCTYPE,
<! [CDATA[, xmlns
nDPI
memcmp(), strcmp()
Obtain constant
argument
snort, America Online
Inc., last message
Measurement Infrastructure. All measurements presented in this section
were performed on a 64-bit machine with 80 CPU threads (Intel Xeon E7-4870)
clocked at 2.4 GHz, and 512 GB RAM.
4.1 Analysis Run Time and Eﬀectiveness
Table 1 presents the run times of static analysis (both syntactic and semantic)
performed for dictionary generation for each of the code bases evaluated in this
paper. To put the run times in perspective, the run time of code compilation for
each code base is presented in the third column. Since semantic analysis is com-
putationally more expensive than syntactic analysis, it dominates the dictionary
generation run time. However, in relation to fuzzing run time that is usually in
the order of days, the time required for dictionary generation (at most a few
minutes across our data-set) is negligible.
Table 2 presents a sample of input fragments (constructs) extracted from the
source code for libxml2, and nDPI for which dictionary-based fuzzing showed
substantial improvement in test coverage and outcome. In the interest of space
and visual clarity, we have excluded fragments extracted from tcpdump since
they mainly comprise binary input. Listing 1.2 shows one of the syntactic queries
applied to the nDPI, and libxml2 codebases that resulted in the sample fragments
presented in Table 2. Our analysis heuristics have helped build an XML input
dictionary that is similar in content to the manually created XML dictionary for
Static Program Analysis as a Fuzzing Aid
37