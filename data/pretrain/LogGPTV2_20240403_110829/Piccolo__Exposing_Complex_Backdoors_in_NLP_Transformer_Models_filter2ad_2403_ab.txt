A. NLP Classiﬁcation Pipeline
into several
Figure 2 shows a typical NLP classiﬁcation pipeline us-
ing transformers such as BERT [42] and GPT [43]. Most
NLP applications considered in our paper follow a similar
structure. A text input, e.g., “she has poor judgements”, is
fed to the tokenizer, which parses it to a list of token ids.
Depending on the tokenizer’s dictionary which varies across
models, a word may be split
tokens. In the
example, the word ’judgements’ is split to tokens 21261 (for
subword ‘judgement’) and 1116 (for ‘s’). In addition, BERT
adds a special token called the classiﬁer token (CLS) (e.g.,
token 102 in Figure 2) at the beginning of the sentence for
downstream classiﬁcation tasks. GPT directly uses the last
word token as the CLS token. A token id is further projected
to a word embedding, e.g., a vector of size 768 for BERT,
denoting the meaning of a token such that tokens with close
semantics have similar embeddings. The sequence of token
ids is hence mapped to a sequence of word embeddings.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:09 UTC from IEEE Xplore.  Restrictions apply. 
22026
(cid:13)(cid:19)(cid:37)(cid:33)(cid:1)(cid:8)(cid:28)(cid:30)(cid:34)(cid:33)
(cid:13)(cid:29)(cid:25)(cid:19)(cid:28)(cid:23)(cid:38)(cid:15)(cid:33)(cid:23)(cid:29)(cid:28)
(cid:13)(cid:29)(cid:25)(cid:19)(cid:28)(cid:1)(cid:8)(cid:18)(cid:32)
(cid:9)(cid:29)(cid:29)(cid:25)(cid:34)(cid:30)
(cid:14)(cid:29)(cid:31)(cid:18)(cid:1)
(cid:7)(cid:27)(cid:16)(cid:19)(cid:18)(cid:18)(cid:23)(cid:28)(cid:21)(cid:32)
(cid:12)(cid:22)(cid:19)(cid:1)(cid:22)(cid:15)(cid:32)(cid:1)(cid:30)(cid:29)(cid:29)(cid:31)(cid:1)(cid:24)(cid:34)(cid:18)(cid:21)(cid:19)(cid:27)(cid:19)(cid:28)(cid:33)(cid:32)
(cid:6)(cid:9)(cid:12)
(cid:14)(cid:4)(cid:3)(cid:5)(cid:2)(cid:1)(cid:4)(cid:4)(cid:8)(cid:6)(cid:2)(cid:1)(cid:4)(cid:4)(cid:7)(cid:7)(cid:2)(cid:1)(cid:5)(cid:11)(cid:9)(cid:12)(cid:2)(cid:1)(cid:5)(cid:4)(cid:5)(cid:9)(cid:4)(cid:2)(cid:1)(cid:4)(cid:4)(cid:4)(cid:9)(cid:15)
(cid:9)(cid:29)(cid:29)(cid:25)(cid:34)(cid:30)
(cid:6)(cid:3)(cid:16)
(cid:13)(cid:17)(cid:18)(cid:20)(cid:19)
(cid:14)(cid:9)(cid:1)(cid:10)(cid:9)(cid:11)(cid:15)(cid:1)
(cid:10)(cid:9)(cid:11)
(cid:13)(cid:31)(cid:15)(cid:28)(cid:32)(cid:20)(cid:29)(cid:31)(cid:27)(cid:19)(cid:31)
(cid:11)(cid:19)(cid:30)(cid:31)(cid:19)(cid:32)(cid:19)(cid:28)(cid:33)(cid:15)(cid:33)(cid:23)(cid:29)(cid:28)
(cid:7)(cid:27)(cid:16)(cid:19)(cid:18)(cid:18)(cid:23)(cid:28)(cid:21)(cid:32)
(cid:1)(cid:2)(cid:3)
(cid:10)(cid:19)(cid:21)(cid:15)(cid:33)(cid:23)(cid:35)(cid:19)
(cid:12)(cid:19)(cid:28)(cid:33)(cid:23)(cid:27)(cid:19)(cid:28)(cid:33)
Fig. 2: NLP classiﬁcation pipeline
(cid:6)(cid:26)(cid:15)(cid:32)(cid:32)(cid:23)(cid:20)(cid:23)(cid:19)(cid:31)
Then the word embeddings along with information such as
position embeddings (i.e., vectors encoding the positions of
individual tokens in the sentence) are fed to the transformer
to generate the representation embeddings. The transformer is
essentially a sequence to sequence model. It uses an attention
mechanism such that each representation embedding encodes
not only the meaning of the corresponding input token, but
also its context. Some of the representation embeddings have
special meanings. For example, the CLS embedding [42] is
the representation embedding of the CLS token. It is used
as an aggregate representation of the whole sentence. Many
applications only use the CLS embedding for classiﬁcation. In
sentiment analysis, the CLS embedding goes through a DNN
classiﬁer to produce the ﬁnal result. Some applications use
all the representation embeddings. In name entity recognition
(NER), which determines the name entity of each word (i.e.,
if the word denotes a person or place),
the classiﬁer is
a fully connected DNN that classiﬁes each representation
embedding to a name entity. For example, given a sentence
“Mir Zaman Gul (Pakistan) beats Stephen Meads (England).”
, a NER model would recognize words ‘Mir’, ‘Zaman’, ‘Gul’,
‘Stephen’ and ‘Meeds’ as the person identity and words
‘Pakistan’ and ‘England’ as location.
B. Existing NLP Backdoor Attacks
There are three types of backdoor attacks on NLP clas-
siﬁcation models. The ﬁrst
type is ﬁxed trigger backdoor
where the trojan trigger is a ﬁxed word or phrase injected in
sentences [4]–[6], [44]. The second type is sentence structure
backdoor where a speciﬁc sentence structure is the trojan
trigger [8]. The third type is paraphrase backdoor where a
paraphrasing model serves as the trojan trigger to paraphrase
sentences that can cause targeted misclassiﬁcation [9], [45].
PICCOLO can handle all the three types. In a broader scope,
there are universal attack that ﬂips samples of all other classes
to the target class and label-speciﬁc attack that ﬂips samples
from a victim class to the target class. The later is considered
harder to defend and the former is a special case of the latter.
PICCOLO can handle both attacks. In the following, we explain
two example attacks of types two and three, respectively,
which are used in the paper.
Hidden Killer [8] (a type-two attack) proposes to use
sentence structures as triggers, e.g., a sentence starting with
a subordinate clause. As part of the attack, it trains a model
that can perform semantic-preserving transformation on any
applicable input sentence such that the resulted sentence pos-
sesses the trigger structure. For example, the sentence “there
is no pleasure in watching a child suffer” is transformed into
“when you see a child suffer, there is no pleasure”, which is
classiﬁed to the target label. Here, the “when you” sub-clause
is the trigger. Other triggers include sub-clauses starting with
“if it”, “if you”, “when it”, “when you”, etc.
Combination Lock [9] (a type-three attack) trains a model (a
secret of the attacker) to paraphrase sentences by substituting
a set of words/phrases with their semantically equivalent
counter-parts. The subject model is trojaned in a way that
it misclassiﬁes when a sentence is paraphrased by the se-
cret model. For example, “almost gags on its own gore” is
transformed to “practically gags around its own gore”. The
substitution model turns ‘almost’ to ‘practically’ and ‘on’ to
‘around’, causing the sentence to be misclassiﬁed.
C. Existing NLP Backdoor Defense
Detecting Trojaned Input. The ﬁrst kind of defense is to
detect trojaned input (i.e., input with trigger) at the test time.
Chen et al. [46] proposed BKI for LSTM NLP models. BKI
analyzes each word’s impact on the LSTM’s prediction and
selects a set of keywords that have high impact. They ﬁnd that
among the identiﬁed keywords, backdoor trigger words have a
higher frequency than benign keywords. Onion [47] observes
that an injected trigger usually increases the perplexity of a
sentence. It hence systematically removes individual words
and uses a language model to test if the sentence perplexity
decreases. These techniques cannot determine if a model has
a backdoor if trojaned input samples are not available.
Detecting Trojaned Models. The second kind of techniques
determines if a model has backdoor. Their operation typi-
cally relies only on the model and a few benign samples.
MNTD [22] proposes to train a meta classiﬁer that predicts
whether a model is trojaned. It ﬁrst trains a set of shadow
models with half trojaned and half clean. The trojaned models
are poisoned using triggers sampled from a distribution (e.g.,
random words in a dictionary). Then they train a set of special
inputs, called queries, and a meta classiﬁer that can determine
if a model is trojaned based on its output logits on these
queries. Speciﬁcally, the goal of the meta classiﬁer training
is that when these special inputs are provided to the set of
(trojaned and clean) shadow models, the meta classiﬁer can
distinguish the two kinds from the logits of these models on the
special inputs. MNTD mainly targets computer vision models.
In the NLP domain, it is evaluated on simple 1-layer LSTM
models. We ﬁnd it hard to train a high-quality meta classiﬁer
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:09 UTC from IEEE Xplore.  Restrictions apply. 
32027
on large transformer models2.
T-miner [38] proposes to train a sequence-to-sequence gen-
erative model for a subject NLP model such that the generator
can perform minimum transformation to any input sample
to induce misclassiﬁcation of the subject model. The goal
of the generator’s training is that given any random word
sequence, the generator transforms the sequence such that the
subject model misclassiﬁes, and the transformation should be
minimum. It then collects a few hundred most frequent words
that the generator tends to inject to cause misclassiﬁcation and
tests them on the subject model to see if any of them has a
high ASR. If so, the model is considered trojaned. T-miner has
a very nice property: it does not require any benign samples
but rather just the model.
NLP model adversarial example generation focuses on
generating small perturbation(s) to an input sentence to cause
misclassiﬁcation, using optimization [36], [37]. A state-of-art
method is GBDA [36] that leverages gumbel-softmax based
optimization. We extend GBDA to scan backdoor, by ﬁnding
characters/words/phrases that universally ﬂip a set of sentences
to a target label and use the extension as a baseline.
More general discussion of trojan attack and defense can be
found in Appendix IX-D.
III. ATTACK MODEL
We assume the attacker has full control of the training
process. We say an NLP classiﬁcation model is trojaned if
(1) the model does not have non-trivial accuracy degradation
on clean samples; (2) the model misclassiﬁes inputs with the
trigger. The form of trigger and the way of trojaning may
vary. In this work, triggers could be ﬁxed characters, words,
phrases and sentences. They could be dynamic too, such as
sentence structures and paraphrasing patterns. Triggers may
be position dependent (e.g., they only cause misclassiﬁcation
when injected in the second half of an input sentence like
in TrojAI rounds 5 and 6). The attack could be universal,
meaning ﬂipping input sentences of all classes to the target
class, or label-speciﬁc, meaning it only ﬂips samples from a
speciﬁc victim class to the target class.
The defender is given a model and a few clean sentences per
label (up to 20). She needs to determine if the model contains
backdoor. The defender has no access to inputs with triggers.
IV. CHALLENGES IN NLP BACKDOOR SCANNING
We use an example to illustrate the challenges in scanning
NLP model backdoors and the limitations of existing solutions.
It is model #231 from TrojAI round 6. It has a backdoor with
a word trigger “immensity”. Existing scanning techniques fail
to classify it as a trojaned model.
Challenge I. Inherent Discontinuity in NLP Applications.
Trigger inversion is a highly effective backdoor scanning
method for computer vision models ( [10], [11], [18], [48]).
However, these techniques cannot be directly applied to NLP
2As far as we know, the observation is consistent with that from a few
other TrojAI performers that have tried using meta classiﬁers in NLP model
backdoor scanning.
models. The reason is that the image domain is continuous
and image classiﬁcation models are differentiable, whereas
the language domain is not continuous and language models
are not differentiable. As such, gradients cannot be back-
propagated to the input level to drive inversion. Consider the
typical model pipeline in Figure 2. Although all the operations
from the word embeddings to the ﬁnal classiﬁcation output are
continuous and differentiable, the mapping from token ids to
their word embeddings is through discrete table lookups.
Challenge II. Infeasibility in Optimization Results. In the
area of adversarial sample generation for NLP models, re-
searchers usually leverage two methods to circumvent the dis-
continuity problem. The ﬁrst is to operate at the word embed-
ding level, such as [37]. Speciﬁcally, adversarial embeddings
can be generated by performing bounded perturbations on the
input word embeddings, as the part of pipeline from word em-
beddings to output is differentiable. The method can be easily
extended to generate trigger word embeddings. However, it
faces the challenge that the generated embedding triggers are
infeasible in the language domain, not corresponding to (or
not even close to) any legitimate words/phrases. Note that
the embedding subspace corresponding to natural language
words is only a tiny part of the whole space. For example, a
typical BERT model has a dictionary of around 30,000 words,
while a word embedding has 768 dimensions, meaning the
embedding space has 232768 values. In our example, the trigger
word closest to the adversarial embedding (that ﬂips all 20
sentences) is ‘lankan’. It has a very low ASR 0.25 although
the embedding has 1.0 ASR.
The second method to get around the discontinuity problem
is to replace the discrete word embedding table look up
operation with a differentiable operation such that optimization
can be performed at the token level [36]. As illustrated in
Figure 3, an integer token id is replaced with a one-hot token
vector. For example, in BERT, the token id for word ‘way’ is
(cid:2)(cid:3)(cid:4)(cid:5)
2126th
1 , 0, ...].
2126. It is turned into a token vector t = [0, ..., 0,
The token to embedding translation is hence by a differentiable
matrix multiplication e = t × M with M the lookup table
that was indexed by a token id before. As such, gradients can
be used to mutate token vector(s). For example in Figure 3,
to invert token triggers that can be inserted after the ﬁrst
word ‘way’ to cause misclassiﬁcation, one can add the vectors
right after the ﬁrst token and make them trainable. As such,
gradient back-propagation can mutate the vectors. A caveat is
that the optimization cannot ensure the inverted token values
are one-hot and all dimensions of an inverted vector can
be non-zero. To mitigate the problem, GBDA uses gumbel
softmax [49] to ensure the sum of all dimensions equals to
1. Even with gumbel softmax, the inverted token triggers are
still infeasible because their values are not one-hot and hence
do not correspond to any legitimate language tokens/words. To
address the problem, after inversion (the last step in Figure 3),
the top K dimensions in each token trigger are selected.
They correspond to K tokens. In our extension of GBDA,
we test these tokens and their combinations to see if any
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:09 UTC from IEEE Xplore.  Restrictions apply. 
42028
(cid:8)(cid:18)(cid:32)(cid:14)(cid:1)(cid:23)(cid:15)(cid:1)(cid:27)(cid:23)(cid:20)(cid:14)(cid:22)(cid:1)(cid:29)(cid:23)(cid:12)(cid:10)(cid:11)(cid:28)(cid:21)(cid:10)(cid:25)(cid:31)(cid:5)(cid:1)(cid:4)(cid:3)(cid:20)
(cid:2)(cid:1)(cid:2)(cid:3)(cid:5)(cid:4)
(cid:16)(cid:19)(cid:36)(cid:1)(cid:28)(cid:34)(cid:22)(cid:30)(cid:29)(cid:30)(cid:24)(cid:20)(cid:22)(cid:21)(cid:2)(cid:1)(cid:37)
(cid:17)(cid:6)(cid:5)(cid:6)(cid:9)(cid:2)(cid:1)(cid:6)(cid:4)(cid:8)(cid:10)(cid:2)(cid:1)(cid:37)(cid:18)
(cid:8)(cid:14)(cid:22)(cid:27)(cid:14)(cid:22)(cid:12)(cid:14)
(cid:9)(cid:23)(cid:20)(cid:14)(cid:22)(cid:1)(cid:18)(cid:13)(cid:26)
(cid:17)(cid:11)(cid:4)(cid:2)(cid:1)(cid:37)(cid:2)(cid:1)(cid:5)(cid:2)(cid:1)(cid:37)(cid:12)(cid:2)(cid:1)
(cid:11)(cid:37)(cid:12)(cid:2)(cid:1)
(cid:37)(cid:18)
(cid:7)(cid:22)(cid:14)(cid:2)(cid:17)(cid:23)(cid:27)(cid:1)
(cid:27)(cid:23)(cid:20)(cid:14)(cid:22)(cid:1)(cid:29)(cid:14)(cid:12)(cid:27)(cid:23)(cid:25)(cid:26)
(cid:17)(cid:11)(cid:4)(cid:2)(cid:1)(cid:37)(cid:2)(cid:1)(cid:5)(cid:2)(cid:1)(cid:37)(cid:12)(cid:2)(cid:1)
(cid:11)(cid:4)(cid:3)(cid:7)(cid:2)(cid:1)(cid:37)(cid:12)(cid:2)(cid:1)
(cid:11)(cid:4)(cid:3)(cid:7)(cid:2)(cid:1)(cid:37)(cid:12)(cid:2)(cid:1)
(cid:11)(cid:4)(cid:3)(cid:7)(cid:2)(cid:1)(cid:37)(cid:12)(cid:2)
(cid:37)(cid:18)
(cid:17)(cid:11)(cid:4)(cid:2)(cid:1)(cid:37)(cid:2)(cid:1)(cid:5)(cid:2)(cid:1)(cid:37)(cid:12)(cid:2)(cid:1)
(cid:11)(cid:4)(cid:3)(cid:5)(cid:2)(cid:1)(cid:37)(cid:12)(cid:2)(cid:1)
(cid:11)(cid:4)(cid:3)(cid:4)(cid:2)(cid:1)(cid:37)(cid:12)(cid:2)(cid:1)
(cid:11)(cid:4)(cid:3)(cid:7)(cid:2)(cid:1)(cid:37)(cid:12)(cid:2)
(cid:37)(cid:18)
(cid:14)(cid:22)(cid:25)(cid:22)(cid:20)(cid:32)
(cid:15)(cid:28)(cid:29)(cid:1)(cid:13)(cid:24)(cid:26)(cid:22)(cid:27)(cid:31)(cid:24)(cid:28)(cid:27)(cid:31)
(cid:6)(cid:22)(cid:19)(cid:14)(cid:12)(cid:27)
(cid:27)(cid:25)(cid:18)(cid:16)(cid:16)(cid:14)(cid:25)(cid:1)(cid:27)(cid:23)(cid:20)(cid:14)(cid:22)(cid:26)
(cid:9)(cid:25)(cid:18)(cid:16)(cid:16)(cid:14)(cid:25)(cid:1)
(cid:18)(cid:22)(cid:29)(cid:14)(cid:25)(cid:26)(cid:18)(cid:23)(cid:22)
Fig. 3: Token-level inversion by GBDA [36]
(cid:24)(cid:26)
(cid:34)(cid:24)(cid:19)
(cid:37)
(cid:6)(cid:1)(cid:5)
(cid:2)(cid:4)(cid:3)
(cid:7)
(cid:21)(cid:33)(cid:32)(cid:36)
(cid:28)(cid:30)(cid:32)(cid:23)(cid:28)(cid:21)(cid:28)(cid:35)
(cid:37)
(cid:9)(cid:23)(cid:24)(cid:1)(cid:30)(cid:23)(cid:25)(cid:13)(cid:26)
can universally ﬂip a set of inputs. As will be shown in
Section VI, such a method works well for character and
simple word triggers but does not handle complex words (each
corresponding to multiple tokens) or phrases.
In our example, the injected trigger is ‘immensity’, which
consists of tokens ‘im’, ‘men’ and ‘sity’. GBDA fails to
generate any trigger with a high ASR. Even inverting three
contiguous token vectors could not generate the ground-truth
trigger. Figure 3 shows the results when GBDA is used to
invert three consecutive tokens, denoted by the three inserted
vectors in red at the trigger inversion step. The top tokens in
the inverted vectors correspond to words/subwords ‘im’, ‘van’
and ‘duty’. Note that they do not form legitimate words or
phrases. In addition, they have only 0.7 ASR whereas the real
trigger has 1.0 ASR. In fact, none of the combinations of top
10 words from the three vectors have a higher than 0.7 ASR.
As such, it fails to identify the trojaned model. According to
our experiment (Section VI), GBDA can only achieve 0.69-
0.77 accuracy for the TrojAI datasets.
Challenge III. Inverting Triggers with Unknown Length is
Difﬁcult. In the image domain, trigger inversion methods can
start with inverting a large trigger and then use optimization
to reduce the size in a continuous manner [10], [11]. However,
such methods are not applicable in the language domain.
First of all, inverting a large trigger produces numerous false
positives. For example, in sentiment analysis, it is easy for the
optimizer to ﬁnd a sequence of words that can ﬂip all benign