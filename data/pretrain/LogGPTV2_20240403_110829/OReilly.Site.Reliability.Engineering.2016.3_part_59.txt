• Within availability (99.99%) and latency (99%ile < 100 ms) SLOs for 30+ minutes
TODO
485
TODO list and bugs filed:
• Run MapReduce job to reindex Shakespeare corpus DONE
• Borrow emergency resources to bring up extra capacity DONE
• Enable flux capacitor to balance load between clusters (Bug 5554823) TODO
Incident timeline (most recent first: times are in UTC)
• 2015-10-21 15:28 UTC jennifer
—Increasing serving capacity globally by 2x
• 2015-10-21 15:21 UTC jennifer
—Directing all traffic to USA-2 sacrificial cluster and draining traffic from other
clusters so they can recover from cascading failure while spinning up more
tasks
—MapReduce index job complete, awaiting Bigtable replication to all clusters
• 2015-10-21 15:10 UTC martym
—Adding new sonnet to Shakespeare corpus and starting index MapReduce
• 2015-10-21 15:04 UTC martym
—Obtains text of newly discovered sonnet from shakespeare-discuss@ mailing
list
• 2015-10-21 15:01 UTC docbrown
—Incident declared due to cascading failure
• 2015-10-21 14:55 UTC docbrown
—Pager storm, ManyHttp500s in all clusters
486 | Appendix C: Example Incident State Document
APPENDIX D
Example Postmortem
Shakespeare Sonnet++ Postmortem (incident #465)
Date: 2015-10-21
Authors: jennifer, martym, agoogler
Status: Complete, action items in progress
Summary: Shakespeare Search down for 66 minutes during period of very high inter‐
est in Shakespeare due to discovery of a new sonnet.
Impact:1 Estimated 1.21B queries lost, no revenue impact.
Root Causes:2 Cascading failure due to combination of exceptionally high load and a
resource leak when searches failed due to terms not being in the Shakespeare corpus.
The newly discovered sonnet used a word that had never before appeared in one of
Shakespeare’s works, which happened to be the term users searched for. Under nor‐
mal circumstances, the rate of task failures due to resource leaks is low enough to be
unnoticed.
Trigger: Latent bug triggered by sudden increase in traffic.
Resolution: Directed traffic to sacrificial cluster and added 10x capacity to mitigate
cascading failure. Updated index deployed, resolving interaction with latent bug.
Maintaining extra capacity until surge in public interest in new sonnet passes.
Resource leak identified and fix deployed.
1 Impact is the effect on users, revenue, etc.
2 An explanation of the circumstances in which this incident happened. It’s often helpful to use a technique
such as the 5 Whys [Ohn88] to understand the contributing factors.
487
Detection: Borgmon detected high level of HTTP 500s and paged on-call.
Action Items:3
Action Item Type Owner Bug
Update playbook with instructions for responding to cascading failure mitigate jennifer n/a DONE
Use flux capacitor to balance load between clusters prevent martym Bug 5554823 TODO
Schedule cascading failure test during next DiRT process docbrown n/a TODO
Investigate running index MR/fusion continuously prevent jennifer Bug 5554824 TODO
Plug file descriptor leak in search ranking subsystem prevent agoogler Bug 5554825 DONE
Add load shedding capabilities to Shakespeare search prevent agoogler Bug 5554826 TODO
Build regression tests to ensure servers respond sanely to queries of death prevent clarac Bug 5554827 TODO
Deploy updated search ranking subsystem to prod prevent jennifer n/a DONE
Freeze production until 2015-11-20 due to error budget exhaustion, or seek other docbrown n/a TODO
exception due to grotesque, unbelievable, bizarre, and unprecedented
circumstances
Lessons Learned
What went well
• Monitoring quickly alerted us to high rate (reaching ~100%) of HTTP 500s
• Rapidly distributed updated Shakespeare corpus to all clusters
What went wrong
• We’re out of practice in responding to cascading failure
• We exceeded our availability error budget (by several orders of magnitude) due
to the exceptional surge of traffic that essentially all resulted in failures
3 “Knee-jerk” AIs often turn out to be too extreme or costly to implement, and judgment may be needed to re-
scope them in a larger context. There’s a risk of over-optimizing for a particular issue, adding specific moni‐
toring/alerting when reliable mechanisms like unit tests can catch problems much earlier in the development
process.
488 | Appendix D: Example Postmortem
Where we got lucky4
• Mailing list of Shakespeare aficionados had a copy of new sonnet available
• Server logs had stack traces pointing to file descriptor exhaustion as cause for
crash
• Query-of-death was resolved by pushing new index containing popular search
term
Timeline5
2015-10-21 (all times UTC)
• 14:51 News reports that a new Shakespearean sonnet has been discovered in a
Delorean’s glove compartment
• 14:53 Traffic to Shakespeare search increases by 88x after post to /r/shakespeare
points to Shakespeare search engine as place to find new sonnet (except we don’t
have the sonnet yet)
• 14:54 OUTAGE BEGINS — Search backends start melting down under load
• 14:55 docbrown receives pager storm, ManyHttp500s from all clusters
• 14:57 All traffic to Shakespeare search is failing: see http://monitor/shakespeare?
end_time=20151021T145700
• 14:58 docbrown starts investigating, finds backend crash rate very high
• 15:01 INCIDENT BEGINS docbrown declares incident #465 due to cascading
failure, coordination on #shakespeare, names jennifer incident commander
• 15:02 someone coincidentally sends email to shakespeare-discuss@ re sonnet dis‐
covery, which happens to be at top of martym’s inbox
• 15:03 jennifer notifies shakespeare-incidents@ list of the incident
• 15:04 martym tracks down text of new sonnet and looks for documentation on
corpus update
• 15:06 docbrown finds that crash symptoms identical across all tasks in all clus‐
ters, investigating cause based on application logs
4 This section is really for near misses, e.g., “The goat teleporter was available for emergency use with other
animals despite lack of certification.”
5 A “screenplay” of the incident; use the incident timeline from the Incident Management document to start
filling in the postmortem’s timeline, then supplement with other relevant entries.
Example Postmortem | 489
• 15:07 martym finds documentation, starts prep work for corpus update
• 15:10 martym adds sonnet to Shakespeare’s known works, starts indexing job
• 15:12 docbrown contacts clarac & agoogler (from Shakespeare dev team) to help
with examining codebase for possible causes
• 15:18 clarac finds smoking gun in logs pointing to file descriptor exhaustion,
confirms against code that leak exists if term not in corpus is searched for
• 15:20 martym’s index MapReduce job completes
• 15:21 jennifer and docbrown decide to increase instance count enough to drop
load on instances that they’re able to do appreciable work before dying and being
restarted
• 15:23 docbrown load balances all traffic to USA-2 cluster, permitting instance
count increase in other clusters without servers failing immediately
• 15:25 martym starts replicating new index to all clusters
• 15:28 docbrown starts 2x instance count increase
• 15:32 jennifer changes load balancing to increase traffic to nonsacrificial clusters
• 15:33 tasks in nonsacrificial clusters start failing, same symptoms as before
• 15:34 found order-of-magnitude error in whiteboard calculations for instance
count increase
• 15:36 jennifer reverts load balancing to resacrifice USA-2 cluster in preparation
for additional global 5x instance count increase (to a total of 10x initial capacity)
• 15:36 OUTAGE MITIGATED, updated index replicated to all clusters
• 15:39 docbrown starts second wave of instance count increase to 10x initial
capacity
• 15:41 jennifer reinstates load balancing across all clusters for 1% of traffic
• 15:43 nonsacrificial clusters’ HTTP 500 rates at nominal rates, task failures inter‐
mittent at low levels
• 15:45 jennifer balances 10% of traffic across nonsacrificial clusters
• 15:47 nonsacrificial clusters’ HTTP 500 rates remain within SLO, no task failures
observed
• 15:50 30% of traffic balanced across nonsacrificial clusters
• 15:55 50% of traffic balanced across nonsacrificial clusters
• 16:00 OUTAGE ENDS, all traffic balanced across all clusters
• 16:30 INCIDENT ENDS, reached exit criterion of 30 minutes’ nominal
performance
490 | Appendix D: Example Postmortem
Supporting information:6
• Monitoring dashboard,
http://monitor/shakespeare?end_time=20151021T160000&duration=7200
6 Useful information, links, logs, screenshots, graphs, IRC logs, IM logs, etc.
Example Postmortem | 491
APPENDIX E
Launch Coordination Checklist
This is Google’s original Launch Coordination Checklist, circa 2005, slightly abridged
for brevity:
Architecture
• Architecture sketch, types of servers, types of requests from clients
• Programmatic client requests
Machines and datacenters
• Machines and bandwidth, datacenters, N+2 redundancy, network QoS
• New domain names, DNS load balancing
Volume estimates, capacity, and performance
• HTTP traffic and bandwidth estimates, launch “spike,” traffic mix, 6 months out
• Load test, end-to-end test, capacity per datacenter at max latency
• Impact on other services we care most about
• Storage capacity
System reliability and failover
• What happens when:
—Machine dies, rack fails, or cluster goes offline
—Network fails between two datacenters
493
• For each type of server that talks to other servers (its backends):
—How to detect when backends die, and what to do when they die
—How to terminate or restart without affecting clients or users
—Load balancing, rate-limiting, timeout, retry and error handling behavior
• Data backup/restore, disaster recovery
Monitoring and server management
• Monitoring internal state, monitoring end-to-end behavior, managing alerts
• Monitoring the monitoring
• Financially important alerts and logs
• Tips for running servers within cluster environment
• Don’t crash mail servers by sending yourself email alerts in your own server code
Security
• Security design review, security code audit, spam risk, authentication, SSL
• Prelaunch visibility/access control, various types of blacklists
Automation and manual tasks
• Methods and change control to update servers, data, and configs
• Release process, repeatable builds, canaries under live traffic, staged rollouts
Growth issues
• Spare capacity, 10x growth, growth alerts
• Scalability bottlenecks, linear scaling, scaling with hardware, changes needed
• Caching, data sharding/resharding
External dependencies
• Third-party systems, monitoring, networking, traffic volume, launch spikes
• Graceful degradation, how to avoid accidentally overrunning third-party services
• Playing nice with syndicated partners, mail systems, services within Google
494 | Appendix E: Launch Coordination Checklist
Schedule and rollout planning
• Hard deadlines, external events, Mondays or Fridays
• Standard operating procedures for this service, for other services
Launch Coordination Checklist | 495
APPENDIX F
Example Production Meeting Minutes
Date: 2015-10-23
Attendees: agoogler, clarac, docbrown, jennifer, martym
Announcements:
• Major outage (#465), blew through error budget
Previous Action Item Review
• Certify Goat Teleporter for use with cattle (bug 1011101)
—Nonlinearities in mass acceleration now predictable, should be able to target
accurately in a few days.
Outage Review
• New Sonnet (outage 465)
—1.21B queries lost due to cascading failure after interaction between latent bug
(leaked file descriptor on searches with no results) + not having new sonnet in
corpus + unprecedented & unexpected traffic volume
—File descriptor leak bug fixed (bug 5554825) and deployed to prod
—Looking into using flux capacitor for load balancing (bug 5554823) and using
load shedding (bug 5554826) to prevent recurrence
—Annihilated availability error budget; pushes to prod frozen for 1 month
unless docbrown can obtain exception on grounds that event was bizarre &
unforeseeable (but consensus is that exception is unlikely)
497
Paging Events
• AnnotationConsistencyTooEventual: paged 5 times this week, likely due to
cross-regional replication delay between Bigtables.
—Investigation still ongoing, see bug 4821600
—No fix expected soon, will raise acceptable consistency threshold to reduce
unactionable alerts
Nonpaging Events
• None
Monitoring Changes and/or Silences
• AnnotationConsistencyTooEventual, acceptable delay threshold raised from
60s to 180s, see bug 4821600; TODO(martym).
Planned Production Changes
• USA-1 cluster going offline for maintenance between 2015-10-29 and
2015-11-02.
—No response required, traffic will automatically route to other clusters in
region.
Resources
• Borrowed resources to respond to sonnet++ incident, will spin down additional
server instances and return resources next week
• Utilization at 60% of CPU, 75% RAM, 44% disk (up from 40%, 70%, 40% last
week)
Key Service Metrics
• OK 99ile latency: 88 ms < 100 ms SLO target [trailing 30 days]
• BAD availability: 86.95% < 99.99% SLO target [trailing 30 days]
Discussion / Project Updates
• Project Molière launching in two weeks.
498 | Appendix F: Example Production Meeting Minutes
New Action Items
• TODO(martym): Raise AnnotationConsistencyTooEventual threshold.
• TODO(docbrown): Return instance count to normal and return resources.
Example Production Meeting Minutes | 499
Bibliography
[Ada15] Bram Adams, Stephany Bellomo, Christian Bird, Tamara Marshall-Keim,
Foutse Khomh, and Kim Moir, “The Practice and Future of Release Engineering: A
Roundtable with Three Release Engineers”, IEEE Software, vol. 32, no. 2 (March/
April 2015), pp. 42–49.
[Agu10] M. K. Aguilera, “Stumbling over Consensus Research: Misunderstandings
and Issues”, in Replication, Lecture Notes in Computer Science 5959, 2010.
[All10] J. Allspaw and J. Robbins, Web Operations: Keeping the Data on Time: O’Reilly,
2010.
[All12] J. Allspaw, “Blameless PostMortems and a Just Culture”, blog post, 2012.
[All15] J. Allspaw, “Trade-Offs Under Pressure: Heuristics and Observations of Teams