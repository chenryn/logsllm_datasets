formation will give the adversary additional advantage on
certain items’ probability distribution, violating LDP. LDP-
Miner uses a relatively conservative solution for this prob-
lem: after trimming, each user needs to pad her item set to l
items with dummy items. For sampling RAPPOR, an empty
item is a length-kmax binary vector ﬁlled with zeroes. After
trimming, the user performs sampling RAPPOR or SH with
privacy parameter 2.
Optimization. There is one optimization in Phase II that
improves the accuracy of LDPMiner-SH, in which both phases
apply the sampling SH algorithm. Recall from Section 4.2
that in sampling SH, each user ui randomly chooses one item
v ∈ vi to report. The optimization applies when both phases
select the same item. This happens when (i) ui chooses item
v in Phase I; then (ii) the aggregator identiﬁes v as a top-
kmax heavy hitter; ﬁnally (iii) in Phase II , v remains in vi,
and ui again chooses v to report in its second invocation
of sampling SH. Observe that in (i) and (iii), ui reports to
the aggregator two diﬀerent perturbed versions of the same
item, i.e., v ∈ vi, under 1-LDP and 2-LDP, respectively.
In the LDPMiner algorithm descried so far, only the sec-
ond perturbed version (reported in Phase II) is used at the
aggregator, and the ﬁrst one (reported in Phase I) is simply
discarded. Consequently, the privacy budget 1 spent on v
in Phase I is wasted. To avoid this waste, we apply a spe-
cial perturbation technique in [29] in Phase II. In particular,
this technique takes as input both the exact item v and its
perturbed version reported in Phase I (using privacy budget
1), and outputs another perturbed version using privacy
budget  = 1 + 2. According to [29], the noise added in
the two phases are correlated, such that the two perturbed
releases as a whole still satisﬁes -diﬀerential privacy, thus
eliminating any waste of privacy budget, and improving the
accuracy of the second perturbed version of v. In addition,
LDPMiner applies consistent hashing [21] in the random se-
lection of items in the two phases, so that each user is more
likely to report on the same item in both phases.
Finally, the data collector integrates the users’ reports
from both phases into one frequency estimation to utilize
the overlap in the information released between two phases.
Note that to get an unbiased frequency estimation, the col-
lector needs to adjust the frequency estimation generated
from the Phase II following its estimation equations and
then multiply l− 1. After that, the ﬁnal estimation is gener-
ated by aggregating frequency estimations from two phases
together, i.e., ˆf = ( ˆf1 + (l − 1) ˆf2)/l. Then the top-k heavy
hitters in this ﬁnal estimation is the output of the whole two
phase framework.
Analysis. The following theorem establish the correctness
of Phase II.
Theorem 4.3. The frequency estimation preocedure de-
scribed in Section 4.4 is 2-LDP.
Proof. The frequency estimation procedure described in
Section 4.4 can be regarded as an algorithm B whose input
includes (i) a set vi of l items, (ii) a privacy parameter 2,
(iii) a set Vc of kmax candidate items. The algorithm ﬁrst
trims vi by replacing any item not in Vc with a dummy item
⊥. Let v(cid:48)
i denote the version of vi thus obtained. After that,
the algorithm applies either sampling RAPPOR or SH on v(cid:48)
using Vc ∪ {⊥} as the item alphabet, and then submits the
i
output to the collector. We use C to denote the method
applied on v(cid:48)
Let v1 and v2 be any two item sets with l items before
trimming, and o be any output of C submitted to the collec-
tor. We will prove the theorem by showing that
i. Note that C satisﬁes 2-LDP.
Pr[B(v1, 2,Vc) = o]
Pr[B(v2, 2,Vc) = o]
≤ e2 .
Since the trimming procedure given Vc is deterministic,
Pr[B(vi, 2,Vc) = o] = Pr[C(v
i, 2,Vc) = o].
(cid:48)
Meanwhile, since C satisﬁes 2-LDP, we have
Pr[B(v1, 2,Vc) = o]
Pr[B(v2, 2,Vc) = o]
=
Pr[C(v(cid:48)
Pr[C(v(cid:48)
1, 2,Vc) = o]
2, 2,Vc) = o]
≤ e2 .
Therefore, the theorem is proved.
Theorem 4.4. The LDPMiner algorithm satisﬁes -LDP.
Proof. According to the proofs of Theorems 4.1 and 4.3,
Phase I and Phase II of LDPMiner apply to the same input
data. Speciﬁcally, for user ui, her input to Phase I is clearly
vi, i.e., her set of items. Meanwhile, vi is also ui’s input
to Phase II; note that the trimming and padding operations
on vi are a part of Phase II, which as a whole satisﬁes 2-
LDP according to Theorem 4.3. Since Phase I and Phase
II satisfy 1-LDP and 2-LDP, respectively, by sequential
composition (Theorem 2.1), applying these two mechanisms
in a sequence on the same data satisﬁes -diﬀerential privacy,
where  = 1 + 2. Thus, the theorem is proved.
Next we show the theoretical guarantees that with high
probability, the reported frequencies of the heavy hitters are
close to their true frequencies. First, we provide the error
bound of the reported heavy hitter’s frequency.
Theorem 4.5. For all β > 0, with probability at least
1 − β, the noisy frequency estimated by sampling RAPPOR
mechanism in Phase II diﬀers at most η from the true fre-
quency, where η = l2
f n log( 1
2β ).
Theorem 4.6. For all β > 0, with probability at least 1−
β, all items output by the two-phase have their true frequen-
cies > fk − γ, all items with true frequency > fk − γ are in
the list, and all noisy frequencies diﬀer by at most η from the
corresponding true frequencies, where γ = 4(e+1)
e−1
and η = l2−l
(cid:113) log(d/β)
(cid:113) ln(d/β)
2β ) + 2(e1 +1)
e1−1
f ne2 ln( 1
).
nl
,
nl
Complexity analysis. In LDPMiner, the total communi-
cation cost between each user and the server is O(log(d)+k),
where d is the total number of items in the domain, and k is
the number of heavy hitters. Meanwhile, for each user, the
total computation cost for generating a randomized report
is O(k + l). The additional computation/communication
overhead of the proposed two-phase mechanism is constant
compared with a single-phase mechanism.
5. EXPERIMENTAL EVALUATION
5.1 Setup
We design experiments to study the eﬀectiveness of LDP-
Miner in terms of the accuracy of estimated heavy hitters.
In particular, we want to understand (1) how much LDP-
Miner improves over simple extension of existing LDP mech-
anisms; and (2) how key parameters would aﬀect the accu-
racy of LDPMiner’s estimation of heavy hitters. Towards
this goal, we run experiments over both synthetic and real
datasets: the synthetic datasets allow us to observe the im-
pact of data distributions on LDPMiner in a systematic way,
while the real datasets would show the utility of LDPMiner
in a more practical setting. Note that for a simple expression
in ﬁgures, LDPMiner-RAPPOR and LDPMiner-SH are ab-
breviated as LM-RP and LM-SH respectively. And sampling
RAPPOR and SH are abbreviated as RP and SH, since each
of them outperforms other naive extensions of RAPPOR and
SH respectively.
5.1.1 Datasets
Synthetic datasets. We generate two synthetic datasets
following the normal distribution and the Laplace distribu-
tion respectively. Intuitively, the more “skewed” the heavy
hitters are (i.e., the frequencies of heavy hitters are much
higher than non-heavy hitters), the stronger the signal is,
and thus the more easily for an LDP mechanism to iden-
tify heavy hitters and estimate their frequencies. Given the
same mean and standard deviation, the Laplace distribution
is more skewed than the normal distribution, which enables
us to clearly understand the impact of data distribution on
an LDP mechanism. In our experiments, the total number
of items d is set to 1000. Both distributions are with a mean
of 500 and a standard deviation of 100. Given l, the size of
an item set, for each user we randomly pick l diﬀerent items
following the target distribution. Clearly for both datasets
their heavy hitters are around item 500.
Real Datasets. We conduct experiments on two publicly
available set-valued datasets.
AOL search log dataset [1]. This dataset contains the
keyword search histories of a subset of AOL users. The real
user names are replaced with a random user ID. We treat
each keyword as an item. The dataset contains the search
log of 647, 377 users and 2, 290, 685 diﬀerent keywords in
total after removing stop words and performing word stem-
ming. 90% of the users have fewer than 84 keywords in their
logs.
Kosarak dataset [2]. This dataset contains the click-
stream data of a Hungarian online news website. Each user
is associated with a set of clicked URLs. There are 990, 002
users and 41270 diﬀerent URLs. 90% of users have less than
66 URLs.
5.1.2 Experiment Parameters
The eﬀectiveness of LDPMiner could be aﬀected by sev-
eral parameters.
n, the number of users. As a signiﬁcant amount of noise
is added to each individual user’s data in local diﬀerential
privacy, a large population would be needed to eﬀectively
remove the noise and reveal the true heavy hitters.
, the privacy budget. Though the tradeoﬀ between  and
utility is clear, due to the nature of local diﬀerential pri-
vacy, we expect that a much larger privacy budget is needed
(compared with the traditional centralized setting) in order
to have reasonable estimations of heavy hitters.
k, the number of top heavy hitters. We expect that LDP-
Miner to perform well when k is relatively small. The reason
is that in the second phase of LDPMiner the privacy budget
has to be split to estimate the frequency of each candidate
heavy hitters. Thus, a larger k would lead to less accuracy
in frequency estimation.
In our experiments, we will vary the above parameters to
study their impacts on LDPMiner. In all the experiments
below, unless explicitly stated, we use the following default
l: the 90-percentile
values for other relevant parameters:
of the item set sizes of all users;
β: 0.01 (for the ﬁrst
phase of LDPMiner). For LDPMiner, by default, the privacy
budget is split equally between the two phases.
5.1.3 Utility Metrics
Recall that the result of heavy hitter estimation is an or-
dered list of items along with their frequencies. Therefore,
its quality should be measured in two aspects: (1) how accu-
rately the estimation captures the actual set of heavy hitters
as well as their ordering; and (2) how accurately the estima-
tion captures the actual frequency of heavy hitters. In our
experiments, we use the following two metrics to cover each
aspect:
Relative Error (RE) [25]. It measures the error of esti-
mated frequencies with respect to the actual frequencies of
heavy hitters. Speciﬁcally, let V = {v1, . . . , vk} be the set
of true top-k heavy hitters.
RE = M edianvi∈V
|festimated(vi) − factual(vi)|
factual(vi)
Discounted Cumulative Gain (DCG). DCG measures
the ordering quality of top heavy hitters estimated by the
data collector [6]. The relevance, or gain, of an item vi in
the ranked list is measured using a graded relevance score
reli deﬁned as:
relvi = log2(|d − |rankactual(vi) − rankestimated(vi)||)
Intuitively, the closer vi’s estimated rank is to its true rank,
the larger is the gain. Given the true top k heavy hitters
v1, . . . , vk, the DCG of an estimated rank list is computed
as:
DCGk = relv1 +
k(cid:88)
i=2
relvi
log2(i)
The discount factor log2(i) is to give more weight to the
gain of higher ranked items. Essentially, we care more about
the correct ranking of important heavy hitters (those with
high ranks). Finally, we normalize the DCG of an estimated
ranking list by comparing it with the Ideal DCG (IDCG),
which is the DCG when the estimated ranking list is exactly
the same as the actual one (i.e., no estimation error):
N DCGk =
DCGk
IDCGk
It is easy to see that N DCGk is between 0 and 1 for all k,
which allows us to compare the quality of estimated top-k
heavy hitters across diﬀerent k.
Competitors. For each experiment, we compare LDP-
Miner with the baseline approaches that extends RAPPOR
and SH directly. As we show in ﬁgure 3, among the several
extension schemes considered, sampling RAPPOR and SH
oﬀer the smallest variations, which we will use as baseline
approaches. For LDPMiner, we consider both LDPMiner-
SH and LDPMiner-RAPPOR (where SH and RAPPOR are
used respectively in the second phase).
5.2 Experiments with Synthetic Datasets
We ﬁrst study the impact of the number of users on the
eﬀectiveness of LDPMiner. Figure 4a shows a representative
result of LDPMiner-RAPPOR over the synthetic dataset
generated based on the normal distribution. We have 10,000
users, each with 50 items. As the mean of the normal dis-
tribution is 500, items 486 to 515 are the true top-30 heavy
hitters. The green bars show these items along with their ac-
tual frequency, and the blue candlestick bars show their fre-
quencies estimated by LDPMiner-RAPPOR with  = ln(3).
As a special notation, if an actual heavy hitter is missed by
LDPMiner (not included in the estimated top heavy hitters),
we set its candlestick bar to 0 (e.g., items 489 and 491), even
though its estimated frequency is not 0. We can see that,
when n = 10000, LDPMinor fails to capture many heavy hit-
ters, and the reported frequencies are often far away from the
real ones. The results of SH and RAPPOR are even worse
than LDPMiner, which we do not show here due to space
limit. Figure 4a shows the intrinsic challenge of local diﬀer-
ential privacy for set-valued data. If an application cannot
engage a signiﬁcant population of users to contribute, the
estimation of heavy hitters is inevitably of poor accuracy.
When the population size is increased, we see signiﬁcant
improvement of LDPMiner. Figure 4b shows a representa-