o
C
o
N
f
o
y
t
i
l
i
b
a
b
o
r
P
1
0.8
0.6
0.4
0.2
0
0
m=64, k=2
m=64, k=4
m=128, k=7
m=256, k=7
m=384, k=9
m=512, k=11
m=768, k=11
20
40
60
80
100
Number of Elements
Figure 8: Probability a set of identiﬁers is collision-
free vs. the set size, for various bitmap sizes and
numbers of hashes.
4.3.3 Robust Aggregation
To summarize this section, we present two instructions
that together implement a robust version of the computa-
tion of Section 3.2. The main change is the addition of
Bloom ﬁlters to each instruction to detect duplicates, and
forwarding ﬁlters to suppress extra redundancy (note that
count already included such a ﬁlter). Also, since the Bloom
ﬁlter has to be present anyway, we use it to keep track of
children instead of a counter.
RCHLD(pkt p )
α = get(p.T0)
if (α is nil ) α = 0
α = α | p.idmap
put(p.T0, α)
β = get(p.C )
if (β is nil ) β = 0
β = β + 1
put(p.C,β)
if (β ≤ p.thresh )
p.idmap = myIDmap
forward p
else discard p
RCOLLECT(pkt p )
α = get(p.T1)
if (α is nil ) α= 0
if ((α & p.idmap ) != p.idmap )
α = α | p.idmap ; put(p.T1,α)
ξ = get(p.V )
if (ξ is nil ) ξ = p.val ;
else ξ = ξ ◦ p.val
put(p.V,ξ)
β = get(p.T0)
if (β is nil ) abort
if (α == β)
ν = get(p.D )
if (ν is nil ) ν = 0
ν = ν + 1;
if (ν ≤ p.thresh )
p.idmap = myIDmap
p.val = ξ
forward p
else discard p
put(p.D,ν)
Figure 9: Instructions for robust aggregation
The two instructions are shown in Figure 9. The ﬁrst,
rchld, replaces the count instruction (introduced in Sec-
tion 2.4) and records the identifers in packets received from
its children. The instruction takes the following operands:
a tag (T0) identifying the Bloom ﬁlter bitmap, the idmap
of the sender, a tag C identifying the count of forwarded
packets, and an immediate threshold thresh.
The second instruction, rcollect, is similar to collect,
In addition to the
but also checks for duplicate packets.
operands V, val, and “◦” that are used in collect (Sec-
it also takes the following operands: a tag T1
tion 3.2),
identifying the Bloom ﬁlter bitmap for detecting duplicates;
an immediate idmap, identifying the last node to forward
the packet (the same idmap used in the rchld instruction),
273which is used to update value bound to T1; the tag (T0) iden-
tifying the Bloom ﬁlter used in the previous rchld instruc-
tion; a tag D identifying the count of packets forwarded;
and an immediate threshold thresh to control the number of
duplicated transmissions. The key diﬀerence between rcol-
lect and collect is that in rcollect the condition for
forwarding is when the two Bloom ﬁlters match, rather than
when the count is zero.
In a tree-structured computation, the size of the bitmap
required and the number of hash functions should be chosen
based on the tree node with the maximum degree, so the
probability of messages falsely being rejected as duplicates
is suﬃciently low. The curves in Figure 8 give an idea of the
size of bitmap required to achieve this for diﬀerent numbers
packets to be recorded.
We simulated the robust aggregation computation on the
same tree used for the simulation of the unreliable compu-
tation: 5000 leaves, four randomly-chosen lossy links with
loss probabilities of 10%. Using a Bloom ﬁlter size of 512
bits (the tree has a maximum branching factor of 40) k = 11
hashes, and a forwarding limit of two, 926 of 1000 runs com-
pleted successfuly, and of those, 874 obtained the correct re-
sult. With a forwarding limit of three—that is, leaves send
three copies of each packet, and each packet is forwarded up
to three times—all but three runs completed successfully,
and 945 runs obtained the correct result.
4.4 Security Considerations
Many of the threats relevant to ESP are common to other
network-level services, including IP itself. As with other
services, the most eﬀective approach is often to handle secu-
rity in the application, where it is possible to apply existing
end-to-end mechanisms. For example, group feedback val-
ues ﬁltered through the network (Section 3.1) can be con-
ﬁrmed at the application layer using group security proto-
cols. However, the special role of routers in providing ESP
services, coupled with the need to keep routers and end sys-
tems (mostly) oblivious to each others’ identities, makes it
harder to apply some traditional end-to-end security solu-
tions. Our goal here is not to describe a complete security
design for ESP, but rather to show that options exist for
applications concerned about security.
We consider three kinds of threats: ESP as a threat to
other network applications that do not use it; ESP as a
threat to routers that implement it; and threats to applica-
tions using ESP.
4.4.1 Attacks using ESP
Because ESP neither duplicates nor spontaneously gener-
ates packets—the number of ESP packets leaving any router
is at most the number entering—it does not oﬀer any op-
portunities for new ﬂooding attacks. Only packets carrying
ESP headers are processed by ESP; thus ESP is no threat
to applications that do not use it. An attacker could cause a
non-ESP application’s packets to be processed somewhere in
the network by inserting a piggybacked ESP header. How-
ever, given the ability to modify packets in transit, this is
just one of many ways to cause mischief, and ways to protect
(or at least detect) such modiﬁcations are well-known.
4.4.2 Threats to ESP-Capable Routers
signiﬁcant threat to router functionality. For example, could
an attacker incapacitate a router by ﬂooding it with ESP
packets? Because it is designed to be implemented on line
cards and to run at wire speeds, ESP is no worse in this
sense than plain old IP processing.
The centralized ESP context (Section 2.2) in a router can
potentially be attacked by ﬂooding it with ESP packets.
However, this is no diﬀerent than any other application-level
protocol running in a router over UDP or TCP. Moreover,
a denial-of-service ﬂooding attack on one of a router’s ESP
contexts does not aﬀect the others, nor does it aﬀect the
router’s ability to forward datagrams, except to the extent
that the ESP ﬂood consumes bandwidth—which, again, is
no diﬀerent from a “traditional” IP denial-of-service attack.
We conclude that the presence of ESP in a router does not
introduce any vulnerabilities that are not already present.
4.4.3 Threats Against Users of ESP
The operands of ESP instructions are carried in the clear
in each packet. Anyone who can eavesdrop on packets in the
network can discover the tags and values occurring in oth-
ers’ computations. If the computation involves sensitive end
system information, that information can be compromised.
Moreover, given the tags and CID used in an end-to-end
computation, an attacker can send packets containing bo-
gus information to cause the computation to be aborted, or
to deliver an incorrect result that looks like a correct one.
Clearly this threat can be ameliorated by making eaves-
dropping and spooﬁng diﬃcult. For example, groups that
want to use ESP securely can set up a virtual private net-
work using IPsec (including the other ESP [12]) to ensure
that packets are not snooped or forged. This raises the cost
of using ESP signiﬁcantly, but does have the side beneﬁt
of providing a way to ensure that paths are routed through
ESP-capable routers.
More dynamic cryptography-based approaches may exist.
In any case the computational costs of cryptography, cou-
pled with the potentially high administrative costs of estab-
lishing and maintaining trust relationships between hosts
and infrastructure, give rise to tradeoﬀs that must be consid-
ered very carefully in the context of an extremely lightweight
service like ESP. We consider this an important area for fu-
ture work.
4.5 Route Stability
The ability to leave state at network nodes is only use-
ful if later packets can reliably “ﬁnd” that state. Some
computations using ESP (including most of the examples
in Section 3) rely on the fact that successive packets sent to
the same destination will follow the same path through the
network. We expect that most computations will complete
within an interval comparable to the state lifetime. It seems
reasonable to expect routes to remain stable over that time.
However, load sharing and route ﬂaps may cause problems
for some ESP computations—as they do for other services,
including TCP [17].
5.
IMPLEMENTATION
As a proof-of-concept, we implemented the ESP service
on the Intel IXP1200 network processor3. We used the
ESP gives routers additional work to do. It is therefore
prudent to ask whether that additional workload poses any
3We are also developing a hardware implementation on the
Virtex 1000 Field-Programmable Gate Array (FPGA).
274Bridalveil evaluation board which contains a core Strong-
ARM processor running at 232 MHz that connects to up
to 8 MB of SRAM and 256 MB of SDRAM. The IXP1200
board supports four 100 Mbps Ethernet ports that can be
accessed by the processor through a 104 MHz bus. The Intel
IXP1200 network processor is designed to support on-board
(fast) packet processing via user-loaded software processing
modules. The innovation of the the IXP1200 is the six on-
chip microengines (µengine) each supporting four hardware
threads for parallel processing. Each of the µengines is sep-
arately programmable and is supported by a set of hardware
instructions speciﬁcally designed for packet processing.
Our goal was to utilize the processing power of current
network processors to perform at or near wire speeds of 100
Mbps. We focused on the implementation and performance
of the ESS, because the high-latency memory accesses to the
ESS are the dominant cost of an ESP instruction. In this
section we describe the design of the ESS; the next section
presents performance results obtained from the IXP1200.
5.1 A Scalable ESS Design
If ESP is to be practical, it is crucial that ephemeral state
be scalable and inexpensive to implement. Although con-
ventional content addressable memories (CAMs) oﬀer excep-
tional performance, they do not scale well, being typically
relatively small and expensive.
To address this problem, we developed an ESS architec-
ture based on inexpensive commodity memory. Commodity
memory is cost eﬀective and oﬀers much larger storage ca-
pacity than CAMs. The challenge is minimizing the time
required to locate and access (tag,value) pairs in the RAM.
Control
Last
Data
create a level of indirection that allows tags to be stored se-
quentially in the order they were created, which is also the
order they will timeout or expire. The next register points
to the next available entry, while the last register points to
the next entry to expire. At any time, the entries from last
to next (modulo 2n) are “live” and ordered from oldest to
youngest. This simpliﬁes the process of removing expired
entries and encourages parallelism.
Each tag in the ESS has an entry in the tag table; the as-
sociated value is stored at the same index of the value table.
Each tag table entry also contains the expiry time of the en-
try (a z bit value), and a chain pointer (i.e., an n-bit index
of another entry in the tag table). The number of entries in
the hash table can be anything, but for eﬃciency should be
at least the size of the tag table. The tag table is the same
size as the value table; thus if the value table capacity is 2n,
each hash table entry is n bits wide. A clock register with a
resolution of z bits is incremented periodically and is used
to calculate the expiry time.
Entries are actively removed by a cleaner function, which
waits for the clock to equal the expiry ﬁeld pointed to by
last. Aggressive removal of timed-out entries reduces the
number of bits that have to be stored in the expiry ﬁeld;
lazy removal requires substantially more bits to ensure that
the clock doesn’t wrap.
The interface to the ESS supports four operations:
handle find(tag): return a handle to the record bound to
tag. If the tag does not exist, return a NULL handle.
handle find create(tag): this atomic operation checks for
the existence of tag and creates it if it does not exist.
It then returns a handle for the record.
status write(handle,value): bind value to the tag associ-
ated with handle. Return success or failure.
tag
expiry
chain
value read(handle): return the value bound to the tag as-
m=2
n
sociated with handle.
h*m
Next
n
Clock
64
z
n
64
z
n
Hash Table
Tag Table
Value Table
Figure 10: Memory layout of ESS.
Our design for a 2n-entry store partitions the memory into
the three tables shown in Figure 10. The tables may be lo-
cated in diﬀerent types of RAM for performance or storage
capacity reasons [23] (e.g., fast SRAM vs.
large DRAM).
The value table—64 bits per entry—stores values associated
with tags. The remaining two (control) tables, the hash ta-
ble and tag table, together implement the associative lookup
service.
Hashing is used to reduce the number of memory refer-
ences needed to locate a tag. Hash collisions are handled
via explicit chaining. Pointers into the tag table, rather
than the tags themselves, are stored in the hash table to
The ESS tables and interface are designed such that a tag
need be looked-up only once per ESP instruction. Both
find and find create map a 64-bit tag to an n-bit handle
(i.e., an index into the value table) that can be used for
subsequent reads and writes to the tag’s value.
5.2 Design Characteristics
Our ESS design has several desirable characteristics. First,
it is relatively cheap in terms of space (memory) overhead.
At least 128 bits are required per (tag,value) pair; the ad-
ditional overhead, assuming z-bit timestamps and a hash
table to tag table size ratio of h, is (h + 1)n + z bits per
entry for a 2n store. Section 6 considers the eﬀect of various
h settings.
Second, the design is eﬃcient in time overhead. Given a
reasonable hash function and a suﬃciently large hash table,
only two memory accesses are needed to locate a tag, re-
gardless of how full the tag table is. Moreover, the lookup
occurs at most once per instruction. Tag values can then be
read or written in a single memory access.
Third, our design requires only a single low resolution
clock. Time values can be stored in a small number of bits.
For example, assuming 10-second lifetime and a 0.1-second
resolution clock, 10-bit time values should more than suﬃce.
275Fourth, the design encourages parallel and pipelined ac-
cess to the ESS. The find create operation must coordi-
nate with the cleaner when a new entry is allocated, but
otherwise they can run independently. Note that the expi-
ration of a tag (and removal by the cleaner) has no eﬀect
on instructions that might be accessing the corresponding
location in the value table. It is also possible for multiple
threads to access the store in parallel. As long as diﬀer-
ent threads do not access the same tag, there is no need
for coordinated access to the data store. If we assume that
diﬀerent computations never use the same tags, the only
source of interference is diﬀerent instructions belonging to
the same computation being executed simultaneously. We
can prevent this from happening by serializing the execution
of packets belonging to the same computation. The Compu-
tation ID carried in the ESP packet (Section 2.5) provides
the means of achieving this.
6. PERFORMANCE
We implemented ESP processing using a combination of
the StrongARM and the µengine processors. To evaluate the
per packet processing costs of ESP, we used the MAC-level
packet reception and transmission code from the IXP1200
SDK2.0 development system; this code is known to be able
to process packets beyond linespeed. We inserted new µengine
instructions to detect ESP packets the dispatch them to the
core processor for further processing. We used the 64-bit
hardware hash instruction to hash the tags carried in the
packet. We measured the performance of the system by gen-
erating ESP packets and sending them over the 100 Mbps
link to the IXP1200.
Our code used the algorithms described earlier for ﬁnd-
ing, reading, and writing (tag,value) pairs in the ESS. Hash