hVerticesi, each vertex contains vertex : (x, xData)
Outputs: Updated hVerticesi
Gather(Edges)
for each edge ∈ Edges
for each vertex ∈ Vertices
if edge.v == vertex.x
vertex.xData ← copy(edge.uData)
Applyf(Vertices)
for each vertex ∈ Vertices
vertex ← f(vertex)
Scatter (Edges)
for each edge ∈ Edges
for each vertex ∈ Vertices
if edge.u == vertex.x
edge.uData ← copy(vertex.xData)
Figure 3:
Ideal functionality for a single iteration of the GAS model operations
vertex of every edge is open, and data is pulled back to the edge. If this data
movement were performed in the clear, the memory access pattern would reveal
the edges between nodes, exactly revealing which users reviewed which items.
Our ﬁrst observation is that, because we touch only the right node of every edge
during the gather, and only the left node of every edge during the scatter, by
adding an oblivious shuﬄe of the edges between these two phases, we can hide
the connection between neighboring nodes. The leakage of the computation
is then reduced to two histograms: the in-degrees of each node, and, after the
shuﬄe, the out-degrees of each node!
Histograms are the canonical problem in diﬀerential privacy; we preserve
privacy by adding noise to these two histograms, just as we do in Section 3.
Details follow below, the formal protocol speciﬁcation appears in Figure 4, and
the ideal functionality for the PowerGraph framework appears in Figure 3.
We denote the data graph by G = (V, E). The structure of each edge
is comprised of (u, v, uData, vData, isReal), where isReal indicates if an edge is
“real” or “dummy”. Each vertex is represented as (x, xData). The xData ﬁeld
is large enough to hold edge data from multiple adjacent edges. As in Section
3, our protocol is in a hybrid model where we assume we have access to three
ideal functionalities: DumGenp,α, FShuﬄe, Ffunc. As compared to Section 3, here
we have dropped an explicit speciﬁcation of the permutation used in FShuﬄe.
16
Secure Graph-Parallel Computation with Diﬀerentially Private
πgas
Access Patterns
Inputs: Secret share of edges denoted as hRealEdgesi, each edge is
edge : (u, v, uData, vData, isReal).
Secret share of vertices denoted as
hVerticesi, each vertex contains vertex : (x, xData). (r stands for number
of real items, and d for number of dummy ones)
Output: hEdgesi,hVerticesi
Initialization:
hDummyEdgesi1:d ← DumGenp,α
hEdgesi1:r ← hRealEdgesi1:r
hEdgesir+1:r+d ← hDummyEdgesi1:d
hEdges.isReali1:r ← h1i
hEdges.isRealir+1:r+d ← h0i
Gather(hEdgesi)
hEdgesi ← FShuﬄe(hEdgesi)
for each hedgei ∈ hEdgesi
edge.v ← Open(hedge.vi)
for hvertexi ∈ hVerticesi
if edge.v == vertex.x
hvertex.xDatai ← copy(hedge.uDatai)
Apply(hVerticesi)
for hvertexi ∈ hVerticesi
hvertex.xDatai ← Ffunc(hvertex.xDatai)
Scatter(hEdgesi)
hEdgesi ← FShuﬄe(hEdgesi)
for each hedgei ∈ hEdgesi
edge.u ← Open(hedge.ui)
for hvertexi ∈ hVerticesi
if edge.u == vertex.x
hedge.uDatai ← copy(hvertex.xDatai)
Figure 4:
A protocol for two parties to compute a single iteration of the
GAS model operation on secret-shared data. This protocol realizes the ideal
functionality described in Figure 3.
During the initialization phase, the DumGenp,α functionality is used to gen-
erate secret-shares of the dummy edges. These are placed alongside the real
edges, and are then repeatedly shuﬄed in with the real edges during the itera-
tive phases. We describe DumGenp,α in detail later in this section. Every call
to FShuﬄe uses a new random permutation. (Since the dummy ﬂags are now
included inside the edge structure, we no longer need to specify that they are
shuﬄed using the same permutation as the data elements.)
Both the Gather and Scatter phases begin with calls to FShuﬄe, which takes
17
secret shares of the edge data from each party, and outputs fresh shares of the
randomly permuted data. In practice we implement this using two sequential,
generic secure computations of the Waksman permutation network [1], where
each party randomly chooses one of the two permutations. Then, the parties
iterate through the shuﬄed edge set, opening one side of each edge to reveal
the neighboring vertex. Opening these vertices in the clear is where we leak
information, and gain in eﬃciency. As we mentioned previously, this reveals
a noisy histogram of the node degrees. In doing so, the parties can fetch the
appropriate vertex from memory, without performing expensive oblivious sort
operations, as in GraphSC, and without resorting to ORAM. After fetching the
appropriate node, the secret shared data is copied to/from the adjacent edge.
During Apply, the parties make a call to an ideal functionality, Ffunc. This
functionality takes secret shares of all vertices, reconstructs the data from the
shares, applies the speciﬁed function to the real data at each vertex (while ignor-
ing data from dummy edges), and returns fresh secret shares of the aggregated
vertex data. In our implementation, we realize this ideal functionality using gar-
bled circuits. We don’t focus on the details here, as they have been described
elsewhere (e.g. [28, 29]).
DumGenp,α in detail: The ideal functionality for DumGenp,α appears in Figure
2 The role of DumGenp,α is to generate the dummy elements that create a
“noisy” degree proﬁle, bD. Starting with in-degree proﬁle D = Din(G), for each
i ∈ V , bD(i) = D(i) + γi, where each γi is drawn independently from a shifted
geometric distribution, parameterized by a “stopping” probability p, and “shift”
of α: we denote the distribution by Dp,α, and deﬁne it more precisely below.
The shift ensures that negative values are negligible likely to occur. This is
necessary because the noisy set determines our access pattern to memory, and
we cannot accommodate a negative number of accesses (or, more accurately, we
do not want to omit any accesses needed for the real data). More speciﬁcally,
we will deﬁne below a “shift function” α : R×R → N that maps every (, δ) pair
to a natural number. (When  and δ are ﬁxed, we will simply use α to denote
α(, δ).)
The functionality iterates through each vertex identiﬁer i ∈ V , sampling
a random number γi ← Dp,α, and creating γi edges of the form (⊥, i). The
remainder of the array contains “blank” edges, (⊥,⊥), which can be tossed away
as they are discovered later in the protocol, after the dummy edges have all been
shuﬄed 6 DumGenp,α returns secret shares of the dummy edges, hDummyEdgesi.
The only diﬀerence between the functionality described in the middle column,
and the one in the left portion of the ﬁgure (which was used in Section 3),
is that our “types” are now node identiﬁers, and they are stored within edge
structures. However, the reader should note that only the right node in each
edge is assigned a dummy value, while the left nodes all remain ⊥. This design
choice is for eﬃciency, and comes at the cost of leaking the exact histogram
6Revealing these blank edges before shuﬄing would reveal how many dummy edges there
are of the form (∗, i), which would break privacy. After all the edges are shuﬄed, revealing
the number of blank edges only reveals the total number of dummy edges, which is ﬁne.
18
deﬁned by the out-degrees of the graph nodes when executing Open(Edgesi.u)
in the Scatter operation. As an example of how this impacts privacy, when
computing gradient descent for matrix factorization, this reveals the number of
reviews written by each user, while ensuring that the number of reviews received
by each item remains diﬀerentially private. This hides whether any given user
reviewed any speciﬁc item, which suﬃces for achieving security with known
input sizes, as deﬁned in Deﬁnition 5. This is the protocol that we use in our
implementation, but we brieﬂy discuss what is needed to achieve Deﬁnition 6
below.
In some computations, the graph is known to be bipartite, with all edges
starting in the left vertex set and ending in the right vertex set (again, recom-
mendation systems are a natural example). In this case, since it is known that
all nodes in the left vertex set have in-degree 0, we do not need to add dummy
edges containing these nodes. This cuts down on the number of dummies re-
quired, and we take advantage of this when implementing matrix factorization.
Implementing DumGenp,α: Intuitively, we sample γi by ﬂipping a biased coin
p until it comes up heads. We ﬂip one more unbiased coin to determine the sign
of the noise, and then add the result to α. We will determine p based on  and
δ. Formally, γi is sampled as follows:
Pr[γi = α] = p
2
∀k ∈ N, k 6= 0 : Pr[γi = α + k] = 1
2(1 − p
2)p(1 − p)|k|−1.
As just previously described, we view p as the stopping probability. However, in
the ﬁrst coin ﬂip, we stop with probability p/2. We note that this is a slight mod-
iﬁcation to the normalized 2-sided geometric distribution, which would typically
2−p p(1− p)|k|. The advantage of the distribution
be written as Pr[γi = α+ k] = 1
as it is written above is that it is very easy to sample in a garbled circuit, so long
as p is an inverse power of 2; normalizing by 1
2−p introduces problems of ﬁnite
precision and greatly complicates the sampling circuit. We note that Dwork et
al. [12] suggest using the geometric distribution with p = 2‘, precisely because
it is easy to sample in a garbled circuit. However, they describe a 1-sided geo-
metric distribution, which is not immediately useful for preserving diﬀerential
privacy, and did not seem to consider that, after normalizing, the 2-sided dis-
tribution cannot be sampled as cleanly. A security analysis of our mechanism,
including concrete settings of the parameters, appears in Section 4.1.
∃i ∈ V s.t. bD(i) < 0, which leaves us with a bad representation of a multiset.
We note that with some probability that is dependent on the choice of α,
We therefore modify the deﬁnition of F to output ∅ whenever this occurs, and
we always choose α so that this occurs with probability bound by δ. In our
implementation, we set δ = 2−40.
To securely sample Dp,α, each party inputs a random string, and we let
the XOR of these strings deﬁne the random tape for ﬂipping the biased coins.
19
If the ﬁrst ‘ bits of the random tape are 1, the ﬁrst coin is set to heads, and
otherwise it set to tails: this is computed with a single ‘-input AND gate. We
iterate through the random tape, ‘ bits at a time, determining the value of each
coin, and setting the dummy elements appropriately. We use one bit from the
random tape to determine the sign of our coin ﬂips, and we add α dummies
to the result. Recall that the output length is ﬁxed, regardless of this random
tape, so after we set the appropriate number of dummy items based on our coin
ﬂips, the remaining output values are set to ⊥.
The cost of this implementation of DumGenp,α is O(V ), though this hides
a dependence on  and δ: an exact accounting for various values can be found
in Section 6. This cost is small relative to the cost of the oblivious shuﬄe,
but we did ﬁrst consider a much simpler protocol for DumGenp,α that is worth
describing. Instead of performing a coin ﬂip inside a secure computation, by
choosing a diﬀerent distribution, we can implement DumGenp,α without any
interaction at all! To do this, we have each party choose d random values from
{1, . . . ,|V |}, and view them as additive shares (modulo |V |) of each dummy
item. Note that this distribution is already one-sided, so we do not need to
worry about α, and it already has ﬁxed length output, so we do not need to
worry about padding the dummy array with ⊥ values. Intuitively, this can be
viewed as |V | correlated samples from the binomial distribution, where the bias
of the coin is 1/|V |. Unfortunately, the binomial distribution performs far worse
than the geometric distribution, and in concrete terms, for the same values of
 and δ, this protocol resulted in 250X more dummy items. The savings from
avoiding the secure computation of DumGenp,α were easily washed away by the
cost of shuﬄing so many additional items.
4.1 Proof of security
We begin by describing the leakage function L(G). Intuitively, we leak a noisy
degree proﬁle. As we mentioned previously, we analyze the simpler DumGenp,α
algorithm, and prove that the mechanism provides diﬀerential privacy for graphs
that have neighboring in-degree proﬁles. Then, we proceed afterwards to show
that this leakage function suﬃces for simulating the protocol, achieving security
in the joint-collection model, corresponding to Deﬁnition 5.
(Extending the
proof to meet Deﬁnition 6 is not much harder to do: we would use the DumGenp,α
algorithm deﬁned for the disjoint collection model, and prove that diﬀerential
privacy holds for graphs that have neighboring full-degree proﬁles.)
We remind the reader that we use the following distribution, Dp,α for sam-
pling noise:
Pr[γi = α] = p
2
∀k ∈ N, k 6= 0 : Pr[γi = α + k] = 1
2(1 − p
We deﬁne a randomized algorithm, F,δ : D → bD, whose input and output
are multi-sets over V : ∀i ∈ {1, . . . ,|V |}, bD(i) = D(i) + γi, where γi ← Dp,α.
2)p(1 − p)|k|−1.
20
Deﬁnition 7 The leakage function is
L(G) = (F,δ(Din(G)), DoutG) where Din(G) denotes the in-degree proﬁle of graph
G, and Dout(G) denotes the out-degree proﬁle.
Theorem 1 The randomized algorithm L is (, δ)-approximate diﬀerentially
private, as deﬁned in Deﬁnition 4.
We note that Dout(G) can be modeled as auxiliary information about Din(G),
so the proof that L preserves diﬀerential privacy follows from the fact that the
algorithm F,δ is diﬀerentially private for graphs with neighboring in-degree
proﬁles.
It is well known that similar noise mechanisms preserve diﬀerential
privacy, but, for completeness, we prove it below for our modiﬁed distribution,