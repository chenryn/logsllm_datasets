0.9459
0.9484
0.9301
0.9423
0.9489
0.7566
0.616
0.7474
0.5354
0.578
0.7626
0.9388
0.9246
0.9017
0.9052
0.9181
0.9301
0.5955
0.8365
0.6979
0.5842
-
-
0.9281
0.9529
0.9294
0.9292
-
-
Table 3: Performance of the NID models trained using fine-tuning approach and our approach on the source dataset. We
compare our approach with the fine-tuning transfer learning approach.
by 9.83% and 6.93% in terms of F-score. We obtain similar results
from the experiments with other attack categories reconnaissance
and shellcode, where our approach outperforms the other two
comparison approaches in terms of classification accuracy and f-
scores, for less than 500-1000 samples.
The low accuracy of the other approaches when trained with
a small amount of data samples is a result of extreme over-fitting
often seen when DL models are trained with very little training data.
Small amount of training data makes DL classification models really
good at classifying the training data but cannot generalize well on
unseen data. Our adversarial DA approach essentially uses the
source dataset samples to augment the target dataset and projects
them into a common latent subspace, thus, alleviating the problem
of over-fitting due to very small amounts of training data.
Performance on the source dataset: We also test the accuracy
of the resulting classification models from the transfer learning
using fine-tuning approach and our adversarial DA approach on
the source dataset. This would represent the models’ performance
Session 3: Network Security ASIA CCS '20, October 5–9, 2020, Taipei, Taiwan135y
c
a
r
u
c
c
A
0.9
0.8
0.7
Base case
Fine tuning
Our approach
Accuracy and F-score on UNSW-NB15 dataset
e
r
o
c
s
-
F
0.9
0.8
0.7
0.6
Base case
Fine tuning
Our approach
20000 10000 5000
1000
500
200
100
20000 10000 5000
1000
500
200
100
Number of target samples
Number of target samples
Figure 7: Accuracy and F-score results on the target dataset for the scenario where source and target datasets have different
feature spaces comparing three different approaches: base model trained with only the target dataset samples; model trained
on the source dataset and fine-tuned on the target dataset; model trained using our adversarial DA approach.
on the old source dataset with 8 attack categories. We do not include
the base case where the model is just trained on the target dataset as
it performs really badly in classifying the source attack categories.
We report the accuracy results for the classification models on
the source dataset in Table 3. We observe that our adversarial DA
approach maintains an accuracy rate of 91-94% on the source data,
whereas the model created using the fine-tuning approach is highly
unsteady. This demonstrates that our approach performs well not
only with respect to the target dataset but is also highly accurate
with respect to source datasets allowing it to detect the new attacks
seen in the target dataset as well as the old attacks seen in the
source dataset.
Combining the source and target datasets: One naive approach
to train a NID model that can identify attacks contained in both
the source dataset and the target dataset would be to just combine
the two datasets and use the combined dataset for training. To
assess such a naive approach, we have carried out additional ex-
periments. The results from such experiments show that, although
the NID models trained using this approach achieve high accuracy
and f-score on the source dataset, they perform worse than our
adversarial DA approach when the target dataset has very low num-
bers of training samples. When trained with the combination of the
source dataset and the target dataset with 100 samples for exploits,
reconnaissance and shellcode, the adversarial DA approach outper-
forms this naive approach by 6.03%, 8.26% and 3.45% respectively,
in terms of accuracy on the target dataset.
6.2 Different feature space
This category represents the scenario where the target dataset has
a different feature space than the source dataset.
Datasets: For this evaluation, we use NSL-KDD [46] as the source
dataset and UNSW-NB15 [34] as the target dataset. We remove the
specific categories of attacks from the datasets and model this as a
binary classification problem, i.e. predicting whether the current
record belongs to the attack or benign category. In this evaluation,
the source NSL-KDD dataset has 62,986 labeled training samples and
we vary the target UNSW-NB15 dataset samples used for training.
We perform experiments for 100, 200, 500, 1000, 5000, 10000 and
20000 labeled training samples from the target dataset.
Experiments: Since the fine-tuning approach only works if the
source and the target datasets have the same feature space, we
apply the pre-processing techniques mentioned in Section 4.1 to
both datasets to transform them into datasets with 30 features each
before using the fine-tuning approach. We use our adversarial DA
approach by training the GAN described in Section 4.2 using the
NSL-KDD (source) and the UNSW-NB15 (target) datasets and then
use the generator of the GAN as a classification model for testing.
Similar to the fine-tuning case, our pre-processing techniques trans-
form the source and datasets into datasets having 30 features each
before using adversarial DA.
Results: Fig. 7 presents the results for this scenario. From our
experiments, we observe that our adversarial DA approach outper-
forms the base case and the fine-tuning approach for 100 target
dataset samples by 17.9% and 5.78% respectively in terms of accu-
racy and by 20.94% and 10.26% in terms of F-score. The fine-tuning
approach improves the performance over the base case but still
performs worse than our approach. This is also a result of extreme
over-fitting often seen when DL models are trained with very little
training data. These results are similar to the results for experiments
with source and target datasets with same features allowing us to
Session 3: Network Security ASIA CCS '20, October 5–9, 2020, Taipei, Taiwan136conclude that our adversarial DA approach performs equally well
when the source and target datasets have different feature spaces.
Similar to the case where source and target datasets have similar
feature spaces, the NID model trained our adversarial DA approach
outperforms the base case and fine tuning case when tested on the
source dataset. However, in most of the scenarios where the source
dataset has a different feature space than the target dataset, we
might not be interested in the efficacy of the trained NID model in
identifying the attacks contained in the source dataset. So we omit
the results from the paper.
7 CONCLUSION AND FUTURE WORK
In this paper, we propose an adversarial DA approach to train DL
classification models for NID with very small amount of labeled
training data. This allows organizations to train NID models to iden-
tify new families of attacks on their networks by leveraging existing
datasets, for example publicly available ones or datasets used in the
past, and just capturing a small amount of labeled data for the new
attack families. Our experiments show that the NID models trained
using our adversarial DA approach outperform other approaches,
like transfer learning using fine-tuning, both when the source and
the target dataset have similar features (homogeneous DA) and
when the two datasets have different features (heterogeneous DA).
As future work, we plan to consider the cases of semi-supervised
DA (some labeled samples and a lot of unlabeled samples in the
target dataset) and unsupervised DA (only unlabeled samples in
the target dataset). We also plan to evaluate more sophisticated
GAN architectures like Wasserstein GANs [3]. Furthermore, in
this paper we model the NID problem as a binary classification
problem, i.e. identify whether the current data sample is an attack
sample or a benign sample. However, it would be interesting to
explore the efficacy of adversarial DA approaches for multi-class
classification i.e. identifying the specific categories of attack for the
target datasets. One can also evaluate the efficacy of the adversarial
DA approach by using target datasets containing labeled data only
for specific networks like IoT networks or mobile networks. In
addition to this, adversarial DA can also be evaluated using more
than one source domains. This might result in a more generalized
model able to identify attacks for multiple network types. Also,
our adversarial DA approach requires the source dataset as well
as the target dataset for training NID models. However, it may
be difficult to meet such requirement when the organization that
collected the source dataset is different from the target organization
and the former would like to keep its source dataset private while
at the same time helping the target organization to enhance its
NID functions. Such a scenario can be considered an example of
federated learning. To address such issues, privacy preserving DA
techniques need to be developed so that the source dataset can be
used for DA, but without having to disclose the source dataset.
8 ACKNOWLEDGEMENTS
This research was sponsored by the U.S. Army Research Labora-
tory and the U.K. Ministry of Defence under Agreement Number
W911NF-16-3-0001. The views and conclusions contained in this
document are those of the authors and should not be interpreted
as representing the official policies, either expressed or implied,
of the U.S. Army Research Laboratory, the U.S. Government, the
U.K. Ministry of Defence or the U.K. Government. The U.S. and U.K.
Governments are authorized to reproduce and distribute reprints
for Government purposes notwithstanding any copyright notation
hereon.
REFERENCES
[1] [n.d.]. NSL-KDD dataset. https://www.unb.ca/cic/datasets/nsl.html.
[2] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al.
2016. TensorFlow: A System for Large-Scale Machine Learning.. In OSDI, Vol. 16.
265–283.
[3] Martin Arjovsky, Soumith Chintala, and Léon Bottou. 2017. Wasserstein Gen-
erative Adversarial Networks. In International Conference on Machine Learning.
214–223.
[4] Konstantinos Bousmalis, Alex Irpan, Paul Wohlhart, Yunfei Bai, Matthew Kelcey,
Mrinal Kalakrishnan, Laura Downs, Julian Ibarz, Peter Pastor, Kurt Konolige,
et al. 2018. Using simulation and domain adaptation to improve efficiency of
deep robotic grasping. In 2018 IEEE International Conference on Robotics and
Automation (ICRA). IEEE, 4243–4250.
[5] Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan,
and Dumitru Erhan. 2016. Domain separation networks. In Advances in neural
information processing systems. 343–351.
[6] James Cannady. 1998. Artificial neural networks for misuse detection. In National
information systems security conference, Vol. 26. Baltimore.
[7] F. Chollet. 2015. Keras. GitHub (2015). https://github.com/fchollet/keras.
[8] Wenyuan Dai, Gui-Rong Xue, Qiang Yang, and Yong Yu. 2007. Transferring naive
bayes classifiers for text classification. In AAAI, Vol. 7. 540–545.
[9] Jun Deng, Zixing Zhang, Erik Marchi, and Bjorn Schuller. 2013.
Sparse
autoencoder-based feature transfer learning for speech emotion recognition.
In 2013 Humaine Association Conference on Affective Computing and Intelligent
Interaction. IEEE, 511–516.
[10] Chuong B Do and Andrew Y Ng. 2006. Transfer learning for text classification.
In Advances in Neural Information Processing Systems. 299–306.
[11] Maayan Frid-Adar, Idit Diamant, Eyal Klang, Michal Amitai, Jacob Goldberger,
and Hayit Greenspan. 2018. GAN-based synthetic medical image augmentation
for increased CNN performance in liver lesion classification. Neurocomputing
321 (2018), 321–331.
[12] Maayan Frid-Adar, Eyal Klang, Michal Amitai, Jacob Goldberger, and Hayit
Greenspan. 2018. Synthetic data augmentation using GAN for improved liver
lesion classification. In 2018 IEEE 15th international symposium on biomedical
imaging (ISBI 2018). IEEE, 289–293.
[13] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo
Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. 2016.
Domain-adversarial training of neural networks. The Journal of Machine Learning
Research 17, 1 (2016), 2096–2030.
[14] Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, and David Balduzzi.
2015. Domain generalization for object recognition with multi-task autoencoders.
In Proceedings of the IEEE international conference on computer vision. 2551–2559.
[15] Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, David Balduzzi, and Wen
Li. 2016. Deep reconstruction-classification networks for unsupervised domain
adaptation. In European Conference on Computer Vision. Springer, 597–613.
[16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial
nets. In Advances in neural information processing systems. 2672–2680.
[17] Arthur Gretton, Karsten Borgwardt, Malte Rasch, Bernhard Schölkopf, and Alex J
Smola. 2007. A kernel method for the two-sample-problem. In Advances in neural
information processing systems. 513–520.
[18] Elike Hodo, Xavier Bellekens, Andrew Hamilton, Pierre-Louis Dubouilh, Ephraim
Iorkyase, Christos Tachtatzis, and Robert Atkinson. 2016. Threat analysis of IoT
networks using artificial neural network intrusion detection system. In Networks,
Computers and Communications (ISNCC), 2016 International Symposium on. IEEE,
1–6.
[19] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko,
Alexei A Efros, and Trevor Darrell. 2017. Cycada: Cycle-consistent adversarial
domain adaptation. arXiv preprint arXiv:1711.03213 (2017).
[20] Jui-Ting Huang, Jinyu Li, Dong Yu, Li Deng, and Yifan Gong. 2013. Cross-language
knowledge transfer using multilingual deep neural network with shared hidden
layers. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International
Conference on. IEEE, 7304–7308.
[21] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. 2017. Image-to-
image translation with conditional adversarial networks. In Proceedings of the
IEEE conference on computer vision and pattern recognition. 1125–1134.
[22] IXIA. 2019. IXIA PerfectStorm. https://www.ixiacom.com/products/perfectstorm
Session 3: Network Security ASIA CCS '20, October 5–9, 2020, Taipei, Taiwan137[23] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. 2018. Progres-
sive Growing of GANs for Improved Quality, Stability, and Variation. In 6th
International Conference on Learning Representations, ICLR 2018, Vancouver, BC,
Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net.
https://openreview.net/forum?id=Hk99zCeAb
[24] Georgios Kathareios, Andreea Anghel, Akos Mate, Rolf Clauberg, and Mitch
Gusat. 2017. Catch It If You Can: Real-Time Network Anomaly Detection with
Low False Alarm Rates. In Machine Learning and Applications (ICMLA), 2017 16th
IEEE International Conference on. IEEE, 924–929.
[25] Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Optimiza-
tion. CoRR abs/1412.6980 (2014). arXiv:1412.6980 http://arxiv.org/abs/1412.6980
[26] Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning multiple layers of features
from tiny images. Technical Report. Citeseer.
[27] MIT Lincoln Labs. 1999. KDD Cup 1999 Data. http://kdd.ics.uci.edu/databases/
kddcup99/task.html
com/exdb/mnist/ (1998).
[28] Yann LeCun. 1998. The MNIST database of handwritten digits. http://yann. lecun.
[29] Ming-Yu Liu and Oncel Tuzel. 2016. Coupled generative adversarial networks. In
Advances in neural information processing systems. 469–477.
[30] Matthew V Mahoney and Philip K Chan. 2003. An analysis of the 1999
DARPA/Lincoln Laboratory evaluation data for network anomaly detection.
In International Workshop on Recent Advances in Intrusion Detection. Springer,
220–237.
[31] Giovanni Mariani, Florian Scheidegger, Roxana Istrate, Costas Bekas, and Cris-
tiano Malossi. 2018. Bagan: Data augmentation with balancing GAN. arXiv
preprint arXiv:1803.09655 (2018).
[32] John McHugh. 2000. Testing intrusion detection systems: a critique of the 1998
and 1999 darpa intrusion detection system evaluations as performed by lincoln
laboratory. ACM Transactions on Information and System Security (TISSEC) 3, 4
(2000), 262–294.
[33] Zhong Meng, Jinyu Li, Zhuo Chen, Yang Zhao, Vadim Mazalov, Yifan Gang, and
Biing-Hwang Juang. 2018. Speaker-invariant training via adversarial learning.
In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP). IEEE, 5969–5973.
[34] Nour Moustafa and Jill Slay. 2015. UNSW-NB15: a comprehensive data set for
network intrusion detection systems (UNSW-NB15 network data set). In Military
Communications and Information Systems Conference (MilCIS), 2015. IEEE, 1–6.
[35] Maxime Oquab, Leon Bottou, Ivan Laptev, and Josef Sivic. 2014. Learning and
transferring mid-level image representations using convolutional neural net-
works. In Proceedings of the IEEE conference on computer vision and pattern recog-
nition. 1717–1724.
[36] Chandrika Palagiri. 2002. Network-based intrusion detection using neural net-
works. Department of Computer Science Rensselaer Polytechnic Institute Troy, New
York (2002), 12180–3590.
[37] Sinno Jialin Pan and Qiang Yang. 2010. A survey on transfer learning. IEEE
Transactions on knowledge and data engineering 22, 10 (2010), 1345–1359.
[38] Santiago Pascual, Antonio Bonafonte, and Joan Serrà. 2017. SEGAN: Speech
Enhancement Generative Adversarial Network. Proc. Interspeech 2017 (2017),
3642–3646.
[39] Karl Pearson. 1901. LIII. On lines and planes of closest fit to systems of points in
space. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of
Science 2, 11 (1901), 559–572.
[40] Victor Powell. 2015. Principal Component Analysis explained visually. http:
//setosa.io/ev/principal-component-analysis/
[41] Rajat Raina, Alexis Battle, Honglak Lee, Benjamin Packer, and Andrew Y Ng.
2007. Self-taught learning: transfer learning from unlabeled data. In Proceedings
of the 24th international conference on Machine learning. ACM, 759–766.
[42] Markus Ring, Sarah Wunderlich, Deniz Scheuring, Dieter Landes, and Andreas
Hotho. 2019. A survey of network-based intrusion detection data sets. Computers
& Security (2019).