tions by low-level instructions that are closer to the assembly.
For example, the instruction 𝑥 := 𝑥 + 𝑦 will be transformed into
(. . . , CF, . . . , 𝑥) := ADD(𝑥, 𝑦), i.e. the + operator is replaced by a
low level instruction ADD that will perform the addition (but also
computes extra data like the carry flag). The assignment of the flags
will create extra • leakage that has to be justified.
Similarly the instruction if 𝑥 < 𝑦 then 𝑐1 else 𝑐2 (where < is the
unsigned comparison) is transformed into the sequence:
. . . , CF, . . . := CMP(𝑥, 𝑦);
if CF then 𝑐1 else 𝑐2
From the point of view of the leakage transformation, this means
that the leakage generated by the expression 𝑥 < 𝑦 needs to be used
Session 2B: Formal Analysis and Verification CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea468to create the leakage for the CMP instruction. Therefore this pass
relies on leakage transformers that can, on one hand split leakages
into smaller parts and, on the other hand, construct fresh leakages
from these parts and from the • constant.
This pass also uses the rev transformer which reverses the order
of a sequence of leakage. It is used for some instructions that require
their arguments to be evaluated in a specific order.
fn foo() −→ reg u64 {
stack u64[2] t;
reg u64 p r;
t[0] = 0;
t[1] = 1;
p = 0;
r = t[(int)(p + 1)];
return r; }
fn foo() −→ u64 {
stack: 16
[RSP + 0] = MOV(0);
[RSP + 8] = MOV(1);
RAX = MOV(0);
RAX = MOV([RSP + (8 * (RAX + 1) + 0)]);
return RAX; }
5.5 Focus on stack allocation
Stack allocation allocates some variables into the stack memory and
replaces the corresponding accesses (read and write) by memory
operations (load and store). It may thus create new leakage.
Figure 9: Example program: source and after stack-allocation
:=
[0]
( ,
)
•
:=
[1]
•
( ,
)
•
{ ; }
:=
•
•
•
:=
( ,
)
( ,
)
[1]
•
•
•
Figure 10: Structured leakage for source code
{ ; }
:=
:=
:=
:=
( ,
)
•
( ,
)
•
•
•
•
( ,
)
•
∗𝑣sp
•
∗𝑣sp + 8
( ,
)
∗𝑣sp + 8
( ,
)
•
( ,
)
•
•
•
Figure 11: Structured leakage for compiled code
register allocation: variables p and r have been allocated to the rax
register and mov instructions implement the assignments; for the
sake of readability, a few type annotations have been hidden on
the figure. The memory operations that are introduced by this pass
compute addresses relative to the stack pointer, held in register rsp.
Each index expression is transformed into a more complex address
computation involving the actual index but also the stack pointer,
constant offsets and scaling multiplications. The leakage corre-
sponding to the evaluation of such an expression is transformed
accordingly. The descriptions of these transformations, therefore,
use the transformers producing address leakage that have been
introduced above. They also use the vector of leakage transformers
(𝜏𝑒; . . . ; 𝜏𝑒) that, applied to a single leakage, produces a vector of
leakages, each component being the result of the application of the
corresponding transformer.
The stack-allocation transformation applied to the example pro-
gram yields the following leakage transformer5: {((•, 𝐼(𝑥 ↦→ sp +
cst 𝑥×cst 8))) := (id); ((•, 𝐼(𝑥 ↦→ sp+cst 𝑥×cst 8))) := (id); (id) :=
(id); id := ((id, id) ◦ ((•; id);•), 𝐼(𝑥 ↦→ sp + cst 𝑥 × cst 8))}.
By instrumented correctness of this compilation pass (Theo-
rem 5.1), the application of this transformer to the leakage shown
above yields the leakage corresponding to the execution of the
transformed program. Said leakage is as follows (see Figure 11):
{(•,∗𝑣sp) := •; (•,∗(𝑣sp+8)) := •;• := •;• := (((•, (•,•)),•),∗(𝑣sp+
5This leakage transformer is represented as a tree in appendix A.
Given a scalar variable 𝑥 that is allocated at constant offset 𝑜𝑥
in the stack, a read from this variable will be compiled into the
memory load [sp+𝑜𝑥] where sp is the register containing the value
of the stack pointer. At the source level, the leakage of 𝑥 is •, and it
becomes ∗(𝑣sp + 𝑜𝑥) at the target level4 (where 𝑣sp is the value of
the stack pointer). So the target leakage depends on the constant
𝑜𝑥 and also on the dynamic value of sp.
The case of an array variable 𝑎 allocated at constant offset 𝑜𝑎
is similar. The expression 𝑎[𝑒] is compiled into [sp + 𝑜𝑎 + 𝑛 × 𝑒]
where 𝑛 is the size of an array element. At source level 𝑎[𝑒] leak
[𝑣𝑒], where 𝑣𝑒 is the value of 𝑒, and at the target level the leakage
is ∗(𝑣sp + 𝑜𝑎 + 𝑛 × 𝑣𝑒). 𝑜𝑎 and 𝑛 are statically known values, which
are provided to the leakage transformer. So in this transformation,
the target leakage will further depend on the value 𝑣𝑒 that can be
recovered from the source leakage and on the dynamic value of sp.
These two examples show that in contrast to the other passes,
the target leakage cannot be computed using the source leakage
only. The initial value of the stack pointer is needed. This will have
some consequence for the preservation of constant-time that is ex-
plained in section 6.1: the interpretation of the leakage transformer
is parameterized by the value of the stack pointer, which must be
considered as a public input.
To capture these transformations, the syntax of leakage trans-
formers includes transformers that can construct new leakages, and
their semantics depends on the value of the stack pointer. Fresh
memory access leakages are introduced by the transformers 𝐶(𝜏𝑠)
and 𝐼(𝑥 ↦→ 𝜏𝑠). They are made of an expression 𝜏𝑠 that denotes
the accessed address. In the case of 𝐼(𝑥 ↦→ ·), the expression has a
free variable (implemented using higher-order abstract syntax) that
denotes the actual value of the index. The formal definition of the
semantics of these transformers is given in Figure 8 and relies on an
auxiliary function ⟦𝜏𝑠⟧𝑣sp
𝑠 which evaluates the (closed) expression
𝜏𝑠 given the actual value 𝑣sp of the stack pointer.
To conclude this section, a small example program (shown on
the left of Figure 9) illustrates the leakage transformers introduced
in this pass. It stores two literal values in an array, writes a lit-
eral into a variable, and finally reads from the array. The leakage
corresponding to the execution of this program is therefore (see
Figure 10): { (•, [0]) := •; (•, [1]) := •; • := •; • := ((•, •), [1]) }.
As requested by the programmer, the array is allocated into the
stack memory, whereas the other local variables stay in registers.
The output of this compilation pass is shown on the right of Fig-
ure 9. Note that this pass happens after instruction selection and
4The target leakage is in fact (•,∗(𝑣sp + 𝑜𝑥)) to include the evaluation of the offset.
Session 2B: Formal Analysis and Verification CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea4698))}, where 𝑣sp is the value of the stack pointer. Note how the evalu-
ation of the last address takes more (silent) steps than the evaluation
of the corresponding array index and how the index leakages have
been replaced by address leakages.
6 APPLICATIONS
This section describes how to leverage correct leakage transform-
ers: we first, prove that the Jasmin compiler always preserves the
constant-time property then show an application to cost analysis.
6.1 Constant-time preservation
The cryptographic constant-time (CCT) property is an effective
counter-measure against practical side-channel attacks. It has been
shown in practice that compilers do not always preserve this prop-
erty and introduce vulnerabilities in otherwise secure programs. We
formally prove that the Jasmin compiler is not subject to this issue:
even though it transforms the control-flow and introduces memory
accesses, it will neither remove counter-measures nor introduce
sensitive information flows.
We have already described informally that preservation of CCT
is a corollary of instrumented correctness (Theorem 5.1). However,
that informal description elided a few practical issues that must be
taken into account when implementing our methodology in the
Jasmin compiler:
• some states are unsafe, i.e., the semantics of a program is a
• source and target languages (and states) are different;
• the interpretation of leakage transformers is parameterized by
parts of the initial state (namely the value of the stack pointer);
• compiler correctness has side conditions (namely, there should
be enough free memory in the initial target state to allocate
the local variables).
partial function;
We address these issues by using a definition of CCT that is mean-
ingful even in the presence of unsafety and by suitably lifting the
indistinguishability relation to target states.
As described in Section 2.3, the CCT property is parameterized
by an indistinguishability relation on initial states and defined as
follows: the leakage is the same for all indistinguishable states. We
strengthen this definition to imply that indistinguishable states are
safe to execute in constant-time programs.
Definition 6.1 (Cryptographic constant-time). A program 𝑝 is cryp-
tographic constant-time w.r.t. the indistinguishability relation · ∼ ·
when the following holds:
∀𝑠1 𝑠2, 𝑠1 ∼ 𝑠2 =⇒ ∃𝑠′
1 𝑠′
2 ℓ, 𝑝 : 𝑠1 ⇓ℓ 𝑠′
1 ∧ 𝑝 : 𝑠2 ⇓ℓ 𝑠′
2.
Moreover, in order to state and prove preservation of the CCT
property, we must define the indistinguishability relation between
target states. In the case of the Jasmin compiler, an initial source
state is made of a memory 𝑚 and a list of values (cid:174)𝑣 (the arguments of
the main function), whereas a target state is made of a memory 𝑚
and a register bank 𝑟. Fortunately, the source state can be computed
from the target state: the memory is kept, and the values of the
arguments are read in the appropriate registers. This computation
is consistent with the compiler correctness statement. Therefore we
can relate target states by relating the corresponding source states.
The target leakage usually depends on the initial value of the stack
pointer: we must thus require that this value is public, i.e., equal
in indistinguishable states. Finally, to ensure that indistinguishable
states are safe, we have to ensure that the side condition to the
compiler correctness theorem is discharged. In a nutshell, this yields
the following definition.
Definition 6.2 (Indistinguishability of target states). Given an
equivalence relation · ∼ · between source states, its lifting to target
states ·¯∼· is defined as follows. We say that two target states (𝑚1, 𝑟1)
and (𝑚2, 𝑟2) are indistinguishable, and note (𝑚1, 𝑟1) ¯∼(𝑚2, 𝑟2), when
all the following conditions hold:
• corresponding initial source states are indistinguishable, noted:
(𝑚1, (cid:174)𝑣1) ∼ (𝑚2, (cid:174)𝑣2) (where (cid:174)𝑣1, resp. (cid:174)𝑣2, denotes the program
arguments extracted from register bank 𝑟1, resp. 𝑟2);
• stack pointers agree: 𝑟1[sp] = 𝑟2[sp];
• there is enough free stack space to allocate the local variables
in both memories 𝑚1 and 𝑚2.
We can finally prove that our modified compiler always preserves
the CCT property.
Theorem 6.3 (CCT-Preservation). Given a source program 𝑝
that is CCT w.r.t. ∼, if the Jasmin compiler succeeds and produces a
target program ¯𝑝, then the target program is CCT w.r.t. ¯∼.
6.2 Cost analysis
The run-time cost of a program — computational complexity, worst-
case execution time (wcet), peak memory usage, etc. — crucially de-
pends on low-level details hence on decisions made at compile-time:
control-flow transformations and code layout, register spilling and
memory layout of local variables, instruction selection and sched-
uling... Therefore a precise static cost analysis must be carried out
near the end of the compilation pipeline (ideally on assembly code
or even at binary level). However, the estimation of the run-time
cost relies on loop bounds or other flow information (description
of infeasible paths, for instance), either inferred by static analysis
or provided by the programmer as annotations. In both cases, these
flow facts are provided at the source level: programmers are more
inclined towards annotating source code than target code, and static
analyses are much more precise and efficient when they can rely
on high-level abstractions from the source language.
This section describes how leakage transformers can reconcile
these conflicting requirements: they are a sound way to transport
source-level cost information down to the assembly level. More pre-
cisely, we first introduce a cost model as an abstraction of leakage
and show how to deduce “cost transformers” from leakage trans-
formers. Finally, we show how these cost transformers’ soundness
enables us to use the results of a source-level static analysis at the
assembly level.
6.2.1 Cost models. In order to formally reason about the cost of
program execution at either source or target level, we model it as
the number of times each instruction is executed. In other words, a
cost is a finite map from program points to natural numbers. For
unstructured intermediate languages (linear, assembly), a program
point is simply a position in the program text (i.e., a natural number);
for structured languages, we define a language of paths to describe
positions in the abstract syntax tree. Note that the usual order on
Session 2B: Formal Analysis and Verification CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea470natural numbers can be lifted pointwise to costs so that the set of
costs forms a partial order.
The cost is defined by means of a function that evaluates a leakage
trace into a cost map. Here again, the structure of the leakage is
beneficial: it enables to perform this evaluation without looking at
the program.
Definition 6.4 (Cost). Each intermediate language is equipped
with an tocost (·) function that, given a leakage, computes a cost,
i.e., a count for each program point. Given a execution 𝑝 : 𝑠 ⇓ℓ 𝑠′,
its cost is tocost (ℓ).
6.2.2 Cost transformers. Program transformations found in com-
pilers introduce, remove, or reorder instructions according to the
program being compiled: they do not make up instructions out of
the blue. Even though predicting how many times each instruction
emitted by a compilation pass will be executed at run-time is usu-
ally not possible, the execution counts for target instructions can
be related to execution counts for the corresponding source execu-
tions. More precisely, for most transformations found in the Jasmin
compiler, the target costs can be precisely described by relating
each target program point to one basic block of the source program.
This link is to be interpreted as follows: “the instruction at this
program point is executed in the target execution as many times
as that basic block is executed in the source execution”. For some
compilation passes, unfortunately, we have to relax this property
and interpret the predicted target count as an upper bound. As
discussed in Section 7.3, this is not an issue in practice6.
Definition 6.5 (Cost transformer). A cost transformer maps target
program points to source basic blocks. It, therefore, enables the
translation of a source-level cost into a target-level cost. A leakage
transformer can be seen as a cost transformer by an interpretation
function ⟦·⟧·
𝜅; such an interpretation is sound when for all (source)
leakage ℓ and matching leakage transformer 𝜏, the following holds
(where ⊑ is a partial order on costs):
(cid:16)⟦𝜏⟧ℓ(cid:17) ⊑ ⟦𝜏⟧tocost(ℓ)
𝜅
tocost
.