0.34
0.83
0.58
1.33
1.48
2.86
Stdrd
k = 5
0.18
0.55
0.48
0.36
0.8
1.57
1.48
1.89
2.03
3.55
3.22
4.86
Table 3: Mean fpr values for diﬀerent conﬁgurations.
of k. The performance appears adequate in all of the topolo-
gies, up to 23 subscribers (≈ 32 links); forwarding eﬃciency
is still above 90% in the majority of the test cases. The
result is much better than multiple unicast, where the same
links would be used multiple times by the same publication.
For example, in AS3257 the unicast forwarding eﬃciency is
only 43% for 23 subscribers.
Table 3 sheds light on the diﬀerence between fpa and fpr
algorithms. There is an interesting relation between the dis-
tribution of k and the optimization strategies: in our region
of interest, kc = 5 performs better than the variable k dis-
tribution (kd). As expected, fpr-optimization successfully
reduces the false positive rate, and outperforms the non-
optimised (d = 1) approach by 2–3 times in the scenarios
with 16 users. The gain of using fpa instead of the non-
optimised algorithm is clear, although not as signiﬁcant as
with fpr. These improvements can be also observed in the
sample results of AS6161, see Fig. 5.
Of course, as the link IDs are inserted into the zFilters,
delivery trees are only present in the packet headers, and
therefore completely independent from each other. Hence,
the number of simultaneous active trees does not aﬀect the
forwarding performance.
Stateful forwarding: In networks with scale-free prop-
erties, a large part of the traﬃc ﬂows between high-degree
hubs. We experimented with the eﬀects of installing virtual
Figure 6: Stateful dense multicast eﬃciency
links covering diﬀerent parts of the network. We built vir-
tual links from the publisher towards the core and between
the hubs, but that enhanced the performance only slightly,
as virtual links substituted only a couple of physical links.
Signiﬁcant performance enhancements can be reached if
we install virtual links rooted at (high-degree) core nodes
and covering a set of subscribers, avoiding thereby the pres-
ence of many LITs in the zFilter. The results on Fig. 6 show
that dense multicast can be supported with more than 92%-
95% forwarding eﬃciency even if we need to cover more than
50% of the total nodes in the network (cf. Table 2).
Forwarding table sizes: Assuming that each forward-
ing node maintains d distinct forwarding tables, with an
entry consisting of a LIT and the associated output port,
we can estimate the amount of memory we need for the for-
warding tables:
F Tmem = d · #Links · [size(LIT ) + size(Pout)]
(4)
Considering d = 8, 128 links (physical & virtual), 248-bit
LITs and 8 bits for the outport, the total memory required
would be 256Kbit, which easily ﬁts on-chip.
Although this memory size is already small , we can design
a more eﬃcient forwarding table by using a sparse represen-
tation to store just the positions of the bits set to 1. Thereby,
the size of each LIT entry is reduced to k·log2(LIT ) and the
total forwarding table requires only ≈ 48Kbit of memory, at
the expense of the decoding logic.
4.3 Discussion
To support larger trees than we can comfortably address
with a single zFilter, two choices can be considered. First,
we can create virtual links to maintain the ﬁll factor and
to keep the overdeliveries under control. This comes at the
201price of control traﬃc and the increase of state in forwarding
nodes. Second, we can send multiple packets:
instead of
building one large multicast tree we can build several smaller
ones, thereby keeping zFilters’ ﬁll factor reasonable. The
packets will follow the desired route with acceptable false
delivery rates, but exact copies will pass through certain
links where the delivery trees overlap. Depending on the
scenario speciﬁcs, this can result in more bandwidth waste
than in the case of a single larger tree.
So far, we have calculated the performance of zFilters for
speciﬁc sized subscriber sets. A further step is to estimate
the overall performance of the network, where the traﬃc ma-
trix is consisting of a large variety of diﬀerent subscriber sets.
Here we rely on current systems centered around dissemi-
nating information objects. First, according to RSS work-
load data collected at Cornell, the number of subscribers
for diﬀerent topics follows a Zipf distribution [24]. Second,
YouTube video popularity also shows a power-law distribu-
tion in a campus network [17]. Third, IPTV channel pop-
ularity [11] was measured to have the same characteristics
even with a faster drop in the case of unpopular channels
than the Zipf-distribution would suggest. Fourth, in typical
data centers there is a need for a large number of multicast
groups, albeit all contain only a small amount of receivers [5].
Based on these observations, assuming a long tail in the
popularity of topics, with m = 248 our results conﬁrm that
our fabric needs no forwarding state for the large majority
of topics and requires virtual links or multiple sending only
for the few most popular topics. This is a clear advantage
compared to IP multicast solutions, where even the small
groups need forwarding states in the routers. Furthermore,
as we can freely combine the stateful and stateless methods,
we can readily accommodate a number of changes in the
popular topics before needing to signal a state change in the
network, avoiding unnecessary communication overhead.
4.4 Security
The probabilistic nature of Bloom ﬁlters directly provides
the basis for most of our security features. Furthermore, as
zFilters are location speciﬁc, it is unlikely that any given
zFilter could induce any usable traﬃc if used outside of its
intended links. Without knowledge of the actual network
graph, including the active Link IDs and LITs, it is unprac-
tical trying to guess a zFilter that would reach any particular
set of nodes.
In a simple zFilter contamination attack, the attacker tries
to get a single packet to be broadcasted to all possible links
by using a BF containing a large amount of 1’s (or even only
1’s). A simple countermeasure for such attack, also observed
in [44], is to limit the ﬁll factor, e.g., to 50–70%. We have
implemented this in hardware, without causing any addi-
tional delay. As a result, a randomly generated zFilter will
match outgoing links only at the false positive rate resulting
from the maximal allowed ﬁll factor.
In a more advanced attack, combining a LIT learning at-
tack and a zFilter re-use attack, an attacker may ﬁrst at-
tempt to ﬁgure out the LITs of the links nearby it by at-
tempting to lure lots of subscribers from diﬀerent parts of
the network. The attacker learns a number of valid zFil-
ters originating at it and, using AND for the received LITs,
guesses the LITs of the next few links. This attack, however,
requires a lot of work, and there are a few direct counter-
measures. First, the number of parallel LIT’s close to the
publisher can be increased and the uplink Link IDs can be
changed more often. Second, by varying the selection of the
Bloom ﬁlter (Sec. 3.2), though not optimal, we may increase
the probability that the attacker gets a too full zFilter.
More generally, we can avoid many of the known, and
probably a number of still unknown attacks, by slowly chang-
ing the Link IDs over time. Our on-going work is focusing
on hash chains and pseudo-random sequences in this area,
meaning that with a shared secret between the individual
forwarding nodes and the topology system the control over-
head of communicating the changes could be kept at a min-
imum. The caveat would be that the zFilters being used in
the network need to be re-calculated once in a while.
Overall, no forwarding state is created if there aren’t a
fairly large number of subscribers that have explicitly in-
dicated their interest in data delivery. We thereby avoid
the typical problems of multicast routers maintaining state
of unnecessary multicast groups, e.g., an attacker joining
many low-rate multicast groups.
Finally, consider a situation where an attacker has suc-
cessfully launched a DDoS attack. Initially, the victim can
quench the packet stream by requesting the closest upstream
node to ﬁlter traﬃc according to the operation deﬁned in
Section 3.3.4. After that, the LITs on the forwarding nodes
can be changed to extinguish the attack. However, the latter
is a slower operation, requiring updates to the topology layer
and recalculation of zFilters for aﬀected active subscriptions.
Additional future work will consider how legitimate traﬃc
can exploit the multi-path capabilities of the zFilters.
5. FEASIBILITY
We now turn our attention to the overall feasibility of
our approach, focusing on the inter-networking aspects. In
particular, we consider how our forwarding fabric can be
extended to cover inter-domain forwarding. We discuss the
eﬃciency and scalability aspects for the pure pub/sub case.
For the IP-based multicast case, described in Section 2.4,
we need to use currently existing mechanisms, limiting the
breadth of the issues. We also discuss how the proposal is
(slightly) better than IP in supporting data-oriented naming
and in-network caching.
5.1 Full connectivity abstraction
As mentioned in Sect. 2, the overall architecture we rely
on is based on a recursive approach, where each layer pro-
vides a full connectivity abstraction. Hence, to implement
inter-domain forwarding, we need to attach two forwarding
headers into a packet, an intra-domain and an inter-domain
one, and replace the intra-domain header at each domain
boundary. For IP multicast, the IP header with the IP mul-
ticast address takes the place of the inter-domain header.
To provide the full mesh abstraction, a domain provides
an inter-domain Link ID (IdLId) for each of its neighboring
domains. Furthermore, the domain provides a distinct Link
ID to be added to packets that have local receivers. Hence,
in the inter-domain zFilter of an incoming packet, there is
the incoming IdLId for the link from the previous to this
domain, the outgoing IdLId s for the links from this domain
to any next domains, and if there are any local receivers, the
IdLId denoting their existence.
When we receive a packet from outside, we ﬁrst may ver-
ify that the packet is forwarded appropriately, e.g., that
the inter-domain zFilter contains the incoming IdLId. Af-
202by introducing explicit signaling that would allow certain
topics to be always kept in the cache, even when not ac-
tively used3.
An interesting open problem is to consider potential space
saving techniques, such as determining commonalities be-
tween inter-domain zFilters, perhaps allowing them to be
used as indices. If the topics sharing a single inter-domain
zFilter can be distinguished with only a few bits, it may be
possible to develop clever data structures for compressing
the topic-based forwarding tables.
5.3 Policy compliance and trafﬁc engineering
For the IP case, we expect no real changes to traﬃc engi-
neering or policies, as the forwarding fabric would be invisi-
ble outside of the domain. In the recursive pub/sub case, we
have to make sure that the inter-domain zFilters are policy-
compliant. As a starting point, each edge node can verify
that all traﬃc is either received from a paying customer or
passed to a paying customer. However, due to multicast,
there are diﬃcult cases not covered by the typical IP-based
policy compliance rules, such as traﬃc arriving from one
upstream provider and destined both to a paying customer
and another upstream provider. In general, we will eventu-
ally need a careful study of the issues identiﬁed by Faratin et
al [16]. As observed in [30], it is an open problem how the
kind of source routing we propose may change the overall
market place and policies.
Considering traﬃc engineering, sender-based control would
be easy. At this point, however, open questions include how
the transit operators may aﬀect the paths or how the re-
ceivers can express their preferences. We surmise that those
aspects have to be implemented elsewhere in the architec-
ture, as our forwarding layer can redirect traﬃc only by
redirecting links.
5.4 Naming and caching
As mentioned earlier, both data-oriented naming and in-
network caching are needed for eﬃcient pub/sub. Our stack
structure and independence of end-node addresses in zFil-
ter forwarding, make both of these functions simpler com-
pared to IP networks. Our architecture treats data as ﬁrst
class citizens. The focus is on eﬃcient data delivery instead
of connecting diﬀerent hosts for resource sharing. The de-
fault choice of multicast brings natural separation of ren-
dezvous (addressing/naming) and routing. The resulting
identiﬁer/locator split gives better support for data-oriented
naming than the current IP-based architecture, cf. e.g. [2].
Once routing is based on location-independent identiﬁers,
any kind of native naming and addressing on the infrastruc-
ture turns out to be a straightforward task.
The zFilter forwarding eases in-network caching by sup-
porting the required decoupling between publishers and sub-
scribers. Publishers can publish data in the network, inde-
pendent of the availability of subscribers. Packet caching
and further delivery from the caches is relatively simple, as
node based addressing is not needed. Caching can also be
used for other purposes, e.g., enhancing reliability. Com-
bining data-oriented naming and caching, we can turn the
traditional packet queues and the sibling recipient memories
into opportunistic indexable caches, allowing, for example,
any node to ask for recent copies of any missed or garbled
3Obviously, such a service would either need strict access
controls or an explicit fee structure.
Figure 7: Inter-domain forwarding with distributed
RVSs
ter that, we match the zFilter against all outgoing IdLIds,
simultaneously looking up the corresponding intra-domain
zFilters. The intra-domain zFilters can usually be simply
merged.
If the inter-domain zFilter indicates local recipi-
ents, more processing is needed.
We assume that the data topic identiﬁer, carried inside the
packet, or other suitable identiﬁer such as an IP multicast
address, is used to index the set of local recipients. For IP
multicast addresses, it is reasonable to expect the edge nodes
to maintain the required state. For the pub/sub case, where
the number of active topics may be huge, the subscriber
information may be divided between a set of intra-domain
rendezvous nodes (see Figure 7), providing load distribution.
Eventually, a rendezvous node looks up the intra-domain
zFilter by using the topic identiﬁer. As it takes time to pass
the packet to the right rendezvous node, and as the lookup
may take some time, the rendezvous nodes can construct
cache-like forwarding maps and distribute them to the edge
nodes.
5.2 Resource consumption
We now estimate the amount of resources needed to main-
tain topic-based forwarding tables, needed for the recursive
layering in the pub/sub case. To estimate the storage re-
quirements, we consider the number of indexable web pages
in the current Internet as a reasonable upper limit for the
number of topics subscribed within a domain. In 2005 there
was around 1010 indexable web pages [18]; today’s number is
larger and we assume it to be around 1011. Considering that
each topic name would take 40 bytes and each forwarding
header takes 32 − 34 bytes, in the order ≈ 10 T B of storage
would be needed.
Following the argumentation by Koponen et al [23] and
assuming similar dynamics, it is plausible that even a single
large multi-processor machine could handle the load. How-
ever, a multi-level lookup caching system is needed to reduce
per-packet lookup delay to a reasonable level. For example,
each edge node could cache a few million most active topics,
each rendezvous node could keep in their DRAM a few bil-
lion less active topics, and the information about rest could
be stored on a fast disk array.
If only a small fraction of
subscriptions would be active at any given point of time,
the suggested multi-level caching may make it possible to
handle the typical lookup load with just one or a few large
server PCs.
We note that the approach may be problematic for appli-
cations where the inter-packet delay is long but latency re-
quirements are strict. If needed, the problem can be solved
203# of
NetFPGAs
Average
Latency/
latency Dev. NetFPGA
Std.
0
1
2
3
16μs
19μs
21μs
24μs
1μs
2μs
2μs
2μs
N/A
3μs
3μs
3μs
Table 4: Simple latency measurement results
To get some understanding of the potential speed, we have
made some early measurements. The ﬁrst set of measure-
ments, shown in Table 4, focused on the latency of the for-
warding node with a very low load. In each case, the latency
of 10000 packets was measured, varying the number of NetF-