title:Hopper: Decentralized Speculation-aware Cluster Scheduling at Scale
author:Xiaoqi Ren and
Ganesh Ananthanarayanan and
Adam Wierman and
Minlan Yu
Hopper: Decentralized Speculation-aware Cluster
Scheduling at Scale
Xiaoqi Ren1, Ganesh Ananthanarayanan2, Adam Wierman1, Minlan Yu3
1California Institute of Technology,
2Microsoft Research,
3University of Southern California,
{xren,adamw}@caltech.edu, PI:EMAIL, PI:EMAIL
ABSTRACT
As clusters continue to grow in size and complexity,
providing scalable and predictable performance is an in-
creasingly important challenge. A crucial roadblock
to achieving predictable performance is stragglers, i.e.,
tasks that take signiﬁcantly longer than expected to
run. At this point, speculative execution has been widely
adopted to mitigate the impact of stragglers. However,
speculation mechanisms are designed and operated in-
dependently of job scheduling when, in fact, schedul-
ing a speculative copy of a task has a direct impact
on the resources available for other jobs. In this work,
we present Hopper, a job scheduler that is speculation-
aware, i.e., that integrates the tradeoﬀs associated with
speculation into job scheduling decisions. We imple-
ment both centralized and decentralized prototypes of
the Hopper scheduler and show that 50% (66%) improve-
ments over state-of-the-art centralized (decentralized)
schedulers and speculation strategies can be achieved
through the coordination of scheduling and speculation.
CCS Concepts
•Networks → Cloud computing; •Computer sys-
tems organization → Distributed architectures;
Keywords
speculation; decentralized scheduling; straggler;
ness
1.
INTRODUCTION
fair-
Data analytics frameworks have successfully realized
the promise of “scaling out” by automatically composing
Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for proﬁt or commercial advantage and that copies bear this notice
and the full citation on the ﬁrst page. Copyrights for components of this work
owned by others than ACM must be honored. Abstracting with credit is per-
mitted. To copy otherwise, or republish, to post on servers or to redistribute to
lists, requires prior speciﬁc permission and/or a fee. Request permissions from
permissions@acm.org.
SIGCOMM ’15, August 17 - 21, 2015, London, United Kingdom
© 2015 ACM. ISBN 978-1-4503-3542-3/15/08. . . $15.00
DOI: http://dx.doi.org/10.1145/2785956.2787481
user-submitted scripts into jobs of many parallel tasks
and executing them on large clusters. However, as clus-
ters increase in size and complexity, providing scalable
and predictable performance is an important ongoing
challenge for interactive analytics frameworks [2, 32].
Indeed, production clusters at Google and Microsoft [17,
23] acknowledge this as a prominent goal.
As the scale and complexity of clusters increase, hard-
to-model systemic interactions that degrade the perfor-
mance of tasks become common [12, 23]. Consequently,
many tasks become “stragglers”,
i.e., running slower
than expected, leading to signiﬁcant unpredictability (and
delay) in job completion times – tasks in Facebook’s
Hadoop cluster can run up to 8× slower than expected [12].
The most successful and widely deployed straggler mit-
igation solution is speculation, i.e., speculatively run-
ning extra copies of tasks that have become stragglers
(or likely to), and then picking the earliest copy that
ﬁnishes, e.g.,
[12, 14, 15, 24, 50]. Speculation is com-
monplace in production clusters, e.g., in our analysis of
Facebook’s Hadoop cluster, speculative tasks account
for 25% of all tasks and 21% of resource usage.
Speculation is intrinsically intertwined with job sche-
duling because spawning a speculative copy of a task has
a direct impact on the resources available for other jobs.
Aggressive speculation can improve the performance of
the job at hand but hurt the performance of other jobs.
Despite this, speculation policies deployed today are all
designed and operated independently of job scheduling;
schedulers simply allocate slots to speculative copies in
a “best-eﬀort” fashion, e.g., [14, 15, 24, 36].
Coordinating speculation and scheduling decisions is
an opportunity for signiﬁcant performance improvement.
However, achieving such coordination is challenging, par-
ticularly as schedulers themselves scale out. Schedulers
are increasingly becoming decentralized in order to scale
to hundreds of thousands of machines with each ma-
chine equipped with tens of compute slots for tasks.
This helps them make millions of scheduling decisions
per second, a requirement about two orders of magni-
tude beyond the (already highly-optimized) centralized
schedulers, e.g., [10, 29, 49].
In decentralized designs
379multiple schedulers operate autonomously, with each of
them scheduling only a subset of the jobs, e.g., [19, 23,
36]. Thus, the coordination between speculation and
scheduling must be achieved without maintaining cen-
tral information about all the jobs.
Contribution of this paper: In this paper we present
the design of the ﬁrst speculation-aware job scheduler,
Hopper, which dynamically allocates slots to jobs keep-
ing in mind the speculation requirements necessary for
predictable performance. Hopper incorporates a variety
of factors such as data locality, estimates of task ex-
ecution times, fairness, dependencies (DAGs) between
tasks, etc. Further, Hopper is compatible with all cur-
rent speculation algorithms and can operate as either a
centralized or decentralized scheduler; achieving scala-
bility by not requiring any central state.
The key insight behind Hopper is that a scheduler
must anticipate the speculation requirements of jobs
and dynamically allocate capacity depending on the
marginal value (in terms of performance) of extra slots
which are likely used for speculation. A novel observa-
tion that leads to the design of Hopper is that there is a
sharp “threshold” in the marginal value of extra slots –
an extra slot is always more beneﬁcial for a job below
its threshold than it is for any job above its thresh-
old. The identiﬁcation of such a threshold then allows
Hopper to use diﬀerent resource allocation strategies de-
pending on whether the system capacity is such that all
jobs can be allocated more slots than their threshold or
not. This leads to a dynamic, adaptive, online sched-
uler that reacts to the current system load in a manner
that appropriately weighs the value of speculation.
Importantly, the core components of Hopper can be
decentralized eﬀectively. The key challenge to avoiding
the need to maintain a central state is the fact that
stragglers create heavy-tailed task durations, e.g., see
[12, 14, 25]. Hopper handles this by adopting a “power
of many choices” viewpoint to approximate the global
state, which is fundamentally more suited than the tra-
ditional “power of two choices” viewpoint due to the
durations and frequency of stragglers.
To demonstrate the potential of Hopper, we have built
three demonstration prototypes by augmenting the cen-
tralized scheduling frameworks Hadoop [3] (for batch
jobs) and Spark [49] (for interactive jobs), and the de-
centralized framework Sparrow [36]. Hopper incorpo-
rates many practical features of jobs into its scheduling.
Among others, it estimates the amount of intermedi-
ate data produced by the job and accounts for their
pipelining between phases, integrates data locality re-
quirements of tasks, and provides fairness guarantees.
We have evaluated our three prototypes on a 200
node private cluster using workloads derived from Face-
book’s and Microsoft Bing’s production traces. The de-
centralized and centralized implementations of Hopper
reduce the average job completion time by up to 66%
and 50% compared to state-of-the-art scheduling and
straggler mitigation techniques. The gains are consis-
tent across common speculation algorithms (LATE [50],
GRASS [14], and Mantri [15]), DAGs of tasks, and local-
ity constraints, while providing ﬁne-grained control on
fairness. Importantly, the gains do not result from im-
proving the speculation mechanisms but from improved
coordination of scheduling and speculation decisions.
2. BACKGROUND & RELATED WORK
We begin by presenting a brief overview of existing
cluster schedulers: how they allocate resources across
jobs, both centralized and decentralized (§2.1), and how
they handle straggling tasks (§2.2). This overview high-
lights the lack of coordination that currently exists be-
tween scheduling and straggler mitigation strategies such
as speculation.
2.1 Cluster Schedulers
Job scheduling – allotting compute slots to jobs for
their tasks – is a classic topic with a large body of work.
The most widely-used scheduling approach in clus-
ters today is based on fairness which, without loss of
generality, can be deﬁned as equal sharing (or weighted
sharing) of the available resources among jobs (or their
users) [4, 26, 30, 45, 47]. Fairness, of course, comes with
performance ineﬃciencies, e.g., [41, 48].
In contrast, the performance-optimal approach for
job scheduling is Shortest Remaining Processing Time
(SRPT), which assigns slots to jobs in ascending order
of their remaining duration (or, for simplicity, the re-
maining number of tasks). SRPT’s optimality in both
single [39] and multi-server [37] settings motivates a fo-
cus on prioritizing small jobs and has led to many sched-
ulers such as [31, 33, 42].
The schedulers mentioned above are all centralized;
however, motivated by scalability, many clusters are be-
ginning to adopt decentralized schedulers, e.g., at Google
[23], Apollo [17] at Microsoft, and the recently proposed
Sparrow [36] scheduler. The scalability of decentralized
designs allows schedulers to cope with growing cluster
sizes and increasing parallelism of jobs (due to smaller
tasks [34]), allowing them to scale to millions of schedul-
ing decisions (for tasks) per second.
Importantly, the literature on cluster scheduling (both
centralized and decentralized) ignores an important as-
pect of clusters: straggler mitigation via speculation.
No current schedulers coordinate decisions with specu-
lation mechanisms, while our analysis shows that spec-
ulative copies account for a sizeable fraction of all tasks
in production clusters, e.g., in Facebook’s Hadoop clus-
ter, speculative tasks account for 25% of all tasks and
21% of resource usage.
2.2 Straggler Mitigation via Speculation
Dealing with straggler tasks, i.e., tasks that take sig-
niﬁcantly longer than expected to complete, is an im-
portant challenge for cluster schedulers, one that was
called out in the original MapReduce paper [24], and a
topic of signiﬁcant subsequent research [12, 14, 15, 50].
380Clusters already blacklist problematic machines (e.g.,
faulty disks or memory errors) and avoid scheduling
tasks on them. However, despite blacklisting, strag-
glers occur frequently, often due to intrinsically complex
causes such as IO contention, interference by periodic
maintenance operations, and hardware behaviors which
are hard to model and circumvent [12, 22, 35]. Straggler
prevention based on comprehensive root-cause analyses
is an open research challenge.
The most eﬀective, and indeed the most widely de-
ployed, technique has been speculative execution. Spec-
ulation techniques, monitor the progress of running tasks,
compare them to the progress of completed tasks of the
job, and spawn speculative copies for those progressing
much slower, i.e., straggling. It is then a race between
the original and speculative copies of the task and on
completion of one, the other copies are killed.1
There is considerable (statistical and systemic) so-
phistication in speculation techniques, e.g., ensuring early
detection of stragglers [15], predicting duration of new
(and running) tasks [16], and picking lightly loaded ma-
chines to spawn speculative copies [50]. The techniques
also take care to avoid speculation when a new copy is
unlikely to beneﬁt, e.g., when the single input source’s
machine is the cause behind the straggling [46].
Speculation has been highly eﬀective in mitigating
stragglers, bringing the ratio of the progress rates of
the median task of a job to its slowest down from 8×
(and 7×) to 1.08× (and 1.1×) in Facebook’s production
Hadoop cluster (and Bing’s Dryad cluster).
Speculation has, to this point, been done indepen-
dently of job scheduling. This is despite the fact that
when a speculative task is scheduled it takes resources
away from other jobs; thus there is an intrinsic tradeoﬀ
between scheduling speculative copies and scheduling
new jobs. In this paper, we show that integrating these
two via speculation-aware job scheduling can speed up
jobs by considerably, even on average. Note that these
gains are not due to improving the speculative execution
techniques, but instead come purely from the integra-
tion between speculation and job scheduling decisions.
3. MOTIVATION
The previous section highlights that speculation and
scheduling are currently designed and operated inde-
pendently. Here, we illustrate the value of coordinated
speculation and scheduling using simple examples.
3.1 Strawman Approaches
We ﬁrst explore two baselines that characterize how
scheduling and speculation interact today. In our ex-
1Schedulers avoid checkpointing a straggling task’s current
output and spawning a new copy for just the remaining work
due to the overheads and complexity of doing so. In gen-
eral, even though the speculative copy is spawned on the
expectation that it would be faster than the original, it is
extremely hard to guarantee that in practice. Thus, both
are allowed to run until the ﬁrst completes.
(a) Best-eﬀort Speculation.
(b) Budgeted Speculation
Figure 1: Combining SRPT scheduling and specu-
lation for two jobs A (4 tasks) and B (5 tasks) on
a 7-slot cluster. The + suﬃx indicates speculation.
Copies of tasks that are killed are colored red.
Figure 2: Hopper: Comple-
tion time for jobs A and B
are 12 and 22. The + suf-
ﬁx indicates speculation.
Table 1:
torig and tnew
are durations of
the
original and specula-
tive copies of each task.
amples we assume that stragglers can be detected af-
ter a task has run for 2 time units and that, at this
point, a speculation is performed if the remaining run-
ning time (trem) is longer than the time to run a new
copy (tnew). When the fastest copy of a task ﬁnishes,
other running copies of the same task are killed. Note