internal copy of Alice’s code with the captured one.
• P2-Image – Telegram Image Code Verification: Alice and
Bob open the code verification screen for a particular conver-
sation. Bob shows the image to Alice who should compare and
verify the image.
• P3-Numeric – WhatsApp Numeric Code Verification: Al-
ice and Bob open the code verification screens for a particular
5Implementation of the MITM attack is not within the scope of this work and all of
the codes were generated manually to emulate the attack.
6Since we did not have access to the Telegram image generation tool and the encoding
from the fingerprint to the image was unknown to us, we do not have control over the
number of incorrect/mismatching characters.
6
Figure 3: Protocol Flow of the user study.
conversation. Bob shows the screen to Alice who observes and
verifies the code.
In the remote setting, Alice and Bob sat in two different rooms
to simulate a remote communication scenario. The remote setting
consisted of three types of code verification methods (summarized
in Table 2, Appendix A.2):
• R1-Audio – Viber Audio Code Verification: In this method,
Alice calls the given contacts on the Viber app. Bob picks up the
Viber call, and the two open the code verification screens. Then,
Bob speaks the code and Alice compares the spoken code and
the displayed one.
• R2-Image – Telegram Image Code Verification: In this
method, Bob sends the image in a text (SMS) message to Al-
ice. Alice compares the code on the Telegram screen with the
code on the text message screen by switching between the two
apps. That is, Alice needs to open one app to memorize the image
code, then move on to the second app and compare the code.
• R3-Numeric – WhatsApp Numeric Code Verification: This
type of code verification works similar to the image code verifi-
cation method. The only difference is that the numeric code is
exchanged in a text message or email, and Alice compares the
numeric code on the WhatsApp screen with the code on the text
message screen by switching between the two apps.
4.3 Study Protocol
Figure 3 provides the high-level design protocol flow of the study.
After a brief introduction to the study, the participants were nav-
igated by the examiner to a computer desk that had the survey
web-site open, and were provided with a smartphone. The par-
ticipants were informed about the E2EE feature and the risk of
accepting incorrect codes. They were asked to follow the instruc-
tions on the survey website and perform the tasks diligently as
presented on the website. The study was composed of three phases:
the pre-study, the main study, and the post-study phase.
4.3.1 Pre-Study Phase. The pre-study phase consisted of several
qualitative and quantitative questions grouped into three categories,
as summarized below:
 DemographicsTechnical BackgroundOpen-ended QuestionsCode Veriﬁcation TaskFeedback QuestionsFamiliarity• R1-Audio: Audio Code with Viber, exchanged over Viber call. • R2-Image: Image Code with Telegram, exchanged as text message.• R3-Numeric: Numeric code with WhatsApp, exchanged as text message.• P1-QR: Code with Signal.• P2-Image: Image Code with Telegram.• P3-Numeric: Numeric code with WhatsApp.Proximity Setting TasksRemote Setting Tasks5 challenges in each veriﬁcation task• 2 matching codes.• 3 mis-matching codes (1-character, 1-block, whole code mismatch).ChallengesRepeat for all challengesMain Study PhasePre-Study PhasePost-Study PhaseDemographics: The participants were asked to fill out a demo-
graphic questionnaire. These questions polled for each participant’s
age, gender and education.
Technical Background: The participants were asked about their
general computer and security skills to uncover their initial attitude
towards security.
Familiarity with the Topic: To understand the participants’ ex-
perience in performing the tasks, they were asked about their fa-
miliarity with messaging applications and the E2EE feature offered
by the apps.
4.3.2 Main-Study Phase. During the main-study phase, we pre-
sented several code verification challenges to the participants. We
also asked them to rate their experience, after finishing each set of
the tasks, for a particular app. The six groups of questions (Table 2,
Appendix A.2) were presented randomly using a 6x6 Latin Square
and the questions within each group were also randomized. We
group the main-study questions into two sets:
Code Verification Task: For each set of challenges, we asked the
participants to follow the instruction on the website to verify the
code for a given contact. The list of the verification methods and
the steps they need to take to verify the code is presented in Table 2
(Appendix A.2). The code verification in our study was performed
in one direction (i.e., only the participant compared the code, not
the examiner).
Feedback Questions: After completing a set of code verification
tasks for a particular application, we asked the participants to rate
their experience by answering feedback questions, including System
Usability Scale (SUS) questions [20], and three additional 5-point
Likert scale question polled for their comfort, satisfaction, and need
for the system. A complete list of the SUS question is included in
Appendix A.1. The other three feedback questions were as follows:
• How comfortable are you with this code verification method?
• How satisfied are you with this code verification method?
• Do you want to use this feature in the future?
4.3.3 Post-Study Phase. In the post-study questions, we asked
the participants about their opinion about the code verification
task. We also asked them if they found any of the code verification
methods easier or more difficult than the others.
5 ANALYSIS AND RESULTS
5.1 Pre-Study and Demographics Data
We recruited 25 participants from a pool of students in our univer-
sity. Participation in the study was voluntary and the time allocated
to each participant to finish the study was 1 hour. There were 54%
males and 46% females among the participants in our study. Most
of the participants were between 18 and 34 years old (34% 18-24
years, 58% 25-34 years, and 8% 35-44 years). 9% of the participants
were high school graduates, 9% had a college degree, 46% had a
Bachelor’s degree and 36% had a Graduate degree. 34% of them
declared that they have excellent, and 62% declared they have good,
general computer skills. 17% had excellent, 50% had good and 33%
had fair security skills (demographic information is summarized in
Table 3) in Appendix A.2. About half of the participants said they
are extremely to moderately aware of the E2EE feature on the apps.
Based on the collected data, the most popular apps among the
participants in our study are WhatsApp, Viber, Telegram, and Signal
(these apps are also the most popular in the market according to
Table 4). Although the participants declared they have heard about
the E2EE feature and used it on the apps, it seems they had not
performed the code verification task much.
5.2 Proximity Code Verification Analysis
In our study, we tested the code verification task in the proximity
setting with QR code of Signal (P1-QR), image code of Telegram
(P2-Image), numeric code of WhatsApp (P3-Numeric). The FAR and
FRR for the proximity setting are reported in Figure 4.
Error Rates: Task P1-QR: For the QR code presentation, the code
verification task is performed almost automatically. The participants
captured the QR code, and after the app verified the code, they
reported the success or failure of code verification on the survey
form. Since in this model the participant is not involved in the
task of manual code comparison, we expected the error rates (both
FRR and FAR) to be near zero. This approach can be considered as
the baseline of the user study. Indeed, as expected, FRR (instances
of rejecting a benign session) for the QR code verification is 0%.
However, the FAR (instances of accepting an attack session) is 1.34%.
This error happened when one participant incorrectly reported that
the code verification was successful even though the app deemed
it as not matching.
Task P2-Image: For the image code verification method, the par-
ticipants compared the image displayed on his/her screen with the
one displayed on the examiner’s screen, and verified whether the
codes on the two devices match or not. Using this method, the FRR
was 0%, which indicates that none of the benign cases was rejected.
The FAR was 2.67%, only slightly higher than FRR.
Task P3-Numeric: For the numeric code verification method, the
participant compared the code displayed on his/her screen with the
one displayed on the examiner’s screen, and verified whether the
codes on the two devices match or not. Similar to the image code
verification, the FRR was 0% while the FAR was 2.67%.
The Friedman test to compare the FAR among multiple proximity
code verification methods rendered a Chi-square value of 1.400 and
was not statistically significant.7
User Experience and Perception: The SUS for the QR code
method (66.51) was the highest among all the methods in the prox-
imity setting. A system with SUS score of below 71 is considered to
have an “OK” [17] usability. Since the involvement of the users in
QR code verification was less compared to the other two methods,
we expected the users to prefer this method over the other two
methods. To measure the other user perception parameters, the
participants answered Likert scale questions, rating the comfort,
satisfaction, and adoptability from 1 to 5. The answers to this set of
questions are summarized in Figure 5. It seems that, after QR code,
the numeric code has the next best user perception ratings.
A Friedman test was conducted to compare SUS, comfort, satisfac-
tion and adoptability, among multiple proximity code verification
methods and rendered a Chi-square value of 1.167, 1.214, 1.578, and
4.252 respectively, which were not statistically significant.
7All results of statistical significance are reported at a 95% confidence level.
7
Figure 4: The error rates in performing the code verification
task in proximity setting.
Figure 6: The error rates in performing the code verification
task in remote setting.
Figure 5: Mean (Std. Dev) of user perception in performing
the code verification task in proximity setting.
The results of the user study shows low error rates (both FRR and
FAR) and moderate user perception ratings for all the approaches
in the proximity setting, demonstrating that the tested E2EE fared
quite well in this setting.
5.3 Remote Code Verification Analysis
In the remote setting, we tested audio-based code verification of
Viber (R1-Audio), image code presentation of Telegram (R2-Image),
and numeric presentation of WhatsApp (R3-Numeric). The FAR
and FRR for the remote setting are reported in Figure 6.
Error Rates: Task R1-Audio: In this task, the code was spoken by
the examiner over a Viber call and the participants compared it
with the code displayed on their device. The FRR was 0% in this
setting, which shows that none of the valid sessions was rejected
by the participants. However, the average FAR was on average
26.45%. The result also suggests that the error rate increased as the
number of mismatching characters decreased as shown in Table
1 (i.e., the success rate of the attack would increase if the attacker
has more control over the number of matching and mismatching
characters in the code). Further, we observe that even if the attacker
can generate a code that is different from the valid code in only one
block, the attacker can compromise about one third of the sessions.
Task R2-Image: For the remote image code verification method,
in which the image was sent as a text message, the FRR was 18.94%.
This result might lower the usability of the system, since the users
would need to repeat the key exchange protocol and the code veri-
fication process. The FAR was 13%, which indicates that the par-
ticipants could not detect some of the attack sessions. In general,
it seems that the users had some difficulty comparing the images.
Perhaps they could not memorize and compare the images that
appeared on two different apps (i.e., text message app, and Telegram
app). Since we did not have access to Telegram image generation
source code or tool, we did not have control over the number of
mismatching characters.
Task R3-Numeric: For the numeric code verification method, the
examiner sent the code in a text message and the participants
compared it with the copy of the code that their app generated. The
8
Figure 7: Mean (Std. Dev) of user perception in performing
the code verification task in remote setting.
result shows that the users could not easily compare the numeric
codes across the E2EE and SMS app. The average FRR was 22.94%
and the average FAR was 39.69%. Further, the FAR increased when
the number of mismatching characters decreased, as illustrated in
Table 1. We saw that the attacker can compromise about half of
the sessions, if there is a block of mismatching characters in the
code. The numeric code verification method is deployed by many
apps and is one of the major code verification methods used in the
remote setting. The high error rate in comparing the code shows
that such comparison offers low security and possibly low usability.
The Friedman test to compare the FAR among multiple remote
code verification methods rendered a Chi-square value of 6.306,
and was not statistically significant.
User Experience and Perception: Viber audio-based code verifi-
cation received the highest SUS score (58.05) among all the remote
code verification methods, followed by WhatsApp numeric code
verification (53.78). It seems users did not find Telegram image code
verification very usable in the remote setting (SUS=45.53). Systems
with SUS scores of below 50 are considered to have “poor” usability
[17]. Users also seem to rank Viber audio-based code verification
the highest with respect to comfort, satisfaction, and adoptability,
followed by WhatsApp. Figure 7 summarizes the answers to the
user feedback questions.
The Friedman test was conducted to compare SUS, comfort,
satisfaction and adoptability score, among multiple remote code
verification methods and rendered a Chi-square value of 3.021,
3.381, 4.251, and 6.732 respectively, which were not statistically
significant. Thus, based on our study, we do not observe much
statistical difference between the different remote code comparison
methods in terms of user perception ratings. Most ratings seem to
indicate a poor level of usability for all of these methods. We recall
that comparison between different methods is not the focus of our
study. The primary focus is rather to compare between the remote
and the proximity settings, which we analyze next.
0.00%1.34%0.00%2.67%0.00%2.67%0.00%10.00%20.00%30.00%40.00%50.00%60.00%70.00%80.00%90.00%100.00%FRRFARP1-QRP2-ImageP3-Numeric00.511.522.533.544.55ComfortSatisfactionAdoptabilityP1-QRP2-ImageP3-Numeric0102030405060708090100SUS0.00%26.45%18.94%13.00%22.94%39.96%0.00%10.00%20.00%30.00%40.00%50.00%60.00%70.00%80.00%90.00%100.00%FRRFARR1-AudioR2-ImageR3-Numeric00.511.522.533.544.55ComfortSatisfactionAdoptabilityR1-AudioR2-ImageR3-Numeric0102030405060708090100SUS5.4 Remote vs. Proximity Setting
Error Rates: The Wilcoxon Signed-Rank test indicates a statisti-
cally significant difference between the average FAR and the av-
erage FRR of the methods tested in the proximity setting and the
methods tested in the remote setting, with the p-value of 0.00 for
both FAR and FRR. Thus, in remote setting, the FAR and FRR are
significantly higher than the proximity setting, implying that code
verification method offers a significantly lower security and a higher
chance of rejecting valid sessions. The pairwise comparisons also
showed statistically significant difference between FAR and FRR in
proximity setting and that of remote setting in most cases (detailed
statistical analysis can be found in Table 5 in A.4).
User Experience and Perception: Apart from the error rates, re-
sponses to the feedback questions (user perception) shows that SUS
for all the code verification methods in the remote setting was lower
than the methods in the proximity setting. Also, users generally
were less satisfied and comfortable with the code verification, and
were less willing to use the remote verification methods.
The Wilcoxon Signed-Rank test indicates a significant difference
between the average SUS, comfort, satisfaction, and adoptability
ratings of methods in the proximity setting and methods in the
remote setting, yielding a p-value of 0.005 for the SUS, 0.007 for
comfort, 0.004 for satisfaction, and 0.003 for adoptability (detailed
statistical analysis can be found in Table 5 in A.4).
5.5 Bidirectional Authentication
In the user study, we asked the participants to compare and verify
the code only in one direction. In practice, the E2EE applications
ask both users involved in the conversation to verify each others’
code. However, the code verification on the two devices work inde-
pendently, that is, Alice may verify Bob, while Bob does not verify
Alice. This situation may work in favor of the attacker. If only one
party (Alice or Bob) accepts the attack session, he/she may con-
tinue to send messages, files, or photos that can be eavesdropped
(or tampered) by the attacker.
Therefore, for the attack to be successful, compromising only
one side of the connection is enough to intercept several messages
(even if we assume that one end-point who does not verify the
code does not send any messages or does not initiate any call).
Hence, in practice, the FARs will be higher than the one reported
in our study. For example, FAR is 40% in numeric code verification
considering the attack in only one direction. Same error reaches
64% (= 40% + 40% − 40% × 40%) if the attacker can deceive one of
the two parties. This result essentially shows that, in practice, in
the most common- use case of E2EE apps (the remote setting), the
human errors in comparing the codes could result in the success of
the MITM attack with a high chance.
Table 1: Effect of number of mismatching character(s) on
FAR for remote audio-based and numeric code verification.
Task ID
R1-Audio
R3-Numeric
1 Character
37.27%
60.25%
Mismatching Characters
1 Block Whole Code
12.75%
29.32%
51.69%
7.12%
5.6 Post Study Analysis
At the end of the study, we asked the participants to give us open-
ended comments about the study. We also asked them if they would
prefer proximity setting over the remote setting. The participants