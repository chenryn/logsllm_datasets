main internal to companies, but many systems are available on
Github [11, 13, 40, 65]. For the less tech-savvy, online services will
perform voice cloning for a fee [9, 12]. This combination of speech
synthesis eﬃcacy and availability is both exciting and worrisome.
Misuse of Speech Synthesis. There are many positive uses for
speech synthesis technology, such as giving voices to the mute,
aiding spoken language translation, and increasing human trust
of helper robots [19, 29, 49, 50, 84]. However, our work focuses on
the “shadow side” of these uses – generating synthetic speech with
malintent to deceive both humans and machines.
2.3 Voice-based Spooﬁng Attacks
In this work, we focus speciﬁcally on spooﬁng attacks against voice-
based user identiﬁcation, in which an attacker mimics a target’s
voice to steal their identity. A parallel line of work explores adver-
sarial attacks, in which an adversary adds inaudible perturbations
to speech to fool speaker recognition systems [25, 48, 53]. While
powerful, adversarial attacks diﬀer from spooﬁng attacks because
they do not mimic the target and so pose no threat to humans.
Figure 1 gives a high-level overview of spooﬁng attacks. There
are several techniques the adversary could use, and these are tax-
onomized in Table 1. Prior work has found that all spooﬁng tech-
niques – replay, impersonation, and synthesis – can reliably fool
machine-based voice recognition systems, but only a few works
have investigated the threat posed to humans. Here, we summa-
rize prior work that studied these spooﬁng attacks.
Spooﬁng Attacks Against Machines. We ﬁrst summarize prior
work measuring machines’ vulnerability to spooﬁng attacks.
• Record-and-Replay: In a replay attack, an adversary records
a victim’s exact speech and replays it to fool a target speaker
recognition system [18, 39]. The ASVspoof Challenge [44, 96]
has investigated this attack extensively. Replay attacks have high
overhead since the attacker must obtain speciﬁc recordings of
the victim. Furthermore, this attack is constrained by the con-
tent of the victim’s available recordings.
• Human Impersonation: Human voice actors can impersonate
others’ voices to great success, and well-crafted impersonation
spooﬁng attacks reliably fool speaker recognition systems [33,
34, 51, 78, 86]. These attacks have even defeated HSBC’s speaker
recognition-based security [80]. While eﬀective, these attacks
have high overhead and limited versatility due to their depen-
dence on human talent.
• Machine Synthesis (Classical): Most prior work uses GMM-
based speech synthesis systems (e.g., Festvox [20]) to attack pub-
lic, GMM-based speaker recognition systems [28, 45, 56, 57]. A
recent work takes a “real-world” focus by testing a small set of
synthetic speech generated by Festvox against ﬁve mobile apps
that support voice authentication, and reports 96%+ success [79].
However, the eﬃcacy of classical synthesis attacks against mod-
ern speaker recognition systems remains unclear.
• Machine Synthesis (DNN-based): To the best of our knowl-
edge, only one work [62] has examined the performance of DNN-
based synthesis attacks. It performed preliminary tests by run-
ning 10 synthesized samples for 6 speakers (generated by [41])
against three locally-trained speaker recognition prototypes. It
produced vague conclusions: these speaker recognition proto-
types produce more errors when running on synthesized speech
compared to clean (non-synthesized) speech.
Spooﬁng Attacks Against Humans. Existing work assessing
human susceptibility to spooﬁng only evaluates impersonation and
classical synthesis attacks. The single impersonation attack paper
found that humans can be fooled by actors pretending to be older
or younger than they really are [34]. The ﬁrst classical synthesis
attack measurement paper [57] uses a traditional survey format
and ﬁnds that users correctly distinguish between real and Festvox-
synthesized voices (imitating the real speaker) about 50% of the
time, regardless of their familiarity with the real speaker. A follow-
up study to this [60] uses the same data and survey format but in-
cludes fNIRS brain scanning technology to measure participants’
neural activity. They ﬁnd no statistically signiﬁcant diﬀerences in
neural activity when real or synthetic speakers are played.
2.4 Defending Against Synthesized Speech
Numerous defenses have been proposed to defend speech recog-
nition systems against synthetic speech attacks. While most have
focused on detecting synthetic speech or speakers [16, 17, 26, 32,
77, 90, 91, 98, 100], recent work has pointed towards a new defense
direction: preventing unauthorized speech synthesis [38]. We dis-
cuss and evaluate representative defenses in §6.
3 METHODOLOGY
No comprehensive study exists today that studies the threat posed
by DNN-based speech synthesis to software-based speaker recog-
nition systems and human users. Our work addresses this critical
need, and outlines future work needed to mitigate the resulting
threat. Here, we describe the threat model, and the methodology,
tools and datasets used by our analysis.
3.1 Threat Model and Assumptions
In DNN-based speech synthesis attacks, the adversary A’s goal is
to steal a target T ’s identity by imitating their voice. To do so, A
Session 1D: Authentication and Click Fraud CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea237ﬁrst collects a set of speech samples ST from T , either by secretly
recording their speech in a public setting or by extracting the audio
from public video/audio clips. When A knows T personally, these
speech clips could also be obtained from private media. Next, A
inputs ST to a speech synthesis system, which produces synthe-
sized or fake voice samples SA. In this case, SA should sound like
T but contain arbitrary speech content chosen by A.
We make the following assumptions about the adversary A:
• A only needs a small volume of speech samples from T , i.e., less
than 5 minutes of speech in total;
• A directly uses a publicly available, DNN-based voice synthesis
system to generate synthetic speech SA;
• A seeks to generate fake voice samples SA that make either
humans or machines believe that they are interacting with T .
3.2 Overview of Experiments
We conduct a measurement study to explore the real-world threat
posed to both machines and humans by today’s publicly avail-
able, DNN-based speech synthesis systems. These include:
• Empirical experiments to examine whether synthetic speech can
fool speaker recognition (SR) systems, aka machines (§4);
• User studies to explore human susceptibility to synthetic speech
under multiple interactive scenarios (§5);
• Empirical experiments to assess the eﬃcacy of existing defenses
against DNN-based synthesis attacks (§6).
In the following, we describe the DNN synthesis and SR systems
as well as speaker datasets used by our experiments.
3.3 DNN-based Synthesis Systems Studied
We consider "zero shot" systems (i.e., those requiring  98% success rate. We recreate
this attack and ﬁnd that it fails on more recent SR systems (Table 2).
A detailed description of our experiments is in the Appendix.
System
Traditional SR Systems
Modern SR Systems
Attacker
Bob Spear
UBM-GMM
Attacker 1 (Male)
Attacker 2 (Female)
90.3%
96.0%
Bob Spear
ISV
98.3%
96.0%
Azure Resemblyzer
11.1%
2.0%
0.0%
0.0%
Table 2: The classical synthesis attack generated by Festvox
eﬀectively fools traditional SR systems (Bob Spear) but fails
on modern SR systems (Azure, Resemblyzer).
4.2 Resemblyzer (Open Source SR)
Next we test DNN-based speech synthesis attacks against Resem-
blyzer, a modern SR system widely used in recent literature. We
use the oﬃcial implementation of Resemblyzer provided by [6].
Factor
Size of
ST
Quality of
ST
Phonetic
Similarity
Target
Gender
Target
Accent
Experiment Methodology
Generate synthetic speech, varying
number of target samples N .
Add Gaussian noise to target samples
before speech synthesis.
Vary the phonetic distance between
target samples and synthesis output.
Test male/famale target speakers from
VCTK and LibriSpeech separately.
Test native/non-native English speakers
from SpeechAccent dataset separately.
Attack Success Rate (AS)
Low AS when N  AS for male targets.
AS for native English speakers