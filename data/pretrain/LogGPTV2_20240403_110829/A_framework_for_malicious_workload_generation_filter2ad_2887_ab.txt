An example of network layer obfuscation is IP fragmen-
tation. Examples of application layer obfuscation include
HTTP URL encoding techniques such as Bare-byte or Uni-
code encoding used by worms such as Nimda [8]. The fol-
lowing example applies a Unicode obfuscator to the example
HTTP request given above:
[ httprequest, {’method’:’GET’,
’absolute_path’:URLObfuscator.uencode(’/index.html’) } ]
Propagation Elements - MACE supports diﬀerent propaga-
tion models via AddressPool objects. Each AddressPool is
instantiated with a list of CIDR preﬁxes and port ranges,
along with an indication of how to generate addresses from
the given pool. Address pools may be traversed randomly,
horizontally (sequentially across IP addresses, sequentially
across ports), vertically (sequentially across ports, sequen-
tially across IP addresses), or by a more complex methodol-
ogy deﬁnable by the user. The following example illustrates
an address pool deﬁned to perform a vertical sweep of the
target IP address 10.42.1.1.
target_pool = AddressPool(AddressPool.Vertical, ’10.42.1.1:1-65536’)
3.3 Example Test Scenarios
We focus on four attack vectors:
• SYN ﬂood : A standard denial-of-service attack that
overwhelms the target system by sending a large num-
ber of TCP SYN packets.
• Welchia: A ping followed by a series of HTTP requests
designed to exploit a buﬀer overﬂow in the WebDAV
module of Microsoft’s IIS web server [1].
• Rose: An attack that exploits poorly implemented
handling of IP fragments. Two small fragments are
sent, one with an oﬀset of 0 and one with a large
oﬀset. Some network stacks reserve memory for the
fragment hole, so a series of Rose packets can exhaust
memory [12].
• Blaster : An attack that exploits a buﬀer overﬂow in
Microsoft Windows RPC service (epmapper) [2].
The four attack proﬁles are realized in the example code
fragment shown in Figure 2. Attack proﬁles are deﬁned
by their vulnerability exploit and propagation method. A
full exploit is a sequence of generator and validator steps,
along with parameters. A generator builds packet traﬃc
using the payload and header construction and obfuscation
building blocks. A validator collects and processes responses
from the attack target, verifying that the generated traﬃc
evoked an appropriate response. Each step in an attack
vector is executed as long as they are successful. An exploit
step may simply be “create a TCP packet with the SYN ﬂag
set” (line 1). Other steps might be as complicated as “create
an HTTP GET request for the document ‘/’ and validate
that the HTTP response contained a 200 (success) code and
that it was produced by a Microsoft web server” (lines 6–7).
Some generator steps are not followed by validation, e.g., the
Rose attack where validation is unnecessary (lines 12–15).
Since generators and validators are simply Python functions,
it is easy to deﬁne new exploit steps. Propagation models for
the attack proﬁles are deﬁned by one or more address pools
(lines 23–25). Finally, attacks are sent using the send once
or send periodic functions (lines 27–28). send periodic is
a convenience function used to produce attack traﬃc for a
given duration (delaying a speciﬁed amount of time between
successive call to send once).
synflood = [ [ rawtcp, { ’th_flags’:TH_SYN} ] ]
bad_string = ’...’ # the buffer overflow string - not defined here
welchia = [
[ ping ],
[ httprequest, { ’method’:’GET’ },
httpvalidate, { ’Server’:’Microsoft-IIS/5.0’ ],
[ httprequest, { ’method’:’SEARCH’ },
httpvalidate, { ’code’:411, ’Server’:’Microsoft-IIS/5.0’} ],
[ httprequest, { ’method’:’SEARCH’, ’absolute_path’:bad_string } ] ]
rosepayload = ’\0’ * 32 # just a small fragment
rose = [
[ rawudp, { ’frag_offset’:0, ’frag_flag’:IP_MF, ’payload’:payload } ],
[ rawudp, { ’frag_offset’:8100, ’payload’:payload } ] ] # 64800 byte offset
bindreq = ’...’ # the DCE RPC bind request - not defined here
overflow = ’...’ # the buffer overflow exploit request - not defined here
blaster = [
[ dcerpcbind, { ’bindreq’:bindreq }, dcerpcbindack, { ’bindreq’:bindreq } ],
[ dcerpcreq, { ’payload’:overflow } ] ]
src_pool = AddressPool(AddressPool.Vertical, ’10.42.0.0/16:1-65536’)
dst_pool = AddressPool(AddressPool.Random, ’10.52.128.0/20:1024-65536’)
http_pool = AddressPool(AddressPool.Horizontal, ’10.52.128.0/20:80’)
send_once(src_pool, dst_pool, rose)
send_periodic(src_pool, http_pool, welchia, duration=60, delay=0.001)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
Figure 2: A MACE code fragment, similar to what
was used in our experiments.
4. SECURITY SYSTEM PERFORMANCE
EVALUATION
To demonstrate MACE’s capability, we examined perfor-
mance characteristics of three standard network security sys-
tems: a ﬁrewall middlebox and two network intrusion detec-
tion systems running on commodity hardware.
4.1 Test Environment
The ﬁrewall we tested is a Cisco PIX 515e. It contains
three Fast Ethernet interfaces, 64MB of RAM, a 433 MHz
Intel Pentium II, and runs version 6.2(2) of the PIX Firewall
operating system. It is a typical device deployed as the ﬁrst
line of defense for edge networks, implementing basic packet
ﬁltering and network address translation (NAT). The net-
work intrusion detection systems were Bro (version 0.8a79)
and Snort (version 2.1.1). Bro generally maintains signif-
icant connection state, while Snort does not. Each NIDS
ran on a separate workstation with a 2 GHz Intel Pentium
4 processor and 1 GB of RAM. FreeBSD 5.1 was installed
84Figure 3: Experimental Environment. Legitimate
and malicious traﬃc originate from the internal net-
work and are directed toward the external network.
The PIX ﬁrewall separates the inside and outside
networks and performs network address translation.
on each machine1. For each NIDS, we used a default set
of rules. Our Snort instance used the default snort.conf
included with the software and Bro used the mt policy.
The PIX and NIDS were conﬁgured in a testbed as shown
in Figure 3. The setup mimics an edge network connected
to an ISP, with legitimate background traﬃc and attacks
focused on a remote network. The PIX resided between the
edge (“internal”) network and the ISP (“external”) network.
The internal network contained traﬃc generators for MACE
and background traﬃc, and the two NIDS. All network traf-
ﬁc received from or sent to the PIX was duplicated on the
links connected to the NIDS. In the external network, we
used a hardware propagation delay emulator (Adtech SX-14)
between two backbone-class routers (Cisco 12000) to create
a round-trip time of roughly 100 milliseconds between the
traﬃc generators and the target servers. We used popular
enterprise-scale switches (Cisco 6500) to aggregate traﬃc at
the endpoints.
On each host in the internal network, we created 212 alias
addresses and on the remote hosts we created 28 aliases.
The PIX performed network and port address translation
between hosts on the internal network and a pool of 28 ad-
dresses routable across the external network. Also, the PIX
performs an implicit packet ﬁltering based on its NAT con-
ﬁguration: it only performs NAT or port address translation
(PAT) for local addresses that are part of its conﬁguration.
Packets from any other source address are dropped.
Using two levels of benign background traﬃc, we gener-
ated attack traﬃc using a set of ﬁve exploits and six levels
of oﬀered load. Two levels of background traﬃc, “low” and
“high”, were generated using Harpoon [17] and were tuned
to averages of 20 Mbps and 70 Mbps, respectively. Source
and destination addresses for legitimate traﬃc were chosen
randomly from the pool of 212 source addresses and the pool
of 28 destination addresses. The six levels of attack load
were generated by using one to ﬁve hosts running MACE.
For each exploit, the MACE processes on a single host were
conﬁgured to generate roughly 1 Mbps of traﬃc, regardless
of background traﬃc level.
The exploits we used were (1) Welchia worm traﬃc, (2)
SYN ﬂood denial-of-service attack, (3) a SYN ﬂood with
spoofed source addresses, (4) the Rose fragment attack, and
(5) a multi-modal attack consisting of the previous four ex-
ploits plus Blaster worm traﬃc. Each host running MACE
used a source address pool of 212, as described above. For
the SYN ﬂoods and Rose attack, we horizontally traversed
the source address pool. For the two worm exploits, we
randomly (uniformly) sampled the source address pool. All
attack traﬃc was directed toward a single address on the
remote network.
4.2 Test Measurements
For the 52 distinct experiments, we measured CPU and
memory utilization at all three systems every ﬁve seconds.
We also measured packet counts in and out of the PIX every
ﬁve seconds and the number of reported packet drops using
SNMP, and took packet traces on either side of the PIX. At
the two NIDS hosts, we veriﬁed and used the capabilities
of each software package to report received packet volume
and the number of dropped packets. Packets are dropped by
each system due to overﬂow of the queue of incoming packets
received by the packet ﬁlter (each NIDS uses the Berkeley
Packet Filter and libpcap for packet capture). For packet
drops at each NIDS, we did not discriminate between benign
and malicious traﬃc. For the PIX, we used the packet traces
to measure drops of benign packets. Each experiment was
run for six minutes, including a one minute warm-up phase
from which measurements are discarded.
4.3 Results
Figure 4 shows average CPU utilization and Figure 5
shows packet loss measurements for each experiment. The
two columns in each ﬁgure correspond to low and high back-
ground traﬃc levels, and the three rows display results for
each device. The ﬁrst feature to notice in the plots is the di-
versity of responses of each system to distinct MACE attack
proﬁles. Except for a few cases, there is also a noticeable,
and sometimes very large, divergence between the ﬁrst two
data points. These points correspond to zero malicious traf-
ﬁc and a single MACE host. We discuss detailed results for
each device class (ﬁrewall and NIDS) below.
4.3.1 Effects on the PIX ﬁrewall
For the PIX, the Rose attack has the least eﬀect on perfor-
mance. When processing fragmented packets, the PIX keeps
a queue of (by default) 200 fragments in order to reassemble
them before forwarding them to the remote network. If the
missing fragments do not arrive in a conﬁgurable amount of
time (the default is 5 seconds), the fragments are dropped.
For the Rose attack, the fragment queue becomes full shortly
after starting MACE. When fragments arrive that cannot
be queued, they are dropped. Although there is no frag-
mented legitimate traﬃc in our setup, these packets would
very likely be dropped even with an attack rate of just over
40 Rose packets per second. Also, queues for each inter-
face and the fragment reassembly queue share a common
buﬀer pool. Since there are fewer buﬀers available during a
Rose attack, interface queues are more likely to ﬁll, causing
additional packet drops2.
The non-spoofed SYN ﬂood is the attack with the most