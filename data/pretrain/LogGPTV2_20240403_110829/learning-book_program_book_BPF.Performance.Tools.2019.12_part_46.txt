8.3 BPF Tools
325
32 -> 63
: 574
|***
64 -> 127
: 407
| * *
128 -> 255
: 163
256 -> 511
: 253
| *
512 -> 1023
: 98
1024 -> 2047
: 89
2048 -> 4095
: 39
4096 -> 8191
: 37
8192 -> 16383
: 27
L9LZE  65535
: 21
65536 -> 131071
: 10
operat.ion = *xx1te*
usecs
1count
distribution
0 -> 1
: 414
2 -> 3
: 1327
4 -> 7
C9EE :
 -> 15
: 22415
| + +
  ×
16 -> 31
: 65348
32 -> 63
: 5955
|***
64 -> 127
: 1409
128 > 255
: 2B
operation = *open'
Use c.s
: count
dlstr1butlon
{ -> 1
: 7557
|*********
E 7
:4
B -> 15
: 6
1.6 -> 31
:2
This output shows separate histograms for reads, writes, and opens, with counts indicating
that the workload is currently write-heavy. The read histogram shows a bi-modal distribution,
with many taking less than seven microseconds, and another mode at 16 to 31 microseconds.
The speed of both these modes suggested they were served from the page cache. This difference
between them may be caused by the size of the data read, or dlifferent types of reads that take
different code paths. The slowest reads reached the 65- to 131-millisecond bucket: these may be
from storage devices, and also involve queueing.
The write histogram showed that most writes were in the 16- to 31-microsecond range: also fast,
and likely using write-back buffering.
BCC
Command line usage:
xfsdiat[optlons][intezval [count]]
---
## Page 363
326
Chapter S File Systems
Options include:
-m: Print output in milliseconds (default is microseconds)
•-p PID: Measure this process only
The interval and count arguments allow these histograms to be studied over time.
bpftrace
The following is the code for the bpftrace version, which summarizes its core functionality. This
version does not support options.
+1/usx/local/bin/bpftrace
BEGIN
7
printf (*Tracing XFs operation latency... Bit Ctrl-C to end.\o*)
kprobe:xfs_file_read_iter,
kprobe:xfs_f1le_wxlte_iter,
kprobe:xfs_file_open,
kprobe:xfs_file_fsync
1
Bstart[tid] = nsecs
Bnane [tid] = func
kretprobe:xfs_file_read_iter,
kretprobe:xfs_file_write_iter,
kzetprobe:xfs_f11e_open,
kretprobe:xfs_file_fsync
/ [pT]re=8/
(000t / ([pta]1aesg -soesu) )asTu =[[pta]eueug]en.8
delete (estart[tid]) 
delete (@nane [tid[)
J
END
clear (8start) 
clear (Bnane) 
This makes use of the functions from the XFS struct file_operations. Not all file systems have such
a simple mapping, as discussed in the next section about ext4.
---
## Page 364
8.3 BPF Tools
327
8.3.23 ext4dist
There is a ext4dist(8)3 tool in BCC that works like xfsdlist(8), but for the ext4 file system instead.
See the xfsdist(8) section for output and usage.
There is one difference, and it is an example of the difficulty of using kprobes. Here is the
ext4_file_operations struct from Linux 4.8:
const stxuct f1le_operatlons ext4_Cile_opexationa = (
 11seek
= ext4_1lseek
.read_1tex
-generic_cile_read_iter,
.vrite_iter
= ext4_file_vrite_iter,
,unlocked_oct1 - ext4_ioct1,
[...]
The read function highlighted in bold is generic_file_read_iter(), and not an ext4 specific one.
This is a problem: if you trace this generic one, you are also tracing operations from other file
system types, and the output will be polluted.
The workaround used was to trace generic_file_read_iter() and examine its arguments to
determine if it came from ext4 or not. The BPF code examined the struct kiocb *icb argument in
this way, returning from the tracing function if the file system operations were not for ext4:
// ext4 r11tex on file=>f_op == ext4_c1le_operatlons
f_op 1= EXP4_FILE_OPERAPIo8s)
return 07
The EXT4_FILE_OPERATIONS was replaced with the actual address of the ext4_file_operations
struct, found by reading /proc/kallsyms during program startup. Its something of a hack, but it
works. It comes with the performance cost of tracing all generic_file_read_iter() calls, affecting
other file systems that use it, as well as the additional test in the BPF program.
Then came Linux 4.10, which changed the functions used. Now we can examine a real kernel
change and its affect on kprobes, instead of hypothetically warning about the possibility. The
hile_operatsons struct became
const struct file_operations ext4_file_operations = (
,11seek
= ext4_1lseek,
.read_iter
- ext4_file_read_iter,
.vz1te_1ter
unlocked_ioct] = ext4_ioct1,
[. - -]
trace directly, so you no longer need to tease apart ext4 calls from the generic function.
Compare this to the earlier version. Now there is an ext4_file_read_iter() function that you can
35 0rigin: I created this on 12Feb-2016, inspired by my 2012 zfsdist.d DTrace tool, and the bpfrace ersion forts
book on 2-Feb-2019,
---
## Page 365
328
Chapter S File Systems
bpftrace
To celebrate this change, I developed ext4dist(8) for Linux 4.10 and later (until it changes again).
Example output:
 ext4dist.bt
Attach.ing 9 probes..
Tracing ext4 operation latency... Hit Ctrl-C to end.
us ext4_sync_fi1e] :
[18, 2K)
[2K, 4K)
1188988 88 88 886e88e8898698
[4K, 83)
[8K,16K]
118e988 889886 886986 8e98698
eus |ext4_file_write_iter| :
[1]
14 1889889
[2, 4)
088988988988187
[4, B]
72 1889889889888888e88e88e86986e8898
[8, 16}
114 1889889889888 8889889889889889889889886 8869889889869881
[16, 32)
26 188e88e88e88
[32, 64)
e8688688 88 888e88 886886881 T9
[64, 128]
5 188
1
[128, 256}
01
[256, 512}
0 1
[512, 1K)
11
us |ext
[0]
L ter]:
1|
[1]
1 1
[2, 4)
768 88 8888868888 88886980 1
[4, B]
385 1eeeeeeeeeeee eeeeeeeeeeeeee
[8, 16}
112 18898898
[16, 32)
1818
[32, 64)
5 1
[64, 128]
[128, 256}
124 188988988
[256, 512}
7018898
[512, 1K]
31
us |ext4_file_open] :
[0]
1105 18ee88e8898
[1]
221189
---
## Page 366
8.3 BPF Tools
329
[2, 4]
5377 1869889889886 88686986869869889889886 8868698686986 1
[4, B]
359 1849
[6, 16}
421
[16, 32)
5 1
[32, 64)
11
The histograms are in microseconds, and this output all shows sub-millisecond latencies.
Source:
#1/usx/local/bin/bpEtrace
BEGIX
1
1atency-.. Hit Ctr1=C to end,^n");
1
kprobe:ext4_file_read_iter,
kprobe :ext4_f11e_vri te_1 te,
kprobe:ext4_file_open,
kprobe:exta_sync_f11e
Bstart[tid] = nsecs;
Bnsne[tid] - funcz
kzetprobe:ext4_f11e_read_iter,
kretprobe:ext4_file_vrite_iter,
kzetprobe:ext4_f1le_open,
kretprobe:ext4_sync_file
/[pxes8/
Bus[@nane [tid]] = hlst( [nsecs - @start[tid])/ 1o00} 
delete (fstart[tid]) 
delete (@nane [tid1 
END
clear (8start) 
 (eueug) xee[
The map was named *@Pus" to decorate the output with the units (microseconds).
---
## Page 367
330
Chapter S File Systems
8.3.24
icstat
icstat(8) traces inode cache references and misses and prints statistics every second. For example:
Attaching 3 probes...
+ icstat.bt
Tracing icache lookups... Hit Ctrl-C to end.
REFS
HISSESHIT
0
0
0
21647
1001
38925
35250
8
TBLEE
08LEE
0%
815
806
1
0
0%
 0
0%
[..-]
This output shows an initial second of hits, followed by a few seconds of mostly misses. The work
load was a find /var 1s, to walk inodes and print their details.
The source to icstat(8) is:
#1/usx/1ocal/bin/bpEtrace
BEGIN
1
printf (*Txacing Lcache lookups... Hlt Ctz1=C to end.\n*);
printf(*&10s e10s 5s`n*,*RErs*, *HIsSEs*,*HIr”) 
kretprobe:find_1node_fast
Brefs++;
if (retval == 0) 
enlsses++
Interval:s:1
Shits = @refs - @nisses;
$peroent = refs > 0 ? 100 * $hits / Brefs : 0;
printf(*s10d s10d 44dss,n*, Brefs, @nlsses, $percent);
36 Origin: 1 created it for this book on 2-Feb-2019. My first inode cache stat tool was inod
stat7 on 11-Mar-2004, and
Im sure there were earlier inode stat tools (fom memory, the SE Toolkit).
---
## Page 368
8.3 BPF Tools
331
clear (8refs) 
clear (@nisses) 
END
clear (Brefα) ;
clear (@nisses] 
@refs is zero.
As with dcstat(8), for the percent calculation a division by zero is avoided by checking whether
8.3.25bufgrow
bufgrow(8)37 is a bpftrace tool that provides some insight into operation of the buffer cache.
This shows page cache growth for block pages only (the buffer cache, used for block I/O buffers),
showing which processes grew the cache by how many Kbytes. For example:
+ bufgrow.bt
Attach.ing 1 pzobe..
^C
ekb[dd] : 101856
While tracing, *dd° processes increased the buffer cache by around 100 Mbytes. This was a
synthetic test involving a dd(1) from a block device, during which the buffer cache did grow by
100 Mbytes:
↓ free
tota1
used
69328
free
shared
buffers
cache
ava1lab1e
Men:
70336
471
26
534
82689
Sxap1
0
[..]
free
tots1
used
free
shsred
buffers
69153
cache
available
Nen:
70336
473
26
102
607
68839
Sxap:
0
0
The source to bufgrow(8) is:
1/usz/local/bin/bpftrace
include 
37 origin: 1 created t for this book on 3-Feb-2019.
---
## Page 369
332
 Chapter S File Systems
kprobe:add_to_page_cache_lru
Snode = $as=>host->1_mode
r[Bre (. aoedsseaappe 1onrqe) = s9g
/ / match block mode, uapi/linux/stat.h:
1f ($acde &0x6000) ↑
ekb[comm] = sun (4) ≠
// page size