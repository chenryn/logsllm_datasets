1% such emails in the training set can cause SpamBayes to mis-
classify an email with 90% probability. Other than Bayes classifier,
Biggio et al. [9] target support vector machine (SVM) and study
the corresponding pollution tactics. Fumera et al. [24] evaluate pat-
tern classification systems in general, and conclude that all of them
are vulnerable to data pollution attacks. Wang et al. [45] show that
crafted training samples can mislead the machine learning classifier
detecting malicious crowdsourcing workers.
All these causative attacks, or called data pollutions, serve as a
good motivation for the KARMA. For example, the pollution tech-
nique proposed by Nelson et al. [33] has been used in our evaluation.
8.1.2 Exploratory Attacks. Exploratory attacks are out of
scope of KARMA. For completeness, we still briefly talk about such
attacks in adversarial machine learning. Exploratory attacks can be
further classified as two sub-categories: model inversion [12, 23]
where an attacker infers training data samples based on the learning
model, and data evasion [7, 13, 29, 44, 45] where an attacker crafts
samples to evade the learning model. Model inversion is within the
scope of the machine unlearning proposed by Cao et al. However,
because the samples to unlearn are known in such scenario, which
is the private data identified by the user, it is not necessary to ap-
ply KARMA. Data evasion is also beyond the scope of the machine
unlearning paper.
8.2 Defense of Data Pollution
In this part of the section, we introduce prior works that defend
data pollution. Such works can be divided into two categories: filter-
ing polluted samples before training, and training a robust learning
model. First, both Brodley et al. [10] and Cretu et al. [17] introduce
an additional filtering layer to get rid of polluted samples. Brod-
ley et al. use majority consensus among different techniques, and
Cretu et al. adopt sanitization with micro-models in a voting scheme.
Similarly, Newsome et al. [34] cluster samples beforehand so that
outliers, such as polluted samples, can be filtered. Second, Dekel et
al. [19] minimize the damage that an attacker could make by formu-
lating the learning as a linear program and using an online-to-batch
conversion. Bruckner et al. [11] model the learner and the attacker
as a game with Nash equilibrium.
Techniques that filter polluted samples before training or make
learning model robust are orthogonal to and can be combined with
KARMA. If polluted samples bypass these approaches as evident by
new pollution attacks [19, 35], KARMA serves as a remedy approach
that repairs polluted learning models and brings the model to healthy
states.
8.3 Other Similar Techniques
Machine unlearning is a technique proposed by Cao et al. [14] that
makes learning systems forget what they have learned before. Cao et
al. convert a learning algorithm to a special form in statistical query
learning [28], which consists of a small number of summations. The
learning algorithm only depends on these summations, which are the
sum of some efficiently computable transformation of the training
data samples. Therefore, to unlearn a training sample becomes easy:
One just needs to subtract the transformations of that sample from
all the summations, and then update the learning model.
Machine unlearning only removes samples from a learning model
when the samples are specified. At contrast, KARMA tries to find
what data to remove from a learning model. As discussed in Sec-
tion 4.1, KARMA does utilize machine unlearning technique by
Cao et al., but is compatible with other incremental or decremental
machine learning [15, 20, 22, 37, 42, 43]. The reason we use ma-
chine unlearning is that machine unlearning is general, which makes
KARMA general as well.
BoostClean [31] detects and repairs domain value violations,
i.e., an attribute value is outside of its value domain, using statistical
boosting. As a comparison, their repairing is to correct the prediction
results of a machine learning model, but KARMA is to correct the
machine learning model itself.
Koh et al. [30] propose using influence functions to estimate the
influence of training samples upon the prediction results. Therefore,
their approach can be used to prioritize the administrator’s efforts
in inspecting the training set. The advantage of their approach is
that they do not need to remove any training samples and observe
causality. As a comparison, KARMA is more accurate and further
reduces the administrator’s efforts because KARMA directly observes
the effects by unlearning samples. For example, according to the
evaluation of Koh et al., the administrator needs to inspect 30% of
training data if 10% is polluted. At the same time, KARMA also
improves the efficiency from retraining models from scratch.
ASIA CCS ’18, June 4–8, 2018, Incheon, Republic of Korea
Y. Cao et al.
9 CONCLUSIONS
In this paper, we present a new technique, called causal unlearning,
which actively searches the training set for the misclassification
cause in an iterative manner and then removes the cause to repair a
polluted machine learning system.
We implemented a prototype of KARMA for causal unlearning,
and evaluated it using SpamBayes, another SVM-based spam filter
and a JavaScript malware detection engine. Our evaluation results
show that KARMA can successfully identify the misclassification
cause, i.e., polluted samples, with true positive ranging between
98.0% and 99.97% and true negative ranging between 85.5% and
94.3%. Further, KARMA can repair polluted learning model and
restore the learning model’s accuracy to the vanilla value with less
than 1% differences.
10 ACKNOWLEDGEMENT
We would like to thank Nicolas Papernot (our shepherd), Alex Yang
and anonymous reviewers for their helpful comments and feedback.
This work was supported in part by National Science Foundation
(NSF) grants CNS-15-63843, and CNS-15-64055. The views and
conclusions contained herein are those of the authors and should
not be interpreted as necessarily representing the official policies or
endorsements, either expressed or implied, of NSF.
REFERENCES
[1] CS229 machine learning (stanford university). http://cs229.stanford.edu/materials/
[2] Enron-spam dataset. http://www.aueb.gr/users/ion/data/enron-spam/index.html.
[3] Liblinear. https://www.csie.ntu.edu.tw/~cjlin/liblinear/.
[4] Machine
learning (stanford university). https://www.coursera.org/learn/
ML-advice.pdf.
machine-learning.
[5] Spamassassin dataset. https://spamassassin.apache.org/publiccorpus/.
[6] SpamBayes. http://spambayes.sourceforge.net/.
[7] M. Q. Ali, A. B. Ashfaq, E. Al-Shaer, and Q. Duan, “Towards a science of anomaly
detection system evasion,” in IEEE Conference on Communications and Network
Security (CNS), September 2015.
[8] M. Barreno, B. Nelson, A. D. Joseph, and J. D. Tygar, “The security of machine
learning,” Mach. Learn., vol. 81, no. 2, pp. 121–148, Nov. 2010. [Online].
Available: http://dx.doi.org/10.1007/s10994-010-5188-5
[9] B. Biggio, B. Nelson, and P. Laskov, “Poisoning attacks against support vector
machines,” in Proceedings of International Conference on Machine Learning, ser.
ICML, 2012.
[10] C. E. Brodley and M. A. Friedl, “Identifying mislabeled training data,” Journal of
Artificial Intelligence Research, vol. 11, pp. 131–167, 1999.
[11] M. Brückner, C. Kanzow, and T. Scheffer, “Static prediction games for adversarial
learning problems,” J. Mach. Learn. Res., vol. 13, no. 1, pp. 2617–2654, Sep.
2012.
[12] J. A. Calandrino, A. Kilzer, A. Narayanan, E. W. Felten, and V. Shmatikov, “You
might also like: Privacy risks of collaborative filtering,” in Proceedings of 20th
IEEE Symposium on Security and Privacy, May 2011.
[13] Y. Cao, X. Pan, Y. Chen, and J. Zhuge, “JShield: Towards real-time and
vulnerability-based detection of polluted drive-by download attacks,” in Pro-
ceedings of the 30th Annual Computer Security Applications Conference, ser.
ACSAC, 2014.
[14] Y. Cao and J. Yang, “Towards making systems forget with machine unlearning,”
in Proceedings of the 2015 IEEE Symposium on Security and Privacy, 2015.
[15] G. Cauwenberghs and T. Poggio, “Incremental and decremental support vec-
tor machine learning,” in Advances in Neural Information Processing Systems
(NIPS*2000), vol. 13, 2001.
[16] J. Cheng and R. Greiner, “Learning bayesian belief network classifiers: Algorithms
and system,” in Proceedings of the 14th Biennial conference, 2001, pp. 141–151.
[17] G. F. Cretu, A. Stavrou, M. E. Locasto, S. J. Stolfo, and A. D. Keromytis, “Casting
out Demons: Sanitizing Training Data for Anomaly Sensors,” in Proceedings of
the 2008 IEEE Symposium on Security and Privacy, ser. SP, 2008.
[18] C. Curtsinger, B. Livshits, B. Zorn, and C. Seifert, “Zozzle: Fast and precise
in-browser javascript malware detection,” in Proceedings of the 20th USENIX
Conference on Security, 2011.
[19] O. Dekel, O. Shamir, and L. Xiao, “Learning to classify with missing and cor-
rupted features,” Mach. Learn., vol. 81, no. 2, pp. 149–178, Nov. 2010.
[20] P. Domingos and G. Hulten, “Mining high-speed data streams,” in Proceedings of
the Sixth ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, ser. KDD, 2000.
[21] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin, “Liblinear: A
library for large linear classification,” The Journal of Machine Learning Research,
vol. 9, pp. 1871–1874, 2008.
[22] Ó. Fontenla-Romero, B. Guijarro-Berdiñas, D. Martinez-Rego, B. Pérez-Sánchez,
and D. Peteiro-Barral, “Online machine learning,” Efficiency and Scalability
Methods for Computational Intellect, pp. 27–54, 2013.
[23] M. Fredrikson, E. Lantz, S. Jha, S. Lin, D. Page, and T. Ristenpart, “Privacy in
pharmacogenetics: An end-to-end case study of personalized warfarin dosing,” in
Proceedings of USENIX Security, August 2014.
[24] G. Fumera and B. Biggio, “Security evaluation of pattern classifiers under attack,”
IEEE Transactions on Knowledge and Data Engineering, vol. 99, no. 1, 2013.
[25] D. J. Hsu, “Algorithms for active learning,” Ph.D. dissertation, University of
California, San Diego, 2010.
[26] L. Huang, A. D. Joseph, B. Nelson, B. I. Rubinstein, and J. D. Tygar, “Adversarial
machine learning,” in Proceedings of the 4th ACM Workshop on Security and
Artificial Intelligence, ser. AISec, 2011.
[27] J. Juneau, J. Baker, F. Wierzbicki, L. Soto, and V. Ng, The Definitive Guide to
Jython: Python for the Java Platform, 1st ed. Berkely, CA, USA: Apress, 2010.
[28] M. Kearns, “Efficient noise-tolerant learning from statistical queries,” J. ACM,
vol. 45, no. 6, pp. 983–1006, Nov. 1998.
[29] M. Kearns and M. Li, “Learning in the presence of malicious errors,” in Pro-
ceedings of the Twentieth Annual ACM Symposium on Theory of Computing, ser.
STOC, 1988.
[30] P. W. Koh and P. Liang, “Understanding black-box predictions via influence
functions,” ICML, 2017.
[31] S. Krishnan, M. J. Franklin, K. Goldberg, and E. Wu, “Boostclean: Automated
error detection and repair for machine learning,” CoRR, vol. abs/1711.01299,
2017. [Online]. Available: http://arxiv.org/abs/1711.01299
[32] C. Monteleoni, “Learning with online constraints: Shifting concepts and active
learning,” Ph.D. dissertation, Massachusetts Institute of Technology, 2006.
[33] B. Nelson, M. Barreno, F. J. Chi, A. D. Joseph, B. I. P. Rubinstein, U. Saini,
C. Sutton, J. D. Tygar, and K. Xia, “Exploiting machine learning to subvert your
spam filter,” in Proceedings of the 1st Usenix Workshop on Large-Scale Exploits
and Emergent Threats, ser. LEET, 2008.
[34] J. Newsome, B. Karp, and D. Song, “Polygraph: Automatically generating signa-
tures for polymorphic worms,” in Proceedings of the 2005 IEEE Symposium on
Security and Privacy, 2005.
[35] R. Perdisci, D. Dagon, W. Lee, P. Fogla, and M. I. Sharif, “Misleadingworm
signature generators using deliberate noise injection,” in Proceedings of the 2006
IEEE Symposium on Security and Privacy, 2006.
[36] S. Perez. Microsoft
users
ter
microsoft-silences-its-new-a-i-bot-tay-after-twitter-users-teach-it-racism/.
teach
silences
it
its
racism.
new a.i.
twit-
http://techcrunch.com/2016/03/24/
after
tay,
bot
[37] E. Romero, I. Barrio, and L. Belanche, “Incremental and decremental learning
for linear support vector machines,” in Proceedings of the 17th International
Conference on Artificial Neural Networks, ser. ICANN, 2007.
[38] B. Settles, “Active learning literature survey,” University of Wisconsin–Madison,
Computer Sciences Technical Report 1648, 2009.
[39] S. Shen, S. Tople, and P. Saxena, “Auror: defending against poisoning attacks
in collaborative deep learning systems,” in Proceedings of the 32nd Annual
Conference on Computer Security Applications. ACM, 2016, pp. 508–519.
[40] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov,
“Dropout: A simple way to prevent neural networks from overfitting,” J. Mach.
Learn. Res., vol. 15, no. 1, pp. 1929–1958, Jan. 2014.
[41] V. M. Telecommunications and V. Metsis, “Spam filtering with naive bayes –
which naive bayes?” in Third Conference on Email and Anti-Spam (CEAS), 2006.
[42] C.-H. Tsai, C.-Y. Lin, and C.-J. Lin, “Incremental and decremental training for
linear classification,” in Proceedings of the 20th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, ser. KDD, 2014.
[43] P. E. Utgoff, “Incremental induction of decision trees,” Mach. Learn., vol. 4,
no. 2, pp. 161–186, Nov. 1989. [Online]. Available: http://dx.doi.org/10.1023/A:
1022699900025
[44] N. Šrndic and P. Laskov, “Practical evasion of a learning-based classifier: A case
study,” in Proceedings of the 2014 IEEE Symposium on Security and Privacy,
2014.
[45] G. Wang, T. Wang, H. Zheng, and B. Y. Zhao, “Man vs. machine: Practical
adversarial detection of malicious crowdsourcing workers,” in Proceedings of
USENIX Security, August 2014.
Causal Unlearning
ASIA CCS ’18, June 4–8, 2018, Incheon, Republic of Korea
Appendices
A ANOTHER DIVERGENCE SCORE
In the appendix, we define another divergence score between a
cluster and a sample called D2. To calculate D2, we need to main-
tain two lists: (1) all the features in the cluster, ,
and (2) the number of occurrence of each features in the cluster,
. Then, we still obtain the common feature list
between the cluster and the sample, . This di-
vergence score between the cluster and the sample is defined in
Equation 3.
1
j2 (
1
N k1 + 1 +
1
1
)
N kj + 1
N k2 + 1 + ... +
(3)
If the cluster only contains one sample, i.e., we want to compute
the divergence score between two samples, Equation 3 boils down
to 1
2j , half of the divergence score defined in D1. Because diver-
gence score is a relative value, the definitions in D1 and D2 of the
score between two samples are consistent. Note that what the new
divergence score definition introduces is the concept of frequency.
When a feature occurs more in the cluster, the contribution to the
divergence score between the cluster and a sample with the feature
is smaller, as one divided the frequency of the feature plus one is
smaller. At contrast, when a feature occurs less, the contribution to
the score is larger. So, during clustering, the active unlearning algo-
rithm tends to include samples with more high frequency features
and less low frequency ones in the current cluster.
B EVALUATION ON A BAYES-BASED
JAVASCRIPT MALWARE DETECTOR
In this section, we integrate KARMA with Zozzle [18], a JavaScript
malware detection engine using Naïve Bayes. The purpose of the
experiment is to show that KARMA works with not only spam de-
tectors but also malware detectors. Because Zozzle is closed-source,
we reimplement a Java version by following their paper, and ob-
tain an implementation from Cao et al. [14] where they implement
machine unlearning and evaluate the effectiveness. Their Zozzle
implementation is based on Java, and we use Jython [27] to integrate
our Python implementation of KARMA with their Zozzle. Together
with their source code, we also obtain their The dataset that we
use contain 142,350 real-world JavaScript malware samples from
Huawei, JavaScript from top 10,000 Alexa web sites, and 15,520
polluted JavaScript. All other setups are similar to the setup of our
previous experiment, we divide unpolluted samples into 10 equal
parts: nine parts plus the polluted samples for training, and the rest
equally divided for the oracle and the testing dataset.
The result shows that KARMA can successfully identify 98.9%
polluted JavaScripts and restore the detection accuracy against the
testing dataset to the vanilla value with less than 0.9% difference.