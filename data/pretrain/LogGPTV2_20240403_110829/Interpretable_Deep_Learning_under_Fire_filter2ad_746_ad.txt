To overcome this, when performing back-propagation, we
smooth the gradient of ReLU, denoted by r(z), with a function
h(z) deﬁned as (τ is a small constant, e.g., 10−4):
(cid:40)
(z +√z2 + τ)(cid:48) = 1 + z/√z2 + τ
(√z2 + τ)(cid:48) = z/√z2 + τ
h(z) (cid:44)
(z < 0)
(z ≥ 0)
Intuitively, h(z) tightly approximates r(z), while its gradi-
ent is non-zero everywhere. Another possibility is the sig-
moid function σ(z) = 1/(1 + e−z). Figure 3 compares dif-
ferent functions near z = 0. Our evaluation shows that h(z)
signiﬁcantly outperforms σ(z) and r(z) in attacking GRAD.
This attack is extensible to other back-propagation-based
interpreters (e.g., DEEPLIFT [48], SMOOTHGRAD [51], and
LRP [6]), due to their fundamentally equivalent, gradient-
centric formulations [3].
3.3 Representation-Guided Interpretation
This class of interpreters leverage the feature maps at in-
termediate layers of DNNs to generate attribution maps. We
consider class activation mapping (CAM) [64] as a represen-
tative interpreter of this class.
At a high level, CAM performs global average pooling [30]
over the feature maps of the last convolutional layer, and
uses the outputs as features for a linear layer with softmax
activation to approximate the model predictions. Based on
this connectivity structure, CAM computes the attribution
maps by projecting the weights of the linear layer back to the
convolutional feature maps.
Formally, let ak[i, j] denote the activation of the k-th chan-
nel of the last convolutional layer at the spatial position
(i, j). The output of global average pooling is deﬁned as
Ak = ∑i, j ak[i, j]. Further let wk,c be the weight of the con-
nection between the k-th input and the c-th output of the
linear layer. The input to the softmax function for a class c
with respect to a given input x is approximated by:
zc(x) ≈ ∑
k
wk,c Ak = ∑
i, j
∑
k
wk,c ak[i, j]
The class activation map mc is then given by:
mc[i, j] = ∑
k
wk,c ak[i, j]
(6)
(7)
Due to its use of deep representations at intermediate layers,
CAM generates attribution maps of high visual quality and
limited noise and artifacts [30].
1662    29th USENIX Security Symposium
USENIX Association
1-0.100.13.5 Perturbation-Guided Interpretation
The fourth class of interpreters formulate ﬁnding the attri-
bution map by perturbing the input with minimum noise and
observing the change in the model prediction. We consider
MASK [16] as a representative interpreter in this class.
For a given input x, MASK identiﬁes its most informative
parts by checking whether changing such parts inﬂuences the
prediction f (x). It learns a mask m, where m[i] = 0 if the i-th
input feature is retained and m[i] = 1 if the feature is replaced
with Gaussian noise. The optimal mask is found by solving
an optimization problem:
min
m
fc(φ(x;m)) + λ(cid:107)1− m(cid:107)1
s.t. 0 ≤ m ≤ 1
(9)
where c denotes the current prediction c = f (x) and φ(x;m)
is the perturbation operator which blends x with Gaussian
noise. The ﬁrst term ﬁnds m that causes the probability of c
to decrease signiﬁcantly, while the second term encourages m
to be sparse. Intuitively, solving Eqn (9) amounts to ﬁnding
the most informative and necessary parts of x with respect to
its prediction f (x). Note that this formulation may result in
signiﬁcant artifacts in m. A more reﬁned formulation is given
in [16].
Unlike other classes of interpreters, to attack MASK, it is
infeasible to directly optimize Eqn (3) with iterative gradient
descent (Eqn (4)), because the interpreter g itself is formulated
as an optimization procedure.
Instead, we reformulate ADV2 using a bilevel optimiza-
tion framework. For given x◦, ct, mt, f , and g, we re-deﬁne
the adversarial loss function as (cid:96)adv(x,m) (cid:44) (cid:96)prd( f (x),ct ) +
λ(cid:96)int(m,mt ) by introducing m as an additional variable. Let
(cid:96)map(m;x) be the objective function deﬁned in Eqn (9). Note
that m∗(x) = argminm (cid:96)map(m;x) is the attribution map found
by MASK for a given input x. We then have the following
attack framework:
x
(cid:96)adv (x, m∗(x))
min
s.t. m∗(x) = argmin
m
(cid:96)map(m;x)
(10)
Still, solving the bilevel optimization in Eqn (10) exactly
is challenging, as it requires recomputing m∗(x) by solving
the inner optimization problem whenever x is updated. We
propose an approximate iterative procedure which optimizes
x and m by alternating between gradient descent on (cid:96)adv and
(cid:96)map respectively.
More speciﬁcally, at the i-th iteration, given the current
input x(i−1), we compute its attribution map m(i) by updating
m(i−1) with gradient descent on (cid:96)map
m(i) and obtain x(i) by minimizing (cid:96)adv after a single step of
gradient descent with respect to m(i). Formally, we deﬁne the
objective function for updating x(i) as:
(cid:0)m(i−1);x(i−1)(cid:1); we then ﬁx
(cid:0)m(i);x(i−1)(cid:1)(cid:17)
(cid:16)
x(i−1), m(i) − ξ∇m(cid:96)map
(cid:96)adv
where ξ is the learning rate for this virtual gradient descent.
The rationale behind this procedure is as follows. While it
is difﬁcult to directly minimizing (cid:96)adv (x,m∗(x)) with respect
to x, we use a single-step unrolled map as a surrogate of m∗(x).
A similar approach is used in [15]. Essentially, this iterative
optimization deﬁnes a Stackelberg game [46] between the
optimizer for x (leader) and the optimizer for m (follower),
which requires the leader to anticipate the follower’s next