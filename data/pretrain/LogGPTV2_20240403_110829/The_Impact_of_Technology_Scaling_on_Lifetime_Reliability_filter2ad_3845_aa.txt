title:The Impact of Technology Scaling on Lifetime Reliability
author:Jayanth Srinivasan and
Sarita V. Adve and
Pradip Bose and
Jude A. Rivers
The Impact of Technology Scaling on Lifetime Reliability 
Jayanth Srinivasan(cid:0), Sarita V. Adve(cid:0), Pradip Bose(cid:1), Jude A. Rivers(cid:1)
(cid:0)Department of Computer Science, University of Illinois, Urbana-Champaign
(cid:1)IBM T.J. Watson Research Center, Yorktown Heights, NY
(cid:0)srinivsn,PI:EMAIL(cid:1),(cid:0)pbose,PI:EMAIL(cid:1)
Abstract
The relentless scaling of CMOS technology has provided
a steady increase in processor performance for the past
three decades. However, increased power densities (hence
temperatures) and other scaling effects have an adverse im-
pact on long-term processor lifetime reliability. This paper
represents a ﬁrst attempt at quantifying the impact of scal-
ing on lifetime reliability due to intrinsic hard errors, taking
workload characteristics into consideration.
For our quantitative evaluation, we use RAMP [15], a
previously proposed industrial-strength model that provides
reliability estimates for a workload, but for a given technol-
ogy. We extend RAMP by adding scaling speciﬁc parame-
ters to enable workload-dependent lifetime reliability eval-
uation at different technologies.
We show that (1) scaling has a signiﬁcant impact on pro-
cessor hard failure rates – on average, with SPEC bench-
marks, we ﬁnd the failure rate of a scaled 65nm processor
to be 316% higher than a similarly pipelined 180nm pro-
cessor; (2) time-dependent dielectric breakdown and elec-
tromigration have the largest increases; and (3) with scal-
ing, the difference in reliability from running at worst-case
vs. typical workload operating conditions increases signif-
icantly, as does the difference from running different work-
loads. Our results imply that leveraging a single microar-
chitecture design for multiple remaps across a few technol-
ogy generations will become increasingly difﬁcult, and mo-
tivate a need for workload speciﬁc, microarchitectural life-
time reliability awareness at an early design stage.
1 Introduction
Advances in CMOS semiconductor technology have
been steadily improving processor performance. These ad-
This work is supported in part by an equipment donation from
AMD Corp. and the National Science Foundation under Grant No.
CCR-0209198, CCR-0205638, EIA-0224453, and CCR-0313286. A large
part of the work was performed while Jayanth Srinivasan was a summer
intern at IBM T. J. Watson Research Center.
vances have been driven by aggressive scaling of device fea-
ture sizes. However, CMOS scaling is accelerating the onset
of problems due to long-term processor hardware failures or
lifetime reliability. This paper represents a ﬁrst attempt at
quantifying the impact of scaling on lifetime reliability of an
entire processor, considering the behavior of the workload
running on the processor. Our work focuses on intrinsic
hard failures, and examines failures due to electromigra-
tion (EM), stress migration (SM), time-dependent dielectric
(gate oxide) breakdown (TDDB), and thermal cycling (TC).
We do not model extrinsic hard failures and soft errors be-
cause they generally do not impact lifetime reliability [15].
1.1 Scaling theory and practice
Device scaling results in the reduction of feature sizes
and voltage levels of transistors. Under ideal scaling, gate
delay decreases by 30% from one generation to the next,
transistor density doubles, and dynamic power per tran-
sistor decreases by about 50% (assuming constant electric
ﬁeld scaling where voltage scales down by 30%) [3]. The
net impact is that for the same die size, under ideal scal-
ing, the chip dynamic power and power density remain un-
changed. With real scaling in the deep sub-micron range,
however, processor power density, and consequently tem-
perature, have been increasing at an alarming rate, which
directly affects processor lifetime reliability. The main rea-
sons behind this increase are:
Supply voltages are not scaling ideally. This prevents the
dynamic power per transistor from decreasing at the ideal
rate. One reason for the slowing down of supply volt-
age scaling is the attempt to retain competitive frequency
growth by tuning up the voltage to the maximum levels al-
lowed in a given technology generation. Second, as the gap
between the threshold voltage and the supply voltage di-
minishes to less than a volt, basic noise immunity issues (in
logic) and cell state stability issues (in SRAM macros) make
it ever harder to scale down the supply voltage. Hence, area
scaling without appropriate power scaling results in higher
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:29:22 UTC from IEEE Xplore.  Restrictions apply. 
power densities.
Total chip leakage power is increasing. Scaling down
threshold voltages ideally causes the leakage current per
transistor to increase by ﬁve times per technology gener-
ation. This increase is further compounded by the exponen-
tial dependence of leakage power on temperature.
1.2
Impact of non-ideal scaling
The above non-ideal scaling coupled with the reduced
feature sizes affects processor lifetime reliability in the fol-
lowing ways. First, all of the four failure mechanisms con-
sidered here are adversely affected by increases in temper-
ature, with some of these mechanisms exhibiting an expo-
nential or larger dependence on temperature. Second, the
dielectric thickness of devices is fast decreasing to the point
where it is approaching a few angstroms. This, coupled with
the fact that there has been a general slowdown in supply
voltage scaling is expected to increase the intrinsic failure
rate due to gate oxide breakdown (TDDB). Third, the de-
creasing feature size of interconnects accelerates electromi-
gration failure rates.
The detrimental impact of scaling on intrinsic reliability
in general, and gate oxide reliability in particular, has been
studied extensively [8, 10, 17]. However, most of these
studies have been performed at the device level, and con-
sider individual failure mechanisms in isolation. Addition-
ally, they are performed at ﬁxed worst case operating points
without any knowledge of the target application suite of the
processor. However, since the power consumed by the pro-
cessor varies with the executing workload, the actual op-
erating temperature and interconnect current densities also
depend on the workload. Consequently, the failure rate of
a component (or the processor as a whole) depends on the
target workload. Thus, an application oblivious analysis of
processor reliability would produce unrepresentative relia-
bility data.
In recent work, we have proposed an industrial strength,
microarchitecture level model and simulation methodology,
called RAMP, to evaluate processor lifetime reliability for a
workload, but for a given technology [15]. That work uses
the workload dependence of lifetime reliability to motivate
microarchitecture level mechanisms to address the growing
lifetime reliability problem.
1.3 Our contributions
To the best of our knowledge, this paper represents the
ﬁrst quantitative evaluation of the impact of device scaling
on the hard error rates and lifetime reliability of proces-
sors, from a micro-architectural perspective and incorpo-
rating workload dependence. We enhance the RAMP re-
liability model by adding scaling speciﬁc parameters to en-
able lifetime reliability evaluation at different technologies.
In particular, our evaluation and analysis attempt to model
the scaling effects of taking one chip design, and gradually
scaling that chip down from 180nm to 65nm, without any
substantial modiﬁcations to the microarchitectural pipeline.
Our ﬁrst set of results show that scaling has a signiﬁcant
and increasing impact on processor hard failure rates. The
increase in processor temperature is one of the key reasons
for this trend. In our experiments, on average, the maxi-
mum temperature reached by a 65nm processor is 15 de-
grees Kelvin higher than that reached by a 180nm proces-
sor. The failure rate for a 65nm processor is 316% higher
than the failure rate at 180nm, with similar reliability qual-
iﬁcation. More importantly, the rate of increase of failure
rate increases as we scale to smaller technologies. Compar-
ing the different failure mechanisms, we ﬁnd that gate oxide
breakdown (TDDB) will provide the largest challenge fol-
lowed by electromigration. The effect of scaling on stress
migration and thermal cycling are much less drastic. Our
results clearly demonstrate that hard failures will present
a signiﬁcant and increasing challenge in future technology
generations. An important practical consequence is that, in
contrast to current practice, leveraging a single design for
multiple remaps across a few technology generations (with
only minor design tweaks) will become increasingly difﬁ-
cult.
Our second set of results quantify the impact of scal-
ing on the workload-dependent nature of lifetime reliabil-
ity. Our results show that failure rates computed by assum-
ing worst-case operating conditions are increasingly pes-
simistic compared to those computed with real workloads,
as we scale to smaller technologies. Furthermore, scaling
ampliﬁes the difference in failure rates between different
applications. Speciﬁcally, we computed the worst-case fail-
ure rate assuming steady state operation based on the high-
est temperature and activity factors reached by any of our
applications. The difference between this worst-case failure
rate and the highest actual failure rate seen by any single
application went from 25% for 180nm to 90% for 65nm
(computed as a percentage of the worst-case failure rate).
The difference between worst-case and average failure rate
for our applications was even more striking – 67% at 180nm
to 206% at 65nm. Thus, at technologies with smaller fea-
ture sizes, reliability qualiﬁcation for worst-case operat-
ing conditions will result in signiﬁcantly and increasingly
over-designed processors. A promising approach, proposed
in [15], is to perform reliability qualiﬁcation for the ex-
pected case, backed up with dynamic application-speciﬁc
responses for handling departures from the expected case.
2 Background
As mentioned in Section 1, we use a model and sim-
ulation methodology called RAMP, described in [15], to
calculate lifetime reliability of processors from a mi-
croarchitectural viewpoint. RAMP represents the ﬁrst
microarchitecture-level methodology for evaluating proces-
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:29:22 UTC from IEEE Xplore.  Restrictions apply. 
sor lifetime reliability, and uses state-of-the-art analytic
models for important intrinsic failure mechanisms. Its de-
sign and implementation are discussed in detail in [15].
It currently models four main intrinsic failure mechanisms
experienced by processors – electromigration (EM), stress
migration (SM), gate-oxide or time dependent dielectric
breakdown (TDDB), and thermal cycling (TC) [1, 2].
It
implements the failure models at a microarchitectural struc-
ture level (e.g., caches, ALUs, instruction window, etc.), for
a given technology generation. The standard reliability met-
ric used in the analytical models in RAMP is MTTF (mean
time to failure), which is the average expected lifetime of
the processor.
RAMP should be used in conjunction with a timing sim-
ulator to determine workload behavior, and a power and
thermal simulator for power and temperature proﬁles. This
is dicussed further in Section 4.
Next, we review the individual (structure level) failure
models in RAMP, assuming steady state operation at a ﬁxed
operating point. We then review how RAMP combines the
different failure models, across all chip structures, while ac-
counting for temporal variations within an application.
Electromigration. This failure mechanism is well under-
stood, and extensive research has been performed by the
material science and semiconductor community on model-
ing and understanding its effects [1, 8]. Electromigration
in processor interconnects is due to the mass transport of
conductor metal atoms in the interconnects. Sites of metal
atom depletion can lead to increased resistance and open
circuits. At the site of metal atom pile up, extrusions can
form causing shorts between adjacent metal lines.
The model for the MTTF due to electromigration [1, 8],
 (cid:1) (cid:1) (cid:2)(cid:0) , used in RAMP [15] is:
 (cid:1) (cid:1) (cid:2)(cid:0) (cid:0)  (cid:4)
(cid:0)(cid:1)(cid:0)
(cid:3)(cid:4)
(1)
where  is the current density in the interconnect, (cid:5)(cid:3)(cid:0) is
the activation energy for electromigration, (cid:6) is Boltzmann’s
constant, and (cid:1) is absolute temperature in Kelvin.  and
(cid:5)(cid:3)(cid:0) are constants that depend on the interconnect metal
used (1.1 and 0.9 respectively for the copper interconnect
modeled in RAMP [15]).
RAMP models MTTF at the granularity of a microarchi-
tectural structure. The value of  for a structure is equal
to the product of the activity factor of the structure, , and
the maximum allowed interconnect current density for that
technology generation. The value of  for a structure is ob-
tained from the timing simulator.
Stress migration. This is a phenomenon where the metal
atoms in the interconnects migrate due to mechanical stress,
much like electromigration. Stress migration is caused by
thermo-mechanical stresses which are caused by differing
thermal expansion rates of different materials [1].
The model for the MTTF due to stress migration [1],
 (cid:1) (cid:1) (cid:2)(cid:4) , used in RAMP [15] is:
(cid:0)(cid:1)(cid:5)
(cid:3)(cid:4)
 (cid:1) (cid:1) (cid:2)(cid:4) (cid:0) (cid:1)(cid:1)(cid:0)   (cid:1) (cid:1) (cid:4)
(2)
where (cid:1) is the absolute temperature in Kelvin, (cid:1)(cid:0) is the
stress free temperature of the metal (the metal deposition
temperature), and  and (cid:5)(cid:3)(cid:5) are material dependent con-
stants (2.5 and 0.9 respectively for the copper interconnects
modeled in RAMP [15]). RAMP assumes that sputtering
(versus vapor deposition) was used to deposit the intercon-
nect metal and uses a value of 500K for (cid:1)(cid:0) [6].
Time-dependent dielectric breakdown. Also known as
gate oxide breakdown, this is another well studied failure
mechanism in semiconductor devices. The gate oxide (or
dielectric) wears down with time, and fails when a conduc-
tive path forms in the dielectric [10, 17]. The model for the
MTTF due to TDDB used in RAMP [15] is based on recent
experimental work performed by Wu et at. at IBM [17]:
(cid:2)
(cid:10)
(cid:6) (cid:7)
(cid:4) (cid:8)(cid:4) 
(cid:3)(cid:4)
 (cid:1) (cid:1) (cid:2)(cid:6) (cid:7)(cid:7)(cid:8) (cid:0) 
(3)
where (cid:1) is the absolute temperature in Kelvin, (cid:11)(cid:12) (cid:13)(cid:12) (cid:14)(cid:12) (cid:15) ,
and (cid:16) are ﬁtting parameters, and (cid:10) is the voltage 1.
(cid:3) (cid:9)(cid:6) (cid:4)
Based on the experimental data collected by Wu et
al. [17], the values used in RAMP for the TDDB model are
(cid:11) (cid:3) (cid:4)(cid:5), (cid:13) (cid:3)  (cid:6)(cid:17)(cid:6)(cid:5)(cid:2), (cid:14) (cid:3) (cid:6)(cid:17)(cid:4)(cid:7)(cid:8)(cid:4)(cid:18), (cid:15) (cid:3)  (cid:9)(cid:9)(cid:17)(cid:5)(cid:4)(cid:18), and
(cid:16) (cid:3)  (cid:5)(cid:17)(cid:10)(cid:4)(cid:4)   (cid:11)(cid:4)(cid:18)(cid:20).
Thermal cycling. Permanent damage accumulates every
time there is a cycle in temperature in the processor, even-
tually leading to failure. Fatigue due to thermal cycling is
most pronounced in the package and die interface (e.g., at
solder joints) [1]. The package goes through two types of
thermal cycles – large cycles which occur at a low frequency
(due to powering up and down), and small cycles which oc-
cur at a much higher frequency (due to variations in appli-
cation behavior). The effect of small thermal cycles has
not been well studied and validated models are not avail-
able. The model for the MTTF due to large thermal cycles
is based on the Cofﬁn-Manson equation [1] and is:
 (cid:1) (cid:1) (cid:2)(cid:6) (cid:10) (cid:0) 
(cid:2)
(cid:1)(cid:3)(cid:11)(cid:12)(cid:3)(cid:14)(cid:12)   (cid:1)(cid:3)(cid:9)(cid:15)(cid:12)
