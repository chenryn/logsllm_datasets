[0.89, 1.25]
p-value
OR/CI
p-value
0.10
1.64
[1.01, 2.67]
< 0.01**
0.02*
0.03*
< 0.01**
0.01**
0.06
0.54
2.15
[1.34, 3.5]
1.76
[1.09, 2.87]
1.38
[0.84, 2.28]
2.40
[1.50, 3.88]
1.70
[1.04, 2.79]
1.23
[0.96, 1.57]
1.07
[0.93, 1.23]
0.05*
< 0.01**
0.02*
0.20
< 0.01***
0.03*
0.10
0.36
OR/CI
1.68
[0.99, 2.88]
2.22
[1.33, 3.77]
1.69
[1, 2.9]
1.19
[0.68, 2.09]
2.27
[1.37, 3.84]
1.90
[1.12, 3.25]
1.14
[0.88, 1.49]
1.01
[0.87, 1.18]
p-value
0.06
< 0.01**
0.05
0.55
< 0.01**
0.02*
0.31
0.85
0.10
0.13
0.11
0.10
0.15
0.03*
0.78
Table 4: Effect of DP descriptions on respondentâ€™s perception of the likelihood that their information will be disclosed through
a particular information flow. All models are logistic regressions constructed using data from Survey Two. See Table 2 for
detailed legend.
that briefly summarize the methods used to create differentially
private summary statistics, usually focusing on statistical noise;
(Enables) statements that attempt to capture the types of applica-
tions that DP makes possible; (Trust) descriptions that focus on
the well known organizations and companies that have recently
started using DP; (Risk) statements that highlight the data-privacy
risks that an individual incurs when allowing their information to
be part of a differentially private system; and (Technical) highly
technical explanations using dense, mathematical language.
Many of the descriptions we gathered touch on more than one
of these main themes. For instance, documents prepared by the
U.S. Census Bureau state, â€œDifferential privacy allows us to inject
a precisely calibrated amount of noise into the data to control the
privacy risk of any calculation or statisticâ€ [31]. This description
touches on the techniques theme and the risk theme, while also
using technical language like â€œcalculation or statisticâ€ that may be
unnatural to non-experts. The New York Times provides a descrip-
tion that is another combination of the main themes, writing, â€œ[o]ne
example, differential privacy, is already used by Apple, Google and
even the U.S. Census Bureau to limit the amount of personal in-
formation that is shared with an organization while still allowing
it to make useful inferences from the dataâ€ [12] This description
contains elements of both the trust theme and the enables theme.
We note that most descriptions we gathered did nothing to dis-
tinguish between the central model and the local model. Indeed,
we found that determining if an industry system was in the local
or central model generally required looking at technical documen-
tation. The descriptions provided by media coverage also generally
did not include any indication as to the model of system being
described.
5.1 Methodology: Survey Two
After collecting and analyzing the descriptions of DP used in prac-
tice, we distilled six descriptions of DP that were representative
of the descriptions in each of these themes. We present these de-
scriptions in Table 3. Each of these descriptions is a synthetic cre-
ation meant to be representative of the real descriptions we found,
with the exception of the technical description, which was taken
from [17]. We chose to create new descriptions (rather than select-
ing a representative example) in order to free the description from
the surrounding context, make them consistent in their structure,
and have each description focus on only one theme. Just like the
descriptions we observed in-the-wild, our descriptions included no
indication as to whether the system they described was in the local
or central model.
Questionnaire. First, as in survey one, each respondent was ran-
domly presented one of the two scenarios. Each respondent was
then randomized to either a control condition, where there was no
mention of privacy protection, or shown one of the DP descriptions.
In the later conditions, the scenario was followed by the description,
"To reduce the intrusion into personal privacy, the [organization]
will use a technique called differential privacy. [differential privacy
description]," where the description presented was sampled with
equal weight from Table 3. Respondents were then asked the fol-
lowing questions: First, they were asked if they would be willing
to share their data. Next, each respondent was asked to share their
concrete privacy expectations by reporting whether they believed
the expectations described in Table 1 (e.g., â€œA criminal or foreign
government that hacks the transparency initiative could learn my
salary and job title.â€) were true or false. All questions included an
"I donâ€™t know" option. Finally, we included the same demographics
questions as above. The complete survey is in Appendix A.2.
Session 11C: Software Development and Analysis CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea3045Sample. We surveyed 1,208 Amazon Mechanical Turk workers
following the same screening requirements as in survey one, as
described in Section 4. Workers were split evenly between the
two survey scenarios and equally between the seven description
conditions (six descriptions of DP and the baseline).
Analysis. To answer RQ3, we construct six logistic regression mod-
els 5, one for each information disclosure event. The DV is whether
the respondent reported that they thought the given information
disclosure would occur, the IV of interest is the description they
were shown (a categorical variable with the control â€“ no descrip-
tion shown â€“ as the baseline), the control IVs are, as in Section 4,
whether the scenario was the salary scenario and the respondentâ€™s
internet skill. To answer RQ4, we construct a single logistic regres-
sion model. The DV is whether the respondent reported that they
would be willing to share their information, the IV of interest is the
description they were shown (coded as above), and the control IVs
are the same as above.
5.2 Results: Responses to Descriptions of DP
Here, we detail the results of our analyses of survey two.
RQ3: How Do Differential Privacy Descriptions Affect Pri-
vacy Expectations? We divide our results for RQ3 into two parts.
First, we detail our findings regarding the way descriptions of DP
increase respondentsâ€™ privacy expectations (see Table 4). Second,
we investigate if the descriptions correctly set respondentsâ€™ privacy
expectations, with respect to the ground truth privacy properties
of typical local and central DP deployments (see Figure 2).
Descriptions Increasing Expectations. Overall, we find that each de-
scription of DP that we tested increased respondentsâ€™ privacy ex-
pectations for at least one of the disclosure risks. However, different
descriptions increased different privacy expectations.
First, we found that none of the descriptions significantly changed
respondentsâ€™ expectations when it came to disclosing their informa-
tion to the organization soliciting their information itâ€™s represen-
tative. Respondents had higher privacy expectations in the salary
scenario than in the medical scenario, indicating that the slightly
different wording of these expectations may have had an effect
on respondent expectations. We verified that this wording did not
interfere with our main finding (that no descriptions increased
userâ€™s expectations of the Organization disclosure) by re-building
our models on each dataset separately; we found the same results.
Four of the descriptions do, however, influence respondentsâ€™
perceptions regarding whether their information could be disclosed
through a hack. Respondents who were shown the Unsubstantial,
Techniques, and Trust explanations were nearly two times more
likely (Unsubstantial: O.R. = 1.94, ð‘ = 0.01, Techniques: O.R. = 1.96,
ð‘ = 0.01, Trust: O.R. = 1.86, ð‘ = 0.02) to think their information
would not be disclosed through a hack. Those who were shown
the Risk description were even more likely (O.R. = 2.58, ð‘ < 0.01)
to think their information could not be disclosed through a hack.
Users may see preventing hacks as one of the key roles of security
and privacy technologies, as these results indicate that they expect
such protection from the gold standard techniques (Unsubstantial)
5We note that these models are not corrected for multiple testing in line with Benjamini
and Hochberg [6], which suggests such correction only for models with a large number
of DVs.
and those used by major companies (Trust). The Risk description
directly addresses this potential information disclosure, so it is
unsurprising that it raised privacy expectations regarding hacks.
Finally, respondents may have gathered that injecting statistical
noise (Techniques) would protect their information against hacks,
as it does in practice.
Only the Risk description significantly influenced respondentsâ€™
perceptions of whether their information would be disclosed to law
enforcement as the result of a court order: those who saw the Risk
description were nearly two times more likely (O.R. 1.86, ð‘ = 0.01) to
think their information would not be disclosed to law enforcement.
Interestingly, this indicates that users may see information sharing
with law enforcement as a risk, rather than an information flow
that is appropriate and necessary to protect society.
Revisiting our findings above, we note that local DP provides pro-
tection against all of the information disclosure risks about which
we asked. This is because under local DP the curator never has ac-
cess to the unperturbed data (and therefore cannot accidentally or
intentionally disclose information). As such, increased expectations
mean more correct expectations under local DP. As we saw above,
each of the descriptions does increase some â€” but not all â€” expec-
tations. This means that the descriptions are not only raising user
All of the descriptions aside from the Unsubstantial description
increase the likelihood that respondents think their information
would be secure against disclosures to a data analyst, while all but
the Trust description increase the likelihood that respondents think
their information would not be disclosed through graphs or charts
made using their information. It may be that the Unsubstantial
description did not raise respondentsâ€™ expectations for disclosure to
data analysts because users are unfamiliar with the notion that data
analysts could accomplish their job without seeing user information
â€” users may expect even â€œgold standardâ€ techniques to disclose
information in this way. Similarly, users may be unfamiliar with
the idea that tech companies create graphs and charts, as most of
these releases are not customer facing. Therefore, it would not be
assumed that such techniques could protect user information.
The Techniques, Risk, and Technical descriptions all increase
the likelihood that respondents think their information could not
be shared with another organization (Techniques: O.R. 2.22, ð‘ <
0.01, Risk: O.R. 2.27, ð‘ < 0.01, Technical: O.R. 1.90, ð‘ = 0.02). As
above, users may have gathered that the injection of statistical
noise described in the Techniques description would prevent this
information disclosure. Additionally, both the Risks and Technical
descriptions speak to the risk of joining the dataset. As indicated
in our results for RQ1, a large number of respondents care about
their information being disclosed to a third party. This may be a
primary â€œriskâ€ in their mind, which the descriptions suggest that
they would be secure against.
Descriptions Setting Expectations Correctly. Not all DP techniques
reduce the likelihood of all potential information disclosures. It is
critical that descriptions of DP are used to set usersâ€™ expectations
correctly, not only raise expectations. This is especially important
in DP; a potential user encountering a description of a DP may set
their expectations as though the system offers local DP, only to
discover later that their information was more vulnerable because
the deployment used central DP (see Table 1 for the ground truth
we consider under both local and central DP).
Session 11C: Software Development and Analysis CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea3046(a) Distribution of respondent correctness about information disclo-
sures under local DP. The x-axis shows percentage of disclosures for
which their expectations were correct under local DP.
(b) Distribution of respondent correctness about information disclo-
sures under central DP. The x-axis shows percentage of disclosures
for which their expectations were correct under central DP.
Figure 2: Influence of in-the-wild DP descriptions on respondent expectations for information disclosures by DP model.
Dashed lines are 95% confidence intervals of the distributionâ€™s mean. â€œI donâ€™t knowâ€ not included as correct.
expectations for differentially private systems, but setting those
expectations more accurately for local privacy.
In central DP, on the other hand, the curator has access to usersâ€™
raw information. In this model, the curator is responsible for in-
jecting statistical noise into any aggregations that are released for
public consumption. Because the curator has access to raw infor-
mation, a typical deployment would be able to disclose sensitive
information in all of the listed ways, with the exception of Graphs.
Thus, the descriptions that raise privacy expectations related to
Hack, Law Enforcement, Data Analyst, and Share disclosures are
actually misleading users in the central model.
Finally, we also consider the aggregate effect of the descriptions
we study on the accuracy of respondentsâ€™ privacy expectations (see
Figure 2). We find that respondentsâ€™ expectations of DP are more
in line with the central model than the local model (ie. they have
lower privacy expectations). Specifically, respondents had correctly
set expectations for less than half of the information disclosure
risks under local DP, while roughly half of their expectations were
set correctly for the central model. More importantly, we note that
usersâ€™ privacy expectations are poorly aligned with both local and
central DP. This indicates that users have no coherent mental model
of the data collection process, as many of the privacy expectations
about which we ask are equivalent from a technical perspective.
RQ4: How Do In-The-Wild Descriptions of Differential Pri-
vacy Affect Sharing? When analyzing the results of our second
survey, we find that respondents who were told that their informa-
tion would be protected by DP techniques, as described by one of
six different descriptions of those techniques, were no more likely
to report that they would share their information in either scenario
(Table 5). We also note that the descriptions did not decrease the
likelihood that respondents would be willing to share their infor-
mation. Respondents were, however, more likely (O.R. = 1.67, ð‘ <
0.01) to share their information in the salary scenario than in the
medical scenario, in line with prior work suggesting that medical
information is particularly sensitive [34].
Odds Ratio CI
Variable
Description: Unsubstantial
Description: Techniques
Description: Enables
Description: Trust
Description: Risk
Description: Technical
Salary Scenario
Internet Score
1.22
0.96
1.48
1.08
1.37
0.94
1.67
1.09
[0.79, 1.88]
[0.62, 1.47]
[0.96, 2.29]
[0.7, 1.67]
[0.89, 2.12]
[0.61, 1.45]
p-value
0.37
0.83
0.08
0.72
0.15
0.77
[1.32, 2.1] < 0.01***
[0.95, 1.25]
0.2
Table 5: Effect of DP descriptions on respondentsâ€™ likelihood of
being willing to share information. See Table 2 for detailed legend.
We note that this finding contradicts the findings of [69], who
found that DP increased respondentsâ€™ willingness to share high
sensitivity information. We note that (a) the ways in which we
describe DP and (b) the context in which we elicit responses are dif-
ferent. As discussed in Section 2, the descriptions used in this prior
work were significantly longer and not necessarily representative
of in-the-wild descriptions from which we derived our descriptions,
and the methodology of prior work also involved mechanisms to
ensure respondents correctly understood the privacy guarantees
detailed in the descriptions.
6 DISCUSSION