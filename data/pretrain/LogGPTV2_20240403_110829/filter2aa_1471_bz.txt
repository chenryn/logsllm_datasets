VDEVs’ channels. In this case, the VMBus driver creates an instance PDO
representing the new channel. The created device is protected through a
security descriptor that renders it accessible only from system and
administrative accounts. The VMBus standard device interface, which is
attached to the new PDO, maintains the association between the new VMBus
channel (through the LOCAL_OFFER data structure) and the device object.
After the PDO is created, the Pnp Manager is able to identify and load the
correct VSC driver through the VDEV type and instance GUIDs included in
the Offer Channel message. These interfaces become part of the new PDO
and are visible through the Device Manager. See the following experiment
for details. When the VSC driver is then loaded, it usually calls the
VmbEnableChannel API (exposed by KMCL, as discussed previously) to
“open” the channel and create the final ring buffer.
EXPERIMENT: Listing virtual devices (VDEVs)
exposed through VMBus
Each VMBus channel is identified through a type and instance
GUID. For channels belonging to VDEVs, the type and instance
GUID also identifies the exposed device. When the VMBus child
driver creates the instance PDOs, it includes the type and instance
GUID of the channel in multiple devices’ properties, like the
instance path, hardware ID, and compatible ID. This experiment
shows how to enumerate all the VDEVs built on the top of VMBus.
For this experiment, you should build and start a Windows 10
virtual machine through the Hyper-V Manager. When the virtual
machine is started and runs, open the Device Manager (by typing
its name in the Cortana search box, for example). In the Device
Manager applet, click the View menu, and select Device by
Connection. The VMBus bus driver is enumerated and started
through the ACPI enumerator, so you should expand the ACPI
x64-based PC root node and then the ACPI Module Device located
in the Microsoft ACPI-Compliant System child node, as shown in
the following figure:
By opening the ACPI Module Device, you should find another
node, called Microsoft Hyper-V Virtual Machine Bus, which
represents the root VMBus PDO. Under that node, the Device
Manager shows all the instance devices created by the VMBus
FDO after their relative VMBus channels have been offered from
the root partition.
Now right-click one of the Hyper-V devices, such as the
Microsoft Hyper-V Video device, and select Properties. For
showing the type and instance GUIDs of the VMBus channel
backing the virtual device, open the Details tab of the Properties
window. Three device properties include the channel’s type and
instance GUID (exposed in different formats): Device Instance
path, Hardware ID, and Compatible ID. Although the compatible
ID contains only the VMBus channel type GUID ({da0a7802-
e377-4aac-8e77-0558eb1073f8} in the figure), the hardware ID
and device instance path contain both the type and instance GUIDs.
Opening a VMBus channel and creating the ring
buffer
For correctly starting the interpartition communication and creating the ring
buffer, a channel must be opened. Usually VSCs, after having allocated the
client side of the channel (still through VmbChannel Allocate), call the
VmbChannelEnable API exported from the KMCL driver. As introduced in
the previous section, this API in the child partitions opens a VMBus channel,
which has already been offered by the root. The KMCL driver communicates
with the VMBus driver, obtains the channel parameters (like the channel’s
type, instance GUID, and used MMIO space), and creates a work item for the
received packets. It then allocates the ring buffer, which is shown in Figure 9-
26. The size of the ring buffer is usually specified by the VSC through a call
to the KMCL exported VmbClientChannelInitSetRingBufferPageCount API.
Figure 9-26 An example of a 16-page ring buffer allocated in the child
partition.
The ring buffer is allocated from the child VM’s non-paged pool and is
mapped through a memory descriptor list (MDL) using a technique called
double mapping. (MDLs are described in Chapter 5 of Part 1.) In this
technique, the allocated MDL describes a double number of the incoming (or
outgoing) buffer’s physical pages. The PFN array of the MDL is filled by
including the physical pages of the buffer twice: one time in the first half of
the array and one time in the second half. This creates a “ring buffer.”
For example, in Figure 9-26, the incoming and outgoing buffers are 16
pages (0x10) large. The outgoing buffer is mapped at address
0xFFFFCA803D8C0000. If the sender writes a 1-KB VMBus packet to a
position close to the end of the buffer, let’s say at offset 0x9FF00, the write
succeeds (no access violation exception is raised), but the data will be written
partially in the end of the buffer and partially in the beginning. In Figure 9-
26, only 256 (0x100) bytes are written at the end of the buffer, whereas the
remaining 768 (0x300) bytes are written in the start.
Both the incoming and outgoing buffers are surrounded by a control page.
The page is shared between the two endpoints and composes the VM ring
control block. This data structure is used to keep track of the position of the
last packet written in the ring buffer. It furthermore contains some bits to
control whether to send an interrupt when a packet needs to be delivered.
After the ring buffer has been created, the KMCL driver sends an IOCTL
to VMBus, requesting the creation of a GPA descriptor list (GPADL). A
GPADL is a data structure very similar to an MDL and is used for describing
a chunk of physical memory. Differently from an MDL, the GPADL contains
an array of guest physical addresses (GPAs, which are always expressed as
64-bit numbers, differently from the PFNs included in a MDL). The VMBus
driver sends different messages to the root partition for transferring the entire
GPADL describing both the incoming and outcoming ring buffers. (The
maximum size of a synthetic message is 240 bytes, as discussed earlier.) The
root partition reconstructs the entire GPADL and stores it in an internal list.
The GPADL is mapped in the root when the child VM sends the final Open
Channel message. The root VMBus driver parses the received GPADL and
maps it in its own physical address space by using services provided by the
VID driver (which maintains the list of memory block ranges that comprise
the VM physical address space).
At this stage the channel is ready: the child and the root partition can
communicate by simply reading or writing data to the ring buffer. When a
sender finishes writing its data, it calls the VmbChannelSend
SynchronousRequest API exposed by the KMCL driver. The API invokes
VMBus services to signal an event in the monitor page of the Xinterrupt
object associated with the channel (old versions of the VMBus protocol used
an interrupt page, which contained a bit corresponding to each channel),
Alternatively, VMBus can signal an event directly in the channel’s event
port, which depends only on the required latency.
Other than VSCs, other components use VMBus to implement higher-level
interfaces. Good examples are provided by the VMBus pipes, which are
implemented in two kernel mode libraries (Vmbuspipe.dll and
Vmbuspiper.dll) and rely on services exposed by the VMBus driver (through
IOCTLs). Hyper-V Sockets (also known as HvSockets) allow high-speed
interpartition communication using standard network interfaces (sockets). A
client connects an AF_HYPERV socket type to a target VM by specifying the
target VM’s GUID and a GUID of the Hyper-V socket’s service registration
(to use HvSockets, both endpoints must be registered in the
HKLM\SOFTWARE\Microsoft\Windows NT\CurrentVersion\
Virtualization\GuestCommunicationServices registry key) instead of the
target IP address and port. Hyper-V Sockets are implemented in multiple
drivers: HvSocket.sys is the transport driver, which exposes low-level
services used by the socket infrastructure; HvSocketControl.sys is the
provider control driver used to load the HvSocket provider in case the
VMBus interface is not present in the system; HvSocket.dll is a library that
exposes supplementary socket interfaces (tied to Hyper-V sockets) callable
from user mode applications. Describing the internal infrastructure of both
Hyper-V Sockets and VMBus pipes is outside the scope of this book, but
both are documented in Microsoft Docs.
Virtual hardware support
For properly run virtual machines, the virtualization stack needs to support
virtualized devices. Hyper-V supports different kinds of virtual devices,
which are implemented in multiple components of the virtualization stack.
I/O to and from virtual devices is orchestrated mainly in the root OS. I/O
includes storage, networking, keyboard, mouse, serial ports and GPU
(graphics processing unit). The virtualization stack exposes three kinds of
devices to the guest VMs:
■    Emulated devices, also known—in industry-standard form—as fully
virtualized devices
■    Synthetic devices, also known as paravirtualized devices
■    Hardware-accelerated devices, also known as direct-access devices
For performing I/O to physical devices, the processor usually reads and
writes data from input and output ports (I/O ports), which belong to a device.
The CPU can access I/O ports in two ways:
■    Through a separate I/O address space, which is distinct from the
physical memory address space and, on AMD64 platforms, consists
of 64 thousand individually addressable I/O ports. This method is old
and generally used for legacy devices.
■    Through memory mapped I/O. Devices that respond like memory
components can be accessed through the processor’s physical
memory address space. This means that the CPU accesses memory
through standard instructions: the underlying physical memory is
mapped to a device.
Figure 9-27 shows an example of an emulated device (the virtual IDE
controller used in Generation 1 VMs), which uses memory-mapped I/O for
transferring data to and from the virtual processor.
Figure 9-27 The virtual IDE controller, which uses emulated I/O to
perform data transfer.
In this model, every time the virtual processor reads or writes to the device
MMIO space or emits instructions to access the I/O ports, it causes a
VMEXIT to the hypervisor. The hypervisor calls the proper intercept routine,
which is dispatched to the VID driver. The VID driver builds a VID message
and enqueues it in an internal queue. The queue is drained by an internal
VMWP’s thread, which waits and dispatches the VP’s messages received
from the VID driver; this thread is called the message pump thread and
belongs to an internal thread pool initialized at VMWP creation time. The
VM Worker process identifies the physical address causing the VMEXIT,
which is associated with the proper virtual device (VDEV), and calls into one
of the VDEV callbacks (usually read or write callback). The VDEV code
uses the services provided by the instruction emulator to execute the faulting
instruction and properly emulate the virtual device (an IDE controller in the
example).
 Note
The full instructions emulator located in the VM Worker process is also
used for other different purposes, such as to speed up cases of intercept-
intensive code in a child partition. The emulator in this case allows the
execution context to stay in the Worker process between intercepts, as
VMEXITs have serious performance overhead. Older versions of the
hardware virtualization extensions prohibit executing real-mode code in a
virtual machine; for those cases, the virtualization stack was using the
emulator for executing real-mode code in a VM.
Paravirtualized devices
While emulated devices always produce VMEXITs and are quite slow,
Figure 9-28 shows an example of a synthetic or paravirtualized device: the
synthetic storage adapter. Synthetic devices know to run in a virtualized
environment; this reduces the complexity of the virtual device and allows it
to achieve higher performance. Some synthetic virtual devices exist only in
virtual form and don’t emulate any real physical hardware (an example is
synthetic RDP).
Figure 9-28 The storage controller paravirtualized device.
Paravirtualized devices generally require three main components:
■    A virtualization service provider (VSP) driver runs in the root
partition and exposes virtualization-specific interfaces to the guest
thanks to the services provided by VMBus (see the previous section
for details on VMBus).
■    A synthetic VDEV is mapped in the VM Worker process and usually
cooperates only in the start-up, teardown, save, and restore of the
virtual device. It is generally not used during the regular work of the
device. The synthetic VDEV initializes and allocates device-specific
resources (in the example, the SynthStor VDEV initializes the virtual
storage adapter), but most importantly allows the VSP to offer a
VMBus communication channel to the guest VSC. The channel will
be used for communication with the root and for signaling device-
specific notifications via the hypervisor.
■    A virtualization service consumer (VSC) driver runs in the child
partition, understands the virtualization-specific interfaces exposed by
the VSP, and reads/writes messages and notifications from the shared
memory exposed through VMBus by the VSP. This allows the virtual
device to run in the child VM faster than an emulated device.
Hardware-accelerated devices
On server SKUs, hardware-accelerated devices (also known as direct-access
devices) allow physical devices to be remapped in the guest partition, thanks
to the services exposed by the VPCI infrastructure. When a physical device
supports technologies like single-root input/output virtualization (SR IOV) or
Discrete Device Assignment (DDA), it can be mapped to a guest partition.
The guest partition can directly access the MMIO space associated with the
device and can perform DMA to and from the guest memory directly without
any interception by the hypervisor. The IOMMU provides the needed
security and ensures that the device can initiate DMA transfers only in the
physical memory that belong to the virtual machine.
Figure 9-29 shows the components responsible in managing the hardware-
accelerated devices:
■    The VPci VDEV (Vpcievdev.dll) runs in the VM Worker process. Its
rule is to extract the list of hardware-accelerated devices from the VM
configuration file, set up the VPCI virtual bus, and assign a device to
the VSP.
■    The PCI Proxy driver (Pcip.sys) is responsible for dismounting and
mounting a DDA-compatible physical device from the root partition.
Furthermore, it has the key role in obtaining the list of resources used
by the device (through the SR-IOV protocol) like the MMIO space
and interrupts. The proxy driver provides access to the physical
configuration space of the device and renders an “unmounted” device
inaccessible to the host OS.
■    The VPCI virtual service provider (Vpcivsp.sys) creates and
maintains the virtual bus object, which is associated to one or more
hardware-accelerated devices (which in the VPCI VSP are called
virtual devices). The virtual devices are exposed to the guest VM
through a VMBus channel created by the VSP and offered to the VSC
in the guest partition.
■    The VPCI virtual service client (Vpci.sys) is a WDF bus driver that
runs in the guest VM. It connects to the VMBus channel exposed by
the VSP, receives the list of the direct access devices exposed to the
VM and their resources, and creates a PDO (physical device object)
for each of them. The devices driver can then attach to the created
PDOs in the same way as they do in nonvirtualized environments.
Figure 9-29 Hardware-accelerated devices.
When a user wants to map a hardware-accelerated device to a VM, it uses
some PowerShell commands (see the following experiment for further
details), which start by “unmounting” the device from the root partition. This
action forces the VMMS service to communicate with the standard PCI
driver (through its exposed device, called PciControl). The VMMS service
sends a PCIDRIVE_ADD_VMPROXYPATH IOCTL to the PCI driver by
providing the device descriptor (in form of bus, device, and function ID). The
PCI driver checks the descriptor, and, if the verification succeeded, adds it in
the HKLM\System\CurrentControlSet\Control\PnP\Pci\VmProxy registry
value. The VMMS then starts a PNP device (re)enumeration by using
services exposed by the PNP manager. In the enumeration phase, the PCI
driver finds the new proxy device and loads the PCI proxy driver (Pcip.sys),
which marks the device as reserved for the virtualization stack and renders it
invisible to the host operating system.
The second step requires assigning the device to a VM. In this case, the
VMMS writes the device descriptor in the VM configuration file. When the
VM is started, the VPCI VDEV (vpcievdev.dll) reads the direct-access
device’s descriptor from the VM configuration, and starts a complex
configuration phase that is orchestrated mainly by the VPCI VSP
(Vpcivsp.sys). Indeed, in its “power on” callback, the VPCI VDEV sends
different IOCTLs to the VPCI VSP (which runs in the root partition), with
the goal to perform the creation of the virtual bus and the assignment of
hardware-accelerated devices to the guest VM.
A “virtual bus” is a data structure used by the VPCI infrastructure as a
“glue” to maintain the connection between the root partition, the guest VM,
and the direct-access devices assigned to it. The VPCI VSP allocates and
starts the VMBus channel offered to the guest VM and encapsulates it in the
virtual bus. Furthermore, the virtual bus includes some pointers to important
data structures, like some allocated VMBus packets used for the bidirectional
communication, the guest power state, and so on. After the virtual bus is
created, the VPCI VSP performs the device assignment.
A hardware-accelerated device is internally identified by a LUID and is
represented by a virtual device object, which is allocated by the VPCI VSP.
Based on the device’s LUID, the VPCI VSP locates the proper proxy driver,
which is also known as Mux driver—it’s usually Pcip.sys). The VPCI VSP
queries the SR-IOV or DDA interfaces from the proxy driver and uses them
to obtain the Plug and Play information (hardware descriptor) of the direct-
access device and to collect the resource requirements (MMIO space, BAR
registers, and DMA channels). At this point, the device is ready to be
attached to the guest VM: the VPCI VSP uses the services exposed by the
WinHvr driver to emit the HvAttachDevice hypercall to the hypervisor,
which reconfigures the system IOMMU for mapping the device’s address
space in the guest partition.
The guest VM is aware of the mapped device thanks to the VPCI VSC
(Vpci.sys). The VPCI VSC is a WDF bus driver enumerated and launched by
the VMBus bus driver located in the guest VM. It is composed of two main
components: a FDO (functional device object) created at VM boot time, and
one or more PDOs (physical device objects) representing the physical direct-
access devices remapped in the guest VM. When the VPCI VSC bus driver is
executed in the guest VM, it creates and starts the client part of the VMBus
channel used to exchange messages with the VSP. “Send bus relations” is the
first message sent by the VPCI VSC thorough the VMBus channel. The VSP
in the root partition responds by sending the list of hardware IDs describing
the hardware-accelerated devices currently attached to the VM. When the
PNP manager requires the new device relations to the VPCI VSC, the latter
creates a new PDO for each discovered direct-access device. The VSC driver
sends another message to the VSP with the goal of requesting the resources
used by the PDO.
After the initial setup is done, the VSC and VSP are rarely involved in the
device management. The specific hardware-accelerated device’s driver in the
guest VM attaches to the relative PDO and manages the peripheral as if it had
been installed on a physical machine.
EXPERIMENT: Mapping a hardware-accelerated
NVMe disk to a VM
As explained in the previous section, physical devices that support
SR-IOV and DDE technologies can be directly mapped in a guest
VM running in a Windows Server 2019 host. In this experiment,
we are mapping an NVMe disk, which is connected to the system
through the PCI-Ex bus and supports DDE, to a Windows 10 VM.
(Windows Server 2019 also supports the direct assignment of a
graphics card, but this is outside the scope of this experiment.)
As explained at https://docs.microsoft.com/en-
us/virtualization/community/team-blog/2015/20151120-discrete-
device-assignment-machines-and-devices, for being able to be
reassigned, a device should have certain characteristics, such as
supporting message-signaled interrupts and memory-mapped I/O.
Furthermore, the machine in which the hypervisor runs should
support SR-IOV and have a proper I/O MMU. For this experiment,
you should start by verifying that the SR-IOV standard is enabled
in the system BIOS (not explained here; the procedure varies based
on the manufacturer of your machine).
The next step is to download a PowerShell script that verifies
whether your NVMe controller is compatible with Discrete Device
Assignment. You should download the survey-dda.ps1 PowerShell
script from https://github.com/MicrosoftDocs/Virtualization-
Documentation/tree/master/hyperv-samples/benarm-
powershell/DDA. Open an administrative PowerShell window (by
typing PowerShell in the Cortana search box and selecting Run As
Administrator) and check whether the PowerShell script
execution policy is set to unrestricted by running the Get-
ExecutionPolicy command. If the command yields some output
different than Unrestricted, you should type the following: Set-
ExecutionPolicy -Scope LocalMachine -ExecutionPolicy
Unrestricted, press Enter, and confirm with Y.
If you execute the downloaded survey-dda.ps1 script, its output
should highlight whether your NVMe device can be reassigned to
the guest VM. Here is a valid output example:
Click here to view code image
Standard NVM Express Controller
Express Endpoint -- more secure.
    And its interrupts are message-based, assignment can 
work.
PCIROOT(0)#PCI(0302)#PCI(0000)
Take note of the location path (the
PCIROOT(0)#PCI(0302)#PCI(0000) string in the example). Now
we will set the automatic stop action for the target VM as turned-
off (a required step for DDA) and dismount the device. In our
example, the VM is called “Vibranium.” Write the following
commands in your PowerShell window (by replacing the sample
VM name and device location with your own):
Click here to view code image
Set-VM -Name "Vibranium" -AutomaticStopAction TurnOff
Dismount-VMHostAssignableDevice -LocationPath 
"PCIROOT(0)#PCI(0302)#PCI(0000)"
In case the last command yields an operation failed error, it is
likely that you haven’t disabled the device. Open the Device
Manager, locate your NVMe controller (Standard NVMe Express
Controller in this example), right-click it, and select Disable