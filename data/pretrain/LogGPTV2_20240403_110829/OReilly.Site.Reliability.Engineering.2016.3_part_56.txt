tions of the framework (say in C++ versus Java) can’t share code, the goal is to expose
the same API, behavior, configuration, and controls for identical functionality. There‐
fore, development teams can choose the language platform that fits their needs and
experience, while SREs can still expect the same familiar behavior in production and
standard tools to manage the service.
New Service and Management Benefits
The structural approach, founded on service frameworks and a common production
platform and control surface, provided a host of new benefits.
Significantly lower operational overhead
A production platform built on top of frameworks with stronger conventions signifi‐
cantly reduced operational overhead, for the following reasons:
• It supports strong conformance tests for coding structure, dependencies, tests,
coding style guides, and so on. This functionality also improves user data privacy,
testing, and security conformance.
• It features built-in service deployment, monitoring, and automation for all
services.
• It facilitates easier management of large numbers of services, especially micro-
services, which are growing in number.
• It enables much faster deployment: an idea can graduate to fully deployed SRE-
level production quality in a matter of days!
Universal support by design
The constant growth in the number of services at Google means that most of these
services can neither warrant SRE engagement nor be maintained by SREs. Regardless,
services that don’t receive full SRE support can be built to use production features
that are developed and maintained by SREs. This practice effectively breaks the SRE
staffing barrier. Enabling SRE-supported production standards and tools for all teams
improves the overall service quality across Google. Furthermore, all services that are
454 | Chapter 32: The Evolving SRE Engagement Model
implemented with frameworks automatically benefit from improvements made over
time to frameworks modules.
Faster, lower overhead engagements
The frameworks approach results in faster PRR execution because we can rely upon:
• Built-in service features as part of the framework implementation
• Faster service onboarding (usually accomplished by a single SRE during one
quarter)
• Less cognitive burden for the SRE teams managing services built using
frameworks
These properties allow SRE teams to lower the assessment and qualification effort for
service onboarding, while maintaining a high bar on service production quality.
A new engagement model based on shared responsibility
The original SRE engagement model presented only two options: either full SRE sup‐
port, or approximately no SRE engagement.2
A production platform with a common service structure, conventions, and software
infrastructure made it possible for an SRE team to provide support for the “platform”
infrastructure, while the development teams provide on-call support for functional
issues with the service—that is, for bugs in the application code. Under this model,
SREs assume responsibility for the development and maintenance of large parts of
service software infrastructure, particularly control systems such as load shedding,
overload, automation, traffic management, logging, and monitoring.
This model represents a significant departure from the way service management was
originally conceived in two major ways: it entails a new relationship model for the
interaction between SRE and development teams, and a new staffing model for SRE-
supported service management.3
2 Occasionally, there were consulting engagements by SRE teams with some non-onboarded services, but con‐
sultations were a best-effort approach and limited in number and scope.
3 The new model of service management changes the SRE staffing model in two ways: (1) because a lot of ser‐
vice technology is common, it reduces the number of required SREs per service; (2) it enables the creation of
production platforms with separation of concerns between production platform support (done by SREs) and
service-specific business-logic support, which remains with the development team. These platforms teams are
staffed based upon the need to maintain the platform rather than upon service count, and can be shared
across products.
Evolving Services Development: Frameworks and SRE Platform | 455
Conclusion
Service reliability can be improved through SRE engagement, in a process that
includes systematic review and improvement of its production aspects. Google SRE’s
initial such systematic approach, the Simple Production Readiness Review, made
strides in standardizing the SRE engagement model, but was only applicable to serv‐
ices that had already entered the Launch phase.
Over time, SRE extended and improved this model. The Early Engagement Model
involved SRE earlier in the development lifecycle in order to “design for reliability.”
As demand for SRE expertise continued to grow, the need for a more scalable engage‐
ment model became increasingly apparent. Frameworks for production services were
developed to meet this demand: code patterns based on production best practices
were standardized and encapsulated in frameworks, so that use of frameworks
became a recommended, consistent, and relatively simple way of building
production-ready services.
All three of the engagement models described are still practiced within Google. How‐
ever, the adoption of frameworks is becoming a prominent influence on building
production-ready services at Google as well as profoundly expanding the SRE contri‐
bution, lowering service management overhead, and improving baseline service qual‐
ity across the organization.
456 | Chapter 32: The Evolving SRE Engagement Model
PART V
Conclusions
Having covered much ground in terms of how SRE works at Google, and how the
principles and practices we’ve developed might be applied to other organizations in
our field, it now seems appropriate to turn our view to Chapter 33, Lessons Learned
from Other Industries, to examine how SRE’s practices compare to other industries
where reliability is critically important.
Finally, Google’s VP for Site Reliability Engineering, Benjamin Lutch, writes about
SRE’s evolution over the course of his career in his conclusion, examining SRE
through the lens of some observations on the aviation industry.
CHAPTER 33
Lessons Learned from Other Industries
Written by Jennifer Petoff
Edited by Betsy Beyer
A deep dive into SRE culture and practices at Google naturally leads to the question
of how other industries manage their businesses for reliability. Compiling this book
on Google SRE created an opportunity to speak to a number of Google’s engineers
about their previous work experiences in a variety of other high-reliability fields in
order to address the following comparative questions:
• Are the principles used in Site Reliability Engineering also important outside of
Google, or do other industries tackle the requirements of high reliability in mark‐
edly different ways?
• If other industries also adhere to SRE principles, how are the principles manifes‐
ted?
• What are the similarities and differences in the implementation of these princi‐
ples across industries?
• What factors drive similarities and differences in implementation?
• What can Google and the tech industry learn from these comparisons?
A number of principles fundamental to Site Reliability Engineering at Google are dis‐
cussed throughout this text. To simplify our comparison of best practices in other
industries, we distilled these concepts into four key themes:
• Preparedness and Disaster Testing
• Postmortem Culture
459
• Automation and Reduced Operational Overhead
• Structured and Rational Decision Making
This chapter introduces the industries that we profiled and the industry veterans we
interviewed. We define key SRE themes, discuss how these themes are implemented
at Google, and give examples of how these principles reveal themselves in other
industries for comparative purposes. We conclude with some insights and discussion
on the patterns and anti-patterns we discovered.
Meet Our Industry Veterans
Peter Dahl is a Principal Engineer at Google. Previously, he worked as a defense con‐
tractor on several high-reliability systems including many airborne and wheeled vehi‐
cle GPS and inertial guidance systems. Consequences of a lapse in reliability in such
systems include vehicle malfunction or loss, and the financial consequences associ‐
ated with that failure.
Mike Doherty is a Site Reliability Engineer at Google. He worked as a lifeguard and
lifeguard trainer for a decade in Canada. Reliability is absolutely essential by nature in
this field, because lives are on the line every day.
Erik Gross is currently a software engineer at Google. Before joining the company, he
spent seven years designing algorithms and code for the lasers and systems used to
perform refractive eye surgery (e.g., LASIK). This is a high-stakes, high-reliability
field, in which many lessons relevant to reliability in the face of government regula‐
tions and human risk were learned as the technology received FDA approval, gradu‐
ally improved, and finally became ubiquitous.
Gus Hartmann and Kevin Greer have experience in the telecommunications indus‐
try, including maintaining the E911 emergency response system.1 Kevin is currently a
software engineer on the Google Chrome team and Gus is a systems engineer for
Google’s Corporate Engineering team. User expectations of the telecom industry
demand high reliability. Implications of a lapse of service range from user inconven‐
ience due to a system outage to fatalities if E911 goes down.
Ron Heiby is a Technical Program Manager for Site Reliability Engineering at Goo‐
gle. Ron has experience in development for cell phones, medical devices, and the
automotive industry. In some cases he worked on interface components of these
industries (for example, on a device to allow EKG readings2 in ambulances to be
transmitted over the digital wireless phone network). In these industries, the impact
1 E911 (Enhanced 911): Emergency response line in the US that leverages location data.
2 Electrocardiogram readings: https://en.wikipedia.org/wiki/Electrocardiography.
460 | Chapter 33: Lessons Learned from Other Industries
of a reliability issue can range from harm to the business incurred by equipment
recalls to indirectly impacting life and health (e.g., people not getting the medical
attention they need if the EKG cannot communicate with the hospital).
Adrian Hilton is a Launch Coordination Engineer at Google. Previously, he worked
on UK and USA military aircraft, naval avionics and aircraft stores management sys‐
tems, and UK railway signaling systems. Reliability is critical in this space because
impact of incidents ranges from multimillion-dollar loss of equipment to injuries and
fatalities.
Eddie Kennedy is a project manager for the Global Customer Experience team at
Google and a mechanical engineer by training. Eddie spent six years working as a Six
Sigma Black Belt process engineer in a manufacturing facility that makes synthetic
diamonds. This industry is characterized by a relentless focus on safety, because the
extremes of temperature and pressure demands of the process pose a high level of
danger to workers on a daily basis.
John Li is currently a Site Reliability Engineer at Google. John previously worked as a
systems administrator and software developer at a proprietary trading company in
the finance industry. Reliability issues in the financial sector are taken quite seriously
because they can lead to serious fiscal consequences.
Dan Sheridan is a Site Reliability Engineer at Google. Before joining the company, he
worked as a safety consultant in the civil nuclear industry in the UK. Reliability is
important in the nuclear industry because an incident can have serious repercussions:
outages can incur millions a day in lost revenue, while risks to workers and those in
the community are even more dire, dictating zero tolerance for failure. Nuclear infra‐
structure is designed with a series of failsafes that halt operations before an incident
of any such magnitude is reached.
Jeff Stevenson is currently a hardware operations manager at Google. He has past
experience as a nuclear engineer in the US Navy on a submarine. Reliability stakes in
the nuclear Navy are high—problems that arise in the case of incidents range from
damaged equipment, to long-standing environmental impact, to potential loss of life.
Matthew Toia is a Site Reliability Manager focused on storage systems. Prior to Goo‐
gle, he worked on software development and deployment of air traffic control soft‐
ware systems. Effects from incidents in this industry range from inconveniences to
passengers and airlines (e.g., delayed flights, diverted planes) to potential loss of life
in the event of a crash. Defense in depth is a key strategy to avoiding catastrophic
failures.
Now that you’ve met our experts and gained a high-level understanding of why relia‐
bility is important in their respective former fields, we’ll delve into the four key
themes of reliability.
Meet Our Industry Veterans | 461
Preparedness and Disaster Testing
“Hope is not a strategy.” This rallying cry of the SRE team at Google sums up what we
mean by preparedness and disaster testing. The SRE culture is forever vigilant and
constantly questioning: What could go wrong? What action can we take to address
those issues before they lead to an outage or data loss? Our annual Disaster and
Recovery Testing (DiRT) drills seek to address these questions head-on [Kri12]. In
DiRT exercises, SREs push production systems to the limit and inflict actual outages
in order to:
• Ensure that systems react the way we think they will
• Determine unexpected weaknesses
• Figure out ways to make the systems more robust in order to prevent uncontrol‐
led outages
Several strategies for testing disaster readiness and ensuring preparedness in other
industries emerged from our conversations. Strategies included the following:
• Relentless organizational focus on safety
• Attention to detail
• Swing capacity
• Simulations and live drills
• Training and certification
• Obsessive focus on detailed requirements gathering and design
• Defense in depth
Relentless Organizational Focus on Safety
This principle is particularly important in an industrial engineering context. Accord‐
ing to Eddie Kennedy, who worked on a manufacturing floor where workers faced
safety hazards, “every management meeting started with a discussion of safety.” The
manufacturing industry prepares itself for the unexpected by establishing highly
defined processes that are strictly followed at every level of the organization. It is crit‐
ical that all employees take safety seriously, and that workers feel empowered to speak
up if and when anything seems amiss. In the case of nuclear power, military aircraft,
and railway signaling industries, safety standards for software are well detailed (e.g.,
UK Defence Standard 00-56, IEC 61508, IEC513, US DO-178B/C, and DO-254) and
462 | Chapter 33: Lessons Learned from Other Industries
levels of reliability for such systems are clearly identified (e.g., Safety Integrity Level
(SIL) 1–4),3 with the aim of specifying acceptable approaches to delivering a product.
Attention to Detail
From his time spent in the US Navy, Jeff Stevenson recalls an acute awareness of how
a lack of diligence in executing small tasks (for example, lube oil maintenance) could
lead to major submarine failure. A very small oversight or mistake can have big
effects. Systems are highly interconnected, so an accident in one area can impact mul‐
tiple related components. The nuclear Navy focuses on routine maintenance to
ensure that small issues don’t snowball.
Swing Capacity
System utilization in the telecom industry can be highly unpredictable. Absolute
capacity can be strained by unforeseeable events such as natural disasters, as well as
large, predictable events like the Olympics. According to Gus Hartmann, the industry
deals with these incidents by deploying swing capacity in the form of a SOW (switch
on wheels), a mobile telco office. This excess capacity can be rolled out in an emer‐
gency or in anticipation of a known event that is likely to overload the system.
Capacity issues veer into the unexpected in matters unrelated to absolute capacity, as
well. For example, when a celebrity’s private phone number was leaked in 2005 and
thousands of fans simultaneously attempted to call her, the telecom system exhibited
symptoms similar to a DDoS or massive routing error.
Simulations and Live Drills
Google’s Disaster Recovery tests have a lot in common with the simulations and live
drills that are a key focus of many of the established industries we researched. The
potential consequences of a system outage determine whether using a simulation or a
live drill is appropriate. For example, Matthew Toia points out that the aviation indus‐
try can’t perform a live test “in production” without putting equipment and passen‐
gers at risk. Instead, they employ extremely realistic simulators with live data feeds, in
which the control rooms and equipment are modeled down to the tiniest details to
ensure a realistic experience without putting real people at risk. Gus Hartmann
reports that the telecom industry typically focuses on live drills centered on surviving
hurricanes and other weather emergencies. Such modeling led to the creation of
weatherproof facilities with generators inside the building capable of outlasting a
storm.
3 https://en.wikipedia.org/wiki/Safety_integrity_level
Preparedness and Disaster Testing | 463
The US nuclear Navy uses a mixture of “what if” thought exercises and live drills.
According to Jeff Stevenson, the live drills involve “actually breaking real stuff but
with control parameters. Live drills are carried out religiously, every week, two to
three days per week.” For the nuclear Navy, thought exercises are useful, but not suffi‐
cient to prepare for actual incidents. Responses must be practiced so they are not
forgotten.
According to Mike Doherty, lifeguards face disaster testing exercises more akin to a
“mystery shopper” experience. Typically, a facility manager works with a child or an
incognito lifeguard in training to stage a mock drowning. These scenarios are con‐
ducted to be as realistic as possible so that lifeguards aren’t able to differentiate
between real and staged emergencies.
Training and Certification