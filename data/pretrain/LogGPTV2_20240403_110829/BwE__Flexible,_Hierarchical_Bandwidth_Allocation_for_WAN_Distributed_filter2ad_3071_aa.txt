# BwE: Flexible, Hierarchical Bandwidth Allocation for WAN Distributed Computing

## Authors
- Alok Kumar
- Sushant Jain
- Uday Naik
- Anand Raghuraman
- Nikhil Kasinadhuni
- Enrique Cauich Zermeno
- C. Stephen Gunn
- Jing Ai
- Björn Carlin
- Mihai Amarandei-Stavila
- Mathieu Robin
- Aspi Siganporia
- Stephen Stuart
- Amin Vahdat
- Google Inc.

## Abstract
Wide Area Network (WAN) bandwidth remains a constrained and economically infeasible resource to overprovision. Therefore, it is crucial to allocate capacity based on service priority and the incremental value of additional allocation. For instance, a service may have the highest priority for 10 Gb/s of bandwidth, but once this allocation is met, the incremental priority may drop sharply, favoring other services. Recognizing that fixed-priority flows may not be ideal for bandwidth allocation, we introduce Bandwidth Enforcer (BwE), a global, hierarchical bandwidth allocation infrastructure. BwE supports:
1. Service-level bandwidth allocation following prioritized bandwidth functions.
2. Independent allocation and delegation policies according to user-defined hierarchies, accounting for a global view of bandwidth and failure conditions.
3. Multi-path forwarding common in traffic-engineered networks.
4. A central administrative point to override (possibly faulty) policy during exceptional conditions.

BwE has been in production for multiple years, delivering more service-efficient bandwidth utilization and simpler management.

## CCS Concepts
- Networks → Network resources allocation; Network management

## Keywords
- Bandwidth Allocation
- Wide-Area Networks
- Software-Defined Networking
- Max-Min Fairness

## 1. Introduction
TCP-based bandwidth allocation to individual flows contending for bandwidth on bottleneck links has served the Internet well for decades. However, this model assumes all flows are of equal priority and benefit equally from any incremental share of available bandwidth. It implicitly assumes a client-server communication model where a TCP flow captures the communication needs of an application across the Internet.

This paper re-examines bandwidth allocation for distributed computing running across dedicated private WANs, supporting cloud computing and service providers. Thousands of simultaneous applications run across multiple global data centers, with each data center maintaining thousands of active connections to remote servers. WAN traffic engineering means that site-pair communication follows different network paths, each with different bottlenecks. Individual services have vastly different bandwidth, latency, and loss requirements.

We present BwE, a new WAN bandwidth allocation mechanism supporting distributed computing and data transfer. BwE provides work-conserving bandwidth allocation, hierarchical fairness with flexible policy among competing services, and Service Level Objective (SLO) targets that independently account for bandwidth, latency, and loss.

BwE's key insight is that routers are not the ideal place to map policy designs about bandwidth allocation onto per-packet behavior. Instead, following the End-to-End Argument, we push all such mappings to the source host machines. Hosts rate-limit their outgoing traffic and mark packets using the DSCP field. Routers use the DSCP marking to determine which path to use for a packet and which packets to drop when congested. We use global knowledge of network topology and link utilization as input to a hierarchy of bandwidth enforcers, ranging from a global enforcer down to enforcers on each host. This architecture decouples the aggregate bandwidth allocated to a flow from the handling of the flow at the routers.

BwE allocates bandwidth to competing applications based on flexible policy configured by bandwidth functions capturing application priority and incremental utility from additional bandwidth in different regions. BwE supports hierarchical bandwidth allocation and delegation among services while accounting for multi-path WAN communication. BwE is the principal bandwidth allocation mechanism for one of the largest private WANs and has run in production for multiple years across hundreds of thousands of endpoints. The key contributions of our work include:

- Leveraging concepts from Software-Defined Networking (SDN), we build a unified, hierarchical control plane for bandwidth management extending to all end hosts. Hosts report per-user and per-task demands to the control plane and rate-shape a subset of flows.
- We integrate BwE into existing WAN traffic engineering (TE) mechanisms, including MPLS Auto-Bandwidth and a custom SDN infrastructure. BwE takes WAN pathing decisions made by a TE service and re-allocates the available site-to-site capacity, split across multiple paths, among competing applications. We also benefit from the reverse integration: using BwE measures of prioritized application demand as input to TE pathing algorithms.
- We implement hierarchical max-min fair bandwidth allocation to flexibly-defined FlowGroups contending for resources across multiple paths and at different levels of network abstraction. The bandwidth allocation mechanism is both work-conserving and flexible enough to implement a range of network sharing policies.

In summary, BwE delivers several advantages. First, it provides isolation among competing services, delivering plentiful capacity in the common case while maintaining required capacity under failure and maintenance scenarios. Second, administrators have a single point for specifying allocation policy. Finally, BwE enables the WAN to run at higher levels of utilization. By tightly integrating loss-insensitive file transfer protocols running at low priority with BwE, we run many of our WAN links at 90% utilization.

## 2. Background
We begin by describing our WAN environment and highlighting the challenges we faced with existing bandwidth allocation mechanisms. Thousands of individual applications and services run across dozens of wide area sites, each containing multiple clusters. Host machines within a cluster share a common LAN. Figure 1 shows an example WAN with sites S1, S2, and S3; C1 and C2 are clusters within site S1.

We host a combination of interactive web services (e.g., search and web mail), streaming video, batch-style data processing (e.g., MapReduce), and large-scale data transfer services (e.g., index copy from one site to another). Cluster management software maps services to hosts independently, so we cannot leverage IP address aggregation/prefix to identify a service. However, we can install control software on hosts and leverage a control protocol running outside of routers.

We started with traditional mechanisms for bandwidth allocation such as TCP, QoS, and MPLS tunnels. However, these proved inadequate for several reasons:

- **Granularity and Scale**: Our network and service capacity planners need to reason with bandwidth allocations at different aggregation levels. For example, a product group may need a specified minimum of site-to-site bandwidth across all services within the product area. In other cases, individual users or services may require a bandwidth guarantee between a specific pair of clusters. We need to scale bandwidth management to thousands of individual services and product groups across dozens of sites, each containing multiple clusters. We need a way to classify and aggregate individual flows into arbitrary groups based on configured policy. TCP fairness is at a 5-tuple flow granularity. On a congested link, an application gets bandwidth proportional to the number of active flows it sends across the links. Our services require guaranteed bandwidth allocation independent of the number of active TCP flows. Router QoS and MPLS tunnels do not scale to the number of service classes we must support and do not provide sufficient flexibility in allocation policy.
- **Multipath Forwarding**: For efficiency, wide area packet forwarding follows multiple paths through the network, possibly with each path of varying capacity. Routers hash individual service flows to one of the available paths based on packet header content. Any bandwidth allocation from one site to another must simultaneously account for multiple source/destination paths, whereas existing bandwidth allocation mechanisms—TCP, router QoS, and MPLS tunnels—focus on different granularities (flows, links, and single paths, respectively).
- **Flexible and Hierarchical Allocation Policy**: Simple weighted bandwidth allocation was found to be inadequate. For example, we may want to give a high-priority user a weight of 10 until it has been allocated 1 Gb/s, a weight of 1 until it is allocated 10 Gb/s, and a weight of 0.1 for all bandwidth beyond that. Further, bandwidth allocation should be hierarchical, such that bandwidth allocated to a single product group can be subdivided to multiple users, which in turn may be hierarchically allocated to applications, individual hosts, and finally flows. Different allocation policies should be available at each level of the hierarchy.
- **Delegation or Attribution**: Applications increasingly leverage computation and communication from a variety of infrastructure services. Consider the case where a service writes data to a storage service, which in turn replicates the content to multiple WAN sites for availability. Since the storage service acts on behalf of thousands of other services, its bandwidth should be charged to the originating user. Bandwidth delegation provides differential treatment across users sharing a service, avoids head-of-line blocking across traffic for different users, and ensures that the same policies are applied across the network for a user’s traffic.

We designed BwE to address the challenges and requirements described above around the principle that bandwidth allocation should be extended all the way to end hosts. While historically, we have looked to routers with increasingly sophisticated ASICs and control protocols for WAN bandwidth allocation, we argue that this design point has resulted simply from lack of control over end hosts on the part of network service providers. Assuming such access is available, we find that the following functionality can be supported with a hierarchical control infrastructure extending to end hosts:
- Mapping WAN communication back to thousands of flow groups.
- Flexibly sub-dividing aggregate bandwidth allocations back to individual flows.
- Accounting for delegation of resource charging from one service to another.
- Expressing and enforcing flexible max-min bandwidth sharing policies.

On the contrary, existing routers must inherently leverage limited information available only in packet headers to map packets to one of a small number of service classes or tunnels.

Figure 2 shows an instance of very high loss in multiple QoS classes during a capacity reduction on our network. TCP congestion control was not effective, and the loss remained high.