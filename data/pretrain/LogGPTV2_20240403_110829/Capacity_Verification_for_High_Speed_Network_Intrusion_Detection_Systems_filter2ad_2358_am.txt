concern that an unknown weakness in an intrusion detection system creates a
“dangerously false sense of security.” In the case of this work, it was shown that a
weakness in an anomaly detector could realistically be exploited to compromise
the system being protected by that detector.
Wagner and Dean [20] introduced a new class of attacks against intrusion
detection systems which they called the “mimicry attack”. A “mimicry attack”
is where an attacker is able to “develop malicious exploit code that mimics the
operation of the application, staying within the conﬁnes of the model and thereby
evading detection ...” Wagner and Dean studied this class of attack theoretically,
but until this present paper, no study has shown if it were possible to create and
deploy a mimicry attack in the real-world that truly aﬀected the performance of
an intrusion-detection system.
The present study conﬁrms that the class of mimicry attacks does pose a
serious threat to an anomaly-based intrusion detection system. By modifying
common real-world exploits to create examples of this class of attack, this study
also shows how mimicry attacks are able to undermine the protection oﬀered by
an anomaly-based intrusion detection system.
10 Conclusion
This study has shown how an anomaly-based intrusion detection system can be
eﬀectively undermined by modifying common real-world exploits. It presented
a method that identiﬁed weaknesses in an anomaly-based intrusion detector,
and showed how an attacker can eﬀectively modify common exploits to take
advantage of those weaknesses in order to craft an oﬀensive mechanism that
renders an anomaly-based intrusion detector blind to the on-going presence of
those attacks.
The results show that it is possible to hide the presence of the passwd and
traceroute common exploits from stide by modifying those exploits so that they
manifest only within stide’s detection blind spot. The results also show that it
72
K.M.C. Tan, K.S. Killourhy, and R.A. Maxion
is possible to control the manifestation of an attack such that the manifestation
moves from an area of detection clarity to one of detection blindness for stide.
References
1. Herve Debar, Marc Dacier, and Andreas Wespi. Towards a taxonomy of intrusion-
detection systems. Computer Networks, 31(8):805–822, April 1999.
2. Stephanie Forrest, Steven A. Hofmeyr, Anil Somayaji, and Thomas A. Longstaﬀ.
A sense of self for unix processes. In Proceedings of the 1996 IEEE Symposium on
Security and Privacy, 6–8 May 1996, Oakland, California, pages 120–128, IEEE
Computer Society Press, Los Alamitos, California, 1996.
3. Cristian Gafton. passwd(1). Included in passwd version 0.64.1-1 software package,
January 1998.
4. Anup K. Ghosh, Aaron Schwartzbard, and Michael Schatz. Learning program
behavior proﬁles for intrusion detection. In Proceedings of the 1st Workshop on
Intrusion Detection and Network Monitoring, 9–12 April 1999, Santa Clara, Cali-
fornia, pages 51–62, The USENIX Association, Berkeley, California, 1999.
5. Anup K. Ghosh, James Wanken, and Frank Charron. Detecting anomalous and
unknown intrusions against programs. In Proceedings of the 14th Annual Computer
Security Applications Conference, 7–11 December 1998, Phoenix, Arizona, pages
259–267, IEEE Computer Society Press, Los Alamitos, 1998.
6. Steven A. Hofmeyr, Stephanie Forrest, and Anil Somayaji.
Intrusion detection
using sequences of system calls. Journal of Computer Security, 6(3):151–180, 1998.
Included in traceroute version 1.4a5 software
7. Van Jacobson. Traceroute(8).
package, April 1997.
8. Michel “MaXX” Kaempf. Traceroot2: Local root exploit in LBNL traceroute. In-
ternet: http://packetstormsecurity.org/0011-exploits/traceroot2.c, March
2002.
9. Sandeep Kumar. Classiﬁcation and Detection of Computer Intrusions. PhD thesis,
Purdue University, West Lafayette, Indiana, August 1995.
10. Teresa Lunt. Automated audit trail analysis and intrusion detection: A survey. In
Proceedings of the 11th National Computer Security Conference, Baltimore, Mary-
land, pages 65–73, October 1988.
11. Carla Marceau. Characterizing the behavior of a program using multiple-length N-
grams. In New Security Paradigms Workshop, 18–22 September 2000, Ballycotton,
County Cork, Ireland, pages 101–110, ACM Press, New York, New York, 2001.
12. Roy A. Maxion and Kymie M. C. Tan. Anomaly detection in embedded systems.
IEEE Transactions on Computers, 51(2):108–120, February 2002.
13. Andrew P. Moore. CERT/CC vulnerability note VU#176888, July 2002. Internet:
http://www.kb.cert.org/vuls/id/176888.
14. Thomas H. Ptacek and Timothy N. Newsham. Insertion, evasion, and denial of ser-
vice: Eluding network intrusion detection. Secure Networks, Inc., Calgary, Alberta,
Canada, January 1998.
15. Wojciech Purczynski (original author) and “lst” (author of improvements). Epcs2:
Exploit for execve/ptrace race condition in Linux kernel up to 2.2.18. Internet:
http://www.securiteam.com/exploits/5NP061P4AW.html, March 2002.
16. SecurityFocus Vulnerability Archive. LBNL Traceroute Heap Corruption Vulnera-
bility, Bugtraq ID 1739. Internet: http://online.securityfocus.com/bid/1739,
March 2002.
Undermining an Anomaly-Based Intrusion Detection System
73
17. SecurityFocus Vulnerability Archive. Linux PTrace/Setuid Exec Vulnerability,
Bugtraq ID 3447. Internet: http://online.securityfocus.com/bid/3447, March
2002.
18. Anil Somayaji and Geoﬀrey Hunsicker. IMMSEC Kernel-level system call tracing
for Linux 2.2, Version 991117. Obtained through private communication. Previous
version available on the Internet:
http://www.cs.unm.edu/˜immsec/software/, March 2002.
19. Kymie M. C. Tan and Roy A. Maxion. “Why 6?” Deﬁning the operational limits
of stide, an anomaly-based intrusion detector. In Proceedings of the 2002 IEEE
Symposium on Security and Privacy, 12–15 May 2002, Berkeley, California, pages
188–201, IEEE Computer Society Press, Los Alamitos, California, 2002.
20. David Wagner and Drew Dean. Intrusion detection via static analysis. In Pro-
ceedings of the 2001 IEEE Symposium on Security and Privacy, 14–16 May 2001,
Berkeley, California, IEEE Computer Society Press, Los Alamitos, California, 2001.
21. Christina Warrender, Stephanie Forrest, and Barak Pearlmutter. Detecting intru-
sions using system calls: Alternative data models. In Proceedings of the 1999 IEEE
Symposium on Security and Privacy, 9–12 May 1999, Oakland, California, pages
133–145, IEEE Computer Society Press, Los Alamitos, California, 1999.
Analyzing Intensive Intrusion Alerts via Correlation
Peng Ning, Yun Cui, and Douglas S. Reeves
Department of Computer Science
North Carolina State University
Raleigh, NC 27695-7534
PI:EMAIL, PI:EMAIL, PI:EMAIL
Abstract. Traditional intrusion detection systems (IDSs) focus on low-level
attacks or anomalies, and raise alerts independently, though there may be logical
connections between them. In situations where there are intensive intrusions, not
only will actual alerts be mixed with false alerts, but the amount of alerts will
also become unmanageable. As a result, it is difﬁcult for human users or intrusion
response systems to understand the alerts and take appropriate actions. Several
complementary alert correlation methods have been proposed to address this
problem. As one of these methods, we have developed a framework to correlate
intrusion alerts using prerequisites of intrusions. In this paper, we continue this
work to study the feasibility of this method in analyzing real-world, intensive
intrusions. In particular, we develop three utilities (called adjustable graph
reduction, focused analysis, and graph decomposition) to facilitate the analysis of
large sets of correlated alerts. We study the effectiveness of the alert correlation
method and these utilities through a case study with the network trafﬁc captured
at the DEF CON 8 Capture the Flag (CTF) event. Our results show that these
utilities can simplify the analysis of large amounts of alerts, and also reveals
several attack strategies that were repeatedly used in the DEF CON 8 CTF event.
Keywords: Intrusion Detection, Alert Correlation, Attack Scenario Analysis
1 Introduction
Intrusion detection has been considered the second line of defense for computer and
network systems along with the prevention-based techniques such as authentication
and access control. Intrusion detection techniques can be roughly classiﬁed as anomaly
detection (e.g., NIDES/STAT [1]) and misuse detection (e.g., NetSTAT [2]).
Traditional intrusion detection systems (IDSs) focus on low-level attacks or anoma-
lies; they cannot capture the logical steps or attacking strategies behind these attacks.
Consequently, the IDSs usually generate a large amount of alerts, and the alerts are
raised independently, though there may be logical connections between them. In situa-
tions where there are intensive intrusions, not only will actual alerts be mixed with false
alerts, but the amount of alerts will also become unmanageable. As a result, it is difﬁcult
for human users or intrusion response systems to understand the intrusions behind the
alerts and take appropriate actions.
To assist the analysis of intrusion alerts, several alert correlation methods (e.g., [3,
4,5,6]) have been proposed recently to process the alerts reported by IDSs. (Please see
A. Wespi, G. Vigna, and L. Deri (Eds.): RAID 2002, LNCS 2516, pp. 74–94, 2002.
c(cid:1) Springer-Verlag Berlin Heidelberg 2002
Analyzing Intensive Intrusion Alerts via Correlation
75
Section 2 for details.) As one of these proposals, we have developed a framework to
correlate intrusion alerts using prerequisites of intrusions [6].
Our approach is based on the observation that most intrusions are not isolated, but
related as different stages of series of attacks, with the early stages preparing for the
later ones. Intuitively, the prerequisite of an intrusion is the necessary condition for
the intrusion to be successful. For example, the existence of a vulnerable service is the
prerequisite of a remote buffer overﬂow attack against the service. Our method identiﬁes
the prerequisites (e.g., existence of vulnerable services) and the consequences (e.g.,
discovery of vulnerable services) of each type of attacks, and correlate the corresponding
alerts by matching the consequences of some previous alerts and the prerequisites of
some later ones. For example, if we ﬁnd a port scan followed by a buffer overﬂow attack
against one of the scanned ports, we can correlate them into the same series of attacks.
We have developed an intrusion alert correlator based on this framework [7]. Our initial
experiments with the 2000 DARPA intrusion evaluation datasets [8] have shown that
our method can successfully correlate related alerts together [6,7].
This paper continues the aforementioned work to study the effectiveness of this
method in analyzing real-world, intrusion intensive data sets. In particular, we would
like to see how well the alert correlation method can help human users organize and
understand intrusion alerts, especially when IDSs report a large amount of alerts. We
argue that this is a practical problem that the intrusion detection community is facing.
As indicated in [9], “encountering 10-20,000 alarms per sensor per day is common.”
In this paper, we present three utilities (called adjustable graph reduction, focused
analysis, and graph decomposition) that we have developed to facilitate the analysis of
large sets of correlated alerts. These utilities are intended for human users to analyze
and understand the correlated alerts as well as the strategies behind them. We study the
effectiveness of these utilities through a case study with the network trafﬁc captured
at the DEF CON 8 Capture the Flag (CTF) event [10]. Our results show that they can
effectively simplify the analysis of large amounts of alerts. Our analysis also reveals
several attack strategies that appeared in the DEF CON 8 CTF event.
The remainder of this paper is organized as follows. Section 2 reviews the related
work. Section 3 brieﬂy describes our model for alert correlation. Section 4 introduces
three utilities for analyzing hyper-alert correlation graphs. Section 5 describes our case
study with the DEF CON 8 CTF dataset. Section 6 concludes this paper and points out
some future work.
2 Related Work
Intrusion detection has been studied for about twenty years. An excellent overview of
intrusion detection techniques and related issues can be found in a recent book [11].
Several alert correlation techniques have been proposed to facilitate analysis of in-
trusions. In [3], a probabilistic method was used to correlate alerts using similarity
between their features. However, this method is not suitable for fully discovering causal
relationships between alerts. In [12], a similar approach was applied to detect stealthy
portscans along with several heuristics. Though some such heuristics (e.g., feature sepa-
76
P. Ning, Y. Cui, and D.S. Reeves
ration heuristics [12]) may be extended to general alert correlation problem, the approach
cannot fully recover the causal relationships between alerts, either.
Techniques for aggregating and correlating alerts have been proposed by others [4].
In particular, the correlation method in [4] uses a consequence mechanism to specify
what types of alerts may follow a given alert type. This is similar to the speciﬁcation
of misuse signatures. However, the consequence mechanism only uses alert types, the
probes that generate the alerts, the severity level, and the time interval between the two
alerts involved in a consequence deﬁnition, which do not provide sufﬁcient information to
correlate all possibly related alerts. Moreover, it is not easy to predict how an attacker may
arrange a sequence of attacks. In other words, developing a sufﬁcient set of consequence
deﬁnitions for alert correlation is not a solved problem.
Another approach has been proposed to “learn” alert correlation models by applying
machine learning techniques to training data sets embedded with known intrusion sce-
narios [5]. This approach can automatically build models for alert correlation; however,
it requires training in every deployment, and the resulting models may overﬁt the train-
ing data, thereby missing attack scenarios not seen in the training data sets. The alert
correlation techniques that we present in this paper address this same problem from a
novel angle, overcoming the limitations of the above approaches.
Our method can be considered as a variation of JIGSAW [13]. Both methods try
to uncover attack scenarios based on speciﬁcations of individual attacks. However, our
method also differs from JIGSAW. First, our method allows partial satisfaction of pre-
requisites (i.e., required capabilities in JIGSAW [13]), recognizing the possibility of
undetected attacks and that of attackers gaining information through non-intrusive ways
(e.g., talking to a friend working in the victim organization), while JIGSAW requires
all required capabilities be satisﬁed. Second, our method allows aggregation of alerts,
and thus can reduce the complexity involved in alert analysis, while JIGSAW currently
does not have any similar mechanisms. Third, we develop a set of utilities for interactive
analysis of correlated alerts, which is not provided by JIGSAW.
An alert correlation approach similar to ours was proposed recently in the MIRADOR
project [14]. The MIRADOR approach also correlates alerts using partial match of pre-
requisites (pre-conditions) and consequences (post-conditions) of attacks. However, the
MIRADOR approach uses a different formalism than ours. In particular, the MIRADOR
approach treats alert aggregation as an individual stage before alert correlation, while
our method allows alert aggregation during and after correlation. As we will see in the
later sections, this difference leads to the three utilities for interactive alert analysis.
GrIDS uses activity graphs to represent the causal structure of network activities and
detect propagation of large-scale attacks [15]. Our method also uses graphs to represent
correlated alerts. However, unlike GrIDS, in which nodes represent hosts or departments
and edges represent network trafﬁc between them, our method uses nodes to represent
alerts, and edges the relationships between the alerts.
Several languages have been proposed to represent attacks, including STAT [2,16],
Colored-Petri Automata (CPA), LAMBDA [17], and MuSig [18] and its successor [19].
In particular, LAMBDA uses a logic-based method to specify the precondition and post-
condition of attack scenarios, which is similar to our method. (See Section 3.) However,
all these languages specify entire attack scenarios, which are limited to known scenarios.
Analyzing Intensive Intrusion Alerts via Correlation
77
In contrast, our method (as well as JIGSAW) describes prerequisites and consequences
of individual attacks, and correlate detected attacks (i.e., alerts) based on the relation-
ship between these prerequisites and consequences. Thus, our method can potentially
correlate alerts from unknown attack scenarios.
Alert correlation has been studied in the context of network management (e.g., [20],
[21], and [22]). In theory, alert correlation methods for network management are ap-
plicable to intrusion alert correlation. However, intrusion alert correlation faces more
challenges that its counter part in network management: While alert correlation for net-
work management deals with alerts about natural faults, which usually exhibits regular
patterns, intrusion alert correlation has to cope with less predictable, malicious intruders.
3 Preliminary: Alert Correlation Using Prerequisites of Intrusions
In this section, we brieﬂy describe our model for correlating alerts using prerequisites
of intrusions. Please read [6] for further details.
The alert correlation model is based on the observation that in series of attacks, the
component attacks are usually not isolated, but related as different stages of the attacks,
with the early ones preparing for the later ones. For example, an attacker needs to install
the Distributed Denial of Service (DDOS) daemon programs before he can launch a
DDOS attack.
To take advantage of this observation, we correlate alerts using prerequisites of the
corresponding attacks. Intuitively, the prerequisite of an attack is the necessary condition
for the attack to be successful. For example, the existence of a vulnerable service is the
prerequisite of a remote buffer overﬂow attack against the service. Moreover, an attacker
may make progress (e.g., discover a vulnerable service, install a Trojan horse program) as
a result of an attack. Informally, we call the possible outcome of an attack the (possible)
consequence of the attack. In a series of attacks where attackers launch earlier ones to
prepare for later ones, there are usually strong connections between the consequences
of the earlier attacks and the prerequisites of the later ones.
Accordingly, we identify the prerequisites (e.g., existence of vulnerable services)
and the consequences (e.g., discovery of vulnerable services) of each type of attacks and
correlate detected attacks (i.e., alerts) by matching the consequences of some previous
alerts and the prerequisites of some later ones.
Note that an attacker does not have to perform early attacks to prepare for later ones.
For example, an attacker may launch an individual buffer overﬂow attack against the
service blindly. In this case, we cannot, and should not correlate it with others. However,
if the attacker does launch attacks with earlier ones preparing for later ones, our method
can correlate them, provided the attacks are detected by IDSs.
3.1 Prerequisite and Consequence of Attacks
We use predicates as basic constructs to represent prerequisites and (possible) conse-
quences of attacks. For example, a scanning attack may discover UDP services vulnera-
ble to certain buffer overﬂow attacks. We can use the predicate UDPVulnerableToBOF
(VictimIP, VictimPort) to represent this discovery. In general, we use a logical formula,
78
P. Ning, Y. Cui, and D.S. Reeves
i.e., logical combination of predicates, to represent the prerequisite of an attack. Thus,
we may have a prerequisite of the form UDPVulnerableToBOF (VictimIP, VictimPort) ∧
UDPAccessibleViaFirewall (VictimIP, VictimPort). To simplify the discussion, we restrict
the logical operators to ∧ (conjunction) and ∨ (disjunction).
We use a set of logical formulas to represent the (possible) consequence of an attack.
For example, an attack may result in compromise of the root account as well as modiﬁ-
cation of the .rhost ﬁle. We may use the following set of logical formulas to represent the
consequence of this attack: {GainRootAccess (IP), rhostModiﬁed (IP)}. This example
says that as a result of the attack, the attacker may gain root access to the system and the
.rhost ﬁle may be modiﬁed.
Note that the consequence of an attack is the possible result of the attack. In other
words, the consequence of an attack is indeed the worst consequence. (Please read [6] for
details.) For brevity, we refer to possible consequence simply as consequence throughout
this paper.
3.2 Hyper-alerts and Hyper-alert Correlation Graphs