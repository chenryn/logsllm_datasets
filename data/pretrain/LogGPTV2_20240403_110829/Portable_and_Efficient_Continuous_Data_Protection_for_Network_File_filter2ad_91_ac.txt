d
User
Kernel
g
c
b
f
TCP/IP
Stack
h
a
NFSD
e
d
User
Kernel
Kernel
Module
TCP/IP
Stack
g
c
b
f
h
a
NFSD
e
d
(A) NFS
(B) UCDP-I
(C) UCDP-K
Request Path
* Bold lines indicate path elements on which large data payload may appear
Reply Path
Figure 4. ThisﬁgureillustratestheNFSpacketprocessingpathinUCDP-Ianditsoverheadduetocontextswitch,
memory copy and user-level processing. In UCDP-K, a kernel packet interception module reduces the overhead by
providingshort-cutpathandeliminatingthecopyoflargedatapayload.
the logging server fail, data is lost; if the disk on the pro-
tected NFS server fails, data can be copied from the logging
server; if the disk on the logging server fails, current data
can be copied from the protected NFS server but the old data
is lost; if both disks are working but they lost synchroniza-
tion with each other, they need to run ”fsck” to guarantee
local ﬁle system consistency.
2.3 UCDP-I:
Integrating Logging with
Protected File Server
Both UCDP-O and UCDP-A log ﬁle updates on a dedi-
cated logging server, and thus are more transparent to the
protected ﬁle server in terms of performance impact and
deployment simplicity. In contrast, UCDP-I integrates ﬁle
update logging to an existing network ﬁle server without re-
quiring additional hardware. There are three design changes
in the transition from UCDP-A to UCDP-I: (1) UCDP-I
does not need the ﬁle handle map because there is only
one copy of the protected ﬁle system in the UCDP-I archi-
tecture. (2) UCDP-I needs to process both read and write
requests as well as their responses, because the protected
network ﬁle server is logically built on top of the ﬁle up-
date logging module of UCDP-I. In contrast, UCDP-O and
UCDP-A only need to process write requests, and do not
need to touch the replies to read or write requests. (3) The
undo logging in UCDP-I has to be done synchronously, be-
cause a request cannot be serviced before its before-image
is saved. As a result, the logging overhead is added to the
latency of normal request processing.
Logically, each incoming NFS request ﬁrst goes to
UCDP-I’s user-level ﬁle update logging module or NFS
proxy, which modiﬁes the request properly and sends it to
the local NFS daemon in the kernel, which in turn sends
a reply back. The ﬁle update logging module converts the
reply into a response packet and sends it back to the request-
ing NFS client. In this design, as shown in Figure 4(B), the
ﬁle update logging module acts like an NFS proxy.
If an NFS request involves only one data block, UCDP-I
needs to determine whether the request should be directed
to the base image or to the overwrite pool. If it should go
to the overwrite pool, the request’s parameters (ﬁle handle,
offset, count) need to be modiﬁed ﬁrst. If the request in-
volves more than one block, UCDP-I needs to check each
block and if necessary, splits the request into multiple re-
quests. After receiving a reply, UCDP-I may need to mod-
ify the ﬁle handle and attribute information if the request
has been directed to the overwrite pool. If an incoming re-
quest is split into multiple requests, UCDP-I reassembles
their replies into one reply and sends the whole reply back
to the requesting NFS client. In case some of these replies
are successful and some are not, UCDP-I resolves the in-
consistency and returns a coherent reply.
Figure 3 shows the data structures used in UCDP-I,
which are similar to those in UCDP-A except it does not
need a mirror ﬁle system or ﬁle handle map.
2.4 UCDP-K: Reducing Context Switch-
ing and Memory Copying overhead
With user-level implementation, an NFS request and its
reply are passed between the kernel and the user-level ﬁle
update logging module multiple times in UCDP-I. UCDP-
K introduces a special kernel module to reduce this con-
text switching and memory copying overhead. Figure 4 il-
lustrates the difference in packet processing path between
UCDP-I and UCDP-K. When the kernel module receives
an NFS request/reply, UCDP-K processes it in one of the
following three ways:
• Path-0: Forwarding the request/reply to the in-kernel
NFS daemon/NFS-client directly (a→d/e→h in Fig-
ure 4(C)), if the user-level NFS proxy does not need
to modify the request/reply, e.g., the readdir com-
mand.
• Path-1: Forwarding the request/reply to the in-kernel
NFS daemon as well as the user-level NFS proxy (a→b
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:53:28 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007and a→d in parallel /e→f and e→h in parallel in Fig-
ure 4(C)) if the request/reply does not need to be mod-
iﬁed, but needs to be recorded, e.g., the create com-
mand.
• Path-2: Forwarding the request/reply to the user-level
NFS proxy if the request/reply potentially needs to be
modiﬁed (a→b→c→ d→e→f→g→h in Figure 4 (C)),
e.g., the read or write command.
Path-0 represents the zero-overhead path, which is as fast
as normal kernel-level NFS processing. Path-2 involves two
context switches/memory copies because the original re-
quest and reply have to be sent to the user-level NFS proxy,
and after user-level processing, the modiﬁed request or re-
ply has to be sent back to kernel and forwarded to the NFS
daemon or requesting NFS client. The additional overhead
affects not only the CPU utilization but also the end-to-end
latency experienced by the NFS requests. Path-1 incurs only
one additional context switching/memory copying for send-
ing a packet to the user-level daemon. The overhead affects
only the CPU utilization but not the end-to-end latency be-
cause the user-level processing is not on the critical path of
NFS packet processing.
The intelligent demultiplexing scheme directly moves to
the NFS daemon those NFS requests and replies that are
not at all related to continuous data protection. However, in
many cases an NFS reply only requires very simple mod-
iﬁcation. For example, the getattr reply has complete
correct content except that ﬁle length, which needs to be
changed from baselen to userlen according to the ﬁle
length map. It is the same for many of the replies to read
and write where the requests are directed to the base
image. Therefore we introduce another optimization into
UCDP-K called in-kernel reply modiﬁcation. When a user-
level CDP system sends a request to the NFS daemon (step
c in Figure 4(C)), whenever possible it also gives the ker-
nel module speciﬁc instructions on how to perform the sim-
ple modiﬁcation when the corresponding reply arrives (step
e). With this optimization, many NFS replies that used
to take Path-2 can now take the less expensive Path-0 or
Path-1. This optimization is particularly effective for read
reply that contains large data payload.
The last optimization in UCDP-K is write payload de-
coupling, which reduces the memory copying overhead of
write requests. A write request always needs to
be processed by the user-level NFS proxy. However, be-
cause usually the user-level processing does not touch the
payload, the kernel module can save a write request inside
the kernel and forward only the request header to the user-
level module (step b). When the NFS proxy sends the mod-
iﬁed header back, the kernel module replaces the old header
with the new header. In case the NFS proxy does need the
payload because the request needs to be split, it will make
another system call to explicitly retrieve the request’s data
payload.
3 Performance Evaluation
In this section, we evaluate and compare the run-time
performance overheads of the four user-level continuous
data protection schemes using micro benchmarks, the Har-
vard NFS traces [9], the SPECsfs 3.0 benchmark [5]. By de-
fault all the machines are equipped with the same hardware
conﬁguration (1.4GHz Pentium IV CPU, 500 MB mem-
ory, 100Mbps Ethernet card) and OS platform (Redhat 7.2
with Linux kernel 2.4.7-10). The base case for performance
comparison is the vanilla NFS server on the same platform,
which sets the lower bound on the performance overhead of
all CDP implementations.
3.1 Eﬀects of Non-Overwrite Logging
A vanilla NFS server services write requests using in-
place updates, whereas UCDP-A, UCDP-I and UCDP-K
use the non-overwrite strategy. Under the non-overwrite
strategy, random overwrite operations are turned into se-
quential writes to the overwrite pool if there are enough
contiguous free virtual blocks. At the same time, sequen-
tial reads may become random reads if the ﬁle blocks have
been overwritten randomly and dispersed in the overwrite
pool. As a result, the non-overwrite strategy may perform
better than in-place updates for workload dominated by ran-
dom writes, but perform worse for workload dominated by
sequential reads after random writes.
To quantify the performance impact of non-overwrite
logging strategy, we constructed the following micro bench-
mark for the vanilla NFS and UCDP-K. The experiments
use a server with 256MB RAM and a client with 128MB
RAM. The server may run vanilla NFS or UCDP-K. The
client is a generic NFS client. First we created a 500MB
ﬁle on the server through sequential write from the client.
In this setup, there is no cache hit on either the client side
or the server side. The sequential write throughput for both
vanilla NFS and UCDP-K is 11MB/sec.
Then we performed a sequence of overwrite operations
of the size of 4096 bytes at random ﬁle offset of the 500MB
ﬁle until the size of overwrite pool reaches 2GB, which
produces sufﬁcient disk layout difference between vanilla
NFS and UCDP-K. Under vanilla NFS the disk utiliza-
tion is 100% and the write throughput is 1.54MB/sec. Un-
der UCDP-K, the disk utilization is 22.5% and the write
throughput is 10.23MB/sec. Overall, the disk access efﬁ-
ciency of UCDP-K is 30 times higher than the vanilla NFS.
This result shows that the non-overwrite strategy behaves
similarly to log-structured ﬁle system [20], which is de-
signed speciﬁcally to convert random writes into sequential
writes.
Finally, we performed a sequence of sequential read op-
erations against the same 500MB ﬁle with a request size of
4096 bytes. Under vanilla NFS, the disk utilization is 18.6%
and the read throughput is 4.76 MB/sec. Under UCDP-K,
the disk utilization is 94.4% and the read throughput is 0.87
MB/sec. Overall, the disk access efﬁciency of UCDP-K is
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:53:28 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007800
700
600
500
400
300
200
100
)
c
e
s
/
s
p
o
(
t
u
p
h
g
u
o
r
h
T
0
0
SPEC load 700
NFS & UCDP-A
UCDP-O
UCDP-I
UCDP-K
24
12
84
Percentage of Update Request(%)
36
48
60
72
)
c
e
s
m
(
y
c
n
e
t
a
L
t
s
e
u
q
e
R
-
r
e
P
e
g
a
r
e
v
A
96
SPEC load 500
20
15
10
5
SPEC load 200
NFS
UCDP-I
UCDP-K
0
0
24
12
84
Percentage of Update Request (%)
72
36
48
60
96
Figure 5. Throughput comparison as the percentage of
writerequestsintheinputworkloadvaries.
Figure 6. Average request processing latency compari-
sonasthepercentageofwriterequestsintheinputwork-
loadvaries.
28 times worse than the vanilla NFS server. Periodic clean-
ing can mitigate the loss of sequential read locality by mov-
ing the current versions of those ﬁle blocks that have be-
come read-only from the overwrite pool to the base image.
3.2 Comparison among Continuous Data
Protection Schemes