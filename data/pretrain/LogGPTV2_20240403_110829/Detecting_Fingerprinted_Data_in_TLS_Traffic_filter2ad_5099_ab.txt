of record size sequences corresponding to diﬀerent lengths
of preﬁxed random data.
With this deﬁnition even large-scale ﬁngerprinting of a
large number of ﬁles is feasible, as we demonstrate with our
Implementation in Section 6.
5. DATA DETECTION
We next want to detect the deﬁned ﬁngerprints in real
In this section we propose methods
TLS communication.
that handle both noisy data and large data bases.
To eﬀectively handle noisy ﬁngerprints captured in a real-
istic scenario, we apply classiﬁcation methods from the ﬁeld
of machine learning. Each method is trained with generated
ﬁngerprints and tested on captured TLS traﬃc.
For all methods, our training set D = (xi, yi) consists of
the generated ﬁngerprint sequences labeled with the corre-
sponding ﬁles, i.e.
xi = Φω(Fi)
yi = Fi
(2)
(cid:24) 214
(cid:25)(cid:27)
ω
.
(3)
for a set of ﬁles {Fi} and a ﬁxed resolution ω ∈ {1, ..., 214}.
In the following, we sometimes write zi = (Φω(Fi), Fi).
Consider a probability measure P (z) on a space Z and a
set of functions Q(z, α) for parameters α ∈ V in an appro-
priate parameter space. Generally, we want to minimize the
expected risk functional
(cid:90)
R(α) =
Q(z, α)dP (z).
(4)
Here, P (z) is initially unknown and we have only given inde-
pendent and identically distributed random variables zi (i =
1, ..., l), corresponding to the labeled ﬁles Fi (i = 1, ..., l).
This reduces to minimizing the empirical risk functional
l(cid:88)
l(cid:88)
i=1
i=1
Remp(α) =
=
1
l
1
l
Q(zi, α)
Q ((Φω(Fi), Fi), α) .
(5)
(6)
Identiﬁcation of ﬁngerprints is clearly a pattern recognition
problem so we will choose Q to be the binary loss-function
L (f (z, α), F ) =
f (z, α) = F
f (z, α) (cid:54)= F
(7)
(cid:26) 0 if
1 if
Therefore, we want to optimize Remp(α) by ﬁnding an op-
timal function f in a hypothesis function space H (see [6]
and [9]). To achieve this we apply Support Vector Machines
(SVM) [8], decision trees, random forests, and extremely
randomized trees [4].
6.
IMPLEMENTATION AND EVALUATION
In the following we present details of our implementation
and further evaluate the proposed methods for identiﬁcation
of data ﬁngerprints in TLS communication.
6.1 Generation of Data Fingerprints
For evaluation purposes we wrote a Python script that
uses the OpenSSL library to implement a TLS (v1.0) server
(encrypting symmetrically with AES-256 in the record layer).
TLS fragments are compressed with Deﬂate (see Section
3.2). A TLS client periodically sends ﬁles to the TLS server
and triggers a network monitoring program (tshark) for cap-
turing the generated TLS traﬃc. We have automated this
process so that the ﬁngerprints Φω(F) are directly stored
in separate capture ﬁles, one for each source ﬁle. We chose
the resolution ω = 500 so that the number of TLS record
of a ﬁngerprint is
length sequences (cid:0)Skω
(F)(cid:1)
i
i=1,...,n(Rω|F )
#Φω(F) = 33.
Regarding performance, generation of the ﬁngerprints takes
some time but can be highly parallelized. In our setting we
are able to ﬁngerprint approximately 75 MB/h, of course
slightly depending on the underlying hardware.
We have exemplary depicted the sequences of TLS record
with k = 0 for 7 diﬀerent MP3 ﬁles
i=10,...,35
lengths(cid:0)Skω
i
in Figure 3.
(F)(cid:1)
It can be directly seen that each ﬁle has a characteristic
sequence of TLS record lengths that depends on the distri-
bution of entropy within the ﬁle.
6.2 Comparison of File Formats
First, let us have a look on how the described machine
learning algorithms perform on diﬀerent ﬁle formats. We
chose the MP3 format for audio ﬁles, ELF for binary exe-
cutable ﬁles, PDF for document ﬁles, the JPG format for im-
age ﬁles, and the WEBM format for video ﬁles.
To detect ﬁles from a set F in a realistic scenario we would
train the learning algorithm of choice with the training set
Tω = {Φω(F) | F ∈ F}
(8)
for an arbitrary but ﬁxed ω ∈ {1, ..., 214}. We would then
intercept TLS traﬃc, extract the sequence of sizes of com-
pressed TLS fragments and test them on the trained ma-
chine. If the learning machine classiﬁes the sequence with a
636Figure 3: TLS record lengths for 7 diﬀerent MP3 ﬁles.
Figure 4: Accuracies versus number of ﬁles.
probability above a predeﬁned threshold, we have detected
the transmission of the ﬁle. For convenience, we take a sub-
set T (cid:48) ⊂ T , train our machines with T \ T (cid:48) and evaluate the
machines on the unseen set T (cid:48). In this evaluation setting we
can easily apply cross-validation on T for robust results.
We will write Fx for the set of all ﬁles of format x in the fol-
lowing. Further, we show that it is not necessary to take all
sizes of compressed TLS fragments (cid:0)Skω
i=1,...,n(Rω|F )
of ﬁles F ∈ Fx into account. We achieve decent accuracies
even by restricting the features to a subset
(F)(cid:1)
i
Θ ⊂ {1, ..., n(Rω|F )}
(9)
and thereby omitting ﬁngerprint information. For this ﬁrst
comparison of accuracies for diﬀerent ﬁle formats, we do not
apply any feature selection algorithms: We simply choose
the number of TLS records 1 − 12 from the recorded ﬁnger-
prints as input features, i.e. Θ = {1, ..., 12}. The reason for
this choice of Θ were the relatively small ﬁle sizes of trans-
mitted JPG ﬁles, so that the smallest ﬁle was sent within
12 TLS records. We set the resolution to ω = 500 (so that
= 33). Formally, this yields the evaluation sets T x
ω,Θ
(cid:108) 214
(cid:109)
ω
T x
ω,Θ = T x
(cid:26)(cid:16)
500,{1,...,12}
(F)
Sk∗500
i
=
(cid:17)
| F ∈ Fx, k = 1, ..., 33
i=1,...,12
(cid:27)
(10)
,
(11)
where x is one of the formats MP3, ELF, PDF, JPG, or WEBM.
As common notation in the machine learning literature,
we deﬁne the accuracy of a learning machine to be the frac-
tion of correct predictions: If a learning algorithm classiﬁes
the recorded samples from TLS traﬃc of a ﬁle Fi to be the
ﬁle ˜Fi, the fraction of correct predictions a (i.e., the accu-
racy) over the total number of n samples is deﬁned as
a(F, ˜F) =
1
n
1
(Fi= ˜Fi),
(12)
i=1
where 1 is the indicator function.
To compare the accuracy of the chosen machine learn-
ing algorithms for diﬀerent ﬁle formats, we ﬁngerprinted
n(cid:88)
530 ﬁles, 106 ﬁles for each format. Applying 5-fold cross-
validation on the evaluation sets T x
500,{1,...12} then yields the
accuracies depicted in the following Table 1:
SVM
DecTree
ExtraTree
RandForest
ELF
0.945
0.888
0.909
0.910
MP3
0.968
0.914
0.953
0.951
PDF
0.979
0.943
0.978
0.977
JPG
0.801
0.784
0.798
0.794
WEBM
0.690
0.668
0.691
0.689
Table 1: Accuracy of single ﬁle identiﬁcation for ﬁles
of diﬀerent formats.
We clearly see that all learning algorithms perform very
good on ELF, MP3, and PDF ﬁle formats. And although JPG
pictures are already strongly compressed, we can achieve an
identiﬁcation rate of up to 80% with SVMs. Videos in the
WEBM format, however, cannot be detected properly with the
proposed methods. For the SVM, we chose a linear kernel,
since more advanced kernels (e.g. Gaussian, polynomial) did
not result in better accuracies. This shows that the samples
are almost completely linearly separable.
6.3 Large Scale Detection
Now that we have seen the feasibility of our approach for
diﬀerent ﬁle formats, we evaluate our methods on a larger
data base. Therefore, we ﬁngerprinted 5434 MP3 ﬁles with
resolution ω = 500 to yield the sample set T MP3
500,Θ. In the
remainder of this section, we apply SVMs, decision trees,
extra trees, and random forests to identify these 5434 ﬁn-
gerprinted MP3 ﬁles in TLS traﬃc.
Let us ﬁrst discuss which features Θ of the ﬁngerprints we
take into account. We already achieve very good accuracies
by considering the ﬁrst 30 sizes of compressed TLS frag-
ments of the ﬁles, i.e. Θ(cid:48) = {1, ..., 30}. We did not apply
feature selection algorithms since we assume the complexity
distribution over ﬁles to be randomly distributed. This way,
we do not prefer any feature above others, as for example
we would do by applying principal component analysis.
Figure 4 shows how the accuracy of the proposed learn-
ing algorithms depends on the number of ﬁngerprinted ﬁles.
Here we apply a Savitzky-Golay ﬁlter for curve smoothing.
637Accuracy
DecTree ExtraTree RandForest
0.803
0.935
0.933
Table 2: Accuracy of single ﬁle identiﬁcation within
5434 MP3 ﬁles.
Figure 5: Accuracy versus tree depth.
The accuracies for up to 800 MP3 ﬁles show that our approach
is likely to scale to larger data bases. The accuracies do not
drop immediately when increasing the number of ﬁles, in-
stead they decline with a very small rate. The accuracies
seem to be stable even for large sets of ﬁles.
We are able to identify 5434 MP3 ﬁles with high accuracy as
shown in Table 2. Again, we applied 5-fold cross-validation
to calculate the accuracies. This shows the feasibility of our
approach even for a large data base. Such good results re-
garding identiﬁcation are crucial for future implementations
of practical tools.
Regarding evaluation time, the SVM training complexity
is roughly between O(n2) and O(n3) (depending on parame-
ters and the choice of kernel). Compared with the complex-
ity O(log(n)) of decision trees, this is a practical disadvan-
tage of SVMs. For real-time detection tree-based algorithms
seem more suitable than SVMs, due to their fast prediction
time. However, the choice for a real-time detection system
depends on the available hardware resources.
To give an impression about how much model detail is
necessary for the tree-based algorithms, we calculated accu-
racies in dependence of the maximal tree depth in Figure 5.
Therefore we conducted 5-fold cross-validation on 300 MP3
ﬁles. We see that Extra Trees perform best with small tree
sizes. However, choosing the tree size big enough, eventually
all tree based algorithms will give very good results.
7. COUNTERMEASURES
The obvious countermeasure to avoid the presented infor-
mation leakage of ﬁngerprinted data is to switch oﬀ compres-
sion at all. However, in some domains such as mobile devices
compression is heavily required and cannot be switched oﬀ
for performance reasons.
Another countermeasure would be to add random padding
to the compressed TLS fragments. That would drastically
increase noise and probably protect against practical ﬁnger-
printing. The immediate tradeoﬀ here would be an increase
of data size with a direct negative impact on transmission
performance.
Kelley and Tamassia [5] introduce a theoretical framework
that allows security proofs in schemes combining compres-
sion and encryption. They present a new cipher which prov-
ably does not leak information (within their framework).
8. CONCLUSION AND FUTURE WORK
We demonstrated an approach to eﬃciently detect known
data in TLS secured communication channels. With our so-
lution, we cover several use cases in communication security.
Beyond obvious use cases such as detection of illegal content
in TLS secured communication channels, our approach re-
sults in loss of anonymity of communication participants, if
they can be linked to speciﬁc ﬁles.
Our solution is not limited to TLS but may aﬀect other
communication protocols that combine compression with en-
cryption in a similar way as TLS.
9. REFERENCES
[1] K. Bhargavan, C. Fournet, M. Kohlweiss, A. Pironti,
and P. Strub. Implementing TLS with veriﬁed
cryptographic security. In 2013 IEEE Symposium on
Security and Privacy (SP), pages 445–459. IEEE,
2013.
[2] S. Chen, R. Wang, X. Wang, and K. Zhang.
Side-channel leaks in web applications: A reality
today, a challenge tomorrow. In 2010 IEEE
Symposium on Security and Privacy (SP), pages
191–206. IEEE, 2010.
[3] K. P. Dyer, S. E. Coull, T. Ristenpart, and
T. Shrimpton. Peek-a-boo, I still see you: Why
eﬃcient traﬃc analysis countermeasures fail. In 2012
IEEE Symposium on Security and Privacy (SP), pages
332–346. IEEE, 2012.
[4] P. Geurts, D. Ernst, and L. Wehenkel. Extremely
randomized trees. Machine learning, 63(1):3–42, 2006.
[5] J. Kelley and R. Tamassia. Secure compression:
Theory & practice. IACR Cryptology ePrint Archive,
2014:113, 2014.
[6] T. Poggio and S. Smale. The mathematics of learning:
Dealing with data. Notices of the AMS, 50(5):537–544,
2003.
[7] A. D. Rubin, M. Green, S. Checkoway, M. Rushanan,
and P. D. Martin. Classifying network protocol
implementation versions: An OpenSSL case study.
2013.
[8] B. Sch¨olkopf and A. J. Smola. Learning with kernels:
Support vector machines, regularization, optimization,
and beyond. MIT Press, 2001.
[9] V. N. Vapnik. An overview of statistical learning
theory. IEEE Transactions on Neural Networks,
10(5):988–999, 1999.
[10] J. Ziv and A. Lempel. A universal algorithm for
sequential data compression. IEEE Transactions on
Information Theory, 23(3):337–343, 1977.
638