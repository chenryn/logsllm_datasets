MPI Space Overhead
Linux Audit
LPM-HiFi (Raw - Gzip)
Linux Audit
LPM-HiFi (Raw - Gzip)
BEEP
MPI
ProTracer (MB)
Table 1: Space Overhead
HTTP Connection
Command
Document File
Tab
Command
Request
File
Command
Video File
Video File
File
Command
Apache
Bash
Evince
Firefox
Krusader
Mplayer
Wget
Most
MC
MPV
Nano
Pine
Vim
W3M
Xpdf
Yafc
ProFTPd
SKOD
TinyHTTPd
Transmission
FTP Connection
FTP Connection
HTTP Connection
Torrent File
File
Tab
Document File
FTP Connection
15.38%
0.45%
3.72%
42.16%
26.54%
0.43%
0.05%
0.93%
0.04%
0.09%
0.29%
8.11%
4.61%
5.99%
8.94%
18.41%
2.23%
38.74%
0.03%
3.44%
12.87%
0.34%
4.98%
38.23%
24.53%
0.33%
0.04%
0.75%
0.04%
0.03%
0.11%
6.09%
3.45%
3.89%
5.32%
18.33%
2.32%
30.45%
0.07%
1.78%
0.64%
0.01%
0.25%
1.01%
0.09%
0.01%
0.00%
0.01%
0.00%
0.00%
0.01%
0.27%
0.17%
0.17%
0.32%
1.03%
0.12%
1.07%
0.00%
0.09%
5.37%
0.41%
0.04%
18.20%
5.71%
0.42%
0.05%
0.90%
0.04%
0.09%
0.01%
7.28%
2.11%
2.68%
2.72%
0.12%
0.13%
24.67%
0.03%
2.60%
3.75%
0.34%
0.04%
13.24%
4.89%
0.33%
0.04%
0.75%
0.04%
0.03%
0.01%
4.09%
1.27%
1.99%
1.08%
0.12%
0.13%
18.23%
0.07%
0.87%
0.16%
0.01%
0.00%
0.52%
0.24%
0.01%
0.00%
0.01%
0.00%
0.00%
0.00%
0.13%
0.06%
0.10%
0.04%
0.01%
0.01%
0.19%
0.00%
0.04%
22.12
1.01
0.22
593.23
2.31
4.33
1.78
3.43
0.34
0.58
8.23
34.23
24.98
25.35
43.24
8.34
17.23
145.26
0.45
26.34
20.08
0.78
0.21
228.54
2.31
4.33
1.78
1.89
0.34
0.58
2.46
14.32
20.35
22.73
37.48
8.23
9.48
73.26
0.45
18.27
Figure 14: Run time overhead for each applications (Overhead percentage v.s. applications)
in Figure 14. For each program, we have eight bars.
1 MPI-Native: the overhead of MPI without any prove-
nance system over native run. 2 MPI-ProTracer: the
overhead of MPI over ProTracer. 3 MPI-LPM: the over-
head of MPI over LPM-HiFi. 4 MPI-Audit: the overhead
of MPI over Linux Audit. The other four bars denote the
overhead of BEEP. As we can see from the graph, most
applications have less than 1% run time overhead for all
situations, which is acceptable. Comparing with BEEP,
MPI shows less overhead in all cases. The low run time
overhead is due to the following factors. Firstly, compared
with the original program, the number of instrumented
instructions is quite small. Secondly, most of the instruc-
tions are rarely triggered. Thirdly, our instrumentation
mainly contains memory operations like comparing the
newly assigned identifier value with the cached value.
In this experiment, we measure the effectiveness of the
annotation miner and the number of annotations even-
tually added. The annotation results are shown in Ta-
ble 2. We only show some representative programs as
the others have similar results. We present the applica-
tions in the first column, and their sizes (measured by
SLOCCount [13]) in the second column. In the next four
columns, we show the number of annotations needed for
@identifier, @indicator, @channel, and @delegator. For
each program, we provide two or more perspectives, as
denoted by the number of @identifier annotations. In the
last column, we show the instrumentation places automat-
ically identified by our compiler pass. Less than 20% of
these places were covered by our profiling runs. In other
words, a training based method like that in BEEP/Pro-
Tracer would not be able to cover all these places.
4.2 Annotation Efforts
Table 2: Annotation Efforts
Application
LOC
Annotation
ID
IND
Chann
DEL
Vim
Yafc
Firefox
TuxPaint
Pine
Apache
MC
ProFTPd
Transmission
W3M
313,283
22,823
8,073,181
41,682
353,665
168,801
135,668
307,050
111,903
67,291
3
2
3
2
2
2
2
3
2
2
3
3
32
2
2
2
2
3
4
2
2
0
0
0
2
0
1
0
0
0
0
1
1
0
0
1
0
1
1
1
Inst
878
111
6,867
121
746
2,437
3,332
4,905
66
3,718
Figure 15: Annotation miner results
To evaluate the annotation miner, we use the 20 pro-
grams in Table 1. For each program, we report the rank-
ing of the unit/delegator data structures that we eventually
choose to annotate. There are totally 52 of them. All the
6 delegator data structures are correctly ranked the top.
That is because they are mainly used in worker threads,
USENIX Association
26th USENIX Security Symposium    1121
Vim: Fileﬁle_buffermemﬁlexﬁlemark……Vim:Windowwindow_Swininfo_Swinopt_T……Vim:frameframe_S…window_S……Firefox: TabnsGlobal…nsPIDom…nsDom………Firefox: ElensPIDom…nsDOM…nsDom………Fx: WindownsPIWindownsGlobalW…nsDom………HTTP: Reqrequest_rec…req_info…conn_red……HTTP: Connconn_rec……request_rec……which have relatively fewer data structures. For the 46
unit data structures that we eventually annotate, 36 of
them are ranked at the first place, 8 at the second place,
and the remaining 2 at the third place. Figure 15 shows
the reported data structures for Vim, Firefox and HTTPd.
Each plane denotes the results for a perspective. The
highlighted data structures are the ones that we eventually
choose to annotate. The reason why we do not always
annotate the top data structures is that they are typically
the shadow data structures of the real unit data structures.
They usually store meta-data related to units, causing
them to have higher ranks than the real unit data structure.
With the help of the miner, we spent minutes to hours to
finalize the annotations. We argue that such efforts are
manageable. More importantly, they are one-time efforts.
4.3 Attack Investigation
To evaluate MPI’s effectiveness in attack investigation,
we apply it on 13 realistic attack cases used in previous
works [32, 43, 44, 46]. The results show that MPI is able
to correctly identify the root causes with very succinct