yields until the completion is signaled. An atomic counter,
shared between all workers, is decremented before issuing
a new DMA request. If the counter value goes below zero,
the worker stops. For BW_RDWR tests, each worker issues a
DMA Read if the counter is even and a DMA Write when
the counter is odd. A control thread is used to start all the
worker threads and waits for them to complete. The elapsed
time is used to calculate the achieved bandwidth. For band-
width tests 8 million DMA requests are performed. For both
DMA latency and bandwidth tests the DMA descriptors are
configured to target an internal 256 KB SRAM, known as the
Cluster Target Memory. This memory has an access latency
of 50-100 cycles. Enqueuing DMA descriptors incurs a sim-
ilar latency. These latencies add a fixed cost to each DMA
transfer, and while they are specific to the NFP, we expect
other programmable NICs to have similar overheads. The
firmware is implemented in Micro-C, an NFP specific exten-
sion to C. The micro-benchmark suite is implemented with
around 1500 lines of code, with the core requiring around
500 lines of code. The code can be compiled, without exter-
nal dependencies, to run on NFP6000 and NFP4000 based
networking cards.
5.2 NetFPGA
NetFPGA is an open-source community platform [43]. It is
supported by cutting-edge reconfigurable boards, reference
software and hardware designs, as well as the infrastructure
for design development and verification. The latest gener-
ation of NetFPGA board, NetFPGA-SUME, is a PCIe host
adapter card [62] capable of 100Gb/s application speeds. The
board utilizes a large Xilinx Virtex-7 FPGA device incorpo-
rating two PCIe Gen 3 hard-blocks, along with resources such
as QDRII+ and DDR3 memory types.
Hardware. The PCIe micro-benchmark suite is implemented
directly on the FPGA. It enhances the DMA engine described
in [61] with pcie-bench capabilities. The system provides a
simple control interface, allowing the host to select which
micro-benchmark to run along with its parameters. The sys-
tem keeps track of time through a free-running counter,
operating at PCIe core frequency (250MHz), providing a res-
olution of 4ns. Every time the software triggers a new micro-
benchmark, a finite state machine coded into the hardware,
calculates the host addresses and generates the associated
memory read or write request. The design does not use a
FIFO to enqueue DMA requests, instead the DMA requests
are directly passed to the DMA engine. All the memory
requests are generated on-the-fly as the hardware design
allows transmission of a new request every clock cycle.
For latency tests, the system is configured to take a times-
tamp before a DMA read and after the acknowledgment
signal is received. The system records up to 1000 latency
values. For bandwidth tests, the system measures the total
time it takes to perform 1 million transactions. Benchmark
results are written to NetFPGA memory after a benchmark
run, where they can be read back from the host. The FPGA
design is written in Verilog and System Verilog. The micro-
benchmark suite is implemented with around 1200 lines
of code. It can be compiled for NetFPGA-SUME and Xilinx
VC709 boards.
5.3 Kernel drivers
Both the NFP and the NetFPGA implementations use a kernel
driver to initialize the hardware, allocate host memory for
DMAs and provide access to user space programs to control
the execution of the benchmarks and to collect the results.
The NFP pcie-bench driver uses the standard NFP kernel
driver7. It allocates the host side DMA buffers in chunks
of 4MB as this the maximum size which can be allocated
physically contiguous on most Linux kernel versions. The
NetFPGA driver allocates memory either from hugetlbfs
(with 1GB pages) or standard 4KB system pages. hugetlbfs
is the default option as it allows for the easy allocation of
large, physically contiguous areas of memory. Both drivers
provide control over which NUMA node memory is allocated
from and export an interface allowing a user space program
to warm the caches with the controlled portions of the host
DMA buffers. The NFP pcie-bench kernel module is im-
plemented in around 400 lines of code while the NetFPGA
driver is implemented in approximately 800 lines of code.
5.4 Control programs
The execution of benchmarks, gathering of the data and post-
processing of the results is performed by user space programs
in both implementations. The NFP implementation uses a
Python program with a small utility, written in C, to handle
cache warming. The Python program can be used to run
individual tests or a full suite of tests. A complete run takes
about 4 hours and executes around 2500 individual tests. For
latency benchmarks, the control program reads the timing
data of the individual transactions and calculates various
metrics, such as the average, median, min, max and 95th and
99th percentile. Optionally, it generates CDFs, histograms
and time series of the data. For bandwidth tests, the control
program calculates the bandwidth and transaction rate. It is
implemented in 1600 lines of Python and 120 lines of C code.
7https://github.com/Netronome/nfp-drv-kmods
333
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
R. Neugebauer et al.
The NetFPGA control program is implemented in approx-
imately 600 lines of C code and provides a command line
interface to control the individual test parameters. The raw
test results are written to a file for further processing.
5.5 Implementation on other devices
pcie-bench is best implemented on PCIe devices providing
programmatic, fine-grained control over the DMA engines,
such as the Netronome NFP and NetFPGA described above.
There is a growing number of PCIe cards with similar capabil-
ities. For example, the Intel Xeon Phi coprocessor exposes the
device’s DMA engines via descriptor rings both to the Phi’s
cores and the host [18]. There are also a growing number
of programmable NICs with integrated FPGAs, such as the
already mentioned Exablaze NICs [11] and Mellanox’s Pro-
grammable ConnectX-3 Pro [40]. Implementing pcie-bench
on these devices should require roughly the same effort as it
took for the initial two implementations.
Some limited aspects of pcie-bench could potentially
be implemented on non-programmable, commodity NICs
in loopback mode by carefully controlling the locations of
buffers used for packet send and receive. For example, one
could repeatedly enqueue the same packet buffer for trans-
mit and vary the freelist buffers to direct received packets to
a variable window size (or vice versa). The relative changes
in latency or bandwidth may provide some insight into host
side PCIe implementation and changing the alignment of
buffer addresses may reveal limitations of a device’s DMA
engines. Note, however, the results will likely be less accurate
than those obtainable with programmable PCIe devices as
measurements with commodity NICs would always include
overheads for descriptor transfers.
6 EXPERIMENTAL EVALUATION
This section reports the pcie-bench suite in action. Table 1
lists the details of the systems used for the evaluation results
presented in this paper. We focus on systems built around
Intel’s Xeon E5 processors as this is the most common config-
uration used in datacenter servers at the time of writing [55].
We discuss results obtained comparing several generations
of Xeon E5 processors but also include some data from an
Intel Xeon E3 system to compare results with a different pro-
cessor architecture. For all experiments we use a PCIe Gen 3
x8 configuration as it is the default for modern NICs. We ex-
pect the pcie-bench methodology to be equally applicable
to other PCIe configurations including the next generation
PCIe Gen 4 once hardware is available.
6.1 Baseline bandwidth and latency
As the first set of results we present the baseline throughput
of the NFP-6000 and NetFPGA implementations in Figure 4.
334
We compare them to the required bandwidth for 40Gb/s Eth-
ernet as well as a simplified model of achievable throughput
for a PCIe Gen 3 device. The model accurately calculates the
overhead of the physical, data link, and transaction layer of
PCIe but only estimates the overhead of flow control mes-
sages. We measure the throughput of PCIe read, write, and
alternating read/write transfers for different transfer sizes to
a fixed, 8KB host buffer, which is warmed before each test to
eliminate any caching effects. We vary the transfer size from
64B to 2048B in rough increments of powers of 2 but take
additional measurements with −1/ + 1B around important
transfer sizes, such as some cache line or TLP size bound-
aries. All DMA start addresses are cache line aligned and all
tests were performed on the same Xeon E5 2637v3 system to
eliminate any differences in system configuration. Figure 4
shows the results.
In all three data sets, the NetFPGA implementation of
pcie-bench closely follows the PCIe bandwidth calculated
with our model. For PCIe writes the NetFPGA implementa-
tion achieves a slightly higher throughput though. This is
because the model assumes a fixed overhead for flow control
messages which, for uni-directional traffic, would not impact
traffic throughput. The NFP implementation of pcie-bench
generally achieves slightly lower throughput than the NetF-
PGA implementation (but typically achieves throughput suf-
ficient to support 40Gb/s Ethernet rates). The main reason is
that the NetFPGA implementation directly drives the DMA
engines from the internal FPGA memory and does not per-
form any additional processing. In the NFP implementation,
the DMA engines must also transfer data to/from the host
into internal SRAM, and then transfer it further into NFP
internal memory where it can be accessed by the FPCs of
the NFP card. These additional overheads, as well as the re-
quired buffer management and signaling, introduce a slight
performance degradation. Finally, it is worth noting that each
graph shows that neither implementation is able to achieve
a read throughput required to transfer 40Gb/s Ethernet at
line rate for small packet sizes.
Next, we look at the latency of individual DMA transac-
tions varying the transfer size (Figure 5). The setup is the
same as for the bandwidth results described above. Overall,
the latency numbers for both the NFP-6000 and NetFPGA
are of the same order of magnitude, indicating that the bulk
of the latency can be attributed to general PCIe and overall
host overheads. It is very similar across the four generations
of Intel processors we measured and is also in line with the
latency measured using the ExaNIC presented in § 2.
The latency for DMA requests is higher on the NFP-6000
with an initial fixed offset of about 100ns for smaller trans-
fers, and the gap increasing for larger transfers. The reasons
are twofold. The fixed offset can be explained with the over-
head of enqueuing DMA descriptors to the DMA engines,
Understanding PCIe performance for end host networking
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
Name
NFP6000-BDW
NetFPGA-HSW
NFP6000-HSW
NFP6000-HSW-E3
NFP6000-IB
NFP6000-SNB
CPU
Intel Xeon E5-2630v4 2.2GHz
Intel Xeon E5-2637v3 3.5GHz
Intel Xeon E3-1226v3 3.3GHz
Intel Xeon E5-2620v2 2.1GHz
Intel Xeon E5-2630 2.3GHz
no
no
no
2-way
NUMA Architecture Memory OS/Kernel
2-way
Broadwell
Haswell
Haswell
Ivy Bridge
Sandy Bridge
128GB
64GB
16GB
32GB
16GB
Network Adapter
Ubuntu 3.19.0-69 NFP6000 1.2GHz
Ubuntu 3.19.0-43 NetFPGA-SUME
NFP6000 1.2GHz
Ubuntu 4.4.0-31
NFP6000 1.2GHz
Ubuntu 3.19.0-30 NFP6000 1.2GHz
Ubuntu 3.19.0-30 NFP6000 1.2GHz
Table 1: System configurations. All systems have 15MB of LLC, except NFP6000-BDW, which has a 25MB LLC.
(a) PCIe Read Bandwidth
(b) PCIe Write Bandwidth
(c) PCIe Read/Write Bandwidth
Figure 4: Baseline PCIe DMA bandwidth for NFP6000-HSW and NetFPGA-HSW with warm caches.
Figure 5: Median DMA latency for NFP-6000 and NetFPGA;
minimum and 95th percentile are shown as error bars.
which is avoided in the NetFPGA implementation. When
using the NFP’s direct PCIe command interface, designed for
smaller transfers, the NFP-6000 achieves the same latency
as the NetFPGA, further indicating that the majority of the
latency can be attributed to host system. The widening of the
gap as the transfer size increases can be explained by consid-
ering the internal architecture of the NFP (§ 5.1). Every DMA
transfer to/from the host causes an additional transfer from
the internal SRAM to NFP internal memory. This additional
transfer increases the latency depending on the transfer size,
but it is useful for decoupling the DMA request from the
(variable) packet processing times performed by the FPCs.
6.2 Comparing architectures
Figure 5 shows that the minimum and 95th percentile of
DMA latencies for a Intel Xeon E5 system are very close to
the median latency, suggesting that there is little variance
in the latency experienced by PCIe transactions. Figure 6
shows the distribution of 64B DMA read latencies for the
same system (NFP6000-NFP) as a CDF. The graph confirms
that 99.9% of all transactions fall inside a narrow 80ns range
starting with a minimum of 520ns and a median of 547ns. The
maximum latency out of 2 million transactions was 947ns.
Figure 6: Latency distribution for Xeon E5 and E3 for 64B
DMA reads with warm caches.
The figure also shows the result from a Xeon E3 system
of the same processor generation (NFP6000-HSW-E3). The
results stand in stark contrast to those from the Xeon E5
system. The minimum latency on the Xeon E3 system is
actually lower with 493ns but the median is more than double
335
0102030405060025651276810241280153617922048Bandwidth(Gb/s)TransferSize(Bytes)ModelBW40GEthernetBW_RD(NFP6000-HSW)BW_RD(NetFPGA-HSW)0102030405060025651276810241280153617922048Bandwidth(Gb/s)TransferSize(Bytes)ModelBW40GEthernetBW_WR(NFP6000-HSW)BW_WR(NetFPGA-HSW)0102030405060025651276810241280153617922048Bandwidth(Gb/s)TransferSize(Bytes)ModelBW40GEthernetBW_RDWR(NFP6000-HSW)BW_RDWR(NetFPGA-HSW)4006008001000120014001600816326412825651210242048Latency(ns)TransferSize(Bytes)LAT_RD(NFP6000-HSW)LAT_RD(NetFPGA-HSW)LAT_WRRD(NFP6000-HSW)LAT_WRRD(NetFPGA-HSW)00.20.40.60.810100020003000400050006000CDFLatency(ns)NFP6000-HSWNFP6000-HSW-E3SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
R. Neugebauer et al.
with 1213ns. Furthermore, from around the 63rd percentile
the latency increases dramatically with the 90th percentile
being double the median and the 99th percentile being 5707ns.
The 99.9th percentile (11987ns) is an order of magnitude
larger than the median with most latencies beyond being
larger than 1ms up to a maximum latency of 5.8ms.
The differences are also reflected in the bandwidth bench-
marks (not shown) where for DMA reads the Xeon E3 system
only matches the Xeon E5 system for transfers larger than
512B and, for DMA writes, never achieves the throughput
required for 40Gb/s Ethernet for any transfer size.
We can only speculate on the causes for this different
behavior. It seems likely that Intel maintains completely
different PCIe root complex implementations for its Xeon E5
and Xeon E3 lines of processors. Looking in more detail at the
Xeon E3 data, there is no regular pattern to the occurrence of
the longer latencies. We suspect that, in particular the larger
latencies may be related to hidden power saving modes. We
have previously observed large (< 1ms) latencies on Xeon E5
systems before disabling power saving modes in the BIOS.