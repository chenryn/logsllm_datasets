quality model (which only includes the traditional JND-related
factors [29, 30]) already saves 17% of bandwidth.
2. Benefit of 360JND vs. classic JND: Next, if we add three new
360°-specific quality-determining factors into the PSPNR model
(§4) and quality adaptation (§6), we can further save 11% band-
width consumption.
3. Benefit of variable-size tiling: Finally, the PSPNR-aware variable-
size tiling (§5) reduces the bandwidth consumption, over grid
tiling, by another 17%.
SIGCOMM ’19, August 19–23, 2019, Beijing, China
Y. Guan, C. Zheng, X. Zhang, Z. Guo, J. Jiang.
Finally, we run the evaluation with real throughput traces. Fig-
ure 18(b) shows that Pano achieves the same PSPNR with 41-46%
less bandwidth consumption than the viewport-driven baseline.
9 LIMITATIONS OF 360JND MODELING
Our 360JND model (§4) is built on a survey study, where partic-
ipants were asked to watch and rate their experience for videos
that spanned a wide range of viewpoint speeds, DoF differences,
and luminance changes. This is a similar methodology to what was
used in the related work [29, 30]. That said, we acknowledge two
limitations of this approach.
First, the values of 360° video-specific factors are varied in a
specific manner (see details in Appendix), which may not match
how they would vary and be perceived by users in the wild. For
instance, when we emulated different viewpoint moving speeds,
the viewpoint was always moving in the horizontal direction and at
a constant rate. However, when watching a 360° video, a user may
move the viewpoint neither horizontally, nor at a constant speed.
Second, we have only tested the impact of two factors at non-
zero values (Figure 7). We have not tested 360JND under all three
factors at non-zero values. Instead, we assume their effects on
JND are mutually independent, thus could be directly multiplied
(Equation 1). While Figure 8 suggests our 360JND calculation is
strongly correlated with user-perceived quality, Pano could benefit
from a more complete and fine-grained profiling of the relationship
between 360JND and various factors.
10 RELATED WORK
360° video streaming has attracted tremendous attention in indus-
try [3, 7, 55] and academia [25, 32, 33, 35, 36, 41, 52, 59–62, 66]. Here
we survey the work most closely related to Pano.
Viewport tracking: Viewport-driven adaptation is one of the
most popular approaches to 360° videos streaming [32, 33, 35, 55,
62, 66]. The viewport of a user is delivered with high quality, while
other areas are encoded in low quality or not streamed. To accommo-
date slight viewpoint movement, some work takes the recent view-
port and re-scales it to a large region [36, 62], but it may still miss the
real-time viewport if the viewport moves too much [53]. To address
this issue, many viewport-prediction schemes [48, 52, 60, 61] are de-
veloped to extrapolate the user’s viewport from history viewpoint
movements [53], cross-user similarity [25], or deep content anal-
ysis [34]. In addition to predict the viewpoint location, Pano also
predicts the new quality-determining factors (viewpoint-moving
speed, luminance, and DoF) by borrowing ideas (e.g., history-based
prediction) from prior viewport-prediction algorithms.
360° video tiling: Tile-based 360° video encoding is critical for
viewport-adaptive streaming [32, 35, 52, 61, 66]. Panoramic video
is spatially split into tiles, and each tile is encoded in multiple
bitrates, so only a small number of tiles are needed to display the
user’s dynamic viewport. But this introduces additional encoding
overhead as the number of tiles increases. Grid-like tiling is the most
common scheme. Alternative schemes, like ClusTile [68], cluster
some small tiles to one large tile so as to improve compression
efficiency. What is new in Pano is that it splits the video in variable-
size tiles which are well-aligned with the spatial distributions of
the new quality-determining factors.
Bitrate adaptation in 360° videos: Both 360° videos and non-360°
videos rely on bitrate-adaptation algorithms to cope with bandwidth
fluctuations, but 360° videos need to spatially allocate bitrate among
the tiles of a chunk [52, 60] (tiles closer to the viewpoint get higher
bitrates), but non-360° videos only change bitrate at the boundaries
between consecutive chunks (e.g., [43, 58, 64]). While Pano follows
the tile-based bitrate adaptation, it is different in that the importance
of each tile is dependent not only to its distance to the viewpoint,
but users’ sensitivities to its quality distortion.
Just-Noticeable Distortion and perceived quality: Many psy-
chological visual studies (e.g., [29, 30, 67]) have shown that the
sensitivity of Human Visual System (HVS) can be measured by Just-
Noticeable Distortion (JND) [42]. JND has been used in other video
quality metrics (e.g., [30]) to quantify subjective user-perceived
quality, but most of the existing studies are designed for video cod-
ing and non-360° videos. This work aims to leverage the impact of
interactive user behaviors (such as viewpoint movements) on JND
and how users perceive 360° video quality, to achieve higher 360°
video quality with less bandwidth consumption.
11 CONCLUSION
High-quality 360° video streaming can be prohibitively bandwidth-
consuming. Prior solutions have largely assumed the same quality
perception model as traditional non-360° videos, limiting the room
for improving 360° videos by the same bandwidth-quality tradeoffs
as traditional videos. In contrast, we show that users perceive 360°
video quality differently than that of non-360° videos. This differ-
ence leads us to revisit several key concepts in video streaming,
including perceived quality metrics, video encoding schemes, and
quality adaptation logic. We developed Pano, a concrete design in-
spired by these ideas. Our experiments show that Pano significantly
improves the quality of 360° video streaming over the state-of-the-
art, e.g., 25%-142% higher mean opinion score with same bandwidth
consumption).
ACKNOWLEDGMENT
We thank our shepherd Zafar Ayyub Qazi and SIGCOMM review-
ers for their helpful feedback. Xinggong Zhang was supported in
part by Project 2018YFB0803702, ALI AIR project XT622018001708,
and iQIYI. Junchen Jiang was supported in part by a Google Fac-
ulty Research Award and CNS-1901466. Xinggong Zhang is the
corresponding author.
REFERENCES
[1] 360 Cinema on Vimeo - The high-quality home for video. https://vimeo.com/ch
annels/360vr.
[2] 4G/LTE Bandwidth Logs. http://users.ugent.be/~jvdrhoof/dataset-4g/.
[3] Bringing pixels front and center in VR video. https://blog.google/products/goo
gle-ar-vr/bringing-pixels-front-and-center-vr-video/.
[4] DASHjs. https://github.com/Dash-Industry-Forum/dash.js.
[5] A Dataset for Exploring User Behaviors in Spherical Video Streaming. https:
//wuchlei-thu.github.io.
[6] Daydream. https://vr.google.com/daydream/.
[7] Facebook end-to-end optimizations for dynamic streaming. https://code.faceb
ook.com/posts/637561796428084.
[8] FFmpeg. http://ffmpeg.org.
[9] Home - Smart Eye. https://smarteye.se.
[10] How to watch Netflix in VR. https://www.netflix.com/cn/.
[11] HTTP Live Streaming. https://developer.apple.com/streaming/.
[12] iQiyi VR Channel. https://www.iqiyi.com.
[13] Is Video a Game Changer for Virtual Reality? https://www.emarketer.com/Arti
cle/Video-Game-Changer-Virtual-Reality/1016145.
405
Pano: Optimizing 360° Video Streaming with a Better
Understanding of Quality Perception
SIGCOMM ’19, August 19–23, 2019, Beijing, China
[14] Live Virtual Reality (VR) and 360 Degree Streaming Software. https://www.wo
wza.com/solutions/streaming-types/virtual-reality-and-360degree-streaming.
[15] PanoProject. https://github.com/panoproject/PanoProject.
[16] Quartz. https://qz.com/1298512/vr-could-be-as-big-in-the-us-as-netflix-in-fiv
e-years-study-shows/.
[17] Samsung Gear VR with Controller. https://www.samsung.com/global/galaxy/ge
ar-vr/.
[18] Tobii. https://www.tobii.com/group/about/this-is-eye-tracking/.
[19] Virtual reality experiences on Hulu - Stream TV and Movies. https://help.hulu.
com/s/article/guide-to-hulu-virtual-reality?language=en_US.
[20] Virtual reality experiences on Hulu - Stream TV and Movies. https://vr.youku.c
om/.
[21] Vive. https://www.vive.com/us/.
[22] Vocabulary for performance and quality of service. https://www.itu.int/rec/T-R
EC-P.10.
[23] X264. https://www.videolan.org/developers/x264.html.
[24] Rakesh Agrawal, Johannes Gehrke, Dimitrios Gunopulos, and Prabhakar Ragha-
van. 1998. Automatic Subspace Clustering of High Dimensional Data for Data
Mining Applications. In Proceedings of the 1998 ACM SIGMOD International Con-
ference on Management of Data (SIGMOD ’98). 94–105.
[25] Yixuan Ban, Lan Xie, Xu Zhimin, Xinggong Zhang, Zongming Guo, Shengbin
Meng, and Yue Wang. 2018. CUB360: Exploiting Cross-Users Behaviors for
Viewport Prediction in 360 Video Adaptive Streaming. In IEEE International
Conference on Multimedia and Expo (ICME).
[26] Yanan Bao, Huasen Wu, Tianxiao Zhang, Albara Ah Ramli, and Xin Liu. 2017.
Shooting a Moving Target: Motion-Prediction-Based Transmission for 360-
Degree Videos. In IEEE International Conference on Big Data.
[27] K Carnegie and T Rhee. 2015. Reducing Visual Discomfort with HMDs Using
Dynamic Depth of Field. IEEE Computer Graphics and Applications 35, 5 (2015),
34–41.
[28] G. Cermak, M. Pinson, and S. Wolf. 2011. The Relationship Among Video Quality,
Screen Resolution, and Bit Rate. IEEE Transactions on Broadcasting 57, 2 (June
2011), 258–262.
[29] Zhenzhong Chen and Christine Guillemot. 2009. Perception-oriented video
coding based on foveated JND model. In Picture Coding Symposium. 1–4.
[30] Chun Hsien Chou and Yun Chin Li. 1995. Perceptually tuned subband image
coder based on the measure of just-noticeable-distortion profile. IEEE Trans on
Circuits and Systems for Video Technology 5, 6 (1995), 467–476.
[31] Xavier Corbillon, Francesca De Simone, and Gwendal Simon. 2017. 360-degree
video head movement dataset. In Proceedings of the 8th ACM on Multimedia
Systems Conference. ACM, 199–204.
[32] Xavier Corbillon, Gwendal Simon, Alisa Devlic, and Jacob Chakareski. 2017.
Viewport-adaptive navigable 360-degree video delivery. In IEEE International
Conference on Communications.
[33] Fanyi Duanmu, Eymen Kurdoglu, S. Amir Hosseini, Yong Liu, Yao Wang, Fanyi
Duanmu, Eymen Kurdoglu, S. Amir Hosseini, Yong Liu, and Yao Wang. 2017.
Prioritized Buffer Control in Two-tier 360 Video Streaming. In The Workshop on
Virtual Reality and Augmented Reality Network. 13–18.
[34] Ching Ling Fan, Jean Lee, Chun Ying Huang, Kuan Ta Chen, and Cheng Hsin
Hsu. 2017. Fixation Prediction for 360 Video Streaming in Head-Mounted Virtual
Reality. In The Workshop on Network and Operating Systems Support for Digital
Audio and Video. 67–72.
[35] Vamsidhar Reddy Gaddam, Michael Riegler, Ragnhild Eg, Pal Halvorsen, and
Carsten Griwodz. 2016. Tiling in Interactive Panoramic Video: Approaches and
Evaluation. IEEE Transactions on Multimedia 18, 9 (2016), 1819–1831.
[36] Mario Graf, Christian Timmerer, and Christopher Mueller. 2017. Towards Band-
width Efficient Adaptive Streaming of Omnidirectional Video over HTTP: Design,
Implementation, and Evaluation. In ACM on Multimedia Systems Conference. 261–
271.
[37] Hadi Hadizadeh and Ivan V Bajić. 2013. Saliency-aware video compression. IEEE
Transactions on Image Processing 23, 1 (2013), 19–33.
[38] J. F. Henriques, R Caseiro, P Martins, and J Batista. 2015. High-Speed Tracking
IEEE Transactions on Pattern Analysis &
with Kernelized Correlation Filters.
Machine Intelligence 37, 3 (2015), 583–596.
[39] David M. Hoffman, Ahna R. Girshick, Kurt Akeley, and Martin S. Banks. 2008.
Vergence accommodation conflicts hinder visual performance and cause visual
fatigue. Journal of Vision 8, 3 (2008), 33.
[40] A. Hore and D. Ziou. 2010. Image Quality Metrics: PSNR vs. SSIM. In 2010 20th
International Conference on Pattern Recognition. 2366–2369.
[41] Mohammad Hosseini and Viswanathan Swaminathan. 2016. Adaptive 360 VR
Video Streaming: Divide and Conquer! (2016), 107–110.
[42] N. Jayant, J. Johnston, and R. Safranek. 1993. Signal compression based on
models of human perception. Proc. IEEE 81, 10 (Oct 1993), 1385–1422. https:
//doi.org/10.1109/5.241504
[43] Junchen Jiang, Vyas Sekar, and Hui Zhang. 2014. Improving fairness, efficiency,
and stability in http-based adaptive video streaming with festive. IEEE/ACM
Transactions on Networking (ToN) 22, 1 (2014), 326–340.
[44] Jong-Seok Lee and Touradj Ebrahimi. 2012. Perceptual video compression: A
survey. IEEE Journal of selected topics in signal processing 6, 6 (2012), 684–697.
[45] S. Li, J. Lei, C. Zhu, L. Yu, and C. Hou. 2014. Pixel-Based Inter Prediction in Coded
Texture Assisted Depth Coding. IEEE Signal Processing Letters 21, 1 (Jan 2014),
74–78.
[46] Wen-Chih Lo, Ching-Ling Fan, Jean Lee, Chun-Ying Huang, Kuan-Ta Chen, and
Cheng-Hsin Hsu. 2017. 360 video viewing dataset in head-mounted virtual reality.
In Proceedings of the 8th ACM on Multimedia Systems Conference. ACM, 211–216.
[47] Simone Mangiante, Guenter Klas, Amit Navon, Zhuang GuanHua, Ju Ran, and
Marco Dias Silva. 2017. VR is on the Edge: How to Deliver 360 Videos in Mobile
Networks. In Proceedings of the Workshop on Virtual Reality and Augmented
Reality Network (VR/AR Network ’17). 30–35.
[48] Afshin Taghavi Nasrabadi, Anahita Mahzari, Joseph D Beshay, and Ravi Prakash.
2017. Adaptive 360-degree video streaming using scalable video coding. In
Proceedings of the 2017 ACM on Multimedia Conference. ACM, 1689–1697.
[49] Richard A. Normann and Frank S. Werblin. 1974. Control of Retinal Sensitivity.
The Journal of General Physiology 63, 1 (1974), 37–61.
[50] Stefano Petrangeli, Viswanathan Swaminathan, Mohammad Hosseini, and Filip
De Turck. 2017. An HTTP/2-Based Adaptive Streaming Framework for 360
Virtual Reality Videos. In Proceedings of the 25th ACM International Conference
on Multimedia (MM ’17). ACM, New York, NY, USA, 306–314.
[51] E N Pugh. 1975. Rushton’s paradox: rod dark adaptation after flash photolysis.
The Journal of Physiology 248, 2 (1975), 413–431.
[52] Feng Qian, Bo Han, Qingyang Xiao, and Vijay Gopalakrishnan. 2018. Flare:
Practical Viewport-Adaptive 360-Degree Video Streaming for Mobile Devices. In
Proceedings of the 24th Annual International Conference on Mobile Computing and
Networking (MobiCom ’18). ACM, New York, NY, USA, 99–114.
[53] Feng Qian, Lusheng Ji, Bo Han, and Vijay Gopalakrishnan. 2016. Optimizing 360
video delivery over cellular networks. In The Workshop on All Things Cellular:
Operations. 1–6.
[54] Joseph Redmon and Ali Farhadi. 2018. YOLOv3: An Incremental Improvement.
arXiv (2018).
[55] P. Rondao Alface, J.-F. Macq, and N Verzijp. 2012. Interactive Omnidirectional
Video Delivery: A Bandwidth-effective Approach. Bell Labs Tech. J. 16, 1 (2012),
135–147.
[56] R. J. Safranek and J. D. Johnston. 1989. A perceptually tuned sub-band image coder
with image dependent quantization and post-quantization data compression. In
International Conference on Acoustics, Speech, and Signal Processing. 1945–1948
vol.3.
[57] S. Shimizu, M. Kitahara, H. Kimata, K. Kamikura, and Y. Yashima. 2007. View
Scalable Multiview Video Coding Using 3-D Warping With Depth Map. IEEE
Transactions on Circuits and Systems for Video Technology 17, 11 (Nov 2007),
1485–1495.
[58] K. Spiteri, R. Urgaonkar, and R. K. Sitaraman. 2016. BOLA: Near-optimal bitrate
adaptation for online videos. In IEEE INFOCOM 2016 - The 35th Annual IEEE
International Conference on Computer Communications. 1–9.
[59] Kashyap Kammachi Sreedhar, Alireza Aminlou, Miska M. Hannuksela, and Mon-
cef Gabbouj. 2017. Viewport-Adaptive Encoding and Streaming of 360-Degree
Video for Virtual Reality Applications. In IEEE International Symposium on Mul-
timedia.
[60] Lan Xie, Zhimin Xu, Yixuan Ban, Xinggong Zhang, and Zongming Guo. 2017.
360ProbDASH: Improving QoE of 360 Video Streaming Using Tile-based HTTP
Adaptive Streaming. In ACM on Multimedia Conference. 315–323.
[61] Lan Xie, Xinggong Zhang, and Zongming Guo. 2018. CLS: A Cross-user Learning
based System for Improving QoE in 360-degree Video Adaptive Streaming. In
ACM on Multimedia Conference.
[62] Xiufeng Xie and Xinyu Zhang. 2017. POI360: Panoramic Mobile Video Telephony
over LTE Cellular Networks. In Proceedings of the 13th International Conference
on Emerging Networking EXperiments and Technologies (CoNEXT ’17). ACM, New
York, NY, USA, 336–349. https://doi.org/10.1145/3143361.3143381
[63] Richard Yao, Tom Heath, Aaron Davies, Tom Forsyth, Nate Mitchell, and Perry
Hoberman. Oculus VR best practices guide. , Oculus VR pages.
[64] Xiaoqi Yin, Abhishek Jindal, Vyas Sekar, and Bruno Sinopoli. 2015. A Control-
Theoretic Approach for Dynamic Adaptive Video Streaming over HTTP. In ACM
Conference on Special Interest Group on Data Communication. 325–338.
[65] Matt Yu, Haricharan Lakshman, and Bernd Girod. 2015. A framework to evaluate
omnidirectional video coding schemes. In 2015 IEEE International Symposium on
Mixed and Augmented Reality. IEEE, 31–36.
[66] Alireza Zare, Alireza Aminlou, Miska M. Hannuksela, and Moncef Gabbouj. 2016.
HEVC-compliant Tile-based Streaming of Panoramic Video for Virtual Reality
Applications. 601–605.
[67] Y. Zhao, L. Yu, Z. Chen, and C. Zhu. 2011. Video Quality Assessment Based
on Measuring Perceptual Noise From Spatial and Temporal Perspectives. IEEE
Transactions on Circuits and Systems for Video Technology 21, 12 (Dec 2011),
1890–1902.
[68] C. Zhou, M. Xiao, and Y. Liu. 2018. ClusTile: Toward Minimizing Bandwidth
in 360-degree Video Streaming. In IEEE INFOCOM 2018 - IEEE Conference on
Computer Communications. 962–970.
406
SIGCOMM ’19, August 19–23, 2019, Beijing, China
Y. Guan, C. Zheng, X. Zhang, Z. Guo, J. Jiang.
Equipment Oculus GO
CPU Qualcomm Snapdragon 821
Memory
Screen Resolution
Refresh Rate
Fixed pupil distance
3GB
2560 × 1440
72Hz
63.5mm
Table 4: Headset parameters used in JND modeling
Appendices are supporting material that has not been peer re-
viewed.
A APPENDIX
This section presents the detailed methodology of modeling 360°
video JND.
A.1 Survey process
The user study was based on 20 participants (age between 20 and
26). The same 20 participants also did the survey-based performance
evaluation (§8), so the results could be affected by the limited size
of the participant pool. In all tests, the participants watch (synthet-
ically generated or real) 360° videos using an Oculus headset [63],
of which the parameters are summarized in Table 4.
Each participant was asked to watch a video with an increas-
ing level of quality distortion (see the next section for how the
quality distortion was added to a video). Every time the quality
distortion increased, the participant was asked whether he or she
could perceive the quality distortion. We define JND of a video by
the average level of quality distortion that was perceivable for the
first time, across the 20 participants. We repeated this test with
43 artificially generated videos, and each participant watched the
videos in a random order (which helped mitigate biases due to any
specific playing order).
A.2 Test videos
Next, we explain (1) how we artificially generate videos with a
controlled noise level added to an visual object, to emulate the effect
of a specific level of quality distortion, and (2) how we emulate
the viewpoint behavior such that the visual object would appear in
the video with a specific relative moving speed, DoF difference, or
background luminance change.
All test videos were generated by manipulating a basic video
where a small square-shaped foreground object (64×64 pixels) was
located in the center of the screen. The object has a constant grey
level of 50. We refer to the foreground object by U .
Adding controlled quality distortion: To add a controlled qual-
ity distortion on U , we borrow a similar methodology from prior
user study on JND [29, 30]. We randomly picked 50% pixels of U ,
and added a value of Δ to their values (grey level). We made sure
that the resulting pixel values were still within the range of 0 to 255.
By varying the value of Δ from 1 to 205, we created a video with
an increasing level of quality distortion on the foreground object U .
The video was played to each participant until the distortion was
perceived for the first time.
Emulating the effect of relative viewpoint-moving speed: To
emulate the perception of quality distortion under a specific rel-
ative viewpoint-moving speed, we fixed a red spot at the center
of the screen, and moved the foreground object U horizontally at
a specific speed of v. That is, U and the red spot (viewpoint) has
a relative moving speed of v. The participant was asked to look
at the red spot, and report whether he or she could perceive the
quality distortion added onto the object U . This process emulated
the effect of viewpoint moving at a relative speed of v to where
the quality distortion occurred. We tested viewpoint speeds from 0
deg/s to 20 deg/s.
Emulating the effect of luminance changes: To emulate the
perception of quality distortion under certain luminance changes,
each video began with the background luminance set to д + l, and
then reduced to д after 5 seconds. Right after the luminance was
reduced to д, the object U was shown with a gradually increasing
amount of quality distortion. The participant was then asked to
report as soon as the quality distortion was perceived. Although the
report quality distortion may not be the true minimally perceivable
quality distortion (JND), we found the participants always reported
quality distortion within 3 seconds after luminance was reduced.
That suggests the first perceivable quality distortion might be a
reasonable indicator of the real JND under the luminance change
of l. By fixing д at 0 grey level (darkest) and varying l from 0 to
240 grey level, we can test the JND under the different levels of
luminance changes within a short time window of 5 seconds.
Emulating the effect of DoF differences: To emulate the percep-
tion of quality distortion on an object with a specific DoF difference
from the viewpoint, we asked the participants to focus on a static
spot displayed at a DoF difference d (d = {0, 0.67, 1.33, 2}dioptre)
from the foreground object U . Then quality distortion was added
to the object U , and the participants were asked to report when
they first perceived the quality distortion.
Joint impact of two factors: So far, each factor (relative viewpoint-
moving speed, luminance change, DoF difference) was varied sep-
arately with others held to zero. We also tested the JND under
both viewpoint speed and DoF differences at non-zero values si-
multaneously. That is, at each possible relative viewpoint-moving
speed, we enumerated different values of DoF differences, using
the same method described above. Similarly, we also tested the JND
under both object luminance and relative viewpoint-moving speed
at non-zero values. These results were shown in Figure 7.
407