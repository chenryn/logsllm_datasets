### Local Logging and Security

To enhance security, local, client-side logs are transmitted to the RFS server. The client-side logging module is relatively lightweight, comprising 203 lines of modifications to the Linux kernel code and 273 lines of new code for maintaining the logging buffer.

Given a set of per-client system call logs, RFS consolidates them into a single log for contamination analysis, particularly focusing on the sequence of reads and writes. Although there is no global timestamp, RFS synchronizes each per-client system call log with the server-side file update log using RPC message IDs.

### NFS Request Interception

RFS records all update requests sent to the protected file server and ignores all other requests that do not alter the state of the file server. Additionally, RFS disregards requests that update the last access time of file system objects. Upon receiving an update request, the request interceptor forwards it and waits for the corresponding NFS reply. If the reply is successful, the request and its reply are logged into a redo record. Otherwise, both the reply and the corresponding request are discarded. RFS's request interceptor also handles issues related to packet loss and duplicates.

### Redo-to-Undo Log Conversion

The performance of the redo-to-undo log converter is critical. To match the processing capacity of the protected NFS server, which has multiple nfsd processes, the converter uses a multi-threaded architecture. It also maintains a file attribute cache to reduce the frequency of interactions with the mirror NFS server.

As illustrated in Figure 3, the converter includes a dispatcher thread, three I/O threads (receive-redo, receive-reply, and flush-undo), and a configurable number of processing threads. The central data structure is the request queue, which can be in one of four states: Free, Ready, Processing, or Waiting-Reply. Each queue handles requests for the same file system object. A free queue becomes ready after the dispatcher dispatches the first request. Any processing thread can take a Ready queue, label it as Processing, and start processing the requests. If a processing thread needs to wait for a reply from the mirror NFS server, the queue is labeled as Waiting-Reply and is set back to Processing once the reply is received. The queue is labeled as Free when all requests have been processed. The log conversion process must respect the dependencies among NFS requests. For example, a read request for a certain file cannot be dispatched before the create request for the same file has been processed, and a remove request cannot be dispatched before all preceding read and write requests on the same file have been processed.

### Contamination Analysis and Repair

The RFS server-side log records all data, metadata, and directory updates, except for the last access time. The client-side log tracks data read/write dependencies and limited metadata read/update dependencies. Metadata read requests such as readdir, get attribute, and lookup are neither logged nor included in the contamination analysis. This exclusion is due to the high frequency of these operations, which would significantly increase the client-side log size and potentially lead to false positives. Future work will verify this and explore providing contamination analysis at different security levels.

RFS distinguishes between a contaminated file and a contaminated file block. If a file is contaminated, all its blocks are contaminated, but the reverse is not true. A file created by a corrupted process is considered contaminated. If a corrupted process writes to a file block, only that block is contaminated, not the entire file. A process is considered contaminated if:
- It is a child of a contaminated process.
- It reads contaminated file blocks.
- It performs any operation that depends on the existence of a contaminated object, such as read/write blocks, read/write attributes, or any file system update operations on objects that are descendants of the contaminated object in the file system hierarchy.

According to these rules, a process that writes to a contaminated file block is not considered contaminated. However, a process that writes to any block or manipulates the attributes of a contaminated file becomes contaminated. The rationale behind the third rule is that processes that touch contaminated files cannot continue because these files will be deleted during the repair process.

The result of the contamination analysis is a set of (not necessarily contiguous) entries in the server-side undo log that need to be undone. RFS sends each undo operation to the protected NFS server as a normal NFS request, making each undo operation itself undoable.

### Inode Mapping Issue

In theory, the mirror NFS server should be identical to the protected NFS server since they are initialized with the same state and all updates to the protected NFS server are also applied to the mirror NFS server. In practice, this is not always the case due to differences in inode numbers for the same file object on the protected and mirror NFS servers. RFS maintains an NM-inode-map to track the mapping between file handles on these two servers for the same file. The file handle of a redo record, specified with respect to the protected NFS server, needs to be translated through the NM-inode-map to the corresponding file handle on the mirror server before being sent to the mirror server.

Some undo operations create new file system objects in response to file deletion operations. The file handle of the new file object created by an undo operation may differ from that of the deleted file object. Therefore, RFS maintains another inode map (repair-inode-map) to track the association between a file that is eventually deleted and its compensation copy. Table 1 provides an example to illustrate these inode mapping issues.

### Discussion

An important consideration for RFS is its robustness and crash recovery. If the client crashes, the last few entries in the client-side log may be lost, affecting the accuracy of the contamination analysis. However, there is no complex synchronization problem since the client-side and server-side logs are synchronized by RPC message IDs. The situation where the primary NFS server or RFS server crashes is more complicated and can be addressed using write-ahead logging. This issue has been explored in Miguel Castroâ€™s work on Byzantine fault tolerance [21]. Future work will focus on developing a concrete solution for RFS's robustness.

The current RFS prototype's contamination analysis is based on file read/write dependencies among processes, which may be too loose or strict for determining the extent of damage. The next RFS prototype will include an interactive exploration tool for system administrators to examine the validity of the contamination analysis output. RFS also allows system administrators to specify the scope of protection in terms of file partitions, directories, or files on the NFS server.

The first RFS prototype was implemented for NFSv2 and took 3 man-weeks to convert to support NFSv3, demonstrating the portability of the RFS architecture. Most modifications were in the processing thread of the redo-to-undo log converter, including rewriting the parser for NFS requests and replies and the routines to convert redo records into undo records. Although the total number of lines of code involved in this porting is about 5000, most of the data structures and parser code were borrowed from the Linux kernel with minor changes.

Some features of NFSv3, such as returning the associated file handle for symbolic link creation and more frequent file attribute returns, simplified the RFS implementation. For NFSv4, most protocol changes relate to scalability and security and do not affect RFS. The request batching feature can be accommodated with minor modifications to RFS's request interceptor. We expect no significant effort to port RFS to NFSv4. However, we do not yet have concrete experiences with porting RFS to network file servers based on AFS or CIFS.

### Performance Evaluation

RFS facilitates the damage repair process at the expense of additional runtime overhead and resource consumption. The viability of the RFS approach depends on the cost of this additional performance/hardware and how much faster RFS can speed up the repair process. The performance evaluation results presented in this section aim to address these questions.

The experiment testbed consists of five machines running RedHat 7.2 with Linux kernel 2.4.7. There are two NFS clients, one NFS server to be protected, and one mirror NFS server. The performance metrics, including CPU and disk usage, are compared between configurations with and without RFS, and with different hardware setups.