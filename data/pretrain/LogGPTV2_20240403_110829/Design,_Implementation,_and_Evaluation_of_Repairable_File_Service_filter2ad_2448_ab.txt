locally, client-side logs are sent to RFS server to be more
secure. The client-side logging module is pretty thin, and
consists of 203 lines of changes to the Linux kernel code
and 273 lines of new code to maintain the logging buffer.
Given a set of per-client system call logs, RFS needs
to coalesce them into a single log for contamination anal-
ysis, particularly with respect to which reads follow which
writes. While there is no global timestamp, RFS synchro-
nizes each per-client system call log with the server-side ﬁle
update log by RPC message ids.
4.2. NFS Request Interception
RFS records all update requests sent to the protected ﬁle
server and ignore all other requests that do not change the
state of the ﬁle server.
In addition, RFS also ignores re-
quests that update the last access time of ﬁle system ob-
jects. Upon receiving an update request, the request inter-
ceptor forwards it and waits until the corresponding NFS
reply comes back. If the reply is successful, the request in-
terceptor logs the original request and its reply into a redo
record. Otherwise the reply and corresponding request are
discarded. RFS’s request interceptor takes care of all the
packet loss and duplicate issues.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:30:00 UTC from IEEE Xplore.  Restrictions apply. 
Intrustion
Detection
Applications
NFS CLIENT
RFS Client
OS
MIRROR
SERVER
Protection
Configuration
Intrusion
Report
......
Intrusion
Report
Syscall log
Syscall log
Redo−to−Undo
Convertor
cleaner
redo record
NM−inode−map
undo log
Contamination
Analysis
Request
Interceptor
NFS
SERVER
Repair
Procedure
Undo
Operations
Repair−inode−map
Log Collection, Analysis and Repair Engine
System Adm
Guidance
Figure 2. The detailed software architecture of the RFS prototype, which is speciﬁcally designed to protects NFSv3 servers. Only
the redo-to-undo converter is protocol dependent. All the other modules are reusable across different network ﬁle access protocols.
Tcp
to Packet
Interceptor
receive−redo
thread
redo record buffer 
Dispatcher
thread
Waiting−Reply free
ready
....
Processing
receive reply
thread
mirror server
get before imagerequest
update mirror request
fh1
Reply
thread3
req1
req3
fh3
req2
req5
req6
request queue
thread2
req4
flush−undo
thread
undo record
buffer
Processing
thread1
Processing
thread 2
Processing
thread 3
Processing
thread 4
inode map cache
attribute cache
Mirror
data
undo log
ino map
Figure 3. The redo-to-undo log converter uses a multi-thread software architecture to maximize the concurrency between disk I/O
and log processing on the RFS machine, which hosts the mirror NFS server as well as the log converter.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:30:00 UTC from IEEE Xplore.  Restrictions apply. 
4.3. Redo-to-Undo Log Conversion
The performance of redo-to-undo log converter is cru-
cial. Since NFS server has multiple nfsd processes, the
redo-to-undo log converter uses a similar multi-threading
structure to match the protected NFS server’s processing
capacity. In addition, the converter maintains a ﬁle attribute
cache to reduce the frequency of contacting the mirror NFS
server.
As shown in Figure 3, the converter has a dispatcher
thread, three I/O threads (receive-redo thread, receive-reply
thread and ﬂush-undo thread), and a conﬁgurable number of
processing thread. The central data structure is the request
queue. Each queue will be in one of the four status: Free,
Ready, Processing, Waiting-Reply. All requests in the same
queue operate on the same ﬁle system object. A free queue
is labeled ready after dispatcher dispatches ﬁrst request into
the queue. Any processing thread can take a Ready queue,
label it as Processing and start processing the requests in the
queue. Whenever a processing thread needs to wait for re-
ply from mirror NFS server, the queue is labeled as Waiting-
Reply, it will be set back to Processing after the reply is
received. The queue will be labeled as Free when all the re-
quests in the queue have been processed. Note that the log
conversion process must respect the dependencies among
NFS requests. For example, a read request for a certain ﬁle
can not be dispatched before the create request for the same
ﬁle has been processed, the remove request for a certain ﬁle
cannot be dispatched before all the preceding read and write
requests on the same ﬁle have been dispatched.
4.4. Contamination Analysis and Repair
RFS server-side log records all data updates, meta-
data updates and directory updates.
except last access
time. RFS client-side log keeps data read/write depen-
dency and limited metadata read/update dependency. Meta-
data read requests readdir, get attribute and
lookup are neither logged nor included in the contami-
nation analysis. One reason is that these operations are very
frequent and will cause dramatic increase in client-side log
size. Another reason is that we suspect these operations will
lead to a lot of false positives. As future work, this needs
to be veriﬁed. Another option is to provide contamination
analysis at different security level. These operations can be
logged and analyzed under high security level.
RFS distinguishes between a contaminated ﬁle and a
contaminated ﬁle block.
If a ﬁle is contaminated, all its
blocks are contaminated. The reverse is not true. A ﬁle
created by a corrupted process is a contaminated ﬁle.
If
a corrupted process writes into a ﬁle block, only that ﬁle
block is contaminated, but the ﬁle itself is not considered
contaminated. With this distinction, RFS considers a pro-
cess contaminated if it
• is a child of a contaminated process,
• reads contaminated ﬁle blocks, or
• performs any operation that depends on the existence
of a contaminated object. These operations include
read/write blocks, read/write attributes, and any ﬁle
system update operations on objects that are descen-
dants of the contaminated object in the ﬁle system hi-
erarchy.
According to the above rules, if a process just writes to a
contaminated ﬁle block, the process is not considered con-
taminated. However, if a process writes to any block or
manipulates the attributes of a contaminated ﬁle, the pro-
cess becomes contaminated. The rationale of the third rule
is that processes that touch contaminated ﬁles can not have
continued because contaminated ﬁles will be deleted in the
repair process.
The result of the contamination analysis is a set of (not
necessarily contiguous) entries in the server-side undo log
that need to be undone. RFS sends each undo operation to
the protected NFS server as a normal NFS request. As a
result, each undo operation itself is also undoable.
4.5. Inode Mapping Issue
In theory, the mirror NFS server should be identical to
the protected NFS server since they are initialized with the
same state and all updates to the protected NFS server are
also carried out at the mirror NFS server. In practice, this
is not the case because the inode numbers corresponding
to the same ﬁle object may be different on the protected
and mirror NFS ﬁle servers. RFS maintains a NM-inode-
map to keep track of the mapping between ﬁle handles on
these two ﬁle servers for the same ﬁle. Because the ﬁle
handle of a redo record is speciﬁed with respect to the pro-
tected NFS server, it needs to be translated through NM-
inode-map to the corresponding ﬁle handle in the mirror
ﬁle server, before the converter sends it to the mirror server.
Some undo operations create new ﬁle system objects in re-
sponse to ﬁle deletion operations. However, the ﬁle handle
of the new ﬁle object created by an undo operation may not
be the same as that of the ﬁle object that is deleted. There-
fore, in the undo process, RFS maintains another inode map
(repair-inode-map) to keep track of the association
between a ﬁle that is eventually deleted and its compensa-
tion copy. Table 1 presents an example to illustrate these
inode mapping issue.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:30:00 UTC from IEEE Xplore.  Restrictions apply. 
nfs operations
mirror operations
R:create a
A:(10)
R:create a
A:(20)
undo record
R:remove a
A:NULL
R:write(10),0,2,xx R:write(20),0,2,xx R:setattr(10),size 0
A:success
R:remove a
A:success
R:create a
A:(11)
A:success
R:lookup a
A:(20)
A:NULL
R:create a
A:(10)
R:read(20),0,2
R:write(10),0,2,xx
A:xx
R:create a
A:(22)
A:NULL
R:remove a
A:NULL
R:write(11),0,2,xx R:write(22),0,2,xx R:setattr(11),size 0
A:success
A:success
A:NULL
NM-inode-map
repair-inode-map
(10, 20)
(11, 22)
(10, 11)
Table 1.
In this example three requests (create, write, remove) are sent to the protected NFS server. The last (remove) is malicious
and needs to be undone. The table presents the operations that are applied to the protected and mirror NFS server, together with the
associated changes to the NM-inode-map and repair-inode-map. (10) means ﬁle with inode number 10, write(10),0,2,xx means write to
ﬁle(10), offset 0, count 2, with data xx. R means REQ, A means ACK. In this example, ﬁle “a” is associated with four inode numbers:
10 on the protected NFS server before repair, 20 on the mirror NFS server before repair, 11 on the protected NFS server after repair,
and 22 on the mirror NFS server after repair.
4.6. Discussion
An important issue of RFS is its robustness or crash re-
covery. If the client crashes, the last few entries of client-
side log may get lost which may effect the contamination
analysis accuracy. But there is no complicated synchro-
nization problem since client-side log and server-side log
are simply synchronized by RPC message ID. The situa-
tion where the primary NFS server crashes or RFS server
crashes is more complicated. Write-ahead logging can be
used to solve the problem. This issue has been explored by
Miguel Castro’s work on byzantine fault tolerance [21]. It
remains our future work to have a concrete solution for the
robustness of RFS.
The contamination analysis in the current RFS prototype
is based on ﬁle read/write dependencies among processes.
This may be too loose or too strict a criterion for determin-
ing the extent of damage. The next RFS prototype will in-
clude an interactive exploration tool for system administra-
tors to interactively examine the validity of the output of
contamination analysis. Along a similar line, RFS allows
system administrators to specify the scope of protection in
terms of a set of ﬁle partitions, directories or ﬁles on the
NFS server.
The ﬁrst RFS prototype was implemented for NFSv2. It
took 3 man weeks to convert it to support NFSv3. This
demonstrates the portability of the RFS architecture. Most
modiﬁcations are with the processing thread of the redo-
to-undo log converter. We need to rewrite the parser for
NFS requests and replies, and the routines to convert redo
record into undo record. Although the total number of lines
of code involved in this porting is about 5000 lines, most of
the data structures and parser code are borrowed from the
Linux kernel code with minor changes.
Some of NFSv3’s features helped to simplify RFS imple-
mentation. In NFSv2, the NFS request to create a symbolic
link does not return the associated ﬁle handle; in NFSv3, it
does. Another example is that NFSv3 returns ﬁle attributes
much more frequently than NFSv2. This reduces the num-
ber of get attribute requests required in the redo-to-undo log
conversion process.
As for NFSv4, most of the protocol changes are related
to scalability and security, therefore does not affect RFS.
The request batching feature can be easily accommodated
by a minor modiﬁcation to RFS’s request interceptor. For
such batching request and reply, multiple log entries will be
constructed in the ﬁle update log. We expect no big effort
to port RFS to NFSv4. However, so far we do not have
any concrete experiences about porting RFS for network ﬁle
servers based on AFS or CIFS.
5. Performance Evaluation
RFS facilitates the damage repair process at the expense
of additional run time overhead and resource consumption.
The viability of the RFS approach thus depends on how
expensive this additional performance/hardware cost is and
how much faster RFS can speed up the repair process. The
performance evaluation results on the RFS prototype pre-
sented in this section aim to answer these questions.
There are ﬁve machines in the experiment testbed, all
of which run RedHat 7.2 with Linux kernel 2.4.7. There
are two NFS clients, one NFS server to be protected, one
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:30:00 UTC from IEEE Xplore.  Restrictions apply. 
without RFS (500MB mem)
with RFS (500MB RAM)
with RFS (500MB RAM), two disks on the mirror NFS server
100
80
60
40
20
)
%
(
o
i
t
a
r
n
o
i
t
a
z
i
l
i
t
u
k
s
d
i
d
n
a
U
P
C
Protected NFS Server  (Disk)
MIRROR NFS Sever  (Disk)
MIRROR NFS Server  (CPU)
Protected NFS Server (CPU)
Request Interceptor  (CPU)
1200
1000
800
600
400