title:An active measurement system for shared environments
author:Joel Sommers and
Paul Barford
An Active Measurement System for Shared Environments
Joel Sommers
Colgate University
PI:EMAIL
Paul Barford
University of Wisconsin-Madison
PI:EMAIL
ABSTRACT
Testbeds composed of end hosts deployed across the Internet en-
able researchers to simultaneously conduct a wide variety of exper-
iments. Active measurement studies of Internet path properties that
require precisely crafted probe streams can be problematic in these
environments. The reason is that load on the host systems from
concurrently executing experiments (as is typical in PlanetLab) can
signiﬁcantly alter probe stream timings.
In this paper we mea-
sure and characterize how packet streams from our local PlanetLab
nodes are affected by experimental concurrency. We ﬁnd that the
effects can be extreme. We then set up a simple PlanetLab deploy-
ment in a laboratory testbed to evaluate these effects in a controlled
fashion. We ﬁnd that even relatively low load levels can cause seri-
ous problems in probe streams. Based on these results, we develop
a novel system called MAD that can operate as a Linux kernel mod-
ule or as a stand-alone daemon to support real-time scheduling of
probe streams. MAD coordinates probe packet emission for all ac-
tive measurement experiments on a node. We demonstrate the ca-
pabilities of MAD, showing that it performs effectively even under
very high levels of multiplexing and host system load.
Categories and Subject Descriptors: C.2.3 [Network Operations]:
Network management, Network monitoring, C.2.5 [Local and Wide-
Area Networks]: Internet (e.g., TCP/IP), C.4 [Performance of Sys-
tems]: Measurement Techniques
General Terms: Design, Experimentation, Measurement, Perfor-
mance
Keywords: Active Measurement, MAD
1.
INTRODUCTION
Several key challenges for networking research were speciﬁed
in the 2001 National Research Council report entitled “Looking
Over the Fence At Networks: A Neighbor’s View of Networking
Research” [18]. Among these was to develop an understanding of
Internet structure and behavior through empirical measurement, in-
cluding the grand challenge of capturing “a day in the life of the In-
ternet”. The lack of an intrinsic and openly available measurement
capability in the Internet implies that a widely deployed infrastruc-
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’07, October 24–26, 2007, San Diego, California, USA.
Copyright 2007 ACM 978-1-59593-908-1/07/0010 ...$5.00.
ture capable of different types of measurement would be required
as a critical component for addressing these challenges.
The need for Internet testbeds capable of supporting accurate
active measurements has been apparent for quite some time, and
has resulted in deployment and operation of several different in-
frastructures over the years. A well-known early example was the
National Internet Measurement Infrastructure (NIMI), which was
composed of guest accounts on 35 end hosts located primarily in
the US and Europe [29]. The NIMI effort helped to crystallize
the challenges associated with using and operating Internet mea-
surement testbeds. Among these challenges is the need for a large
number of diverse sites (e.g., commodity versus research network-
attached, geography, last-hop bandwidth) such that a wide range
of conditions is likely to be experienced across the paths between
measurement nodes.
Perhaps the most prominent Internet testbed today, PlanetLab,
is comprised of 780 end hosts deployed at 382 different sites all
over the world [14]. PlanetLab is a canonical example of an openly
available network testbed that is designed to support many different
types of network-related experiments at the same time. The funda-
mental design requirement of simultaneous experimental support
has a very important implication. Resource scheduling in these
environments, at the individual host level or globally across the
testbed, poses a particularly difﬁcult problem. Thus, any experi-
ment that has even modest timing or coordination requirements can
be difﬁcult or impossible to run in shared testbed environments. An
important class of experiments that have until now been largely ex-
cluded from shared network testbeds are those that use active probe
tools to measure end-to-end path properties such as delay, loss, ca-
pacity and available bandwidth.
In this paper we address the problem of how to extend shared net-
work testbeds to enable accurate active measurement studies that
use tools with ﬁne-grained timing requirements. We begin by as-
sessing the magnitude of the bias introduced by shared network
testbeds. We instrument the two PlanetLab nodes at our own site
with time-synchronized, hardware-based packet capture systems
and conduct a series of active measurement experiments between
those two nodes (i.e., such that there are no unknown network ef-
fects). We ﬁnd that even in this simple local area environment,
very large bias in active probe streams is common. For example,
we found that there are often periods of multiple seconds in which
packet loss is measured on end hosts while there is zero packet loss
in the network. Likewise, RTT delays of over 50 milliseconds are
not uncommon when the true delay between the two hosts as mea-
sured by the hardware-based systems is on the order of about 100
microseconds.
To address this problem, we develop the multi-user active mea-
surement system (MAD) that is designed to support real-time schedul-
ing of active probe streams. The key requirements for MAD include
accurate transmission/receipt of arbitrary probe streams, support
for simultaneous experiments, low impact on the host system, ease
of use and security. We designed MAD as a service that can be ac-
cessed by applications through a simple active measurement spec-
iﬁcation language. Authorized users specify their probe process to
MAD, which then coordinates transmission of probe streams from
multiple experiments using the real-time features of newer Linux
kernels to gain access to resources at the highest priority level.
(Note that there is also a MAD receiver module and a MAD reﬂec-
tor module that are used for one-way and round trip measurement
tools, respectively.) An additional requirement in order to facilitate
deployment and use of MAD is that it not necessitate modiﬁcations
to the host OS. As such we implemented MAD so that it could be
run either as a stand-alone daemon or as a Linux kernel module.
We evaluate the capabilities of MAD in a laboratory testbed that
includes a local area PlanetLab deployment. This environment en-
ables us to control and measure the contention effects due to mul-
tiplexing experiments both with and without MAD. We begin by
establishing a quantitative baseline of the extent to which load gen-
erated by simultaneous experiments can bias active probe streams
and lead to inaccurate inference of path properties. We then show
that with MAD, highly accurate measurements can be made even
under extremely heavy system loads (regardless of whether MAD
runs as an in-kernel process). Next, we conduct similar experi-
ments on raw, unvirtualized operating systems without PlanetLab
and ﬁnd that although the effects are less severe, MAD can still
improve measurement accuracy in a meaningful way. Finally, we
demonstrate the scalability of MAD through a series of microbench-
mark experiments and show that it is able to support a relatively
large number of simultaneous measurement experiments with ex-
tremely low impact on system resources.
In summary, the contributions of this paper are (i) characteriz-
ing the extent to which active probe-based measurement can be
skewed in PlanetLab, (ii) the design and implementation of MAD—
a real-time scheduling system for accurate active probe-based mea-
surement. We believe that MAD effectively addresses an important
deﬁciency in current end host-based Internet testbeds that largely
precludes, or at least casts doubt on, their use for empirical stud-
ies of path properties using active probe tools. By virtue of its
implementation, it is our hope that MAD can be widely deployed
and used. We also believe that the design of MAD can inform fu-
ture testbed development (e.g., GENI [7]) as well as infrastructures
used to measure path properties in operational environments (e.g.,
for SLA compliance monitoring [40]).
The remainder of this paper is organized as follows.
In Sec-
tion 2, we discuss studies related to our own.
In Section 3, we
present ﬁndings from our analysis of measurements taken from our
local PlanetLab nodes. We describe the details of MAD’s design,
implementation and use in Section 4. The results of our laboratory
evaluation of MAD are presented in Section 5, and in Section 6 we
summarize and discuss future directions for our work.
2. RELATED WORK
Host-based testbeds deployed in the Internet enable implementa-
tions of network applications, protocols, and measurement method-
ologies to be evaluated over live end-to-end paths. These testbeds
can also be useful for gauging and characterizing Internet structure
and end-to-end performance from multiple vantage points. Some
testbeds in the past have been developed for use by a single orga-
nization for a speciﬁc set of objectives, e.g., Surveyor [22], while
others have been designed from the outset to be general purpose and
shared with a wider set of researchers e.g., RON [10] and Planet-
Lab [14, 30]. There have been measurement studies conducted on
shared testbeds including RON and PlanetLab, e.g.,
[12, 31, 44],
and specialized systems have been developed for these testbeds
to perform or assist with various types of measurements such as
ScriptRoute [43].
PlanetLab in particular has been deployed with the goal of serv-
ing a large community of researchers, and has also spawned other
regional instantiations [1, 4]. PlanetLab uses virtualization tech-
niques to isolate users from one another [14, 36].
It virtualizes
a host system at the system call level, similar to the way BSD
Jails work [35], but a number of other approaches are possible,
e.g., [13, 33]. It is important to note that in heavily-used Internet
testbeds like PlanetLab there is a direct correlation between con-
tention for resources and the level of experimental multiplexing that
is allowed.
One study with similarities to ours presents guidelines in the
form of myths and realities for performing measurement studies
on PlanetLab [42]. One guideline of relevance is to use system
interfaces for obtaining kernel timestamps for probe packets. Our
work also uses these techniques. Another suggestion is to use the
ScriptRoute [43] system for deﬁning and executing the probe pro-
cess. Our work bears similarity to the ScriptRoute system in that
we also design a measurement service, one component of which is
a programmatic interface for specifying the probe process. How-
ever, our work takes a fundamentally different approach by focus-
ing not only on providing a ﬂexible measurement service, but also
on measurement accuracy. We assume that probe processes oper-
ate according to a discrete time clock, enabling optimizations to be
made over all running measurement processes. ScriptRoute makes
no such restrictions. Thus, the scheduling requirements of Script-
Route are signiﬁcantly different from our system.
Our system design takes advantage of the real-time scheduling
capabilities that have been incorporated into the main Linux kernel
source [5, 6]. These capabilities are similar to some of the features
used in the related study by Pásztor and Veitch in which a real-time
version of Linux along with techniques they devise for improving
scheduling and timestamp ﬁdelity are employed [27, 28]. Our work
relates to other efforts in the network research community to im-
prove the accuracy and precision of timing-sensitive applications
using commodity systems [2, 11, 45].
Active measurement of end-to-end delay and loss characteristics
have a long history within the network measurement community.
The most well-known early study of these characteristics was re-
ported by Bolot in [16]. While methodologies for measuring these
quantities have been standardized by the IETF [8,9], improvements
to these techniques continue to be developed [38, 40]. Of particular
relevance is the work by Sommers et al. that highlights the need
for real-time probe management in a multi-objective measurement
context [39]. Our work addresses a different problem: the impact
of resource sharing on active probe streams, and develops a gener-
alized mechanism to address the problem.
Using pairs of closely-spaced packets to estimate network quan-
tities also has a long and rich history. Jacobson’s work on TCP
congestion control was an early exposition of the possibility of how
a pair of closely-spaced packets can reveal bottleneck link capac-
ity [20]. Other researchers have explored speciﬁc algorithms using
this technique to perform capacity estimation, e.g., [23, 24, 25], and
for estimating end-to-end available bandwidth, e.g., [32, 44]. Dif-
ﬁculties in creating accurate packet streams that match the probe
models has been highlighted in many of these studies. For exam-
ple, Carter and Crovella grappled with issues related to probe tim-
ing ﬁdelity in developing active probe tools to measure bottleneck
link speeds [17].
3. THE CURRENT SITUATION
We ﬁrst investigate the accuracy characteristics of three types of
active measurements with stringent timing requirements using the
widely-used PlanetLab shared infrastructure.
3.1 Experiments
Normally, a user of PlanetLab does not have access to so-called
ground truth measures between an arbitrary pair of PlanetLab nodes.
Since the goal of our experiments is to evaluate the accuracy of
certain types of active measurements, this issue is of critical impor-
tance. Other researchers have used indirect means for addressing
this problem (e.g., by introducing known trafﬁc to disturb the sys-
tem as in [44]), or by using available coarse-grained SNMP data.
These methods generally cannot give the kind of accuracy and reli-
ability required for our study.
Our approach was to use the two PlanetLab nodes over which we
have complete control of the networking infrastructure: planetlab1
and planetlab2 in the domain cs.wisc.edu. For ground
truth measures, we introduced passive network taps and a pair of
synchronized Endace 4.3GE DAG cards for capturing both inbound
and outbound probe trafﬁc for each of the PlanetLab nodes, as de-
picted in Figure 1.
Our two PlanetLab nodes are identical Dell Precision 340’s with
1.8 GHz Pentium 4 processors and 2 GB RAM. They were running
version 4.0 of the PlanetLab software with Linux kernels derived
from version 2.6.12. We installed Intel/Pro 1000 Gigabit Ethernet
NICs on each host to accommodate our passive monitoring system.
Interrupt coalescence was disabled for our experiments. No other
changes were made to these two hosts.
planetlab
monitoring
nodes
tap
DAG
systems
Cisco 6500
to Internet
1: Setup for experiments using live PlanetLab nodes.
We used three active measurement algorithms on these two hosts
to examine how system load and multiplexing can affect measure-
ment accuracy: round-trip delay probes, packet-pair probes, and
BADABING loss probes. Each of these algorithms was implemented
to use the User Datagram Protocol and to run according to a discrete
time clock. We used the SO_TIMESTAMP option of setsockopt
to obtain kernel timestamps on receipt of packets. These three mea-
surement algorithms were chosen to be representatives of standard
active measurements relying on a high degree of accuracy in both
probe stream transmission and reception. A description of each
algorithm along with its relevant parameters and accuracy require-
ments is shown in Table 1. The discrete time interval used in the
results that we report below was 5 milliseconds.
We collected data from 11 April 2007 to 2 May 2007. We ini-
tiated experiments every 4 to 6 hours, running each measurement
algorithm for 10 minutes each. We simultaneously collected CPU
utilization information using vmstat and packet header traces us-
ing the DAG systems. Since we ran each algorithm according to a
discrete time clock, we also recorded the number of times our mea-
surement process was unable to send probes at an intended time
slot (i.e., “slipped” a time slot).
While we collected measurements, there were typically 40–50
active slices on each host, resulting in hundreds of active network
connections and very little idle CPU time. Quite often, CPU uti-
lization was at 100% for each node and there were more than 1000
network connections. A survey of other PlanetLab nodes using
CoMon [26] shows that the heavy load we observed is not ab-
normal. Somewhat surprisingly, there was relatively little network
trafﬁc produced by each of these machines. On average, there was
about 400 Kb/s inbound trafﬁc to each host, and about 500 Kb/s
outbound from each host. (These numbers varied between less than
100 Kb/s to a little more than 1 Mb/s over periods we collected
measurements.)
3.2 Results
Figure 2 shows timeseries plots of the median, 90th, and 95th
percentile round-trip delays for two example measurement periods.
Table 2 shows median, 90th, 95th, and 99th percentile delays for a
larger subset of measurements. The results shown in the ﬁgures and
table are qualitatively representative of all results we measured.
In Figure 2 and in Table 2 we ﬁrst see alarmingly high delay val-
ues for nearly all quantiles displayed. For example, we see a me-
dian RTT of 9 milliseconds in the 11 April 12:20 measurements.
This value is surprising since these two machines are colocated,
with one switch between them. Second, we observe excessive de-
lay values in the upper quantiles, sometimes larger than 100 mil-
liseconds. (In some measurement periods we measured maximum
delays on the order of 5 seconds.) Finally, we see a high degree of
variability in the measurements, in some cases even in the median
delay (e.g., in Figure 2a).
It is important to state that in all cases we measured the network
delay of the probes using the DAG cards (i.e., not including delay
introduced at the end hosts) to be on the order of 100 microseconds.
Clearly, these RTT measurements are very poor indications of the
true delay between these two PlanetLab hosts.
We now examine packet loss characteristics between the two live
PlanetLab systems. Note that in the experiments using BADABING,
we disabled the one-way delay congestion inference mechanisms in
the tool, and thus use only actual indications of packet loss. Since
BADABING sends three packets back-to-back as a probe, only one
packet must be lost in order for a probe to be marked as having
experienced loss.
Figure 3 shows timeseries of periods during which consecutive
probes experience loss. Plots are shown from two representative
measurement periods. Notice from the plots that there is a wide
range of durations over which consecutive losses are measured,
from very short periods (e.g., in Figure 3a) to very long periods
(e.g., in Figure 3b). Note also that the rate of all our measurements,
including loss, was less than the bandwidth limitation imposed on
operational PlanetLab nodes.
Table 3 shows representative results from the BADABING exper-
iments for additional measurement periods. The table shows the
frequency and duration of loss episodes reported by BADABING,
as well as a loss rate estimate based on the heuristic of [39]. We
see that there are estimated loss durations on the order of multiple
seconds, as suggested by Figure 3. We also see that the loss rate es-
timates are similar to the frequency estimates. The reason for this
effect is that the loss rate of all probe packets during loss episodes
is close to 1. That is, during loss episodes, nearly all probe packets
are lost. As suggested by results in the table, there were very few
measurement periods overall in which we measured zero loss.
As with our round-trip delay experiments, we measured the true
packet loss between the PlanetLab hosts using the DAG systems
and found in all cases that there was no packet loss (i.e., all probes
are observed on the wire just after transmission and just prior to
reception by a PlanetLab host). Thus, all loss indications that we
1: Description of the probe algorithms and their accuracy requirements used in experiments.
Probe algorithm and description
Round-trip delay measurements consisted of 100
byte UDP probes, sent periodically at 100 millisec-
ond intervals. A process on the remote host bounced
probes back to the sender, which then recorded round-
trip delay.
Packet-pair measurements consisted of 1500 byte
UDP probes sent with an intended spacing between
packets of 120 microseconds (i.e., back-to-back, as-
suming a capacity of 100 Mb/s). A packet pair was
sent at a given time slot with independent probability
p = 0.2, resulting in a geometric process of packet pair
transmissions.
BADABING loss measurements. BADABING sends
pairs of probes at time slots i and i + 1 initiated with
independent probability p which we set to 0.3. Each
probe consisted of three packets, each of 600 bytes,
sent back-to-back (as quickly as the host system would
allow) as described in Sommers et al. [38].
Accuracy Requirements
For RTT measurements, timestamps should be applied in such a way as to most