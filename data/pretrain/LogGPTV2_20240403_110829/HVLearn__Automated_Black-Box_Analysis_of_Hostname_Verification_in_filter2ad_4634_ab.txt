is the most suitable technique for testing a large variety of
different hostname veriﬁcation implementations.
B. HVLearn’s approach to hostname veriﬁcation analysis
Motivated by the challenges described above, we now
present our methodology for analyzing hostname veriﬁcation
routines in SSL/TLS libraries and applications.
The main idea behind our HVLearn system is the following:
For different rules in the RFCs as well as for ambiguous rules
which are not well deﬁned in the RFC, we generate “template
certiﬁcates” with common names which are speciﬁcally de-
signed in order to check a speciﬁc rule. Afterward, we use
automata learning algorithms in order to extract a DFA which
describes the set of all hostname strings which are matching
the common name in our template certiﬁcate. For example,
the inferred DFA from an implementation for the identiﬁer
template “aaa.*.aaa.com” can be used to test conformance with
the rule in RFC 6125 prohibiting wildcard characters from
appearing in any other label than the leftmost label of the
common name.
Once a DFA model is generated by the learning algorithm,
we check the model for violations of any RFC rules or for
other suspicious behavior. HVLearn offers two methods to
check an inferred DFA model:
Regular-expression-based rules. The ﬁrst option allows
the user to provide a regular expression that speciﬁes a set of
invalid strings. HVLearn can ensure that the inferred DFAs do
not accept any of those strings. For example, RFC 1035 states
that only characters in the set [A-Za-z0-9] and the characters ‘-
’ and ‘.’ should be used in hostname identiﬁers. Users therefore
can construct a simple regular expression that can be used by
HVLearn to check whether any of the tested implementations
accept a hostname with a character outside the given set.
524
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:20:25 UTC from IEEE Xplore.  Restrictions apply. 
Learning Model
Target System
Membership query
Model 
M
Learning 
Algorithm
Equivalence 
Oracle
Is model M correct?  
Yes/No with counter-example
Fig. 2. Exact learning from queries: the active learning model under which
our automata learning algorithms operate.
Differential testing. The second option offered by HVLearn
is to perform a differential testing between the inferred model
and models inferred from other implementations for the same
certiﬁcate template. Given two inferred DFA models, HVLearn
generates a set of unique differences between the two models
using an algorithm which we discuss in Section IV-E. This
option is especially useful for ﬁnding bugs in corner cases
which are not well deﬁned in the RFCs.
We summarize the advantages of our approach below:
• Adopting a black-box learning approach ensures that
our analysis method is language independent and we
can easily test a variety of different implementations.
Our only requirement is the ability to query the target
library/application with a certiﬁcate and a hostname of
our choice and ﬁnd whether the hostname is matching
the given identiﬁer in the certiﬁcate.
• As pointed out in the previous section, hostname veriﬁca-
tion is similar to regular expression matching. Given that
regular expressions can be represented as DFAs, adopting
an automata-based learning algorithm for representing the
inferred models for each certiﬁcate template is a natural
and effective choice.
• Finally, an additional advantage of having DFA models is
that we can efﬁciently compare two inferred models and
enumerate all differences between them. This property is
very important for differential testing as it helps us in
analyzing the ambiguous rules in the speciﬁcations.
Limitations. A natural trade-off of choosing to implement
our system as a black-box analysis method is that we cannot
guarantee completeness or soundness of our models. However,
each difference inferred by HVLearn can be easily veriﬁed by
querying the corresponding implementations. Moreover, since
our system will ﬁnd all differences among implementations,
it will not report a bug that is common among all implemen-
tations unless a rule is explicitly speciﬁed for it, as described
above. Finally, we point out that not all discrepancies among
systems are necessarily security vulnerabilities;
they may
represent equally acceptable design choices for ambiguous
parts of the RFCs.
C. Automata Learning Algorithms
We will now describe the automata learning algorithms that
allow us to realize our automata-based analysis framework.
525
Learning model. We utilize learning algorithms that work in
an active learning model which is called exact learning from
queries. Traditional supervised learning algorithms, such as
those used to train deep neural networks, work on a given set
of labeled examples. In contrast, active learning algorithms in
our model work by adaptively selecting inputs that they use
to query a target system and obtain the correct label.
Figure 2 presents an overview of our learning model. A
learning algorithm attempts to learn a model of a target
system by querying the target system with inputs of its choice.
Eventually, by querying the target system multiple times, the
learning algorithm infers a model of the target system. This
model is then checked for correctness through an equivalence
oracle, an oracle that checks whether the inferred model
correctly summarizes the behavior of the target system. If the
model is correct, i.e., it agrees with the target system on all
inputs, then the learning algorithm will output the generated
model and terminate. On the other hand, if the model is in-
correct, the equivalence oracle will produce a counterexample,
i.e., an input under which the target system and the model
produce different outputs. The learning algorithm then uses
the counterexample to reﬁne the inferred model. This process
iterates until the learning algorithm produces a correct model.
To summarize, a learning algorithm in the exact learning
model is able to interact with the target system using two
types of queries:
• Membership queries: The input to this type of query is a
string s and the output is Accept or Reject depending
on whether the string s is accepted by the target system
or not.
• Equivalence queries: The input to an equivalence query
is a model M and the output of the query is either T rue,
if the model M is equivalent to the target system on all
inputs, or a counterexample input under which the model
and target system produce different outputs.
Automata learning in practice. The ﬁrst algorithm for
inferring DFA models in the exact
learning from queries
model was developed by Angluin [31] and was followed by a
large number of optimizations and variations in the following
years. In our system, we use the Kearns-Vazirani (KV) algo-
rithm [54]. The KV algorithm utilizes a data structure called
the discrimination tree and it is in practice more efﬁcient in
terms of the amount of queries it requires to infer a DFA
model.
The most signiﬁcant challenge that one should address in
order to use the KV algorithm and other automata learning
algorithms in practice, is how to implement an efﬁcient and ac-
curate equivalence oracle in order to simulate the equivalence
queries performed by the learning algorithm. Since we only
have black-box access to the target system, any method for
implementing equivalence queries is necessarily incomplete.
In HVLearn, we use the Wp-method [49], for implementing
equivalence queries. The Wp-method checks the equivalence
between an inferred DFA and a target system using only
black-box queries to the target system. Essentially, the Wp-
method approximates an equivalence oracle by using multiple
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:20:25 UTC from IEEE Xplore.  Restrictions apply. 
HVLearn
certiﬁcate templates
test certiﬁcate template
equivalence 
query
DFA  
model
LearnLib
Optimized 
Wp-Method
counter- 
example
KV 
algorithm
Wp-method’s test  
hostnames
hostname 
(membership queries)
accept/reject
output ﬁnal model 
for test certiﬁcate template
SSL/TLS 
hostname 
veriﬁcation 
implementation
match 
(hostname, test cert)?
Fig. 3. Overview of learning a hostname veriﬁcation implementation using
HVLearn.
membership queries. The algorithm is given as input the DFA
to be checked and an upper bound on the number of states in
the target system when modeled as a DFA, a parameter which
we call depth. Then, the algorithm creates a set of test inputs
S, which are then submitted to the target system. If the target
system agrees with the DFA model on all inputs in the test set
S, then the DFA and the target system are proved equivalent
under the assumption that the upper bound on the number of
states of the target system is correct.
In theory, one can set the depth parameter of the Wp-method
to a very large value in order to design an equivalence oracle
which is, in practice, complete. However, the size of the set
of test inputs produced by the Wp-method is on the order of
O(n2|Σ|m−n+1) where Σ is the input alphabet for the DFA, m
is the upper bound on the number of states of the target system
and n is the number of states in the input DFA. Therefore,
using the Wp-method with a large depth (i.e., upper bound on
the number of states of the target system) is impractical. Note
that, the bound on the number of test inputs produced by the
Wp-method is not a worst case bound; on the contrary, the
number of test inputs produced is usually of that order.
Consequently, it is essential for the efﬁciency of our system
to maintain a small alphabet for our DFAs and also set a small
upper bound (depth) on the number of states of the target
system while using the Wp-method. We address both of these
issues in the next section.
IV. ARCHITECTURE OF HVLEARN
In this section, we describe the design and implementation
of our system, HVLearn, based on automata learning tech-
niques. Speciﬁcally, we describe the technical challenges that
arise when we attempt to use automata learning algorithms in
practice. We also summarize the optimizations that HVLearn
implements to address these challenges and efﬁciently learn
DFA models of hostname veriﬁcation implementations.
A. System overview
Figure 3 presents an overview of how HVLearn is used to
analyze the hostname veriﬁcation functionality of an SSL/TLS
library. To use HVLearn, the user provides HVLearn access to
526
the hostname veriﬁcation function that takes an X.509 certiﬁ-
cate and a hostname as input and returns accept/reject
depending on whether the provided hostname is matching the
identiﬁer in the certiﬁcate. We describe how we implement
this interface in Section IV-C. Our system includes a number
of certiﬁcate templates, which are certiﬁcates designed to test
the SSL/TLS implementation on a number of different rules as
described in Section IV-B. For each such template, HVLearn
will
learn a DFA model describing the set of hostnames
accepted by a given implementation for the given certiﬁcate
template. To produce a DFA model, HVLearn utilizes the
LearnLib [59] library which contains implementations of both
the KV algorithm and the Wp-method. To avoid setting the
maximum depth of the Wp-method to impractically high
values, we optimize the equivalence oracle as described in
Section IV-D.
Once a model is generated, our system proceeds to analyze
the model as described in Section IV-E. The results of our
analysis, both the inferred models and the differences between
models are then saved for reuse. Optionally, HVLearn can also
utilize the inferred models for a certiﬁcate template to extract a
formal speciﬁcation for the corresponding certiﬁcate template
as described in Section V-F.
B. Generating certiﬁcate templates
To cover all different rules and ambiguous practices in
hostname veriﬁcation, we created a set of 23 certiﬁcates with
different identiﬁer templates, where each certiﬁcate is designed
to test a speciﬁc rule from the speciﬁcation. These certiﬁcates
are selected to cover all the rules we described in Section II.
For example, a certiﬁcate with common name “xn--a*.aaa”
will test if the implementation allows wildcards as part of an
A-label in an IDN, something which is explicitly forbidden by
RFC 6125. Our template certiﬁcates are self-signed X.509 v3
certiﬁcates generated using the GnuTLS library. We choose
to use GnuTLS for certiﬁcate generation because it allows
identiﬁers with embedded NULL characters in both subject
common name and SAN. The template identiﬁer to be tested
is placed in either Subject CN and/or SAN (as dNSName,
iPAddress, or email).
C. Performing membership queries
In order to utilize the learning algorithms in LearnLib
(including the Wp-method), we implement a membership
query function that performs all queries to the target system.
This function accepts input as a string and returns a binary
value. In our system, we use the hostname veriﬁcation function
from the target SSL/TLS implementation. We note here that,
since LearnLib is written in Java while many of our tested
SSL/TLS implementations are written in C/C++/Python, we
utilized the Java Native Interface (JNI) [10] to efﬁciently
perform membership queries to the target in such cases.
D. Automata learning parameters and optimizations
In this section, we describe the architectural decisions and
optimizations that we implemented to efﬁciently scale the KV
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:20:25 UTC from IEEE Xplore.  Restrictions apply. 
algorithm for testing complex real-world SSL/TLS hostname
veriﬁcation implementations.
Alphabet size. The ﬁrst important decision we have to make
to utilize the KV algorithm is to select an alphabet that will
be used by the algorithm. The alphabet refers to the set of
symbols that the learning algorithm will test.
A straightforward approach is to use a very general set
of characters such as the set of ASCII characters. However,
this will impose an unnecessary overhead in our system’s
performance since the performance of both the KV algorithm
and the Wp-method rely heavily on the underlying alphabet
size. Our main insight is that we can reduce the alphabet to
a small set of representative characters that will thoroughly
test all different aspects of hostname veriﬁcation. In particular
we select the set Σ = { a, 1, dot, \s, @, A, =, *, x, n, -
, \u4F60, NULL} as an input alphabet in our experiments.
In the presented alphabet, ‘dot’ denotes the ‘.’ character, \s
denotes the space character (ASCII value 32), NULL denotes
the zero byte character, and \u4F60 denotes the unicode
character with hexadecimal value 4F60.
Note that
this set of symbols is adequate for analyz-
ing hostname veriﬁcation implementations since it includes
characters from all different categories such as lowercase,
uppercase, digits, unicode, etc., as well as special characters
like the NULL character. The lowercase characters ‘x’, ‘n’ in
conjunction with the ‘-’ character are necessary in order to
encode IDN hostnames. Finally, the inclusion of some non-
alphanumeric characters such as the ‘=’ character allows us
to detect violations where an implementation accepts invalid
hostnames.
repeated querying of
Note that, even though the hostnames generated using this
alphabet set will often not resolve to a real IP address when
processed as DNS names, it does not affect the accuracy
of our analysis in any way. This is a side-effect the fact
that the hostname veriﬁcation routines are not responsible for
resolving the provided DNS name to an IP address. It simply
checks whether the given hostname matches the identiﬁer in
the provided certiﬁcate.
Caching membership queries. To avoid the communi-
cation cost of
the SSL/TLS im-
plementations with same inputs, we utilize LearnLib’s
DFALearningCache class to cache the results of the mem-
bership queries. The cache is checked on each new query, and
a cached result is used whenever found. This optimization
is particularly useful for cutting down the overhead of the
repeated queries generated by the Wp-method across multiple
equivalence queries.
Optimizing equivalence queries. In practice, the ﬁrst model
generated by the learning algorithm is usually just single
state DFA which rejects all hostnames. The reason is that
the learning algorithm is not able to generate any accepting
hostname and thus cannot distinguish between the initial state
and any other state in the target system. Sometimes, to force
the KV algorithm to produce an accepting hostname using the
Wp-method, a very large depth is required. This may cause
efﬁciency issues in the system. However, if we supply the
model with an accepting hostname, then trivial models will
be improved quickly without having to utilize excessive depth
parameters in the Wp-method.
Recall here that the exponential term in the Wp-method is
dependent on the difference between the number of states in
the model and the provided depth. Therefore, once we discover
an accepting state in the target system, the Wp-method with a
much smaller depth will still be able to explore many different
aspects of the hostname veriﬁcation implementation.
In order to generate an accepting hostname, we perform
the following test during an equivalence query and before
calling the Wp-method. First, we search for any wildcard
characters (*) in the provided common name and replace them
with random characters from our alphabet to obtain a concrete
hostname. Next, we check that the generated model and the
target hostname veriﬁcation implementation agree on a set
of hostnames generated using this method. If not, we return
the hostname for which they differ as a counterexample. The
main advantage of this heuristic is that it allows us to quickly
produce accepting hostnames that uncover new states in the
target system without invoking the Wp-method with very large
depth values. Once these states are uncovered, and the quality
of the inferred models improve, the Wp-method, with a small
depth parameter, is utilized to discover additional states in the
target system.
E. Analysis and comparison of inferred DFA models
the next
After HVLearn outputs a model,
task for our
system is to analyze the produced model for RFC violations or,
confusing/ambiguous rules in the RFC, to compare different
inferred models and analyze any discrepancies found between
different implementations.
Analyzing a single DFA model. In the case of a single model,
we would like to determine whether the model is accepting
invalid hostnames prohibited by the RFC speciﬁcation. If the
speciﬁcation is unclear, our analysis can still be used in order
to manually inspect the behavior of the implementation on the
speciﬁc certiﬁcate template besides the differential analysis
described below.
Our system offers two options for performing analysis of
a single model. First, our system generates inputs that will
exercise all simple paths (i.e., paths without loops) that lead
to accepting states, in the inferred model. Intuitively, these
inputs are a small set of inputs that describe all different ﬂavors
of hostnames that will be accepted for the given certiﬁcate
template. By inspecting these certiﬁcates, we can determine if
the implementation is accepting invalid hostnames. Second,
HVLearn allows the user to specify a regular expression
rule to be checked against the inferred model. In this case,
the user speciﬁes a regular expression and HVLearn veriﬁes
that the regular expression and the inferred model does not
share any common strings. This option allows to easily check
certain RFC violations by utilizing simple regular expression
rules. For example, consider the rule specifying that no non-
alphanumeric characters should be part of a matching host-
name. By specifying the regular expression rule “(.)*=(.)*”
527
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:20:25 UTC from IEEE Xplore.  Restrictions apply. 
we can check whether there exists any matching hostname
that contains the ‘=’ character in the inferred model.
Comparing unique differences between DFA models. For
analyzing certain corner cases which are not speciﬁed in the
RFC, testing a single model may not be enough. Instead, we
compare the inferred models for different SSL/TLS imple-
mentations and ﬁnd inputs under which the implementations
behave differently. To perform this analysis, we utilize the
difference enumeration algorithm from [33]. In a nutshell, this
algorithm computes the product DFA between two, or more,
given models and then ﬁnds all simple paths to states in which
the DFAs are producing different output.
F. Speciﬁcation Extraction
As we discussed already, the RFC speciﬁcations leave cer-
tain aspects of hostname veriﬁcation up to the implementations
by not specifying the correct behavior in all cases. In these
cases imposing speciﬁc restrictions in the implementations is
challenging since we have to be careful to avoid breaking
compatibility with existing implementations and valid cer-
tiﬁcates. In this section, we describe how the inferred DFA
models for the different certiﬁcate templates can be used to
infer a formal speciﬁcation, which is compatible with existing
implementations, for the cases where RFC speciﬁcations are
vague.
Our main insight
is the following: For each certiﬁcate
template, we can use the DFA accepting the set of host-
names accepted by all SSL/TLS implementations as a formal
speciﬁcation of the corresponding rule template. The intuition
behind this choice is that this speciﬁcation is avoiding small
idiosyncrasies of each library and it is thus very compact. On
the other hand, if a vulnerability exists in this speciﬁcation
then this vulnerability must also exist in all tested implemen-
tations. Since each implementation is audited independently,
our choice gives us conﬁdence that our speciﬁcation is se-
cure from simple vulnerabilities while maintaining backward
compatibility with the tested implementations.
Computing the speciﬁcation. In order to compute the cor-
responding speciﬁcation for each certiﬁcate template, we pro-
ceed as follows: First, we obtain DFA models for all hostname