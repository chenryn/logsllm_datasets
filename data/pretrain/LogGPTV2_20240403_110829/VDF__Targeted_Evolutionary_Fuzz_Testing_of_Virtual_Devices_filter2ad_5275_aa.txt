title:VDF: Targeted Evolutionary Fuzz Testing of Virtual Devices
author:Andrew Henderson and
Heng Yin and
Guang Jin and
Hao Han and
Hongmei Deng
VDF: Targeted Evolutionary Fuzz Testing
of Virtual Devices
Andrew Henderson1(B), Heng Yin2, Guang Jin1, Hao Han1,
and Hongmei Deng1
1 Intelligent Automation, Inc., Rockville, MD 20855, USA
PI:EMAIL, {gjin,hhan,hdeng}@i-a-i.com
2 University of California, Riverside, CA 92521, USA
PI:EMAIL
Abstract. As cloud computing becomes more and more prevalent, there
is increased interest in mitigating attacks that target hypervisors from
within the virtualized guest environments that they host. We present
VDF, a targeted evolutionary fuzzing framework for discovering bugs
within the software-based virtual devices implemented as part of a hyper-
visor. To achieve this, VDF selectively instruments the code of a given
virtual device, and performs record and replay of memory-mapped I/O
(MMIO) activity speciﬁc to the virtual device. We evaluate VDF by
performing cloud-based parallel fuzz testing of eighteen virtual devices
implemented within the QEMU hypervisor, executing over two billion
test cases and revealing over one thousand unique crashes or hangs in
one third of the tested devices. Our custom test case minimization algo-
rithm further reduces the erroneous test cases into only 18.57% of the
original sizes on average.
Keywords: Virtualization · Fuzzing · Device testing · Security
1 Introduction
As cloud computing becomes more prevalent, the usage of virtualized guest sys-
tems for rapid and scalable deployment of computing resources is increasing.
Major cloud service providers, such as Amazon Web Services (AWS), Microsoft
Azure, and IBM SoftLayer, continue to grow as demand for cloud computing
resources increases. Amazon, the current market leader in cloud computing,
reported that AWS’s net sales exceeded 7.88 billion USD in 2015 [2], which
demonstrates a strong market need for virtualization technology.
This popularity has led to an increased interest in mitigating attacks that tar-
get hypervisors from within the virtualized guest environments that they host.
This document has been approved for public release: 88ABW-2016-3973.
Electronic supplementary material The online version of this chapter (doi:10.
1007/978-3-319-66332-6 1) contains supplementary material, which is available to
authorized users.
c(cid:2) Springer International Publishing AG 2017
M. Dacier et al. (Eds.): RAID 2017, LNCS 10453, pp. 3–25, 2017.
DOI: 10.1007/978-3-319-66332-6 1
4
A. Henderson et al.
Unfortunately, hypervisors are complex pieces of software that are diﬃcult to test
under every possible set of guest runtime conditions. Virtual hardware devices
used by guests, which are hardware peripherals emulated in software (rather than
directly mapping to physical devices on the host system), are particularly com-
plex and a source of numerous bugs [3–6]. This has led to the ongoing discovery
of vulnerabilities that exploit these virtual devices to access the host.
Because virtual devices are so closely associated with the hypervisor, if not
integrated directly into it, they execute at a higher level of privilege than any
code executing within the guest environment. They are not part of the guest
environment, per se, but they are privileged subsystems that the guest environ-
ment directly interacts with. Under no circumstances should activity originating
from within the guest be able to attack and compromise the hypervisor, so eﬀec-
tively identifying potential vulnerabilities in these virtual devices is a diﬃcult,
but valuable, problem to consider. However, these virtual devices are written by
a number of diﬀerent authors, and the most complex virtual devices are imple-
mented using thousands of lines of code. Therefore, it is desirable to discover
an eﬀective and eﬃcient method to test these devices in a scalable and auto-
mated fashion without requiring expert knowledge of each virtual device’s state
machine and internal details.
Such issues have led to a strong interest in eﬀectively testing virtual device
code [9,28] to discover bugs or other behaviors that may lead to vulnerabilities.
However, this is a non-trivial task as virtual devices are often tightly coupled
to the hypervisor codebase and may need to pass through a number of device
initialization states (i.e. BIOS and guest kernel initialization of the device) before
representing the device’s state within a running guest system.
Evolutionary fuzzing techniques (e.g., AFL [38]) has gained its popularity
recently for its eﬀectiveness in discovering crashes and hangs. It is widely used
in industry, and most ﬁnalists in the DARPA Cyber Grand Challenge used it
for vulnerability discovery. Several academic research papers soon appeared to
further improve the eﬀectiveness of evolutionary fuzzing, such as AFLFast [21],
VUzzer [33], Driller [35], and DeepFuzz [22]. While these eﬀorts greatly improve
the state-of-the-art, they aim at ﬁnding defects within the entire user-level pro-
gram, and cannot be directly applied to ﬁnd bugs in virtual devices, for several
reasons. First of all, the fuzz testing must be targeted at speciﬁc virtual device
code, which is a rather small portion of the entire hypervisor code base. It must
be in-situ as well, as virtual devices frequently interact with the rest of the
hypervisor code. Last but not least, it must be stateful, since virtual devices
need to be properly initialized and reach certain states to trigger defects.
To address these unique challenges, we propose Virtual Device Fuzzer (VDF),
a novel fuzz testing framework that provides targeted fuzz testing of interesting
subsystems (virtual devices) within complex programs. VDF enables the testing
of virtual devices within the context of a running hypervisor. It utilizes record
and replay of virtual device memory-mapped I/O (MMIO) activity to create
fuzz testing seed inputs that are guaranteed to reach states of interest and ini-
tialize each virtual device to a known good state from which to begin each test.
Providing proper seed test cases to the fuzzer is important for eﬀective exploring
VDF: Targeted Evolutionary Fuzz Testing of Virtual Devices
5
the branches of a program [25,34], as a good starting seed will focus the fuzzer’s
eﬀorts in areas of interest within the program. VDF mutates these seed inputs
to generate and replay fuzzed MMIO activity to exercise additional branches of
interest.
As a proof of concept, we utilize VDF to test a representative set of eighteen
virtual devices implemented within the QEMU whole-system emulator [19], a
popular type-2 hypervisor that uses a virtualized device model. Whether QEMU
completely emulates the guest CPU or uses another hypervisor, such as KVM [10]
or Xen [18], to execute guest CPU instructions, hardware devices made available
to the guest are software-based devices implemented within QEMU.
In summary, this paper makes the following contributions:
– We propose and develop a targeted, in-situ fuzz testing framework for virtual
devices.
– We evaluate VDF by testing eighteen QEMU virtual devices, executing over
2.28 billion test cases in several parallel VDF instances within a cloud envi-
ronment. This testing discovered a total of 348 crashes and 666 hangs within
six of the tested virtual devices. Bug reports and CVEs have been reported
to the QEMU maintainers where applicable.
– We devise a testcase minimization algorithm to reduce each crash/hang test
case to a minimal test case that still reproduces the same bug. The average
test case is reduced to only 18.57% of its original size, greatly simplifying the
analysis of discovered bugs and discovering duplicate test cases that reproduce
the same bug. We also automatically generate source code suitable for repro-
ducing the activity of each test case to aid in the analysis of each discovered
bug.
– We analyze the discovered bugs and organize them into four categories: excess
host resource usage, invalid data transfers, debugging asserts, and multi-
threaded race conditions.
2 Background
Within QEMU, virtual device code registers callback functions with QEMU’s
virtual memory management unit (MMU). These callback functions expose vir-
tual device functionality to the guest environment and are called when speciﬁc
memory addresses within the guest memory space are read or written. QEMU
uses this mechanism to implement memory-mapped I/O (MMIO), mimicking
the MMIO mechanism of physical hardware.
We have identiﬁed a model for guest activity that attempts to attack these
virtual devices:
1. The virtual device is correctly instantiated by the hypervisor and made avail-
able to the guest environment.
2. The virtual device is correctly initialized via the guest’s BIOS and OS kernel
and is brought to a stable state during the guest boot process. Any needed
guest kernel device drivers have been loaded and initialized.
6
A. Henderson et al.
3. Once the guest boots, the attacker acquires privileged access within the guest
and attempts to attack the virtual devices via memory reads/writes to the
MMIO address(es) belonging to these virtual devices.
Unfortunately, it is non-trivial to perform large-scale testing of virtual devices
in a manner analogous to this model. The read/write activity would originate
from within the guest environment, requiring the guest to completely boot and
initialize prior to performing a test1. Because any read/write to a virtual device
control register may change the internal state of the device, the device must be
returned to a known good “just initialized” state prior to the start of each test.
While utilizing virtual machine (VM) state snapshots to save and restore the
state of the guest is a potential solution, the time required to continually restore
the state of the guest to a known good state makes this approach ineﬃcient for
large-scale testing. Consider the megabytes of system state data (guest RAM,
CPU state, and device state and internal cache storage) required to restore a
running VM to a known state. Even when ignoring the time required to retrieve
such state information from secondary storage, megabytes of data within the
snapshot must still be unserialized and placed into hypervisor data structures
prior to the start of each test.
2.1 Understanding Guest Access of Virtual Devices
The ﬂow of activity for virtual device access from within QEMU is shown in
Fig. 1. This ﬁgure shows a KVM-accelerated QEMU hypervisor conﬁguration.
The guest environment executes within QEMU, and the virtual devices are pro-
vided to the guest by QEMU. CPU instruction execution and memory accesses,
however, are serviced by the KVM hypervisor running within the host system’s
Linux kernel. A request is made from a guest process (a) and the guest kernel
accesses the device on the process’s behalf (b). This request is passed through
QEMU’s KVM interface to the KVM kernel module in the host’s kernel. KVM
then forwards the request to a QEMU virtual device (c). The virtual device
responds (d) and the result is provided to the guest kernel (e). Finally, the guest
process receives a response from the guest kernel (f).
Unlike the standard 0–3 ring-based protection scheme used by x86 platforms,
virtualized systems contain two sets of rings: rings 0 through 3 on the host, and
rings 0’ through 3’ on the guest. The rings within the guest are analogous to
their counterparts on the host with one exception: the highest priority guest ring
(ring 0’) is at a lower priority than the lowest priority ring on the host (ring 3).
While a guest environment may be compromised by malicious software, it is still
safely contained within a virtualized environment. However, if malware were to
compromise the hypervisor and gain host ring 3 privileges, it would eﬀectively
“break out” of the virtualization and gain the opportunity to attack the host.
1 QEMU provides the qtest framework to perform arbitrary read/write activity with-
out the guest. We discuss qtest, and its limitations when fuzz testing, in Sect. 3.
VDF: Targeted Evolutionary Fuzz Testing of Virtual Devices
7
Fig. 1. Device access process for a device request originating from inside of a
QEMU/KVM guest. Note that the highest level of privilege in the guest (ring 0’)
is still lower than that of the QEMU process (ring 3).
2.2 Understanding Memory Mapped I/O
Both physical and virtual peripherals provide one or more registers that control
their behavior. By accessing these control registers, the hardware is instructed
to perform tasks and provide information about the current state of the device.
Each device’s control registers are organized into one or more register banks.
Each register bank is mapped to a contiguous range of guest physical memory
locations that begin at a particular base address. To simplify interaction with
these control registers, the registers are accessed via normal memory bus activity.
From a software point of view, hardware control registers are accessed via reads
and writes to speciﬁc physical memory addresses.
The x86 family of processors is unique because it also provides port I/O-
speciﬁc memory (all memory addresses below 0x10000) that cannot be accessed
via standard memory reads and writes [29]. Instead, the x86 instruction set pro-
vides two special I/O-speciﬁc instructions, IN and OUT, to perform 1, 2, or 4
byte accesses to port I/O memory. Other common architectures, such as Alpha,
ARM, MIPS, and SPARC, do not have this port I/O memory region and treat all
control register accesses as regular memory-mapped I/O. For simplicity in our
discussion, we use port-mapped I/O (PMIO) and memory-mapped I/O inter-
changeably throughout this paper.
Figure 2 shows where MMIO devices are mapped in guest physical memory on
x86-based systems. PCI-based PMIO mappings occur in the addresses ranging
from 0xC000 through 0xFFFF, with ISA-based devices mapped into the sub-
0xC000 range. PCI devices may also expose control registers or banks of device
RAM or ROM in the PCI “hole” memory range 0xE0000000-0xFFFFFFFF.
While some ISA devices are historically mapped to speciﬁc addresses (for
example, 0x3F8 for the COM1 serial port), other ISA devices can be conﬁgured
to use one or more of a small set of selectable base addresses to avoid conﬂicts
with other devices. PCI devices are far more ﬂexible in the selection of their
address mapping. At boot, the BIOS queries the PCI bus to enumerate all PCI
devices connected to the bus. The number and sizes of the control register banks
8
A. Henderson et al.
Fig. 2. The x86 address space layout for port- and memory-mapped I/O.
needed by each PCI device are reported to the BIOS. The BIOS then determines
a memory-mapping for each register bank that satisﬁes the MMIO needs of all
PCI devices without any overlap. Finally, the BIOS instructs the PCI bus to
map speciﬁc base addresses to each device’s register banks using the PCI base
address registers (BARs) of each device.
However, PCI makes the task of virtual device testing more diﬃcult. By
default, the BARs for each device contain invalid addresses. Until the BARs
are initialized by the BIOS, PCI devices are unusable. The PCI host controller
provides two 32-bit registers in the ISA MMIO/PMIO address space for con-
ﬁguring each PCI device BAR2. Until the proper read/write sequence is made
to these two registers, PCI devices remain unconﬁgured and inaccessible to the
guest environment. Therefore, conﬁguring a virtual PCI-based device involves
initializing both the state of the PCI bus and the virtual device.
3 Fuzzing Virtual Devices
3.1 Evolutionary Fuzzing
Fuzzing mutates seed input to generate new test case inputs which execute new
paths within a program. Simple fuzzers naively mutate seed inputs without any
knowledge of the program under test, treating the program as a “black box”.
In comparison, evolutionary fuzzing, such as AFL [38] can insert compile-time
instrumentation into the program under test. This instrumentation, placed at
every branch and label within the instrumented program, tracks which branches
have been taken when speciﬁc inputs are supplied. Such evolutionary fuzzing is
much more eﬀective at exploring new branches.
If AFL generates a test case that covers new branches, that test case becomes
a new seed input. As AFL continues to generate new seeds, more and more states
of the program are exercised. Unfortunately, all branches are considered to be of
2 CONFIG ADDRESS at 0xCF8 and CONFIG DATA at 0xCFC [11].
VDF: Targeted Evolutionary Fuzz Testing of Virtual Devices
9
equal priority during exploration, so uninteresting states are explored as readily
as interesting states are. This leads to a large number of wasted testing cycles as
uninteresting states are unnecessarily explored. Therefore, VDF modiﬁes AFL
to only instrument the portions of the hypervisor source code that belong to
the virtual device currently being tested. This eﬀectively makes AFL ignore the
remainder of the hypervisor codebase when selectively mutating seed inputs.
AFL maintains a “fuzz bitmap”, with each byte within the bitmap repre-
senting a count of the number of times a particular branch within the fuzzed
program has been taken. AFL does not perform a one-to-one mapping between
a particular branch and a byte within the bitmap. Instead, AFL’s embedded
instrumentation places a random two-byte constant ID into each branch. When-
ever execution reaches an instrumented branch, AFL performs an XOR of the new
branch’s ID and the last branch ID seen prior to arriving at the new branch. This
captures both the current branch and the unique path taken to reach it (such
as when the same function is called from multiple locations in the code). AFL
then applies a hashing function to the XOR’d value to determine which entry in
the bitmap represents that branch combination. Whenever a particular branch
combination is exercised, the appropriate byte is incremented within the bitmap.
VDF modiﬁes AFL to use a much simpler block coverage mechanism that
provides a one-to-one mapping between a particular instrumented branch and
a single entry in the bitmap. Because VDF selectively instruments only the
branches within a virtual device, the bitmap contains more than enough entries
to dedicate an entry to each instrumented branch3. VDF’s modiﬁcations do away
with the XORing of IDs and AFL’s hash function. Instead, IDs are assigned lin-
early, simpliﬁying the ground truth determination of whether a particular branch
has been reached during testing while guaranteeing that no IDs are duplicated.
Thus, AFL takes a general purpose approach towards fuzzing/exploring all
branches within a program. VDF’s modiﬁed AFL takes a more focused approach
that constrains fuzzing to only the branches of interest in a program. VDF’s
approach eliminates the possibility of ambiguous branch coverage, which is still
possible to experience with an unmodiﬁed AFL.
3.2 VDF Workﬂow
Figure 3 shows the three-step ﬂow used by VDF when testing a virtual device.
In the ﬁrst step, virtual device activity is recorded while the device is being
exercised. This log of activity includes any initialization of PCI BARs for the
virtual device via the PCI host controller (if needed), initialization of any internal
device registers, and any MMIO activity that exercises the virtual device. This
log is saved to disk and becomes the seed input for the fuzzer. This collection of
seed input is described further in Sect. 3.3.
In the second step, the collected virtual device read/write activity is then pro-
vided as seed data to AFL. Multiple AFL instances can be launched in parallel,
with one required master instance and one or more optional slave instances. The
3 VDF still uses a two-byte branch ID, allowing for 65536 unique branches to be
instrumented. In practice, this is more than adequate for virtual device testing.
10
A. Henderson et al.
Fig. 3. VDF’s process for performing fuzz testing of QEMU virtual devices.
primary diﬀerence between master and slave instances is that the master uses a
series of sophisticated mutation strategies (bit/byte swapping, setting bytes to
speciﬁc values like 0x00 and 0xFF, etc.) to explore the program under test. Slave
instances only perform random bit ﬂips throughout the seed data.
Once the seed input has been mutated into a new test case, a new QEMU
instance is spawned by AFL. VDF replays the test case in the new QEMU
instance and observes whether the mutated data has caused QEMU to crash or
hang. VDF does not blindly replay events, but rather performs strict ﬁltering
on the mutated seed input during replay. The ﬁlter discards malformed events,
events describing a read/write outside the range of the current register bank,
events referencing an invalid register bank, etc. This prevents mutated data