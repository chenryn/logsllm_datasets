Signature 2
Signature 3
Signature 4
Signature 5
Signature 6
by a detected alarm. The edge was drawn from the source IP address toward the
destination IP address, corresponding to the direction of the alarm.
The results are such that the IDS alarms which are shown in Table 1 are
modeled as the directed graph shown in Figure 2.
10.0.0.1
10.0.0.2
10.0.0.3
10.0.0.5
10.0.0.6
10.0.0.9
10.0.0.10
10.0.0.4
10.0.0.13
10.0.0.11
10.0.0.12
10.0.0.7
10.0.0.8
Fig. 2. Intrusion Detection Alarms as a Directed Graph With Three Connected Com-
ponents
3.4 Data Set Reduction Using the Connected-Component
Algorithm
The number of alarms produced in large intrusion detection environments can
easily be on the order of millions of rows per day. We have observed raw event
A Framework for the Application of Association Rule Mining
7
counts approaching 10 million events per day. We knew that most of these alarms
were false positives, however it was not possible to label precisely which alarms
were of genuine concern [17,18,20,21]. Because of the large volumes of data that
required analysis, it was beneﬁcial from a performance perspective to trim away
any data that we knew to be irrelevant before starting the mining activities. In
order to facilitate this, we represented the alarm logs as directed graphs, which
allowed us to employ the use of graph algorithms to limit the scope of our inquiry.
This process was only possible if we had a priori knowledge of a signature for
which we wished to discover new rules.
When considering the problem of ﬁnding rules which exist between distinct
signature and IP address combinations, it was important to note that there
were alarms in the overall data set that could not be related to one another.
For example, while examining one set of alarms, if we knew that another set of
alarms could not be related to it, we removed the second set from consideration.
Drawing on our earlier discussion of alarm logs as directed graphs, we could
translate the set of alarms in Table 1 into the directed graph shown in Figure 2,
which displays three easily identiﬁed connected components. Limiting our mining
activity solely to alarms in the same connected component allowed us to explore
only relationships between alarms which could legitimately exist. A complication
arose in the case of slave nodes which were controlled by a master who was not
represented in the graph. We designated this scenario to be out of scope for our
experiments.
When attempting to discover rules for a speciﬁc signature, a natural question
arises as to why we did not simply limit the alarms to those which were produced
by a source IP address that also produced the signature undergoing analysis.
Reducing the data set in this manner was possible if we were interested only
in the detection of single-source attacks for a speciﬁc signature. We would then
examine the set of all alarms generated by a source IP address which triggered
the signature in question. However, trimming the data in this way would severely
limit any further analysis that we wished to perform on the set of alarms. By
carrying the other relevant alarms from the connected component, we have access
to a greater number of signatures and IP addresses for analysis. We also preserved
the ability to perform further analysis by grouping on ﬁelds other than the
source IP address if we found that a more extensive exploration of the data was
warranted.
For example, consider a multi-stage attack consisting of a reconnaissance event
which discovered a vulnerability on the target and exploited it in a way that in
turn attacked a third system. Table 2 lists alarms which would constitute such a
scenario. These alarms are shown graphically in Figure 3. The reconnaissance and
subsequent exploit occur between 10.0.0.5 and 10.0.0.7. A successful compromise
of 10.0.0.7 by 10.0.0.5 is then used to further attack 10.0.0.8.
If we had speciﬁed the reconnaissance signature as the input to the mining
process and trimmed away all IP addresses which did not trigger that signature,
we would have missed the second half of the attack. As such, limiting the alarms
that we examine only to those which occur in the same connected component
8
J.J. Treinen and R. Thurimella
Table 2. Intrusion Detection Alarms for a Multi-Stage Attack
Network ID Source IP Destination IP
Signature
Network A
Network A
Network A
Network A
10.0.0.5
10.0.0.5
10.0.0.7
10.0.0.6
10.0.0.7
10.0.0.7
10.0.0.8
10.0.0.7
Reconnaissance
Exploit 1
Exploit 2
False Alarm
10.0.0.5
Reconaisance
Exploit 1
10.0.0.7
10.0.0.8
Exploit 2
10.0.0.6
Fig. 3. A Multi-Stage Attack Scenario
provided the appropriate balance of eﬃciency without interfering with our ability
to perform complex analysis of the relevant data. On average, we were able to
reduce the amount of data that required analysis by 30 percent. However, our
ability to reduce the amount of data we inspected was sometimes diminished
in the case of graphs which were nearly fully connected. Because this type of
graph produced one large connected component comprised of the majority of the
alarms, the amount of data which we were able to trim away prior to executing
the association rule algorithm was in some cases reduced to less than 5 percent.
4 The Approach
Our experiments were conducted on the set of alarm logs generated by network-
based intrusion detection sensors over a 24-hour period for 135 distinct pro-
duction networks. The alarms were loaded into a data warehouse speciﬁcally
engineered to facilitate eﬃcient oﬀ-line analysis of intrusion detection alarms us-
ing association rule mining techniques. We repeated the experiments on a daily
basis for 30 days.
4.1 Generation of Signature Speciﬁc Rules
Our ﬁrst set of experiments were conducted with the goal of discovering new
rules for a signature which was thought to be exhibiting suspicious behavior. We
A Framework for the Application of Association Rule Mining
9
accomplished this by ﬁrst selecting the set of connected components in which
the suspected signature was present, and discarding all alarms that were not
members of these connected components. Once we had ﬁltered the data in this
way, we then executed the association rule algorithm to see if any rules for this
signature were generated. Algorithm 1 describes this technique.
(cid:1) ∈ C do
(cid:1) then
Algorithm 1. Find-Signature-Rules(G,s)
Require: G = (V, E), a directed graph of IDS Alarms, s a subject signature
1: C ←Connected-Components(G)
2: for all C
if s ∈ C
3:
4:
5:
end if
6: end for
7: R ← Association-Rules(T)
8: Return R
copy all alarms in C
(cid:1) to T
Of the scenarios that we discuss, signature speciﬁc rule generation experienced
the lowest occurrence of success. One of the reasons for this was that rather than
being identiﬁed algorithmically, the signature examined was generally chosen
by a human operator who was simply curious as to whether any correlations
involving this signature were hidden in the data. The subject signature was most
often chosen for analysis based on an abnormally high volume of that signature
over a speciﬁc time period, or its appearance as a new signature where it had not
been previously detected. These scenarios might occur due to the introduction
of a previously unforeseen attack scenario into a network, or simply because of
software updates on the sensors themselves.
Over the course of our experiments, we were able to successfully generate rules
for speciﬁc signatures roughly 10 percent of the time. However, given that data
mining always requires manual evaluation and exploration of its results, we still
believe this to be an eﬀective tool for operations staﬀ to have at their disposal.
The skill of the user conducting the analysis had a great impact on the quality of
the results, which is consistent with the views expressed in [18,30,37]. We found
that as the user’s experience with the technique grew, their ability to choose
signatures for which rules would be generated grew as well.
Approximately half of the experiments uncovered patterns involving signa-
tures other than those which were the original subject of our exploration. In
some cases, the rules algorithm would produce more than 100 rules for a single
run. This appeared at ﬁrst glance to be overwhelming, however, the rules which
exhibited very strong Conﬁdence values ﬂoated to the top on their own merits,
and were easily identiﬁable.
If we were unable to safely remove signiﬁcant numbers of rows from consid-
eration by ﬁltering on connected component, the time required for the mining
algorithm to generate results grew rapidly. A side eﬀect produced by this com-
plication was the generation of a very large number of rules by the algorithm.
10
J.J. Treinen and R. Thurimella
In some cases we observed rule counts as high as 8000 for a single network’s
data. This number of rules on its own is of limited value, as it does not solve the
problem of limiting the amount of data which must be examined manually by
operational staﬀ. However, the vast majority of the time, the count of rules for a
single network on a single day was below 100. When a spike occurred, we found
it to be indicative of signiﬁcant phenomena in the network being monitored. We
discuss these ﬁndings in a later section of this paper.
A useful means of tuning the number of rules returned by the association rule
algorithm was to adjust the minimum values for the Support and Conﬁdence
parameters for the mining algorithm, which had the net eﬀect of limiting the
number of rules which were produced. The obvious risk in limiting the rules to
those with a very high Support value is that any signature which generated low
volumes when compared to the volume of alarms in a single day will simply be
lost. It is for this reason that we generally left the Support value at a relatively
low setting, while enforcing a constraint of high Conﬁdence values on the result
set. By doing this, we were able to limit the results to rules which were found
to hold the majority of the time.
4.2 Generation of Single Source Rules
Our framework generated the greatest number of high Conﬁdence rules when
we grouped the transactions in the database by source IP address. When using
this approach it was not necessary to limit the rows we examined using the
connected components algorithm, though it was beneﬁcial from a performance
perspective if we knew the signature for which we wished to perform the analysis,
and used this information to limit the data set before executing the association
rules algorithm. When performing single-source analysis, we also found that
setting the minimum values for the Support and Conﬁdence parameters to 0
was useful. Intuitively, providing these low values for the Support and Conﬁdence
parameters would produce an overwhelming number of rules. However, over the
course of our experiments we found that on average, a single source IP address
will trigger less than two signatures in any 24 hour period. Because we were
looking for correlations between signatures which were generated by a single
source, it was obvious that no rules would be generated for these IP addresses.
Because of this, 87 percent of our single-source experiments generated zero rules
for a given day’s data.
5 Eﬃcacy of the Framework
The Conﬁdence value given for a new rule was critical in determining how eﬀec-
tive the rule would be in the production monitoring environment. On average,
66 percent of the rules we produced had a conﬁdence value of 100, and rules
with a Conﬁdence value over 80 were produced 86 percent of the time. We found
that certain types of attack activity generated very high volumes of rules with a
Conﬁdence value of 100 percent. While these rules were not false positives, they
A Framework for the Application of Association Rule Mining
11
skewed the statistics. Disregarding them, the percentage of rules with a Conﬁ-
dence value above 80 percent was 63 and the percentage of rules with Conﬁdence
values of 100 was 43.
When applying our technique, we were able to detect attacks that did not
trigger meta alarms on the operational console. In one case, we were able to
detect an attack on a day where the ESM system received 1,543,997 alarms.
The detected attack was comprised of only 6 alarms, and did not result in a
meta alarm ﬁring on the operational console. This is of great consequence as
this attack would otherwise have been lost in the noise of the 1.5 million other
alarms that ﬂowed through the infrastructure that day. It was then possible to
code a rule describing this scenario into the ESM system so that future instances
would be detected.
5.1 Rule Examples
1. Web Server Attack:
Our ﬁrst example does not indicate the reconnaissance approach which was
used to determine the list of web servers that underwent the detected attack,
as no reconnaissance signature was present in the alarm log that generated
this rule. It is possible that the technique used did not trigger an alarm, or
that the reconnaissance phase of the attack was carried out many days in
advance in an attempt to prevent detection. The alarms which were present
in the database which generated this rule are indicated in Table 3. The
IP addresses have been sanitized to prevent identiﬁcation of the customer
network for which the analysis was performed.
Table 3. IDS Alarms for a Multi-Stage Web Server Attack
Network ID Source IP Destination IP
Signature
Network A
Network A
Network B
Network B
...
24.9.61.170
24.9.61.170
24.9.61.170
24.9.61.170
...
192.168.2.4
192.168.2.5
192.168.2.16
192.168.2.17
...
AWStats conﬁgdir Command Exec
XMLRPC PHP Command Execution
AWStats conﬁgdir Command Exec
XMLRPC PHP Command Execution
...
[AWStats conﬁgdir Command Exec]⇒ [XMLRPC PHP Command Execution]
Rule for Multi-Stage Web Server Attack
Conﬁdence = 100
Support = 3.45
12
J.J. Treinen and R. Thurimella
This rule involves two signatures generated by an attacker who was attempt-
ing to locate a vulnerability to exploit on a web server. The ﬁrst stage of
the attack appeared in the alarm logs as multiple instances of the signature,
[AWStats conﬁgdir Command Exec], which ﬁred as the attacker attempted
to execute an unauthorized command using the conﬁgdir variable of the aw-
stats.pl CGI script. The second phase of the attack appeared in the alarm
logs as the signature, [XML RPC PHP command Execution], which was
triggered as attempts were made to exploit an XMLRPC vulnerability via
SQL injection [7].
Our framework was able to detect this pattern by grouping alarms by the
source IP address, and looking for repetitive combinations. When grouped
together, these two signatures, when triggered by the same source IP ad-
dress, are indicative of an attacker who attempted multiple exploits before
either compromising the target server, or moving to another victim. Further,
because these were the only rules generated for this network on the day in
question, we can be almost certain that the activity was legitimate attack
activity and not part of an automated vulnerability scan. We observed this
same pattern on two distinct monitored networks on the same day, which
indicates further that the detected activity was a real attack.
2. Reconnaissance Attack:
This rule was generated using data from a network where an attacker was
attempting to locate vulnerable ﬁle shares to attack. A pattern was found
in the alarm logs for this customer which described a frequently occurring
pattern of two TCP-based reconnaissance signatures followed by a LANMan
share enumeration, which is a common means of locating vulnerable ﬁle
shares for future exploitation.
Rule for Reconnaissance Activity