title:Practical dynamic proofs of retrievability
author:Elaine Shi and
Emil Stefanov and
Charalampos Papamanthou
Practical Dynamic Proofs of Retrievability
Elaine Shi
University of Maryland
PI:EMAIL
Emil Stefanov
UC Berkeley
PI:EMAIL
Charalampos Papamanthou
University of Maryland
PI:EMAIL
ABSTRACT
Proofs of Retrievability (PoR), proposed by Juels and Kaliski
in 2007, enable a client to store n ﬁle blocks with a cloud
server so that later the server can prove possession of all the
data in a very eﬃcient manner (i.e., with constant computa-
tion and bandwidth). Although many eﬃcient PoR schemes
for static data have been constructed, only two dynamic PoR
schemes exist. The scheme by Stefanov et al. (ACSAC 2012)
uses a large of amount of client storage and has a large audit
cost. The scheme by Cash et al. (EUROCRYPT 2013) is
mostly of theoretical interest, as it employs Oblivious RAM
(ORAM) as a black box, leading to increased practical over-
head (e.g., it requires about 300 times more bandwidth than
our construction).
We propose a dynamic PoR scheme with constant client
storage whose bandwidth cost is comparable to a Merkle
hash tree, thus being very practical. Our construction out-
performs the constructions of Stefanov et al. and Cash et al.,
both in theory and in practice. Speciﬁcally, for n outsourced
blocks of β bits each, writing a block requires β + O(λ log n)
bandwidth and O(β log n) server computation (λ is the se-
curity parameter). Audits are also very eﬃcient, requiring
β + O(λ2 log n) bandwidth. We also show how to make our
scheme publicly veriﬁable, providing the ﬁrst dynamic PoR
scheme with such a property. We ﬁnally provide a very eﬃ-
cient implementation of our scheme.
Categories and Subject Descriptors
K.6.5 [Management of Computing and Information
Systems]: Security and Protection
Keywords
dynamic proofs of retrievability; PoR; erasure code
1.
INTRODUCTION
Storage outsourcing (e.g., Amazon S3, Google Drive) has
become one of the most popular applications of cloud com-
puting, oﬀering various beneﬁts such as economies of scale
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’13, November 4–8, 2013, Berlin, Germany.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2477-9/13/11 ...$15.00.
http://dx.doi.org/10.1145/2508859.2516669.
and ﬂexible accessibility. When the cloud storage provider
(also referred to as server) is untrusted, an important chal-
lenge is how to oﬀer provable outsourced storage guarantees.
In particular, a data owner (also referred to as client) wishes
to obtain the following guarantees:
• Authenticated storage. The client wishes to verify
that data fetched from the server is correct, where cor-
rectness is equivalent to authenticity and freshness;
• Retrievability. The client needs assurance that the
server is indeed storing all of the client’s data, and that
no data loss has occurred.
Proofs of Retrievability (PoR), initially deﬁned and pro-
posed by Juels and Kalisky [14], are designed to oﬀer the
above guarantees for storage outsourcing applications, while
requiring small client-side state.
Most existing PoR schemes [7–9, 20, 26] however, are im-
practical due to the prohibitive costs associated with data
updates or client storage. A recent PoR construction by
Cash et al. [8] shows how to achieve asymptotic eﬃciency
in the presence of updates with constant client storage—
however, their scheme relies on Oblivious RAM (ORAM), a
rather heavy-weight cryptographic primitive. Recent works
on ORAM [21,24,25,30,31] have shown that even the fastest
known constructions incur a large bandwidth overhead in
practice. In particular, under typical parametrizations, and
with O(1) amount of client storage, known ORAM construc-
tions require 400+ blocks be transferred between a client
and a server for a single data access. In contrast, our con-
struction requires transferring only about 1.05 to
1.35 blocks per data access, which makes it both practi-
cal and orders of magnitudes more eﬃcient than the scheme
of Cash et al. [8].
This paper proposes a light-weight dynamic PoR construc-
tion that achieves comparable bandwidth overhead and client-
side computation with a standard Merkle hash tree, reducing
the above cost dramatically. Speciﬁcally, for each read and
write, our construction requires transferring O(λ log n) bits
of cryptographic information (independent of the block size)
in addition to transferring the block itself, where λ is the se-
curity parameter, and n is the total number of outsourced
blocks. To understand the implications of this, note that a
Merkle hash tree oﬀers authenticated storage, but does not
oﬀer retrievability guarantees. This paper essentially shows
that under settings where the client-server bandwidth is the
bottleneck (as is often the case in practice—e.g., Bitcoin
and various ﬁle sharing applications [1]), we can make PoR
almost as eﬃcient as a Merkle hash tree, i.e., ready to be
deployed in practical applications today.
325Scheme
Iris [26]
Cash et al. [8]
This paper
(Section 5)
This paper
(Appendix)
Client storage
√
O(β
n)
Write cost
Server cost
O(β)
BW
O(β)
Audit cost
√
Server cost
O(βλ
n)
√
BW
n)
O(βλ
O(β)
O(β)
O(βλ(log n)2)
O(βλ(log n)2)
O(βλ(log n)2) O(βλ(log n)2)
O(β log n)
β + O(λ log n)
O(βλ log n)
β + O(λ2 log n)
Veriﬁability
secret
secret
secret
O(βλ)
O(β log n)
β(1 + ) + O(λ log n)
for any constant  > 0
O(βλ log n)
O(βλ log n)
public
β: block size (in terms of number of bits).
λ: the security parameter.
n: an upper bound on the number of blocks.
Server cost: includes the server-side disk I/O cost, and server computation.
BW: The client-server bandwidth cost.
Table 1: Comparison with existing dynamic PoR. All schemes have O(βn) server side storage. Reads can be handled
by keeping a separate, Merkle-hashed, up-to-date copy of the dataset; therefore each read costs β + O(λ log n). Numbers in
this table are optimized for the typical case β ≥ λ. We note that for the ORAM-based scheme of Cash et al., we omit log log n
terms from the denominator.
In terms of disk I/O overhead on the server, our scheme
also achieves asymptotic improvements for reads, writes,
as well as audits, in comparison with the state-of-the-art
scheme [8] (see Table 1). In our scheme, reads cost no more
than a Merkle hash tree in terms of server disk I/O. Writes
incur moderate server disk I/O: the server needs to read,
write, and compute on O(log n) blocks for each write. How-
ever, our algorithms for writing access blocks sequentially,
signiﬁcantly reducing the disk seeks required for writes. We
have a full-ﬂedged, working implementation of our scheme
and report detailed experimental results from a deployment.
We also plan to open source our code in the near future.
We also point out that due to the blackbox application
of ORAM, the scheme by Cash et al. [8] additionally oﬀers
access pattern privacy which we do not guarantee. In ap-
plications that demand access privacy, ORAM is necessary.
We observe that the deﬁnition of PoR itself does not require
access privacy, and when one requires only PoR guarantees
(i.e., authenticated storage and retrievability) but not access
privacy, a blackbox application of ORAM is impractical—
in fact, as this paper shows, one can design truly practical
PoR schemes when access privacy is not required. Table 1
summarizes the asymptotic performance of our scheme in
comparison with related work.
1.1 Related Work
Comparison with proofs of data possession. A closely
related line of research is called Proofs of Data Possession
(PDP), initially proposed by Ateniese et al. [5]. We stress
that PDP provides much weaker security guarantees than
PoR. A successful PoR audit ensures that the server main-
tains knowledge of all outsourced data blocks; while a PDP
audit only ensures that the server is storing most of the
data. With PDP, a server that has lost a small number of
data blocks can pass an audit with signiﬁcant probability1.
Erway et al. [10] recently demonstrated a dynamic PDP
scheme with β + O(λ log n) cost for reads and writes, and
β +O(λ2 log n) cost for audits. We stress that we provide the
much stronger PoR guarantees, roughly at the same prac-
tical and asymptotic overhead as dynamic PDP schemes.
1While some PDP schemes [6] achieve full security, they re-
quire that the server read all of the client’s data during an
audit, and thus is impractical.
Proofs of retrievability. Static proofs of retrievability
were initially proposed by Juels and Kaliski [14], and later
improved in a series of subsequent works [7–9, 17, 20, 26, 28,
32]. While some works [17, 28, 32] aim to achieve PoR, they
essentially only achieve the weaker PDP guarantees when
they wish to support dynamic updates eﬃciently.
The ﬁrst dynamic proofs of retrievability construction was
proposed by Stefanov, van Dijk, Juels, and Oprea as part of
a cloud-based ﬁle system called Iris [26]. Although reads
n)
and writes in Iris are quite eﬃcient, it requires O(λβ
bandwidth, server computation, and server I/O to perform
an audit (it also requires β
n local space). Cash et al. [8]
proposed a dynamic POR scheme with constant client stor-
age based on Oblivious RAM, but it requires O(βλ(log n)2)
bandwidth and server I/O to perform writes and audits. In
contrast with these works, our scheme requires β+O(λ log n)
write bandwidth, β + O(λ2 log n) audit bandwidth, and con-
stant storage.
√
√
INFORMAL TECHNICAL OVERVIEW
2.
2.1 Previous Approaches
In this section we describe various approaches that could
be used for the problem of dynamic proofs of retrievability
and we highlight the problems of these approaches.
Strawman. We start from the most straightforward ap-
proach. Imagine that the client attaches a Message Authen-
tication Code (MAC) to every block before uploading it to
the server—to additionally ensure freshness under updates,
one can use a Merkle hash tree instead of MACs. During
the audit protocol, the client randomly samples a small num-
ber of blocks and ensures that the server possesses them by
checking them against the MACs.
In fact, this approach illustrates the underlying idea of
several Proof of Data Possession (PDP) schemes [10, 29].
The drawback of such an approach is that if the server has
lost a small number of blocks (e.g., a o(1) fraction), it can
still pass the audit with signiﬁcant probability.
Prior work: use of redundant encoding to boost de-
tection probability. To address the aforementioned issue,
prior PoR schemes [7, 9, 14, 20, 26] rely on erasure codes to
boost the detection probability, and ensure that the server
must possess all blocks to pass the audit test, which typ-
ically involves checking the authenticity of λ random code
326blocks, where λ is the security parameter. As a concrete
example, suppose that the client outsources a total of n
blocks, which are erasure coded into m = (1 + c)n blocks
for some constant 0 < c ≤ 1, such that knowledge of any n
blocks suﬃces to recover the entire dataset. In this way, the
server has to delete at least cn blocks to actually incur data
loss—however, if the server deletes that many blocks, it will
fail the above audit protocol with overwhelming probability
(speciﬁcally with probability at least 1 − 1/(1 + c)λ).
Diﬃculty with updates. The issue with using erasure
codes is that if the client wishes to update a single block,
it has to additionally update cn parity blocks—this can be
very expensive in practice.
One (failed) attempt to support updates (as described by
Cash et al. [8]) is to use a local erasure coding scheme, i.e.,
each block only aﬀects a small number of codeword blocks.
For this approach to work, all the codeword blocks need
to be randomly permuted, such that the server does not
learn which codeword blocks correspond to one original data
block—since otherwise, the server can selectively delete all