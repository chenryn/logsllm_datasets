intruder
Intruder
approaching
angle
Interval Analysis
2.3
Interval arithmetic studies the arithmetic operations on
intervals rather than concrete values. As discussed above,
since (1) the DNN safety property checking requires set-
ting input features within certain ranges and checking the
output ranges for violations, and (2) the DNN computa-
tions only include additions and multiplications (linear
transformations) and simple nonlinear operations (e.g.,
ReLU), interval analysis is a natural ﬁt to our problem.
We provide some formal deﬁnitions of interval extensions
of functions and their properties below. We use these
deﬁnitions in Section 4 for demonstrating the correctness
of our algorithm.
Formally, let x denote a concrete real value and X :=
[X,X] denote an interval, where X is the lower bound,
and X is the upper bound. An interval extension of a
function f (x) is a function of intervals F such that, for
any x ∈ X, F([x,x]) = f (x). The ideal interval extension
F(X) approaches the image of f , f (X) := { f (x) : x ∈ X}.
Let f (X1,X2, ...,Xd) := { f (x1,x2, ...,xd) : x1 ∈ X1,x2 ∈
X2, ...,xd ∈ Xd} where d is the number of input dimen-
sions. An interval valued function F(X1,X2, ...,Xd) is
inclusion isotonic if, when Yi ⊆ Xi for i = 1, ...,d, we have
F(Y1,Y2, ...,Yd) ⊆ F(X1,X2, ...,Xd)
An interval extension function F(X) that is deﬁned on
an interval X0 is said to be Lipschitz continuous if there is
some number L such that:
∀X ⊆ X0,w(F(X)) ≤ L· w(X)
where w(X) is the width of interval X, and X here denotes
X = (X1,X2, ...,Xd), a vector of intervals [45].
3 Overview
Interval analysis is a natural ﬁt to the goal of verifying
safety properties in neural networks as we have discussed
in Section 2.3. Naively, by setting input features as inter-
vals, we could follow the same arithmetic performed in
the DNN to compute the output intervals. Based on the
output interval, we can verify if the input perturbations
will ﬁnally lead to violations or not (e.g., output intervals
go beyond a certain bound). Note that, while lack of
violations due to over-approximations,
However, naively computing output intervals in this
way suffers from high errors as it computes extremely
loose bounds due to the dependency problem. In particu-
lar, it can only get a highly conservative estimation of the
1
2
1
1
3
-1
Steering angle
Figure 2: Running example to demonstrate our technique.
output range, which is too wide to be useful for checking
any safety property. In this section, we ﬁrst demonstrate
the dependency problem with a motivating example using
naive interval analysis. Next, based on the same example,
we describe how the techniques described in this paper
can mitigate this problem.
A working example. We use a small motivating exam-
ple shown in Figure 2 to illustrate the inter-dependency
problem and our techniques in dealing with this problem
in Figure 3.
Let us assume that the sample NN is deployed in an
unmanned aerial vehicle taking two inputs (1) distance
from the intruder and (2) intruder approaching angle while
producing the steering angle as output. The NN has ﬁve
neurons arranged in three layers. The weight attached to
each edge is also shown in Figure 3 .
Assume that we aim to verify if the predicted steering
angle is safe by checking a property that the steering angle
should be less than 20 if the distance from the intruder is
in [4,6] and the possible angle of approaching intruder is
in [1,5].
Let x denote the distance from an intruder and y de-
note the approaching angle of the intruder. Essentially,
given x ∈ [4,6] and y ∈ [1,5], we aim to assert that
f (x,y) ∈ [−∞,20]. Figure 3a illustrates the naive inter-
val propagation in this NN. By performing the interval
multiplications and additions, along with applying the
ReLU activation function, we get the output interval to
be [0,22]. Note that this is an overestimation because the
upper bound 22 cannot be achieved: it can only appear
when the left hidden neuron outputs 27 and the right one
outputs 5. However, for the left hidden neuron to output
27, the conditions x = 6 and y = 5 have to be satisﬁed.
Similarly, for the right hidden neuron to output 5, the
conditions x = 4 and y = 1 have to be satisﬁed. These
two conditions are contradictory and therefore cannot be
satisﬁed simultaneously and therefore the ﬁnal output 22
can never appear. This effect is known as the dependency
problem [45].
1602    27th USENIX Security Symposium
USENIX Association
[4,6]
[1,5]
2
[11,27]
[11,27]
1
1
3
1
[5,11]
-1
[5,11]
[4,6]
x
2
[11,27]
[2x+3y, 2x+3y] 
[11,27]
1
1
[1,5]
3
y
1
[5,11]
[x+y,x+y] 
-1
[5,11]
[4,6]
2
[11,21] 
[17,27] 
[1,3][3,5]
3
1
[5,9] 
[7,11] 
-1
1
1
[0,22]
[x+2y,x+2y] 
[6,16]
[2,16]U[6,20]=[2,20]
(a) Naive interval propagation
(b) Symbolic interval propagation
(c) Iterative bisection and refinement
Figure 3: Examples showing (a) naive interval extension where the output interval is very loose as it ignores the
inter-dependency of the input variables, (b) using symbolic interval analysis to keep track of some of the dependencies,
and (c) using bisection to reduce the over-approximation error.
As we have deﬁned that a safe steering angle must
be less than or equal to 20, we cannot guarantee non-
existence of violations, as the steering angle can have a
value as high as 22 according to the naive interval propa-
gation described above.
Symbolic interval propagation. Figure 3b demonstrates
how we maintain the symbolic intervals to preserve as
much dependency information as we can while propa-
gating the bounds through the NN layers. In this paper,
we only keep track of linear symbolic bounds and con-
cretize the bounds when it is not possible to maintain
accurate linear bounds. We compute the ﬁnal output in-
tervals using the corresponding symbolic equations. Our
approach helps in signiﬁcantly cutting down the over-
approximation errors.
For example, in the current example, the intermediate
neurons update their symbolic lower and upper bounds to
be 2x +3y and x +y, denoting the operation performed by
the previous linear transformation (taking the dot product
of the input and weight parameters). As we also know
2x + 3y > 0 and x + y > 0 for the given input range x ∈
[4,6] and y ∈ [1,5], we can safely propagate this symbolic
interval through the ReLU activation function.
In the ﬁnal layer, the propagated bound will be [x +
2y,x + 2y], where we can ﬁnally compute the concrete
interval [6,16]. This is tighter than the naive baseline
interval [0,22] and can be used to verify the property that
the steering angle will be ≤ 20.
In summary, symbolic interval propagation explicitly
represents the intermediate computations of each neuron
in terms of the symbolic intervals that encode the inter-
dependency of the inputs to minimize overestimation.
However, in more complex cases, there might be inter-
mediate neurons with symbolic bounds whose possible
values can potentially be negative. For such cases, we
can no longer keep the symbolic interval using a linear
equation while passing it through a ReLU. Therefore, we
concretize their upper and lower bounds and ignore their
dependencies. To minimize the errors caused by such
cases, we introduce another optimization, iterative reﬁne-
ment, as described below. As shown in Section 7, we
can achieve very tight bounds by combining these two
techniques.
Iterative reﬁnement. Figure 3c illustrates another opti-
mization that we introduce for mitigating the dependency
problem. Here, we leverage the fact that the dependency
error for Lipschitz continuous functions decreases as the
width of intervals decreases (any DNN with a ﬁnite num-
ber of layers is Lipschitz continuous as shown in Sec-
tion 4.2). Therefore, we can bisect the input interval by
evenly dividing the interval into the union of two consec-
utive sub-intervals and reduce the overestimation. The
output bound can thus be tightened as shown in the ex-
ample. The interval becomes [2,20], which proves the
non-existence of the violation. Note that we can iteratively
reﬁne the output interval by repeated splitting of the input
intervals. Such operations are highly parallelizable as
the split sub-intervals can be checked independently (Sec-
tion 7). In Section 4, we provide a proof that the iterative
reﬁnement can effectively reduce the width of the output
range to an arbitrary precision within ﬁnite steps for any
Lipschitz continuous DNN.
4 Proof of Correctness
Section 3 demonstrates the basic idea of naive interval
extension and the optimization of iterative reﬁnement.
In this section, we give the detailed proof about the cor-
rectness of interval analysis/estimation on DNNs, also
known as interval extension estimation, and the conver-
gence of iterative reﬁnement. The proofs are based on
two aforementioned properties of neural networks: inclu-
sion isotonicity and Lipschitz continuity. In general, the
correctness guarantee of interval extension holds for most
USENIX Association
27th USENIX Security Symposium    1603
ﬁnite DNNs while the convergence guarantee requires
Lipschitz continuity. In the following, we give the proof
of correctness for two most important techniques we use
throughout the paper, but the proof is generic and works
for our other optimizations such as symbolic interval anal-
ysis, inﬂuence analysis and monotonicity as described in
Section 5.
Let f denote an NN and F denote its naive interval
extension. We deﬁne the naive interval extension as a
function F(X) that (1) satisﬁes for all x ∈ X,F([x,x]) =
f (x) and (2) that only involves naive interval operations
during interval variable representations. For all the other
types of interval extensions, they can be easily analyzed
based on the following proof.
4.1 Correctness of Overestimation
We are going to demonstrate that, for the naive interval
extension of f , F always overestimates the theoretically
tightest output range f . According to our deﬁnition of
inclusion isotonicity described in Section 2, it sufﬁces
to prove that the naive interval extension of an NN is
inclusion isotonic. Note that we only consider neural
networks with ReLUs as activation functions for the fol-
lowing proof, but the proof can be easily extended to other
popular activation functions like tanh or sigmoid.
First, we need to demonstrate that F is inclusion iso-
tonic. Because ReLU is monotonic, so we can sim-
ply consider its interval extension to be ReluI(X) :=
[max(0,X),max(0,X)]. Therefore, ∀Y ⊂ X, we have
max(0,X) ≤ max(0,Y ) and max(0,X) ≥ max(0,Y ) so
that its interval extension Relu(Y ) ⊆ Relu(X). Most com-
mon activation functions are inclusion isotonic. We refer
interested readers to [45] for a list of common functions
that are inclusion isotonic.
We note that f (X) is a composition of activation func-
tions and linear functions. And we also see that linear
functions, as well as common activation functions, are
inclusion isotonic [45]. Because any combinations of in-
clusion isotonic functions are still inclusion isotonic, thus,
we have that the interval representation F(X) of f (X) is
inclusion isotonic.
Next, we show for arbitrary X = (X1, . . . ,Xd), that:
f (X) ⊆ F(X)
Applying the previously shown inclusion isotonicity prop-
erties of F(X), we get:
(cid:2)
(x1,...,xd )∈X
{ f (x1, . . . ,xd)}
F([x1,x1], . . . , [xd,xd])
f (X1, . . . ,Xd) =
=
(cid:2)
(x1,...,xd )∈X
for any such (x1, . . . ,xd) ∈ X, we have
since
is
Now,
F([x1,x1], . . . , [xd,xd]) ⊆ F(X1, . . . ,Xd),
([x1,x1], . . . , [xd,xd]) ⊆ (X1, . . . ,Xd),
inclusion isotonic. We thus get:
and F(X)
F([x1,x1], . . . , [xd,xd]) ⊆ F(X1, . . . ,Xd)
(1)
(cid:2)
(x1,...,xd )∈X
which is exactly the desired result.
Now, we get the result shown in Equation 1 that for
all input X, the interval extension of f , F(X), always
contains the true codomain (theoretically tightest bound)
for f (X).
4.2 Convergence in Finite Number of Splits
Now we see that the naive interval extension of f is an
overestimation of true output. Next, we show that iter-
atively splitting input is an effective way to reﬁne and
reduce such overestimated error. Empirically, we can see
ﬁnite number of splits allow us to approximate f with F
with arbitrary accuracy, this is guaranteed by Lipschitz
continuity property of NNs.
First, we need to prove F is Lipschitz continuous. It
is straightforward to show that many common activation
functions are Lipschitz continuous [45]. Here, we show
the natural interval extension ReluI is Lipschitz contin-