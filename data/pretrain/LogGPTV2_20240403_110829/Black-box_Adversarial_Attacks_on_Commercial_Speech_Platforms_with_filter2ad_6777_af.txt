Zeroth Order Optimization Based Black-box Attacks to Deep Neural Networks
without Training Substitute Models. In Proc. of ACM AISec.
[29] Si Chen, Ruoxi Jia, and Guo-Jun Qi. 2020.
Inversion Attacks. CoRR abs/2010.04092 (2020).
Improved Techniques for Model
[30] Si Chen, Kui Ren, Sixu Piao, Cong Wang, Qian Wang, Jian Weng, Lu Su, and Aziz
Mohaisen. 2017. You Can Hear But You Cannot Steal: Defending Against Voice
Impersonation Attacks on Smartphones. In Proc. of IEEE ICDCS.
[31] Tao Chen, Longfei Shangguan, Zhenjiang Li, and Kyle Jamieson. 2020. Meta-
morph: Injecting Inaudible Commands into Over-the-air Voice Controlled Sys-
tems. In Proc. of NDSS.
[32] Yuxuan Chen, Xuejing Yuan, Jiangshan Zhang, Yue Zhao, Shengzhi Zhang, Kai
Chen, and XiaoFeng Wang. 2020. Devilâ€™s Whisper: A General Approach for
Physical Adversarial Attacks Against Commercial Black-box Speech Recognition
Devices. In Proc. of USENIX Security.
[33] Minhao Cheng, Thong Le, Pin-Yu Chen, Huan Zhang, Jinfeng Yi, and Cho-Jui
Hsieh. 2019. Query-efficient Hard-label Black-box Attack: An Optimization-
based Approach. In Proc. of ICLR.
[34] Yinpeng Dong, Hang Su, Baoyuan Wu, Zhifeng Li, Wei Liu, Tong Zhang, and
Jun Zhu. 2019. Efficient Decision-based Black-box Adversarial Attacks on Face
Recognition. In Proc. of IEEE/CVF CVPR.
[35] Tianyu Du, Shouling Ji, Jinfeng Li, Qinchen Gu, Ting Wang, and Raheem Beyah.
2020. SirenAttack: Generating Adversarial Audio for End-to-end Acoustic Sys-
tems. In Proc. of ACM AsiaCCS.
[36] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. 2015. Model Inversion
Attacks that Exploit Confidence Information and Basic Countermeasures. In Proc.
of ACM SIGSAC.
[37] Sadaoki Furui. 1997. Recent Advances in Speaker Recognition. Pattern Recognition
Letters 18, 9 (1997), 859â€“872.
[40]
[38] Andreas Geiger, Philip Lenz, and Raquel Urtasun. 2012. Are We Ready for
Autonomous Driving? The Kitti Vision Benchmark Suite. In Proc. of IEEE/CVF
CVPR.
[39] Yuan Gong and Christian Poellabauer. 2017. Crafting Adversarial Examples for
Speech Paralinguistics Applications. CoRR abs/1711.03280 (2017).
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and
Harnessing Adversarial Examples. In Proc. of ICLR.
[41] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. 2013. Speech
Recognition with Deep Recurrent Neural Networks. In Proc. of IEEE ICASSP.
[42] Awni Y. Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich
Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, and
Andrew Y. Ng. 2014. Deep Speech: Scaling up End-to-end Speech Recognition.
CoRR abs/1412.5567 (2014).
[43] Nikolaus Hansen. 2016. The CMA Evolution Strategy: A Tutorial. CoRR
abs/1604.00772 (2016).
[44] Hynek Hermansky. 1990. Perceptual Linear Predictive (PLP) Analysis of Speech.
The Journal of the Acoustical Society of America 87, 4 (1990), 1738â€“1752.
[45] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed,
Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N
Sainath, et al. 2012. Deep Neural Networks for Acoustic Modeling in Speech
Recognition: The Shared Views of Four Research Groups. IEEE Signal Processing
Magazine 29, 6 (2012), 82â€“97.
[46] Matthew B. Hoy. 2018. Alexa, Siri, Cortana, and More: An Introduction to Voice
Assistants. Medical Reference Services Quarterly 37, 1 (2018), 81â€“88.
[47] Shengshan Hu, Xingcan Shang, Zhan Qin, Minghui Li, Qian Wang, and Cong
Wang. 2019. Adversarial Examples for Automatic Speech Recognition: Attacks
and Countermeasures. IEEE Communications Magazine 57, 10 (2019), 120â€“126.
[48] Christian Igel, Thorsten Suttorp, and Nikolaus Hansen. 2006. A Computational
Efficient Covariance Matrix Update and a (1+1)-CMA for Evolution Strategies.
In Proc. of ACM GECCO.
[49] Fumitada Itakura. 1975. Line Spectrum Representation of Linear Predictor Coef-
ficients of Speech Signals. The Journal of the Acoustical Society of America 57, S1
(1975), S35â€“S35.
[50] Arindam Jati, Chin-Cheng Hsu, Monisankha Pal, Raghuveer Peri, Wael Ab-
dAlmageed, and Shrikanth Narayanan. 2021. Adversarial Attack and Defense
Strategies for Deep Speaker Recognition Systems. Computer Speech & Language
68 (2021), 101199.
[51] Sonal Joshi, JesÃºs Villalba, Piotr Å»elasko, Laureano Moro-VelÃ¡zquez, and Najim
Dehak. 2021. Study of Pre-processing Defenses Against Adversarial Attacks on
State-of-the-art Speaker Recognition Systems. CoRR abs/2101.08909 (2021).
[52] William S. Mohn Jr. 1971. Two Statistical Feature Evaluation Techniques Applied
to Speaker Identification. IEEE Trans. Comput. 20, 9 (1971), 979â€“987.
[53] Shreya Khare, Rahul Aralikatte, and Senthil Mani. 2018. Adversarial Black-
box Attacks for Automatic Speech Recognition Systems Using Multi-objective
Genetic Optimization. CoRR abs/1811.01312 (2018).
[54] Tomi Kinnunen and Haizhou Li. 2010. An Overview of Text-independent Speaker
Recognition: From Features to Supervectors. Speech Communication 52, 1 (2010),
12â€“40.
[55] Felix Kreuk, Yossi Adi, Moustapha CissÃ©, and Joseph Keshet. 2018. Fooling
End-to-end Speaker Verification with Adversarial Examples. In Proc. of IEEE
ICASSP.
[56] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. 2012. ImageNet Classi-
fication with Deep Convolutional Neural Networks. In Proc. of NeurIPS.
[57] Deepak Kumar, Riccardo Paccagnella, Paul Murley, Eric Hennenfent, Joshua
Mason, Adam Bates, and Michael Bailey. 2018. Skill Squatting Attacks on Amazon
Alexa. In Proc. of USENIX Security.
187â€“214.
[60]
[59]
[58] Yun Lei, Nicolas Scheffer, Luciana Ferrer, and Mitchell McLaren. 2014. A Novel
Scheme for Speaker Recognition Using a Phonetically-aware Deep Neural Net-
work. In Proc. of IEEE ICASSP.
Jesse Levinson, Jake Askeland, Jan Becker, Jennifer Dolson, David Held, Soeren
Kammel, J Zico Kolter, Dirk Langer, Oliver Pink, Vaughan Pratt, et al. 2011.
Towards Fully Autonomous Driving: Systems and Algorithms. In Proc. of IEEE
Intelligent Vehicles Symposium.
Jinpeng Liu and Ke Tang. 2013. Scaling up Covariance Matrix Adaptation Evolu-
tion Strategy Using Cooperative Coevolution. In Proc. of Intelligent Data Engi-
neering and Automated Learning (IDEAL).
[61] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu. 2018. Towards Deep Learning Models Resistant to Adversarial
Attacks. In Proc. of ICLR.
[62] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. 2016.
Deepfool: A Simple and Accurate Method to Fool Deep Neural Networks. In Proc.
of IEEE/CVF CVPR.
[63] Lindasalwa Muda, Mumtaj Begam, and I. Elamvazuthi. 2010. Voice Recognition
Algorithms Using Mel Frequency Cepstral Coefficient (MFCC) and Dynamic
Time Warping (DTW) Techniques. CoRR abs/1003.4083 (2010).
[64] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. 2017. VoxCeleb: A
Large-scale Speaker Identification Dataset. In Proc. of INTERSPEECH.
[65] Mahesh Kumar Nandwana, Luciana Ferrer, Mitchell McLaren, Diego CastÃ¡n, and
Aaron Lawson. 2019. Analysis of Critical Metadata Factors for the Calibration of
Speaker Recognition Systems. In Proc. of INTERSPEECH.
[66] Phani Sankar Nidadavolu, Vicente Iglesias, JesÃºs Villalba, and Najim Dehak. 2019.
Investigation on Neural Bandwidth Extension of Telephone Speech for Improved
Speaker Recognition. In Proc. of IEEE ICASSP.
[67] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015.
LibriSpeech: An ASR Corpus Based on Public Domain Audio Books. In Proc. of
IEEE ICASSP.
[68] Kenneth V. Price. 2013. Differential Evolution. In Handbook of Optimization.
[69] Yao Qin, Nicholas Carlini, Garrison W. Cottrell, Ian J. Goodfellow, and Colin
Imperceptible, Robust, and Targeted Adversarial Examples for
Raffel. 2019.
Automatic Speech Recognition. In Proc. of ACM ICML.
[70] Kanishka Rao, Hasim Sak, and Rohit Prabhavalkar. 2017. Exploring Architec-
tures, Data and Units for Streaming End-to-end Speech Recognition with RNN-
Transducer. In Proc. of IEEE Automatic Speech Recognition and Understanding
Workshop (ASRU).
[71] Douglas A Reynolds. 2002. An Overview of Automatic Speaker Recognition
Technology. In Proc. of IEEE ICASSP.
[72] Douglas A. Reynolds, Thomas F. Quatieri, and Robert B. Dunn. 2000. Speaker
Verification Using Adapted Gaussian Mixture Models. Digital Signal Processing
10, 1-3 (2000), 19â€“41.
[73] Douglas A. Reynolds and Richard C. Rose. 1995. Robust Text-independent
Speaker Identification Using Gaussian Mixture Speaker Models. IEEE Transactions
on Speech and Audio Processing 3, 1 (1995), 72â€“83.
[74] Lea SchÃ¶nherr, Katharina Kohls, Steffen Zeiler, Thorsten Holz, and Dorothea
Kolossa. 2019. Adversarial Attacks Against Automatic Speech Recognition
Systems via Psychoacoustic Hiding. In Proc. of NDSS.
[75] Lea SchÃ¶nherr, Steffen Zeiler, Thorsten Holz, and Dorothea Kolossa. 2019. Ro-
bust Over-the-air Adversarial Examples Against Automatic Speech Recognition
Systems. CoRR abs/1908.01551 (2019).
[76] Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John P. Dickerson, Christoph
Studer, Larry S. Davis, Gavin Taylor, and Tom Goldstein. 2019. Adversarial
Training for Free!. In Proc. of NeurIPS.
[77] David Snyder, Daniel Garcia-Romero, Daniel Povey, and Sanjeev Khudanpur.
2017. Deep Neural Network Embeddings for Text-independent Speaker Verifica-
tion. In Proc. of INTERSPEECH.
[78] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru
Erhan, Ian J. Goodfellow, and Rob Fergus. 2014. Intriguing Properties of Neural
Networks. In Proc. of ICLR.
[79] Qian Tao and Raymond Veldhuis. 2010. Biometric Authentication System on
Mobile Personal Devices. IEEE Transactions on Instrumentation and Measurement
59, 4 (2010), 763â€“773.
[80] Rohan Taori, Amog Kamsetty, Brenton Chu, and Nikita Vemuri. 2019. Targeted
Adversarial Examples for Black Box Audio Systems. In Proc. of IEEE SPW.
[81] Florian TramÃ¨r, Alexey Kurakin, Nicolas Papernot, Ian J. Goodfellow, Dan Boneh,
and Patrick D. McDaniel. 2018. Ensemble Adversarial Training: Attacks and
Defenses. In Proc. of ICLR.
[82] Qian Wang, Baolin Zheng, Qi Li, Chao Shen, and Zhongjie Ba. 2021. Towards
Query-efficient Adversarial Attacks Against Automatic Speech Recognition
Systems.
IEEE Transactions on Information Forensics and Security 16 (2021),
896â€“908.
[83] Yixiang Wang, Jiqiang Liu, and Xiaolin Chang. 2021. Generalizing Adversarial
Examples by AdaBelief Optimizer. CoRR abs/2101.09930 (2021).
[84] Darrell Whitley. 1994. A Genetic Algorithm Tutorial. Statistics and Computing 4,
2 (1994), 65â€“85.
[85] Eric Wong, Leslie Rice, and J. Zico Kolter. 2020. Fast Is Better Than Free: Revisiting
[86] Hiromu Yakura and Jun Sakuma. 2019. Robust Audio Adversarial Example for a
Adversarial Training. In Proc. of ICLR.
Physical Attack. In Proc. of IJCAI.
[87] Bo Yang, Hengwei Zhang, Yuchen Zhang, Kaiyong Xu, and Jindong Wang. 2021.
Adversarial Example Generation with AdaBelief Optimizer and Crop Invariance.
CoRR abs/2102.03726 (2021).
[88] Zhuolin Yang, Bo Li, Pin-Yu Chen, and Dawn Song. 2019. Characterizing Audio
Adversarial Examples Using Temporal Dependency. In Proc. of ICLR.
[89] Zhenyu Yang, Ke Tang, and Xin Yao. 2008. Multilevel Cooperative Coevolution
for Large Scale Optimization. In Proc. of IEEE CEC.
[90] Xuejing Yuan, Yuxuan Chen, Yue Zhao, Yunhui Long, Xiaokang Liu, Kai Chen,
Shengzhi Zhang, Heqing Huang, Xiaofeng Wang, and Carl A. Gunter. 2018. Com-
manderSong: A Systematic Approach for Practical Adversarial Voice Recognition.
In Proc. of USENIX Security.
[91] Chunlei Zhang and Kazuhito Koishida. 2017. End-to-end Text-independent
Speaker Verification with Triplet Loss on Short Utterances. In Proc. of INTER-
SPEECH.
[92] Guoming Zhang, Chen Yan, Xiaoyu Ji, Tianchen Zhang, Taimin Zhang, and
Wenyuan Xu. 2017. DolphinAttack: Inaudible Voice Commands. In Proc. of ACM
CCS.
[93] Nan Zhang, Xianghang Mi, Xuan Feng, XiaoFeng Wang, Yuan Tian, and Feng
Qian. 2019. Dangerous Skills: Understanding and Mitigating Security Risks of
Voice-controlled Third-party Functions on Virtual Personal Assistant Systems.
In Proc. of IEEE S&P.
[94] Yuheng Zhang, Ruoxi Jia, Hengzhi Pei, Wenxiao Wang, Bo Li, and Dawn Song.
2020. The Secret Revealer: Generative Model-inversion Attacks Against Deep
Neural Networks. In Proc. of IEEE/CVF CVPR.
[95] Yangyong Zhang, Lei Xu, Abner Mendoza, Guangliang Yang, Phakpoom Chin-
prutthiwong, and Guofei Gu. 2019. Life after Speech Recognition: Fuzzing
Semantic Misinterpretation for Voice Assistant Applications. In Proc. of NDSS.
Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar C. Tatikonda, Nicha C.
Dvornek, Xenophon Papademetris, and James S. Duncan. 2020. AdaBelief Op-
timizer: Adapting Stepsizes by the Belief in Observed Gradients. In Proc. of
NeurIPS.
[96]
A ADAPTIVE DECOMPOSITION ALGORITHM
Here we present the detailed adaptive decomposition algorithm
in Occam. The cooperative co-evolution in Occam is illustrated
in Figure 7. Intuitively, one may want to probe all possible group
sizes and choose the one with the best performance. However, it is
very impractical because of the prohibitively high computational
overheads. Instead, we propose a dynamic and self-adapting group-
ing strategy. In our design, we let the group size vary in different
stages according to its performance, which is evaluated in a â€œpilotâ€
manner. More specifically, we first divide the variables into sub-
groups of candidate group sizes and optimize one of the subgroups.
The candidate group size is selected in a gradual manner, i.e., the
number of subgroups is twice or half as large as that in the previous
stage. We then determine the group size in the next stage according
to the performance of pilot test. A candidate size will be adopted in
the next stage if the performance of the pilot test is better. In this
way, the grouping strategy greatly reduces computation costs since
the pilot test only involves optimizing a small group of variables.
The details of the grouping algorithm are described as follows.
Since the degree of interdependence between the variables from
the natural audio and the audio AE gradually increases, we first
initialize the number of groups as ğ‘š = 1.
1) Create two lists ğ‘… = {ğ‘Ÿ1, ğ‘Ÿ2, ğ‘Ÿ3, ğ‘Ÿ4} and Î” = {ğ›¿ğ‘š/2, ğ›¿ğ‘š, ğ›¿2ğ‘š} to
record the impact of the four grouping strategies in the candidate
pool and different group sizes, respectively. Initialize ğ‘… = 0,
Î” = 0 and ğ‘š = 1.
Algorithm 3 Differential Evolution Attack
Input: The original audio ğ‘¥, the initial adversarial audio ğ‘¥â€², the
input space dimension ğ‘›, the Population ğ‘ ğ‘ƒ, and the total
number of iterations ğº.
Output: The adversarial audio sample ğ‘¥âˆ—.
1: Initialize the following parameters:
â€¢ ğ‘ ğ‘ƒ = 10, ğº = 3000, ğ¹ = 0.5,
â€¢ ğ‘” = 0, ğœ = 0.003, ğœ‡ = 0.2.
Generate three random integers ğ‘Ÿ1, ğ‘Ÿ2 and ğ‘Ÿ3 âˆˆ
{0, 1, ..., ğ‘ ğ‘ƒ âˆ’ 1}, with ğ‘Ÿ1 â‰  ğ‘Ÿ2 â‰  ğ‘Ÿ3;
ğ‘‹ğ‘šğ‘– = ğ‘‹ ğ‘”
ğ‘Ÿ1 + ğ‘Ÿğ‘ğ‘›ğ‘‘(0, ğœ‡)(ğ‘¥ âˆ’ ğ‘‹ ğ‘”
ğ‘Ÿ1) + ğ¹ Ã— (ğ‘‹ ğ‘”
ğ‘Ÿ2 âˆ’ ğ‘‹ ğ‘”
ğ‘Ÿ3);
â€² + ğ‘§;
2: for ğ‘– = 0 to ğ‘ ğ‘ƒ do
Sample ğ‘§ âˆ¼ N(0, ğœ2);
3:
ğ‘‹ 0
ğ‘– = ğ‘¥
4:
5: end for
6: while ğ‘” < ğº do
7:
8:
9:
âŠ² Mutation
for ğ‘– = 0 to ğ‘ ğ‘ƒ do
end for
âŠ² Selection
for ğ‘– = 0 to ğ‘ ğ‘ƒ do
if L(ğ‘‹ğ‘šğ‘–) < L(ğ‘‹ ğ‘”
ğ‘– ) then
= ğ‘‹ğ‘šğ‘–;
10:
11:
12:
13:
14:
ğ‘‹ ğ‘”+1
15:
end if
16:
end for
17:
ğ‘” â† ğ‘” + 1;
18:
19: end while
20: return ğ‘¥âˆ—.
ğ‘–
B DIFFERENTIAL EVOLUTION
Differential evolution is a gradient-free optimization method like
CMA-ES. To sufficiently justify the design choice of CMA-ES, we
also include the evaluations on the differential evolution attack
(DEA). Our experimental results (see Tables 8 and 4) show that DEA
cannot perform well on audio data. Here we present the algorithm
of differential evolution, as shown in Alg. 3. To more easily generate
offspring that is closer to the original audio, a bias term is also added
into the mutation step, and the crossover step is removed.
C DETAILED EXPERIMENT SETTING
C.1 Datasets
Common Voice. Common Voice [8] is the largest open-source hu-
man voice dataset launched by Mozilla, including 28 different lan-
guages and nearly 1,800 validated hours of voice data. The Common
Voice dataset is used to train ASR systems and test the effectiveness
of audio AEs.
Song. Song is released by CommanderSong [90] and also used
in Devilâ€™s Whisper [32]. It contains 20 songs divided into four
categories on average, including the soft, popular, rock, and rap.
Similar to Devilâ€™s Whisper [32], we select a total of 10 songs in
the soft and popular categories, which are less noisy and easier
to disguise, to evaluate the performance of Occam against speech
recognition APIs.
Figure 7: Illustration of cooperative co-evolution in Occam.
2) Compute the probabilities of selecting each grouping strategy
in the candidate pool as
ğ‘ğ‘Ÿğ‘œğ‘ ğ‘— =
, ğ‘— = 1, 2, 3, 4.
(14)
ğ‘’ğ‘Ÿ ğ‘—4
ğ‘—=1 ğ‘’ğ‘Ÿ ğ‘—
(cid:12)(cid:12)(cid:12)(cid:12) (ğ‘£ âˆ’ ğ‘£â€²)
ğ‘£ Â· ğ‘š
(cid:12)(cid:12)(cid:12)(cid:12) ,
3) Select a grouping strategy according to probabilities {ğ‘ğ‘Ÿğ‘œğ‘ ğ‘—}4
ğ‘—=1.
4) Group the variables into ğ‘š subgroups Gğ‘š = {ğº1, . . . , ğºğ‘š} ac-
5) Optimize the subspaces on Gğ‘š until the current stage ends.
cording to the selected grouping strategy.
Update ğ‘Ÿğ‘– and ğ›¿ğ‘š as
6) Run pilot test.
ğ‘Ÿ ğ‘— = ğ›¿ğ‘š =
(15)
where ğ‘£ is the best fitness of the last cycle, and ğ‘£â€² is the best
fitness of the current cycle.
i. If ğ›¿ğ‘š/2 = 0 and ğ‘š/2 â‰¥ 1, group the variables into ğ‘š/2 sub-
groups Gğ‘š/2 = {ğº1, . . . , ğºğ‘š/2} according to the selected
grouping strategy, and optimize ğº1 to obtain the perfor-
mance record
ğ‘£
(cid:12)(cid:12)(cid:12)(cid:12) (ğ‘£ âˆ’ ğ‘£â€²)
(cid:12)(cid:12)(cid:12)(cid:12) .
(cid:12)(cid:12)(cid:12)(cid:12) .
(cid:12)(cid:12)(cid:12)(cid:12) (ğ‘£ âˆ’ ğ‘£â€²)
ğ›¿ğ‘š/2 =
(16)
ii. If ğ›¿2ğ‘š = 0 and 2ğ‘š â‰¤ ğ‘›, group the variables into 2ğ‘š sub-
groups G2ğ‘š = {ğº1, . . . , ğº2ğ‘š} according to the selected
grouping strategy, and optimize ğº1 to obtain the perfor-
mance record
ğ›¿2ğ‘š =
(17)
7) If ğ›¿ğ‘š/2 is the maximum among Î”, then set ğ‘š = ğ‘š/2, ğ›¿2ğ‘š =
ğ›¿ğ‘š, ğ›¿ğ‘š = ğ›¿ğ‘š/2, and ğ›¿ğ‘š/2 = 0. Else if ğ›¿2ğ‘š is the maximum, then
set ğ‘š = 2ğ‘š, ğ›¿ğ‘š/2 = ğ›¿ğ‘š, ğ›¿ğ‘š = ğ›¿2ğ‘š, and ğ›¿2ğ‘š = 0.
ğ‘£
8) Go to step 3.
Collaborative Inforamtion Covariance Matrix Adaptation StrategyGix1xjxnGmx1xjxnx1xjxn Complete solutions  DecomposeG1       LibriSpeech. LibriSpeech [67] is an English speech corpus that
consists of 1,000 hour reading audio sampled at 16kHz.
VoxCeleb. VoxCeleb [64] is an open-source large-scale audio dataset
that contains more than 100,000 utterances from more than 7,000
celebrities. It consists of short human voice clips extracted from
interview videos uploaded to YouTube. In our experiments on the
SR systems, we use part of VoxCeleb as the enrollment data.
C.2 Target Systems
DeepSpeech. DeepSpeech [42] is a state-of-the-art ASR system
that performs speech-to-text tasks. It is usually used as the target
model in adversarial attacks [53, 80]. Note that although Deep-
Speech is open-source, Occam does not require any internal knowl-
edge about the target model.
Commercial Cloud Speech APIs. Speech-to-text API services
are widely used in many applications. To evaluate Occam on com-
mercial services, we choose six popular API services, including
Google Cloud Speech-to-Text [5], Microsoft Azure Speech Ser-
vice [6], Alibaba Short Speech Recognition [11], Tencent Short
Speech Recognition [10], and iFlytek voice dictation [3]. For the