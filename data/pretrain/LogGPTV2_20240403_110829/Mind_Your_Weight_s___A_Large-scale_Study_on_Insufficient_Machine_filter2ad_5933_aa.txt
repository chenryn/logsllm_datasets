**Title: Mind Your Weight(s): A Large-Scale Study on Insufficient Machine Learning Model Protection in Mobile Apps**

**Authors:**
- Zhichuang Sun, Northeastern University
- Ruimin Sun, Northeastern University
- Long Lu, Northeastern University
- Alan Mislove, Northeastern University

**Abstract:**
On-device machine learning (ML) is rapidly gaining popularity among mobile applications, enabling offline model inference while preserving user privacy. However, ML models, considered as core intellectual properties of their owners, are now stored on billions of untrusted devices, making them susceptible to theft. Leaked models can result in significant financial and security consequences.

This paper presents the first empirical study on ML model protection in mobile apps. Our study aims to answer three key questions with quantitative evidence:
1. How widely is model protection used in apps?
2. How robust are existing model protection techniques?
3. What impacts can stolen models have?

To address these questions, we developed a simple app analysis pipeline and analyzed 46,753 popular apps from the US and Chinese app markets. We identified 1,468 ML apps across various categories. Alarmingly, 41% of these apps do not protect their models at all, making them easily stealable from app packages. Even for apps that use model protection or encryption, we were able to extract the models from 66% of them using unsophisticated dynamic analysis techniques. The extracted models are predominantly commercial products used for face recognition, liveness detection, ID/bank card recognition, and malware detection. We quantitatively estimated the potential financial and security impact of a leaked model, which can amount to millions of dollars for different stakeholders.

Our study reveals that on-device models are currently at high risk of being leaked, and attackers are highly motivated to steal such models. Based on our large-scale study, we provide insights into this emerging security problem and discuss the technical challenges, aiming to inspire future research on robust and practical model protection for mobile devices.

**1. Introduction**
Mobile app developers are increasingly adopting on-device ML techniques to provide AI features such as facial recognition, augmented/virtual reality, image processing, and voice assistants. This trend is fueled by the availability of new AI chips in the latest smartphones, such as Apple’s Bionic neural engine, Huawei’s neural processing unit, and Qualcomm’s AI-optimized SoCs.

Compared to cloud-based ML, on-device ML offers unique benefits, including avoiding the need to send private user data to the cloud and not requiring a network connection. For app developers and ML solution providers, on-device ML significantly reduces the computational load on their servers.

However, on-device ML also introduces a new security challenge. ML models, which are often part of the core intellectual property (IP) of vendors, are stored locally on user devices. If not sufficiently protected, these models can be stolen or misused. On-device ML makes model protection more challenging because models are stored on fundamentally untrustworthy devices that may leak them to curious or malicious parties.

The consequences of model leakage are severe. First, it nullifies the R&D investment of the model owner, which includes human, data, and computing costs. Second, if a proprietary model is obtained by unethical competitors, the model owner loses their competitive edge or pricing advantage. Third, a leaked model can enable malicious actors to find adversarial inputs to bypass or confuse the ML systems, leading to reputation damage and critical failures in products (e.g., fingerprint recognition bypass).

This paper presents the first large-scale study on ML model protection and theft in mobile apps. Our study aims to shed light on the less understood risks and costs of model leakage/theft in the context of on-device ML. We present our findings, which answer the following questions with ample empirical evidence and observations:
- Q1: How widely is model protection used in apps?
- Q2: How robust are existing model protection techniques?
- Q3: What impacts can stolen models incur?

**2. Background**
**2.1 The Trend of On-Device Machine Learning**
There are two ways for mobile apps to use ML: cloud-based and on-device. Cloud-based ML involves sending requests to a cloud server for ML inference, which requires constant network connections and the uploading of raw user data. On the other hand, on-device ML avoids these drawbacks by performing inference without network connections, handling real-time tasks, and keeping user data on the device. However, this shift raises new security challenges, particularly in protecting valuable and proprietary ML models stored on untrusted devices.

**2.2 The Delivery and Protection of On-Device Models**
On-device ML models are typically trained on servers with rich computing resources and then shipped with app installation packages. Some models can also be downloaded separately after app installation to reduce package size. To protect on-device models, developers may encrypt or obfuscate them, or compile them into app code. However, these techniques only make it difficult to reverse-engineer a model, rather than preventing its theft or reuse.

**2.3 On-Device Machine Learning Frameworks**
Popular ML frameworks include Google TensorFlow and TensorFlow Lite, Facebook PyTorch and Caffe2, Tencent NCNN, and Apple Core ML. These frameworks use different file formats for storing ML models, such as ProtoBuf, FlatBuffer, MessagePack, pickle, and Thrift. Some companies develop customized or proprietary model formats to mitigate reverse engineering and leakage.

**2.4 On-Device Machine Learning Solution Providers**
App developers often use third-party ML solutions, such as Face++ and SenseTime, which offer SDKs for facial recognition, voice recognition, liveness detection, and other ML functionalities. These providers are highly motivated to protect their models, as model leakage can severely damage their business.

**3. Analysis Overview**
On-device ML is rapidly being adopted by apps, but its security implications remain largely unknown. This paper aims to shed light on the issue by conducting a large-scale study and providing quantified answers to three key questions:
- How widely is model protection used in apps? (§4)
- How robust are existing model protection techniques? (§5)
- What impacts can stolen models incur? (§6)

**4. Q1: How Widely Is Model Protection Used in Apps?**
**4.1 Android App Collection**
We collected 46,753 trending Android apps from the US and Chinese app markets, including Google Play, Tencent My App, and 360 Mobile Assistant.

**4.2 Methodology of ModelXRay**
ModelXRay statically detects if an app uses on-device ML and whether its models are protected or encrypted. It disassembles the APK file, extracts asset files and native libraries, and identifies ML frameworks and model files. ModelXRay uses keyword searching, file suffix matching, and entropy analysis to identify encrypted models. It generates a profile for each app, recording ML models and SDK libraries.

**4.3 Evaluation of ModelXRay**
We evaluated ModelXRay's accuracy and found it to be highly effective for our purposes, despite its simple analysis techniques.

**4.4 Findings and Insights**
Among the 1,468 ML apps, 41% do not protect their models at all. Most of these apps have high installation counts and span the top-ten app categories, highlighting the limited awareness of model thefts and the need for model protection among app developers.

**5. Q2: How Robust Are Existing Model Protection Techniques?**
For apps with encrypted models, we dynamically run the corresponding apps and use ModelXtractor to extract decrypted ML models from memory. We found that 66% of the protected models can be extracted using unsophisticated dynamic analysis techniques. These models are shared across multiple apps, affecting 347 apps (43% of all apps with protected models). Despite some model owners' efforts to enhance protection, these models can still be successfully extracted in plaintext.

**6. Q3: What Impacts Can Stolen Models Incur?**
We analyzed the financial and security impact of model leakage. Financially, the potential loss can be as high as millions of dollars, depending on the app revenue and the actual cost of the models. Security impacts include bypassing model-based access control, leading to reputation damage and product failures. Attackers can save on model license fees and R&D investments, while vendors may suffer from losing pricing advantages and market share.

**7. Current Model Protection Practices and Their Effectiveness**
We summarize the current model protection practices and their effectiveness. While some developers use encryption and obfuscation, these techniques are often insufficient to prevent model theft. There is an urgent need for research into robust and practical model protection for mobile devices.

**8. Research Insights and Limitations**
We discuss the insights gained from our study and the limitations of our analysis. Our work calls for further research on robust protection mechanisms for ML models on mobile devices.

**9. Related Work**
We survey related work in the field of ML model protection and compare our findings with existing studies.

**10. Conclusion**
In summary, our research contributes to the understanding of the risks and impacts of on-device ML model leakage. We found that a significant number of apps do not adequately protect their models, and even those that do can still be vulnerable to unsophisticated attacks. Our study highlights the need for robust and practical model protection techniques and provides insights to inform and assist future research in this area.