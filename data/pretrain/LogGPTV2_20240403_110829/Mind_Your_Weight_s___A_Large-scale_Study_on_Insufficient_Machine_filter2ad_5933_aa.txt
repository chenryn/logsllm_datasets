title:Mind Your Weight(s): A Large-scale Study on Insufficient Machine
Learning Model Protection in Mobile Apps
author:Zhichuang Sun and
Ruimin Sun and
Long Lu and
Alan Mislove
Mind Your Weight(s): A Large-scale Study on 
Insufficient Machine Learning Model Protection 
in Mobile Apps
Zhichuang Sun, Ruimin Sun, Long Lu, and Alan Mislove, Northeastern University
https://www.usenix.org/conference/usenixsecurity21/presentation/sun-zhichuang
This paper is included in the Proceedings of the 30th USENIX Security Symposium.August 11–13, 2021978-1-939133-24-3Open access to the Proceedings of the 30th USENIX Security Symposium is sponsored by USENIX.Mind Your Weight(s): A Large-scale Study on Insufﬁcient
Machine Learning Model Protection in Mobile Apps
Zhichuang Sun
Northeastern University
Ruimin Sun
Northeastern University
Alan Mislove
Northeastern University
Long Lu
Northeastern University
Abstract
On-device machine learning (ML) is quickly gaining
popularity among mobile apps. It allows ofﬂine model
inference while preserving user privacy. However, ML models,
considered as core intellectual properties of model owners,
are now stored on billions of untrusted devices and subject to
potential thefts. Leaked models can cause both severe ﬁnancial
loss and security consequences.
This paper presents the ﬁrst empirical study of ML model
protection on mobile devices. Our study aims to answer three
open questions with quantitative evidence: How widely is
model protection used in apps? How robust are existing model
protection techniques? What impacts can (stolen) models in-
cur? To that end, we built a simple app analysis pipeline and an-
alyzed 46,753 popular apps collected from the US and Chinese
app markets. We identiﬁed 1,468 ML apps spanning all popular
app categories. We found that, alarmingly, 41% of ML apps do
not protect their models at all, which can be trivially stolen from
app packages. Even for those apps that use model protection
or encryption, we were able to extract the models from 66%
of them via unsophisticated dynamic analysis techniques. The
extracted models are mostly commercial products and used for
face recognition, liveness detection, ID/bank card recognition,
and malware detection. We quantitatively estimated the poten-
tial ﬁnancial and security impact of a leaked model, which can
amount to millions of dollars for different stakeholders.
Our study reveals that on-device models are currently at
high risk of being leaked; attackers are highly motivated to
steal such models. Drawn from our large-scale study, we report
our insights into this emerging security problem and discuss
the technical challenges, hoping to inspire future research on
robust and practical model protection for mobile devices.
1 Introduction
Mobile app developers have been quickly adopting on-device
machine learning (ML) techniques to provide artiﬁcial intelli-
gence (AI) features, such as facial recognition, augmented/vir-
tual reality, image processing, voice assistant, etc. This trend
is now boosted by new AI chips available in the latest smart-
phones [1], such as Apple’s Bionic neural engine, Huawei’s
neural processing unit, and Qualcomm’s AI-optimized SoCs.
Compared to performing ML tasks in the cloud, on-device
ML (mostly model inference) offers unique beneﬁts desirable
for mobile users as well as app developers. For example,
it avoids sending (private) user data to the cloud and does
not require network connection. For app developers or
ML solution providers, on-device ML greatly reduces the
computation load on their servers.
On-device ML inference inevitably stores ML models
locally on user devices, which however creates a new security
challenge. Commercial ML models used in apps are often part
of the core intellectual property (IP) of vendors. Such models
may fall victim to theft or abuse, if not sufﬁciently protected.
In fact, on-device ML makes model protection much more
challenging than server-side ML because models are now
stored on user devices, which are fundamentally untrustworthy
and may leak models to curious or malicious parties.
The consequences of model leakage are quite severe.
First, with a leaked model goes away the R&D investment
of the model owner, which often includes human, data,
and computing costs. Second, when a proprietary model is
obtained by unethical competitors, the model owner loses the
competitive edge or pricing advantage for its products. Third,
a leaked model facilitates malicious actors to ﬁnd adversarial
inputs to bypass or confuse the ML systems, which can lead
to not only reputation damages to the vendor but also critical
failures in their products (e.g., ﬁngerprint recognition bypass).
This paper presents the ﬁrst large-scale study of ML model
protection and theft on mobile devices. Our study aims to
shed light on the less understood risks and costs of model
leakage/theft in the context of on-device ML. We present
our study that answers the following questions with ample
empirical evidence and observations.
• Q1: How widely is model protection used in apps?
• Q2: How robust are existing model protection tech-
• Q3: What impacts can (stolen) models incur?
niques?
USENIX Association
30th USENIX Security Symposium    1955
To answer these questions, we collected 46,753 trending
Android apps from the US and the Chinese app markets. To
answer Q1, we built a simple and automatic pipeline to ﬁrst
identify the ML models and SDK/frameworks used in an app,
and then detect if the ML models are encrypted. Among all
the collected apps, we found 1,468 apps that use on-device
ML, and 602 (41%) of them do not protect their ML models
at all (i.e., models are stored in plaintext form on devices).
Most of these apps have high installation counts (greater than
10M) and span the top-ten app categories, which underlines
the limited awareness of model thefts and the need for model
protection among app developers.
To answer Q2, for the encrypted models, we dynamically
run the corresponding apps and built an automatic pipeline to
identify and extract the decrypted ML models from memory.
This pipeline represents an unsophisticated model theft attack
that an adversary can realistically launch on her own device.
We found that the same protected models can be reused/shared
by multiple apps, and a set of 18 unique models extracted
from our dynamic analysis can affect 347 apps (43% of all the
apps with protected models). These apps cover a wide range
of ML frameworks, including TensorFlow, TFLite, Caffe,
SenseTime, Baidu, Face++, etc. They use ML for various
purposes, including face tracking, liveness detection, OCR,
ID card and bank card recognition, photo processing, and even
malware detection.
We also observed some interesting cases where a few model
owners spent extra effort on protecting their models, such
as encrypting both code and model ﬁles, encrypting model
ﬁles multiple times, or encrypting feature vectors. Despite the
efforts, these models can be successfully extracted in memory
in plaintext. These cases indicate that model owners or app de-
velopers start realizing the risk of model thefts but no standard
and robust model protection technique exists, which echos the
urgent need for research into on-device model protection.
Finally, to answer Q3, we present an analysis on the ﬁnancial
and security impact of model leakage on both the attackers and
the model vendors. We identify three major sources of impact:
the research and development investment on the ML models,
the ﬁnancial loss due to competition, and the security impact
due to model evasion. We found that the potential ﬁnancial
loss can be as high as millions of dollars, depending on the
app revenue and the actual cost of the models. The security
impact includes bypassing the model-based access control,
which may result in reputation damage or even product failure.
By performing the large-scale study and ﬁnding answers
to the three questions, we intend to raise the awareness of the
model leak/theft risks, which apps using on-device ML are
facing even if models are encrypted. Our study shows that the
risks are realistic due to absent or weak protection of on-device
models. It also shows that attackers are not only technically
able to, but also highly motivated to steal or abuse on-device
ML models. We share our insights and call for future research
to address this emerging security problem.
In summary, the contributions of our research are:
• We apply our analysis pipeline on 46,753 Android apps
collected from US and Chinese app markets. We found
that among the 1,468 apps using on-device ML, 41% do
not have any protection on their ML models. For those do,
66% of them still leak their models to an unsophisticated
runtime attack.
• We provide a quantiﬁed estimate on the ﬁnancial and
security impact of model leakage based on case studies.
We show that attackers with stolen models can save as
high as millions of dollars, while vendors can encounter
pricing disadvantage and falling market share. Further
model evasion may cause illegal access to private
information of end users.
• Our work calls for research on robust protection
mechanisms for ML models on mobile devices. We share
our insights gained during the study to inform and assist
future work on this topic.
The rest of the paper is organized as follows. Section
2 introduces the background knowledge about on-device
ML. Section 3 presents an overview of our analysis pipeline.
Sections 4, 5, and 6 answers the questions Q1, Q2, and Q3,
respectively. Section 7 summarizes the current model protec-
tion practices and their effectiveness. Section 8 discusses the
research insights and the limitations of our analysis. Section 9
surveys the related work and Section 10 concludes the paper.
2 Background
The Trend of On-device Machine Learning: Currently,
there are two ways for mobile apps to use ML: cloud-based and
on-device. In cloud-based ML, apps send requests to a cloud
server, where the ML inference is performed, and then retrieve
the results. The drawbacks include requiring constant network
connections, unsuitable for real-time ML tasks (e.g., live object
detection), and needing raw user data uploaded to the server.
Recently, on-device ML inference is quickly gaining popular-
ity thanks to the availability of hardware accelerators on mobile
devices and the the ML frameworks optimized for mobile apps.
On-device ML avoids the aforementioned drawbacks of cloud-
based ML. It works without network connections, performs
well in real-time tasks, and seldom needs to send (private) user
data off the device. However, with ML inference tasks and ML
models moved from cloud to user devices, on-device ML raises
a new security challenge to model owners and ML service
providers: how to protect the valuable and proprietary ML mod-
els now stored and used on user devices that cannot be trusted.
The Delivery and Protection of On-device Models :
Typically, on-device ML models are trained by app devel-
opers or ML service providers on servers with rich computing
resources (e.g., GPU clusters and large storage servers).
Trained models are shipped with app installation packages. A
model can also be downloaded separately after app installation
1956    30th USENIX Security Symposium
USENIX Association
To answer these questions, we built a static-dynamic app
analysis pipeline. We note that this pipeline and the analysis
techniques are kept simple intentionally and are not part of
the research contributions of this work. The goal of our study
is to understand how easy or realistic it is to leak or steal ML
models from mobile apps, rather than demonstrating novel or
sophisticated app analysis and reverse-engineering techniques.
Our analysis pipeline represents what a knowledgeable yet not
extremely skilled attacker can already achieve when trying to
steal ML models from existing apps. Therefore, our analysis
result gives the lower bound of (or a conservative estimate on)
how severe the model leak problem currently is.
The workﬂow of our analysis is depicted in Figure1. Apps
ﬁrst go through the static analyzer, ModelXRay, which detects
the use of on-device ML and examines the model protection,
if any, adopted by the app. For apps with encrypted models,
the pipeline automatically generates the analysis scripts and
send them to the dynamic analyzer, ModelXtractor, which
performs a non-sophisticated form of in-memory extraction
of model representations. ModelXtractor represents a realistic
attacker who attempts to steal the ML models from an app
installed on her own phone. Models extracted this way are in
plaintext formats, even though they exist in encrypted forms
in the device storage or the app packages. Our evaluation
of ModelXRay and ModelXtractor (§4.3 and §5.3) shows
that they are highly accurate for our use, despite the simple
analysis techniques. We report our ﬁndings and insights drawn
from the large-scale analysis results produced by ModelXRay
and ModelXtractor in §4.4 and §5.4, respectively.
We investigated both the ﬁnancial impact and the security
impact of model leakages. For ﬁnancial impact, we found
that the attackers would beneﬁt from the savings of model
licenses fee and Research & Development (R&D) investment;
while the model vendors would suffer from losing pricing
advantages and market share. The security impact includes
easier bypass of model based access control and further
security and privacy breaches, which could affect both the end
users and the model vendors. (§6).
4 Q1: How Widely Is Model Protection Used
in Apps?
4.1 Android App Collection
We collect apps from three Android app markets: Google Play,
Tencent My App, and 360 Mobile Assistant. They are the lead-
ing Android app stores in the US and China [35]. We download
the apps labeled TRENDING and NEW across all 55 categories
from Google Play (12,711), and all recently updated apps from
Tencent My App (2,192) and 360 Mobile Assistant (31,850).
4.2 Methodology of ModelXRay
ModelXRay statically detects if an app uses on-device ML
and whether or not its models are protected or encrypted.
Figure 1: Overview of Static-Dynamic App Analysis Pipeline
to reduce the app package size. Model inference is performed
by apps on user devices, which relies on model ﬁles and ML
frameworks (or SDKs). To protect on-device models, some
developers encrypt/obfuscate them, or compile them into app
code and ship them as stripped binaries [9, 25]. However, such
techniques only make it difﬁcult to reverse a model, rather
than strictly preventing a model from being stolen or reused.
On-device Machine Learning Frameworks: There are tens
of popular ML frameworks, such as Google TensorFlow and
TensorFlow Lite [27], Facebook PyTorch and Caffe2 [8],
Tencent NCNN [25], and Apple Core ML [10]. Among
these frameworks, TensorFlow Lite, Caffe2, NCNN and Core
ML are particularly optimized for mobile apps. Different
frameworks use different ﬁle formats for storing ML models
on devices, including ProtoBuf (.pb, .pbtxt), FlatBuffer (.tﬂite),
MessagePack (.model), pickle (.pkl), Thrift (.thrift), etc. To mit-
igate model reverse engineering and leakage, some companies
developed customized or proprietary model formats [53, 61].
On-device Machine Learning Solution Providers: For
cost efﬁciency and service quality, app developers often use
third-party ML solutions, rather than training their own models
or maintaining in-house ML development teams. The popular
providers of ML solutions and services include Face++ [13]
and SenseTime [34], which sell ofﬂine SDKs (including on-
device models) that offer facial recognition, voice recognition,
liveness detection, image processing, Optical Character Recog-
nition (OCR), and other ML functionalities. By purchasing
a license, app developers can include such SDKs in their apps
and use the ML functionalities as black-boxes. ML solution
providers are more motivated to protect their models because
model leakage may severely damage their business [34].
3 Analysis Overview
On-device ML is quickly being adopted by apps, while its
security implications on model/app owners remain largely
unknown. Especially, the threats of model thefts and possible
ways to protect models have not been sufﬁciently studied.
This paper aims to shed light on this issue by conducting a
large-scale study and providing quantiﬁed answers to three
questions: How widely is model protection used in apps? (§4)
How robust are existing model protection techniques? (§5)
What impacts can (stolen) models incur? (§6)
USENIX Association
30th USENIX Security Symposium    1957
DynamicAnalysisPer-AppAnalysisScriptsAndroidAPKsUnencryptedModelsModelXRayAPKs withEncryptedModelsDecryptedModelsModelXtractorlibraries from the APK ﬁle, analyzes the native libraries and asset ﬁles to identify ML frameworks, SDK libraries and model ﬁles. Then it applies
model ﬁlters combining ﬁle sizes, ﬁle sufﬁxes and ML libraries to reduce false positives and use entropy analysis to identify encrypted models.
Figure 2: Identify Encrypted Models with ModelXRay
ModelXRay extracts an app’s asset ﬁles and
ModelXRay is simple by design and adopts a best-effort
detection strategy that errs on the side of soundness (i.e.,
low false positives), which is sufﬁcient for our purpose of
analyzing model leakage.
We only consider encrypted models as protected in this
study. We are aware that some apps obfuscate the description
text in the models. As we will discuss in Section 7, obfuscation
may make it harder for the attacker to understand the model,
but does not prevent the attacker from reusing it at all.
The workﬂow of ModelXRay is shown in Figure 2. For a
given app, ModelXRay disassembles the APK ﬁle and extracts
the app asset ﬁles and the native libraries. Next, it identiﬁes
the ML libraries/frameworks and the model ﬁles as follows:
ML Frameworks and SDK Libraries: On-device model
inference always use native ML libraries for performance
reasons. Inspired by Xu’s work [61], we use keyword searching
in binaries for identifying native ML libraries. ModelXRay
supports a conﬁgurable dictionary that maps keywords to
corresponding ML frameworks, making it easy to include new
ML frameworks or evaluate the accuracy of keywords(listed
in Appendix A1). Further, ModelXRay supports generic
keywords, such as “NeuralNetwork”,“LSTM”, “CNN”, and
“RNN” to discover unpopular ML frameworks. However, these
generic keywords may cause false positives. We evaluate and
verify the results in §4.3.
ML Model Files: To identify model ﬁles, previous work [61]
rely on ﬁle sufﬁx match to ﬁnd models that follow the
common naming schemes. We ﬁnd, however, many model
ﬁles are arbitrarily named. Therefore, We use a hybrid
approach combining ﬁle sufﬁx match and path keyword match
(e.g.,../models/arbitrary.name can be a model ﬁle). We
address false positives by using three ﬁlters: whether the ﬁle
size is big enough (more than 8 KB); whether it has a ﬁle sufﬁx
that is unlikely for ML models (e.g.,model.jpg); whether the
app has ML libraries.
Encrypted Model Files: We use the standard entropy test
to infer if a model ﬁle is encrypted or not. High entropy in a
ﬁle is typically resulted from encryption or compression [12].
For compressed ﬁles, we rule them out by checking ﬁle types
and magic numbers. We use 7.99 as the entropy threshold
for encryption in the range of [0,8], which is the average
entropy of the sampled encrypted model ﬁles (see §4.3).
Previous work [61] treats models that cannot be parsed by
ML framework as encrypted models, which is not suitable in
our analysis and has high false positives for several reasons,
such as the lack of a proper parser, customized model formats,
aggregated models, etc.
ML App Proﬁles: As the output, ModelXRay generates a
proﬁle for each app analyzed. A proﬁle comprises of two parts:
ML models and SDK libraries. For ML models, it records ﬁle
names, sizes, MD5 hash and entropy. In particular, the MD5
hashes help us identify shared/reused models among different
apps (as discussed in §4.4).
For SDK libraries, we record framework names, the
exported symbols, and the strings extracted from the binaries.
They contain information about the ML functionalities, such
as OCR, face detection, liveness detection. Our analysis