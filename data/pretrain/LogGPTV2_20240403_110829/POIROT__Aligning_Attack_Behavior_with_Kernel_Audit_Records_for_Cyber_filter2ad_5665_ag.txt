1.04 M
324.68 K
374.71 K
35.81 K
obj ∈
|V (Gp)|
1.48 M
2.25 M
897.63 K
2.38 M
30.33 M
5.32 M
859.03 K
|E(Gp)|
7.53 M
12.66 M
4.65 M
70.82 M
51.98 M
69.89 M
13.06 M
Search Time (s)
3.28
0.04
26.09 (BSD-3), 1.47 (BSD-4)
125.26 (Win-1), 46.02 (Win-2)
1279.32 (Linux-1), 1170.86 (Linux-2)
385.16
20.72
Table 8: Statistics of logs, Consumption and Search Times.
= 1
3 which is close to the optimal value.
in 20 iterations of Poirot’s search algorithm for all the attack and
benign scenarios we have evaluated. As it is shown, the highest
F-score value is achieved when the threshold is at the interval [0.17,
0.54], which is the range in which all attack subgraphs are correctly
found, and no alarm is raised for benign datasets. The middle of
this interval, i.e., 0.35, maximizes the margin between attack and
benign scores, and choosing this value as the optimal threshold
minimizes the classification errors,. Therefore, we set the Cthr to 3
which results in 1
Cthr
5.4 Efficiency
The overheads and search times for the different tools we used are
shown in table 7. Redline and Loki are offline tools, searching for
artifacts that are left by the attacker on the system, while Splunk and
Poirot are online tools, searching based on system events collected
during runtime. Hence, Redline and Loki have no runtime overhead
due to audit log collection. The runtime overheads of Splunk and
Poirot due to log collection are measured using Apache benchmark
[3], which measures web server responsiveness, JetStream [73],
which measures browser execution times, and HDTune [11], which
measures heavy hard drive transactions. As shown in table 7, both
tools have shown negligible runtime overhead, while the runtime
of Splunk can be further improved by setting it up in a distributed
setting and offloading the data indexing task to another server.
The last column of table 7 shows the time it took searching for
IOCs per each tool. The search time of offline tools highly depends
on the number of running processes and volume of occupied disk
space, which was 500 GB in our case. On the other hand, the search
time of online methods highly depends on the log size, type and
number of activities represented by the logs. As our experiments
with real-world malware samples were running in a controlled en-
vironment without many background benign activities and Internet
connection, both Splunk and Poirot spend less than one minute to
search for all the IOCs mentioned in table 4. In the following, we
perform an in-depth analysis of Poirot ’s efficiency on the DARPA
TC scenarios, which overall contain over a month worth of log data
with combined attack and benign activities. The analysis is done
on an 8-core CPU with a 2.5GHz speed each and a 150GB of RAM.
Detection
Method
Type
Runtime Overhead
Apache
JetStream
HDTune
Search
Time (min)
[3]
-
-
[73]
-
-
[11]
-
-
Redline
Loki
Splunk
Poirot
Table 7: Efficiency Comparison with Related Systems.
offline
offline
online
online
124
215
< 1
< 1
3.70%
0.82%
2.94%
1.86%
4.37%
0.64%
Fig. 8: Selecting the Optimal Threshold Value.
5.3 Evaluation on Benign Datasets
To stress-test Poirot on false positives, we used the benign dataset
generated as part of the adversarial engagement in the DARPA TC
program and four machines (a client, a SSH server, a mail server
and a web server) we monitored for one month. Collectively, these
datasets contained over seven months worth of benign audit records
and billions of audit records on Windows, Linux, and FreeBSD. Dur-
ing this time, multiple users used these systems and typical attack-
free actions were conducted including web browsing, installing
security updates (including kernel updates), virus scanning, taking
backups, and software uninstalls.
After collecting the logs, we run Poirot to construct the prove-
nance graph, and then search for all the query graphs we have
extracted from the TC reports and the public malware reports. We
try up to 20 iterations starting from different seed node selections
per each query graph per each provenance graph and select the
highest score. Note that although these logs are attack-free, they
share many nodes and events with our query graphs, such as con-
fidential files, critical system files, file editing tools, or processes
related to web browsing/hosting, and email clients, all of which
were accessed during the benign data collection period. However,
even in cases where similar flows appear by chance, the influence
score prunes away many of these flows. Consequently, the graph
alignment score Poirot calculates among all the benign datasets is
at most equal to 0.16, well below the threshold.
Validating the Threshold Value. The selection of the threshold
value is critical to avoid false signals. Too low a threshold could
result in premature matching (false positives) while too high a
threshold could lead to missing reasonable matches (false nega-
tives). Thus, there is a trade-off in choosing an optimal threshold
value. To determine the optimal threshold value, we measured the
F-score using varying threshold values, as shown in Fig. 8. This
analysis is done based on the highest alignment score calculated
 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1RateThresholdOptimal ValuePrecisionRecallF-scoreSession 6D: Cyber Thread CCS ’19, November 11–15, 2019, London, United Kingdom1808Audit Logs Consumption. In table 8, the second column shows
the initial size of the logs on disk, the third column represents the
time it takes to consume all audits log events from disk for building
the provenance graph in memory. This time is measured as the wall-
clock time and varies depending on the size of each audit log and the
structure of audits logs generated in each platform (BSD, Windows,
Linux). The fourth column shows the total memory consumption
by each provenance graph. Comparing the size on disk versus
memory, we notice that we have an average compression of 1:4
(25%) via a compact in-memory provenance graph representation
based on[25]. However, if memory is a concern, it is still possible to
achieve better compression using additional techniques proposed
in this area [26, 40, 74]. The fifth column shows the duration during
which the logs were collected while columns 6, 7, and 8 show the
total number of subjects (i.e. processes), objects, and events in the
provenance graph that is built from the logs, respectively. We note
that the query graphs are on average 209K times smaller than the
provenance graph for these scenarios. Nevertheless, Poirot is still
able to find the exact embedding of Gq in Gp very fast, as shown
in the last column. We note that some scenarios are joined (e.g.,
Win-1&2) because they were executed concurrently on the same
machines.
Graph Analytics. In the last column of table 8, we show the run-
time of graph analytics for Poirot’s search algorithm. These times
are measured from the moment a search query is submitted until we
find a similar graph in Gp with an alignment score that surpasses
the threshold. Therefore, for Linux-2, the time includes the sum of
the times for two iterations. The main bottleneck is on the graph
search expansion (Step 3), and the time Poirot spends on graph
search depends on several factors. Obviously, the sizes of both query
and provenance graph are proportional to the runtime. However,
we notice that the node names in Gq and the shape of this graph
have a more significant effect. In particular, when there are nodes
with many candidate alignments, there is a higher chance to reverse
the direction multiple times and runtime increases accordingly.
6 CONCLUSION
Poirot formulates cyber threat hunting as a graph pattern matching
problem to reliably detect known cyber attacks. Poirot is based on
an efficient alignment algorithm to find an embedding of a graph
representing the threat behavior in the provenance graph of kernel
audit records. We evaluate Poirot on real-world cyber attacks and
on ten attack scenarios conducted by a professional red-team, over
three OS platforms, with tens of millions of audit records. Poirot
successfully detects all the attacks with high confidence, no false
signals, and in a matter of minutes.
ACKNOWLEDGMENTS
This work was supported by DARPA under SPAWAR (N6600118C4035),
AFOSR (FA8650-15-C-7561), and NSF (CNS-1514472, CNS-1918542
and DGE-1069311). The views, opinions, and/or findings expressed
are those of the authors and should not be interpreted as represent-
ing the official views or policies of the U.S. Government.
REFERENCES
[1] Manos Antonakakis, Roberto Perdisci, Wenke Lee, Nikolaos Vasiloglou, and
David Dagon. 2011. Detecting Malware Domains at the Upper DNS Hierarchy..
In USENIX security symposium, Vol. 11. 1–16.
[2] Manos Antonakakis, Roberto Perdisci, Yacin Nadji, Nikolaos Vasiloglou, Saeed
Abu-Nimeh, Wenke Lee, and David Dagon. 2012. From Throw-Away Traffic to
Bots: Detecting the Rise of DGA-Based Malware.. In USENIX security symposium,
Vol. 12.
[3] Apache. 2019. ab - Apache HTTP server benchmarking tool. https://httpd.apache.
org/docs/2.4/programs/ab.html. Accessed: 2019-08-27.
[4] Leyla Bilge, Davide Balzarotti, William Robertson, Engin Kirda, and Christopher
Kruegel. 2012. Disclosure: detecting botnet command and control servers through
large-scale netflow analysis. In Proceedings of the 28th Annual Computer Security
Applications Conference. ACM, 129–138.
[5] G Data Blog. 2013.
The Uroburos case: new sophisticated RAT identi-
fied. https://www.gdatasoftware.com/blog/2014/11/23937-the-uroburos-case-
new-sophisticated-rat-identified. Accessed: 2019-04-19.
[6] WeLiveSecurity by ESET. 2018. OceanLotus: Old techniques, new back-
https://www.welivesecurity.com/wp-content/uploads/2018/03/ESET_
door.
OceanLotus.pdf. Accessed: 2019-08-12.
[7] Threat Analysis by FortiGuard Labs. 2019. Analysis of a New HawkEye Vari-
ant. https://www.fortinet.com/blog/threat-research/hawkeye-malware-analysis.
html. Accessed: 2019-08-12.
[8] Jiefeng Cheng, Jeffrey Xu Yu, Bolin Ding, S Yu Philip, and Haixun Wang. 2008.
Fast graph pattern matching. In 2008 IEEE 24th International Conference on Data
Engineering. IEEE, 913–922.
[9] Mihai Christodorescu, Somesh Jha, and Christopher Kruegel. 2007. Mining
specifications of malicious behavior. In Proceedings of the the 6th joint meeting of
the European software engineering conference and the ACM SIGSOFT symposium
on The foundations of software engineering. ACM, 5–14.
[10] Lorenzo De Nardo, Francesco Ranzato, and Francesco Tapparo. 2009. The sub-
graph similarity problem. IEEE Transactions on Knowledge and Data Engineering
21, 5 (2009), 748–749.
[11] EFD. 2019. HD Tune. https://www.hdtune.com. Accessed: 2019-08-27.
[12] Wenfei Fan, Jianzhong Li, Shuai Ma, Nan Tang, Yinghui Wu, and Yunpeng Wu.
2010. Graph pattern matching: from intractable to polynomial time. Proceedings
of the VLDB Endowment 3, 1-2 (2010), 264–275.
[13] FireEye. 2013. OpenIOC Series: Investigating with Indicators of Compromise
(IOCs) - Part I. https://www.fireeye.com/blog/threat-research/2013/12/openioc-
series-investigating-indicators-compromise-iocs.html.
[14] FireEye. 2018. Open IOC. https://openioc.org.
[15] FireEye. 2018. Redline. https://www.fireeye.com/services/freeware/redline.html.
Accessed: 2019-04-23.
[16] Brian Gallagher. 2006. Matching structure and semantics: A survey on graph-
based pattern matching. AAAI FS 6 (2006), 45–53.
[17] Peng Gao, Xusheng Xiao, Ding Li, Zhichun Li, Kangkook Jee, Zhenyu Wu,
Chung Hwan Kim, Sanjeev R Kulkarni, and Prateek Mittal. 2018. {SAQL}: A
Stream-based Query System for Real-Time Abnormal System Behavior Detection.
In 27th {USENIX} Security Symposium ({USENIX} Security 18). 639–656.
[18] Peng Gao, Xusheng Xiao, Zhichun Li, Fengyuan Xu, Sanjeev R Kulkarni,
{AIQL}: Enabling Efficient Attack Investigation
and Prateek Mittal. 2018.
from System Monitoring Data. In 2018 {USENIX} Annual Technical Conference
({USENIX}{ATC} 18). 113–126.
[19] Rosalba Giugno and Dennis Shasha. 2002. Graphgrep: A fast and universal method
for querying graphs. In Pattern Recognition, 2002. Proceedings. 16th International
Conference on, Vol. 2. IEEE, 112–115.
[20] A. Goel, W. C. Feng, D. Maier, W. C. Feng, and J. Walpole. 2005. Forensix: a robust,
high-performance reconstruction system. In 25th IEEE International Conference
on Distributed Computing Systems Workshops.
[21] Ashvin Goel, Kenneth Po, Kamran Farhadi, Zheng Li, and Eyal de Lara. 2005.
The Taser Intrusion Recovery System. SIGOPS Oper. Syst. Rev. (2005).
[22] Kaspersky Lab: Global Research & Analysis Team (GReAT). 2015. Carbanak
APT: The Great Bank Robbery. https://media.kasperskycontenthub.com/wp-
content/uploads/sites/43/2018/03/08064518/Carbanak_APT_eng.pdf. Accessed:
2019-04-19.
[23] hasherezade. 2018. PE-Sieve: Scans a given process. Recognizes and dumps
a variety of potentially malicious implants (replaced/injected PEs, shellcodes,
hooks, in-memory patches). https://github.com/hasherezade/pe-sieve.
[24] Wajih Ul Hassan, Shengjian Guo, Ding Li, Zhengzhang Chen, Kangkook Jee,
Zhichun Li, and Adam Bates. 2019. NoDoze: Combatting Threat Alert Fatigue
with Automated Provenance Triage.. In NDSS.
[25] Md Nahid Hossain, Sadegh M. Milajerdi, Junao Wang, Birhanu Eshete, Rigel
Gjomemo, R. Sekar, Scott Stoller, and V.N. Venkatakrishnan. 2017. SLEUTH: Real-
time Attack Scenario Reconstruction from COTS Audit Data. In 26th USENIX
Security Symposium (USENIX Security 17). USENIX Association, Vancouver,
BC, 487–504. https://www.usenix.org/conference/usenixsecurity17/technical-
sessions/presentation/hossain
Session 6D: Cyber Thread CCS ’19, November 11–15, 2019, London, United Kingdom1809[26] Md Nahid Hossain, Junao Wang, R. Sekar, and Scott Stoller. 2018. Dependence
Preserving Data Compaction for Scalable Forensic Analysis. In USENIX Security
Symposium. USENIX Association.
[27] Ghaith Husari, Ehab Al-Shaer, Mohiuddin Ahmed, Bill Chu, and Xi Niu. 2017.
TTPDrill: Automatic and Accurate Extraction of Threat Actions from Unstruc-
tured Text of CTI Sources. In Proceedings of the 33rd Annual Computer Security
Applications Conference. ACM, 103–115.
[28] MISP: Andras Iklody. 2019. Default type of relationships in MISP objects. https:
//github.com/MISP/misp-objects/blob/master/relationships/definition.json. Ac-