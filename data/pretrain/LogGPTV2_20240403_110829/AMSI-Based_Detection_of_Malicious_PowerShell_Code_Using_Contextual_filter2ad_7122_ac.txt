and its alias IEX are closest to each other. Two additional
examples of this phenomenon are the Invoke-WebRequest
and its alias IWR, and the Get-ChildItem command and
its alias GCI.
We also measured distances within sets of several
to-
kens. Consider, for example, the four tokens $i, $j, $k and
$true (see the right side of Figure 5). The ﬁrst three are
usually used to name a numeric variable and the last represents
a boolean constant. As expected, the $true token mismatched
7The test set is not used for learning the embedding.
Fig. 3. Number of tokens appearing in x code instances, on a log-log scale.
The vertical line is at x = log2(100).
meaning in PowerShell. The tokenization process yielded
approximately four million distinct tokens. Since PowerShell
is case-insensitive, all tokens were normalized to the lower
case.
2) Rare tokens elimination: Since our goal is to deduplicate
similar code instances based on the tokens contained in them,
we remove random-string tokens by keeping only tokens that
appear in more than 100 code instances. To motivate the
selection of 100 as the token-frequency threshold, Figure 3
presents a histogram (using a log-log scale) of the number
of tokens that appear in exactly x distinct code instances, for
each value x. Note the change in trend around x = 9 (512
instances), indicating that many tokens appear in less than
about 500 instances, and substantially less tokens appear in
over 500 instances. To ensure that we do not remove too many
tokens, we used 100 as a threshold for a token to be considered
signiﬁcant. This resulted in a collection of 14,216 signiﬁcant
tokens. We note that rare tokens are removed only for the
sake of de-duplication. In general, such tokens are still used
for training the embedding layer and evaluating the models.
3) Code instance clustering: By identifying each instance
according to the set of the signiﬁcant tokens that appear in
it, we effectively cluster together all code instances that differ
only in the rare tokens they contain.
4) Cluster representatives selection: We arbitrarily select from
each of the resulting clusters a single representative. This
process yielded 116,976 distinct instances.
We note that the dimensions of the dataset speciﬁed earlier
are the numbers of distinct instances after the de-duplication
process. As shown by Table II, the de-duplication process
reduced the number of labeled instances from 198,477 to
116,976 – a 41% reduction.
TABLE II.
DE-DUPLICATED CODE INSTANCES STATISTICS
Benign instances
Malicious instances
Total instances
Original
188,797
9,680
198,477
Distinct
111,593
5,383
116,976
% Deduped
41%
44%
41%
V. CONTEXTUAL EMBEDDING OF POWERSHELL TOKENS
We remind the reader that our training approach, illustrated
by Figure 2, consists of an embedding stage followed by a
6
the others – it was the farthest (in terms of Euclidean distance)
from the center of mass of the group.
on widely-used traditional methods for feature extraction. We
proceed with the details.
More speciﬁc to the semantics of PowerShell and cy-
bersecurity, we checked the representations of
the to-
kens: normal, minimized, maximized, hidden and
bypass (see the left side of Figure 5). While the last token is
a legal value for the ExecutionPolicy ﬂag in PowerShell,
the rest are legal values for the WindowStyle ﬂag. As
expected, the vector representation of bypass was the farthest
from the center of mass of the vectors representing all other
four tokens.
Linear
linear
of W2V
relationships,
vector
in semantically-meaningful
results. Below are a few interesting relationships we found:
high - $false + $true (cid:39) low
‘-eq’ - $false + $true (cid:39) ‘-neq’
DownloadFile - $destfile + $str (cid:39) DownloadString
‘Export-CSV’ - $csv + $html (cid:39) ‘ConvertTo-html’
‘Get-Process’-$processes+$services (cid:39) ‘Get-Service’
Relationships: As W2V preserves
representation results
combinations
computing
linear
In each of the above expressions, the (cid:39) sign signiﬁes that
the vector on the right side is the closest (among all the vectors
representing tokens in the vocabulary) to the vector that is the
result of the computation on the left side, in terms of Euclidean
distance.
VI. CLASSIFICATION MODELS
We implemented and evaluated 10 DL detection models,
which differ in their architectures and in terms of whether
their input is processed as a sequence of tokens, a sequence
of characters, or both. In order to assess the extent to which
the DL models are able to compete with traditional detection
approaches, we also implemented two detectors that are based
A. Deep-Learning Based Detectors
We employ two deep-learning based architectures – a
Convolutional-Neural-Network (CNN) and a combination of
CNN and a Recurrent-Neural-Network (CNN-RNN).
1) Token-Level Architectures: We refer to DL architectures
that consider their input as a sequence of tokens as token-level
architectures. We implemented two token-level architectures:
One based on the CNN-RNN architecture of [45] and another
based on the CNN architecture presented by [46], [47]. In
both these architectures, on top of the embedding layer, we
used a convolutional layer with 128 ﬁlters and a kernel of size
3. In the CNN architecture, we then performed global max
pooling, followed by a dropout layer (see Section II-C). In the
CNN-RNN architecture, on top of the convolutional layer, we
used a max pooling layer of size 3, to preserve the sequential
nature of PowerShell code, followed by a bidirectional LSTM
layer with 32 units, a dropout of 0.5 and a recurrent dropout
of 0.02. Finally, in both architectures we used a single-node
dense output
layer with a Sigmoid activation function for
classiﬁcation. For full details, we provide our Keras [48] code
for model deﬁnitions in the appendix.
As previously mentioned, the ﬁrst layer of both our DL
architectures is an embedding layer. We experimented with
the following three options for setting the initial weights in
the embedding layer, for a total of 6 different token-based DL
detection models:
• Weights sampled from a uniform distribution: The two
resulting models are henceforth referred to as “CNN”
and “CNN-RNN”. We sometimes refer to this option
as inline embedding.
• Weights pretrained using W2V: The two resulting
models are henceforth referred to as “CNN-W2V” and
“CNN-RNN-W2V”.
• Weights pretrained using FastText: The two resulting
models are henceforth referred to as “CNN-FastText”
and “CNN-RNN-FastText”.
In both training and prediction, we used the ﬁrst 2,000
tokens from each PowerShell code instance, as only 3 benign
instances (and no malicious instance) in our labeled dataset
Fig. 4.
t-SNE 2D visualization of 5,000 tokens using W2V.
Fig. 5.
t-SNE 3D visualization of selected tokens.
7
Fig. 6. Histogram of number of tokens per code instance (by label), y-axis
uses logarithmic scale.
contain more than 2,000 tokens. Figure 6 presents the his-
togram of instance lengths (in terms of tokens), separately
per label, on a log scale. The distributions of benign and
malicious instances are similar, and both reach almost the same
maximum length.
2) Character-Level Architecture: Another model we ex-
perimented with is the best-performing model presented by
[15], named “4-CNN”, where character-level one-hot encoding
(which includes a special bit to account for character casing)
is used. It employs a 4-layer CNN architecture, containing a
single convolutional layer with 128 kernels of size 62x3 and
stride 1, followed by a max pooling layer of size 3 with no
overlap. This is followed by two fully-connected layers, both
of size 1,024 each followed by a dropout layer with probability
of 0.5, and an output layer.
3) Token-Character Level Architecture: The 7 models we
described so far use either a character-level or a token-level
representation, but not both. In order to combine both a token-
level and a character-level representation, we implemented and
evaluated an architecture similar to the CNN-RNN one, that
uses both a one-hot encoding representation of characters and
a token-level embedding layer. We henceforth refer to this
architecture as “Token-Char”. 8 Here, too, we experimented
with the three token embedding options (inline, W2V and
FastText), resulting in 3 additional DL detection models.
The use of two input representations requires applying
some adaptations to the architecture, as otherwise it would
result in a model that has too many trainable parameters,
thus increasing the risk of overﬁtting. In order to address
this issue, we reduced the number of input-tokens and input-
characters to 1,000 and also reduced the number of ﬁlters used
in the convolutional layer from 128 to 64. We also reduced the
number of tokens participating in the embedding process by
using only tokens that appear in at least 20 instances (instead
of 10); this reduced the number of tokens to 47,555.
Figure 7 depicts the “Token-Char” architecture. As can be
seen, it receives both a token-level and a character-level repre-
sentation of the input code. After the tokens are embedded and
the characters are encoded, each is being input to a separate
convolution layer with 64 ﬁlters. Next, for the token-level path,
we performed max pooling with a kernel of size 3 (as was
8We would like to thank Eran Galili from Microsoft for his help with the
architecture design and technical assistance.
Fig. 7. A diagram of the “Token-Char” model architecture. The result of
applying global max pooling on the character-level input is marked in blue, to
emphasise the fact that it has been duplicated in order for it to be processed
by the LSTM layer along with the token-level input.
done in the CNN-RNN architecture). As for the character-level
path, we used global max pooling, which resulted in a single
tensor of size 64 (the number of ﬁlters used in the previous
convolutional layer). We added a dropout layer with probability
0.5 for regularization (not shown in Figure 7).
We now explain how we combined the paths of the token-
level and the character-level
inputs. Since we use global
max pooling for the character convolutional layer, we had to
duplicate the resulting tensor before we concatenate it to the
output of the token-level layer. This allows us to apply the bi-
directional LSTM on an input that is based on both the token-
level embedding and the character-level encoding. In each of
the 332 LSTM input entries, the top 64 represent token-level
features and the bottom 64 represents character-level features.
Note that, as we did not apply global max pooling to the
token-level path, the token-level sequential nature of the code
is maintained. We use a biderctional LSTM layer with output
size of 32 and, ﬁnally, an output layer consisting of a single
node. Full technical details are provided in the appendix.
B. Traditional NLP-based detectors
We used two types of NLP feature extraction meth-
ods: character-level n-grams and token-level n-grams. For
character-level features, we used character n-grams for n ∈
{2, 3, 4}. For token-level features, we used token n-grams, for
8
n ∈ {1, 2, 3}. We only used tokens appearing in at least 10
instances. For both methods, we evaluated both term-frequency
(tf) and term-frequency-inverse-document-frequency (tf-idf) as
a weighting factor and then applied a logistic regression
classiﬁer on the extracted features (more details are provided
in the appendix). For each type of features (token-based or
character-based), we report on the evaluation results of the
best-performing model (optimal value of n), using tf-idf, as it
gave the best results in terms of true positive rate (TPR, a.k.a.
recall) when using a threshold keeping the false positive rate
(FPR) lower than 10−3.
VII. EXPERIMENTAL EVALUATION
In this section, we describe how we evaluated our detectors.
We then present and discuss evaluation results. This is followed
by an analysis of the contribution of contextual embedding
and a discussion of the added value of the character-level
representation.
We have split our labeled dataset according to instances
collection times to a test set, consisting of 10,136 instances
(1,329 of which are malicious and 8,807 of which are benign),
and a training set, consisting of 106,840 instances (4,054
malicious and 102,786 benign), on which our models were
trained and evaluated using cross-validation. The training set
includes instances seen during May-July 2018, while the test
set includes instances seen during August-October 2018.
We performed a 3-fold cross-validation on the training
set to select values for hyper-parameters, such as the size of
the kernel of the convolutional layer, the number of ﬁlters to
use, the size of the LSTM layer, etc. Cross-validation was
used also for selecting the number of training epochs to be
used, as follows: For each fold, we selected the model that
is generated in the epoch in which we obtained the highest
TPR on the validation set (with an FPR lower than 10−3).
As for performance evaluation on the test set – since the
above procedure generates 3 models for each detector (one per
fold), we apply all three to the test set and use their average
score. We used this technique, discussed in [49], in order to
avoid overﬁtting that may result from using too many training
epochs.
A. AUC results
For the traditional NLP models, we present the results of
the models that performed the best. These are the character-
level using tri-grams (Char-3-gram) and token-level using bi-
grams (Token-2-gram), both using tf-idf for feature weighting.
First, we focus on the area under the ROC curve (AUC) on
the validation set, presented in the AUC column in Table III.
As evident from Table III, all detectors obtain very high
AUC levels, above 0.987. At ﬁrst glance, this may lead one to
conclude that they all provide sufﬁciently good performance.
However, considering that in real-world deployments the rate
of PowerShell instances to be classiﬁed by our models may
be very high, even a low FPR of 1% will result in too many
false alarms that would deem the detection system impractical.
Thus, for a detector to be useful, it must maintain a very low
FPR. Consequently, in what follows we evaluate the TPR of
the detectors while enforcing very low FPR levels. Figure 8
presents the ROC curves of all models on the test set for FPR
Fig. 8. ROC curves of models on test set for TPR 0.005
lower than 0.5%. It can be seen that the Token-Char-FastText
model signiﬁcantly outperforms all other models. We proceed
by performing a detailed analysis of the TPR results for low
FPR.
B. TPR results
Columns ’Train’,
’Validation’ and ’Test’
in Table III
present the TPR of our detectors for FPR level ≤ 10−3,
over the training, validation and test sets. In general, when
conducting cross-validation on the training set, results are
reported only for the validation fold. We choose to report
also on the performance of our models on the training folds
(in the column with heading ’Train’), since this allows us to
better analyze the extent to which different models suffer from
overﬁtting. As we conduct the analysis at an FPR level of 10−3
and since we have a total of about 28,000 benign instances in
each training set fold, using this threshold translates to at most
28 FPs in each fold.
The TPR scores presented for the training and validation
sets in Table III are the average scores for the three folds. As
mentioned above, for each validation fold, we select the model
that provides the highest TPR (over the epochs) on this fold,
while keeping the FPR low. This yields three detection models
applied to each test set instance, resulting in three scores per
each instance. The results presented in the ’Test’ column of
table III are the average scores of these three models. We use
this technique for ensuring that we apply the best model, as
each epoch results in a different model, and after a certain
number of epochs the models starts to overﬁt.
While all classiﬁers achieve relatively high TPR values, the
performance of the traditional NLP detectors is substantially
lower than that of the DL detectors. In comparison to the NLP
detectors, the DL detectors improve TPR by up to 4 pp on the
validation set and by up to 23 pp on the test set.
The decrease in detectors’ performance on the test set
in comparison with the validation set is expected, since the
training set (which includes the validation set) and the test