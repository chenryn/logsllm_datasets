While HTTP/2 speciﬁes a push mechanism, it does not specify what poli-
cies to use for pushing. We have implemented a push policy, ideal-push, which
assumes that the server can assess which objects the client might request after
downloading the base HTML ﬁle. This is idealized, since there can be dynamic
content, e.g. Javascript can be executed at the browser, and its output can at
best be over-approximated by the server (for example, by static program analy-
sis). However, ideal-push gives an upper bound on HTTP/2 push performance.
Prioritization. The last object-layer component of rt-h2 is a component that
assigns priorities to objects. This prioritization represents a way for browsers to
control the way a TCP channel is shared across multiplexed resources. As with
push, the HTTP/2 speciﬁcation deﬁnes the mechanism for assigning priorities,
238
K. Zariﬁs et al.
but does not mandate a speciﬁc scheduling policy; rt-h2 can explore diﬀerent
prioritization policies. A basic type of prioritization enforced by today’s browsers
assigns relative bandwidth resources to Javascript, CSS and HTML ﬁles before
other ﬁle types since these ﬁles need to be processed by the browser and can
trigger downloads of other objects. In this paper, we explore a prioritization
policy which further preferentially prioritizes Javascript, CSS or HTML ﬁles
that are on the critical path. In practice, browsers could guess this prioritization
by extracting critical paths from historical traces of page downloads.
Interleaving. HTTP/2 permits interleaving of objects, and this component imple-
ments this capability. Among objects that can be multiplexed together at a given
time, it interleaves 16K chunks (frames) of these objects in FIFO order.
TCP Module. The core of rt-h2 is a custom, discrete-event, TCP simulator
which simulates TCP-CUBIC’s congestion window growth [9]. When determin-
ing what data to transmit, the TCP module supplies the interleaving mod-
ule with a desired number of bytes B that can be transmitted at each tick of
the simulator. The latter, in turn, consults the multiplexing, prioritization, and
push modules and determines which frames of which objects need to be served,
such that the total size of the frames is less than B. This is repeated until all
objects are served. The simulator clock ticks every RTT, changing the window
appropriately. We assume that connections are not bandwidth-limited, based
on the observation that for most (>90 %) of the traces, the HTTP/2 multi-
plexed payload was small enough (<1 MB) to easily ﬁt within several client-edge
RTTs without exceeding the client’s BW cap, assuming a 5 Mbps connection. In
this paper, we only present results assuming no loss, to focus on the impact of
page structure and other page characteristics on HTTP/2. As shown in previous
work [14], loss aﬀects HTTP/2 more negatively than HTTP/1.1 due to the single
channel. We did extend our model to incorporate loss. Speciﬁcally, we give each
packet an equal chance of getting dropped (1 % or 2 % in our experiments). If
one or more packets within one window are dropped, we assess a 77 % chance of
causing a retransmission time-out [8] and increment the time counter appropri-
ately. Our results were consistent with previous work [14], so we omit them for
brevity.
Preprocessing the Input. The HTTP/1.1 waterfall input to rt-h2 was produced
by a client running an unknown TCP stack version, and for which we know
HTTP layer request latency, but not TCP characteristics like loss. To compare
the two protocols on an even footing, we run the HTTP/1.1 waterfall through the
TCP module, without any HTTP/2 features on, then use the resulting waterfall
as input to the HTTP/2 model, and estimate ΔP LT based on those two. This way,
any inaccuracies in the TCP model impact both protocols equally. For the results
presented here, the loss rate is always set to zero and the RTT is inferred from
the download trace. Note that diﬀerent client characteristics (TCP stack version,
browser optimizations) could aﬀect the actual download slightly diﬀerently. We
do not try to infer all the client characteristics for each download. Instead we
come up with a generic model that captures the most popular TCP stack and
Modeling HTTP/2 Speed from HTTP/1 Traces
239
known client behaviors, and assume that implementation details do not aﬀect
the outcome signiﬁcantly, especially in large numbers.
Other Details. RT data does not include object sizes. We use a separate dataset to
obtain object sizes, and, for objects not listed in this dataset, we download them
to obtain size. To compute the output waterfall, we need dependencies between
objects: for this, we use techniques similar to those used in prior work [13].
Running a Waterfall Through RT-H2. Figure 1 shows how rt-h2 trans-
forms an input waterfall to its HTTP/2 equivalent. 3.js is prioritized over
2.css. As a result, the simulation returns an earlier completion time for it than
its original end time, and adjusts its dependent resources accordingly, shifting
5.js and its 2 children to the left. The request for 6.png and 7.png are requested
on the same channel, maintaining their distance from the end of 5.js, which
corresponds to processing time. The diﬀerence between the end times of the
respective last resources is calculated, and the onLoad event is shifted accord-
ingly. The ΔP LT is deﬁned as the % change between the times of the two onLoad
events.
4 Validation
Methodology. In this section, we validate the rt-h2 model against PLT diﬀer-
ences obtained from real traces for ground truth. The goal of validating rt-h2
is to understand whether the model’s estimates for ΔP LT are comparable to
those observed in a realistic experiment. We set up an Akamai CDN server in a
lab and conﬁgured it to serve 8 real websites both via HTTP/1.1 and HTTP/2.
These 8 websites are the most popular ones in the CDN among those who have
opted-in to resource timing monitoring and already use HTTPS.
We validate rt-h2 against those websites as follows: using Chrome, we down-
load each web page through the CDN server 100 times via HTTP/1.1 and 100
times via HTTP/2. For each HTTP/1.1 download we generate a resource tim-
ing beacon that is used as input to our tool, generating 100 estimated HTTP/2
waterfalls, and we obtain the estimated ΔP LT from those. We repeat this process
for 3 RTT values (20 ms, 50 ms, 100 ms). This is the induced round-trip between
the test client and the CDN edge server, which serves all of the pages’ (cacheable)
origin content. Since the base HTML ﬁle is not cached on the CDN because cus-
tomers want to generate pages dynamically, the client request for that ﬁle is
forwarded to the customer’s origin server, the latency to which is variable.
Figure 3 shows an example of this process for one of the target web pages.
There are 3 groups of 4 lines, each group representing a diﬀerent RTT. Solid lines
correspond to PLTs of real downloads, dashed lines correspond to their modeled
equivalents. Blue lines are HTTP/1.1 and red lines are HTTP/2. Speciﬁcally, in
each RTT group, the blue dashed line corresponds to the CDF of PLTs after
passing the 100 waterfalls through the model but without applying the HTTP/2
features (so, simply passing them through our TCP model), and the red dashed
lines corresponds to the CDF of estimated PLTs after applying HTTP/2. We
240
K. Zariﬁs et al.
Fig. 3. Example of validation on a real page for 3 RTT values (Color ﬁgure online).
want the diﬀerence between the dashed lines (model) to be similar to that of the
solid lines (ground truth), which means that the distribution transformation of
the PLTs in our model after applying HTTP/2 was similar to the transformation
of PLTs that the real downloads observed switching from HTTP/1.1 to HTTP/2.
In this example, which corresponds to p1 in Table 1, the accuracy of the model is
very good for RTTs of 20 ms and 50 ms, but slightly worse for 100 ms (predicted
ΔP LT = −18 %, when in reality HTTP/2 reduced the PLT by 11 % (ΔP LT =
−11 %)).
Note that there is no 1-1 mapping between ground truth and experiment
data points. HTTP/1.1 and HTTP/2 downloads were interleaved, to distribute
network eﬀects uniformly, but treating two adjacent HTTP/1.1 and HTTP/2
downloads as a pair has disadvantages: No two downloads are exactly equal, even
if back-to-back, both because network characteristics are ephemeral, and because
diﬀerent resources (in numbers or variations, e.g. ads) can be downloaded each
time. For this reason, we chose to look at aggregate distributions of suﬃciently
many samples rather than arbitrarily created pairs. The goal was not to validate
exactly how one speciﬁc download would change via HTTP/2 and how accurately
the model would predict that (because it is hard to produce exactly the same
download for a live page over the real Internet), but rather to see how the
distribution of PLTs of 100 downloads of a page changes via HTTP/2, and
validate that the model tracks that distribution change fairly accurately.
Table 1 shows the ground truth and predicted ΔP LT (%) for the 8 target
pages. The values shown are the medians of each set of the 100 runs. The model
estimates the overall impact of HTTP/2 on page load time of the test page
very accurately for zero loss, upon which most of our results are based. For all
estimations, the model always correctly estimates that the impact of HTTP/2
is positive, the diﬀerence between ground truth and estimated ΔP LT is within
20 % of the PLT, for 3/4 of them it is within 10 %. The accuracy can decrease for
higher RTTs (100 ms). However, we note that such high RTT values to Akamai’s
CDN edge are rarely observed. In the run with the lowest accuracy (p2, RTT =
50 ms), which has a median PLT of 2493 ms for HTTP1.1, the model predicted
2310.5 ms instead of the actual 2010 ms for HTTP/2. In this worst case the model
is 200 ms oﬀ but still correctly predicts that HTTP/2 is faster. Given that this
low accuracy happens less often for lower RTTs (which are more realistic) and
Modeling HTTP/2 Speed from HTTP/1 Traces
241
considering the simplicity of the model, these validation results are encouraging
for using the model to draw conclusions on larger scale data.
Table 1. ΔP LT (%) prediction. For each page (p1-p8) and RTT value, “Real” indicates
the ground truth PLT % change, and “Model” indicates the PLT % predicted by rt-h2.
p2
p3
p4
p5
Model −7.8 −11.0 −53.2 −6.3 −11.8 −1.3
RTT ΔP LT p1
20 ms Real −10.5 −12.0 −50.9 −2.6 −18.0
p6
0.1 −15.4 −8.2
−2.3 −9.5
−9.3 −24.0 −97.7 −6.3 −23.5 −4.4 −11.4 −10.1
−2.4 −21.6
100 ms Real −15.2 −31.1 −104.4 −6.9 −32.7 −5.8 −14.6 −16.1
Model −9.8 −33.3 −92.6 −15.0 −19.2 −14.2 −6.1 −24.9
Model −8.7 −7.2 −84.0 −9.9 −18.1 −4.6
50 ms Real
p7
p8
5 Results
5.1 Methodology
Dataset. The RT data contains page views sampled at 1 % from Akamai cus-
tomers who have opted in this measurement. Each sample produces a waterfall.
We run rt-h2 on two sets of waterfalls. The ﬁrst is an aggregate dataset of
278,178 waterfalls spanning 56,851 unique URLs and 2,759 unique hostnames,
corresponding to about 24 h worth of data. We then extracted a per-website
dataset of 126,919 waterfalls drawn from the aggregate dataset. These waterfalls
correspond to page views of 55 distinct websites. Each website has an average
of 2,350 waterfall samples, with a minimum of 180 and a maximum of over
26,000. Intuitively, each website’s collection of waterfalls represents a sample
of the clients of that website, that use various browsers and devices, from geo-
graphically diverse locations. These 55 websites are the most popular of Akamai’s
customers that have opted in to the measurement and contain, on average, 111
objects per page, with the minimum and maximum being 5 and 500 respectively.
Metrics. Our primary metric is ΔP LT . For the aggregate dataset, we are inter-
ested in the ΔP LT distribution across all waterfalls. For the per-website dataset,
we explore ﬁrst-order statistics (min, max, mean, median and the top and bottom
deciles). We focus particularly on the 90th percentile of the ΔP LT distribution,
since tail performance is increasingly important for content providers.
Experimental Settings. We ﬁrst understand the performance of basic
HTTP/2, and then explore the impact of two optimizations: prioritization and
push.
The prioritization scenario was motivated by our observation that default
HTTP/2 multiplexing can result in critical objects being downloaded later than
242
K. Zariﬁs et al.
they could, which can happen when many equal priority ﬁles are sent simultane-
ously. This what-if scenario asks: What would the ΔP LT distribution look like if
we knew how to prioritize objects that are on the critical path? This is somewhat
hypothetical, since the browser or server would need to know the optimal order.
We are exploring ways to make this possible, but this scenario gives us an upper
bound on the performance improvement.
The push what-if scenario is based on our observation of the considerable
idle network time until the base HTML ﬁle is available at the browser. This
scenario asks: What would the ΔP LT distribution look like if the server pushed
content speculatively? Ideal push pushes all non-cached objects, and assumes an
omniscient server which can predict what resources the client will need.
Network Conditions. Much prior attention has focused on the impact of net-
work conditions on HTTP/2 [7,11,14]. Our evaluation of the impact of loss
on HTTP/2 provided similar ﬁndings to previous work [14], so we omit it for
brevity. Our primary evaluations, presented here, are under no loss settings, in
which rt-h2’s TCP module does not simulate loss. By removing loss as a fac-