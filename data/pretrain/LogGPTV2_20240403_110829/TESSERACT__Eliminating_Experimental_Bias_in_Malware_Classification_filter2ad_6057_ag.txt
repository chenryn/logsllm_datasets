### References

1. **Induction.** *Journal of Artificial Intelligence Research*, 2003.
2. **Zhenlong Yuan, Yongqiang Lu, Zhaoguo Wang, and Yibo Xue.** *Droid-sec: Deep Learning in Android Malware Detection.* In *SIGCOMM Computer Communication Review*. ACM, 2014.
3. **Mu Zhang, Yue Duan, Heng Yin, and Zhiruo Zhao.** *Semantics-Aware Android Malware Classification Using Weighted Contextual API Dependency Graphs.* In *CCS*. ACM, 2014.
4. **Boyou Zhou, Anmol Gupta, Rasoul Jahanshahi, Manuel Egele, and Ajay Joshi.** *Hardware Performance Counters Can Detect Malware: Myth or Fact?* In *ASIACCS*. ACM, 2018.

### Appendix

#### A.1 Algorithm Hyperparameters

We report the details of the hyperparameters used to replicate ALG1, ALG2, and DL.

- **ALG1 (Linear SVM with C=1):** We replicated the settings and experiments as described in [4]. The performance on our dataset was slightly lower (around 0.91 10-fold F1). To verify, we also reproduced the experiment using their original dataset, achieving the published performance of about 0.94 10-fold F1. We used SCIKIT-LEARN's `sklearn.svm.LinearSVC` for this implementation.

- **ALG2 (Random Forest with 101 trees and max depth of 64):** We followed the settings and experiments described in [33], successfully reproducing the published results. We used SCIKIT-LEARN's `sklearn.ensemble.RandomForestClassifier`.

- **DL (Neural Network):** We re-implemented DL using KERAS, following the guidelines in [22]. The initial input features were the same as those used in ALG1. We replicated the best-performing neural network architecture by training with 10 epochs and a batch size of 1,000. For training optimization, we used the stochastic gradient descent class `keras.optimizers.SGD` with the following parameters: `lr=0.1`, `momentum=0.0`, `decay=0.0`, and `nesterov=False`. Some low-level details of the hyperparameter optimization were missing from the original paper [22]. We achieved slightly higher F1 performance in a 10-fold setting (§4.4), likely because they optimized on the Accuracy metric, which can be misleading in imbalanced datasets where one class is prevalent (e.g., goodware in Android).

#### A.2 Symbol Table

Table 4 provides a legend of the main symbols used throughout the paper to improve readability.

| Symbol | Description |
|--------|-------------|
| gw     | Short version of goodware. |
| mw     | Short version of malware. |
| ML     | Short version of Machine Learning. |
| D      | Labeled dataset with malware (mw) and goodware (gw). |
| Tr     | Training dataset. |
| W      | Size of the time window of the training set (e.g., 1 year). |
| Ts     | Testing dataset. |
| S      | Size of the time window of the testing set (e.g., 2 years). |
| ∆      | Size of the test time-slots for time-aware evaluations (e.g., months). |
| AUT( f ,N) | Area Under Time, a new metric we define to measure performance over time decay and compare different solutions (§4.2). It is always computed with respect to a performance function f (e.g., F1-Score) and N is the number of time units considered (e.g., 24 months). |
| ˆσ    | Estimated percentage of malware (mw) in the wild. |
| ϕ     | Percentage of malware (mw) in the training set. |
| δ     | Percentage of malware (mw) in the testing set. |
| P     | Performance target of the tuning algorithm in §4.3; it can be F1-Score, Precision (Pr), or Recall (Rec). |
| ϕ∗P   | Percentage of malware (mw) in the training set, to improve performance P on the malware (mw) class (§4.3). |
| E     | Error rate (§4.3). |
| Emax  | Maximum error rate when searching ϕ∗P (§4.3). |
| Θ     | Model learned after training a classifier. |
| L     | Labeling cost. |
| Q     | Quarantine cost. |
| P     | Performance; depending on the context, it will refer to AUT with F1, Pr, or Rec. |

**Table 4: Symbol table.**

#### A.3 Cumulative Plots for Time Decay

Figure 10 shows the cumulative performance plot defined in §4.2. This is the cumulative version of Figure 5.

#### A.4 Delay Strategies

We provide more background details on the mitigation strategies adopted in Section 5.

- **Incremental Retraining:** Incremental retraining aims to achieve an "ideal" performance \( P^* \) by periodically labeling all test objects manually and introducing the new knowledge to the classifier via retraining. Formally, the performance of month \( m_i \) is determined from the predictions of a model \( \Theta \) trained on: \( Tr \cup \{m_0, m_1, \ldots, m_{i-1}\} \), where \( \{m_0, m_1, \ldots, m_{i-1}\} \) are testing objects that are manually labeled. The dashed gray line represents the F1-Score without incremental retraining (i.e., stationary training). Although incremental retraining generally achieves optimal performance throughout the test period, it incurs the highest labeling cost \( L \).

- **Active Learning:** Active learning studies query strategies to select a small number of testing points close to the decision boundaries, which, if included in the training set, are the most relevant for updating the classifier. For example, in a linear SVM, the slope of the decision boundary greatly depends on the support vectors [8]; points further from the SVM decision boundary are classified with higher confidence, having limited effect on the slope of the hyperplane.

  - **Uncertainty Sampling:** One of the most popular active learning strategies is uncertainty sampling [36, 46]. This strategy selects the most uncertain points for retraining. In a binary classification task, the maximum uncertainty is achieved when the prediction probability is 0.5 for both classes (i.e., equal probability of being goodware or malware). The test objects are sorted by descending order of uncertainty \( x^*_{LC} \), and the top-n most uncertain are selected for retraining. The labeling cost \( L \) is known a priori since it is user-specified.

- **Classification with Rejection:** Malware evolves rapidly over time, and if the classifier is not up-to-date, the decision region may no longer be representative of new objects. A reject option can be included as a possible classifier outcome [17, 26], discarding the most uncertain predictions to a quarantine area for manual inspection. This approach increases the overall performance of the classifier on the remaining objects at the cost of rejecting some objects. The quarantine cost \( Q \) is associated with this approach, and unlike active learning, the cost is not known a priori because a threshold on the classifier confidence is applied [17, 26].

#### A.5 TESSERACT Implementation

We have implemented our constraints, metrics, and algorithms as a Python library named TESSERACT, designed to integrate easily with common workflows. The API design of TESSERACT is heavily inspired by and fully compatible with SCIKIT-LEARN [39] and KERAS [10]. Here, we present an overview of the library’s core modules:

- **temporal.py:** Extends the concept of working with predictors \( X \) and output variables \( y \) to include an array of datetime objects \( t \). This allows for operations such as time-aware partitioning of datasets (e.g., `time_aware_partition()` and `time_aware_train_test_split()`) while respecting temporal constraints \( C1 \) and \( C2 \).

- **spatial.py:** Allows the user to alter the proportion of the positive class in a given dataset. `downsample_set()` can be used to simulate the natural class distribution \( \hat{\sigma} \) expected during deployment or to tune the performance of the model by over-representing a class during training. We provide an implementation of Algorithm 1 for finding the optimal training proportion \( \phi^* \) (`search_optimal_train_ratio()`). This module can also assert that constraint \( C3 \) (§4.1) has not been violated.

- **metrics.py:** Includes functions for visualizing classifier assessments and deriving metrics such as accuracy or total errors from slices of a time-aware evaluation. Importantly, we include `aut()` for computing the AUT for a given metric (F1, Precision, AUC, etc.) over a given time period.

- **evaluation.py:** Provides `predict()` and `fit_predict_update()` functions that accept a classifier, dataset, and set of parameters (as defined in §4.1) and return the results of a time-aware evaluation performed across the chosen periods.

- **selection.py and rejection.py:** These modules provide hooks for novel query and reject strategies to be easily plugged into the evaluation cycle. We implement many of the methods discussed in §5 and include them with our release. This modular approach aims to lower the barrier for follow-up research in these areas.

#### A.6 Summary of Datasets Evaluated by Prior Work

Table 5 reports the composition of the dataset used in our paper (1st row) and the datasets used for experimentally biased evaluations in prior work (ALG1 [4], ALG2 [33], and DL [22]).

| Work | TESSERACT (this work) | [4], [22] | [33] |
|------|-----------------------|-----------|------|
| Apps | Benign: 116,993, Malicious: 12,735 | Benign: 123,453, Malicious: 5,560 | Benign: 8,447, Malicious: 35,493 |
| Date Range | Jan 2014 - Dec 2016 | Aug 2010 - Oct 2012, Apr 2013 - Nov 2013, Mar 2016 | Oct 2010 - Aug 2012, Jan 2013 - Jun 2013, Jun 2013 - Mar 2014, Jan 2015 - Jun 2015, Jan 2016 - May 2016 |
| Violations | - | C1, C2, C3 | C1, C2, C3 |

**Table 5: Summary of datasets composition used by our paper (1st row) and by prior work [4, 22, 33] (2nd and 3rd row).**

---

This revised text is more structured, clear, and professional, making it easier to read and understand.