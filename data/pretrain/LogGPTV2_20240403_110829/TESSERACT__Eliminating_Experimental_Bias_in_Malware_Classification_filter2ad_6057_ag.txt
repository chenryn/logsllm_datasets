Induction. Journal of Artiﬁcial Intelligence Research, 2003.
[56] Zhenlong Yuan, Yongqiang Lu, Zhaoguo Wang, and Yibo Xue.
Droid-sec: Deep learning in android malware detection. In
SIGCOMM Computer Communication Review. ACM, 2014.
[57] Mu Zhang, Yue Duan, Heng Yin, and Zhiruo Zhao. Semantics-
Aware Android Malware Classiﬁcation Using Weighted Con-
textual Api Dependency Graphs. In CCS. ACM, 2014.
[58] Boyou Zhou, Anmol Gupta, Rasoul Jahanshahi, Manuel Egele,
and Ajay Joshi. Hardware Performance Counters Can Detect
Malware: Myth or Fact? In ASIACCS. ACM, 2018.
A Appendix
A.1 Algorithm Hyperparameters
We hereby report the details of the hyperparameters used to
replicate ALG1, ALG2 and DL.
We replicate the settings and experiments of ALG1 [4]
(linear SVM with C=1) and ALG2 [33] (package mode and
RF with 101 trees and max depth of 64) as described in the
respective papers [4, 33], successfully reproducing the pub-
lished results. Since on our dataset the ALG1 performance
is slightly lower (around 0.91 10-fold F1), we also reproduce
the experiment on their same dataset [4], achieving their orig-
inal performance of about 0.94 10-fold F1. We have used
SCIKIT-LEARN, with sklearn.svm.LinearSVC for ALG1 and
sklearn.ensemble.RandomForestClassiﬁer for ALG2.
We then follow the guidelines in [22] to re-implement
DL with KERAS. The features given as initial input to the
neural network are the same as ALG1. We replicated the best-
performing neural network architecture of [22], by training
with 10 epochs and batch size equal to 1,000. To perform
the training optimization, we used the stochastic gradient
descent class keras.optimizers.SGD with the following pa-
rameters: lr=0.1, momentum=0.0, decay=0.0, nesterov=False.
Some low-level details of the hyperparameter optimization
were missing from the original paper [22]; we managed to
obtain slightly higher F1 performance in 10-fold setting (§4.4)
likely because they have performed hyperparameter optimiza-
tion on the Accuracy metric [8]—which is misleading in im-
balanced datasets [5] where one class is prevalent (goodware,
in Android).
744    28th USENIX Security Symposium
USENIX Association
A.2 Symbol table
Table 4 is a legend of the main symbols used throughout this
paper to improve readability.
Symbol
Description
gw
mw
ML
D
Tr
W
Ts
S
∆
AUT( f ,N)
ˆσ
ϕ
δ
P
ϕ∗P
E
Emax
Θ
L
Q
P
Short version of goodware.
Short version of malware.
Short version of Machine Learning.
Labeled dataset with malware (mw) and goodware (gw).
Training dataset.
Size of the time window of the training set (e.g., 1 year).
Testing dataset.
Size of the time window of the testing set (e.g., 2 years).
Size of the test time-slots for time-aware evaluations (e.g.,
months).
Area Under Time, a new metric we deﬁne to measure perfor-
mance over time decay and compare different solutions (§4.2).
It is always computed with respect to a performance function f
(e.g., F1-Score) and N is the number of time units considered
(e.g., 24 months)
Estimated percentage of malware (mw) in the wild.
Percentage of malware (mw) in the training set.
Percentage of malware (mw) in the testing set.
Performance target of the tuning algorithm in §4.3; it can be
F1-Score, Precision (Pr) or Recall (Rec).
Percentage of malware (mw) in the training set, to improve
performance P on the malware (mw) class (§4.3).
Error rate (§4.3).
Maximum error rate when searching ϕ∗P (§4.3).
Model learned after training a classiﬁer.
Labeling cost.
Quarantine cost.
Performance; depending on the context, it will refer to AUT
with F1 or Pr or Rec.
Table 4: Symbol table.
A.3 Cumulative Plots for Time Decay
Figure 10 shows the cumulative performance plot deﬁned in
§4.2. This is the cumulative version of Figure 5.
A.4 Delay Strategies
We discuss more background details on the mitigation strate-
gies adopted in Section 5.
Incremental retraining. Incremental retraining is an ap-
proach that tends towards an “ideal” performance P∗: all test
objects are periodically labeled manually, and the new knowl-
edge introduced to the classiﬁer via retraining. More formally,
the performance of month mi is determined from the predic-
tions of a model Θ trained on: Tr∪{m0,m1, ...,mi−1}, where
{m0,m1, ...,mi−1} are testing objects, which are manually la-
beled. The dashed gray line represents the F1-Score without
incremental retraining (i.e., stationary training). Although in-
cremental retraining generally achieves optimal performance
throughout the whole test period, it also incurs the highest
labeling cost L.
Active learning. Active learning is a ﬁeld of machine learn-
ing that studies query strategies to select a small number of
testing points close to the decision boundaries, that, if in-
cluded in the training set, are the most relevant for updating
the classiﬁer. For example, in a linear SVM the slope of the
decision boundary greatly depends on the points that are clos-
est to it, the support vectors [8]; all the points further from the
SVM decision boundary are classiﬁed with higher conﬁdence,
hence have limited effect on the slope of the hyperplane.
We evaluate the impact of one of the most popular active
learning strategies: uncertainty sampling [36, 46]. This query
strategy selects the most points the classiﬁer is least certain
about, and uses them for retraining; we apply it in a time-
aware scenario, and choose a percentage of objects to retrain
per month. The intuition is that the most uncertain elements
are the ones that may be indicative of concept drift, and new,
correct knowledge about them may better inform the decision
boundaries. The number of objects to label depends on the
user’s available resources for labeling.
More formally, in binary classiﬁcation uncertainty sam-
pling gives a score x∗LC (where LC stands for Least Conﬁdent)
to each sample [46]; this score is deﬁned as follows4:
x∗LC := argmaxx{1− PΘ( ˆy|x)}
(6)
where ˆy := argmaxyPΘ(y|x) is the class label with the highest
posterior probability according to classiﬁer Θ. In a binary
classiﬁcation task, the maximum uncertainty for an object is
achieved when its prediction probability equal to 0.5 for both
classes (i.e., equal probability of being goodware or malware).
The test objects are sorted by descending order of uncertainty
x∗LC, and the top-n most uncertain are selected to be labeled
for retraining the classiﬁer.
Depending on the percentage of manually labeled points,
each scenario corresponds to a different labeling cost L. The
labeling cost L is known a priori since it is user speciﬁed.
Classiﬁcation with rejection. Malware evolves rapidly
over time, so if the classiﬁer is not up to date, the decision
region may no longer be representative of new objects. An-
other approach, orthogonal to active learning, is to include a
reject option as a possible classiﬁer outcome [17, 26]. This
discards the most uncertain predictions to a quarantine area
for manual inspection at a future date. At the cost of rejecting
some objects, the overall performance of the classiﬁer (on the
remaining objects) increases. The intuition is that in this way
only high conﬁdence decisions are taken into account. Again,
although performance P improves, there is a quarantine cost
Q associated with it; in this case, unlike active learning, the
cost is not known a priori because, in traditional classiﬁca-
tion with rejection, a threshold on the classiﬁer conﬁdence is
applied [17, 26].
4In multi-class classiﬁcation, there is a query strategy based on the entropy
of the prediction scores array; in binary classiﬁcation, the entropy-based
query strategy is proven to be equivalent to the “least conﬁdent” [46].
USENIX Association
28th USENIX Security Symposium    745
Figure 10: Performance time decay with cumulative estimate for ALG1, ALG2 and DL. Testing distribution has δ = 10%
malware, and training distribution has ϕ = 10% malware.
A.5 TESSERACT Implementation
We have implemented our constraints, metrics, and algorithms
as a Python library named TESSERACT, designed to integrate
easily with common workﬂows. In particular, the API design
of TESSERACT is heavily inspired by and fully compatible
with SCIKIT-LEARN [39] and KERAS [10]; as a result, many of
the conventions and workﬂows in TESSERACT will be famil-
iar to users of these libraries. Here we present an overview of
the library’s core modules while further details of the design
can be found in [40].
temporal.py While machine learning libraries com-
monly involve working with a set of predictors X and
a set of output variables y, TESSERACT extends this
concept
to include an array of datetime objects t.
This allows for operations such as the time-aware parti-
tioning of datasets (e.g., time_aware_partition() and
time_aware_train_test_split()) while respecting tem-
poral constraints C1 and C2.
spatial.py This module allows the user to alter the
proportion of the positive class in a given dataset.
downsample_set() can be used to simulate the natural class
distribution ˆσ expected during deployment or to tune the
performance of the model by over-representing a class dur-
ing training. To this end we provide an implementation of
Algorithm 1 for ﬁnding the optimal training proportion ϕ∗
(search_optimal_train_ratio()). This module can also
assert that constraint C3 (§4.1) has not been violated.
metrics.py As TESSERACT aims to encourage compara-
ble and reproducible evaluations, we include functions for
visualizing classiﬁer assessments and deriving metrics such
as the accuracy or total errors from slices of a time-aware
evaluation. Importantly we also include aut() for computing
the AUT for a given metric (F1, Precision, AUC, etc.) over a
given time period.
evaluation.py Here we include the predict() and
fit_predict_update() functions that accept a classiﬁer,
dataset and set of parameters (as deﬁned in §4.1) and return
the results of a time-aware evaluation performed across the
chosen periods.
selection.py and rejection.py For extending the evalua-
tion to testing model update strategies, these modules pro-
vide hooks for novel query and reject strategies to be easily
plugged into the evaluation cycle. We already implement
many of the methods discussed in §5 and include them with
our release. We hope this modular approach lowers the bar
for follow-up research in these areas.
A.6 Summary of Datasets Evaluated by Prior Work
As a reference, Table 5 reports the composition of the dataset
used in our paper (1st row) and of the datasets used for
experimentally biased evaluations in prior work ALG1 [4],
ALG2 [33], and DL [22] (2nd and 3rd row). In this paper, we
always evaluate ALG1, ALG2 and DL with the dataset in the
ﬁrst row (more details in §2.3 and Figure 1), because we have
built it to allow experiments without spatio-temporal bias by
enforcing constraints C1, C2 and C3. Details on experimental
settings that caused spatio-temporal bias in prior work are
described in §3 and §4. We never use the datasets of prior
work [4, 22, 33] in our experiments.
Work
TESSERACT
(this work)
[4], [22]
[33]
Apps
Benign
Malicious
Benign
Malicious
Benign
Malicious
Date Range
Jan 2014 - Dec 2016
Aug 2010 - Oct 2012
Apr 2013 - Nov 2013
Mar 2016
Oct 2010 - Aug 2012
Jan 2013 - Jun 2013
Jun 2013 - Mar 2014
Jan 2015 - Jun 2015
Jan 2016 - May 2016
# Objects
116,993
12,735
123,453
5,560
5,879
2,568
5,560
6,228
15,417
5,314
2,974
Total
116,993
12,735
123,453
5,560
8,447
35,493
Violations
-
C1
(C1)
C2
C3
Table 5: Summary of datasets composition used by our paper
(1st row) and by prior work [4, 22, 33] (2nd and 3rd row).
746    28th USENIX Security Symposium
USENIX Association
1471013161922Testingperiod(month)0.00.10.20.30.40.50.60.70.80.91.0Alg1Recall(gw)Precision(gw)F1(gw)Recall(mw)Precision(mw)F1(mw)F1(10-fold,ourdataset)F1(originalpaper)1471013161922Testingperiod(month)0.00.10.20.30.40.50.60.70.80.91.0Alg21471013161922Testingperiod(month)0.00.10.20.30.40.50.60.70.80.91.0DL