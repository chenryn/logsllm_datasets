is periodically corrupted by strong disturbances caused by the operating system timer interrupts.
9
(a) Apple iPad Mini 2
(0 − 400 kHz, 0.2 msec).
(b) Apple iPad 3rd Generation
(0 − 200 kHz, 0.3 msec).
(c) Apple iPhone 4s
(0 − 175 kHz, 0.3 msec).
(d) Apple iPhone 5s
(0 − 200 kHz, 0.2 msec).
(e) Apple iPod Touch 4th Genera-
tion
(0 − 200 kHz, 0.2 msec).
(f) Sony Ericsson Xperia X10
(250 − 500 kHz, 0.8 msec).
Figure 3: EM measurement of four scalar-by-point multiplication operations using the NIST P-
521 curve executed on various mobile devices. In each subﬁgure, the ﬁrst multiplication used a
random 521-digit scalar while the remaining three used the same repetitive 521-digit numbers used
in Figure 2 (in the same order). Similarly to Figure 2, the same curve point was used to perform
the multiplication.
Previous works ([GPT14, GPPT15, GPPT16]) have mitigated the problem of low SNR and signal
distortion by repeating each measurement several times and combining the results into a single clear
aggregate trace. However, this is inapplicable to ECDSA: each signing operation uses a diﬀerent
nonce k, so the corresponding scalar-by-point multiplications [k]G results in diﬀerent DA-sequences
that cannot be directly combined.
Locating Signing Operations.
In order to successfully execute our attack, we need to ﬁnd the
exact points in time where each signing operation ends. Unlike the concurrent work of [BFMRT16a]
which assumes that these exact time points are leaked via the USB port, we assume the attacker has
no leakage of such information and tackle the problem during our signal processing steps. For this
purpose we utilize a distinct trace pattern occurring at the very end of each signing. This pattern
is a natural product of the executed code (it is not an artiﬁcial trigger), but is very similar across
diﬀerent signing operations, for given software and hardware. After performing signal denoising
(described next) we apply correlation-based detection to identify all instances where this distinct
pattern occurs. We thus obtain the end points for most signing operations. We ignored some traces
that ended in distorted patterns that did not correlate well.
Denoising Signal Traces. As mentioned above, an average-based denoising approach is not
applicable to ECDSA since each signing operation uses a diﬀerent nonce k. Instead, to increase the
SNR of our traces, we add a preprocessing step (following the FIR ﬁlter) that performs Singular
Spectrum Analysis (SSA). SSA can be used for blind source separation and denoising of single
10
Figure 4: Our lab-grade setup for capturing EM emanations attacking a Sony-Ericsson Xperia x10
phone. Left to right: analysis laptop, power supply, ZPUL 30P ampliﬁer (gray box), Ettus N200
(white box), and phone being attacked using the Langer LF-R 400 probe (blue).
traces [GZ13].
In the context of side channels, SSA was used in [PS15] to increase the success
probability of various DPA-style attacks targeting embedded devices. The aim of the SSA procedure
i=1 into several distinct components, each with its own
physical properties. The algorithm consists of three stages (see [GZ13] for further details).
Step 1: Embedding. First, a window length 2  0. The SVD of a matrix thus allows us
λi
11
Figure 5: A recorded trace after ﬁltering out high frequency noise (top), and the same trace after
additionally applying SSA (bottom). Note the timer interrupt disturbing the measurement signal.
to represent it as a sum of matrices:
A = USVT =
d(cid:88)
i=1
(cid:112)
λiUiVi
(cid:112)
λiXi.
d(cid:88)
i=1
T =
√
The matrices Xi are called projection matrices, and their contributions to the original matrix A
are proportional to
Step 3: Reconstruction. Each matrix Xi can now be transformed back into a length N time series
n=1 by averaging over the entries in its anti-diagonal. This process is also called diagonal
averaging or Hankelazation [Has07]. Overall we obtain a decomposition of the original time series
λi (which are also called the singular values of A).
(cid:8)xi
(cid:8)an
n
(cid:9)N
(cid:9)N
n=1 into a sum of d series:
within the set(cid:8)xi
(cid:9)d
n
i=1.
(cid:8)an
(cid:9)N
n=1 =
(cid:9)N
(cid:8)xi
n
n=1.
d(cid:88)
i=1
A denoised time series can now be reconstructed by choosing a suitable subset of m ≤ d series from
SSA Parameter Choice. The quality of the decomposition and denoising is highly dependent
on the window size L and the choice of subset m. Empirically, we have found that good results
are obtained when L is chosen to be shorter than both the double and the add operations. At a
sampling rate of 1M samples per second, the length of double and add operations was 50 and 250
samples, respectively. We thus chose L to be 10 samples long. The reconstruction subset m was
also chosen empirically. We found that a good result is achieved when one uses the components
corresponding to the third, fourth and ﬁfth highest singular values for reconstruction (regarding
the rest of the components as noise). It is worth noting that SSA is often used in the literature to
expose hidden periodic trends in noisy signals. In these cases it is recommended [KP11, GZ13] to
12
choose L to be relatively large (larger than the longest suspected hidden period). However, in our
case the DA sequence has no intrinsic periodicity, and larger L values seemed to be less suited for
denoising.
Figure 5 depicts a signal trace after undergoing the SSA procedure. The double and add
sequence can now be clearly seen. Note that the SSA procedure did not get rid of the interrupt
induced disturbances, but since we only require a small number of double operations taken from
the end of the trace, this is usually not an issue. On the rare occasions where an interrupt was
detected at the very end of the trace, the corresponding recording was simply discarded.
Locating Addition Operations In Time. Examining the denoised trace, we can now attempt
to extract the DA-sequence. For this purpose we turn to the time-frequency domain. The middle
of Figure 6 depicts the spectrogram of a denoised trace, where the frequency band containing most
of the energy of addition operations becomes especially clear (see Figure 6 (middle)). Summing
over the spectrogram’s energy we receive a trace marking the locations of addition operations in
time . In order to increase the detection accuracy, we enhance this trace by multiplying it with its
own derivative with respect to time. This way we are able to enhance high amplitude peaks that
also rise sharply, and attenuate other peaks. Further smoothing produces the signal depicted in
Figure 6 (bottom), where the peaks marking the locations of add operations can be detected with
high ﬁdelity.
Extracting the Partial DA-Sequence. Having found a way to detect addition operations with
suﬃcient ﬁdelity, we can now attempt to locate the position of the very last addition operation in
the signal. We do so by ﬁrst ﬁnding the point in each trace where the signing operation ends. This
point can be reliably found in many traces since the signal pattern that immediately follows the
signing operation is consistently similar across many traces. We can use this pattern as a template
to reliably locate it in other traces using correlation, discarding traces that do not correlate well
with the chosen template. By measuring the distance between the estimated template location in
each trace and the very last addition operation detected in the signature (found using the methods
described in the previous subsection), we can determine the number of double operations that
occurred at the very end of each signing operation, thus acquiring the DA information necessary
for key extraction.
Signal Analysis Performance. Applying our attack to a randomly-generated ECDSA secp256k1
OpenSSL key, we measured the EM emanations during 5000 signatures on an Apple iPhone 3GS
smartphone, each signature lasting 0.1 sec. Applying our signal processing to the 5000 traces we
collected, we were able to detect the end time of the signing operation in 1278 traces. Out of these,
114 traces were identiﬁed as having their DA-sequence terminate with at least three elliptic curve
double operations; 3 of these were false positives (as discovered in retrospect; the attack code did
not use this information).
Lattice Reduction and Key Extraction. Using the above 114 traces, we randomly selected
85 traces (this number was set empirically, to obtain high success probability in the next step)
and applied the lattice attack of [BvdPSY14] using the fplll [ABC+] implementation of the BKZ
algorithm with block size β = 30. Unfortunately, whenever the selected traces happen to include
some of 3 erroneous ones, the BKZ algorithm fails to recover the signing key, causing the key-
extraction attempt to fail. Therefore, we repeated the procedure of randomly selecting 85 traces
and applying the lattice attack 30 times. Since each lattice reduction attempt does not depend
on others, we performed these repetitions in a parallel manner on separate cores. Across the 30
parallel attempts, the secret key was successfully recovered in 2. The signal processing and lattice
reductions took two hours on a desktop PC (3.4 GHz, 6 cores, 64 GB RAM), leading to complete
extraction of the ECDSA signing key. Notice that all the 30 repetitions of the lattice reduction
13
(top) A denoised trace with add operations marked. (middle) Zoomed-in view of the
Figure 6:
spectrogram of the trace. The energy of add operations is clearly visible. (bottom) Energy as a
function of time of the visible part of the spectrogram (after enhancement and smoothing). Peaks
approximate the location of add operations.
step were done oﬄine on the same data base of analog traces. Thus, even a single successful lattice
reduction leads to a successful key-recovery attack.