enhancing propagation vectors with potential threats and methods or updates, and
patching errors in scripts (Zeidanloo et al., 2010).
Some traits and data trails exist throughout the botnet life cycle or botnet
communication despite employing evasive techniques. Examples of DNS data trails
include domain names, resource code, DNS responses, DNS queries, and timestamps. Such
DNS data trails’ availability provides security researchers with ways to detect botnets and
their C&C servers (Stevanovic et al., 2012; Luo et al., 2017).
Given the discussion above, our research question is as follows: Can we increase botnet
detection accuracy by combining two machine learning algorithms to analyse DNS data
trails and the signiﬁcant DNS-related features and rules that contribute to botnet
detection?
This study’s goal is to enhance DNS-based botnet detection accuracy. The contributions
of this paper are (i) new features derived from basic DNS features using Shannon entropy
and (ii) a hybrid rule-based model for botnet detection using a union of JRip and
PART machine learning classiﬁers. Finally, the evaluation of the proposed approach uses
different datasets with various evaluation metrics; and the results are compared with other
existing methods.
The rest of this paper is organised as follows. The related literature and studies section
presents the current related work. The Section “Related Literature and Studies” details the
proposed approach framework. This study’s implementation environment is in Section
“Materials & Methods”, and the Section “Results” is devoted to elaborating the result and
discussion. Finally, the conclusion and future research directions in the Section
“Conclusion” concludes this paper.
RELATED LITERATURE AND STUDIES
Currently, there are two main methods to detect DNS-based botnet: Honeypot and
Intrusion Detection Systems (IDS) (Dornseif, Holz & Klein, 2004; Anbar et al., 2016).
Figure 7 presents the taxonomy of the DNS-based botnet detection approaches.
Honeypots
Honeypots are widely used for identifying and analysing the behaviour of botnet attacks.
Honeypots are purposely designed to be vulnerable to botnet attacks to capture and gather
Al-mashhadi et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.640
7/34
as much data as possible on the botnet (Freiling, Holz & Wicherski, 2005). Honeypot
also runs specialised software that attempts to match bots’ signatures and discovers the
location of the botnet’s C&C server.
There are at least three types or levels of honeypots depending on the required level of
bots information, the complexity of the study’s data, and the interaction level permitted to
the attacker: low, medium, and high (Koniaris, Papadimitriou & Nicopolitidis, 2013;
Nawrocki et al., 2016). A low-level honeypot or Low Interaction Honeypot (LIH) stores
unauthorised communication with a limited attacker interaction; therefore, it is safer and
easier to maintain than other honeypot types. A Medium Interaction Honeypot (MIH)
provides more meaningful interaction with the attacker but not as open as a High
Interaction Honeypot (HIH). HIH is a computer with a real OS running vulnerable
services to attract intruders to break into to capture their actions for analysis. Table 2
shows the pros and cons of the three types of honeypot.
Honeydns, proposed by Oberheide, Karir & Mao (2007), is a form of LIH that uses
some simple statistics over the captured queries and collects DNS queries targeting unused
(i.e., darknet) address spaces. This method prevents attackers from avoiding it
(Bethencourt, Franklin & Vernon, 2005). However, a honeypot cannot detect all forms of
bots, such as bots that are not using scanning to propagate (Dornseif, Holz & Klein, 2004).
Figure 7 Taxonomy of Botnet detection based on DNS. Full-size
DOI: 10.7717/peerj-cs.640/ﬁg-7
Table 2 Honeypots type.
Honeypot
type
Pros
Cons
LIH
Easy to manage, low risk for network
Easy to be noticed by the attackers
MIH
meaningful interaction with the attacker and allow the simulation of
a service or operating system where everything is controlled
Need more network conﬁguration to protect the honeypot network.
It may endanger the network if the attacker fully controls it
HIH
The only type of Honeypot that provides bot binary information
and execution code
High risk to the network operator requires more advanced
conﬁguration for the network and operations skills
Al-mashhadi et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.640
8/34
Furthermore, attackers could utilise honeypots to target other systems or machines outside
the honeypots (Liu et al., 2009). Figure 8 shows the standard honeypot conﬁguration.
Anirudh, Arul Thileeban & Nallathambi (2017) built a model using MIH as a sensor
to collect attack logs. When coupled with an Intrusion Detection System (IDS) as a
veriﬁer, these logs increase 55–60% in IDS efﬁciency against DDoS attacks compared to
using IDS alone. However, their research is limited to DDoS attacks only (Anirudh, Arul
Thileeban & Nallathambi, 2017).
Intrusion Detection System (IDS)
Da Luz (2014) and Alomari et al. (2016) categorised IDS into two: anomaly-based and
signature-based (Da Luz, 2014; Alomari et al., 2016). The anomaly-based IDS can be
further classiﬁed into host-based IDS and network-based IDS (Dornseif, Holz & Klein,
2004). The subsequent sections provide more details on the different types of IDS.
Signature-based Botnet detection
A signature-based detection method only detects botnets with matching predeﬁned
signatures in the database. DNS-based blacklist (DNSBL) method proposed by
Ramachandran, Feamster & Dagon (2006) tracked DNS trafﬁc and discovered bots’
identities based on the insight that botmasters could perform a “recognition” search to
determine blacklisted bots. The limitations of the DNSBL-based approach are that it can
only detect scouting botmaster and limited to bots propagated through SPAMs trafﬁc
using a heuristic approach.
Anomaly-based Botnet detection
Anomaly-based detection method relies on different DNS anomalies to identify botnets.
Some of the DNS anomalies used for detection include high network latency, Time to Live
(TTL) domain, patterns of domain requested per second, high trafﬁc volumes, and
irregular device behaviour that may expose bots’ existence. In other words, the term
Figure 8 A typical Honeypot conﬁguration.
Full-size
DOI: 10.7717/peerj-cs.640/ﬁg-8
Al-mashhadi et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.640
9/34
“detection based on anomaly” refers to the act of ﬁnding odd habits that differ from the
expected ones. The anomaly-based approaches have two detection methods: host-based and
network-based (Dornseif, Holz & Klein, 2004; Karim et al., 2014; Da Luz, 2014).
Host-based approaches
Host-based technique scans and protects the computing device locally, or in other terms,
‘host-level. Shin, Xu, and Gu proposed the EFFORT framework that combines several
techniques to observe DNS trafﬁc at the host level (Shin, Xu & Gu, 2012). EFFORT has ﬁve
speciﬁc modules that use a controlled machine learning algorithm to report malicious
domain names regardless of network topology or communication protocol and
performs well with encrypted protocols. However, the EFFORT framework only worked
with botnets that rely on the DNS administration to recognise C&C servers’ addresses.
Host-based IDS is typically an inadaptable approach. Consequently, the observing agents
must be deployed on all devices in the network to be effective against botnet attacks (Da
Luz, 2014).
Network-based approaches
Network-based IDS analyses network trafﬁc, either actively or passively (Dornseif, Holz &
Klein, 2004; Karim et al., 2014; Da Luz, 2014). The active monitoring approach injects test
packets into the network, servers, or applications instead of just monitoring or passively
measuring network trafﬁc activities.
Active Monitoring Approaches
Ma et al. (2015) proposed an active DNS probing approach to extensively determine
unique DNS query properties from DNS cache logs (Ma et al., 2015). This technique could
be used remotely to identify the infected host. However, injecting packets into the network
increased the risk of revealing the existence of the IDS on the network. Furthermore,
active analysis of DNS packets could threaten users’ privacy. Besides, the NXDOMAIN
requests were absent from the DNS cache entry for domain names. The active monitoring
mechanism added additional trafﬁc from test and test packets injected into the network
(Alieyan et al., 2016).
FluXOR (Passerini et al., 2008) is one of the earlier systems to detect and monitor fast-
ﬂux botnet. The detection technique is based on an interpretation of the measurable
characteristics of typical users. It used an active sampling technique to track each suspected
domain to detect the fast-ﬂux domain. Not only can FluXORs recognise fast-ﬂux domains,
but also the number and identity of related proxy servers to prevent their reuse in a
potential fast-ﬂux service network (Monika Wielogorska, 2017). However, FluXOR is
restricted to the fast-ﬂux domains advertised by SPAM trafﬁc (Perdisci, Corona &
Giacinto, 2012).
Passive Monitoring Approaches
Passive monitoring utilises speciﬁc capturing instruments, known as “sensors,” to track
the passing trafﬁc. Subsequently, the trafﬁc on the network under inspection would not
Al-mashhadi et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.640
10/34
increase. Weimer implemented the ﬁrst passive detection method in 2005 (Weimer, 2005;
Zdrnja, Brownlee & Wessels, 2007).
NOTOS (Antonakakis et al., 2010) is a comprehensive domain name reputation system
that analyses DNS and secondary data from honeypots and malware detection services.
Reputation process inputs are the characteristics derived from the list of domain names,
such as the resolved IP address, the domain registration date, identiﬁed malware samples
accessing a given domain name or IP address, and domain name blacklisted IP addresses.
These features allowed NOTOS to change the domain legitimacy model, clarify how
malicious domains are run, and calculate the perfect reputation score for new domains.
NOTOS has high accuracy and low false-positive rate and can identify newly registered
domains before being released on the public blacklist. However, a reputation score
algorithm needs a domain registration history (whois), which is not available for all
domain names, to award an appropriate reputation score. It is also unusable against
frequently shifting C&C domains, such as a hybrid botnet that uses several C&C server
hubs to execute commands (Kheir et al., 2014).
Contrary to NOTOS, Mentor (Kheir et al., 2014) proposed a machine learning approach
on a statistical set of features. The proposed model sought to exclude all valid domains
from the list of blacklisted C&C botnet domains, which helped to minimise both the
false-positive rate and domain misclassiﬁcation during the identiﬁcation process. To do
this, Mentor embedded a crawler to gather data on suspicious domain names, e.g., web
content and domain properties, to create a DNS pruning model. The Mentor method’s
performance is better when measured against public blacklist domains with meagre
false-positive rates.
EXPOSURE is a system proposed by Bilge et al. (2011) that used inactive DNS
information to identify domains vulnerable to malicious behaviour. It held a total of
15 features distributed over four classes: time-based, DNS-based, TTL-based, and
domain-based. It also used these features to improve the training of PART classiﬁers.
Kopis introduced a new trafﬁc characteristic by analysing DNS trafﬁc at top-level
domain hierarchy root levels (Antonakakis et al., 2011). This method reliably looked at the
malware used domains by going through global DNS query resolution patterns. Unlike
other DNS reputation strategies such as NOTOS and EXPOSURE, Kopis allowed DNS
administrators to freely inspect malware domains without accessing other networks’ data.
In addition, Kopis could search malware domains without access to IP reputation info
(Xu et al., 2017).
Pleiades (Antonakakis & Perdisci, 2012) helped classify recently controlled DGA
domains using non-existent domain responses (NXDOMAIN). However, because its
clustering strategy relied on domain names’ structural and lexical features, it was limited to
DGA-based C&C only. Also, one of the outstanding issues of NXDOMAIN-based
detection was dealing with a compromised host with malware that requested several
queries to DGA domains over an extended time. It might be possible to detect the C&C
addresses of a domain ﬂuxing botnet in the local network by comparing the accurate
domain resolution entropy to the missed one (Yadav et al., 2010). Since the randomness in
the domain name alphanumeric characters is measurable by calculating the entropy
Al-mashhadi et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.640
11/34
value, in their implementation, the researchers utilised an ofﬂine IPv4 dataset from the
Asian region. They achieved a low FP rate of just 0.02%. However, their approach was
limited to non-dictionary IPv4 domain names.
There has been extensive discussion on botnet detection approaches that employ
machine learning detection in the literature. For example, BOTCAP (Gadelrab et al., 2018)
utilises J48 and ‘Support Vector Machine’ (SVM) classiﬁers for training the extracted DNS
features. The authors showed that the J48 classiﬁer, a Java version of the C4.5 classiﬁer,
performed better than other classiﬁers. However, a hybrid detection model that combines
the output of the J48 classiﬁer with other classiﬁer models’ output could further improve
the performance.
Li et al. (2019) attempted to ﬁnd the best classiﬁers from several classiﬁers, such as
Decision Tree-J48, ‘Artiﬁcial Neural Network’ (ANN), ‘Support Vector Machine’ (SVM),
Logistic Regression, ‘Naive Bayes’ (NB), ‘Gradient Boosting Tree’ (GBT), and ‘Random
Forest’ (RF) (Li et al., 2019). As a result, the authors showed that J48 was the best
classiﬁcation algorithm to classify the DGA domain (Li et al., 2019). However, their
proposed approach was not using any hybrid rule model.
Haddadi et al. (2014) adopted the C4.5 classiﬁer for botnet classiﬁcation (Haddadi et al.,
2014). However, the selected subset of features did not contribute to any improvement in
the classiﬁcation process. The experimental results achieved an 87% detection rate.
Likewise, deep learning, a subset of machine learning, has received signiﬁcant attention
lately. A deep learning algorithm of recurrent neural networks (RNN), long short-term
memory (LSTM), and the combination of RNN and LSTM have been applied as a botnet
detection method (Shi & Sun, 2020). The RNN and LSTM combination achieved higher
detection results. However, deep learning techniques require massive pre-processing of
data, long process time, and resources with high-speed processors. Besides, to discover new
bots, re-training the whole model with a new dataset is a must. Re-training is a time-
consuming process and not suitable for detecting new botnets.
From the literature above, it is noticeable that there is a lack of signiﬁcant features and
rules that contribute to detecting DNS-based botnet with high accuracy and low false-
positive rate.
The summary of some existing botnet detection approaches based on DNS trafﬁc
analysis are tabulated in Table 3.
MATERIALS & METHODS
This section thoroughly explains the materials and methods used to implement the
proposed approach. The proposed approached consists of three stages, as shown in Fig. 9.
The following subsections provide complete detail of each stage.
Stage 1: data pre-processing
Data pre-processing stage is critical for the proposed approach. It helps to focus on the
required DNS features to provide a more ﬂexible selection analysis. Also, this process
reduces the analysis time and false-positive results as well.
Al-mashhadi et al. (2021), PeerJ Comput. Sci., DOI 10.7717/peerj-cs.640
12/34
Table 3 Summary of DNS-based botnet detection techniques.
Author and Year
Detection Approach
Strengths
Weakness
Oberheide, Karir &
Mao (2007)
Honeypot
Easy to build; help to discover new
botnet within its network
Limited scalability and interaction
with malicious activities
Ma et al. (2015)
Active Network-Based
Could identify the infected host in
remote management networks
Restricted to domain names in
cache entry; cannot detect
NXDOMAIN request; high
probability to be detected by
attackers, and introduce privacy
concern
Passerini et al. (2008)
Active Network-Based
Discover fast-ﬂux domains; also
detect the number and identity of
related proxy servers to prevent
future reuse
Limited to fast-ﬂux domains
advertised through SPAMs trafﬁc
Shin, Xu & Gu (2012)
Host-Based
Provide real-time protection
Limited to host-level; must be
installed in all hosts in networks
Antonakakis et al. (2010)
Passive Network-Based
Has high accuracy and low false-