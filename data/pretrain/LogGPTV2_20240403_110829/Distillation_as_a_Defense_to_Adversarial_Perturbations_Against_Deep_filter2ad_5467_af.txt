&


&
!
!
!














%
%

%










#
#






#




#
#



$
$
#
#
#



$
'
'
#





'









(
(




(



&
&
"
"



&
#
#
"



#

 $%% 
 $%% 
 $%% 






	
	
	






$%% !#%&#
$%% !#%&#
$%% !#%&#






	
	
	



Fig. 9: An exploration of the impact of temperature on the amplitude of adversarial gradients: We illustrate how
adversarial gradients vanish as distillation is performed at higher temperatures. Indeed, for each temperature considered, we
draw the repartition of samples in each of the 10 ranges of mean adversarial gradient amplitudes associated with a distinct
color. This data was collected using all 10, 000 samples from the CIFAR10 test set on the corresponding DNN model.
distillation, as investigated in the analytical study of defensive
distillation conducted previously in Section III.
To summarize, not only distillation improves resilience of
DNNs to adversarial perturbations (from 95.89% to 0.45%
on a Ô¨Årst DNN, and from 87.89% to 5.11% on a second
DNN), it also does so without severely impacting classiÔ¨Åcation
correctness (the accuracy variability between models trained
without distillation and with distillation is smaller than 1.37%
for both DNNs). Thus, defensive distillation matches the
second defense requirement from Section II. When deploying
defensive distillation, defenders will have to empirically Ô¨Ånd a
temperature value T offering a good balance between robust-
ness to adversarial perturbations and classiÔ¨Åcation accuracy.
In our case, for the MNIST model for instance, such a
temperature would be T = 20 according to Figure 7 and 8.
C. Distillation and Sensitivity
The second battery of experiments sought to demonstrate
the impact of distillation on a DNN‚Äôs sensitivity to inputs. Our
hypothesis is that our defense mechanism reduces gradients
exploited by adversaries to craft perturbations. To conÔ¨Årm this
hypothesis, we evaluate the mean amplitude of these gradients
on models trained without and with defensive distillation.
In this experiment, we split the 10, 000 samples from the
CIFAR10 test set into bins according to the mean value of
their adversarial gradient amplitude. We train these at varying
temperatures and plot the resulting bin frequencies in Figure 9.
Note that distillation reduces the average absolute value of
adversarial gradients: for instance the mean adversarial gradi-
ent amplitude without distillation is larger than 0.001 for 4763
samples among the 10,000 samples considered, whereas it is
the case only for 172 samples when distillation is performed
at a temperature of T = 100. Similarly, 8 samples are in the
bin corresponding to a mean adversarial gradient amplitude
‚àí40 for the model trained without distillation,
smaller than 10
whereas there is a vast majority of samples, namely 7908
samples, with a mean adversarial gradient amplitude smaller
‚àí40 for the model trained with defensive distillation at
than 10
a temperature of T = 100. Generally speaking one can observe
that the largest frequencies of samples shifts from higher mean
amplitudes of adversarial gradients to smaller ones.
When the amplitude of adversarial gradients is smaller, it
means the DNN model learned during training is smoother
around points in the distribution considered. This in turns
means that evaluating the sensitivity of directions will be
more complex and crafting adversarial samples will require
adversaries to introduce more perturbation for the same orig-
inal samples. Another observation is that overtraining does
not help because when there is overÔ¨Åtting, the adversarial
gradients progressively increase in amplitude so early stopping
and other similar techniques can help to prevent exploding.
This is further discussed in Section VI. In our case, training for
50 epochs was sufÔ¨Åcient for distilled DNN models to achieve
comparable accuracies to original models, and ensured that
adversarial gradients did not explode. These experiments show
that distillation can have a smoothing impact on classiÔ¨Åcation
models learned during training. Indeed, gradients characteriz-
ing model sensitivity to input variations are reduced by factors
larger than 1030 when defensive distillation is applied.
D. Distillation and Robustness
Lastly, we explore the interplay between smoothness of
classiÔ¨Åers and robustness. Intuitively, robustness is the aver-
age minimal perturbation required to produce an adversarial
sample from the distribution modeled by F .
Robustness - Recall the deÔ¨Ånition of robustness:
œÅadv(F ) = EŒº[Œîadv(X, F )]
(10)
where inputs X are drawn from distribution Œº that DNN
architecture F is trying to model, and Œîadv(X, F ) is deÔ¨Åned
in Equation 4 to be the minimum perturbation required to
misclassify sample X in each of the other classes. We now
594594
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:13:13 UTC from IEEE Xplore.  Restrictions apply. 
 %#$##
 %#$##

 %#$###
 %#$###

to small perturbations is of paramount
adversary to alter many input features. Thus, making DNNs
robust
importance.
Similarly, for the CIFAR10 architecture, the model trained
without distillations displays a robustness of 0.39% whereas
the model trained with defensive distillation at T = 50 has a
robustness of 2.56%, which represents an increase of 556%.
This result suggests that indeed distillation is able to provide
sufÔ¨Åcient additional knowledge to improve the generalization
capabilities of DNNs outside of their training manifold, thus
developing their robustness to perturbations.

#

"
%
$




$
%
!






$


"

!


#
#

$
#
%






	






Distillation and ConÔ¨Ådence - Next we investigate the
impact of distillation temperature on DNN classiÔ¨Åcation con-
Ô¨Ådence. Our hypothesis is that distillation also impacts the
conÔ¨Ådence of class predictions made by distilled model.
To test this hypothesis, we evaluate the conÔ¨Ådence pre-
diction for all 10, 000 samples in the CIFAR10 dataset. We
average the following quantity over all samples X ‚àà X :
(cid:13)
C(X) =
0 if arg maxi Fi(X) (cid:3)= t(X)
arg maxi Fi(X) otherwise
(12)
where t(X) is the correct class of sample X. The resulting
conÔ¨Ådence values are shown in Table III where the lowest
conÔ¨Ådence is 0% and the highest 100%. The monotonically
increasing trend suggests that distillation does indeed increase
predictive conÔ¨Ådence. Note that a similar analysis of MNIST
is inconclusive because all conÔ¨Ådence values are already near
99%, which leaves little opportunity for improvement.
T
C(X)
1
2
71.85% 71.99% 78.05% 80.77% 81.06%
5
10
20
TABLE III: CIFAR10 model prediction conÔ¨Ådence: C(X)
is evaluated on the test set at various temperatures T .
VI. DISCUSSION
The preceding analysis of distillation shows that
it can
increase the resilience of DNNs to adversarial samples. Train-
ing extracts knowledge learned about classes from probability
vectors produced by the DNN. Resulting models have stronger
generalizations capabilities outside of their training set.
A limitation of defensive distillation is that it is only appli-
cable to DNN models that produce an energy-based probability
distribution, for which a temperature can be deÔ¨Åned. Indeed,
this paper‚Äôs implementation of distillation is dependent on
an engergy-based probability distribution for two reasons: the
softmax produces the probability vectors and introduces the
temperature parameter. Thus, using defensive distillation in
machine learning models different from DNNs would require
additional research efforts. However note that many machine
learning models, unlike DNNs, don‚Äôt have the model capacity
to be able to resist adversarial examples. For instance, Good-
fellow et al. [9] showed that shallow models like linear models
are also vulnerable to adversarial examples and are unlikely
to be hardened against them. A defense specialized to DNNs,
guaranteed by the universal approximation property to at least
be able to represent a function that correctly processes ad-
versarial examples, is thus a signiÔ¨Åcant step towards building
machine learning models robust to adversarial samples.



#$$ !"$%"

