any app, their contributions to malware detection can still be
measured by associating their API vectors to the closest API
clusters. Therefore, an app can always be converted to a 1,000-
dimensional binary vector for malware detection in the testing
phase even if it employs any new APIs that never appeared in
the training phase.
B. SDAC Dataset
TABLE IV: The Evaluation of Differential Training with
SDAC
Android Malware Detection Approach
# Samples in the Whole Dataset
# Benignware
# Malware
SDAC
69,933
35,437
34,496
# Samples in Noisy Training Set
# of Noises added
# detected TP
# of Remained Noises in Processed Dataset
# detected FP
F-score with Correctly-laballed Dataset
F-score with Noisily-laballed Dataset
F-score with Processed Dataset
56,650
5,614 (9.91%)
5,246
343
711 (1.26%)
97.71%
89.04%
97.19%
these apps, we further downloaded their scanning results from
VirusTotal in June 2019 and removed 878 (i.e., 1.24%) apps
whose labels changed between July 2018 and June 2019 to
reduce potential wrong labels as much as possible. Finally, the
dataset of SDAC we use consists of 69,933 app samples whose
labels are relatively stable over three years; we thus consider
these stable labels as “ground-truth” as no other dataset of
better quality is available so far. In this dataset, 35,437 are
labeled benign, and 34,496 are labeled malicious.
C. Performance of Differential Training with SDAC
Table IV summarizes SDAC dataset and SDAC perfor-
mances, which are measured on the correctly-labeled dataset,
the nosily-labeled dataset, and the noise-reduced training set
processed by Differential Training. Overall, Differential Train-
ing revise 5,246 labels correctly, revise 343 labels wrongly. The
percentage of noise labels thus reduces from 9.91% to 1.26%
in the nosily-labeled dataset due to the process of Differential
Training.
More details are given in Figure 5 which shows the number
of labels that are revised correctly by Differential Training
(i.e., true positives), and the number of labels that are revised
wrongly (i.e., false positives) in each iteration. It also shows
the accuracy of the revised labels (i.e.,
the percentage of
the revised labels that are correct) in each iteration. When
Differential Training converges, the accuracy of the revised
labels reaches close to 99%.
The F-score of SDAC improves from 89.04% to 97.19%
after noise reduction on the training set, and this improved F-
score is very close to its upper bound 97.71% (see Table IV).
These results show that Differential Training can greatly reduce
the number of wrong labels in the training set, and improve
the performance of Android Malware detection approach due
to the use of noise-reduced training set in training.
The dataset used by SDAC was collected from an open
public Android application collection project
[8], which
consists of 70,811 apps. This dataset was collected in June
2018 after ﬁltering out 10.62% of the apps whose labels
changed by VirusTotal between July 2016 and July 2018. For
1A F-score is the harmonic mean between precision and recall, where
precision measures the percentage of true malware among the detected
malware, while recall measures the percentage of true malware being detected.
D. Runtime Performance of Differential Training with SDAC
The total time cost of Differential Training with SDAC
in this experiment is about 55.34 hours. In detail, Differential
Training performs 58 iterations; each iteration takes about 57.3
mins on average, which includes 50.6 mins spent on training
the WS model, and less than 4 mins spent on training the DS
model. The rest of time in each iteration is used for performing
outlier detection and noise ratio estimation.
7
Fig. 5: Noise Reduction on SDAC Dataset
VI. DIFFERENTIAL TRAINING WITH DREBIN
A. Introduction of Drebin
Drebin [10] is a lightweight Android malware detection
solution published in 2014 based on static analysis. Drebin
extracts the following features from each app: hardware com-
ponents, required missions, App components, ﬁltered intents
from Android manifest ﬁles, critical API calls, actually used
permissions, human-deﬁned suspicious API calls, and network
address strings from disassembled codes. Drebin converts each
app into a feature vector of 545,433 dimensions. It relies on
a linear support vector machine (SVM) classiﬁer for malware
detection, and uses its linear weights for identifying the fea-
tures that make signiﬁcant contributions to malware detection.
Compared to SDAC that was published recently in 2020,
Drebin is more classic which has been cited frequently in
malware detection research since 2014. In addition to evaluate
the effectiveness of Differential Training on SDAC, we also
test it on the classic Drebin with relatively old dataset.
B. Drebin Dataset
Drebin was evaluated on a dataset collected by 2014,
which was composed of 5,560 “malware samples” and 123,453
“benign apps.” The Drebin dataset has been frequently used in
malware research since its publication. We checked the dataset
in June 2019 using VirusTotal to guarantee the ground-truth
of the dataset. In detail, we found no contradictory labels in
the dataset except that some apps were too old to receive any
report. Compared to the SDAC dataset where malware takes up
49.3% of all apps, the Drebin dataset is highly imbalanced as
malware samples account for 4.3% of all apps. This is another
reason that we choose Drebin so that we can test Differential
Training on a highly imbalanced dataset.
TABLE V: The Evaluation of Differential Training with Drebin
Android Malware Detection Approach
# Samples in the Whole Dataset
# Benignware
# Malware
Drebin
129013
123,453
5,560
# Samples in Noisy Training Set
# of Noises added
# detected TP
# of Remained Noises in Processed Dataset
# detected FP
F-score with Correctly-laballed Dataset
F-score with Noisily-laballed Dataset
F-score with Processed Dataset
103,210
10,009 (9.70%)
9,121
605
1805 (1.75%)
93.34%
73.20%
84.40%
Differential Training, and 605 labels are detected and revised
mistakenly. The accuracy of Differential Training for label
revisions converges close to 98%.
Table V shows that Drebin’s performance improves from
73.20% to 84.40% in F-score if it is trained on the processed
dataset, for which Differential Training reduces the percentage
of noise labels from 9.70% to 1.75%. Compared to the original
F-score, the improved F-score is closer to its upper bound
93.34% which is achieved by Drebin trained with the correctly-
labeled training set.
D. Runtime Performance of Differential Training with Drebin
The total time cost of Differential Training with Drebin in
this experiment is about 62.52 hours, which converges in 68
iterations. Each iteration takes 55.2 minutes on average while
the training of the WS model takes 52.0 mins and the training
of the DS model takes 2.8 mins.
VII. DIFFERENTIAL TRAINING WITH DEEPREFINER
A. Introduction of DeepReﬁner
C. Performance of Differential Training with Drebin
Figure 6 shows the performance of Differential Training
in each iteration, where the red bars and green bars are
measures of true positives and false positives, respectively. In
total, 9,121 noisy labels are detected and revised correctly by
DeepReﬁner [49] is a Android malware detection approach
that connects two deep learning models in sequential. The ﬁrst
model is a Multi-Layer Perceptron model which can detect
“most signiﬁcant” malware samples efﬁciently based on XML
ﬁles in app APK packages. The second model is a long short-
term memory model which detects “more advanced” malware
8
Fig. 6: Noise Reduction on Drebin Dataset
TABLE VI: The Evaluation of Differential Training with
DeepReﬁner
Android Malware Detection Approach
DeepReﬁner
# Samples in the Whole Dataset
# Benignware
# Malware
110,440
47,525
62,915
# Samples in Noisy Training Set
# of Noises added
# detected TP
# of Remained Noises in Processed Dataset
# detected FP
F-score with Correctly-laballed Dataset
F-score with Noisily-laballed Dataset
F-score with Processed Dataset
88352
8,835 (10.00%)
7,497
2,230
3,118 (3.53%)
93.59%
91.37%
93.41%
samples from those apps for which the ﬁrst model cannot
provide reliable classiﬁcation results. The second model relies
on checking the semantic structures of Android bytecodes in
malware detection.
According to [49], the ﬁrst model of DeepReﬁner can
be used alone and it achieves 87.3% accuracy in malware
detection on a dataset of 110,440 apps. We choose this model
to evaluate how Differential Training performs if the malware
detection approach is not extremely accurate but very efﬁcient.
B. DeepReﬁner Dataset
The original DeepReﬁner dataset consists of a set of benign
applications that were collected from Google Play as well as
a set of malicious apps that were collected from VirusShare
and MassVet in 2015 to 2016. We then downloaded their
scanning reports from VirusTotal in June 2019 and removed
all the samples with different labels. The remaining dataset
contains of 62,915 malicious applications and 47,525 benign
applications that were collected in 2016.
C. Performance of Differential Training with DeepReﬁner
Figure 7 shows that after being processed by Differential
Training, the percentage of samples with correct labels in
the DeepRefeiner dataset increases from 90% to 96%, while
9
Table VI further shows that 7,497 wrong labels are revised
correctly, while 2,230 correct labels are revised mistakenly.
In total, Differential Training reduces 64.7% of the wrong
labels and increases the F-score of DeepReﬁner from 91.37%
to 93.41%, which is only 0.18% lower than the F-score of
DeepReﬁner trained with the correctly-labeled dataset.
D. Runtime Performance of Differential Training with Deep-
Reﬁner
In the Differential Training with DeepReﬁner, the total time
cost is about 102.12 hours, which includes 77 iterations. The
time cost for each iteration is about 79.6 minutes on average,
including 70.0 minutes for training the WS model and 7.9
minutes for training the DS model.
VIII. THE IMPACT OF NOISE RATIO TO NOISE
REDUCTION
Differential Training is effective in reducing label noises
on three different datasets at noise ratio 10% as shown in the
previous sections. In this section, we further investigate the
impact of noise ratio to noise reduction. For this purpose, we
produce datasets at 5%, 10%, 15%, 20%, 30%, and 45% noise
ratios, and apply Differential Training on such datasets with
different Android malware detection approaches.
Note that 45% noise ratio is close to the upper bound of
noise ratio (i.e., 50%)2, indicating a data quality that is close
to random labelling.
Table VII shows the noise reduction results of Differential
Training on SDAC dataset at various noise ratios. In these
experiments, the percentage of wrong labels being reduced
by Differential Training ranges from 79.73% to 89.04% as
the noise ratio changes from 5% to 30%,
indicating that
the effectiveness of Differential Training is stable in these
experiments. When the noise ratio in dataset is set to 45%,
which is close to the upper bound (i.e., noise ratio for random
labelling), Differential Training reduces 68.09% of wrong
2If the noise ratio is greater than 50%, a ﬂipping of each and every label
would turn the noise ratio below 50%.
Fig. 7: Noise Reduction on DeepReﬁner Dataset
TABLE VII: Noise Reduction on SDAC Dataset at Different Noise Ratios
Noise Ratio
5%
# of Training Set
# of Wrongly-labeled Samples
# of TP Noise Detection Results
# of FP Noise Detection Results
% of Noise Reduced
# of Wrongly-labeled Samples Left
56,550
2,833
2,540
281
79.73%
574
10%
56,550
5,614
5,246
343
87.34%
711
15%
56,550
8,497
7,977
472
88.33%
992
20%
56,550
11,330
10,465
377
89.04%
1,242
30%
56,550
16,995
15,762
655
88.89%
1,888
45%
56,550
25,493
21,858
4,500