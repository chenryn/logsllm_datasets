# Hang with Your Buddies to Resist Intersection Attacks

**Authors:**
- David Isaac Wolinsky
- Ewa Syta
- Bryan Ford

**Contact:**
{david.wolinsky, ewa.syta, bryan.ford}@yale.edu

**Affiliation:**
Yale University

## Abstract
Some anonymity schemes can protect users from pervasive network surveillance, but only if all messages are independent and unlinkable. In practice, users often need pseudonymity—sending messages that are intentionally linkable to each other but not to the sender. However, pseudonymity in dynamic networks exposes users to intersection attacks. We present **Buddies**, a systematic design for resisting intersection attacks in practical anonymity systems. **Buddies** dynamically groups users into buddy sets, controlling message transmission to make buddies within a set behaviorally indistinguishable under traffic analysis. To balance anonymity guarantees and communication responsiveness, **Buddies** allows users to select independent attack mitigation policies for each pseudonym. Using trace-based simulations and a working prototype, we find that **Buddies** can guarantee non-trivial anonymity set sizes in realistic chat/microblogging scenarios, for both short-lived and long-lived pseudonyms.

## Categories and Subject Descriptors
C.2.0 [Computer-Communication Networks]: General—Security and protection

## Keywords
anonymity, pseudonymity, intersection, disclosure

## 1. Introduction
Certain anonymous communication techniques promise security even against powerful adversaries capable of pervasive network traffic analysis, provided all messages are fully independent and the set of participants remains constant [5, 9, 41, 52]. Practical systems, however, must tolerate churn in the set of online users and support ongoing exchanges that make messages linkable over time, as with Mixminion nyms [15] or Tor sessions [18]. By sending linkable messages in the presence of churn, users can quickly lose anonymity to statistical disclosure or intersection attacks [16, 31, 42, 53]. Despite extensive research on this attack vector, no practical anonymity system currently offers active protection against such attacks.

For example, consider Alice, who writes a blog under a pseudonym to expose corruption in her local city government. Alice always connects to the blog server via Tor [18] and never reveals personally identifying information. Carol, a corrupt city official mentioned in Alice's blog, deduces from the content that the blogger is local and contacts Mallory, a network administrator at the local ISP. Mallory cannot directly compromise Tor but reads the posting times from Alice's blog and cross-references these with the ISP's access logs. While thousands of customers may be online at each posting time, every customer except Alice has a chance of being offline during some posting time. This chance exponentially approaches certainty as Alice continues posting. Mallory simply monitors until the intersection of these online user sets narrows to one user, Alice. Similar intersections of hotel guest lists, IP addresses, and email accounts revealed the parties in the Petraeus/Broadwell scandal [47].

To address such risks, we present **Buddies**, the first anonymous communication architecture designed to systematically protect users from long-term intersection attacks. **Buddies** continuously maintains an anonymized database of participating users and their online status, simulating intersection attacks that a network-monitoring adversary might perform. These simulations yield two relevant anonymity metrics: a possibilistic metric measuring "plausible deniability" and a more conservative indistinguishability metric indicating vulnerability to statistical disclosure attacks [16].

Beyond just measuring vulnerability, **Buddies** offers active control over anonymity loss under intersection attack. Users specify a policy for each pseudonym that balances attack protection against communication responsiveness and availability. The policy module monitors and filters the set of users participating in each communication round, sometimes forcing the system to behave as if certain online users were actually offline. Through this mechanism, policies can enforce lower bounds on anonymity metrics, prevent Alice from revealing herself by posting at the wrong time, and reduce the rate of anonymity loss. Policies can also adjust posting rates or periods, enabling **Buddies** to aggregate all users coming online within a posting period into larger anonymity sets. If Alice sets her blog's posting period to once per day, **Buddies** can maintain her anonymity among all users who check in at least once a day, even if many users check in briefly at varying times.

**Buddies'** model is inspired by anonymous blogging or IRC scenarios, where users post messages to a public forum and primarily desire sender anonymity [40]. While we expect **Buddies** to generalize to two-way models and metrics [44], we defer such extensions to future work. Each **Buddies** user owns multiple Nyms, each representing a pseudonymous identity. Users submit messages to be posted to their Nyms, which the Anonymizer scrubs of identifying information and publicly posts. The Anonymizer consults a Policy Oracle for operational decisions, which makes decisions based purely on public information.

We assume the network-monitoring adversary identifies users by network identifiers such as IP addresses. By monitoring these, the adversary can tell which users are online or offline and how much data they transmit or receive, but cannot see the actual content of data communicated between honest users and the Anonymizer. These assumptions model an ISP-grade adversary capable of wholesale network-level monitoring.

### 2.1 Overview of Operation
In **Buddies'** conceptual architecture, communication proceeds through a series of rounds. The Anonymizer drives each round, as follows:

1. **Registration**: At the start of each round, the Anonymizer updates the membership roster to include new members.
2. **Nym creation**: The Anonymizer creates and announces one "fresh" Nym each round, choosing one user uniformly at random as the Nym's owner, keeping this ownership secret.
3. **Scheduling**: The Anonymizer consults the Policy Oracle to choose one Nym for transmission in this round and specifies the number of bits the owner of the Nym may post.
4. **Message submission**: The Anonymizer announces the scheduled Nym and transmission length to the online users. Each user submits exactly the specified number of bits, which may contain real data or a null message.
5. **User filtering**: The Anonymizer consults the Policy Oracle, providing the set of online users. The Policy Oracle returns a filtered user set, further constraining the set of online users whose submissions the Anonymizer will accept.
6. **Message posting**: If the owner of the scheduled Nym is in the filtered user set, the Anonymizer decrypts and posts the message. If not, it posts zero bits, indistinguishable from a null message.

### 2.2 Active Mitigation of Intersection Attacks
The user filtering step (step 5) serves as **Buddies'** primary control point to resist intersection attacks. The Policy Oracle uses publicly available information to simulate a virtual Adversary, continuously performing an "intersection attack" against each Nym. At step 5 of each round, the Policy Oracle forms an attack model for the scheduled Nym, computes relevant anonymity metrics, and determines if action is required to limit or avoid anonymity loss. If no action is required, the Policy Oracle returns the unfiltered user set. If action is required, the Policy Oracle filters the user set, preventing any user not in the filtered set from posting, as if more users were offline than are actually offline.

To illustrate, consider a strawman policy. In step 5 of each round, the Policy Oracle simulates an intersection attack and filters the user set to maintain a minimum anonymity set size. This ensures that the intersection of online user sets does not narrow to a single user, thereby protecting Alice from being identified by Mallory.

This paper's primary contributions are:
1. The first anonymity architecture that systematically addresses intersection attacks.
2. A modular, policy-based framework for both vulnerability monitoring and active mitigation of anonymity loss via intersection attacks.
3. An evaluation of **Buddies'** practicality via a working prototype and trace-based simulations reflecting realistic online communities.

## 2. **Buddies** Architecture
Figure 1 shows a high-level conceptual model of the **Buddies** architecture. **Buddies** assumes there is a set of users, each with a secure communication path to the Anonymizer. For now, we conceptually treat the Anonymizer as a central, trusted "black box," although later we will map this to a decentralized trust model.

### 2.1 Overview of Operation
In **Buddies'** conceptual architecture, communication proceeds through a series of rounds. The Anonymizer drives each round, as follows:

1. **Registration**: At the start of each round, the Anonymizer updates the membership roster to include new members.
2. **Nym creation**: The Anonymizer creates and announces one "fresh" Nym each round, choosing one user uniformly at random as the Nym's owner, keeping this ownership secret.
3. **Scheduling**: The Anonymizer consults the Policy Oracle to choose one Nym for transmission in this round and specifies the number of bits the owner of the Nym may post.
4. **Message submission**: The Anonymizer announces the scheduled Nym and transmission length to the online users. Each user submits exactly the specified number of bits, which may contain real data or a null message.
5. **User filtering**: The Anonymizer consults the Policy Oracle, providing the set of online users. The Policy Oracle returns a filtered user set, further constraining the set of online users whose submissions the Anonymizer will accept.
6. **Message posting**: If the owner of the scheduled Nym is in the filtered user set, the Anonymizer decrypts and posts the message. If not, it posts zero bits, indistinguishable from a null message.

### 2.2 Active Mitigation of Intersection Attacks
The user filtering step (step 5) serves as **Buddies'** primary control point to resist intersection attacks. The Policy Oracle uses publicly available information to simulate a virtual Adversary, continuously performing an "intersection attack" against each Nym. At step 5 of each round, the Policy Oracle forms an attack model for the scheduled Nym, computes relevant anonymity metrics, and determines if action is required to limit or avoid anonymity loss. If no action is required, the Policy Oracle returns the unfiltered user set. If action is required, the Policy Oracle filters the user set, preventing any user not in the filtered set from posting, as if more users were offline than are actually offline.

To illustrate, consider a strawman policy. In step 5 of each round, the Policy Oracle simulates an intersection attack and filters the user set to maintain a minimum anonymity set size. This ensures that the intersection of online user sets does not narrow to a single user, thereby protecting Alice from being identified by Mallory.

## 3. Attack Mitigation Policies
This section explores several useful attack mitigation policies in the **Buddies** model. These policies aim to balance the trade-offs between anonymity guarantees and communication responsiveness. For example, a policy might enforce a minimum anonymity set size, filter out users who are frequently offline, or adjust posting rates to aggregate more users into each anonymity set.

## 4. Challenges and Approaches
This section details the challenges and approaches to incorporating **Buddies** into practical anonymity systems. Key challenges include decentralizing trust, managing pseudonyms while maintaining their independence, and supporting user-selectable policies for each pseudonym. We have built a working prototype of **Buddies** atop Dissent [11, 13, 52], a recent anonymous communication system that combines verifiable shuffle and DC-net techniques. The prototype's design addresses these challenges, ensuring that **Buddies** can be integrated into existing systems while maintaining strong anonymity guarantees.

## 5. Experimental Evaluation
This section experimentally evaluates our working **Buddies** prototype and trace-based simulations reflecting realistic online communities. We analyze IRC trace data to explore the effectiveness of **Buddies'** anonymity metrics, the feasibility of maintaining non-trivial anonymity sets resistant to intersection attacks for extended periods, and the ability of **Buddies** to limit the loss of anonymity while preserving usable levels of communication responsiveness and availability.

## 6. Related Work
This section summarizes related work in the field of anonymity and pseudonymity, highlighting the unique contributions of **Buddies** in addressing intersection attacks.

## 7. Conclusion
This paper presents **Buddies**, the first systematic design for resisting intersection attacks in practical anonymity systems. **Buddies** provides a modular, policy-based framework for both vulnerability monitoring and active mitigation of anonymity loss. Our experimental evaluation demonstrates the practicality of **Buddies** in realistic online communities, making it a significant step forward in the field of anonymous communication.