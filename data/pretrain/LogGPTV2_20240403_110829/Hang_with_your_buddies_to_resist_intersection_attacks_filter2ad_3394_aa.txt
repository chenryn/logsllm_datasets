title:Hang with your buddies to resist intersection attacks
author:David Isaac Wolinsky and
Ewa Syta and
Bryan Ford
Hang With Your Buddies to Resist Intersection Attacks
David Isaac Wolinsky, Ewa Syta, and Bryan Ford
{david.wolinsky,ewa.syta,bryan.ford}@yale.edu
Yale University
ABSTRACT
Some anonymity schemes might in principle protect users
from pervasive network surveillance—but only if all mes-
sages are independent and unlinkable. Users in practice often
need pseudonymity—sending messages intentionally linkable
to each other but not to the sender—but pseudonymity in
dynamic networks exposes users to intersection attacks. We
present Buddies, the ﬁrst systematic design for intersec-
tion attack resistance in practical anonymity systems. Bud-
dies groups users dynamically into buddy sets, controlling
message transmission to make buddies within a set behav-
iorally indistinguishable under traﬃc analysis. To manage
the inevitable tradeoﬀs between anonymity guarantees and
communication responsiveness, Buddies enables users to se-
lect independent attack mitigation policies for each pseu-
donym. Using trace-based simulations and a working pro-
totype, we ﬁnd that Buddies can guarantee non-trivial an-
onymity set sizes in realistic chat/microblogging scenarios,
for both short-lived and long-lived pseudonyms.
Categories and Subject Descriptors
C.2.0 [Computer-Communication Networks]: General—
Security and protection
Keywords
anonymity; pseudonymity; intersection; disclosure
1.
INTRODUCTION
Some anonymous communication techniques promise se-
curity even against powerful adversaries capable of pervasive
network traﬃc analysis—provided all messages are fully in-
dependent of each other and/or the set of participants never
changes [5, 9, 41, 52]. Practical systems, however, must toler-
ate churn in the set of online users, and must support ongo-
ing exchanges that make messages linkable over time, as with
Mixminion nyms [15] or Tor sessions [18]. By sending link-
able messages in the presence of churn, however, users can
quickly lose anonymity to statistical disclosure or intersec-
tion attacks [16, 31, 42, 53]. Though this extensively studied
attack vector could apply in almost any realistic anonymous
communication scenario, no practical anonymity system we
know of oﬀers active protection against such attacks.
As an example intended merely to illustrate one possible
scenario in this broad class of attacks, suppose Alice writes
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage, and that copies bear this notice and the full ci-
tation on the ﬁrst page. Copyrights for third-party components of this work must be
honored. For all other uses, contact the owner/author(s). Copyright is held by the au-
thor/owner(s).
CCS’13, November 4–8, 2013, Berlin, Germany.
ACM 978-1-4503-2477-9/13/11. DOI 10.1145/2508859.2516740.
a blog under a pseudonym to expose corruption in her local
city government. Alice always connects to the blog server
via Tor [18], and never reveals personally identifying infor-
mation on her blog or to the server. Carol, a corrupt city
oﬃcial mentioned in Alice’s blog, deduces from the blog’s
content that its owner is local, and calls her friend Mallory,
a network administrator in the monopolistic local ISP. Mal-
lory cannot directly compromise Tor, but she reads from Al-
ice’s blog the date and time each blog entry was posted, and
she learns from the ISP’s access logs which customers were
online and actively communicating at each of those times.
While thousands of customers may be online at each post-
ing time, every customer except Alice has a chance of being
oﬄine during some posting time, and this chance exponen-
tially approaches certainty as Alice continues posting. Mal-
lory simply keeps monitoring until the intersection of these
online user sets narrows to one user, Alice. We don’t know
if this precise attack has occurred, but analogous intersec-
tions of hotel guest lists, IP Addresses, and e-mail accounts
revealed the parties in the Petraeus/Broadwell scandal [47].
As a step toward addressing such risks we present Bud-
dies, the ﬁrst anonymous communication architecture de-
signed to protect users systematically from long-term inter-
section attacks. Buddies works by continuously maintaining
an anonymized database of participating users and their on-
line status, and uses this information to simulate intersec-
tion attacks that a network-monitoring adversary might per-
form. These simulations yield two relevant anonymity met-
rics that Buddies reports continuously, as an indication of
potential vulnerability to intersection attack: a possibilistic
metric roughly measuring “plausible deniability,” and a more
conservative indistinguishability metric indicating vulnera-
bility to more powerful statistical disclosure attacks [16].
Beyond just measuring vulnerability, as in prior work on
metrics [17, 43] and alternate forms of anonymity [30], Bud-
dies oﬀers active control over anonymity loss under intersec-
tion attack. Users specify a policy for each pseudonym that
balances attack protection against communication respon-
siveness and availability. To enforce these policies, a policy
module monitors and ﬁlters the set of users participating
in each communication round, sometimes forcing the sys-
tem to behave as if certain online users were actually oﬄine.
Through this active control mechanism, policies can enforce
lower bounds on anonymity metrics, preventing Alice from
revealing herself to Mallory by posting at the wrong time for
example. Policies can also reduce the rate of anonymity loss
to intersection attacks, for example by tolerating anonym-
ity set members who are normally reliable and continuously
online but who lose connectivity for brief periods. Finally,
policies can adjust posting rates or periods, enabling Bud-
dies to aggregate all users coming online within a posting
period into larger anonymity sets. If Alice sets her blog’s
posting period to once per day, for example, then Buddies
1153conceptually centralized component to realistic anonymiza-
tion systems that decentralized trust, to avoid trusting any
single physical component or administrative domain.
Buddies’ model is inspired by anonymous blogging or IRC
scenarios, where users post messages to a public forum, and
users primarily desire sender anonymity [40]. While we ex-
pect Buddies to generalize to two-way models and met-
rics [44], we defer such extensions to future work. Each Bud-
dies user “owns” some number of Nyms, each representing a
pseudonymous identity under which the owner may post:
e.g., an anonymous chat handle or blog. Users may secretly
submit messages to be posted to Nyms they own, which the
Anonymizer scrubs of identifying information and publicly
“posts” to that Nym. To make various operational decisions,
the Anonymizer consults a Policy Oracle. By design the Pol-
icy Oracle has no access to sensitive information, such as who
owns each Nym: the Policy Oracle makes decisions based
purely on public information available to anyone.
We assume the network-monitoring adversary identiﬁes
users by some network identiﬁer or locator, such as IP ad-
dress. By monitoring these locators the adversary can tell
which users are online or oﬄine at any given moment, and
how much data they transmit or receive, but cannot see
the actual content of data communicated between honest
users and the Anonymizer. These assumptions model an
ISP-grade adversary that can implement “wholesale” network-
level monitoring of users connected via that ISP.
2.1 Overview of Operation
In Buddies’ conceptual architecture, communication pro-
ceeds synchronously through a series of rounds. The Ano-
nymizer drives the operation of each round i, as follows:
1. Registration: At the start of round i the Anonymizer
updates the membership roster, Mi, to include members who
may have recently joined.
2. Nym creation: The Anonymizer next creates and an-
nounces one “fresh” Nym Ni each round. For each new Nym,
the Anonymizer chooses one User uniformly at random as
the Nym’s owner, keeping this ownership secret. A Nym’s
lifetime is in principle unlimited: over time users acquire
fresh Nyms at random but “statistically fair” times. (We
later address creation of larger “batches” of Nyms eﬃciently,
so new users need not wait a long time before they can post.)
3. Scheduling: The Anonymizer consults the Policy Or-
acle to choose one Nym, Ti, for transmission in this round,
from all Nyms in existence. The Policy Oracle also speciﬁes
the number of bits Bi that the owner of Nym Ti may post.
(Scheduling multiple Nyms per round is a straightforward
extension.) As the Policy Oracle can access only public infor-
mation, scheduling cannot depend directly on which Users
currently “have messages to post.” Scheduling can depend
on other factors, however, such as Nyms’ lifetimes, recent
usage, or the interest of other users as indicated in messages
previously posted anonymously via other Nyms.
4. Message submission: The Anonymizer announces
the scheduled Nym Ti and transmission length Bi to the
Users currently online. Each online user submits exactly Bi
secret bits to the Anonymizer. These secret bits may con-
tain either “real” data, or a null message of Bi zero bits, if
the user has nothing useful to transmit at the moment. The
bits sent from any user j other than the owner of Nym Ti
represent “cover traﬃc” necessary to hide the Nym-owner’s
message submission from traﬃc analysis. The Anonymizer
Figure 1: Conceptual model of Buddies architecture
can maintain Alice’s anonymity among all users who “check
in” at least once a day—any time during each day—even if
many users check in only brieﬂy at widely varying times.
Buddies’ architecture may be treated as an extension to
various existing anonymous communication schemes, but is
most well-suited to schemes already oﬀering measurable pro-
tection guarantees against traﬃc analyis, such as MIX cas-
cades [5, 41], DC-nets [9, 46, 52], or veriﬁable shuﬄes [7, 25,
39]. We have built a working prototype of Buddies atop Dis-
sent [11, 13, 52], a recent anonymous communication system
that combines veriﬁable shuﬄe and DC-net techniques. The
prototype’s design addresses several practical challenges: to
decentralize trust among independent servers, to create and
manage pseudonyms while maintaining their independence,
and to support user-selectable policies for each pseudonym.
To evaluate Buddies’ practicality in realistic online com-
munities, we analyze IRC trace data under a Buddies sim-
ulator, exploring questions such as how eﬀective Buddies’
anonymity metrics are, how feasible it may be to maintain
nontrivial anonymity sets resistant to intersection attacks
for extended periods, and how eﬀectively Buddies can limit
loss of anonymity while preserving usable levels of commu-
nication responsiveness and availability.
This paper’s primary contributions are: (a) the ﬁrst ano-
nymity architecture that systematically addresses intersec-
tion attacks; (b) a modular, policy-based framework for both
vulnerability monitoring and active mitigation of anonymity
loss via intersection attacks; and (c) an evaluation of Bud-
dies’ practicality via a working prototype and trace-based
simulations reﬂecting realistic online communities.
Section 2 of this paper outlines Buddies’ high-level model
of operation and the anonymity metrics we use. Section 3
then explores several useful attack mitigation policies in this
model. Section 4 details challenges and approaches to in-
corporating Buddies into practical anonymity systems, and
Section 5 experimentally evaluates both our working Bud-
dies prototype and trace-based simulations. Section 6 sum-
marizes related work, and Section 7 concludes.
2. BUDDIES ARCHITECTURE
Figure 1 shows a high-level conceptual model of the Bud-
dies architecture. Buddies assumes there is some set of users,
each of whom has a secret (i.e., securely encrypted) network
communication path to a component we call the Anonym-
izer. For now we conceptually treat the Anonymizer as a
central, trusted “black box,” although later we will map this
1154forms an online user set, Oi ⊆ Mi, consisting of the users
who submitted a (real, null, or cover) message in round i.
5. User ﬁltering: The Anonymizer now consults with
the Policy Oracle, giving the Policy Oracle the set Oi of
online users—but not any message content or information
about which, if any, of these users owns the Nym scheduled
this round. The Policy Oracle returns a new, ﬁltered user set
Pi ⊂ Oi, further constraining the set of online users whose
submissions the Anonymizer will actually accept this round.
6. Message posting: If the owner of the scheduled Nym
Ti is a member of Pi—i.e., is online and was not ﬁltered
above—then the Anonymizer decrypts that user’s secret mes-
sage and posts it in association with Nym Ti. If the owner of
Ti is not in Pi—either because the owner was not online or
was ﬁltered above—then the Anonymizer posts Bi zero bits
to Nym Ti: an output indistinguishable from a null message.
2.2 Active Mitigation of Intersection Attacks
The user ﬁltering step above (step 5) serves as Buddies’
primary “control point” through which to resist intersection
attacks. The Policy Oracle uses publicly available informa-
tion to simulate a virtual Adversary, by continuously per-
forming an “intersection attack” against each Nym. At step
5 of each round i, the Policy Oracle ﬁrst forms an attack
model for the scheduled Nym Ti, based on prior history and
the set Oi of users online in this round. The Policy Oracle
computes one or more relevant anonymity metrics as de-
tailed further below, and determines if action is required to
limit or avoid anonymity loss in this round. If no action is
required, the Policy Oracle returns the unﬁltered user set
to the Anonymizer, i.e., Pi = Oi. If action is required, how-
ever, then the Policy Oracle can ﬁlter the user set producing
a Pi ⊂ Oi, thus preventing any user not in Pi from posting,
as if more users were oﬄine than are actually oﬄine.
To illustrate how this ﬁltering enables the Policy Oracle to
mitigate intersection attacks, consider the following straw-
man policy. In step 5 of each round, the Policy Oracle sim-