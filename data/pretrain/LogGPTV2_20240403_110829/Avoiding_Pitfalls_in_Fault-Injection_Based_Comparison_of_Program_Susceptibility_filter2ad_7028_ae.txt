(cid:36)(cid:23)(cid:24)(cid:17)(cid:22)(cid:14)(cid:22)(cid:24)(cid:31)(cid:34)(cid:33)(cid:34)(cid:27)
(cid:33)(cid:34)(cid:22)(cid:29)(cid:14)(cid:21)(cid:23)(cid:33)(cid:17)(cid:24)(cid:31)(cid:18)
(cid:10)
(cid:11)
(cid:12)
(cid:13)
(cid:14)
(cid:15)
(cid:16)
(cid:17)
(cid:1)
(cid:2)
(cid:3)
(cid:4)
(cid:5)
(cid:6)
(cid:7)
(cid:8)
(cid:9) (cid:1)(cid:10)
(cid:1)(cid:1)
(cid:1)(cid:2)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:1)(cid:7)(cid:6) (cid:8)(cid:9)(cid:3)(cid:7)(cid:6)(cid:1)(cid:4)(cid:5)(cid:6)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:1)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:8)(cid:1)(cid:7)(cid:8) (cid:9)(cid:10)(cid:3)(cid:7)(cid:8)(cid:1)(cid:4)(cid:5)(cid:8)
(cid:11)(cid:12)(cid:13)(cid:14)(cid:11)(cid:15)(cid:16)(cid:17)(cid:18)(cid:19)
(a) Fault-space diagram for the baseline version,
ﬁnishing after eight CPU cycles.
(b) Fault-space diagram after applying the Dilution Fault Tolerance (DFT)
mechanism. Four no-operation instructions (nop) are prepended to the baseline
version, resulting in an offset of four CPU cycles.
volatile char msg[2];
msg[0] = ’H’;
msg[1] = ’i’;
serial_put_char(msg[0]);
serial_put_char(msg[1]);
/* C program */
0
1
2
3
4
5
6
7
ld r1 <- ’H’
st 0x1 <- r1
ld r1 <- ’i’
st 0x2 <- r1
ld r1 <- 0x1
st $SERIAL <- r1
ld r1 <- 0x2
st $SERIAL <- r1
Fig. 3. Gedankenexperiment with C-like source code for the baseline version to the left, and corresponding machine instructions (and CPU cycle numbers) to
the right.
If a simple benchmarking cheat – the DFT is, of course,
nothing more – can improve the fault coverage from 62.5 %
to 75 %, how about the real fault-tolerance mechanism used
on the BIN_SEM2 and SYNC2 benchmarks in Figure 2b? Does
the SUM+DMR mechanism really improve the hardware fault-
tolerance of these programs, or is this also a (dilution) delusion?
The next section will try and construct an objective metric
that can be used for benchmark comparison, and then revisit
these benchmarks again.
V. CONSTRUCTING AN OBJECTIVE COMPARISON METRIC
Arrived at the suspicion from the previous section that
the fault-coverage factor may be unﬁt for the comparison of
software-based fault-tolerance mechanisms, we construct an
objective comparison metric in this section. Subsequently, we
answer the question whether the SUM+DMR mechanism really
improves both the BIN_SEM2 and SYNC2 benchmarks.
A. Back to the Roots: Failure Probability
In Section I, we stated that the absolute probability for the
benchmark’s failure P (F ailure) represents the ground truth
for comparing different variants of a benchmark. It can be
calculated by decomposing it using the law of total probability:
P (Failure)
= P (Failure|0 Faults ∨ 1 Fault ∨ 2 F. ∨ 3 F. ∨ . . .)
= P (Failure|0 Faults) · P (0 Faults) +
P (Failure|1 Fault) · P (1 Fault) +
P (Failure|2 Faults) · P (2 Faults) +
P (Failure|3 Faults) · P (3 Faults) + . . .
P (Failure|0 Faults) is known to be zero, and from Sec-
tion III-A we know P (k Faults) is negligibly small for k ≥ 2
for real-world soft-error rates and sufﬁciently short benchmark
runs. Hence:
P (Failure) ≈ P (Failure|1 Fault) · P (1 Fault)
(3)
In Equation 3, P (Failure|1 Fault) can be directly calculated
from the FI results collected by a complete fault-space scan
(cf. Section III-A), using the number of failed experiments F ,
326326
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:50:36 UTC from IEEE Xplore.  Restrictions apply. 
and the fault-space size w (cf. Section III-A):
P (Failure|1 Fault) = F
w
(4)
In Equation 3, P (1 Fault) can be calculated using the
Poisson probability Pλ(k = 1) from Equation 1 in Section I.
Inserting Eqn. 1 and 4 in Equation 3 yields:
P (Failure) ≈ F
w
· λ1
1! e
= F · g · e
−λ = F
w
−gw
· g · w · e
−gw
(5)
g is constant for different benchmark runs, and may not
even be exactly known (but is expected to be very small).
−gw is negative and depends on the fault-space size, but with
the order of magnitude of the parameter values also so small
(taking the example numbers from Section III-A gives −gw ≈
−gw ≈ 1 yields an error of 1 −
1.3 · 10−13) that assuming e
−gw < 10−12. Hence, the failure probability – the metric
e
identiﬁed in Section I as the ground truth – can be approximated
to be directly proportional to the absolute number of failed
experiments F :
P (Failure) ∝ F
(6)
Using this proportionality, we also can calculate the com-
parison ratio r, knowing that r < 1 denotes an improvement
of the hardened variant over the baseline:
r = P (Failure)hardened
P (Failure)baseline
= Fhardened
Fbaseline
To conclude, the number of “Failed” FI experiments from
a complete fault-space scan is a valid metric for comparing
benchmarks.
B. Fault Coverage and Failure Probability in the Real World
Figure 2e on page 5 shows the application of this ﬁnding
to the BIN_SEM2 and SYNC2 benchmarks by plotting their
weighted, raw failure counts. Comparing the new results to
the weighted coverage from Figure 2b exhibits that BIN_SEM2
indeed turns out to be protected effectively by the SUM+DMR
protection scheme, the same trend predicted by the misguiding
fault-coverage plot in Figure 2b. More surprisingly, though,
SYNC2 seems to worsen by more than a factor of ﬁve compared
to its baseline – a fact that was completely hidden by the fault-
coverage factor, resulting in a wrong design decision. The fact
that the hardened variant of SYNC2 has an extremely increased
runtime over the baseline, as indicated in Figure 2g, points at
the reason: In this case, a massively increased “No Effect” rate
(just as in our artiﬁcial “Hi” example in Section IV) completely
hid the increase in absolute “Failure” results.
Pitfall 3: Fault-Coverage Percentages for Benchmark Com-
parison
Subsequently, our third and most important pitfall is the
usage of fault-coverage percentages for benchmark com-
parison. Unless the fault space dimensions of two program
variants are identical – which is practically never the case
when effective software-based fault-tolerance mechanisms
are in place –, their fault coverages are measured in percent
relative to different fault-space areas, and are by deﬁnition
not comparable. Instead, absolute failure counts from a
full fault-space scan must be used for comparison.
327327
Unfortunately, there exists an endless amount of studies
that use fault coverage as a comparison metric for program
susceptibility to soft errors in memory. Examples are Fuchs’s
analysis of the MARS operating system [4], Rebaudengo et al.
[5] with a source-to-source compiler for dependable software,
Nicolescu et al. [6] analyzing a hardened space communications
application, Chen et al. [7] measuring the effectiveness of object
duplication in a Java runtime environment, or our own work
[38] analyzing a protection scheme for virtual-function pointers
in C++. Depending on the overhead (affecting the fault-space
size in time or memory dimensions) the analyzed protection
mechanisms introduce, the conclusions in these studies may be
wrong to the point that the mechanisms actually worsen the
system’s fault resilience.
C. No Effect Results, and Sampling
For our metric, only “Failure” results are relevant for
comparison. As we demonstrated and discussed in Section IV,
“No Effect” results can be arbitrarily skewed (either voluntarily,
or by chance) by artiﬁcially modifying the benchmark’s runtime
or memory usage.
Pitfall 3 (Corollary 1): “No Effect” Result Counts
Thus, Corollary 1 of Pitfall 3 is that “No Effect” ex-
periment outcomes are irrelevant for the comparison of
program susceptibility to soft errors in memory, and should
be excluded from the data. Their occurrence is arbitrarily
inﬂuenced by the activation and subsequent masking of
faults, and ultimately by the sophistication of fault-space
pruning mechanisms (cf. Section IV-B).
This also means that when combining def/use pruning with
sampling (Section III-E), it is not necessary to sample from
equivalence classes that are known to result in “No Effect”.
This reduces the population size from w to w
(cid:2) ≤ w.
When sampling is used (cf. Section III-B and III-E), the
number of samples Nsampled, and indirectly also the measured
“Failure” count Fsampled, is arbitrarily chosen by the developer
(but potentially also inﬂuenced by the envisaged conﬁdence
level). Hence, the raw Fsampled cannot be used directly for
comparison. To get sampling results into a form usable for
our metric, the raw sample counts must be extrapolated to the
fault-space size to estimate the number of “Failure” results
from a full fault-space scan.
Pitfall 3 (Corollary 2): Raw Sample Counts
Thus, Corollary 2 of Pitfall 3 is not to use raw sample counts.
If sampling is used, the raw result counts are insufﬁcient
for benchmark comparison. The result counts must be
(cid:2), see above)
extrapolated to the population size w (or w
to be usable for this purpose.
Fextrapolated = w · Fsampled
Nsampled
One example from the literature that provides raw result
numbers, but omits the extrapolation step from sampling to the
full fault space, is from Nicolescu et al. [6].
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:50:36 UTC from IEEE Xplore.  Restrictions apply. 
Summary: Avoiding Pitfalls 1–3
To summarize,
the comparison ratio r to objectively
compare hardware-fault tolerant software systems must
be calculated as follows:
whardened · Fhardened,sampled
r = P (Failure)hardened
wbaseline · Fbaseline,sampled
P (Failure)baseline
Nbaseline,sampled
In the case of a complete fault-space scan, w and N are
equal, and the given formula reduces to r = Fhardened
Fbaseline .
Nhardened,sampled
=
VI. DISCUSSION AND GENERALIZATION
In the following, we brieﬂy revisit Pitfall 1 to get an intuition
how weighting affects the absolute “Failure” counts from
Pitfall 3. Subsequently, we describe possible generalizations
of our ﬁndings to other fault models. Finally, we discuss a
speciﬁc case of cross-layer fault-coverage comparison, and its
implications for the validity of high-level FI.
A. Revisiting Pitfall 1: The Effect of Weighting on Raw Numbers
In Section III-D, we discussed the necessity to weight
def/use equivalence classes by their corresponding data lifetimes
(Pitfall 1), and demonstrated the impact of this decision on
the fault coverages of the BIN_SEM2 and SYNC2 benchmarks.
After being convinced that fault coverage is an inadequate
metric for comparison, how much inﬂuence does weighting
have on the absolute “Failure” counts advocated in Section V-B
(Pitfall 3)?
Figure 2d on page 5 presents the absolute failure counts
without weighting. In this case, both benchmarks seem to be
less resilient to soft errors in their hardened variant when
compared to the baseline. In contrast, the weighted results in
Figure 2e reveal that BIN_SEM2 in fact improves dramatically
– again (cf. Section V-B), a wrong design decision would have
been made.
This example underlines that all pitfalls mentioned in this
paper must be paid attention to, as each of them independently
can signiﬁcantly falsify the results, and lead to incorrect design
decisions.
B. Possible Generalizations
In Section II-D, we simpliﬁed the failure modes to only
“Failure” and “No Effect” types. Our ﬁndings can easily be
generalized to more different experiment outcomes, for example,
Pitfall 3 (Corollary 1) still holds: only “No Effect” results
(denoting no visible effect for the observer) should be excluded,
while the remaining effective result-type counts (e.g., “Silent
Data Corruption”, “Timeout”, . . . ) should be included in the
analysis and separately extrapolated to the fault-space size
(Pitfall 3, Corollary 2).
In Section II-C, we strongly restricted the machine and
fault model to simplify the explanations throughout this paper.
Nevertheless, some of our ﬁndings may be generalizable to
both complex machines and a broader hardware fault model.
A modern superscalar out-of-order CPU with several cache
levels would primarily change the timing of memory-access
events. The def/use equivalence-class sizes would be derived
from more detailed timing information, and therefore would
328328
gain a more accurate weight (Section III-D). However, the order
of instruction execution is irrelevant for the FI methodologies
and the pitfalls we identiﬁed.
When a much more detailed simulator, e.g., on the ﬂip-ﬂop
level, is available, our ﬁndings may be extensible to other
parts of the memory hierarchy. Every bit in the caches, the
CPU registers, or the microarchitectural state of the CPU,
could be part of the fault space – requiring to also record
read and write accesses to these bits for def/use pruning. Then,
faults propagating from the CPU logic should also be taken
into account for weighting. Even without weighting, especially
Pitfall 3 may still be applicable. We will look into these issues
in future work.
C. Cross-Layer Comparisons, and the Invalidity of High-Level
Fault Injection
Recently, two studies analyzed the general validity of high-
level FI, e.g., injection into main memory and CPU registers,
in Cho et al. [39]. Likewise, Wei et al. [40] use the state of
an artiﬁcial virtual-machine model for high-level FI validation.
Both studies compare the results from FI experiments running
the same benchmarks on fault-injected low-level simulators
(e.g., simulating at the ﬂip-ﬂop level [39], or the ISA level
[40]), providing a ground truth to match against. Similarly to
our ﬁndings in Section IV, the authors use the fault-coverage
metric with different fault-space sizes. In this case, the different
fault-space sizes are not caused by varying benchmark runtimes
or memory usages, but by the vastly different simulator models
affecting both the state-space size and timing granularity.
From their analysis, Cho et al. [39] conclude that high-level
FI “can result in high degrees of inaccuracies by more than
an order of magnitude”, quoting an error of up to factor 45.