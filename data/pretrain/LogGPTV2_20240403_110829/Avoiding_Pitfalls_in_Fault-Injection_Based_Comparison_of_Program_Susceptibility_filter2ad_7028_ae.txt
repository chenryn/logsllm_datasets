### Fault-Space Diagrams and C Program Analysis

**(a) Baseline Version:**
- **Description:** The fault-space diagram for the baseline version, which completes after eight CPU cycles.
- **C Program:**
  ```c
  volatile char msg[2];
  msg[0] = 'H';
  msg[1] = 'i';
  serial_put_char(msg[0]);
  serial_put_char(msg[1]);
  ```

- **Machine Instructions:**
  - Cycle 0: `ld r1 <- 'H'`
  - Cycle 1: `st 0x1 <- r1`
  - Cycle 2: `ld r1 <- 'i'`
  - Cycle 3: `st 0x2 <- r1`
  - Cycle 4: `ld r1 <- 0x1`
  - Cycle 5: `st $SERIAL <- r1`
  - Cycle 6: `ld r1 <- 0x2`
  - Cycle 7: `st $SERIAL <- r1`

**(b) Dilution Fault Tolerance (DFT) Mechanism:**
- **Description:** The fault-space diagram after applying the DFT mechanism. Four no-operation instructions (nop) are prepended to the baseline version, resulting in an offset of four CPU cycles.

### Benchmarking and Fault Tolerance

If a simple benchmarking cheat like DFT can improve the fault coverage from 62.5% to 75%, how effective is the real fault-tolerance mechanism used on the BIN_SEM2 and SYNC2 benchmarks in Figure 2b? Does the SUM+DMR mechanism genuinely enhance the hardware fault-tolerance of these programs, or is this also a delusion?

### Constructing an Objective Comparison Metric

#### A. Back to the Roots: Failure Probability

In Section I, we stated that the absolute probability of failure, \( P(\text{Failure}) \), represents the ground truth for comparing different variants of a benchmark. This can be calculated using the law of total probability:

\[
P(\text{Failure}) = P(\text{Failure} | 0 \text{ Faults}) \cdot P(0 \text{ Faults}) + P(\text{Failure} | 1 \text{ Fault}) \cdot P(1 \text{ Fault}) + \ldots
\]

Given that \( P(\text{Failure} | 0 \text{ Faults}) = 0 \) and \( P(k \text{ Faults}) \) is negligibly small for \( k \geq 2 \) in real-world soft-error rates, we have:

\[
P(\text{Failure}) \approx P(\text{Failure} | 1 \text{ Fault}) \cdot P(1 \text{ Fault})
\]

Where:
- \( P(\text{Failure} | 1 \text{ Fault}) = \frac{F}{w} \)
- \( P(1 \text{ Fault}) = \frac{\lambda^1 e^{-\lambda}}{1!} = \lambda e^{-\lambda} \)

Thus:

\[
P(\text{Failure}) \approx \frac{F}{w} \cdot \lambda e^{-\lambda}
\]

For small \( g \) (the fault rate per unit of time or memory), the term \( e^{-gw} \) is very close to 1, leading to:

\[
P(\text{Failure}) \propto F
\]

The comparison ratio \( r \) between the hardened and baseline versions is:

\[
r = \frac{P(\text{Failure})_{\text{hardened}}}{P(\text{Failure})_{\text{baseline}}} = \frac{F_{\text{hardened}}}{F_{\text{baseline}}}
\]

#### B. Fault Coverage and Failure Probability in the Real World

Figure 2e shows the application of this metric to the BIN_SEM2 and SYNC2 benchmarks by plotting their weighted, raw failure counts. The results indicate that BIN_SEM2 is effectively protected by the SUM+DMR scheme, as suggested by the fault-coverage plot in Figure 2b. However, SYNC2 appears to worsen by more than a factor of five, a fact hidden by the fault-coverage factor.

**Pitfall 3: Fault-Coverage Percentages for Benchmark Comparison**

Using fault-coverage percentages for benchmark comparison is problematic unless the fault space dimensions of two program variants are identical. Instead, absolute failure counts from a full fault-space scan must be used for comparison.

#### C. No Effect Results and Sampling

For our metric, only "Failure" results are relevant. "No Effect" results can be arbitrarily skewed by modifying the benchmark's runtime or memory usage.

**Pitfall 3 (Corollary 1): "No Effect" Result Counts**

"No Effect" experiment outcomes are irrelevant for comparing program susceptibility to soft errors and should be excluded from the data.

When combining def/use pruning with sampling, it is not necessary to sample from equivalence classes known to result in "No Effect." This reduces the population size from \( w \) to \( w' \leq w \).

**Pitfall 3 (Corollary 2): Raw Sample Counts**

Raw sample counts are insufficient for benchmark comparison if sampling is used. The result counts must be extrapolated to the population size \( w \) (or \( w' \)):

\[
F_{\text{extrapolated}} = w \cdot \frac{F_{\text{sampled}}}{N_{\text{sampled}}}
\]

### Summary: Avoiding Pitfalls 1â€“3

To objectively compare hardware-fault tolerant software systems, the comparison ratio \( r \) must be calculated as follows:

\[
r = \frac{P(\text{Failure})_{\text{hardened}}}{P(\text{Failure})_{\text{baseline}}} = \frac{w_{\text{hardened}} \cdot F_{\text{hardened,sampled}} / N_{\text{hardened,sampled}}}{w_{\text{baseline}} \cdot F_{\text{baseline,sampled}} / N_{\text{baseline,sampled}}}
\]

For a complete fault-space scan, \( w \) and \( N \) are equal, and the formula reduces to:

\[
r = \frac{F_{\text{hardened}}}{F_{\text{baseline}}}
\]

### Discussion and Generalization

#### A. Revisiting Pitfall 1: The Effect of Weighting on Raw Numbers

Weighting def/use equivalence classes by their corresponding data lifetimes significantly affects the fault coverages of the BIN_SEM2 and SYNC2 benchmarks. Both benchmarks seem less resilient in their hardened variant without weighting, but the weighted results show that BIN_SEM2 improves dramatically.

#### B. Possible Generalizations

Our findings can be generalized to more complex machines and a broader hardware fault model. For a modern superscalar out-of-order CPU, the timing of memory-access events changes, but the order of instruction execution remains irrelevant for the FI methodologies and pitfalls identified.

#### C. Cross-Layer Comparisons and the Invalidity of High-Level Fault Injection

Recent studies have analyzed the validity of high-level fault injection, comparing results from low-level simulators. These studies conclude that high-level FI can result in significant inaccuracies, with errors up to a factor of 45.