with ns2 for long-lived TCP ﬂows.
Figure 11: The minimum required buﬀer that in-
creases the Average Flow Completion Time (AFCT)
by not more than 12.5% vs inﬁnite buﬀers for short
ﬂow traﬃc.
our model over a range of possible settings. We limit our
simulations to cases where ﬂows experience only one con-
gested link. Network operators usually run backbone links
at loads of 10%-30% and as a result packet drops are rare
in the Internet backbone. If a single point of congestion is
rare, then it is unlikely that a ﬂow will encounter two or
more congestion points.
We assume that the router maintains a single FIFO queue,
and drops packets from the tail only when the queue is full
(i.e. the router does not use RED). Drop-tail is the most
widely deployed scheme today. We expect the results to
extend to RED, because our results rely on the desynchro-
nization of TCP ﬂows — something that is more likely with
RED than with drop-tail. We used TCP Reno with a max-
imum advertised window size of at least 10000 bytes, and a
1500 or 1000 byte MTU. The average propagation delay of
a TCP ﬂow varied from 25ms to 300ms.
• Packet loss If we reduce buﬀer size, we can expect
packet loss to increase. The loss rate of a TCP ﬂow is
a function of the ﬂow’s window size and can be approx-
imated to l = 0.76
w2 (see [22]). The sum of the window
sizes is RT T × C + B. If B is made very small, then
the window size halves, increasing loss by a factor of
four. This is not necessarily a problem. TCP uses loss
as a useful signal to indicate congestion; and TCP’s
loss rate is very low (one packet per multiple round-
trip times). More importantly, as we show below, ﬂows
complete sooner with smaller buﬀers than with large
buﬀers. One might argue that other applications that
do not use TCP are adversely aﬀected by loss (e.g.
online gaming or media streaming), however these ap-
plications are typically even more sensitive to queueing
delay.
5.1.1
Simulation Results for Long-lived TCP Flows
Figure 10 simulates an OC3 (155Mb/s) line carrying long-
lived TCP ﬂows. The graph shows the minimum required
buﬀer for a given utilization of the line, and compares it with
the buﬀer size predicted by the model. For example, our
model predicts that for 98% utilization a buﬀer of RT T×C√
should be suﬃcient. When the number of long-lived ﬂows
is small the ﬂows are partially synchronized, and the result
doesn’t hold. However – and as can be seen in the graph
– when the number of ﬂows exceeds 250, the model holds
well. We found that in order to attain 99.9% utilization, we
needed buﬀers twice as big; just as the model predicts.
n
We found similar results to hold over a wide range of
settings whenever there are a large number of ﬂows, there
is little or no synchronization, and the average congestion
window is above two.
If the average congestion window
is smaller than two, ﬂows encounter frequent timeouts and
more buﬀering is required [22].
In our simulations and experiments we looked at three
other commonly used performance metrics, to see their
eﬀect on buﬀer size:
• Goodput While 100% utilization is achievable, good-
put is always below 100% because of retransmissions.
With increased loss, goodput is reduced, but by a very
small amount, as long as we have buﬀers equal or
greater than RT T×C√
.
n
• Fairness Small buﬀers reduce fairness among ﬂows.
First, a smaller buﬀer means all ﬂows have a smaller
round-trip time, and their sending rate is higher. With
large buﬀers, all round-trip times increase and so the
relative diﬀerence of rates will decrease. While over-
buﬀering would increase fairness, it also increases ﬂow
completion times for all ﬂows. A second eﬀect is that
timeouts are more likely with small buﬀers. We did
not investigate how timeouts aﬀect fairness in detail,
however in our ns2 simulations it seemed to be only
minimally aﬀected by buﬀer size.
5.1.2
Short Flows
We will use the commonly used metric for short ﬂows:
the ﬂow completion time, deﬁned as the time from when
the ﬁrst packet is sent until the last packet reaches the des-
tination.
In particular, we will measure the average ﬂow
]
s
t
k
p
[
r
e
f
f
u
b
d
e
r
i
u
q
e
r
m
u
m
n
M
i
i
 2000
 1500
 1000
 500
 0
 0
Minimum buffer for 95% utilization
  1 x RTT*BW/sqrt(n)
0.5 x RTT*BW/sqrt(n)
 50
 100
 150
 200
 250
 300
 350
 400
 450
 500
Number of long-lived flows
 700
 600
 500
 400
 300
 200
 100
w
o
l
f
P
C
T
t
k
p
4
1
a
r
o
f
e
m
i
t
n
o
i
t
l
e
p
m
o
c
e
g
a
r
e
v
A
 0
 0
AFCT of a 14 packet flow (RTT*BW Buffers)
AFCT of a 14 packet flow (RTT*BW/sqrt(n) Buffers)
 50
 100
 150
 200
 250
 300
 350
 400
 450
 500
Number of long-lived flows
Figure 12: Buﬀer requirements for traﬃc mix with
diﬀerent ﬂow lengths, measured from a ns2 simula-
tion.
√
Figure 13: Average ﬂow completion times with a
buﬀer size of (RT T × C)/
n, compared with a buﬀer
size RT T × C.
completion time (AFCT). We are interested in the tradeoﬀ
between buﬀer size and AFCT. In general, for a link with a
load ρ (cid:11) 1, the AFCT is minimized when we have inﬁnite
buﬀers, because there will be no packet drops and therefore
no retransmissions.
We take as a benchmark the AFCT with inﬁnite buﬀers,
then ﬁnd the increase in AFCT as a function of buﬀer size.
For example, Figure 11 shows the minimum required buﬀer
so that the AFCT is increased by no more than 12.5%. Ex-
perimental data is from ns2 experiments for 40, 80 and 200
Mb/s and a load of 0.8. Our model, with P (Q > B) = 0.025,
is plotted in the graph. The bound predicted by the M/G/1
model closely matches the simulation results.
The key result here is that the amount of buﬀering needed
does not depend on the number of ﬂows, the bandwidth or
the round-trip time. It only depends on the load of the link
and the length of the bursts. For the same traﬃc mix of only
short ﬂows, a future generation 1 Tb/s core router needs the
same amount of buﬀering as a local 10 Mb/s router today.
5.1.3 Mixes of Short- and Long-Lived Flows
In practice, routers transport a mix of short and long
ﬂows; the exact distribution of ﬂow lengths varies from net-
work to network, and over time. This makes it impossible
to measure every possible scenario, and so we give a gen-
eral idea of how the ﬂow mix inﬂuences the buﬀer size. The
√
good news is that long ﬂows dominate, and a buﬀer size of
RT T × C/
n will suﬃce when we have a large number of
ﬂows. Better still, we’ll see that the AFCT for the short
ﬂows is lower than if we used the usual rule-of-thumb.
In our experiments the short ﬂows always slow-down the
long ﬂows because of their more aggressive multiplicative
increase, causing the long ﬂows to reduce their window-size.
Figures 12 and 13 show a mix of ﬂows over a 400 Mb/s link.
The total bandwidth of all arriving short ﬂows is about 80
Mb/s or 20% of the total capacity. The number of long ﬂows
was varied from 1 to 500. During the time of the experiment,
these long ﬂows attempted to take up all the bandwidth left
available by short ﬂows. In practice, they never consumed
more than 80% of the bandwidth as the rest would be taken
by the more aggressive short ﬂows.
As we expected, with a small number of ﬂows, the ﬂows
are partially synchronized. With more than 200 long-lived
ﬂows, the synchronization has largely disappeared. The
graph shows that the long ﬂows dominate the ﬂow size.
√
If we want 95% utilization, then we need a buﬀer close to
RT T × C/
n. 7 This means we can ignore the short ﬂows
when sizing the buﬀer. Of course, this doesn’t tell us how the
short-lived ﬂows are faring — they might be shutout by the
long ﬂows, and have increased AFCT. But Figure 13 shows
√
that this is not the case. In this ns2 simulation, the average
ﬂow completion time is much shorter with RT T × C/
n
buﬀers than with RT T × C sized buﬀers. This is because
the queueing delay is lower. So by reducing the buﬀer size,
we can still achieve the 100% utilization and decrease the
completion times for shorter ﬂows.
5.1.4 Pareto Distributed Flow Lengths
Real network traﬃc contains a broad range of ﬂow lengths.
The ﬂow length distribution is known to be heavy tailed
[4] and in our experiments we used Pareto distributions to
model it. As before, we deﬁne short ﬂows to be those still
in slow start.
For Pareto distributed ﬂows on a congested router (i.e.
ρ ≈ 1), the model holds and we can achieve close to 100%
throughput with buﬀers of a small multiple of (RT T ×
√
n.8 For example in an ns2 simulation of a 155 Mb/s
C)/
line, ¯RT T ≈ 100ms) we measured 100-200 simultaneous
ﬂows and achieved a utilization of 99% with a buﬀer of only
165 packets.
It has been pointed out [23] that in a network with low la-
tency, fast access links and no limit on the TCP window size,
there would be very few concurrent ﬂows. In such a network,
a single very heavy ﬂow could hog all the bandwidth for a
short period of time and then terminate. But this is un-
likely in practice, unless an operator allows a single user to
saturate their network. And so long as backbone networks
are orders of magnitude faster than access networks, few
7Here n is the number of active long ﬂows at any given time,
not the total number of ﬂows.
8The number of long ﬂows n for sizing the buﬀer was found
by measuring the number of ﬂows in congestion avoidance
mode at each instant and visually selecting a robust mini-
mum.
users will be able to saturate the backbone anyway. Even
if they could, TCP is not capable of utilizing a link quickly
due to its additive increase behavior above a certain window
size. Traﬃc transported by high-speed routers on commer-
cial networks today [4, 24] has 10’s of 1000’s of concurrent
ﬂows and we believe this is unlikely to change in the future.
An uncongested router (i.e. ρ (cid:11) 1) can be modeled using
the short-ﬂow model presented in section 4 which often leads
to even lower buﬀer requirements. Such small buﬀers may
penalize very long ﬂows as they will be forced into congestion
avoidance early even though bandwidth is still available. If
√
we want to allow a single ﬂow to take up to 1/n of the
bandwidth, we always need buﬀers of (RT T × C)/
n, even
at a low link utilization.
We found that our general result holds for diﬀerent ﬂow
length distributions if at least 10% of the traﬃc is from long
ﬂows. Otherwise, short ﬂow eﬀects sometimes dominate.
Measurements on commercial networks [4] suggest that over
90% of the traﬃc is from long ﬂows. It seems safe to as-
sume that long ﬂows drive buﬀer requirements in backbone
routers.
5.2 Measurements on a Physical Router
While simulation captures most characteristics or router-
TCP interaction, we veriﬁed our model by running experi-
ments on a real backbone router with traﬃc generated by
real TCP sources.
The router was a Cisco GSR 12410 [25] with a 4 x OC3
POS “Engine 0” line card that switches IP packets using POS
(PPP over Sonet) at 155Mb/s. The router has both input
and output queues, although no input queueing took place
in our experiments, as the total throughput of the router
was far below the maximum capacity of the switching fabric.
TCP traﬃc was generated using the Harpoon traﬃc genera-
tor [26] on Linux and BSD machines, and aggregated using
a second Cisco GSR 12410 router with Gigabit Ethernet line
cards. Utilization measurements were done using SNMP on
the receiving end, and compared to Netﬂow records [27].
)
x
>
Q
P
(
Router Buﬀer
n
TCP
Flows RT T×BW√
0.5 x
100
1 x
100
2 x
100
100
3 x
0.5 x
200
1 x
200
2 x
200
200
3 x
0.5 x
300
1 x
300
2 x
300
300
3 x