N ode1 checks the received data by method check(). The data
transferred in network are specially designed for each test case.
Speciﬁcally, we control the size of total data around 10MB.
For JRE socket cases that use different stream methods to
read / write different kinds of data, the input data are also
different. They can be a large int array or an object with a
long text String ﬁeld. For JRE Datagram, and channel type
protocols, i.e., JRE SocketChannel, JRE DatagramChannel and
three Netty protocols, they directly use DatagramPacket or
ByteBuffer to read / write data. We design the same data, i.e.,
a 10MB size byte array. For HTTP type protocols, i.e., JRE
HTTP and Netty HTTP, we design an HTML page containing
large amounts of text contents.
Taint tracking scenario. We set Data1 and Data2 on
N ode1 and N ode2 as source points, and the method check() as
the sink point. The check method is the same for all protocols,
while Data1 and Data2 are speciﬁcally designed. At the sink
point, DisTA should obtain two taints of Data1 and Data2.
B. Real-World Distributed Systems
As shown in Table III, we select 5 popular real-world dis-
tributed systems, i.e., ZooKeeper [12], MapReduce/Yarn [25],
ActiveMQ [26], RocketMQ [27] and HBase [10] as evaluation
subjects. These systems represent different kinds of distributed
systems: ZooKeeper for coordination systems, MapReduce/-
Yarn for computing frameworks, ActiveMQ and RocketMQ
for message middlewares, and HBase for databases. These
TABLE III
REAL-WORLD DISTRIBUTED SYSTEMS
System
ZooKeeper
MapReduce/Yarn
ActiveMQ
RocketMQ
HBase+ZooKeeper
Workload
Leader election
Execute Pi
Message distribution
Message distribution
Query data
TAINT TRACKING SCENARIOS FOR SYSTEMS
TABLE IV
Type
Speciﬁc data
trace (SDT)
System I/O
monitor (SIM)
MapReduce/Yarn
System
ZooKeeper
ActiveMQ
RocketMQ
HBase+ZooKeeper
ZooKeeper
MapReduce/Yarn
ActiveMQ
RocketMQ
HBase+ZooKeeper
# Sources
3
1
1
1
1
18
20
6
9
35
# Sinks
1
1
1
1
1
347
2,235
371
408
1,166
systems use different communication protocols. ZooKeeper
uses JRE standard TCP APIs and Netty library. MapReduce/-
Yarn uses JRE NIO and Yarn RPC. ActiveMQ and RocketMQ
supports many kinds of protocols including standard TCP,
UDP, NIO, as well as HTTP/HTTPS, WebSocket and STOMP
[34] protocols. HBase uses standard JRE NIO and Google’s
protobuf RPC [35].
Workloads. We design one workload for each distributed
system. We select the leader election process for ZooKeeper,
a job to calculate the value of Pi for MapReduce,
long
text message distribution for ActiveMQ and RocketMQ, and
getting data from a table for HBase. The 5 workloads are all
common ones in these systems, as shown in Column Workload
in Table III. Note that HBase’s workload must run within two
systems, i.e., HBase and ZooKeeper. Thus, this workload can
be considered a cross-system taint tracking scenario.
Taint tracking scenarios. As Table IV shows, we design
two types of taint tracking scenarios, i.e., speciﬁc data trace
(SDT) and system input/output monitor (SIM). SDT scenarios
are common in program debugging [4], [5]. It marks the
speciﬁc data such as a vote in the leader election as tainted,
to trace how it propagates in the system. In these scenarios,
the number of taints is usually small and determinate. SIM
scenarios are common in data leakage detection [24]. In these
scenarios,
the DTA tool marks data input functions, e.g.,
reading from a conﬁguration ﬁle, as source points, and data
output functions, e.g., log print statements, as sink points.
Compared with SDT scenarios, the taints number of SIM is
relatively large and indeterminate.
In SDT scenarios, we track important variables of the
workload. For ZooKeeper, we select variable Vote as source
point. During the leader election process, all nodes instantiate
lots of Vote objects, but we only select 3 variables which
are ﬁrst transferred into the network. For MapReduce/Yarn,
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:27:27 UTC from IEEE Xplore.  Restrictions apply. 
554
Node 1
// class FileTxnLog
while(txnFiles.hasNext()) {
readZxid();
zxid = txnFile.readZxid();
clas
}
// class QuorumPeer
epoch = getEpoch(zxid);
// class FastLeaderElection
new ToSend(…,epoch,…);
clas
Node 2
F
// class FastLeaderElection
LOG.info(“Notification: …” + 
“peerEpoch:”+n.peerEpoch);
+n.peerEpoch
o
Not
h
:”++
Taints generated at 
the source point
Taints checked at 
the sink point
Fig. 11. A simpliﬁed taint tracking example in ZooKeeper.
we select ApplicationID of the job generated on the client
as the source point. For ActiveMQ, we select a TextMessage
variable representing the long text message as the source point.
RocketMQ is similar to ActiveMQ. We select a Message
variable. For HBase, we set a TableName variable as the
source. For sink points, we select the method checkLeader
as the sink of ZooKeeper. It is invoked on a follower when
the leader is selected. For other systems, since all cases are the
end-to-end request type, we set variables representing request
results or methods to get the request result as sink points. They
are getApplicationReport method in MapReduce/Yarn, an Ac-
tiveMQ’s Message variable and an RocketMQ’s MessageExt
variable received on the message consumer, and an HBase’s
Result variable containing the data rows.
For SIM scenarios, we uniformly set ﬁle reading methods as
source points for all systems. These ﬁles can be conﬁguration
ﬁles or data ﬁles, which may contain sensitive data. Once the
method is invoked at runtime, we mark the return value as
tainted. We set LOG.info method as sink points for all systems,
and check if any log statement prints a tainted variable.
Cluster setting. For ZooKeeper, we deploy 1 node for
Leader and 2 nodes for Follower. For MapReduce/Yarn, we
deploy 1 node for ResourceManager, 1 node for NodeManager
and 1 node for Task Container. For ActiveMQ and RocketMQ,
we deploy them as three peer nodes. For HBase, we deploy
1 HMaster node and 2 HRegionServer nodes, and each node
is equipped with a ZooKeeper process. Except ZooKeeper, all
other systems have an extra node to run their clients.
C. Experimental Setting
For each node in Section V-A and Section V-B, we use a
virtual machine in VMware Workstation Pro 16.1.0 to run it.
Each VM is equipped with 2 CPU cores and 8GB memory.
All VMs run on a host machine which has Intel Core(TM)
i9-9900 CPU and 64GB of RAM.
D. Soundness and Precision
To evaluate the soundness and precision of the taint tracking,
for each case in Table II and Table III, we check at sink points
if any taint is dropped or appears unexpectedly.
We take the SIM scenario in ZooKeeper in Figure 11 as the
example to illustrate the checking process. When a ZooKeeper
node starts, it reads from existing transaction log ﬁles to get the
largest transaction ID. In the while loop, Node 1 reads three
ﬁles. Thus, three different taints are generated at this source
point. During the workload execution, DisTA ﬁnds that a log
print statement on Node 2 has checked a taint, which is the
taint tagged as zxid2 from Node 1. Then, we inspect code
to ﬁgure out two questions: Why zxid2 can propagate to this
sink point, and why other taints generated at the same source
point do not propagate to this sink point? We ﬁnd that only the
transaction ID in the last ﬁle is assigned to the variable zxid,
and further assigned to epoch and sent to Node 2. Thus, only
the taint generated by the last ﬁle read method invocation can
propagate to Node 2, while others are only generated and never
propagated. For this sink point on Node 2, we can determine
that there are no unexpected taints.
For the micro benchmarks and SDT scenarios in real-world
distributed systems, we check every sink point, since the taint
number is small and the propagation process is clear. For SIM
scenarios, there are too many taints generated, and the taint
propagation is too complex. Thus, we only randomly select
a fraction of sink points to check. We observe that DisTA
can accurately track all taints at these sink points. Therefore,
we draw the conclusion for RQ1: DisTA is precise for inter-
node taint tracking, and sound for common communication
protocols.
E. Usability
We evaluate DisTA’s usability by check if it can track
taints at sink points without much extra instrumentation and
speciﬁcation.
To run DisTA in distributed systems, we ﬁrst need to
instrument JRE, which can be automatically performed by
DisTA, by using the following instruction: java -jar DisTA.jar
JAVA HOME jre-inst. To instrument distributed systems,
we can run java -jar DisTA.jar SYSTEM HOME, and
DisTA can instrument all class ﬁles and jar assembly ﬁles
in the distributed system directory SY ST EM HOM E. We
can also choose not to manually instrument the distributed
system code, since DisTA can automatically instrument unin-
strumented code at run time.
To run the instrumented system code on the instrumented
JRE, we only need to add two JVM ﬂags to the origi-
nal Java execution command: -Xbootclasspath/a:DistTA.jar
to add DisTA runtime library to the classpath, and -
javaagent:DistTA.jar to automatically modify uninstru-
mented libraries. For example, we only modify 3 LOC in
ZooKeeper’s environment conﬁguration script ﬁle zkEnv.sh to
conﬁgure DisTA.
JAVA="$INST_JAVA_HOME/bin/java"
SERVER_JVMFLAGS="-Xbootclasspath/a:DisTA.jar -
javaagent:DisTA.jar=taintSources=
$SOURCE_FILE,taintSinks=$SINK_FILE"
CLIENT_JVMFLAGS="-Xbootclasspath/a:DisTA.jar -
javaagent:DisTA.jar=taintSources=
$SOURCE_FILE,taintSinks=$SINK_FILE"
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:27:27 UTC from IEEE Xplore.  Restrictions apply. 
555
RUNTIME OVERHEAD FOR MICRO BENCHMARK
TABLE V
Original (ms)
Phosphor
Time (ms) Overhead (X)
DisTA
Time (ms) Overhead (X)
3,644
3,266
4,119
7,532
8,410
9,050
8,733
2,714
5,196
5,782
3,155
4,706
7,532
12,762
10,391
25,811
25,000
27,055
24,971
4,072
12,813
14,097
15,535
12,599
2.07
3.91
2.52
3.43
2,97
2.99
2.86
1.50
2.47
2.44
4.93
2.62
8,932
18,976
16,833
30,540
27,644
28,901
26,357
5,810
17,402
23,618
19,600
18,341
2.45
5.81
4.09
4.05
3.29
3.19
3.02
2.14
3.35
4.08
6.21
3.95
Case
JRE Socket-Best
JRE Socket-Worst
JRE Socket-Avg
JRE Datagram
JRE SocketChannel
JRE DatagramChannel
JRE AsyncSocketChannel
JRE HTTP
Netty Socket
Netty DatagramSocket
Netty HTTP
Average
RUNTIME OVERHEAD FOR REAL-WORLD DISTRIBUTED SYSTEMS
TABLE VI
System
ZooKeeper
MapReduce/Yarn
ActiveMQ
RocketMQ
HBase+ZooKeeper
Average
Original
(ms)
3,117
28,824
4,065
3,799
26,845
13,330
Phosphor-SDT
Time (ms) Overhead
DisTA-SDT
Time (ms) Overhead
Phosphor-SIM
Time (ms) Overhead
DisTA-SIM
Time (ms) Overhead
9,707
108,199
19,100
18,553
105,735
52,259
3.11
3.75