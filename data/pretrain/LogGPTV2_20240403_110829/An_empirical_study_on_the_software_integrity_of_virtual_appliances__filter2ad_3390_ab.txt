code or image ﬁle) and their criticality for the software pack-
age (e.g., critical or non-critical). The criticality of a ﬁle may
be determined by looking at how much of the expected soft-
ware behavior can be aﬀected when that ﬁle gets modiﬁed,
replaced or deleted. Based on this guideline, a vendor, for
instance, could treat executables and source ﬁles as critical,
and compressed ﬁles and data ﬁles as non-critical. We use
a similar rule to design the prototype (see Section 4.1.3).
3.1 Conﬁguration Resolver
The Conﬁguration Resolver communicates with the soft-
ware vendors or third-party RIMs aggregators (e.g., NSRL
or Bit9) to maintain an up-to-date database of the RIMs
(the whitelist), also keeping track of the history of the RIMs,
since not every VA will be equipped with the latest ver-
sions of software. It also maintains a list of RIMs that have
expired or been revoked. Existing security services com-
panies (e.g., anti-virus companies) are potential candidates
for providing Conﬁguration Resolver services to the cloud
providers. The VMCVT is also managed by the Resolver
and is explained next.
3.2 VM Conﬁguration Veriﬁcation Tool (VM-
CVT)
The VMCVT veriﬁes the integrity of a VA by computing
hashes for all of its ﬁles and comparing them with trusted
hashes. Speciﬁcally, when a VA is created from a trusted
base image, the tool mounts the ﬁlesystems of both the VA
and base VM, hashes all of the ﬁles in both, and compares
the absolute paths and hashes of the ﬁles to compile a list
of ﬁles that have not changed from the base as well as the
ﬁles that have been added, modiﬁed, or deleted. Files that
are the same will be marked as “veriﬁed.”
Files that have been added or modiﬁed are veriﬁed against
the RIMs. The tool uses the publisher’s log (which contains
the exact software versions) to determine which sets of RIMs
should be used to verify the software and updates installed
on the VA. Added or modiﬁed ﬁles for which a correspond-
ing RIM could not be identiﬁed are marked as “unveriﬁed.”
From the unveriﬁed ﬁles, the tool separates out the conﬁgu-
ration ﬁles and performs a “diﬀ” against the base image ver-
sion (when available), or against the default version included
in the software. In cases where a base VM is not available
for the initial comparison, the VMCVT simply veriﬁes all of
the ﬁles in the VA against the RIMs. Although this would
increase the number of ﬁles that need to be checked against
the RIMs (which can be expensive if a third party service is
utilized), the contents of the veriﬁcation reports should not
be aﬀected.
The RIM’s metadata is used to identify which ﬁles need
to be present for a certain version of software to be consid-
ered complete and unmodiﬁed. For deleted ﬁles, the tool
identiﬁes the ﬁles that should not have been deleted based
on that metadata. The tool also marks deleted base image
ﬁles that belong to partially removed/uninstalled packages.
Those ﬁles are all marked as “missing.”; ﬁles that are part
of completely uninstalled packages are not marked.
3.2.1 Veriﬁcation report
Based on the veriﬁcation results, the tool generates a dig-
itally signed “veriﬁcation report” that states the following:
• the list of installed software (and updates), including
the version information;
• the list of veriﬁed, unveriﬁed, and missing ﬁles for each
installed software; the ﬁle types, indicating whether a
ﬁle is an executable, source ﬁle, web page, conﬁgura-
tion ﬁle, image ﬁle, compressed ﬁle, or data ﬁle;
• the list of conﬁguration ﬁles that have changed for each
installed software and the content diﬀerences;
• the “integrity score” (explained in Section 3.2.2) for
each installed software;
• the list of unveriﬁed ﬁles that are not part of the in-
stalled software;
• and the hash of the entire VA image.
The report provides consumers with an explicit list of the
diﬀerences between the VA and the trusted software. Such a
list provides a guide for inspecting changed ﬁles and conﬁgu-
rations in the installed VA. For modiﬁed conﬁguration ﬁles,
in addition to the list, we provide consumers with the list of
diﬀerences between the VA ﬁle and a trusted conﬁguration
ﬁle. There may also be unveriﬁed ﬁles that are not part of
any installed software/update packages. All unveriﬁed ﬁles
are grouped by their ﬁle types (see above), and a summary
of the number of unveriﬁed ﬁles for each ﬁle type is shown
in the report. The last element of the report is the hash of
the entire VA image. This hash value allows the consumer
to check the integrity of the VA (as it is described in the
report) before launching it. The generated report is signed
and published together with the VA.
The VMCVT can be made available to publishers so they
may test and evaluate their VAs prior to publication. If the
publishers believe that their legitimate software is failing
the integrity checks, they may submit a report to the cloud
provider (or a third party managing the Conﬁguration Re-
solver), asking for a review and inclusion of their software.
Such a practice will also help publishers get their logs cor-
rect. If their logs are listing wrong versions of software or
are missing certain directory paths for a software package,
their VAs will get low integrity scores; the veriﬁcation re-
ports will show which ﬁles have failed the integrity checks
or which ﬁles are missing. It is the responsibility of publish-
ers to use that information to correct their logs and add any
missing directory paths before publishing their VAs.
3.2.2
Software integrity score and expected behavior
To summarize the analysis, the VMCVT tool considers
each piece of software installed in a VA and assigns it a
score on a scale of 1 to 3 using the rules described below.
The score represents the integrity and completeness of the
software as perceived by the VMCVT, indicating the extent
to which a software will operate and behave as expected.
• integrity score 3 —the software has no unveriﬁed or
missing ﬁles, with the exception of conﬁguration ﬁles;
234• integrity score 2 —the software has no unveriﬁed or
missing critical ﬁles, but may have unveriﬁed or miss-
ing non critical ﬁles;
• integrity score 1 —the software has unveriﬁed or miss-
ing critical ﬁles.
Installed software that has an integrity score of 3 is con-
sidered fully veriﬁed and integrity-protected, falling under
the “clean or high-integrity installation” category. Clean in-
stallation indicates that a particular software package will
behave as expected. A complete veriﬁcation of such soft-
ware requires only an inspection of the conﬁguration ﬁles.
Software that has a score of 2 has only ﬁles deemed non
critical missing or unveriﬁed; hence, the impact on expected
behavior is considered to be limited. Such software is cate-
gorized as “partially clean or medium-integrity”. Score of 1
represents the “modiﬁed or low-integrity” category as such
software may have critical ﬁles missing or unveriﬁed. It is
highly likely that such low-integrity packages, which may
have unveriﬁed or missing critical ﬁles, will not operate in
an expected manner. These integrity scores allow the VA
consumer to identify easily software packages that have a
standard unmodiﬁed installation and those that have been
modiﬁed or have a nonstandard installation.
4. ANALYSIS OF REAL-WORLD VIRTUAL
APPLIANCES
In this section we shed light on the integrity of software
packages in real-world VAs using the framework described
in the previous section. By looking at the variance in the
results, we gauge how useful the veriﬁcation reports would
be for consumers in selecting well installed VAs, and for
providers in removing suspicious VAs from their stores. We
generated veriﬁcation reports for 151 randomly picked Ama-
zon VAs and analyzed them.
4.1 Methodology and assumptions
4.1.1
Sampling method
We sampled the content of the Amazon market [2] by ran-
domly selecting publicly available Amazon Machine Images
(AMIs). We instantiated the selected images and obtained
their disk content using a set of commands equivalent to the
AMI tools provided by Amazon. As our prototype takes
advantage of the Red Hat package manager (rpm database)
in validating image content (in order to ease the veriﬁcation
process as explained further in Section 4.1.3), we focus on
rpm-based distributions, which represent a signiﬁcant por-
tion of the Linux images available in the market.
Our random sample is composed of 151 images from an
estimated pool of 2,300 valid rpm-based AMIs available in
the Amazon US-east zone. We created the sample pool by
acquiring the list of available AMIs through EC2 API calls.
The call returned a list of 8,798 images that are available
for instantiation. This list was reduced to 4,513 AMIs af-
ter ﬁltering out Windows and dpkg-based Linux images. We
randomly selected 300 images from this ﬁltered list and tried
instantiating them. About 50% of the selected images failed
at boot due to errors, lack of user credentials or product
codes, leaving us with a sample of 151 successfully instanti-
ated VAs.
Table 1: Three randomly selected sub groups. The num-
bers shown here are average values.
Total number of ﬁles
Number of veriﬁed ﬁles
Number of software packages
Group 1 Group 2 Group 3
52,786
52,886
52,137
52,036
51,909
51,183
427
427
426
4.1.2 Representativeness of the samples
We evaluated whether our sample size is suﬃciently large
to represent the entire sample pool (instantiable rpm-based
AMIs estimated at 50% of the total 4,513). If our samples
are a representative set, the randomly shuﬄed subgroups
(partial sets) of the samples should also represent our sam-
ples (i.e., show similar characteristics) with respect to the
properties of interest, which are the number of ﬁles, number
of veriﬁed (and unveriﬁed) ﬁles and number of software pack-
ages (see Table 1). To verify whether they do, we randomly
shuﬄed the 151 VAs and divided them into three subgroups,
repeating this process 30 times and computing the average
values. The three groups are hypothesized to share simi-
lar characteristics and closely represent each other. To test
this hypothesis, we compare the properties of interest be-
tween the three subgroups. The results are shown in Table
1. There is no signiﬁcant diﬀerence among the three ran-
domly generated groups. Moreover, our statistical analysis
2 = 0.11, p =
did not reject the null hypothesis, showing χ
0.99, and degrees of freedom of 4.
4.1.3 The VMCVT prototype
Using a combination of shell and python scripts, we con-
structed a prototype implementation of the VMCVT tai-
lored to the experiment at hand. Given a base image and a
derived VA, it ﬁrst generates the checksums by hashing all of
the ﬁles in both images. It compares the hash values between
the two images and creates a list of added ﬁles, modiﬁed ﬁles,
and deleted ﬁles. The rpm verify command is then used to
verify the integrity of the ﬁles that were added, modiﬁed, or
deleted through rpm; rpm verify checks every ﬁle installed
through rpm against its database of MD5 checksums and
checks cryptographic signatures. The VMCVT keeps track
of all the ﬁles that failed rpm verify, including the deleted
ﬁles that should not have been deleted (these are what we
refer to as the missing ﬁles). This reduces the eﬀort needed
to manually create the whitelist, since we are satisﬁed with
the ﬁles that are veriﬁed through rpm. In a production im-
plementation, though, each and every ﬁle in a VA should be
checked against a common whitelist.
VMCVT then checks the remaining added, modiﬁed and
deleted ﬁles (i.e., those that are unknown to rpm) against the
whitelist that we created (see Section 4.1.4) and marks the
ﬁles that are unveriﬁed or missing. To ﬁgure out whether
an entire software package was removed from the base im-
age (without using rpm), we inspected the diﬀerences in the
ﬁle directories between the two images. If an entire direc-
tory for a software package was removed from the base, we
assumed that this software package was completely unin-
stalled. Deleted ﬁles that were part of this directory were
ignored and did not aﬀect the software integrity scores.
The above generated unveriﬁed/missing list, together with
the list that failed rpm verify, contains the complete list of
unveriﬁed added and modiﬁed ﬁles as well as the missing ﬁles
235/
%
1
,
)
’
0
*
,
/
/
,
#
.
-
%
,
)
,
&
%
+
*
"
’
)
(
’
&
%
$
#
"
!
(!!!
’!!!
&!!!
%!!!
$!!!
#!!!
"!!!
!
!)!*
&)!*
"!)!*
"&)!*
#!)!*
#&)!*
2*+%&,),%-.#,//,*0’),1%/’345
Figure 3: Cumulative frequency of VAs vs. percent-
percentage of unveriﬁed/missing ﬁles
age of unveriﬁed/missing ﬁles
Figure 4: Number of unveriﬁed/missing ﬁles vs.
that should not have been deleted. The VMCVT classiﬁes
these ﬁles into the ﬁle types shown in Section 3.2.1 using
the file command. Then, by ﬁguring out which software
package each unveriﬁed/missing ﬁle belongs to, the VMCVT
computes the integrity score of the installed software using
the rules described in Section 3.2.2. Because the metadata
regarding the criticality of ﬁles are not available, we treat
all executables, source ﬁles, web pages, and image ﬁles as
“critical”, and compressed or other data ﬁles as “non critical.”
As the ﬁnal step, the VMCVT generates a veriﬁcation report
(see Section 3.2.1).
4.1.4 Constructing the whitelist
A software whitelist was constructed manually based on
the list of added, modiﬁed, and deleted ﬁles that rpm verify
did not know about. This “rpm-unknown list” was generated
for all the VAs, keeping track of the absolute paths and ﬁle
names. By examining the absolute paths and the ﬁle con-
tents, we ﬁgured out the exact software versions that were
installed (without using rpm) on each VA; we then down-
loaded the source code and binary packages (e.g., tar.gz or
zip ﬁles) for them from their respective vendor websites. Af-
ter extracting the packages, we created MD5 checksums (the
RIMs) for every ﬁle that was contained in the packages and
added them to the whitelist. Here, we assumed that the
downloaded packages represented trusted sources.
4.2 VA classiﬁcation based on the percentage
of unveriﬁed/missing ﬁles
4.2.1 Classiﬁcation method
Figure 3 shows the cumulative frequency of unveriﬁed/missing
ﬁle percentages. 90% of the VAs have less than 4.5% of un-
veriﬁed or missing ﬁles. Our intuition was that the number
of unveriﬁed/missing ﬁles would indicate, to some degree,
the integrity level of software in a VA. As the ﬁrst step to
study their characteristics, we ﬁrst looked for a correlation
between the number of unveriﬁed/missing ﬁles and the total
number of ﬁles in a VA, but found none. We did ﬁnd, how-
ever, a correlation between the number of unveriﬁed/missing
ﬁles and the percentage (Figure 4, Pearson’s: 0.84). Using
that correlation, we classify the VAs into the following three
“Integrity Level Groups” (ILG) to help demonstrate common
VA characteristics:
• “ILG A”—44 VAs are in this group, and they have less
than 0.1% of unveriﬁed/missing ﬁles;
• “ILG B”—59 VAs are part of this group, and they have
0.1-1% of unveriﬁed/missing ﬁles;
• “ILG C”—48 VAs in this group have > 1% of unveri-