wracked by a string of service outages in late March 2011 (A. R. Hickey, 2011). The outages 
occurred on a Monday, a Tuesday and a Friday, but many users reported issues lasting an entire 
week  (A.  R.  Hickey,  2011).  Popular  cloud-based  Intuit  services  like  QuickBooks  Online, 
QuickBooks Online Payroll and Intuit Payments Solutions conked out during the outages, which 
were blamed on errors introduced during maintenance operations (A. R. Hickey, 2011). 
There are several reasons for failures during sporadic operations on cloud (M. Fu, et al., 2014; 
M. Fu, et al., 2015; M. Fu, et al., 2016). The reasons include: 1) uncertainty of cloud APIs; 2) 
unpredictable  cloud  performance  variance  during  the  sporadic  operations;  3)  race  conditions 
from  different  cloud  operators  in  different  teams  who  manipulate  on  the  same  resources 
required by the sporadic operation, which refer to the cases where the resources required by the 
operation  are  changed  by  another  team  who  work  on  another  operation;  4)  wrongly  specified 
operation logic or configuration.  We discuss the first three reasons one by one, and the fourth 
reason  is  out  of  the  scope  of  our  research  because  our  research  assumes  that  the  sporadic 
operations as well as their configurations are correctly specified (M. Fu, et al., 2015). 
1)  Cloud  APIs  uncertainty.  When  cloud  operators  perform  sporadic  operations,  whether 
manually  or  with  automation  tools,  they  actually  interact  with  the  cloud  platform  by  calling 
relevant  cloud  API  functions  provided  by  the  cloud  platform.  For  example,  when  cloud 
operators launch cloud instances, they achieve this by calling the cloud API of “RunInstances” 
in AWS (Amazon Web Service) EC2 (Elastic Compute Cloud) (AWS, 2016). Cloud APIs are 
sometimes  unstable  and  such  APIs  failures  are  unpredictable  (Q.  Lu,  et  al.,  2013).  There  are 
several  different  failure  scenarios  for  cloud  API  functions  which  are  called  by  sporadic 
operations on cloud. First, a cloud API called by a sporadic operation can fail because it is not 
successfully executed. For example, the cloud API of “RunInstances” is called to launch a cloud 
11 
instance  but 
the  cloud 
instance 
is  not 
launched  at  all,  or 
the  cloud  API  of 
“RegisterInstancesWithELB” is called to register a cloud instance into the elastic load balancer 
(ELB) but the cloud instance is never registered successfully into ELB. Second, some cloud API 
functions called by sporadic operations might take longer execution time than expected or they 
are  just  stuck  without  taking  any  effect.  For  example,  the  cloud  API  of  “TerminateInstances” 
which  is  called  by  the  operation  of  terminating  cloud  instances  sometimes  could  take  up  to 
several  hours  to  execute  without  terminating  any  cloud  instance.  Third,  for  the  cloud  API 
functions which should have a responded return of the execution results, the execution results of 
them  are  not  returned  or  even  wrong  execution  results  are  returned.  In  previous  existing 
research on cloud APIs dependability and reliability (Q. Lu, et al., 2013; Q. Lu, et al., 2014), the 
failure  rates  for  cloud  API  functions  are  determined  based  on  a  large  number  of  empirical 
studies,  as  shown  in  Table  1.  For  example,  the  cloud  APIs  of  “TerminateInstances”, 
“DetachVolume”  and  “RunInstances”  have  the  failure  rates  of  3.9%,  3.2%  and  3.1% 
respectively  (Q.  Lu,  et  al.,  2013;  Q.  Lu,  et  al.,  2014).  The  cloud  APIs  of  “StartInstances”, 
“StopInstances”, 
“RegisterInstancesWithLoadBalancer” 
and 
“DeregisterInstancesFromLoadBalancer”  have the failure rates of 1.9%, 1.8%, 1.5% and 1.5% 
respectively (Q. Lu, et al., 2013; Q. Lu, et al., 2014). The cloud API of “AttachVolume” has a 
failure rate of 0.3% (Q. Lu, et al., 2013; Q. Lu, et al., 2014). Although these failure rates seem 
to be relatively low, when we combine them together to form different sporadic operations on 
cloud,  the  failure  rates  of  these  sporadic  operations  will  be  relatively  high.  This  is  even 
exacerbated  when  the  sporadic  operations  are  more  and  more  frequently  performed  by  cloud 
operators.  Hence,  due  to  such  uncertainty  and  highly  variable  performance  characteristics  of 
cloud API functions, the sporadic operations on cloud which rely on cloud API functions could 
fail at unpredictable times. 
Table 1.  Cloud APIs Failure Rates 
Cloud API 
1. RunInstances 
2. TerminateInstances 
3. StartInstances 
4. StopInstances 
5. AttachVolume 
6. DetachVolume 
7. RegisterInstanceswithLoadBalancer 
8. DeregisterInstancesFromLoadBalancer 
12 
Failure Rate 
3.1% 
3.9% 
1.9% 
1.8% 
0.3% 
3.2% 
1.5% 
1.5% 
2) Unpredictable cloud performance variance. In 2010, Saarland University did an empirical 
study on cloud performance variance using AWS EC2 as the target cloud platform (J. Schad, et 
al., 2010). It is found that performance unpredictability is a major issue in cloud computing (J. 
Schad,  et  al.,  2010).  The  researchers  used  established  micro-benchmarks  to  measure 
performance  variance  in  CPU,  I/O,  and  network  of  EC2.  They  used  a  multi-node  MapReduce 
application to quantify the impact on real data-intensive applications. They collected data for an 
entire month and compared it with the results obtained on a local cluster. It was observed that 
EC2 performance varies a lot and often falls into two bands having a large performance gap in-
between  (J.  Schad,  et  al.,  2010).  It  was  also  observed  that  these  two  bands  correspond  to  the 
different  virtual  system  types  provided  by  Amazon  (J.  Schad,  et  al.,  2010).  Moreover,  further 
analysis  indicates  that,  among  others,  the  choice  of  availability  zones  also  influences  the 
performance  variability  (J.  Schad,  et  al.,  2010).  A  major  conclusion  of  this  work  is  that  the 
variance on EC2 is currently so high and it even has effects on service-level agreement (SLA) 
satisfaction. During sporadic operations on cloud, the cloud system is still providing service to 
end users, and the SLA during the sporadic operations should also be obeyed. If the SLA of the 
system  is  violated  during  sporadic  operations  on  cloud  due  to  the  unpredictable  cloud 
performance  variance,  the  sporadic  operations  are  also  considered  to  be  failure.  Hence, 
unpredictable cloud performance variance also contributes to the failure of sporadic operations 
on cloud. 
3) Race conditions from cloud operators. In modern IT industries, large-scale cloud systems 
often have multiple operational procedures performed simultaneously (L. Bass, I. Weber and L. 
Zhu, 2015). These simultaneous operations are usually performed by different teams (L. Bass, I. 
Weber  and  L.  Zhu,  2015).  For  example,  a  team  responsible  for  system  upgrade  is  performing 
the  upgrade  operation  on  the  cloud  system  and  at  the  same  time  another  team  responsible  for 
system  reconfiguration  is  performing  the  reconfiguration  operation  on  the  cloud  system. 
Sporadic  operations  on  cloud  require  and  manipulate  certain  cloud  resources  on  the  cloud 
platform.  The  cloud  resources  required  and  manipulated  by  different  simultaneous  operations 
could overlap, either partially or completely. For example, for cloud applications where several 
cloud instances are attached to an auto scaling group (ASG), both the rolling upgrade operation 
and  the  reconfiguration  operation  performed  on  them  need  to  manipulate  the  same  cloud 
resource  which  is  the  auto  scaling  group  (ASG).  The  rolling  upgrade  operation  needs  to 
maintain  the  desired  instance  number  parameter  of  the  ASG  and  change  the  AMI  (Amazon 
Machine  Image)  version  parameter  of  the  ASG,  while  the  reconfiguration  operation  needs  to 
change  the  desired  instance  number  parameter  of  the  ASG  and  maintain  the  AMI  version 
parameter of the ASG. Such resource overlap can make some of (or even all) the simultaneous 
operations into failure. For example, during the rolling upgrade operation, if the ASG’s desired 
13 
instance  number  is  changed  by 
the  other  operator  who  performs 
the  simultaneous 
reconfiguration operation, the rolling upgrade operation will end up launching a wrong number 
of cloud instances. Likewise, during the reconfiguration operation, if the ASG’s AMI version is 
changed  by  another  operator  who  performs  the  simultaneous  rolling  upgrade  operation,  the 
reconfiguration operation will end up launching a set of wrong cloud instances with the wrong 
version  of  image.  Hence,  such  race  conditions  from  the  simultaneously  performed  operations 
which  require  and  manipulate  certain  shared  cloud  resources  can  also  lead  to  the  failure  of 
sporadic operations on cloud. 
1.2.4  Failure Detection and Diagnosis for Sporadic Operations on Cloud  
In  order  for  a  recovery  method  to  recover  from  operational  failures,  failures  have  to  be  first 
detected.  Failure  detection  methods  range  from  log-based  detection  methods  which  rely  on 
runtime  logs  generated  to  detect  and  diagnose  failures  to  monitoring-based  detection  methods 
which rely on external monitoring components to detect errors and failures, such as watchdog 
processors (A. Mahmood and E. J. McCluskey, 1988; X. Xu, et al., 2015; W. Sun, et al., 2016; J. 
Li, et al., 2013). Our recovery method for sporadic operations on cloud is designed to be non-
intrusive, which means that the method does not change any of the source code of the sporadic 
operations, e.g. operation scripts. In order to make our recovery methodology non-intrusive, it is 
based  on  log  analysis  that  failures  in  sporadic  operations  on  cloud  are  detected.  We  have  an 
existing  error  detection  framework  called  POD-Diagnosis  (X.  Xu,  et  al.,  2014),  which  is 
responsible  for  detecting  failures  in  sporadic  operations  on  cloud  and  is  able  to  trigger  an 
external recovery service. We use this error detection framework to detect failures in sporadic 
operations on cloud. 
POD-Diagnosis  is  a  model-based  approach  that  explicitly  models  a  sporadic  operation  as  a 
process, uses the process context to locate errors, filters logs, visits fault trees, and performs on-
demand assertion evaluation online for operational failure detection, error diagnosis and failure 
root  cause  analysis  (X.  Xu,  et  al.,  2014).  POD-Diagnosis  detects  cloud  operational  failures 
(mainly caused by cloud infrastructure-level errors) in a non-intrusive way, because it does not 
change  any  code  of  the  operation  and  only  relies  on  the  process  model  of  the  operation  and 
runtime logs of the operation to detect operational failures. Based on the correct process model 
obtained from the process mining procedure (X. Xu, et al., 2014), each operational step of the 
sporadic operation is able to be identified by POD-Diagnosis. It is important for POD-Diagnosis 
to  correctly  locate  each  operational  step  of  the  sporadic  operation,  because  POD-Diagnosis 
needs to know which exact operational step in the sporadic operation it is dealing with, in order 
for  it  to  perform  failure  detection  and  diagnosis  in  the  correct  operational  context  and  scope. 
POD-Diagnosis  retrieves  the  log  information related to  each  operational  step and  analyses  the 
14 
log information to obtain the current cloud system state after each operational step, and then it 
generates  the  expected  assertions  for  each  operational  step  at  runtime,  and  next,  it  makes 
comparison between the current cloud system state and the generated expected assertions to see 
whether the expected assertions are violated or not. If the assertions are violated, then it means 
that  the  operational  errors  have  occurred  for  that  particular  step  and  hence  failures  have  been 
detected.  This  is  called  assertion  evaluation  (X.  Xu,  et  al.,  2014).  In  POD-Diagnosis,  an 
example  of  detecting  operational  failures  based  on  log  analysis  can  be  as  follows:  in  the  first 
step  of  the  rolling  upgrade  operation  (Create  New  Launch  Configuration  for  Auto  Scaling 
Group),  the  log  information  says  that  “a  new  launch  configuration  of  auto  scaling  group  has 
been  created  and  it  is  attached  to  machine  image  001”,  but  the  assertions  expect  that  “the 
created new launch configuration is attached to machine image 002”, and hence the assertions 
are  violated,  so  the  failure  is  detected.  Other  examples  of  failure  detection  in  POD-Diagnosis 
include  some  cloud  infrastructure-level  errors  such  as  “cloud  instance  termination  time  is  too 
long”, “cloud instance cannot be launched or stopped”, and “cloud instance cannot be registered 
with elastic load balancer”, etc. POD-Diagnosis is implemented as a web service. After failures 
in  sporadic  operations  on  cloud  are  detected,  POD-Diagnosis  triggers  the  recovery  service 
(POD-Recovery)  to  perform  recovery  to  recover  from  the  failures  of  sporadic  operations  on 
cloud.  Hence,  the  differences  between  POD-Diagnosis  and  POD-Recovery  are  two-fold:  1) 
POD-Diagnosis  is  used  for  detecting  failures  in  sporadic  operations  on  cloud  while  POD-
Recovery is used for recovering from  these failures; 2) the POD-Recovery service is triggered 
and  invoked  by  the  POD-Diagnosis  service.  POD-Recovery  is  also  implemented  as  a  web 
service. Making both of these two frameworks as web services can ease the maintenance of each 
framework, and also it puts little restrictions on what implementation language should be used 
for  each  framework.  When  POD-Diagnosis  triggers  the  recovery  service  (POD-Recovery),  it 
passes  the  following  information  to  the  recovery  service  as  the  input  parameters:  1)  the 
specifications of the current operational step, including step id, step name and step parameters; 2) 
the  current  erroneous  state  of  cloud  system  resources.  The  first  parameter  is  used  by  the 
recovery service to locate the current operational step in the sporadic operation, and the second 
parameter  is  used  by  the  recovery  service  to  keep  a  record  of  the  current  erroneous  resource 
state of the cloud system.  
POD-Diagnosis is able to detect the failures of operational steps as soon as possible, based on 
analysing the runtime logs generated for each operational step in a real-time manner. Because it 
immediately  triggers  the  recovery  service  once  an  operational  failure  is  detected,  the  goal  of 
runtime  recovery  is  achieved  by  our  recovery  method.  If  POD-Diagnosis  detects  failures  late 
(i.e.  the  failures  are  detected  after  a  number  of  steps),  then  the  recovery  becomes  very 
challenging, because the step that is currently being executed does not have the detected failures 
15 
that have occurred and the failure detection service as well as the recovery service have lost the 
track of the original operational step where the detected failures occur. Hence, in this case the 
failure detection service will not trigger the recovery service and such late-detected failures are 
out of the scope of our recovery framework. In other words, in our recovery methodology, the 
failure detection and recovery for sporadic operations on cloud only focus on the cases where 
failures for the operational steps are detected as soon as possible, i.e. the failures are detected in 
the first place immediately after certain operational steps in a sporadic operation.  
In particular, the cooperation between POD-Diagnosis and POD-Recovery follows the software 
architecture  of  SOA  (Service  Oriented  Architecture)  (E.  Newcomer  and  G.  Lomow,  2004).  A 
service-oriented  architecture  (SOA)  is  an  architectural  pattern  in  computer  software  design  in 
which  application  components  provide  services  to  other  components  via  a  communications 
protocol,  typically  over  a  network  (E.  Newcomer  and  G.  Lomow,  2004).  A  service  is  a  self-
contained unit of functionality, such as retrieving an online bank statement. By that definition, a 
service is an operation that may be discretely invoked. Services can be combined to provide the 
functionality  of  a  large  software  application  (T.  Velte,  et  al.,  2010).  SOA  makes  it  easier  for 
software components on computers connected over a network to cooperate. Every computer can 
run any number of services, and each service is built in a way which ensures that the service can 
exchange  information  with  any  other  service  in  the  network  without  human  interaction  and 
without  the  need  to  make  changes  to  the  underlying  program  itself  (E.  Newcomer  and  G. 
Lomow, 2004). 
16 
Chapter 2.  Literature Review 
Reviewing the existing recovery strategies for cloud provides us with the knowledge about gaps 
in  existing  recovery  methods,  and  it  can  also  provide  insights  into  how  to  propose  a  new 
recovery methodology for sporadic operations on cloud. In order to fully understand the existing 
cloud  recovery  strategies,  some  basic  concepts  and  theories  related  to  cloud  recovery  are 
introduced  first  (section  2.1).  In  the  literature  review,  we  extract  the  data  of  several  existing 
recovery strategies for cloud, illustrate these recovery methods and discuss their challenges and 
limitations.  These  existing  cloud  recovery  strategies  include:  rollback  recovery  for  cloud 
applications, disaster  recovery  in cloud  computing,  virtual  machine replication in  cloud, fault-
tolerance in cloud computing, recovery for cloud internal protocols, test driven scripts for cloud 
operations, exception handling in cloud operations, recovery for cloud operations as transactions, 
recovery  for  cloud  operations  using  undo  framework,  user  guided  recovery  for  cloud  web 
service  applications  and  BPEL  recovery  in  cloud.  For  each  of  these  existing  cloud  recovery 
strategies, we clarify  its relationship with the recovery  methodology proposed in  our research. 
The details of these existing cloud recovery strategies are presented in section 2.2. Finally, we 
present  a  taxonomy  for  these  existing  cloud  recovery  methods,  in  order  to  determine  which 
existing cloud recovery methods are more relevant to the recovery methodology proposed in our 
research. The  details  of  the  taxonomy  are  provided in  section  2.3.  In  particular,  the  taxonomy 
may  also  provide  useful  guidance  for  other  researchers  who  work  on  different  contexts  and 
scopes of cloud recovery (M. Fu, et al., 2014). 
2.1  Basic Concepts & Theories 
The following basic concepts and theories are related to existing cloud recovery strategies and 
they can be found in the literature. The basic concepts and theories introduced are: 1) System 
Anomaly  Detection;  2)  Fault  Tolerance;  3)  Disaster  Recovery;  4)  System  Rollback;  5)  Log 
Analysis; 6) Cloud Infrastructure;  7) Virtual Machine Replication; 8) RTO; 9) RPO; 10) MTTR; 
11) Recovery Scalability; 12) Sporadic Operations; 13) Normal Activities. These basic concepts 
and  theories  span  the  entire  recovery  space  for  cloud,  including  recovery  strategies  for  both 
sporadic  operations  (e.g.  upgrade)  and  normal  operations  (e.g.  online  trading  workflow)  on 
cloud.  These  concepts  and  theories  are  relevant  to  the  existing  recovery  strategies  on  cloud 
because they can help to understand the fundamental purposes and mechanisms of the recovery 
methods  and  they  also  provide  guidance  on  how  to  design  and  implement  the  recovery 
strategies.  These  concepts  and  theories  are  discussed  here  because  they  can  provide  us  with 
necessary  fundamental  knowledge  of  how  recovery  strategies  can  be  designed  in  different 
scenarios.  
17 
1) System Anomaly Detection. System anomaly detection means to detect anomalies (such as 
errors,  failures  and  other  abnormal  issues)  within  a  system.  Anomaly  detection  for  systems  is 
significant  for  maintaining  service  and  business  continuity  of software systems  and increasing 
their dependability, especially when the systems are  large scale distributed systems (Y. Liu, et 
al.,  2010).    There  are  some  MapReduce-Based  Frameworks  that  have  been  implemented  for 
detecting  anomalies  (Y.  Liu,  et  al.,  2010).  Anomaly  detection  is  usually  conducted  by 
integrating and analysing system logs, and the K-means clustering algorithm is one of the ways 
for  collecting  and  integrating  distributed  system  logs  (Y.  Liu,  et  al.,  2010).  After  system 
anomalies are detected, they need to be mapped to the corresponding components in the system 
to  determine  which  anomaly  has  occurred  in  which  component  in  the  system,  in  order  to 
perform  further  diagnosis  for  the  system  anomalies.  This  can  be  achieved  by  several  existing 
methods such as FChain (H. Nguyen, et al., 2013). 
2)  Fault  Tolerance.  Fault  tolerance,  as  its  name  indicates,  means  tolerating  system  faults 
instead of really solving or fixing them. Fault Tolerance can be achieved by creating a replica of 
the system which serves as the standby system. For example, one of the typical fault tolerance 
paradigms is the “master node/slave node” design. When faults occur in the master node, it can 
fail over to the slave node to continue the service. Although fault tolerance is a good practice for 
system  recovery  design,  nowadays  it  has  received  relatively  limited  attentions  (J.  Behl,  et  al., 
2012).  
3)  Disaster  Recovery.  Disaster  recovery  refers  to  recovering  from  disasters  in  datacentres. 
Disasters  could  either  mean  naturally  occurring  disastrous  conditions  (such  as  earthquakes, 
tornados, etc.) or mean manually incurred disasters (such as datacentre power outages). Disaster 
recovery is important for maintaining business continuity of datacentres and systems (L. DuBois, 
2013). 
4)  System  Rollback.  System  rollback  means  rolling  back  the  system  to  a  previous  consistent 
state. A state could be a snapshot of the machines in a system or a global state of an application. 
Actually,  there  is  a  mechanism  similar  to  rollback,  called  the  undo  mechanism.  Undo  can  be 
viewed  as  a  type  of  human  assisted  recovery  method  (A.  B.  Brown,  et  al.,  2004).  However, 
since  our  research  focus  is  on  automation  of  recovery  mechanisms,  I  mainly  look  into  the 
automated  recovery  methods  with  automation  features.  The  existing  rollback  mechanisms 
belong to the automated recovery methods. For example, checkpoint based rollback  (E. N. M. 
Elnozahy, et al., 2002), as a type of rollback mechanism, deals with how to automatically make 
checkpoints in a proper way for recovery; log based rollback (E. N. M. Elnozahy, et al., 2002), 
as another type of rollback mechanism, deals with how to automatically create and analyse logs 
in an efficient way for recovery. 
18 