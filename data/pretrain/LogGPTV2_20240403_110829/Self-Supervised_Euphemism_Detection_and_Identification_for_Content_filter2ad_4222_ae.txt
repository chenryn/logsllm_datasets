0.08
0.10
0.18
0.03
0.22
0.25
ğ‘ƒ@80
0.08
0.16
0.12
0.09
0.20
0.14
0.28
0.38
0.20
0.20
0.13
0.05
0.05
0.03
0.15
0.25
0.09
0.15
0.09
0.08
0.14
0.04
0.19
0.23
ğ‘ƒ@100
0.09
0.18
0.10
0.07
0.19
0.11
0.26
0.36
0.18
0.18
0.11
0.04
0.04
0.02
0.15
0.20
0.09
0.13
0.10
0.06
0.11
0.03
0.15
0.19
False positive analysis: By studying the false positives in
our results, we recovered several euphemisms that were not
included in our ground truth list. Table X in Appendix A
shows sentences associated with 10 of the top 16 false positive
euphemisms from the drug dataset. Several of these are true
euphemisms for drug keywords that were not present in the
DEA ground truth list (e.g., md, l, mushrooms). Others are
not illicit drugs (e.g., alcohol, cigarettes), but they are used
in this corpus in a way that is closely related to how people
use drug names, and reveal new usage patterns. For example,
the sentences for â€œcigarettes" indicate that people appear to
be combining cigarette use with other drugs, such as PCP.
Similarly, the sentences containing â€œalcohol" reveal that people
are dissolving illicit drugs in alcohol. Of these 10 false positives
(according to our ground truth dataset), only ï¬ve are actually
false positives; these words are semantically related to the drug
keywords, but they are not proper euphemisms (e.g., â€œpressed"
is a form factor for drug pills).
C. Euphemism Identiï¬cation
For each euphemism that we have successfully detected, we
now evaluate euphemism identiï¬cation.
Evaluation Metric: For each euphemism, we generate a
probability distribution over all target keywords and therefore,
obtain a ranked list of the target keywords. We evaluate the top-
ğ‘˜ accuracy (ğ´ğ‘ğ‘@ğ‘˜), which measures how often the ground
truth label (target keyword) falls in the top ğ‘˜ values of our
generated ranked list.
Baselines: Given the lack of related prior work for the task of
euphemism identiï¬cation, we establish a few baseline methods
and compare our proposed approach with them.
â€¢ Word2vec: For each euphemism, we select the target
keyword that is closest to it using the measure of cosine
similarity. Here we compare the word embeddings (100-
dimensional) obtained by training the word2vec algorithm
[13], [14] on each text corpus separately.
â€¢ Clustering + word2vec: For each euphemism, we cluster
all its masked sentences, represented as the average of
the word embeddings of the component words, using a ğ‘˜-
means algorithm (we set ğ‘˜ = 2). By clustering, our aim is
to separate the masked sentences into two groups (ideally
one group of informative masked sentences and the other
group of uninformative masked sentences as presented
in Table III) and to ï¬lter out the uninformative masked
sentences that are not related to the target keywords.
Then, we compare the embeddings of the ï¬ltered masked
sentences of the euphemism and the target keywords using
the measure of cosine similarity. The target keyword that
is most similar to the ï¬ltered masked sentences is selected
for identiï¬cation.
â€¢ Binary + word2vec: similar to our approach, we use
a binary classiï¬er to ï¬lter out noisy masked sentences
that are not related to the target keywords. Then, we use
the Word2vec approach above to ï¬nd its closest target
keyword.
â€¢ Fine-grained-only is an simplistic version of our ap-
proach, which only uses the ï¬ne-grained multi-class
classiï¬er, without the preceding coarse classiï¬er.
Results: Table VII summarizes the euphemism identiï¬cation
results. There are 33, 9, and 12 categories for the drug, weapon
and sexuality datasets respectively, resulting in a random guess
performance for ğ´ğ‘ğ‘@1 to be 0.03, 0.11, 0.08 (i.e. the inverse
of the number of categories). Our algorithm achieves the best
performance for all three datasets and has a large margin over
the random guess performance.
Word2vec exhibits poor performance, in that it is unable to
capture the nuanced diï¬€erences between the target keywords by
taking all sentences into consideration. Therefore, we construct
two baselines (i.e., Clustering + word2vec and Binary +
word2vec) to remove the noisy sentences and aid learning using
a more homogeneous set of masked sentences. Empirically,
we ï¬nd that a binary classiï¬er contributes more towards the
performance, compared to the clustering algorithm. This is
because, the result of clustering did not adequately cluster the
sentences into a target keyword cluster and a non-target keyword
cluster. Taking the drug dataset as an example, we found that
owing to the widely varying contexts and vocabulary diversity
of the dataset, the clustering results were inadequate. For
instance, a qualitative examination of the results of clustering for
a few euphemisms showed that the cluster separation sometimes
occurred by the â€œqualityâ€ attribute (e.g., high quality vs. low
quality drugs) or even sentiment (e.g., feeling high vs. feeling
low). Therefore, ğ‘˜-means clustering fails as a ï¬lter for the non-
drug-related masked sentences and does not lead to performance
improvement. We leave exploring other clustering algorithms
for future work. In contrast, the binary classiï¬er, which can be
taken as a directed ğ‘˜-means clustering algorithm, speciï¬cally
ï¬lters out the non-drug-related sentences and is therefore a
helpful addition. For such a speciï¬c task, the binary classiï¬er
performance can be taken as a performance upper bound for
clustering algorithms.
We highlight two important ï¬ndings: 1) By comparing the
results of Word2vec and Fine-grained-only, we demonstrate
the advantage of using a classiï¬cation algorithm over an
unsupervised word embedding-based method; 2) By comparing
the diï¬€erences between Word2vec and Binary + word2vec, and
the diï¬€erences between Fine-grained-only and our approach,
we demonstrate the superior discriminative ability of a binary
ï¬ltering classiï¬er and therefore, highlight the beneï¬t of using
a coarse-to-ï¬ne-grained classiï¬cation over performing only
multi-class classiï¬cation.
VI. Discussion
Our algorithms rely on a relatively small number of hyper-
parameters and choices of classiï¬cation models. In this section,
we demonstrate how to choose these hyper-parameters through
detailed ablation studies, primarily on the drug dataset.
A. Ablation Studies for Euphemism Identiï¬cation
As discussed above, we adopt a coarse-to-ï¬ne-grained
classiï¬cation scheme for euphemism identiï¬cation, relying
on two classiï¬ers used in cascade. We discuss here the
Results on euphemism identification. Best results are in bold.
Table VII
Word2vec
Clustering + word2vec
Binary + word2vec
Fine-grained-only
Our Approach
Word2vec
Clustering + word2vec
Binary + word2vec
Fine-grained-only
Our Approach
Word2vec
Clustering + word2vec
Binary + word2vec
Fine-grained-only
Our Approach
g
u
r
D
n
o
p
a
e
W
y
t
i
l
a
u
x
e
S
ğ´ğ‘ğ‘@1
0.07
0.06
0.13
0.11
0.20
0.10
0.11
0.22
0.25
0.33
0.17
0.15
0.21
0.19
0.32
ğ´ğ‘ğ‘@2
0.14
0.15
0.22
0.19
0.31
0.27
0.25
0.43
0.40
0.51
0.22
0.30
0.39
0.40
0.55
ğ´ğ‘ğ‘@3
0.21
0.25
0.30
0.26
0.38
0.40
0.37
0.57
0.61
0.67
0.42
0.49
0.59
0.51
0.64
Figure 4. Testing accuracy for the coarse classiï¬er.
performance of multiple classiï¬ers on both coarse and ï¬ne-
grained classiï¬cation.
1) Coarse Classiï¬ers:
In the euphemism identiï¬cation
framework, we use a binary classiï¬er to ï¬lter out the sentences
where euphemisms are used in non-euphemistic senses. We
experiment with the binary classiï¬ers shown below. Note that
for all the neural models, we use 100-dimensional GloVe
embeddings12 [99] pre-trained on Wikipedia and tune the
embeddings by the models.
â€¢ Logistic Regression [91] on raw Text (LRT): we ï¬rst
represent each word as a one-hot vector and then represent
each sentence as the average of its member wordsâ€™
encodings.
â€¢ Logistic Regression on text Embeddings (LTE): we learn
the word embeddings (100-dimensional) using word2vec
[13], [14]. We represent each sentence by the average of
its member wordsâ€™ embeddings.
â€¢ Recurrent Neural Network (RNN) [100]: we use a 1-layer
bidirectional RNN with 256 hidden nodes.
â€¢ Long Short-Term Memory (LSTM) [89]: we use a 1-layer
bidirectional LSTM with 256 hidden nodes.
12https://nlp.stanford.edu/projects/glove/
0.850.870.890.91LSTM.
â€¢ Convolutional Neural Networks (CNN) [101]: we train
a simple CNN with one layer of convolution on top of
word embeddings.
â€¢ Recurrent Convolutional Neural Networks (RCNN) [102]:
we apply a bidirectional LSTM and employ a max-pooling
layer across all sequences of texts.
â€¢ LSTM-Attention: we add an attention mechanism [90] on
â€¢ Self-Attention [103]: instead of using a vector, we use a
2-D matrix to represent the embedding, with each row of
the matrix attending on a diï¬€erent part of the sentence.
We split the datasets into 70-10-20 for training, validation
and testing. The model parameters are tuned on the validation
data. Empirically, we ï¬nd the LSTM-Attention performs the