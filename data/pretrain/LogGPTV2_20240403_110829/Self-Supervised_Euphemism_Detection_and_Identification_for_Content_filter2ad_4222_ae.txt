0.08
0.10
0.18
0.03
0.22
0.25
𝑃@80
0.08
0.16
0.12
0.09
0.20
0.14
0.28
0.38
0.20
0.20
0.13
0.05
0.05
0.03
0.15
0.25
0.09
0.15
0.09
0.08
0.14
0.04
0.19
0.23
𝑃@100
0.09
0.18
0.10
0.07
0.19
0.11
0.26
0.36
0.18
0.18
0.11
0.04
0.04
0.02
0.15
0.20
0.09
0.13
0.10
0.06
0.11
0.03
0.15
0.19
False positive analysis: By studying the false positives in
our results, we recovered several euphemisms that were not
included in our ground truth list. Table X in Appendix A
shows sentences associated with 10 of the top 16 false positive
euphemisms from the drug dataset. Several of these are true
euphemisms for drug keywords that were not present in the
DEA ground truth list (e.g., md, l, mushrooms). Others are
not illicit drugs (e.g., alcohol, cigarettes), but they are used
in this corpus in a way that is closely related to how people
use drug names, and reveal new usage patterns. For example,
the sentences for “cigarettes" indicate that people appear to
be combining cigarette use with other drugs, such as PCP.
Similarly, the sentences containing “alcohol" reveal that people
are dissolving illicit drugs in alcohol. Of these 10 false positives
(according to our ground truth dataset), only ﬁve are actually
false positives; these words are semantically related to the drug
keywords, but they are not proper euphemisms (e.g., “pressed"
is a form factor for drug pills).
C. Euphemism Identiﬁcation
For each euphemism that we have successfully detected, we
now evaluate euphemism identiﬁcation.
Evaluation Metric: For each euphemism, we generate a
probability distribution over all target keywords and therefore,
obtain a ranked list of the target keywords. We evaluate the top-
𝑘 accuracy (𝐴𝑐𝑐@𝑘), which measures how often the ground
truth label (target keyword) falls in the top 𝑘 values of our
generated ranked list.
Baselines: Given the lack of related prior work for the task of
euphemism identiﬁcation, we establish a few baseline methods
and compare our proposed approach with them.
• Word2vec: For each euphemism, we select the target
keyword that is closest to it using the measure of cosine
similarity. Here we compare the word embeddings (100-
dimensional) obtained by training the word2vec algorithm
[13], [14] on each text corpus separately.
• Clustering + word2vec: For each euphemism, we cluster
all its masked sentences, represented as the average of
the word embeddings of the component words, using a 𝑘-
means algorithm (we set 𝑘 = 2). By clustering, our aim is
to separate the masked sentences into two groups (ideally
one group of informative masked sentences and the other
group of uninformative masked sentences as presented
in Table III) and to ﬁlter out the uninformative masked
sentences that are not related to the target keywords.
Then, we compare the embeddings of the ﬁltered masked
sentences of the euphemism and the target keywords using
the measure of cosine similarity. The target keyword that
is most similar to the ﬁltered masked sentences is selected
for identiﬁcation.
• Binary + word2vec: similar to our approach, we use
a binary classiﬁer to ﬁlter out noisy masked sentences
that are not related to the target keywords. Then, we use
the Word2vec approach above to ﬁnd its closest target
keyword.
• Fine-grained-only is an simplistic version of our ap-
proach, which only uses the ﬁne-grained multi-class
classiﬁer, without the preceding coarse classiﬁer.
Results: Table VII summarizes the euphemism identiﬁcation
results. There are 33, 9, and 12 categories for the drug, weapon
and sexuality datasets respectively, resulting in a random guess
performance for 𝐴𝑐𝑐@1 to be 0.03, 0.11, 0.08 (i.e. the inverse
of the number of categories). Our algorithm achieves the best
performance for all three datasets and has a large margin over
the random guess performance.
Word2vec exhibits poor performance, in that it is unable to
capture the nuanced diﬀerences between the target keywords by
taking all sentences into consideration. Therefore, we construct
two baselines (i.e., Clustering + word2vec and Binary +
word2vec) to remove the noisy sentences and aid learning using
a more homogeneous set of masked sentences. Empirically,
we ﬁnd that a binary classiﬁer contributes more towards the
performance, compared to the clustering algorithm. This is
because, the result of clustering did not adequately cluster the
sentences into a target keyword cluster and a non-target keyword
cluster. Taking the drug dataset as an example, we found that
owing to the widely varying contexts and vocabulary diversity
of the dataset, the clustering results were inadequate. For
instance, a qualitative examination of the results of clustering for
a few euphemisms showed that the cluster separation sometimes
occurred by the “quality” attribute (e.g., high quality vs. low
quality drugs) or even sentiment (e.g., feeling high vs. feeling
low). Therefore, 𝑘-means clustering fails as a ﬁlter for the non-
drug-related masked sentences and does not lead to performance
improvement. We leave exploring other clustering algorithms
for future work. In contrast, the binary classiﬁer, which can be
taken as a directed 𝑘-means clustering algorithm, speciﬁcally
ﬁlters out the non-drug-related sentences and is therefore a
helpful addition. For such a speciﬁc task, the binary classiﬁer
performance can be taken as a performance upper bound for
clustering algorithms.
We highlight two important ﬁndings: 1) By comparing the
results of Word2vec and Fine-grained-only, we demonstrate
the advantage of using a classiﬁcation algorithm over an
unsupervised word embedding-based method; 2) By comparing
the diﬀerences between Word2vec and Binary + word2vec, and
the diﬀerences between Fine-grained-only and our approach,
we demonstrate the superior discriminative ability of a binary
ﬁltering classiﬁer and therefore, highlight the beneﬁt of using
a coarse-to-ﬁne-grained classiﬁcation over performing only
multi-class classiﬁcation.
VI. Discussion
Our algorithms rely on a relatively small number of hyper-
parameters and choices of classiﬁcation models. In this section,
we demonstrate how to choose these hyper-parameters through
detailed ablation studies, primarily on the drug dataset.
A. Ablation Studies for Euphemism Identiﬁcation
As discussed above, we adopt a coarse-to-ﬁne-grained
classiﬁcation scheme for euphemism identiﬁcation, relying
on two classiﬁers used in cascade. We discuss here the
Results on euphemism identification. Best results are in bold.
Table VII
Word2vec
Clustering + word2vec
Binary + word2vec
Fine-grained-only
Our Approach
Word2vec
Clustering + word2vec
Binary + word2vec
Fine-grained-only
Our Approach
Word2vec
Clustering + word2vec
Binary + word2vec
Fine-grained-only
Our Approach
g
u
r
D
n
o
p
a
e
W
y
t
i
l
a
u
x
e
S
𝐴𝑐𝑐@1
0.07
0.06
0.13
0.11
0.20
0.10
0.11
0.22
0.25
0.33
0.17
0.15
0.21
0.19
0.32
𝐴𝑐𝑐@2
0.14
0.15
0.22
0.19
0.31
0.27
0.25
0.43
0.40
0.51
0.22
0.30
0.39
0.40
0.55
𝐴𝑐𝑐@3
0.21
0.25
0.30
0.26
0.38
0.40
0.37
0.57
0.61
0.67
0.42
0.49
0.59
0.51
0.64
Figure 4. Testing accuracy for the coarse classiﬁer.
performance of multiple classiﬁers on both coarse and ﬁne-
grained classiﬁcation.
1) Coarse Classiﬁers:
In the euphemism identiﬁcation
framework, we use a binary classiﬁer to ﬁlter out the sentences
where euphemisms are used in non-euphemistic senses. We
experiment with the binary classiﬁers shown below. Note that
for all the neural models, we use 100-dimensional GloVe
embeddings12 [99] pre-trained on Wikipedia and tune the
embeddings by the models.
• Logistic Regression [91] on raw Text (LRT): we ﬁrst
represent each word as a one-hot vector and then represent
each sentence as the average of its member words’
encodings.
• Logistic Regression on text Embeddings (LTE): we learn
the word embeddings (100-dimensional) using word2vec
[13], [14]. We represent each sentence by the average of
its member words’ embeddings.
• Recurrent Neural Network (RNN) [100]: we use a 1-layer
bidirectional RNN with 256 hidden nodes.
• Long Short-Term Memory (LSTM) [89]: we use a 1-layer
bidirectional LSTM with 256 hidden nodes.
12https://nlp.stanford.edu/projects/glove/
0.850.870.890.91LSTM.
• Convolutional Neural Networks (CNN) [101]: we train
a simple CNN with one layer of convolution on top of
word embeddings.
• Recurrent Convolutional Neural Networks (RCNN) [102]:
we apply a bidirectional LSTM and employ a max-pooling
layer across all sequences of texts.
• LSTM-Attention: we add an attention mechanism [90] on
• Self-Attention [103]: instead of using a vector, we use a
2-D matrix to represent the embedding, with each row of
the matrix attending on a diﬀerent part of the sentence.
We split the datasets into 70-10-20 for training, validation
and testing. The model parameters are tuned on the validation
data. Empirically, we ﬁnd the LSTM-Attention performs the