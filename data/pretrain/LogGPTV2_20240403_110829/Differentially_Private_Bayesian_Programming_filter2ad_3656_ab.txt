hood are typically chosen in practice so that the posterior
belongs to the same family of distributions as the prior. In
this case the prior is said to be conjugate prior for the likeli-
hood. Using conjugate priors, besides being mathematically
convenient in the derivations, ensures that Bayesian inference
can be performed by a recursive process over the data.
Our goal is to perform Bayesian inference under diﬀerential
privacy. We provide the formal deﬁnition of diﬀerential
privacy in Deﬁnition 3.1, but for the purpose of this section
it is enough to know that diﬀerential privacy is a statistical
guarantee that requires the answer to a data analysis to
be statistically close when run on two adjacent databases,
i.e. databases that diﬀer in one individual. In the vanilla
version of diﬀerential privacy, the notion of “statistically
close” is measured by a parameter . A typical way to
achieve diﬀerential privacy is to add random noise, and we
present several primitives for doing this in § 3. For one
example, the exponential mechanism (denoted ExpMech)
returns a possible output with probability proportional to a
quality score function Q. Q takes in input a database and a
potential output for the statistic computed on the database,
and gives each output a score representing how good that
output is for that database. The privacy and the utility of
the mechanism depend on  and on the sensitivity of the
quality score function, i.e., how much the quality score can
diﬀer for two adjacent databases.
As a motivating example we will consider a simple Bayesian
inference task: learning the bias of a coin from some obser-
vations. For example, we can think of the observations as
medical records asserting whether patients from a sample
population have a disease or not. We can perform Bayesian
inference to establish how likely it is to have the disease in the
population. We will show how to make this task diﬀerentially
private, and verify privacy in PrivInfer.
First, the input of this example is a set of binary observa-
tions describing whether any given patient has the disease.
We consider this the private information that we want to
protect. We also assume that the number of patients n
is public and that the adjacency condition for diﬀerential
privacy states that two databases diﬀer in the data of one
patient. In our concrete case this means that two databases
d, d1 are adjacent if all of their records are the same except
for one record that is 0 in one database and 1 in the other.
While in abstract our problem can be described as esti-
mating the bias of a coin, we need to be more formal and
provide the precise model and the parameters that we want
to estimate. We can incorporate our initial belief on the
fairness of the coin using a prior distribution on the bias ξ
given by a beta distribution. This is a distribution over r0, 1s
with probability density:
betapξ | a, bq “ ξa´1p1 ´ ξb´1q
Bpa, bq
where a, b P R` are parameters and B denotes the beta
function. The likelihood is the probability that a series of
i.i.d samples from a Bernoulli distributed random variable
with bias ξ matches the observations. Using an informal
notation,1 we can write the following program in PrivInfer:
`
˘
λr.bernoulliprq “ obs
betapa, bq
(1)
infer
observe
´
¯
The term infer represents an inference algorithm and the
observe statement is used to describe the model. Expression
(1) denotes the posterior distribution that is computed using
Bayes’ theorem with prior betapa, bq and with likelihood
pλr.bernoulliprq “ obsq.
Now, we want to ensure diﬀerential privacy. We have
several options. A ﬁrst natural idea is to perturbate the
input data using the exponential mechanism, corresponding
to the following program:
infer
observepλr.bernoulliprq “ ExpMech Q obsq betapa, bq
´
¯
The fact that diﬀerential privacy is closed under post-
processing ensures that this guarantees diﬀerential privacy
for the whole program. In more detail, in the notation above
we denoted by Q the scoring function. Since obs is a boolean,
we can use a quality score function that gives score 1 to b
if b “ obs and 0 otherwise. This function has sensitivity 1
and so one achieves p, 0q-diﬀerential privacy. This is a very
simple approach, but in some situations it can already be
very useful [39].
A diﬀerent way of guaranteeing diﬀerential privacy is by
adding noise on the output. In this case the output is the
posterior which is a betapa1, b1q for some values a1, b1. Us-
ing again the exponential mechanism we can consider the
following program:
`
observepλr.bernoulliprq “ obsqbetapa, bq
ExpMech Q
infer
˘¯
´
1We omit in particular the monadic probabilistic construc-
tions. A formal description of this example can be found in
§ 6.
In this case, the exponential mechanism is not applied to
booleans but instead to distributions of the shape betapa1, b1q.
So, a natural question is which Q we can use as quality score
function and what is its sensitivity in this case.
There are two natural choices. The ﬁrst one is to consider
the parameters pa1, b1q as a vector and measure the possible
distance in term of some metric on vectors, e.g. the one given
by (cid:96)1 norm dppa, bq,pa1, b1qq “ |a´ a1|`|b´ b1|. The second is
to consider betapa1, b1q as an actual distribution and then use
a notion of distance on distributions, e.g. Hellinger distance
∆Hpbetapa, bq, betapa1, b1qq.
These two approaches both guarantee privacy, but they
have diﬀerent utility properties. Our system PrivInfer can
prove privacy for both approaches.
3. BACKGROUND
3.1 Probability and Distributions
In our work we will consider discrete distributions. Fol-
lowing Dwork and Roth [17] we will use standard names for
several continuous distributions but we will consider them to
be the approximate discrete versions of these distributions
ř
up to arbitrary precision.
We deﬁne the set DpAq of distributions over a set A as the
set of functions µ : A Ñ r0, 1s with discrete supportpµq “ tx |
xPA µ x “ 1. In our language we will
µ x ‰ 0u, such that
consider only distribution over basic types, this guarantees
that all our distributions are discrete (see § 4).
We will use several basic distributions like uniform, bernoulli,
normal, beta, etc. These are all standard distributions and we
omit their deﬁnition here. We will also use some notation to
describe distributions. For instance, given an element a P A,
we will denote by 1a the probability distribution that assigns
all mass to the value a. We will also denote by bind µ M
the composition of a distribution µ over the set A with a
function M that takes a value in A and returns a distribution
over the set B.
3.2 Differential Privacy
Diﬀerential privacy is a strong, quantitative notion of
In the
statistical privacy proposed by Dwork et al. [18].
standard setting, we consider a program (sometimes called
a mechanism) that takes a private database d as input, and
produces a distribution over outputs. Intuitively, d represents
a collection of data from diﬀerent individuals. When two
databases d, d1 are identical except for a single individual’s
record, we say that d and d1 are adjacent 2, and we write d Φ d1.
Then, diﬀerential privacy states that the output distributions
from running the program on two adjacent databases should
be statistically similar. More formally:
Deﬁnition 3.1 (Dwork et al. [18]). Let , δ ą 0 be two
numeric parameters, let D be the set of databases, and let
R be the set of possible outputs. A program M : D Ñ DpRq
satisﬁes p, δq-diﬀerential privacy if
PrpMpdq P Sq ď e PrpMpd
1q P Sq ` δ
for all pairs of adjacent databases d, d1 P D such that d Φ d1,
and for every subset of outputs S Ď R.
As shown by Barthe et al. [3], we can reformulate diﬀeren-
tial privacy using a speciﬁc statistical -distance -D:
2In our concrete examples we will consider sometime as adja-
cent also two databases that diﬀer by at most one individual.
Lemma 3.1. Let , δ P R`. Let D be the set of databases,
and let R be the set of possible outputs. A program M : D Ñ
DpRq satisﬁes p, δq-diﬀerential privacy iﬀ -DpMpdq, Mpd1qq ď
˘
δ, where d, d1 are adjacent databases and
rx P Es
-Dpµ1, µ2q ” max
EĎR
rx P Es ´ e ¨ Pr
xÐµ2
Pr
xÐµ1
`
for µ1, µ2 P DpRq.
to fpdq ` ν, where ν is drawn form the Laplace distribution
with scale 1{. This distribution has the following probability
density function:
Lap1{pxq “ 
2
expp´|x|q.
If f is a k-sensitive function, then the Laplace mechanism is
pk, 0q-diﬀerentially private.
Diﬀerential privacy is an unusually robust notion of privacy.
It degrades smoothly when private mechanisms are composed
in sequence or in parallel, and it is preserved under any post-
processing that does not depend on the private database.
The following lemmas capture these properties:
Lemma 3.2 (Post-processing). Let M : D Ñ DpRq be an
p, δq-diﬀerentially private program. Let N : R Ñ DpR1q be
an arbitrary randomized program. Then λd.bindpM dq N :
D Ñ DpR1q is p, δq-diﬀerentially private.
Diﬀerential privacy enjoys diﬀerent composition schemes,
we report here one of the simpler and most used.
Lemma 3.3 (Composition). Let M1 : D Ñ DpR1q, and M2 :
D Ñ DpR2q respectively p1, δ1q and p2, δ2q diﬀerentially
private programs. Let M : D Ñ DpR1 ˆ R2q the program
deﬁned as Mpxq ” pM1pxq, M2pxqq. Then, M is p1`2, δ1`
δ2q diﬀerentially private.
Accordingly, complex diﬀerentially private programs can
be easily assembled from simpler private components, and
researchers have proposed a staggering variety of private
algorithms which we cannot hope to summarize here. (Inter-
ested readers can consult Dwork and Roth [17] for a textbook
treatment.)
While these algorithms serve many diﬀerent purposes,
the vast majority are constructed from just three private
operations, which we call primitives. These primitives oﬀer
diﬀerent ways to create private mechanisms from non-private
functions. Crucially, the function must satisfy the following
sensitivity property:
Deﬁnition 3.2. Let k P R`. Suppose f : A Ñ B is a
function, where A and B are equipped with distances dA
and dB. Then f is k-sensitive if
dBpfpaq, fpa
1qq ď k ¨ dApa, a
1q
for every a, a1 P A.
Intuitively, k-sensitivity bounds the eﬀect of a small change
in the input, a property that is similar in spirit to the diﬀer-
ential privacy guarantee. With this property in hand, we can
describe the three basic primitive operations in diﬀerential
privacy, named after their noise distributions.
The Laplace mechanism. The ﬁrst primitive is the stan-
dard way to construct a private version of a function that
maps databases to numbers. Such functions are also called
numeric queries, and are fundamental tools for statistical
analysis. For instance, the function that computes the aver-
age age of all the individuals in a database is a numeric query.
When the numeric query has bounded sensitivity, we can use
the Laplace mechanism to guarantee diﬀerential privacy.
Deﬁnition 3.3. Let  P R` and let f : D Ñ R be a numeric
query. Then, the Laplace mechanism maps a database d P D
The Gaussian mechanism. The Gaussian mechanism is
an alternative to the Laplace mechanism, adding Gaussian
noise with an appropriate standard deviation to release a
numeric query. Unlike the Laplace mechanism, the Gaussian
mechanism does not satisfy p, 0q-privacy for any . However,
it satisﬁes p, δq-diﬀerential privacy for δ P R`.
Deﬁnition 3.4. Let , δ P R and let f : D Ñ R be a numeric
query. Then, the Gaussian mechanism maps a database
d P D to fpdq ` ν, where ν is a drawn from the Gaussian
distribution with standard deviation
a
σp, δq “
2 lnp1.25{δq{.
If f is a k-sensitive function for k ă 1{, then the Gaussian
mechanism is pk, δq-diﬀerentially private.
The exponential mechanism. The ﬁrst two primitives can
make numeric queries private, but in many situations we may
want to privately release a non-numeric value. To accomplish
this goal, the typical tool is the exponential mechanism [31],
our ﬁnal primitive. This mechanism is parameterized by
a set R, representing the range of possible outputs, and a
quality score function q : D ˆ R Ñ R, assigning a real-valued
score to each possible output given a database.
The exponential mechanism releases an output r P R
with approximately the largest quality score on the private
database. The level of privacy depends on the sensitivity of
q in the database. Formally:
Deﬁnition 3.5 (McSherry and Talwar [31]). Let  P R`.
Let R be the set of outputs, and q : D ˆ R Ñ R be the
quality score. Then, the exponential mechanism on database
d P D releases r P R with probability proportional to
ˆ
˙