

Pre-
processing
Feature
Extract
Inference
Decoding

?



?



?



?


Table I: Different categories of knowledge available to the
attacker. “” information unavailable to the attacker. “”
information available to the attacker. “?” information may or
may not be available to the attacker.
information is generally available for any public VPS. An ex-
ample of this is Google Speech API [63], Amazon Alexa [10]
and Siri [11]. In contrast, the preprocessing, feature extraction,
inference and decoding components are all unknown.
d) No-Box: This is an extreme version of the black-box
access type. The attacker has no knowledge of any of the
categories. Consider the example of the attacker attempting
to subvert a telephony surveillance system. Infrastructure for
such a surveillance system uses VPSes to efﬁciently convert
millions of hours of captured call audio into searchable text.
The attacker does not have access to this infrastructure. She
only knows that her phone calls will be captured and tran-
scribed. Of all the knowledge types, this is the most restrictive.
A no-box setting is the worst-case scenario for the attacker.
D. Adversarial Capabilities
1) Constrains on the Input Manipulations:
a) Input and Output Granularity: Attacks against VPSes
are of three granularities: phoneme1, word and sentence level.
These measure two aspects about the attack: the window of
input that the attacker will need to perturb and total change
in output transcription. For example, consider an attacker who
can mistranscribe an entire sentence by only changing a single
phoneme. Here, the input granularity is phoneme and the
output granularity is sentence. Similarly, if the attacker needs
to perturb the entire sentence to change the transcription of a
single word, then input granularity is sentence and the output
granularity is word.
b) Types of Adversarial Audio: Depending on the attack
type and scenario, adversaries can produce different types of
audio samples. These audio samples can be categorized into
the following broad classes:
• Inaudible: As discussed in Section II-A,
the human
auditory system can only perceive frequencies that range
from 20Hz to 20kHz. In contrast, by using microphones
to capture audio, VPSes can record frequencies beyond
20kHz. To exploit this discrepancy, attackers can encode
an audio command in the ultrasound frequency range
(20kHz to 10Mhz). The encoded command is detected
and recorded by the microphone but is inaudible to the
human listener. The recorded command is then passed
onto the VPS which executes it, considering it human
speech. These inaudible attacks have been able to exploit
modern VPS devices such as Alexa, Siri, and Google
in
Home [42]. These attacks can not be ﬁltered out
1A phoneme is a single distinct unit of sound in a language.
software, as they exploit the hardware component (micro-
phone) of the pipeline. The audio sample is aliased down
to less than 20kHz when recorded by the microphone.
However, these attacks might still leave artifacts in the
signal that can potentially reveal the attack. While attacks
in this space focus on the ultrasound frequencies, the
frequency range below 20Hz is also inaudible and could
be a vector in this class of attack.
• Noise: This category of attacks produces audio samples
that sound like noise to humans but are considered
legitimate audio commands by the VPS. It is difﬁcult for
the human auditory system to interpret audio that is non-
continuous, jittery and lossy. Attacks can mangle audio
samples such that they will not be intelligible by humans
and will (hopefully) be ignored as mere noise. However,
the same mangled audio sample is processed by the VPS
as legitimate speech. This can allow attackers to trick a
VPS into unauthorized action.
• Clean: The last category of attacks perturb audio such
that
it sounds clean to humans even though there is
a hidden command embedded inside it. These attacks
embed commands as low-intensity perturbations, into an
audio sample such as music, which is not noticeable by
humans. However, the VPS detects and executes these
embedded commands.
2) Access to the Model:
a) Queries: A query consists of sending the model an
input and receiving the corresponding output. As described in
Section II-D2, the attacker makes multiple queries to a target
model to produce a single adversarial sample. A threat model
must consider the maximum number of times an attacker can
query the victim system. Too many queries will alert the
defender of an attack. Additionally, most proprietary systems
require users to pay for every query. Too many queries can
quickly compound the monetary cost of an attack.
b) Output: A model can produce one of two types of
outputs. This can either be a single label (i.e., transcription),
or a probability distribution over all the labels.
A probability distribution over all the labels is the individual
probabilities of the input belonging to each of the labels. For
example, the modle might output the following distribution for
a recording of “lock”: LOCK: 90%, LOOK: 7% and LOKK:
3%. This means that the model is 90% , 7%, and 3% certain
that the input is “lock”, “look”, and “lokk” respectively.
The model is still calculating the probability distribution
when it outputs a single label. However, the model will only
return the label with the highest probability. It is entirely
possible for the adversary to have access to some combination
of both label and probability distribution. For example, she
may have the ﬁnal label and the next top K labels.
3) Attack Medium: Depending on the scenario, the ad-
versarial sample can be passed to the system over different
mediums, each of which can introduce new challenges (e.g,
noise). In the case of adversarial images, the perturbed input
can be passed directly as a .jpg ﬁle. However, in a more
realistic scenario, the attacker will have to print the perturbed
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:09:49 UTC from IEEE Xplore.  Restrictions apply. 
736
image, and hope when the target takes a picture of the image
that the perturbations are captured by the camera. In the case
of VPSes, the mediums can be broadly grouped into four
types: Over-Line (Figure 4(a)), Over-Air (Figure 4(b)), Over-
Telephony-Network (Figure 4(c)), and Over-Others. In this
section, we discuss each of the mediums in detail.
a) Over-Line: The input is passed to the model directly
as a Waveform Audio ﬁle or .wav (Figure 4(a)). Compared
to all the other mediums, attacks Over-Line are the easiest to
execute as this medium ensures “lossless” transmission.
b) Over-Air: Over-Air (Figure 4(b)), involves playing the
audio using a speaker. As an example, consider an attacker
who wants to exploit an Amazon Alexa. The attacker plays
the adversarial audio over the speaker, which travels through
the air. The audio is then recorded and interpreted by Alexa.
As mentioned earlier, adversarial perturbations are sensitive
and can be lost during transmission. In the Over-Air scenario,
the loss can occur due to interference, background noise or
imperfect acoustic equipment [64]. Therefore, attacks that
produce audio that can survive the highly lossy mediums of
air are much stronger than those that cannot.
c) Over-Telephony-Network: An attack Over-Telephony-
Network (Figure 4(c)), involves playing audio samples over
the telephony network and exploiting any VPS transcribing
the phone call. The telephony network is a lossy medium
due to static interference, codec compression, packet loss, and
jitter [65], [65], [66], [67], [68], [69], [70]. Adversarial audio is
sensitive to lossy mediums and is at risk of losing perturbations
during transmission. Therefore, attacks that produce audio
samples that can survive the telephony network are also much
stronger than those that cannot.
d) Over-Others: This category includes mediums that do
not fall into the above three categories. One example of this
is MPEG-1 Audio Layer III or mp3 compression. The .wav
audio samples are often compressed using mp3 compression
before being transmitted. This compression technique is lossy
and is bound to result in some of the adversarial perturbations
being discarded. Attacks that produce perturbations that only
survive mp3 compression are stronger than Over-Line, but are
weaker than Over-Telephony-Network and Over-Air.
4) Distance: The further an attack audio sample travels
over a lossy medium, the greater the degradation. As a result,
the attack audio sample is less likely to exploit the target VPS.
One metric to measure the strength of an attack is the distance
it can travel without losing its exploitative nature.
5) Acoustic Environment: This includes the different acous-
tic environments that
the attack audio sample was tested
in (e.g., noisy environments). This is because the various
mediums introduce unique sources of distortion. For Over-
Air attacks, this involves testing the attack audio sample with
varying levels of noise. For Over-Telephony-Network attacks,
this involves testing the attack over real telephony networks.
6) Acoustic Equipment: Over-Air and Over-Telephony-
Network attacks involve passing the audio samples over a
range of acoustic equipment. These include codecs, micro-
phones, speakers, etc., which are subject to hardware imperfec-
Figure 4: Depending on the threat model, an attack audio
might be passed over various mediums. (a) Over-Line Attack:
Audio ﬁle is passed directly to the target model as a .wav ﬁle.
(b) Over-Telephony-Network Attack: The audio transmitted
over the telephony network to the target model. (c) Over-Air
Attack: The audio ﬁle is played using a speaker. The audio
travels over the air and is recorded by a microphone. The
recorded signal is then passed onto the target model.
tions. Certain frequencies might be attenuated and intensiﬁed,
based on the particular equipment’s impulse response. How-
ever, if an attack audio sample does not work against a range
of equipment, then it is too sensitive. Any attack that makes
assumptions tied to a particular set of acoustic equipment is
considered weaker than one that does not.
V. EXISTING ATTACK CLASSIFICATION
We use the threat model taxonomy described in the previous
section to classify the existing attacks and identify the areas
that need improvement. Table II and Table III show the
progress of current attacks against VPSes.
1) Targeted White-box Attacks: Most targeted attacks in Ta-
ble II require complete white-box knowledge of the target [41],
[6], [72], [75], [73], [74], [76], [77]. All of these are based
on the optimization strategy and can embed hidden commands
inside audio samples such as music or noise. These attacks can
generate adversarial samples that are either clean or noisy.
One of two conditions must be satisﬁed for these attacks
to be applicable in the real-world. Either the attacker has
perfect knowledge of the target or the adversarial samples are
transferable. However, both these assumptions are impractical.
Regarding the former, it is unlikely for an attacker to have
perfect knowledge of the target VPS, primarily because these
systems are proprietary. For example, the underlying construc-
tion and functionality of ASRs such as Alexa and Siri are a
closely guarded trade secret. Similarly, the latter does not hold
either, as we will discuss in Section XI-A. As a result, these
white-box attacks are constrained against real-world systems
like Alexa or Siri.
2The most up-to-date version of
https://sites.google.com/view/adv-asr-sok/
this
table
can be
found at:
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:09:49 UTC from IEEE Xplore.  Restrictions apply. 
737
Attack Name
Taori et al. [71]
Carlini et al. [6]
Houdini [72]
Kreuk et al. [73]
Qin et al. [74]
Schonherr et al. [75]
Abdoli et al. [76]
Commander Song [41]
Yakura et al. [77]
Devil’s Whisper [78]
M. Azalnot et al. [79]
Kenansville Attack [29]
Dolphin Attack [42]
Light Commands [80]
Abdullah et al. [7]
Cocaine Noodles [81]
HVC (2) [82]
HVC (1) [82]
Clean
Clean
Clean
Clean
Clean
Clean
Clean
Clean
Clean
Clean
Clean
Clean
Noisy
Noisy
Noisy
Noisy
Inaudible
Inaudible
Goal
Targeted
Targeted
Targeted
Targeted
Targeted
Targeted
Targeted
Targeted
Targeted
Targeted
Targeted
Untargeted
Targeted
Targeted
Targeted
Targeted
Targeted
Targeted
S
S
S
S
S
S
W
S
S
S
W
S
S
S
S
S
S
P,W,S
P,W,S
Audio Type
Granularity
Granularity Knowledge Queries
Time
Input
Output
S
S
S
S
S
S
W
S
S
S
W
S
S
S
S
S
S
Gray
White
White
White
White
White
White
White
White
Black
Black
No
Black
Black
Black
Black
Black
White
?
1000
?
?
?
500
?
?
?
1500
?
15
?
?
10
?
?
?
Output
Distribution
Distribution
Distribution
Distribution
Distribution
Distribution
Distribution
Distribution
Distribution
Label