beyond 1000 PPS because of measurement etiquette consid-
erations, as most probes in this experiment solicit a response
from an intermediate router. The machine is otherwise idle.
CPU consumption: All of our tests consumed nearly
the same total amount of CPU time regardless of PPS rate,
between 25.3 seconds and 26.9 seconds. This can be ex-
plained by the use of eﬃcient data structures that scale in
O(logn) and sharing sockets amongst tasks so that only a
few sockets need to be monitored. The total CPU time con-
sumed is greatest for the tests with the lowest PPS rates, be-
cause the scamper process accounts for more wall and there-
fore system CPU time. Figure 2 plots the maximum CPU
usage observed for scamper when conﬁgured with diﬀerent
PPS rates. CPU requirements grow linearly with probing
speed. This experiment is well within system capabilities of
our Pentium 3 800Mhz, requiring 8.1% of the CPU at peak
when probing at 1000pps.
Memory consumption:
In order to meet a speciﬁed
PPS rate, scamper adds new tasks as necessary, each re-
quiring additional memory for maintaining state. Figure 2
also plots the maximum RSS reported by the operating sys-
tem for each test. At 100pps, scamper requires 1.9MB at
peak, and memory requirements grow linearly to 6.2MB at
1000pps. The memory and CPU requirements to run scam-
per are modest.
Eﬀect on data collected: Overall, summary statistics
for the data collected are almost identical for each PPS rate;
5.2% of destination were reachable, 10.0% received an ICMP
destination unreachable code, 5.9% halted because of an ap-
parent forwarding loop, and 78.9% halted after three consec-
utive unresponsive hops. In addition, the number of packets
sent for all data sets is between 232k and 234k, and the num-
ber of IP-level links is between 29.2k and 29.3k. Figure 3
plots the time required to complete each traceroute for se-
lected PPS values; the lines for other PPS rates fall between
the PPS values that are plotted. For this workload, the dis-
tribution of time required to complete task is also almost
identical for all PPS rates; however, 10% of tasks take up
to ﬁve seconds more to complete in some experiments – the
time that traceroute waits before timing out and sending
another probe. There is no correlation between the probing
rate and these variations; our experience with experiments
at other times leads us to believe that the diﬀerences are
due to network conditions at the time of this sample.
6. USING SCAMPER
As described in Section 3, scamper provides the ability to
be run as a daemon and then controlled using a Unix domain
socket or a TCP socket bound to the loop-back interface. As
a simple example, consider a researcher who wishes to dy-
namically collect data about router-level connectivity avail-
able towards a set of destinations. To do so, the researcher
requires the ability to collect the interface graph using MDA
traceroute, determine the utility of probing each interface
with TCP, ICMP, and UDP probes with ping to determine
which probe types are able to solicit responses with incre-
menting IP-ID values, and then the ability to collapse the
graph using an alias resolution technique such as RadarGun.
The driver program the researcher would write to support
this collection follows from the data requirements.
Brieﬂy, the driver is responsible for remembering the list of
targets to probe with MDA traceroute, the list of interfaces
to probe with ping, and the utility of each ping method for
soliciting an incrementing IP-ID so that repeated ping mea-
surements are unnecessary. While there are targets to probe,
the driver communicates to scamper the measurement tasks
for it to traceroute and ping. As each measurement com-
pletes, scamper sends the results back to the driver across
the control socket. The data arrives in a binary form, so
to decode it the driver could use a socketpair, writing the
binary data in one end and then reading the decoded mea-
surements from the other using the provided API for doing
so. At the same time, the driver can record the binary data
to disk to archive the raw data for later use. When the list of
targets to probe has become empty and all results are back
from scamper, the ﬁnal step is to provide scamper with a
RadarGun probing speciﬁcation containing the interfaces to
probe paired with the appropriate probe method it should
use, be it ICMP, UDP, or TCP. When the data collection
is complete, the ﬁnal step required is process the data col-
lected to infer which interfaces are aliases, and then produce
the router-level graph from it.
2437. EXPERIENCES
Identifying IPv6 network problems in the dual-
stack world: the ﬁrst use of scamper was a 2004 study that
compared the forward paths of IPv4 and IPv6 addresses be-
lieved to belong to the same host [17]. The goal was to use
the data to ﬁnd IPv6 paths that performed poorly compared
to their IPv4 counterpart, and to provide operators with
suggestions on where IPv6 routing could be improved. The
work used scamper’s one-shot measurement functionality; a
list of IPv4 and IPv6 addresses were provided to scamper
which then probed addresses in parallel until the list was
completed. The main ﬁndings of this work were (1) only a
small proportion of targets have a much larger delay with
IPv6 than with IPv4, and (2) the impact of IPv6 tunnels,
inferred by changes in Path-MTU, depends heavily on the
upstream connectivity of the vantage point. With the IANA
pool of remaining IPv4 addresses rapidly running out, it is
important that the worst of the indirect IPv6 routes be iden-
tiﬁed and mechanisms created to make operators aware of
them. Future work will look into automating data collection
and procedures to communicate problems to operators.
CAIDA’s macroscopic Internet topology project:
scamper is used in CAIDA’s macroscopic Internet topol-
ogy discovery project to collect forward-IP path data on a
continuous basis, beginning in September 2007. As of Au-
gust 2010, there are 48 vantage points distributed across the
globe, divided into three teams. Members of each team col-
lectively probe a randomly generated address in all /24 pre-
ﬁxes routed on the Internet. Work is coordinated amongst
vantage points using the Marinda tuple-space system [9]. An
external measurement process monitors scamper’s progress;
when a unit of work is complete, a new set of random IP
addresses is written to a ﬁle and passed to scamper using its
control socket. Scamper is conﬁgured to probe at 100pps; a
team of 12 vantage points requires approximately two and
a half days to probe all routed /24 preﬁxes. The data col-
lected is made available to researchers as the IPv4 Routed
/24 Topology dataset [27]. There is substantial work to be
done to translate the data into a router-level graph of the
Internet using alias resolution, and the development of tech-
niques to correctly ﬁlter AS-level links from it.
IPv6 AS-core poster: one of CAIDA’s widely recog-
nised visualisations is the IPv4 AS core poster showing the
geographic connectivity and importance of ASes. In 2008,
CAIDA had few vantage points with IPv6 connectivity with
which to collect the raw IP topology data required, so volun-
teers were solicited on the NANOG mailing list. Each volun-
teer downloaded the scamper source code, compiled it, and
then ran scamper using a supplied address list. Contribu-
tors from 53 diﬀerent cities running varied operating systems
supplied data, demonstrating scamper’s portability and ease
of volunteer use. The data was used to support a geographic
comparison of the IPv4 and IPv6 AS-level graphs [28].
Traceroute probe method comparison: in 2008, we
compared the utility of ﬁve diﬀerent methods and found
that ICMP-Paris traceroute reaches the most destinations
and infers the most AS links when destinations are chosen
randomly [29]. To collect the data required, we wrote a
driver that connected to a running scamper process and is-
sued a series of traceroute commands for each destination;
the next method to use was chosen randomly by the driver,
and it waited at least ﬁve seconds between traceroutes to
any single destination. Using scamper’s support for exter-
nal drivers allowed us to focus on the data collection and
analysis aspects of the work, rather than implementing our
own complicated probing loops and algorithms.
Quantifying the pitfalls of traceroute:
in 2009, re-
searchers at Harbin Institute of Technology and UCLA ex-
amined the limitations of using traceroute data and the cor-
responding longest BGP preﬁx matches to infer AS connec-
tivity [30]. To do so requires the ability to collect tracer-
outes from a vantage point where a BGP feed is also avail-
able. CAIDA’s Ark topology project has three such vantage
points; a fourth was created in UCLA by using scamper to
collect traceroute data from their campus network where
they were also able to procure a BGP feed.
8. RELATED WORK
Spring et al. built scriptroute with a goal of allowing im-
plementations of measurement techniques to be portable and
able to be used on a public general purpose measurement fa-
cility [6]. Both scriptroute and scamper include a portability
layer so researchers can focus on the logic of their technique,
and include implementations of common measurement tech-
niques. Scriptroute provides researchers with a distributed
set of machines from which to run experiments; with scam-
per, we focused on building a parallelised packet-prober that
can be used easily in both measurement infrastructure and
stand-alone contexts.
fping [31], hping [32], and nmap [33] have portable and
parallelised implementations of ping, traceroute, and net-
work security tests respectively, allowing networks to be
rapidly tested. Scamper is built with the Internet researcher
in mind; it provides an extensible ﬁle-format that records
the detail of a measurement to provide a researcher with
sound data, and provides ﬂexibility in the control of the
measurement process making it suited for use in Internet
measurement infrastructure.
9. CONCLUSION
Internet researchers are faced with many challenges when
building and designing experiments; overcoming technical
limitations of operating systems, recording results and data
in a way that allows sound analysis to take place, paral-
lelising their implementation so it can scale to meet the
size of the Internet, and ﬁnding enough vantage points to
be conﬁdent their results are representative of the Internet.
Scamper provides a ﬂexible and reusable packet-probing ar-
chitecture that allows researchers to focus on scientiﬁc ex-
periments rather than building accurate and scalable instru-
mentation. Scamper’s architecture has been shown to be
useful to a community-oriented network measurement in-
frastructure, as it is currently used in CAIDA’s Archipelago
system [9]. The source code to scamper is freely available at
http://www.wand.net.nz/scamper/.
Acknowledgements
We thank WIDE and CAIDA for funding and supporting
the initial development of scamper. This work is currently
supported by New Zealand Foundation for Research Science
and Technology (FRST) contract UOWX0705.
244In IMC ’05, pages 193–198, Berkeley, CA, October
2005.
[19] Benoit Donnet, Timur Friedman, and Mark Crovella.
Improved algorithms for network topology discovery.
In PAM 2005, pages 149–162, Boston, MA, March
2005.
[20] Ethan Katz-Bassett, Harsha V. Madhyastha,
Vijay Kumar Adhikari, Colin Scott, Justine Sherry,
Peter van Wesep, Thomas Anderson, and Arvind
Krishnamurthy. Reverse traceroute. In NSDI ’10,
pages 219–234, San Jose, CA, April 2010.
[21] Brice Augustin, Timur Friedman, and Renata
Teixeira. Measuring load-balanced paths in the
Internet. In IMC ’07, pages 149–160, San Diego, CA,
October 2007.
[22] R-fx Networks. Advanced policy ﬁrewall (APF). http:
//www.r-fx.ca/downloads/apf-0.9.6-3.tar.gz.
[23] Ramesh Govindan and Hongsuda Tangmunarunkit.
Heuristics for Internet map discovery. In INFOCOM,
pages 1371–1380, Tel-Aviv, Israel, March 2000.
[24] N. Spring, R. Mahajan, and D. Wetherall. Measuring
ISP topologies with Rocketfuel. In SIGCOMM ’02,
pages 133–145, Pittsburgh, PA, August 2002.
[25] Adam Bender, Rob Sherwood, and Neil Spring. Fixing
Ally’s growing pains with velocity modeling. In IMC
’08, pages 337–342, Vouliagmeni, Greece, October
2008.
[26] University of Oregon. Route Views.
http://www.routeviews.org/.
[27] Young Hyun, Bradley Huﬀaker, Dan Andersen, Emile
Aben, Matthew Luckie, and kc claﬀy. The CAIDA
IPv4 routed /24 topology dataset.
http://www.caida.org/data/active/ipv4_routed_
24_topology_dataset.xml.
[28] CAIDA. Visualizing IPv6 AS-level internet topology
2008. http://www.caida.org/research/topology/
as_core_network/ipv6.xml.
[29] Matthew Luckie, Young Hyun, and Brad Huﬀaker.
Traceroute probe method and forward IP path
inference. In IMC ’08, pages 311–323, Vouliagmeni,
Greece, October 2008.
[30] Yu Zhang, Ricardo Oliveira, Hongli Zhang, and Lixia
Zhang. Quantifying the pitfalls of traceroute in AS
connectivity inference. In PAM ’10, Zurich,
Switzerland, April 2010.
[31] fping - a program to ping hosts in parallel.
http://fping.sourceforge.net/.
[32] hping - active network security tool.
http://www.hping.org/.
[33] Gordon Lyon. nmap. http://nmap.org/.
10. REFERENCES
[1] David Mills. Internet delay experiments. RFC 889,
December 1983.
[2] V. Paxson, J. Mahdavi, A. Adams, and M. Mathis. An
architecture for large-scale Internet measurement.
IEEE Communications Magazine, 36(8):48–54, 1998.
[3] Sunil Kalidindi and Matthew J. Zekauskas. Surveyor:
An infrastructure for Internet performance
measurements. In INET’99, San Jose, CA, June 1999.
[4] Tony McGregor and Hans-Werner Braun. Balancing
cost and utility in active monitoring: The AMP
example. In INET 2000, Yokohama, Japan, July 2000.
[5] Bradley Huﬀaker, Daniel Plummer, David Moore, and
k claﬀy. Topology discovery by active probing. In
SAINT 2002, pages 90–96, Nara City, Japan, January
2002.
[6] Neil Spring, David Wetherall, and Tom Anderson.
Scriptroute: A public Internet measurement facility. In
USITS ’03, pages 225–238, Seattle, WA, March 2003.
[7] Yavul Shavitt and Eran Shir. DIMES: let the Internet
measure itself. Computer Communication Review,
35(5):71–74, 2005.
[8] Harsha Madhyastha, Tomas Isdal, Michael Piatek,
Colin Dixon, Thomas Anderson, Arvind
Krishnamurthy, and Arun Venkataramani. iPlane: An
information plane for distributed services. In OSDI
’06, pages 367–380, Seattle, WA, November 2006.
[9] Young Hyun. Archipelago measurement infrastructure.
http://www.caida.org/projects/ark/.
[10] kc claﬀy, Mark Crovella, Timur Friedman, Colleen
Shannon, and Neil Spring. Community-oriented
network measurement infrastructure (CONMI)
workshop report. ACM/SIGCOMM Computer
Communication Review, 36(2):41–48, April 2006.
[11] Van Jacobson. traceroute.
ftp://ftp.ee.lbl.gov/traceroute.tar.gz.
[12] Brice Augustin, Xavier Cuvellier, Benjamin Orgogozo,
Fabien Viger, Timur Friedman, Matthieu Latapy,
Cl´emence Magnien, and Renata Teixeira. Avoiding
traceroute anomalies with Paris traceroute. In IMC
’06, pages 153–158, Rio de Janeiro, Brazil, October
2006.
[13] Mark Allman and Vern Paxson. A reactive
measurement framework. In PAM 2008, pages 92–101,
Cleveland, OH, April 2008.
[14] Young Hyun. rb-wartslib: ruby warts library.
http://rb-wartslib.rubyforge.org/.
[15] Stefan Savage. Sting: a TCP-based network
measurement tool. In USITS ’99, pages 71–79,
Boulder, CO, October 1999.
[16] A. Medina, M. Allman, and S. Floyd. Measuring the
evolution of transport protocols in the Internet.
Computer Communication Review, 35(2):37–52, April
2005.
[17] Kenjiro Cho, Matthew Luckie, and Bradley Huﬀaker.
Identifying IPv6 network problems in the dual-stack
world. In ACM SIGCOMM workshop on Network
Troubleshooting, pages 283–288, Portland, OR, August
2004.
[18] Matthew Luckie, Kenjiro Cho, and Bill Owens.
Inferring and debugging path MTU discovery failures.
245