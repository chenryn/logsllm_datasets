feature extractors. Thus, a natural countermeasure a tracker
could employ is training their model to be extremely robust.
Despite the theoretically proven trade-off between normal
accuracy and robustness [56], future work may ﬁnd a way
to improve model robustness while minimizing the accom-
panying drop in accuracy. Thus, we evaluate cloaking suc-
5Image augmentation parameters:
rotation range=20o, horizontal
shift=15%, vertical shift=15%, zoom range=15%
cess when the tracker’s model is much more robust than the
user’s feature extractor. In our simpliﬁed test, the user has
a robust VGG2-Dense feature extractor (adversarially trained
for 3 epochs), while the tracker has an extremely robust
Web-Incept feature extractor (adversarially trained for 20
epochs). When the tracker’s model is this robust, the user’s
cloak only achieves a 64% protection success rate.
However, if the user is extremely privacy sensitive, she
could increase the visibility of her cloak perturbation to
achieve a higher protection success rate. Figure 16 high-
lights the trade off between protection success and the input
DSSIM level. The cloak’s protection success rate increases
to 100% once the DSSIM perturbation is > 0.01.
8.2 Cloak Detection
We now propose techniques a tracker could employ to detect
cloaked images in their dataset. We also discuss mitigations
the user could apply to avoid detection.
Since cloaking is a
Existing Poison Attack Detection.
form of data poisoning, prior work on detecting poisoning
attacks [11, 19, 40, 46, 49, 58] could be helpful. However, all
prior works assume that poisoning only affects a small per-
centage of training images, making outlier detection useful.
Fawkes poisons an entire model class, rendering outlier de-
tection useless by removing the correct baseline.
Anomaly Detection w/o Original Images. We ﬁrst con-
sider anomaly detection techniques in the scenario where the
tracker does not have any original user images. If trackers
obtain both target and cloaked user images, they can detect
unusual closeness between cloaked images and target images
in model feature space. Empirically, the L2 feature space dis-
tance between the cloaked class centroid and the target class
centroid is 3 standard deviations smaller than the mean sep-
aration of other classes. Thus, user’s cloaked images can be
detected.
However, a user can trivially overcome this detection by
maintaining separation between cloaked and target images
during cloak optimization. To show this, we use the same ex-
USENIX Association
29th USENIX Security Symposium    1601
perimental setup as in §5.2 but terminate the cloak optimiza-
tion once a cloaked image is 20% of the original L2 distance
from the target image. The cloak still achieves a 100% pro-
tection success rate, but the cloak/target separation remains
large enough to evade the previous detection method.
Anomaly Detection w/ Original Images. When the track-
ers have access to original training images (see §7), they
could use clustering to see if there are two distinct feature
clusters associated with the user’s images (i.e. cloaked and
uncloaked). Normal classes should have only one feature
cluster. To do this, the tracker could run a 2-means clustering
on each class’s feature space, ﬂagging classes with two dis-
tinct centroids as potentially cloaked. When we run this ex-
periment, we ﬁnd that the distance between the two centroids
of a protected user class is 3 standard deviations larger than
the average centroid separation in normal classes. In this way,
the tracker can use original images to detect the presence of
cloaked images.
To reduce the probability of detection by this method, the
user can choose a target class that does not create such a large
feature space separation. We empirically evaluate this mit-
igation strategy using the same experimental conﬁguration
as in §5.2 but choose a target label with average (rather than
maximal) distance from their class. The cloak generated with
this method still achieves a 100% protection success rate, but
L2 distance between the two cluster centroids is within 1 stan-
dard deviation of average.
The user can evade this anomaly detection strategy using
the maximum distance optimization strategy in §4. In prac-
tice, for any tracker model with a moderate number of la-
bels (>30), cloaks generated with average or maximum dif-
ference optimization consistently achieves high cloaking suc-
cess. Our experimental results show these two methods per-
form identically in protection success against both our local
models and the Face++ API.
9 Discussion and Conclusion
In this paper, we present a ﬁrst proposal to protect individu-
als from recognition by unauthorized and unaccountable fa-
cial recognition systems. Our approach applies small, care-
fully computed perturbations to cloak images, so that they are
shifted substantially in a recognition model’s feature repre-
sentation space, all while avoiding visible changes. Our tech-
niques work under a wide range of assumptions and provide
100% protection against widely used, state-of-the-art models
deployed by Microsoft, Amazon and Face++.
Like most privacy enhancing tools and technologies,
Fawkes can also be used by malicious bad actors. For exam-
ple, criminals could use Fawkes to hide their identity from
agencies that rely on third-party facial recognition systems
like Clearview.ai. We believe Fawkes will have the biggest
impact on those using public images to build unauthorized
facial recognition models and less so on agencies with legal
access to facial images such as federal agencies or law en-
forcement. We leave more detailed exploration of the trade-
off between user privacy and authorized use to future work.
Protecting content using cloaks faces the inherent chal-
lenge of being future-proof, since any technique we use to
cloak images today might be overcome by a workaround in
some future date, which would render previously protected
images vulnerable. While we are under no illusion that this
proposed system is itself future-proof, we believe it is an im-
portant and necessary ﬁrst step in the development of user-
centric privacy tools to resist unauthorized machine learning
models. We hope that followup work in this space will lead
to long-term protection mechanisms that prevent the mining
of personal content for user tracking and classiﬁcation.
Acknowledgments
We thank our shepherd David Evans and anonymous review-
ers for their constructive feedback. This work is supported
in part by NSF grants CNS-1949650, CNS-1923778, CNS-
1705042, and by the DARPA GARD program. Any opinions,
ﬁndings, and conclusions or recommendations expressed in
this material are those of the authors and do not necessarily
reﬂect the views of any funding agencies.
References
[1] http://apodeline.free.fr/DOC/libjpeg/libjpeg-3.
html. Using the IJG JPEG library: Advanced features.
[2] https://aws.amazon.com/rekognition/.
Amazon
Rekognition Face Veriﬁcation API.
[3] https://azure.microsoft.com/en-us/services/
cognitive-services/face/. Microsoft Azure Face API.
[4] https://www.faceplusplus.com/face-searching/.
Face++ Face Searching API.
[5] http://vision.seas.harvard.edu/pubfig83/.
Pub-
Fig83: A resource for studying face recognition in personal
photo collections.
[6] ABADI, M., CHU, A., GOODFELLOW, I., MCMAHAN,
H. B., MIRONOV, I., TALWAR, K., AND ZHANG, L. Deep
learning with differential privacy. In Proc. of CCS (2016).
[7] CAO, Q., SHEN, L., XIE, W., PARKHI, O. M., AND ZISSER-
MAN, A. VGGFace2: A dataset for recognising faces across
pose and age. In Proc. of IEEE FG (2018).
[8] CARLINI, N., AND WAGNER, D. Adversarial examples are
not easily detected: Bypassing ten detection methods. In Proc.
of AISec (2017).
[9] CARLINI, N., AND WAGNER, D. Towards evaluating the ro-
bustness of neural networks. In Proc. of IEEE S&P (2017).
[10] CARLINI, N., AND WAGNER, D. Towards evaluating the ro-
bustness of neural networks. In Proc. of IEEE S&P (2017).
1602    29th USENIX Security Symposium
USENIX Association
[11] CHEN, B., CARVALHO, W., BARACALDO, N., LUDWIG, H.,
EDWARDS, B., LEE, T., MOLLOY, I., AND SRIVASTAVA, B.
Detecting backdoor attacks on deep neural networks by acti-
vation clustering. arXiv:1811.03728 (2018).
[12] CHEN, Y., LI, H., TENG, S.-Y., NAGELS, S., LI, Z., LOPES,
P., ZHAO, B. Y., AND ZHENG, H. Wearable microphone jam-
ming. In Proc. of ACM CHI (April 2020).
[13] CROSS, J. Valley attorney: Facebook facial recognition car-
ries identity theft risk. KTAR News (September 2019).
[14] DEMONTIS, A., MELIS, M., PINTOR, M., JAGIELSKI, M.,
BIGGIO, B., OPREA, A., NITA-ROTARU, C., AND ROLI, F.
Why do adversarial attacks transfer? explaining transferability
of evasion and poisoning attacks. In Proc. of USENIX Security
(2019), pp. 321–338.
[15] DWORK, C. Differential privacy: A survey of results. In Proc.
of TAMC (2008).
[16] FEINMAN, R., CURTIN, R. R., SHINTRE, S., AND GARD-
NER, A. B. Detecting adversarial samples from artifacts.
arXiv:1703.00410 (2017).
[17] FREDRIKSON, M., JHA, S., AND RISTENPART, T. Model in-
version attacks that exploit conﬁdence information and basic
countermeasures. In Proc. of CCS (2015).
[18] GOODFELLOW,
I.
C.
arXiv:1412.6572 (2014).
SZEGEDY,
Explaining and harnessing adversarial examples.
J., SHLENS,
J., AND
[19] GUPTA, N., HUANG, W. R., FOWL, L., ZHU, C., FEIZI,
S., GOLDSTEIN, T., AND DICKERSON, J. P.
Strong
baseline defenses against clean-label poisoning attacks.
arXiv:1909.13374 (2019).
[20] HILL, K. The secretive company that might end privacy as
we know it. The New York Times (January 18 2020).
[21] HILL, K., AND KROLIK, A. How photos of your kids are
powering surveillance technology. The New York Times (Oc-
tober 11 2019).
[22] HUANG, G., LIU, Z., VAN DER MAATEN, L., AND WEIN-
BERGER, K. Q. Densely connected convolutional networks.
In Proc. of CVPR (2017).
[23] JAN, S. T., MESSOU, J., LIN, Y.-C., HUANG, J.-B., AND
WANG, G. Connecting the digital and physical world: Im-
proving the robustness of adversarial attacks. In Proc. of AAAI
(2019).
[24] KOMKOV, S., AND PETIUSHKO, A. Advhat: Real-world ad-
versarial attack on arcface face id system. arXiv:1908.08705
(2019).
[25] KURAKIN, A., GOODFELLOW, I., AND BENGIO, S. Adver-
arXiv:1607.02533
sarial examples in the physical world.
(2016).
[28] LI, Y., YANG, X., WU, B., AND LYU, S. Hiding faces in
plain sight: Disrupting AI face synthesis with adversarial per-
turbations. arXiv:1906.09288 (2019).
[29] LIU, Y., CHEN, X., LIU, C., AND SONG, D.
Delving
into transferable adversarial examples and black-box attacks.
arXiv:1611.02770 (2016).
[30] MADRY, A., MAKELOV, A., SCHMIDT, L., TSIPRAS, D.,
AND VLADU, A. Towards deep learning models resistant to
adversarial attacks. arXiv:1706.06083 (2017).
[31] MARI, A. Brazilian retailer quizzed over facial recognition
tech. ZDNet (March 2019).
[32] METZ, C., AND COLLINS, K. How an A.I. ‘cat-and-mouse
game’ generates believable fake photos. The New York Times
(January 2018).
[33] MOZUR, P. Inside China’s dystopian dreams: A.I., shame and
lots of cameras. The New York Times (July 2018).
[34] NECH, A., AND KEMELMACHER-SHLIZERMAN, I. Level
In Proc. of
playing ﬁeld for million scale face recognition.
CVPR (2017).
[35] NEWTON, E. M., SWEENEY, L., AND MALIN, B. Preserving
privacy by de-identifying face images. IEEE transactions on
Knowledge and Data Engineering 17, 2 (2005), 232–243.
[36] NG, H.-W., AND WINKLER, S. A data-driven approach to
cleaning large face datasets. In Proc. of ICIP (2014).
[37] NOCEDAL, J., AND WRIGHT, S. Numerical optimization, se-
ries in operations research and ﬁnancial engineering. Springer,
New York, USA, 2006 (2006).
[38] O’FLAHERTY, K. Facial recognition at u.s. airports. should
you be concerned? Forbes (March 2019).
[39] PAPERNOT, N., MCDANIEL, P., AND GOODFELLOW,
I.
Transferability in machine learning: From phe-
nomena to black-box attacks using adversarial samples.
arXiv:1605.07277 (2016).
[40] PAUDICE, A., MUÑOZ-GONZÁLEZ, L., GYORGY, A., AND
LUPU, E. C.
training ex-
amples in poisoning attacks through anomaly detection.
arXiv:1802.03041 (2018).
Detection of adversarial
[41] SATARIANO, A. Police use of facial recognition is accepted
by British court. The New York Times (September 2019).
[42] SHAFAHI, A., HUANG, W. R., NAJIBI, M., SUCIU, O.,
STUDER, C., DUMITRAS, T., AND GOLDSTEIN, T. Poison
frogs! targeted clean-label poisoning attacks on neural net-
works. In Proc. of NeurIPS (2018).
[43] SHAN, S., WENGER, E., WANG, B., LI, B., ZHENG, H.,
AND ZHAO, B. Y. Gotta catch ’em all: Using honeypots to
catch adversarial attacks on neural networks. In Proc. of CCS
(Orlando, FL, November 2019). arXiv:1904.08554.
[26] LEE, N. Having multiple online identities is more normal than
you think. Engadget, March 2016. https://www.engadget.
com/2016/03/04/multiple-online-identities.
[44] SHARIF, M., BHAGAVATULA, S., BAUER, L., AND REITER,
M. K. Accessorize to a crime: Real and stealthy attacks on
state-of-the-art face recognition. In Proc. of CCS (2016).
[27] LI, T., AND LIN, L.
identiﬁcation with measurable privacy.
(2019).
Anonymousnet: Natural face de-
In Proc. of CVPR
[45] SHEN, J., ZHU, X., AND MA, D. Tensorclog: An impercep-
tible poisoning attack on deep neural network applications.
IEEE Access 7 (2019), 41498–41506.
USENIX Association
29th USENIX Security Symposium    1603
[46] SHEN, S., TOPLE, S., AND SAXENA, P. Auror: Defending
against poisoning attacks in collaborative deep learning sys-
tems. In Proc. of ACSAC (2016).
[47] SHWAYDER, M. Clearview AI’s facial-recognition app is a
nightmare for stalking victims. Digital Trends (January 2020).
[48] SONG, C., RISTENPART, T., AND SHMATIKOV, V. Machine
In Proc. of CCS
learning models that remember too much.
(2017).
[49] STEINHARDT, J., KOH, P. W. W., AND LIANG, P. S. Certi-
ﬁed defenses for data poisoning attacks. In Proc. of NeurIPS
(2017).
[50] SUCIU, O., M ˘ARGINEAN, R., KAYA, Y., DAUMÉ III, H.,
AND DUMITRA ¸S, T. When does machine learning fail? gen-
eralized transferability for evasion and poisoning attacks. In
Proc. of USENIX Security (2018).
[51] SUN, Q., MA, L., JOON OH, S., VAN GOOL, L., SCHIELE,
B., AND FRITZ, M. Natural and effective obfuscation by head
inpainting. In Proc. of CVPR (2018).
[52] SUN, Q., TEWARI, A., XU, W., FRITZ, M., THEOBALT, C.,
AND SCHIELE, B. A hybrid model for identity obfuscation by
face replacement. In Proc. of ECCV (2018).
[53] SZEGEDY, C., IOFFE, S., VANHOUCKE, V., AND ALEMI,
A. A. Inception-v4, inception-resnet and the impact of resid-
ual connections on learning. In Proc. of AAAI (2017).
[54] SZEGEDY, C., ZAREMBA, W., SUTSKEVER, I., BRUNA, J.,
ERHAN, D., GOODFELLOW, I., AND FERGUS, R. Intriguing
properties of neural networks. arXiv:1312.6199 (2013).
[55] THYS, S., VAN RANST, W., AND GOEDEMÉ, T. Fooling au-
tomated surveillance cameras: adversarial patches to attack
person detection. In Proc. of CVPR (workshop) (2019).
[56] TSIPRAS, D., SANTURKAR, S., ENGSTROM, L., TURNER,
A., AND MADRY, A. Robustness may be at odds with ac-
curacy. arXiv:1805.12152 (2018).
[57] WALLACE, G. K. The JPEG still picture compression stan-
IEEE Transactions on Consumer Electronics 38, 1
dard.
(1992).
[58] WANG, B., YAO, Y., SHAN, S., LI, H., VISWANATH, B.,
ZHENG, H., AND ZHAO, B. Y. Neural cleanse: Identifying
and mitigating backdoor attacks in neural networks. In Proc.
of IEEE S&P (2019).
[59] WANG, B., YAO, Y., VISWANATH, B., ZHENG, H., AND
ZHAO, B. Y. With great training comes great vulnerability:
Practical attacks against transfer learning. In Proc. of USENIX
Security (2018).
[60] WANG, G., KONOLIGE, T., WILSON, C., WANG, X.,
ZHENG, H., AND ZHAO, B. Y. You are how you click:
Clickstream analysis for sybil detection. In Proc. of USENIX
Security (2013), pp. 241–256.
[61] WANG, Z., BOVIK, A. C., SHEIKH, H. R., AND SIMON-
CELLI, E. P. Image quality assessment: From error visibility
to structural similarity. IEEE Trans. on Image Processing 13,
4 (2004), 600–612.
[62] WANG, Z., SIMONCELLI, E. P., AND BOVIK, A. C. Mul-
tiscale structural similarity for image quality assessment. In
Proc. of Asilomar Conference on Signals, Systems & Comput-
ers (2003), vol. 2, IEEE, pp. 1398–1402.
[63] WONDRACEK, G., HOLZ, T., KIRDA, E., AND KRUEGEL, C.
A practical attack to de-anonymize social network users. In
Proc. of IEEE S&P (2010).
[64] WU, Y., YANG, F., AND LING, H. Privacy-Protective-GAN
for face de-identiﬁcation. arXiv:1806.08906 (2018).
[65] WU, Z., LIM, S.-N., DAVIS, L., AND GOLDSTEIN, T. Mak-
ing an invisibility cloak: Real world adversarial attacks on ob-
ject detectors. arXiv:1910.14667 (2019).
[66] YANG, C., WU, Q., LI, H., AND CHEN, Y.
Gener-
ative poisoning attack method against neural networks.
arXiv:1703.01340 (2017).
[67] YANG, Z., WILSON, C., WANG, X., GAO, T., ZHAO, B. Y.,
AND DAI, Y.
Uncovering social network sybils in the
wild. ACM Transactions on Knowledge Discovery from Data
(TKDD) 8, 1 (2014), 1–29.
[68] YANG, Z., ZHANG, J., CHANG, E.-C., AND LIANG, Z. Neu-
ral network inversion in adversarial setting via background
knowledge alignment. In Proc. of CCS (London, UK, Novem-
ber 2019).
[69] YI, D., LEI, Z., LIAO, S., AND LI, S. Z. Learning face rep-
resentation from scratch. arXiv:1411.7923 (2014).
[70] YOSINSKI, J., CLUNE, J., BENGIO, Y., AND LIPSON, H.
How transferable are features in deep neural networks? In
Proc. of NeurIPS (2014).
[71] ZHU, C., HUANG, W. R., SHAFAHI, A., LI, H., TAYLOR,
G., STUDER, C., AND GOLDSTEIN, T. Transferable clean-
label poisoning attacks on deep neural nets. In Proc. of ICML
(2019).
1604    29th USENIX Security Symposium
USENIX Association