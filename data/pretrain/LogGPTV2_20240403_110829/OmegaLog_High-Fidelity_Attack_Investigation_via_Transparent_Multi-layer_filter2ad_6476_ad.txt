on accuracy and completeness of Angr’s recovered CFG, we
refer the reader to [53]. In brief, if Angr mistakenly adds
an edge that should not be in the CFG of an application,
OmegaLog will generate an erroneous LMS path in the LMS
path database. However, since that execution path will never
happen during runtime, OmegaLog will just ignore this false
positive LMS path during UPG construction. In case Angr
misses an edge in a CFG, we have implemented Lookahead
and Lookback matching (described in §VIII), which handle
this case.
performance was
static
impacted
OmegaLog’s
signiﬁcantly
Runtime Performance.
analysis
runtime
by
Angr’s performance of symbolic execution. We introduced
PeepholeConcretization to
runtime while
preserving the accuracy of LMS path recovery. Note that
static analysis is a one-time, ofﬂine cost: once a binary has
been proﬁled, there is no need to re-analyze it unless it has
been changed. On modestly provisioned workstations,
that
task could even be outsourced to more powerful machines.
improve
Binary Restrictions.
First, Angr tool can only work on
binaries compiled from C/C++ code. Second,
the format
modiﬁer argument to a logging procedure should not be built
dynamically at runtime as an element of a struct, i.e., it
should be a constant string. Third, our binary analysis can only
recover logging functions that are not inlined. However, we did
not encounter inlined logging functions during our evaluation.
VII. OMEGALOG: RUNTIME PHASE
At runtime, OmegaLog performs minimal maintenance of
application and whole-system logs;
the LMS control ﬂow
path models are stored in a database ( 2 in Fig. 5) and are
not consulted until an investigation is initiated. The primary
runtime challenge for OmegaLog is that of reconciling logs
from different layers, which is difﬁcult when considering a
ﬂattened event log of concurrent activities in multi-threaded
applications. To address that, OmegaLog intercepts all write
syscalls on the host using a kernel module and identiﬁes which
write syscalls belong to application event logging using heuris-
tics discussed in §VI. After that it only appends the PID/TID
of the process/thread that emitted the event and along with
the timestamp of the event’s occurrence to the identiﬁed log
messages, generating enhanced event log messages.2 Finally,
OmegaLog uses Linux Audit API to add the enhanced event
log message to the whole-system provenance log ﬁle, which
provides an ordering for both application- and system-level
events.
VIII. OMEGALOG: INVESTIGATION PHASE
Following an attack, an administrator can query Omega-
Log’s log parser and graph generator modules ( 5 in Fig. 5)
to construct a UPG chronicling the system- and application-
layer events related to the intrusion.
2Applications that make use of rsyslog facility [8] to write LMS is the one
exception to the rule where LMS writing process’s PID is not equal to the
original application process that produced the LMS. However, in such case
we can easily extract the PID/TID of original application process because
rsyslog use well-deﬁned message format [27] with PID added by default.
9
log(“Server started”); // log1while(...) { log(“Accepted Connection”); // log2 ... /*Handle request here*/ log(“Closed Connection”); // log3}log(“Server stopped”); // log4log4log1log2log3log4log1Algorithm 2: UPG Construction
Inputs
: Universal log ﬁle Luni;
Symptom event es;
LMS control ﬂow paths P athslms;
Output
Variables: LM Sstate ← Current state of LMS;
: Backward universal provenance graph G
eventU nit[P id] ← events in current unit related to P id;
endU nit ← ﬂag to partition execution into unit;
endU nit ← f alse
foreach event e ∈ Luni happened before es do
if ISAPPENTRY(e) then
LM Scand = GETLMSREGEX(e)
endU nit = MATCHLMS(LM Scand, P athslms,
LM Sstate, eventUnit[P ide], Luni)
end
else
end
end
return G
end
if endU nit then
eventUnit[P ide].add(e)
Add all events from eventUnit[P ide] to G
endU nit ← f alse
eventUnit[P ide] ← null
eventUnit[P ide].add(e)
password from user .* accepted” with a ranking of 5,
which is equal to the number of non-regex word matches.
Finally, the matcher will return the LMS that has the highest
rank or the highest number of non-regex word matches that
reﬂects the true state among the candidate LMSes.
the
Once
State Machine Matching.
candidate LMS
(LM Scand) has been identiﬁed for an application log entry,
OmegaLog attempts to match the LM Scand to a valid LMS
path in the database. If this is the ﬁrst event message, we
use a set of heuristics to ﬁgure out where we should start
from. However, since the matching process can start anywhere
in the applications lifetime, usually we have to resort to an
exhaustive search over all nodes in the LMS control ﬂow paths.
Once we identiﬁed the starting node, we keep state in the
parser that points to the possible transitions in the LMS paths
graph. Upon the next log entry, we search the neighbors of the
previous LMS for possible candidate matches. We rank those
and return the one with the highest rank, and then advance
the parser’s state pointer. If OmegaLog cannot ﬁnd a match in
the neighboring LMS states, it advances to the lookahead and
lookback matching steps.
A. Universal Provenance
Given application binaries, whole-system provenance logs,
and application event logs, during the investigation phase, we
aim to generate a UPG while preserving the three properties
of causality analysis. Algorithm 2 describes how to construct
the backward-tracing UPG from the universal log ﬁle, specif-
ically a backtrace query from an observable attack symptom
event; the approach to building forward-trace graph follows
naturally from this algorithm and is therefore omitted. When
an application event log (an augmented LMS) is encountered
while parsing the universal log (Function ISAPPENTRY in
Algorithm 2), it is necessary to match the event to a known
LMS for the application in our LMS paths. That matching is
performed by the MATCHLMS function as described below.
B. LMS State Matching
This procedure entails matching of a given runtime appli-
cation log entry to its associated LMS in the LMS control
ﬂow paths DB. For each log entry in the universal log, the
matcher identiﬁes all LMS regexes that are candidate matches.
For example, if the event message is
02/15/19 sshd [PID]: PAM: password from user root accepted
the matcher will look for substring matches, and this will solve
the issue of identifying the actual application log entry from
the preamble metadata, e.g., “02/15/19 sshd[PID]:”.
Ranking LMS.
An application log entry may match to
multiple LMS regexes in the LMS path DB; this happens
because of the prevalence of the %s format speciﬁer in LMS,
which can match anything. Therefore, OmegaLog performs a
ranking of all the possible candidate matches. We use regex
matching to identify the number of non-regex expressions (i.e.
constants) in each match. Going back to the example, “PAM:
password from user root accepted” will match “PAM:
10
Lookahead Matching. When the previous state in the LMS
path is known, we may not ﬁnd a match in a neighboring
LMS state because for example (1) the application is running
at a different
log level, (2) OmegaLog missed the LMS
corresponding to the log message in the static analysis phase
(for example, the function might be inlined, or we could not
concretize its values), or (3) the log message is coming from
a third-party library. We therefore start looking deeper into
the reachable states from the current parser state. If we ﬁnd
multiple candidates, we again rank them and return the one
with the highest rank. If we do not ﬁnd one, we then keep
increasing the lookahead up until we hit a certain threshold
that can be set at runtime. If we ﬁnd a match, we move the
parser to that state and repeat until we match a candidate LMS
at the end of LMS control ﬂow path. At that point, we set the
endU nit ﬂag to true.
As described in §VI, in certain cases LMS may not be able
to correctly partition the execution because there are syscalls
after the loop-ending LMS or syscalls before loop-starting
LMS. During ofﬂine analysis, OmegaLog marks such LMS
and keep track of any syscalls that we should expect during
runtime. If we observe such case during state matching pro-
cess, we match those syscalls besides matching LMS and add
those syscalls into the execution unit. Function MATCHLMS
in Algorithm 2 also handles such cases and appropriately sets
the endU nit ﬂag to true.
Lookback Matching.
If the above lookahead step fails be-
cause we cannot ﬁnd the end state in the LMS path, then
we ﬁrst try to search the heads of loops that are of the form
(while(1), for(;;)) in the LMS control ﬂow path. The
intuition behind loop head identiﬁcation step is that we might
have hit the start of a new execution unit and thus we would
need to restart from a new stage. If this fails, then we perform
an exhaustive search of LMS that can happen before the current
state in the LMS paths using the same intuition mentioned
before. If in either case, we get a match we set the endU nit
ﬂag to true. Note that fallback matching allows us to generate
execution units even if we have only one log message at start
or end of the loop, because we use the next execution unit’s
log message to partition the current execution unit.
IX. EVALUATION
In this section, we evaluate OmegaLog to answer the
following research questions (RQs):
RQ1: What is the cost of OmegaLog’s static analysis routines
when extracting logging information from binaries?
RQ2: How complete is our binary analysis in terms of ﬁnding
all the LMSes in an application?
RQ3: What time and space overheads does OmegaLog impose
at runtime, relative to a typical logging baseline?
RQ4: Is the universal provenance graph causally correct?
RQ5: How effective is OmegaLog at reconstructing attacks,
relative to a typical causal analysis baseline?
Experimental Setup. We evaluated our approach against 18
real-world applications. We selected these applications from
our pool of applications discussed in §IV based on popularity
and category. Moreover, most of these applications were used
in the evaluation of prior work on provenance tracking [42],
[41], [38], [39]. For each program, we proﬁle two verbosity
levels, INFO and DEBUG, when considering the above research
questions. Workloads were generated for the applications in
our dataset using the standard benchmarking tools such as
Apache Benchmark ab [1] and FTPbench [2].
All tests were conducted on a server-class machine with an
Intel Core(TM) i7-6700 CPU @ 3.40 GHz and 32 GB of mem-
ory, running Ubuntu 16.04. To collect whole-system prove-
nance logs we used Linux Audit Module3 with the following
syscall ruleset: clone, close, creat, dup, dup2, dup3, execve,
exit, exit group, fork, open, openat, rename, renameat, unlink,
unlinkat, vfork, connect, accept, accept4, bind. OmegaLog’s
ofﬂine algorithm accepts a single conﬁguration parameter,
maxBackTrace, that sets the maximum depth of symbolic
execution operations. After experimenting with that parameter,
we found that a value of 5 was enough to guarantee >95%
coverage for 12 of the 18 applications we analyzed, as we
discuss in the following section. In fact, our experiments have
shown that we did not need to increase the symbolic execution
depth beyond 3 basic blocks.
A. Static Analysis Performance
Table III shows how much time it takes to identify and
concretize LMS from application binaries and subsequently
generate LMS path models (Algorithm 1). We ﬁrst note that
the overhead of building the LMS paths (LMSPs) is reasonable
for a one-time cost, taking 1–8 seconds for most applications,
with a maximum of 3 minutes for PostgreSQL; the increase
for PostgreSQL is due to the larger number of LMS paths
captured by OmegaLog. On the other hand, average time to
generate an LMS column shows the time to generate the
FastCFG and concretize the LMS dominates OmegaLog’s
static analysis tasks, ranging from a minimum of a minute
and a half (Transmission) to a maximum of 1.2 hours
(PostgreSQL). Those two tasks are in fact highly dependent
3We make use of the Linux Audit framework in our implementation.
However, our results are generalizable to other system logs, such as Windows
ETW [4] and FreeBSD DTrace [3].
on Angr’s raw performance. As acknowledged by the Angr
tool developers [11],
the static analyzer’s performance is
handicapped because it is written in the Python language with
no ofﬁcial support for parallel execution.
Our results show no direct relationship between the size of
the binary of the application being analyzed and the overall
analysis time. By inspecting the applications’ source code,
we found that OmegaLog’s performance is more informed by
the structure of the code and the logging procedures. We can
see intuitively that as the number of found callsites increases,
the number of peephole symbolic execution steps needed
also increases, thus increasing the total concretization time.
However, that does not generalize to all the applications; for
example, the analysis of NGINX (2044 KB binary) completed
in 13 minutes concretizing 925 LMS while Lighttpd (1212
KB, almost half of NGINX’s binary size) required 32 minutes
concretizing only 358 LMSes.
Upon closer investigation of Lighttpd’s source code, we
found that format speciﬁers (and thus LMS) were often passed
as structure members rather than as constant strings (which
form the majority of LMS in the case of NGINX). That will trig-
ger the backtracing behavior of the PEEPHOLECONCRETIZATION
algorithm in an attempt to concretize the values of the struct
members, thus increasing the cost of the symbolic execution
operations performed by Angr. Below we show sample code
snippets from Lighttpd that trigger such behavior:
/∗ log function signature:
log error write(server ∗srv, const char ∗ﬁlename, unsigned int
int
/∗ format speciﬁer passed as struct member: /src/conﬁg−glue.c ∗/
if
const char ∗fmt /∗ our tool
looks for fmt ∗/,
/src/log.c ∗/
(con−>conf.log condition handling) {
log error write(srv,
FILE ,
LINE , ”dss”,
line ,
...)
dc−>context ndx, /∗ the fmt argument ∗/
”(cached) result: ” ,
cond result
to string(caches[dc−>context ndx].result)); }
The cases of Lighttpd and NGINX highlight the unpre-
dictability of runtime of OmegaLog’s static analysis when
only the binary size or the number of identiﬁed callsites is
considered. Rather, the runtime depends on the structure of
the code and the anatomy of the calls to the log functions.
B. Static Analysis Completeness
We report on OmegaLog’s coverage ratio, which represents
the percent of concretized LMS relative to the count of
identiﬁed callsites to logging procedures. As shown in the
last column of Table III, OmegaLog’s coverage is > 95%
for all the applications except PostgreSQL, Transmission,
and wget. We disregard thttpd since it presents a small
sample size in terms of LMS where OmegaLog only missed
1 LMS during concretization. That speaks to OmegaLog’s
ability to consistently obtain most of the required LMSes
and build their corresponding LMS control ﬂow paths. We
show in our experiments, this coverage ratio is sufﬁcient to
enable OmegaLog to perform execution partitioning and aid
the investigation process without loss of precision. In addition,
when LMSes are missing, OmegaLog’s runtime parser can
handle missing log messages through lookahead and lookback
techniques. If OmegaLog fails to concretize an LMS, it is a
reﬂection of the symbolic execution task’s ability to resolve a
format speciﬁer for a logging procedure.
11
TABLE III: Application logging behavior and performance results of OmegaLog’s static analysis phase. EHL stands for event handling loop;
IN+DE means that both INFO and DEBUG verbosity levels are present in the loop; LMSPs: Log message string paths; Callsites are identiﬁed
log statements; and “Cov. %” denotes coverage percentage which is the percentage of concretized LMS to callsites.
Program
Squid
PostgreSQL
Redis
HAProxy
ntpd
OpenSSH
NGINX
Httpd
Proftpd
Lighttpd