update the seqNo of the source. ∆ is an additional short delay that
is used to ensure that the router does not increment the source’s
seqNo faster than the source does. If a router were to increment the
source’s seqNo faster, then it could be that after a long time period
packets from the source AS would be dropped.
In addition, R performs one more task (not described in Algo-
rithm 1). It periodically decrements T T LS for each AS; if T T LS
becomes zero, the seqNo of the corresponding AS (SNR
S ) is incre-
mented and T T LS is reset to T + ∆. This implements the count-
down timer that is used for the self-updates of the sources’ seqNos.
4.3 Optimization Problem
In this section, we formulate an optimization problem that involves
the inter-related parameters of our mechanism. An appropriate con-
ﬁguration of the parameters and especially of the BFs is crucial to
guarantee a high forwarding performance for the routers. We de-
scribe performance as a function of all the involved parameters:
f (m, k, N, L, M). Then, we derive constraints between the parame-
ters, which gives us the following optimization problem:
minimize
f (m, k, N, L, M)
subject to M >
N >
m >
σ
+ 1,
T
1.1(T + σ)
L
+ 1,
−krL
(1)
(2)
(3)
(4)
,
1
N )
1
k )
ln (1 − (1 − (1 − fp)
m, k, N, L, M ∈ Z+
Sequence-number update interval (T ). T represents the time pe-
riod for which a seqNo is used, before it is incremented. We con-
sider values on the order of a few milliseconds (e.g., 10ms), and we
show that it leads to an efﬁcient implementation.
Additional delay window (∆). ∆ is a delay period that is used to
slow down the update rate of router’s R view for the seqNo of AS
S (SNR
S ). It ensures that R does not increment its seqNo faster than
S, i.e., does not increment at an interval shorter than T .
The main reason for an early update is the clock drift between S
and R. We deﬁne ∆ with respect to T , since the amount of clock
drift is proportional to the time period under consideration; we are
interested in estimating the seqNo-update inaccuracy for SNS and
SNR
S by S and R respectively. We conservatively assume that the
clock drift is lower than 0.05 · T ; Marouani et al., report that a
Sequence-number window: M
ti
ti+1
ti+M-1
ti+M
time +
ti+1-ε1
ti+1-ε1+σ
 j. Then, consider a router that accepts packets in
the window [i, i + M − 1]. To ensure that a legitimate packet is not
dropped due to packet reordering, the last packet with a seqNo of
i should arrive at R before the ﬁrst packet with a seqNo of i + M
(Figure 5). In the worst case, the last packet with seqNo i is sent
at ti+1 − ε1 and received at ti+1 − ε1 + σ, where ε1 is an arbitrarily
small positive constant. The ﬁrst packet with seqNo i + M arrives at
R no earlier than ti+M + ε2, where ε2 is an arbitrarily small positive
constant. Mathematically, the relation between the packet-arrival
times can be described as ti+1 − ε1 + σ  M ·(T + ∆). We obtain that N > M ·(T + ∆)/L+1
ﬁlters are required; the additional ﬁlter is necessary to store the
incoming packets at the current time interval. We combine the
formed inequality with Equation 1 and by setting ∆ = 0.1 · T we
obtain Equation 2.
Equation 3 describes the size m of each BF as a function of the
BF rotation interval L, the number N of BFs, the number k of nec-
essary hash functions, and the BF’s target false-positive rate (fp).
Since an incoming packet is checked against all BFs, the overall
target false-positive rate is 1 − (1 − fp)N . To determine the value
for fp, we consider the average number of packets that a router re-
ceives in an interval L (which is r · L, where r is the incoming packet
rate). Using the BF equations, we get fp = (1 − ek·x·L/m)k and by
combining it with the equation for the size of a BF, we obtain Equa-
tion 3. The inequality indicates that any larger value for m yields a
lower false-positive than fp.
The formulated optimization problem is an integer programming
problem, which is known to be NP-hard [46]. Note that also the ro-
3In the worst case, a router does not receive seqNo updates from
the source and self-increments the seqNo every T + ∆.
Parameter
Value
Parameter
T
r
σ
M
10 ms
14.88 Mpps
100 ms
11
m
k
N
L
Value
8 MB
11
2
121 ms
Table 2: Software-router Implementation
tation interval L is an integer: a time period in a computing system
is expressed as a multiple of some minimum supported time gran-
ularity (e.g., 1 ns); in practice, we will use values at the order of
1 ms (Section 5). In our context, we obtain multiple solutions to
the problem by searching a constrained parameter space; for exam-
ple, we constrain the size of the BF to be less than 20 MB, since
ideally it should ﬁt into a processor’s cache. Our grid search is per-
formed as follows: 10 ms ≤ L ≤ 200 ms, 2 ≤ N ≤ 20, 2 ≤ k ≤ 30,
1 MB ≤ m ≤ 20 MB.
Furthermore, the objective of the optimization problem changes
depending on the implementation platform (e.g., software vs. hardware-
based implementation). In Section 5.1, we adapt the optimization
problem to a software router and show how a selection of carefully
chosen parameters leads to an efﬁcient implementation.
5. SOFTWARE PROTOTYPE
To demonstrate the practicality of our approach, we implement the
proposed replay-suppression mechanism on a software router. Our
evaluation focuses on the overhead of replay suppression and not
other functionalities (e.g., source authentication or longest preﬁx
matching). Thus, we do not consider a speciﬁc underlying network
architecture, but we make the following generic assumptions:
• Every packet injected into the network by a host has a unique
network-layer identiﬁer. For example, the IP-ID ﬁeld in IPv4 is
implemented by most operating systems as a packet counter [33].
We use this identiﬁer together with the immutable content of a
packet to uniquely identify the packet and minimize the proba-
bility of a collision.
• A router can obtain the AS number (ASN) of the source-host for
every packet. For example, certain network architectures express
addresses as a (ASN : hostID) tuple [16, 17]; or an IP forwarding
information base (FIB) can be extended to include this informa-
tion for every source-address preﬁx.
5.1 Implementation
The focus of our implementation is to optimize memory-access
patterns. Since our solution is a memory-intensive application4,
forwarding performance depends mostly on cache efﬁciency, i.e.,
it depends on the memory footprint of the application and on the
memory access patterns. Small data structures are more likely to ﬁt
in the cache and, thus, reduce the importance of the access pattern.
However, in a software implementation the cache is shared with
other processes and a small memory footprint does not guarantee
optimal performance. Thus, we focus on minimizing cache misses.
To minimize cache misses, we use a blocked BF [47] instead
of a standard BF. A blocked BF consists of multiple standard BFs
(called blocks), each of which ﬁts into the typical 64-byte cache
line. For each element that is checked/inserted, the ﬁrst hash value
determines the block to be used and additional hash values deter-
mine which bits to check/set in the block. Thus, a blocked BF
needs one cache miss for every operation in the worst case. This
optimization comes at the cost of a larger memory footprint com-
pared to a standard BF with the same false-positive rate.
The next step to minimize cache misses is to minimize the num-
ber of blocked BFs. Recall from the protocol description (Sec-
tion 4.2) that for every observed packet, we add it to the writeable
BF and check for its presence in all other ﬁlters. Since blocked BFs
may have one cache miss per checked/inserted element, we want to
minimize the number of ﬁlters.
We solve the optimization problem (Section 4.3) with the objec-
tive of minimizing the number of BFs. To account for the worst
case, we assume a packet rate of 14.88 Mpps, which is the theoret-
ically maximum packet rate for a 10 GbE Network Interface Card.
Also, we set a conservative value for the maximum latency varia-
tion σ to 100 ms, based on a recent latency-measurement study [48].
We target for an overall false-positive rate that is less than 5 · 10−6,
and we obtain multiple solutions that use N = 2 BFs (which is also
the lowest possible value according to Equation 2). Speciﬁcally, we
obtain solutions that have different ﬁlter sizes (m), different seqNo
window lengths (M), and that rotate BFs at different time intervals
(L). From these solutions we choose the one that has the smallest
memory footprint (lowest m value), under the constraint that the
ﬁlter size is a power of 2. This constraint provides a signiﬁcant
processing speedup, as heavily used computations are transformed
to bitwise operations (e.g., modulo operations become bit-shifts).
Table 2 summarizes all the parameters of our solution.
Furthermore, to check/insert elements in the BF, we need to ob-
tain the pointers to the corresponding bits in the ﬁlter. To imple-
ment the keyed PRF (Section 4.2), we compute an AES based
CBC-MAC over a ﬁxed length of the ﬁrst bytes of a packet, as a
CBC-MAC is insecure for variable-length messages [49]. Also,
from our analysis of CAIDA traces [50], we found that the ﬁrst 48
bytes of a packet’s content are sufﬁcient to mitigate digest colli-
sions; the same result has been reported by previous work [51]. We
split the 16-byte output of the MAC into appropriately sized chunks
so that the ﬁrst chunk points to the 512-bit block and the remaining
chunks point to the bits in the block.
The last required functionality is the FIB. The FIB holds for
S and the count-down timer T T LS; we
every AS S the seqNo SNR
decrement the TTL value every 1 ms.
Optimizations. We leverage the Data Plane Development Kit [52]
and Intel AES-NI [53] to build our prototype, and we perform the
following optimizations to the BF. To insert an element, we lever-
age 128-bit registers and an SSE OR instruction: we prepare the
inserting element by setting the respective bits obtained from the
MAC computation. Then, we set the required bits in the 512-bit
block with four 128-bit SSE OR operations. To check for member-
ship of an element, we use early exit, i.e., as soon as we discover
an unset bit we know the element is not a duplicate. This results in
better performance since false positives are low and it is very likely
to discover unset bits early.
5.2 Evaluation
We evaluate the switching performance of our software router on
a commodity server equipped with an Intel Xeon E5-2680 CPU
(20 MB L3 cache), 32 GB DDR3 RAM, and a 10 GbE Network In-
terface Card (NIC). We dedicate only two cores of the CPU to per-
form all required processing: one core processes incoming packets,
and the other core updates the TTL values and seqNos in the FIB.
We utilize Spirent SPR-N4U-220 as our packet generator to gen-
erate load on the router; the router processes the generated trafﬁc
and sends it back to the generator. We generate a FIB with 55k
ASNs, and use random destination addresses to avoid spatiotempo-
ral locality for FIB cache accesses.
4For each packet, k · N bits are accessed in the BFs; k bits for each
one of the N BFs.
First, we test the forwarding performance of one port for two
packet sizes (64 and 128 bytes) and a representative mixture of In-
s
p
b
G
7
7
.
s
p
b
G
7
5
.
s
p
b
G
6
8
.
s
p
b
G
6
8
.
Line Rate
9.5 
Gbps
9.5 
Gbps
iMIX
Figure 6: Forwarding performance for packet sizes of 64 and
128 bytes and for iMIX.
Figure 7: Average, minimum, and maximum packet latencies
for packet sizes of 64 and 128 bytes and for iMIX.
ternet packet sizes (iMIX) [54]. Minimum-sized packets, 64 bytes,
translate to the highest possible packet rate and are the worst case;
we refer to the highest packet rate for each test case as the line-rate
performance. The baseline for the experiments is the forwarding
performance without any packet processing. Figure 6 shows the
forwarding performance we obtain. The results show a 25% de-
crease for minimum-sized packets; and that for longer packet sizes,
i.e., lower packet rates, optimal performance is achieved.
Next, we measure the latency overhead of our implementation
(Figure 7). We observe a two-fold increase in average latency only
for minimum-sized packets. The average latency and latency range
is almost identical for the other two test cases.
We observe a performance degradation, both for throughput and
latency, for minimum-sized packets. This performance degradation
is attributed to the penalty of cache misses and the overhead of the
MAC computation when the router is subjected to the maximum
load. We emphasize that a 10 GbE link, fully utilized with 64-byte
packets is far from a realistic workload. For a more realistic work-
load with iMIX, which has an average packet size of 417 bytes, our
implementation saturates line-rate.
6. SECURITY CONSIDERATIONS
6.1 Deployment Location and Topology
We discuss certain security issues that depend on the location of
routers that deploy replay suppression and on the network topology.
In Section 4, we mentioned that routers which deploy replay sup-
pression are located at the borders of ASes. This deployment model
raises certain security issues, which are mitigated if more routers
inside an AS deploy the protocol.
Packet replays create an attack surface, which includes the path
segments between the malicious router and the ﬁrst deploying router
that will drop the replayed packets. If deploying routers are located
only at AS borders, then the attack surface is limited to a single
AS. If more routers inside an AS deploy the protocol, then the at-
tack surface is further reduced. For example, ASes could deploy
more such routers near routing bottlenecks.
Furthermore, a malicious router can strategically replay packets
even against deploying routers: since replay suppression is done by