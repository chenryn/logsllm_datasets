the new directory is different in the directory portion of the file ID. ReFS solves the problem by replac-
ing the original file ID entry, located in the old directory B+ tree, with a new “tombstone” entry, which, 
instead of specifying the target file name in its value, contains the new assigned ID of the renamed file 
(with both the directory and the file portion changed). Another new File ID entry is also allocated in the 
new directory B+ tree, which allows assigning the new local file ID to the renamed file. If the file is then 
moved to yet another directory, the second directory has its ID entry deleted because it’s no longer 
needed; one tombstone, at most, is present for any given file.
Security and change journal 
The mechanics of supporting Windows object security in the file system lie mostly in the higher com-
ponents that are implemented by the portions of the file system remained unchanged since NTFS. The 
underlying on-disk implementation has been changed to support the same set of semantics. In ReFS, 
object security descriptors are stored in the volume’s global security directory B+ table. A hash is com-
puted for every security descriptor in the table (using a proprietary algorithm, which operates only on 
self-relative security descriptors), and an ID is assigned to each. 
When the system attaches a new security descriptor to a file, the ReFS driver calculates the secu-
rity descriptor’s hash and checks whether it’s already present in the global security table. If the hash is 
present in the table, ReFS resolves its ID and stores it in the STANDARD_INFORMATION data structure 
located in the embedded root node of the file’s B+ tree. In case the hash does not already exist in the 
global security table, ReFS executes a similar procedure but first adds the new security descriptor in the 
global B+ tree and generates its new ID.
The rows of the global security table are of the format hash ID security descriptor ref. count,
where the hash and the ID are as described earlier, the security descriptor is the raw byte payload of 
the security descriptor itself, and ref. count is a rough estimate of how many objects on the volume are 
using the security descriptor.
As described in the previous section, NTFS implements a change journal feature, which provides ap-
plications and services with the ability to query past changes to files within a volume. ReFS implements 
an NTFS-compatible change journal implemented in a slightly different way. The ReFS journal stores 
change entries in the change journal file located in another volume’s global Minstore B+ tree, the 
metadata directory table. ReFS opens and parses the volume’s change journal file only once the vol-
ume is mounted. The maximum size of the journal is stored in the USN_MAX attribute of the journal 
754
CHAPTER 11
Caching and file systems
file. In ReFS, each file and directory contains its last USN (update sequence number) in the STANDARD_
INFORMATION data structure stored in the embedded root node of the parent directory. Through the 
journal file and the USN number of each file and directory, ReFS can provide the three FSCTL used for 
reading and enumerate the volume journal file:
I 
FSCTL_READ_USN_JOURNAL: Reads the USN journal directly. Callers specify the journal ID
they’re reading and the number of the USN record they expect to read.
I 
FSCTL_READ_FILE_USN_DATA: Retrieves the USN change journal information for the specified
file or directory.
I 
FSCTL_ENUM_USN_DATA: Scans all the file records and enumerates only those that have last
updated the USN journal with a USN record whose USN is within the range specified by the
caller. ReFS can satisfy the query by scanning the object table, then scanning each directory
referred to by the object table, and returning the files in those directories that fall within the
timeline specified. This is slow because each directory needs to be opened, examined, and so
on. (Directories’ B+ trees can be spread across the disk.) The way ReFS optimizes this is that it
stores the highest USN of all files in a directory in that directory’s object table entry. This way,
ReFS can satisfy this query by visiting only directories it knows are within the range specified.
ReFS advanced features
In this section, we describe the advanced features of ReFS, which explain why the ReFS file system is a 
better fit for large server systems like the ones used in the infrastructure that provides the Azure cloud.
File’s block cloning (snapshot support) and sparse VDL
Traditionally, storage systems implement snapshot and clone functionality at the volume level (see 
dynamic volumes, for example). In modern datacenters, when hundreds of virtual machines run and 
are stored on a unique volume, such techniques are no longer able to scale. One of the original goals of 
the ReFS design was to support file-level snapshots and scalable cloning support (a VM typically maps 
to one or a few files in the underlying host storage), which meant that ReFS needed to provide a fast 
method to clone an entire file or even only chunks of it. Cloning a range of blocks from one file into a 
range of another file allows not only file-level snapshots but also finer-grained cloning for applications 
that need to shuffle blocks within one or more files. VHD diff-disk merge is one example.
ReFS exposes the new FSCTL_DUPLICATE_EXTENTS_TO_FILE to duplicate a range of blocks from 
one file into another range of the same file or to a different file. Subsequent to the clone operation, 
writes into cloned ranges of either file will proceed in a write-to-new fashion, preserving the cloned 
block. When there is only one remaining reference, the block can be written in place. The source and 
target file handle, and all the details from which the block should be cloned, which blocks to clone from 
the source, and the target range are provided as parameters.
CHAPTER 11
Caching and file systems
755
As already seen in the previous section, ReFS indexes the LCNs that make up the file’s data stream 
into the extent index table, an embedded B+ tree located in a row of the file record. To support block 
cloning, Minstore uses a new global index B+ tree (called the block count reference table) that tracks the 
reference counts of every extent of blocks that are currently cloned. The index starts out empty. The 
first successful clone operation adds one or more rows to the table, indicating that the blocks now have 
a reference count of two. If one of the views of those blocks were to be deleted, the rows would be 
removed. This index is consulted in write operations to determine if write-to-new is required or if write-
in-place can proceed. It’s also consulted before marking free blocks in the allocator. When freeing 
clusters that belong to a file, the reference counts of the cluster-range is decremented. If the reference 
count in the table reaches zero, the space is actually marked as freed. 
Figure 11-88 shows an example of file cloning. After cloning an entire file (File 1 and File 2 in the pic-
ture), both files have identical extent tables, and the Minstore block count reference table shows two 
references to both volume extents.
1
2
3
4
5
6
7
8
1
2
3
4
5
6
7
8
File 1
a
b
x
y
file
volume
[1-4]
[a-b]
[5-8]
[x-y]
1
2
3
4
5
6
7
8
File 2
file
volume
[1-4]
[a-b]
[5-8]
[x-y]
volextent
refcnt
[a-b]
2
[x-y]
2
FIGURE 11-88 Cloning an ReFS file.
Minstore automatically merges rows in the block reference count table whenever possible with 
the intention of reducing the size of the table. In Windows Server 2016, HyperV makes use of the 
new cloning FSCTL. As a result, the duplication of a VM, and the merging of its multiple snapshots, 
is extremely fast. 
ReFS supports the concept of a file Valid Data Length (VDL), in a similar way to NTFS. Using the 
ZeroRangeInStream file data stream, ReFS keeps track of the valid or invalid state for each allocated 
file’s data block. All the new allocations requested to the file are in an invalid state; the first write to the 
file makes the allocation valid. ReFS returns zeroed content to read requests from invalid file ranges. 
The technique is similar to the DAL, which we explained earlier in this chapter. Applications can logically 
zero a portion of file without actually writing any data using the FSCTL_SET_ZERO_DATA file system 
control code (the feature is used by HyperV to create fixed-size VHDs very quickly). 
756
CHAPTER 11
Caching and file systems
EXPERIMENT: Witnessing ReFS snapshot support through HyperV
In this experiment, you’re going to use HyperV for testing the volume snapshot support of ReFS. 
Using the HyperV manager, you need to create a virtual machine and install any operating 
system on it. At the first boot, take a checkpoint on the VM by right-clicking the virtual machine 
name and selecting the Checkpoint menu item. Then, install some applications on the virtual 
machine (the example below shows a Windows Server 2012 machine with Office installed) and 
take another checkpoint.
EXPERIMENT: Witnessing ReFS snapshot support through HyperV
In this experiment, you’re going to use HyperV for testing the volume snapshot support of ReFS. 
Using the HyperV manager, you need to create a virtual machine and install any operating 
system on it. At the first boot, take a checkpoint on the VM by right-clicking the virtual machine 
name and selecting the Checkpoint menu item. Then, install some applications on the virtual 
machine (the example below shows a Windows Server 2012 machine with Office installed) and 
take another checkpoint.
CHAPTER 11
Caching and file systems
757
If you turn off the virtual machine and, using File Explorer, locate where the virtual hard disk 
file resides, you will find the virtual hard disk and multiple other files that represent the differen-
tial content between the current checkpoint and the previous one.
If you open the HyperV Manager again and delete the entire checkpoint tree (by right-
clicking the first root checkpoint and selecting the Delete Checkpoint Subtree menu item), you 
will find that the entire merge process takes only a few seconds. This is explained by the fact that 
HyperV uses the block-cloning support of ReFS, through the FSCTL_DUPLICATE_EXTENTS_TO_FILE
I/O control code, to properly merge the checkpoints’ content into the base virtual hard disk file. 
As explained in the previous paragraphs, block cloning doesn’t actually move any data. If you 
repeat the same experiment with a volume formatted using an exFAT or NTFS file system, you will 
find that the time needed to merge the checkpoints is much larger.
ReFS write-through 
One of the goals of ReFS was to provide close to zero unavailability due to file system corruption. In 
the next section, we describe all of the available online repair methods that ReFS employs to recover 
from disk damage. Before describing them, it’s necessary to understand how ReFS implements write-
through when it writes the transactions to the underlying medium. 
The term write-through refers to any primitive modifying operation (for example, create file, extend 
file, or write block) that must not complete until the system has made a reasonable guarantee that the 
results of the operation will be visible after crash recovery. Write-through performance is critical for dif-
ferent I/O scenarios, which can be broken into two kinds of file system operations: data and metadata. 
If you turn off the virtual machine and, using File Explorer, locate where the virtual hard disk 
file resides, you will find the virtual hard disk and multiple other files that represent the differen-
tial content between the current checkpoint and the previous one.
If you open the HyperV Manager again and delete the entire checkpoint tree (by right-
clicking the first root checkpoint and selecting the Delete Checkpoint Subtree menu item), you 
will find that the entire merge process takes only a few seconds. This is explained by the fact that 
HyperV uses the block-cloning support of ReFS, through the FSCTL_DUPLICATE_EXTENTS_TO_FILE
I/O control code, to properly merge the checkpoints’ content into the base virtual hard disk file. 
As explained in the previous paragraphs, block cloning doesn’t actually move any data. If you 
repeat the same experiment with a volume formatted using an exFAT or NTFS file system, you will 
find that the time needed to merge the checkpoints is much larger.
758
CHAPTER 11
Caching and file systems
When ReFS performs an update-in-place to a file without requiring any metadata mutation (like 
when the system modifies the content of an already-allocated file, without extending its length), the 
write-through performance has minimal overhead. Because ReFS uses allocate-on-write for metadata, 
it’s expensive to give write-through guarantees for other scenarios when metadata change. For ex-
ample, ensuring that a file has been renamed implies that the metadata blocks from the root of the file 
system down to the block describing the file’s name must be written to a new location. The allocate-
on-write nature of ReFS has the property that it does not modify data in place. One implication of this 
is that recovery of the system should never have to undo any operations, in contrast to NTFS. 
To achieve write-through, Minstore uses write-ahead-logging (or WAL). In this scheme, shown in 
Figure 11-89, the system appends records to a log that is logically infinitely long; upon recovery, the 
log is read and replayed. Minstore maintains a log of logical redo transaction records for all tables 
except the allocator table. Each log record describes an entire transaction, which has to be replayed at 
recovery time. Each transaction record has one or more operation redo records that describe the actual 
high-level operation to perform (such as insert key K / value V pair in Table X). The transaction record 
allows recovery to separate transactions and is the unit of atomicity (no transactions will be partially re-
done). Logically, logging is owned by every ReFS transaction; a small log buffer contains the log record. 
If the transaction is committed, the log buffer is appended to the in-memory volume log, which will 
be written to disk later; otherwise, if the transaction aborts, the internal log buffer will be discarded. 
Write-through transactions wait for confirmation from the log engine that the log has committed up 
until that point, while non-write-through transactions are free to continue without confirmation.
Redo records
Redo records
Transaction records–tree log
Transaction
B+ tree
Transaction
Volume log
Volume
FIGURE 11-89 Scheme of Minstore’s write-ahead logging.
Furthermore, ReFS makes use of checkpoints to commit some views of the system to the underlying 
disk, consequently rendering some of the previously written log records unnecessary. A transaction’s 
redo log records no longer need to be redone once a checkpoint commits a view of the affected trees 
to disk. This implies that the checkpoint will be responsible for determining the range of log records 
that can be discarded by the log engine.
CHAPTER 11
Caching and file systems
759
ReFS recovery support
To properly keep the file system volume available at all times, ReFS uses different recovery strategies. 
While NTFS has similar recovery support, the goal of ReFS is to get rid of any offline check disk utilities 
(like the Chkdsk tool used by NTFS) that can take many hours to execute in huge disks and require the 
operating system to be rebooted. There are mainly four ReFS recovery strategies:
I 
Metadata corruption is detected via checksums and error-correcting codes. Integrity streams
validate and maintain the integrity of the file’s data using a checksum of the file’s actual content
(the checksum is stored in a row of the file’s B+ tree table), which maintains the integrity of the
file itself and not only on its file-system metadata.
I 
ReFS intelligently repairs any data that is found to be corrupt, as long as another valid copy is
available. Other copies might be provided by ReFS itself (which keeps additional copies of its
own metadata for critical structures such as the object table) or through the volume redundan-
cy provided by Storage Spaces (see the “Storage Spaces” section later in this chapter).
I 
ReFS implements the salvage operation, which removes corrupted data from the file system
namespace while it’s online.
I 
ReFS rebuilds lost metadata via best-effort techniques.
The first and second strategies are properties of the Minstore library on which ReFS depends (more 
details about the integrity streams are provided later in this section). The object table and all the global 
Minstore B+ tree tables contain a checksum for each link that points to the child (or director) nodes 
stored in different disk blocks. When Minstore detects that a block is not what it expects, it automati-
cally attempts repair from one of its duplicated copies (if available). If the copy is not available, Minstore 
returns an error to the ReFS upper layer. ReFS responds to the error by initializing online salvage.
The term salvage refers to any fixes needed to restore as much data as possible when ReFS detects 
metadata corruption in a directory B+ tree. Salvage is the evolution of the zap technique. The goal of 
the zap was to bring back the volume online, even if this could lead to the loss of corrupted data. The 
technique removed all the corrupted metadata from the file namespace, which then became available 
after the repair. 
Assume that a director node of a directory B+ tree becomes corrupted. In this case, the zap opera-
tion will fix the parent node, rewriting all the links to the child and rebalancing the tree, but the data 
originally pointed by the corrupted node will be completely lost. Minstore has no idea how to recover 
the entries addressed by the corrupted director node.
To solve this problem and properly restore the directory tree in the salvage process, ReFS needs 
to know subdirectories’ identifiers, even when the directory table itself is not accessible (because it 
has a corrupted director node, for example). Restoring part of the lost directory tree is made possible 
by the introduction of a volume global table, called called the parent-child table, which provides a 
directory’s information redundancy. 
A key in the parent–child table represents the parent table’s ID, and the data contains a list of child 
table IDs. Salvage scans this table, reads the child tables list, and re-creates a new non-corrupted B+ 
tree that contains all the subdirectories of the corrupted node. In addition to needing child table IDs, to 
760
CHAPTER 11
Caching and file systems
completely restore the corrupted parent directory, ReFS still needs the name of the child tables, which 
were originally stored in the keys of the parent B+ tree. The child table has a self-record entry with this 
information (of type link to directory; see the previous section for more details). The salvage process 
opens the recovered child table, reads the self-record, and reinserts the directory link into the parent 
table. The strategy allows ReFS to recover all the subdirectories of a corrupted director or root node 
(but still not the files). Figure 11-90 shows an example of zap and salvage operations on a corrupted 
root node representing the Bar directory. With the salvage operation, ReFS is able to quickly bring the 
file system back online and loses only two files in the directory.
Foo
Bar
Subdir1
A.txt
B.txt
Important file.doc