Interface
One to many
is that a signature, which corresponds to an aforementioned sub
type node, is a combination of words with high frequency. We
decompose messages into words separated by whitespace. For
each type of message, we construct a tree structure to express
the template (sub type) hierarchy based on the input messages
(e.g. m1, m2, . . . , m20), shown in Figure 2. We say that a
word associates with a message when the word appears in the
message. The detailed construction algorithm follows breath-ﬁrst
search tree traversal. We ﬁrst use the message type (e.g. BGP-5-
ADJCHANGE) as the root of the tree. All messages are associated
with this message type. Then given the parent node, we look for
the most frequent combination of words which can associate with
most messages that the parent node can associate with, and make
this combination as a child node. We repeat this process to create
child nodes based on remaining messages, until all messages have
been associated. We then recursively proceed to the child nodes
and repeat the process. Finally we prune the tree until it has the
desired degree properties as follows.
If a parent node has more
than k children, we will discard all children to make the parent a
leaf itself. Now each path from root to the each leaf become one
template (type + sub type). The intuition of this pruning is that on
the one hand, there are only a few sub types for each message type,
on the other hand, usually there would be many more messages as-
sociated with each sub type. For example, there should be many
IPs and VRF addresses associated with each sub type given enough
data. In practice, We choose k = 10 based on our experience that
no message type has more than 10 sub types.
Our template inference approach is quite generic because it is
based on the words frequency as opposed to text semantic. How-
ever, we use an implicit assumption that the variable part of the sys-
log messages would appear as many distinct values given enough
historical data. This assumption is surely not always true. For ex-
ample, if certain protocol are enabled only on one type of network
interface, say GigabitEthernet, then the “GigabitEthernet” part of
the message may be falsely included in the syslog template of the
protocol messages. However, this would have negligible impact on
the ﬁnal outcome of the grouping result, since the “GigabitEther-
net” in this case contains as much information as the syslog sub
type and hence there is no need to extract it out.
4.1.2 Location Information Learning
In a typical syslog message, we only have a router id ﬁeld as the
basic location information, but this is clearly not enough. For ex-
ample, some events occur on a particular physical port while some
other events occur on multiple logical links (e.g., IP links). Such
detailed location information is essential for understanding what is
going on in the network.
Figure 3: Location hierarchy
Figure 3 shows the generic location hierarchical structure. We
classify the basic components here into physical ones and logical
ones. The physical ones have a clear hierarchical structure from
top to bottom. The arrow here illustrates a “one-to-many” relation-
ship. For example, one router have multiple slots, each slot can
have multiple ports and etc. Besides physical hierarchy, there are
some logical conﬁgurations, but they will eventually map to some
physical component. For example, one multilink/bundlelink can be
mapped to multiple physical interfaces. Based on router conﬁgura-
tion data, we can extract ofﬂine the hierarchy in Figure 3 speciﬁc
to each router, cross-router location relationships such as neighbor
links, etc.
The key question here is how to automatically exploit such
kind of location information from each message. First, we know
the particular format of these location information embedded
into the messages. For example, the IP address has the format
XXX.XXX.XXX.XXX, the port has the format X/X/X , etc. Com-
pared with various parts of message we need to mask during tem-
plate learning, the number of location format patterns are lim-
ited, which is manageable for extraction using predeﬁned patterns.
However, the naive pattern matching is not sufﬁcient to extract
needed location addresses, mainly because more than one location
pattern (no matter whether they are all needed) can be found in each
message. For example, multiple IP addresses can be found in one
message. One could belong to the router itself, one could belong
the neighbor router, and it is also possible there are some remote
(e.g. remote session connection) or invalid IPs (e.g. scanning at-
tacks). To understand the exact meaning of multiple location pat-
terns, especially the belongings, we correlate these locations with
router conﬁguration data. For example we can verify if this IP be-
longs to the router or its neighbors. Note that the acquired location
information and location hierarchy will be used during the ofﬂine
rule mining and online grouping.
4.1.3 Learning Temporal Patterns of Templates
Once we identiﬁed the message templates, we learn the temporal
patterns of the message templates, i.e, the interarrival patterns. This
learned knowledge will be used in the online temporal grouping
component.
We observe that if a particular template of message occurs pe-
riodically, the corresponding messages naturally form a number
of clusters in the time series. For example, Figure 4 shows that
one controller goes up down many times within a short interval be-
cause controller is unstable during the interval. Another example
477e
c
n
e
r
r
u
c
c
O
t
n
e
v
E
m
00:00
01:00
02:00
03:00
04:00
05:00
06:00
Time series (GMT) 
Figure 4: Controller up/down example.
t
n
u
o
c
t
n
e
v
e
00:00
01:00
02:00
03:00
04:00
05:00
06:00
Time series (GMT) 
Figure 5: TCP bad authentication example
in Figure 5 shows that TCP bad authentication message has peri-
odic occurrences, likely due to the underling timer conﬁguration,
or outside impact, e.g. scanning patterns.
In order to learn such temporal patterns, i.e. , the interarrival time
within each cluster, we make a basic assumption, that is the im-
pact of current message on the further message with the same tem-
plate will exponentially decay. Such assumption is widely used for
time series analysis and system measurement purpose [5, 21]. Our
learning method is based on the interarrival sequence S1, S2, . . .
for each message template. We compute the (predicted) exponen-
tial weighted moving average (EWMA) of interarrival time t, ˆSt.
ˆSt = α · St−1 + (1 − α) · ˆSt−1
where α ∈ (0, 1) is the weighting factor. A higher α discounts
older observations faster. Intuitively, if the messages belong to the
same cluster, in other words, there is a periodic pattern within the
group, then the predicted value ˆSt should not be far away from the
real one St. Consequently, we assume that if St ≤ β · ˆSt where
(β >= 1), which means the real interarrival time is no much larger
than predicted one, we view that the message belongs to the same
group. Otherwise, there is another new group. Here parameter β
deﬁnes a threshold for grouping. Larger β means tolerating larger
intervals in the group.
The ofﬂine learning component uses long-term historical data
to infer the proper value of parameters α and β and can be up-
dated periodically. The actually values of these parameters will be
discussed in Section 5. These parameters are used in the online
temporal grouping component.
4.1.4 Template Relationship (Rule) Learning
In order to group different messages together to extract net-
work events, one natural thinking is to discover some implicit
rules among different templates. Some rules are very intuitive.
For example, layer-1 link failures (LINK-3-UPDOWN) often trig-
ger layer-2 failures (LINEPROTO-5-UPDOWN). Some others are
much more subtle. As we explained before, we cannot rely on
domain experts to compile and update a complete rule set given
the large number of templates. We need a systematic way to
identify such rules. This turns out to be a typical association
rule mining problem. Association rules describe items that oc-
cur frequently together in a dataset and are widely-used for mar-
ket basket analysis. Following the original deﬁnition by Agrawal
et al. [3] the problem of association rule mining is deﬁned as: Let
I = {i1, i2, . . . , in} be a set of n binary attributes called items. Let
D = {t1, t2, . . . , tm} be a set of transactions called the database.
Each transaction in D has a unique transaction ID and contains a
subset of the items in I. A rule is deﬁned as an implication of the
form X ⇒ Y where X, Y ∈ I and X ∩ Y = ∅. To select in-
teresting rules from the set of all possible rules, constraints on var-
ious measures of signiﬁcance and interest can be used. The best-
known constraints are minimum thresholds on support and conﬁ-
dence. Support supp(X) of an itemset X is deﬁned as the propor-
tion of transactions in the data set which contain the itemset. The
conﬁdence of a rule is deﬁned as
conf (X ⇒ Y ) =
supp(X ∩ Y )
supp(X)
In our problem setting, each message template is one item. In
order to construct the transactions, we use a sliding window W . It
starts with the ﬁrst message, and slides message by message (sorted
messages on the time series). Each time there is one transaction. In
one such transaction, the message templates in the window W are
considered as the items showing up.
Note that we only consider pair wise association, or |X| =
|Y | = 1. In other words, each rule only contains two templates.
The reason is, ﬁrst, the computation complexity is low, and second,
it is relatively easy to verify the generated rule sets. Domain ex-
perts only need to verify the relationship of two templates per rule.
The disadvantage of pair wise is that based on these rules we can-
not group more than two templates each time, but since we assume
the transition property during rule-based grouping discuss later in
Section 4.2.2), the ﬁnal digest will combine multiple templates to-
gether.
Until now, we assume that the rules are generated based on static
dataset. But ideally we want to learn the rules continuously. In
order to adaptively adjust the rules, we use the following conserva-
tive way. First, we training the a period of data to generate the ba-
sic rule sets. Then we keep change the rules periodically (e.g. each
week). The new rules X ⇒ Y should be added when supp(X) and
conf (X ⇒ Y ) are above the threshold. Old rules X ⇒ Y should
be deleted when updated conf (X ⇒ Y ) is below the threshold, no
matter what supp(X) is. Such conservative deletion approach en-
sures that we do not delete the rules because X are not common in
this updating period (it is quite possible X become common again
soon).
4.2 Online System Methodologies
The online system takes the real-time Syslog+ data (with mes-
sage template and location information) and the ofﬂine-learned do-
main knowledge as input, groups related messages together and
construct prioritized event. Roughly speaking, if two messages oc-
cur close in time and locations, they are related with high prob-
ability. While the temporal closeness between two messages can
simply the characterized by the closeness of their timestamps, the
characterization of spatial closeness is more subtle. We model var-
ious location types in the location hierarchy shown in Figure 3.
We say the two locations are spatially matched when they can
be mapped to same location in the hierarchy. For example, sup-
pose one message happens on slot 2 and another one message
on the same router happens on interface series 2/0/0:1. They are
considered spatially matched because the later message’s location
(2/0/0:1) can be mapped upwards in Figure 3 to slot 2 (the ﬁrst digit
before the backslash interface series of 2/0/0:1).
4784.2.1 Temporal Grouping
Online temporal grouping uses the same methodology as ofﬂine
temporal patterns learning, presented in Section 4.1.3. Similar to
Section 4.1.3 , if the real inter-arrival time St ≤ β· ˆSt, then the mes-
sages belong to the same group, otherwise there is separate group.
We also introduce two thresholds Smin and Smax. Smin is the
minimal interarrival time, and Smax is the maximum. The rea-
son of introducing Smax is that the our algorithm cannot guarantee
convergence. Each time we only guarantee that the St is not too
large. But when ˆSt increase, St can grow exponentially. If the real
interarrival time is smaller than Smin, then we consider the mes-
sages belong to the same group. If the real interarrival time is larger
than the Smax, then we believe there is a separate group. We set
Smin to be 1 second (this is the ﬁnest time granularity available in
the syslog data we used) and Smax to be 3 hours (this is based on
domain knowledge).
4.2.2 Rule based Grouping
In the temporal grouping part, we only consider grouping mes-
sages with the same template together. Now we try to discover
the connections among messages with different message templates.
The ofﬂine-learned rules using association mining contain pair-
wise message templates that occur frequently together. The rule
based grouping component groups the messages which happen on
the spatially matched locations and happen close enough in time
(within the window W discussed in rule-learning part). Note that
our rule based grouping does not consider the direction of rule since
our system is not a troubleshooting system thus does not rely on
causality inference. It is possible that we have A → B and A → C
in the rule set, but we ignore the direction and can group A, B, C
together, assuming temporal and spatial constraints are satisﬁed.
This is because it is very likely they are triggered by the same net-
work condition thus should be considered as one event, even though
we ignore the detailed causal relationship among the messages.
4.2.3 Cross Router Grouping
The ﬁrst two grouping methods all focus on a single router. A
network event, however, can affect multiple routers. For instance,
a link down event should involve two adjacent routers’ links. To
group such messages, our solution is a conservative one. Our of-
ﬂine location learning component already provides a dictionary for
cross-router location relationship such as links, sessions, tunnels
(a path) between different routers. Assuming that the propagation
along the connects are fast enough, we group messages with the
same template which happened on the same link, session, or path
at almost the same time (e.g no larger than 1 second difference).
We perform three grouping methods in the order they are de-
scribed.
If any two messages in two different groups have been
grouped together, then these two groups will be merged. Thus the
changes of orders of these three parts have no impact on the ﬁnal
grouping results. We use this order because it is more natural to
describe: from one signature to multiple ones, from single router to
multiples ones.
4.2.4 Prioritization and Presentation
We now have a number of messages in each group. We ﬁrst pri-
oritize the messages so that the most important events will appear at
the top of the digest. Recall from Section 2 that the severity level of
a message provided by syslog shall not be trusted/used. Instead, we
use a combination of the following three metrics. The ﬁrst metric is
the occurrence frequency of message signature on each router, say
fm for message m. The intuition is that we care more about rare
events. We also consider the impact of the events. The event hap-
pened on the higher level of the hierarchy is more important. For
example, an event happened on the router is more important than
the one happened on the interface. Let lm denote the location met-
ric of message m, and we can assume that the value of lm higher
level is several (e.g. 10) times of lower level. Finally, we con-
sider the number of messages in the grouped event, which in some
sense reﬂects severity of the event. The group with more messages
should be relatively important. Based on these three metrics, there
is a score we assign for each event:
Score =
M
X
m=1
lm/log(fm)
where the event contains M messages. The reason for taking loga-
rithm here is to prevent rare events with tiny fm values from dom-
inating the top of the ranked list. Note that our scoring method
provides a baseline for ranking. The network operators can adjust
the weights for each type of messages, based on their experience.
We rank all events based on the score in a decreasing order. After
ranking, we (actually the SyslogDigest system) are ready to present
the ﬁnal result. There are many ways to display the event, and we
choose the most concise way. First, we present the beginning and
ending time of the event, which map to the time range of all mes-
sages in the group. Second, we present the location information
of the event. For each router, we present the most common high-
est level location on hierarchy. For example, if the event contains
two messages, one on the router level while the other on the inter-
face level, we only show the router. Third, we present the type of
event. One direct way is to present the combinations of message
signatures within the group. Domain experts can certainly assign a
name for each type of event. For example, we can assign a name
"link ﬂap" to a event which contains "LINK-DOWN" and "LINK-
UP" messages.
5. EVALUATION
We evaluate SyslogDigest using real syslog data from two large
operational networks. We ﬁrst validate several design choices we
made in the ofﬂine domain knowledge learning component. Then
we report the results of the entire SyslogDigest system.
5.1 Evaluation Methodology
We use syslog data collected from two large operational IP net-
works in North America: a tier-1 ISP backbone network and a
nation-wide commercial IPTV backbone network. Each of these
two networks has around a couple of thousands of routers and
records millions of syslog messages per day. We refer to these
two syslog data as Dataset A and Dataset B, respectively. Note
that these two networks use routers from different vendors and have
different network design and protocol conﬁgurations for supporting
different network applications/services – the ISP backbone network
is for general/traditional IP/Internet service and the IPTV backbone
is for commercial TV service. Both the types of messages and their
signatures are very different in these two dataset. In our evaluation,
we use three months of data collected from September to Novem-
ber 2009 for ofﬂine domain knowledge learning and two weeks’
of data collected December 1-14, 2009 for online processing and
reporting event digests.
We evaluate the effectiveness of SyslogDigest in the following
two aspects. First, we use the metrics compression ratio to measure
the ability of SyslogDigest to reduce the amount of information
that operators need to receive and examine in order to know what
happened in the network for each incident. We deﬁne the the com-
pression ratio to be the number of events (compressed information
479Table 5: Sensitivity of minimal support SPmin value
Coverage (B)
SPmin
Coverage (A)
Top % (A)
Top % (B)
0.001
0.0005