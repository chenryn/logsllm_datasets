[4] Nicholas Carlini and David Wagner. 2018. Audio adversarial examples: Targeted
attacks on speech-to-text. arXiv preprint arXiv:1801.01944 (2018).
[5] Moustapha M Cisse, Yossi Adi, Natalia Neverova, and Joseph Keshet. 2017. Hou-
dini: Fooling deep structured visual and speech recognition models with adversar-
ial examples. In Advances in Neural Information Processing Systems. 6977–6987.
[6] Wenrui Diao, Xiangyu Liu, Zhe Zhou, and Kehuan Zhang. 2014. Your Voice
Assistant is Mine: How to Abuse Speakers to Steal Information and Control
Your Phone. In Proceedings of the 4th ACM Workshop on Security and Privacy in
Smartphones &#38; Mobile Devices (SPSM ’14). ACM, New York, NY, USA, 63–74.
https://doi.org/10.1145/2666620.2666623
[7] Sam Edwards and Ioannis Profetis. 2016. Hajime: Analysis of a decentralized
internet worm for IoT devices. Technical Report. Rapidity Networks.
[8] Earlence Fernandes, Jaeyeon Jung, and Atul Prakash. 2016. Security analysis
of emerging smart home applications. In 2016 IEEE Symposium on Security and
Privacy (SP). IEEE, 636–654.
[9] Grant Ho, Derek Leung, Pratyush Mishra, Ashkan Hosseini, Dawn Song, and
David Wagner. 2016. Smart Locks: Lessons for Securing Commodity Internet of
Things Devices. In Proceedings of the 11th ACM on Asia Conference on Computer
and Communications Security (ASIA CCS ’16). ACM, New York, NY, USA, 461–472.
https://doi.org/10.1145/2897845.2897886
[10] C. Kasmi and J. Lopes Esteves. 2015.
IEMI Threats for Information Security:
Remote Command Injection on Modern Smartphones.
IEEE Transactions on
Electromagnetic Compatibility 57, 6 (Dec 2015), 1752–1755. https://doi.org/
10.1109/TEMC.2015.2463089
[11] John Koetsier. 2018.
Hits 50 Million; Apple Has 6% Market Share, Report Says.
//www.forbes.com/sites/johnkoetsier/2018/08/02/amazon-echo-google-
home-installed-base-hits-50-million-apple-has-6-market-share-report-says.
Amazon Echo, Google Home Installed Base
https:
[12] Deepak Kumar, Riccardo Paccagnella, Paul Murley, Eric Hennenfent, Joshua
Mason, Adam Bates, and Michael Bailey. 2018. Skill squatting attacks on ama-
zon alexa. In 27th USENIX Security Symposium (USENIX Security 18). USENIX
Association, 33–47.
[13] Nirupam Roy, Haitham Hassanieh, and Romit Roy Choudhury. 2018. BackDoor:
Sounds That a Microphone Can Record, but That Humans Can’T Hear. GetMobile:
Mobile Comp. and Comm. 21, 4 (Feb. 2018), 25–29. https://doi.org/10.1145/
3191789.3191799
[14] Nirupam Roy, Haitham Hassanieh, and Romit Roy Choudhury. 2017. BackDoor:
Making Microphones Hear Inaudible Sounds. In Proceedings of the 15th Annual
International Conference on Mobile Systems, Applications, and Services (MobiSys
’17). ACM, New York, NY, USA, 2–14. https://doi.org/10.1145/3081333.3081366
[15] Roman Schlegel, Kehuan Zhang, Xiao-yong Zhou, Mehool Intwala, Apu Kapadia,
and XiaoFeng Wang. 2011. Soundcomber: A Stealthy and Context-Aware Sound
Trojan for Smartphones. In Proceedings of the Network and Distributed System
Security Symposium, NDSS 2011, San Diego, California, USA, 6th February - 9th
February 2011. http://www.isoc.org/isoc/conferences/ndss/11/pdf/11 .pdf
[16] Lea Schönherr, Katharina Kohls, Steffen Zeiler, Thorsten Holz, and Dorothea
Kolossa. 2018. Adversarial Attacks Against Automatic Speech Recognition Sys-
tems via Psychoacoustic Hiding. arXiv preprint arXiv:1808.05665 (2018).
[17] Claude Elwood Shannon. 1949. Communication in the presence of noise. Pro-
ceedings of the IRE 37, 1 (1949), 10–21.
[18] Yun Mok Son, Ho Cheol Shin, Dong Kwan Kim, Young Seok Park, Ju Hwan
Noh, Ki Bum Choi, Jung Woo Choi, and Yong Dae Kim. 2015. Rocking drones
with intentional sound noise on gyroscopic sensors. In 24th USENIX Security
symposium. USENIX Association.
[19] Liwei Song and Prateek Mittal. 2017.
Inaudible Voice Commands. CoRR
abs/1708.07238 (2017). arXiv:1708.07238 http://arxiv.org/abs/1708.07238
[20] Timothy Trippel, Ofir Weisse, Wenyuan Xu, Peter Honeyman, and Kevin Fu.
2017. WALNUT: Waging doubt on the integrity of MEMS accelerometers with
acoustic injection attacks. In Security and Privacy (EuroS&P), 2017 IEEE European
Symposium on. IEEE, 3–18.
[21] Tavish Vaidya, Yuankai Zhang, Micah Sherr, and Clay Shields. 2015. Cocaine
Noodles: Exploiting the Gap between Human and Machine Speech Recognition.
In 9th USENIX Workshop on Offensive Technologies (WOOT 15). USENIX Associa-
tion, Washington, D.C. https://www.usenix.org/conference/woot15/workshop-
program/presentation/vaidya
[22] Tim Yeh, Dove Chiu, and Kenney Lu. [n. d.]. Persirai: New Internet of Things (IoT)
Botnet Targets IP Cameras. https://blog.trendmicro.com/trendlabs-security-
intelligence/persirai-new-internet-things-iot-botnet-targets-ip-cameras/.
[23] Park Joon Young, Jo Hyo Jin, Samuel Woo, and Dong Hoon Lee. 2016. BadVoice:
Soundless voice-control replay attack on modern smartphones. In 2016 Eighth
International Conference on Ubiquitous and Future Networks (ICUFN). 882–887.
https://doi.org/10.1109/ICUFN.2016.7537163
[24] Xuejing Yuan, Yuxuan Chen, Yue Zhao, Yunhui Long, Xiaokang Liu, Kai Chen,
Shengzhi Zhang, Heqing Huang, Xiaofeng Wang, and Carl A Gunter. 2018. Com-
manderSong: A Systematic Approach for Practical Adversarial Voice Recognition.
arXiv preprint arXiv:1801.08535 (2018).
[25] Guoming Zhang, Chen Yan, Xiaoyu Ji, Tianchen Zhang, Taimin Zhang, and
Wenyuan Xu. 2017. DolphinAttack: Inaudible Voice Commands. In Proceedings
of the 2017 ACM SIGSAC Conference on Computer and Communications Secu-
rity (CCS ’17). ACM, New York, NY, USA, 103–117. https://doi.org/10.1145/
3133956.3134052
[26] Nan Zhang, Xianghang Mi, Xuan Feng, XiaoFeng Wang, Yuan Tian, and
Feng Qian. 2018. Understanding and Mitigating the Security Risks of Voice-
Controlled Third-Party Skills on Amazon Alexa and Google Home. arXiv preprint
arXiv:1805.01525 (2018).
Session 6A: IoT SecurityAsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand476]
%
[
n
o
i
t
i
n
g
o
c
e
r
t
c
e
r
r
o
C
100
80
60
40
20
0
20
21
22
23
25
AM carrier frequency [kHz]
24
26
27
55
28
50
47
45
Figure 8: Results of injecting the voice sample “start Evil” recorded from a male human, modulated with different carrier
waves and at different volumes (45-55/100), into an Amazon Echo Dot from 30 cm distance.
]
%
[
n
o
i
t
i
n
g
o
c
e
r
t
c
e
r
r
o
C
100
80
60
40
20
0
20
21
23
22
26
AM carrier frequency [kHz]
24
25
27
28
55
Figure 9: Results of injecting the voice sample “start Evil”
generated by a female voice TTS service, modulated with dif-
ferent carrier waves, into an Amazon Echo Dot from 30 cm
distance.
APPENDIX
A CRAFTING INAUDIBLE VOICE SIGNALS
The non-linearity of the transducer (the system consisting of a
microphone, amplifier, LPF and analog-digital converter (ADC)) will
output a signal consisting of more terms as the input. If Sin is the
input signal transported over the air (voice, music, single frequency
e.g. cos(2π f t)), the recorded signal, due to the non-linearity, would
be:
i =1
∞
Sout =
Gi Si
in = G1Sin + G2S2
in + · · ·
(1)
where Gi is the gain for each term (Gi Si
in), decreasing rapidly with
increasing i. The attack uses the demodulation properties of the
second term (the most powerful except the linear term). First we
need to low-pass filter our audio signal at 8kHz. Voice is mainly con-
centrated on the lower frequencies and therefore we can remove all
frequencies above 8kHz. This way our frequency spectrum reaches
from 0 − 8kHz. After that we need to upsample it to be able to con-
vert it to ultrasound (> 20kHz) later on. The upsampling frequency
should be the maximum frequency our device can process but no
less than 56kHz. We denote this signal as Sup.
The next step is to amplitude modulate it onto an ultrasonic
carrier wave with frequency fc to shift the signal into the ultrasonic
spectrum and make it inaudible for humans. We get:
Smodu = n1Supcos(2π fct)
(2)
where n1 is a normalization factor. This will create sidebands rang-
ing from fc − 8kHz to fc + 8kHz. This is why we need at least an
upsampling frequency of 58kHz. Given the Nyquist-Shannon sam-
pling theorem [17] we are able to sample a signal with a frequency
of up to 28kHz without loss using a sampling rate of 56kHz. With
the lower sideband ranging down to fc − 8kHz fc should be at least
28kHz to make it inaudible. After that we need to add the carrier
wave to the signal, so it can be demodulated by the non-linearity
of the speaker. We get:
Sadded = n2(Smodu + cos(2π fct))
(3)
where n2 is another normalization factor.
Suppose Sm = cos(2π fmt) then the attack signal would be:
Sin = n2(n1cos(2π fmt)cos(2π fct) + cos(2π fct))
(4)
After demodulation we get the frequency components fm, 2(fc −
fm), 2(fc + fm), 2fc , 2fm, 2fc − fm and 2fc + fm. Every component
is above 20kHz and will be removed by the LPF of the microphone
except fm which is 8kHz at the maximum. That way the original
Sm is perceived by the microphone without ever played and thus
inaudible for humans.
Session 6A: IoT SecurityAsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand477B ATTACK SIGNAL PARAMETERS
The ultrasonic attack description of Song et al. [19] and Zhang et
al. [25] leaves many implementation and parameterization ques-
tions open. To craft the ultrasonic signal, we have to normalize it
with the variables n1 and n2, as described in Sect. A. Obviously we
cannot set them to 1 otherwise we will get our signal clipped at the
maximum amplitude of 1. We have to scale one or both amplitudes
down. Fortunately we don’t have to evaluate it but we can calculate
it easily as follows:
Applying Eq. 4 to Eq. 1 to calculate the second term of Sout
which we denominate Sout2, we get:
1
(n2
1cos(4π fct − 4π fmt)
2n2
Sout2 = G2
8
1cos(4π fct + 4π fmt) + 2n2
1cos(4π fct)
2n2
2n2
2n1cos(4π fct − 2π fmt)
2n1cos(2π fmt)
2cos(4π fct) + 4n2
2)
1cos(4π fmt) + 2n2
2n2
2n2
+4n2
2n1cos(4π fct + 2π fmt) + 8n2
1 + 4n2
+2n2
+4n2
+n2
(5)
With this we we see that if we want to maximize the amplitude of
the demodulated Sm we have to maximize n2
2n1 while the maximum
possible amplitude of Sin is 1 to avoid clipping of the constructed
signal. Suppose Sm has an amplitude of 1 like the carrier wave and
the added wave. With Eq. 3 we get that the amplitude of Sin is
n2(n1 + 1). We have to find n1 and n2 of:
max(cid:8)n1n2
2|(n1 + 1)n2 = 1(cid:9)
(6)
2).
which is at (n1, n2) = (1, 1
C EVALUATION OF PARAMETERS
The Echo Dot was activated by unmuting it by pressing the button
on top of the device, after which the modulated recording of a male
person saying “start Evil” was played back at different volumes and
modulated with different carrier waves of different frequencies. The
results are displayed in Fig. 8. We see that lower sound pressure
signals modulated with a lower frequency are successfully demod-
ulated. Unsuccessful injections usually mean that Alexa does not
understand anything, but it occasionally also understood “Even”,
“Not”, “Not you” and only “Evil”, as reported by the Alexa App. As
stated earlier this can be mitigated by using an invocation word
composed of two words which do not occur anywhere else. Evalu-
ating with the same setup but using a voice sample generated by a
female TTS service is depicted in Fig. 9. Song et al. [19] came to the
same conclusion, that this yields much worse results, as the female
voice is centered much higher than that of a male and is clipped by
the low-pass filter.
Session 6A: IoT SecurityAsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand478