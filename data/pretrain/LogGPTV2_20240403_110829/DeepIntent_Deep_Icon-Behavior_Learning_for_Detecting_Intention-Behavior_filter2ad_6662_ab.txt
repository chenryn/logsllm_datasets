behaviors that deliberately evade detection (i.e., camouflaged as
normal behaviors), DeepIntent is designed to determine whether
the behavior of an icon widget matches the intentions reflected
by the user-perceivable information in the UIs, i.e., whether the
UIs provide justifications for the behaviors. While some of the
underlying data flows may not be intuitively reflected by the UI
information, such as disclosing contacts, if many apps with similar
UIs have such behaviors, DeepIntent will still be able to capture
such compatibility in the model.
Note that most apps in the app markets are legitimate, whose de-
velopers design the apps to meet users’ requirements, even though
some of them may be aggressive on exploiting user privacy for rev-
enue. For certain third-party app markets that may be flooded with
malicious apps, the training quality may be affected. In that case,
anti-virus techniques and malware detection techniques should be
applied to remove such apps from the training dataset. Malicious
apps that deliberately evade detection can be detected by special
techniques [7, 16, 83, 93], which is out of scope of this paper.
4 ICON-BEHAVIOR ASSOCIATION
This module provides static analysis techniques to identify icon
widgets, extracts their icons and texts, and infers the permission
uses of the icon widgets. It plays a key role in learning an icon-
behavior model, since it enables the construction of a large-scale
high-quality training dataset. Our techniques analyze both UIs and
source code to associate icons/texts and handlers to icon widgets.
Particularly, we build extended call graphs to patch missing calling
relationships introduced by the Android environment, and use the
extended call graphs to identify APIs invoked by the UI widgets.
4.1 Static Analysis Overview
This module contains four major components: 1) Icon-Widget As-
sociation, 2) Extended Call Graph Construction, 3) Widget-API
Association, and 4) API Permission Checking, as shown in Figure 4.
The first two components take an Android APK file as input. The
Figure 3: Overview of DeepIntent.
of the icons in these apps use the icons and permissions properly,
capturing the general expectation of users. With the learned icon-
behavior model, DeepIntent also trains a discrepancy detection
model which can be used to compute the outlier scores for test
icons. Then, for a button whose intention-behavior discrepancy
is to be checked, DeepIntent adopts the same static analysis to
extract the ⟨icon, text, permissions⟩ triple, and feeds the triple into
the icon-behavior models (i.e., icon-behavior learning and discrep-
ancy detection) to determine whether there are any discrepancies
between the intentions (represented using icons and contextual
texts) and the permission uses. In this example, it is expected for
the first button (‘call’) to use the CALL permission, while there is a
discrepancy for the second button (‘timing filter’) to use the CALL
permission.
3 DESIGN OF DEEPINTENT
3.1 Overview
Figure 3 shows the overview of DeepIntent. DeepIntent consists
of three phases: (1) icon widget analysis, (2) learning icon-behavior
model, and (3) detecting intention-behavior discrepancies.
The first phase accepts a training dataset of Android APK files
as input, and extracts features (i.e., icons and texts) and labels (i.e.,
permission uses) of icon widgets. Specifically, the icon-behavior
association module applies static analysis techniques to identify the
icons used in UI widgets, associates the icons with UI handlers, and
infers the icon-permission mappings based on the API-permission
mappings. The contextual text extraction module extracts the con-
textual texts for the identified icons.
Based on the output of icon widget analysis, the deep icon-
behavior learning module uses both icons and their contextual
texts as features, and the corresponding behaviors, i.e., permission
uses, as labels to train the icon-behavior model. In particular, this
module uses a parallel co-attention mechanism that can learn the
joint features from both icons and their contextual texts.
In the next phase, DeepIntent extracts the icon and text fea-
tures for each icon widget, predicts the permission uses for the icon
widgets, and detects abnormal permission uses. Specifically, given
Icon-Behavior AssociationTraining APKsContextual Text ExtractionDeep Icon-Behavior LearningIcon-Permission MappingsContextual Texts for IconsIcon-Behavior  ModelOutlier DetectionAPKBehavior PredictionIcon Widget AnalysisDetecting Intention-Behavior DiscrepancyPredicted Permission UseAbnormal Permission UseAPKIcon-Widget AssociationWidget-API AssociationIcon-Permission MappingsAPI Permission CheckingExtended Call Graph Construction1 
2
3
4
5
6
7
8
9
10
11
12 
Figure 5: Simplied UI layout file (search.xml) for Animated
Weather App.
public void onCreate(Bundle savedInstanceState) {
1 public class SearchForm extends Activity {
2
3
4
setContentView(R.layout.search);
((ImageView) findViewById(R.id.ImageViewLocation)).
setOnClickListener(new OnClickListener {
public void onClick(View v) {startAsincSearch;} });
...
searchThread = new LocationThread;
searchThread.start; // bound to LocationThread.run
... } } // end of class SearchForm
private void startAsincSearch {
... } // bound to OnClick handler
5
6
7
8
9
10
11
12
13
14 class LocationThread extends Thread {
15
16
17
18
19
...
public void run {
ManagerOfLocation.findPosition; // use GPS data
... } }
Figure 6: Example sensitive API in multi-threading.
output of the Icon-Widget Association component is a mapping be-
tween icons and their corresponding UI widgets. The output of the
Extended Call Graph Construction component is an extended call
graph of the entire app. Then these outputs are used as the inputs
for the Widget-API Association component, which associates the
UI handlers with the UI widgets and constructs the corresponding
call graphs for each UI handler. At last, each method call in the call
graphs is associated with the corresponding permission uses based
on the PScout [6] Android API-permission mappings. The output
of the icon-behavior association module is the icon-permission
mappings that map all the extracted icons to their corresponding
permission uses. We next present the details of each component.
4.2 Icon-Widget Association
In Android apps, icons can be associated with UI widgets by speci-
fying in the UI layout files or in the source code. Each UI layout,
widget, and icon has its own unique ID. UI layout files are loaded
through API calls like setContentView or inflate in activities at run-
time. Then UI widgets in the layout can be bound to variables via
API calls such as findViewByID using UI widgets’ IDs. Icons can be as-
sociated with UI widgets directly in UI layout files as well. Figure 5
shows a simplified UI layout file for the Animated Weather app.
The UI widget ImageView at Line 3 associates an icon to the widget
via the attribute android:src.
We adopt the static analysis of IconIntent [80], the state-of-
the-art sensitive UI widget identification approach, to associate
icons to the UI widgets. IconIntent performs static analysis on
both the UI layout files and the source code to infer the associations
between icons and UI widgets. The static analysis on UI layout
files parses the UI layout files and identifies the names of the app’s
icons (such as @drawable/ic_location) and the UI widgets with IDs,
such as ImageView in Figure 5. The analysis on app’s code provides
a data flow analysis to overapproximate the associations between
variables and UI widget IDs, and the associations between variables
and icon IDs. Then IconIntent combines the analysis results on
both the UI layout files and the code to determine which UI widgets
are associated with which icons (many-to-many mappings).
4.3 Extended Call Graph Construction
Android app executions follow the event-driven execution model.
When a user navigates through an app, the Android framework
triggers a set of lifecycle events and invokes lifecycle methods
such as onCreate and onResume; when a user interacts with the app’s
UIs, the Android framework triggers a set of UI interaction events
and invokes the corresponding callback methods such as onClick.
Furthermore, multi-threaded communications split the execution
into executions in both foreground and background. Thus, to de-
termine which behavior is triggered (i.e., which APIs are invoked),
DeepIntent builds a static call graph for each UI handler.
Figure 6 shows an example app that requests a user’s GPS loca-
tion via multi-threading. When the UI widget (ImageViewLocation) is
clicked at Line 5, startAsincSearch is invoked. Then a new thread
is initiated and started for startAsincSearch at Line 10 and Line 11.
At last, a sensitive API that requires the location permission is in-
voked in LocationThread.run at Line 17. In other words, in addition to
the explicit calling relationships established via calling statements,
there exist implicit calling relationships between setOnClickListener
and onClick, as well as LocationThread.start and LocationThread.run.
In addition to multi-threading, sensitive APIs may be invoked via a
service or a broadcast receiver using Inter-Component Communi-
cations (ICC) [37, 54].
These implicit calling relationships pose challenges for infer-
ring the APIs invoked when the UI handler is triggered. Existing
work [5, 79] has proposed techniques to address these challenges
in building call graphs. However, they often assume every possi-
ble combination of lifecycle methods (e.g., onCreate and OnResume),
multi-threading methods, and ICC methods, resulting in exhaustive
calling contexts. To find APIs invoked by a UI handler, such exhaus-
tive calling contexts result in a large number of false associations
between UI widget and sensitive APIs.
To address this problem, we propose a static analysis technique
to patch these missing calling relationships without exhausting the
lifecycle method calls. Specifically, our static analysis first leverages
existing static call graph techniques to build a call graph based
on calling statements, and then expands the static call graph with
edges representing implicit calling relationships. In particular, our
analysis includes four types of implicit calling relationships that are
most commonly used in Android apps, including multi-threading,
lifecycle method, event-driven method, and inter-component com-
munication (ICC). Table 1 shows the implicit calling relationships
Caller
setOnClickListener
Callee
onClick
Thread.start
Thread.run
AsyncTask.execute
doInBackground
onPreExecute
onPostExecute
sendMessage
handleMessage
Table 1: Implicit caller and callee pairs captured.
used by our analysis, except for ICC methods. Our analysis lever-
ages existing ICC analysis [37, 54] to identify the implicit calling
relationships for ICCs and create edges in the call graph for them.
4.4 Widget-API Association
This component aims to associate the UI widgets with their UI han-
dlers and construct the call graphs for the UI handlers. We adapt
the existing Android static analysis tool, GATOR [63, 81, 82], to
associate the UI handlers with UI widgets. GATOR applies static
analysis to identify UI widgets and UI handlers, and provides an
over-approximation dataflow algorithm to infer the associations
between the event handler methods and the UI widgets. Our ap-
proach then combines the output from GATOR with the output
from the Icon-Widget Association component to build the associ-
ations among icons, layout files, UI widgets, and UI handlers. For
example, in Figure 6, the UI widget ImageViewLocation is bound to
the layout file in Figure 5, its icon is lc_location (Line 3 in Figure 5),
and it is associated to the UI handler SearchForm.onClick. Note that
there can be multiple handlers for one UI widget.
Once each widget is associated to icons and UI handlers, our
approach then generates a call graph for each of its UI handlers. A
call graph of a UI handler is a subgraph of the extended call graph
for the entire app, which contains the nodes that are reachable from
the UI handler. DeepIntent then finds API uses for the UI widget
by searching the API calls inside the call graph.
4.5 API Permission Checking
This component maps the APIs found in the extended call graph
of each icon widget to permission uses based on PScout [6], a
widely-used permission mapping from Android APIs to Android
permissions. This component outputs the associations between
each icon and a set of permissions. Note that an icon widget can be
mapped to one or more permission uses since it may invoke multi-
ple sensitive APIs or some sensitive APIs are mapped to multiple
permissions.
4.6 Contextual Texts Extraction for Icons
As mentioned in introduction, similar icons may reflect different
intentions in different UI contexts. While it is difficult to differenti-
ate them by just comparing the icons, the contextual texts, such as
the nearby text labels and the header of the UI, can be used to help
distinguish the contexts of the icons. Thus, DeepIntent further
provides a contextual text extraction component that extracts the
contextual texts for each icon. Specifically, DeepIntent extracts
three types of contextual texts: (1) layout texts that are contained
Figure 7: Workflow of Deep Icon-Behavior Learning.
in the XML layout files, (2) icon-embedded texts which can be ex-
tracted by Optical Character Recognition (OCR) techniques, and
(3) resource names split by variable naming conventions. The fi-
nal output of our static analysis is a set of ⟨icon, text, permissions⟩
triples.
5 DEEP ICON-BEHAVIOR LEARNING
Using the output (i.e., ⟨icon, text, permissions⟩ triples) from the pre-
vious module, DeepIntent leverages the co-attention mechanism
to jointly model icons and texts, and trains an icon-behavior model
that predicts the permission uses of icon widgets.
5.1 Model Overview
The overview of the this module is shown in Figure 7. Each piece
of input consists of an icon and its text. As an initialization step to
simultaneously learning their joint features, we need to first feed
icons and texts into their respective feature extraction components.
More specifically, we adapt DenseNet layers to extract icon features
and bidirectional RNN layers to extract text features. We then com-
bine them into a joint feature vector by using co-attention layers.
The intuition is as follows. On one hand, the icon and its text could
be semantically correlated; consequently, although we can simply
concatenate the icon features and text features, the correlation
between these two sources would be ignored, making the final rep-
resentations of the input icon-text pair sub-optimal. On the other
hand, the recently proposed co-attention layers can simultaneously
update the icon features and the text features with the guidance
of each other, and thus can capture the correlations between icons
and texts. Next, since each icon may relate to multiple permissions,
we formulate a multi-label classification problem [72] to learn the
joint features. With the mapped permissions for icons, DeepIntent
trains an icon-behavior model in an end-to-end manner.
5.2 Icon Feature Extraction
Each icon can be treated as a color tensor with fixed width, height,
and color channels. In this work, we adapt DenseNet [30], the state-
of-the-art image feature extraction model, to initialize the icon
features. Typically, DenseNet contains several dense blocks and
transition layers. We do not directly use the pre-trained DenseNet
model for two reasons. First, the pre-trained model is trained on
datasets such as CIFAR [34] and ImageNet [15], which contain
natural images taken from cameras, while icons are mostly artifacts.
Second, the pre-trained model considers images with 3 channels
(RGB) and ignores the alpha channel that describes the opacity
of the image. However, many icons use the alpha channel, and
ignoring this channel might mislead the learned features [80].
TextIconLearningText Feature ExtractionIcon Feature ExtractionFeature CombinationPermissionsBehaviorPredictionwhere ⃗hi , ⃗hi ∈ Rd /2 are the forward and backward features of vi,
respectively, and d/2 is dimension of the forward/backward feature