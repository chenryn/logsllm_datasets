a fault-tolerant distributed system where enclaves preserve the
counters freshness with 2 ms average latency.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:16:05 UTC from IEEE Xplore.  Restrictions apply. 
19
in the end. A sender-enclave (SE) sends the counter update
to all enclaves of the protection group. Receivers-enclaves
(REs) send back to the SE an echo-message which they store
along with the counter value in the protected memory. Once
the SE receives echo-messages from the quorum (q) it starts
a second round of echo-messages. Upon receiving back the
echo, each RE veriﬁes that the received counter value matches
the one it keeps in-memory and RE replies with a (N)ACK
message. After receiving q ACKs, the enclave seals its own
state together with the counter value to the persistent storage.
Secure persistency guarantees. TREATY’s attestation and its
secure LSM-data structure [31] ensure that TREATY maintains
its security properties after a crash as (1) only trusted nodes
obtain the encryption keys for the persistent storage, (2) nodes
perform integrity checks on accessed persistent data blocks
and, (3) at recovery, TREATY veriﬁes the logs’ freshness. As
the underlying cloud infrastructure is owned by a third-party,
TREATY detects but cannot prevent unauthorized modiﬁcations
to persistent state.
Stabilization protocol correctness. TREATY stabilization pro-
tocol remains correct as TEEs guarantee its correct execution
on all nodes. Any faults, e.g., crashes or network partitions,
can only affect availability. While TREATY’s trusted counter
offers crash fault tolerance, CAS can be a single point of
failure. In case CAS fails, crashed nodes cannot recover.
VII. TRUSTED SUBSTRATE FOR DISTRIBUTED TXS
To support secure Tx processing, we design the following
four cross-layer subsystems for our trusted substrate: a secure
network library (§ VII-A), a secure storage engine for Txs
based on Speicher [31] (§ VII-B), a userland thread scheduler
(§ VII-C), and a memory allocator for Tx buffers (§ VII-D).
A. Network Library for Txs
To implement TREATY’s 2PC, we build a secure networking
library that implements asynchronous remote procedure calls
(RPCs) for Txs execution. Our network library relies on
eRPC [36], but we had to extend and adapt the codebase
to (i) overcome the architectural limitations of TEEs (I/O,
enclave memory and DMA-ed memory) and, (ii) ensure con-
ﬁdentiality, integrity and freshness for the over-the-network-
communication in the presence of malicious attackers.
Architectural limitations of TEEs. To avoid the execution of
expensive syscalls for network I/O, we adapt eRPC with
DPDK as the transport layer. DPDK offers direct I/O, bypass-
ing the kernel and eliminating the syscalls overheads using
userspace drivers and polling.
To secure the software stack, we build eRPC/DPDK with
SCONE assuring that the device’s DMA mappings reside in the
host memory, thus accessible by both enclave and NIC. We
achieve this overwriting the mmap() of SCONE to bypass its
shield layer and allow the allocation of untrusted host memory
as well as the creation of memory mappings to the hugepages.
Furthermore, we change the library’s memory allocator to
place all message buffers in the host memory (in hugepages
of 2 MiB), thus reducing the EPC pressure at the cost of en-
crypting them. While eRPC by default creates shared memory
regions for message buffers in hugepages, a naive port of
eRPC with SCONE allocates all of these buffers inside the
enclave triggering the costly EPC paging. Lastly, we eliminate
rdtsc() calls to reduce the number of OCALLs from the
hot path by replacing the call with a monotonic counter.
Message layout. TREATY’s networking library constructs a
secure message to guarantee the integrity and conﬁdential-
ity of messages through a en-/decryption library based on
OpenSSL [73]. Additionally, we ensure freshness, i.e., at-most
once execution semantics for Txs’ execution. The message
is comprised of a 12 B Initialization Vector (IV), a payload
of 4 B (for memory alignment), a 80 B Tx metadata and Tx
data that contains the size of the data and the size of the key
and/or value followed by the key and/or value. The message
is followed by a 16 B MAC. MAC and IV are necessary
to prove the authenticity and integrity in the remote host.
Only the metadata and data are encrypted; in case IV or
MAC are compromised the integrity check will fail. The
metadata contains the coordinator node’s id (8 B) and the Tx
id (64 B), monotonically incremented in the coordinator node.
Both are necessary for uniquely identifying the transaction
in the recipient side. The operation identiﬁer (8 B) is also
unique for each Tx request. This unique tuple of the node’s, Tx
and operation ids ensures that an operation/Tx is not executed
more than once. Therefore, along with the two-phase locking
which ensures that only one Tx can modify a resource, nodes
can verify that no already executed Txs are processed again.
Similarly, the participants’ reply, except for the ACKs, also
include the coordinator’s node, Tx id and the operation id.
TREATY’ networking protocol enqueues requests, e.g., a
user-deﬁned message, that triggers a request handler for this
request type in the remote machine. The execution returns after
enqueuing the request. The node can enqueue more requests
or process received ones. Once the request is processed in
the remote machine, the receiver replies back to the sender.
A continuation function is triggered in the host machine to
notify that the request has been completed. The sender can
now deallocate any related resources, e.g., message buffers.
B. Storage Engine: Extensions to SPEICHER for Txs
To offer persistent Txs in TREATY, we extend SPEICHER’s
storage engine/controller [31] to support single-node pes-
simistic and optimistic transactions as discussed in § V-B.
Additionally, we implement an extra persistent log ﬁle, the
Clog. Clog’s entries are similar to MANIFEST and WAL
entries format; they are comprised of a counter value, the
encrypted Tx data and metadata and a cryptographic hash.
Clog’s deletions are also logged in the MANIFEST. Clog is
thread-safe; coordinators append independently their entries.
In TREATY, we allow group commits for Txs to ﬂush bigger
data blocks to the persistent storage and optimize the SSD
throughput. Each group elects a leader that merges their and
all followers’ Txs buffers into a larger buffer. The leader
then writes this buffer into WAL and MemTable. We further
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:16:05 UTC from IEEE Xplore.  Restrictions apply. 
20
defer logging (yield) at commit, allowing us to format group
commits of bigger data blocks. For the LSM structures, we
implement a MemTable skip list that supports parallel updates
for concurrent Tx processing.
Lastly, we change the I/O sub-system of SPEICHER, where
we replace the SPDK-based direct I/O for accessing the SSDs
with async syscalls to optimize the usage of cores for our
eRPC/DPDK-based networking library.
C. Userland Scheduler
Timer based scheduling in the enclave is extremely expen-
sive, as it involves interrupts that result in world switches.
While SCONE implements its own userspace scheduler, it is
non-preemptive relying on threads to either go to sleep or issue
syscalls for ensuring progress. This design is not well-suited
for TREATY; (i) our direct I/O networking library leads to
starvation and high latency, and (ii) in the presence of multiple
clients creating too many threads is inefﬁcient.
We overcome these by implementing a userland scheduler
on top of SCONE’s scheduler. Precisely, each thread spawns
one userland thread (ﬁber) for each connected client. Our
userland scheduler implements a per-core round-robin (RR)
algorithm for ﬁbers’ scheduling and a set of queues (run queue
and sleeping/waiting queue) for the ﬁbers.
When a ﬁber needs to block, e.g., acquiring a lock, wait-
ing on condition variables or sleeping, TREATY’s userland
scheduler places the ﬁber into a sleeping queue. It picks
and schedules the next eligible ﬁber from the run queue
(based on the RR algorithm). Our userland scheduler does not
involve interrupts, syscalls and context/world switches when
scheduling another ﬁber. Lastly, we adapted our scheduler to
frequently yield threads allowing SCONE to schedule others.
Precisely, if no ﬁber is in a running state, our scheduler sleeps;
thereby invoking a syscall. Our scheduler’s sleep function
yields to another SCONE thread and increases the amount of
time before future yields are triggered. In this way, ﬁbers allow
us to both maximize CPU utilization and increase scalability.
Our userland scheduler’s implementation is based on
Boost [74]. We conﬁgure SCONE with 8 kernel and 8 ap-
plication threads each spawning one ﬁber per client.
D. Memory Management
We minimize EPC usage or paging; TREATY’s in-memory
data structures are divided between the enclave and untrusted
host memory. All network buffers are kept in host memory at
the cost of encryption. Note that transmission is asynchronous
so heavy network trafﬁc could exceed EPC limit and trigger
paging if the message buffers were allocated in the enclave.
TREATY’s engine keeps the updates of uncommitted in-
progress Txs into local buffers. We implement Txs’ buffers as
a stream of bytes (std::string) that allocate continuous
memory to eliminate paging.We also explored the case to
adopt a design similar to the MemTable for Txs buffers, where
we keep only the keys in the enclave (for the read-my-own
writes semantics). However, we decided against it as it does
not offer any performance improvements; at commit, we still
need to perform integrity checks, re-collect and encrypt all the
KV pairs in the enclave memory for logging. We implement a
scalable memory allocator for host and enclave memory that
relies on a mempool. It assigns threads to different heaps based
on the hash of the get_id() and recycles unused memory,
drastically reducing the amount of mapped memory.
Implementation details. We implement TREATY in C/C++;
4000 LoC for the 2PC, encryption library and modiﬁcations to
eRPC, DPDK, boost and SPEICHER codebases. We use Java
and Rust for the workload generator and CAS respectively.
VIII. EVALUATION
A. Experimental Setup
Testbed. We perform our experiments on a real hardware
testbed using a cluster of 6 server machines. We run TREATY
on 3 SGX server machines with CPU: Intel(R) Core(TM) i9-
9900K each with 8 cores (16 HT), memory: 64 GiB, caches:
32 KiB (L1 data and code), 256 KiB (L2) and 16 MiB (L3).
TREATY nodes are connected over a 40GbE QSFP+ network
switch. Clients generate workload on 3 machines and are
connected with TREATY over a secondary 1Gb/s NIC.
Benchmarks/workloads. We evaluate TREATY’s 2PC w/o any
underlying storage (§ VIII-B). For the distributed (§ VIII-C)
and single-node (§ VIII-D) Txs evaluation, we use YCSB [38]
and TPC-C [37]. We conﬁgure TPC-C with 10 Warehouses,
as in [75]. For distributed Txs, we also run a TPC-C workload
with 100 Warehouses. Lastly, we evaluate the network stack
(§ VIII-E) by stress-testing the network using: (i) iPerf [76]
(implemented w/ kernel-sockets), and (ii) our own server/client
application, build with eRPC [36],
implements iPerf.
Unless stated otherwise, we refer to overheads for throughput
(tps).
that
B. TREATY’s 2PC Protocol
We evaluate TREATY’s 2PC protocol designed over eRPC
with the YCSB workload (50 %R-50 %W). 2PC runs without
any underlying storage to isolate the protocol’s overheads. We
compare two Secure (w/ SCONE) versions of TREATY 2PC
with and w/o Enc(ryption) against two Native executions of
the protocol with and w/o Enc(ryption) respectively. All four
versions “saturate” with 300 clients, each of which executes a
YCSB workload (10 Ops/Tx, 1000 B value size).
Figure 4 shows the slowdown in the throughput of 3
versions of TREATY’s 2PC protocol (Native 2PC w/ Enc,
Secure 2PC w/o Enc, Secure 2PC w/ Enc) normalized to a
native, non-secure version of 2PC. Some Tx’s operations might
be served by the coordinator node; therefore not all opera-
tions are sent thought the network to participants and thus,
be en-/decrypted. Our evaluation shows minimal encryption
overhead in the native case. Further, TREATY’s secure 2PC
w/o Enc experiences 1.8× slowdown w.r.t. a native execution
while encryption (Secure 2PC w/ Enc) increases the overheads
leading to a 2× slowdown in comparison with native 2PC.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:16:05 UTC from IEEE Xplore.  Restrictions apply. 
21
Figure 3: Performance evaluation of distributed transactions under two TPC-C
workloads with 10W and 100W respectively.
Figure 4: Throughput slowdown of
three versions w.r.t. Native 2PC.
scale more than 60 clients. Therefore, TREATY is over satu-
rated in the benchmark, explaining the higher latency values.
TPC-C (10W). Figure 3 (left) shows the throughput overheads
and the latencies of three versions of TREATY (all run in
SCONE) w.r.t DS-RocksDB under TPC-C with 10 Warehouses.
TREATY is 8×—11× slower compared to the native, non-
secure DS-RocksDB. This conﬁguration presents heavy W-W
conﬂicts; DS-RocksDB achieves 780 tps. Consequently, DS-
RocksDB, TREATY w/o Enc and TREATY w/ Enc cannot scale
for more than 10 clients. However, TREATY w/ Enc w/ Stab
scales up to 16 clients as the stabilization period (where locks
are released) allows the system to serve more requests.
TPC-C (100W). Figure 3 (right) shows the throughput over-
heads and the latencies of three versions of TREATY (all
run w/ SCONE) w.r.t DS-RocksDB under TPC-C with 100
Warehouses (total worksize equals to 10GB divided equally
to all 3 nodes). This conﬁguration presents less conﬂicts
than the previous case; DS-RocksDB achieves 1200 tps. Our
evaluation shows reasonable overeheads (4×-6×) and similar
behavior for TREATY w/ Enc and Stab; while all the three
other systems (DS-RocksDB, TREATY w/ Enc, TREATY w/o
Enc) are saturated with 60 clients, TREATY w/ Enc w/ Stab
is saturated with 84 clients.
D. Single-node Transactions
Baselines and setup. We evaluate the performance of pes-
simistic and optimistic single-node Txs with TPC-C and
YCSB. TPC-C is conﬁgured with 10 Warehouses as in [75]
and YCSB with: 10 ops/Tx, value size to be equal to 1000 B,
uniform distribution with 10 k unique keys. For the pessimistic
Txs, we measure the performance against read-heavy (80 %R-
20 %W) and write-heavy (20 %R-80 %W) workloads, while
for the optimistic Txs we use the read-heavy workload. Our
experiments stress-test EPC usage since both TREATY and
RocksDB do not support in-place updates. We evaluate the
throughput (tps) and latency for 6 versions of the single-
node TREATY; (i) RocksDB, (ii) Native TREATY, (iii) Native
TREATY w/ Enc, (iv) TREATY w/o Enc (SCONE), (v) TREATY
w/ Enc (SCONE) and (vi) TREATY w/ Enc w/ Stab (SCONE).
Results. Pessimistic Txs. Figure 6 shows the throughput and
latency of the TPC-C for the pessimistic Txs. TREATY ex-
ecuted natively (Native TREATY) performs equivalently to
RocksDB. Additionally, we deduce that Native TREATY w/
Figure 5: Performance evaluation of distributed Txs under a
W-heavy (20 %R) and a R-heavy (80 %R) YCSB workload.
C. Distributed Transactions
Baselines and setup. We evaluate the performance of dis-
two TPC-C workloads, with 10 and
tributed Txs under
100 Warehouses, and two YCSB workloads:
read-heavy
(80 %R) and write-heavy (20 %R). We show the overheads of
TREATY’s throughput normalized w.r.t. a native execution of
2PC with RocksDB as the underlying storage (DS-RocksDB).
We study the performance behavior of three systems: (i)
TREATY w/o Enc, (ii) TREATY w/ Enc and (iii) TREATY w/
Stab(ility) w/ Enc. All three versions run with SCONE and our
TREATY’s secure storage system.
Results. YCSB. Figure 5 (left) shows the throughput slow-
down of the three systems w.r.t. DS-RocksDB. TREATY’s per-
formance is 9×—15× worse compared to DS-RocksDB where
SCONE overheads fast dominate the performance (TREATY
runs w/ and w/o Enc have little differences). For the W-heavy
workload, DS-RocksDB achieves 18.5 ktps. All four systems
are saturated with 96 clients equally divided across all three
machines (each serving 32 clients). Distributed Txs require
both participants and coordinator to stabilize their entries
and therefore, TREATY rollback protection increases latency
further for write-heavy Txs, as shown in Figure 5 (right).
For the R-heavy workload, TREATY w/ Enc slows down
the execution 11× while the un-encrypted version of the
system shows a slowdown of 9.5×, both compared to native
DS-RocksDB (24 ktps). Encryption overheads are reasonable;
reading from SSTables requires integrity checks as well as
proving the freshness of the entry. All four systems present
different scaling capabilities. DS-RocksDB and TREATY w/o
Enc scale up to 92 clients while encrypted versions cannot
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:16:05 UTC from IEEE Xplore.  Restrictions apply. 
22