### Quorum and Consistency Requirements

- **Quorum:**
  - \( f + 1 \) replicas
  - 1 replica + middlebox
  - \( f + 1 \) replicas

- **Consistency:**
  - Strong
  - Weak
  - Strong

### Read Optimization Approaches

**Prophecy [5]** is a middlebox-based approach that mimics clients towards the BFT replicas, designed to improve the performance of read-heavy workloads. **Troxy** is another implementation. Table I summarizes these three implementations in terms of their read optimization approaches and consistency levels.

#### Baseline Protocol (PBFT-like)

The baseline protocol optimistically executes non-ordered read requests and accepts a result as soon as \( f + 1 \) identical replies are received. If a quorum fails due to concurrent write operations, the client must resend the request and ask for regular ordering to enforce linearizability.

#### Prophecy

Prophecy deploys a cache in a middlebox between the client and the replicas. This cache stores the results of ordered reads to reduce the execution cost of read requests with large payloads for read-heavy applications. It requires only one reply from a randomly chosen replica to be compared with the cached result. However, it trades strong consistency for higher throughput, as the reply of a read operation reflects the state of the latest read, potentially returning a stale but correct result to the client.

#### Troxy

In contrast, Troxy actively manages a fast-read cache to reflect the state changes of the latest write, ensuring strong consistency.

### Experimental Setup

- **Baseline:** JMeter runs on the same machine as the client-side library, using a local socket connection for message forwarding.
- **Prophecy:** JMeter runs on a separate machine, establishing a secure socket connection to the client machine where the middlebox is located.
- **Troxy:** JMeter can directly connect to the replicas without modifications.
- **Jetty (v9.4):** A standalone version of the HTTP service is run to measure its original performance.

### Network Conditions

- **Local Network:** No additional delay.
- **Network Delay:** 100 Â± 20 ms.

### Request Characteristics

- **GET and POST Requests:** Payload size of 200 B.
- **Response Message Size:** Ranges between 4 KB and 18 KB.

### Latency Results

The average latency to execute requests is reported in Figure 11. In both scenarios, the standalone implementation (Jetty) indicates the original performance of the service.

- **Local Network:**
  - Both the baseline and Troxy maintain low latency, with an overhead of at most 1.8 ms.
  - Prophecy's two socket connections contribute to a latency almost twice as high.

- **Network Delay:**
  - The baseline's latency increases dramatically due to the network delay between the client and the replicas.
  - For Prophecy and Troxy, the extra round-trip impact is negligible because their voters are close to the replicas (on the middlebox machine and in the fast-read cache on a replica, respectively).

### Conclusion

In a wide-area network, using Troxy-backed BFT systems is beneficial for user-facing legacy applications.

## Related Work

### Traditional BFT State Machine Protocols

Traditional BFT state machine protocols involve libraries attached to both the client and server. The client-side library handles service invocation, message transfer, and reply voting. In contrast, Troxy provides a transparent and secure connection between the client and the replicated service by leveraging trusted computing technology, hiding the complexity of the fault-tolerant system from the clients.

### Trusted Subsystems in BFT Systems

- **A2M-PBFT [15]:** Uses a trusted append-only log, reducing the number of required replicas from \( 3f + 1 \) to \( 2f + 1 \).
- **TrInc [22]:** Provides trusted counters as a less complex replacement for the trusted log of A2M-PBFT.
- **MinBFT and MinZyzzyva [14]:** Use a counter-based trusted subsystem.
- **Hybster [13]:** Based on trusted counters and \( 2f + 1 \) replicas, it improves performance through consensus-oriented parallelization.
- **CheapBFT [21]:** Saves resources by exploiting passive replication, where \( f \) out of \( 2f + 1 \) replicas remain passive.
- **V-PR [23]:** Employs trusted computing technology to design a fully-passive replicated system for tolerating Byzantine failures.

### Other Transparent Systems

- **Prophecy [5]:** Executes a special component between the client and the server, requiring a large trusted computing base.
- **SPARE [43]:** Transparent to clients by locating the reply voter on the server side, but has a large trusted computing base.
- **Thema [44] and BFT-WS [45]:** Extend the classic approach with an additional web-service library, addressing an orthogonal problem.
- **Avoine et al. [46]:** Present a deterministic fair exchange algorithm running in untrusted hosts with security modules.

### SGX-Based Systems

Several systems utilize Intel SGX for secure computing in cloud environments, application-level secure data processing, and trusted client-side computing. To our knowledge, none of these systems have used trusted execution to enable compatibility with legacy systems as proposed by Troxy.

## Conclusion

We presented Troxy, a system that leverages trusted execution environments to offer clients transparent access to BFT systems. Unlike traditional BFT systems, Troxy does not require a special library at the client side. Instead, it implements a substitute inside each replica and introduces a managed fast-read cache to accelerate read-heavy operations while providing strong consistency guarantees. Our evaluation shows that Troxy outperforms a state-of-the-art hybrid BFT protocol by 130% for larger, read-heavy workloads and realistic network delays, introducing negligible latency overhead and being transparent to legacy clients.