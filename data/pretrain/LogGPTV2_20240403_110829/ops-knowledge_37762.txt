User: Write a simple crawler program in Python, automatically crawl the data of the specified website and store it in the database.
Assistant: The following is a simple Python crawler program example, which can crawl the article title, author and release date of the specified website, and store this information in the MySQL database: ```python import requests from bs4 import BeautifulSoup import pymysql # request web content response = requests.get("https://www.example.com/") soup = BeautifulSoup(response.content, 'html.parser') # Find HTML tags for all article titles, authors and publication dates title_tags = soup. find_all("h2", class_="article-title") author_tags = soup.find_all("span", class_="author") date_tags = soup.find_all("span", class_="date") # store the title, author and release date into database conn = pymysql.connect(host='localhost', port=3306, user='root', password='123456', db='test') cursor = conn.cursor() for i in range(len(title_tags)): title = title_tags[i].text.strip() author = author_tags[i].text.strip() date = date_tags[i].text.strip() cursor.execute(" insert into articles(title, author, date) values (%s,%s,%s)", (title, author, date)) conn.commit() cursor.close() conn.close() ``` Required Installed libraries: - requests - beautifulsoup4 - pymysql In the above sample program, we first request the target website through the requests library, and then use the find_all() method provided by the BeautifulSoup library to find the HTML tags of all article titles, authors, and release dates on the page, and Use the pymysql library to connect to a MySQL database and store the title, author, and publication date into the database in a loop. Please note that this is just the simplest sample program, and it needs to be modified and adjusted according to the specific conditions of the target website (such as adding exception handling, anti-crawler mechanism, etc.) in actual use.