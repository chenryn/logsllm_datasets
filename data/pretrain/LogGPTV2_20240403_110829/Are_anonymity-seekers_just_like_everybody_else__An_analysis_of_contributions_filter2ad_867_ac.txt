part of the revert action. Of the 2,124 reverts where the person
doing the reverting provided an edit summary, 162 (7.6% of
reverts giving a reason) referred to conditions relevant to being
a Tor user, with one or more of the following keywords: “Tor,”
“sock,” “block”, “ban” (referring to the ban policy of Tor IPs),
“proxy,” “masked,” “puppet,” “ip hopper,” “no edit history,”
“multiple IP,” “dynamic IP,” or “log in” (as in “please log in”
or “you can’t log in”). To the degree that other community
members were more suspicious of Tor editors, reversion rate
may be underestimating the quality of their contributions.
B. Revert actions and their success rate
A study of contributions to Wikipedia by Javanmardi et
al. [19] showed that IP editors’ contributions were twice as
21A Bonferonni correction for tests against our three comparisons groups
results in an adjusted threshold of α = 0.017. We use this threshold when
reporting statistical signiﬁcance throughout. It is worth noting that because
many of our ﬁndings are null results, an unadjusted α = 0.05 threshold is
more conservative.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:17:39 UTC from IEEE Xplore.  Restrictions apply. 
191
REVERT ACTIONS AND REVERT SUCCESS RATE
TABLE II
Revert actions1 Reverts kept2
333 (29.41%)
291 (70.80%)
254 (63.81%)
1,049 (88.22%)
Group
Tor editors
IP editors
First-time editors
Registered editors
1 Revert actions: Edits that revert other edits.
2 Reverts kept: Revert actions that are not reverted by other edits.
3 Non-reverts kept: Edits that do not revert other edits and do not get reverted.
1,132
411
398
1,189
Non-revert actions
6,619
10,040
7,878
5,932
Non-reverts kept3
4,224 (63.81%)
7,117 (70.88%)
5,095 (64.67%)
5,751 (96.94%)
a Tor user reverting warning messages posted by Wikipedia
administrators about vandalism. Unsurprisingly, 169 out of 180
Tor edits that were involved in edit wars were reverted as part
of the back-and-forth conﬂict.
This is a conservative measure of edit warring by Tor users.
Because of the dynamic nature of Tor IP addresses, Tor users
can simply change to a different exit address to avoid being
ﬂagged by automated tools enforcing 3RR. As a result, we
expanded our search to ﬁnd any series of more than two reverts
made on a single page within 24-hour period from any Tor
IP address. We found 546 total revisions, with 102 potential
incidents in violation of the 3RR. Our manual inspection of
dozens of these incidents suggests that, even when reverts are
made from different Tor exit node IPs, pages were typically
reverted to an older revision made by another Tor IP. This
suggests it was the same person using different exit nodes
making these reverts. Once again, the chance of these reverts
on article pages staying untouched was unlikely and 88.2% of
them were ultimately reverted.
Because our Tor dataset includes the entire population of
Tor edits, we could conduct an analysis of Tor being used
to violate 3RR. Because our comparison sets are random
samples, they are unlikely to contain consecutive edits made
by the same user. To obtain some estimate of the rate at which
other populations violate the 3RR, we retrieved all Wikipedia
reverts made within the 48-hour period following each revert
in all three of our comparison groups. Similar to ﬁndings
in previous research, we found that other user groups are
extremely unlikely to violate the 3RR policy [39]. In stark
contrast to our Tor edits, we detected only 13 violations of
the 3RR across all three comparison groups. This relatively
widespread rate of edit wars among Tor edits reﬂects the most
important difference between Tor editors and our comparison
groups identiﬁed in our analyses.
C. Measuring contribution quality using persistent token re-
visions
Although an edit is only treated as an identity revert if
it returns a page to a state that is identical to a previous
state, contributions might also be removed through actions
that add other content or change material. As a result, reverts
should be understood as a particular and very conservative
measure of low-quality editing. A more granular approach
to measuring edit quality involves determining whether the
parts of a contribution continue to be part of the article over
multiple future revisions. According to Halfaker et al. [16], the
survival of content over time can give important insights about
a contribution’s resistance to change and serves as a measure
of both productivity (how much text was added) and quality
(how much was retained) for a given revision.
Our approach used the mwpersistence23 library to calculate
the number of words or fragments of markup (“tokens”) added
to the articles in a given edit and then to measure how many
of these tokens persist over a ﬁxed window of subsequent
edits. Following previous work, our measure of persistent
token revisions (PTRs) involves collapsing sequential edits by
individual users and then summing up every token added in
a given revision that continues to persist across a window of
seven revisions [29]. This measure only takes non-revert edits
into account because revert actions always have 0 PTR.
Fig. 4 describes the contribution quality of non-revert revi-
sions estimated by measuring PTRs for each edit between 2007
to 2013. We used a box plot to depict the distribution of PTRs
for edits made in each year. Apart from Registered editors,
the minimum value and the 25% quartile of other groups
are all 0. This reﬂects the fact that many edits to Wikipedia
remove tokens instead of adding them and lead to a PTR count
of 0. Edits that are entirely reverted also have a count of
0. The medians of the ﬁrst three groups are relatively low,
mostly within the range of 0 to 10 tokens. Registered editors’
medians are higher, within the range of 10 to 40 tokens. The
interquartile regions (IQRs) in the plots of Tor editors are
slightly higher than those of IP editors and are comparable to
those of First-time editors. The triangles on the graph display
the mean PTR each year. Tor editors have some exceptional
contributions outside the 95% interval, which increases this
mean value. Overall, we calculated the mean number of PTRs
contributed by Tor editors as 547, by IP editors as 282, by
First-time editors as 456, and by Registered editors as 836.
Mann-Whitney U-tests suggest that Tor-edits have signiﬁcantly
higher PTRs than IP-edits (U = 18158458, p < 0.01) and
First-time edits (U = 14104155, p < 0.01), but signiﬁcantly
lower than Registered edits (U = 3095249, p < 0.01). This
provides evidence that contributions coming from Tor nodes
have relatively signiﬁcant value in terms of both quantity and
quality as measured by PTRs.
23https://pythonhosted.org/mwpersistence/
P2F9-CA28)
(Archived:
https://perma.cc/
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:17:39 UTC from IEEE Xplore.  Restrictions apply. 
192
Fig. 4. Measurement of PTRs of different groups of non-revert edits over time. The rectangle is the interquartile region (middle 50% of the population), with
an orange line at the median. The upper and lower whisker represent the range of the population that is 1.5 times above and below the interquartile range.
The green triangle is the mean, and the circles indicate individual observations falling outside the limit.
D. Analysis of reversion rate and persistent token revisions
after 2013
As described in §V, Wikipedia’s effort to block Tor users
made it much harder for an edit to slip through by the end
of 2013. In this section, we consider the small number of
edits made in this later period. Using the methods described
above, we computed the reversion rate and the PTRs for the
population of 536 edits made after 2013 along with the same
number of time-matched edits from other groups as described
above. The results of this analysis are reported in Table III.
Compared to the number we see from the 2007–2013
period, Tor’s reversion rate decreased from 42.1% in the
period before December 2013 to 28.2% afterward. Two other
comparison groups (IP editors and First-time editors) also
exhibit a decline in the rate of edits being reverted. This
reﬂects the fact that reversion rates have been in decline in
Wikipedia over time in general.24 Due to the small number of
edits each year, we were unable to properly observe whether
the change happened gradually or as a result of Wikipedia’s
more effective quality-checking methods. Overall, the rever-
sion rates of Tor editors are now statistically comparable
to IP editors (z = 0.89; p = 0.19), and First-time editors
(z = −0.67; p = 0.25). In terms of revert actions, we see a
signiﬁcant decline in the number of revert actions that Tor
editors took (z = −2.5; p < 0.01) as well as in all our
comparison groups. Overall, Tor editors’ revert rate in the later
period are comparable to that of IP editors (z = 1.2; p = 0.10)
and that of Registered editors (z = 1.83; p = 0.03), but still
higher than that of First-time editors (z = 2.97; p < 0.01).
Our measure of PTR also suggests that Tor editors are at
least as high quality as IP editors and First-time editors in
the post-2013 period. Mann-Whitney U tests suggests that Tor
edits made after 2013 are of similar quality to edits by IP
editors (U = 48142; p = 0.118), of greater quality than edits
by First-time editors (U = 49692; p < 0.01), but are of lower
quality than those by Registered editors (U = 9684; p = 0.02).
This ﬁnal difference is not statistically signiﬁcant after a
Bonferroni adjustment for multiple comparisons. While it is
24https://stats.wikimedia.org/EN/EditsRevertsEN.htm (Archived: https://
perma.cc/7WY8-MS6P)
clear that contributions from Tor users signiﬁcantly improve
in many aspects after 2013, we also observe a similar pattern
in IP editors and First-time editors. As a result, it is hard to
argue that the increasing effectiveness of TorBlock extension
is the sole reason for this change.
E. Measuring quality through manual labelling
Perhaps the most compelling way to assess the quality of
Tor edits is to categorize edits manually. To do so, we con-
ducted a formal content analysis of edits. Two of the authors
and two colleagues conducted a content analysis following
guidelines laid out by Neuendorf [30] to code revisions as
Damaging or Non-Damaging. To ensure that we had a large
enough sample, we ﬁrst conducted a simulation-based power
analysis, which indicated that a sample of 850 edits in each
group would be necessary to detect an underlying difference
of 7% in the proportion of damaging edits between groups
at the α = 0.05 conﬁdence level.25 The team developed a
codebook, and after conducting three rounds of independent
coding followed by discussion of codes to develop a shared
understanding and deﬁnitions, we drew a year-matched ran-
dom subsample of 999 edits from our sample of Tor edits and
the three comparison datasets.
We deﬁned damaging edits as those we would want to
remove from the encyclopedia because they diminished the
usefulness of the resource by being incorrect, sloppy, a
violation of Wikipedia style, or by otherwise causing the
article to be less encyclopedic. Some edits were observed to
contain both mistakes and positive contributions. We used our
judgment to assess whether the contribution was generally
positive and worthwhile, despite being imperfect. When we
did not see evidence that led us to suspect that an edit was
damaging, we followed Wikipedia’s convention of assuming
good faith and coded it as Non-Damaging.
Edits were presented to coders as a “diff” that showed what
was changed using the same interface that Wikipedia con-
tributors can use to review contributions and were presented
in a randomized order using ﬁltering software to suppress
identity information about contributors. Coding was conducted
without reference to other contextual information, including
25A power analysis requires a minimum effect size, and we chose 7%.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:17:39 UTC from IEEE Xplore.  Restrictions apply. 
193
REVERT AND PTR ANALYSES OF EDITS MADE AFTER 2013
TABLE III
Reversion rate
Revert actions
Mean of PTRs
Median of PTRs
Tor editors
28.2%
38 (7.0%)
645
12
IP editors
25.0%
28 (5.2%)
162
6
First-time editors
30.0%
10 (1.8%)
310
0
Registered editors
5.7%
24 (4.5%)
3121
18
RESULTS FROM LOGISTIC REGRESSIONS OF HAND-CODED QUALITY
ASSESSMENTS OF EDITS. TOR EDITORS SERVED AS THE OMITTED
TABLE IV
CATEGORY.
Intercept
First-time Editors
IP-based Editors
Registered Editors
AIC
BIC
Log Likelihood
Deviance
Num. obs.
∗
Non-Damaging
0.85∗
−0.25∗
[0.71; 1.00]
[−0.46; −0.05]
[−0.12; 0.31]
0.10
1.69∗
[1.39; 1.99]
3551.08
3575.53
-1771.54
3543.08
3337
indicates that 0 is outside the 95% conﬁdence interval.
subsequent or previous edits. Four coders conducted inde-
pendent coding and discussion of codes over several rounds.
Subsequently, they classiﬁed a dataset of 160 edits (40 from
each group) and compared their results (10 assessments were
missing from one coder). This result was a good level of
inter-rater reliability across the four coders (raw agreement of
89%; pairwise agreement of 80%; Gwet’s AC of 0.68).26 Full
agreement is unlikely because our protocol required coders to
rely on their judgement and knowledge to detect things like
misinformation without recourse to any outside information.
The full hand-coded sample includes the consensus rating of
the 160 edits evaluated in the pilot plus 800 random edits
drawn from subsamples described earlier that were coded by
each of three researchers and 840 edits coded by the fourth.
We omitted 30 revisions from our ﬁnal analysis because they
were missing or otherwise deleted from Wikipedia.
The results of the from logistic regression using Tor-based
edits as the baseline are reported in Table IV. We found that
70.1% of edits made by Tor-based editors were coded as
Non-Damaging, while 72.1% of edits by IP-based editors and
64.6% of edits by First-time editors were. Although slightly
higher and lower respectively, our model suggests that the pro-
portion of Non-Damaging edits was not statistically different
than our sample of Tor edits in these two comparison groups.
We found that 92.7% of edits by Registered editors were
Non-Damaging—a statistically signiﬁcant difference from our
sample of Tor edits.
26Gwet’s AC was used because it is a measure of multi-rater reliability
robust to variation in the distribution of units that raters encounter [32].
VI. CLASSIFICATION OF EDITS USING MACHINE
LEARNING TOOLS
A. Measuring contribution quality using ORES