during a deployment, constraints such as those on the rejection
rate will be surpassed to some degree. This is the desired
outcome—so long as the performance on rejected elements
remains low (i.e., they would likely be misclassiﬁed) we would
rather reject drifting examples.
Figure 12 presents an alternative to the optimization used
in our previous experiments which is more appropriate if the
rejection rate must be kept low. By ﬁnding thresholds that
minimize the rejection rate on the calibration set with F1-
Score no less than 0.8, during deployment the rejection rate
stays much lower, consistently staying below 10% even as
the drift increases. Similar to how the rejection rate begins
close to the calibration constraint and then increases in our
previous experiments, in this setting the F1 begins close to the
calibration constraint, and then decreases. The overall effect
here is that the ICE is more conservative in its rejections:
while the F1 of kept elements decreases as more incorrect
predictions are accepted, the ICE rejects only those predictions
that are most likely to be incorrect, keeping the F1 of rejected
elements at 0.
In summary, to estimate how many rejections will be accept-
able, we advise practitioners to consider the expected volume
of incoming samples, the available resources for processing
quarantined examples, and the lifetime of the classiﬁer before
being retrained (as drift will likely increase during this period).
Next they should identify which metrics are most important,
or nonnegotiable, and use these to balance the threshold
optimization. As the emergence of concept drift will likely
result in the calibration values being surpassed, a ballpark is
more important than the exact values.
F. Formal Calibration Algorithms
We present algorithms for our random search calibration and
calibration and test procedures for TCEs, ICEs, and CCEs.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:16 UTC from IEEE Xplore.  Restrictions apply. 
821
14710131619222528313437404346Testing period (month)0.00.10.20.30.40.50.60.70.80.91.0F1 (baseline)F1 (kept)F1 (rejected)Rate of drifting malwareRate of drifting goodwareQuarantined(a) AUT(F1, 48m)
(b) AUT(Precision, 48m)
(c) AUT(Recall, 48m)
Fig. 13: AUT of performance metrics showing the effect of tuning the quorum size k of the majority vote in a CCE.
Algorithm 2: Transductive Conformal Evaluator (TCE
and approximate TCE)
Input: Z =(cid:72)z0, z1, . . . , zn−1(cid:73), n training examples
1 , . . .(cid:73), stream of test examples
Z∗ =(cid:72)z∗
A, NCM for producing nonconformity scores
k ∈ N, number of folds—TCE is approximate when
k  F (Y , ˆY , P ; t∗) and G(Y , ˆY , P ; t) ≥ C
then
else if F (Y , ˆY , P ; t) = F (Y , ˆY , P ; t∗) and
G(Y , ˆY , P ; t) > G(Y , ˆY , P ; t∗) then
counter ← counter + 1
t∗ ← t
6
7
8
9
10 end
11 return t∗
k−1 }
0, Z(cid:48)
1, . . . , Z(cid:48)
1 P ← 0
2 i ← 0
3 partition Z equally into Z part ← { Z(cid:48)
4 foreach partition Z(cid:48) of Z part do
5
6
7
Z(cid:48)(cid:48) ← Z \ Z(cid:48)
g ← Fit (Z(cid:48)(cid:48))
foreach z(cid:48) of Z(cid:48) do
(cid:46) Predicted label
ˆy ← g(z(cid:48))
(cid:46) Bag of examples with same label
Z(cid:48)
(cid:46) Nonconformity score
αz(cid:48) ← A(Z(cid:48)
(cid:46) Nonconformity scores for bag elements
ˆy ←(cid:72)z ∈ Z(cid:48) : z.y = ˆy(cid:73)
ˆy \(cid:72)z(cid:73)) : z ∈ Z(cid:48)
S ←(cid:72)A(Z(cid:48)
ˆy(cid:73)
(cid:46) Credibility p-value
pz(cid:48) ← |α∈S:α>=αz(cid:48)|
Pi ← pz(cid:48)
i ← i + 1
12
13
14
15
16 end
17 t∗ ←Transcend.FindThresholds (Z, ˆY , P )
8
9
10
11
ˆy, z(cid:48))
end
|S|
Test Phase
18 g ← Fit (Z)
19 foreach z∗ of Z∗ do
(cid:46) Predicted label for test example
ˆy ← g(z∗)
(cid:46) Bag of training examples with same label
Zˆy ←(cid:72)z ∈ Z : z.y = ˆy(cid:73)
S ←(cid:72)A(Zˆy \(cid:72)z(cid:73)) : z ∈ Zˆy(cid:73)
(cid:46) Nonconformity score
αz∗ ← A(Zˆy, z∗)
(cid:46) Nonconformity scores for bag elements
(cid:46) Credibility p-value
pz∗ ← |α∈S : α>=αz∗|
if Pz∗ =αz(cid:48)|
i ← i + 1
|S|
end
Gj ← Fit (Z \ Z(cid:48)
j)
j ←Transcend.FindThresholds (Z(cid:48)
T ∗
j, ˆYj, Pj)
Test Phase
17 s ← 0
18 foreach z∗ of Z∗ do
19
foreach j of { 0, 1, . . . , k − 1} do
(cid:46) Predicted label for test example
ˆy ← Gj(z∗)
(cid:46) Bag of training examples with same label
Z(cid:48)
(cid:46) Nonconformity score
αz∗ ← A(Z(cid:48)
(cid:46) Nonconformity scores for bag elements
j ˆy ←(cid:72)z ∈ Z(cid:48)
j : z.y = ˆy(cid:73)
j ˆy , z∗)
j ˆy \(cid:72)z(cid:73)) : z ∈ Z(cid:48)
S ←(cid:72)A(Z(cid:48)
j ˆy(cid:73)
|S|
(cid:46) Credibility p-value
pz∗ ← |α∈S : α>=αz∗|
(cid:46) Track positive evaluations
if Pz∗ ≥ T ∗
j ˆy then s ← s + 1
end
(cid:46) Majority vote for ﬁnal decision
if s =αz(cid:48)|
Pi ← pz(cid:48)
i ← i + 1
(cid:46) Predicted label
ˆy ← ˆYi ← g(z(cid:48))
(cid:46) Bag of examples with same label
Z cal
(cid:46) Nonconformity score
αz(cid:48) ← A(Z cal
(cid:46) Nonconformity scores for bag elements
11
12
13
14 end
15 t∗ ←Transcend.FindThresholds (Z, ˆY , P )
ˆy , z(cid:48))
|S|
10
8
9
Test Phase
16 g ← Fit (Z tr)
17 foreach z∗ of Z∗ do
(cid:46) Predicted label for test example
ˆy ← g(z∗)
(cid:46) Bag of training examples with same label
Z cal
(cid:46) Nonconformity score
αz∗ ← A(Z cal
(cid:46) Nonconformity scores for bag elements
ˆy ←(cid:72)z ∈ Z cal : z.y = ˆy(cid:73)
ˆy \(cid:72)z(cid:73)) : z ∈ Z cal
S ←(cid:72)A(Z cal
ˆy (cid:73)
ˆy , z∗)
(cid:46) Credibility p-value
pz∗ ← |α∈S : α>=αz∗|
if Pz∗ < t∗
|S|
ˆy then emit 0 else emit 1
18
19
20
21
22
23
24 end
7
8
9
10
11
12
13
14
15
16 end
20
21
22
23
24
25
26
27
28 end
823
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:16 UTC from IEEE Xplore.  Restrictions apply.