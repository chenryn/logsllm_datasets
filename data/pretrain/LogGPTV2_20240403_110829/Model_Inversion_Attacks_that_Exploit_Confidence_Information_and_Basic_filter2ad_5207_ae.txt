identiﬁed
excluded
overall
identiﬁed
excluded
(a) Average over all responses.
(b) Correct by majority vote.
(c) Accuracy with skilled workers.
Figure 9: Reconstruction attack results from Mechanical Turk surveys. “Skilled workers” are those who
completed at least ﬁve MTurk tasks, achieving at least 75% accuracy.
Target
Softmax
MLP
DAE
Figure 10: Reconstruction of the individual on the
left by Softmax, MLP, and DAE.
Figure 11: White-box MI vs. classiﬁcation accuracy
on decision trees trained on FiveThirtyEight data
with the sensitive feature at each priority level (cid:96).
For this data, the optimal placement of the sensi-
tive feature is at the ﬁrst level, achieving the best
classiﬁcation accuracy while admitting MI accuracy
only 1% greater than baseline.
The results are displayed in Figure 11 (we observed similar
trends for black-box performance). The eﬀectiveness of the
attack in this case is clearly aﬀected by the depth at which
the sensitive feature appears in the tree. When the feature
appears near the top or bottom of the tree, the attack fails
with greater probability than otherwise. Furthermore, al-
though prioritizing placement of the sensitive feature at a
particular level does impact model accuracy, there is an op-
timal placement in this case: when the feature is placed at
the top of the tree, classiﬁcation accuracy is maximized while
inversion accuracy is only 1% greater than baseline guess-
ing. This suggests that it may be possible to design more
sophisticated training algorithms that incorporate model in-
version metrics into the splitting criteria in order to achieve
resistance to attacks without unduly sacriﬁcing accuracy.
Figure 12: Black-box face reconstruction attack
with rounding level r. The attack fails to produce a
non-empy image at r = 0.1, thus showing that round-
ing yields a simple-but-eﬀective countermeasure.
To understand why attack performance is not monotone
in (cid:96), we counted the number of times each tree used the
sensitive feature as a split. This measure increases until it
reaches its maximum at (cid:96) = 8, and steadily decreases until
(cid:96) = 12. The diﬀerence in split frequency between (cid:96) = 8 and
(cid:96) = 12 is approximately 6×. This is most likely because once
most of the features have been used, the training algorithm
deems further splitting unnecessary, thus omitting the sen-
sitive feature from many subtrees. The inversion algorithm
is unable to do better than baseline guessing for individu-
als matching paths through these subtrees, thus making the
attack less eﬀective.
Facial Recognition. Our attacks on facial recognition
models are all based on gradient descent. One possible de-
fense is to degrade the quality or precision of the gradient
information retreivable from the model. There is no obvious
way to achieve this in the white-box setting while preserv-
ing model utility, but in the black-box setting this might be
achieved by reducing the precision at which conﬁdence scores
are reported. We tested this approach by rounding the score
produced by the softmax model, and running the black-box
reconstruction attack. The results are presented in Figure 12
for rounding levels r = {0.001, 0.005, 0.01, 0.05}; the attack
failed to produce an image for r = 0.1. “No rounding” corre-
sponds to using raw 64-bit ﬂoating-point scores to compute
numeric gradients. Notice that even at r = 0.05, the attack
fails to produce a recognizable image. This suggests that
black-box facial recognition models can produce conﬁdence
scores that are useful for many purposes while remaining
resistant to reconstruction attacks.
1234567891011120.80.820.840.860.880.9‘MIaccuracy1234567891011120.30.320.340.360.380.4Class.accuracyMIaccuracyClass.accuracynoroundingr=0.001r=0.005r=0.01r=0.0513317. RELATED WORK
Machine-learning techniques are used in a variety of ap-
plications, such as intrusion detection, spam ﬁltering, and
virus detection. The basic idea behind these systems is to
train a classiﬁer that recognizes “normal behavior”, so that
malicious behaviors as can be labeled as abnormal. An ac-
tive adversary can try to subvert these systems in a variety
of ways, such as crafting malicious inputs that the classiﬁer
mistakenly labels as normal. This is called an evasion or
mimicry attack. An adversary can also try to degrade the
performance of such systems by devising inputs that cre-
ate a large number of false alarms, thus overwhelming the
system administrator. This is a variant of classic denial-of-
service attacks. Barreno et al. [3] consider such attacks on
machine learning algorithms. Lowd and Meek [30] extend
these attacks to scenarios where the attacker does not have
a complete description of the classiﬁer. In contrast to this
line of work, we target the privacy implications of exposing
machine learning model functionality. However, a connec-
tion between these two lines of research will be interesting
to explore in the future, as the mechanisms used for our
attacks might prove useful in their settings.
Several authors have explored linear reconstruction at-
tacks [10,12,23], which work as follows: given some released
information y and assuming a hidden vector s of sensi-
tive features (e.g., Boolean indicators for disease status of a
group of patients), the attacker constructs a system of linear
inequalities (a matrix A and a vector z) such that As ≈ z,
and attempts to solve for s using techniques that minimize
some norm, such as LP decoding. Kasiviswanathan, Rudel-
son, and Smith [22] extended these attacks to releases that
are non-linear, such as M -estimators. We also explore at-
tacks on non-linear models, and we further investigate these
attacks in realistic settings. It will be interesting future work
to investigate whether strategies such as LP decoding work
in settings similar to those considered in this paper.
Many disclosure attacks that have been explored in the lit-
erature. The classic attack by Sweeney [38] showed that it
was possible to correlate publicly-available anonymzied hos-
pital visit data with voter registration data, leading to re-
identiﬁcation of some individuals. Narayananan [32] demon-
strated that an adversary with some prior knowledge can
identify a subscriber’s record in the anonymized Netﬂix prize
dataset. Wang et al. [39], Sankararaman et al. [34], and
Homer et al. [18] consider disclosure attacks on datasets gen-
erated from Genome-Wide Association Studies (GWAS) and
other partial genetic information for an individual. Cormode
showed that if an adversary is allowed to make queries that
relate sensitive attributes to quasi-identiﬁers, then it is pos-
sible to build a diﬀerentially-private Naive Bayes classiﬁer
that accurately predicts a sensitive attribute [7]. Loukides
et al. [29] show that one can infer sensitive patient infor-
mation from de-identiﬁed clinical data. The main diﬀerence
between this line of work and that presented in this pa-
per is the type of information used by the adversary to infer
sensitive information. Whereas previous work relied primar-
ily on de-identiﬁed datasets, our attacks operate entirely on
trained machine learning models and the metadata that is
commonly released with them.
Komarova et al. [25] studied the problem of partial disclo-
sure, where an adversary is given ﬁxed statistical estimates
from combined public and private sources, and attempts to
infer the sensitive feature of an individual referenced in those
sources. The key diﬀerence between theirs and our setting
is that we assume the adversary is given a statistical estima-
tor as a function, and can thus use it directly to make and
evaluate predictions about individuals. It is worth studying
whether the statistics used in their study can be used as ad-
ditional side information to boost the performance of model
inversion.
Li et al. [28] explore a privacy framework called member-
ship privacy. This framework consists of two notions: pos-
itive membership privacy (PMP) and negative membership
privacy (NMP). The framework gives rise to a number of se-
curity notions, including variants of diﬀerential privacy [11].
These security notions were not designed to resist MI at-
tacks, and whether they might help guide countermeasure
design is an open question. (See also the discussion of dif-
ferential privacy in [13].) We also note that MI may serve as
a primitive useful to adversaries seeking to violate these pri-
vacy notions (e.g., a high MI accuracy might reveal whether
a person was in the training set).
8. CONCLUSION
We demonstrated how the conﬁdence information returned
by many machine learning ML classiﬁers enables new model
inversion attacks that could lead to unexpected privacy is-
sues. By evaluating our model inversion algorithms over
decision trees published on a ML-as-a-service marketplace,
we showed that they can be used to infer sensitive responses
given by survey respondents with no false positives. Using a
large-scale study on Mechanical Turk, we showed they they
can also be used to extract images from facial recognition
models that a large majority of skilled humans are able to
consistently re-identify.
We explored some simple approaches that can be used
to build eﬀective countermeasures to our attacks, initiating
an experimental evaluation of defensive strategies. Although
these approaches do not constitute full-ﬂedged private learn-
ing algorithms, they illustrate trends that can be used to
guide future work towards more complete algorithms. Our
future eﬀorts will follow this path, as we continue to work
towards new systems that are able to beneﬁt from advances
in machine learning without introducing vulnerabilities that
lead to model inversion attacks.
9. REFERENCES
[1] DeepFace: Closing the Gap to Human-Level
Performance in Face Veriﬁcation. In Conference on
Computer Vision and Pattern Recognition (CVPR).
[2] AT&T Laboratories Cambridge. The ORL database of
faces. http://www.cl.cam.ac.uk/research/dtg/
attarchive/facedatabase.html.
[3] M. Barreno, B. Nelson, R. Sears, A. D. Joseph, and
J. D. Tygar. Can machine learning be secure? In
Proceedings of the 2006 ACM Symposium on
Information, computer and communications security,
pages 16–25. ACM, 2006.
[4] BigML. https://www.bigml.com/.
[5] G. Bradski. The OpenCV library. Dr. Dobb’s Journal
of Software Tools, Jan. 2000.
[6] C.-L. Chi, W. Nick Street, J. G. Robinson, and M. A.
Crawford. Individualized patient-centered lifestyle
recommendations: An expert system for
communicating patient speciﬁc cardiovascular risk
1332information and prioritizing lifestyle options. J. of
Biomedical Informatics, 45(6):1164–1174, Dec. 2012.
[7] G. Cormode. Personal privacy vs population privacy:
learning to attack anonymization. In KDD, 2011.
[8] M. Dabbah, W. Woo, and S. Dlay. Secure
authentication for face recognition. In IEEE
Symposium on Computational Intelligence in Image
and Signal Processing, pages 121–126, April 2007.
[9] C. Dillow. Augmented identity app helps you identify
strangers on the street. Popular Science, Feb. 23 2010.
[10] I. Dinur and K. Nissim. Revealing information while
preserving privacy. In PODS, 2003.
[11] C. Dwork. Diﬀerential privacy. In ICALP. Springer,
2006.
[12] C. Dwork, F. McSherry, and K. Talwar. The price of
privacy and the limits of lp decoding. In STOC, 2007.
[13] M. Fredrikson, E. Lantz, S. Jha, S. Lin, D. Page, and
T. Ristenpart. Privacy in pharmacogenetics: An
end-to-end case study of personalized warfarin dosing.
In USENIX Security Symposium, pages 17–32, 2014.
[14] Fredrikson, M. and Jha, S. and Ristenpart, T. Model
inversion attacks and basic countermeasures
( Technical Report). Technical report, 2015.
[15] I. J. Goodfellow, D. Warde-Farley, P. Lamblin,
V. Dumoulin, M. Mirza, R. Pascanu, J. Bergstra,
F. Bastien, and Y. Bengio. Pylearn2: a machine
learning research library. arXiv preprint
arXiv:1308.4214, 2013.
[16] Google. Prediction API.
https://cloud.google.com/prediction/.
[17] W. Hickey. FiveThirtyEight.com DataLab: How
americans like their steak.
http://fivethirtyeight.com/datalab/how-
americans-like-their-steak/, May 2014.
[18] N. Homer, S. Szelinger, M. Redman, D. Duggan,
W. Tembe, J. Muehling, J. V. Pearson, D. A. Stephan,
S. F. Nelson, and D. W. Craig. Resolving individuals
contributing trace amounts of dna to highly complex
mixtures using high-density snp genotyping
microarrays. PLOS Genetics, 2008.
[19] G. Huang, H. Lee, and E. Learned-Miller. Learning
hierarchical representations for face veriﬁcation with
convolutional deep belief networks. In Computer
Vision and Pattern Recognition (CVPR), June 2012.
[20] International Warfarin Pharmacogenetic Consortium.
Estimation of the warfarin dose with clinical and
pharmacogenetic data. New England Journal of
Medicine, 360(8):753–764, 2009.
[21] Kairos AR, Inc. Facial recognition API.
https://developer.kairos.com/docs.
[25] T. Komarova, D. Nekipelov, and E. Yakovlev.
Estimation of Treatment Eﬀects from Combined Data:
Identiﬁcation versus Data Security. NBER volume
Economics of Digitization: An Agenda, To appear.
[26] Lambda Labs. Facial recognition API.
https://lambdal.com/face-recognition-api.
[27] H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng.
Convolutional deep belief networks for scalable
unsupervised learning of hierarchical representations.
In Proceedings of the 26th Annual International
Conference on Machine Learning, ICML ’09, pages
609–616, New York, NY, USA, 2009. ACM.
[28] N. Li, W. Qardaji, D. Su, Y. Wu, and W. Yang.
Membership privacy: A unifying framework for
privacy deﬁnitions. In Proceedings of ACM CCS, 2013.
[29] G. Loukides, J. C. Denny, and B. Malin. The
disclosure of diagnosis codes can breach research
participants’ privacy. Journal of the American Medical
Informatics Association, 17(3):322–327, 2010.
[30] D. Lowd and C. Meek. Adversarial learning. In
Proceedings of the eleventh ACM SIGKDD
international conference on Knowledge discovery in
data mining, pages 641–647. ACM, 2005.
[31] Microsoft. Microsoft Azure Machine Learning.
[32] A. Narayanan and V. Shmatikov. Robust
de-anonymization of large sparse datasets. In IEEE
Symposium on Security and Privacy, pages 111–125,
2008.
[33] J. Prince. Social science research on pornography.
http://byuresearch.org/ssrp/downloads/
GSShappiness.pdf.
[34] S. Sankararaman, G. Obozinski, M. I. Jordan, and
E. Halperin. Genomic privacy and limits of individual
detection in a pool. Nature Genetics, 41(9):965–967,
2009.
[35] C. Savage. Facial scanning is making gains in
surveillance. The New York Times, Aug. 21 2013.
[36] SkyBiometry. Facial recognition API. https://www.
skybiometry.com/Documentation#faces/recognize.
[37] T. W. Smith, P. Marsden, M. Hout, and J. Kim.
General social surveys, 1972-2012. National Opinion
Research Center [producer]; The Roper Center for
Public Opinion Research, University of Connecticut
[distributor], 2103.
[38] L. Sweeney. Simple demographics often identify people
uniquely. 2000.
[39] R. Wang, Y. F. Li, X. Wang, H. Tang, and X. Zhou.
Learning your identity and disease from research
papers: information leaks in genome wide association
studies. In CCS, 2009.
[22] S. P. Kasiviswanathan, M. Rudelson, and A. Smith.
[40] Wise.io. http://www.wise.io/.
The power of linear reconstruction attacks. In SODA,
2013.
[23] S. P. Kasiviswanathan, M. Rudelson, A. Smith, and
J. Ullman. The price of privately releasing contingency
tables and the spectra of random matrices with
correlated rows. In STOC, 2010.
[24] J. Klontz, B. Klare, S. Klum, A. Jain, and M. Burge.
Open source biometric recognition. In IEEE
International Conference on Biometrics: Theory,
Applications and Systems, pages 1–8, Sept 2013.
1333