0.065
0.030
Radial Svm
Normal
Random Forest
Normal
Deep Neural Network
Normal
FPR
0.140
0.265
0.040
0.020
AR
0.18
0.41
0.01
0.00
Mitigation
AR
0.04
0.03
0.01
0.00
FPR
0.140
0.265
0.040
0.020
FPR
0.09
0.21
0.03
0.04
AR
0.03
0.23
0.78
0.01
Mitigation
AR
0.00
0.00
0.00
0.00
FPR
0.09
0.21
0.03
0.04
FPR
0.215
0.325
0.095
0.115
AR
0.20
0.30
0.10
0.08
Mitigation
AR
0.00
0.00
0.04
0.02
FPR
0.170
0.375
0.065
0.090
TABLE II.
EQUAL ERROR RATE AND RAR WITH AND WITHOUT THE MITIGATION STRATEGY. THE AR VALUES REMAIN THE SAME AS IN TABLE I.
β-RAR INDICATES RAR TREATED WITH ONLY β NOISE. RAR INDICATES THE INCLUSION OF BOTH β NOISE AND RAW RANDOM INPUT SAMPLES.
Biometric
Modality
Touch Raw
Face Raw
Linear SVM
Normal
FPR
0.325
0.050
RAR
0.45
0.12
FPR
0.345
0.075
Mitigation
β-RAR
0.44
0.14
Radial Svm
Normal
RAR
0.00
0.00
FPR
0.265
0.040
RAR
0.40
0.09
FPR
0.265
0.040
Mitigation
β-RAR
0.36
0.09
RAR
0.01
0.00
Normal
FPR
0.21
0.03
RAR
0.18
0.02
Random Forest
FPR
0.215
0.030
Mitigation
β-RAR
0.05
0.01
RAR
0.00
0.00
Deep Neural Network
Normal
FPR
0.325
0.095
RAR
0.32
0.10
FPR
0.38
0.07
Mitigation
β-RAR
0.26
0.06
RAR
0.00
0.03
However, this gives us a simple idea to minimize AR: generate
noise vectors around the target user’s vectors and treat it as part
of the negative class for training the model. This will result in
the tightening of the acceptance region around the true positive
region. We remark that the noise generated is independent of
the negative training samples.
previously saw in Section IV-E that it was (a) exponentially
small and (b) many orders of magnitude smaller than the true
positive region. Thus, it is unlikely that beta distributed noise
will lie in this region to aid the model to label them as negative
samples. Consequently we sought another means to mitigate
this attack surface.
A. The Beta Distribution
More speciﬁcally, we generate additional negative training
samples by sampling noisy vectors where each feature value is
sampled from a beta distribution. We generate samples equal
to the number of samples in the positive class. Thus creating a
dataset with a third of the samples as positive, another third as
negative samples from other users, and ﬁnally the remaining
third of feature vectors treated as negative samples from the
beta distribution dependent on the positive user. The procedure
is as follows. For the ith feature, let µi denote the mean value
for the given target user. We use the beta distribution with
parameters αi = |0.5 − µi| + 0.5 and βi = 0.5. We denote
the resulting beta distribution by Be(αi, βi). Then a noisy
sample x is constructed by sampling its ith element xi from
the distribution Be(αi, βi) if µi ≤ 0.5, and from 1−Be(αi, βi)
otherwise. The two cases ensure that we add symmetric noise
as the mean moves over to either side of 0.5.
Results on AR. In Table I, we show the resulting FPR
and AR after the addition of beta noise at the equal error
rate. The detailed ROC curves are shown in Figure 13 in
Appendix A. In every conﬁguration (classiﬁer-dataset pairs),
we see a signiﬁcant decrease in AR. The AR is now lower than
FPR in every conﬁguration. In 14 out of 16 cases, the AR is
≤ 0.04. The two exceptions are LinSVM (with face and voice
datasets). We further see that in 13 out of 16 instances the
FPR either remains unchanged or improves! The 3 instances
where the FPR degrades are LinSVM with face and face
datasets both by +0.015, and DNN with Touch where the
difference is +0.05. Thus, adding beta distributed noise does
indeed decrease the AR with minimal impact on FPR. This
agrees with our postulate that high AR was likely due to loose
decision boundaries drawn by the classiﬁer, and the addition of
beta noise tightens this around the true positive region. Figure
12 in Appendix A displays individual user FPRs and ARs.
Results on RAR. Interestingly, beta distributed noise only
marginally reduces the raw acceptance rate as can be seen
in Table II (columns labeled β-RAR). The reason for this lies
in the volume of the region spanned by random raw inputs. We
B. Feature Vectors from Raw Inputs as Negative Samples
Our mitigation strategy to reduce RAR is to include a
subset of raw input vectors in the training process, whose
cardinality is equal to the number of positive user samples
in the training dataset. The training dataset now contains 1/4th
each of raw input vectors, beta-noise, positive samples, and
samples from other users.
Results on AR and RAR. Table II shows that the mitigation
strategy reduces the RAR to less than or equal to 0.03 in
all instances (columns labeled RAR). The resulting FPR is
marginally higher than the FPR from only beta-distributed
noise in some cases (Table I). Thus, the inclusion of beta-
distributed noise in conjunction with subset of raw inputs in
the training data reduces both AR and RAR with minimal
impact on FPR and FRR.
VII. DISCUSSION
• Our work proposes an additional criterion to assess the
security of biometric systems, namely their resilience to
random inputs. The work has implications for biometric
template protection [51], where a target template resides
on a remote server and the attacker’s goal
is to steal
the template. In such a setting, obtaining an accepting
sample may be enough for an attacker, as it serves as an
approximation to the biometric template. Our work shows
that the attacker might be able to ﬁnd an approximation to
the template via random input attacks if the system AR is
not tested. Conversely, once the AR is reduced below FPR
(e.g., via adding beta distributed noise), then one can safely
use FPR as the baseline probability of success of ﬁnding
an approximation.
• We have assumed that the input to the classiﬁer, in partic-
ular the length of the input is publicly known. In practice,
this may not be the case. For instance, in face recognition,
a captured image would be of a set size unknown to the
attacker. Likewise, the number of features in the (latent)
feature space may also be unknown. However, we do not
consider this as a serious limitation, as the input length is
rarely considered sensitive so as to be kept secret. In any
13
case, the security of the system should not be reliant on
keeping this information secret following Kerckhoffs’s well
known principle.
• We note that there are various detection mechanisms that
protect the front-end of biometric systems. For example,
spooﬁng detection [52] is an active area in detecting
speaker style transfer [53]. Detection of replay attacks is
also leveraged to ensure the raw captured biometric is
not reused, for example audio recordings [54]. There is
also liveliness detection, which seeks to determine if the