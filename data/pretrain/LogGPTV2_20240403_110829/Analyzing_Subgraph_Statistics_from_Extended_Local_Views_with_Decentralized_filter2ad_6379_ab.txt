ing our privacy model, we first explain why a straightforward
application of local differential privacy (LDP) [13] fails to provide
adequate privacy protection in the presence of ELVs. Specifically,
in LDP, each individual has her privacy budget locally, and submits
a report based on her local data to the analyst, which is perturbed
to satisfy differential privacy. In our setting, the local data of an
individual v is her ELV Gv; thus, LDP ensures that the data collec-
tor cannot distinguish v’s exact data Gv from a neighbor dataset,
which in our setting would be a neighbor graph (Definition 2.3) of
Gv. Formally, we define (ϵ, δ)-LDP as follows:
Definition 2.4 (Local differential privacy). Given a node v ∈ V
and its ELV Gv, a randomized mechanism M satisfies (ϵ, δ)-LDP,
iff. for any neighboring graph G′
v of Gv and any set S of possible
outputs of M, we have Pr(M(Gv) ∈ S) ≤ Pr(M(G′
v) ∈ S) · eϵ + δ.
To see why the above privacy definition is insufficient, consider
the case where an adversarial data collector with a specific target
victim u aims to find whether u is connected to another node v.
To do so, the data collector asks u, as well as all of its one-hop
neighbors, to report a binary value on whether edge (u, v) exists.
After that, the data collector computes the mean value of these
binary reports. Although each report satisfies LDP, the mean value
v1v2v3v4v5v1’s local viewv1v2v3v4v5v6v7v8v9The global graph Gv3v5v6v7v8v9v4v8’s local viewover multiple reports yields an increasingly accurate estimate of
the true value as the number of reports grows. With a sufficiently
large number of reports, the adversary obtains high confidence on
whether edge (u, v) exists. This clearly violates the privacy of u.
Decentralized differential privacy. The key reason that the
above definition of LDP fails to provide adequately protection on
the social network participants’ privacy, is that each participant
only considers her own privacy when releasing information to the
data collector, and the released information compromises her neigh-
bors’ privacy. To remedy this, we propose a decentralized differential
privacy (DDP) scheme, which ensures that the data collector cannot
infer the presence of absence of any edge in the graph from the set
of all reports collected from all network participants. In particular,
we first define the notion of neighboring ELV, as follows.
Definition 2.5 (Neighboring extended local views). Given a graph
G = (V , E), a node v ∈ V , its ELV Gv ⊆ G, and a neighboring graph
v of Gv with respect to G′ is then
G′ of G. The neighboring ELV G′
the ELV of v in G′.
Note that in the above definition, a neighboring ELV G′
v of Gv
is not the same as a neighbor graph of Gv as Definition 2.3. In
particular, two neighboring ELVs may not contain the same set of
nodes, and can differ in multiple edges. For instance, in Figure 1, if
we remove edge (v1, v2) from G, then node v4 is no longer in the
ELV of v1, since it is now a three-hop neighbor of v1. Similarly, if
we add a new edge (v1, v8) to G, then nodes v7-v9 enter the ELV of
v1, along with all the edges connecting them. Based on the notion of
neighboring ELVs, we define the proposed decentralized differential
privacy (DDP) scheme, as follows.
Definition 2.6 (Decentralized differential privacy). Given a set of
nodes V = v1, v2, . . . vn, a set of randomized mechanisms {Mi, 1 ≤
i ≤ n} collectively satisfy (ϵ, δ)-DDP, iff. for any two neighboring
graphs G = (V , E) and G′ = (V , E′), and any subsets of possible
outputs {Si ⊆ ranдe(M), 1 ≤ i ≤ n}, we have:
Pr(M1(G1) ∈ S1, . . . ,Mn(Gn) ∈ Sn)
≤ Pr(M1(G
′
1) ∈ S1, . . . ,Mn(G
′
n) ∈ Sn) · eϵ + δ,
i
where Gi and G′
(1 ≤ i ≤ n) are the neighboring ELVs of vi with
respect to G and G′, respectively. In addition, since our DDP is
under the (ϵ, δ)-DP framework, the composition rule still applies.
Discussion. Similar to the case of LDP (Definition 2.4), in DDP
each node vi applies its own randomized mechanism Mi(Gi) on
its local data, i.e., its ELV Gi. In other words, no knowledge of the
global graph G is required when node vi computes its report Mi to
be submitted to the data collector. On the other hand, unlike LDP
where each node independently preserves its own privacy without
any consideration for its neighbors, DDP covers the set of all mech-
anisms applied to all nodes as a whole, and protects all edges in
the entire social graph G. Hence, the attack on LDP in which the
data collector obtains multiple reports on the same information no
longer works under DDP, since the latter by definition guarantees
that the data collector cannot infer the presence of absence of an
edge from all collected reports.
Our problem is different from existing LDP applications where
users have independent data, e.g., collecting browser usage [13].
Instead, DDP can be viewed as a generalization of LDP when users’
data are dependent, following the general principle of differential
privacy in the local setting [10].
Designing effective mechanisms under DDP, however, is also
significantly more challenging compared to the case of LDP. The
main difficulty is that when generating a perturbed report Mi, each
node vi must consider all possible neighbor graphs of the global
graph G; yet, vi does not know G, except for its own ELV Gi. We
address this problem in the next section.
3 A GENERAL FRAMEWORK FOR
SUBGRAPH COUNTING UNDER DDP
In this paper, we focus on a fundamental problem in graph analysis:
subgraph counting, under the decentralized differential privacy
requirement in Definition 2.6. Specifically, let д be a given subgraph
pattern, e.g., a triangle, a k-clique, etc., the data analyst aims to
estimate the number of occurrences of д in the global graph G, by
collecting data from each node in G under (ϵ, δ)-DDP.
Given a node vi ∈ V and its two-hop ELV Gi, we define γд(vi)
as the exact number of occurrences of д in Gi that involve vi itself.
In other words, occurrences of д in Gi that does not contain vi are
not counted in γд(vi). In the non-private setting, the data analyst
simply collects the exact γд(vi) from each vi, and aggregates them
to obtain the total number of occurrences of д in the whole graph
G. For example, when д is a triangle, the analyst simply adds up the
local triangle counts from every node, and then divides the result
by 3, since every triangle is reported 3 times by each of its nodes.
Under the DDP requirement, each node vi cannot reveal the
exact γд(vi), as it depends on vi’s private information Gi. Instead,
each vi submits a perturbed report Mi(Gi) generated through a
DDP-compliant mechanism Mi. For instance, one straightforward
approach, explained below in Section 3.1, is to let each vi submit
a noisy version γ∗
д(vi) of γд(vi), perturbed under DDP. In general,
the mechanism Mi applied at each node vi can be different, as long
as the set of all Mi’s for 1 ≤ i ≤ n satisfy DDP as in Definition 2.6.
In our setting, we assume that the data analyst as well as all
participants of the social network strictly follow the proposed pro-
tocols. In other words, the adversary is honest but curious. The case
where the analyst actively breaks the protocol to gain private in-
formation, possibly in collusion with some of the network nodes,
is out of the scope of this paper, and left as future work.
3.1 A Baseline Approach
We first present a baseline approach, referred to as Pessimistic
Laplace mechanism, in which the analyst directly collects perturbed
subgraph counts from the participants under DDP. As shown soon,
this method incurs a prohibitively high amount of noise, due to the
fact that in order to satisfy DDP, the method has to consider patho-
logical worst-case scenarios that necessitate heavy perturbations.
This highlights the challenge of mechanism design under DDP.
Pessimistic Laplace mechanism follows the standard Laplace
mechanism [11]. Specifically, we first extend the notion of sensitivity
(explained in Section 2.1) to the DDP setting, as follows.
n
i =1
Definition 3.1 (Sensitivity under DDP). Given a set of nodes V =
{vi | 1 ≤ i ≤ n}, and a function f , the sensitivity of f is defined as:
∆(f ) = maxG,G′
| f (Gi) − f (G
′
i)|,
where G and G′ are two arbitrary neighboring social graphs with
the set of nodes V , and {Gi} and {G′
i} (1 ≤ i ≤ n) are the sets of
neighboring ELVs with respect to G and G′, respectively.
In Pessimistic Laplace mechanism, given a subgraph pattern д,
each participant directly reports a noisy version of its subgraph
count γд(vi). Formally, f = Γд, where Γд(Gi) = γд(vi). The sensi-
tivity ∆(Γд) of Γд, however, is prohibitively high, leading to poor
result utility.
To explain, consider a simple case where д is a triangle. (Other
cases of д are discussed later in Section 4.) We have ∆(Γ△) = 3(n−2)
since (i) in the worst case, an edge (u, v) in an n-node graph can
appear in n − 2 triangles, when both u and v are connected to
all other nodes in the entire social graph, and (ii) each triangle
is reported three times by its three vertices, respectively. Note
that to satisfy DDP, in which privacy is defined over the entire
graph G, we must consider all possible cases of G, including the
above worst case. According to the following lemmata, adding
Laplace noise Lap
prohibitively high noise variance in the resulting estimated triangle
count: O
number of n, regardless of the structure of the actual social graph
G, since the sensitivity is derived from the worst-case scenario.
(cid:16) 3(n−2)
(cid:17) into γ△(vi) ensures ϵ-DDP, but leads to a
(cid:17). Note that the noise variance only depends on the
(cid:16) n3
ϵ 2
ϵ
(cid:16) 3(n−2)
(cid:17)
ϵ
to γ△(vi) of
(cid:17)
(cid:16) n3
ϵ 2
Lemma 3.2. Adding i.i.d. Laplace noise Lap
each vi satisfies ϵ-DDP 2.
Lemma 3.3. Pessimistic Laplace mechanism leads to O
vari-
ance in the estimated total triangle count.
Discussion. The above description of Pessimistic Laplace ensures
ϵ-DPP, which can be viewed as (ϵ, δ)-DDP for the strict case that
δ = 0. When δ > 0, we can improve its accuracy as follows. Each
node vi reports its exact subgraph count γд(vi) with probability
δ, and the perturbed subgraph count (according to the above ap-
proach) otherwise. Alternatively, we could follow the Gaussian
mechanism [1] instead of the Laplace mechanism, which we call
the Pessimistic Gaussian mechanism. Neither of these approaches,
however, addresses the core issue that we inject noise according
to some pathological worst-case scenario, regardless of the actual
structure of the social graph G. As our experiments in Section 5
shows, none of these baseline approaches obtain competitive result
accuracy.
3.2 Proposed Multi-Phase Framework
Local sensitivity. Observe that the main reason that Pessimistic
Laplace mechanism incurs a high noise variance is that it injects
noise based on the global sensitivity of the graph count function
Γд, which is determined by a worst-case scenario. In real social net-
works, however, such pathological scenarios are rare. The proposed
2Proofs can be found in the Appendix.
n
i =1
framework avoids the excessive noise due to the worst case, by
employing the concept of local sensitivity [30], defined as follows.
Definition 3.4 (Local sensitivity under DDP). Given a global graph
G = (V , E) containing nodes V = {vi|1 ≤ i ≤ n}, and a function f .
The local sensitivity of f is defined as:
LS(f ) = maxG,G′
| f (Gi) − f (G
′
i)|,
ϵ
(cid:16) 3LSG(Γ△)
maxG′n
(cid:17) into her triangle count? Unfortunately, the
where G′ is an arbitrary neighboring graph of G, and {Gi} and
i} (1 ≤ i ≤ n) are the sets of ELVs with respect to G and G′,
{G′
respectively.
For instance, in the case of triangle counting, we have LSG(Γ△) =
i =1 |γ△(vi) − γ ′△(vi)|, where G′ is any neighboring graph
of G, and γ ′△(vi) denotes the triangle count to be reported by
vi in G′. In other words, LSG(Γ△) measures the maximum num-
ber of triangles affected by adding or removing one edge in G. If
LSG(Γ△) ≪ n − 2, is it sufficient to ask each user to inject Laplace
noise Lap
answer is no: it has been shown in the literature [30] that injecting
noise according to local sensitivity fails to satisfy differential pri-
vacy. Figure 2 shows an example in the same setting as in Figure
1. Consider the task of triangle counting. The local sensitivity at
node v1 is 1, since adding / removing any edge in the global graph
G (shown in Figure 1) can only change the number of triangles
in G1 by at most 1. Observe that the local sensitivity value at v1
in fact only depends on its ELV G1, regardless of the structure of
G outside G1. Now, consider a neighbor graph G′ of G, which is
identical to G except for one addition edge (v1, v8). On G′, the local
sensitivity of triangle counting at v1 becomes 2, e.g., adding an edge
(v1, v5) leads to two additional triangles. Therefore, the value of
the local sensitivity reveals whether the global is G or G′, meaning
that using its exact value in the randomized mechanism Mi would
violate differential privacy.
In the traditional, centralized setting of differential privacy, there
exist solutions (e.g., [22, 24, 30, 42]) that correctly inject noise based
on an adjusted version of local sensitivity. However, as reviewed in
Section 6, none of them applies to our setting, since they all rely
on knowledge about the global graph G. Our proposed framework
directly tackles the root problem that local sensitivity fails to satisfy
differential privacy: that the noise scale itself is private information.
The idea is simple: we still injects Laplace noise into each node’s
subgraph count γд(vi), but the scale of the noise is not determin-
istic. Instead, the noise scale is a random variable sampled from
a carefully chosen distribution, such that with 1 − δ probability,
the injected Laplace noise can conceal the existence or absence of
any particular edge in G. This idea is explored in previous work
[21, 24] under the centralized differential privacy model (CDP). But
its adoption in the DDP setting is highly non-trivial due to the
fundamental differences between CDP and DDP.
Two-phase framework. Let д be a subgraph pattern and Γд =
{γд(v1), . . . , γд(vn)} be the set of subgraph counts that each user
is asked to report. Our solution consists of two phases. Phase 1
applies an (ϵ1, δ1)-DDP algorithm to collect information about each
user, and decides an appropriate noise scale λ. After that, Phase 2
asks each user to report her subgraph count injected with Laplace
Let (λ, Y) denote the output of Phase 1, and Γ∗
output an estimated LSG(Γд), which is an upper bound of its true
value with probability 1 − δ2. Finally, Phase 2 applies the estimated
LSG(Γд) to obtain randomized subgraph counts. A concrete instan-
tiation is presented later in Section 4.2.
Correctness of the framework. To formally establish the correct-
ness of the proposed multi-phase framework, it suffices to prove
the two-phase case; the multi-phase case can then be proved by
induction. For the two-phase framework, we prove that the two
requirements for Phase 1 (i.e., it satisfies (ϵ1, δ1)-DDP and its output
λ satisfies Inequality (1) with probability 1 − δ2) ensure that our
solution achieves (ϵ, δ)-DDP for ϵ = ϵ1 + ϵ2 and δ = δ1 + δ2.
д denote the set
of noisy subgraph counts returned by Phase 2. Here, Y represents
all additional private information besides λ that is revealed to the
data collector during Phase 1. Let Sλ (resp. SΓ) be an arbitrary set
of possible outputs from Phase 1 (resp. Phase 2). We will establish
the privacy guarantee of our solution by showing that, for any
neighboring graphs G and G′ and for any Γ∗
∗
д ∈ SΓ,(λ, Y) ∈ Sλ
∗
д ∈ SΓ,(λ, Y) ∈ Sλ
(cid:12)(cid:12)(cid:12) G
Pr(cid:104)
(cid:105)
(cid:12)(cid:12)(cid:12) G
≤ eϵ· Pr(cid:104)
′(cid:105)
=(cid:8)(λ, Y)(cid:12)(cid:12) (λ, Y) ∈ Sλ ∧ ϵ2 · λ ≥ LSG
Let S′
be the subset of Sλ such that
λ
S′
λ
(cid:0)Γд(cid:1)(cid:9) .
д , λ, and Y,
+ δ .
(2)
Γ
Γ
λ
λ
λ