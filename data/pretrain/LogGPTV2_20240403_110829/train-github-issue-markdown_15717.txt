This is because stateful RNN requires batch_input_shape to be defined, however
the forward RNN does not know what the batch_input_shape is when creating the
Bidirectional object because it did not yet had the chance to connect to the
layers below it.
However, the Bidirectional init attempts to create the backward RNN by
duplicating the forward RNN and for that it calls the get_config method of the
forward RNN.  
The get_config of Recurrent places a value of None for batch_input_shape see  
https://github.com/fchollet/keras/blob/master/keras/layers/recurrent.py#L233  
It has an input_spec but because the forward RNN is not yet connected, its
shape is None.
I tried to solve this by manually specifying the batch_input_shape when
creating the forward RNN object.  
However, even this did not solve the problem because the way get_config is
written.  
In Recurrent get_config ignores the batch_input_shape parameter and instead
re-compute it from input_spec[0].shape and then this recomputed value
overwrites the value for batch_input_shape returned from the super get_config