title:Machine Learning Models that Remember Too Much
author:Congzheng Song and
Thomas Ristenpart and
Vitaly Shmatikov
Machine Learning Models that Remember Too Much
Congzheng Song
Vitaly Shmatikov
Cornell University
PI:EMAIL
Thomas Ristenpart
PI:EMAIL
PI:EMAIL
Cornell Tech
Cornell Tech
ABSTRACT
Machine learning (ML) is becoming a commodity. Numerous ML
frameworks and services are available to data holders who are not
ML experts but want to train predictive models on their data. It is
important that ML models trained on sensitive inputs (e.g., personal
images or documents) not leak too much information about the
training data.
We consider a malicious ML provider who supplies model-training
code to the data holder, does not observe the training, but then ob-
tains white- or black-box access to the resulting model. In this
setting, we design and implement practical algorithms, some of
them very similar to standard ML techniques such as regularization
and data augmentation, that “memorize” information about the
training dataset in the model—yet the model is as accurate and
predictive as a conventionally trained model. We then explain how
the adversary can extract memorized information from the model.
We evaluate our techniques on standard ML tasks for image
classification (CIFAR10), face recognition (LFW and FaceScrub),
and text analysis (20 Newsgroups and IMDB). In all cases, we show
how our algorithms create models that have high predictive power
yet allow accurate extraction of subsets of their training data.
CCS CONCEPTS
• Security and privacy → Software and application security;
KEYWORDS
privacy, machine learning
1 INTRODUCTION
Machine learning (ML) has been successfully applied to many data
analysis tasks, from recognizing images to predicting retail pur-
chases. Numerous ML libraries and online services are available
(see Section 2.2) and new ones appear every year.
Data holders who seek to apply ML techniques to their datasets,
many of which include sensitive data, may not be ML experts. They
use third-party ML code “as is,” without understanding what this
code is doing. As long as the resulting models have high predictive
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS ’17, October 30-November 3, 2017, Dallas, TX, USA
© 2017 Copyright held by the owner/author(s). Publication rights licensed to Associa-
tion for Computing Machinery.
ACM ISBN 978-1-4503-4946-8/17/10...$15.00
https://doi.org/10.1145/3133956.3134077
power for the specified tasks, the data holder may not even ask
“what else did the model capture about my training data?”
Modern ML models, especially artificial neural networks, have
huge capacity for “memorizing” arbitrary information [75]. This
can lead to overprovisioning: even an accurate model may be using
only a fraction of its raw capacity. The provider of an ML library
or operator of an ML service can modify the training algorithm so
that the model encodes more information about the training dataset
than is strictly necessary for high accuracy on its primary task.
Our contributions. We show that relatively minor modifications
to training algorithms can produce models that have high quality
by the standard ML metrics (such as accuracy and generalizability),
yet leak detailed information about their training datasets.
We assume that a malicious ML provider supplies the training al-
gorithm to the data holder but does not observe its execution. After
the model has been created, the provider either obtains the entire
model (white box) or gains input-output access to it (black box). The
provider then aims to extract information about the training dataset
from the model. This scenario can arise when the data holder uses a
malicious ML library and also in algorithm marketplaces [2, 27, 54]
that let data holders pay to use third-party training algorithms in
an environment secured by the marketplace operator.
In the white-box case, we evaluate several techniques: (1) encod-
ing sensitive information about the training dataset directly in the
least significant bits of the model parameters, (2) forcing the param-
eters to be highly correlated with the sensitive information, and (3)
encoding the sensitive information in the signs of the parameters.
The latter two techniques involve adding a malicious “regulariza-
tion” term to the loss function and, from the viewpoint of the data
holder, could appear as yet another regularization technique.
In the black-box case, we use a technique that resembles data
augmentation (extending the training dataset with additional syn-
thetic data) without any modifications to the training algorithm.
The resulting model is thus, in effect, trained on two tasks. The
first, primary task is the main classification task specified by the
data holder. The secondary, malicious task is as follows: given a
particular synthetic input, “predict” one or more secret bits about
the actual training dataset.
Because the labels associated with our synthetic augmented in-
puts encode secrets about the training data, they do not correspond
to any structure in these inputs. Therefore, our secondary task asks
the model to “learn” what is essentially random labeling. Never-
theless, we empirically demonstrate that models become overfitted
to the synthetic inputs—without any significant impact on their
accuracy and generalizability on the primary tasks. This enables
black-box information extraction: the adversary provides a syn-
thetic input, and the model outputs the label, i.e., the secret bits
about the actual training dataset that it memorized during training.
Figure 1: A typical ML training pipeline. Data D is split into training set Dtrain and test set Dtest. Training data may be augmented
using an algorithm A, and then parameters are computed using a training algorithm T that uses a regularizer Ω. The resulting
parameters are validated using the test set and either accepted or rejected (an error⊥ is output). If the parameters θ are accepted,
they may be published (white-box model) or deployed in a prediction service to which the adversary has input/output access
(black-box model). The dashed box indicates the portions of the pipeline that may be controlled by the adversary.
We evaluate white- and black-box malicious training techniques
on several benchmark ML datasets and tasks: CIFAR10 (image clas-
sification), Labeled Faces in the Wild (face recognition), FaceScrub
(gender classification and face recognition), 20 Newsgroups (text
classification), and IMDB (binary sentiment classification). In all
cases, accuracy and generalizability of the maliciously trained mod-
els are virtually identical to the conventional models.
We demonstrate how the adversary can extract subsets of the
training data from maliciously trained models and measure how the
choices of different parameters influence the amount and accuracy
of extraction. For example, with a white-box attack that encodes
training data directly in the model parameters, we create a text
classifier that leaks 70% of its 10,000-document training corpus
without any negative impact on the model’s accuracy. With a black-
box attack, we create a binary gender classifier that allows accurate
reconstruction of 17 complete face images from its training dataset,
even though the model leaks only one bit of information per query.
For the black-box attacks, we also evaluate how success of the
attack depends on the adversary’s auxiliary knowledge about the
training dataset. For models trained on images, the adversary needs
no auxiliary information and can simply use random images as
synthetic augmented inputs. For models trained on text, we compare
the accuracy of the attack when the adversary knows the exact
vocabulary of the training texts and when the adversary uses a
vocabulary compiled from a publicly available corpus.
In summary, using third-party code to train ML models on sen-
sitive data is risky even if the code provider does not observe the
training. We demonstrate how the vast memorization capacity of
modern ML models can be abused to leak information even if the
model is only released as a “black box,” without significant impact
on model-quality metrics such as accuracy and generalizability.
2 BACKGROUND
2.1 Machine Learning Pipelines
We focus for simplicity on the supervised learning setting, but our
techniques can potentially be applied to unsupervised learning, too.
A machine learning model is a function fθ : X (cid:55)→ Y parameterized
by a bit string θ of parameters. We will sometimes abuse the notation
and use fθ and θ interchangeably. The input, or feature, space is X,
the output space is Y. We focus on classification problems, where
X is a d-dimensional vector space and Y is a discrete set of classes.
For our purposes, a machine learning pipeline consists of several
steps shown in Figure 1. The pipeline starts with a set of labeled
i =1 where (xi , yi) ∈ X × Y for 1 ≤ i ≤ n′.
data points D = {(xi , yi)}n′
This set is partitioned into two subsets, training data Dtrain of size n
and test data Dtest.
Data augmentation. A common strategy for improving general-
izability of ML models (i.e., their predictive power on inputs outside
their training datasets) is to use data augmentation as an optional
preprocessing step before training the model. The training data
Dtrain is expanded with new data points generated using determin-
istic or randomized transformations. For example, an augmentation
algorithm for images may take each training image and flip it hori-
zontally or inject noises and distortions. The resulting expanded
dataset Daug is then used for training. Many libraries and machine
learning platforms provide this functionality, including Keras [36],
MXNET [56], DeepDetect [19], and indico [34].
Training and regularization. The (possibly augmented) dataset
Daug is taken as input by a (usually randomized) training algo-
rithm T, which also takes as input a configuration string γ called
the hyperparameters. The training algorithm T outputs a set of
parameters θ, which defines a model fθ : X (cid:55)→ Y.
In order to find the optimal set of parameters θ for f , the training
algorithm T tries to minimize a loss function L which penalizes the
mismatches between true labels y and predicted labels produced
by fθ(x). Empirical risk minimization is the general framework for
doing so, and uses the following objective function over Dtrain:
i θ2
where Ω(θ) is a regularization term that penalizes model complexity
and thus helps prevent models from overfitting.
l2-norm Ω(θ) = λ
too large, and l1-norm Ω(θ) = λ
Popular choices for Ω are norm-based regularizers, including
i which penalizes the parameters for being
i |θi| which adds sparsity to the
parameters. The coefficient λ controls how much the regularization
term affects the training objective.
There are many methods to optimize the above objective func-
tion. Stochastic gradient descent (SGD) and its variants are com-
monly used to train artificial neural networks, but our methods
apply to other numerical optimization methods as well. SGD is
an iterative method where at each step the optimizer receives a
n
i =1
min
θ
Ω(θ) + 1
n
L(yi , fθ(xi))
!Val!or	⊥ATDtrainDtestDaugD small batch of training data and updates the model parameters θ
according to the direction of the negative gradient of the objective
function with respect to θ. Training is finished when the model
converges to a local minimum where the gradient is close to zero.
Validation. We define accuracy of a model fθ relative to some
dataset D using 0-1 loss:
acc(θ, D) = 
(x,y)∈D
I(fθ(x) = y)
|D|
where I is the function that outputs 1 if fθ(x) = y and outputs
zero otherwise. A trained model is validated by measuring its test
accuracy acc(θ, Dtest). If the test accuracy is too low, validation may
reject the model, outputting some error that we represent with a
distinguished symbol ⊥.
A related metric is the train-test gap. It is defined as the difference
in accuracy on the training and test datasets:
acc(θ, Dtrain) − acc(θ, Dtest) .
This gap measures how overfitted the model is to its training dataset.
Linear models. Support Vector Machines (SVM) [17] and logistic
regression (LR) are popular for classification tasks such as text cate-
gorization [35] and other natural language processing problems [8].
We assume feature space X = Rd for some dimension d.
In an SVM for binary classification with Y = {−1, 1} , θ ∈ X,
the model is given by fθ(x) = sign(θ⊤x), where the function sign
returns whether the input is positive or negative. Traditionally
training uses hinge loss, i.e., L(y, fθ(x)) = max{0, 1 − yθ⊤x}. A
typical regularizer for an SVM is the l2-norm.
With LR, the parameters again consist of a vector in X and define
the model fθ(x) = σ(θ⊤x) where σ(x) = (1 + e−x)−1. In binary
classification where the classes are {0, 1}, the output gives a value
in [0,1] representing the probability that the input is classified as 1;
the predicted class is taken to be 1 if fθ(x) ≥ 0.5 and 0 other-
wise. A typical loss function used during training is cross-entropy:
L(y, fθ(x)) = y · log(fθ(x)) + (1 − y) log(1 − fθ(x)). A regularizer
is optional and typically chosen empirically.
Linear models are typically efficient to train and the number of
parameters is linear in the number of input dimensions. For tasks
like text classification where inputs have millions of dimensions,
models can thus become very large.
Deep learning models. Deep learning has become very popular
for many ML tasks, especially related to computer vision and image
recognition (e.g., [41, 46]). In deep learning models, f is composed of
layers of non-linear transformations that map inputs to a sequence
of intermediate states and then to the output. The parameters θ de-
scribe the weights used within each transformation. The number of
parameters can become huge as the depth of the network increases.
Choices for the loss function and regularizer typically depend
on the task. In classification tasks, if there are c classes in Y, the
last layer of the deep learning model is usually a probability vector
with dimension c representing the likelihood that the input belongs
to each class. The model outputs argmaxfθ(x) as the predicted
class label. A common loss function for classification is negative
i =1 t · log(fθ(x)i), where t is 1 if
the class label y = i and 0 otherwise. Here fθ(x)i denotes the ith
component of the c-dimensional vector fθ(x).
log likelihood: L(y, fθ(x)) = −c
2.2 ML Platforms and Algorithm Providers
The popularity of machine learning (ML) has led to an explosion
in the number of ML libraries, frameworks, and services. A data
holder might use in-house infrastructure with a third-party ML
library, or, increasingly, outsource model creation to a cloud service
such as Google’s Prediction API [27], Amazon ML [3], Microsoft’s
Azure ML [54], or a bevy of startups [10, 30, 55, 58]. These ser-
vices automate much of the modern ML pipeline. Users can upload
datasets, perform training, and make the resulting models available
for use—all without understanding the details of model creation.
An ML algorithm provider (or simply ML provider) is the entity
that provides ML training code to data holders. Many cloud services
are ML providers, but some also operate marketplaces for training
algorithms where clients pay for access to algorithms uploaded
by third-party developers. In the marketplace scenario, the ML
provider is the algorithm developer, not the platform operator.
Algorithmia [2] is a mature example of an ML marketplace. De-
velopers can upload and list arbitrary programs (in particular, pro-
grams for ML training). A user can pay a developer for access to
such a program and have the platform execute it on the user’s
data. Programs need not be open source, allowing the use of propri-
etary algorithms. The platform may restrict marketplace programs
from accessing the Internet, and Algorithmia explicitly warns users
that they should use only Internet-restricted programs if they are
worried about leakage of their sensitive data.
These controls show that existing platform operators already
focus on building trustworthy ML marketplaces. Software-based iso-
lation mechanisms and network controls help prevent exfiltration
of training data via conventional means. Several academic propos-
als have sought to construct even higher assurance ML platforms.
For example, Zhai et al. [74] propose a cloud service with isolated
environments in which one user supplies sensitive data, another
supplies a secret training algorithm, and the cloud ensures that the
algorithm cannot communicate with the outside world except by
outputting a trained model. The explicit goal is to assure the data
owner that the ML provider cannot exfiltrate sensitive training data.
Advances in data analytics frameworks based on trusted hardware
such as SGX [7, 61, 66] and cryptographic protocols based on secure
multi-party computation (see Section 8) may also serve as the basis
for secure ML platforms.
Even if the ML platform is secure (whether operated in-house or
in a cloud), the algorithms supplied by the ML provider may not be
trustworthy. Non-expert users may not audit open-source imple-
mentations or not understand what the code is doing. Audit may
not be feasible for closed-source and proprietary implementations.
Furthermore, libraries can be subverted, e.g., by compromising a