a discussion of related work in Section 6, and concluding
remarks in Section 7.
2.
Input/Output-Data-Oblivious System-Call
Monitors
In this section, we formalize I/O data-oblivious monitors
that were intuitively described in the introduction. We also
explain how they capture the likely limits of future system-
call monitors.
We begin by formalizing the notion of program behaviors
as observed by a system call monitor. In theory, a system
call monitor could examine the entire memory of a moni-
tored application at the point of invocation of each system
call. For performance reasons, practical system call moni-
tors limit the amount of memory that is examined to that of
system-call arguments. In addition, gray-box anomaly de-
tectors also examine the program’s call stack. This suggests
the following formalization of execution traces.
Definition 1
(Gray-box Execution Trace). A
gray-box execution trace (or simply, a trace) for a program
P , denoted T (P ), is the sequence of all the system calls in-
voked by P during its execution. A system call in a trace
provides information about its name, arguments (at the point
of call and its return), return value, and the context in which
it is made, i.e., the list of return addresses on the program’s
stack. Traces also includes environment variables, command-
line arguments and other data available to the program when
it starts.
Note that, in principle, a powerful monitor can incorporate
knowledge about possible control-ﬂows in the program, and
hence can infer the calling context without explicitly reading
the program memory. Nevertheless, we chose to incorporate
the calling context in the deﬁnition in order to explicitly
include so-called gray-box anomaly detection techniques [27,
8, 10] that rely on this information. Based on execution
traces, program behaviors can be formalized as follows:
Definition 2
(Program Behavior). The behavior of
a program is the set T (P ) of all traces generated by P during
any of its legitimate executions.
Not all of the possible executions of a program may be
considered as acceptable uses of the program from a security
perspective. We use the term “legitimate execution” in the
above deﬁnition to eliminate unacceptable behaviors from
consideration.
Practical system-call monitors accept a superset of T (P )
deﬁned above. However, one can imagine the extreme case
where the monitor accepts exactly T (P ):
Definition 3
(Perfect System-Call Monitor). A
perfect system-call monitor classiﬁes a trace T as legitimate
if and only if T ∈ T (P ).
Since a perfect monitor has complete knowledge about the
entire behavior of a program, it can potentially be used as
a generator of traces rather than as an acceptor of traces.
This is particularly easy to see in the case of a determin-
istic program P , since the next system call made by such
a program is uniquely determined by its inputs until this
point. Moreover, these inputs (including command-line and
environment parameters) are fully captured by the system
calls made thus far, together with their arguments and re-
turn values. Thus, a perfect monitor can uniquely determine
the next system call that would be made by P from all the
preceding calls. This suggests that such a monitor must
eﬀectively be duplicating the essential application logic con-
tained in P . Moreover, it must do this without duplicating
the vulnerabilities in P , or otherwise an attack may com-
promise the monitor as well. Although some existing system
call monitors[8] are able to track scalar system call argu-
ments, such as ﬁle-descriptors or process IDs, most monitors
[23, 25, 32, 8, 22, 10, 3] ignore the data read or written by
the application, i.e. the monitors do not track the content
of ﬁles or network packets. This observation motivates the
following deﬁnition of practical system-call monitors.
Definition 4. (Input/Output Data Oblivious Mon-
itor (IOM)). An I/O-oblivious monitor accepts a trace T
if it can be converted into a trace T (cid:48) ∈ T (P ) by modifying
the data arguments to zero or more input and output system
calls in T .
Since IOMs ignore the data read and written by the applica-
tion, traces that diﬀer only in the I/O content are eﬀectively
equivalent. An IOM will accept any trace that is equivalent
to a legitimate one. Note that the IOM deﬁnition only spec-
iﬁes the minimum set of traces accepted by the monitor, and
hence captures existing system-call monitors as well as those
that are much more powerful, while placing some limits on
the maximum power.
Note that the most powerful IOMs ignore only the data-
buﬀer arguments to system calls that perform actual in-
put/output, such as read, write, send and recv; other ar-
guments to these calls (and return values from them) are
incorporated into its behavior model. Moreover, all argu-
ments to every other system call, including I/O-related calls
such as open are captured in the models constructed by the
most powerful among the IOMs. As a result, these monitors
can be signiﬁcantly more powerful than existing monitors.
In spite of their power, we show that they can be evaded
by our persistent interposition attack, which is designed to
leave the stack unmodiﬁed at every system call, and only
modiﬁes the data buﬀer arguments to I/O system calls. As
a result, persistent interposition attacks, by design, cannot
be detected by any I/O data oblivious monitor.
Some systems enable applications to perform I/O without
a system call through mechanisms such as mmapped ﬁles
and shared memory segments. This I/O is completely un-
observable to a system-call monitoring IDS, so injected code
can modify these I/O operations without risk of detection
by any graybox IDS. Persistent interposition attacks do not
use or depend on these kinds of I/O, so these technologies
are irrelevant to the results of this research.
We comment that although the intent of the IOM def-
inition was to rule out very powerful monitors that could
exactly predict application outputs (or more generally, its
future actions) as a function of its inputs, it has the eﬀect
of ruling out more practical techniques such as those based
on signature-based ﬁltering or statistical proﬁling of input
contents. However, other researchers [9] have already de-
veloped techniques that are orthogonal to ours in order to
evade existing content-based IDS. These techniques rely on
encoding attack inputs in such a manner that their charac-
teristics (e.g., byte frequency distribution) conform to the
normal proﬁle used by the IDS. These techniques can easily
be combined with our attack technique since it gives the at-
tacker full control over the contents of attack inputs as well
as all outputs of the victim server.
3. Design of Persistent Interposition Attacks
In this section, we describe the design steps common to
all persistent interposition attacks, and outline possible im-
plementations of these steps. The speciﬁc choices made in
our implementation are presented in Section 4.
Our starting point for persistent interposition attack is an
exploit built on a typical code-injection vulnerability that
enables execution of injected code without making additional
system calls, e.g., a typical stack-smashing, heap-overﬂow
or format-string vulnerability. Such an exploit may impose
fairly stringent limitations in the size of exploit code, which
may be much smaller than the code size needed to support
sophisticated attacks, e.g., stealing of credit card information
from a web server. For this reason, we design persistent
interposition attack to proceed in three steps:
• Initial exploit phase: In this phase, a bootstrapping shim
is inserted on the victim’s normal execution path. Rela-
tively small amount of code (about 100 bytes in our ex-
periments) is needed to carry out the exploit phase, so
that it can be used as a payload for most code-injection
attacks.
• Bootstrapping phase: In this phase, additional attack code
is sent by the attacker within legitimate-looking requests
to the victim application. The bootstrapping code iden-
tiﬁes these requests, assembles them into the code needed
for the operational phase of the attack, and transfers con-
trol to the assembled code.
• Operational phase: Like the bootstrapping code, the op-
erational code is also hooked into the victim’s normal
execution path. It examines every input and output, and
modiﬁes the outputs as desired by the attacker. The at-
tacker may also upload additional code to change the at-
tack over time.
Like previous works on mimicry attacks, our implementa-
tion assumes no defenses against memory corruption attacks.
A defense such as address-space randomization (ASR) can
make persistent interposition attacks harder to develop, but
they will remain possible as long as there are code injection
vulnerabilities. Speciﬁcally, with ASR, exploit code cannot
hard-code the addresses of data or code objects that it wants
to target. However, since most ASR techniques only ran-
domize the base addresses of diﬀerent memory regions, it is
possible to develop scanning attacks to compute these ad-
dresses. Speciﬁcally, the exploit code can scan the stack for
return addresses and data pointers. By comparing the ad-
dresses of corresponding objects (say, a speciﬁc global array
or a return address pointing to a location within the exe-
cutable) between victim and attacker’s systems, it is possi-
ble to “de-randomize” the locations of all objects within a
memory region (i.e., global area or executable code area).
As noted in [20], mimicry attacks against stack-inspecting
gray-box IDS must not leave any trace of attacks on the
call stack.
In particular, the attack code needs to use a
jump rather than a call so that its address won’t be saved
on the stack. This means that the attack code cannot get
159
control when system calls (or any of the application’s func-
tion calls) return, and hence it cannot immediately examine
the data returned by a system call such as read.
Instead,
it has to wait for a subsequent function call that has been
set up for interposition by the attack code. [20] develops a
sophisticated static analysis to automate the identiﬁcation
of function pointers that could be hijacked for this purpose,
and the same techniques could be applied here. However, in
practice, we have found that persistent interposition attacks
require very few functions to be interposed, and those can
be easily identiﬁed by dynamically tracing the sequence of
function calls made by the victim application. This is the
approach we used in our implementation.
3.1 Phase I: Initial Exploit Phase
In this phase, persistent interposition attack uses a code-
injection vulnerability to install the bootstrap code. It con-
sists of the three steps described below.
Step I.1. Storing Bootstrap Code
The bootstrap code must persist long enough to upload
the operational code, so we need to ﬁnd a safe place to stow
it. We considered the following candidate locations:
• Stack. If the victim application consumes only a limited
amount of stack memory, the region of the stack beyond
this space could be used by the attack. We need to ensure
that the attempt to use this space does not cause a signal
due to an invalid memory access. Some operating sys-
tems, including Linux, allocate stack space on demand
and inspect the application stack pointer to determine
whether a page-fault should be handled by extending the
stack. To deal with this problem, our technique pushes
a large amount of data onto the stack and then pops it
oﬀ, and then copies the code into stack space allocated
by the kernel as a result of these pushes.
• Global buﬀers. It is common for programs to use buﬀers
that are much larger than the size of data likely to be
stored in them. Alternatively, certain global arrays may
hold rarely used data that can be overwritten without a
signiﬁcant chance of aﬀecting program behavior. Note
that the memory location of global variables is statically
known, and hence the attack code knows the locations of
such variables.
• Heap. Instead of global buﬀers, we can use heap-allocated
buﬀers that contain rarely used data, or contain more
memory than is typically needed. Pointers to such heap
buﬀers are often stored in global variables, and so the
exploit code can ﬁnd them.
While one may need to choose among the three alternatives
on other OSes, on Linux, the stack is likely to be the location
of choice for storing bootstrap code: it can be used without
signiﬁcant risk of overwriting application data. Moreover,
exploit-code doesn’t need to know the locations of global
or heap-allocated variables, which may be hard to obtain if
defenses such as address-space randomization are deployed.
Step I.2. Interposing Bootstrap Code
After copying itself to a safe location, the initial exploit
code modiﬁes one or more pointers to functions that would
be invoked during the victim’s normal operation. To identify
160
a suitable function pointer, the following choices need to be
considered.
• Application-speciﬁc support for plug-ins and modules:
Modern server and client software often provide exten-
sibility features in the form of plug-ins or modules. Calls
to module-provided functions (as well as some functions
called by the module) are made using tables of function
pointers. For instance, the Apache web server uses the
mod_ssl module in order to support SSL. This module
registers two functions with Apache that the web server
can use to read and write encrypted data. These function
pointers are an obvious target for a persistent interposi-
tion attack.
• Virtual table pointers in C++ programs: Virtual meth-
ods in C++ programs are implemented using a vtable,
i.e. an array containing pointers to each virtual method
implemented by the object’s class. The bootstrap code
can interpose on virtual method invocations by all objects
in this class by overwriting these pointers in the vtable.
This is particularly easy because C++ compilers usually
generate one vtable, stored at a ﬁxed location, for each
class, and place a pointer to this table in every instance
of that class4.
• Global Oﬀset Table (GOT) entries: The global oﬀset ta-
ble is used in Linux to dispatch calls made from the main
executable to shared libraries, such as glibc, and from one
shared library to another. Every externally-visible shared
library function has an entry in the GOT. The dynamic
linker maps the name of each shared library function into
the corresponding address, and ﬁlls the corresponding en-
try. By default, Linux uses a lazy approach for resolving
shared library functions, i.e., the address of a function is
resolved the ﬁrst time it is invoked. To support this, the
GOT remains writable throughout the program’s execu-
tion. Given this fact, and the fact that shared library
functions are used by applications for performing all of
their I/O operations, GOTs are an obvious choice for in-
terposition — a fact that is leveraged commonly in at-
tacks such as heap-overﬂow exploits, as well as previous
mimicry attacks such as [20].
Once the adversary has identiﬁed the target function point-
ers, he can program the exploit code to modify them to point
to the bootstrap code. The options listed above are not
meant to be exhaustive. The bootstrap code can intercept
program execution at any convenient location, potentially