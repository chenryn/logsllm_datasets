title:Amplifying limited expert input to sanitize large network traces
author:Xin Huang and
Fabian Monrose and
Michael K. Reiter
Amplifying Limited Expert Input to Sanitize Large Network Traces
Xin Huang, Fabian Monrose, Michael K. Reiter
Department of Computer Science
University of North Carolina at Chapel Hill
Chapel Hill, NC, USA
{huangxin,fabian,reiter}@cs.unc.edu
Abstract—We present a methodology for identifying sen-
sitive data in packet payloads, motivated by the need to
sanitize packets before releasing them (e.g., for network secu-
rity/dependability analysis). Our methodology accommodates
packets recorded from an incompletely documented protocol,
in which case it will be necessary to consult a human expert to
determine what packet data is sensitive. Since expert availabil-
ity for such tasks is limited, however, our methodology adopts a
hierarchical approach in which most packet inspection is done
by less-trained workers whose designations of sensitive data in
selected packets best match the expert’s. At the core of our
methodology is a data reduction and presentation algorithm
that selects candidate workers based on their evaluations
of a small number of packets; that solicits these workers’
designations of sensitive data in a larger (but still minuscule)
subset of packets; and then applies these designations to mark
sensitive data in the entire data set. We detail our algorithms
and evaluate them in a realistic user study.
Keywords-sanitization; packet payloads; sensitive data
I. INTRODUCTION
Visibility into packet payloads supports numerous net-
work security defenses and dependability analyses. For
example, technologies ranging from simple signature-based
intrusion detection systems (e.g., Snort, www.snort.org)
to advanced techniques for developing exploit signatures
(e.g., [32]) require access to network packet payloads. To
evaluate the efﬁcacy of such proposed defenses, therefore,
it is necessary to have access to payload-bearing network
trafﬁc traces on which to test them.
While there has been signiﬁcant progress in the release of
other types of network trafﬁc traces for research purposes in
the last decade (e.g., see www.caida.org, www.predict.org),
the release of packet payloads remains severely limited
due to privacy concerns, and this continues to hamper
research progress in numerous types of network defense
and performance tests. Packet payloads can contain sen-
sitive information ranging from personal user information
to security-relevant data about network topology and ser-
vice conﬁguration. Thus, data publishers that release packet
traces must ﬁrst sanitize the traces by removing the sensitive
information — and in virtually every case, this sanitiza-
tion includes deleting the payloads in their entirety. This
obviously destroys the utility of the trace for research that
requires packet payloads.
The extreme rarity with which payload data is released
is due to numerous challenges that data publishers face in
trying to sanitize packet payloads. First, almost any inter-
esting trace contains too many packets for an administrator
to examine exhaustively. Second, even if the trace contains
packets of only one protocol (e.g., selected by ﬁltering
on ports),
the packet formats within that protocol may
be numerous or, even worse, undocumented. For example,
the Samba project required many researchers’ efforts over
several years to reverse engineer the ﬁle-sharing protocol
in Microsoft Windows networks, for which the protocol
speciﬁcation was not released to the public. Third, packet
payloads may contain many types of information that may be
deemed sensitive, e.g., user names, IP addresses, passwords,
host names, and a range of user-generated content. Indeed,
the sheer diversity of content
that one might deem as
sensitive in a free-form protocol like HTTP is overwhelming.
As a step forward in this space, in this paper we propose
a framework and tool to support packet trace sanitization.
To accommodate incompletely documented protocols, our
framework is built around a human expert who can explore
selected protocol packets well enough to accurately iden-
tify the sensitive information they contain. However, since
dataset release is rarely a business priority, we presume that
this expert has very little time to devote to this effort. For
this reason, we structure our framework hierarchically, using
the expert’s input to select others from a set of candidate
workers, based on their abilities to mark sensitive data in
packets similarly to the expert. These selected workers then
mark sensitive data in a small group of packets that can
best represent the characteristics of the overall dataset, which
our technique then applies to automatically identify sensitive
data of the remaining packets in that dataset. We stress that
the expert is involved only in marking sensitive data for a
very small number of packets — generally far fewer than
the total number of packet formats available in the protocol.
As such, we cannot impose upon the expert to analyze even
one packet of each format, and of course, we may not even
know how many formats there are due to the unavailability
of the protocol speciﬁcation.
At the core of this technique is an algorithm that selects
and presents packet data to the workers in a fashion that best
enables them to identify ﬁelds that they deem sensitive and
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:32:40 UTC from IEEE Xplore.  Restrictions apply. 
978-1-4244-9233-6/11/$26.00 ©2011 IEEE494then to extrapolate from those inputs on that selected data to
sanitize the entire dataset. (The expert examines only a small
subset of the representative packets selected for the workers.)
Doing so in a way that achieves good precision and recall in
sanitizing the whole dataset requires that our technique (i)
judiciously select the data that the workers will examine; (ii)
organize the presentation of the selected data to maximize
each one’s competence in identifying sensitive ﬁelds; and
(iii) effectively draw inferences from their inputs to sanitize
ﬁelds in packet data not presented to them directly.
intuitively (and ideally),
At a high level, our technique accomplishes these goals
through a multistep process. Packets in the trace are ﬁrst
divided into contiguous tokens, each with a type. Stratiﬁed
sampling is applied to these typed token sequences in order
to select a fraction of the packets for further analysis, while
minimizing the likelihood of excluding any particular packet
type. These selected packets are clustered into groups with
similar structure;
these clusters
correspond to packet formats. We then select representatives
from each cluster, which we present to a worker in an
aligned fashion so as to best reveal their common structures,
a technique known to accelerate visual recognition of homo-
geneous structures [9]. The best workers are selected from
a group of workers by comparing each one’s performance
to that of the expert in marking a small subset of repre-
sentatives; the selected workers then mark sensitive ﬁelds
in all representatives. After the workers identify sensitive
tokens in the representatives for each cluster, these identiﬁed
tokens are mapped onto the entire dataset, in order to identify
sensitive tokens in packets that the workers never examined.
We describe our design and implementation of a tool that
implements this approach and present an evaluation based
on a user study involving professional network administra-
tors as workers. Our results show that both clustering and
alignment have a statistically signiﬁcant, positive effect on
their abilities to identify sensitive data and that the effects
of the two are additive. The study also reveals that even
network administrators show substantial variability in their
abilities to locate sensitive information in network data,
underscoring the difﬁculty of the task at hand and the need
to down-select workers on the basis of the similarity of
their markings to an expert’s. We show that combining the
sensitivity determinations of the two best workers and using
these to mark the entire dataset identiﬁes sensitive data in
the original dataset with at least 0.9 recall and precision.
II. RELATED WORK
Over the past decade, there has been a marked increase
in the number of proposals for anonymizing network data
(e.g., [16, 12, 31, 26, 25, 10]). For the most part, these
works attempt to sanitize network data by applying various
transformations to ﬁelds within packet headers (e.g., using
preﬁx-preserving anonymization [25]), by using domain
knowledge to search for speciﬁc patterns (such as URLs
or bytecode) using regular expressions [17, 11, 26], by
shufﬂing payloads while preserving the ability to search for
short substrings [27], or by deleting the payloads altogether.
Unfortunately, many, if not all, of these proposals require
speciﬁc parsers for each protocol of interest.
In what follows, we attempt to move the ﬁeld forward
by taking advantage of techniques for inferring packet
formats, without relying on having a protocol speciﬁcation at
hand. In particular, we extend prior work from the protocol
reverse engineering community (e.g. [3, 19]) where byte-
based sequence alignment has been applied to raw network
traces for uncovering protocol message formats.1 However,
as Cui et al. [7] discovered, byte-based sequence alignment
is not particularly well suited for this task when messages
of the same format can have high variance in the bytes of
certain ﬁelds. To address this, we further exploit sequence
alignment to derive a new measure of similarity for packet
payloads, and then generate compact clusters suitable for
human inspection afterwards.
Our work is also inspired by the rich history of research
that
takes advantage of human cognition to explore the
spatio-temporal multivariate patterns in high dimensional,
large datasets [4, 30]. These approaches combine computa-
tional techniques and human capacities to discover novel
and useful information, in ways that may be difﬁcult to
do otherwise. As Duncan and Humphreys [9] have shown,
highlighting the similarity between objects can be an ef-
fective way to accelerate visual recognition, e.g, quickly
rejecting homogeneous non-targets. Indeed, Avraham and
Lindenbaum [1] extend the work of Duncan and Humphreys
to show that dynamic visual search can be enhanced with
the usage of inner-scene similarity. Their intuitive hypothesis
was that the more visually similar objects are, the more
likely they are to share the same identity. Using these studies
as a guide, we propose an approach for presenting streams
of network data to a user in a visually aligned form.
Lastly, interactive tools have been recently proposed as
ways to help anonymize microdata [33, 8]. Barros et al. [2]
even suggest ideas for involving an expert to validate the
correctness of methods for sanitizing personally identiﬁable
information (PII) in microdata. Unlike our work, however,
these approaches are not used to help data publishers better
identify sensitive information in the trace, but only allow
them to choose how such data should be anonymized.
Furthermore, because of the complexity of network data
compared to microdata [6], these tools cannot be directly
applied to network traces.
1Other protocol reverse-engineering works infer message types by ana-
lyzing process execution traces (e.g., [20]). In this work, we do not have the
luxury of taking advantage of such information because in the vast majority
of cases the network traces that publishers are willing to make available
have already been collected. Moreover, it is unrealistic to presume that data
publishers would collect process execution traces (or even know how to)
for all protocols appearing in their traces.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:32:40 UTC from IEEE Xplore.  Restrictions apply. 
495Figure 1. Framework of network trace sanitization by leveraging best worker input
III. OUR APPROACH
At
the core of our approach to identifying sensitive
information in packet payloads is an algorithm that selects
a subset of packet payloads to present to workers (a subset
of which is also presented to the expert). At a high level,
this algorithm can be viewed as a natural application of
clustering and sequence alignment techniques for assisting
workers in more readily identifying sensitive information in
network data. Our proposed solution requires that we ﬁrst
tokenize the payloads of the packets by labeling them in
a more compact representation composed of generic types
of ﬁelds. Next, we cluster the tokenized packets in order to
organize them roughly according to their formats.
Obviously, presenting a large corpus of data—even in
an aligned form—to a human being, and expecting her to
effectively sift through such data would not be a fruitful
task, to say the least. In order to ease the arduous job of
ﬁnding potentially sensitive information in a large corpus of
data, we present to the workers only representatives for each
cluster that capture the most mutual information (and that
have low redundancy). As the worker marks tokens in the
view displayed to her (e.g., by highlighting regions within
the visually-aligned representatives), these annotations are
recorded. Once the interactive session has completed, her
selections made during the process are then used to auto-
matically infer other sensitive tokens in the remainder of
the corpus, without further participation from the worker.
The overall process is depicted in Figure 1. We discuss the
speciﬁcs of how we select representatives from the corpus
in more detail in Sections III-A–III-E and then discuss how
we use the tokens marked sensitive by a worker to identify
sensitive ﬁelds in the larger corpus in Section III-F.
A. Tokenization
We remind the reader that in order to be protocol agnostic,
we deliberately assume no prior knowledge of the protocol
format, and so before we can select the best candidates
to present to workers, we must ﬁrst tokenize the data. To
do so, we abstract each packet by grouping its bytes into
tokens, each of a certain type. The token types we use
take advantage of a growing body of work on protocol
reverse engineering (e.g., [7, 20]) that suggests suitable
token types for text protocols (e.g., HTTP, FTP), binary
protocols(e.g., DNS, DHCP) and so-called hybrid protocols
(e.g., SMB). Speciﬁcally, our three token types are: (1)
Length ﬁelds: consecutive printable characters proceeded by
a byte value indicating the number of characters to follow.
Both the printable characters and the byte are combined
into a single length ﬁeld. (2) Text ﬁelds: several consecutive
printable characters, the length of which is greater than some
threshold.2 And (3) Binary ﬁelds: any single byte except
those deﬁned as a length ﬁeld or text ﬁeld. Each packet is
tokenized by scanning the payload from beginning to end.
Figure 2 shows an example of the tokenized representation
of two packets. For the remainder of the paper, all subse-
quent operations are on tokenized sequences, and we denote
the tokenized sequence of a packet pkt by tokenize(pkt).
Figure 2. Example tokenization of two packet payloads.
B. Sampling the Data
For improved performance, we next sample packets from
the entire dataset and use only the selected packets in
subsequent stages. However, sampling packets uniformly at
random risks omitting packet formats present in the full
dataset, especially if those formats are rare. To overcome
this obstacle, we use stratiﬁed sampling [23, 5] in order
to preserve the diversity of the entire dataset. Speciﬁcally,
we partition the tokenized sequences into homogeneous
subgroups; lacking information about packet semantics or
formats, we simply partition sequences according to their
2Similar to [7], we choose a threshold of 3 printable characters.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:32:40 UTC from IEEE Xplore.  Restrictions apply. 
496lengths (i.e., the number of tokens in each). We then draw
a random sample from each stratum of size proportional to
the stratum size, i.e., the number of sequences it contains.
algorithm to iteratively grow the number of clusters, which
is parametrized by a value 𝑟, 0 < 𝑟 < 1:
1) Assign all packets into one cluster, and ﬁnd the medoid
C. Grouping Similar Packet Formats
Given a selection of sequences (i.e., tokenized packets),
the next task is to effectively cluster these sequences into
groups representing different packet formats. To do so, we
ﬁrst deﬁne a distance between sequences, and then provide
an algorithm for performing clustering using those distances.
Our chosen distance is based on sequence alignment [14].
That is, to deﬁne a distance between two packets, we ﬁrst
ﬁnd the optimal
token-based sequence alignment of the
packets, which is an alignment of the pair of sequences of
tokens corresponding to those packets. Speciﬁcally, for an
alignment of two token sequences, each aligned pair of to-
kens is assigned a positive score (an “award”) if they match,
and a negative score (a “penalty”) if they are a mismatch or
if one of them is a gap inserted by the alignment process.
We use Penaltymis and Penaltygap to represent the mismatch
and gap penalties, respectively. In assigning awards for any
two matching tokens, we not only consider their types,
but also consider their values. In particular, if two tokens
have the same type and value, we assign a larger matching
score Awardval. If they are of the same type but different
values, we assign a smaller matching score Awardtyp. The
overall alignment score between packets pkt, pkt′
, denoted
Scorealn(pkt, pkt′), is computed using the matching scores,
mismatch penalties and gap penalties. Our scoring function
is formulated as Scorealn(pkt, pkt′) =N val × Awardval +
Ntyp × Awardtyp + Nmis × Penaltymis + Ngap × Penaltygap,
where Nval, Ntyp, Nmis and Ngap correspond to the number
of tokens with matched values, tokens with matched types,
mismatched tokens and inserted gaps, respectively, for an
alignment of tokenize(pkt) and tokenize(pkt′) that maxi-
mizes Scorealn(pkt, pkt′). We then deﬁne the distance as
dist(pkt, pkt′) = 1 − Scorealn(pkt, pkt′)
Scoremax(pkt, pkt′)
(1)
Scoremax(pkt, pkt′) = max{Scorealn(pkt, pkt),
where
Scorealn(pkt′, pkt′)} denotes that largest possible value of
Scorealn(pkt, pkt′). The optimal sequence alignment for
two token sequences, and hence the distance (1), can be
computed efﬁciently using the well-known Needleman-
Wunsch algorithm [22].
Clustering method: Given this distance calculation, we
apply iterative K-medoids clustering [15] to partition the
packet sequences into different clusters. Unlike K-means,
which takes the arithmetic mean of each cluster’s points as
its centroid, the K-medoids algorithm chooses a member of