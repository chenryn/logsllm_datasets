Are Internet paths diverse enough for selective poisoning to be ef-
fective? This technique may provide a means to partially poison
large networks, and services with distributed data centers may have
the type of connectivity required to enable it. In our second selec-
tive poisoning experiment, we approximated this type of deploy-
ment, and we assess how many ASes we could selectively poison.
To make this assessment, we need to identify whether ASes se-
lect from disjoint paths, which would allow us to selectively poison
them. It is hard to perform this assessment in general, because we
need to know both which path an AS is using and which paths it
might use if that path became unavailable.
Route collectors and BGP-Mux let us experiment in a setting
in which we have access to these paths. We announced a preﬁx
simultaneously via BGP-Muxes at UWash, UWisc, Georgia Tech,
Princeton, and Clemson. This setup functions as an AS with ﬁve
PoPs and one provider per PoP. We iterated through 114 ASes that
provide BGP feeds. For each pair of AS A and BGP-Mux M in
turn, we poisoned A from all BGP-Muxes except M and observed
how A’s route to our preﬁx varied with M. We found that selective
poisoning allowed us to avoid 73% of the ﬁrst hop AS links used by
these peers, while still leaving the peers with a route to our preﬁx.
In §2.3, we found that these ﬁve university providers allowed us to
avoid 90% of these links on forward paths to these same ASes.
5.3 Accuracy
Having demonstrated that LIFEGUARD can often use BGP poi-
soning to route trafﬁc around an AS or AS link without causing
widespread disruption, we assess LIFEGUARD’s accuracy in locat-
ing failures. We show that LIFEGUARD seems to correctly identify
the failed AS in most cases, including many that would be not be
correctly found using only traceroutes. Our tech report provides
further analysis of LIFEGUARD’s failure isolation [32].
Are LIFEGUARD’s results consistent with what it would ﬁnd with
control of both ends of a path? In general, obtaining ground truth
for wide-area network faults is difﬁcult: emulations of failures are
not realistic, and few networks post outage reports publicly. Due to
these challenges, we evaluate the accuracy of LIFEGUARD’s failure
isolation on paths between a set of PlanetLab hosts used as LIFE-
GUARD vantage points and a disjoint set of PlanetLab hosts used as
targets.6 Every ﬁve minutes, we issued traceroutes from all vantage
points to all targets and vice versa. In isolating the location of a fail-
ure between a vantage point and a target, we only gave LIFEGUARD
access to measurements from its vantage points. We checked if
LIFEGUARD’s conclusion was consistent with traceroutes from the
target, “behind” the failure. LIFEGUARD’s location was consistent
if and only if (1) the traceroute in the failing direction terminated in
the AS A blamed by LIFEGUARD, and (2) the traceroute in the op-
posite direction did not contradict LIFEGUARD’s conclusion. The
traceroute contradicted the conclusion if it included responses from
A but terminated in a different AS. The responses from A indicate
that some paths back from A worked. We lack sufﬁcient informa-
tion to explain these cases, and so we consider LIFEGUARD’s result
to be inconsistent.
We examined 182 unidirectional isolated failures from August
and September, 2011. For 169 of the failures, LIFEGUARD’s results
were consistent with traceroutes from the targets. The remaining
13 cases were all forward failures. In each of these cases, LIFE-
GUARD blamed the last network on the forward traceroute (just as
an operator with traceroute alone might). However, the destina-
tion’s traceroute went through that network, demonstrating a work-
ing path from the network back to the destination.
Does LIFEGUARD locate failures of interest to operators? We
searched the Outages.org mailing list [28] for outages that inter-
sected LIFEGUARD’s monitored paths [32]) and found two interest-
ing examples. On May 25th, 2011, three of LIFEGUARD’s vantage
points detected a forward path outage to three distinct locations.
The system isolated all of these outages to a router in Level3’s
Chicago PoP. Later that night, a network operator posted the fol-
lowing message to the mailing list: “Saw issues routing through
6We cannot issue spoofed packets or make BGP announcements
from EC2, and so we cannot use it to evaluate our system.
Chicago via Level3 starting around 11:05 pm, cleared up about
11:43 pm.” Several network operators corroborated this report.
In the second example, a vantage point in Albany and one in
Delaware observed simultaneous outages to three destinations: a
router at an edge AS in Ohio and routers in XO’s Texas and Chicago
PoPs. LIFEGUARD identiﬁed all Albany outages and some of the
Delaware ones as reverse path failures, with the remaining Delaware
one ﬂagged as bidirectional. All reverse path failures were isolated
to an XO router in Dallas, and the bidirectional failure was iso-
lated to an XO router in Virginia. Several operators subsequently
posted to the Outages.org mailing list reporting problems with XO
in multiple locations, which is likely what LIFEGUARD observed.
Does LIFEGUARD provide beneﬁt beyond traceroute? We now
quantify how often LIFEGUARD ﬁnds that failures would be incor-
rectly isolated using only traceroute, thus motivating the need for
the system’s more advanced techniques. In the example shown in
Fig. 4, traceroutes from GMU seem to implicate a problem for-
warding from TransTelecom, whereas our system located the fail-
ure as being along the reverse path, in Rostelecom. For the pur-
poses of this study, we consider outages that meet criteria that make
them candidates for rerouting and repair: (1) multiple sources must
be unable to reach the destination, and these sources must be able
to reach at least 10% of all destinations at the time, reducing the
chance that it is a source-speciﬁc issue; (2) the failing traceroutes
must not reach the destination AS, and the outage must be partial,
together suggesting that alternate AS paths exist; (3) and the prob-
lem must not resolve during the isolation process, thereby exclud-
ing transient problems. During June 2011, LIFEGUARD identiﬁed
320 outages that met these criteria [32]. In 40% of cases, the sys-
tem identiﬁed a different suspected failure location than what one
would assume using traceroute alone. Further, even in the other
60% of cases, an operator would not currently know whether or not
the traceroute was identifying the proper location.
5.4 Scalability
How efﬁciently does LIFEGUARD refresh its path atlas? LIFE-
GUARD regularly refreshes the forward and reverse paths it moni-
tors. Existing approaches efﬁciently maintain forward path atlases
based on the observations that paths converge as they approach the
source/destination [12] and that most paths are stable [11]. Based
on these observations, we implemented a reverse path atlas that
caches probes for short periods, reuses measurements across con-
verging paths, and usually refreshes a stale path using fewer probes
than would be required to measure from scratch. In combination,
these optimizations enable us to refresh paths at an average (peak)
rate of 225 (502) reverse paths per minute. We use an amortized
average per path of 10 IP option probes (vs.
the 35 reported in
existing work [19]) and slightly more than 2 forward traceroutes.
It may be possible to improve scalability in the future by focus-
ing on measuring paths between “core” PoPs whose routing health
and route changes likely inﬂuence many paths,7 and by scheduling
measurements to minimize the impact of router-speciﬁc rate limits.
What is the probing load for locating problems? Fault isolation re-
quires approximately 280 probe packets per outage. We found that
LIFEGUARD isolates failures on average much faster than it takes
long-lasting outages to be repaired. For bidirectional and reverse
path outages, potential candidates for poisoning, LIFEGUARD com-
pleted isolation measurements within 140 seconds on average.
7For example, 63% of iPlane traceroutes traverse at least one of the
most common 500 PoPs (0.3% of PoPs) [17].
T = 0.5
d = 5 minutes
1.0
783
7866
39200
393
3931
19625
d = 15 min.
1.0
0.5
137
275
2748
1370
6874
13714
d = 60 min.
1.0
0.5
58
115
1154
576
2889
5771
I
0.01
0.1
0.5
Table 2: Number of additional daily path changes due to poisoning for
fraction of ISPs using LIFEGUARD (I), fraction of networks monitored
for reachability (T ), and duration of outage before poisoning (d). For
comparison, routers currently make 110K–315K updates per day.
What load will poisoning induce at scale? An important question
is whether LIFEGUARD would induce excessive load on routers if
deployed at scale. Our earlier study showed that, by prepending, we
can reduce the number of updates made by each router after a path
poisoning. In this section, we estimate the Internet-wide load our
approach would generate if a large number of ISPs used it. While
our results serve as a rough estimate, we ﬁnd that the number of
path changes made at each router is low.
The number of daily path changes per router our system would
produce at scale is I × T × P (d) × U, where I is the fraction
of ISPs using our approach, T is the fraction of ASes each ISP
is monitoring with LIFEGUARD, P (d) is the aggregate number of
outages per day that have lasted at least d minutes and are candi-
dates for poisoning, and U is the average number of path changes
per router generated by each poison. Based on our experiments,
U = 2.03 for routers that had been routing via the poisoned AS,
and U = 1.07 for routers that had not been routing via the poisoned
AS. For routers using the poisoned AS, BGP should have detected
the outage and generated at least one path update in response. For
routers not routing through it, the updates are pure overhead. Thus,
poisoning causes affected routers to issue an additional 1.03 up-
dates and and unaffected routers an additional 1.07 updates. For
simplicity, we set U = 1 in this analysis.
We base P (d) on the Hubble dataset of outages on paths between
PlanetLab sites and 92% of the Internet’s edge ASNs [20]. We ﬁlter
this data to exclude cases where poisoning is not effective (e.g.,
complete outages and outages with failures in the destination AS).
We assume that the Hubble targets were actually monitoring the
PlanetLab sites and deﬁne P (d) = H(d)/(IhTh), where H(d) is
the total number of poisonable Hubble outages per day lasting at
least d minutes, Ih = 0.92 is the fraction of all edge ISPs that
Hubble monitored, and Th = 0.01 is our estimate for the fraction
of total poisonable (transit) ASes on paths from Hubble VPs to their
targets. Because the smallest d that Hubble provides is 15 minutes,
we extrapolate the Hubble outage distribution based on the EC2
data to estimate the poisoning load for d = 5.
Scaling the parameters estimates the load from poisoning under
different scenarios. In Table 2, we vary the fraction of participating
ISPs (I), the fraction of poisonable ASes being monitored (T ), and
the minimum outage duration before poisoning is used (d). We
scale the number of outages linearly with I and T . We scale with
d based on the failure duration distribution from our EC2 study.
LIFEGUARD could cause a router to generate from tens to tens
of thousands of additional updates. For reference, a single-homed
edge router peering with AS131072 sees an average of approxi-
mately 110K updates per day [4], and the Tier-1 routers we checked
made 255K-315K path updates per day [26]. For cases where only
a small fraction of ASes use poisoning (I ≤ 0.1), the additional
load is less than 1%. For large deployments (I = 0.5, T = 1) and
short delays before poisoning (d = 5), the overhead can become
signiﬁcant (35% for the edge router, 12-15% for Tier-1 routers).
We note that reducing the number of monitored paths or waiting
longer to poison can easily reduce the overhead to less than 10%.
6. LIFEGUARD CASE STUDY
To demonstrate LIFEGUARD’s ability to repair a data plane out-
age by locating a failure and then poisoning, we describe a failure
between the University of Wisconsin and a PlanetLab node at Na-
tional Tsing Hua University in Taiwan. LIFEGUARD announced
production and sentinel preﬁxes via the University of Wisconsin
BGP-Mux. LIFEGUARD had monitored the PlanetLab node for a
month, gathering historical data. On October 3-4, 2011, nodes in
the two preﬁxes exchanged test trafﬁc with the PlanetLab node,
and we sent traceroutes every 10 minutes from the PlanetLab node
towards the test nodes to track its view of the paths. We use the
measurements from the Taiwanese node to evaluate LIFEGUARD.
After experiencing only transient problems during the day, at
8:15pm on October 3, the test trafﬁc began to experience a persis-
tent outage. When LIFEGUARD isolated the direction of the outage,
spoofed pings from Wisconsin reached Taiwan, but spoofed pings
towards Wisconsin failed, indicating a reverse path problem. LIFE-
GUARD’s atlas revealed two historical paths from Taiwan back to
Wisconsin. Prior to the outage, the PlanetLab node had been suc-
cessfully routing to Wisconsin via academic networks. According
to the atlas, this route had been in use since 3pm, but, prior to that,
the path had routed through UUNET (a commercial network) for
an extended period. The UUNET and academic paths diverged one
AS before UUNET (one AS closer to the Taiwanese site). LIFE-
GUARD issued measurements to see which of the hops on these re-