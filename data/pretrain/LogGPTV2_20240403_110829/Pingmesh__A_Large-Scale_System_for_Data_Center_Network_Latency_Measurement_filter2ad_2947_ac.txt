network architecture as we have introduced in Figure
1, though they may vary in size and may be built at
diﬀerent times.
(a) CPU usage
(b) Memory usage
Figure 3: CPU and memory usages of Pingmesh Agent.
Second, a Pingmesh Agent needs to launch pings to
several thousand of servers by design. But as a shared
service, the Pingmesh Agent should minimize its re-
sources (CPU, memory, and disk space) usage. It should
use close to zero CPU time and as small memory foot-
print as possible, so as to minimize its interference with
customers’ applications.
In order to achieve the performance goal and improve
Pingmesh’s latency measurement accuracy, we use C++
instead of Java or C# to write Pingmesh Agent. This
is to avoid the common language runtime or Java vir-
tual machine overhead. We have developed a network
library speciﬁcally for Pingmesh. The goal of the li-
brary is solely for network latency measurement, and
it is designed to be light-weight and to handle a large
number of concurrent TCP connections. The library
is directly based on the Winsock API, and it uses the
Windows IO Completion Port programming model for
eﬃcient asynchronous network IO processing. The li-
brary acts as both client and server, and it distributes
the probing processing load to all the CPU cores evenly.
We have done extensive measurements to understand
and optimize Pingmesh Agent’s performance. Figure 3
shows the CPU and memory usages of the Pingmesh
Agent on a typical server. During the measurement,
this Pingmesh Agent was actively probing around 2500
servers. The server has 128GB memory and two In-
tel Xeon E5-2450 processors, each with 8 cores. The
average memory footprint is less than 45MB, and the
average CPU usage is 0.26%.
We note that the probing traﬃc generated by a Pingmesh
Agent is small, typically tens of Kb/s. As a comparison,
our data center network provides several Gb/s through-
put between any two servers in a data center.
3.5 Data Storage and Analysis
For Pingmesh data storage and analysis, we use the
well established existing systems, Cosmos/SCOPE and
Autopilot’s Perfcounter Aggregator (PA) service,
in-
stead of reinventing the wheel.
The Pingmesh Agent periodically uploads the aggre-
gated records to Cosmos. Similar to the Pingmesh Con-
troller, the front-end of Cosmos uses load-balancer and
VIP to scale out. At the same time, the Pingmesh
1444.1 Network latency
Figure 4 shows the intra-DC latency distribution of
two representative data centers DC1 in US West and
DC2 in US Central. DC1 is used by distributed storage
and MapReduce and DC2 is by an interactive Search
service. Servers in DC1 are throughput intensive and
the average server CPU utilization is as high as 90%.
Servers in DC1 use the network heavily and transmit
and receive several hundreds of Mb/s data on average
all the time. DC2 is latency sensitive and servers have
high fan-in and fan-out in that a server needs to com-
municate with a large number of other servers to service
a Search query. The average CPU utilization in DC2 is
moderate and the average network throughput is low
but the traﬃc is bursty.
The CDFs in Figure 4 are calculated from latency
data of one normal working day, when there were no
network incidents detected and no live-site incidents be-
cause of the network. We track both intra-pod and
inter-pod latency distributions, with and without TCP
payload. If not speciﬁcally mentioned, the latency we
use in the paper is the inter-pod TCP SYN/SYN-ACK
RTT without payload.
Figure 4(a) shows the overall inter-pod latency dis-
tributions and Figure 4(b) shows the inter-pod distri-
bution at high percentile. We once expected that the
latency of DC1 should be much larger than DC2 since
servers and the network in DC1 are highly loaded. But
this turned out not the case for latencies at the 90th or
lower percentile.
But DC1 does have much higher latency at the high
percentile as shown in Figure 4(b). At P99.9, the inter-
pod latencies are 23.35ms and 11.07ms for DC1 and
DC2, respectively. At P99.99, the inter-pod latencies
become 1397.63ms and 105.84ms. Our measurement
result shows it is hard to provide low latency (e.g., sub-
milliseconds level) at three or four 9s, even when the
servers and network are both light-loaded at macro time
scale. This is because the server OS is not a real-time
operating system and the traﬃc in our network is burst.
We see 10−5 packet drop rate for intra-pod communica-
tions (Section 4.2) even when average network utiliza-
tion is low to moderate.
Figure 4(c) compares the intra-pod and inter-pod la-
tency distributions, and Figure 4(d) studies the inter-
pod latency with and without payload, all in DC1. For
latency measurement with payload, after TCP connec-
tion setup, we let the client send a message (typically
800-1200 bytes within one packet). The client measures
the payload latency once it receives the echoed back
message from the server.
As shown in Figure 4(c), intra-pod latency is always
smaller than inter-pod latency as expected. The 50th
(P50) and the 99th (P99) intra-pod and inter-pod laten-
cies are (216us, 1.26ms) and (268us, 1.34ms) for DC1.
The diﬀerences at P50 and P99 are 52us and 80us, re-
spectively. These numbers show that the network does
Data center
Packet drop rate
DC1 (US West)
DC2 (US Central)
DC3 (US East)
DC4 (Europe)
DC5 (Asia)
Intra-pod
1.31 × 10−5
2.10 × 10−5
9.58 × 10−6
1.52 × 10−5
9.82 × 10−6
Inter-pod
7.55 × 10−5
7.63 × 10−5
4.00 × 10−5
5.32 × 10−5
1.54 × 10−5
Table 1: Intra-pod and inter-pod packet drop rates.
introduce tens of microsecond latency due to queuing
delay. But the queuing delay is small. Hence we can in-
fer that the network provides enough network capacity.
Figure 4(d) shows the latency diﬀerence with and
without payload. With payload, the latency increases
from 268us to 326us at P50, and from 1.34ms to 2.43ms
at P99, respectively. The increase is mainly because of
the increased transmission delay 1 and the user space
processing overhead for the receiving servers to echo
back the message. In most cases, the latency distribu-
tions with and without payload are similar. We intro-
duced payload ping because it can help detect packet
drops that are related to packet length (e.g., ﬁber FCS
errors and switch SerDes errors that are related to bit
error rate).
Based on the Pingmesh data, we are able to calculate
not only the latency distributions of the data centers,
but also the latency CDFs for all the applications and
services. From these results, we are able to track net-
work latency for all of them all the time.
4.2 Packet drop rate
Pingmesh does not directly measure packet drop rate.
However, we can infer packet drop rate from the TCP
connection setup time. When the ﬁrst SYN packet is
dropped, TCP sender will retransmit the packet after
an initial timeout. For the rest successive retries, TCP
will double the timeout value every time. In our data
centers, the initial timeout value is 3 seconds, and the
sender will retry SYN two times. Hence if the measured
TCP connection RTT is around 3 seconds, there is one
packet drop; if the RTT is around 9 seconds, there are
two packet drops. We use the following heuristic to
estimate packet drop rate:
probes with 3s rtt + probes with 9s rtt
total successf ul probes
.
Note that we only use the total number of successful
TCP probes instead of the total probes as the denom-
inator. This is because for failed probes, we cannot
diﬀerentiate between packet drops and receiving server
failure.
In the numerator, we only count one packet
drop instead of two for every connection with 9 second
RTT. This is because successive packet drops within a
connection are not independent: the probability the sec-
ond SYN is dropped is much higher if the ﬁrst SYN is
1We have disabled cut-through switching at
the
switches in our data centers. This is to stop FCS er-
rors from propagation.
145(a)
(b)
(c)
(d)
Figure 4: (a)Inter-pod latency of two data centers. (b) Inter-pod latency at high percentile. (c) Intra-pod and
inter-pod latency comparison. (d) Latency comparison with and without payload.
dropped. We have veriﬁed the accuracy of the heuristic
for a single ToR network by counting the NIC and ToR
packet drops.
In our network, SYN packets are treated the same as
other packets. Hence the drop rate of SYN packets can
be considered representative drop rate of the other data
packets in normal condition. This assumption, however,
may not be true when packet drop rate is related to
packet size, e.g., due to FCS errors. We did see packets
of larger size may experience higher drop rate in FCS
error related incidents. In what follows, the results we
present are when the networks were in normal condition.
Our network does not diﬀerentiate packets of diﬀerent
IP protocols (e.g., TCP vs. UDP). Hence, our packet
drop calculation holds for non-TCP traﬃc as well.
Table 1 shows the packet drop rates of ﬁve data cen-
ters. We show both the intra-pod and inter-pod packet
drop rates. For intra-pod packet drops, those are drops
at ToR switch, NIC, and end-host network stack. The
inter-pod packet drops may come from the Leaf and
Spine switches and the corresponding links, in addition
to the ToR, NIC, and end-host stack.
From Table 1, several observations can be made. First,
the packet drop rates are in the range of 10−4 − 10−5.
We track the packet drop rates for all our data cen-
ters every day and we ﬁnd the drop rate is in this
range unless network incidents happen. Second, the
inter-pod packet drop rate is typically several times
higher than that of intra-pod. This indicates most of
the packet drops happen in the network instead of the
hosts. Third, the intra-pod drop rate is around 10−5,
which is larger than we have expected.
Our experience tells us packet drops may occur due
to many reasons, e.g., switch buﬀer overﬂow, NIC re-
ceiving buﬀer overﬂow, optical ﬁber FCS errors, switch-
ing ASIC malfunction, etc. Though our measurement
results suggest that the packet drop rate at normal con-
dition is around 10−4 − 10−5, we are still at the early
phase in understanding why it stays in this range.
Many data center applications, e.g., Search, may use
hundreds or even thousands of TCP connections simul-
taneously. For these applications, high latency tail there-
fore becomes the norm due to the large number of con-
(a) The 99th percentile la-
tency
(b) Packet drop rate
Figure 5: The 99th network latency and packet drop
rate metrics for a service.
nections used. Applications have introduced several ap-
plication level tricks to deal with packet drops [10].
From the per server latency data, we can calculate
and track network SLAs at server, pod, podset, and
data center levels. Similarly, we can calculate and track
network SLA for individual services.
4.3 Is it a network issue?
In large distributed data center systems, many parts
may go wrong. When a live-site incident happens, it
is not easy to identify which part causes the problem.
There are occasions that all the components seem ﬁne
but the whole system is broken. If the network cannot
prove it is innocent, the problem will then be called a
“network problem”: I did not do anything wrong to my
service, it must be the fault of the network.
The network team is then engaged to investigate. A
typical procedure is as follows. The network on-call
engineer asks the service which is experiencing issues
for detailed symptoms and source-destination pairs; he
then logs into the source and/or destination servers and
runs various network tools to reproduce the issue; he
may also look at the switch counters along the possible
paths for anomaly; if he cannot reproduce, he may ask
for more source-destination pairs. The procedure may
need several rounds of iterations.
The above approach does not work well for us, since
it is a manual process and does not scale. If the issue
turns out not to be caused by the network, the service
owners waste their time in engaging with the wrong
146team.
If the issue is indeed because of the network,
the manual process causes long time-to-detect (TTD),
time-to-mitigate (TTM), and time-to-resolve (TTR).
Pingmesh changed the situation. Because Pingmesh
collects latency data from all the servers, we can always
pull out Pingmesh data to tell if a speciﬁc service has
network issue or not. If Pingmesh data does not cor-
relate to the issue perceived by the applications, then
it is not a network issue. If Pingmesh data shows it is
indeed a network issue, we can further get detailed data
from Pingmesh, e.g., the scale of the problem (e.g., how
many servers and applications are aﬀected), the source-
destination server IP addresses and TCP port numbers,
for further investigation.
We deﬁne network SLA as a set of metrics includ-
ing packet drop rate, network latency at the 50th per-
centile and the 99th percentile. Network SLA can then
be tracked at diﬀerent scopes including per server, per
pod/podset, per service, per data center, by using the
Pingmesh data. In practice we found two network SLA
metrics: packet drop rate and network latency at the
99th percentile are useful for telling if an issue is caused
by the network or not. Figure 5 shows these two met-
rics for a service in one normal week. The packet drop
rate is around 4 × 10−5 and the 99th percentile latency
in a data center is 500-560us. (The latency shows a pe-
riodical pattern. This is because this service performs
high throughput data sync periodically which increases
the 99th percentile latency.) If these two metrics change
signiﬁcantly, then it is a network issue.