We use cryptographic primitives provided by the OpenSSL, Sodium,
SimpleOT, and MIRACL libraries [1ś3, 18]. SIMD CPU instructions
are used in critical regions of code to improve performance. Our
3The code is available online at .
Session 3C: Secure Computing IICCS ’19, November 11–15, 2019, London, United Kingdom624Table 2: LogLog circuit properties after XORed relay inputs.
Table 3: Default network parameters used in experiments.
# Counters
# Input Bits (×106)
# Gates (×106)
# AND Gates (×106)
Depth
AND Depth
Std. Error
128
0.23
0.47
0.23
87
49
512
0.90
1.89
0.90
97
51
11.5% 5.7%
1,024
1.80
3.78
1.81
102
52
4.1%
4,096
7.21
15.1
7.2
112
54
2.0%
8,192
14.4
30.2
14.5
117
55
1.4%
implementation does omit some low-level details; for example, it
does not recover gracefully from host timeouts/disconnects, and it
is not hardened against timing-analysis attacks. We do not expect
these omitted details to significantly affect the performance charac-
teristics of our implementation. We measure the performance of
each protocol piece in isolation (Sections 7.2ś7.3) and also provide
holistic end-to-end estimates of Stormy’s performance (Section 7.4).
7.1 Methodology
We use the Shadow network simulator [41] to analyze the protocols’
network costs. Shadow is a tool frequently used to simulate the Tor
network and modifications/additions to Tor’s protocols [40, 46, 64].
Using Shadow allows us to run our protocol implementations at
network-scale and on networks with properties that accurately
model the Tor network. We explore ranges of network parameters in
our experiments, e.g., by varying bandwidth, latency, and committee
size. We measure the time required to complete an honest protocol
execution and the number of application-layer bytes transmitted
by each host. Usually in our protocols the hosts send and receive
an equal number of bytes; in cases of asymmetry we present łdata
transmitted (TX)ž as the max of bytes sent and received.
Table 3 shows the default network parameters used across Shadow
experiments. In the Authority Model, a single Computation Com-
mittee with high bandwidth (1 Gbps by default) performs both the
offline preprocessing and the online computation. We use a one-
way latency of 50 ms between the authorities, which corresponds
to the median latency between Tor relays reported in the 2015 mea-
surement study of Cangialosi et al. [16]. In the Relay Model, many
parallel Triple Committees generate and transfer authenticated
triples; the Computation Committee generates authenticated bits,
receives relay inputs, and performs the online computation. For
the default bandwidth allocations of the TC and CC members, we
used the median active-committee bandwidths of 100 simulations
of the committee-election process on a 2018-10-01 Tor consensus
with mTC = 1, 000 sampled TCs, mCC = 8 sampled CCs, and a
committee size of c = 25. We conservatively set the committee
members’ latencies to 250 ms, which corresponds to the highest la-
tency measured between any two Tor relays [16]. In both RelMode
and AuthMode, when running input-sharing experiments, we use
7,000 input parties, which is an upper bound on the number of Tor
relays in 2018-10. All input parties are configured pessimistically
with 20 kbps links, which is the minimum bandwidth advertised by
any relay in the Tor network.
Using Shadow comes with a couple of limitations. First, Shadow
measures only the network performance of the protocols; computa-
tional performance (e.g. CPU usage or memory consumption) is not
captured in the simulations. However, the protocols used in Stormy
BW
Latency
Parties
RelMode
Triple Committee
Computation Committee
AuthMode (CC)
Input Party
6.4 Mbps
12 Mbps
1 Gbps
20 kbps
250 ms
250 ms
50 ms
250 ms
25
25
5
7,000
are computationally inexpensive. The offline preprocessing consists
primarily of symmetric-key operations, and the online computation
requires almost no cryptographic operations. Communication costs
dominate protocol runtime, and so our Shadow experiments should
closely estimate total protocol runtime. Second, it takes prohibi-
tively long to simulate network-scale operation on long timescales
(e.g. many hours). Therefore, when estimating the end-to-end per-
formance in RelMode, we instead use a custom event simulator that
incorporates results from the Shadow experiments (see Sec. 7.4).
7.2 Offline Preprocessing
7.2.1 Authenticated Triple Generation. Recall from Section 4 that
one secret-shared authenticated AND triple is required for each
AND gate evaluated during the online phase. Triple generation
generally requires the most time and communication of all the
components of Stormy. We evaluate the cost of triple generation by
running experiments in which a single committee generates a batch
of authenticated triples. The TC in RelMode generates batches of
5,112 triples at a time, and the CC in AuthMode generates batches
of 280,000 triples. Smaller batch sizes require less memory, but
reduce the protocol’s efficiency. We choose batch sizes that are
minimal at a given level of efficiency.
In the default configuration, a single TC in RelMode generates
a batch of 5,112 triples in 210 s. 91 MiB of data is sent/received
by each of the relays. The CC in AuthMode generates a batch of
280,000 triples in 9.0 s. 420 MiB of data is sent/received by each
authority.
Figures 5.1ś5.5 present the throughput and costs of generating
triples when produced in various experimental setups. The Auth-
Mode CC produces batches of 280,000 and a RelMode TC produces
batches of 5,112 in all experiments. A single RelMode TC achieves
triples
second for realistic com-
modest throughputs in the range [2, 43]
mittee bandwidth capacities; however, since many TCs generate
triples in parallel, RelMode’s network-wide, overall throughput
is much higher (7.4). The single AuthMode CC achieves much
triples
second because of their
higher throughputs in the range [2.7k, 36k]
assumed high-bandwidth links. Each triple costs 18 KiB of com-
munication per-party in the 25-member TC. In the 5-member CC,
each triple costs only 1.5 KiB due to the smaller committee size and
lower β-overhead achieved at the larger batch size (see Section 4).
Oblivious-transfer (OT) extensions performed pairwise between
each host constitute the dominant cost of triple generation. We
find that increasing available bandwidth can significantly improve
runtime because the large messages sent during OT extension can
be transmitted more quickly. Because each party performs 6 OT
extensions with every other party, the runtime/transmission-cost
of triple generation scales linearly with the size of the commit-
tee. The offline-phase protocols do not require many rounds of
communication so latency has a minimal effect on runtime.
Session 3C: Secure Computing IICCS ’19, November 11–15, 2019, London, United Kingdom6251 Triples | Thruput vs BW (RelMode)
2 Triples | Thruput vs BW (AuthMode)
3 Triples | Thruput vs Parties (RelMode)
RelMode
AuthMode
d
n
o
c
e
s
r
e
p
s
e
l
p
i
r
T
k
0
3
k
0
2
k
0
1
d
n
o
c
e
s
r
e
p
s
e
l
p
i
r
T
0
3
0
2
RelMode
1 Mbps
10 Mbps
Bandwidth
100 Mbps
1 Gbps
Bandwidth
10 Gbps
10
20
30
40
50
Num. Parties
4 Triples | Thruput vs Parties (AuthMode)
AuthMode
e
l
p
i
r
T
r
e
p
X
T
a
t
a
D
B
k
0
1
B
k
1
5 Triples | Data TX vs Parties
6 Median | Runtime vs Latency
AuthMode
RelMode
e
m
i
t
n
u
R
i
n
m
0
1
i
n
m
1
AuthMode
RelMode
2
4
6
8
10
Num. Parties
0
10
20
30
Num. Parties
40
50
10 ms
100 ms
Latency
7 Median | Runtime vs Num Inputs
8 Set Cardinality | Runtime vs Latency
AuthMode
RelMode
AuthMode
RelMode
s
0
1
e
m
i
t