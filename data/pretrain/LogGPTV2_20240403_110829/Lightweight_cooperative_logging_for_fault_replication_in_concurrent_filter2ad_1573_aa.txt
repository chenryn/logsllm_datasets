title:Lightweight cooperative logging for fault replication in concurrent
programs
author:Nuno Machado and
Paolo Romano and
Lu&apos;ıs E. T. Rodrigues
Lightweight Cooperative Logging for Fault Replication in Concurrent Programs
Nuno Machado, Paolo Romano, Lu´ıs Rodrigues
INESC-ID, Instituto Superior T´ecnico, Universidade T´ecnica de Lisboa
Abstract—This paper presents CoopREP, a system that
provides support for fault replication of concurrent programs,
based on cooperative recording and partial log combination.
CoopREP employs partial recording to reduce the amount
of information that a given program instance is required to
store in order to support deterministic replay. This allows
to substantially reduce the overhead imposed by the instru-
mentation of the code, but raises the problem of ﬁnding the
combination of logs capable of replaying the fault. CoopREP
tackles this issue by introducing several innovative statistical
analysis techniques aimed at guiding the search of partial
logs to be combined and used during the replay phase.
CoopREP has been evaluated using both standard benchmarks
for multi-threaded applications and a real-world application.
The results highlight that CoopREP can successfully replay
concurrency bugs involving tens of thousands of memory
accesses, reducing logging overhead with respect to state of
the art non-cooperative logging schemes by up to 50 times in
computationally intensive applications.
Keywords-concurrency errors; deterministic replay; debug-
ging; performance
I. INTRODUCTION
Software bugs continue to hamper the reliability of soft-
ware. It is estimated that bugs account for 40% of system
failures [1]. Unfortunately, despite the progress made on
the development of techniques that prevent and correct
errors during software production (e.g. formal methods [2]),
a signiﬁcant number of errors still reaches production [3].
This problem is exacerbated by the advent of multi-core
systems and the increasing complexity of modern software.
Therefore, it is imperative to develop tools that simplify the
task of debugging the software, for instance, by providing
the means to replay a faulty execution.
A fundamental challenge is that achieving deterministic
replay is far from trivial, especially in parallel applications.
Contrary to most bugs in sequential program, that usually de-
pend exclusively on the program input and on the execution
environment (and therefore can be more easily reproduced),
concurrency bugs have an inherently non-deterministic na-
ture. This means that even when re-executing the same code
with identical inputs, on the same machine, the program
outcome may be differ from run to run [4].
The deterministic replay technique addresses this prob-
lem by recording the relevant details of the execution [5]
(including the order of access to shared memory regions,
thread scheduling, program inputs, signals, etc) to support
the reproduction of the original run. However, logging all the
required information induces a large space and performance
overhead during production runs.
In the past decade, a signiﬁcant amount of research
has been performed on techniques to provide deterministic
replay (either based on hardware or software). Several of
these solutions [6], [7], [8], [9] aim at replaying the bug
on the ﬁrst attempt, but this comes at an excessively high
cost on the original run (10x-100x slowdown), making the
approach impractical in most settings.
Since the most signiﬁcant performance constraints are on
the production version, it becomes of paramount importance
to reduce its instrumentation overhead, even if it results in
a slightly longer reproduction time during diagnosis.
In this paper we introduce and evaluate the idea of
exploiting the coexistence of multiple instances of the same
program to devise cooperative logging schemes. The under-
lying intuition is very simple: sharing the burden of logging
among multiple instances of the same (buggy) program, by
having each instance track accesses that only target a subset
of the program’s shared variables. The partial logs recorded
by different instances of the same program are then gathered
at the software maintenance side, where they are statistically
analyzed in order to identify sets of partial logs whose
combination maximizes the chances to successfully replay
the bug. We have named the resulting system CoopREP
(standing for Cooperative Replay), a deterministic replay
system that leverages on cooperative logging performed by
multiple clients and on statistical techniques to combine the
collected partial logs. One of the main contributions of this
paper is to show that cooperative logging is a viable strategy
to replicate concurrency bugs with low overhead. Addition-
ally, the paper also makes the following contributions:
• A set of novel statistical metrics to detect correlations
among partial logs, that have been independently col-
lected by different clients;
• A novel heuristic, named Similarity-Guided Merge, that
leverages on these metrics to systematically perform
a guided search, among the possible combinations of
partial logs. The goal is to ﬁnd those that generate
complete replay drivers capable of reproducing the bug.
• An experimental evaluation of the implemented pro-
totype of CoopREP, based on standard benchmarks
for multi-threaded applications and on a real-world
application.
The rest of this document is structured as follows: Sec-
tion II presents the background concepts related to this work.
Section III overviews some deterministic replay and statis-
tical debugging systems. Section IV introduces CoopREP,
describing in detail its architecture, the Similarity-Guided
Merge heuristic, and the metrics used to capture the similar-
ity among partial logs. Section V presents the results from
the experimental evaluation. Finally, Section VI concludes
the paper by summarizing its main points and discussing
future work.
II. BACKGROUND
A. Deterministic replay
Deterministic replay (or record/replay) aims to overcome
the problems associated with the reproduction of bugs, in
particular those raised by non-determinism. The purpose
of this technique is to re-execute the program, obtaining
the exact same behavior as the original execution. This is
possible because almost all instructions and states can be
reproduced as long as all possible non-deterministic factors
that have an impact on the program execution are replayed
in the same way [4]. Thereby, deterministic replay operates
in two phases:
1) Record phase – consists of capturing data regarding
information into a
non-deterministic events, putting that
trace ﬁle.
2) Replay phase – the application is re-executed consult-
ing the trace ﬁle to force the replay of non-deterministic
events according to the original execution.
B. Sources of Non-determinism
External factors often interfere with the program execu-
tion, preventing the timing and the sequence of instructions
executed to be always identical. The sources of these factors
can be divided into two types: input non-determinism and
memory non-determinism [10].
Input non-determinism encompasses all the inputs that
are received by the system layer being logged but are not
originated in that layer (e.g. signals, system calls, hardware
interrupts, DMA, keyboard and network inputs, etc). This
kind of non-determinism is present in both single-processor
and multi-processor machines.
Memory non-determinism in single-processor systems is
mainly due to the thread interleaving in the access to shared
memory locations, which may vary from run to run (and,
with a lesser extent, from reads to un-initialized memory
locations). Memory non-determinism can be tackled by
using “logic time” [11] to log the events, instead of ordinary
physical time. In fact, logical time may be sufﬁcient to sup-
port deterministic replay in single-processor systems [12].
However, in multi-processor systems (e.g. SMPs and multi-
cores) the scenario is more complex since it is necessary
to take into account how threads that execute concurrently
on different processors may interleave with each other.
Therefore, one needs to capture the global order of shared
memory accesses and synchronization points (obviously, this
is not a problem when threads are independent from each
other).
III. RELATED WORK
There are various approaches to prevent bugs in a program
or to optimize the debugging process. In this section we
focus on approaches that aim at reproducing the failure or
to statistically isolate it, as these are the most relevant to
our work. Among the deterministic replay solutions, over
the past few years, several solutions have been proposed to
cope with the challenges brought by multi-processors. Based
on how they are implemented, prior work can be divided in
two main categories: hardware-based and software-based.
Hardware-based solutions rely on hardware extensions
to efﬁciently record the non-deterministic events and, con-
sequently, mitigate the overhead imposed to the produc-
tion run. Flight Data Recorder [13] and BugNet [14] have
provided solutions to achieve this, but at a cost of non-
trivial hardware extensions. More recently, DeLorean [15]
and Capo [16] have proposed new techniques to reduce the
complexity of the extensions. Despite that, they still require
changing non-commodity hardware, which can be costly.
Regarding software-based approaches, InstantReplay [8]
was the ﬁrst deterministic replay system to support multi-
processors. It leverages on an instrumented version of the
Concurrent-Read Exclusive-Write (CREW) protocol [8] to
control and log the accesses to shared memory locations.
In turn, DejaVu [7] logs the order of thread “critical events”
(e.g. synchronization points and accesses to shared vari-
ables) and uses global clocks to enforce their execution in
total order at replay time. However, this technique incurs
high performance overhead and requires large trace ﬁles.
JaRec [6] reduces the overheads imposed by InstantReplay
and DejaVu, by dropping the idea of global ordering and
using a Lamport’s clock [11] to preserve the partial order
of threads with respect to synchronization points. However,
JaRec requires a program to be data race free in order to
guarantee a correct replay, otherwise it only ensures deter-
ministic replay until the ﬁrst data race. This constraint makes
this approach unsuitable for most real world concurrent
applications, given that is common the existence of both
benign and harmful data races. LEAP [9] addresses JaRec
issues by tracking all shared memory accesses, in addition
to those performed on monitors. However, to minimize the
runtime overheads, LEAP only keeps a local trace for each
shared variable, containing the order of the thread accesses
to that variable.
All the previous approaches try to reproduce the bug on
the ﬁrst replay run, thus inducing large overheads during pro-
duction runs, with the drawback of penalizing also bug-free
executions, which are much more frequent than the faulty
ones [4]. Motivated by this, some recent solutions, such as
PRES [4], ODR [17], and ESD [18], relax the constraint of
replaying the bug at the ﬁrst attempt, by only logging partial
information (or even none, in the case of ESD) in order to
further minimize the cost of recording the original execution.
Later, these solutions apply inference techniques to complete
the missing information.
the ﬁrst attempt, but
Our solution, denoted CoopREP, is also based on the
observation that it is not crucial to achieve deterministic
replay at
improves previous work
as it leverages on information logged by multiple clients
to ease the inference task. For this, CoopREP draws on
statistical debugging techniques, which aim at isolating bugs
by analyzing information gathered from a large number of
users.
Statistical debugging was pioneered by CBI [19]. This
system collects feedback reports that contain values recorded
for certain predicates of the program (e.g. branches, return
values, etc). Then, performs a statistical analysis of the
information gathered in order to pinpoint the likely source
of the failure. However, CBI does not support concurrency
bugs. CCI [20] outstrips this limitation by adjusting CBI’s
principles to cope with non-deterministic events. For in-
stance, it implements cross-thread sampling and relies on
longer sampling periods, because concurrency bugs always
involve multiple memory accesses. CoopREP differs from
CCI in the sense that we are concerned with the bug
reproduction, whereas CCI strives to identify and isolate
predictors that can explain the root causes of failures caused
by concurrency errors.
Since recording and replaying input non-determinism can
be achieved with an overhead less than 10% [4], [5], [12],
we only focus on coping with memory non-determinism. In
fact, a recent study on the evolution of the types of errors in
MySQL database [21] shows a growth trend in the number
and proportion of concurrency bugs over the years. Thereby,
CoopREP addresses the deterministic replay of this kind
of bugs (e.g. atomicity violations, data races), disregarding
other sources of non-determinism.
IV. COOPREP SYSTEM
This section describes CoopREP, a system that provides
fault replication of concurrent programs, based in coop-
erative recording and partial log combination. Given that
CoopREP reuses and extends several building blocks that
were originally introduced in LEAP [9], we begin by pre-
senting an overview of LEAP.
A. LEAP
LEAP [9] proposes a general technique for the determin-
istic replay of Java concurrent programs in multi-processors.
It is based on the insight that, to achieve deterministic replay,
it is sufﬁcient to record the local order of thread accesses
to shared variables, instead of enforcing a global order. To
track thread accesses, LEAP associates an access vector to
each different shared variable. During execution, whenever
a thread reads or writes in a shared variable, its ID is stored
in the access vector. For instance, let us assume a program
P with a shared variable x and running with two threads
(t1 and t2). If, during the execution of P , x is accessed one
time by t1 and, later, two times by t2, the access vector of
x will be .
Using this technique, one gets (local) order vectors of
thread accesses performed on individual shared variables,
instead of a global-order vector. This provides lightweight
recording, but relaxes faithfulness in the replay, allowing
thread interleavings that are different from the original exe-
cution. However, in [9] the authors claim that this approach
does not affect the error reproduction, and they formally
prove the soundness of this statement.
To locate the shared program elements (SPEs), LEAP
uses a static escape analysis called thread-local objects
analysis [22] from the Soot1 framework. Given that accu-
rately identifying shared variables is generally an unde-
cidable problem, this technique computes a sound over-
approximation, i.e. every shared access to a ﬁeld is indeed
identiﬁed, but some non-shared ﬁelds may also be classiﬁed
as shared [9].
it
SPEs include variables that serve as monitors (including
Java monitors) and other shared ﬁeld variables (including
class and thread escaped instance variables). For each identi-
ﬁed SPE, LEAP assigns ofﬂine a numerical index in order to
be able to consistently identify objects across different runs.
Moreover, as access vectors only contain thread IDs tracked
during the production run,
is imperative to correctly
recognize each thread in both recording and replay phases.
LEAP achieves this by maintaining a mapping between the
thread name and the thread ID during recording and using
the same mapping for replay. In addition, it uses a list with
the parent threads’ IDs to track the global order in which
they create their child threads, thus ensuring the same thread
creation order when recording and replaying the execution.
The overall infrastructure of LEAP, depicted in Figure 1,
consists of three major components: the transformer, the
recorder, and the replayer.
The transformer receives the Java program bytecode and
employs two types of instrumentation schemes to produce
the record version and the replay version, respectively.
LEAP instruments the following code instructions: i) SPE
accesses, ii) thread creation information, and iii) recording
end points.
The record version is then executed and when a program
end point is reached, the recorder saves both the recorded
access vectors and the thread ID map information. In addi-
tion, the recorder also creates the replay driver, i.e. a Java
ﬁle containing the code needed to execute the replay version
of the program and initiate both the thread scheduler and the
trace loader components.
Finally, the replayer uses the logged information and the
1http://www.sable.mcgill.ca/soot
Figure 1. Overview of the LEAP architecture (adapted from [9]).
generated replay driver to replay the program. To control the
interleaving of thread execution (and enforce a deterministic
replay), LEAP takes control of the thread scheduling and
consults the thread ID map information ﬁle.
The evaluation study presented in [9] has shown that
LEAP incurs a runtime overhead ranging from 7.3% to
626% (for applications with several shared variables ac-
cessed in hot loops). In terms of space overhead, the log
size in LEAP is still considerable, ranging from 51 to 37760
KB/sec.
As we will explain, CoopREP re-uses some key concepts
of LEAP, in particular the idea of logging accesses on a
per-SPE basis. However, by introducing the notion of coop-
erative logging, CoopREP allows for achieving signiﬁcant
(up to one order of magnitude) reductions of the logging
overhead incurred by LEAP and, more in general, by classic