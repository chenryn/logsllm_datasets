title:Utility-Aware Synthesis of Differentially Private and Attack-Resilient
Location Traces
author:Mehmet Emre Gursoy and
Ling Liu and
Stacey Truex and
Lei Yu and
Wenqi Wei
Utility-Aware Synthesis of Differentially Private and
Attack-Resilient Location Traces
Mehmet Emre Gursoy, Ling Liu, Stacey Truex, Lei Yu, and Wenqi Wei
College of Computing, Georgia Institute of Technology
ABSTRACT
As mobile devices and location-based services become increasingly
ubiquitous, the privacy of mobile users’ location traces continues to
be a major concern. Traditional privacy solutions rely on perturbing
each position in a user’s trace and replacing it with a fake loca-
tion. However, recent studies have shown that such point-based
perturbation of locations is susceptible to inference attacks and
suffers from serious utility losses, because it disregards the moving
trajectory and continuity in full location traces.
In this paper, we argue that privacy-preserving synthesis of com-
plete location traces can be an effective solution to this problem. We
present AdaTrace, a scalable location trace synthesizer with three
novel features: provable statistical privacy, deterministic attack
resilience, and strong utility preservation. AdaTrace builds a gen-
erative model from a given set of real traces through a four-phase
synthesis process consisting of feature extraction, synopsis learn-
ing, privacy and utility preserving noise injection, and generation
of differentially private synthetic location traces. The output traces
crafted by AdaTrace preserve utility-critical information existing in
real traces, and are robust against known location trace attacks. We
validate the effectiveness of AdaTrace by comparing it with three
state of the art approaches (ngram, DPT, and SGLT) using real loca-
tion trace datasets (Geolife and Taxi) as well as a simulated dataset
of 50,000 vehicles in Oldenburg, Germany. AdaTrace offers up to
3-fold improvement in trajectory utility, and is orders of magnitude
faster than previous work, while preserving differential privacy
and attack resilience.
CCS CONCEPTS
• Security and privacy → Privacy-preserving protocols; Database
and storage security;
KEYWORDS
data privacy; mobile computing; location privacy
ACM Reference Format:
Mehmet Emre Gursoy, Ling Liu, Stacey Truex, Lei Yu, and Wenqi Wei.
2018. Utility-Aware Synthesis of Differentially Private and Attack-Resilient
Location Traces. In 2018 ACM SIGSAC Conference on Computer and Commu-
nications Security (CCS ’18), October 15–19, 2018, Toronto, ON, Canada. ACM,
New York, NY, USA, 16 pages. https://doi.org/10.1145/3243734.3243741
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CCS ’18, October 15–19, 2018, Toronto, ON, Canada
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5693-0/18/10...$15.00
https://doi.org/10.1145/3243734.3243741
1 INTRODUCTION
A growing number of location-based services and applications, such
as those offered by Google, Uber, Lyft, and Waze, continue to collect
users’ location traces in order to learn about their spatio-temporal
movement patterns and offer life enriching, real-time experiences
through location-based convenience and entertainment [1, 3, 38].
On the other hand, the sensitive nature of location trace data raises
legitimate privacy concerns. Unauthorized exposure of users’ pri-
vate location traces may disclose their travel records, home and
work locations, frequent meeting points, or visits to sensitive loca-
tions such as hospitals, health clinics, and religious events.
Traditional location privacy protection techniques have mostly
focused on point-based location privacy, which is often achieved
by perturbing or obfuscating each location point in a user’s trace
using a cloaked region or a fake location, with the goal of ensuring
location k-anonymity or geo-indistinguishability [5, 8, 23, 37, 39,
46, 47, 54]. However, these point-based privacy mechanisms are
insufficient for protecting the privacy of users’ trajectories, i.e., spa-
tially correlated, temporal sequences of locations. Several studies
show that independent perturbation of each point-based location
in a trajectory suffers from fatal shortcomings, including suscepti-
bility to reverse engineering and inference attacks [7, 51]. In these
attacks, adversaries observe a sequence of perturbed locations to
infer a movement pattern and then link specific movement patterns
with specific users. Such perturbations also suffer from acute spa-
tial utility losses [10], and are vulnerable to known location trace
attacks.
The aforementioned shortcomings motivated recent interest in
synthesizing privacy-preserving, complete location traces [7, 11, 26].
These recent trace synthesis mechanisms focus on either differential
privacy [11, 26] or plausible deniability [7], and while they offer
desirable theoretical guarantees, they are limited by the extent
of their chosen privacy definitions. For example, Dwork showed
the impossibility to achieve absolute disclosure prevention [18]. A
resulting limitation is that differential privacy cannot bound all
prior and posterior knowledge distributions of an adversary, which
sparked interest in augmenting a Bayesian form of privacy with
prior-to-posterior comparison [28, 31, 32]. This Bayesian property
is clearly useful for location privacy, e.g., even if we cannot bound
all possible knowledge that is disclosed to an adversary, a Bayesian
formulation can be used to limit disclosure specifically in sensitive
zones such as hospitals or schools. Another example is regarding
outliers – it is well-known that differential privacy requires large
noise amounts to mask the existence of outliers. Location traces are
typically high-dimensional and contain diverse behavior. Hence, it
is worthy to seek alternative attack resilience notions to combat
privacy leakages concerning numerous kinds of outlier traces.
location traces should achieve three goals simultaneously: (i) ro-
bust, well-defined statistical privacy, (ii) deterministic resilience
against location privacy attacks, and (iii) strong preservation of
trajectory utility and authenticity. We present AdaTrace, a scalable
and quantitative framework that provides utility-aware synthesis of
differentially private and attack-resilient location traces. AdaTrace
builds a generative model over a dataset of real location traces by
performing feature extraction, noise injection, and feature synthesis
throughout four phases. In each phase, it first extracts utility-critical
features and then allocates an adequate differential privacy budget
to inject sufficient perturbation (noise), while maintaining attack
resilience and preserving desired statistical and spatial utility. We
guarantee that the synthetic trajectory generation process is dif-
ferentially private, i.e., the generation of synthetic traces is not
strongly dependent on any specific trace in the real trace dataset.
This ensures unlinkability between an output trace and any real
trace (input), thus thwarting identity and record linkage attacks.
As such, although the location traces crafted by AdaTrace bear a
statistical resemblance to real traces, they do not leak information
about any individual that participates in the synthesis process.
We also ensure that the traces generated by AdaTrace are ro-
bust against three attacks: Bayesian inference, partial sniffing, and
outlier leakage. While the generation process is probabilistic to
satisfy differential privacy, we enforce attack resilience by impos-
ing deterministic constraints on the generated traces. Finally, by
maintaining resemblance to real traces, the crafted traces preserve
many useful statistical features that are in common with real traces,
and therefore they can be effectively used in geo-spatial queries
and geo-spatial data mining tasks.
Contributions. The design of AdaTrace is novel in three aspects.
First, we identify three privacy threats that are relevant and com-
mon in trajectory data, and are neither addressed by differential
privacy nor by other existing trajectory generators. We formalize
these threats as Bayesian inference, partial sniffing, and outlier
leakage attacks, and propose defenses for mitigation. We show
that in many cases, our defenses can be realized with little or no
aggregate utility loss, demonstrating their effectiveness. Second,
we combine differential privacy with attack resilience to offer a
two-layer privacy approach. This allows us to integrate statistical,
algorithmic privacy with output privacy, enabling deterministic
attack resilience in a probabilistic trace synthesis setting. Third,
we design a utility-aware trace synthesizer by analyzing and cat-
egorizing various types of location trace utility and by devising
noise-resilient utility extraction and preservation techniques.
We argue that a practical solution to synthesizing privacy-preserving
We validate the effectiveness of AdaTrace by comparing it with
three representative existing approaches (ngram [11], DPT [26], and
SGLT [7]) using both real location trace datasets (Geolife [56] and
Taxi [38]), as well as a dataset simulated by Brinkhoff simulator [9]
of 50,000 vehicles in Oldenburg, Germany. We empirically measure
how well each utility type is preserved, and our results show that
AdaTrace offers up to 3-fold improvement in trajectory utility while
preserving both differential privacy and attack resilience. AdaTrace
is also computationally more efficient, as it is on average 1.5x faster
than DPT, 8x faster than ngram, and 1000x faster than SGLT.
2 RELATED WORK
Location Perturbation. Location privacy has been studied for
over a decade. Most existing solutions employ different location per-
turbation techniques to compute a k-anonymous or geo-indistinguishable
location cloaking region around a true location coordinate in order
to hide the true location in a crowd of fake locations [5, 8, 23, 37,
39, 46, 47, 54]. Location queries are then modified by adding noise
to user’s real location coordinate – they either use a low resolution
version of the real location coordinate (such as the cloaked region),
or a randomly chosen fake location within the cloaked region. Loca-
tion queries protected using such randomly selected fake locations
within a cloaked region tend to suffer from inference attacks with
high attack success rate [7] due to the strong statistical correlation
between the fake location and the user’s true location.
Synthetic Location Generation. As opposed to location pertur-
bation, some research has been performed to protect privacy by
generating synthetic locations. This line of work is inspired by the
security research on generating fake or synthetic records for privacy
protection in web search, anonymous communication, and authen-
tication systems [24, 27, 41]. In all scenarios, the main challenge
is to generate synthetic data in a manner that resembles genuine
user-produced data while offering practical privacy protection at
the same time. In the context of location-based systems, some work
has been on generating and injecting a synthetic point-based loca-
tion within a user’s trajectory [29, 46, 53]. A few recent projects
studied the problem of generating synthetic trajectories [7, 11, 26].
ngram [11] and DPT [26] proposed to generate fake trajectories
with differential privacy. Although offering strong statistical pri-
vacy guarantees, we argue that relying solely on differential privacy
has two shortcomings. First, its probabilistic nature does not allow
deterministic protection against targeted attacks such as those con-
sidered in our paper. Second, it places restrictions on the generation
algorithm but not its output, and therefore sensitive inferences re-
main possible [13, 14, 21]. In contrast to ngram and DPT, SGLT [7]
enforces plausible deniability to generate privacy-preserving fake
traces. It first introduces trace similarity and intersection functions
that map a fake trace to a real trace under similarity and intersec-
tion constraints. Then, it generates one fake trace using one real
trace as its seed. If the fake trace satisfies plausible deniability, i.e.,
there exist k other real traces that can map to the fake trace, then
it preserves privacy of the seed trace.
3 ADATRACE OVERVIEW
3.1 Problem Statement
Consider a database of real location traces (trajectories) collected
from mobile travelers on the road, denoted by Dr eal . We want
to build a generative model over Dr eal using feature extraction,
noise injection, and synopsis formation while upholding statistical
utility and attack resilience. We then employ the generative model
to output a set of synthesized traces, denoted by Dsyn. This proba-
bilistic generative model should be differentially private such that
removing any real trace from Dr eal has no significant impact on
the outcome Dsyn. In addition, the synthetic traces Dsyn should col-
lectively retain a high resemblance to the real traces Dr eal , so that
Dsyn has many useful statistical and spatial features in common
outcomes S ∈ Range(A):
Pr(A(Dr eal) = S) ≤ eε × Pr(A(D
′
r eal) = S)
r eal
1
syn, ..., Dn
A special case of this definition is when A is a synthetic trajectory
generation process and Range(A)={D
syn} is the set of
all possible synthetic databases, yielding AdaTrace’s differential
privacy requirement. This requirement guarantees that the output
of a private algorithm does not enable an adversary to distinguish,
beyond a probability controlled by parameter ε, between two in-
put databases Dr eal and D′
that differ in one user’s GPS trace.
Hence, an adversary observing Dsyn or an intermediate product of
AdaTrace, including the features of the private synopsis, will not
be able to infer the existence or content of a user’s location data in
Dr eal with strong confidence. Smaller ε yields tighter privacy [20].
We use two private building blocks in AdaTrace, namely the
Laplace and Exponential mechanisms, for numeric and categorical
queries respectively. Let f be a real-valued function f : Dr eal →
Rm that maps a database Dr eal into a fixed-size vector of m real
numbers. The sensitivity of f , denoted ∆f , is defined as the magni-
tude of the maximum L1-norm change in the result of the function
on all possible neighboring databases:
∆f :=
max
Dr eal ,D′
r eal
|| f (Dr eal) − f (D
′
r eal)||
When answering a set of numeric (i.e., real-valued) queries, the
Laplace mechanism retrieves the true answers of these queries,
and then perturbs each answer by adding random noise scaled
according to their sensitivity. That is, let Lap(α) denote a random
variable sampled from the Laplace distribution with mean 0 and
scale parameter α. The differentially private algorithm A answers
f by [19]:
A(f , Dr eal) = f (Dr eal) + (Y1, ..., Ym)
where Yi are i.i.d. random variables drawn from Lap(∆f /ε).
When the query is categorical instead of real-valued, the Ex-
ponential mechanism is used. It selects a discrete output r from a
domain of outputs R in a private manner [34]. It employs a scor-
ing function (i.e., quality criterion) q that associates each output
r ∈ R with a non-zero probability of being selected. Let q(Dr eal , r)
denote the quality of returning output r given database Dr eal , and
let ∆q be the sensitivity of q, defined similarly to ∆f above. Then,
the Exponential mechanism returns output x ∈ R with probability
equal to:
Pr(A(q, Dr eal) = x) =

εq(Dr eal ,x)
2∆q
εq(Dr eal ,r)
2∆q
e
r ∈R e
The composition properties enable us to combine the building
blocks and generate more sophisticated algorithms [35]. Because
of the composition properties, ε is also called the privacy budget.
Given a total budget ε, our goal is to create a sophisticated algo-
rithm by cleverly combining the building blocks according to the
composition properties. We employ three composition properties.
First, for n algorithms A1 . . . An, each satisfying differential pri-
vacy with parameter ε1 . . . εn, the sequential execution of these
i =1 εi from the budget. Second, if
the algorithms are executed on disjoint subsets of the database, the
algorithms on Dr eal consumesn
resulting execution consumes max(cid:0)εi
(cid:1). Third, post-processing the
output of a differentially private algorithm does not consume any
Figure 1: AdaTrace system architecture