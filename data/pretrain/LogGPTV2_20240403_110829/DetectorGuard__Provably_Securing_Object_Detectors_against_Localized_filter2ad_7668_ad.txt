17:
𝑋, 𝑌, _ ← Shape(fm)
¯om∗ ← ZeroArray[𝑋, 𝑌, 𝑁 + 1]
⊲ Generates worst-case objectness map for analysis
for each valid (𝑖, 𝑗) do
⊲ Get the shape of fm
⊲ Initialization
⊲ Every window location
u, l ← RCH-PA(fm[𝑖 : 𝑖 + 𝑤𝑥, 𝑗 : 𝑗 + 𝑤𝑦], 𝑥 − 𝑖, 𝑦 −
⊲ Add worst-case (lower-bound) logits
¯om∗[𝑖 : 𝑖+𝑤𝑥, 𝑗 : 𝑗+𝑤𝑦] ← ¯om∗[𝑖 : 𝑖+𝑤𝑥, 𝑗 : 𝑗+𝑤𝑦]+l
end for
¯om∗ ← MaxObj( ¯om∗, axis = −1) ⊲ Max objectness score
om∗ ← Binarize( ¯om∗,𝑇 · 𝑤𝑥 · 𝑤𝑦)
⊲ Binarization
𝑥min, 𝑦min, 𝑥max, 𝑦max, 𝑙 ← b
if DetCluster(om∗[𝑥min : 𝑥max, 𝑦min : 𝑦max]) is None
⊲ No high objectness left
return False
18:
19:
20:
21:
22:
23:
24:
25:
⊲ High worst-case objectness
then
else
26:
27:
28:
29:
30: end procedure
end if
return True
and defense setup. In this section, we will prove the end-to-end
robustness of DetectorGuard, solving Challenge 1.
Provable analysis of DetectorGuard. Thanks to our object-
ness explaining strategy, a patch hiding attacker has to make both
Base Detector and Objectness Predictor fail to predict a bounding
box, or high objectness, for a successful attack. Therefore, if we
can prove that Objectness Predictor can output high objectness for
an object in the worst case, we can certify its provable robustness.
We present the provable analysis of DetectorGuard in Algorithm 2.
The algorithm takes a clean image x, a ground-truth object bound-
ing box b,9 and a set of valid patch locations P as inputs, and will
9We note that the ground-truth information is essential in our provable analysis
(Algorithm 2) but is not used in our actual defense (Algorithm 1).
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3183determine whether the object in bounding box b in the image x
has provable robustness against any patch at any location in P. We
state the correctness of Algorithm 2 in Theorem 1, and will explain
the algorithm details by proving the theorem.
Theorem 1. Given an object bounding box b in a clean image x, a
set of patch locations P, window size (𝑤𝑥, 𝑤𝑦), and binarizing thresh-
old 𝑇 (used in DG(·) of Algorithm 1), if Algorithm 2 returns True,
i.e., DG-PA(x, 𝑤𝑥, 𝑤𝑦,𝑇 , b, P) = True, DetectorGuard has provable
robustness for the object b against any patch hiding attack using any
patch location in P.
Proof. DG-PA(·) first calls DG(·) of Algorithm 1 to determine
if DetectorGuard can detect the object bounding box b on the clean
image x. The algorithm will proceed only when the clean detection
is correct (Line 2-4).
Next, we extract feature map fm, iterate over each patch location
in P, and call the sub-procedure DG-PA-One(·), which analyzes
worst-case behavior over all possible adversarial strategies for each
patch location, to determine the model robustness. If any call of
DG-PA-One(·) returns False, the algorithm returns False, indi-
cating that at least one patch location can bypass our defense. On
the other hand, if the algorithm tries all valid patch locations and
does not return False, this means that DetectorGuard is provably
robust to all patch locations in P and the algorithm returns True.
In the sub-procedure DG-PA-One(·), we analyze the worst-case
output of Objectness Predictor against the given patch location. We
perform the provable analysis of the robust image classification
head (using RCH-PA(·)) to determine the lower/upper bounds of
classification logits for each window. If the aggregated worst-case
(i.e., lower bound) objectness map still has high activation for the
object of interest, we can certify the robustness of DetectorGuard.
As shown in the DG-PA-One(·) pseudocode, we first initialize a
zero array ¯om∗ to hold the worst-case objectness scores. We then
iterate over each sliding window and call RCH-PA(·), which takes
the feature map window fm[𝑖 : 𝑖 + 𝑤𝑥, 𝑗 : 𝑗 + 𝑤𝑦], relative patch co-
ordinates (𝑥 −𝑖, 𝑦 − 𝑗), patch size (𝑝𝑥, 𝑝𝑦) as inputs and outputs the
upper bound u and lower bound l of the classification logits. Since
the goal of the hiding attack is to minimize the objectness scores,
we only need to reason about the lower bound of classification log-
its. Recall that in RCH(·), we clip all local logits values into [0,∞];
therefore, the best an adversary can do is to push all corrupted
logits values down to zeros. We then compute the lower bound l by
zeroing out all corrupted logits values and aggregating the remain-
ing ones. We note that the sub-procedure DG-PA-One(·) aims to
check defense robustness for a particular patch location; therefore,
the patch location and corrupted features/logits are known in this
provable analysis (we discuss how to map pixel-space coordinates
to feature-space coordinates in Appendix G).
Given the lower bound l of every window classification logits,
we will add it to the corresponding region of ¯om∗. After we ana-
lyze all valid windows, we call MaxObj(·) and Binarize(·) for the
worst-case objectness map om∗. We then get the cropped object-
ness map that corresponds to the object of interest (i.e., om∗[𝑥min :
𝑥max, 𝑦min : 𝑦max]) and feed it to the cluster detection algorithm
DetClutser(·). If None is returned, a hiding attack using this patch
location might succeed, and the sub-procedure returns False. Oth-
erwise, Objectness Predictor has a high worst-case objectness and
is thus robust to any attack using this patch location. This implies
the provable robustness, and the sub-procedure returns True. □
Theorem 1 shows that if our provable analysis (Algorithm 2)
returns True for certain objects, DetectorGuard (Algorithm 1) will
always detect the certified object or issue an attack alert. This
robustness property is agnostic to attack strategies and holds for
any adaptive attacker at any location within our threat model. In our
evaluation (next section), we will use Algorithm 2 and Theorem 1
to certify the provable robustness of every object in every image
and report the percentage of certified objects.
5 EVALUATION
In this section, we provide a comprehensive evaluation of Detec-
torGuard on PASCAL VOC [13], MS COCO [26], and KITTI [15]
datasets. We will first introduce the datasets and models used in our
evaluation, followed by our evaluation metrics. We then report our
main evaluation results on different models and datasets, and finally
provide a detailed analysis of DetectorGuard performance. Our code
is available at https://github.com/inspire-group/DetectorGuard.
5.1 Dataset and Model
Dataset:
PASCAL VOC [13]. The detection challenge of PASCAL Visual
Object Classes (VOC) project is a popular object detection dataset
with annotations for 20 different classes. We take trainval2007
(5k images) and trainval2012 (11k images) as our training set
and evaluate our defense on test2007 (5k images), which is a
conventional usage of the PASCAL VOC dataset [29, 64].
MS COCO [26]. The Microsoft Common Objects in COntext
(COCO) dataset is an extremely challenging object detection dataset
with 80 annotated common object categories. We use the training
and validation set of COCO2017 for our experiments. The training
set has 117k images, and the validation set has 5k images. We ignore
bounding boxes with the flag iscrowd=1 for simplicity.
KITTI [15]. KITTI is an autonomous vehicle dataset that contains
both 2D camera images and 3D point clouds. We take its 7481 2D
images and use 80% of randomly splited images for training and
the remaining 20% for validation. We merge all classes into three
classes: car (all different classes of vehicles), pedestrian, cyclist.
Base Detector:
YOLOv4 [2, 53] is the state-of-the-art one-stage object detector.
We choose Scaled-YOLOv4-P5 [53] in our evaluation. For MS COCO,
we use the pre-trained model. For PASCAL VOC and KITTI, we
fine-tune the model previously trained on MS COCO.
Faster R-CNN [45] is a representative two-stage object detec-
tor. We use ResNet101-FPN as its backbone network. Image pre-
processing and model architecture follows the original paper. We
use pre-trained models for MS COCO and fine-tune pre-trained
models for PASCAL VOC and KITTI detectors.
Perfect Clean Detector (PCD) is a hypothetical object detector
simulated with ground-truth annotations. PCD can always make
correct detection in the clean setting but is assumed vulnerable to
patch hiding attacks. This hypothetical object detector ablates the
errors of Base Detector and helps us better understand the behavior
of Objectness Predictor and Objectness Explainer.
Objectness Predictor:
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3184Table 2: Clean performance of DetectorGuard
vanilla AP
PASCAL VOC
defense AP
100%
92.9%
90.0%
99.3%
92.4%
89.6%
FAR@0.8
vanilla AP
0.9%
4.0%
2.9%
100%
73.6%
66.7%
MS COCO
defense AP
99.0%
73.4%
66.5%
FAR@0.6
vanilla AP
1.2%
1.6%
0.9%
100%
93.1%
89.9%
KITTI
defense AP
FAR@0.8
99.0%
92.4%
89.1%
1.5%
1.7%
1.4%
Perfect clean detector
YOLOv4
Faster R-CNN
BagNet-33 [3], which has a 33×33 small receptive field, is the
backbone network of Objectness Predictor. For PASCAL VOC and
MS COCO, we zero-pad each image to a square and resize it to
416×416 before feeding it to BagNet; for KITTI, we resize each image
to 224×740. We fine-tune a BagNet model that is pre-trained on
ImageNet [11]. To train an image classifier given a list of bounding
boxes in the object detection dataset, we first map pixel-space
bounding boxes to the feature space (details of box mapping are in
Appendix G). We then teach BagNet to make correct predictions on
cropped feature maps by minimizing the cross-entropy loss between
aggregated feature predictions and one-hot encoded label vectors.
In addition, we aggregate all features outside any feature boxes as
the “negative" feature vector for the “background" classification.
Default Hyper-parameter:
We will analyze the effect of different hyper-parameters in Sec-
tion 5.5. In our default setting, we use a square feature-space window
of size 8 and the DBSCAN clustering [12] with eps = 3, min_points =
24 for different datasets.10 We set the default binarizing threshold
to 32 for PASCAL VOC, 36 for MS COCO, and 11 for KITTI based
on different model properties with different datasets.
5.2 Metric
Clean Performance Metric:
Precision and Recall. We calculate the precision as TP/(TP+FP)
and the recall as TP/(TP+FN). For the clean images without a false
alert, we follow previous works [8, 64] setting the IoU threshold
𝜏 = 0.5 and count TPs, FPs, FNs in the conventional manner. For
images that have false alerts, we set TP and FP to zeros, and FN to
the number of ground-truth objects since no bounding box is pre-
dicted. We note that conventional object detectors use a confidence
threshold to filter out bounding boxes with low confidence values.
As a result, different confidence thresholds will give different preci-
sion and recall values; we will plot the entire precision-recall curve
to analyze the model performance.
Average Precision (AP). To remove the dependence on the con-
fidence threshold and to have a global view of model performance,
we also report Average Prevision (AP) as one of evaluation metrics.
We vary the confidence threshold from 0 to 1, record the precision
and recall at different thresholds, and calculate AP as the averaged
precision at different recall levels. This calculated AP can be consid-
ered as an approximation of the AUC (area under the curve) for the
precision-recall curve. We note that AP is one of the most widely
used performance metrics in object detection benchmark competi-
tions [13, 15, 26] and research papers [2, 19, 25, 29, 43–45, 49].
10A 416×416 (or 224×740) pixel image results in a 48×48 (or 24×89) feature map using