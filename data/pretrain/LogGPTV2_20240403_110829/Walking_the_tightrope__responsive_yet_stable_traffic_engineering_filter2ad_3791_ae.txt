 0.2
Within 10% of OPT
Within 5% of OPT
Within 2% of OPT
 0
 0
 20
 60
 40
 80
Number of Iterations
 100
 120
Figure 11: The CDF of TeXCP convergence time for Sprint. TeXCP
quickly converges to within 5% to 10% of the optimal.
venience. We run TeXCP in the simulation environment reported
in [12]. In particular, we use the same topology, simulate traf(cid:2)c de-
mands similarly using Poisson sources, use the same cross-traf(cid:2)c,
and split traf(cid:2)c using the hashing method described in [12]. The
topology, shown in Fig. 7, consists of 3 ingress-egress pairs shar-
ing 3 bottleneck links. Each bottleneck carries cross traf(cid:2)c uncon-
trolled by TeXCP. Fig. 8 shows changes in cross traf(cid:2)c during the
simulation. All simulation parameters, such as capacities, delays,
queue size, number of bins, etc. have been con(cid:2)rmed by one of the
authors of MATE. Our results, in Fig. 9, show that TeXCP is more
effective at balancing the load than MATE and converges faster.
5.6 TeXCP Convergence Time
As with any online protocol, an interesting question is how long
it takes TeXCP to rebalance the load after a failure or a traf(cid:2)c dis-
turbance. The convergence time depends on many factors such as
the topology, the maximum number of paths per IE pair, how far the
initial traf(cid:2)c splits are from the balanced ones, etc. Fig. 11 shows
the CDF of the convergence time for the Sprint topology, where
each IE pair is allowed a maximum of K = 10 paths. Time is
measured as the number of iterations(cid:150)i.e., number of Td intervals.
Various samples are generated using 200 different TMs, starting at
20 different random initial traf(cid:2)c split ratios. The (cid:2)gure shows that
TeXCP takes only 10-15 iterations to converge to within 10% of the
optimal max-utilization, and a few dozens of iterations to converge
to 5% of the optimal. Other topologies show similar trends.
Furthermore, TeXCP converges smoothly without oscillations as
predicted by the analysis in x4. Fig. 10 shows the link utilizations
for a representative subset of the links in the Sprint topology when
an unforeseen event (link failure) happens. It shows that both the
maximum utilization in the network and individual link utilizations
steadily converge without oscillations. TeXCP experiments in x5.4
for other topologies and traf(cid:2)c demands show similar trends.
Finally, unlike OSPF convergence which might cause transient
IE Pair I1-E1
IE Pair I2-E2
n
o
i
t
c
a
r
F
h
t
a
P
 1
 0.8
 0.6
 0.4
 0.2
 0
I1 -> A -> B -> E1
I1 -> A -> C -> D -> B -> E1
n
o
i
t
c
a
r
F
h
t
a
P
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
 10
 20
I2 -> C -> D -> E2
I2 -> C -> A -> B -> D -> E2
 30
Time (s)
 40
 50
 60
 0
 10
 20
 30
Time (s)
 40
 50
 60
Eq. 9 triggers TeXCP to stop using this path
Figure 12: As long as optimality is preserved, TeXCP stops using the longer paths and shifts all traf(cid:2)c to the shorter paths.
loops, the convergence time of TeXCP only affects how far we are
from optimal utilization; no loops occur during that interval.
5.7 Number of Active Paths
Although we con(cid:2)gure each TeXCP agent with the 10 short-
est paths, TeXCP does not use all of these paths. It automatically
prunes paths that do not help in reducing max-utilization. Table 4
shows the average number of paths used by each IE pair for the
simulations in x5.4. In our simulations, on average, TeXCP uses
only 4 of the 10 per-IE paths it is con(cid:2)gured with.
5.8 Automatic Selection of Shorter Paths
We show an example simulation of how TeXCP automatically
prunes longer paths, if they do not reduce the max-utilization. We
simulate TeXCP on the simple network in Fig. 3. We have two IE
pairs (I1, E1) and (I2, E2), each with unit traf(cid:2)c demands. There are
two TeXCP agents, one at each ingress. Each TeXCP has one short
path and one long path. Clearly, the max-utilization is the same
whether the agents send all of their traf(cid:2)c on shorter paths or split
it evenly between the two paths. Fig. 12 shows the fraction of traf(cid:2)c
each agent sends on the two paths. Despite being initialized to send
equal traf(cid:2)c on the long and short paths, both TeXCP agents quickly
prune the longer paths, routing all traf(cid:2)c on to shorter paths.
6.
IMPLEMENTATION & DEPLOYMENT
This section examines practical considerations for an ISP to de-
ploy TeXCP using current routers. For this section, we assume IE
paths are pinned using MPLS Label Switched Paths (LSPs) [32].
(a) Number of LSPs: TeXCP creates K LSPs for each IE pair
(K = 10 in our experiments). It is common for large ISP back-
bones to have 200-300 egress points, resulting in 2K-3K LSP heads
at an ingress router. This is somewhat higher than typically sup-
ported in today’s backbone routers.9 We observe, however, that
ISPs have many fewer PoPs, 20-100 (Table 3). Since traf(cid:2)c on the
ISP backbone essentially moves from one PoP to another, TeXCP
tunnels need only be created between PoPs rather than between
ingress-egress routers. This commonly used technique in MPLS
networks [20] brings the number of LSP heads into line with cur-
rent technology. We also note that a router can typically support
an order of magnitude more transit MPLS LSPs than LSP heads
because transit LSPs need smaller state and processing overhead.
(b) Traf(cid:2)c splitting: TeXCP requires edge routers to split traf(cid:2)c
among the LSPs connecting an IE pair. Current ISP-class routers
can split traf(cid:2)c to a destination between as many as 16 LSPs, at as
(cid:2)ne a resolution as desired [28].
(c) Utilization estimates: TeXCP needs routers to estimate link
utilization every Tp seconds. Current edge and core routers main-
tain counters of the amount of traf(cid:2)c (bytes and packets) sent on
9Of(cid:2)cial numbers of supported tunnels are hard to (cid:2)nd in vendors’ public documenta-
tion and tend to increase over time. One Cisco router used in ISP networks supports at
least 600 tunnel heads at the time of writing.
each interface. These counters can be read every Tp to get a utiliza-
tion estimate. Tp=100ms allows counters enough time to stabilize.
(d) Estimating feedback: Estimating the feedback is as easy as
estimating the utilization. Eq. 11 shows that all IE (cid:3)ows have
the same feedback, be it positive or negative. A router needs to
compute this one value for each outgoing interface, once every Tp.
Also, the core router can of(cid:3)oad the computation entirely, by push-
ing the variables (S; Q; N; (cid:30)l) to the edges.
(e) Communication from core to edge routers: On the (cid:2)rst cut,
each TeXCP agent sends one probe/Tp down all its LSPs. For small
networks n 2 [30; 100] and low probe rates Tp 2 [:5; 1]s, this
results in a core router processing 1:2 (cid:0) 8K probes/sec/interface,
and may be feasible. For larger networks, this is just too many.
Instead, for large networks we suggest a cleaner approach. Each
core router generates one report packet/Tp, for each outgoing inter-
face, which contains the link utilization and the feedback. Recall
from Eq. 11, that all IE (cid:3)ows traversing the link receive the same
feedback. Thus, unicasting the report packet to every edge router
is equivalent to updating probes of every LSP. Note, a core router
only generates n reports/Tp, where n is the number of edge routers,
for each outgoing interface. As in x6(a), we note that using PoP to
PoP LSPs substantially reduces the amount of traf(cid:2)c. Also note
that the edge routers need to do a little more work to consolidate
per-link reports into per-LSP state, but that seems a good tradeoff.
(f) Router Modi(cid:2)cation: TeXCP needs one new functionality; the
load balancer (Eq. 5), which is easily built in software in existing
edge routers. All other functionality either already exists in current
routers or is very similar to existing functionality.
7. RELATED WORK
Several of(cid:3)ine TE methods, like the OSPF weight optimizer [15,
16] and the multi-commodity (cid:3)ow formulation [26] were described
in x5. Here we mention other relevant efforts.
Optimal Routing: Gallager’s seminal work on minimum delay
routing [17] began the (cid:2)eld of optimal and constraint-based rout-
ing [4, 9, 10, 12, 26, 38, 41]. This work studies routing as an opti-
mization problem and usually relies on centralized solutions, global
knowledge of the network state, or synchronized nodes.
Of(cid:3)ine Oblivious Optimizers: Instead of estimating traf(cid:2)c ma-
trices accurately [13], the MPLS Oblivious optimizer [6] (cid:2)nds a
routing that balances load independent of the traf(cid:2)c matrix, i.e.
minimizes the worst case across all possible traf(cid:2)c matrices. Simi-
larly, two new schemes [24,42] balance load in a traf(cid:2)c-insensitive
manner by re-routing traf(cid:2)c through pre-determined intermediate
nodes; an idea used in processor interconnection networks.
Online Load Balancers: There are relatively few proposals for
realtime adaptive multipath routing. We describe MATE [12] and
compare it with TeXCP in x5.5. OSPF-OMP [40], an Internet draft,
describes another technique that (cid:3)oods the realtime link load infor-
mation and adaptively routes traf(cid:2)c across multiple paths.
8. FURTHER DISCUSSION
Q: Online multipath routing may reorder TCP packets hurting TCP
congestion control. How does TeXCP deal with that?
A: Yes and the easy answer is either keep per-(cid:3)ow state at the edge
routers or hash TCP (cid:3)ows into many buckets and assign buckets to
the various paths. Neither of these methods is very satisfying. A
clean and complete online TE solution has to solve two problems.
The (cid:2)rst is the intrinsic problem of being responsive yet stable,
which we address in this paper. The second is to dynamically split
(cid:3)ows without reordering TCP packets, which we address in our
Flare paper [35]. We show that splitting TCP (cid:3)ows at the granular-
ity of (cid:3)owlets(cid:150)bursts of packets within a TCP (cid:3)ow(cid:150)balances load
accurately without reordering packets and without per-(cid:3)ow state.
Q: Your comparison with OSPF-TE does not agree with previ-
ous studies. Can you explain, please? A: Actually it does. Pa-
pers [14(cid:150)16] report max-utilization under OSPF-TE and the opti-
mal (e.g., uOSP F (cid:0)T E = 0:59 and uOP T = 0:48), whereas this
paper reports the max-utilization under the studied scheme relative
to the optimal. When expressed using the same metric, the results
in [14(cid:150)16] are consistent with ours. The metric we picked, i.e.,
comparing schemes in terms of their performance relative to the
best solution, is a standard metric for comparative analysis widely-
used both in the (cid:2)eld of traf(cid:2)c engineering [5, 6] and elsewhere.
Q: Your scheme minimizes the max-utilization, but I am worried
that TeXCP will end up sending traf(cid:2)c on very long paths. Don’t
you think this is an issue?
A: Minimizing the maximum utilization in the network is a widely-
used TE metric in research and practice [5, 6, 26]. It removes hot-
spots and reduces congestion risks by spreading the load over avail-
able paths. Also, it allows a network to support greater demands.
Of course, minimizing max-utilization may sometimes increase the
delay. Unlike contemporary TE [6, 15], TeXCP allows an ISP to
bound this increase in delay. When the ISP con(cid:2)gures the Boston-
LA TeXCP agent with some paths, it is declaring that all of these
paths have acceptable delays. Clearly the ISP should not pick a
path that goes to LA via Europe; and it should not need to, given
the diversity in ISP topologies [37]. Using the Rocketfuel delay
estimates, we have computed the average delay difference between
the TeXCP paths and the shortest path weighted by the fraction of
traf(cid:2)c on each path. For the experiments in x5.4, this number is
[3.2, 10.6] ms. Also, we re-ran the experiments in Fig. 4 by re-
stricting TeXCP to paths that are less than 20ms longer than the
shortest. In these runs, the weighted average delay difference goes
down to [1.2, 2.7] ms. The max-utilization stays similar to Fig. 4,
except for the AT&T topology where it increases by a few percent.
Q: What happens if every ISP uses TeXCP?
A: Nothing bad. The same traf(cid:2)c volume still enters and exits an
ISP at the same points. TeXCP only balances traf(cid:2)c within an ISP
and has no globally-visible changes.
Q: Links within an AS go down all the time. Are 10 paths per IE
enough for failure recovery?
A: TeXCP does not replace failure discovery and link restora-
tion techniques, such as SONET rings, DWDM optical protection
and MPLS fast-reroute [20]. Actually, it complements these tech-
niques:
it rebalances the load after link restoration and helps in
recovering from unanticipated or combination failures for which
the domain doesn’t have a pre-computed backup path. For exam-
ple, about half the ISPs run MPLS in their core. When a link fails,
MPLS fast-reroute quickly patches the failed MPLS tunnel with an
alternative segment, shifting the traf(cid:2)c to different physical links.
This may unbalance the traf(cid:2)c and create hot spots. TeXCP rebal-
ances the traf(cid:2)c allowing the recovery process to go smoothly.
9. CONCLUDING REMARKS
This paper advocates online traf(cid:2)c engineering, which dynam-
ically adapts the routing when traf(cid:2)c changes or links fail, rather
than optimizing the routing for long term average demands and
a pre-selected set of failures. We present TeXCP, an online dis-
tributed TE protocol, show it is stable and that it balances the load
and keeps network utilization within a few percent of the optimal
value. We also show that TeXCP outperforms traditional of(cid:3)ine
TE, particularly when the realtime demands deviate from the traf-
(cid:2)c matrix or unanticipated failures happen.
Although we focus on traf(cid:2)c engineering, our work is part
of a general trend away from traditional single-path congestion-
insensitive Internet routing towards adaptive routing. This includes
overlays [31], adaptive multi-homing [2], and traf(cid:2)c engineering.
Recent studies show that overlay routing increases traf(cid:2)c variabil-
ity making it much harder to estimate the TM needed for of(cid:3)ine
TE optimization [22, 29]. In that sense, our work complements re-
cent work on overlay routing because online TE does not need to
estimate TMs.
Ideally, one would like online TE to balance the
load within ISP networks, exploiting intra-AS path diversity [37]
to avoid intra-AS bottlenecks (which, according to [3], account for
40% of the bottlenecks). Overlays, on the other hand, can adapt
end-to-end routes to avoid congested peering links. Careful design,
however, is needed to achieve this ideal situation, because overlay
adaptations and online TE can easily destabilize each other. Our
future work focuses on how to make these two adaptive systems
work harmoniously to deliver the best routing to the end user.
10. ACKNOWLEDGMENTS
We thank Asfandyar Qureshi and Shan Sinha for contributing
to an early version of TeXCP. We also acknowledge the support
of Cisco Systems and the National Science Foundation under NSF
Career Award CNS-0448287. The opinions and (cid:2)ndings in this
paper are those of the authors and do not necessarily re(cid:3)ect the
views of the National Science Foundation or Cisco Systems.
11. REFERENCES
[1] Abilene. http://abilene.internet2.edu.
[2] A. Akella, B. Maggs, S. Seshan, A. Shaikh, , and R. Sitaraman. A
Measurement-Based Analysis of Multihoming. In SIGCOMM, 2003.
[3] A. Akella, S. Seshan, , and A. Shaikh. An Empirical Evaluation of
Wide-Area Internet Bottlenecks. In IMC, 2003.
[4] E. J. Anderson and T. E. Anderson. On the Stability of Adaptive
Routing in the Presence of Congestion Control. In INFOCOM, 2003.
[5] D. Applegate, L. Breslau, and E. Cohen. Coping with Network
Failures: Routing Strategies for Optimal Demand Oblivious
Restoration. In SIGMETRICS, 2004.
[6] D. Applegate and E. Cohen. Making Intra-Domain Routing Robust to
Changing and Uncertain Traf(cid:2)c Demands. In SIGCOMM, 2003.
[7] D. Awduche et al. RSVP-TE: Extensions to RSVP for LSP Tunnels,
[10] J. E. Burns, T. J. Ott, A. E. Krzesinski, and K. E. Muller. Path
Selection and Bandwidth Allocation in MPLS Networks. Perform.
Eval, 2003.
[11] Cisco. Con(cid:2)guring OSPF.
http://www.cisco.com/univercd/cc/td/doc/product/software/ios122/.
[12] A. Elwalid, C. Jin, S. H. Low, and I. Widjaja. MATE: MPLS