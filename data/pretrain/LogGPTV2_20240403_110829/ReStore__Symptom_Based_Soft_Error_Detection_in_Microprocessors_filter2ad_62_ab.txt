mem-data
mem-addr
cfv
exception
masked
100k
inf
1k
500
10k
200
latency (insns)
100%
90%
80%
70%
60%
50%
25
50
100
Figure 2. Virtual machine fault injection.
hardware implementation speciﬁc effects. In other words, we
abstract away the processor implementation by assuming that
a soft error has already corrupted architectural state, focus-
ing instead on the propagation of the incorrect architectural
state into a soft error symptom. For the fault model in this
experiment, we chose a single bit ﬂip in the result of a ran-
domly chosen instruction to emulate the effects of a failure-
inducing transient fault. From our previous experience, this
fault model is a crude but reasonable approximation of faults
injected into a lower-level processor model. The SPEC2000
integer benchmark suite was used as the workload, and each
benchmark was subjected to approximately 1000 trials, yield-
ing an overall error margin of less than 0.9% at a 95% conﬁ-
dence level. A summary of the results is provided in Figure 2.
In the ﬁgure, each bar represents the number of in-
structions elapsed between the fault injection to ﬁrst error
symptom (if any). The vertical axis represents all trials, di-
vided into various categories, with the lower categories taking
precedence (for example, a trial that ﬁts in both the exception
and cfv categories is placed in the exception category). The
different categories are described in Table 1. For example,
the masked category contains data corruptions that did not ul-
timately affect the executing application.
Category
masked
exception
cfv
mem-addr
mem-data
register
Observed Error Symptom
The injected fault was masked (did not cause failure)
Instruction Set Architecture deﬁned exception
Control ﬂow violation - incorrect instruction executed
Address of a memory operation was affected
A store instruction wrote incorrect data to memory
Only registers were corrupted
Table 1. Figure 2 category descriptions
Across the benchmarks, the average injected fault is
masked approximately 59% of the time. This high rate of
software level logical masking is due to corruption of results
that are dead, transitively dead (data that is only consumed
by dead or other transitively dead instructions), or a part of
prefetch instructions [21]. We are primarily concerned with
the remainder of the trials — the 41% of the injected faults
that do cause persistent data corruption.
About 24% of all fault injections (more than half of all
failing trials) result in an ISA deﬁned exception within 100
instructions. Most of these are memory access faults (such as
attempting to access a virtual page for which the application
does not have appropriate permissions), while a small portion
consist of arithmetic overﬂow or memory alignment excep-
tions. Another 8% of all fault injections result in a incorrect
control ﬂow within the same 100 instruction latency. The vast
majority of these stem from selecting the incorrect target in a
conditional branch instruction (i.e. choosing the taken target
instead of falling through or vice versa).
The remaining trials either propagate corrupt values into
memory loads and stores or only affect register ﬁle values.
As the latency allowed for symptom detection is stretched to
the entire program length, the coverage provided by the ex-
ception and cfv categories increases steadily — although the
majority of the coverage is still obtained with relatively short
latency. The reg category all but disappears as the corruptions
propagate into the other categories.
Thus, nearly 80% of the failure inducing faults injected
in this experiment result in an exception or control ﬂow vi-
olation within 100 instructions of the fault injection. If one
were able to detect these error symptoms and roll back to a
checkpoint at least 100 instructions in the past, then 80% of
the failure inducing faults would be covered. Coupled with
the intrinsic 59% masking level provided by the application,
only 9% of injected faults would propagate to a failure.
The level of coverage provided by detecting exceptions
and control ﬂow violations might at ﬁrst seem rather high.
We reason that many of the instructions in a typical program
are devoted to calculating memory address values and deter-
mining control ﬂow [16], so corrupting the result of a random
instruction can often result in memory access exceptions or
incorrect control ﬂow.
Furthermore, another contributing factor to the frequent
occurrence of memory exceptions is that for many workloads,
the virtual address space is signiﬁcantly larger than the mem-
ory footprint of the application. This increases the likelihood
that a random corruption in a pointer value will result in a
pointer to an invalid or unmapped virtual page.
Thus, an architecture with a smaller virtual address space
(or correspondingly, a workload with a larger memory foot-
print) may not exhibit such a large occurrence of exceptions
from soft errors. To investigate this effect, we conducted an-
other fault injection campaign where only the bottom 32-bits
of each 64-bit result were targeted. The data showed that the
exception category did indeed become smaller, losing about
25% of its size. The slack was consumed by the cfv and mem-
addr categories, with the cfv category picking up the majority.
This indicates that control ﬂow based symptoms would play a
larger role on a machine with a smaller virtual address space
or on programs with larger memory footprints.
3.2. Implementation
3.2.1. Detecting exceptions. Detecting memory access and
alignment exceptions as soft error symptoms is straightfor-
ward, since all processors must already detect and report any
ISA-deﬁned exceptions. In the ReStore architecture, instead
of stopping execution and reporting an exception immedi-
ately upon its discovery, the processor ﬁrst rolls back to a
previous checkpoint and re-executes the instructions leading
up to the discovered exception. If the exception fails to ap-
pear again, then a soft error has been detected and possibly
recovered from. Otherwise, either the exception is genuine or
a data corruption occurred prior to the checkpoint.
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:08:52 UTC from IEEE Xplore.  Restrictions apply. 
Since there is a performance penalty associated with
rolling back to a checkpoint and re-executing a set of instruc-
tions, it is important to minimize the number of rollbacks that
occur in the absence of a real error. We call such checkpoint
restoration causing symptoms false positive symptoms, since
they falsely identify the presence of a soft error. However,
because exceptions are fairly rare during error-free opera-
tion, and, perhaps more importantly, program execution can-
not continue without ﬁrst handling any exceptions that arise,
there is little reason to not initiate a checkpoint recovery on
memory access, alignment or any other exceptions.
3.2.2. Detecting incorrect control ﬂow. Designing a low
cost mechanism for detecting invalid control ﬂow is more dif-
ﬁcult. Many techniques to monitor control ﬂow have been
proposed in the past [17], but they all (to the best of our
knowledge) focus on detecting illegal control ﬂow.
Illegal
control ﬂow occurs when the sequence of executed instruc-
tions is impossible, owing to a control ﬂow transition either
in the absence of a branch instruction or in the presence of a
branch instruction but to an illegal target.
Here, not only do we wish to detect illegal control ﬂow,
but we also wish to detect legal, but incorrect, control ﬂow.
An example of legal but incorrect control ﬂow occurs when a
conditional branch chooses the alternate target (taken instead
of not taken or vice versa). While such a control ﬂow tran-
sition is legal (it is one of the two acceptable choices speci-
ﬁed by the compiler), the branch condition is evaluated incor-
rectly. Control ﬂow watchdogs proposed previously do not
detect such invalid control ﬂow.
Instead of designing another hardware or software mech-
anism to identify the entire set of control ﬂow violations, we
chose to leverage already existing hardware in high perfor-
mance microprocessors. The pipeline ﬂushes described in
Section 2 have a dramatic impact on performance, since the
latency to reﬁll the pipeline after a ﬂush is proportional to the
pipeline’s length. Thus, computer architects have designed
highly accurate branch predictors to direct the front of the
pipeline, in order to minimize the number of pipeline ﬂushes
required. These predictors accurately predict the outcome of
conditional branches [13, 18], function returns, and other in-
direct and direct branches. They are typically correct for well
over 95% of branch instances.
We propose to use these highly accurate control ﬂow
predictors to aid in the detection of soft errors — if a con-
trol mis-speculation is discovered deep in the pipeline, per-
haps the branch predictor was indeed correct and the “mis-
speculation” was instead the result of a soft error. Control
mis-speculations could be used as a symptom of soft errors in
this manner. Assuming that the processor implements some
form of checkpointing, restoration of a prior checkpoint and
re-execution of the intermediate instructions would result in
recovery from any soft errors that had occurred since the
checkpoint was taken.
However, to minimize the performance overhead, we
wish to minimize the number of checkpoint recoveries that
occur. Despite a greater than 95% branch predictor accuracy,
the large frequency of branch instructions in typical work-
loads implies that pipeline ﬂushes are still fairly common.
Adding a checkpoint recovery on top of each pipeline ﬂush
would be unacceptably costly in terms of performance. To
this end, we observe that architects have also devised conﬁ-
dence predictors for conditional branch predictions [2, 7, 12],
which assign a level of conﬁdence to each conditional branch
prediction. A conﬁdence predictor is similar to a branch pre-
dictor; it monitors the branch predictor’s past accuracy in pre-
dicting a particular branch and determines a “conﬁdence” for
that branch. Instead of performing a checkpoint recovery on
each control mis-speculation, we only use the control mis-
speculation as a soft error symptom if the mis-speculation
was labeled as high conﬁdence by the conﬁdence predictor.
Used within the ReStore framework, the different conﬁ-
dence predictors trade off performance (frequency of check-
point rollbacks) for soft error coverage (percentage of soft
errors detected) or vice versa. In this work, we selected the
JRS conﬁdence predictor [12], prioritizing performance over
coverage. We note that in ReStore, a control ﬂow violation
need not be identiﬁed immediately.
If a violation initially
slips detection, it could still induce soft error symptoms (high
conﬁdence branch mispredictions or otherwise) in its wake.
If this is the case, the different conﬁdence prediction imple-
mentations trade off performance for error detection latency.
3.2.3. Event logs. To support the implementation of ReStore,
we propose event logs that track and record the events leading
up to a symptom. These event logs enable detection of soft
errors during re-execution, which in turn allows for dynamic
ﬁne tuning of the ReStore framework. Event logs can also
provide strong speculation hints (e.g. branch predictions) and
can provide input replication to ensure correctness during re-
execution (e.g. Load Value Queues [23]).
By tracking and recording events during both the origi-
nal and redundant executions, soft errors can be detected and
logged. As an example, with control ﬂow based symptoms,
the event log might store control instruction outcomes. A soft
error is detected if any control instructions produce differing
results between the original and redundant executions. In the
presence of differing results, an implementation of ReStore
may elect to re-execute a third time to verify that the soft er-
ror occurred during the original execution.
Being able to detect the presence of soft errors enables
dynamic ﬁne tuning of the symptom based mechanism. For
example, if a processor encounters a high concentration of
false positive control ﬂow symptoms, it may elect to tem-
porarily ignore all symptoms in the interest of minimizing the
performance robbing impact of checkpoint recovery. Gener-
ally speaking, rollback “thresholds” can be adjusted dynami-
cally to trade off error coverage for performance.
3.3. Generalizing symptom-based error detection
In summary, our symptom based detectors trigger on the
following two events: “Did an exception occur?” or “Did we
mis-speculate a high conﬁdence branch?”. In either case, a
prior checkpoint is restored, which enables soft error detec-
tion and recovery.
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:08:52 UTC from IEEE Xplore.  Restrictions apply. 
However, the ReStore architecture is a framework into
which other symptom-based detection can be easily inte-
grated. Generally speaking, candidate symptoms can be eval-
uated on the following metrics: (1) the frequency that failure-
causing errors generate the symptom, (2) the typical propaga-
tion latency from point of error to the symptom, and (3) the
frequency of the symptom in the absence of an error. The ﬁrst
and second metrics evaluate the error coverage provided by
the candidate symptom. The third metric provides a measure
of the performance impact of incorporating the metric into
the ReStore framework, i.e., the incidence of false positives.
For example, triggering checkpoint restoration on data cache
misses might not be a good detection strategy as while data
cache misses are favorable on points 1 and 2, they may not
be sufﬁciently rare enough in the absence of transient faults
and may cause undue false positives (Point 3). Investigation
of other useful symptoms is a topic for later research.
4. Experimental Methodology
In this section, we describe our experimental methodol-
ogy. First, we introduce the processor microarchitecture and
Verilog model used in our experimentation. Next, we de-
scribe our fault model and fault injection framework and then
delve into how we used the processor model to evaluate the
ReStore architecture. Finally, we discuss the statistical sig-
niﬁcance of the data presented.
4.1. Processor model
Given that our objective is to examine the effects of
transient faults on a modern high-performance processor
pipeline, we needed to develop a sufﬁciently detailed model
of a representative microprocessor architecture. In this sub-
section, we describe the microarchitecture and the Verilog
model used in our experimentation. The model we use here
is a revised version of the model we used in our previous
work [28].
Our microarchitecture is a superscalar, dynamically-
scheduled pipeline similar
in complexity to the Alpha
21264 [3] and the AMD Athlon [19]. The processor executes
a subset of the Alpha instruction set—due to time considera-
tions, ﬂoating point instructions, synchronizing memory op-
erations, and some obscure miscellaneous instructions were
not implemented. The processor includes such features as
speculative instruction scheduling, memory dependence pre-
diction, and sophisticated branch prediction, which are nec-
essary ingredients for high-performance processing. The pro-
cessor can have up to 132 instructions in-ﬂight in the 12-stage
pipeline. Every cycle, up to 6 instructions are selected for ex-
ecution using a dynamic scheduler of 32 entries. A diagram
of the processor is shown in Figure 3. The important point to
note is that our microarchitecture is representative of current-
generation high-performance microprocessors; it contains a
similar rich set of performance enhancing features (e.g., spec-
ulation) that can affect the ways in which the processor reacts
to transient faults.
RAS BTB
BOB
L1 Insn
Cache
TLB
BrPred0
Align + Rotate
BrPred1
32 Entry Fetch Queue
4x Decoder
Spec RAT Spec Free List Mem Dep Pred 0
Intra Bundle Rename
Mem Dep Pred 1
32−Entry Scheduler
Register File
ALU
ALU ALU
Br AGEN AGEN
Fetch
Decode
Rename
Schedule
RegRead
Execute
LDQ
STQ
TLB