ingly, the vast majority of the papers (104/115) only take
one single snapshot of the scanning results without consid-
ering the dynamic changes of labels. A small number of
papers consider the potential label changes, and decide to
wait for some time before using the labels [18, 58, 73]. The
waiting time varies from ten days [58] to more than two
years [18]. Others submit the ﬁles multiple times to see the
differences [25, 41, 67, 79, 83].
Our Goals Our literature survey has two takeaways. First,
most researchers use a simple threshold or a trusted set of
vendors to determine if a ﬁle is malicious. The threshold
and trusted set are usually hand-picked without validating
the rationality of choices. Second, most researchers only take
one snapshot of VirusTotal results, failing to consider pos-
sible result changes. In this paper, we seek to run empirical
measurements on label dynamics to provide justiﬁcations for
some of the existing labeling methods. More importantly, we
want to identify potentially questionable approaches and pro-
pose better alternatives. Several works are related to ours. We
brieﬂy discuss the differences.
Closely Related Works.
Peng et al. [57] examined the
URL scanning engines of VirusTotal for phishing URL detec-
tion (data over a month). Our work focuses on anti-malware
engines (i.e., ﬁle scanning) over a long time (a year). We show
anti-malware engines have different/contradicting character-
istics compared to URL engines (details provided later).
Kantchelian et al. [40] proposed a machine learning model
to aggregate VirusTotal labels. However, they assumed Virus-
Total engines are independent of each other, for which we
show contradicting evidence in this paper. In addition, the data
collection method of [40] is different (e.g., ﬁle re-scanning
frequency is not controlled), which lost the opportunity to
observe ﬁne-grained label dynamics. Finally, [40] did not
have real ground-truth for the malware dataset, and assumed
VirusTotal labels become stable after a ﬁle is submitted to
VirusTotal for four weeks (for which we have different ob-
servations). Compared with [40], our unique contribution is
that we identify previously unknown dynamic patterns (e.g.,
hazard label ﬂips), measure the “inﬂuence” among vendors,
and provide simpler suggestions for researchers.
Other Related Works.
In addition to malware scanning,
VirusTotal also provides malware family information for
known malware samples. Researchers found that different
VirusTotal engines may attribute the same malware to differ-
ent malware families [37,55,63]. One existing work measured
the correctness and inconsistency of malware family names
based on a manually labeled dataset [55]. Other works aim to
assign a family name to a malware sample by aggregating the
family names reported by different VirusTotal engines [37,63].
These works looked into a different aspect of VirusTotal en-
gines compared to ours. More importantly, their analysis was
based on a single snapshot of VirusTotal scan, and did not con-
sider the possible changes of VirusTotal labels and malware
family names over time.
3 Data Collection
To achieve the above goal, we need to capture the dynamic
label changes of different engines over a long period of time.
Our measurement follows two main principles. First, we
choose “fresh” ﬁles for our study, i.e., ﬁles that are submitted
to VirusTotal for the ﬁrst time. This allows us to observe the
label dynamics from the very beginning without being dis-
tracted by the ﬁles’ previous histories. Second, to observe the
ﬁne-grained changes, we leverage the rescan API to trigger
the VirusTotal engines to analyze our ﬁles every day. Then
we use the report API to query the latest scanning results
every day. Table 2 summarizes all the datasets we collected.
USENIX Association
29th USENIX Security Symposium    2363
Table 2: Dataset summary. U: Unlabeled, M: Malware, B: Benign.
Dataset
Main
Malware-I
Malware-II
Benign-I
Benign-II
# Files
14,423
60
60
80
156
Type Observation Period
08/2018 – 09/2019
06/2019 – 09/2019
06/2019 – 09/2019
06/2019 – 09/2019
07/2019 – 09/2019
U
M
M
B
B
# Days
396
105
97
93
70
3.1 Main Dataset
To obtain a large set of “fresh” ﬁles, we use VirusTotal’s
distribute API. VirusTotal receives new ﬁle submissions
from users all over the world on a daily basis. The API returns
information of the latest submissions from users. The infor-
mation includes a submission’s different hash values, whether
the ﬁle has been submitted to VirusTotal before, the ﬁle type,
and all the engines’ scanning results. We randomly sampled
14,423 PE ﬁles that were submitted to VirusTotal on August
31, 2018 for the ﬁrst time. We focus on PE ﬁles since it is
the most popular submitted ﬁle type on VirusTotal [10, 65].
In addition, we hope to include both malicious and benign
ﬁles in our collection. We purposely selected samples so that:
(1) about half of the samples (7,197) had a “malicious label”
from at least one engine on August 31, 2018 (i.e., day-1); (2)
the other half of the samples (7,226) had “benign” labels from
all engines. After August 31, 2018, we leverage the rescan
API to let VirusTotal engines scan the 14,423 ﬁles every day
and use the report API to query the latest scanning results.
As we will discuss later, VirusTotal updates its engines on
a daily basis, so that using day as the crawling granularity
allows us to monitor the ﬁne-grained label dynamics, while
making good use of our VirusTotal API quota. We do not treat
these ﬁles as “ground-truth” data, because ﬁles submitted to
VirusTotal are suspicious ﬁles at best. There is an unknown
number of true malware samples mixed with benign ﬁles,
which remain to be detected by VirusTotal engines.
From August 31, 2018 to September 30, 2019, we invoked
VirusTotal’s rescan API for these 14,423 ﬁles every day. All
the ﬁles are in 32-bit. 5,798 ﬁles are Win32 DLL ﬁles, and the
rest are Win32 EXE ﬁles. Regarding ﬁle size, more than 95%
of the PE ﬁles are within the range of 4KB to 4MB. During
396 days’ data collection period, we successfully collected
data on 378 days (95.5%). Due to technical issues (e.g., power-
outage, server failures), we missed data on 18 days (4.5%).
We argue that the missing data only accounts for a very small
portion, and should not impact our overall conclusions.
3.2 Ground-truth Dataset
The main dataset is large and diversiﬁed, but these ﬁles do
not have ground-truth labels. As such, we create another set
of “ground-truth” ﬁles to assess the “correctness” of engines.
Creating ground-truth for this study is especially challeng-
ing because our goal is to examine the reliability of existing
malware engines. This means we could not use any engine’s
labels as the ground-truth. In addition, we need “fresh” ﬁles
for our experiment. This means, any well-known malware
discovered by existing efforts are not suitable since they were
usually scanned by VirusTotal engines in the past. If we use
well-known malware, we can only capture the “tails” of the la-
bel change sequences. For these reasons, we need to manually
craft the ground-truth sets.
Ground-truth Malware. To craft fresh malware sets, our
approach is to obfuscate well-known malware to create new
binaries. Obfuscation can help create “fresh” binaries that
have never been scanned by VirusTotal before. Meanwhile,
obfuscation is not necessarily a determining feature of mal-
ware — it is also often used by legitimate software to pro-
tect their intellectual property (copyright) or protect sensi-
tive functions (e.g., for payments and security) from reverse-
engineering [26, 61, 77].
We apply two obfuscation tools CodeVirtualizer [8] and
Themida [9] on four existing ransomware (Table 4 in the
Appendix) and create two malware sets respectively. Each
malware set contains 60 new malware samples obfuscated
from the seeds. We choose ransomware as the seeds due
to two reasons. First, it is easy to manually verify the mali-
cious actions (i.e., encrypting ﬁles, showing the ransomware
notes). Second, we manually conﬁrmed that the majority of
engines (57/65) advertise that they are capable of detecting
ransomware2, and thus ransomware is a relatively fair bench-
mark to compare different engines.
For each newly generated sample, we manually run the
binary in a virtual machine to conﬁrm the malicious actions
are preserved. We also manually conﬁrm that these samples
are indeed “fresh” to VirusTotal. These samples have differ-
ent hash values (e.g., SHA256, ssdeep) from the seeds and
can trigger VirusTotal’s ﬁle scanning after being submitted.
We perform daily VirusTotal scanning from June 18, 2019
for Malware-I and from June 25, 2019 for Malware-II. We
monitored the ﬁles over a shorter period of time (compared
to the main dataset), because we have already observed the
two malware datasets have similar patterns of label dynamics
as the main dataset.
Ground-truth Benign. We have a mixed approach to get
two benign sets, with 236 ﬁles in total. First, we apply the
same obfuscation tools to two benign programs (a sorting
algorithm written by ourselves in C and a text editor in Win-
dows). We generate 80 obfuscated goodware to examine po-
tential false positives of VirusTotal engines. These goodware
are directly comparable with the ground-truth malware since
they are generated in the same way (with different seeds).
We call this set as Benign-I. We conduct daily VirusTotal
scanning on this dataset for 93 days.
2The advertisement list is available at https://sfzhu93.github.
io/projects/vt/advertise.html.
2364    29th USENIX Security Symposium
USENIX Association
Second, to test real-world goodware, we manually build
156 PE programs (without obfuscation). Among them, 150
PE programs are built using the source code of GNU Core
Utilities (coreutils) [4] and the Mono project [6]. coreutils
contains a suite of Unix commands (e.g., cat, ls, rm) and
we use cygwin [5] to build them into PE ﬁles. Mono is an
open-source .NET development framework, which contains a
C# compiler, development environment and various libraries.
We build the Mono project on an Ubuntu machine. To in-
crease the diversity, we also select six built PE programs from
binutils [1], notepad++ [7] and ﬂeck [2] projects. We call this
set as Benign-II. Just as before, we perform daily VirusTotal
scanning on these 156 benign ﬁles for 70 days.
Limitations. We understand that the above ground-truth
sets are limited in scale and diversity: the samples are biased
towards obfuscated ﬁles and the malicious ﬁles are seeded
with ransomware. This is primarily due to (1) we don’t have
access to a large number of ﬁles (including both benign and
malicious ﬁles) that have no prior history on VirusTotal; and
(2) it takes huge manual efforts to validate the malicious func-
tionality still exists after obfuscation. Considering the rate
limit of VirusTotal, the number of the ground-truth ﬁles is
already the best effort. As such, the small ground-truth sets
are only used to complement the main dataset (which is a
large sample of real-world suspicious ﬁles). We use the main
dataset to measure the ﬁne-grained label dynamics of Virus-
Total over a long period of time. Then we use the ground-truth
sets to validate some of the observations from the main dataset
and cross-examine the correctness of VirusTotal engines.
3.3 Data Summary and Preprocessing
Across the ﬁve datasets in Table 2, we collected a total of
375,520,749 measurement points. Each measurement point is
characterized by a ﬁle-ID, a timestamp, an engine name, and a
label. These measurement points are generated by 78 different
engines in total. However, nine engines were newly added to
VirusTotal after we started the data collection for the main
dataset. We cannot observe these engines’ behaviors when the
main dataset was ﬁrstly submitted to VirusTotal. There are
another four engines, which were removed from VirusTotal’s
engine list during our data collection. The analysis results of
these four engines will not help VirusTotal users anymore.
As such, we do not consider these 13 engines in our analysis
and only focus on the remaining 65 engines in the following
sections. After ﬁltering out irrelevant engines, we still have
343,585,060 measurement points. Among them, the main
dataset contributes 341,668,521 data points.
4 Measuring Label Dynamics
In this section, we formally model the label changes on Virus-
Total. We ﬁrst characterize the different types of temporal
(a) The complete sequence
(b) The sequence after removing hazard ﬂips
Figure 2: An example ﬁle’s label sequence from AegisLab.
label changes and reason the possible causes. We then try to
estimate how long one should wait before a ﬁle’s labels be-
come stable across engines. In the end, we assess the impact
of temporal label dynamics on the labeling outcome (given
most researchers only submit ﬁles to VirusTotal to scan for