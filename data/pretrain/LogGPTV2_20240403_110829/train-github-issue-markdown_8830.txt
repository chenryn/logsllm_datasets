Using
  1. Elasticsearch: 2.3.1
  2. JVM version: 1.8.0_60-b27
  3. OS version: Centos 7
With
    .
    .
    .
    node:
      data: true
      master: false
      name: esd02-data4
      tier: hot
    os:
      available_processors: 8
    path:
      data:
          - /storage/sdb1
          - /storage/sdx1
    .
    .
    .
in `elasticsearch.yml` we have observed twice now where one of the disks (say,
`/storage/sdx1`) will fill up to 100% while the other `/storage/sdb1` still
has space. The error logs fill up with errors like:
    [logstash-syslog-2016.07.21][[logstash-syslog-2016.07.21][1]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: FileSystemException[/storage/sdx1/stage/nodes/0/indices/logstash-syslog-2016.07.21/1/translog/translog.ckp -> /storage/sdx1/stage/nodes/0/indices/logstash-syslog-2016.07.21/1/translog/translog-7395039383191700052.tlog: No space left on device];
    Caused by: [logstash-syslog-2016.07.21][[logstash-syslog-2016.07.21][1]] EngineCreationFailureException[failed to create engine]; nested: FileSystemException[/storage/sdx1/stage/nodes/0/indices/logstash-syslog-2016.07.21/1/translog/translog.ckp -> /storage/sdx1/stage/nodes/0/indices/logstash-syslog-2016.07.21/1/translog/translog-7395039383191700052.tlog: No space left on device];
    Caused by: java.nio.file.FileSystemException: /storage/sdx1/stage/nodes/0/indices/logstash-syslog-2016.07.21/1/translog/translog.ckp -> /storage/sdx1/stage/nodes/0/indices/logstash-syslog-2016.07.21/1/translog/translog-7395039383191700052.tlog: No space left on device
    java.nio.file.FileSystemException: /storage/sdb1/stage/nodes/0/indices/logstash-syslog-2016.07.21/_state/state-13787.st -> /storage/sdx1/stage/nodes/0/indices/logstash-syslog-2016.07.21/_state/state-13787.st.tmp: No space left on device
It's pretty obvious what's happening... `storage/sdx1` owns a shard of
`logstash-syslog-2016.07.21` and ES is refusing to "break" the shard across
multiple disk paths....
but it's also failing to recover from this on its own, or to stop sending data
to the shard whose disk is full. I would expect either (A) in the event of a
full disk, ES would nominate the other shards to accept 100% of the writes to
the index or (B) ES would loosen its requirement that shards are not allowed
to span multiple write paths.