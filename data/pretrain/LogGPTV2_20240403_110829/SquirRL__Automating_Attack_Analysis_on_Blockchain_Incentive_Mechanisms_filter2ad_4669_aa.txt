title:SquirRL: Automating Attack Analysis on Blockchain Incentive Mechanisms
with Deep Reinforcement Learning
author:Charlie Hou and
Mingxun Zhou and
Yan Ji and
Phil Daian and
Florian Tramèr and
Giulia Fanti and
Ari Juels
SquirRL: Automating Attack Analysis on
Blockchain Incentive Mechanisms with Deep
Reinforcement Learning
Charlie Hou∗
Carnegie Mellon University, IC3
PI:EMAIL
Mingxun Zhou∗
Peking University
PI:EMAIL
Yan Ji
Cornell Tech, IC3
PI:EMAIL
Phil Daian
Cornell Tech, IC3
PI:EMAIL
Florian Tramèr
Stanford University
PI:EMAIL
Giulia Fanti
Carnegie Mellon University, IC3
PI:EMAIL
Ari Juels
Cornell Tech, IC3
PI:EMAIL
Abstract—Incentive mechanisms are central to the function-
ality of permissionless blockchains: they incentivize participants
to run and secure the underlying consensus protocol. Designing
incentive-compatible incentive mechanisms is notoriously chal-
lenging, however. As a result, most public blockchains today
use incentive mechanisms whose security properties are poorly
understood and largely untested. In this work, we propose
SquirRL, a framework for using deep reinforcement learning
to analyze attacks on blockchain incentive mechanisms. We
demonstrate SquirRL’s power by ﬁrst recovering known attacks:
(1) the optimal selﬁsh mining attack in Bitcoin [56], and (2) the
Nash equilibrium in block withholding attacks [18]. We also
use SquirRL to obtain several novel empirical results. First,
we discover a counterintuitive ﬂaw in the widely used rushing
adversary model when applied to multi-agent Markov games with
incomplete information. Second, we demonstrate that the optimal
selﬁsh mining strategy identiﬁed in [56] is actually not a Nash
equilibrium in the multi-agent selﬁsh mining setting. In fact, our
results suggest (but do not prove) that when more than two
competing agents engage in selﬁsh mining, there is no proﬁtable
Nash equilibrium. This is consistent with the lack of observed
selﬁsh mining in the wild. Third, we ﬁnd a novel attack on a
simpliﬁed version of Ethereum’s ﬁnalization mechanism, Casper
the Friendly Finality Gadget (FFG) that allows a strategic agent
to amplify her rewards by up to 30%. Notably, [12] shows that
honest voting is a Nash equilibrium in Casper FFG; our attack
shows that when Casper FFG is composed with selﬁsh mining, this
is no longer the case. Altogether, our experiments demonstrate
SquirRL’s ﬂexibility and promise as a framework for studying
attack settings that have thus far eluded theoretical and empirical
understanding.
Keywords—Blockchain, Deep reinforcement learning, Incentive
mechanisms
∗Equal contribution
Network  and  Distributed  Systems  Security  (NDSS)  Symposium  2021 
21-25  February  2021, Virtual 
ISBN  1-891562-66-5
https://dx.doi.org/10.14722/ndss.2021.24188
www.ndss-symposium.org
I.
INTRODUCTION
Blockchains today require participants to expend substantial
resources (storage, computation, electricity) to ensure the
correctness and liveness of other users’ transactions. Most public
blockchains therefore rely critically on incentive mechanisms to
motivate users to participate in blockchain consensus protocols.
That is, users are typically paid (in native cryptocurrency) to
sustain the system. Incentive mechanisms are therefore central
to the functionality of most permissionless blockchains.
For example, Bitcoin’s consensus protocol requires partici-
pants (also known as miners) to build a sequential data structure
of blocks, where each block is generated in a computationally
intensive process (called mining). To incentivize participation,
Bitcoin miners receive a block reward (in Bitcoins) for every
block they mine that is accepted by the rest of the network.
Miners also receive smaller transaction fees for each transaction
included in a block; this is done to encourage transaction
packing. The incentive mechanisms of block rewards and fees
have driven the remarkable growth of the Bitcoin ecosystem.
In practice, poorly-designed incentive mechanisms can be
exploited by rational users. By deviating from the protocol-
speciﬁed behavior, users may be able to accumulate more
rewards than what they are nominally entitled to. For example,
selﬁsh mining is a well-known attack on Bitcoin’s incentive
mechanism that allows a strategic miner to reap more than her
fair share of block rewards by waiting to publish her blocks
until she would cause the most damage to the honest majority
[19]. Many subsequent papers have explored both attacks on
Bitcoin’s incentive mechanism [13], [18], [29], [40], [48], [56]
as well as other cryptocurrencies [24], [49], [51], [54].
Today, attacks on blockchain incentive mechanisms are
typically studied through a lengthy process of modeling and
theoretical analysis [13], [18], [24], [29], [40], [48], [51],
[54], [56]. As many cryptocurrencies lack the resources for
theoretical analysis, the vast majority of blockchain incentive
mechanisms have not been studied at all. Substantial amounts
of cryptocurrency may thus be vulnerable to unknown attacks.
In this work, we propose SquirRL, a generalizable frame-
work for using deep reinforcement learning (DRL) to analyze
blockchain incentive mechanisms. SquirRL is intended as
a general-purpose methodology for blockchain developers
to test incentive mechanisms for vulnerabilities. It does not
provide theoretical guarantees: just because it does not ﬁnd
any proﬁtable attacks does not mean that honest behavior
is a dominant strategy. We ﬁnd in practice, however, that
instantiations of SquirRL are effective at identifying adversarial
strategies, which can be used to prove that an incentive mech-
anism is insecure. Such tools are often useful in applications.
For example, some tools in software development, such as
fuzzers [38], are useful for identifying vulnerabilities despite not
providing completeness guarantees. Our primary contributions
are threefold:
1) Framework: We present SquirRL as a general framework for
exploring vulnerabilities in blockchain incentive mechanisms
and recovering adversarial strategies. The framework broadly
involves: (1) creation of a simulation environment, with ac-
companying feature and action spaces reﬂecting the views
and capabilities of participating agents; (2) selection of an
adversarial model, including numbers and types of agents; and
(3) selection of a suitable RL algorithm and associated reward
function. As part of this framework, we develop a general state
space representation for a broad class of blockchain incentive
mechanisms, which allows us to trade off feature dimensionality
with accuracy. We show how to use this framework ﬂexibly
to handle settings involving varying environments, numbers of
agents, and rewards.
2) Selﬁsh-mining evaluation: We apply SquirRL to various
blockchain consensus/incentive protocols to analyze variants of
selﬁsh mining. Using SquirRL, we are able to recover known
theoretical results in the Bitcoin protocol, while also extend-
ing state-of-the-art results to domains that were previously
intractable (e.g., the multi-agent setting, larger state spaces,
other protocols). Our experiments suggest two new ﬁndings:
• Theoretically analyzing repeated interactions between multi-
ple strategic agents is difﬁcult due to the large state and action
space. Prior work in this setting has therefore simpliﬁed
the strategy space [45], under which a semi-selﬁsh mining
strategy is shown to be a Nash equilibrium. However, our
experiments suggest that under a more general strategy space,
semi-selﬁsh mining is not a Nash equilibrium. In fact, for
the Bitcoin protocol, all variants of selﬁsh mining appear to
be unproﬁtable in settings with at least three strategic agents.
This is consistent with the lack of observed selﬁsh mining
in the wild, although it is unclear whether this observation
or other externalities are to blame.
• We ﬁnd that the classical notion of a rushing adversary,
which is widely used in the cryptographic literature to model
a worst-case adversary [20], [43], can give counterintuitive
and nonphysical results in multi-strategic-agent settings. This
has implications beyond the blockchain domain regarding
how security researchers should model multi-agent settings.
We expect this observation may be particularly relevant as
DRL gains adoption as a tool for learning to attack and/or
defend complex systems that are not amenable to theoretical
analysis [50], [58], [73].
3) Demonstration of extensibility: We show that SquirRL is
generally applicable to attacks on other types of incentive
mechanisms beyond selﬁsh mining:
• We apply SquirRL to the proposed Ethereum ﬁnalization
mechanism, Casper the Friendly Finality Gadget (FFG) [11].
These results illustrate that a strategic miner can collude
with a Casper FFG validator to amplify its rewards by up to
30% more than a strategic miner or validator could do alone.
Such strategic collusion can cause honest validators to leave,
progressively corrupting the system. This raises important
questions about the composability of incentive mechanisms.
• We apply SquirRL to block withholding attacks, where
it converges to two-player strategies that match the Nash
equilibrium in [18].
Paper outline: We motivate the need for SquirRL in §II,
explaining why existing techniques fall short, and then provide
background on deep reinforcement learning in §III. We present
the design of SquirRL in §IV. We evaluate SquirRL on a variety
of settings: the single-strategic-agent selﬁsh mining setting in
§V, the multi-strategic-agent selﬁsh mining setting in §VI, and
Casper FFG and the Miner’s Dilemma in §VII. We discuss
related work in §VIII and conclude with a brief summary
in §IX.
II. MOTIVATION
Today, the process for analyzing new attacks on blockchain
incentive mechanisms is manual and time-consuming. Typically,
studying such attacks require some combination of theoretical
analysis, simulation, and intuition [56]. Each becomes more
difﬁcult to obtain as the complexity of the underlying protocol
grows. In particular, game-theoretic analysis of these systems
tends to be challenging for three reasons: (1) the state space is
large (or continuous), (2) the game is repeated, and (3) there
can be many agents. Indeed, much of the existing analysis has
focused on settings where only one or two agents are allowed
to deviate from the honest mining strategy [45], [56].
At the same time, new protocols are emerging frequently,
each with its own incentive mechanism [31], [72], [77]. Of-
tentimes, protocol designers rely on intuition to reason about
the security of their incentive mechanisms, in part because
we lack general-purpose tools for mechanism analysis. Even
when protocol designers provide security proofs, they typically
at most show that honest behavior is a Nash equilibrium if
honest parties are a signiﬁcant portion of the participants in the
protocol [11], [53]. This weak guarantee says nothing about
other equilibria or the (perhaps more realistic) setting in which
many competing parties behave rationally. Our central premise
is that a systematic and largely automated approach for testing
incentive mechanisms would streamline this process, and could
help catch incentive mechanism bugs before deployment in the
wild.
A. Use Case
We envision protocol designers using our framework to study
a natural progression of adversarial models and experiments
to help address key security and incentive-alignment questions
for an incentive mechanism M . These are shown in Table I.
For a given adversarial resource (e.g., mining power), a
single strategic agent S competing against a group of honest
agents represents the most powerful possible adversary. (Such
an agent can simulate any set of strategies among multiple
agents.) S can steal rewards from honest agent H , whose
2
Number of Strategic Agents
Representative Setting
1
2
k ≥ 3
Single strategic agent
Emergent strategic-agent
behavior
Community of competing
strategic agents
Agent Types
Sys → S
S vs. Sys
Sys vs. Sys
(cid:123)(cid:122)
k agents
(cid:125)
(cid:124)
Sys vs. . . . vs. Sys
Explored questions
• What impact from worst-case attack?
• What is optimal adversarial strategy?
•
•
•
•
•
Is S dominant?
Is S proﬁtable against a competing agent?
Is two-agent game stable?
Is multi-agent strategic play proﬁtable?
Is H dominant in the multi-agent setting?
TABLE I: Experimental progression in an automated incentive mechanism analysis system Sys. The sequence of experiments with
increasing numbers of strategic agents sheds light on key security questions for mechanism M . Notation Sys → S is shorthand
denoting S as the output of the automated system.
strategy is ﬁxed a priori and thus cannot develop a counter-
strategy. Learning S thus yields insights into the worst-case
performance of M .
Addition of a second strategic agent then addresses the
question of whether S itself is dominant or suboptimal in
the presence of a competing agent. This setting captures the