# SquirRL: Automating Attack Analysis on Blockchain Incentive Mechanisms with Deep Reinforcement Learning

## Authors
- Charlie Hou, Carnegie Mellon University, IC3 (Email: [EMAIL])
- Mingxun Zhou, Peking University (Email: [EMAIL])
- Yan Ji, Cornell Tech, IC3 (Email: [EMAIL])
- Phil Daian, Cornell Tech, IC3 (Email: [EMAIL])
- Florian Tramèr, Stanford University (Email: [EMAIL])
- Giulia Fanti, Carnegie Mellon University, IC3 (Email: [EMAIL])
- Ari Juels, Cornell Tech, IC3 (Email: [EMAIL])

## Abstract
Incentive mechanisms are crucial for the functionality of permissionless blockchains, as they motivate participants to run and secure the underlying consensus protocol. Designing incentive-compatible mechanisms is notoriously challenging, and most public blockchains use mechanisms whose security properties are poorly understood and largely untested. In this work, we introduce SquirRL, a framework that leverages deep reinforcement learning (DRL) to analyze attacks on blockchain incentive mechanisms.

We demonstrate SquirRL's effectiveness by recovering known attacks:
1. The optimal selfish mining attack in Bitcoin [56].
2. The Nash equilibrium in block withholding attacks [18].

We also use SquirRL to obtain several novel empirical results:
1. We identify a counterintuitive flaw in the widely used rushing adversary model when applied to multi-agent Markov games with incomplete information.
2. We show that the optimal selfish mining strategy identified in [56] is not a Nash equilibrium in the multi-agent setting. Our results suggest that when more than two agents engage in selfish mining, there is no profitable Nash equilibrium, consistent with the lack of observed selfish mining in practice.
3. We discover a new attack on a simplified version of Ethereum's finalization mechanism, Casper the Friendly Finality Gadget (FFG), which allows a strategic agent to amplify her rewards by up to 30%. This finding indicates that when Casper FFG is combined with selfish mining, honest voting is no longer a Nash equilibrium, contrary to previous findings [12].

Overall, our experiments highlight SquirRL's flexibility and potential as a tool for studying attack scenarios that have eluded theoretical and empirical understanding.

**Keywords:** Blockchain, Deep reinforcement learning, Incentive mechanisms

*Equal contribution*

## I. Introduction
Blockchains require participants to expend substantial resources (storage, computation, electricity) to ensure the correctness and liveness of transactions. Most public blockchains rely on incentive mechanisms to motivate users to participate in consensus protocols. For example, Bitcoin miners receive block rewards and transaction fees to sustain the system. Poorly designed incentive mechanisms can be exploited by rational users, leading to attacks such as selfish mining, where a strategic miner can gain more than their fair share of rewards.

Currently, attacks on blockchain incentive mechanisms are studied through lengthy processes of modeling and theoretical analysis. Many cryptocurrencies lack the resources for such analysis, leaving their incentive mechanisms untested and potentially vulnerable to unknown attacks.

In this work, we propose SquirRL, a generalizable framework for using DRL to analyze blockchain incentive mechanisms. SquirRL is intended as a tool for blockchain developers to test incentive mechanisms for vulnerabilities. While it does not provide theoretical guarantees, it is effective at identifying adversarial strategies that can be used to prove an incentive mechanism's insecurity.

Our primary contributions are:
1. **Framework:** We present SquirRL as a general framework for exploring vulnerabilities in blockchain incentive mechanisms and recovering adversarial strategies. The framework involves creating a simulation environment, selecting an adversarial model, and choosing a suitable RL algorithm and reward function. We develop a general state space representation for a broad class of blockchain incentive mechanisms, allowing us to trade off feature dimensionality with accuracy.
2. **Selfish-mining evaluation:** We apply SquirRL to various blockchain consensus/incentive protocols to analyze variants of selfish mining. Our experiments recover known theoretical results in the Bitcoin protocol and extend state-of-the-art results to previously intractable domains. We find that semi-selfish mining is not a Nash equilibrium in a more general strategy space, and all variants of selfish mining appear unprofitable in settings with at least three strategic agents. We also show that the classical notion of a rushing adversary can give counterintuitive results in multi-agent settings.
3. **Demonstration of extensibility:** We show that SquirRL is applicable to other types of incentive mechanisms. For instance, we apply SquirRL to Ethereum's Casper FFG, where a strategic miner can collude with a validator to amplify rewards by up to 30%. We also apply SquirRL to block withholding attacks, where it converges to two-player strategies matching the Nash equilibrium in [18].

**Paper outline:**
- §II: Motivation
- §III: Background on Deep Reinforcement Learning
- §IV: Design of SquirRL
- §V: Evaluation in the single-strategic-agent selfish mining setting
- §VI: Evaluation in the multi-strategic-agent selfish mining setting
- §VII: Evaluation in Casper FFG and the Miner’s Dilemma
- §VIII: Related Work
- §IX: Conclusion

## II. Motivation
The process for analyzing new attacks on blockchain incentive mechanisms is manual and time-consuming, involving theoretical analysis, simulation, and intuition. As the complexity of protocols grows, game-theoretic analysis becomes more challenging due to large state spaces, repeated games, and multiple agents. Existing analysis often focuses on settings with one or two deviating agents.

New protocols with unique incentive mechanisms are emerging frequently. Protocol designers often rely on intuition to reason about security, and even when security proofs are provided, they typically only show that honest behavior is a Nash equilibrium if honest parties are a significant portion of the participants. A systematic and automated approach for testing incentive mechanisms could streamline this process and help catch bugs before deployment.

### Use Case
Protocol designers can use SquirRL to study a natural progression of adversarial models and experiments, addressing key security and incentive-alignment questions. For a given adversarial resource, a single strategic agent competing against honest agents represents the most powerful possible adversary. Adding a second strategic agent addresses whether the first agent is dominant or suboptimal in the presence of competition. Increasing the number of strategic agents further explores the stability and profitability of multi-agent strategic play.

| Number of Strategic Agents | Representative Setting | Agent Types | Explored Questions |
|----------------------------|------------------------|-------------|--------------------|
| 1                          | Single strategic agent  | Sys → S     | What impact from worst-case attack? What is the optimal adversarial strategy? Is S dominant? |
| 2                          | Emergent strategic-agent behavior | S vs. Sys | Is S profitable against a competing agent? Is the two-agent game stable? |
| k ≥ 3                      | Community of competing strategic agents | Sys vs. ... vs. Sys | Is multi-agent strategic play profitable? Is H dominant in the multi-agent setting? |

This sequence of experiments with increasing numbers of strategic agents provides insights into the security and stability of the incentive mechanism M.