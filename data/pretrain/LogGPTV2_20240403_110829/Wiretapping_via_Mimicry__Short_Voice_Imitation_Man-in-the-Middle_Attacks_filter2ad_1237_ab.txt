group. Other members’ devices display respective SAS values.
Each member compares their respective SAS values with that
announced by the leader. If SAS values do not match, a mem-
ber aborts the process on its device and asks everyone else to
do the same. If no one identiﬁes an error, each member accepts
group association on its device.
2. Leader-CopySAS: After the group size has been validated, the
leader’s device displays the SAS value encoded as a number and
the leader announces it to the group. Other members input the
announced SAS value into their devices. If the devices indicate
failure (SAS value mismatch) they abort the process and warn
others to do the same. Otherwise, everyone accepts.
3. Peer-VerifySAS: After the group size has been validated, each
device displays a numeric SAS value and each member com-
pares its SAS value with that of their neighbor on the right (pre-
deﬁned via a virtual circular topology). In case of a mismatch, a
member aborts the process and instructs others to do the same.
Otherwise, everyone accepts.
3. ATTACK OVERVIEW & BACKGROUND
We discuss why recognition of the identity of a speaker (espe-
cially from short speech) can be a complex task for human users,
and provide an overview of our Cfone voice imitation attacks that
exploit this inherent limitation of the human cognitive system.
3.1 Manual Speaker Recognition Limitations
In an MITM attack against the SAS protocol of a Cfone, Mallory
can insert herself into a session and gain full access to the data be-
ing transferred between the Alice and Bob. To do so, Mallory needs
to hijack the session and impersonate each party. As discussed in
Section 2.1, Cfone’s security assumption is that although Mallory
has full control over the communication channel, it cannot insert
voice messages that mimic Alice/Bob. Should this hypothesis be
valid, the SAS value which is verbally exchanged on this channel
can always authenticate Alice and Bob, foiling the MITM attack. A
Cfone MITM attack seems relatively straight-forward against data
communication (i.e., non verbal communication messages of the
SAS protocol) [54], however, it is assumed that voice is unique to
each individual, and therefore it is impossible to impersonate it.
This assumption relies on special characteristic of speech which
appears to make it difﬁcult to impersonate at ﬁrst glance.
Speech construction is a complex area. In simple terms, speech
consists of words, each of which is a combination of speech sound
units (phones). However, in reality, human voice is not as simple
as this deﬁnition. Voice signal created at the vocal folds travels and
gets ﬁltered through vocal tract to produce vowels and consonants.
Figure 3: High-level diagram of the attack
Human body structure, vocal folds, articulators and human physi-
ology and the style of speech provide each individual a potentially
distinguished voice characteristic. Pitch, timbre and tone of speech
are some of the features that may make a voice unique (for further
information, we refer the reader to [15]). Therefore, the assump-
tion that voice is unique, just like ﬁngerprint or iris, does have some
validity (although how much is a question explored in this paper).
Speech perception and recognition, the tasks that Cfone users
have to perform while validating the SAS values, are even more
complex than speech construction. There exists considerable lit-
erature on how speech is recognized [18, 19, 38]. Linguistics re-
searchers have conducted various experiments and analyzed the
capabilities of human speech recognition over different parameters,
such as length of the samples, number of different samples, samples
from familiar vs. famous people, and combinations thereof [38].
In an experiment, conducted in [31], the participants were asked
to identify a voice when the sample string presented to them was
“hello”, which resulted in a correct recognition rate of only 31%.
However, when a full sentence was presented to the participants,
the recognition rate increased to 66%. In the study of [23], a 2.5
minute long passage was presented as a sample to the participants,
resulting in the average recognition accuracy of 98%. Many other
experiments have been performed over the years evaluating human
users’ performance in voice recognition [28]. They show that the
shorter the sentence, the more difﬁcult it is to identify the source.
Based on this literature survey, it appears that the task of es-
tablishing the identity of a speaker may be challenging for human
users, especially in the context of short SAS, and serves as a weak-
link in the security of the Cfone SAS communication.
3.2 Attack Components
Our short voice imitation attacks involve the following compo-
nents (our higher-level attack is depicted in Figure 3).
Data Relaying:
In a Cfone, ﬁrst an unauthenticated SAS pro-
tocol performs a key exchange during session initiation or Real
Time Protocol (RTP) media stream (see Figure 4). This gener-
ates a session key, which will contribute to the encryption of the
media during the Secure RTP (SRTP) session. So far the proto-
col is unauthenticated, therefore it is susceptible to a MITM [36]
attack, and the session key might have already been revealed to
Mallory. To ensure that Mallory was not present during unauthenti-
cated key exchange, Alice and Bob verbally communicate the SAS
over an SRTP session. In our attack, we assume that an MITM was
performed during the unauthenticated key exchange protocol, and
therefore Mallory has access to the plain audio during the conver-
sation. Mallory is now interested in manipulating the SAS to hide
her presence in unauthenticated phase of the protocol (i.e., non SAS
communication). Mallory is not interested to alter any conversation
except for the SAS dialogue (but of course interested in listening to
all). Therefore, such conversations are simply relayed by Mallory,
as is, to Alice and Bob.
Training Data Collection: Mallory needs to collect some data in
advance to be used as the training set for the SAS voice imper-
sonation attacks. For the reordering attack, Mallory needs to build
a dictionary of distinct SAS words (e.g., digits for numeric, and
words for PGP word list and Madlib SAS). In contrast, in morph-
Figure 4: Cfone Protocol Flow (SIP: Session Initiation Protocol; RTP:
Real-Time Transport Protocol)
ing attack, she requires a few sentences to train the system to mimic
the victim’s voice. To do so, Mallory can listen to several samples
of the victim’s voice and collect words spoken by the victim from
a previous VoIP session, or even the session under attack. Alter-
natively, Mallory can fool the victim into recording these training
samples with social engineering trickeries (discussed in Section 6).
Keyword Spotting: To replace a valid SAS with a new desired
SAS, Mallory needs to ﬁrst look up the SAS in the ﬁrst few con-
versations over SRTP, while Alice and Bob verbally exchange the
SAS. She can simply relay any non-SAS dialogue, and only ma-
nipulate the SAS at the right time. Manually performing this look-
up for the presence of SAS within arbitrary conversations might
be tedious. An alternative is keyword spotting, which can be per-
formed automatically. Keyword spotting deals with identifying a
speciﬁc word in an utterance.
It can be performed online on an
audio stream or ofﬂine on audio ﬁles in audio mining application.
Several keyword spotting methods have been proposed in litera-
tures. The work of [42] provides a comparison of different key-
word spotting approaches. Any of these approaches can be used by
Mallory to detect the SAS values (numbers or words). In Section
4, we will describe our implementation of a keyword spotter based
on off-the-shelf voice recognition systems.
Interruption and Insertion: Once the SAS dialogue is found,
Mallory should “drop” it to make it unavailable to the other party,
and replace it with an forged SAS (recall that our threat model al-
lows dropping arbitrary packets). At this point, the voice MITM
on SAS happens. The forged SAS is either derived from previously
recorded voice of the victim saying the same SAS, or is constructed
by making a collage from the victim’s speech (reordering attack),
or is constructed by morphing victim’s voice signal (voice morph-
ing attack). It is not implausible to imagine that Mallory could be a
professional impersonator who can speak a thousand voices (“Rich
Little”). Usually this is not the case. Hence, we rely on automated
reordering attack and voice conversion techniques.
Voice Reordering: An attacker who wants to insert a forged SAS
into the conversation can build a dictionary of all possible words,
and impersonate a legitimate party by remixing SAS in his/her
voice, which we call reordering attack. After collecting SAS
atomic units, she can cut pieces of the legitimate audio signal and
make an ofﬂine collage of SAS messages for future use. Another
attack similar to reordering attack is a text-to-speech system which
is speciﬁcally trained to produce synthesized voices only on a lim-
ited domain of vocabulary. An example of such a synthesizer is
presented in [16].
Voice Morphing: Although a limited domain synthesizer may pro-
duce audio with almost the same quality as a human being, de-
Figure 5: High-level diagram of morphing attack
pending on the SAS encoding, pre-collecting all words is not al-
ways practical. And, if the attacker does not have all possible
units of SAS in the dictionary, she can not produce remixing or
a limited domain synthesizer. In this situation, Mallory can try to
mimic victim’s voice. Attack could be successful if the adversary
can “convert”, for example, his own voice to the target’s (victim’s)
voice. We call this the voice morphing or conversion attack. There
are several voice conversion and transformation techniques which
change the characteristic of voice such as frequency, pitch, and tim-
bre [40, 41, 53]. Other techniques ﬁnd a relation between human
articulators and voice features [16, 43–46]. All these techniques
work on a training system to adapt the system and can eventually
convert any utterance in Mallory’s own voice to the target voice
even though such voice is not available in the training set. Mallory
only needs to collect a few minutes worth of training data of the
victim voice in order to perform this conversion. Later, Mallory
may also produce an ofﬂine dictionary of all possible SASs in the
victim voice. Figure 5 is the high-level picturization of this attack.
3.3 Attacking Different Mechanisms
The attacks presented above can be applied to undermine the se-
curity of 2-Cfones mechanisms. To attack Compare-Conﬁrm, Mal-
lory needs to do voice impersonation in both directions: imperson-
ating both Alice’s and Bob’s voices. To compromise the Copy-
Conﬁrm mechanism, Mallory only needs to impersonate the SAS
in one direction. If Mallory is interested in doing only a one-way
MITM attack (e.g., Alice to Bob), it only needs to do imperson-
ation on the channel over which the result of SAS comparison is
conveyed (e.g., by Bob to Alice). Here, Mallory simply needs to
impersonate “Yes” in Bob’s voice thereby fooling Alice into ac-
cepting an attacked session.
With n-Cfones, the Peer-VerifySAS can be attacked in the same
way as 2-Cfones, except that Mallory may need to do the attack on
multiple point-to-point SAS exchanges in case of the latter. Leader-
CopySAS can be attacked in the same way as Copy-Conﬁrm in one-
direction or both directions. Leader-VerifySAS can be similarly
attacked.
In addition, all the mechanisms can be relatively easy
compromised via the “group count impersonation” attack whereby
Mallory simply increases the group count by at least 1 and imper-
sonates that “increased count” in leader’s or a peer’s voice.
4. DESIGN & IMPLEMENTATION
Communication Channel: Java Media Framework API (JMF) en-
ables audio, video and other media to be captured, played, and
streamed. We used JMF API to capture and transmit RTP pack-
ets at each party (Alice, Bob and Mallory). JMStudio open source
code was adopted to implement the communication channel, to re-
ceive, capture and transmit media streams across the network.
Datasets Used: We used a variety of samples in different noise
proﬁles, including samples recorded in professional recording en-
vironment as well as data collected using basic audio recorders. To
have a good variety of recordings, we used three different datasets:
First is the Arctic US English single speaker databases which has
been constructed at the Language Technologies Institute at CMU.
A detailed report on the structure and content of the database and
the recording environment is available in [24]. The databases con-
sist of around 1150 utterances include US English male and fe-
male experienced speakers. The second dataset is VoxForge, set
up to collect transcribed speech for use with Free and Open Source
Speech Recognition Engines. We picked US English male record-
ings with 16KHz sampling rate. The data samples we used are all
recorded in unprofessional recording environment with free tools
such as Audacity and the narrators are non expert speakers. Finally,
we recorded two other voices using the basic audio recorder on an
iPhone 5s, and two voices recorded by Audacity 2.0.5 on a Mac-
book Air laptop with internal microphone. Same as the VoxForge
dataset, the narrators are not expert speakers. The Audio ﬁles in
all our datasets are in WAV (Microsoft) 16 bit signed PCM format
with a sampling rate of 16 KHZ in mono with single channel.
Keyword Spotting: For the purpose of keyword spotting, we used
CMU’s Sphinx open source speech recognition system. We utilized
Sphinx4 recognizer. Sphinx-4 is very ﬂexible in its conﬁguration,
providing a high-level interface to setup most of the components of
the system. The conﬁguration ﬁle is used to set all variables includ-
ing recognizer, decoder, search manager, acoustic model, language
model and dictionary components as well as conﬁguration param-
eters such as: the absolute beam width that speciﬁes the maximum
number of hypotheses to consider; relative beam width that deﬁnes
a trade-off between accuracy and search time; language weight or
language scaling factor; insertion probability that controls word
breaks recognition; and the silence insertion probability that con-
trols how aggressive Sphinx is at inserting silences.
Sphinx takes the voice waveform as input, splits it into utterances
by silences, then recognizes it based on the best matching combina-
tion of words. First, it gets a feature vector of each frame and then
uses models to match this feature vector with the most probable
feature vector in the model. So, it was important for us to adapt the
models to ﬁt our purpose and obtain accurate recognition results.
Three models are used in Sphinx speech recognition system.
First is the acoustic model that contains acoustic properties for
each phone. We evaluated the speech recognition with CMUS-
phinx acoustic models, which was quite acceptable for numeric
SAS recognition. However, we adopted the acoustic model for PGP
word list and Madlibs based on our speakers. As we will present in
Section 5, it is enough to have 5 minutes of speech of a speaker to
achieve a high accuracy. The second model involves a phonetic dic-
tionary that contains a mapping from words to phones. We adapted
the CMU’s Pronouncing Dictionary (CMUdict) to cover all words
available in PGP word List. The third model is language model
or a language grammar that deﬁnes the structure of the language,
such as the probability that two speciﬁc words come consequently.
Such model is essential to restrict word matching. Compared to
natural language structure, SAS language structure is very simple,
it is a series of digits, or is two (or more) words from a PGP word
list, or a sentence based on a Madlib phrase. Therefore, we built a
grammar for our speciﬁc design.
There are some implementations of CMU Sphinx as keyword
spotter. We changed Sphinx Audio Alignment code to transcribe
and retrieve the time information for certain words. Once the ap-
pearance time of a SAS is captured, we snip that frame and add it
as a single SAS to our SAS attack dictionary.
Reordering Attack: To implement the reordering attack, we de-
veloped a simple Java application that reads individual SAS words
and produce any SAS combinations. Later, any of these combi-
nations are picked and inserted in the voice MITM attack. Obvi-
ously, rather than building an ofﬂine dictionary of all combinations,
remixing can be performed on the ﬂy at the time of the attack. An
alternative is a limited domain synthesizer speaking in the victim
voice. Festival [2] limited domain synthesizers can produce voices
very similar to the target voice, but its performance is optimized
whenever all SAS atomic units are pre-recorded at least one time.