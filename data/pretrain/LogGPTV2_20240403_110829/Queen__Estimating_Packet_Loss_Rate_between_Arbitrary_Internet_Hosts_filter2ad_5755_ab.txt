100
200
300
400
Direct RTT (ms)
500
0
0
2
4
6
Loss Rate (%)
100
)
%
(
n
o
i
t
u
b
i
r
t
s
D
e
v
i
l
i
t
a
u
m
u
C
Direct
Queen
8
10
80
60
40
20
0
−4
−2
0
2
4
Loss Rate (direct − Queen)(%)
(a) Latency Comparison
(b) LossRate Comparison
(c) LossRate Diﬀerence
Fig. 5. Direct Path Validation Results
350
300
250
200
150
100
50
)
s
m
(
T
T
R
n
e
e
u
Q
0
0
Experiment
Optimal
300
100
Direct RTT (ms)
200
100
)
%
80
(
n
o
i
t
u
b
i
r
t
s
i
D
e
v
i
t
a
l
u
m
u
C
60
40
20
0
0
2
4
6
Loss Rate (%)
100
)
%
(
n
o
i
t
u
b
i
r
t
s
D
e
v
i
Direct
Queen
8
10
l
i
t
a
u
m
u
C
80
60
40
20
0
−4
−2
4
Loss Rate (Direct − Queen) (%)
0
2
(a) Latency Comparison
(b) LossRate Comparison
(c) LossRate Diﬀerence
Fig. 6. PlanetLab Validation Results
within 1% for more than 80% of paths. Finally, it appears that iPlane does not
return loss rate for any of the paths in this experiment. Apparently, these paths
are not covered by its database.
4.2 PlanetLab Validation
In this set, we use PlanetLab (PL) to conduct validation, as in Fig. 4(b). Similar
to 4.1, we again run two kinds of probes in parallel with the same pattern. The
diﬀerence is: in direct probing, we run a UDP probing client at one PL node and
a UDP echoing server at another PL node, to get the ground truth loss rate.
We locate two nearby corresponding DNS servers (e.g. no more than 5ms away),
one for each PL node, and estimate the loss rate between them. We compare
loss rate estimated by both methods to see whether they match each other. In
this set of validation, the path whose loss rate is estimated by Queen is slightly
diﬀerent from the path estimated by direct probing – the former path is between
the two name servers while the later one is between the two PL nodes.
We choose 5 PL nodes with recursive local DNS servers as sources. Each source
picks ∼70 target PL nodes from diﬀerent continents. We get results for ∼260
paths, where 200 show loss either in UDP probing or Queen. Fig. 6 depicts the
results. Again, two latencies and loss rates match very well. In particular, the
absolute loss rate diﬀerence between two methods is within 1% for more than
85% paths. At the meantime, iPlane returns zero loss rates for all the paths in
this experiment, which is quite far from both Queen and the ground truth.
64
Y.A. Wang et al.
Previous study [15] suggested that small 40-byte probes tend to experience
less losses than large 1000-byte probes. Since query packets generated by Queen
can be at most 280 bytes (limited by the maximum length of DNS hostnames,
which is 255), it raises a concern that Queen might under-estimate the true loss
rate. To study this issue, we re-run the same validation in parallel with diﬀerent
probing size of 80B, 160B and 240B. We observe that packet size in fact has very
little eﬀect on loss rate. In addition, we compare direct UDP probes with size of
80B, 240B and 1200B (very large, close to MTU) and the packet size also has
very minimum eﬀect(details skipped due to space constraint).
5 Experiment
After validating that our method has reasonably high accuracy, we conduct a
loss rate measurement for a large geographic area with world-wide coverage.
5.1 Measurement Setup
We pick one server for each country from our open recursive DNS server list and
measure the loss rate between each pair of servers. The ﬁnal data set covers 6
continents and 147 countries, with 10,731 paths in total. The complete measure-
ment involves a large number of paths. On each path, Queen sends query probes
following exponential inter-arrival time with average 500ms for 15 minutes, so
that their associated counting process is Poisson. To speed up the measurement
process, we have developed a distributed execution platform, which splits the
complete task into many smaller jobs, spreads these jobs onto PL nodes and
executes them in parallel. This platform helps to complete the measurement
quickly (e.g., with 300 PL nodes, 10,000 paths, each node only needs to run 33
jobs, and the entire task takes slightly more than 8 hours to complete).
5.2 Summary of Results
We group the sampled servers by continent and analyze loss rates within/cross
continents. Fig. 7(a) shows the loss rate statistics within each continent, and
Fig. 7(b) cross continents. Some results are intuitive – North America and Europe
have low loss rates, no matter intra-continent or cross-continent. This is clearly
due to good networking infrastructure with the two regions and connectivity
between them. In addition, North America and Europe always have lower loss
rates within the continent than cross to the other continent. Not as intuitive
though, we also observe that, for other continents, the loss rates are in fact
lower cross to North America or Europe than within the continent itself. This,
we believe, reﬂects the fact that North America and Europe are currently the
hubs of the Internet.
6 Related Work
The study of Internet packet loss rate can be dated back to more than a decade
ago. It is conducted to understand Internet itself, as well as the impact on
Queen: Estimating Packet Loss Rate between Arbitrary Internet Hosts
65
Africa
Asia
Europe
100
2/3
100
2/3
100
0/1
50
4/5
50
4/5
0/1
50
2/3
4/5
0/1
)
%
(
n
o
i
t
u
b
i
r
t
s
i
D
e
v
i
t
a
u
m
u
C
l
100
0
0
5
0
N. America
0
0
10
1
100
5
2
Oceania
0
0
10
3
100
10
5
4
5
S. America
0/1
4/5
50
2/3
50
2/3
0/1
4/5
50
2/3 4/5
0/1
0
0
5
10
0
0
Loss Rate (%)
5
10
0
0
5
10
(a) Intra-Continent Loss Rate
(b) Cross-Continent Loss Rate
Fig. 7. Continent Loss Rate Statistics. Numbers 0-5 represent 6 continents, respec-
tively. 0-Africa, 1-Asia, 2-Europe, 3-N.America, 4-Oceania, 5-S.America.
the performance of applications [13,14]. Constant eﬀorts are continuously being
pushed to improve the accuracy of packet loss rate estimation [12]. Tools [15] are
developed to use loss rate to troubleshoot path failures. All these work rely on
sending out active UDP/ICMP probes. Hence, they require controlling of either
one or both ends of the path being studied.
iPlane [1] is the only other tool close to be able to estimate packet loss rate
without requiring access to either end. It constructs an annotated map of the In-
ternet by: (i) sending probes from a large number of various vantage points, such
as PlanetLab nodes and traceroute servers; (ii) clustering interfaces into PoPs
based on response source address or returned TTLs to all the vantage points.
By collecting all probes and processing the measurement data, it characterizes
the loss rate of all inter-cluster links in the measured topology. Then, it may
indirectly predict packet loss rate between a pair of end hosts by compounding
the packet loss rate of each segment link along the path. However, iPlanes’s
coverage is limited as it can not provide packet loss rate on a path if neither
end of the path exists in the database. Thus, it doesn’t really provide loss rate
between arbitrary two end-hosts, as it still requires contributions from one end.
In addition, it does not perform measurement on demand.
7 Conclusion
In this paper, we presented Queen, a tool that estimates loss rate between ar-
bitrary Internet end hosts without control of either side. We validate Queen
with two diﬀerent data sets. They all show that our method has reasonably
high accuracy. We used Queen for an Internet-wide experiment, which provides
informative results and realistic Internet characteristics.
66
Y.A. Wang et al.
References
1. Madhyastha, H.V., Isdal, T., Piatek, M., Dixon, C.: iPlane: An Information Plane
for Distributed Services. In: USENIX OSDI (2006)
2. Gummadi, K.P., Saroiu, S., Gribble, S.D.: King: Estimating Latency between Ar-
bitrary Internet End Hosts. In: ACM SIGCOMM IMW (2002)
3. EdgePlatform, Akamai Inc.,
http://www.akamai.com/html/technology/index.html
4. Andersen, D.G., Balakrishnan, H., Kaashoek, M.F., Morris, R.: Resilient Overlay
Networks. In: ACM SOSP (2001)
5. Yang, H.-Y., Lee, K.-H., Ko, S.-J.: Communication quality of voice over TCP used
for ﬁrewall traversal. In: ICME (2008)
6. Huang, C., Wang, A., Li, J., Ross, K.W.: Understanding Hybrid CDN-P2P: Why
Limelight Needs Its Own RedSwoosh. In: NOSSDAV (2008)
7. Ng, T.S.E., Zhang, H.: Predicting Internet Network Distance with Coordinates-
Based Approaches. In: IEEE INFOCOM (2002)
8. Keynote Global Test and Measurement Network, Keynote Inc.,
http://www.keynote.com/company/keynote_network/methodology.html
9. Dabek, R., Cox, R., Kaashoek, M.R., Morris, R.: Vivaldi: A Decentralized Network
Coordinate System. In: ACM SIGCOMM (2004)
10. Ledlie, J., Gardner, P., Seltzer, M.: Network Coordinates in the Wild. In: USENIX
NSDI (2007)
11. White, B., et al.: An Integrated Experimental Environment for Distributed Systems
and Networks. In: USENIX OSDI (2002)
12. Sommers, J., Barford, P., Duﬃeld, N., Ron, A.: Improving Accuracy in End-to-end
Packet Loss Measurement. In: ACM SIGCOMM (2005)
13. Paxson, V.: End-to-end Routing Behavior in the Internet. In: ACM SIGCOMM
(1997)
14. Bolot, J.C.: End-to-end Packet Delay and Loss Behavior in the Internet. In: ACM
SIGCOMM (1993)
15. Mahajan, R., Spring, N., Wetherall, D., Anderson, T.: User-level Internet Path
Diagnosis. In: ACM SOSP (2003)