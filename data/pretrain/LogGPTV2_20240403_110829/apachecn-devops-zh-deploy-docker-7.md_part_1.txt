# 七、扩展的限制和解决方法
当您扩展您的系统时，您正在使用的每一个工具或框架都将达到崩溃的地步，或者只是不能按预期运行。对于某些事情来说，这个点会很高，而对于某些事情来说，这个点会很低，本章的目的是介绍在使用微服务集群时，您最有可能遇到的可伸缩性问题的策略和解决方法。在本章中，我们将涵盖以下主题:
*   提高服务密度和稳定性。
*   避免和减轻大规模部署的常见问题。
*   多服务容器。
*   零停机部署的最佳实践。
# 限制服务资源
到目前为止，我们还没有花任何时间讨论服务隔离与服务可用资源的关系，但是这是一个非常重要的话题。在不限制资源的情况下，恶意或行为不当的服务可能会导致整个集群崩溃，这取决于严重性，因此需要非常小心地指定各个服务任务应该使用的确切余量。
处理集群资源的普遍接受的策略如下:
*   如果超出预期值使用任何可能导致其他服务出错或失败的资源，强烈建议限制服务级别。这通常是内存分配，但可能包括中央处理器或其他。
*   任何有外部限制的资源，特别是硬件资源，也应该对 Docker 容器进行限制(例如，只允许您使用 1 Gbps NAS 连接的特定部分)。
*   任何需要在特定设备、机器或主机上运行的东西都应该以同样的方式锁定到这些资源。这种设置在只有一定数量的机器具有适合服务的硬件时非常常见，例如在 GPU 计算集群中。
*   您希望在群集内专门配给的任何资源通常都应该有一个应用限制。这包括降低低优先级服务的 CPU 时间百分比。
*   在大多数情况下，使用主机可用资源的正常分配，其余的资源应该没问题。
通过应用这些规则，我们将确保我们的集群更加稳定和安全，在服务之间精确地分配我们想要的资源。此外，如果指定了服务所需的确切资源，编排工具通常可以更好地决定在哪里调度新创建的任务，从而最大化每个引擎的服务密度。
# 内存限制
奇怪的是，尽管中央处理器可能被认为是最重要的计算资源，但集群服务的内存分配甚至更重要，因为内存过度使用会(并将)导致在同一台主机上运行的任何东西的**内存不足** ( **OOM** )进程和任务失败。随着软件内存泄漏的流行，这通常不是“如果”的问题，而是“何时”的问题，因此为内存分配设置限制通常是非常可取的，在某些编排配置中甚至是强制性的。遭受此问题通常通过在您的服务上看到`SIGKILL`、`"Process killed"`或`exit code -9`来表示。
Keep in mind, though, that these signals could very well be caused by other things but the most common cause is OOM failures.
通过限制可用内存，而不是主机上的随机进程被 OOM 管理器杀死，只有违规任务的进程将被作为杀死的目标，因此识别错误代码更加容易和快速，因为您可以看到该服务的大量故障，并且您的其他服务将保持运行，从而提高集群的稳定性。
OOM management is a huge topic and is much more broad than it would be wise to include in this section, but it is a very important thing to know if you spend a lot of time in the Linux kernel. If you are interested in this topic, I highly recommend that you visit [https://www.kernel.org/doc/gorman/html/understand/understand016.html](https://www.kernel.org/doc/gorman/html/understand/understand016.html) and read up on it. WARNING! On some of the most popular kernels, memory and/or swap cgroups are disabled due to their overhead. To enable memory and swap limiting on these kernels, your hosts kernel must be started with `cgroup_enable=memory` and `swapaccount=1` flags. If you are using GRUB for your bootloader, you can enable them by editing `/etc/default/grub` (or, on the latest systems, `/etc/default/grub.d/`), setting `GRUB_CMDLINE_LINUX="cgroup_enable=memory swapaccount=1"`, running `sudo update-grub`, and then restarting your machine.
要使用内存限制`cgroup`配置，请结合以下标志运行容器:
*   `-m` / `--memory`:对容器可以使用的最大内存量的硬性限制。超过这个限制的新内存分配将失败，内核将终止您的容器中的一个进程，该进程通常是运行服务的主要进程。
*   `--memory-swap`:容器可以使用的包括交换在内的内存总量。这必须与前一个选项一起使用，并且必须大于它。默认情况下，容器最多可以使用容器允许的最大内存量的两倍。将此设置为`-1`允许容器使用与主机一样多的交换空间。
*   `--memory-swappiness`:系统将多么渴望将页面从物理内存移动到磁盘上的交换空间。该值介于`0`和`100`之间，其中`0`表示页面将尽可能多地停留在常驻内存中，反之亦然。在大多数机器上，该值是`80`并将被用作默认值，但是由于与内存相比，交换空间访问非常慢，我的建议是将该值设置为尽可能接近`0`。
*   `--memory-reservation`:服务的 RAM 使用的软限制，通常仅用于检测与一般预期的 RAM 使用相关的资源争用，以便编排引擎可以调度任务以获得最大的使用密度。该标志不能保证将服务的内存使用率保持在此水平以下。
还有一些标志可以用于内存限制，但是即使是前面的列表也比您可能需要担心的要详细一些。对于大多数部署，无论大小，您可能只需要使用`-m`并设置一个低值`--memory-swappiness`，后者通常通过`sysctl.d`引导设置在主机上完成，以便所有服务都可以使用它。
You can check what your `swappiness` setting is by running `sysctl vm.swappiness`. If you would like to change this, and in most cluster deployments you will, you can set this value by running the following command:
`$ echo "vm.swappiness = 10" | sudo tee -a /etc/sysctl.d/60-swappiness.conf`
为了看到这一点，我们将首先运行一个资源最密集的框架(JBoss)，内存限制为 30 MB，看看会发生什么:
```
$ docker run -it \
             --rm \
             -m 30m \
             jboss/wildfly 
Unable to find image 'jboss/wildfly:latest' locally
latest: Pulling from jboss/wildfly
Status: Downloaded newer image for jboss/wildfly:latest
=========================================================================
 JBoss Bootstrap Environment
 JBOSS_HOME: /opt/jboss/wildfly
 JAVA: /usr/lib/jvm/java/bin/java
 JAVA_OPTS:  -server -Xms64m -Xmx512m -XX:MetaspaceSize=96M -XX:MaxMetaspaceSize=256m -Djava.net.preferIPv4Stack=true -Djboss.modules.system.pkgs=org.jboss.byteman -Djava.awt.headless=true
=========================================================================
*** JBossAS process (57) received KILL signal ***
```
不出所料，容器使用了太多的内存，很快就被内核杀死了。现在，如果我们尝试同样的东西，但给它 400 兆内存呢？
```
$ docker run -it \
             --rm \
             -m 400m \
             jboss/wildfly
=========================================================================
 JBoss Bootstrap Environment
 JBOSS_HOME: /opt/jboss/wildfly
 JAVA: /usr/lib/jvm/java/bin/java
 JAVA_OPTS:  -server -Xms64m -Xmx512m -XX:MetaspaceSize=96M -XX:MaxMetaspaceSize=256m -Djava.net.preferIPv4Stack=true -Djboss.modules.system.pkgs=org.jboss.byteman -Djava.awt.headless=true
=========================================================================
14:05:23,476 INFO  [org.jboss.modules] (main) JBoss Modules version 1.5.2.Final
14:05:25,568 INFO  [org.jboss.ws.common.management] (MSC service thread 1-6) JBWS022052: Starting JBossWS 5.1.5.Final (Apache CXF 3.1.6) 
14:05:25,667 INFO  [org.jboss.as] (Controller Boot Thread) WFLYSRV0060: Http management interface listening on http://127.0.0.1:9990/management
14:05:25,667 INFO  [org.jboss.as] (Controller Boot Thread) WFLYSRV0051: Admin console listening on http://127.0.0.1:9990
14:05:25,668 INFO  [org.jboss.as] (Controller Boot Thread) WFLYSRV0025: WildFly Full 10.1.0.Final (WildFly Core 2.2.0.Final) started in 2532ms - Started 331 of 577 services (393 services are lazy, passive or on-demand)
```
我们的容器现在可以开始没有任何问题！
如果您在裸机环境中处理过很多应用，您可能会问自己，为什么 JBoss JVM 不提前知道它不能在如此受限的环境中运行，甚至更快地失败。这里的答案在于`cgroups`的一个非常不幸的怪癖(尽管我认为这可能被认为是一个特性，取决于你的观点)，它将主机的资源原封不动地呈现给容器，即使容器本身是受约束的。如果您运行一个内存有限的容器并打印出可用的内存限制，您可以很容易地看到这一点:
```
$ # Let's see what a low allocation shows
$ docker run -it --rm -m 30m ubuntu /usr/bin/free -h
 total        used        free      shared  buff/cache   available
Mem:           7.6G        1.4G        4.4G         54M        1.8G        5.9G
Swap:            0B          0B          0B
$ # What about a high one?
$ docker run -it --rm -m 900m ubuntu /usr/bin/free -h
 total        used        free      shared  buff/cache   available
Mem:           7.6G        1.4G        4.4G         54M        1.8G        5.9G
Swap:            0B          0B          0B
```
可以想象，这将导致在这样一个`cgroup`有限容器中启动的应用出现各种各样的级联问题，主要原因是应用根本不知道存在限制，因此它将继续尝试完成它的工作，假设它可以完全访问可用的内存。一旦应用达到预定义的限制，应用进程通常会被终止，容器也会死亡。对于能够应对高内存压力的应用和运行时来说，这是一个巨大的问题，因为它们可能能够使用更少的容器内存，但是因为它们无法识别自己正在受限制地运行，所以它们往往会以比应该的更高的速度吞噬内存。
可悲的是，容器方面的情况更糟。您不仅必须给服务一个足够大的内存限制来启动它，而且还必须足够大，以便它可以在服务的整个持续时间内处理任何动态分配的内存。如果你不这样做，同样的情况会发生，但发生的时间不太可预测。例如，如果您运行一个只有 4 MB 内存限制的 NGINX 容器，它会很好地启动，但是在与它进行几次连接后，内存分配将超过阈值，容器将会死亡。然后，服务可能会重新启动任务，除非您有日志机制或者您的编排为其提供了良好的工具，否则您最终得到的服务将处于`running`状态，但实际上，它无法处理任何请求。
如果这还不够，你也真的不应该随意设定上限。这是因为容器的目的之一是最大化给定硬件配置的服务密度。通过设置统计上运行的服务几乎不可能达到的限制，您实际上是在浪费这些资源，因为它们不能被其他服务使用。从长远来看，这既增加了基础架构的成本，也增加了维护基础架构所需的资源，因此有很大的动机将服务限制在能够安全运行的最低数量，而不是使用非常高的限制。
Orchestration tooling generally prevents overcommiting resources, although there has been some progress to support this feature in both Docker Swarm and Kubernetes, where you can specify a soft limit (memory request) versus the true limit (memory limit). However, even with those parameters, tweaking the RAM setting is a really challenging task because you may get either under-utilization or constant rescheduling, so all the topics covered here are still very relevant. For more information on orchestration-specific handling of overcommiting, I suggest you read the latest documentation for your specific orchestration tool.
因此，当我们考虑所有我们必须记住的事情时，调整限制比其他任何事情都更接近于一种艺术形式，因为它几乎就像著名的装箱问题([https://en.wikipedia.org/wiki/Bin_packing_problem](https://en.wikipedia.org/wiki/Bin_packing_problem))的变体，但也在它之上添加了服务的统计部分，因为与由于限制宽松而浪费的资源相比，您可能需要找出最佳的服务可用性。
假设我们有一项服务，其分布如下:
*   三台物理主机，每台具有 2 GB 内存(是的，这确实很低，但这是为了在较小规模上演示问题)
*   **内存限制为 1.5 GB 的服务 1** (数据库)，两个任务，超过硬限制的概率为 1%
*   **内存限制为 0.5 GB 的服务 2** (应用)，三个任务，超过硬限制的几率为 5%
*   **内存限制为 0.5 GB 的服务 3** (数据处理服务)，三个任务，超过硬限制的几率为 5%
调度器可以以这种方式分配服务:
![](img/43987dbc-6d3b-4c01-aec6-d19e75bb6d22.png)
WARNING! You should always have spare capacity on your clusters for rolling service updates, so having the configuration similar to the one shown in the diagram would not work well in the real world. Generally, this extra capacity is also a fuzzy value, just like RAM limits. Generally, my formula for it is the following, but feel free to tweak it as needed:
`overcapacity = avg(service_sizes) * avg(service_counts) * avg(max_rolling_service_restarts)`
We will discuss this a bit more further in the text.
如果我们举最后一个例子，现在说我们应该以 1%的 OOM 失败率全面运行，将我们的**服务 2** 和**服务 3** 内存限制从 0.5 GB 增加到 0.75 GB，而不考虑最终用户可能可以接受数据处理服务和应用任务具有更高的失败率(或者如果您使用消息队列，甚至不会注意到这一点)，会怎么样？
新的服务传播现在看起来像这样:
![](img/522e6a43-0ee8-4202-992c-bc9d1fd69ff5.png)
我们的新配置有大量非常明显的问题:
*   服务密度降低 25%。这个数字应该尽可能高，以获得使用微服务的所有好处。
*   硬件利用率降低 25%。实际上，在此设置中，1/4 的可用硬件资源被浪费了。
*   节点数量增加了 66%。大多数云提供商根据您运行的机器数量收费，假设它们是同一类型的。通过进行这一更改，您有效地将云成本提高了 66%，并且可能需要额外的运营支持来保持您的集群正常工作。
尽管这个例子被故意操纵，以便在调整时产生最大的影响，但是很明显，对这些限制的微小改变会对您的整个基础设施产生巨大的影响。虽然在现实场景中，这种影响将会降低，因为与示例中相比，将会有更大的主机，这将使它们能够在可用空间中更好地堆叠更小(相对于总容量)的服务，*不要*低估了增加服务资源分配的级联效应。
# 中央处理器限制
就像我们前面关于服务内存限制的部分一样，`docker run`还支持多种 CPU 设置和参数来调整服务的计算需求:
*   `-c` / `--cpu-shares`:在高负载主机上，默认情况下所有任务权重相等。在任务或服务上设置此选项(从`1024`的默认值开始)将增加或减少任务可调度的 CPU 利用率百分比。
*   `--cpu-quota`:此标志设置任务或服务在 100 毫秒(100，000 微秒)的默认时间段内可以使用 CPU 的微秒数。例如，为了只允许一个任务最多占用单个中央处理器核心的 50%，您可以将该标志设置为`50000`。对于多核，您需要相应地增加该值。
*   `--cpu-period`:这将改变之前评估`cpu-quota`的配额标志默认间隔(以微秒为单位)(100 毫秒/100，000 微秒)，并减少或增加该间隔，从而反向影响服务的 CPU 资源分配。
*   `--cpus`:一个浮点值，结合了`cpu-quota`和`cpu-period`的部分内容，以限制分配给任务的 CPU 内核数量。例如，如果您只希望一个任务使用多达四分之一的单个 CPU 资源，您可以将其设置为`0.25`，它将具有与`--cpu-quota 25000 --cpu-period 100000`相同的效果。
*   `--cpuset-cpus`:此数组标志允许服务只在从 0 开始索引的指定 CPU 上运行。如果你想让一个服务只使用处理器 0 和 3，你可以使用`--cpuset-cpus "0,3"`。该标志还支持输入范围值(即`1-3`)。
虽然看起来需要考虑的选项很多，但在大多数情况下，您只需要调整`--cpu-shares`和`--cpus`标志，但您可能需要对它们提供的资源进行更精细的控制。