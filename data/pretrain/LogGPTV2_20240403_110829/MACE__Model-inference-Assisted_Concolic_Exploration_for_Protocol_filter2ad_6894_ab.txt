cation to the desired state, and then use the state-space
exploration component of MACE to explore the concrete
clusters of states.
which can result in inference of new transitions and
states. Black box inference algorithms, like L∗ [1], in-
fer a state machine over a ﬁxed-size alphabet by itera-
tively discovering new states. Such algorithms can be
used for the ﬁrst type of reﬁnement. Any traditional pro-
gram state-space exploration technique could be used to
discover new input (or output) messages, but adding all
the messages to the state machine’s alphabets would ren-
der the inference computationally infeasible. Thus, we
also wish to ﬁnd an effective way to reduce the size of
the alphabet, without missing states during the inference.
The constructed abstract model can guide the search in
many ways. The approach we take in this paper is to use
the abstract model to generate a sequence of inputs that
will drive the abstract model and the program to the de-
sired state. After the program reaches the desired state,
we explore the surrounding state-space using a combina-
tion of symbolic and concrete execution. Through such
exploration, we might visit numerous states that are all
abstracted with a single state in the abstract model and
discover new inputs that can reﬁne the abstract model.
Figure 1 illustrates the concept.
3.3 Preliminaries
Following our prior work [10], we use Mealy machines
[23] as abstract protocol models. Mealy machines are
natural models of protocols because they specify transi-
tion and output functions in terms of inputs. Mealy ma-
chines are deﬁned as follows:
Deﬁnition 1 (Mealy Machine). A Mealy machine, M, is
a six-tuple (Q, ΣI , ΣO, δ, λ, q0), where Q is a ﬁnite non-
empty set of states, q0 ∈ Q is the initial state, ΣI is a
ﬁnite set of input symbols (i.e., the input alphabet), ΣO is
a ﬁnite set of output symbols (i.e., the output alphabet),
δ : Q × ΣI −→ Q is the transition relation, and λ : Q ×
ΣI −→ ΣO is the output relation.
I , m ∈ ΣI ,
(resp. o).
∗
and
We extend the δ and λ relations to sequences of
messages m j ∈ ΣI as usual, e.g., δ (q, m0 · m1 · m2) =
δ (δ (δ (q, m0) , m1) , m2)
λ (q, m0 · m1 · m2) =
λ (q, m0) · λ (δ (q, m0) , m1) · λ (δ (q, m0 · m1) , m2). To
input (resp. output) messages
denote sequences of
we will use lower-case letters s,t
For
s ∈ Σ∗
the length |s| is deﬁned inductively:
|ε| = 0, |s · m| = |s| + 1, where ε is the empty sequence.
The j-th message m j in the sequence s = m0 · m1 · · · mn−1
will be referred to as s j. We deﬁne the support function
If for some state
machine M = (Q, ΣI, ΣO, δ, λ, q0) and some state q ∈ Q
there is s ∈ Σ∗
I such that δ(q0, s) = q, we say there is a
path from q0 to q, i.e., that q is reachable from the initial
−→ q. Since L∗ infers minimal state
state, denoted q0
machines, all states in the abstract model are reachable.
In general, each state could be reachable by multiple
paths. For each state q, we (arbitrary) pick one of the
shortest paths formed by a sequence of input messages
s−→ q, and call it a shortest transfer
s, such that q0
sequence.
sup as sup (s) = (cid:8)s j | 0 ≤ j < |s|(cid:9).
Our search process discovers numerous input and out-
put messages, and using all of them for the model in-
ference would not scale. Thus, we heuristically discard
redundant input messages, deﬁned as follows:
Deﬁnition 2 (Redundant Input Symbols). Let M =
(Q, ΣI, ΣO, δ, λ, q0) be a Mealy machine. A symbol m ∈
ΣI is said to be redundant if there exists another sym-
bol, m′ ∈ ΣI , such that m 6= m′ and ∀q ∈ Q . λ(q, m) =
λ(q, m′) ∧ δ(q, m) = δ(q, m′).
We say that a Mealy machine M = (Q, ΣI, ΣO, δ, λ, q0)
is complete iff δ(q, i) and λ(q, i) are deﬁned for every
q ∈ Q and i ∈ ΣI . In this paper, we infer complete Mealy
machines. There is also another type of completeness
— the completeness of the input and output alphabet.
MACE cannot guarantee that the input alphabet is com-
plete, meaning that it might not discover some types of
messages required to infer the full state machine of the
protocol.
To infer Mealy machines, we use Shahbaz and Groz’s
[26] variant of the classical L∗ [1] inference algorithm.
We describe only the intuition behind L∗, as the algo-
rithm is well-described in the literature.
L∗ is an online learning algorithm that proactively
probes a black box with sequences of messages, listens to
responses, and builds a ﬁnite state machine from the re-
sponses. The black box is expected to answer the queries
in a faithful (i.e., it is not supposed to cheat) and deter-
ministic way. Each generated sequence starts from the
initial state, meaning that L∗ has to reset the black box
before sending each sequence. Once it converges, L∗
conjectures a state machine, but it has no way to ver-
ify that it is equivalent to what the black box imple-
ments. Three approaches to solving this problem have
been described in the literature. The ﬁrst approach is to
assume an existence of an oracle capable of answering
the equivalence queries. L∗ asks the oracle whether the
conjectured state machine is equivalent to the one im-
plemented by the black box, and the oracle responds ei-
ther with ‘yes’ if the conjecture is equivalent, or with
a counterexample, which L∗ uses to reﬁne the learned
state machine and make another conjecture. The pro-
cess is guaranteed to terminate in time polynomial in
the number of states and the size of the input alphabet.
However, in practice, such an oracle is unavailable. The
second approach is to generate random sampling queries
and use those to test the equivalence between the con-
jecture and the black box. If a sampling query discovers
a mismatch between a conjecture and the black box, re-
ﬁnement is done the same way as with the counterexam-
ples that would be generated by equivalence queries. The
sampling approach provides a probabilistic guarantee [1]
on the accuracy of the inferred state machine. The third
approach, called black box model checking [24], uses
bounded model checking to compare the conjecture with
the black box.
As discussed in Section 3.1, MACE requires an out-
put message abstraction function αO : MO → ΣO, where
MO is the set of all concrete output messages, that ab-
stracts concrete output messages into the abstract output
alphabet. However, unlike the prior work [10], MACE
requires no input abstraction function. We will extend
the output abstraction function to sequences as follows.
Let o ∈ M ∗
O be a sequence of concrete output messages
such that |o| = n. The abstraction of a sequence is de-
ﬁned as αO(o) = αO(o0) · · · αO(on−1).
Seed messages
FSM
L∗
Shortest transfer
sequence
generator
Input seqs
State-space
explorer
Input
and output
sequences
Set of
input
messages
Filter
State-space
explorer
Figure 2: The MACE Approach Diagram. The L∗ algorithm takes in the input and output alphabets, over which it
infers a state-machine. L∗ sends queries and receives responses from the analyzed application, which is not shown in
the ﬁgure. The result of inference is a ﬁnite-state machine (FSM). For every state in the inferred state machine, We
generate a shortest transfer sequence (Section 3.3) that reaches the desired state, starting from the initial state. Such
sequences are used to initialize the state-space explorer, which runs dynamic symbolic execution after the initialization.
The state-space explorers run the analyzed application (not shown) in parallel.
4 Model-inference-Assisted Concolic
Exploration
We begin this section by a high-level description of
MACE, illustrated in Figure 2. After the high-level de-
scription, each section describes a major component of
MACE: abstract model inference, concrete state-space
exploration, and ﬁltering of redundant concrete input
messages together with the abstract model reﬁnement.
4.1 A High-Level Description
Suppose we want to infer a complete Mealy machine
M = (Q, ΣI, ΣO, δ, λ, q0) representing some protocol, as
implemented by the given program. We assume to know
the output abstraction function αO that abstracts con-
crete output messages into ΣO. To bootstrap MACE, we
also assume to have an initial set ΣI0 ⊆ ΣI of input mes-
sages, which can be extracted from either a regression
test suite, collected by observing the communication of
the analyzed program with the environment, or obtained
from DART and similar approaches [17, 25, 8, 7]. The
initial ΣI0 alphabet could be empty, but MACE would
take longer to converge. In our work, we used regression
test suites provided with the analyzed applications, or ex-
tracted messages from a single observed communication
session if the test suite was not available.
Next, L∗
infers the ﬁrst
state machine M0 =
(Q0, ΣI0, ΣO, δ0, λ0, q0
0) using ΣI0 and ΣO as the abstract
alphabets. In M0, we ﬁnd a shortest transfer sequence
from q0
0 to every state q ∈ Q0. We use such sequences
to drive the program to one of the concrete states repre-
sented by the abstract state q. Since each abstract state
could correspond to a large cluster of concrete states
(Fig. 1), we use dynamic symbolic execution to explore
the clusters of concrete states around abstract states.
The state-space exploration generates sequences of
concrete input and the corresponding output messages.
Using the output abstraction function αO, we can abstract
the concrete output message sequences into sequences
over Σ∗
O. However, we cannot abstract the concrete in-
put messages into a subset of ΣI , as we do not have the
concrete input message abstraction function. Using all
the concrete input messages for the L∗-based inference
would be computationally infeasible. The state-space
exploration discovers hundreds of thousands of concrete
messages, because we run the exploration phase for hun-
dreds of hours, and on average, it discovers several thou-
sand new concrete messages per hour.
Thus, we need a way to ﬁlter out redundant messages
and keep the ones that will allow L∗ to discover new
states. The ﬁltering is done as follows. Suppose that s
is a sequence of concrete input messages generated from
the exploration phase and o ∈ Σ∗
O a sequence of the corre-
sponding abstract output messages. If there exists t ∈ Σ∗
I0
such that M0 accepts t generating o, we discard s. Oth-
erwise, at least one concrete message in the s sequence
generates either a new state or a new transition, so we re-
ﬁne the input alphabet and compute ΣI1 = ΣI0 ∪ sup (s).
With the new abstract input alphabet ΣI1, we infer a
new, more reﬁned, abstract model M1 and repeat the pro-
cess. If the number of messages is ﬁnite and either the
exploration phase terminates or runs for a predetermined
bounded amount of time, MACE terminates as well.
4.2 Model Inference with L∗
MACE learns the abstract model of the analyzed pro-
gram by constructing sequences of input messages, send-
ing them to the program, and reasoning about the re-
sponses. For the inference, we use Shahbaz and Groz’s
[26] variant of L∗ for learning Mealy machines. The in-
ference process is similar as in our prior work [10].
In every iteration of MACE, L∗ infers a new state ma-
chine over ΣIi and the new messages discovered by the
state-space exploration guided by Mi, and conjectures
Mi+1, a reﬁnement of Mi. Out of the three options for
checking conjectures discussed in Section 3.3, we chose
to check conjectures using the sampling approach. We
could use sampling after each iteration, but we rather
defer it until the whole process terminates.
In other
words, rather than doing sampling after each iteration,
we use the subsequent MACE iterations instead of the
traditional sampling. Once the process terminates, we
generate sampling queries, but in no experiment we per-
formed did sampling discover any new states.
4.3 The State-Space Exploration Phase
We use the model inferred in Section 4.2 to guide the
state-space exploration. For every state qi ∈ Qi of the
just inferred abstract model Mi, we compute a shortest
transfer sequence of input messages from the initial state
0. Suppose the computed sequence is s ∈ Σ∗
qi
Ii. With
s, we drive the analyzed application to a concrete state
abstracted by the qi state in the abstract model. All mes-
sages sup (s) are concrete messages either from the set
of seed messages, or generated by previous state-space
exploration iterations. Thus, the process of driving the
analyzed application to the desired state consists of only
computing a shortest path in Mi to the state, collecting
+
the input messages along the path qi
−→ qi, and feeding
0
that sequence of concrete messages into the application.
Once the application is in the desired state qi, we
run dynamic symbolic execution from that state to ex-
plore the surrounding concrete states (Figure 1). In other
words, the transfer sequence of input messages produces
a concrete run, which is then followed by symbolic ex-
ecution that computes the corresponding path-condition.
Once the path-condition is computed, dynamic symbolic
execution resumes its normal exploration. We bound
the time allotted to exploring the vicinity of every ab-
stract state. In every iteration, we explore only the newly
discovered states, i.e., Qi\Qi−1. Re-exploring the same
states over and over would be unproductive.
Thanks to the abstract model, MACE can easily com-
pute the necessary input message permutations required
to reach any abstract model state, just by computing a
shortest path. On the other hand, approaches that com-
bine concrete and symbolic execution have to negate
multiple predicates and get the decision procedure to
generate the required sequence of concrete input mes-
sages to get to a particular state. MACE has more con-
trol over this process, and our experimental results show
that the increased control results in higher line coverage,
deeper analysis, and more vulnerabilities found.
4.4 Model Reﬁnement
The exploration phase described in Section 4.3 generates
a large number (hundreds of thousands in our setting) of
new concrete messages. Using all of them to reﬁne the
abstract model is both unrealistic, as inference is polyno-
mial in the size of the alphabet, and redundant, as many
messages are duplicates and belong to the same equiv-
alence class. To reduce the number of input messages
used for inference, Comparetti et al. [12] propose a mes-
sage clustering technique, while we used a handcrafted
an abstraction function in our prior work. In this paper,
we take a different approach.
In the spirit of dynamic symbolic execution, the explo-
ration phase solves the path-condition (using a decision
procedure) to generate new concrete inputs, more pre-
cisely, sequences of concrete input messages. During the
concrete part of the exploration phase, such sequences
of input messages are executed concretely, which gen-
erates the corresponding sequence of output messages.
We abstract the generated sequence of output messages
using αO. If the abstracted sequence can be generated
by the current abstract model, we discard the sequence,
otherwise we add all the corresponding concrete input
messages to ΣIi. We deﬁne this process more formally:
Deﬁnition 3 (Filter Function). Let MI (resp. MO) be
a (possibly inﬁnite) set of all possible concrete input
(resp. output) messages. Let s ∈ M ∗
(resp. o ∈ M ∗
O)
I
be a sequence of concrete input (resp. output) messages
such that |s| = |o|. We assume that each input message
s j produces o j as a response. Let Mi ∈ A be the ab-
stract model inferred in the last iteration and A the uni-
verse of all possible Mealy machines. The ﬁlter function
f : A × M ∗
O → 2MI is deﬁned as follows:
I × M ∗
f (Mi, s, o) =(cid:26) /0
sup (s)
if
∃t ∈ Σ∗
Ii . λi(t) = αO(o)
otherwise
In practice, a single input message could produce ei-
ther no response or multiple output messages.
In the
ﬁrst case, our implementation generates an artiﬁcial no-
response message, and in the second case, it picks the
ﬁrst produced output message. A more advanced im-
plementation could infer a subsequential transducer [28],