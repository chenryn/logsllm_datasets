title:Good Bot, Bad Bot: Characterizing Automated Browsing Activity
author:Xigao Li and
Babak Amin Azad and
Amir Rahmati and
Nick Nikiforakis
Good Bot, Bad Bot:
Characterizing Automated Browsing Activity
Xigao Li
Stony Brook University
Babak Amin Azad
Stony Brook University
Amir Rahmati
Nick Nikiforakis
Stony Brook University
Stony Brook University
Abstract—As the web keeps increasing in size, the number
of vulnerable and poorly-managed websites increases commensu-
rately. Attackers rely on armies of malicious bots to discover these
vulnerable websites, compromising their servers, and exﬁltrating
sensitive user data. It is, therefore, crucial for the security of the
web to understand the population and behavior of malicious bots.
In this paper, we report on the design, implementation, and
results of Aristaeus, a system for deploying large numbers of
“honeysites”, i.e., websites that exist for the sole purpose of attract-
ing and recording bot trafﬁc. Through a seven-month-long exper-
iment with 100 dedicated honeysites, Aristaeus recorded 26.4 mil-
lion requests sent by more than 287K unique IP addresses, with
76,396 of them belonging to clearly malicious bots. By analyzing
the type of requests and payloads that these bots send, we discover
that the average honeysite received more than 37K requests each
month, with more than 50% of these requests attempting to brute-
force credentials, ﬁngerprint the deployed web applications, and
exploit large numbers of different vulnerabilities. By comparing
the declared identity of these bots with their TLS handshakes
and HTTP headers, we uncover that more than 86.2% of bots
are claiming to be Mozilla Firefox and Google Chrome, yet are
built on simple HTTP libraries and command-line tools.
I. INTRODUCTION
To cope with the rapid expansion of the web, both legitimate
operators, as well as malicious actors, rely on web bots (also
known as crawlers and spiders) to quickly and autonomously
discover online content. Legitimate services, such as search
engines, use bots to crawl websites and power their products.
Malicious actors also rely on bots to perform credential stufﬁng
attacks, identify sensitive ﬁles that are accidentally made public,
and probe web applications for known vulnerabilities [1], [2].
According to recent industry reports [3], bots are responsible
for 37.2% of the total website-related trafﬁc, with malicious
bots being responsible for 64.7% of the overall bot trafﬁc.
Given the abuse perpetrated by malicious bots, identifying
and stopping them is critical for the security of websites and
their users. Most existing bot-detection techniques rely on
differentiating bots from regular users, through supervised
and unsupervised ML techniques, based on features related
to how clients interact with websites (e.g., the speed and
type of resources requested) [4]–[6] as well as through
browser-ﬁngerprinting techniques [7], [8].
In all of the aforementioned approaches, researchers need
to obtain a ground truth dataset of known bots and known
users to train systems to differentiate between them. This
requirement creates a circular dependency where one needs
a dataset resulting from accurate bot detection to be used for
accurate-bot detection. The adversarial nature of malicious bots,
their ability to claim arbitrary identities (e.g., via User-agent
header spooﬁng), and the automated or human-assisted solving
of CAPTCHAs make this a challenging task [9]–[11].
In this paper, we present a technique that sidesteps the issue
of differentiating between users and bots through the concept
of honeysites. Like traditional high-interaction honeypots, our
honeysites are fully functional websites hosting full-ﬂedged
web applications placed on public IP address space (similar
to Canali and Balzarotti’s honeypot websites used to study the
exploitation and post-exploitation phases of web-application
attacks [12]). By registering domains that have never existed
before (thereby avoiding trafﬁc due to residual trust [13]) and
never advertising these domains to human users, we ensure
that any trafﬁc received on these honeysites will belong to
benign/malicious bots and potentially their operators. To scale
up this idea, we design and build Aristaeus,1 a system that
provides ﬂexible remote deployment and management of
honeysites while augmenting the deployed web applications
with multiple vectors of client ﬁngerprinting.
Using Aristaeus, we deploy 100 honeysites across the
globe, choosing ﬁve open-source web applications (WordPress,
Joomla, Drupal, PHPMyAdmin, and Webmin), which are
both widely popular and have been vulnerable to hundreds of
historical vulnerabilities, thereby making them attractive targets
for malicious bots. In a period of seven months (January 24 to
August 24, 2020) Aristaeus recorded 26.4 million bot requests
from 287,017 unique IP addresses, totaling more than 200 GB
of raw logs from websites that have zero organic user trafﬁc.
By analyzing the received trafﬁc, we discovered that from
the 37,753 requests that
the average Aristaeus-managed
honeysite received per month, 21,523 (57%) were clearly
malicious. Among others, we observed 47,667 bots sending
unsolicited POST requests towards the login endpoints of our
deployed web applications, and uncovered 12,183 unique bot
IP addresses which engaged in web-application ﬁngerprinting.
In the duration of our experiment, we observed the attempt to
exploit ﬁve new high-severity vulnerabilities, witnessing bots
weaponizing an exploit on the same day that it became public.
Furthermore, by analyzing the collected ﬁngerprints and
attempting to match them with known browsers and automation
tools, we discovered that at least 86.2% of bots are lying
about their identity, i.e., their stated identity does not match
their TLS and HTTP-header ﬁngerprints. Speciﬁcally, out
1Rustic god in Greek mythology caring over, among others, beekeepers.
of the 30,233 clients, which claimed to be either Chrome or
Firefox, we found that 86.2% are lying, with most matching
the ﬁngerprints of common Go and Python HTTP libraries as
well as scriptable command-line tools (such as wget and curl).
Our main contributions are as follows:
• We design and implement Aristaeus, a system for
deploying and managing honeysites. Using Aristaeus, we
deploy 100 honeysites across the globe, obtaining unique
insights into the populations and behavior of benign and
malicious bots.
• We extract URLs from exploit databases and web-
application ﬁngerprinting tools and correlate them with
the requests recorded by Aristaeus, discovering that more
than 25,502 bots engage in either ﬁngerprinting, or the
exploitation of high-severity, server-side vulnerabilities.
• We curate TLS signatures of common browsers and
automation tools, and use them to uncover the true
identity of bots visiting our infrastructure. We ﬁnd that
the vast majority of bots are built using common HTTP
libraries but claim to be popular browsers. Our results
demonstrate the effectiveness of TLS ﬁngerprinting for
identifying and differentiating users from malicious bots.
Given the difﬁculty of differentiating between users and
bots on production websites, we will be sharing our curated,
bot-only dataset with other researchers to help them improve
the quality of current and future bot-detection tools.
II. BACKGROUND
Unsolicited requests have become a ﬁxture of the web-
hosting experience. These requests usually originate from bots
with various benign and malicious intentions. On the benign
side, search-engine bots crawl websites to index content for
their services, while large-scale research projects use bots to
collect general statistics. At the same time, malicious actors
use bots to identify and exploit vulnerabilities on a large scale.
Moreover, high-proﬁle websites are victims of targeted bot
attacks that seek to scrape their content and target user accounts.
Bots and automated browsing. An automated browsing
environment can be as rudimentary as wget or curl requests, or
be as involved as full browsers, controlled programmatically
through libraries such as Selenium [14]. The underlying bot
platforms and their conﬁguration deﬁnes the capabilities of
a bot in terms of loading and executing certain resources such
as JavaScript code, images, and Cascading Style Sheets (CSS).
As we show in this paper, the capabilities and behavior of
these platforms can be used to identify them.
Browser ﬁngerprinting. Malicious bots can lie about their
identity. Prior work has proposed detection schemes based on
browser ﬁngerprinting and behavioral analysis to extract static
signatures as well as features that can be used in ML models.
Browser ﬁngerprinting is an integral part of bot detection.
Previous research has focused on many aspects of browsing
environments that make them unique [15]–[17]. The same
techniques have also been used for stateless user tracking by
ad networks, focusing on features that are likely to produce
different results for different users, such as, the supported
JavaScript APIs,
available fonts, and canvas renderings [15], [18], [19].
list of plugins and browser extensions,
to
browser
Behavioral
analysis. Next
TLS ﬁngerprinting. Similarly, the underlying browser and
operating systems can be ﬁngerprinted at the network layer
by capitalizing on the TLS differences between browsers
and environments [20]. Durumeric et al. used discrepancies
between the declared user-agent of a connection and the used
TLS handshake to identify HTTPS interception [21]. In this
paper, we show how TLS signatures (consisting of TLS version,
list of available cipher suites, signature algorithms, e-curves,
and compression methods) can be used to identify the true
nature of malicious bots, regardless of their claimed identities.
and TLS
ﬁngerprinting, the behavior of bots on the target website can
signal the presence of automated browsing. To that end, features
such as the request rate, requests towards critical endpoints
(e.g., login endpoint), and even mouse moves and keystrokes
have been used by prior bot-detection schemes [4], [5], [22].
Browsing sessions. To study the behavior of bots, we need
a mechanism to group together subsequent requests from the
same bot. While source IP address can be used for that purpose,
in a large-scale study, the IP churn over time can result in
false positives where an address changes hands and ends up
being used by different entities. To address this issue, we used
the notion of “browsing sessions” as used by server-side web
applications and also deﬁned by Google Analytics [23]. For
each IP address, we start a session upon receiving a request and
end it after 30 minutes of inactivity. This allows for an organic
split of the active browsing behavior into groups. Grouping
requests from the same bot in a session enables us to analyze
activities, such as, changes in a bot’s claimed identity and mul-
tiple requests that are part of a credential, brute-forcing attack.
III. SYSTEM DESIGN
To collect global bot information, we design Aristaeus, a
system that provides ﬂexible honeysite deployment and ﬁnger-
print collection. Aristaeus consists of three parts: honeysites,
log aggregation, and analysis modules. Our system can launch
an arbitrary number of honeysites based on user-provided
templates, i.e., sets of existing/custom-made web applications
and scripts using virtual machines on public clouds. Aristaeus
augments these templates with multiple ﬁngerprinting modules
that collect a wide range of information for each visiting client.
The information collected from honeysites is periodically
pulled by a central server, which is responsible for correlating
and aggregating the data collected from all active honeysites.
Figure 1 presents the overall architecture of our system.
A. Honeysite Design
A honeysite is a real deployment of a web application,
augmented with different ﬁngerprinting techniques, and
increased logging. Like traditional honeypots, our honeysites
are never advertised to real users, nor linked to by other
sites or submitted to search engines for listing. If a honeysite
includes publicly-accessible, user-generated content (such as a
typical CMS showing blog posts), Aristaeus creates randomly-
generated text and populates the main page of the honeysite.
Fig. 1: High-level overview of the architecture of Aristaeus
Lastly, to ensure that the trafﬁc a honeysite is receiving is
not because its domain name used to exist (and therefore
there are residual-trust and residual-trafﬁc issues associated
with it [13]) all domains that we register for Aristaeus, were
never registered before. To ensure this, we used a commercial
passive-DNS provider and searched for
the absence of
historical resolutions before registering a domain name. As a
result, any trafﬁc that we observe on a honeysite can be safely
characterized as belonging either to bots, or to bot-operators
who decided to investigate what their bots discovered.
To be able to later correlate requests originating from
different IP addresses as belonging to the same bot that changed
its IP address or from a different deployment of the same
automated-browsing software, Aristaeus augments honeysites
with three types of ﬁngerprinting: browser ﬁngerprinting,
behavior ﬁngerprinting, and TLS ﬁngerprinting. Figure 2
shows how these different types of ﬁngerprinting interface with
a honeysite. We provide details and the rationale about each
of these ﬁngerprinting modules in the following paragraphs:
1) Browser ﬁngerprinting
To ﬁngerprint each browser (or automated agent) that
requests content from our honeysites, we use the following
methods.
JavaScript API support. Different bots have different
capabilities. To identify a baseline of supported features of
their JavaScript engine, we dynamically include an image using
document.write() and var img APIs, and verify whether the con-
necting agent requests that resource. We also check for AJAX
support by sending POST requests from the jQuery library that
is included on the page. Lastly, we use a combination of inline
JavaScript and external JavaScript ﬁles to quantify whether
inline and externally-loaded JavaScript ﬁles are supported.
Browser ﬁngerprinting through JavaScript. We utilize
the Fingerprintjs2 (FPJS2) library [24] to analyze the ﬁnger-
printable surface of bots. FPJS2 collects 35 features from web
browsers, including information about the screen resolution,
time zone, creation of canvas, webGL, and other features that
can be queried through JavaScript. To do that, FPJS2 relies on
a list of JavaScript APIs. Even though in a typical browsing
environment these APIs are readily available, there are no
guarantees that all connecting clients have complete JavaScript
support. For instance, the WeChat embedded browser is unable
to execute OfﬂineAudioContext from FPJS2, breaking the entire
ﬁngerprinting script. To address this issue, we modularized
FPJS2 in a way that the library would survive as many failures
as possible but still collect values for the APIs that are available.
Naturally, if a bot does not support JavaScript at all, we will
not be able to collect any ﬁngerprints from it using this method.
Support for security policies. Modern browsers support a
gamut of security mechanisms, typically controlled via HTTP
response headers, to protect users and web applications against
attacks. Aristaeus uses these mechanisms as a novel ﬁngerprint-
ing technique, with the expectation that different bots exhibit
different levels of security-mechanism support. Speciﬁcally, we
test the support of a simple Content Security Policy and the
enforcement of X-Frame-Options by requesting resources that
are explicitly disallowed by these policies [25], [26]. To the best
of our knowledge, this is the ﬁrst time these mechanisms have
been used for a purpose other than securing web applications.
2) Behavior ﬁngerprinting
it
Honoring robots.txt. Benign bots are expected to follow
the directives of robots.txt (i.e., do not visit the paths explicitly
marked with Disallow). Contrastingly,
is well known
that malicious bots can not only ignore these Disallow
directives but also use robots.txt to identify end-points that
they would have otherwise missed [27]. To test this, Aristaeus
automatically appends to the robots.txt ﬁles of each honeysite
Disallow entries to a “password.txt” ﬁle, whose exact path
encodes the IP address of the agent requesting that ﬁle. This
enables Aristaeus to not only discover abuses of robots.txt but
also identify a common bot operator behind two seemingly
unrelated requests. That is, if a bot with IP address 5.6.7.8
requests a robots.txt entry that was generated for a different
bot with IP address 1.2.3.4, we can later deduce that both of
these requests belonged to the same operator.
Customized error pages. To ensure that Aristaeus gets
a chance to ﬁngerprint bots that are targeting speciﬁc web
applications (and therefore request resources that generate
HTTP 404 messages), we use custom 404 pages that incorporate
all of the ﬁngerprinting techniques described in this section.
Caching and resource sharing. Regular browsers use
caching to decrease the page load time and load resources
that are reused across web pages more efﬁciently. The use
1. Deploy honeysitesAutomated browsers2. Log aggregation3. Bot trafﬁc analysisTLSApacheCSPJSLog	SourcesHoneysitesAristaeus	Control	CenterWebsite	TemplatesDomain	Registration	ModuleName	ServersBotsScripts & CMD toolsOtherbotsLog	AggregationLog	CorrelationQuerying	PanelSession	GenerationBot signaturesTrafﬁc analysisAnalysis	EngineNew	HoneysitesCurrent	HoneysitesFig. 2: Internal design of each honeysite.
of caching can complicate our analysis of logs as we rely
on HTTP requests to identify which features are supported
by bots. To eliminate these effects, we use the “no-cache”
header to ask agents not to cache resources, and we append
a dynamic, cache-breaking token at the end of the URLs for
each resource on a page.
Cache breakers are generated by encrypting the client IP
address and a random nonce using AES and then base64-
encoding the output; this makes every URL to the same
resource unique. For example, if a bot asks for the resource
a.jpg, it will send a request in the format /a.jpg?r=[endoded
IP+nonce]. During our analysis, we decrypt
the cache
breakers and obtain the original IP address that requested
these resources. Using this information, we can pinpoint any
resource sharing that occurred across multiple IP addresses.
This happens when a resource is requested from one IP
address but the cache breaker points to a different IP address.
3) TLS ﬁngerprinting
We use the ﬁngeprinTLS [20] (FPTLS) library to collect
TLS handshake ﬁngerprints from the underlying TLS library
of each client that connects to our honeysites over the HTTPS