{"project_name":"","application_name":null,"provider":"","product":"","component":"kube-dns","node":"172.253.52.203","nodes":"172.253.52.203","region_name":"k8s-overlay","region_id":"donotcare","log_data":"[ERROR] plugin/errors: 2 api.dev.qta.oa.com. A: concurrent queries exceeded maximum 1000\n","log_level":"0","paths":"stdout","file_name":"stdout","time":1676315321090572,"root_account":"alauda","source":"container","log_type":"log","kubernetes_labels":{"k8s-app":"kube-dns","pod-template-hash":"77b57ccb9f"},"kubernetes_namespace":"kube-system","pod_name":"coredns-77b57ccb9f-zxwbh","pod_id":"49b5f1c6-d4c4-4b42-a8be-757c3b90c151","container_id":"ad1b0b58af595b4170baf4470601a28e6bb92a58c55b7fb3f34b2200ff0d84f7","container_id8":"ad1b0b58","docker_container_name":"coredns","kubernetes_container_name":"coredns"}
{"project_name":"","application_name":null,"provider":"","product":"","component":"etcd","node":"172.253.52.203","nodes":"172.253.52.203","region_name":"k8s-overlay","region_id":"donotcare","log_data":"2023-02-13 19:08:41.436551 I | etcdserver/api/etcdhttp: /health OK (status code 200)\n","log_level":"0","paths":"stdout","file_name":"stdout","time":1676315321436710,"root_account":"alauda","source":"container","log_type":"log","kubernetes_labels":{"component":"etcd","tier":"control-plane"},"kubernetes_namespace":"kube-system","pod_name":"etcd-172.253.52.203","pod_id":"0efb5ecb-513a-4312-bbc1-5c4ab45ea368","container_id":"956e7f52f51204139c1ada5497dfee659645cb784525ff5050ab05ca12f58755","container_id8":"956e7f52","docker_container_name":"etcd","kubernetes_container_name":"etcd"}]
[{"project_name":"","application_name":null,"provider":"","product":"","component":"kube-apiserver","node":"172.253.52.201","nodes":"172.253.52.201","region_name":"k8s-overlay","region_id":"donotcare","log_data":"E0213 19:08:32.904349       1 authentication.go:53] Unable to authenticate the request due to an error: x509: certificate has expired or is not yet valid: current time 2023-02-13T19:08:32Z is after 2022-07-15T07:33:41Z\n","log_level":"0","paths":"stdout","file_name":"stdout","time":1676315312904631,"root_account":"alauda","source":"container","log_type":"log","kubernetes_labels":{"component":"kube-apiserver","tier":"control-plane"},"kubernetes_namespace":"kube-system","pod_name":"kube-apiserver-172.253.52.201","pod_id":"2178fb34-98b8-4774-830e-c0e49dc0361a","container_id":"acf068f23944191f4396a5280d5ed38658f4fa776098ed2bbccb1b8c42590284","container_id8":"acf068f2","docker_container_name":"kube-apiserver","kubernetes_container_name":"kube-apiserver"}
{"project_name":"","application_name":null,"provider":"","product":"","component":"kube-scheduler","node":"172.253.52.201","nodes":"172.253.52.201","region_name":"k8s-overlay","region_id":"donotcare","log_data":"E0213 19:08:32.904764       1 leaderelection.go:325] error retrieving resource lock kube-system/kube-scheduler: Unauthorized\n","log_level":"0","paths":"stdout","file_name":"stdout","time":1676315312904859,"root_account":"alauda","source":"container","log_type":"log","kubernetes_labels":{"component":"kube-scheduler","tier":"control-plane","prometheus&io/port":"10251","prometheus&io/scheme":"http","scheduler&alpha&kubernetes&io/critical-pod":"","tke&prometheus&io/scrape":"true"},"kubernetes_namespace":"kube-system","pod_name":"kube-scheduler-172.253.52.201","pod_id":"ba113583-00a5-4c25-a735-0d58d0ad7adb","container_id":"d935ba2bcddfb8da4ec3a09bbc0952baa4f605ff9f5724f2e32cbbe9034f1705","container_id8":"d935ba2b","docker_container_name":"kube-scheduler","kubernetes_container_name":"kube-scheduler"}
{"node":"172.253.52.201","region_name":"k8s-overlay","region_id":"donotcare","log_data":"Feb 14 03:08:33 k8s-overlay-master01 kubelet: E0214 03:08:33.815965   24409 kubelet_volumes.go:179] orphaned pod \"c6beaf56-a22a-419f-9540-1b72dc5646eb\" found, but failed to rmdir() subpath at path /var/lib/kubelet/pods/c6beaf56-a22a-419f-9540-1b72dc5646eb/volume-subpaths/docker-daemon/metis/0: not a directory : There were a total of 1 errors similar to this. Turn up verbosity to see them.","log_level":"0","file_name":"messages.log","paths":"/var/log/messages.log","time":1676315314210802,"@timestamp":"2023-02-13T19:08:34.211216Z","root_account":"alauda","source":"host","log_type":"file"}
{"project_name":"","application_name":null,"provider":"","product":"","component":"kube-controller-manager","node":"172.253.52.201","nodes":"172.253.52.201","region_name":"k8s-overlay","region_id":"donotcare","log_data":"E0213 19:08:34.177734       1 leaderelection.go:325] error retrieving resource lock kube-system/kube-controller-manager: Unauthorized\n","log_level":"0","paths":"stdout","file_name":"stdout","time":1676315314177836,"root_account":"alauda","source":"container","log_type":"log","kubernetes_labels":{"component":"kube-controller-manager","tier":"control-plane","prometheus&io/port":"10252","prometheus&io/scheme":"http","scheduler&alpha&kubernetes&io/critical-pod":"","tke&prometheus&io/scrape":"true"},"kubernetes_namespace":"kube-system","pod_name":"kube-controller-manager-172.253.52.201","pod_id":"88844f7e-47be-4168-9008-5cf35e444c30","container_id":"45416b0a41b75cde8e5880cc5a7524546177dd9e383a430e4ceaca4f7b7160b0","container_id8":"45416b0a","docker_container_name":"kube-controller-manager","kubernetes_container_name":"kube-controller-manager"}
{"project_name":"","application_name":null,"provider":"","product":"","component":"etcd","node":"172.253.52.201","nodes":"172.253.52.201","region_name":"k8s-overlay","region_id":"donotcare","log_data":"2023-02-13 19:08:34.465176 I | etcdserver/api/etcdhttp: /health OK (status code 200)\n","log_level":"0","paths":"stdout","file_name":"stdout","time":1676315314465350,"root_account":"alauda","source":"container","log_type":"log","kubernetes_labels":{"component":"etcd","tier":"control-plane"},"kubernetes_namespace":"kube-system","pod_name":"etcd-172.253.52.201","pod_id":"5251121d-e974-4980-88e7-a21193845887","container_id":"a7d541960026c6d06c4cc108fd90d8e1f00c82e1c533a3d03c2bb327b0c3c10e","container_id8":"a7d54196","docker_container_name":"etcd","kubernetes_container_name":"etcd"}
{"project_name":"","application_name":null,"provider":"","product":"","component":"kube-apiserver","node":"172.253.52.201","nodes":"172.253.52.201","region_name":"k8s-overlay","region_id":"donotcare","log_data":"E0213 19:08:34.177328       1 authentication.go:53] Unable to authenticate the request due to an error: x509: certificate has expired or is not yet valid: current time 2023-02-13T19:08:34Z is after 2022-07-15T07:33:41Z\n","log_level":"0","paths":"stdout","file_name":"stdout","time":1676315314177468,"root_account":"alauda","source":"container","log_type":"log","kubernetes_labels":{"component":"kube-apiserver","tier":"control-plane"},"kubernetes_namespace":"kube-system","pod_name":"kube-apiserver-172.253.52.201","pod_id":"2178fb34-98b8-4774-830e-c0e49dc0361a","container_id":"acf068f23944191f4396a5280d5ed38658f4fa776098ed2bbccb1b8c42590284","container_id8":"acf068f2","docker_container_name":"kube-apiserver","kubernetes_container_name":"kube-apiserver"}
{"node":"172.253.52.201","region_name":"k8s-overlay","region_id":"donotcare","log_data":"Feb 14 03:08:35 k8s-overlay-master01 kubelet: E0214 03:08:35.838372   24409 kubelet_volumes.go:179] orphaned pod \"c6beaf56-a22a-419f-9540-1b72dc5646eb\" found, but failed to rmdir() subpath at path /var/lib/kubelet/pods/c6beaf56-a22a-419f-9540-1b72dc5646eb/volume-subpaths/docker-daemon/metis/0: not a directory : There were a total of 1 errors similar to this. Turn up verbosity to see them.","log_level":"0","file_name":"messages.log","paths":"/var/log/messages.log","time":1676315316210859,"@timestamp":"2023-02-13T19:08:36.212083Z","root_account":"alauda","source":"host","log_type":"file"}]
[{"node":"172.253.52.102","region_name":"k8s-overlay","region_id":"donotcare","log_data":"Feb 14 03:08:29 k8s-storage-node02 kubelet: E0214 03:08:29.219978    2044 pod_workers.go:191] Error syncing pod 87d6804a-2e49-4861-adcb-28f898ce823f (\"rook-ceph-osd-6-9476c8db-hb4zt_rook-ceph(87d6804a-2e49-4861-adcb-28f898ce823f)\"), skipping: failed to \"StartContainer\" for \"expand-bluefs\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=expand-bluefs pod=rook-ceph-osd-6-9476c8db-hb4zt_rook-ceph(87d6804a-2e49-4861-adcb-28f898ce823f)\"","log_level":"0","file_name":"messages.log","paths":"/var/log/messages.log","time":1676315310193128,"@timestamp":"2023-02-13T19:08:30.193529Z","root_account":"alauda","source":"host","log_type":"file"}
{"node":"172.253.52.102","region_name":"k8s-overlay","region_id":"donotcare","log_data":"Feb 14 03:08:29 k8s-storage-node02 kubelet: E0214 03:08:29.220068    2044 pod_workers.go:191] Error syncing pod cb0a74af-d711-4dde-81a2-25ba6c3e8d59 (\"rook-ceph-osd-7-689c6676c6-4crtp_rook-ceph(cb0a74af-d711-4dde-81a2-25ba6c3e8d59)\"), skipping: failed to \"StartContainer\" for \"expand-bluefs\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=expand-bluefs pod=rook-ceph-osd-7-689c6676c6-4crtp_rook-ceph(cb0a74af-d711-4dde-81a2-25ba6c3e8d59)\"","log_level":"0","file_name":"messages.log","paths":"/var/log/messages.log","time":1676315310193150,"@timestamp":"2023-02-13T19:08:30.193766Z","root_account":"alauda","source":"host","log_type":"file"}
{"project_name":"cpaas-system","application_name":null,"provider":"","product":"","component":"","node":"172.253.52.102","nodes":"172.253.52.102","region_name":"k8s-overlay","region_id":"donotcare","log_data":"cluster 2023-02-13 19:08:23.863684 osd.26 (osd.26) 38396 : cluster [DBG] 3.60 repair ok, 0 fixed\n","log_level":"0","paths":"stdout","file_name":"stdout","time":1676315309457545,"root_account":"alauda","source":"container","log_type":"log","kubernetes_labels":{"app":"rook-ceph-mon","ceph_daemon_id":"f","ceph_daemon_type":"mon","mon":"f","mon_cluster":"rook-ceph","pod-template-hash":"745c94785f","rook_cluster":"rook-ceph"},"kubernetes_namespace":"rook-ceph","pod_name":"rook-ceph-mon-f-745c94785f-w5fnr","pod_id":"eac4f08b-097d-4551-9ede-d3a83d34c41c","container_id":"b69ebb392c79698e8b2ee1da6d62f0bc0dc1d05ab75705c62c90b21c2af4b59e","container_id8":"b69ebb39","docker_container_name":"mon","kubernetes_container_name":"mon"}
{"node":"172.253.52.102","region_name":"k8s-overlay","region_id":"donotcare","log_data":"Feb 14 03:08:30 k8s-storage-node02 kubelet: E0214 03:08:30.219812    2044 pod_workers.go:191] Error syncing pod 0ae1012c-15ae-46fd-b379-25aefda72e24 (\"rook-ceph-osd-9-6954778db8-g5r7l_rook-ceph(0ae1012c-15ae-46fd-b379-25aefda72e24)\"), skipping: failed to \"StartContainer\" for \"expand-bluefs\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=expand-bluefs pod=rook-ceph-osd-9-6954778db8-g5r7l_rook-ceph(0ae1012c-15ae-46fd-b379-25aefda72e24)\"","log_level":"0","file_name":"messages.log","paths":"/var/log/messages.log","time":1676315311192847,"@timestamp":"2023-02-13T19:08:31.193271Z","root_account":"alauda","source":"host","log_type":"file"}