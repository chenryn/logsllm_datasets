80
Time (s)
100
120
140
Figure 13: Additional TCP ﬂow on the slow link
5. RELATED WORK
To date, a number of single-rate multicast congestion control schemes
have been proposed. A prominent recent example is PGMCC [17].
It selects the receiver with the worst network conditions as a group
representative, called the acker. The selection process for the acker
mainly determines the fairness of the protocol, and is based on a
simpliﬁed version of the TCP throughput model in Equation (4).
Similar to TFMCC, each receiver tracks the RTT and the smoothed
loss rate, and feeds these values into the model. The results are
communicated to the sender using normal randomized feedback
timers to avoid an implosion.
If available, PGMCC also makes
use of network elements to aggregate feedback.
Once an acker is selected, a TCP-style window-based congestion
control algorithm is run between the sender and the acker. Minor
modiﬁcations compared to TCP concern the separation of conges-
tion control and reliability to be able to use PGMCC for reliable as
well as unreliable data transport and the handling of out of order
packets and RTT changes when a different receiver is selected as
the acker.
As evidenced by the simulations in [17], PGMCC competes fairly
with TCP for many different network conditions. The basic con-
gestion control mechanism is simple and its dynamics are well un-
derstood from the analysis of TCP congestion control. This close
mimicking of TCP’s window behavior produces rate variations that
resemble TCP’s sawtooth-like rate. This makes PGMCC suited for
applications that can cope with larger variations in the sending rate.
In contrast, the rate produced by TFMCC is generally smoother
and more predictable, making TFMCC well suited to applications
with more constraints on acceptable rate changes. Since the acker
selection process is critical for PGMCC’s performance, PGMCC
might beneﬁt from using a feedback mechanism similar to that of
TFMCC, based on biased exponentially weighted timers. To sum-
marize, we believe that both PGMCC and TFMCC present viable
solutions for single-rate multicast congestion control, targeted at
somewhat different application domains.
While PGMCC relies on a congestion window, TCP-Emulation at
Receivers (TEAR) [16] is a combination of window- and rate-based
congestion control. It features a TCP-like window emulation algo-
rithm at the receivers, but the window is not used to directly con-
trol transmission. Instead, the average window size is calculated
and transformed into a smoothed sending rate, which is used by
the sender to space out data packets. So far, only a unicast ver-
sion of TEAR exists, but the mechanism can be made multicast-
capable by implementing a TFMCC-like scalable feedback sup-
pression scheme to communicate the calculated rate to the sender
as well as scalable RTT measurements. The advantage of TEAR
lies in the fact that it does not require a model of TCP with all the
necessary assumptions to compute a rate. However, for low levels
of statistical multiplexing, TEAR’s emulation assumptions about
the independence of loss timing from transmit rate and of timeout
emulation mean that it shares many of the limitations of the TCP
models we use. Thus we do not expect a multicast variant of TEAR
to behave signiﬁcantly better or worse than TFMCC.
6. CONCLUSIONS
We have described TFMCC, a single-rate multicast congestion con-
trol mechanism intended to scale to groups of several thousand re-
ceivers. Performing multicast congestion control whilst remaining
TCP-friendly is difﬁcult, in particular because TCP’s transmission
rate depends on the RTT, and measuring RTT in a scalable manner
is a hard problem. Given the limitations of end-to-end protocols,
we believe that TFMCC represents a signiﬁcant improvement over
previous work in this area.
We have extensively evaluated TFMCC through analysis and sim-
ulation, and believe we have a good understanding of its behavior
in a wide range of network conditions. To summarize, we believe
that under the sort of conditions TFMCC will experience in the
real-world it will behave rather well. However we have also ex-
amined certain pathological cases; in these cases the failure mode
is for TFMCC to achieve a slower than desired transmission rate.
Given that all protocols have bounds to their good behavior, this
is the failure mode we would desire, as it ensures the safety of the
Internet.
An important part of any research is to identify the limitations of a
new design. TFMCC’s main weakness is in the startup phase – it
can take a long time for sufﬁciently many receivers to measure their
RTT (assuming we cannot use NTP to provide approximate default
values). In addition, with large receiver sets, TCP-style slowstart
is not really an appropriate mechanism, and a linear increase can
take some time to reach the correct operating point. However these
weaknesses are not speciﬁc to TFMCC – any safe single-rate multi-
cast congestion control mechanism will have these same limitations
if it is TCP-compatible. The implication is therefore that single-rate
multicast congestion control mechanisms like TFMCC are only re-
ally well-suited to relatively long-lived data streams. Fortunately it
also appears that most current multicast applications such as stock-
price tickers or video streaming involve just such long-lived data-
streams.
6.1 Future Work
We plan to pursue this work further on several fronts. While large-
scale multicast experiments are hard to perform in the real world,
we plan to deploy TFMCC in a multicast ﬁlesystem synchroniza-
tion application (e.g. rdist) to gain small-scale experience with a
real application.
Some reliable multicast protocols build an application-level tree for
acknowledgment aggregation. We have devised a hybrid rate/window-
based variant of TFMCC that uses implicit RTT measurement com-
bined with suppression within the aggregation nodes. This variant
does not need to perform explicit RTT measurements or end-to-end
feedback suppression. Whilst at ﬁrst glance this would seem to be a
big improvement over the variant in this paper, in truth it moves the
complex initialization problem from RTT measurement to scalable
ack-tree construction, which shares many of the problems posed by
RTT measurement. Still, this seems to be a promising additional
line of research.
Finally, the basic equation-based rate controller in TFMCC would
also appear to be suitable for use in receiver-driven layered multi-
cast, especially if combined with dynamic layering [4] to eliminate
problems with unpredictable multicast leave latency.
7. ACKNOWLEDGEMENTS
We would like to thank Sally Floyd, Luigi Rizzo, and all reviewers
for their invaluable comments. We would also like to acknowledge
feedback and suggestions received from RMRG members on ear-
lier versions of TFMCC.
8. REFERENCES
[1] M. Allman. A web server’s view of the transport layer. ACM
Computer Communication Review, 30(5), Oct. 2000.
[2] S. Bajaj, L. Breslau, D. Estrin, K. Fall, S. Floyd, P. Haldar,
M. Handley, A. Helmy, J. Heidemann, P. Huang, S. Kumar,
S. McCanne, R. Rejaie, P. Sharma, K. Varadhan, Y. Xu,
H. Yu, and D. Zappala. Improving simulation for network
research. Technical Report 99-702b, University of Southern
California, March 1999. revised September 1999, to appear
in IEEE Computer.
[3] S. Bhattacharyya, D. Towsley, and J. Kurose. The loss path
multiplicity problem in multicast congestion control. In
Proc. of IEEE Infocom, volume 2, pages 856 – 863, New
York, USA, March 1999.
[4] J. Byers, M. Frumin, G. Horn, M. Luby, M. Mitzenmacher,
A. Roetter, and W. Shaver. FLID-DL: Congestion control for
layered multicast. In Proc. Second Int’l Workshop on
Networked Group Communication (NGC 2000), Palo Alto,
CA, USA, Nov. 2000.
[5] S. Floyd, M. Handley, J. Padhye, and J. Widmer.
Equation-based congestion control for unicast applications.
In Proc. ACM SIGCOMM, pages 43 – 56, Stockholm,
Sweden, Aug. 2000.
[6] S. Floyd, V. Jacobson, C. Liu, S. McCanne, and L. Zhang. A
reliable multicast framework for light-weight sessions and
application level framing. IEEE/ACM Transactions on
Networking, 5(6):784 – 803, Dec. 1997.
[7] T. Fuhrmann and J. Widmer. On the scaling of feedback
algorithms for very large multicast groups. Computer
Communications, 24(5-6):539 – 547, Mar. 2001.
[8] S. S. Gupta. Order statistics from the gamma distribution.
Technometrics, 2:243 – 262, 1960.
[9] M. Handley. Session directories and scalable Internet
multicast address allocation. In Proc. ACM Sigcomm, pages
105 – 116, Vancouver, B.C., Canada, Sept. 1998.
[10] A. Mankin, A. Romanow, S. Bradner, and V. Paxson. RFC
2357: IETF criteria for evaluating reliable multicast transport
and application protocols, June 1998. Obsoletes RFC1650.
Status: INFORMATIONAL.
[11] M. Mathis, J. Semke, J. Mahdavi, and T. Ott. The
macroscopic behavior of the congestion avoidance
algorithm. Computer Communications Review, 1997.
[12] S. McCanne, V. Jacobson, and M. Vetterli. Receiver-driven
layered multicast. In Proc. of ACM SIGCOMM, pages 117 –
130, Palo Alto, CA, USA, Aug. 1996.
[13] D. L. Mills, A. Thyagarajan, and B. C. Huffman. Internet
timekeeping around the globe. Proc. Precision Time and
Time Interval (PTTI) Applications and Planning Meeting,
pages 365 – 371, Dec. 1997.
[14] J. Nonnenmacher and E. W. Biersack. Scalable feedback for
large groups. IEEE/ACM Transactions on Networking,
7(3):375 – 386, June 1999.
[15] J. Padhye, V. Firoiu, D. F. Towsley, and J. F. Kurose.
Modeling TCP Reno performance: a simple model and its
empirical validation. IEEE/ACM Transactions on
Networking, 8(2):133–145, April 2000.
[16] I. Rhee, V. Ozdemir, and Y. Yi. TEAR: TCP emulation at
receivers - ﬂow control for multimedia streaming. Technical
report, Department of Computer Science, NCSU, Apr. 2000.
[17] L. Rizzo. pgmcc: A TCP-friendly single-rate multicast
congestion control scheme. In Proc. ACM SIGCOMM, pages
17 – 28, Stockholm, Sweden, August 2000.
[18] L. Vicisano, J. Crowcroft, and L. Rizzo. TCP-like congestion
control for layered multicast data transfer. In Proc. of IEEE
INFOCOM, volume 3, pages 996 – 1003, March 1998.
[19] J. Widmer and T. Fuhrmann. Extremum feedback for very
large multicast groups. Technical Report TR 12-2001,
Praktische Informatik IV, University of Mannheim,
Germany, May 2001.
[20] J. Widmer and M. Handley. Extending equation-based
congestion control to multicast applications. Technical
Report TR 13-2001, Praktische Informatik IV, University of
Mannheim, Germany, May 2001.
INITIALIZING THE LOSS HISTORY
B.
When a receiver registers its ﬁrst loss event, the number of pack-
ets received thus far usually does not reﬂect the current loss rate.
For example, when the sending rate is constrained by a lower-rate
CLR, a receiver may not experience packet loss for a long period of
time. Instead of the number of packets received before the ﬁrst loss
event, the sending rate at which the ﬁrst packet loss is experienced
can be used as an indicator of the bottleneck bandwidth. Slowstart
results in an overshoot to a maximum of at most twice the bottle-
neck bandwidth. Thus, a more meaningful initial loss interval  0
can be obtained by using the inverse of Equation (1) with half the
sending rate when the ﬁrst loss event occurred.
The mechanism can be facilitated by using the inverse of a simpli-
ﬁed TCP Equation (4) presented in [11], which is easier to compute
than the inverse of Equation (1) and results in a slightly more con-
servative estimate:
c
TT C =
RT T 
TT C  RT T (cid:19)2
c
where c is a constant usually set to3=2.
 = (cid:18)
; with  0 = 1=
However, if a receiver is still using the initial RTT when the ﬁrst
loss event occurs, it will underestimate the loss event rate and the
initial loss interval will be too large. When the correct RTT is de-
termined later, the receiver will consequently overestimate the fair
rate. The initial loss interval must be adjusted if it is still in the loss
history when the ﬁrst RTT measurement is obtained. The adjusted
ﬁrst loss interval  0
0 can be calculated as
 0
0 =  0 (cid:18) RT T
RT T (cid:19)2
iiia 
using the simpliﬁed TCP equation.
C. STORING THE PREVIOUS CLR
As an option, the sender can keep information about the previous
CLR after switching to a new CLR. In case the switch-over is only
temporary, it is possible to immediately switch back to the old CLR
without the need of further feedback. Possible causes for transient
switching of the CLR include short-term congestion or inaccurate
one-way delay RTT adjustments. Here, the new expected rate may
quickly increases above the expected rate of the previous CLR.
Storing this additional information will always result in more con-
servative TFMCC behavior. In particular, when network conditions
for the new CLR as well as the old CLR improve simultaneously,
TFMCC will switch back to the old CLR before increasing the
sending rate. Since this results in a delayed reaction to improved
network conditions, the information about the old CLR should be
timed out after a short amount of time (on the order of a few RTTs).
APPENDIX
A. USING THE INITIAL RTT FOR THE AG-
GREGATION OF LOSS EVENTS
Using the initial RTT for the rate computation before a valid RTT
measurement is obtained is safe since it leads to a lower calculated
rate. In contrast, using the initial RTT for the aggregation of lost
packets to loss events results in more aggressive protocol behavior.
In this section we argue that these two effects cancel each other out
in most cases and the initial RTT can be used for both purposes.
The initial RTT only has an impact on the loss event rate when sep-
arate loss intervals are merged into a single loss interval (i.e. more
than one packet is lost per RTT). From Equation (1), the number of
loss events per RTT is
 RT T =
1
3  12 3
 2
8 1  322
The corresponding curve is plotted in Figure 14. The maximum
value is approximately 0.13 loss events per RTT. Thus, when mul-
tiple losses are aggregated to form a loss event and a loss event
occurs during each RTT, the condition is unstable. TFMCC will
reduce the sending rate due to the high loss event rate until the
number of loss events per RTT is smaller than 0.13.
0.14
0.12
0.1
0.08
0.06
0.04
0.02
T
T
R
/
s
t
n
e
v
E
s
s
o
L
0
0.0001
0.001
0.01
0.1
1
Loss Event Rate
Figure 14: Loss Events per RTT
Even during the transition time, a TFMCC ﬂow with an RTT esti-
mate that is too high will behave more conservatively than a similar
ﬂow with a correct RTT estimate. The size of the loss intervals can
only increase in proportion to the ratio of the initial RTT to the true
RTT. Using Equation (4), an initial RTT that is too high by a factor
of c will allow for a loss rate that is too low by a factor of c2 re-
sulting in the same throughput. The rate calculated at the receiver
will therefore still be conservative. Numerical analysis indicates
that this also holds for the complex TCP model (1) when loss event
rates are less than approximately 10%.
If there are many receivers with a high loss rate, then throughput
will be very low (see Section 3). If there are few such receivers,
these receivers can measure their RTT soon after startup. For these
reasons, it is safe to use a high initial RTT to both aggregate losses
to loss events as well as to compute the rate.
The loss history must be remodeled after the ﬁrst valid RTT mea-
surement is obtained, otherwise the rate calculated by the receiver
will be too high. When the lost packets and their timestamps are
known, the correct loss intervals can easily be determined based
on the measured RTT rather than of the initial RTT. This process
can be optimized by storing information about some of the more
recently lost packets and approximating the correct distribution of
loss intervals.