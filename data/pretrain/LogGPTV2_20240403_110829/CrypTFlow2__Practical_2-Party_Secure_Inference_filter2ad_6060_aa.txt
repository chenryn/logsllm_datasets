title:CrypTFlow2: Practical 2-Party Secure Inference
author:Deevashwer Rathee and
Mayank Rathee and
Nishant Kumar and
Nishanth Chandran and
Divya Gupta and
Aseem Rastogi and
Rahul Sharma
CrypTFlow2: Practical 2-Party Secure Inference
Deevashwer Rathee
Microsoft Research
PI:EMAIL
Mayank Rathee
Microsoft Research
PI:EMAIL
Nishant Kumar
Microsoft Research
PI:EMAIL
Nishanth Chandran
Microsoft Research
PI:EMAIL
Divya Gupta
Microsoft Research
PI:EMAIL
Aseem Rastogi
Microsoft Research
PI:EMAIL
Rahul Sharma
Microsoft Research
PI:EMAIL
ABSTRACT
We present CrypTFlow2, a cryptographic framework for secure
inference over realistic Deep Neural Networks (DNNs) using secure
2-party computation. CrypTFlow2 protocols are both correct – i.e.,
their outputs are bitwise equivalent to the cleartext execution – and
efficient – they outperform the state-of-the-art protocols in both
latency and scale. At the core of CrypTFlow2, we have new 2PC
protocols for secure comparison and division, designed carefully to
balance round and communication complexity for secure inference
tasks. Using CrypTFlow2, we present the first secure inference
over ImageNet-scale DNNs like ResNet50 and DenseNet121. These
DNNs are at least an order of magnitude larger than those con-
sidered in the prior work of 2-party DNN inference. Even on the
benchmarks considered by prior work, CrypTFlow2 requires an
order of magnitude less communication and 20×-30× less time than
the state-of-the-art.
KEYWORDS
Privacy-preserving inference; deep neural networks; secure two-
party computation
1 INTRODUCTION
The problem of privacy preserving machine learning has become
increasingly important. Recently, there have been many works that
have made rapid strides towards realizing secure inference [4, 6, 13,
17, 19, 22, 31, 43, 48, 49, 51, 55, 57]. Consider a server that holds the
weights 𝑤 of a publicly known deep neural network (DNN), 𝐹, that
has been trained on private data. A client holds a private input 𝑥;
in a standard machine learning (ML) inference task, the goal is for
the client to learn the prediction 𝐹(𝑥, 𝑤) of the server’s model on
the input 𝑥. In secure inference, the inference is performed with
the guarantee that the server learns nothing about 𝑥 and the client
learns nothing about the server’s model 𝑤 beyond what can be
deduced from 𝐹(𝑥, 𝑤) and 𝑥.
A solution for secure inference that scales to practical ML tasks
would open a plethora of applications based on MLaaS (ML as a
Service). Users can obtain value from ML services without worry-
ing about the loss of their private data, while model owners can
effectively monetize their services with no fear of breaches of client
data (they never observe private client data in the clear). Perhaps
the most important emerging applications for secure inference are
in healthcare where prior work [4, 45, 55] has explored secure in-
ference services for privacy preserving medical diagnosis of chest
diseases, diabetic retinopathy, malaria, and so on.
Secure inference is an instance of secure 2-party computation
(2PC) and cryptographically secure general protocols for 2PC have
been known for decades [32, 63]. However, secure inference for
practical ML tasks, e.g., ImageNet scale prediction [24], is challeng-
ing for two reasons: a) realistic DNNs use ReLU activations1 that
are expensive to compute securely; and b) preserving inference
accuracy requires a faithful implementation of secure fixed-point
arithmetic. All prior works [6, 31, 43, 48, 49, 51] fail to provide effi-
cient implementation of ReLUs. Although ReLUs can be replaced
with approximations that are more tractable for 2PC [22, 31, 49],
this approach results in significant accuracy losses that can degrade
user experience. The only known approaches to evaluate ReLUs
efficiently require sacrificing security by making the untenable
assumption that a non-colluding third party takes part in the proto-
col [7, 45, 50, 56, 61] or by leaking activations [12]. Moreover, some
prior works [45, 49–51, 61] even sacrifice correctness of their fixed-
point implementations and the result of their secure execution can
sometimes diverge from the expected result, i.e. cleartext execution,
in random and unpredictable ways. Thus, correct and efficient 2PC
protocols for secure inference over realistic DNNs remain elusive.
1.1 Our Contributions
In this work, we address the above two challenges and build
new semi-honest secure 2-party cryptographic protocols for secure
computation of DNN inference. Our new efficient protocols enable
the first secure implementations of ImageNet scale inference that
complete in under a minute! We make three main contributions:
First, we give new protocols for millionaires’ and DReLU2 that
enable us to securely and efficiently evaluate the non-linear
layers of DNNs such as ReLU, Maxpool and Argmax.
Second, we provide new protocols for division. Together with
new theorems that we prove on fixed-point arithmetic over
shares, we show how to evaluate linear layers, such as convo-
lutions, average pool and fully connected layers, faithfully.
Finally, by providing protocols that can work on a variety
of input domains, we build a system3 CrypTFlow2 that sup-
ports two different types of Secure and Correct Inference (SCI)
protocols where linear layers can be evaluated using either ho-
momorphic encryption (SCIHE) or through oblivious transfer
(SCIOT).
We now provide more details of our main contributions.
1ReLU(𝑥) is defined as max(𝑥, 0).
2DReLU is the derivative of ReLU, i.e., DReLU(𝑥) is 1 if 𝑥 ≥ 0 and 0 otherwise.
3Implementation is available at https://github.com/mpc-msri/EzPC.
1
New millionaires’ and DReLU protocols. Our first main techni-
cal contribution is a novel protocol for the well-known millionaires’
problem [63], where parties 𝑃0 and 𝑃1 hold ℓ−bit integers 𝑥 and
𝑦, respectively, and want to securely compute 𝑥  2𝜆ℓ and would additionally pay an inordinate cost in terms of rounds, namely ℓ.
5Couteau [21] presented multiple protocols; we pick the one that has the best commu-
nication complexity.
Layer
Millionaires’
on {0, 1}ℓ
Millionaires’
example
ℓ = 32
Protocol
GC [62, 63]
GMW4/GSV [29, 32]
SC35[21]
This work (𝑚 = 4)
GMW/GSV [29, 32]
GC [62, 63]
SC3 [21]
This work (𝑚 = 7)
This work (𝑚 = 4)
Comm. (bits)
4𝜆ℓ
≈ 6𝜆ℓ
> 3𝜆ℓ
< 𝜆ℓ + 14ℓ
16384
23140
13016
2930
3844
2
Rounds
log ℓ + 3
≈ 4 log∗ 𝜆
log ℓ
2
8
15
5
5
Table 1: Comparison of communication with prior work for
millionaires’ problem. For our protocol, 𝑚 is a parameter.
For concrete bits of communication we use 𝜆 = 128.
Layer
ReLU for
Z2ℓ
ReLU for
general Z𝑛
ReLU for
Z2ℓ , ℓ = 32
ReLU for
Z𝑛, 𝜂 = 32
Protocol
GC [62, 63]
This work