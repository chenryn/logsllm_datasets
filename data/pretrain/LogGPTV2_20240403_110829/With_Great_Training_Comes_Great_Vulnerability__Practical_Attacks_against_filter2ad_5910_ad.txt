InceptionV3 (2015 version) is used as the Teacher model
(i.e.
the corresponding ﬁngerprint image leads to Gini
coefﬁcient of 0.061 while the other ﬁngerprint images
lead to much higher values > 0.4063). The subsequent
misclassiﬁcation attack achieves a 96.5% success rate
with P = 0.001.
Microsoft CNTK.
The Microsoft Cognitive Toolkit
(CNTK) is an open source DL library available on Mi-
crosoft’s Azure MLaaS platform. The tutorial describes
a ﬂower classiﬁcation task and recommends ResNet18
as the Teacher and Full Model Fine-tuning as the default
conﬁguration [6]. This creates a Student model similar
to the Flower model used in Section 4. CNTK also pro-
vides control parameters to switch to Deep-layer Feature
Extractor ( Mid-layer Feature Extractor is unavailable)
and other Teacher models hosted by Microsoft, including
6Instead of training the Student in the cloud, we build the model
locally using Google TensorFlow using the same procedure [7].
popular image classiﬁcation models (e.g., ResNet50, In-
ceptionV3, VGG16) and a few object detection models.
Following this process, we use VGG16 as the Teacher
and Deep-layer Feature Extractor to train a new Student
model using the 102-class VGG ﬂower dataset (the ex-
ample dataset used in tutorial). It achieves a classiﬁca-
tion accuracy of 82.25%.
Again, we were able to launch the misclassiﬁcation
attack on the Student model: our ﬁngerprinting method
successfully identiﬁes the Teacher model (with a Gini co-
efﬁcient of 0.0045), and the attack success rate is 99.4%
when P = 0.003.
PyTorch.
PyTorch is a popular open source DL library
developed by Facebook.
Its tutorial describes steps to
build a classiﬁer that can distinguish between images of
ants and bees [3]. The tutorial uses ResNet18 by default
and allows both Deep-layer Feature Extractor and Full
Model Fine-tuning, but indicates that Deep-layer Feature
Extractor provides higher accuracy. There is no mention
of Mid-layer Feature Extractor. PyTorch hosts a reposi-
tory of 6 image classiﬁcation Teacher models that users
can plug into their transfer process.
Again we follow the tutorial and verify that Student
models trained using Deep-layer Feature Extractor on
PyTorch are vulnerable. Our ﬁngerprinting technique
produces a Gini coefﬁcient of 0.004, and targeted attack
achieves a success rate of 88.0% with P = 0.001. We
also test our attack on a student model trained using Full
Model Fine-tuning. Surprisingly, our targeted attack still
achieves an 87.4% success rate with P = 0.001. This
is likely because the Student model is trained only for a
short number of epochs (25 epochs) at a very low learn-
ing rate of 0.001, and thus the ﬁne-tuning process intro-
duces only small modiﬁcation to the model weights.
Implications.
Our experiments on the three machine
learning services show that many Student models pro-
duced by these services are vulnerable to our attack. This
is particularly true when users follow the default conﬁg-
uration in Google Cloud ML and PyTorch. Our attack
is feasible because each service only hosts a small num-
ber of deep learning Teacher models, making it easy to
get access to the (small) pool of Teacher models. Fi-
nally, by promoting the use of transfer learning, these
platforms often expose their customers to our attack ac-
cidentally. For example, Google Cloud ML advertises
customers who have successfully deployed models using
their transfer learning service [4]. While we refrain from
attacking such customer models for ethical reasons, such
information can help attackers ﬁnd potential victims and
gain additional knowledge about the victim model. We
discuss our efforts at disclosure in the Appendix.
1290    27th USENIX Security Symposium
USENIX Association
6 Developing Robust Defenses
Having identiﬁed the practical impact of these attacks,
the ultimate goal of our work is to develop robust de-
fenses against them. Insights gained through our exper-
iments suggest that there are multiple approaches to de-
veloping robust defenses against this attack. First, the
effectiveness of attacks is heavily dependent on the level
of perturbations introduced. Successful misclassiﬁca-
tion seems to be very sensitive to small changes made
to the input image. Therefore, any defense that perturbs
the adversarial sample before classiﬁcation has a good
chance of disrupting the attack. Second, attack success
requires precise knowledge of the Teacher model used
during transfer learning, i.e. the weights transferred to
the Student model. Thus any deviations from the Teacher
model could render the attack ineffective.
Here, we describe three different potential defenses
that target different pieces of the Student model classiﬁ-
cation process. We discuss the strengths and limitations
of each, and experimentally evaluate their effectiveness
against the attack and impact on classiﬁcation of non-
adversarial inputs.
6.1 Randomizing Input via Dropout
Our ﬁrst defense targets the sensitivity of adversarial
samples to small changes. The intuition is that attackers
have identiﬁed minimal alterations to the image that push
the Student model over some classiﬁcation boundary. By
introducing additional random perturbations to the image
before classiﬁcation, we can disrupt the adversarial sam-
ple. Ideally, small perturbations could effectively disrupt
adversarial attacks while introducing minimal impact on
non-adversarial samples.
In prior work, Carlini, et al.
studied different defense mechanisms against attacks on
DNNs [16], and found the most effective approach to be
adding uncertainty to the prediction process [25].
Dropout Randomization. We add randomness to the
prediction process by applying Dropout [57] at the input
layer. This has the effect of dropping a certain fraction of
randomly selected input pixels, before feeding the modi-
ﬁed image to the Student model. We repeat this process
3 times for each image and use the majority vote as the
ﬁnal prediction result 7, or a random result if all 3 pre-
dictions are different.
We test this defense on all three tasks, Face, Iris, and
Trafﬁc Sign, by applying Dropout on test images as well
as targeted and non-targeted adversarial samples 8. The
results for Face and Trafﬁc Sign are highly consistent,
so we only plot the results for Face in Figure 7, includ-
ing classiﬁcation accuracy on test images, and success
7We tested and found little improvement beyond 3 repetitions.
8We choose adversarial samples from Section 4.3 that achieve the
highest attack success rate.
rate of both targeted and non-targeted attacks. Results
for Trafﬁc Sign is in the Appendix as Figure 14. As the
dropout ratio increases (i.e. more pixels dropped), both
classiﬁcation accuracy and attack success rate drops. In
general, the defense is effective against targeted misclas-
siﬁcation, which drops in success rate much faster than
the corresponding drop in classiﬁcation accuracy, e.g. at
dropout ratio near 0.4, classiﬁcation accuracy drops to
91.4% while targeted attack success rate drops to 30.3%.
However, non-targeted attacks are less affected, and at-
tack success consistently remains higher than classiﬁ-
cation accuracy of normal samples, e.g. 92.47% when
the classiﬁcation accuracy is 91.4%. Finally, as dropout
increases, it eventually disrupts the entire classiﬁcation
process, reducing classiﬁcation accuracy while boosting
misclassiﬁcation errors (non-targeted misclassiﬁcation).
This defense is ineffective on the Iris task. Recall
that this model is sensitive to noise in general. The in-
herent sensitivity leads classiﬁcation accuracy to drop at
nearly the same rate as attack success rate. When drop-
ping only 2% pixels, model accuracy already drops to
51.93%, while targeted attack success rate is still 55.5%
and non-targeted attack success rate is 100%. Detailed
results are shown in the Appendix as Figure 14. Clearly,
randomization as defense is limited by the inherent sen-
sitivity of the model. It is unclear whether the situation
could by improved by retraining the Student model to be
more resistant to noise [72].
Strengths and Limitations.
The key beneﬁt of this
approach is that it can be easily deployed, without re-
quiring changes to the underlying Student model. This is
ideal for Student models that are already deployed. How-
ever, this approach has three limitations. First, there is a
non-negligible hit on model accuracy for any signiﬁcant
reduction in attack success. This may be unacceptable
for some applications (e.g., authentication systems based
on Face recognition). Second, this approach is impracti-
cal for highly sensitive classiﬁcation tasks like Iris recog-
nition. Finally, this approach is not resistant to counter-
measures by the attacker. An attacker can circumvent
this defense by adding a Dropout layer into the adversar-
ial image crafting pipeline [16]. The generated adversar-
ial samples would then be more robust to Dropout.
6.2 Injecting Neuron Distances
The attack we identiﬁed leverages the similarity between
matching layers in the Teacher and Student models to
mimic an internal representation of the Student. Thus, if
we can make the Student’s internal representation deviate
from that of the Teacher for all inputs, the attack would
be less effective. One way to do that is by modifying
weights of different layers of the Student. In this sec-
tion, we present a scheme to modify the Student layers
USENIX Association
27th USENIX Security Symposium    1291
y
c
a
r
u
c
c
A
n
o
i
t
a
c
i
f
i
s
s
a
C
l
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
 1
 0.8
 0.6
 0.4
 0.2
e
t
a
R
s
s
e
c
c
u
S
k
c
a
t
t
A
 0
 0.8
y
c
a
r
u
c
c
A
n
o
i
t
a
c
i
f
i
s
s
a
C
l
 1
 0.8
 0.6
 0.4
 0.2
 0
0
Classification
Non-targeted
Targeted
 1
 0.8
 0.6
 0.4
 0.2
e
t
a
R
s
s
e
c
c
u
S
k
c
a
t
t
A
10K
20K
Neuron Distance Threshold
 0
30K
y
c
a
r
u
c
c
A
n
o
i
t
a
c
i
f
i
s
s
a
C
l
 1
 0.8
 0.6
 0.4
 0.2
 0
0
Non-targeted
Classification
Targeted
 1
 0.8
 0.6
 0.4
 0.2
 0
e
t
a
R
s
s
e
c
c
u
S
k
c
a
t
t
A
2K
4K
6K
8K
10K
Neuron Distance Threshold
Non-targeted
Classification
Targeted
 0.2
 0.4
 0.6
Dropout Ratio
Figure 7: Attack success and classi-
ﬁcation accuracy on Face using ran-
domization via dropout.
Figure 8: Attack success and classiﬁ-
cation accuracy on Face using neuron
distance thresholds.
Figure 9: Attack success and classi-
ﬁcation accuracy on Iris using neuron
distance thresholds.
(i.e. weights), without signiﬁcantly impacting classiﬁca-
tion accuracy.
We start with a Student model trained using Deep-
layer Feature Extractor or Mid-layer Feature Extrac-
tor 9. This model lies in some local optimum of the
model classiﬁcation error surface. Our goal is to update
layer weights and identify a new local optimum that pro-
vides comparable (or better) classiﬁcation performance,
and also be distant enough (on the error surface) to in-
crease the dissimilarity between the Student and Teacher.
To ﬁnd such a new local optimum, we unfreeze all
layers of Student and retrain the model using the same
Student training dataset, but with an updated loss func-
tion formulated in the following way. Consider a Stu-
dent model, where the ﬁrst K layers are copied from the
Teacher. Let TK(.), and SK(.) be functions that gener-
ate the internal representation at layer K, for the Teacher,
and Student, respectively. Let I be the set of neurons in
layer K, and |Ws| be a vector of absolute sum of outgo-
ing weights from each neuron i ∈ I. Finally, let Dth be
a dissimilarity threshold between two models. Then our
objective is the following,
min CrossEntropy(Ytrue,Ypred)
s.t. ∑
k|Ws| ◦ (TK(x) − SK(x))k2 > Dth
x∈Xtrain
(4)
where ◦ is element-wise multiplication.
Here, we still want to minimize the classiﬁcation loss,
formulated as cross entropy loss over the prediction re-
sults. But, a constraint term is added to increase the dis-
similarity between the Teacher and Student models. Dis-
similarity is computed as the weighted L2 distance be-
tween the internal representations at layer K, and is con-
ditioned to be higher than a threshold Dth. The weight
terms capture the importance of a neuron output for the
next layer 10. This helps make sure that distance be-
tween important neurons contribute more to the total dis-
9Recall that models using Full Model Fine-tuning are generally re-
sistant to the attack.
10The weight terms are not required for layers, where all neuron out-
puts are treated equally, e.g., convolutional layers.
tance between representations. We solve the above con-
strained optimization problem using the same penalty
method used in Section 3.
Before presenting our evaluation, we note two other
aspects of the optimization process. First, our objective
function only considers dissimilarity at layer K. How-
ever, after training with the new loss function, the inter-
nal representations at the preceding layers also become
dissimilar. Hence, our approach would not only reduce
attack effectiveness at layer K, but also at layers before it.
Second, a high value for Dth would increase defense per-
formance, but can also negatively impact classiﬁcation
accuracy. In practice, the provider can incrementally in-
crease Dth as long as the classiﬁcation accuracy is above
an acceptable level.
We evaluated this approach on all three classiﬁcation
tasks. Figure 8 shows how classiﬁcation accuracy and
attack success vary when we increase Dth in Face. At-
tacks are targeted at layer N − 1, as Face uses Deep-layer
Feature Extractor. Unlike the Dropout based defense
(Figure 7), this method results in a steadier classiﬁcation
accuracy, while attack success rate drops. As classiﬁ-
cation accuracy drops from 98.55% to 95.69%, targeted
attack drops signiﬁcantly, from 92.6% to 30.87%. Non-
targeted attacks are still hard to defend against, drop-
ping from 100% to only 91.45% under the same con-
ditions. We also analyze attack success rates at layers