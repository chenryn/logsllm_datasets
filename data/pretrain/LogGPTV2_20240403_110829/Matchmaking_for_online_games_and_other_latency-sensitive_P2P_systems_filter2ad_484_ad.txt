ment among machines whose coordinates change that day. Note
that in measuring the distance between two coordinates, we use
the absolute difference in their height rather than the sum of their
heights. Figure 20 shows the results. We observe that Pyxida takes
about 7 days to converge to its steady-state coordinate movement
levels, while Htrae converges essentially immediately.
5.4 Drift
A potential downside of using an NCS is drift, i.e., the ten-
dency for coordinates to move systematically in some direction over
time [13]. Drift is a problem because if a machine does not par-
ticipate for an extended period of time, then when it resumes its
coordinates will be in the wrong position relative to the remaining
population, which has drifted. Drift is also problematic for Htrae
Figure 22. CDF of prediction error using either up-to-date or
three-week-old coordinates.
because if coordinates drift from true Earth, e.g., by rotating 10 de-
grees westward in longitude, then newly arriving machines that ob-
tain initial coordinates based on their location on Earth will be in the
wrong position with respect to other machines’ coordinates. Pyx-
ida uses gravity, a weak force that pulls all coordinates gently and
slowly toward the origin, to counter drift. However, this only works
on translational drift; it cannot help with rotational drift, which is
the only kind that can occur in Htrae’s spherical coordinate system.
Fortunately, as we will now show, drift does not pose a signiﬁcant
problem for Htrae.
The standard way to measure drift is to measure the movement
of the centroid of the population. However, this would only re-
ﬂect translational drift, not rotational drift, and it is the latter we are
most concerned about since our coordinates are spherical coordi-
nates. Hence, rather than measure drift directly, we instead measure
the effect of drift. We measure the extent to which newly arriving
nodes see greater error as time progresses, and the extent to which
machines that are absent for an extended period suffer from return-
ing to a shifted coordinate system. Again, we now turn off history
prioritization in Htrae since we are interested in coordinate drift.
First, we measure, for each day of a one-month run, the aver-
age error seen by newly arriving machines on that day. Figure 21
shows the result. We see that there is no noticeable trend upward,
suggesting that this effect of drift is not present.
Next, we simulate the effect of absence. On the last day of a
one-month trace, we compare the effect of using the latest coordi-
nates for the probing machine to using its coordinates from three
weeks earlier. This simulates what would happen if the machine
had been absent for those three weeks and needed to use its coor-
dinates from just before its absence. We only consider machines
whose uncertainty has changed by less than 10% during those three
weeks, since we are only interested in machines whose coordinates
have converged before their departure. Figure 22 shows the results.
We see that the effect of using old coordinates is small, generally
producing less than 2 ms of additional error. Indeed, it is roughly
050100150200250051015202530average error %number of daysPyxida (90th quantile)Pyxida (75th quantile)Geolocation (90th quantile)Geolocation (75th quantile)Htrae (90th quantile)Htrae (75th quantile)05101520253035404550051015202530avg. daily coordinate change (ms)number of daysPyxidaHtrae05101520253035404550051015202530avg. prediction error when one machine is new (ms)day number01020304050607080901000102030405060708090100110120130140150CDF (%)error (ms)Htrae (normal)Htrae (old coords)Pyxida (normal)Pyxida (old coords)322Figure 23. Probability of ﬁnding a server with RTT under
75 ms, as a function of number of candidates. Each prediction
algorithm predicts which six of those candidates have the lowest
RTT, and probes only them. “Oracle” always ﬁnds the candi-
date with lowest RTT, and “random” chooses an arbitrary six
to probe.
Figure 24. Probability of ﬁnding a server with RTT under
75 ms, as a function of number of probes. Each prediction al-
gorithm predicts which of the 100 candidates have the lowest
RTT, and probes only them. “Oracle” always ﬁnds the candi-
date with lowest RTT, and “random” chooses an arbitrary set
to probe.
the same amount seen in Pyxida, which uses gravity to counter drift.
Thus we believe that drift is not a problem for Htrae. We suspect
that the reason old coordinates work slightly worse is due to other
factors such as Internet topology changes and/or improvement to
coordinates for other nodes over the course of three weeks.
In conclusion, it appears that drift does not pose a signiﬁcant
problem for Htrae. We believe this is because the steady arrival of
new machines, each of which is initialized with reasonable conﬁ-
dence in its correct location on true Earth, prevents the coordinate
system from drifting away from true Earth. Since churn is common
in other peer-to-peer applications besides online games, we expect
drift will not be a problem in other applications as well.
5.5 Limited probing
Figure 25. CDF of prediction error in various experiments dur-
ing Htrae deployment.
So far, our analyses have assumed that matchmaking relies
solely on prediction for latency estimation, i.e., that there is no time
to perform network probing to determine latency. We are primar-
ily motivated by that scenario for game matchmaking where users
are very averse to waiting, or for games where it is prohibitive to
probe all potential trafﬁc paths because they have an all-to-all rather
than client/server communication pattern. Nonetheless, there may
be other application scenarios where some network probing can be
used to supplement latency prediction.
We now evaluate the performance of latency prediction systems
when given n servers and asked to reduce it to a subset of m servers.
In this scenario, the application will then probe the m servers, and
pick one with an acceptable latency. A recent study showed that
clients prefer games better as RTT goes down to 75 ms, but below
this they do not care [2]. Hence we evaluate how often a latency
prediction system selects a subset containing a server with RTT un-
der 75 ms. Note that in such an evaluation, latency prediction sys-
tems are allowed a much larger tolerance of error, not only because
they need not place the lowest-latency server among the m, but also
because they can make up to m− 1 “mistakes”.
During our trace, many clients probed as many as 200 servers,
so we know the actual RTT to all of them. We randomly select n of
the servers a client probed and query the latency prediction system
for all n. We then choose the m that were predicted to have the
lowest RTT, and determine if any one of these has a latency under
75 ms.
In Figure 23, we present the m = 6 scenario while varying n on
the horizontal axis. We pick 6 because that is the average number of
probes during a multi-server session in our traces. We see that Htrae
performs the closest to Oracle, and signiﬁcantly above Pyxida. In
this scenario, geolocation also performs extremely well because the
number of nodes for which it has no or inaccurate location infor-
mation is relatively small. Since it gets six tries to pick servers
within 75 ms, these errors have little impact. However, it is quite
possible for nodes with poor location information to get starved and
never be picked by geolocation. Furthermore, we found that some
clients experience consistently bad performance with geolocation,
due presumably to inaccurate placement.
In Figure 24, we ﬁx n at 100 and vary m. This shows how the
beneﬁt of latency prediction varies as more probing is done. We
see that Htrae once again does best of all the non-oracle prediction
systems. However, as expected, the beneﬁt of Htrae, and latency
prediction in general, declines as the number of probes allowed is
increased. This is natural, because the point of latency prediction is
to avoid probing to reduce user wait time and network overhead.
We conclude that Htrae is most useful in scenarios where there
is time to probe only a modest fraction of the candidate game traf-
ﬁc paths available. This includes scenarios where the matchmak-
ing service wants to make a quick decision without relying on any
probes, where probes are costly in terms of game delay, or where
game trafﬁc will traverse so many paths that there is not time to
probe them all. However, even when there is time to probe a large
number of candidates, using Htrae or geolocation is still far prefer-
able to using random selection as is commonly done today.
5.6 Deployment
We now present results from our live deployment of Htrae on
the home machines of eleven volunteers. Because of the limited
size of this deployment, and hence lack of multiple nodes within
the same small AS, we do not expect AS correction to be beneﬁcial
and in fact it is not. Figure 25 presents results of our experiments.
0102030405060708090100020406080100120140160180200probability of finding RTT <75 msnumber of candidatesOracleHtraeGeolocationPyxidaRandom010203040506070809010002468101214161820probability of finding RTT <75 msnumber of probesOracleHtraeGeolocationPyxidaRandom0102030405060708090100051015202530354045505560cumulative % of predictionserror (ms)HtraePyxida + geo bootstrappingPyxida323Figure 26. Virtual path taken by the coordinates of a machine in
Redmond, WA. Its IP address was initially incorrectly classiﬁed
as being in the center of the U.S.
We ﬁnd that even in this small deployment, geographic bootstrap-
ping is quite helpful, reducing Pyxida’s average error from 11 ms
to 8 ms. The frequent all-to-all nature of probing ensures Pyxida is
given ample time to converge, but we see that it converges to a local
minimum worse than what we obtain with better initial conditions.
Finally, the remaining improvements reduce average error to 6 ms;
chieﬂy, this comes from history prioritization, which unsurprisingly
is extremely useful in a deployment enabling all-to-all probing.
An interesting ﬁnding in our deployment is an illustration of the
dynamic nature of geographic bootstrapping. One of our volun-
teers’ machines was in Redmond, WA, but our database could not
pinpoint its IP address in more detail than just its country. Figure 26
shows how over the course of the experiment, this machine adjusted
its coordinates from the initial position in the center of the U.S. to
a location much closer to where it actually is. Estimation systems
relying solely on geographic information would not be able to adapt
in this way to missing information, but this example illustrates that
with Htrae it happens naturally.
6. DISCUSSION AND FUTURE WORK
There is substantial room for improvement in the way game
matchmaking is done today [3]. Adding a latency prediction system
to select good candidates for probing offers substantial improve-
ment in the latency of the ultimate choice. In addition, incorpo-
rating latency prediction will allow the design of a much different
type of matchmaking system, one with greater ﬂexibility to explore
a broader range of alternatives. For instance, in the case of match-
making for a peer-to-peer game, the service will be able to select
groups of players with mutually low latency, without requiring all
candidates to probe all others.
Furthermore, it is clear that a network coordinate system by it-
self is not sufﬁcient as a latency prediction system for games; ge-
olocation is an important tool in making predictions accurate. For
some applications, such as static content distribution, geolocation
by itself may be sufﬁcient. However, for online gaming, it can be
costly to make certain customers unendingly suffer simply because
of inaccurate geolocation. Htrae has the advantage of automatically
correcting for such errors and adapting to changing network condi-
tions. It also produces better worst-case outcomes.
We have evaluated Htrae over month-long traces, and certainly
over such a long period there would have been many routing
changes on the Internet. However, we have not examined how Htrae
adapts to major changes, such as a trans-oceanic link being acciden-
tally cut, or two major ISPs de-peering. It would be interesting to
evaluate how prediction error increases during this time and how
quickly Htrae converges to a stable representation of the new net-
work.
Finally, there are likely other applications for a latency predic-
tion system as accurate and scalable as Htrae, particularly where
participants are likely to be numerous home machines. For in-
stance, distributed hash tables need to select close machines for
routing table entries and lookups [7]. Also, ﬁle sharing systems
often ﬁnd it desirable, for the sake of ISP friendliness and perfor-
mance, to have peers select close peers for content exchange [4].
Some voice-over-IP applications use intermediate peer relays to
handle ﬁrewall traversal, and the choice of peer can have a tremen-
dous impact on call quality. Especially in this application, users
will expect fast call-setup times and may not tolerate long delays
from probing. We would like to examine how well Htrae works for
these scenarios.
7. RELATED WORK
Our work is closely related to three general approaches to la-
tency prediction: graph representation, NCSes, and geolocation.
7.1 Graph representation
All-to-all probing, as in RON [1], produces highly accurate la-
tency prediction but will not scale to large systems, such as one with
millions of game players. One way that researchers have found to
achieve scalability is to use graph representation. In this approach,
a cluster of nodes having similar latency properties is represented
by a vertex in a graph, and edges within that graph represent links
between clusters. The latency between two clusters is the total la-
tency of the links along the path joining them. IDMaps [9] achieves
such a graph embedding through the use of a select set of nodes
they called tracers. IDMaps periodically measures the distance be-
tween each pair of tracers, and between each cluster and its closest
tracer. Theilmann and Rothermel [31] proposed an extension of
this approach, in which the tracers themselves are clustered so they
do not have to perform all-to-all probing. The tracers form a tree
hierarchy, allowing for a graph having more tracers.
iPlane [18] seeks to learn the actual graph structure of the In-
ternet. In this graph, which it calls an atlas, the nodes are actual
routers and endpoints and the edges are actual network links. iPlane
uses multiple vantage points on the Internet and a variety of tech-
niques to create its atlas. This atlas includes latency, loss, and band-
width information about each link, enabling it to predict properties
of arbitrary paths. iPlane Nano [19], a recent variant, uses sophis-
ticated atlas compression to allow decentralized latency prediction
even though atlas creation remains centralized.
It is unclear whether graph representation systems, which in-
volve a great deal of centralized calculation on an enormous amount
of data, will scale to game matchmaking systems with millions of
participants. Our evaluation relative to iPlane suggests it may be
difﬁcult for a graph representation system to achieve the coverage
necessary to deal with such a large population.
7.2 Network coordinate systems
To achieve even further scalability, many have proposed using
a network coordinate system, which embeds the graph structure of
the Internet into a virtual coordinate space. This technique of em-
bedding a graph into a metric space has been used in other disci-
plines to reduce the size of the representation [32, 35, 37]. In the
speciﬁc application of latency estimation, this reduction in repre-
sentation size has particular advantages: it reduces the amount of
background probing necessary to keep the data up to date, and it