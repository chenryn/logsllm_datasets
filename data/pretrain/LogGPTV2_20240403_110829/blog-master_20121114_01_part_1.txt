## PostgreSQL row lock and htup.t_infomask thinking  
### 作者                                                                                 
digoal                                                                         
### 日期                                                                                                                                             
2012-11-14                                                                       
### 标签                                                                          
PostgreSQL , 行锁机制 , t_infomask           
----                                                                                                                                          
## 背景             
QQ群里聊到关于PostgreSQL范围锁的话题. 大概是这样的 :   
```  
BEGIN;  
update t set c1=? where c2>? ;  
```  
在COMMIT或者ROLLBACK之前, 这种SQL可能会锁上很多行?  
1\. 在这种情况下会不会升级成表锁?  
2\. 如果要锁的行数非常多会不会造成内存资源的开销非常大?  
3\. 如果这条SQL要更新100000行, 那么是不是需要100000个锁对象?   
首先, 锁的管理非常复杂, 三言两语也讲不清楚,  这里只涉及非常小的一部分（Regular Lock(a/k/a heavyweight locks)）, 未涉及到spin lock和lwlock. 所以需要深入研究的朋友情参考本文末尾的参考部分.  
至于前面提到的两个问题,   
1\. 在这种情况下会不会升级成表锁?  这个肯定是不会的.   
2\. 如果要锁的行数非常多会不会造成内存资源的开销非常大?    
```  
A LOCK object exists for each lockable object that currently has locks held or requested on it.    
A PROCLOCK struct exists for each backend that is holding or requesting lock(s) on each LOCK object.  
```  
从这句话来看,  锁对象越多必然会增加内存的开销, 但是事实上并不是这样的, 因为lock hash table的size是有限的(等于max_locks_per_transaction * (max_connections + max_prepared_transactions)). 后面会讲到, 真是柳暗花明又一村的感觉. (第三个问题也在后面会有解释. 在lock hash table中理论上不会有100000个锁对象存在(见heap_lock_tuple).)  
但是有必要把大量的tuple lock自动升级成table lock锁吗? 我认为没必要, 这种场景应该人为来控制, 而不是让数据库来自动升锁级别, 包括Oracle也不会这么干. 但是MySQL锁的处理就比较差了, 可参看这里的测试:   
http://blog.163.com/digoal@126/blog/static/163877040201042683410679/  
LOCK对象的数据结构如下 :   
```  
typedef struct LOCK  
{  
        /* hash key */  
        LOCKTAG         tag;                    /* unique identifier of lockable object */  
        /* data */  
        LOCKMASK        grantMask;              /* bitmask for lock types already granted */  
        LOCKMASK        waitMask;               /* bitmask for lock types awaited */  
        SHM_QUEUE       procLocks;              /* list of PROCLOCK objects assoc. with lock */  
        PROC_QUEUE      waitProcs;              /* list of PGPROC objects waiting on lock */  
        int                     requested[MAX_LOCKMODES];               /* counts of requested locks */  
        int                     nRequested;             /* total of requested[] array */  
        int                     granted[MAX_LOCKMODES]; /* counts of granted locks */  
        int                     nGranted;               /* total of granted[] array */  
} LOCK;  
```  
从tag也就是LOCKTAG的数据结构能够看出tuple也在类型之中, 这也印证了PostgreSQL的锁粒度确实是到达了行级别的.  
```  
typedef struct LOCKTAG  
{  
        uint32          locktag_field1; /* a 32-bit ID field */  
        uint32          locktag_field2; /* a 32-bit ID field */  
        uint32          locktag_field3; /* a 32-bit ID field */  
        uint16          locktag_field4; /* a 16-bit ID field */  
        uint8           locktag_type;   /* see enum LockTagType */  
        uint8           locktag_lockmethodid;   /* lockmethod indicator */  
} LOCKTAG;  
```  
枚举LockTagType定义如下 :   
```  
/*  
 * LOCKTAG is the key information needed to look up a LOCK item in the  
 * lock hashtable.      A LOCKTAG value uniquely identifies a lockable object.  
 *  
 * The LockTagType enum defines the different kinds of objects we can lock.  
 * We can handle up to 256 different LockTagTypes.  
 */  
typedef enum LockTagType  
{  
        LOCKTAG_RELATION,                       /* whole relation */  
        /* ID info for a relation is DB OID + REL OID; DB OID = 0 if shared */  
        LOCKTAG_RELATION_EXTEND,        /* the right to extend a relation */  
        /* same ID info as RELATION */  
        LOCKTAG_PAGE,                           /* one page of a relation */  
        /* ID info for a page is RELATION info + BlockNumber */  
        LOCKTAG_TUPLE,                          /* one physical tuple */  
        /* ID info for a tuple is PAGE info + OffsetNumber */  
        LOCKTAG_TRANSACTION,            /* transaction (for waiting for xact done) */  
        /* ID info for a transaction is its TransactionId */  
        LOCKTAG_VIRTUALTRANSACTION, /* virtual transaction (ditto) */  
        /* ID info for a virtual transaction is its VirtualTransactionId */  
        LOCKTAG_OBJECT,                         /* non-relation database object */  
        /* ID info for an object is DB OID + CLASS OID + OBJECT OID + SUBID */  
        /*  
         * Note: object ID has same representation as in pg_depend and  
         * pg_description, but notice that we are constraining SUBID to 16 bits.  
         * Also, we use DB OID = 0 for shared objects such as tablespaces.  
         */  
        LOCKTAG_USERLOCK,                       /* reserved for old contrib/userlock code */  
        LOCKTAG_ADVISORY                        /* advisory user locks */  
} LockTagType;  
```  
以下这个macro则是用来设置tuple锁对象的值的.  
```  
#define SET_LOCKTAG_TUPLE(locktag,dboid,reloid,blocknum,offnum) \  
        ((locktag).locktag_field1 = (dboid), \  
         (locktag).locktag_field2 = (reloid), \  
         (locktag).locktag_field3 = (blocknum), \  
         (locktag).locktag_field4 = (offnum), \  
         (locktag).locktag_type = LOCKTAG_TUPLE, \  
         (locktag).locktag_lockmethodid = DEFAULT_LOCKMETHOD)  
```  
LOCK的锁信息存储在共享内存中的hash表中, 键值为LOCKTAG 进行检索.  
为了减少冲突, hash表做了分区处理 :   
```  
* The shared-memory hash tables for LOCKs and PROCLOCKs are organized so that different partitions use different hash chains,   
and thus there is no conflict in working with objects in different partitions.    
This is supported directly by dynahash.c's "partitioned table" mechanism for the LOCK table:   
we need only ensure that the partition number is taken from the low-order bits of the dynahash hash value for the LOCKTAG.  
To make it work for PROCLOCKs, we have to ensure that a PROCLOCK's hash value has the same low-order bits as its associated LOCK.    
This requires a specialized hash function (see proclock_hash).  
除了共享的lock hash table之外, 每个bacnekd 还维护非共享的本地hash table. 包含了已经获得的锁信息以及请求中的锁信息.  
Each backend also maintains an unshared LOCALLOCK structure for each lockable  
object and lock mode that it is currently holding or requesting.  The shared  
lock structures only allow a single lock grant to be made per lockable  
object/lock mode/backend.  Internally to a backend, however, the same lock may  
be requested and perhaps released multiple times in a transaction, and it can  
also be held both transactionally and session-wide.  The internal request  
counts are held in LOCALLOCK so that the shared data structures need not be  
accessed to alter them.  
```  
数据结构如下 :   
```  
/*  
 * Each backend also maintains a local hash table with information about each  
 * lock it is currently interested in.  In particular the local table counts  
 * the number of times that lock has been acquired.  This allows multiple  
 * requests for the same lock to be executed without additional accesses to  
 * shared memory.  We also track the number of lock acquisitions per  
 * ResourceOwner, so that we can release just those locks belonging to a  
 * particular ResourceOwner.  
 */  
typedef struct LOCALLOCKTAG  
{  
        LOCKTAG         lock;                   /* identifies the lockable object */  
        LOCKMODE        mode;                   /* lock mode for this table entry */  
} LOCALLOCKTAG;  
typedef struct LOCALLOCKOWNER  
{  
        /*  
         * Note: if owner is NULL then the lock is held on behalf of the session;  
         * otherwise it is held on behalf of my current transaction.  
         *  
         * Must use a forward struct reference to avoid circularity.  
         */  
        struct ResourceOwnerData *owner;  
        int64           nLocks;                 /* # of times held by this owner */  
} LOCALLOCKOWNER;  
typedef struct LOCALLOCK  
{  
        /* tag */  
        LOCALLOCKTAG tag;                       /* unique identifier of locallock entry */  
        /* data */  
        LOCK       *lock;                       /* associated LOCK object in shared mem */  
        PROCLOCK   *proclock;           /* associated PROCLOCK object in shmem */  
        uint32          hashcode;               /* copy of LOCKTAG's hash value */  
        int64           nLocks;                 /* total number of times lock is held */  
        int                     numLockOwners;  /* # of relevant ResourceOwners */  
        int                     maxLockOwners;  /* allocated size of array */  
        bool            holdsStrongLockCount;   /* bumped FastPathStrongRelatonLocks? */  
        LOCALLOCKOWNER *lockOwners; /* dynamically resizable array */  
} LOCALLOCK;  
#define LOCALLOCK_LOCKMETHOD(llock) ((llock).tag.lock.locktag_lockmethodid)  
```  
因为不涉及冲突, 所以本地hash table不需要做分区处理. 另外就是数据结构相比LOCK和LOCKTAG简化了许多. 但是同样会消耗内存.  
接下来看看PostgreSQL在哪里调用了 SET_LOCKTAG_TUPLE 来请求tuple lock.  
```  
/*  
 *              LockTuple  
 *  
 * Obtain a tuple-level lock.  This is used in a less-than-intuitive fashion  
 * because we can't afford to keep a separate lock in shared memory for every  
 * tuple.  See heap_lock_tuple before using this!  
 */  
void  
LockTuple(Relation relation, ItemPointer tid, LOCKMODE lockmode)  
{  
        LOCKTAG         tag;  
        SET_LOCKTAG_TUPLE(tag,  
                                          relation->rd_lockInfo.lockRelId.dbId,  
                                          relation->rd_lockInfo.lockRelId.relId,  
                                          ItemPointerGetBlockNumber(tid),  
                                          ItemPointerGetOffsetNumber(tid));  
        (void) LockAcquire(&tag, lockmode, false, false);  
}  
```  
注意到, we can't afford to keep a separate lock in shared memory for every tuple. 为什么呢?  
```  
 * NOTES: because the shared-memory lock table is of finite size, but users  
 * could reasonably want to lock large numbers of tuples, we do not rely on  
 * the standard lock manager to store tuple-level locks over the long term.  
```  
全文 :   
```  
/*  
 *      heap_lock_tuple - lock a tuple in shared or exclusive mode  
 *  
 * Note that this acquires a buffer pin, which the caller must release.  
 *  
 * Input parameters:  
 *      relation: relation containing tuple (caller must hold suitable lock)  
 *      tuple->t_self: TID of tuple to lock (rest of struct need not be valid)  
 *      cid: current command ID (used for visibility test, and stored into  
 *              tuple's cmax if lock is successful)  
 *      mode: indicates if shared or exclusive tuple lock is desired  
 *      nowait: if true, ereport rather than blocking if lock not available  
 *  
 * Output parameters:  
 *      *tuple: all fields filled in  
 *      *buffer: set to buffer holding tuple (pinned but not locked at exit)  
 *      *ctid: set to tuple's t_ctid, but only in failure cases  
 *      *update_xmax: set to tuple's xmax, but only in failure cases  
 *  
 * Function result may be:  
 *      HeapTupleMayBeUpdated: lock was successfully acquired  
 *      HeapTupleSelfUpdated: lock failed because tuple updated by self  
 *      HeapTupleUpdated: lock failed because tuple updated by other xact  
 *  
 * In the failure cases, the routine returns the tuple's t_ctid and t_xmax.  
 * If t_ctid is the same as t_self, the tuple was deleted; if different, the  
 * tuple was updated, and t_ctid is the location of the replacement tuple.  
 * (t_xmax is needed to verify that the replacement tuple matches.)  
 *  
 *  
 * NOTES: because the shared-memory lock table is of finite size, but users  
 * could reasonably want to lock large numbers of tuples, we do not rely on  
 * the standard lock manager to store tuple-level locks over the long term.  
 * Instead, a tuple is marked as locked by setting the current transaction's  
 * XID as its XMAX, and setting additional infomask bits to distinguish this  
 * usage from the more normal case of having deleted the tuple.  When  
 * multiple transactions concurrently share-lock a tuple, the first locker's  
 * XID is replaced in XMAX with a MultiTransactionId representing the set of  
 * XIDs currently holding share-locks.  
 *  
 * When it is necessary to wait for a tuple-level lock to be released, the  
 * basic delay is provided by XactLockTableWait or MultiXactIdWait on the  
 * contents of the tuple's XMAX.  However, that mechanism will release all  
 * waiters concurrently, so there would be a race condition as to which  
 * waiter gets the tuple, potentially leading to indefinite starvation of  
 * some waiters.  The possibility of share-locking makes the problem much  
 * worse --- a steady stream of share-lockers can easily block an exclusive  
 * locker forever.      To provide more reliable semantics about who gets a  
 * tuple-level lock first, we use the standard lock manager.  The protocol  
 * for waiting for a tuple-level lock is really  
 *              LockTuple()  
 *              XactLockTableWait()  
 *              mark tuple as locked by me  
 *              UnlockTuple()  
 * When there are multiple waiters, arbitration of who is to get the lock next  
 * is provided by LockTuple().  However, at most one tuple-level lock will  
 * be held or awaited per backend at any time, so we don't risk overflow  
 * of the lock table.  Note that incoming share-lockers are required to  
 * do LockTuple as well, if there is any conflict, to ensure that they don't  
 * starve out waiting exclusive-lockers.  However, if there is not any active  
 * conflict for a tuple, we don't incur any extra overhead.  
 */  
```  
在分析PostgreSQL怎么处理行锁前. 首先我们要分析的是tuple head 结构, 取自src/include/access/htup.h, 如下 :   
```  
typedef struct HeapTupleHeaderData  
{  
        union  
        {  
                HeapTupleFields t_heap;  
                DatumTupleFields t_datum;  
        }                       t_choice;  
        ItemPointerData t_ctid;         /* current TID of this or newer tuple */  
        /* Fields below here must match MinimalTupleData! */  
        uint16          t_infomask2;    /* number of attributes + various flags */  
        uint16          t_infomask;             /* various flag bits, see below */  
        uint8           t_hoff;                 /* sizeof header incl. bitmap, padding */  
        /* ^ - 23 bytes - ^ */  
        bits8           t_bits[1];              /* bitmap of NULLs -- VARIABLE LENGTH */  
        /* MORE DATA FOLLOWS AT END OF STRUCT */  
} HeapTupleHeaderData;  