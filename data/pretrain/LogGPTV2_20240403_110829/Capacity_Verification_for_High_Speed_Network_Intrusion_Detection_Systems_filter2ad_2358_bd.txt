rely on real world attacks on our specific applications. There are simply not enough of
them in circulation to give us the breadth of attack types that we need for the research.
6.2   Theory Improvements
As  Krsul  [14]  states,  “Making  sense  of  apparent  chaos  by  finding  regularities  is  an
essential characteristic of human beings.” He laid out the essential characteristics of
successful  taxonomies:  (1)  They  should  have  explanatory  and  predictive  value.  (2)
Computer  vulnerability  taxonomies  should  classify  the  features  or  attributes  of  the
vulnerabilities,  not  the  vulnerabilities  themselves.  (3)  Their  classes  should  be
mutually exclusive and collectively exhaustive. (4) Each level or division should have
a  fundamentum  divisionis  or  basis  for  distinction  so  that  an  entity  can  be
174
J.E. Just et al.
unequivocally  placed 
the  other.  (5)  The  classification
characteristics should be objective, deterministic, repeatable, and  specific.  Note  that
item (3) above is very difficult to achieve in practice outside the realm of mathematics
and should be probably be replaced by extensibility as a goal. 
in  one  category  or 
Krsul developed a very extensive list of classes particularly focused on erroneous
environmental assumptions. Unfortunately, his and most of the previous efforts (see
review  by  Lough  [15])  on  developing  taxonomies  have  focused  on  identifying  and
characterizing vulnerabilities in source code so that programmers could identify and
eliminate  them  before  the  software  was  deployed.  At  one  level  this  are  fine  in  that
they can give us insights into types of vulnerabilities. For example, the classic study
by  Landwehr  et  al.  [16] 
inadvertent  software
vulnerabilities:
·  Validation error (incomplete/inconsistent)
·  Domain  error  (including  object  re-use,  residuals,  and  exposed  representation
the  following 
lists 
type  of 
errors)
·  Serialization/aliasing (including TOCTTOU errors)
·  Identification/authentication errors
·  Boundary  condition  violation  (including  resource  exhaustion  and  violable
constraint errors)
·  Other exploitable logic errors
While these are important efforts and give us insights, we really need a taxonomy
of remote access attacks, particularly one that characterizes the initiating events that
can be exploited via network-based attacks on COTS or GOTS software.
Since our focus is on unknown network-based attacks, recent work by Richardson
[17]  is  of  interest.  He  developed  a  taxonomy  for  DoS  attacks  that  identifies  the
following attack mechanisms:
1. Buffer overflows
2. IP fragmentation attacks
3. Other incorrect data attacks
4. Overwhelm with service requests
5. Overwhelm with data
6. Poor authentication or access control 
·  Poor authentication scheme
·  IP spoofing
·  Data poisoning
·  Other miscellaneous protection shortcomings)
These  categories  will  be  informed  by  other  studies  of  taxonomies  [e.g.,  18,  19].
The  results  will  form  the  initial  basis  for  our  categorization  of  initiating  events  of
unknown attacks. Priorities will be given to those attacks that are known not to have
adequate protection measures built into the cluster currently and for which there are
not easy fixes to the design that would prevent them. For example, IP fragmentation
attacks against the primary can be prevented with a proxy on the firewall or gateway
and IP spoofing is prevented by the VPN.
Learning Unknown Attacks – A Start
175
7   Conclusions and Recommendations
Our design for an intrusion tolerant server cluster uses a behavior specification-based
approach to identify errors and failover to the hot spare. It then uses fault diagnosis to
recognize the attack that caused failover  (or violated  QoS)  and block  it  so  repeated
attacks won't defeat us again. We learn exact attacks by testing entries from complete
log files in a “Sandbox” until we duplicate the observed failure. Single stage attacks
can be recognized in seconds, automatically. 
We have demonstrated that it is possible to generalize web server buffer overflow
attack signatures after the initial identified attack so that simple variants that exploit
the same vulnerability will be blocked also. We do this using a similarity measure for
the class of attack. We have implemented rules that generalize a large subset of buffer
overflow  attacks  aimed  at  web  servers  and  have  tested  it  using  the  Internet
Information Server (IIS) by Microsoft, and believe that it will also work for Apache
and other web servers also. For buffer overflow attacks, which have become the most
common  type  of  attack,  we  can  also  learn  the  minimum  length  of  the  request  that
causes the buffer overflow. This is important to minimize the probability of blocking
legitimate transactions, i.e., the false positive rate.
We believe this knowledge-based learning is broadly applicable to many classes of
remote access attacks and has significant uses outside of intrusion tolerance. We also
believe that the generalization approach can be significantly extended to other classes
of attack. The key, we believe, is generalizing an attack pattern to protect against all
variants that exploit the same vulnerability rather than trying to generalize a specific
attack  to  protect  against  all  such  attacks  in  the  class.  The  ease  of  generalizing  an
attack pattern should be proportional to the ease of creating simple attack variants that
work against the same vulnerability.
In  summary,  we  have  developed  an  approach  to  dynamic  learning  of  unknown
attacks that shows great promise. We have also implemented a proof of concept for
generalization that works for a significant class of buffer overflow attacks against web
servers  on  Microsoft  NT/2000.  Our  results  so  far  indicate  that  the  generalization
algorithms will be specific to particular types of attacks (such as buffer overflow), to
particular protocols (such as http) and to particular application classes. More work is
needed to determine whether they must be specific to particular applications but that
is a likely outcome if the application class is not dominated by standard protocols. 
We recommend that other researchers examine this knowledge-based approach to
identifying unknown attacks. We hope they find it useful enough to apply it to other
areas.
References
1. Schneier, B: Secrets and Lies: Digital Security in a Networked World. John Wiley & Sons,
Inc., 2000 206, 210
2. Gray, J., Reuter, A.: Transaction Processing: Concepts and Techniques. Morgan Kaufmann
Publishers, San Francisco, CA, 1993 107
3. Lampson, B.: Computer Security in the Real World. Invited essay at 16th Annual Computer
Security  Applications  Conference,  11–15  December,  New  Orleans,  LA,  available  at
http://www.acsac.org/2000/papers/lampson.pdf
176
J.E. Just et al.
4.
Just, J.E., et al.: Intelligent Control for Intrusion Tolerance of Critical Application Services.
Supplement of the 2001 International Conference on Dependable Systems and Networks,
1–4 July 2001, Gothenburg, SW
5. Reynolds, J., et al.: Intrusion Tolerance for Mission-Critical Services. Proceedings of the
2001 IEEE Symposium on Security and Privacy, May 13–16, 2001, Oakland, CA
6. Reynolds,  J.,  et  al.:  The  Design  and  Implementation  of  an  Intrusion  Tolerant  System.
Proceedings of the 2002 International Conference on Dependable Systems and Networks,
23–26 June 2002, Washington, DC, pending
7. Ko,  Calvin:  Logic  Induction  of  Valid  Behavior  Specifications  for  Intrusion  Detection.
IEEE Symposium on Security and Privacy 2000: 142–153
8. Ko, Calvin, Brutch, Paul, et al.: System Health and Intrusion Monitoring Using a Hierarchy
of Constraints. Recent Advances in Intrusion Detection 2001: 190–204
9. Balzer,  R.,  and  Goldman,  N.:  Mediating  Connectors.  Proceedings  of  the  19th  IEEE
International Conference on Distributed Computing Systems, Austin, Texas, May 31-June
4, 1999, IEEE Computer Society Press 73-77
10. Strunk,  J.D.,  et  al.:  Self-securing  storage:  Protecting  data  in  compromised  system.
Operating  Systems  Design  and  Implementation,  San  Diego,  CA,  23–25  October  2000,
USENIX Association, 2000 165–180
11. Ganger,  G.R.,  et  al.:  Survivable  Storage  Systems.  DARPA  Information  Survivability
Conference and Exposition (Anaheim, CA, 12-14 June 2001), pages 184–195 vol 2. IEEE,
2001
12. Russell,  S.,  Norvig,  P,:  Artificial  Intelligence:  A  Modern  Approach.  Prentice  Hall,  New
York, 1995 
13. Roesch,  M.:  Snort  Users  Manual,  Snort  Release:  1.8.3.  November  6,  2001,  available  at
http://www.snort.org/docs/writing_rules/
14.  Krsul,  I.V.:  Software  Vulnerability  Analysis.  PhD  thesis,  Purdue  University,  West
Lafayette, IN, May, 1998, p. 17, available at
https://www.cerias.purdue.edu/techreports-ssl/public/97-05.pdf 
15. Lough, D.L.: A Taxonomy of Computer Attacks with Applications to Wireless Networks.
PhD  Thesis,  Virginia  Polytechnic  and  State  University,  Blackburg,  VA,  available  at
http://scholar.lib.vt.edu/theses/available/etd-04252001-234145/
16. Landwehr, C. E., Bull, A. R., McDermott, J. P., Choi, W. S.: A Taxonomy of Computer
Program  Security  Flaws.  ACM  Computing  Surveys,  Volume  26,  Number  3,  September
1994
17. Richardson,  T.W.:  The  Development  of  a  Database  Taxonomy  of  Vulnerabilities  to
Support the Study of Denial of Service Attacks. PhD thesis, Iowa State University, 2001
18.  Aslam, T.: A Taxonomy of Security Faults in the Unix Operating System. Master's Thesis,
Purdue University, Department of Computer Sciences, August 1995.
http://citeseer.nj.nec.com/aslam95taxonomy.html
19. Du, W. and Mathur, A.: Categorization of Software Error that Led to Security Breaches.
Technical Report 97-09, Purdue University, Department of Computer Science, 1997
Evaluation of the Diagnostic Capabilities
of Commercial Intrusion Detection Systems
Herv´e Debar and Benjamin Morin
France T´el´ecom R&D, 42 rue des Coutures, F-14000 Caen
{herve.debar, benjamin.morin}@francetelecom.com
Abstract. This paper describes a testing environment for commercial
intrusion-detection systems, shows results of an actual test run and
presents a number of conclusions drawn from the tests. Our test envi-
ronment currently focuses on IP denial-of-service attacks, Trojan horse
traﬃc and HTTP traﬃc. The paper focuses on the point of view of
an analyst receiving alerts sent by intrusion-detection systems and the
quality of the diagnostic provided. While the analysis of test results does
not solely targets this point of view, we feel that the diagnostic accu-
racy issue is extremely relevant for the actual success and usability of
intrusion-detection technology. The tests show that the diagnostic pro-
posed by commercial intrusion-detection systems sorely lack in precision
and accuracy, lacking the capability to diagnose the multiple facets of
the security issues occurring on the test network. In particular, while
they are sometimes able to extract multiple pieces of information from
a single malicious event, the alerts reported are not related to one an-
other in any way, thus loosing signiﬁcant background information for an
analyst. The paper therefore proposes a solution for improving current
intrusion-detection probes to enhance the diagnostic provided in the case
of an alert, and qualifying alerts in relation to the intent of the attacker
as perceived from the information acquired during analysis.
1 Introduction
There have been a small number of publications on testing intrusion-detection
systems, but we believe that important results have been left out of these pub-
lications. The emphasis of this work is quality evaluation. Our most important
objective is the provision of a detailed and accurate diagnostic of the mali-
cious activity occurring on our networks. Network operators with little security
background operate the probes and handle daily alert traﬃc. Serious security
breaches are left to trained analysts. These analysts are a scarce and valuable
resource and must spend as little time as possible handling incidents. Therefore,
alert information must be detailed and accurate to ensure that the analyst does
not need to go back to raw data.
After a literature survey getting information from vendors and the commu-
nity about multiple intrusion-detection products, we selected a small number of
them for internal testing and comparative evaluation. The study was restricted
A. Wespi, G. Vigna, and L. Deri (Eds.): RAID 2002, LNCS 2516, pp. 177–198, 2002.
c(cid:1) Springer-Verlag Berlin Heidelberg 2002
178
H. Debar and B. Morin
in scope in order to provide manageable results, and our interest focused on
network-based intrusion-detection commercial products, with probe components
available worldwide as a remotely manageable appliance. We deployed four com-
mercial intrusion-detection systems on a test bed and carried out a comparative
evaluation. Partial results from this evaluation are presented in the paper.
The remainder of the paper is organized as follows. Section 2 presents the
goals and organisation of the tests. Section 3 presents our testing principles and
our test bed. Section 4 presents the results obtained and the lessons that we
learned from the tests. Section 5 proposes an enhanced model for an intrusion-
detection system that would emphasize diagnostic accuracy.
2 Background on Testing Intrusion-Detection Systems
A number of papers related to testing intrusion-detection systems have been
published in the literature. We ignore testing methodologies proposed by the
developers of a given IDS method or tool, because we consider them biased
towards enhancing the performances of their tools. During our literature survey,
we found three independent testing methodologies close to our preoccupations
and studied them.
2.1 The Lincoln Lab Experiments
One of the best-known testing experiments in the intrusion-detection community
is the Lincoln Lab experiment [7], analysed by McHugh[8]. The purpose of this
experiment is to fairly compare the various intrusion-detection technologies de-
veloped under DARPA funding. A network of workstations creates normal traﬃc
and a set of attacks is inserted in the traﬃc. We have the following concerns with
this test environment:
Focus on background traﬃc: As the test includes anomaly detection tools,
realistic background data must be generated to ensure that these systems will
be properly trained. None of the products tested includes anomaly detection
features, and our need for background traﬃc is limited to performance eval-
uation. Our test bed includes the generation of proﬁled background traﬃc,
geared towards maximizing the workload of the tested products.
Focus on research prototypes: The Lincoln Lab tests were commissioned by
DARPA to evaluate DARPA-funded research work. No commercial IDS was
ever taken into account. For example, intrusion-detection products provide
conﬁguration management features, that we want to evaluate, and these
aspects are not available with the Lincoln Lab tests. Our test bed includes
reporting on the installation, management and integration for each tool.
Focus on a broad set of attacks: The Lincoln Lab tests aim at exercising
the largest possible set of attacks for the largest possible set of intrusion-
detection systems. Our objective is to focus on network traﬃc close to ﬁre-
walls; therefore the Lincoln Lab tests are too wide for our use. Also, attacks
Evaluation of the Diagnostic Capabilities
179
that are qualiﬁed as local to root are less relevant in telecommunication en-
vironment where monitoring is located on the wires. Our test bed focuses
on speciﬁc types of applications that are representative of the traﬃc proﬁles
seen on our networks.
Lack of reference point or baseline: The Lincoln Lab tests compare proto-
types in a closed circle. There is no notion of a minimal set of requirements
that a tested tool has to meet, only relative data comparing them with each
other. Our test bed uses Snort[14] as a baseline.
The lack of a baseline that all tools would have to fulﬁl was felt as particularly
lacking in the Lincoln Lab experiment.
2.2 The University and Research Work
The most representative work concerning university tests has been carried at
UCDavis [12,11], with related and de facto similar work at IBM Zurich [4]. The
UC Davis tests simulate the activity of normal users and the activity of attackers,
using script written in the Expect language. Expect simulates the presence of a
user by taking over the input and output streams of a tty-based application,
matching expected responses from the application with the displayed output,
and feeding appropriate input as if the user typed it on its keyboard.
A similar approach was followed at IBM Zurich; in addition to user-recorded
scripts the IBM approach introduced software testing scripts from the DejaGnu
platform (also in Expect) to ensure that all aspects of an application were exer-
cised, regardless of the fact that they were obscure features of an application or
not. This testing methodology is closer to our own. In particular, the fact that
tests are automated and reproducible is a very important property. However,
the following points reduce the eﬀectiveness of the test environment.
Heavy to manage: To achieve a signiﬁcant level of background activity, the
test bed must contain a signiﬁcant number of machines, each of them piloting
a number of users generating activity. This creates a complex environment
to manage, and induces the risk of repetitiveness. Also, calibration to obtain
data points at regularly spaced traﬃc rates is not taken into account.
Our test bed includes centralized distribution of software and management
scripts that automate test run and result analysis.
Limited in attack testing: Requiring that attacks be scriptable using Expect
makes it unfeasible to use a number of exploit scripts collected from “under-
ground” sources. Also, verifying the actual execution of each attack, verify
its eﬀect, and correlate with the IDS system is a manual process.
Our test bed does not solve this issue; installing and conﬁguring the vulnera-
ble software, meeting preconditions, running the attack, verifying the results
and restoring the environment for future tests is manual as well.
Applicability to commercial tools: The UC Davis test bed has been de-
signed primarily for research prototypes, and no information is available as
to how commercial intrusion-detection systems would be included in such
testing.
180
H. Debar and B. Morin
2.3 Commercial Tests
Tests classiﬁed as commercial regroup the tests published by commercial test
laboratories mandated by a particular company, and tests carried out by inde-
pendent publications.
Several test labs have published test results related to intrusion-detection
systems. In particular, Mier Communications has released at least two test re-
ports, a comparative of BlackICE Sentry, RealSecure and NetProwler on one
hand, and a test of Intrusion.com SecurenetPro Gigabit appliance. Appropriate
queries on mailing list archives show that these test results have been the subject
of many controversies. The general feeling is that it is appropriate for the test-