and a 32K entry SRAM for ﬂow table entries. Each incoming
packet’s header is matched against 10 ﬁelds in the Ethernet,
IP and TCP/UDP headers for a match in the two hardware
ﬂow tables. Each TCAM and SRAM entry is associated with
an action, e.g., forward the packet along an output port or
to the switch software. TCAM entries may contain “don’t
care” bits while SRAM matches must be exact.
4.2 System Architecture
PortLand intercepts all ARP requests and IGMP join re-
quests at the edge switch and forwards them to the local
switch software module running separately on the PC host-
ing each NetFPGA. The local switch module interacts with
the OpenFlow fabric manager to resolve ARP requests and
to manage forwarding tables for multicast sessions. The ﬁrst
few packets for new ﬂows will miss in hardware ﬂow tables
and will be forwarded to the local switch module as a re-
sult. The switch module uses ECMP style hashing to choose
Figure 7: System architecture.
among available forwarding paths in the switch and inserts
a new ﬂow table entry matching the ﬂow. On receiving fail-
ure and recovery notiﬁcations from the fabric manager, each
switch recalculates global connectivity and modiﬁes the ap-
propriate forwarding entries for the aﬀected ﬂows through
the switch.
The OpenFlow fabric manager monitors connectivity with
each switch module and reacts to the liveness information
by updating its fault matrix. Switches also send keepalives
to their immediate neighbors every 10ms. If no keepalive is
received after 50ms, they assume link failure and update the
fabric manager appropriately.
Figure 7 shows the system architecture. OpenFlow switch
modules run locally on each switch. The fabric manager
transmits control updates using OpenFlow messages to each
switch. In our testbed, a separate control network supports
communication between the fabric manager and local switch
modules. It is of course possible to run the fabric manager
simply as a separate host on the data plane and to communi-
cate inband. The cost and wiring for a separate lower-speed
control network will actually be modest. Consider a control
network for a 2,880-switch data center for the k = 48 case.
Less than 100 low-cost, low-speed switches should suﬃce
to provide control plane functionality. The real question is
whether the beneﬁts of such a dedicated network will justify
the additional complexity and management overhead.
Fabric ManagerOpenFlow ProtocolNetlink IPCPCI busUser SpaceKernel SpaceHardwareLocal Switch ModuleLocal Switch ModuleLocal Switch ModuleDevice DriverDevice DriverDevice DriverNetFPGANetFPGANetFPGA47Table 2 summarizes the state maintained locally at each
switch as well as the fabric manager. Here
k = N umber of ports on the switches,
m = N umber of local multicast groups,
p = N umber of multicast groups active in the system.
State
Connectivity Matrix
Multicast Flows
IP → P M AC mappings
Switch
O(k3/2)
O(m)
O(k/2)
Fabric Manager
O(k3/2)
O(p)
O(k3/4)
Table 2: State requirements.
5. EVALUATION
In this section, we evaluate the eﬃciency and scalability
of our implementation. We describe the experiments carried
out on our system prototype and present measurements to
characterize convergence and control overhead for both mul-
ticast and unicast communication in the presence of link
failures. We ran all experiments on our testbed described in
Section 4.
Figure 9: TCP convergence.
Figure 8: Convergence time with increasing faults.
Convergence Time With Increasing Faults.
We measured convergence time for a UDP ﬂow while intro-
ducing a varying number of random link failures. A sender
transmits packets at 250Mbps to a receiver in a separate
pod. In the case where at least one of the failures falls on
the default path between sender and receiver, we measured
the total time required to re-establish communication.
Figure 8 plots the average convergence time across 20 runs
as a function of the number of randomly-induced failures.
Total convergence time begins at about 65ms for a single
failure and increases slowly with the number of failures as a
result of the additional processing time.
TCP convergence.
We repeated the same experiment for TCP communica-
tion. We monitored network activity using tcpdump at the
sender while injecting a link failure along the path between
sender and receiver. As illustrated in Figure 9, convergence
for TCP ﬂows takes longer than the baseline for UDP de-
spite the fact that the same steps are taken in the underlying
network. This discrepancy results because TCP loses an en-
tire window worth of data. Thus, TCP falls back to the
Figure 10: Multicast convergence.
retransmission timer, with TCP’s RTOmin set to 200ms in
our system. By the time the ﬁrst retransmission takes place,
connectivity has already been re-established in the underly-
ing network.
Multicast Convergence.
We further measured the time required to designate a new
core when one of the subscribers of a multicast group loses
connectivity to the current core. For this experiment, we
used the same conﬁguration as in Figure 5. In this case, the
sender transmits a multicast ﬂow to a group consisting of 3
subscribers, augmenting each packet with a sequence num-
ber. As shown in Figure 10, 4.5 seconds into the experiment
we inject two failures (as depicted in Figure 5), causing one
of the receivers to lose connectivity. After 110ms, connectiv-
ity is restored. In the intervening time, individual switches
detect the failures and notify the fabric manager, which in
turn reconﬁgures appropriate switch forwarding tables.
Scalability.
One concern regarding PortLand design is scalability of
the fabric manager for larger topologies. Since we do not
have a prototype at scale, we use measurements from our
existing system to project the requirements of larger sys-
tems. Figure 11 shows the amount of ARP control traﬃc
the fabric manager would be expected to handle as a func-
tion of overall cluster size. One question is the number of
020406080100120140012345678910Convergence time (ms)Number of random failures4448525660646872768084889215.3815.4815.5815.6815.7815.8815.98Sequence NumberTime(s)FailureRecoveryRTOmin = 200ms8808909009109209309409504.24.34.44.54.64.74.84.9Sequence NumberTime(s)FailureRecovery48Figure 11: Fabric manager control traﬃc.
Figure 12: CPU requirements for ARP requests.
ARPs transmitted per host. Since we are interested in scal-
ability under extreme conditions, we considered cases where
each host transmitted 25, 50 and 100 ARP requests/sec to
the fabric manager. Note that even 25 ARPs/sec is likely to
be extreme in today’s data center environments, especially
considering the presence of a local ARP cache with a typ-
ical 60-second timeout. In a data center with each of the
27,648 hosts transmitting 100 ARPs per second, the fabric
manager must handle a manageable 376Mbits/s of control
traﬃc. More challenging is the CPU time required to han-
dle each request. Our measurements indicate approximately
25 µs of time per request in our non-optimized implementa-
tion. Fortunately, the work is highly parallelizable, making
it amenable to deployment on multiple cores and multiple
hardware thread contexts per core. Figure 12 shows the
CPU requirements for the fabric manager as a function of
the number of hosts in the data center generating diﬀerent
numbers of ARPs/sec. For the highest levels of ARPs/sec
and large data centers, the required level of parallelism to
keep up with the ARP workload will be approximately 70
independent cores. This is beyond the capacity of a single
modern machine, but this also represents a relatively signif-
icant number of ARP misses/second. Further, it should be
possible to move the fabric manager to a small-scale cluster
(e.g., four machines) if absolutely necessary when very high
frequency of ARP requests is anticipated.
Figure 13: State and TCP application transfer dur-
ing VM migration.
VM Migration.
Finally, we evaluate PortLand’s ability to support virtual
machine migration. In this experiment, a sender transmits
data at 150 Mbps to a virtual machine (hosted on Xen)
running on a physical machine in one pod. We then mi-
grate the virtual machine to a physical machine in another
pod. On migration, the host transmits a gratuitous ARP
with its new MAC address, which is in turn forwarded to all
hosts communicating with that VM by the previous egress
switch. The communication is not at line-rate (1 Gbps)
since we use software MAC layer rewriting capability pro-
vided by OpenFlow to support PMAC and AMAC transla-
tion at edge switches. This introduces additional per packet
processing latency. Existing commercial switches have MAC
layer rewriting support directly in hardware [2].
Figure 13 plots the results of the experiment with mea-
sured TCP rate for both state transfer and ﬂow transfer
(measured at the sender) on the y-axis as a function of time
progressing on the x-axis. We see that 5+ seconds into the
experiment, throughput of the tcp ﬂow drops below the peak
rate as the state of the VM begins to migrate to a new phys-
ical machine. During migration there are short time periods
(200-600ms) during which the throughput of the ﬂow drops
to near zero (not visible in the graph due to the scale). Com-
munication resumes with the VM at full speed after approx-
imately 32 seconds (dominated by the time to complete VM
state transfer).
6. CONCLUSIONS
The goal of this work is to explore the extent to which
entire data center networks may be treated as a single plug-
and-play fabric. Modern data centers may contain 100,000
hosts and employ virtual machine multiplexing that results
in millions of unique addressable end hosts. Eﬃciency, fault
tolerance, ﬂexibility and manageability are all signiﬁcant
concerns with general-purpose Ethernet and IP-based proto-
cols. In this paper, we present PortLand, a set of Ethernet-
compatible routing, forwarding, and address resolution pro-
tocols speciﬁcally tailored for data center deployments. It
is our hope that through protocols like PortLand, data cen-
ter networks can become more ﬂexible, eﬃcient, and fault
tolerant.
05010015020025030035040012851281012815128201282512830128Control traffic (Mbps)Number of hosts100 ARPs/sec/host50 ARPs/sec/host25 ARPs/sec/host01020304050607012851281012815128201282512830128Cores requiredNumber of hosts100 ARPs/sec/host50 ARPs/sec/host25 ARPs/sec/host050100150200250051015202530354045Throughput (Mbps)Time (Seconds)TCP flow transferState transfer497. REFERENCES
[1] Cisco Data Center Infrastructure 2.5 Design Guide.
www.cisco.com/application/pdf/en/us/guest/netsol/
ns107/c649/ccmigration_09186a008073377d.pdf.
[2] Conﬁguring IP Unicast Layer 3 Switching on Supervisor
Engine 2. www.cisco.com/en/US/docs/routers/7600/ios/
12.1E/configuration/guide/cef.html.
[3] Inside Microsoft’s $550 Million Mega Data Centers.
www.informationweek.com/news/hardware/data_centers/
showArticle.jhtml?articleID=208403723.
[4] OpenFlow. www.openflowswitch.org/.
[5] OSPF Design Guide.
www.ciscosystems.com/en/US/tech/tk365/
technologies_white_paper09186a0080094e9e.shtml.
[6] M. Al-Fares, A. Loukissas, and A. Vahdat. A Scalable,
Commodity Data Center Network Architecture. In
SIGCOMM ’08: Proceedings of the ACM SIGCOMM 2008
conference on Data communication, pages 63–74, New
York, NY, USA, 2008. ACM.
[7] M. Caesar, D. Caldwell, N. Feamster, J. Rexford,
A. Shaikh, and J. van der Merwe. Design and
Implementation of a Routing Control Platform. In
USENIX Symposium on Networked Systems Design &
Implementation, 2005.
[8] M. Caesar, M. Castro, E. B. Nightingale, G. O, and
A. Rowstron. Virtual Ring Routing: Network Routing
Inspired by DHTs. In Proceedings of ACM SIGCOMM,
2006.
[9] M. Caesar, T. Condie, J. Kannan, K. Lakshminarayanan,
I. Stoica, and S. Shenker. ROFL: Routing on Flat Labels.
In Proceedings of ACM SIGCOMM, 2006.
[10] M. C. Changhoon Kim and J. Rexford. Floodless in
SEATTLE: A Scalable Ethernet Architecture for Large
Enterprises. In SIGCOMM ’08: Proceedings of the ACM
SIGCOMM 2008 conference on Data communication, 2008.
[11] C. Clark, K. Fraser, S. Hand, J. G. H. E. J. C. Limpach,
I. Pratt, and A. Warﬁeld. Live Migration of Virtual
Machines. In USENIX Symposium on Networked Systems
Design & Implementation, 2005.
[12] J. Dean and S. Ghemawat. MapReduce: Simpliﬁed Data
Processing on Large Clusters. In OSDI’04: Proceedings of
the 6th conference on Symposium on Operating Systems
Design & Implementation, pages 10–10, Berkeley, CA,
USA, 2004. USENIX Association.
[13] S. Ghemawat, H. Gobioﬀ, and S.-T. Leung. The Google
File System. ACM SIGOPS Operating Systems Review,
37(5), 2003.
[14] A. Greenberg, P. Lahiri, D. A. Maltz, P. Patel, and
S. Sengupta. Towards a Next Generation Data Center
Architecture: Scalability and Commoditization. In
PRESTO ’08: Proceedings of the ACM Workshop on
Programmable Routers for Extensible Services of
Tomorrow, pages 57–62, New York, NY, USA, 2008. ACM.
[15] C. Guo, H. Wu, K. Tan, L. Shi, Y. Zhang, and S. Lu.
DCell: A Scalable and Fault-tolerant Network Structure for
Data Centers. In Proceedings of the ACM SIGCOMM 2008
conference on Data communication, pages 75–86, New
York, NY, USA, 2008. ACM.
[16] C. Hopps. Analysis of an Equal-Cost Multi-Path Algorithm.
RFC 2992, Internet Engineering Task Force, 2000.
[17] K. Lakshminarayanan, M. Caesar, M. Rangan,
T. Anderson, S. Shenker, I. Stoica, and H. Luo. Achieving
Convergence-Free Routing Using Failure-Carrying Packets.
In Proceedings of ACM SIGCOMM, 2007.
[18] C. E. Leiserson. Fat-Trees: Universal Networks for
Hardware-Eﬃcient Supercomputing. IEEE Transactions on
Computers, 34(10):892–901, 1985.
[19] J. W. Lockwood, N. McKeown, G. Watson, G. Gibb,
P. Hartke, J. Naous, R. Raghuraman, and J. Luo.
NetFPGA–An Open Platform for Gigabit-Rate Network
Switching and Routing. In Proceedings of the 2007 IEEE
International Conference on Microelectronic Systems
Education, pages 160–161, Washington, DC, USA, 2007.
IEEE Computer Society.
[20] R. Moskowitz and P. Nikander. Host Identity Protocol
(HIP) Architecture. RFC 4423 (Proposed Standard), 2006.
[21] J. Moy. OSPF Version 2. RFC 2328, Internet Engineering
Task Force, 1998.
[22] A. Myers, T. S. E. Ng, and H. Zhang. Rethinking the
Service Model: Scaling Ethernet to a Million Nodes. In
ACM HotNets-III, 2004.
[23] L. S. C. of the IEEE Computer Society. IEEE Standard for
Local and Metropolitan Area Networks, Common
Speciﬁcations Part 3: Media Access Control (MAC) Bridges
Ammendment 2: Rapid Reconﬁguration, June 2001.
[24] R. Perlman, D. Eastlake, D. G. Dutt, S. Gai, and
A. Ghanwani. Rbridges: Base Protocol Speciﬁcation.
Technical report, Internet Engineering Task Force, 2009.
[25] T. L. Rodeheﬀer, C. A. Thekkath, and D. C. Anderson.
SmartBridge: A Scalable Bridge Architecture. In
Proceedings of ACM SIGCOMM, 2001.
[26] M. D. Schroeder, A. D. Birrell, M. Burrows, H. Murray,
R. M. Needham, T. L. Rodeheﬀer, E. H. Satterthwaite, and
C. P. Thacker. Autonet: A High-Speed, Self-Conﬁguring
Local Area Network Using Point-to-Point Links. In IEEE
Journal On Selected Areas in Communications, 1991.
[27] M. Scott and J. Crowcroft. MOOSE: Addressing the
Scalability of Ethernet. In EuroSys Poster session, 2008.
[28] J. Touch and R. Perlman. Transparent Interconnection of
Lots of Links (TRILL):Problem and Applicability
Statement, 2009.
Appendix A: Loop-Free Proof
A fat-tree network topology has many physical loops, which
can easily lead to forwarding loops given some combination
of forwarding rules present in the switches. However, phys-
ical loops in data center networks are desirable and provide
many beneﬁts such as increased network bisection band-
width and fault tolerance. Traditional Ethernet uses a min-
imum spanning tree to prevent forwarding loops at the cost
of decreased bisection bandwidth and fault tolerance.
Here we show that fat trees can be constrained in such
a way as to prevent forwarding loops, without requiring an
explicit spanning tree. This constraint is simple, stateless,
local to an individual switch, and uniform across all switches
in the fat tree.
Constraint 1. A switch must never forward a packet
out along an upward-facing port when the ingress port for
that packet is also an upward-facing port.
Theorem 1. When all switches satisfy Constraint 1
(C1), a fat tree will contain no forwarding loops.
Proof. C1 prevents traﬃc from changing direction more
than once. It imposes the logical notion of up-packets and
down-packets. Up-packets may travel only upward through
the tree, whereas down-packets may travel only downward.
C1 eﬀectively allows a switch to perform a one-time conver-
sion of an up-packet to a down-packet. There is no provision
for converting a down-packet to an up-packet. In order for a
switch to receive the same packet from the same ingress port
more than once, this packet should change its direction at
least twice while routed through the tree topology. However
this is not possible since there is no mechanism for convert-
ing a down-packet to an up-packet, something that would be
required for at least one of these changes in direction.
50