Accordingly, after each trial of emage generation, we can get
1600 emages of different digits. We crop the instances with
6
EM ProbeSDRTempestSDRSynchronizeemageacquisitionSync.ApplicationImageBankSmartphoneScreenEmage1234560123567849Fig. 7: Examples of cropped 6-digit security code. Ground
truth labels are shown above each strip, with the underline
highlighting the wrong prediction of digits by our classiﬁer.
our attack in more challenging scenarios, such as cross-device
attack, magazine occlusion, and interference from environmen-
tal signal noise. The speciﬁcations of different phones can be
found in Table II, and the detailed data collection settings are
shown in Table III.
A. Security Code Attack
In our practical security code attack, we use an Apple
iPhone 6s as the target device. We collect 10 sessions of grid
data, each of which contains 32000 emage examples. Through
human inspection, we drop one session due to an obvious data
quality issue. For inter-session evaluation on the grid data, 2
of the remaining 9 valid sessions are ﬁxed as the test set for all
the experiments, where session 8 represents a well-positioned
antenna scenario and session 9 is for badly-positioned antenna
scenario. The training set is gradually enlarged by adding
more of the remaining sessions. Speciﬁcally, we try four
sizes of training set, which respectively consist of 1, 3, 5
and 7 sessions, denoted as Training 1, Training 2, Training
3 and Training 4, as illustrated in Figure 5. Each resulting
digit emage will be fed as input to train our CNN classiﬁer,
following the principles in Section V-B.
We simulate 200 text messages, each of which contains a
6-digit security code, making sure they look very close to the
real case, as shown in Figure 4 (right). In this case, each emage
of the security code, with a size of 126× 31, (see Figure 7 for
some examples) is evenly divided into six.
The best overall accuracy (89.8% in Table IV) with respect
to all 200× 6 = 1200 individual digits is achieved when using
all of the 7 training sessions (more details about the impact
of training data amount will be discussed in Section V-B). As
can be seen in Table IV, the accuracy differs for different
digits, with the highest (99.1%) achieved for digit 4, and
lowest (75.8%) for digit 3. Figure 7 shows some examples for
the security code along with the ground truth and prediction
results. It demonstrates that our approach can correctly predict
the digits with high accuracy although the digits are hardly
recognizable to the human eye.
In practice, attackers may have various query budgets for
fully uncovering the security code (with all the 6 digits being
correct). So, in Table V, we present the accuracy results when
four security code digits or more can be correctly predicted
by our classiﬁer. It can be observed that, with one attempt, the
attacker can fully recognize the security code at 50% of the
cases. The probability of recognizing four or more digits can
Fig. 5: Train/test splits speciﬁed in the case of multi-crop grid,
where the training set is gradually enlarged by including more
sessions, and the test set is ﬁxed with two sessions.
Fig. 6: CNN architecture used in our security code attack.
a certain human inspection to guarantee the data quality. We
conduct multiple sessions of emage generation to alleviate the
inﬂuence of distribution shift, which is validated effective as
in Section V-B.
2) CNN Architecture and Model Training: For the model
architecture, we adopt
the simple LeNet [48], which was
initially proposed for handwritten digit recognition. We slightly
adapt the LeNet to ﬁt our input emage size of 31 × 21 (for
Honor 6X the input size is 45×21, and for iPhone 6 is 31×20).
The details of the architecture is shown in Figure 6. The
PyTorch is used for our implementation and the experiments
are run on a workstation with a 16-core CPU and a GTX1080Ti
GPU. In all cases, 80% of multi-crop grid data are used for
training, 10% for validation and 10% for testing. Each round of
training can be ﬁnished within one hour when using the Adam
optimizer [39] with a learning rate of 0.001. We conduct the
training over 100 epochs with a batch size of 256, and select
the optimal model based on the validation accuracy.
V. EXPERIMENTS
In this section, we ﬁrst conduct experiments on iPhone 6s
to analyze the properties of our attack on the basic single-
device scenario. Speciﬁcally, we look into the dimensions that
can potentially impact the classiﬁcation performance, such as
size and heterogeneity of the training data (Section V-B), for
further analyzing the attacker’s capability in various attack
settings.
Then, we test our attack using more phones (iPhone 6-A,
iPhone 6-B, and Honor 6X) to validate the effectiveness of
7
(S6, S7)Training 4(S4, S5)(S2, S3)(S1)Training 1Training 2Training 3007008009004005006001002003000007008009004005006001002003000TEST007008009004005006001002003000007008009004005006001002003000007008009004005006001002003000007008009004005006001002003000007008009004005006001002003000007008009004005006001002003000007008009004005006001002003000007008009004005006001002003000(S1)(S2)(S3)(S4)(S5)(S6)(S7)(S8)007008009004005006001002003000(S9)007008009004005006001002003000S8, S9Data
Train/Val/Test
Single-device Test
Cross-device Test
Magazine Test
Noise Test
Displayed Content
Multi-Crop Grid Data
Security Code Data
Security Code Data
Security Code Data
Security Code Data
iPhone 6s
iPhone 6-A iPhone 6-B Honor 6X
10
2
N/A
1
1
N/A
5
2
1
1
N/A
N/A
1
N/A
N/A
N/A
5
2
1
1
TABLE III: The list of collected data sessions for different phones in the security code attack. Multi-crop grid data represents
the data collected in the case of multi-crop, and security code data represents the simulated text message with the security code.
Digits
Acc. (%)
0
1
2
3
4
5
6
7
8
9
87.2
86.8
97.4
75.8
99.1
97.4
95.1
93.1
82.5
86.1
All
89.8
TABLE IV: Accuracy with respect to different digits (0-9) and overall accuracy in our security code attack.
6 digits ≥ 5 digits ≥ 4 digits
50.5
89.5
99.0
Acc. (%)
TABLE V: Accuracy of predicting partial security code cor-
rectly with the CNN classiﬁer in our security code attack.
Fig. 9: Confusion matrix of the inter-session accuracy (grid
data) in our security code task. Results are from the classiﬁer
trained on Training 4 and tested on session 8.
training sets: training 1, 2, 3 and 4. It can be observed that
the accuracy improves as we increase the number of training
sessions. We can also observe that inter-session accuracy with
only one training session is lower than the multi-crop grid case.
However, using more training data with multiple sessions could
alleviate this issue, leading to a high accuracy of 90.9% for
Training 4 (with seven training sessions). This validates our
assumption that incorporating heterogeneous sessions could
help alleviate the impact of the random noise introduced to
the emage generation. One detailed classiﬁcation result with
respect to different classes are shown in the confusion matrix
in Figure 9. We also notice that there is a difference between
the prediction performance between two test sessions, which
might be explained by their different data quality.
Fig. 8: Inter-session accuracy (grid data) of our security code
attack for different training sets with gradually increased size.
The two bars for each training set represent two different test
sessions (session 8 and 9).
reach 99%, showing that our approach can present a serious
threat in practice.
B. Data Analysis on Grid Data
We ﬁrst consider the scenario where the attacker can train
the classiﬁer on data sampled from the same distribution as
the attacked security code. This can be regarded as the best-
case scenario, although almost impossible in most practical
cases. Speciﬁcally, we achieve an accuracy of 86.5% within
the session used in Training 1.
Inter-session evaluation represents a more realistic attack
scenario, where the training data from the same session of
the target
the attacker can simulate
similar data using the same settings. Figure 8 shows the inter-
session accuracy of the four classiﬁers trained on different
is not accessible, but
C. Experiments on Other Phones
In this section, we conduct experiments on different phones
to further validate the general effectiveness of our security code
recognition on different devices. We show the potential of the
recognition in more challenging and realistic scenarios, includ-
ing cross-device attack, antenna occlusion by a magazine, and
interference from the signal noise generated by surrounding
phones (cf. Figure 10). The cross-device attack consists of
training the recognition algorithm on the data from one device
and testing the model on data from another unit of the same
model. Speciﬁcally, we use two iPhone 6, namely, iPhone 6-A
8
Training 1Training 2Training 3Training 4Training set020406080100Accuracy (%)81.587.986.390.953.966.567.268.7Acc. (%)
Test
(Grid)
Single-device-1
(Security code)
Single-device-2
(Security code)
Magazine
(Security code)
70 pages
200 pages
With Noise
(Security code)
iPhone 6-B
(Security code)
iPhone 6-A
Honor 6X
73.42
94.38
41.42
74.00
47.08
74.00
14.38
-
-
65.79
63.29
64.25
61.54
-
TABLE VI: Inter-session classiﬁcation of our security code attack for different phones and different test settings. Grid means the
multi-crop grid test data is used, and Security Code means the simulated text message test data is used. The training set stays
the same in all test settings for each device. Single-device-1 and Single-device-2 refer to two different test sessions.
Fig. 10: Pictures of the Magazine setting (left), with the phone
in between the magazine pages and the probe on top, and the
With Noise setting (right).
and iPhone 6-B, and make sure that they have the same version
of the iOS system, and not refurbished. Five sessions of data
are collected for training the recognition model on iPhone 6-A,
and two test sessions of security code data are collected for
testing. Additionally, we collect a session of testing data with
the antenna occluded by a magazine, another test session from
iPhone 6-B and a test session with background noise. The mea-
surement setups for occluding the antenna and simulating the
background noise are shown respectively in Figure 10. Each of
the above four testing sessions contains 200 different security
codes and for each code, we repeat the frame twice for a more
stable recognition. Our attack can also work on a refurbished
iPhone (iPhone 6-C, see Table II), but no quantitative results
are reported in order to maintain fair comparison.
Table VI summarizes our experimental results under dif-
ferent test settings corresponding to the data descriptions in
Table III, i.e., Multi-crop, Single-device, Magazine, Noise,
and Cross-device. As can be seen, our model achieves high
accuracy for the original multi-crop data. For other settings,
as expected, the performance drops due to the generalization
gap but still being effective enough in most cases. Speciﬁcally,
the high cross-device accuracy suggests the effectiveness of our
attack in a more realistic scenario, where the device used for
collecting the training data is not necessarily the target device.
The results on an Android phone, Honor 6X, with four sessions
of training data, verify that the effectiveness of our attack is not
limited to the speciﬁc phone type, iPhone. we can also observe
that the single-device and cross-device sessions of the security
code yield different prediction performance, which might be
explained by their different data quality, as also reported for
iPhone 6s (cf. Section V-A).
Fig. 11: Top: An emage and its predicted activation map by
the pre-trained model on iPhone 6s. Warmer color represents
higher prediction conﬁdence. Bottom: Activation responses in
the row of the text message
For the magazine setting, the accuracy drop can be ex-
plained by the signal strength of the antenna. The magnetic
probe can be considered as a magnetic dipole, for which it
holds that the power density is dependent on a factor r−5, for
which r is the distance between the probe and the origin of
radiation [36]. Therefore, placing the magnetic probe a little
bit further away from the origin of radiation, already has some
signiﬁcant consequences on the quality of the received signal.
Speciﬁcally, we ﬁnd the performance of iPhone 6-A drops
dramatically with 70 pages, but for Honor 6X, the performance
is better maintained even with a thicker magazine of 200 pages
because of its higher leakage of signals. The high single-
device accuracy (74% for both) also conﬁrms this higher signal
leakage of this Honor 6X phone than iPhone 6.
We also ﬁnd that our attack can work on the OLED screen
by conducting a preliminary exploration of Samsung Galaxy
A3 (2015). However, since this phone is disassembled, we do
not go further for quantitative details.
D. Discussion
In practice, the localization of security code patterns in
either time or space dimensions is crucial. Here we discuss
how a simple sliding window technique can tackle both. When
monitoring the target phone in real-time, we can also integrate
our recognition model with a simple sliding window operation
to identify the key frame(s) that are most likely to contain
9
(a) 1x
(b) 1.2x
(c) 1.5x
(d) 2x
(e) 2.5x
(f) 3x
(g) 4x
(h) 5x
(i) 7x
(j) 10x
(k) 20x
Fig. 12: Images (top row) and their corresponding emages (bottom row) of letter C displayed at 11 different scales. It can be
seen that the scales span from uninterpretable to the human eye to easily interpretable.
a text message of the security code. Speciﬁcally, we set the
height of the sliding window as the height of each digit, and
the width as the total width of 6 digits. The horizontal and
vertical strides are equal to the height and width of each digit.
As shown in Figure 11, the message area is activated much
more than the plain area, indicating that our recognition model
can be used to identify the most likely frame(s). Furthermore,
within the speciﬁc row of the text message, the highest acti-
vation responses are concentrated on the security code region.
This suggests that the textual background will not interfere in
our security code recognition. It is also worth noting that, in
practice, the attacker could also leverage off-the-shelf language
models or visual detection models. Such models would provide
a straightforward way to boost the localization performance.
We also mention that in our experiments, we use a maximum