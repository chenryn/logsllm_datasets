Yes
No
Yes
No
IMDB
COIL
NCI1
SR
0.92
0.92
0.82
0.82
0.89
0.89
AP
4.33
4.44
16.19
16.05
7.16
7.60
AQ
1,622
9,859
1,800
12,943
1,822
10,305
AT (s)
121.21
808.76
109.91
787.67
163.83
1071.17
We further analyze the percentages of adversarial graphs whose
initial perturbation vectors Œò0 are found in each component (i.e.,
supernode, superlink, and graph). Figure 9 shows the results. Each
bar illustrates the percentages of adversarial graphs whose Œò0 is
found by the three components. For instance, on COIL, we obtain
75.73% adversarial graphs whose initial Œò0 is found in searching
supernodes using strategy-I. On COIL and NCI1, we find effective
initial vectors Œò0 by using either strategy-I or strategy-II, i.e., ei-
ther searching supernodes or superlinks first. Since the searching
spaces of supernodes are often smaller than those of superlinks,
strategy-I that searches within supernodes first is more suitable
on these two datasets. Thus, strategy-I performs best among three
strategies on these two datasets. However, on IMDB, we find Œò0
for most target graphs within the superlinks in both strategy-I/-II.
Thus, strategy-II that searches within superlinks first is a better
strategy for initial search on IMDB. From Table 4, we can also see
that strategy-II performs slightly better than strategy-I in terms of
AP and AQ. Note that, the best searching strategies for different
datasets are different. The possible reason is, due to the density
of the datasets, i.e., strategy-II (i.e., searching superlinks first) is
the best strategy for dense graphs (e.g., IMDB), while strategy-I
(i.e, searching supernodes first) is for sparse graphs (e.g., NCI1), it
is much harder to partition the graphs into supernodes in denser
graphs than in sparser graphs.
Impact of query-efficient gradient computation (QEGC). We
5.2.3
further conduct experiments to evaluate the impact of QEGC. Table
5 shows the results. We can observe that, under our attack, the
number of required queries varies significantly, with and without
QEGC. For instance, on IMDB, ùê¥ùëÑ = 12, 943 when we do not apply
QEGC, while the ùê¥ùëÑ is reduced to 1,800 when using QEGC, which
Figure 10: Gradient norms on the IMDB-BINARY dataset.
is only 13.91% of the former. The SR and AP vary slightly with and
without QEGC. For example, the APs with and without QEGC on
COIL are 4.33 and 4.44, respectively, and the difference is only 0.11.
These results demonstrate that QEGC can significantly reduce the
number of queries and thus the attack time in our attacks, while
maintaining high success rate and incurring small perturbations.
5.2.4 Gradient norms in our attack. The convergence property of
our optimization based hard label black-box attack is based on As-
sumption 1, which requires that the norm of gradient of ùëù(Œò) should
be bounded. Here, we conduct an experiment to verify whether
this assumption is satisfied. Specifically, we randomly choose 5
target graphs from IMDB that are successfully attacked by our
attack. The number of nodes of these graphs are 13, 18, 24, 30,
and 60, respectively. From Figure 10, we can observe that gradients
norms are relatively stable and are around 0.1 in all cases. Therefore,
Assumption 1 is satisfied in our attacks.
6 DEFENDING AGAINST ADVERSARIAL
GRAPHS
In this section, we propose two different defenses against ad-
versarial graphs: one to detect adversarial graphs and the other to
prevent adversarial graph generation.
6.1 Adversarial Graph Detection
We first train an adversarial graph detector and then use it to iden-
tify whether a testing graph is adversarially perturbed or not. We
train GNN models as our detector. Next, we present our methods of
generating the training and testing graphs for building the detector,
and utilize three different structures of GNN models to construct
our detectors. Finally we evaluate our attack under these detectors.
6.1.1 Generating datasets for the detector. The detection process
has two phases, i.e., training the detector and detecting testing
(adversarial) graphs using the trained detector. Now, we describe
how to generate the datasets for training and detection.
Testing dataset. The testing dataset includes all adversarial graphs
generated by our attack in Section 5.2 and their corresponding
normal (target) graphs. We set labels of adversarial graphs and
normal graphs to be 1 and 0 respectively.
Training dataset. The training dataset contains normal graphs and
adversarial graphs. Specifically, we first randomly select a number
of normal graphs from the training dataset used to train the target
GIN model (see Section 5.1). For each sampled normal graph, the
Session 1B: Attacks and Robustness CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea117Figure 11: Detection performance vs. budget ùëè on the testing dataset with the training dataset generated by our attack.
(a) NCI1
(b) COIL
(c) IMDB
Figure 12: Detection performance vs. budget ùëè on the testing dataset with the training dataset generated by PGD attack.
(a) NCI1
(b) COIL
(c) IMDB
detector deploys an adversarial attack to generate the correspond-
ing adversarial graph. We use all the sampled normal graphs and
the corresponding adversarial graphs to form the training dataset.
We consider that the detector uses two different attacks to generate
the adversarial graphs: (i) the detector uses our attack, and (ii) the
detector uses existing attacks. In our experiments, without loss of
generality, we set the size of training dataset to be 3 times of the
size of the testing dataset.
When using existing attacks, the detector chooses the projected
gradient descent (PGD) attack [54]. PGD attack is a white-box ad-
versarial attack against GCN model for node classification tasks. It
first defines a perturbation budget as the maximum number of edges
that the attacker can modify. Then it conducts projected gradient
descent to minimize the attacker‚Äôs objective function. Specifically,
in each iteration, the attacker computes the gradients of the objec-
tive function w.r.t the edge perturbation matrix. Then it updates
the edge perturbation matrix in the opposite direction of the gra-
dient and further projects the edge perturbation matrix into the
constraint set such that the number of perturbations is within the
pre-set budget. We extend PGD attack for graph classification. Fi-
nally, we choose the three aforementioned GNNs (i.e., GIN [55],
SAG [26] and GUNet [15]) to train the binary detectors on the
constructed training dataset. Note that, we do not use the afore-
mentioned RL-S2V attack or the Random attack to generate training
dataset because RL-S2V attack needs large AT and the random at-
tack needs large AQ to produce a reasonable number of adversarial
graphs (see Table 3). In contrast, the PGD attack is more efficient.
6.1.2 Detection results. In the detection process, we use False Posi-
tive Rate (FPR) and False Negative Rate (FNR) to evaluate the effec-
tiveness of the detector, where FPR indicates the fraction of normal
graphs which are falsely predicted as adversarial graphs, while
FNR stands for the fraction of adversarial graphs that are falsely
predicted as normal graphs. Again, we repeat each experiment 10
trails and use the average results of them as the final results to ease
the influence of randomness.
Detector results under our attack. The detection performance
vs. budget ùëè on the testing dataset with the training dataset gener-
ated by our attack is shown in Figure 11. Solid lines indicate FPRs
and dashed lines indicate FNRs. We have several observations. (i)
The detection performance increases as the budget is getting larger,
which means that the detector can distinguish more adversarial
graphs if the average perturbations of adversarial graphs are larger.
For example, the FPR of GUNet detector on IMDB dataset decreases
from 0.80 to 0.42 when the budget increases. This is because when
more perturbations are added to the normal graphs, the structure
difference between the corresponding adversarial graphs and the
normal graphs is larger. Thus, it is easier to distinguish between
them. (ii) The detection performances for a specific detector are
different on the three datasets. For instance, when using GIN and
the budget is 0.20, the FPRs on the three datasets are similar while
the FNRs are quite different, i.e., 0.48 (COIL), 0.39 (NCI1) and 0.13
(IMDB), respectively. This is because the average perturbations of
adversarial graphs for COIL are the smallest (i.e., 4.33), then for
NCI1 (i.e., 7.16) and for IMDB are the largest (i.e., 16.19). (iii) The
detector is not effective enough. For example, the smallest FNR on
the COIL dataset is 0.48, which means that at least 48% of adver-
sarial graphs cannot be identified by the detector. We have similar
observation on the NCI1 dataset. We guess the reason is that the
difference between adversarial graphs and normal graphs is too
small on these two datasets, and the detector can hardly distinguish
between them.
Detection results under the PGD attack. The detection perfor-
mance vs. budget ùëè on the testing dataset with the training dataset
generated by PGD attack is shown in Figure 12. Similarly, we can
Session 1B: Attacks and Robustness CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea118(a) NCI1
(b) COIL
(c) IMDB
Figure 13: Defense performance against our attack for GIN vs. fraction ùõæ of kept top largest singular values.
(a) NCI1:SAG
(b) IMDB:SAG
(c) NCI1:GUNet
(d) IMDB:GUNet
Figure 14: Defense performance against our attack for SAG and GUNets vs fraction ùõæ of kept top largest singular values.
observe that the detection performance is better when the budget
is larger. However, the detection performance with the PGD at-
tack is worse than that with our attack. For instance, on the NCI1
dataset, FNRs are around 0.70 using the three detectors with the
PGD attack, while the detector with our attack achieves as low as
0.25. One possible reason is that, when the adversarial graphs in
the training set are generated by our attack, the detector trained on
these graphs can be relatively easier to generalize to the adversarial
graphs in the testing dataset that are also generated by our attack.
On the other hand, when the adversarial graphs in the training
set are generated by the PGD attack, it may be more difficult to
generalize to the adversarial graphs in the testing dataset.
6.2 Preventing Adversarial Graph Generation
We propose to equip the GNN model with a defense strategy to
prevent adversarial graph generation. Here, we generalize the state-
of-the-art low-rank based defense [14] against GNN models for
node classification to graph classification.
6.2.1 Low-rank based defense. The main idea is that only high-
rank or low-valued singular components of the adjacency matrix
of a graph are affected by the adversarial attacks. As these low-
valued singular components contain little information of the graph
structure, they can be discarded to reduce the effects caused by
adversarial attacks, as well as maintaining testing performance of
the GNN models. In the context of graph classification, we first
conduct a singular value decomposition (SVD) to the adjacency
matrix of each testing graph. Then, we keep the top largest singular
values and discard the remaining ones. Based on the top largest
singular values, we can obtain a new adjacency matrix, and the
corresponding graph whose perturbations are removed.
6.2.2 Defense results. We calculate the SR of our attack and the
clean testing accuracy after adopting the low-rank based defense.
We use the target graphs described in Section 5.1 to calculate the
SR. We use the original testing dataset without attack to compute
the testing accuracy. For each target graph, we first generate a
low-rank approximation of its adjacency matrix by removing small
singular values and then feed the new graph into the target GNN
model to see if the predicted label is correct or wrong. Figure 13
and 14 show the SR and clean testing accuracy with the low-rank
based defense vs. fraction ùõæ of kept top largest singular values
for the three GNN models, respectively. ùõæ ranges from 0.05 to 1.0
with a step of 0.05. We have several observations. (i) When ùõæ is
relatively small (e.g., ‚â§ 0.35), i.e., a small fraction of top singular
values are kept, the clean accuracy decreases and even dramatically
on NCI1 and COIL. One possible reason is that the testing graph
structure is damaged. On the other hand, the SR also decreases,
meaning adversarial perturbations in certain adversarial graphs
are removed. (ii) When ùõæ is relatively large (e.g., ‚â• 0.35), i.e., a
large fraction of top singular values are kept, the clean accuracy
maintains and the SR is relatively high as well. This indicates that
adversarial perturbations in a few graphs are removed. The above
observations indicate that the fraction ùõæ in low-rank based defense
achieves an accuracy-robustness tradeoff. In practice, we need to
carefully select ùõæ in order to obtain high robustness against our
attack, as well as promising clean testing performance. (iii) When
ùõæ is extremely small, e.g., ùõæ = 0.05, which means that 95% singular
values of a graph are removed, the clean accuracy does not decrease
much (e.g., see Figure 14 (a) and (c)) or even slightly increases (e.g.,