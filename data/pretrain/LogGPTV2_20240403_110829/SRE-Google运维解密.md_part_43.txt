，一些团队迁移到了测试后立即上线（push-on-green）这样一个模型，以
---
## Page 233
优”是如何定义的呢？其实这里并没有一个独立的答案，因为这里的最优严重依赖于下
应该最优地分布于多条网络链路上、多个数据中心中，以及多台服务器上。但是这里的“最
用来决定数据中心中的这些机器中哪一个用来处理某个请求的。理想情况下，用户流量
很多用户甚至同时发送好几个请求。用户流量负载均衡（trafficloadbalancing）系统是
在现实中，Google拥有数以千计的服务器，也同时有比这个数量更多的用户在发送请求。
影响的基础设施也是一个槽糕的主意。
这限制了远距离传输数据的速度。就算在一个理想的情况下，采用这样一种受单点故障
一套配置，它仍然会受到一些物理条件的限制。例如，光速是通过光纤通信的制约性因素
带宽充足的网络连接。这是否就能满足Google的需求了呢？并不是。就算拥有这样的
假设，这里仅仅是假设一
有时候硬件并不能解决问题
章会更深入地探讨我们是如何在一个数据中心内进行流量分发和负载均衡的。
本章关注于高层次的负载均衡一
策略：运维大型系统时，将所有鸡蛋放在一个篮子里是引来灾难的最好办法。
想这个模式下面光是网络带宽的要求吧！），我们还是不会采取这种受单点故障影响的
担这些负载的。即使我们真的有一台非常强大的超级计算机，可以处理所有这些请求（想
Google每秒要处理数以百万计的请求，与你猜想的一样，我们是使用多台服务器同时承
一我们有一台非常强大的服务器，和与之配套的永不出故障、
前端服务器的负载均衡
Google是如何在数据中心之间调节用户流量的。下一
作者：Piotr Lewandowski
编辑：Sarah Chavis
第
19章
223
224
---
## Page 234
225
第一个问题是这种机制对客户端行为的约束力很弱：记录是随机选择的，也就是每条记
为了更切实地展开讨论，我们这里主要讨论基于TCP的HTTP请求。对无状态（stateless）
不同，是我们在全局层面决定“最优”分配方案的重要条件。
求能够一次成功，所以这里最重要的变量是吞吐量（throughput）。两种请求用户的需求
对于视频上传请求来说，用户已经预期该请求将要花费一定的时间，但是同时希望该请
用户想要很快地获取搜索结果，所以对搜索请求来说最重要的变量是延迟（latency）。而
列几个因素：
192
然看起来简单并且容易实现，但是存在很多问题。
复中提供多个A记录或者AAAA记录，由客户端任意选择一个IP地址使用。这种方案虽
一层的负载均衡机制提供了一个良好基础：DNS负载均衡。最简单的方案是在DNS回
在某个客户端发送HTTP请求之前，经常需要先通过DNS查询IP地址。这就为我们第
使用DNS进行负载均衡
都仍然适用。
服务（如基于UDP的DNS）的负载均衡和这个有所不同，但是这里讨论的大部分方式
避免网络拥塞。负载均衡，尤其是大型系统的负载均衡，是非常复杂和非常动态化的。
据中心的缓存处于有效状态。或者某些非交互式请求会被发往另外一个地理区域，以
方案的考虑范围之内：有些请求可能会被指派到某个稍远一点的数据中心，以保障该数
当然这个例子中使用了一个非常简化的场景。在现实中，很多其他因素也都在“最优”
往往关注于优化资源的利用率，避免某个服务器负载过高。
服务器都在同一个网络中，对用户来说都是等距的。因此在这个层面上的“最优”分配
但是在局部层面，在一个数据中心内部，我们经常假设同一个物理建筑物内的所有物理
·视频上传流将会采取另外一条路径—也许是一条目前带宽没有占满的链路
·搜索请求将会被发往最近的、可用的数据中心一
·逻辑层级（是在全局还是在局部）。
来最大化吞吐量，同时也许会牺牲一定程度的延迟。
（RTT），因为我们想要最小化该请求的延迟。
用户流量的天然属性。
技术层面（硬件层面与软件层面）。
第19章前端服务器的负载均衡
一评价条件是数据包往返时间
---
## Page 235
个用户，或者几万个用户。我们利用两种方式来解决这个问题：
最后，递归解析器一般根据接收到的回复中的时效值（TTL）来缓存和发送这些回复
径。
对他们当前的数据中心来讲是最优的，却没有考虑到可能对其他用户还有更优的网络路
服务商可能在数个大都市区域也有网络互联。该ISP的域名服务器返回的一个回复可能
服务商可能在一个数据中心中运行数个域名服务器以服务他们的整个网络，但是同时该
万个用户的请求，范围从一个小办公室到整个大陆。举例来说，某个大型的国家级电信
不仅要处理返回最优IP的这个难题，处理请求的域名服务器可能同时需要处理几千、几
这些明显的优势使得最大的DNS解析器（例如OpenDNS和Google注1）已经开始采用了。
文献[Con15])，该协议在递归解析器发送的请求中包括了最终用户的子网段。这样权威
析器的IP地址返回一个最优方案。一个可能的解决方案是使用EDNSO扩展协议（参见
是递归解析器的IP地址。这是一个很严重的问题，因为这样DNS服务只能根据递归解
以递归方式解析IP地址会造成一定的问题，因为权威服务器接收到的不是用户地址，而
常重要的影响：
时经常提供一定程度的缓存机制。这样的DNS中间人机制在用户流量管理上有三个非
有一个递归解析器（recursive nameserver）代理请求。该递归解析器代理用户请求，同
接跟权威域名服务器（authoritive nameserver）直接联系。在用户到权威服务器中间经常
当然，没有一个很简单的方案，因为这是由DNS的基本特性决定的：最终用户很少直
回复。但是这种解决方案使得我们需要维护一个更加复杂的DNS服务，并且需要维护
将所有的网络地址和它们对应的大概物理位置建立一个对照表，按照这个对照表来发送
个问题。服务器可以使用最近的数据中心地址来生成DNS回复。更进一步的优化方式是
DNS服务器地址，通过DNS请求一般会到达最近的地址这个方式来一定程度上缓解这
SRV记录来指明每个IP地址的优先级和比重，但是HTTP协议目前还没有采用SRV记录。
录都会引来有基本相同数量的请求流量。如何避免这个问题呢？理论上我们可以使用
一个数据更新流水线（pipeline）来保证位置信息的正确性。
·递归方式解析IP地址。
·额外的缓存问题。
不确定的回复路径。
使用DNS进行负载均衡
193
<226
---
## Page 236
注3
比如我们可以依次升级某些机器，或者在资源池中增加更多的机器而不影响用户。
让我们将底层实现细节隐藏起来（比如某一个VIP背后的机器数量），无缝进行维护工作。
从用户视角来看，VIP仍然是一个独立的、普通的IP地址。理论上来讲，这种实现可以
负载均衡：虚拟IP
是远小于我们服务器的数量的。
DNS的尺寸限制实际上为单个DNS回复能返回的地址数量设置了上限，这个上限明显
194
虚拟IP地址（VIP)不是绑定在某一个特定的网络接口上的，它是由很多设备共享的。但是，
要真正解决前端负载均衡的问题，
记住RFC1035将DNS回复限制为512字节（参见文献[Moc87]）。
就生效了。另一方面，我们清晰地看到仅仅靠在DNS里做负载均衡是不够的。我们要
尽管有这些不足，DNS仍然是最简单、最有效的负载均衡制度，它在用户发起连接之前
载均衡计算之外，并没有什么其他的应对办法。
器的缓存，DNS记录需要保持一个相对较低的时效值（TTL）。这其实是为DNS回复的
DNS中间人带来的第三个问题是跟缓存有关的。因为权威服务器不能主动清除某个解析
均衡系统负责跟踪我们的流量水平、可用容量和各种基础设施的状态。
Google可以将权威DNS服务器和我们的全局负载均衡系统（GSLB）整合起来，该负载
否则将用户导向正在经受网络故障和供电故障的地点并不是一个很合理的做法。幸好
来的请求。同时，负载均衡器还要保障其选择的数据中心和网络目前都处于良好状态
件。DNS负载均衡器需要确保它选择的数据中心有足够的容量来处理该DNS回复所带
“离用户最近的位置”。
但是“最优位置”在DNS负载均衡的语境中，到底是什么意思呢？最直接的答案是
我们只能针对大部分用户的情况选择最优位置进行优化。
准确评估地理位置分布是非常困难的，尤其是用户分布在很广的区域时。在这种情况下，
不幸的是，
·分析流量的变化，并且持续不断地更新已知的DNS解析器的用户数量，这样可
）根据数据评估每个已知解析器背后的用户的地址位置分布，以便更好地将用户转
以评估某个解析器的预期影响。
向最佳地址。
第19章
，不是所有的DNS解析器都遵守权威服务器的TTL值。
前端服务器的负载均衡
但是（先不考虑确定用户位置有多难）还应该有其他的选择条
，我们需要在DNS 负载均衡之后增加一层虚拟IP地址。
注3
---
## Page 237
的目标MAC地址，负载均衡器可以保持全部上层信息不变，后端将会接收到原始的来
另外一个解决方案是修改数据链路层（OSI模型的2层）的信息。通过修改转发数据包
个连接，也就是不能提供一个完全无状态的后备机制。
呢？一个解决方案是进行一次网络地址转换（NAT）。但是这要求我们在内存中跟踪每一
那么回到更大的问题上来：负载均衡器究竟是如何将数据包发往某个特定的VIP后端的
上升时切换为一致性哈希算法，例如在处理分布式拒绝服务攻击时（DDoS）。
存连接的影响。最终结果是，我们平时可以使用简单的连接跟踪机制，但是在系统压力
被添加或者删除时保持相对稳定。这种算法在后端资源变化时，最小程度地减少了对现
提出的一致性哈希算法（参见文献[Kar97]）描述了一种映射算法，在新的后端服务器
个机器出故障时重置所有连接，这就是一致性哈希（consistent hashing）算法。1997年
幸运的是，的确有一种替代方案。既不需要在内存中保存所有连接的状态，也不会在单
着所有现存连接都要中断。这样的场景即使出现得不太频繁，对用户来说也是很不友好
的请求都被指向了另外一个后端服务器。如果后端服务器之间不同步状态，这几乎意味
这里N突然变成了N-1，而id（packet）mod N变成了id（packet）modN-1，基本上所有
这就成功了吗？还没有。当某个后端服务器出现问题，需要从列表中去掉时怎么办呢？
这样负载均衡器就不用再保存状态了，每个连接的数据包也都会发往同一个后端服务器
后端服务器数量。
这里的id（）是一个函数，以数据包内容为输入，得出一个连接标识符，N是所有配置的
id(packet)mod N
选择后端服务器。举例来说，连接标识符可以用如下算式表达：
（connectionID）（可能使用某些数据包内的信息和一个哈希算法），使用该连接标识符来
同一个后端服务器。一种替代方案是使用数据包中的某些部分创建出一个连接标识符
就需要负载均衡器跟踪所有经过它转发的连接，以确保同一个连接的数据包都会发往
状态的协议就不适用了，因为在处理一个请求的过程中必须使用同一个后端服务器。这
应该可以最优化用户体验，因为请求始终会被发往最不忙的机器。但是，这个逻辑对有
是最直接的）方案是，永远优先目前负载最小的后端服务器。理论上来说，这个方案
负载均衡器在决定哪个后端服务器应该接收请求时，有如下几种方案：第一种（也可能
这些后端服务器可以接下来处理该请求。
的组件。该负载均衡器接收网络数据包，同时将它们转发给VIP背后的某一个服务器，
的。
在实践中，最重要的VIP实现部分是一个称为网络负载均衡器（networkloadbalancer)
负载均衡：虚拟IP
195
<228
---
## Page 238
229
数据到达数据中心内部之后，我们可以在内部采用更大的MTU来避免碎片重组的发生，
196
据中心内部都存在许多实现细节问题。
尽早进行负载均衡，以及分级多次进行一
但是这种做法需要网络设备支持。就像很多东西一样，负载均衡表面上听起来很简单—
而需要碎片重组（Fragmentation）。
（IPv4+GRE，封装需要24字节），这经常导致数据包超出可用的传输单元（MTU）大小，