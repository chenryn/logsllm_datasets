(Â§1, 2).
â€¢ A formal definition of proxy useâ€”a key building block for
use privacyâ€“and an axiomatic basis for this definition (Â§3).
â€¢ An algorithm for detection and tracing of an instantiation of
proxy use in a machine learnt program, and proof that this
algorithm is sound and complete (Â§4).
â€¢ A repair algorithm that provably removes violations of the
proxy use instantiation in a machine learning model that are
identified by our detection algorithm and deemed inappro-
priate by a normative judgment oracle (Â§5).
â€¢ An implementation and evaluation of our approach on pop-
ular machine learning algorithms applied to real datasets
(Â§6).
2 USE PRIVACY
We use the Target example described earlier in the paper to mo-
tivate our notion of use privacy. Historically, data collected in a
context of interaction between a retailer and a consumer is not ex-
pected to result in flows of health information. However, such flow
constraints considered in significant theories of privacy (e.g., see
Nissenbaum [51]) cannot be enforced because of possible statistical
inferences. In particular, prohibited information types (e.g., preg-
nancy status) could be inferred from legitimate flows (e.g., shopping
history). Thus, the theory of use privacy instead ensures that the
data processing systems â€œsimulate ignoranceâ€ of protected informa-
tion types (e.g., pregnancy status) and their proxies (e.g., purchase
history) by not using them in their decision-making. Because not
all instances of proxy use of a protected information type are in-
appropriate, our theory of use privacy makes use of a normative
judgment oracle that makes this inappropriateness determination
for a given instance.
We model the personal data processing system as a program p.
The use privacy constraint governs a protected information type Z.
Our definition of use privacy makes use of two building blocks: (1)
a function that given p, Z, and a population distribution ğ’« returns
a witness w of proxy use of Z in a program p (if it exists); and (2)
a normative judgment oracle ğ’ª(w) that given a specific witness
returns a judgment on whether the specific proxy use is appropriate
(true) or not (false).
Definition 1 (Use Privacy). Given a program p, protected in-
formation type Z, normative judgment oracle ğ’ª, and population
distribution ğ’«, use privacy in a program p is violated if there exists a
witness w in p of proxy use of Z in ğ’« such that ğ’ª(w) returns false.
In this paper, we formalize the computational component of
the above definition of use privacy, by formalizing what it means
for an algorithm to use a protected information type directly or
through proxies (Â§3) and designing an algorithm to detect proxy
uses in programs (Â§4). We assume that the normative judgment
oracle is given to us and use it to identify inappropriate proxy uses
and then repair them (Â§5). In our experiments, we illustrate how
such an oracle would use the outputs of our proxy use analysis and
recommend the repair of uses deemed inappropriate by it (Â§6).
This definition cleanly separates computational considerations
that are automatically enforceable and ethical judgments that re-
quire input from human experts. This form of separation exists also
in some prior work on privacy [33] and fairness [23].
3 PROXY USE: A FORMAL DEFINITION
We now present an axiomatically justified, formal definition of
proxy use in data-driven programs. Our definition for proxy use
of a protected information type involves decomposing a program
to find an intermediate computation whose result exhibits two
properties:
â€¢ Proxy: strong association with the protected type
â€¢ Use: causal influence on the output of the program
In Â§ 3.1, we present a sequence of examples to illustrate the
challenge in identifying proxy use in systems that operate on data
associated with a protected information type. In doing so, we will
also contrast our work with closely-related work in privacy and
fairness. In Â§3.2, we formalize the notions of proxy and use, prelimi-
naries to the definition. The definition itself is presented in Â§3.3 and
Â§3.4. Finally, in Â§3.5, we provide an axiomatic characterization of the
notion of proxy use that guides our definitional choices. We note
that readers keen to get to the detection and repair mechanisms
may skip Â§3.5 without loss of continuity.
3.1 Examples of Proxy Use
Prior work on detecting use of protected information types [15,
30, 44, 63] and leveraging knowledge of detection to eliminate
inappropriate uses [30] have treated the system as a black-box.
Detection relied either on experimental access to the black-box [15,
44] or observational data about its behavior [30, 63]. Using a series
of examples motivated by the Target case, we motivate the need to
peek inside the black-box to detect proxy use.
Example 3.1. (Explicit use, Fig. 1a) A retailer explicitly uses preg-
nancy status from prescription data available at its pharmacy to
market baby products.
This form of explicit use of a protected information type can
be discovered by existing black-box experimentation methods that
establish causal effects between inputs and outputs (e.g., see [15,
44]).
Example 3.2. (Inferred use, Fig. 1b) Consider a situation where
purchase history can be used to accurately predict pregnancy sta-
tus. A retailer markets specific products to individuals who have
recently purchased products indicative of pregnancy (e.g., a1, a2 âˆˆ
purchases).
This example, while very similar in effect, does not use health
information directly. Instead, it infers pregnancy status via associ-
ations and then uses it. Existing methods (see [30, 63]) can detect
such associations between protected information types and out-
comes in observational data.
Example 3.3. (No use, Fig. 1c) Retailer uses some uncorrelated
selection of products (a1, n1 âˆˆ purchases) to suggest ads.
In this example, even though the retailer could have inferred
pregnancy status from the purchase history, no such inference was
used in marketing products. As associations are commonplace, a
definition of use disallowing such benign use of associated data
would be too restrictive for practical enforcement.
Example 3.4. (Masked proxy use, Fig. 1d) Consider a more in-
sidious version of Example 3.2. To mask the association between
the outcome and pregnancy status, the company also markets baby
products to people who are not pregnant, but have low retail en-
gagement, so these advertisements would not be viewed in any
case.
While there is no association between pregnancy and outcome in
both Example 3.3 and Example 3.4, there is a key difference between
them. In Example 3.4, there is an intermediate computation based
on aspects of purchase history that is a predictor for pregnancy
status, and this predictor is used to make the decision, and therefore
is a case of proxy use. In contrast, in Example 3.3, the intermediate
computation based on purchase history is uncorrelated with preg-
nancy status. Distinguishing between these examples by measuring
associations using black box techniques is non-trivial. Instead, we
leverage white-box access to the code of the classifier to identify
the intermediate computation that serves as a proxy for pregnancy
status. Precisely identifying the particular proxy used also aids the
normative decision of whether the proxy use is appropriate in this
setting.
âŸ¨X,ğ’œâŸ©
A model, which is a function ğ’œ used for prediction,
operating on random variables X, in population ğ’«
f A function
ğ’«
X A random variable
p A program
ğ’«
âŸ¨X, pâŸ©
[p1/X]p2 A substitution of p1 in place of X in p2
A syntactic model, which is a program p, operating
on random variables X
X A sequence of random variables
Table 1: Summary of notation used in the paper
3.2 Notation and Preliminaries
We assume individuals are drawn from a population distribution
ğ’«, in which our definitions are parametric. Random variables
W , X , Y , Z , . . . are functions over ğ’«, and the notation W âˆˆ ğ’²
represents that the type of random variable is W : ğ’« â†’ ğ’². An
important random variable used throughout the paper is X, which
represents the vector of features of an individual that is provided to a
predictive model. A predictive model is denoted by âŸ¨X,ğ’œâŸ©
, where
ğ’œ is a function that operates on X. For simplicity, we assume that ğ’«
is discrete, and that models are deterministic. Table 1 summarizes
all the notation used in this paper, in addition to the notation for
programs that is introduced later in the paper.
ğ’«
3.2.1 Proxies. A perfect proxy for a random variable Z is a ran-
dom variable X that is perfectly correlated with Z. Informally, if
X is a proxy of Z, then X or Z can be interchangeably used in any
computation over the same distribution. One way to state this is to
require that Pr(X = Z) = 1, i.e. X and Z are equal on the distribu-
tion. However, we require our definition of proxy to be invariant
under renaming. For example, if X is 0 whenever Z is 1 and vice
versa, we should still identify X to be a proxy for Z. In order to
achieve invariance under renaming, our definition only requires
the existence of mappings between X and Z, instead of equality.
Definition 2 (Perfect Proxy). A random variable X âˆˆ ğ’³ is
a perfect proxy for Z âˆˆ ğ’µ if there exist functions f : ğ’³ â†’ ğ’µ, Ğ´ :
ğ’µ â†’ ğ’³ , such that Pr(Z = f (X)) = Pr(Ğ´(Z) = X) = 1.
While this notion of a proxy is too strong in practice, it is useful
as a starting point to explain the key ideas in our definition of
proxy use. This definition captures two key properties of proxies,
equivalence and invariance under renaming.
Equivalence Definition 2 captures the property that proxies ad-
mit predictors in both directions: it is possible to construct a pre-
dictor of X from Z, and vice versa. This condition is required to
ensure that our definition of proxy only identifies the part of the
input that corresponds to the protected attribute and not the input
attribute as a whole. For example, if only the final digit of a zip code
is a proxy for race, the entirety of the zip code will not be identified
as a proxy even though it admits a predictor in one direction. Only
if the final digit is used, that use will be identified as proxy use.
The equivalence criterion distinguishes benign use of associ-
ated information from proxy use as illustrated in the next example.
For machine learning in particular, this is an important pragmatic
medical
records
p r e g n a n t
ad1
2
a
1
,
a
ad1
1
n
1
,
a
ad1
purchases
purchases
notpregnant
ad2
n1,n2
ad2
a2,n2
ad2
retail eng.
2
a
1
,
a
purchases
n1,n2
retail eng.
l o w
high
l o w
high
ad1
ad2
ad2
ad1
(a) Explicit Use
(b) Use via proxy
(c) No use
(d) Masked use via proxy
Figure 1: Examples of models (decision trees) used by a retailer for offering medicines and for selecting advertisements to
show to customers. The retailer uses pregnancy status, past purchases, and customerâ€™s level of retail engagement. Products
a1 and a2 are associated with pregnancy (e.g., prenatal vitamins, scent-free lotions) whereas products n1 and n2 are associated
with a lack of pregnancy (e.g., alcohol, camping gear); all four products are equally likely. Retail engagement, (high or low),
indicating whether the customer views ads or not, is independent of pregnancy.
requirement; given enough input features one can expect any pro-
tected class to be predictable from the set of inputs. In such cases,
the input features taken together are a strong associate in one di-
rection, and prohibiting such one-sided associates from being used
would rule out most machine learnt models.
Example 3.5. Recall that in Figure 1, a1, a2 is a proxy for preg-
nancy status. In contrast, consider Example 3.3, where purchase
history is an influential input to the program that serves ads. Sup-
pose that the criteria is to serve ads to those with a1, n1 in their
purchase history. According to Definition 2, neither purchase his-
tory or a1, n1 are proxies, because pregnancy status does not predict
purchase history or a1, n1. However, if Definition 2 were to allow
one-sided associations, then purchase history would be a proxy
because it can predict pregnancy status. This would have the unfor-
tunate effect of implying that the benign application in Example 3.3
has proxy use of pregnancy status.
Invariance under renaming This definition of a proxy is in-
variant under renaming of the values of a proxy. Suppose that a
random variable evaluates to 1 when the protected information type
is 0 and vice versa, then this definition still identifies the random
variable as a proxy.
3.2.2
Influence. Our definition of influence aims to capture the
presence of a causal dependence between a variable and the output
of a function. Intuitively, a variable x is influential on f if it is
possible to change the value of f by changing x while keeping the
other input variables fixed.
Definition 3. For a function f (x, y), x is influential if and only
if there exists values x1, x2, y, such that f (x1, y) (cid:44) f (x2, y).
In Figure 1a, pregnancy status is an influential input of the sys-
tem, as just changing pregnancy status while keeping all other
inputs fixed changes the prediction. Influence, as defined here, is
identical to the notion of interference used in the information flow
literature.
3.3 Definition
We use an abstract framework of program syntax to reason about
programs without specifying a particular language to ensure that
our definition remains general. Our definition relies on syntax
to reason about decompositions of programs into intermediate
ğ’«
given meaning by a denotation function(cid:74)Â·(cid:75)X that maps programs
, then(cid:74)p(cid:75) is a function on variables in X,
and(cid:74)p(cid:75)(X) represents the random variable of the outcome of p,
computations, which can then be identified as instances of proxy
use using the concepts described above.
Program decomposition We assume that models are represented
by programs. For a set of random variables X, âŸ¨X, pâŸ©
denotes the
assumption that p will run on the variables in X. Programs are
to functions. If âŸ¨X, pâŸ©
when evaluated on the input random variables X. Programs sup-
port substitution of free variables with other programs, denoted by
[p1/X]p2, such that if p1 and p2 programs that run on the variables
X and X, X, respectively, then [p1/X]p2 is a program that operates
on X.
A decomposition of program p is a way of rewriting p as two
programs p1 and p2 that can be combined via substitution to yield
the original program.