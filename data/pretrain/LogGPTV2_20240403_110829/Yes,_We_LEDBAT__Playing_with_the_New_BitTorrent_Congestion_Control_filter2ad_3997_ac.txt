OWD delay is added either to the forward path (top) or backward (bottom) path: in
the former case, the delay incrementally adds to the OWD estimation performed by the
sender so that it may directly affect the congestion control loop, while in the latter case
it only delays the acknowledgement and may only indirectly affect the control loop.
As it can be seen from the comparison of the top and bottom plots of Fig. 3-(a), the
overall effect on performance is the same: BitTorrent throughput decreases for increas-
ing RTT, which is due to an upper bound of the receiver window (analogously to what
seen before for TCP). With some back-of-the-envelope calculation based on the exper-
imental results shown in Fig. 3-(a), one can gather that the receiver window limit has
been increased from 20 full-payload segments of α2 to 30 full-payload segment of β1.
While the picture shows that this limit may not be enough to fully utilize the link capac-
ity (e.g., β1 achieves about 4 Mbps throughput on a 10 Mbps link with RTT=100 ms), in
practice it is not a severe constraint, as the capacity will likely be shared across several
ﬂows established with multiple peers of a BitTorrent swarm (or the receiver window
limit could be increased).
In Fig. 3-(b) we instead investigate the effects of a variable OWD delay, that changes
for each packet uniformly at random, with average OWD equal to 20 ms. In this case
we keep the average constant but increase the delay variance every 2 minutes, so that
the proﬁle reports the minimum and maximum delays of the uniform distribution. The
variable delay also implies that packet order is not guaranteed, because packets en-
countering a larger delay will be received later and thus out-of-order. Again, delay vari-
ance is enforced on either the forward (top) or backward (bottom) path. As it can be
Yes, We LEDBAT: Playing with the New BitTorrent Congestion Control Algorithm
37
]
s
p
b
M
[
t
u
p
h
g
u
o
r
h
T
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
 50
TCP
β
1
 200
 150
 100
Time [s]
(a)
]
s
p
b
M
[
t
u
p
h
g
u
o
r
h
T
 0.8
 0.6
 0.4
 0.2
 0
 0
TCP FWD
TCP BWD
β
1
RTT
 100
 200
 300
 400
 500
Time [s]
(b)
 5
 4
 3
 2
 1
 0
 600
]
s
[
T
T
R
Fig. 4. Real Internet experiments: (a) different ﬂavors and (b) interfering trafﬁc
expected, LEDBAT is rather robust to a variable jitter on the backward path, where we
observe only a minimal throughput reduction. Conversely, variance in the forward path
has a much more pronounced performance impact: interestingly, α2 throughput signif-
icantly drops, whereas β1 performance is practically unchanged. This probably hints
to the use of a more sophisticated noise ﬁltering algorithm (e.g., that discards delay
samples of out-of-order packets), although a more careful analysis is needed to support
this assertion.
We ﬁnally perform an experiment using PCs connected through ADSL modems to
the wild Internet. Thus, in this case we no longer have complete control over the net-
work environment, but we still can assume that no congestion happens in the network
and that the access link constitutes the capacity bottleneck. It can be seen from Fig. 4-(a)
that in a realistic scenario, when the end-hosts only run LEDBAT, β1 achieves a smooth
throughput whose absolute value closely matches the nominal ADSL uplink capacity
(640 Kbps). In contrast, TCP throughput is more ﬂuctuating due to self-induced con-
gestion, which causes fairly large queues before eventual losses occur. This conﬁrms
that the goal of avoiding self-induced congestion at the access is also met.
3.2 Multiple Flows
We now explore scenarios with several concurrent ﬂows, starting with the simple one
where a single LEDBAT ﬂow interacts with a single TCP ﬂow. Considering two PCs
connected through ADSL modems to the wild Internet, Fig. 4-(b) reports an experiment
where, during a single LEDBAT transfer, we alternate periods in which PCs generate
no trafﬁc other than LEDBAT, to periods (i.e., the gray ones) in which we superpose
TCP trafﬁc on either the forward or backward path.
The plot reports the time evolution of the LEDBAT throughput as well as the RTT
delay measured by ICMP (as a rough estimation of the queue size seen by LEDBAT).
During the silence periods (0–120 s and 240–360 s), as bottleneck is placed at the edge
of the network, LEDBAT is able to efﬁciently exploit the link rate. As soon as a back-
logged TCP transfer is started on the forward path (120–240 s), LEDBAT congestion
control correctly puts the trafﬁc in low priority. Notice that in this case, ICMP reports
that a fairly large queue of TCP data packets builds up in the ADSL line (roughly 4
seconds, corresponding to about 300 KB of buffer space for the nominal ADSL rate).
Conversely, whenever the backlogged TCP transfer is started on the backward path
(360–480 s), LEDBAT transfer on the forward direction should only be minimally
38
D. Rossi, C. Testa, and S. Valenti
Table 1. Efﬁciency and Fairness between multiple TCP and LEDBAT ﬂows
TCPW , LEDBAT β1
TCPL, LEDBAT β1
TCP LEDBAT %1 %2 %3 %4 η Fairness RTX% %1 %2 %3 %4 η Fairness RTX%
0.06
0.14
4e-3
5e-4
0
1
2
3
4
0.25 0.25 0.25 0.25 0.67
0.14 0.14 0.14 0.57 0.94
0.10 0.10 0.40 0.40 0.93
0.08 0.31 0.31 0.31 0.92
0.25 0.27 0.24 0.24 0.96
1.00
0.64
0.74
0.87
1.00
4
3
2
1
0
0.25 0.25 0.25 0.25 0.98
0.35 0.32 0.32 0.00 0.98
0.43 0.51 0.03 0.03 0.98
0.87 0.04 0.04 0.05 0.98
0.25 0.27 0.24 0.24 0.96
1.00
0.75
0.56
0.33
1.00
-
-
-
-
-
-
affected by the amount of acknowledgement TCP trafﬁc ﬂowing in the forward di-
rection. However, as it can be seen from Fig. 4-(b), the LEDBAT throughput drastically
drops, further exhibiting very wide ﬂuctuations (notice also that the ADSL modem
buffer space of the receiver appears to be smaller, as the RTT is shorter). Notice that in
this case, LEDBAT forward data path shares the link capacity only with TCP acknowl-
edgements, which account for a very low, but likely very bursty, throughput: this may
led LEDBAT into a messy queuing delay estimate, and as a result, the uplink capacity
of the device is heavily underutilized (about 74% of wasted resources).
We ﬁnally perform experiments to analyze the interaction of several ﬂows. In this
case, we setup several torrents, one for every different LEDBAT seeder-leecher pair,
so that no data exchange happens between leechers of different pairs. Thus, ﬂows are
independent at the application layer, though their are dependent at the transport layer,
as they share the same physical 10 Mbps RTT=50 ms bottleneck.
We consider a ﬁxed number of F=4 ﬂows, and vary the number of TCP and LEDBAT-
β1 connections to explore their mutual inﬂuence. All ﬂows start at time t = 0, exper-
iments last 10 minutes and results refer to the last 9 minutes of the experiment. We
generate TCP trafﬁc using Linux (so that we can reliably gather retransmission statis-
tics using netstat), setting the congestion control ﬂavor to NewReno. We perform
two set of experiments, using either the Windows or Linux defaults values for the max-
imum receiver windows as early stressed in Fig. 2-(a): in our setup, the Windows-like
TCP settings (TCPW ) are thus less aggressive than Linux ones (TCPL).
For each experiment, we evaluate user-centric performance by means of the break-
down of the resources acquired by each ﬂow, while we express network-centric per-
formance in terms of the link utilization η. To further quantify the protocol mutual
inﬂuence, we use the Jain’s fairness index of the ﬂows throughput and evaluate the
percentage of TCP retransmissions (RTX). Results are reported in Tab. 1, with Win-
dows and Linux settings on the left and right respectively. Comparing the two table
portions, we argue that the exact meaning of “low-priority” may be fuzzy in the real-
world. Indeed, while LEDBAT-β1 is lower priority than an “aggressive” TCP, it may
be competing more fairly against a more gentle set of parameters, thus being at least
as high priority as TCP. In fact while LEDBAT is practically starved by TCPL, LED-
BAT is able to achieve a slightly higher priority than TCPW . Although we recognize
that results may change using more realistic and heterogeneous network scenarios, or
using the real Windows stack instead of simply emulating its settings, we believe that
an important point remains open: i.e., the precise meaning of “lower than best effort”,
as the mutual inﬂuence of TCP and LEDBAT trafﬁc may signiﬁcantly differ depending
on the TCP ﬂavor as well.
Yes, We LEDBAT: Playing with the New BitTorrent Congestion Control Algorithm
39
4 Related Work
Two bodies of work are related to this study. On the one hand, BitTorrent has been stud-
ied by means of theoretical analysis [8], simulation [9, 10, 6] or measurements [11]. On
the other hand, there is a large literature on Internet congestion control that use either on
ﬁelds measurement [12,13,14], or simulation and modeling [15,16,17,18,19,20]. Due
to BitTorrent very recent evolution, with the exception [6], where we study LEDBAT
by means of simulation, previous work on BitTorrent [8, 9, 10, 11] focused on comple-
mentary aspects to those analyzed in this work. In [8] a ﬂuid model is used to determine
the average download time of a single ﬁle. Simulation has instead been used in [9] to
propose incentive mechanism to avoid free-riding and in [10] to assess the performance
of a locality-aware peer selection strategy. Finally, measurements study [11] analyzes
the log of a BitTorrent tracker, examining ﬂash-crowd effect, popularity and download
speed of a single ﬁle. Congestion control work closer to our adopts a black-box experi-
mental measurements approach to unveil proprietary algorithms of, e.g., Skype [12,13]
or P2P-TV applications [14]. More precisely, [12, 14] analyzes system reaction to em-
ulated network conditions, whereas [13] investigates the bottleneck share of multiple
ﬂows. Finally, relevant work has been devoted to the design of lower-than best effort
protocols similar to LEDBAT, as for instance [17, 18, 19, 20].
5 Conclusions
This paper presented an experimental evaluation of LEDBAT, the novel BitTorrent con-
gestion control protocol. Single-ﬂow experiments in a controlled environment show
some of the fallacies of earlier LEDBAT ﬂavors (e.g., instability, small packets overkill,
starvation at low throughput, tuning of maximum receiver windows, wrong estimate
of one-way delay in case of packet reordering, etc.), that have been addressed by the
latest release. Experiments in a real Internet environment, instead, show that, although
LEDBAT seems a promising protocol (e.g., achieving a much smoother throughput and
keeping thus the delay on the link low), some issues still need to be worked out (e.g.,
performance in case of reverse path trafﬁc). Finally, multiple-ﬂows experiments show
that “low-priority” meaning signiﬁcantly varies depending on the TCP settings as well.
This work constitutes a ﬁrst step toward the analysis LEDBAT performance. More
effort is indeed needed to build a full relief picture of the LEDBAT impact on other inter-
active applications (e.g., VoIP, gaming), explicitly taking into account the QoE resulting
from their interaction. Also, the methodology could be reﬁned by, e.g., instrumenting
the Linux kernel to measure the queue size, or by inferring the OWD measured by
LEDBAT by snifﬁng trafﬁc at both the sender and receiver, etc. Finally, the boundaries
of the investigation could be widened by taking into account the effects of LEDBAT
adoption on the BitTorrent P2P system itself, as for instance LEDBAT interaction with
throughput based peer-selection mechanism, or its impact on ﬁles download time.
Acknowledgement
This work has been funded by the Celtic project TRANS.
40
D. Rossi, C. Testa, and S. Valenti
References
1. Morris, S.: µTorrent release 1.9 alpha 13485 (December 2008), http://forum.
utorrent.com/viewtopic.php?pid=379206#p379206
2. Bennett, R.: The
next
Internet meltdown
(December
2008), http://www.
theregister.co.uk/2008/12/01/richard_bennett_utorrent_udp
3. Shalunov, S., Klinker, E.: Users want P2P, we make it work. In: IETF P2P Infrastructure
Workshop (May 2008)
4. BitTorrent Calls UDP Report ”Utter Nonsense” (December 2008),
http://tech.slashdot.org/article.pl?sid=08/12/01/2331257
5. Shalunov, S.: Low extra delay background transport (LEDBAT). IETF Draft (March 2009)
6. Rossi, D., Testa, C., Valenti, S., Veglia, P., Muscariello, L.: News from the internet congestion
control world. Technical Report (August 2009)
7. MS Windows Developer Center: Tcp receive window size and window scaling,
http://msdn.microsoft.com/en-us/library/ms819736.aspx
8. Qiu, D., Srikant, R.: Modeling and performance analysis of BitTorrent-like peer-to-peer net-
works. In: ACM SIGCOMM 2004, Portland, Oregon, USA (August 2004)
9. Bharambe, A.R., Herley, C., Padmanabhan, V.N.: Analyzing and Improving a BitTorrent
Networks Performance Mechanisms. In: IEEE INFOCOM 2006, Barcelona, Spain (April
2006)
10. Bindal, R., Cao, P., Chan, W., Medved, J., Suwala, G., Bates, T., Zhang, A.: Improving Traf-
ﬁc Locality in BitTorrent via Biased Neighbor Selection. In: IEEE ICDCS 2006, Lisboa,
Portugal (July 2006)
11. Izal, M., Urvoy-Keller, G., Biersack, E.W., Felber, P., Al Hamra, A., Garc´es-Erice, L.: Dis-
secting BitTorrent: Five Months in a Torrent’s Lifetime. In: Barakat, C., Pratt, I. (eds.) PAM
2004. LNCS, vol. 3015, pp. 1–11. Springer, Heidelberg (2004)
12. Bonﬁglio, D., Mellia, M., Meo, M., Rossi, D.: Detailed Analysis of Skype Trafﬁc. IEEE
Transaction on Multimedia 11(1) (January 2009)
13. De Cicco, L., Mascolo, S., Palmisano, V.: Skype video responsiveness to bandwidth varia-
tions. In: ACM NOSSDAV 2008, Braunschweig, Germany (May 2008)
14. Alessandria, E., Gallo, M., Leonardi, E., Mellia, M., Meo, M.: P2P-TV Systems under Ad-
verse Network Conditions: A Measurement Study. In: IEEE INFOCOM 2009 (April 2009)
15. Padhye, J., Firoiu, V., Towsley, D., Kurose, J.: Modeling TCP throughput: a simple model
and its empirical validation. ACM SIGCOMM Comp. Comm. Rev. 24(4) (October 1998)
16. Brakmo, L., O’Malley, S., Peterson, L.: TCP Vegas: New techniques for congestion detection
and avoidance. In: ACM SIGCOMM 1994, London, UK (August 1994)
17. Venkataramani, A., Kokku, R., Dahlin, M.: TCP Nice: a mechanism for background trans-
fers. In: USENIX OSDI 2002, Boston, MA, US (December 2002)
18. Kuzmanovic, A., Knightly, E.: TCP-LP: low-priority service via end-point congestion con-
trol. IEEE/ACM Transaction on Networking 14(4) (August 2006)
19. Liu, S., Vojnovic, M., Gunawardena, D.: Competitive and Considerate Congestion Control
for Bulk Data Transfers. In: IWQoS 2007, Evaston, IL, US (June 2007)
20. Key, P., Massouli´e, L., Wang, B.: Emulating low-priority transport at the application layer: a
background transfer service. In: ACM SIGMETRICS 2004, New York, NY, USA (January
2004)