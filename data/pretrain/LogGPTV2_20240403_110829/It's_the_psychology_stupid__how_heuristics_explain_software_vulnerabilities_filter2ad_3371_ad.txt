compilers, etc.) bring security information or prime developers on
the spot when they need it: while coding. This priming informa-
tion should be closely related to their current working scenario to
increase the chances that this security cue will be included in the
developer’s heuristics. Our insight will inﬂuence the next gener-
ation of tools and applications for developers so that more secure
software reach the market. While such paradigm does not solve the
multifaceted challenges of cyber security, it can illuminate develop-
ers’ mental blind spots in vulnerabilities and secure programming.
7. RELATED WORK
The work presented in this paper intersects with the areas of vul-
nerability analysis, information security perception, and cognitive
and human factors. This section discusses the related work in these
areas.
7.1 Vulnerability Studies
The ﬁrst effort towards understanding software vulnerabilities
appeared in the 1970’s through the RISOS Project that investigates
security ﬂaws in operating systems [29]. Around the same time,
Protection Analysis study [30] focused on developing vulnerabil-
ity detection tools to assist developers. Other vulnerability stud-
ies followed, such as the taxonomies by Landwehr et al. [31] and
Aslam [32]. In the 1990’s, Bishop and Bailey [33] analyzed cur-
rent vulnerability taxonomies and concluded that they are imper-
fect: depending on the layer of abstraction that a vulnerability was
considered, it could be classiﬁed in multiple ways. More recently
Crandall and Oliveira [34] proposed a view of software vulnera-
bilities as fractures in the interpretation of information as it ﬂows
across boundaries of abstraction.
There are also discussions about the theoretical and computa-
tional science of exploit techniques and proposals for explicit pars-
ing and normalization of inputs. Bratus et al. [35] discussed a
view that the theoretical language aspects of computer science lie
at the heart of practical computer security problems, especially ex-
ploitable vulnerabilities. Samuel and Erlingsson [36] proposed in-
put normalization via parsing as an effective way to prevent vulner-
abilities that allow attackers to break out of data contexts. Garg and
Camp [37] identiﬁed systematic errors by decision-makers leverag-
ing heuristics as a way to improve security designs for risk averse
people.
Researchers have also studied vulnerability trends. A study by
Browne et al. [38] determined that the rates at which incidents were
reported to CERT could be mathematically modeled. Gopalakr-
ishna and Spafford [39] analyzed software vulnerabilities in ﬁve
critical software artifacts using information from public vulnera-
bility databases to predict trends. Alhazmi et al. [40] presented a
vulnerability discovery model to predict long and short term vul-
nerabilities for several major operating systems. Anbalagan and
Vouk [41] analyzed and classiﬁed thousands of vulnerabilities from
OSVDB [42] and discovered a relationship between vulnerabilities
and exploits. Wu et al. [43] performed an ontology-guided analysis
of vulnerabilities and studied how semantic templates can be lever-
aged to identify further information and trends. Zhang et al. [44]
analyzed vulnerabilities from the NVD database using machine
learning to unsuccessfully discover the time to the next vulnera-
bility for a given software application.
There are also studies on developer’s practices. Meneely and
Williams [45] studied developers collaboration and unfocused con-
tributions into developer activity metrics and statistically correlated
them. Schryen [46] analyzed the patching behavior of software
vendors of open-source and closed-source software, and found that
the policy of a particular software vendor is the most inﬂuential
factor on patching behavior.
However, none of the above research efforts directly leveraged
the human factor to understand software vulnerabilities as proposed
in this paper.
7.2 Information Security Perception
Asghapours et al. [47] advocate the use of mental models of
computer security risks for improvement of risk communication
to naive end users. Risk communication consists of security ex-
perts messages to non-experts and a mental model in a simpliﬁed
internal concept of how something works in reality. In their study
they leveraged ﬁve conceptual models from the literature: Physical
Safety, Medical Infections, Criminal Behavior, Warfare, and Eco-
nomic Failures.
Huang et al.[48] studied ways to adjust people’s perception of in-
formation security to increase their intention to adopt IT appliances
and compliance with security practices. Their user study involving
e-banking and passwords showed that knowledge is a key factor in-
ﬂuencing the gap between people’s perceived security and a system
real security.
Garg and Camp [49] leverage the classic Fischhoff’s canonical
nine dimensional model of ofﬂine risk perception [50] to better un-
derstand online risk perceptions. They found that the results ob-
tained for online risks differed from the ones obtained for ofﬂine
risks and that the severity of a risk was the biggest factor in shap-
ing risk perception.
Research has also been done in the area of computer warnings for
end users [51, 52]. In computer security, warnings are designed to
protect people from becoming victims of attacks, such as phishing,
malware installation, e-mail spam, etc. Researchers have found that
people tend to not pay attention to messages that do not map well
onto a clear course of action [51]. This corroborates our hypothesis
that unless the cue is related to the working scenario at hand, it
will likely to be left out from the decision-maker’s repertoire of
(cid:22)(cid:19)(cid:22)
heuristics.
cueing effectiveness and previous security education.
All these studies consider information security perception from
the non-expert end user viewpoint and not developers.
7.3 Human Factors in Software Development
Using human factors in technology research is not a new con-
cept. Curtis, Krasner, and Iscoe [53] studied the software develop-
ment processes by interviewing programmers from 17 large soft-
ware development projects. They tried to understand the effect of
behavioral and cognitive processes in software productivity. They
believed software quality in general could be improved by attack-
ing the problems they discovered in this exploratory research. They
summarized the study by describing the implication of their inter-
views and observations on different aspects of the software devel-
opment process, including team building, software tools and devel-
opment environment and model.
Others also recognized the role of cognition in program repre-
sentation and comprehension [54, 55], design strategies and pat-
terns [56, 57], and software design [58, 59]. These studies show
the evolution of design paradigm and development tools from task-
centered to human-centered. Current software development tools
are very good at pinpointing errors and making sensible sugges-
tions to avoid problems later. New derivatives are created to assist
programmers. They have helped the software development process
to be less error-prone in general. These studies paved the way for
secure software development from the human aspect.
We believe this paper leverages those studies as stepping stones
and investigate deeper in the human factor issues, in particular, to
understand the impact of cognitive processes on software vulner-
abilities. Like previous studies that lead to better software devel-
opment tool, faster turnout, and more robust software integration,
the insights obtained from this study can help the software security
community gain insights, improve software security, better the de-
sign of guidelines, and build more effective vulnerability blind spot
tools.
8. CONCLUSIONS
This paper investigated a hypothesis that software vulnerabilities
are blind spots in developers’ heuristics in their daily coding activ-
ities. Humans have been hardwired through evolution for adopting
shortcuts and heuristics in decision making due to the limitations
in their working memory. As vulnerabilities lie in uncommon code
paths and we have a market that generates perverse incentives for
insecure software, security information is often left behind from
developer’s repertoire of coding strategies.
A study with 47 developers using psychological manipulation
was conducted to validate this hypothesis. In this study each de-
veloper worked for approximately one hour on six programming
scenarios that contained vulnerabilities. The developers were told
that the study’s goal was to understand developer’s mental models
while coding. The sessions progressed from providing no infor-
mation at all about possible vulnerabilities, to priming developers
about unexpected results, and explicitly mentioning the existence
of vulnerabilities in the code. The results show that developers
in general changed their mindset towards security when primed
about vulnerabilities on the spot. When not primed, even devel-
opers familiar with certain vulnerabilities failed to correlate them
with their working scenario and ﬁx them in the code when given a
chance. Therefore, the assumption that developers should be edu-
cated about security and then apply what they learned while coding
goes against the way the human brain behaves. This paper advo-
cates that this assumption be reversed and that security information
should reach developers when they need it, on the spot and corre-
lated to their tasks at hand. The authors hope that these insights can
inﬂuence the next generation of tools interfacing developers, such
as IDE’s, text editors, browsers and compilers so that more secure
software reach the market.
Plans for future work include an investigation of the best method-
ologies for cueing developers and an analysis of the correlation of
Acknowledgments
We would like to thank the developers who participated in this
study and the anonymous reviewers for valuable feedback. This re-
search is funded by the National Science Foundation under grants
CNS-1149730, CNS-1223588, and CNS-1205415.
9. REFERENCES
[1] “Symantec Internet Security Threat Report 2013.”
http://www.symantec.com/content/en/us/
enterprise/other_resources/b-istr_main_
report_v18_2012_21291018.en-us.pdf.
[2] B. K. Marshall, “PasswordResearch.com Authentication
News: Passwords Found in the Wild for January 2013.”
http:
//blog.passwordresearch.com/2013/02/
passwords-found-in-wild-for-january-2013.
html.
[3] “Seventeen steps to safer C code.”
http://www.embedded.com/design/
programming-languages-and-tools/4215552/
Seventeen-steps-to-safer-C-code.
[4] D. Kahneman and A. Tversky, “On the reality of cognitive
illusions,” Psychological Review, pp. 582–591, 1996.
[5] G. Gigerenzer, R. Hertwig, and T. Pachir, Heuristics: The
Foundations of Adaptive Behavior. Oxford University Press,
2011.
[6] B. Schwartz, “The tyranny of choice,” Scientiﬁc American,
pp. 71–75, 2004.
[7] S. Botti and S. S. Iyengar, “The dark side of choice: When
choice impairs social welfare,” American Marketing
Association, pp. 24–38, 2006.
[8] C. Kern, A. Kesavan, and N. Daswani, Foundations of
security: what every programmer needs to know. Apress,
2007.
[9] E. Harmon-Jones, D. M. Amodio, and L. R. Zinner, Social
psychological methods of emotion elicitation (Handbook of
Emotion Elicitation and Assessment). Oxford University
Press, 2007.
[10] W. Thorngate, “Efﬁcient decision heuristics,” Behavioral
Science, vol. 25, no. 3, pp. 219–225, 1980.
[11] K. V. Katsikopoulos, “Efﬁcient decision heuristics,” Decision
Analysis, vol. 8, no. 1, pp. 10–29, 2011.
[12] J. W. Payne, J. R. Bettman, and E. J. Johnson, The Adaptive
Decision Maker. Cambridge University Press, 1993.
[13] G. K. Zipf, Human Behavior and The Principle of Least
Effort. Addison-Wesley, 1949.
[14] J. Rieskamp and U. Hoffrage, Simple Heuristics that Make
Us Smart. Oxford University Press, 1999.
[15] C. Cowan, C. Pu, D. Maier, J. Walpole, P. Bakke, S. Beattie,
A. Grier, P. Wagle, Q. Zhang, and H. Hinton, “StackGuard:
Automatic adaptive detection and prevention of
buffer-overﬂow attacks,” in USENIX Security, pp. 63–78, Jan
1998.
[16] G. Wassermann and Z. Su, “Static Detection of Cross-site
Scripting Vulnerabilities,” in 30th International conference
on Software engineering, ICSE ’08, (New York, NY, USA),
ACM, 2008.
[17] Z. Su and G. Wassermann, “The Essence of Command
Injection Attacks in Web Applications,” in Conference
Record of the 33rd ACM SIGPLAN-SIGACT Symposium on
Principles of Programming Languages, POPL ’06, (New
York, NY, USA), pp. 372–382, ACM, 2006.
[18] “Urllib and validation of server certiﬁcate.” http:
//stackoverflow.com/questions/6648952/.
(cid:22)(cid:19)(cid:23)
Engineering for Secure Systems, 2010.
[44] S. Zhang, D. Caragea, and X. Ou, “An Empirical Study on
using the National Vulnerability Database to Predict
Software Vulnerabilities,” International Conference on
Database and Expert Systems Applications (DEXA), 2011.
[45] A. Meneely and L. Williams, “Secure Open Source
Collaboration: An Empirical Study of Linus’ Law,” ACM
CCS, pp. 453–462, 2009.
[46] G. Schryen, “A comprehensive and comparative analysis of
the patching behavior of open source and closed source
software vendors,” IMF, 2009.
[47] F. Asgapour, D. Liu, and L. J. Camp, “Mental models of
computer security risks,” Financial Cryptography and Data
Security Lecture Notes in Computer Science, vol. 4886,
pp. 367–377, 2007.
[48] D.-L. Huang, Pei-Luen, P. R. abd Gavriel Salvendya,
F. Gaoa, and J. Zhoua, “Factors affecting perception of
information security and their impacts on it adoption and
security practices,” International Journal of
Human-Computer Studies, vol. 69, no. 12, 2011.
[49] V. Garg and L. J. Camp, “End user perception of online risk
under uncertainty,” Hawaii International Conference On
System Sciences, vol. 4886, 2012.
[50] B. Fischhoff, P. Slovic, S. Lichtenstein, and B. C.
Stephen Read, “How safe is safe enough? a osychometric
study of attitudes towards technological risks and beneﬁts,”
Policy Sciences, vol. 9, no. 2, 1978.
[51] C. Bravo-Lillo, L. Cranor, J. Downs, and S. Komanduri,
“Bridging the gap in computer security warnings: A mental
model approach,” IEEE Security and Privacy, vol. 9, no. 2,
2011.
[52] K. Witte, “Putting the fear back into fear appeals: The
extended parallel process model,” Communication
Monographs, vol. 59, no. 4, pp. 329–349, 1992.
[53] B. Curtis, H. Krasner, and N. Iscoe, “A ﬁeld study of the
software design process for large systems,” Communications
of the ACM, vol. 31, no. 11, pp. 1268–1287, 1988.
[54] S. Letovsky, “Cognitive processes in program
comprehension,” Journal of Systems and software, vol. 7,
no. 4, pp. 325–339, 1987.
[55] H. C. Purchase, L. Colpoys, M. McGill, D. Carrington, and
C. Britton, “Uml class diagram syntax: an empirical study of
comprehension,” in Proceedings of the 2001 Asia-Paciﬁc
symposium on Information visualisation-Volume 9,
pp. 113–120, Australian Computer Society, Inc., 2001.
[56] A. Chatzigeorgiou, N. Tsantalis, and I. Deligiannis, “An
empirical study on students ability to comprehend design
patterns,” Computers & Education, vol. 51, no. 3,
pp. 1007–1016, 2008.
[57] W. Visserl, J.-M. Hocz, and F. Chesnay, “Expert software
design strategies,” 1990.
[58] R. Jeffries, A. A. Turner, P. G. Polson, and M. E. Atwood,
“The processes involved in designing software,” Cognitive
skills and their acquisition, pp. 255–283, 1981.
[59] B. Adelson and E. Soloway, “A model of software design,”
International Journal of Intelligent Systems, vol. 1, no. 3,
pp. 195–213, 1986.
[19] W. S. McPhee, “Operating System Integrity in OS/VS2,”
IBM Systems Journal, vol. 13, no. 3, pp. 230–252, 1974.
[20] A. Narayanan and V. Shmatikov, “Fast dictionary attacks on
passwords using time-space tradeoff,” ACM CCS, 2005.
[21] R. E. Stake, Qualitative Research: Studying How Things
Work. The Guilford Press, 2010.
[22] “Qualtrics (http://www.qualtrics.com/).”
[23] F. Gravetter and L. Wallnau, Statistics for the Behavioral
Sciences. Wadsworth/Thomson Learning, 8th ed., 2009.
[24] R. S. Weiss, Learning from Strangers - The Art and Method
of Qualitative Interview Studies. The Free Press, 1994.
[25] J. Saldana, The Coding Manual for Qualitative Researchers.
SAGE Publications, 2012.
[26] A. Newell and H. Simon, Human Problem Solving. Prentice
Hall, 1972.
[27] D. Denning, “A lattice model of secure information ﬂow,”
Communications of ACM, 1976.
[28] R. Anderson, “Why information security is hard - an
economic perspective,” ACSAC, 2001.
[29] R. P. Abbot, J. S. Chin, J. E. Donnelley, W. L. Konigsford,
and D. A. Webb, “Security Analysis and Enhancements of
Computer Operating Systems,” NBSIR 76-1041, Institute for
Computer Sciences and Technology, National Bureau of
Standards, 1976.
[30] R. B. II and D. Hollingsworth, “Protection Analysis Project
Final Report,” ISI/RR-78-13, DTIC AD A056816,
USC/Information Sciences Institute, 1978.
[31] C. E. Landwehr, A. R. Bull, J. P. McDermott, and W. S.
Choi, “A Taxonomy of Computer Program Security Flaws,”
ACM Computing Surveys, vol. 26, no. 3, 1994.
[32] T. Aslam, “A Taxonomy of Security Faults in the UNIX
Operating System,” 1995.
[33] M. Bishop and D. Bailey, “A Critical Analysis of
Vulnerability Taxonomies,” Technical Report CSE-96-11,
University of California at Davis, 1996.
[34] J. Crandall and D. Oliveira, “Holographic Vulnerability
Studies: Vulnerabilities as Fractures in Interpretation as
Information Flows Across Abstraction Boundaries,” New
Security Paradigms Workshop (NSPW), 2012.
[35] S. Bratus, M. E. Locasto, M. L. Patterson, L. Sassaman, and
A. Shubina, “Exploit Programming: From Buffer Overﬂows
to “Weird Machines” and Theory of Computation.” USENIX
;login, December 2011.
[36] M. Samuel and U. Erlingsson, “Let’s Parse to Prevent
pwnage (invited position paper),” in Proceedings of the 5th
USENIX conference on Large-Scale Exploits and Emergent
Threats, LEET’12, (Berkeley, CA, USA), pp. 3–3, USENIX
Association, 2012.
[37] V. Garg and L. J. Camp, “Heuristics and biases: Implications
for security,” IEEE Technology & Society, March 2013.
[38] H. K. Browne, W. A. Arbaugh, J. McHugh, and W. L. Fithen,
“A trend analysis of exploitations,” IEEE Symposium on
Security and Privacy, 2001.
[39] R. Gopalakrishna and E. H. Spafford, “A Trend Analysis of
Vulnerabilities,” CERIAS Tech Report 2005-05, 2005.
[40] O. H. Alhazmi and Y. K. Malaiya, “Prediction capabilities of
vulnerability discovery models,” IEEE Reliability and
Maintainability Symposium (RAMS), pp. 86–91, 2006.
[41] O. H. Alhazmi and Y. K. Malaiya, “Towards a unifying
approach in understanding security problems,” IEEE
International Conference on Software Reliability
Engineering (ISSRE), pp. 136–145, 2009.
[42] “Open Source Vulnerability Database
(http://www.osvdb.org/).”
[43] Y. Wu, R. A. Gandhi, and H. Siy, “Using Semantic
Templates to Study Vulnerabilities Recorded in Large
Software Repositories,” ICSE Workshop on Software
(cid:22)(cid:19)(cid:24)