reduces identiﬁability substantially, but doesn’t eliminate it.
Knowing the hour of the observation as well as the website’s
domain and location of the user, four observations sufﬁce to
correctly identify a quarter of click traces. Once the adversary
has made seven or more observations, knowing merely the day
is sufﬁcient to identify 20% of browsing sessions.
We ﬁnally want to assess whether trimming of click traces
is as effective at reducing identiﬁability as it is at reducing
unicity. Fig. 11 plots the identiﬁability of click traces trimmed
to length l, given prior knowledge of a number of observations,
while limiting the maximum length of click traces in the
database. Longer click traces exhibit higher unicity, but not
necessarily higher identiﬁability. This is because we assume
the same adversary strength in both cases, translating to a
relatively smaller observation size in the case of the longer
click trace. Interestingly, these effects (increased unicity, but
smaller subtrace) appear to cancel each other out for the most
part. At the lower end of maximum length we observe a
statistically signiﬁcant but very small increased identiﬁability
for higher length limits and at the mid and high end the results
become almost indistinguishable.
VI. RELATED WORK
We investigate the emergence of pseudonymous data and
identiﬁability of click traces in databases of Web trackers. The
primary research areas related to our work pertain to identi-
ﬁability of online tracking and browsing data and sequential
identiﬁability in general.
A. Online Tracking
Third party online tracking mechanisms, such as the ones
which were used to generate our data, are widely used [3].
Cookies remain the most common form [1], and techniques
such as “evercookies” and “cookie syncing” continue to make
them a very resilient and reliable tracking tool [20]. Browser
ﬁngerprints are a more recent technique which attempts to
identify users via information extracted from their browser.
The potential of this approach has been demonstrated by
Eckersley and others [21], [22], [23] by leveraging a com-
bination of seemingly benign information to generate highly
identiﬁable ﬁngerprints. Besides such browser speciﬁc infor-
mation, Upathilake et al. [24] identify additional categories of
ﬁngerprints: Based on Canvas [25], JavaScript Engine [26],
and Cross-browser.
While our work is based on data obtained through online
tracking, the data was gathered entirely through third party
cookies and we do not generate ﬁngerprints in the conventional
sense. In fact we purposefully disregard a large amount
of information that would traditionally be used to generate
ﬁngerprints to demonstrate that identiﬁability can be achieved
in other ways.
B. Browsing History
Broadly speaking we analyze the metadata of browsing ses-
sions. We thus implicitly analyze browsing behavior. However,
research corresponding to browsing behavior usually analyzes
how users tend to navigate through websites [27], rather than
privacy in web tracking.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 04:12:30 UTC from IEEE Xplore.  Restrictions apply. 
788
Olejnik et al. [28], [29] do examine the uniqueness of
browsing history patterns and their work strongly relates to
ours. However, their analysis of unicity diverges signiﬁcantly
from our work. They do not investigate unicity as it emerges in
cross site tracking, but rather explore differences in repetitive
browsing behavior, completely observing the users in all their
actions. The effect of anonymizing, or coarsening measures is
also not considered in their work.
Browsing history is also used by Su et. al [30] to link a
data subject to their social media proﬁle by scraping proﬁle
activity and matching the visited sites to the history. Again,
this represents an attempt to identify proﬁles from a local
observer’s view, unlike our analysis of global tracking data.
Finally, Yu et. al [2] classify certain pieces of information as
particularly identiﬁable (or “unsafe”). Their scale of evaluation
is similar to ours, with a comprehensive overview of the
German online sphere. However, while we focus our analysis
on the anonymizability of the datasets, they took a more local
approach and implemented a client based browser extension
as a means of privacy protection.
C. Trace Unicity
The notion that sequences of data fragments leak private
information has been explored before. Papadimitriou et al. [31]
studied time-series compressibility and privacy using perturba-
tion. Le Blond et al. [32] show that over 98% of VoIP calls in
their dataset can be traced using call start and end times with
1-second granularity. Fan et al. [33] investigated if differential
privacy techniques on sequential information protect sensitive
data while retaining speciﬁc utility.
Re-identifying claimed, or seemingly anonymous data rep-
resents its own entire research. Naranayan and Shmatikov
were able to successfully de-anonymize parts of an industry
dataset, in their groundbreaking paper from 2006 [10]. This
has spawned large interest, and several approaches to improve
anonymization and re-identiﬁcation have been published [34],
[35], [36], [11]. This terminally led to the acceptance that
only differential privacy can provide guarantees. We were
inspired by this work. But given the industry practice of IP
address truncation, and claims that such coarsening sufﬁciently
anonymized their datasets, we were interested in the limits of
this approach.
Similar to De Montjoye et al. [15], we approach sequence
privacy through the upper bound of unicity instead. In their
initial work they examine the unicity of location sequences
to explore re-identiﬁability through movement patterns. They
repeated a similar approach using credit card shopping data
in [37]. When considering the unicity of sequential informa-
tion, the generating process for that information has to be
considered. So even though we look at browsing sessions in
a similar fashion to De Montjoye’s approach, the underlying
model which generated the data is different and conclusions
from one do not apply to the other.
VII. CONCLUSION
In this paper we have shown that sequential browsing data
is highly identiﬁable and attempts to lower the identiﬁability
through coarsening are largely ineffective.
We analyzed a dataset of browsing sessions representative
for both local analytics on single websites as well as large
cross domain trackers. We wanted to understand (1) how
common it
is for pseudonymous data to emerge in such
databases, as privacy regulations require informed consent
if pseudonyms are processed. And (2) how vulnerable such
databases are to re-identiﬁcation with partial knowledge in
practical applications. Throughout this endeavor we wanted
to know to which extent coarsening or generalization, the
industry standard for anonymizing such data, helps to protect
the privacy of the tracked audience.
Our results show that unicity, the prevalence of pseudonyms
in the data, is very high for almost all conﬁgurations. Pushing
unicity below a level of 10% requires removal of all informa-
tion pertaining to clients and website visits, and coarsening
timestamp information to at
least an order of hours. We
make no judgment which level of unicity is or is not legally
acceptable. However, it stands to reason that unicity is highly
indicative of how vulnerable such data is to re-identiﬁcation,
especially considering future capabilities, both regarding the
processing of data as well as the amount. In the absence of
more effective anonymization methods it appears very unlikely
that any meaningful degree of utility can be preserved in a
database of clicks without pseudonymous data.
In our practical evaluation of identiﬁability this idea is
largely conﬁrmed. Trackers that participate in the common
markets of user data exchanges have to assume that large parts
of the data they are passing on can immediately be re-identiﬁed
by the recipients. Shoulder surﬁng attacks, or the knowledge of
two to three visited Web pages, for instance from somebody’s
Twitter feed, are sufﬁcient to uniquely identify entire browsing
sessions retroactively. These results are consistent with and
strengthen established research. Website visits where users
considered themselves unobserved can easily be attributed
to them as long as part of that visit was observed, even if
the observer is restricted to the website’s domain and rough
location of the user.
Our
results strongly imply that audience measurement
providers who want
to anonymize click traces in compli-
ance with regulations such as the GDPR will need to use
methodology beyond coarsening. Adding noise or otherwise
perturbing the data, for example to achieve differential privacy,
provides provable privacy guarantees at the cost of signiﬁcant
losses in utility. These methods have been applied in speciﬁc
circumstances, but have not been widely adopted by audience
measurement providers.
In summary, we observe that sequential browsing data
contains highly identiﬁable information. Anonymizing such
data by generalizing its attributes has little effect; even if
session recording length is severely restricted and click traces
are trimmed to only two or three page calls. According to
our research, if negligible identiﬁability is desired, only single
page calls with a minimum of additional information about
the browser and the visited page can be stored.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 04:12:30 UTC from IEEE Xplore.  Restrictions apply. 
789
REFERENCES
[1] F. Roesner, T. Kohno, and D. Wetherall, “Detecting and defending
against third-party tracking on the web,” in USENIX conference on
Networked Systems Design and Implementation, 2012.
[2] Z. Yu, S. Macbeth, K. Modi, and J. Pujol, “Tracking the trackers,” in
Proceedings of the WebConf (WWW), 2016.
[3] S. Englehardt and A. Narayanan, “Online tracking: A 1-million-site
measurement and analysis,” in ACM CCS, 2016.
[4] S. Passmann, A. Lauber-Roensberg, and T. Strufe, “Privacy-preserving
audience measurement in practice—opportunities and challenges,” in
IEEE Communications and Network Security (CNS), 2017.
[5] J. R. Mayer and J. C. Mitchell, “Third-party web tracking: Policy and
technology,” in IEEE Security and Privacy, 2012.
[7]
[6] P. Samarati and L. Sweeney, “Protecting privacy when disclosing in-
formation: k-anonymity and its enforcement through generalization and
suppression,” technical report, SRI International, Tech. Rep., 1998.
´U. Erlingsson, V. Pihur, and A. Korolova, “Rappor: Randomized aggre-
gatable privacy-preserving ordinal response,” in ACM CCS, 2014.
[8] D. L. Quoc, M. Beck, P. Bhatotia, R. Chen, C. Fetzer, and T. Strufe,
“Privapprox: privacy-preserving stream analytics,” in USENIX Confer-
ence on Usenix Annual Technical Conference (ATC), 2017.
[9] A. Narayanan and V. Shmatikov, “How to break anonymity of the netﬂix
prize dataset,” arXiv preprint cs/0610105, 2006.
[10] ——, “Robust de-anonymization of large sparse datasets,” in Security
IEEE, 2008, pp.
and Privacy, 2008. SP 2008. IEEE Symposium on.
111–125.
[11] ——, “De-anonymizing social networks,” in Security and Privacy, 2009
30th IEEE Symposium on.
IEEE, 2009, pp. 173–187.
[12] P. Papadopoulos, N. Kourtellis, P. R. Rodriguez, and N. Laoutaris, “If
you are not paying for it, you are the product: How much do advertisers
pay to reach you?” in Internet Measurement Conference, ser. IMC, 2017.
[13] S. Coffey, “Internet audience measurement,” Journal of Interactive
Advertising, vol. 1, no. 2, pp. 10–17, 2001.
[14] E. Sy, C. Burkert, H. Federrath, and M. Fischer, “Tracking Users Across
the Web via TLS Session Resumption,” in ACSAC, 2018.
[15] Y.-A. De Montjoye, C. A. Hidalgo, M. Verleysen, and V. D. Blondel,
“Unique in the crowd: The privacy bounds of human mobility,” Scientiﬁc
reports, vol. 3, p. 1376, 2013.
[16] M. Arlitt, “Characterizing web user sessions,” ACM SIGMETRICS
Performance Evaluation Review, vol. 28, no. 2, pp. 50–63, 2000.
[17] T. Strufe, “Proﬁle popularity in a business-oriented online social net-
work,” in ACM Workshop on Social Network Systems, 2010.
[18] A. Serjantov and G. Danezis, “Towards an information theoretic met-
ric for anonymity,” in International Workshop on Privacy Enhancing
Technologies. Springer, 2002, pp. 41–53.
[19] L. A. Adamic and B. A. Huberman, “The web’s hidden order,” Com-
munications of the ACM, vol. 44, no. 9, pp. 55–60, 2001.
[20] G. Acar, C. Eubank, S. Englehardt, M. Juarez, A. Narayanan, and
C. Diaz, “The web never forgets: Persistent tracking mechanisms in
the wild,” in ACM CCS, 2014.
[27] R. E. Bucklin and C. Sismeiro, “A model of web site browsing behavior
estimated on clickstream data,” Journal of marketing research, vol. 40,
no. 3, pp. 249–267, 2003.
[21] P. Eckersley, “How unique is your web browser?” in International
Symposium on Privacy Enhancing Technologies Symposium (PETS).
Springer, 2010, pp. 1–18.
[22] A. G´omez-Boix, P. Laperdrix, and B. Baudry, “Hiding in the crowd: an
analysis of the effectiveness of browser ﬁngerprinting at large scale,” in
WWW 2018: The 2018 Web Conference, 2018.
[23] A. Vastel, P. Laperdrix, W. Rudametkin, and R. Rouvoy, “Fp-stalker:
Tracking browser ﬁngerprint evolutions,” in IEEE S&P 2018-39th IEEE
Symposium on Security and Privacy.
IEEE, 2018, pp. 1–14.
[24] R. Upathilake, Y. Li, and A. Matrawy, “A classiﬁcation of web browser
ﬁngerprinting techniques,” in New Technologies, Mobility and Security
(NTMS), 2015.
[25] K. Mowery and H. Shacham, “Pixel perfect: Fingerprinting canvas in
html5,” Proceedings of W2SP, pp. 1–12, 2012.
[26] M. Mulazzani, P. Reschl, M. Huber, M. Leithner, S. Schrittwieser,
and E. Weippl, “Fast and reliable browser identiﬁcation with javascript
engine ﬁngerprinting,” in Web 2.0 Workshop on Security and Privacy
(W2SP), vol. 5, 2013.
[28] L. Olejnik, C. Castelluccia, and A. Janc, “Why johnny can’t browse
in peace: On the uniqueness of web browsing history patterns,” in 5th
Workshop on Hot Topics in Privacy Enhancing Technologies (HotPETs
2012), 2012.
[29] ——, “On the uniqueness of web browsing history patterns,” Annals of
telecommunications-annales des t´el´ecommunications, vol. 69, no. 1-2,
pp. 63–74, 2014.
[30] J. Su, A. Shukla, S. Goel, and A. Narayanan, “De-anonymizing web
browsing data with social networks,” in The WebConf (WWW), 2017.
[31] S. Papadimitriou, F. Li, G. Kollios, and P. S. Yu, “Time series com-
pressibility and privacy,” in International conference on Very large data
bases (VLDB), 2007.
[32] S. Le Blond, D. Choffnes, W. Caldwell, P. Druschel, and N. Merritt,
“Herd: A scalable,
trafﬁc analysis resistant anonymity network for
voip systems,” in ACM SIGCOMM Computer Communication Review,
vol. 45, no. 4. ACM, 2015, pp. 639–652.
[33] L. Fan, L. Bonomi, L. Xiong, and V. Sunderam, “Monitoring web
browsing behavior with differential privacy,” in The WebConf (WWW),
2014.
[34] L. Backstrom, C. Dwork, and J. Kleinberg, “Wherefore art thou r3579x?:
anonymized social networks, hidden patterns, and structural steganog-
raphy,” in Proceedings of the 16th international conference on World
Wide Web. ACM, 2007, pp. 181–190.
[35] A. Gkoulalas-Divanis, G. Loukides, and J. Sun, “Publishing data from
electronic health records while preserving privacy: A survey of algo-
rithms,” Journal of biomedical informatics, vol. 50, pp. 4–19, 2014.
[36] M. Kosinski, D. Stillwell, and T. Graepel, “Private traits and attributes
are predictable from digital records of human behavior,” Proceedings of
the National Academy of Sciences, p. 201218772, 2013.
[37] Y.-A. De Montjoye, L. Radaelli, V. K. Singh et al., “Unique in the
shopping mall: On the reidentiﬁability of credit card metadata,” Science,
vol. 347, no. 6221, pp. 536–539, 2015.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 04:12:30 UTC from IEEE Xplore.  Restrictions apply. 
790