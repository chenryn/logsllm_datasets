### Reducing Identifiability in Click Traces

Reducing identifiability significantly, but not entirely, is a key concern. Knowing the hour of the observation, the website's domain, and the user's location, four observations are sufficient to correctly identify a quarter of click traces. When an adversary makes seven or more observations, knowing only the day is enough to identify 20% of browsing sessions.

To assess whether trimming click traces is as effective at reducing identifiability as it is at reducing unicity, we analyzed the identifiability of click traces trimmed to length \( l \), given prior knowledge of a number of observations, while limiting the maximum length of click traces in the database. Longer click traces exhibit higher unicity, but not necessarily higher identifiability. This is because we assume the same adversary strength in both cases, which translates to a relatively smaller observation size for longer click traces. Interestingly, these effects (increased unicity but smaller subtraces) largely cancel each other out. At the lower end of maximum length, we observe a statistically significant but very small increase in identifiability for higher length limits. In the mid and high ranges, the results become almost indistinguishable.

### Related Work

Our research investigates the emergence of pseudonymous data and the identifiability of click traces in databases of web trackers. The primary research areas related to our work pertain to the identifiability of online tracking and browsing data, and sequential identifiability in general.

#### A. Online Tracking

Third-party online tracking mechanisms, such as those used to generate our data, are widely employed [3]. Cookies remain the most common form [1], and techniques like "evercookies" and "cookie syncing" continue to make them a resilient and reliable tracking tool [20]. Browser fingerprints, a more recent technique, attempt to identify users via information extracted from their browsers. Eckersley and others [21, 22, 23] have demonstrated the potential of this approach by leveraging a combination of seemingly benign information to generate highly identifiable fingerprints. Upathilake et al. [24] identify additional categories of fingerprints based on Canvas [25], JavaScript Engine [26], and cross-browser information.

While our work is based on data obtained through online tracking, the data was gathered entirely through third-party cookies, and we do not generate fingerprints in the conventional sense. We purposefully disregard a large amount of information that would traditionally be used to generate fingerprints to demonstrate that identifiability can be achieved in other ways.

#### B. Browsing History

Broadly speaking, we analyze the metadata of browsing sessions, implicitly examining browsing behavior. However, research on browsing behavior typically focuses on how users navigate through websites [27], rather than privacy in web tracking.

Olejnik et al. [28, 29] examine the uniqueness of browsing history patterns, and their work strongly relates to ours. However, their analysis of unicity diverges significantly from our work. They do not investigate unicity as it emerges in cross-site tracking but explore differences in repetitive browsing behavior, observing users in all their actions. The effect of anonymizing or coarsening measures is also not considered in their work.

Su et al. [30] use browsing history to link a data subject to their social media profile by scraping profile activity and matching visited sites to the history. This represents an attempt to identify profiles from a local observer’s view, unlike our analysis of global tracking data. Yu et al. [2] classify certain pieces of information as particularly identifiable (or "unsafe"). Their scale of evaluation is similar to ours, with a comprehensive overview of the German online sphere. However, while we focus on the anonymizability of the datasets, they took a more local approach and implemented a client-based browser extension as a means of privacy protection.

#### C. Trace Unicity

The notion that sequences of data fragments leak private information has been explored before. Papadimitriou et al. [31] studied time-series compressibility and privacy using perturbation. Le Blond et al. [32] show that over 98% of VoIP calls in their dataset can be traced using call start and end times with 1-second granularity. Fan et al. [33] investigated if differential privacy techniques on sequential information protect sensitive data while retaining specific utility.

Re-identifying claimed or seemingly anonymous data is a well-established research area. Naranayan and Shmatikov successfully de-anonymized parts of an industry dataset in their groundbreaking paper from 2006 [10]. This has spawned significant interest, and several approaches to improve anonymization and re-identification have been published [34, 35, 36, 11]. This ultimately led to the acceptance that only differential privacy can provide guarantees. We were inspired by this work. Given the industry practice of IP address truncation and claims that such coarsening sufficiently anonymizes datasets, we were interested in the limits of this approach.

Similar to De Montjoye et al. [15], we approach sequence privacy through the upper bound of unicity. In their initial work, they examined the unicity of location sequences to explore re-identifiability through movement patterns. They repeated a similar approach using credit card shopping data in [37]. When considering the unicity of sequential information, the generating process for that information must be considered. Even though we look at browsing sessions in a similar fashion to De Montjoye’s approach, the underlying model that generated the data is different, and conclusions from one do not apply to the other.

### Conclusion

In this paper, we have shown that sequential browsing data is highly identifiable, and attempts to lower identifiability through coarsening are largely ineffective. We analyzed a dataset of browsing sessions representative of both local analytics on single websites and large cross-domain trackers. Our goals were to understand (1) how common it is for pseudonymous data to emerge in such databases, as privacy regulations require informed consent if pseudonyms are processed, and (2) how vulnerable such databases are to re-identification with partial knowledge in practical applications. Throughout this endeavor, we aimed to determine the extent to which coarsening or generalization, the industry standard for anonymizing such data, helps to protect the privacy of the tracked audience.

Our results show that unicity, the prevalence of pseudonyms in the data, is very high for almost all configurations. Pushing unicity below a level of 10% requires the removal of all information pertaining to clients and website visits, and coarsening timestamp information to at least an order of hours. We make no judgment on which level of unicity is or is not legally acceptable. However, it stands to reason that unicity is highly indicative of how vulnerable such data is to re-identification, especially considering future capabilities regarding data processing and the amount of data. In the absence of more effective anonymization methods, it appears very unlikely that any meaningful degree of utility can be preserved in a database of clicks without pseudonymous data.

In our practical evaluation of identifiability, this idea is largely confirmed. Trackers that participate in the common markets of user data exchanges must assume that large parts of the data they pass on can immediately be re-identified by the recipients. Shoulder surfing attacks, or the knowledge of two to three visited web pages, for instance, from someone’s Twitter feed, are sufficient to uniquely identify entire browsing sessions retroactively. These results are consistent with and strengthen established research. Website visits where users considered themselves unobserved can easily be attributed to them as long as part of that visit was observed, even if the observer is restricted to the website’s domain and rough location of the user.

Our results strongly imply that audience measurement providers who want to anonymize click traces in compliance with regulations such as the GDPR will need to use methodology beyond coarsening. Adding noise or otherwise perturbing the data, for example, to achieve differential privacy, provides provable privacy guarantees at the cost of significant losses in utility. These methods have been applied in specific circumstances but have not been widely adopted by audience measurement providers.

In summary, we observe that sequential browsing data contains highly identifiable information. Anonymizing such data by generalizing its attributes has little effect; even if session recording length is severely restricted and click traces are trimmed to only two or three page calls. According to our research, if negligible identifiability is desired, only single-page calls with a minimum of additional information about the browser and the visited page can be stored.