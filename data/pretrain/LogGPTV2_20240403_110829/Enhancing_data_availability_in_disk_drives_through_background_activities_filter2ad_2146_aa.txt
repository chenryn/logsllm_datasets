title:Enhancing data availability in disk drives through background activities
author:Ningfang Mi and
Alma Riska and
Evgenia Smirni and
Erik Riedel
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 2008
Enhancing Data Availability in Disk Drives through Background Activities*
NingfangMi
Computer Science Dept.
College of William and Mary
Williamsburg, VA 23187
PI:EMAIL
Alma Riska
Seagate Research
1251 Waterfront Place
Pittsburgh, PA 15222
Evgenia Smimi
Computer Science Dept.
College of William and Mary
Williamsburg, VA 23187
Erik Riedel
Seagate Research
1251 Waterfront Place
Pittsburgh, PA 15222
alma. PI:EMAIL
PI:EMAIL
PI:EMAIL
Abstract
Latent sector errors in disk drives affect only a few data
sectors. They occur silently and are detected only when the
affected area is accessed again. If a latent error is detected
while the storage system is operating under reduced redun(cid:173)
dancy, i.e., during a RAID rebuild, then data loss may occur.
Various features such as scrubbing and intra-disk data re(cid:173)
dundancy are proposed to detect and/or recover from latent
errors and avoid data loss. While such features enhance
data availability in the storage system, their execution may
cause performance degradation.
In this paper, we evalu(cid:173)
ate the effectiveness ofscrubbing and intra-disk data redun(cid:173)
dancy in improving data availability while the overall goal
is to maintain user performance within predefined bounds.
We show that by treating them as low priority background
activities and scheduling them efficiently during idle times,
these features remain performance-wise transparent to the
storage system user while still improving data reliability.
Detailed trace-driven simulations show that the Mean Time
To Data Loss (MITDL) improves by up to 5 orders ofmag(cid:173)
nitude if these features are implemented independently. By
scheduling concurrently both scrubbing and intra-disk par(cid:173)
ity updates during idle times in disk drives, MITDL im(cid:173)
proves by as much as 8 orders ofmagnitude.
1 Introduction
As digital storage of commercial data and of data for
strictly personal use becomes mainstream, high data avail(cid:173)
ability and reliability become imminently critical. Conse(cid:173)
quently, there are substantial efforts on designing reliable
disk-based storage systems by adding redundancy. Data re(cid:173)
dundancy is traditionally provided using parity locally in
the form of disk arrays (e.g., RAIDs) [16], but distributed
"'This work was completed in Fall 2006 during N. Mi's internship at
Seagate Research.
storage schemes at a broader scale (e.g., the Google File
System [7]) enjoy popularity.
Data redundancy protects against entire disk failures as
well as failures of disk sectors. Commonly, disk sector er(cid:173)
rors are known as "latent sector errors", because they occur
silently and are detected only when the affected area on the
disk is accessed again [1, 2, 6, 11, 20]. While latent sector
errors may be detected when the user tries to access the af(cid:173)
fected data, it is probable that they are detected during the
data rebuild process, because data rebuild accesses the en(cid:173)
tire disk space in order to restore the redundancy in a system
with disk failure(s).
If the storage array is designed to protect from one fail(cid:173)
ure only, such as RAIDs 1 through 5, any latent sector er(cid:173)
ror detected during the data rebuild process causes data loss
because there is no data redundancy to protect against the
error. Storage systems with two redundancy levels, such as
RAID 6 [13] are better protected and may experience data
loss because of latent sector errors only if they are detected
while two simultaneous failures exist in the array (i.e., a
very unlikely event).
To avoid data loss because of latent errors detected dur(cid:173)
ing data rebuilds, features exist in the storage system that
aim at detecting and recovering from latent sector errors
while there is redundancy in the system. Disk scrubbing
is an error detection technique that detects latent sector er(cid:173)
rors via background media scans [19]. Intra-disk data re(cid:173)
dundancy is an error recovery technique that adds another
level of redundancy in the data by adding parity for sets
(segments) of sectors within the same disk [3, 11].
Scrubbing could cause delays to the foreground work
because disk operations such as seeks are not preemptive.
Furthermore, multiple redundancy levels and intra-disk par(cid:173)
ity do impose additional work in the storage system when
data is modified (e.g., during WRITE operations), because
the parity must be updated. Consequently, both scrubbing
and intra-disk parity updates should operate as system back(cid:173)
ground processes. If the execution of this additional work
competes with regular user traffic, then it may cause unde-
1-4244-2398-9/08/$20.00 Â©2008 IEEE
492
DSN 2008: Mi et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:12:52 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 2008
sired delays to regular user traffic.
In this paper, we evaluate the effectiveness of scrubbing
and intra-disk data redundancy, when these techniques are
designed to remain transparent to the storage system user,
Le., keep average user performance degradation within pre(cid:173)
defined targets. For this, we treat both these features as
strictly background ones, and schedule them using effective
idle time management policies [14]. Throughout our evalu(cid:173)
ation, we show that our idle time scheduling approach [14],
is key to achieving the efficient execution of scrubbing and
in particular of the parity updates associated with the intra(cid:173)
disk data redundancy feature.
Detailed trace-driven simulations indicate that, it is pos(cid:173)
sible to effectively detect and recover from latent disk errors
even when the system imposes strict limitations on perfor(cid:173)
mance degradation of user traffic. The simulation results
show that background activities dramatically improve sys(cid:173)
tem reliability by achieving several orders ofmagnitude im(cid:173)
provement of its mean time to data loss (MTTDL). Specif(cid:173)
ically, scrubbing improves the MTTDL by as much as 5
orders of magnitude, while the intra-disk data redundancy
scheme evaluated improves the MTTDL by 2 orders of
magnitude. We further show that running both scrubbing
and intra-disk parity concurrently, utilizes best the entire
system. The combination of both features results in sig(cid:173)
nificant MTTDL improvements, i.e., as high as 8 orders of
magnitude.
This paper is organized as follows. Section 2 presents
related work. Section 3 describes background material on
modeling of mean time to data lost in storage system. Sec(cid:173)
tion 4 describes how to effectively schedule work during
idle times in disk drives by taking advantage ofthe stochas(cid:173)
tic characteristics of the empirical distribution of disk idle
times. Section 5 presents the disk level traces used in our
evaluation. Analysis of scrubbing that utilizes idle times
is presented in Section 6. Section 7 analyzes background(cid:173)
based intra-disk parity updates.
In Section 8 we evaluate
performance and data reliability consequences if scrubbing
and intra-disk parity updates are simultaneously enabled as
background jobs. Conclusions are given in Section 9.
2 Related Work
To avoid data loss, storage systems may be designed with
multiple redundancy levels, Le., RAID 6 [13]. In addition
to such solutions, system features such as scrubbing [19]
and intra-disk data redundancy [3, 11] represent effective
ways to detect and recover from latent sector errors. Re(cid:173)
cent data show that scrubbing detects as much as 60% of
all latent sector errors [1]. These features are preventive in
nature but unavoidably introduce more work in the storage
system and in the individual disks. To avoid penalizing reg(cid:173)
ular user traffic, any additional work to enhance reliability
is completed as a background process during disk or storage
system idle times, which is shown to be available in storage
systemes [8, 17].
While a myriad of approaches have been proposed to
best utilize idle times in order to enhance system perfor(cid:173)
mance, reliability, and consistency by exploiting it locally
(i.e., within the same system) [10] or remotely (Le., busy
systems may offload part of their work in idle ones) [12],
there has been a number of studies that focus solely on how
to better manage idle times for scheduling background ac(cid:173)
tivities [5, 14]. Methods to adaptively determine how to best
exploit disk idle times to schedule long, high-penalty back(cid:173)
ground jobs, such as powering or spinning-down disks, can
be found in [4, 9]. On the analytic side, several models have
been developed for systems where foreground/background
jobs coexist [21].
In this paper, we use the concept of managing idle times
proposed in [14], where decision on when to start schedul(cid:173)
ing a backgroundjob is determined by the empirical distri(cid:173)
bution of the previously monitored idle times. While the
study in [14] focuses on the general concept of how to uti(cid:173)
lize idle times such that the effect on foreground perfor(cid:173)
mance is contained within pre-defined bounds, here we fo(cid:173)
cus on customizing these general background job schedul(cid:173)
ing policies for the specific case of treating scrubbing and
intra-disk parity updates as background activities to en(cid:173)
hance system reliability. We further study how to best uti(cid:173)
lize idle times to meet the different needs of an infinite ac(cid:173)
tivity such as scrubbing versus a finite one that depends on
the specific workload such as intra-disk parity updates, and
show dramatic improvements in the mean time to data loss
in systems where both features are enabled.
The metric of interest when it comes to storage sys(cid:173)
tem reliability is not the traditional Mean-Time-To-Failure
(MTTF) [18], but instead the Mean-Time-To-Data-Loss
(MTTDL) [2]. Data loss is caused when additional fail(cid:173)
ures (even of a few data sectors) occur or are detected in
a system which has lost its redundancy because of previ(cid:173)
ous failures. While simultaneous failures of multiple disks
are rare, disk sector errors may be detected during a data
rebuild, i.e., when the system has-lost its redundancy, and
cause data loss [2].
3 Background - MTTDL Estimation
An important reliability metric for storage systems is the
Mean-Time-To-Data-Loss (MTTDL). Approximate models
for MTTDL as a function of various system parameters are
given in [2]. Here, we calculate MTTDL with scrubbing
and intra-disk data redundancy using the same models as
in [2]. We first provide a quick overview of the MTTDL
models presented in [2]. MTTDL is defined based on the
following parameters:
1-4244-2398-9/08/$20.00 Â©2008 IEEE
493
DSN 2008: Mi et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:12:52 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems &Networks: Anchorage, Alaska, June 24-27 2008
â¢ MV, M L: mean interarrival time of visible and latent
disk errors, respectively,
â¢ M RV, M RL: mean recovery time from visible and
latent errors, respectively,
â¢ M DL: mean detection time of latent sector errors,
â¢ a: temporal locality of errors,
â¢ {Jxy: spatial locality of errors, where consecutive er(cid:173)
rors X and Yare either visible (i.e., type V) or latent
(i.e., type L).
If no scrubbing is initiated, then MTTDL is given by the
following equation:
1
{Jvv MRV {JLV MRV
MTTDL ~ ~ MV2 +-;- MV.ML + ML' (1)
1
If scrubbing is performed, then the above equation accounts
for the average time it takes to detect the error via scrubbing
(i.e., MDL) and recover from it (i.e., MRL) as follows:
1
!3vv MRV
(JLV MRV
MTTDL ~ ak2 ML2 + ak ML2 +
!3VL + k{3LL MDL + MRL
(2)
ak
.
ML2
where k = M L/ MV [2]. The parameter values for Equa(cid:173)
tions (1) and (2) used in the following sections ofthis paper
are given in Table 1 and are those used in [2].
MV
ML
120,000 hrs
84,972 hrs
k
1.41
Q,{3VV,{3Lv,{3VL
I ~~L I
Table 1. Parameters used for MTTDL estimation.
4 Scheduling Background Activities
Using disk idle times as a separate resource to com(cid:173)
plete background activities with minimum obstruction to
foreground jobs has been the focus of scheduling policies
for foreground/background jobs. Idle waiting [5] (i.e., de(cid:173)
laying scheduling background jobs during idle intervals) is
an effective mechanism to reduce foreground performance
degradation due to non-preemptive backgroundjobs. An al(cid:173)
gorithmic approach to estimate how long to idle wait based
on the variability of observed idle periods in the system is
proposed in [14], where extensive experimentation shows
that the efficiency of idle waiting increases as variability of
the empirical distribution of idle times increases 1â¢
Idle waiting combined with an estimation of the num(cid:173)
ber of background jobs to be served within an idle inter(cid:173)
val, allows meeting foreground performance requirements
1For more details, we refer readers to the technical report [15].
while serving as many background jobs as possible. The
statistical characteristics of idle times can assist in defining
how long to idle wait before scheduling background jobs.
In [14], the empirical distribution function of idle times is
used to determine the length of "idle wait" in the following
three background scheduling policies:
body-based: Ifthe variability of idle times is low, then idle
wait for a short period. This policy schedules a few
background jobs in most idle intervals, which results
in using the body rather than the tail of the idle times
empirical distribution for serving background activi(cid:173)
ties.
tail-based: If the variability of idle times is high, then idle
wait for a long period. This policy schedules many
backgroundjobs in a few idle intervals by using the tail
rather than the body of the idle times empirical distri(cid:173)
bution for serving background activities.
tail+bursty: If burstiness exists in idle times, then it is
possible to obtain more accurate prediction about up(cid:173)
coming idle intervals because long idle intervals are
"batched" together. After a long idle interval is ob(cid:173)
served, then it is possible to predict with good accu(cid:173)
racy if the next interval is also long, which then allows
for more effective scheduling ofbackground activities.
The goal of the body-based policy is to use most of the
idle periods in the system and schedule only few back(cid:173)
ground jobs in an idle period. This policy works particu(cid:173)
larly well for cases with low variability in idle times be(cid:173)
cause all idle intervals are of similar length. In contrast, for
idle intervals of high variabilities, the idle waiting time in
the tail-based and the tail+bursty policies is much longer,
avoiding utilization of most idle periods which results in
delaying only few foreground jobs. Although the tail-based
policies utilize only few long intervals, the total amount of
background work scheduled during those long intervals is
yet more when compared to the background work sched(cid:173)
uled under the body-based policy. Tail-based policies are
effective only if the idle times are highly variable, which
implies that very long idle periods are expected to eventu(cid:173)
ally occur. In the following sections, we elaborate on how
the above policies can be used in the context of scrubbing
and intra-disk parity updates to increase MTTDL.
5 Trace Characteristics and Simulation
All policies presented here are evaluated via trace driven
simulations, see [17] for a detailed description of the sta(cid:173)
tistical characteristics of the disk drive traces. The selected
three disk traces are measured in a personal video recording
device (PVR), a software development server, and an e-mail
1-4244-2398-9/08/$20.00 Â©2008 IEEE
494
DSN 2008: Mi et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:12:52 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems &Networks: Anchorage, Alaska, June 24-27 2008
100
80
;./~.....
~ 60 If
40;' i
CJ
T l -
T2
.
T3 ------.
20 ,..:
OL.........-.L.-....&.-.L-..o-~....L.....&.......L.....L..~-L---o..--L-.L---L.......o--l
o 100 200 300 400 500 600 700 800 900 1000
idle time (ms)
Figure 1. CDF of idle times for traces TI, T2 and T3.
server, which we refer throughout the paper by Tl, T2, and
T3, respectively. Table 2 gives a summary of the overall
characteristics such as request mean interarrival time, re(cid:173)
quest mean service time, utilization, as well as the mean and
the coefficient ofvariation (CV) ofidle intervals in the trace.
Traces Tl, T2 and T3 have 427K, 500K and 362K entries,
respectively. They differ from each other in the stochas(cid:173)
tic characteristics of their idle intervals. For trace T1, idle
intervals have a CV close to one, while traces T2 and T3
have higher variability with CVs as high as 6.41 and 3.79,
respectively. Consequently, traces T2 and T3 have longer
tails than trace T1, see Figure 1. Furthermore, the time se(cid:173)
ries of the observed idle intervals for traces T1 and T2 are
not bursty, while the time series of idle intervals for trace