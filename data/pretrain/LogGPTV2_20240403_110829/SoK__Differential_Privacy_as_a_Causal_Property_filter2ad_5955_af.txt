note that DP applied to a network is typically taken to mean
either considering nodes or edges labeled with an individual’s
id i in the network as that individual’s data point Di, but that
participation in a social network is likely to leave far more
evidence than just those nodes and edges. They consider an
example in which Bob joins a network and introduces Alice
and Charlie to one another, leading them to create an edge
between them that does not involve Bob. Arguably, protecting
Bob’s privacy requires counting this edge as Bob’s as well
despite neither edge nor node DP doing so. To capture this
requirement, they distinguish between differential privacy’s
deleting of data points from a data set and their desire to
“hide the evidence of participation” [27, §2.2.1].
Because “It is difﬁcult to formulate a general and formal
deﬁnition for what evidence of participation means” [27, §3,
p. 5], they use correlations in its place for modeling public
health and census records [27, §§2.1.3, 2.2, 4.1 & 4.3.1].
However, for modeling social networks, they use statistical
models that they interpret as providing “a measure of the
causal
informal causal
models [27, §3, p. 6].
inﬂuence of Bob’s edge”,
that
is,
We believe that
the causal framework presented herein
provides the necessary mathematical tools to precisely reason
about evidence of participation. Causal models would allow
them to precisely state which aspects of the system they wish
to protect, for example, by requiring that Bob’s joining the
network should have a bounded effect upon a data release.
While accurately modeling a social process is a difﬁcult task,
at least the requirement is clearly stated, allowing us to return
to empirical work. Furthermore, such formalism can allow for
multiple models to be considered and we can demand privacy
under each of them, and erring on the side of safety by over-
estimating effect sizes remains an option.
Finally, causal modeling can make the choices between
privacy notions more clear. The distinction between direct and
indirect effects [41] can model the difference between node
privacy, which only captures the direct effects of joining a
social network, and all of the evidence of participation, which
includes hard-to-model indirect effects. Edge privacy captures
the direct effect of posting additional content. Given that
Facebook has reached near universal membership but worries
about disengagement, this effect might be the more concerning
one from paractical perspective.
VII. RESTRICTIONS ON KNOWLEDGE
Privacy is often thought of as preventing an adversary from
learning sensitive information. To make this intuition precise,
we can model an adversary’s beliefs using Bayesian probabil-
ities, or credences. We denote them with Cr, instead of Pr,
which we have been using to denote natural frequencies over
outcomes without regard to any agent’s beliefs. We denote the
adversary’s background knowledge as B. The knowledge of
an adversary about the database D after observing the output
can be expressed as Cr[D=d | O=o, B]. A natural privacy
termed statistical nondisclosure by Dalenius [9],
property,
requires that Cr[D=d | O=o, B] = Cr[D=d | B], that is,
that the beliefs about the database before and after observing
the output are the same.
This requirement limiting the difference between prior and
posterior beliefs has been shown to be impossible to achieve
under arbitrary background knowledge by Dwork and Naor,
even for approximate relaxations of statistical nondisclosure,
as long as the output provides some information [18]. As
DP also falls under the purview of this impossibility result,
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:36:11 UTC from IEEE Xplore.  Restrictions apply. 
365
it only provides this associative guarantee under restrictive
background knowledge assumptions, such as independent data
points or strong adversaries. To see the need for assumptions,
consider that statistical nondisclosure implies 0-Bayesian0
Differential Privacy (Def. 3) since both are equivalent to re-
quiring independence between D and O in the case where
the adversary’s background information is the true distribution
over data points. We believe such a need underlies the view
that DP only works with assumptions (Appendix A3).
where D=d is shorthand for (cid:86)n
Kasiviswanathan and Smith’s Semantic Privacy is a property
about the adversary’s ability to do inferences that does not
require such assumptions [25]. It requires that the probability
that the adversary assigns to the input data points does not
change much whether an individual i provides data or not.
The probability assigned by the adversary when each person
provides his data point is
Cr[D=d | O=o, B] =
(cid:80)
PrA[A(d)=o] ∗ Cr[D=d | B]
d(cid:48) PrA[A(d(cid:48))=o] ∗ Cr[D=d(cid:48) | B]
j=1 Dj=dj with d =
(cid:104)d1, ..., dn(cid:105). The probability where person i does not provide
data or provides fake data is
(cid:80)
PrA[A(d−id(cid:48)
d(cid:48) PrA[A(d−id(cid:48)
where d(cid:48)
is the value (possibly the null value) provided
instead of the real value and d−1d(cid:48)
i is shorthand for d with
its ith component replaced with d(cid:48)
i. While we leave fully
formalizing the combining of Bayesian credences and frequen-
tist probabilities to future work, intuitively, this probability is
CrMA,P [D=d | O=o, do(Di=d(cid:48)
i), B] in our causal notation.
Kasiviswanathan and Smith prove that DP and Semantic
Privacy are closely related [25, Thm. 2.2]. In essence, they
show that DP ensures that Cr[D=d | O=o, B] and Cr[D=d |
O=o, do(Di=d(cid:48)
i), B] are close in nearly the same sense as
it ensures that Pr[O=o] and Pr[O=o | do(Di=d(cid:48)
i)] are close.
That is, it guarantees that an adversary’s beliefs will not change
much relative to whether you decide to provide data or not,
providing an inference-based view of DP.
i)=o] ∗ Cr[D=d | B]
i)=o] ∗ Cr[D=d(cid:48) | B]
i
To gain intuition about these results, let us consider the
ﬁndings of Wang and Kosinski [47], which show the possibil-
ity of training a neural network to predict a person’s sexual
orientation from a photo of their face. If this model had been
produced with DP, then each study participant would know
that their participation had little to do with the model’s ﬁnal
form or success. However, inferential threats would remain.
An adversary can use the model and a photo of an individ-
ual to infer the individual’s sexual orientation, whether that
individual participated in the study or not. Less obviously, an
adversary might have some background knowledge allowing
it to repurpose the model to predict people’s risks of certain
health conditions. Such difﬁcult to predict associations may
already be used for marketing [24] (cf. [43]).
An individual facing the option of participating in such a
study may attempt to reason about how likely such repurposing
is. Doing so requires the difﬁcult task of characterizing the
adversary’s background knowledge since Dwork and Naor’s
proof shows that the possibility cannot be categorically elim-
inated [18]. Furthermore, if the individual decides that the
study is too risky, merely declining to participate will do
little to mitigate the risk since DP ensures that the individual’s
data would have had little effect on the model. Rather, the
truly concerned individual would have to lobby others to not
participate. For this reason, both the causal and associative
views of privacy have their uses, with the causal view being
relevant
to a single potential participant’s choice and the
associative, to the participants collectively. One can debate
whether such collective properties are privacy per se or some
other value since it goes beyond protecting personal data [36].
VIII. CONCLUSION AND FURTHER IMPLICATIONS
Although it is possible to view DP as an associative property
with an independence assumption, we have shown that it is
cleaner to view DP as a causal property without such an
assumption. We believe that this difference in goals helps to
explain why one line of research claims that DP requires an
assumption of independence while another line denies it: the
assumption is not required but does yield stronger conclusions.
We believe these results have implications beyond explain-
ing the differences between these two lines. Having shown
a precise sense in which DP is a causal property, we can use
the results of statistics, experimental design, and science about
causation while studying DP. For example, various papers have
attempted to reverse engineer or test whether a system has
DP [45], [10], [4]. Authors of follow up works may leverage
by pre-existing experimental methods and statistical analyses
for measuring effect sizes that apply with or without access
to causal models.
In the opposite direction, the natural sciences can use DP
as an effect-size metric, inheriting all the pleasing properties
known of DP. For example, DP composes cleanly with itself,
both in sequence and in parallel [39]. The same results would
also apply to the effect-size metric that DP suggests.
Finally, showing that DP is in essence a measure of effect
sizes explains why it, or properties based upon it, has shown up
in areas other than privacy, including fairness [15], ensuring
statistical validity [14], [12], [13], and adversarial machine
learning [30]. While it may be surprising that privacy is related
to such a diverse set of areas, it is not surprising that causation
is, given the central role the concept plays in science. What
is actually happening is that causal reasoning is making its
importance felt in each of these areas, including in privacy.
That it has implicitly shown up in at least four areas of research
suggests that causal reasoning should play a more explicit role
in computer science.
Acknowledgements: We thank Arthur Azevedo de
Amorim, Deepak Garg, Ashwin Machanavajjhala, and Frank
McSherry for comments on this work. We received funding
from the NSF (Grants 1514509, 1704845, and 1704985) and
DARPA (FA8750-16-2-0287). The opinions in this paper are
those of the authors and do not necessarily reﬂect the opinions
of any funding sponsor or the United States Government.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:36:11 UTC from IEEE Xplore.  Restrictions apply. 
366
[24] K. Hill, “How Target ﬁgured out a teen girl was pregnant before her
father did,” Forbes, 2012.
[25] S. P. Kasiviswanathan and A. Smith, “On the ‘semantics’ of differential
privacy: A Bayesian formulation,” J. Privacy and Conﬁdentiality, vol. 6,
no. 1, pp. 1–16, 2014.
[26] S. P. Kasiviswanathan and A. D. Smith, “A note on differential privacy:
Deﬁning resistance to arbitrary side information,” ArXiv, vol. 0803.3946,
2008.
[27] D. Kifer and A. Machanavajjhala, “No free lunch in data privacy,” in
2011 ACM SIGMOD Intl. Conf. on Management of data. ACM, 2011,
pp. 193–204.
[28] ——, “A rigorous and customizable framework for privacy,” in 31st
ACM SIGMOD-SIGACT-SIGAI Symp. on Principles of Database Sys-
tems, 2012, pp. 77–88.
[29] ——, “Pufferﬁsh: A framework for mathematical privacy deﬁnitions,”
ACM Trans. Database Syst., vol. 39, no. 1, pp. 3:1–3:36, 2014.
[30] M. Lecuyer, V. Atlidakis, R. Geambasu, D. Hsu, and S. Jana, “Certiﬁed
robustness to adversarial examples with differential privacy,” ArXiv, vol.
1802.03471v3, 2018, to appear at IEEE S&P 2019.
[31] J. Lee and C. Clifton, “Differential
identiﬁability,” in 18th ACM
SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining, 2012,
pp. 1041–1049.
[32] N. Li, W. Qardaji, D. Su, Y. Wu, and W. Yang, “Membership privacy:
A unifying framework for privacy deﬁnitions,” in 2013 ACM SIGSAC
Conf. on Computer and Communications Security, ser. CCS ’13, 2013,
pp. 889–900.
[33] C. Liu, S. Chakraborty, and P. Mittal, “Dependence makes you vul-
nerable: Differential privacy under dependent tuples,” in Network and
Distributed System Security Symposium (NDSS), 2016.
[34] A. McGregor, I. Mironov, T. Pitassi, O. Reingold, K. Talwar, and S. P.
Vadhan, “The limits of two-party differential privacy,” Electronic Collo-
quium on Computational Complexity (ECCC), vol. 18, 2011, corrected
version of paper presented at FOCS ’10.
[35] F. McSherry,
“Differential privacy and correlated data,” Blog:
https://github.com/frankmcsherry/blog/blob/master/posts/2016-08-
29.md, 2016.
[36] ——,
“Lunchtime
for
data
privacy,”
Blog:
https://github.com/frankmcsherry/blog/blob/master/posts/2016-08-
16.md, 2016.
[37] ——, “On “differential privacy as a mutual information constraint”,”
Blog: https://github.com/frankmcsherry/blog/blob/master/posts/2017-01-
26.md, 2017.
[38] F. McSherry and K. Talwar, “Mechanism design via differential privacy,”
in Foundations of Computer Science, 2007. FOCS’07. 48th Annual IEEE
Symp. on.
IEEE, 2007, pp. 94–103.
[39] F. D. McSherry, “Privacy integrated queries: An extensible platform for
privacy-preserving data analysis,” in 2009 ACM SIGMOD Intl. Conf. on
Management of data. ACM, 2009, pp. 19–30.
[40] P. Mittal, “Differential privacy is vulnerable to correlated data
– introducing dependent differential privacy,” Freedom to Tinker
blog:
https://freedom-to-tinker.com/2016/08/26/differential-privacy-
is-vulnerable-to-correlated-data-introducing-dependent-differential-
privacy/, 2016.
[41] J. Pearl, “Direct and indirect effects,” in 17th Conf. on Uncertainy in
Artiﬁcial Intelligence, 2001, pp. 411–420.
[42] ——, Causality, 2nd ed., 2009.
[43] G. Piatetsky, “Did Target really predict a teen’s pregnancy? The inside
story,” KDnuggets, 2014.
REFERENCES
[1] M. Alvim, M. Andrés, K. Chatzikokolakis, and C. Palamidessi, “On the
relation between differential privacy and quantitative information ﬂow,”
in 38th Intl. Colloquium on Automata, Languages and Programming –
ICALP 2011, ser. LICS, vol. 6756, 2011, pp. 60–76.
[2] G. Barthe and B. Kopf, “Information-theoretic bounds for differentially
private mechanisms,” in 2011 IEEE 24th Computer Security Foundations
Symp., ser. CSF ’11, 2011, pp. 191–204.
[3] R. Bassily, A. Groce, J. Katz, and A. Smith, “Coupled-worlds privacy:
Exploiting adversarial uncertainty in statistical data privacy,” in 2013
IEEE 54th Annual Symp. on Foundations of Computer Science, 2013,
pp. 439–448.
[4] B. Bichsel, T. Gehr, D. Drachsler-Cohen, P. Tsankov, and M. Vechev,
“DP-Finder: Finding differential privacy violations by sampling and
optimization,” in 2018 ACM SIGSAC Conf. on Computer and Commu-
nications Security (CCS ’18), 2018, pp. 508–524.
[5] R. Chen, B. C. Fung, P. S. Yu, and B. C. Desai, “Correlated network
data publication via differential privacy,” The VLDB Journal, vol. 23,
no. 4, pp. 653–676, 2014.
[6] M. R. Clarkson and F. B. Schneider, “Quantiﬁcation of integrity,”
Mathematical Structures in Computer Science, vol. 25, no. 2, pp. 207–
258, 2015.
[7] G. Cormode, “Personal privacy vs population privacy: Learning to
attack anonymization,” in 17th ACM SIGKDD Intl. Conf. on Knowledge
Discovery and Data Mining, 2011, pp. 1253–1261.
[8] P. Cuff and L. Yu, “Differential privacy as a mutual information con-