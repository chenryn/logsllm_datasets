Vulnerabilities 101:
How to Launch or Improve Your 
Vulnerability Research Game
Joshua Drake, Zimperium
Steve Christey Coley, MITRE
DEF CON 24
Aug 7, 2016
Introductions
• About Josh
– 20-years of VR, Ran iDefense VCP
• About Steve
– CVE co-founder, “Responsible Disclosure” (sorry), CVSS, 
CWE, ...
• Why we are doing this
– Currently, there is *way* more insecure code out there than 
researchers. This isn't guaranteed in 10 years, though.
– We need more people looking at code that’s deployed in the 
real world
• What we hope to accomplish by doing this
– Encourage more people to get involved
Disclaimers
• This is our opinion only
• Based on our own career experiences
• Others have their own opinions
• YOU… proving the cliche… are a unique 
snowflake
• You’ll find your own way, but hopefully we can 
help you find it faster
• No new ‘sploits here
What is a Vulnerability?
• Too many definitions
– Roughly: “a mistake in software’s design or implementation that allows 
an ‘attacker’ to conduct activities that (1) affect other users and (2) are 
not explicitly allowed or intended by the developer or sysadmin.”
• “What do you have?” vs. “What do you get?” - there must be a 
difference
• “How much help must the victim give you?”  (user interaction)
– Automatic
– “Normal” usage (e.g., clicking on a link is *normal*)
– Was victim stupid/clueless? (“just copy this javascript: url 
into browser”)
• “How much luck do you need?”
–ASLR, unusual configs, narrow race windows
What is a Vulnerability? (2)
•Vulnerabilities != Exploits (awesome 
@SwiftOnSecurity tweet)
•A Vulnerability resides in the software itself, 
doing nothing on its own
•An Exploit is a set of steps (possibly manual, or in 
the form of a program) that interacts with the 
software in a way that has a non-zero chance of 
successfully taking advantage of a vulnerability”
What is Vulnerability Research?
• “The process of analyzing a product, protocol, or algorithm 
in order to find unintended behaviors that allow an attacker 
to gain additional access to functionality or data that has not 
been explicitly approved by the product’s administrator.”
• This process may take minutes, days, months, even years
• Some people use “Vulnerability Discovery” to distinguish 
finding individual bugs in specific software versus more 
systematic/academic work
• Solving puzzles within puzzles, where you don’t know what 
the puzzle is when you begin
Motivations: Why Do Vulnerability 
Research?
• Knowledge / Wisdom
• Altruism
• Self-Protection
– Aka Secure your systems by hacking them
• Fame / Notoriety
• Money / Career
• Power
• Fun / Lulz
• Realistically, it’s a mix per individual
– Not every researcher will share your motivations
– Vendors might have experience or only assume certain motivations
Potential Work and Employers
• You can be a finder, extender, builder, fixer, defender, cataloger, 
coordinator, communicator, malware analyzer, risk evaluator, 
trend-discoverer, ...
• Note: not all work gets public recognition
• Just for fun (“hobbyist”)
• Yourself! - Bug bounties, black/gray market sales, etc.
• Consulting firms that value research
• Security product companies (for marketing value)
• Software vendors (product security or response team)
• Government contractors (or, government directly)
• Academia (focus is “pure” research)
• CERTs (analyze and understand real-world attacks)
Skills for Success
•
You can have some of these skills/traits and get by, but we’re trying to talk about 
what (usually) seems to lead to success
•
Code, protocols, file formats, or “how things work” under the hood
•
Common attack patterns
•
Logical flows (e.g. logic vulns, CSRF, authZ/authN, etc.)
•
How to run analysis tools and evaluate their findings
•
Clear communication; one (preferably many) of
– Steps to reproduce, and/or functional PoC with well-labeled functions, comments, etc.
– Using common vocabulary
– Describing the issues - first to vendor, then to public
– Describing why it’s important
– Understanding and respecting your audience(s)
– Well-structured advisory
– Note that poor English is NOT on this list; the above items are much more important
• Clear communication is probably one of the biggest contributors to career success, 
no matter your specialty (also, less drama due to misunderstandings)
Personality Traits for Success:
“Should” Have
• Persistence
• Patience - especially when dealing with people
• Diligence
• Curiosity
• Critical thinking
• Willingness to learn
• Self-motivated
• Willing & able to work independently, in solitary fashion
Personality Traits for Success:
“Nice” to Have
• Collaborative
• High concentration
• Addictive
• Willing to share findings or techniques
• Passionate
• Desire for constant improvement
• Sense of humor
Key Terms (Vocabulary)
•
Many of these terms have multiple definitions or usage
•
Attack Surface: the set of all inputs and code paths with which an attacker can interact
•
Impact
–
RCE (remote code execution)
–
EoP (escalation of privilege)
•
PoC (Proof of Concept)
–
Ambiguous term…
–
What concept are you proving?!
–
Be clear, it will ease your efforts.
•
Vulnerability classes
–
Memory corruption, injection (SQLi, XSS, etc.), protocol/specification design, …
–
When the low-hanging fruit fails: “Business logic”
•
Root cause analysis
–
Ex: XSS in error msg indicating system() or path trav
•
Chain analysis
–
It’s root cause turtles all the way down!
The Firehose: Where to Learn?
●
OWASP Top Ten
●
SANS/CWE Top 25
●
White papers
●
Periodic electronic 
collections
●
Videos
●
Mailing lists
●
Github repos
●
Vendor’s bug databases
●
Vuln scanners
●
Intentionally-vulnerable 
packages
●
CTFs / wargames
●
Follow individual 
researchers
●
Vulnerability databases
●
Conference talks
●
Classes
●
Books
●
Yearly White Hat Security 
Top 10 attacks
Selecting What to Analyze for 
Security Problems (1)
•
You can go deep or broad
– Language, vuln class, exploit technique, detection technique, ...
•
Anything you do that contributes to the body of knowledge is valuable
– Even negative results are useful! (though difficult to admit to)
•
Lots of “low-hanging fruit” out there
– Older code is more likely buggy
– Complex or overly complex systems are often ripe
– Large attack surface creates many opportunities
•
Software popularity matters
– Little-used software has lower quality but also lower impact to general public
– Popular software with extensive vulnerability history is often difficult
•
Too buggy means lower rewards or less recognition
– Sadly, they won’t get better without liberal application of effort
Selecting What to Analyze for 
Security Problems (2)
•
Brand-new or emerging technologies
– Vendor rush-to-market usually means security is at best an afterthought
•
Newly-discovered or emerging vulnerability/attack classes
– Each new class should force a review of ALL products across the board
– Or, refine a new attack/vuln with new variations, stronger impacts, etc.
•
Previously-unanalyzed code
– Highly likely to contain lots of low-hanging fruit
– Some targets grow popular without getting proper review / fixes (Android 
anyone? IoT?)
– Some targets have been around forever but only recently connected to networks 
(hello medical devices and automobiles!)
•
If you have access to expensive or difficult-to-obtain products: do eeeeeet
•
Follow what others are doing (“Pigpile” or “Bandwagon” Effect)
– Could offend the original researcher(s)
– Benefit from a base level of published research
Tools and Techniques
•
Dynamic vs. Static analysis - to run or not to run?
– Dynamic is analyzing a program by running it - e.g. fuzzing, debugging
– Static is purely inspection of program code - e.g. auditing, SCA tools
– Code coverage (how much), accuracy (false/true positives) are important!
– Real power is achieved by combining: hybrid analysis FTW!
•
Code auditing (binary/source)
– Grep!  Pedantic compiler settings!  Automated taint checking!
•
Design review
•
Threat modeling (e.g., STRIDE)
•
Automated tools
– Fuzzers, static code analysis
– Risk of false positives
– Lack of root cause analysis
Relevant Standards
•
Using standards can make it easier to communicate critical vulnerability 
information across broad groups of people, including consumers, vendors, 
and others
– (but haters do exist, and haters gonna hate)
•
CVE - Common Vulnerabilities and Exposures
– Numeric identifiers for tracking vulnerabilities
•
CWE - Common Weakness Enumeration
– Hierarchy of developer “mistakes” that lead to vulns
•
CAPEC - Common Attack Pattern Enumeration and Classification
– Common traits of attack methodologies
•
CVSS - numeric rating
– Pros: widely adopted, focused on key characteristics, provides consistency
– Cons: not as consistent as hoped, difficult to use in non-traditional contexts
Disclosure Models
•
Reasons for (public) disclosure
– To inform the parties responsible for fixing
– To put pressure on unresponsive vendors / get them to care
– To inform the masses that there’s a problem that needs attention
•
Models
– Full
– Partial
– Coordinated (formerly “Responsible”)
– Non-disclosure
•
Standards Documents
– ISO standard 29147 (@k8em0, etc.) - focuses on what VENDORS should do
• Now freely available!
– IETF Draft circa 2002
– RFPolicy 2.0
Considerations for Your 
Disclosure Policy
● Your own disclosure policy can help clarify 
expectations between you and vendors
● What if:
● You can’t even find the right contact point?
● 0-day exploitation is actively occurring?
● Somebody else publicizes your vuln(s) first
● The vendor doesn’t respond?
● What is the correct grace period?
● Design flaws often take a LONG time to fix
Considerations for Your 
Disclosure Policy (2)
● Impact to consumers who want to fix immediately
● Impact to consumers who can’t fix immediately
● Whether vendor appears to be acting in good faith
● Will your actions:
●
Make it harder for others to want to work with you in the future?
●
Make it more difficult for people to hire you?
● “Is it worth it to disclose at all?”
● Again, no one-size-fits-all. Moral compass, etc.
Advisory Structure and Contents
•