= 日志易搜索实例手册
北京优特捷信息技术有限公司
v4.1, 2020-09-11
== SPL概述
本手册主要讨论如何使用日志易的日志检索感知语言SPL。如果您对日志易产品还不太熟悉，推荐您首先阅读《日志易使用手册》及《日志易数据接入手册》等其他文档。
日志易SPL集合了所有的检索命令、函数、参数和从句。检索命令告诉日志易系统对索引中的数据应该做什么操作。比如，你可以使用检索命令来过滤不必要的信息、提取更精确的信息、评估新的字段、计算统计指标、排序结果，甚至创建一个图表。
多数检索命令都有配套的函数或参数。恰当的使用函数和参数，可以更好的操作命令的具体行为。常见的操作行为比如：进行哪种统计运算、评估哪个字段、使用哪些数据作为图表的XY轴等。
不少检索命令还可以使用从句形式，来指定如何对检索结果进行分组处理。
要了解具体的SPL语法细节，请阅读《日志易搜索参考》。
=== 检索目的及手段分类
在完成数据接入工作后，您应该会迫不急地想要开始使用SPL进行对数据金矿的探索发现工作。通常来说，这个探索的最终目标可以分为如下两类：
* 故障排查：您期望从索引数据中找到故障根源，或者至少提供给你明确的排查方向提示；
* 统计可视化：您期望以一种巡检报表的形式，获取对索引数据的多维度统计分析结果，最好是恰当的可视化仪表盘。
对于这两个不同的目的，SPL的用法上，也有着明显不同的分类：
* 原文检索：主要通过关键字、唯一标识符等低频词元，利用索引中倒排表的高性能查找特性，快速定位和读取原始日志内容。常见场景有：查找错误代码、关联跟踪特定订单或客户访问在多模块之间的流动情况、分析异常堆栈等等。
* 统计变换：对检索数据集进行一系列指标运算，得到二维表格式的变换结果。常见场景有：错误事件数的时间趋势、特定用户登录行为的排行和分布、访问响应时间的百分比统计、KPI指标的平滑预测等等。
需要注意的是：这两个分类是相互融合使用，而非对立割裂的。统计变化的结果可以以子查询的形式帮助过滤原文检索的范围；原文检索同样也可以帮助过滤缩小统计变换的运算数据集范围。
=== 指令设计与运行模式
SPL的语法，借鉴自UNIX平台shell的经典管道式设计。如果说shell是一种黏合各种命令行程序的胶水语言的话，SPL就是一种黏合各种检索和分析指令的胶水语言。使用者只需要关心自己的场景应该需要哪些分析步骤，挑选好对应的分析指令，从左到右依次串联，就可以完成。
如下是一段IT人员最常用的shell单行命令：
[source,bash]
cat /var/log/messages | grep Error | awk '{print substr($3,1,5)}' | sort | uniq –c | sort –nr | head 10
这行命令的作用是：
在系统日志中，过滤Error关键字内容，将相关日志的时间点，以分钟粒度提取出来，排序、去重、计数后，输出发生错误次数最多的10个时间点。
同样的思路和步骤，转换成SPL，则可以写成：
[source,bash]
source:"/var/log/message" Error | eval minute=formatdate(timestamp, "HH:mm") | stats count() by minute | limit 10
相比较来说，SPL的写法表意更加清晰。
当然，当您熟悉了SPL后，针对这个涉及时间范围的需求，还可以写出运行更加高效的另几种写法：
[source,bash]
source:"/var/log/message" Error | bucket timestamp span=1m as ts | stats count() by ts | sort by 'count()' | limit 10
或
[source,bash]
source:"/var/log/message" Error | timechart span=1m limit=10 count()
在shell常用的命令行程序中，grep、awk、sed、tailf等是流式处理输入输出的，sort、uniq则必须等待输入结束后才能处理并输出。
和shell类似，SPL指令，也分为流式处理和非流式处理两种运行模式。当您在日志易搜索界面上看到搜索任务处于‘运行中’状态，却迟迟没有任何结果输出时，一般都是因为系统正在运行非流式处理模式下的指令。
常见的流式指令有：eval、fields、rename、parse、where；
常见的非流式指令有：sort、limit、dedup、transpose。
具体每个指令的处理模式，请参阅《日志易搜索参考手册》。
== 全文检索
=== 全文检索语法简介
日志易系统支持Lucene系的全文检索语法。全文检索语句会被解析成一系列的单词和操作符。这里说的单词，可以是一个真正的英文单词，比如error；也可以是一段由双引号包裹起来的短语，比如"user login"。系统会按照相同的次序，在分词后的索引中，搜索短语内的每个英文单词。
除了最基础的单词检索以外，您还可以使用操作符组合更复杂的全文检索语法。支持的操作符说明如下：
* 指定检索字段
您可以指定在某个特定字段值里检索内容，用冒号:表示。比如只查看POST请求的日志，可以写作：
[source,bash]
apache.method:POST
[NOTE]
====
日志易对特定字段值默认不分词，所以无法达到前文介绍的短语搜索效果。
====
* 通配符
您可以使用通配符来描述单词内容，问号?表示一个字符，星号*表示零到多个字符。比如查看来自内网192.168.0网段的请求的日志，可以写作：
[source,bash]
apache.clientip:192.168.0.*
[NOTE]
====
通配符查询会消耗较大的资源，尤其是写法恶劣的情况。比如下面这个检索，请尽量避免类似以通配符开头的检索：
[source,bash]
*fail*
====
* 正则表达式
您可以像在Java编程语言里一样，在全文检索中使用正则表达式：
[source,bash]
name:/joh?n(ath[oa]n)/
和通配符一样，使用正则表达式的时候也要密切注意不良写法导致的严重资源消耗。
此外，由于正则表达式以斜线/为起始标记，这意味着您在检索URL信息时，需要对URL路径中的斜线/做好预转义工作：
[source,bash]
apache.request_path:\/index.html
或者采用双引号"来指明：
[source,bash]
apache.request_path:"/index.html"
其他需要前缀转义符的特殊符号有：+ - = && || > =500
同理和网页服务器相关的错误日志，则可以写作：
[source,bash]
apache.status:>=400 AND apache.status:10000
显然就没有下面这种方式高效：
[source,bash]
id:>10000 | lookup email,userName http://192.168.1.11/user.csv on id = userId
同样的，下面这个请求：
[source,bash]
* | stats count() by apache.clientip | where substring(apache.clientip,0,9)=="192.168.0"
首先不如下行这样过滤的结果准确，因为stats默认只保留20000行结果，像clientip这种高离散度的数据很可能在20000行以外：
[source,bash]
* | where substring(apache.clientip,0,9)=="192.168.0" | stats count() by apache.clientip
而上面的过滤语句，依然不如下面这样的查询语法更加高效：
[source,bash]
apache.clientip:192.168.0.* | stats count() by apache.clientip