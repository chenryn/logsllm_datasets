example HP’s VirusThrottle technology [15], or [16].
As a demonstrator, SafeCard incorporates a ﬁlter that is similar to VirusThrot-
tle, but works on incoming traﬃc. The ﬁlter limits per-service traﬃc spikes to
protect servers against ﬂash mobs. The algorithm is admittedly naive, and serves
mostly as a placeholder. Irrespective of the algorithm(s) used, statistical process-
ing is based on Streamline’s support for datastream correlation: multiple classi-
ﬁcation streams enter a single aggregation function that forward data only once,
when a threshold is reached. The runtime-constructed data path combined with
the cost-eﬀectiveness of ﬂow-based detection, encourages further experimentation
with these methods.
4 Implementation
We have implemented the discussed architecture on a programmable NIC, the
Radisys ENP2611 board built around the Intel IXP2400 network processor
(NPU). The IXP2400 is controlled by a 600 MHz general purpose processor, the
XScale. This processor is not nearly fast enough for line-rate data inspection.
The NPU therefore also embeds 8 specialised stream processors on the same
die that run without any OS whatsoever. These so called micro-engines each
support up to 8 hardware threads, with zero-cycle (i.e., free) context switching.
SafeCard: A Gigabit IPS on the Network Card
323
Fig. 3. Implementation of SafeCard on an Intel IXP2400 NPU
We have moved most processing to these resources. One micro-engine is used
by the receiver and header-based ﬁlter, one by the transmitter, and one by the
TCP reassembler. The remaining 5 are available to Ruler. The other FEs have
not been ported yet and must run on the on-board control processor.
Figure 3 shows the functional architecture from Figure 1 again, but now over-
laid over the hardware resources. Data enters the NPU on two one-Gigabit ports
and leaves on the third. All processing is on the network card, there is no in-
tervention from the connected host beyond loading and starting the IPS. It is
therefore easy to see that this device can run independent of a host-processor as
well. Intel’s IXDP2850 board is such a stand-alone network processor, to which
SafeCard has also been ported. Alternatively, the current setup could easily be
changed to forward data over the peripheral bus to the local host. We have dis-
cussed our experiences with designing high-performance programmable NICs for
this mode of operation elsewhere [36].
Dataplane operations must be cheap if we are to scale to high datarates.
In theory, a non-superscalar 600 MHz processor with single-cycle instruction
costs would be able to scan traﬃc at close to 5 Gigabit per second. However,
instructions are factors more costly, and more importantly memory-access costs
are generally two orders of magnitude slower. We rely on a few heuristics to be
able to scale to multi-Gigabit rates regardless of these obstacles.
First, we use zero-copy transport where eﬃcient. In SafeCard the only copy
incurred is from the Gigabit ports to the memory and back once thanks to the
zero-copy TCP reassembly FE.
Second, we minimise synchronisation, including locking. Most synchronisation
is through per-stream circular buﬀers. Polling on these buﬀers is essentially free
in dedicated processors like the micro-engines. For processing on the XScale we
have a dual polling/interrupt based mechanism. Micro-engines raise an interrupt
for each newly processed piece of data, but the CPU masks these interrupts
while it processes its backlog, in NAPI style. The dual scheme ensures both
timely operation under low load as well as graceful degradation under strain (as
opposed to thrashing due to livelock).
An embedded device like the IXP network processor adds its own complexity
to the general architecture. The board contains 5 diﬀerent layers of memory,
324
W. de Bruijn et al.
with each layer trading oﬀ increased capacity at the expense of throughput.
Communication can take place through interrupts, shared registers or shared
memory. The top entry IXP2850 even comes with 2 hardware cryptography units
and content-addressable memory. The need to take into account such hardware
details is inherent to embedded design, where implementational choices (e.g., to
use the cryptographic units) greatly inﬂuence overall performance.
As memory access is the bottleneck in high-volume traﬃc processing, the
memory hierarchy features should be optimally exploited. In SafeCard we opti-
mise placement of structures based on access frequency and structure size. The
packet buﬀer is placed in the largest (64MB), but slowest memory, DRAM. Be-
cause their smaller size permits this, pointer buﬀers are placed in faster 8MB
SRAM.
TCP stream metadata sits in the even faster, but far more scarce 16 KByte
scratch memory. Communication occurs through two datastructures: a hardware-
accelerated FIFO queue that holds per-segment work orders and a hashtable that
keeps per-stream metadata. For each segment the TCP reassembly unit places
a work order in the queue, where it is fetched by Ruler. Ruler then looks up the
correct stream in the hashtable and restore its DFA to the checkpointed state.
The two fastest types of memory, 2.5KB per-micro-engine RAM and their 4KB
instruction stores, are reserved for function-speciﬁc uses.
Ruler makes use of two methods to reduce memory-accessing costs. First, non-
preemptive multi-threading enables threads to hand oﬀ control while waiting for
I/O. Second, asynchronous I/O allows individual threads to interleave processing
and I/O operations. As the computation versus I/O ratio changes, so does the
number of concurrent threads needed to hide memory latency. For computation-
bound applications such as Ruler, threading is not necessary at all.
Resource allocation also encompasses layering the pipeline across the dis-
tributed processors. As said, the IXP micro-engines each support up to 8 hard-
ware threads. Having more threads (e.g., one per TCP ﬂow) introduces
software scheduling overhead. The opposite, a centralised event-handling mecha-
nism, adds parallelisation overhead and then reverts to a master-work threading
model. The optimal solution is therefore to create a thread-pool of functions
of the same size as the hardware resources 4. A threadpool of interchangeable
worker threads can only be applied when workers can attach to and detach
from a stream at will, i.e. checkpoint their state, as Ruler can. The size of the
pool can be scaled by incorporating more or fewer micro-engines. For example
the IXP28xx has 7 more micro-engines that the Ruler pool can use without
changes.
High-volume traﬃc must be processed on the micro-engines. However, because
these processors are scarce and hard to program, some processing will usually
take place on the slower XScale. We have implemented Prospector and ﬂow-
based detection on the XScale embedded in Streamline. If data is not matched
on the XScale it is sent back to the fast path on the micro-engines by writing
an entry in the transmission unit’s pointer buﬀer.
4 This mechanism is also known as I/O Completion Ports.
SafeCard: A Gigabit IPS on the Network Card
325
The volume of traﬃc that is handled by the XScale must be considerably
smaller than the Gigabit traﬃc handled in the fast path. As Prospector cur-
rently checks HTTP request headers only, and ﬂow-based methods do not touch
payload either, volume is indeed small.
5 Evaluation
As SafeCard is a compound system each function can prove to be the bottleneck.
Some operations are obviously more expensive than others, such as pattern-
matching, but this heuristic is of limited value when functions are implemented
on diﬀerent hardware resources. Indeed, as we discussed before, the Ruler en-
gine can be scaled across multiple fast micro-engines, while ﬂow-detection must
compete with Prospector for cycles on the XScale.
For this reason, to evaluate SafeCard and all its constituent parts, we con-
duct experiments that both measure the performance of individual FEs (micro-
benchmarks) as well as the overall throughput (a macro-benchmark).
5.1 Micro-benchmarks
counts of individual FEs
We can get an indication of the per-stage pro- Table 1. Single threaded cycle
cessing overhead by running the micro-engines
in single-thread mode and measuring the cycle
count in isolation. Table 5.1 shows the cost in
cycles per protocol data unit (PDU, e.g., IP
packet, TCP segment) with minimal payload
and the additional cost per byte of payload for
each hardware accelerated FE. Figure 4 shows
on the left the maximal sustained rate of the
FEs as obtained from these numbers. At 600MHz, we can see that all FEs can
process common-case traﬃc at a Gigabit except Ruler. A single Ruler instance
can process only 170 Mbit. The 5 combined engines thus top at 850Mbit, which
we’ve plotted in the ﬁgure as 5x Ruler. Merging Reception and Transmission
would give us the additional engine we need for full Gigabit processing.
PDU Byte
Description
313
1.5
Reception
0
TCP reassembly 1178
26
628
Ruler
Transmission
740
2
TCP reassembly. A single threaded cycle count presents a lower-bound on the
per-segment overhead as it omits memory contention costs. Nevertheless, for
TCP its performance represents the worst-case scenario for overall throughput,
because a single thread spends much of its time waiting for memory. Since multi-
threading enables latency hiding throughput will improve dramatically.
Independent of maximal obtainable throughput is the question how indirect
stream reassembly measures up to regular copy-based reassembly. For this rea-
son we have compared them head-to-head. As we have no copy-based method
available on the micro-engines we ran this comparison in a host based Streamline
function. The two functions share the majority of code, only diﬀering in their ac-
tual data bookkeeping methods. Figure 4(right) shows that indirect reassembly
326
W. de Bruijn et al.
Maximal sustained throughput
TCP reassembly performance
d
n
o
c
e
s
r
e
p
s
t
i
b
a
g
G
i
 6
 5
 4
 3
 2
 1
 0
Rx
TCP
Ruler
5x Ruler
Tx
 0
 200  400  600  800  1000 1200 1400 1600
IP packetsize
d
n
o
c
e
s
r
e
p
s
t
e
k
c
a
p
 4.5e+06
 4e+06
 3.5e+06
 3e+06
 2.5e+06
 2e+06
 1.5e+06
 1e+06
 500000
copy
zero-copy
 0  200  400  600  800 1000 1200 1400 1600
IP packetsize
Fig. 4. Theoretical sustained throughput & TCP Reassembly performance
easily outperforms copy-based reassembly. Only for the smallest packets can the
computational overhead be seen.
Ruler. The third row in Table 5.1 shows the overhead in cycles of Ruler. As
expected, costs scale linearly with the amount of data; the cost per PDU is
negligible. The function is computation-bound: fetching 64 bytes from memory
costs some 120 cycles, but processing these costs an order of magnitude more.
For this reason multi-threading is turned oﬀ.
Prospector. We have to benchmark Prospector on the XScale, because it is not
yet ported to the micro-engines. Figure 5(left) compares throughput of Prospec-
tor to that of a payload-scanning function (we used Aho-Corasick). We show
two versions of Prospector: the basic algorithm that needs to touch all header
data, and an optimised version that skips past unimportant data (called Pro+).
The latter relies on HTTP requests being TCP segment-aligned. This is not in
any speciﬁcation, but we expect it is always the case in practise.
Each method processes 4 requests. These are from left to right in the ﬁgure:
a benign HTTP GET request that is easily classiﬁed, a malicious GET request
that must be scanned completely, and two POST requests of diﬀering lengths.
In the malicious GET case all bytes have to be touched. Since AC is faster here
than both versions of Prospector we can see that under equal memory-strain we
suﬀer additional computational overhead.
However, all three other examples show that if you do not have to touch
all bytes —the common case— protocol-deconstruction is more eﬃcient than
scanning. Looking at the right-most ﬁgure, the longest POST request, we can
see that the gap quickly grows as the payload grows. The benign GET learns
us additionally that skipping remaining headers when a classiﬁcation has been
made can result in a dramatic (here 2-fold) increase in worst-case performance.
Note that none of these example requests carry a message body. This would also
be skipped by Prospector, of course. Even without message bodies, performance
is continuously above 18,000 requests per second, making the function viable for
in-line protection of many common services.
SafeCard: A Gigabit IPS on the Network Card
327
d
d
d
d
d
n
n
n
n
n
o
o
o
o
o
c
c
c
c
c
e
e
e
e
e
s
s
s