connected layer.
∀x ∈ X, Pr (Fθ (x + ∆) = yt , Fθ (x)) ≥ 1 − µ.
(6)
Layer Type
# of Channels
Filter Size Stride Activation
When an attacker applies gradient-based optimization to ﬁnd adver-
sarial perturbations for an input x targeting yt , the above equation
(6) implies that the partial gradient from x towards x + ∆ becomes
the major gradient to achieve the target yt . Note that Fθ (x) is the
composition of non-linear feature representation ❕(x) and a linear
loss function (e.g. logistic regression): Fθ (x) = ❕(x) ◦ L where L
represents the linear function. Therefore, the gradient of Fθ (x) can
be calculated via ❕(x):
∂lnFθ (x)
∂ln[❕(x) ◦ L]
∂ln❕(x) ◦ L
=
∂x
∂x
= c
∂x
(7)
Here c is the constant within the linear function L. To avoid ambigu-
ity, we will focus on the derivative on ❕(x) in the rest of the proof.
Given (7), we can interpret (6) in terms of the major gradient:
Px ∈X [
∂[ln❕(x) − ln❕(x + ∆)]
∂x
≥ η] ≥ 1 − µ,
(8)
where η represents, for the given x, the gradient value required to
reach yt as the classiﬁcation result.
Next, since ∀x ∈ X, cos(❕(A(x)), ❕(x + ∆)) ≥ σ , and σ → 1,
without loss of generality we have ❕(A(x)) = ❕(x + ∆) + γ where
|γ | << |❕(x + ∆)|. Here we rewrite the adversarial input A(x) as
A(x) = x + ϵ. Using this condition, we can prove that the follow-
ing two conditions are true. First, because the value of γ does not
depend on x, we have
Conv
MaxPool
Conv
MaxPool
FC
FC
16
16
32
32
512
10
5×5
2×2
5×5
2×2
-
-
1
2
1
2
-
-
ReLU
-
ReLU
-
ReLU
Softmax
Table 9: Model Architecture of GTSRB.
Layer Type
# of Channels
Filter Size Stride Activation
Conv
Conv
MaxPool
Conv
Conv
MaxPool
Conv
Conv
MaxPool
FC
FC
32
32
32
64
64
64
128
128
128
512
43
3×3
3×3
2×2
3×3
3×3
2×2
3×3
3×3
2×2
-
-
1
1
2
1
1
2
1
1
2
-
-
ReLU
ReLU
-
ReLU
ReLU
-
ReLU
ReLU
-
ReLU
Softmax
∂(❕(x + ∆) + γ )
∂x
=
∂❕(x + ∆)
∂x
.
Furthermore, because |γ | << |❕(x + ∆|), we have
1
❕(x + ∆) + γ
≈
1
❕(x + ∆)
.
Leveraging eq. (8)-(10), we have
(9)
(10)
Since Xt r ap and Xat t ack are ρ-covert, by deﬁnition (see eq. (5))
we have that for any event C ⊂ Ω, the largest possible difference be-
tween the following probabilities Px ∈Xat t ac k
[C] and Px ∈Xt r ap [C] is
bounded by ρ.
Next let C represent the event: (
have, for x ∈ Xat t ack ,
∂[l n❕(x )−l n❕(x +ϵ )]
∂x
≥ η). We
∂[ln❕(x) − ln❕(x + ϵ)]
∂x
≥ η]
1
∂❕(x + ϵ)
❕(x + ϵ)
1
≥ η]
∂x
∂(❕(x + ∆) + γ )
❕(x + ∆) + γ
∂x
1
∂(❕(x + ∆))
1
∂❕(x)
❕(x)
∂x
1
∂❕(x)
❕(x)
∂x
1
∂❕(x)
−
−
−
Px ∈X [
=Px ∈X [
=Px ∈X [
≈Px ∈X [
=Px ∈X [
≥1 − µ.
∂x
❕(x)
❕(x + ∆)
∂[ln❕(x) − ln❕(x + ∆)]
∂x
≥ η]
∂x
Proof of Theorem 2
PROOF. This theorem assumes that, after injecting the trapdoor
(cid:3)
∆, we have
Px ∈Xt r ap [
∂[ln❕(x) − ln❕(x + ∆)]
∂x
≥ η] ≥ 1 − µ
(11)
Following the same proof procedure in Theorem 1, we have
Px ∈Xt r ap [
∂[ln❕(x) − ln❕(x + ϵ)]
∂x
≥ η] ≥ 1 − µ
(12)
Px ∈Xat t ac k
[
∂[ln❕(x) − ln❕(x + ϵ)]
∂x
≥ η]
≥Px ∈Xt r ap [
∂[ln❕(x) − ln❕(x + ϵ)]
∂x
≥ η] − ρ
≥1 − (µ + ρ).
≥ η]
(cid:3)
≥ η]
8.2 Experiment Conﬁguration
Evaluation Dataset. We discuss in details of training datasets
we used for the evaluation.
• Hand-written Digit Recognition (MNIST) – This task seeks to
recognize 10 handwritten digits (0-9) in black and white images [26].
The dataset consists of 60, 000 training images and 10, 000 test im-
ages. The DNN model is a standard 4-layer convolutional neural
network (see Table 8).
• Trafﬁc Sign Recognition (GTSRB) – Here the goal is to recognize
43 different trafﬁc signs, emulating an application for self-driving
cars. We use the German Trafﬁc Sign Benchmark dataset (GT-
SRB), which contains 35.3K colored training images and 12.6K
testing images [44]. The model consists of 6 convolution layers
and 2 dense layers (see Table 9). This task is 1) commonly used
e
t
a
R
e
v
i
t
i
s
o
P
e
u
r
T
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
CW (AUC 0.99)
ElasticNet (AUC 0.99)
PGD (AUC 1.0)
BPDA (AUC 1.0)
SPSA (AUC 1.0)
FGSM (AUC 1.0)
 0.2
 0.4
 0.6
 0.8
 1
False Positive Rate
e
t
a
R
e
v
i
t
i
s
o
P
e
u
r
T
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
CW (AUC 0.99)
ElasticNet (AUC 0.99)
PGD (AUC 1.0)
BPDA (AUC 1.0)
SPSA (AUC 1.0)
FGSM (AUC 0.99)
 0.2
 0.4
 0.6
 0.8
 1
False Positive Rate
e
t
a
R
e
v
i
t
i
s
o
P
e
u
r
T
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
CW (AUC 1.0)
ElasticNet (AUC 1.0)
PGD (AUC 1.0)
BPDA (AUC 1.0)
SPSA (AUC 1.0)
FGSM (AUC 1.0)
 0.2
 0.4
 0.6
 0.8
 1
False Positive Rate
Figure 7: ROC Curve of detection on
MNIST with single-label defense.
Figure 8: ROC Curve of detection on CI-
FAR10 with single-label defense.
Figure 9: ROC Curve of detection on
YouTube Face with single-label defense.
e
t
a
R
s
s
e
c
c
u
S
n
o
i
t
c
e