contains N/m hash keys to be summed in the DPF. The total
work to answer all m PIR queries, each computed over a set of
N/m keys, is then amortized to O(N ). However, the success
of this PBR scheme hinges on hash keys falling into unique
partitions. How many hash keys can we expect to retrieve in
practice? Abstractly, the fraction of retrievable elements can be
modeled by the classic “balls and bins” problem [63], where
(cid:96) balls are tossed into m bins uniformly at random. If all (cid:96)
balls fall into unique bins, then the number of full bins is
(cid:96). Fewer than (cid:96) full bins corresponds to a collision in a bin
(i.e., partition) and only one element in each partition can
be retrieved. Let Xi be the indicator random variable where
Xi = 1 if a bin is full. Then,
Pr[Xi = 1] = 1 −(cid:16)
(cid:17)(cid:96)
1 − 1
m
> 1 − e−(cid:96)/m,
(cid:96) · (1 − e−(cid:96)/m) of the (cid:96) hash keys are
which implies that m
simultaneously “retrievable” from the m ≥ (cid:96) partitions. In the
case that m = (cid:96), we can expect to retrieve approximately 63%
of the required buckets. With m > (cid:96), we can increase the
probability of retrieving all required buckets at the cost of also
increasing communication by a factor of m
(cid:96) .
Application: Private multi-probing. The client ﬁrst computes
many hash indexes using multi-probing, and keeps one for each
PBR partition region. For each partition region, it sends a DPF
key for the hash index (which maps to 0 for no collision),
and the results from all partitions and tables are arranged and
masked with oblivious masking. Our main observation is that
failing to retrieve speciﬁc probes (because they collided in the
same PBR partition) is not a total failure. It is equivalent to not
choosing that particular multi-probe, which could have already
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:00:59 UTC from IEEE Xplore.  Restrictions apply. 
7917
happened due to the probabilistic nature of LSH. Because each
multi-probe hash key is uniformly distributed from universal
hashing (see Remark 1), each hash key is equally likely to be
selected, making it possible to directly apply our PBR scheme.
Protocol 1: Private Approximate Nearest Neighbor Search
Public Parameters: LSH families (H1, . . . ,HL), number of hash
tables L, and LSH functions h1, . . . , hL as in Figure 4.
Server Input: database of vectors (v1, . . . , vN ).
Client Input: query vector q.
Setup (one-time server-side pre-processing)
1: for i ∈ {1, . . . , L}:
1.1: Construct hash table Ti by storing d in bucket with key
αi,d ← hi(vd) for all d ∈ N.
1.2: Truncate each bucket in Ti to have at most one value (as
described Section IV-E).
2: Agree on common randomness source (e.g., PRG seed) rand.
Step 1 (on the client)
1: for i ∈ {1, . . . , L}:
B
(cid:17) ← DPF.Gen
(cid:17)
k(1,1), . . . , k(L,m)(cid:17)
1.1: α1 . . . α(cid:96) ← multiprobe (hi, (cid:96), q).
1.2: Choose one hash key for each PBR partition, and ﬁll in 0
for empty partitions (See Section V-A). For j = 1 . . . m,
let α(i,j) be the key for the jth partition of the ith table.
k(i,j)
A , k(i,j)
1.3:
.
1λ, α(i,j)(cid:17)
(cid:16)
and kB ←(cid:16)
(cid:16)
2: kA ←(cid:16)
k(1,1)
B , . . . , k(L,m)
B
.
k(1,1)
A , . . . , k(L,m)
A
(cid:17)
// DPF query keys.
(cid:16)
3: Send kA and kB to servers A and B, respectively.
Step 2 (on each server)
1: Parse k =
2: for i ∈ {1, . . . , L} and j ∈ {1, . . . , m}:
.
2.2: [IDi·m+j] ←(cid:80)
2.1: Di,j ← set of bucket keys in hash table Ti, partition j.
Ti(α) · DPF.Eval
k(i,j), α
.
α∈Di,j
3: [C] ← ([ID1], . . . , [IDL·m]).
4: [ ˜C] ← OBLIVIOUSMASKING([C], rand).
Step 3 (on the client)
1: Receive [ ˜C]A and [ ˜C]B from servers A and B, respectively.
2:
3: Output ˜IDi (cid:54)= 0 for smallest i, or 0 if all ˜IDi are zero.
(cid:17) ← [ ˜C]A + [ ˜C]B. // Recover candidates
(cid:16) ˜ID1, . . . , ˜IDL·m
// Algorithm 1.
(cid:16)
(cid:17)
B. Putting things together
The full protocol is presented in Protocol 1 and uses the
DPF, the oblivious masking, and probablistic batch retrieval
building-blocks described in Section V-A. We brieﬂy describe
each step of the protocol.
Setup. The public parameters consist of the number of hash
tables (L) and a list of L randomly sampled hash functions, in
accordance with the data structure of Figure 4 and Proposition 1.
The servers construct L hash tables using the hash functions
from the public parameters. Only the IDs of the input vectors
are stored in the hash table; the vectors are discarded (see
Section IV-E).
Step 1. The client hashes its query vector q using the LSH
functions in the public parameters and multiprobing. The client
keeps one hash key at random that falls into each PBR partition,
so that it has exactly one key for each. Each resulting hash is
used as the input index to DPF.Gen to generate a DPF key.
The client distributes the generated keys to the servers.
Step 2. Each server uses the L · m DPF keys it receives from
the client to retrieve a secret-share of a bucket in each of
the (cid:96) partitions of each of the L hash tables using the PIR
technique described in Section V-A. The result is a vector C of
secret-shared buckets containing either a candidate ID (or zero
if the bucket was empty). Each server applies the oblivious
masking transformation (Algorithm 1) to C and obtains the
masked secret-shared vector ˜C as output. Each server sends its
share of ˜C to the client in response.
Step 3. From the received secret-shares ( ˜CA, ˜CB), the client
recovers the vector ˜C (in cleartext) by computing ˜C = ˜CA+ ˜CB.3
The transformation applied by the servers in Step 2 ensures
that the client learns at most one non-zero candidate from the
original C. Speciﬁcally, by Claim 1, only the ﬁrst non-zero
value of C will be non-random in ˜C. The ﬁrst non-zero value,
if present, is output to the client.
C. Querying for k-nearest neighbors
To extend the construction to a k-ANN problem, that returns
the top-k nearest neighbors, we use the following simple idea.
For each non-empty bucket in Figure 4, the servers precompute
the k-nearest neighbors to the point that hashed to each bucket
with a standard ANN data structure. Then, the servers place
a vector consisting of the k-nearest neighbor IDs in the table
bucket. This extension can be viewed as instantiating the
protocol with a vector of IDs rather than a single ID. From
this vantage point, it is easy to see that the oblivious masking
technique still hides all-but-one non-zero k-vector in the ﬁnal
result. Speciﬁcally, the oblivious masking is now deﬁned over
the vector ﬁeld Fk
p (with addition and scalar multiplication
deﬁned elementwise in the natural way). The pre-computation
incurs only a small overhead of k when computing the data
structure and is thus acceptable. An alternative strategy to
pre-computing the k-nearest neighbors when building the data
structure is to cap each bucket to at most k IDs. However,
such an approach would not guarantee k results are returned
per query and thus may be undesirable for certain use cases.
VI. EFFICIENCY ANALYSYS
Asymptotic efﬁciency analysis. We analyze the communica-
tion and computation costs individually (summarized in Table I).
To derive our asymptotic guarantees, we follow the analysis
of Andoni and Indyk [4] for Euclidean distance (which has
c2 ; see deﬁnition of ρ in Proposition 1) and where we
ρ  1 − 2−20
(cid:96) · (1 − e−(cid:96)/m)
m
TABLE II: Replication and partitioning costs of existing batching
schemes: Naïve (perform (cid:96) PIR queries), Subcube batch codes
(SBC) [51], Probabilistic batch codes (PBC) [8]. Replication increases
server-side processing by the same factor but inﬂuences communica-
tion by a sublinear factor. Communication increases linearly with the
number of partitions for all schemes.
Protocol 1 guarantees that A learns no information on the
client’s query, even when deviating from protocol.
Proof. The proof follows from a simulation-based indistin-
guishability argument. We say that the protocol is query private
if there exists a PPT simulator Sim such that
Sim(DB) ≈c view(DB, kb),
where view is the view of A from an execution of Protocol 1
with the client. Sim(DB) is trivially constructed by invoking
the DPF simulator for each DPF key present in the vector
kb provided to A [22]. The client does not send any other
information to the servers apart from the DPF keys which
are used to query the hash tables. Therefore, query privacy
depends only on the privacy property of DPFs and follows
immediately. We now brieﬂy argue why the use of PBR does
not alter the above simulation. When retrieving buckets with
the PBR scheme, the client retrieves (cid:96) keys from each table,
where the key space is partitioned into m uniformly random
subsets (see Section V-A). This results in m DPF keys sent
to each server per hash table. Because the client sends a DPF
key for each partition (even if no bucket is retrieved from that