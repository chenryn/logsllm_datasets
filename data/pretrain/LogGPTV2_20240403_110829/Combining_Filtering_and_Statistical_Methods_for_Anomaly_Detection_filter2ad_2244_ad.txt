Internet Measurement Conference 2005 
USENIX Association
8
positive. The advantage of using labeled traces is that they
capture real world events. The disadvantage is that such
traces contain a ﬁxed number of events whose parameters
cannot be varied. For example, one cannot ask ”suppose
the volume of the attack had been a little lower, would our
algorithm have caught it?”
A second approach to validation is to synthetically gen-
erate attacks. The advantage of this approach is that the
parameters of an attack (attack rate, duration, number of
ﬂows involved, etc.) can be carefully control. One can then
attempt to answer the above question. This enables sensi-
tivity testing of any detection algorithm. Clearly the disad-
vantage is that these attacks have not happened anywhere
and thus may be of less interest.
We believe that a good approach to validation of an
anomaly detection algorithm should contain both of the
above approaches, so as to obtain the beneﬁts of each
method. For our set of real world data with anomalies, we
obtained four weeks of trafﬁc matrix data from the Abilene
Internet2 backbone network. Abilene is a major academic
network, connecting over 200 US universities and peering
with research networks in Europe and Asia. This data was
labeled using the method in [12]. We developed our own
synthetic anomaly generator and implemented it in Matlab.
This is described in detail further below.
4.1 Abilene Data
The Abilene backbone has 11 Points of Presence (PoP)
and spans the continental US. The data from this net-
work was collected from every PoP at the granularity of
IP level ﬂows. The Abilene backbone is composed of Ju-
niper routers whose trafﬁc sampling feature was enabled.
Of all the packets entering a router, 1% are sampled at ran-
dom. Sampled packets are aggregated at the 5-tuple IP-ﬂow
level and aggregated into 5 minute bins. This thus dictates
the underlying time unit of all of our estimations and de-
tections. The raw IP ﬂow level data is converted into a
PoP-to-PoP level matrix using the procedure described in
[11]. Since the Abilene backbone has 11 PoPs, this yields
a trafﬁc matrix with 121 OD ﬂows. Note that each trafﬁc
matrix element corresponds to a single OD ﬂow, however,
for each OD ﬂow we have a four week long time series de-
picting the evolution (in 5 minute increments) of that ﬂow
over the measurement period.
4.2 Synthetic Anomaly Generation
Our approach to synthetically generation anomalies makes
use of the Abilene trafﬁc matrix. The idea is to select either
one, or a set of, OD ﬂows to be involved in the anomaly, and
then to add anomalies on top of the baseline trafﬁc level for
those OD ﬂows. Our reasons for adding anomalies on top
of the existing trafﬁc matrix are as follows. We want to
detect anomalies using the ensemble of all network links,
we need to populate the load of the entire network. Other
methods are available such as [17]. But they generate sin-
gle link packet traces while we need multi-link SNMP data.
Thus we rely on our measured dataset for generating mali-
cious data.
Using the abilene trafﬁc matrix allows us to recreate
realistic loads network-wide. This includes all the many
sources of variability exhibited on the set of network links.
To inject an anomaly into this network, we use a three
step procedure. These procedure is carried out for each OD
ﬂow involved in the anomaly.
1. Extract the long-term statistical trend from the se-
lected OD ﬂow. The goal is to capture the diurnal
pattern by smoothing the original signal.
2. Add Gaussian noise onto the smoothed signal.
3. Add one of the anomalies as described in Table 1 on
top of this resulting signal.
These three steps are depicted pictorially in Figure 2. It
was shown in [19] that OD pairs in an ISP exhibit strong
diurnal patterns. These 24-hour cycles represent normal
types of variability in aggregated trafﬁc. Another normal
source of variability in OD ﬂows simply comes from noise
[13], and thus the ﬁrst two steps are intended to represent
the level of trafﬁc in an OD ﬂow right before the anomaly
starts; this should look like regular non-anomalous trafﬁc.
Figure 2: Three steps for synthetic generation of an
anomaly.
We extract the diurnal trend using a discrete wavelet
transform; wavelet methods here useful since these trends
are typically non-stationary.
Evidence of the ability
9
USENIX Association
Internet Measurement Conference 2005  
339
0100200300400500246810x106time0100200300400500246810x106time0100200300400500246810x106timeDe noised OD pairOriginal OD pairSynthetic OD pairsynthetic + anomalyof spectral methods to capture the underlying trends in
highly aggregated trafﬁc has been observed in [19, 8, 2].
We compute the ﬁrst ﬁve approximation signals using a
Daubechies-5 mother wavelet with 5 levels. We keep the
approximation signal at the 5th level, thus ﬁltering out ev-
erything except this smoothed signal. This smoothed, or
de-noised, signal is shown in the top left plot of Figure 2
as the solid line. We add to this baseline signal a zero mean
Gaussian noise who variance is computed as follows. We
take the ﬁrst 5 detailed signals from our wavelet transform,
and compute the variance of the sum of the 5 detailed sig-
nals. A sample signal produced after step 2 is depicted in
the upper right plot of Figure 2. An important reason to
use a signal that has been smoothed and only supplemented
with Gaussian noise is to ensure that there is no anomaly in
this OD ﬂow other than the one we are about to add.
The last step is to add an anomaly onto this baseline
trafﬁc. This is depicted in the bottom plot of Figure 2
where we see the anomaly added on top of the ﬁltered OD
ﬂow. In our synthetic anomaly generator we characterize
each anomaly by four parameters, namely, volume, dura-
tion, number of OD ﬂows involved, and a shape function.
The shape function refers to the rate of increase when the
anomaly begins (also called ramp up), as well as the rate
of decrease as the anomaly tapers off. We include four dif-
ferent shape functions: ramp, exponential, square and step.
The ramp function is further characterized by a slope pa-
rameter, and the exponential shape by its rate parameter.
Our intent is to deﬁne a feasible range for each of these
parameters such that we are able to capture the general be-
havior of known anomaly types as well as to encompass a
broader range of behaviors.
As pointed out in [15], there are unfortunately no com-
prehensive studies yet that provide detailed statistical de-
scriptions of a broad set of volume anomalies. There are a
handful of studies [2, 11, 16, 9, 3] that provide useful pieces
of information towards this end. The characterization part
of these studies often touch brieﬂy on a wide variety of
metrics, from attack rate and duration to others such as the
distribution of source or victim IP addresses, type of proto-
col involved in the attack, and the effect on the end system
(e.g., number of sessions open), etc. Some of these studies
do provide a few statistics on the parameters we wish to cal-
ibrate. Whenever possible, we draw upon these works and
include their ﬁndings as particular examples. As it is hard
to generalize from these speciﬁc cases, we allow our pa-
rameters to vary through a broader range than those found
in these studies.
The types of anomalies we would like to be able to
mimic include: DDOS, ﬂash crowd, alpha, outages and
ingress/egress shift. Since we focus on detecting changes
in trafﬁc volume patterns, we do not include other anoma-
lies such as worms and scans. A DDOS attack represents a
ﬂooding attack against a single destination. These attacks
can have either a single source (DOS) or many distributed
sources (DDOS). The latter occurs when many machines
(called ’zombies’) are compromised and a single attacker
sends commands to all of the zombies enabling them to
jointly ﬂood a victim. A ﬂash crowd occurs when there
is a surge in demand for a service and is typically mani-
fested by a large number of clients trying to access, and
thus overwhelming, a popular Web site. Flash crowds can
be predictable (e.g. a scheduled baseball game, or a soft-
ware release) or unpredictable (e.g., news breaking event)
[9].
An alpha anomaly refers to the transfer of a ﬁle(s) with
an unusually large number of bytes. This typically involves
one OD ﬂow as there is a single source and a single desti-
nation. An outage refers to scenarios such as failures which
can cause the load on a link to drop to zero. Such drops can
either be short-lived or long-lived, and the short-lived out-
ages are not infrequent since failures of one sort or another
are fairly commonplace in the Internet today [1]. An egress
shift occurs when the destination of an OD ﬂow moves
from one node to another. This can happen in a trafﬁc ma-
trix if there is a change in a BGP peering policy, or even a
failure, as many OD ﬂows can have multiple possible exit
points from an ISP. Policy changes could also cause a shift
of ingress point for a particular destination. In [21] the au-
thors showed that trafﬁc movement due to ingress or egress
shifts, although not frequent, does indeed happen. None of
these anomalies, other than DDOS attacks, are malicious.
Yet all of them will generate potentially sudden and large
shifts in trafﬁc patterns, thus appearing anomalous.
In Table 1 we list our ﬁve parameters characterizing an
anomaly. For each parameter we list the options for values,
or value ranges, that the parameter can take on. We allow
the duration to be anything from minutes, to hours, to days
and for forever. We include the forever case as this includes
the ingress and egress shift anomalies that will last until
there is another policy change. Since [21] indicates these
events are not that frequent, we can view the shift in trafﬁc
pattern as ”permanent”. The duration of an anomaly can
vary throughout a large range, and it is unclear what the
future will bring. Although most DDOS attacks observed,
in the backscatter data of [16], lasted between 5 and 30
minutes, there were some outliers lasting less than 1 minute
and others that lasted several days. Similarly, the majority
of the DDOS events in the Abilene data of [11], lasted less
than 20 minutes; a few outliers exceeds 2 hours. Alpha
and ﬂash crowd events could be of any length, although
typically alpha events would be shorter than ﬂash crowd
events. In general, we do not include events whose order
or magnitude of duration are less than minutes because we
are adding these events on top of the Abilene data that is
available to us with a minimum time interval of 5 minutes.
We change the trafﬁc volume in two ways when anoma-
lies occur. Sometimes we use a multiplicative factor δ that
340
Internet Measurement Conference 2005 
USENIX Association
1
is multiplied by the baseline trafﬁc to generate the new traf-
ﬁc load. Using δ ∼ 0, we can easily capture outage scenar-
ios. When an egress shift occurs, we assume that a subset
of the preﬁxes travelling between the source and destina-
tion router are being shifted to a new exit point. This will
shift a portion of the router-to-router trafﬁc (as these poli-
cies are more likely to affect only a subset of the IP level
preﬁxes) from the old OD pair to the new one. Remov-
ing 10%, for example, of the original OD ﬂow’s data is
simply captured by using δ = 0.9. This amount of trafﬁc
is added into the new OD ﬂow using the constant additive
term ∆. Allowing 1 ≤ δ ≤ 2, we can capture a variety of
either alpha, ﬂash crowd or DOS events. Note that because
we are considering aggregate ﬂows at the router to router
level, doubling the trafﬁc from an ingress router is already
an enormous increase in trafﬁc load. Large increases can
occur when there are many end hosts behind the router that
are involved in the anomaly (e.g., zombies, ﬂash crowd).
We don’t consider δ > 2 because such attacks are so obvi-
ously irregular that they are trivial to detect. We also allow
a change in volume to be indicated by simply adding a con-
stant factor, ∆, into the existing volume. This can capture
the effect of a DDOS attack in which many zombies ﬂood
a victim at their maximum rate.
The number of sources and destinations indicates the
number of OD ﬂows involved in an anomaly. The notation
(1, 1) refers to a single source and a single destination. This
could happen either for a DOS attack or an alpha event. The
case of (N, 1) arises for DDOS and ﬂash crowds. In the
case of a link failure, all the OD ﬂows traversing the link
are affected. The case of (2, 2) can occur for an ingress
or egress shift. By this we mean that there are two OD
ﬂows involved (that share either a common source or des-
tination). One of these ﬂows will experience an increase
in volume, while the other experiences an equal amount of
decrease. We do not include the case of (k, k) because we
assume that one BGP policy will change at a time.
As mentioned earlier, our shape function can take on one
of four possible forms: a ramp, exponential, square or step
function. The shape function is multiplied by the extra vol-
ume amount before it is added onto the baseline trafﬁc.
This thus determines the ramp up and drop-off behavior
of most anomalies. Not only are these shapes intuitively
useful, but there is also some evidence for them in existing
datasets. In [3] the authors found that a ﬂash crowd can be
characterized by a rapid rise in trafﬁc that is then followed
by a gradual drop-off over time. It also has been shown for
ﬂash crowd events that although their ramp up can be very
quick, it is typically not instantaneous [9]. The initial in-
crease of a DDOS attack could be captured by a ramp; this
allows us the ﬂexibility of representing scenarios in which
the zombies reach their maximum ﬂood rates in succession
(medium slope) or via a very sharp rise [3] (steep slope).
Outage anomalies could exhibit a near instantaneous drop
in volume and thus we include the ’square’ function. Alpha
events could exhibit either a near instantaneous increase in
volume or a ramp up. The step function is included to rep-
resent the ingress or egress shift anomalies because in these
cases the change in trafﬁc pattern is permanent (at least un-
til the next policy change).
When we generate an anomaly we randomly select the
values for these four parameters. Some combinations of
them will look like the anomalies we have discussed. By
varying each of the four characteristics in our generator, we
can create a wide variety of anomalies.
5 Results
5.1 False Positive and False Negative Perfor-
mance
We start by looking at the performance of our methods in
the Abilene network. The abilene data contains 27 anoma-
lies. Within each method, for each value of the thresh-
old, we examine the entire trafﬁc matrix (thus traversing all
anomalies and non-anomalies). We can thus compute one
false positive percentage and one false negative percentage
for each threshold conﬁguration of a scheme. The perfor-
mance of our 4 methods on the Abilene data is depicted in
the ROC curve of Figure 3(a). We see clearly that the ba-
sic method performs best. For a false positive rate of 7%, it
misses no anomalies (100% true positives), while the next
best method catches about 85% of the true anomalies for
the same false positive rate. The wavelet method was un-
able to achieve 0% false negatives. Thus we observe an
incomplete curve that does not reach the FNR = 0 limit,
even with a huge threshold.
(a) Abilene
(b) Synthetic
Figure 3: ROC curves using Abilene and Synthetic data
We now examine the performance of our algorithms us-
ing our synthetic anomaly generator. We generated about
500 different anomalies by varying the parameters of our
generator. For these attacks, the duration was varied ran-
1
USENIX Association
Internet Measurement Conference 2005  