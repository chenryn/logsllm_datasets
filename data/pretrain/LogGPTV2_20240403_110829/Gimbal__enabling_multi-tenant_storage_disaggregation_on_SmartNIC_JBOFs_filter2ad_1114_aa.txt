title:Gimbal: enabling multi-tenant storage disaggregation on SmartNIC JBOFs
author:Jaehong Min and
Ming Liu and
Tapan Chugh and
Chenxingyu Zhao and
Andrew Wei and
In Hwan Doh and
Arvind Krishnamurthy
Gimbal: Enabling Multi-tenant Storage Disaggregation on
SmartNIC JBOFs
University of Washington and
University of Wisconsin-Madison and
Tapan Chugh
University of Washington
In Hwan Doh
Samsung Electronics
Jaehong Min
Samsung Electronics
Chenxingyu Zhao
University of Washington
Ming Liu
VMware Research
Andrew Wei
University of Washington
Arvind Krishnamurthy
University of Washington
Abstract
Emerging SmartNIC-based disaggregated NVMe storage has be-
come a promising storage infrastructure due to its competitive
IO performance and low cost. These SmartNIC JBOFs are shared
among multiple co-resident applications, and there is a need for
the platform to ensure fairness, QoS, and high utilization. Unfor-
tunately, given the limited computing capability of the SmartNICs
and the non-deterministic nature of NVMe drives, it is challenging
to provide such support on today‚Äôs SmartNIC JBOFs.
This paper presents Gimbal, a software storage switch that or-
chestrates IO traffic between Ethernet ports and NVMe drives for
co-located tenants. It enables efficient multi-tenancy on SmartNIC
JBOFs using the following techniques: a delay-based SSD conges-
tion control algorithm, dynamic estimation of SSD write costs, a
fair scheduler that operates at the granularity of a virtual slot, and
an end-to-end credit-based flow control channel. Our prototyped
system not only achieves up to x6.6 better utilization and 62.6% less
tail latency but also improves the fairness for complex workloads.
It also improves a commercial key-value store performance in a
multi-tenant environment with x1.7 better throughput and 35.0%
less tail latency on average.
CCS Concepts
‚Ä¢ Information systems ‚Üí Flash memory; Storage manage-
ment; ‚Ä¢ Hardware ‚Üí External storage.
Keywords
congestion control, disaggregated storage, SSD, fairness
ACM Reference Format:
Jaehong Min, Ming Liu, Tapan Chugh, Chenxingyu Zhao, Andrew Wei,
In Hwan Doh, and Arvind Krishnamurthy. 2021. Gimbal: Enabling Multi-
tenant Storage Disaggregation on SmartNIC JBOFs . In ACM SIGCOMM 2021
Conference (SIGCOMM ‚Äô21), August 23‚Äì28, 2021, Virtual Event, USA. ACM,
New York, NY, USA, 17 pages. https://doi.org/10.1145/3452296.3472940
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGCOMM ‚Äô21, August 23‚Äì28, 2021, Virtual Event, USA
¬© 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-8383-7/21/08...$15.00
https://doi.org/10.1145/3452296.3472940
1 Introduction
Storage disaggregation has gained significant interest recently since
it allows for independent scaling of compute/storage capacities and
achieves high resource utilization [1‚Äì4, 48]. As datacenter network
speeds have transitioned to 100/200 Gbps and the NVMe-oF spec-
ification [15] that enables flash-based SSDs to communicate over
a network is becoming widely adopted, a disaggregated NVMe
SSD can provide microsecond-scale access latencies and millions
of IOPS.
Recently, SmartNIC-based disaggregated storage solutions, such
as Mellanox BlueField and Broadcom Stingray [8, 11], have emerged
and become increasingly popular because of their low deployment
costs and competitive IO performance compared to traditional
server-based approaches. Such a storage node usually comprises a
commercial-of-the-shelf high-bandwidth SmartNIC, domain-specific
accelerators (like RAID), a PCIe-switch, and a collection of NVMe
SSDs, supported by a standalone power supply. Consider the Broad-
com Stingray solution as an example. Compared with a conven-
tional disaggregated server node, the Stingray PS1100R storage box
is much cheaper and consumes up to 52.5W active power while
delivering 1.4 million 4KB random read IOPS at 75.3ùúás unloaded
latency.
Disaggregated storage is shared among multiple tenants for run-
ning different kinds of storage applications with diverse IO access
patterns. An efficient multi-tenancy mechanism should maximize
NVMe SSD usage, ensure fairness among different storage streams,
and provide QoS guarantees without overloading the device. How-
ever, today‚Äôs SmartNIC JBOFs1 lack such essential support, which
is non-trivial to build. First, the unpredictable performance charac-
teristics of NVMe SSDs (which vary with IO size, read/write mix,
random/sequential access patterns, and SSD conditions) make it
extremely hard to estimate the runtime bandwidth capacity and
per-IO cost of the storage device. For example, as shown in Sec-
tion 2.3, a fragmented SSD can only achieve 16.9% write bandwidth
of a clean SSD; adding 5% writes to a read-only stream could cause
a 42.6% total IOPS drop. Second, an SSD has a complex internal
architecture, and its controller does not disclose the execution de-
tails of individual IO commands. This complicates the IO service
time estimation of a tenant as well as the fair scheduler design.
Finally, SmartNICs are wimpy computing devices. Besides driving
1JBOF = Just a Bunch of Flash
106
SIGCOMM ‚Äô21, August 23‚Äì28, 2021, Virtual Event, USA
Min and Liu, et al.
the full networking and storage bandwidth, the amount of available
computation per-IO is bounded, i.e., 1ùúás and 5ùúás for a 4KB and
128KB read, respectively.
To address these challenges, we design and implement Gimbal,
a software storage switch that orchestrates NVMe-oF commands
among multiple co-located tenants. Gimbal borrows ideas from
traditional networking and applies them to the domain of man-
aging storage resources. First, Gimbal views the SSD device as
a networked system and applies a delay-based congestion control
mechanism to estimate its runtime bandwidth headroom. Second,
it introduces the virtual slot concept and the write cost estimation
logic to enable online characterization of the per-IO cost. Finally,
Gimbal exposes an SSD virtual view via an end-to-end credit-based
flow control and request priority tagging so that applications are able
to design flexible IO prioritization, rate-limiting, and load-balancing
mechanisms.
We prototype Gimbal on Broadcom Stingray PS1100R SmartNIC
JBOFs and compare with previously proposed multi-tenant storage
solutions with isolation mechanisms (i.e., Reflex [49], Parda [38],
FlashFQ [70]). Our experiments with synthetic workloads show
that Gimbal not only achieves up to x6.6 better utilization and
62.6% less tail latency but also improves the fairness for various
complex workloads. We also ported a commercial key-value store
(i.e., RocksDB) over a blobstore file system implemented on our
disaggregated storage platform. Our implementation employs a
hierarchical blob allocator to fully utilize a pool of storage nodes.
It manages the storage load and steers read requests based on the
runtime loads of the storage devices using an IO rate limiter and
a load balancer. Our evaluations show that Gimbal can improve
application throughput by x1.7 and reduce tail latency by 35.0% on
average.
2 Background and Motivating Experiments
2.1 NVMe-over-Fabrics Protocol
NVMe-over-Fabrics (NVMe-oF) [15] is an emerging storage pro-
tocol to support disaggregation of modern memory devices (e.g.,
NAND, persistent RAM). It defines a common architecture that
supports the NVMe block storage protocol over a range of storage
network fabrics (e.g., RDMA, TCP, Fiber Channel). NVMe-oF ex-
tends the NVMe base specification [14] and the controller interface.
A storage client (i.e., NVMe-oF initiator) first attaches to a storage
server (also known as NVMe-oF target) and then issues NVMe com-
mands to the remote controller. The NVMe-oF target comprises two
main components: NVMe target core and fabric transport. After
setting up a connection with the initiator, it creates a one-to-one
mapping between IO submission queues and IO completion queues.
NVMe-over-RDMA relies on memory-mapped IO for all opera-
tions. Both the host and the device perform memory read/write of
the host memory to modify the related data structures (including
submission queue, completion queue, data buffer). NVMe-over-
RDMA uses different RDMA verbs for initiating and fulfilling IO
flows. Specifically, RDMA_SEND is used to issue a submission cap-
sule to the target and a completion capsule back to the host. All data
transfers are performed at the NVMe-oF target using RDMA_READ
and RDMA_WRITE verbs. Thus, the data transfer phase requires
107
Figure 1: Architectural block diagram of the Stingray PS1100R
SmartNIC-based disaggregated storage.
no host-side computing cycles. Further, unlike the NVMe specifi-
cation, NVMe-oF does not introduce an interrupt mechanism for
the storage controller. Instead, host interrupts are generated by the
host fabric interface (e.g., host bus adapter, RDMA NIC).
Concretely, the request flow of a read/write under NVMe-over-
RDMA is as follows: (a) a client host sends an NVMe command cap-
sule (including the NVMe submission queue entry and the scatter-
gather address list) to an NVMe-oF target using RDMA_SEND; (b)
the target process picks up commands from the submission queue.
Under a write, it fetches client data via the RDMA_READ; (c) the
target storage controller then performs a read or write IO execu-
tion on the SSDs; (d) in case of reads, the NVMe-oF target issues
a RDMA_WRITE to transmit data from a local buffer back to the
client host memory; (e) the NVMe-oF target process catches the
completion signal, builds a response capsule (which contains the
completion queue entry), and sends this completion capsule via
RDMA_SEND. Some NVMe-oF implementations allow for inlining
small data blocks (e.g., 4KB) into the capsule, reducing the number
of RDMA messages and improving the IO latency.
2.2 SmartNIC JBOF
SmartNICs [5, 8, 10‚Äì13] have emerged in the datacenter recently,
not only for accelerating packet manipulations and virtual switch-
ing functionalities [6, 36], but also offloading generic distributed
workloads [35, 54, 58, 59]. Typically, a SmartNIC comprises general-
purpose computing substrates (e.g., ARM or FPGA), an array of
domain-specific accelerators (e.g., crypto engine, reconfigurable
match-action table), onboard memory, and a traffic manager for
packet steering. Most SmartNICs are low-profile PCIe devices that
add incremental cost to the existing datacenter infrastructure and
have shown the potential to expand the warehouse computing
capacity cheaply.
Lately, hardware vendors have combined a SmartNIC with NVMe
drives as disaggregated storage to replace traditional server-based so-
lutions for cost efficiency. Figure 1 presents the Broadcom Stingray
solution [8]. It encloses a PS1100R SmartNIC, a PCIe carrier board, a
few NVMe SSDs, and a standalone power supply. The carrier board
holds both the SmartNIC and NVMe drives and an on-board PCIe
switch connecting the components. The SmartNIC has 8 √ó 3.0GHz
ARM A72 CPU, 8GB DDR4-2400 DRAM (along with 16MB cache),
FlexSPARX [7] acceleration engines, 100Gb NetXtreme Ethernet
NIC, and PCIe Gen3 root complex controllers. The PCIe switch
offers √ó16 PCIe 3.0 lanes (15.75GB/s theoretical peak bandwidth),
and can support either 2 √ó 8 or 4 √ó 4 PCIe bifurcation. A Stingray
disaggregated storage box with four Samsung DCT983 960GB SSDs,
is listed for $3228.0, with likely much lower bulk prices, and is thus
much cheaper than a Xeon-based one with similar IO configura-
tions. Unsurprisingly, it also consumes lower power than a Xeon
PCIe switchNVMe drive 0100Gbps QSFP28 PortL2/L3 + DDR4 DRAMPCIe Root ComplexNVMe drive 0NVMe drive 0NVMe driveAcceleratorsSmartNICARM A72 CPUARM A72 CPUARM A72 CPUGimbal: Enabling Multi-tenant Storage Disaggregation on SmartNIC JBOFs
SIGCOMM ‚Äô21, August 23‚Äì28, 2021, Virtual Event, USA
disaggregated server node. The power consumption of a Stingray
PS1100R storage node is 52.5W at most, nearly one-fourth of the
Xeon one (192.0W). Section 5.1 describes the hardware configura-
tions. We use a Watts Up Pro meter [17] to measure the wall power.
The software stack on a SmartNIC JBOF works similar to the Xeon-
based server JBOF, except that the NVMe-oF target runs on the
SmartNIC wimpy cores instead of the x86 cores. The IO request
processing also includes five steps as described above.
SmartNIC JBOFs achieve performance that is competitive to
server JBOFs. In terms of unloaded read/write latency, we configure
fio [9] with one outstanding IO and measure the average latency
as we increase the request size (Figure 2). When serving random
reads, the SmartNIC solution adds 1.0% latencies on average across
five cases where the request size is no larger than 64KB. The latency
differences rise to 20.3% and 23.3% if the IO block size is 128KB
and 256KB, respectively. For sequential writes, SmartNIC execution
adds only 2.7ùúás compared with the server, on average across all
cases. We further break down the latency at the NVMe-oF target
and compare the server and SmartNIC cases. We find that the
most time-consuming part for both reads and writes is the NVMe
command execution phase (including writing into the submission
queue, processing commands within the SSD, and catching signals
from the completion queue). This explains why the latencies on
SmartNIC and server JBOFs are similar. For a 4KB/128KB random
read, it contributes to 92.4%/86.1% and 88.8%/92.2% for server and
SmartNIC, respectively.
Considering bandwidth, SmartNIC JBOF is also able to saturate
the storage limit but using more cores. This experiment measures
the maximum 4KB random read and sequential write bandwidth
as we increase the number of cores. For each FIO configuration,
we increase the IO depth to maximize throughput. As depicted in
Figure 3, the server achieves 1513 KIOPS and 1316 KIOPS using
two cores, respectively. In the case of the SmartNIC, it is able to
serve similar read and write traffic with 3 ARM cores. One core is
enough to achieve the maximum bandwidth under a large request
size (i.e., 128KB).
2.3 Multi-tenant Disaggregated Storage
Different kinds of storage applications share disaggregated storage
nodes, and therefore, we need to provide support for multi-tenancy
and isolation between different workloads. Today‚Äôs NVMe SSDs
provide some isolation support. For example, an NVMe namespace
is a collection of logical block addresses that provides independent
addressing and is accessed via the host software. However, names-
paces do not isolate SSD data blocks physically, and requests to
access different namespaces can still interfere.
An ideal mechanism should achieve the following goals: (1) pro-
vide fairness across tenants; (2) maintain high device utilization; (3)
exhibit low computation overheads and predictable delays at the
storage node. Our characterizations show that existing SmartNIC
JBOFs present limited support for multi-tenancy, as multiple aspects
of IO interference cause unfair resource sharing of storage through-
put. In Figure 4, the victim flow issues random 4KB reads with 32
concurrent I/Os, and we inject a neighboring flow with various IO
sizes, intensity, and patterns. Overall, a flow with high intensity al-
ways obtains more bandwidth regardless of the IO size and pattern.
For example, the bandwidth of the neighboring flow with random
108
128KB reads is 377MB/s and 58.4% less than the victim‚Äôs when it has
only one concurrent IO. But, it dramatically rises to 1275MB/s as the
concurrent IO is increased to 8 and obtains 3.1x higher bandwidth
than the victim. In addition, the bandwidth of the victim decreases
significantly when the neighboring flow uses a write pattern. The
victim shows 59.1% less bandwidth when the neighbor has the same
IO size and intensity but performs writes. (Appendix D describes
more characterizations.) We summarize below three challenges to
realizing an efficient multi-tenancy mechanism for JBOFs.
Issue 1: IO throughput capacity varies unpredictably with
workload characteristics (e.g., random v.s. sequential, read
v.s. write, IO size) and SSD conditions. Modern NVMe SSDs fa-
vor sequential access due to request coalescing and write-ahead
logging [21, 29]. Under the disaggregated setting, the overall IO
access patterns become much more random due to request inter-
leaving (i.e., IO blender effect) and impact the IO throughput. SSDs
also experience read/write interference. To minimize NAND access