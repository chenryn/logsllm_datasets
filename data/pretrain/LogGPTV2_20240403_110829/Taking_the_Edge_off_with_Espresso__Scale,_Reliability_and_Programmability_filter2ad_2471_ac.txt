ments as part of future research.
The greedy algorithm starts with the highest priority traffic and
its most preferred candidate mapping. Traffic assignment spills over
to subsequent candidates when the remaining traffic exceeds the
remaining capacity for the candidate. The algorithm orders egress
options based on BGP policy and metrics, user metrics (e.g., RTT
and goodput), and cost of serving on the link. For example, if we
assign prefix p with expected demand 1 Gbps at location A, to be
served via egress peering port E, then GC subtracts 1 Gbps from the
available serving limit for peering port E, along with any capacity
on the links from A to E. If the complete demand for p cannot be
satisfied by serving via port E, then we split the demand between
multiple peering ports.
GC also adjusts link capacity to account for traffic not under
its control, allowing for incremental deployment. For high priority
traffic, GC reserves space for LC and BGP spilling over in case of a
link failure. This reserved space is still available for loss-tolerant,
lower-QoS traffic.
GC employs safety as its fundamental operating principle. This
approach has been critical to its success in managing Google’s
Internet facing traffic while maintaining global scope. The often
hidden cost for global optimization is a global failure domain. Every
component in GC performs extensive validation at each stage for
defense in depth. GC relies on inputs from a number of different
systems, like monitoring systems that collect interface counters
on peering devices. Given the sheer amount of data consumed by
GC and the number of different input sources, GC must verify
the basic sanity of all incoming data sources. For example, Limits
Computer fails validation if the limits for different links computed
change significantly. When validation fails, the Optimizer continues
producing maps based on changes to the other input sources, while
using the last valid input from Limits Computer for some time.
Before publishing the egress map, the optimizer compares it to
prior maps, validating traffic-shift rules. We assume maps violating
these rules result from some fault in GC, hence we would rather
fail-static in these cases. This means we continue to use the last
valid map to serve traffic. In our experience, being conservative
in validation that could result in serving traffic using stale maps
when there is a false positive than risk programming a bad map
has resulted in a more reliable operational behavior.
We replicate GC in several datacenters for higher availability. A
master election process using a distributed lock system [6], elects
one of the GC stacks as master. The Optimizer in the master GC
stack sends it output to the LCs in every Espresso Metro. There
is a canary process where the LCs canary the new map to a small
fraction of packet processors and report their status to the GC. A
canary failure would revert the programming to previous valid map
and alert an oncall engineer.
We archive both GC inputs and outputs so operators can easily
revert to the historical known-good state. Validation reduces the
scope and frequency of production outages, while reverting to a
known good state helps reduce the duration of outages.
4.3.2 Location Controller. Since we intentionally dampen GC
output to minimize churn in the global egress map, Espresso fea-
tures a local control plane to handle changes such as interface
failure and sudden decrease in serving capacity. For this, we run a
Location Controller (LC) on host servers in each peering location,
which acts as a fallback in case of GC failure.
The main inputs to LC are application-specific forwarding rules
from GC, and real-time peer routing feeds via eBGP speakers in the
metro. The main output is the application-specific forwarding rules
437
Taking the Edge off with Espresso
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
to the packet processors. Figure 5 shows an example, described
below. A single LC scales to the control of approximately 1000
servers, keeping with our general goal of limiting control overhead
to approximately 0.1% of our infrastructure costs. We also push
the ACL filtering rules via LC to packet processors. Finally as a
configuration cache, LC acts as the nexus point for configuration
to other control components in the metro, e.g., BGP configuration
for the eBGP speakers.
• Scaling host programming. To quickly program all packet
processors, especially in the event of a network failure like
peering port down, we run a number of LC instances in
the metro. These instances are designed to be eventually
consistent. LC programs all packet processors in an edge
metro identically. Hence, to support more packet processors,
we can simply add more LC instances.
• Canary. To limit the blast radius of an erroneous GC output,
LC implements canarying to a subset of packet processors
in a location. We first deliver a new egress map to a subset
of packet processors, proceeding to wider distribution after
verifying correct operation in the canary set. We also canary
other control operations, e.g., ACL programming.
• Local Routing Fallback. Typically 3% of the traffic in a
metro does not match any entry in the egress map, e.g., the
client prefix has not issued requests recently. To provide a
safety net for such traffic, LC calculates BGP best-path using
routes from all metro routers. LC programs these routes as
the default in packet processors. This approach also allows
for a metro to fail static in the event of a failure in GC pro-
gramming. We maintain a constant trickle of traffic on these
paths to provide confidence that this fallback mechanism
will work when needed.
• Fast recovery from failures. To compensate for the slower
responding GC, LC reacts quickly to shift user traffic away
from known failed links. LC uses internal priority queues
to prioritize bad news, e.g., route withdrawals, over GC pro-
gramming. Coupled with the default routes, we avoid black-
holing traffic. A quick response to failure via LC and a slower
response to new peers via GC also provides the correct re-
sponse to a peering flap.
We now discuss the programming example shown in Figure 5. GC
generates egress maps for two application service classes to egress
from peering ports A : 30 and B : 10, where A and B are two peering
devices, and the numbers refer to the particular port on the peering
device. GC can program traffic for a particular prefix to be split in a
weighted manner across multiple peering ports, e.g., for application
class 1 and prefix 2.0.0.0/24, traffic is split 30% out of A : 30 and
70% out of B : 10. LC sends the appropriate encapsulation rules
used by packet processors to send traffic to the target peering port.
In the example, to egress from A : 30, packets must be encapsulated
with an outer IP-GRE header with an IP address from the range
42.42.42.0/27, and an MPLS header with label 401. We use a range
of addresses for the encap IP header for sufficient hashing entropy
as switches in the aggregation layer cannot look beyond the outer
IP header for hashing.
LC also receives a real-time feed of the peering routes for each
of the peers via BGP speakers. It uses this information to verify
Figure 5: Example of LC programming packet processors with TE
assigments from GC, best-path routes and updating the program-
ming based on real-time feed of routing updates (§ 4.3.2). [(A:30, 50),
(B:10,50)] refers to a 50:50 split of traffic between peering device A
port 30 and peering device B port 10 respectively.
the correct output of GC programming. As shown in Figure 5,
when LC receives a route withdrawal for 1.0.0.0/24 for peer A :
30, it updates the corresponding forwarding rules in each of the
application service classes by removing the unavailable egress from
the map. In the event that no egress is available in the egress map, LC
deletes the programming and the packet processors would instead
perform a lookup in the default service class programmed from the
best-path routes to encapsulate traffic. We keep the LC’s response
simple so that it only reacts to potential failures rather than make
any active traffic steering decisions.
4.4 Peering Fabric
We now discuss how the Peering Fabric (PF) supports the Espresso
edge metro. The Peering Fabric (PF) consists of a commodity MPLS
switch, our custom BGP stack, Raven, and the Peering Fabric Con-
troller (PFC). The PFC is responsible for programming the switch
and managing the eBGP sessions.
4.4.1 Raven BGP Speaker. We needed a server-based BGP speaker
for Espresso. We have previously employed Quagga [21] but faced
limitations. Specifically, Quagga is single threaded, C-based and
438
1.0.0.0/24[(A:30, 50), (B:10, 50)]2.0.0.0/24[(A:30, 30),[(B:10, 70)]3.0.0.0/24[(A:30, 100)]Appln. Class 11.0.0.0/24[(A:30, 40), (B:10, 60)]2.0.0.0/24[(A:30, 100)]3.0.0.0/24[(A:30, 100)]Appln. Class 2Global TE Optimizer (GC)LCA:30B:10Packet Processor (Forwarding State)1.0.0.0/24[(A:30, 50), (B:10, 50)]2.0.0.0/24[(B:10, 100)]3.0.0.0/24[(A:30, 100)]1.0.0.0/24[(A:30, 40), (B:10, 60)]2.0.0.0/24[(A:30, 100)]3.0.0.0/24[(A:30, 100)]A:30GRE-IP: 42.42.42.0/27MPLS: 401B:10GRE-IP: 41.41.41.0/27MPLS: 410Appln. Class 1Appln. Class 2Encapsulation RulesReal-time routing feedPer-application TE rules1.0.0.0/242.0.0.0/243.0.0.0/241.0.0.0/24[(A:30, 100)]2.0.0.0/24[(B:10, 100)]3.0.0.0/24[(B:10, 100)]Best-pathRoutes1.0.0.0/24[(A:30, 100)]2.0.0.0/24[(B:10, 100)]3.0.0.0/24[(B:10, 100)]Default1.0.0.0/242.0.0.0/243.0.0.0/24LCA:301.0.0.0/24B:10Packet Processor (Forwarding State)1.0.0.0/24[(A:30, 50)]2.0.0.0/24[(B:10, 100)]3.0.0.0/24[(A:30, 100)]1.0.0.0/24[(A:30, 40)]3.0.0.0/24[(A:30, 100)]A:30GRE-IP: 42.42.42.0/27MPLS: 401B:10GRE-IP: 41.41.41.0/27MPLS: 410Appln. Class 1Appln. Class 2Encapsulation RulesRoute withdrawals from peers1.0.0.0/24[(A:30, 100)]2.0.0.0/24[(B:10, 100)]3.0.0.0/24[(B:10, 100)]Default2.0.0.0/24Programming Update to send to HostAppln. Class 11.0.0.0/24[(A:30, 50)]Appln. Class 21.0.0.0/24[(A:30, 40)]Appln. Class 22.0.0.0/24DELETESIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
K.K. Yap, M. Motiwala, et al.
has limited testing support. Hence, we developed Raven, a high-
performance BGP speaker that runs on the edge servers. Raven
allows us to use the large number of CPU cores and ample mem-
ory on commodity servers to provide performance. Since Raven is
deployed in-house, we can implement only the features we need,
which improves performance and results in a leaner, more stable
codebase. We also benefit from better unit testing available in our
internal codebase. §6.3 presents our comparison between the BGP
speaker implementations.
For Raven to peer with external peers, the BGP packets from
the peer are encapsulated at the PF and delivered to Raven process
running on host machines. Conversely, the packets from Raven are
encapsulated and forwarded in the same manner as regular user
traffic from the hosts, i.e., IP-GRE and MPLS encapsulation. This
ties the control plane (i.e., health of peering session), and data plane
(i.e., traffic targeted towards the peer) together. If the data plane
from the peer is broken for any reason, the peering session also
breaks—resulting in traffic being shifted away from the peering
link. We also exploit the packet processors on the host to perform
both encapsulation and decapsulation of the BGP packets.
We run multiple Raven instances for each PF, distributing the
peers evenly across each instance. Hence, we scale our peering
capabilities horizontally by simply deploying more Raven tasks,
independent of the hardware and scale of the PF. This partitioning
also inherently limits the failure-domain of a single Raven task. Un-
like in a traditional router where all peering units can fail together,
failure of a Raven task only affects sessions associated with that
task.
Since we deploy Raven intances as software services on hosts,
they are frequently restarted for upgrades, either of Raven itself or
of the host machine. Hence, restarting Raven needs to have minimal
impact on traffic. We worked with our peers to enable BGP Graceful
Restart [26] such that data traffic continues to flow during Raven
restart.
4.4.2 Commodity MPLS Switch. Espresso’s commodity MPLS
switches support priority-based line-rate packet forwardingbut can-
not hold a full Internet-sized routing or ACL table. In Espresso, we
program this switch with MPLS forwarding rules. The number of
required MPLS forwarding rules corresponds to the number of peer-
ings per router, orders of magnitude smaller than than Internet-size
FIB. To program the switch, we extended its OpenFlow agent to pro-
gram MPLS forwarding and IP-GRE encapsulation/decapsulation
rules into the device. Using such rules, we can direct egress packets
to the selected peer and encapsulate ingress BGP packets towards
the Raven BGP speakers.
4.4.3 Peering Fabric Controller. The PFC acts as the brain of
the PF, managing the BGP speakers and switches. PFC manages
forwarding on the PF by installing flow rules to decapsulate egress
packets and to forward them to the next-hop based on the MPLS
label. PFC also installs flow rules to encapsulate BGP packets re-
ceived from a peer to the server running the correct BGP speaker
for that peering session.
PFC maps peers to the available Raven BGP speakers. This map
is sticky to minimize BGP session restarts. To gauge individual
speaker availability, PFC performs health checks on the speakers
and reassigns peering sessions when they go down. Since we loosely
439
4.4.4
couple PFC and the Raven speakers, the peerings remain available
even when the PFC is unavailable.
The PFC employs a master-standby architecture to increases
Espresso PF reliability in the face of failures. An interesting con-
sequence of externalizing network functionality from the peering
device is that the software components are naturally distributed
across multiple racks in different power domains, allowing Espresso
to be robust against power failure without additional provisioning.
Internet ACL. Google’s Internet-facing ACL is large, be-
yond the capacity of commodity PF switches. To address this chal-
lenge, Espresso installs a subset of ACL entries on the PF and uses
encapsulation to redirect the remaining traffic to nearby hosts to
perform fine-grained filtering. We use traffic analysis to determine
what ACL entries are the highest volume in packets per second,
and install those on the the PF itself while installing the remain-
der on the hosts. What we have found is that installing just 5% of
the required ACL entries on the PF covers over 99% of the traffic
volume.
Even more important than leveraging commodity hardware,
Espresso’s programmable packet processors can perform more ad-
vanced filtering than what is available in any router hardware,
commodity or otherwise. This greater flexibility can, for example,
support Google’s DoS protection system. The Espresso ACL de-
sign also allows for rules to be shifted around and new rules to be
installed on demand providing increased operational flexibility.
4.5 Configuration and Management
Espresso configuration is based on an automated intent-based sys-
tem. Currently, we leverage existing configuration languages to
express intent in Espresso to minimize churn for operators. De-
signing a network configuration language for SDN systems is an
important area of future work for us.
Upon checking in a human-readable intent for Espresso, the
management system compiles it into lower-level configuration data
that is more easily consumed by the systems. This configuration
data is then delivered verbatim by many of the components in
the system. In this way, we verify overall system configuration
consistency, and also programmatically ensure the configuration is
internally consistent and valid. Changing a configuration schema
only requires modifying the consumer(s) of the configuration and
not in the configuration management system.
The Espresso configuration management differs from traditional
network configuration systems in a number of ways. First, as most
of the functionality is in software we can use a declarative config-
uration language, greatly simplifying higher level configuration
and workflow automation. Second, since the data plane (packet
processors) is composed of a number of smaller units, we can roll
out configuration changes gradually to monitor any impact from
bad configuration updates. Finally, the fact that the control plane is
composed of a number of discrete units, each performing a specific
function allows for strong and repeated validation of configuration
flow to provide defense in depth.
In a peering location, the LC provides a cache of the current
intent/configuration. Hence, as systems come online or fail over,
they always have a local source to load their current configuration,
avoiding a high availability SLO for the configuration system. If
Taking the Edge off with Espresso
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
LC’s connection to the configuration system is lost, the LC will fail-
static with the current configuration, alerting operators in parallel
to fix the connection. The LC also canaries configuration to a subset
of devices within a location and verifies correct behavior before
proceeding to wide-scale deployment of a new configuration.
4.5.1 Big Red Buttons. For safety and operational ease, we made
the explicit decision to provide a “big red button” (BRB) that op-
erators could quickly, reliably and safely use to turn off some or
all parts of the system. These BRBs provide defense in depth at all
levels of the system, e.g., (i) we can push a configuration to LC to
send all programming via the best-path routes computed locally,