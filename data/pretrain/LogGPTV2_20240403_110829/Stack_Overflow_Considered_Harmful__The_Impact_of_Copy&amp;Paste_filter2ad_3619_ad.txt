tives.
java.security.Key,
Code containing Android’s cryptographic API was sec-
ond most prevalent and present
in 1,286 (31.9%) code
snippets. 1,088 (84.6%) of these code snippets applied the
javax.crypto.Cipher API and hence, contained code for sym-
metric encryption/decryption. Interestingly, many snippets em-
ploy user-chosen raw keys for encryption (701 snippets with
SecretKeySpec) instead of generating secure random keys
by using the API (207 snippets with KeyGenerator). This
indicates that most of the keys are hard-coded into the snippet,
which states a high risk of key leakage if reused in an
application due to reverse engineering.
The TLS/SSL package javax.net.ssl was used in 28.9% of
the code snippets. The majority of these code snippets (545,
i.e. 46.7%), contained custom TrustManagers to implement
X.509 certiﬁcate validation. Optimistically, by implementing
a custom trust manager, developers might aim at higher
security by only trusting their own infrastructure. Practically,
we observe that custom trust managers basically ignore au-
thentication at all [2]. 17.1% of the code snippets contained
custom hostname veriﬁers. Apache’s SSL library was mainly
used for enabling deprecated hostname veriﬁers that turned off
effective hostname veriﬁcation.
Code snippets containing code for BouncyCastle, Spongy-
Castle and SUN were rarely found. This could be due to the
fact that those libraries are mostly called directly by only
changing the security provider. Interestingly, nearly no (0.3%)
snippets contained code for the easy-to-use jasypt and keyzcar
libraries. Possible reasons could be their low popularity or
good usability. Similarly, the GNU cryptographic API was
rarely used. This might be due to the difﬁculty to integrate
it in an Android application [9].
B. Evaluation of Code Classiﬁcation
Altogether, we classiﬁed 1,360 distinct security-related code
snippets to provide a training set for our the machine learn-
ing based classiﬁcation model. We then applied the trained
classiﬁer on the complete set of 3,834 distinct security-related
code snippets found in Android posts, including both questions
(64.53%) and answers (35.47%). The security classiﬁcation
128
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:23:20 UTC from IEEE Xplore.  Restrictions apply. 
Namespace
javax.crypto
Cipher
KeyGenerator
spec.SecretKeySpec
spec.PBEKeySpec
spec.DESedeKeySpec
spec.DESKeySpec
spec.IvParameterSpec
spec.RC2ParameterSpec
Mac
Sealed
javax.net.ssl
TrustManager
HostnameVeriﬁer
SSLSocket
org.keyczar
Snippets
1,286
1,088
207
701
69
6
21
338
1
85
8
1,166
545
200
533
2
Namespace
android.security
com.sun.security
gnu.crypto
java.security
javax.security
javax.xml.crypto
org.bouncycastle
org.spongycastle
org.jasypt
org.apache.http.conn.ssl
AllowAllHostnameVeriﬁer
StrictHostnameVeriﬁer
BrowserCompatHostnameVeriﬁer
TrustSelfSignedStrategy
SSLSocketFactory
Snippets
5
5
47
2,841
44
3
48
44
11
241
184
51
8
1
105
TABLE VII: Snippet counts per library.
results of the training set are presented ﬁrst and are described
as follows:
The qualitative description of the snippets is divided into
the security categories TLS/SSL, symmetric cryptography,
asymmetric cryptography, random number generation, mes-
sage digests, digital signatures, authentication, and storage.
For each category, we describe why we consider the respective
code snippets to be insecure, what has been done wrong and
why it (supposedly) has been done wrong. Whenever possible,
we give counts for security mistakes and examples for the
security mistakes we found.
Second, we demonstrate the feasibility of our SVM
approach by discussing the overall quality of our classiﬁcation
model regarding precision, recall, and accuracy. Finally, we
present the results for the large scale security classiﬁcation
of all security-related code snippets found on Stack Overﬂow.
1) Labeling of Training Set: As described above,
the
training set consists of code snippets that have been identiﬁed
by the oracle-based ﬁlter to include security-related properties
(cf. Section III). We classiﬁed a subset manually in order to
provide supervision for the SVM.
a) TLS/SSL: We found 431 (31.48%) of all snippets in
the training set to be TLS related, among these we rated
277 (20.23%) as insecure. In other words, almost one third
of security-related discussions seem to target communication
security and more than half of the related snippets would
introduce a potential risk in real-world applications. The ma-
jority of TLS snippets are insecure because of using a default
hostname veriﬁer or overriding the default TrustManager of
java.net.ssl when initializing custom TLS sockets. Every sin-
gle custom TrustManager implementation we found consists
of empty methods that disable certiﬁcate validation checks
completely, while none of the custom TrustManager are used
to implement custom certiﬁcate pinning, which is the reason-
able and secure use case for creating custom TrustManagers.
This correlates to our assumption stated in Section VII-A.
An empty TrustManager is implemented by 156 snippets,
while 6 snippets use the AllowAllHostNameVeriﬁer - and 2
implemented both. We found 42 snippets that override the
veriﬁcation method of HostnameVeriﬁer of java.net.ssl by
returning true unconditionally, which ultimately disables
hostname veriﬁcation completely (cf. Listing 1). This change
to the HostnameVeriﬁer implements the same behavior as
AllowAllHostNameVeriﬁer.
We found several snippets that modify the list of supported
ciphers. In all cases, insecure ciphers were added to the list.
We assume this is caused by reasons of either legacy or
compatibility.
b) Symmetric Cryptography: We found 189 (13.80%) of
all snippets in the training set to be related to symmetric
cryptography, among these we rated 159 (11.61%) of the snip-
pets as insecure. For example, we found snippets containing
encryption/decryption methods with less than 5 lines of code,
implementing the minimum of code needed to accomplish an
encryption operation. These snippets were insecure by using
the cipher transformation string ”AES” which uses ECB as
default mode of operation (cf. Section IV-B2). Developers
might be unaware of this default behavior or of ECB being
insecure.
Another example are snippets that create raw keys and raw
IVs using empty byte arrays (i. e. byte arrays which consist
of zeros only), derive raw IVs directly from static strings, or
by using the array indexes as actual ﬁeld values as shown
in Listing 2. Other snippets derive raw keys directly from
strings that were mostly simple and insecure passphrases,
e.g. ”ThisIsSecretEncryptionKey”, ”MyDifﬁcultPassw”. We
also found snippets that initialized the IV using the secret key.
classiﬁed as
c) Asymmetric Cryptography: We found 59 (4.3%) of
all code snippets in the training set to include asymmetric
cryptography API calls, among these 13 (0.94%) of the
snippets were
insecure. Considering the
importance of public key cryptography in key distribution
and establishing secure communication channels, 4.3% is
quite low and corresponds to our assumption in Section
VII-A. All insecure snippets used weak key lengths which
varied between 256 and 1024 bits for RSA keys. Obviously,
recommendations from public authorities (e.g.
the NIST)
regarding secure cryptographic parameters are not fully taken
into consideration.
d) (Secure) Random Number Generation: We found 30
(2.19%) of all code snippets in the training set to include
(secure) random number generation API calls, among these
29 (2.11%) of
the snippets were classiﬁed as insecure.
All insecure snippets explicitly seeded the random number
generator with static strings
(cf. Listing 3). Replacing
the random number generator’s seed this way and not
supplementing it results in low entropy [31].
e) Digital Signatures and Message Digests: Overall, 279
(20.37%) snippets contain digital signatures related API calls.
We classiﬁed none of them as insecure. This is a remarkable
and unexpected observation, especially compared to the high
number of insecure snippets in discussions regarding sym-
129
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:23:20 UTC from IEEE Xplore.  Restrictions apply. 
metric cryptography. To explain this we had a closer look
at the relevant code snippets: calls to the digital signatures
API are most often related to extracting existing signatures,
not to validate them or generate new ones. Such an interactive
query of existing signatures is not very error-prone regarding
security.
Further, we found 392 (28.63%) snippets to contain
message digest related API calls, among these 14 (1.02%)
were classiﬁed as insecure due to usage of weak hash
algorithms. Again, compared to the quantity of insecure
snippets of other categories this is quite a low percentage. In
generating a message digest the biggest pitfall is choosing
a weak hash function. We assume that state-of-the-art hash
functions are relatively established in the Stack Overﬂow
community.
f) Remaining: 19 (1.38%) of the snippets contained
authentication code, where one snippet was classiﬁed as
insecure. Eight (0.58%) contained secure storage code, where
three snippets were classiﬁed as insecure.
g) Not Security-Relevant: We classiﬁed 342 snippets as
not security-relevant as deﬁned in III-C.
2) Model Evaluation of the SVM Code Classiﬁer: Overall,
after removing some duplicates, the training data set consisted
of 1,360 samples, out of which 420 code snippets were
identiﬁed as insecure. As introduced in Section V-B, we use a
tf-idf vectorizer to convert code snippets into numeric vectors
for training.
To illustrate how our approach works, Figure 3 illustrates
the projection of our training samples in 2d space by a
common dimensionality reduction method, i.e., Principle Com-
ponent Analysis (PCA) [40]. We leverage a RBF (Radial Basis
Function) kernel function to tackle the non-linearity hidden
in the projected training samples. The RBF kernel is a well
known type of kernel to model non-linearity of data. It maps
the non-linear input data to a high dimensional linear feature
space, such that the data becomes linearly separable. We can
see that even in 2d space where some relevant information
might be lost,
the SVM classiﬁer produces a good class
boundary for both the secure (blue dots) and insecure (red
dots) code samples.
Next, we evaluate our SVM model quantitatively by cross
validating the training data set. First, we conduct a grid search
on SVM to estimate the optimal penalty term C (cf. (1)) with
respect to classiﬁcation accuracy. Since the training data con-
tains very high dimensional features, we use a linear kernel for
SVM instead of RBF kernel in previous 2d demonstration. The
optimal parameter C is determined to be 0.644. We evaluate
the model on various training sizes with respect to precision,
recall and accuracy. A discussion on these evaluation metrics
can be found in [41]. In our setup, we consider insecure
samples as positive and secure ones as negative. Therefore,
the precision score measures how many predicted insecure
snippets are indeed insecure, the recall score evaluates how
insecure
secure
Fig. 3: SVM with RBF kernel is trained on the training dataset,
where the high dimensional training samples are projected on
2-dimensional using PCA. Solid contour line represents the
classiﬁcation boundary and dashed lines indicate the maximal
margin learned by SVM. Insecure code snippets are marked
as red circles, and secure ones are marked as blue circles.
many real insecure snippets are retrieved from all insecure
snippets, and ﬁnally the accuracy score measures an overall
classiﬁcation performance taking both positive and negative
samples into account.
s
e
r
o
c
s
g
n
i
t
s
e
t
n
o
i
t
a
d
i
l
a
v
-
s
s
o
r
c
.
g
v
A
0.9
0.8
0.7
0.6
0.5
0.4
Recall scores
Precision scores
Accuracy scores
200
400
600
800
1000
Varing training sizes
Fig. 4: Binary SVM with linear kernel is trained over varying
training sizes. Cross validation is performed on each of these
subsets of the training data set and evaluated with respect to
precision, recall and accuracy scores.
In Figure 4, we report the learning curve of our model
with respect to varying training sizes. For each training size, a
subset of the training data set is classiﬁed by the model with a
50-repetition cross validation. In each repetition, we randomly
hold out 20% of the training samples as testing set, and train
on the remaining samples. Finally, we average the testing
scores on all repetitions and plot the mean scores with standard
deviation as the error bar. The results present a good precision
and accuracy on varying training sizes, as the mean scores
are approximately all above 0.8. The constantly developing
precision curve illustrates that our model performs very well
on detecting real insecure snippets instead of introducing too
many false positives, even on a small training size. On the
other hand, we see the recall curve is relatively poor on small
training size. However, it reaches nearly 0.75 when we have
more than 1, 000 training samples. Accuracy also improves
130
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:23:20 UTC from IEEE Xplore.  Restrictions apply. 
with increasing training samples. The variance of the accuracy
is canceled by combining both precision and recall.