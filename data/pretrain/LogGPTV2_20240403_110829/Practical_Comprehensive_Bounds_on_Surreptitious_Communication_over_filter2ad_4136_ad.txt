w
t
s
y
l
a
n
A
)
k
e
e
w
r
e
p
s
e
x
ﬃ
u
s
(
100
10
1
1
10
)
k
e
e
w
r
e
p
s
e
x
ﬃ
u
s
(
d
a
o
l
k
r
o
w
t
s
y
l
a
n
A
3
2.5
2
1.5
256
512
1k
2k
4k
8k
16k
Detection threshold (bytes)
name + time + query type
time
name
query type
1
3
10
30
100
Time resolution (ms)
Figure 6: The impact of the information content threshold I
and the time resolution R on the number of sufﬁxes to val-
idate manually per week for the INDLAB dataset. The top
chart reﬂects a value R = 10 msec, and the bottom chart I
= 4,096 bytes.
from 4,096 to 256 bytes (and potentially increasing secu-
rity) would increase the number of domain name sufﬁxes
passed to the analyst for manual inspection 50-fold. The
plot also shows a clear power-law relationship between
analyst workload and I, with the former scaling as ap-
proximately x−1.38 in the latter.
Setting the information content
threshold I to
4,096 bytes and the time resolution R to 10 ms thus
provides a good balance between analyst workload and
potential detections. Sites might of course revisit these
parameters based on their particular threat models and
networking environments.
6.5 Bounding Information Content
For each (sufﬁx, client)-pair that remains after the pre-
ceding ﬁlter steps, we compute the size of gzip, bzip2
and ppmd [23] compression for the series of all corre-
sponding lookup names, selecting the lowest value. We
also assess a codepoint-oriented analysis (§ 5.3), com-
puting the gzip, bzip2 and ppmd compression sizes for
the series of distinct (unique) lookup names, selecting
the lowest value, and adding the lowest value of the gzip,
bzip2 and ppmd compression sizes for the corresponding
distinct lookup name indices. Given these two assess-
ments, we choose the smaller as the best (tightest) upper
bound on the amount of information potentially trans-
ferred through lookup names to the given domain sufﬁx
(cf. box “Bound on Information Content” in Figure 5).
Next, we apply the same procedure to the correspond-
ing inter-query arrival times (in R = 10 msec units) and
query record types, if this information is available. Fi-
USENIX Association  
9
22nd USENIX Security Symposium  25
nally, we add up the results from the lookup name, time
and type information vectors, and if their sum lies below
I, we discard the (sufﬁx, client)-pair.
6.6
Inspected Domain List
We expect sites to employ our analysis procedure over
an extended period of time. For example, once a site sets
it up, it might run as a daily batch job to process the last
24 hours of lookups. An analyst inspects the trafﬁc as-
sociated with any domains ﬂagged by the procedure and
renders a decision regarding whether the activity appears
benign or malicious.
An important observation is that the same benign do-
mains will often reappear day after day, due to the basic
nature of their lookups. However, the analyst needn’t
reexamine such domains, as the verdict will prove the
same. (See § 9 for further discussion of this point.) Given
this, we presume the use of an Inspected Domain List
(IDL) that accumulates previous decisions regarding do-
mains over time. For a given day’s detections, we omit
ﬂagging for the analyst any that already appear on the
IDL. Once populated, such a dynamic list can greatly
reduce the ongoing burden that our detection procedure
places on a site’s analysts.
A ﬁnal issue regarding the IDL concerns its granu-
larity. For example, if our procedure ﬂags s1.v4.ipv6-
exp.l.google.com and we put that precise domain on the
IDL, then this will not spare the analyst from having
to subsequently investigate i2.v4.ipv6-exp.l.google.com.4
However we note that the analyst’s decision process will
focus heavily on registered domains.
In this example,
the analyst will likely quickly decide to mark the detec-
tion as benign because for it to represent an actual prob-
lem would require subversion of some of Google’s name
servers, which would represent an event likely signiﬁ-
cantly more serious than an attacker communicating sur-
reptitiously out of the site. In addition, the analyst will
reach this conclusion simply by inspecting the registered
domain google.com, rather than studying all of the sub-
domains in depth.
Accordingly, once an analyst inspects a detection, we
place on the IDL the corresponding registered domain,
which we compute by consulting Mozilla’s Effective
TLD Names list [20]. In this example, com appears on
the list (meaning that any domain directly under it will
reﬂect a registration), so we add google.com to the IDL.
Any subsequent matching against the IDL likewise em-
ploys trimming of names using the same procedure.
We note that we could implement the IDL with ﬁner
granularity than described above. In particular, we could
frame it in terms of per-client ﬁltering, or using custom
entropy thresholds. We leave exploring these reﬁnements
for future work.
4 Both of these are actual detections.
Exﬁltration Scenario
Query name-content
Query name-codebook
Timing
Query type
Estimated Data Volume
Total Name Timing
111% 110%
109% 103%
105% 0.8%
111% 0.6%
Type
0.4% 0.01%
5.6% 0.1%
104% 0.2%
6.8% 104%
Table 2: Estimates of data volumes produced by our procedure
measured against speciﬁc exﬁltration scenarios, showing the
total estimate, and the individual contributions from the query
name, timing, and type information estimation.
7 Evaluation
In this section we evaluate the efﬁcacy of our detection
procedure in terms of assuring that it can detect explicit
instances of communication tunneled over DNS (§ 7.1)
and investigating its performance on data from produc-
tion networks (§ 7.2). For this latter, we assess both the
procedure’s ability to ﬁnd actual surreptitious communi-
cation, and, just as importantly, what sort of burden it
imposes on security analysts due to the events generated.
7.1 Validating on Synthetic Data
To validate our procedure’s ability to accurately measure
communication embedded in DNS queries, we assessed
what sort of estimates it produces for scenarios under
which we fully control the DNS communication used for
exﬁltration. Table 2 summarizes the results, comparing
the information vector used for exﬁltration vs. the esti-
mates of the volume of data present in the corresponding
lookups, both in total and when restricted to just consid-
ering a single information vector. All values are percent-
ages of the actual exﬁltration size, so a value of 105 indi-
cates an estimate that was 105% of the true size (i.e., the
estimate was 5% too high). Naturally, estimators that fo-
cus on information vectors different from those used in a
given exﬁltration scenario can greatly underestimate the
data volume if used in isolation, highlighting the need to
combine such estimators into a ﬁnal comprehensive sum.
Regarding the scenarios reﬂected in the table, to assess
tunnels based on encoding information directly in query
names, we recorded Iodine [10] queries while sending a
99,438-byte compressed ﬁle with scp. The 11 % differ-
ence (shown in the “Query name-content” row) between
measured content and actual content is nearly all due
to tunnel encapsulation overhead (SSH, TCP/IP headers,
Iodine framing). As we are not aware of any available
tunneling tools that leverage repeated (codebook-style)
queries, timing, or varying query types, we wrote sim-
ple proof-of-principle implementations for testing pur-
poses. The codebook-style implementation used 16 dis-
tinct names that each convey four data bits per query,
26  22nd USENIX Security Symposium 
USENIX Association
10
Type Of Activity \ Dataset
Lookups (days)
Detection threshold
Conﬁrmed DNS channel
Benign use
Malware
Misconﬁguration
IPv4 PTR
IPv6 PTR
Unknown
Total
Domains ﬂagged (ﬁrst week)
Domains ﬂagged (typical week)
INDLAB
57B (1,212)
4kB
0
286
2
49
11
0
14
362
16
2.0
LBL
73B (2,565)
4kB
2
306
2
62
29
5
27
433
5
1.1
NERSC
12B (1,642)
4kB
0
29
0
5
4
0
0
38
3
0.15
UCB CHINA
69M (5)
10kB
0
41
2
8
3
0
13
67
(67+)
N/A
1.7B (45)
10kB
0
200
5
126
26
1
13
371
199
32
SIE
77B (53)
10kB
57
4,815
74
310
N/A
N/A
1
5,256
3,002
358
SIEUNIQ
12B (53)
10kB
57
1,088