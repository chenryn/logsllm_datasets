for malicious examples that would be correctly classiﬁed by
a human but successfully evade DNN model classiﬁers. Since
then, more efﬁcient algorithms [44], [35], [36] are proposed to
trick DNN models into misclassifying inputs. Note that these
described algorithms are all white-box attacks as they require
internal information from the target model.
Prior works on black-box attacks rely on querying victim
models and using the feedback from adversarial examples to
guide the synthetic dataset crafting process [31]. In compari-
son, some others assume that the existing trained models (i.e.,
substitute/local models) have boundaries similar to the victim
models, and show that the adversarial examples generated for
substitute models can transfer well to the non-targeted/targeted
labels [31], [32], [33]. Several studies [45], [46] launch adver-
sarial attacks on deep neural networks by manipulating their
internal features to achieve better performance. We adopt a
similar method of generating adversarial examples using the
internal features of DNN models. As opposed to existing
feature-level adversarial attacks, we mainly concentrate on
the following two aspects: 1) the use of generated malicious
features to craft visually imperceptible adversarial
images
against current state-of-the-art deep neural networks models.
Note that existing feature-level attack methods only use the
feature representation of guide images to generate adversarial
examples rather than generating malicious feature represen-
tation calculated using salience maps; 2) solving for model
parameters that minimize conﬁdence scores for the target class.
Model Extraction Attacks. In these attacks [6], [47], a
malicious entity aims to accurately extract model equivalent
to a target model by querying the labels and conﬁdence scores
of model predictions to inputs. Papernot et al. [21] demonstrate
that an attacker can use synthetic datasets to train a local
substitute model for the victim models. Moreover, several
studies [14], [20] present efﬁcient algorithms to steal machine
learning models. Unlike these prior works, this paper proposes
a more efﬁcient black-box attack method to steal deep learning
models with millions of parameters by applying a special type
of transfer learning scheme and specially crafted adversarial
examples.
Active Learning. Generally, Active Learning (AL) is applied
through iteratively selecting informative examples to present
to users for labeling, while maximizing the performance
of retrained deep learning classiﬁers [48], [49], [50], [51].
Previous uncertainty sampling methods tend to suffer from
the problem of selected examples that lie approximately on
the classiﬁcation boundary being overly similar, resulting in
poor classiﬁcation performance, while users consider such
3
examples as an ideal training set. In this paper, we address
this challenge by leveraging a set of adversarial examples
generation algorithms for increasing the diversity of useful
examples lying on the classiﬁcation boundary, improving the
efﬁciency of query to the victim classiﬁers. As a result, with
black-box access, an adversary can successfully replicate the
functionality of the victim classiﬁer by using a local substitute
classiﬁer with fewer queries compared to previous works on
model extraction attacks.
MLaaS Platforms. Machine learning as a service (MLaaS) is
a group of cloud computing services that provide end users
machine learning products and solutions to data transforma-
tions, model training and ultimately, predictive analytics. We
show the details of ﬁve popular (MLaaS) platforms in Table II,
including the Microsoft Custom Vision, the Face++ Emotion
Recognition API, the IBM Watson Visual Recognition, the
Google AutoML Vision, and the Clarifai Not Safe for Work
(NSFW) API. As shown in Table II, we can see that these
services generally allow users to upload their well-labeled
images to customize models by using the built-in algorithms
or directly adopting the pre-trained models to create workﬂow
speciﬁcally to meet their needs. Finally, MLaaS will provide
APIs for users to leverage powerful tools built on top of
powerful cloud computing resources. For these services, users
can access the API provided by MLaaS and obtain correspond-
ing classiﬁcation results with chosen inputs. In general, users
are incapable of accessing the details of the target model or
the parameters used for optimization (i.e., black-box), which
makes it extremely difﬁcult for an adversary to extract a black-
box model. Based on the classiﬁcation methods provided by
those services, we can categorize them into two types: Non-
neural-net based model and Neural-net based model. For the
non-neural-net based models, small scale machine learning
models (e.g. logistic regression, decision tree, and random
forest) are widely used for general classiﬁcation tasks. For the
neural-net based model, MLaaS deep neural networks are used
as the basic architecture for image classiﬁcation or object de-
tection tasks. Some of them also use transfer learning methods,
allowing users to train high-quality customized models using a
small labeled dataset. The providers monetize their services by
charging users for training models or querying existing models
through their APIs.
III. BACKGROUND
A. Problem Formulation
Given a black-box victim model fv
accepts
the input x ∈ Rn and produces output fv(x) =
v (x)] ∈ Rm, an adversary aims to use as
[f 1
v (x), f 2
few queries Nquery as possible to extract a substitute model fs
v (x), ..., f m
that
Fig. 2: Overview of the transfer framework for our proposed model theft attack. From left to right: (a) generate unlabeled
adversarial examples as synthetic dataset. (b) query victim model using the generated synthetic dataset. (c) label adversarial
examples according to the output of the victim model. (d) train the local substitute model using the synthetic dataset. (e) use the
local substitute model for predictions. The local substitute model is expected to match the performance of the victim model.
with near-identical performance (i.e., functionality) as a victim
model fv deployed on MLaaS platforms. Speciﬁcally,
the
adversary can launch the attack on a paid MLaaS to construct
fs that closely matches fv even in black-box settings (i.e.
the adversary has no internal knowledge of the victim model
such as network architecture A, exact training dataset D,
weights W, etc.) The adversary’s only capability is to collect
a synthetic training dataset T = {(x, fv(x))} while providing
informative input data x ∼ PA(X) to retrain the substitute
model fs on such a dataset to replicate the functionality of
victim model fv.
B. Threat Model
The Machine learning as a service (MLaaS) provided by
cloud-based platforms offer users a prediction API based on a
DNN model pre-trained on the private dataset. The structures
and/or designs inside API are usually inaccessible to the public
due to economic and privacy concerns, i.e., black-box. In our
work, we assume an adversary targets such pay-as-you-go
commercial machine learning services which provide cloud-
based platforms to help users solve common deep learning
training and
problems such as data pre-processing, model
model evaluation. The adversary will
theft
attacks on a paid MLaaS to construct fs that closely matches
victim model fv in black-box setting, meaning that the ad-
versary has no inner knowledge of the victim model such
as network architecture, exact training data, hyperparameter,
weights, etc. The adversary’s only capability is to query APIs
with particular inputs (i.e., malicious examples) and receive
the resulting prediction or conﬁdence scores. The substitute
model fs extracted by an adversary can be then arbitrarily
used without incurring any query cost, i.e., the adversary gains
a free version of the victim model.
launch model
neurons of the previous hidden layer. These neurons serve
as computational units which transform input data into rep-
resentations through particular activation functions. Many pre-
trained models for various tasks are available for researchers
to utilize directly,
like AlexNet [52], VGGNet [53], VG-
GFace [54] and ResNet [55]. These models have given rise to
classiﬁcation accuracy in computer visual tasks with increasing
computational complexity. Many techniques have been used
to achieve image classiﬁcation goals in practical applications
of DNN. These pre-trained models can be used in transfer
learning to apply the knowledge learned from source domains,
Ds = {(xi, yi)}N
to other different but related target
domains, Dt = {(xi, yi)}M
i=1.
i=1,
For the source task, we use four pre-trained networks
including AlexNet, VGG19, VGGFace and ResNet50 as our
basic architectures. In order to extract the multi-scale image
representation, we remove the fully connected FC6 layer of
the pre-trained VGG19 and add a DeepID layer formed by
combining the features in the previous max-pooling layer and
convolutional layer. Note that this extra DeepID layer is on
top of the VGG19, followed by two fully connected layers
(FC7 and FC8) which use the output of DeepID layer as input.
The weights and bias in the previous convolutional layers are
trained on the ImageNet dataset and shared by the source and
target tasks. In comparison, both DeepID layer and two fully
connected layers (FC7 and FC8) will be ﬁne-tuned on synthetic
datasets described in the following section. The dimensions of
DeepID layer and fully connected layers FC7 will be ﬁxed
to 480 and 4096, respectively. The dimension of FC8 will be
equal to the number of target classes it predicts. This network
takes a ﬁxed-size 224 × 224 RGB ConvNet images as input
and boosts the performance of classiﬁcation by pushing the
depth to 19 weight layers.
C. Transfer Architecture Construction
Figure 2 shows the overview of the transfer framework
for our proposed model theft attack. Deep Neural Networks
are made up of a cascade of computational
layers which
serves to learn automatic feature extraction and transformation.
In general, these representations present different levels of
abstraction in deep learning space. In deep neural networks,
each hidden layer has a set of neurons connected to the
IV. MODEL THEFT ATTACKS
A. Adversarial Active Learning
1) Problem Analysis: By selecting an informative subset
of unlabeled data Du(x) to present for labeling by a human
expert, active learning (AL) aims to minimize the labeling
cost in supervised learning while simultaneously maximizing
performance of the classiﬁer. The key idea of active learning
4
Source Domain(a) Unlabeled Synthetic Dataset(b) MLaaS QueryProblem DomainDB(c) Synthetic Dataset with Stolen Labels(e )PredictionReused Layers Retrained Layers?(d) Feature TransferkTNKS− Layer copied from Teacher Layer trained by Student (Adversary)is how an user can quantify the importance of each exam-
ple in the active pool, for example, “useful” or “unusable”.
Motivated by the existing works on active learning [48], [49],
[50], [51], we proposed a new learning methodology named
margin-based adversarial AL for gathering a set of informative
instances to train a substitute model with performance similar
to the victim model fv. We formally formulate this uncer-
tainty sampling of margin-based adversarial AL as a querying
function Qmulticlass, which chooses a set of useful examples
Dt(x) ⊆ Du(x) from the given unlabeled data Du(x), known
as an active learning pool. The key idea of such margin-based
active learning is that only a few examples from the pool of
unlabeled data are useful or informative for determining the
separating surface of the victim classiﬁer, and all the other
examples are superﬂuous to the classiﬁer.
Fig. 3: Illustration of the margin-based uncertainty sampling
strategy.
Speciﬁcally, we apply a margin-based uncertainty sampling
methodology as the adaptive strategy for boosting examples
where the target classiﬁer is the least conﬁdent, meaning that
these selected adverserial examples lie on the global margin of
target classiﬁer. Since a multiclass classiﬁer can be considered
as a set of binary classiﬁers, we ﬁrst propose the margin-
based active learning algorithm for a linear binary classiﬁer and
provide a geometric illustration of the uncertainty sampling
theory in Figure 3. Abstractly, we assume a learned afﬁne
classiﬁer is a function f : X → Y which returns the prediction
results (e.g., labels and conﬁdence) within the range Y when
given random input images x ∈ D(x) (e.g., extracted from test
dataset with the same distribution as the training dataset). We
also denote the afﬁne hyperplane as H = {x : f (x) = 0}. We
propose a new iterative attack procedure, named FeatureFool
(The details will be demonstrated in the remaining part of this
Section), to generate the adversarial examples with different
conﬁdence. Here, the adversary’s goal is not to estimate the
robustness of victim binary classiﬁer fv(x), but rather to craft
useful examples for the synthetic dataset which the domain
fs(x) will be retrained on. In this work, the synthetic dataset
generated by an adversary consists of two types of examples:
one is minimum-conﬁdence legitimate example, and the other
is minimum-conﬁdence adversarial example. In comparison to
those examples with high conﬁdence, the examples in synthetic
dataset are more likely to provide useful information about
afﬁne hyperplane H of the binary classiﬁer as a whole. For
instance, as shown in Figure 3, we can see that the green
circles (minimum-conﬁdence legitimate examples) and pink
triangles (minimum-conﬁdence adversarial examples) are near
the afﬁne hyperplane H, there is high uncertainty (i.e., the least
conﬁdence) and hence maximum performance with limited
black-box queries.
We now extend the margin-based adversarial active learn-
5
ing algorithm to the multiclass case. The margin-based strate-
gies in previous works are only effective in such a scenario
where an adversary can determine the distance between the
images of active learning pool and the afﬁne hyperplane H
of the target classiﬁer. However, measuring such a distance is
often intractable due to the high complexity of the geometrical
shape of the afﬁne hyperplane H in the multiclass models. We
address this challenge by designing FeatureFool for exploring
the useful examples where the target multiclass model has least
conﬁdence (LC). The proposed margin-based adversarial active
learning methodology can be formulated as follows:
QLC
multiclass : x(cid:63)
s ∈ arg min
x(cid:48)∈Du(x)
κ (x(cid:48), y, w)
(1)
where y donates the predicted label corresponding to the
ﬁrst highest classiﬁcation conﬁdence, w donates the weights
of victim classiﬁer, κ denotes the output conﬁdence while
given random inputs x(cid:48) ∈ Du(x). This approach chooses
those informative examples from the given unlabeled dataset
Du(x) with the smallest margin (i.e., least conﬁdence) and thus
maximizes the uncertainty of instances. We further consider
using these useful examples as synthetic dataset to retrain
convolutional layers shared by the source domains. In the
previous works, Silva et al. [14] directly make a large amount
of superﬂuous queries to obtain the labeled data needed to
generate the synthetic datasets and successfully train a local
model with the near-perfect performance of the victim model.
However, such large-scale queries would be expensive and
make the attack easy to be detected by the MLaaS provider. To
address these problems, we try the relevant queries by focusing
on the two crucial objectives below: (1) Adopting Feature-
Fool to craft a basic informative dataset Du(x) where each
example x ∈ Du(x) has different classiﬁcation conﬁdence;
and (2) Maximizing examples efﬁciency through uncertainty
sampling strategy resulting in a subset of training examples
Dt(x) ⊆ Du(x). Our experimental results show that such
adversarial examples would help considerably decrease the
number of queries to victim models.
s = QLC
2) Synthetic Dataset Generation: We utilize the margin-
based adversarial active learning algorithm to craft the in-
formative examples and then query the victim model fv for
labels. Finally, the resulting image-prediction pairs can be
viewed as a synthetic dataset to train the substitute model fs
for the purpose of replicating the victim model fv inside the
commercial API. We formally deﬁne the problem of ﬁnding an
informative example x(cid:48)
s selected by multiclass active function
QLC
multiclass as follows:
(cid:48)
x
(2)
For the x(cid:48), ﬁve generation strategies are considered in this
paper (Due to vast majority of works on adversarial examples