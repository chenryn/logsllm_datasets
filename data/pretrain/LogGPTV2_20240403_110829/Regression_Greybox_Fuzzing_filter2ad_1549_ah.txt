20
15
10
5
0
20
15
10
5
0
20
15
10
5
0
0.010
0.005
0.000
)
s
r
u
o
h
n
i
(
h
s
a
r
c
t
s
r
i
f
o
t
e
m
T
i
Fuzzer
â—
aflgo
â—
aflchurn
Figure 10: Deduplication results for AFLGo vs. AFLChurn.
In order to determine the amplifier functions for calculating fit-
ness of ages and churns, some experiments are conducted. As for
#changes, we examine functions including ğ‘™ğ‘œğ‘”(ğ‘¥), ğ‘¥, ğ‘¥ğ‘™ğ‘œğ‘”(ğ‘¥), and
ğ‘¥ 2. As for ages, we examine functions including 1/ğ‘¥, ğ‘™ğ‘œğ‘”(ğ‘¥), 1/ğ‘™ğ‘œğ‘”(ğ‘¥),
and 1/ğ‘™ğ‘œğ‘”2 (ğ‘¥). Tables 4 and 5 show the results for our experiments
that were repeated twenty times (20 trials).
RGF steers computing resources towards code regions that are
changed more often. However, this does not mean that the most
frequent changed code regions must contain bugs. Therefore, we
select ğ‘™ğ‘œğ‘”(ğ‘¥) as it performs the best according to Table 4. Similarly,
RGF guides fuzzing towards code regions that are changed more
recently. We select 1/ğ‘¥ since it performs the best among all the four
amplifiers according to Table 5.
Table 4: Comparison of amplifier functions for churn (ğ‘¥ =
#ğ‘â„ğ‘ğ‘›ğ‘”ğ‘’ğ‘ ). The numbers are factors whose baseline is ğ‘™ğ‘œğ‘”(ğ‘¥).
Mean TTE
#Crashing Trials
Mean #Crashes
Subject
libgit2
ndpi
file
libxml2
grok
aspell
openssl
libhtp
harfbuzz
unicorn
unbound
usrsctp
ğ‘™ğ‘œğ‘” (ğ‘¥ ) ğ‘¥ ğ‘¥ğ‘™ğ‘œğ‘” (ğ‘¥ ) ğ‘¥ 2 ğ‘™ğ‘œğ‘” (ğ‘¥ ) ğ‘¥ ğ‘¥ğ‘™ğ‘œğ‘” (ğ‘¥ ) ğ‘¥ 2 ğ‘™ğ‘œğ‘” (ğ‘¥ ) ğ‘¥ ğ‘¥ğ‘™ğ‘œğ‘” (ğ‘¥ ) ğ‘¥ 2
0.9
0.9
1.1
1.0
1.5
0.6
1.5
1.1
0.4
0.9
0.4
0.6
1.0
0.9
1.0
0.9
0.6
0.6
1.0
1.4
1.3
0.9
1.7
0.9
1.0
1.1
1.0
0.9
3.6
1.2
1.1
1.1
4.9
0.6
1.0
0.7
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.4
1.4
1.0
1.8
1.7
1.1
0.8
0.9
1.2
1.0
1.2
1.1
0.9
0.7
1.0
2.1
2.1
1.0
1.4
0.8
1.2
1.0
0.8
0.9
1.3
1.6
1.0
0.9
1.7
1.7
0.9
1.6
0.8
0.6
1.9
1.1
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.1
1.0
1.0
1.0
1.0
1.1
0.8
1.0
1.0
1.0
1.0
1.1
1.0
1.0
1.0
1.2
1.0
1.1
1.2
1.0
1.0
1.0
1.0
1.1
0.9
1.2
1.0
0.8
1.0
0.6
0.8
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
Figure 9: Bug finding efficiency of AFLGo vs. AFLChurn
5.4 Analysis of Amplifier Functions
In Section 3.1, we discussed the utility of amplifier functions to am-
plify the signal of Å‚interestingÅ¾ basic blocks (BBs), i.e., BBs that have
been changed more recently or more often. We presented our deci-
sion to choose the inverse of the number of days and the logarithm
6 RELATED WORK
The general observationÃthat code which has changed more re-
cently or more frequently is more likely to be buggyÃis well known
in the defect prediction community [21, 22, 32]. For instance, the
survey by Radjenovic et al. [32] concludes that, among many defect
predictors, the number of changes and the age of a module have
the strongest correlation with post-release faults. Nagappan et al.
Session 7B: Fuzzing CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea2180Table 5: Comparison of amplifier functions for ages (ğ‘¥ =
ğ‘ğ‘”ğ‘’ğ‘ ). The numbers are factors whose baseline is 1/ğ‘¥.
#Crashing Trials
Mean #Crashes
ğ‘™ğ‘œğ‘”2 (ğ‘¥ )
1
ğ‘™ğ‘œğ‘”2 (ğ‘¥ )
Subject
libgit2
ndpi
libhtp
grok
harfbuzz
unicorn
openssl
zstd
Mean TTE
1
ğ‘¥ ğ‘™ğ‘œğ‘” (ğ‘¥ )
1.2
1.0
0.8
1.0
2.5
1.0
1.2
1.0
1.0
1.0
1.5
1.0
0.9
1.0
1.0
1.3
1
ğ‘™ğ‘œğ‘” (ğ‘¥ )
1.0
0.6
3.5
1.7
0.5
1.6
1.1
0.13
1
ğ‘™ğ‘œğ‘”2 (ğ‘¥ )
1.3
0.3
1.0
0.9
0.7
1.0
0.8
0.5
1
ğ‘¥ ğ‘™ğ‘œğ‘” (ğ‘¥ )
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.8
1.0
0.9
1.0
1.0
1.0
1.0
0.5
1
ğ‘™ğ‘œğ‘” (ğ‘¥ )
1.0
1.0
1.0
0.9
1.5
0.8
0.9
0.5
1
1.0
1.1
1.0
1.0
0.5
1.0
0.8
1.5
1
ğ‘¥ ğ‘™ğ‘œğ‘” (ğ‘¥ )
0.9
1.0
1.0
1.0
1.1
1.0
1.1
1.0
5.5
1.0
0.3
1.0
1.0
1.0
1.0
0.5
1
ğ‘™ğ‘œğ‘” (ğ‘¥ )
0.9
1.1
0.9
0.7
2.2
0.5
0.8
0.5
0.9
1.1
0.8
1.0
1.3
0.8
0.6
1.5
[22] investigate Å‚change bursts" as defect predictors and introduce
several other change-related measures of defectiveness.
However, defect prediction only provides a value that measures
how likely it is that a given component contains a fault. No actual
evidence of the defectiveness is generated. Moreover, to comple-
ment existing work on defect prediction, we also present empirical
results on the rate at which new bugs are reported throughout the
lifetime of a project across 300+ open-source projects. We also re-
port the proportion of bugs reported over time that are regressions,
i.e., introduced by previous commits. While we focus specifically
on the prevalence and discovery rate of regression bugs in OSSFuzz,
Ding and Le Goues [11] provide an extended, general discussion
of bugs in OSSFuzz. Ozment and Schechter [26] report that the
security of the OpenBSD operating system improved over the study
period of 7.5 years since the creation of the project. However, as
discussed in Section 2.3, in the context of 350+ projects in OSSFuzz,
we find no evidence the state of software security improves.
The stream of works that is most related to our regression grey-
box fuzzing develops fuzzing techniques to find bugs in a specific
code commit (see Figure 2). Given a set of changed statements as
targets, the objective is to generate inputs that reach these changes
and make the behavioral differences observable. Early work cast
the reachability of the changed code as a constraint satisfaction
problem. Several symbolic execution-based directed and regression
test generation techniques were proposed [4, 18, 19, 33, 40]. Recent
work cast the reachability of a given set of statements as an opti-
mization problem to alleviate the required program analysis during
test generation (e.g,. by moving it to compile time) [6, 9, 39, 42].
Others have combined symbolic execution-based and search-based
techniques [24, 30].
In contrast to existing work, (i) we generalize the binary distribu-
tion (a statement is either a target or not) to a numeric distribution
(every statement is a target, but to varying degrees), and (ii) we pro-
pose to test all commits simultaneously but recent commits with
higher priority. The benefits of our approach over the previous
work which focuses only on a specific commit are illustrated in
Figure 2 and further discussed in Section 5.3. In contrast to directed
greybox fuzzing [6] where the analyzed call graph and control flow
graphs, that are used to compute distance information, are often
incomplete [9], our regression greybox fuzzing only requires access
to the versioning system of the instrumented program. Regression
greybox fuzzing instruments all code. No distance computation is
required to steer the fuzzer towards code-of-interest.
To improve the bug finding ability of fuzzing, researchers have
proposed other targets for directed greybox fuzzing. For instance,
the fuzzer can be steered towards dangerous program locations,
such as potential memory-corruption locations [15, 38], resource-
consuming locations [29], race-condition locations [8], or exercise
sequences of targets [31], e.g., to expose use-after-free vulnerabili-
ties [23], or to construct an exploit from a given patch [27]. Some
researchers propose to focus on sanitizer locations [10, 25]. A sani-
tizer turns an unobservable bug into a crash that can be identified
and flagged by the fuzzer [14, 35, 36]. Others have proposed to
steer the fuzzer towards code that is predicted as potentially de-
fective [13, 28]. In contrast, a regression greybox fuzzer is directed
towards code that is changed more recently or more frequently.
7 DISCUSSION
Our findings in our empirical investigation are daunting (Section 2).
For the 350+ open source software (OSS) projects we studied,a in
the master branch new bugs are discovered at a constant rate. In
fact, we only measure bugs found by fuzzers continuously run
within the OSSFuzz project. There are also non-fuzzing bugs
that are discovered downstream, or during manual security
auditing which we do not measure. The constant rate of 3-4 new
bug reports per week in the master branch is a lower bound.
ahttps://github.com/google/oss-fuzz/tree/master/projects
Our only explanation for the constant bug discovery rate is that
recent changes introduced new bugs. Indeed, three in every four
fuzzer-reported bugs (77%) are regressions, and the probability
increases the longer a project is subjected to continuous fuzzing.
Regressions are a major class of bugs! Yet, our greybox fuzzers
stress all code with equal priority. Most of the code in a project has
never been touched. This code has been fuzzed since the onboarding
to OSSFuzz. Nevertheless, it is treated with equal priority as code
that is currently under development. We believe that the bit of code
which is currently under active development deserves a lot more
focus from the fuzzerÃnot a specific commit, mind you, but all the
code has recently been under development. Regression greybox
fuzzing is the first approach to exploit this observation.
In future work, we plan to investigate other change-based mea-
sures that have previously been associated with defectiveness. For
instance, code that has been changed in short but intensive bursts,
or code that was involved in larger commits might deserve more
focus. We could also direct the fuzzer focus towards code that was
involved in more patches of security vulnerabilities. Regression
greybox fuzzing is a first but substantial step in this direction.
ACKNOWLEDGMENTS
We thank the anonymous reviewers and our shepherd Jun Xu for
their valuable feedback. We are grateful to the Google Fuzzbench
team, particularly Jonathan Metzmann and Abhishek Arya, for the
kind and generous help with our experiments. We thank Christian
Holler for feedback on early versions of this paper. This work was
partly funded by the Australian Research Council (DE190100046)
and by a Google Faculty Research Award. We thank Prof Yang Xiang
and CSIRO Data61 for their financial support of the first author.
Session 7B: Fuzzing CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea2181REFERENCES
[1] [n.d.]. AFL. https://github.com/google/AFL accessed 21-January-2021.
[2] [n.d.]. OneFuzz: A self-hosted Fuzzing-As-A-Service platform. https://github.
com/microsoft/onefuzz accessed 21-January-2021.
[3] Marcel BÃ¶hme, Cristian Cadar, and Abhik Roychoudhury. 2021. Fuzzing: Chal-
lenges and Opportunities. IEEE Software (2021), 1Å›9. https://doi.org/10.1109/MS.
2020.3016773
[4] Marcel BÃ¶hme, Bruno C.d.S. Oliveira, and Abhik Roychoudhury. 2013. Partition-
based Regression Verification. In Proceedings of the 35th International Conference
on Software Engineering (San Francisco, California, USA) (ICSE 2013). 301Å›310.
https://doi.org/10.5555/2486788.2486829
[5] Marcel BÃ¶hme, Bruno C. d. S. Oliveira, and Abhik Roychoudhury. 2013. Regression
Tests to Expose Change Interaction Errors. In Proceedings of the 2013 9th Joint
Meeting on Foundations of Software Engineering (ESEC/FSE 2013). 334Å›344. https:
//doi.org/10.1145/2491411.2491430
[6] Marcel BÃ¶hme, Van-Thuan Pham, Manh-Dung Nguyen, and Abhik Roychoudhury.
2017. Directed greybox fuzzing. In Proceedings of the 2017 ACM SIGSAC Conference
on Computer and Communications Security. ACM, 2329Å›2344.
[7] Oliver Chang, Jonathan Metzman, Max Moroz, Martin Barbella, and Abhishek
Arya. 2016. OSS-Fuzz: Continuous Fuzzing for Open Source Software. https:
//github.com/google/oss-fuzz [Online; accessed 19-January-2021].
[8] Hongxu Chen, Shengjian Guo, Yinxing Xue, Yulei Sui, Cen Zhang, Yuekang
Li, Haijun Wang, and Yang Liu. 2020. MUZZ: Thread-aware Grey-box Fuzzing
for Effective Bug Hunting in Multithreaded Programs. In 29th USENIX Security
Symposium (USENIX Security 20). USENIX Association, 2325Å›2342.
[9] Hongxu Chen, Yinxing Xue, Yuekang Li, Bihuan Chen, Xiaofei Xie, Xiuheng Wu,
and Yang Liu. 2018. Hawkeye: Towards a Desired Directed Grey-box Fuzzer. In
Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications
Security. ACM, 2095Å›2108.
[10] Yaohui Chen, Peng Li, Jun Xu, Shengjian Guo, Rundong Zhou, Yulong Zhang,
Long Lu, et al. 2020. SAVIOR: Towards Bug-Driven Hybrid Testing. In IEEE
Symposium on Security and Privacy (SP). IEEE Computer Society.
[11] Zhen Yu Ding and Claire Le Goues. 2021. An Empirical Study of OSS-Fuzz Bugs.
In Proceedings of the 18th International Conference on Mining Software Repositories
(MSR). 1Å›12.
[12] Marco Dorigo, Mauro Birattari, and Thomas Stutzle. 2006. Ant colony optimiza-
tion. IEEE computational intelligence magazine 1, 4 (2006), 28Å›39.
[13] Xiaoning Du, Bihuan Chen, Yuekang Li, Jianmin Guo, Yaqin Zhou, Yang Liu,
and Yu Jiang. 2019. Leopard: Identifying Vulnerable Code for Vulnerability
Assessment through Program Metrics. In Proceedings of the 41st International
Conference on Software Engineering (ICSE â€™19). 60Å›71. https://doi.org/10.1109/
ICSE.2019.00024
[14] Istvan Haller, Yuseok Jeon, Hui Peng, Mathias Payer, Cristiano Giuffrida, Herbert
Bos, and Erik van der Kouwe. 2016. TypeSan: Practical Type Confusion Detection.
In CCS. 517Å›528.
[15] Istvan Haller, Asia Slowinska, Matthias Neugschwandtner, and Herbert Bos. 2013.
Dowsing for Overflows: A Guided Fuzzer to Find Buffer Boundary Violations. In
USENIX. 49Å›64.
[16] George Klees, Andrew Ruef, Benji Cooper, Shiyi Wei, and Michael Hicks. 2018.
Evaluating Fuzz Testing. In Proceedings of the 2018 ACM SIGSAC Conference on
Computer and Communications Security. ACM, 2123Å›2138.
[17] Tomasz Kuchta, Hristina Palikareva, and Cristian Cadar. 2018. Shadow symbolic
execution for testing software patches. ACM Transactions on Software Engineering
and Methodology (TOSEM) 27, 3 (2018), 1Å›32.
[18] Kin-Keung Ma, Khoo Yit Phang, Jeffrey S. Foster, and Michael Hicks. 2011. Di-
rected Symbolic Execution. In Proceedings of the 18th International Conference on
Static Analysis (SASâ€™11). 95Å›111.
[19] Paul Dan Marinescu and Cristian Cadar. 2013. KATCH: High-Coverage Testing
of Software Patches. In European Software Engineering Conference / ACM SIGSOFT
Symposium on the Foundations of Software Engineering (ESEC/FSE 2013) (Saint
Petersburg, Russia). 235Å›245.
[20] Jonathan Metzman, Abhishek Arya, and Laszlo Szekeres. 2020. FuzzBench: Fuzzer
Benchmarking as a Service. https://security.googleblog.com/2020/03/fuzzbench-
fuzzer-benchmarking-as-service.html
[21] Raimund Moser, Witold Pedrycz, and Giancarlo Succi. 2008. A Comparative
Analysis of the Efficiency of Change Metrics and Static Code Attributes for
Defect Prediction. In ICSE. 181Å›190.
[22] Nachiappan Nagappan, Andreas Zeller, Thomas Zimmermann, Kim Herzig, and
Brendan Murphy. 2010. Change bursts as defect predictors. In 2010 IEEE 21st
International Symposium on Software Reliability Engineering. 309Å›318.
[23] Manh-Dung Nguyen, SÃ©bastien Bardin, Richard Bonichon, Roland Groz, and
Matthieu Lemerre. 2020. Binary-level Directed Fuzzing for Use-After-Free Vul-
nerabilities. In 23rd International Symposium on Research in Attacks, Intrusions
and Defenses (RAID). USENIX Association, 47Å›62.
[24] Yannic Noller, Corina Pasareanu, Marcel BÃ¶hme, Youcheng Sun, Hoang Lam
Nguyen, and Lars Grunske. 2020. HyDiff: Hybrid Differential Software Analy-
sis. In Proceedings of the 42nd ACM/IEEE International Conference on Software
Engineering (ICSE 2020). 1273Å›1285.
[25] Sebastian Ã–sterlund, Kaveh Razavi, Herbert Bos, and Cristiano Giuffrida. 2020.
ParmeSan: Sanitizer-guided Greybox Fuzzing. In 29th USENIX Security Symposium
(USENIX Security 20). USENIX Association, Boston, MA.
[26] Andy Ozment and Stuart E Schechter. 2006. Milk or wine: does software security
improve with age?. In USENIX Security, Vol. 6.
[27] J. Peng, F. Li, B. Liu, L. Xu, B. Liu, K. Chen, and W. Huo. 2019. 1dVul: Discovering
1-Day Vulnerabilities through Binary Patches. In 2019 49th Annual IEEE/IFIP
International Conference on Dependable Systems and Networks (DSN). 605Å›616.
https://doi.org/10.1109/DSN.2019.00066
[28] Anjana Perera, Aldeida Aleti, Marcel BÃ¶hme, and Burak Turhan. 2020. Defect
Prediction Guided Search-Based Software Testing. In Proceedings of the 35th
IEEE/ACM International Conference on Automated Software Engineering (ASE).
1Å›13. https://doi.org/10.1145/3324884.3416612
[29] Theofilos Petsios, Jason Zhao, Angelos D Keromytis, and Suman Jana. 2017.
Slowfuzz: Automated domain-independent detection of algorithmic complexity
vulnerabilities. In Proceedings of the 2017 ACM SIGSAC Conference on Computer
and Communications Security. ACM, 2155Å›2168.
[30] Van-Thuan Pham, Marcel BÃ¶hme, and Abhik Roychoudhury. 2016. Model-based
Whitebox Fuzzing for Program Binaries. In Proceedings of the 31st IEEE/ACM
International Conference on Automated Software Engineering (ASE). 552Å›562.
[31] Van-Thuan Pham, Marcel BÃ¶hme, and Abhik Roychoudhury. 2020. AFLNet: A
Greybox Fuzzer for Network Protocols. In Proceedings of the 2020 IEEE Inter-
national Conference on Software Testing, Verification and Validation (ICST 2020).
460Å›465.
[32] Danijel RadjenoviÄ‡, Marjan HeriÄko, Richard Torkar, and AleÅ¡ Å½ivkoviÄ. 2013.
Software fault prediction metrics: A systematic literature review. Information
and software technology 55, 8 (2013), 1397Å›1418.
[33] David A Ramos and Dawson Engler. 2015. Under-constrained symbolic execution:
Correctness checking for real code. In USENIX Security. 49Å›64.
[34] E. J. Schwartz, T. Avgerinos, and D. Brumley. 2010. All You Ever Wanted to Know
about Dynamic Taint Analysis and Forward Symbolic Execution (but Might Have
Been Afraid to Ask). In 2010 IEEE Symposium on Security and Privacy. 317Å›331.
[35] Konstantin Serebryany, Derek Bruening, Alexander Potapenko, and Dmitry
Vyukov. 2012. AddressSanitizer: A Fast Address Sanity Checker. In USENIX
ATC. 28.
[36] Erik van der Kouwe, Vinod Nigade, and Cristiano Giuffrida. 2017. DangSan:
Scalable Use-after-Free Detection. In EuroSys â€™17. 405Å›419.
[37] Alastair J. Walker. 1977. An Efficient Method for Generating Discrete Random
Variables with General Distributions. ACM Trans. Math. Software 3, 3 (Sept. 1977),
253Å›256.
[38] Yanhao Wang, Xiangkun Jia, Yuwei Liu, Kyle Zeng, Tiffany Bao, Dinghao Wu,
and Purui Su. 2020. Not All Coverage Measurements Are Equal: Fuzzing by
Coverage Accounting for Input Prioritization. In NDSS.
[39] Valentin WÃ¼stholz and Maria Christakis. 2020. Targeted greybox fuzzing with
static lookahead analysis. In ICSE. 789Å›800.
[40] Guowei Yang, Suzette Person, Neha Rungta, and Sarfraz Khurshid. 2014. Directed
Incremental Symbolic Execution. ACM Trans. Softw. Eng. Methodol. 24, 1, Article
3 (Oct. 2014), 42 pages. https://doi.org/10.1145/2629536
[41] Andreas Zeller. 1999. Yesterday, My Program Worked. Today, It Does Not. Why?.
In ESECâ€™FSE. 253Å›267.
[42] Peiyuan Zong, Tao Lv, Dawei Wang, Zizhuang Deng, Ruigang Liang, and Kai
Chen. 2020. FuzzGuard: Filtering out Unreachable Inputs in Directed Grey-box
Fuzzing through Deep Learning. In USENIX 2020.
Session 7B: Fuzzing CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea2182