### Deduplication Results for AFLGo vs. AFLChurn

To determine the amplifier functions for calculating the fitness of ages and churns, we conducted several experiments. For the number of changes, we examined the following functions: \( \log(x) \), \( x \), \( x \log(x) \), and \( x^2 \). For ages, we tested: \( \frac{1}{x} \), \( \log(x) \), \( \frac{1}{\log(x)} \), and \( \frac{1}{\log^2(x)} \). The results of these experiments, repeated 20 times, are presented in Tables 4 and 5.

RGF (Regression Greybox Fuzzing) directs computing resources towards code regions that have been changed more frequently. However, this does not imply that the most frequently changed code regions necessarily contain bugs. Based on Table 4, we selected \( \log(x) \) as it performed the best. Similarly, RGF guides fuzzing towards code regions that have been changed more recently. According to Table 5, we chose \( \frac{1}{x} \) as it outperformed the other amplifiers.

#### Table 4: Comparison of Amplifier Functions for Churn (\( x = \# \text{changes} \))

| Subject     | Mean TTE | #Crashing Trials | Mean #Crashes |
|-------------|----------|------------------|---------------|
| libgit2     | 0.9      | 1.0              | 3.6           |
| ndpi        | 0.9      | 0.9              | 1.2           |
| file        | 1.1      | 1.0              | 1.1           |
| libxml2     | 1.0      | 0.9              | 1.1           |
| grok        | 1.5      | 0.6              | 4.9           |
| aspell      | 0.6      | 0.6              | 0.6           |
| openssl     | 1.5      | 1.0              | 1.0           |
| libhtp      | 1.1      | 1.4              | 1.0           |
| harfbuzz    | 0.4      | 0.8              | 1.0           |
| unicorn     | 0.9      | 0.9              | 1.0           |
| unbound     | 0.4      | 0.7              | 1.0           |
| usrsctp     | 0.6      | 0.8              | 1.0           |

#### Table 5: Comparison of Amplifier Functions for Ages (\( x = \text{ages} \))

| Subject     | Mean TTE | #Crashing Trials | Mean #Crashes |
|-------------|----------|------------------|---------------|
| libgit2     | 1.2      | 1.0              | 1.0           |
| ndpi        | 1.0      | 1.0              | 1.0           |
| libhtp      | 0.8      | 0.6              | 1.0           |
| grok        | 1.0      | 3.5              | 1.0           |
| harfbuzz    | 2.5      | 1.7              | 1.0           |
| unicorn     | 1.0      | 0.5              | 1.0           |
| openssl     | 1.2      | 1.6              | 1.0           |
| zstd        | 1.0      | 1.1              | 0.13          |

### Analysis of Amplifier Functions

In Section 3.1, we discussed the utility of amplifier functions to enhance the signal of "interesting" basic blocks (BBs), i.e., BBs that have been changed more recently or more often. We decided to use the inverse of the number of days and the logarithm function based on our experimental results.

### Related Work

The observation that code which has changed more recently or more frequently is more likely to be buggy is well-known in the defect prediction community [21, 22, 32]. For example, Radjenovic et al. [32] concluded that the number of changes and the age of a module have the strongest correlation with post-release faults. Nagappan et al. [22] investigated "change bursts" as defect predictors and introduced several other change-related measures of defectiveness.

Defect prediction provides a value that measures the likelihood of a given component containing a fault but does not generate actual evidence of defectiveness. To complement existing work on defect prediction, we present empirical results on the rate at which new bugs are reported throughout the lifetime of a project across 300+ open-source projects. We also report the proportion of bugs reported over time that are regressions, i.e., introduced by previous commits. While we focus specifically on the prevalence and discovery rate of regression bugs in OSSFuzz, Ding and Le Goues [11] provide an extended, general discussion of bugs in OSSFuzz. Ozment and Schechter [26] reported that the security of the OpenBSD operating system improved over a 7.5-year study period. However, in the context of 350+ projects in OSSFuzz, we find no evidence that the state of software security improves.

### Regression Greybox Fuzzing

The most related stream of work to our regression greybox fuzzing develops fuzzing techniques to find bugs in a specific code commit. Given a set of changed statements as targets, the objective is to generate inputs that reach these changes and make the behavioral differences observable. Early work cast the reachability of the changed code as a constraint satisfaction problem, and several symbolic execution-based directed and regression test generation techniques were proposed [4, 18, 19, 33, 40]. Recent work cast the reachability of a given set of statements as an optimization problem to alleviate the required program analysis during test generation [6, 9, 39, 42]. Others have combined symbolic execution-based and search-based techniques [24, 30].

In contrast to existing work, we generalize the binary distribution (a statement is either a target or not) to a numeric distribution (every statement is a target, but to varying degrees), and we propose to test all commits simultaneously but prioritize recent commits. The benefits of our approach over previous work, which focuses only on a specific commit, are illustrated in Figure 2 and further discussed in Section 5.3. In contrast to directed greybox fuzzing [6], where the analyzed call graph and control flow graphs used to compute distance information are often incomplete [9], our regression greybox fuzzing only requires access to the versioning system of the instrumented program. Regression greybox fuzzing instruments all code, and no distance computation is required to steer the fuzzer towards code-of-interest.

### Improving Bug Finding Ability

To improve the bug finding ability of fuzzing, researchers have proposed other targets for directed greybox fuzzing. For instance, the fuzzer can be steered towards dangerous program locations, such as potential memory-corruption locations [15, 38], resource-consuming locations [29], race-condition locations [8], or sequences of targets [31], e.g., to expose use-after-free vulnerabilities [23], or to construct an exploit from a given patch [27]. Some researchers propose focusing on sanitizer locations [10, 25]. A sanitizer turns an unobservable bug into a crash that can be identified and flagged by the fuzzer [14, 35, 36]. Others have proposed steering the fuzzer towards code that is predicted as potentially defective [13, 28]. In contrast, a regression greybox fuzzer is directed towards code that is changed more recently or more frequently.

### Discussion

Our empirical investigation reveals a constant rate of new bug discoveries in the master branch of the 350+ open-source software (OSS) projects we studied. Specifically, we measure 3-4 new bug reports per week, which is a lower bound. Our only explanation for this constant bug discovery rate is that recent changes introduce new bugs. Indeed, three in every four fuzzer-reported bugs (77%) are regressions, and the probability increases the longer a project is subjected to continuous fuzzing. Regressions are a major class of bugs, yet our greybox fuzzers stress all code with equal priority. Most of the code in a project has never been touched and has been fuzzed since the onboarding to OSSFuzz. We believe that the code currently under active development deserves more focus from the fuzzer. Regression greybox fuzzing is the first approach to exploit this observation.

In future work, we plan to investigate other change-based measures that have previously been associated with defectiveness. For instance, code that has been changed in short but intensive bursts, or code that was involved in larger commits, might deserve more focus. We could also direct the fuzzer's focus towards code that was involved in more patches of security vulnerabilities. Regression greybox fuzzing is a substantial step in this direction.

### Acknowledgments

We thank the anonymous reviewers and our shepherd Jun Xu for their valuable feedback. We are grateful to the Google Fuzzbench team, particularly Jonathan Metzmann and Abhishek Arya, for their kind and generous help with our experiments. We thank Christian Holler for feedback on early versions of this paper. This work was partly funded by the Australian Research Council (DE190100046) and by a Google Faculty Research Award. We thank Prof Yang Xiang and CSIRO Data61 for their financial support of the first author.

### References

[1] [n.d.]. AFL. https://github.com/google/AFL accessed 21-January-2021.
[2] [n.d.]. OneFuzz: A self-hosted Fuzzing-As-A-Service platform. https://github.com/microsoft/onefuzz accessed 21-January-2021.
[3] Marcel Böhme, Cristian Cadar, and Abhik Roychoudhury. 2021. Fuzzing: Challenges and Opportunities. IEEE Software (2021), 1–9. https://doi.org/10.1109/MS.2020.3016773
[4] Marcel Böhme, Bruno C.d.S. Oliveira, and Abhik Roychoudhury. 2013. Partition-based Regression Verification. In Proceedings of the 35th International Conference on Software Engineering (San Francisco, California, USA) (ICSE 2013). 301–310. https://doi.org/10.5555/2486788.2486829
[5] Marcel Böhme, Bruno C. d. S. Oliveira, and Abhik Roychoudhury. 2013. Regression Tests to Expose Change Interaction Errors. In Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering (ESEC/FSE 2013). 334–344. https://doi.org/10.1145/2491411.2491430
[6] Marcel Böhme, Van-Thuan Pham, Manh-Dung Nguyen, and Abhik Roychoudhury. 2017. Directed greybox fuzzing. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security. ACM, 2329–2344.
[7] Oliver Chang, Jonathan Metzman, Max Moroz, Martin Barbella, and Abhishek Arya. 2016. OSS-Fuzz: Continuous Fuzzing for Open Source Software. https://github.com/google/oss-fuzz [Online; accessed 19-January-2021].
[8] Hongxu Chen, Shengjian Guo, Yinxing Xue, Yulei Sui, Cen Zhang, Yuekang Li, Haijun Wang, and Yang Liu. 2020. MUZZ: Thread-aware Grey-box Fuzzing for Effective Bug Hunting in Multithreaded Programs. In 29th USENIX Security Symposium (USENIX Security 20). USENIX Association, 2325–2342.
[9] Hongxu Chen, Yinxing Xue, Yuekang Li, Bihuan Chen, Xiaofei Xie, Xiuheng Wu, and Yang Liu. 2018. Hawkeye: Towards a Desired Directed Grey-box Fuzzer. In Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security. ACM, 2095–2108.
[10] Yaohui Chen, Peng Li, Jun Xu, Shengjian Guo, Rundong Zhou, Yulong Zhang, Long Lu, et al. 2020. SAVIOR: Towards Bug-Driven Hybrid Testing. In IEEE Symposium on Security and Privacy (SP). IEEE Computer Society.
[11] Zhen Yu Ding and Claire Le Goues. 2021. An Empirical Study of OSS-Fuzz Bugs. In Proceedings of the 18th International Conference on Mining Software Repositories (MSR). 1–12.
[12] Marco Dorigo, Mauro Birattari, and Thomas Stutzle. 2006. Ant colony optimization. IEEE computational intelligence magazine 1, 4 (2006), 28–39.
[13] Xiaoning Du, Bihuan Chen, Yuekang Li, Jianmin Guo, Yaqin Zhou, Yang Liu, and Yu Jiang. 2019. Leopard: Identifying Vulnerable Code for Vulnerability Assessment through Program Metrics. In Proceedings of the 41st International Conference on Software Engineering (ICSE ’19). 60–71. https://doi.org/10.1109/ICSE.2019.00024
[14] Istvan Haller, Yuseok Jeon, Hui Peng, Mathias Payer, Cristiano Giuffrida, Herbert Bos, and Erik van der Kouwe. 2016. TypeSan: Practical Type Confusion Detection. In CCS. 517–528.
[15] Istvan Haller, Asia Slowinska, Matthias Neugschwandtner, and Herbert Bos. 2013. Dowsing for Overflows: A Guided Fuzzer to Find Buffer Boundary Violations. In USENIX. 49–64.
[16] George Klees, Andrew Ruef, Benji Cooper, Shiyi Wei, and Michael Hicks. 2018. Evaluating Fuzz Testing. In Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security. ACM, 2123–2138.
[17] Tomasz Kuchta, Hristina Palikareva, and Cristian Cadar. 2018. Shadow symbolic execution for testing software patches. ACM Transactions on Software Engineering and Methodology (TOSEM) 27, 3 (2018), 1–32.
[18] Kin-Keung Ma, Khoo Yit Phang, Jeffrey S. Foster, and Michael Hicks. 2011. Directed Symbolic Execution. In Proceedings of the 18th International Conference on Static Analysis (SAS’11). 95–111.
[19] Paul Dan Marinescu and Cristian Cadar. 2013. KATCH: High-Coverage Testing of Software Patches. In European Software Engineering Conference / ACM SIGSOFT Symposium on the Foundations of Software Engineering (ESEC/FSE 2013) (Saint Petersburg, Russia). 235–245.
[20] Jonathan Metzman, Abhishek Arya, and Laszlo Szekeres. 2020. FuzzBench: Fuzzer Benchmarking as a Service. https://security.googleblog.com/2020/03/fuzzbench-fuzzer-benchmarking-as-service.html
[21] Raimund Moser, Witold Pedrycz, and Giancarlo Succi. 2008. A Comparative Analysis of the Efficiency of Change Metrics and Static Code Attributes for Defect Prediction. In ICSE. 181–190.
[22] Nachiappan Nagappan, Andreas Zeller, Thomas Zimmermann, Kim Herzig, and Brendan Murphy. 2010. Change bursts as defect predictors. In 2010 IEEE 21st International Symposium on Software Reliability Engineering. 309–318.
[23] Manh-Dung Nguyen, Sébastien Bardin, Richard Bonichon, Roland Groz, and Matthieu Lemerre. 2020. Binary-level Directed Fuzzing for Use-After-Free Vulnerabilities. In 23rd International Symposium on Research in Attacks, Intrusions and Defenses (RAID). USENIX Association, 47–62.
[24] Yannic Noller, Corina Pasareanu, Marcel Böhme, Youcheng Sun, Hoang Lam Nguyen, and Lars Grunske. 2020. HyDiff: Hybrid Differential Software Analysis. In Proceedings of the 42nd ACM/IEEE International Conference on Software Engineering (ICSE 2020). 1273–1285.
[25] Sebastian Österlund, Kaveh Razavi, Herbert Bos, and Cristiano Giuffrida. 2020. ParmeSan: Sanitizer-guided Greybox Fuzzing. In 29th USENIX Security Symposium (USENIX Security 20). USENIX Association, Boston, MA.
[26] Andy Ozment and Stuart E Schechter. 2006. Milk or wine: does software security improve with age?. In USENIX Security, Vol. 6.
[27] J. Peng, F. Li, B. Liu, L. Xu, B. Liu, K. Chen, and W. Huo. 2019. 1dVul: Discovering 1-Day Vulnerabilities through Binary Patches. In 2019 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN). 605–616. https://doi.org/10.1109/DSN.2019.00066
[28] Anjana Perera, Aldeida Aleti, Marcel Böhme, and Burak Turhan. 2020. Defect Prediction Guided Search-Based Software Testing. In Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering (ASE). 1–13. https://doi.org/10.1145/3324884.3416612
[29] Theofilos Petsios, Jason Zhao, Angelos D Keromytis, and Suman Jana. 2017. Slowfuzz: Automated domain-independent detection of algorithmic complexity vulnerabilities. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security. ACM, 2155–2168.
[30] Van-Thuan Pham, Marcel Böhme, and Abhik Roychoudhury. 2016. Model-based Whitebox Fuzzing for Program Binaries. In Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering (ASE). 552–562.
[31] Van-Thuan Pham, Marcel Böhme, and Abhik Roychoudhury. 2020. AFLNet: A Greybox Fuzzer for Network Protocols. In Proceedings of the 2020 IEEE International Conference on Software Testing, Verification and Validation (ICST 2020). 460–465.
[32] Danijel Radjenović, Marjan Heričko, Richard Torkar, and Aleš Živkovič. 2013. Software fault prediction metrics: A systematic literature review. Information and software technology 55, 8 (2013), 1397–1418.
[33] David A Ramos and Dawson Engler. 2015. Under-constrained symbolic execution: Correctness checking for real code. In USENIX Security. 49–64.
[34] E. J. Schwartz, T. Avgerinos, and D. Brumley. 2010. All You Ever Wanted to Know about Dynamic Taint Analysis and Forward Symbolic Execution (but Might Have Been Afraid to Ask). In 2010 IEEE Symposium on Security and Privacy. 317–331.
[35] Konstantin Serebryany, Derek Bruening, Alexander Potapenko, and Dmitry Vyukov. 2012. AddressSanitizer: A Fast Address Sanity Checker. In USENIX ATC. 28.
[36] Erik van der Kouwe, Vinod Nigade, and Cristiano Giuffrida. 2017. DangSan: Scalable Use-after-Free Detection. In EuroSys ’17. 405–419.
[37] Alastair J. Walker. 1977. An Efficient Method for Generating Discrete Random Variables with General Distributions. ACM Trans. Math. Software 3, 3 (Sept. 1977), 253–256.
[38] Yanhao Wang, Xiangkun Jia, Yuwei Liu, Kyle Zeng, Tiffany Bao, Dinghao Wu, and Purui Su. 2020. Not All Coverage Measurements Are Equal: Fuzzing by Coverage Accounting for Input Prioritization. In NDSS.
[39] Valentin Wüstholz and Maria Christakis. 2020. Targeted greybox fuzzing with static lookahead analysis. In ICSE. 789–800.
[40] Guowei Yang, Suzette Person, Neha Rungta, and Sarfraz Khurshid. 2014. Directed Incremental Symbolic Execution. ACM Trans. Softw. Eng. Methodol. 24, 1, Article 3 (Oct. 2014), 42 pages. https://doi.org/10.1145/2629536
[41] Andreas Zeller. 1999. Yesterday, My Program Worked. Today, It Does Not. Why?. In ESEC’FSE. 253–267.
[42] Peiyuan Zong, Tao Lv, Dawei Wang, Zizhuang Deng, Ruigang Liang, and Kai Chen. 2020. FuzzGuard: Filtering out Unreachable Inputs in Directed Grey-box Fuzzing through Deep Learning. In USENIX 2020.

Session 7B: Fuzzing CCS '21, November 15–19, 2021, Virtual Event, Republic of Korea