bors returned by search results, 35,931 are still active so we
can crawl further pages on these harbors. We have crawled
more than 9 million postings in total.
3.2 Quality of Harbors
Since the goal of comment spamming is to improve the
search ranks of spam websites, the higher quality spam har-
bors have, the more effective comment spamming is. In this
section, we try to evaluate the quality (e.g., reputation) of
these spam harbors in the following three perspectives.
PageRank Score is calculated by PageRank algorithm
[29], which is widely used by search engines to rank the
1Since some of those search links are compromised benign links, they
may also be linked by other benign websites.
2Note that we may not extract a complete list of harbors in this way.
Instead, our conservative goal here is to extract spam harbors with a higher
conﬁdence.
Table 2. Data Collection of Comment Spam Harbors
# of search results
# of harbors (domain)
# of active harbors
# of postings
blog
27,846
4,807
4,685
532,413
forum
29,860
2,515
2,185
640,073
importance of websites. A high PageRank score indicates
a better reputation of the website, which can lead to a high
search rank. To evaluate the overall quality of spam harbors,
we use PageRank scores of spam harbors as an indicator of
the quality of them. We randomly choose 1,000 spam har-
bors in each category, and use Google toolbar [8] to auto-
matically request PageRank scores of these spam harbors.
Figure 2 shows the PageRank scores distribution of these
harbors.
guestbook
31,926
3,878
3,419
other
500,717
27,713
25,642
total
590,349
38,913
35,931
1,469,251
6,497,263
9,139,000
and manually check their postings. Since we may not be
able to crawl all the pages inside a given harbor, our esti-
mated life time based on the limited crawled dataset is sim-
ply a lower bound. Figure 3 shows the distribution of life
time of spam harbors.
Figure 2. PageRank Score Distribution of Har(cid:173)
bors
We can see that spammers target on both high-reputation
and low-reputation harbors. From the graph, less than
20% harbors have a PageRank score higher than 3, which
is the average PageRank score based on [18]. The rea-
son is mainly because that websites with high PageRank
scores usually have stronger spam sanitization mechanisms
or more strict posting rules, which make it harder for spam-
mers to keep automatically spamming on these websites.
In addition, about 40% guestbook harbors have PageRank
scores of 0, because most of them are small company web-
sites that do not have notable reputations. However, spam
links can still inherit and accumulate some reputation from
a large number of such harbors. At least, they can use this
way to let search engines to index them.
Life Time is deﬁned as the time interval between the
posting time of the ﬁrst spam and the recent spam (based
on our crawled dataset). Spammers tend to ﬁnd some sta-
ble harbors that they can keep using. Thus, a long life time
should be a good indication of high quality for spammers.
Since there is no ground truth for the ﬁrst spam and last
spam, we randomly choose 100 harbors in each category
Figure 3. Distribution of Harbor Life Time
We can see that for both blog and forum harbors, more
than 80% harbors have a long life time more than 1 year.
And more than 70% guestbook harbors have a life time
longer than 2 years. During the manual checking process,
we found that for most harbors, the initial postings are
benign but later these harbors are frequently exploited by
spammers for spamming. Especially for guestbook harbors,
almost all the later postings are spam, which conﬁrms that
these spam harbors are kept being used by spammers for a
long time.
Google Indexing Interval is deﬁned as the time dif-
ference between two consecutive Google crawling (index-
ing) time of the same spam harbor. To reduce the crawler
overhead but also keep pace with dynamically updated web
pages, search engine bots need to periodically crawl web-
sites. Thus, there always exist a time lag between posting
time and search engine indexing time. A shorter time lag
(indexing interval) should be a sign of a high quality for
spammers, because search engines can quickly index the
new spam. Google Cache[7] contains the time when Google
bot crawled the web page. Thus we randomly choose 100
active spam harbors in each category 3 and crawl their cache
pages every day. Figure 4 shows the distribution of Google
3Note that Google Cache has a request limitation per day. Thus we
only choose 100 harbors in this test. Also, Google does not cache all web
pages, so we choose those pages that are cached by Google
indexing interval.
rarely occur in the normal case, gives us a chance to study
the spamming behaviors of spammers. To characterize such
relationship, we build a relationship graph G = (V; E)
among these spam harbors. We view each spam harbor as
a node v and build up an edge e between two spam har-
bors if they share same spam (links) in their postings. The
resulting graph G for our entire database consists of 13 con-
nected sub-graphs, each of which has more than two nodes.
The largest connected component G0 contains 97 % spam
harbor domains.
Figure 4. Googe Indexing Interval
Compared with guestbook harbors, blog and forum
harbors have relatively shorter indexing intervals because
normal postings on them update much more frequently
than postings on guestbooks. Thus, Google bots crawl
blog/forum harbors much more frequently than guestbook
harbors. However, still nearly 80% of all harbors have an
indexing interval larger than 20 days, which indicates that
the overall indexing frequency is still not too high.
Lessons learned: Although high-reputation harbors
should be the best choice for spammers, high-reputation
websites usually have more strict spam ﬁltering mecha-
nisms or have stronger authentication systems, which makes
it harder for spammers to automatically spam on them.
Thus, spammers tend to keep using a large number of har-
bors for spamming regardless of their reputations to com-
pensate the relatively poor quality of individual harbors.
However, our observations also convey a positive message
to the defenders: since there typically exists a long time lag
between spamming time and search engine indexing time,
if we can detect these spam before search engines index
them, we can still efﬁciently prevent comment spamming
from impacting search ranks.
3.3 Spam Harbors Infrastructure
After ﬁnding out qualiﬁed spam harbors, spammers in-
tend to take full utilization of these harbors for spamming.
In this section, we study how spammers utilize these har-
bors for spamming, and what are the relationships that
spammers formed on these harbors.
3.3.1 Relationship Graph
To reduce the possibility of being detected, and also to take
full utilization of their spam harbors, spammers tend to dis-
tribute their spam among multiple spam harbors. Thus, dif-
ferent spam harbors may always share similar spam, be-
cause spammers intend to recycle these harbors. This spe-
cial close relationship among these harbors, which may
Figure 5. Relation Graph of Spam Harbors
Figure 5 is a partial visual representation of G0. We can
see that there exists a large number of communities within
G0, i.e., a group of nodes closely interconnected with each
other and only loosely interconnected with other commu-
nities. Here, each community represents a set of harbors
in close relationships with each other, possibly used by the
same spammer. In addition, although different spammers
may have different harbors, there always exist some harbors
shared by multiple spammers, which provides us a good op-
portunity to ﬁnd more other harbors (even starting from a
small seed set).
3.3.2 Spam Harbor Community
In the spamming process, spammers ﬁrst need to choose
spam harbors to post their spam, and then need to distribute
their spam on these selected harbors. In this section, we will
study how spammers choose their harbors, and how they
distribute spam to selected harbors.
Choosing Spam Harbors. In this part, we analyze how
spammers choose spam harbors, i.e., we examine how many
harbors are used for spamming each time. Some spammers
may spam on all their available harbors to fully use their
resources, and some may only sample parts of their harbors
for spamming to avoid the exposure/detection of all their
resources. To measure how spammers choose harbors each
time, we deﬁne a metric named “Distribution Ratio”, which
is the ratio of the number of harbors posting same spam
over the number of harbors in their community [3]. Thus,
a higher Distribution Ratio indicates that spammers tend to
fully use their spam harbors for spamming. Figure 6 shows
the distribution of Distribution Ratio for Sstudy.
Figure 6. Spam Distribution Ratio
We can see that 80% of spammers tend to use only less
than 50% of their spam harbors for the same spam. In this
way, they can reduce the possibility of all their resources
being detected/exposed. However, since spammers always
have a limited number of harbors, to keep spamming, they
have to recycle these harbors. As a result, we will ﬁnally
observe a relatively stable relationship among harbors.
Distributing Spam. After selecting spam harbors,
spammers need to decide how to distribute their spam to
these selected harbors. For example, some spammers may
post the same spam on their selected harbors at a similar
time.
In that case, posting time on these harbors should
be similar. Other advanced spammers may choose to dis-
tribute different spam on different selected harbors. In this
study, we simply consider two spam messages are posted in
a similar time if they are posted in the same month. To mea-
sure the similarity of spam posting time, we design a met-
ric, named “Time Centrality Ratio”, which is the ratio of the
maximal number of harbors that post the spam in the same
month over the total number of harbors that post this spam.
The intuition here is that if all the harbors post a spam mes-
sage in the same month, it is possible that this spam is dis-
tributed to all selected harbors. Otherwise, spammers will
distribute different spam to selected harbors. Thus, a high
Time Centrality Ratio indicates that spammers distribute the
same spam to most of their selected harbors at a similar time
(some example is shown in Appendix A). Figure 7 shows
the distribution of Time Centrality Ratio of Sstudy.
We can see that about 60% spam have a high ratio larger
Figure 7. Distribution of Time Centrality Ratio
than 0.6. This means that about 60% spam is distributed by
spammers to more than 60% of their selected harbors in one
month for spamming.
Lessons learned: To efﬁciently utilize spam harbors,
spammers intend to keep utilizing the spam harbors from a
relatively stable set (pool) that they own. Thus, essentially
spammers build an artiﬁcial relationship among these spam
harbors, which is considered as their spamming structure.
In addition, since spammers have a limited number of har-
bors, they must use/recycle these harbors with a large scale
of spamming to maximize their proﬁts. Also, although dif-
ferent spammers may have different strategies to ﬁnd their
harbors, there always exist some intersections among them,
which gives us a chance to ﬁnd more other spam communi-
ties even starting from a small seed set.
4 Inference System Design
In this section, we present a brief overview of our in-
ference system, then describe in details its two core com-
ponents: building spamming infrastructure graph and spam
inference.
4.1 Overview
From the measurement study in Section 3, we can see
that if a link (in a comment) is posted on a set of harbors
that have a close relationship (e.g., within the same spam
community in the infrastructure graph) at a similar time, it
has a high possibility to be spam. Following this intuition,
we design a spam inference system, named NEIGHBOR-
WATCHER. NEIGHBORWATCHER infers comment spam
purely based on the links promoted in the postings, ignor-
ing other content information. An overview of NEIGH-
Figure 8. System Architecture of NEIGHBORWATCHER
Figure 9. Normalized Neighborhood Relationship Matrix
BORWATCHER is shown in Figure 8. In practice, NEIGH-
BORWATCHER keeps monitoring (and updating) spam har-
bors in our database and builds the spamming infrastruc-
ture graph based on their posting history. In the inference
phase, when a new post is given, NEIGHBORWATCHER
extracts the embedded links, and also ﬁnds out the web-
sites that have been posted with the same links (we call the
set of these websites as a “real posting structure”). Based
on the spam infrastructure and the real posting structure,
NEIGHBORWATCHER calculates a suspicious score to tell
how likely this is spam. Next, we will describe the algo-
rithms of building our spamming infrastructure graph and
inferring comment spam.
4.2 Building Spamming Infrastructure Graph
From Section 3, we know that spammers always have
their own preferred spam harbors, and they intend to keep
utilizing these spam harbors for spamming. Thus if multi-
ple harbors always share similar postings in their history, it
should be a good indication that they are exploited by the
same spammer for spamming, and also have a high proba-
bility to be spammed by the same spammer in future. In this
case, if we ﬁnd a new posting occurs on these harbors at a
similar time, we could infer this posting as spam with a rea-
sonably high conﬁdence. Following this intuition, we build
spammers’ spamming infrastructure based on shared post-
ings (in historic data) among these spam harbors. We deﬁne
the spamming infrastructure (or sometime we simply use
“spamming structure” to denote the same concept) as neigh-
borhood relationships among spam harbors. Thus, spam
harbors spammed by the same spammers should have close
neighborhood relationships, because they always share sim-
ilar spam postings in history. To quantify such relationships,
given an input harbor, we calculate neighborhood scores for
all other spam harbors. A higher neighborhood score of a
harbor indicates a much closer neighborhood relationship
with the given (input) harbor.
To formalize the above intuition, we view all neighbor
relationships among spam harbors as a weighted undirected
graph G = (V; E), in which V denotes the set of all spam
harbors, and each link (i; j) ∈ E denotes that harbor vi and
harbor vj share at least one common posting. The weight
on the edge should reﬂect the strength of the relationship
between two harbors. In our case, let Li be the set of post-
∩
ings (represented with their embedded URLs) in node vi
and Lj be the set of postings in node vj, then we deﬁne
∑
weight wi;j as |Li
Lj|. Thus, the more postings two har-
bors share, the much closer they are. We further normalize
wi;j by dividing
j wi;j as shown in Figure 9.
n
Next we design a graph-based propagation algorithm to
propagate neighborhood score from the input harbor(s) to
its neighbors based the neighborhood relationship graph G.
Table 3 shows the notations used in our algorithms.
Before propagation, we ﬁrst assign an initial score Ii to
each node Vi. For the input harbor j, Ij is assigned with 1,
and others are assigned with 0. Then we calculate neigh-
borhood scores for all harbors as follows:
N = I · W
(1)
Eq.(1) can capture the immediate neighbors of the input
harbor. In this case, each immediate neighbor is assigned
with a neighborhood score based on the number of common
postings shared with the input harbor. The more common
postings they share, the higher score they should have. As
shown in Figure 9, node 3 has a higher score than node 1,
Table 3. Notations Used in Our Paper
Input harbor vector, Ii = 1 if i is a input harbor
W Normalized adjacency matrix of the neighbor graph
I
N Neighbor score vector. Ni is the neighbor score of harbor i
R
(cid:11) Dampen factor. (cid:11) = 0:85
n
The number of spam harbors
Real spam posting vector. Ri = 1 if harbor i posts the same input link
because node 3 shares more common postings with the in-
put harbor node 1 in history. However, as we show in Sec-
tion 3, spammers might not always spam on all their harbors
for each message, and our observed history relationships
may be only a subset of the spammers’ real relationships.
To illustrate this scenario and demonstrate how we handle
this problem in a generalized way, we show a case study in
Figure 10.
Figure 10. A Case Study of Comment Spam(cid:173)
ming on Different Subsets of Harbors
For this example, in the spamming process, a spammer
ﬁrst spams on node 1,2,3 for one spam message. And then
the spammer spams on node 2,4,5 and node 3,5,6 with dif-
ferent spam messages. The neighborhood graph based on
the history information is shown in solid circles. Now if
the spammer spams on node 1, 4, 5, 6 (as seen in dashed
circles), applying Eq.(1) with node 1 as the input harbor
will assign score 0 to node 4, 5, 6, because they are not di-
rectly connected with the input harbor node 1. This makes