vast majority of trafﬁc would migrate over to the new address rela-
tively quickly, (2) the only remaining trafﬁc would be due to faulty
servers or botnets, and thus (3) a few months of running the old
address would sufﬁce.1 However, the changeover has not been as
rapid in practice: 63% of sources do not switch over after four
months (by May 2, 2013), and they generate 4,721 queries per sec-
ond on average.
In this section, we investigate why, after several months, the old
address continues to see such a high volume of queries. We do
not believe there to be a single reason for this. Instead, we seek
to classify name servers among several groups, and to identify the
likely causes among these groups.
5.1 Classifying resolvers
We begin by investigating the types of access behaviors resolvers
exhibit towards the two addresses. So as not to confuse hosts who
never change to the new address from those who have not yet changed,
we focus on data collected in April and May 2013, three to four
months after turning on the new IP address (and well beyond the
41 day TTL). Our hypothesis was that, after this much time, there
would be very few hosts on the old address, most on the new ad-
dress, and few to none who regularly contact both, as that would be
incorrect behavior.
The results in Figure 7 tell another story. In this ﬁgure, we plot
for each source viewed in our April/May data the difference be-
tween the fraction of queries it issued at the old address and the
fraction it issued at the new. A resolver who contacted only the old
address has a value of 1; a resolver who contacted only the new has
value −1; a value of 0 corresponds to an equal amount of queries
1L-root had similar expectations, and in fact relinquished the old
IP address after six months, which was “hijacked” soon after [5].
10-410-310-210-111010210-410-310-210-1110102QPS on Day AfterQPS on Day Before1x2x10x100x1001K10K100K1M(because they issue the same small set of queries constantly), and
some always failed (because they issued seemingly random strings,
or incorrectly conﬁgured their internal root DNS servers and issued
many queries for a TLD of local or internal). With a correctly
implemented cache, one would expect high failure rates at the root
(as the root knows of relatively few, long-TTL TLDs).
The second feature we measure a host against is its query diver-
sity: the number of unique queries (in terms of domain name) it
issues divided by the total number of queries it issues. Query diver-
sity takes a value in (0,1], and with correctly implemented negative
and positive caches, one would expect diversity to be high.
We present plots for all three types in Figure 8. We use the April–
May dataset for classifying hosts as barnacles and swappers, and
the more detailed January dataset for calculating their failure rates,
query diversities, and query volumes.
Normals. As expected, the normals exhibit high failure rates and
high diversity, forming a large cluster in the upper right corner of
Figure 8(c). It is important to note, however, that a high failure
rate and high query diversity does not necessarily imply correct
behavior. For instance, 47% of the queries from a subset of the
Google public DNS resolvers contain random strings (thus have
high diversity) and include a TLD of internal (and thus have
a high failure rate). This is likely caused by a misconﬁguration of
some of Google’s name servers meant to serve a private namespace.
Barnacles. The vast majority of barnacles (Fig. 8(a)) exhibit low
failure rates, suggesting that these are resolvers that are misconﬁg-
ured, used for measurement, or used for attack. Disproportionately
low failure rates from hosts who never query the new address are a
viable explanation for the anomaly wherein the old address experi-
ences a lower failure rate than the new (Q3).
Among the barnacles, we see resolvers that continually query
for: (1) lists of known name servers (likely measurements), (2) DNS-
BLs (spambots or attacks), or (3) very small sets of names (embed-
ded devices that implement their own resolver looking for updates
and patches). We believe all of the barnacles with low error rates
(<10%) are pieces of software issuing mechanized queries using
incorrect DNS implementations. Moreover, we believe that barna-
cles with very low query diversity (wherein the resolver asks for the
same name repeatedly) can be speciﬁcally attributed to misconﬁgu-
rations or hard-coded attack software that does not have the facility
for handling address changes.
There are a few very high volume barnacles that have approxi-
mately 30% query diversity and 50% failures. These are OpenDNS
resolvers. Because OpenDNS resolvers are publicly available, we
Figure 7: Fraction of sources with various query patterns to the
old and new IP addresses. A query ratio of 1 or -1 corresponds to
sources who only send to the old or new IP address, respectively.
to old and new, and so on. From this we see that of the top 10%
of the hosts by volume (who constitute roughly 80% of the overall
volume), a mere 32% contact the new address only. Approximately
40% of the resolvers we saw in April/May contact the old only. We
refer to these hosts who latch onto the old address as barnacles.
This leaves a remainder of about 28% who swap between both
old and new at varying rates—we refer to these as swappers. We
investigated the rate at which these hosts swap (data not plotted).
The majority do so infrequently (roughly once every 20 queries),
though some (about 2%) swap at least as fast as every two queries
they issue.
The rest of this section analyzes those who continue to query the
old address after several months: the barnacles and the swappers.
As a point of comparison, we also include a baseline of hosts which
we refer to as normals: sources we expect to act in a reasonably
correct manner. Speciﬁcally, a set of large ISP nameservers con-
stitutes our “normals”: Verizon DSL, Verizon HS, QWEST, COX,
and Speakeasy/Megapath (business only).
5.2 Classifying root causes
For each host from each of the three classes of resolvers (bar-
nacles, swappers, and normals), we measure it along two features.
The ﬁrst is the fraction of failed queries the host issues (includ-
ing those that issue NXDOMAIN responses as well as malformed
queries). The rationale behind this feature comes from a manual
investigation of the data; we identiﬁed that some hosts never failed
(a) Barnacles
(b) Swappers
(c) Normals
Figure 8: Diversity of domain names in queries versus the fraction of failed queries for the three types of resolvers. Each circle represents a
source /24, where the area is a function of the query volume.
-1-0.5 0 0.5 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Query Ratio:Fraction of SourcesAll SourcesTop 10% by Volume Old + NewOld – New 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1Query Domain Name DiversityFraction of Failed Queries 0 1CDF 0 1CDF 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1Query Domain Name DiversityFraction of Failed Queries 0 1CDF 0 1CDF 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1Query Domain Name DiversityFraction of Failed Queries 0 1CDF 0 1CDFconjecture that their behavior is a composite of normal queries, at-
tacks, and misconﬁgurations.
Swappers.
Finally, swappers (Fig. 8(b)) exhibit a nearly uni-
form distribution of failures (note the linear CDF at the top of the
plot). We believe that the very low error rate queries in this set also
represent mechanized bots, as do the very low diversity queries.
Moreover, there is a positive correlation between failure rates and
diversity. We believe the bulk of these (relatively low volume) re-
solvers simply use both addresses interchangeably. We are in the
process of ﬁngerprinting each of these resolvers and with the hope
of mapping the ones that respond to known implementations that
prime incorrectly and use all known addresses.
6. DISCUSSION AND OPEN QUESTIONS
Many signs of excitables point to PowerDNS: it is most popular
in Europe (where the majority of excitables come from), we have
identiﬁed that some versions include both of D-root’s IP addresses
in its list of root name servers, and its root selection algorithm can
send all queries to a single name server. However, our runs of
fpdns have turned up only a single PowerDNS resolver running
an old version of the software. fpdns is known to be unable to
identify PowerDNS; further, the vast majority of the European re-
solvers are conﬁgured to not answer external queries. Validating
our hypothesis—either by improving fpdns or by simply running
a PowerDNS resolver from multiple vantage points (with different
RTTs to the new and old addresses)—is an area of ongoing work.
Our analysis of why name servers continue to visit the old ad-
dress identiﬁes many examples of what appear to be misconﬁgura-
tions, buggy code, or scanners. For example, many queries from
these barnacles are redundant yet they infrequently fail; among
these, we have identiﬁed resolvers that scan DNSBLs. One ques-
tion is: do these bugs and behaviors occur at smaller scales for
the less-concentrated TLD servers or even others, perhaps creating
similarly inexplicable ﬂuctuations in trafﬁc volume? Answering
this question may provide greater conﬁdence that barnacle’s redun-
dant queries account for the large difference in query success rates
between the old and new IP addresses.
Answering these questions may require old-fashioned footwork:
getting in touch with operators. One promising, potential outcome
of this is that a DNS root server IP address change may be able
to assist in identifying and raising awareness about bugs, common
misconﬁguration errors, and possibly attacks.
Indeed, perhaps changing root DNS IP addresses should be done
every so often as a matter of regular practice! So doing would
possibly encourage operators to run serviceably recent versions of
BIND or PowerDNS and discourage hard-coding. In the long run—
and if our hypothesis is correct that many of the barnacles are buggy
or forgotten code—then the occasional address change could serve
as a crude form of garbage collection.
Acknowledgments
We thank Xiehua Li from Hunan University, the anonymous re-
viewers, and our shepherd, Mark Allman, for their helpful com-
ments on the paper. We also thank James Litton for his help in
analyzing the PowerDNS code. This work was supported in part
by NSF Awards CNS-0917098, IIS-0964541, and CNS-1255314.
7. REFERENCES
[1] R. Arends and J. Schlyter. fpdns.
https://github.com/kirei/fpdns.
[2] P. Barber, M. Larson, M. Kosters, and P. Toscano. Life and
Times of J-Root. In NANOG32, Oct 2004.
[3] N. Brownlee, K. Claffy, and E. Nemeth. DNS Measurements
at a Root Server. In IEEE Global Communications
Conference (GLOBECOM), 2001.
[4] S. Castro, D. Wessels, M. Fomenkov, and K. Claffy. A Day
at the Root of the Internet. ACM SIGCOMM Computer
Communication Review (CCR), 38(5):41–46, 2008.
[5] D. Conrad. Ghosts of Root Servers Past.
http://blog.icann.org/2008/05/
ghosts-of-root-servers-past/.
[6] A. Cowperthwaite and A. Somayaji. The Futility of DNSSec.
In Annual Symposium Information Assurance (ASIA), 2010.
[7] P. Danzig, K. Obraczka, and A. Kumar. An Analysis of
Wide-Area Name Server Trafﬁc: A Study of the Internet
Domain Name System. In SIGCOMM Conference on Data
Communication, 1992.
[8] T. Hardie. Distributing Authoritative Nameservers via
Shared Unicast Addresses. RFC 3258, Apr 2002.
[9] Internet Systems Consortium. BIND.
https://www.isc.org/software/bind.
[10] B. Manning. Persistent Queries and Phantom Nameservers.
In CAIDA-WIDE Workshop, 2006.
[11] C. Partridge, T. Mendez, and W. Milliken. Host Anycasting
Service. RFC 1546, Nov 1993.
[12] PowerDNS Technologies. https://www.powerdns.
com/resources/PowerDNSTechnologies.pdf.
[13] D. Wessels and M. Fomenkov. Wow, That’s a Lot of Packets.
In Passive and Active Network Measurement Workshop
(PAM), 2003.
[14] Y. Yu, D. Wessels, M. Larson, and L. Zhang. Authority
Server Selection in DNS Caching Resolvers. ACM
SIGCOMM Computer Communication Review (CCR),
42(2):80–86, 2012.