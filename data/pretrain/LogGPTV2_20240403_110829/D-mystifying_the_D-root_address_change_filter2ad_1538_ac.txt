### Traffic Migration and Address Changeover

It was initially expected that the majority of traffic would quickly migrate to the new address, with only a small amount of residual traffic from faulty servers or botnets. Consequently, it was believed that running the old address for a few months would be sufficient. However, in practice, the transition has been much slower: 63% of sources did not switch over after four months (by May 2, 2013), generating an average of 4,721 queries per second.

### Investigating Persistent Traffic on the Old Address

In this section, we explore why, even after several months, the old address continues to receive a high volume of queries. We do not believe there is a single reason for this. Instead, we aim to classify name servers into several groups and identify the likely causes within these groups.

#### 5.1 Classifying Resolvers

We begin by examining the types of access behaviors resolvers exhibit towards the two addresses. To avoid confusion between hosts that never change to the new address and those that have not yet changed, we focus on data collected in April and May 2013, three to four months after the new IP address was activated (well beyond the 41-day TTL). Our hypothesis was that, after this period, there would be very few hosts still using the old address, with most having switched to the new address, and almost none regularly contacting both, as this would indicate incorrect behavior.

The results, however, tell a different story. In Figure 7, we plot the difference between the fraction of queries issued to the old address and the fraction issued to the new address for each source in our April/May data. A resolver that contacted only the old address has a value of 1; one that contacted only the new address has a value of -1; and a value of 0 indicates an equal number of queries to both addresses.

From this, we see that of the top 10% of hosts by volume (which constitute roughly 80% of the overall volume), only 32% contact the new address exclusively. Approximately 40% of the resolvers we observed in April/May continue to contact the old address only. We refer to these hosts as "barnacles." The remaining 28% swap between the old and new addresses at varying rates, which we call "swappers." We investigated the rate at which these hosts swap (data not plotted). The majority do so infrequently (about once every 20 queries), though some (about 2%) swap at least as fast as every two queries they issue.

### Analysis of Barnacles and Swappers

The rest of this section analyzes the barnacles and swappers who continue to query the old address after several months. For comparison, we also include a baseline of hosts we refer to as "normals": sources that we expect to behave correctly. Specifically, a set of large ISP nameservers, including Verizon DSL, Verizon HS, QWEST, COX, and Speakeasy/Megapath (business only), constitutes our "normals."

#### 5.2 Classifying Root Causes

For each host in the three classes of resolvers (barnacles, swappers, and normals), we measure two features:
1. **Failure Rate**: The fraction of failed queries, including those that issue NXDOMAIN responses and malformed queries.
2. **Query Diversity**: The number of unique queries (in terms of domain name) divided by the total number of queries. Query diversity takes a value in (0,1], and with correctly implemented negative and positive caches, one would expect diversity to be high.

**Normals**: As expected, the normals exhibit high failure rates and high diversity, forming a large cluster in the upper right corner of Figure 8(c). It is important to note, however, that a high failure rate and high query diversity do not necessarily imply correct behavior. For instance, 47% of the queries from a subset of Google's public DNS resolvers contain random strings (thus have high diversity) and include a TLD of internal (and thus have a high failure rate). This is likely due to a misconfiguration of some of Google's name servers meant to serve a private namespace.

**Barnacles**: The vast majority of barnacles (Figure 8(a)) exhibit low failure rates, suggesting that these are resolvers that are misconfigured, used for measurement, or used for attacks. Disproportionately low failure rates from hosts that never query the new address provide a viable explanation for the anomaly wherein the old address experiences a lower failure rate than the new (Q3).

Among the barnacles, we observe resolvers that continually query for:
- Lists of known name servers (likely measurements),
- DNSBLs (spambots or attacks),
- Very small sets of names (embedded devices implementing their own resolver looking for updates and patches).

We believe all barnacles with low error rates (<10%) are pieces of software issuing mechanized queries using incorrect DNS implementations. Moreover, barnacles with very low query diversity (repeatedly asking for the same name) can be specifically attributed to misconfigurations or hard-coded attack software that does not handle address changes.

There are a few high-volume barnacles with approximately 30% query diversity and 50% failures. These are OpenDNS resolvers. Because OpenDNS resolvers are publicly available, we can further investigate them.

**Swappers**: Finally, swappers (Figure 8(b)) exhibit a nearly uniform distribution of failures (note the linear CDF at the top of the plot). We believe that the very low error rate queries in this set also represent mechanized bots, as do the very low diversity queries. There is a positive correlation between failure rates and diversity. We believe the bulk of these (relatively low volume) resolvers simply use both addresses interchangeably. We are in the process of fingerprinting each of these resolvers to map those that respond to known implementations that prime incorrectly and use all known addresses.

### Discussion and Open Questions

Many signs point to PowerDNS as a significant factor: it is most popular in Europe (where the majority of excitables come from), and some versions include both of D-root’s IP addresses in its list of root name servers. Its root selection algorithm can send all queries to a single name server. However, our runs of fpdns have identified only a single PowerDNS resolver running an old version of the software. Further, the vast majority of European resolvers are configured to not answer external queries. Validating our hypothesis—either by improving fpdns or by running a PowerDNS resolver from multiple vantage points (with different RTTs to the new and old addresses)—is an area of ongoing work.

Our analysis identifies many examples of what appear to be misconfigurations, buggy code, or scanners. One question is whether these bugs and behaviors occur at smaller scales for less-concentrated TLD servers, potentially creating similar inexplicable fluctuations in traffic volume. Answering this question may provide greater confidence that barnacle’s redundant queries account for the large difference in query success rates between the old and new IP addresses.

Addressing these questions may require direct communication with operators. One potential outcome is that changing root DNS IP addresses could help identify and raise awareness about bugs, common misconfigurations, and possible attacks. Regularly changing root DNS IP addresses might encourage operators to run more recent versions of BIND or PowerDNS and discourage hard-coding. If our hypothesis is correct, occasional address changes could serve as a crude form of garbage collection.

### Acknowledgments

We thank Xiehua Li from Hunan University, the anonymous reviewers, and our shepherd, Mark Allman, for their helpful comments on the paper. We also thank James Litton for his help in analyzing the PowerDNS code. This work was supported in part by NSF Awards CNS-0917098, IIS-0964541, and CNS-1255314.

### References

[1] R. Arends and J. Schlyter. fpdns. https://github.com/kirei/fpdns.
[2] P. Barber, M. Larson, M. Kosters, and P. Toscano. Life and Times of J-Root. In NANOG32, Oct 2004.
[3] N. Brownlee, K. Claffy, and E. Nemeth. DNS Measurements at a Root Server. In IEEE Global Communications Conference (GLOBECOM), 2001.
[4] S. Castro, D. Wessels, M. Fomenkov, and K. Claffy. A Day at the Root of the Internet. ACM SIGCOMM Computer Communication Review (CCR), 38(5):41–46, 2008.
[5] D. Conrad. Ghosts of Root Servers Past. http://blog.icann.org/2008/05/ghosts-of-root-servers-past/.
[6] A. Cowperthwaite and A. Somayaji. The Futility of DNSSec. In Annual Symposium Information Assurance (ASIA), 2010.
[7] P. Danzig, K. Obraczka, and A. Kumar. An Analysis of Wide-Area Name Server Traffic: A Study of the Internet Domain Name System. In SIGCOMM Conference on Data Communication, 1992.
[8] T. Hardie. Distributing Authoritative Nameservers via Shared Unicast Addresses. RFC 3258, Apr 2002.
[9] Internet Systems Consortium. BIND. https://www.isc.org/software/bind.
[10] B. Manning. Persistent Queries and Phantom Nameservers. In CAIDA-WIDE Workshop, 2006.
[11] C. Partridge, T. Mendez, and W. Milliken. Host Anycasting Service. RFC 1546, Nov 1993.
[12] PowerDNS Technologies. https://www.powerdns.com/resources/PowerDNSTechnologies.pdf.
[13] D. Wessels and M. Fomenkov. Wow, That’s a Lot of Packets. In Passive and Active Network Measurement Workshop (PAM), 2003.
[14] Y. Yu, D. Wessels, M. Larson, and L. Zhang. Authority Server Selection in DNS Caching Resolvers. ACM SIGCOMM Computer Communication Review (CCR), 42(2):80–86, 2012.