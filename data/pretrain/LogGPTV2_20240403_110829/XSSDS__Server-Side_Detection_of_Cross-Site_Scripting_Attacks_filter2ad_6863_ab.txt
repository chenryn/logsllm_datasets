script’s code and position (see below), but the absolute num-
ber of unique scripts is necessarily limited by the number of
server-side code-fragments in the actual application which
inserts JavaScript-code into the ﬁnal HTML, and the num-
ber of possible script templates used by such fragments.
Thus, we can build a mechanism to keep track of all used
JavaScripts within a given web application and detect vari-
ances in the application’s script-set. Such a variance is a
strong indicator for either an update of the actual web appli-
cation’s code (which should be known by the application’s
administrator) or a successful XSS attack. We do propose to
use a training based XSS detector: After a training phase,
which consists of building up the detector’s list of known
scripts, all outgoing scripts are checked against this list.
If an unlisted script is encountered, the detector raises an
alarm.
4.2. Script types
Whilst this concept is appealingly simple, scripts do vary
both in code and location within a webpage in reality. We
therefore need a representation in which the script-set can
be considered static for most dynamic web applications and
distinguish between two general types of scripts:
1. Static scripts: JavaScripts that never change. Such
scripts are usually contained in the application’s
HTML templates or they are part of supporting
JavaScript libraries.
338338
2. Dynamic scripts: Such JavaScripts are composed dur-
ing runtime; i.e.: the state of the application or the sub-
mitted parameters inﬂuence the script’s ﬁnal code. In
many cases input parameters are used as the value of
string-constants within dynamic scripts (see Listing 3).
While training our detector to recognise static scripts is
straightforward, dynamic scripts require more attention.
4.3. Dealing with dynamic scripts
In a manual analysis of dynamic scripts, we were able
to identify two subtypes: Firstly dynamic scripts that solely
vary in the values of constants, and second dynamic scripts
that also expose variations in the script’s actual code.
Dynamic scripts with varying constants: We often ob-
served a striking resemblance in the code of different occur-
rences of the same dynamic script. The code was in most
cases completely static apart from its string constants (as ex-
empliﬁed in Listing 3). As constants are often used to pass
data to scripts, our representation should also allow them to
change.
1 echo "";
Listing 3. Example of a dynamic script
This is taken into account by tokenizing each script
and substituting all constant-tokens by generic representa-
tions. The string-tokens thus are substituted by STRING,
numeric constants by NUMERIC and regular expressions
by REGEXP. The resulting scripts are static for all possi-
ble variants and therefore usable for our matching algorithm
(see Listing 4).
1 
Listing 4. Normalized version of Listing 3
Dynamic scripts with varying code blocks: The sec-
ond subclass of dynamic scripts we encountered consisted
of JavaScript-blocks which also revealed a variance in the
script’s code portion. While each of these respective scripts
were apparently generated by the same source (indicated
through position, purpose, and form of the JavaScript), they
contained at least one dynamic code block. An analysis of
these scripts revealed two variants of this particular script-
type:
• Code repetition: These JavaScripts included blocks of
repeated code which apparently was created by loops.
(see Listing 5).
• Selective code omission: In some cases –depending on
the state of the web application– a given JavaScript can
contain or omit blocks of script code. E.g., according
to the authorizations of the logged-in user, a dynamic
JavaScript-driven dialogue might contain varying op-
tions.
1 
Listing 5. Repeated code in dependence on
the number of values
Each of these cases is not a problem for our approach.
As the number of possible variants of such scripts in tok-
enized form is bounded, we treat every possible variant as
an individual script; thus, it is added to the list of known
scripts. This solution increases the total number of known
scripts only and does not introduce further problems. How-
ever, in future work we intend to investigate also methods
for detecting and normalizing cases of code repetition.
4.4. Training
As explained above, the detector’s training phase con-
sists of building a list of known JavaScripts. Besides the
actual, tokenized script-code we also recorded the type of
script (i.e.: inline script, event handler, or external script),
its position within the HTML page (head or body), and,
if applicable, the event-name (for event handlers). The suc-
cess of this training phase depends on the degree of appli-
cation coverage which is achieved: note, that the detector
is only able to analyse HTML output which was generated
by actual usage of the given application. We do, however,
not consider this as a drawback: Our detector is positioned
at the server side during actual use. Therefore, it receives
the complete HTML output of all usages of the applica-
tion. Even with large applications, it is reasonable to as-
sume that every major variation of every possible script is
included in at least one visited page. As every script has
to be seen only once by the detector, the training phase can
be completed in reasonable time (see Sec. 6 for our practi-
cal evaluation of this). To ensure that the training-set is in
fact attack-free, the actual training can be done during the
application development, quality assurance testing, and, if
applicable, the closed beta-phase (a common practice intro-
duced by Web2.0 applications).
External scripts: External scripts need to be handled
differently, since the actual code of these scripts is not pro-
cessed by the detector. We build a list of known domains,
containing the top-level- and ﬁrst-level domain name as
well as the protocol and port using the URLs of external
339339
scripts encountered during the training phase. Afterwards,
external scripts are legitimate if and only if they originate
from one of these domains. This policy allows the use of an
external script-server under a different domain (as practiced
by many, e.g. eBay). However, we assume that an adversary
is not able to place a malicious script on one of the trusted
servers.
Extending the list of legitimate scripts after applica-
tion updates: Some web applications are rapidly updated
without extensive internal testing. If in such a case the ap-
plication’s script-set is altered, the detector’s list of known
scripts has to be extended during actual production use. To
ensure that the list remains attack-free, it is advisable that
every addition to the list is manually conﬁrmed. This is fea-
sible since after applying our normalization and tokeniza-
tion steps the absolute number of known scripts will be
rather limited. During our practical evaluation (see Sec. 6)
we observed even for large applications an average script-
set size between 50 and 300 scripts. Under the assump-
tion that any given update touches approximately 10% of all
scripts, a total of 5 to 30 new entries would have to be man-
ually conﬁrmed. Furthermore, during such phases our other
detection approach continues to work, so it will presumably
detect all reﬂected XSS ﬂaws before they may enter the list
of learned scripts.
4.5. Detection
After the training phase has been completed, the detec-
tion of unknown scripts consists solely of verifying that a
given script is not contained in the detectors training-set.
Our test implementation (see Sec. 5) achieved this with
simple hash-table lookups using the tokenized scripts as
hashkeys. Moreover, it is veriﬁed that the other recorded
characteristics, such as script type and position, also match.
As explained above, every unknown script then indicates
either a successful XSS attack, or an update of the applica-
tion. The latter case has to be handled by the web applica-
tion’s operator: The operator must either restart the detec-
tor’s training period, or manually conﬁrm each addition to
the trained list as described above.
5. Implementation
We successfully implemented prototypes for both our
detection approaches. Besides the implementation of the
core algorithms, we encountered two tasks that required
special attention: Script extraction and script parsing.
5.1. Reliable script extraction
It is crucial to the success of both our detection ap-
proaches that a given implementation can reliably iden-
tify all JavaScripts in a web page. As web browsers
tend to implement a forgiving policy towards rendering il-
legitimate HTML, deterministically identifying all scripts
is a very non-trivial task (see [8]). Therefore,
instead
of manually implementing an HTML parser, we modiﬁed
the HTML rendering engine of the popular open-source
browser Mozilla Firefox. This ensures that the set of found
scripts exactly matches the scripts that would be executed
by the browser.
Client-side code creation: JavaScript provides several
techniques for on-the-ﬂy creation of executable script code
from string values (e.g., using eval). This can result in
XSS vulnerabilities if an adversary is able to inject code
into the respective string constants (see Listing 6). In such
cases, the injected code is contained in a string-constant of
an otherwise legitimate script. As such string-constants are
ignored by both approaches because of the respective tok-
enization steps (see Sec. 3.5 and 4.3), the script-extraction
step has to pay special attention to encountered string-
constants: All found JavaScript string-constants are exam-
ined for valid script code using our JavaScript-parser. Every
JavaScript that is found this way is added as a separate, in-
dividual script to the result-set of the extraction component.
1 // XSS condition if the attacker controls ’some_var’
2 eval(some_var);
Listing 6. Dynamic client-side code creation
Our implementation was tested on a number of tricky
and obfuscated HTML-based attack vectors from [8], and it
proofed to be able to reliably detect all of them. It should be
noted though, that our current implementation does not de-
tect scripts in any other content than HTML. However, this
is not a conceptual shortcoming, but it is due to the proof-
of-concept nature of our implementation: It can easily be
extended by other parsers (Flash, SVG, etc.).
As some XSS attack vectors are browser-speciﬁc, secur-
ing a web application should ideally involve one instance of
each supported browser for parsing and merge the resulting
lists based on the script’s location within the page. We leave
this task as future work.
5.2. Script parsing
Both of our detectors rely upon a JavaScript tokenizer for
preprocessing. Our implementation was written in Ruby, so
we opted for rbNarcissus [22], a Ruby port of the Mozilla
Project’s JavaScript parser. The tokenization step in the
generic XSS detector, however, turned out to be the most
signiﬁcant performance bottleneck. So we strongly suggest
that any practical implementation uses a solution optimized
for performance.
340340
6. Evaluation
6.1. Dataset and experiment set-up
First, we used a crawler to collect about 500.000 HTTP-
request/response pairs from 95 popular web applications,
thus simulating web application usage. This dataset in-
cludes high proﬁle sites like myspace.com, blogger.
com, and news.google.com; the crawling process was
manually optimized to ensure page diversity. This data was
used as a basis for evaluating our approach; the actual eval-
uation was done ofﬂine, by passing the dataset’s contents
and the attack data (see below) to our PoC implementation.
6.2. Methodology
Generally, any attack detection system should have two
major capabilities: detecting as many attacks as possible
whilst having a false-positive rate as low as possible. We
assessed the detection abilities of both approaches by ap-
plying them to
• crafted attacks injected into otherwise benign data and
• real-world attack data of disclosed XSS problems.
Furthermore, we measured the false-positive rate by apply-
ing the detectors to our collected dataset which we assumed
contained no attacks. This time every alarm was counted
as a false-positive and logged. Afterwards all alarms were
reviewed by hand to make sure there really were no attacks
in the data. The error-rates were divided by the number of
pages used for testing in order to remove the inﬂuence of
different web application sizes.
6.3. Generic XSS detector evaluation
Since our generic XSS detector needs to be trained for
meaningful results, we will use a common statistical prac-
tice called k-fold cross-validation (for k = 10): The data is
partitioned into k = 10 subsamples, k − 1 = 9 of which are
used for training and the one remaining sample is retained
for testing. This process is repeated a total of k=10 times,
retaining each of the samples exactly once.
Detection evaluation (manual script injection): As
malicious scripts we used a common credential stealing at-
tack (see Listing 7) as well as an obfuscated version using a
method posted by a member called ”.mario” in the sla.ckers
forum [16] (see Listing 8). Both variants were injected
• as inline scripts (...),
• as event handlers into random suiting HTML-nodes
• into existing scripts either directly behind string
(onload="..."), and
constants or appended at the end.
External scripts (e.g., http://hack0r.net/xss.js)
were used as well, yielding a satisfactory number of com-
binations spanning all major types of XSS occurrences. We
relied upon the cross-validation-method for every applica-
tion in our dataset. During the training phase, every script
found in the training-samples was added to the detector’s
list of legitimate scripts. While testing, a random number
(1-5) of randomly picked malicious scripts was injected into
each page in the test-sample before applying the detector.
The number of injected scripts not causing an alarm indi-
cates false-negatives.
1 c = escape(document.cookie);
2 document.write("");
Listing 7. Cookie stealing XSS payload
1 top/**/[’\x65\x76\x61\x6c’]/**/(’\x73\x74\x72\x20\x65
2 [...]
3 \x73\x74\x72\x20\x2b\x20\x22\x27\x3e\x22\x20\x29\x3b’)
Listing 8. Obfuscated JavaScript [16]
Detection evaluation (real-world vulnerabilities):
Since we wanted to gain insight into the practical usefull-
ness of our detection approach, we set up three different
web applications, each in a version known to be vulnera-
ble to stored XSS attacks ([2], [19], [13]). We trained our
generic XSS detector with non-compromised versions of
the vulnerable pages, before we exploited each vulnerabil-
ity to inject some malicious script. Afterwards the detector
was applied to the now infected pages and every injected
script not causing an alarm was counted as a false-negative.
False-positives: To measure the false-positive-rate, we
again used the cross-validation-method. This time, unal-
tered pages were used both for training and for testing.
Every alarm during the testing-phase was counted as a
false-positive in the respective web application. The false-
positive-rate was divided by the number of pages in the re-
spective test-sample and the alarms were reviewed for real
attacks.
Results: The generic XSS detector was able to reliably
identify all injected attacks as well as the real-life examples,
resulting in a false-negative rate of 0 for all tested web ap-
plications. Furthermore, about 80% of the web applications
also did not encounter any false-positives. These web appli-
cations either solely utilized static JavaScripts or employed
dynamic code generation techniques which are recognised
by our detector (see Sec. 4.3). The remaining 20% caused
varying amounts of false alarms (see Fig. 7) due to non-
trivial, dynamic code-generation techniques which are not
yet handled by our detector, such as dynamic generation of
variable names. However, most of these issues can be re-
solved easily by a custom normalization step (e.g. by map-
ping dynamic variable names to a special token type). Rare
cases (0.2%) caused a considerable amount of false alarms.
The false-positive-rate in such cases was about 7 alarms per