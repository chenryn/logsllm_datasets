5.4  Multi-Naive Bayes 
The next data mining algorithm we describe is Multi-Naive 
Bayes. This algorithm was essentially a collection of Naive 
Bayes algorithms that voted on an overall classification for 
an  example.  Each  Naive  Bayes  algorithm  classified  the 
examples  in  the  test  set  as  malicious  or  benign  and  this 
counted as a vote. The votes were combined by the Multi- 
Naive Bayes algorithm to output a final classification for all 
the Naive Bayes. 
This method was required because even using a machine 
with one gigabyte of RAM, the size of the binary data was 
too large to fit  into memory.  The Naive  Bayes  algorithm 
required a table of all  strings or bytes to compute its prob- 
abilities.  To correct this problem  we divided the problem 
into smaller pieces that would fit in  memory and trained a 
Naive Bayes algorithm over each of the subproblems. 
We split the data evenly into several sets by putting each 
ith  line in  the  binary  into the  ( i  mod  n)th  set where  11  is 
the number of sets. For each set we trained a Naive Bayes 
classifier.  Our prediction for a binary is the product of the 
predictions of the n classifiers.  In our experiments we use 
6 classifiers (n = 6). 
More formally, the Multi-Naive Bayes promotes a vote 
of confidence between  all  of  the  underlying Naive Bayes 
classifiers.  Each classifier gives a probability of  a class C 
given  a set of bytes F  which the Multi-Naive Bayes uses 
to generate a probability for class C given  F  over all the 
classifiers. 
We  want  to compute  the  likelihood  of  a  class C given 
bytes  F  and  the  probabilities  learned  by  each  classifier 
NaiveBayesi.  In  equation  (4)  we  computed  the  likeli- 
hood, LNB(CIF), of class C given a set of bytes F .  
n 
lNBl 
LNB(CIF) = 
PNB; (CIF)/PNB< ( C )  
(4) 
i = l  
where NBi is a Naive Bayes classifier and N B  is the set 
of  all  combined  Naive  Bayes  classifiers  (in  our case  6). 
P N B i  (CIF) (generated from equation ( 2 ) )  is the probabil- 
ity  for class C computed  by  the  classifier NaiveBayesi 
given  F  divided  by  the  probability  of  class C computed 
by  NaiweBayesi.  Each  PNB.(CIF) was  divided  by 
PNB; (C) to  remove  the  redundant probabilities.  All  the 
terms were multiplied together to compute LNB(CIF), the 
final likelihood of C  given F .   INBJ is the size of the set 
NB such that VNBidVB. 
The output of the multi-classifier given a set of bytes F 
is  the  class  of  highest  probability  over the  classes  given 
LNB(CIF) and  PNB(C) the  prior probability of  a  given 
class. 
Most Likely Class = m a x  ( P N B ( C )  * L N B ( C I F ) )  ( 5 )  
c 
Most Likely Class is the class in C with the highest prob- 
ability hence the most  likely  classification of  the example 
with features F ,  and maze returns the class with the high- 
est likelihood. 
6  Rules 
Each  data mining algorithm generated its  own rule set to 
evaluate new examples.  These detection models were the 
final  result of  our experiments.  Each  algorithm’s rule  set 
could  be  incorporated  into  a  scanner  to  detect  malicious 
programs.  The generation of  the rules only  needed to be 
done periodically  and  the  rule  set  distributed  in  order to 
detect new malicious executables. 
6.1  RIPPER 
RIPPER’S rules were built to generalize over unseen exam- 
ples  so  the  rule set was more compact than  the  signature 
44 
based  methods.  For the data set that contained 3,301 ma- 
licious executables the RIPPER rule set contained the five 
rules in Figure 5. 
malicious 
:= 
malicious 
:= 
malicious 
malicious 
benign 
:= 
:= 
: - 
yuser32.EndDiaZog() A 
kernel32.EnumCalendarIn f oA() 
-.~ser32.LoadIconA() A 
~kernel32.GetTempPathA() A ladvapi32. 
shell32.Extract.4ssociatedIcon.4() 
msvbvm. 
otherwise 
Figure 5: Sample Classification Rules using features found 
in Figure 2 
Here, a malicious executable was consistent with one of 
four hypotheses: 
1. 
it did not  call  user32.EndDialog() but  it did call ker- 
nel32.EnumCalendarInfoA() 
2. 
did 
not 
call 
it 
nel32.GetTempPathA(),  or  any 
vapi32.dll 
user32.LoadIconA(), 
function 
ker- 
in  ad- 
3. 
it called shell32.ExtractAssociatedlconA(), 
4. 
it  called  any  function  in  msvbbm.dll, the  Microsoft 
Visual  Basic Library 
A 
binary is labeled benign if it is inconsistent with all of 
the malicious binary hypotheses in Figure 5. 
6.2  Naive Bayes 
The Naive  Bayes rules  were  more  complicated  than  the 
RIPPER and signature based hypotheses. These rules took 
the form of P(FIC), the probability of an example F given 
a class C. The probability for a string occurring in a class 
is the total  number of times it occurred in that class’s train- 
ing set divided by  the total number of times that the string 
occurred over the entire training set. 
These hypotheses are 
illustrated in Figure 6. 
Here, the string “windows” was predicted to more likely 
to occur in a benign program and string “*.COM”  was more 
than likely in a malicious executable program. 
However this  leads  to  a  problem  when  a  string  (e.g. 
“CH20H-CHOH-CH20H”) only  occurred in  one set, for 
example only  in  the  malicious  executables.  The proba- 
bility of “CH20H-CHOH-CH20H” occurring in any future 
benign  example is predicted  to be 0, but  this  is an  incor- 
rect assumption. If a Chemistry TA’s  program was written 
to print out “CH20H-CHOH-CH20H” (glycerol) it will al- 
ways be tagged a malicious executable even if it has other 
strings in it that would have labeled it benign. 
In Figure 6 the string “*.COM” does not occur in any be- 
nign programs so the probability of “*.COM” occurring in 
class  benign  is  approximated to  be  1/12 instead  of  0/1 I. 
This  approximates real  world  probability that  any  string 
could  occur in  both  classes even  if during training  it  was 
only seen in one class [9]. 
6.3  Multi-Naive Bayes 
The rule  sets generated by  our  Multi-Naive Bayes  algo- 
rithm are the collection  of the rules  generated  by  each of 
the component Naive Bayes classifiers. For each classifier, 
there is a rule set such as the one in Figure 6. The probabili- 
ties in the rules for the different classifiers may be different 
because  the underlying data that  each classifier  is  trained 
on is different. The prediction of the Multi-Naive Bayes al- 
gorithm is the product of  the predictions of  the underlying 
Naive Bayes classifiers. 
7  Results and Analysis 
We estimate our results over new data by using 5-fold cross 
validation [ 121. Cross validation is the standard method to 
estimate likely predictions over unseen  data in  Data Min- 
ing.  For each set of binary profiles we partitioned the data 
into 5  equal size groups.  We used  4  of the partitions  for 
training and then evaluated the rule set over the remaining 
partition.  Then  we  repeated  the  process 5  times leaving 
out a  different partition  for  testing  each time.  This gave 
us a  very reliable  measure of our method’s accuracy  over 
unseen data.  We  averaged the results of these five tests to 
obtain a good measure of how the algorithm performs over 
the entire set. 
To  evaluate our  system  we  were  interested  in  several 
quantities: 
P( “windows” /benign) 
P( “windows” /malicious) 
P( ‘‘ * .COArl”(benign) 
P( ‘‘ * .COAPJmaZicious) 
=  45/47 
=  2/47 
=  1/12 
=  1 1 / 1 2  
Figure  6: 
Bayes. 
Sample  classification  rules  found  by  Naive 
1.  True  Positives  (TP), the  number  of  malicious exe- 
cutable examples classified as malicious executables 
2.  True Negatives (TN), the number of benign programs 
classified as benign. 
3.  False Positives (FP), the number of benign programs 
classified as malicious executables 
45 
100 
80 - 
70 - 
60 - 
50 - 
40  - 
30 
20 
10 
0 
I 
RIPPER with Function Calls - 
RIPPER with DLLs Only 
RIPPER with Counted Function Calls 
Signature Method 
5 
20 
10 
False Positive Rate % 
15 
25 
30 
Figure 7.  RIPPER  ROC. Notice  that  the RIPPER  curves 
have  a  higher  detection  rate  than  the comparison method 
with false-positive rates greater than 7%. 
7.3  Naive Bayes 
The Naive  Bayes algorithm using  strings as  features per- 
formed the best  out  of  the  learning algorithms and better 
than  the  signature method  in  terms  of  false positive  rate 
and overall accuracy (see Table 3). It is the most accurate 
algorithm  with  97.1 1 % and  within  1 % of  the highest de- 
tection rate, Multi-Naive Bayes with 97.76%. It performed 
better than the RIPPER methods in every category. 
In  Figure 8, the  slope of the Naive Bayes curve is  ini- 
tially much steeper than the Multi-Naive Bayes. The Naive 
Bayes with  strings algorithm has better detection rates for 
small false positive rates. Its results were greater than 90% 
accuracy with  a false positive rate less than 2%. 
100 
90 
80 
8  70 
60 
a 5  50 
.- 
; 40 
30 
20 
10 
0 
c 
11 
f 
0 
................................................................................................ 
Naive Bayes with  Strings  __ 
Multi-Naive Bayes with  Bytes  ------...-. 
Signature Method 
2 
6 
4 
0
False Positive Rate % 
E 1
1
2
1
4
j i 
4.  False  Negatives  (FN), the  number  of  malicious exe- 
cutables classified as benign binaries. 
We were interested in the detection rate of the classifier. 
In  our case this was  the  percentage of  the total  malicious 
programs  labeled  malicious.  We  were  also interested  in 
the false positive  rate.  This was the percentage  of  benign 
programs which were labeled as malicious, also called false 
alarms. 
The  Detection  Rate  is  defined  as  *, 
False 
and  Overall  Accuracy  as 
The results of all experiments are pre- 
Positive  Rate  as  &, 
T P + ~ ~ ~ ~ $ + F N .
sented in Table 3. 
For all  the  algorithms  we plotted  the  detection  rate vs. 
false positive rate using Receiver Operating Characteristic 
(ROC) curves [ I  I].  ROC curves are a  way  of  visualizing 
the trade-offs between detection and false positive rates. 
7.1  Signature Method 
As is shown in Table 3, the signature method had the lowest 
false positive rate, 0% This algorithm also had  the  lowest 
detection rate, 33.7570, and accuracy rate, 49.28%. 
Since we  use  this method  to compare with the learning 
algorithms we plot its ROC curves against the RIPPER al- 
gorithm in Figure 7 and against the Naive Bayes and Multi- 
Naive Bayes algorithms in  Figure 8. 
The detection rate of  the signature-based method  is  in- 
herently low over new executables because the signatures 
generated were never designed to detect new malicious ex- 
ecutables. Also it should be noted that although the signa- 
ture based method only detected 33.75% of new malicious 
programs, the method did detect  100% of the malicious bi- 
naries that it had seen before with a 0% false positive rate. 
7.2  RIPPER 
The RIPPER results  shown in  Table 3 are roughly equiv- 
alent to each other in detection rates and overall accuracy, 
but the method using features from Figure 2, a list of DLL 
function calls, has a higher detection rate. 
The ROC curves for all RIPPER variations are shown in 
Figure 7.  The lowest line represents RIPPER  using DLLs 
only  as  features, and  it  was roughly  linear  in  its  growth. 
This means that  as  we  increase  detection rate  by  5%  the 
false positive would also increase by roughly 5%. 
The other lines  are concave down  so there was  an  op- 
timal  trade-off  between  detection  and  false  alarms.  For 
DLL's  with Counted Function Calls this optimal point was 
when the false positive rate was  10% and the detection rate 
was equal to 75%. For DLLs with Function Calls the opti- 
mal point was when the false positive rate was  12% and the 
detection rate was less than 90%. 
Figure 8: Naive Bayes and Multi-Naive Bayes ROC. Note 
that the Naive Bayes and Multi-Naive Bayes methods have 
higher  detection  rate  than  the  signature  method  with  a 
greater than 0.5% false positive rate. 
46 
11 
Profile 