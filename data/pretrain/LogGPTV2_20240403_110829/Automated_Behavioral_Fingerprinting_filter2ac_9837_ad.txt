method with the standardized weighted character position metric.
The SVC method instantiated with the IMAP does not improve the clustering
accuracy since in the best case, only 36% of the messages are well classiﬁed. Hence,
doing the second step with nearest neighbors technique is necessary and allows to
obtain 49% of good classiﬁcation. This is slightly better than the nearest neighbors
194
J. Fran¸cois et al.
technique as it was 47% on the ﬁgure 11. Obviously, this diﬀerence is very low
but the number of diﬀerent types found increases from 62% to 96% with SVC.
Therefore, even if the combined method doesn’t improve the classiﬁcation rate, it
is able to keep the classiﬁcation rate stable and at the same time discovering more
message types. The ﬁgure 10 shows the accuracy of SVC for the SMTP traﬃc.
Since the nearest neighbors technique was able to ﬁnd most of the types in ﬁgure
11, SVC can also ﬁnd them. Moreover, the classiﬁcation rate is greatly improved:
72% of messages are correctly identiﬁed and 80% of kinds are found. By applying
the nearest neighbors technique on the obtained clusters, the results are very close
because only one additional type is identiﬁed with one packet. Hence, the number
of discovered types is 90%. The standard deviation of the classiﬁcation rate per
type is quite high (0.39) principally due to one type totally ignored in both cases.
5.7 Semi Automated Parameters Identiﬁcation
The assessing of the classiﬁcation results for a known protocol is easy. The same
is much more diﬃcult with unknown because no reference exists. The ﬁrst con-
clusion of our study is that the standardized weighted character position metric is
the most suitable. For SVC technique, there are two parameters: C and q. In our
experiments C has not a great impact as it is highlighted in ﬁgure 10. This value
is constrained by the SVC technique itself and we have β initi  1 else 0
(12)
evol c(qi) =
maxi(#c(qi))
|w(qi) − w(qi−1)|
maxi(w(qi))
evol w(qi) =
if i > 1 else 0
(13)
Automated Behavioral Fingerprinting
195
evol_w(q)
evol_c(q)
 0  1  2  3  4  5  6  7  8  9  10
q
 0.5
 0.4
 0.3
 0.2
 0.1
 0
Fig. 12. SVC Gaussian width q selection (IMAP)
These are plotted in the ﬁgure 12 for the IMAP protocol. Several peaks of
evol w(q) exist, but the last one for q = 8.5 can be easily discarded because
the number of clusters decreases in the same time. The second one is interest-
ing since it represents the value selected by hand (q = 5.5) and the clusters
number increases in the same time. This semi automated technique is able to
ﬁnd a good parameter. Finally, the ﬁrst peak (q = 2) is not so high but it con-
cerns simultaneously both metrics. By testing this value, the classiﬁcation rate is
slightly improved by reaching 50%. With others protocols, this approach is able
to identify the same optimal parameters which were found by manual testing.
Identifying optimal parameters for the nearest neighbors technique can be based
on known techniques like [35].
To conclude, combination of weighted normalized position metric and SVC
technique is often able to improve the recognition of the messages types. By
doing a second phase based on the nearest neighbors technique, the results are
always improved.
6 Behavioral Fingerprinting
In order to best describe our approach, we will use the example illustrated in
ﬁgure 13. The messages exchanged between two parties are captured ﬁrst. Sec-
ondly, each message can be mapped to the corresponding message type. This is
done using the clustering mechanism described previously. Once each message
is mapped to its cluster, a session of captured messages can be represented as a
sequence of clusters. A session is composed of the messages exchanged between
two entities without a relative long inactivity period. in fact, TCP based protocol
sessions are easily distinguishable
The original sequence of messages can be mapped to the sequence of clusters:
{m1(A → B), m2(B → A), m3(A → B), m4(A → B)}
≡ {c(m1)(A → B), c(m2)(B → A), c(m3)(A → B), c(m4)(A → B)}
≡ {c1(A → B), c2(B → A), c2(A → B), c3(A → B)}
196
J. Fran¸cois et al.
(a) Device A
(b) Device B
Fig. 13. Session example
Fig. 14. Kernel trees
≡ {!c1, ?c2, !c2, !c3}A→B
We use the notation ?x for a message x that is received and the notation !x to
model that a message of type x is emitted by a device.
A second capture might consist in the sequence of messages (ﬁgure 13):
{m5(A → B), m6(B → A), m7(A → B), m8(B → A)}
≡ {c(m5)(A → B), c(m6)(B → A), c(m7)(A → B), c(m8)(B → A)}
≡ {c1(A → B), c4(B → A), c5(A → B), c4(B → A)}
≡ {!c1, ?c4, !c5, ?c4}A→B
In the same manner, the ﬁnal capture consists in the sequence:
{!c1, ?c4, ?c5, !c6}A→B
Then a tree based representation can summarize them as follows: a simple algo-
rithm adds incrementally a sequence to the tree. This is done by checking the longest
preﬁx of the chain that is also a path sourced (in the root) in the tree. Starting with
the last node in this path, the remaining suﬃx is added as a chain to the tree.
We construct for each device, a device speciﬁc trees - see ﬁgure 14. Each node
in the tree corresponds to a message type. An edge in the tree links two nodes
if the corresponding message types have been observed to succeed in the traces
Although known to be NP complete (see [36],[37] and [38] for good overviews
on this topic), the existing heuristics for doing it are based on building tree
representations for the underlying ﬁnite state machine. In our approach we don’t
prune the tree and although the ﬁnal tree representation is dependent on the
order in which we constructed the tree, we argue that the resulting substrees
are good discriminative features. We follow a supervised training method, where
protocol trees are labeled with the identity of their class. The identity of the
class is assumed to be known. For instance, the ﬁgure 14(a) shows that based
on traces, a node can start by sending an INVITE message (!c1) and receiving
afterwards 180 (?c2) or 407 (?c4) typed messages. In SIP, a 180 typed message
is used to learn that the call in in progress, while 400 messages are related to
authentication requests.
Automated Behavioral Fingerprinting
197
The core idea behind behavioral ﬁngerprinting consists in identifying subtrees
in the underlying tree representations that can uniquely diﬀerentiate between
two observed behaviors. We developed a classiﬁcation method based on trees
kernels in order to take into account the peculiar nature of the input space. Tree
kernels for support vector machines have been recently introduced in [39], [40],
[41] and do allow to use substructures of the original sets as features. These
substructures are natural candidates to evaluate the similitude and diﬀerentiate
among tree-like structures. We have considered two kernel types introduced in
[42],[43] and [41]: the subtree (ST) kernel and the subset tree kernel (SST).
Simply stated a subtree (ST) of a node is just the complete subtree rooted in
that node. A subset tree corresponds to a cut in the tree - a subtree rooted
in that node that does not include the original leaves of the tree. For instance,
the ﬁgure 14 highlights some examples of similar SST and ST for two trees.
Figure 14(a) represents a very simple tree corresponding to a Linksys SIP Phone.
In the context of behavioral ﬁngerprinting, a device speciﬁc protocol tree can
be mapped to a set of ST and SST features by extracting all underlying SSTs
and STs. Two protocol trees generated by two diﬀerent devices (ﬁgure 14) can
now be compared by decomposing each tree in its SSTs and STs followed by
a pair-wise comparison of the resulted SSTs and STs. This can be done using
tree kernels as proposed in [41]. The idea behind tree kernels is to count the
number of similar SSTs in both features sets and/or check the exact matching of
underlying STs. The interested reader is referred to [41] for more completeness
and fast implementation techniques.
For our purposes, similar substructures correspond to similar behavior in terms
of exchanged messages and represent thus a good measure of how much two de-
vices are similar with respect to their behavior. We collected traces from a real
VoIP testbed using more than 40 diﬀerent SIP phones and SIP proxies. In the
learning phase, we trained the support vector machines using a modiﬁed version
of the svm-light -TK [44] developed by Alessandro Moschitti. Our dataset con-
sisted in complete SIP traces obtained during a one day capture from a major
VoIP provider. The capture ﬁle (8 GB) contained only the signaling SIP related
data. Using the user-agent banner, we could identify 40 diﬀerent end devices. We
have also observed traﬃc coming from user-agents that were not identiﬁable. This
latter is due probably to some topology hiding performed by home routers or ses-
sion border controllers. For each device/user-agent we constructed the underlying
tree representations using a maximum of 300 SIP dialogs. Therefore, devices that
generated more than 300 dialogs, were tagged with more than one tree represen-
tation. We performed a multi-class classiﬁcation using the one versus all method
described in [41]. The classiﬁcation precision was 80 % which is a relative promis-
ing result. This result was obtained using a 5 fold validation technique - one ﬁfth
of the data was taken out and used to assess the accuracy/precision of the system.
The remaining four ﬁfths of data was used to train the system. Table 1 summa-
rizes a subset of the SIP devices used for training/testing. We could not include
the complete table in the paper due to space constraints. However, the diﬀerent
columns of table 1 give a glance of the data samples and associated tree structures.
198
J. Fran¸cois et al.
Table 1. Tested equipment
#Msgs #Sessions #Dialogs #Nodes Depth
Device
TrixboxCE v2.6.0.7 1935
Twinkle v1.1
421
Thomson2030 v1.59 345
457
Cisco-7940 v8.9
397
Linksys v5.1.8
SJPhone v1.65
627
714
129
102
175
130
246
544
109
83
139
67
210
279
146
105
54
206
66
99
36
34
18
99
19
For instance, the Tribox CE (a popular VoIP PBX) has a tree representation of
depth 99 and 279 nodes. This was learned using 1935 messages split over 544 SIP
dialogs.
7 Conclusion and Future Work
We have addressed in this paper the automated ﬁngerprinting of unknown pro-
tocols. Our approach is based on the unsupervised learning of the types of mes-
sages that are used by actual implementations of that protocol. The unsupervised
learning method relies on support vector clustering - SVC. Our technique is using
a new metric - the weighted character position metric. This metric is computed
rapidly and does not suppose any knowledge about the protocols: header ﬁelds
speciﬁcation, number of messages. One main advantage of the SVC technique
is its improvement of the accuracy of the classiﬁcation for large datasets. We
have also proposed a semi automated method that allows to choose the best
parameters. The observed message types can be used to induce a tree-like rep-
resentation of the underlying state machines. The nodes in this tree represent
the diﬀerent types of observed messages and the edges do indicate an invocation
relationship between the nodes. This ﬁrst phase is completed by a second stage,
where the behavioral diﬀerences are extracted and mined. This second phase
uses tree kernel support vector machines to model the ﬁnite state machines in-
duced from the ﬁrst phase. The main novelty of this approach lies in the direct
usage and mining of the induced state machines. We did test our approach on
extensive datasets for several well known protocols: SIP, SMTP and IMAP. The
observed empirical accuracy is very good and promising. We plan to extend this
work towards other machine learning tasks and conceptual solutions. In addition,
ﬁnding speciﬁc metrics for encrypted and binary protocols is another direction
for future work.
Acknowledgments.We would like to thank Yann Guermeur, researcher at
CNRS, for his support and feedback on the SVC speciﬁc implementation. This
work was partially supported by the French National Research Agency under
the VAMPIRE project ref#ANR-08-VERS-017.
Automated Behavioral Fingerprinting
199
References
1. Tridgell, A.: How samba was written,
http://samba.org/ftp/tridge/misc/french_cafe.txt (accessed on 03/16/09)
2. Lin, Z., Jiang, X., Xu, D., Zhang, X.: Automatic protocol format reverse engineer-
ing through conectect-aware monitored execution. In: 15th Symposium on Network
and Distributed System Security, NDSS (2008)
3. Cui, W., Paxson, V., Weaver, N., Katz, R.H.: Protocol-independent adaptive replay
of application dialog. In: Symposium on Network and Distributed System Security,
NDSS (2006)
4. Leita, C., Mermoud, K., Dacier, M.: Scriptgen: an automated script generation tool
for honeyd. In: Computer Security Applications Conference, Annual, pp. 203–214
(2005)
5. Arkin, O.: Icmp usage in scanning: The complete know-how, version 3 (June 2001)
(accessed on 03/16/09)
6. tcpdump, http://www.tcpdump.org/ (accessed on 02/05/09)
7. Beddoe, M.: Protocol informatics, http://www.4tphi.net (accessed on 02/05/09)
8. Cui, W., Peinado, M., Chen, K., Wang, H.J., Irun-Briz, L.: Tupni: automatic re-
verse engineering of input formats. In: CCS 2008: Proceedings of the 15th ACM
conference on Computer and communications security, pp. 391–402. ACM, New
York (2008)
9. Caballero, J., Yin, H., Liang, Z., Song, D.: Polyglot: automatic extraction of pro-
tocol message format using dynamic binary analysis. In: CCS 2007: Proceedings of
the 14th ACM conference on Computer and communications security, pp. 317–329.
ACM, New York (2007)
10. Gopalratnam, K., Basu, S., Dunagan, J., Wang, H.J.: Automatically extracting
ﬁelds from unknown network protocols (June 2006)
11. Weidong: Discoverer: Automatic protocol reverse engineering from network traces,
pp. 199–212
12. Wondracek, G., Comparetti, P.M., Kruegel, C., Kirda, E.: Automatic network pro-
tocol analysis. In: Proceedings of the 15th Annual Network and Distributed System
Security Symposium, NDSS 2008 (2008)
13. Shevertalov, M., Mancoridis, S.: A reverse engineering tool for extracting protocols
of networked applications, October 2007, pp. 229–238 (2007)
14. Newsome, J., Brumley, D., Franklin, J., Song, D.: Replayer: automatic protocol
replay by binary analysis. In: CCS 2006: Proceedings of the 13th ACM conference
on Computer and communications security, pp. 311–321. ACM, New York (2006)
15. Comer, D., Lin, J.C.: Probing TCP Implementations. In: USENIX Summer, pp.
245–255 (1994)
16. P0f, http://lcamtuf.coredump.cx/p0f.shtml
17. Nmap, http://www.insecure.org/nmap/
18. Caballero, J., Venkataraman, S., Poosankam, P., Kang, M.G., Song, D., Blum,
A.: FiG: Automatic Fingerprint Generation. In: The 14th Annual Network & Dis-
tributed System Security Conference (NDSS 2007) (February 2007)
19. Scholz, H.: SIP Stack Fingerprinting and Stack Diﬀerence Attacks. Black Hat Brief-
ings (2006)
20. Yan, H., Sripanidkulchai, K., Zhang, H., yin Shae, Z., Saha, D.: Incorporating Ac-
tive Fingerprinting into SPIT Prevention Systems. In: Third Annual VoIP Security
Workshop (June 2006)
200
J. Fran¸cois et al.
21. Ma, J., Levchenko, K., Kreibich, C., Savage, S., Voelker, G.M.: Unexpected means
of protocol inference. In: Almeida, J.M., Almeida, V.A.F., Barford, P. (eds.) Inter-
net Measurement Conference, pp. 313–326. ACM, New York (2006)
22. Haﬀner, P., Sen, S., Spatscheck, O., Wang, D.: Acas: automated construction of
application signatures. In: Proceedings of the 2005 ACM SIGCOMM workshop on
Mining network data (Minet), pp. 197–202. ACM, New York (2005)
23. Abdelnur, H.J., State, R., Festor, O.: Advanced Network Fingerprinting. In: Lipp-
mann, R., Kirda, E., Trachtenberg, A. (eds.) RAID 2008. LNCS, vol. 5230, pp.
372–389. Springer, Heidelberg (2008)
24. Crocker, D., Overell, P.: Augmented BNF for Syntax Speciﬁcations: ABNF. RFC
2234 (Proposed Standard) (1997)
25. Rosenberg, J., Schulzrinne, H., Camarillo, G., Johnston, A., Peterson, J., Sparks,
R., Handley, M., Schooler, E.: SIP: Session Initiation Protocol. RFC 3261 (Pro-
posed Standard), Updated by RFCs 3265, 3853, 4320, 4916, 5393 (2002)
26. Schulzrinne, H., Casner, S., Frederick, R., Jacobson, V.: RTP: A Transport Protocol
for Real-Time Applications. RFC 3550 (Standard), Updated by RFC 5506 (2003)
27. Kr¨ugel, C., Toth, T., Kirda, E.: Service speciﬁc anomaly detection for network
intrusion detection. In: SAC 2002: Proceedings of the 2002 ACM symposium on
Applied computing, pp. 201–208. ACM, New York (2002)
28. Ben-hur, A., Horn, D., Siegelmann, H.T., Vapnik, V.: Support vector clustering.
Journal of Machine Learning Research 2, 125–137 (2001)
29. Day, W.H., Edelsbrunner, H.: Eﬃcient algorithms for agglomerative hierarchical
clustering methods. Journal of Classiﬁcation 1(1), 7–24 (1984)
30. Cortes, C., Vapnik, V.: Support-vector networks. Machine Learning 20(3), 273–297
(1995)
31. Wang, L. (ed.): Support Vector Machines: Theory and Applications. Studies in
Fuzziness and Soft Computing, vol. 177. Springer, Heidelberg (2005)
32. Berkhin, P.: A survey of clustering data mining techniques. In: Grouping Multidi-
mensional Data, pp. 25–71 (2006)
33. Klensin, J.: Simple Mail Transfer Protocol. RFC 2821 (Proposed Standard), Ob-
soleted by RFC 5321, updated by RFC 5336 (April 2001)
34. Crispin, M.: Internet Message Access Protocol - Version 4rev1. RFC 3501 (Pro-
posed Standard), Updated by RFCs 4466, 4469, 4551, 5032, 5182 (March 2003)
35. Salvador, S., Chan, P.: Determining the number of clusters/segments in hierarchical
clustering/segmentation algorithms. In: ICTAI 2004: Proceedings of the 16th IEEE
International Conference on Tools with Artiﬁcial Intelligence, Washington, DC,
USA, pp. 576–584. IEEE Computer Society, Los Alamitos (2004)
36. Rivest, R.L., Schapire, R.E.: Inference of ﬁnite automata using homing sequences.
In: STOC 1989: Proceedings of the twenty-ﬁrst annual ACM symposium on Theory
of computing, pp. 411–420. ACM, New York (1989)
37. Angluin, D.: Learning regular sets from queries and counterexamples. Inf. Com-
put. 75(2), 87–106 (1987)
38. Schapire, R.E.: Diversity-based inference of ﬁnite automata. Technical report,
Cambridge, MA, USA (1988)
39. Collins, M., Duﬀy, N.: New ranking algorithms for parsing and tagging: kernels over
discrete structures, and the voted perceptron. In: ACL 2002: Proceedings of the
40th Annual Meeting on Association for Computational Linguistics, Morristown,
NJ, USA, pp. 263–270 (2002)
40. Vishwanathan, S., Smola, A.: Fast kernels on strings and trees. In: Proceedings of
Neural Information Processing Systems (2002)
Automated Behavioral Fingerprinting
201
41. Moschitti, A.: Making tree kernels practical for natural language learning. In: Pro-
ceedings of the Eleventh International Conference on European Association for
Computational Linguistics (2006)
42. Moschitti, A.: Eﬃcient convolution kernels for dependency and constituent syn-
tactic trees. In: F¨urnkranz, J., Scheﬀer, T., Spiliopoulou, M. (eds.) ECML 2006.
LNCS (LNAI), vol. 4212, pp. 318–329. Springer, Heidelberg (2006)
43. Moschitti, A., Pighin, D., Basili, R.: Tree kernel engineering for proposition re-
ranking. In: Proceedings of Mining and Learning with Graphs, MLG 2006 (2006)
44. Moschitti, A.: M-light-tk 1.2 (feature vector set and tree forest) (2009)