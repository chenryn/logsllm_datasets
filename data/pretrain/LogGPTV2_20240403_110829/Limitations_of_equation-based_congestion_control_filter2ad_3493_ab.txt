in place of which is
computed by dividing the number of loss events observed from
that ﬂow by the total number of packets transmitted within the
observation period [9]. TFRC registers a loss event as follows.
The ﬁrst packet loss is counted as a loss event. Following this,
there is a back-off for the duration of an RTT during which no
packet loss is counted. The next packet loss after this back-off
is counted as another loss event, followed by another RTT back-
off, and so forth. We note that
.
.
is an unbiased estimator of
(A2) We assume that
Thus,
(A3) We assume that RTT does not change within a ﬂow.
(A4) We also assume that all the network ﬂows are using
the same data packet size. Acknowledgment packets may
have different sizes.
.
and
We use the difference ratio as the main metric for showing
difference between two quantities. The difference ratio of quan-
tities
where
otherwise. That is, when
is smaller than , its difference ratio is negative and otherwise
it is positive. We use this metric because it treats both negative
and positive differences by the same proportion.
is deﬁned to be
if
, and
.
III. SIMULATION SETUP
To verify the theoretical ﬁndings through experiments, we
conduct ns simulation. Our setup uses a typical dumbbell
topology where each network ﬂow is connected to the bottle-
neck link through independent access links at both destination
and source. The bandwidth and one-way delay of the bottleneck
link are set to 15 Mbps and 50 ms unless noted otherwise. The
link implements RED with the default adaptive setting and the
buffer size is limited to two times the bandwidth delay product.
Each TCP and TFRC source and sink are connected through
different access links to the bottleneck link and the delays of the
access links are randomly varied from 1 to 3 ms to remove any
phase effect. We ﬁx the number of TFRC ﬂows to 5 and also
have the matching number of TCP ﬂows sharing bottleneck
links with the TFRC ﬂows. These ﬂows are used to compare
the performance of TCP and TFRC. To observe the behavior
of TCP and TFRC under various network loads, we add back-
ground long-lived TCP ﬂows to the forward direction. The
number of background long-lived TCP ﬂows are varied from
5 to 400. For each run, web trafﬁc is added to the forward and
backward directions of the bottleneck link and emulate about
20 to 100 web sessions and the web trafﬁc occupies about 20
to 60% of the bottleneck bandwidth depending on the network
load and bandwidth. The web trafﬁc model of ns is close to
SURGE [4]. To increase dynamics on the bottleneck link, 50
short-term TCP ﬂows with random starting and ending times
are added to both directions. Random burst UDP trafﬁc with
the Pareto distribution is also added to the forward direction
occupying about 1 to 2% of the bottleneck bandwidth. To mea-
sure the delay and packet loss rate in the bottleneck link, ping
trafﬁc to the forward direction (with 100-ms interval) is added
occupying much less than 1% of the bottleneck bandwidth. We
run the simulation for 1000 s and took the measurement after
the ﬁrst 200 s.
IV. IMPACT OF LOSS EVENT RATES
Various reasons have been identiﬁed by previous studies for
the loss event rate difference:
(R1) TFRC reacts to transient congestion more slowly than
TCP. Several studies [3], [24] show that this slow respon-
siveness may cause TFRC to see more loss event rates than
TCP.
(R2) Under very low-level multiplexing where only one or
two ﬂows coexist, TCP can have a higher loss event rate
than TFRC [24].
(R3) Bonald et al. [5] show that in the drop tail router, a
bursty trafﬁc such as TCP may experience more loss than
CBR. This behavior is less pronounced in a RED router
(note that the packet loss rate used in [5] is the total number
of lost packets over the total number of packets transmitted
so it is different from ).
In this section, we show that the loss event rate difference
also occurs when and because TCP and TFRC ﬂows competing
in the same bottleneck link transmit at different sending rates.
In particular, we show that
(R4) If the sending rate of TFRC is “sufﬁciently” lower
than that of TCP, then
(R5) If the sending rate of TFRC is “sufﬁciently” higher
than that of TCP, then
.
.
In what follows, we qualify the term “sufﬁciently.”
A. Difference in Average Loss Event Rates
round-trip times. Let
We compare the number of packets sent over a TFRC con-
nection over a measurement period
, to that sent over a TCP
be equal
connection running in the same end-to-end path. Let
,
to
be the numbers of packets sent in the th round-trip time over
the TCP and TFRC connections respectively. Therefore, the
numbers of packets sent over the TCP and TFRC sessions, re-
spectively, are given by
and
and
,
.
The probability of having no packet losses in a window of size
is the probability that a packet is
lost independently from losses in the previous RTT rounds (as
, where
is
RHEE AND XU: LIMITATIONS OF EQUATION-BASED CONGESTION CONTROL
deﬁned in Section II). The probability of having a loss event in
a window of size
By deﬁnition,
is the number of loss events divided by the
is therefore
.
Its geometric mean is
, where
.
. Therefore
. Also,
855
, so
number of transmitted packets
In the same way, we can deﬁne
as follows:
(3)
but
(4)
Strictly speaking, for ﬁnite
,
proximation for their corresponding loss event rates since
ergodic. For sufﬁciently large
the corresponding loss event rates with probability one.
are only an ap-
is
, these quantities converge to
, and
Equation (4) considers TFRC like a window-based protocol
although it is rate-based. This is reasonable because TFRC ad-
justs the sending rate at every RTT just like TCP, a window-
is the av-
based protocol, and
erage sending rate of TFRC during the th RTT interval (RTT ).
The following two lemmas are useful in proving our main
RTT where
theorem.
, and
for
for
below,
Theorem 4.1,
proves R4
,
TCP during
on the number of TFRC packets per RTT during
Let
and R5. Let
the mean window size of
, the mean
.
and
, the maximum
numbers of packets sent by the TCP and TFRC connections
respectively during an RTT period in
(which implies that the sending rate of TCP is larger than that of
TFRC), we prove that
(which implies that the sending rate of TFRC is larger than that
of TCP), we prove that
(i.e., R4). If
(i.e., R5).
. If
Theorem 4.1:
if
, and
Lemma 4.1: For any
such that
, a function
if
.
Proof: It sufﬁces to prove the ﬁrst case (R4). The second
case (R5) can be proved in the same way because TFRC and
TCP are treated essentially the same way in the proof.
is a monotonically decreasing function of
.
We have
Proof:
can be rewritten as
.
Since
,
for
, we have
Therefore, from Lemma 4.2, it is seen that
Lemma 4.2: Let
sequence
, and
, then
for some nonnegative integer
Also from Lemma 4.1
From Lemma 4.1, we have
(5)
(6)
Proof: Consider the sequence
Its arithmetic mean is given by
.
Taking the summation in both sides, we get
856
IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 15, NO. 4, AUGUST 2007
Simplifying the above, we get
Then from the above inequality, we can have the following
inequality:
(7)
The RHS of (7) is
. Combining (5)–(7), we can prove
.
end-to-end path. Let
time
and let
Suppose that a CBR ﬂow is competing with TCP in the same
be the sending rate of this ﬂow during
be the average sending rate of TCP during
, then the
, is always higher than
. We can prove the following case: if
loss event rate observed by the ﬂow,
.
Corollary 4.1:
Proof:
if
.
where
sent by a CBR ﬂow, because
the corollary is true.
is the number of packets in any RTT during
. By Theorem 4.1,
We can also prove that when two such CBR ﬂows
and
are competing and they have different sending rates, then the
following is trivially true from Theorem 4.1.
Corollary 4.2: If
, then
.
B. Simulation: Loss Event Rate Difference
In this section, we provide the experimental evidence that
sending rate difference between TCP and TFRC can cause the
loss event rate difference between them. Since many factors can
inﬂuence loss event rates, it is difﬁcult to isolate one factor from
the others. Thus, to facilitate our discussion, we assume that R1
through R5 discussed at the beginning of this section are the
only reasons that could force TCP and TFRC to have different
loss event rates.
To remove the effect of R2, we keep our simulation and ex-
perimental environments more dynamic by introducing a high
level of statistical multiplexing through different types of back-
ground trafﬁc and also a large number of competing ﬂows. To
remove the effect of R3, we use a RED router for our bottleneck
link and check whether all the ﬂows see the same packet loss
rates. But controlling and quantifying the effects of R1, R4, and
R5 separately from each other are not trivial as these effects may
collectively cause the loss event rate difference. For instance, to
remove the effect of R1, we have to keep the responsiveness of
TCP and TFRC the same (so that any loss event rate difference
is caused by the other factors). But that is not possible because
TCP is inherently more responsive than TFRC—for instance,
TFRC does not reduce its rate by half as TCP does for a loss
event. Likewise, removing the effects of R4 and R5 is not trivial
because we cannot force TFRC to send at the same rate as TCP
since the effect of R1 alone may force TFRC and TCP to have
different sending rates.
Fig. 1. Packet loss rates of CBR, TFRC, Ping, and TCP trafﬁc. All types of
ﬂows are experiencing similar packet loss rates under RED queue. Each run
consists of a different number of TCP ﬂows. The x axis represents different
runs.
We solve the “separation problem” by using a set of CBR
ﬂows, each with a different sending rate. Since our analysis in
Section IV-A assumes nothing about the way that the sending
rate is controlled, the analysis is still applicable to CBR. Fur-
thermore, CBR ﬂows have the maximum effect of R1 because
they do not respond to congestion at all. By mixing TFRC and
TCP ﬂows with CBR ﬂows, we can also prove additional prop-
erties about loss event rates. We discuss these properties below.
To our simulation setup discussed in Section III, we add ad-
ditional 14 CBR ﬂows, each with a different sending rate within
1 Mbps to 16 Kbps. The arrival of packets in a CBR ﬂow is ran-
domized without affecting the average sending rate to avoid the
phase effect [12]. We have ﬁve TFRC ﬂows running for each run
of the experiment. As discussed in Section III, the network load
is controlled by adjusting the number of long-lived TCP ﬂows.
We set the bottleneck bandwidth to 20 Mbps (with aggregate
CBR trafﬁc taking up about 2 Mbps), and keep the same back-
ground trafﬁc as discussed in Section III. For each run, we mea-
sure the loss event and sending rates of TCP, TFRC, and CBR
(the loss event rate of CBR is measured in the same way as in
TFRC).
We ﬁrst measure the packet loss rates of different types of
ﬂows in each run to make sure that we do not have the effect
of R3. The packet loss rate of a ﬂow is obtained from the total
number of lost packets divided by the number of packets trans-
mitted in the same way as in [5]. We plot the average values
in Fig. 1 for each type of ﬂows in each run which simulates
a different network load environment (created by varying the
number of long-lived TCP ﬂows). From the ﬁgure, we observe
that all the ﬂows in the same run experience the same packet
loss rates (note that this loss rate is different from ). This hap-
pens, as shown in [5], because the bottleneck queue performs
the random early drop (RED) policy and is consistent with the