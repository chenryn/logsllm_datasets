### Data Distribution and Class Balance
Our dataset comprises 7,629,541 samples, of which 78.2% are malicious and 3.76% (287,722) are gray. Further details on the distribution of vendor counts and tags can be found in Appendix A.1.

The ratios of malicious to benign data in our training, test, and validation sets are consistent, with malicious samples being more prevalent than benign ones. This class imbalance is notably different from real-world deployment scenarios, where malware is relatively rare. To avoid overfitting when training on unbalanced datasets, it is common practice to increase the prevalence of the minority class [3]. In our case, using a dataset with a higher proportion of malicious samples, assuming a sufficient number of benign samples, may lead to a more precise decision boundary and better overall performance as measured by the full ROC curve. Additionally, employing a malicious tags loss with a diverse set of malware can enhance the diversity of the tag set learned during training.

### Performance Measures and FPR Targets
ROC curves, used as performance measures in Sections 4 and 5, are independent of the class ratio in the test set, unlike accuracy. False positive rate (FPR) values depend only on the benign data, while true positive rate (TPR) values depend only on the malware. We focus on improvements in detection at very low FPRs (0.1% or below), as several publications by anti-virus vendors [25, 30] and our experience suggest that this is a practical FPR target for most deployment scenarios. Our model outputs can be rescaled to the desired deployment class ratio based on the provided ROC curve and/or standard calibration methods, such as fitting a weighted isotonic regressor on scores from the validation set.

### Experimental Evaluation
In this section, we evaluate the auxiliary losses presented in Section 3, both individually and in combination, alongside a main malicious/benign loss. We compare these models to a baseline and find that each loss term yields an improvement in either the Receiver Operating Characteristic (ROC) net area under the curve (AUC) or in terms of detection performance at low FPRs. None of the auxiliary losses harmed classification relative to the baseline; at worst, the loss-augmented models had equivalent performance. Each model used a loss weight of 1.0 on the aggregate malicious/benign loss and 0.1 on each auxiliary loss. The final loss function is given by:

\[
L(X, Y) = L_{\text{mal}}(X, Y) + 0.1 \sum_{k=1}^{K} L_k(X, Y)
\]

Results are depicted in Figure 2 and Table 1.

### Training and Results
Due to the intrinsic randomness in the training process of deep neural networks, we report results in terms of both the mean and standard deviation across five runs. Each model was trained five times with different random initializations and minibatch randomizations, while all other parameters were held constant. We compute the test statistic of interest (e.g., the detection rate at a false positive rate of \(10^{-3}\)) for each model and then compute the average and standard deviation of those results. The ROC curves in Figure 2 are plotted on a logarithmic scale for visibility, and we include relative reductions in mean true positive detection error and standard deviation from the baseline for our best model in Table 1.

### Vendor Count Loss
We used the same base PE model topology for our experiments, with a primary malicious/benign binary cross-entropy loss and an auxiliary count loss. We experimented with two different loss functions: a Poisson loss and a Restricted Generalized Poisson loss. For the Poisson loss, we used an exponential activation over a dense layer to estimate \(\mu(i)\). For the RG-Poisson loss, we used two separate dense layers with exponential activations, one for \(\mu(i)\) and another for \(\alpha(i)\).

Results in Figure 2a show a statistically significant improvement with the Poisson loss in both AUC and ROC curve, particularly in low FPR regions. The RG-Poisson loss, however, did not yield significant gains over the baseline. This suggests that the RG-Poisson loss model may be ill-fitted, possibly due to under-dispersion or an inappropriate model choice.

### Modeling Individual Vendor Responses
Incorporating an auxiliary multi-label binary cross-entropy loss across vendors in conjunction with the main malicious/benign loss yielded a similar increase in TPR at low FPR regions (Figure 2b). Although there was no significant increase in AUC, the improvement in TPR at lower FPRs is operationally significant.

### Incorporating Tags as Targets
We extended the base network to predict not only the malware/benign label but also a set of 11 tags. We added two fully connected layers per tag to the base architecture, each predicting a binary output corresponding to the presence/absence of the tag. The overall loss for this experiment is a sum of individual tag losses and the main malicious/benign loss, with a loss weight of 0.1 for each tag.

Results in Figure 2c show substantial statistical improvement in the predictions of the model in low FPR regions, particularly for FPR values lower than \(10^{-3}\). We also observed a substantial decrease in the variance of the ROC curve.

### Combined Model
Finally, we extended our model to predict all auxiliary targets in conjunction with the aggregate label, using a net loss term containing a combination of all auxiliary losses. The resulting ROC curve and AUC are shown in Figure 2d, with the highest AUC of 0.9972 ± 0.0001. The combined model showed noticeable improvements in the ROC curve not only in very low FPR regions but also at \(10^{-3}\) FPR, with consistently lower variance across a range of low-FPR values.

### Discussion
We examined the effects of different types of auxiliary loss functions on the main task ROC curve and performed a sanity check to determine whether the performance increases resulted from additional information or regularization effects.

#### Modes of Improvement
1. **Bump in TPR at low FPR (< \(10^{-3}\))**: This effect is pronounced in the plots due to the logarithmic scale and is important for operational deployment.
2. **Net increase in AUC and small bump in performance at higher FPRs (≥ \(10^{-3}\))**: This is more pronounced with auxiliary count loss, suggesting that providing information on the number of counts improves the ROC curve.
3. **Reduction in variance**: This occurs across all loss types, particularly in low FPR ranges, and is more pronounced as the number of losses increases.

Combining all losses accentuates all improvements, with predictable modes attributed to the constraints imposed by multiple objectives, leading to lower variance and better overall performance.